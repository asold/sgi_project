{
  "title": "Cloud-EGAN: Rethinking CycleGAN From a Feature Enhancement Perspective for Cloud Removal by Combining CNN and Transformer",
  "url": "https://openalex.org/W4379184889",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5055382963",
      "name": "Xianping Ma",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A5100772810",
      "name": "Yiming Huang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A5100412323",
      "name": "Xiaokang Zhang",
      "affiliations": [
        "Wuhan University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5040559125",
      "name": "Man-On Pun",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A5100705731",
      "name": "Bo Huang",
      "affiliations": [
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4206909333",
    "https://openalex.org/W6757526026",
    "https://openalex.org/W2900639982",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4294215798",
    "https://openalex.org/W2982296207",
    "https://openalex.org/W4285800390",
    "https://openalex.org/W3120917255",
    "https://openalex.org/W2804451008",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W4226051283",
    "https://openalex.org/W3043408700",
    "https://openalex.org/W4214760051",
    "https://openalex.org/W3009297390",
    "https://openalex.org/W4312585227",
    "https://openalex.org/W4213333386",
    "https://openalex.org/W2333267146",
    "https://openalex.org/W4321376230",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W4292829111",
    "https://openalex.org/W4285059972",
    "https://openalex.org/W3202923600",
    "https://openalex.org/W3197957534",
    "https://openalex.org/W4304099129",
    "https://openalex.org/W4281624580",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3190334976",
    "https://openalex.org/W6772853553",
    "https://openalex.org/W4226530543",
    "https://openalex.org/W4226013274",
    "https://openalex.org/W4210642697",
    "https://openalex.org/W3098881417",
    "https://openalex.org/W4285505614",
    "https://openalex.org/W3114924374",
    "https://openalex.org/W3119125170",
    "https://openalex.org/W4288391265",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2593414223",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W4319865627",
    "https://openalex.org/W4318681845",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W4207042493",
    "https://openalex.org/W2562637781",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W4206947033",
    "https://openalex.org/W4205659311",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4295832403",
    "https://openalex.org/W4322746808",
    "https://openalex.org/W2914757358",
    "https://openalex.org/W3114037660",
    "https://openalex.org/W2983309662",
    "https://openalex.org/W2095396982",
    "https://openalex.org/W2114770744",
    "https://openalex.org/W2940726923",
    "https://openalex.org/W2292891435",
    "https://openalex.org/W6798007176",
    "https://openalex.org/W6783602928",
    "https://openalex.org/W4206242275",
    "https://openalex.org/W6801063040",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2978980369",
    "https://openalex.org/W2907674209",
    "https://openalex.org/W3200041017",
    "https://openalex.org/W3179374681",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3104282073",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3088852543",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Cloud cover presents a major challenge for geoscience research of remote sensing images with thick clouds causing complete obstruction with information loss while thin clouds blurring the ground objects. Deep learning (DL) methods based on convolutional neural networks (CNNs) have recently been introduced to the cloud removal task. However, their performance is hindered by their weak capabilities in contextual information extraction and aggregation. Unfortunately, such capabilities play a vital role in characterizing remote sensing images with complex ground objects. In this work, the conventional cycle-consistent generative adversarial network (CycleGAN) is revitalized from a feature enhancement perspective. More specifically, a saliency enhancement (SE) module is first designed to replace the original CNN module in CycleGAN to re-calibrate channel attention weights to capture detailed information for multi-level feature maps. Furthermore, a high-level feature enhancement (HFE) module is developed to generate contextualized cloud-free features while suppressing cloud components. In particular, HFE is composed of both CNN- and transformer-based modules. The former enhances the local high-level features by employing residual learning and multi-scale strategies, while the latter captures the long-range contextual dependencies with the Swin transformer module to exploit high-level information from a global perspective. Capitalizing on the SE and HFE modules, an effective Cloud-Enhancement GAN, namely Cloud-EGAN, is proposed to accomplish thin and thick cloud removal tasks. Extensive experiments on the RICE and the WHUS2-CR datasets confirm the impressive performance of Cloud-EGAN.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023 4999\nCloud-EGAN: Rethinking CycleGAN From a\nFeature Enhancement Perspective for Cloud\nRemoval by Combining CNN and Transformer\nXianping Ma , Student Member, IEEE, Yiming Huang , Xiaokang Zhang , Member, IEEE,\nMan-On Pun , Senior Member, IEEE,a n dB oH u a n g\nAbstract—Cloud cover presents a major challenge for geoscience\nresearch of remote sensing images with thick clouds causing com-\nplete obstruction with information loss while thin clouds blurring\nthe ground objects. Deep learning (DL) methods based on convo-\nlutional neural networks (CNNs) have recently been introduced to\nthe cloud removal task. However, their performance is hindered by\ntheir weak capabilities in contextual information extraction and\naggregation. Unfortunately, such capabilities play a vital role in\ncharacterizing remote sensing images with complex ground objects.\nIn this work, the conventional cycle-consistent generative adversar-\nial network (CycleGAN) is revitalized from a feature enhancement\nperspective. More speciﬁcally, a saliency enhancement (SE) module\nis ﬁrst designed to replace the original CNN module in CycleGAN to\nre-calibrate channel attention weights to capture detailed informa-\ntion for multi-level feature maps. Furthermore, a high-level feature\nenhancement (HFE) module is developed to generate contextual-\nized cloud-free features while suppressing cloud components. In\nparticular, HFE is composed of both CNN- and transformer-based\nmodules. The former enhances the local high-level features by\nemploying residual learning and multi-scale strategies, while the\nlatter captures the long-range contextual dependencies with the\nSwin transformer module to exploit high-level information from a\nglobal perspective. Capitalizing on the SE and HFE modules, an\neffective Cloud-Enhancement GAN, namely Cloud-EGAN, is pro-\nposed to accomplish thin and thick cloud removal tasks. Extensive\nexperiments on the RICE and the WHUS2-CR datasets conﬁrm\nthe impressive performance of Cloud-EGAN.\nManuscript received 4 April 2023; revised 10 May 2023; accepted 23 May\n2023. Date of publication 2 June 2023; date of current version 8 June 2023. This\nwork was supported in part by the National Key R&D Program of China under\nGrant 2018YFB1800800, in part by the Basic Research Project under Grant\nHZQB-KCZYZ-2021067 of Hetao Shenzhen-HK S&T Cooperation Zone,\nShenzhen Outstanding Talents Training Fund 202002, Guangdong Research\nProjects under Grant 2017ZT07X152 and Grant 2019CX01X104, in part by the\nGuangdong Provincial Key Laboratory of Future Networks of Intelligence under\nGrant 2022B1212010001, in part by the National Natural Science Foundation\nof China under Grant 41801323, and in part by the National Key Research and\nDevelopment Program of China under Grant 2020YFA0714003.(Xianping Ma\nand Yiming Huang contributed equally to this work.) (Corresponding authors:\nXiaokang Zhang; Man-On Pun.)\nXianping Ma, Yiming Huang, and Man-On Pun are with the School of\nScience and Engineering, The Chinese University of Hong Kong, Shen-\nzhen 518172, China (e-mail: xianpingma@link.cuhk.edu.cn; 222012014@\nlink.cuhk.edu.cn; simonpun@cuhk.edu.cn).\nXiaokang Zhang is with the School of Information Science and Engineering,\nWuhan University of Science and Technology, Wuhan 430081, China (e-mail:\nnatezhangxk@gmail.com).\nBo Huang is with the Department of Geography, The University of Hong\nKong, Hong Kong SAR 999077, China (e-mail: bohuang@cuhk.edu.hk).\nDigital Object Identiﬁer 10.1109/JSTARS.2023.3280947\nIndex Terms —Cloud removal, cycle-consistent generative\nadversarial network (CycleGAN), feature enhancement, remote\nsensing images, transformer.\nI. INTRODUCTION\nE\nARTH observation technology has facilitated the acquisi-\ntion of remote sensing images. These images have been\nsuccessfully used to extract land surface information in many\ncritical applications, including object detection [1], [2], [3],\nscene classiﬁcation[4], [5], [6], and semantic segmentation[7],\n[8], [9], [10]. However, such optical satellite images are in-\nevitably susceptible to the atmospheric and illumination con-\nditions, which incurs degradation in image quality. In particular,\nremote sensing images commonly suffer from the contamination\nof cloud layers, signiﬁcantly diminishing the signal quality ob-\ntained by satellite sensors. Speciﬁcally, the cloud layers heavily\nreduce the visibility and saturation of images, hindering the\nsubsequent image applications[11]. While thin-cloud-covered\nregions still exhibit limited ground features, the contextual in-\nformation beneath thick clouds is completely lost. Compared\nwith natural digital images, remote sensing images contain more\ncomplex spatial structures and richer spectral information for\nground object characterization, making cloud removal more\nchallenging. Therefore, the development of efﬁcient signal pro-\ncessing algorithms is strongly desired to accurately recover the\ngenuine land surface information from remote sensing images\ndistorted by cloud layers. In the literature, existing cloud re-\nmoval methods can be classiﬁed into two approaches, namely\nconventional methods based on hand-crafted features and deep\nlearning (DL)-based methods [12], [13], [14], [15], [16],\n[17], [18].\nConventional methods, such as multitemporal dictionary\nlearning (MDL)[19], thin cloud removal using homomorphic ﬁl-\nter (TCHF)[20], and signal transmission principles and spectral\nmixture analysis (ST-SMA)[21], require hand-crafted features\nto estimate the cloud distribution. In particular, MDL learned\ndictionaries of cloud-covered and cloud-free regions separately\nin the spectral domain whereas TCHF utilized a classic homo-\nmorphic ﬁlter in the frequency domain. Furthermore, ST-SMA\nwas developed based on signal transmission and spectral mixture\nanalysis. Despite their many advantages, these methods were\ndesigned for thin cloud removal while overlooking the thick\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5000 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\ncloud scenarios. Moreover, their feasibility and performance are\ntypically limited by irregular cloud distribution and the choice\nof hand-crafted features.\nDriven by the rapid development of DL techniques, DL-based\ncloud removal methods have attracted substantial research atten-\ntion, owing to the superior performance of DL models in mining\nrepresentative features from remote sensing images[22].M o s t\nexisting DL-based cloud removal methods in the literature were\nbuilt upon convolutional neural networks (CNNs) by exploiting\nabstract and conceptual representations of remote sensing im-\nages. Generally speaking, DL-based networks for cloud removal\ncan be divided into two categories, namely the pure encoder–\ndecoder methods[11], [23], [24] and the generative adversarial\nnetworks (GAN)-based networks [12], [25], [26], [27], [28],\n[29], [30]. For the pure encoder–decoder networks, multiscale\nfeatures-CNN [23] explored the multiscale high-level features\nto detect thin-cloud, thick-cloud, and no-cloud pixels simulta-\nneously while residual learning and channel attention mecha-\nnism [11] integrated residual connection with a channel attention\nmechanism to capture details in different convolutional layers.\nFurthermore, conditional variational autoencoders (CV AE)[24]\napplied a probabilistic graphical model with CV AE to restore\ncloud-free images according to the image degradation process.\nThe abovementioned encoder–decoder models employ the en-\ncoder to extract enriched features from remote sensing images,\nwhile the decoder is exploited to interpret abstract information\nbefore recovering the detailed information of cloud-free images.\nHowever, these methods are handicapped by their weak feature\nrepresentation capability of CNNs. As a result, additional efforts\nare required to enhance the feature representation capability of\nCNNs to generate high-quality cloud-free images.\nSimilar to the encoder–decoder methods, the GAN-based\nmodels also consist of two parts, i.e., the generator and dis-\ncriminator [31]. Owing to its remarkable capability of modeling\nthe relationship between input and output data, GAN has gained\ntremendous popularity in computer vision. For the cloud re-\nmoval task, conditional GAN (cGAN)[25] employed a simple\nUNet-based structure as the generator while PatchGAN[32]\nas the discriminator. Furthermore, a hybrid loss function us-\ning the structural similarity (SSIM) loss[33] was designed to\nimprove the SSIM of the generated images with the ground\ntruth. Recently, spatial attention GAN (SpAGAN) [27] was\nproposed to remove clouds by integrating local-to-global spatial\nattention to the generator whereas MSDA-CR[29] proposed\na grid network based on cloud-distortion-aware representation\nlearning to model the effects of cloud reﬂection and trans-\nmission. In addition, AMGAN-CR [30] generated attention\nmaps through an attentive recurrent network and employed an\nattentive residual network to remove clouds according to the\nattention maps. These methods have improved the GAN-based\nframeworks by enhancing the encoder or loss function design\nthrough a single-directional mapping, i.e., from cloudy images\nto cloud-free images.\nRecently, the cycle-consistent GAN (CycleGAN) model[34]\nhas been widely applied to transfer image styles. CycleGAN at-\ntempts to learn a bidirectional mapping between domains while\nincorporating cycle-consistency loss and identity loss to effec-\ntively retain the color composition and texture. CloudGAN[12]\nintroduced CycleGAN into cloud removal to learn the mapping\nof feature representations between cloudy images and their\ncorresponding cloud-free images in a cyclic structure. In the\ncloud removal task, it is also necessary to learn global color\ncomposition and texture outside the cloud area before predicting\nthe objects under the cloud in the forward process. The reverse\nstage in the cycle process can promote the learning of these\nglobal representations in the forward process by restoring the\noriginal cloud map. However, it suffers from blurred edges due to\nits straightforward encoding structure and the lack of modeling\nchannel and spatial relationships. On this basis, SAR-to-optical\nimage translation using SSIM and perceptual loss-based Cy-\ncleGAN [26] introduced the least squares loss function[35]\ninto the CycleGAN to improve its training stability in image\ntranslation. Furthermore, multimodal GAN (MMGAN)[28] was\ndeveloped to generate multiple most likely cloud-free outputs\nbefore selecting the best generated cloud-free images through\na perception-based image quality evaluator. Despite their many\nadvantages, these methods suffer from a poor performance in\nreconstructing detailed features of remote sensing images as they\nare straightforward extensions from models originally devised\nfor natural images. Compared with natural scene images, remote\nsensing images exhibit more severe spectral heterogeneity and\nmore complex spatial relationships of ground objects[36], [37].\nTypically, undesired cloud layers have various thicknesses, and\nimages are acquired under different lighting conditions[38].\nAs a result, the performance of those image restoration models\ndeveloped for natural scene images is usually poor if directly\napplied to cloud removal. Furthermore, it is challenging for these\nmodels to handle large-scale cloud removal tasks due to their\nprohibitively expensive computational complexity.\nTo improve the representation capability of CNNs and GAN\nwith long-range contextual information, the newly developed\ntransformer has been introduced into the cloud removal tasks.\nEmpowered by its nonlocal attention mechanism, the trans-\nformer can establish long-range dependencies with impressive\nscalability [39], [40]. For instance, SAR-enhanced cloud re-\nmoval with global-local fusion [15] added Swin transformer\nlayer [41] after each convolutional layer for cross-window\nfeature interaction. CloudTran [42] replaced the CNN-based\nencoder with an axial transformer [43] to estimate the low-\nresolution cloud-free images. However, the transformer is only\nregarded as a feature extractor to exploit global information\nwhile lacking the capability to fully extract enriched local\nfeatures. Compared with the transformer, CNN exploits and\naggregates enriched local features using the local receptive ﬁelds\nin the convolutional layers[3], [10]. One trivial approach to take\nadvantage of both transformer and CNN is to directly construct\na dual-branch encoder to extract global and local information\nby transformer and CNN, respectively[44], [45], [46], [47].\nMore recently, the authors in[48] and [49] proposed to use\nCNN to extract multiscale features while exploring the ability\nof transformer to enhance these multiscale features. In contrast,\nFang et al.[50] further integrated the Swin Transformer layers\nand the convolutional layers by exploiting spatial attention after\neach Swin Transformer layer. However, all methods aforemen-\ntioned failed to explore the potential enhancement of high-level\nsemantic features provided by exploiting the synergy of CNN\nMA et al.: CLOUD-EGAN: RETHINKING CycleGAN FROM A FEATURE ENHANCEMENT PERSPECTIVE FOR CLOUD REMOV AL 5001\nFig. 1. (a) Overview of Cloud-EGAN framework.GX2Y and GY 2X are the two generators andDX and DY are the two discriminators.X, ˜X, Y , ˜Y represent\nthe authentic cloudy images, the generated cloudy images, the authentic cloud-free images, and the generated cloud-free images, respectively. (b)Discriminator\nfollows the PatchGAN structure. (c) Generator is based on the UNet framework with different output sizes from each SE block. Note that the two generators,\nGX2Y and GY 2X, are designed with the same structure. Similarly, the two discriminators,DX and DY , have the same structure.\nand transformer. Thus, it is of great practical interest to investi-\ngate how to ﬁll this gap by combining CNN and transformer in\nthe cloud removal task.\nMotivated by the aforementioned challenges, this work in-\ntroduces a CycleGAN-based model for thin and thick cloud\nremoval from two different enhancement perspectives. First,\nthe backbone is enhanced by a saliency enhancement (SE)\nmodule to extract hierarchical discriminant features with\nmore saliency. Furthermore, in sharp contrast to the ex-\nisting models that utilize CNN to enhance high-level fea-\ntures [6], [9], [23], this work proposes to explore enriched\nhigh-level features by jointly exploiting CNN and transformer.\nThe main contributions of this work can be summarized as\nfollows:\n1) An SE module is utilized to generate enhanced hierarchi-\ncal feature maps derived from each convolutional block\nby recalibrating the attention weights of feature channels.\nAs a result, cloud-covered components and blurred edges\nare reduced;\n2) A high-level feature enhancement (HFE) module is de-\nvised between the encoder and the decoder to effectively\nexplore and aggregate high-level features. Speciﬁcally,\nHFE is composed of a CNN-based HFE (CHFE) module\nand a transformer-based HFE (THFE) module. CHFE is\ndesigned to exploit high-level local features to harvest suf-\nﬁcient detailed information while THFE long-range con-\ntextual information. CHFE and THFE are integrated under\nthe cloud-enhancement GAN (Cloud-EGAN) framework\nto retain the global features of the restored cloud-clear\nimages;\n3) Extensive experimental results on the RICE and WHUS2-\nCR datasets verify the superiority of Cloud-EGAN in seg-\nregating clouds and preserving high-quality land surface\ninformation.\nThe rest of this article is organized as follows. SectionII elabo-\nrates on the proposed model while extensive experimental results\nare presented and analyzed in SectionIII. Finally, SectionIV\nconcludes this article.\nII. METHODOLOGY\nIn this work, a CycleGAN-based architecture with SE and\nHFE modules in the generator is proposed to extract and ag-\ngregate enhanced local and global features from remote sens-\ning images. In the following, an overview of the proposed\nCloud-EGAN is presented before each of its key components\nis elaborated. Finally, hybrid loss functions employed in the\nproposed model are devised.\nA. Framework\nAs depicted in Fig.1(a), the proposed Cloud-EGAN is de-\nveloped based on CycleGAN that consists of two generators\nGX2Y and GY 2X and two discriminatorsDX and DY .M o r e\nspeciﬁcally, for a supervised cloud removal task, the authentic\ncloudy image X serves as the input to the generatorGX2Y\nto reconstruct the predicted cloud-free image ˜Y that is then\n5002 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 2. Architecture of the SE module in (a) encoding and (b) decoding.\ndiscriminated by DY with the authentic cloud-free imageY.\nMeanwhile, according to the cyclic consistency principle, the\ngenerator GY 2X is employed to generate the cloudy image˜X\nfrom ˜Y. The same operation is performed on the inputY in the\nCloud-EGAN.\nAs illustrated in Fig.1(b), the discriminators DX and DY\nadopt a PatchGAN structure with stacked hierarchical convolu-\ntional blocks to determine the authenticity of˜Y. Furthermore,\nthe generator is developed based on an UNet architecture[51] by\ncapitalizing on symmetrical concatenations between an encoder\nand a decoder, as shown in Fig.1(c). Speciﬁcally, the generator\ncombines the SE and HFE modules while SE exploits hierar-\nchical features by reassigning attention weights to feature maps\nat each level. The resulting high-level feature maps are then fed\ninto the HFE module to further enhance feature representation\nthrough the combination of CNN and transformer. After that,\na convolutional prediction head is utilized at the end of the\ngenerator to recover cloud-clear images. More details about\nthe SE and HFE modules will be elaborated in the following\nsections.\nB. Saliency Enhancement\nFollowing the classical channel attention mechanism[52],\nthe SE module adaptively exploits more salient features from\nremote sensing images at multiple feature levels by assigning\nlearnable attention weights to feature channels. As a result, SE\ncan enhance information restoration from heavily cloudy regions\nand generate high-quality cloud-free features.\nFig. 2(a) illustrates the encoding process in which uk ∈\nRDk×Hk×Wk denotes thekth level feature map generated from\nthe ﬁrst convolutional block (SE_Conv), whereDk is the chan-\nnel dimension, Hk = H/2k and Wk = W/2k. Furthermore, a\nglobal average pooling (GAP) layer as a channel descriptor is\napplied to exploit enriched features and produce outputzk\nzk = G(uk)= 1\nHk ×Wk\nHk∑\ni=1\nWk∑\nj=1\nuk(i,j) (1)\nwhere G stands for the GAP function. After that, two1 ×1\nSE_Conv blocks are utilized to compute the attention weights\nthrough convolution operations with outputsk ∈ RDk×1×1 be-\ning given by\nsk = δ(W2 (W1(zk))) (2)\nwhere W1 and W2 are parameters of the two convolutional\nblocks andδ(··· ) is the sigmoid function. Finally, the SE output\ndenoted by ˜xk ∈ RDk×Hk×Wk is derived by multiplying sk\nwith uk\n˜xk = sk ⊙ uk (3)\nwhere ⊙ represents the point multiplication operation.\nThe decoding process depicted in Fig. 2(b) is similar to\nFig. 2(a) with the convolutional block (SE_Conv) being replaced\nby an upsampling SE_Conv.\nC. High-Level Feature Enhancement\nThe HFE module is designed to learn enriched high-level\nlocal and nonlocal features by combining CHFE and THFE, as\nshown in Fig.3. As a result, it is beneﬁcial to further characterize\ncloud-free representations and propagate contextual information\nacross the feature maps from a global perspective, which can\nmaintain the spatial structure of the restored features identical\nto the ground truth.\nMore speciﬁcally, a residual learning module [53] and a\ndilated convolutional module[54] are used in CHFE to process\nhigh-level features in parallel. In particular, high-level features\nFh ∈ RD×H\n16 ×W\n16 are fed into the residual learning module con-\ntaining three successive residual blocks named HFE_ResConv\nto extract critical ground information while reducing the fea-\nture discrepancy between cloud-covered and cloud-free images.\nMeanwhile, Fh is passed through a convolutional block with\nresidual structure named HFE_Conv, and three dilated convolu-\ntional blocks named HFE_DilatedConv with different dilation\nrates to exploit multiscale contextual information while alleviat-\ning cloud-covered features. After that, the concatenated outputs\nare further enhanced through an HFE_Conv block to restore the\noriginal feature size. Finally, the outputs of the residual learning\nmodule and dilated convolutional module are added together to\nform reﬁned feature mapsF ∈ RD×H\n16 ×W\n16 .\nFollowing the approach of the classical Swin transformer[41],\nTHFE splitsF into nonoverlapping patches in the patch partition\nmodule before projecting the patches to an arbitrary dimension\nˆDusing a linear embedding layer. The patches are then fed into a\nsuccessive Swin transformer block and a patch merging layer to\ngenerate higher level feature representations. More speciﬁcally,\nas depicted in Fig.3(b), each successive Swin transformer block\nconsists of the residual architecture, four layer-normalization\n(LN) layers, a window-based multihead self-attention (WMSA)\nmodule, a shifted WMSA (SWMSA) module, and two multi-\nlayer perceptron (MLP) layers with GELU function.\nThe operation of successive Swin transformer blocks is shown\nin Fig.3(b). For each head of the WMSA and SWMSA, the input\nfeatures FS are fed into the Swin transformer block to calculate\nthe multihead self-attention (MSA) as follows:\nQS = FSWQ,KS = FSWK,VS = FSWV (4)\nand\nAtt(FS)= φ\n(QSKS\nT\n√\nd\n+ BS\n)\nVS (5)\nwhere QS, KS, and VS denote the projected query, key, and\nvalue features, respectively whileWQ, WK, and WV the cor-\nresponding parameter metrics. Furthermore, BS is the learn-\nable relative position embedding term in the Swin transformer\nMA et al.: CLOUD-EGAN: RETHINKING CycleGAN FROM A FEATURE ENHANCEMENT PERSPECTIVE FOR CLOUD REMOV AL 5003\nFig. 3. Illustrations of (a) the HFE module, (b) successive Swin Transformer block, and (c) MSA, LN and MLP represent the LN layer and MLP layer,\nrespectively. WMSA and SWMSA are MSA modules with common and shifted windowing conﬁgurations, respectively. (a) HFE. (b) Swin transformer.\n(c) Multihead self-attention.\nwhereas Att(FS) represents the output of self-attention for each\nhead. In addition,φ(·) is the softmax function andd = ˆD/4 is\nthe channel dimension for each head.\nAfter that, the features of each2 ×2 neighboring patches\ngenerated by the Swin transformer block are concatenated by\nthe patch merging layer. We denote byH/32, W/32, and 4 ˆD\nthe height, width, and channel after the patch merging layer,\nrespectively. Finally, after two Swin transformer blocks and the\nreshape operation to maintain the same size as the inputFh,t h e\noutput of the HFE module˜Fh ∈ RD×H\n16 ×W\n16 can be obtained.\nD. Loss Functions\nIn this work, a novel hybrid loss function comprising the\nadversarial loss Ladv, the cycle consistency lossLcyc, the per-\nceptual loss Lper, and the identity loss Lid is introduced to\nguide the training of our proposed model. It is notable that\nLadv is utilized to train both generators and discriminators, while\nLcyc,Lper, andLid are employed for training the generators. The\nexpression of the hybrid loss functionL can be formulated as\nfollows:\nL = Ladv + λcycLcyc + λperLper + λidLid (6)\nwhere λcyc,λper, andλid are adjustable weights of the three loss\ncomponents. More details about each loss function are provided\nin the following sections.\n1) Adversarial Loss: The adversarial loss aims to make\nthe reconstructed cloud-free images close to the correspond-\ning ground truth. Adopting a structure similar to the classical\nCycleGAN, we deﬁne the adversarial loss as\nLX2Y\nadv = Ey∼Pdata(y)[logDY (y)]\n+ Ex∼Pdata(x)[log(1 −DY (GX2Y (x))] (7)\n5004 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nLY 2X\nadv = Ex∼Pdata(x)[logDX(x)]\n+ Ey∼Pdata(y)[log(1 −DX(GY 2X(y))] (8)\nwhere x and y are the input cloudy and cloud-free image sam-\nples, respectively. Furthermore,Pdata(x) and Pdata(y) represent\nthe distributions of cloudy and cloud-free images. The total\nadversarial objectiveLadv is comprised ofLX2Y\nadv and LY 2X\nadv used\nto train forward process and reverse process, respectively.\n2) Cycle Consistency Loss:The cycle consistency loss mea-\nsures the pixel-wise difference between the generated images\nand their corresponding ground truth. It is adopted to reduce\nblurring regions and keep the reconstructed images closer to\nthe ground truth. The cycle consistency loss takes the following\nform:\nLcyc = Ex∼Pdata(x) [||GY 2X(GX2Y (x)) −x||1]\n+ Ey∼Pdata(y) [||GX2Y (GY 2X(y)) −y||1] (9)\nwhere GX2Y and GY 2X are the two generators in the Cloud-\nEGAN and ||·|| 1 stands for the L1-norm of the enclosed\nquantity.\n3) Perceptual Loss: Based on the computation of losses in\npixel colors and edges, the perceptual loss[55] is introduced to\nmeasure the consistency between convolutional outputs of the\nground truth and the restored images obtained by a pretrained\nnetwork, e.g., VGG19 pretrained on the ImageNet[56]. More-\nover, the capability of extracting perceptual semantic features\nvia the convolutional layers can be evaluated. Mathematically,\nthe expression of the perceptual loss can be deﬁned as\nLper =\n∑\nk\n1\nCkHkWk\n[\nEx∼Pdata(x)||φk(x′) −φk(x)||1\n+Ey∼Pdata(y)||φk(y′) −φk(y)||1\n]\n(10)\nwhere φk denotes the feature map extracted from thekth layer\nin the pretrained VGG19 network, andCk, Hk, Wk denote the\nnumber of channels, height, and width of thekth feature map,\nrespectively. Moreover,x′and x represent the pixel intensities in\nthe original cloudy images and the generated cloudy images by\nCloud-EGAN, respectively. Meanwhile,y′ and y stand for the\npixel intensities in the ground truth cloud-free images and the\ngenerated cloud-free images by Cloud-EGAN, respectively.\n4) Identity Loss: The identity loss aims to retain the color\nconsistency between the input and the output. For the cloud\nremoval task, the clouds are expected to be eliminated in the gen-\nerated cloud-free images and the cloud-free regions are expected\nto remain unchanged in texture details and color compositions.\nThe proposed model can avoid color distortion in cloud-free\nregions by applying identity loss. It can be formulated as follows:\nLid = Ex∼Pdata(x)[||GX2Y (x)) −x||1]\n+ Ey∼Pdata(y)[||GY 2X(y) −y||1]. (11)\nIII. EXPERIMENTAL RESULTS\nIn this section, experimental datasets will be ﬁrst described.\nAfter that, the parameter settings and evaluation metrics are\nintroduced before the comparisons with other DL-based models\nare reported and analyzed.\nA. Datasets\nIn this section, the proposed model is evaluated on the RICE\ndataset [57] and the WHUS2-CR dataset[58]. Speciﬁcally, the\nRICE dataset comprises two subdatasets named RICE1 and\nRICE2. In particular, the RICE1 contains 500 pairs of cloud-\ncovered and cloud-free images from Google Earth, with the\nground resolution being 5 m/pixel. Most of the samples in RICE1\nare thin clouds where the ground objects are mostly identiﬁ-\nable. In sharp contrast, the RICE2 dataset includes Landsat-8\nimages of 736 groups of the ground resolution 30 m/pixel. The\nimages in this dataset contain abundant thick clouds, where\nthe ground objects are hardly identiﬁable. Taking into account\nthe large discrepancy in terms of cloud thickness and image\nresolution, we perform our evaluation on these two subdatasets\nseparately. Furthermore, in sharp contrast to MSDA-CR[29]\nand CR-MSS [58] that utilize multispectral data as input, we\nmainly focus on visible (RGB) bands in our evaluation. This is\nbecause RGB images are more commonly available[11], [12],\n[28], [59]. However, we also perform supplement experiments\nto demonstrate that the proposed model can work well with\nmultispectral data by exploiting both RGB and near-infrared\n(NIR) data.\nThe images in the RICE dataset are of size512 ×512 pixels\neach. Moreover, the WHUS2-CR dataset involves 848 pairs of\nSentinel-2 image patches of size256 ×256 pixels. The acqui-\nsition time lag of the cloud-covered images and their corre-\nsponding cloud-free images is less than 10 days. Furthermore,\n400 and 100 image pairs were chosen for training and testing\nin the RICE1 dataset, respectively. In addition, 589 and 147\npairs were adopted as the training and testing set in the RICE2\ndataset, respectively. For the WHUS2-CR dataset, 679 pairs\nwere obtained as training data, and the remaining 169 pairs were\nreserved for testing. Some typical samples in the RICE1, RICE2,\nand WHUS2-CR datasets are displayed in Fig.4.\nB. Implementation Details\nIn the generator of the Cloud-EGAN, four convolutional\nlayers with a kernel size of4 ×4 and stride of 2 are utilized\nin the encoder and decoder, with{32,64,128,256} channels\nfor the former and{256,128,64,32}for the latter. After that, a\nconvolutional layer with a kernel size of4 ×4, a stride of 1 and\n3 channels is utilized to restore the cloud-free images with the\nsame size as the input. In the discriminator, four convolutional\nlayers with a kernel size of4 ×4 and a stride of 2 are exploited\nwith {64,128,256,512}channels. Meanwhile, a convolutional\nlayer with a kernel size of4 ×4, a stride of 1, and a channel\nnumber of 1 is used to discriminate whether the generated cloud-\nfree images are authentic or not. Notably, these convolutional\nlayers are followed by the instance normalization[34] and the\nLeaky ReLU function[60] parameterized by 0.2, except for the\nclassiﬁer in the decoder and the discriminator.\nIn Cloud-EGAN, the learning rateαwas initially set to 0.0001\nbefore being decayed by half after every 20 epochs. Furthermore,\nMA et al.: CLOUD-EGAN: RETHINKING CycleGAN FROM A FEATURE ENHANCEMENT PERSPECTIVE FOR CLOUD REMOV AL 5005\nFig. 4. Typical image samples in the (a) RICE1, (b) RICE2 and (c) WHUS2-\nCR datasets. The ﬁrst and the second rows are cloud-covered images and cloud-\nfree images, respectively.\nThe batch size was set to 4. In addition, the Adam optimizer[61]\nwith default momentum parameters, i.e.,β1 = 0.9 and β2 =\n0.999 was adopted. Finally,λcyc, λper, andλid in the loss function\nwere set to 10, 1, and 9, respectively. All experiments were\nimplemented on a single NVIDIA GeForce RTX 3090 GPU\nwith 24-GB RAM.\nThe proposed Cloud-EGAN is compared against six state-of-\nthe-art DL-based cloud removal methods, namely cGAN[25],\nCloudGAN [12], SpAGAN[27],C V A E[24],M S D A - C R[29],\nand MMGAN[28].\nC. Metrics\nTwo widely used metrics, SSIM[62] and peak signal-to-noise\nratio (PSNR) [63], were utilized for quantitative evaluation.\nSpeciﬁcally, SSIM is expressed as\nSSIM = 2μxμy + C1\nμx2 + μy2 + C1\n2σxy + C2\nσx2 + σy2 + C2\n(12)\nwhere μx, σx, and σxy represent the average, variance, and\ncovariance, respectively. C1 and C2 are constants for stabi-\nlizing the division with a weak denominator. A larger SSIM\nvalue stands for the greater similarity between the generated\ncloud-free images and ground truth, which indicates a higher\nquality of the generated cloud-free images. Moreover, PSNR is\ndeﬁned as\nPSNR =2 0l o g10\nMAXI√\nMSE\n(13)\nwhere\nMSE = 1\nM ×N\nM∑\ni=1\nN∑\nj=1\n∥I(i,j) −J(i,j)∥2 (14)\nand MAXI represents the possible maximum pixel value in the\ngenerated cloud-free imagesI. Moreover, the generated cloud-\nfree imageI and the corresponding ground-truthJ are of size\nM ×N ×3, and (i,j) represents the pixel index inI and J.\nA larger PSNR value represents less image distortion in the\nreconstructed cloud-free images.\nFinally, we evaluated the computational complexity of the\nproposed method using the following metrics, namely the ﬂoat-\ning point operation count (FLOPs), the number of parame-\nters (M), and the frames per second (FPS). More speciﬁcally,\nFLOP is used to evaluate the model complexity whereasM\nmeasures the memory requirement. In addition, FPS is used\nto evaluate the execution speed. For computationally efﬁcient\nmodels, their FLOP andM should be small while FPS being\nlarge.\nD. Performance Comparison\nAs illustrated in Fig. 5, the results obtained by Cloud-\nEGAN achieved lower spectral distortion and more signiﬁcant\nSSIM with the ground truth in the thin-cloud-covered scenar-\nios. Moreover, the results obtained by cGAN and CloudGAN\nsuffered from much loss of texture details with some blur-\nring areas while failing to thoroughly restore the land sur-\nface information in the generated cloud-free images. Com-\npared with cGAN and CloudGAN, SpAGAN and MSDA-\nCR showed better results with more explicit texture details.\nHowever, some color distortions have been observed. As a\nresult, the color information of the ground surface could not\nbe fully restored. Finally, despite the fact that the results of\nCV AE and MMGAN achieved color compositions similar to the\nground truth, some slightly-blurred edges were noticed in several\npatches.\nIn contrast, Cloud-EGAN performed best among all methods\nunder evaluation in the thick-cloud-covered scenarios of the\nRICE2 dataset, as shown in Fig.6. It generated images with\nbetter texture structures and color compositions. In comparison,\ncGAN and CloudGAN could not remove clouds thoroughly,\nwhich generated some edge-blurring and color-distortion ar-\neas in noncloudy regions. Moreover, the results obtained by\nSpAGAN exhibited severe loss of details since the structure\ninformation of ground scenarios could not be completely re-\ncovered. Furthermore, it was observed that the results of CV AE,\nMSDA-CR, and MMGAN showed more spatial features similar\nto the ground truth, though some slight color distortions were\nobserved.\nFor the WHUS2-CR dataset, as depicted in Fig. 7,t h e\ncolor compositions and the texture details of the cloud-free\nimages generated by Cloud-EGAN were more similar to the\nground truth. In contrast, cGAN and CloudGAN showed the\nworst performance due to their limited feature extraction ca-\npability. Compared with cGAN and CloudGAN, SpAGAN\nand MSDA-CR obtained better results with more evident\nbackgrounds and details, but some color distortion scenes\nremained in cloudless regions. Finally, the color tones in\nthe results of CV AE and MMGAN were visually close to\nthe ground truth. However, some contextual information was\nlost, and clouds were not segregated thoroughly in these\nmodels.\nQuantitative results on the RICE1, RICE2, and WHUS2-CR\ndatasets are shown in TableI. Cloud-EGAN achieved higher\nPSNR and SSIM values than other DL-based methods due to the\n5006 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 5. Visual comparison of cloud removal results obtained by different models in thin-cloud-covered scenarios of the RICE1 dataset. (a) Cloudy images,\n(b) Ground Truth, (c) cGAN[25], (d) CloudGAN[12], (e) SpAGAN[27], (f) CV AE[24],( g )M S D A - C R[29], (h) MMGAN[28], and (i) Proposed Cloud-EGAN.\nFig. 6. Visual comparison of cloud removal results obtained by different models in thick-cloud-covered scenarios of the RICE2 dataset. (a) Cloudy images,\n(b) Ground Truth, (c) cGAN[25], (d) CloudGAN[12], (e) SpAGAN[27], (f) CV AE[24],( g )M S D A - C R[29], (h) MMGAN[28], (i) Proposed Cloud-EGAN.\nMA et al.: CLOUD-EGAN: RETHINKING CycleGAN FROM A FEATURE ENHANCEMENT PERSPECTIVE FOR CLOUD REMOV AL 5007\nFig. 7. Visual comparison of cloud removal results obtained by different models in the WHUS2-CR dataset. (a) Cloudy images, (b) Ground Truth, (c) cGAN[25],\n(d) CloudGAN[12], (e) SpAGAN[27], (f) CV AE[24],( g )M S D A - C R[29], (h) MMGAN[28], (i) Proposed Cloud-EGAN.\nTABLE I\nQUANTITATIVERESULTS FORPSNR AND SSIM VALUES\nexploration and aggregation of enriched local and global features\nin hierarchical and deep contextualized space. In particular,\nthe results labeled as Cloud-EGAN* were generated with the\nproposed Cloud-EGAN usingfour input bands, i.e., RGB and\nNIR. It is evident from TableI that the proposed Cloud-EGAN\ncan also work well in multispectral scene. Furthermore, it is\nshown that the NIR band could indeed further improve the\nperformance of cloud removal. Therefore, the cloud-covered\nand cloud-free regions could be more accurately characterized,\nwhich aided in maintaining the recovered images close to the\nground truth.\nE. Ablation Study\n1) Cycle-Consistence: In order to evaluate the necessity of\ncycle-consistence that requires two generators and discrimi-\nnators, we conducted an ablation experiment as shown in the\nsecond line from the bottom in TableII, which is the result\nof the conventional GAN framework with only one generator-\ndiscriminator pair. The experiment results showed that the\ncycle-consistent mechanism enabled the generator to learn better\nglobal representations to promote the prediction of the ground\nobjects of cloud-free areas.\n5008 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nTABLE II\nQUANTITATIVERESULTS WITH DIFFERENT MODULES FOR THEGENERATOR\nTABLE III\nQUANTITATIVERESULTS WITH DIFFERENT LOSS FUNCTIONS\nTABLE IV\nCOMPARISON ONCOMPUTATIONALCOMPLEXITY MEASURED BY A256 ×256\nINPUT ON ASINGLE NVIDIA GEFORCE RTX 3090 GPU\n2) Model Components:We compared the quantitative results\nby eliminating various modules in the proposed Cloud-EGAN\nframework, as shown in TableII. Note that the elimination of the\nSE module by only utilizing the convolutional layers followed\nby instance normalization and leaky ReLU function still follows\nthe same UNet-based architecture as Fig.1(c). Inspection of\nTable II reveals that integrating all feature enhancement mod-\nules resulted in the best performance in terms of both PSNR\nand SSIM. Accordingly, this conﬁrmed the beneﬁts of these\nmodules in aggregating enriched contextualized features and\nrestoring ground surface information sufﬁciently. Notably, SE\ncan be further developed by other channel-based or spatial-based\nattention modules. We provided a unique perspective on using\nsqueeze-and-excitation module [52] to enhance convolutional\nnetworks comprehensively. Considering the versatility and com-\nplexity, we ﬁnally chose this classical channel attention module\nas the feature enhancement structure in this work. Moreover,\nFig. 8. Comparison of the feature maps via introducing SE and HFE in the\nproposed Cloud-EGAN. Note that the brighter regions are paid higher attention\nto during the training process, where more contextualized features will be\nexploited. (a) Original image. (b) Feature map of the output through the ﬁrst\nconvolutional block in the ﬁrst SE module. (c) Feature map of the output through\nthe ﬁrst SE module. (d) Feature map of the input of HFE. (e) Feature map of the\noutput through HFE.\nthe quantitative results without the SE modules were better than\nthose obtained without HFE. In other words, HFE plays a critical\nrole in cloud removal performance, further demonstrating the\nnecessity of enhancing high-level features for remote sensing\nimages. More speciﬁcally, THFE enables the model to learn\nmore global representations, facilitating the model to better\npredict the objects under the cloudy area. As shown in TableII,\nthe results generated with THFE were better than those without\nTHFE. Similar observations regarding CHFE can be made in\nTable II, which suggests models with CHFE can learn more\ndetailed representations.\n3) Effectiveness of Adding Perceptual Loss:To evaluate the\neffectiveness of the perceptual loss, we compared the proposed\nhybrid loss function with the loss function in the classical\nCycleGAN, as shown in TableIII. The adjustable weightsλcyc\nand λid of the loss function in the classical CycleGAN were\nset to 10 and 9, respectively. It is observed that there was a\nMA et al.: CLOUD-EGAN: RETHINKING CycleGAN FROM A FEATURE ENHANCEMENT PERSPECTIVE FOR CLOUD REMOV AL 5009\nFig. 9. Comparison of the training convergence of the classical CycleGAN and\nCloud-EGAN on the (a) RICE1 dataset, (b) RICE2 dataset, and (c) WHUS2-CR\ndataset.\nnon-negligible improvement in terms of PSNR and SSIM after\nincorporating perceptual loss.\nF . Model Complexity Analysis\nTable IV shows the complexity evaluation results of all meth-\nods conducted in our work. SPAGAN achieved the best perfor-\nmance on these metrics since it only used simple CNN-based\nmodules while the generation performance is poor. Compared\nto most other methods, the proposed Cloud-EGAN achieves\nsigniﬁcantly improved performance with low computational\ncomplexity by exploiting the convolution operations and the\nhigh-efﬁcient WMSA module. Meanwhile, we added more mod-\nules including the CHFE module and a THFE module to enhance\nthe high-level features. Therefore, the proposed Cloud-EGAN\nachieved a better cloud removal performance at the cost of a\nlarger number of parameters and a lower inference speed.\nG. Discussion\nThe experimental results have demonstrated that Cloud-\nEGAN performs better than existing DL-based models in the\ncloud removal task. This superior performance can be attributed\nto the cyclic structure and the integration of the SE and HFE\nmodules. More speciﬁcally, Cloud-EGAN learns the mapping\nof feature representations between cloudy images and the cor-\nresponding cloud-free images in a cyclic-consistent way, which\nis conducive to strengthening the model capability of feature\nrepresentation. Moreover, the combination of SE and HFE can\neffectively extract and aggregate contextual information, which\nis conducive to generating high-quality cloud-free images simi-\nlar to the ground truth. The effectiveness of introducing SE and\nHFE can be validated from the feature maps shown in Fig.8.\nNotably, the informative feature details are further enhanced\nthrough SE and HFE. As a result, cloud-removed scenes with\nenriched ground information can be preserved in Cloud-EGAN.\nIn addition, we compared the training loss convergence us-\ning Cloud-EGAN and the classical CycleGAN on the RICE1,\nRICE2, and WHUS2-CR datasets. It is observed in Fig.9(a)–(c)\nthat Cloud-EGAN obtained better convergence performance\nthan CycleGAN due to the novel framework and the incorpora-\ntion of the perceptual loss.\nIV . CONCLUSION\nIn this work, a novel CycleGAN-based architecture, named\nCloud-EGAN, has been proposed to perform supervised cloud\nremoval tasks, which can effectively remove thin and thick\nclouds while preserving spectral and spatial consistency with\nthe land surface. Compared with existing DL-based models\ndeveloped for removing clouds, the proposed Cloud-EGAN\nutilizes a cyclic architecture while integrating the SE and HFE\nmodules to enhance the ability to identify remote sensing images\nwith complex ground objects. While the cyclic architecture is\ndesigned to recalibrate the weights of hierarchical channels, the\nintegration of the SE and HFE modules is employed to further\naggregate local and global high-level contextualized features. As\na result, the proposed Cloud-EGAN can more effectively exploit\nmultilevel enriched features with more saliency to highlight\n5010 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nground information while suppressing cloud components and\nblurred edges through the integration of CNN and transformer.\nExtensive simulation results on the RICE and WHUS2-CR\ndatasets have conﬁrmed the superior cloud removal performance\nachieved by Cloud-EGAN as compared to existing DL-based\nmethods for removing thin and thick clouds.\nThere are several extensions of this study that can be fur-\nther explored. First, it is of great practical interest to further\ninvestigate how to construct a more computationally efﬁcient\nmodel for various cloud-covered scenarios. Furthermore, it is\ninteresting to consider applying the proposed Cloud-EGAN\nto large-scale remote sensing datasets, such as Sentinel-2 and\nLandsat-9 images in an unsupervised or semisupervised manner.\nFinally, end-to-end designs of cloud removal and other down-\nstream tasks, such as semantic segmentation, will be explored\nin future research.\nREFERENCES\n[1] W. Ma et al., “Feature split–merge–enhancement network for remote\nsensing object detection,” IEEE Trans. Geosci. Remote Sens., vol. 60,\nJan. 2022, Art. no. 5616217.\n[2] G. Li, Z. Liu, Z. Bai, W. Lin, and H. Ling, “Lightweight salient object\ndetection in optical remote sensing images via feature correlation,”IEEE\nTrans. Geosci. Remote Sens., vol. 60, Jan. 2022, Art. no. 5617712.\n[3] G. Shi, J. Zhang, J. Liu, C. Zhang, C. Zhou, and S. Yang, “Global context-\naugmented objection detection in VHR optical remote sensing images,”\nIEEE Trans. Geosci. Remote Sens., vol. 59, no. 12, pp. 10604–10617,\nDec. 2021.\n[4] W. Wang, Y . Chen, and P. Ghamisi, “Transferring CNN with adaptive\nlearning for remote sensing scene classiﬁcation,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, Jul. 2022, Art. no. 5533918.\n[5] Y . Hu, X. Huang, X. Luo, J. Han, X. Cao, and J. Zhang, “Variational self-\ndistillation for remote sensing scene classiﬁcation,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, Jul. 2022, Art. no. 5627313.\n[6] X. Tang, Q. Ma, X. Zhang, F. Liu, J. Ma, and L. Jiao, “Attention consistent\nnetwork for remote sensing scene classiﬁcation,”IEEE J. Sel. Topics Appl.\nEarth Observ. Remote Sens., vol. 14, pp. 2030–2045, Jan. 2021.\n[7] H. Li et al., “Global and local contrastive self-supervised learning for se-\nmantic segmentation of HR remote sensing images,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, Jan. 2022, Art. no. 5618014.\n[8] L. Ding et al., “Looking outside the window: Wide-context transformer\nfor the semantic segmentation of high-resolution remote sensing images,”\nIEEE Trans. Geosci. Remote Sens., vol. 60, Apr. 2022, Art. no. 6512405.\n[9] R. Liu, L. Mi, and Z. Chen, “AFNet: Adaptive fusion network for remote\nsensing image semantic segmentation,”IEEE Trans. Geosci. Remote Sens.,\nvol. 59, no. 9, pp. 7871–7886, Sep. 2020.\n[10] Z. Sun, W. Zhou, C. Ding, and M. Xia, “Multi-resolution transformer\nnetwork for building and road segmentation of remote sensing image,”\nISPRS Int. J. Geo- Inf., vol. 11, no. 3, 2022, Art. no. 165.\n[11] X. Wen, Z. Pan, Y . Hu, and J. Liu, “An effective network integrating\nresidual learning and channel attention mechanism for thin cloud removal,”\nIEEE Geosci. Remote Sens. Lett., vol. 19, Mar. 2022, Art. no. 6507605.\n[12] P. Singh and N. Komodakis, “Cloud-GAN: Cloud removal for sentinel-\n2 imagery using a cyclic consistent generative adversarial networks,”\nin Proc. IEEE IGARSS Int. Geosci. Remote Sens. Symp. , 2018,\npp. 1772–1775.\n[13] P. Ebel, Y . Xu, M. Schmitt, and X. X. Zhu, “SEN12MS-CR-TS: A\nremote-sensing data set for multimodal multitemporal cloud removal,”\nIEEE Trans. Geosci. Remote Sens., vol. 60, Jan. 2022, Art. no. 5222414.\n[14] J. Zhou, X. Luo, W. Rong, and H. Xu, “Cloud removal for optical remote\nsensing imagery using distortion coding network combined with com-\npound loss functions,”Remote Sens., vol. 14, no. 14, 2022, Art. no. 3452.\n[15] F. Xu et al., “GLF-CR: SAR-enhanced cloud removal with global–local\nfusion,” ISPRS J. Photogrammetry Remote Sens., vol. 192, pp. 268–278,\n2022.\n[16] T.-Y . Ji, D. Chu, X.-L. Zhao, and D. Hong, “A uniﬁed framework of cloud\ndetection and removal based on low-rank and group sparse regularizations\nfor multitemporal multispectral images,” IEEE Trans. Geosci. Remote\nSens., vol. 60, Feb. 2022, Art. no. 5303015.\n[17] J. Li et al., “Thin cloud removal fusing full spectral and spatial features\nfor Sentinel-2 imagery,”IEEE J. Sel. Topics Appl. Earth Observ. Remote\nSens., vol. 15, pp. 8759–8775, Oct. 2022.\n[18] Q. Xiong, G. Li, X. Yao, and X. Zhang, “SAR-to-optical image translation\nand cloud removal based on conditional generative adversarial networks:\nLiterature survey, taxonomy, evaluation indicators, limits and future direc-\ntions,” Remote Sens., vol. 15, no. 4, 2023, Art. no. 1137.\n[19] M. Xu, X. Jia, M. Pickering, and A. J. Plaza, “Cloud removal based on\nsparse representation via multitemporal dictionary learning,”IEEE Trans.\nGeosci. Remote Sens., vol. 54, no. 5, pp. 2998–3006, May 2016.\n[20] H. Shen, H. Li, Y . Qian, L. Zhang, and Q. Yuan, “An effective thin\ncloud removal procedure for visible remote sensing images,”ISPRS J.\nPhotogrammetry Remote Sens., vol. 96, pp. 224–235, 2014.\n[21] M. Xu, M. Pickering, A. J. Plaza, and X. Jia, “Thin cloud removal based on\nsignal transmission principles and spectral mixture analysis,”IEEE Trans.\nGeosci. Remote Sens., vol. 54, no. 3, pp. 1659–1669, Mar. 2016.\n[22] L. Ma, Y . Liu, X. Zhang, Y . Ye, G. Yin, and B. A. Johnson, “Deep learning\nin remote sensing applications: A meta-analysis and review,”ISPRS J.\nPhotogrammetry Remote Sens., vol. 152, pp. 166–177, 2019.\n[23] Z. Shao, Y . Pan, C. Diao, and J. Cai, “Cloud detection in remote sensing\nimages based on multiscale features-convolutional neural network,”IEEE\nTrans. Geosci. Remote Sens., vol. 57, no. 6, pp. 4062–4076, Jun. 2019.\n[24] H. Ding, Y . Zi, and F. Xie, “Uncertainty-based thin cloud removal network\nvia conditional variational autoencoders,” inProc. Asian Conf. Comput.\nVis., 2022, pp. 469–485.\n[25] X. Wang, G. Xu, Y . Wang, D. Lin, P. Li, and X. Lin, “Thin and thick cloud\nremoval on remote sensing image by conditional generative adversarial\nnetwork,” inProc. IEEE IGARSS Int. Geosci. Remote Sens. Symp., 2019,\npp. 1426–1429.\n[26] J. Hwang, C. Yu, and Y . Shin, “SAR-to-optical image translation using\nSSIM and perceptual loss based cycle-consistent GAN,” inProc. IEEE\nInt. Conf. Inf. Commun. Technol. Convergence, 2020, pp. 191–194.\n[27] H. Pan, “Cloud removal for remote sensing imagery via spatial attention\ngenerative adversarial network,” 2020,arXiv:2009.13015.\n[28] Y . Zhao, S. Shen, J. Hu, Y . Li, and J. Pan, “Cloud removal using multimodal\nGAN with adversarial consistency loss,”IEEE Geosci. Remote Sens. Lett.,\nvol. 19, Jul. 2021, Art. no. 8015605.\n[29] W. Yu, X. Zhang, and M.-O. Pun, “Cloud removal in optical remote sens-\ning imagery using multiscale distortion-aware networks,”IEEE Geosci.\nRemote Sens. Lett., vol. 19, Jan. 2022, Art. no. 5512605.\n[30] M. Xu, F. Deng, S. Jia, X. Jia, and A. J. Plaza, “Attention mechanism-based\ngenerative adversarial networks for cloud removal in Landsat images,”\nRemote Sens. Environ., vol. 271, 2022, Art. no. 112902.\n[31] I. Goodfellow et al., “Generative adversarial networks,”Commun. ACM,\nvol. 63, no. 11, pp. 139–144, 2020.\n[32] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\nwith conditional adversarial networks,” inProc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2017, pp. 1125–1134.\n[33] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image\nrestoration with neural networks,”IEEE Trans. Comput. Imag.,v o l .3 ,\nno. 1, pp. 47–57, Mar. 2017.\n[34] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks,” inProc. IEEE\nInt. Conf. Comput. Vis., 2017, pp. 2223–2232.\n[35] X. Mao, Q. Li, H. Xie, R. Y . Lau, Z. Wang, and S. Paul Smolley, “Least\nsquares generative adversarial networks,” inProc. IEEE Int. Conf. Comput.\nVis., 2017, pp. 2794–2802.\n[36] X. Zhang, W. Yu, M.-O. Pun, and W. Shi, “Cross-domain landslide\nmapping from large-scale remote sensing images using prototype-guided\ndomain-aware progressive representation learning,”ISPRS J. Photogram-\nmetry Remote Sens., vol. 197, pp. 1–17, 2023.\n[37] X. Ma, X. Zhang, Z. Wang, and M.-O. Pun, “Unsupervised domain adapta-\ntion augmented by mutually boosted attention for semantic segmentation\nof VHR remote sensing images,” IEEE Trans. Geosci. Remote Sens.,\nvol. 61, 2023, Art. no. 5400515.\n[38] W. Liu, K. Quijano, and M. M. Crawford, “YOLOv5-tassel: Detecting\ntassels in RGB UA V imagery with improved YOLOv5 based on transfer\nlearning,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 15,\npp. 8085–8094, Sep. 2022.\n[39] A. Vaswani et al., “Attention is all you need,” inProc. Adv. Neural Inf.\nProcess. Syst., 2017, pp. 5998–6008.\n[40] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” 2020,arXiv:2010.11929.\nMA et al.: CLOUD-EGAN: RETHINKING CycleGAN FROM A FEATURE ENHANCEMENT PERSPECTIVE FOR CLOUD REMOV AL 5011\n[41] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 10012–10022.\n[42] D. Christopoulos, V . Ntouskos, and K. Karantzalos, “Cloudtran: Cloud\nremoval from multitemporal satellite images using axial transformer net-\nworks,”Int. Arch. Photogrammetry, Remote Sens. Spatial Inf. Sci., vol. 43,\npp. 1125–1132, 2022.\n[43] J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans, “Axial attention\nin multidimensional transformers,” 2019,arXiv:1912.12180.\n[44] L. Wang, R. Li, D. Wang, C. Duan, T. Wang, and X. Meng, “Transformer\nmeets convolution: A bilateral awareness network for semantic segmen-\ntation of very ﬁne resolution urban scene images,”Remote Sens., vol. 13,\nno. 16, 2021, Art. no. 3065.\n[45] L. Gao et al., “STransFuse: Fusing swin transformer and convolutional\nneural network for remote sensing image semantic segmentation,”IEEE\nJ. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 10990–11003,\nOct. 2021.\n[46] C. Wang et al., “Translution-snet: A semisupervised hyperspectral image\nstripe noise removal based on transformer and CNN,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, Jul. 2022, Art. no. 5533114.\n[47] K. Jiang, Z. Wang, C. Chen, Z. Wang, L. Cui, and C.-W. Lin, “Magic ELF:\nImage deraining meets association learning and transformer,” inProc. 30th\nACM Int. Conf. Multimedia, 2022, pp. 1–10.\n[48] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, “Uctransnet: Rethink-\ning the skip connections in u-net from a channel-wise perspective with\ntransformer,” in Proc. AAAI Conf. Artif. Intell., 2022, vol. 36, no. 3,\npp. 2441–2449.\n[49] X. Ma, X. Zhang, and M.-O. Pun, “A crossmodal multiscale fusion network\nfor semantic segmentation of remote sensing data,”IEEE J. Sel. Topics\nAppl. Earth Observ. Remote Sens., vol. 15, pp. 3463–3474, Apr. 2022.\n[50] J. Fang, H. Lin, X. Chen, and K. Zeng, “A hybrid network of CNN and\ntransformer for lightweight image super-resolution,” inProc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit. Workshops, 2022, pp. 1103–1112.\n[51] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks\nfor biomedical image segmentation,” in Proc. Int. Conf. Med. Image\nComput. Comput.- Assist. Interv., 2015, pp. 234–241.\n[52] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” inProc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 7132–7141.\n[53] L. Mou and X. X. Zhu, “Vehicle instance segmentation from aerial image\nand video using a multitask learning residual fully convolutional net-\nwork,”IEEE Trans. Geosci. Remote Sens., vol. 56, no. 11, pp. 6699–6711,\nNov. 2018.\n[54] Q. Liu, M. Kampffmeyer, R. Jenssen, and A.-B. Salberg, “Dense dilated\nconvolutions’ merging network for land cover classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens., vol. 58, no. 9, pp. 6309–6320, Sep. 2020.\n[55] C. Zhou, J. Zhang, J. Liu, C. Zhang, R. Fei, and S. Xu, “Perceppan: Towards\nunsupervised pan-sharpening based on perceptual loss,”Remote Sens.,\nvol. 12, no. 14, 2020, Art. no. 2318.\n[56] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and F.-F. Li, “Imagenet:\nA large-scale hierarchical image database,” inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2009, pp. 248–255.\n[57] D. Lin, G. Xu, X. Wang, Y . Wang, X. Sun, and K. Fu, “A remote sensing\nimage dataset for cloud removal,” 2019,arXiv:1901.00600.\n[58] J. Li, Z. Wu, Z. Hu, Z. Li, Y . Wang, and M. Molinier, “Deep learning based\nthin cloud removal fusing vegetation red edge and short wave infrared\nspectral information for Sentinel-2A imagery,”Remote Sens., vol. 13, no. 1,\npp. 157–188, 2021.\n[59] L. Sun, Y . Zhang, X. Chang, Y . Wang, and J. Xu, “Cloud-aware generative\nnetwork: Removing cloud from optical remote sensing images,”IEEE\nGeosci. Remote Sens. Lett., vol. 17, no. 4, pp. 691–695, Apr. 2020.\n[60] A. Singh and L. Bruzzone, “Sigan: Spectral index generative adversarial\nnetwork for data augmentation in multispectral remote sensing images,”\nIEEE Geosci. Remote Sens. Lett., vol. 19, Sep. 2021, Art. no. 6003305.\n[61] S. Bock and M. Weiß, “A proof of local convergence for the Adam\noptimizer,” inProc. IEEE Int. Joint Conf. Neural Netw., 2019, pp. 1–8.\n[62] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality\nassessment: From error visibility to structural similarity,”IEEE Trans.\nImage Process., vol. 13, no. 4, pp. 600–612, Apr. 2004.\n[63] Q. Huynh-Thu and M. Ghanbari, “Scope of validity of PSNR in im-\nage/video quality assessment,”Electron. Lett., vol. 44, no. 13, pp. 800–801,\n2008.\nXianping Ma (Student Member, IEEE) received the\nbachelor’s degree in geographical information sci-\nence from Wuhan University, Wuhan, China, in 2019.\nHe is currently working toward the Ph.D. degree\nin computer and information engineering with The\nChinese University of Hong Kong, Shenzhen, China.\nHis research interests include remote sensing\nimage processing, deep learning, and multimodal\nlearning.\nYiming Huang received the bachelor’s degree in\ninformation engineering from the Guangdong Uni-\nversity of Technology, Guangzhou, China, in 2022.\nHe is currently working toward the master’s degree\nin communication engineering with The Chinese Uni-\nversity of Hong Kong, Shenzhen, China.\nHis research interests include remote sensing and\ndeep learning, and cloud removal.\nXiaokang Zhang (Member, IEEE) received the\nPh.D. degree in photogrammetry and remote sensing\nfrom The School of Remote Sensing and Information\nEngineering, Wuhan University, Wuhan, China, in\n2018.\nFrom 2019 to 2022, he was a Postdoctoral Research\nAssociate with The Hong Kong Polytechnic Univer-\nsity, Hong Kong, and The Chinese University of Hong\nKong, Shenzhen, China. He is currently a specially\nappointed Professor with the School of Information\nScience and Engineering, Wuhan University of Sci-\nence and Technology, Wuhan, China. He has authored or coauthored more than\n20 scientiﬁc publications in international journals and conferences. His research\ninterests include remote sensing image analysis, computer vision, and deep\nlearning.\nDr. Zhang is currently a Reviewer for more than ten renowned interna-\ntional journals, such as the IEEE TRANSACTIONS ON NEURAL NETWORKS AND\nLEARNING SYSTEMS, Information Fusion, and the IEEE TRANSACTIONS ON\nGEOSCIENCE AND REMOTE SENSING.\n5012 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nMan-On Pun (Senior Member, IEEE) received\nthe B.Eng. degree in electronic engineering from\nThe Chinese University of Hong Kong, Shenzhen\n(CUHKSZ), Shenzhen, China, in 1996, the M.Eng.\ndegree in computer science from the University of\nTsukuba, Tsukuba, Japan, in 1999, and the Ph.D.\ndegree in electrical engineering from the University\nof Southern California (USC) at Los Angeles, Los\nAngeles, CA, USA, in 2006.\nHe was a Postdoctoral Research Associate with\nPrinceton University, Princeton, NJ, USA, from 2006\nto 2008. He held research positions with Huawei, NJ, USA, the Mitsubishi\nElectric Research Labs (MERL), Boston, MA, USA, and Sony, Tokyo, Japan. He\nis currently an Associate Professor with the School of Science and Engineering,\nCUHKSZ. His research interests include artiﬁcial intelligence (AI) Internet of\nThings (AIoT) and applications of machine learning in communications and\nsatellite remote sensing.\nProf. Pun was the recipient of best paper awards from the IEEE Vehicular\nTechnology Conference 2006 Fall, the IEEE International Conference on Com-\nmunication 2008, and the IEEE Infocom’09. He is the Founding Chair of the\nIEEE Joint Signal Processing Society-Communications Society Chapter, Shen-\nzhen. He was an Associate Editor for the IEEE TRANSACTIONS ON WIRELESS\nCOMMUNICATIONS from 2010 to 2014.\nBo Huang received the Ph.D. degree in remote sens-\ning and mapping from the Institute of Remote Sensing\nApplications, Chinese Academy of Sciences, Beijing,\nChina, in 1997.\nHe is currently a Chair Professor with the Depart-\nment of Geography, The University of Hong Kong,\nHong Kong. His research interests include most as-\npects of GIScience, speciﬁcally the design and devel-\nopment of models and algorithms for uniﬁed satellite\nimage fusion, spatiotemporal statistics, and multiob-\njective spatial optimization, and their applications in\nenvironmental monitoring and sustainable land use and transportation planning.\nDr. Huang is currently an Associate Editor for theInternational Journal of\nGeographical Information Science(Taylor & Francis) and the Editor-in-Chief\nof Comprehensive GIS(Elsevier), a three-volume GIS sourcebook.",
  "topic": "Cloud computing",
  "concepts": [
    {
      "name": "Cloud computing",
      "score": 0.8307033181190491
    },
    {
      "name": "Computer science",
      "score": 0.8099567294120789
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6429212689399719
    },
    {
      "name": "Deep learning",
      "score": 0.6080859899520874
    },
    {
      "name": "Transformer",
      "score": 0.5615719556808472
    },
    {
      "name": "Exploit",
      "score": 0.555971622467041
    },
    {
      "name": "Feature learning",
      "score": 0.45316046476364136
    },
    {
      "name": "Feature extraction",
      "score": 0.444979727268219
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4417659044265747
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.43737685680389404
    },
    {
      "name": "Distributed computing",
      "score": 0.35630297660827637
    },
    {
      "name": "Computer security",
      "score": 0.09526649117469788
    },
    {
      "name": "Engineering",
      "score": 0.0789976418018341
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}