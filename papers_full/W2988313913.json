{
  "title": "Porous Lattice-based Transformer Encoder for Chinese NER",
  "url": "https://openalex.org/W2988313913",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2328091689",
      "name": "Mengge Xue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114015791",
      "name": "Bowen Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097019438",
      "name": "Ting-Wen Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098449489",
      "name": "Yue Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3189505338",
      "name": "Erli Meng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1995576685",
      "name": "Bin Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2963201387",
    "https://openalex.org/W2950037171",
    "https://openalex.org/W2062270497",
    "https://openalex.org/W2930726706",
    "https://openalex.org/W2952180055",
    "https://openalex.org/W2250739653",
    "https://openalex.org/W2252066972",
    "https://openalex.org/W2605215742",
    "https://openalex.org/W2964093505",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2964035777",
    "https://openalex.org/W2963682821",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2966874979",
    "https://openalex.org/W2915716523",
    "https://openalex.org/W2963338481",
    "https://openalex.org/W2951635603",
    "https://openalex.org/W2789290064",
    "https://openalex.org/W2964400841",
    "https://openalex.org/W2963472581",
    "https://openalex.org/W3098727690",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2945061189",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W2964302946",
    "https://openalex.org/W2567657016",
    "https://openalex.org/W2890459330",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2965690110",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2951289103",
    "https://openalex.org/W2251435463",
    "https://openalex.org/W800621058",
    "https://openalex.org/W2559281960",
    "https://openalex.org/W2970480111",
    "https://openalex.org/W2952079278",
    "https://openalex.org/W2970323499",
    "https://openalex.org/W2250709962"
  ],
  "abstract": "Incorporating lattices into character-level Chinese named entity recognition is an effective method to exploit explicit word information. Recent works extend recurrent and convolutional neural networks to model lattice inputs. However, due to the DAG structure or the variable-sized potential word set for lattice inputs, these models prevent the convenient use of batched computation, resulting in serious inefficient. In this paper, we propose a porous lattice-based transformer encoder for Chinese named entity recognition, which is capable to better exploit the GPU parallelism and batch the computation owing to the mask mechanism in transformer. We first investigate the lattice-aware self-attention coupled with relative position representations to explore effective word information in the lattice structure. Besides, to strengthen the local dependencies among neighboring tokens, we propose a novel porous structure during self-attentional computation processing, in which every two non-neighboring tokens are connected through a shared pivot node. Experimental results on four datasets show that our model performs up to 9.47 times faster than state-of-the-art models, while is roughly on a par with its performance. The source code of this paper can be obtained from https://github.com/xxx/xxx.",
  "full_text": "Porous Lattice Transformer Encoder for Chinese NER\nMengge Xue1,2, Bowen Yu1,2, Tingwen Liu1,2‚àó, Yue Zhang3\nErli Meng4 and Bin Wang4\n1Institute of Information Engineering, Chinese Academy of Sciences, China\n2School of Cyber Security, University of Chinese Academy of Sciences, China\n3School of Engineering, Westlake University, Hangzhou, China\n4Xiaomi AI Lab, Xiaomi Inc., Beijing, China\n{xuemengge, yubowen, liutingwen}@iie.ac.cn\nyue.zhang@wias.org.cn\n{wangbin11, mengerli}@xiaomi.com\nAbstract\nIncorporating lexicons into character-level Chinese NER by lattices is proven effective to exploit\nrich word boundary information. Previous work has extended RNNs to consume lattice inputs\nand achieved great success. However, due to the DAG structure and the inherently unidirectional\nsequential nature, this method precludes batched computation and sufÔ¨Åcient semantic interaction.\nIn this paper, we propose PLTE, an extension of transformer encoder that is tailored for Chinese\nNER, which models all the characters and matched lexical words in parallel with batch process-\ning. PLTE augments self-attention with positional relation representations to incorporate lattice\nstructure. It also introduces a porous mechanism to augment localness modeling and maintain\nthe strength of capturing the rich long-term dependencies. Experimental results show that PLTE\nperforms up to 11.4 times faster than state-of-the-art methods while realizing better performance.\nWe also demonstrate that using BERT representations further substantially boosts the performance\nand brings out the best in PLTE.\n1 Introduction\nNamed Entity Recognition (NER) is a fundamental task in natural language processing (NLP), which\naims to automatically discover named entities and identify their corresponding categories from plain text.\nNLP tasks such as information retrieval (Berger and Lafferty, 2017), relation extraction (Yu et al., 2019)\nand entity linking (Xue et al., 2019) require the NER as one of their preprocessing components. Recent\nstudies show that English NER models have achieved improved performance by integrating character\ninformation into word representations based on sequence labeling. Different from English NER, East\nAsian languages including Chinese are written without explicit word boundary. One intuitive way to solve\nthis problem is to segment the input sentences into words Ô¨Årst, and then to apply word sequence labeling\n(Yang et al., 2016; He and Sun, 2017a). However, such methods suffer from error propagation between\nthese two subtasks.\nTo overcome this limitation, efforts have been devoted to incorporating word information by leveraging\nlexicon features (Peng and Dredze, 2015; Cao et al., 2018; Wu et al., 2019). As recent state-of-the-art\n(SOTA) lattice-based method, Zhang and Yang (2018) integrated matched lexical words information into\ncharacter sequence with a directed acyclic graph (DAG) structure using lattice LSTM. While obtaining\npromising results, this model faces two challenges. First, as a extension to the non-parallelizable\nsequential LSTM to a DAG structured model, lattice LSTM is restricted to preprocess one character at\na time, which can make it infeasibly to deploy. Second, due to the inherently unidirectional sequential\nnature, lattice LSTM fails to incorporate the word-level semantics into the representation of the characters\nexcept for the last character in each word, despite that such information can be crucial for character-level\nsequence tagging. Taking the sentence in Figure 1 as an example, lattice LSTM decodes the information\nof the lexical word ‚Äú Âçó‰∫¨Â∏Ç(NanJing City)‚Äù to ‚ÄúÂ∏Ç(City)‚Äù but skips the other two inside characters\n‚ÄúÂçó(South)‚Äù and ‚Äú‰∫¨(Capital)‚Äù, although the semantics and boundary information of ‚ÄúÂçó‰∫¨Â∏Ç(NanJing\nCity)‚Äù can be useful knowledge for predicting the tag of ‚ÄúÂçó(South)‚Äù as ‚ÄúB-LOC‚Äù.\n‚àóCorresponding author.\narXiv:1911.02733v3  [cs.CL]  28 Oct 2020\nÂçó‰∫¨(Nanjing)\nt12\nt1\nÂçó(South)\nt2\n‰∫¨(Capital)\nt3\nÂ∏Ç(City)\nt4\nÈïø(Long)\nt5\nÊ±ü(River)\nt7\nÊ°•(Bridge)\nt6\nÂ§ß(Big)\nÂ∏ÇÈïø(Mayor)\nt14\nÊ±üÂ§ßÊ°•(Jiang Daqiao)\nt16\nt13\nÂçó‰∫¨Â∏Ç(Nanjing City)\nt17\nÂ§ßÊ°•(Bridge)\nt15\nÈïøÊ±ü(Yangtze River)\nt8\n‰Ωç(Locates)\nt9\n‰∫é(In)\nt10\n‰∏≠(Center)\nt11\nÂõΩ(Country)\n‰Ωç‰∫é(Locates In)\nt18\nt19\n‰∏≠ÂõΩ(China)\nFigure 1: Example of word character lattice. Restricted by the unidirectional sequential nature, lattice\nLSTM cannot model the semantic interaction between word ‚ÄùÂçó‰∫¨Â∏Ç(Nanjing City)‚Äù and its constituent\ncharacters ‚ÄùÂçó(South)‚Äù and ‚Äù‰∫¨(Capital)‚Äù, resulting in the loss of crucial information for tagging. Besides,\nlattice LSTM cannot perform batched computation due to the directed acyclic graph input structure.\nIn this paper, we address these issues by considering a novel Porous Lattice Transformer Encoder\n(PLTE). Inspired by previous research on machine translation (Xiao et al., 2019; Sperber et al., 2019),\nwhich integrated lattice-structured inputs into self-attention models, we propose a lattice transformer\nencoder for Chinese NER by introducing lattice-aware self-attention, which borrows the idea from the\nrelative positional embedding (Shaw et al., 2018) to make self-attention aware of the relative position\ninformation in lattice structure. Considering that self-attention network calculates attention weights\nbetween each pair of tokens in a sequence regardless of their distance, we simply concatenate all the\ncharacters and lexical words as input to consume lattices without resorting to the DAG structure. In this\nway, characters coupled with lexical words can be processed in batches. A lexical word representation\nis allowed to build a direct relation with the included characters by lattice-aware self-attention, thus\naddressing the second issue.\nSome work (Yang et al., 2018; Yang et al., 2019) demonstrates that self-attention beneÔ¨Åts from locality\nmodeling, especially for the NER task. As we can see from the example in Figure 1, the word ‚Äú ‰Ωç\n‰∫é(Locates In)‚Äù is the immediate and most obvious feature to guide the neighboring character ‚ÄúÊ°•(Bridge)‚Äù\nto be identiÔ¨Åed as ‚ÄúE-LOC‚Äù instead of ‚ÄúE-PER‚Äù, while ‚Äù‰∏≠ÂõΩ(China)‚Äù has no contribution to this decision.\nGiven this observation, we further introduce a novel porous mechanism to enhance the local dependencies\namong neighboring tokens. The key insight is to modify the self-attention architecture by replacing the\nfully-connected topology with a pivot-shared structure. In this particular, every two non-neighboring\ntokens are connected by a shared pivot node to strengthen the dependency for two neighboring tokens.\nExperimental results on four datasets demonstrate that our model performs up to 11.4 times faster than\nbaselines and achieves better performance. Furthermore, we show that our model can be easily integrated\ninto the pre-trained language model such as BERT (Devlin et al., 2019), and combining them further\nimproves the state of the art.\nIn summary, this paper makes the following contributions: (1) We investigate lattice transformer\nencoder for Chinese NER, which is capable of handling lattices in batch mode and capturing dependencies\nbetween characters and matched lexical words. (2) We revise lattice-aware attention distribution via a\nporous mechanism, which enhances the ability of capturing useful local context. (3) Experimental results\nshow that the proposed model is effective and efÔ¨Åcient. The source code of this paper can be obtained\nfrom https://github.com/strawberryx/PLTE.\n2 Related Work\nOur work is in line with NER models based on neural networks and lattice transformer models.\nHuang et al. (2015) proposed a BiLSTM-CRF model for NER and achieved strong performance. Santos\nand Guimaraes (2015) used word- and character-level representations based on the CharWNN deep neural\nnetwork. Lample et al. (2016) designed a character LSTM and word LSTM for NER. Compared to our\nwork, these word-based methods suffer from segmentation errors.\nTo avoid segmentation errors, most recent NER models are built upon character sequence labeling.\nPeng and Dredze (2015) proposesd a joint training objective for three types of neural embeddings to\nbetter recognize entity boundary. Lu et al. (2016) presented a position-sensitive skip-gram model to\nlearn multi-prototype Chinese character embeddings. He and Sun (2017a) took the positional character\nembeddings into account. Although these methods achieve promising performance, they ignore word\ninformation lying in character sequence.\nSome work exploits rich word boundary and semantic information in character sequence. Cao et al.\n(2018) applied an adversarial transfer learning framework to integrate the task-shared word boundary\ninformation into Chinese NER. Liu et al. (2019) explored four different strategies for Word-Character\nLSTM. Gui et al. (2019a) proposed a CNN-based NER model that incorporates lexicons using a rethinking\nmechanism. Recent state-of-the-art methods exploit lattice-structured models to integrate latent word\ninformation into character sequence, which has been proven effective on various NLP tasks (Su et al.,\n2017; Tan et al., 2018) . SpeciÔ¨Åcally, Zhang and Yang (2018) utilized the lattice LSTM to leverage explicit\nword information over character sequence labeling. Based on this method, Gui et al. (2019b) and Sui et\nal. (2019) formulated the lattice structure as a graph and leveraged Graph Neural Networks (GNNs) to\nintegrate lexical knowledge. However, for the NER task, coupling pre-trained language models such as\nBERT (Devlin et al., 2019) with GNNs and Ô¨Åne-tuning them can be non-trivial.\nLattice transformer has been exploited in NMT (Xiao et al., 2019), as well as speech translation (Sperber\net al., 2019; Zhang et al., 2019). Compared with existing work, our proposed porous lattice transformer\nencoder is different in both motivation and structure. We revise the fully-connected attention distribution\nwith a pivot-shared structure via the porous mechanism to enhance the local dependencies among\nneighboring tokens.1 To our knowledge, we are the Ô¨Årst to design a lattice transformer for Chinese NER.\n3 Background\nIn this section, we Ô¨Årst brieÔ¨Çy review the self-attention mechanism, then move on to current lattice\nTransformer that our PLTE model is built upon.\n3.1 Self-Attention\nSelf-attention mechanism has attracted increasing attention due to their Ô¨Çexibility in parallel computation\nand dependency modeling. Given an input sequence representation X = {x1,¬∑¬∑¬∑ ,xn}‚àà Rn√ód, we\ncan Ô¨Årst transform it into queries Q = XWQ ‚ààRn√ódk, keys K = XWK ‚ààRn√ódk, and values V =\nXWV ‚ààRn√ódv , where {WQ,WK,WV }are trainable parameters. The output sequence representation\nis calculated as:\nAtt(Q,K,V) = Softmax(QKT\n‚àödk\n)V, (1)\nwhere ‚àödk is the scaling factor.\n3.2 Lattice Transformer\nTransformer has been used for many NLP tasks, notably machine translation and language modeling (Wang\net al., 2019; Devlin et al., 2019). By invoking multi-layer self-attention for global context modeling,\nTransformer enables paralleled computation and addresses the inherent sequential computation shortcom-\ning of RNNs. Lattice Transformer is a generalization of the standard transformer architecture to accept\nlattice-structured inputs, it linearizes the lattice structure and introduces a position relation score matrix to\nmake self-attention aware of the topological structure of lattice:\nAtt(Q,K,V) = Softmax(QKT + R‚àödk\n)V, (2)\nwhere R ‚ààRn√ón encodes the lattice-dependent relations between each pair of elements from the lattices,\nand its computational method relies on the speciÔ¨Åc relation deÔ¨Ånition according to the task objective.\n4 Models\nThe overall structure of our model is shown in Figure 2(a), which consists of 3 main components, lattice\ninput layer, Porous lattice transformer encoder and BiGRU-CRF decoding.\n1Differences between lattice self-attention and porous lattice self-attention are shown in Figure 1 in the Appendix.\nt1\nt1 t2 t9 t10 s\nt2\nt9\nt10\ns\n‚Ä¶ ‚Ä¶\nÊ±üÂ§ßÊ°•\nJiang Daqiao\nÂçó‰∫¨Â∏Ç\nNanjing \nCity\nÂ∏ÇÈïø\nmayor\nÂ§ßÊ°•\nRiver\nÂçó‰∫¨\nNanjing\nÈïøÊ±ü\nYangtze \nRiver\nÊ°•\nBridge\nÂçó\nSouth\n‰∫¨\nCapital\nÂ∏Ç\nCity\nÂ§ß\nBig\nÈïø\nLong\nÊ±ü\nRiver\nCharacter Embedding Lookup Table Word Embedding Lookup Table\nPMHA\nAdd & Norm\nFeed Forwad\nAdd & Norm\nLattice Input SAvg Pooling\nLattice-aware Position Encoding\nDecoding Layer\nMask\n‚Ä¶\n(a)\nt10 :e3:4\nÂ∏ÇÈïø(Mayor)\nt8 :e1:2\nÂçó‰∫¨(Nanjing)\nt12 :e4:7\nÊ±üÂ§ßÊ°•(Jiang Daqiao)\nr5 r6 r7 r8\nr5 r7\nr5\nr5\nr5\nr1\nr6 r1\nr6\nr8 r8 r8\nr8\nr8\nr8\nr8\nr8\nr8 r8\nr2r7\nr5 r5\nr2 r5\nr3\nr4\nr1 r7\nr7\nr2\nr7\nt1\nt2\nt4\nt9\nt10\nt4\ns\nt9t2\nt1\nt10 s\nt1 :c1\nÂçó(South)\nt2 :c2\n‰∫¨(Capital)\nt3 :c3\nÂ∏Ç(City)\nt4 :c4\nÈïø(Long)\nt5 :c5\nÊ±ü(River)\nt6 :c6\nÂ§ß(Big)\nt7 :c7\nÊ°•(Bridge)\nt9 :e1:3\nÂçó‰∫¨Â∏Ç(Nanjing City)\nt11 :e4:5\nÈïøÊ±ü(Yangtze River)\nt13 :e6:7\nÂ§ßÊ°•(Bridge) (b)\nFigure 2: (a) The overall architecture of PLTE (best viewed in color). Characters and lexical words are\nshown in yellow and green, respectively. We concatenate character and word embeddings as lattice input.\nWhen decoding, we mask words and just make sequence labeling for characters; and (b) Illustration of the\nrelative position relation matrix. Notice that we present several relations among partial tokens as instances.\nDifferent colors indicate different relations deÔ¨Åned in Figure 3. For instance, the relation between t4 and\nt10 is r6, since that ‚Äú Èïø(Long)‚Äù is included in ‚ÄúÂ∏ÇÈïø(Mayor)‚Äù. The circle Ô¨Ålled with lines denotes that we\ndon‚Äôt compute attention between non-neighboring tokens due to our porous mechanism.\n4.1 Lattice Input Layer\nThe input layer aims to embed both semantic information and position information of tokens into their\ntoken embeddings.\nWord-Character Embedding Formally, let S = {c1,...,c M }denotes a sentence, where ci is the i-th\ncharacter. The lexical words in the lexicon that match a character subsequence can be formulated as ei:j,\nwhere the index of the Ô¨Årst and last letters are iand j, respectively. Similarly, we can also represent ci\nas ei:i. As shown in the top half of Figure 2(b), e3:4 indicates the lexical word named ‚Äú Â∏ÇÈïø(Mayor)‚Äù\nwhich contains c3 named ‚ÄúÂ∏Ç(City)‚Äù and c4 named ‚ÄúÈïø(Long)‚Äù. Each character ci can be turned into the\nvector xc\ni which includes it‚Äôs character embedding and bigram embedding. By looking up the vector from\na pre-trained word embedding matrix, each matched lexical word ei:j is represented as a vector xw\ni:j.\nLattice-Aware Position Encoding Since self-attention architecture contains no recurrence, to make the\nmodel aware of the sequence order, we add position embedding to the semantic embedding of each token.\nSpeciÔ¨Åcally, the position of a character is deÔ¨Åned as its absolute position in the input sequence S. And the\nposition of a matched word is the position of its Ô¨Årst character. For example, in Figure 2(b), the position\nof word ‚Äú Âçó‰∫¨(Nanjing)‚Äù is 1 because this sentence begins with ‚ÄúÂçó(South)‚Äù.\nFinally, since position information is incorporated into token embeddings, we can simply put the\nmatched words to the end of the character sequence Sand form a new token sequence T = {ti}N\ni=1 to\nconsume lattice structure, where N is the sum of the number of characters and words. See the top half of\nFigure 2(b) for the detailed correspondence.\n4.2 Porous Lattice Transformer Encoder\nAs mentioned in the Introduction, our primary goal is to adapt the standard transformer to the task\nof Chinese NER with lattice inputs. To this end, we Ô¨Årst propose lattice-aware self-attention to con-\nsume input tokens and the relative position information of lattice structure. Then, we design a porous\nmechanism which learns sparse attention coefÔ¨Åcients by replacing the fully-connected topology with a\n\tùëü# ùëû = ùëò ‚àí 1 ùëí*:,is left adjacent to ùëí-:.\n\tùëü/ ùëù < ùëò ‚â§ ùëû < ùëô ùëí*:,is left intersected with\tùëí-:.\n\tùëü4 ùëù ‚â§ ùëò ‚â§ ùëô ‚â§ ùëû ùëí*:, includes ùëí-:.\n\tùëü5 ùëû < ùëò ‚àí 1 ùëí*:, is non-neighboring to ùëí-:.\nConditions Relations\n\tùëü6 ùëô = ùëù ‚àí 1 ùëí*:, is right adjacent to ùëí-:.\n\tùëü7 ùëò < ùëù ‚â§ ùëô < ùëû ùëí*:,is right intersected with\tùëí-:.\n\tùëü8 ùëò ‚â§ ùëù ‚â§ ùëû ‚â§ ùëô ùëí*:, is included in ùëí-:.\n\tùëü5 ùëô < ùëù ‚àí 1 ùëí*:, is non-neighboring to ùëí-:.\nConditions Relations\nFigure 3: Relation between ep:q and ek:l. We use the block Ô¨Ålled with dots and lines to present ep:q and\nek:l, respectively. Notice that if p= q= k= l, we denote the relation between ep:q and ek:l as r5. And\nrelation r7 consists of two cases.\npivot-shared structure to enhance the association between neighboring elements. We also use multi-head\nattention (Vaswani et al., 2017) to capture information from different representation subspaces jointly.\nLattice-Aware Self-Attention (LASA) The position embedding method described above only indicates\nthe sequential order and cannot capture the relative position information of the lattice-structured input.\nFor example, in Figure 2(b), the sequential distance from ‚Äú Â∏Ç(City)‚Äù or ‚ÄúÂ∏ÇÈïø(Mayor)‚Äù to ‚ÄúÈïø(Long)‚Äô\nis 1 under previous position deÔ¨Ånition. Actually, ‚Äú Èïø(Long)‚Äù is included in ‚ÄúÂ∏ÇÈïø(Mayor)‚Äù and right\nadjacent to ‚ÄúÂ∏Ç(City)‚Äù, but absolute position fails to make a distinction. To address this issue, we propose\na relative position relation matrix L‚ààNN√óN to present such position information. Similar to (Xiao et\nal., 2019), we enumerate all possible relations between each pair of elements ep:q and ek:l in Figure 3.\nWe give a detailed and vivid example in Figure 2(b). For two tokens ti and tj refering to ep:q and ek:l\nrespectively, the matrix entry Li,j is the pre-deÔ¨Åned relation between them, such as L1,2 = r1.\nMore concretely, in order to make Llearnable, we Ô¨Årst represent Las the relation position embedding,\na 3D tensor R ‚ààRN√óN√ódr by looking up a trainable embedding matrix A ‚ààR8√ódr , where dr is the\nrelational embedding dimensionality. Note that here we deÔ¨Åne eight types of embedding instead of seven\nrelations in Figure 3. The additional embedding is introduced to represent the interaction relation with a\nshared pivot node (described in the next section) and facilitate parallel computation. Then, to incorporate\nsuch position relations into attention layer, we adapt Equation 2 as follows:\nŒ±=Softmax( QKT +einsum( ‚Äúik,ijk‚Üíij‚Äù,Q,RK)‚àödk\n) (3)\nAtt(Q,K,V)= Œ±V+einsum(‚Äúik,ikj ‚Üíij‚Äù,Œ±,RV ), (4)\nwhere RK ‚ààRN√óN√ódk and RV ‚ààRN√óN√ódv are two relation embedding tensors which are added to\nthe keys and values respectively to indicate relation between input tokens. In our case, Q is a 2D array of\nshape [N √ódk] while RK is a 3D array and we need to result in a new array of shape [N √óN], with the\nelement in i-th row and j-th column is ‚àëdk\nk=1 QikRK\nijk. To implement this operation, we apply einsum2\nto sum out the dimension of the hidden size, which is an operation computing multilinear expressions\n(i.e., sums of products) using the Einstein summation convention.\nPorous Multi-Head Attention (PMHA) Considering that standard self-attention mechanism encodes\nsequences by relating sequence items to another one through computation of pairwise similarity, it\ndisperses the distribution of attention and overlooks the local knowledge provided by neighboring\nelements, which is crucial for NER. To maintain the strength of capturing long distance dependencies\nand enhance the ability of capturing short-range dependencies, we sparsify the transformer architecture\nby replacing the fully-connected topology with a pivot-shared structure referenced by (Guo et al., 2019).\nSpeciÔ¨Åcally, given element set Eand its embedding matrix X, where ei:j ‚ààEand xi:j ‚ààX (if ei:j is a\ncharacter then xi:j = xc\ni else xi:j = xw\ni:j ), we deÔ¨Åne erk\ni:j as the element set whose relation with ei:j is rk,\nxrk\ni:j as the concatenation of the embeddings where each embedding represents the corresponding element\nin erk\ni:j. we also deÔ¨Åne the neighboring set of ei:j as Œµ={er1\ni:j; er2\ni:j; er3\ni:j; er4\ni:j; er5\ni:j; er6\ni:j}, then we update the\n2This operator is available in Numpy, TensorFlow, and Pytorch.\nhidden state hi:j of ei:j with multi-head attention as follows:\nhi:j = [z1\ni:j; z2\ni:j; ...; zH\ni:j]WO\nzh\ni:j = Att(xi:jWQ\nh ,ci:jWK\nh ,ci:jWV\nh ), h‚àà[1,H]\nci:j = [xr1\ni:j; xr2\ni:j; xr3\ni:j; xr4\ni:j; xr5\ni:j; xr6\ni:j; s]\ns = 1\nn\n‚àë\ni,j\nxi:j,\n(5)\nwhere WQ\nh ,WK\nh ,WV\nh are trainable projection matrices corresponding to the h-th head, zh is the h-th\noutput, H is the number of heads and Att() is deÔ¨Åned in Equation 4. As we can see, in our porous multi-\nhead attention, one element ei:j just makes direct attention computation with its neighboring elements\nand models the non-local compositions via the pivot node s. As illustrated in Figure 2(b), ei:j doesn‚Äôt\ncompute attention directly with the element set er7\ni:j, thus we mask them. Under this lightweight porous\nstructure, our transformer encoder has an approximate ability to strengthen local dependencies among\nneighboring tokens and keep the ability to capture long distance dependencies.\n4.3 BiGRU-CRF Decoding\nAfter extracting the semantic information by the porous lattice transformer encoder layer, we feed\nthe character sequence representations into a BiGRU-CRF decoding layer to make sequence tagging.\nSpeciÔ¨Åcally, taking [xc\n1; h1:1],..., [xc\nn; hn:n] as input, a bidirectional GRU is implemented to produce\nforward state ‚àí ‚Üíh t and backward state ‚Üê ‚àíh t for each time step, and then we concatenate these two separate\nhidden states as the encoding output of the t-th character, donated as ht = [‚àí ‚Üíh t; ‚Üê ‚àíh t].\nFinally, a standard CRF layer is used on top of h1,h2,..., hn to make sequence tagging. For a label\nsequence y = {y1,y2,...,y n}, we deÔ¨Åne its probability to be:\nP(y|S)= exp(‚àë\ni(Wyi\nCRFhi+b\n(yi‚àí1,yi)\nCRF ))\n‚àë\ny‚Ä≤exp(‚àë\ni(W\ny‚Ä≤\ni\nCRFhi+b\n(y‚Ä≤\ni‚àí1,y‚Ä≤\ni)\nCRF ))\n, (6)\nwhere y‚Ä≤denotes all possible tag sequences, Wyi\nCRF is a model parameter speciÔ¨Åc to yi, and b(yi‚àí1,yi)\nCRF is\nthe transition score between yi‚àí1 and yi. For decoding, we use the Ô¨Årst-order Viterbi algorithm to Ô¨Ånd the\nlabel sequence that obtains highest score.\n4.4 Training\nGiven a set of manually labeled training data {(Si,yi)}|N\ni=1, sentence-level log-likelihood loss with L2\nregularization is used to train the model:\nL=\nN‚àë\ni=1\nlog(P(yi|Si)) + Œª\n2 ‚à•Œò ‚à•2, (7)\nwhere Œªis the L2 regularization weight and Œò represents the parameter set.\n5 Experiments\nWe conduct experiments to investigate the effectiveness of our proposed PLTE method across different\ndomains. Standard precision (P), recall (R) and F1-score (F1) are used as evaluation metrics.\n5.1 Experimental Setup\n5.1.1 Data\nWe evaluate our model on four datasets, including OntoNotes (Ralph et al., 2011), MSRA (Levow, 2006),\nWeibo NER (Peng and Dredze, 2015; He and Sun, 2017b) and a Chinese Resume dataset (Zhang and\nYang, 2018). We use the same training, valid and test split as (Zhang and Yang, 2018). For these datasets,\nboth OntoNotes and MSRA are in news domain, while Weibo and Resume come from social media.\nOntoNotes\nInput Models P R F1\nGold seg\nChe et al. (2013) 77.71 72.51 75.02\nWang et al. (2013) 76.43 72.32 74.32\nYang et al. (2016) 72.98 80.15 76.40\nNo seg\nLattice LSTM (2018) 76.35 71.56 73.88\nLR-CNN (2019a) 76.40 72.60 74.45\nCAN-NER(2019) 75.05 72.29 73.64\nPLTE 76.78 72.54 74.60\nBERT-Tagger 78.01 80.35 79.16\nLattice LSTM[BERT] 79.79 79.41 79.60\nLR-CNN[BERT] 79.41 80.32 79.86\nPLTE[BERT] 79.62 81.82 80.60\nResume\nModels P R F1\nLattice LSTM (2018) 94.81 94.11 94.46\nCAN-NER(2019) 95.05 94.82 94.94\nLR-CNN (2019a) 95.37 94.84 95.11\nPLTE 95.34 95.46 95.40\nBERT-Tagger 96.12 95.45 95.78\nLattice LSTM[BERT] 95.79 95.03 95.41\nLR-CNN[BERT] 95.68 96.44 96.06\nPLTE[BERT] 96.16 96.75 96.45\nTable 1: Main results on OntoNotes and Resume\nMSRA\nModels P R F1\nZhou et al. (2013) 91.86 88.75 90.28\nLu et al. (2016) - - 87.94\nCao et al. (2018) 91.73 89.58 90.64\nLattice LSTM (2018) 93.57 92.79 93.18\nCAN-NER(2019) 93.53 92.42 92.97\nLR-CNN (2019a) 94.50 92.93 93.71\nPLTE 94.25 92.30 93.26\nBERT-Tagger 94.43 93.86 94.14\nLattice LSTM[BERT] 93.99 92.86 93.42\nLR-CNN[BERT] 94.68 94.03 94.35\nPLTE[BERT] 94.91 94.15 94.53\nWeibo\nModels P R F1\nPeng and Dredze (2016) 66.47 47.22 55.28\nHe and Sun (2017a) 61.68 48.82 54.50\nCao et al. (2018) 59.51 50.00 54.43\nLattice LSTM (2018) 52.71 53.92 53.13\nLR-CNN (2019a) 65.06 50.00 56.54\nPLTE 62.21 49.54 55.15\nBERT-Tagger 67.12 66.88 67.33\nLattice LSTM[BERT] 61.08 47.22 53.26\nLR-CNN[BERT] 64.11 67.77 65.89\nPLTE[BERT] 72.00 66.67 69.23\nTable 2: Main results on MSRA and Weibo\n5.1.2 Baseline Methods\nWe compare our proposed model to several recent lexicon-enhanced character-based models.\nLattice LSTM. Lattice LSTM (Zhang and Yang, 2018) exploits lexical information in character\nsequence through gated recurrent cells, which can avoid segmentation errors.\nLR-CNN. LR-CNN (Gui et al., 2019a) is the latest SOTA method of Chinese NER, which incorporates\nlexicons using a rethinking mechanism.\nFurthermore, to explore the effectiveness of pre-trained language model, we implement several baselines\nbased on BERT representations.\nBERT-Tagger. BERT-Tagger (Devlin et al., 2019) uses the outputs from the last layer of model\nBERTbase as the character-level enriched contextual representations to make sequence labelling.\nPLTE[BERT]/LR-CNN[BERT]/Lattice LSTM[BERT].These three models replace character em-\nbeddings with the pre-trained BERT representations, and use softmax layer to make sequence tagging.\n5.1.3 Hyper-parameter settings\nIn our experiments, we use the same character embeddings, character bigram embeddings and word embed-\ndings as (Zhang and Yang, 2018), which are pre-trained on Chinese Giga-word3 using Word2vec (Mikolov\net al., 2013) and Ô¨Åne-tuned during training. The model is trained using stochastic gradient descent with\nthe initial learning rate of 0.045 and the weight decay of 0.05. Dropout is applied to the embeddings and\nGRU layer with a rate of 0.5 and the transformer encoder with 0.3. For the biggest dataset MSRA and the\nsmallest dataset Weibo, we set the dimensionality of GRU hidden states as 200 and 80 respectively. For\nthe other datasets, this dimension is set to 100. What‚Äôs more, the hidden size and the number of heads are\nset to 128 and 6, respectively. For models based on BERT, we Ô¨Åne-tune BERT representation layer during\ntraining. We use BertAdam to optimize all trainable parameters, select the best learning rate from 1e-5 to\n1e-4 on the development set.\n5.2 Results\nOntoNotes. Table 1 illustrates our experimental results on OntoNotes. The ‚ÄúInput‚Äù column indicates\nwhether the input sentences are segmented or not, where methods in Gold seg process word sequences\nwith gold segmentation and No seg indicates that the input sentence is a character sequence.\n3https://catalog.ldc.upenn.edu/LDC2011T13\nModels Word2vec BERT\nLattice LSTM LR-CNN PLTE Lattice LSTM LR-CNN PLTE\nOntoNotes 1√ó 2.23√ó 11.4√ó 1√ó 1.96√ó 6.21√ó\nMSRA 1√ó 1.57√ó 8.48√ó 1√ó 1.97√ó 7.11√ó\nWeibo 1√ó 2.41√ó 9.12√ó 1√ó 2.02√ó 6.48√ó\nResume 1√ó 1.44√ó 9.68√ó 1√ó 1.46√ó 5.57√ó\nTable 3: Testing-time speedup of different models. Lattice LSTM and LR-CNN can only run with\nbatch size=1 while our PLTE model runs with batch size=16.\nWith gold-standard segmentation, all of the word-level models (Che et al., 2013; Wang et al., 2013;\nYang et al., 2016) achieve strong performance by using segmentation and external labeled data. But\nsuch information is not available in most datasets, such that we only use pre-trained character and word\nembeddings as our resource.\nUnder No-segmentation settings, we Ô¨Årst compare 3 widely-used non-BERT models. Our PLTE model\nachieves the best F1 score and gains a 0.72% improvement over lattice LSTM in F1 score since our\nmodel integrates lexical words information into self-attention computation in a more effective way.4 With\npre-trained BERTbase, BERT-Tagger leads to a signiÔ¨Åcant boost in performance to79.16%. On this basis,\nour proposed PLTE[BERT] model outperforms the BERT-Tagger by1.44% in F1 score on OntoNotes.\nMSRA/Weibo/Resume Tables 1 and 2 present comparisons among various methods on the MSRA,\nWeibo, and Resume datasets. Existing statistical methods explore the rich statistical features (Zhou et al.,\n2013) and character embedding features (Lu et al., 2016). For neural models, some existing models use\nmulti-task learning (Peng and Dredze, 2016; Cao et al., 2018) or semi-supervised learning (He and Sun,\n2017a). CAN-NER (Zhu and Wang, 2019) investigate a character-based convolutional attention network\ncoupled with GRU for Chinese NER.\nConsistent with observations on OntoNotes, all the lexicon-enhanced methods achieve higher F1\nscores than character-based methods, which demonstrates the usefulness of lexical word information.\nWith pre-trained contextual representations, BERT-based models outperform non-BERT models by a\nlarge margin. Even though the original BERT model already provides strong prediction power, PLTE\nconsistently improves over BERT-Tagger, lattice LSTM[BERT] and LR-CNN[BERT], which indicates\nthat our proposed PLTE model can make better use of these semantic representations. Another interesting\nobservation is that PLTE gains more signiÔ¨Åcant improvement when combined with BERT compared with\nother lexicon-enhanced methods. We suspect that it is because PLTE is more capable of fully leveraging\nthe language information embedded in the input representations. While the embeddings pre-trained by\nWord2vec are not as informative to PLTE to fulÔ¨Åll its potential, BERT representation can well capture\nrich semantic patterns and help PLTE improve the performance.\n5.3 Experimental Discussion\n5.3.1 EfÔ¨Åciency Advantage\nPLTE also outperforms current lexicon-enhanced methods in efÔ¨Åciency. Table 3 lists the test times of\ndifferent models with different input representations on all four benchmarks. As we can see, PLTE runs\nup to 11.4 and 5.11 times faster than lattice LSTM and LR-CNN respectively with Word2vec embeddings\non OneNotes. Similar efÔ¨Åciency improvement can also be observed on other datasets under both two\nkinds of input representations. Aligning word-character lattice structure for batch running can be usually\nnon-trivial (Sui et al., 2019) and both lattice LSTM and LR-CNN have no ability in batch-running due to\nthe DAG structure or variable-sized lexical words set. In contrast, PLTE overcomes this limitation since\nwe can simply concatenate all the elements as input thanks to the lattice-aware self-attention mechanism,\nwhich calculates attention weights between each pair of tokens by matrix multiplication, thus can be\ncomputed parallelly in batches.\nTo investigate the inÔ¨Çuence of the different sentence lengths, we conduct experiments on OntoNotes by\nsplitting this dataset into Ô¨Åve parts according to sentence length. The results in Figure 4(a) demonstrate\n4Case study is provided in Table 1 in the Appendix.\n20 40 60 80 100\nSentence Length\n0\n50\n100\n150\n200\n250\n300Sen/s\nLattice LSTM\nLR-CNN\nPLTE(batch_size=1)\nPLTE(batch_size=4)\n(a)\nPLTE -LASA -PM -LASA-PM\nModels\n50\n60\n70\n80\n90F1-score\nOntoNotes\nMSRA\nWeibo\nResume (b)\nFigure 4: (a) Test speed against the sentence length. Sen/s denotes the number of sentences processed per\nsecond; and (b) An ablation study of our proposed model. For model without lattice-aware self-attention\n(-LASA), we take character sequence as input, and one character just computes multi-head self-attention\nweights with its adjacent characters and the shared pivot node. For model without porous mechanism\n(-PM), we directly utilize multi-head LASA to aggregate the weighted information of each word with\nfully-connected attention connections. For PLTE-LASA-PMHA, we apply multi-head self-attention to\neach pair of elements from the input character sequence.\nthat PLTE runs faster than lattice LSTM and LR-CNN with different sentence lengths, especially for short\nsentences. In particular, when the sentence length is less than 20, PLTE(batch size=4) runs 9.64 times\nfaster than lattice LSTM and 8.81 times faster than LR-CNN. When the sentence length increases, the\nefÔ¨Åciency gains from batching computation decline gradually due to the limited computing resources\nof a single GPU. Besides, even if we set the batch size as 1, PLTE still has remarkable advantage in\nspeed, since lattice LSTM demands multiple recurrent computation steps, and the rethinking mechanism\nin LR-CNN is also computationally expensive.\n5.3.2 Model Ablation study\nWe conduct an ablation study on four datasets to understand the effectiveness of each component, the\nresults are shown in Figure 4(b). We can observe that: (1) Removing the LASA module hurts the results\nby 3.63%, 0.99%, 2.81% and 0.58% F1 score on four datasets respectively, which indicates that lexicons\nplay an important role in character-level Chinese NER. (2) By introducing the porous mechanism (PM),\nwe can enhance the ability of capturing useful local context, which is beneÔ¨Åcial to NER, while maintaining\nthe strength of capturing long-term dependencies. (3) PLTE-PM performs worse than PLTE-LASA-PM,\nwhich conÔ¨Årms that the standard LASA is not suitable for NER because it takes into account all the signals\nand disperses the distribution of attention, while NER may be beneÔ¨Åted more from local modeling. (4)\nPLTE-LASA outperforms PLTE-LASA-PM on most datasets, which shows that the porous mechanism\ncan also beneÔ¨Åt self-attention when only taking characters as input.\n6 Conclusion\nWe presented PLTE, a porous lattice transformer encoder which incorporates lexicons into character-level\nChinese NER. PLTE enables the interaction between the matched lexical words and their constituent\ncharacters, and proceeds in batches with the lattice-aware self-attention. It also learns a porous attention\ndistribution to enhance the ability of localness modeling. We evaluate the proposed model on four Chinese\nNER datasets. Using Word2vec embeddings, our PLTE outperforms various baselines and performs up to\n11.4 times faster than previous lattice-based method. Switching to BERT representations, PLTE achieves\nmore signiÔ¨Åcant performance gain than existing methods. There are multiple venues for future work,\nwhere one promising direction is to apply our model to the pre-training procedure of Chinese Transformer\nlanguage models.\nReferences\nAdam Berger and John Lafferty. 2017. Information Retrieval as Statistical Translation. In SIGIR Forum, pages\n219‚Äì226.\nPengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, and Shengping Liu. 2018. Adversarial Transfer Learning for\nChinese Named Entity Recognition with Self-Attention Mechanism. In EMNLP, pages 182‚Äì192.\nWanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named Entity Recognition with\nBilingual Constraints. In HLT-NAACL, pages 52‚Äì62.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In NAACL, pages 4171‚Äì4186.\nTao Gui, Ruotian Ma, Qi Zhang, Lujun Zhao, Yu-Gang Jiang, and Xuanjing Huang. 2019a. CNN-Based Chinese\nNER with Lexicon Rethinking. In IJCAI, pages 4982‚Äì4988.\nTao Gui, Yicheng Zou, Qi Zhang, Minlong Peng, Jinlan Fu, Zhongyu Wei, and Xuanjing Huang. 2019b. A\nlexicon-based graph neural network for Chinese NER. In EMNLP, pages 1039‚Äì1049.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-Transformer.\nIn NAACL, pages 1315‚Äì1325.\nHangfeng He and Xu Sun. 2017a. A UniÔ¨Åed Model for Cross-Domain and Semi-Supervised Named Entity\nRecognition in Chinese Social Media. In AAAI, pages 3216‚Äì3222.\nHangfeng He and Xu Sun. 2017b. F-Score Driven Max Margin Neural Network for Named Entity Recognition in\nChinese Social Media. In EACL, pages 713‚Äì718.\nZhiheng Huang, Wei Liang Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF Models for Sequence Tagging.\narXiv: Computation and Language.\nGuillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural\nArchitectures for Named Entity Recognition. In NAACL:HLT, pages 260‚Äì270.\nGina-Anne Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and\nnamed entity recognition. In In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,\npages 108‚Äì117.\nWei Liu, Tongge Xu, Qinghua Xu, Jiayu Song, and Yueran Zu. 2019. An Encoding Strategy Based Word-\nCharacter LSTM for Chinese NER. In NAACL, pages 2379‚Äì2389.\nYanan Lu, Yue Zhang, and Dong-Hong Ji. 2016. Multiprototype Chinese Character Embedding. In LREC, pages\n855‚Äì859.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of\nwords and phrases and their compositionality. In NIPS, pages 3111‚Äì3119.\nNanyun Peng and Mark Dredze. 2015. Named Entity Recognition for Chinese Social Media with Jointly Trained\nEmbeddings. In EMNLP, pages 548‚Äì554.\nNanyun Peng and Mark Dredze. 2016. Improving Named Entity Recognition for Chinese Social Media with Word\nSegmentation Representation Learning. In ACL, pages 149‚Äì155.\nWeischedel Ralph, Pradhan Sameer, Ramshaw Lance, Palmer Martha, Xue Nianwen, Marcus Mitchell, Taylor\nAnn, Greenberg Craig, Hovy Eduard, Belvin Robert, and et al. 2011. Ontonotes release 4.0. In LDC2011T03,\nPhiladelphia, Penn.: Linguistic Data Consortium.\nCicero Nogueira Dos Santos and Victor Guimaraes. 2015. Boosting Named Entity Recognition with Neural\nCharacter Embeddings. In Proceedings of the Fifth Named Entity Workshop, pages 25‚Äì33.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In\nNAACL, pages 464‚Äì468.\nMatthias Sperber, Graham Neubig, Ngoc-Quan Pham, and Alex Waibel. 2019. Self-Attentional Models for Lattice\nInputs. In ACL, pages 1185‚Äì1197.\nJinsong Su, Zhixing Tan, Deyi Xiong, Rongrong Ji, Xiaodong Shi, and Yang Liu. 2017. Lattice-based recurrent\nneural network encoders for neural machine translation. In AAAI, pages 3302‚Äì3308.\nDianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, and Shengping Liu. 2019. Leverage lexical knowledge for Chinese\nnamed entity recognition via collaborative graph network. In EMNLP, pages 3821‚Äì3831.\nZhixing Tan, Jinsong Su, Boli Wang, Yidong Chen, and Xiaodong Shi. 2018. Lattice-to-sequence attentional\nneural machine translation models. In Neurocomputing, pages 138‚Äì147.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is All you Need. In NIPS, pages 5998‚Äì6008.\nMengqiu Wang, Wanxiang Che, and Christopher D. Manning. 2013. Effective Bilingual Constraints for Semi-\nsupervised Learning of Named Entity Recognizers. In AAAI, pages 919‚Äì925.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning\ndeep transformer models for machine translation. In ACL, pages 1810‚Äì1822.\nFangzhao Wu, Junxin Liu, Chuhan Wu, Yongfeng Huang, and Xing Xie. 2019. Neural Chinese Named Entity\nRecognition via CNN-LSTM-CRF and Joint Training with Word Segmentation. In WWW, pages 3342‚Äì3348.\nFengshun Xiao, Jiangtong Li, Hai Zhao, Rui Wang, and Kehai Chen. 2019. Lattice-Based Transformer Encoder\nfor Neural Machine Translation. In ACL, pages 3090‚Äì3097.\nMengge Xue, Weiming Cai, Jinsong Su, Linfeng Song, Yubin Ge, Yubao Liu, and Bin Wang. 2019. Neural\nCollective Entity Linking Based on Recurrent Random Walk Network Learning. In IJCAI, pages 5327‚Äì5333.\nJie Yang, Zhiyang Teng, Meishan Zhang, and Yue Zhang. 2016. Combining Discrete and Neural Features for\nSequence Labeling. In CICLing, pages 919‚Äì925.\nBaosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, and Tong Zhang. 2018. Modeling\nLocalness for Self-Attention Networks. In EMNLP, pages 4449‚Äì4458.\nBaosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and Zhaopeng Tu. 2019. Convolutional Self-\nAttention Networks. In NAACL, pages 4040‚Äì4045.\nBowen Yu, Zhenyu Zhang, Tingwen Liu, Bin Wang, Sujian Li, and Quangang Li. 2019. Beyond Word Attention:\nUsing Segment Attention in Neural Relation Extraction. In IJCAI, pages 5401‚Äì5407.\nYue Zhang and Jie Yang. 2018. Chinese NER Using Lattice LSTM. In ACL, pages 1554‚Äì1564.\nPei Zhang, Niyu Ge, Boxing Chen, and Kai Fan. 2019. Lattice Transformer for Speech Translation. InACL, pages\n6475‚Äì6484.\nJ. Zhou, W. Qu, and F. Zhang. 2013. Chinese Named Entity Recognition via Joint IdentiÔ¨Åcation and Categoriza-\ntion. Chinese Journal of Electronics, pages 225‚Äì230.\nYuying Zhu and Guoxin Wang. 2019. CAN-NER: Convolutional Attention Network for Chinese Named Entity\nRecognition. In NAACL, pages 3384‚Äì3393.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7497292757034302
    },
    {
      "name": "Encoder",
      "score": 0.7137021422386169
    },
    {
      "name": "Computation",
      "score": 0.6806979179382324
    },
    {
      "name": "Exploit",
      "score": 0.6742928624153137
    },
    {
      "name": "Lattice (music)",
      "score": 0.539781928062439
    },
    {
      "name": "Transformer",
      "score": 0.5343899726867676
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5035991072654724
    },
    {
      "name": "Named-entity recognition",
      "score": 0.4775320291519165
    },
    {
      "name": "Autoencoder",
      "score": 0.44876644015312195
    },
    {
      "name": "Theoretical computer science",
      "score": 0.40428465604782104
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39912575483322144
    },
    {
      "name": "Algorithm",
      "score": 0.3643028140068054
    },
    {
      "name": "Parallel computing",
      "score": 0.33620762825012207
    },
    {
      "name": "Artificial neural network",
      "score": 0.2899778485298157
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 14
}