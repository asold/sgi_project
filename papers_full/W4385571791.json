{
  "title": "Analyzing Transformers in Embedding Space",
  "url": "https://openalex.org/W4385571791",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3168733083",
      "name": "Guy Dar",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2736090994",
      "name": "Mor Geva",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2099891597",
      "name": "Ankit Gupta",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A86633810",
      "name": "Jonathan Berant",
      "affiliations": [
        "Tel Aviv University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2515741950",
    "https://openalex.org/W3166956420",
    "https://openalex.org/W632330365",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W6764724796",
    "https://openalex.org/W6794025307",
    "https://openalex.org/W2905381038",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3091782154",
    "https://openalex.org/W2971569798",
    "https://openalex.org/W2907252220",
    "https://openalex.org/W3117576675",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W1912570122",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6676984168",
    "https://openalex.org/W2514713644",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W3195709780",
    "https://openalex.org/W3174617925",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2996657533",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6811340617",
    "https://openalex.org/W3104350794",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W4231799185",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W1974511160",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4294955582",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4385572928",
    "https://openalex.org/W4286892891",
    "https://openalex.org/W4292692470",
    "https://openalex.org/W4287110638",
    "https://openalex.org/W4288289156",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2966610483",
    "https://openalex.org/W4385573190",
    "https://openalex.org/W2988217457"
  ],
  "abstract": "Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity. First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by “translating” the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained. Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 16124–16170\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAnalyzing Transformers in Embedding Space\nGuy Dar1 Mor Geva2 Ankit Gupta1 Jonathan Berant1\n1The Blavatnik School of Computer Science, Tel-Aviv University\n2Allen Institute for Artificial Intelligence\n{guy.dar,joberant}@cs.tau.ac.il, morp@allenai.org,\nankitgupta.iitkanpur@gmail.com\nAbstract\nUnderstanding Transformer-based models has\nattracted significant attention, as they lie at the\nheart of recent technological advances across\nmachine learning. While most interpretabil-\nity methods rely on running models over in-\nputs, recent work has shown that an input-\nindependent approach, where parameters are\ninterpreted directly without a forward/back-\nward pass is feasible forsome Transformer pa-\nrameters, and for two-layer attention networks.\nIn this work, we present a conceptual frame-\nwork where all parameters of a trained Trans-\nformer are interpreted by projecting them into\nthe embedding space , that is, the space of\nvocabulary items they operate on. Focusing\nmostly on GPT-2 for this paper, we provide di-\nverse evidence to support our argument. First,\nan empirical analysis showing that parameters\nof both pretrained and fine-tuned models can\nbe interpreted in embedding space. Second,\nwe present two applications of our framework:\n(a) aligning the parameters of different mod-\nels that share a vocabulary, and (b) construct-\ning a classifier without training by “translat-\ning” the parameters of a fine-tuned classifier to\nparameters of a different model that was only\npretrained. Overall, our findings show that\nat least in part, we can abstract away model\nspecifics and understand Transformers in the\nembedding space.\n1 Introduction\nTransformer-based models [Vaswani et al., 2017] cur-\nrently dominate Natural Language Processing [Devlin\net al., 2018; Radford et al., 2019; Zhang et al., 2022] as\nwell as many other fields of machine learning [Doso-\nvitskiy et al., 2020; Chen et al., 2020; Baevski et al.,\n2020]. Consequently, understanding their inner work-\nings has been a topic of great interest. Typically, work\non interpreting Transformers relies on feeding inputs\nto the model and analyzing the resulting activations\n[Adi et al., 2016; Shi et al., 2016; Clark et al., 2019].\nThus, interpretation involves an expensive forward, and\nsometimes also a backward pass, over multiple inputs.\nMoreover, such interpretation methods are conditioned\non the input and are not guaranteed to generalize to all\ninputs. In the evolving literature on static interpreta-\ntion, i.e., without forward or backward passes, [Geva\net al., 2022b] showed that the value vectors of the\nTransformer feed-forward module (the second layer of\nthe feed-forward network) can be interpreted by pro-\njecting them into the embedding space, i.e., multiply-\ning them by the embedding matrix to obtain a repre-\nsentation over vocabulary items.1 [Elhage et al., 2021]\nhave shown that in a 2-layer attention network, weight\nmatrices can be interpreted in the embedding space as\nwell. Unfortunately, their innovative technique could\nnot be extended any further.\nIn this work, we extend and unify the theory and\nfindings of [Elhage et al., 2021] and [Geva et al.,\n2022b]. We present a zero-pass, input-independent\nframework to understand the behavior of Transformers.\nConcretely, we interpretall weights of a pretrained lan-\nguage model (LM) in embedding space, including both\nkeys and values of the feed-forward module ([Geva\net al., 2020, 2022b] considered just FF values) as well\nas all attention parameters ([Elhage et al., 2021] ana-\nlyzed simplified architectures up to two layers of atten-\ntion with no MLPs).\nOur framework relies on a simple observation. Since\n[Geva et al., 2022b] have shown that one can project\nhidden states to the embedding space via the embed-\nding matrix, we intuit this can be extended to other\nparts of the model by projecting to the embedding\nspace and then projecting back by multiplying with a\nright-inverse of the embedding matrix. Thus, we can\nrecast inner products in the model as inner products\nin embedding space. Viewing inner products this way,\nwe can interpret such products as interactions between\npairs of vocabulary items. This applies to (a) interac-\ntions between attention queries and keys as well as to\n(b) interactions between attention value vectors and the\nparameters that project them at the output of the atten-\ntion module. Taking this perspective to the extreme,\none can view Transformers as operating implicitly in\nthe embedding space. This entails the existence of a\nsingle linear space that depends only on the tokenizer,\n1We refer to the unique items of the vocabulary as vo-\ncabulary items, and to the (possibly duplicate) elements of a\ntokenized input as tokens. When clear, we might use the term\ntoken for vocabulary item.\n16124\nEA\nEB\nLayer 18 Head 1\n(’women’, ’ Marie’)\n(’ actresses’, ’ Marie’) \n(’women’, ’ Anne’)\n(’Women’, ’ Anne’) \n(’woman’, ’ Marie’) \n(’Women’, ’ Marie’)\nW\ni\nModel B\nL1       L2        L3        L4       L5\nL1  L2    L3    L4    L5\nVO\nEE\nWQK\nEE\nT\nT\nE\nV\nE\nK\nEmbedding Space Compute dot-product in embedding space between all  FF \nkey pairs* from all layers\nK\njK\nParameter Interpretation Parameter Alignment Zero-shot Stitching\ni\nj\nModel A\nPack them into a similarity matrix\nAverage absolute similarity over layer pairs\nL1       L2       L3        L4       L5\n      Matching \n         layers\ntop k\ntop k\ntop k\nLayer 10 Dim 8\nTai \n#jin \nJin \nMakoto \n#etsu\nShin \nHai\ntop k\nLayer 21 Dim 400\n#July\nJuly\n#February\n#January\n#Feb\nNovember\n#October\nLayer 17 Head 6\n(’ legally’, ’ legal’)\n(’ legal’, ’ sentencing’) \n(’ legal’, ’ arbitration’)\n(’ boycot’, ’ boycott’) \n(’ legal’, ’ criminal’)\n(’ legal’, ’ Judicial’)\n(’ legal’, ’ rulings’)\nEA EB\n-1\nModel A   \nModel B  \nModel B  \nModel A   \nQ1: How to transition between \ndifferent representation  spaces?\nEA EB\n-1\nQ2: How can we transfer \nknowledge between \nmodels?\nrepresentations can be \ntranslated (to some degree) \nthus:\nEmbedding Space\nStitching \nLayer\nStitching Layer\nModel B\nFinetuned\nModel A\nPretrained\nEB-1EA\nEmbedding space\n* other parameters work too Stitched model\nEmbedding Space\nFeature space A Feature space B\nL1   L2    L3    L4   L5\nFigure 1: Applications of the embedding space view. Left: interpreting parameters in embedding space. The most active\nvocabulary items in a feed-forward key ( k) and a feed-forward value ( v). The most active pairs of vocabulary items in an\nattention query-key matrix WQK and an attention value-output matrix WVO (see §2). Center: Aligning the parameters of\ndifferent BERT instances that share a vocabulary. Right: Zero-shot “stitching”, where representations of a fine-tuned classifier\nare translated through the embedding space (multiplying by EAE−1\nB ) to a pretrained-only model.\nin which parameters of different Transformers can be\ncompared. Thus, one can use the embedding space to\ncompare and transfer information across different mod-\nels that share a tokenizer.\nWe provide extensive empirical evidence for the va-\nlidity of our framework, focusing mainly on GPT-2\nmedium [Radford et al., 2019]. We use GPT-2 for two\nreasons. First, we do this for concreteness, as this pa-\nper is mainly focused on introducing the new frame-\nwork and not on analyzing its predictions. Second, and\nmore crucially, unlike many other architectures (such\nas BERT [Devlin et al., 2018], RoBERTa [Liu et al.,\n2019], and T5 [Raffel et al., 2019]), the GPT family has\na linear language modeling head (LM head) – which is\nsimply the output embedding matrix. All the other ar-\nchitectures’ LM heads are two layer networks that con-\ntain non-linearities before the output embedding ma-\ntrix. Our framework requires a linear language model-\ning head to work. That being said, we believe in prac-\ntice this will not be a major obstacle, and we indeed see\nin the experiments that model alignment works well for\nBERT in spite of the theoretical difficulties. We leave\nthe non-linearities in the LM head for future work.\nOn the interpretation front (Fig. 1, Left), we provide\nqualitative and quantitative evidence that Transformer\nparameters can be interpreted in embedding space. We\nalso show that when fine-tuning GPT-2 on a sentiment\nanalysis task (over movie reviews), projecting changes\nin parameters into embedding space yields words that\ncharacterize sentiment towards movies. Second (Fig. 1,\nCenter), we show that given two distinct instances of\nBERT pretrained from different random seeds [Sellam\net al., 2022], we can align layers of the two instances\nby casting their weights into the embedding space. We\nfind that indeed layer i of the first instance aligns well\nto layer i of the second instance, showing the different\nBERT instances converge to a semantically similar so-\nlution. Last (Fig. 1, Right), we take a model fine-tuned\non a sentiment analysis task and “transfer” the learned\nweights to a different model that was only pretrained by\ngoing through the embedding spaces of the two mod-\nels. We show that in 30% of the cases, this procedure,\ntermed stitching, results in a classifier that reaches an\nimpressive accuracy of 70% on the IMDB benchmark\n[Maas et al., 2011] without any training.\nOverall, our findings suggest that analyzing Trans-\nformers in embedding space is valuable both as an\ninterpretability tool and as a way to relate differ-\nent models that share a vocabulary and that it opens\nthe door to interpretation methods that operate in\nembedding space only. Our code is available at\nhttps://github.com/guyd1995/embedding-space.\n2 Background\nWe now present the main components of the Trans-\nformer [Vaswani et al., 2017] relevant to our analysis.\nWe discuss the residual stream view of Transformers,\nand recapitulate a view of the attention layer param-\neters as interaction matrices WVO and WQK [Elhage\net al., 2021]. Similar to them, we exclude biases and\nlayer normalization from our analysis.\n2.1 Transformer Architecture\nThe Transformer consists of a stack of layers, each\nincludes an attention module followed by a Feed-\nForward (FF) module. All inputs and outputs are se-\nquences of N vectors of dimensionality d.\n16125\nAttention Module takes as input a sequence of rep-\nresentations X ∈RN×d, and each layer Lis parame-\nterized by four matrices W(L)\nQ ,W(L)\nK ,W(L)\nV ,W(L)\nO ∈\nRd×d (we henceforth omit the layer superscript for\nbrevity). The input X is projected to produce queries,\nkeys, and values: Qatt = XWQ,Katt = XWK,Vatt =\nXWV . Each one of Qatt,Katt,Vatt is split along\nthe columns to H different heads of dimensionality\nRN×d\nH , denoted by Qi\natt,Ki\natt,V i\natt respectively. We\nthen compute H attention maps:\nAi = softmax\n(\nQi\nattKiT\natt√\nd/H\n+ M\n)\n∈RN×N,\nwhere M ∈RN×N is the attention mask. Each atten-\ntion map is applied to the corresponding value head as\nAiVi\natt, results are concatenated along columns and pro-\njected via WO. The input to the module is added via\na residual connection, and thus the attention module’s\noutput is:\nX+ Concat\n[\nA1V1\natt,...,A iVi\natt,...,A HVH\natt\n]\nWO.\n(1)\nFF Module is a two-layer neural network, applied\nto each position independently. Following past termi-\nnology [Sukhbaatar et al., 2019; Geva et al., 2020],\nweights of the first layer are calledFF keys and weights\nof the second layer FF values . This is an analogy\nto attention, as the FF module too can be expressed\nas: f(QKT)V, where f is the activation function,\nQ ∈RN×d is the output of the attention module and\nthe input to the FF module, and K,V ∈Rdff×d are the\nweights of the first and second layers of the FF module.\nUnlike attention, keys and values are learnable parame-\nters. The output of the FF module is added to the output\nof the attention module to form the output of the layer\nvia a residual connection. The output of the i-th layer\nis called the i-th hidden state.\nEmbedding Matrix To process sequences of dis-\ncrete tokens, Transformers use an embedding matrix\nE ∈Rd×e that provides a d-dimensional representa-\ntion to vocabulary items before entering thefirst Trans-\nformer layer. In different architectures, including GPT-\n2, the same embedding matrix E is often used [Press\nand Wolf, 2016] to take the output of the last Trans-\nformer layer and project it back to the vocabulary di-\nmension, i.e., into the embedding space. In this work,\nwe show how to interpret all the components of the\nTransformer model in the embedding space.\n2.2 The Residual Stream\nWe rely on a useful view of the Transformer through\nits residual connections popularized by [Elhage et al.,\n2021].2 Specifically, each layer takes a hidden state as\n2Originally introduced in [nostalgebraist, 2020].\ninput and adds information to the hidden state through\nits residual connection. Under this view, the hidden\nstate is a residual stream passed along the layers, from\nwhich information is read, and to which information is\nwritten at each layer. [Elhage et al., 2021] and [Geva\net al., 2022b] observed that the residual stream is of-\nten barely updated in the last layers, and thus the final\nprediction is determined in early layers and the hidden\nstate is mostly passed through the later layers.\nAn exciting consequence of the residual stream view\nis that we can project hidden states in every layer into\nembedding space by multiplying the hidden state with\nthe embedding matrix E, treating the hidden state as\nif it were the output of the last layer. [Geva et al.,\n2022a] used this approach to interpret the prediction\nof Transformer-based language models, and we follow\na similar approach.\n2.3 WQK and WVO\nFollowing [Elhage et al., 2021], we describe the at-\ntention module in terms of interaction matrices WQK\nand WVO which will be later used in our mathemati-\ncal derivation. The computation of the attention mod-\nule (§2.1) can be re-interpreted as follows. The at-\ntention projection matrices WQ,WK,WV can be split\nalong the column axis to H equal parts denoted by\nWi\nQ,Wi\nK,Wi\nV ∈ Rd×d\nH for 1 ≤ i ≤ H. Similarly,\nthe attention output matrix WO can be split along the\nrow axis into H heads, Wi\nO ∈R\nd\nH ×d. We define the\ninteraction matrices as\nWi\nQK := Wi\nQWiT\nK ∈Rd×d,\nWi\nVO := Wi\nVWi\nO ∈Rd×d.\nImportantly, Wi\nQK,Wi\nVO are input-independent. Intu-\nitively, WQK encodes the amount of attention between\npairs of tokens. Similarly, in Wi\nVO, the matrices WV\nand WO can be viewed as a transition matrix that deter-\nmines how attending to certain tokens affects the sub-\nsequent hidden state.\nWe can restate the attention equations in terms of the\ninteraction matrices. Recall (Eq. 1) that the output of\nthe i’th head of the attention module isAiVi\natt and the fi-\nnal output of the attention module is (without the resid-\nual connection):\nConcat\n[\nA1V1\natt,...,A iVi\natt,...,A HVH\natt\n]\nWO = (2)\nH∑\ni=1\nAi(XWi\nV)Wi\nO =\nH∑\ni=1\nAiXWi\nVO.\nSimilarly, the attention mapAi at the i’th head in terms\nof WQK is (softmax is done row-wise):\nAi = softmax\n(\n(XWi\nQ)(XWi\nK)T\n√\nd/H\n+ M\n)\n(3)\n= softmax\n(\nX(Wi\nQK)XT\n√\nd/H\n+ M\n)\n.\n16126\n3 Parameter Projection\nIn this section, we propose that Transformer parameters\ncan be projected into embedding space for interpreta-\ntion purposes. We empirically support our framework’s\npredictions in §4-§5.\nGiven a matrix A ∈RN×d, we can project it into\nembedding space by multiplying by the embedding ma-\ntrix E as ˆA= AE ∈RN×e. Let E′be a right-inverse\nof E, that is, EE′ = I ∈ Rd×d.3 We can recon-\nstruct the original matrix with E′as A = A(EE′) =\nˆAE′. We will use this simple identity to reinterpret\nthe model’s operation in embedding space. To simplify\nour analysis we ignore LayerNorm and biases. This\nhas been justified in prior work [Elhage et al., 2021].\nBriefly, LayerNorm can be ignored because normaliza-\ntion changes only magnitudes and not the direction of\nthe update. At the end of this section, we discuss why\nin practice we choose to use E′ = ET instead of a\nseemingly more appropriate right inverse, such as the\npseudo-inverse [Moore, 1920; Bjerhammar, 1951; Pen-\nrose, 1955]. In this section, we derive our framework\nand summarize its predictions in Table 1.\nAttention Module Recall that Wi\nVO := Wi\nVWi\nO ∈\nRd×d is the interaction matrix between attention values\nand the output projection matrix for attention head i.\nBy definition, the output of each head is: AiXWi\nVO =\nAi ˆXE′Wi\nVO. Since the output of the attention module\nis added to the residual stream, we can assume accord-\ning to the residual stream view that it is meaningful to\nproject it to the embedding space, similar to FF val-\nues. Thus, we expect the sequence of N e-dimensional\nvectors (AiXWi\nVO)E = Ai ˆX(E′Wi\nVOE) to be inter-\npretable. Importantly, the role of Ai is just to mix the\nrepresentations of the updated N input vectors. This\nis similar to the FF module, where FF values (the pa-\nrameters of the second layer) are projected into embed-\nding space, and FF keys (parameters of the first layer)\ndetermine the coefficients for mixing them. Hence, we\ncan assume that the interpretable components are in the\nterm ˆX(E′Wi\nVOE).\nZooming in on this operation, we see that it takes the\nprevious hidden state in the embedding space ( ˆX) and\nproduces an output in the embedding space which will\nbe incorporated into the next hidden state through the\nresidual stream. Thus, E′Wi\nVOE is a transition matrix\nthat takes a representation of the embedding space and\noutputs a new representation in the same space.\nSimilarly, the matrix Wi\nQK can be viewed as a bilin-\near map (Eq. 2.3). To interpret it in embedding space,\nwe perform the following operation with E′:\nXWi\nQKXT = (XEE′)Wi\nQK(XEE′)T =\n(XE)E′Wi\nQKE′T(XE)T = ˆX(E′Wi\nQKE′T) ˆXT.\nTherefore, the interaction between tokens at different\npositions is determined by ane×ematrix that expresses\n3E′exists if d≤eand Eis full-rank.\nthe interaction between pairs of vocabulary items.\nFF Module [Geva et al., 2022b] showed that FF\nvalue vectors V ∈ Rdff×d are meaningful when pro-\njected into embedding space, i.e., for a FF value vector\nv∈Rd, vE ∈Re is interpretable (see §2.1). In vector-\nized form, the rows of VE ∈Rdff×e are interpretable.\nOn the other hand, the keys Kof the FF layer are mul-\ntiplied on the left by the output of the attention module,\nwhich are the queries of the FF layer. Denoting the\noutput of the attention module by Q, we can write this\nproduct as QKT = ˆQE′KT = ˆQ(KE′T)T. Because Q\nis a hidden state, we assume according to the residual\nstream view that ˆQis interpretable in embedding space.\nWhen multiplying ˆQby KE′T, we are capturing the in-\nteraction in embedding space between each query and\nkey, and thus expect KE′T to be interpretable in em-\nbedding space as well.\nOverall, FF keys and values are intimately connected\n– the i-th key controls the coefficient of the i-th value,\nso we expect their interpretation to be related. While\nnot central to this work, we empirically show that key-\nvalue pairs in the FF module are similar in embedding\nspace in Appendix B.1.\nSubheads Another way to interpret the matrices\nWi\nVO and Wi\nQK is through the subhead view. We use\nthe following identity: AB = ∑b\nj=1 A:,jBj,:, which\nholds for arbitrary matrices A ∈ Ra×b,B ∈ Rb×c,\nwhere A:,j ∈Ra×1 are the columns of the matrix A\nand Bj,: ∈R1×c are the rows of the matrix B. Thus,\nwe can decompose Wi\nVO and Wi\nQK into a sum of d\nH\nrank-1 matrices:\nWi\nVO =\nd\nH∑\nj=1\nWi,j\nV Wi,j\nO , W i\nQK =\nd\nH∑\nj=1\nWi,j\nQ Wi,j\nK\nT\n.\nwhere Wi,j\nQ ,Wi,j\nK ,Wi,j\nV ∈ Rd×1 are columns of\nWi\nQ,Wi\nK,Wi\nV respectively, and Wi,j\nO ∈R1×d are the\nrows of Wi\nO. We call these vectors subheads. This\nview is useful since it allows us to interpret subheads\ndirectly by multiplying them with the embedding ma-\ntrix E. Moreover, it shows a parallel between interac-\ntion matrices in the attention module and the FF mod-\nule. Just like the FF module includes key-value pairs as\ndescribed above, for a given head, its interaction matri-\nces are a sum of interactions between pairs of subheads\n(indexed by j), which are likely to be related in embed-\nding space. We show this is indeed empirically the case\nfor pairs of subheads in Appendix B.1.\nChoosing E′ = ET In practice, we do not use an\nexact right inverse (e.g. the pseudo-inverse). We use\nthe transpose of the embedding matrix E′ = ET in-\nstead. The reason pseudo-inverse doesn’t work is that\nfor interpretation we apply a top-koperation after pro-\njecting to embedding space (since it is impractical for\nhumans to read through a sorted list of 50K tokens).\nSo, we only keep the list of the vocabulary items that\nhave the k largest logits, for manageable values of k.\n16127\nSymbol Projection Approximate Projection\nFF values v vE vE\nFF keys k kE ′T kE\nAttention query-key Wi\nQK E′Wi\nQKE′T ETWi\nQKE\nAttention value-output Wi\nVO E′Wi\nVOE E TWi\nVOE\nAttention value subheads Wi,j\nV Wi,j\nV E′T Wi,j\nV E\nAttention output subheads Wi,j\nO Wi,j\nO E W i,j\nO E\nAttention query subheads Wi,j\nQ Wi,j\nQ E′T Wi,j\nQ E\nAttention key subheads Wi,j\nK Wi,j\nK E′T Wi,j\nK E\nTable 1: A summary of our approach for projecting Transformer components into embedding space. The ‘Approximate Projec-\ntion’ shows the projection we use in practice where E′= ET.\nIn Appendix A, we explore the exact requirements for\nE′ to interact well with top- k. We show that the top\nkentries of a vector projected with the pseudo-inverse\ndo not represent the entire vector well in embedding\nspace. We define keep-krobust invertibility to quantify\nthis. It turns out that empirically ET is a decent keep-k\nrobust inversefor Ein the case of GPT-2 medium (and\nsimilar models) for plausible values of k. We refer the\nreader to Appendix A for details.\nTo give intuition as to whyET works in practice, we\nswitch to a different perspective, useful in its own right.\nConsider the FF keys for example – they are multiplied\non the left by the hidden states. In this section, we sug-\ngested to re-cast this as hT K = (hT E)(E′K). Our\njustification was that the hidden state is interpretable\nin the embedding space. A related perspective (domi-\nnant in previous works too; e.g. [Mickus et al., 2022])\nis thinking of the hidden state as an aggregation of\ninterpretable updates to the residual stream. That is,\nschematically, h = ∑k\ni=1 αiri, where αi are scalars\nand ri are vectors corresponding to specific concepts in\nthe embedding space (we roughly think of a concept as\na list of tokens related to a single topic). Inner product\nis often used as a similarity metric between two vec-\ntors. If the similarity between a column Ki and h is\nlarge, the corresponding i-th output coordinate will be\nlarge. Then we can think of K as a detector of con-\ncepts where each neuron (column in K) lights up if a\ncertain concept is “present” (or a superposition of con-\ncepts) in the inner state. To understand which concepts\neach detector column encodes we see which tokens it\nresponds to. Doing this for all (input) token embed-\ndings and packaging the inner products into a vector\nof scores is equivalent to simply multiplying by ET on\nthe left (where E is the input embedding in this case,\nbut for GPT-2 they are the same). A similar argument\ncan be made for the interaction matrices as well. For\nexample for WVO, to understand if a token embedding\nei maps to a ej under a certain head, we apply the ma-\ntrix to ei, getting eT\ni WVO and use the inner product as\na similarity metric and get the score eT\ni WVOej.\n4 Interpretability Experiments\nIn this section, we provide empirical evidence for the\nviability of our approach as a tool for interpreting\nTransformer parameters. For our experiments, we use\nHuggingface Transformers ([Wolf et al., 2020]; Li-\ncense: Apache-2.0).\n4.1 Parameter Interpretation Examples\nAttention Module We take GPT-2 medium (345M pa-\nrameters; [Radford et al., 2019]) and manually analyze\nits parameters. GPT-2 medium has a total of 384 at-\ntention heads (24 layers and 16 heads per layer). We\ntake the embedded transition matrices E′Wi\nVOEfor all\nheads and examine the top-kpairs of vocabulary items.\nAs there are only 384 heads, we manually choose a\nfew heads and present the top-kpairs in Appendix C.1\n(k = 50). We observe that different heads capture dif-\nferent types of relations between pairs of vocabulary\nitems including word parts, heads that focus on gen-\nder, geography, orthography, particular part-of-speech\ntags, and various semantic topics. In Appendix C.2 we\nperform a similar analysis for WQK. We supplement\nthis analysis with a few examples from GPT-2 base and\nlarge (117M, 762M parameters – respectively) as proof\nof concept, similarly presenting interpretable patterns.\nA technical note: WVO operates on row vectors,\nwhich means it operates in a “transposed” way to stan-\ndard intuition – which places inputs on the left side and\noutputs on the right side. It does not affect the theory,\nbut when visualizing the top-ktuples, we take the trans-\npose of the projection (E′Wi\nVOE)T to get the “natural”\nformat (input token, output token). With-\nout the transpose, we would get the same tuples, but\nin the format (output token, input token).\nEquivalently, in the terminology of linear algebra, it\ncan be seen as a linear transformation that we repre-\nsent in the basis of row vectors and we transform to the\nbasis of column vectors, which is the standard one.\nFF Module Appendix C.3 provides examples\nof key-value pairs from the FF modules of GPT-2\nmedium. We show random pairs (k,v) from the set of\nthose pairs such that when looking at the top-100 vo-\ncabulary items for kand v, at least 15% overlap. Such\npairs account for approximately 5% of all key-value\npairs. The examples show how key-value pairs often\nrevolve around similar topics such as media, months,\norgans, etc. We again include additional examples from\nGPT-2 base and large.\nKnowledge Lookup Last, we show we can use em-\nbeddings to locate FF values (or keys) related to a par-\n16128\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nK\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nV\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nWV\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nWO\naligned\nrandom\nFigure 2: Left: Average Rk score (k = 100) across tokens per layer for activated parameter vectors against both the aligned\nhidden state ˆhat the output of the layer and a randomly sampled hidden stateˆhrand. Parameters are FF keys (top-left), FF values\n(top-right), attention values (bottom-left), and attention outputs (bottom-right).\nticular topic. We take a few vocabulary items related to\na certain topic, e.g., [‘cm’, ‘kg’, ‘inches’],\naverage their embeddings,4 and rank all FF values (or\nkeys) based on their dot-product with the average. Ap-\npendix C.4 shows a few examples of FF values found\nwith this method that are related to programming, mea-\nsurements, and animals.\n4.2 Hidden State and Parameters\nOne merit of zero-pass interpretation is that it does not\nrequire running inputs through the model. Feeding in-\nputs might be expensive and non-exhaustive. In this\nsection and in this section only, we run a forward pass\nover inputs and examine if the embedding space repre-\nsentations of dynamically computed hidden states are\n“similar” to the representations of the activated static\nparameter vectors. Due to the small number of exam-\nples we run over, the overall GPU usage is still negligi-\nble.\nA technical side note: we use GPT-2, which applies\nLayerNorm to the Transformer output before project-\ning it to the embedding space with E. Thus, conser-\nvatively, LayerNorm should be considered as part of\nthe projection operation. Empirically, however, we ob-\nserve that projecting parameters directly without Lay-\nerNorm works well, which simplifies our analysis in\n§3. Unlike parameters, we apply LayerNorm to hidden\nstates before projection to embedding space to improve\ninterpretability. This nuance was also present in the\n4We subtract the average embeddingµfrom Ebefore av-\neraging, which improves interpretability.\ncode of [Geva et al., 2022a].\nExperimental Design We use GPT-2 medium and\nrun it over 60 examples from IMDB (25,000 train,\n25,000 test examples; [Maas et al., 2011]). 5 This pro-\nvides us with a dynamically-computed hidden state h\nfor every token and at the output of every layer. For\nthe projection ˆh ∈Re of each such hidden state, we\ntake the projections of the m most active parameter\nvectors {ˆxi}m\ni=1 in the layer that computedhand check\nif they cover the dominant vocabulary items ofˆhin em-\nbedding space. Specifically, let top-k(wE) be the k\nvocabulary items with the largest logits in embedding\nspace for a vector w∈Rd. We compute:\nRk(ˆx1,..., ˆxm,ˆh) =|top-k(ˆh) ∩⋃m\ni=1 top-k(ˆxi)|\nk ,\nto capture if activated parameter vectors cover the main\nvocabulary items corresponding to the hidden state.\nWe find the m most active parameter vectors sepa-\nrately for FF keys ( K), FF values ( V), attention value\nsubheads (WV) (see §3), and attention output subheads\n(WO), where the activation of each parameter vector\nis determined by the vector’s “coefficient” as follows.\nFor a FF key-value pair(k,v) the coefficient is σ(qTk),\nwhere q ∈Rd is an input to the FF module, and σ is\nthe FF non-linearity. For attention, value-output sub-\nhead pairs (v,o) the coefficient is xTv, where xis the\n5Note that IMDB was designed for sentiment analysis and\nwe use it here as a general-purpose corpus.\n16129\ninput to this component (for attention head i, the input\nis one of the rows of AiX, see Eq. 3).\nResults and Discussion Figure 2 presents the Rk\nscore averaged across tokens per layer. As a baseline,\nwe compare Rk of the activated vectors {ˆxi}m\ni=1 of the\ncorrectly-aligned hidden state ˆhat the output of the rel-\nevant layer (blue bars) against the Rk when randomly\nsampling ˆhrand from all the hidden states (orange bars).\nWe conclude that representations in embedding space\ninduced by activated parameter vector mirror, at least\nto some extent, the representations of the hidden states\nthemselves. Appendix §B.2 shows a variant of this\nexperiment, where we compare activated parameters\nthroughout GPT-2 medium’s layers to the last hidden\nstate, which produces the logits used for prediction.\n4.3 Interpretation of Fine-tuned Models\nWe now show that we can interpret thechanges a model\ngoes through during fine-tuning through the lens of em-\nbedding space. We fine-tune the top-3 layers of the 12-\nlayer GPT-2 base (117M parameters) with a sequence\nclassification head on IMDB sentiment analysis (binary\nclassification) and compute the difference between the\noriginal parameters and the fine-tuned model. We then\nproject the difference of parameter vectors into embed-\nding space and test if the change is interpretable w.r.t.\nsentiment analysis.\nAppendix D shows examples of projected differ-\nences randomly sampled from the fine-tuned layers.\nFrequently, the difference or its negation is projected to\nnouns, adjectives, and adverbs that express sentiment\nfor a movie, such as ‘amazing’, ‘masterpiece’, ‘incom-\npetence’, etc. This shows that the differences are in-\ndeed projected into vocabulary items that characterize\nmovie reviews’ sentiments. This behavior is present\nacross WQ,WK,WV,K, but not V and WO, which cu-\nriously are the parameters added to the residual stream\nand not the ones that react to the input directly.\n5 Aligning Models in Embedding Space\nThe assumption Transformers operate in embedding\nspace leads to an exciting possibility – we can relate\ndifferent models to one another so long as they share\nthe vocabulary and tokenizer. In §5.1, we show that we\ncan align the layers of BERT models trained with dif-\nferent random seeds. In §5.2, we show the embedding\nspace can be leveraged to “stitch” the parameters of a\nfine-tuned model to a model that was not fine-tuned.\n5.1 Layer Alignment\nExperimental Design Taking our approach to the ex-\ntreme, the embedding space is a universal space, which\ndepends only on the tokenizer, in which Transformer\nparameters and hidden states reside. Thus, we can align\nparameter vectors from different models in this space\nand compare them even if they come from different\nmodels, as long as they share a vocabulary.\nTo demonstrate this, we use MultiBERTs ([Sellam\net al., 2022]; License: Apache-2.0), which contains 25\ndifferent instantiations of BERT-base (110M parame-\nters) initialized from different random seeds.6 We take\nparameters from two MultiBERT seeds and compute\nthe correlation between their projections to embedding\nspace. For example, let VA,VB be the FF values of\nmodels Aand B. We can project the values into em-\nbedding space: VAEA,VBEB, where EA,EB are the\nrespective embedding matrices, and compute Pearson\ncorrelation between projected values. This produces a\nsimilarity matrix ˜S ∈R|VA|×|VB|, where each entry\nis the correlation coefficient between projected values\nfrom the two models. We bin ˜Sby layer pairs and aver-\nage the absolute value of the scores in each bin (differ-\nent models might encode the same information in dif-\nferent directions, so we use absolute value) to produce\na matrix S∈ RL×L, where Lis the number of layers –\nthat is, the average (absolute) correlation between vec-\ntors that come from layer ℓA in model A and layer ℓB\nin Model B is registered in entry (ℓA,ℓB) of S.\nLast, to obtain a one-to-one layer alignment, we use\nthe Hungarian algorithm [Kuhn, 1955], which assigns\nexactly one layer from the first model to a layer from\nthe second model. The algorithm’s objective is to max-\nimize, given a similarity matrix S, the sum of scores of\nthe chosen pairs, such that each index in one model is\nmatched with exactly one index in the other. We repeat\nthis for all parameter groups (WQ,WK,WV,WO,K).\nResults and Discussion Figure 3 (left) shows the re-\nsulting alignment. Clearly, parameters from a certain\nlayer in model A tend to align to the same layer in\nmodel B across all parameter groups. This suggests\nthat different layers from different models that were\ntrained separately (but with the same training objective\nand data) serve a similar function. As further evidence,\nwe show that if not projected, the matching appears ab-\nsolutely random in Figure §3 (right). We show the same\nresults for other seed pairs as well in Appendix B.3.\n5.2 Zero-shot Stitching\nModel stitching [Lenc and Vedaldi, 2015; Csisz ´arik\net al., 2021; Bansal et al., 2021] is a relatively under-\nexplored feature of neural networks, particularly in\nNLP. The idea is that different models, even with differ-\nent architectures, can learn representations that can be\naligned through a linear transformation, termed stitch-\ning. Representations correspond to hidden states, and\nthus one can learn a transformation matrix from one\nmodel’s hidden states to an equivalent hidden state in\nthe other model. Here, we show that going through\nembedding space one can align the hidden states of two\nmodels, i.e., stitch, without training.\nGiven two models, we want to find a linear stitch-\ning transformation to align their representation spaces.\n6Estimated compute costs: around 1728 TPU-hours for\neach pre-training run, and around 208 GPU-hours plus 8\nTPU-hours for associated fine-tuning experiments.\n16130\nK V WK\nWQ WV WO\nK V WK\nWQ WV WO\nFigure 3: Left: Aligning in embedding space the layers of two different BERT models initialized from different random seeds\nfor all parameter groups. Layers that have the same index tend to align with one another. Right: Alignment in feature space\nleads to unintelligible patterns.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\nLayer\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70Accuracy\nFigure 4: Accuracy on the IMDB evaluation set. We ran\nstitching randomly 11 times and obtained 3 models with\nhigher than random accuracy when stitching over top layers.\nDashed red line indicates random performance.\nAccording to our theory, given a hidden state v ∈Rd1\nfrom model A, we can project it to the embedding space\nas vEA, where EA is its embedding matrix. Then, we\ncan re-project to the feature space of model B, with\nE+\nB ∈Re×d2 , where E+\nB is the Penrose-Moore pseudo-\ninverse of the embedding matrixEB.7 This transforma-\ntion can be expressed as multiplication with the kernel\nKAB := EAE+\nB ∈Rd1×d2 . We employ the above ap-\nproach to take representations of a fine-tuned classifier,\nA, and stitch them on top of a model B that was only\npretrained, to obtain a new classifier based on B.\nExperimental Design We use the 24-layer GPT-2\nmedium as model A and 12-layer GPT-2 base model\ntrained in §4.3 as model B. We fine-tune the last three\nlayers of model B on IMDB, as explained in §4.3.\nStitching is simple and is performed as follows. Given\nthe sequence of N hidden states Hℓ\nA ∈RN×d1 at the\noutput of layer ℓ of model A (ℓ is a hyperparameter),\nwe apply the stitching layer, which multiplies the hid-\nden states with the kernel, computing Hℓ\nAKAB. This\nresults in hidden states HB ∈RN×d2 , used as input to\nthe three fine-tuned layers from B.\nResults and Discussion Stitching produces models\nwith accuracies that are higher than random on IMDB\nevaluation set, but not consistently. Figure 4 shows\nthe accuracy of stitched models against the layer in-\ndex from model Aover which stitching is performed.\n7Since we are not interested in interpretation we use an\nexact right-inverse and not the transpose.\nOut of 11 random seeds, three models obtained accu-\nracy that is significantly higher than the baseline 50%\naccuracy, reaching an accuracy of roughly 70%, when\nstitching is done over the top layers.\n6 Related Work\nInterpreting Transformers is a broad area of research\nthat has attracted much attention in recent years. A\nlarge body of work has focused on analyzing hidden\nrepresentations, mostly through probing [Adi et al.,\n2016; Shi et al., 2016; Tenney et al., 2019; Rogers et al.,\n2020]. [V oita et al., 2019a] used statistical tools to an-\nalyze the evolution of hidden representations through-\nout layers. Recently, [Mickus et al., 2022] proposed\nto decompose the hidden representations into the con-\ntributions of different Transformer components. Un-\nlike these works, we interpret parameters rather than\nthe hidden representations.\nAnother substantial effort has been to interpret spe-\ncific network components. Previous work analyzed sin-\ngle neurons [Dalvi et al., 2018; Durrani et al., 2020],\nattention heads [Clark et al., 2019; V oita et al., 2019b],\nand feedforward values [Geva et al., 2020; Dai et al.,\n2021; Elhage et al., 2022]. While these works mostly\nrely on input-dependent neuron activations, we inspect\n“static” model parameters, and provide a comprehen-\nsive view of all Transformer components.\nOur work is most related to efforts to interpret spe-\ncific groups of Transformer parameters. [Cammarata\net al., 2020] made observations about the interpretabil-\nity of weights of neural networks. [Elhage et al., 2021]\nanalyzed 2-layer attention networks. We extend their\nanalysis to multi-layer pre-trained Transformer mod-\nels. [Geva et al., 2020, 2022a,b] interpreted feedfor-\nward values in embedding space. We coalesce these\nlines of work and offer a unified interpretation frame-\nwork for Transformers in embedding space.\n7 Discussion\nWhile our work has limitations (see §8), we think the\nbenefits of our work overshadow its limitations. We\nprovide a simple approach and a new set of tools to in-\nterpret Transformer models and compare them. The\nrealm of input-independent interpretation methods is\n16131\nstill nascent and it might provide a fresh perspective\non the internals of the Transformer, one that allows to\nglance intrinsic properties of specific parameters, dis-\nentangling their dependence on the input. Moreover,\nmany models are prohibitively large for practitioners to\nrun. Our method requires only a fraction of the com-\npute and memory requirements, and allows interpreting\na single parameter in isolation.\nImportantly, our framework allows us to view pa-\nrameters from different models as residents of a canon-\nical embedding space, where they can be compared in\nmodel-agnostic fashion. This has interesting implica-\ntions. We demonstrate two consequences of this obser-\nvation (model alignment and stitching) and argue future\nwork can yield many more use cases.\n8 Limitations\nOur work has a few limitations that we care to high-\nlight. First, it focuses on interpreting models through\nthe vocabulary lens. While we have shown evidence for\nthis, it does not preclude other factors from being in-\nvolved. Second, we used E′= ET, but future research\nmay find variants of Ethat improve performance. Ad-\nditionally, most of the work focused on GPT-2. This is\ndue to shortcomings in the current state of our frame-\nwork, as well as for clear presentation. We believe non-\nlinearities in language modeling are resolvable, as is\nindicated in the experiment with BERT.\nIn terms of potential bias in the framework, some\nparameters might consider terms related to each due to\nstereotypes learned from the corpus.\nReferences\nY . Adi, E. Kermany, Y . Belinkov, O. Lavi, and Y . Gold-\nberg. Fine-grained analysis of sentence embed-\ndings using auxiliary prediction tasks, 2016. URL\nhttps://arxiv.org/abs/1608.04207.\nA. Baevski, H. Zhou, A. Mohamed, and M. Auli.\nwav2vec 2.0: A framework for self-supervised learn-\ning of speech representations, 2020. URL https:\n//arxiv.org/abs/2006.11477.\nY . Bansal, P. Nakkiran, and B. Barak. Revisiting\nmodel stitching to compare neural representations.\nIn NeurIPS, 2021.\nA. Bjerhammar. Application of calculus of matrices\nto method of least squares : with special reference\nto geodetic calculations. In Trans. Roy. Inst. Tech.\nStockholm, 1951.\nN. Cammarata, S. Carter, G. Goh, C. Olah, M. Petrov,\nL. Schubert, C. V oss, B. Egan, and S. K. Lim.\nThread: Circuits. Distill, 2020. doi: 10.23915/\ndistill.00024. https://distill.pub/2020/circuits.\nM. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan,\nand I. Sutskever. Generative pretraining from pix-\nels. In H. D. III and A. Singh, editors, Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 1691–1703. PMLR,\n13–18 Jul 2020. URL https://proceedings.\nmlr.press/v119/chen20s.html.\nK. Clark, U. Khandelwal, O. Levy, and C. D. Man-\nning. What does BERT look at? an analysis of\nbert’s attention. CoRR, abs/1906.04341, 2019. URL\nhttp://arxiv.org/abs/1906.04341.\nA. Csisz ´arik, P. Kor ¨osi-Szab´o, ´A. K. Matszangosz,\nG. Papp, and D. Varga. Similarity and matching of\nneural network representations. In NeurIPS, 2021.\nD. Dai, L. Dong, Y . Hao, Z. Sui, B. Chang, and\nF. Wei. Knowledge neurons in pretrained transform-\ners, 2021. URL https://arxiv.org/abs/\n2104.08696.\nF. Dalvi, N. Durrani, H. Sajjad, Y . Belinkov, A. Bau,\nand J. Glass. What is one grain of sand in the\ndesert? analyzing individual neurons in deep nlp\nmodels, 2018. URL https://arxiv.org/\nabs/1812.09355.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.\nBert: Pre-training of deep bidirectional transformers\nfor language understanding, 2018. URL https:\n//arxiv.org/abs/1810.04805.\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and\nN. Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale, 2020. URL\nhttps://arxiv.org/abs/2010.11929.\nN. Durrani, H. Sajjad, F. Dalvi, and Y . Belinkov.\nAnalyzing individual neurons in pre-trained lan-\nguage models. CoRR, abs/2010.02695, 2020. URL\nhttps://arxiv.org/abs/2010.02695.\nN. Elhage, N. Nanda, C. Olsson, T. Henighan,\nN. Joseph, B. Mann, A. Askell, Y . Bai, A. Chen,\nT. Conerly, N. DasSarma, D. Drain, D. Gan-\nguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones,\nJ. Kernion, L. Lovitt, K. Ndousse, D. Amodei,\nT. Brown, J. Clark, J. Kaplan, S. McCan-\ndlish, and C. Olah. A mathematical frame-\nwork for transformer circuits, 2021. URL\nhttps://transformer-circuits.pub/\n2021/framework/index.html.\nN. Elhage, T. Hume, C. Olsson, N. Nanda,\nT. Henighan, S. Johnston, S. ElShowk, N. Joseph,\nN. DasSarma, B. Mann, D. Hernandez, A. Askell,\nK. Ndousse, A. Jones, D. Drain, A. Chen,\nY . Bai, D. Ganguli, L. Lovitt, Z. Hatfield-Dodds,\nJ. Kernion, T. Conerly, S. Kravec, S. Fort, S. Ka-\ndavath, J. Jacobson, E. Tran-Johnson, J. Kaplan,\nJ. Clark, T. Brown, S. McCandlish, D. Amodei,\nand C. Olah. Softmax linear units. Trans-\nformer Circuits Thread , 2022. https://transformer-\ncircuits.pub/2022/solu/index.html.\n16132\nK. Ethayarajh. How contextual are contextualized\nword representations? comparing the geometry of\nbert, elmo, and gpt-2 embeddings, 2019. URL\nhttps://arxiv.org/abs/1909.00512.\nJ. Gao, D. He, X. Tan, T. Qin, L. Wang, and\nT. Liu. Representation degeneration problem in\ntraining natural language generation models. In\nInternational Conference on Learning Represen-\ntations, 2019. URL https://openreview.\nnet/forum?id=SkEYojRqtm.\nM. Geva, R. Schuster, J. Berant, and O. Levy. Trans-\nformer feed-forward layers are key-value memo-\nries, 2020. URL https://arxiv.org/abs/\n2012.14913.\nM. Geva, A. Caciularu, G. Dar, P. Roit, S. Sadde,\nM. Shlain, B. Tamir, and Y . Goldberg. Lm-\ndebugger: An interactive tool for inspection and\nintervention in transformer-based language models.\narXiv preprint arXiv:2204.12130, 2022a.\nM. Geva, A. Caciularu, K. R. Wang, and Y . Gold-\nberg. Transformer feed-forward layers build pre-\ndictions by promoting concepts in the vocabulary\nspace, 2022b. URL https://arxiv.org/\nabs/2203.14680.\nP. Jaccard. The distribution of the flora in the alpine\nzone. The New Phytologist , 11(2):37–50, 1912.\nISSN 0028646X, 14698137. URL http://www.\njstor.org/stable/2427226.\nH. W. Kuhn. The hungarian method for the assignment\nproblem. Naval research logistics quarterly, 2(1-2):\n83–97, 1955.\nK. Lenc and A. Vedaldi. Understanding image rep-\nresentations by measuring their equivariance and\nequivalence. 2015 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 991–\n999, 2015.\nY . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoy-\nanov. Roberta: A robustly optimized bert pretraining\napproach, 2019. URL https://arxiv.org/\nabs/1907.11692.\nA. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng,\nand C. Potts. Learning word vectors for sentiment\nanalysis. In Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguis-\ntics: Human Language Technologies , pages 142–\n150, Portland, Oregon, USA, June 2011. Associa-\ntion for Computational Linguistics. URL http://\nwww.aclweb.org/anthology/P11-1015.\nT. Mickus, D. Paperno, and M. Constant. How to\ndissect a muppet: The structure of transformer em-\nbedding spaces. arXiv preprint arXiv:2206.03529 ,\n2022.\nE. H. Moore. On the reciprocal of the general algebraic\nmatrix. Bull. Am. Math. Soc., 26:394–395, 1920.\nnostalgebraist. interpreting gpt: the logit lens,\n2020. URL https://www.lesswrong.\ncom/posts/AcKRB8wDpdaN6v6ru/\ninterpreting-gpt-the-logit-lens .\nhttps://www.lesswrong.com/\nposts/AcKRB8wDpdaN6v6ru/\ninterpreting-gpt-the-logit-lens .\nR. Penrose. A generalized inverse for matrices. In\nMathematical proceedings of the Cambridge philo-\nsophical society, volume 51, pages 406–413. Cam-\nbridge University Press, 1955.\nO. Press and L. Wolf. Using the output embedding to\nimprove language models, 2016. URL https://\narxiv.org/abs/1608.05859.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nand I. Sutskever. Language models are unsupervised\nmultitask learners. In OpenAI blog, 2019.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu. Exploring\nthe limits of transfer learning with a unified text-to-\ntext transformer, 2019. URL https://arxiv.\norg/abs/1910.10683.\nA. Rogers, O. Kovaleva, and A. Rumshisky. A\nprimer in bertology: What we know about how bert\nworks, 2020. URL https://arxiv.org/abs/\n2002.12327.\nW. Rudman, N. Gillman, T. Rayne, and C. Eickhoff.\nIsoscore: Measuring the uniformity of vector space\nutilization. CoRR, abs/2108.07344, 2021. URL\nhttps://arxiv.org/abs/2108.07344.\nT. Sellam, S. Yadlowsky, I. Tenney, J. Wei, N. Saphra,\nA. D’Amour, T. Linzen, J. Bastings, I. R. Turc,\nJ. Eisenstein, D. Das, and E. Pavlick. The multi-\nBERTs: BERT reproductions for robustness analy-\nsis. In International Conference on Learning Repre-\nsentations, 2022. URL https://openreview.\nnet/forum?id=K0E_F0gFDgA.\nX. Shi, I. Padhi, and K. Knight. Does string-based\nneural MT learn source syntax? In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1526–1534, Austin,\nTexas, Nov. 2016. Association for Computational\nLinguistics. doi: 10.18653/v1/D16-1159. URL\nhttps://aclanthology.org/D16-1159.\nS. Sukhbaatar, E. Grave, G. Lample, H. Jegou, and\nA. Joulin. Augmenting self-attention with persistent\nmemory. arXiv preprint arXiv:1907.01470, 2019.\nI. Tenney, D. Das, and E. Pavlick. BERT rediscovers\nthe classical NLP pipeline. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4593–4601, Florence,\nItaly, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1452. URL\nhttps://aclanthology.org/P19-1452.\n16133\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need, 2017. URL\nhttps://arxiv.org/abs/1706.03762.\nE. V oita, R. Sennrich, and I. Titov. The bottom-up\nevolution of representations in the transformer: A\nstudy with machine translation and language mod-\neling objectives, 2019a. URL https://arxiv.\norg/abs/1909.01380.\nE. V oita, D. Talbot, F. Moiseev, R. Sennrich, and\nI. Titov. Analyzing multi-head self-attention: Spe-\ncialized heads do the heavy lifting, the rest can\nbe pruned. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5797–5808, Florence, Italy, July\n2019b. Association for Computational Linguistics.\ndoi: 10.18653/v1/P19-1580. URL https://\naclanthology.org/P19-1580.\nL. Wang, J. Huang, K. Huang, Z. Hu, G. Wang, and\nQ. Gu. Improving neural language generation with\nspectrum control. In International Conference on\nLearning Representations, 2020. URL https://\nopenreview.net/forum?id=ByxY8CNtvr.\nT. Wolf, L. Debut, V . Sanh, J. Chaumond, C. De-\nlangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, J. Davison, S. Shleifer, P. von\nPlaten, C. Ma, Y . Jernite, J. Plu, C. Xu, T. L.\nScao, S. Gugger, M. Drame, Q. Lhoest, and A. M.\nRush. Transformers: State-of-the-art natural lan-\nguage processing. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations , pages 38–45.\nAssociation for Computational Linguistics, Octo-\nber 2020. URL https://www.aclweb.org/\nanthology/2020.emnlp-demos.6.\nS. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen,\nS. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin, T. Mi-\nhaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,\nP. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer.\nOpt: Open pre-trained transformer language mod-\nels, 2022. URL https://arxiv.org/abs/\n2205.01068.\n16134\nA Rethinking Interpretation\n0 100 200 300 400 500\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnormal distribution\nFF values\nhidden states\n0 5000 10000 15000 20000 25000 30000\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nnormal distribution\nFF values\nhidden states\n0 100 200 300 400 500\n0.0\n0.2\n0.4\n0.6\n0.8\nnormal distribution\nFF values\nhidden states\n0 5000 10000 15000 20000 25000 30000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnormal distribution\nFF values\nhidden states\n0 100 200 300 400 500\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\nnormal distribution\nFF values\nhidden states\n0 5000 10000 15000 20000 25000 30000\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nnormal distribution\nFF values\nhidden states\nFigure 5: Each row represents a model in the following order from top to bottom: GPT-2 base, GPT-2 medium, GPT-\n2 large. Left: The keep-k inverse scores for three distributions: normal distribution, hidden states, and FF values, for\nk∈{10,50,100,200,300,500}. Right: for k∈{10,50,100,200,300,500}.\nThe process of interpreting a vector v in [Geva et al., 2022b] proceeds in two steps: first the projection of the\nvector to the embedding space (vE); then, we use the list of the tokens that were assigned the largest values in the\nprojected vector, i.e.: top-k(vE), as the interpretation of the projected vector. This is reasonable since (a) the\nmost activated coordinates contribute the most when added to the residual stream, and (b) this matches how we\neventually decode: we project to the embedding space and consider the top-1 token (or one of the few top tokens,\nwhen using beam search).\nIn this work, we interpret inner products and matrix multiplications in the embedding space: given two vectors\nx,y ∈Rd, their inner product xTycan be considered in the embedding space by multiplying with E and then by\none of its right inverses (e.g., its pseudo-inverse E+ [Moore, 1920; Bjerhammar, 1951; Penrose, 1955]): xTy =\nxTEE+y= (xTE)(E+y). Assume xEis interpretable in the embedding space, crudely meaning that it represents\nlogits over vocabulary items. We expect y, which interacts with x, to also be interpretable in the embedding\n16135\nspace. Consequently, we would like to take E+yto be the projection of y. However, this projection does not take\ninto account the subsequent interpretation using top- k. The projected vector E+ymight be harder to interpret in\nterms of its most activated tokens. To alleviate this problem, we need a different “inverse” matrix E′that works\nwell when considering the top- koperation. Formally, we want an E′with the following “robustness” guarantee:\nkeep-k(xTE)keep-k(E′y) ≈xTy, where keep-k(v) is equal to vfor coordinates whose absolute value is in\nthe top-k, and zero elsewhere.\nThis is a stronger notion of inverse – not only isEE′≈I, but even when truncating the vector in the embedding\nspace we can still reconstruct it with E′.\nWe claim thatET is a decent instantiation ofE′and provide some empirical evidence. While a substantive line of\nwork [Ethayarajh, 2019; Gao et al., 2019; Wang et al., 2020; Rudman et al., 2021] has shown that embedding matri-\nces are not isotropic (an isotropic matrixEhas to satisfy EET = αIfor some scalar α), we show that it is isotropic\nenough to makeET a legitimate compromise. We randomly sample 300 vectors drawn from the normal distribution\nN(0,1), and compute for every pair x,y the cosine similarity between xTyand keep-k(xTE)keep-k(E′y) for\nk = 1000, and then average over all pairs. We repeat this for E′∈{E+,ET}and obtain a score of 0.10 for E+,\nand 0.83 for ET, showing the ET is better under when using top- k. More globally, we compare E′∈{E+,ET}\nfor k∈{10,50,100,200,300,500}with three distributions:\n- x,y drawn from the normal N(0,1) distribution\n- x,y chosen randomly from the FF values\n- x,y drawn from hidden states along Transformer computations.\nIn Figure 5 we show the results, where dashed lines represent E+ and solid lines represent ET. The middle\nrow shows the plots for GPT-2 medium, which is the main concern of this paper. For small values of k (which\nare more appropriate for interpretation), ET is superior to E+ across all distributions. Interestingly, the hidden\nstate distribution is the only distribution where E+ has similar performance to ET. Curiously, when looking at\nhigher values of kthe trend is reversed (k= {512,1024,2048,4096,10000,15000,20000,30000}) - see Figure 5\n(Right).\nThis settles the deviation from findings showing embedding matrices are not isotropic, as we see that indeed ask\ngrows, ET becomes an increasingly bad approximate right-inverse of the embedding matrix. The only distribution\nthat keeps high performance with ET is the hidden state distribution, which is an interesting direction for future\ninvestigation.\nFor completeness, we provide the same analysis for GPT-2 base and large in Figure 5. We can see that GPT-2\nbase gives similar conclusions. GPT-2 large, however, seems to show a violent zigzag movement for E+ but for\nmost values it seems to be superior to ET. It is however probably best to use ET since it is more predictable. This\nzigzag behavior is very counter-intuitive and we leave it for future work to decipher.\nB Additional Material\nB.1 Corresponding Parameter Pairs are Related\nWe define the following metric applying on vectors after projecting them into the embedding space:\nSimk(ˆx,ˆy) =|top-k(ˆx) ∩top-k(ˆy)|\n|top-k(ˆx) ∪top-k(ˆy)|\nwhere top-k(v) is the set of ktop activated indices in the vectorv(which correspond to tokens in the embedding\nspace). This metric is the Jaccard index [Jaccard, 1912] applied to the top- k tokens from each vector. In Figure\n6, Left, we demonstrate that FF key vectors and their corresponding value vectors are more similar (in embedding\nspace) than two random key and value vectors. In Figure 6, Right, we show a similar result for attention value and\noutput vectors. In Figure 6, Bottom, the same analysis is done for attention query and key vectors. This shows that\nthere is a much higher-than-chance relation between corresponding FF keys and values (and the same for attention\nvalues and outputs).\nB.2 Final Prediction and Parameters\nWe show that the final prediction of the model is correlated in embedding space with the most activated parameters\nfrom each layer. This implies that these objects are germane to the analysis of the final prediction in the embedding\nspace, which in turn suggests that the embedding space is a viable choice for interpreting these vectors. Figure\n7 shows that just like §4.2, correspondence is better when hidden states are not randomized, suggesting their\nparameter interpretations have an impact on the final prediction.\n16136\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nK and V\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nWV and WO\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\nWQ and WK\naligned\nrandom\nFigure 6: Average Simk(ˆx,ˆy) for k = 100by layer, where blue is when matching pairs are aligned, and orange is when pairs\nare shuffled within the layer. Top Left: FF keys and FF values. Top Right: The subheads of WO and WV . Bottom: The\nsubheads of WQ and WK.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\nK\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nV\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nWV\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nWO\naligned\nrandom\nFigure 7: Left: Average Rk score (k = 100) across tokens per layer for activated parameter vectors against both the aligned\nhidden state ˆhat the output of the final layer and a randomly sampled hidden state ˆhrand. Parameters are FF keys (top-left), FF\nvalues (top-right), attention values (bottom-left), and attention outputs (bottom-right).\n16137\nB.3 Parameter Alignment Plots for Additional Model Pairs\nAlignment in embedding space of layers of pairs of BERT models trained with different random seeds for addi-\ntional model pairs.\nSeed 1 VS Seed 2\nK V WK\nWQ WV WO\nK V WK\nWQ WV WO\nSeed 2 VS Seed 3\nK V WK\nWQ WV WO\nK V WK\nWQ WV WO\nSeed 3 VS Seed 4\nK V WK\nWQ WV WO\nK V WK\nWQ WV WO\nSeed 4 VS Seed 5\nK V WK\nWQ WV WO\nK V WK\nWQ WV WO\n16138\nC Example Cases\nC.1 WVO Matrices\nBelow we show output-value pairs from different heads\nof GPT-2 medium. For each head, we show the 50 pairs\nwith the largest values in the e×e transition matrix.\nThere are 384 attention heads in GPT-2 medium from\nwhich we manually choose a subset. Throughout the\nsection some lists are marked with asterisks indicating\nthe way this particular list was created:\n* - pairs of the form (x,x) were excluded from the\nlist\n** - pairs where both items are present in the corpus\n(we use IMDB training set).\nAlong with GPT-2 medium, we also provide a few\nexamples from GPT-2 base and GPT-2 large.\nC.1.1 Low-Level Language Modeling\nGPT-2 Medium - Layer 21 Head 7*\n(’NF’, ’FN’),\n(’Ram’, ’ Ramos’),\n(’Hug’, ’ Hughes’),\n(’gran’, ’GR’),\n(’FN’, ’NF’),\n(’CLA’, ’CL’),\n(’McC’, ’ McCain’),\n(’Marsh’, ’ Marshall’),\n(’ Hughes’, ’Hug’),\n(’Tan’, ’ Tanner’),\n(’nih’, ’NH’),\n(’NRS’, ’NR’),\n(’ Bowman’, ’Bow’),\n(’ Marshall’, ’Marsh’),\n(’Jac’, ’ Jacobs’),\n(’Hay’, ’ Hayes’),\n(’ Hayes’, ’Hay’),\n(’McC’, ’ McCorm’),\n(’NI’, ’NR’),\n(’ sidx’, ’ Dawson’),\n(’ Tanner’, ’Tan’),\n(’gra’, ’GR’),\n(’JA’, ’jac’),\n(’zos’, ’zo’),\n(’NI’, ’NF’),\n(’McC’, ’ McCull’),\n(’ Jacobs’, ’Jac’),\n(’ Beetle’, ’ Beet’),\n(’GF’, ’FG’),\n(’jas’, ’ja’),\n(’Wil’, ’ Wilkinson’),\n(’ Ramos’, ’Ram’),\n(’GRE’, ’GR’),\n(’ NF’, ’FN’),\n(’ McCorm’, ’McC’),\n(’Scar’, ’ Scarborough’),\n(’ Baal’, ’Ba’),\n(’FP’, ’FG’),\n(’FH’, ’FN’),\n(’ Garfield’, ’Gar’),\n(’jas’, ’jac’),\n(’nuts’, ’nut’),\n(’WI’, ’ Wis’),\n(’ Vaughn’, ’ Vaughan’),\n(’FP’, ’PF’),\n(’RNA’, ’RN’),\n(’ Jacobs’, ’jac’),\n(’FM’, ’FN’),\n(’ Knox’, ’Kn’),\n(’NI’, ’nic’)\nGPT-2 Medium- Layer 19 Head 13(first letter/consonant\nof the word and last token of the word)\n(’ R’, ’senal’), # arsenal\n(’senal’, ’R’),\n(’ G’, ’vernment’), # government\n(’ Madness’, ’ M’),\n(’ M’, ’ Mayhem’),\n(’ W’, ’nesday’), # wednesday\n(’vernment’, ’G’),\n(’M’, ’ Madness’),\n(’ N’, ’lace’), # necklace\n(’nesday’, ’W’),\n(’Rs’, ’senal’),\n(’ g’, ’vernment’),\n(’ N’, ’farious’), # nefarious\n(’eneg’, ’ C’),\n(’ r’, ’senal’),\n(’ F’, ’ruary’), # february\n(’senal’, ’RIC’),\n(’ R’, ’ondo’),\n(’ N’, ’ Mandela’), # nelson\n(’ Mayhem’, ’M’),\n(’ RD’, ’senal’),\n(’ C’, ’estine’),\n(’Gs’, ’vernment’),\n(’RF’, ’senal’),\n(’ N’, ’esis’),\n(’ N’, ’Reviewed’),\n(’ C’, ’arette’), # cigarette\n(’rome’, ’ N’),\n(’ N’, ’theless’), # nonetheless\n(’lace’, ’N’),\n(’ H’, ’DEN’),\n(’ V’, ’ versa’),\n(’ P’, ’bably’), # probably\n(’vernment’, ’GF’),\n(’g’, ’vernment’),\n(’GP’, ’vernment’),\n(’ C’, ’ornia’), # california\n(’ilipp’, ’ F’),\n(’ N’, ’umbered’),\n(’ C’, ’arettes’),\n(’RS’, ’senal’),\n(’ N’, ’onsense’),\n(’RD’, ’senal’),\n(’RAL’, ’senal’),\n(’ F’, ’uci’),\n(’R’, ’ondo’),\n(’ RI’, ’senal’),\n(’ H’, ’iday’), # holiday\n(’senal’, ’ Rx’),\n(’ F’, ’odor’)\nGPT-2 Medium - Layer 20 Head 9\n(’On’, ’ behalf’),\n(’ On’, ’ behalf’),\n(’ on’, ’ behalf’),\n(’during’, ’ periods’),\n(’within’, ’ bounds’),\n(’ inside’, ’ envelope’),\n(’outside’, ’door’),\n(’inside’, ’ envelope’),\n(’ Under’, ’ regime’),\n16139\n(’ during’, ’ periods’),\n(’ LIKE’, ’lihood’),\n(’ on’, ’ occasions’),\n(’Under’, ’ regime’),\n(’inside’, ’door’),\n(’during’, ’period’),\n(’Like’, ’lihood’),\n(’ During’, ’ periods’),\n(’Inside’, ’ envelope’),\n(’for’, ’ sake’),\n(’ inside’, ’ doors’),\n(’ under’, ’ regime’),\n(’ ON’, ’ behalf’),\n(’for’, ’ purposes’),\n(’On’, ’ occasions’),\n(’inside’, ’ doors’),\n(’ on’, ’ basis’),\n(’ Under’, ’ regimes’),\n(’outside’, ’doors’),\n(’inside’, ’ Osc’),\n(’During’, ’ periods’),\n(’ inside’, ’door’),\n(’ UNDER’, ’ regime’),\n(’ under’, ’ regimes’),\n(’Under’, ’ regimes’),\n(’inside’, ’doors’),\n(’inside’, ’zx’),\n(’during’, ’ period’),\n(’inside’, ’ascript’),\n(’Inside’, ’door’),\n(’ On’, ’ occasions’),\n(’BuyableInstoreAndOnline’, ’ysc’),\n(’ Inside’, ’ envelope’),\n(’during’, ’ pauses’),\n(’under’, ’ regime’),\n(’ on’, ’ occasion’),\n(’outside’, ’ doors’),\n(’ UNDER’, ’ banner’),\n(’within’, ’ envelope’),\n(’ here’, ’abouts’),\n(’during’, ’ duration’)\nGPT-2 Base - Layer 10 Head 11**\n(’ sources’, ’ources’)\n(’ repertoire’, ’ reperto’)\n(’ tales’, ’ stories’)\n(’ stories’, ’ tales’)\n(’ journals’, ’ magazines’)\n(’stories’, ’ tales’)\n(’ journal’, ’ journals’)\n(’ magazines’, ’Magazine’)\n(’ magazines’, ’ newspapers’)\n(’ reperto’, ’ repertoire’)\n(’ cameras’, ’ Camer’)\n(’ source’, ’ sources’)\n(’ newspapers’, ’ magazines’)\n(’ position’, ’ positions’)\n(’ tale’, ’ tales’)\n(’ positions’, ’ position’)\n(’ obstacles’, ’ hurdles’)\n(’ chores’, ’ tasks’)\n(’ journals’, ’ papers’)\n(’ role’, ’ roles’)\n(’ hurdles’, ’ obstacles’)\n(’ journals’, ’ journal’)\n(’ windows’, ’ doors’)\n(’ ceiling’, ’ ceilings’)\n(’ loophole’, ’ loopholes’)\n(’ Sources’, ’ources’)\n(’source’, ’ sources’)\n(’ documentaries’, ’ films’)\n(’ microphone’, ’ microphones’)\n(’ cameras’, ’ camera’)\n(’Journal’, ’ journals’)\n(’ restrooms’, ’ bathrooms’)\n(’ tasks’, ’ chores’)\n(’ perspectives’, ’ viewpoints’)\n(’ shelf’, ’ shelves’)\n(’ rooms’, ’ bedrooms’)\n(’ hurdle’, ’ hurdles’)\n(’ barriers’, ’ fences’)\n(’ magazines’, ’ journals’)\n(’ journals’, ’Magazine’)\n(’ sources’, ’ source’)\n(’ manuals’, ’ textbooks’)\n(’ story’, ’ stories’)\n(’ labs’, ’ laboratories’)\n(’ tales’, ’ Stories’)\n(’ chores’, ’ duties’)\n(’ roles’, ’ role’)\n(’ ceilings’, ’ walls’)\n(’ microphones’, ’ microphone’)\n(’ pathway’, ’ pathways’)\nGPT-2 Large - Layer 27 Head 6\n(’ where’, ’upon’),\n(’where’, ’upon’),\n(’with’, ’ regard’),\n(’with’, ’ regards’),\n(’ with’, ’ regards’),\n(’ Where’, ’upon’),\n(’ Like’, ’lihood’),\n(’of’, ’ course’),\n(’ with’, ’ regard’),\n(’ LIKE’, ’lihood’),\n(’Where’, ’upon’),\n(’from’, ’ afar’),\n(’with’, ’stood’),\n(’ FROM’, ’ afar’),\n(’ like’, ’lihood’),\n(’ WHERE’, ’upon’),\n(’Like’, ’lihood’),\n(’ with’, ’stood’),\n(’ of’, ’ course’),\n(’of’, ’course’),\n(’Of’, ’ course’),\n(’ from’, ’ afar’),\n(’ WITH’, ’ regard’),\n(’ where’, ’abouts’),\n(’with’, ’ impunity’),\n(’ WITH’, ’ regards’),\n(’With’, ’stood’),\n(’for’, ’ purposes’),\n(’with’, ’ respect’),\n(’ With’, ’stood’),\n(’like’, ’lihood’),\n(’ Of’, ’ course’),\n(’With’, ’ regard’),\n(’ With’, ’ regard’),\n(’where’, ’abouts’),\n(’ WITH’, ’stood’),\n(’With’, ’ regards’),\n(’ OF’, ’ course’),\n(’ From’, ’ afar’),\n(’ with’, ’ impunity’),\n(’ With’, ’ regards’),\n(’ with’, ’ respect’),\n(’From’, ’ afar’),\n(’with’, ’standing’),\n(’ on’, ’ behalf’),\n16140\n(’ by’, ’products’),\n(’ for’, ’ purposes’),\n(’ or’, ’acle’),\n(’for’, ’ sake’),\n(’ with’, ’standing’)\nC.1.2 Gender\nGPT-2 Medium - Layer 18 Head 1\n(’women’, ’ Marie’),\n(’ actresses’, ’ Marie’),\n(’women’, ’ Anne’),\n(’Women’, ’ Anne’),\n(’woman’, ’ Marie’),\n(’Women’, ’ Marie’),\n(’woman’, ’ Anne’),\n(’Woman’, ’ Marie’),\n(’ actresses’, ’ Anne’),\n(’ heroine’, ’ Marie’),\n(’Women’, ’Jane’),\n(’ heroine’, ’ Anne’),\n(’women’, ’Jane’),\n(’Women’, ’ actresses’),\n(’Woman’, ’ Anne’),\n(’Women’, ’ Esther’),\n(’women’, ’ Esther’),\n(’girls’, ’ Marie’),\n(’Mrs’, ’ Anne’),\n(’ actress’, ’ Marie’),\n(’women’, ’ actresses’),\n(’Woman’, ’Jane’),\n(’ girls’, ’ Marie’),\n(’ actresses’, ’Jane’),\n(’Woman’, ’Anne’),\n(’Girls’, ’ Marie’),\n(’women’, ’Anne’),\n(’Girls’, ’ Anne’),\n(’Woman’, ’ actresses’),\n(’ Women’, ’ Marie’),\n(’ Women’, ’ Anne’),\n(’ girls’, ’ Anne’),\n(’girl’, ’ Anne’),\n(’Women’, ’Anne’),\n(’Woman’, ’Women’),\n(’girls’, ’ Anne’),\n(’ actresses’, ’Anne’),\n(’women’, ’ Michelle’),\n(’ Actress’, ’ Marie’),\n(’girl’, ’ Marie’),\n(’ Feminist’, ’ Anne’),\n(’ women’, ’ Marie’),\n(’Women’, ’ Devi’),\n(’Women’, ’ Elizabeth’),\n(’ actress’, ’ Anne’),\n(’Mrs’, ’Anne’),\n(’answered’, ’Answer’),\n(’woman’, ’Anne’),\n(’Woman’, ’maid’),\n(’women’, ’Marie’)\nGPT-2 Large - Layer 27 Head 12\n(’ herself’, ’ Marie’),\n(’ hers’, ’ Marie’),\n(’she’, ’ Marie’),\n(’ she’, ’ Marie’),\n(’ her’, ’ Marie’),\n(’She’, ’ Marie’),\n(’ hers’, ’Maria’),\n(’ actresses’, ’ actresses’),\n(’ herself’, ’Maria’),\n(’ her’, ’Maria’),\n(’ herself’, ’ Anne’),\n(’She’, ’Maria’),\n(’ hers’, ’ Louise’),\n(’ herself’, ’ Louise’),\n(’ hers’, ’ Anne’),\n(’ hers’, ’pher’),\n(’she’, ’Maria’),\n(’ actress’, ’ actresses’),\n(’ herself’, ’ Isabel’),\n(’ herself’, ’pher’),\n(’ she’, ’Maria’),\n(’ SHE’, ’ Marie’),\n(’ herself’, ’ Gloria’),\n(’ herself’, ’ Amanda’),\n(’ Ivanka’, ’ Ivanka’),\n(’ her’, ’ Louise’),\n(’ herself’, ’ Kate’),\n(’ her’, ’pher’),\n(’ her’, ’ Anne’),\n(’ she’, ’pher’),\n(’she’, ’ Louise’),\n(’ herself’, ’Kate’),\n(’ she’, ’ Louise’),\n(’ she’, ’ Anne’),\n(’ She’, ’ Marie’),\n(’she’, ’ Gloria’),\n(’She’, ’ Louise’),\n(’ hers’, ’ Gloria’),\n(’ herself’, ’ Diana’),\n(’She’, ’ Gloria’),\n(’she’, ’ Anne’),\n(’she’, ’pher’),\n(’Her’, ’ Marie’),\n(’ she’, ’ Gloria’),\n(’ Paleo’, ’ Paleo’),\n(’ hers’, ’ Diana’)\nGPT-2 Base - Layer 9 Head 7**\n(’ her’, ’ herself’)\n(’She’, ’ herself’)\n(’ she’, ’ herself’)\n(’she’, ’ herself’)\n(’Her’, ’ herself’)\n(’ She’, ’ herself’)\n(’ SHE’, ’ herself’)\n(’their’, ’ themselves’)\n(’ hers’, ’ herself’)\n(’Their’, ’ themselves’)\n(’ Her’, ’ herself’)\n(’ Their’, ’ themselves’)\n(’ THEIR’, ’ themselves’)\n(’ HER’, ’ herself’)\n(’ their’, ’ themselves’)\n(’They’, ’ themselves’)\n(’His’, ’ himself’)\n(’ herself’, ’erest’)\n(’they’, ’ themselves’)\n(’his’, ’ himself’)\n(’Their’, ’selves’)\n(’ They’, ’ themselves’)\n(’ herself’, ’ Louise’)\n(’their’, ’selves’)\n(’her’, ’ herself’)\n(’ his’, ’ himself’)\n(’ herself’, ’ Marie’)\n(’He’, ’ himself’)\n(’She’, ’ Louise’)\n(’ they’, ’ themselves’)\n16141\n(’their’, ’chairs’)\n(’ herself’, ’ dow’)\n(’ herself’, ’eva’)\n(’ THEY’, ’ themselves’)\n(’ herself’, ’ Mae’)\n(’ His’, ’ himself’)\n(’clinton’, ’enegger’)\n(’She’, ’erest’)\n(’ her’, ’ Louise’)\n(’ herself’, ’ Devi’)\n(’ Their’, ’selves’)\n(’Their’, ’chairs’)\n(’ Himself’, ’enegger’)\n(’ she’, ’ Louise’)\n(’ herself’, ’ Anne’)\n(’Its’, ’ itself’)\n(’ her’, ’erest’)\n(’ herself’, ’ Christina’)\n(’she’, ’erest’)\n(’their’, ’ selves’)\nC.1.3 Geography\nGPT-2 Base - Layer 11 Head 2**\n(’ Halifax’, ’ Scotia’)\n(’Saudi’, ’ Arabia’)\n(’ Nova’, ’ Scotia’)\n(’ Tamil’, ’ Nadu’)\n(’ Finnish’, ’onen’)\n(’ Saudi’, ’ Arabia’)\n(’Pitt’, ’sburgh’)\n(’Dutch’, ’ijk’)\n(’ Schwartz’, ’enegger’)\n(’ Afghans’, ’ Kabul’)\n(’ Icelandic’, ’sson’)\n(’ Finland’, ’onen’)\n(’Pitt’, ’enegger’)\n(’ Czech’, ’oslov’)\n(’ Manitoba’, ’ Winnipeg’)\n(’ Malaysian’, ’ Lumpur’)\n(’ Swedish’, ’borg’)\n(’ Saskatchewan’, ’ Sask’)\n(’ Chennai’, ’ Nadu’)\n(’ Argentine’, ’ Aires’)\n(’ Iceland’, ’ Icelandic’)\n(’ Swedish’, ’sson’)\n(’ Tasman’, ’ Nadu’)\n(’Houston’, ’ Astros’)\n(’Colorado’, ’ Springs’)\n(’ Kuala’, ’ Lumpur’)\n(’Tai’, ’pport’)\n(’Houston’, ’ Dynamo’)\n(’ Manitoba’, ’Marginal’)\n(’ Afghan’, ’ Kabul’)\n(’ Buenos’, ’ Aires’)\n(’ Alberta’, ’ Calgary’)\n(’ Stockholm’, ’sson’)\n(’ Sweden’, ’borg’)\n(’Brazil’, ’ Paulo’)\n(’ Iceland’, ’sson’)\n(’ Winnipeg’, ’ Manitoba’)\n(’ Sweden’, ’sson’)\n(’ Carolina’, ’ Hurricanes’)\n(’ Dutch’, ’ijk’)\n(’ Swed’, ’borg’)\n(’ Aki’, ’pport’)\n(’ Winnipeg’, ’Marginal’)\n(’ Argentine’, ’ pes’)\n(’ Halifax’, ’imore’)\n(’ Brisbane’, ’enegger’)\n(’ Melbourne’, ’ Nadu’)\n(’ Adelaide’, ’ Nadu’)\n(’ Cambod’, ’ Nguyen’)\n(’ Vietnamese’, ’ Nguyen’)\nGPT-2 Medium - Layer 16 Head 6*\n(’ Chennai’, ’ Mumbai’),\n(’India’, ’ Mumbai’),\n(’ Mumbai’, ’ Chennai’),\n(’ Queensland’, ’ Tasmania’),\n(’India’, ’ Rahul’),\n(’India’, ’ Gujar’),\n(’ Chennai’, ’ Bangalore’),\n(’England’, ’Scotland’),\n(’ Chennai’, ’ Kerala’),\n(’ Delhi’, ’ Mumbai’),\n(’Britain’, ’Scotland’),\n(’ Bangalore’, ’ Mumbai’),\n(’Pakistan’, ’India’),\n(’Scotland’, ’Ireland’),\n(’ Mumbai’, ’ Bangalore’),\n(’ Bangalore’, ’ Chennai’),\n(’ Aadhaar’, ’ Gujar’),\n(’ Mumbai’, ’ Maharashtra’),\n(’ Maharashtra’, ’ Gujarat’),\n(’ Gujarat’, ’ Gujar’),\n(’Australian’, ’Australia’),\n(’India’, ’ Gujarat’),\n(’ Rahul’, ’ Gujar’),\n(’ Maharashtra’, ’ Mumbai’),\n(’Britain’, ’England’),\n(’India’, ’ Chennai’),\n(’ Mumbai’, ’ Bombay’),\n(’ Tamil’, ’ Kerala’),\n(’ Hindi’, ’ Mumbai’),\n(’ Tasmania’, ’ Tasman’),\n(’ Mumbai’, ’India’),\n(’ Hindi’, ’ Gujar’),\n(’ Maharashtra’, ’ Gujar’),\n(’ Australians’, ’Austral’),\n(’ Maharashtra’, ’ Kerala’),\n(’India’, ’ Bangalore’),\n(’India’, ’ Kerala’),\n(’India’, ’ Bombay’),\n(’Australia’, ’Austral’),\n(’ Aadhaar’, ’India’),\n(’ Sharma’, ’ Mumbai’),\n(’Australian’, ’Austral’),\n(’ Mumbai’, ’ Kerala’),\n(’Scotland’, ’England’),\n(’ Mumbai’, ’ Gujar’),\n(’ Rahul’, ’ Mumbai’),\n(’ Queensland’, ’ Tasman’),\n(’ Tamil’, ’ Chennai’),\n(’ Gujarat’, ’ Maharashtra’),\n(’India’, ’ Modi’)\nGPT-2 Medium - Layer 16 Head 2*\n(’Austral’, ’ Australians’),\n(’Australia’, ’Austral’),\n(’ Canberra’, ’Austral’),\n(’Austral’, ’ Canberra’),\n(’ Winnipeg’, ’ Edmonton’),\n(’Australian’, ’Austral’),\n(’ Alberta’, ’ Edmonton’),\n(’Australia’, ’ Australians’),\n(’ Australians’, ’Austral’),\n(’Ukraine’, ’ovych’),\n16142\n(’ Quebec’, ’ Canad’),\n(’Australian’, ’ Australians’),\n(’ Winnipeg’, ’ Manitoba’),\n(’ Manitoba’, ’ Winnipeg’),\n(’Canadian’, ’Canada’),\n(’Moscow’, ’ Bulgar’),\n(’ Manitoba’, ’ Edmonton’),\n(’berra’, ’Austral’),\n(’Austral’, ’Australian’),\n(’ Ukrainians’, ’ovych’),\n(’Canada’, ’ Canadians’),\n(’ Canberra’, ’ Australians’),\n(’Canada’, ’Canadian’),\n(’ Yanukovych’, ’ovych’),\n(’Canada’, ’ Trudeau’),\n(’ Dmitry’, ’ Bulgar’),\n(’ Australia’, ’Austral’),\n(’ Mulcair’, ’ Canad’),\n(’berra’, ’ Canberra’),\n(’Turkish’, ’oglu’),\n(’udeau’, ’Canada’),\n(’ Edmonton’, ’ Oilers’),\n(’Australia’, ’ Canberra’),\n(’Canada’, ’ Edmonton’),\n(’ Edmonton’, ’ Calgary’),\n(’ Alberta’, ’ Calgary’),\n(’udeau’, ’ Trudeau’),\n(’ Calgary’, ’ Edmonton’),\n(’Canadian’, ’ Trudeau’),\n(’Australian’, ’ Canberra’),\n(’ Vancouver’, ’ Canucks’),\n(’Australia’, ’Australian’),\n(’ Vancouver’, ’ Fraser’),\n(’Canadian’, ’ Edmonton’),\n(’Austral’, ’elaide’),\n(’Tex’, ’ Braz’),\n(’Canada’, ’ RCMP’),\n(’Moscow’, ’sov’),\n(’Russia’, ’ Bulgar’),\n(’ Canadians’, ’Canada’)\nGPT-2 Medium - Layer 21 Head 12*\n(’ Indonesian’, ’ Indones’),\n(’ Vietnamese’, ’ Nguyen’),\n(’ Indonesian’, ’ Jakarta’),\n(’ Indonesian’, ’ Indonesia’),\n(’Turkish’, ’oglu’),\n(’ Indonesia’, ’ Indones’),\n(’ Jakarta’, ’ Indones’),\n(’ Korean’, ’ Koreans’),\n(’ Turkish’, ’oglu’),\n(’ Taiwan’, ’ Taiwanese’),\n(’ Thai’, ’ Nguyen’),\n(’ Brazilian’, ’Brazil’),\n(’ Indones’, ’ Indonesia’),\n(’Tai’, ’ Taiwanese’),\n(’ Istanbul’, ’oglu’),\n(’ Indones’, ’ Indonesian’),\n(’ Indones’, ’ Jakarta’),\n(’ Laos’, ’ Nguyen’),\n(’ Slovenia’, ’ Sloven’),\n(’ Koreans’, ’ Korean’),\n(’ Cambod’, ’ Nguyen’),\n(’Italy’, ’zzi’),\n(’ Taiwanese’, ’Tai’),\n(’ Indonesia’, ’ Jakarta’),\n(’ Indonesia’, ’ Indonesian’),\n(’ Bulgarian’, ’ Bulgaria’),\n(’ Iceland’, ’ Icelandic’),\n(’ Korea’, ’ Koreans’),\n(’Brazil’, ’ Brazilian’),\n(’ Bulgarian’, ’ Bulgar’),\n(’ Malaysian’, ’ Malays’),\n(’ Ankara’, ’oglu’),\n(’ Bulgaria’, ’ Bulgarian’),\n(’ Malays’, ’ Indones’),\n(’ Taiwanese’, ’ Tai’),\n(’Turkey’, ’oglu’),\n(’Brazil’, ’ Janeiro’),\n(’Italian’, ’zzi’),\n(’ Kuala’, ’ Malays’),\n(’Japanese’, ’ Fuk’),\n(’ Jakarta’, ’ Indonesian’),\n(’ Taiwanese’, ’ Taiwan’),\n(’ Erdogan’, ’oglu’),\n(’ Viet’, ’ Nguyen’),\n(’ Philippine’, ’ Filipino’),\n(’ Jakarta’, ’ Indonesia’),\n(’ Koreans’, ’ Jong’),\n(’ Filipino’, ’ Duterte’),\n(’ Azerbaijan’, ’ Azerbai’),\n(’ Bulgar’, ’ Bulgarian’)\nGPT-2 Large - Layer 23 Head 5\n(’Canada’, ’ Trudeau’),\n(’ Canadians’, ’ Trudeau’),\n(’Canadian’, ’ Trudeau’),\n(’ Queensland’, ’ Tasman’),\n(’ Tasman’, ’ Tasman’),\n(’ Canada’, ’ Trudeau’),\n(’ Canberra’, ’ Canberra’),\n(’ Winnipeg’, ’ Winnipeg’),\n(’ Canberra’, ’ Tasman’),\n(’Canadian’, ’Canada’),\n(’ Canadian’, ’ Trudeau’),\n(’ Brisbane’, ’ Brisbane’),\n(’ Quebec’, ’ Trudeau’),\n(’Canadian’, ’ Canadian’),\n(’ Brisbane’, ’ Tasman’),\n(’ Tasmania’, ’ Tasman’),\n(’Canadian’, ’ Canadians’),\n(’ RCMP’, ’ Trudeau’),\n(’ Manitoba’, ’ Trudeau’),\n(’ Queensland’, ’ Brisbane’),\n(’ Queensland’, ’ Canberra’),\n(’Canada’, ’ Saskatchewan’),\n(’Canadian’, ’ Saskatchewan’),\n(’Canada’, ’ Canadian’),\n(’ RCMP’, ’ Saskatchewan’),\n(’ Canberra’, ’ Brisbane’),\n(’ Canadians’, ’Canada’),\n(’ Winnipeg’, ’ Trudeau’),\n(’Canadian’, ’ Canada’),\n(’Canada’, ’ Canadians’),\n(’Australian’, ’ Canberra’),\n(’ Melbourne’, ’ Canberra’),\n(’ RCMP’, ’ Canad’),\n(’ Canadians’, ’ Canadians’),\n(’CBC’, ’ Trudeau’),\n(’ Canadian’, ’ Canadian’),\n(’Canadian’, ’ Winnipeg’),\n(’ Australians’, ’ Canberra’),\n(’ Quebec’, ’Canada’),\n(’ Canadian’, ’Canada’),\n(’ NSW’, ’ Canberra’),\n(’Toronto’, ’ Canad’),\n(’Canada’, ’Canada’),\n(’ NSW’, ’ Tasman’),\n(’ RCMP’, ’ RCMP’),\n(’ Canadian’, ’ Canadians’),\n16143\n(’ Saskatchewan’, ’ Saskatchewan’),\n(’ Canadians’, ’ Saskatchewan’),\n(’Canadian’, ’ Canad’),\n(’ Ottawa’, ’ Winnipeg’)\nC.1.4 British Spelling\nGPT-2 Medium - Layer 19 Head 4\n(’ realise’, ’ Whilst’),\n(’ Whilst’, ’ Whilst’),\n(’ realised’, ’ Whilst’),\n(’ organise’, ’ Whilst’),\n(’ recognise’, ’ Whilst’),\n(’ civilisation’, ’ Whilst’),\n(’ organisation’, ’ Whilst’),\n(’ whilst’, ’ Whilst’),\n(’ organising’, ’ Whilst’),\n(’ organised’, ’ Whilst’),\n(’ organis’, ’ Whilst’),\n(’ util’, ’ Whilst’),\n(’ apologise’, ’ Whilst’),\n(’ emphas’, ’ Whilst’),\n(’ analyse’, ’ Whilst’),\n(’ organisations’, ’ Whilst’),\n(’ recognised’, ’ Whilst’),\n(’ flavours’, ’ Whilst’),\n(’ colour’, ’ Whilst’),\n(’colour’, ’ Whilst’),\n(’ Nasa’, ’ Whilst’),\n(’ Nato’, ’ Whilst’),\n(’ analys’, ’ Whilst’),\n(’ flavour’, ’ Whilst’),\n(’ colourful’, ’ Whilst’),\n(’ colours’, ’ Whilst’),\n(’ realise’, ’ organising’),\n(’ behavioural’, ’ Whilst’),\n(’ coloured’, ’ Whilst’),\n(’ learnt’, ’ Whilst’),\n(’ favourable’, ’ Whilst’),\n(’isation’, ’ Whilst’),\n(’ programmes’, ’ Whilst’),\n(’ realise’, ’ organis’),\n(’ authorised’, ’ Whilst’),\n(’ practise’, ’ Whilst’),\n(’ criticised’, ’ Whilst’),\n(’ organisers’, ’ Whilst’),\n(’ organise’, ’ organising’),\n(’ analysed’, ’ Whilst’),\n(’ programme’, ’ Whilst’),\n(’ behaviours’, ’ Whilst’),\n(’ humour’, ’ Whilst’),\n(’isations’, ’ Whilst’),\n(’ tyres’, ’ Whilst’),\n(’ aluminium’, ’ Whilst’),\n(’ realise’, ’ organised’),\n(’ favour’, ’ Whilst’),\n(’ ageing’, ’ Whilst’),\n(’ organise’, ’ organis’)\nC.1.5 Related Words\nGPT-2 Medium - Layer 13 Head 8*\n(’ miraculous’, ’ mirac’),\n(’ miracle’, ’ mirac’),\n(’ nuance’, ’ nuanced’),\n(’ smarter’, ’Better’),\n(’ healthier’, ’ equitable’),\n(’ liberated’, ’ liberating’),\n(’ untouched’, ’ unaffected’),\n(’ unbiased’, ’ equitable’),\n(’failed’, ’ inconsistent’),\n(’ liberated’, ’ emanc’),\n(’ humane’, ’ equitable’),\n(’ liberating’, ’ liberated’),\n(’failed’, ’ incompatible’),\n(’ miracles’, ’ mirac’),\n(’ peacefully’, ’ consensual’),\n(’ unconditional’, ’ uncond’),\n(’ unexpectedly’, ’ unexpected’),\n(’ untouched’, ’ unconditional’),\n(’ healthier’, ’Better’),\n(’ unexpected’, ’ unexpectedly’),\n(’ peacefully’, ’ graceful’),\n(’ emancipation’, ’ emanc’),\n(’ seamlessly’, ’ effortlessly’),\n(’ peacefully’, ’ honorable’),\n(’ uncond’, ’ unconditional’),\n(’ excuses’, ’ rubbish’),\n(’ liberating’, ’ emanc’),\n(’ peacefully’, ’ equitable’),\n(’ gracious’, ’ Feather’),\n(’ liberated’, ’ emancipation’),\n(’ nuances’, ’ nuanced’),\n(’ avoids’, ’icable’),\n(’ freeing’, ’ liberated’),\n(’ freeing’, ’ liberating’),\n(’ lousy’, ’ inconsistent’),\n(’failed’, ’ lousy’),\n(’ unaffected’, ’ unconditional’),\n(’ivable’, ’ equitable’),\n(’Honest’, ’ equitable’),\n(’ principled’, ’erning’),\n(’surv’, ’ survival’),\n(’ lackluster’, ’ocre’),\n(’ liberating’, ’ equitable’),\n(’Instead’, ’Bah’),\n(’ inappropriate’, ’ incompatible’),\n(’ emanc’, ’ emancipation’),\n(’ unaffected’, ’ unchanged’),\n(’ peaceful’, ’ peacefully’),\n(’ safer’, ’ equitable’),\n(’ uninterrupted’, ’ unconditional’)\nGPT-2 Medium - Layer 12 Head 14*\n(’ died’, ’ perished’),\n(’ dies’, ’ perished’),\n(’ testifying’, ’ testify’),\n(’ interven’, ’ intervened’),\n(’ advising’, ’ advises’),\n(’ disband’, ’ disbanded’),\n(’ perished’, ’lost’),\n(’ perished’, ’ died’),\n(’ applaud’, ’ applauded’),\n(’ dictate’, ’ dictates’),\n(’ prevailed’, ’ prev’),\n(’ advising’, ’ advise’),\n(’thood’, ’shed’),\n(’orsi’, ’Reviewed’),\n(’ perished’, ’ dies’),\n(’ publishes’, ’published’),\n(’ prevail’, ’ prevailed’),\n(’ dies’, ’ died’),\n(’ testifying’, ’ testified’),\n(’ testify’, ’ testifying’),\n(’ governs’, ’ dictates’),\n(’ complicity’, ’ complicit’),\n(’ dictate’, ’ dictated’),\n(’CHO’, ’enough’),\n(’independence’, ’ skelet’),\n16144\n(’ prescribe’, ’ Recomm’),\n(’ perished’, ’essential’),\n(’CHO’, ’noticed’),\n(’ approving’, ’avorable’),\n(’ perished’, ’ perish’),\n(’ oversee’, ’ overseeing’),\n(’shed’, ’ skelet’),\n(’chart’, ’EY’),\n(’ overseeing’, ’ presiding’),\n(’pees’, ’ fundament’),\n(’appro’, ’ sanction’),\n(’ prevailed’, ’ prevail’),\n(’ regulates’, ’ governs’),\n(’shed’, ’tails’),\n(’chart’, ’ Period’),\n(’hower’, ’lihood’),\n(’ prevail’, ’ prev’),\n(’helps’, ’ aids’),\n(’ dict’, ’ dictated’),\n(’ dictates’, ’ dictated’),\n(’itta’, ’ Dise’),\n(’CHO’, ’REC’),\n(’ORTS’, ’exclusive’),\n(’helps’, ’ Helpful’),\n(’ciples’, ’bart’)\nGPT-2 Medium - Layer 14 Head 1*\n(’ incorrectly’, ’ misunderstand’),\n(’ properly’, ’ Proper’),\n(’ incorrectly’, ’ inaccur’),\n(’ wrongly’, ’ misunderstand’),\n(’ incorrectly’, ’ misinterpret’),\n(’ incorrectly’, ’ incorrect’),\n(’ incorrectly’, ’ mistakes’),\n(’ incorrectly’, ’ misunderstanding’),\n(’ properly’, ’ proper’),\n(’ incorrectly’, ’fail’),\n(’ incorrectly’, ’ faulty’),\n(’ incorrectly’, ’ misrepresent’),\n(’ fails’, ’ failing’),\n(’ incorrectly’, ’ inaccurate’),\n(’ incorrectly’, ’ errors’),\n(’ Worse’, ’ harmful’),\n(’ wrong’, ’ misunderstand’),\n(’ improperly’, ’ misunderstand’),\n(’ incorrectly’, ’wrong’),\n(’ incorrectly’, ’ harmful’),\n(’ incorrectly’, ’ mistake’),\n(’ incorrectly’, ’ mis’),\n(’ fails’, ’fail’),\n(’ Worse’, ’ detrimental’),\n(’ properly’, ’ rightful’),\n(’ inappropriately’, ’ misunderstand’),\n(’ unnecessarily’, ’ harmful’),\n(’ unnecessarily’, ’ neglect’),\n(’ properly’, ’ correctly’),\n(’ Worse’, ’ Worst’),\n(’ fails’, ’ failure’),\n(’ adequately’, ’ satisfactory’),\n(’ incorrectly’, ’ defective’),\n(’ mistakenly’, ’ misunderstand’),\n(’ Worse’, ’ harming’),\n(’ incorrectly’, ’ mishand’),\n(’ adequately’, ’adequ’),\n(’ incorrectly’, ’ misuse’),\n(’ fails’, ’Failure’),\n(’ Worse’, ’ hurts’),\n(’wrong’, ’ misunderstand’),\n(’ incorrectly’, ’ mistakenly’),\n(’ fails’, ’ failures’),\n(’ adequately’, ’ adequate’),\n(’ correctly’, ’ properly’),\n(’ Worse’, ’ hurting’),\n(’ correctly’, ’ Proper’),\n(’ fails’, ’ fail’),\n(’ incorrectly’, ’ mistaken’),\n(’ adversely’, ’ harming’)\nGPT-2 Large - Layer 24 Head 9\n(’ interviewer’, ’ interviewer’),\n(’ lectures’, ’ lectures’),\n(’ lecture’, ’ lecture’),\n(’ interview’, ’Interview’),\n(’ interview’, ’ interview’),\n(’ interview’, ’ interviewer’),\n(’ interviewing’, ’ interviewing’),\n(’ magazine’, ’ magazine’),\n(’ Reviews’, ’ Reviews’),\n(’ reviewer’, ’ reviewer’),\n(’ reviewers’, ’ reviewers’),\n(’ lectures’, ’ lecture’),\n(’ testers’, ’ testers’),\n(’ editors’, ’ editors’),\n(’ interviewer’, ’ interview’),\n(’ Interview’, ’Interview’),\n(’ interviewer’, ’Interview’),\n(’Interview’, ’Interview’),\n(’ lecture’, ’ lectures’),\n(’ interviewing’, ’ interviewer’),\n(’ journal’, ’ journal’),\n(’ interviewer’, ’ interviewing’),\n(’ blogs’, ’ blogs’),\n(’ editorial’, ’ editorial’),\n(’ tests’, ’ tests’),\n(’ presentations’, ’ presentations’),\n(’ Editorial’, ’ Editorial’),\n(’ interview’, ’ Interview’),\n(’ reviewer’, ’ reviewers’),\n(’ interviews’, ’Interview’),\n(’ interview’, ’ interviewing’),\n(’ interviewer’, ’ Interview’),\n(’ interviews’, ’ interview’),\n(’ Interview’, ’ Interview’),\n(’ interviewing’, ’Interview’),\n(’Interview’, ’ interviewer’),\n(’ testifying’, ’ testifying’),\n(’ reviewers’, ’ reviewer’),\n(’ blogging’, ’ blogging’),\n(’ broadcast’, ’ broadcast’),\n(’ Interview’, ’ interviewer’),\n(’ magazine’, ’ magazines’),\n(’ editorial’, ’ Editorial’),\n(’ interview’, ’ interviews’),\n(’ interviewing’, ’ interview’),\n(’ Interview’, ’ interview’),\n(’ interviews’, ’ interviews’),\n(’ tests’, ’tests’),\n(’ interviews’, ’ interviewing’),\n(’Interview’, ’ interview’)\nGPT-2 Medium - Layer 14 Head 13*\n(’ editorial’, ’ editors’),\n(’ broadcasting’, ’ broadcasters’),\n(’ broadcasts’, ’ broadcasting’),\n(’ broadcasts’, ’ broadcast’),\n(’ broadcasters’, ’ Broadcasting’),\n(’ Editorial’, ’ editors’),\n(’ broadcast’, ’ broadcasters’),\n(’ broadcast’, ’ Broadcasting’),\n(’ lecture’, ’ lectures’),\n16145\n(’ broadcasting’, ’ Broadcast’),\n(’ broadcaster’, ’ broadcasters’),\n(’ broadcasts’, ’ broadcasters’),\n(’ publishing’, ’ Publishers’),\n(’ broadcast’, ’ broadcasting’),\n(’ Broadcasting’, ’ broadcasters’),\n(’ Publishing’, ’ Publishers’),\n(’ lectures’, ’ lecture’),\n(’ editorial’, ’ Editors’),\n(’ broadcasting’, ’ broadcast’),\n(’ broadcasts’, ’ Broadcasting’),\n(’ broadcasters’, ’ broadcasting’),\n(’ journalistic’, ’ journalism’),\n(’Journal’, ’reports’),\n(’ Broadcasting’, ’ Broadcast’),\n(’Publisher’, ’ Publishers’),\n(’ Broadcasting’, ’azeera’),\n(’Journal’, ’Reporting’),\n(’ journalism’, ’ journalistic’),\n(’ broadcaster’, ’ Broadcasting’),\n(’ broadcaster’, ’ broadcasting’),\n(’ broadcasting’, ’ broadcaster’),\n(’ publication’, ’ editors’),\n(’journal’, ’ journalism’),\n(’Journal’, ’ Journalists’),\n(’ documentaries’, ’ documentary’),\n(’ filmed’, ’ filming’),\n(’ publishing’, ’ publishers’),\n(’Journal’, ’ journalism’),\n(’ broadcasts’, ’ Broadcast’),\n(’ broadcasters’, ’ broadcast’),\n(’Journal’, ’ articles’),\n(’reports’, ’ reporting’),\n(’ manuscript’, ’ manuscripts’),\n(’ publishing’, ’ publish’),\n(’ broadcasters’, ’azeera’),\n(’ publication’, ’ Publishers’),\n(’ publications’, ’ Publishers’),\n(’ Newsp’, ’ newspapers’),\n(’ broadcasters’, ’ Broadcast’),\n(’Journal’, ’ Readers’)\nC.2 Query-Key Matrices\nGPT-2 Large - Layer 19 Head 7**\n(’ tonight’, ’Friday’),\n(’ Copyright’, ’Returns’),\n(’TM’, ’review’),\n(’ Weekend’, ’Preview’),\n(’ tonight’, ’Thursday’),\n(’ recently’, ’Closure’),\n(’ Copyright’, ’Contents’),\n(’ Copyright’, ’Wisconsin’),\n(’ Copyright’, ’Methods’),\n(’ tonight’, ’Sunday’),\n(’ tomorrow’, ’ postpone’),\n(’ tomorrow’, ’ tonight’),\n(’ recently’, ’acerb’),\n(’ Copyright’, ’Rated’),\n(’ myself’, ’ my’),\n(’ Copyright’, ’Cop’),\n(’ Wednesday’, ’Closure’),\n(’ Billion’, ’ 1935’),\n(’ tonight’, ’Saturday’),\n(’ tonight’, ’ celebr’),\n(’ tomorrow’, ’ postponed’),\n(’ Copyright’, ’Show’),\n(’ Wednesday’, ’Friday’),\n(’ Copyright’, ’Earn’),\n(’ Billion’, ’ 1934’),\n(’ Eric’, ’Larry’),\n(’ 2015’, ’Released’),\n(’ Copyright’, ’Rat’),\n(’ tomorrow’, ’ postp’),\n(’ 2017’, ’Latest’),\n(’ previous’, ’obin’),\n(’ controversial’, ’Priv’),\n(’ recently’, ’ nightly’),\n(’Base’, ’ LV’),\n(’ recently’, ’Project’),\n(’ historically’, ’ globalization’),\n(’ recently’, ’ vulner’),\n(’ tonight’, ’Wednesday’),\n(’ Copyright’, ’Abstract’),\n(’ Tuesday’, ’Friday’),\n(’ Anthony’, ’Born’),\n(’ Budget’, ’Premium’),\n(’ tonight’, ’Welcome’),\n(’yle’, ’lite’),\n(’ Wednesday’, ’Latest’),\n(’ Latest’, ’show’),\n(’ B’, ’ pione’),\n(’ Copyright’, ’cop’),\n(’ Pablo’, ’ Dia’),\n(’ recent’, ’Latest’)\nGPT-2 Medium - Layer 22 Head 1\n(’ usual’, ’ usual’),\n(’ occasional’, ’ occasional’),\n(’ aforementioned’, ’ aforementioned’),\n(’ general’, ’ usual’),\n(’ usual’, ’ slightest’),\n(’agn’, ’ealous’),\n(’ traditional’, ’ usual’),\n(’ free’, ’amina’),\n(’ major’, ’ major’),\n(’ frequent’, ’ occasional’),\n(’ generous’, ’ generous’),\n(’ free’, ’lam’),\n(’ regular’, ’ usual’),\n(’ standard’, ’ usual’),\n(’ main’, ’ usual’),\n(’ complete’, ’ Finished’),\n(’ main’, ’liest’),\n(’ traditional’, ’ traditional’),\n(’ latest’, ’ aforementioned’),\n(’ current’, ’ aforementioned’),\n(’ normal’, ’ usual’),\n(’ dominant’, ’ dominant’),\n(’ free’, ’ministic’),\n(’ brief’, ’ brief’),\n(’ biggest’, ’liest’),\n(’usual’, ’ usual’),\n(’ rash’, ’ rash’),\n(’ regular’, ’ occasional’),\n(’ specialized’, ’ specialized’),\n(’ free’, ’iosis’),\n(’ free’, ’hero’),\n(’ specialty’, ’ specialty’),\n(’ general’, ’iosis’),\n(’ nearby’, ’ nearby’),\n(’ best’, ’liest’),\n(’ officially’, ’ formal’),\n(’ immediate’, ’mediate’),\n(’ special’, ’ ultimate’),\n(’ free’, ’otropic’),\n(’ rigorous’, ’ comparative’),\n(’ actual’, ’ slightest’),\n16146\n(’ complete’, ’ comparative’),\n(’ typical’, ’ usual’),\n(’ modern’, ’ modern’),\n(’ best’, ’ smartest’),\n(’ free’, ’ free’),\n(’ highest’, ’ widest’),\n(’ specialist’, ’ specialist’),\n(’ appropriate’, ’ slightest’),\n(’ usual’, ’liest’)\nGPT-2 Large - Layer 20 Head 13 **\n(’ outdoors’, ’ outdoors’),\n(’ outdoor’, ’ outdoors’),\n(’ Gre’, ’burg’),\n(’ healing’, ’ healing’),\n(’ indoor’, ’ outdoors’),\n(’ Hemp’, ’burg’),\n(’ Ticket’, ’ Ticket’),\n(’ accommodations’, ’ accommodations’),\n(’eco’, ’aco’),\n(’prem’, ’otti’),\n(’ Candy’, ’cott’),\n(’ decorative’, ’ ornament’),\n(’yan’, ’ava’),\n(’ deadlines’, ’ schedule’),\n(’ Lor’, ’ian’),\n(’ architectural’, ’ ornament’),\n(’ Ratings’, ’ Ratings’),\n(’ Bod’, ’za’),\n(’ exotic’, ’ exotic’),\n(’ food’, ’ baths’),\n(’ Marketplace’, ’ Marketplace’),\n(’ heal’, ’ healing’),\n(’ Ex’, ’ilus’),\n(’ indoors’, ’ outdoors’),\n(’ therm’, ’ therm’),\n(’ bleach’, ’ coated’),\n(’ Sod’, ’opol’),\n(’ District’, ’ Metropolitan’),\n(’ Anonymous’, ’ Rebell’),\n(’ Corn’, ’burg’),\n(’ indoor’, ’ indoors’),\n(’ R’, ’vale’),\n(’rom’, ’otti’),\n(’ ratings’, ’ Ratings’),\n(’ attendance’, ’ attendance’),\n(’ destinations’, ’ destinations’),\n(’ VIDEOS’, ’ VIDEOS’),\n(’yan’, ’opol’),\n(’ Suffolk’, ’ville’),\n(’ retali’, ’ against’),\n(’mos’, ’oli’),\n(’ pacing’, ’ pacing’),\n(’ Spectrum’, ’ QC’),\n(’ Il’, ’ian’),\n(’ archived’, ’ archived’),\n(’ Pledge’, ’ Pledge’),\n(’alg’, ’otti’),\n(’ Freedom’, ’USA’),\n(’anto’, ’ero’),\n(’ decorative’, ’ decoration’)\nGPT-2 Medium - Layer 0 Head 9\n(’59’, ’27’),\n(’212’, ’39’),\n(’212’, ’38’),\n(’217’, ’39’),\n(’37’, ’27’),\n(’59’, ’26’),\n(’54’, ’88’),\n(’156’, ’39’),\n(’212’, ’79’),\n(’59’, ’28’),\n(’57’, ’27’),\n(’212’, ’57’),\n(’156’, ’29’),\n(’36’, ’27’),\n(’217’, ’79’),\n(’59’, ’38’),\n(’63’, ’27’),\n(’72’, ’39’),\n(’57’, ’26’),\n(’57’, ’34’),\n(’59’, ’34’),\n(’156’, ’27’),\n(’91’, ’27’),\n(’156’, ’38’),\n(’63’, ’26’),\n(’59’, ’25’),\n(’138’, ’27’),\n(’217’, ’38’),\n(’72’, ’27’),\n(’54’, ’27’),\n(’36’, ’29’),\n(’72’, ’26’),\n(’307’, ’39’),\n(’37’, ’26’),\n(’217’, ’57’),\n(’37’, ’29’),\n(’54’, ’38’),\n(’59’, ’29’),\n(’37’, ’28’),\n(’307’, ’38’),\n(’57’, ’29’),\n(’63’, ’29’),\n(’71’, ’27’),\n(’138’, ’78’),\n(’59’, ’88’),\n(’89’, ’27’),\n(’561’, ’79’),\n(’212’, ’29’),\n(’183’, ’27’),\n(’54’, ’29’)\nGPT-2 Medium - Layer 17 Head 6*\n(’ legally’, ’ legal’),\n(’ legal’, ’ sentencing’),\n(’ legal’, ’ arbitration’),\n(’ boycot’, ’ boycott’),\n(’ legal’, ’ criminal’),\n(’ legal’, ’ Judicial’),\n(’ legal’, ’ rulings’),\n(’ judicial’, ’ sentencing’),\n(’ marketing’, ’ advertising’),\n(’ legal’, ’ confidential’),\n(’ protesting’, ’ protest’),\n(’ recruited’, ’ recruit’),\n(’ recruited’, ’ recruits’),\n(’ judicial’, ’ criminal’),\n(’ legal’, ’ exemptions’),\n(’ demographics’, ’ demographic’),\n(’ boycott’, ’ boycot’),\n(’ sentencing’, ’ criminal’),\n(’ recruitment’, ’ recruits’),\n(’ recruitment’, ’ recruit’),\n(’ Constitutional’, ’ sentencing’),\n(’ Legal’, ’ sentencing’),\n(’ constitutional’, ’ sentencing’),\n(’ legal’, ’ subpoena’),\n16147\n(’ injury’, ’ injuries’),\n(’ FOIA’, ’ confidential’),\n(’ legal’, ’ licenses’),\n(’ donation’, ’ donations’),\n(’ disclosure’, ’ confidential’),\n(’ negotiation’, ’ negotiating’),\n(’ Judicial’, ’ legal’),\n(’ legally’, ’ criminal’),\n(’ legally’, ’ confidential’),\n(’ legal’, ’ jur’),\n(’ legal’, ’ enforcement’),\n(’ legal’, ’ lawyers’),\n(’ legally’, ’ enforcement’),\n(’ recruitment’, ’ recruiting’),\n(’ recruiting’, ’ recruit’),\n(’ criminal’, ’ sentencing’),\n(’ legal’, ’ attorneys’),\n(’ negotiations’, ’ negotiating’),\n(’ legally’, ’ arbitration’),\n(’ recruited’, ’ recruiting’),\n(’ legally’, ’ exemptions’),\n(’ legal’, ’ judicial’),\n(’ voting’, ’ Vote’),\n(’ negotiated’, ’ negotiating’),\n(’ legislative’, ’ veto’),\n(’ funding’, ’ funded’)\nGPT-2 Medium - Layer 17 Head 7\n(’tar’, ’idia’),\n(’ [...]’, ’...\"’),\n(’ lecture’, ’ lectures’),\n(’ Congress’, ’ senate’),\n(’ staff’, ’ staffers’),\n(’ Scholarship’, ’ collegiate’),\n(’ executive’, ’ overseeing’),\n(’ Scholarship’, ’ academic’),\n(’ academ’, ’ academic’),\n(’.\"’, ’...\"’),\n(’ [’, ’...\"’),\n(’\";’, ’...\"’),\n(’ Memorial’, ’priv’),\n(’ festival’, ’conference’),\n(’crew’, ’ supervisors’),\n(’ certification’, ’ grading’),\n(’ scholarship’, ’ academic’),\n(’ rumored’, ’ Academic’),\n(’ Congress’, ’ delegated’),\n(’ staff’, ’ technicians’),\n(’Plex’, ’ CONS’),\n(’ congress’, ’ senate’),\n(’ university’, ’ tenure’),\n(’ Congress’, ’ appointed’),\n(’ Congress’, ’ duly’),\n(’ investigative’, ’ investig’),\n(’ legislative’, ’ senate’),\n(’ademic’, ’ academic’),\n(’bench’, ’ academic’),\n(’ scholarship’, ’ tenure’),\n(’ campus’, ’ campuses’),\n(’ staff’, ’ Facilities’),\n(’ Editorial’, ’mn’),\n(’ clinic’, ’ laboratory’),\n(’ crew’, ’ crews’),\n(’ Scholarship’, ’ academ’),\n(’ staff’, ’ staffer’),\n(’icken’, ’oles’),\n(’?\"’, ’...\"’),\n(’ Executive’, ’ overseeing’),\n(’ academic’, ’ academ’),\n(’ Congress’, ’atra’),\n(’aroo’, ’anny’),\n(’ academic’, ’ academia’),\n(’ Congress’, ’ Amendments’),\n(’ academic’, ’ academics’),\n(’student’, ’ academic’),\n(’ committee’, ’ convened’),\n(’\",’, ’...\"’),\n(’ove’, ’idia’)\nGPT-2 Medium - Layer 16 Head 13\n(’ sugg’, ’ hindsight’),\n(’ sugg’, ’ anecdotal’),\n(’ unsuccessfully’, ’ hindsight’),\n(’didn’, ’ hindsight’),\n(’orously’, ’staking’),\n(’illions’, ’uries’),\n(’until’, ’era’),\n(’ lobbied’, ’ hindsight’),\n(’ incorrectly’, ’ incorrect’),\n(’ hesitate’, ’ hindsight’),\n(’ECA’, ’ hindsight’),\n(’ regret’, ’ regrets’),\n(’inventoryQuantity’, ’imore’),\n(’consider’, ’ anecdotal’),\n(’ errone’, ’ incorrect’),\n(’ someday’, ’ eventual’),\n(’illions’, ’Murray’),\n(’ recently’, ’recent’),\n(’ Learned’, ’ hindsight’),\n(’before’, ’ hindsight’),\n(’ lately’, ’ealous’),\n(’upon’, ’rity’),\n(’ja’, ’ hindsight’),\n(’ regretted’, ’ regrets’),\n(’ unsuccessfully’, ’udging’),\n(’ lately’, ’dated’),\n(’ sugg’, ’ anecd’),\n(’ inform’, ’imore’),\n(’ lately’, ’recent’),\n(’ anecd’, ’ anecdotal’),\n(’orously’, ’ hindsight’),\n(’ postwar’, ’ Era’),\n(’ lately’, ’ recent’),\n(’ skept’, ’ cynicism’),\n(’ sugg’, ’informed’),\n(’ unsuccessfully’, ’ealous’),\n(’ebin’, ’ hindsight’),\n(’ underest’, ’ overest’),\n(’ Jinn’, ’ hindsight’),\n(’ someday’, ’2019’),\n(’ recently’, ’turned’),\n(’ sugg’, ’ retrospect’),\n(’ unsuccessfully’, ’didn’),\n(’ unsuccessfully’, ’gged’),\n(’ mistakenly’, ’ incorrect’),\n(’assment’, ’)</’),\n(’ja’, ’didn’),\n(’illions’, ’ hindsight’),\n(’ sugg’, ’ testimony’),\n(’jri’, ’ hindsight’)\nGPT-2 Medium - Layer 12 Head 9\n(’ PST’, ’ usual’),\n(’etimes’, ’ foreseeable’),\n(’uld’, ’uld’),\n(’ Der’, ’ Mankind’),\n(’ statewide’, ’ yearly’),\n(’ guarantees’, ’ guarantees’),\n(’ Flynn’, ’ Logged’),\n(’borne’, ’ foreseeable’),\n16148\n(’ contiguous’, ’ contiguous’),\n(’ exceptions’, ’ exceptions’),\n(’ redist’, ’ costly’),\n(’ downstream’, ’ day’),\n(’ ours’, ’ modern’),\n(’ foreseeable’, ’ foreseeable’),\n(’ Posted’, ’ Posted’),\n(’ anecdotal’, ’ anecdotal’),\n(’ moot’, ’ costly’),\n(’ successor’, ’ successor’),\n(’ any’, ’ ANY’),\n(’ generational’, ’ modern’),\n(’ temporarily’, ’ costly’),\n(’ overall’, ’ overall’),\n(’ effective’, ’ incentiv’),\n(’ future’, ’ tomorrow’),\n(’ ANY’, ’ lifetime’),\n(’ dispatch’, ’ dispatch’),\n(’ legally’, ’ WARRANT’),\n(’ guarantees’, ’ incentiv’),\n(’ listed’, ’ deductible’),\n(’ CST’, ’ foreseeable’),\n(’ anywhere’, ’ any’),\n(’ guaranteed’, ’ incentiv’),\n(’ successors’, ’ successor’),\n(’ weekends’, ’ day’),\n(’iquid’, ’ expensive’),\n(’ Trib’, ’ foreseeable’),\n(’ phased’, ’ modern’),\n(’ constitutionally’, ’ foreseeable’),\n(’ any’, ’ anybody’),\n(’ anywhere’, ’ ANY’),\n(’ veto’, ’ precedent’),\n(’ veto’, ’ recourse’),\n(’ hopefully’, ’ hopefully’),\n(’ potentially’, ’ potentially’),\n(’ ANY’, ’ ANY’),\n(’ substantive’, ’ noteworthy’),\n(’morrow’, ’ day’),\n(’ancial’, ’ expensive’),\n(’listed’, ’ breastfeeding’),\n(’ holiday’, ’ holidays’)\nGPT-2 Medium - Layer 11 Head 10\n(’ Journalism’, ’ acron’),\n(’ democracies’, ’ governments’),\n(’/-’, ’verty’),\n(’ legislatures’, ’ governments’),\n(’ocracy’, ’ hegemony’),\n(’osi’, ’ RAND’),\n(’ Organizations’, ’ organisations’),\n(’ellectual’, ’ institutional’),\n(’ Journalists’, ’ acron’),\n(’eworks’, ’ sponsors’),\n(’ Inqu’, ’ reviewer’),\n(’ocracy’, ’ diversity’),\n(’ careers’, ’ Contributions’),\n(’gency’, ’\\\\-’),\n(’ellectual’, ’ exceptions’),\n(’ Profession’, ’ specializing’),\n(’online’, ’ Online’),\n(’ Publications’, ’ authorised’),\n(’Online’, ’ Online’),\n(’ sidx’, ’ Lazarus’),\n(’eworks’, ’ Networks’),\n(’ Groups’, ’ organisations’),\n(’ Governments’, ’ governments’),\n(’ democracies’, ’ nowadays’),\n(’ psychiat’, ’ Mechdragon’),\n(’ educ’, ’ Contributions’),\n(’ Ratings’, ’ organisations’),\n(’vernment’, ’spons’),\n(’...\"’, ’),\"’),\n(’ Caucas’, ’ commodity’),\n(’ dictators’, ’ governments’),\n(’istration’, ’ sponsor’),\n(’iquette’, ’ acron’),\n(’ Announce’, ’ answ’),\n(’ Journalism’, ’ empowering’),\n(’Media’, ’ bureaucr’),\n(’ Discrimination’, ’ organizations’),\n(’ Journalism’, ’Online’),\n(’FAQ’, ’sites’),\n(’ antitrust’, ’ Governments’),\n(’...\"’, ’...\"’),\n(’Questions’, ’ acron’),\n(’rities’, ’ organisations’),\n(’ Editorial’, ’ institutional’),\n(’ tabl’, ’ acron’),\n(’ antitrust’, ’ governments’),\n(’ Journalism’, ’ Everyday’),\n(’icter’, ’ Lieberman’),\n(’ defect’, ’SPONSORED’),\n(’ Journalists’, ’ organisations’)\nGPT-2 Medium - Layer 22 Head 5 (names and parts of names\nseem to attend to each other here)\n(’ Smith’, ’ovich’),\n(’ Jones’, ’ovich’),\n(’ Jones’, ’Jones’),\n(’ Smith’, ’Williams’),\n(’ Rogers’, ’opoulos’),\n(’Jones’, ’ovich’),\n(’ Jones’, ’inez’),\n(’ug’, ’ Ezek’),\n(’ Moore’, ’ovich’),\n(’orn’, ’roit’),\n(’van’, ’actionDate’),\n(’ Jones’, ’inelli’),\n(’ Edwards’, ’opoulos’),\n(’ Jones’, ’ Lyons’),\n(’Williams’, ’opoulos’),\n(’Moore’, ’ovich’),\n(’ Rodriguez’, ’hoff’),\n(’ North’, ’ suburbs’),\n(’ Smith’, ’chio’),\n(’Smith’, ’ovich’),\n(’ Smith’, ’opoulos’),\n(’Mc’, ’opoulos’),\n(’Johnson’, ’utt’),\n(’ Jones’, ’opoulos’),\n(’Ross’, ’Downloadha’),\n(’pet’, ’ilage’),\n(’ Everett’, ’ Prairie’),\n(’ Cass’, ’isma’),\n(’ Jones’, ’zynski’),\n(’Jones’, ’Jones’),\n(’ McCl’, ’elman’),\n(’ Smith’, ’Jones’),\n(’ Simmons’, ’opoulos’),\n(’ Smith’, ’brown’),\n(’ Mc’, ’opoulos’),\n(’ Jones’, ’utt’),\n(’ Richards’, ’Davis’),\n(’ Johnson’, ’utt’),\n(’ Ross’, ’bred’),\n(’ McG’, ’opoulos’),\n(’ Stevens’, ’stadt’),\n(’ra’, ’abouts’),\n(’ Johnson’, ’hoff’),\n16149\n(’ North’, ’ Peninsula’),\n(’ Smith’, ’Smith’),\n(’Jones’, ’inez’),\n(’ Hernandez’, ’hoff’),\n(’ Lucas’, ’Nor’),\n(’ Agu’, ’hoff’),\n(’Jones’, ’utt’)\nGPT-2 Medium - Layer 19 Head 12\n(’ 2015’, ’ADVERTISEMENT’),\n(’ 2014’, ’2014’),\n(’ 2015’, ’2014’),\n(’ 2015’, ’Present’),\n(’ 2013’, ’2014’),\n(’ 2017’, ’ADVERTISEMENT’),\n(’ 2016’, ’ADVERTISEMENT’),\n(’itor’, ’ Banner’),\n(’2015’, ’ Bulletin’),\n(’2012’, ’ Bulletin’),\n(’2014’, ’ Bulletin’),\n(’ Airl’, ’Stream’),\n(’2016’, ’ Bulletin’),\n(’ 2016’, ’2014’),\n(’2017’, ’ Bulletin’),\n(’ 2013’, ’ 2014’),\n(’ 2012’, ’2014’),\n(’ stadiums’, ’ventions’),\n(’ 2015’, ’ Bulletin’),\n(’2013’, ’ Bulletin’),\n(’ 2017’, ’2014’),\n(’ 2011’, ’ 2011’),\n(’ 2014’, ’ 2014’),\n(’ 2011’, ’ 2009’),\n(’ mile’, ’eming’),\n(’ 2013’, ’ADVERTISEMENT’),\n(’ 2014’, ’2015’),\n(’ 2014’, ’Present’),\n(’ 2011’, ’2014’),\n(’ 2011’, ’2009’),\n(’ 2015’, ’ 2014’),\n(’ 2013’, ’ Bulletin’),\n(’ 2015’, ’2015’),\n(’ 2011’, ’ 2003’),\n(’ 2011’, ’ 2010’),\n(’ 2017’, ’Documents’),\n(’2017’, ’iaries’),\n(’ 2013’, ’2015’),\n(’2017’, ’Trend’),\n(’ 2011’, ’2011’),\n(’ 2016’, ’Present’),\n(’ 2011’, ’ 2014’),\n(’ years’, ’years’),\n(’Plug’, ’Stream’),\n(’ 2014’, ’ADVERTISEMENT’),\n(’2015’, ’Present’),\n(’ 2018’, ’thora’),\n(’ 2017’, ’thora’),\n(’ 2012’, ’ 2011’),\n(’ 2012’, ’ 2014’)\nC.3 Feedforward Keys and Values\nKey-value pairs, (ki,vi), where at least 15% of the top-kvo-\ncabulary items overlap, with k = 100. We follow our fore-\nrunner’s convention of calling the index of the value in the\nlayer “dimension” (Dim).\nHere again we use two asterisks ( **) to represent lists\nwhere we discarded tokens outside the corpus vocabulary.\nGPT-2 Medium - Layer 0 Dim 116\n#annels #Els\n#netflix #osi\ntelev #mpeg\n#tv #vous\n#avi #iane\n#flix transmitter\nTelevision Sinclair\n#outube Streaming\n#channel #channel\nVid mosqu\n#Channel broadcaster\ndocumentaries airs\n#videos Broadcasting\nHulu broadcasts\nchannels streams\n#levision channels\nDVDs broadcasters\nbroadcasts broadcasting\n#azeera #RAFT\nMPEG #oded\ntelevised htt\naired transmissions\nbroadcasters playback\nStreaming Instruction\nviewership nic\n#TV Sirius\nKodi viewership\nITV radio\n#ovies #achers\nchannel channel\nGPT-2 Medium - Layer 3 Dim 2711\npurposes purposes\nsake sake\npurpose reasons\nreasons purpose\nconvenience ages\nreason reason\nSeasons #ummies\n#Plex #going\nReasons foreseeable\n#ummies Reasons\n#asons #reason\n#lation #pur\n#alsh Developers\n#agos #akers\n#ACY transl\nSTATS Reason\n#itas consideration\nages #purpose\n#purpose beginners\n#=[ awhile\n#gencies Pur\nMillennium #benefit\nBrewers #atel\nFestival #tun\nEVENT pur\n#payment Ages\n#=- preservation\n#printf Metatron\nbeginners um\nExpo #KEN\nGPT-2 Medium - Layer 4 Dim 621\n#ovie headlined\nnewspapers pestic\ntelevision dime\neditorial describ\n#journal Afric\nbroadcasters broadcasts\n16150\n#Journal #(’\npublication #umbnails\nNewsweek #adish\nZeit #uggest\ncolumnist splash\nEditorial #ZX\nnewsletter objectionable\ncartoon #article\n#eport Bucc\ntelev #London\nradio reprint\nheadlined #azine\n#ribune Giov\nBBC #ender\nreprint headline\nsitcom #oops\nreprinted #articles\nbroadcast snipp\ntabloid Ajax\ndocumentaries marqu\njournalist #(\"\nTV #otos\nheadline mast\nnews #idem\nGPT-2 Medium - Layer 7 Dim 72\nsessions session\ndinners sessions\n#cation #cation\nsession #iesta\ndinner Booth\n#eteria screenings\nDinner booked\n#Session #rogram\nrehears vacation\nbaths baths\nLunch #pleasant\n#hops meetings\nvisits #Session\nSession greet\n#session #athon\nmeetings Sessions\nchatting boarding\nlunch rituals\nchats booking\nfestivities Grape\nboarding #miah\n#workshop #session\n#rooms Pars\n#tests simulated\nseated Dispatch\nvisit Extras\nappointments toile\n#vu Evening\n#rations showers\n#luaj abroad\nGPT-2 Medium - Layer 10 Dim 8\nMiy Tai\n#imaru #jin\nGong Jin\nJinn Makoto\nXia #etsu\nMakoto Shin\nKuro Hai\nShin Fuj\n#Tai Dai\nYamato Miy\nTai #iku\nIchigo Yun\n#Shin Ryu\n#atsu Shu\nHaku Hua\nChun Suzuki\n#ku Yang\nQing Xia\nTsuk #Shin\nHua #iru\nJiang Yu\nNanto #yu\nmanga Chang\nYosh Nan\nyen Qian\nOsaka #hao\nQian Fuk\n#uku Chun\n#iku Yong\nYue #Tai\nGPT-2 Medium - Layer 11 Dim 2\nprogressing toward\n#Progress towards\n#progress Pace\n#osponsors progression\n#oppable #inness\nadvancement onward\nprogress canon\nProgress #progress\n#senal pace\n#venge #peed\nqueue advancement\n#pun advancing\nprogression progressing\n#wagon ladder\nadvancing path\n#cknowled honoring\n#Goal ranks\nmomentum standings\n#zag goal\n#hop #grand\npursuits momentum\n#encing #ometer\n#Improve timetable\nSTEP nearing\n#chini quest\nstandings spiral\n#eway trajectory\n#chie progress\n#ibling accelerating\nEsports escal\nGPT-2 Medium - Layer 15 Dim 4057\nEDITION copies\nversions Version\ncopies #edition\nversion #Version\nVersion version\nedition #download\neditions download\nreprint versions\n#edition #Download\nEDIT copy\nEdition #release\nreproduce #version\noriginals release\n#edited #copy\nVERS VERS\n#Versions #pub\n#Publisher Download\nreprodu #released\n16151\n#uploads editions\nplaythrough edition\nPrinted reprint\nreproduction Release\n#Reviewed #Available\ncopy #published\n#Version #Published\npaperback EDITION\npreview print\nsurv #Quantity\n#Download #available\ncirculate RELEASE\nGPT-2 Medium - Layer 16 Dim 41\n#duino alarm\n#Battery alarms\nMorse signal\nalarms circuit\nGPIO GPIO\nLEDs timers\nbatteries voltage\n#toggle signals\nsignal circuitry\ncircuitry electrical\n#PsyNetMessage circuits\nalarm LEDs\nautop standby\nsignalling signalling\n#volt signaling\nvolt lights\nsignals Idle\nvoltage triggers\nLED batteries\nelectrom Morse\ntimers LED\nmalfunction #LED\namplifier button\nradios Signal\nwiring timer\n#Alert wiring\nsignaling buzz\n#Clock disconnect\narming Arduino\nArduino triggered\nGPT-2 Medium - Layer 17 Dim 23\nresponsibility responsibility\nResponsibility respons\nresponsibilities responsibilities\n#ipolar Responsibility\n#responsible oversee\nduties #respons\n#respons duties\nsuperv supervision\nsupervision superv\n#abwe stewards\nAdin chore\nrespons oversight\noversee oversees\nentrusted responsible\noverseeing #responsible\nhelicop handling\npresided handles\noverseen overseeing\n#dyl chores\nresponsible manage\n#ADRA managing\nreins duty\n#accompan Respons\nchores charge\noversees reins\nsupervised handle\nblame oversaw\noversaw CONTROL\n#archment RESP\nRESP tasks\nGPT-2 Medium - Layer 19 Dim 29\nsubconscious thoughts\nthoughts thought\n#brain Thoughts\n#Brain minds\nmemories mind\nOCD thinking\nflashbacks #thought\nbrainstorm imagination\nAnxiety Thinking\n#mind Thought\nfantas imagin\namygdala thinker\nimpuls #thinking\nThinking #mind\n#Memory memories\nThoughts #think\ndreams imagining\n#ocamp impulses\n#Psych fantasies\n#mares think\nmentally urges\n#mental desires\nmind dreams\n#thinking delusions\n#Mind subconscious\n#dream emotions\npsyche imag\nprefrontal #dream\nPTSD conscience\nMemories visions\nGPT-2 Medium - Layer 20 Dim 65\nexercises volleyball\n#Sport tennis\n#athlon sports\nExercise sport\n#ournaments #basketball\nvolleyball Tennis\nRecre soccer\nMahjong golf\n#basketball playground\nexercise Golf\nbowling athletics\nskating #athlon\nspar athletic\nskiing rugby\ngymn amusement\n#sports gymn\ndrills sled\n#Training #Sport\ntournaments cricket\nsled Soccer\nVolunte amuse\nskate Activities\ngolf recreational\n#Pract Ski\ndunk activities\n#hower basketball\nathletics #games\nsport skating\nSolitaire hockey\n#BALL #sports\n16152\nGPT-2 Medium - Layer 21 Dim 86\nIDs number\nidentifiers #number\nsurname #Number\nsurn Number\nidentifier NUM\ninitials numbers\n#Registered Numbers\nNAME #Numbers\n#names address\npseudonym #address\n#codes #Num\nnomine #NUM\nnames addresses\nusername Address\n#IDs identifier\nID #Address\nregistration #num\n#76561 ID\n#soDeliveryDate numbering\n#ADRA IDs\nCLSID #ID\nnumbering identifiers\n#ername identification\n#address numer\naddresses digits\ncodes #numbered\n#Names numerical\nregist Ident\nname numeric\nNames Identification\nGPT-2 Medium - Layer 21 Dim 400\n#July Oct\nJuly Feb\n#February Sept\n#January Dec\n#Feb Jan\nNovember Nov\n#October Aug\nJanuary #Oct\nFeb May\nOctober #Nov\n#September Apr\nSeptember March\n#June April\n#Sept #Sept\nFebruary June\n#November #Aug\n#April October\nApril #Feb\nJune July\n#December December\nAugust Sep\n#March November\nSept #Jan\nDecember #May\nAug August\nMarch Jul\n#August Jun\n#Aug September\n#wcs January\nApr February\nGPT-2 Medium - Layer 23 Dim 166\n#k #k\n#ks #K\n#kish #ks\n#K #KS\n#kat k\n#kus #kt\n#KS K\n#ked #kr\n#kr #kl\n#kB #kish\n#kan #kos\n#kw #king\n#ket #ked\n#king #kie\n#kb #KB\n#kos #kk\n#kHz #kowski\n#kk #KR\n#kick #KING\n#kers #KT\n#kowski #KK\n#KB #KC\n#krit #kw\n#KING #kb\n#kt #Ka\n#ksh #krit\n#kie #KN\n#ky #kar\n#KY #kh\n#ku #ket\nGPT-2 Medium - Layer 23 Dim 907\nhands hand\nhand #Hand\n#hands Hand\n#hand #hand\nfingers hands\n#feet Hands\nfingertips fist\nclaws #hands\npaw finger\npaws handed\nmetab thumb\npalms fingers\nfingert foot\n#Hand #handed\nfists paw\nwrists handing\nlevers #finger\nthumbs #hander\ntentacles fingertips\nfeet claw\nlimb fingert\nslider #Foot\n#handed Stick\n#dimension arm\njaws #Accessory\nskelet #fing\nlapt Foot\nankles index\nweap toe\nfoot #auntlet\nGPT-2 Large - Layer 25 Dim 2685**\n#manager engineering\n#Engineers Marketing\nchemist #engineering\nhumanities Communications\nsciences #communications\nanthropology anthropology\nlingu Engineering\n#engineering lingu\npsychologist psychology\nCoordinator neurolog\n16153\nAnalyst Economics\n#iologist designer\naccountant sociology\nstrategist communications\n#ographer marketing\ncurator pharmac\nEngineers sciences\narchae economics\nDesigner Accounting\nEditing #econom\nbiologist chemist\n#ologist merch\npsychologists pharm\ntheolog economist\nMarketing architect\n#Manager engineer\nArchitects Architect\nsociology #technical\nengineer architects\nphysicist logistics\nGPT-2 Large - Layer 21 Dim 3419**\n#overty impoverished\n#wana poverty\npoverty poorest\n#Saharan poorer\npoorest Yemen\nPoverty families\nmalnutrition Poverty\nSenegal marginalized\nimpoverished refugees\n#poor subsistence\nGujar displaced\nhomelessness hardship\nHomeless refugee\n#heid households\nRamadan migrant\n#Palest disadvantaged\npoorer Sudan\nRahman oppressed\n#amily socioeconomic\nilliter peasant\nMahmoud homeless\nHaitian poor\n#advertisement Ethiopian\n#hya Kaf\n#African Rw\nwealthier #poor\nAfricans Af\ncaste rural\nhomeless #fam\nHait needy\nGPT-2 Large - Layer 25 Dim 2442**\nTracker tracking\ngau Tracker\ncharts tracker\ntracker Tracking\n#Measure quant\nmeasurement #Stats\nmeasuring gau\n#Tracker GPS\ngauge Track\ntracking estimating\nTracking tally\n#Monitor #ometers\n#chart tracked\nMeter calculate\n#HUD calculating\n#ometers measurement\nsurve gauge\n#Stats estimation\n#Statistics monitoring\ncalculate #stats\nMeasure #tracking\nquant track\n#asuring measuring\nCalculator Monitoring\n#ometer #Detailed\ncalculator #ometer\nMonitoring estim\n#Maps stats\npione charts\ntimet timet\nGPT-2 Base - Layer 9 Dim 1776\nradios cable\nantennas modem\nradio wireless\nmodem WiFi\nvoltage wired\ntransformer broadband\nEthernet Ethernet\ntelev radios\n#Radio power\nelectricity radio\nloudspe Cable\nkW Wireless\n#radio telephone\nbroadband network\nvolt signal\nmicrophones Networks\ntelecommunications networks\ncable electricity\nTelephone wifi\namplifier #levision\nwifi coax\nbroadcasting transmit\ntransistor transmitter\nRadio TV\nwireless Network\nLTE television\nwatts transmission\nmicrowave router\ntelephone cables\namps amplifier\nGPT-2 Base - Layer 9 Dim 2771\narous increase\nfreeing increasing\nincent accelerating\nstimulate allev\ninduce exped\ndiscourage enhanced\ninducing aggrav\nmitigating enhance\nstimulating inhib\nemanc improving\nalleviate infl\nempowering #oint\npreventing alien\n#ufact alter\n#HCR enabling\ninfluencing incre\nhandc indu\ndisadvant #Impro\n#roying intens\narresting improve\nallev easing\n16154\nweaken elevate\ndepri encouraging\ndissu accelerate\nimpede enlarg\nconvol energ\nencouraging accent\n#xiety acceler\n#akening depri\nlowering elong\nGPT-2 Base - Layer 1 Dim 2931\nevening week\n#shows evening\nnight night\n#sets morning\n#lav afternoon\nafternoon month\n#/+ #’s\nNight #naissance\nLoll #genre\nKinnikuman semester\nWeekend #ched\nmorning #ague\n#enna weekend\nSaturday latest\nSunday #cher\nweek #EST\nBlossom #icter\n#Night happens\n#atto day\n#vertising happened\n#spr #essim\n#Sunday Masquerade\n#morning #ished\n#Thursday sounded\nWeek #ching\nPanc pesky\nEvening #chy\n#allery trope\n#ADVERTISEMENT #feature\n#Street #fy\nGPT-2 Base - Layer 0 Dim 1194\nPay receipts\n#Pay depos\nrefund Deposit\npolice deduct\n#pay #milo\n#paying #igree\n#Tax #eln\ndebit levied\nPayPal deposit\nATM #enforcement\ncops endot\ntax #soType\nID paperwork\n#payment deposits\npayment loopholes\ncheckout waivers\n#police receipt\nagents waive\nDMV loophole\napplication arresting\ncard commissioner\napplications Forms\noffice transporter\narrested Dupl\n#paid confisc\npay Clapper\n#tax #ventures\nRCMP #Tax\nPAY whistleblowers\nAPPLIC #ADRA\nGPT-2 Base - Layer 9 Dim 2771\nflaws flaws\nlurking weaknesses\nfailings dangers\nvulnerabilities scams\ninaccur shortcomings\nscams pitfalls\nshortcomings injust\nflawed faults\nglitches flawed\npitfalls abuses\ninconsistencies imperfect\nrigged lurking\nbiases wrongdoing\ndeficiencies corruption\nweaknesses inaccur\ndiscrepancies inadequ\nhypocrisy fraud\nrigging inequ\ndeceptive weakness\nmisinformation scam\n#urities hazards\nlur problematic\nimperfect hoax\nregress danger\n#abase failings\n#errors problems\n#lived injustice\nabuses plagiar\nmisinterpret plag\nsuspic deceptive\nC.4 Knowledge Lookup\nGiven a few seed embeddings of vocabulary items we find\nrelated FF values by taking a product of the average embed-\ndings with FF values.\nSeed vectors:\n[\"python\", \"java\", \"javascript\"]\nLayer 14 Dim 1215 (ranked 3rd)\nfilesystem\ndebugging\nWindows\nHTTP\nconfigure\nPython\ndebug\nconfig\nLinux\nJava\nconfiguration\ncache\nUnix\nlib\nruntime\nkernel\nplugins\nvirtual\nFreeBSD\nhash\nplugin\nheader\nfile\nserver\nPHP\n16155\nGNU\nheaders\nApache\ninitialization\nMozilla\nSeed vectors: [\"cm\", \"kg\", \"inches\"]\nLayer 20 Dim 2917 (ranked 1st)\npercent\nyears\nhours\nminutes\nmillion\nseconds\ninches\nmonths\nmiles\nweeks\npounds\n#%\nkilometers\nounces\nkilograms\ngrams\nkilometres\nmetres\ncentimeters\nthousand\ndays\nkm\nyards\nYears\nmeters\n#million\nacres\nkg\n#years\ninch\nSeed vectors: [\"horse\", \"dog\", \"lion\"]\nLayer 21 Dim 3262 (ranked 2nd)\nanimal\nanimals\nAnimal\ndogs\nhorse\nwildlife\nAnimals\nbirds\nhorses\ndog\nmammal\nbird\nmammals\npredator\nbeasts\nWildlife\nspecies\n#Animal\n#animal\nDogs\nfish\nrabbits\ndeer\nelephants\nwolves\npets\nveterinary\ncanine\nbeast\npredators\nreptiles\nrodent\nprimates\nhunting\nlivestock\ncreature\nrabbit\nrept\nelephant\ncreatures\nhuman\nhunters\nhunter\nshark\nRept\ncattle\nwolf\nHumane\ntiger\nlizard\n16156\nD Sentiment Analysis Fine-Tuning Vector Examples\nThis section contains abusive language\nClassification Head Parameters\nBelow we show the finetuning vector of the classifier weight. “POSITIVE” designates the vector corresponding to the label\n“POSITIVE”, and similarly for “NEGATIVE”.\nPOSITIVE NEGATIVE\n----------- ------------\n#yssey bullshit\n#knit lame\n#etts crap\npassions incompetent\n#etooth inco\n#iscover bland\npioneers incompetence\n#emaker idiots\nPione crappy\n#raft shitty\n#uala idiot\nprosper pointless\n#izons retarded\n#encers worse\n#joy garbage\ncherish CGI\nloves FUCK\n#accompan Nope\nstrengthens useless\n#nect shit\ncomr mediocre\nhonoured poorly\ninsepar stupid\nembraces inept\nbattled lousy\n#Together fuck\nintrig sloppy\n#jong Worse\nfriendships Worst\n#anta meaningless\nIn the following sub-sections, we sample 4 difference vectors per each parameter group (FF keys, FF values; attention query,\nkey, value, and output subheads), and each one of the fine-tuned layers (layers 9-11). We present the ones that seemed to contain\nrelevant patterns upon manual inspection. We also report the number of “good” vectors among the four sampled vectors for\neach layer and parameter group.\nFF Keys\nLayer 9\n4 out of 4\n16157\ndiff -diff\n----------- ---------------\namazing seiz\nmovies coerc\nwonderful Citiz\nlove #cffff\nmovie #GBT\ncinematic targ\nenjoyable looph\nwonderfully Procedures\nbeautifully #iannopoulos\nenjoy #Leaks\nfilms #ilon\ncomedy grievance\nfantastic #merce\nawesome Payments\n#Enjoy #RNA\ncinem Registrar\nfilm Regulatory\nloving immobil\nenjoyment #bestos\nmasterpiece #SpaceEngineers\ndiff -diff\n--------------- ------------\nreperto wrong\ncongratulations unreasonable\nCitation horribly\nthanks inept\nRecording worst\nrejo egregious\nProfile #wrong\nTradition unfair\ncanopy worse\n#ilion atro\nextracts stupid\ndescendant egreg\n#cele bad\nenthusiasts terribly\n:-) ineffective\n#photo nonsensical\nawaits awful\nbeliever #worst\n#IDA incompetence\nwelcomes #icably\ndiff -diff\n------- ----------\nmovie seiz\nfucking Strongh\nreally #etooth\nmovies #20439\ndamn #Secure\nfunny Regulation\nshit Quarterly\nkinda concess\nREALLY Recep\nMovie #aligned\nstupid targ\n#movie mosqu\ngoddamn #verning\ncrap FreeBSD\nshitty PsyNet\nfilm Facilities\ncrappy #Lago\ndamned #Register\n#Movie #\"}],\"\ncheesy Regist\ndiff -diff\n------------ ------------\nincompetence #knit\nbullshit #Together\ncrap Together\nuseless versatile\npointless #Discover\nincompetent richness\nidiots #iscover\nincompet forefront\ngarbage inspiring\nmeaningless pioneering\nstupid #accompan\ncrappy unparalleled\nshitty #Explore\nnonexistent powerfully\nworthless #\"},{\"\nWorse #love\nlame admired\nworse #uala\ninco innovative\nineffective enjoyed\nLayer 10\n4 out of 4\n16158\ndiff -diff\n--------------- -------------\nquotas wonderfully\n#RNA wonderful\ncessation beautifully\nsubsidy amazing\n#SpaceEngineers fantastic\nplacebo incredible\nexemptions amazingly\ntreadmill great\nLabs unforgettable\nreceipt beautiful\nmoratorium brilliantly\ndesignation hilarious\nineligible love\nreimbursement marvelous\nroundup vividly\nArticles terrific\nPubMed memorable\nwaivers #Enjoy\nCitiz loving\nlandfill fascinating\ndiff -diff\n------------------ -------------\nisEnabled wonderfully\nguiActiveUnfocu... beautifully\n#igate cinem\nwaivers cinematic\nexpires wonderful\nexpire amazing\nreimb Absolutely\nexpired storytelling\n#rollment fantastic\n#Desktop Definitely\nprepaid unforgettable\n#verning comedy\n#andum movie\nreimbursement comedic\nAdvisory hilarious\npermitted #movie\n#pta #Amazing\nissuance scenes\nPriebus Amazing\n#iannopoulos enjoyable\ndiff -diff\n------------ ----------\nhorror #deals\nwhim #iband\nsubconscious [&\nunrealistic #heid\nimagination #APD\nviewers withdrew\nenjoyment #Shares\nnostalgia mathemat\nabsolute [+]\nsentimental #Tracker\nunreal #zb\nKubrick testified\nawe #ymes\ninspiration mosqu\nsubtle #Commerce\ncinematic administr\nperfection feder\ncomedic repaired\nfantasy #pac\nmindless #Community\ndiff -diff\n------------- -------------\n#Leaks loving\nquotas love\n#RNA loved\nsubsidy lovers\n#?’\" wonderful\nPenalty lover\n#iannopoulos nostalgic\n#>] alot\ndiscredited beautiful\n#conduct amazing\n#pta great\nwaivers passionate\nAuthorization admire\n#admin passion\nHHS lovely\narbitrarily loves\n#arantine unforgettable\n#ERC proud\nmemorandum inspiration\n#Federal #love\nLayer 11\n4 out of 4\n16159\ndiff -diff\n----------- -----------\ninco cherish\npointless #knit\nNope #terday\nbullshit #accompan\ncrap prosper\nuseless versatile\nnonsense friendships\nfutile #uala\nanyways Lithuan\nanyway cherished\nmeaningless redes\nclueless inspires\nlame Proud\nwasting friendship\nbogus exceptional\nvomit #beaut\nnonsensical #ngth\nretarded pioneering\nidiots pioneers\nshit nurt\ndiff -diff\n--------------- -----------\n#SpaceEngineers love\nnuisance definitely\n#erous always\n#aband wonderful\nBrist loved\nracket wonderfully\nPenalty cherish\nbystand loves\n#iannopoulos truly\nCitiz enjoy\nCodec really\ncourier #olkien\n#>] beautifully\n#termination #love\nincapac great\n#interstitial LOVE\nfugitive never\nbreaching adore\ntarg loving\nthug amazing\ndiff -diff\n----------- ------------\n#accompan bad\nPione crap\ncelebrate inefficient\n#Discover stupid\n#knit worse\npioneering mistake\nrecogn incompetence\nreunited mistakes\ncomr incompetent\nthriving miser\n#iscover garbage\ncommemorate retarded\nRemem #bad\necstatic poor\nforefront ineffective\nenthusi retard\nrenewed Poor\ncolle bullshit\nInspired inept\n#uala errors\ndiff -diff\n------------ ------------\n#knit bullshit\npassions crap\n#accompan idiots\n#ossom goddamn\n#Explore stupid\nwelcomes shitty\npioneering shit\nforefront garbage\nembraces fuck\npioneers incompetence\nintertw crappy\n#izons bogus\n#iscover useless\nunparalleled idiot\nevolving #shit\nTogether pointless\nvibrant stupidity\nprosper fucking\nstrengthens nonsense\n#Together FUCK\nFF Values\nLayer 9\n0 out of 4\nLayer 10\n0 out of 4\nLayer 11\n0 out of 4\nWQ Subheads\nLayer 9\n3 out of 4\n16160\ndiff -diff\n------------ ------------\n#ARGET kinda\n#idal alot\n#--+ amazing\nPrev interesting\n#enger wonderful\n#iannopoulos definitely\n#report unbelievable\n#RELATED really\nissuance amazingly\n#earcher pretty\nPrevious nice\nLegislation absolutely\n#astical VERY\n#iper wonderfully\n#>[ incredible\n#</ hilarious\nVendor funny\n#\"> fantastic\n#phrine quite\n#wcsstore defin\ndiff -diff\n------------ -----------\nbullshit strengthens\nbogus Also\nfaux #helps\nspurious adjusts\nnonsense #ignt\nnonsensical evolves\ninept helps\ncrap grew\njunk grows\nshitty #cliffe\nfake recognizes\nincompetence #assadors\ncrappy regulates\nphony flourished\nsloppy improves\ndummy welcomes\nmediocre embraces\nlame gathers\noutrage greets\ninco prepares\ndiff -diff\n---------- ------------\nalot Provision\nkinda coerc\namazing Marketable\ndefinitely contingency\npretty #Dispatch\ntho seiz\nhilarious #verning\nVERY #iannopoulos\nreally #Reporting\nlol #unicip\nwonderful Fiscal\nthats issuance\ndont provision\npics #Mobil\ndoesnt #etooth\nunderrated policymakers\nfunny credential\nREALLY Penalty\n#love #activation\nalright #Officials\nLayer 10\n4 out of 4\n16161\ndiff -diff\n-------- ---------\ncrap #Register\nshit Browse\nbullshit #etooth\nstupid #ounces\nshitty #verning\nhorrible #raft\nawful #egu\nfucking #Lago\ncomedic Payments\ncrappy #orsi\ncheesy Coinbase\ncomedy #ourse\nfuck #iann\nmediocre #\"}],\"\nterrible #onductor\nmovie #obil\nbad #rollment\ngimmick #ivot\nfiller #Secure\ninept #ETF\ndiff -diff\n------------- ------------\nlove Worse\nunforgettable Nope\nbeautiful #Instead\nloved Instead\n#love #Unless\nloving incompetence\namazing incapable\n#joy Unless\ninspiring #failed\npassion incompet\nadventure incompetent\nloves ineffective\nexcitement #Fuck\njoy #Wr\nLOVE inept\ntogether spurious\nmemories #Failure\nwonderful worthless\nenjoyment obfusc\nthemes inadequate\ndiff -diff\n------------------ ------------\n#knit crap\n#\"},{\" bullshit\n#\"}],\" stupid\n#estones inept\n#Learn shit\n#ounces idiots\n#egu shitty\n#Growing crappy\n#ributes incompetence\n#externalAction... fuck\n#encers pointless\nBrowse nonsense\njointly nonsensical\nGrowing stupidity\n#ossom gimmick\nhonoured inco\n#accompan lame\n#agos incompetent\n#raft mediocre\n#iership bland\ndiff -diff\n--------- -----------\ncrap #egu\nbullshit #etooth\nshit #verning\n:( #ounces\nlol #accompan\nstupid coh\nfiller #assadors\nshitty #pherd\nfucking #acio\npointless #uchs\nidiots strengthens\nanyways #reprene\nnonsense Scotia\nanyway #rocal\ncrappy reciprocal\nstupidity Newly\nfuck fost\n#shit #ospons\nanymore #onductor\nNope governs\nLayer 11\n3 out of 4\n16162\ndiff -diff\n------------- ------------\n#utterstock amazing\n#ARGET movie\n#cffff alot\n#etooth scenes\n#Federal comedy\nPOLITICO movies\n#Register cinematic\n#Registration greatness\n#rollment wonderful\n#ETF storytelling\n#ulia film\nPayments tho\n#IRC masterpiece\nRegulatory films\nAlternatively Kubrick\n#RN realism\n#pta comedic\nRegulation cinem\n#GBT #movie\n#\":\"\"},{\" genre\ndiff -diff\n------------- ------------------\n#also meaningless\n#knit incompetence\nhelps inco\nstrengthens pointless\n:) incompetent\nbroaden Worse\n#ossom inept\nincorporates nonsensical\n#Learn coward\nincorporate unint\n#\"},{\" obfusc\nenjoy excuses\nenjoyed panicked\ncomplementary useless\n#etts bullshit\nenhances stupid\nintegrates incompet\n#ospons incomprehensibl...\ndiffers stupidity\n#arger lifeless\ndiff -diff\n------------- ---------------\namazing #iannopoulos\nbeautifully expired\nlove ABE\nwonderful Yiannopoulos\nwonderfully liability\nunforgettable #SpaceEngineers\nbeautiful #isance\nloving Politico\n#love waivers\n#beaut #utterstock\nenjoyable excise\n#Beaut #Stack\ninspiring phantom\nfantastic PubMed\ndefin #ilk\nincredible impunity\nmemorable ineligible\ngreatness Coulter\namazingly issuance\ntimeless IDs\nWK Subheads\nLayer 9\n3 out of 4\n16163\ndiff -diff\n------- ----------\nenclave horrible\n#. pretty\n#; alot\n#omial MUCH\napiece VERY\n#assian nothing\n#.</ #much\n#ulent terrible\n#,[ crappy\n#eria strange\n#ourse everything\nexerc very\n#\\/ shitty\n#Wire nice\n#arium many\n#icle wonderful\n#.[ genuinely\n#/$ beautiful\n#API much\n#ium really\ndiff -diff\n------------- -----------\nThen any\nInstead #ady\nUnfortunately #imate\nWhy #cussion\nSometimes #ze\nSecondly appreci\n#Then #raq\nBut currently\nLuckily #kers\nAnyway #apixel\nAnd active\nSuddenly significant\nThankfully #ade\nEventually #imal\nSomehow specific\nFortunately #ability\nMeanwhile anyone\nWhat #ker\nObviously #unction\nBecause reap\ndiff -diff\n----------- ---------\nbullshit #avorite\nanyway #ilyn\ncrap #xtap\nanyways #insula\nunless #cedented\nnonsense #aternal\n#falls #lyak\nfuck #rieve\n#. #uana\nfallacy #accompan\n#tics #ashtra\n#punk #icer\ndamned #andum\n#fuck Mehran\nstupidity #andise\nshit #racuse\ncommercials #assadors\nbecause #Chel\ndespite rall\nmovies #abella\nLayer 10\n2 out of 4\n16164\ndiff -diff\n------------ ------------\n#, Nope\nwork Instead\n#icle Thankfully\n#. Surely\noutdoors #Instead\ninspiring Fortunately\nexped Worse\nahead Luckily\ntogether #Thankfully\ntouches Unless\nout Apparently\npersonalized Perhaps\n#joy #Unless\n#unction #Fortunately\nwarm Sorry\nexceptional Secondly\nexperience #Luckily\nlasting #Rather\ninteg Hence\n#astic Neither\ndiff -diff\n-------- ---------\n#sup #etting\nAmazing #liness\n#airs #ktop\nawesome #ulkan\nBless #enthal\nLoving #enance\nmy #yre\n#OTHER #eeds\n#BW omission\n#perfect #reys\n#-) #lihood\namazing #esian\n#adult #holes\nperfect syndrome\nwelcome grievance\nRated offenders\n#Amazing #wig\n#anch #hole\nFANT #creen\n#anche #pmwiki\nLayer 11\n2 out of 4\ndiff -diff\n---------- ------------\nshots #Kind\nshit suscept\nbullshit Fathers\nstuff #Footnote\ntits concess\ncrap #accompan\nboobs Strait\ncreepy #orig\nnoises #ESE\nspectacle #ufact\nboring Founder\nthings #iere\neverything #HC\nnoise #Prev\n#anim #alias\nugly participated\ngarbage #Have\nstupidity #coe\nvisuals #Father\nselfies strugg\ndiff -diff\n-------------- -----------\n#ly #say\nstorytelling actionGroup\nsounding prefers\nspectacle #ittees\n#ness #reon\n#hearted presumably\ncinematic waivers\n#est #aucuses\nportrayal #Phase\nquality #racuse\npaced #arge\ncombination #hers\njuxtap #sup\nrepresentation #later\nmixture expired\n#!!!!! stricter\nfilmmaking #onds\nenough #RELATED\nthing #rollment\nrendition #orders\nWV Subheads\nLayer 9\n4 out of 4\n16165\ndiff -diff\n----------- ----------\n#\":\"\"},{\" honestly\n#etooth definitely\n#ogenesis hilarious\n#verning alot\nbroker amazing\n#ounces funn\nthreatens cinem\n#astical Cinem\nfoothold comedic\nintruder Absolutely\n#vernment comedy\n#activation absolutely\n#Oracle amazingly\nfugitive satire\nvisitor underrated\n#assian really\nbarrier fantastic\n#\":[ enjoyable\n#vier REALLY\n#oak wonderful\ndiff -diff\n-------- -------------\ncrap jointly\nshit #verning\nbullshit #pora\nfucking #rocal\nidiots #raft\nfuck #etooth\ngoddamn #estead\nstupid #ilitation\nFUCK #ourse\n#fuck migr\nshitty #ourses\ndamn #iership\n#shit Pione\nlol #iscover\nfuckin pioneering\nnonsense #egu\ncrappy #ivities\nkinda neighbourhood\nFuck pioneer\nidiot nurt\ndiff -diff\n------------ --------------\ncrap Pione\nbullshit pioneers\nshit complementary\nvomit pioneering\nnonsense #knit\nstupid #raits\nidiots Browse\nfucking #iscover\n#shit strengthened\nidiot #rocal\nfuck prosper\ngimmick Communities\nstupidity neighbourhoods\ngoddamn #Learn\nshitty strengthens\nincompetence #iscovery\nlame #ributes\nFUCK strengthen\ninco #izons\nblah Mutual\ndiff -diff\n--------- --------------\nanime #rade\nkinda #jamin\nstuff #ounces\nshit #pherd\nlol Unable\ntho #pta\nrealism Roche\ndamn Payments\n:) Gupta\nfucking #odan\nalot #uez\nmovie #adr\nfunny #ideon\nanyways #Secure\nenjoyable #raught\ncrap Bei\ncomedy sovere\ngenre unsuccessfully\nanyway #moil\nfun #Register\nLayer 10\n4 out of 4\n16166\ndiff -diff\n------------- ------------\n#knit crap\nwelcomes bullshit\nTogether idiots\nGrowing stupid\n#Explore shitty\npioneering incompetence\ncomplementary pointless\nmilestone goddamn\npioneer retarded\n#Together lame\nstrengthens Worse\n#ossom crappy\npioneers incompet\n#Learn shit\njointly stupidity\n#Growing fucking\nembraces Nope\n#\"},{\" FUCK\nsharing incompetent\n#Discover pathetic\ndiff -diff\n----------- ---------\n#\"}],\" crap\n#verning stupid\n#etooth shit\n#\"},{\" fucking\nBrowse fuck\n#Register shitty\n#Lago bullshit\n#raft crappy\n#egu idiots\njointly horrible\n#iership stupidity\nstrengthens kinda\nScotia goddamn\n#ounces awful\n#uania mediocre\n#iann pathetic\nworkspace #fuck\nseiz damn\nPayments FUCK\n#Learn damned\ndiff -diff\n------------ -------------\nbullshit inspiring\nincompetence unforgettable\nWorse #knit\nidiots #love\ncrap passions\ndummy cherish\nincompetent richness\nNope timeless\nstupid loves\nretarded passionate\nlame beautifully\nnonexistent overcoming\nwasting unique\n#Fuck highs\nbogus nurture\nworse unparalleled\nnonsense vibrant\nineligible #beaut\npointless intertw\ninco insepar\ndiff -diff\n------------ -------------\nbullshit Pione\ncrap pioneers\nstupid pioneering\nnonsense complementary\nincompetence #knit\nidiots #Learn\nshit #accompan\nstupidity pioneer\npointless invaluable\ninco #ossom\nretarded #Together\nidiot Browse\nvomit versatile\nlame welcomes\nmeaningless #\"},{\"\ngoddamn admired\nnonsensical jointly\ngarbage Sharing\n#shit Together\nuseless #Discover\nLayer 11\n4 out of 4\n16167\ndiff -diff\n------------ ------------\nProvision alot\nissuance amazing\nSecurities kinda\n#ogenesis fucking\nHoldings awesome\nRegulatory funny\nindefinitely damn\nAdvisory REALLY\ndesignation hilarious\nunilaterally tho\nProvince unbelievable\nRegulation fuckin\n#Lago wonderful\nissued doesnt\nRecep definitely\nAdvis thats\n#verning yeah\nbroker fantastic\n#Mobil badass\nPolicy dont\ndiff -diff\n------------ ---------\ncrap #rocal\nfucking #verning\nbullshit #etooth\nfuck #uania\ngoddamn caches\nshit Browse\n#fuck #\"},{\"\nstupidity #imentary\npathetic exerc\nspoiler #Lago\nstupid #\"}],\"\ninept #cium\nblah #enges\nFUCK #ysis\nawful quarterly\nshitty #iscover\ntrope Scotia\nGodd #resso\ninco #appings\nincompetence jointly\ndiff -diff\n-------------- ------------\npioneers bullshit\npioneering crap\nBrowse shit\nPione idiots\ncomplementary stupid\n#knit vomit\nprosper incompetence\n#raits nonsense\n#Trend gimmick\n#ributes stupidity\n#Learn idiot\nstrengthen shitty\nstrengthened fucking\n#ossom lame\npioneer crappy\n#iscover goddamn\n#Growing pointless\nprosperity inco\nneighbourhoods #shit\n#owship Nope\ndiff -diff\n------------ -------------\nWorse #knit\nbullshit pioneers\nNope pioneering\ncrap inspiring\nincompetence #iscover\nidiots complementary\nincompetent pioneer\nstupid #ossom\nincompet passionate\npointless passions\ninco journeys\nStupid unique\nmeaningless embraces\nnonsense admired\nlame forefront\nidiot richness\nworse invaluable\n#Fuck prosper\nwhining vibrant\nnonsensical enriched\nWO Subheads\nLayer 9\n0 out of 4\nLayer 10\n0 out of 4\nLayer 11\n0 out of 4\n16168\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n8\n□\u0013 A2. Did you discuss any potential risks of your work?\n8\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4,5\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4,5\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n4,5\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nIMDB is a well studied dataset and has been discussed many times before\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nIMDB is a well studied dataset and has been discussed many times before\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4\nC □\u0013 Did you run computational experiments?\n4,5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4,5 – wherever budget is known\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n16169\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4,5 – no hyperparameters were searched\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4,5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n16170",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.826958417892456
    },
    {
      "name": "Transformer",
      "score": 0.8020080327987671
    },
    {
      "name": "Interpretability",
      "score": 0.7729448080062866
    },
    {
      "name": "Computer science",
      "score": 0.6523056030273438
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5201826095581055
    },
    {
      "name": "Vocabulary",
      "score": 0.5082506537437439
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5066352486610413
    },
    {
      "name": "Machine learning",
      "score": 0.4118679165840149
    },
    {
      "name": "Engineering",
      "score": 0.12436455488204956
    },
    {
      "name": "Electrical engineering",
      "score": 0.0773678719997406
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}