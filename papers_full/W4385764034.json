{
    "title": "A Survey on Efficient Training of Transformers",
    "url": "https://openalex.org/W4385764034",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2337603917",
            "name": "Bohan Zhuang",
            "affiliations": [
                "Australian Regenerative Medicine Institute",
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A1479773632",
            "name": "Jing Liu",
            "affiliations": [
                "Australian Regenerative Medicine Institute",
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A3045690129",
            "name": "Zizheng Pan",
            "affiliations": [
                "Monash University",
                "Australian Regenerative Medicine Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2129072291",
            "name": "Haoyu He",
            "affiliations": [
                "Monash University",
                "Australian Regenerative Medicine Institute"
            ]
        },
        {
            "id": "https://openalex.org/A3207900119",
            "name": "Yuetian Weng",
            "affiliations": [
                "Australian Regenerative Medicine Institute",
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2098201046",
            "name": "Chunhua Shen",
            "affiliations": [
                "Zhejiang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3000364559",
        "https://openalex.org/W4287646898",
        "https://openalex.org/W2976872728",
        "https://openalex.org/W2991040477",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W4386709668",
        "https://openalex.org/W3044604993",
        "https://openalex.org/W4283761305",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W3035618017",
        "https://openalex.org/W2785961331",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2805003733",
        "https://openalex.org/W4321011699",
        "https://openalex.org/W3020605687",
        "https://openalex.org/W4221167110",
        "https://openalex.org/W4288621368",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4281622880",
        "https://openalex.org/W2953235111",
        "https://openalex.org/W2892283076",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4360831795",
        "https://openalex.org/W4299802238",
        "https://openalex.org/W2763421725",
        "https://openalex.org/W4240391021",
        "https://openalex.org/W4289293816",
        "https://openalex.org/W2990844796",
        "https://openalex.org/W3034340181",
        "https://openalex.org/W2794952988",
        "https://openalex.org/W4294635920",
        "https://openalex.org/W2945667196",
        "https://openalex.org/W4214686755",
        "https://openalex.org/W4281758439",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4313043552",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W3204998121",
        "https://openalex.org/W3204105967",
        "https://openalex.org/W4287208846",
        "https://openalex.org/W2899748887",
        "https://openalex.org/W4298187553",
        "https://openalex.org/W4287077733",
        "https://openalex.org/W3005842225",
        "https://openalex.org/W2949804919",
        "https://openalex.org/W3206453033",
        "https://openalex.org/W2616127482",
        "https://openalex.org/W2992505801",
        "https://openalex.org/W3204801262",
        "https://openalex.org/W4287126759",
        "https://openalex.org/W3175505246",
        "https://openalex.org/W3174394143",
        "https://openalex.org/W2963433607",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W3170863103",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3169769781",
        "https://openalex.org/W2622263826",
        "https://openalex.org/W4386076084",
        "https://openalex.org/W4376983087",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W2995435108",
        "https://openalex.org/W3176828726",
        "https://openalex.org/W2963376662",
        "https://openalex.org/W3137278571",
        "https://openalex.org/W3204181204",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3216308415",
        "https://openalex.org/W3010768098",
        "https://openalex.org/W3177323791",
        "https://openalex.org/W2469490737",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2894740066",
        "https://openalex.org/W2524428287",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3170925726",
        "https://openalex.org/W2757910899",
        "https://openalex.org/W4288057195",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W3129831491",
        "https://openalex.org/W2338908902",
        "https://openalex.org/W2606722458",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4288017660",
        "https://openalex.org/W3166164410",
        "https://openalex.org/W2996428491"
    ],
    "abstract": "Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.",
    "full_text": "A Survey on Efficient Training of Transformers\nBohan Zhuang1† , Jing Liu1 , Zizheng Pan1 , Haoyu He1 , Yuetian Weng1 , Chunhua Shen2\n1ZIP Lab, Monash University\n2Zhejiang University\n{bohan.zhuang, jing.liu1, zizheng.pan, haoyu.he, yuetian.weng}@monash.edu, chunhuashen@zju.edu.cn\nAbstract\nRecent advances in Transformers have come with\na huge requirement on computing resources, high-\nlighting the importance of developing efficient\ntraining techniques to make Transformer training\nfaster, at lower cost, and to higher accuracy by\nthe efficient use of computation and memory re-\nsources. This survey provides the first systematic\noverview of the efficient training of Transformers,\ncovering the recent progress in acceleration arith-\nmetic and hardware, with a focus on the former.\nWe analyze and compare methods that save com-\nputation and memory costs for intermediate tensors\nduring training, together with techniques on hard-\nware/algorithm co-design. We finally discuss chal-\nlenges and promising areas for future research.\n1 Introduction\nDeep learning is a recent most profound approach which has\nrevolutionised machine learning (ML) and artificial intelli-\ngence and is leading the fourth industrial revolution. At its\ncore, the success of deep learning depends on the vast compu-\ntational resources available and an extremely large amounts\nof labeled data. Despite the huge excitement generated by the\nrecent developments, deep learning models, especially Trans-\nformers [Vaswaniet al., 2017], have become formidably large\nand computationally intensive, resulting in two pressing chal-\nlenges at the fundamental level.\nThe first issue concerns the intensive computation of train-\ning large Transformer-based models. A widely discussed en-\nergy study of deep learning models [Strubell et al., 2019] es-\ntimates that training a Transformer base model with neural\narchitecture search (NAS) [So et al., 2019 ] produces about\n626,155 pounds of planet-warming carbon dioxide, equal\nto the lifetime emissions of five cars; as models grow big-\nger, their demand for computing is outpacing improvements\nin hardware efficiency. For example, GPT-3 [Brown et al.,\n2020] (the precursor to ChatGPT) was trained on half a tril-\nlion words and equips with 175 billion parameters. Notably,\naccording to the technical overview of GPT-31, it would take\n†Correspondence should be addressed to BZ.\n1https://lambdalabs.com/blog/demystifying-gpt-3/\n355 GPU-years and cost at least $4.6M for a single train-\ning run, estimated with theoretical 28 TFLOPS for V100 and\nlowest 3-year reserved cloud pricing. Therefore, the true\ngroundbreaking success of deep learning, such as ChatGPT,\nis exclusively dominated by large and rich enterprises such\nas Google or Microsoft. It becomes extremely important to\nmake deep learning tenable in computation and energy effi-\nciency for Green AI [Schwartz et al., 2020], and democratize\nAI to wider communities with limited resources.\nThe second issue comes with the exponentially growing\ntraining memory proportional to the attention-based model\nsize. For example, the largest language model in literature\ngrows from 345M with BERT-large [Kenton and Toutanova,\n2019] in 2018, to hundreds of billions till now with models\nsuch as MT-NLG [Smith et al., 2022 ] equipped with 530B\nparameters. Therefore, these SOTA massive models call for\nmemory efficient training techniques to reduce the memory\nfootprint of storing intermediate tensors and data exchanges\n(communications) across accelerators, while ensuring high\nprocessing elements (PE) utilization.\nIn this survey, we review the generic techniques that boost\ncomputation and memory efficiency for training attention-\nbased models, i.e., Transformers, as shown in Figure 1. We\ncharacterize them by the technical innovations and primary\nuse case, summarize them and draw connections between\nthem. We are primarily interested in arithmetic innovations\nthat improve the training efficiency of Transformers and also\nbriefly discuss hardware/algorithm codesign advances. We\nleave the review of hardware accelerator design, a broad\nclass, as future work.\n2 Computation Efficiency\n2.1 Optimization\nOptimizer. To achieve a faster convergence rate for gra-\ndient descent, a classic solution is to fuse the momentum\ntechnique, where each step is a combination of the steepest\ndescent direction and the most recent iterate displacement,\nhelping to accelerate gradient descent in the relevant direc-\ntion and dampens oscillations. The seminal works include\nNesterov’s accelerated gradient [Nesterov, 1983] for convex\noptimization and proximal gradient with momentum [Li et\nal., 2017] towards non-convex problems, etc. To meet the de-\nmand of large-scale optimization of machine learning mod-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6823\nComputation Efficiency\nEffective\nInitialization\nSparse Training\nAccelerated\nOptimizer\nOver-\nparameterization\nLarge Batch\nTraining\nIncremental\nLearning\nData Selection\nToken Masking\nImportance\nSampling\nParallism\nQuantized Training\nRematerialization\nOffloading\nParameter-efficient\nTuning\nOptimization\nMemory Efficiency\nHardware/Algorithm\nCodesign\nTraining Efficiency\nSparse Matrix\nMultiplication \nHardware-aware\nLow-precision\nEfficient Attention\nFigure 1: Overview of the efficient training of Transformers including computation efficiency, memory efficiency and hardware/algorithm\ncodesign perspectives.\nels, dominant optimizers are designed in a stochastic fashion.\nIn particular, stochastic gradient descent (SGD) with momen-\ntum and the adaptive learning rate estimation method Adam\n[Kingma and Ba, 2015 ] are widely used to train deep neu-\nral networks. Empirically, training Transformers with Adam\noutperforms the SGD counterpart, and [Zhang et al., 2019b]\ndemystifies that a heavy-tailed distribution of the noise in\nstochastic gradients is the main cause of SGD’s poor perfor-\nmance and understands Adam through the lens of adaptive\nnoise clipping. By default, AdamW [Loshchilov and Hutter,\n2019], a variant of Adam which decouples the L2 regular-\nization and the weight decay, is the most widely used op-\ntimizer for Transformers. More recently, Google searches\noptimization algorithms and discovers a simple and effec-\ntive optimizer called Lion [Chen et al. , 2023 ]. Lion only\nkeeps track of the momentum with the first-order gradient,\nand its update only considers the sign direction and has the\nsame magnitude for each parameter, which is very different\nfrom the adaptive optimizers like AdamW. In practice, Lion\nin general converges faster, and is more memory-efficient\nand accurate than AdamW for training Transformers on var-\nious benchmarks. We refer readers to [Lin et al., 2020b;\nBottou et al., 2018 ] for more details about accelerated op-\ntimization methods in machine learning.\nTo improve the generalization of Transformers, Sharpness-\naware minimization (SAM) [Foret et al., 2021 ] seeks to si-\nmultaneously minimize loss value and loss sharpness, based\non the connection between the geometry of the loss land-\nscape and generalization, i.e., a flatter minimum tends to im-\nprove generalization. The following work [Chen et al., 2022]\napplies SAM to Transformer, observing significant accuracy\ngains via smoothing the loss surface. However, SAM needs to\nsolve a bi-level min-max optimization problem, which nearly\ndoubles the training time. To accelerate optimization, [Du\net al., 2022a] proposes stochastic weight perturbation to pre-\nserve the generalization capability and sharpness-sensitive\nsubset selection strategies. More recently, [Du et al., 2022b]\ndesigns a near zero-cost proxy of the sharpness loss by replac-\ning the sharpness estimation as the KL-divergence between\nthe two consecutive update steps.\nInitialization. A good initialization is essential to stabilize\ntraining, enable higher learning rate, accelerate convergence,\nand improve generalization. Thus, many works have been\nproposed for better initialization of Transformers. Specifi-\ncally, Fixup[Zhang et al., 2019a] proposes to properly rescale\na standard initialization to ensure proper gradient norm to\navoid exploding or vanishing gradients, which can train very\ndeep networks with over 10,000 layers without adding nor-\nmalization layers. Based on the insight that the function\ncomputed by normalized residual blocks is close to the iden-\ntity function (i.e., unit variance), the following works ReZero\n[Bachlechner et al., 2021] and SkipInit [De and Smith, 2020]\nsimply initialize each layer to perform the identity operation.\nSpecifically, they add a learnable scaling multiplier on the\noutput of each residual block:\nxl+1 = xl + αlFl(xl), (1)\nwhere xl and Fl(·) are the input and the function at layer\nl, where the function can be multi-head self-attention layers\n(MSA) and feed-forward networks (FFN), and αl is simply\ninitialized to 0. Customized to Transformers, T-Fixup[Huang\net al., 2020 ] analyzes that part of the optimization difficulty\ncomes from the unstable early updates in the Adam optimizer\nas the variance of the second-order momentum is unbounded.\nTherefore, it follows Fixup to adopt rescaling schemes for\nthe initialization of residual blocks. All the above-mentioned\nmethods remove batch/layer normalization from all blocks\nand train without learning rate warmup. On training deep\nvision Transformers (ViT), [Touvron et al., 2021a] proposes\nchannel-wise learnable scaling factors and empirically ob-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6824\nserve that re-introducing the warmup and layer normalization\ntechniques can make training more stable.\nApart from the rescaling paradigm, some literature pro-\nposes to improve initialization from a new perspective by\nleveraging the relationship between self-attention and con-\nvolutional layers. [Cordonnier et al., 2020 ] proves that a\nmulti-head self-attention layer with N heads and a relative\npositional encoding with dimension D ≥ 3 can be repa-\nrameterized to express any convolutional layer of filter size√\nN ×\n√\nN. The attention can be decomposed into a content\nterm and a (relative) positional term, where the latter deter-\nmines the center and width of attention of each head. Based\non this property, ConViT[d’Ascoli et al., 2021] learns to con-\ntrol the locality by adding a soft gating parameter to balance\nthe two terms, which has the effect of incorporating soft con-\nvolutional inductive biases into global self-attention.\nSparse training. The key idea of sparse training is to di-\nrectly train sparse subnetworks instead of the full networks\nfrom scratch without sacrificing accuracy. The reliability\nwas first demonstrated by the lottery ticket hypothesis (LTH)\n[Frankle and Carbin, 2019] that a dense, randomly initialized\nnetwork contains subnetworks (winning tickets) which can be\ntrained in isolation to match the accuracy of the original net-\nwork. However, LTH requires identifying the winning tickets\nin an alternating train-prune-retrain manner, which makes the\ntraining extremely costly for large models and datasets, limit-\ning the practical benefits. In light of this, follow-up works\nwith higher training efficiency can be roughly categorized\ninto three categories: (i) find sparse networks once at initial-\nization by measuring the importance of connections on the\nloss, eliminating the need for the complex iterative optimiza-\ntion schedule [Lee et al., 2019; Wang et al., 2020] ; (ii) iden-\ntify the winning tickets in Transformers at a very early train-\ning stage via low-cost schemes and then merely train these\nearly tickets until convergence [You et al., 2020; Chen et al.,\n2021d]; (iii) use an alternating pruning and growing sched-\nule to dynamically update model sparsity patterns throughout\ntraining, suitable for general architectures [Evci et al., 2020;\nChen et al., 2021c].\nOverparameterization. Practical DNNs are heavily over-\nparameterized, where the number of learnable parameters is\nmuch larger than the number of training samples. It is ob-\nserved that overparameterization empirically improves both\nconvergence and generalization, with theoretical guarantee\nthough not sufficient. The early work [Arora et al., 2018 ]\nmathematically proves that increasing depth as overparame-\nterization in linear neural networks can accelerate SGD con-\nvergence. [Li and Liang, 2018 ] further explores two-layer\nnon-linear neural networks and [Allen-Zhu et al., 2019b ]\nproves that SGD can converge to global minima on the train-\ning objective of DNNs in polynomial time, assuming train-\ning samples are not duplicated, and the number of parame-\nters is polynomial in the number of training samples and net-\nwork depth. In terms of generalization, [Allen-Zhu et al.,\n2019a] theoretically proves that a sufficiently overparameter-\nized (three-layer) neural network generalizes to the popula-\ntion risk and an intriguing property is that there exists an ac-\ncurate network in the close neighborhood of any point on the\nSGD training trajectory with high probability over random\ninitialization. Note that it has deep connections with LTH as it\npartially explains why LTH stands in sparse training as good\nsmall sub-networks with low risks are plentiful due to over-\nparameterization. Applied to Transformers, [Li et al., 2020]\nexploits the faster convergence and better generalization from\nthe overparameterization theory to design an efficient train-\ning pipeline: training a very large model, then perform early\nstopping and heavily compress it, analogous to LTH.\nLarge batch training. Another prevailing way to acceler-\nate training is to use a large batch size, delivering a reduced\nnumber of iterations per epoch and better computing resource\nutilization. From the statistical view, large batch training re-\nduces the variance of the stochastic gradient estimates, so a\nreliable step size needs to be tuned for better convergence\n[Bottou et al., 2018]. At the era of convolutional neural net-\nworks, [Goyal et al., 2017] employs the linear scaling of the\nlearning rate to train ResNet-50 on ImageNet with a batch\nsize of 8,192 in 1 hour. More advanced step size estima-\ntion methods are then proposed. The widely used methods\nare LARS [You et al., 2017 ] for SGD and LAMB [You et\nal., 2019] for Adam, which propose to use layerwise adaptive\nlearning rates for ResNet and Transformers respectively. The\nlayerwise adaptation strategy can be formulated as\nwi\nt+1 = wi\nt − ηt\nϕ(\n\r\rwi\nt\n\r\n\r)\n\r\rγi\nt\n\r\n\r γi\nt, (2)\nwhere ηt, wi\nt and γi\nt are the learning rate, parameters and the\nmomentum-based gradients of the i-th layer at time step t,\nϕ is a scaling function. It equips with a normalization term\nthat provides robustness to exploding gradients and plateaus,\nand the scaling term ensures that the norm of the update is of\nthe same order as that of the parameter, promoting faster con-\nvergence. More recently, more powerful optimization meth-\nods customized to large batch training have been empirically\nshown to perform well. For example, [Kaddour, 2022] shows\nthat averaging the weights of a certain number of latest check-\npoints can facilitate faster training. DeepMind in [Hoffmann\net al., 2022 ] trains over 400 Transformer language models\nwith varying scales of model size and # of training tokens,\nreaching to a practical hypothesis that the model size and\nthe number of training tokens should be scaled equally for\ncompute-optimal LLM training.\nIncremental learning. The high-level concept of incre-\nmental learning is relaxing the original challenging opti-\nmization problem into a sequence of easy-to-optimize sub-\nproblems, where the solution of one sub-problem can serve as\na good initialization to the subsequent one to circumvent the\ntraining difficulty, in analogy with annealing. Some works\n[Gong et al., 2019; Gu et al., 2021 ] propose to accelerate\nBERT pretraining by progressively stacking layers, properly\ninitializing a larger model from a smaller one. [Zhang and\nHe, 2020 ] goes in a reverse direction to train Transformers\nwith stochastic depth via layer dropping, where it progres-\nsively increases dropping rate along both time dimension and\ndepth dimension. Customized to ViT, AutoProg [Li et al.,\n2022a] proposes to automatically decide whether, where and\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6825\nhow much should the model grow during progressive learn-\ning using neural architecture search. A key observation is\nthat progressively increasing the resolution of the input im-\nages (reducing the patch size) can significantly accelerate ViT\ntraining, aligning with the widely known training dynamics\nthat focus on low-frequency structure in the early stage and\nhigh-frequency semantics in the latter stage.\n2.2 Data Selection\nApart from the model efficiency, data efficiency is also a cru-\ncial factor of efficient training.\nToken masking. Token masking is a dominant approach in\nself-supervised pre-training tasks, such as masked language\nmodeling (MLM) [Kenton and Toutanova, 2019; Brown et\nal., 2020 ] and masked image modeling (MIM) [Bao et al.,\n2022; He et al., 2022]. The spirit of token masking is to ran-\ndomly mask some input tokens and train the model to predict\nthe missing content, e.g., vocabulary id or pixels, with the\ncontext information from the visible tokens. Since squeez-\ning the sequence length reduces both the computational and\nmemory complexity quadratically, skipping processing the\nmasked tokens brings considerable training efficiency gain\nfor MLM and MIM. For MLM, [Song et al., 2019] proposes\nto jointly pre-train the encoder and decoder for language gen-\neration tasks while removing the masked tokens in the de-\ncoder to save memory and computation costs. For MIM, rep-\nresentative work [He et al., 2022 ] shows that in vision, re-\nmoving the masked image patches before the encoder demon-\nstrates stronger performance and 3× or more lower overall\npre-training time and memory consumption than keeping the\nmasked tokens. A similar phenomenon is also found in [Li\net al., 2022b] that for language-image pre-training, randomly\nmasking and removing the masked image patches shows 3.7×\nfaster overall pre-training time than the original CLIP [Rad-\nford et al., 2021].\nImportance sampling. Importance sampling over data,\nalso known as data pruning, is theoretically guaranteed to ac-\ncelerate stochastic gradient algorithms for supervised learn-\ning by prioritizing informative training examples, mainly\nbenefiting from variance reduction. For DNNs, a principal\nway of estimating per-sample importance is to use gradient\nnorm, and [Katharopoulos and Fleuret, 2018; Johnson and\nGuestrin, 2018] use different approximations to make calcu-\nlating these norms tractable. [Paul et al., 2021] further speeds\nup the sampling process similar to the early-bird LTH, but in\nthe data domain, that simple average gradient norms or error\nℓ2-norms over several weight initializations can be used to\nidentify important examples at the very early stage in train-\ning. More recently, [Sorscher et al., 2022 ] shows an excit-\ning analytic theory that the scaling of test error with dataset\nsize can break beyond power scaling laws and be reduced to\nat least exponential scaling if equipped with a superior data\npruning metric, and it employs a self-supervised metric us-\ning k-means clustering. It demonstrates a promising direc-\ntion towards more efficient neural scaling laws based on data\nimportance sampling.\nMethod Class\nMicike\nvicius et al. [Micikevicius et al., 2018] AMP\nChen et al. [Chen et al., 2016] Rematerialization\nHerrmann et al. [Herrmann et al., 2019] Rematerialization\nZeRO-Offload [Ren et al., 2021] Offloading\nBeaumont et al. [Beaumont et al., 2021] Offloading + Rematerization\nZeRO [Rajbhandari et\nal., 2020] DP+MP+AMP\nMegatron-LM [Shoeybi et al., 2019] DP+TP\nGPipe [Huang et al., 2019] DP+PP\ntorchgpipe [Kim et al., 2020] PP+Rematerization\nMegatron-LM∗[Narayanan et al., 2021] DP+TP+PP+AMP\nWang et\nal. [Wang et al., 2018] FP8 Training\nCambier et al. [Cambier et al., 2020] FP8 Training\nMesa [Pan et al., 2021] 8-bit ACT\nACTNN [Chen et al., 2021a] 2-bit ACT\nGACT [Liu et al., 2022] 2-bit ACT\n[Lester et al.\n, 2021],\n[Jia et al., 2022],\n[Houlsby et al., 2019]\nAddition-based PET\nBitfit [Zaken et\nal., 2022],\nLoRA [Hu et al., 2022] Reparameterization-based PET\nTable 1: Summary of memory efficient training methods. Abbrevi-\nations include: AMP= Automatic Mixed Precision, DP = Data Par-\nallelism, MP = Model Parallelism, TP = Tensor Parallelism, PP =\nPipeline Parallelism, ACT = Activation Compressed Training and\nPET = Parameter-efficient Tuning.\n3 Memory Efficiency\nApart from the computation burden, the growing model size\nof large Transformer models, e.g., from BERT [Kenton and\nToutanova, 2019] 345M parameter model to GPT-3 of 1.75\ntrillion parameters, is a key bottleneck for training as they\ndo not fit into the memory of a single device. We first ana-\nlyze the memory consumption of the existing model training\nframeworks, which is occupied by 1) model states, includ-\ning optimizer states (e.g., momentum and variance in Adam),\ngradients and parameters; and 2) activations (we ignore tem-\nporary buffers and idle fragmented memory as they are rel-\natively small). We summarize the memory efficient training\nmethods in Table 1. In the following, we discuss dominant\nsolutions to optimize memory usage.\nParallelism. Training large DNNs with parallelism across\ndevices is a common practice to meet the memory demands.\nThere are basically two paradigms: Data Parallelism (DP)\nwhich distributes a minibatch of data across different devices\nand Model Parallelism (MP) which allocates subgraphs of a\nmodel across multiple workers. For DP, with the increase\nof available workers, the batch size is close to linear scal-\ning. Large batch training discussed in Sec. 2 is developed for\nthis case. However, it is obvious that DP has high commu-\nnication/computation efficiency but poor memory efficiency\n- when model becomes large, the single device cannot store\nthe model replica and the synchronized communications for\ngradients can hinder the scalability of DP. Therefore, DP it-\nself is only suitable for training small to moderate models.\nTo improve the scalability of DP, one solution for Trans-\nformer is parameter sharing [Lan et al., 2020], known as Al-\nbert, but it limits the representational power. More recently,\nZeRO [Rajbhandari et al., 2020 ] incorporates uniform par-\ntitioning strategy with DP, where each data parallel process\nmerely deals with one partition of model states, working in\na mixed precision regime. To deal with very large DNNs,\none always need to utilize model parallelism to allocate dif-\nferent layers across multiple accelerators in a “vertical” man-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6826\nner. Though MP has good memory efficiency, its communi-\ncation and computation efficiency is low due to high volume\ndata transfer cross devices and poor PE utilization. Luck-\nily, there are two strategies to further boost MP efficiency in\nan orthogonal “horizontal” dimension, including Tensor Par-\nallelism (TP) and Pipeline Parallelism (PP). TP partitions a\ntensor operation in a layer across workers for faster computa-\ntion and more memory saving. Customized to Transformer-\nbased models, Megatron-LM [Shoeybi et al., 2019 ] slices\nboth MSA and FFN across GPUs and requires only a few ex-\ntra All-Reduce operations in the forward and backward pass,\nallowing them to train models up to 8.3 billion parameters\nusing 512 GPUs. In terms of PP, it was originally proposed\nin GPipe [Huang et al., 2019 ], which splits the input mini-\nbatch into multiple smaller micro-batches, enabling different\naccelerators (sequential layers are partitioned across acceler-\nators) to work on different micro-batches simultaneously be-\nfore applying a single synchronous gradient update for the\nentire mini-batch. However, it still suffers from pipeline bub-\nbles (accelerator idle time) that reduce efficiency. In partic-\nular, PyTorch implements the torchgpipe [Kim et al., 2020],\nwhich performs micro-batch PP with checkpointing, allow-\ning scaling to a large number of micro-batches to minimize\nthe bubble overhead.\nNote that DP and MP are orthogonal and so one can use\nboth simultaneously to train larger models with higher com-\nputation and memory capacity. For example, Megatron-LM∗\n[Narayanan et al., 2021] and DeepSpeed [Rasley et al., 2020]\ncompose tensor, pipeline, and data parallelism to scale train-\ning to thousands of GPUs.\nQuantized training. The standard routine for training\nneural networks adopts full-precision (i.e., FP32). In\ncontrast, quantized training trains neural networks from\nscratch in reduced precision by compressing the activa-\ntions/weights/gradients into low-bit values ( e.g., FP16 or\nINT8). It has been shown in previous works that reduced\nprecision training [Zhou et al., 2016; Hubara et al., 2017 ]\ncan accelerate neural network training with favorable per-\nformance. For Transformers, the most widely adopted ap-\nproach is automatic mixed-precision (AMP) training [Mi-\ncikevicius et al., 2018 ]. Specifically, AMP stores a master\ncopy of weights in full-precision for updates while the acti-\nvations, gradients and weights are stored in FP16 for arith-\nmetic. Compared to full-precision training, AMP is able to\nachieve faster training/inference speed and reduce memory\nconsumption during network training. For example, based\non a batch size of 64 and image resolution of 224 × 224,\ntraining a DeiT-B [Touvron et al., 2021b ] on RTX3090 un-\nder AMP is 2× faster than full-precision training (305 vs.\n124 images/s), as well as consuming 22% less peak GPU\nmemory (7.9GB vs. 10.2GB). While it is commonly be-\nlieved that at least 16-bits is necessary to train networks with-\nout impacting model accuracy [Micikevicius et al., 2018;\nDas et al., 2018 ], the most recent support for FP8 training\non NVIDIA H100 has shown promising results on Trans-\nformer training, where training DeiT-S and GPT [Brown\net al., 2020 ] under FP8 can match those of 16-bit train-\ning. Apart from reduced precision training which simul-\ntaneously quantizes activations/weights/gradients, activation\ncompressed training (ACT) [Chen et al., 2021a ] stores low-\nprecision approximate copies of activations while computing\nthe forward pass exactly, which helps to reduce the overall\nmemory consumption during training. The saved activations\nare then dequantized to the original precision in the backward\npass to calculate gradients. Recent work [Pan et al., 2021;\nLiu et al., 2022] further propose to customize ACT to support\nmemory-efficient Transformer training.\nRematerialization and offloading. Rematerialization,\nalso known as checkpointing [Chen et al., 2016], is a widely\nused technique for space-time tradeoff that only stores a\nportion of activations/weights during the forward pass and\nrecomputes the rest during the backward pass. [Chen et\nal., 2016 ] provides a simple periodic schedule which was\nimplemented in PyTorch2, but it is only optimal for homoge-\nneous sequential networks. More advanced methods such as\n[Herrmann et al., 2019 ] implements optimal checkpointing\nfor heterogeneous networks 3. In terms of offloading, it is a\ntechnique to use external memory such as CPU memory as\nan extension of GPU memory to increase memory capacity\nduring training, through communications between GPU\nand CPU. The model states as well as activations, can be\noffloaded to CPU, but the optimal choice needs to minimize\ncommunication cost (i.e., data movement) to/from GPU,\nreduce CPU computation and maximize GPU memory\nsaving. A representative work is ZeRO-Offload [Ren et al.,\n2021], which offers optimal offloading strategy customized\nto mixed-precision training with Adam optimizer. It offloads\nall fp32 model states and the fp16 gradients on the CPU\nmemory, and computes the fp32 parameter updates on CPU.\nThe fp16 parameters are kept on GPU and the forward and\nbackward computations are on GPU. For the best of both\nworlds, [Beaumont et al., 2021] proposes to jointly optimize\nactivation offloading and rematerialization.\nParameter-efficient tuning. The public model zoo repre-\nsented by HuggingFace, which contains rich pretrained mod-\nels that are ready to be downloaded and executed, is con-\ntributing significantly to reductions in training costs. Effi-\ncient tuning these readily available models is becoming a\nprevailing way to drastically cut training costs. As a pow-\nerful alternative for the vanilla full fine-tuning, parameter-\nefficient tuning (PET) only updates a small number of ad-\nditional parameters while freezing the pretrained model to\nsignificantly reduce the storage burden, which scales with\ndynamic deployment scenarios without the need to store a\nseparate instance of model for each case. The general PET\napproaches can be categorized into addition-based methods\nand reparameterization-based methods. The former attaches\nadditional trainable parameters to the pretrained model and\nonly tune these parameters. For example, [Lester et al., 2021;\nJia et al., 2022 ] add trainable parameters to the input space,\nand [Houlsby et al., 2019 ] adds the adapter module twice\nto each Transformer block after the MSA and FFN. How-\never, the extra parameters introduce additional computation\n2https://pytorch.org/\n3https://gitlab.inria.fr/hiepacs/rotor\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6827\nand memory overhead during inference. To tackle this chal-\nlenge, the latter proposes to tune parameters that are inher-\nently in the model [Zaken et al., 2022 ] or new parameters\nthat can be reparameterized into the model [Hu et al., 2022],\nthereby yielding no sacrifice on the inference efficiency. In-\nspired by the observation that large language pretrained mod-\nels have low intrinsic dimension [Aghajanyan et al., 2021 ],\nthe representative work LoRA[Hu et al., 2022] approximates\nthe update of self-attention weights into two low-rank matri-\nces, which can be merged into the pretrained weights dur-\ning inference. Notably, one of the most recognized efforts\nfor democratizing LLM is Stanford Alpaca 4, which is fine-\ntuned from the open-sourced LLaMA models [Touvron et\nal., 2023] using the 52K instruction-following data generated\nfrom ChatGPT. To fine-tune it cheaply and efficiently, its vari-\nant Alpaca-LoRA 5 further adopts the low-rank LoRA to en-\nable instruct-tuning LLaMA on customer hardware, showing\ntraining can be done within hours on a single RTX 4090.\nOpen-source frameworks. There are several widely\nadopted prototypes for training large Transformer models\nat scale, in which Microsoft DeepSpeed 6, HPC-AI Tech\nColossal-AI7 and Nvidia Megatron-LM 8 are the pioneering\nones. Specifically, DeepSpeed is implemented mainly based\non [Rasley et al., 2020] and ZeRO series works [Rajbhandari\net al., 2020; Renet al., 2021], Colossal-AI is built upon[Bian\net al., 2021 ], and Megatron-LM implements [Narayanan et\nal., 2021]. All of these support data and model parallelism in\nmixed precision, along with other general practices such as\noffloading and rematerialization. More libraries for efficient\ndistributed training include but not limited to HuggingFace\nTransformers9, MosaicML Composer 10, Baidu PaddlePad-\ndle11, Bytedance Lightseq12, EleutherAI GPT-NeoX13, etc.\n4 Hardware/Algorithm Co-design\nApart from computing and memory burden, designing effi-\ncient hardware accelerators enables faster training and infer-\nence for DNNs. Specifically, compared with central process-\ning units (CPUs), graphics processing units (GPUs) are more\npowerful to perform matrix multiplication due to the high\ndegree of parallelism. For applications that focus on spe-\ncific computation tasks, application-specific integrated cir-\ncuits (ASICs) have the advantage of low power consumption,\nand high training/inference speed. For example, a tensor pro-\ncessing unit (TPU) designed by Google delivered 30∼80×\nhigher performance-per-watt than contemporary CPUs and\nGPUs [Jouppi et al., 2017 ]. However, ASICs are not eas-\nily reprogrammable or adaptable to a new task. In contrast,\nField Programmable Gate Arrays (FPGAs) are designed to be\n4https://github.com/tatsu-lab/stanford alpaca\n5https://github.com/tloen/alpaca-lora\n6https://github.com/microsoft/DeepSpeed\n7https://github.com/hpcaitech/ColossalAI\n8https://github.com/NVIDIA/Megatron-LM\n9https://github.com/huggingface/transformers\n10https://github.com/mosaicml/composer\n11https://github.com/PaddlePaddle/Paddle\n12https://github.com/bytedance/lightseq\n13https://github.com/EleutherAI/gpt-neox\nreprogrammed to perform different functions as needed, and\ncan also be used as a prototype for ASICs before finalizing the\ndesign. To further optimize the training efficiency of DNNs,\nespecially Transformers, hardware-algorithm co-design takes\nthe constraints and capabilities of the hardware into account\nwhen designing the algorithm, which will be introduced in\nthe following subsections.\nSparse matrix multiplication. To reduce the computation\noverhead of Transformers, sparse general matrix multiplica-\ntion (SpGEMM), which involves multiplying a sparse matrix\nwith a dense matrix, takes advantage of the sparsity of the at-\ntention matrices to reduce the number of computations. There\nare several popular sparse matrix computation libraries, such\nas Intel Math Kernel Library 14 on CPU and cuSPARSE 15,\nCUSP16 and 2:4 structured sparsity 17[Mishra et al., 2021] on\nGPU. However, due to the irregular sparsity, SpGEMM is of-\nten hardware unfriendly to general-purpose processors, such\nas CPUs and GPUs. To handle this, specialized hardware ac-\ncelerators, such as FPGAs and ASICs, are required to handle\nthe poor data locality issue. For example, OuterSPACE[Pal et\nal., 2018] transforms matrix multiplication into an outer prod-\nuct procedure and eliminates redundant memory accesses by\ndecoupling multiplication from accumulation. To take full\nadvantage of this reduction without introducing significant\noverheads, OuterSPACE builds a custom accelerator with re-\nconfigurable memory hierarchy and achieves a mean speedup\nof 7.9× over the CPU running Intel Math Kernel Library and\n14.0× against the GPU running CUSP. Furthermore, to alle-\nviate the data movement bottleneck caused by high sparsity,\nViTCoD [You et al., 2023 ] uses a learnable auto-encoder to\ncompress the sparse attentions to a much more compact rep-\nresentation and designs encoder and decoder engines to boost\nthe hardware utilization.\nHardware-aware low-precision. Lowering the precision\nof the computations reduces the amount of memory and com-\nputation, which can be implemented in hardware-friendly\nfixed-point or integer representations instead of floating-point\nones. As a result, we can use lower precision multipli-\ners, adders, and memory blocks, resulting in a significant\nimprovement in power consumption and speedup. More-\nover, low-precision arithmetic can be combined with other\ntechniques, such as pruning and low-rank approximation, to\nachieve further acceleration. For example, Sanger [Lu et\nal., 2021] uses 4-bit queries and keys to compute the quan-\ntized prediction of sparse attention matrix. Then, the sparse\nattention masks are rearranged into structured blocks and\nhandled by reconfigurable hardware. The following work\nDOTA [Qu et al., 2022 ] identifies unimportant connections\nin attention using low-rank transformation and low-precision\ncomputation. By incorporating token-level parallelism and\nout-of-order execution, DOTA achieves a 152.6× speedup\nover GPU.\n14https://www.intel.com/content/www/us/en/developer/tools/\noneapi/onemkl.html\n15https://docs.nvidia.com/cuda/cusparse/\n16https://cusplibrary.github.io/\n17https://developer.nvidia.com/blog/\naccelerating-inference-with-sparsity-using-ampere-and-tensorrt/\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6828\nEfficient attention. Apart from the sparse matrix multi-\nplication and low-precision computation, several pioneering\nworks focus on efficient and lightweight attention implemen-\ntation in hardware [Ham et al., 2020; Ham et al., 2021;\nDao et al., 2022]. Specifically, A 3 [Ham et al., 2020 ] only\nselects those keys that are likely to have high similarity with\nthe given queries to reduce the amount of computation in at-\ntention. ELSA [Ham et al., 2021 ] filters out irrelevant keys\nfor a particular query based on the hashing similarity to save\ncomputation. With an efficient hardware accelerator, ELSA\nachieves a speedup of 58.1× as well as three orders of mag-\nnitude improvements in energy efficiency compared to an\nNvidia V100 GPU equipped with 16GB memory. Notably,\nFlashAttention [Dao et al., 2022] proposes to exploit tiling to\nreduce the I/O communication between GPU high bandwidth\nmemory (HBM) and on-chip SRAM, which is becoming a de-\nfault fast and memory-efficient attention module for speedup.\n5 Conclusion and Future Research\nIn this survey, we have reviewed several important factors\nthat improve the training of Transformers: 1) appropriate\ninitialization and optimization paradigms that can accelerate\nthe convergence rate with fewer training iterations, resulting\nin lower computational costs; 2) higher data efficiency by\nsampling informative training samples towards more efficient\nneural scaling laws of test error with respect to dataset size;\n3) memory-efficient techniques to meet the memory require-\nments for training large Transformers, which requires jointly\noptimizing PE utilization, memory and communication foot-\nprints across accelerators, using parallelism, low-precision\narithmetic, checkpointing and offloading strategies, etc; 4)\nhardware and algorithm co-design to maximize the training\nscalability on hardware platforms.\nWe finally highlight several promising directions based on\nthe reviewed progress: (i) joint training and deployment ef-\nficiency optimization. In reality, we usually need to de-\nploy models to diverse tasks and platforms with different re-\nsource constraints. Therefore, it is highly desirable to de-\nvelop new methods for efficiently training an elastic supertnet\nthat supports many diverse architectural configurations fol-\nlowing single-shot NAS [Chen et al., 2021b ] or mixture of\nexperts [Fedus et al., 2022], for multiple tasks and budget re-\nquirements; (ii) on-device training [Lin et al., 2020a] on the\nedge with limited resources (e.g., low battery and memory\ncapacity), to avoid frequently transmitting data which results\nin privacy and latency issues; (iii) combine efficient approx-\nimation techniques such as token/model pruning, low-rank\nfactorization, lightweight architecture design, dynamic neural\nnetworks and etc, to reduce the model size and computational\ncost in the complimentary sense; (iv) a standard benchmark to\nevaluate and compare the efficient training methods. Having\nsuch a benchmark would fasten the adoption of these methods\nand lead to actual costs reduction down the line.\nReferences\n[Aghajanyan et al., 2021] Armen Aghajanyan, Sonal Gupta,\net al. Intrinsic dimensionality explains the effectiveness of\nlanguage model fine-tuning. In ACL, 2021.\n[Allen-Zhu et al., 2019a] Zeyuan Allen-Zhu, Yuanzhi Li,\nand Yingyu Liang. Learning and generalization in over-\nparameterized neural networks, going beyond two layers.\nIn NeurIPS, 2019.\n[Allen-Zhu et al., 2019b] Zeyuan Allen-Zhu, Yuanzhi Li,\nand Zhao Song. A convergence theory for deep learning\nvia over-parameterization. In ICML, 2019.\n[Arora et al., 2018] Sanjeev Arora, Nadav Cohen, and Elad\nHazan. On the optimization of deep networks: Implicit\nacceleration by overparameterization. In ICML, 2018.\n[Bachlechner et al., 2021] Thomas Bachlechner, Bod-\nhisattwa Prasad Majumder, et al. Rezero is all you need:\nFast convergence at large depth. In UAI, 2021.\n[Bao et al., 2022] Hangbo Bao, Li Dong, et al. Beit: Bert\npre-training of image transformers. In ICLR, 2022.\n[Beaumont et al., 2021] Olivier Beaumont, Eyraud-Dubois,\net al. Efficient combination of rematerialization and of-\nfloading for training dnns. In NeurIPS, 2021.\n[Bian et al., 2021] Zhengda Bian, Hongxin Liu, et al.\nColossal-ai: A unified deep learning system for large-scale\nparallel training. arXiv preprint arXiv:2110.14883, 2021.\n[Bottou et al., 2018] L´eon Bottou, Frank E Curtis, and Jorge\nNocedal. Optimization methods for large-scale machine\nlearning. SIAM Review, 2018.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, et al. Language models are few-shot\nlearners. In NeurIPS, 2020.\n[Cambier et al., 2020] L´eopold Cambier, Anahita Bhiwandi-\nwalla, et al. Shifted and squeezed 8-bit floating point for-\nmat for low-precision training of deep neural networks. In\nICLR, 2020.\n[Chen et al., 2016] Tianqi Chen, Bing Xu, et al. Training\ndeep nets with sublinear memory cost. arXiv preprint\narXiv:1604.06174, 2016.\n[Chen et al., 2021a] Jianfei Chen, Lianmin Zheng, et al.\nActnn: Reducing training memory footprint via 2-bit acti-\nvation compressed training. In ICML, 2021.\n[Chen et al., 2021b] Minghao Chen, Houwen Peng, Jianlong\nFu, and Haibin Ling. Autoformer: Searching transformers\nfor visual recognition. In ICCV, 2021.\n[Chen et al., 2021c] Tianlong Chen, Yu Cheng, et al. Chas-\ning sparsity in vision transformers: An end-to-end explo-\nration. In NeurIPS, 2021.\n[Chen et al., 2021d] Xiaohan Chen, Yu Cheng, et al. Early-\nbert: Efficient bert training via early-bird lottery tickets. In\nACL, 2021.\n[Chen et al., 2022] Xiangning Chen, Cho-Jui Hsieh, et al.\nWhen vision transformers outperform resnets without pre-\ntraining or strong data augmentations. In ICLR, 2022.\n[Chen et al., 2023] Xiangning Chen, Chen Liang, et al. Sym-\nbolic discovery of optimization algorithms. arXiv preprint\narXiv:2302.06675, 2023.\n[Cordonnier et al., 2020] Jean-Baptiste Cordonnier, Andreas\nLoukas, and Martin Jaggi. On the relationship between\nself-attention and convolutional layers. In ICLR, 2020.\n[Dao et al., 2022] Tri Dao, Daniel Y . Fu, et al. FlashAtten-\ntion: Fast and memory-efficient exact attention with IO-\nawareness. In NeurIPS, 2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6829\n[Das et al., 2018] Dipankar Das, Naveen Mellempudi, et al.\nMixed precision training of convolutional neural networks\nusing integer operations. In ICLR, 2018.\n[De and Smith, 2020] Soham De and Sam Smith. Batch\nnormalization biases residual blocks towards the identity\nfunction in deep networks. In NeurIPS, 2020.\n[Du et al., 2022a] Jiawei Du, Hanshu Yan, et al. Efficient\nsharpness-aware minimization for improved training of\nneural networks. In ICLR, 2022.\n[Du et al., 2022b] Jiawei Du, Daquan Zhou, et al.\nSharpness-aware training for free. In NeurIPS, 2022.\n[d’Ascoli et al., 2021] St´ephane d’Ascoli, Hugo Touvron,\net al. Convit: Improving vision transformers with soft con-\nvolutional inductive biases. In ICML, 2021.\n[Evci et al., 2020] Utku Evci, Trevor Gale, et al. Rigging the\nlottery: Making all tickets winners. In ICML, 2020.\n[Fedus et al., 2022] William Fedus, Barret Zoph, and Noam\nShazeer. Switch transformers: Scaling to trillion parame-\nter models with simple and efficient sparsity.JMLR, 2022.\n[Foret et al., 2021] Pierre Foret, Ariel Kleiner, et al.\nSharpness-aware minimization for efficiently improving\ngeneralization. In ICLR, 2021.\n[Frankle and Carbin, 2019] Jonathan Frankle and Michael\nCarbin. The lottery ticket hypothesis: Finding sparse,\ntrainable neural networks. In ICLR, 2019.\n[Gong et al., 2019] Linyuan Gong, Di He, et al. Efficient\ntraining of bert by progressively stacking. In ICML, 2019.\n[Goyal et al., 2017] Priya Goyal, Piotr Doll ´ar, et al. Accu-\nrate, large minibatch sgd: Training imagenet in 1 hour.\narXiv preprint arXiv:1706.02677, 2017.\n[Gu et al., 2021] Xiaotao Gu, Liyuan Liu, et al. On the trans-\nformer growth for progressive bert training. In NAACL,\n2021.\n[Ham et al., 2020] Tae Jun Ham, Sung Jun Jung, et al. Aˆ\n3: Accelerating attention mechanisms in neural networks\nwith approximation. In HPCA, 2020.\n[Ham et al., 2021] Tae Jun Ham, Yejin Lee, et al. Elsa:\nHardware-software co-design for efficient, lightweight\nself-attention mechanism in neural networks. In ISCA,\n2021.\n[He et al., 2022] Kaiming He, Xinlei Chen, et al. Masked\nautoencoders are scalable vision learners. In CVPR, 2022.\n[Herrmann et al., 2019] Julien Herrmann, Olivier Beau-\nmont, et al. Optimal checkpointing for heterogeneous\nchains: how to train deep neural networks with limited\nmemory. arXiv preprint arXiv:1911.13214, 2019.\n[Hoffmann et al., 2022] Jordan Hoffmann, Sebastian\nBorgeaud, et al. Training compute-optimal large language\nmodels. arXiv preprint arXiv:2203.15556, 2022.\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, et al.\nParameter-efficient transfer learning for nlp. In ICML,\n2019.\n[Hu et al., 2022] Edward J Hu, yelong shen, et al. Lora:\nLow-rank adaptation of large language models. In ICLR,\n2022.\n[Huang et al., 2019] Yanping Huang, Youlong Cheng, et al.\nGpipe: Efficient training of giant neural networks using\npipeline parallelism. In NeurIPS, 2019.\n[Huang et al., 2020] Xiao Shi Huang, Felipe Perez, et al. Im-\nproving transformer optimization through better initializa-\ntion. In ICML, 2020.\n[Hubara et al., 2017] Itay Hubara, Matthieu Courbariaux,\net al. Quantized neural networks: Training neural net-\nworks with low precision weights and activations. JMLR,\n2017.\n[Jia et al., 2022] Menglin Jia, Luming Tang, Bor-Chun\nChen, Claire Cardie, Serge Belongie, Bharath Hariharan,\nand Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022.\n[Johnson and Guestrin, 2018] Tyler B Johnson and Carlos\nGuestrin. Training deep models faster with robust, approx-\nimate importance sampling. In NeurIPS, 2018.\n[Jouppi et al., 2017] Norman P Jouppi, Cliff Young, et al.\nIn-datacenter performance analysis of a tensor processing\nunit. In ISCA, 2017.\n[Kaddour, 2022] Jean Kaddour. Stop wasting my time! sav-\ning days of imagenet and bert training with latest weight\naveraging. In NeurIPS workshop, 2022.\n[Katharopoulos and Fleuret, 2018] Angelos Katharopoulos\nand Franc ¸ois Fleuret. Not all samples are created equal:\nDeep learning with importance sampling. In ICML, 2018.\n[Kenton and Toutanova, 2019] Jacob Devlin Ming-\nWei Chang Kenton and Lee Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers\nfor language understanding. In NAACL-HLT, 2019.\n[Kim et al., 2020] Chiheon Kim, Heungsub Lee, et al.\ntorchgpipe: On-the-fly pipeline parallelism for training gi-\nant models. arXiv preprint arXiv:2004.09910, 2020.\n[Kingma and Ba, 2015] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. In ICLR,\n2015.\n[Lan et al., 2020] Zhenzhong Lan, Mingda Chen, et al. Al-\nbert: A lite bert for self-supervised learning of language\nrepresentations. In ICLR, 2020.\n[Lee et al., 2019] Namhoon Lee, Thalaiyasingam Ajanthan,\nand Philip Torr. Snip: Single-shot network pruning based\non connection sensitivity. In ICLR, 2019.\n[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah\nConstant. The power of scale for parameter-efficient\nprompt tuning. In EMNLP, 2021.\n[Li and Liang, 2018] Yuanzhi Li and Yingyu Liang. Learn-\ning overparameterized neural networks via stochastic gra-\ndient descent on structured data. In NeurIPS, 2018.\n[Li et al., 2017] Qunwei Li, Yi Zhou, et al. Convergence\nanalysis of proximal gradient with momentum for noncon-\nvex optimization. In ICLR, 2017.\n[Li et al., 2020] Zhuohan Li, Eric Wallace, et al. Train big,\nthen compress: Rethinking model size for efficient train-\ning and inference of transformers. In ICML, 2020.\n[Li et al., 2022a] Changlin Li, Bohan Zhuang, et al. Auto-\nmated progressive learning for efficient training of vision\ntransformers. In CVPR, 2022.\n[Li et al., 2022b] Yanghao Li, Haoqi Fan, et al. Scaling\nlanguage-image pre-training via masking. arXiv preprint\narXiv:2212.00794, 2022.\n[Lin et al., 2020a] Ji Lin, Wei-Ming Chen, et al. Mcunet:\nTiny deep learning on iot devices. In NeurIPS, 2020.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6830\n[Lin et al., 2020b] Zhouchen Lin, Huan Li, and Cong Fang.\nAccelerated optimization for machine learning. Nature\nSingapore: Springer, 2020.\n[Liu et al., 2022] Xiaoxuan Liu, Lianmin Zheng, et al.\nGACT: activation compressed training for generic network\narchitectures. In ICML, 2022.\n[Loshchilov and Hutter, 2019] Ilya Loshchilov and Frank\nHutter. Decoupled weight decay regularization. In ICLR,\n2019.\n[Lu et al., 2021] Liqiang Lu, Yicheng Jin, et al. Sanger: A\nco-design framework for enabling sparse attention using\nreconfigurable architecture. In MICRO, 2021.\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan\nNarang, et al. Mixed precision training. In ICLR, 2018.\n[Mishra et al., 2021] Asit Mishra, Jorge Albericio Latorre,\net al. Accelerating sparse deep neural networks. arXiv\npreprint arXiv:2104.08378, 2021.\n[Narayanan et al., 2021] Deepak Narayanan, Mohammad\nShoeybi, et al. Efficient large-scale language model train-\ning on gpu clusters using megatron-lm. In SC, 2021.\n[Nesterov, 1983] Yurii Nesterov. A method for uncon-\nstrained convex minimization problem with the rate of\nconvergence o (1/kˆ 2). In Doklady ANSSSR, volume 269,\npages 543–547, 1983.\n[Pal et al., 2018] Subhankar Pal, Jonathan Beaumont, et al.\nOuterspace: An outer product based sparse matrix multi-\nplication accelerator. In HPCA, 2018.\n[Pan et al., 2021] Zizheng Pan, Peng Chen, et al. Mesa:\nA memory-saving training framework for transformers.\narXiv preprint arXiv:2111.11124, 2021.\n[Paul et al., 2021] Mansheej Paul, Surya Ganguli, et al.\nDeep learning on a data diet: Finding important examples\nearly in training. In NeurIPS, 2021.\n[Qu et al., 2022] Zheng Qu, Liu Liu, et al. Dota: detect and\nomit weak attentions for scalable transformer acceleration.\nIn ASPLOS, 2022.\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, et al.\nLearning transferable visual models from natural language\nsupervision. In ICML, 2021.\n[Rajbhandari et al., 2020] Samyam Rajbhandari, Jeff Rasley,\net al. Zero: Memory optimizations toward training trillion\nparameter models. In SC, 2020.\n[Rasley et al., 2020] Jeff Rasley, Samyam Rajbhandari,\nOlatunji Ruwase, and Yuxiong He. Deepspeed: System\noptimizations enable training deep learning models with\nover 100 billion parameters. In KDD, 2020.\n[Ren et al., 2021] Jie Ren, Samyam Rajbhandari, et al. Zero-\noffload: Democratizing billion-scale model training. In\nUSENIX ATC, 2021.\n[Schwartz et al., 2020] Roy Schwartz, Jesse Dodge, Noah A\nSmith, and Oren Etzioni. Green ai. Communications of\nthe ACM, 2020.\n[Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Pat-\nwary, et al. Megatron-lm: Training multi-billion parameter\nlanguage models using model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\n[Smith et al., 2022] Shaden Smith, Mostofa Patwary, et al.\nUsing deepspeed and megatron to train megatron-turing\nnlg 530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990, 2022.\n[So et al., 2019] David So, Quoc Le, and Chen Liang. The\nevolved transformer. In ICML, pages 5877–5886, 2019.\n[Song et al., 2019] Kaitao Song, Xu Tan, et al. Mass:\nMasked sequence to sequence pre-training for language\ngeneration. In ICML, 2019.\n[Sorscher et al., 2022] Ben Sorscher, Robert Geirhos, et al.\nBeyond neural scaling laws: beating power law scaling via\ndata pruning. In NeurIPS, 2022.\n[Strubell et al., 2019] Emma Strubell, Ananya Ganesh, and\nAndrew McCallum. Energy and policy considerations for\ndeep learning in nlp. In ACM, 2019.\n[Touvron et al., 2021a] Hugo Touvron, Matthieu Cord, et al.\nGoing deeper with image transformers. In ICCV, 2021.\n[Touvron et al., 2021b] Hugo Touvron, Matthieu Cord, et al.\nTraining data-efficient image transformers & distillation\nthrough attention. In ICML, 2021.\n[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gau-\ntier Izacard, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, et al.\nAttention is all you need. In NeurIPS, 2017.\n[Wang et al., 2018] Naigang Wang, Jungwook Choi, et al.\nTraining deep neural networks with 8-bit floating point\nnumbers. In NeurIPS, 2018.\n[Wang et al., 2020] Chaoqi Wang, Guodong Zhang, and\nRoger Grosse. Picking winning tickets before training by\npreserving gradient flow. In ICLR, 2020.\n[You et al., 2017] Yang You, Igor Gitman, and Boris Gins-\nburg. Large batch training of convolutional networks.\narXiv preprint arXiv:1708.03888, 2017.\n[You et al., 2019] Yang You, Jing Li, et al. Large batch op-\ntimization for deep learning: Training bert in 76 minutes.\nIn ICLR, 2019.\n[You et al., 2020] Haoran You, Chaojian Li, et al. Drawing\nearly-bird tickets: Towards more efficient training of deep\nnetworks. In ICLR, 2020.\n[You et al., 2023] Haoran You, Zhanyi Sun, et al. Vitcod: Vi-\nsion transformer acceleration via dedicated algorithm and\naccelerator co-design. In HPCA, 2023.\n[Zaken et al., 2022] Elad Ben Zaken, Yoav Goldberg,\net al. Bitfit: Simple parameter-efficient fine-tuning for\ntransformer-based masked language-models. In ACL,\n2022.\n[Zhang and He, 2020] Minjia Zhang and Yuxiong He. Ac-\ncelerating training of transformer-based language models\nwith progressive layer dropping. In NeurIPS, 2020.\n[Zhang et al., 2019a] Hongyi Zhang, Yann N Dauphin, and\nTengyu Ma. Fixup initialization: Residual learning with-\nout normalization. In ICLR, 2019.\n[Zhang et al., 2019b] Jingzhao Zhang, Sai Praneeth Karim-\nireddy, et al. Why adam beats sgd for attention models. In\nOpenReview, 2019.\n[Zhou et al., 2016] Shuchang Zhou, Yuxin Wu, et al. Dorefa-\nnet: Training low bitwidth convolutional neural net-\nworks with low bitwidth gradients. arXiv preprint\narXiv:1606.06160, 2016.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nSurvey Track\n6831"
}