{
    "title": "Large Language Models Can Self-Improve",
    "url": "https://openalex.org/W4389523706",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5101940974",
            "name": "Jiaxin Huang",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5061613634",
            "name": "Shixiang Gu",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5101315776",
            "name": "Le Hou",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5082763858",
            "name": "Yuexin Wu",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5024842018",
            "name": "Xuezhi Wang",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5100537890",
            "name": "Hongkun Yu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5019539533",
            "name": "Jiawei Han",
            "affiliations": [
                "Google (United States)",
                "University of Illinois Urbana-Champaign"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2891012317",
        "https://openalex.org/W1592410721",
        "https://openalex.org/W4385571260",
        "https://openalex.org/W4304697829",
        "https://openalex.org/W4283768109",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4229445617",
        "https://openalex.org/W4283828387",
        "https://openalex.org/W2962800603",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W4221149883",
        "https://openalex.org/W4385571831",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W4288548690",
        "https://openalex.org/W4385571045",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2153332911",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4287126729",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W3138392969",
        "https://openalex.org/W2951936329",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4287393336",
        "https://openalex.org/W2976223659",
        "https://openalex.org/W4285429195",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W4389520273",
        "https://openalex.org/W4303648545",
        "https://openalex.org/W4366565380",
        "https://openalex.org/W4304194220",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2134797427",
        "https://openalex.org/W3034850762",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W2964277612",
        "https://openalex.org/W4281567264",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W4302366816",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W4379089709",
        "https://openalex.org/W4385571219",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4226369848",
        "https://openalex.org/W2794325560",
        "https://openalex.org/W4300978888",
        "https://openalex.org/W4226024653",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W4281975731",
        "https://openalex.org/W3153451655",
        "https://openalex.org/W3110300144",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W3023690688"
    ],
    "abstract": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate “high-confidence” rationale-augmented answers for unlabeled questions using Chain-of-Though (CoT) prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that without any ground truth label, our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%→82.1% on GSM8K, 90.0%→94.4% on OpenBookQA, and 63.4%→67.9% on ANLI-A3) and can also be adapted to extreme low-resource cases where even training questions and CoT prompts are limited. We conduct ablation studies and show that fine-tuning on diverse reasoning paths is critical for self-improvement.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1051–1068\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models Can Self-Improve\nJiaxin Huang1∗ Shixiang Shane Gu2 Le Hou2† Yuexin Wu2 Xuezhi Wang2\nHongkun Yu2 Jiawei Han1\n1University of Illinois at Urbana-Champaign 2Google\n1{jiaxinh3, hanj}@illinois.edu 2{shanegu, lehou, crickwu,\nxuezhiw, hongkuny}@google.com\nAbstract\nLarge Language Models (LLMs) have achieved\nexcellent performances in various tasks. How-\never, fine-tuning an LLM requires extensive su-\npervision. Human, on the other hand, may im-\nprove their reasoning abilities by self-thinking\nwithout external inputs. In this work, we\ndemonstrate that an LLM is also capable of\nself-improving with only unlabeled datasets.\nWe use a pre-trained LLM to generate “high-\nconfidence” rationale-augmented answers for\nunlabeled questions using Chain-of-Though\n(CoT) prompting and self-consistency, and fine-\ntune the LLM using those self-generated so-\nlutions as target outputs. We show that with-\nout any ground truth label, our approach sig-\nnificantly improves the general reasoning abil-\nity of PaLM 540B model (74.4%→82.1% on\nGSM8K, 90.0% →94.4% on OpenBookQA,\nand 63.4% →67.9% on ANLI-A3) and can\nalso be adapted to extreme low-resource cases\nwhere even training questions and CoT prompts\nare limited. We conduct ablation studies and\nshow that fine-tuning on diverse reasoning\npaths is critical for self-improvement.\n1 Introduction\nScaling has enabled Large Language Models\n(LLMs) to achieve state-of-the-art performance on\na range of Natural Language Processing (NLP)\ntasks (Wang et al., 2018, 2019; Rajpurkar et al.,\n2016). More importantly, new capabilities have\nemerged from LLMs as they are scaled to hun-\ndreds of billions of parameters (Wei et al., 2022b):\nin-context few-shot learning (Brown et al., 2020)\nmakes it possible for an LLM to perform well\non a task it never trained on with only a handful\nof examples; Chain-of-Thought (CoT) prompting\n(Wei et al., 2022c; Kojima et al., 2022) demon-\nstrates strong reasoning ability of LLMs across\ndiverse tasks with or without few-shot examples;\n∗Work was done during Google internship.\n†Corresponding author.\nself-consistency (Wang et al., 2022c) further im-\nproves the performance via self-evaluating multiple\nreasoning paths.\nDespite these incredible capabilities of models\ntrained on large text corpus (Brown et al., 2020;\nChowdhery et al., 2022), fundamentally improving\nthe model performances beyond few-shot baselines\nstill requires finetuning on an extensive amount\nof high-quality superviseddatasets. FLAN (Wei\net al., 2021; Chung et al., 2022) and T0 (Sanh et al.,\n2022) curated tens of benchmark NLP datasets to\nboost zero-shot task performances on unseen tasks;\nInstructGPT (Ouyang et al., 2022) crowd-sourced\nmany human answers for diverse sets of text in-\nstructions to better align their model to human\ninstructions; Minerva (Lewkowycz et al., 2022)\nparsed the full ArXiv database carefully for rele-\nvant articles to excel on challenging competitive\nmath and science datasets. The need for large anno-\ntated data for supervised LLM training still remains\na burden for low-resource applications or specific\ndomains where only limited annotations are avail-\nable.\nIn this paper, we study how an LLM capa-\nble of in-context few-shot learning and chain-of-\nthought reasoning, is able to self-improve its rea-\nsoning ability without supervised data. We show\nthat using only input sequences (without ground\ntruth output sequences) from multiple NLP task\ndatasets, a pre-trained LLM is able to improve per-\nformances for both in-domain and out-of-domain\ntasks. Our method is shown in Figure 1: we\nfirst sample multiple predictions using few-shot\nChain-of-Thought (CoT) (Wei et al., 2022c) as\nprompts, filter “high-confidence” predictions us-\ning majority voting (Wang et al., 2022c), and fi-\nnally finetune the LLM on these high-confidence\npredictions. The resulting model shows improved\nreasoning in both greedy and multi-path evalu-\nations. We call the model fine-tuned in this\nway as Language Model Self-Improved (LMSI).\n1051\nNote that LMSI depends on in-context few-shot\nlearning and chain-of-thought reasoning abilities\nwhich small language models do not necessar-\nily have. We empirically verify LMSI using a\npre-trained 540B PaLM model (Chowdhery et al.,\n2022), where our method not only significantly im-\nproves training task performances (74.4%→82.1%\non GSM8K, 90.0%→94.4% on OpenBookQA, and\n63.4%→67.9% on ANLI-A3), but also enhances\nout-of-domain (OOD) tasks, without relying on\nsupervised ground truth answers. Lastly, we ex-\nplore more extreme cases where training questions\nand human-curated CoTs are also limited, and pro-\npose self-generating additional input questions and\nfew-shot CoT prompts for model self-improving.\nWe hope our simple approaches and strong empir-\nical results could inspire more future work by the\ncommunity to investigate optimal performances of\npretrained LLMs without additional human super-\nvision.\nOur contributions are summarized as follows:\n• We demonstrate that a large language model can\nself-improve by taking datasets without ground\ntruth outputs, by leveraging CoT reasoning (Wei\net al., 2022c) and self-consistency (Wang et al.,\n2022c) to generate diverse reasoning paths for\nself-training, and can achieve great improvments\non in-domain multi-task performances as well as\nout-of-domain generalization.\n• We provide detailed ablation studies on train-\ning sample formatting and sampling tempera-\nture after fine-tuning, and identify critical design\nchoices for most successful self-improvement by\nLLMs.\n• We further propose two approaches for model\nself-improving under extreme low-resource cases\nwhere even training questions and CoT prompts\nare limited, and achieve 74.2% on zero-shot\nGSM8K, against 43.0% by Kojima et al. (2022)\nor 70.1% through its naive extension with Wang\net al. (2022c).\nThe rest of this paper is organized as follows.\nSection 2 discusses related work. Section 3 lays\nout our method in detail. Section 4 shows our\nsetup for experiments. Section 5 demonstrates our\nexperiment results with ablation studies. Section 6\nconcludes our work. The chain-of-thought prompts\nused in our work are included in Appendix A.\n2 Related Work\nLearning from explanations. Augmenting a ma-\nchine learning model with explanations has been\nstudied in existing literature extensively. For ex-\nample, in the supervised learning setting, a model\ncan be fine-tuned using human-annotated rationales\n(Zaidan et al., 2007; Ling et al., 2017a; Narang\net al., 2020; Camburu et al., 2018; Cobbe et al.,\n2021; Chung et al., 2022). A few works have\nalso looked at how explanations can help the mod-\nels in various settings, e.g., in-context learning\n(Lampinen et al., 2022) and in distillation (Pruthi\net al., 2022). Lightman et al. (2023) treat explana-\ntions as process supervision to train a reward model.\nIn this paper, we focus more on the unsupervised\nlearning setting, where we do not assume we have\na rationale-augmented training dataset available,\nsince human-annotated rationales can be expen-\nsive.\nFew-shot explanations improves reasoning in\nLLMs. Recently, a lot of progress has been\nmade towards improving LLMs’ reasoning abil-\nities via prompting or in-context learning. Wei\net al. (2022c) propose Chain-of-Thought prompt-\ning, which prompts the language model to gener-\nate a series of natural-language-based intermediate\nsteps, and show it can help language models better\nsolve complex and multi-step reasoning tasks, with\nrecent study (Wang et al., 2022a) analyzing the rel-\nevant contents and correct reasoning order being\nthe most crucial factor of the success of Chain-of-\nThought prompting. Wang et al. (2022c) improve\nChain-of-Thought prompting by sampling multiple\ndiverse reasoning paths and finding the most con-\nsistent answers via majority voting. Kojima et al.\n(2022); Zhang et al. (2022) propose to prompt the\nlanguage model with “Let’s think step by step” to\ngenerate reasoning in a zero-shot fashion. Zhou\net al. (2022) decompose the questions into multiple\nsub-questions, and ask the language model to solve\neach sub-question sequentially.\nRefining explanations. More recent work pro-\nposes to further refine the generated reasoning\npaths as some of them could be unreliable. For\nexample, Ye and Durrett (2022) calibrate model\npredictions based on the reliability of the explana-\ntions, Jung et al. (2022) show that inducing a tree\nof explanations and inferring the satisfiability of\neach explanation can further help judge the correct-\nness of explanations. Li et al. (2022a) show that\n1052\n2\nAlex is 10-8 = 2 \nyears old.\nAlex’s age is in the \nmiddle of 8 and 10. \nAlex is 9 years old.\n(8+10)/2 = 9. \nThe answer is 9.\n…\nLanguage \nModel\nQ: John buys 20 cards and 1/4 are \nuncommon. How many uncommon \ncards did he get?\nA: John gets 20 * 1/4 = 5 uncommon \ncards. The answer is 5.\n…\nQ: Amy is 10. Jake is 8. Alex’s age is \nright in the middle. How old is Alex?\nA:\nCoT examples\nQ:\nA:\nQ: … How old is Alex?\nA: \n… (8+10)/2=9 … answer is 9.\nMixed formats of selected reasoning pathsSelf-training\nTraining-set questions or \nself-generated questions\nMultiple path \ndecoding\nMajority \nVoting\nby answer\n<latexit sha1_base64=\"HOCrR75PTdXMU4JpjxYtpz9BBcY=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkVI8FLx4r2A9oQ9lsNu3aTTbsToRS+h+8eFDEq//Hm//GbZuDtj4YeLw3w8y8IJXCoOt+O4WNza3tneJuaW//4PCofHzSNirTjLeYkkp3A2q4FAlvoUDJu6nmNA4k7wTj27nfeeLaCJU84CTlfkyHiYgEo2ildp+FCs2gXHGr7gJknXg5qUCO5qD81Q8Vy2KeIJPUmJ7npuhPqUbBJJ+V+pnhKWVjOuQ9SxMac+NPF9fOyIVVQhIpbStBslB/T0xpbMwkDmxnTHFkVr25+J/XyzC68aciSTPkCVsuijJJUJH56yQUmjOUE0so08LeStiIasrQBlSyIXirL6+T9lXVq1dr97VKo57HUYQzOIdL8OAaGnAHTWgBg0d4hld4c5Tz4rw7H8vWgpPPnMIfOJ8/ramPLA==</latexit>\n···\n<latexit sha1_base64=\"HOCrR75PTdXMU4JpjxYtpz9BBcY=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkVI8FLx4r2A9oQ9lsNu3aTTbsToRS+h+8eFDEq//Hm//GbZuDtj4YeLw3w8y8IJXCoOt+O4WNza3tneJuaW//4PCofHzSNirTjLeYkkp3A2q4FAlvoUDJu6nmNA4k7wTj27nfeeLaCJU84CTlfkyHiYgEo2ildp+FCs2gXHGr7gJknXg5qUCO5qD81Q8Vy2KeIJPUmJ7npuhPqUbBJJ+V+pnhKWVjOuQ9SxMac+NPF9fOyIVVQhIpbStBslB/T0xpbMwkDmxnTHFkVr25+J/XyzC68aciSTPkCVsuijJJUJH56yQUmjOUE0so08LeStiIasrQBlSyIXirL6+T9lXVq1dr97VKo57HUYQzOIdL8OAaGnAHTWgBg0d4hld4c5Tz4rw7H8vWgpPPnMIfOJ8/ramPLA==</latexit>\n···\n9\n… Alex is 9 years old … \n…\nInput:\nOutput:\nCoT examples\nQ: … How old is Alex?\nA: Let’s think step-by-step.\nFigure 1: Overview of our method. With Chain-of-Thought (CoT) examples as demonstration (Wei et al., 2022c),\nthe language model generates multiple CoT reasoning paths and answers (temperature T >0) for each question.\nThe most consistent answer is selected by majority voting (Wang et al., 2022c). The CoT reasoning paths that lead\nto the answer with the highest confidence are augmented by mixed formats, and are fed back to the model as the\nfinal training samples.\nsampling a diverse set of prompts from the training\ndata, and a voting verifier can be used to improve\nmodel’s reasoning performance. Xi et al. (2023)\nand Zheng et al. (2023) propose to polish the prob-\nlem progressively before the model reaching a sta-\nble answer. Zelikman et al. (2022) proposes better\nrationale generation by augmenting ground truth\nanswers as hints when predicted answers are incor-\nrect. Our work is orthogonal to these lines of work,\nas we utilize refined explanations for model self-\nimprovement, and could readily incorporate these\nother refinement techniques for generating higher-\nquality self-training data. Our work is closely re-\nlated to Zelikman et al. (2022) where we both pro-\npose to fine-tune a model on self-generated CoT\ndata, but our method does not require ground truth\nlabels and shows stronger empirical results with\nmulti-task generalization. Different from existing\nwork, we show that a mixture of the reasoning-\npath refinement techniques can be combined to fur-\nther improve the quality of the generated reasoning\npaths, which is shown to be effective in boosting\nmodel’s performance via self-improvement.\nSelf-training models. One related line of work\nis self-training (see a survey from Amini et al.\n(2022)). The key idea is to assign pseudo labels\nfrom a learned classifier to unlabeled data, and use\nthese pseudo-labeled examples to further improve\nthe original model training, e.g., (RoyChowdhury\net al., 2019; Xie et al., 2020; He et al., 2020; Chen\net al., 2021). Different from such prior work, our\nproposed self-improvement framework uses CoT\nprompting plus self-consistency to obtain high-\nconfidence solutions on a large set of unlabeled\ndata to augment the fine-tuning process.\nDistillation and dark knowledge. Language\nmodels are known to preserve parametric knowl-\nedge (Schick and Schütze, 2020a,b) during the pre-\ntraining stage. Our method tangentially relates\nto rich literature on distillation (Ba and Caruana,\n2014; Hinton et al., 2015), where a student net-\nwork imitates a teacher network’s classifier pre-\ndictions on input examples. A key detail is to\nlearn from soft targets instead of hard predicted\nlabels, as softmax outputs with a high temperature\nreveal more detailed relative class likelihoods, col-\nloquially known as dark knowledge(Hinton et al.,\n2015; Korattikara Balan et al., 2015). Recent stud-\nies (Zelikman et al., 2022; Snell et al., 2022; Eisen-\nstein et al., 2022) show that dark knowledgewithin\nLLMs can be retrieved with more computation at\ninference time, such as adding informative instruc-\ntions into the input sequence and output CoT gen-\neration (Wei et al., 2022c; Kojima et al., 2022).\nRecent works (Magister et al., 2022; dhar et al.,\n2023; Ho et al., 2023) demonstrated that distilla-\ntion on explanations generated from large models\ncan increase the reasoning abilities of smaller mod-\nels with ground truth filtering.\n3 Method\nThe overview of our method is illustrated in\nFig. 1: We are given a pre-trained Large Lan-\nguage Model (LLM) M and a question-only train-\n1053\nTable 1: Examples of 3 self-generated CoT reasoning paths given a question. Output 1 and 3 are the most consistent\nreasoning paths based on majority voting and kept as self-training data.\nQuestion: Stefan goes to a restaurant with his family. They order an appetizer that costs $10 and\n4 entrees that are $20 each. If they tip 20% of the total, what is the total amount of money that\nthey spend?\nMultiple Path Decoding:\nOutput 1: The appetizer costs $10. The entrees cost $20 each so in total 4 * $20 = $80. This means\nthe total cost is $10 + $80 = $90. They tip 20% of it, so the total amount they spend is $90 * 1.2\n= $108. The answer is 108.\nOutput 2: The appetizer costs $10 and the entrees are $20 each. There are 4 entrees so the sum is\n$20 * 4 = $80. The waiter gets 20% of the total. 20% of $80 is $80 * .2 = $16. The answer is $80\n+ $16 = $96. (Incorrect reasoning path)\nOutput 3: The appetizer costs $10. The entrees cost 4 * $20 = $80. The tip is 20% of the total, so\nit is 20% of the $90 they have spent. The tip is 0.2 * 90 = $18. The total they spent is $90 + $18\n= $108. The answer is 108.\ning dataset Dtrain = {xi}D\ni=1 with few-shot Chain-\nof-Thought (CoT) examples (Wei et al., 2022c).\nWe apply multiple path decoding with a sampling\ntemperature T > 0 for generating m reason-\ning paths and answers {ri1 , ri2 , . . . , rim }for each\nquestion xi in Dtrain, and use majority voting (self-\nconsistency) to select the most consistent, highest\nconfidence answer (Wang et al., 2022c). We then\nkeep all reasoning paths that lead to the most con-\nsistent answer, apply mixed formats of prompts\nand answers for augmentation, and fine-tune the\nmodel on these self-generated reasoning-answer\ndata. We consider our approach as making the\nmodel self-improve. In the following sections, we\ndetail important designs within our method, along\nwith additional approaches for the model to self-\nimprove without supervised data.\n3.1 Generating and Filtering Multiple\nReasoning Paths\nSelf-consistency (Wang et al., 2022c) brings large\nimprovements on reasoning tasks (e.g., 56.5% →\n74.4% on GSM8K test set), and the gap between\ngreedy decoding and diverse decoding shows there\nis a potential for further improving the reason-\ning ability of M, using the self-selected high-\nconfidence reasoning paths as training data.\nFor each training question xi, we sample m CoT\nreasoning paths, denoted as{ri1 , ri2 , . . . , rim }(see\nTable 1 for examples). An example of a train-\ning question with the self-generated CoT rea-\nsoning paths is shown in Table 1. Since M is\n0.0 0.2 0.4 0.6 0.8 1.0\nConfidence\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n0\n200\n400\n# of Questions\nFigure 2: The relation of accuracy and confidence of\nthe majority-voted answer after multiple path decoding\non GSM8K training-set questions. A recent study (Ka-\ndavath et al., 2022) shows that language models are not\nperfectly-calibrated though their calibration increases\nwith model size, and models with more than 10B param-\neters are reasonably calibrated on some few-shot tasks.\nThis aligns well with our study and serve as the basis of\nthis self-improving method.\nprompted with the CoT examples from Wei et al.\n(2022c), we apply the same output parsing with\n“The answer is” to generate their predicted an-\nswers {yi1 , yi2 , . . . , yim }. The most consistent\nanswer, which is not necessarily a correct an-\nswer, is selected by majority voting, denoted as\n˜yi = arg maxyij\n∑m\nk=1 I(yij = yik ). In Table 1,\nthe most consistent answer ˜y is 108, derived by\noutput path 1 and output path 3, while the output\npath 2 makes a mistake in calculating the cost of\nthe foods. For all the training questions, we fil-\nter the CoT reasoning paths that reach ˜y as the\nfinal answer to be put into the self-training data,\n1054\nTable 2: An example of how a reasoning path is augmented into four formats of training data with different prompts\n(in input) and answer styles (in output). Specifically, the CoT prompting examplesused for each tasks are listed in\nAppendix A.2. The Standard prompting examplesare the same question-answer pairs with CoT prompting examples,\nexcept that reasoning is removed.\nQuestion: Amy is 10 years old. Jake is 8 years old. Alex’s age is right in the middle. How old is Alex?\nSelected Chain-of-Thought: Amy is 10 years old. Jake is 8 years old. Alex’s age is in the middle of\nAmy and Jake, so Alex is ( 8 + 10 ) / 2 = 9 years old. The answer is 9.\nMixed-formats of training data:\nFormat 1: Input: [CoT prompting examples]+ ‘\\n’ + [Question]+ ‘\\n’ + ‘A:’\nOutput: Amy is 10 years old. Jake is 8 years old. Alex’s age is in the middle of Amy and Jake, so Alex\nis ( 8 + 10 ) / 2 = 9 years old. The answer is 9.\nFormat 2: Input: [Standard prompting examples]+ ‘\\n’ + [Question]+ ‘\\n’ + ‘A:’\nOutput: The answer is 9.\nFormat 3: Input: [Question]+ ‘\\n’ + ‘A: Let’s think step by step.’\nOutput: Amy is 10 years old. Jake is 8 years old. Alex’s age is in the middle of Amy and Jake, so Alex\nis ( 8 + 10 ) / 2 = 9 years old. The answer is 9.\nFormat 4: Input: [Question]+ ‘\\n’ + ‘A:’\nOutput: The answer is 9.\ndenoted as Dself−consistent = {xi, ˜ri}, where\n˜ri = {rij |1 ≤j ≤m, yij = ˜yi}.\nSince we do not use any ground truth labels\nto filter out cases where ˜yi ̸= yi, it is important\nthat the self-generated CoT reasoning paths are\nmostly reliable and incorrect answers do not hurt\nthe self-improvement of the model. We plot the re-\nlation between the accuracy and confidence of self-\ngenerated CoT paths for each question in GSM8K\ntraining set in Fig. 2. The confidence is the num-\nber of CoT paths leading to ˜y divided by the total\npath number m. The y-axis shows the accuracy\nof ˜y under a certain confidence. The circle area\nand the color darkness shows the number of ques-\ntions under a certain confidence. We can observe\nthat confident answers are more likely to be cor-\nrect, which means that when a question has many\nconsistent CoT paths, then the corresponding ˜y is\nmore likely to be correct. On the other hand, when\n˜y is wrong, it is likely to be supported by fewer\nCoT paths, and brings little noise to the training\nsamples.\n3.2 Training with Mixed Formats\nTo prevent the language model from overfitting\nto specific prompts or answer styles, we create\nfour different formats for each reasoning path to\nbe mixed in the self-training data, shown in Ta-\nble 2. In the first format, a few Chain-of-Thought\nexamples (questions followed by reasoning paths\nleading to the correct final answers) are prepended\nto the new question, while the language model\noutput is trained to be the same with the filtered\nCoT reasoning paths. In the second format, we\nuse examples of questions and their direct answers\nas standard prompting, and the language model\noutput is supposed to also only contain the direct\nanswer. The third and fourth format are similar to\nthe first and second format, except that no exam-\nple of question-answer pairs are given, so that the\nmodel will learn to think on its own in an in-context\nzero-shot manner. In the third format, where we\nwant the model to output CoT reasoning without\nprepending examples containing CoT reasonings,\nwe append “Let’s think step by step.” at the end of\nthe input sequence, to guide the language model\nto generate step-by-step CoT reasoning paths (Ko-\njima et al., 2022). The mixed formats of training\nsamples are then used to fine-tune the pre-trained\nlanguage model M.\n3.3 Generating Questions and Prompts\nIn some cases where even training questions\nor human-curated CoT prompts are limited, our\nmethod may not generate sufficient training sam-\nples for language model self-training. Therefore,\n1055\nwe investigate how to self-generate more training\nquestions as well as example prompts to further\nreduce human effort.\nQuestion Generation. Previous work (Yoo et al.,\n2021; Meng et al., 2022) discuss few-shot data aug-\nmentation by generating diverse training samples\nusing LLMs. However, those methods are designed\nfor classification tasks and require ground truth la-\nbel for each few-shot example. We use a simple\nyet effective approach to generate diverse questions\n(without using ground truth answers) from a few\nexample questions. Specifically, we randomly sam-\nple and concatenate example questions in a random\norder as input prompt, and let the language model\ngenerate consecutive sequences as new questions.\nWe repeat the process to obtain a large set of new\nquestions, then use self-consistency (Wang et al.,\n2022c) to only keep the questions that have a highly\nconfident answer. Those questions are then used as\nself-generated training questions.\nPrompt Generation. Given a set of questions,\nhumans can write CoT examples as reasoning paths\nleading to the final answer. In zero-shot setting\nwithout manual prompts, we can generate these\nCoT paths using the model itself. Following (Ko-\njima et al., 2022), we start the answer with “A: Let’s\nthink step by step.” and let the language model gen-\nerate the consecutive reasoning paths. We then use\nthose generated reasoning paths as examples for\nfew-shot CoT prompting.\n4 Experimental Setup\nTasks and Datasets. We demonstrate the effec-\ntiveness of our method on three types of tasks1:\n• Arithmetic reasoning: We use the math problem\nset GSM8K (Cobbe et al., 2021), and a reading\ncomprehension benchmark DROP (Dua et al.,\n2019) which requires numerical reasoning. We\nfollow (Zhou et al., 2022) to partition the DROP\ndataset into football related and non-football re-\nlated subsets for training.\n• Commonsense reasoning: We use the Open-\nBookQA (Mihaylov et al., 2018) dataset, and the\nAI2 Reasoning Challenge (ARC) (Clark et al.,\n2018) dataset. Note that for ARC, we only use\n1We evaluate on the test sets of GSM8K, ARC, Open-\nBookQA, and ANLI, and the dev set of DROP (ground truth\nlabels of the test set are not publicly available).\nthe Challenge sub-set (ARC-c) in our experi-\nments. Both datasets contain multiple-choice\nquestions.\n• Natural Language Inference: We use the Ad-\nversarial NLI (ANLI) (Mihaylov et al., 2018)\nsubsets, ANLI-A2 and ANLI-A3, which are the\nmore challenging subsets compared to ANLI-A1.\nThese datasets contain pairs of sentences with\nrelations of entailment, neutral, or contradiction.\nModels, Training settings and Hyperparameters.\nWe follow previous studies (Wei et al., 2022c;\nWang et al., 2022c) and conduct our experiments on\nthe PaLM 540B model (Chowdhery et al., 2022), an\nautoregressive Transformer-based language model.\nThe CoT examples for each dataset are listed in\nAppendix A.2. We generate m = 32 reasoning\npaths for each question in a training set, followed\nby format augmentation in Sec. 3.2. For DROP and\nANLI-A2/A3, we sample 5k examples for reason-\ning path generation to reduce the training burden;\nFor other datasets, we keep the whole training set.\nFor each dataset, we fine-tune the model for 10k\nsteps with a learning rate of 5e−5 and a batch size\nof 32. We use a sampling temperature of T = 0.7\nwith the pre-trained model as suggested by (Wang\net al., 2022c). We use T = 1.2 for the language\nmodel after self-improvement (LMSI ). We set the\nmaximum number of decoded steps to 256 for all\nexperiments.\n5 Experiments and Results\nWe conduct a series of experiments to demonstrate\nthe effectiveness of our proposed self-improving\nmethod. First, we apply our method on each in-\ndividual dataset (task) and report the results. We\nthen merge the generated data from all datasets and\ntrain one model to study the generalization ability\nof the model on unseen datasets as in (Wei et al.,\n2021). In addition to the results of using generated\nCoT reasoning paths, we show studies on generat-\ning input questions and few-shot prompts. We end\nwith ablation studies on model sizes and hyperpa-\nrameters.\n5.1 Main Results\nWe list the results of using the 540B PaLM model\nbefore and after LMSI in Table 3. For each model,\nduring test time, we apply three separate prompting\nmethods on all six datasets: standard-prompting,\nCoT-Prompting, and Self-Consistency. We observe\n1056\nTable 3: Accuracy results on six reasoning benchmarks with or without LMSI using different prompting method.\nPrompting Methodw. or w/oLMSI GSM8K DROP ARC -c OpenBookQA ANLI-A2 ANLI-A3\nStandard-Prompting w/oLMSI 17.9 60.0 87.1 84.4 55.8 55.8\nw.LMSI 32.2(+14.3) 71.7(+11.7) 87.2(+0.1) 92.0(+7.6) 64.8(+9.0) 66.9(+11.1)\nCoT-Prompting w/oLMSI 56.5 70.6 85.2 86.4 58.9 60.6\nw.LMSI 73.5(+17.0) 76.2(+5.6) 88.3(+3.1) 93.0(+6.6) 65.3(+6.4) 67.3(+6.7)\nSelf-Consistency w/oLMSI 74.4 78.2 88.7 90.0 64.5 63.4\nw.LMSI 82.1 (+7.7) 83.0(+4.8) 89.8(+1.1) 94.4(+4.4) 66.5(+2.0) 67.9(+4.5)\nTable 4: Comparison of CoT-prompting accuracy results on six Out-Of-Domain benchmarks with or without training\non six In-Domain (GSM8K, DROP, ARC-c, OpenBookQA, ANLI-A2, ANLI-A3) training-set questions.\nSelf-training data AQUA SV AMP StrategyQA ANLI-A1 RTE MNLI -M/MM\nw/oLMSI - 35.8 79.0 75.3 68.8 79.1 72.0/74.0\nw.LMSI GSM8K + DROP + ... 39.0(+3.2) 82.8(+3.8) 77.8(+2.5) 79.2(+10.4) 80.1(+1.0) 81.8/82.2(+9.8/+8.2)\nthat after LMSI , the performance of all three\nprompting methods increase by a large margin. We\nobserve significant improvement, comparing self-\nconsistency versus LMSI with self-consistency:\n+7.7% on GSM8K, +4.8% on DROP, +4.4% on\nOpenBookQA, and +4.5% on ANLI-A3. This\nshows that our proposed method is quite effective.\nFurthermore, the single path CoT-Prompting perfor-\nmance of LMSI is close to or even better than the\nmultiple path Self-Consistency performance of the\nmodel without LMSI , showing that LMSI truly\nhelps the language model learn from the multiple\nconsistent reasoning paths. We also apply LMSI\non a recently proposed public language model, UL2\n(20B) (Tay et al., 2022), and show the results in Ap-\npendix A.1. Compared to the 540B PaLM model\n(decoder-only), UL2 has a smaller scale, and a dif-\nferent architecture (encoder-decoder). We observe\nthat for most datasets, LMSI still outperforms the\noriginal UL2 results, but the improvement is not as\nlarge as that on the 540B PaLM model.\nMulti-task self-training for unseen tasks. To\ndemonstrate the generalization ability of LMSI ,\nwe conduct experiments of self-training on a mix-\nture of the training-set questions from the above\nsix datasets (denoted as In-Domain tasks), then use\nthe same model checkpoint for the evaluation on\nsix Out-Of-Domain (OOD) tasks, as shown in Ta-\nble 4. Of all the OOD tasks: (1) AQUA(Ling et al.,\n2017b) and SV AMP(Patel et al., 2021) are arith-\nmetic reasoning tasks; (2) StrategyQA (Geva et al.,\n2021) is a commonsense reasoning task; (3) ANLI-\nA1 (Nie et al., 2019), RTE (Dagan et al., 2005)\nand MNLI-M/MM (Williams et al., 2018) are nat-\nural language inference tasks.2 Among these tasks,\nAQUA, StrategyQA, and RTE are significantly\ndifferent from any In-Domain task, and have their\nown few-shot prompts. From Table 4, we observe\nthat LMSI achieves higher accuracy results on all\nOOD tasks, showing that the overall reasoning abil-\nity of the language model is improved.\nImportance of training with augmented formats.\nWe demonstrate the importance of training lan-\nguage models with augmented formats (both Chain-\nof-Thought prompting and direct prompting, and\nboth few-shot prompting and zero-shot prompt-\ning). In Table 5, we list the results of LMSI with\nall four formats, the results of LMSI with only\ndirect answer formats, and the results of LMSI\nwith only few-shot Chain-of-Thought prompting\nformats. The results show that without the CoT\nformats, the language model can still self-improve,\nbut the performance gain drops by a large amount\ncompared to using all four formats. However, if\nonly using few-shot CoT prompting format for self-\ntraining, the model can overfit to the prompting\nstyle and may not generalize well on downstream\ntasks.\n5.2 Pushing the limit of self-improvements\nSelf-Generating Questions We further explore\nthe few-shot setting where there are only lim-\nited training questions in the target domain. On\nGSM8K, we sample 10 real questions as few-shot\n2We evaluate on the test set of SV AMP and ANLI, the\ndev set of MNLI and RTE (ground truth labels of the test\nsets are not publicly available). For StrategyQA we use the\nquestion-only set from (bench collaboration, 2022).\n1057\nTable 5: Ablation study: LMSI with different combina-\ntions of training format on GSM8K dataset.\nResults onGSM8K\nStd. Prompting CoT Prompting\nw/oLMSI 17.9 56.5\nLMSIw/o CoT formats 23.6(+5.7) 61.6(+5.1)\nLMSIonly few-shot CoT 29.2(+11.3) 69.4(+12.9)\nLMSIw/ CoT formats 32.2 (+14.3) 73.5(+17.0)\nsamples, and use the language model to generate\nmore training questions using the method in Sec-\ntion 3.3. We then self-train the language model\nwith these generated questions and list the results in\nTable 6. The results show that using self-generated\nquestions still improves the reasoning ability of\nlanguage models, but using the real training-set\nquestions leads to better results.\nTable 6: Accuracy on GSM8K test set after self-training\non different question sets. Results are shown for both\nCoT-Prompting (CoT) and Self-Consistency (SC).\nQuestions used GSM8K\nfor Self-Training CoT SC\nw/oLMSI - 56.5 74.4\nw.LMSI Generated 66.2 (+9.7) 78.1(+3.7)\nw.LMSI Training-set 73.5 (+17.0) 82.1(+7.7)\nSelf-Generating Few-Shot CoT Prompts. We\nexplore the situation where no in-domain CoT ex-\namples are provided for a task. We apply the Step-\nby-Step method (Kojima et al., 2022) to generate\nCoT examples using the language model as de-\nscribed in Section 3.3, and show the results in Fig-\nure 3. We observe that few-shot prompting with\nself-generated Step-by-Step CoT examples substan-\ntially outperforms the Step-by-Step (Kojima et al.,\n2022) baseline (66.2% vs 53.8% at 10 paths, 74.2%\nvs 70.1% at 40 paths), and nearly matches the per-\nformance of human-written few-shot CoT (Wei\net al., 2021) (74.4% at 40 paths (Wang et al.,\n2022c)). The strong performance of “Few-Shot\nw/ Step-by-Step” despite the limited accuracy of\nprompt examples (43.0% for greedy Step-by-Step)\nlikely comes from leveraging more diverse CoT\nprompts for multi-path decoding (Li et al., 2022b),\nwhere at 40 paths it uses 20 generate prompt-\ntemplates, each with 4-shot CoT examples, i.e. a\ntotal of 80 generated CoT examples compared to 8\nhuman-written examples use in Wei et al. (2022c).\nSince we did not use training questions or few-shot\nCoT examples, 74.2% also marks the new state-of-\nthe-art zero-shot performance on GSM8K.\n10 20 30 40 50 60 70 80\nTotal Sample Paths\n0.55\n0.60\n0.65\n0.70\n0.75Accuracy\nStep-by-Step\nFew-Shot w/ Step-by-Step\nFigure 3: Accuracy results on GSM8K test set us-\ning 540B model with multi-path sampling and self-\nconsistency (Wang et al., 2022c). “Step-by-Step” is the\nbaseline performance of Kojima et al. (2022) plus self-\nconsistency (Wang et al., 2022c), while our “Few-Shot\nw/ Step-by-Step” uses exemplers self-generated from\nStep-by-Step (greedy decoding) for few-shot prompting\nthe LLM.\n5.3 Distillation to smaller models\nTable 7: Distillation from 540B model to small models.\nWe see that distilled smaller models outperform models\nthat are one-tier larger.\nResults onGSM8K\n8 billion 62 billion 540 billion\nw/oLMSI 5.0 29.7 56.5\nDistilled fromLMSI 33.4(+28.4) 57.4(+27.7) -\nWe also explore whether the knowledge can\nbe distilled to smaller models, such as in distil-\nlation (Hinton et al., 2015) and in Zelikman et al.\n(2022). We use the same set of training samples\ngenerated by the 540B PaLM model, but fine-tune\non models with smaller sizes (8B PaLM model\nand 62B PaLM model respectively), and show the\nresults of CoT-prompting in Table 7. It is interest-\ning to point out that after distillation from LMSI\n, the 62B model can outperform the pre-trained\n540B model, and the 8B model can outperform\nthe pre-trained 62B model. This implies that for\ndownstream applications with limited computing\nresources, the reasoning knowledge from large\nmodels can be used to largely enhance small mod-\nels to achieve competitive performance.\n5.4 Hyperparameter Studies\nSampling Temperature after Self-Improvement.\nWe study the effect of varying the temperature T\n1058\nfor multiple path decoding after LMSI is applied.\nSpecifically, we vary T between [0.7, 1.0, 1.2, 1.5]\nand show the results on GSM8K and DROP dataset\nrespectively in Fig. 4. As shown in the figure,\nT = 1.2 benefits both datasets the most, and is\nused in the Self-Consistency method for LMSI\non all datasets. We notice that the optimal T af-\nter model self-improvement is larger than the op-\ntimal T = 0.7 (Wang et al., 2022c) before self-\nimprovement. We believe the reason is that after\ntraining the model, the entropy of the output distri-\nbution is reduced.\n0.7 1.0 1.2 1.5\nSampling Temperature\n80\n81\n82\n83Accuracy\n DROP\nGSM8K\nFigure 4: Accuracy results of LMSI on GSM8K and\nDROP test set when different sampling temperatures are\napplied for Self-Consistency.\nNumber of Sampled Reasoning Paths. We\nstudy whether the number of sampled reasoning\npaths m for Self-Consistency largely affects the\naccuracy after LMSI is applied. We show the ac-\ncuracy on GSM8K test set for models both with\nor without LMSI in Fig. 5. For both cases, setting\nm = 15already achieves a reasonably good accu-\nracy, and using a larger m only brings marginal\nimprovements. We also notice that after Self-\nImprovement, using 5 paths for Self-Consistency\ncan already surpass the performance of using 32\npaths for model without Self-Improvement. Thus,\nwith a well-improved model, huge computing re-\nsources can be saved when applied to real applica-\ntions.\n6 Conclusions\nWe demonstrated that a Large Language Model\n(LLM) is capable of improving its performance on\nreasoning datasets by training on its own generated\nlabels, given input questions only. Experiments\nusing the PaLM model with 540 billion parameters\nshow that LMSI improves the accuracy scores by\n1.1% to 7.7% on six datasets, without training on\nground truth labels. Furthermore, we show that\n1 5 10 15 20 25 32\n# Sampled Reasoning Path\n50\n60\n70\n80Accuracy\n LMSI\nw/o LMSI\nFigure 5: Accuracy results with or without LMSI on\nGSM8K test set using different numbers of sampled\nreasoning path for Self-Consistency.\nit is possible for the LLM to self-improve even\non its own generated questions and few-shot CoT\nprompts. As part of our future work, we plan to\ncombine large-scale generated data from LMSI\nand existing supervised data, to further improve the\nperformance of LLMs.\nLimitations\nOur approach mainly relies on the effectiveness of\ndemonstration-based in-context few-shot learning\nwhich works most effectively on large language\nmodels, according to Wei et al. (2022a). For exam-\nple, Zelikman et al. (2022) showed that a 6B model,\nGPT-J, achieves only 3.1% accuracy on GSM8K\nwith few-shot CoT prompting, while GPT-3 (175\nB) achieves 46.9%, according to Wei et al. (2022c).\nMoreover, a recent study (Kadavath et al., 2022)\nshows that language model calibration increases\nwith model size. This aligns well with our observa-\ntions that larger models are better at self-improving.\nBased on these existing studies, we believe that\nLMSI is more applicable to large-scale language\nmodels. In addition, we show that distillation from\nlarge models to small models are very promising\nin Sec. 5.3. Therefore, smaller models can also\nbe improved when large model APIs are accessi-\nble. We are fortunate to have enough resources for\nthis work. Though the computation requirements\nfor training large-scale language models are still\nprohibitively high for most researchers to conduct\nempirical studies along this line, we believe that\nour findings are conceptually useful for the NLP\ncommunity by providing new insights for the prop-\nerties of large language models.\nAcknowledgments\nWe thank anonymous reviewers for valuable and\ninsightful feedback.\n1059\nReferences\nMassih-Reza Amini, Vasilii Feofanov, Loic Pauletto,\nEmilie Devijver, and Yury Maximov. 2022. Self-\ntraining: A survey.\nJimmy Ba and Rich Caruana. 2014. Do deep nets really\nneed to be deep? Advances in neural information\nprocessing systems, 27.\nBIG bench collaboration. 2022. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities\nof language models. ArXiv, abs/2206.04615.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Neurips.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language expla-\nnations. In S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing\nSystems 31, pages 9539–9549. Curran Associates,\nInc.\nXiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong\nWang. 2021. Semi-supervised semantic segmenta-\ntion with cross pseudo supervision. In IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek B Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinodku-\nmar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier García,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Oliveira\nMoreira, Rewon Child, Oleksandr Polozov, Kather-\nine Lee, Zongwei Zhou, Xuezhi Wang, Brennan\nSaeta, Mark Díaz, Orhan Firat, Michele Catasta, Ja-\nson Wei, Kathleen S. Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm:\nScaling language modeling with pathways. ArXiv,\nabs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Adams\nYu, Albert Webson, Xinyun Chen, Gaurav Mishra,\nZhuyun Dai, Shixiang Shane Gu, Mirac Suzgun, Vin-\ncent Zhao, Aakanksha Chowdhery, Sharan Narang,\nYanping Huang, Andrew Dai, Hongkun Yu, Ed H.\nChi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny\nZhou, Quoc V . Le, and Jason Wei. 2022. Scaling\ninstruction-finetuned language models. In arxiv.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nJacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training verifiers to solve\nmath word problems. ArXiv, abs/2110.14168.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In MLCW.\nKumar Shri dhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2023. Distilling reasoning capabilities into\nsmaller language models. In ACL.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDrop: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In NAACL.\nJacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael\nCollins, and David Mimno. 2022. Honest students\nfrom untrusted teachers: Learning an interpretable\nquestion-answering pipeline from a pretrained lan-\nguage model. arXiv preprint arXiv:2210.02498.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346–\n361.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In International Conference on\nLearning Representations.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nNamgyu Ho, Laura Schmid, and Se-Young Yun.\n2023. Large language models are reasoning teachers.\nArXiv.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations.\n1060\nSaurav Kadavath, Tom Conerly, Amanda Askell, T. J.\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zachary Dodds, Nova DasSarma, Eli Tran-\nJohnson, Scott Johnston, Sheer El-Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, John\nKernion, Shauna Kravec, Liane Lovitt, Kamal\nNdousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom B. Brown, Jack Clark, Nicholas Joseph,\nBenjamin Mann, Sam McCandlish, Christopher Olah,\nand Jared Kaplan. 2022. Language models (mostly)\nknow what they know. ArXiv, abs/2207.05221.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. Neural\nInformation Processing Systems (NeurIPS).\nAnoop Korattikara Balan, Vivek Rathod, Kevin P Mur-\nphy, and Max Welling. 2015. Bayesian dark knowl-\nedge. Advances in neural information processing\nsystems, 28.\nAndrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y .\nChan, Kory Matthewson, Michael Henry Tessler, An-\ntonia Creswell, James L. McClelland, Jane X. Wang,\nand Felix Hill. 2022. Can language models learn\nfrom explanations in context?\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Venkatesh\nRamasesh, Ambrose Slone, Cem Anil, Imanol\nSchlag, Theo Gutman-Solo, Yuhuai Wu, Behnam\nNeyshabur, Guy Gur-Ari, and Vedant Misra. 2022.\nSolving quantitative reasoning problems with lan-\nguage models. ArXiv, abs/2206.14858.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\nJian-Guang Lou, and Weizhu Chen. 2022a. On the\nadvance of making language models better reasoners.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\nJian-Guang Lou, and Weizhu Chen. 2022b. On the\nadvance of making language models better reasoners.\nArXiv, abs/2206.02336.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Har-\nrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl\nCobbe. 2023. Let’s verify step by step. ArXiv,\nabs/2305.20050.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017a. Program induction by rationale gener-\nation: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers).\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017b. Program induction by rationale gener-\nation: Learning to solve and explain algebraic word\nproblems. In ACL.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. ArXiv,\nabs/2212.08410.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language models:\nTowards zero-shot language understanding. ArXiv,\nabs/2202.04538.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In EMNLP.\nSharan Narang, Colin Raffel, Katherine Lee, Adam\nRoberts, Noah Fiedel, and Karishma Malkan. 2020.\nWt5?! training text-to-text models to explain their\npredictions.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2019. Adversarial\nnli: A new benchmark for natural language under-\nstanding. ArXiv, abs/1910.14599.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nArkil Patel, S. Bhattamishra, and Navin Goyal. 2021.\nAre nlp models really able to solve simple math word\nproblems? In NAACL.\nDanish Pruthi, Rachit Bansal, Bhuwan Dhingra,\nLivio Baldini Soares, Michael Collins, Zachary C.\nLipton, Graham Neubig, and William W. Cohen.\n2022. Evaluating Explanations: How Much Do Ex-\nplanations from the Teacher Aid Students? Transac-\ntions of the Association for Computational Linguis-\ntics, 10:359–375.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\nAruni RoyChowdhury, Prithvijit Chakrabarty, Ashish\nSingh, SouYoung Jin, Huaizu Jiang, Liangliang Cao,\nand Erik G. Learned-Miller. 2019. Automatic adap-\ntation of object detectors to new domains using self-\ntraining. In CVPR, pages 780–790.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2022. Multitask prompted training enables\nzero-shot task generalization. In ICLR.\nTimo Schick and Hinrich Schütze. 2020a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Conference of the Eu-\nropean Chapter of the Association for Computational\nLinguistics.\n1061\nTimo Schick and Hinrich Schütze. 2020b. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. ArXiv, abs/2009.07118.\nCharlie Snell, Dan Klein, and Ruiqi Zhong. 2022.\nLearning by distilling context. arXiv preprint\narXiv:2209.15189.\nYi Tay, Mostafa Dehghani, Vinh Quang Tran, Xavier\nGarcía, Jason Wei, Xuezhi Wang, Hyung Won Chung,\nDara Bahri, Tal Schuster, Huaixiu Zheng, Denny\nZhou, Neil Houlsby, and Donald Metzler. 2022. Ul2:\nUnifying language learning paradigms.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. ArXiv, abs/1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Black-\nboxNLP@EMNLP.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2022a.\nTowards understanding chain-of-thought prompting:\nAn empirical study of what matters. In Annual Meet-\ning of the Association for Computational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, and Denny Zhou. 2022b. Rationale-\naugmented ensembles in language models. ArXiv,\nabs/2207.00747.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022c. Self-consistency\nimproves chain of thought reasoning in language\nmodels. ArXiv, abs/2203.11171.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\n2022a. Emergent abilities of large language models.\nArXiv, abs/2206.07682.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022b. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Brian Ichter, Fei Xia, Quoc Le, and\nDenny Zhou. 2022c. Chain of thought prompting\nelicits reasoning in large language models. Advances\nin Neural Information Processing Systems, 35.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL.\nZhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng,\nSongyang Gao, Tao Gui, Qi Zhang, and Xuanjing\nHuang. 2023. Self-polish: Enhance reasoning in\nlarge language models via problem refinement.\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and\nQuoc V . Le. 2020. Self-training with noisy student\nimproves imagenet classification. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 10684–10695.\nXi Ye and Greg Durrett. 2022. The unreliability of\nexplanations in few-shot in-context learning.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-\nWoo Lee, and Woomyeong Park. 2021. Gpt3mix:\nLeveraging large-scale language models for text aug-\nmentation. In EMNLP Findings.\nOmar Zaidan, Jason Eisner, and Christine Piatko. 2007.\nUsing “annotator rationales” to improve machine\nlearning for text categorization. NAACL.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\nGoodman. 2022. Star: Bootstrapping reasoning with\nreasoning.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alexan-\nder J. Smola. 2022. Automatic chain of thought\nprompting in large language models. ArXiv,\nabs/2210.03493.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo\nLi, and Yu Li. 2023. Progressive-hint prompting\nimproves reasoning in large language models. ArXiv,\nabs/2304.09797.\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-\nto-most prompting enables complex reasoning in\nlarge language models. ArXiv, abs/2205.10625.\n1062\nA Appendix\nA.1 Results on UL2 model\nWe also apply LMSI on a recently proposed public language model, UL2 (Tay et al., 2022), using the\npre-trained model at step 2,650,000 3. We use a fixed set of hyperparameters for fine-tuning on each\ndataset. Specifically, we generate m = 40reasoning paths for each question in a training set for majority\nvoting. We fine-tune the model for 10k steps with a learning rate of 5e−5 and a batch size of 32. For\nmultiple path decoding, we use a sampling temperature of T = 0.5 with the pre-trained UL2 model\nfollowing Tay et al. (2022), and set T = 0.7 for the language model after LMSI . We set the maximum\nnumber of decode steps to 256 for all experiments.\nThe results are shown in Table 8. For arithmetic reasoning datasets, we follow (Tay et al., 2022) to\nprovide both exact matching accuracy scores as well as accuracy scores after an equation-correction\npostprocessing step. We observe that for most datasets, LMSI still improves the reasoning accuracy\n(+1.6% on DROP, +1.2% on OpenBookQA, and +0.7% on ANLI-A2), but the improvement on UL2 is\nnot as large as that on 540B. We think the reason is that, since LMSI exploits the implicit rationale of\nlanguage models, and the capacity of a language model is determined by its size, larger models can capture\nmore high-order semantics and are more likely to benefit from LMSI . For example, on the adversarial\nentailment tasks of ANLI (which is a three-class classification problem with labels “yes”, “no”, or “it\nis not possible to tell”), the UL2 model w/o LMSI only achieves an accuracy of marginally above1/3,\nimplying that the model is slightly better than doing random guess on this challenging task without any\ntraining. Our proposed LMSI can still improve the performance under this hard case by training on its\nimplicit knowledge from self-generated paths.\nTable 8: Accuracy results on six reasoning benchmarks with LMSI on UL2. On GSM8K and DROP, we also\ninclude accuracy scores after an equation-correction postprocessing step.\nPrompting Method GSM8K DROP ARC -c OpenBookQA ANLI-A2 ANLI-A3\nw/oLMSI CoT-Prompting 5.4/7.1 11.1/16.8 49.9 53.6 35.9 33.8\nSelf-Consistency 6.4/9.9 16.8/26.5 54.7 54.0 37.4 36.8\nLMSI CoT-Prompting 6.1/8.6 11.4/17.1 50.9 53.8 35.4 34.4\nSelf-Consistency 7.9/10.2 18.1/28.1 54.9 55.2 38.1 37.4\nA.2 Chain-of-Thought Prompts for Each Dataset\nWe list the Chain-of-Thought Prompts for each dataset for “CoT-Prompting” experiments and self-\ngenerated training samples.\n3UL2: https://github.com/google-research/google-research/tree/master/ul2\n1063\nTable 9: Few-shot CoT prompts for GSM8K and SV AMP, from (Wei et al., 2022c).\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\nthere will be 21 trees. How many trees did the grove workers plant today?\nA: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\nSo, they must have planted 21 - 15 = 6 trees. The answer is 6.\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\nchocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\ndid Jason give to Denny?\nA: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\nlollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\nhave now?\nA: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\nin total he has 7 + 2 = 9 toys. The answer is 9.\nQ: There were nine computers in the server room. Five more computers were installed each day, from\nmonday to thursday. How many computers are now in the server room?\nA: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\nThe answer is 29.\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\ngolf balls did he have at the end of wednesday?\nA: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\nWednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nA: She bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. She had $23 in\nbeginning, so now she has $23 - $15 = $8. The answer is 8.\nTable 10: Few-shot CoT prompts for OpenBookQA, from (Wang et al., 2022b).\nQ: Poison causes harm to which of the following? (a) a Tree (b) a robot (c) a house (d) a car\nA: Poison will harm living things, only a tree is a living thing. The answer is (a).\nQ: As you look deeper into a Marbel you can see (a) the future (b) minut defects (c) colors (d) the other side\nA: Marbel is not transparent, so you can not see the other side. Marbel does not necessarily have multiple colors.\nYou will see minut defects. The answer is (b).\nQ: When food is reduced in the stomach (a) the mind needs time to digest (b) take a second to digest what I said\n(c) nutrients are being deconstructed (d) reader’s digest is a body of works\nA: The food is being deconstructed in the stomach during digestion. The answer is (c).\nQ: The sun is responsible for (a) puppies learning new tricks (b) children growing up and getting old (c) flowers\nwilting in a vase (d) plants sprouting, blooming and wilting\nA: The sun can affect the growing of living things, like plants. The answer is (d).\n1064\nTable 11: Few-shot CoT prompts for DROP (nonfootball), from (Zhou et al., 2022).\nQ: Since the 1970s, U.S. governments have negotiated managed-trade agreements, such as the North\nAmerican Free Trade Agreement in the 1990s, the Dominican Republic-Central America Free Trade\nAgreement in 2006, and a number of bilateral agreements. In Europe, six countries formed the European\nCoal and Steel Community in 1951 which became the European Economic Community in 1958. Two core\nobjectives of the EEC were the development of a common market, subsequently renamed the single market,\nand establishing a customs union between its member states. How many years did the European Coal and\nSteel Community exist?\nA: According to the passage, the European Coal and Steel Community was established in 1951 and became\nthe EEC in 1958. 1958 - 1951 = 7. So the answer is 7.\nQ: In the county, the population was spread out with 23.50% under the age of 18, 8.70% from 18 to 24,\n29.70% from 25 to 44, 24.70% from 45 to 64, and 13.30% who were 65 years of age or older. How many\nmore percent are under the age of 18 compared to the 18 to 24 group?\nA: According to the passage, 23.5% are under the age of 18, and 8.7% are from ages 18 to 24. 23.5% - 8.7%\n= 14.8%. So the answer is 14.8.\nQ: Playing in their second straight Thanksgiving game, the Eagles struggled especially on defense, where\nthey were unable to stop the much-hyped Lions offense. The worst of it all was how unproven rookie Eric\nRowe was tasked with covering wide receiver Calvin Johnson, leading to Johnson catching 3 touchdowns.\nStafford’s five passing touchdowns, including three of them to Johnson was too much for the Eagles to\novercome and for the second consecutive time this season, the Eagles gave up 45 points in a game. With the\nloss, the Eagles drop to 4-7 on the season and 6-1 when playing on Thanksgiving. How many TD passes did\nStafford throw other than to Johnson?\nA: According to the passage, Stafford threw 5 TD passes, 3 of which were to Johnson. 5 - 3 = 2. So the\nanswer is 2.\nTable 12: Few-shot CoT prompts for DROP (football), from (Zhou et al., 2022).\nQ: The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a\n27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter,\nthe 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking\na 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD. How many yards do the\nshortest touchdown run and the longest touchdown pass combine for?\nA: All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown\nrun. The smallest number among 27, 9, 11 is 9. So the shortest touchdown run was 9 yards. All the\ntouchdown passes are: a 12-yard touchdown pass. So the longest touchdown pass was 12 yards. So the\nshortest touchdown run and the longest touchdown pass combine for 9 + 12 = 21 yards. So the answer is 21\nyards.\nQ: The Steelers went home for a duel with the Baltimore Ravens. Pittsburgh would deliver the opening\npunch in the first quarter with a 1-yard touchdown from running back Rashard Mendenhall. The Ravens\nwould make it even as running back Willis McGahee got a 9-yard TD. The Ravens kicker Billy Cundiff got\na 45-yard field goal in the second quarter, concluding the first half with a 10-7 lead. The Steelers brought\nthe game into overtime with a 38-yard field goal by Andrew Foster. The Ravens Billy Cundiff pulled off a\nwinning 33-yard field goal in overtime. How many points did the Ravens have at halftime?\nA: The Ravens kicker Billy Cundiff got a 45-yard field goal in the second quarter, concluding the first half\nwith a 10-7 lead. So the Ravens had 10 points at halftime. So the answer is 10 points.\nQ: The Vikings flew to Bank of America Stadium to face the Carolina Panthers. After a scoreless first\nquarter, Carolina got on the board with quarterback Matt Moore finding fullback Brad Hoover on a 1-yard\nTD pass. After yet another scoreless quarter, Carolina sealed the game as Matt Moore completed a 42-yard\ntouchdown pass to wide receiver Steve Smith. How many scoreless quarters were there?\nA: The first and third quarters were the scoreless quarters. So there are 2 scoreless quarters. So the answer is\n2.\n1065\nTable 13: Few-shot CoT prompts for NLI tasks, including ANLI and MNLI, from (Wang et al., 2022b).\nPremise:\n\"Conceptually cream skimming has two basic dimensions - product and geography.\"\nBased on this premise, can we conclude the hypothesis \"Product and geography are what make cream skimming\nwork.\" is true?\nOPTIONS:\n- yes\n- no\n- it is not possible to tell\nA: Based on \"cream skimming has two basic dimensions\" we can’t infer that these two dimensions are what\nmake cream skimming work. The answer is it is not possible to tell.\nPremise:\n\"One of our member will carry out your instructions minutely.\"\nBased on this premise, can we conclude the hypothesis \"A member of my team will execute your orders with\nimmense precision.\" is true?\nOPTIONS:\n- yes\n- no\n- it is not possible to tell\nA: \"one of\" means the same as \"a member of\", \"carry out\" means the same as \"execute\", and \"minutely\" means\nthe same as \"immense precision\". The answer is yes.\nPremise:\n\"Fun for adults and children.\"\nBased on this premise, can we conclude the hypothesis \"Fun for only children.\" is true?\nOPTIONS:\n- yes\n- no\n- it is not possible to tell\nA: \"adults and children\" contradicts \"only children\". The answer is no.\nPremise:\n\"He turned and smiled at Vrenna.\"\nBased on this premise, can we conclude the hypothesis \"He smiled at Vrenna who was walking slowly behind\nhim with her mother.\" is true?\nOPTIONS:\n- yes\n- no\n- it is not possible to tell\nA: the premise does not say anything about \"Vrenna was walking\". The answer is it is not possible to tell.\nPremise:\n\"well you see that on television also\"\nBased on this premise, can we conclude the hypothesis \"You can see that on television, as well.\" is true?\nOPTIONS:\n- yes\n- no\n- it is not possible to tell\nA: \"also\" and \"as well\" mean the same thing. The answer is yes.\nPremise:\n\"Vrenna and I both fought him and he nearly took us.\"\nBased on this premise, can we conclude the hypothesis \"Neither Vrenna nor myself have ever fought him.\" is\ntrue?\nOPTIONS:\n- yes\n- no\n- it is not possible to tell\nA: \"Vrenna and I both\" contradicts \"neither Vrenna nor myself\". The answer is no.\n1066\nTable 14: Few-shot CoT prompts for ARC-challenge, from (Wang et al., 2022c).\nQ: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most\nheat? (a) dry palms. (b) wet palms. (c) palms covered with oil. (d) palms covered with lotion.\nA: Dry surfaces will more likely cause more friction via rubbing than other smoother surfaces, hence dry\npalms will produce the most heat. The answer is (a).\nQ: Which factor will most likely cause a person to develop a fever? (a) a leg muscle relaxing after exercise.\n(b) a bacterial population in the bloodstream. (c) several viral particles on the skin. (d) carbohydrates being\ndigested in the stomach.\nA: Option (b), bacterial population is the most likely cause for a person developing fever. The answer is (b).\nQ: Which change in the state of water particles causes the particles to become arranged in a fixed position?\n(a) boiling. (b) melting. (c) freezing. (d) evaporating.\nA: When water is freezed, the particles are arranged in a fixed position; the particles are still moving for all\nother options. The answer is (c).\nQ: When a switch is used in an electrical circuit, the switch can (a) cause the charge to build. (b) increase\nand decrease the voltage. (c) cause the current to change direction. (d) stop and start the flow of current.\nA: The function of a switch is to start and stop the flow of a current. The answer is (d).\nTable 15: Few-shot CoT prompts for AQUA, from (Wang et al., 2022c).\nQ: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the\nnumbers is? Answer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\nA: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean\nwould be 50. The answer is (a).\nQ: If a / b = 3/4 and 8a + 5b = 22,then find the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e)\n7/2\nA: If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simplifies to 8a + 20a / 3 = 22, which means\n44a / 3 = 22. So a is equal to 3/2. The answer is (b).\nQ: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then find the distance? Answer Choices:\n(a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km\nA: The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. The answer is (e).\nQ: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (a) 1156 (b) 1392\n(c) 1480 (d) 1562 (e) 1788\nA: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401\nthree-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b).\n1067\nTable 16: Few-shot CoT prompts for StrategyQA, from (Wei et al., 2022c).\nQ: Do hamsters provide food for any animals?\nA: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals.\nThe answer is yes.\nQ: Could Brooke Shields succeed at University of Pennsylvania?\nA: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as\nthe University of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania.\nThe answer is yes.\nQ: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\nA: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic\nnumber squared is less than 5. The answer is no.\nQ: Yes or no: Is it common to see frost during some college commencements?\nA: College commencement ceremonies can happen in December, May, and June. December is in the winter,\nso there can be frost. Thus, there could be frost at some commencements. The answer is yes.\nQ: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\nA: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6\nmonths. Thus, a llama could not give birth twice during the War in Vietnam. The answer is no.\nQ: Yes or no: Would a pear sink in water?\nA: The density of a pear is about 0.6 g/cm3, which is less than water. Objects less dense than water float.\nThus, a pear would float. The answer is no.\nTable 17: Few-shot CoT prompts for RTE, from (Wang et al., 2022b).\nPremise:\n\"No Weapons of Mass Destruction Found in Iraq Yet.\"\nBased on this premise, can we conclude the hypothesis \"Weapons of Mass Destruction Found in Iraq.\" is true?\nA: \"No Weapons of Mass Destruction Found\" contradicts \"Weapons of Mass Destruction Found\". The answer is\nno.\nPremise:\n\"A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful\ngathered in downtown Chicago to mark the installation of new Pope Benedict XVI.\"\nBased on this premise, can we conclude the hypothesis \"Pope Benedict XVI is the new leader of the Roman\nCatholic Church.\" is true?’\nA: \"installation of new Pope Benedict XVI.\" means \"Pope Benedict XVI is the new leader\". The answer is yes.\nPremise:\n\"A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to\nbe featured on BBC One’s Crimewatch. Colette Aram, 16, was walking to her boyfriend’s house in Keyworth,\nNottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her\nhome. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates\nlater.\"\nBased on this premise, can we conclude the hypothesis \"Paul Stewart Hutchinson is accused of having stabbed a\ngirl.\" is true?\nA: The premise does not say Paul Stewart Hutchinson \"stabbed\" this girl. The answer is no.\nPremise:\n\"Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it\nwill discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.\"\nBased on this premise, can we conclude the hypothesis \"Herceptin can be used to treat breast cancer.\" is true?\nA: \"Herceptin was approved to treat breast cancer\" implies that \"Herceptin can be used to treat breast cancer\".\nThe answer is yes.\n1068"
}