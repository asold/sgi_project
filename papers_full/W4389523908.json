{
  "title": "Can Large Language Models Capture Dissenting Human Voices?",
  "url": "https://openalex.org/W4389523908",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105571861",
      "name": "Noah Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975019503",
      "name": "Na An",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096395986",
      "name": "James Thorne",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2186512589",
    "https://openalex.org/W2254249950",
    "https://openalex.org/W4385573072",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2740168486",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4386566656",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W3098467034",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W4312107542",
    "https://openalex.org/W4287252513",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2985347336",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4285228387",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4385570032",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3163078977",
    "https://openalex.org/W4385573849",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4378499145",
    "https://openalex.org/W4385571697",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W3156031972",
    "https://openalex.org/W4313459382",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W4255825300",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4200444669",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3166716308",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W3174102190",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W4322717046",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3100292568",
    "https://openalex.org/W4281748205",
    "https://openalex.org/W4379051439",
    "https://openalex.org/W4381573334",
    "https://openalex.org/W4389524313",
    "https://openalex.org/W4379958381",
    "https://openalex.org/W3166002735",
    "https://openalex.org/W4304697829",
    "https://openalex.org/W3196841052",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2132481658",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4321594373",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1965555277"
  ],
  "abstract": "Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques to estimate the multinomial distribution: Monte Carlo Estimation (MCE) and Log Probability Estimation (LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4569–4585\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCan Large Language Models Capture Dissenting Human Voices?\nNoah Lee∗\nKAIST AI\nnoah.lee@kaist.ac.kr\nNa Min An∗\nKAIST AI\nnaminan@kaist.ac.kr\nJames Thorne\nKAIST AI\nthorne@kaist.ac.kr\nAbstract\nLarge language models (LLMs) have shown im-\npressive achievements in solving a broad range\nof tasks. Augmented by instruction fine-tuning,\nLLMs have also been shown to generalize in\nzero-shot settings as well. However, whether\nLLMs closely align with the human disagree-\nment distribution has not been well-studied, es-\npecially within the scope of natural language\ninference (NLI). In this paper, we evaluate\nthe performance and alignment of LLM dis-\ntribution with humans using two different tech-\nniques to estimate the multinomial distribution:\nMonte Carlo Estimation (MCE) and Log Prob-\nability Estimation (LPE). As a result, we show\nLLMs exhibit limited ability in solving NLI\ntasks and simultaneously fail to capture human\ndisagreement distribution. The inference and\nhuman alignment performances plunge even\nfurther on data samples with high human dis-\nagreement levels, raising concerns about their\nnatural language understanding (NLU) ability\nand their representativeness to a larger human\npopulation.1\n1 Introduction\nNatural language inference (NLI) has long served\nas a fundamental testbed to evaluate the ability of a\nmodel to recognize entailment and capture plausi-\nble inference relations between pairs of sentences\n(Dagan et al., 2006; Bowman et al., 2015; Williams\net al., 2018). When constructing datasets, conven-\ntional processes result in a single label per instance\neven if multiple annotators contribute, which lim-\nits the full representation of diverse opinions that\nmight arise in a larger human population. Thus,\nrecent datasets have become more attentive to in-\ncorporating multiple interpretations (Pavlick and\nKwiatkowski, 2019; Nie et al., 2020b; Glockner\net al., 2023) to capture dissenting human opinions.\n∗Equal contribution\n1The source code for the experiments is available at\nhttps://github.com/xfactlab/emnlp2023-LLM-Disagreement.\nO\nRead the following and determine if the hypothesis can be inferred from the premise.\nPremise: She smiled back.\nHypothesis: She was so happy she couldn’t stop smiling.\nOPTIONS: entailment, contradiction, neutral    \nAnswer:\n1) Generation\n2) Estimation\n3) Comparison\nLPE\nMCEn\ncontradictoryent21. entailmentneutralneut\\n\n\\t\nnenenc\nk213neut\\n\nvs.nec\necnn\nLLM\nnec\nHuman\nFigure 1: The Proposed LLM Distribution Estimation\nTechniques, MCE and LPE. We estimate LLM disagree-\nment with either MCE or LPE utilizing generated LLM\noutputs and compare the estimated LLM distribution\nwith human disagreement distribution.\nMeanwhile, instruction fine-tuning large lan-\nguage models (LLMs) has elicited remarkable gen-\neralizability to diverse unseen tasks (Zhao et al.,\n2023). Not only can they generate free-form texts,\nbut they can also select one answer from multiple\noptions given in the input prompt. However, while\nmany works study user interaction and conversa-\ntional usage (Liang et al., 2022), limited works\nevaluate these instruction-following LLMs on a\nfoundational NLI task. Therefore, we aim to an-\nswer the following questions: Can LLMs capture\ndissenting voices that naturally arise in the dataset?\nAre LLMs representative of the voices of the anno-\ntators in inference tasks?\n4569\nWith this in mind, we jointly assess on a number\nof instruction-following LLMs, Flan-T5 (Chung\net al., 2022), Flan-UL2 (Tay et al., 2022), OPT-\nIML-Max (Iyer et al., 2022), and GPT-3 (Ouyang\net al., 2022), on their performance on human opin-\nion distribution datasets - ChaosNLI (Nie et al.,\n2020b) and PK2019 (Pavlick and Kwiatkowski,\n2019). For the process of using the model output\ndistribution as an estimate of human disagreement\ndistribution, we offer novel estimation methods:\nMonte Carlo Estimation (MCE) and Log Probabil-\nity Estimation (LPE) (Figure 1).\nWe find that the state-of-the-art GPT-3 model\ndoes not outperform smaller models such as fine-\ntuned BERT (Devlin et al., 2019) and partially fine-\ntuned Flan-T5-XXL in solving inference problems.\nFurthermore, it yields higher Jensen-Shannon Dis-\ntance (JSD) (Endres and Schindelin, 2003) and\nDistribution Calibration Error (DCE) (Baan et al.,\n2022) than BERT for the ChaosNLI datasets. Each\nmodel is optimized using different estimation meth-\nods and prompt types, where GPT/Flan-T5-XXL\nattains the best performances in NLI capability and\nhuman alignment when using LPE/MCE. Our pa-\nper’s contributions are as follows:\n• To the best of our knowledge, we are the first\nto test generative LLMs jointly on the perfor-\nmance and human disagreement on NLI.\n• We suggest two probability distribution esti-\nmation techniques for LLMs to represent dis-\nagreement and perform empirical evaluations\nto with respect to the human disagreement\ndistribution.\n• We study the model sensitivity to estimation\nmethods and prompt types to demonstrate how\nthese contribute to the ability of models to\nrepresent human-level disagreement for NLI.\n2 Related Works\n2.1 Disagreement in NLI\nConsidering only a single label in NLI datasets\nis bound to fail in capturing the diverse range of\nuser opinions and could lead to misrepresentations\nof language models. To measure inherent human\ndisagreements in NLI, Nie et al., 2020b and Pavlick\nand Kwiatkowski, 2019 collected large number of\nhuman annotations (e.g., 100 and 50 annotations for\nChaosNLI and PK2019) per instance for common\nNLI datasets such as SNLI (Bowman et al., 2015)\nand MultiNLI (Williams et al., 2018). When taking\nthe majority vote from these additional annotations,\n22% of the instances exhibited a change in label\ncompared to the original dataset (Nie et al., 2020b).\nTo characterize and reproduce the extent of hu-\nman disagreement in NLI tasks, previous works\ndirectly fine-tuned language models (Nie et al.,\n2020b) and implemented distribution estimation\nmethods (Zhou et al., 2022) using the labeled data.\nOther studies have constructed losses to better cal-\nibrate the ambiguity (Meissner et al., 2021) and\nproposed an ensemble of models to detect disagree-\ning samples (Zhang and de Marneffe, 2021).\nFor measuring the distance between two distri-\nbutions, Kullback–Leibler (KL) Divergence (Kull-\nback and Leibler, 1951) or its symmetric version,\nJensen-Shannon Distance (JSD) (Endres and Schin-\ndelin, 2003) are widely used. Baan et al., 2022\nargued that Expected Calibration Error (ECE), the\ndifference between the average accuracy and confi-\ndence (Naeini et al., 2015; Guo et al., 2017), cannot\ncapture inherent human disagreement. Therefore,\nfor models to better calibrate to human disagree-\nment, accuracy-agnostic metrics such as DCE have\nbeen introduced (Baan et al., 2022).\n2.2 Alignment of Instruction-tuned LLMs\nLLMs have demonstrated the ability to follow ex-\namples provided in-context (Brown et al., 2020)\nand have further been developed to follow natural\nlanguage instructions (Mishra et al., 2022; Ouyang\net al., 2022; Chung et al., 2022). Instruction-\nfollowing LLMs are fine-tuned with various tasks\nand are expected to generalize well to tasks the\nmodel was not trained on (Zhao et al., 2023).\nFor example, GPT-3 is fine-tuned using reinforce-\nment learning with human feedback to produce\nresponses that align with human values (Ouyang\net al., 2022). Despite such efforts, Santurkar et al.,\n2023 identified that LLMs capture only a single\nperspective, exhibiting left-leaning tendencies and\nexcluded demographic groups. Here, we study\nwhether LLMs appropriately reflect a diversity of\nviewpoints in the NLI task setting.\n3 Methods\nWe estimate and quantify dissenting human voices\nusing the multinomial soft-label distribution of\nLLMs with two proposed methods:\n4570\n3.1 Log Probability Estimation (LPE)\nWe use a single instance returning log probabilities\nof top-k2 token candidates to estimate the categor-\nical distribution of the labels. This method sums\nover all valid options3 (vj) to estimate the model\nprobability for class j, a method often adopted in a\nmultiple-choice style evaluation of generative lan-\nguage models (Hendrycks et al., 2021; Santurkar\net al., 2023). Although the LPE method requires\na single generation for each instance, it cannot be\napplied to all types of models4. Additionally, the\nmethod is limited in cases where more than one to-\nken is generated as it requires exhaustive mapping\nof the determining token probability. Furthermore,\nas models only return probabilities for top-k tokens,\nthere is an unknown non-constant probability mass.\nWe estimate this as follows, where C is the total\nnumber of classes of the task:\np( ˆ yj|x) ≈\n∑k\ni=1 exp lpi ·1i∈vj\n∑C\nj=1\n∑k\ni=1 exp lpi ·1i∈vj\n(1)\n3.2 Monte Carlo Estimation (MCE)\nDecoding strategies such as beam search or greedy\nsearch do not exploit the full distribution of the pos-\nsible generation options. Furthermore, API-based\nlanguage model services limit the number of re-\nturned token-level probabilities. Alternatively, to\nreconstruct the distribution of outputs from genera-\ntive LLMs, we introduce an intuitive way that sam-\nples a large number5 of generated outputs consider-\ning the valid options6 (vj) for class j. This method\nis based on a Monte Carlo method (Metropolis and\nUlam, 1949) to estimate the probability distribution.\nEven though the MCE method can be computation-\nally expensive, it can be applied to any model and\nprompt type to capture the multinomial distribu-\ntion of a classification setting. MCE is defined as\nfollows:\np( ˆ yj|x) ≈1\nn\nn∑\ni=1\n1i∈vj (2)\n2k is set to 5 for all the models to match the maximum\nlogprobs size of OpenAI Completion API.\n3See Appendix C for examples.\n4GPT-3.5-Turbo does not supportlogprobs.\n5Sample size of 100 is heuristically chosen to match the\nsize of human annotation for ChaosNLI.\n6See Appendix C for examples.\n4 Experimental Design\n4.1 Data\nFirst, we test the inference ability of LLMs in chal-\nlenging datasets, ANLI (Adversarial NLI) (Nie\net al., 2020a) and QNLI (Wang et al., 2018). We\nopt for the round 3 version of ANLI (n = 1,200),\nwhich contains more contexts from diverse do-\nmains such as Wikipedia, Common Crawl, Sto-\nryCloze (Mostafazadeh et al., 2016), and CBT (Hill\net al., 2016). QNLI (Wang et al., 2018) (n = 5,463)\nis converted from the Stanford Question Answer-\ning Dataset (SQuAD) (Rajpurkar et al., 2016) to an\nNLI dataset, and the task is to decide whether the\nsentence contains the answer to the question.\nSecond, we jointly evaluate LLMs on ChaosNLI\n(Nie et al., 2020b), and PK2019 (Pavlick and\nKwiatkowski, 2019) to examine both the accuracy\nand how the model distribution aligns with the hu-\nman disagreement distribution. These datasets con-\nsist of two task settings: ChaosNLI-α(n = 1,532),\nwhere models must select one of the two hypothe-\nses, and ChaosNLI-S (n = 1,514), M (n = 1,599),\nand a subset 7 of PK2019 (n = 299) where mod-\nels must assign the relationship ( e.g., entailment,\ncontradiction, or neutral) for a pair of premise and\nhypothesis. We also pick out a challenging sub-\nset of the ChaosNLI datasets, which we denote as\nHighChaosNLI, consisting of the top 100 samples\nhaving the greatest human disagreement level.\nLastly, to trace possible causes of the disagree-\nment occurring in LLMs, we use the round 1 ver-\nsion of the DisagreementNLI dataset (n = 318),\nwhere the samples from ChaosNLI are annotated\nwith one of the 10 categories (e.g., probabilistic) of\npotential sources of disagreement (Jiang and Marn-\neffe, 2022). While the primary focus is slanted\ntowards identifying why humans disagree, we uti-\nlize and link the disagreement taxonomy to un-\ncover whether the disagreement in LLMs aligns\nwith those of humans.\n4.2 Models\nWe categorize numerous LLMs with varying levels\nof supervision on the NLI task 8: Full Exposure\n(FE), Partial Exposure (PE), Minimum/Unknown\nExposure (MUE), and No Exposure (NE). For FE\nmodels, we follow the baseline setup of Nie et al.,\n2020b by fine-tuning BERT (340M) (Devlin et al.,\n7JOCI & DNC datasets of PK2019 are discarded as the\nannotation setting greatly varies from ChaosNLI.\n8See Appendix B for more details.\n4571\nModel LPE (NS) MCE (NS) MCE (OS)\nAcc↑ JSD↓ DCE↓ Acc↑ JSD↓ DCE↓ Acc↑ JSD↓ DCE↓\nFlan-T5-L (780M) 59.3 0.293 0.326 62.3 0.289 0.321 59.7 0.290 0.322\nFlan-T5-XL (3B) 65.7 0.253 0.282 72.0 0.236 0.254 70.3 0.238 0.256\nFlan-T5-XXL (11B) 68.7 0.258 0.277 71.0 0.263 0.277 74.3 0.232 0.244\nFlan-UL2 (20B) 67.7 0.260 0.281 72.3 0.247 0.259 76.0 0.241 0.246\nOPT-IML-M-S (1.3B) 57.0 0.294 0.337 54.7 0.312 0.354 59.3 0.298 0.337\nOPT-IML-M-L (30B) 72.0 0.273 0.286 62.0 0.280 0.303 72.7 0.233 0.252\nGPT-3-D3 (175B) 66.7 0.330 0.345 67.0 0.334 0.349 58.0 0.344 0.376\nGPT-3-D2 (175B) 64.0 0.282 0.317 62.7 0.279 0.313 49.3 0.315 0.364\nStable Vicuna(13B) 45.7 0.328 0.379 43.7 0.504 0.568 41.7 0.502 0.567\nTable 1: Human Alignment Performances of LLMs on Subsets of ChaosNLI Datasets with Different Estimation\nMethods - LPE/MCE (Prompt Types - Shuffled NS/OS). We present the average results of ChaosNLI-α, S, and M;\nfor each dataset, we randomly sample 100 instances. The model categorizations are the same as Table 2. Bold texts\nindicate the best value for each model and metric.\nNS\nOPTIONS: hypothesis 1, hypothesis 2\nOPTIONS: entailment,  contradiction, neutral \nOPTIONS: 1: hypothesis 1, 2: hypothesis 2\nOPTIONS: 1: entailment,  2: contradiction, 3: neutral OS\nFigure 2: Two Prompt Types - OS and NS for Two\nTypes of Tasks. The former does not have a number in\nfront of each option choice.\n2019) and RoBERTa (355M) (Liu et al., 2019).\nSince instruction-following LLMs do not have full\nsupervision of NLI, we assign these LLMs to one\nof the PE, MUE, and NE models.\nFirst, the PE models include Flan-T5 (780M,\n3B, 11B) (Chung et al., 2022), Flan-UL2 (20B)\n(Tay et al., 2022), and OPT-IML-Max (1.3B\nand 30B) 9 (Iyer et al., 2022). We label\nGPT-3-D2 (text-davinci-002) and GPT-3-D3\n(text-davinci-003) (175B) (Ouyang et al., 2022)\nas MUE models because, although the models are\nvariants from Ouyang et al., 2022, it is unknown\nto which extent the model is exposed to NLI tasks.\nFinally, the sole NE model that we test is Stable\nVicuna (13B) (Chiang et al., 2023)10 because it has\nno exposure to NLI tasks. All the hyperparameters\nwe use to generate the outputs of these models are\nlisted in Appendix A.\n9These models will be referred as OPT-IML-M-S and OPT-\nIML-M-L for convenience.\n10Results of poor-performing models such as Stanford Al-\npaca (7B) and Dolly-v2 (12B) are not reported.\n4.3 Prompt Types\nWe adopt mostly the same prompt template 11\nacross different types of models within each sub-\ndataset. Within a dataset and a model, we test two\ntypes of prompts: (1) Option Selection (OS), in\nwhich the model has to predict the name of the\nclass label for the entailment relation, and (2) Num-\nber Selection12 (NS), in which the model has to\nselect the number assigned to the relationship class\n(Figure 2). Additionally, as LLMs are known to be\nsensitive to minor input modifications (Liang et al.,\n2022; Sun et al., 2023), we test the effect of prompt\nvariations over a single prompt.\nNS requires the model to predict a single token\nof a target number and can be used with both MCE\nand LPE. OS, on the other hand, is not considered\nin the LPE formulation to encourage a scalable,\ncomprehensive generation strategy to estimate hu-\nman disagreement distribution since if we allow\nLPE-OS, the token-specific probability of a model\noutput which may vary by instance/dataset/task has\nto be mapped per class. We implement random\nordering of the options in the prompt, as also men-\ntioned in Santurkar et al., 2023, to mitigate the\nsensitivity due to the order of the options, which\nwe call shuffled OS and NS throughout the paper.\n4.4 Metrics\nWe investigate the distribution differences between\nhumans and LLMs at the sample level with JSD,\nwhich is a symmetric version of KL divergence\n11Refer to Appendix F for specific prompt examples.\n12A multiple-choice format similar to the prompt suggested\nin the MMLU Benchmark (Hendrycks et al., 2021)\n4572\nModel ANLI-R3 QNLI ChaosNLI-α ChaosNLI-S ChaosNLI-M PK2019\nChance 33.3 50.0 50.0 33.3 33.3 33.3\nFull Exposure (FE)\nBERT-L∗(340M) 43.5 92.7 68.2 (+0.2) 73.8 (+1.2) 56.9 (-4.3) -\nRoBERTa-L∗(355M) 44.4 98.9 83.7 (+1.6) 78.7 (+3.8) 63.5 (-3.9) -\nPartial Exposure (PE)\nFlan-T5-L(780M) 46.3 90.2 73.1 (+1.9) 54.8 (-4.6) 59.7 (+7.8) 76.6 (+6.5)\nFlan-T5-XL(3B) 54.3 93.1 83.3(+1.2) 71.2 (+1.1) 60.2 (+1.0) 76.9 (-7.4)\nFlan-T5-XXL(11B) 58.2 93.7 84.9(+1.6) 67.9 (+0.8) 72.6(+8.5) 82.1(-0.6)\nFlan-UL2(20B) 56.6 94.9 86.5(+1.8) 79.9(+6.4) 71.7 (+4.8) 74.6 (-14.3)\nOPT-IML-M-S(1.3B) 34.6 80.6 53.6 (-0.7) 66.1(+7.2) 50.3 (+1.2) 57.5 (-3.1)\nOPT-IML-M-L(30B) 38.5 70.4 72.7 (-1.8) 77.1 (+7.3) 65.4 (-3.5) 68.3 (-14.5)\nMinimal/Unknown Exposure (MUE)\nGPT-3-D3(175B) 47.8 79.0 76.5 (+2.3) 62.7 (+5.6) 63.3 (+9.1) 69.5 (-0.2)\nGPT-3-D2(175B) 44.8 77.1 72.6 (+1.5) 56.3 (+6.0) 49.9 (-0.7) 45.5 (-10.6)\nNo Exposure (NE)\nStable Vicuna(13B) 33.5 49.5 55.6 (+2.1) 34.2 (-5.6) 45.4 (+8.5) 61.2 (+14.5)\nTable 2: Inference Performances of LLMs on Various Datasets. We use MCE (n = 5 for ANLI-R3/QNLI and n =\n500 for ChaosNLI/PK2019) with shuffled OS. For GPTs and Stable Vicuna, we use LPE with shuffled NS. The\nvalues inside the parentheses indicate the accuracy change from the old to new labels. We report the accuracy results\nof the FE models (∗) from Nie et al., 2020a for ANLI-R3, Devlin et al., 2019 and Liu et al., 2019 for BERT-L and\nRoBERTa in QNLI, and Nie et al., 2020b for ChaosNLI datasets. All the outputs are averaged over three runs, and\nbold and underlined texts indicate the first and the second best value for each column.\n(Endres and Schindelin, 2003). In addition, we\nevaluate human uncertainty with DCE (Baan et al.,\n2022) to examine how the tendencies of these two\nmeasures compare.\nJSD(p||q) =\n√\nKL(p||m) +KL(q||m)\n2\nDCE(p,q) =1\n2||p −q||1\nwhere KL(p||q) =∑\ni pilog(pi\nqi\n), m = p+q\n2\n5 Results\nLLMs are sensitive to different estimation meth-\nods and prompt types.To select the optimal es-\ntimation methods and prompt types for each model,\nwe examine three cases13 - (1) LPE (NS), (2) MCE\n(NS), and (3) MCE (OS) for 100 randomly selected\nexamples in ChaosNLI subsets (Table 1). All the\nPE models perform the best using MCE (OS) or\nMCE (NS). Meanwhile, GPT-3-D3 performs better\nusing LPE (NS) than either MCE method, hinting\nthat larger models (>100B) may not need costly\nmethods to estimate the model distribution. Sim-\nilarly, for GPT-3-D2 and Stable Vicuna, a drastic\n13We exclude LPE (OS) due to the reason outlined in Sec-\ntion 4.3.\nnegative effect of using MCE methods is exhibited,\nespecially when using OS. Hence, we choose MCE\n(OS) for the PE models and LPE (NS) for the MUE\nand NE models.\nThe NLI capability of LLMs does not only\nincrease due to model size. In Table 2, even\nthough GPT-3-D3 has the largest parameters (175\nbillion) and surpasses GPT-3-D2 and Stable Vi-\ncuna, its accuracy is significantly outperformed by\nthe PE models across ANLI-R3, QNLI, ChaosNLI,\nand PK2019 datasets. For ChaosNLI-S espe-\ncially, GPT-3-D3 shows comparably lower perfor-\nmances than any FE and PE models. The lead-\ning PE models are Flan-UL2 and Flan-T5-XXL\nacross most of the tested datasets (Table 2). The\nbest PE model achieves 9 to 15% higher accu-\nracy in ANLI-R3/ChaosNLI-M than the best FE\nmodel (i.e., RoBERTa-L). However, Flan-T5-UL2\nis marginally higher than RoBERTa-L by 1 to 3%\nin ChaosNLI-α/S, and Flan-T5-XXL even achieves\n9.1% higher than RoBERTa-L for ChaosNLI-M.\nWithin the Flan-T5 family, scaling the model leads\nto enhanced inference performances. However, the\nlargest model across all the tested models - GPT-3-\nD3 does not always attain the best accuracy, sug-\ngesting that model size alone is not a critical factor\nfor performance on NLI.\n4573\nJSD (α) DCE (α) JSD (S) DCE (S) JSD (M) DCE (M) JSD (PK) DCE (PK)\n0.1\n0.2\n0.3\n0.4\n0.5\nDistance from Human Distribution ↓\nB∗ RB∗ FL FXL FXXL FUL2 OS OL G3 G2 SV Chance\nFigure 3: Human Alignment Performances of LLMs on ChaosNLI and PK2019 Datasets. The model categorizations\nand estimation methods are the same as Table 2. All the outputs are averaged over three runs. We additionally\nvisualize pairwise model similarity using JSD in Appendix D.\nDoes multiple annotation help? For most of\nthe models making inferences on the re-annotated\ndatasets of ChaosNLI, improvements in NLI ac-\ncuracy are observed, with the exception of OPT-\nIML-M-S. (Refer to the values inside parentheses\nin Table 2). This supports the necessity of hav-\ning increased multiple annotations for tasks that\nhumans are expected to disagree with. Also, it is\nnoticeable how all these models, even if they were\nexposed to a sample of the train set with the orig-\ninal label, show better performances in the newly\nannotated ChaosNLI. However, we detect an accu-\nracy decrease between the old and new labels in\nthe PK2019 dataset for most of the models except\nfor Flan-T5-L and Stable Vicuna. We hypothesize\nthis is due to the way in which the final label was\nselected in Pavlick and Kwiatkowski, 2019: anno-\ntators were asked to select an interval score which\nwas later manually discretized.\nAlignment with human disagreement is not al-\nways better for larger models.To examine how\nclosely the estimated distribution of LLMs aligns\nwith the human disagreement distribution, we com-\npare sample-level measures of JSD and DCE be-\ntween humans and LLMs (Figure 3). Similar to\nthe accuracy results (Table 2), GPT-3-D3 fails to\nalign with the human label distribution compared to\nsome well-performing PE models, such as Flan-T5-\nXXL and Flan-T5-UL2. Also, each model displays\na similar tendency between JSD and DCE, suggest-\ning that either one of the metrics might be enough\nto measure human alignment.\nAs can be observed in Figure 3, none of the\nLLMs show less JSD/DCE values than RoBERTa-\nL in ChaosNLI-α/S. Within LLMs, there is no one\nleading model that performs well across all datasets.\nFor example, while Flan-UL2 scores the lowest\nJSD/DCE value in the ChaosNLI-αdataset, OPT-\nIML-M-L shows the lowest distance from human\ndistribution in the ChaosNLI-M dataset. It is impor-\ntant to note that GPT-3-D3 shows worse JSD/DCE\nthan RoBERTa-L for all ChaosNLI datasets, and\nit even performs worse than Stable Vicuna in\nChaosNLI-M. Intriguingly, the Flan-T5 family\nbenefits from scaling model size in ChaosNLI\ndatasets, but Flan-T5-large does not show the high-\nest JSD/DCE in PK2019 datasets.\nEffect of Human Entropy on LLM Disagree-\nment We filter out a challenging subset, High-\nChaosNLI, which is the top 100 selected samples\nwith the highest human disagreement levels based\non the entropy of each instance. We observe a\nplunge in accuracy as well as a rise in JSD/DCE\nfor every model (Table 3) compared to the human\nalignment performances for full datasets in Table 2.\nStill, the leading model concerning inference abil-\nity (i.e., Flan-T5-XXL) is unchanged, obtaining the\nhighest accuracy of 52% in HighChaosNLI. On the\nother hand, it is notable how Stable Vicuna displays\nthe lowest JSD/DCE compared to the other models\n(Table 3). Nevertheless, with the hint of the worst\naccuracy out of all the models for full ChaosNLI\ndatasets (Figure 3) and high entropy levels (Fig-\nure 4), we conclude that it is a mere coincidence\nthat Stable Vicuna exhibits the best performance in\nterms of human alignment performances in High-\nChaosNLI dataset (Table 3).\nWe further attempt to investigate the possible\n4574\nFigure 4: Histogram of Human and LLM Entropy Levels for ChaosNLI Datasets. The distributions of Flan-T5-XXL\nand GPT-3-D3/Stable Vicuna are estimated using MCE (OS) and LPE (NS), respectively, same as Table 2.\nHighChaosNLIModel Acc↑ JSD↓ DCE↓\nFlan-T5-L(780M) 44.0 0.256 0.318\nFlan-T5-XL(3B) 48.0 0.268 0.336\nFlan-T5-XXL(11B) 52.0 0.300 0.362\nFlan-UL2(20B) 50.3 0.321 0.378\nOPT-IML-M-S(1.3B) 51.0 0.254 0.293\nOPT-IML-M-L(30B) 50.7 0.266 0.312\nGPT-3-D3(175B) 50.0 0.435 0.494\nGPT-3-D2(175B) 45.7 0.310 0.354\nStable Vicuna(13B) 42.7 0.189 0.240\nTable 3: Inference and Human Alignment Performances\nof LLMs on HighChaosNLI. The model categorizations\nand estimation methods are the same as Table 2. All the\noutputs are averaged over three runs.\ncauses of this phenomenon by spanning out the\nentropy distribution. On the consistent finding that\nGPT-3-D3 performs worse than Flan-T5-XXL in\nsolving NLI tasks (Table 2) and capturing human\ndisagreement levels (Figure 3), even in the High-\nChaosNLI dataset (Table 3), as can be observed\nin Figure 4, GPT tends to be more overconfident,\nshowing a entropy of less than 0.1 in most samples.\nIn contrast, the human entropy is mostly evenly\ndistributed in the range of 0.4 to 0.6 for ChaosNLI-\nαand 0.8 to 1.0 for ChaosNLI-S/M. On the other\nhand, Flan-T5-XXL exhibits lower confidence than\nGPT-3-D3 but higher confidence than humans, and\nStable Vicuna is uncertain in most instances.\nEffect of Varying PromptsTo observe the effect\nof prompt sensitivity on varying prompt templates,\nwe craft variations of the pre-selected prompt. For\nSNLI and MNLI, we sample out five prompt vari-\nants from the Flan repository 14 and make sensi-\nble variants for ChaosNLI- α as it is not part of\nthe Flan mixture. From Table 4, it is shown that\nthe prompt variation generally benefits Flan mod-\nels in the ChaosNLI-S/M datasets as they were\nexposed to the prompt templates. However, the\npre-selected single prompt is beneficial in perfor-\nmance for the ChaosNLI-αdataset for Flan models\nand all datasets in GPT-3-D3 and Stable Vicuna.\nThe performance drop using prompt variation is\neven more severe for GPT-3-D3, suggesting the\npreferred usage of a carefully crafted single prompt\nover using unseen input templates. However, this\ndoes not mean that the single prompt should always\nbe preferred since variations of prompts may dis-\nplay fairer performance trends of diverse models\nwithin the ground of robustness.\nWhat causes LLMs to disagree?Sources of hu-\nman disagreement have been well studied, but there\nis a lack of study of the disagreement sources for\nLLMs. We try to find the causes of LLM disagree-\nments by drawing a relationship between LLM en-\ntropy level and human disagreement sources (dis-\ncussed in Jiang and Marneffe, 2022) for each sam-\nple (Figure 5). However, no visible correlation\nof LLM entropy on human entropy is displayed\nacross identified sources of human disagreement.\nThis suggests that the cause of LLM disagreements\nmay be due to factors other than human entropy\nand disagreement sources. Thus, Under the naive\nassumption that LLMs will attend to similar cues\nto humans, we are not fully uncovering the lens of\nwhy LLMs truly disagree.\n14https://github.com/google-research/FLAN/\n4575\nModel ChaosNLI-α ChaosNLI-S ChaosNLI-M\nAcc↑ JSD↓ DCE↓ Acc↑ JSD↓ DCE↓ Acc↑ JSD↓ DCE↓\nFlan-T5-L (780M) 72.9 0.228 0.255 54.6 0.341 0.382 59.7 0.299 0.325\nw/ Prompt Variation 64.9 0.279 0.317 63.3 0.303 0.330 64.7 0.303 0.331\nFlan-T5-XL (3B) 83.2 0.166 0.171 71.8 0.255 0.264 64.5 0.272 0.304\nw/ Prompt Variation 81.6 0.184 0.193 73.8 0.231 0.243 68.3 0.271 0.297\nFlan-T5-XXL (11B) 84.9 0.159 0.154 67.9 0.270 0.289 72.6 0.271 0.293\nw/ Prompt Variation 83.8 0.162 0.164 68.7 0.259 0.279 71.6 0.260 0.285\nGPT-3-D3 (175B) 76.5 0.254 0.249 62.7 0.348 0.374 63.3 0.373 0.402\nw/ Prompt Variation 72.3 0.285 0.29 50.1 0.402 0.453 51.5 0.403 0.452\nStable Vicuna(13B) 55.6 0.310 0.368 34.2 0.390 0.451 45.4 0.303 0.342\nw/ Prompt Variation 51.4 0.324 0.386 29.9 0.431 0.503 42.4 0.337 0.382\nTable 4: Inference and Human Alignment Performances on the ChaosNLI Datasets with and without Prompt\nVariations. The estimation methods for each model are the same as Figure 4. All the outputs are averaged over three\nruns, and bold texts indicate the best value for each model and column.\n6 Discussion\nLLMs do not perform well in NLI.Despite min-\nimal, unknown, or absence of exposure to the NLI\ntask, we anticipated that state-of-the-art LLMs such\nas GPT-3 and Stable Vicuna could reason with this\nrelatively basic inference problem. The models are\ntrained with billions of parameters and are known\nto be effective in helping real-world users solve\ndiverse, complex tasks (Ouyang et al., 2022). How-\never, the unforeseen poor performance of these\nmodels casts doubt as to whether they possess true\ngeneral language understanding abilities.\nThe problem is exacerbated for distilled models\n(e.g. Stable Vicuna) that are fine-tuned using pro-\nprietary LLMs, a performance discrepancy issue\nsimilarly raised by Gudibande et al., 2023. Since\nsmaller LLMs fully or partially trained with NLI\ntasks could perform much better than the MUE and\nNE models, this hints at a task-specific latent factor\nin NLI tasks where supervised training is benefi-\ncial and required for a wider definition of natural\nlanguage understanding. In fact, as these LLMs\ncan simply be fine-tuned to perform better for NLI\ntasks, a stricter evaluation criterion is needed to as-\nsess the genuine understanding capability of LLMs.\nCharacterizing Disagreement with respect to\nAmbiguity and Uncertainty Previous studies re-\nlate multiple annotations not only to disagreement\n(Uma et al., 2021; Gordon et al., 2021), but also to\nambiguity (Min et al., 2020; Tamkin et al., 2022;\nLiu and Liu, 2023), and mostly to uncertainty (Fox\nand Ülkümen, 2011; Xiao and Wang, 2021; Kuhn\net al., 2022; Zhan et al., 2023; Hu et al., 2023). The\ndefinitions of ambiguity, uncertainty, and disagree-\nment have the potential to be conflated and disam-\nbiguated. In our paper, we use the multinomial soft\nlabel estimate of a model as a representation of\n“disagreement”. When estimating this distribution\nwith MCE, our modeling assumption treats each\nquery to the model is analogous to asking an in-\ndividual annotator to provide a label. In contrast,\nLPE is analogous to asking an individual to assign\nthe scores to each option. Whereas most works\nexploit disagreement or uncertainty to improve var-\nious NLP task performances (Zhang et al., 2021;\nFornaciari et al., 2021b; Yu et al., 2022; Zhou et al.,\n2023) our study focuses on evaluating the mod-\nels. We find that using both methods for estimating\nthe multinomial label distribution by querying the\nlanguage model are not calibrated well with the\nhuman annotations.\nOther domain tasks are transferable to NLI.\nOur work can be expanded to test LLMs on other\nNLP applications (Plank, 2022) such as Question\nAnswering (De Marneffe et al., 2019), Fact Verifi-\ncation (Thorne et al., 2018), and Toxic Language\nDetection (Schmidt and Wiegand, 2017; Sandri\net al., 2023). Further, our method can be applied\nfor tasks that contain disagreements since they are\neasily transferable to NLI tasks (Dagan et al., 2006)\nlike the QNLI dataset from Table 2, for example,\ninstead of directly asking controversial questions\n(e.g., abortion) to the model (Santurkar et al., 2023),\nthe question format can be modified into a declara-\ntive statement in the premise and place a possible\nanswer in the hypothesis with a binary True/False\n4576\nFigure 5: Relationship between Human and LLM Entropy Levels Divided with Different Human Disagreement\nReasons. The estimation methods for each model are the same as Figure 4.\nlabel (Dagan et al., 2006). Thus, if these compli-\ncated tasks can be formulated in a way where the\nLLM can estimate a multinomial distribution over\na set of classes, our methods are applicable.\nHowever, we should consider the target tasks\nwhen tracing “human disagreement” only when\nit is a significant signal that needs to be captured.\nFor example, since it is important to include diverse\nopinions, we can easily apply our methods to detect\ndisagreements in hate speech (Schmidt and Wie-\ngand, 2017). In contrast, spotting disagreement in\nthe arithmetic reasoning task (Cobbe et al., 2021)\nmight be less important since it often requires a\nlogical step-by-step reasoning procedure to obtain\nan accurate answer.\nHow can we better align LLMs to represent dis-\nsenting voices? We point out the current limi-\ntation of utilizing LLMs to represent a larger hu-\nman population, especially when disagreements\nare present. The causes of this phenomenon are\nindiscernible due to the entanglement of miscali-\nbration of out-of-distribution (OOD) inference, ad-\nditional noise due to disagreement and ambiguity,\nprompt sensitivity, and more aspects that are yet\nto be identified. Even though simple remedies of\ntemperature scaling (Ackley et al., 1985; Wang\net al., 2022), incorporating logit bias, constrained\ndecoding (Ziems et al., 2023), or direct supervision\nto multiple annotations (Zhang et al., 2021; Forna-\nciari et al., 2021a) might mitigate the misalignment,\nthese methods are unrealistic and not scalable due\nto the exhaustive hyperparameter tuning and ad-\nditional data collection required to represent the\npopulation of interest.\nHowever, as LLM applications are becoming\nmore ubiquitous, it is important for them to faith-\nfully represent a larger population, preferably in-\ncluding the voices of minorities. Thus, we suggest\nthat future LLMs could be improved to reflect hu-\nman disagreements in diverse means, for example,\nby fine-tuning with ambiguous instances (Liu et al.,\n2023). As LLMs are shown to be aware of their\nignorance (Kadavath et al., 2022) and have the abil-\nity to express their level of confidence (Lin et al.,\n2022; Zhou et al., 2023), we expect future works to\naddress similar approaches in the aspect of align-\nment towards the human disagreement distribution.\nIn this way, the reconstructed model distribution\nwith MCE and LPE may better capture different\ninterpretations from human individuals, aiding ac-\ncountability.\n7 Conclusions\nIn this paper, we compare the performance of\ninstruction-following generative LLMs with other\nfully fine-tuned smaller models on the fundamental\nNLI task. First, by experimenting on four differ-\nent NLI datasets, we show LLMs are not perform-\ning well in the NLI task, considering their touted\nlanguage comprehension capabilities. Further, in\nagreement with the need for multiple annotations\nfor disagreeable NLP tasks, LLMs also fail to\nalign with human disagreements in the ChaosNLI\nand PK2019 datasets. Additional development is\nneeded to capture representative human distribu-\ntions, as well as to discover key factors to disagree-\nment sources that can influence the LLM’s answer\ndistribution.\nLimitations\nThis work shows the limited ability of billion-scale\nLLMs in inference and disagreement tasks. Al-\n4577\nthough we test with the dataset annotated with nu-\nmerous human subjects per sample, 100 people\nmay not be enough to represent the human dis-\nagreement distribution well. After more releases\nof human label variation datasets, our study can be\nextended by covering a wider range of model types\nand creating evaluation benchmarks to measure the\ndegree of disagreement. If we have robust LLMs\nin inference and disagreement, we could then try\nto find the latent factors that might not be human-\ninterpretable but lead to disagreement in LLMs and\ncompare them with those of humans.\nEthics Statement\nAs our work directly employs trained large lan-\nguage models without any extra process of fine-\ntuning, the risks and potential biases incurred by\nthe model checkpoints (e.g., dataset selection, train-\ning configurations) remain the same as the original\nworks.\nAcknowledgments\nThis work was supported by Institute of Informa-\ntion & communications Technology Planning &\nEvaluation (IITP) grant funded by the Korea gov-\nernment (MSIT) (No.2019-0-00075, Artificial In-\ntelligence Graduate School Program (KAIST)) and\nArtificial intelligence industrial convergence clus-\nter development project funded by the Ministry\nof Science and ICT (MSIT, Korea) & Gwangju\nMetropolitan City.\nReferences\nDavid H Ackley, Geoffrey E Hinton, and Terrence J Se-\njnowski. 1985. A learning algorithm for boltzmann\nmachines. Cognitive science, 9(1):147–169.\nJoris Baan, Wilker Aziz, Barbara Plank, and Raquel\nFernandez. 2022. Stop measuring calibration when\nhumans disagree. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1892–1915, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges. Evaluat-\ning Predictive Uncertainty, Visual Object Classifi-\ncation, and Recognising Tectual Entailment: First\nPASCAL Machine Learning Challenges Workshop,\nMLCW 2005, Southampton, UK, April 11-13, 2005,\nRevised Selected Papers, pages 177–190. Springer.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung, volume 23,\npages 107–124.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDominik Maria Endres and Johannes E Schindelin.\n2003. A new metric for probability distribu-\ntions. IEEE Transactions on Information theory ,\n49(7):1858–1860.\nTommaso Fornaciari, Alexandra Uma, Silviu Paun, Bar-\nbara Plank, Dirk Hovy, and Massimo Poesio. 2021a.\nBeyond black & white: Leveraging annotator dis-\nagreement via soft-label multi-task learning. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n2591–2597, Online. Association for Computational\nLinguistics.\n4578\nTommaso Fornaciari, Alexandra Uma, Silviu Paun, Bar-\nbara Plank, Dirk Hovy, Massimo Poesio, et al. 2021b.\nBeyond black & white: Leveraging annotator dis-\nagreement via soft-label multi-task learning. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies. Associ-\nation for Computational Linguistics.\nCraig R Fox and Gülden Ülkümen. 2011. Distinguish-\ning two dimensions of uncertainty. Fox, Craig R.\nand Gülden Ülkümen (2011),“Distinguishing Two\nDimensions of Uncertainty, ” in Essays in Judgment\nand Decision Making, Brun, W., Kirkebøen, G. and\nMontgomery, H., eds. Oslo: Universitetsforlaget.\nMax Glockner, Ieva Stali¯unait˙e, James Thorne, Gisela\nVallejo, Andreas Vlachos, and Iryna Gurevych. 2023.\nAmbifc: Fact-checking ambiguous claims with evi-\ndence. arXiv e-prints, pages arXiv–2104.\nMitchell L Gordon, Kaitlyn Zhou, Kayur Patel, Tat-\nsunori Hashimoto, and Michael S Bernstein. 2021.\nThe disagreement deconvolution: Bringing machine\nlearning performance metrics in line with reality. In\nProceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems, pages 1–14.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang\nGeng, Hao Liu, Pieter Abbeel, Sergey Levine, and\nDawn Song. 2023. The false promise of imitating\nproprietary llms. arXiv preprint arXiv:2305.15717.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International conference on machine learn-\ning, pages 1321–1330. PMLR.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason\nWeston. 2016. The goldilocks principle: Reading\nchildren’s books with explicit memory representa-\ntions. In 4th International Conference on Learning\nRepresentations, ICLR 2016.\nMengting Hu, Zhen Zhang, Shiwan Zhao, Minlie\nHuang, and Bingzhe Wu. 2023. Uncertainty in natu-\nral language processing: Sources, quantification, and\napplications. arXiv preprint arXiv:2306.04459.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nNan-Jiang Jiang and Marie-Catherine de Marneffe.\n2022. Investigating reasons for disagreement in natu-\nral language inference. Transactions of the Associa-\ntion for Computational Linguistics, 10:1357–1374.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2022.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\nIn The Eleventh International Conference on Learn-\ning Representations.\nSolomon Kullback and Richard A Leibler. 1951. On\ninformation and sufficiency. The annals of mathe-\nmatical statistics, 22(1):79–86.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTeaching models to express their uncertainty in\nwords. arXiv preprint arXiv:2205.14334.\nAlisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr,\nPeter West, Alexander Koller, Swabha Swayamdipta,\nNoah A Smith, and Yejin Choi. 2023. We’re afraid\nlanguage models aren’t modeling ambiguity. arXiv\npreprint arXiv:2304.14399.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZhu Liu and Ying Liu. 2023. Ambiguity meets uncer-\ntainty: Investigating uncertainty estimation for word\nsense disambiguation. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, pages\n3963–3977, Toronto, Canada. Association for Com-\nputational Linguistics.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nJohannes Mario Meissner, Napat Thumwanit, Saku Sug-\nawara, and Akiko Aizawa. 2021. Embracing ambi-\nguity: Shifting the training target of NLI models. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 862–869,\nOnline. Association for Computational Linguistics.\nNicholas Metropolis and Stanislaw Ulam. 1949. The\nmonte carlo method. Journal of the American statis-\ntical association, 44(247):335–341.\n4579\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. Ambigqa: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5783–\n5797.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470–3487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nMahdi Pakdaman Naeini, Gregory Cooper, and Milos\nHauskrecht. 2015. Obtaining well calibrated proba-\nbilities using bayesian binning. In Proceedings of the\nAAAI conference on artificial intelligence, volume 29.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020a. Adversarial\nnli: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885–4901.\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020b. What\ncan we learn from collective human opinions on nat-\nural language inference data? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9131–9143,\nOnline. Association for Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nEllie Pavlick and Tom Kwiatkowski. 2019. Inherent\ndisagreements in human textual inferences. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:677–694.\nBarbara Plank. 2022. The “problem” of human label\nvariation: On ground truth in data, modeling and\nevaluation. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 10671–10682, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nMarta Sandri, Elisa Leonardelli, Sara Tonelli, and Elis-\nabetta Ježek. 2023. Why don’t you do it right?\nanalysing annotators’ disagreement in subjective\ntasks. In Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics, pages 2420–2433.\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo\nLee, Percy Liang, and Tatsunori Hashimoto. 2023.\nWhose opinions do language models reflect? arXiv\npreprint arXiv:2303.17548.\nAnna Schmidt and Michael Wiegand. 2017. A survey\non hate speech detection using natural language pro-\ncessing. In Proceedings of the fifth international\nworkshop on natural language processing for social\nmedia, pages 1–10.\nJiuding Sun, Chantal Shaib, and Byron C Wallace.\n2023. Evaluating the zero-shot robustness of\ninstruction-tuned language models. arXiv preprint\narXiv:2306.11270.\nAlex Tamkin, Kunal Handa, Avash Shrestha, and Noah\nGoodman. 2022. Task ambiguity in humans and\nlanguage models. arXiv preprint arXiv:2212.10711.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Jason Wei, Xuezhi Wang, Hyung Won Chung,\nDara Bahri, Tal Schuster, Steven Zheng, et al. 2022.\nUl2: Unifying language learning paradigms. In The\nEleventh International Conference on Learning Rep-\nresentations.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nAlexandra N Uma, Tommaso Fornaciari, Dirk Hovy, Sil-\nviu Paun, Barbara Plank, and Massimo Poesio. 2021.\nLearning from disagreement: A survey. Journal of\nArtificial Intelligence Research, 72:1385–1470.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. EMNLP 2018,\npage 353.\nYuxia Wang, Minghan Wang, Yimeng Chen, Shimin\nTao, Jiaxin Guo, Chang Su, Min Zhang, and Hao\nYang. 2022. Capture human disagreement distribu-\ntions by calibrated networks for natural language\ninference. In Findings of the Association for Compu-\ntational Linguistics: ACL 2022, pages 1524–1535.\n4580\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nYijun Xiao and William Yang Wang. 2021. On hal-\nlucination and predictive uncertainty in conditional\nlanguage generation. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n2734–2744.\nYu Yu, Hassan Sajjad, and Jia Xu. 2022. Learning\nuncertainty for unknown domains with zero-target-\nassumption. In The Eleventh International Confer-\nence on Learning Representations.\nRunzhe Zhan, Xuebo Liu, Derek F Wong, Cuilian\nZhang, Lidia S Chao, and Min Zhang. 2023. Test-\ntime adaptation for machine translation evaluation\nby uncertainty minimization. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n807–820.\nShujian Zhang, Chengyue Gong, and Eunsol Choi. 2021.\nLearning with different amounts of annotation: From\nzero to many labels. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 7620–7632.\nXinliang Frederick Zhang and Marie-Catherine\nde Marneffe. 2021. Identifying inherent disagree-\nment in natural language inference. In Proceedings\nof the 2021 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4908–4915.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nKaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto.\n2023. Navigating the grey area: Expressions of\noverconfidence and uncertainty in language models.\narXiv preprint arXiv:2302.13439.\nXiang Zhou, Yixin Nie, and Mohit Bansal. 2022. Dis-\ntributed NLI: Learning to predict human opinion dis-\ntributions for language reasoning. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 972–987, Dublin, Ireland. Association\nfor Computational Linguistics.\nCaleb Ziems, William Held, Omar Shaikh, Jiaao Chen,\nZhehao Zhang, and Diyi Yang. 2023. Can large lan-\nguage models transform computational social sci-\nence? arXiv preprint arXiv:2305.03514.\nModel Precision Acc JSD\nFlan-T5-XXL FP32 75.4 0.233\nBF16 75.1 0.233\nTable 5: Effect of Precision on Inference and Human\nAlignment Performances for Flan-T5-XXL. The distri-\nbution is estimated using MCE (OS), same as Table 2.\nThe outputs are averaged over three ChaosNLI sub-\ndatasets.\nA Hyperparameters\nGenerally, we try to set similar hyperparameters to\nall the models with some exceptions due to model\nperformance and/or cost issues.\nTemperature To scale the confidence of the gen-\nerated output in a post-hoc manner, we unify the\ntemperature to be 1 ( i.e., no scaling). There ex-\nist other precedents that use a smaller temperature\nfor a more deterministic output (Santurkar et al.,\n2023) or compare outputs of models with varying\ntemperatures (Ouyang et al., 2022). However, as\nwe jointly assess LLMs on the accuracy of NLI\nand human disagreement alignment, we argue that\nhaving a fixed, un-scaled temperature to generate\nmodel outputs better aligns with our research goal\nof estimating model outputs to capture human dis-\nagreement distribution.\nGeneration Length Easily adjustable by all\nAPIs, including OpenAPI and Huggingface, we\nhave varying generation lengths per prompt design.\nAs discussed in Section 4.3, NS is a cost-efficient\nalternative method for OS, solely needing a single\noutput token of numbers. Thus in LPE, a method\nfor single token probability output, we only use the\nOS prompt for effective token probability calcula-\ntion. We set a maximum token output length of 10\nfor MCE and 1 for LPE.\nFloating Point We load models of size greater\nthan 10 billion parameters (except for GPT-3) with\nhalf the precision (bfloat16; BF16). We observe\nFlan-T5-XXL shows a negligible increase in per-\nformance when using the original precision (single-\nprecision floating-point; FP32) (Table 5).\nB Levels of NLI Exposure\nWe outline the level of exposure to the NLI task\nfor each model since it is an influential factor that\naffects the accuracy and human-alignment perfor-\nmances of the models.\n4581\nB.1 Full Exposure (FE) Models\nThe models below are fine-tuned with the training\nset of an NLI task as outlined in Nie et al., 2020b.\n• Models: BERT and RoBERTa\nB.2 Partial Exposure (PE) Models\nThese models are partially exposed to the NLI task\nin the fine-tuning stage. However, the extent of\nexposure is different by the adopted fine-tuning\nstrategy, thus listed in decreasing order.\nFlan Collection\n• Models: Flan-T5 models and Flan-UL2\n• The Flan Collection (Longpre et al., 2023) is a\ncollection of datasets in the format of instruc-\ntions to enable generalization to diverse un-\nseen tasks. It employs a fine-tuning strategy of\na maximum of 1836 NLP tasks with some NLI\ntasks taken into account ( e.g., ANLI, RTE,\nMNLI, QNLI, SNLI, etc.).15\nInstruction Meta-Learning (IML) Bench\n• Models: OPT-IML-M models\n• Instruction Meta Learning (IML) Bench (Iyer\net al., 2022) is a more common benchmark\nthat uses 1500+ NLP tasks in the fine-tuning\nstage. Flan is a major portion of this bench-\nmark, with other major portions in other large\ndatasets. We expect some NLI exposure but\nnot as strong as the models fine-tuned by the\nFlan dataset.\nB.3 Minimal/Unknown Exposure (MUE)\nModels\nThe models below are unknown to the extent of\nexposure to a specific NLI task.\n• Models: GPT-3-D2, GPT-3-D3\nThe InstructGPT paper (Ouyang et al., 2022)\ndoes elaborate that the models utilizes a reward\nmodel in the process of RLHF (Reinforcement\nLearning from Human Feedback), and it is fine-\ntuned by a variety of NLP datasets, including\nMNLI. However, the serviced models are not di-\nrectly mapped to the models of the paper, leaving\nthe exposure to NLI largely unknown16.\n15https://github.com/google-research/FLAN/tree/main\n16Refer to the OpenAI Documentation\nB.4 No Exposure (NE) Models\nThe below model does not have any exposure to a\nspecific NLI task.\n• Model: Stable Vicuna\nC Postprocessing\nUnlike conventional approaches of fine-tuning\nmodels directly on the downstream NLI dataset,\none of the challenges in assessing an NLI task is\nthe variability of generated outputs. To transform\nand choose valid options from the generated out-\nputs, we conduct postprocessing through a manu-\nally crafted dictionary for each option ( See Valid\noption examples on the last page.).\nD Distribution Alignment Among LLMs\nWe illustrate the averaged sample-level JSD en-\ntropy for each model pair (Figure 6) to visualize\nthe trend of alignment among LLMs. Throughout\nall four JSD distribution plots, the scale and range\nof the JSD values differ for each data. Still, the gen-\neral trend is maintained, where ChaosNLI-αshows\nlow JSD values overall, likely attributed to lower\ntask difficulty witnessed by the performance gap\namong datasets in Table 2. The best-performing\nmodels, Flan-T5-XXL and Flan-UL2, present the\nlowest disagreement in entropy for all plots.\nAlthough the size and type of model are influ-\nencing factors, the most consistent factor is the\ntype of instruction fine-tuning introduced for each\nmodel. Throughout all plots, the alignment is well\nshown for the group of models fine-tuned by the\nFlan dataset and the IML Bench. As we expect\nmore research in the scope of human alignment in\nNLP, the evaluation of the human alignment among\nthe models with the same fine-tuning process can\nalso be studied and reported.\nHowever, a strong distinction needs to be made\nin which an overall lower number of JSD values in\nthis plot does not mean that a model has always had\na good performance in human disagreement align-\nment. This figure merely delineates the alignment\ntrends among models.\nE Effect of Few-Shot Examples\nWe observe no consistent benefit nor harm in ex-\nperimenting with few-shot settings that resemble\nthe human annotation process more than zero-shot\nsettings (Table 6). In fact, zero-shot evaluation gen-\nerally seems to show better performances across\n4582\nChaosNLI-𝛼\nPK2019ChaosNLI-M\nChaosNLI-S\nFigure 6: JSD Distribution between All Combinations of Pairs for LLMs. The darker plot indicates a similar\ndistribution between a pair of models. The estimation methods for each model are the same as Table 2.\ndatasets and models compared to the few-shot eval-\nuations. In the case of Stable Vicuna, the perfor-\nmance increases in the 1-shot setting forα-NLI and\nthe 3-shot setting for SNLI. However, we notice\na plunge in 5-shot performance, especially for the\nMNLI dataset.\nF Prompt Examples\nWe present examples of prompts we used during\nthe generation process in Figure 1 (See two prompt\nexamples on the last page.). We incorporate a sug-\ngested general prompt template pre-specified for a\nspecific model. For example, we implement a hu-\nman and assistant-style prompt template for Stable\nVicuna. Otherwise, we leave the template format\nthe same for the rest of the models.\n4583\nModel ChaosNLI-α ChaosNLI-S ChaosNLI-M\nAcc↑ JSD↓ DCE↓ Acc↑ JSD↓ DCE↓ Acc↑ JSD↓ DCE↓\nFlan-T5-XXL (0 Shot) 85.0 0.160 0.155 67.3 0.271 0.291 72.6 0.269 0.290\n+ 1 Shot 84.9 0.159 0.154 68.0 0.278 0.296 74.8 0.261 0.278\n+ 3 Shot 83.6 0.163 0.160 67.0 0.285 0.304 74.0 0.271 0.290\n+ 5 Shot 84.9 0.158 0.154 65.7 0.288 0.309 73.2 0.275 0.295\nGPT-3-D3 (0 Shot) 76.1 0.254 0.249 62.4 0.348 0.374 63.0 0.376 0.405\n+ 1 Shot 77.7 0.240 0.235 6.2 0.400 0.433 61.1 0.414 0.445\n+ 3 Shot 80.7 0.233 0.216 55.7 0.407 0.442 58.2 0.436 0.471\n+ 5 Shot 81.7 0.233 0.213 57.9 0.396 0.426 62.2 0.425 0.454\nStable Vicuna (0 Shot) 55.7 0.310 0.368 33.5 0.391 0.454 46.2 0.304 0.342\n+ 1 Shot 64.3 0.290 0.336 38.6 0.377 0.436 41.7 0.311 0.355\n+ 3 Shot 56.2 0.296 0.346 43.4 0.351 0.405 32.8 0.352 0.418\n+ 5 Shot 56.1 0.298 0.350 35.8 0.379 0.441 28.8 0.370 0.441\nTable 6: Inference and Human Alignment Performances on the ChaosNLI Datasets for Zero-shot and Few-shot\nSettings. The estimation methods for each model are the same as Table 2. Bold texts indicate the best value for each\nmodel and column.\n4584\nValid option examples for ChaosNLI-α/S/M and two prompt types - OS and NS\ndict_alphanli_OS = {'1' : ['1','Hypothesis 1',...]\n'2' : ['2','Hypothesis 2',...]}\ndict_alphanli_NS = {'1' : '1'\n'2' : '2'}\ndict_s&mnli_OS = {'e' : ['entail','infer','yes', ...]\n'c' : ['contradict','oppose','no', ...]\n'n' : ['neutral','unanswerable',...]}\ndict_s&mnli_NS = {'e' : '1'\n'c' : '2'\n'n' : '3'}\nPrompt example for ChaosNLI-α using OS\nINPUT\nRead the following and determine if the hypothesis can be inferred from the premise.\nObservation Start: My roommates put up their Christmas tree this year.\nObservation End: This is what it’s like living with a cat.\nHypothesis 1: The roommates soon had to take the tree down.\nHypothesis 2: The cat enjoyed the ornaments and garland and slept under the tree.\nOptions: Hypothesis 1, Hypothesis 2\nOUTPUT\nAnswer: <Generated Output>\nPrompt example for ChaosNLI-S/M using NS\nINPUT\nRead the following and determine if the hypothesis can be inferred from the premise.\nPremise: This town, which flourished between 6500 and 5500 b.c. ... appear on Anatolian kilims.\nHypothesis: This town is over 8000 years old.\nOptions: 1: entailment, 2: contradiction, 3: neutral\nOUTPUT\nAnswer: <Generated Output>\n4585",
  "topic": "Representativeness heuristic",
  "concepts": [
    {
      "name": "Representativeness heuristic",
      "score": 0.7506270408630371
    },
    {
      "name": "Multinomial distribution",
      "score": 0.5517626404762268
    },
    {
      "name": "Computer science",
      "score": 0.5400558114051819
    },
    {
      "name": "Inference",
      "score": 0.510256290435791
    },
    {
      "name": "Population",
      "score": 0.4881454408168793
    },
    {
      "name": "Distribution (mathematics)",
      "score": 0.42895251512527466
    },
    {
      "name": "Scope (computer science)",
      "score": 0.41622859239578247
    },
    {
      "name": "Weighting",
      "score": 0.410474956035614
    },
    {
      "name": "Natural language processing",
      "score": 0.39992332458496094
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36063066124916077
    },
    {
      "name": "Econometrics",
      "score": 0.3472873866558075
    },
    {
      "name": "Statistics",
      "score": 0.3271038234233856
    },
    {
      "name": "Mathematics",
      "score": 0.1580238938331604
    },
    {
      "name": "Sociology",
      "score": 0.11306723952293396
    },
    {
      "name": "Demography",
      "score": 0.10403645038604736
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Radiology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210099236",
      "name": "Kootenay Association for Science & Technology",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    }
  ]
}