{
  "title": "Curved Representation Space of Vision Transformers",
  "url": "https://openalex.org/W4393147269",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2136501916",
      "name": "Juyeop Kim",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2098767586",
      "name": "Junha Park",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2102539351",
      "name": "Songkuk Kim",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2114119808",
      "name": "Jong-Seok Lee",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2136501916",
      "name": "Juyeop Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098767586",
      "name": "Junha Park",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102539351",
      "name": "Songkuk Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114119808",
      "name": "Jong-Seok Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3204773485",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W6790734990",
    "https://openalex.org/W6758955634",
    "https://openalex.org/W2947850552",
    "https://openalex.org/W7071247091",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6762585180",
    "https://openalex.org/W6729756640",
    "https://openalex.org/W6600351811",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6810938606",
    "https://openalex.org/W6683595889",
    "https://openalex.org/W2254249950",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W2786622092",
    "https://openalex.org/W6795276571",
    "https://openalex.org/W2990405359",
    "https://openalex.org/W6718212895",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2796438033",
    "https://openalex.org/W2767609493",
    "https://openalex.org/W2529714286",
    "https://openalex.org/W2945125794",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3093783084",
    "https://openalex.org/W6797790494",
    "https://openalex.org/W3213783001",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2996456641",
    "https://openalex.org/W2970121940",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W2208555118",
    "https://openalex.org/W2970338090",
    "https://openalex.org/W2951603627",
    "https://openalex.org/W2963389226",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963982496",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W4294576319",
    "https://openalex.org/W2029027380",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3122110922",
    "https://openalex.org/W2949807021",
    "https://openalex.org/W3129389511",
    "https://openalex.org/W3214243459",
    "https://openalex.org/W4225871896",
    "https://openalex.org/W3175544090",
    "https://openalex.org/W4287123589",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4226021252"
  ],
  "abstract": "Neural networks with self-attention (a.k.a. Transformers) like ViT and Swin have emerged as a better alternative to traditional convolutional neural networks (CNNs). However, our understanding of how the new architecture works is still limited. In this paper, we focus on the phenomenon that Transformers show higher robustness against corruptions than CNNs, while not being overconfident. This is contrary to the intuition that robustness increases with confidence. We resolve this contradiction by empirically investigating how the output of the penultimate layer moves in the representation space as the input data moves linearly within a small area. In particular, we show the following. (1) While CNNs exhibit fairly linear relationship between the input and output movements, Transformers show nonlinear relationship for some data. For those data, the output of Transformers moves in a curved trajectory as the input moves linearly. (2) When a data is located in a curved region, it is hard to move it out of the decision region since the output moves along a curved trajectory instead of a straight line to the decision boundary, resulting in high robustness of Transformers. (3) If a data is slightly modified to jump out of the curved region, the movements afterwards become linear and the output goes to the decision boundary directly. In other words, there does exist a decision boundary near the data, which is hard to find only because of the curved representation space. This explains the underconfident prediction of Transformers. Also, we examine mathematical properties of the attention operation that induce nonlinear response to linear perturbation. Finally, we share our additional findings, regarding what contributes to the curved representation space of Transformers, and how the curvedness evolves during training.",
  "full_text": "Curved Representation Space of Vision Transformers\nJuyeop Kim, Junha Park, Songkuk Kim*, Jong-Seok Lee*\nSchool of Integrated Technology / BK21 Graduate Program in Intelligent Semiconductor Technology\nYonsei University, Korea\n{juyeopkim, junha.park, songkuk, jong-seok.lee}@yonsei.ac.kr\nAbstract\nNeural networks with self-attention (a.k.a. Transformers) like\nViT and Swin have emerged as a better alternative to tra-\nditional convolutional neural networks (CNNs). However,\nour understanding of how the new architecture works is\nstill limited. In this paper, we focus on the phenomenon\nthat Transformers show higher robustness against corruptions\nthan CNNs, while not being overconfident. This is contrary to\nthe intuition that robustness increases with confidence. We re-\nsolve this contradiction by empirically investigating how the\noutput of the penultimate layer moves in the representation\nspace as the input data moves linearly within a small area. In\nparticular, we show the following. (1) While CNNs exhibit\nfairly linear relationship between the input and output move-\nments, Transformers show nonlinear relationship for some\ndata. For those data, the output of Transformers moves in a\ncurved trajectory as the input moves linearly. (2) When a data\nis located in a curved region, it is hard to move it out of the\ndecision region since the output moves along a curved trajec-\ntory instead of a straight line to the decision boundary, result-\ning in high robustness of Transformers. (3) If a data is slightly\nmodified to jump out of the curved region, the movements af-\nterwards become linear and the output goes to the decision\nboundary directly. In other words, there does exist a decision\nboundary near the data, which is hard to find only because of\nthe curved representation space. This explains the undercon-\nfident prediction of Transformers. Also, we examine mathe-\nmatical properties of the attention operation that induce non-\nlinear response to linear perturbation. Finally, we share our\nadditional findings, regarding what contributes to the curved\nrepresentation space of Transformers, and how the curved-\nness evolves during training.\nIntroduction\nSelf-attention-based neural network architectures, including\nVision Transformers (Dosovitskiy et al. 2021), Swin Trans-\nformers (Liu et al. 2021), etc. (hereinafter referred to as\nTransformers), have shown to outperform traditional convo-\nlutional neural networks (CNNs) in various computer vision\ntasks. The success of the new architecture has prompted a\nquestion, how Transformers work, especially compared to\nCNNs, which would also shed light on deeper understand-\ning of CNNs and eventually neural networks.\n*Corresponding authors\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Input space\n(b) Representation space (c) Representation space\n(ResNet50) (Swin-T)\nFigure 1: 2D projected movements of (a) the data (black dot)\nin the input space and corresponding output features in the\nrepresentation space for (b) ResNet50 and (c) Swin-T.\nIn addition to the improved task performance (e.g., clas-\nsification accuracy) compared to CNNs, Transformers also\nshow desirable characteristics in other aspects. It has been\nshown that Transformers are more robust to adversarial per-\nturbations than CNNs (Bai et al. 2021; Naseer et al. 2021;\nPaul and Chen 2022). Moreover, Transformers are reported\nnot overconfident in their predictions unlike CNNs (Min-\nderer et al. 2021) (and we show that Transformers are ac-\ntually underconfident in this paper).\nThe high robustness, however, does not comport with un-\nderconfidence. Intuitively, a data that is correctly classified\nby a model with lower confidence is likely to be located\ncloser to the decision boundary (see Appendix for detailed\ndiscussion). Then, a smaller amount of perturbation would\nmove the data out of the decision region, which translates\ninto lower robustness of the model. However, the previous\nresults claim the opposite.\nTo mitigate the contradiction of robustness and undercon-\nfidence, this paper presents our empirical study to explore\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13142\nthe representation space of Transformers and CNNs. More\nspecifically, we focus on the linearity of the models, i.e., the\nchange of the output feature (which is simply referred to as\noutput in this paper) with respect to the linear change of the\ninput data. It is known that adversarial examples are a result\nof models being too linear, based on which the fast gradient\nsign method (FGSM) was introduced to show that deep neu-\nral networks can be easily fooled (Goodfellow, Shlens, and\nSzegedy 2015). Motivated by this, we examine the input-\noutput relationship of Transformers through the course that\nthe input is gradually perturbed along the direction deter-\nmined by FGSM.\nFig. 1 visualizes the representation spaces of CNNs and\nTransformers comparatively (see Appendix for implementa-\ntion details). An image data from ImageNet (Russakovsky\net al. 2015), marked with the black dot in Fig. 1a, is gradu-\nally modified by a fixed amount along two mutually orthog-\nonal directions. The corresponding outputs of ResNet50 (He\net al. 2016) and Swin-T (Liu et al. 2021) are obtained, which\nare shown after two-dimensional projection in Figs. 1b and\n1c, respectively. While the gradual changes of the input pro-\nduce almost linear changes in the output of ResNet50, the\noutput trajectory of Swin-T is nonlinear around the original\noutput (and then becomes linear when the change of the out-\nput is large), i.e., the representation space is locally curved.\nWe empirically show that this curved representation space\nresults in the aforementioned contradiction.\nOur main research questions and findings can be summa-\nrized as follows.\n1. How does the representation space of Transformers\nlook like? To answer this, we analyze the movement of the\npenultimate layer’s output with respect to the linear move-\nment of the input. We use the adversarial gradient produced\nby FGSM (Goodfellow, Shlens, and Szegedy 2015) as the\ndirection of movement in the input space, to investigate the\nlinearity of the feature space of the models. We find that\nthe directions of successive movements of the output sig-\nnificantly change in the case of Transformers unlike CNNs,\nindicating that the representation space of Transformers\nis locally curved.\n2. What makes Transformers robust to input perturbation?\nWe find that the curved regions in the representation\nspace account for the robustness of Transformers. When\na data is located in a curved region, a series of linear per-\nturbations to the input move the output point along a curved\ntrajectory. This makes it hard to move the data out of its de-\ncision region along a short and straight line, which explains\nhigh robustness of Transformers for the data.\n3. Then, why is the prediction of Transformers undercon-\nfident? Although it takes many steps to escape from a curved\ndecision region and reach a decision boundary, we find that\na decision boundary is actually located closely to the origi-\nnal output. We demonstrate a simple trick to reach the deci-\nsion boundary quickly. I.e., when a small amount of random\nnoise is added to the input data, its output can jump out of\nthe locally curved region and arrive at a linear region, from\nwhich a closely located decision boundary can be reached by\nadding only a small amount of perturbation. This reveals that\nthe decision boundary exists near the original data in the\nrepresentation space, which explains the underconfident\npredictions of Transformers.\nWe also present additional observations examining what\ncontributes to the curved representation space of Transform-\ners and when the curvedness is formed during training.\nThe Appendix of this paper can be found in the following\nlink: https://arxiv.org/abs/2210.05742.\nRelated Work\nSince the first application of the self-attention mechanism\nto vision tasks (Dosovitskiy et al. 2021), a number of stud-\nies have shown that the models built with traditional con-\nvolutional layers are outperformed by Transformers utiliz-\ning self-attention layers in terms of task performance (Liu\net al. 2021; Chu et al. 2021; Huang et al. 2021; Li et al.\n2021; Touvron et al. 2021; Wang et al. 2021; Xiao et al.\n2021; Yang et al. 2021; Yuan et al. 2021; Liu et al. 2022a).\nThere have been efforts to compare CNNs and Transformers\nin various aspects. Empirical studies show that Transform-\ners have higher adversarial robustness than CNNs (Paul and\nChen 2022; Naseer et al. 2021; Aldahdooh, Hamidouche,\nand Deforges 2021; Bhojanapalli et al. 2021), which seems\nto be due to the reliance of Transformers on lower frequency\ninformation than CNNs (Park and Kim 2022; Benz et al.\n2021). Other studies conclude that Transformers are cali-\nbrated better than CNNs yielding overconfident predictions\n(Guo et al. 2017; Thulasidasan et al. 2019; Wen et al. 2021;\nMinderer et al. 2021). However, there has been no clear ex-\nplanation encompassing both higher robustness and lower\nconfidence of Transformers.\nUnderstanding how neural networks work has been an im-\nportant research topic. A useful way for this is to investigate\nthe input-output mapping formed by a model. Since mod-\nels with piecewise linear activation functions (e.g., ReLU)\nimplement piecewise linear mappings, several studies inves-\ntigate the characteristics of linear regions, e.g., counting the\nnumber of linear regions as a measure of model expressiv-\nity (or complexity) (Montufar et al. 2014; Hanin and Rol-\nnick 2019a,b; Telgarsky 2015; Serra, Tjandraatmadja, and\nRamalingam 2018; Raghu et al. 2017) and examining lo-\ncal properties of linear regions (Zhang and Wu 2020). Some\nstudies examine the length of the output curve for a given\nunit-length input (Raghu et al. 2017; Price and Tanner 2021;\nHanin, Jeong, and Rolnick 2022). There also exist some\nworks that relate the norm of the input-output Jacobian ma-\ntrix to generalization performance (Sokoli´c et al. 2017; No-\nvak et al. 2018). However, the input-output relationship of\nTransformers has not been explored previously, which is fo-\ncused in this paper.\nOn the Ostensible Contradiction of High\nRobustness and Underconfidence\nModel Calibration\nIt is desirable that a trained classifier is well-calibrated by\nmaking prediction with reasonable certainty, e.g., for data\nthat a classifier predicts with confidence (i.e., probability\nof the predicted class) of 80%, its accuracy should also be\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13143\n80% in average. A common measure to evaluate model cal-\nibration is the expected calibration error (ECE) defined as\n(Naeini, Cooper, and Hauskrecht 2015)\nECE =\nKX\ni=1\nP(i) · |oi − ei|, (1)\nwhere K is the number of bins of confidence, P(i) is the\nfraction of data falling into bin i, oi is the accuracy of the\ndata in bin i, and ei is the average confidence of the data\nin bin i. One limitation of ECE is that it does not distin-\nguish between overconfidence and underconfidence because\nthe sign of the difference between the accuracy and the con-\nfidence is ignored. Therefore, we define signed ECE (sECE)\nto augment ECE, as follows.\nsECE =\nKX\ni=1\nP(i) · (oi − ei). (2)\nAn overconfident model will have higher confidence than\naccuracy, resulting in a negative sECE value. An undercon-\nfident model, in contrast, will show a positive value of sECE.\nWe compare the calibration of CNNs, including ResNet50\n(He et al. 2016) and MobileNetV2 (Sandler et al. 2018;\nHoward et al. 2019), and Transformers, including ViT-B/16\n(Dosovitskiy et al. 2021) and Swin-T (Liu et al. 2021), on\nthe ImageNet validation set using ECE and sECE in Fig. 2\n(see Fig. 11 in Appendix for the results of other models).\nCNNs show negative ECE values and bar plots below the\n45◦ line, indicating overconfidence in prediction, which is\nconsistent with the previous studies (Guo et al. 2017). On\nthe other hand, Transformers are underconfident, showing\npositive sECE and bar plots over the 45 ◦ line. This com-\nparison result is interesting: Transformers reportedly show\nhigher classification accuracy than CNNs, but in fact with\nlower confidence.\nPassage to Decision Boundary\nIt is a common intuition that if a model classifies a data with\nlow confidence, the data is likely to be located near a deci-\nsion boundary (see Appendix for detailed discussion). Based\non the above results, therefore, the decision boundaries of\nTransformers are assumed to be formed near the data com-\npared to CNNs. To validate this, we formulate a procedure\nto examine the distance to a decision boundary from a data\nthrough a linear travel. Concretely, we aim to solve the fol-\nlowing optimization problem:\narg min\nϵ\nC(x′) ̸= y, x′ = x + ϵ · d, (3)\nwhere x is the input data, y is the true class label of x, C\nis the classifier, d is the travel direction, ϵ is a positive real\nnumber indicating the travel length, and x′ is the traveled\nresult of x. We set the travel direction d as the adversarial\ngradient produced by FGSM, i.e.,\nd = sign(∇xJ(C(x), y)), (4)\nwhere J is the classification loss function (i.e., cross-\nentropy). Note that ∥d∥2 =\n√\nD, where D is the dimen-\nsion of x. Refer to Algorithm 1 in Appendix for the detailed\nprocedure to solve the optimization problem in Eq. 3.\n0 .2 .4 .6 .8 1\nConfidence\n0\n.2\n.4\n.6\n.8\n1Accuracy\nECE: 4.16%\nsECE: -4.09%\n0\n.2\n.4\n.6\n.8\n1\n0 .2 .4 .6 .8 1\nConfidence\n0\n.2\n.4\n.6\n.8\n1Accuracy\nECE: 2.95%\nsECE: -2.84%\n0\n.2\n.4\n.6\n.8\n1\n(a) ResNet50 (b) MobileNetV2\n0 .2 .4 .6 .8 1\nConfidence\n0\n.2\n.4\n.6\n.8\n1Accuracy\nECE: 5.06%\nsECE: +4.59%\n0\n.2\n.4\n.6\n.8\n1\n0 .2 .4 .6 .8 1\nConfidence\n0\n.2\n.4\n.6\n.8\n1Accuracy\nECE: 6.19%\nsECE: +6.19%\n0\n.2\n.4\n.6\n.8\n1\n(c) ViT-B/16 (d) Swin-T\nFigure 2: Reliability diagrams of CNNs and Transformers.\nTransparency of bars represent the ratio of the number of\ndata in each confidence bin. ECE and sECE values are also\nshown in each case.\n(a) ResNet50 (b) MobileNetV2\n(c) ViT-B/16 (d) Swin-T\nFigure 3: Lengths (ϵ) of the travel to decision boundaries\nwith respect to the confidence for the ImageNet validation\ndata. Black lines represent average values.\nFig. 3 shows the obtained values of ϵ with respect to\nthe confidence values for the ImageNet validation data (see\nFig. 12 in Appendix for the results of other models). On the\ncontrary to our expectation, decision boundaries seem to be\nlocated farther from the data in the input space for Trans-\nformers than CNNs. This contradiction is resolved in the\nfollowing section.\nResolving the Contradiction\nShape of Representation Space\nAs mentioned in the Introduction, the FGSM attack was\nfirst introduced to show that the linearity of a model causes\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13144\n(a) Input space (b) Representation space\nFigure 4: Illustration of the input-output relationship of Transformers in terms of the trajectories in the input space and the\nrepresentation space.\nits vulnerability to adversarial perturbations (Goodfellow,\nShlens, and Szegedy 2015). To resolve the contradiction\nbetween high robustness (a large distance to the decision\nboundary) and underconfidence (a small distance to the de-\ncision boundary) of Transformers in the previous section,\ntherefore, we examine the degree of linearity of the input-\noutput relationship, i.e., how linear movements in the input\nspace appear in the representation space of Transformers.\nWe divide the travel intoN steps as\nx(n) = x(0) + n · ϵ\nN d, (n = 0,1, ··· , N) (5)\nwhere x(0) = x and x(N) are the initial and final data points,\nrespectively. For each x(n), we obtain its output feature at\nthe penultimate layer, which is denoted as z(n). Unlike the\ntravel in the input space, the magnitude and direction of the\ntravel appearing in the representation space may change at\neach step. Thus, the movement at step n is defined as\nd(n)\nz = z(n) − z(n−1), (6)\nfrom which the magnitude (ω (n)) and relative direction\n(θ(n)) are obtained as\nω(n) = ∥d(n)\nz ∥, θ (n) = cos−1\n \nd(n)\nz · d(n+1)\nz\n∥d(n)\nz ∥∥d(n+1)\nz ∥\n!\n.\n(7)\nWe consider three different ways to determine d:\n• dFGSM (blue-colored trajectory in Fig. 4): FGSM direc-\ntion for x(0) (as in Eq. 4).\n• dr+FGSM (yellow-colored trajectory in Fig. 4): FGSM di-\nrection determined for the randomly perturbed datax(0)\nr =\nx(0) + ϵr · r, where r is a random vector (∥r∥ 2 =\n√\nD)\nand ϵr controls the amount of this “random jump.”\n• drFGSM+FGSM (red-colored trajectory in Fig. 4): FGSM\ndirection determined for the data perturbed in the direction\nof dr+FGSM, i.e., x(0)\nrFGSM = x(0) + ϵr+FGSM ·dr+FGSM,\nwhere ϵr+FGSM controls the amount of this jump.\nResNet50 MobileNetV2 ViT-B/16 Swin-T\ndFGSM\n0 10 20 30 40 50\nn\n0\n1\n4\n1\n2\n3\n4\n(n)\n0 10 20 30 40 50\nn\n0\n1\n4\n1\n2\n3\n4\n(n)\n0 10 20 30 40 50\nn\n0\n1\n4\n1\n2\n3\n4\n(n)\n0 10 20 30 40 50\nn\n0\n1\n4\n1\n2\n3\n4\n(n)\n(a) (b) (c) (d)\ndr+FGSM\n0 10 20 30 40 50\nn\n0\n1\n4\n1\n2\n3\n4\n(n)\n0 10 20 30 40 50\nn\n0\n1\n4\n1\n2\n3\n4\n(n)\n0 10 20 30 40 50\nn\n0\n1\n4\n1\n2\n3\n4\n(n)\n0 10 20 30 40 50\nn\n0\n1\n4\n1\n2\n3\n4\n(n)\n(e) (f) (g) (h)\nFigure 5: Direction changes of output features with respect\nto the travel step (n). Light-gray regions: Range between the\nminimum and maximum values. Dark-gray regions: Range\nbetween the first quartile (Q1) and the third quartile (Q3).\nBlack lines: Medians (Q2). Red dots: Mean values.\nFigs. 5a-5d show the direction changes in travel for\nResNet50, MobileNetV2, ViT-B/16 and Swin-T when d =\ndFGSM, ϵ = .05, and N = 50. See Fig. 13 in Appendix\nfor the results of other travel directions, which shows a\nsimilar trend. For ResNet50 and MobileNetV2, the direc-\ntion does not change much (Figs. 5a and 5b), which in fact\nholds regardless of the travel direction in the input space\n(see Figs. 13a and 13b in Appendix). This indicates that\nthe input-output relationship of CNNs is fairly linear\naround the data. In contrast, ViT-B/16 and Swin-T shows\nlocally nonlinear input-output relationship; θ(n) is signifi-\ncantly large in early steps of travel (Figs. 5c and 5d), even\nfor other travel directions in the input space (see Figs. 13c\nand 13d in Appendix). I.e., Transformers generatenonlin-\near response to linear perturbation and the representa-\ntion space of Transformers is curved around the data.\nFigs. 5e-5h show the direction changes in travel when\nd = dr+FGSM, with ϵr = .05 except for ViT-B/16 using\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13145\n0 20\nDistance to DB\n0.00\n0.02\n0.04\n0.06Probability\n0 20\nDistance to DB\n0.00\n0.02\n0.04\n0.06Probability\n(a) ResNet50 (b) MobileNetV2\n0 200\nDistance to DB\n0.00\n0.01\n0.02Probability\n0 10 20\nDistance to DB\n0.00\n0.02\n0.04\n0.06Probability\n(c) ViT-B/16 (d) Swin-T\nFigure 6: Distribution of distance from the original output to\nthe decision boundary (DB) in the representation space.\n(a) ViT-B/16 (b) Swin-T\nFigure 7: Distance from the original output to the decision\nboundary (DB) in the representation space with respect to\nconfidence. Colors indicate θ(1). Black lines represent aver-\nage values.\nϵr = .20 (see Appendix for discussion). It can be observed\nthat the direction does not change much after the random\njump. I.e., the curvedness of the representation space is\nlocalized around the data. Therefore, by makingx(0) jump\na certain distance in a random directionr, z(0) can pass over\nthe curved region without meandering in the early steps and\nmake linear movements afterwards (zr(n) in Fig. 4b).\nRobustness and Underconfidence of Transformers\nFig. 6 shows the distribution of the Euclidean distance from\nthe original output to the decision boundary in the represen-\ntation space (i.e.,||z(N)−z(0)||). Note that the distance scale\nis different between the models. Interestingly, the distance\ndistributions for ViT-B/16 and Swin-T are bimodal, i.e., the\ndata are grouped into those having small distances and those\nhaving large distances.\nWe examine this phenomenon further in Fig. 7, which\nshows scatter plots between the confidence and the dis-\ntance, where the colors represent θ(1) of the corresponding\ndata. Note that θ(1) is highly correlated to the total direc-\ntion change (PN−1\nn=1 θ(n)), and thus is used as a measure of\nResNet50 MobileNetV2 ViT-B/16 Swin-T\nϵIFGSM = .001\n0 1\n4\n1\n2\n3\n4\n(1)\n0\n.2\n.4\n.6\n.8\n1Accuracy\n0\n.2\n.4\n.6\n.8\n1\n0 1\n4\n1\n2\n3\n4\n(1)\n0\n.2\n.4\n.6\n.8\n1Accuracy\n0\n.2\n.4\n.6\n.8\n1\n0 1\n4\n1\n2\n3\n4\n(1)\n0\n.2\n.4\n.6\n.8\n1Accuracy\n0\n.2\n.4\n.6\n.8\n1\n0 1\n4\n1\n2\n3\n4\n(1)\n0\n.2\n.4\n.6\n.8\n1Accuracy\n0\n.2\n.4\n.6\n.8\n1\n(a) (b) (c) (d)\nϵIFGSM = .002\n0 1\n4\n1\n2\n3\n4\n(1)\n0\n.2\n.4\n.6\n.8\n1Accuracy\n0\n.2\n.4\n.6\n.8\n1\n0 1\n4\n1\n2\n3\n4\n(1)\n0\n.2\n.4\n.6\n.8\n1Accuracy\n0\n.2\n.4\n.6\n.8\n1\n0 1\n4\n1\n2\n3\n4\n(1)\n0\n.2\n.4\n.6\n.8\n1Accuracy\n0\n.2\n.4\n.6\n.8\n1\n0 1\n4\n1\n2\n3\n4\n(1)\n0\n.2\n.4\n.6\n.8\n1Accuracy\n0\n.2\n.4\n.6\n.8\n1\n(e) (f) (g) (h)\nFigure 8: Accuracy after the I-FGSM attack with respect\nto θ(1). Transparency of the bars represents the ratio of the\nnumber of samples in each bin of θ(1). Red dashed lines in-\ndicate the overall accuracy after the attack.\ncurvedness of the representation space around the data (see\nAppendix). It is clear that the curvedness dichotomizes the\ndata: those associated with small values of θ(1) are located\nin linear regions (marked with yellowish colors), while those\nassociated with large values of θ(1) are located in curved re-\ngions (marked with greenish colors). In particular, the data in\nthe latter group show larger distances to the decision bound-\naries, and thus become more robust against adversarial at-\ntacks. In other words, since they are located in curved re-\ngions, an attack on them becomes challenging.\nTo validate this, we apply the iterative FGSM attack (I-\nFGSM) (Kurakin, Goodfellow, and Bengio 2017), which is\none of the strong attacks, to the correctly classified Ima-\ngeNet validation data. We set the maximum amount of per-\nturbation to ϵIFGSM=.001 or .002, the number of iterations to\nT=10, and the step size to ϵIFGSM/T. Fig. 8 shows the clas-\nsification accuracy after the attack with respect to θ(1). We\ncan observe that the data having large values of θ(1) show\nhigh robustness (i.e., high accuracy even after the attack),\nwhich makes the overall robustness of Transformers higher\nthan that of CNNs.\nWe hypothesize that the curved representation space also\ncauses the underconfident prediction of Transformers. That\nis, as shown in Fig. 4b, the decision boundary is actually\nclose to the data point (on the left side of the data), but the\ncurved travel (blue-colored trajectory in Fig. 4b) reaches the\ndecision boundary at a farther location. To validate this hy-\npothesis, we add a small amount of noise to the input data in\norder to check if the decision boundary at a closer location\ncan be found if the data jumps out of the curved region (i.e.,\nreaching zr(N) from zr(0) in Fig. 4b).\nFigs. 9a and 9b show the relationship of the distance to\ndecision boundaries for original outputs (x-axis) and jumped\nimages (y-axis) for Swin-T. The direction for travel is indi-\ncated in the axis. It can be observed that when the FGSM\ndirection is computed and used after random jump (d =\ndr+FGSM; yellow-colored trajectory in Fig. 4), the distance\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13146\n(a) ViT-B/16\n(b) Swin-T\nFigure 9: Relationship of the length of travel (ϵ) to decision\nboundaries (DB) for original images (x-axis) and jumped\nimages (y-axis). Colors indicate θ(1).\nis significantly reduced (left figures in Figs. 9a and 9b; most\ndata points under the 45◦ line). As shown in Figs. 5g and 5h,\nthe travel becomes less curved and thus the decision bound-\nary can be reached effectively. The 2D projected movements\nafter the jump in Fig. 16 in Appendix also supports this.\nFurthermore, the random jump can be made even more ef-\nfective by setting the jump direction to the FGSM direc-\ntion that would have been found if random jump was ap-\nplied (d = drFGSM+FGSM; red-colored trajectory in Fig. 4),\nresulting in further reduction in distance (right figures in\nFigs. 9a and 9b).\nThe reduced distance to the boundary by random jump\nimplies that the jumped input data can be made misclassified\nby adding a smaller amount of perturbation than the origi-\nnal input data. Fig. 10 demonstrates that this actually works.\nThe figure shows example images perturbed linearly in the\nFGSM direction (i.e., x(N)) and those first undergone ran-\ndom jump (ϵr=.05) and then perturbed linearly in the FGSM\ndirection (i.e., xr(N)) for Swin-T. It is clear that the images\nare easily misclassified with significantly reduced amounts\nof perturbation (smaller ϵ and higher PSNR) after the ran-\ndom jump passing over curved regions.\nNonlinearity of Attention Operation\nWhy do curves tend to appear in the representation space\nof Transformers only, and not in CNNs? In this section, we\nexplain this theoretically by revisiting convolution and self-\nattention operations. Note that we use matrices to denote in-\nputs and outputs instead of vectors for better explanation of\nthe operations. Empirical results of this section can be found\nin the next section and Table 1 in Appendix.\nDeep neural networks transform data points through con-\ntiguous blocks that perform similar operations. CNNs, for\ninstance, comprise layers of a convolution operation and ac-\ntivation. As well known, a convolution is a linear operation\n(Hayes 1996), i.e., an increment P to the input X converts\ninto the addition of separate responses:\nConv(X + P) =Conv(X) + Conv(P). (8)\nActivation functions may imbue the transformation with\nnonlinearity in theory, which is very limited in reality. Re-\nLUs are linear until the input data travels to the negative re-\ngion. In the case of sigmoid functions, the input data is sup-\nposed to linger in the non-saturated region, which is pseudo-\nlinear. Therefore, the main building block of CNNs is a lin-\near transformation.\nAt the heart of Transformers, an attention block trans-\nforms an input query into the weighted sum of neighborval-\nues, which is a linear projection of input tokens. The weights\nare calculated as softmax of attention scores A, which is an\ninner product of query and key:\nA(X) =XWqW⊤\nk X⊤, (9)\nAttn(X) =softmax(A/\np\nDk)XWv, (10)\nwhere Wq, Wk, and Wv are the projection heads for query,\nkey, and value, respectively, and Dk is the column dimen-\nsion of Wk. If X is moved by P, A will change as follows:\nA(X + P) =(X + P)WqW⊤\nk (X⊤ + P⊤) (11)\n=XWqW⊤\nk X⊤ + PWqW⊤\nk P⊤+\nXWqW⊤\nk P⊤ + PWqW⊤\nk X⊤ (12)\n=A(X) +A(P)+\nXWqW⊤\nk P⊤ + PWqW⊤\nk X⊤\n| {z }\nresidual\n. (13)\nAs shown in Eq. 13, the attention score is not linear and the\ndeviation from the linear response is the combination of the\nprojection heads and input data. During the inference oper-\nation, the projection heads are fixed and the linear pertur-\nbation to the input data will generate a varying degree of\nnonlinearity depending on the magnitude of the input and\nthe angle between the projection head and the input (see\nFig. 17 in Appendix for detailed discussion). Additionally,\nthe softmax function in Eq. 10 augments the nonlinearity of\nthe attention operation.\nAdditional Intriguing Observations\nIn this section, in addition to the aforementioned main dis-\ncoveries, we share our additional intriguing observations, for\nwhich we leave further detailed analysis as future work.\nContribution of Components to Curvedness\nWhich component in Transformers fortifies the curvedness\nof the representation space? When ResNet50 and Swin-T\nare compared (Table 1 in Appendix), we find that in both\nmodels the activation functions contribute the most to the\nincrease of θ(1). GELU causes curvedness more than ReLU\nbecause the former is more nonlinear than the latter. In the\ncase of ResNet50, the convolutional layers and batch nor-\nmalization (BatchNorm) do not cause curvedness of the rep-\nresentation space. In contrast, for Swin-T, the layer normal-\nization (LayerNorm) and self-attention layers intensify the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13147\nFigure 10: Example images that are perturbed by FGSM so as to reach decision boundaries and become misclassified. The total\namount of perturbation (ϵ) and the peak signal-to-noise ratio (PSNR) in dB are also shown. Top: Perturbed images. Bottom:\nImages perturbed after random jump (ϵr = .05).\ncurvedness. The result of these compound contributions of\ndifferent components appears as the curvedness of the rep-\nresentation space of Transformers.\nThis observation raises an interesting question about Con-\nvNeXt (Liu et al. 2022b), which is a CNN but uses GELU\nand LayerNorm instead of ReLU and BatchNorm, respec-\ntively: Which trend will it follow, CNNs or Transform-\ners? Surprisingly, we observe that ConvNeXt-Tiny follows\nthe trend of Transformers, rather than CNNs (Table 1 and\nFigs. 18a and 19a in Appendix). This indicates that the\ncurvedness in the representation space highly depends on\nthe particular components used in models, and is not just a\nproblem of models being CNNs or Transformers.\nKnowledge Distillation and Curvedness\nAnother interesting model we find is DeiT-Ti (Touvron et al.\n2021), a convolution-free Transformer, and its distilled ver-\nsion, which we refer to as DeiT-Ti-Distilled. We observe\nthat as expected, DeiT-Ti follows the trend of Transformers,\ni.e., underconfidence with high robustness (Figs. 18b and\n19b in Appendix). However, DeiT-Ti-Distilled, knowledge-\ndistilled DeiT-Ti with a CNN teacher, tends to follow the\ntrend of CNNs, i.e., overconfidence with low robustness\n(Figs. 18c and 19c in Appendix). The results in Table 1 also\ncoincides with this observation, where the values of θ(1) are\nreduced for DeiT-Ti-Distilled compared to DeiT-Ti. This in-\ndicates that knowledge distillation can also affect the non-\nlinearity of Transformers.\nCurved Space During Training\nFor deeper understanding of the curved regions in the repre-\nsentation space, we look into the training process of Trans-\nformers. We observe that for the data located in curved re-\ngions, the loss does not change much from the early train-\ning stage (Fig. 20 in Appendix; no change in loss for bot-\ntom rows in the figure, which show large values of θ(1)).\nThis phenomenon can also be observed from another view,\nin terms of the relationship between the loss at a certain\nepoch and the loss change from the epoch until the end of\ntraining (Fig. 21 in Appendix; loss values for the data re-\nsiding in curved regions - dark-colored points in the figure -\nare hardly reduced already from 30 epochs). In other words,\ncertain training data seem to be trapped in curved regions,\nwhich obstructs the training of the network.\nWhen do curved regions start to form? When we check the\nrelationship of θ(1) at a certain training stage and θ(1) after\ntraining, we observe that once a data is trapped in a curved\nregion, it hardly escapes the region and θ(1) becomes larger\nduring training, i.e., the curvedness gets severer (Fig. 22 in\nAppendix; data points mostly above the 45◦ line).\nConclusion\nWe studied the input-output relationship of Transformers by\nexamining the trajectory of the output in the representation\nspace with respect to linear movements in the input space.\nThe experimental results indicated that the representation\nspace of Transformers is curved around some data, which\nexplains high robustness and underconfident prediction of\nTransformers.\nFuture Work\nIn general, understanding the behavior of a certain neural\nnetwork model, either analytically or empirically, is a dif-\nficult task, which cannot be accomplished by a single pa-\nper but requires a lot of research efforts. We have focused\non the input-output relationship along the adversarial direc-\ntion generated by FGSM, which revealed the existence of\ncurvedness in the representation space of Transformers. We\nbelieve that we have opened a new perspective of under-\nstanding Transformers, and many derivative research ques-\ntions will naturally follow, e.g., consideration of different\ntravel directions, input-output relationship of various build-\ning blocks of neural networks, effects of different training\nrecipes, effects of training datasets, etc., which we leave as\nfuture works. It is also our hope that our work promotes fur-\nther interesting research topics (e.g., ways to reduce/inten-\nsify curvedness during or after training, measures of local/-\nglobal curvedness, theoretical analysis of curvedness, etc.)\nand applications (e.g., effective adversarial attacks consider-\ning curvedness, robust model architectures, robust training\nmethods, etc.).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13148\nAcknowledgements\nThis work was supported in part by the fund (IITP 2022-\n0-00117) from the Korea government, MSIT, and in part\nby the NRF grant funded by the Korean government, MSIT\n(2021R1A2C2011474).\nReferences\nAldahdooh, A.; Hamidouche, W.; and Deforges, O. 2021.\nReveal of vision transformers robustness against adversarial\nattacks. arXiv preprint arXiv:2106.03734.\nBai, Y .; Mei, J.; Yuille, A.; and Xie, C. 2021. Are transform-\ners more robust than CNNs? In NIPS.\nBenz, P.; Ham, S.; Zhang, C.; Karjauv, A.; and Kweon, I. S.\n2021. Adversarial robustness comparison of vision trans-\nformer and MLP-Mixer to CNNs. In Proceedings of the\nBritish Machine Vision Conference (BMVC).\nBhojanapalli, S.; Chakrabarti, A.; Glasner, D.; Li, D.; Un-\nterthiner, T.; and Veit, A. 2021. Understanding robustness\nof transformers for image classification. In ICCV.\nChu, X.; Tian, Z.; Wang, Y .; Zhang, B.; Ren, H.; Wei, X.;\nXia, H.; and Shen, C. 2021. Twins: Revisiting the design of\nspatial attention in vision transformers. In NIPS.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn image is worth 16x16 words: Transformers for image\nrecognition at scale. In ICLR.\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Explain-\ning and harnessing adversarial examples. In ICLR.\nGuo, C.; Pleiss, G.; Sun, Y .; and Weinberger, K. Q. 2017.\nOn calibration of modern meural networks. In ICML.\nHanin, B.; Jeong, R.; and Rolnick, D. 2022. Deep ReLU\nnetworks preserve expected length. In ICLR.\nHanin, B.; and Rolnick, D. 2019a. Complexity of linear re-\ngions in deep networks. In ICML.\nHanin, B.; and Rolnick, D. 2019b. Deep ReLU networks\nhave surprisingly few activation patterns. In NIPS.\nHayes, M. H. 1996. Statistical digital signal processing and\nmodeling. John Wiley & Sons, Inc., 1st edition.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR.\nHoward, A.; Sandler, M.; Chu, G.; Chen, L.-C.; Chen, B.;\nTan, M.; Wang, W.; Zhu, Y .; Pang, R.; Vasudevan, V .; Le,\nQ. V .; and Adam, H. 2019. Searching for MobileNetV3. In\nICCV.\nHuang, Z.; Ben, Y .; Luo, G.; Cheng, P.; Yu, G.; and Fu, B.\n2021. Shuffle transformer: Rethinking spatial shuffle for vi-\nsion transformer. arXiv preprint arXiv:2106.03650.\nKurakin, A.; Goodfellow, I. J.; and Bengio, S. 2017. Adver-\nsarial machine learning at scale. In ICLR.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021. LocalViT: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707.\nLiu, Z.; Hu, H.; Lin, Y .; Yao, Z.; Xie, Z.; Wei, Y .; Ning, J.;\nCao, Y .; Zhang, Z.; Dong, L.; et al. 2022a. Swin transformer\nv2: Scaling up capacity and resolution. In CVPR.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV.\nLiu, Z.; Mao, H.; Wu, C.-Y .; Feichtenhofer, C.; Darrell, T.;\nand Xie, S. 2022b. A ConvNet for the 2020s. In CVPR.\nMinderer, M.; Djolonga, J.; Romijnders, R.; Hubis, F.; Zhai,\nX.; Houlsby, N.; Tran, D.; and Lucic, M. 2021. Revisiting\nthe calibration of modern neural networks. In NIPS.\nMontufar, G. F.; Pascanu, R.; Cho, K.; and Bengio, Y . 2014.\nOn the number of linear regions of deep neural networks. In\nNIPS.\nNaeini, M. P.; Cooper, G.; and Hauskrecht, M. 2015. Ob-\ntaining well calibrated probabilities using Bayesian binning.\nIn AAAI.\nNaseer, M.; Ranasinghe, K.; Khan, S.; Hayat, M.; Khan, F.;\nand Yang, M.-H. 2021. Intriguing properties of vision trans-\nformers. In NIPS.\nNovak, R.; Bahri, Y .; Abolafia, D. A.; Pennington, J.; and\nSohl-Dickstein, J. 2018. Sensitivity and generalization in\nneural networks: An empirical study. In ICLR.\nPark, N.; and Kim, S. 2022. How do vision transformers\nwork? In ICLR.\nPaul, S.; and Chen, P.-Y . 2022. Vision transformers are ro-\nbust learners. In AAAI.\nPrice, I.; and Tanner, J. 2021. Trajectory growth lower\nbounds for random sparse deep ReLU networks. In ICML.\nRaghu, M.; Poole, B.; Kleinberg, J.; Ganguli, S.; and Sohl-\nDickstein, J. 2017. On the expressive power of deep neural\nnetworks. In ICML.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\nBerg, A. C.; ; and Fei-Fei, L. 2015. ImageNet large scale\nvisual recognition challenge. IJCV, 115(3): 211–252.\nSandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; and\nChen, L.-C. 2018. MobileNetV2: Inverted residuals and lin-\near bottlenecks. In CVPR.\nSerra, T.; Tjandraatmadja, C.; and Ramalingam, S. 2018.\nBounding and counting linear regions of deep neural net-\nworks. In ICML.\nSokoli´c, J.; Giryes, R.; Sapiro, G.; and Rodrigues, M. R.\n2017. Robust large margin deep neural networks. IEEE\nTransactions on Signal Processing.\nTelgarsky, M. 2015. Representation benefits of deep feed-\nforward networks. arXiv preprint arXiv:1509.08101.\nThulasidasan, S.; Chennupati, G.; Bilmes, J. A.; Bhat-\ntacharya, T.; and Michalak, S. 2019. On mixup training: Im-\nproved calibration and predictive uncertainty for deep neural\nnetworks. In NIPS.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In ICML.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13149\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In ICCV.\nWen, Y .; Jerfel, G.; Muller, R.; Dusenberry, M. W.; Snoek,\nJ.; Lakshminarayanan, B.; and Tran, D. 2021. Combining\nensembles and data augmentation can harm your calibration.\nIn ICLR.\nXiao, T.; Singh, M.; Mintun, E.; Darrell, T.; Doll ´ar, P.; and\nGirshick, R. 2021. Early convolutions help transformers see\nbetter. In NIPS.\nYang, J.; Li, C.; Zhang, P.; Dai, X.; Xiao, B.; Yuan, L.; and\nGao, J. 2021. Focal attention for long-range interactions in\nvision transformers. In NIPS.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.-H.;\nTay, F. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token ViT:\nTraining vision transformers from scratch on ImageNet. In\nICCV.\nZhang, X.; and Wu, D. 2020. Empirical studies on the prop-\nerties of linear regions in deep neural networks. In ICLR.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13150",
  "topic": "Computer vision",
  "concepts": [
    {
      "name": "Computer vision",
      "score": 0.5691086649894714
    },
    {
      "name": "Transformer",
      "score": 0.5126637816429138
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4627307653427124
    },
    {
      "name": "Representation (politics)",
      "score": 0.4621160328388214
    },
    {
      "name": "Computer science",
      "score": 0.4568321406841278
    },
    {
      "name": "Engineering",
      "score": 0.1786651909351349
    },
    {
      "name": "Electrical engineering",
      "score": 0.13243865966796875
    },
    {
      "name": "Political science",
      "score": 0.09270471334457397
    },
    {
      "name": "Voltage",
      "score": 0.08897402882575989
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}