{
  "title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong\\n Baselines for Grammar Induction",
  "url": "https://openalex.org/W3004117589",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2550451402",
      "name": "Kim, Taeuk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1705230916",
      "name": "Choi Jihun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3014486829",
      "name": "Edmiston, Daniel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2525180176",
      "name": "Lee, Sang-goo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2949399644",
    "https://openalex.org/W2097606805",
    "https://openalex.org/W2963451457",
    "https://openalex.org/W2955572395",
    "https://openalex.org/W2889260178",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2967659330",
    "https://openalex.org/W2791751435",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2619818172",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2963522904",
    "https://openalex.org/W2953130735",
    "https://openalex.org/W2932637973",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2949579048",
    "https://openalex.org/W2153568660",
    "https://openalex.org/W2143331230",
    "https://openalex.org/W2963580443",
    "https://openalex.org/W1978470410",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W2113075298",
    "https://openalex.org/W2799253188",
    "https://openalex.org/W2129882630",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2973723395",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2963411763",
    "https://openalex.org/W2949794029",
    "https://openalex.org/W2798569372",
    "https://openalex.org/W2963393721",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2963073938"
  ],
  "abstract": "With the recent success and popularity of pre-trained language models (LMs)\\nin natural language processing, there has been a rise in efforts to understand\\ntheir inner workings. In line with such interest, we propose a novel method\\nthat assists us in investigating the extent to which pre-trained LMs capture\\nthe syntactic notion of constituency. Our method provides an effective way of\\nextracting constituency trees from the pre-trained LMs without training. In\\naddition, we report intriguing findings in the induced trees, including the\\nfact that pre-trained LMs outperform other approaches in correctly demarcating\\nadverb phrases in sentences.\\n",
  "full_text": "Published as a conference paper at ICLR 2020\nARE PRE-TRAINED LANGUAGE MODELS AWARE OF\nPHRASES ? S IMPLE BUT STRONG BASELINES FOR\nGRAMMAR INDUCTION\nTaeuk Kim1, Jihun Choi1, Daniel Edmiston2 & Sang-goo Lee1\n1Dept. of Computer Science and Engineering, Seoul National University, Seoul, Korea\n2Dept. of Linguistics, University of Chicago, Chicago, IL, USA\n{taeuk,jhchoi,sglee}@europa.snu.ac.kr, danedmiston@uchicago.edu\nABSTRACT\nWith the recent success and popularity of pre-trained language models (LMs) in\nnatural language processing, there has been a rise in efforts to understand their in-\nner workings. In line with such interest, we propose a novel method that assists us\nin investigating the extent to which pre-trained LMs capture the syntactic notion\nof constituency. Our method provides an effective way of extracting constituency\ntrees from the pre-trained LMs without training. In addition, we report intriguing\nﬁndings in the induced trees, including the fact that some pre-trained LMs outper-\nform other approaches in correctly demarcating adverb phrases in sentences.\n1 I NTRODUCTION\nGrammar induction, which is closely related to unsupervised parsing and latent tree learning, allows\none to associate syntactic trees, i.e., constituency and dependency trees, with sentences. As gram-\nmar induction essentially assumes no supervision from gold-standard syntactic trees, the existing\napproaches for this task mainly rely on unsupervised objectives, such as language modeling (Shen\net al., 2018b; 2019; Kim et al., 2019a;b) and cloze-style word prediction (Drozdov et al., 2019) to\ntrain their task-oriented models. On the other hand, there is a trend in the natural language pro-\ncessing (NLP) community of leveraging pre-trained language models (LMs), e.g., ELMo (Peters\net al., 2018) and BERT (Devlin et al., 2019), as a means of acquiring contextualized word represen-\ntations. These representations have proven to be surprisingly effective, playing key roles in recent\nimprovements in various models for diverse NLP tasks.\nIn this paper, inspired by the fact that the training objectives of both the approaches for grammar\ninduction and for training LMs are identical, namely, (masked) language modeling, we investigate\nwhether pre-trained LMs can also be utilized for grammar induction/unsupervised parsing, espe-\ncially without training. Speciﬁcally, we focus on extracting constituency trees from pre-trained LMs\nwithout ﬁne-tuning or introducing another task-speciﬁc module, at least one of which is usually\nrequired in other cases where representations from pre-trained LMs are employed. This restriction\nprovides us with some advantages: (1) it enables us to derive strong baselines for grammar induc-\ntion with reduced time and space complexity, offering a chance to reexamine the current status of\nexisting grammar induction methods, (2) it facilitates an analysis on how much and what kind of\nsyntactic information each pre-trained LM contains in its intermediate representations and atten-\ntion distributions in terms of phrase-structure grammar, and (3) it allows us to easily inject biases\ninto our framework, for instance, to encourage the right-skewness of the induced trees, resulting in\nperformance gains in English unsupervised parsing.\nFirst, we brieﬂy mention related work (§2). Then, we introduce the intuition behind our proposal in\ndetail (§3), which is motivated by our observation that we can cluster words in a sentence according\nto the similarity of their attention distributions over words in the sentence. Based on this intuition, we\ndeﬁne a straightforward yet effective method ( §4) of drawing constituency trees directly from pre-\ntrained LMs with no ﬁne-tuning or addition of task-speciﬁc parts, instead resorting to the concept\nof Syntactic Distance (Shen et al., 2018a;b). Then, we conduct experiments ( §5) on the induced\nconstituency trees, discovering some intriguing phenomena. Moreover, we analyze the pre-trained\n1\narXiv:2002.00737v1  [cs.CL]  30 Jan 2020\nPublished as a conference paper at ICLR 2020\nLMs and constituency trees from various points of view, including looking into which layer(s) of\nthe LMs is considered to be sensitive to phrase information (§6).\nTo summarize, our contributions in this work are as follows:\n• By investigating the attention distributions from Transformer-based pre-trained LMs, we\nshow that there is evidence to suggest that several attention heads of the LMs exhibit syn-\ntactic structure akin to constituency grammar.\n• Inspired by the above observation, we propose a method that facilitates the derivation of\nconstituency trees from pre-trained LMs without training. We also demonstrate that the\ninduced trees can serve as a strong baseline for English grammar induction.\n• We inspect, in view of our framework, what type of syntactic knowledge the pre-trained\nLMs capture, discovering interesting facts, e.g., that some pre-trained LMs are more aware\nof adverb phrases than other approaches.\n2 R ELATED WORK\nGrammar induction is a task whose goal is to infer from sequential data grammars which generalize,\nand are able to account for unseen data (Lari & Young (1990); Clark (2001); Klein & Manning\n(2002; 2004), to name a few). Traditionally, this was done by learning explicit grammar rules (e.g.,\ncontext free rewrite rules), though more recent methods employ neural networks to learn such rules\nimplicitly, focusing more on the induced grammars’ ability to generate or parse sequences.\nSpeciﬁcally, Shen et al. (2018b) proposed Parsing-Reading-Predict Network (PRPN) where the con-\ncept of Syntactic Distance is ﬁrst introduced. They devised a neural model for language modeling\nwhere the model is encouraged to recognize syntactic structure. The authors also probed the possi-\nbility of inducing constituency trees without access to gold-standard trees by adopting an algorithm\nthat recursively splits a sequence of words into two parts, the split point being determined according\nto correlated syntactic distances; the point having the biggest distance becomes the ﬁrst target of\ndivision. Shen et al. (2019) presented a model called Ordered Neurons (ON), which is a revised\nversion of LSTM (Long Short-Term Memory, Hochreiter & Schmidhuber (1997)) which reﬂects the\nhierarchical biases of natural language and can be used to compute syntactic distances. Shen et al.\n(2018a) trained a supervised parser relying on the concept of syntactic distance.\nOther studies include Drozdov et al. (2019), who trained deep inside-outside recursive autoencoders\n(DIORA) to derive syntactic trees in an exhaustive way with the aid of the inside-outside algorithm,\nand Kim et al. (2019a) who proposed Compound Probabilistic Context-Free Grammars (compound\nPCFG), showing that neural PCFG models are capable of producing promising unsupervised parsing\nresults. Li et al. (2019) proved that an ensemble of unsupervised parsing models can be beneﬁcial,\nwhile Shi et al. (2019) utilized additional training signals from pictures related with input text. Dyer\net al. (2016) proposed Recurrent Neural Network Grammars (RNNG) for both language modeling\nand parsing, and Kim et al. (2019b) suggested an unsupervised variant of the RNNG. There also\nexists another line of research on task-speciﬁc latent tree learning (Yogatama et al., 2017; Choi et al.,\n2018; Havrylov et al., 2019; Maillard et al., 2019). The goal here is not to construct linguistically\nplausible trees, but to induce trees ﬁtted to improving target performance. Naturally, the induced\nperformance-based trees need not resemble linguistically plausible trees, and some studies (Williams\net al., 2018a; Nangia & Bowman, 2018) examined the apparent fact that performance-based and\nlingusitically plausible trees bear little resemblance to one another.\nConcerning pre-trained language models (Peters et al. (2018); Devlin et al. (2019); Radford et al.\n(2019); Yang et al. (2019); Liu et al. (2019b), inter alia)—particularly those employing a Trans-\nformer architecture (Vaswani et al., 2017)—these have proven to be helpful for diverse NLP down-\nstream tasks. In spite of this, there is no vivid picture for explaining what particular factors contribute\nto performance gains, even though some recent work has attempted to shed light on this question. In\ndetail, one group of studies (Raganato & Tiedemann (2018); Clark et al. (2019); Ethayarajh (2019);\nHao et al. (2019); V oita et al. (2019),inter alia) has focused on dissecting the intermediate represen-\ntations and attention distributions of the pre-trained LMs, while the another group of publications\n(Mareˇcek & Rosa (2018); Goldberg (2019); Hewitt & Manning (2019); Liu et al. (2019a); Rosa &\nMareˇcek (2019), to name a few) delve into the question of the existence of syntactic knowledge in\n2\nPublished as a conference paper at ICLR 2020\nFigure 1: Self-attention heatmaps from two different pre-trained LMs. (Left) A heatmap for the\naverage of attention distributions from the 7th layer of the XLNet-base (Yang et al., 2019) model\ngiven the sample sentence. (Right) A heatmap for the average of attention distributions from the 9th\nlayer of the BERT-base (Devlin et al., 2019) model given another sample sentence. We can easily\nspot the chunks of words on the two heatmaps that are correlated with the constituents of the input\nsentences, e.g., (Left) ‘the price of plastics’, ‘took off in 1987’, ‘Quantum Chemical Corp.’, (Right)\n‘when price increases can be sustained’, and ‘he remarks’.\nTransformer-based models. Particularly, Mareˇcek & Rosa (2019) proposed an algorithm for extract-\ning constituency trees from Transformers trained for machine translation, which is similar to our\napproach.\n3 M OTIVATION\nAs pioneers in the literature have pointed out, the multi-head self-attention mechanism (Vaswani\net al., 2017) is a key component in Transformer-based language models, and it seems this mecha-\nnism empowers the models to capture certain semantic and syntactic information existing in natural\nlanguage. Among a diverse set of knowledge they may capture, in this work we concentrate on\nphrase-structure grammar by seeking to extract constituency trees directly from their attention in-\nformation and intermediate weights.\nIn preliminary experiments, where we manually visualize and investigate the intermediate repre-\nsentations and attention distributions of several pre-trained LMs given input, we have found some\nevidence which suggests that the pre-trained LMs exhibit syntactic structure akin to constituency\ngrammar in some degree. Speciﬁcally, we have noticed some patterns which are often displayed\nin self-attention heatmaps as explicit horizontal lines, or groups of rectangles of various sizes. As\nan attention distribution of a word in an input sentence corresponds to a row in a heatmap matrix,\nwe can say that the appearance of these patterns indicates the existence of groups of words where\nthe attention distributions of the words in the same group are relatively similar. Interestingly, we\nhave also discovered the fact that the groups of words we observed are fairly correlated with the\nconstituents of the input sentence, as shown in Figure 1 (above) and Figure 3 (in Appendix A.1).\nEven though we have identiﬁed some patterns which match with the constituents of sentences, it is\nnot enough to conclude that the pre-trained LMs are aware of syntactic phrases as found in phrase-\nstructure grammars. To demonstrate the claim, we attempt to obtain constituency trees in a zero-shot\nlearning fashion, relying only on the knowledge from the pre-trained LMs. To this end, we suggest\nthe following, inspired from our ﬁnding: two words in a sentence are syntactically close to each\nother (i.e., the two words belong to the same constituent) if their attention distributions over words\nin the sentence are also close to each other. Note that this implicitly presumes that each word is\nmore likely to attend more on the words in the same constituent to enrich its representation in the\npre-trained LMs. Finally, we utilize the assumption to compute syntactic distances between each\npair of adjacent words in a sentence, from which the corresponding constituency tree can be built.\n3\nPublished as a conference paper at ICLR 2020\n4 P ROPOSED METHOD\n4.1 S YNTACTIC DISTANCE AND TREE CONSTRUCTION\nWe leverage the concept of Syntactic Distance proposed by Shen et al. (2018a;b) to draw con-\nstituency trees from raw sentences in an intuitive way. Formally, given a sequence of words in a\nsentence, w1,w2,...,w n, we compute d = [d1,d2,...,d n−1] where di corresponds to the syntac-\ntic distance between wi and wi+1. Each di is deﬁned as follows:\ndi = f(g(wi),g(wi+1)), (1)\nwhere f(·,·) and g(·) are a distance measure function and representation extractor function, respec-\ntively. The function g converts each word into the corresponding vector representation, while f\ncomputes the syntactic distance between the two words given their representations. Once d is de-\nrived, it can be easily converted into the target constituency tree by a simple algorithm following\nShen et al. (2018a).1 For details of the algorithm, we refer the reader to Appendix A.2.\nAlthough previous studies attempted to explicitly train the functions f and gwith supervision (with\naccess to gold-standard trees, Shen et al. (2018a)) or to obtain them as a by-product of training\nparticular models that are carefully designed to recognize syntactic information (Shen et al., 2018b;\n2019), in this work we stick to simple distance metric functions forf and pre-trained LMs for g, for-\ngoing any training process. In other words, we focus on investigating the possibility of pre-trained\nLMs possessing constituency information in a form that can be readily extracted with straightfor-\nward computations. If the trees induced by the syntactic distances derived from the pre-trained LMs\nare similar enough to gold-standard syntax trees, we can reasonably claim that the LMs resemble\nphrase-structure.\n4.2 P RE-TRAINED LANGUAGE MODELS\nWe consider four types of recently proposed language models. These are: BERT (Devlin et al.,\n2019), GPT-2 (Radford et al., 2019), RoBERTa (Liu et al., 2019b), and XLNet (Yang et al., 2019).\nThey all have in common that they are based on the Transformer architecture and have been proven\nto be effective in natural language understanding (Wang et al., 2019) or generation. We handle two\nvariants for each LM, varying in the number of layers, attention heads, and hidden dimensions,\nresulting in eight different cases in total. In particular, each LM has two variants. (1) base: consists\nof l=12 layers, a=12 attention heads, andd=768 hidden dimensions, while (2) large: hasl=24 layers,\na=16 attention heads, and d=1024 hidden dimensions. 2 We deal with a wide range of pre-trained\nLMs, unlike previous work which has mostly analyzed a speciﬁc model, particularly BERT. For\ndetails about each LM, we refer readers to the respective original papers.\nIn terms of our formulation, each LM instance provides two categories of representation extractor\nfunctions, Gv and Gd. Speciﬁcally, Gv refers to a set of functions {gv\nj|j = 1,...,l }, each of which\nsimply outputs the intermediate hidden representation of a given word on the jth layer of the LM.\nLikewise, Gd is a set of functions {gd\n(j,k)|j = 1,...,l,k = 1,...,a + 1}, each of which outputs\nthe attention distribution of an input word by the kth attention head on the jth layer of the LM.\nEven though our main motivation comes from the self-attention mechanism, we also deal with the\nintermediate hidden representations present in the pre-trained LMs by introducing Gv, considering\nthat the hidden representations serve as storage of collective information taken from the processing\nof the pre-trained LMs. Note that kranges up to a+ 1, not a, implying that we consider the average\nof all attention distributions on the same layer in addition to the individual ones. This averaging\nfunction can be regarded as an ensemble of other functions in the layer which are specialized for\ndifferent aspects of information, and we expect that this technique will provide a better option in\nsome cases as reported in previous work (Li et al., 2019).\n1 Our parsing algorithm is an unbiased method in contrast to one (named as COO parser by Dyer et al.\n(2019)) employed in most previous studies (Shen et al., 2018b; 2019; Htut et al., 2018; Li et al., 2019; Shi\net al., 2019). This choice enables us to investigate the exact extent to which pre-trained LMs contribute to their\nperformance on unsupervised parsing, considering the fact revealed recently by Dyer et al. (2019) that theCOO\nparser has potential issues that it prefers right-branching trees and does not cover all possible tree derivations.\nFurthermore, we can directly adjust the right-branching bias using our method in Section 4.4 if needed.\n2In case of GPT-2, ‘GPT2’ corresponds to the ‘base’ variant while ‘GPT2-medium’ to the ‘large’ one.\n4\nPublished as a conference paper at ICLR 2020\nOne remaining issue is that all the pre-trained LMs we use regard each input sentence as a sequence\nof subword tokens, while our formulation assumes words cannot be further divided into smaller\ntokens. To resolve this difference, we tested certain heuristics that guide how subword tokens for\na complete word should be exploited to represent the word, and we have empirically found that\nthe best result comes when each word is represented by an average of the representations of its\nsubwords.3 Therefore, we adopt the above heuristic in this work for cases where a word is tokenized\ninto more than two parts.\n4.3 D ISTANCE MEASURE FUNCTIONS\nFor the distance measure function f, we prepare three options (Fv) for Gv and two options (Fd) for\nGd. Formally, f ∈Fv∪Fd, where Fv = {COS,L1,L2}, Fd = {JSD ,HEL }. COS, L1, L2, JSD,\nand HEL correspond to Cosine, L1, and L2, Jensen-Shannon, and Hellinger distance respectively.\nThe functions in Fv are only compatible with the elements of Gv, and the same holds for Fd and\nGd. The exact deﬁnition of each function is listed in Appendix A.3.\n4.4 I NJECTING BIAS INTO SYNTACTIC DISTANCES\nOne of the main advantages we obtain by leveraging syntactic distances to derive parse trees is that\nwe can easily inject inductive bias into our framework by simply modifying the values of the syn-\ntactic distances. Hence, we investigate whether the extracted trees from our method can be further\nreﬁned with the aid of additional biases. To this end, we introduce a well-known bias for English\nconstituency trees—the right-skewness bias—in a simple linear form. 4 Namely, our intention is\nto inﬂuence the induced trees such that they are moderately right-skewed following the nature of\ngold-standard parse trees in English.\nFormally, we compute ˆdi by appending the following linear bias term to every di:\nˆdi = di + λ·AVG(d) ×(1 −1/(m−1) ×(i−1)), (2)\nwhere AVG(·) outputs an average of all elements in a vector, λis a hyperparameter, and iranges\nfrom 1 to m = n−1. We write ˆd = [ˆd1, ˆd2,..., ˆdm] in place of d to signify biased syntactic\ndistances.\nThe main purpose of introducing such a bias is examining what changes are made to the resulting\ntree structures rather than boosting quantitative performanceper se, though it is of note that it serves\nthis purpose as well. We believe that this additional consideration is necessary based on two points.\nFirst, English is what is known as a head-initial language. That is, given a selector and argument,\nthe selector has a strong tendency to appear on the left, e.g., ‘eat food’, or ‘to Canada’. Head-initial\nlanguages therefore have an in-built preference for right-branching structures. By adjusting the bias\ninjected into syntactic distances derived from pre-trained LMs, we can ﬁgure out whether the LMs\nare capable of inducing the right-branching bias, which is one of the main properties of English\nsyntax; if injecting the bias does not inﬂuence the performance of the LMs on unsupervised parsing,\nwe can conjecture they are inherently capturing the bias to some extent. Second, as mentioned\nbefore, we have witnessed some previous work (Shen et al., 2018b; 2019; Htut et al., 2018; Li et al.,\n2019; Shi et al., 2019) where the right-skewness bias is implicitly exploited, although it could be\nregarded as not ideal. What we intend to focus on is the question about which beneﬁts the bias\nprovides for such parsing models, leading to overall performance improvements. In other words, we\nlook for what the exact contribution of the bias is when it is injected into grammar induction models,\nby explicitly controlling the bias using our framework.\n3We also tried other heuristics following previous work (Kitaev & Klein, 2018), e.g., using the ﬁrst or last\nsubword of a word as representative, but this led to no performance gains.\n4It is necessary to carefully design biases for other languages as they have their own properties.\n5\nPublished as a conference paper at ICLR 2020\n5 E XPERIMENTS\n5.1 G ENERAL SETTINGS\n5.1.1 D ATASETS\nIn this section, we conduct unsupervised constituency parsing on two datasets. The ﬁrst dataset is\nWSJ Penn Treebank (PTB, Marcus et al. (1993)), in which human-annotated gold-standard trees are\navailable. We use the standard split of the dataset—2-21 for training, 22 for validation, and 23 for\ntest. The second one is MNLI (Williams et al., 2018b), which is originally designed to test natural\nlanguage inference but often utilized as a means of evaluating parsers. It contains constituency trees\nproduced by an external parser (Klein & Manning, 2003). We leverage the union of two different\nversions of the MNLI development set as test data following convention (Htut et al., 2018; Drozdov\net al., 2019), and we call it the MNLI test set in this paper. Moreover, we randomly sample 40K\nsentences from the training set of the MNLI to utilize them as a validation set. To preprocess the\ndatasets, we follow the setting of Kim et al. (2019a) with the minor exceptions that words are not\nlower-cased and number characters are preserved instead of being substituted by a special character.\n5.1.2 I MPLEMENTATION DETAILS\nFor implementation, to compare pre-trained LMs in an uniﬁed manner, we resort to an integrated\nPyTorch codebase that supports all the models we consider.5 For each LM, we tune the best combi-\nnation of f and gfunctions using the validation set. Then, we derive a set of d for sentences in the\ntest set using the chosen functions, followed by the resulting constituency trees converted from each\nd by the tree construction algorithm in Section 4.1. In addition to sentence-level F1 (S-F1) score,\nwe report label recall scores for six main categories: SBAR, NP, VP, PP, ADJP, and ADVP. We also\npresent the results of utilizing ˆd instead of d, empirically setting the bias hyperparameter λas 1.5.\nWe do not ﬁne-tune the LMs on domain-speciﬁc data, as we here focus on ﬁnding their universal\ncharacteristics.\nWe take four na¨ıve baselines into account, random (averaged over 5 trials), balanced, left-branching,\nand right-branching binary trees. In addition, we present two more baselines which are identical to\nour models except that their g functions are based on a randomly initialized XLNet-base rather\nthan pre-trained ones. To be concrete, We provide ‘Random XLNet-base ( Fv)’ which applies the\nfunctions in Fv on random hidden representations and ‘Random XLNet-base (Fd)’ that utilizes the\nfunctions in Fd and random attention distributions, respectively. Considering the randomness of\ninitialization and possible choices for f, the ﬁnal score for each of the baselines is calculated as an\naverage over 5 trials of each possible f, i.e., an average over 5 ×3 runs in case of Fv and 5 ×2\nruns for Fd. These baselines enable us to estimate the exact advantage we obtain by pre-training\nLMs, effectively removing additional unexpected gains that may exist. Furthermore, we compare\nour parse trees against ones from existing grammar induction models.\nAll scripts used in our experiments will be publicly available for reproduction and further analysis.6\n5.2 E XPERIMENTAL RESULTS ON PTB\nIn Table 1, we report the results of the various models on the PTB test set. First of all, our method\ncombined with pre-trained LMs shows competitive or comparable results in terms of S-F1 even\nwithout the right-skewness bias. This result implies that the extracted trees from our method can be\nregarded as a baseline for English grammar induction. Moreover, pre-trained LMs show substan-\ntial improvements over Random Transformers (XLNet-base), demonstrating that training language\nmodels on large corpora, in fact, enables the LMs to be more aware of syntactic information.\nWhen the right-skewness bias is applied to syntactic distances derived from pre-trained LMs, the\nS-F1 scores of the LMs increase by up to ten percentage points. This improvement indicates that\nthe pre-trained LMs do not properly capture the largely right-branching nature of English syntax, at\nleast when observed through the lens of our framework. By explicitly controlling the bias through\nour framework and observing the performance gap between our models with and without the bias,\n5https://github.com/huggingface/transformers\n6https://github.com/galsang/trees_from_transformers\n6\nPublished as a conference paper at ICLR 2020\nTable 1: Results on the PTB test set. Bold numbers correspond to the top 3 results for each column.\nL: layer number, A: attention head number (A VG: the average of all attentions).†: Results reported\nby Kim et al. (2019a). ‡: Approaches in which COO parser is utilized.\nModel f L A S-F1 SBAR NP VP PP ADJP ADVP\nBaselines\nRandom Trees - - - 18.1 8% 23% 12% 18% 23% 28%\nBalanced Trees - - - 18.5 7% 27% 8% 18% 27% 25%\nLeft Branching Trees - - - 8.7 5% 11% 0% 5% 2% 8%\nRight Branching Trees - - - 39.4 68% 24% 71% 42% 27% 38%\nRandom XLNet-base (Fv) - - - 19.6 9% 26% 12% 20% 23% 24%\nRandom XLNet-base (Fd) - - - 20.1 11% 25% 14% 19% 22% 26%\nPre-trained LMs (w/o bias)\nBERT-base JSD 9 A VG 32.4 28% 42% 28% 31% 35% 63%\nBERT-large HEL 17 A VG 34.2 34% 43% 27% 39% 37% 57%\nGPT2 JSD 9 1 37.1 32% 47% 27% 55% 27% 36%\nGPT2-medium JSD 10 13 39.4 41% 51% 21% 67% 33% 44%\nRoBERTa-base JSD 9 4 33.8 40% 38% 33% 43% 42% 57%\nRoBERTa-large JSD 14 5 34.1 29% 46% 30% 37% 28% 40%\nXLNet-base HEL 9 A VG 40.1 35% 56% 26% 38% 47% 68%\nXLNet-large L2 11 - 38.1 36% 51% 26% 41% 45% 69%\nPre-trained LMs (w/ bias λ=1.5)\nBERT-base HEL 9 A VG 42.3 45% 46% 49% 43% 41% 65%\nBERT-large HEL 17 A VG 44.4 55% 48% 48% 52% 41% 62%\nGPT2 JSD 9 1 41.3 43% 49% 38% 58% 27% 43%\nGPT2-medium HEL 2 1 42.3 54% 50% 39% 56% 24% 41%\nRoBERTa-base JSD 8 A VG 42.1 51% 44% 44% 55% 40% 66%\nRoBERTa-large JSD 12 A VG 42.3 40% 50% 43% 44% 48% 56%\nXLNet-base HEL 7 A VG 48.3 62% 53% 50% 58% 49% 74%\nXLNet-large HEL 11 A VG 46.7 57% 50% 54% 50% 57% 73%\nOther models\nPRPN(tuned)†‡ - - - 47.3 50% 59% 46% 57% 44% 32%\nON(tuned)†‡ - - - 48.1 51% 64% 41% 54% 38% 31%\nNeural PCFG† - - - 50.8 52% 71% 33% 58% 32% 45%\nCompound PCFG† - - - 55.2 56% 74% 41% 68% 40% 52%\nwe conﬁrm that the main contribution of the bias comes from its capability to capture subordinate\nclauses (SBAR) and verb phrases (VP). This observation provides a hint for what some previous\nwork on unsupervised parsing desired to obtain by introducing the bias to their models. It is in-\ntriguing to see that all of the existing grammar induction models are inferior to the right-branching\nbaseline in recognizing SBAR and VP (although some of them already utilized the right-skewness\nbias), implying that the same problem—models do not properly capture the right-branching nature—\nmay also exist in current grammar induction models. One possible assumption is that the models\ndo not need the bias to perform well in language modeling, although future work should provide a\nrigorous analysis about the phenomenon.\nOn the other hand, the existing models show exceptionally high recall scores on noun phrases (NP),\neven though our pre-trained LMs also have success to some extent in capturing noun phrases com-\npared to na ¨ıve baselines. From this, we conjecture that neural models trained with a language\nmodeling objective become largely equipped with the ability to understand the concept of NP. In\ncontrast, the pre-trained LMs record the best recall scores on adjective and adverb phrases (ADJP\nand ADVP), suggesting that the LMs and existing models capture disparate aspects of English syn-\ntax to differing degrees. To further explain why some pre-trained LMs are good at capturing ADJPs\nand ADVPs, we manually investigated the attention heatmaps of the sentences that contain ADJPs\nor ADVPs. From the inspection, we empirically found that there are some keywords—including\n‘two’, ‘ago’, ‘too’, and ‘far’—which have different patterns of attention distributions compared to\nthose of their neighbors and that these keywords can be a clue for our framework to recognize the\nexistence of ADJPs or ADJPs. It is also worth mentioning that ADJPs and ADVPs consist of a\nrelatively smaller number of words than those of SBAR and VP, indicating that the LMs combined\nwith our method have strength in correctly ﬁnding small chunks of words, i.e., low-level phrases.\nMeanwhile, in comparison with other LM models, GPT-2 and XLNet based models demonstrate\ntheir effectiveness and robustness in unsupervised parsing. Particularly, the XLNet-base model\nserves as a robust baseline achieving the top performance among LM candidates. One plausible\nexplanation for this outcome is that the training objective of XLNet, which considers both autoen-\ncoding (AE) and autoregressive (AR) features, might encourage the model to be better aware of\n7\nPublished as a conference paper at ICLR 2020\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n16\n20\n24\n28\n32\n36\n40Sentence-level F1\n bert-base\ngpt2\nroberta-base\nxlnet-base\n0 2 4 6 8 10 12 14 16 18 20 22 24\nLayer\n16\n20\n24\n28\n32\n36\n40Sentence-level F1\n bert-large\ngpt2-medium\nroberta-large\nxlnet-large\nFigure 2: The best layer-wise S-F1 scores of each LM instance on the PTB test set. (Left) The\nperformance of the X-‘base’ models. (Right) The performance of the X-‘large’ models.\nphrase structure than other LMs. Another possible hypothesis is that AR objective functions (e.g.,\ntypical language modeling) are more effective in training syntax-aware neural models than AE ob-\njectives (e.g., masked language modeling), as both GPT-2 and XLNet are pre-trained on AR variants.\nHowever, it is hard to conclude what factors contribute to their high performance at this stage.\nInterestingly, there is an obvious trend that the functions in Fd—the distance measure functions for\nattention distributions—lead most of the LM instances to the best parsing results, indicating that\nderiving parse trees from attention information can be more compact and efﬁcient than extracting\nthem from the LMs’ intermediate representations, which should contain linguistic knowledge be-\nyond phrase structure. In addition, the results in Table 1 show that large parameterizations of the\nLMs generally increase their parsing performance, but this improvement is not always guaranteed.\nMeanwhile, as we expected in Section 4.2 and as seen in the ‘A’ (attention head number) column\nof Table 1, the average of attention distributions in the same layer often provides better results than\nindividual attention distributions.\n5.3 E XPERIMENTAL RESULTS ON MNLI\nWe present the results of various models on the MNLI test set in Table 3 of Appendix A.5. We\nobserve trends in the results which mainly coincide with those of the PTB dataset. Particularly, (1)\nright-branching trees are strong baselines for the task, especially showing their strengths in captur-\ning SBAR and VP clauses/phrases, (2) our method resorting to the LM instances is also comparable\nto the right-branching trees, demonstrating its superiority in recognizing different aspects of phrase\ncategories including prepositional phrases (PP) and adverb phrases (ADVP), and (3) attention dis-\ntributions seem more effective for distilling the phrase structures of sentences than intermediate\nrepresentations.\nHowever, there are some issues worth mentioning. First, the right-branching baseline seems to be\neven stronger in the case of MNLI, recording a score of over 50 in sentence-level F1. We conjecture\nthat this result comes principally from two reasons: (1) the average length of sentences in MNLI is\nmuch shorter than in PTB, giving a disproportionate advantage to na ¨ıve baselines, and (2) our data\npreprocessing, which follows Kim et al. (2019a), removes all punctuation marks, unlike previous\nwork (Htut et al., 2018; Drozdov et al., 2019), leading to an unexpected advantage for the right-\nbranching scheme. Moreover, it deserves to consider the fact that the gold-standard parse trees in\nMNLI are not human-annotated, rather automatically generated.\nSecond, in terms of consistency in identifying the best choice of f and gfor each LM, we observe\nthat most of the best combinations of f and g tuned for PTB do not correspond well to the best\nones for MNLI. Does this observation imply that a speciﬁc combination of these functions and the\nresulting performance do not generalize well across different data domains? To clarify, we manually\ninvestigated the performance of some combinations of f and g, which are tuned on PTB but tested\non MNLI instead. As a result, we discover that particular combinations of f and gwhich are good\nat PTB are also competitive on MNLI, even though they fail to record the best scores on MNLI.\n8\nPublished as a conference paper at ICLR 2020\nTable 2: Results of training a pseudo-optimum fideal with PTB and XLNet-base model.\nModel f L A S-F1 SBAR NP VP PP ADJP ADVP\nBaselines (from Table 1)\nRandom XLNet-base (Fv) - - - 19.6 9% 26% 12% 20% 23% 24%\nRandom XLNet-base (Fd) - - - 20.1 11% 25% 14% 19% 22% 26%\nXLNet-base (λ=0) JSD 9 A VG 40.1 35% 56% 26% 38% 47% 68%\nXLNet-base (λ=1.5) HEL 7 A VG 48.3 62% 53% 50% 58% 49% 74%\nTrained models (w/ gold trees)\nRandom XLNet-base fideal - - 41.2 28% 58% 29% 50% 35% 41%\nXLNet-base (worst case) fideal 1 - 58.0 47% 75% 56% 71% 50% 61%\nXLNet-base (best case) fideal 7 - 65.1 61% 82% 67% 78% 55% 73%\nConcretely, the union of fd(JSD) and gd\n(9,13)—the best duo for the XLNet-base on PTB—achieves\n39.2 in sentence-level F1 on MNLI, which is very close to the top performance (39.3) we can obtain\nwhen leveraging the XLNet-base. It is also worth noting that GPT-2 and XLNet are efﬁcient in\ncapturing PP and ADVP respectively, regardless of the data domain and the choice off and g.\n6 F URTHER ANALYSIS\n6.1 PERFORMANCE COMPARISON BY LAYER\nTo take a closer look at how different the layers of the pre-trained LMs are in terms of parsing\nperformance, we retrieve the best sentence-level F1 scores from the lth layer of an LM from all\ncombinations of f and gl, with regard to the PTB and MNLI respectively. Then we plot the scores\nas graphs in Figure 2 for the PTB and Figure 4 in Appendix A.4 for the MNLI. Each score is from\nthe models to which the bias is not applied.\nFrom the graphs, we observe several patterns. First, XLNet-based models outperform other com-\npetitors across most of the layers. Second, the best outcomes are largely shown in the middle layers\nof the LMs akin to the observation from Shen et al. (2019), except for some cases where the ﬁrst lay-\ners (especially in case of MNLI) record the best. Interestingly, GPT-2 shows a decreasing trend in its\noutput values as the layer becomes high, while other models generally exhibit the opposite pattern.\nMoreover, we discover from raw statistics that regardless of the choice of f and gl, the parsing per-\nformance reported as S-F1 is moderately correlated with the layer numberl. In other words, it seems\nthat there are some particular layers in the LMs which are more sensitive to syntactic information.\n6.2 E STIMATING THE UPPER LIMIT OF DISTANCE MEASURE FUNCTIONS\nAlthough we have introduced effective candidates forf, we explore the potential of extracting more\nsophisticated trees from pre-trained LMs, supposing we are equipped with a pseudo-optimum f,\ncall it fideal. To obtain fideal, we train a simple linear layer on each layer of the pre-trained LMs with\nsupervision from the gold-standard trees of the PTB training set, while gremains unchanged—the\npre-trained LMs are frozen during training. We choose the XLNet-base model as a representative\nfor the pre-trained LMs. For more details about experimental settings, refer to Appendix A.6.\nIn Table 2, we present three new results using fideal. As a baseline, we report the performance\nof fideal with a randomly initialized XLNet-base. Then, we list the worst and best result of fideal\naccording to g, when it is combined with the pre-trained LM. We here mention some ﬁndings from\nthe experiment. First, comparing the results with the pre-trained LM against one with the random\nLM, we reconﬁrm that pre-training an LM apparently enables the model to capture some aspects of\ngrammar. Speciﬁcally, our method is comparable to the linear model trained on the gold-standard\ntrees. Second, we ﬁnd that there is a tendency for the performance of fideal relying on different LM\nlayers to follow one we already observed in Section 6.1—the best result comes from the middle\nlayers of the LM while the worst from the ﬁrst and last layer. Third, we identify that the LM has\na potential to show improved performance on grammar induction by adopting a more sophisticated\nf. However, we emphasize that our method equipped with a simple f without gold-standard trees\nis remarkably reasonable in recognizing constituency grammar, being especially good at catching\nADJP and ADVP.\n9\nPublished as a conference paper at ICLR 2020\n6.3 C ONSTITUENCY TREE EXAMPLES\nWe visualize several gold-standard trees from PTB and the corresponding tree predictions for com-\nparison. For more details, we refer readers to Appendix A.7.\n7 C ONCLUSIONS AND FUTURE WORK\nIn this paper, we propose a simple but effective method of inducing constituency trees from pre-\ntrained language models in a zero-shot learning fashion. Furthermore, we report a set of intuitive\nﬁndings observed from the extracted trees, demonstrating that the pre-trained LMs exhibit some\nproperties similar to constituency grammar. In addition, we show that our method can serve as a\nstrong baseline for English grammar induction when combined with (or even without) appropriate\nlinguistic biases.\nOn the other hand, there are still remaining issues that can be good starting points for future work.\nFirst, although we analyzed our method based on two popular datasets, we focused only on English\ngrammar induction. As each language has its own properties (and correspondingly would need\nindividualized biases), it is desirable to expand this work to other languages. Second, it would also\nbe desirable to investigate whether further improvements can be achieved by directly grafting the\npre-trained LMs onto existing grammar induction models. Lastly, by verifying the usefulness of the\nknowledge from the pre-trained LMs and linguistic biases for grammar induction, we want to point\nout that there is still much room for improvement in the existing grammar induction models.\nACKNOWLEDGMENTS\nWe would like to thank Reinald Kim Amplayo and the anonymous reviewers for their thoughtful\nand valuable comments. This work was supported by the National Research Foundation of Korea\n(NRF) grant funded by the Korea government (MSIT) (NRF2016M3C4A7952587).\nREFERENCES\nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hul-\nlender. Learning to rank using gradient descent. In Proceedings of the 22nd international confer-\nence on Machine learning, pp. 89–96. ACM, 2005.\nJihun Choi, Kang Min Yoo, and Sang-goo Lee. Learning to compose task-speciﬁc tree structures.\nIn Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\nAlexander Clark. Unsupervised induction of stochastic context-free grammars using distributional\nclustering. In Proceedings of the 2001 workshop on Computational Natural Language Learning-\nVolume 7, pp. 13, 2001.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look\nat? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, pp. 276–286, Florence, Italy, August 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June\n2019.\nAndrew Drozdov, Patrick Verga, Mohit Yadav, Mohit Iyyer, and Andrew McCallum. Unsupervised\nlatent tree induction with deep inside-outside recursive auto-encoders. InProceedings of the 2019\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pp. 1129–1141, Minneapolis,\nMinnesota, June 2019.\n10\nPublished as a conference paper at ICLR 2020\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language Technologies, pp. 199–209, San Diego,\nCalifornia, June 2016.\nChris Dyer, G ´abor Melis, and Phil Blunsom. A critical analysis of biased parsers in unsupervised\nparsing. arXiv preprint arXiv:1909.09428, 2019.\nKawin Ethayarajh. How contextual are contextualized word representations? comparing the ge-\nometry of BERT, ELMo, and GPT-2 embeddings. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pp. 55–65, Hong Kong, China, November\n2019.\nYoav Goldberg. Assessing bert’s syntactic abilities.arXiv preprint arXiv:1901.05287, 2019.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. Visualizing and understanding the effectiveness of bert.\narXiv preprint arXiv:1908.05620, 2019.\nSerhii Havrylov, Germ´an Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax\nand semantics. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pp. 1118–1128, Minneapolis, Minnesota, June 2019.\nJohn Hewitt and Christopher D. Manning. A structural probe for ﬁnding syntax in word representa-\ntions. In Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-\npers), pp. 4129–4138, Minneapolis, Minnesota, June 2019.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nPhu Mon Htut, Kyunghyun Cho, and Samuel Bowman. Grammar induction with neural language\nmodels: An unusual replication. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, pp. 371–373, Brussels, Belgium, November\n2018.\nYoon Kim, Chris Dyer, and Alexander Rush. Compound probabilistic context-free grammars for\ngrammar induction. In Proceedings of the 57th Annual Meeting of the Association for Computa-\ntional Linguistics, pp. 2369–2385, Florence, Italy, July 2019a.\nYoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and G ´abor Melis. Unsuper-\nvised recurrent neural network grammars. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pp. 1105–1117, Minneapolis, Minnesota, June 2019b.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nNikita Kitaev and Dan Klein. Constituency parsing with a self-attentive encoder. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 2676–2686, Melbourne, Australia, July 2018.\nDan Klein and Christopher Manning. Corpus-based induction of syntactic structure: Models of\ndependency and constituency. In Proceedings of the 42nd Annual Meeting of the Association for\nComputational Linguistics (ACL-04), pp. 478–485, Barcelona, Spain, July 2004.\nDan Klein and Christopher D. Manning. A generative constituent-context model for improved gram-\nmar induction. In Proceedings of the 40th Annual Meeting of the Association for Computational\nLinguistics, pp. 128–135, Philadelphia, Pennsylvania, USA, July 2002.\nDan Klein and Christopher D Manning. Accurate unlexicalized parsing. In Proceedings of the 41st\nAnnual Meeting on Association for Computational Linguistics-Volume 1, pp. 423–430. Associa-\ntion for Computational Linguistics, 2003.\n11\nPublished as a conference paper at ICLR 2020\nKarim Lari and Steve J Young. The estimation of stochastic context-free grammars using the inside-\noutside algorithm. Computer speech & language, 4(1):35–56, 1990.\nBowen Li, Lili Mou, and Frank Keller. An imitation learning approach to unsupervised parsing.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.\n3485–3492, Florence, Italy, July 2019.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Linguistic\nknowledge and transferability of contextual representations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pp. 1073–1094, Minneapolis, Min-\nnesota, June 2019a.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019b.\nJean Maillard, Stephen Clark, and Dani Yogatama. Jointly learning sentence embeddings and syntax\nwith unsupervised tree-lstms. Natural Language Engineering, 25(4):433–449, 2019.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated\ncorpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993.\nDavid Mare ˇcek and Rudolf Rosa. Extracting syntactic trees from transformer encoder self-\nattentions. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Inter-\npreting Neural Networks for NLP, pp. 347–349, Brussels, Belgium, November 2018.\nDavid Mareˇcek and Rudolf Rosa. From balustrades to pierre vinken: Looking for syntax in trans-\nformer self-attentions. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pp. 263–275, Florence, Italy, August 2019.\nNikita Nangia and Samuel Bowman. Listops: A diagnostic dataset for latent tree learning. In\nProceedings of the 2018 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Student Research Workshop, pp. 92–99, 2018.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pp. 2227–2237, New Orleans, Louisiana, June\n2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nAlessandro Raganato and J ¨org Tiedemann. An analysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Ana-\nlyzing and Interpreting Neural Networks for NLP , pp. 287–297, Brussels, Belgium, November\n2018.\nRudolf Rosa and David Mareˇcek. Inducing syntactic trees from bert representations. arXiv preprint\narXiv:1906.11511, 2019.\nYikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, and Yoshua\nBengio. Straight to the tree: Constituency parsing with neural syntactic distance. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 1171–1180, Melbourne, Australia, July 2018a.\nYikang Shen, Zhouhan Lin, Chin wei Huang, and Aaron Courville. Neural language modeling by\njointly learning syntax and lexicon. In International Conference on Learning Representations ,\n2018b.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. Ordered neurons: Integrating\ntree structures into recurrent neural networks. In International Conference on Learning Repre-\nsentations, 2019.\n12\nPublished as a conference paper at ICLR 2020\nHaoyue Shi, Jiayuan Mao, Kevin Gimpel, and Karen Livescu. Visually grounded neural syntax\nacquisition. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pp. 1842–1861, Florence, Italy, July 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics , pp. 5797–5808, July\n2019.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations, 2019.\nAdina Williams, Andrew Drozdov, and Samuel R. Bowman. Do latent tree learning models identify\nmeaningful structure in sentences? Transactions of the Association for Computational Linguis-\ntics, 6:253–267, 2018a.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\ntence understanding through inference. InProceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018b.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\nXlnet: Generalized autoregressive pretraining for language understanding. Advances in Neural\nInformation Processing Systems 32, pp. 5754–5764, 2019.\nDani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. Learning to com-\npose words into sentences with reinforcement learning. In International Conference on Learning\nRepresentations, 2017.\n13\nPublished as a conference paper at ICLR 2020\nA A PPENDIX\nA.1 A TTENTION HEATMAP EXAMPLES\nFigure 3: Self-attention heatmaps for the average of all attention distributions from the 7th layer of\nthe XLNet-base model, given a set of input sentences.\n14\nPublished as a conference paper at ICLR 2020\nA.2 T REE CONSTRUCTION ALGORITHM WITH SYNTACTIC DISTANCES\nAlgorithm 1 Syntactic Distances to Binary Constituency Tree (originally from Shen et al. (2018a))\n1: S = [w1, w2, . . . , wn]: a sequence of words in a sentence of length n.\n2: d = [d1, d2, . . . , dn−1]: a vector whose elements are the distances between every two adjacent words.\n3: function TREE (S, d)\n4: if d = []then\n5: node ←Leaf(S[0])\n6: else\n7: i ←arg maxi(d)\n8: childl ←TREE (S≤i, d<i)\n9: childr ←TREE (S>i, d>i)\n10: node ←Node(childl, childr)\n11: end if\n12: return node\n13: end function\nA.3 D ISTANCE MEASURE FUNCTIONS\nTable 3: The deﬁnitions of distance measure functions for computing syntactic distances between\ntwo adjacent words in a sentence. Note that r = gv(wi), s = gv(wi+1), P = gd(wi), and Q =\ngd(wi+1), respectively. d: hidden embedding size, n: the number of words (w) in a sentence (S).\nFunction (f) Deﬁnition\nFunctions for intermediate representations (Fv)\nCOS(r,s)\n(\nr⊤s/\n(\n(∑d\ni=1 ri2)\n1\n2 ·(∑d\ni=1 si2)\n1\n2\n)\n+ 1\n)\n/2\nL1(r,s) ∑d\ni=1 |ri −si|\nL2(r,s) ( ∑d\ni=1(ri −si)2)\n1\n2\nFunctions for attention distributions (Fd)\nJSD (P∥Q) (( DKL(P∥M) +DKL(Q∥M))/2)\n1\n2\nwhere M = (P + Q)/2\nand DKL(A∥B) =∑\nw∈SA(w) log(A(w)/B(w))\nHEL (P,Q) 1√\n2 (∑n\ni=1(√pi −√qi)2)\n1\n2\nA.4 P ERFORMANCE COMPARISON BY LAYER ON MNLI\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n20\n24\n28\n32\n36\n40\n44\n48\n52Sentence-level F1\n bert-base\ngpt2\nroberta-base\nxlnet-base\n0 2 4 6 8 10 12 14 16 18 20 22 24\nLayer\n20\n24\n28\n32\n36\n40\n44\n48\n52Sentence-level F1\n bert-large\ngpt2-medium\nroberta-large\nxlnet-large\nFigure 4: The best layer-wise S-F1 scores of each LM instance on the MNLI test set. (Left) The\nperformance of the X-‘base’ models. (Right) The performance of the X-‘large’ models.\n15\nPublished as a conference paper at ICLR 2020\nA.5 E XPERIMENTAL RESULTS ON MNLI\nTable 4: Results on the MNLI test set. Bold numbers correspond to the top 3 results for each column.\nL: layer number, A: attention head number (A VG: the average of all attentions).†: Results reported\nby Htut et al. (2018) and Drozdov et al. (2019). ‡: Approaches in which COO parser is utilized. ∗:\nThese results are not strictly comparable to ours, due to the difference in data preprocessing.\nModel f L A S-F1 SBAR NP VP PP ADJP ADVP\nBaselines\nRandom Trees - - - 21.4 11% 25% 16% 22% 22% 27%\nBalanced Trees - - - 20.0 8% 29% 11% 20% 22% 32%\nLeft Branching Trees - - - 8.4 6% 13% 1% 4% 1% 8%\nRight Branching Trees - - - 51.9 65% 28% 75% 47% 45% 30%\nRandom XLNet-base (Fv) - - - 22.0 12% 26% 15% 22% 22% 25%\nRandom XLNet-base (Fd) - - - 23.5 14% 26% 18% 22% 22% 25%\nPre-trained LMs (w/o bias)\nBERT-base HEL 9 10 36.1 36% 37% 34% 45% 26% 42%\nBERT-large JSD 17 10 37.0 38% 32% 34% 50% 22% 39%\nGPT2 JSD 1 10 44.0 43% 53% 31% 60% 24% 40%\nGPT2-medium JSD 3 12 49.1 57% 32% 61% 44% 35% 37%\nRoBERTa-base JSD 10 9 36.2 26% 35% 34% 50% 23% 44%\nRoBERTa-large JSD 3 6 39.8 20% 28% 35% 30% 28% 27%\nXLNet-base HEL 1 6 39.0 25% 39% 28% 59% 35% 44%\nXLNet-large HEL 1 15 42.2 32% 49% 27% 62% 32% 49%\nPre-trained LMs (w/ bias λ=1.5)\nBERT-base HEL 2 12 52.7 64% 35% 70% 50% 46% 30%\nBERT-large HEL 4 4 51.7 63% 31% 71% 49% 46% 30%\nGPT2 HEL 1 10 52.2 57% 53% 49% 62% 32% 42%\nGPT2-medium HEL 2 1 53.9 53% 57% 50% 62% 29% 44%\nRoBERTa-base HEL 2 3 52.0 64% 31% 72% 49% 47% 30%\nRoBERTa-large L1 23 - 52.7 55% 40% 65% 53% 43% 41%\nXLNet-base L2 8 - 54.9 57% 49% 61% 55% 44% 57%\nXLNet-large L2 12 - 53.5 54% 47% 59% 51% 48% 60%\nOther models\nPRPN-UP†‡ - - - 48.6 ∗ - - - - - -\nPRPN-LM†‡ - - - 50.4 ∗ - - - - - -\nDIORA† - - - 51.2 ∗ - - - - - -\nDIORA(+PP)† - - - 59.0 ∗ - - - - - -\nA.6 E XPERIMENTAL DETAILS FOR TRAINING IDEAL DISTANCE MEASURE FUNCTION\nIn this part, we present the detailed speciﬁcations of the experiments introduced in Section 6.2. We\nassume fideal is only compatible with the functions in Gv, as the functions in Gd are not suitable for\ntraining as the sizes of the representations provided by Gd are variable according to the length of an\ninput sentence. To train the pseudo-optimal functionfideal, we minimize a pair-wise learning-to-rank\nloss following previous work (Burges et al., 2005; Shen et al., 2018a):\nLrank\ndist =\n∑\ni,j>i\n[1 −sign(dgold\ni −dgold\nj )(dpred\ni −dpred\nj )]+, (3)\nwhere dgold and dpred are computed from the gold tree and our predicted one, respectively. [x]+ is\ndeﬁned as max(0,x). We train the fideal with the PTB training set for 5 epochs. Each batch of\nthe training set contains 16 sentences. We use an ADAM optimizer (Kingma & Ba, 2014) with the\nlearning rate 5e-4. We train the variations of fideal differentiated by the choice of gin Gv and report\nthe best result in the Table 2. Each fideal is chosen based on its performance on the PTB validation\nset. Considering the randomness of training, every result for fideal is averaged over 3 different trials.\nA.7 C ONSTITUENCY TREE EXAMPLES\nWe randomly select six sentences from PTB and visualize their trees, where the resulting group\nof trees for each sentence consists of a gold constituency tree and two induced trees (one without\nthe right-skewness bias and the other with the bias) from our best model—XLNet-base. The ‘T’\ncharacter in the induced trees indicates a dummy tag.\n16\nPublished as a conference paper at ICLR 2020\nS\nNP-SBJ\nDT\nThese\nVP\nVBP\ninclude\nNP\nNP\nDT\na\nNN\nchild-care\nNN\ninitiative\nCC\nand\nNP\nNP\nNNS\nextensions\nPP\nIN\nof\nNP\nNP\nJJ\nsoon-to-expire\nNN\ntax\nNNS\nbreaks\nPP\nIN\nfor\nNP\nNP\nJJ\nlow-income\nNN\nhousing\nCC\nand\nNP\nNN\nresearch-and-development\nNNS\nexpenditures\nT\nT\nT\nT\nT\nThese\nT\ninclude\nT\na\nT\nT\nchild-care\nT\ninitiative\nT\nT\nT\nT\nand\nT\nT\nT\nT\nextensions\nT\nof\nT\nsoon-to-expire\nT\nT\ntax\nT\nbreaks\nT\nT\nfor\nT\nT\nlow-income\nT\nhousing\nT\nT\nand\nT\nT\nresearch-and-development\nT\nexpenditures\nT\nT\nT\nThese\nT\ninclude\nT\nT\nT\na\nT\nT\nchild-care\nT\ninitiative\nT\nT\nand\nT\nT\nT\nextensions\nT\nof\nT\nT\nsoon-to-expire\nT\nT\nT\ntax\nT\nbreaks\nT\nT\nT\nfor\nT\nT\nlow-income\nT\nhousing\nT\nT\nand\nT\nT\nresearch-and-development\nT\nexpenditures\nFigure 5: Gold (top) and predicted trees (onewithout the bias in themiddle, the other with the bias at\nthe bottom) for the sentence ‘These include a child-care initiative and extensions of soon-to-expire\ntax breaks for low-income housing and research-and-development expenditures’.\n17\nPublished as a conference paper at ICLR 2020\nS\nCC\nBut\nNP-SBJ\nNP\nNNP\nHOFI\nPOS\n's\nJJ\nfirst\nNN\noffer\nVP\nMD\nwould\nVP\nVB\nhave\nVP\nVBN\ngiven\nNP\nNP\nNNP\nIdeal\nPOS\n's\nJJ\nother\nNNS\nshareholders\nNP\nNP\nQP\nRB\nabout\nCD\n10\nNN\n%\nPP\nIN\nof\nNP\nDT\nthe\nJJ\ncombined\nNN\ncompany\nT\nT\nT\nBut\nT\nT\nT\nHOFI\nT\nT\n's\nT\nT\nfirst\nT\noffer\nT\nT\nT\nwould\nT\nhave\nT\ngiven\nT\nT\nT\nIdeal\nT\nT\n's\nT\nT\nother\nT\nshareholders\nT\nT\nT\nabout\nT\nT\nT\n10\nT\n%\nT\nof\nT\nT\nthe\nT\nT\ncombined\nT\ncompany\nT\nT\nT\nBut\nT\nT\nHOFI\nT\nT\nT\n's\nT\nT\nfirst\nT\noffer\nT\nT\nT\nwould\nT\nhave\nT\ngiven\nT\nT\nT\nIdeal\nT\nT\n's\nT\nT\nother\nT\nshareholders\nT\nT\nabout\nT\nT\nT\n10\nT\n%\nT\nT\nof\nT\nT\nthe\nT\nT\ncombined\nT\ncompany\nFigure 6: Gold (top) and predicted trees (one without the bias in the middle, the other with the bias\nat the bottom) for the sentence ‘But HOFI ‘s ﬁrst offer would have given Ideal ’s other shareholders\nabout 10 % of the combined company’.\n18\nPublished as a conference paper at ICLR 2020\nS\nS\nNP-SBJ\nPRP\nIt\nVP\nVBD\nwas\nNP-PRD\nNNP\nFriday\nDT\nthe\nNNP\n13th\nCC\nand\nS\nNP-SBJ\nDT\nthe\nNN\nstock\nNN\nmarket\nVP\nVBD\nplummeted\nNP-EXT\nQP\nRB\nnearly\nCD\n200\nNNS\npoints\nT\nT\nT\nT\nIt\nT\nwas\nT\nT\nFriday\nT\nT\nthe\nT\n13th\nT\nT\nT\nand\nT\nT\nthe\nT\nT\nstock\nT\nmarket\nT\nT\nplummeted\nT\nT\nnearly\nT\nT\n200\nT\npoints\nT\nT\nT\nIt\nT\nT\nwas\nT\nT\nFriday\nT\nT\nthe\nT\n13th\nT\nT\nand\nT\nT\nT\nthe\nT\nT\nstock\nT\nmarket\nT\nT\nplummeted\nT\nT\nnearly\nT\nT\n200\nT\npoints\nFigure 7: Gold (top) and predicted trees (one without the bias in the middle, the other with the bias\nat the bottom) for the sentence ‘It was Friday the 13th and the stock market plummeted nearly 200\npoints’.\n19\nPublished as a conference paper at ICLR 2020\nS\nPP-TMP\nIN\nUntil\nADVP\nRB\nrecently\nNP-SBJ\nNP\nJJ\nnational\nNNS\ngovernments\nPP-LOC\nIN\nin\nNP\nNNP\nEurope\nVP\nVP\nVBD\ncontrolled\nNP\nNP\nJJS\nmost\nPP\nIN\nof\nNP\nDT\nthe\nNN\nair\nNN\ntime\nCC\nand\nVP\nVBD\nallowed\nNP\nADJP\nJJ\nlittle\nCC\nor\nDT\nno\nNN\nadvertising\nT\nT\nT\nT\nUntil\nT\nrecently\nT\nT\nnational\nT\nT\ngovernments\nT\nT\nin\nT\nEurope\nT\nT\nT\ncontrolled\nT\nT\nT\nT\nmost\nT\nT\nof\nT\nthe\nT\nair\nT\ntime\nT\nT\nT\nand\nT\nallowed\nT\nT\nT\nlittle\nT\nT\nor\nT\nno\nT\nadvertising\nT\nT\nT\nUntil\nT\nrecently\nT\nT\nT\nT\nnational\nT\ngovernments\nT\nT\nin\nT\nEurope\nT\nT\ncontrolled\nT\nT\nT\nmost\nT\nT\nof\nT\nT\nthe\nT\nT\nair\nT\ntime\nT\nT\nT\nand\nT\nallowed\nT\nT\nlittle\nT\nT\nor\nT\nT\nno\nT\nadvertising\nFigure 8: Gold (top) and predicted trees (one without the bias in the middle, the other with the bias\nat the bottom) for the sentence ‘Until recently national governments in Europe controlled most of\nthe air time and allowed little or no advertising’.\n20\nPublished as a conference paper at ICLR 2020\nS\nADVP\nRB\nNevertheless\nNP-SBJ\nNNP\nMs.\nNNP\nGarzarelli\nVP\nVBD\nsaid\nSBAR\nS\nNP-SBJ-1\nPRP\nshe\nVP\nVBD\nwas\nVP\nVBN\nswamped\nPP-CLR\nIN\nwith\nNP\nNP\nNN\nphone\nNNS\ncalls\nPP-TMP\nIN\nover\nNP\nDT\nthe\nNN\nweekend\nPP-2\nIN\nfrom\nNP\nJJ\nnervous\nNNS\nshareholders\nT\nT\nT\nNevertheless\nT\nT\nMs.\nT\nGarzarelli\nT\nT\nT\nsaid\nT\nshe\nT\nT\nT\nT\nT\nT\nwas\nT\nswamped\nT\nwith\nT\nT\nphone\nT\ncalls\nT\nT\nT\nover\nT\nthe\nT\nweekend\nT\nT\nfrom\nT\nT\nnervous\nT\nshareholders\nT\nT\nNevertheless\nT\nT\nT\nMs.\nT\nGarzarelli\nT\nT\nsaid\nT\nT\nshe\nT\nT\nT\nwas\nT\nswamped\nT\nT\nwith\nT\nT\nT\nphone\nT\ncalls\nT\nT\nT\nT\nover\nT\nthe\nT\nweekend\nT\nT\nfrom\nT\nT\nnervous\nT\nshareholders\nFigure 9: Gold (top) and predicted trees (one without the bias in the middle, the other with the bias\nat the bottom) for the sentence ‘Nevertheless Ms. Garzarelli said she was swamped with phone calls\nover the weekend from nervous shareholders’.\n21\nPublished as a conference paper at ICLR 2020\nS\nNP-SBJ\nNNS\nAnalysts\nCC\nand\nNNS\ncompetitors\nADVP\nRB\nhowever\nVP\nVBP\ndoubt\nSBAR\nS\nNP-SBJ\nDT\nthe\nNNS\nnumbers\nVP\nVBD\nwere\nADJP-PRD\nRB\nthat\nJJ\nhigh\nT\nT\nT\nAnalysts\nT\nT\nand\nT\ncompetitors\nT\nT\nT\nhowever\nT\ndoubt\nT\nT\nT\nT\nthe\nT\nnumbers\nT\nwere\nT\nT\nthat\nT\nhigh\nT\nT\nT\nAnalysts\nT\nT\nand\nT\ncompetitors\nT\nT\nT\nhowever\nT\ndoubt\nT\nT\nT\nthe\nT\nnumbers\nT\nT\nwere\nT\nT\nthat\nT\nhigh\nFigure 10: Gold ( top) and predicted trees (one without the bias in the middle, the other with the\nbias at the bottom) for the sentence ‘Analysts and competitors however doubt the numbers were that\nhigh’.\n22",
  "topic": "Simple (philosophy)",
  "concepts": [
    {
      "name": "Simple (philosophy)",
      "score": 0.6905869841575623
    },
    {
      "name": "Computer science",
      "score": 0.6846005916595459
    },
    {
      "name": "Natural language processing",
      "score": 0.5976466536521912
    },
    {
      "name": "Grammar",
      "score": 0.5939561128616333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48179787397384644
    },
    {
      "name": "Linguistics",
      "score": 0.4020988345146179
    },
    {
      "name": "Philosophy",
      "score": 0.05986467003822327
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    }
  ],
  "cited_by": 46
}