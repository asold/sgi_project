{
    "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge",
    "url": "https://openalex.org/W4385570140",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2934935891",
            "name": "Jiangjie Chen",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A1981201842",
            "name": "Wei Shi",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5041550808",
            "name": "Ziquan Fu",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2375466389",
            "name": "Sijie Cheng",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2098784551",
            "name": "Lei Li",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2131222654",
            "name": "Yanghua Xiao",
            "affiliations": [
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2891012317",
        "https://openalex.org/W2968297680",
        "https://openalex.org/W3173805051",
        "https://openalex.org/W4283796213",
        "https://openalex.org/W2088421158",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W3213868621",
        "https://openalex.org/W4382202696",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4285309087",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W3199350934",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W2951936329",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W2037719467",
        "https://openalex.org/W3173673636",
        "https://openalex.org/W3176750236",
        "https://openalex.org/W4230262515",
        "https://openalex.org/W4385574359",
        "https://openalex.org/W1944651790",
        "https://openalex.org/W3174409617",
        "https://openalex.org/W2002110857",
        "https://openalex.org/W4287891464",
        "https://openalex.org/W3092288641",
        "https://openalex.org/W2979716919",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4285287265",
        "https://openalex.org/W4385572965",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W2962736243",
        "https://openalex.org/W4292963003",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4297412056",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3175591618",
        "https://openalex.org/W3169432481",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W2107901333",
        "https://openalex.org/W3210277894",
        "https://openalex.org/W3161374759",
        "https://openalex.org/W3174131299",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W4385574034",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3164329620",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W4285255684",
        "https://openalex.org/W102708294",
        "https://openalex.org/W4386566648",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4385572727",
        "https://openalex.org/W3034830866"
    ],
    "abstract": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \"lions don't live in the ocean\", is also ubiquitous in the world but rarely mentioned explicitly in text.What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge.We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions.We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 9890–9908\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nSay What You Mean!Large Language Models Speak Too Positively about\nNegative Commonsense Knowledge\nJiangjie Chen♠, Wei Shi♠, Ziquan Fu♡∗, Sijie Cheng♠, Lei Li♣, Yanghua Xiao♠♢†\n♠Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n♡System Inc. ♣University of California, Santa Barbara\n♢Fudan-Aishu Cognitive Intelligence Joint Research Center\n{jjchen19, sjcheng20, shawyh}@fudan.edu.cn\nwshi22@m.fudan.edu.cn, frank@system.com, leili@cs.ucsb.edu\nAbstract\nLarge language models (LLMs) have been\nwidely studied for their ability to store and\nutilize positive knowledge. However, nega-\ntive knowledge, such as “ lions don’t live in\nthe ocean”, is also ubiquitous in the world but\nrarely mentioned explicitly in the text. What do\nLLMs know about negative knowledge? This\nwork examines the ability of LLMs to nega-\ntive commonsense knowledge. We design a\nconstrained keywords-to-sentence generation\ntask (CG) and a Boolean question-answering\ntask (QA) to probe LLMs. Our experiments\nreveal that LLMs frequently fail to generate\nvalid sentences grounded in negative common-\nsense knowledge, yet they can correctly answer\npolar yes-or-no questions. We term this phe-\nnomenon the belief conflict of LLMs. Our fur-\nther analysis shows that statistical shortcuts and\nnegation reporting bias from language model-\ning pre-training cause this conflict.1\n1 Introduction\nMost of the world knowledge exists in a positive\nand affirmative form (Molnar, 2000; Barker and\nJago, 2012; Vrandeˇci´c and Krötzsch, 2014; Speer\net al., 2017). As a result, large language models\n(LLMs) pre-trained on a colossal amount of texts,\nsuch as GPT-3 (Brown et al., 2020; Ouyang et al.,\n2022) and PaLM (Chowdhery et al., 2022), have\ndemonstrated their remarkable abilities for storing\nand utilizing positive knowledge in downstream\ntasks. In contrast, negative knowledge, such as\nthe commonsense statement that “lions do not live\nin the ocean ”, is rarely mentioned in the textual\nworld (Hossain et al., 2022).2 Such negative knowl-\nedge also exists in the real world, and is important\n∗Work done while at Brain Technologies, Inc.\n†Corresponding author.\n1Resources of this paper are available at https://\ngithub.com/jiangjiechen/uncommongen.\n2Hossain et al. (2022) report that sentences with negation\nhold up to 14.5% in the CommonsenseQA dataset (Talmor\net al., 2019), 8.7% in QNLI (Rajpurkar et al., 2016), and\n22.6-29.9% in general-purposed texts.\nThe Constrained Generation Task \n- Make a correct commonsense sentence based on the keywords.\nThe Question Answering Task \n- Answer the commonsense question.\nLLM\nLLM\nQuestion \nDo lions live in the ocean?\nKeywords \nlion, located at, ocean\nAnswer \nNo\nSentence \nLions live in the ocean.\nConflict?\nFigure 1: An example of the probing tasks studied in\nthis paper. For the same negative commonsense knowl-\nedge <lion, located at, ocean> which is false, we find\nLLMs often fail to generate texts grounded in such neg-\native knowledge while knowing its validity according\nto question answering.\nfor cognitive skills such as knowingwhat is not true\nor what not to think (MacDonald, 1965; Minsky,\n1997; Barker and Jago, 2012). Therefore, we ask\nthis question: Do LLMs (such as GPT-3 models)\nacquire such implicit negative knowledge through\nextensive language modeling pre-training?\nOne important way of probing LLMs, which are\nmostly generative models, is checking whether the\ngenerated texts are knowledge-grounded. This is\nbecause the generation of texts is a direct man-\nifestation of a model’s internal beliefs towards\nworld knowledge (Kassner et al., 2021; Sumers\net al., 2021; Tafjord et al., 2022). 3 Knowledge-\ngrounded text generation has been a focus of NLP\nresearch (Yu et al., 2022). For example, the COM-\nMON GEN benchmark (Lin et al., 2020) evaluates\ngenerative commonsense reasoning that organizes\nconcepts as keyword input and generates a sentence\ngrounded in commonsense knowledge. However,\nprevious work does not consider negative knowl-\nedge, nor do they probe the consistency between\n3Our definition of belief is derived from Kassner et al.\n(2021), which is the assignment of a truth value to a propo-\nsition. In our study, the context for the proposition is the\nworld knowledge that models learned. Therefore, we define a\nmodel’s belief about such knowledge as its prediction about\nthe truth value of a certain piece of world knowledge.\n9890\nwhat models know and what they generate. An-\nother line of work on probing (Petroni et al., 2019;\nEttinger, 2020; Kassner and Schütze, 2020; Cao\net al., 2021) is conducted through the mask-infilling\ntask. However, this task mainly evaluates bidirec-\ntional models (Devlin et al., 2019), and is not natu-\nral for unidirectional LLMs. Also, this task suffers\nfrom the open-world problem in evaluation, i.e.,\nthere could be multiple valid answers to fill the\nmask. This is vital for evaluating negative knowl-\nedge, which has an infinite answer space,e.g., lions\ndon’t live in the sky, water, desk, car, etc.\nIn this study, we investigate the belief of LLMs\nabout negative commonsense knowledge through\nthe lens of text generation. Since LLMs have be-\ncome a foundational service (Bommasani et al.,\n2021) and cannot be easily trained, we apply in-\ncontext learning (Brown et al., 2020) for the prob-\ning tasks, which is tuning-free. We design a Con-\nstrained Sentence Generation (CG) probing task,\nfollowing Lin et al. (2020), where the model must\ngenerate a knowledge-grounded sentence based on\na given triple <s,r,o>. For example, given a triple\n“<lion, located at, ocean>”, a model should gener-\nate “lions do not live in the ocean ”. This task is\nrather simple and clear. The output sentence ba-\nsically contains the same information as the input\nkeywords. Thus, the generated texts are easy to\nevaluate according to the appearance of negation.\nWe also add a Boolean Question Answering (QA)\ntask that asks LLMs whether a knowledge triple is\nvalid, which shows their beliefs about this piece of\nknowledge. An example is given in Figure 1.\nIn our experiments, we find that LLMs of dif-\nferent sizes and shapes often produce hallucinated\nclaims of negative knowledge, even if they answer\nyes-or-no questions about it correctly. We term\nthis phenomenon the belief conflict, i.e., actions\n(generating texts with it) conflict with its belief (an-\nswering question about it). Hallucinated generation\nof negative knowledge is seen in both our probing\ntasks and downstream tasks, such as explanation\ngeneration (Chen et al., 2022; Jung et al., 2022),\nwhere negative knowledge plays an important role\nin the argumentation of refutation. Further analysis\nshows that this problem stems from the statistical\nshortcuts and reporting bias of negation during pre-\ntraining. Moreover, such implicit biases can be\nalleviated through explicit reasoning with Chain-\nof-Thought prompting (Wei et al., 2022b), such as\nsyllogistic deduction and related fact comparison.\nThe main contributions of this paper are sum-\nmarized as follows: 1) We are the first to investi-\ngate LLMs’ belief about negative knowledge in the\ncommonsense domain, which may shed light on\na previously unstudied aspect of LLMs’ abilities.\n2) We propose to probe generative LLMs through\nconstrained sentence generation, which is effective\nfor evaluating generated texts grounded in positive\nand negative knowledge. 3) Through extensive ex-\nperiments, we identify and analyze LLMs’ belief\nconflict phenomenon on negative commonsense\nknowledge, and provide insights on the causes and\nsolutions of such problems.\n2 Related Work\nNegative Knowledge Negative knowledge refers\nto information that describes what is not true, what\ncannot be done, or what does not exist, while every-\nthing that exists is positive (Molnar, 2000; Barker\nand Jago, 2012). It plays an important role in the\nhuman reasoning process, because to think effec-\ntively, we need to know what “not to think” (Min-\nsky, 1997). Current research of negative knowl-\nedge in NLP mainly focuses on developing nega-\ntive knowledge bases that store relational negative\ncommonsense knowledge (Arnaout et al., 2021;\nSafavi et al., 2021; Arnaout et al., 2022) and utiliz-\ning negative knowledge within arguments or expla-\nnations to refute a candidate (Camburu et al., 2018;\nAggarwal et al., 2021; Chen et al., 2022). This pa-\nper is based on these resources to probe the belief\nof LLMs about the relations of everyday concepts\nthat are not true.\nUnderstanding Negation in Texts The manifes-\ntation of negative knowledge in texts is the phe-\nnomenon of negation (Horn and Wansing, 2022),\nwhich is difficult for pre-trained LMs to under-\nstand, e.g., filling “ birds cannot [MASK]” with\n“fly” (Kassner and Schütze, 2020). Negation has\nbeen shown to be spuriously correlated with neg-\native or contradictory labels due to the data dis-\ntribution (Gururangan et al., 2018; Ettinger, 2020;\nLai et al., 2021; Branco et al., 2021; Tian et al.,\n2022), raising doubts about the performance of pre-\nvious models. Furthermore, LMs may ignore the\nexistence of negative words when understanding\ntexts (Kassner and Schütze, 2020) or processing\nprompts (Jang et al., 2022), which can be allevi-\nated with unlikelihood training objective (Welleck\net al., 2020) during training (Hosseini et al., 2021)\nor specifying pragmatic contexts (Gubelmann and\n9891\nHandschuh, 2022). While most current research\nfocuses on NLU, this work fills in a gap in the\ninvestigation of the negation phenomenon in the\ncontext of text generation.\nKnowledge-Grounded Language Models A\nmajor goal of NLP has been to ground LMs in\nworld knowledge, such as factual knowledge (Vran-\ndeˇci´c and Krötzsch, 2014) and commonsense\nknowledge (Speer et al., 2017). A line of\nwork (Petroni et al., 2019; Kassner and Schütze,\n2020; Cao et al., 2021) directly probes the knowl-\nedge implicitly learned by LMs through mask-\ninfilling. However, such a probing paradigm only\nworks for contextual LMs such as BERT (De-\nvlin et al., 2019), leaving generative ones, espe-\ncially modern LLMs, understudied. Another line\nof work focuses on making LM-generated sen-\ntences grounded in knowledge (Petroni et al., 2020;\nLiu et al., 2021). Lin et al. (2020) designed a\nconstrained text generation task, COMMON GEN,\nwhich asks a model to generate a sentence given\na set of concepts, testing the generative common-\nsense reasoning of LMs. However, these studies do\nnot investigate text generation grounded in negative\nknowledge, which is the focus of this work.\nIn-Context Learning In-context learning (ICL;\nBrown et al., 2020) has become a prevailing\nparadigm for deploying LLMs (e.g., the GPT-3 fam-\nily Brown et al., 2020; Chen et al., 2021; Ouyang\net al., 2022) for downstream tasks. Through ICL,\nLLMs can solve tasks directly based on input-\noutput examples without parameter updates (Min\net al., 2022a; Rubin et al., 2022). Furthermore, re-\ncent work (Wei et al., 2022b; Wang et al., 2022)\nreveals that the ceiling performance determined by\nthe scaling law can be beaten with ICL by generat-\ning immediate rationales, i.e., the Chain of Thought\n(CoT) prompting. Since LLMs are becoming a\nfoundational service that do not need fine-tuning,\nour probing on LLMs are based on ICL.\n3 Probing Protocol\nIn this section, we set up an evaluation protocol\nto understand what LLMs know about (negative)\ncommonsense knowledge of everyday concepts.\n3.1 The CSK-PN Dataset\nWe limit the scope of the knowledge probed to\nrelational knowledge between commonsense con-\ncepts, i.e., relational knowledge triples , which\nDesires (n=73)\nIsA (n=813)\nHasA (n=139)\nHasProperty (n=405)\nCapableOf (n=533)\nMadeOf (n=37)NotMadeOf (n=12)\nNotCapableOf (n=537)\nNotHasProperty (n=190)\nNotHasA (n=69)\nNotIsA (n=108)\nNotDesires (n=1084)\n20.33%\n10.13%\n13.33%13.43%\n27.10%\nFigure 2: The configuration of the CSK-PN dataset.\nexist widely in knowledge graphs and are com-\nmonly studied by the community (Auer et al., 2007;\nVrandeˇci´c and Krötzsch, 2014; Speer et al., 2017).\nGiven a triplet in the form of <s,r,o> with a sub-\nject concept s, a relation rand an object concept\no, we define a negative fact as ¬r(s,o) if the truth\nvalue of r(s,o) is False according to common-\nsense knowledge, and a (positive) fact if otherwise.\nDataset Statistics We build the probing dataset\n(denoted as CSK-PN) based on the knowledge\ntriples filtered by Safavi et al. (2021), which are the\nchallenging ones sourced from ConceptNet (Speer\net al., 2017). We also remove invalid triples with\npronouns, negation, and adjectives as subjects or\nobjects. The final dataset contains a total of 4,000\ntriples with six pairs of positive or negative rela-\ntions (e.g., ISA and NOTISA), and the positive and\nnegative splits have the same size (1:1). Detailed\ninformation of CSK-PN is shown in Figure 2.\n3.2 Probing Task Formulation\nThe most commonly used probing task for under-\nstanding whether LMs have certain types of knowl-\nedge is mask-infilling (Devlin et al., 2019; Petroni\net al., 2020; Kassner and Schütze, 2020). However,\nthis task is not suitable for generative LMs, as the\nmask must exist at the end of a sentence.\nWe argue that LLMs, which are mainly autore-\ngressive text generation models (Radford et al.,\n2019; Brown et al., 2020; Ouyang et al., 2022; Scao\net al., 2022), should be investigated by text genera-\ntion with text decoding from a large sentence space.\nTherefore, we propose to useConstrained Sentence\nGeneration (CG) as the primary task to investigate\nLLMs, coupled with Boolean Question Answering\n(QA) for comparison, which is a common approach\nto probing the belief of models (Tafjord et al., 2022;\nRichardson et al., 2022).\n9892\nTask 1: Boolean Question Answering (QA)\nThe Boolean QA task requires LLMs to express\nits belief about a fact by answering a yes-or-no\nquestion. We first transform every triplet <s,r,o>\ninto a yes or no question q, where we remove the\nnegation in r for negative facts. For example, a\nprompt goes like this:\nAnswer commonsense questions with yes or no:\n(Examples for in-context learning)\nQuestion: do lions live in the ocean?\nAnswer: no\nwhere underlined texts are completed by LLMs. To\ngenerate the questions, we adopt InstructGPT us-\ning in-context learning (§4.1). The questions are\n94% valid according to a manual inspection of 50\nrandom cases.4\nTask 2: Constrained Sentence Generation (CG)\nGenerating texts is a direct manifestation of a\nmodel’s belief. However, evaluating generated\ntexts is notoriously difficult in NLP, especially with-\nout references. Therefore, we design a keyword-\nto-sentence task to make the probing more control-\nlable, which is similar to COMMON GEN (Lin et al.,\n2020). Given a triple <s,r,o>, models need to gen-\nerate sentences grounded in (negative) knowledge,\ni.e., add negation cues ( e.g., not, unable) in the\nsentence if necessary, e.g.,\nWrite a short and factual sentence according to\ncommonsense based on the keywords:\n(Examples for in-context learning)\nKeywords: lion, located at, ocean\nSentence: lions don’t live in the ocean.\nWe remove the NOT prefix from the negated re-\nlations. Note that we allow the paraphrasing of\nthe input keywords, making it a soft-constrained\nsentence generation task.\n3.3 Evaluation Metrics\nMetric for QA The QA task can be easily eval-\nuated by checking the generated token yes and no\n(cased and uncased). We define TP and TN as\nthe accuracy on the positive and negative splits in\nCSK-PN, and Acc as the accuracy on the whole\ndataset (i.e., Acc = (TP + TN)/2, since the posi-\ntive and negative splits have equal size). For rare\nscenarios (<1%) that LLMs do not generate ayes\nor no token, we compare the conditional probabil-\nity of these two tokens.\n4Bad cases are mostly due to the quality of the triples, e.g.,\n<swim, has property, full of water>: is swimming full of water?\nMetric for CG Due to the controlled task setting,\nwhich essentially forces LLMs to decide whether\nand how to add a negation cue during decoding,\nthe CG task can be efficiently evaluated by detect-\ning the existence of negation cues (e.g., not, un-\nable, etc.) in the generations. Following the QA\ntask, we also use TP and TN as accuracy metrics.\nTo implement this metric, we first use keywords-\nbased matching for negation cues, followed by a\nRoBERTa model (Liu et al., 2019) as atoken clas-\nsifier looking for unmatched negation cues.5 This\nmetric produces 1 or 0 based on the finding of nega-\ntion cues in a sentence. After manual inspection of\n200 cases, we find that this metric is correct 97%\nof the time, which is reliable for evaluating such\na constrained probing task. Errors are mostly due\nto double negations and ambiguous negative cues\n(e.g., less, opposite, etc.), which are quite rare.\nCan we trust negation detection as the metric to\nevaluate CG? We manually evaluate the factu-\nality of generated texts based on commonsense\nknowledge and see whether the CG metric (detec-\ntion of negation) correlates well with humans in\nthis task. Note that only the sentences that make\ncommon sense and adhere to the keywords con-\nstraints are accepted as true during manual anno-\ntation. After examining 100 cases, we find that\nthe agreement between human judgment and this\nmetric achieves 95%. This is predictable, since this\ntask is rather easy and constrained, yet LLMs do\nnot solve it well, especially not very consistent with\nthe QA task. Errors made by the metric are mostly\nbecause 1) generated sentences use uncertain ad-\nverbs to modify the sentences,e.g., may, some, etc.;\n2) noisy triples in the dataset. Overall, we think\nthis metric is trustworthy and evaluates this task far\nbetter than most popular text generation metrics.\n4 Do LLMs have negative commonsense\nknowledge?\nIn this section, we use CSK-PN to investigate\nLLMs’ belief about negative commonsense knowl-\nedge. More importantly, can LLMs generate texts\ngrounded in negative commonsense knowledge?\n4.1 Probing LLMs with In-Context Learning\nTo execute the probing tasks without fine-tuning,\nwe exploit the few-shot in-context learning (Brown\n5The model is trained on theCONDA QA dataset (Ravichan-\nder et al., 2022), which has 14,182 QA pairs with more than\n200 unique negation cues.\n9893\nModel k Perf. on QA Perf. on CG Cns.\nTP TN Acc TP TN Acc\nFlan-T5\n(3B)\n2 79.1 84.0 81.5 96.5 19.4 57.9 56.2\n10 82.7 80.2 81.4 96.9 19.8 58.4 59.7\nFlan-T5\n(11B)\n2 84.1 81.0 82.6 97.5 15.9 56.7 57.7\n10 85.4 80.8 83.1 97.6 28.2 62.9 65.9\nGPT-3 2 76.0 58.9 67.5 83.9 28.4 56.1 54.4\n10 74.7 66.9 70.8 30.9 79.8 55.3 53.7\nCodex002\n2 89.2 81.7 85.4 96.6 38.0 67.3 70.1\n10 88.1 81.8 84.9 93.2 68.8 81.0 84.5\nInstruct-\nGPTcurie\n001\n2 85.2 51.1 68.2 90.1 21.9 56.0 67.3\n10 70.0 65.8 67.9 71.5 40.8 56.1 58.2\nInstruct-\nGPT001\n2 78.1 83.6 80.9 94.9 25.0 60.0 57.7\n10 79.5 81.6 80.6 79.2 55.4 67.3 68.2\nInstruct-\nGPT002\n2 81.7 86.1 83.9 92.9 48.7 72.1 71.2\n10 84.1 84.7 84.4 88.9 61.4 75.1 77.5\nInstruct-\nGPT003\n2 87.9 81.3 84.6 95.1 58.1 76.6 80.5\n10 89.0 79.5 84.2 91.1 73.6 82.3 87.9\nChatGPT 2 82.9 82.0 82.4 89.8 69.8 79.8 79.2\n10 81.5 85.7 83.6 90.4 78.4 84.4 84.1\nTable 1: Main results of different LLMs, which are\nobtained with kexamples (|E+|= |E−|). Cns. denotes\nthe consistency between QA and CG. The best results\nare bolded and the second best are underlined.\net al., 2020) ability of LLMs. We manually write 32\nexamples, with 16 examples for positive knowledge\n(denoted as E+) and 16 for negative knowledge\n(E−).6 In the experiments, we randomly sample\na total number of k examples from E+ and E−,\nwhere |E+|= |E−|if not specified.7\nChoices of LLMs We use LLMs that can do\nin-context learning, so that models stay fixed dur-\ning probing. We choose Flan-T5 (Chung et al.,\n2022), GPT-3 (175B, davinci; Brown et al.,\n2020) and GPT-3.5 series, e.g. Codex (≥175B,\ncode-davinci-002; Chen et al., 2021) and\nInstructGPT (Ouyang et al., 2022): all are ca-\npable of in-context learning. Flan-T5 is an\nencoder-decoder LLM with instruction tuning\nbased on T5 (Raffel et al., 2020). Codex ex-\ntends GPT-3 through code training and instruc-\ntion fine-tuning, and InstructGPT extends Codex\nthrough further tuning of the instructions. In our\nexperiments, we mainly explore GPT-3.5 mod-\nels. We use the 6.7B variant of InstructGPT\n(text-curie-001) and the ≥175B variants,\ni.e., text-davinci-001 (tuned on instruc-\ntions), text-davinci-002 (tuned on code and\n6Examples can be found in Appendix A.1\n7Example prompts for two tasks are in Appendix A.2.\ninstructions), and text-davinci-003 (further\ntuned with reinforcement learning with human\nfeedback, RLHF).8 For deterministic predictions,\nall models use greedy decoding (temperature as\n0.0)9. We use InstructGPT002 as the default LLM\nfor experiments due to its powerful capability and\nthe fact that it has been extensively researched and\napplied as of the time of writing this paper. We\nalso include the recent ChatGPT (OpenAI, 2022),\nwhich is built upon InstructGPT and trained with\ndialogue data and RLHF.\n4.2 The Belief Conflict\nWe report the results of the probing tasks in Table 1\nfor LLMs with 2- and 10-shot in-context learning.\nBased on the results, we discover a clear conflict\nof LLMs, that LLMs behave inconsistently in QA\nand CG tasks on negative commonsense knowl-\nedge, which we term belief conflict. Such conflict\nmanifests itself in two ways: the gap between TP\nand TN on the CG task, and the gap of TN be-\ntween the QA and CG tasks. In general, belief\nconflicts exist across LLMs of various sizes and\nstructures. Ablated results per relation is presented\nin Appendix B.3.\nWhen specifically asked, LLMs can distin-\nguish between positive and negative commonsense\nknowledge, as evidenced by stable and balanced\nscores for positive and negative splits in the QA\ntask. For CG, LLMs seem to accurately gener-\nate sentences grounded in positive knowledge ac-\ncording to TP. However, they perform poorly in\nnegative knowledge, even for the best-performing\nLLMs, i.e., Codex 002, InstructGPT 002,003, as\nshown by the lower bars of the CG on the neg-\native split.10 Also, the inconsistency between QA\nand CG reflects this conflict, as the content gener-\nated by a trustworthy AI system should consistent\nand faithful to what it believes. We present a case\nstudy and error analysis in Appendix B.5.\nAmong these LLMs, InstructGPT003 and Chat-\nGPT achieve much better results than others. We\nassume that such improvements are probably a re-\nsult of training LLMs with human feedback (e.g.,\n8https://beta.openai.com/docs/\nmodel-index-for-researchers\n9We find our findings in the experiments are consistent for\ndifferent temperatures, according to Appendix B.1.\n10The only exception is GPT-3 ( davinci). It scores\npoorly on the positive split with 10-shot learning, with TN\nexceeding TP. This happens when k ≥ 4, while its 6.7B\nvariant (curie) behaves consistently with others. Detailed\nresults for GPT-3 are in Appendix B.2.\n9894\n024 8 16 3220\n40\n60\n80\n100\nk-shot ICL\nTP\nTN\nAcc\n(a) Results (%) on QA.\n024 8 16 3220\n40\n60\n80\n100\nk-shot ICL\nTP\nTN\nAcc\n(b) Results (%) on CG.\nFigure 3: Performance change for InstructGPT 002 on\nboth tasks as the number of example (k) increases.\nRLHF) based on the disclosed differences between\nthem by OpenAI. Another evidence is that the re-\ncent ChatGPT also expresses great capabilities of\ngenerating negative knowledge, even better than\nInstructGPT003 in this regard. We hypothesize that\nthis is because negative knowledge and rebuttal\nstatements are frequently used in human feedback\nto steer the model, e.g., admitting errors or instruct-\ning the model not to do something. To validate\nthis claim, future work could conduct more rigor-\nous comparisons on public available LLMs, which\nwould be an interesting research problem to trace\ncertain abilities of LLMs to a specific period of\ntraining.\nSensitivity to the Number of In-Context Exam-\nples To find whether adding more examples helps\nsolve the probing tasks, we increase the in-context\nexamples from 0 to 32. Figure 3(a) shows a con-\nsistent finding with previous results, that LLMs are\nso good at answering yes or no questions that the\nnumber of examples does not affect much of the\nQA performance. Figure 3(b) shows that, adding\nmore examples helps generate both positive and\nnegative commonsense knowledge. However, the\ngap between TP and TN in the CG task still exists.\n5 Analysis on the Belief Conflict\n5.1 Could keywords as task input hinder the\nmanifestation of LLMs’ belief?\nThe task input difference for CG and QA leads to a\nconcern that LMs may find it easier to understand\nnatural questions (QA) than keywords (CG); hence,\nthe belief conflict. In response to this concern, we\nchange the input of the two tasks. For example, the\nkeywords-to-answer task takes the form as:\nCan these keywords form a truthful common sense\nfact? Answer with yes or no.\nKeywords: lion, located at, ocean\nAnswer: no\nTP TN Acc\n0\n20\n40\n60\n80\n100\nquestion-to-answer\nkeywords-to-answer\n(a) Results (%) on QA.\nTP TN Acc\n0\n20\n40\n60\n80\n100\nkeywords-to-sentence\nquestion-to-sentence\n(b) Results (%) on CG.\nFigure 4: Results of InstructGPT 002 when switching\nthe task inputs between question and keywords, where\nk= 10. Columns with error bars show the ranges of the\ninfluence brought by different instruction wordings.\nAs for the question-to-sentence task:\nAnswer the question by writing a short sentence\nthat contains correct common sense knowledge.\nQuestion: do lions live in the ocean?\nSentence: lions don’t live in the ocean.\nResults In Figure 4(a), we see a 4-point perfor-\nmance decrease given keywords as input for QA,\nwhich is not significant in comparison, and the\nresults on the positive and negative splits are as\nbalanced as before. This implies that LLMs’ imbal-\nanced performance in CG is not due to the use of\nkeywords as input. In Figure 4(b), CG performance\nis greatly improved givenquestion as input, approx-\nimating QA results. Our assumption is that CG is\nbasically transformed into QA, because the textual\ncorpus has seen too many negated texts following\na Boolean question and rephrasing it, e.g., “...? No,\nlions do not live in the ocean.” To validate this, we\nprovide LLMs with zero-shot question-to-sentence\ninstructions, and check if the output sentences start\nwith yes or no given an input question. If our\nassumption is correct, models without examples\nwill be biased toward QA even with a question-to-\nsentence instruction. The results of models opti-\nmized for instructions show that: 84.58% of sen-\ntences generated by InstructGPT002 begin with yes\nor no, and 80.28% for InstructGPT 003. With 10\nexamples, this number drops to less than 4%. Thus,\nthese results confirms that question-to-sentence\ngeneration degenerates to the QA task.\nAs a result, we conclude that the keyword-to-\nsentence (CG) is an appropriate and challenging\ntask to probe generative LLMs. Employing key-\nwords as input does not impact LLMs’ grasp of the\ntask (Figure 4(a)), while using questions as input\nmay produce shortcuts that obscure whether LLMs\ncan generate texts of negative commonsense knowl-\nedge (Figure 4(b)). Even if we use different instruc-\n9895\ntion wordings (instructions are at Appendix A.2),\nnone escapes the belief conflict, as shown by the\nerror bars in Figure 4. Additionally, this experi-\nment brings up the problem of how LLMs encode\ncommonsense knowledge. According to this exper-\niment, commonsense knowledge seems to be stored\nin LLMs in the same manner as it is in the corpus.\nLLMs struggle to generalize them, as evidenced by\nthe keyword inputs for negative knowledge that do\nnot have a statistical shortcut from pre-training.\n5.2 Will the keyword co-occurrence within\ncorpus affect LLMs’ generation?\nLLMs are essentially statistical models. In this\nexperiment, we investigate the influence of word\nco-occurrence in the corpus on the CG task, which\nis one of the most common statistical factors. We\ncategorize the dataset into buckets based on key-\nwords co-occurrence on naturally existing corpora\nsuch as OMCS (706K sentences, Singh et al., 2002)\nand Wikipedia (1M, a subset built by Gao et al.\n(2021)). The co-occurrence for each triple is calcu-\nlated by\n∑\ni,j cooccur(wi,wj)\nlslo , where wi ∈s,wj ∈o,\nand ls,lo denote the word count of subject sand\nobject o, discarding stopwords.\nFrom Figure 5, we have an interesting finding\nthat three of the best-performing LLMs from Ta-\nble 1 suffer from a performance drop at the>1000\nbucket of the negative split (TN), the most frequent\ndata bucket. In contrast, LLMs achieve the best per-\nformance this bucket on the positive split (TP). We\nconclude that the hard-to-generate negative knowl-\nedge for LLMs tend to be those in which they have\nseen many subjects and objects appear together.\nFor example, worm and bird usually co-occur in\nsentences, but models tend to generate “worms can\neat birds.” Such statistical shortcuts hinder the\ngeneration of negative knowledge. This is also val-\nidated by TP results, where LLMs find it easy to\ngenerate sentences with frequently co-occurring\nentities in a positive fact.\n5.3 How does the balance of positive and\nnegative examples affect negation bias?\nA possible answer for the difference between CG\nand QA is that: LMs suffer from reporting bias\nof negation during pre-training, while answering\nquestions with yes or no is quite balanced in the\ncorpora. We validate this problem by mitigating\nthe negation bias through adjusting the examples\nof positive and negative cases. With more E−s,\n0~100\n(n=1399)\n100~200\n(n=227)\n200~500\n(n=196)\n500~1000\n(n=74)\n>1000\n(n=104)\n70\n80\n90\n100TP (%)\n code-davinci-002\ntext-davinci-002\ntext-davinci-003\n0~100\n(n=1189)\n100~200\n(n=245)\n200~500\n(n=306)\n500~1000\n(n=134)\n>1000\n(n=126)\nKeywords Co-occurrence Buckets\n40\n50\n60\n70\n80TN (%)\nFigure 5: 10-shot CG results of three best-performing\nLLMs on different co-occurrence buckets. a ∼bde-\nnotes that keywords co-occurrence in a bucket ranges\nfrom ato b. nis the number of triples in a bucket.\nLLMs are encouraged to generate more negations.\nResults Figure 6(a), 6(b) adjust the ratio η =\n|E−|\nk while fixing k. Figure 6(a) shows that\nInstructGPT002 is very resilient against the ex-\nample ratio in the QA task, except for extreme\ncases where only E+s or E−s are presented (i.e.,\nη ∈{0,1}). This also demonstrates the robust-\nness of adopting QA results as LLMs’ belief. In\nFigure 6(b), the CG performance on the negative\nsplit is improving as η grows. The turning point\nappears somewhere near η ∈(0.9,1) when E−\ntakes over all the examples. Also, TP drops as\nE+ becomes less. What if we add E− without\ndropping E+? In Figure 6(c), 6(d), we keep E+\nas constant ( |E+|= 5) and increase |E−|from\n5 to 15. With enough amount of E+, TN to CG\ncontinues to increase without sacrificing TP.\nOverall, Figure 6 presents the possibility that we\ncan overcome the belief conflict brought about by\nreporting bias by increasing negated texts in the\ntraining data or in-context examples. However, this\nis not always feasible in practice.\n5.4 Do Chain-of-Thought help generate texts\nwith negative commonsense knowledge?\nCan the implicit reporting bias be overcome by\nexplicit reasoning? Recent studies (Wei et al.,\n2022b,a) discover that the Chain-of-Thought (CoT)\nprompting technique shows the emergent reason-\ning abilities of LLMs. CoT generates intermediate\nsteps in natural language, extending <input, output>\nto <input, chain-of-thought, output>. We adopt\ntwo instances of CoT: deductive reasoning and fact\ncomparison, whose examples are manually written,\n9896\n0 0.2 0.4 0.6 0.8 120\n40\n60\n80\n100\nηQA (k= 10)\nTP\nTN\nAcc\n(a) Results (%) on QA.\n0 0.2 0.4 0.6 0.8 120\n40\n60\n80\n100\nηCG (k= 10)\nTP\nTN\nAcc\n(b) Results (%) on CG.\n5 7 9 11 13 1520\n40\n60\n80\n100\n|E−|(k= 5 +|E−|)\nTP\nTN\nAcc\n(c) Results (%) on QA.\n5 7 9 11 13 1520\n40\n60\n80\n100\n|E−|(k= 5 +|E−|)\nTP\nTN\nAcc\n(d) Results (%) on CG.\nFigure 6: Results of InstructGPT 002 as the numbers\nof E+ and E− change. Figure (a) and (b) increase\nη = |E−|/k while fixing k = 10. Figure (c) and (d)\nadd more E− while fixing |E+|= 5.\nwhich are in Appendix A.1.\nDeductive Reasoning Prompting We instantiate\nCoT with deductive argumentation in the form of\nsyllogism (two premises and one conclusion). The\nprompt is extended into <input, “ Let’s think step\nby step: ...”, output> with intermediate steps. A\nnatural way to identify a negative proposition is de-\nductive reasoning with modus tollens, i.e., denying\nthe consequent (Speranza and Horn, 2010; Bobzien,\n2020): “If P then Q. Not Q. Therefore, Not P.” For\nexample, “If something is a intelligent being (P),\nthen it must have the ability to think (Q). Comput-\ners cannot think (Not Q). Therefore, computers are\nnot intelligent beings (Not P).”\nTo reason about positive propositions, we\nuse modus ponens logic, i.e., affirming the an-\ntecedent (Bobzien, 2020): “If P then Q. P. There-\nfore, Q.” For example, “ Things with lightweight\nbodies and strong wing muscles (P) can usually\nfly (Q). Birds have these physical characteristics\n(P). Therefore, birds can fly. (Q)” Notice that the\ndeduction is not strictly logical but is enough to\narrive at commonsense knowledge.\nFact Comparison Prompting Deduction empha-\nsizes the intensional aspects of the fact, whereas\nfact comparison highlights the extensional compar-\nison between counterpart facts (Fitting, 2006). For\nModel CoT k= 2(1:1) k= 10(1:1)\nTP TN Acc TP TN Acc\nCodex002\nNone 96.6 38.0 67.3 93.2 68.8 81.0\nDeduction 86.9 56.6 71.7 83.5 73.0 78.3\nFact 92.9 53.7 73.3 86.8 76.6 81.7\nInstruct-\nGPT002\nNone 92.9 51.4 72.1 88.9 61.4 75.1\nDeduction 87.0 57.3 72.1 84.3 70.7 77.5\nFact 89.1 55.5 72.2 85.5 69.2 77.4\nTable 2: Performance on the CG task when enhanced\nwith different types of CoT prompting, i.e., deductive\nargumentation (Deduction) and fact comparison (Fact).\nexample, the related fact for “lions do not live in\nthe ocean” is “lions live in the land”. A negative\nfact often comes with a core fact that is true, which\nhas been shown to be useful in explaining why a\nclaim is wrong (Cheng et al., 2022). Therefore, we\nextend the <input, output> in each example by <in-\nput, “Related fact: ...”, output>. For positive cases,\nwe write a related fact for consistent examples.\nResults Table 2 displays the results of Codex002\nand InstructGPT002. Both CoT instances improve\nLLMs’ performance on TN, showing the benefit\nof explicit reasoning for deriving negative knowl-\nedge, where different models prefer different ratio-\nnales. However, the increase in TN comes at the ex-\npense of a performance drop in TP. This is mostly\nbecause models previously predicted most of the\ncases to be positive, making TP irrationally high.\nOverall, these results suggest that, even though\nLLMs picked up implicit bias during pre-training,\nit can be overcome by making the reasoning chain\nexplicit.\nNevertheless, deductive reasoning seems to be\nmore rigid about confirming commonsense knowl-\nedge with a lower TP. This can be attributed to\nthe fact that commonsense knowledge contains ex-\nceptions (Allaway et al., 2022), e.g., birds can fly\nbut penguins can’t. Thus, LLMs with deductive\nreasoning may hold concerns about exceptions for\nconfirming a commonsense fact, leading to a signif-\nicant lower TP than fact comparison. We conduct a\nsimple experiment of exceptions in Appendix B.4,\nwhich shows that adding adverbs of degree ( e.g.,\nusually, generally) in the texts alleviates the belief\nconflict, but the problem still exists.\n6 Closing Remarks\nIn this study, we explored and quantified the lim-\nitations of LLMs in generating texts grounded in\n9897\nnegative commonsense knowledge that they seem\nto know, a phenomenon we term as “belief con-\nflict”. To investigate this, we probe LLMs with a\nconstrained sentence generation (CG) task, coupled\nwith a QA task. Our experiments demonstrated the\nexistence of the belief conflict in all LLMs when\nit comes to negative knowledge, which is mostly\nbrought by quantifiable statistical shortcuts such as\nkeywords co-occurrence. We also see that this can\nbe lessened by giving more in-context examples of\nnegative knowledge or by using a chain-of-thought\n(CoT) prompting method to explain the explicit rea-\nsoning process for deriving negative knowledge.\nWith the rapid increase of the study on language-\nbased reasoning (Clark et al., 2020; Tafjord et al.,\n2021; Wei et al., 2022b), there would be cause for\nconcern if LLMs have trouble generating proofs or\nreasoning steps with negative knowledge. With all\nthe good scores they achieve at QA tasks, whether\nthey can be trusted with their knowledge expressed\nduring generation, which is one of the most promi-\nnent way of human-AI interaction, is still question-\nable. In this sense, the study of negative knowledge\ncreates a good testbed for assessing real language-\nbased reasoning skills for LLMs without the statis-\ntical heuristics they memorized. We hope that the\nfindings in this work could raise the awareness of\nthe community on negative knowledge for LLMs\nin downstream text generation tasks.\nLimitations\nIn this work, we highlight that the probing tasks are\nplaced in the commonsense domain that are gen-\nerally acknowledged by people in most situations.\nWe do not consider the exceptions of commonsense\nknowledge, which has gradually drawn some re-\nsearch attentions (Do and Pavlick, 2021; Allaway\net al., 2022). Exceptions are important for negative\nknowledge and are widely used in tasks such as\nargumentation or deductive reasoning. However,\nin the experiments, we find that such exceptions\nmight make models generate commonsense state-\nments with uncertain adverbs (e.g., may, some, etc.)\non rare cases.\nAnother limitation of this work is that the prob-\ning task is based only on relational commonsense\nknowledge from commonsense knowledge bases\nsuch as ConceptNet. We design the keyword-to-\nsentence task mostly for the purpose of convenient\nevaluation for text generation, which is notoriously\nknown as difficult. The probing and evaluation of\nLLMs’ belief about negative knowledge in more\ncomplex tasks are beyond the scope of this work,\nbut really interesting and challenging. Also, other\ntypes of knowledge could be studied in a similar\nway, such as negative social, temporal and spatial\nknowledge, to name but a few.\nIn this paper, we identify the belief conflict prob-\nlem in LLMs through extensive experiments. Fu-\nture work could explore more advanced training\nor prompting-based methods to improve the con-\nsistency between a model’s belief and its actions\n(text generation for various tasks), especially for\nnegative knowledge.\nEthical Statement\nThe commonsense knowledge triples from Con-\nceptNet may include offensive and biased sen-\ntences, which may also exist in the dataset that we\nuse in this work. As stated before, the identification\nof commonsense negative knowledge may slightly\nvary from people from different cultural and social\nbackground when considering exceptions.\nAcknowledgement\nWe thank the anonymous reviewers for their valu-\nable comments. We also thank Siyu Yuan and\nJian Xie from Fudan University, and Kexun Zhang,\nYujian Liu, Qingxiu Dong and Xuandong Zhao\nfrom UC Santa Barbara for their useful sugges-\ntions and discussions for the manuscript. This re-\nsearch is funded by the Science and Technology\nCommission of Shanghai Municipality Grant (No.\n22511105902).\nReferences\nShourya Aggarwal, Divyanshu Mandowara, Vishwa-\njeet Agrawal, Dinesh Khandelwal, Parag Singla, and\nDinesh Garg. 2021. Explanations for Common-\nsenseQA: New Dataset and Models. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3050–3065, Online.\nAssociation for Computational Linguistics.\nEmily Allaway, Jena D Hwang, Chandra Bhagavatula,\nKathleen McKeown, Doug Downey, and Yejin Choi.\n2022. Penguins don’t fly: Reasoning about generics\nthrough instantiations and exceptions. arXiv preprint\narXiv:2205.11658.\nHiba Arnaout, Simon Razniewski, Gerhard Weikum,\nand Jeff Z. Pan. 2021. Negative knowledge for open-\nworld wikidata. In Companion Proceedings of the\n9898\nWeb Conference 2021, WWW ’21, page 544–551,\nNew York, NY , USA. Association for Computing\nMachinery.\nHiba Arnaout, Simon Razniewski, Gerhard Weikum,\nand Jeff Z. Pan. 2022. Uncommonsense: Informative\nnegative knowledge about everyday concepts. In Pro-\nceedings of the 31st ACM International Conference\non Information & Knowledge Management, CIKM\n’22, page 37–46, New York, NY , USA. Association\nfor Computing Machinery.\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\nLehmann, Richard Cyganiak, and Zachary Ives. 2007.\nDbpedia: A nucleus for a web of open data. In The\nsemantic web, pages 722–735. Springer.\nStephen Barker and Mark Jago. 2012. Being positive\nabout negative facts. Philosophy and Phenomenolog-\nical research, pages 117–138.\nSusanne Bobzien. 2020. Ancient Logic. In Edward N.\nZalta, editor, The Stanford Encyclopedia of Philos-\nophy, Summer 2020 edition. Metaphysics Research\nLab, Stanford University.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nRuben Branco, António Branco, João António Ro-\ndrigues, and João Ricardo Silva. 2021. Shortcutted\ncommonsense: Data spuriousness in deep learning\nof commonsense reasoning. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1504–1521, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language explana-\ntions. In Advances in Neural Information Processing\nSystems, volume 31. Curran Associates, Inc.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1860–1874, Online.\nAssociation for Computational Linguistics.\nJiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao\nLi, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua\nXiao, and Hao Zhou. 2022. E-KAR: A benchmark\nfor rationalizing natural language analogical reason-\ning. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022 , pages 3941–3955,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nSijie Cheng, Zhiyong Wu, Jiangjie Chen, Zhixing Li,\nYang Liu, and Lingpeng Kong. 2022. Unsuper-\nvised explanation generation via correct instantia-\ntions. arXiv preprint arXiv:2211.11160.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In Pro-\nceedings of the Twenty-Ninth International Joint Con-\nference on Artificial Intelligence, IJCAI-20 , pages\n3882–3890. International Joint Conferences on Arti-\nficial Intelligence Organization. Main track.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNam Do and Ellie Pavlick. 2021. Are rotten apples edi-\nble? challenging commonsense inference ability with\nexceptions. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n2061–2073, Online. Association for Computational\nLinguistics.\n9899\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nMelvin Fitting. 2006. Intensional logic.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nReto Gubelmann and Siegfried Handschuh. 2022. Con-\ntext matters: A pragmatic study of PLMs’ negation\nunderstanding. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 4602–4621,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel Bowman, and Noah A. Smith.\n2018. Annotation artifacts in natural language infer-\nence data. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 107–112,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nLaurence R. Horn and Heinrich Wansing. 2022. Nega-\ntion. In Edward N. Zalta and Uri Nodelman, editors,\nThe Stanford Encyclopedia of Philosophy , Winter\n2022 edition. Metaphysics Research Lab, Stanford\nUniversity.\nMd Mosharaf Hossain, Dhivya Chinnappa, and Eduardo\nBlanco. 2022. An analysis of negation in natural lan-\nguage understanding corpora. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n716–723, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nArian Hosseini, Siva Reddy, Dzmitry Bahdanau, R De-\nvon Hjelm, Alessandro Sordoni, and Aaron Courville.\n2021. Understanding by understanding not: Model-\ning negation in language models. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1301–1312,\nOnline. Association for Computational Linguistics.\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2022. Can\nlarge language models truly understand prompts? a\ncase study with negated prompts. arXiv preprint\narXiv:2209.12711.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations.\narXiv preprint arXiv:2205.11822.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. Asso-\nciation for Computational Linguistics.\nNora Kassner, Oyvind Tafjord, Hinrich Schütze, and\nPeter Clark. 2021. BeliefBank: Adding memory to a\npre-trained language model for a systematic notion\nof belief. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 8849–8861, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nYuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang,\nand Dongyan Zhao. 2021. Why machine reading\ncomprehension models learn shortcuts? In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021 , pages 989–1002, Online.\nAssociation for Computational Linguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S. Yu.\n2021. Kg-bart: Knowledge graph-augmented bart\nfor generative commonsense reasoning. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n35(7):6418–6425.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nCharles MacDonald. 1965. The role of negation in hu-\nman knowledge. Laval théologique et philosophique,\n21(1):80–114.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022a. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791–2809, Seattle, United States.\nAssociation for Computational Linguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022b. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nMarvin Minsky. 1997. Negative expertise.\nGeorge Molnar. 2000. Truthmakers for negative truths.\nAustralasian Journal of philosophy, 78(1):72–86.\n9900\nOpenAI. 2022. Chatgpt.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRocktäschel, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. In Automated\nKnowledge Base Construction.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAbhilasha Ravichander, Matt Gardner, and Ana Maraso-\nvi´c. 2022. Condaqa: A contrastive reading compre-\nhension dataset for reasoning about negation. arXiv\npreprint arXiv:2211.00295.\nEhud Reiter. 2019. Natural language generation\nchallenges for explainable ai. arXiv preprint\narXiv:1911.08794.\nKyle Richardson, Ronen Tamari, Oren Sultan, Reut\nTsarfaty, Dafna Shahaf, and Ashish Sabharwal.\n2022. Breakpoint transformers for modeling\nand tracking intermediate beliefs. arXiv preprint\narXiv:2211.07950.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655–2671, Seattle, United States.\nAssociation for Computational Linguistics.\nTara Safavi, Jing Zhu, and Danai Koutra. 2021.\nNegatER: Unsupervised Discovery of Negatives in\nCommonsense Knowledge Bases. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 5633–5646, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nPush Singh, Thomas Lin, Erik T. Mueller, Grace Lim,\nTravell Perkins, and Wan Li Zhu. 2002. Open mind\ncommon sense: Knowledge acquisition from the gen-\neral public. In On the Move to Meaningful Internet\nSystems 2002: CoopIS, DOA, and ODBASE, pages\n1223–1237, Berlin, Heidelberg. Springer Berlin Hei-\ndelberg.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. Proceedings of the AAAI Conference\non Artificial Intelligence, 31(1).\nJ.L. Speranza and Laurence R. Horn. 2010. A brief his-\ntory of negation. Journal of Applied Logic, 8(3):277–\n301.\nTheodore R Sumers, Robert D Hawkins, Mark K Ho,\nand Thomas L Griffiths. 2021. Extending rational\nmodels of communication from beliefs to actions.\narXiv preprint arXiv:2105.11950.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3621–3634, Online.\nAssociation for Computational Linguistics.\nOyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark.\n2022. Entailer: Answering questions with faithful\nand truthful chains of reasoning. arXiv preprint\narXiv:2210.12217.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nBing Tian, Yixin Cao, Yong Zhang, and Chunxiao Xing.\n2022. Debiasing nlu models via causal intervention\nand counterfactual reasoning.\n9901\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78–85.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research . Survey Certifica-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\nInternational Conference on Learning Representa-\ntions.\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2022. A\nsurvey of knowledge-enhanced text generation. ACM\nComputing Surveys (CSUR).\n9902\nTask 1: Boolean Question Answering (QA)\nAnswer the commonsense questions with yes or no:\n/* Examples */\nQuestion: can birds fly?\nAnswer: yes\n###\nQuestion: is water spicy?\nAnswer: no\n/* Test data */\nQuestion: are needles used for writing?\nAnswer: no\nTask 2: Constrained Sentence Generation (CG)\nWrite a short and factual sentence according to common-\nsense based on the keywords:\n/* Examples */\nKeywords: birds, capable of, fly\nSentence: birds can fly.\n###\nKeywords: water, has property, spicy\nSentence: water isn’t spicy.\n/* Test data */\nKeywords: needles, used for, writing\nSentence: needles are not used for writing.\nTable 3: Example prompts of the two probing tasks for\nin-context learning, which consists of a task instruc-\ntion at the beginning and several in-context examples.\nUnderlined texts denote the model completion.\nA Demonstrations for In-Context\nLearning\nA.1 Manually-written Examples for\nIn-Context Learning\nSome of the manually designed examples are\nshown in Table 6.\nA.2 Example Prompts for the Probing Tasks\nThe task inputs to the LLMs are presented in Ta-\nble 3. Note that instructions can be replaced by\nothers. LLMs with in-context learning are known\nto be sensitive to the wording and examples in the\nprompts (Min et al., 2022b). Therefore, we manu-\nally write 4 interchangeable instructions for each\nprobing tasks. For the QA task, the instructions\ninclude:\n1. Answer the commonsense questions with yes\nor no.\n2. Choose “yes” or “no” to indicate whether\nyou agree or disagree with the commonsense\nquestions.\n3. Respond to the questions using “yes” or\n“no”.\n4. Indicate whether the commonsense questions\nare correct or incorrect by writing “yes” or\n“no”.\n0 0.20.40.60.8 10\n20\n40\n60\n80\n100\ntemperature\nTP\nTN\nAcc\n(a) Results (%) on QA.\n0 0.20.40.60.8 10\n20\n40\n60\n80\n100\ntemperature\nTP\nTN\nAcc\n(b) Results (%) on CG.\nFigure 7: Performance change for InstructGPT 002 on\nboth tasks as the temperature changes.\n0 2 4 6 10 160\n20\n40\n60\n80\n100\nk-shot ICL\nTP\nTN\nAcc\n(a) GPT-3 davinci (175B)\n0 2 4 6 10 160\n20\n40\n60\n80\n100\nk-shot ICL\nTP\nTN\nAcc\n(b) GPT-3 curie (6.7B)\nFigure 8: CG results of GPT-3 for thedavinci (175B)\nand curie (6.7B) variants, where |E−|= |E+|. Un-\nlike other LLMs, the TN of GPT-3 surpasses TP when\nk≥4.\nFor the CG task, the instructions include:\n1. Write a short and factual sentence according\nto commonsense based on the keywords:\n2. Use the keywords to create a short and fac-\ntual sentence that accurately reflects common-\nsense knowledge.\n3. Create a short, factual sentence based on the\nkeywords and what is generally accepted as\ntrue.\n4. Construct a factual and concise statement\nbased on the provided keywords and common-\nsense knowledge.\nB Additional Results\nB.1 Sensitivity to Temperature Tuning\nFigure 7 shows that temperature does not influence\nmuch of the performance, thus the findings of this\npaper are not sensitive to temperature tuning.\nB.2 Abnormal Results of GPT-3 ( davinci)\nDifferent from the trends of other LLMs reported\nin §4.2, GPT-3 davinci shows a confusing pat-\ntern of the results on the CG task. A more de-\n9903\nT otal CapableOf Desires MadeOf IsA HasProperty HasA\n40\n50\n60\n70\n80\n90\n100TP (%)\nT otal NotCapableOf NotDesires NotMadeOf NotIsA NotHasProperty NotHasA\nRelation\n40\n50\n60\n70\n80\n90\n100TN (%)\ncode-davinci-002 text-davinci-002 text-davinci-003\nFigure 9: The 10-shot CG results per relation type on\nCSK-PN. The results are obtained with 10-shot learning.\nndenotes the triple number per relation.\ntailed experiment in Figure 8(a) shows that, when\nk< 4, GPT-3 (davinci) performs similarly with\nits sibling LLMs, with TP greatly surpasses TN. TN\ncontinues to enlarge as kincreases, even beating\nTP. Based on Acc over the whole dataset, GPT-3\ndoes not achieve results as good as other GPT-3\nderivatives. However, a smaller version of GPT-\n3 (i.e., curie, 6.7B) does not express such pat-\ntern, according to Figure 8(a). We do not have\nproper reasons for this finding, but further train-\ning on code and instruction tuning (i.e., Codex and\nInstructGPT) seem to fix this problem.\nB.3 Results of Different Relation Types\nWhat types of relations do LLMs find the most dif-\nficult to verbalize? As seen in Figure 9, we see\nLLMs achieve good results in the positive split.\nOn the negative split, LLMs unanimously believe\nNOTHASPROPERTY to be the most difficult rela-\ntions.\nB.4 Do LLMs hold concerns about exceptions\nfor commonsense knowledge?\nCommonsense knowledge usually comes with ex-\nceptions. Could LLMs answer or generate com-\nmonsense knowledge incorrectly be because they\nare thinking about exceptions? For example, “birds\ncan fly, but penguins cannot. ” (Allaway et al.,\n2022). So when asked “ can birds fly? ”, LLMs\nmay think of a counterexample and thus arrive at\nthe answer no. We rephrase the in-context exam-\nples by adding adverbs of degree (e.g., typically,\ngenerally, usually, most, etc.) to make the tasks be\nabout the commonsense instead of exceptions. For\ninstance, we rewrite “can birds fly?” into “can most\nbirds fly?” or “can birds generally fly?”, and “lions\nModel Exception Perf. on QA Perf. on CG\nTP TN Acc TP TN Acc\nCodex002\n- 88.1 81.8 84.9 93.2 68.8 81.0\n✓ 87.2 79.6 83.4 91.9 72.2 82.1\nInstruct-\nGPT002\n- 84.1 84.7 84.4 88.9 61.4 75.1\n✓ 84.0 85.4 84.7 90.9 70.1 80.5\nTable 4: 10-shot QA and CG results of LLMs when\nadding adverbs of degree into texts, making them some-\nhow consider exceptions of commonsense knowledge.\nWell generated negated sentence\n1 Triple <deer, desires, be shoot by hunter>\nLabel negative\nGeneration Deer do not desire to be shot by hunters.\nWeak negation\n2 Triple <person, desires, eat alone>\nLabel negative\nGeneration Some people desire to eat alone.\nUnfaithful generated sentence\n3 Triple <student, desires, exam>\nLabel negative\nGeneration Students generally desire to do well on\nexams.\nWrong data label\n4 Triple <horse, is a, bird>\nLabel positive\nGeneration horses are not birds.\nHigh co-occurrence words\n5 Triple <worm, capable of, eat bird>\nLabel negative\nGeneration Worms can eat birds.\nTable 5: Examples of the generated texts by\nInstructGPT002 on the CG task.\ndon’t live in the ocean.” into “lions don’t usually\nlive in the ocean.” In this way, we make language\nexplicitly convey uncertainty (Reiter, 2019) and try\nto rule out exceptions in the tasks.\nBased on the results in Table 4, we find that\nadding adverbs of degree to the texts does im-\nprove LLMs’ performance on both CG and QA.\nThis suggests that LLMs do hold a certain amount\nof concerns toward exceptions when dealing with\ncommonsense reasoning, especially for negative\nknowledge. However, considering exceptions with\nthis trick still does not resolve the belief conflict.\nAlso, this approach could also serve as a useful\ntrick for future commonsense research.\nB.5 Case Study\nTable 5 presents some examples of generated by\nInstructGPT002 (10-shot). In the 1st case, the\nmodel correctly generated negative commonsense\nsentences. The 2nd one suffers from the problem\n9904\nof weak negation, i.e., for negative triple, the model\nsometimes use “may” or “some” for weak negation,\nwhich is not detected by the negation cue detector\nmetric. The 3rd one suffers from unfaithful genera-\ntion to the constraints, where the model generates\ninformation outside the input triples to avoid gen-\nerating negation. The 4th one is wrong due to the\nnoise in the dataset. The 5th one is probably due\nto the high co-occurrence of the concept worms\nand birds, the model finally generates a positive\nsentence.\n9905\nExamples for Positive Commonsense Knowledge\n1 Triple <birds, capable of, fly>\nSentence Birds can fly.\nQuestion Can birds fly?\nDeduction Things with lightweight bodies and strong wing muscles can usually fly. Birds have these physical characteris-\ntics.\nFact Birds have wings.\n2 Triple <playing tennis, causes, feeling relaxed>\nSentence Playing tennis makes one feel relaxed.\nQuestion Does playing tennis cause someone to feel relaxed?\nDeduction Sport can make people feel relaxed. Tennis is a kind of sport.\nFact Tennis is a kind of sport.\n3 Triple <basketball players, desires, winning>\nSentence Basketball players want to win.\nQuestion Do basketball players want to win?\nDeduction Winning is an important goal for many athletes. Basketball players are athletes.\nFact Athletes usually desire winning in competitions.\n4 Triple <people, desires, relax after work>\nSentence People want to relax after work.\nQuestion Do people want to relaxed after work?\nDeduction Tired people want to relax. Work makes people tired.\nFact People will be tired after work.\n5 Triple <sheepskin, used for, writing>\nSentence Sheepskin can used for writing.\nQuestion can sheepskin be used for writing?\nDeduction Things with a smooth and consistent surface can be used for writing. Sheepskins have that texture.\nFact Sheepskin is the hide of a sheep.\nExamples for Negative Commonsense Knowledge\n1 Triple <shoes, has a, sleeves>\nSentence Shoes have no sleeve.\nQuestion Do shoes have sleeves?\nDeduction Sleeves are parts of garments that cover the arms. Shoes are not garments.\nFact Shoe is a type of footwear.\n2 Triple <banana, is a, tree>\nSentence Bananas are not trees.\nQuestion Are bananas a kind of trees?\nDeduction If something is a tree, then it has an elongated trunk. Bananas do not have elongated trunks.\nFact bananas are a type of fruit.\n3 Triple <computer, is a, intelligent being>\nSentence Computers aren’t intelligent beings.\nQuestion Is a computer an intelligent being?\nDeduction Intelligent beings have the ability to think. Computers cannot think like humans do.\nFact Computer is a type of electronic device.\n4 Triple <guns, used for, healing>\nSentence Guns can’t be used for healing.\nQuestion Are guns used for healing?\nDeduction Healing instruments are tools that are used to treat injuries or illnesses. Guns are not tools that are used to treat\ninjuries or illnesses.\nFact Guns are used for killing.\n5 Triple <elephant, capable of, jump>\nSentence Elephants cannot jump.\nQuestion Can elephants jump?\nDeduction Jumping needs sufficient force to overcome the effects of gravity. Elephants are too heavy to overcome gravity.\nFact elephants can walk slowly.\nTable 6: Some of the manually written examples used in in-context learning.\n9906\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthical Statement\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nGrammarly, Quillbot. For grammar check and writing polish.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3.1\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3.1\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nOpen-sourced resource.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 3.1\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSection 3.1\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nOpen-sourced resource.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3.1\nC □\u0013 Did you run computational experiments?\nSection 4, Section 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9907\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3.2\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n9908"
}