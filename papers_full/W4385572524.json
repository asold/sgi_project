{
  "title": "Reimagining Retrieval Augmented Language Models for Answering Queries",
  "url": "https://openalex.org/W4385572524",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2703614960",
      "name": "Wang-Chiew Tan",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2113621046",
      "name": "Yuliang Li",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2097198879",
      "name": "Pedro Rodriguez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102324767",
      "name": "Richard James",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2889501458",
      "name": "Xi Victoria Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154833104",
      "name": "Alon Halevy",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A4259474567",
      "name": "Wen-tau Yih",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4224247158",
    "https://openalex.org/W3176229980",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4283378943",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W3184222203",
    "https://openalex.org/W2751448157",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2492893878",
    "https://openalex.org/W2561715562"
  ],
  "abstract": "We present a reality check on large language models and inspect the promise of retrieval-augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the parametric nature of vanilla large language models. We give initial experimental findings that semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks.",
  "full_text": "Reimagining Retrieval Augmented Language Models\nfor Answering Queries\n[Reality Check Theme Track]\nWang-Chiew Tan Yuliang Li Pedro Rodriguez\nRichard James* Xi Victoria Lin Alon Halevy Scott Yih\nMeta\n{wangchiew,yuliangli,victorialin,ayh,scottyih}@meta.com rich@richjames.co *\nAbstract\nWe present a reality check on large language\nmodels and inspect the promise of retrieval-\naugmented language models in comparison.\nSuch language models are semi-parametric,\nwhere models integrate model parameters and\nknowledge from external data sources to make\ntheir predictions, as opposed to the paramet-\nric nature of vanilla large language models.\nWe give initial experimental findings that semi-\nparametric architectures can be enhanced with\nviews, a query analyzer/planner, and prove-\nnance to make a significantly more powerful\nsystem for question answering in terms of ac-\ncuracy and efficiency, and potentially for other\nNLP tasks.\n1 Introduction\nAs language models have grown larger (Kaplan\net al., 2020; Hoffmann et al., 2022), they have\nfared better and better on question answering\ntasks (Hendrycks et al., 2021) and have become\nthe foundation of impressive demos like Chat-\nGPT (Ouyang et al., 2022; ChatGPT3-OpenAI).\nModels like GPT-3 (Brown et al., 2020) and Chat-\nGPT generate fluent, human-like text, which comes\nthe potential for misuse as in high-stakes health-\ncare settings (Dinan et al., 2021). Large language\nmodels (LLMs) also come with several significant\nissues (Hoffmann et al., 2022; Bender et al., 2021).\nLLMs are costly to train, deploy, and maintain,\nboth financially and in terms of environmental im-\npact (Bender et al., 2021). These models are also\nalmost always the exclusive game of industrial com-\npanies with large budgets. Perhaps most impor-\ntantly, the ability of LLMs to make predictions is\nnot commensurate with their ability to obtain in-\nsights about their predictions. Such models can\nbe prompted to generate false statements (Wallace\net al., 2019a), often do so unprompted (Asai et al.,\n2022) and when combined with its ability to easily\nfool humans, can lead to misuse (Macaulay, 2020).\nIn recent years, we have seen the promise of\nretrieval-augmented language models partially ad-\ndressing the aforementioned shortcomings (Guu\net al., 2020; Lewis et al., 2020; Borgeaud et al.,\n2021; Izacard et al., 2022; Yasunaga et al., 2022a).\nThe architecture of such models issemi-parametric,\nwhere the model integrates model parameters and\nknowledge from external data sources to make its\npredictions. The first step of performing a task\nin these architectures is to retrieve relevant knowl-\nedge from the external sources, and then perform\nfiner-grained reasoning. Some of the benefits these\narchitectures offer are that the external sources can\nbe verified and updated easily, thereby reducing\nhallucinations (Shuster et al., 2021a) and making\nit easy to incorporate new knowledge and correct\nexisting knowledge without needing to retrain the\nentire model (Lewis et al., 2020). Models that fol-\nlow semi-parametric architectures (SPA) are typi-\ncally smaller than LLMs and they have been shown\nto outperform LLMs on several NLP tasks such as\nopen domain question answering (see Table 1). Re-\ncent work that extends LLMs with modular reason-\ning and knowledge retrieval (Karpas et al., 2022;\nLangChain) is also a type of SPA.\nIn this paper we argue that building on the core\nideas of SPA, we can potentially construct much\nmore powerful question answering systems that\nalso provide access to multi-modal data such as\nimage, video and tabular data. We describe POST-\nTEXT , a class of systems that extend SPA in three\nimportant ways. First, POST TEXT allows the ex-\nternal data to include views, a concept we borrow\nfrom database systems (Garcia-Molina et al., 2008).\nA view is a function over a number of data sources,\nV = f(D1, ..., Dn). In databases, SQL queries\nare used to define tabular views. For example, V\ncan be a table of records of minors that is derived\nfrom a table of person records by selecting only\nthose with age <18. In general, however, views\nneed not be tabular. When a view is materialized\nModel #Params Outperformed LLM’s sizes Tasks\nREALM (Guu et al., 2020) 330M 11B (T5) Open-QA\nRETRO (Borgeaud et al., 2021) 7.5B 178B (Jurassic-1), 280B (Gopher) Language modeling\nAtlas (Izacard et al., 2022) 11B 175B (GPT-3), 540B (PaLM) Multi-task NLU, Open-QA\nRAG (Lewis et al., 2020) 400M 11B (T5) Open-QA\nFiD (Izacard and Grave, 2021) 770M 11B (T5), 175B (GPT-3) Open-QA\nTable 1: The sizes of SPA models with those of comparable or outperformed LLMs.\n(i.e., executed and stored), it may be useful for an-\nswering certain queries1 more effectively. In this\npaper, we adopt a more general notion of views,\nnot limited to results of SQL queries, which can\n(compositionally) support a variety of user ques-\ntions. Views are particularly important to support\nmulti-modal data, because combinations of data\nfrom multiple modalities can be modeled as views.\nSecond, POST TEXT contains a question analyzer\nand planner module that decides on the best strat-\negy to answer a question that may involve first\nanswering multiple subquestions in sequence or in\nparallel. This module bears similarity to query op-\ntimization techniques in database systems but will\ngo significantly beyond the techniques established\nin database systems since, there are multiple dif-\nferent ways to answer a natural language question,\nespecially with the availability of multi-modal data.\nFinally, POST TEXT supports computing the prove-\nnance of answers to questions. The provenance-\naware answer generator module can track the ev-\nidence (training data or external sources) that is\nused for the answers, even if views are used as\nintermediate results.\nWe illustrate the power of POST TEXT with ex-\namples in the next section and also the overview\nof its architecture. In the remaining sections, we\ndescribe the different components of POST TEXT .\n2 Overview of PostText\nExample 1 Consider a setting where we answer\nquestions over data that includes images of dishes\nand text with restaurant reviews. We can create a\nview that aligns these two data sets so we can an-\nswer more complex queries readily. The view, the\ntable in the middle of Figure 1, aligns dishes with\nrelevant reviews and the corresponding restaurants.\nNote that creating this view involves an intermedi-\nate step of identifying the name of the dish in an\nimage. The view also stores the provenance links\nto the actual reviews from which the snippets were\n1We use queries and questions interchangeably.\nextracted. There are also provenance links for the\nimages and the name of the dish (not shown).\nThis view can be used to answer questions that\nwould be more difficult without it. For example, if\na person recalls a nice dish she had in the past but\ndoes not remember its name and is trying to figure\nout which restaurants serve the same dish and what\nare the reviews, she can pose the question, which\nincludes both the question in text and an image of\nthe dish. The answer states the name of the dish\nin question and lists restaurants with top reviews\nfor that dish, along with images of the dish and\nsnippets of those reviews and their provenance.\nExample 2 The same view can also be used to an-\nswer the question “how many reviews raved about\nShaking beef?”. The answer requires counting the\nnumber of reviews that are synonymous to very\npositive reviews about Shaking beef. The view\nsurfaces the reviews associated with Shaking beef\nimmediately and alleviates the amount of work that\nis required to compute the answer otherwise.\nThe examples show that some questions can\nbe answered more easily if they are supported by\nviews that surface useful associations between data.\nIn fact, indices are a type of views to accelerate\nlookups between an item and its attributes. In\ndatabase systems, views have been used extensively\nto enable more efficient query answering (Halevy,\n2001; Goldstein and Larson, 2001) with significant\nwork on automatically materializing a set of indices\nfor efficient query answering (Jindal et al., 2018;\nDas et al., 2019). A set of views and indices are\ndefined automatically or manually in anticipation\nof a set of frequently asked queries under a budget\nconstraint, e.g., space, so that during runtime, most\nof the incoming queries can be answered immedi-\nately or after applying simple operations over the\nviews. Otherwise, the system falls back to answer-\ning the queries using the actual data sources. In\nother words, POST TEXT prefers to use views to\nanswer the questions, which will likely to be more\nefficient and accurate in general but otherwise, the\nsystem falls back to the traditional question answer-\nFigure 1: Multimodal question with multimodal answer. The view (middle) associates the dishes with its corre-\nsponding review snippets and images. The provenance links show where the snippets are extracted from. There are\nalso provenance links for the images and name of the dish (not shown).\ning strategy. In addition to query answering, views\nhave also been used to define content-based access\ncontrol (Bertino and Sandhu, 2005), i.e., which\nparts of the data are accessible and by whom.\nThe examples also show how provenance is pro-\nvided as part of the answer. In these examples, it\nhappened that provenance was easily determined\nthrough the provenance links that are already cap-\ntured in the views. If actual data sources are ac-\ncessed, the links to the data sources used (e.g.,\nspans of text documents, parts of images, segments\nof videos) to derive the answer are provided as part\nof the answer. If the answer is generated by the lan-\nguage model, we trace how POST TEXT derives the\nanswer from parametric knowledge and retrieved\ndata through analyzing its weights or determining\n“influential” parametric knowledge (Section 6) sim-\nilarly to (Akyürek et al., 2022).\nPostText architecture POST TEXT enhances the\ncore architecture of semi-parametric models with\nthree components: views, a query analyzer & plan-\nner (QAP), and a provenance-aware answer gener-\nator (PAG). In addition, all components including\nthe “traditional” knowledge retrievers are equipped\nto manage both structured and unstructured data of\ndifferent modalities.\nFigure 2 shows the architecture of POST TEXT .\nViews are synthesized from different types of ex-\nternal data sources (e.g., text, images, videos, and\ntabular data), which can be public or private. When\na question is posed in natural language (NL), the\nQAP module interprets and decomposes the ques-\ntion into subquestions whose answers can be com-\nposed to obtain an answer to the input question.\nQAP coordinates with the knowledge retriever to\nderive the data needed to answer these questions. It\nalso coordinates with the PAG module with its plan\nso that provenance-aware answers can be returned.\nAdding these components raises interesting chal-\nlenges such as what views should we construct and\nhow do we construct and maintain these views auto-\nmatically as data sources changes? What is a good\nplan for deriving an answer and how do we choose\namong alternative plans? And how do we measure\nthe “goodness” of an answer with provenance?\nIn the remaining sections, we describe the chal-\nlenges associated with each of these components\n3 Data Sources and Views\nData Sources Most existing work on retrieval\naugmented language models are focused on text.\nMore recently, (Chen et al., 2022; Yasunaga et al.,\n2022b; Sheynin et al., 2022) has applied SPA mod-\nels on image-text and text-only corpus. The data\nsources in POST TEXT are multi-modal, unstruc-\ntured or structured. They can be external public\ndata sources or private ones.\nViews Views are results computed (not necessarily\nthrough SQL queries) from data sources or other\nFigure 2: Semi-parametric architectures enhanced with views, a query analyzer & planner module, and a provenance-\naware answer generator. The data sources may be public or private.\nviews. For example, a view can be a document\ninvolving data of different modalities (e.g., an im-\nage or a table). Views are powerful constructs for\nsurfacing important and useful associations that are\nnot obvious otherwise, whether they are associa-\ntions from data within one data source or across\nmultiple data sources. The table in Figure 1 is a\nview over restaurant reviews from Yelp, Google,\nand images provided by restaurants. This view\nmakes it easier to compute the number of reviews\nassociated with each dish in each restaurant or even\nacross all restaurants. This view also makes it eas-\nier to determine the answer as to which dishes has\nmore reviews than Shaking beef at Tamarine.\nIndexes are a special type of views. They as-\nsociate an item with its attribute. Several imple-\nmentations of retrieval augmented language mod-\nels (Guu et al., 2020; Lewis et al., 2020; Izacard\net al., 2022) already construct indices that associate\na document with its nearest neighbors. Recently,\nGPT-index (GPT-Index, 2022) developed a set of\nAPIs for creating data structures that can be tra-\nversed using LLMs to answer queries. The data\nstructures are structured indexes and can be used\nto determine an answer to a question.\nRelational views are extensively used in data\nwarehouses for optimizing queries. Indexes and\nviews are typically created by users or database\nadministrators or they can be automatically se-\nlected (Agrawal et al., 2000; Schnaitter et al., 2007;\nJindal et al., 2018) and tuned (Agrawal et al., 2006;\nBruno and Chaudhuri, 2008) to efficiently answer\nqueries of a given workload (Das et al., 2019),\nwhich are queries that are anticipated to be fre-\nquently occurring. In typical settings, a set of views\nare constructed, usually under a budget constraint\nsuch as space, to maximize the queries that can be\nanswered (either directly or through applying a few\nsimple operators on the views) in a given workload.\nWhen a new query arrives after the views are con-\nstructed, the query optimizer determines the best\nplan to adopt for computing the answer. Queries\nare directly executed over the views if possible.\nOtherwise, it falls back to old strategy of answer-\ning the query with the data sources. For example,\nearly last year, in anticipation of frequent queries\nabout statistics of past World Cups due to the World\nCup 2022 event at the end of the year, a set of views\nabout the different World Cup statistics could have\nbeen constructed a priori so that most World Cup\nrelated questions can be directly answered using\nthe views.\nWe hypothesize that views in POST TEXT can\nbring similar benefits to question answering. The\nright views will make it easier for the QAP mod-\nule and the knowledge retriever to discover and\nobtain relevant data and subsequently for the an-\nswer generator to derive the right answers. Existing\nSPAs (Guu et al., 2020; Lewis et al., 2020; Izacard\net al., 2022) are already leveraging dense-vector in-\ndices to accelerate the retrieval of document spans.\nIn POST TEXT with views being available, it is a nat-\nural extension to annotate each view with a descrip-\ntion of its content (e.g., “ Restaurants and highly\nranked dishes”), which would make it even easier\nfor the knowledge retriever to find the relevant data.\nThe core challenges in developing views are how\ndo we determine what is a “right” set of views to\nmaterialize automatically or semi-automatically?\nHow do we incrementally maintain such views as\ndata sources are updated? These problems are ex-\ntensively studied in the database community and\nit will be interesting to explore those ideas that\ntransfer to the POST TEXT .\nThe architecture can also be instrumented in\nsuch a way that views are the only sources of data\nfor the knowledge retriever (i.e., actual data sources\nare excluded). Hence, in this case, views act as a\ngateway that define which parts of the data sources\nare accessible by the knowledge retriever to answer\nqueries. Finer-grained access control can also be in-\nstrumented through views as described in (Bertino\nand Sandhu, 2005). With views, it is also possible\nto enable a finer-grained public-private autoregres-\nsive information retrieval privacy system (Arora\net al., 2022).\n4 Question Analyzer & Planner\nThe question analyzer and planner (QAP) module\nexamines the input question and generates a plan,\ni.e., a sequence of sub-questions whose answers\ncan be combined to form an answer to the input\nquestion. For each subquestion in the plan, QAP\nfirst checks whether external knowledge is needed.\nIf not, the language model can be used to derive\nthe answer. Otherwise, the subquestion is passed\nto the knowledge retriever to discover and retrieve\nrelevant data for the subquestion at hand. The re-\nsults from the knowledge retriever and the plan\nare passed to PAG (i.e., the rightmost green box\nin Figure 2). It is still an open and challenging\nquestion to determine whether a language model\ncan confidently answer a question (Kamath et al.,\n2020; Si et al., 2022). Any solution to this problem\nwill help improve the plan generator.\nAn example plan from the QAP module for our\nrunning example is as follows: (1) find the name of\nthe dish X in the input image, (2) find restaurants\nthat serve X, (3) find the top restaurant among\nthe results from (2). This plan is viable because\n(a) there is an index associating embeddings of\nimages with the name of the main entity of the im-\nage, (b) there exists a view as shown in Figure 1,\nwhich supports the search for restaurants that serve\na particular dish. Top answers can be derived by\ncomputing the scores of the reviews or approxi-\nmating it based on the sentiment of the reviews\nand then ranking the results based on such scores.\nThe information from (2) is passed to PAG which\nwill compute the answer along with its provenance.\nThis plan is based on the heuristic to push selection\nconditions early before joining/combining different\ndata sources if needed. The conditions in the ques-\ntion are “good version” and “this dish”. In this case,\nno joins are required as the view already combines\nthe required information in one place. Hence, QAP\nseeks to first find the name of the dish to narrow\ndown the reviews restricted to this dish. Alterna-\ntively, it could also retrieve all good reviews before\nconditioning on the name of the dish. Yet another\nplan could be to match the image directly to the im-\nages of the view to find the top reviews. Or, it may\ndecide to directly retrieve only top reviews with\nimages similar to the image in the question from\nthe external data sources and condition the answer\nbased on the name of the restaurant mentioned in\nthe reviews.\nIn all possible plans, the knowledge retriever is\nresponsible for discovering and retrieving the rele-\nvant data for the QAP plan. In addition to the logic\nthat may be needed for decomposing the question\ninto subquestions, a plan is also needed for compos-\ning the subanswers obtained to form an answer to\nthe input question. The plan is shared with the PAG\nmodule for deriving the associated provenance.\nA fundamental challenge in developing the QAP\nmodule is how to derive candidate plans and decide\nwhat is the “best” plan for answering the ques-\ntion when there are different ways to obtain an\nanswer. Achieving this requires understanding how\nto compare amongst alternative plans for deriving\nan answer to the question. This problem bears sim-\nilarity to query evaluation techniques for database\nsystems (e.g., (Graefe, 1993)). It will be interest-\ning to investigate whether database query planning\ntechniques and ideas can synergize with question\nunderstanding and planning techniques (e.g., (Wolf-\nson et al., 2020; Dunietz et al., 2020; Zhao et al.,\n2021; Xiong et al., 2021) to develop a comprehen-\nsive query planner. Emerging work such as chain\nof thought reasoning (Wei et al., 2022), where a\nsequence of prompts are engineered to elicit better\nanswers, ReAct (Yao et al., 2022), where reasoning\nand action techniques are applied for deriving an\nanswer, and more recently, work that generates a\nplan which can call LMs for resolving subques-\ntions (Cheng et al., 2022) are also relevant. These\ntechniques so far are restricted to text and does not\ncompare among different plans.\nAnother challenge in the context of NL ques-\ntions is that while there is a single correct answer\nto an SQL query over a database, there are po-\ntentially many different correct answers to a NL\nquestion (Si et al., 2021; Min et al., 2020; Chen\net al., 2020). Hence the space of possible plans to\nderive the “best” answer most efficiently is even\nmore challenging in this case.\nWe are advocating for a system that can rea-\nson and compare at least some viable strategies to\narrive at a best plan for deriving a good answer effi-\nciently. Naturally, one can also train a LM to create\na plan. Our belief is that taking a more systematic\nroute to planning can relief the need for the amount\nof training data required and will also aid prove-\nnance generation through its ability to describe the\nsteps it took and the sources of data used in each\nstep to generate an answer. As we shall explain\nin Section 5, the cost and accuracy of knowledge\nretrievers can also play a role in determining what\nis a better strategy for computing a good answer.\n5 Knowledge Retriever\nThe role of the knowledge retriever is to provide\nthe information that the system lacks in order to\nfulfill the given task, typically at the inference time.\nMore importantly, we envision that the knowledge\nretriever proposed in our framework has the abil-\nity to access knowledge stored in different sources\nand modalities, retrieve and integrate the relevant\npieces of information, and present the output in a\ntabular data view. The structured output contains\nraw data items (e.g., text documents, images or\nvideos) and and optionally different metadata, such\nas textual description of each data item. Such struc-\ntured output allows downstream (neural) models to\nconsume the retrieved knowledge efficiently and\nalso allows developers and users to validate the\nprovenance conveniently. Existing information re-\ntrieval models mostly focus on a single form of data.\nBelow we first describe briefly how knowledge re-\ntrieval is done for unstructured and structured data.\nWe then discuss the technical challenges for build-\ning a unified knowledge retriever, as well as recent\nresearch efforts towards this direction.\nRetrievers for unstructured data For unstruc-\ntured data, such as a large collection of documents\n(i.e., text corpus) or images, knowledge retrieval\nis often reduced to a simple similarity search prob-\nlem, where both queries and data in the knowledge\nsource are represented as vectors in the same vec-\ntor space (Turney and Pantel, 2010). Data points\nthat are close to the query are considered as rele-\nvant and thus returned as the knowledge requested.\nTraditional information retrieval methods, whether\nrelying on sparse vector representations, such as\nTFIDF (Salton et al., 1975) and BM25 (Robert-\nson et al., 2009), or dense representations, such\nas LSA (Deerwester et al., 1990), DSSM (Huang\net al., 2013), DPR (Karpukhin et al., 2020), are the\ncanonical examples of this paradigm. Notice that\nthe vector space model is not restricted to text but\nis also applicable to problems in other modalities,\nsuch as image tagging (Weston et al., 2011) and\nimage retrieval (Gordo et al., 2016).\nRetrievers for structured data When the knowl-\nedge source is semi-structured (e.g., tables) or struc-\ntured (e.g., databases), the query can be structured\nand allows the information need to be defined in\na more precise way. Because the data is typically\nstored in a highly optimized management system\nand sometimes only accessible through a set of\npredefined API calls, the key technical challenge\nin the knowledge retriever is to formulate the in-\nformation need into a formal, structured query.\nTo map natural language questions to structured\nqueries, semantic parsing is the key technical com-\nponent for building a knowledge retriever for struc-\ntured data. Some early works propose mapping the\nnatural language questions to a generic meaning\nrepresentation, which is later translated to the for-\nmal language used by the target knowledge base\nthrough ontology matching (Kwiatkowski et al.,\n2013; Berant et al., 2013). Others advocate that\nthe meaning representation should be closely tight\nto the target formal language (Yih et al., 2015),\nsuch as SPARQL for triple stores. Because of\nthe success of deep learning, especially the large\npre-trained language models, semantic parsing has\nmostly been reduced to a sequence generation prob-\nlem (e.g., Text-to-SQL). For example, RASAT (Qi\net al., 2022) and PICARD (Scholak et al., 2021),\nwhich are generation models based on T5 (Raffel\net al., 2020), give state-of-the-art results on bench-\nmarks like Spider (Yu et al., 2018) and CoSQL (Yu\net al., 2019).\nTowards a unified knowledge retriever As\nknowledge can exist in different forms, a unified\nknowledge retriever that can handle both struc-\ntured and unstructured data in different modalities\nis more desirable. One possible solution for re-\nalizing a unified retriever is to leverage multiple\nsingle-source knowledge retrievers. When a query\ncomes in, the QAP module first decomposes it into\nseveral smaller sub-queries, where each sub-query\ncan be answered using one component knowledge\nretriever. The results from multiple knowledge re-\ntrievers can be integrated and then returned as the\nfinal output. However, several technical difficul-\nties, including how to accurately decompose the\nquestion and how to join the retrieved results often\nhinder the success of this approach. Alternatively,\nunifying multiple sources of information in a stan-\ndard representation, using text as a denominator\nrepresentation, has been promoted recently (Oguz\net al., 2022; Zeng et al., 2022). If all data items have\na corresponding textual description, it is possible\nfor the knowledge retriever to use only text-based\nretrieval techniques to find relevant data items once\nall input entities of non-textual modality have been\nmapped to their corresponding textual descriptions.\nSuch approach circumvents the complexity of\nmanaging multiple knowledge stores in different\nformat. Moreover, with the success of large multi-\nlingual and multi-modal language models (Con-\nneau and Lample, 2019; Aghajanyan et al., 2022),\ndata of different structures or from different modal-\nities can naturally share the same representation\nspace. While unifying multiple sources of informa-\ntion through representation learning seems to be a\npromising direction, it should be noted that certain\nstructured information may be lost in the process.\nFor example, by flatting a knowledge graph to se-\nquences of (subject, predicate, object) triples, the\ngraph structure is then buried in the textual form.\nWhether the information loss limits the retriever’s\nability to handle certain highly relational queries\nremains to be seen.\n6 Provenance-aware answer generators\n6.1 Semi-Parametric Engine\nDemonstrating the provenance of a QA model\nprediction should center on identifying the data—\nwhether in training data, retrieval corpora, or\ninput—that is most influential in causing the model\nto make a particular prediction. For example, given\nthe question “who was the first U.S. president?”, the\nsystem should return the correct answer “George\nWashington” and references to training or retrieval\ncorpora that are—to the model—causally linked\nto the answer. If the training or retrieval data in-\ncluded Washington’s Wikipedia page, a typical hu-\nman would expect for this to be included. However,\nthe requirement we impose is causal and counter-\nfactual: had the model not used that data, the pre-\ndiction should change. If the prediction does not\nchange, then from the causal perspective, there\nmay be other data that is either more influential\nor duplicative (e.g., if whitehouse.gov is in the\ntraining data, it is duplicative). Next, we describe\ncommon semi-parametric models and sketch how\nthis casually-based answer provenance could be ob-\ntained and computational challenges to overcome.\nProvided an input prompt and retrieved text,\nsemi-parametric models like ATLAS (Izacard et al.,\n2022) or passing documents as prompts to GPT-\n3 (Kasai et al., 2022) are adept at generating free-\ntext, short answers. Likewise, parametric models\nwith flexible input like GPT-3 can be combined\nwith retrievers to achieve a similar goal; alterna-\ntively, transformer models can be retrofitted with\nlayers so that passages can be integrated in embed-\nding space (Borgeaud et al., 2021). While retrieval-\naugmentation is no catch-all panacea to model hal-\nlucination, it does mitigate the problem (Shuster\net al., 2021b). Additionally, models’ explanations\ncan make it easier to know when to trust models\nand when not to (Feng and Boyd-Graber, 2022).\nIn the case of QA models that take question plus\nretrieved text as input, there are several options.\nFirst, the model could provide several alternative\nanswers which provide insight into the distribution\nof model outputs, rather than just a point estimate.\nSecond, the model could provide a combination of\nfeature-based explanations such as token saliency\nmaps and the model’s confidence in a correct an-\nswer (Wallace et al., 2019b). When combined, they\ncan jointly influence the degree to which humans\ntrust the model (Lai and Tan, 2019). However, to\nprovide a complete account of model behavior, we\nmust return to the training of model and the data\nused. In short, we endeavor to identify the combi-\nnation of input, training data, and retrieved text that\ncaused the model to produce the distribution of out-\nputs (i.e., answer(s)). This is, of course, challeng-\ning due to scale of language model training data\nlike C4 (Raffel et al., 2020) and the Pile (Gao et al.,\n2020) and that establishing causal—and therefore\nmore faithful—explanations of model behavior is\ndifficult. Training data attribution is one promising\nidea in this direction—it uses gradient and embed-\nding based methods to attribute inference behavior\nto training data (Akyürek et al., 2022). For ex-\nample, influence functions (Hampel, 1974; Han\net al., 2020) and TracIn (Pruthi et al., 2020) link\npredictions to specific training examples, but are\ncomputationally expensive and are approximate\nrather than exact solutions. To firmly establish\na causal connection, one could fully re-train the\nmodel without the identified training examples, but\nthis is prohibitively expensive in practice. Future\ndevelopment of efficient training data attribution,\ncombined with methods like interpretations of in-\nput plus retrieved data, is a promising direction\ntowards more complete explanations of model pre-\ndictions.\n6.2 Tabular Engine\nAs described at the end of Section 4, the knowl-\nedge retriever will pass on the data obtained to PAG.\nThe QAP module will pass information about its\nplan to PAG. If the data obtained is tabular and a\nSQL query is generated, the information is passed\nto the tabular engine of PAG to compute the re-\nquired answer(s). The recent advances in Text-to-\nSQL (Wang et al., 2020; Zhao et al., 2022) provide\na good technical foundation for generating such\nSQL queries.\nIn most cases, it is not difficult to understand\nthe correspondence between the natural language\nquestion and the SQL query that is generated. Once\nthe SQL query is obtained, provenance can be sys-\ntematically derived. In databases, the notion of\nprovenance is well-studied (Cheney et al., 2009)\nfor a large class of SQL queries; from explaining\nwhy a tuple is in the output (i.e., the set of tuples in\nthe database that led to the answer), where a value\nin a tuple is copied from (i.e., which cell in the\nsource table is the value copied from) (Buneman\net al., 2001) to how that tuple was derived, which\nis formalized as semirings (Green et al., 2007),\na polynomial that essentially describes conjunc-\ntion/disjunction of records required materialize a\nrecord in the result. Database provenance has also\nbeen extended to aggregate queries (Amsterdamer\net al., 2011). Since one can derive the mapping be-\ntween the input question and the SQL query that is\ngenerated and also derive the provenance from the\ndata sources based on the SQL query, it becomes\npossible to understand how the input question led\nto the answers given by POST TEXT .\nPutting all together, POST TEXT first explains\nthat the name of the image (i.e., “a good version of\nthis dish”) referred in question is Shaking beef. It\nthen shows the SQL query that is generated for the\nquestion “Where can I find a good version of Shak-\ning beef” and the ranking function used for ranking\nthe rows of restaurants with reviews for the dish\nShaking beef. For our running example, the answer\nis obtained from the first row of the table in Fig-\nure 1. Specifically, the answer is summarized from\nthe column Dish and Review snippets/embeddings.\nThe actual snippets are found following the prove-\nnance links captured in the column Provenance.\nA more direct relationship between the summary\nand the actual review snippets can also be estab-\nlished (Carmeli et al., 2021).\nThe success of this approach depends on how\nfar we can push database provenance systemati-\ncally as SQL queries can still be far more com-\nplex than what is investigated in past research (e.g.,\ncomplex arithmetic and aggregate functions involv-\ning also negation, group filters, and functions over\nvalues of different modalities). As an alternative\nto executing the SQL query over the tables ob-\ntained, the tabular engine can also choose to de-\nploy table question answering (tableQA) methods\nwhere a model directly searches the tabular data\nfor answers based on the input question (Sun et al.,\n2016). Tapas (Herzig et al., 2020) and Tapex (Liu\net al., 2022) are two example solutions for tableQA\nthat formulates tableQA as sequence understand-\ning/generation tasks. Like other recent tableQA\nworks (Glass et al., 2021; Herzig et al., 2021), they\nconsider the problem of computing the answer from\na single input. It will be interesting to explore how\nto explain the results obtained using tableQA meth-\nods and how tableQA methods can be extended to\nhandle multi-hop questions where the answer may\nspan multiple tables or involve different types of\naggregations, reasoning and modalities.\n7 Preliminary Findings\nTo test our hypothesis that views are valuable for\nanswering queries, especially queries that involve\ncounting or aggregation, we have implemented\na first version of POST TEXT 2 and compared it\nagainst some QA baselines.\nThe current implementation of POST TEXT as-\nsumes views over the underlying data are avail-\nable in tabular format. The QAP module simply\nroutes the query to a view-based engine (VBE) or\na retrieval-based engine (RBE) to answer the query.\nVBE picks the best view and translates the natu-\nral language query into an SQLite query against\nthe view using OpenAI’s gpt-3.5-turbo/gpt-4\nmodel. It then executes the SQLite query against\n2PostText source code will be made available soon.\nVBE RBE DBChain DBChain (no views)\nS 3.45 2.81 3.37 2.72\nM 3.79 2.69 3.28 2.61\nL 3.11 2.44 2.95 1.95\nTable 2: Results with GPT-3.5-turbo. Sizes of (S)mall,\n(M)edium, (L)arge are 1.1MB, 2.4MB, and 5.6MB re-\nspectively.\nVBE RBE DBChain DBChain (no views)\nS 3.33* 2.10* 2.14* 1.10*\nM 3.55 1.93 2.35 1.51*\nL 3.08 2 1.97 1.11*\nTable 3: Results with GPT-4. * indicates that timeouts\nor API errors were encountered during experimentation.\nthe view to obtain a table result which is then trans-\nlated into English as the final answer. VBE also an-\nalyzes the SQLite query to compute the provenance\nof the answers. At present, it does so by simply re-\ntrieving all tuples that contributed to every (nested)\naggregated query that is a simple (select-from-\nwhere-groupby-having clause) and does not handle\nnegations. An example of the VBE process is de-\nscribed in Appendix B. RBE is implemented with\nLangchain’s RetrievalQAwithSources library. It\nfirst retrieves top-k documents that are relevant for\nthe query and then conditions its answer based on\nthe retrieval. The answer and the ids of the retrieved\ndocuments are returned.\nFor our experiments, we use the 42 multihop\nqueries over 3 synthetic personal timelines of dif-\nferent sizes from TimelineQA’s benchmark (Tan\net al., 2023). The personal timelines model the\ndaily activities (e.g., the trips made, things bought,\npeople talked to) of a person over a period of time.\nWe create a view around each type of activity\n(e.g., trips, shopping, daily_chats) for VBE. For\nfurther comparison, we also ran Langchain’s SQL-\nDatabaseChain (DBChain) to perform QA over the\nsame VBE views. Furthermore, we ran it over\ntimelines loosely structured as a binary relation\nof (date,description) pairs (called DBChain (no\nviews)). We compared the returned answers against\nthe ground truth answers by grading them on a\nscale of 1-5, with a LLM, where 5 means the re-\nturned answer has the same meaning as the ground\ntruth answer (the grading scheme is described in\nthe Appendix C).\nOur results are shown in Tables 2 and 3.\nAcross both tables, the results on DBChain vs.\nDBChain(no views) reveal that adding some struc-\nture (in this case adding views) is crucial for better\nperformance. Although the benchmark is a rela-\ntively small dataset, the scale of the timelines al-\nready reveals an impact on the accuracy across all\nQA systems. For DBChain, the drop in accuracy\nas the size increases because it sometimes relies\non generating SQL queries that return all relevant\nrecords and passing all the records to the language\nmodel to compute the aggregate. When the results\nreturned are large, which tends to be the case for\nlarger timelines, the token limit of the LLM is often\nexceeded. VBE has a similar downward trend. It\ntends to generate queries that push the aggregates\nto the SQL engine and hence, avoids the issue of\nexceeding the token limit of the language models\nfor many cases encountered in DBChain. Still, as\nthe timeline gets larger, the result returned by the\ngenerated SQL query tends to be bigger and when\nthese results are passed to the verbalization com-\nponent to compose an answer in English, this may\nsometimes exceed the token limit of the language\nmodel. We also found that on a handful of cases,\nit so happens that the SQL query generated for L\nis invalid compared with those generated for the\nsparse dataset.\nThe scores of RBE is relatively stable across all\ndata densities. But overall, it tends to score lower\ncompared with VBE and DBChain . This is be-\ncause RBE relies on retrieving the topk documents\nfrom an index to condition the answers upon, re-\ngardless of the size of the timeline. However, these\nretrieved documents may not contain all the nec-\nessary information for answering the question in\ngeneral. Even though the grading scores may not\nreveal this, the answers tend to be “more wrong”\nfor aggregate queries over a larger timeline.\n8 Conclusion\nPOST TEXT enhances the core ideas of semi-\nparametric architectures with views, a query an-\nalyzer & planner, and a provenance-aware answer\ngenerator. Our initial results indicate that POST-\nTEXT is more effective on queries involving count-\ning/aggregation when we provide structured views\nto facilitate computation. We plan to further de-\nvelop and investigate POST TEXT to automatically\ndetermine what views to construct, how does one\ngenerate plans and compare amongst plans, and\nhow can one measure the quality of answers with\nprovenance.\nLimitations and Ethical Considerations\nWe point out the limitations of large language mod-\nels (costly to train, deploy, maintain, hallucinate,\nopaque). The vision of POST TEXT shows promise\nof less costly training, maintenance, and more ex-\nplainability. However, no actual system is built yet\nto validate these claims and it is also not clear that a\nsystem with POST TEXT architecture will be easier\nto deploy since it has more components.\nReferences\nArmen Aghajanyan, Bernie Huang, Candace Ross,\nVladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,\nand Luke Zettlemoyer. 2022. CM3: A causal\nmasked multimodal model of the internet. CoRR,\nabs/2201.07520.\nSanjay Agrawal, Surajit Chaudhuri, and Vivek R.\nNarasayya. 2000. Automated selection of materi-\nalized views and indexes in SQL databases. In VLDB\n2000, Proceedings of 26th International Conference\non Very Large Data Bases, September 10-14, 2000,\nCairo, Egypt, pages 496–505. Morgan Kaufmann.\nSanjay Agrawal, Eric Chu, and Vivek Narasayya. 2006.\nAutomatic physical design tuning: Workload as a\nsequence. In Proceedings of the 2006 ACM SIGMOD\nInternational Conference on Management of Data ,\nSIGMOD ’06, page 683–694, New York, NY , USA.\nAssociation for Computing Machinery.\nEkin Akyürek, Tolga Bolukbasi, Frederick Liu, Binbin\nXiong, Ian Tenney, Jacob Andreas, and Kelvin Guu.\n2022. Tracing knowledge in language models back\nto the training data. In Findings of the Association\nfor Computational Linguistics: EMNLP. Association\nfor Computational Linguistics.\nYael Amsterdamer, Daniel Deutch, and Val Tannen.\n2011. Provenance for aggregate queries. In\nProceedings of the 30th ACM SIGMOD-SIGACT-\nSIGART Symposium on Principles of Database Sys-\ntems, PODS 2011, June 12-16, 2011, Athens, Greece,\npages 153–164. ACM.\nSimran Arora, Patrick Lewis, Angela Fan, Jacob Kahn,\nand Christopher Ré. 2022. Reasoning over public\nand private data in retrieval-based systems.\nAkari Asai, Matt Gardner, and Hannaneh Ha-\njishirzi. 2022. Evidentiality-guided generation for\nKnowledge-Intensive NLP tasks. In Conference of\nthe North American Chapter of the Association for\nComputational Linguistics. Association for Compu-\ntational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 610–623.\nJonathan Berant, Andrew Chou, Roy Frostig, and\nPercy S. Liang. 2013. Semantic parsing on free-\nbase from question-answer pairs. In Proceedings of\nEmpirical Methods in Natural Language Processing.\nE. Bertino and R. Sandhu. 2005. Database security -\nconcepts, approaches, and challenges. IEEE Transac-\ntions on Dependable and Secure Computing, 2(1):2–\n19.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W Rae, Erich Elsen, and Laurent Sifre.\n2021. Improving language models by retrieving from\ntrillions of tokens.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Proceed-\nings of Advances in Neural Information Processing\nSystems. Curran Associates, Inc.\nNicolas Bruno and Surajit Chaudhuri. 2008. Con-\nstrained physical design tuning. Proc. VLDB Endow.,\n1(1):4–15.\nPeter Buneman, Sanjeev Khanna, and Wang Chiew Tan.\n2001. Why and where: A characterization of data\nprovenance. In ICDT, volume 1973 of Lecture Notes\nin Computer Science, pages 316–330.\nNofar Carmeli, Xiaolan Wang, Yoshihiko Suhara, Ste-\nfanos Angelidis, Yuliang Li, Jinfeng Li, and Wang-\nChiew Tan. 2021. Constructing explainable opinion\ngraphs from reviews. In WWW ’21: The Web Confer-\nence 2021, Virtual Event / Ljubljana, Slovenia, pages\n3419–3431. ACM / IW3C2.\nChatGPT3-OpenAI. Chatgpt: Optimizing language\nmodels for dialogue.\nAnthony Chen, Gabriel Stanovsky, Sameer Singh, and\nMatt Gardner. 2020. MOCHA: A dataset for train-\ning and evaluating generative reading comprehension\nmetrics. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 6521–6532, Online. Association for\nComputational Linguistics.\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and\nWilliam W. Cohen. 2022. Murag: Multimodal\nretrieval-augmented generator for open question an-\nswering over images and text.\nJames Cheney, Laura Chiticariu, and Wang Chiew Tan.\n2009. Provenance in databases: Why, how, and\nwhere. Found. Trends Databases, 1(4):379–474.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\nNoah A. Smith, and Tao Yu. 2022. Binding language\nmodels in symbolic languages.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nSudipto Das, Miroslav Grbic, Igor Ilic, Isidora Jovandic,\nAndrija Jovanovic, Vivek R. Narasayya, Miodrag\nRadulovic, Maja Stikic, Gaoxiang Xu, and Surajit\nChaudhuri. 2019. Automatically indexing millions\nof databases in microsoft azure sql database. In Pro-\nceedings of the 2019 International Conference on\nManagement of Data, SIGMOD ’19, page 666–679,\nNew York, NY , USA. Association for Computing\nMachinery.\nScott C. Deerwester, Susan T. Dumais, Thomas K. Lan-\ndauer, George W. Furnas, and Richard A. Harshman.\n1990. Indexing by latent semantic analysis. Jour-\nnal of the American society for information science,\n41:391–407.\nEmily Dinan, Gavin Abercrombie, A Stevie Bergman,\nShannon Spruit, Dirk Hovy, Y-Lan Boureau, and\nVerena Rieser. 2021. Anticipating safety issues in\nE2E conversational AI: Framework and tooling.\nJesse Dunietz, Gregory Burnham, Akash Bharadwaj,\nOwen Rambow, Jennifer Chu-Carroll, and David Fer-\nrucci. 2020. To test machine comprehension, start\nby defining comprehension. In Proceedings of the\nAssociation for Computational Linguistics.\nShi Feng and Jordan Boyd-Graber. 2022. Learning to\nexplain selectively: A case study on question answer-\ning. In Proceedings of Empirical Methods in Natural\nLanguage Processing. Association for Computational\nLinguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The pile: An\n800GB dataset of diverse text for language modeling.\nHector Garcia-Molina, Jeffrey D. Ullman, and Jennifer\nWidom. 2008. Database Systems: The Complete\nBook, 2 edition. Prentice Hall Press, USA.\nMichael R. Glass, Mustafa Canim, Alfio Gliozzo, Sa-\nneem A. Chemmengath, Vishwajeet Kumar, Rishav\nChakravarti, Avi Sil, Feifei Pan, Samarth Bharadwaj,\nand Nicolas Rodolfo Fauceglia. 2021. Capturing row\nand column semantics in transformer based question\nanswering over tables. In NAACL-HLT, pages 1212–\n1224. Association for Computational Linguistics.\nJonathan Goldstein and Per-Åke Larson. 2001. Opti-\nmizing queries using materialized views: A practical,\nscalable solution. In Proceedings of the 2001 ACM\nSIGMOD International Conference on Management\nof Data, SIGMOD ’01, page 331–342, New York,\nNY , USA. Association for Computing Machinery.\nAlbert Gordo, Jon Almazán, Jerome Revaud, and Diane\nLarlus. 2016. Deep image retrieval: Learning global\nrepresentations for image search. In Computer Vi-\nsion – ECCV 2016, pages 241–257, Cham. Springer\nInternational Publishing.\nGPT-Index. 2022. [link].\nGoetz Graefe. 1993. Query evaluation techniques for\nlarge databases. ACM Comput. Surv., 25(2):73–169.\nTodd J. Green, Gregory Karvounarakis, and Val Tannen.\n2007. Provenance semirings. In Proceedings of\nthe Twenty-Sixth ACM SIGACT-SIGMOD-SIGART\nSymposium on Principles of Database Systems, June\n11-13, 2007, Beijing, China, pages 31–40. ACM.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. REALM: Retrieval-\nAugmented language model Pre-Training. In Pro-\nceedings of the International Conference of Machine\nLearning.\nAlon Y . Halevy. 2001. Answering queries using views:\nA survey. The VLDB Journal, 10(4):270–294.\nFrank R Hampel. 1974. The influence curve and its\nrole in robust estimation. Journal of the American\nStatistical Association, 69(346):383–393.\nXiaochuang Han, Byron C. Wallace, and Yulia Tsvetkov.\n2020. Explaining black box predictions and unveil-\ning data artifacts through influence functions. In\nProceedings of the Association for Computational\nLinguistics. Association for Computational Linguis-\ntics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nJonathan Herzig, Thomas Müller, Syrine Krichene, and\nJulian Eisenschlos. 2021. Open domain question\nanswering over tables via dense retrieval. In NAACL-\nHLT, pages 512–519. Association for Computational\nLinguistics.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Martin Eisen-\nschlos. 2020. Tapas: Weakly supervised table parsing\nvia pre-training. In ACL, pages 4320–4333. Associa-\ntion for Computational Linguistics.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training Compute-Optimal\nlarge language models.\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for web search using\nclickthrough data. In Proceedings of the 22nd ACM\nInternational Conference on Information & Knowl-\nedge Management, CIKM ’13, page 2333–2338, New\nYork, NY , USA. Association for Computing Machin-\nery.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the Eu-\nropean Chapter of the Association for Computational\nLinguistics. Association for Computational Linguis-\ntics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nAlekh Jindal, Konstantinos Karanasos, Sriram Rao, and\nHiren Patel. 2018. Selecting subexpressions to ma-\nterialize at datacenter scale. Proc. VLDB Endow.,\n11(7):800–812.\nAmita Kamath, Robin Jia, and Percy Liang. 2020. Se-\nlective question answering under domain shift. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 5684–5696. Associa-\ntion for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nEhud Karpas, Omri Abend, Yonatan Belinkov, Barak\nLenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\nBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhl-\ngay, Noam Rozen, Erez Schwartz, Gal Shachaf,\nShai Shalev-Shwartz, Amnon Shashua, and Moshe\nTenenholtz. 2022. Mrkl systems: A modular, neuro-\nsymbolic architecture that combines large language\nmodels, external knowledge sources and discrete rea-\nsoning.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. RealTime QA: What’s the answer right now?\narXiv [cs.CL].\nTom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke\nZettlemoyer. 2013. Scaling semantic parsers with\non-the-fly ontology matching. In Proceedings of\nthe 2013 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1545–1556, Seattle,\nWashington, USA. Association for Computational\nLinguistics.\nVivian Lai and Chenhao Tan. 2019. On human predic-\ntions with explanations and predictions of machine\nlearning models: A case study on deception detec-\ntion. In Proceedings of the Conference on Fairness,\nAccountability, and Transparency. Association for\nComputing Machinery.\nLangChain. [link].\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Proceedings of\nAdvances in Neural Information Processing Systems.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022.\nTAPEX: table pre-training via learning a neural SQL\nexecutor. In ICLR. OpenReview.net.\nThomas Macaulay. 2020. Someone let a gpt-3 bot loose\non reddit — it didn’t end well.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5783–\n5797, Online. Association for Computational Lin-\nguistics.\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan\nPeshterliev, Dmytro Okhonko, Michael Schlichtkrull,\nSonal Gupta, Yashar Mehdad, and Scott Yih. 2022.\nUniK-QA: Unified representations of structured and\nunstructured knowledge for open-domain question\nanswering. In Findings of the Association for Compu-\ntational Linguistics: NAACL 2022, pages 1535–1546,\nSeattle, United States. Association for Computational\nLinguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund\nSundararajan. 2020. Estimating training data influ-\nence by tracing gradient descent. In Proceedings of\nAdvances in Neural Information Processing Systems.\nCurran Associates, Inc.\nJiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan,\nYu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi\nZhang, and Zhouhan Lin. 2022. RASAT: Integrating\nrelational structures into pretrained seq2seq model\nfor text-to-sql. In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguis-\ntics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified Text-to-Text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nG. Salton, A. Wong, and C. S. Yang. 1975. A vector\nspace model for automatic indexing. Commun. ACM,\n18(11):613–620.\nKarl Schnaitter, Serge Abiteboul, Tova Milo, and Neok-\nlis Polyzotis. 2007. On-line index selection for shift-\ning workloads. In 2007 IEEE 23rd International\nConference on Data Engineering Workshop, pages\n459–468.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. PICARD: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9895–9901, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nShelly Sheynin, Oron Ashual, Adam Polyak, Uriel\nSinger, Oran Gafni, Eliya Nachmani, and Yaniv Taig-\nman. 2022. Knn-diffusion: Image generation via\nlarge-scale retrieval.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021a. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 16-20 November, 2021, pages 3784–\n3803. Association for Computational Linguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021b. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP. Association for Computational Linguistics.\nChenglei Si, Chen Zhao, and Jordan Boyd-Graber. 2021.\nWhat’s in a name? answer equivalence for open-\ndomain question answering. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9623–9629, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nChenglei Si, Chen Zhao, Sewon Min, and Jordan L.\nBoyd-Graber. 2022. Revisiting calibration for ques-\ntion answering. ArXiv, abs/2205.12507.\nHuan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su,\nand Xifeng Yan. 2016. Table cell search for question\nanswering. In Proceedings of the 25th International\nConference on World Wide Web, WWW ’16, page\n771–782, Republic and Canton of Geneva, CHE. In-\nternational World Wide Web Conferences Steering\nCommittee.\nWang-Chiew Tan, Jane Dwivedi-Yu, Yuliang Li, Lam-\nbert Mathias, Marzieh Saeidi, Jing Nathan Yan, and\nAlon Y . Halevy. 2023. Timelineqa: A benchmark\nfor question answering over timelines. In ACL (to\nappear).\nPeter D Turney and Patrick Pantel. 2010. From fre-\nquency to meaning: Vector space models of se-\nmantics. Journal of artificial intelligence research,\n37:141–188.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019a. Universal adversarial\ntriggers for attacking and analyzing NLP. In Pro-\nceedings of Empirical Methods in Natural Language\nProcessing. Association for Computational Linguis-\ntics.\nEric Wallace, Pedro Rodriguez, Shi Feng, and Jordan\nBoyd-Graber. 2019b. Trick me if you can: Human-in-\nthe-loop generation of adversarial question answering\nexamples. In Transactions of the Association for\nComputational Linguistics, pages 387–401.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. RAT-SQL:\nrelation-aware schema encoding and linking for text-\nto-sql parsers. In ACL, pages 7567–7578. Associa-\ntion for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nJason Weston, Samy Bengio, and Nicolas Usunier. 2011.\nWsabie: Scaling up to large vocabulary image anno-\ntation. In Twenty-Second International Joint Confer-\nence on Artificial Intelligence.\nTomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-\nner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question understand-\ning benchmark. Transactions of the Association for\nComputational Linguistics, 8:183–198.\nWenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei\nDu, Patrick S. H. Lewis, William Yang Wang, Yashar\nMehdad, Wen-tau Yih, Sebastian Riedel, Douwe\nKiela, and Barlas Oguz. 2021. Answering com-\nplex open-domain questions with multi-hop dense\nretrieval. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. 2022a.\nRetrieval-augmented multimodal language modeling.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-Tau Yih. 2022b.\nRetrieval-Augmented multimodal language model-\ning.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jian-\nfeng Gao. 2015. Semantic parsing via staged query\ngraph generation: Question answering with knowl-\nedge base. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 1321–1331, Beijing, China. Association for\nComputational Linguistics.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof\nChoromanski, Adrian Wong, Stefan Welker, Federico\nTombari, Aveek Purohit, Michael Ryoo, Vikas Sind-\nhwani, Johnny Lee, Vincent Vanhoucke, and Pete\nFlorence. 2022. Socratic models: Composing zero-\nshot multimodal reasoning with language.\nChen Zhao, Yu Su, Adam Pauls, and Emmanouil Anto-\nnios Platanios. 2022. Bridging the generalization gap\nin text-to-sql parsing with schema expansion. In ACL\n(1), pages 5568–5578. Association for Computational\nLinguistics.\nChen Zhao, Chenyan Xiong, Hal Daumé III, and Jor-\ndan Boyd-Graber. 2021. Multi-step reasoning over\nunstructured text with beam dense retrieval. In North\nAmerican Association of Computational Linguistics.\nA Appendix\nB View-based QA\nExample run of POST TEXT with the query \"When\nwas the last time I chatted with Avery?\":\nThis query is first matched against a set of avail-\nable views and the best one is picked if there\nis sufficient confidence. In this case, the view\ndaily_chat_log is selected.\nThe query is first translated into an SQLite\nquery:\nSELECT MAX(date)\nFROM daily_chat_log\nWHERE friends LIKE '%Avery%'\nThe SQLite query is then cleaned and “relaxed”.\nFor example, on occasions, an attribute that does\nnot exist is used in the query even though this hap-\npens rarely. In this case, no cleaning is required.\nThe conditions over TEXT types are also relaxed.\nWe convert equality conditions (e.g., friends =\n’Avery’) to LIKE conditions (e.g., friends LIKE\n’%Avery%’) and further relax LIKE condition with\na user-defined CLOSE_ENOUGH predicate.\nSELECT MAX(date)\nFROM daily_chat_log\nWHERE (friends LIKE '%Avery%' OR\nCLOSE_ENOUGH('%Avery%', friends))\nThe above query is executed and the results\nobtained is shown below. We then verbalized\nan answer based on the table result. Result:\n[(’2022/12/26’)]\nReturned answer (verbalized) : The last time I\nchatted with Avery was on December 26, 2022.\nWe observe that Langchain’s SQL-\nDatabaseChain provides a very similar func-\ntionality of matching an incoming query against\navailable tables and generating an SQL query over\nthe matched tables. However, SQLDatabaseChain\ndoes not clean or relax query predicates, and\nrequires one to specify a limit on the number of\nrecords returned. Furthermore, it does not compute\nthe provenance of the answer obtained, as we will\ndescribe in the next section. As we also described\nin Section 7, view-based QA generally outperforms\nSQLDatabaseChain because of its ability to push\naggregates to the database engine instead of relying\non the language model to aggregate the results\n(after using the database engine to compute the\nrelevant records for answering the query.\nProvenance queries: PostText generates queries\nto retrieve records that contributed to the answer\nreturned above. It does so by analyzing every\nselect-from-where-groupby-having subquery\nin the generated query to find tuples that con-\ntributed to every such subquery. For example, the\nfollowing SQL queries are generated to compute\nprovenance.\nSELECT name\nFROM pragma_table_info('daily_chat_log')\nwhere pk;\nq0:\nSELECT eid\nFROM daily_chat_log\nWHERE (friends LIKE '%Avery%' OR\nCLOSE_ENOUGH('%Avery%', friends))\nThe first query above returns the key of the table\nand the second retrieves the keys from the table\nthat contributed to the returned answer.\n[('q0', ('e152',)), ('q0', ('e154',)), ('q0',\n('e169',)), ('q0', ('e176',)), ...]\nC Grading scheme\nThe following is our grading scheme used for grad-\ning the answers generated by different systems\nagainst the ground truth answer:\n• 5 means the systems’s answer has the same\nmeaning as the TRUE answer.\n• 4 means the TRUE answer can be determined\nfrom the system’s answer.\n• 3 means there is some overlap in the system’s\nanswer and the TRUE answer.\n• means there is little overlap in the system’s\nanswer and the TRUE answer.\n• 1 means the system’s answer is wrong, it has\nno relationship with the TRUE answer.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8571030497550964
    },
    {
      "name": "Language model",
      "score": 0.7308881282806396
    },
    {
      "name": "Question answering",
      "score": 0.7254365086555481
    },
    {
      "name": "Parametric statistics",
      "score": 0.6628472208976746
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5849258899688721
    },
    {
      "name": "Natural language processing",
      "score": 0.5657511949539185
    },
    {
      "name": "Parametric model",
      "score": 0.5032088160514832
    },
    {
      "name": "Query language",
      "score": 0.4705154001712799
    },
    {
      "name": "Information retrieval",
      "score": 0.450672447681427
    },
    {
      "name": "Planner",
      "score": 0.43100452423095703
    },
    {
      "name": "Data modeling",
      "score": 0.4118267893791199
    },
    {
      "name": "Database",
      "score": 0.12257909774780273
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    }
  ]
}