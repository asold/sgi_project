{
    "title": "InterpreT: An Interactive Visualization Tool for Interpreting Transformers",
    "url": "https://openalex.org/W3154594410",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2140535399",
            "name": "Vasudev Lal",
            "affiliations": [
                "Intel (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A3156544522",
            "name": "Arden Ma",
            "affiliations": [
                "Intel (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A3043343850",
            "name": "Estelle Aflalo",
            "affiliations": [
                "Intel (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2182747375",
            "name": "Phillip Howard",
            "affiliations": [
                "Intel (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2003738541",
            "name": "Ana Simões",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2883921311",
            "name": "Daniel Korat",
            "affiliations": [
                "Intel (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A88950744",
            "name": "Oren Pereg",
            "affiliations": [
                "Intel (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A2241052686",
            "name": "Gadi Singer",
            "affiliations": [
                "Intel (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2013413257",
            "name": "Moshe Wasserblat",
            "affiliations": [
                "Intel (Israel)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3038035611",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W4288086191",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W3021934057",
        "https://openalex.org/W2948771346",
        "https://openalex.org/W2799186171",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2251294039",
        "https://openalex.org/W2962843214",
        "https://openalex.org/W2251648804",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4288375898",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2970863760",
        "https://openalex.org/W3101662419",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970727289",
        "https://openalex.org/W3117560413",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2604356472"
    ],
    "abstract": "Vasudev Lal, Arden Ma, Estelle Aflalo, Phillip Howard, Ana Simoes, Daniel Korat, Oren Pereg, Gadi Singer, Moshe Wasserblat. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. 2021.",
    "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 135–142\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n135\nInterpreT: An Interactive Visualization Tool for Interpreting\nTransformers\nVasudev Lal1, Arden Ma1, Estelle Aﬂalo1, Phillip Howard1,\nAna Paula Q Simoes1, Daniel Korat2, Oren Pereg2, Gadi Singer1, Moshe Wasserblat2\n1Intel Labs, Cognitive Computing Research, USA\n2Intel Labs, Artiﬁcial Intelligence Lab, Israel\n{firstname.lastname}@intel.com\nAbstract\nWith the increasingly widespread use of\nTransformer-based models for NLU/NLP\ntasks, there is growing interest in understand-\ning the inner workings of these models, why\nthey are so effective at a wide range of tasks,\nand how they can be further tuned and im-\nproved. To contribute towards this goal of en-\nhanced explainability and comprehension, we\npresent InterpreT, an interactive visualization\ntool for interpreting Transformer-based mod-\nels. In addition to providing various mech-\nanisms for investigating general model be-\nhaviours, novel contributions made in Inter-\npreT include the ability to track and visual-\nize token embeddings through each layer of\na Transformer, highlight distances between\ncertain token embeddings through illustrative\nplots, and identify task-related functions of at-\ntention heads by using new metrics. Inter-\npreT is a task agnostic tool, and its functional-\nities are demonstrated through the analysis of\nmodel behaviours for two disparate tasks: As-\npect Based Sentiment Analysis (ABSA) and\nthe Winograd Schema Challenge (WSC).\n1 Introduction\nIn recent years, Transformer-based models\n(Vaswani et al., 2017) such as BERT (Devlin\net al., 2019), GPT-2 (Radford et al., 2019),\nXLNET (Yang et al., 2019) and RoBERTa (Liu\net al., 2019) have demonstrated state-of-the-art\nperformance in many NLP tasks and have become\nthe gold standard. However, there are many open\nquestions regarding the behavior of these models.\nPhenomena such as why Transformers perform\nwell on speciﬁc examples but not others, as well\nas how their internal mechanisms facilitate their\nability to generalize to new tasks and settings\n(or lack therof) are not yet fully understood.\nObservations and insights which help answer\nthese questions will be pivotal in driving the\nconstruction of more powerful and robust models.\nThe pursuit of such answers have spurred the\ndevelopment of a wide variety of analytical stud-\nies and tools to enable the visualization of infor-\nmation encapsulated in Transformer-based mod-\nels. Clark et al. (2019), studied the attention mech-\nanisms of a pre-trained BERT model to ﬁnd that\ncertain heads correspond to speciﬁc linguistic pat-\nterns. Jawahar et al. (2019) investigated the distri-\nbution of phrase-level information throughout the\nlayers of BERT using t-SNE (van der Maaten and\nHinton, 2008). The visualization tools of Aken\net al. (2020) and Reif et al. (2019) perform a layer-\nwise analysis of BERT’s hidden states to under-\nstand the internal workings of Transformer-based\nmodels that are ﬁne-tuned for question-answering\ntasks. Other tools, such as Vig (2019), focus\non visualizations of the attention matrices of pre-\ntrained Transformer models. In the work of Ten-\nney et al. (2020), the authors introduce an inter-\nactive platform for the visualization and interpre-\ntation of NLP models. The tool includes, among\nother capabilities, attention visualizations, embed-\nding space visualizations, and aggregate analysis.\nOther related tools include those by Wallace et al.\n(2019) and Hoover et al. (2020). The increasingly\nlarge body of work on the interpretability and eval-\nuation of Transformer-based models reveals the\ngrowing need for the development of tools and\nsystems to aid in the ﬁne-grained analysis and un-\nderstanding of these models and their performance\non complex language understanding tasks.\nWith this goal in mind, we present InterpreT1,\na tool for interpreting Transformers. A key con-\ntribution of InterpreT is that it is a single system\nthat enables users to track hidden representations\n1The source code for InterpreT, along with a live demo\nand screencast describing its functionality is available at\nhttps://github.com/IntelLabs/nlp-architect/tree/master/solutions/InterpreT\n136\nof tokens throughout each layer of a Transformer\nmodel, as well as visualize and analyze attention\nhead behaviors. Similarly to Tenney et al. (2020),\nInterpreT enables dynamic point selection, aggre-\ngation of attention head statistics, visualization of\nattention head matrices, and the ability to compare\nmodels. Novel contributions made in InterpreT\ninclude the ability to track and visualize token\nembeddings through each layer of a Transformer\n(Section 3.2), highlight distances between certain\ntoken embeddings through illustrative plots (Sec-\ntion 3.6), and identify task-related functions of at-\ntention heads by using new metrics (Section 3.3).\nSection 4 demonstrates how the new features\nintroduced in InterpreT can be used to obtain\nnovel insights into the underlying mechanisms\nused by Transformers to tackle diverse tasks such\nas Aspect-Based Sentiment Analysis (ABSA) and\nthe Winograd Schema Challenge (WSC). More\ngenerally, these demonstrations illustrate how\nsuch features enable rich, granular analysis of\nTransformer models.\n2 System Design and Workﬂow\nThe system ﬂow consists of two main stages: of-\nﬂine extraction of model speciﬁc and task speciﬁc\ninformation such as targets, predictions, relevant\nhidden states, and attention matrices (henceforth\nreferred to as “collateral”) and running the appli-\ncation itself. During the ofﬂine stage, the extracted\nhidden states are processed using t-SNE before be-\ning saved to a ﬁle. The collateral generated for a\nspeciﬁc model and task is independent of collat-\neral from other models and tasks, which enables\nthe user to either run the app to examine a sin-\ngle model or to compare two different models that\nwere evaluated on the same task and data. In this\nlatter case, the collateral ﬁles for the two models\nare linked at runtime. A detailed speciﬁcation for\nthe collateral, along with the source code used to\nrun InterpreT can be found in our GitHub.\n3 Application Features\n3.1 Overview\nKey features of InterpreT include plots for the vi-\nsualization and tracking of t-SNE representations\nof hidden states through the layers of a Trans-\nformer, a plot presenting summary statistics, cus-\ntom metrics to quantify attention head behavior,\nand attention matrix visualizations. In addition,\nInterpreT includes a multi-select feature that en-\nables groups of examples to be selected in the t-\nSNE plot and used as input to other plots in the ap-\nplication, as well as the ﬂexibility to be used both\nfor analyzing a single model and for visualizing\nthe differences in behaviors between two models.\nIn general, the core functionalities present in In-\nterpreT are model and task agnostic, working for\na wide-variety of architectures, sequence lengths,\nand tasks.\n3.2 t-SNE Embeddings\nA central component of InterpreT is the abil-\nity to visualize the contextualized embeddings of\nspeciﬁc tokens throughout the layers of a Trans-\nformer. Following van Aken et al. (2019) and\nJawahar et al. (2019), we use t-SNE to project\nhidden representations of tokens after each Trans-\nformer layer onto a two-dimensional space, creat-\ning disjoint t-SNE spaces for each layer of each\nmodel. In the resulting t-SNE plot, token embed-\ndings can be visualized for a speciﬁc model and\nlayer, and colored using various color schemes\n(Figure 1d). An example selected in the t-SNE\nplot is tracked and continues to be highlighted in\nthe new t-SNE space when the model or the layer\nis changed.\n3.3 Head Summary\nInterpreT includes a head summary plot that dis-\nplays attention head summary statistics for each\nhead and layer (Figure 1b). For a given sen-\ntence, all attention weights are obtained in a\nmatrix of size (num layers × num heads ×\nsentence length ×sentence length) and com-\npute statistics over the ﬁnal two dimensions, yield-\ning a summary plot of size (num layers ×\nnum heads). The following statistics are cur-\nrently supported:\nThe Standard Deviation of an attention head is\ngenerated by calculating the standard deviation of\nthe corresponding attention matrix weights. Intu-\nitively, the standard deviation of an attention head\nincreases as the attention patterns become less uni-\nform, allowing a user to easily identify heads that\nexhibit interesting behaviors.\nThe Attention Matrix Correlation is obtained\nby computing the correlation between an atten-\ntion matrix and an arbitrary, same-size matrix. In\nSection 4.1.2, this correlation is computed using a\nbinary matrix that encodes syntactic dependency\nrelations, analogous to the parse matrix used in\n137\nFigure (1) The InterpreT user interface (rearranged for print) for the task of coreference resolution (see Section\n4.2). The UI includes a short description of the currently selected models and example at the top, along with the\nmain features (a-e) described in Section 3.\nPereg et al. (2020). This formulation of a “gram-\nmar correlation” metric provides an indicator of\nan attention head’s ability to identify syntactic re-\nlations in a sentence.\nThe Task-Speciﬁc Attention Intensity option\nallows a user to deﬁne and display custom met-\nrics that highlight speciﬁc attention patterns. In\nSection 4.2.2, a “coreference intensity” metric is\ndevised to pinpoint attention heads with an afﬁn-\nity for identifying coreference relationships. For\nthis metric, each entry in the summary plot repre-\nsents the attention weight between the coreferent\nspans being evaluated (if the span contains more\nthan one token, the maximum is taken), for each\nhead of each layer.\nWhen running InterpreT with two models, the\nhead summary plot can be used to visualize differ-\nences in the summary statistics between both mod-\nels. As mentioned previously, the multi-select fea-\nture can be used with any of the summary statistic\noptions. When using multi-select, the statistics are\naveraged over the selected examples, enabling the\nuser to analyze general trends in attention behav-\nior.\n3.4 Attention Matrix/Map\nSimilarly to other systems, InterpreT provides the\nability to display the attention patterns and weights\nexhibited by speciﬁc attention heads, which can be\nselected by clicking on a speciﬁc head and layer in\nthe head summary plot. These attention patterns\ncan be displayed as either a heatmap (“matrix”\nview) or a token “map” (“map” view) visualization\nused in Clark et al. (2019). There is an option to\nswitch between the two views in-app (Figure 1c).\nThese visualizations can become unwieldy when\nusing large sequence lengths, but this will not af-\nfect the functionality of the rest of the system.\n3.5 Summary Table\nA short summary table is provided, which contains\ntask-speciﬁc information such as predicted token\nclassiﬁcations and the gold (target) labels for the\nselected sentence/example (Figure 1a).\n138\n(a)\n (b)\n (c)\nFigure (2) Baseline (a) and LIBERT (b,c) ﬁnal layer t-SNE embeddings of aspect terms colored by domain (a,b)\nand aspect extraction sentence level F1 score (c) as seen in InterpreT.\n3.6 Average t-SNE Distance Per Layer\nTo complement t-SNE visualization of the hidden\nstates, InterpreT also introduces a novel plot show-\ning the average t-SNE space distance between spe-\nciﬁc groups of terms across all of the Transform-\ners’ layers (Figure 1e). Section 4.2.1 demonstrates\nhow information conveyed in this plot contributes\ntowards novel interpretations of the inner work-\nings of BERT.\n4 Use Cases\nThe examples presented in this section focus on\nthe analysis of bidirectional encoders using Inter-\npreT, however the system can be applied to gener-\native models or encoder-decoder architectures as\nwell, so long as the appropriate collateral can be\ngenerated. Further examples of use cases along\nwith instructions on how to use InterpreT for cus-\ntom applications is detailed in our GitHub.\n4.1 Cross-Domain Aspect Based Sentiment\nAnalysis (ABSA)\nA fundamental task in ﬁne-grained sentiment anal-\nysis is the extraction of aspect and opinion terms.\nFor example, in the sentence “The chocolate cake\nwas incredible”, the aspect term is chocolate cake\nand the opinion term is incredible. Supervised\nlearning approaches have shown promising results\nin single-domain setups where the training and the\ntesting data are from the same domain. However,\nthese approaches typically do not scale across do-\nmains, where only unlabeled data is available for\nthe target domain. It has been shown that syntax,\nwhich is a basic trait of language and is therefore\ndomain invariant, can help bridge the gap between\ndomains (Ding et al., 2017; Wang and Jialin Pan,\n2018).\nIn a recent work (Pereg et al., 2020), externally\ngenerated dependency relations are integrated into\na pre-trained BERT model through the addition\nof a 13th attention head which incorporates the\ndependency relations into its Syntactically-Aware\nSelf-Attention Mechanism. This model is referred\nto as Linguistically Informed BERT (LIBERT).\nInterpreT is used to analyze LIBERT and a Base-\nline model that shares the same size and structure\nas LIBERT but does not incorporate syntactic in-\nformation for the cross-domain ABSA task, where\nboth models are ﬁne-tuned on laptop reviews and\nare evaluated on restaurant reviews (Pontiki et al.,\n2014, 2015; Wang et al., 2016). LIBERT and\nthe Baseline model achieved aspect extraction F1\nscores of 0.5143 and 0.4254 respectively on vali-\ndation data from the restaurant domain.\n4.1.1 Visualizing the Domain Gap\nInterpreT is used to visualize how the incorpo-\nration of dependency relations in LIBERT con-\ntributes to bridging the gap between domains. Fig-\nure 2 depicts the ﬁnal layer aspect term t-SNE em-\nbeddings from the restaurant and laptop domains\nproduced by LIBERT and Baseline. The plot of\nthe Baseline embeddings (2a) gives a prototypical\ndepiction of the “domain gap” challenge present\nin cross-domain setups, through the clear separa-\ntion of in-domain (blue) and out-of-domain (red)\naspects. Conversely, the plot of LIBERT’s embed-\ndings (2b) demonstrates how LIBERT has learned\nto push the embeddings of some aspect terms from\nthe out-of-domain region into the in-domain re-\ngion, effectively overcoming the “domain gap”\nchallenge for these examples. Furthermore, in the\nplot colored by the aspect extraction F1 score (2c),\nit is seen that LIBERT achieves a high F1 score on\nthe out-of-domain examples that now overlap with\nin-domain examples, highlighting the usefulness\nof such visualizations for analyzing model perfor-\nmance and extensibility.\n139\n(a)\n(b)\nFigure (3) InterpreT’s Head Summary plot displaying aggregated grammar correlation using multi-selection for\nLIBERT (a) along with an example of the the attention matrix of selected attention head (head 13 in layer 4) (b).\n4.1.2 Grammar Correlation\nA key feature of InterpreT is the addition of met-\nrics to help identify attention heads which carry\nout speciﬁc functions. For analyzing LIBERT, the\n“grammar correlation” metric described in Sec-\ntion 3.3 is used to identify attention heads with an\nafﬁnity for detecting syntactic relations. Figure 3a\ndemonstrates the result of using multi-selection to\ncompute the average grammar correlation in each\nof LIBERT’s attention heads aggregated over mul-\ntiple examples.\nAs expected, the Syntactically-Aware Self At-\ntention head (head 13) tends to show much higher\ngrammar correlation than the regular Self Atten-\ntion heads. Utilizing the granularity provided in\nthe head summary plot, it is observed that LIB-\nERT’s 13th head seems to only express an afﬁnity\nfor parsing syntactic relations in layers 2,3,4, and\n11. This is unexpected behavior, as the syntax in-\nformation is relayed identically to the 13th head\nacross all layers. To investigate further, InterpreT\ncan be used to display attention matrices from\nhead 13 in layers that have high grammar corre-\nlation. One such attention matrix, for an out-of-\ndomain example, is displayed in Figure 3b. In this\nattention matrix visualization, it can be seen how\nLIBERT’s 13th head identiﬁes syntactic relations\nsuch as the adjectival modiﬁer relation between\n“staff” and “attentive”, and how this can be use-\nful for the cross-domain ABSA task where “staff”\nand “attentive” are aspect and opinion terms (re-\nspectively) in an out-of-domain example.\n4.2 Coreference Resolution in the Winograd\nSchema Challenge (WSC)\nIn this section, the utility of InterpreT is show-\ncased for a markedly different task: coreference\nresolution. Coreference resolution is a challeng-\ning NLP task that often requires a nuanced under-\nstanding of context and sentence semantics. This\ntask is the basis of the Winograd Schema Chal-\nlenge (WSC) from the SuperGLUE benchmark\n(Alex Wang, 2020), where the goal is to deter-\nmine whether or not a pronoun is the correct ref-\nerent of a given noun phrase. In this analysis of\nWSC, InterpreT demonstrates how information in\nthe attention matrices and the hidden states of a\nTransformer can be used to understand the implicit\nmechanisms contributing to its ability to identify\ncoreferent terms. BERT-base (uncased) is chosen\nfor this analysis and is ﬁne-tuned using the WSC\ntask training set.\nExample Coreference Candidates\n(Fred, he) (George, he)\n“... got back” False True\n“... got up” True True\nTable (1) Predictions of the ﬁne-tuned BERT model\nfor the two examples. The values in bold are correct\npredictions.\n4.2.1 Spatial Convergence of Coreferent\nTerms\nWhile analyzing WSC with InterpreT, the sys-\ntem’s wide-ranging capabilities gave rise to a\nnovel observation, wherein it was discovered that a\nﬁne-tuned BERT model pushes closer together the\nembeddings of terms it predicts to be coreferent.\nFigure 4a displays the average distance per layer\n140\n(a)\n(b)\nFigure (4) InterpreT summary plots for WSC. These plots display summary statistics for the average predicted\nspan token distance per layer (a) and coreference intensity metric (b) for ﬁne-tuned BERT aggregated over the full\ndataset.\n(a)\n (b)\n(c)\n (d)\nFigure (5) InterpreT plots tracking speciﬁc examples in WSC. These plots depict the ﬁnal layer t-SNE embed-\ndings and attention map visualizations of head 10 layer 7 for the following examples: “Fred watched TV while\nGeorge went out to buy groceries. After an hour he got back” (a,c), and “Fredwatched TV while George went out\nto buy groceries. After an hour he got up.” (b,d). In (a) and (b), the yellow stars indicate candidate mention spans,\nand “He” and “George” are almost overlapping.\nbetween terms which BERT predicts to be coref-\nerent (blue) and terms which BERT predicts to not\nbe coreferent (red), aggregated over the full WSC\ndataset. It is observed that in BERT’s ﬁnal layers,\nthe model learns to modify the hidden representa-\ntions of terms to increase or decrease the distance\nbetween them based on whether or not it predicts\nthey are coreferents. This behavior can also be\nseen in the green trace, which measures the dif-\nference in the average distance of terms predicted\nto be coreferent and those that are not predicted to\nbe coreferent.\nAdditionally, Figures 5a and 5b show a speciﬁc\nexample of this phenomenon with the sentences:\n141\n“Fred watched TV while George went out to buy\ngroceries. After an hour he got back” (Figure 5a\nand Table 1) and “Fredwatched TV while George\nwent out to buy groceries. After an hour he got\nup.” (Figure 5b and Table 1). These two exam-\nples show how changing a single token (“ back”\nbecame “up”) signiﬁcantly alters the sentence se-\nmantics, as in the ﬁrst example, “he” refers to\n“George”, and in the second example “he” refers\nto “Fred”. InterpreT enables us to visualize this\nbehavior using the t-SNE plots. Figure 5a show\nhow for the ﬁrst example, “he” and “George” are\nmuch closer together than “he” and “Fred” are.\nFigure 5b shows how in the second example, the\nchange from “he got back” to “he got up” is re-\nﬂected in BERT’s behavior, where the representa-\ntion of “Fred” to be pushed much closer to “he”\nthan in the ﬁrst example.\n4.2.2 Attention Patterns between Coreferent\nTerms\nAnother feature of InterpreT is the ability to utilize\ncustom metrics, such as the “coreference inten-\nsity” metric described in Section 3.3. Coreference\nintensity is visualized using the head summary\nplot in Figure 4b. The ﬁgure shows that the ﬁne-\ntuned model highlights attention heads that seem\nto perform well as coreferent predictors. Darker\nshades of red correspond to higher attention be-\ntween the two coreferents being evaluated. It ap-\npears that the heads which are the most involved\nin the coreference resolution task after ﬁne-tuning\nare the 7th head of layer 10 and the 3rd head of\nlayer 11.\nThis new metric is used to examine the example\npreviously presented with “Fred”, “George”, and\n“he”. Figures 5c and 5d show the attention ma-\ntrix visualizations for the head selected in Figure\n4b (head 7 in layer 10). The token map visualiza-\ntion depicts how “he” attends heavily to “George”\nin the ﬁrst example (5c) while attending to both\n“Fred” and “George” in the second example (5d).\n5 Conclusion and Future Work\nInterpreT is a generic system for interpreting\nTransformers, as evident through its suite of tools\nfor understanding general model behaviors and\nfor enabling granular analysis of attention patterns\nand hidden states for individual examples. The\ncapabilities provided by InterpreT empower users\nwith new insights into what their models are learn-\ning, as illustrated in the visualization of the mit-\nigation of the “domain gap” for ABSA and in\nthe novel discovery of the spatial convergence of\ncoreferent terms in WSC. These examples show-\ncase how the ﬁne-grained analysis enabled by In-\nterpreT affords a higher level of insight that is\nindispensable for interpreting model behavior for\ncomplex language understanding tasks.\nInterpreT is an ongoing development effort. Fu-\nture work will include support for additional use\ncases as well as additional analysis and interactiv-\nity features, such as the ability to dynamically add\nand modify examples while the app is running.\n6 Acknowledgements\nWe thank the anonymous reviewers for their com-\nments and suggestions.\nReferences\nBetty van Aken, Benjamin Winter, Alexander L ¨oser,\nand Felix A. Gers. 2019. How does bert answer\nquestions? Proceedings of the 28th ACM Interna-\ntional Conference on Information and Knowledge\nManagement.\nBetty van Aken, Benjamin Winter, Alexander L ¨oser,\nand Felix A. Gers. 2020. Visbert: Hidden-state vi-\nsualizations for transformers. In Companion Pro-\nceedings of the Web Conference 2020 , WWW ’20,\npage 207–211, New York, NY , USA. Association for\nComputing Machinery.\nNikita Nangia Amanpreet Singh Julian Michael Felix\nHill Omer Levy Samuel R. Bowman Alex Wang,\nYada Pruksachatkun. 2020. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does bert\nlook at? an analysis of bert’s attention. In Black-\nBoxNLP@ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYing Ding, Jianfei Yu, and Jing Jiang. 2017. Recur-\nrent neural networks with auxiliary labels for cross-\ndomain opinion target extraction. In Association\nfor the Advancement of Artiﬁcial Intelligence, pages\n3436––3442.\n142\nBenjamin Hoover, Hendrik Strobelt, and Sebastian\nGehrmann. 2020. exBERT: A Visual Analysis Tool\nto Explore Learned Representations in Transformer\nModels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics: System Demonstrations , pages 187–196, On-\nline. Association for Computational Linguistics.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3651–3657, Florence, Italy. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9(86):2579–2605.\nOren Pereg, Daniel Korat, and Moshe Wasserblat.\n2020. Syntactically aware cross-domain aspect and\nopinion terms extraction. In Proceedings of the 28th\nInternational Conference on Computational Lin-\nguistics, pages 1772–1777, Barcelona, Spain (On-\nline). International Committee on Computational\nLinguistics.\nMaria Pontiki, Dimitris Galanis, Haris Papageorgiou,\nSuresh Manandhar, and Ion Androutsopoulos. 2015.\nSemEval-2015 task 12: Aspect based sentiment\nanalysis. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation (SemEval 2015),\npages 486–495, Denver, Colorado. Association for\nComputational Linguistics.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos,\nHarris Papageorgiou, Ion Androutsopoulos, and\nSuresh Manandhar. 2014. SemEval-2014 task 4:\nAspect based sentiment analysis. In Proceedings of\nthe 8th International Workshop on Semantic Evalua-\ntion (SemEval 2014), pages 27–35, Dublin, Ireland.\nAssociation for Computational Linguistics.\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language mod-\nels are unsupervised multitask learners.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. In Advances in Neural Information Process-\ning Systems, volume 32, pages 8594–8603. Curran\nAssociates, Inc.\nIan Tenney, James Wexler, Jasmijn Bastings, Tolga\nBolukbasi, Andy Coenen, Sebastian Gehrmann,\nEllen Jiang, Mahima Pushkarna, Carey Radebaugh,\nEmily Reif, and Ann Yuan. 2020. The language in-\nterpretability tool: Extensible, interactive visualiza-\ntions and analysis for nlp models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nJesse Vig. 2019. Visualizing attention in transformer-\nbased language representation models.\nEric Wallace, Jens Tuyls, Junlin Wang, Sanjay Sub-\nramanian, Matt Gardner, and Sameer Singh. 2019.\nAllenNLP interpret: A framework for explaining\npredictions of NLP models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP): System Demonstrations , pages\n7–12, Hong Kong, China. Association for Compu-\ntational Linguistics.\nWenya Wang and Sinno Jialin Pan. 2018. Recursive\nneural structural correspondence network for cross-\ndomain aspect and opinion co-extraction. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 1—-11.\nWenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and\nXiaokui Xiao. 2016. Recursive neural conditional\nrandom ﬁelds for aspect-based sentiment analysis.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n616–626, Austin, Texas. Association for Computa-\ntional Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime\nCarbonell, Ruslan Salakhutdinov, and Quoc V .\nLe. 2019. Xlnet: Generalized autoregres-\nsive pretraining for language understand-\ning. Cite arxiv:1906.08237Comment: Pre-\ntrained models and code are available at\nhttps://github.com/zihangdai/xlnet."
}