{
  "title": "Scientific Keyphrase Identification and Classification by Pre-Trained Language Models Intermediate Task Transfer Learning",
  "url": "https://openalex.org/W3117034213",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2111233918",
      "name": "Seoyeon Park",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A334769912",
      "name": "Cornelia Caragea",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W187834739",
    "https://openalex.org/W2102733276",
    "https://openalex.org/W2135644134",
    "https://openalex.org/W630883834",
    "https://openalex.org/W2098921539",
    "https://openalex.org/W2914076857",
    "https://openalex.org/W2970438301",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W2116512345",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W2251833032",
    "https://openalex.org/W331019419",
    "https://openalex.org/W2750939664",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2608018997",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2962903510",
    "https://openalex.org/W2962970841",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W2750621889",
    "https://openalex.org/W2970467549",
    "https://openalex.org/W2951299559",
    "https://openalex.org/W2750859203",
    "https://openalex.org/W3035328829",
    "https://openalex.org/W2952570576",
    "https://openalex.org/W2579538068",
    "https://openalex.org/W2963265326",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2987154886",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Scientific keyphrase identification and classification is the task of detecting and classifying keyphrases from scholarly text with their types from a set of predefined classes. This task has a wide range of benefits, but it is still challenging in performance due to the lack of large amounts of labeled data required for training deep neural models. In order to overcome this challenge, we explore pre-trained language models BERT and SciBERT with intermediate task transfer learning, using 42 data-rich related intermediate-target task combinations. We reveal that intermediate task transfer learning on SciBERT induces a better starting point for target task fine-tuning compared with BERT and achieves competitive performance in scientific keyphrase identification and classification compared to both previous works and strong baselines. Interestingly, we observe that BERT with intermediate task transfer learning fails to improve the performance of scientific keyphrase identification and classification potentially due to significant catastrophic forgetting. This result highlights that scientific knowledge achieved during the pre-training of language models on large scientific collections plays an important role in the target tasks. We also observe that sequence tagging related intermediate tasks, especially syntactic structure learning tasks such as POS Tagging, tend to work best for scientific keyphrase identification and classification.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5409–5419\nBarcelona, Spain (Online), December 8-13, 2020\n5409\nScientiﬁc Keyphrase Identiﬁcation and Classiﬁcation by Pre-Trained\nLanguage Models Intermediate Task Transfer Learning\nSeo Yeon Park\nComputer Science Department\nUniversity of Illinois at Chicago\nspark313@uic.edu\nCornelia Caragea\nComputer Science Department\nUniversity of Illinois at Chicago\ncornelia@uic.edu\nAbstract\nScientiﬁc keyphrase identiﬁcation and classiﬁcation is the task of detecting and classifying\nkeyphrases from scholarly text with their types from a set of predeﬁned classes. This task has a\nwide range of beneﬁts, but it is still challenging in performance due to the lack of large amounts\nof labeled data required for training deep neural models. In order to overcome this challenge, we\nexplore pre-trained language models BERT and SciBERT with intermediate task transfer learn-\ning, using 42 data-rich related intermediate-target task combinations. We reveal that intermediate\ntask transfer learning on SciBERT induces a better starting point for target task ﬁne-tuning com-\npared with BERT and achieves competitive performance in scientiﬁc keyphrase identiﬁcation and\nclassiﬁcation compared to both previous works and strong baselines. Interestingly, we observe\nthat BERT with intermediate task transfer learning fails to improve the performance of scientiﬁc\nkeyphrase identiﬁcation and classiﬁcation potentially due to signiﬁcant catastrophic forgetting.\nThis result highlights that scientiﬁc knowledge achieved during the pre-training of language mod-\nels on large scientiﬁc collections plays an important role in the target tasks. We also observe that\nsequence tagging related intermediate tasks, especially syntactic structure learning tasks such as\nPOS Tagging, tend to work best for scientiﬁc keyphrase identiﬁcation and classiﬁcation.\n1 Introduction\nScientiﬁc Keyphrase Identiﬁcation and Classiﬁcation (SKIC) is the task of identifying and classifying\nscientiﬁc terms in research papers. An effective keyphrase identiﬁcation and classiﬁcation system can\nbeneﬁt a wide range of natural language processing and information retrieval tasks including question\nanswering (Quarteroni and Manandhar, 2006), question generation (Subramanian et al., 2018), and expert\nﬁnding (Chen et al., 2015; Augenstein et al., 2017). Several works formulate SKIC as a classiﬁcation\ntask using neural methods and word embeddings (Luan et al., 2018; Augenstein et al., 2017; Liu et al.,\n2017) and show promising results compared to previous hand-crafted feature processing with sequence\nlabeling such as Conditional Random Fields (Lee et al., 2017).\nScientiﬁc keyphrases are very diverse and this diversity leads to a burden for the creation of large\ndataset collections, which results in data scarcity problems for deep neural networks since domain experts\nare required to obtain reliable annotations for keyphrase identiﬁcation and classiﬁcation. In order to\novercome the small size data problem of SKIC, Augenstein and Søgaard (2017) propose deep multi-task\nlearning and reveal that several related tasks such as noun and verb phrase chunking, super-sense tagging,\nand multi-word expression identiﬁcation are helpful to SKIC. However, the results of this approach are\nstill low (Augenstein et al., 2017), which implies that there is room for improvement.\nRecent works in natural language processing show that employing unsupervised pre-trained language\nmodels such as BERT (Devlin et al., 2019) and SciBERT (Beltagy et al., 2019), a variant of BERT that is\npre-trained on scholarly texts, can bring substantial improvement in the performance of various Natural\nLanguage Understanding (NLU) tasks (Jiang and de Marneffe, 2019; Klein and Nabi, 2019). Further-\nmore, Phang et al. (2018) and Pruksachatkun et al. (2020) propose to further improve these language\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:\nhttp://creativecommons.org/licenses/by/4.0/\n5410\nmodels by using intermediate task transfer learning. Intermediate task transfer learning is a simple strat-\negy of ﬁne-tuning on a data-rich intermediate task before ﬁne-tuning on the downstream target tasks. To\nthis end, we investigate the performance of intermediate task transfer learning for SKIC. Speciﬁcally, we\nrun our experiments on seven intermediate tasks on BERT and SciBERT and six target tasks (three for\nkeyphrase boundary identiﬁcation and three for keyphrase type classiﬁcation) using three datasets of sci-\nentiﬁc papers that cover different domains, e.g., Materials Science, Physics, Computer Science, Natural\nLanguage Processing, and Artiﬁcial Intelligence.\nOur contributions are summarized as follows: First, we build a transfer learning framework employing\na diverse range of intermediate tasks covering sequence tagging with semantic and syntactic aspects, and\nnatural language inference. We achieve competitive performance over both strong baselines and previous\nworks. While transfer learning using SciBERT successfully achieves improvement in performance, we\nobserve that intermediate transfer learning using BERT causes catastrophic forgetting (Chronopoulou et\nal., 2019) with a signiﬁcant performance deterioration. Second, we empirically observe that speciﬁc in-\ntermediate task types are more useful as intermediate tasks for SKIC. Speciﬁcally, sequence tagging tasks\nsuch as POS tagging are preferable than natural language inference tasks such as entailment recognition.\nThird, we provide a qualitative analysis of extracted keyphrases and show that our transfer learning\nsuccessfully returns keyphrases. This qualitative analysis proves the reliability of our proposed methods.\n2 Related Work\nScientiﬁc keyphrase identiﬁcation and classiﬁcation (SKIC) was proposed at SemEval 2017 Task 10 as\nTask A: Keyphrase Identiﬁcation, and Task B: Keyphrase Classiﬁcation (Augenstein et al., 2017). The\nauthors of this shared task proposed to employ the BIO schema, which refers to the beginning (B),\ninside (I), or outside (O) of keyphrases, respectively. Most of the earlier approaches to SKIC rely on\nhand-crafted linguistic features. For example, Lee et al. (2017) and Marsi et al. (2017) employ syntactic\nand semantic features such as Part-of-Speech tags and word lemmas, as input to Conditional Random\nFields (CRFs). Liu et al. (2017) formulate SKIC as a supervised multi-class classiﬁcation problem.\nThey exploit pre-trained word embeddings and linguistically inspired features, e.g., noun phrase features\nand orthographic features, which represent character and symbolic features of given tokens, as input to\nSupport Vector Machines. With the success of neural models, recent works try to address SKIC using\nneural architectures while exploiting the BIO schema. Although both tasks, keyphrase identiﬁcation\nand keyphrase classiﬁcation according to their types, are very important, many works focused only on\nkeyphrase extraction/generation or identiﬁcation/segmentation (Meng et al., 2017; Xiong et al., 2019;\nPatel and Caragea, 2019; Alzaidy et al., 2019; Chen et al., 2020). The classiﬁcation task is less explored\npossibly due to a lack of a large number of gold-label keyphrase classiﬁcation datasets. Precisely, there\nare only a few publicly available datasets for keyphrase classiﬁcation (QasemiZadeh and Schumann,\n2016; Augenstein et al., 2017; Luan et al., 2018) and these datasets are small in size. To overcome the\nsmall dataset size problem, Augenstein and Søgaard (2017) proposed the multitask learning of SKIC.\nAccording to this work, they reveal that sequence tagging auxiliary tasks such as Chunking, Super-sense\nTagging, Multi-words Expressions Identiﬁcation, and FrameNet Target Identiﬁcation are beneﬁcial to\nSKIC within a multitask learning framework (with one auxiliary task at a time).\nThe recent unsupervised pre-trained language models such as BERT (Devlin et al., 2019) achieve state-\nof-the-art performance in many downstream NLP tasks, such as named entity recognition e.g., 95.5%\nF1 on the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003). Han and Eisenstein (2019)\napply BERT and use ﬁne-tuning of BERT to reduce the vocabulary gap between canonical domains and\nhistorical domains, within an unsupervised approach. Moreover, the domain-speciﬁc BERT models such\nas SciBERT (Beltagy et al., 2019), produce better results compared to a general domain-based BERT\nwith respect to scientiﬁc knowledge required tasks such as scientiﬁc term classiﬁcation e.g., 64.57% F1\non the SciIE dataset (Luan et al., 2018). To exploit these pre-trained language models, Phang et al. (2018)\npropose a data-rich intermediate transfer learning and prove that this method provides a better starting\npoint of target tasks. Extended from this, in our work, we propose the transfer of the intermediate tasks\nﬁne-tuning to overcome the scarcity of gold-labeled large SKIC datasets.\n5411\nFigure 1: The overview of intermediate task transfer learning for keyphrase identiﬁcation and classiﬁca-\ntion by using pre-trained language models. The words from the input shown in bold represent keyphrases\nand their types are shown in the classiﬁcation output.\n3 Methodology\nGiven a document d = {w1, w2, . . . wn}, where n denotes the length of the document and wk is a word\nin d (k = 1, ··· , n), the objective of SKIC is to identify and classify keyphrases of d by employing\nan output sequence {y1, y2, . . . yn}, where yk follows BIO scheme for pre-deﬁned classes, e.g., task,\nmaterial, process. In the BIO scheme, ‘B’ denotes the beginning word of a keyphrase, ‘I’ refers to the\ninside word of a keyphrase, and ‘O’ indicates all other words that are not part of a keyphrase. An example\nof input-output pairs of keyphrase identiﬁcation and classiﬁcation is shown in Figure 1.\n3.1 Experimental Pipeline\nOur transfer learning consists of two phases as shown in Figure 1: (1) Pre-trained language models BERT\nand SciBERT are ﬁne-tuned respectively on a single intermediate task; (2) The pre-trained language\nmodels’ ﬁne-tuned-parameters are transferred to the target task and ﬁne-tuned further on each target\ntask. In target task ﬁne-tuning, we apply different learning rates to retain knowledge that is acquired from\nintermediate tasks. Speciﬁcally, as shown in Figure 1, the target task’s fully connected classiﬁer uses a\nhigher learning rate, while pre-trained language models’ parameters are assigned as a lower learning rate\nto retain knowledge.\n3.2 Intermediate Tasks\nTo explore what intermediate tasks are effective for keyphrase identiﬁcation and classiﬁcation, we choose\nvarious tasks. Intermediate tasks statistics are shown in Table 1. Note that for intermediate tasks that are\nrelated to sequence tagging, we report phrase-level instance numbers.\nWe choose seven intermediate tasks based on the following considerations: 1) Augenstein and Sø-\ngaard (2017) propose several auxiliary tasks where they improve performance of SKIC using a multitask\nlearning framework; 2) Pruksachatkun et al. (2020) prove the usefulness of Natural Language Inferences\n(NLIs) in intermediate task transfer learning for general Natural Language Understanding. In our set-\nting, document understanding is essential to identifying the precise keyphrases (or the important parts)\nfor a document. In experiments, we use two NLI tasks where each task has different vocabulary sets,\ni.e., general domain (MNLI) and scientiﬁc domain (SciTail); 3) A traditional NLP sequence tagging\ntask, i.e., Part-Of-Speech Tagging, to understand whether a model that acquire better ability of under-\nstanding grammatical structures improves the performance of SKIC. The detailed information of each\nintermediate task follows.\nSupersense Supersense tagging (Johannsen et al., 2014) is the task of mapping semantic units such as\ncompounds, phrase and several other linguistic terms to Wordnet’s lexicographers classes using Semcor\n5412\nTask Name |Train| |Dev| |Class| Task Type Domain\nSupersense 8,006 1,186 3 Sequence tagging Newswire, Novel\nMWEs 11,890 1,146 57 Sequence tagging Online review\nFrameNet 9,334 4,000 92 Sequence tagging Open domain\nChunking 104,054 11,588 12 Sequence tagging Wall Street Journal\nSciTail 23,596 1,304 2 Natural language inference Science exams\nMNLI 392,703 20,000 3 Natural language inference Fiction, Letters, Telephone Speech\nPOS Tagging 334,180 54,138 15 Sequence tagging Wikipedia, Talks, Literature\nTable 1: The overview of intermediate tasks in our experiments.\n3.0 corpus.1 In this work, we focus on noun supersense classes which are Person, Location and Group.\nFor example, in a given sentence ‘Boston Red Sox outﬁelder Jackie Jensen said he played baseball on\nmonday night’, the supersense of Boston Red Sox is Group, and that of Jackie Jensen is Person.\nMWEs Multiword Expressions Identiﬁcation and Classiﬁcation (Schneider and Smith, 2015) is the\ntask of detecting and classifying idiosyncratic noun and verb combinations to literal categories, which\nare high-level ontological semantic classes, using Streusle corpus. 2 For example, in a given sentence\n‘We have been blessed to ﬁnd elite ﬂyers online and would not use anyone else to handle our postcards\nposters etc. ’, examples of high-level ontological semantic classes are: for blessed is verb.cognition, for\nﬁnd is verb.cognition, for use is verb.social and for postcards posters is noun.artifact.\nFrameNet FrameNet Target Identiﬁcation (Das et al., 2014) is the task of detecting semantic frame\nevoking word sequences or words in the FrameNet 1.7 corpus, which contains lexical and predicate-\narguments. Frames are words or phrases that describe events, relations, objects and the participants in it.\nFor example, a target such as moist in a sentence evokes the frame Being as state.\nSciTail SciTail (Khot et al., 2018) is the task of recognizing the entailment of a hypothesis that is\nconstructed from a science question and its corresponding answer by employing the premise. The dataset\nis collected by crowd-sourcing and by multiple-choice science questions from 4th-grade and 8th-grade\nexams. For example, the relation between the following two sentences, ‘Neurons receive information\nfrom dendrites which are then passed to the soma cell body. ’and ‘Dendrites from the cell body receives\nimpulses from other neurons. ’is labeled as Entail.\nMNLI Multi-Genre Natural Language Inference (Williams et al., 2018) is the task of determining\ntextual entailment in sentence pairs across a variety of genres of written and spoken English, ranging\nfrom ﬁction to face-to-face conversations. For example, the relation between the following two sentences,\n‘This approach provides perhaps a better technique for isolating the actual costs of the emissions caps.’,\n‘There is no way to estimate the actual cost of emissions caps.’ is labeled as Contradiction.\nChunking Text Chunking is the task of detecting the chunks of words in CoNLL-2000 shared dataset\n(Tjong Kim Sang and Buchholz, 2000). The chunk tags include various grammatical classes such as\nnoun phrase, verb phrase, prepositional phrase and these tags follow the BIO schema. For a sentence\n‘He reckons the current account deﬁcit will narrow to only #1.8 billion in September’ can be annotated\nas,‘[NP He] [VP reckons] [NP the current account deﬁcit] [VP will narrow] [PP to] [NP only # 1.8\nbillion] [PP in] [NP September]’, where NP refers to noun phrase, VP refers to verb phrase, PP refers\nto prepositional phrase.\nPOS Tagging POS Tagging is the task of labeling each word in a sentence with its part of speech tag.\nIn this work, we employ the English POS tagging annotation collection of universal dependency parsing\n(McDonald et al., 2013). As an example, the following sentence, ‘Aesthetic appreciation and spanish\nart. ’, has the grammar class sequence as [‘ADJ’, ‘NOUN’, ‘CCONJ’, ‘ADJ’, ‘NOUN’].\n1https://web.eecs.umich.edu/ mihalcea/downloads.html\n2https://github.com/nert-nlp/streusle\n5413\nTask Name |Train| |Dev| |Test| |Class| |Avg KP per Doc| |Avg Doc Word Count| Domain\nSemEval 2017 5,992 1,076 1,817 3 18 162 Computer Science, Physics,\nand Material Science\nACL RD-TEC 2.01,930 214 1,088 7 12 107 Natural Language Processing\nSciIE 5,712 641 1,677 6 17 120 Artiﬁcial Intelligence\nTable 2: The statistics of target tasks in our experiments.\n3.3 Target Tasks\nTable 2 shows the characteristics of our target tasks. The detailed information is presented below.\nSemEval 2017 Task 103 SemEval 2017 (Augenstein et al., 2017) is a scientiﬁc keyphrase boundary\nidentiﬁcation and classiﬁcation dataset. The SemEval 2017 dataset covers three domains: materials\nscience, physics, and computer science. The dataset has three pre-deﬁned classes which are Process,\nTask, and Material.\nACL RD-TEC 2.0 4 ACL (QasemiZadeh and Schumann, 2016) is a term and entity categorization\ntask in scientiﬁc text. The ACL dataset covers the domain related to natural language processing. The\nACL dataset has seven pre-deﬁned classes, which areTechnology (Tech), Tool, Language Resources (Lr),\nLanguage Resources Product (Lr-prod), Measurement, Model, and Other. Note that the class Other here\nis different from the class Other in the BIO scheme that represents non-keyphrase.\nSciIE5 SciIE (Luan et al., 2018) covers the task of detecting scientiﬁc entities, their relations, and\ncoreference clusters. In this work, we focus on term identiﬁcation and classiﬁcation. The SciIE dataset\ncovers the domain related to artiﬁcial intelligence in computer science. The SciIE dataset has six pre-\ndeﬁned classes, which are Generic, Metric, Method, Task, Material, and OtherScientiﬁcTerm (Other).\n4 Experiments\nBaseline We compare the intermediate task transfer learning with the following baselines.\n•BiLSTM (Augenstein and Søgaard, 2017)A 3-layer BiLSTM with SENNA embeddings6 for each\ntarget task.\n•BiLSTM-MTL (Augenstein and Søgaard, 2017) Multitask learning of 3-layer BiLSTMs with\nSENNA embeddings.\n•BERT (Devlin et al., 2019) BERTbase ﬁne-tuning on a single target task.\n•SciBERT (Beltagy et al., 2019) SciBERT ﬁne-tuning on a single target task.\nExperimental Setup For the SemEval 2017 and SciIE, we use the published train, validation, and test\nsets. For the ACL RD-TEC 2.0 dataset, we perform 60/10/30 split to create the train, validation, and test\nsets. We estimate models’ hyper-parameters via a grid search over combinations. The ranges of param-\neters we explore are the following: Batch size [1,4,8,16,32,64], learning rate [0.01, 0.0001], momentum\n[0.5, 0.9]. The information about the best hyper-parameters setting is as follows. For each target task,\nwe use the batch size 1. The intermediate tasks of Supersense, MWEs, FrameNet use batch size 4, while\nChunking uses 32, POS Tagging uses 64, SciTail uses 16, and MNLI uses 4. For BERT and SciBERT\nﬁne-tuning, we use SGD optimizer with learning rate 5e-3 with momentum 0.9. In intermediate-task\ntransfer learning, we train intermediate tasks with learning rate 5e-3 without applying momentum. We\nset a higher learning rate as 5e-3 and set a lower learning rate as 5e-4, both with momentum 0.9. We run\n3https://scienceie.github.io/\n4https://github.com/languagerecipes/acl-rd-tec-2.0\n5http://nlp.cs.washington.edu/sciIE/\n6https://ronan.collobert.com/senna/\n5414\nSemEval 2017 Task 10 ACL RD-TEC 2.0 SciIE\nIdentiﬁcation ClassiﬁcationIdentiﬁcation ClassiﬁcationIdentiﬁcation Classiﬁcation\nBiLSTM (Augenstein and Søgaard, 2017)67.71 38.01 81.85 58.51 72.33 58.05\nBiLSTM-MTL w/ Supersense 63.93 43.54 81.36 58.95 72.65 54.33\nBiLSTM-MTL w/ MWEs 72.42 45.49 80.69 56.87 72.92 55.21\nBiLSTM-MTL w/ FrameNet 65.18 45.24 81.68 58.89 70.44 52.60\nBiLSTM-MTL w/Chunking 63.96 42.86 81.37 57.84 75.40 59.43\nBERT (Devlin et al., 2019) 60.40 46.82 79.50 51.67 81.02 65.44\nTransfer From Supersense/ BERT63.52 49.02 71.03 44.78 75.53 57.40\nTransfer From MWEs/ BERT 59.22 45.92 70.70 43.13 75.22 57.05\nTransfer From FrameNet/ BERT61.81 46.06 71.04 42.64 73.82 55.06\nTransfer From SciTail/ BERT 60.11 47.81 63.77 39.87 74.59 55.73\nTransfer From MNLI/ BERT 49.44 44.52 64.51 40.22 73.21 54.42\nTransfer From Chunking/ BERT65.87 47.90 77.72 45.54 75.81 55.18\nTransfer From POS Tagging/ BERT59.64 42.08 76.97 45.77 75.87 52.89\nSciBERT (Beltagy et al., 2019) 66.70 48.21 79.77 65.11 81.02 67.44\nTransfer From Supersense/ SciBERT70.39 51.58 81.62 67.75 82.65 70.20\nTransfer From MWEs/ SciBERT72.09 50.90 83.65 67.55 80.55 67.94\nTransfer From FrameNet/ SciBERT73.89 53.70 82.95 66.73 80.94 69.63\nTransfer From SciTail/ SciBERT68.49 55.52 82.89 68.20 81.28 67.57\nTransfer From MNLI/ SciBERT70.35 54.08 76.51 63.16 76.51 75.23\nTransfer From Chunking/ SciBERT72.07 53.75 83.28 66.35 77.08 66.20\nTransfer From POS Tagging/ SciBERT70.90 56.90 88.01 69.90 78.62 66.64\nTable 3: The results of BERT and SciBERT intermediate task transfer learning in comparison with\nprevious work. We use the micro-average F1 score. The results are highlighted with green (↑) and\nred (↓) with respect to BERT (middle) and SciBERT (bottom). Underlined scores are best within each\ngroup and bold scores are best overall.\ntraining for a maximum of 10 epochs with negative log-likelihood loss. Aside from these details, we fol-\nlow the SciBERT paper for all other training hyper-parameters. All experiments are done in p3.2xlarge\nsettings of Amazon Web Services. All model parameters are estimated on the validation set of each task.\nWe evaluate the performance of each model using phrase-level micro-averaged F1 and use the exact\nmatch metric (Kim et al., 2010).\n5 Results and Analysis\n5.1 Discussion\nTable 3 presents our identiﬁcation and classiﬁcation results. We make the following observations.\nFirst, we observe that SciBERT generally shows higher performance across all of our target tasks\nin comparison to BERT. Interestingly, BERT and SciBERT have different vocabulary set which only\noverlapped 42% (Beltagy et al., 2019). We posit that scientiﬁc knowledge that is available in SciBERT\nboosts the performance of SKIC. However, the ﬁne-tuning of SciBERT on keyphrase identiﬁcation for\nboth SemEval 2017 and ACL datasets still remains challenging since the performance is lower than the\nbest performance of the previous work (Augenstein and Søgaard, 2017). For example, the best baseline\nF1-score on SemEval 2017 keyphrase identiﬁcation is 72.42%, whereas that of SciBERT ﬁne-tuned on\nSemEval 2017 keyphrase identiﬁcation achieves only 66.70%.\nSecond, our SciBERT transfer learning achieves the best results in all identiﬁcation and classiﬁcation\ntarget tasks. This implies that intermediate task ﬁne-tuning leads to better starting points for the target\ntask ﬁne-tuning by injecting the knowledge related to SKIC. Speciﬁcally, in keyphrase identiﬁcation and\nclassiﬁcation of the SemEval 2017 dataset, the model gains signiﬁcant effectiveness when FrameNet is\nemployed as an intermediate task (+7.19 F1), and when POS Tagging (+8.69 F1) is used as an interme-\ndiate task, respectively. For the ACL dataset, we observe that exploiting POS Tagging as an intermediate\ntask achieves the best performance of both identiﬁcation (+6.16 F1) and classiﬁcation (+4.79 F1). This\nimplies that a better syntactic understanding ability of the model induces performance improvement. The\nSciIE dataset performs the best with transfer from Supersense in keyphrase identiﬁcation (+1.63 F1), and\nMNLI in keyphrase classiﬁcation (+7.79 F1).\n5415\nFigure 2: The keyphrase classiﬁcation results confusion matrix visualization of each target dataset. The\nx-axis refers to true classes and the y-axis refers to predicted classes in each target task. The numbers\nare normalized by the count of predicted keyphrases.\nMore interestingly, the contribution of intermediate tasks to target tasks is different by theirtask types.\nWhen natural language inference tasks are employed as intermediate tasks, we generally observe perfor-\nmance degradation, while most of the sequence tagging tasks are consistently helpful for SKIC. Syntac-\ntically sequence tagging tasks such as chunking and POS Tagging are also generally helpful to SKIC.\nAnother important observation is that the domain difference in natural language inference tasks (general\nand scientiﬁc domain) results in different outputs. For example, in all three datasets, transferring from\nMNLI in SciBERT shows better performance than transferring from SciTail in BERT. MNLI and SciTail\nlie on similar inference recognition settings but have different domains, which are general domains and\nscientiﬁc domains, respectively.\nThird, we observe that BERT potentially suffers from catastrophic forgetting as opposed to SciBERT.\nIn BERT transfer learning, most of the intermediate tasks fail to provide useful knowledge for the target\ntasks (i.e., they appear to make the ﬁne-tuned models forget the good information learned within the\npre-trained BERT), and hence, result in a severe deterioration of performance. For example, on the ACL\nRD-TEC 2.0 keyphrase identiﬁcation task, when SciTail is employed as an intermediate task on BERT\ntransfer learning, the performance is decreased by as much as -15.73 in F1, compared to single BERT\nﬁne-tuning. As we can see from Table 3, all cases of BERT intermediate task transfer learning in ACL\nand SciIE dataset degrade performance in comparison to BERT, while most of the cases in SciBERT\ntransfer learning improve performance compared to that of SciBERT.\n5416\nArticle from SemEval 2017 Task 10\nPV cells are one of the most promising technologies for conversion of incident solar radiation into electric power.\nHowever, this technology is still far from being able to compete with fossil fuel-based energy conversion technologies\nbecause of its relatively low efﬁciency and energy density. Theoretically, there are three unavoidable losses that limit the solar\nconversion efﬁciency of a device with a single absorption threshold or band gap Eg: (1) incomplete absorption,\nwhere photons with energies below Eg are not absorbed; (2) thermalization or carrier cooling, where solar photons\nwith sufﬁcient energy generate electron-hole pairs and then immediately lose almost all energy in excess of Eg in the form of heat;\nand (3) radiative recombination, where a small fraction of the excited states radioactively recombine with the ground state\nat the maximum power output (Hanna & Nozik, 2006; Henry, 1980). Taking an air mass of 1.5 as an example,\nfor different band gap Eg these three losses can be calculated and the results are indicated by areas S1, S2, and S3 in Fig. 1.\nNote that the area under the outer curve is the solar power per unit area, and that only S4 can be delivered to the load.\nGold Keyphrases\n(‘PV cells’, ‘Task’) (‘conversion of incident solar radiation into electric power’, ‘Process’)\n(‘incident solar radiation’, ‘Material’) (‘electric power’, ‘Material’)\n(‘fossil fuel-based energy conversion’, ‘Process’) (‘absorption’, ‘Process’)\n(‘incomplete absorption’, ‘Process’) (‘thermalization’, ‘Process’) (‘carrier cooling’, ‘Process’)\n(‘photons’, ‘Material’) (‘solar photons’, ‘Material’)\n(‘electron-hole pairs’, ‘Material’) (‘radiative recombination’, ‘Process’) (‘solar power’, ‘Process’)\nKeyphrase Outputs of Transfer from POS Tagging/ SciBERT\n(‘PV cells’, ‘Material’) (‘conversion of incident solar radiation into electric power’, ‘Task’)\n(‘fossil fuel based energy conversion technologies’, ‘Material’) (‘absorption’, ‘Process’)\n(‘incomplete absorption’, ‘Process’) (‘carrier cooling’, ‘Process’),\n(‘photons’, ‘Material’) (‘photons’, ‘Material’)\n(‘electron-hole pairs’, ‘Material’) (‘heat’, ‘Process’) (‘radiative recombination’, ‘Process’)\n(‘air mass’, ‘Material’) (‘solar’, ’Material’)\nTable 4: The comparison between gold keyphrase and classiﬁed keyphrase outputs of SemEval 2017\nTask 10 dataset.\n5.2 Error Analysis\nWe visualize confusion matrices of the best performing SciBERT transfer learning keyphrase classiﬁ-\ncation results in Figure 2. Speciﬁcally, for the SemEval 2017 Task 10 dataset and the ACL dataset, we\nplot the result of SciBERT transfer learning from POS tagging. For the SciIE dataset, we plot the re-\nsult of SciBERT transfer learning from MNLI. The numbers in Figure 2 represent how many classiﬁed\nkeyphrases belong to each true class. For example, in SemEval 2017 Task 10 dataset confusion matrix,\nthe cell corresponding to rowProcess and column Task refers to the ratio of keyphrases predicted asTask\nbut which should be classiﬁed as Process to the total number of keyphrases that are classiﬁed as Task.\nConsequently, each row in every confusion matrix sums up to 1.\nWe observe that SemEval 2017 data suffers from mis-classifying the class Task possibly due to the\nimbalanced data distribution. The distribution of categories in SemEval 2017 Task 10, Process, Mate-\nrial, Task is 50%/35%/15%, respectively. Moreover, our SciBERT transfer learning makes erroneous\npredictions between Process and Task categories due to the subjectivity of two classes as shown in Table\n4. Table 4 presents an example article of the SemEval 2017 and its keyphrase classiﬁcation comparison\nbetween gold labels and SciBERT transfer learning from POS Tagging. We observe that the keyphrase\nconversion of incident solar radiation into electric power is annotated as Process, while it is reasonable\nto think of it as Task, which is the output of our transfer learning model prediction. This type of er-\nror is not necessarily a shortcoming of SciBERT, but rather of the data annotation and its subjectivity.\nAccording to this, we can also conﬁrm the difﬁculties of the SKIC data collections.\nFor the ACL dataset, we observe that the category Language Resource (LR) is mis-classiﬁed as Other\nclass. For example, treebank is annotated as LR but our model predicts it as Other. Further, the data\nimbalanced also causes performance degradation in the ACL dataset. This is because the proportion of\nLR, which is one of the classes in the ACL dataset, is very small in size (5.7%), while the percentage of\nOther is signiﬁcantly higher (42.8%). Interestingly, in contrast to the above two target datasets, in the\nSciIE dataset, our SciBERT transfer learning generally performs very well.\n5417\n6 Conclusion, Discussion, and Future Work\nWe investigated the performance of data-rich intermediate task transfer learning for scientiﬁc keyphrase\nidentiﬁcation and classiﬁcation (SKIC) using pre-trained language models BERT and SciBERT. We per-\nform experiments on SciBERT and BERT with a total of 42 pairs of intermediate and target tasks, where\nintermediate tasks are drawn from sequence tagging and natural language inference. We found that\nemploying sequence tagging tasks as intermediate tasks on SciBERT performs the best on three pub-\nlicly available keyphrase identiﬁcation and classiﬁcation datasets. Further, the intermediate task transfer\nlearning using SciBERT outperforms both the previous work and strong baselines by a large margin.\nSpeciﬁcally, for the SemEval 2017 Task 10 dataset, using FrameNet as an intermediate task on SciBERT\ntransfer learning yields +7.19 improvement in F1 in keyphrase identiﬁcation, and using POS Tagging\nyields +8.69 improvement in F1 in keyphrase classiﬁcation. For the ACL RD-TEC 2.0 dataset, using\nPOS Tagging leads to +6.16 F1 in keyphrase identiﬁcation, and +4.79 F1 in keyphrase classiﬁcation.\nFor the SciIE dataset, using Supersense brings +1.63 F1 in keyphrase identiﬁcation, and +7.79 F1 in\nkeyphrase classiﬁcation. According to these results, syntactic structure learning related intermediate\ntasks such as POS Tagging are preferable for SKIC tasks. Further, looking at our intermediate task do-\nmain difference that lies in natural language inference tasks, we explore their impact on the pre-trained\nlanguage models. In particular, we empirically showed that using MNLI as an intermediate task on\nSciBERT transfer learning returns higher performance than employing SciTail as an intermediate task on\nBERT transfer learning. Future works in this area will beneﬁt from the improvement of the available in-\ntermediate tasks and other related intermediate tasks will be explored. Moreover, a better understanding\nof when and why these intermediate tasks are working is one of the interesting future directions.\nInterestingly, we observe that BERT suffers from a serious drop in performance possibly due to catas-\ntrophic forgetting. In particular, we observe that almost all of the intermediate tasks fail to provide better\nstarting points for SKIC pre-trained language models ﬁne-tuning. While SemEval 2017 Task 10 dataset\nachieves the best results when transfer from Chunking on BERT, this result is lower than single SciB-\nERT ﬁne-tuning. On ACL RD-TEC 2.0 and SciIE, no intermediate task produces higher performance on\nBERT intermediate task transfer learning. One potential reason could be a large vocabulary gap between\nour domain and the collections used to pre-train BERT. In the future, we plan to analyze the differences\nbetween BERT and SciBERT to better understand the effects of transfer learning for SKIC.\nAcknowledgements\nWe thank Isabelle Augenstein for several clariﬁcations of the task and the evaluation approach. We also\nthank our anonymous reviewers for their constructive comments and feedback, which helped improve our\npaper. This research is supported in part by NSF CAREER award #1802358, NSF CRI award #1823292,\nand UIC Discovery Partners Institute to Cornelia Caragea. Any opinions, ﬁndings, and conclusions\nexpressed here are those of the authors and do not necessarily reﬂect the views of NSF.\nReferences\nRabah Alzaidy, Cornelia Caragea, and C. Lee Giles. 2019. Bi-LSTM-CRF sequence labeling for keyphrase\nextraction from scholarly documents. In Ling Liu, Ryen W. White, Amin Mantrach, Fabrizio Silvestri, Julian J.\nMcAuley, Ricardo Baeza-Yates, and Leila Zia, editors, The World Wide Web Conference, WWW 2019, San\nFrancisco, CA, USA, May 13-17, 2019, pages 2551–2557. ACM.\nIsabelle Augenstein and Anders Søgaard. 2017. Multi-task learning of keyphrase boundary classiﬁcation. In\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers), pages 341–346, Vancouver, Canada, July. Association for Computational Linguistics.\nIsabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, and Andrew McCallum. 2017. SemEval\n2017 task 10: ScienceIE - extracting keyphrases and relations from scientiﬁc publications. In Proceedings of\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 546–555, Vancouver, Canada,\nAugust. Association for Computational Linguistics.\n5418\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientiﬁc text. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615–3620, Hong Kong,\nChina, November. Association for Computational Linguistics.\nHung-Hsuan Chen, II Ororbia, G Alexander, and C Lee Giles. 2015. Expertseer: A keyphrase based expert\nrecommender for digital libraries. arXiv preprint arXiv:1511.02058.\nWang Chen, Hou Pong Chan, Piji Li, and Irwin King. 2020. Exclusive hierarchical decoding for deep keyphrase\ngeneration. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages\n1095–1105, Online, July. Association for Computational Linguistics.\nAlexandra Chronopoulou, Christos Baziotis, and Alexandros Potamianos. 2019. An embarrassingly simple ap-\nproach for transfer learning from pretrained language models. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 2089–2095, Minneapolis, Minnesota, June. Association for Compu-\ntational Linguistics.\nDipanjan Das, Desai Chen, André F. T. Martins, Nathan Schneider, and Noah A. Smith. 2014. Frame-semantic\nparsing. Computational Linguistics, 40(1):9–56, March.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsupervised domain adaptation of contextualized embeddings\nfor sequence labeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,\npages 4238–4248, Hong Kong, China, November. Association for Computational Linguistics.\nNanjiang Jiang and Marie-Catherine de Marneffe. 2019. Evaluating BERT for natural language inference: A\ncase study on the CommitmentBank. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 6086–6091, Hong Kong, China, November. Association for Computational Linguistics.\nAnders Johannsen, Dirk Hovy, Héctor Martínez Alonso, Barbara Plank, and Anders Søgaard. 2014. More or\nless supervised supersense tagging of twitter. In Proceedings of the Third Joint Conference on Lexical and\nComputational Semantics (*SEM 2014) , pages 1–11, Dublin, Ireland, August. Association for Computational\nLinguistics and Dublin City University.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question\nanswering. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.\nSu Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. SemEval-2010 task 5 : Automatic\nkeyphrase extraction from scientiﬁc articles. In Proceedings of the 5th International Workshop on Semantic\nEvaluation, pages 21–26, Uppsala, Sweden, July. Association for Computational Linguistics.\nTassilo Klein and Moin Nabi. 2019. Attention is (not) all you need for commonsense reasoning. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 4831–4836, Florence, Italy,\nJuly. Association for Computational Linguistics.\nLung-Hao Lee, Kuei-Ching Lee, and Yuen-Hsien Tseng. 2017. The NTNU system at SemEval-2017 task 10:\nExtracting keyphrases and relations from scientiﬁc publications using multiple conditional random ﬁelds. In\nProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 951–955, Van-\ncouver, Canada, August. Association for Computational Linguistics.\nSijia Liu, Feichen Shen, Vipin Chaudhary, and Hongfang Liu. 2017. MayoNLP at SemEval 2017 task 10: Word\nembedding distance pattern for keyphrase classiﬁcation in scientiﬁc publications. In Proceedings of the 11th\nInternational Workshop on Semantic Evaluation (SemEval-2017), pages 956–960, Vancouver, Canada, August.\nAssociation for Computational Linguistics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identiﬁcation of entities, rela-\ntions, and coreference for scientiﬁc knowledge graph construction. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pages 3219–3232, Brussels, Belgium, October-November.\nAssociation for Computational Linguistics.\n5419\nErwin Marsi, Utpal Kumar Sikdar, Cristina Marco, Biswanath Barik, and Rune Sætre. 2017. NTNU-1@ScienceIE\nat SemEval-2017 task 10: Identifying and labelling keyphrases with conditional random ﬁelds. In Proceed-\nings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) , pages 938–941, Vancouver,\nCanada, August. Association for Computational Linguistics.\nRyan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,\nKeith Hall, Slav Petrov, Hao Zhang, Oscar Täckström, Claudia Bedini, Núria Bertomeu Castelló, and Jungmee\nLee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meet-\ning of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 92–97, Soﬁa, Bulgaria,\nAugust. Association for Computational Linguistics.\nRui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, and Yu Chi. 2017. Deep keyphrase gen-\neration. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 582–592, Vancouver, Canada, July. Association for Computational Linguistics.\nKrutarth Patel and Cornelia Caragea. 2019. Exploring word embeddings in crf-based keyphrase extraction from\nresearch papers. In Mayank Kejriwal, Pedro A. Szekely, and Raphaël Troncy, editors, Proceedings of the 10th\nInternational Conference on Knowledge Capture, K-CAP 2019, Marina Del Rey, CA, USA, November 19-21,\n2019, pages 37–44. ACM.\nJason Phang, Thibault Févry, and Samuel R Bowman. 2018. Sentence encoders on stilts: Supplementary training\non intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088.\nYada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara\nVania, Katharina Kann, and Samuel R. Bowman. 2020. Intermediate-task transfer learning with pretrained\nlanguage models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5231–5247, Online, July. Association for Computational Linguistics.\nBehrang QasemiZadeh and Anne-Kathrin Schumann. 2016. The ACL RD-TEC 2.0: A language resource for\nevaluating term extraction and entity recognition methods. InProceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16) , pages 1862–1868, Portorož, Slovenia, May. European\nLanguage Resources Association (ELRA).\nSilvia Quarteroni and Suresh Manandhar. 2006. User modeling for adaptive question answering and information\nretrieval. In FLAIRS Conference, pages 776–781.\nNathan Schneider and Noah A. Smith. 2015. A corpus and model integrating multiword expressions and super-\nsenses. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, pages 1537–1547, Denver, Colorado, May–June. Associ-\nation for Computational Linguistics.\nSandeep Subramanian, Tong Wang, Xingdi Yuan, Saizheng Zhang, Adam Trischler, and Yoshua Bengio. 2018.\nNeural models for key phrase extraction and question generation. In Proceedings of the Workshop on Machine\nReading for Question Answering , pages 78–88, Melbourne, Australia, July. Association for Computational\nLinguistics.\nErik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 shared task chunking. In\nFourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic\nWorkshop.\nErik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learn-\ning at HLT-NAACL 2003, pages 142–147.\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages\n1112–1122, New Orleans, Louisiana, June. Association for Computational Linguistics.\nLee Xiong, Chuan Hu, Chenyan Xiong, Daniel Campos, and Arnold Overwijk. 2019. Open domain web keyphrase\nextraction beyond language modeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 5175–5184, Hong Kong, China, November. Association for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8321271538734436
    },
    {
      "name": "Task (project management)",
      "score": 0.7433114051818848
    },
    {
      "name": "Transfer of learning",
      "score": 0.7210710048675537
    },
    {
      "name": "Identification (biology)",
      "score": 0.7067301273345947
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6258506178855896
    },
    {
      "name": "Forgetting",
      "score": 0.6096240282058716
    },
    {
      "name": "Natural language processing",
      "score": 0.5158225893974304
    },
    {
      "name": "Multi-task learning",
      "score": 0.4324301481246948
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.43177396059036255
    },
    {
      "name": "Machine learning",
      "score": 0.32703647017478943
    },
    {
      "name": "Linguistics",
      "score": 0.12902095913887024
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    }
  ],
  "cited_by": 19
}