{
  "title": "Adaptive Contrastive Learning on Multimodal Transformer for Review Helpfulness Prediction",
  "url": "https://openalex.org/W4385573727",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101561868",
      "name": "T. Q. Nguyen",
      "affiliations": [
        "National University of Singapore",
        "VinUniversity"
      ]
    },
    {
      "id": "https://openalex.org/A5011376608",
      "name": "Xiaobao Wu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5050386762",
      "name": "Anh Tuan Luu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5025881601",
      "name": "Zhen Hai",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A5086674741",
      "name": "Lidong Bing",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3156636935",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2593833795",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W4283802945",
    "https://openalex.org/W3035207996",
    "https://openalex.org/W3080412027",
    "https://openalex.org/W3199893015",
    "https://openalex.org/W2983128379",
    "https://openalex.org/W2911594722",
    "https://openalex.org/W4286895139",
    "https://openalex.org/W1671770126",
    "https://openalex.org/W2964284628",
    "https://openalex.org/W2019983800",
    "https://openalex.org/W3188030217",
    "https://openalex.org/W4309443149",
    "https://openalex.org/W3199926081",
    "https://openalex.org/W3035565303",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3128419247",
    "https://openalex.org/W4297833089",
    "https://openalex.org/W1985554184",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3175377564",
    "https://openalex.org/W2157364932",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2087294982",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3199527540",
    "https://openalex.org/W2804582864",
    "https://openalex.org/W2783640434",
    "https://openalex.org/W2949963192",
    "https://openalex.org/W2124833832",
    "https://openalex.org/W3197032408",
    "https://openalex.org/W3202125623"
  ],
  "abstract": "Modern Review Helpfulness Prediction systems are dependent upon multiple modalities, typically texts and images. Unfortunately, those contemporary approaches pay scarce attention to polish representations of cross-modal relations and tend to suffer from inferior optimization. This might cause harm to model’s predictions in numerous cases. To overcome the aforementioned issues, we propose Multi-modal Contrastive Learning for Multimodal Review Helpfulness Prediction (MRHP) problem, concentrating on mutual information between input modalities to explicitly elaborate cross-modal relations. In addition, we introduce Adaptive Weighting scheme for our contrastive learning approach in order to increase flexibility in optimization. Lastly, we propose Multimodal Interaction module to address the unalignment nature of multimodal data, thereby assisting the model in producing more reasonable multimodal representations. Experimental results show that our method outperforms prior baselines and achieves state-of-the-art results on two publicly available benchmark datasets for MRHP problem.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10085–10096\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nAdaptive Contrastive Learning on Multimodal Transformer for\nReview Helpfulness Predictions\nThong Nguyen1,2, Xiaobao Wu 3, Anh Tuan Luu 3∗,\nCong-Duy Nguyen3, Zhen Hai4, Lidong Bing4\n1National University of Singapore, Singapore\n2VinAI Research, Vietnam\n3Nanyang Technological University, Singapore\n4DAMO Academy, Alibaba Group\ne0998147@u.nus.edu, anhtuan.luu@ntu.edu.sg\nAbstract\nModern Review Helpfulness Prediction sys-\ntems are dependent upon multiple modalities,\ntypically texts and images. Unfortunately,\nthose contemporary approaches pay scarce at-\ntention to polish representations of cross-modal\nrelations and tend to suffer from inferior opti-\nmization. This might cause harm to model’s\npredictions in numerous cases. To overcome\nthe aforementioned issues, we propose Multi-\nmodal Contrastive Learning for Multimodal\nReview Helpfulness Prediction (MRHP) prob-\nlem, concentrating on mutual information be-\ntween input modalities to explicitly elaborate\ncross-modal relations. In addition, we intro-\nduce Adaptive Weighting scheme for our con-\ntrastive learning approach in order to increase\nflexibility in optimization. Lastly, we propose\nMultimodal Interaction module to address the\nunalignment nature of multimodal data, thereby\nassisting the model in producing more reason-\nable multimodal representations. Experimental\nresults show that our method outperforms prior\nbaselines and achieves state-of-the-art results\non two publicly available benchmark datasets\nfor MRHP problem.\n1 Introduction\nCurrent e-commerce sites such as Amazon, Ebay,\netc., construct review platforms to collect user feed-\nback concerning their products. These platforms\nplay a fundamental role in online transactions since\nthey help future consumers collect useful reviews\nwhich assist them in deciding whether to make\nthe purchase or not. Unfortunately, nowadays the\nnumber of user-generated reviews is overwhelming,\nraising doubts related to the relevance and verac-\nity of reviews. Therefore, there is a need to verify\nthe quality of reviews before publishing them to\nprospective customers. As a result, this inspires a\nrecent surge of interest targeting the Review Help-\nfulness Prediction (RHP) problem.\n∗Corresponding Author\nProduct Information\nThe Cooks Standard 6-Quart Stainless Steel Stockpot\nwith Lid is made with 18/10 stainless steel with an\naluminum disc layered in the bottom. The aluminum\ndisc bottom provides even heat distribution and prevents\nhot spots. Tempered glass lid with steam hole vent\nmakes viewing food easy. Stainless steel riveted handles\noffer durability. Induction compatible. Works on gas,\nelectric, glass, ceramic, etc. Oven safe to 500F, glass lid\nto 350F. Dishwasher safe.\nReview 1\nI needed a stainless steel pot for canning my tomatoes. I\nlearned the hard way that you have to use a non-reactive\npot or else your end result will be inedible (I thought I\nwas using stainless steel but quickly realized it wasnt) I\nheaded to Amazon and came across this Cooks Standard\nSS Cookpot with cover and bought it after reading the\nreviews. I have had it for just under a year and it still\nlooks just as good as the day I bought it. I couldn’t be\nhappier with my purchase! Oh, and by the way, this one\nactually is stainless steel unlike the other pot I bought\nthat said it was and wasn’t.\nReview 2\nI ordered it on May 21st. What a waste of time and\nmoney.\nReview 1 Review 2\nLabel score 4 1\nMCR score 0.168 3.637\nOur Model score 4.651 0.743\nTable 1: Example of unreasonable predictions in the\nMultimodal Review Helpfulness Prediction task.\nTwo principal groups of early efforts focus on\npurely textual data. The first group follows feature\nengineering techniques, retrieving argument-based\nfeatures (Liu et al., 2017), lexical features (Kr-\n10085\nishnamoorthy, 2015), and semantic features (Kim\net al., 2006), as input to their classifier. Inherently,\ntheir methods are labor-intensive and vulnerable to\nthe typical issues of conventional machine learning\nmethods. Instead of relying on manual features,\nthe second group leverages deep neural models, for\ninstance, RNN (Alsmadi et al., 2020) and CNN\n(Chen et al., 2018), to learn rich features automat-\nically. Nonetheless, their approach is ineffective\nbecause the helpfulness of a review is not only\ncontingent upon textual information but also other\nmodalities.\nTo cope with the above issues, recent works (Liu\net al., 2021b; Han et al., 2022) proposed to utilize\nmulti-modality via the Multi-perspective Coherent\nReasoning (MCR) model. Hypothesizing that a\nreview is helpful if it exhibits coherent text and\nimages with the product information, those works\ntake into account both textual and visual modality\nof the inputs, then estimate their coherence level\nto discern whether the reviews are helpful or un-\nhelpful. However, the MCR model contains a detri-\nmental drawback. Particularly, it aims to maximize\nthe scores sp of positive (helpful) product-review\npairs while minimizing those sn of negative (un-\nhelpful) pairs. Hence, it was assumed that follow-\ning the aforementioned manner would project fea-\ntures with similar semantics to stay close and those\nwith disparate ones to be distant apart. Unfortu-\nnately, in multimodal learning, this was shown not\nto be the case, causing the model to learn ad-hoc\nrepresentations (Zolfaghari et al., 2021). This is\none reason leading to unreasonable predictions of\nMCR in Table 1. As it can be seen, even though\nReview 1 closely relates to the product of “6-Quart\nStainless Steel Stockpot”, the model classifies it as\nunhelpful. In addition, the target of Review 2’s text\ncontent is vague because it does not specifically\ncorrespond to the “ Stockpot”. In fact, it can be\nused for any product. Moreover, the image does\nnot clearly show any hint of the “Stockpot” as well.\nDespite such vagueness, the output of MCR for\nReview 2 is still helpful.\nAs a remedy to this problem, we propose Cross-\nmodal Contrastive Learning to mine the mutual\ninformation of cross-modal relations in the input\nto capture more sensible representations. Nonethe-\nless, plainly applying symmetric gradient pattern,\nwhich is similar to MCR that they assign equivalent\npenalty to snand sp, is inflexible. In cases thatspis\nsmall and sn is already negatively skewed, or both\nsp and sn are positively skewed, it is irrational to\nassign equivalent penalties to both sp and sn. Last\nbut not least, MCR directly leverages Coherent\nReasoning, repeatedly enforcing alignment among\nmodalities in the input. This ignores the unaligned\nnature of multimodal input, for example, images\nmight only refer to a particular section in the text,\nhence do not completely align with the textual con-\ntent. In consequence, strictly forming alignment\ncan make the model learn inefficient multimodal\nrepresentations (Tsai et al., 2019).\nTo overcome the above problems, we propose\nan adaptive scheme to accomplish the flexibility in\nthe optimization of our contrastive learning stage.\nFinally, we propose to adopt a multimodal attention\nmodule that reinforces one modality’s high-level\nfeatures with low-level ones of other modalities.\nThis not only relaxes the alignment assumption but\nalso informs one modality of information of others,\nencouraging refined representation learning.\nIn sum, our contributions are three-fold:\n• We propose an Adaptive Cross-modal Con-\ntrastive Learning for Review Helpfulness Pre-\ndiction task by polishing cross-modal relation\nrepresentations.\n• We propose a Multimodal Interaction module\nwhich correlates modalities’ features without\ndepending upon the alignment assumption.\n• We conducted extensive experiments on two\ndatasets for the RHP problem and found that\nour method outperforms other baselines which\nare both textual-only and multimodal, and ob-\ntains state-of-the-art results on those bench-\nmarks.\n2 Model Architecture\nIn this section we delineate the overall architecture\nof our MRHP model. Particular modules of our\nsystem are depicted in Figure 1.\n2.1 Problem Definition\nGiven a product item p, which consists of a de-\nscription Tp and images Ip, and a set of reviews\nR = {r1,...,r N}, where each review is com-\nposed of user-generated text Tr\ni and images Ir\ni,\nRHP model’s task is to generate the scores\nsi = f(p,ri), 1 ≤i≤N (1)\n10086\nThe rug gives visitors\na destination for\nconvenience after\nstepping forward the\ndoor\nThis is the best\npurchase I have\never made ...\nText\nEncoder\nText\nEncoder\nImage\nEncoder\nImage\nEncoder\nMultimodal Interaction Module\nIntra-modal Inter-modal\nOutput Score\nMulti-head Attention\nLinear Linear\nQγ[0]\nQγ[i]\nLinear\nQ VKD x\nLinear\nQγ[D]\nXη[0]\nProduct Text Review Text Review ImageProduct Image\nInter-modal Intra-modal\nLinear\nKp Kr Ap Ar\nHp Hr Vp Vr\nh p h r v p v r\nf (p,r)\nFigure 1: Diagram of our Multimodal Review Helpfulness Prediction model.\nwhere Nis the number of reviews for productpand\nf is the scoring function of the RHP model. Em-\npirically, each score estimated by f indicates the\nhelpfulness level of each review, and the ground-\ntruth is the descending sort order of helpfulness\nscores.\n2.2 Encoding Modules\nOur model accepts product description Tp, product\nimages Ip, review text Tr\ni , and review images Ir\ni\nas input. The encoding process of those elements\nis described as follows.\nText EncodingProduct description and review text\nare sequences of words. Each sequence is indexed\ninto the word embedding layer and then passed into\nthe respective LSTM layer for product or review.\nKp = LSTMp(Wemb(Tp)) (2)\nKr = LSTMr(Wemb(Tr)) (3)\nwhere Kp ∈Rlp×d, Kr ∈Rlr×d, lp and lr are\nthe sequence lengths of product and review text\nrespectively, and dis the hidden size.\nImage Encoding We follow Anderson et al. (2018)\nto take detected objects as embeddings of the im-\nage. In particular, a pre-trained Faster R-CNN\nis applied to extract ROI features for m objects\n{a1,a2,..., am}from the product and review im-\nages. Subsequently, we encode extracted features\nusing the self-attention module (SelfAttn) (Vaswani\net al., 2017)\nA= SelfAttn({a1,a2,..., am}) (4)\nwhere A ∈Rm×d and dis the hidden size. Here\nwe use Ap and Ar to indicate product and review\nimage features, respectively.\n2.3 Multimodal Interaction Module\nWe consider two components γ, ηwith their inputs\nXγ, Xη, where ηis the concatenation of input el-\nements apart from the one in γ. For instance, if\nγ = Kp, then η = [Kr,Ap,Ar], where [.,.] indi-\ncates the concatenation operation. We define each\ncross-modal attention block to have three compo-\nnents Q, K, and V:\nQγ = Xγ ·WQγ (5)\nKη = Xη ·WKη (6)\nVη = Xη ·WVη (7)\nwhere WQγ ∈ Rdγ×dk, WKη ∈ Rdη×dk, and\nWVη ∈Rdη×dv are weight matrices. The inter-\naction between γ and ηis computed in the cross-\nattention manner\nZγ = CMγ(Xγ,Xη) =softmax\n(\nQγ ·KT\nη√dk\n)\n·Vη\n(8)\nOur full module comprises Dlayers of the above-\nmentioned attention block, as indicated in the right\n10087\npart of Figure 1. Theoretically, the computation is\ncarried out as follows\nQγ[0] =Xγ (9)\nT[i] =CMγ[i](LN(Qγ[i−1]),LN(Xη)) (10)\nUγ[i] =T[i] +Qγ[i−1] (11)\nQγ[i] =GeLU(Linear(Uγ[i])) (12)\nwhere LN denotes layer normalization operator. We\niteratively estimate cross-modal features for prod-\nuct text, product images, review text, and review\nimages with a view to obtaining Hp, Vp, Hr, and\nVr.\nHp = Qp\nk[D], V p = Qp\na[D] (13)\nHr = Qr\nk[D], V r = Qr\na[D] (14)\nAfter our cross-modal interaction module, we\nproceed to pass features to undertake relation fu-\nsion in three paths: intra-modal, inter-modal, and\nintra-review.\nIntra-modal Fusion The intra-modal alignment\nis calculated for two relation kinds: (1) product\ntext - review text and (2) product image - review\nimage. Firstly, we learn alignment among intra-\nmodal features via self-attention modules\nHintraM = SelfAttn([Hp,Hr]) (15)\nVintraM = SelfAttn([Vp,V r]) (16)\nThen intra-modal hidden representations are fed to\na CNN, and continuously a max-pooling layer to\nattain salient entries\nzintraM = MaxPool(CNN([HintraM,V intraM]))\n(17)\nInter-modal Fusion Similar to intra-modal align-\nment, inter-modal one is calculated for two types\nof relations as well: (1) product text - review im-\nage and (2) product image - review text. The first\nstep is also to relate feature components using self-\nattention modules\nHprd_txt - rvw_img = SelfAttn([Hp,V r]) (18)\nHprd_img - rvw_txt = SelfAttn([Vp,Hr]) (19)\nWe adopt a mean-pool layer to aggregate inter-\nmodal features and then concatenate the pooled\nvectors to construct the final inter-modal represen-\ntation\nIprd_txt - rev_img = MeanPool(Hprd_txt - rvw_img)\n(20)\nIprd_img - rev_txt = MeanPool(Hprd_img - rvw_txt)\n(21)\nzinterM = [Iprd_txt - rvw_img,Iprd_img - rvw_txt] (22)\nIntra-review Fusion The estimation of intra-\nreview module completely mimics the inter-modal\nmanner. The only discrimination is that the esti-\nmation is taken upon two different relations: (1)\nproduct text - product image and (2) review text -\nreview image.\nHprd_txt - prd_img = SelfAttn([Hp,V p]) (23)\nHrvw_txt - rev_img = SelfAttn([Hr,V r]) (24)\nGprd_txt - prd_img = MeanPool(Hprd_txt - prd_img)\n(25)\nGrvw_txt - rvw_img = MeanPool(Hrvw_txt - rvw_img)\n(26)\nzintraR = [Gprd_txt - prd_img,Grvw_txt - rvw_img] (27)\nFinally, we concatenate intra-modal, inter-modal,\nand intra-review output, and then feed the concate-\nnated vector to the linear layer to obtain the ranking\nscore:\nzfinal = [zintraM,zinterM,zintraR] (28)\nf(p,ri) =Linear(zfinal) (29)\n3 Training Strategies\n3.1 Adaptive Cross-modal Contrastive\nLearning\nIn this section, we explain the formulation and\nadaptive pattern along with its derivation of our\nCross-modal Contrastive Learning.\nCross-modal Contrastive Learning First of all,\nwe extract hidden states of helpful product-review\npairs. Second of all, hidden features are max-\npooled to extract meaningful entries.\nhp = MaxPool(Hp), hr = MaxPool(Hr) (30)\nvp = MaxPool(Vp), vr = MaxPool(Vr) (31)\nWe formulate our contrastive learning framework\ntaking positive and negative pairs from the above-\nmentioned cross-modal features. In our framework,\nwe hypothesize that pairs established by modalities\n10088\nof the same sample are positive, whereas those\nformed by modalities of distinct ones are negative.\nLCE = −\nB∑\ni=1\nsim(t1\ni,t2\ni)+\nB∑\nj=1,k=1,j̸=k\nsim(t1\nj,t2\nk)\n(32)\nwhere t1,t2 ∈ {hp,hr,vp,vr}, and B denotes\nthe batch size in the training process.\nAdaptive Weighting The standard contrastive ob-\njective suffers from inflexible optimization due to\nirrational gradient assignment to positive and nega-\ntive pairs. As a result, to tackle the problem, we pro-\npose the Adaptive Weighting Strategy for our con-\ntrastive framework. Initially, we introduce weights\nϵp and ϵn to represent distances from the optimum,\nthen integrate them into positive and negative terms\nof our loss.\nLAdaptiveCE = −\nB∑\ni=1\nϵp\ni ·sim(t1\ni,t2\ni)\n+\nB∑\nj=1,k=1,j̸=k\nϵn\nj,k ·sim(t1\nj,t2\nk)\n(33)\nwhere ϵp\ni = [ op −sim(t1\ni,t2\ni)]+ and ϵn\nj,k =\n[sim(t1\nj,t2\nk) −on]+. Investigating the intuition to\ndetermine the values for op and on, we continue\nto conduct derivation and arrive in the following\ntheorem\nTheorem 1 Adaptive Contrastive Loss (33) has\nthe hyperspherical form:\nLAdaptiveCE =\nB∑\ni=1\n(\nsim(t1\ni,t2\ni) −op\n2\n)2\n+\nB∑\nj=1,k=1,j̸=k\n(\nsim(t1\nj,t2\nk) −on\n2\n)2\n−C,\nwhere C >0\nWe provide the proof for Theorem (1) in the Ap-\npendix section. As a consequence, theoretically the\ncontrastive objective arrives in the optimum when\nsim(t1\ni,t2\ni) = op\n2 and sim(t1\nj,t2\nk) = on\n2 . Based\nupon this observation, in our experiments we set\nop = 2and on = 0.\n3.2 Training Objective\nFor the Review Helpfulness Prediction problem,\nthe model’s parameters are updated according to\nthe pairwise ranking loss as follows\nLranking =\n∑\ni\nmax(0,β −f(pi,r+) +f(pi,r−))\n(34)\nwhere r+and r−are random reviews in which r+\npossesses a higher helpfulness level than r−. We\njointly combine the contrastive goal with the rank-\ning objective of the Review Helpfulness Prediction\nproblem to train our model\nL= LAdaptiveCE + Lranking (35)\n4 Experiments\nDatasetSplit Category (Product / Review)\nClothing Electronics. Home\nLazada Train & Dev8K/130K 5K/52K 4K/16K\nTest 2K/32K 1K/13K 1K/13K\nAmazonTrain & Dev16K/349K 13K/325K 18K/462K\nTest 4K/87K 3K/80K 5K/111K\nTable 2: Statistics of MRHP datasets.\n4.1 Datasets\nWe evaluate our methods on two publicly avail-\nable benchmark datasets for MRHP task: Lazada-\nMRHP and Amazon-MRHP.\nLazada-MRHP (Liu et al., 2021b) consists of prod-\nuct items and artificial reviews on Lazada.com, an\ne-commerce platform in Southest Asia. All of the\ntexts in the dataset are expressed in Indonesian.\nAmazon-MRHP (Liu et al., 2021b) is collected\nfrom Amazon.com, the large-scale international\ne-commerce platform. Product information and\nassociated reviews are in English and extracted\nbetween 2016 and 2018.\nBoth datasets comprise 3 categories: (i) Cloth-\ning, Shoes & Jewelry (Clothing), (ii) Electronics\n(Electronics), and (iii) Home & Kitchen (Home).\nWe present the statistics of them in Table 2.\n4.2 Implementation Details\nWe use a 1-layer LSTM with hidden dimension\nsize of 128. We initialize our word embedding\nwith fastText embedding (Bojanowski et al., 2017)\nfor Lazada-MRHP dataset and 300-dimensional\nGloVe pretrained word vectors (Pennington et al.,\n2014) for Amazon-MRHP dataset. We set our mul-\ntimodal attention module to have D= 5attention\nlayers. For the visual modality, we extract 2048-\ndimensional ROI features from each image and\nencode them into 128-dimensional vectors. Our\n10089\nType Method Clothing Electronics Home\nMAP N@3 N@5 MAP N@3 N@5 MAP N@3 N@5\nText-only\nBiMPM 60.0 52.4 57.7 74.4 67.3 72.2 70.6 64.7 69.1\nEG-CNN 60.4 51.7 57.5 73.5 66.3 70.8 70.7 63.4 68.5\nConv-KNRM 62.1 54.3 59.9 74.1 67.1 71.9 71.4 65.7 70.5\nPRH-Net 62.1 54.9 59.9 74.3 67.0 72.2 71.6 65.2 70.0\nMultimodal\nSSE-Cross 66.1 59.7 64.8 76.0 68.9 73.8 72.2 66.0 71.0\nDR-Net 66.5 60.7 65.3 76.1 69.2 74.0 72.4 66.3 71.4\nMCR 68.8 62.3 67.0 76.8 70.7 75.0 73.8 67.0 72.2\nOur Model 70.3 64.7 69.0 78.2 72.4 76.5 75.2 68.8 73.7\nTable 3: Helpfulness Prediction results on Lazada-MRHP dataset.\nType Method Clothing Electronics Home\nMAP N@3 N@5 MAP N@3 N@5 MAP N@3 N@5\nText-only\nBiMPM 57.7 41.8 46.0 52.3 40.5 44.1 56.6 43.6 47.6\nEG-CNN 56.4 40.6 44.7 51.5 39.4 42.1 55.3 42.4 46.7\nConv-KNRM 57.2 41.2 45.6 52.6 40.5 44.2 57.4 44.5 48.4\nPRH-Net 58.3 42.2 46.5 52.4 40.1 43.9 57.1 44.3 48.1\nMultimodal\nSSE-Cross 65.0 56.0 59.1 53.7 43.8 47.2 60.8 51.0 54.0\nDR-Net 65.2 56.1 59.2 53.9 44.2 47.5 61.2 51.8 54.6\nMCR 66.4 57.3 60.2 54.4 45.0 48.1 62.6 53.5 56.6\nOur Model 67.4 58.6 61.6 56.5 47.6 50.8 63.5 54.6 57.8\nTable 4: Helpfulness Prediction results on Amazon-MRHP dataset.\nentire model is trained end-to-end with Adam opti-\nmizer (Kingma and Ba, 2014) and batch size of 32.\nFor the training objective, we set the value of the\nmargin in the ranking loss to be 1.\n4.3 Baselines\nWe compare our proposed architecture against the\nfollowing baselines:\n• BiMPM (Wang et al., 2017): a ranking model\nwhich encodes input sentences in two direc-\ntions to ascertain the matching result.\n• Conv-KNRM (Dai et al., 2018): a CNN-\nbased model which encodes n-gram of multi-\nple lengths and uses kernel pooling to generate\nthe final ranking score.\n• EG-CNN (Chen et al., 2018): a CNN-based\nmodel targeting data scarcity and OOV prob-\nlem in RHP task via taking advantage of\ncharacter-based representations and domain\ndiscriminators.\n• PRH-Net (Fan et al., 2019): a baseline to\npredict helpfulness of a review by taking into\nconsideration both product text and product\nmetadata.\n• DR-Net (Xu et al., 2020): a cross-modality\napproach that models contrast in associated\ncontexts by leveraging decomposition and re-\nlation modules.\n• SSE-Cross (Abavisani et al., 2020): multi-\nmodal model to fuse different modalities with\nstochastic shared embeddings.\n• MCR (Liu et al., 2021b): a baseline model\nfocusing on coherent reasoning.\n4.4 Automatic Evaluation\nIn Table 3 and 4, we follow previous work (Liu\net al., 2021b) to report Mean Average Preci-\nsion (MAP), Normalized Discounted Cumulative\nGain (NDCG@N) (Järvelin and Kekäläinen, 2017)\nwhere N = 3and N = 5. As it can be seen, multi-\nmodal approaches achieve better performance than\ntext-only ones.\nFor Lazada-MRHP dataset, we achieve an ab-\nsolute improvement of NDCG@3 of 2.4 points in\n10090\nDataset Clothing Electronics Home\nMAP N@3 N@5 MAP N@3 N@5 MAP N@3 N@5\nLazada 4.48·10−2 1.55·10−2 3.93·10−2 4.54·10−3 1.05·10−4 2.63·10−3 1.09·10−3 3.40·10−2 3.68·10−3\nAmazon3.45·10−2 4.22·10−2 1.86·10−2 4.37·10−3 2.81·10−2 3.04·10−2 2.04·10−3 3.30·10−3 6.50·10−3\nTable 5: Significance test of the results of our model against MCR model.\nClothing, NDCG@5 of 1.5 points in Electronics,\nand MAP of 1.4 points in Home over the previ-\nous best method, which is MCR. In addition, our\nmodel also obtains better results than the best text-\nonly RHP model, which is PRH-Net, with a gain of\nNDCG@3 of 9.8 points in Clothing, NDCG@5 of\n4.3 points in Electronics, and MAP of 3.6 points in\nHome. Those results prove that our method can pro-\nduce reasonable rankings for associated reviews.\nFor Amazon dataset, which is written in English,\nour model outperforms MCR on all 3 categories, by\nNDCG@5 of 1.4 points in Clothing, 2.7 points in\nElectronics, and 1.2 points in Home, respectively.\nThese results have verified that our interaction mod-\nule and optimization approach can come up with\nmore useful multimodal fusion than previous state-\nof-the-art baselines, not only in English context but\nother language one as well.\nWe also perform significance tests to evaluate\nthe statistical significance of our improvement on\ntwo datasets Amazon-MRHP and Lazada-MRHP,\nand note p-values in Table 5. As shown in the table,\nall of the p-values are smaller than 0.05, verifying\nthe statistical significance in the enhancement of\nour method against prior best MRHP model, MCR\n(Liu et al., 2021b).\n4.5 Case Study\nIn Table 1, we introduce an example of one prod-\nuct item and two reviews extracted from Electron-\nics category of Amazon-MRHP dataset. Whereas\nMCR fails to predict relevant helpfulness scores,\nour model successfully produces sensible rankings\nfor both of them. We hypothesize that our Multi-\nmodal Interaction module learns more meaningful\nrepresentations and Adaptive Contrastive Learning\nframework acquires more logical hidden states of\nrelations among input elements. Thus, our model\nis able to generate more rational outcomes.\n4.6 Ablation Study\nIn this section, we proceed to study the impact of\n(1) Adaptive Contrastive Learning framework and\n(2) Cross-modal Interaction module.\nAdaptive Contrastive Learning It is worth not-\ning from Table 6 that plainly integrating con-\ntrastive learning brings less enhancement to the\nperformance, with the improvement of NDCG@3\ndropping 0.53 points in Lazada-MRHP dataset,\nNDCG@5 waning 0.84 points in Amazon-MRHP\ndataset. Furthermore, completely removing con-\ntrastive objective hurts performance, as NDCG@3\nscore decreasing 0.77 points in Lazada-MRHP,\nand MAP score declining 1.06 points in Amazon-\nMRHP. We hypothesize that the model loses the\nability to learn efficient representations for cross-\nmodal relations.\nCross-modal Interaction In this ablation, we elim-\ninate the cross-modal interaction module. As\nshown in Table 6, without the module, the improve-\nment is downgraded, for instance, N@3 drops 1.89\npoints in Lazada-MRHP dataset, MAP shrinks1.39\npoints in Amazon-MRHP dataset. It is hypothe-\nsized that without the module, the model is rigidly\ndependent upon the alignment nature among multi-\nmodal input elements, which brings about insensi-\nble modeling because in most cases, cross-modal\nelements are irrelevant to be bijectively mapped\ntogether.\nDatasetModel MAP N@3 N@5\nLazada\nOur Model 78.15 72.43 76.49\n- w/o Adaptive Weighting77.90 71.90 75.97\n- w/o Contrastive Objective77.69 71.66 75.85\n- w/o Cross-modal Module77.32 70.54 74.86\nAmazon\nOur Model 56.49 47.62 50.79\n- w/o Adaptive Weighting56.03 46.98 49.95\n- w/o Contrastive Objective55.43 46.30 49.02\n- w/o Cross-modal Module55.10 45.67 48.50\nTable 6: Ablation study in Electronics category of\nLazada-MRHP and Amazon-MRHP datasets.\n4.7 Impact of Contrastive Learning on\nCross-modal Relations\nDespite improved performances, it remains a\nquandary that whether the enhancement stems from\nmore meaningful representations of input samples,\nwhich we hypothesize as a significant benefit of\nour contrastive learning framework. For deeper\ninvestigation, we decide to statistically measure\n10091\nLabel Model Intra-modal Inter-modal Intra-review\nCS L2 CS L2 CS L2\n1 MCR 0.785±0.002 3.852±0.067 0.843±0.002 11.719±0.001 0.845±0.002 14.631±0.001\nOur Model0.875±0.002 6.545±0.007 0.957±0.002 13.934±0.027 0.953±0.002 15.160±0.036\n4 MCR 0.533±0.004 1.014±0.051 0.712±0.010 9.476 ±0.001 0.617±0.001 8.519 ±0.001\nOur Model0.433±0.001 0.981±0.005 0.564±0.001 4.179 ±0.017 0.538±0.001 3.827 ±0.020\nTable 7: Intra-modal, Inter-modal, and Intra-review distances in Home category of Lazada-MRHP dataset.\nLabel Model Intra-modal Inter-modal Intra-review\nCS L2 CS L2 CS L2\n1 MCR 0.785±0.006 8.532±0.292 0.686±0.001 9.696±0.300 0.880±0.002 9.620±0.217\nOur Model0.971±0.001 10.663±0.770 0.976±0.001 13.234±0.493 0.970±0.001 12.222±0.431\n4 MCR 0.697±0.009 3.045±0.139 0.624±0.001 3.179±0.830 0.781±0.001 5.098±0.636\nOur Model0.571 +- 0.001 1.572 +- 0.0370.488 +- 0.001 1.460 +- 0.0080.487 +- 0.001 3.555 +- 0.001\nTable 8: Intra-modal, Inter-modal, and Intra-review distances in Home category of Amazon-MRHP dataset.\ndistances among input samples using standard dis-\ntance functions. Table 7 and 8 reveal the results\nof our experiment. In particular, we estimate the\ncosine distance (CS) and L2 distance (L2) between\ntokens of (1) product text - review text and product\nimage - review image (intra-modal), (2) product\ntext - review image and product image - review text\n(inter-modal), and (3) product text - product im-\nage and review text - review image (intra-review),\nthen calculate the mean value of all samples. As it\ncan be seen, our frameworks are more efficient in\nattracting elements of helpful pairs and repelling\nthose of unhelpful pairs.\n5 Related Work\n5.1 Review Helpfulness Prediction\nPast works that pursue Review Helpfulness Predic-\ntion (RHP) dilemma follow text-only approaches.\nIn general, they extract salient information, for in-\nstance lexical (Krishnamoorthy, 2015), argument\n(Liu et al., 2017), and emotional features (Martin\nand Pu, 2014) from reviews. Subsequently, these\nfeatures are fed to a standard classifier such as\nRandom Forest (Louppe, 2014) in order to pro-\nduce the output score. Inspired by the meteoric\ndevelopment of computation resources, contempo-\nrary approaches seek to take advantage of deep\nlearning techniques to tackle the RHP problem.\nFor instance, Wang et al. (2017) propose multi-\nperspective matching between review and product\ninformation via applying attention mechanism. Fur-\nthermore, Chen et al. (2018); Dai et al. (2018) adapt\nCNN models to learn textual representations in var-\nious views.\nIn reality, review content are not only determined\nby texts but also other modalities. As a conse-\nquence, Fan et al. (2019) integrate metadata in-\nformation of the target product into the prediction\nmodel. Abavisani et al. (2020) filter out uninforma-\ntive signals before fusing various modalities. More-\nover, Liu et al. (2021b) perform coherent reasoning\nto ascertain the matching level between product\nand numerous review items.\n5.2 Contrastive Estimation\nDifferent from architectural techniques such as\nKnowledge Distillation (Hinton et al., 2015; Hahn\nand Choi, 2019; Nguyen and Luu, 2022) or Vari-\national AutoEncoder (Zhao et al., 2020; Nguyen\net al., 2021; Nguyen and Luu, 2021; Wang et al.,\n2019), Contrastive Learning has been introduced as\na representation-based but universal mechanism to\nenhance natural language processing performance.\nProposed by Chopra et al. (2005), Contrastive\nLearning has been widely adopted in myriad prob-\nlems of Natural Language Processing (NLP).\nAs an approach to polish text representations,\nGao et al. (2021); Zhang et al. (2021); Liu et al.\n(2021a); Nguyen and Luu (2021) employ con-\ntrastive loss to advance sentence embeddings and\ntopic representations. For downstream tasks, Cao\nand Wang (2021) propose negative sampling strate-\ngies to generate noisy output so that the model can\nlearn to distinguish correct summaries from incor-\nrect ones in Document Summarization. For Spoken\nQuestion Answering (SQA), You et al. (2021) intro-\nduce augmentation algorithms in their contrastive\nlearning stage so as to capture noisy-invariant rep-\n10092\nresentations of utterances. Additionally, Ke et al.\n(2021) inherit the formulation of the contrastive\nobjective to construct distillation loss which trans-\nfers knowledge of the previous task to the current\none. Their proposals are to improve tasks in the\nAspect Sentiment Classification domain. Unfortu-\nnately, despite the surge of interest in exercising\ncontrastive learning for NLP, research works to\nadapt the method to the MRHP task have been\nscant.\n6 Conclusion\nIn this paper, we propose methods to polish rep-\nresentation learning for the Multimodal Review\nHelpfulness Prediction task. In particular, we aim\nto advance cross-modal relation representations by\nlearning mutual information through contrastive\nlearning. In order to further enhance our frame-\nwork, we propose an adaptive weighting strategy\nto encourage flexibility in optimization. Moreover,\nwe integrate a cross-modal interaction module to\nloose the model’s reliance on unalignment nature\namong modalities, continuing to refine multimodal\nrepresentations. Our framework is able to outper-\nform prior baselines and achieve state-of-the-art\nresults on the MRHP problem.\n7 Limitations\nDespite the novelty and benefits of our method\nfor Multimodal Review Helpfulness Prediction\n(MRHP) problem, it does include some drawbacks.\nFirstly, even though empirical results demonstrate\nthat our approach not only works in English con-\ntexts, we have not conducted the verification in\nmultilingual circumstances, in which product or\nreview texts are written in different languages. If\na model is corroborated to work efficiaciously in\nsuch contexts, it is capable of providing myriad\nbenefits for practical implementation, for example,\ne-commerce applications can leverage such one\nsingle model for multiple cross-lingual scenarios.\nFurthermore, our work can also be extended to\nother domains. For instance, in movie assessment,\nwe need to determine whether the review suits the\nmaterial in the film, or visual scenes in the com-\nment are consistent with the textual content. These\nwould form our prospective future directions.\nSecondly, in the MRHP problem, there are sev-\neral relationships that contrastive learning could\nexploit to burnish the performance. In particular,\nperforming contrastive discrimination between two\nsets of reviews is able to furnish the model with\nuseful set-based representations, which consolidate\ngeneral knowledge for better helpfulness predic-\ntion. Similar insights are applicable for two sets\nof product information. At the moment, we leave\nsuch promising perspectives for future work.\n8 Acknowledgement\nThis work was supported by Alibaba Innovative\nResearch (AIR) programme with research grant\nAN-GC-2021-005.\nReferences\nMahdi Abavisani, Liwei Wu, Shengli Hu, Joel Tetreault,\nand Alejandro Jaimes. 2020. Multimodal categoriza-\ntion of crisis events in social media. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 14679–14689.\nAbdalraheem Alsmadi, Shadi AlZu’bi, Mahmoud Al-\nAyyoub, and Yaser Jararweh. 2020. Predicting\nhelpfulness of online reviews. arXiv preprint\narXiv:2008.10129.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\n2018. Bottom-up and top-down attention for image\ncaptioning and visual question answering. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6077–6086.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the associa-\ntion for computational linguistics, 5:135–146.\nShuyang Cao and Lu Wang. 2021. Cliff: Contrastive\nlearning for improving faithfulness and factuality in\nabstractive summarization. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6633–6649.\nCen Chen, Yinfei Yang, Jun Zhou, Xiaolong Li, and\nForrest Bao. 2018. Cross-domain review helpfulness\nprediction based on convolutional neural networks\nwith auxiliary domain discriminators. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 2\n(Short Papers), pages 602–607.\nSumit Chopra, Raia Hadsell, and Yann LeCun. 2005.\nLearning a similarity metric discriminatively, with\napplication to face verification. In 2005 IEEE Com-\nputer Society Conference on Computer Vision and\nPattern Recognition (CVPR’05) , volume 1, pages\n539–546. IEEE.\nZhuyun Dai, Chenyan Xiong, Jamie Callan, and\nZhiyuan Liu. 2018. Convolutional neural networks\n10093\nfor soft-matching n-grams in ad-hoc search. In Pro-\nceedings of the eleventh ACM international confer-\nence on web search and data mining, pages 126–134.\nMiao Fan, Chao Feng, Lin Guo, Mingming Sun, and\nPing Li. 2019. Product-aware helpfulness prediction\nof online reviews. In The World Wide Web Confer-\nence, pages 2715–2721.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6894–6910.\nSangchul Hahn and Heeyoul Choi. 2019. Self-\nknowledge distillation in natural language processing.\nIn Proceedings of the International Conference on\nRecent Advances in Natural Language Processing\n(RANLP 2019), pages 423–430.\nWei Han, Hui Chen, Zhen Hai, Soujanya Poria,\nand Lidong Bing. 2022. Sancl: Multimodal re-\nview helpfulness prediction with selective attention\nand natural contrastive learning. arXiv preprint\narXiv:2209.05040.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network (2015).\narXiv preprint arXiv:1503.02531, 2.\nKalervo Järvelin and Jaana Kekäläinen. 2017. Ir eval-\nuation methods for retrieving highly relevant doc-\numents. In ACM SIGIR Forum, volume 51, pages\n243–250. ACM New York, NY , USA.\nZixuan Ke, Bing Liu, Hu Xu, and Lei Shu. 2021. Clas-\nsic: Continual and contrastive learning of aspect sen-\ntiment classification tasks. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6871–6883.\nSoo-Min Kim, Patrick Pantel, Timothy Chklovski, and\nMarco Pennacchiotti. 2006. Automatically assessing\nreview helpfulness. In Proceedings of the 2006 Con-\nference on empirical methods in natural language\nprocessing, pages 423–430.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nSrikumar Krishnamoorthy. 2015. Linguistic features for\nreview helpfulness prediction. Expert Systems with\nApplications, 42(7):3751–3759.\nChe Liu, Rui Wang, Jinghua Liu, Jian Sun, Fei Huang,\nand Luo Si. 2021a. Dialoguecse: Dialogue-based\ncontrastive learning of sentence embeddings. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2396–\n2406.\nHaijing Liu, Yang Gao, Pin Lv, Mengxue Li, Shiqiang\nGeng, Minglan Li, and Hao Wang. 2017. Using\nargument-based features to predict and analyse re-\nview helpfulness. arXiv preprint arXiv:1707.07279.\nJunhao Liu, Zhen Hai, Min Yang, and Lidong Bing.\n2021b. Multi-perspective coherent reasoning for\nhelpfulness prediction of multimodal reviews. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 5927–\n5936.\nGilles Louppe. 2014. Understanding random\nforests: From theory to practice. arXiv preprint\narXiv:1407.7502.\nLionel Martin and Pearl Pu. 2014. Prediction of helpful\nreviews using emotions extraction. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 28.\nThong Nguyen and Anh Tuan Luu. 2021. Contrastive\nlearning for neural topic model. Advances in Neural\nInformation Processing Systems, 34:11974–11986.\nThong Nguyen, Anh Tuan Luu, Truc Lu, and Tho\nQuan. 2021. Enriching and controlling global se-\nmantics for text summarization. arXiv preprint\narXiv:2109.10616.\nThong Thanh Nguyen and Anh Tuan Luu. 2022. Im-\nproving neural cross-lingual abstractive summariza-\ntion via employing optimal transport distance for\nknowledge distillation. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 36,\npages 11103–11111.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. 2019. Multimodal transformer for\nunaligned multimodal language sequences. In Pro-\nceedings of the conference. Association for Computa-\ntional Linguistics. Meeting, volume 2019, page 6558.\nNIH Public Access.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYue Wang, Jing Li, Hou Pong Chan, Irwin King,\nMichael R Lyu, and Shuming Shi. 2019. Topic-aware\nneural keyphrase generation for social media lan-\nguage. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2516–2526.\nZhiguo Wang, Wael Hamza, and Radu Florian. 2017.\nBilateral multi-perspective matching for natural lan-\nguage sentences. In Proceedings of the 26th Inter-\nnational Joint Conference on Artificial Intelligence,\npages 4144–4150.\n10094\nNan Xu, Zhixiong Zeng, and Wenji Mao. 2020. Reason-\ning with multimodal sarcastic tweets via modeling\ncross-modality contrast and semantic association. In\nProceedings of the 58th annual meeting of the as-\nsociation for computational linguistics, pages 3777–\n3786.\nChenyu You, Nuo Chen, and Yuexian Zou. 2021. Self-\nsupervised contrastive cross-modality representation\nlearning for spoken question answering. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 28–39.\nDejiao Zhang, Shang-Wen Li, Wei Xiao, Henghui Zhu,\nRamesh Nallapati, Andrew O Arnold, and Bing Xi-\nang. 2021. Pairwise supervised contrastive learning\nof sentence representations. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5786–5798.\nHe Zhao, Dinh Phung, Viet Huynh, Trung Le, and Wray\nBuntine. 2020. Neural topic model via optimal trans-\nport. In International Conference on Learning Rep-\nresentations.\nMohammadreza Zolfaghari, Yi Zhu, Peter Gehler, and\nThomas Brox. 2021. Crossclr: Cross-modal con-\ntrastive learning for multi-modal video representa-\ntions. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1450–1459.\n10095\nA Hyperspherical Form of Adaptive Contrastive Loss\nWe have the initial formulation of the adaptive contrastive loss\nLAdaptiveCE = −\nB∑\ni=1\nϵp\ni ·sim(t1\ni,t2\ni) +\nB∑\nj=1,k=1,j̸=k\nϵn\nj,k ·sim(t1\nj,t2\nk) (36)\nWe first substitute ϵp\ni = [op −sim(t1\ni,t2\ni)]+ and ϵn\nj,k = [sim(t1\nj,t2\nk) −on]+ into the above equation,\nLAdaptiveCE =\nB∑\ni=1\nsim(t1\ni,t2\ni)2 −op ·sim(t1\ni,t2\ni) +\nB∑\nj=1,k=1,j̸=k\nsim(t1\ni,t2\ni)2 −on ·sim(t1\nj,t2\nk) (37)\n=\nB∑\ni=1\n(\nsim(t1\ni,t2\ni) −op\n2\n)2\n+\nB∑\nj=1,k=1,j̸=k\n(\nsim(t1\nj,t2\nk) −on\n2\n)2\n−C (38)\nwhere C =\n(op\n2\n)2\n+\n(on\n2\n)2\n. Now we obtain the spherical form of our contrastive loss.\n10096",
  "topic": "Helpfulness",
  "concepts": [
    {
      "name": "Helpfulness",
      "score": 0.7917650938034058
    },
    {
      "name": "Computer science",
      "score": 0.7656631469726562
    },
    {
      "name": "Modalities",
      "score": 0.6060597896575928
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5948866009712219
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.49747374653816223
    },
    {
      "name": "Machine learning",
      "score": 0.4828394055366516
    },
    {
      "name": "Weighting",
      "score": 0.42492741346359253
    },
    {
      "name": "Mutual information",
      "score": 0.4236522316932678
    },
    {
      "name": "Flexibility (engineering)",
      "score": 0.4135141670703888
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210142044",
      "name": "VinUniversity",
      "country": "VN"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    }
  ],
  "cited_by": 7
}