{
  "title": "It Depends: Dependency Parser Comparison Using A Web-based Evaluation Tool",
  "url": "https://openalex.org/W2252147815",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A5101829031",
      "name": "Jinho D. Choi",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A5038106045",
      "name": "Joel Tetreault",
      "affiliations": [
        "Yahoo (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5053693571",
      "name": "Amanda Stent",
      "affiliations": [
        "Yahoo (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2153274216",
    "https://openalex.org/W162171320",
    "https://openalex.org/W2569308312",
    "https://openalex.org/W2149383868",
    "https://openalex.org/W2140721185",
    "https://openalex.org/W2105103433",
    "https://openalex.org/W2105549195",
    "https://openalex.org/W8550301",
    "https://openalex.org/W1517853909",
    "https://openalex.org/W1491975949",
    "https://openalex.org/W2250861254",
    "https://openalex.org/W2131698806",
    "https://openalex.org/W1859633635",
    "https://openalex.org/W1934889228",
    "https://openalex.org/W1605386991",
    "https://openalex.org/W2143391265",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2027979924",
    "https://openalex.org/W2114211800",
    "https://openalex.org/W2115847145",
    "https://openalex.org/W2157889740",
    "https://openalex.org/W1500431650",
    "https://openalex.org/W2099521220",
    "https://openalex.org/W1970849810",
    "https://openalex.org/W2251837567",
    "https://openalex.org/W2070536585",
    "https://openalex.org/W2094061585",
    "https://openalex.org/W2083386188",
    "https://openalex.org/W2106310992",
    "https://openalex.org/W2128092251",
    "https://openalex.org/W2146000892",
    "https://openalex.org/W76754153",
    "https://openalex.org/W2142222368",
    "https://openalex.org/W2160073299",
    "https://openalex.org/W4241738801",
    "https://openalex.org/W2151651070",
    "https://openalex.org/W2103076621",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W168425931",
    "https://openalex.org/W2158899491"
  ],
  "abstract": "Jinho D. Choi, Joel Tetreault, Amanda Stent. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015.",
  "full_text": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural Language Processing, pages 387–396,\nBeijing, China, July 26-31, 2015.c⃝2015 Association for Computational Linguistics\nIt Depends: Dependency Parser Comparison\nUsing A Web-based Evaluation Tool\nJinho D. Choi\nEmory University\n400 Dowman Dr.\nAtlanta, GA 30322, USA\njchoi31@emory.edu\nJoel Tetreault\nYahoo Labs\n229 West 43rd St.\nNew York, NY 10036, USA\ntetreaul@yahoo-inc.com\nAmanda Stent\nYahoo Labs\n229 West 43rd St.\nNew York, NY 10036, USA\nstent@yahoo-inc.com\nAbstract\nThe last few years have seen a surge in the\nnumber of accurate, fast, publicly avail-\nable dependency parsers. At the same\ntime, the use of dependency parsing in\nNLP applications has increased. It can be\ndifﬁcult for a non-expert to select a good\n“off-the-shelf” parser. We present a com-\nparative analysis of ten leading statistical\ndependency parsers on a multi-genre cor-\npus of English. For our analysis, we de-\nveloped a new web-based tool that gives\na convenient way of comparing depen-\ndency parser outputs. Our analysis will\nhelp practitioners choose a parser to op-\ntimize their desired speed/accuracy trade-\noff, and our tool will help practitioners ex-\namine and compare parser output.\n1 Introduction\nDependency parsing is a valuable form of syn-\ntactic processing for NLP applications due to its\ntransparent lexicalized representation and robust-\nness with respect to ﬂexible word order languages.\nThanks to over a decade of research on statisti-\ncal dependency parsing, many dependency parsers\nare now publicly available. In this paper, we re-\nport on a comparative analysis of leading statis-\ntical dependency parsers using a multi-genre cor-\npus. Our purpose is not to introduce a new pars-\ning algorithm but to assess the performance of ex-\nisting systems across different genres of language\nuse and to provide tools and recommendations\nthat practitioners can use to choose a dependency\nparser. The contributions of this work include:\n• A comparison of the accuracy and speed of\nten state-of-the-art dependency parsers, cov-\nering a range of approaches, on a large multi-\ngenre corpus of English.\n• A new web-based tool, D EPEND ABLE , for\nside-by-side comparison and visualization of\nthe output from multiple dependency parsers.\n• A detailed error analysis for these parsers\nusing DEPEND ABLE , with recommendations\nfor parser choice for different factors.\n• The release of the set of dependencies used\nin our experiments, the test outputs from all\nparsers, and the parser-speciﬁc models.\n2 Related Work\nThere have been several shared tasks on de-\npendency parsing conducted by CoNLL (Buch-\nholz and Marsi, 2006; Nivre and others, 2007;\nSurdeanu and others, 2008; Haji ˇc and others,\n2009), SANCL (Petrov and McDonald, 2012),\nSPMRL (Seddah and others, 2013), and Se-\nmEval (Oepen and others, 2014). These shared\ntasks have led to the public release of numerous\nstatistical parsers. The primary metrics reported\nin these shared tasks are: labeled attachment score\n(LAS) – the percentage of predicted dependencies\nwhere the arc and the label are assigned correctly;\nunlabeled attachment score (UAS) – where the arc\nis assigned correctly; label accuracy score ( LS) –\nwhere the label is assigned correctly; and exact\nmatch (EM) – the percentage of sentences whose\npredicted trees are entirely correct.\nAlthough shared tasks have been tremendously\nuseful for advancing the state of the art in depen-\ndency parsing, most English evaluation has em-\nployed a single-genre corpus, the WSJ portion of\nthe Penn Treebank (Marcus et al., 1993), so it\nis not immediately clear how these results gen-\n387\nBC BN MZ NW PT TC WB ALL\nTraining 171,120 206,057 163,627 876,399 296,437 85,466 284,975 2,084,081\nDevelopment 29,962 25,274 15,422 147,958 25,206 11,467 36,351 291,640\nTest 35,952 26,424 17,875 60,757 25,883 10,976 38,490 216,357\nTraining 10,826 10,349 6,672 34,492 21,419 8,969 12,452 105,179\nDevelopment 2,117 1,295 642 5,896 1,780 1,634 1,797 15,161\nTest 2,211 1,357 780 2,327 1,869 1,366 1,787 11,697\nTable 1: Distribution of data used for our experiments. The ﬁrst three/last three rows show the number of\ntokens/trees in each genre. BC: broadcasting conversation, BN: broadcasting news, MZ: news magazine,\nNW: newswire, PT: pivot text, TC: telephone conversation, WB: web text, ALL : all genres combined.\neralize.1 Furthermore, a detailed comparative er-\nror analysis is typically lacking. The most de-\ntailed comparison of dependency parsers to date\nwas performed by McDonald and Nivre (2007;\n2011); they analyzed accuracy as a function of\nsentence length, dependency distance, valency,\nnon-projectivity, part-of-speech tags and depen-\ndency labels.2 Since then, additional analyses of\ndependency parsers have been performed, but ei-\nther with respect to speciﬁc linguistic phenom-\nena (e.g. (Nivre et al., 2010; Bender et al., 2011))\nor to downstream tasks (e.g. (Miwa and others,\n2010; Petrov et al., 2010; Yuret et al., 2013)).\n3 Data\n3.1 OntoNotes 5\nWe used the English portion of the OntoNotes 5\ncorpus, a large multi-lingual, multi-genre cor-\npus annotated with syntactic structure, predicate-\nargument structure, word senses, named entities,\nand coreference (Weischedel and others, 2011;\nPradhan and others, 2013). We chose this corpus\nrather than the Penn Treebank used in most pre-\nvious work because it is larger (2.9M vs. 1M to-\nkens) and more diverse (7 vs. 1 genres). We used\nthe standard data split used in CoNLL’123, but re-\nmoved sentences containing only one token so as\nnot to artiﬁcially inﬂate accuracy.\nTable 1 shows the distribution across genres\nof training, development, and test data. For the\nmost strict and realistic comparison, we trained all\nten parsers using automatically assigned POS tags\nfrom the tagger in ClearNLP (Choi and Palmer,\n2012a), which achieved accuracies of 97.34 and\n97.52 on the development and test data, respec-\ntively. We also excluded any “morphological” fea-\n1The SANCL shared task used OntoNotes and the Web\nTreebanks instead for better generalization.\n2A detailed error analysis of constituency parsing was per-\nformed by (Kummerfeld and others, 2012).\n3conll.cemantix.org/2012/download/ids/\nture from the input, as these are often not available\nin non-annotated data.\n3.2 Dependency Conversion\nOntoNotes provides annotation of constituency\ntrees only. Several programs are available for con-\nverting constituency trees into dependency trees.\nTable 2 shows a comparison between three of\nthe most widely used: the LTH (Johansson and\nNugues, 2007),4, Stanford (de Marneffe and Man-\nning, 2008), 5 and ClearNLP (Choi and Palmer,\n2012b)6 dependency converters. Compared to the\nStanford converter, the ClearNLP converter pro-\nduces a similar set of dependency labels but gen-\nerates fewer unclassiﬁed dependencies (0.23% vs.\n3.62%), which makes the training data less noisy.\nBoth the LTH and ClearNLP converters pro-\nduce long-distance dependencies and use function\ntags for the generation of dependency relations,\nwhich allows one to generate rich dependency\nstructures including non-projective dependencies.\nHowever, only the ClearNLP converter adapted\nthe new Treebank guidelines used in OntoNotes.\nIt can also produce secondary dependencies (e.g.\nright-node raising, referent), which can be used for\nfurther analysis. We used the ClearNLP converter\nto produce dependencies for our experiments.\nLTH Stanford ClearNLP\nLong-distance ✓ ✓\nSecondary 1 2 4\nFunction tags ✓ ✓\nNew TB format ✓\nTable 2: Dependency converters. The “secondary”\nrow shows how many types of secondary depen-\ndencies that can be produced by each converter.\n4http://nlp.cs.lth.se/software\n5http://nlp.stanford.edu/software\n6http://www.clearnlp.com\n388\nParser Approach Language License\nClearNLP v2,37 Transition-based, selectional branching (Choi and McCallum, 2013) Java Apache\nGN138 Easy-ﬁrst, dynamic oracle (Goldberg and Nivre, 2013) Python GPL v2\nLTDP v2.0.39 Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Python n/a\nMate v3.6.110 Maximum spanning tree, 3rd-order features (Bohnet, 2010) Java GPL v2\nRBG11 Tensor decomposition, randomized hill-climb (Lei et al., 2014) Java MIT\nRedshift12 Transition-based, non-monotonic (Honnibal et al., 2013) Cython FOSS\nspaCy13 Transition-based, greedy, dynamic oracle, Brown clusters Cython Dual\nSNN14 Transition-based, word embeddings (Chen and Manning, 2014) Java GPL v2\nTurbo v2.215 Dual decomposition, 3rd-order features (Martins et al., 2013) C++ GPL v2\nYara16 Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Java Apache\nTable 3: Dependency parsers used in our experiments.\n4 Parsers\nWe compared ten state of the art parsers repre-\nsenting a wide range of contemporary approaches\nto statistical dependency parsing (Table 3). We\ntrained each parser using the training data from\nOntoNotes. For all parsers we trained using the\nautomatic POS tags generated during data prepro-\ncessing, as described above.\nTraining settings For most parsers, we used the\ndefault settings for training. For the SNN parser,\nfollowing the recommendation of the developers,\nwe used the word embeddings from (Collobert and\nothers, 2011).\nDevelopment data ClearNLP, LTDP, SNN and\nYara make use of the development data (for pa-\nrameter tuning). Mate and Turbo self-tune param-\neter settings using the training data. The others\nwere trained using their default/“standard” param-\neter settings.\nBeam search ClearNLP, LTDP, Redshift and\nYara have the option of different beam settings.\nThe higher the beam size, the more accurate the\nparser usually becomes, but typically at the ex-\npense of speed. For LTDP and Redshift, we ex-\nperimented with beams of 1, 8, 16 and 64 and\nfound that the highest accuracy was achieved at\nbeam 8.17 For ClearNLP and Yara, a beam size of\n7www.clearnlp.com\n8cs.bgu.ac.il/˜yoavg/software/sdparser\n9acl.cs.qc.edu/˜lhuang\n10code.google.com/p/mate-tools\n11github.com/taolei87/RBGParser\n12github.com/syllog1sm/Redshift\n13honnibal.github.io/spaCy\n14nlp.stanford.edu/software/nndep.shtml\n15www.ark.cs.cmu.edu/TurboParser\n16https://github.com/yahoo/YaraParser\n17Due to memory limitations we were unable to train Red-\nshift on a beam size greater than 8.\n64 produced the best accuracy, while a beam size\nof 1 for LTDT, ClearNLP, and Yara produced the\nbest speed performance. Given this trend, we also\ninclude how those three parsers perform at beam 1\nin our analyses.\nFeature Sets RBG, Turbo and Yara have the op-\ntions of different feature sets. A more complex or\nlarger feature set has the advantage of accuracy,\nbut often at the expense of speed. For RBG and\nTurbo, we use the ”Standard” setting and for Yara,\nwe use the default (”not basic”) feature setting.\nOutput All the parsers other than LTDP output\nlabeled dependencies. The ClearNLP, Mate, RBG,\nand Turbo parsers can generate non-projective de-\npendencies.\n5 D EPEND ABLE : Web-based Evaluation\nand Visualization Tool\nThere are several very useful tools for evaluating\nthe output of dependency parsers, including the\nvenerable eval.pl18 script used in the CoNLL\nshared tasks, and newer Java-based tools that sup-\nport visualization of and search over parse trees\nsuch as TedEval (Tsarfaty et al., 2011), 19 Mal-\ntEval (Nilsson and Nivre, 2008) 20 and “What’s\nwrong with my NLP?”. 21 Recently, there is mo-\nmentum towards web-based tools for annotation\nand visualization of NLP pipelines (Stenetorp and\nothers, 2012). For this work, we used a new web-\nbased tool, D EPEND ABLE , developed by the ﬁrst\nauthor of this paper. It requires no installation and\nso provides a convenient way to evaluate and com-\npare dependency parsers. The following are key\nfeatures of DEPEND ABLE :\n18ilk.uvt.nl/conll/software.html\n19www.tsarfaty.com/unipar/\n20www.maltparser.org/malteval.html\n21whatswrong.googlecode.com\n389\nFigure 1: Screenshot of our evaluation tool.\n• It reads any type of Tab Separated Value\n(TSV) format, including the CoNLL formats.\n• It computes LAS, UAS and LS for parse out-\nputs from multiple parsers against gold (man-\nual) parses.\n• It computes exact match scores for multiple\nparsers, and “oracle ensemble” output, the\nupper bound performance obtainable by com-\nbining all parser outputs.\n• It allows the user to exclude symbol tokens,\nprojective trees, or non-projective trees.\n• It produces detailed analyses byPOS tags, de-\npendency labels, sentence lengths, and de-\npendency distances.\n• It reports statistical signiﬁcance values for all\nparse outputs (using McNemar’s test).\nDEPEND ABLE can be also used for visualizing\nand comparing multiple dependency trees together\n(Figure 2). A key feature is that the user may\nselect parse trees by specifying a range of accu-\nracy scores; this enabled us to perform the er-\nror analyses in Section 6.5. D EPEND ABLE al-\nlows one to ﬁlter trees by sentence length and\nhighlights arc and label errors. The evalua-\ntion and comparison tools are publicly avail-\nable at http://nlp.mathcs.emory.edu/\nclearnlp/dependable.\nFigure 2: Screenshot of our visualization tool.\n6 Results and Error Analysis\nIn this section, we report overall parser accu-\nracy and speed. We analyze parser accuracy\nby sentence length, dependency distance, non-\nprojectivity, POS tags and dependency labels, and\ngenre. We report detailed manual error analy-\nses focusing on sentences that multiple parsers\nparsed incorrectly.22 All analyses, other than pars-\ning speed, were conducted using the D EPEND -\nABLE tool.23 The full set of outputs from all\nparsers, as well as the trained models for each\nparser, available at http://amandastent.\ncom/dependable/.\nWe also include the greedy parsing results of\nClearNLP, LTDP, and Yara in two of our anal-\nyses to better illustrate the differences between\nthe greedy and non-greedy settings. The greedy\nparsing results are denoted by the subscript ‘ g’.\nThese two analyses are the overall accuracy re-\nsults, presented in Section 6.1 (Table 4), and the\noverall speed results, presented in Section 6.2 (\n(Table 5 and Figure ). All other analyses exclude\nthe ClearNLPg, LTDPg and Yarag.\n22For one sentence in the NW data, the LTDP parser failed\nto produce a complete parse containing all tokens, so we\nremoved this sentence for all parsers, leaving 11,696 trees\n(216,313 tokens) in the test data.\n23We compared the results produced by D EPEND ABLE\nwith those produced by eval07.pl, and veriﬁed that LAS,\nUAS, LA, and EM were the same when punctuation was\nincluded. Our tool uses a slightly different symbol set than\neval07.pl: !\"#$%&’()*+,-./:;<=>?@[\\]ˆ ‘{|}˜\n390\nWith Punctuation Without Punctuation\nOverall Exact Match Overall Exact Match\nLAS UAS LS LAS UAS LS LAS UAS LS LAS UAS LS\nClearNLPg 89.19 90.63 94.94 47.65 53.00 61.17 90.09 91.72 94.29 49.12 55.01 61.31\nGN13 87.59 89.17 93.99 43.78 48.89 56.71 88.75 90.54 93.32 45.44 51.20 56.88\nLTDPg n/a 85.75 n/a n/a 46.38 n/a n/a 87.16 n/a n/a 48.01 n/a\nSNN 86.42 88.15 93.54 42.98 48.53 55.87 87.63 89.59 92.70 43.96 49.83 55.91\nspaCy 87.92 89.61 94.08 43.36 48.79 55.67 88.95 90.86 93.32 44.97 51.28 55.70\nYarag 85.93 87.64 92.99 42.94 47.77 54.79 87.39 89.32 92.24 44.25 49.44 54.96\nClearNLP 89.87 91.30 95.28 49.38 55.18 63.18 90.64 92.26 94.67 50.61 56.88 63.24\nLTDP n/a 88.18 n/a n/a 51.62 n/a n/a 89.17 n/a n/a 53.54 n/a\nMate 90.03 91.62 95.29 49.66 56.44 62.71 90.70 92.50 94.67 50.83 58.36 62.72\nRBG 89.57 91.45 94.71 46.49 55.49 58.45 90.23 92.35 94.01 47.64 56.54 58.07\nRedshift 89.48 91.01 95.04 49.71 55.82 62.70 90.27 92.00 94.42 50.88 57.28 62.78\nTurbo 89.81 91.50 95.00 48.08 55.33 60.49 90.49 92.40 94.34 49.29 57.09 60.52\nYara 89.80 91.36 95.19 50.07 56.18 63.36 90.47 92.24 94.57 51.02 57.53 63.42\nTable 4: Overall parsing accuracy. The top 6 rows and the bottom 7 rows show accuracies for greedy and\nnon-greedy parsers, respectively.\n6.1 Overall Accuracy\nIn Table 4, we report overall accuracy for each\nparser. For clarity, we report results separately\nfor greedy and non-greedy versions of the parsers.\nOver all the different metrics, MATE is a clear\nwinner, though ClearNLP, RBG, Redshift, Turbo\nand Yara are very close in performance. Look-\ning at only the greedy parsers, ClearNLPg shows a\nsigniﬁcant advantage over the others.\nWe conducted a statistical signiﬁcance test for\nthe the parsers (greedy versions excluded). All\nLAS differences are statistically signiﬁcant at p <\n.01 (using McNemar’s test), except for: RBG vs.\nRedshift, Turbo vs. Yara, Turbo vs. ClearNLP and\nYara vs. ClearNLP. All UAS differences are sta-\ntistically signiﬁcant at p < .01 (using McNemar’s\ntest), except for: SNN vs. LTDP, Turbo vs. Red-\nshift, Yara vs. RBG and ClearNLP vs. Yara.\n6.2 Overall Speed\nWe ran timing experiments on a 64 core machine\nwith 16 Intel Xeon E5620 2.40 GHz processors\nand 24G RAM, and used the unix time com-\nmand to time each run. Some parsers are multi-\nthreaded; for these, we ran in single-thread mode\n(since any parser can be externally parallelized).\nMost parsers do not report model load time, so we\nﬁrst ran each parser ﬁve times with a test set of\n10 sentences, and then averaged the middle three\ntimes to get the average model load time. 24 Next,\nwe ran each parser ﬁve times with the entire test\nset and derived the overall parse time by averag-\ning the middle three parse times. We then sub-\ntracted the average model time from the average\n24Recall we exclude single-token sentences from our tests.\nparse time and averaged over the number of sen-\ntences and tokens.\nSent/Sec Tokens/Sec Language\nClearNLPg 555 10,271 Java\nGN13 95 1,757 Python\nLTDPg 232 4,287 Python\nSNN 465 8,602 Java\nspaCy 755 13,963 Cython\nYarag 532 9,838 Java\nClearNLP 72 1,324 Java\nLTDP 26 488 Python\nMate 30 550 Java\nRBG 57 1,056 Java\nRedshift 188 3,470 Cython\nTurbo 19 349 C++\nYara 18 340 Java\nTable 5: Overall parsing speed.\nFigure 3: Number of sentences parsed per second\nby each parser with respect to sentence length.\nTable 5 shows overall parsing speed for each\nparser. spaCy is the fastest greedy parser and Red-\nshift is the fastest non-greedy parser. Figure 3\n391\nshows an analysis of parsing speed by sentence\nlength in bins of length 10. As expected, as sen-\ntence length increases, parsing speed decreases re-\nmarkably.\n6.3 Detailed Accuracy Analyses\nFor the following more detailed analyses, we used\nall tokens (including punctuation). As mentioned\nearlier, we exclude ClearNLPg, LTDPg and Yarag\nfrom these analyses and instead use their respec-\ntive non-greedy modes yielding higher accuracy.\nSentence Length We analyzed parser accuracy\nby sentence length in bins of length 10 (Figure 4).\nAs expected, all parsers perform better on shorter\nsentences. For sentences under length 10, UAS\nranges from 93.49 to 95.5; however, UAS de-\nclines to a range of 81.66 and 86.61 for sen-\ntence lengths greater than 50. The most accurate\nparsers (ClearNLP, Mate, RBG, Redshift, Turbo,\nand Yara) separate from the remaining when sen-\ntence length is more than 20 tokens.\nFigure 4: UAS by sentence length.\nDependency Distance We analyzed parser ac-\ncuracy by dependency distance (depth from each\ndependent to its head; Figure 5). Accuracy falls\noff more slowly as dependency distance increases\nfor the top 6 parsers vs. the rest.\nProjectivity Some of our parsers only produce\nprojective parses. Table 6 shows parsing accuracy\nfor trees containing only projective arcs (11,231\ntrees, 202,521 tokens) and for trees containing\nnon-projective arcs (465 trees, 13,792 tokens). As\nbefore, all differences are statistically signiﬁcant\nat p < .01 except for: Redshift vs. RBG for over-\nall LAS; LTDP vs. SNN for overall UAS; and\nTurbo vs. SpaCy for overall UAS. For strictly pro-\njective trees, the LTDP parser is 5th from the top in\nUAS. Apart from this, the grouping between “very\ngood” and “good” parsers does not change.\nFigure 5: UAS by dependency distance.\nProjective only Non-proj. only\nLAS UAS LAS UAS\nClearNLP 90.20 91.62 85.10 86.72\nGN13 88.00 89.57 81.56 83.37\nLTDP n/a 90.24 n/a 57.83\nMate 90.34 91.91 85.51 87.40\nRBG 89.86 91.72 84.83 86.94\nRedshift 89.90 91.41 83.30 85.12\nSNN 86.83 88.55 80.37 82.32\nspaCy 88.31 89.99 82.15 84.08\nTurbo 88.36 89.90 83.50 85.30\nYara 90.20 91.74 83.92 85.74\nTable 6: Accuracy for proj. and non-proj. trees.\nDependency Relations We were interested in\nwhich dependency relations were computed with\nhigh/low overall accuracy, and for which accuracy\nvaried between parsers. The dependency relations\nwith the highest average LAS scores ( > 97%)\nwere possessive, hyph, expl, hmod, aux,\ndet and poss. These relations have strong lexi-\ncal clues (e.g. possessive) or occur very often\n(e.g. det). Those with the lowest LAS scores\n(< 50%) were csubjpass, meta, dep, nmod\nand parataxis. These either occur rarely or are\nvery general (dep).\nThe most “confusing” dependency relations\n(those with the biggest range of accuracies across\nparsers) were csubj, preconj, csubjpass,\nparataxis, meta and oprd (all with a spread\nof > 20%). The Mate and Yara parsers each had\nthe highest accuracy for 3 out of the top 10 “con-\nfusing” dependency relations. The RBG parser\n392\nhad the highest accuracy for 4 out of the top 10\n“most accurate” dependency relations. SNN had\nthe lowest accuracy for 5 out of the top 10 “least\naccurate” dependency relations, while the RBG\nhad the lowest accuracy for another 4.\nPOS Tags We also examined error types by part\nof speech tag of the dependent. The POS tags with\nthe highest average LAS scores ( > 97%) were\nthe highly unambiguous tags POS, WP$, MD, TO,\nHYPH, EX, PRP and PRP$. With the exception of\nWP$, these tags occur frequently. Those with the\nlowest average LAS scores (< 75%) were punctu-\nation markers ((, ) and :, and the rare tags AFX,\nFW, NFP and LS.\nGenres Table 7 shows parsing accuracy for each\nparser for each of the seven genres comprising\nthe English portion of OntoNotes 5. Mate and\nClearNLP are responsible for the highest accuracy\nfor some genres, although accuracy differences\namong the top four parsers are generally small.\nAccuracy is highest for PT (pivot text, the Bible)\nand lowest for TC (telephone conversation) and\nWB (web data). The web data is itself multi-genre\nand includes translations from Arabic and Chi-\nnese, while telephone conversation data includes\ndisﬂuencies and informal language.\n6.4 Oracle Ensemble Performance\nOne popular method for achieving higher accuracy\non a classiﬁcation task is to use system combina-\ntion (Bj ¨orkelund and others, 2014; Le Roux and\nothers, 2012; Le Roux et al., 2013; Sagae and\nLavie, 2006; Sagae and Tsujii, 2010; Haffari et\nal., 2011). D EPEND ABLE reports ensemble upper\nbound performance assuming that the besttree can\nbe identiﬁed by an oracle (macro), or that the best\narc can be identiﬁed by an oracle ( micro). Ta-\nble 8 provides an upper bound on ensemble per-\nformance for future work.\nLAS UAS LS\nMacro 94.66 96.00 97.82\nMicro 96.52 97.61 98.40\nTable 8: Oracle ensemble performance.\nThe highest match was achieved between the RBG\nand Mate parser (62.22 UAS). ClearNLP, GN13\nand LTDP all matched with Redshift the best, and\nRBG, Redshift and Turbo matched with Mate the\nbest. SNN, spaCy and Turbo did not match well\nwith other parsers; their respective ”best match”\nscore was never higher than 55.\n6.5 Error Analysis\nFrom the test data, we pulled out parses where\nonly one parser achieved very high accuracy, and\nparses where only one parser had low accuracy\n(Table 9). As with the detailed performance anal-\nyses, we used the most accurate version of each\nparser for this analysis. Mate has the highest num-\nber of “generally good” parses, while the SNN\nparser has the highest number of “uniquely bad”\nparses. The SNN parser tended to choose the\nwrong root, but this did not appear to be tied to the\nnumber of verbs in the sentence - rather, the SNN\nparser just makes the earliest “reasonable” choice\nof root.\nParser UAS ≥90 = 100 < 90 < 90\nAll others UAS < 90 < 90 ≥90 = 100\nClearNLP 42 11 45 15\nLTDP 29 12 182 36\nGN13 26 8 148 65\nMate 75 19 44 10\nRBG 49 21 49 15\nRedshift 38 17 28 8\nSNN 70 23 417 142\nspaCy 48 17 218 73\nTurbo 54 15 28 14\nYara 33 15 27 7\nTable 9: Differential parsing accuracies.\nTo further analyze these results, we ﬁrst looked at\nthe parse trees for “errorful” sentences where the\nparsers agreed. From the test data, we extracted\nparses for sentences where at least two parsers got\nUAS of < 50%. This gave us 253 sentences. The\ndistribution of these errors across genres varied:\nPT - 2.8%, MZ - 3.5%, BN - 9.8%, NW - 10.3%,\nWB - 17.4%, BC - 25.3%, TC - 30.8%.\nBy manual comparison using the D EPEND -\nABLE tool, we identiﬁed frequently occurring po-\ntential sources of error. We then manually anno-\ntated all sentences for these error types. Figure 6\nshows the number of “errorful” sentences of each\ntype. Punctuation attachment “errors” are preva-\nlent. For genres with “noisy” text (e.g. broadcast\nconversation, telephone conversation) a signiﬁcant\nproportion of errors come from fragmented sen-\ntences or those containing backchannels or disﬂu-\nencies. There are also a number of sentences with\nwhat appeared to be manual dependency labeling\nerrors in the gold annotation.\n393\nBC BN MZ NW PT TC WB\nLAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS\nClearNLP 88.95 90.36 89.59 91.01 89.56 91.24 89.79 91.08 95.88 96.68 87.17 88.93 87.93 89.83\nGN13 86.75 88.40 87.38 88.87 87.31 89.10 87.36 88.84 94.06 95.00 85.68 87.60 85.20 87.19\nLTDP n/a 86.81 n/a 87.43 n/a 88.87 n/a 88.40 n/a 93.52 n/a 85.85 n/a 86.37\nMate 89.03 90.73 89.30 90.82 90.09 91.92 90.28 91.68 95.71 96.64 87.86 89.87 87.86 89.89\nRBG 88.64 90.58 88.99 90.86 89.28 91.45 89.85 91.47 95.27 96.41 87.36 89.65 87.12 89.61\nRedshift 88.60 90.19 88.96 90.46 89.11 90.90 89.63 90.99 95.36 96.22 87.14 88.99 87.27 89.31\nSNN 85.35 87.08 86.13 87.78 86.00 87.92 86.17 87.74 93.47 94.64 83.50 85.74 84.29 86.50\nspaCy 87.27 89.05 87.70 89.31 87.37 89.29 88.00 89.52 94.28 95.27 85.67 87.65 85.16 87.40\nTurbo 87.05 88.70 87.58 89.04 88.34 90.02 87.95 89.33 94.39 95.36 85.91 87.93 85.66 87.70\nYara 88.90 90.53 89.40 90.89 89.72 91.42 90.00 91.41 95.41 96.32 87.35 89.19 87.55 89.61\nTotal 2211 1357 780 2326 1869 1366 1787\nTable 7: Parsing accuracy by genre.\nFigure 6: Common error types in erroneous trees.\n6.6 Recommendations\nEach of the transition-based parsers that was in-\ncluded in this evaluation can use varying beam\nwidths to trade off speed vs. accuracy, and each\nparser has numerous other parameters that can be\ntuned. Notwithstanding all these variables, we\ncan make some recommendations. Figure 7 illus-\ntrates the speed vs. accuracy tradeoff across the\nparsers. For highest accuracy (e.g. in dialog sys-\ntems), Mate, RBG, Turbo, ClearNLP and Yara are\ngood choices. For highest speed (e.g. in web-scale\nNLP), spaCy and ClearNLP g are good choices;\nSNN and Yara g are also good choices when ac-\ncuracy is relatively not as important.\n7 Conclusions and Future Work\nIn this paper we have: (a) provided a detailed com-\nparative analysis of several state-of-the-art statis-\ntical dependency parsers, focusing on accuracy\nFigure 7: Speed with respect to accuracy.\nand speed; and (b) presented D EPEND ABLE , a\nnew web-based evaluation and visualization tool\nfor analyzing dependency parsers. D EPEND ABLE\nsupports a wide range of useful functionalities.\nIn the future, we plan to add regular expression\nsearch over parses, and sorting within results ta-\nbles. Our hope is that the results from the eval-\nuation as well as the tool will give non-experts\nin parsing better insight into which parsing tool\nworks well under differing conditions. We also\nhope that the tool can be used to facilitate evalua-\ntion and be used as a teaching aid in NLP courses.\nSupplements to this paper include the tool,\nthe parse outputs, the statistical models for each\nparser, and the new set of dependency trees for\nOntoNotes 5 created using the ClearNLP depen-\ndency converter. We do recommend examining\none’s data and task before choosing and/or train-\ning a parser. Are non-projective parses likely or\ndesirable? Does the data contain disﬂuencies, sen-\ntence fragments, and other “noisy text” phenom-\nena? What is the average and standard deviation\nfor sentence length and dependency length? The\nanalyses in this paper can be used to select a parser\nif one has the answers to these questions.\n394\nIn this work we did not implement an ensemble\nof parsers, partly because an ensemble necessarily\nentails complexity and/or speed delays that render\nit unusable by all but experts. However, our anal-\nyses indicate that it may be possible to achieve\nsmall but signiﬁcant increases in accuracy of de-\npendency parsing through ensemble methods. A\ngood place to start would be with ClearNLP, Mate,\nor Redshift in combination with LTDP and Turbo,\nSNN or spaCy. In addition, it may be possible to\nachieve good performance in particular genres by\ndoing “mini-ensembles” trained on general pur-\npose data (e.g. WB) and genre-speciﬁc data. We\nleave this for future work. We also leave for fu-\nture work the comparison of these parsers across\nlanguages.\nIt remains to be seen what downstream impact\ndifferences in parsing accuracy of 2-5% have on\nthe goal task. If the impact is small, then speed\nand ease of use are the criteria to optimize, and\nhere spaCy, ClearNLPg, Yarag and SNN are good\nchoices.\nAcknowledgments\nWe would like to thank the researchers who\nhave made available data (especially OntoNotes),\nparsers (especially those compared in this work),\nand evaluation and visualization tools. Special\nthanks go to Boris Abramzon, Matthew Honnibal,\nTao Lei, Danqi Li and Mohammad Sadegh Rasooli\nfor assistance in installation, trouble-shooting and\ngeneral discussion. Additional thanks goes to the\nkind folks from the SANCL-SPMRL community\nfor an informative discussion of evaluation and vi-\nsualization tools. Finally, we would like to thank\nthe three reviewers, as well as Martin Chodorow,\nDean Foster, Joseph Le Roux and Robert Stine, for\nfeedback on this paper.\nReferences\nEmily M. Bender, Dan Flickinger, Stephan Oepen, and\nYi Zhang. 2011. Parser evaluation over local and\nnon-local deep dependencies in a large corpus. In\nProceedings of EMNLP.\nAnders Bj ¨orkelund et al. 2014. Introducing the IMS-\nWrocław-Szeged-CIS entry at the SPMRL 2014\nshared task: Reranking and morpho-syntax meet\nunlabeled data. In Proceedings of the First Joint\nWorkshop on Statistical Parsing of Morphologically\nRich Languages and Syntactic Analysis of Non-\nCanonical Languages.\nBernd Bohnet. 2010. Very high accuracy and fast de-\npendency parsing is not a contradiction. In Proceed-\nings of COLING.\nSabine Buchholz and Erwin Marsi. 2006. CoNLL-X\nshared task on multilingual dependency parsing. In\nProceedings of CoNLL.\nDanqi Chen and Christopher Manning. 2014. A fast\nand accurate dependency parser using neural net-\nworks. In Proceedings of EMNLP.\nJinho D. Choi and Andrew McCallum. 2013.\nTransition-based Dependency Parsing with Selec-\ntional Branching. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL’13, pages 1052–1062.\nJinho D. Choi and Martha Palmer. 2012a. Fast and Ro-\nbust Part-of-Speech Tagging Using Dynamic Model\nSelection. In Proceedings of the 50th Annual Meet-\ning of the Association for Computational Linguis-\ntics, ACL’12, pages 363–367.\nJinho D. Choi and Martha Palmer. 2012b. Guidelines\nfor the Clear Style Constituent to Dependency Con-\nversion. Technical Report 01-12, Institute of Cogni-\ntive Science, University of Colorado Boulder, Boul-\nder, CO, USA.\nRonan Collobert et al. 2011. Natural language pro-\ncessing (almost) from scratch. Journal of Machine\nLearning Research, 12:2493–2537.\nMarie-Catherine de Marneffe and Christopher D. Man-\nning. 2008. The Stanford typed dependencies\nrepresentation. In Proceedings of the COLING\nworkshop on Cross-Framework and Cross-Domain\nParser Evaluation.\nYoav Goldberg and Joakim Nivre. 2013. Training de-\nterministic parser with non-deterministic oracles. In\nProceedings of TACL.\nGholamreza Haffari, Marzieh Razavi, and Anoop\nSarkar. 2011. An ensemble model that combines\nsyntactic and semantic clustering for discriminative\ndependency parsing. In Proceedings of ACL-HLT.\nJan Haji ˇc et al. 2009. The CoNLL-2009 shared\ntask: Syntactic and semantic dependencies in mul-\ntiple languages. In Proceedings of CoNLL.\nMatthew Honnibal, Yoav Goldberg, and Mark Johnson.\n2013. A non-monotonic arc-eager transition system\nfor dependency parsing. In Proceedings of CoNLL.\nLiang Huang, Suphan Fayong, and Yang Guo. 2012.\nStructured perceptron with inexact search. In Pro-\nceedings of the NAACL.\nRichard Johansson and Pierre Nugues. 2007. Ex-\ntended constituent-to-dependency conversion for\nEnglish. In Proceedings of NODALIDA.\n395\nJonathan K. Kummerfeld et al. 2012. Parser show-\ndown at the wall street corral: An empirical inves-\ntigation of error types in parser output. In Proceed-\nings of EMNLP.\nJoseph Le Roux et al. 2012. DCU-Paris13 systems\nfor the SANCL 2012 shared task. In Proceedings\nof the First Workshop on Syntactic Analysis of Non-\nCanonical Language (SANCL).\nJoseph Le Roux, Antoine Rozenknop, and Jennifer\nFoster. 2013. Combining PCFG-LA models with\ndual decomposition: A case study with function la-\nbels and binarization. In Proceedings of EMNLP.\nTao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and\nTommi Jaakkola. 2014. Low-rank tensors for scor-\ning dependency structures. In Proceedings of the\n52nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1381–1391, Baltimore, Maryland, June. Association\nfor Computational Linguistics.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a large anno-\ntated corpus of english: the Penn treebank. Compu-\ntational Linguistics, 19(2):313–330.\nAndr´e F. T. Martins, Miguel B. Almeida, and Noah A.\nSmith. 2013. Turning on the turbo: Fast third-order\nnon-projective turbo parsers. In Proceedings of the\nACL.\nRyan McDonald and Joakim Nivre. 2007. Character-\nizing the errors of data-driven dependency parsing\nmodels. In Proceedings of EMNLP-CoNLL.\nRyan McDonald and Joakim Nivre. 2011. Analyzing\nand integrating dependency parsers. Computational\nLinguistics, 37(1):197–230.\nMakoto Miwa et al. 2010. A comparative study of syn-\ntactic parsers for event extraction. In Proceedings of\nBioNLP.\nJens Nilsson and Joakim Nivre. 2008. MaltEval:\nAn evaluation and visualization tool for dependency\nparsing. In Proceedings of LREC.\nJoakim Nivre et al. 2007. The CoNLL 2007 shared\ntask on dependency parsing. In Proceedings of\nCoNLL.\nJoakim Nivre, Laura Rimell, Ryan McDonald, and Car-\nlos G ´omez Rodr ´ıguez. 2010. Evaluation of de-\npendency parsers on unbounded dependencies. In\nProceedings of the 23rd International Conference\non Computational Linguistics (Coling 2010), pages\n833–841, Beijing, China, August. Coling 2010 Or-\nganizing Committee.\nStephan Oepen et al. 2014. SemEval 2014 Task 8:\nBroad-Coverage Semantic Dependency Parsing. In\nProceedings of the 8th International Workshop on\nSemantic Evaluation (SemEval 2014), pages 63–72.\nSlav Petrov and Ryan McDonald. 2012. Overview of\nthe 2012 shared task on parsing the web. In Pro-\nceedings of the First Workshop on Syntactic Analysis\nof Non-Canonical Language (SANCL) Shared Task.\nSlav Petrov, Pi-Chuan Chang, Michael Ringgaard, and\nHiyan Alshawi. 2010. Uptraining for accurate de-\nterministic question parsing. In Proceedings of the\n2010 Conference on Empirical Methods in Natural\nLanguage Processing, pages 705–713, Cambridge,\nMA, October. Association for Computational Lin-\nguistics.\nSameer Pradhan et al. 2013. Towards robust linguis-\ntic analysis using OntoNotes. In Proceedings of\nCoNLL.\nMohammad Sadegh Rasooli and Joel R. Tetreault.\n2015. Yara parser: A fast and accurate dependency\nparser. CoRR, abs/1503.06733.\nKenji Sagae and Alon Lavie. 2006. Parser combina-\ntion by reparsing. In Proceedings HLT-NAACL.\nKenji Sagae and Jun’ichi Tsujii. 2010. Dependency\nparsing and domain adaptation with data-driven LR\nmodels and parser ensembles. In Trends in Parsing\nTechnology: Dependency Parsing, Domain Adapta-\ntion, and Deep Parsing, pages 57–68. Springer.\nDjam´e Seddah et al. 2013. Overview of the SPMRL\n2013 shared task: A cross-framework evaluation of\nparsing morphologically rich languages. In Pro-\nceedings of the 4th Workshop on Statistical Parsing\nof Morphologically Rich Languages.\nPontus Stenetorp et al. 2012. BRAT: A web-based tool\nfor NLP-assisted text annotation. In Proceedings of\nthe EACL.\nMihai Surdeanu et al. 2008. The CoNLL-2008 shared\ntask on joint parsing of syntactic and semantic de-\npendencies. In Proceedings of CoNLL.\nReut Tsarfaty, Joakim Nivre, and Evelina Andersson.\n2011. Evaluating dependency parsing: Robust and\nheuristics-free cross-annotation evaluation. In Pro-\nceedings of EMNLP.\nRalph Weischedel et al. 2011. OntoNotes: A large\ntraining corpus for enhanced processing. In Joseph\nOlive, Caitlin Christianson, and John McCary, ed-\nitors, Handbook of Natural Language Processing\nand Machine Translation. Springer.\nDeniz Yuret, Laura Rimell, and Aydin Han. 2013.\nParser evaluation using textual entailments. Lan-\nguage Resources and Evaluation, 47(3).\n396",
  "topic": "Parsing",
  "concepts": [
    {
      "name": "Parsing",
      "score": 0.7956695556640625
    },
    {
      "name": "Dependency grammar",
      "score": 0.7781857252120972
    },
    {
      "name": "Computer science",
      "score": 0.753210723400116
    },
    {
      "name": "Dependency (UML)",
      "score": 0.7311873435974121
    },
    {
      "name": "Natural language processing",
      "score": 0.6020437479019165
    },
    {
      "name": "Computational linguistics",
      "score": 0.5763598680496216
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4480781555175781
    },
    {
      "name": "Programming language",
      "score": 0.3789623975753784
    }
  ]
}