{
  "title": "Proof of concept of a workflow methodology for the creation of basic canine head anatomy veterinary education tool using augmented reality",
  "url": "https://openalex.org/W2801249178",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A5026667419",
      "name": "Roxie Christ",
      "affiliations": [
        "Glasgow School of Art",
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A5041894676",
      "name": "Julien Guevar",
      "affiliations": [
        "North Carolina State University"
      ]
    },
    {
      "id": "https://openalex.org/A5051179318",
      "name": "Matthieu Poyade",
      "affiliations": [
        "Glasgow School of Art"
      ]
    },
    {
      "id": "https://openalex.org/A5011495763",
      "name": "Paul Rea",
      "affiliations": [
        "University of Glasgow"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1964809186",
    "https://openalex.org/W1970314973",
    "https://openalex.org/W2031169371",
    "https://openalex.org/W2145046385",
    "https://openalex.org/W2120877934",
    "https://openalex.org/W1963798853",
    "https://openalex.org/W1495726115",
    "https://openalex.org/W2267775033",
    "https://openalex.org/W1998822420",
    "https://openalex.org/W1997484302",
    "https://openalex.org/W1992054115",
    "https://openalex.org/W1988455886",
    "https://openalex.org/W2191781635",
    "https://openalex.org/W2144116465",
    "https://openalex.org/W2075057776",
    "https://openalex.org/W1529289001",
    "https://openalex.org/W2281473978",
    "https://openalex.org/W2235619726",
    "https://openalex.org/W2095368950",
    "https://openalex.org/W1777735180",
    "https://openalex.org/W1994123477",
    "https://openalex.org/W2019961671",
    "https://openalex.org/W2587497605",
    "https://openalex.org/W2029060213",
    "https://openalex.org/W2017465021",
    "https://openalex.org/W1601138754",
    "https://openalex.org/W2091643909",
    "https://openalex.org/W2289646037",
    "https://openalex.org/W2024337161"
  ],
  "abstract": "Neuroanatomy can be challenging to both teach and learn within the undergraduate veterinary medicine and surgery curriculum. Traditional techniques have been used for many years, but there has now been a progression to move towards alternative digital models and interactive 3D models to engage the learner. However, digital innovations in the curriculum have typically involved the medical curriculum rather than the veterinary curriculum. Therefore, we aimed to create a simple workflow methodology to highlight the simplicity there is in creating a mobile augmented reality application of basic canine head anatomy. Using canine CT and MRI scans and widely available software programs, we demonstrate how to create an interactive model of head anatomy. This was applied to augmented reality for a popular Android mobile device to demonstrate the user-friendly interface. Here we present the processes, challenges and resolutions for the creation of a highly accurate, data based anatomical model that could potentially be used in the veterinary curriculum. This proof of concept study provides an excellent framework for the creation of augmented reality training products for veterinary education. The lack of similar resources within this field provides the ideal platform to extend this into other areas of veterinary education and beyond.",
  "full_text": "RESEA RCH ARTICL E\nProof of concept of a workflow methodology\nfor the creation of basic canine head anatomy\nveterinary education tool using augmented\nreality\nRoxie Christ\n1,2\n, Julien Guevar\n3\n, Matthieu Poyade\n2\n, Paul M. Rea\n1\n*\n1 Anatomy Facility, Thoms on Building, School of Life Sciences, College of Medical, Veterinary and Life\nSciences, Univers ity of Glasgow, Glasgow, United Kingdom, 2 School of Simulation and Visualisation, The\nGlasgow School of Art, Glasgow, United Kingdom , 3 Department of Clinical Sciences, College of Veterinary\nMedicine, North Carolina State University, Raleigh, NC, United States of America\n* Paul.Rea @glasgow .ac.uk\nAbstract\nNeuroanatomy can be challenging to both teach and learn within the undergraduate veteri-\nnary medicine and surgery curriculum. Traditional techniques have been used for many\nyears, but there has now been a progression to move towards alternative digital models and\ninteractive 3D models to engage the learner. However, digital innovations in the curriculum\nhave typically involved the medical curriculum rather than the veterinary curriculum. There-\nfore, we aimed to create a simple workflow methodology to highlight the simplicity there is in\ncreating a mobile augmented reality application of basic canine head anatomy. Using canine\nCT and MRI scans and widely available software programs, we demonstrate how to create\nan interactive model of head anatomy. This was applied to augmented reality for a popular\nAndroid mobile device to demonstrate the user-friendly interface. Here we present the pro-\ncesses, challenges and resolutions for the creation of a highly accurate, data based anatom-\nical model that could potentially be used in the veterinary curriculum. This proof of concept\nstudy provides an excellent framework for the creation of augmented reality training prod-\nucts for veterinary education. The lack of similar resources within this field provides the ideal\nplatform to extend this into other areas of veterinary education and beyond.\nIntroduction\nThe practice of medicine and veterinary medicine relies heavily on clinicians thorough and\nworking knowledge of 3D anatomy. Skills needed in a clinician’s repertoire include physical\nexamination, interpretation of imaging data, including advanced imaging, correctly diagnos-\ning, and procedures such as surgery all requiring an in-depth anatomy knowledge [1].\nTraditionally, this knowledge acquisition has been a mainstay of medical education and\nclinical training. Students in the past have relied on lengthy didactic lectures, cadaveric dissec-\ntion, textbook figures and simplified models to develop their knowledge and anatomy skills\n[1,2]. Indeed, anatomy has been viewed as the foundation of medical training and budgets\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 1 / 16\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Christ R, Guevar J, Poyade M, Rea PM\n(2018) Proof of concept of a workflow\nmethodolo gy for the creation of basic canine head\nanatomy veterinary education tool using\naugmented reality. PLoS ONE 13(4): e0195866.\nhttps://do i.org/10.1371/j ournal.pone .0195866\nEditor: Francesco Staffieri, University of Bari,\nITALY\nReceived: August 4, 2017\nAccepted: March 30, 2018\nPublished: April 26, 2018\nCopyright: © 2018 Christ et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: All relevant data are\nwithin the paper.\nFunding: The authors received no specific funding\nfor this work.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nhave, in the past, been funding dissection programs [1,3]. However, over recent years, there\nhas been a reduction in the amount of time allocated to anatomy teaching, including dissec-\ntion, and a lack of qualified staff to teach clinically applied anatomy [4–6].\nAs a consequence of this, and the rapidly progressing field of digital products in anatomical\nand medical education, there has been an explosion onto the market of a wide range of prod-\nucts [7–10]. Ever since the development of the first major game-changer in digital anatomy of\nthe Visible Human Project, there are now a plethora of tools available [11].\nHowever, unlike human anatomy and medical training using digital products, there has\nbeen a serious lack of progress in this field from the veterinary perspective. Certainly, there\nhave been some attempts to develop educational and training materials for the veterinary com-\nmunity. These have however been around isolated cases including the rat brain, frog and limbs\nof the horse [12–14]. In addition, they did not have sufficient levels of detail that would be\nrequired for veterinary students to embed into the curriculum effectively. A more accurate\nrepresentation was based on the Visible Animal Project (VAP), which attempted to create a\n3D database of anatomical items of the dog trunk [15]. However, it lacked the detail, as so\nmany do, of the cranial anatomy of the dog.\nSimilarly, there has been a rise in the popularity of virtual reality (VR) in human anatomy\neducation [16]. However the challenge here is to make the invisible visible. Even the most skil-\nful dissection of specimens can only reveal certain aspects of structural relationships, and only\nafter significant investment of time and resources [17–19]. VR, however, can make many\naspects of the invisible visible to a user in an immersive way, as quickly as navigating to a web-\nsite or accessing a mobile application. If the VR is convincing enough, students can not only\ninteract with structures and concepts in ways that expand their 3D reasoning, but they can\nbegin to practice clinical skills, such as assessment sequences and problem solving, all at their\nown pace [17–20]. One of the key advantages to using VR is that a vast variety of structures or\nscenarios can be generated with more ease than any dissection or didactic lecture, and be\nexplored and repeated as a student wishes. Additionally, VR applications involving 3D models\nof anatomical structures are often generated using medical imaging scans, which can account\nfor a plethora of variances, structures [17–19; 21].\nVR may have many advantages, but it still only immerses users in a completely alternate\n‘reality.’ Some have argued that difficulties bridging the gap between VR and real world appli-\ncations may temper the benefits [17–19]. But what happens when virtual and digital aspects\nare layered over real-time reality, if real world images and scenarios could be augmented with\ndigital features and objects?\nAugmented reality (AR) applications seek to do exactly this—to enhance real world experience\nwith virtual aspects [22,23]. This can be done in a variety of ways, but the most prominent style of\nAR involves scanning real-time images of the world from a device camera and displaying those\nimages overlaid with digital elements, such as information, 2D graphics, and 3D models that the\nuser can interact with [22–24]. These digital components bear significance on whatever is\nscanned, such as providing information, showing relevant aspects, or in some cases making a sort\nof game out of the real-world scenario. AR has recently been used in many capacities involving\neducation, training, simulations, and even in enhancing surgical procedures [18–20; 22–26].\nTherefore, the purpose of this study was to harness new technologies but develop it in a\nvery unique manner. We wanted to take advantage of many 3D modelling techniques, and the\nbenefits of ubiquitous digital learning, by attempting to create an effective, novel modality for\nveterinary students to learn 3D canine head anatomy using highly accurate models generated\nfrom MRI and CT scans in an engaging augmented reality (AR) format. Since the canine skull\nand brain represent some of the more complicated areas for veterinary students to study, and\nare currently under-represented in available resources, it also provides a unique opportunity\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 2 / 16\nto trial new educational approaches. Therefore, the goal of this project was to explore method-\nologies for segmenting MRI and CT scans, generating and refining models of key elements of\nthe canine skull and brain from them, and making them available in an interactive, intuitive\nAR platform.\nMaterials\nData\nA variety of software and hardware was used in this study to process the data, generate 3D\nmodels and integrate them into the final AR application. The following details each medical\nscan used, software package utilized and the apparatus needed to execute each stage of the\nstudy, and is summarised in Tables 1 and 2. This study was considered as sub-threshold for\nspecific ethical approval by the convenor of the School of Veterinary Medicine ethics commit-\ntee, as the work involved only analysis of data routinely recorded from normal and necessary\nclinical procedures.\nIn addition, the following hardware was used in this study:\n• PC (HP Z230 Workstation)\n• HTC One M8 mobile phone\n• Samsung Tablet Galaxy 10”\nMethods\nThe methodology utilized in this project involved three stages: data extraction, development of\naccurate 3D anatomical models, and integration of those models into an interactive AR\nTable 1. This demonstr ates the scans used for data extraction either from computeri sed tomogra phy (CT) or\nmagneti c resonance imaging (MRI).\nScan Type Specific ations Purpose\nCanine Head CT CT images of the head were obtained using a dual slice\nCT scanner (Siemens Somatom Spirit).\nView and segment canine\nskull\nCanine MRI in T1W, T2W,\nT1W with GAD contrast\nMagnetic resonance (MR) imaging of the brain\nperformed using a 1.5-Tesla unit (Siemens Magnet om\nEssenza)\nView and segment canine\nbrain and substructu res\nCanine T2 CISS MRI\n(isotropic voxels)\nMagnetic resonance (MR) imaging of the brain\nperformed using a 1.5-Tesla unit (Siemens Magnet om\nEssenza)\nView and segment canine\nbrain and substructu res\nhttps://d oi.org/10.1371/j ournal.pon e.0195866.t00 1\nTable 2. Software packages used, the reasons for this, and the web links.\nSoftwar e Purpose Web link\n3D Slicer View medical scan data\nSegment anatomica l contours\nGenerate 3D represent ations and models\nhttps://www.s licer.org/\nAutodesk 3ds\nMax\nCreate and manually alter polygonal meshes\nRetopolog ize existing models\nhttp://www .autodesk.co.uk /products/\n3ds-max/ overview\nZbrush 4R6 Sculpt polygonal meshes with variety of tools\nReduce polygon counts with Remesher\nhttp://pixo logic.com/zb rush/features/\noverview/\nUnity 5.3 Build interactive applications https://unit y3d.com/\nVuforia Use plugin in Unity to create Augment ed Reality\napplicati ons using image targets\nhttp://www .vuforia.com/\nhttps://d oi.org/10.1371/j ournal.pon e.0195866.t00 2\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 3 / 16\nplatform. The data extraction process entailed segmentation of the acquired CT and MRI\nscans of the canine head in 3D Slicer to highlight structures of interest and generate basic mod-\nels. These models were then refined using a variety of methods in both 3DS Max and Zbrush.\nFinally, an interactive AR platform was built using Unity, in which a user can interact with\neach model set in an exciting AR experience. The workflow methodology is summarised in\nFigs 1–3 and explained below.\nData extraction\nTo obtain anatomically accurate models, segmentation was performed on both CT and MRI\nscans, to reconstruct the canine skull and brain in 3D. Segmentation was performed in 3D\nSlicer, via a Digital Imaging and Communications in Medicine (DICOM) stack. Manual slice-\nby-slice segmentation was used with the Threshold Paint to ensure an accurate reconstruction.\nThe Model Maker in 3D slicer was used to create the 3D skull with a balancing between\nsmoothing of natural ridges and the thinness of the slices used. The Laplacian smoother set to\n63 and the Decimation at 0.11 were deemed most appropriate.\nThe MRI dataset for extraction of the brain had the best resolution in the T2 dorsal plane\nfor segmentation of the brain. To ensure accurate representation of the brain, manual slice-by-\nslice segmentation was used to identify the larger structures like the forebrain, cerebellum and\nbrainstem, but also to differentiate the different sulci and gyri of the brain. However, due to a\nFig 1. Workflow methodo logy related to data extract ion. From “YES” this leads to Fig 2, Modelling .\nhttps://d oi.org/10.1371/j ournal.pon e.0195866.g0 01\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 4 / 16\nnumber of issues related to the resolution of these scans, it was decided that a T2 CISS MRI\nwould be better suited. The latter is a 3D scan of 1mm\n3\nvoxels. Although it does not always\ngive good differentiation between soft tissues like grey and white matter, it does have a high\ncontrast between cerebrospinal fluid and soft tissue (Fig 4). Laplacian smoothing set to 38 and\nDecimation at 0.21 generated a satisfactory model with recognisable contouring of sulci, gyri,\nlobes and fissure and proportionality of the cerebrum. Accurate volume of the cerebellum\ncould be obtained but nor its surface texture nor its lobes could be precisely recreated using\nthe T2 CISS MRI data. The same was true for the brainstem with accurate volume but not suf-\nficient precise data to reconstruct its exact surface.\nRetopping and refining skull. The skull model generated by 3D Slicer’s Model Maker\nwas smooth, but had many holes. Some of these holes were inherent to the structure of the\nskull itself, whilst others were artefacts from the scan and shortcomings in the model making.\nIn addition, the skull contained 964,354 polygons, far in excess for rendering in a mobile appli-\ncation. Therefore, manual retopology in 3DS Max was chosen to clean the mesh for mobile\napplications, yet at the same time remove false holes from the model. “Draw on” was initially\nselected for the surface of the skull and “conform” was used to gently blanket the plane onto\nthe contour of the skull. However, due to over-sensitivity in this method, a full manual\napproach to retopology was used. The “strip” and “extend” options were utilised to draw a line\nof connected square polygons along a contour of the original mesh e.g. along the edge of the\nmandible, zygomatic arch, midline of the skull and nasal bones. Irregularities were smoothed\nand regulated using the “Relax” tool, making the spacing between polygons more regular. This\nwas carried out until a clean retopologized mesh was obtained over half of the skull down the\nmidline (Fig 5). As the skull is generally symmetrical, half of the original mesh needed to be\nrefined and retopologized in this manner.\nFig 2. Workflow methodo logy related to modelling. From “Adequate” it leads to Fig 3, AR Interface.\nhttps://d oi.org/10.1371/j ournal.pon e.0195866.g0 02\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 5 / 16\nThe “Symmetry” modifier in 2DS Max was utilised to create a symmetrical mirrored mesh,\naltered and adjusted until the mirrored items lined up, and joined together with the “Bridge”\nfeature of the “Extend” tool. This resulted in a full retoplogized mesh which, in 3D Slicer, had a\npolygon count of 964,354, but the retopologized skull and mandible had a polygon count of\n128,653, 13% of the original count. The final step here was then to use “OpenSubdiv” thus sub-\ndividing the existing polygons to smooth the mesh, interpolating the lines, and thus giving a\nsmoothing effect on all edges (Fig 6).\nSculpting and remeshing the brain. To ensure the brain was corrected for minor anoma-\nlies, and ensuring the professional appearance to it, Zbrush was used. There were no major\nissues with the brain, unlike the skull, and following import, a simple smoothing tool was used\nover each gyrus and elements of the brainstem and cerebellum. The “Dam” tool helped to\nsharpen the grooves of the sulci, and the “Clay Build-Up” tool was used to thicken areas which\nhad lost mass or developed gaps through the carving process. At this stage, any minor anatom-\nical adjustments could be made ensuring anatomical accuracy of the model. Finally, the brain\nneeded to be retopologized by using the “Remesher” facility in the “Geometry” menu. This\nallowed for reduction of the polygon count from 706,036 to 342,294 polygons, yet still main-\ntaining a high degree of anatomical accuracy and model cleanliness. The final model once\nshaded materials were applied can be seen in Fig 7.\nFig 3. Workflow methodo logy related to the augmen ted reality (AR) Interface.\nhttps://d oi.org/10.1371/j ournal.pon e.0195866.g0 03\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 6 / 16\nInterface development\nTrial version. To create an interface, a simplified PC platform was created first with\nUnity, prior to the augmented reality (AR) platform. This involved creating the basic function-\nality of three key scenes initially. These were a “Start” screen, a scene for the skull and one for\nthe brain. The “Start” scene included a simple title panel with two buttons, one for the brain\nand one for the skull. A “Back” button was also included which returned to the opening scene.\nFig 4. Manual segmentat ion of the contour of the forebrain in the dorsal plane of the T2 CISS MRI.\nhttps://do i.org/10.1371/j ournal.pone .0195866.g00 4\nFig 5. Retopology mesh over half of the original model, with (left) and without edged faces (right).\nhttps://d oi.org/10.1371/j ournal.pon e.0195866.g0 05\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 7 / 16\nFunctionality was then added including rotation, highlighting section and user interface\n(UI) elements linked to sub-object selections. Initially “MouseOrbitZoom” was used as a trial\nplatform to use the camera function to orbit around and zoom in on a selected target during\nthe game. In addition, a Generic Script and a Particular script were used to enable smooth\nmovement between scenes in the interactive application.\nFollowing this trial build for the platform, the AR element was developed using Vuforia\ndevelopment kit for Android in a new Unity project. An AR camera and an Image Target were\nadded to each of the scenes with the updated version of Javascript installed and applied to the\nHTC One M8 for functionality testing.\nFinal version. As with the experimental trial version, three scenes were created: “Start”,\nSkull” and “Brain”. The “Start” scene was created using the same AR camera setup as the inter-\nactive scenes, with a semi-transparent panel to allow the user to get live camera feed behind\nthe menu. The “Skull” and “Brain” options were adapted from the AR trial scene, this time\nincluding a semi-transparent “Top Panel” with title information and a back button for return-\ning to the “Start” menu.\nFig 6. Various views of the completed skull model with the OpenSu bdiv modifier and no edged faces.\nhttps://do i.org/10.1371/j ournal.pone .0195866.g00 6\nFig 7. Final brain model when shaded materials have been applied.\nhttps://d oi.org/10.1371/j ournal.pon e.0195866.g0 07\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 8 / 16\nSagittal sections of the CT skull and MRI brain were used for the final image targets in the\nVuforia Developer Portal. Trial builds were then created to test navigation, model placement\nand model rendering. Test Game Objects were created to trial the functionality from the PC\nversion for the AR. Generic and Particular scripts were imported from the previous trial proj-\nect, and utilised on trial objects within the AR scene with colours and semi-transparent custom\npanels corresponding to each object.\nFunctional development. This was created in an alpha version using the final models\nwith colliders. Information panels were created for each sub-object in each scene, with differ-\nent colours for better visual differentiation of items. A scrollbar was also installed that would\nwork on touch, and scroll freely. Information panels giving a brief description of the sub-\nobject, highlighting important landmarks and features, and links to further discussion and\nresources were applied to each panel as appropriate.\nSimple sphere and capsule colliders were created for each sub-object and Generic and Par-\nticular classes were applied to Empty Game Objects and sub-objects, as trialled initially. In\naddition, rotation functionality needed to be embedded into the AR application. Trialling\ndemonstrated numerous issues: bugs, few were “clamped”, and inability to adjust the rotation\nfor a moving camera. Therefore, a custom-made rotation facility was created. This was\ndesigned to be applied directly to the object needing rotation, so that no camera was involved.\nConditional statements were applied that prevented rotation on a simple touch only. Rotation\nonly occurred when there was a change in position on the touch or a “delta position”.\nFor rotation, clamping was trialled, with the notion that simply adjusting the rotation direc-\ntion based on the object’s orientation would be enough to establish completely intuitive rota-\ntion. While this concept proved worthy when the mobile device was held directly in front of\nthe image target, it failed to adjust this performance to odd angles between the image target\nand the mobile device, as is necessary for a full AR experience.\nSo instead of clamping the movement, the rotation script was made to retrieve information\nabout the Main Camera’s relative axis, and then to adjust the way the rotation of the object\nbehaves based on these vectors and its local axis. In this manner, the rotation behaviour would\nalways feel intuitive and behave as expected, no matter the angle at which the mobile device is\nbeing held around the target.\nOnce all these elements had been refined, the overall aesthetic of the application was regu-\nlated and beautified, to maximize the user’s intuitive feel and enjoyment of the application,\nwhile optimizing the potential educational output by ensuring clean, attractive, informative\nsimplicity.\nResults\nThe methods employed in this study produced the following augmented reality application\nwhich allows the user to explore and interact with anatomical models of the canine skull and\nbrain, utilising the functionality depicted in Fig 8A and 8B.\nThe “Start Up” screen employs a semi-transparent menu panel through which the user gets\nlive camera feed. The buttons included navigate to the following scenes, for the skull, brain or\nacknowledgements page. The image targets needed for each scene have been rendered on to\neach button (Fig 9).\nThe “Skull” button navigates to the skull scene where the user hovers over the image target\ncreated from the canine CT. This allows the skull model to be visualised as depicted din Fig 10.\nThe user can rotate the model in all three axes by touching and dragging on the screen. The\n“Reset” button returns the model to its original position. Selecting the main part of the skull\nhighlights it and triggers a pop up panel providing key anatomical information, which can be\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 9 / 16\nscrolled through (Fig 11). Selecting individual anatomical territories will reveal further infor-\nmation related to that site.\nFrom the “Start Up” menu, the user can also select the “Brain” button, navigating to that\ntopic. The user then utilises the image target, created from the canine MRI. This allows the\ncanine brain model to appear, which the user can rotate and reset in the same fashion as the\nskull. Selection of specific anatomical regions (e.g. forebrain, cerebellum, brainstem etc.) will\nthen reveal further information (Fig 12). In addition, an acknowledgments page was also\ncreated.\nFig 8. A. Basic navigation between scenes. B. Intended functiona lity features.\nhttps://d oi.org/10.1371/j ournal.pon e.0195866.g0 08\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 10 / 16\nDiscussion\nThe aim of this project was to create a workflow methodology for development of a mobile\naugmented reality application, potentially to be used for veterinary students learning basic\ncanine head anatomy, both of the skull and brain, in an exciting and intuitive interface. We\nhave shown, through adoption of a variety of commonly available software packages and\nFig 9. The Start Up menu showing skull, brain and acknow ledgments options.\nhttps://d oi.org/10.1371/j ournal.pon e.0195866.g0 09\nFig 10. Canine skull menu.\nhttps://do i.org/10.1371/j ournal.pone .0195866.g01 0\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 11 / 16\nFig 11. Skull selecte d in the skull scene.\nhttps://do i.org/10.1371/j ournal.pone .0195866.g01 1\nFig 12. Forebrain selected in the brain scene.\nhttps://do i.org/10.1371/j ournal.pone .0195866.g01 2\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 12 / 16\nimaging, how simple it is to create a mobile AR application to potentially be used in future vet-\nerinary education.\nAnatomy within the veterinary curriculum represents a significant proportion on which\nclinical training originates, and forms the basis for communication of diagnosis and treatment\nto the owners and other professionals alike [27,28]. Indeed, teaching within a modern day vet-\nerinary curriculum can include a number of modalities, similar to a medical degree pro-\ngramme [29–31]. However, like any curriculum, there are always areas that students find more\nchallenging than other.\nStudents undertaking veterinary education, like medical training programmes, find the\nconcept of the nervous system, and all related aspects of it, difficult [29,32]. Nowadays, there\nare a plethora of digital technologies available to aid learning in the medical anatomical field\n[7–10], but perhaps not so much within the veterinary educational arena [33–35]. Some are\nemerging; however, they are not advancing at the rate that they are within human anatomical\neducation and training.\nTherefore, it is timely that we have developed a clear methodology for the creation of digital\nveterinary education related products. Given the inadequacies around the teaching of veteri-\nnary neuroanatomy, and issues related to visualising structures, modern alternatives are much\nneeded. Indeed, some of the first work in this area was related to computer assisted learning\nand the development of learning modules via digital lectures, online tutorials and question\nand answer packages [34]. Certainly this was innovative for its time but technology and our\nunderstanding of its educational uses has improved significantly.\nPreviously the “Visible Animal Project” represented the first 3D anatomical animal model\ndesigned specifically for veterinary training. However, the fine detail of canine anatomy was\nnot realised, and lacked detail. From then, “Virtual Canine Anatomy: The Head” was designed\nand implemented into the first year of the veterinary dissection curriculum within Colorado\nState University. However, it was built upon 2D views and the illusion was portrayed of a 3D\nobject, but was not as engaging as anticipated.\nThus far, to our knowledge, there is no interactive canine computer aided learning package\nthat offers interactivity and immersion, hence this study using modern day technologies. To\nenable this, we followed the advice of Clark and Mayer [36] who discussed that for digital tech-\nnologies to be effective, they advocate the use of good visuals, text and segmenting the different\naspects of learning. Within this AR workflow, we have adopted these elements into the canine\nneuroanatomy to engage the learner in the visualising the detailed anatomy using accessible\ntechnology, in this case with a popular smart phone. Indeed, it also complies with it being an\ninviting and interesting environment, responsive (in that it is visually active) and provides\nfeedback and information related to each of the anatomical areas [37]. Therefore, what we\nhave created has the potential to be engaging in the learning process, however the next stage\nfor this study would be formal testing of the end-user–veterinary students.\nLimitations\nIn relation to the CT scan data, the only slight drawback was in the fact that it had to be manu-\nally manipulated for segmentation. Although it can take a little longer than more automatic\ntechniques, it certainly does allow for identification of clear distinctions between bony\nstructures.\nHowever, the initial MRI dataset that was initially to be used did not have such accessibility\nand ease of use. The resolution in the T2 dorsal plane showed excellent definition, not only of\nthe larger structures (e.g. cerebrum and cerebellum), but the “out-of-plane” resolution was\nproblematic. The original plan was to segment both larger structures and also the areas\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 13 / 16\nbetween grey and white matter. This would have benefited from an educational perspective in\nbeing able to show these clinically relevant areas. It also logically seemed possible under the T1\nweighting for grey versus white matter distinction. However, segmenting the full 3D brain and\nits components produced a cubic and completely unrecognisable model. These scans are not\nrecommended for attempting indirect volume rendering, or the generation of 3D polygonal\nmeshes to be used in other formats. While employing direct volume rendering modules onto\nthe voxel stack itself is able to provide fascinating insight (into a 3D understanding of internal\nstructures to the user), which can be done in both 3D Slicer and Osirix, generation of clean\npolygonal meshes representing many structures is not feasible. Clinical MRI dataset are there-\nfore of value for an initial assessment of a technique like the one described here to give the stu-\ndent an appropriate and accurate volume relationship between forebrain, brainstem and\ncerebellum. However, better quality scans (research scan dataset) would be required if more\ndetailed neuroanatomy is needed.\nFuture work\nExisting models in this field are rather rudimentary and serve to illuminate what is possible,\nrather than creating a full educational function. However, with careful refinement and invest-\nment in these models, there is much opportunity for advancement of the anatomical accuracy\nthrough, for example, fine detail of the smaller facial bones. These could be further developed\nwith micro-CT data, to ensure a more accurate representation of the skeletal anatomy.\nA difficulty with this type of work, which merits further research, is refining the accuracy\nand detail of the canine brain for a mobile augmented reality application. The potential here is\nfor higher resolution MR datasets, dissections and photogrammetry combined to provide a\nphotorealistic and highly accurate reconstruction.\nThis full incorporation of anatomical and also potentially neuroanatomy and neuroscience\nwould need to be educationally validated by those veterinary surgeons who specialise in neuro-\nsurgery and clinically applied research. It would also need validation from ultimately the end\nuser–students. A well-designed trial with both alpha and beta phase testing would be necessary\nto ensure the application of this type of teaching tool.\nConclusion\nThe purpose of this project was to establish the processes for a methodology in the creation of\nan augmented reality application for basic canine head anatomy. We have clearly identified\nthe advantages and drawbacks of different approaches in the creation of a robust and interac-\ntive augmented reality tool for veterinary education. Of course, further validation is needed\nboth from specialist neurological and neurosurgical veterinary clinicians and the students\nthemselves. However, this process clearly sets out the workflow methodology in the creation of\na novel, innovative, different and cutting edge tool for enhancing learning opportunities with a\nvisual, tactile and engaging manner. Now, we have shown the basic recipe for those involved\nin veterinary education who are keen to develop ideas and innovations, but tailor make it for\neach of your teaching, learning and assessment methods both locally, nationally and interna-\ntionally. This type of technological advance and application is not only limited to veterinary\neducation but can be opened up to ensure an immersive learning environment for anything\nrequiring visual and tactile learning.\nAuthor Contributions\nConceptualization: Julien Guevar, Matthieu Poyade, Paul M. Rea.\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 14 / 16\nFormal analysis: Julien Guevar, Matthieu Poyade.\nMethodology: Roxie Christ, Paul M. Rea.\nProject administration: Julien Guevar, Matthieu Poyade.\nSoftware: Roxie Christ.\nSupervision: Julien Guevar, Matthieu Poyade, Paul M. Rea.\nValidation: Roxie Christ.\nVisualization: Roxie Christ, Matthieu Poyade.\nWriting – original draft: Roxie Christ, Julien Guevar, Matthieu Poyade, Paul M. Rea.\nWriting – review & editing: Roxie Christ, Julien Guevar, Matthieu Poyade, Paul M. Rea.\nReferences\n1. Sugand K, Abrahams P, Khurana A. The Anatomy of Anatomy: A Review for its Modernis ation. Anatom-\nical Sciences Educatio n. 2010; 3(2): 83–93. https:// doi.org/10.10 02/ase.13 9 PMID: 202052 65\n2. Elizondo-O maña R.E., Guzma ´ n-Lo ´ pez S. and De Los Angeles Garcı ´ a-Rodrı ´ guez M. Dissection as a\nteaching tool: past, present, and future. The Anatomica l Record Part B: The New Anatomist. 2005; 285\n(1): 11–15.\n3. Turney B.W. Anatomy in a modern medical curricul um. Ann R Coll Surg Engl. 2007; 89: 104–107.\nhttps://doi.or g/10.130 8/003588407X 168244 PMID: 17346399\n4. Verhoeven BH, Verwijnen GM, Scherpbier AJ, Van Der Vleuten CPM. Growth of medical knowledge.\nMed Educ. 2002; 36: 711–71 7. PMID: 12191053\n5. Patel KM, Moxham BJ. The relationships between learning outcomes and methods of teaching anat-\nomy as perceived by profession al anatomists . Clinical Anatomy. 2008; 21: 182–189. https://doi.or g/10.\n1002/ca.2 0584 PMID: 181892 77\n6. Ashwell KW, Halasz P. An Acrobat-based program for gross anatomy revision. Medical Education.\n2004; 38: 1185–1 186. https://doi.or g/10.111 1/j.1365-292 9.2004.0 1990.x PMID: 15507017\n7. 3D4Medic al. http://www.3d4 medical.com [Acces sed 11th July 2017]\n8. Anatomy.TV https:// www.anatomy .tv [Acces sed 11th July 2017]\n9. BodyViz http://www.bod yviz.com [Accessed 11th July 2017]\n10. Cyber Anatomy Holographic TM http://c yber-anatom y.com/Holo graphic.php [Acces sed 11th July 2017]\n11. Spitzer VM, Whitlock DG. Atlas of the Visible Human Male: Reverse Engineeri ng of the Human Body.\n1998. Sadbury , Jones & Barlett.\n12. Robertson D, Johnston W, Nip W. The Whole Frog Project. The Second Interna tional WWW Confer-\nence 1994. Available: http://frog gy.lbl.gov/pa pers/WWW .94/paper.ht ml [Accesse d 11\nth\nJuly 2017]\n13. Toga AW, Santori EM, Hazani R, Ambach K. A 3D digital map of the rat brain. Brain Research Bulletin.\n1995; 38(1): 77–85. PMID: 7552378\n14. Martinelli MJ, Kuriashk in IV, Carragher BO, Clarkson RB, Baker GJ. Magnetic Resonan ce Imaging of\nthe Equine Metacarpo phalangeal Joint: Three Dimens ional Recons truction and Anatomic Analysis. Vet-\nerinary Radiolog y and Ultrasound . 1997; 38(3): 193–19 9. PMID: 9238790\n15. Bottcher P, Maierl J, Schieman n T, Glaser C, Weller R, Hoehne KH et al. The Visible Animal Project : A\nThree Dimens ional, Digital Databas e for High Quality Three Dimensional Recons truction. Veterinary\nRadiolog y and Ultrasound . 1999; 40(6): 611–61 6. PMID: 10608688\n16. Burdea GC, Coiffet P. Virtual Reality Technolo gy, second edition . 2003. Wiley–Inter science.\n17. Chien CH, Chen CH, Jeng TS. 2010 March. An interactive augmente d reality system for learning anat-\nomy structure. In Proceedings of the International MultiCon ference of Engineers and Comp uter Scien-\ntists (Vol. 1). International Association of Engineers\n18. Ma M, Fallavoll ita P, Seelbach I, Heide AM, Euler E, Waschk e J et al. Person alized augmente d reality\nfor anatomy education . Clinical Anatomy. 2016; 29: 446–45 3. https://doi.or g/10.1002 /ca.22675 PMID:\n26646315\n19. Zhu E., Hadadgar A., Masiello I. and Zary N., 2014. Augmen ted reality in healthcare education : an inte-\ngrative review. PeerJ, 2, p.e469. https://doi. org/10.7717/p eerj.469 PMID: 25071992\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 15 / 16\n20. Seymour NE, Gallagher AG, Roman SA, O’Brien MK, Bansal VK, Anders en D.K. et al. Virtual reality\ntraining improves operating room performance: results of a random ized, double-bli nded study. Annals\nof Surgery. 2002; 236(4): 458–46 4. https://doi.or g/10.109 7/01.SLA.000 002896 9.51489.B4 PMID:\n12368674\n21. Khot Z, Quinlan K, Norman GR, Wainman B. The relative effectiv eness of computer-bas ed and tradi-\ntional resource s for education in anatomy. Anatomica l Sciences Education. 2013; 6(4), pp.211–215\nhttps://doi.or g/10.100 2/ase.135 5 PMID: 23509000\n22. Barsom EZ, Graafland M, Schijven MP. Systematic review on the effectiv eness of augmented reality\napplicatio ns in medical training. Surgical Endoscop y. 2016; 30(10): 4174–4 183. https://doi.or g/10.\n1007/s00 464-016-4 800-6 PMID: 26905573\n23. Jamali SS, Shiratuddi n MF, Wong KW, Oskam CL. Utilising Mobile-Augm ented Reality for Learning\nHuman Anatomy. Proced ia-Social and Behavioral Sciences. 2015; 197: 659–668.\n24. Kamphu is C, Barsom E, Schijven M, Christoph N. Augmen ted reality in medical education ? Perspec-\ntives on Medical Education. 2014; 3(4): 300–31 1. https://doi.or g/10.100 7/s40037-013- 0107-7 PMID:\n24464832\n25. Baker DK, Fryberger CT, Ponce BA. The Emergence of Augmented Reality in Orthopaed ic Surgery\nand Educatio n. The Orthopaed ic Journal at Harvard Medical School; 2015: 8–16.\n26. Parkes R, Forrest N, Baillie S. A mixed reality simulator for feline abdomin al palpation training in veteri-\nnary medicine. Studies in Health Technolo gy and Informatics. 2009; 142:244–246. PMID: 19377159\n27. Boon JM, Meiring JH, Richards PA. Clinical Anatomy as the Basis for Clinical Examinati on: Develop-\nment and Evaluation of an Introdu ction to Clinical Examinati on in a Problem-O rientated Medical Curric-\nulum. Clinical Anatomy. 2002; 15: 45–50. https://d oi.org/10.100 2/ca.1091 PMID: 118355 44\n28. Turney BW. Anatomy in a Modern Medical Curriculu m. Annals of the Royal College of Surgeons of\nEngland. 2007; 89(2): 102–10 7.\n29. Jastrow H, Hollinde rbaumer A. On the Use and Value of New Media and How Medical Students Assess\nTheir Effectivenes s in Learning Anatomy. The Anatomica l Record: Part B, New Anatomist . 2004; 280\n(1): 20–29.\n30. Raffan H, Guevar J, Poyade M, Rea PM (2017) Canine neuroanatom y: Developm ent of a 3D recon-\nstruction and interactive application for undergrad uate veterinary education . PLoS ONE 12(2):\ne0168911. https:// doi.org/10.13 71/journal.p one.016 8911 https://doi.or g/10.137 1/journal.po ne.0168 911\nPMID: 281924 61\n31. Dale V. Educational methods and technolog ies in undergrad uate veterinary medicine. A case study of\nveterinary teaching and learning at Glasgo w, 1949–2 006. Thesis submitted for the degree of Doctor of\nPhilosophy in The Faculty of Veterinary Medicine, Univers ity of Glasgow, 2008.\n32. Ramos RL, Smith PT, Croll SD, Brumberg JC. Demons trating Cerebral Vascular Networks: A Compari-\nson of Methods for the Teaching Laboratory. Journal of Undergraduate Neurosc ience Educatio n. 2008;\n6(2): 53–59.\n33. Borden NM. 3D Angiograp hic Atlas of Neurovas cular Anatomy and Pathology , New York: Cambridge\nUniversity Press. 2007.\n34. Holmes MA, Nicholls PK. Compute r-aided veterina ry learning at the University of Cambridge . The Vet-\nerinary Record. 1996; 138(9): 199–203 . PMID: 8686151\n35. Linton A, Schoen feld-Tacher R, Whalen LR. Developing and Implemen ting an Assesment Method to\nEvaluate a Virtual Canine Anatomy Program. Journal of Veterinary Medical Education. 2005; 32(2):\n249–254. PMID: 16078179\n36. Clark RC and Mayer RE. e-Learnin g and the Science of Instruction: Proven Guideline s for Consumers\nand Designers of Multime dia Learning, Third Edition. 2011. John Wiley & Sons, Inc. 10.1002 /\n97811182559 71.\n37. Mayer RE. Comp uter Games for Learning: An Evidence -Based Approac h Hardcover – 8. MIT Press.\n2014. ISBN-10 : 0262027577.\nAR for veterina ry education\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.01958 66 April 26, 2018 16 / 16",
  "topic": "Workflow",
  "concepts": [
    {
      "name": "Workflow",
      "score": 0.753989577293396
    },
    {
      "name": "Curriculum",
      "score": 0.7054482698440552
    },
    {
      "name": "Augmented reality",
      "score": 0.6606301069259644
    },
    {
      "name": "Computer science",
      "score": 0.5581315755844116
    },
    {
      "name": "Human–computer interaction",
      "score": 0.39405500888824463
    },
    {
      "name": "Psychology",
      "score": 0.12039056420326233
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I119511950",
      "name": "Glasgow School of Art",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I7882870",
      "name": "University of Glasgow",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I137902535",
      "name": "North Carolina State University",
      "country": "US"
    }
  ],
  "cited_by": 28
}