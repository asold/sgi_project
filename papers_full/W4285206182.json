{
  "title": "Enhancing Chinese Pre-trained Language Model via Heterogeneous Linguistics Graph",
  "url": "https://openalex.org/W4285206182",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101424236",
      "name": "Yanzeng Li",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5109416747",
      "name": "Jiangxia Cao",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5100546036",
      "name": "Xin Cong",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5100389530",
      "name": "Zhenyu Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5101188063",
      "name": "Bowen Yu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5087012171",
      "name": "Hongsong Zhu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5103214505",
      "name": "Tingwen Liu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Information Engineering",
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2944852028",
    "https://openalex.org/W2962946486",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2876111955",
    "https://openalex.org/W3106031450",
    "https://openalex.org/W2889968917",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3035490055",
    "https://openalex.org/W2788474500",
    "https://openalex.org/W2954089321",
    "https://openalex.org/W2971871542",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3170962005",
    "https://openalex.org/W1840106123",
    "https://openalex.org/W3173220653",
    "https://openalex.org/W3090232624",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2970480111",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3035051781",
    "https://openalex.org/W2788824299",
    "https://openalex.org/W1662382123",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W1982498087",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2806081754",
    "https://openalex.org/W2252066972",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2963628345",
    "https://openalex.org/W2033295622",
    "https://openalex.org/W2912473624",
    "https://openalex.org/W2964321699"
  ],
  "abstract": "Yanzeng Li, Jiangxia Cao, Xin Cong, Zhenyu Zhang, Bowen Yu, Hongsong Zhu, Tingwen Liu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1986 - 1996\nMay 22-27, 2022c‚Éù2022 Association for Computational Linguistics\nEnhancing Chinese Pre-trained Language Model\nvia Heterogeneous Linguistics Graph\nYanzeng Li1‚àó, Jiangxia Cao2,3‚àó, Xin Cong2,3, Zhenyu Zhang2,3\nBowen Yu2,3, Hongsong Zhu2,3‚Ä†, Tingwen Liu2,3‚Ä†\n1Wangxuan Institute of Computer Technology, Peking University. Beijing, China\n2Institute of Information Engineering, Chinese Academy of Sciences. Beijing, China\n3School of Cyber Security, University of Chinese Academy of Sciences. Beijing, China\nliyanzeng@stu.pku.edu.cn\n{caojiangxia, congxin, zhangzhenyu1996}@iie.ac.cn\n{yubowen, zhuhongsong, liutingwen}@iie.ac.cn\nAbstract\nChinese pre-trained language models usually\nexploit contextual character information to\nlearn representations, while ignoring the lin-\nguistics knowledge, e.g., word and sentence\ninformation. Hence, we propose a task-free en-\nhancement module termed as Heterogeneous\nLinguistics Graph (HLG) to enhance Chinese\npre-trained language models by integrating\nlinguistics knowledge. SpeciÔ¨Åcally, we con-\nstruct a hierarchical heterogeneous graph to\nmodel the characteristics linguistics structure\nof Chinese language, and conduct a graph-\nbased method to summarize and concretize\ninformation on different granularities of Chi-\nnese linguistics hierarchies. Experimental re-\nsults demonstrate our model has the ability\nto improve the performance of vanilla BERT,\nBERTwwm and ERNIE 1.0 on 6 natural lan-\nguage processing tasks with 10 benchmark\ndatasets. Further, the detailed experimental\nanalyses have proven that this kind of mod-\nelization achieves more improvements com-\npared with previous strong baseline MWA.\nMeanwhile, our model introduces far fewer\nparameters (about half of MWA) and the\ntraining/inference speed is about 7x faster\nthan MWA. Our code and processed datasets\nare available at https://github.com/\nlsvih/HLG.\n1 Introduction\nPre-trained Language Models (PLM) (Peters et al.,\n2018; Devlin et al., 2019; Radford et al., 2018;\nYang et al., 2019) have recently demonstrated the\neffectiveness on a variety of natural language pro-\ncessing (NLP) tasks, such as machine translation\nand text summarization. For a speciÔ¨Åc downstream\ntask, the parameters of PLMs can be Ô¨Åne-tuned\n‚àóBoth authors contributed equally to this work\n‚Ä†Corresponding Author\nwith accurately labeled instances or weakly labeled\ninstances of the task to achieve better performance.\nIn recent, there are a series of studies on adapting\nPLMs for Chinese (Meng et al., 2019; Sun et al.,\n2019; Cui et al., 2019a; Sun et al., 2020; Wei et al.,\n2019; Diao et al., 2020; Lai et al., 2021). Many re-\nsearchers introduce the Chinese-speciÔ¨Åc linguistics\nknowledge such as word information into PLMs by\nconducting elaborate self-supervised tasks to pre-\ntrain Chinese PLMs from scratch. Nevertheless,\npre-training a PLM is computationally expensive\nand time-consuming since it needs large-scale Chi-\nnese corpus and heavy computational resources.\nThe high cost makes it difÔ¨Åcult for researchers to\npre-train a PLM from scratch.\nAn alternative way is to integrate the Chinese-\nspeciÔ¨Åc linguistics knowledge into pre-trained\nPLMs in the Ô¨Åne-tuning stage in downstream tasks\ndirectly. Following this idea, the task-free enhance-\nment module is widely used in the Ô¨Åne-tuning stage\nby adding an additional adapter in PLMs to in-\ntegrate external knowledge (Li et al., 2020). As\nshown in Figure 1, the enhancement module is in-\nserted between PLMs and task-speciÔ¨Åc module,\nand its inputs are the hidden representations of\nPLMs and embeddings of external knowledge. To\nachieve the goal of integrating external knowledge\ninto PLMs in the Ô¨Åne-tuning stage, the enhance-\nment module should have the following character-\nistics. First, as a plug-in adapter module in Ô¨Åne-\ntuning stage, it should maintain consistent output\nformulation with PLM. Second, it should not in-\ntroduce unacceptable time or space complexity for\ntraining and inference. Third, it should improve the\nperformance of downstream tasks universally.\nWith the core idea of the enhancement mod-\nule, Li et al. (2020) proposed a multi-source word-\naligned model (MWA) to enhance PLMs by inte-\ngrating Chinese Word Segmentation (CWS) bound-\n1986\n‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶\nPre-trained \nLanguage Model\n‚Ä¶‚Ä¶\nTask-Specific\nModel\nFine-tuning\n‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶\nPre-trained \nLanguage Model\n‚Ä¶‚Ä¶\nTask-Specific\nModel\nFine-tuning\nEnhancement Module\n‚Ä¶\nExternal\nKnowledge\nFigure 1: The diagram of Enhancement Module frame-\nwork. Left: Fine-tuning PLM ordinarily. Right: En-\nhancement for Fine-tuning PLM.\naries information implicitly. It Ô¨Årst exploits var-\nious CWS tools to generate multiple word se-\nquences and then utilizes word-aligned attention\nwith a mixed pooling to integrate the word infor-\nmation into characters. Experimental results show\nthat MWA has the ability to utilize CWS segmen-\ntation information to enhance Chinese PLMs to\nachieve SOTA performance in many downstream\nNLP tasks. However, MWA has two weaknesses:\n1) EfÔ¨Åciency Degradation: The model structure of\nMW A is naturally non-parallel and cannot beneÔ¨Åt\nfrom GPU acceleration (detailed in ¬ß4.3.3), which\nresults in time inefÔ¨Åciencies in both training and\ninference processes. 2) Linguistic Information\nLoss: MW A utilizes a pooling-based mechanism to\nperform interaction between characters and words.\nSuch a heuristic method could not make full use of\ninformation, resulting in sub-optimal results.\nTo tackle the aforementioned limitations, we pro-\npose Heterogeneous Linguistics Graph ( HLG),\nwhich is Graph Neural Network (GNN) based\nmethod to integrate CWS information to enhance\nPLMs. SpeciÔ¨Åcally, the hierarchical CWS informa-\ntion is Ô¨Årst conducted by a heterogeneous graph,\nwhich contains character nodes, word nodes and\nsentence nodes. The edge between nodes indicates\nthe inclusion relationship of the grammatical struc-\nture between the linguistic hierarchies. Then, a\nsimple but effective multi-step information propa-\ngation (MSIP) is proposed to incorporate the lin-\nguistics knowledge of heterogeneous graph to en-\nhance Chinese PLMs inductively. In this way, we\ncan obtain adequate information interaction among\ncharacters, words and sentences. Furthermore, the\ninternal implementation of HLG is highly paral-\nlelized, which is conducive to GPU accelerate and\nraises the operating efÔ¨Åciency.\nIn summary, we abstract out an adapter com-\nponent named enhancement module for PLMs to\nintegrate external knowledge during the Ô¨Åne-tuning\nstage. In this paradigm, we further introduce HLG\nto integrate CWS information delicately and model\nit via an effective MSIP. Extensive experiments\nconducted on 10 benchmark datasets of 6 NLP\ntasks demonstrate that our model outperforms the\nBERT, BERTwwm and ERNIE 1.0 signiÔ¨Åcantly\nand steadily. Comparing with MW A, a strong base-\nline that also incorporates CWS information to en-\nhance PLMs, our model achieves a steady improve-\nment with the same information. Meanwhile, com-\npared with previous work, MWA, our proposed\nHLG introduces only half additional parameters\nand the training/inference speed is about 7x faster.\n2 Preliminaries\n2.1 Pre-trained Language Model (PLM)\nAs mentioned in ¬ß1, the pre-trained language mod-\nels (PLMs) have achieved great success in many\nNLP applications with the 2-stage paradigm of pre-\ntraining and Ô¨Åne-tuning. The PLMs usually per-\nform pre-training on large-scale unlabeled corpus\nin virtue of self-supervised reconstruction tasks.\nFor example, BERT (Bidirectional Encoder Repre-\nsentations from Transformers) (Devlin et al., 2019)\nis a typical well-known PLM, which conducts\nmasked language modeling and next sentence pre-\ndiction as pre-training tasks. After completing the\npre-training, the PLMs learn substantial contextual-\nized text representations, and then adapt Ô¨Åne-tuning\non speciÔ¨Åc downstream tasks.\nIn Chinese NLP, PLMs are generally character-\nbased models (Li et al., 2019; Cui et al., 2019a).\nSpeciÔ¨Åcally, given a character sequence:\nS = [c1,c2,...,c n] (1)\nthe outputs of Chinese PLMs can be treated as the\ncharacter-level representations H ‚ààRn√ód, where\nthe dis the dimension of representation.\n2.2 Chinese Word Segmentation\nAs the same as most East-Asian languages, Chinese\nlanguage is written without explicit word delimiters\nand the character is the smallest morpheme unit in\nChinese linguistic (Cai and Zhao, 2016). Although\ncharacter-based models could achieve good per-\nformance (Li et al., 2019), Li et al. (2020) point\nout that introducing Chinese Word Segmentation\n1987\nÂåó ‰∫¨ Ë•ø Â±± Ê£Æ Êûó ÂÖ¨ Âõ≠[CLS] [SEP]Characters\nWords\nSentences\nÂ±±mountain\nÊ£ÆÊûó\nforest\nÂÖ¨Âõ≠\npark\nS2\nË•ø\nwest\nS3\nË•øÂ±±\nwestern mountain\nÊ£ÆÊûóÂÖ¨Âõ≠\nforestal park\n[SEP]Âåó‰∫¨\nBeijing[CLS]\nS1\nFigure 2: Overview of HLG structure. Different colored lines represent sentences formed by different CWS tools.\n(CWS) information to character-based models can\neffectively improve the model performance.\nWe give a formality deÔ¨Ånition of segmenter and\nits partition strategy œÄ. Given a sentence consisting\nof a sequence of characters as Eq. 1, a segmenter\nis deÔ¨Åned as:\nSEGMENTER ‚â°œÄ: S ‚ÜíS‚Ä≤\nwhere œÄis a partition strategy of sentence. SpeciÔ¨Å-\ncally, œÄpartition and group the character sequence\nSinto the word sequence S‚Ä≤:\nœÄ(S) = S‚Ä≤= [w1,w2,...,w m] (2)\nwhere m ‚â§ n and wi = [ cs,cs+1,...,c s+l‚àí1]\nis the i-th segmented word with a length of l\nand s is the index of wi‚Äôs Ô¨Årst character in S.\nNamely, the word wi is a sequence of characters\n{cs,cs+1,...,c s+l‚àí1}, and the sentence S‚Ä≤is a se-\nquence of words {w1,w2,...,w m}.\n2.3 MW A for Enhancing Chinese PLM\nLi et al. (2020) carried out researches on integrat-\ning CWS information into Chinese PLMs. The au-\nthors brought an architecture named Multi-source\nWord-aligned Attention (MWA) to incorporate\nmulti-granularity segmentation via pooling atten-\ntion weights among characters within the word.\nFormally, given a character sequence Sas Eq. 1\nand its partition strategy œÄas Eq. 2. The character-\nbased representation H could be gained via PLM,\nMW A conducted self-attention between characters:\nA = softmax\n((KWk)(QWq)T\n‚àö\nd\n)\nwhere Q and K are both H, dis deÔ¨Åned in ¬ß2.1,\nand A represents the attention score matrix. We\ndecompose A over columns as [a1,a2,..., an],\nand then perform partition œÄ on it: œÄ(A) =\n[{a1,a2},{a3},...{as\nc,..., as+l‚àí1\nc }...,{an‚àí1,an}]\nwhere s and l are deÔ¨Åned in ¬ß2.2. Pooling each\ngroup of partitioned columns:\nai\nw = MixPooling({as\nc,..., as+l‚àí1\nc })\nin which MixPooling is deÔ¨Åned in Yu et al.\n(2014). The gained Aw = [ a1\nw,a2\nw,..., am\nw ] ‚àà\nRn√óm is the character-to-word attention weight\nmatrix. After performing alignment-wise multi-\nply (Li et al., 2020) between character-to-word at-\ntention weight matrix Aw and the character-based\nrepresentation H, the enhanced character-based rep-\nresentation which integrates CWS information can\nbe obtained.\nIn essence, the MWA conducts interaction be-\ntween characters and words via character-to-word\nattention weight matrix Aw, implicitly summary\nthe information from characters, and performs Mix-\nPooling to aggregate the word-based segmentation\ninformation and concrete the character-level repre-\nsentation.\n3 Heterogeneous Linguistics Graph\nThis section introduces the components of our\nmodel HLG which instantiates the enhancement\nmodule by exploiting the CWS information. We\nÔ¨Årst brieÔ¨Çy explain the graph convolutional net-\nwork as our base encoder, and then describe the\ngraph construction of HLG. Finally, we give the\ndetails of the multi-step information propagation\n(MSIP) to integrate the CWS information into\nPLMs.\n3.1 Graph Convolutional Network\nGraph Convolutional Network (GCN) (Bruna et al.,\n2014; Kipf and Welling, 2017; Defferrard et al.,\n2016) is a powerful tool to extend the convolution\noperation from the grid data to irregular graph data.\n1988\nThe basic idea of GCN is to aggregate the represen-\ntations of neighbors to obtain better representation\nexpression of nodes in the graph. For instance, con-\nsider a homogeneous graphG= (V,E) constructed\nby nodes set Vand edges set E. A ‚ààR|V|√ó|V| is a\nbinary adjacency matrix where each element Aij\ndenotes whether node ihas an edge with node j\nin the edge set E. Formally, a standard GCN layer\ncan be abstracted as:\nHout = œÉ(ÀÜAHinW), ÀÜA = Norm(A) (3)\nwhere Hin denotes the input representation matrix,\nHout is the updated representation matrix, Norm(¬∑)\nmeans row normalizing function, ÀÜA is the normal-\nized adjacency matrix, œÉ(¬∑) is the ReLU function\nand W is a parameter matrix. After such convolu-\ntion operation, the representation Hin were aggre-\ngated rely on edge connections deÔ¨Åned by A, and\ntransformed into Hout by linear multiplication and\nactive function.\n3.2 Graph Construction\nWe build a heterogeneous graph G= (C,W,S,E)\nto model the structure of Chinese linguistic, where\nC, W, S, Edenote the character nodeset, word\nnodeset, sentence nodeset and edge set, respec-\ntively. Besides, different from homogeneous graph,\nHLG models relationship between three granulari-\nties of linguistic in a hierarchical way.\nAs presented in Figure 2, Gis composed of three\nhierarchies including characters, words and sen-\ntences. In this case, we employed three different\nCWS tools, and got three different segmentation re-\nsults, which resulted in three sentences with slightly\ndifferent semantics. Note that the same word seg-\nmentation results in the same position obtained\nby different CWS tools will be regarded as the\nsame word node to enhance the interaction (e.g.,\nBeijing and park in Figure 2). This purpose is to\ndenoise the mistake word nodes brought by seg-\nmenter error. If a word is segmented by multiple\nsegmenters at the same time, the corresponding\nword node will have a higher vertex degree. Such\nnodes with higher betweenness centrality will lead\nto a stronger inÔ¨Çuence on the followed information\npropagation and achieve the effect of denoising\nintuitively, like the vote-based multi-model ensem-\nble.\nIn HLG, only one adjacency matrix A is not\nenough to describe the hierarchical relationships be-\ntween characters, words and sentences. Hence, we\nùêÄ\"\t\n\"#$\nùêÄ\" \t\n$#% ùêÄ\"\t\n%#$\nùêÄ\"\t\n$#\"\nùêá! ùêáùíÑ!\n+\n+ùëê\nùë§\nùë†\nùë§‚Ä≤\nùëê‚Ä≤\nùêá#\nùêá$\nùêáùíò!\nFigure 3: Illustration of learning procedure of MSIP.\nThe colored circles denote characters, words or sen-\ntences representations. The green, orange and gray lines\ndescribe the summarization (Eq. 4), concretization (Eq.\n6) and skip connection (Eq. 7) operations, respectively.\nconduct two interaction matrices ¬ØAc2w ‚ààR|W|√ó|C|\nand ¬ØAw2s ‚ààR|S|√ó|W| to indicate aforementioned\nrelationships. To be speciÔ¨Åc, we take the ¬ØAc2w as\nan example (the one for ¬ØAw2s is analogous), the\nelement ¬ØAc2w\nij denotes whether word ihas an edge\nwith character jin the edge set E. Similar to Eq.\n3, we also denote normalized interaction matrices\nas ÀÜA\nc2w\nand ÀÜA\nw2s\n.\n3.3 Multi-Step Information Propagation\nTo model the granularities hierarchical relation-\nships in G, we devise a multi-step information prop-\nagation to learn the linguistics knowledge. In CWS,\nthe partition and group processes could be consid-\nered as the partition of semantic representation and\nthe aggregation of separated semantic respectively\n(detailed in ¬ß2.2). Inspired by CWS processes, we\nintroduce two operations into MSIP to simulate\nsuch processes and named as summarization and\nconcretization. Figure 3 shows the information\npropagation procedure of MSIP.\nSummarization. The summarization operation\nfocuses on generalizing hierarchical word and sen-\ntence representations (e.g., from character-level to\nword-level). SpeciÔ¨Åcally, given a heterogeneous\ngraph Gand corresponding character representa-\ntions Hc from PLM, the summarization operation\ncan be formulated as follows:\nHw = œÉ(ÀÜA\nc2w\nHcWc2w),\nHs = œÉ(ÀÜA\nw2s\nHwWw2s),\n(4)\nwhere the Wc2w, Ww2s are parameter matrices,\nHw, Hs are the interim representations of words\nand sentences.\n1989\nConcretization. Concretization is the inverse op-\neration of summarization, it is used to repartition\nthe semantics from high-level to low-level (e.g.\nfrom sentence-level to word-level). To do so, we\nÔ¨Årst calculate the normalized interaction matrices\nÀÜA\ns2w\nand ÀÜA\nw2c\n, which can be simply obtained\nby Ô¨Årst transposed then normalized the predeÔ¨Åned\ninteraction matrices ¬ØAw2s and ¬ØAc2w, respectively.\nThus, we have:\nÀÜA\ns2w\n= Norm\n(\n(¬ØAw2s)‚ä§)\n,\nÀÜA\nw2c\n= Norm\n(\n(¬ØAc2w)‚ä§)\n,\n(5)\nwhere (¬∑)‚ä§is the transpose function. Afterward,\nthe concretization operation is deÔ¨Åned as follows:\nH\nw‚Ä≤\n= œÉ(ÀÜA\ns2w\nHsWs2w),\nH\nc‚Ä≤\n= œÉ(ÀÜA\nw2c\nHw‚Ä≤\nWw2c),\n(6)\nwhere Ws2w and Ww2c are parameter matrices,\nH\nw‚Ä≤\nand H\nc‚Ä≤\nare also interim word and character\nrepresentations, Hw‚Ä≤\ndenote the Ô¨Ånal word repre-\nsentations deÔ¨Åned in Eq. 7.\nSkip Connection. Intuitively, it is difÔ¨Åcult to gen-\nerate satisÔ¨Åed low-level representations from the\nhigh-level representations directly. For example,\nit is easy to learn a few sentence representations\nfrom dozens of word representations, but hard to\ngenerate dozens of word representations from a few\nsentence representations.\nTo mitigate this problem, in this paper, we in-\ntroduce the skip connection to enhance the MSIP,\nwhich is to simulate the self-loop in vanilla GCN.\nAs shown in Figure 3, we add skip connections\nbetween the summarization representations and the\nconcretization representations directly. Formally,\nthe skip connection can be simply expressed as:\nHw‚Ä≤\n= H\nw‚Ä≤\n+ œÉ(HwWw),\nHc‚Ä≤\n= H\nc‚Ä≤\n+ œÉ(HcWc),\n(7)\nwhere Ww and Wc are parameter matrices. Fur-\nthermore, Hc‚Ä≤\ndenote the Ô¨Ånal representations for\ncharacters, which incorporates the Ô¨Åne-grained lin-\nguistics knowledge in G.\n4 Experiments\n4.1 Experimental Setting\nFor a fair comparison with MW A, which also gives\nan enhancement module by incorporating CWS in-\nformation. We conduct the same experiments on\nÔ¨Åve NLP tasks with various benchmark datasets.\nThree frequently-used Chinese PLMs: BERT (De-\nvlin et al., 2019), ERNIE 1.0 (Sun et al., 2019) and\nBERTwwm (Cui et al., 2019a) are employed as\nthe basic PLM to enhance. Three CWS tools: thu-\nlac (Sun et al., 2016a), ictclas (Zhang et al., 2003)\nand hanlp (He, 2014) are employed to gain the seg-\nmentation information. The time of pre-processing\nincluding applying CWS tools is ignored in the ex-\nperimental report. In the production, preprocessing\nand inference can be asynchronously executed in\nparallel (while inference a batch of data, the subse-\nquence data can be preprocessed with multiprocess)\n(Cheng et al., 2019), all three of the CWS tools\nwe‚Äôve introduced are fast enough to achieve this\neffect. According to rough estimates and technical\nreports, the processing speed of these tools are thu-\nlac 1221KB/s, ictclas 769KB/s, hanlp 1375KB/s,\nrespectively.\nSpeciÔ¨Åcally, we instantiate the enhancement\nmodule as HLG and incorporate with downstream\ntask-speciÔ¨Åc model. To verify the effectiveness of\nHLG, we execute 5 times Ô¨Åne-tuning on 10 bench-\nmark datasets of 6 NLP tasks and report the aver-\nage score. The tasks include Sentiment ClassiÔ¨Åca-\ntion (SC), Document ClassiÔ¨Åcation (DC), Named\nEntity Recognition (NER), Sentence Pair Match-\ning (SPM), Natural Language Inference (NLI) and\nMachine Reading Comprehension (MRC). SpeciÔ¨Å-\ncally, the following benchmark datasets are chosen\nto evaluate the performance: 1) SC: ChnSenti1 and\nweibo100k2 sentiment datasets are used for eval-\nuating the capacity of short text classiÔ¨Åcation. 2)\nDC: THUCNews (Sun et al., 2016b) dataset con-\ntains 10 types of news for performing long text\nclassiÔ¨Åcation. 3) NER: Ontonotes 4.0 (Weischedel\net al., 2011) and MSRA-NER (Levow, 2006a) are\nused for testing model in sequence tagging task. 4)\nSPM: LCQMC (Liu et al., 2018) and BQ (Chen\net al., 2018) are used to evaluate the text match-\ning ability of model. 5) NLI: We conduct experi-\nments on the Chinese part of XNLI (Conneau et al.,\n2018) dataset, and adopt the same pre-processing\nstrategy as ERNIE (Sun et al., 2019). 6) MRC:\nCommonly used datasets DRCD (Shao et al., 2018)\nand CMRC2018 (Cui et al., 2019b) are tested.\nCMRC2018 is only evaluated on dev set as same\nas (Wei et al., 2019; Sun et al., 2020).\n1https://github.com/pengming617/bert_\nclassification\n2https://github.com/SophonPlus/\nChineseNlpCorpus/\n1990\nSC NER SPM\nCHNSENTI WEIBO 100 K MSRA-NER ONTONOTES LCQMC BQ\nBERT 94.72 97.31 93.62 79.18 86.50 84.73\n+MW A 95.34(+0.62) 98.14(+0.83) 93.86(+0.24) 79.86(+0.68) 86.92(+0.42) 84.87(+0.14)\n+HLG 95.83(+1.11) 98.17(+0.86) 93.82(+0.20) 80.42(+1.24) 87.79(+1.29) 85.01(+0.28)\nBERTwwm 94.38 97.36 93.83 79.28 86.11 84.75\n+MW A 95.01(+0.63) 98.13(+0.77) 93.84(+0.01) 80.32(+1.04) 86.28(+0.17) 85.02(+0.27)\n+HLG 95.25(+0.87) 98.11(+0.75) 93.96(+0.13) 80.46(+1.18) 88.13(+2.02) 84.98(+0.23)\nERNIE 1.0 95.17 97.30 93.97 77.74 87.27 84.78\n+MW A 95.52(+0.35) 98.18(+0.88) 94.04(+0.07) 78.78(+1.04) 87.58(+0.31) 85.06(+0.28)\n+HLG 95.83(+0.66) 98.22(+0.92) 94.04(+0.07) 79.16(+1.42) 87.80(+0.53) 85.04(+0.26)\nDC NLI MRC\nTHUN EWS XNLI DRCD[EM | F1] CMRC2018[EM | F1]\nBERT 96.78 78.19 85.57 91.16 66.36 85.88\n+MW A 97.13(+0.35) 78.42(+0.23) 86.86(+1.29) 92.22(+1.06) 67.21(+0.85) 86.22(+0.34)\n+HLG 97.20(+0.42) 78.68(+0.49) 86.96(+1.39) 92.28(+1.12) 67.30(+0.94) 86.27(+0.39)\nBERTwwm 97.01 77.92 84.11 90.46 66.20 85.85\n+MW A 97.28(+0.27) 78.68(+0.76) 87.00(+2.89) 92.21(+1.75) 67.43(+1.23) 86.49(+0.64)\n+HLG 97.32(+0.31) 79.01(+1.09) 86.92(+2.81) 92.15(+1.69) 67.51(+1.31) 86.53(+0.68)\nERNIE 1.0 97.04 78.04 87.85 92.85 65.74 85.78\n+MW A 97.34(+0.30) 78.71(+0.67) 88.61(+0.76) 93.72(+0.87) 67.12(+1.38) 86.30(+0.52)\n+HLG 97.35(+0.31) 78.80(+0.76) 88.58(+0.73) 93.60(+0.75) 67.03(+1.29) 86.26(+0.48)\nTable 1: The experimental results on various datasets. All of the experiments except CMRC2018 are conducted\non test set, the reported values are F1 unless speciÔ¨Åed (EM means exact match score). We run each experiment\nwith a random seed for Ô¨Åve times and report the average score. Numbers in brackets indicate the relative increment\nbrought by enhancement module. The bold numbers mark the highest value within the same base-model.\nWe implement the presented approach in Py-\nTorch and Ô¨Åne-tune the downstream tasks on multi-\nple Nvidia Tesla V100 GPUs. The basic architec-\nture of PLMs and pre-trained parameters are pro-\nvided by Huggingface (Wolf et al., 2020). The ini-\ntial learning rate and other hyper-parameters refer\nto the previous works reported (Cui et al., 2019a;\nLi et al., 2020; Sun et al., 2020). Since the pa-\nrameters of PLMs have been optimized, while the\nparameters of HLG and the downstream tasks are\nuntrained. Hence, the learning rate of HLG part\nis larger than PLM part, we manually tuned the\nlearning rates of PLM and HLG separately.\n4.2 Experimental Results\nThe experimental results are shown in Table 1.\nOverall, we can observe that both HLG and MW A\noutperform baseline models (BERT, BERTwwm\nand ERNIE 1.0). Comparing with WMA, HLG\nachieves further improvement and signiÔ¨Åcantly out-\nperforms baseline models on 10 tasks. In detail,\nHLG outperforms MW A on ChnSent, weibo100k,\nMSRA-NER, ontonotes, LCQMC, BQ, THUC-\nNews and XNLI tasks, and obtains comparable\nresults on DRCD and CMRC2018 datasets.\nFor the text classiÔ¨Åcation tasks, namely SC\nand DC, HLG respectively achieves 0.88% and\n0.84% average improvement on ChnSenti and\nweibo100k dataset, while MWA gains 0.53% and\n0.82%. Meanwhile, HLG obtains 0.35% improve-\nment on the long text multi-classiÔ¨Åcation bench-\nmark THUCNews, and MW A gets 0.31% points.\nComparing with text classiÔ¨Åcation tasks, the\nimprovements over NER tasks are more obvious.\nThe main reason may be that CWS explicitly pro-\nvides the word boundaries, which are important\nto recognize entities accurately. On the ontonotes\ndataset, the promotion of HLG (1.28% averagely)\nis distinctly higher than that of MWA (0.92% av-\neragely). Compared to the strong baseline models,\nthe F1 scores of MSRA-NER have improved aver-\nage 0.13% and 0.10% by HLG and MW A, respec-\ntively.\nHLG achieves the best results on the text match-\ning tasks (SPM) and its variant NLI, which brings\n1.28% average improvement to LCQMC, 0.26%\naverage improvement to BQ, and 0.78% average\nimprovement to XNLI. The improvements of HLG\nare much higher than that of MWA (0.3%, 0.23%\nand 0.55%). As described in Chen et al. (2020) and\n1991\nNo. CWS tool Accumulative Word Count\nChnSenti weibo100k\n0 None 0 0\n1 thulac(Sun et al., 2016a) 69,877 398,046\n2 ictclas(Zhang et al., 2003) 78,695 452,059\n3 hanlp(He, 2014) 82,768 479,134\n4 pkuseg(Luo et al., 2019) 84,273 481,201\n5 jieba(Sun, 2013) 85,062 483,390\nTable 2: Adding the CWS tools one by one, and accu-\nmulate the total number of word nodes.\nLyu et al. (2021), text matching tasks can beneÔ¨Åt\nfrom the interaction between the paired sentences.\nHLG follows them to construct graphs over sen-\ntence pairs collectively, which naturally obtains\nadvantages in text matching tasks.\nFor MRC task, HLG and MWA achieve com-\nparable results on those datasets. HLG gets an\naverage improvement of 1.41 in EM and 0.85 in F1\nscore, while MW A gets 1.4 EM and 0.86 F1 score.\nHowever, HLG has dominant advantage in training\nspeed and inference speed. Detail analysis of time\nefÔ¨Åciency is in ¬ß4.3.3.\n4.3 Analyses\n4.3.1 Ablation Study\nWe conduct ablation experiments to explore the\neffectiveness of the number of CWS tools. The\nablation experiments are organized on sentiment\nclassiÔ¨Åcation task, ChnSenti and weibo100k dev\nset. As shown in Table 2, 5 popular CWS tools\nare added into our model successively according\nto the order, and we also show the total number\nof word nodes in our HLG. Meanwhile, the infor-\nmation from multiple word segmentation tools can\nbe integrated at the same time without increasing\nparameter size in HLG (only the A is changed).\nFigure 4 shows the performance of BERT+HLG\nwith different numbers of CWS tools on ChnSenti\nand weibo100k dev sets. Experimental results\ndemonstrate the effectiveness of introducing word\nsegmentation information. We can observe that\nwhen the number of CWS tools is larger, the num-\nber of generated word nodes gradually increasing\nto converge, and the performance of the model\nslightly is not always increasing as the word count.\nThe more CWS tools introduced will bring more\ndiversity but also bring noise caused by segmenter\nerror. In practice, we Ô¨Ånd using 4 or more CWS\ntools can slightly increase the performance but take\nmuch longer preprocessing time, hence we select\nthe elbow of the curve as the number of CWS tools.\nWeibo 100k\nWord Node Count Word Node Count\nF1 score (%) F1 score (%)\nChnSenti\nFigure 4: The histogram chart is the cumulative number\nof word nodes obtained by CWS tools, and the line\nchart is the performance of the model (BERT+HLG) in\ndev-set with the corresponding number of CWS tools.\nModel Params. (K = 3) F1\nBERT 110M 79.28\n+MW A 117.7M(+7.7M) 79.68(+0.40)\n+HLG 113.5M(+ 3.5M) 79.75(+0.47)\nBERTwwm 110M 79.32\n+MW A 117.7M(+7.7M) 79.77(+0.45)\n+HLG 113.5M(+ 3.5M) 80.16(+0.84)\nERNIE 1.0 110M 79.75\n+MW A 117.7M(+7.7M) 79.98(+0.23)\n+HLG 113.5M(+ 3.5M) 80.21(+0.46)\nTable 3: The amount of additional parameters and per-\nformance improvement of MW A and HLG.\nThat is, using 3 as the number of CWS tools might\nbe a balance between the performance of model\nand the cost of preprocessing. This number also\ncoincides with the conÔ¨Åguration in MW A.\n4.3.2 Parameter-EfÔ¨Åcient Analysis\nIn general, the enhancement module should be able\nto bring performance improvements without unac-\nceptable space complexity. Therefore, we conduct\na comparative experiment on XNLI dev set to ex-\nplore the performance improvement and the space\noverhead between MW A and HLG.\nTo be speciÔ¨Åc, the number of parameters in\nMWA depends on the dimension of PLM‚Äôs rep-\nresentation and the number of CWS tools K. Con-\ncretely, MWA containsK transformer layers and\n1 aggregation layer. Nevertheless, our HLG only\ndepends on the dimension of PLM‚Äôs representation\nand simply contains 4 basic GCN layers and 2 skip\nconnections. Thus, the number of parameters of\n1992\nModel Params. F1\nBERTwwm-base 110M 79.32\n+HLG(random tokenizer) 113.5M 79.16(-0.16)\n+HLG(character tokenizer) 113.5M 79.41(+0.09)\n+HLG(thulac) 113.5M 79.68(+0.36)\n+HLG(ictlas) 113.5M 79.91(+0.59)\n+HLG(hanlp) 113.5M 79.81(+0.49)\n+HLG(thulac+ictlas+hanlp) 113.5M 80.16(+0.84)\nTable 4: The performance comparison between random\ntokenizer, character tokenizer that segments each char-\nacter into a single word, and sole segmenters.\nthem can be calculated as:\nsize(MW A) = K√ó(4 √ód2)Transformer + d2\nsize(HLG) = (4 √ód2)GCN + 2 √ód2\nAs discussed before, we employ 3 CWS tools\nin both MWA and HLG. Table 3 reports the per-\nformance of BERT, BERTwwm, and ERNIE 1.0\non the XNLI dev set. Obviously, HLG can get a\ngreater performance improvement with only half\nadditional parameters. It shows that as an enhance-\nment module, HLG is superior to MWA in terms\nof parameter utilization efÔ¨Åciency.\nIn addition, to verify the impact of the addi-\ntional parameters, we also conduct an ablation\nexperiment on XNLI dev set that utilizes the ran-\ndom tokenizer, the single-character tokenizer, and\nsole segmenter to obtain the different word seg-\nmentation results, and send those results to HLG to\neliminate the additional beneÔ¨Åt from the change of\nneural network structure and the increase of param-\neters. The results are shown in Table 4, which indi-\ncates that the increment of parameters can slightly\naffect character-based model performance, and the\nCWS information is signiÔ¨Åcantly useful to promote\nthe performance of character-based PLM.\n4.3.3 Time EfÔ¨Åciency Analysis\nTime efÔ¨Åciency is an important indicator in the\nreal-world production. Less training time and infer-\nence time means lower costs. In order to analyze\nthe additional time cost of different enhancement\nmodules, we conduct comparative experiments\namong BERT, BERT+MW A, and BERT+HLG on\nChnSenti, LCQMC and XNLI datasets. For the\nfair comparison, we remain other hyper-parameters\nconsistent for the three models.\nAs shown in Figure 5, we compare time cost\nduring training and inference between vanilla\nBERT, BERT+MWA and BERT+HLG. We can\nobserve that the training time and inference time of\nFigure 5: The training time, inference time of vanilla\nBERT, BERT+MWA and BERT+HLG on ChnSenti,\nLCQMC and XNLI benchmarks. All of these time dose\nnot include CWS process.\nBERT+HLG are basically consistent with vanilla\nBERT. However, when MWA is introduced, the\naverage training time increases by 7 times, and\nthe average inference time increases by 7.6 times.\nThis is because MW A must calculate aligned atten-\ntion weights token by token, and it cannot beneÔ¨Åt\nfrom CUDNN parallelization, resulting in terrible\noperating efÔ¨Åciency. On the contrary, HLG is com-\nposed of GCNs, and its internal implementation\nis basically the simplest non-linear transformation.\nTherefore, HLG could be maximally accelerated\nthrough the optimized matrix operation of CUDNN\nprimitive, which only produces a negligible impact\non time efÔ¨Åciency.\n5 Related Works\nPre-training language models, such as ELMo (Pe-\nters et al., 2018), BERT (Devlin et al., 2019), XL-\nNET (Yang et al., 2019) and GPT (Radford et al.,\n2018), have shown their powerful performances on\nvarious natural language processing tasks and have\nbeen applied in many applications.\nIn recent past, there are studies adapting PLMs\nfor Chinese with Chinese-speciÔ¨Åc features such as\nword information. Glyce (Meng et al., 2019) pro-\nposed to use the glyph information of Chinese char-\nacters to enhance PLMs. ERNIE 1.0/2.0 (Sun et al.,\n2019, 2020) and BERTwwm (Cui et al., 2019a)\nused the whole word mask to learn the structure\nof words or entities in the pre-training stage and\nconducted more and better pre-training tasks to per-\nceive large-scale data. NEZHA (Wei et al., 2019)\nused a series of methods such as functional relative\n1993\npositional encoding and whole word masking to\nimprove the pre-training tasks, which had brought\nimprovement. ZEN (Diao et al., 2020) adopted\nn-gram masking to enhance pre-trained encoder\nand obtained outstanding performance. Lattice-\nBERT (Lai et al., 2021) introduced word lattice in-\nformation (Zhang and Yang, 2018) into pre-training\nframework via lattice position attention.\nAs a fundamental feature of Chinese, word seg-\nmentation information is Ô¨Çexibility, granularity,\nand easy-to-get (Sproat and Emerson, 2003; Levow,\n2006b). Further, Zhang et al. (2018); Li et al. (2019,\n2020) conducted detailed research and experiments\non the application of CWS in deep learning, and\nfound that CWS information can effectively im-\nprove the performance of Chinese character-based\nPLMs.\nRecently, a lot of works have been proposed to\nprompt NLP applications by constructing graph on\ntext and modeling with graph neural networks. Yao\net al. (2019) Ô¨Årst constructed word co-occurrence\ngraph between documents and introduced GCN to\nmodeling and aggregating document representa-\ntion for text classiÔ¨Åcation. Chen et al. (2020); Lyu\net al. (2021) constructed lattice graph to maintain\nmulti-granularity information and external knowl-\nedge in Chinese short text matching task. Nguyen\nand Grishman (2018) proposed performing GCN\nover dependency trees to extract event trigger. Sui\net al. (2019) conducted a character-word interaction\ngraph and performed graph attention network on\nit to recognize Chinese named entities. Shu et al.\n(2020) introduced a bipartite-graph based trans-\nformer PLM for integrating hierarchical semantic\ninformation.\n6 Conclusion\nIn this paper, we propose HLG which acts as the en-\nhancement module to enhance Chinese PLMs with\nCWS information. The HLG Ô¨Årstly constructs het-\nerogeneous graph based on multiple word segmen-\ntations to model the hierarchy of Chinese. Then,\nthe MSIP is proposed to model the Ô¨Åne-grained\nlinguistics knowledge of the heterogeneous graph.\nExperimental results on 6 NLP tasks with 10 bench-\nmark datasets demonstrate that the performance of\nour model outperforms previous work, MW A. Be-\nsides the performance improvements, HLG intro-\nduces only half the additional parameters of MW A\nand its training/inference speed is 7x faster than\nMW A. At present, the experimental results of HLG\nare lagging behind SOTA, and we will try to mi-\ngrate it to some of the latest PLMs. Besides, HLG\nhas the expansibility to introduce the representation\nlayer of the CWS model directly, or introduce some\nother information sources such as the knowledge\ngraph, etc. We leave these further improvements to\nthe future.\nAcknowledgement\nThis work is supported by the National Key Re-\nsearch and Development Program of China (grant\nNo.2021YFB3100600 and 2020YFB2103803), the\nStrategic Priority Research Program of Chinese\nAcademy of Sciences (grant No.XDC02040400),\nthe Youth Innovation Promotion Association of\nCAS (Grant No. 2021153) and National Natural\nScience Foundation of China (Grant No.61902394).\nThis work performed while the Ô¨Årst author was at\nIIE, CAS.\nReferences\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and\nYann LeCun. 2014. Spectral networks and locally\nconnected networks on graphs. In 2nd International\nConference on Learning Representations, ICLR 2014,\nBanff, AB, Canada, April 14-16, 2014, Conference\nTrack Proceedings.\nDeng Cai and Hai Zhao. 2016. Neural word segmenta-\ntion learning for Chinese. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 409‚Äì420,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nJing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe\nLu, and Buzhou Tang. 2018. The BQ corpus: A large-\nscale domain-speciÔ¨Åc Chinese corpus for sentence\nsemantic equivalence identiÔ¨Åcation. In Proceedings\nof the 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4946‚Äì4951, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nLu Chen, Yanbin Zhao, Boer Lyu, Lesheng Jin, Zhi\nChen, Su Zhu, and Kai Yu. 2020. Neural graph\nmatching networks for Chinese short text matching.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 6152‚Äì\n6158, Online. Association for Computational Lin-\nguistics.\nYang Cheng, Dan Li, Zhiyuan Guo, Binyao Jiang, Ji-\naxin Lin, Xi Fan, Jinkun Geng, Xinyi Yu, Wei Bai,\nLei Qu, et al. 2019. Dlbooster: Boosting end-to-end\ndeep learning workÔ¨Çows with ofÔ¨Çoading data prepro-\ncessing pipelines. In Proceedings of the 48th Inter-\nnational Conference on Parallel Processing, pages\n1‚Äì11.\n1994\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing\nYang, Shijin Wang, and Guoping Hu. 2019a. Pre-\ntraining with whole word masking for chinese bert.\narXiv preprint arXiv:1906.08101.\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng\nChen, Wentao Ma, Shijin Wang, and Guoping Hu.\n2019b. A span-extraction dataset for Chinese ma-\nchine reading comprehension. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5883‚Äì5889, Hong Kong,\nChina. Association for Computational Linguistics.\nMicha√´l Defferrard, Xavier Bresson, and Pierre Van-\ndergheynst. 2016. Convolutional neural networks on\ngraphs with fast localized spectral Ô¨Åltering. In Ad-\nvances in Neural Information Processing Systems 29:\nAnnual Conference on Neural Information Process-\ning Systems 2016, December 5-10, 2016, Barcelona,\nSpain, pages 3837‚Äì3845.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2020. ZEN: Pre-training Chinese\ntext encoder enhanced by n-gram representations.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 4729‚Äì4740, Online.\nAssociation for Computational Linguistics.\nHan He. 2014. HanLP: Han Language Processing.\nGitHub Repository.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classiÔ¨Åcation with graph convolutional\nnetworks. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings .\nOpenReview.net.\nYuxuan Lai, Yijia Liu, Yansong Feng, Songfang Huang,\nand Dongyan Zhao. 2021. Lattice-BERT: Leverag-\ning multi-granularity representations in Chinese pre-\ntrained language models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1716‚Äì1731, Online.\nAssociation for Computational Linguistics.\nGina-Anne Levow. 2006a. The third international Chi-\nnese language processing bakeoff: Word segmenta-\ntion and named entity recognition. In Proceedings of\nthe Fifth SIGHAN Workshop on Chinese Language\nProcessing, pages 108‚Äì117, Sydney, Australia. Asso-\nciation for Computational Linguistics.\nGina-Anne Levow. 2006b. The third international Chi-\nnese language processing bakeoff: Word segmenta-\ntion and named entity recognition. In Proceedings of\nthe Fifth SIGHAN Workshop on Chinese Language\nProcessing, pages 108‚Äì117, Sydney, Australia. Asso-\nciation for Computational Linguistics.\nXiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han,\nArianna Yuan, and Jiwei Li. 2019. Is word segmen-\ntation necessary for deep learning of Chinese repre-\nsentations? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3242‚Äì3252, Florence, Italy. Association for\nComputational Linguistics.\nYanzeng Li, Bowen Yu, Xue Mengge, and Tingwen Liu.\n2020. Enhancing pre-trained Chinese character rep-\nresentation with word-aligned attention. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 3442‚Äì3448, On-\nline. Association for Computational Linguistics.\nXin Liu, Qingcai Chen, Chong Deng, Huajun Zeng,\nJing Chen, Dongfang Li, and Buzhou Tang. 2018.\nLCQMC:a large-scale Chinese question matching\ncorpus. In Proceedings of the 27th International Con-\nference on Computational Linguistics, pages 1952‚Äì\n1962, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nRuixuan Luo, Jingjing Xu, Yi Zhang, Xuancheng Ren,\nand Xu Sun. 2019. Pkuseg: A toolkit for multi-\ndomain chinese word segmentation. arXiv preprint\narXiv:1906.11455.\nBoer Lyu, Lu Chen, Su Zhu, and Kai Yu. 2021. Let:\nLinguistic knowledge enhanced graph transformer\nfor chinese short text matching. In Proceedings of\nthe AAAI Conference on ArtiÔ¨Åcial Intelligence, vol-\nume 35, pages 13498‚Äì13506.\nYuxian Meng, Wei Wu, Fei Wang, Xiaoya Li, Ping Nie,\nFan Yin, Muyu Li, Qinghong Han, Xiaofei Sun, and\nJiwei Li. 2019. Glyce: Glyph-vectors for chinese\ncharacter representations. In Advances in Neural\nInformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 2742‚Äì2753.\nThien Huu Nguyen and Ralph Grishman. 2018. Graph\nconvolutional networks with argument-aware pool-\ning for event detection. In Proceedings of the Thirty-\nSecond AAAI Conference on ArtiÔ¨Åcial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nÔ¨Åcial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in ArtiÔ¨Åcial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 5900‚Äì5907. AAAI Press.\n1995\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227‚Äì2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning.\nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng,\nand Sam Tsai. 2018. Drcd: a chinese machine\nreading comprehension dataset. arXiv preprint\narXiv:1806.00920.\nXiaobo Shu, Mengge Xue, Yanzeng Li, Zhenyu Zhang,\nand Tingwen Liu. 2020. Big-transformer: Integrat-\ning hierarchical features for transformer via bipartite\ngraph. In 2020 International Joint Conference on\nNeural Networks (IJCNN), pages 1‚Äì8.\nRichard Sproat and Thomas Emerson. 2003. The Ô¨Årst\ninternational Chinese word segmentation bakeoff. In\nProceedings of the Second SIGHAN Workshop on\nChinese Language Processing, pages 133‚Äì143, Sap-\nporo, Japan. Association for Computational Linguis-\ntics.\nDianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, and\nShengping Liu. 2019. Leverage lexical knowledge\nfor Chinese named entity recognition via collabora-\ntive graph network. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 3830‚Äì3840, Hong Kong, China. As-\nsociation for Computational Linguistics.\nJ Sun. 2013. Jieba chinese word segmentation tool.\nGitHub Repository.\nMaosong Sun, Xinxiong Chen, Kaixu Zhang, Zhipeng\nGuo, and Zhiyuan Liu. 2016a. Thulac: An efÔ¨Åcient\nlexical analyzer for chinese. Technical report.\nMaosong Sun, Jingyang Li, Zhipeng Guo, Z Yu,\nY Zheng, X Si, and Z Liu. 2016b. Thuctc: an ef-\nÔ¨Åcient chinese text classiÔ¨Åer. GitHub Repository.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0: A\ncontinual pre-training framework for language under-\nstanding. In Proceedings of the AAAI Conference on\nArtiÔ¨Åcial Intelligence, volume 34, pages 8968‚Äì8975.\nJunqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong\nHuang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin\nJiang, Xiao Chen, and Qun Liu. 2019. Nezha: Neural\ncontextualized representation for chinese language\nunderstanding. arXiv preprint arXiv:1909.00204.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2011. Ontonotes 4.0. Linguistic\nData Consortium LDC2011T03.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38‚Äì45, Online. Association\nfor Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754‚Äì5764.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nGraph convolutional networks for text classiÔ¨Åcation.\nIn Proceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, volume 33, pages 7370‚Äì7377.\nDingjun Yu, Hanli Wang, Peiqiu Chen, and Zhihua\nWei. 2014. Mixed pooling for convolutional neural\nnetworks. In International Conference on Rough\nSets and Knowledge Technology , pages 364‚Äì375.\nSpringer.\nHua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun\nLiu. 2003. HHMM-based Chinese lexical analyzer\nICTCLAS. In Proceedings of the Second SIGHAN\nWorkshop on Chinese Language Processing, pages\n184‚Äì187, Sapporo, Japan. Association for Computa-\ntional Linguistics.\nQi Zhang, Xiaoyu Liu, and Jinlan Fu. 2018. Neural\nnetworks incorporating dictionaries for chinese word\nsegmentation. In Proceedings of the Thirty-Second\nAAAI Conference on ArtiÔ¨Åcial Intelligence, (AAAI-\n18), New Orleans, Louisiana, USA, February 2-7,\n2018, pages 5682‚Äì5689. AAAI Press.\nYue Zhang and Jie Yang. 2018. Chinese NER using\nlattice LSTM. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1554‚Äì1564,\nMelbourne, Australia. Association for Computational\nLinguistics.\n1996",
  "topic": "Zh√†ng",
  "concepts": [
    {
      "name": "Zh√†ng",
      "score": 0.6663816571235657
    },
    {
      "name": "Computer science",
      "score": 0.6037006974220276
    },
    {
      "name": "Computational linguistics",
      "score": 0.5275777578353882
    },
    {
      "name": "Linguistics",
      "score": 0.4373430609703064
    },
    {
      "name": "Natural language processing",
      "score": 0.4317779242992401
    },
    {
      "name": "Graph theory",
      "score": 0.41840702295303345
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4112769067287445
    },
    {
      "name": "Philosophy",
      "score": 0.2009429931640625
    },
    {
      "name": "Mathematics",
      "score": 0.1944459080696106
    },
    {
      "name": "China",
      "score": 0.1572040319442749
    },
    {
      "name": "History",
      "score": 0.14886242151260376
    },
    {
      "name": "Combinatorics",
      "score": 0.12994074821472168
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}