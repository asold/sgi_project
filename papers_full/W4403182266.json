{
    "title": "Prompting Video-Language Foundation Models With Domain-Specific Fine-Grained Heuristics for Video Question Answering",
    "url": "https://openalex.org/W4403182266",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5101456347",
            "name": "Ting Yu",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Hangzhou Dianzi University",
                "Hangzhou Normal University",
                "Institute of Computing Technology",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5101731463",
            "name": "K. S. Fu",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Hangzhou Dianzi University",
                "Hangzhou Normal University",
                "Institute of Computing Technology",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5100669242",
            "name": "Shuhui Wang",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Hangzhou Dianzi University",
                "Hangzhou Normal University",
                "Institute of Computing Technology",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5028597017",
            "name": "Qingming Huang",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Hangzhou Dianzi University",
                "Hangzhou Normal University",
                "Institute of Computing Technology",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5050817770",
            "name": "Jun Yu",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Hangzhou Dianzi University",
                "Hangzhou Normal University",
                "Institute of Computing Technology",
                "University of Chinese Academy of Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6850625674",
        "https://openalex.org/W6847076894",
        "https://openalex.org/W6811340617",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W4385570262",
        "https://openalex.org/W6843369699",
        "https://openalex.org/W6851592950",
        "https://openalex.org/W6849177959",
        "https://openalex.org/W6858543224",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W6790978476",
        "https://openalex.org/W6848903804",
        "https://openalex.org/W4387211390",
        "https://openalex.org/W4387211684",
        "https://openalex.org/W6853313673",
        "https://openalex.org/W4312246181",
        "https://openalex.org/W4386075754",
        "https://openalex.org/W4386071468",
        "https://openalex.org/W4377235294",
        "https://openalex.org/W6811013733",
        "https://openalex.org/W4386065353",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W4386076539",
        "https://openalex.org/W6798805250",
        "https://openalex.org/W4386902900",
        "https://openalex.org/W3016658915",
        "https://openalex.org/W3119243803",
        "https://openalex.org/W4390873290",
        "https://openalex.org/W6843018836",
        "https://openalex.org/W3168640669",
        "https://openalex.org/W4313186260",
        "https://openalex.org/W6804277322",
        "https://openalex.org/W6852908585",
        "https://openalex.org/W3204588463",
        "https://openalex.org/W3023993913",
        "https://openalex.org/W2765716052",
        "https://openalex.org/W2904452845",
        "https://openalex.org/W2997805943",
        "https://openalex.org/W4313163028",
        "https://openalex.org/W2962949233",
        "https://openalex.org/W2954199749",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2998166190",
        "https://openalex.org/W3167092180",
        "https://openalex.org/W4312974690",
        "https://openalex.org/W4312339522",
        "https://openalex.org/W4399399458",
        "https://openalex.org/W4362653417",
        "https://openalex.org/W4395069639",
        "https://openalex.org/W6852932568",
        "https://openalex.org/W4389518831",
        "https://openalex.org/W4384917104",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W6790019176",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W3035265375",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3198377975",
        "https://openalex.org/W4312310776",
        "https://openalex.org/W4321021726",
        "https://openalex.org/W4389776363",
        "https://openalex.org/W4312563428",
        "https://openalex.org/W3175961224",
        "https://openalex.org/W3166608351",
        "https://openalex.org/W4313071966",
        "https://openalex.org/W4404782290",
        "https://openalex.org/W3034730770",
        "https://openalex.org/W4383112678",
        "https://openalex.org/W6850037782",
        "https://openalex.org/W4390873732",
        "https://openalex.org/W2974161034",
        "https://openalex.org/W2951161814",
        "https://openalex.org/W2982139784",
        "https://openalex.org/W3027790991",
        "https://openalex.org/W3181758331",
        "https://openalex.org/W4390872809",
        "https://openalex.org/W2963541336",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W4312651322",
        "https://openalex.org/W3197457832",
        "https://openalex.org/W6852580281",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4319165322",
        "https://openalex.org/W4388843160",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4318718936",
        "https://openalex.org/W4379958249",
        "https://openalex.org/W4366330503",
        "https://openalex.org/W4377130772",
        "https://openalex.org/W3213562172",
        "https://openalex.org/W4392224191",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W4376312579",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4296151206",
        "https://openalex.org/W4385571046",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4385965953",
        "https://openalex.org/W4318620664",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W4391710035",
        "https://openalex.org/W4378505261"
    ],
    "abstract": "Video Question Answering (VideoQA) represents a crucial intersection between\\nvideo understanding and language processing, requiring both discriminative\\nunimodal comprehension and sophisticated cross-modal interaction for accurate\\ninference. Despite advancements in multi-modal pre-trained models and\\nvideo-language foundation models, these systems often struggle with\\ndomain-specific VideoQA due to their generalized pre-training objectives.\\nAddressing this gap necessitates bridging the divide between broad cross-modal\\nknowledge and the specific inference demands of VideoQA tasks. To this end, we\\nintroduce HeurVidQA, a framework that leverages domain-specific entity-action\\nheuristics to refine pre-trained video-language foundation models. Our approach\\ntreats these models as implicit knowledge engines, employing domain-specific\\nentity-action prompters to direct the model's focus toward precise cues that\\nenhance reasoning. By delivering fine-grained heuristics, we improve the\\nmodel's ability to identify and interpret key entities and actions, thereby\\nenhancing its reasoning capabilities. Extensive evaluations across multiple\\nVideoQA datasets demonstrate that our method significantly outperforms existing\\nmodels, underscoring the importance of integrating domain-specific knowledge\\ninto video-language models for more accurate and context-aware VideoQA.\\n",
    "full_text": "1\nPrompting Video-Language Foundation Models\nwith Domain-specific Fine-grained Heuristics for\nVideo Question Answering\nTing Yu, Member, IEEE, Kunhao Fu, Shuhui Wang, Member, IEEE, Qingming Huang, Fellow, IEEE,\nJun Yu, Senior Member, IEEE\nAbstract‚ÄîVideo Question Answering (VideoQA) represents a\ncrucial intersection between video understanding and language\nprocessing, requiring both discriminative unimodal comprehen-\nsion and sophisticated cross-modal interaction for accurate in-\nference. Despite advancements in multi-modal pre-trained mod-\nels and video-language foundation models, these systems often\nstruggle with domain-specific VideoQA due to their generalized\npre-training objectives. Addressing this gap necessitates bridging\nthe divide between broad cross-modal knowledge and the specific\ninference demands of VideoQA tasks. To this end, we introduce\nHeurVidQA, a framework that leverages domain-specific entity-\naction heuristics to refine pre-trained video-language foundation\nmodels. Our approach treats these models as implicit knowledge\nengines, employing domain-specific entity-action prompters to\ndirect the model‚Äôs focus toward precise cues that enhance\nreasoning. By delivering fine-grained heuristics, we improve the\nmodel‚Äôs ability to identify and interpret key entities and actions,\nthereby enhancing its reasoning capabilities. Extensive evalua-\ntions across multiple VideoQA datasets demonstrate that our\nmethod significantly outperforms existing models, underscoring\nthe importance of integrating domain-specific knowledge into\nvideo-language models for more accurate and context-aware\nVideoQA.\nIndex Terms‚Äîvideo question answering, discriminative uni-\nmodal comprehension, cross-modal interaction, domain-specific\nheuristics, video-language foundation models, entity-action rela-\ntionships, context-aware reasoning.\nI. I NTRODUCTION\nT\nHE evolution of large-scale text pre-training has sig-\nnificantly advanced the capabilities of Large Language\nModels (LLMs) [1], [2], [3] such as GPT-3 [4] and BERT [5],\nempowering them with an unprecedented ability to compre-\nhend and process complex linguistic structures. These break-\nthroughs have paved the way for sophisticated natural language\nprocessing applications [6], [7]. Building on the success of\nThis work was supported by National Natural Science Foundation of\nChina under Grant No. 62002314 and Zhejiang Provincial Natural Science\nFoundation of China under Grant No. LY23F020005.\nT. Yu and K. Fu are with the School of Information Science and Tech-\nnology, Hangzhou Normal University, Hangzhou 311121, China (e-mail:\nyut@hznu.edu.cn; fukunhao@stu.hznu.edu.cn).\nS. Wang is with the Key Laboratory of Intelligent Information Processing,\nInstitute of Computing Technology, Chinese Academy of Sciences, Beijing\n100190, China (e-mail: wangshuhui@ict.ac.cn).\nQ. Huang is with the School of Computer Science and Technology,\nUniversity of Chinese Academy of Sciences, Beijing 101408, China (e-mail:\nqmhuang@ucas.ac.cn).\nJ. Yu is with the Key Laboratory of Complex Systems Modeling and\nSimulation, School of Computer Science and Technology, Hangzhou Dianzi\nUniversity, Hangzhou 310018, China (email: yujun@hdu.edu.cn).\nAction-Entity Heuristic Generation\nHeuristicBoostingAnswer Inference\nA\n(Q)How did the bride react when the lady in grey hugged her?\n(V)\n(H)<Action -[verb],  0.78><Entity -[noun],  0.38>‚Ä¶\n[CLS] A video of  [noun][CLS] A video of  [verb]‚Ä¶\nA(Q) VFMs\n(V)\nVFMsActions\nEntities\n(Q)(V)\nEAPrompter\n(P)(V) (He)(Ha) (P)\nQAHead\nEAPrompter\nVFMs\nQAReasoner\n(1)\n(2)\nFig. 1. Comparison of Recent Advanced VideoQA Models with the Proposed\nHeurVidQA Framework: (1) Recent methods utilize VFMs to generate gen-\neral, domain-agnostic representations, complemented by task-specific heads\nfor question-answering. (2) Our HeurVidQA framework enhances VFMs by\nintegrating domain-specific entity-action prompts, blending implicit general\nknowledge with fine-grained domain-specific insights, to improve cross-modal\nunderstanding and reasoning in VideoQA tasks.\nLLMs, the development of Large Multi-modality Pre-trained\nModels (LMMs) [8], [9], [10], [11], exemplified by models\nlike CLIP [12] and DALL-E [13], represents a significant\nleap forward by integrating textual and visual data during pre-\ntraining. This fusion has driven remarkable progress in fields\nrequiring deep cross-modal understanding, including robotics\n[14], medical diagnostics [15], [16], and interactive gaming\nenvironments [17].\nVideo Question Answering (VideoQA) [18], [19], [20], [21]\nstands out as a particularly challenging yet rapidly growing\nfield within vision-language bridging research [22], [23]. It\ndemands both discriminative unimodal understanding [5], [24],\n[25] and comprehensive cross-modal interaction [12], [22],\n[26], [27], integrating video understanding with language\nprocessing to infer reliable answers. While traditional ap-\nproaches have explored enhanced video-linguistic models [28],\n[29] and adaptive cross-modal interactions [30], the advent\nof Video Language Foundation Models (VFMs) [22], [31],\n[32], such as ALPRO [33] and SiaSamRea [34], has brought\nabout a paradigm shift. These models, pre-trained on extensive\nvideo-text datasets [35], [36], have been pivotal in equipping\nmachines with broad cross-modal knowledge, setting new\nstandards for video content interpretation and advancing the\nboundaries of VideoQA [18], [19], [20].\narXiv:2410.09380v1  [cs.CV]  12 Oct 2024\n2\nDespite the advanced capabilities of VFMs, their general-\nized pre-training often underperforms in specialized VideoQA\ndomains. This gap underscores the need for precise, context-\naware adaptations to fully harness these models. Integrating\nprompt engineering with VFMs offers a promising solution\nby providing tailored prompts that steer the models toward\ndomain-specific challenges, ensuring more accurate outcomes.\nHowever, the success of prompt-based approaches heavily\nrelies on the design and quality of the prompts themselves.\nThis is particularly true in VideoQA, where understanding the\ninterplay between entities and their actions within videos is\nparamount. Generic prompts may not suffice for tasks that\ndemand deep, context-specific insights, underscoring the need\nfor a more refined strategy.\nIn this study, we introduce HeurVidQA, a novel framework\ndesigned to enhance video-language foundation models with\ndomain-specific entity-action heuristics to address the complex\ndemands of VideoQA. These complex demands arise from the\nneed to process diverse and intricate video content, requiring\nnot only an understanding of temporal sequences and spatial\nrelationships but also the ability to handle varied question\ntypes that demand nuanced, context-aware reasoning. Our\napproach transforms these large-scale models into dynamic\nknowledge engines capable of navigating such intricate content\nwith high precision and adaptability. We employ a strategy\nthat integrates domain-specific, fine-grained heuristics into\nthe prompt design, refining focus on essential cross-modal\nelements. As depicted in Figure 1, we utilize instantiated\nprompt templates and specific regions within video frames to\nenable the prompter to accurately identify dynamic actions\nthat vary spatially and evolve entities over time. This strategic\nuse of heuristic-based prompts, informed by a deep under-\nstanding of key entities and their interactions, sharpens the\nmodel‚Äôs focus and enhances analytical depth. Subsequently,\nleveraging these finely designed, context-aware entity-action\nheuristics, we prompt VFMs to generate coherent and con-\ntextually aligned responses.HeurVidQA underscores the crit-\nicality of recognizing key entities and actions for effective\nvideo content analysis. By ensuring the model‚Äôs focus aligns\nwith the question intricacies specific to a video, we enable a\nmore contextually informed and impactful analysis, yielding\nanswers that are accurate and deeply contextualized.\nIn summary, our contributions are threefold:\n‚Ä¢ We introduce HeurVidQA, a framework that enhances\nvideo-language foundation models for VideoQA by in-\ntegrating general implicit priors with domain-specific,\nfine-grained knowledge, thereby improving cross-modal\nunderstanding and reasoning capabilities.\n‚Ä¢ We develop a domain-specific prompting mechanism\nutilizing fine-grained entity-action heuristics to guide the\nmodel in accurately identifying and interpreting dynamic\nactions and evolving entities within videos.\n‚Ä¢ We validate the effectiveness of our approach through\ncomprehensive evaluations on four VideoQA datasets\nacross different domains, demonstrating robust and gen-\neralizable performance compared to existing methods.\nII. R ELATED WORK\nA. Video Question Answering\nVideo Question Answering (VideoQA) has emerged as a\ncritical area in cross-modal research, driven by significant\nadvancements in vision-language integration. Early efforts in\nthis domain focused on adapting ImageQA models to video\ncontexts, primarily leveraging LSTM-based encoders [37],\n[38]. However, these approaches were limited by their reliance\non unimodal understanding, often failing to capture the in-\nherent temporal dynamics of video content. This shortcoming\nunderscores the necessity for advancing research towards\ncross-modal understanding approaches. To address this, atten-\ntion mechanisms [39], [40] have been investigated to extract\nlinguistically guided visual features. Further advancements\ninclude the development of hierarchical attention mechanisms\n[38], [41], co-memory networks [42], and heterogeneous\nmemory-enhanced models [43], which have proven effective\nin modeling the complex interplay between video and question\nfeatures. The advent of the Transformer model [44] brought\na revolutionary shift in natural language processing (NLP)\nand found application in VideoQA by Li et al. [39]. This\nadvancement significantly enhanced unimodal understanding\nwithin VideoQA. However, the visual modality‚Äôs limitations\nnecessitated the development of novel cross-modal frame-\nworks. Leveraging self-attention and co-attention mechanisms,\nTransformers facilitated deeper cross-modal interaction, en-\nabling the extraction of pertinent visual features. Addition-\nally, structured representations, including heterogeneous graph\nalignment networks [45], structure-aware interaction models\n[46], and Dynamic Graph Transformers [18], further advanced\nreasoning capabilities in VideoQA. Recent studies in VideoQA\nhave also tackled challenges related to confounders and com-\npositional reasoning. Grounding indicators [47] have been\ndeveloped to mitigate spurious correlations, while question de-\ncomposition engines [48] offer valuable insights into composi-\ntional reasoning. To address confounding factors, multimodal\ncausal reasoning frameworks [19] have been introduced, along\nwith adaptive spatial-temporal attention mechanisms [20]. As\nresearch into multi-granularity advances, emerging approaches\nincreasingly explore the impact of varying granularities on\nVideoQA performance [49], [50], [51], [52], focusing on\nmulti-level intra- and inter-granularity relations to enhance\ncross-modal comprehension. Another approach [47], [53] to\nsolving VideoQA tasks focuses on identifying video frames\nrelevant to the question, drawing inspiration from similar\ntext-to-video retrieval (TVR) tasks. For example, Wu et al.\n[54] explored both text-free and text-guided frame selection\nstrategies in TVR, determining an optimal strategy that bal-\nances accuracy and computational efficiency. In the context of\nmulti-modal pre-training on large-scale vision-text data [55],\n[22], [9], [33], [26], [56], [12], Transformer-based models\nhave demonstrated remarkable advancements. While prompt\nlearning and engineering [12] have enhanced pre-training, the\ncustomization of prompts specifically tailored to entities and\nactions in videos remains underexplored. This paper addresses\nthis gap by constructing task-specific prompts finely tuned to\nentities and actions, thereby enabling superior spatiotemporal\n3\nreasoning in VideoQA. Our novel approach enriches video\ncontent understanding, pushing the boundaries of VideoQA\nperformance. In this paper, we introduce a novel heuristic\nknowledge engine, EAPropmter, designed to extract detailed\naction and entity information embedded in videos. By lever-\naging diverse cropping strategies and customized prompt tem-\nplates, EAPropmter significantly elevates the model‚Äôs ability to\nidentify and interpret fine-grained information. This innovative\ndata-centric approach offers a groundbreaking solution to\nVideoQA, paving the way for further exploration in this field.\nB. Vision-and-Language Pre-training\nThe paradigm of pre-training on large-scale visual-language\ndata followed by fine-tuning for specific downstream tasks has\ndemonstrated significant success in cross-modal applications,\nincluding VideoQA. Vision-and-Language Pre-trained (VLP)\nmodels [12], [57], such as CLIP [12] and ALIGN [57], com-\nmonly use standardized pre-training objectives like masked\nlanguage modeling (MLM) and image-text matching (ITM) on\nextensive vision-text datasets. However, earlier VLP models,\nsuch as OSCAR [56], depended heavily on pre-trained object\ndetectors for visual feature extraction. This reliance led to\nincreased computational overhead and introduced noise into\nthe image-text data, which ultimately impacted downstream\nperformance. Despite their strong performance across various\ntasks, these VLP models face two significant limitations: first,\nthey often struggle to effectively model complex vision-text\ninteractions, and second, they tend to overfit to the noisy data\nprevalent in large-scale image-text datasets, which ultimately\nhampers their generalization performance. To overcome these\nlimitations, various strategies have been proposed. For in-\nstance, Li et al. [26] introduced an intermediate image-text\ncontrastive loss to enhance cross-modal alignment and seman-\ntic understanding while mitigating the effects of noisy data\nusing Momentum Distillation. Similarly, BLIP [22] advanced\nthe quality of vision-text pairs through the Captioning and Fil-\ntering (CapFilt) approach, which fine-tunes a captioner using\nlanguage modeling and a filter with cross-modal tasks, thereby\nimproving the model‚Äôs handling of vision-text relationships.\nIn the domain of video processing, VideoBERT [58] extended\nthe BERT model to video analysis but neglected the crucial\nroles of textual cues and cross-modal interactions. Meanwhile,\nActBERT [59] depended on a pre-trained object detector\nto generate object pseudo-labels, but its limited detection\ncategories and high computational costs resulted in suboptimal\nperformance. Moving beyond one-directional transfer models,\nunified transformer-based architectures, as proposed by Wang\net al. [31], have emerged to handle multimodal input sources,\nenabling joint pre-training for image-language and video-\nlanguage tasks, thus benefiting both image and video-related\nassignments. More recently, with the advancement of prompt\nlearning, VLP models have benefited from the integration of\ncarefully designed prompts, which have significantly enhanced\ntheir performance in downstream tasks. This approach allows\nfor greater control and precision, enabling models to produce\ntask-specific outcomes more effectively. The success of prompt\nlearning lies in its ability to address challenges such as limited\ndata and complex annotations, positioning it as a promising\ndirection for advancing cross-modal research. By incorporat-\ning these techniques, researchers have achieved impressive\nresults, further narrowing the gap between vision and language\nunderstanding. Our proposed HeurVidQA framework marks a\nsignificant departure by incorporating novel visual heuristics\nthat enhance the VLP model with fine-grained perceptual capa-\nbilities. Leveraging the robust cross-modal learning strengths\nof these models, our approach not only improves their ability\nto capture intricate visual nuances but also bridges the gap\nbetween pre-training and downstream task execution. This\nintegration of new visual cues within HeurVidQA represents a\npivotal advancement in refining and advancing state-of-the-art\nvisual language processing.\nC. Prompt Learning\nPrompt learning [4], [60], [61] has emerged as a powerful\ntechnique in the cross-modal field, allowing models to achieve\ntask-specific results by incorporating targeted prompts during\ntraining. This approach enhances control and accuracy, provid-\ning a promising solution to challenges posed by limited data\nand complex annotations. At its core, prompt learning can be\nlikened to a ‚Äùfill-in-the-blank‚Äù task for pre-training models. By\nmasking certain words in sentences and prompting the model\nto predict the missing tokens, the model learns to comprehend\ncontextual relationships between words in a sentence. CLIP\n[12], in particular, showcases the efficacy of prompt-based\nlearning for image recognition. Through contrastive learning\non a large-scale noisy dataset, it aligns relevant image-text\npairs and distinguishes unrelated ones, effectively achieving\nimage-text alignment. For image recognition, it incorporates\nthe label into a descriptive sentence, enabling the model to\npredict specific words (e.g., ‚ÄúAn image about a [MASK]‚Äù).\nRemarkably, CLIP achieves impressive accuracy in image\nrecognition tasks even without fine-tuning, thanks to the\nflexibility of prompt design, making it applicable to various\nimage-text tasks. Building on the success of CLIP, ALIGN\n[57] further advances vision-language models by training on\na massive dataset comprising 1.8 billion image-text pairs.\nALIGN surpasses CLIP‚Äôs predictive performance, achieving\nsuperior results. Meanwhile, CoOp [62] introduces a prompt\nfine-tuning method, adapting NLP techniques to the cross-\nmodal domain. By employing learnable tokens as prompts\nand minimizing classification losses, CoOp effectively learns\nSoft Prompts with minimal annotated data, overcoming the\nlimitations of manually tuned prompts. This innovation yields\nsignificant performance improvements and greater flexibility.\nHowever, CoOp‚Äôs learned context struggles to generalize to\nunseen classes, a challenge addressed by CoCoOp [63]. Co-\nCoOp generates input-conditional vectors via a lightweight\nnetwork, dynamically adjusting prompts to improve general-\nization across varying classes. This method enhances perfor-\nmance on unseen classes, solidifying CoCoOp‚Äôs robustness.\nAdditionally, Ma et al. [64] tackles the overfitting issue\nin CoOp by projecting gradients onto a low-rank subspace\nduring back-propagation. Exploring prompt techniques further,\nresearchers have focused on enhancing tasks through verb\n4\nDataEngineering HeuristicBoosting Answer InferenceEntity-Action HeuristicsGenerationPerform0.02\nKiss0.13\nHug0.78\nL TAM\nL SEM\nLPred\nVFMs‚Ä¶\n‚Ä¶\nùë£!\"#\nùë°!\"#$\nùë£!\"#\nùë°!\"#$\nActionPrompter\nEntityPrompter Ceremony0.12\nBride0.38\nWedding0.48\nActionHeuristics\nTemporalcrop&Verb prompts\nSpatial crop&NounPrompts\n[CLS] A video of  [verb1].[CLS] A video of  [verb2].[CLS] A video of  [verb3].\n[CLS] A video of  [noun1].[CLS] A video of  [noun2].[CLS] A video of  [noun3].‚Ä¶\n‚Ä¶\nVFMs EntityHeuristics\nVFMs\nQ: How did the bride react when the lady in grey hugged her?A: Return the hug\nRaw video&QAlabel\nQAReasoner\nùëé!\nùëí\"\nùí±\nùí¨\nTemporalLandscape\nSpatialLandscape\nFig. 2. Overview of the Proposed HeurVidQA Framework. The framework consists of two primary stages: Entity-Action Heuristic Generation and Heuristic-\nEnhanced Answer Inference. In the first stage, EAPrompter extracts action heuristics through temporal-aware action detection and entity heuristics via\nspatial-aware entity detection using spatial-temporal-aware crops and instantiated prompt templates. These heuristics, sourced from a predefined vocabulary,\ninclude confidence scores for each detected action and entity. The second stage utilizes a Vision-Language Foundation Model (VFM) to process multimodal\nfeatures through self-attention and cross-attention modules, resulting in a semantically enriched cross-modal fusion that supports both final answer prediction\nand anticipation of actions and entities in the video.\nand noun prompts. For instance, Li et al. [65] introduce\nCSA-Net, leveraging the RegionCLIP [66] model to create\nfine-grained regional semantic visual spaces, which enhances\ncontent comprehension. While prompt learning has been pre-\ndominantly explored in image-based tasks, Li et al. [33] extend\nthese innovations to video models, using a pre-trained entity\nprompter to generate pseudo-labels, thereby achieving notable\nimprovements across various benchmarks. However, video\ncontent goes beyond just entities; it crucially involves actions\nthat convey vital information. Recognizing this, our approach\nleverages both entities and actions in the video to construct\nheuristics, empowering our model with enhanced temporal\nand spatial reasoning capabilities for video-based question\nanswering. Incorporating these fine-grained details has led to\nstate-of-the-art performance on the challenging NExT-QA [67]\ndataset, demonstrating the effectiveness of our approach in\nadvancing video question answering tasks within the cross-\nmodal domain.\nIII. M ETHOD\nThe overall architecture of our HeurVidQA is illustrated in\nFigure 2. In our approach, we seek to accurately predict the\nanswer y to a given question Q, posed in natural language,\nbased on the content of a raw video V. The following equation\nformalizes this objective:\nÀÜy = argmax\ny‚ààA\nFW (y|V, Q, A), (1)\nwhere FW represents the VideoQA model parameterized by\nweights W. The function‚Äôs output, ÀÜy, varies based on the\nVideoQA task configuration, offering an open-ended response\nfrom a global answer set or a selected choice from multiple\nalternatives. For multiple-choice QA, we establish a linkage\nbetween the question and candidate answers with a [PAD]\ntoken, supplemented by the inclusion of a specialized [CLS]\ntoken positioned at the inception of the textual sequence:\nT MC (Q, ai|ai ‚àà AMC ) = [CLS]Q [PAD] ai, (2)\nwhere T MC is the conversion function that converts multiple-\nchoise question Q and its candidate answers ai to the input\nformat. For open-ended QA, where no predefined candidate\nanswers exist, the [CLS] token is directly associated with\nthe commencement of the question. The formulation of the\nconversion function is as follows:\nT OE(Q) = [CLS]Q. (3)\nOur model is designed to be adaptable across these varied\nformats, enabling comprehensive applicability within the field\nof VideoQA.\nIn the following, we outline the workflow of the proposed\nHeurVidQA framework, organized into three key phases. The\nData Engineering phase establishes the foundation by op-\ntimizing visual inputs for the EAPrompter through strategic\nclipping and crafting video entity-action templates. Next, the\nEntity-Action Heuristics Generation subsection introduces\nthe EAPrompter‚Äôs architecture and training, focusing on syn-\nthesizing heuristic insights from the processed data. Finally,\nthe Heuristic Boosting Answer Inference phase utilizes these\nheuristics to enable synergistic interaction between VFMs and\nthe knowledge engine, enhancing answer inference perfor-\nmance.\nA. Data Engineering\nOur HeurVidQA framework begins by preparing videos\nand prompts for analysis. We segment the videos temporally\nand spatially, targeting key moments and areas likely to hold\nrelevant information. Simultaneously, we generate domain-\nspecific prompts using action-related verbs and object-focused\nnouns anticipated in the video content. These tailored prompts\nguide the foundation models in identifying critical elements.\nThis phase is crucial for data preparation, ensuring the sub-\nsequent heuristic generation stages are well-informed and\neffective. To enhance both temporal and spatial sensitivity in\nvideo cropping, we employ distinct strategies tailored to each\ndimension. Temporal sensitivity is addressed by consistently\n5\nMulti-Granularity Contrastive Learning\nVideo Encoder\nText Encoder\nùëâ!\"#$ùëâ!\"#%‚Ä¶ùëâ!\"#&\nùëá!\"#$\nùëá!\"#%‚Ä¶ùëá!\"#&\nThe fish moved its body when it was pulled onto the shore by the man in white. \nVideo Data\nText Data ùëá!\"#$V!\"#$¬∑\n‚Ä¶\nùëá!\"#$V%¬∑ ùëá!\"#$V&¬∑ùëá!\"#%V!\"#$¬∑ ùëá!\"#%V!\"#%¬∑ ùëá!\"#%V!\"#&¬∑\nùëá!\"#&V!\"#$¬∑ ùëá!\"#&V!\"#%¬∑ ùëá!\"#&V!\"#&¬∑\n‚Ä¶‚Ä¶\n‚Ä¶\n‚Ä¶‚Ä¶\n‚Ä¶\nùë£!\"#\nùë°!\"#\nFig. 3. Training Framework of EAPrompter. The Symmetric Contrastive Loss\nis employed to jointly train a video encoder and a text encoder, facilitating\nthe prediction of correct pairings within batches of {video, text} samples.\ncropping identical spatial regions across keyframes, ensuring\nfocused capture of evolving actions. For spatial sensitivity,\nwe extract varied regions across different frames, providing a\ncomprehensive spatial understanding of the entities involved.\nThe resulting video segments are denoted as ÀÜV t for temporally\ncropped segments and ÀÜV s for spatially cropped segments. Ad-\nditionally, the textual component is refined through a prompt\nengineering process. We employ pre-defined prompt templates\n(detailed in Table VIII) for action and entity identification,\nsuch as ‚ÄúA video of [NOUN]‚Äù and ‚Äú A video of a [VERB].‚Äù\nThese placeholders are systematically replaced with verbs\nand nouns extracted from the top 1,000 most frequent terms\nfound in domain-specific QA pairs. This approach ensures\nthat the prompts are highly relevant to the domain, enhancing\nthe model‚Äôs ability to generate precise and context-aware\nresponses. This preprocessing stage primes our model by\nequipping it with fine-grained visual cues and aligning it with\ncontextually relevant textual prompts, laying a robust founda-\ntion for the extraction of meaningful entity-action heuristics.\nB. Entity-Action Heuristics Generation\nAfter data engineering, our HeurVidQA framework pro-\ngresses to the crucial stage of generating domain-specific\nfine-grained heuristics. This phase employs a two-pronged\napproach: (1) Action Heuristics Generation focuses on captur-\ning verbs and their temporal dynamics within video content,\nenabling the model to comprehend ongoing activities; (2)\nEntity Heuristics Generation targets the identification and\ntracking of nouns and their spatial attributes, ensuring the\naccurate reference of key subjects within the visual space. This\ndual approach ensures that both temporal and spatial aspects\nare effectively captured, enhancing the model‚Äôs capacity for\nprecise video question answering.\n1) EntityActionPrompter: The core of our heuristic gen-\neration lies within the EntityActionPrompter (EAPrompter),\nwhich operates through two sub-modules: the action prompter\n(AP) and the entity prompter (EP), each tailored to capture\ndistinct yet complementary aspects of the video content‚Äôs\nentity-action landscape. The AP enhances temporal precision\nby randomly cropping identical spatial regions across sparsely\nselected video frames, while the EP augments spatial acuity\nby extracting varied regions across different frames. Both\nmodules share an identical architecture, adopting standard clip-\nbased visual-language model structures with a video branch\nand a text branch. For the video branch, both AP and EP\nemploy a 12-layer TimeSformer 224 [24] to capture video\nembeddings. It takes as input a clip V ‚àà RC√óF√óH√óW , which\nundergoes random sampling, scaling, cropping, and random\nmasking to enhance computational efficiency and consistent\nperformance.\nTo facilitate the formula expression, the unimodal\nvisual embeddings derived from AP are denoted as\n{vt\ncls, vt\n1, . . . , vt\nNtv\n}, where vt\ncls and Nt\nv corresponds to video\nglobal temporal embedding and temporal cropping numbers\nrespectively. Visual embeddings from EP are denoted as\n{vs\ncls, vs\n1, . . . , vs\nNsv\n}, where vs\ncls and Ns\nv are video global\nspatial embedding and spatial cropping numbers. For the text\nbranch, we employ a multi-layer bidirectional transformer [5]\nto encode text semantics in a hierarchical collaborative pars-\ning pattern. This procedure produces corresponding prompts\nembeddings, {ta1\ncls, ta2\ncls, . . . , taM\ncls } and {te1\ncls, te2\ncls, . . . , teN\ncls} for\naction and entity prompts, respectively. For cross-modality\nalignment, a similarity function s(¬∑) is optimized between the\nvideo and textual embeddings as follows:\ns(vcls, tcls) =fv(vcls) ¬∑ ft(tcls), (4)\nwhere a higher similarity score is obtained for matching videos\nand prompts.\nBoth AP and EP are pre-trained on webly sourced video-\ntext pairs with video-text contrastive (VTC) loss. As depicted\nin Figure 3, the symmetric contrastive loss plays a key role\nin enhancing cross-modal alignment. By pulling positive pairs\ncloser (on the diagonal) and pushing negative pairs further\napart (off-diagonal), this mechanism drives the encoder rep-\nresentations to converge in a low-dimensional space, thereby\noptimizing inter-modal alignment. The symmetric contrastive\nloss for the vision-to-text modality is calculated as follows:\nLv2t = ‚àí\nBX\ni=1\nlog exp(s(vi\ncls, ti\ncls)/œÑ)PB\nj=1 exp(s(vi\ncls, tj\ncls)/œÑ)\n. (5)\nThe symmetric contrastive loss for the text-to-vison contrastive\nloss is computed as:\nLt2v = ‚àí\nBX\ni=1\nlog exp(s(ti\ncls, vi\ncls)/œÑ)PB\nj=1 exp(s(ti\ncls, vj\ncls, )/œÑ)\n, (6)\nwhere B represents the batch size and œÑ signifies a learnable\ntemperature parameter, the prompter, after being pre-trained\non extensive datasets, exhibits strong alignment capabilities\nfor video and textual data. To prevent external noise from\naffecting its performance, we freeze the prompter‚Äôs parameters\nduring the entity-action heuristic generation stage. Through\nthis approach, the EAPrompter effectively constructs a detailed\nentity-action map of the video content, providing a heuristic-\ndriven analysis that greatly enhances the accuracy and rele-\nvance of the subsequent answer inference stage.\n2) Heuristic Generation: In the Heuristic Generation sub-\nsection, we introduce a key innovation in our approach: the\nutilization of latent heuristic information embedded within\n6\nTemporalActionModelingActionHeuristic Generation\nA video of  perform.[CLS]\nA footage of  kiss.[CLS]\nA video of  hugging.[CLS]\n..\nPerform  0.03Kiss0.02..Hugging0.82\n..\nL TAM\nPerform  0.13Kiss      0.18..Hugging  0.62\nHeuristic Labels\nùë£!\"#$\nVideo-language Foundation Models\nùë°!\"#%‚Ä¶\nFig. 4. Operational Principle of Action Prompter. The Action Prompter\nprocesses instantiated templates alongside sparsely selected video frames,\nwhich have been refined through a space-time cropping strategy.\nvideo frames for VideoQA tasks. This involves generating\nheuristic labels through the action prompter module, as de-\npicted in Figure 4. The action prompter processes instan-\ntiated templates alongside carefully selected video frames,\napplying a precise space-time cropping strategy to capture\nrich action information. By leveraging the knowledge gained\nduring pre-training, the prompter aligns these templates with\nthe corresponding frames, resulting in heuristic labels charac-\nterized by detailed probability distributions. Specifically, the\naction heuristics can be derived by calculating the softmax-\nnormalized similarity between the temporal embeddings of the\nvideo, denoted as ÀÜvt\ncls, and the action prompt embeddings,\nrepresented as tam\ncls\nM\nm=1. This formulation is mathematically\nexpressed as follows:\nhÀÜvt,am = exp(s(ÀÜvt\ncls, tam\ncls )/œÑ)\nPM\nm=1 exp(s(ÀÜvt\ncls, tam\ncls )/œÑ)\n. (7)\nThe entity heuristics, produced by calculating the softmax-\nnormalized similarity between the spatial embeddings of the\nvideo ÀÜvs\ncls and all the entity prompt embeddings {ten\ncls}N\nn=1,\ncan be formulated as follows:\nhÀÜvs,en = exp(s(ÀÜvs\ncls, ten\ncls)/œÑ)\nPN\nn=1 exp(s(ÀÜvs\ncls, ten\ncls)/œÑ)\n. (8)\nFigure 5 illustrates several examples of the entity-action\nheuristics generated by our approach. Our approach differs\nfrom traditional object detection methods that require sub-\nstantial computing power for object identification. our method\nleverages targeted heuristic generation, by efficiently extract-\ning relevant entity and action information across temporal and\nspatial dimensions, reducing the computational burden while\nmaintaining high precision in VideoQA tasks.\nC. Heuristic Boosting Answer Inference\nHeuristic boosting answer inference stage leverages heuris-\ntic labels from the previous phase to strengthen interactions\nbetween VFMs and the heuristic knowledge engine, refining\nthe model‚Äôs ability to infer answers accurately. HeurVidQA\nutilizes VFMs to capture contextual video-language embed-\ndings, followed by a QA reasoner to conduct question-\nanswering interaction and communication. The QA reasoner\nstacks cross-modality alignment layers to fuse visual-textual\nembeddings to explore in-depth, informative clues. Specifi-\ncally, it comprises several self-attention blocks, cross-attention\nlayers, and feed-forward networks to deliver cross-modal fu-\nsions {ecls, e1, . . . , eNv+Nt}, where ecls serves as a pivotal\nlink between the EAPrompter and the QA Reasoner. Instead\nof directly utilizing the fused embedding ecls to predict\nthe answer, HeurVidQA skillfully introduces the generated\ndomain-specific entity-action heuristics as supervisory targets\nwith Temporal Action Modeling (TAM) and Spatial Entity\nModeling (SEM) losses. TAM Loss enhances the model‚Äôs\nability to understand temporal action sequences, while SEM\nLoss improves the recognition of spatial relationships among\nentities. These loss functions are pivotal in transferring the\nlatent knowledge from the prompters to the baseline model,\nguiding the VFMs toward more precise reasoning in VideoQA\ntasks. Through two distinct classifiers c1(¬∑) and c2(¬∑), we\ninitially obtained the normalized confidence scores of action\nand entity heuristics:\npÀÜa,am = c1(ecls), p ÀÜe,en = c2(ecls), (9)\nwhere pÀÜa,am and pÀÜe,en denote the classifier-derived probability\ndistributions for action and entity heuristics, respectively. The\nsubscripts am and en correspond to the mth action and nth\nentity within the respective distributions.\nThe TAM loss is calculated as the cross-entropy between\nthe classifier-derived action heuristic distribution pÀÜa and the\ntarget distribution hÀÜvcls,am:\nLTAM = ‚àí\nMX\nm=1\nhÀÜvcls,am ¬∑ log pÀÜa,am. (10)\nSimilarly, the SEM loss is calculated as the cross-entropy\nbetween the classifier-derived entity heuristic distribution pÀÜe\nand the target distribution hÀÜvcls,en:\nLSEM = ‚àí\nNX\nn=1\nhÀÜvcls,en ¬∑ log pÀÜe,en. (11)\nGuided by the supervision of entity-action heuristics, the\nfusion embedding retrieves the unimodal information that\nmight have been lost during cross-modal interaction, enabling\nHeurVidQA to achieve fine-grained spatiotemporal reasoning.\nTo predict the final answer ÀÜy, we use an independent classifier,\nwhich takes the heuristic-enhanced fusion representation as\ninput, formulated as follows:\nÀÜy = c3(ecls). (12)\nComparing the predictive answers ÀÜy with the ground truth y,\nwe calculate the prediction loss as follows:\nLpred = ‚àí\nX\ny log ÀÜy. (13)\n7\nThe complete training objective of HeurVidQA can be ob-\ntained by combining the prediction loss with the heuristic\nlosses as follows:\nL = Lpred + Œ±LTAM + (1‚àí Œ±)LSEM . (14)\nWhere Œ± represents a hyperparameter set to 0.5 in our exper-\niments, it serves to balance the weighting between action and\nentity prompts. To explore other configurations, we introduced\na dynamic gating mechanism that adjusts the influence of\naction and entity prompts based on the input data and training\nstage. This question-guided gating mechanism involves the\nfollowing steps: processing the text input via VFMs to extract\nthe global tcls embedding and passing this embedding through\na gating network composed of two multi-layer perceptrons, a\ndropout function, and a sigmoid activation function.\ng(tcls) = Sigmoid(MLP2(Dropout(MLP1(tcls)))). (15)\nThrough the gating mechanism, we obtain a scalar value\nranging from 0 to 1, representing the gate weight for the\naction prompt. Next, the gate weight for the entity prompt\nis calculated by subtracting the action prompt‚Äôs weight from\n1. Finally, these gate weights are applied to the TAM loss and\nSEM loss. The combined loss function is then formulated as\nfollows:\nL = Lpred + gLTAM + (1‚àí g)LSEM . (16)\nThis adaptive gating scheme has demonstrated its effective-\nness in enhancing both the model‚Äôs adaptability and stability\nby dynamically adjusting the influence of action and entity\nprompts.\nIV. E XPERIMENTS\nWe have conducted an extensive analysis to evaluate the per-\nformance of HeurVidQA across several established VideoQA\nbenchmark datasets, including NExT-QA [67], MSVD-QA\n[38], MSRVTT-QA [38], and SUTD-TrafficQA [68].\nA. Datasets\n‚Ä¢ NExT-QA [67] presents complex challenges requiring\nadvanced causal and temporal reasoning. It comprises\n5,440 videos averaging 44 seconds in length, with ap-\nproximately 52K annotated QA pairs categorized into\ncausal, temporal, and descriptive questions. We also con-\nsider the AT Phard split from Buch et al. [69], featuring\nquestions that demand sophisticated video comprehension\nbeyond single-frame analysis.\n‚Ä¢ MSVD-QA [38] is derived from the MSVD dataset and\nincludes 10s video clips across 1,970 videos. It features\naround 51K QA pairs automatically generated based on\nvideo captions, covering diverse topics like video content,\nobject recognition, and scene understanding.\n‚Ä¢ MSRVTT-QA [38] is similar to MSVD-QA but on a\nlarger scale, with 10K videos averaging 15 seconds each\nand containing 244K QA pairs. These are generated from\nvideo descriptions and serve to train and evaluate models\nin open-ended question answering.\n‚Ä¢ SUTD-TrafficQA [68] serves as a critical resource for\nadvancing research in traffic-related question answer-\ning, requiring a deep understanding of traffic events\nand their causal relationships. It comprises over 10,000\nvideos depicting diverse traffic incidents, supplemented\nby 62,535 human-annotated QA pairs‚Äî56,460 for train-\ning and 6,075 for testing. This comprehensive dataset\nrigorously evaluates the cognitive abilities of models\nin understanding complex traffic scenarios. All tasks\nfollow a multiple-choice format without constraints on\nthe number of possible answers.\nB. Implementation Details\nOur method builds on ALPRO [33] as the video-language\nfoundation model, fine-tuned using 4 NVIDIA GeForce 3090\nGPUs. We used weights pre-trained on WebVid-2M and CC-\n3M datasets, alongside prompter weights pre-trained on noisy\nvideo-text pairs. The AdamW optimizer was employed, with\na weight decay of 0.001 and a unified learning rate of 5e-5\nacross datasets. A linear decay schedule was used to adjust\nthe learning rate dynamically, promoting rapid convergence\ninitially and precise convergence during later training stages.\nFor raw video processing, videos were rescaled to 224 √ó224,\nand a random sparse sampling strategy was applied to ex-\ntract 16 frames while preserving their sequential order. The\nEAPrompter followed a distinct processing strategy, resizing\nvideos to 256 √ó 256 before cropping a 224 √ó 224 area.\nAfter random sampling, 50-70% of the original spatial area\nwas randomly masked to capture specific actions and entities\neffectively. For the Entity-Action Heuristic Generation, spaCy\nwas used to extract the top 1,000 frequent verbs and nouns as\naction and entity candidates. Heuristics were discarded if the\nhighest-scoring entity had a normalized similarity score below\n0.1. The training was conducted over 10 epochs for NExT-\nQA and SUTD-TrafficQA, and 15 epochs for MSVD-QA and\nMSRVTT-QA datasets.\nC. Experimental Results\nWe evaluated our method against several state-of-the-art\n(SoTA) models on four VideoQA datasets: NExT-QA, SUTD-\nTrafficQA, MSVD-QA, and MSRVTT-QA. Our comparisons\nincluded non-pretrained models like VGT [18] and CoVGT\n[73], as well as pre-trained VL models, such as MIST [20]\nand our baseline, ALPRO [33].\n1) Comparison on NExT-QA: Table I presents the compar-\native results on the NExT-QA dataset, encompassing both non-\npretrained and pretrained VL models. Notably, we emphasize\nthe baseline method, ALPRO, to demonstrate the improve-\nments achieved through heuristic prompts. The results indicate\nthat pretrained visual-language (VL) models generally surpass\nnon-pretrained models, as evidenced by the performance of\nVGT and CoVGT compared to their pretrained counterparts,\nVGT (PT) [18] and CoVGT (PT) [73]. Our method consis-\ntently achieves optimal or near-optimal results across overall\nperformance metrics (Acc@All) and specific question types\n(Causal, Temporal, and Descriptive).\n8\nTop5ActionHeuristics1.Sing2.Perform3.Record 4.Concert5.Document\nTop5ActionHeuristics1. Drive2. Bring3. Seek4. Arrive5. Exit \nTop5ActionHeuristics1. Marry2. Married3. Suit4. Couple5. Propose\n(0.55)(0.35)(0.05)(0.02)(0.01)\n(0.55)(0.35)(0.05)(0.02)(0.01)\n(0.93)(0.04)(0.01)(0.01)(0.01)\nTop5EntityHeuristics1.Cats2.Fur3.Step4.Half5.Lies\n(0.86)(0.12)(0.01)(0.02)(0.01)\nTop5EntityHeuristics1.Television2.TV3.Screening4.Videos5.Screen\n(0.69)(0.12)(0.05)(0.03)(0.03)\nTop5EntityHeuristics1.Grapes2.Children3.Child4.Kid5.Kids\n(0.95)(0.02)(0.01)(0.01)(0.01)\n(a)(b)\nFig. 5. Visualizations of heuristics generated by EAPrompter. We selected commonly used verbs and nouns as candidates for action and entity prompts,\nrespectively, and labeled the top five with their associated probabilities (in brackets). To obtain heuristics for actions (a) and entities (b), we applied distinct\nprocessing strategies: consistent regions across frames for action prompts, and varying regions for entity prompts to mimic real-world scenarios. Labels with\na probability below 10% were filtered out, with significant heuristics highlighted in red.\nTABLE I\nCOMPARISON WITH STATE -OF-THE -ART RESULTS ON THE VALIDATION STANDARD AND TEST SETS OF THE NEXT-QA DATASET. ACC@C, A CC@T, AND\nACC@D DENOTE ACCURACY FOR CAUSAL , TEMPORAL , AND DESCRIPTIVE QUESTIONS , RESPECTIVELY , EXPRESSED AS A PERCENTAGE (%). T HE BEST\nRESULT IS HIGHLIGHTED IN BOLD , AND THE 2ND BEST RESULT IS UNDERLINED .\nMethod Pre-trained NExT-QA Val NExT-QA Test\nAcc@C Acc@T Acc@D Acc@All Acc@C Acc@T Acc@D Acc@All\nSeViLA (2023) [53] - 73.4 68.8 83.5 73.4 - - - -\nLSTP (2024) [70] - 72.8 66.5 81.2 72.1 - - - -\nCREMA (2024) [71] - 74.4 69.4 81.6 73.9 - - - -\nCoMem (2018) [42] ‚úó 45.2 49.1 55.3 48.0 45.9 50.0 54.4 48.5\nEVQA (2019) [37] ‚úó 42.5 46.3 45.8 44.2 43.3 46.9 45.6 44.9\nHME (2019) [43] ‚úó 46.2 48.2 58.3 48.7 46.8 48.9 57.4 49.2\nHCRN (2020) [72] ‚úó 45.9 49.3 53.7 48.2 47.0 49.3 54.0 48.9\nHGA (2020) [45] ‚úó 46.3 50.7 59.3 49.7 48.1 49.1 57.8 50.0\nIGV (2022) [47] ‚úó - - - - 48.6 51.7 59.6 51.3\nVGT (2022) [18] ‚úó 52.3 55.1 64.1 55.0 51.6 51.9 63.7 53.7\nCoVGT (2023) [73] ‚úó 58.8 57.4 69.4 60.0 58.5 57.0 66.8 59.4\nALPRO (2022) [33] ‚úì 57.6 57.8 71.7 59.9 55.8 56.0 71.9 58.3\nVGT (PT) (2022) [18] ‚úì 53.4 56.4 69.5 56.9 52.8 54.5 67.3 55.7\nMIST-CLIP (2023) [20] ‚úì 54.6 56.6 66.9 57.2 - - - -\nSeViTFiD (2023) [74] ‚úì 54.0 54.1 71.3 56.7 - - - 55.2\nVFC (2023) [75] ‚úì 57.6 53.3 72.8 58.6 - - - -\nCoVGT (PT) (2023) [73] ‚úì 59.7 58.0 69.9 60.7 58.0 58.0 68.4 59.7\nHeurVidQA (Ours) ‚úì 58.8 58.4 73.0 60.9 58.1 57.1 71.1 59.9\nIt is noteworthy that despite employing ALPRO as the base-\nline model with identical pre-training weights, HeurVidQA\nsurpasses ALPRO significantly under the same experimental\nconditions. Specifically, HeurVidQA exhibits a 1.0% improve-\nment on the validation set and a 1.6% enhancement on the\ntest set. This performance boost is primarily driven by the\nstrategic incorporation of additional action and entity heuristic\ninformation, which significantly enhances the model‚Äôs fine-\ngrained visual perception capabilities. A focused analysis\nreveals HeurVidQA‚Äôs distinct advantages in handling temporal\nquestions, where it outperforms models with larger parame-\nters or more input frames. This superiority stems from the\nEAPrompter‚Äôs ability to capture essential action information,\nwhich is crucial for answering temporal queries. Notably, this\naction information, although not explicitly present in the ques-\ntions or answers, provides precise prompts that significantly\nimprove the model‚Äôs response accuracy. Similarly, HeurVidQA\nexcels in descriptive questions, thanks to Entity Prompter‚Äôs\neffectiveness in identifying and describing a broad range of en-\ntity categories from visual data. By establishing relevant con-\nnections between the entities and the questions and answers,\nthe Entity Prompter guides the baseline model in navigating\nentity-dense video content, thereby enhancing its descriptive\nquestion-answering capabilities. While HeurVidQA may not\noutperform certain methods in causal reasoning tasks, it is\nimportant to recognize that many leading approaches, such\nas CoVGT (PT) [73], leverage manually crafted complex\nmethodologies, including intricate graph structures designed\n9\nTABLE II\nCOMPARATIVE EXPERIMENTAL RESULTS OF THE PROPOSED HEURVIDQA MODEL AND EXISTING STATE -OF-THE -ART (SOTA) MODELS ON THE\nMSRVTT-QA ( LEFT ) AND MSVD-QA ( RIGHT ) BENCHMARKS . THE RESULTS ARE REPORTED AS PERCENTAGES (%), WITH THE BEST OUTCOMES\nHIGHLIGHTED IN BOLD .\nMethod MSRVTT-QA MSVD-QA\nWhat Who How When Where All What Who How When Where All\nEVQA (2018) [37] 18.9 38.7 83.5 70.5 29.2 26.4 9.7 42.4 83.8 72.4 53.6 23.3\nCoMem (2018) [42] 23.9 42.5 74.1 69.0 42.9 32.0 19.6 48.7 81.6 74.1 31.7 31.7\nHME (2019) [43] 26.5 43.6 82.4 76.0 28.6 33.0 22.4 50.1 73.0 70.7 42.9 33.7\nCAN (2019) [76] 26.7 43.4 83.7 75.3 35.2 33.2 21.1 47.9 84.1 74.1 57.1 32.4\nSTVQA (2019) [77] 24.5 41.2 78.0 76.5 34.9 30.9 18.1 50.0 83.8 72.4 28.6 31.3\nTSN (2019) [78] 27.9 46.1 84.1 77.8 37.6 35.4 25.0 51.3 83.8 78.4 59.1 36.7\nHGA (2020) [45] 29.2 45.7 83.5 75.2 34.0 35.5 23.5 50.4 83.0 72.4 46.4 34.7\nMHMAN (2020) [79] 28.7 47.1 85.1 77.1 35.2 35.6 23.3 50.7 84.1 72.4 53.6 34.6\nDualVGR (2021) [80] 29.4 45.6 79.8 76.7 36.4 35.5 28.7 53.9 80.0 70.7 46.4 39.0\nVGT (2022) [18] - - - - - 39.7 - - - - - -\nCoVGT (2023) [73] - - - - - 38.3 - - - - - -\nALPRO (2022) [33] 36.0 51.7 85.7 79.6 42.3 41.9 36.0 57.0 82.2 72.4 46.6 44.7\nVGT (PT) (2022) [18] - - - - - 39.7 - - - - - -\nMGIN (2023) [21] - - - - - 38.2 - - - - - 39.7\nAll-in-one+ (2023) [81] - - - - - 39.5 - - - - - 43.8\nCoVGT (PT) (2023) [73] - - - - - 40.0 - - - - - -\nHeurVidQA (Ours) 36.1 51.5 83.1 77.4 46.2 42.1 36.4 60.8 83.5 72.4 50.0 46.4\nTABLE III\nEXPERIMENTAL COMPARATIVE RESULTS OF THE PROPOSED HEURVIDQA MODEL AND EXISTING SOTA MODELS ON SUTD-T RAFFIC QA. O VERALL\nACCURACY IS PRESENTED AS A PERCENTAGE (%). T HE BEST OUTCOMES ARE HIGHLIGHTED IN BOLD . B: B ASIC UNDERSTANDING , F: F ORECASTING\nTASK , R: R EVERSE REASONING , C: C OUNTERFACTUAL INFERENCE , I: I NTROSPECTION , A: ATTRIBUTION . (C) AND (A) REFER TO PROMPT ADDITION\nSTRATEGIES , WHILE (C) AND (C*) DENOTE TRAINING PROMPTS WITH /WITHOUT ADAPTER HEADS .\nMethod B F R C I A Acc@All\nCLIP + Template 31.8 36.0 29.9 41.8 22.1 33.4 32.3\nCLIP-Adapter 35.8 32.0 35.4 42.3 33.1 32.1 34.8\nTVQA [82] - - - - - - 35.2\nHCRN [43] - - - - - - 36.5\nEclipse [68] - - - - - - 37.1\nLORA [83] 38.7 38.7 36.7 37.9 34.5 38.1 38.3\nPrompt learning (C*) [62] 40.3 33.2 41.0 46.5 34.9 38.4 39.7\nPrompt learning (C) [62] 42.4 32.4 45.2 55.5 40.7 43.6 42.9\nPrompt learning (A) [84] 41.7 31.5 40.1 48.4 33.1 41.4 41.1\nTem-adapter (128 frames) [30] 46.0 36.5 44.6 55.0 34.5 47.7 46.0\nHeurVidQA (w/o EAPrompter) 44.5 40.3 51.5 48.1 36.5 48.2 45.6\nHeurVidQA (Ours) 45.0 40.3 49.2 50.3 41.2 49.5 46.3\nfor logical and causal reasoning. These approaches, combined\nwith doubled input frame settings, naturally excel in causal\nreasoning tasks. Despite not securing the top spot in this\ndomain, HeurVidQA has made significant progress, particu-\nlarly in addressing the complexity of causal reasoning content,\nwhich requires an integrated analysis of various video ele-\nments. The combined efforts of the action and entity prompters\nwithin the EAPrompter contribute to notable advancements in\nour baseline model‚Äôs performance.\nTo further validate the effectiveness of our approach in en-\nhancing temporal-spatial reasoning, we conducted experiments\non the AT Phard dataset, a subset of the NExT-QA dataset\nspecifically designed to challenge single-frame inference. This\nsubset allows us to assess whether performance improvements\nare due to genuine enhancements in temporal-spatial reasoning\nor merely increased model complexity. As shown in Table\nIV, HeurVidQA surpasses previous models across all question\ncategories, establishing a new state-of-the-art. Remarkably,\ncompared to the baseline model, the HeurVidQA exhibited\nan approximately 2% increase in performance on the more\nchallenging dataset, compared to a roughly 1% enhancement\non the original NExT-QA dataset. This differential in perfor-\nmance improvement underscores the capacity of EAPrompter\nto significantly bolster the answer reasoning capabilities of\nVideoQA tasks. Importantly, this enhancement is particularly\npronounced in datasets that present a higher level of difficulty,\nhighlighting EAPrompter‚Äôs potential to navigate and address\ncomplex VideoQA challenges effectively. Subsequent ablation\nexperiments confirm that Heuristic Prompting significantly\ncontributes to these performance gains.Moreover, we include\nrecently proposed methods [53], [70], [71] leveraging LLMs,\nmarked in gray due to their large model parameters and ex-\ntensive pre-training data. Although these methods excel across\nall metrics, they require significantly more computational\nresources, with model sizes and data needs far exceeding those\nof pre-trained vision-language models. Given this disparity,\nthese methods are distinguished in gray.\n10\nTABLE IV\nCOMPARISON ON THE ATP-HARD SUBSET OF THE NEXT-QA DATASET. IN\nADDITION TO THE ORIGINAL VALIDATION SET , WE HAVE INCLUDED A\nCHALLENGING SUBSET IDENTIFIED BY ATP THAT REQUIRES\nVIDEO -LEVEL UNDERSTANDING FOR EFFECTIVE EVALUATION .\nPrompter Acc@C Acc@T Acc@All\nATP [69] 19.6 22.6 20.8\nTemporal[ATP] [69] 38.4 36.5 37.6\nHGA [45] 43.3 45.3 44.1\nVFC [75] 39.9 38.3 39.3\nSeViTFiD [74] 43.3 46.5 -\nALPRO [33] 44.4 46.6 45.3\nHeurVidQA (Ours) 46.9 47.8 47.3\n2) Comparison on MSVD-QA and MSRVTT-QA: To assess\nthe effectiveness of EAPrompter on short video datasets, we\nconducted experiments on the MSVD-QA and MSRVTT-QA\ndatasets, with results summarized in Table II. The findings\nindicate that HeurVidQA achieves notable improvements on\nthese datasets. While gains on short videos are modest com-\npared to long video datasets, HeurVidQA offers clear advan-\ntages. It outperforms VGT and CoVGT by 2.4% and 3.8%,\nrespectively, using only half the input frames. This success is\nlargely attributed to EAPrompter‚Äôs ability to accurately iden-\ntify entities and actions, enhancing spatial reasoning. However,\nthe limited duration of the videos in these datasets often leads\nto a higher prevalence of description and perception questions,\nwhich affects overall performance. Despite this, HeurVidQA\nremains competitive with more complex models that utilize\nintricate graph structures for logical and causal reasoning.\nAdditionally, while approaches like MGIN focus on multi-\nlevel intra-granularity and inter-granularity relations, they may\nsometimes neglect the relevance of the question text itself.\nIn contrast, HeurVidQA maintains a balanced focus, leading\nto consistent performance gains. However, it is important to\nacknowledge that the limited action and entity information\nwithin the MSVD-QA and MSRVTT-QA datasets, combined\nwith the dominance of descriptive questions (e.g., ‚ÄúWhat‚Äù\nquestions), diminishes the impact of integrating EAPrompter.\nAs a result, the improvements on these datasets are not as\nsignificant. Additionally, in the MSRVTT-QA dataset, noise\nintroduced by the Prompter may negatively affect perfor-\nmance, particularly when addressing ‚ÄúHow‚Äù questions, further\nreducing the effectiveness of EAPrompter in these contexts.\n3) Comparison on SUTD-TrafficQA: To assess\nHeurVidQA‚Äôs generalization across different domains,\nwe evaluated it on the specialized SUTD-TrafficQA dataset,\nfocused on traffic scenarios. Our approach consistently\noutperformed state-of-the-art methods. Notably, HeurVidQA\nachieved a 0.3% improvement over Tem-adapter [30],\nwhich uses 126 frames, while HeurVidQA utilized only 16.\nSignificant gains were observed in Introspection questions,\nwith a remarkable 6.7% increase, illustrating the model‚Äôs\nenhanced ability to analyze self-reflective queries within video\ncontent. Conversely, the Counterfactual inference questions,\nwhich involve hypothetical scenarios (e.g., speculating\noutcomes had the blue car not accelerated), proved more\nchallenging. HeurVidQA trailed the best-performing method\nby 4.7%, a limitation likely due to the pre-trained data\nTABLE V\nCOMPARISON OF DIFFERENT PROMPT STRATEGIES . ‚ÄúW/ PROMPTER GATE‚Äù\nREFERS TO THE INTEGRATION OF A GATING MECHANISM THAT\nAUTOMATICALLY ADJUSTS THE WEIGHT DISTRIBUTION BETWEEN ENTITY\nAND ACTION PROMPTS . THE RATIOS IN PARENTHESES INDICATE THE\nRELATIVE PROPORTION OF ENTITY -TO-ACTION PROMPTS WITHIN THE\nOVERALL HEURISTIC POOL .\nPrompter Acc@C Acc@T Acc@D Acc@All\nw/o EAPrompter 57.61 57.75 71.69 59.85\nw/ EP (1:0) 58.42 58.31 71.81 60.47\nw/ AP (0:1) 57.81 58.56 73.75 60.53\nw/ EAPrompter (1:1) 57.19 58.31 71.3 59.75\nw/ EAPrompter Gate 58.84 58.37 72.97 60.89\nthat lacks specialized knowledge in traffic scenarios. While\nHeurVidQA exhibits commendable inferential capabilities\nacross various domains, its performance in complex reverse\nreasoning tasks remains constrained. Nevertheless, the model\nshowed a significant improvement over the baseline across all\nquestion types, underscoring EAPrompter‚Äôs effectiveness in\nenhancing reasoning abilities, especially within traffic video\nQA tasks.\nD. Ablation Study\nWe conducted several ablation experiments on the NExT-\nQA dataset to explore the effects of different factors, including\nheuristic ratios, sparsity versus density, heuristic quantities,\nand prompt templates. The goal was to understand how each\ncomponent influences the overall performance of HeurVidQA.\nAside from these specific parameters, all other experimental\nsettings remained consistent with those defined earlier. The\nfindings from these ablation studies provide deeper insights\ninto the critical elements that contribute to the effectiveness\nof our proposed framework.\n1) Heuristic Ratios: To evaluate the impact of Heuristic\nPrompting, we conducted experiments by not only adding or\nremoving the prompters but also varying the ratio between\naction and entity heuristics. We compared fixed heuristic ratios\nagainst a dynamic ratio adjusted by a gating mechanism.\nTable V details the results, showing a clear performance boost\nover the baseline model. Notably, the degree of improve-\nment is influenced by the ratio of heuristic prompts used.\nIntroducing either the Action Prompter or Entity Prompter\nsignificantly enhanced the model‚Äôs performance. However,\nusing both prompts together yielded mixed results depending\non their relative weighting. To achieve an optimal ratio,\nwe incorporated a gating mechanism, enabling the model\nto dynamically calculate the appropriate proportion of each\nprompt based on the characteristics of the current training\nsample. This approach consistently improved performance,\nas visualized in Figure 7, where we gradually increased\nthe proportion of action prompts. Our analysis showed that\nwhen the model lacked action prompts, adding entity prompts\nonly marginally improved performance, as the baseline model\nalready incorporated substantial entity information during pre-\ntraining. However, equalizing the action and entity prompts\ninitially introduced noise, leading to performance fluctuations.\nFurther increasing the action prompt proportion significantly\n11\n2 4 6 8 10 12 14 16\n#Frames\n52\n54\n56\n58\n60Acc@C(%)\n(a) Acc@C\n2 4 6 8 10 12 14 16\n#Frames\n52\n54\n56\n58\n60Acc@T(%)\n (b) Acc@D\n2 4 6 8 10 12 14 16\n#Frames\n68\n70\n72\n74Acc@D(%)\n (c) Acc@T\n2 4 6 8 10 12 14 16\n#Frames\n54\n56\n58\n60\n62Acc@All(%)\n (d) Acc@All\nFig. 6. Variations in test accuracy with different video frame sampling strategies across question types: (a) Causal, (b) Descriptive, (c) Temporal, and (d)\noverall performance within the NExT-QA dataset. Sampling frames range from 2 to 16, showing consistent performance gains as frame count increases. The\nblue line represents HeurVidQA‚Äôs performance, while the orange line shows baseline ALPRO‚Äôs performance. Notably, causal and descriptive questions improve\nsignificantly with 4 frames, while temporal questions benefit more at 16 frames.\nw/o EAP w/o AP w/ EAP w/ EAP G w/o EP\n#Action Weights\n56\n57\n58\n59\n60Acc@C(%)\n(a) Acc@C\nw/o EAP w/o AP w/ EAP w/ EAP G w/o EP\n#Action Weights\n56\n57\n58\n59\n60Acc@T(%) (b) Acc@T\nw/o EAP w/o AP w/ EAP w/ EAP G w/o EP\n#Action Weights\n71\n72\n73\n74\n75Acc@D(%)\n(c) Acc@D\nw/o EAP w/o AP w/ EAP w/ EAP G w/o EP\n#Action Weights\n59.5\n60.0\n60.5\n61.0\n61.5Acc@All(%) (d) Acc@All\nFig. 7. Analysis of the impact of different heuristic prompt components. The\nvisualization shows model performance with varying proportions of action\nprompts. The red column indicates the baseline model without heuristic\naugmentation, while deeper blue columns represent higher action prompt\nratios within the heuristic set. The curve illustrates the trend in model\neffectiveness as action prompts increase. EAP: EAPrompter, AP: Action\nPrompter, EP: Entity Prompter, EAP G: EAPrompter Gate.\nTABLE VI\nDETAILED RESULTS ON THE NEXT-QA BENCHMARK WITH VARYING\nNUMBERS OF FRAMES .\n#frms Acc@C Acc@T Acc@D Acc@All\nBaseline (ALPRO) frame ablation experiment\n1 51.9 52.8 67.7 54.7\n4 56.3 55.7 69.4 58.1\n8 57.2 57.6 69.4 59.2\n16 57.6 57.8 71.7 59.9\nHeurVidQA (ours) frame ablation experiment\n1 54.2 55.8 70.4 57.2\n4 57.8 57.4 71.3 59.8\n8 58.0 58.4 71.3 60.3\n16 58.8 58.4 73.0 60.9\nenhanced performance, indicating that both action and entity\nprompts are essential for the model, despite substantial pre-\ntraining emphasis on entity information.\n2) Sparse Vs. Dense: Figure 6 presents the ablation results\nfor varying numbers of sampled frames per video, with can-\ndidate values of N ‚àà {1, 4, 8, 16}. We compared HeurVidQA\nagainst the baseline under identical conditions. The blue line\ndelineates the performance metrics of HeurVidQA, whereas\nthe orange line illustrates the performance of the baseline\nmodel. Upon incrementally augmenting the number of input\nvideo frames, both the baseline model and HeurVidQA ex-\nhibited similar trends in performance. However, HeurVidQA\nconsistently outperformed the baseline model across all frame\nconfigurations, demonstrating the comprehensive enhancement\nimparted by the EAPrompter. Table VI provides detailed\nexperimental results, clearly showing the superiority of our\nmethod over the baseline, even with fewer sampled frames.\nThe influence of additional frames varies by question type:\ncausal questions benefit the most (around a 4.7% increase),\nwhile temporal and descriptive questions see similar improve-\nments (about 2.6% each). Temporal questions likely demand\nmore frames, yet merely adding frames does not suffice for\nmajor gains. Descriptive questions, less dependent on temporal\ndata, achieve 70.4% effectiveness with just one frame but still\nbenefit from additional frames.\n3) Impact of actions and entities candidate quantities:\nWe conducted a comprehensive ablation study to assess the\nimpact of actions and entities candidate quantities on model\nperformance, with results detailed in Table VII. The first row\nshowcases the baseline model‚Äôs performance using four frames\nwithout incorporating heuristics. We then systematically in-\ncreased the number of verbs and nouns from 400 to 2000. The\nfindings suggest that a balanced quantity of heuristics signifi-\ncantly improves visual-text alignment. However, an excessive\nnumber of heuristics introduces low-frequency elements and\nnoise, which can degrade model performance.\n4) Impact of prompt templates: The design of prompt\ntemplates is critical to the effectiveness of the EAPrompter,\nas it directly influences the extraction of entity and action\nheuristics from video content. To explore this, we crafted\n10 distinct templates for various heuristic types, as shown in\nTable VIII. We then conducted an ablation study using both\nthe long video dataset NExT-QA and the short video dataset\nMSVD-QA. Interestingly, the impact of prompt templates\nvaried across datasets, as illustrated in Table IX. For the\nNExT-QA dataset, which features long videos with abundant\nactions and entities, using complex prompt templates led\nto a slight performance decline. This suggests that more\nconfident heuristics from refined templates might inadvertently\ndistract the model, leading to errors in reasoning and answer\nselection. Conversely, the MSVD-QA dataset, which focuses\non descriptive and perceptual queries, benefited significantly\n12\nTABLE VII\nRESULTS VARY WITH DIFFERENT QUANTITIES OF CANDIDATE ACTIONS\nAND ENTITIES .\n#count Acc@C Acc@T Acc@D Acc@All\n- 57.6 57.8 71.7 59.9\n400 58.1 58.4 73.5 60.6\n600 58.4 59 73.1 60.9\n1000 58.2 59.4 73.6 61\n1600 58.7 58.4 72.7 60.8\n2000 57.3 58.3 71.8 60.2\nTABLE VIII\nPROMPT TEMPLATES FOR INSTANTIATING TEXT PROMPTS .\nTemplate Type Prompt templates\nEntities\nA video of a {}.\nA video of the entity {}.\nA video contains the entity of {}.\nA shooting of a {}.\nA shooting of the {}.\nA shooting contains the entity of {}.\nA video footage of a {}.\nA video footage of the {}.\nA footage contains the entity of {}.\nA video recording about the entity of {}.\nActions\nA video contains the action of {}.\nA video about the action of {}.\nA video recording about the action of {}.\nA video shooting of the action {}.\nA video of action {} being performed.\nA footage of the action of {}.\nA shooting of the action {}.\nA shooting of {} in action.\nA clip of {} in action.\nA clip contains the action of {}.\nfrom the optimized templates. This enhancement indicates that\nrefined templates help provide richer visual heuristics, thereby\nimproving the model‚Äôs performance in perception-based tasks.\nE. Complexity analysis\nWe conducted experiments on the model‚Äôs complexity.\nDetailed results are provided in Tables X and XI. Table\nX reveals that our model uses less than half the trainable\nparameters compared to popular contrastive models like Jus-\ntAsk, ATP, and VGT while achieving superior performance.\nNotably, leveraging EAPrompter does not increase the num-\nber of trainable parameters but introduces additional frozen\nparameters specifically for generating heuristic information.\nThis increment is justified by the significant performance\ngains observed. Moreover, the unified design of the EP\nand AP underscores their importance, as removing either\ncomponent impacts performance without reducing the total\nTABLE IX\nEXPERIMENTAL RESULTS USING DIFFERENT TYPES OF PROMPT\nTEMPLATES . ‚ÄúS IMPLE TEMP.‚Äù REFERS TO SIMPLE AND IDENTICAL\nPROMPT TEMPLATES , WHILE ‚ÄúCOMPLEX TEMP.‚Äù INVOLVES USING\nTEMPLATES SPECIFICALLY TAILORED FOR ENTITIES AND ACTIONS\nMSVD-QA NExT-QA\nSimple Temp. 45.3 60.9\nComplex Temp. 46.6 60.2\nTABLE X\nCOMPARATIVE ANALYSIS OF MODEL PARAMETERS BETWEEN THE\nBASELINE AND OUR PROPOSED HEURVIDQA MODEL . THE TABLE ALSO\nHIGHLIGHTS THE PERFORMANCE COMPARISON BETWEEN OUR MODEL\nAND THE BASELINE UNDER IDENTICAL CONFIGURATION .\nMethod Trainable\nparam\nTotal\nparam Acc@All\nJustAsk [85] 600M 600M 45.3\nATP [69] - 428M 54.3\nVGT [18] 511M 511M 55.7\nPAXION [86] 8.2M 482M 56.9\nHeurVidQA w/o EAP 237M 237M 59.9\nHeurVidQA w/ AP 237M 469M 60.5\nHeurVidQA w/ EP 237M 469M 60.4\nHeurVidQA (ours) 237M 469M 60.9\n50\n52\n54\n56\n58\n60\n62\n0\n50 0\n10 00\n15 00\n20 00\n25 00\n30 00\n1 4 8 16\nAcc@All(%)\nGFLOPs\n# Frames\nW/  O EAPW/  APW/  EPW/  EAPW/  O EAPW/  EAP\nFig. 8. The time complexity of models across various configurations and their\ncorresponding performance outcomes. The line chart illustrates the overall\nperformance of HeurVidQA and the comparative models on the NExT-QA\ndataset. The bar chart shows the GFLOPs required for training a single sample.\nparameter count. This highlights EAPrompter‚Äôs effectiveness\nin enhancing VideoQA tasks without substantially increasing\nmodel complexity. In Table XI, we thoroughly analyze the\ntime complexity of the HeurVidQA model across various\nconfigurations, using the Giga Floating Point Operations per\nSecond (GFLOPs) metric to quantify computational demand\nfor training and inference on a single sample. Moreover,\nour model demonstrates a GFLOP advantage over SeViLA,\nemphasizing its efficiency. To provide a clearer understanding,\nFigure 8 presents visualizations, contrasting HeurVidQA with\nthe baseline model‚Äôs performance on the NExT-QA dataset.\nThe results indicate that while the integration of EAPrompter\nmarginally increases computational overhead as the number\nof input frames rises, this increase diminishes as more frames\nare added. Notably, during inference, HeurVidQA‚Äôs GFLOPs\nalign closely with the baseline, confirming that EAPrompter\ndoes not add to the computational burden in this phase. Thus,\nthe enhancements provided by EAPrompter in HeurVidQA\nstrike a balance between performance gains and computational\nefficiency, without significantly increasing the model‚Äôs overall\ncomplexity.\n13\nC: Why does the baby hold the green cup upside down in the middle of the video?0. Moving backwards. 1.Part of child s costume. 2. Drop the toy out.3. Remove it. 4. Excited.\nGT: 2 HeurVidQA: 2 ALPRO:3\nC: How is the baby counting after the middle of the video?0. With his toe. 1.With an abacus. 2. Click laptop.3. With his hands. 4. With a calculator.\nGT:  3 HeurVidQA: 3 ALPRO: 1\nT: Whatdoes the lady do while the baby is about to crash into a street light?0. Reach out to hold baby. 1.Lift baby up. 2. Push baby towards the pavement.3. Touch the girl. 4. Pull the rope.\nGT:  2 HeurVidQA: 2 ALPRO: 2\nT: Howis the woman in green feeling watching the man in brown and the girl ?0. Terrible.1.Afraid. 2. Happy.3. Shy. 4. Weary.\nGT:  2 HeurVidQA: 2 ALPRO: 3\nD: Howdid the lady in flower shirt see her surrounding clearly?0. Wear spectacles. 1.Rubs her eyes. 2. Wave hand and move body.3. Move away from camera. 4. Swing body.\nGT:  0 HeurVidQA: 0 ALPRO: 0\n0. Wear thin clothes. 1.Wrap inside a blanket. 2. Wear hoodie.3. Long sleeve clothing. 4. Wear winter clothes.D:How did the baby kept himself warm?\nGT:  3 HeurVidQA: 3 ALPRO: 0\nC: whydoes the man in blue laughs while recording the video near the beginning?0. Find the dance entertaining. 1.Excited to throw someone. 2. His fingers got dirty.3. Pick up cloth. 4. Look at boy in dark blue.\nGT: 0 HeurVidQA: 1 ALPRO: 1\nT: Whatdoes the dog do after the boy puts his hand on the sofa at the start?0. Dops stick on sofa. 1.Continue resting. 2. Sits up.3. Touch his arm. 4. Jumps down the sofa.\nGT:  3 HeurVidQA: 4 ALPRO: 3\n1. 2.\n3.\n5.\n8.\n4.\n6.\n7.\nFig. 9. Visualization of the question-answering results generated by the proposed HeurVidQA model on the NExT-QA dataset. The green letter at the top\nof each example indicates the question type: C for Causal, T for Temporal, and D for Descriptive. The ground truth answer, along with the predictions from\nHeurVidQA and the baseline ALPRO model, is shown at the bottom of each example. Correct answers are highlighted in green, while incorrect answers are\nmarked in red.\nTABLE XI\nCOMPARATIVE ANALYSIS OF MODEL GIGA FLOATING POINT OPERATIONS\nPER SECOND (GFLOP S) METRICS ACROSS VARIOUS CONFIGURATIONS .\nTHE OVERALL PERFORMANCE INDICATORS FOR THE NEXT-QA DATASET\nARE INCLUDED FOR A CONSOLIDATED COMPARISON .\nMethod GFLOPs Acc@All\nSeViLA [53] 51181 73.4\nALPRO [33] 439 59.9\nHeurVidQA w/ AP 534 60.5\nHeurVidQA w/ EP 534 60.4\nHeurVidQA (ours) 629 60.9\nF . Qualitative Analysis\nFigure 9 showcases the prediction outcomes generated by\nHeurVidQA on various question categories within the NExT-\nQA dataset. Each question is prefixed with a letter indi-\ncating its type, C for Causal, T for Temporal, and D for\nDescriptive. To comprehensively evaluate our approach, we\nanalyze HeurVidQA with the comparative model ALPRO.\nThree key observations emerged from this analysis: (1) Both\nHeurVidQA and ALPRO effectively address questions with\nextended temporal scopes, such as comprehending the lady‚Äôs\nbehavior in Example 5. (2) HeurVidQA surpasses ALPRO in\nlocalizing smaller visual objects and reasoning about chal-\nlenging questions. For instance, in Example 4, where the\nvideo captures a confined spatial and temporal presence of\nwomen, HeurVidQA‚Äôs enhanced performance is attributed to\nits heuristic prompts, which enable it to detect and identify\ndiverse entities and actions, leading to improved sensitivity\nin temporal reasoning questions. (3) HeurVidQA outperforms\nALPRO on more complex long-term questions. This advan-\ntage stems from its augmented language modeling capabili-\nties, facilitated by heuristic prompts, allowing the model to\nmanage multiple actions and physical information, ultimately\ncontributing to superior performance. (4) While HeurVidQA\ndemonstrates excellent performance in addressing descriptive\nquestions, its efficacy is constrained when tackling intricate\ncausal and temporal scenarios. This limitation is primarily\nattributed to the model‚Äôs reliance on explicitly derived action\nentities from video content as primary sources of heuristics.\nSuch an approach, while effective for straightforward descrip-\ntive tasks, does not inherently equip HeurVidQA with the\nnuanced inferential capabilities requisite for dissecting and\n14\naddressing more complex reasoning challenges. Consequently,\nthe model‚Äôs performance in managing scenarios that demand\nhigh-level complex reasoning exhibits certain boundaries.\nV. C ONCLUSIONS AND FUTURE DIRECTIONS\nThis paper presents the HeurVidQA model, which leverages\nprinciples of prompt learning and engineering to enhance\nthe capabilities of pre-trained visual language models in\nVideoQA tasks. By integrating heuristic prompts that target\nboth action information across temporal frames and entity\ninformation across spatial regions, we introduce two novel\nloss functions: Prompt Action Modeling and Prompt Entity\nModeling. Additionally, we propose a dynamic gating mech-\nanism to maintain a balanced emphasis between action and\nentity prompts,optimizing the model‚Äôs reasoning and inference\ncapabilities.\nWhile HeurVidQA demonstrates substantial improvements\nin VideoQA tasks, it is not without limitations. A key limita-\ntion lies in its reliance on domain-specific heuristic prompts,\nwhich, while effective, may restrict its generalization across\nsignificantly different domains. Additionally, the model‚Äôs per-\nformance in handling highly abstract or counterfactual reason-\ning scenarios remains limited, as it predominantly leverages\nexplicit action-entity heuristics derived from video content. To\naddress these limitations, future work could explore adaptive\nprompt learning mechanisms that dynamically adjust to diverse\ndomains without requiring manual heuristic definitions. Incor-\nporating more advanced reasoning techniques, such as causal\ninference and counterfactual analysis, could further enhance\nthe model‚Äôs robustness in complex scenarios. Additionally,\nintegrating knowledge graphs or external world knowledge\nmay provide the model with a richer contextual understanding,\nenabling it to handle a broader range of VideoQA challenges.\nFinally, reducing the model‚Äôs dependency on large-scale pre-\ntraining data could improve its scalability and applicability to\nresource-constrained environments.\nREFERENCES\n[1] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. ,\n‚ÄúLlama: Open and efficient foundation language models,‚Äù arXiv preprint\narXiv:2302.13971, 2023.\n[2] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li,\nX. Wang, M. Dehghani, S. Brahma et al., ‚ÄúScaling instruction-finetuned\nlanguage models,‚Äù arXiv preprint arXiv:2210.11416 , 2022.\n[3] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\nM. Diab, X. Li, X. V . Lin et al. , ‚ÄúOpt: Open pre-trained transformer\nlanguage models, 2022,‚Äù URL https://arxiv. org/abs/2205.01068.\n[4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., ‚ÄúLanguage mod-\nels are few-shot learners,‚Äù Advances in neural information processing\nsystems, vol. 33, pp. 1877‚Äì1901, 2020.\n[5] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training\nof deep bidirectional transformers for language understanding,‚Äù arXiv\npreprint arXiv:1810.04805, 2018.\n[6] W. Peng, J. Yi, F. Wu, S. Wu, B. Zhu, L. Lyu, B. Jiao, T. Xu, G. Sun,\nand X. Xie, ‚ÄúAre you copying my model? protecting the copyright of\nlarge language models for eaas via backdoor watermark,‚Äù arXiv preprint\narXiv:2305.10036, 2023.\n[7] X. Zhu, J. Guan, M. Huang, and J. Liu, ‚ÄúStorytrans: Non-parallel\nstory author-style transfer with discourse representations and content\nenhancing,‚Äù arXiv preprint arXiv:2208.13423 , 2022.\n[8] H. Liu, C. Li, Q. Wu, and Y . J. Lee, ‚ÄúVisual instruction tuning,‚Äù\nAdvances in neural information processing systems , vol. 36, 2024.\n[9] J. Li, D. Li, S. Savarese, and S. Hoi, ‚ÄúBlip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language\nmodels,‚Äù arXiv preprint arXiv:2301.12597 , 2023.\n[10] B. Lin, B. Zhu, Y . Ye, M. Ning, P. Jin, and L. Yuan, ‚ÄúVideo-llava:\nLearning united visual representation by alignment before projection,‚Äù\narXiv preprint arXiv:2311.10122 , 2023.\n[11] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al. , ‚ÄúGpt-4\ntechnical report,‚Äù arXiv preprint arXiv:2303.08774 , 2023.\n[12] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., ‚ÄúLearning transferable\nvisual models from natural language supervision,‚Äù in International\nconference on machine learning . PMLR, 2021, pp. 8748‚Äì8763.\n[13] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen,\nand I. Sutskever, ‚ÄúZero-shot text-to-image generation,‚Äù in International\nConference on Machine Learning . PMLR, 2021, pp. 8821‚Äì8831.\n[14] Y . Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schu-\nurmans, and P. Abbeel, ‚ÄúLearning universal policies via text-guided\nvideo generation,‚Äù Advances in Neural Information Processing Systems ,\nvol. 36, 2024.\n[15] M. Wang, Y . Li, B. Huang, C. Yuan, Y . Wang, Y . Luo, and B. Huang,\n‚ÄúSelf-feedback transformer: A multi-label diagnostic model for real-\nworld pancreatic neuroendocrine neoplasms data,‚Äù in International Con-\nference on Medical Image Computing and Computer-Assisted Interven-\ntion. Springer, 2023, pp. 521‚Äì530.\n[16] C.-H. Chiu, H.-W. Chung, Y .-J. Chen, Y . Shi, and T.-Y . Ho, ‚ÄúToward\nfairness through fair multi-exit framework for dermatological disease\ndiagnosis,‚Äù arXiv preprint arXiv:2306.14518 , 2023.\n[17] G. Wang, Y . Xie, Y . Jiang, A. Mandlekar, C. Xiao, Y . Zhu, L. Fan, and\nA. Anandkumar, ‚ÄúV oyager: An open-ended embodied agent with large\nlanguage models,‚Äù arXiv preprint arXiv:2305.16291 , 2023.\n[18] J. Xiao, P. Zhou, T.-S. Chua, and S. Yan, ‚ÄúVideo graph transformer for\nvideo question answering,‚Äù in Computer Vision‚ÄìECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings,\nPart XXXVI. Springer, 2022, pp. 39‚Äì58.\n[19] C. Zang, H. Wang, M. Pei, and W. Liang, ‚ÄúDiscovering the real\nassociation: Multimodal causal reasoning in video question answering,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 19 027‚Äì19 036.\n[20] D. Gao, L. Zhou, L. Ji, L. Zhu, Y . Yang, and M. Z. Shou, ‚ÄúMist:\nMulti-modal iterative spatial-temporal transformer for long-form video\nquestion answering,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2023, pp. 14 773‚Äì14 783.\n[21] Y . Wang, M. Liu, J. Wu, and L. Nie, ‚ÄúMulti-granularity interaction and\nintegration network for video question answering,‚Äù IEEE Transactions\non Circuits and Systems for Video Technology , 2023.\n[22] J. Li, D. Li, C. Xiong, and S. Hoi, ‚ÄúBlip: Bootstrapping language-image\npre-training for unified vision-language understanding and generation,‚Äù\nin International Conference on Machine Learning . PMLR, 2022, pp.\n12 888‚Äì12 900.\n[23] J. Wang, P. Zhou, M. Z. Shou, and S. Yan, ‚ÄúPosition-guided text prompt\nfor vision-language pre-training,‚Äù in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2023, pp.\n23 242‚Äì23 251.\n[24] G. Bertasius, H. Wang, and L. Torresani, ‚ÄúIs space-time attention all you\nneed for video understanding,‚Äù arXiv preprint arXiv:2102.05095, vol. 2,\nno. 3, p. 4, 2021.\n[25] X. Liu, H. Peng, N. Zheng, Y . Yang, H. Hu, and Y . Yuan, ‚ÄúEfficientvit:\nMemory efficient vision transformer with cascaded group attention,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 14 420‚Äì14 430.\n[26] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H.\nHoi, ‚ÄúAlign before fuse: Vision and language representation learning\nwith momentum distillation,‚Äù Advances in neural information processing\nsystems, vol. 34, pp. 9694‚Äì9705, 2021.\n[27] J. Liu, G. Wang, J. Xie, F. Zhou, and H. Xu, ‚ÄúVideo question answering\nwith semantic disentanglement and reasoning,‚Äù IEEE Transactions on\nCircuits and Systems for Video Technology , 2023.\n[28] Y . Zhuang, D. Xu, X. Yan, W. Cheng, Z. Zhao, S. Pu, and J. Xiao,\n‚ÄúMultichannel attention refinement for video question answering,‚Äù ACM\nTransactions on Multimedia Computing, Communications, and Applica-\ntions (TOMM), vol. 16, no. 1s, pp. 1‚Äì23, 2020.\n[29] J. Zhang, J. Shao, R. Cao, L. Gao, X. Xu, and H. T. Shen, ‚ÄúAction-\ncentric relation transformer network for video question answering,‚Äù\nIEEE Transactions on Circuits and Systems for Video Technology ,\nvol. 32, no. 1, pp. 63‚Äì74, 2020.\n15\n[30] G. Chen, X. Liu, G. Wang, K. Zhang, P. H. Torr, X.-P. Zhang, and\nY . Tang, ‚ÄúTem-adapter: Adapting image-text pretraining for video ques-\ntion answer,‚Äù in Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2023, pp. 13 945‚Äì13 955.\n[31] J. Wang, D. Chen, Z. Wu, C. Luo, L. Zhou, Y . Zhao, Y . Xie, C. Liu, Y .-G.\nJiang, and L. Yuan, ‚ÄúOmnivl: One foundation model for image-language\nand video-language tasks,‚Äù Advances in neural information processing\nsystems, vol. 35, pp. 5696‚Äì5710, 2022.\n[32] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, ‚ÄúLess\nis more: Clipbert for video-and-language learning via sparse sampling,‚Äù\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2021, pp. 7331‚Äì7341.\n[33] D. Li, J. Li, H. Li, J. C. Niebles, and S. C. Hoi, ‚ÄúAlign and prompt:\nVideo-and-language pre-training with entity prompts,‚Äù in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 4953‚Äì4963.\n[34] W. Yu, H. Zheng, M. Li, L. Ji, L. Wu, N. Xiao, and N. Duan,\n‚ÄúLearning from inside: Self-driven siamese sampling and reasoning for\nvideo question answering,‚Äù Advances in Neural Information Processing\nSystems, vol. 34, pp. 26 462‚Äì26 474, 2021.\n[35] H. Xu, Q. Ye, X. Wu, M. Yan, Y . Miao, J. Ye, G. Xu, A. Hu,\nY . Shi, G. Xu et al. , ‚ÄúYouku-mplug: A 10 million large-scale chinese\nvideo-language dataset for pre-training and benchmarks,‚Äù arXiv preprint\narXiv:2306.04362, 2023.\n[36] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, ‚ÄúFrozen in time: A\njoint video and image encoder for end-to-end retrieval,‚Äù in Proceedings\nof the IEEE/CVF International Conference on Computer Vision , 2021,\npp. 1728‚Äì1738.\n[37] K.-H. Zeng, T.-H. Chen, C.-Y . Chuang, Y .-H. Liao, J. C. Niebles, and\nM. Sun, ‚ÄúLeveraging video descriptions to learn video question answer-\ning,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence ,\nvol. 31, no. 1, 2017.\n[38] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y . Zhuang, ‚ÄúVideo\nquestion answering via gradually refined attention over appearance and\nmotion,‚Äù in Proceedings of the 25th ACM international conference on\nMultimedia, 2017, pp. 1645‚Äì1653.\n[39] X. Li, J. Song, L. Gao, X. Liu, W. Huang, X. He, and C. Gan, ‚ÄúBeyond\nrnns: Positional self-attention with co-attention for video question an-\nswering,‚Äù in The 33rd AAAI Conference on Artificial Intelligence, vol. 8,\n2019.\n[40] J. Jiang, Z. Chen, H. Lin, X. Zhao, and Y . Gao, ‚ÄúDivide and conquer:\nQuestion-guided spatio-temporal contextual attention for video question\nanswering,‚Äù in Proceedings of the AAAI Conference on Artificial Intel-\nligence, vol. 34, no. 07, 2020, pp. 11 101‚Äì11 108.\n[41] F. Zhang, R. Wang, F. Zhou, and Y . Luo, ‚ÄúErm: Energy-based refined-\nattention mechanism for video question answering,‚Äù IEEE Transactions\non Circuits and Systems for Video Technology, vol. 33, no. 3, pp. 1454‚Äì\n1467, 2022.\n[42] J. Gao, R. Ge, K. Chen, and R. Nevatia, ‚ÄúMotion-appearance co-memory\nnetworks for video question answering,‚Äù in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2018, pp.\n6576‚Äì6585.\n[43] C. Fan, X. Zhang, S. Zhang, W. Wang, C. Zhang, and H. Huang,\n‚ÄúHeterogeneous memory enhanced multimodal attention model for video\nquestion answering,‚Äù in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , 2019, pp. 1999‚Äì2007.\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Advances in\nneural information processing systems , vol. 30, 2017.\n[45] P. Jiang and Y . Han, ‚ÄúReasoning with heterogeneous graph alignment\nfor video question answering,‚Äù in Proceedings of the AAAI Conference\non Artificial Intelligence , vol. 34, no. 07, 2020, pp. 11 109‚Äì11 116.\n[46] J. Park, J. Lee, and K. Sohn, ‚ÄúBridge to answer: Structure-aware graph\ninteraction network for video question answering,‚Äù in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 15 526‚Äì15 535.\n[47] Y . Li, X. Wang, J. Xiao, W. Ji, and T.-S. Chua, ‚ÄúInvariant grounding for\nvideo question answering,‚Äù in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , 2022, pp. 2928‚Äì2937.\n[48] M. Gandhi, M. O. Gul, E. Prakash, M. Grunde-McLaughlin, R. Krishna,\nand M. Agrawala, ‚ÄúMeasuring compositional consistency for video\nquestion answering,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022, pp. 5046‚Äì5055.\n[49] Y . Wang, M. Liu, J. Wu, and L. Nie, ‚ÄúMulti-granularity interaction and\nintegration network for video question answering,‚Äù IEEE Transactions\non Circuits and Systems for Video Technology, vol. 33, no. 12, pp. 7684‚Äì\n7695, 2023.\n[50] K. Guo, D. Tian, Y . Hu, C. Lin, Y . Sun, J. Zhou, X. Duan, J. Gao, and\nB. Yin, ‚ÄúCfmmc-align: Coarse-fine multi-modal contrastive alignment\nnetwork for traffic event video question answering,‚Äù IEEE Transactions\non Circuits and Systems for Video Technology , pp. 1‚Äì1, 2024.\n[51] L. Li, T. Jin, W. Lin, H. Jiang, W. Pan, J. Wang, S. Xiao, Y . Xia,\nW. Jiang, and Z. Zhao, ‚ÄúMulti-granularity relational attention network\nfor audio-visual question answering,‚Äù IEEE Transactions on Circuits and\nSystems for Video Technology, pp. 1‚Äì1, 2023.\n[52] T. Yu, K. Fu, J. Zhang, Q. Huang, and J. Yu, ‚ÄúMulti-granularity con-\ntrastive cross-modal collaborative generation for end-to-end long-term\nvideo question answering,‚Äù IEEE Transactions on Image Processing ,\nvol. 33, pp. 3115‚Äì3129, 2024.\n[53] S. Yu, J. Cho, P. Yadav, and M. Bansal, ‚ÄúSelf-chained image-language\nmodel for video localization and question answering,‚Äù arXiv preprint\narXiv:2305.06988, 2023.\n[54] M. Wu, M. Cao, Y . Bai, Z. Zeng, C. Chen, L. Nie, and M. Zhang,\n‚ÄúAn empirical study of frame selection for text-to-video retrieval,‚Äù arXiv\npreprint arXiv:2311.00298, 2023.\n[55] T. Yu, X. Lin, S. Wang, W. Sheng, Q. Huang, and J. Yu, ‚ÄúA comprehen-\nsive survey of 3d dense captioning: Localizing and describing objects\nin 3d scenes,‚Äù IEEE Transactions on Circuits and Systems for Video\nTechnology, vol. 34, no. 3, pp. 1322‚Äì1338, 2024.\n[56] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu,\nL. Dong, F. Wei et al., ‚ÄúOscar: Object-semantics aligned pre-training for\nvision-language tasks,‚Äù in Computer Vision‚ÄìECCV 2020: 16th European\nConference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XXX\n16. Springer, 2020, pp. 121‚Äì137.\n[57] C. Jia, Y . Yang, Y . Xia, Y .-T. Chen, Z. Parekh, H. Pham, Q. Le, Y .-H.\nSung, Z. Li, and T. Duerig, ‚ÄúScaling up visual and vision-language\nrepresentation learning with noisy text supervision,‚Äù in International\nConference on Machine Learning . PMLR, 2021, pp. 4904‚Äì4916.\n[58] C. Sun, A. Myers, C. V ondrick, K. Murphy, and C. Schmid, ‚ÄúVideobert:\nA joint model for video and language representation learning,‚Äù in\nProceedings of the IEEE/CVF international conference on computer\nvision, 2019, pp. 7464‚Äì7473.\n[59] L. Zhu and Y . Yang, ‚ÄúActbert: Learning global-local video-text repre-\nsentations,‚Äù in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , 2020, pp. 8746‚Äì8755.\n[60] X. L. Li and P. Liang, ‚ÄúPrefix-tuning: Optimizing continuous prompts\nfor generation,‚Äù arXiv preprint arXiv:2101.00190 , 2021.\n[61] F. Petroni, T. Rockt ¬®aschel, P. Lewis, A. Bakhtin, Y . Wu, A. H. Miller,\nand S. Riedel, ‚ÄúLanguage models as knowledge bases?‚Äù arXiv preprint\narXiv:1909.01066, 2019.\n[62] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, ‚ÄúLearning to prompt for vision-\nlanguage models,‚Äù International Journal of Computer Vision , vol. 130,\nno. 9, pp. 2337‚Äì2348, 2022.\n[63] Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu,\nZiwei, ‚ÄúConditional prompt learning for vision-language models,‚Äù in\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2022, pp. 16 816‚Äì16 825.\n[64] C. Ma, Y . Liu, J. Deng, L. Xie, W. Dong, and C. Xu, ‚ÄúUnderstanding\nand mitigating overfitting in prompt tuning for vision-language models,‚Äù\nIEEE Transactions on Circuits and Systems for Video Technology ,\nvol. 33, no. 9, pp. 4616‚Äì4629, 2023.\n[65] J. Li, L. Zhang, K. Zhang, B. Hu, H. Xie, and Z. Mao, ‚ÄúCascade seman-\ntic prompt alignment network for image captioning,‚Äù IEEE Transactions\non Circuits and Systems for Video Technology, vol. 34, no. 7, pp. 5266‚Äì\n5281, 2024.\n[66] Y . Zhong, J. Yang, P. Zhang, C. Li, N. Codella, L. H. Li, L. Zhou,\nX. Dai, L. Yuan, Y . Liet al., ‚ÄúRegionclip: Region-based language-image\npretraining,‚Äù in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition , 2022, pp. 16 793‚Äì16 803.\n[67] J. Xiao, X. Shang, A. Yao, and T.-S. Chua, ‚ÄúNext-qa: Next phase of\nquestion-answering to explaining temporal actions,‚Äù in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2021, pp. 9777‚Äì9786.\n[68] L. Xu, H. Huang, and J. Liu, ‚ÄúSutd-trafficqa: A question answering\nbenchmark and an efficient network for video reasoning over traffic\nevents,‚Äù in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , 2021, pp. 9878‚Äì9888.\n[69] S. Buch, C. Eyzaguirre, A. Gaidon, J. Wu, L. Fei-Fei, and J. C.\nNiebles, ‚ÄúRevisiting the‚Äù video‚Äù in video-language understanding,‚Äù in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 2917‚Äì2927.\n[70] Y . Wang, Y . Wang, P. Wu, J. Liang, D. Zhao, and Z. Zheng, ‚ÄúLstp:\nLanguage-guided spatial-temporal prompt learning for long-form video-\ntext understanding,‚Äù arXiv preprint arXiv:2402.16050 , 2024.\n16\n[71] S. Yu, J. Yoon, and M. Bansal, ‚ÄúCrema: Multimodal compositional video\nreasoning via efficient modular adaptation and fusion,‚Äù arXiv preprint\narXiv:2402.05889, 2024.\n[72] T. M. Le, V . Le, S. Venkatesh, and T. Tran, ‚ÄúHierarchical conditional\nrelation networks for video question answering,‚Äù in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, 2020,\npp. 9972‚Äì9981.\n[73] J. Xiao, P. Zhou, A. Yao, Y . Li, R. Hong, S. Yan, and T.-S. Chua,\n‚ÄúContrastive video question answering via video graph transformer,‚Äù\narXiv preprint arXiv:2302.13668 , 2023.\n[74] S. Kim, J.-H. Kim, J. Lee, and M. Seo, ‚ÄúSemi-parametric video-\ngrounded text generation,‚Äù arXiv preprint arXiv:2301.11507 , 2023.\n[75] L. Momeni, M. Caron, A. Nagrani, A. Zisserman, and C. Schmid, ‚ÄúVerbs\nin action: Improving verb understanding in video-language models,‚Äù in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2023, pp. 15 579‚Äì15 591.\n[76] T. Yu, J. Yu, Z. Yu, and D. Tao, ‚ÄúCompositional attention networks with\ntwo-stream fusion for video question answering,‚Äù IEEE Transactions on\nImage Processing, vol. 29, pp. 1204‚Äì1218, 2019.\n[77] Y . Jang, Y . Song, C. D. Kim, Y . Yu, Y . Kim, and G. Kim, ‚ÄúVideo question\nanswering with spatio-temporal reasoning,‚Äù International Journal of\nComputer Vision, vol. 127, pp. 1385‚Äì1412, 2019.\n[78] T. Yang, Z.-J. Zha, H. Xie, M. Wang, and H. Zhang, ‚ÄúQuestion-aware\ntube-switch network for video question answering,‚Äù in Proceedings of\nthe 27th ACM International Conference on Multimedia, 2019, pp. 1184‚Äì\n1192.\n[79] T. Yu, J. Yu, Z. Yu, Q. Huang, and Q. Tian, ‚ÄúLong-term video question\nanswering via multimodal hierarchical memory attentive networks,‚Äù\nIEEE Transactions on Circuits and Systems for Video Technology ,\nvol. 31, no. 3, pp. 931‚Äì944, 2020.\n[80] J. Wang, B.-K. Bao, and C. Xu, ‚ÄúDualvgr: A dual-visual graph reasoning\nunit for video question answering,‚Äù IEEE Transactions on Multimedia ,\nvol. 24, pp. 3369‚Äì3380, 2021.\n[81] D. Ko, J. S. Lee, M. Choi, J. Chu, J. Park, and H. J. Kim, ‚ÄúOpen-\nvocabulary video question answering: A new benchmark for evaluating\nthe generalizability of video question answering models,‚Äù in Proceedings\nof the IEEE/CVF International Conference on Computer Vision , 2023,\npp. 3101‚Äì3112.\n[82] J. Lei, L. Yu, M. Bansal, and T. L. Berg, ‚ÄúTvqa: Localized, compo-\nsitional video question answering,‚Äù arXiv preprint arXiv:1809.01696 ,\n2018.\n[83] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\nand W. Chen, ‚ÄúLora: Low-rank adaptation of large language models,‚Äù\narXiv preprint arXiv:2106.09685 , 2021.\n[84] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan,\nand S.-N. Lim, ‚ÄúVisual prompt tuning,‚Äù in European Conference on\nComputer Vision. Springer, 2022, pp. 709‚Äì727.\n[85] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, ‚ÄúJust ask:\nLearning to answer questions from millions of narrated videos,‚Äù in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 1686‚Äì1697.\n[86] Z. Wang, A. Blume, S. Li, G. Liu, J. Cho, Z. Tang, M. Bansal, and\nH. Ji, ‚ÄúPaxion: Patching action knowledge in video-language foundation\nmodels,‚Äù Advances in Neural Information Processing Systems , vol. 36,\n2024."
}