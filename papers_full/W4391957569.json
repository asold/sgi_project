{
    "title": "Creating Edge AI from Cloud-based LLMs",
    "url": "https://openalex.org/W4391957569",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2589245392",
            "name": "Qifei Dong",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2156635061",
            "name": "Xiangliang Chen",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A416104820",
            "name": "Mahadev Satyanarayanan",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2060132914",
        "https://openalex.org/W2766304518",
        "https://openalex.org/W3180243620",
        "https://openalex.org/W2125444986",
        "https://openalex.org/W1966202481",
        "https://openalex.org/W2955945313",
        "https://openalex.org/W2916266369",
        "https://openalex.org/W2029906397"
    ],
    "abstract": "Cyber-human and cyber-physical systems have tight end-to-end latency bounds, typically on the order of a few tens of milliseconds. In contrast, cloud-based large-language models (LLMs) have end-to-end latencies that are two to three orders of magnitude larger. This paper shows how to bridge this large gap by using LLMs as offline compilers for creating task-specific code that avoids LLM accesses. We provide three case studies as proofs of concept, and discuss the challenges in generalizing this technique to broader uses.",
    "full_text": null
}