{
  "title": "HTR-VT: Handwritten text recognition with vision transformer",
  "url": "https://openalex.org/W4402191334",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2141385404",
      "name": "Li Yu-Ting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2351111922",
      "name": "Chen De-xiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2358795919",
      "name": "Tang Ting-long",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347471057",
      "name": "Shen Xi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6747818627",
    "https://openalex.org/W6748151506",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W6786931323",
    "https://openalex.org/W6842724052",
    "https://openalex.org/W3112478873",
    "https://openalex.org/W3034726763",
    "https://openalex.org/W3038203375",
    "https://openalex.org/W6703092425",
    "https://openalex.org/W6732921768",
    "https://openalex.org/W4382202677",
    "https://openalex.org/W3031548983",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2152928267",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2079735306",
    "https://openalex.org/W6780560095",
    "https://openalex.org/W6760739341",
    "https://openalex.org/W6732239032",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W4382468776",
    "https://openalex.org/W4360841345",
    "https://openalex.org/W4283276297",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6803432384",
    "https://openalex.org/W4386065847",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W6790255483",
    "https://openalex.org/W4229453476",
    "https://openalex.org/W4292893847",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W4231138784",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4315884068",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3212076252",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3162013555",
    "https://openalex.org/W2965373594"
  ],
  "abstract": null,
  "full_text": "HTR-VT: Handwritten Text Recognition with Vision Transformer\nYuting Li†a, Dexiong Chenb, Tinglong Tanga, Xi Shenc\naChina Three Gorges University, China\nbMax Planck Institute of Biochemistry, Germany\ncIntellindust, China\nAbstract\nWe explore the application of Vision Transformer (ViT) for handwritten text recognition. The limited availability of labeled data in\nthis domain poses challenges for achieving high performance solely relying on ViT. Previous transformer-based models required\nexternal data or extensive pre-training on large datasets to excel. To address this limitation, we introduce a data-efficient ViT method\nthat uses only the encoder of the standard transformer. We find that incorporating a Convolutional Neural Network (CNN) for\nfeature extraction instead of the original patch embedding and employ Sharpness-Aware Minimization (SAM) optimizer to ensure\nthat the model can converge towards flatter minima and yield notable enhancements. Furthermore, our introduction of the span\nmask technique, which masks interconnected features in the feature map, acts as an effective regularizer. Empirically, our approach\ncompetes favorably with traditional CNN-based models on small datasets like IAM and READ2016. Additionally, it establishes a\nnew benchmark on the LAM dataset, currently the largest dataset with 19,830 training text lines. The code is publicly available at:\nhttps://github.com/YutingLi0606/HTR-VT.\nKeywords:\nHandwritten Text Recognition, Vision Transformer, Mask Strategy, Sharpness-Aware Minimization, Data-efficient\n1. Introduction\nThe task of handwritten text recognition aims at recognizing\nthe text in an image that has been scanned from a document.\nThe standard approach [36, 38, 22, 20] typically involves two\nsteps: first, using a detector to identify the lines of text, and then\npredicting the sequence of characters that make up each line.\nThis paper focuses on the latter task, which aims to accurately\npredict the text in a given line image. As described in LAM [34]:\n\"This level of annotation granularity has been chosen as it is a\ngood trade-off between word-level and paragraph-level in terms\nof required time, cost, and amount of supervision and because\nit is common in HTR research.\" This rationale for creating the\ndataset aligns with our initial decision to focus our research on\nline-level recognition. The importance of line-level recognition\nis significant. At the same time, recognizing handwritten text\nlines is a difficult task due to variations in writing styles between\nindividuals and the presence of cluttered backgrounds (examples\nare provided in Figure 3 and in the supplementary material).\nPrevious approaches mainly relied on Convolutional Neural\nNetworks (CNNs) [2, 4, 20, 50] or recurrent models [36, 39, 11]\nto address this challenge .\nRecent success of Vision Transformer (ViT) [35] in computer\nvision tasks has motivated researchers to explore its potential in\nhandwritten text recognition. However, ViT does not introduce\nany strong inductive bias in its model design and is recognized\nfor its dependency on vast quantities of annotated training data\n†Corresponding Author.\nto deliver good performance. Considering the limited number\nof annotated samples available for handwritten text recognition\n(as shown in Table 1), earlier transformer-based methods, uti-\nlizing the standard transformer architecture (both encoder and\ndecoder), relied on large-scale real-world or synthetic data for\npre-training [18, 48] to achieve satisfactory performance.\nIn this paper, we introduce a simple and data-efficient ViT-\nbased model that solely employs the encoder component of the\nstandard transformer for handwritten text recognition. Our objec-\ntive is to propose a novel ViT-like model to perform well on this\ntask while making minimal modifications to the standard ViT ar-\nchitecture. Our preliminary findings indicate that ViT can deliver\nsatisfactory results, particularly on the LAM dataset [34], which\nis the most extensive dataset containing 19,830 training sam-\nples. We use this dataset as the basis of our experimental design,\nwhich establishes a benchmark for assessing and contrasting our\nproposed model. Instead of using a patch embedding to generate\ninput tokens, we demonstrate through experimentation that us-\ning a widely-used ResNet-18 [23] to extract intermediate visual\nfeature representations as input tokens is much more conducive\nto stable training and significantly better performance. Addition-\nally, we show that employing Sharpness-Aware Minimization\n(SAM) [55] as optimizer enforces the model to converge to-\nwards flatter minima and randomly replacing span tokens with\nlearnable tokens can alleviate overfitting and achieve consistent\nimprovement across various dataset scales.\nDespite its simplicity, our approach achieves promising perfor-\nmance on standard benchmarks. On the largest dataset LAM [34]\n(containing 19,830 training samples), our approach outperforms\nPreprint submitted to Pattern Recognition (Accepted for publication) September 16, 2024arXiv:2409.08573v1  [cs.CV]  13 Sep 2024\nboth CNN-based and transformer-based approaches by a clear\nmargin. On small-scale datasets such as IAM [ 33] (contain-\ning 6,428 training samples) and READ2016 [ 11] (containing\n8,349 training samples), we achieve better performance than\nother transformer-based approaches and competitive perfor-\nmance compared with CNN-based approaches.\nThe main contributions of this paper are summarized as the\nfollowing:\n• We propose a simple and data-efficient approach for hand-\nwritten text recognition, with minimal modifications on the\nViT.\n• We empirically show that without pre-training or any addi-\ntional data, our ViT-like model can achieve state-of-the-art\nperformance on handwritten text recognition.\n2. Related Work\nTraditional approaches for Handwritten Text Recognition. Net-\nwork architectures for handwritten text recognition today typ-\nically use a combination of convolutional layers and recurrent\nlayers. A number of convolutional layers are stacked and placed\nat the start of the network to extract local features from text-line\nimages, followed by recurrent layers, specifically Bi-directional\nLong Short-Term Memory (BLSTM) [49] layers. These recur-\nrent layers process the features sequentially to output character\nprobabilities based on contextual dependencies. Such an archi-\ntecture results in a Convolutional Recurrent Neural Network\n(CRNN) [ 36, 22, 38, 37]. The models are typically trained\nusing the Connectionist Temporal Classification (CTC) loss\n[14], which allows for dealing with label sequences of shorter\nlength than predicted sequences, without knowledge of charac-\nter segmentation. Encoder-Decoder-based architectures have\nalso been explored for handwritten text recognition [17, 39, 26].\nIn [ 17], the CTC loss is replaced with the cross-entropy loss,\nand the sequence alignment is achieved via an attention-based\nencoder-decoder architecture. A special end-of-line token is\nintroduced to stop the recurrent process. While these models\ncan obtain lower test error rates, some of them often require\ncomplex pre/post-processing steps and suffer from the lack of\ncomputation parallelization inherently, which affects both train-\ning and inference time. Recently, Fully Convolutional Networks\n(FCNs) [4, 20, 2] have been proposed as an alternative to tradi-\ntional CRNNs. FCNs simulate the dependency modeling pro-\nvided by LSTM by combining them with GateBlocks layers [50],\nwhich implement a selection mechanism similar to that of LSTM\ncells. Each gate in GateBlocks is made up of Depth-wise Sepa-\nrable Convolutions [25], which reduce the number of parameters\nand speed up the training process. OrigamiNet [ 4] focuses on\nlearning to unfold the input paragraph image into a single text\nline. This transformation network enables using the standard\nCTC loss [ 14] and processing the image in a single step. In\ncontrast, Coquenet et al. [2] proposed models that incorporate a\nvertical attention mechanism to recurrently generate line features\nand perform an implicit line segmentation. While FCNs have\nobtained state-of-the-art results in recent years, they may still\nstruggle with long-range contextual dependencies.\nDataset Training Validation Test Language Charset\nIAM [33] 6,482 976 2,915 English 79\nREAD2016 [11] 8,349 1,040 1,138 German 89\nLAM [34] 19,830 2,470 3,523 Italian 89\nTable 1: Datasets for handwritten text recognition.Number of training,\nvalidation, and testing samples in IAM [ 33], READ2016 [11] and LAM [34]\nare presented in the table. We also include the number of characters in their\nalphabet.\nTransformer-based models for Handwritten Text Recognition.\nTransformer-based architectures have not been widely explored\nin handwritten text recognition, but some recent approaches\nhave used Transformers in place of RNNs. These models often\nrequire pre-training on large real or synthetic datasets to achieve\ncomparable performance to mainstream models.\nTrOCR [18] is a recent approach to handwritten text recogni-\ntion that integrates two powerful pre-trained models respectively\nfrom computer vision and NLP, BEiT [44] and RoBERTa [45].\nBEiT is a vision transformer that functions as an encoder and\nis pre-trained on ImageNet-1K, a dataset of 1.2 million images,\nwhile RoBERTa serves as a decoder that generates texts. To\npre-train the TrOCR model, Li et al. [ 18] synthesize a large-\nscale dataset consisting of both printed and synthetically gener-\nated handwritten text lines in English, totaling approximately\n687 million and 18 million in the first stage. In this stage, the\ndataset is not public. In the second stage, they built two rela-\ntively small datasets corresponding to printed and handwritten\ndownstream tasks, containing millions of textline images each.\nFinally, the model is fine-tuned on real-world data, such as the\nIAM dataset [33]. Kang et al. [48] use Transformer models with\nmulti-head self-attention layers at the textual and visual stages\nand trains with a synthetic dataset of 138,000 lines. Another\nrecent approach, Text-DIAE [56], employs a transformer-based\narchitecture that incorporates three pretext tasks as learning ob-\njectives to be optimized during pretraining without the usage of\nlabeled data. Some methods [ 57, 1] explored document-level\nrecognition and also applied transformer architectures. While\ntransformer-based models have shown promising results in line-\nlevel handwritten text recognition, they still require large-scale\nreal-world or synthetic data for pre-training.\nData-efficient Transformer for Handwritten Text Recognition.\nThe DeiT [30] is the first work to demonstrate that Transformers\ncan be learned on mid-sized datasets (i.e., ImageNet-1k [27])) in\nrelatively shorter training episodes. Besides using augmentation\nand regularization procedures, the main contribution of DeiT\n[30] is a novel distillation that relies on a distillation token. Liu\net al. [ 32] propose a dense relative localization loss to improve\nViTs’ data efficiency. DropKey [ 51] is a recent data-efficient\nmethodology to effectively improve the dropout technique in\nViT by moving dropout operations ahead of attention matrix\ncalculation and setting the Key as the dropout unit, yielding a\ndropout-before-softmax scheme.\n2\n3. Method\nIn this section, we present our approach to handwritten text\nrecognition. Given an input handwritten text line I ∈RW×H,\nwhere W and H are the width and height of the image, our\napproach encodes the image into a set of spatially-aware features\n{xi}i∈[1,2,...,L] using a CNN extractor. The number of features\nL = WH\nS wS h\n, determined by the down-sampling ratio of the width\nand height of the image, denoted as S w and S h, respectively. We\nthen use a transformer encoder to take these features as input\ntokens and output character predictions. The entire model is\noptimized using the Connectionist Temporal Classification [14]\n(CTC) loss. Our method is summarized in Figure 1.\nIn Section 3.1, we revisit the architecture of the Vision Trans-\nformer (ViT)[35]. In Section 3.2, we describe our data-efficient\nViT approach for handwritten text recognition, which involves a\nCNN feature extractor, Sharpness-Aware Minimization (SAM)\n[55] and a new masking strategy: span mask strategy. We pro-\nvide implementation details in Section 3.2.\n3.1. Preliminary: Vision Transformer (ViT)\nVision Transformer (ViT) [35] decomposes each image into\na sequence of tokens with a fixed length, where the tokens rep-\nresent non-overlapping image patches. Similar to BERT [ 40],\nViT adds an additional class token xcls to the sequence, which\nrepresents the global information of the image. To retain po-\nsitional information, position embeddings are explicitly added\ninto each patch including the class token. Note that our model\nremoves the additional class token and uses sinusoidal position\nembeddings by [13] to the encoder’s inputs, as used in MAE\n[12].\nSubsequently, all tokens undergo processing via stacked trans-\nformer encoders [ 13], A transformer encoder comprises N\nblocks, with each block featuring a multi-head self-attention\n(MSA) layer followed by a feed-forward network (FFN). The\nFFN, which includes a simple two-layer MLP, is augmented\nby the GELU activation function [41] after the first linear layer.\nFurthermore, layer normalization (LN) [ 42] is applied before\nevery block, and residual shortcuts [ 23] are used after every\nblock. The processing of the n-th block can be expressed as:\nyn = xn−1 + MSA\n\u0010\nLN\n\u0010\nxn−1\u0011\u0011\nxn = yn + FFN (LN (yn))\n(1)\nwhere xn−1 ∈RL×C is the input of the n-th block, N and C de-\nnote the number of tokens and the dimension of the embedding,\nrespectively.\n3.2. ViT for handwritten text recognition\nWe present a ViT-based model designed for handwritten text\nrecognition with minimal adjustments to the standard ViT [35].\nOur proposed network architecture is depicted in Figure 1. ViT\nalone is not stable for handwritten text recognition (see Sec-\ntion 4.3). Therefore, we suggest three modifications: i) a CNN\nfeature extractor to obtain features for each input token, enabling\npowerful feature extraction, ii) a span feature masking strategy\nFigure 1: Architecture overview.Our approach encodes a text-line image into\nfeatures using a CNN feature extractor. The transformer encoder takes these\nfeatures as input tokens output character predictions. During the training, the\nspan input tokens are replaced by learnable mask tokens. The entire model is\noptimized using CTC [14] loss.\nto replace masking tokens with learnable tokens, effectively\nalleviating the impact of overfitting, and iii) employ Sharpness-\nAware Minimization (SAM) optimizer to ensure that the model\ncan converge towards flatter minima.\nCNN feature extractor. To make our pipeline simple, we adopt\nthe widely-used ResNet-18 [23] as our CNN feature extractor,\nwith minor adjustments made to accommodate line-level hand-\nwritten text images. Specifically, we remove the final residual\nblock and adjust the stride to produce features with enough\ninformation for character recognition while maintaining the two-\ndimensional nature of the task. More details about the modifica-\ntion as well as experiments of additional CNN feature extractors\nare provided in the supplementary material.\nSpan feature masking strategy. Our work draws inspiration from\nBERT [40], SpanBERT [53] and MASS [52], which leverage the\nprediction of randomly masked words or tokens to learn expres-\nsive language representations. We have adapted this methodol-\nogy to our specific task and observed the benefits of employing\nrandom feature masking. Furthermore, our intuition suggests\nthat the feature extractor can capture a board receptive field. To\nenhance the model’s comprehension of contextual information\n3\nencompassing neighboring ink pixels, we propose expanding\nthe masking range.\nPrecisely, the feature map after the CNN feature extractor\nis flattened to a sequence of tokens with dimensions L ×C,\nwhere L represents the sequence length and C represents the\nfeature dimension. We randomly mask the span of tokens with\na maximum span length s (i.e., the number of interconnected\ntokens). In total τL tokens are masked and replaced with a\nlearnable token, where τis a hyperparameter defining the mask\nratio. More details of the span mask strategy are provided in the\nsupplementary material.\nSharpness-Aware Minimization (SAM). Sharpness-Aware Mini-\nmization (SAM), proposed by Foret et al. [55], is an optimization\nmethod that enhances the generalization of deep neural networks\n(DNNs). It aims to find model parameters that reside in flat min-\nima, ensuring a uniformly low loss across the model. Given our\nobjective function LCTC and the parameters of the DNN θ, the\nSAM optimizer is designed to find θsuch that:\nmin\nθ\nmax\n∥ϵ∥2≤ρ\nLCTC(θ+ ϵ), (2)\nwhere ϵ represents a perturbation vector, and ρ is the size of\nthe neighborhood within which the algorithm minimizes the\nsharpness of the loss function. The SAM algorithm functions by\nalternately identifying the worst-case perturbation ϵ that max-\nimizes the loss within an ℓ2-norm ball of radius ρ, and then\nupdating the DNN parameters θto minimize this perturbed loss.\nImplementation details. We employ a ViT encoder with 4 layers.\nEach layer is with a dimension of 768 and 6 heads. The hidden\ndimension of MLP in the feed-forward network (FFN) is 3,072.\nLarger ViT models do not bring obvious gain. For our span mask\nstrategy, we set the mask ratio to 0.4 and the span length to 8 in\nall datasets. An ablation study of the mask ratio and span length\nis provided in Section 4.3. For all experiments, we use a batch\nsize of 128 and optimize all our models with the AdamW [43]\noptimizer for 100,000 iterations with a weight decay of 0.5. We\nperform a warm-up-cosine learning rate schedule with the max\nlearning rate equal to 1e-3 and use 1,000 iterations for warm\nup. Trainings are performed on a single GPU RTX 4090 (24Gb)\nand in the following experiments, models are trained for almost\n16 hours. Similar to OrigamiNet [ 4], we use the exponential\nmoving average (EMA) method with a decay rate of 0.9999. For\ndata augmentation, we fix the input image resolution to 512 x\n64 and use random transformation, erosion, dilation, color jitter,\nand elastic distortion. We set the probability of using each data\naugmentation to 0.5, and they can be combined with each other.\n4. Experiments\nIn this section, we evaluate the performance of our model for\nline-level recognition. Our experimental results demonstrate that\nour model achieves state-of-the-art results on the LAM [34] and\nIAM [33] datasets. Moreover, our model competes well with\nother state-of-the-art models on the READ2016 [ 11] datasets.\nIt is worth noting that our model achieves good performance\nwithout any pre-training or synthetic data and without relying\non any pre/post-processing steps.\nTo further analyze the performance of our model, we conduct\nan ablation study by modifying the standard ViT [35] architec-\nture. Specifically, we investigate the impact of different mask\nstrategies and hyperparameters on the READ2016 dataset and\nexamine how the SAM optimizer [55] affects our model’s per-\nformance.\n4.1. Dataset and evaluation metrics\nWe evaluated our model’s performance on three commonly\nused datasets for handwritten text recognition: LAM [ 34],\nREAD2016 [ 11], and IAM [ 33]. Among these datasets,\nREAD2016 and IAM are widely recognized as benchmarks\nfor handwritten text recognition, while LAM is currently the\nlargest available line-level handwritten text recognition dataset.\nThe information about the datasets is provided in Table 1. Note\nthat we report the performance on the test set with the model\nachieving the best performance on the validation sets.\nLAM [34]. The Ludovico Antonio Muratori (LAM) dataset is a\nmassive handwritten text recognition dataset of Italian ancient\nmanuscripts, which was edited by a single author over a span of\n60 years. It consists of a total of 25,823 lines and has a lexicon\nof over 23,000 unique words. The dataset is split into 19,830\nlines for training, 2,470 lines for validation, and 3,523 lines for\ntesting, with a charset size of 89. The dataset was annotated\nat the line level, with each line’s bounding box and diplomatic\ntranscription provided. During the transcription process, stroke-\nout text, illegible words due to stains and scratches, and special\nsymbols not representable in Unicode were replaced with the #\nsymbol. This is currently the largest line-level handwritten text\nrecognition dataset available and could be an ideal choice for\ndemonstrating the potential of our model.\nREAD2016 [11]. READ2016 was proposed in the ICFHR 2016\ncompetition on handwritten text recognition. It comprises a sub-\nset of the Ratsprotokolle collection used in the READ project,\nwith color images representing Early Modern German handwrit-\ning. The dataset provides segmentation at the page, paragraph,\nand line levels. For line-level tasks, the dataset has a total of\n8349 training images, 1040 validation images, and 1138 test\nimages, with a character set size of 89.\nIAM [33]. IAM is a well-known offline handwriting benchmark\ndataset for modern English. It comprises 1,539 scanned text\npages of English texts extracted from the LOB corpus, which\nwere handwritten by 657 different writers. The training set of\nIAM has 747 documents (6,482 lines), the validation set has\n116 documents (976 lines), and the test set has 336 documents\n(2,915 lines). The IAM dataset consists of grayscale images\nof English handwriting with a resolution of 300 dpi. In this\nwork, we utilized the line level with the commonly used split, as\ndescribed in Table 1.\n4\nMethod Test CER Test WER Param.\nCNN + BLSTM⋆ [36] 5.8 18.4 9.3M\nGFCN⋆ [20, 34] 5.2 18.5 1.4M\nCRNN⋆ [22, 34] 3.8 12.9 18.2M\nOrigamiNet-12⋆ [4, 34] 3.1 11.2 39.0M\nOrigamiNet-18⋆ [4, 34] 3.1 11.1 77.1M\nOrigamiNet-24⋆ [4, 34] 3.0 11.0 115.3M\nTransformer-based models\nViT [35] 6.1 19.1 37M\nViT + DropKey [35, 51] 5.7 16.5 37M\nDeiT [30] 5.9 18.7 6M\nTransformer§⋆ [48, 34] 10.2 22.0 54.7M\nTrOCR§⋆ [18, 34] 3.6 11.6 385.0M\nHTR-VT 2.8 7.4 53.5M\n§reports results using extra training data.\n⋆ indicates re-implementations by LAM [34].\nTable 2: Comparison with state-of-the-art approaches on LAM [34] dataset\n(19,830 training samples).We outperform all the competitive approaches with\na clear margin. The improvement is more important for the transformer-based\napproaches.\nMethod Test CER Test WER Param.\nCNN + RNN [11] 5.1 21.1 -\nCNN + BLSTM [17] 4.7 - -\nFCN [24] 4.6 21.1 19.2M\nV AN [2] 4.1 16.3 2.7M\nTransformer-based models\nViT [35] 8.5 29.6 37M\nViT + DropKey [35, 51] 8.1 26.4 37M\nDeiT [30] 8.4 28.7 6M\nDAN [1] 4.1 17.6 7.6M\nHTR-VT 3.9 16.5 53.5M\nTable 3: Comparison with state-of-the-art approaches on READ2016 [11]\ndataset (8,349 training samples).We achieve comparable performance.\n4.2. Comparison with state-of-the-art approaches\nEvaluation metrics. We use Character Error Rate (CER) and\nWord Error Rate (WER) as performance measures. CER is cal-\nculated as the Levenshtein distance between two strings, which\nis the sum of character substitutions (SUBc), insertions (INSc),\nand deletions (DELc) required to transform one string into the\nother, divided by the total number of characters in the ground\ntruth (GTc). Formally, CER is given by:\nCER = SUBc + INSc + DELc\nGTc\n. (3)\nSimilarly, WER is calculated as the sum of word substitutions\n(SUBw), insertions ( INSw), and deletions ( DELw) needed to\ntransform one string into another, divided by the total number\nof words in the ground truth ( GTw). Mathematically, WER is\nexpressed as:\nWER = SUBw + INSw + DELw\nGTw\n(4)\nWe conducted a comparative study of current state-of-the-art\nmethods on the LAM [ 34], READ2016 [ 11], and IAM [ 33]\ndatasets respectively. Our approach surpassed previous state-\nof-the-art models on the LAM [ 34] and IAM [ 33] datasets\nand achieved comparable performance on the READ2016 [11]\ndataset. The results presented in Tables 2, 3 and 4 were achieved\nwithout the utilization of any external language models, such as\nMethod Test CER Test WER Param.\nGFCN [20] 8.0 28.6 1.4M\nGFCN⋆ [20, 34] 8.0 28.6 1.4M\nCRNN⋆ [22, 34] 7.8 27.8 18.2M\nCNN + BLSTM [36] 8.3 24.9 9.3M\nCNN + BLSTM⋆ [36, 34] 7.7 26.3 9.3M\nOrigamiNet-12 [4] 5.3 - 39.0M\nOrigamiNet-12⋆ [4, 34] 6.0 22.3 39.0M\nOrigamiNet-18 [4] 4.8 - 77.1M\nOrigamiNet-18⋆ [4, 34] 6.6 24.2 77.1M\nOrigamiNet-24 [4] 4.8 - 115.3M\nOrigamiNet-24⋆ [4, 34] 6.5 23.9 115.3M\nV AN [2] 5.0 16.3 2.7M\nTransformer-based models\nViT [35] 32.4 68.5 37.0M\nViT + DropKey [35, 51] 34.2 70.1 37.0M\nDeiT [30] 32.0 68.4 6.0M\nTransformer§[48] 4.7 15.5 54.7M\nTransformer♠[48, 46] 7.6 24.5 54.7M\nTrOCR§[18] 3.4 - 385.0M\nTrOCR⋆ [18, 34] 7.3 37.5 385.0M\nHTR-VT 4.7 14.9 53.5M\n§reports results using extra training data.\n⋆ and ♠indicate re-implementations by LAM [34] and by [46].\nTable 4: Comparison with state-of-the-art approaches on the test set of\nIAM [33] dataset (6,482 training samples).Our approach exceeded the previ-\nous state-of-the-art model.\nn-grams or similar techniques. Specifically, on the LAM [ 34]\ndataset, our method achieved a CER of 2.8 and a WER of 7.4,\noutperforming all models tested on this dataset. On the IAM\n[33] dataset, our approach exceeded the previous state-of-the-art\nmodel, V AN [2], with a CER improvement of 0.3 and a WER im-\nprovement of 1.4. On the READ2016 [11] dataset, our method\nreached a CER of 3.9, surpassing the state-of-the-art method\nV AN [2] and DAN [1] by 0.2, and closely matching its WER.\nFurthermore, when compared to all transformer-based meth-\nods, our approach consistently led the field, except on the IAM\ndataset [33] where TrOCR [18] achieved a CER of 3.4. However,\nit is noteworthy that TrOCR [18] uses pre-trained CV and NLP\nmodels and a large-scale synthetic dataset, which is not publicly\navailable, to pre-train their model. Transformer [48] also relies\non a large amount of synthetic data for training. Despite this,\nour method still outperforms it. In addition, we also conduct\na fair comparison to two recent works on data-efficient trans-\nformers: DeiT [30] and DropKey [51]. We achieve clearly better\nperformance than them on all three datasets. Training details of\nDeiT [30] and DropKey [51] are provided in the supplementary\nmaterial. These results demonstrate the data-efficiency of our\nproposed model.\nIn summary, our research presents a competitive handwritten\ntext recognition model that stands out against state-of-the-art\nmethods, particularly on the LAM [34] and IAM [33] datasets,\nand competes well on the READ2016 [11] dataset without resort-\ning to any external language models, pre-training or synthetic\ndata commonly used in the field.\nFor many years, the CNN + BLSTM paradigm has been the\ndominant approach in handwritten text recognition. However,\nour proposed method represents a significant shift in this trend,\nmarkedly enhancing the performance of transformer-based mod-\nels. This breakthrough has the potential to steer the entire field of\nhandwritten text recognition toward new and exciting directions.\n5\nMethods LAM [34] IAM [33] READ2016 [11]Val CER Val WER Val CER Val WER Val CER Val WERViT⋆ 5.7 16.7 26.6 57.1 9.4 35.2Ours w/o. CNN extractor 5.5 15.7 20.7 53.5 8.9 33.7Ours w/o. SAM 2.7 7.4 3.4 11.2 4.8 20.1Ours w/o. Span Mask 2.9 7.8 3.7 12.1 5.1 21.9Ours 2.6 6.9 3.3 10.8 4.5 19.4\n⋆ViT is equivalent to our approach without CNN extractor nor Span Mask.\nTable 5: Ablation study of our approach on LAM [34], IAM [33] and\nREAD2016 [11] datasets.We reported the performance of the standard ViT\nand studied the effect of our architecture without CNN feature extractor, SAM,\nSpan Masking, respectively, on the results.\nIAM [33] READ2016 [11]Layers Heads Val CER Val WER Test CER Test WER Val CER Val WER Test CER Test WER8 6 3.6 11.8 5.2 16.2 4.8 20.1 4.2 17.64 6 3.3 10.8 4.7 14.9 4.5 19.4 3.9 16.52 6 3.5 11.4 5.1 16.1 4.3 18.8 3.9 16.71 6 4.1 13.6 6.0 18.9 5.0 21.4 4.8 20.0Layers Heads Val CER Val WER Test CER Test WER Val CER Val WER Test CER Test WER4 8 3.5 11.3 4.9 15.6 4.6 20.0 4.1 17.44 6 3.3 10.8 4.7 14.9 4.5 19.4 3.9 16.54 4 3.3 10.9 4.7 14.8 4.4 18.8 4.1 17.64 2 3.5 11.4 5.1 16.0 4.5 19.5 4.0 17.7\nTable 6: Ablation study of more hyperparameters on IAM [33] and\nREAD2016 [11] datasets.We studied the effect of different transformer encoder\nlayers and attention heads on the results.\n4.3. Ablation studies and visualization analysis\nIn this section, we delve into two core areas of our study:\nablation studies and visualization analysis. The ablation studies\nare comprehensive, examining the impact of key building blocks\nwithin our model and exploring the influence of decoder and\ncritical hyperparameters. These include the masking ratio and\nspan length, as well as the number of transformer encoder and\ndecoder layers and attention heads. The visualization analysis\ngrants us deeper insights into the effectiveness of our span mask\nstrategy. Additionally, we present several qualitative results that\nshowcase the effectiveness of our model.\nWe hope that our research can serve as a solid basis that can\nbe readily and swiftly used by future researchers. For this reason,\nwe have intentionally refrained from incorporating intricate and\nopaque components into our model, which could pose difficulties\nin explanation.\nEffect of CNN feature extractor. We achieved relatively good\nresults on LAM [34] and READ2016 [11] datasets using only\nthe standard ViT encoder. This encouraged us to consider the\nViT architecture as a promising approach for handwritten text\nrecognition tasks. However, we observed that training with the\nViT encoder alone resulted in unstable performance and slow\nconvergence speed on the IAM dataset [33], making it difficult\nto compete with CNN-based models. To improve performance,\nwe introduced a CNN-based feature extractor before the ViT\nencoder to combine the transformer’s global feature extraction\ncapabilities with the CNN’s ability to extract local features via\na strong inductive bias. Our experiments showed that this mod-\nification significantly improved the model’s performance and\nconvergence speed.\nEffect of employing Sharpness-Aware Minimization(SAM) [55]\noptimizer. We found that convergence to a flatter minimum\nFigure 2: Visualization of attention maps with different masking strategies\non IAM dataset.In the original image, we highlight the region corresponding\nto the token of interest with a red bounding box and average the attention across\nall heads. We observe that when no masking or random masking strategy is\nemployed, each token focuses solely on its own information, as indicated by the\nilluminated regions in the image. However, when we apply the span masking\nstrategy, a noticeable shift occurs, allowing the token to attend to a broader range\nof information.\ncan mitigate overfitting in Handwritten Text Recognition (HTR)\nmodels. To facilitate this, we utilized the Sharpness-Aware\nMinimization (SAM) optimizer, which is straightforward to\napply, for locating these flatter minima. Our experimental results\nshow that validation CER and WER on READ2016 increased\nfrom 4.8 to 4.5 and from 20.1 to 19.4 with SAM [55]optimizer,\nindicating that it has a significant impact on HTR tasks.\nEffect of span feature masking. When labeled data is limited,\noverfitting can become problematic for transformer-based mod-\nels. To address this issue, we proposed a new feature masking\nstrategy to reduce overfitting and improve model performance as\ndescribed in section 3.2. As shown in Table 5, the span feature\nmasking provides consistent and clear improvement across all\nthe datasets.\nImpact of different hyperparameters. We investigate the impact\nof different transformer encoder layers and attention heads. The\nresults are illustrated in Table 6. On IAM dataset, taking the\nnumber of layers to 4 and attention heads of 6 achieved the\nbest validation CER and WER. To maintain consistency, we\nemployed this set of parameters across all our experiments. We\nalso investigate the impact of different masking strategies. The\nresults are illustrated in Table 7. We can see masking tokens\n(‘Span Length = 1’ ) or span feature masking strategy ( ‘Span\nLength > 1’ ) improve the performance for most cases. Span\nfeature masking performs better than random masking tokens\nand masking none of the tokens. For hyperparameters, taking\nthe masking ratio of 0.4 and a span length of 8 is optimal, which\nis used for all our experiments. However, larger span lengths\n(16) reduce the performances, possibly due to the inability to\nlearn context-related information.\nImpact of transformer decoder. Similar to TrOCR [18], we em-\nploy a standard transformer decoder and utilize beam search to\nproduce the final output. We utilized our optimal encoder as the\nbaseline to systematically investigate the impact of the decoder\non the overall model performance. The increased number of\n6\nMask Ratio Span Length Val CER Val WER\n0.0 None ⋆ 5.1 21.9\n0.2\n1§\n4.9 20.6\n0.4 4.8 20.0\n0.6 5.1 21.8\n0.2\n4\n4.6 19.8\n0.4 4.7 20.1\n0.6 4.9 20.7\n0.2\n8\n4.6 19.7\n0.4 4.5 19.4\n0.6 4.9 20.9\n0.2\n16\n5.0 21.5\n0.4 5.2 22.6\n0.6 5.3 23.1\n⋆ indicates our approach without any masking strategy.\n§is equivalent to standard random masking.\nTable 7: Ablation study on the masking strategy on READ2016 [11] dataset.\nWe studied the effect of different mask ratios.\nparameters from adding a decoder constrained us to use a batch\nsize of 64 to maintain consistency across all ablations. Our ex-\nperiments with decoders of varying layer counts, as shown in\nTable 8, demonstrated that incorporating a transformer decoder\ndid not facilitate better convergence nor prevent overfitting.\nVisualization of attention maps. In our study, we examine the\nvariations in attention maps when different masking strategies\nare employed in Figure 2. We averaged the attention across all\nheads to generate the attention maps displayed. The detailed\nexplanations are as follows:\nFirstly, our image size is fixed at 64 x 512, which, after patch\nembedding, transforms into a shape of 1 x 128, viewed as 128\ntokens represented by 128 vertical stripes in the figure. The to-\nkens selected for visualization correspond to the areas enclosed\nin red boxes in the original image. In the left image, the letter\n\"o\" is highlighted, while in the right image, it is the letter \"l\".\nAccording to the principle of self-attention, our selected token\nshould pay more attention to other tokens with higher similarity,\nwhich is represented as lighter colors in the attention map. In\nboth no-mask and random-mask scenarios, we can observe that\nin the left image, the letter \"o\" in \"nvasion\" and \"bodies\" is\nhighlighted, and in the right image, the two \"l\" letters in \"will\"\nare illuminated. This indicates that under no mask and random\nmask conditions, attention is mainly focused on the token itself.\nHowever, a significant change is observed in the span masking\nscenario. More areas are noticed, indicating that when using\nspan masking, tokens are able to \"attend to a broader range of\ninformation\". This highlights the effectiveness of span masking\nin enabling tokens to capture more contextual information. The\nimproved contextual awareness provided by span masking facil-\nitates a more comprehensive understanding of the text, which\nis vital for accurate recognition in handwritten text recognition\ntasks.\nThe more examples of the attention maps are provided in the\nsupplementary material.\nComparison of training time. Few methods mention the total\ntime required to complete their training, yet this is extremely im-\nportant for this task. Most approaches that rely on pre-training\nIAM [33]Decoder Layers Val CER Val WER Test CER Test WER Param.0 3.3 10.8 4.7 14.9 53.5M8 - - - - 129.2M4 5.0 15.3 7.7 21.3 91.4M2 5.0 15.1 7.8 21.1 72.5M1 5.3 15.7 8.1 21.6 63.0M\nTable 8: Ablation study of adding decoder and training more iterations on\nIAM [33] dataset.We studied the effect of different add transformer decoder\nlayers and training iterations on the results. When the number of decoder layers\nis 8, the model is hard to converge.\nArchitecture 1k iters Total number of epochs / iters Training time Param. Input size(H x W)GFCN [20] 543 s 186 (Early stop) / 75.6k iters 11.4 h 1.4M 128 x original WV AN [2] 420 s 2100 (Early stop) / 850.4k iters 99.3 h 2.7M original H x WOrigamiNet-24 [4] 476 s 100k iters 13.2 h 115.3M 32 x 600HTR-VT 586 s 100k iters 16.3 h 53.5M 64 x 512\nNote that the reported training times are approximate.\nTable 9: Comparison of the training times across different methods.We have\nre-implemented the above methods to compare training times, using the same\nbatch size of 64.\nor additional data consume significantly expensive computa-\ntional resources. We compared our method with CNN-based\napproaches GFCN [20], V AN [2] and OrigamiNet-24 [4] in Ta-\nble 9. It is important to highlight that the V AN method did not\nresize images to a fixed resolution but instead used the original\nimage pixels from datasets such as IAM [33]. Similarly, GFCN\nmentioned that the experiments for the IAM dataset with an\nimage height of 128px, preserving the original width. This reso-\nlution is much larger than the fixed resolution we used, which is\n512x64. OrigamiNet also used a fixed resolution of 600x32, and\nour approach of using a fixed resolution follows OrigamiNet.\nAs shown in Table 9, our proposed transformer-based method\nremains competitive in terms of training time.\nQualitative results. We provide visual results in Figure 3 for\nIAM [ 33] (First row), READ2016 [ 11] (Second row) and\nLAM [34] (Third row). From this, one can recognize the task\nis challenging, as the visual content present in the text line is\nnot very visible and the background is quite noisy. However,\nour approach can still produce reasonable predictions on these\nexamples. It is worth noting that in the final image, the ground\ntruth label was annotated incorrectly. Despite this error, our\nproposed model was still able to accurately recognize the correct\nhandwritten text from the original image, which demonstrates\nthe robustness and effectiveness of the proposed approach.\n5. Discussion\nAlthough our approach has made notable strides in\ntransformer-based line-level recognition, there is still room for\nimprovement in our current method. The significance of data\naugmentation for handwriting recognition cannot be overstated,\nand we have observed that certain data augmentation methods\npreviously utilized in HTR may have adverse effects. Inves-\ntigating new types of data augmentation specifically tailored\nfor handwriting is a potential direction. Furthermore, delving\ndeeper into mask strategies represents an intriguing avenue for\n7\nFigure 3: Results on example lines from the IAM [33] (First row), READ2016\n[11] (Second row) and LAM [34] (Third row) of the best performing model.\nexploration; learnable mask strategies adapted for handwriting\ncould prove more beneficial. Lastly, expanding from line-level\nto paragraph-level or page-level recognition will be the focus of\nour future research.\n6. Conclusion\nIn this work, we have presented a simple and data-efficient\napproach for handwritten text recognition. With minimal modifi-\ncations to the ViT architecture, we have successfully developed a\nViT-like model that surpasses state-of-the-art performance with-\nout requiring pre-training or additional data. Notably, our ex-\nperiments highlight the remarkable data efficiency of our model\ncompared to ViT and DeiT, while preserving its superior gen-\neralizability even in scenarios with vast amounts of available\ndata. These findings provide a promising direction for improving\nthe performance of handwritten text recognition, particularly in\nlimited data scale settings.\nReferences\n[1] D. Coquenet, C. Chatelain, T. Paquet, Dan: a segmentation-free document\nattention network for handwritten document recognition, IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence (2023).\n[2] D. Coquenet, C. Chatelain, T. Paquet, End-to-end handwritten paragraph\ntext recognition using a vertical attention network, IEEE Transactions on\nPattern Analysis and Machine Intelligence 45 (1) (2022) 508–524.\n[3] T. Bluche, Joint line segmentation and transcription for end-to-end hand-\nwritten paragraph recognition, Advances in neural information processing\nsystems 29 (2016).\n[4] M. Yousef, T. E. Bishop, Origaminet: weakly-supervised, segmentation-\nfree, one-step, full page text recognition by learning to unfold, in: Pro-\nceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2020, pp. 14710–14719.\n[5] X. Yang, E. Yumer, P. Asente, M. Kraley, D. Kifer, C. Lee Giles, Learning\nto extract semantic structure from documents using multimodal fully con-\nvolutional neural networks, in: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2017, pp. 5315–5324.\n[6] A. K. Bhunia, A. Das, A. K. Bhunia, P. S. R. Kishore, P. P. Roy, Hand-\nwriting recognition in low-resource scripts using adversarial learning, in:\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2019, pp. 4767–4776.\n[7] A.-L. Bianne-Bernard, F. Menasri, R. A.-H. Mohamad, C. Mokbel, C. Ker-\nmorvant, L. Likforman-Sulem, Dynamic and contextual information in\nhmm modeling for handwritten word recognition, IEEE transactions on\npattern analysis and machine intelligence 33 (10) (2011) 2066–2080.\n[8] S. Espana-Boquera, M. J. Castro-Bleda, J. Gorbe-Moya, F. Zamora-\nMartinez, Improving offline handwritten text recognition with hybrid\nhmm/ann models, IEEE transactions on pattern analysis and machine\nintelligence 33 (4) (2010) 767–779.\n[9] A. Graves, J. Schmidhuber, Offline handwriting recognition with multi-\ndimensional recurrent neural networks, Advances in neural information\nprocessing systems 21 (2008).\n[10] C. Wigington, C. Tensmeyer, B. Davis, W. Barrett, B. Price, S. Cohen,\nStart, follow, read: End-to-end full-page handwriting recognition, in:\nProceedings of the European conference on computer vision (ECCV),\n2018, pp. 367–383.\n[11] J. A. Sanchez, V . Romero, A. H. Toselli, E. Vidal, Icfhr2016 competition on\nhandwritten text recognition on the read dataset, in: 2016 15th International\nConference on Frontiers in Handwriting Recognition (ICFHR), IEEE,\n2016, pp. 630–635.\n[12] K. He, X. Chen, S. Xie, Y . Li, P. Dollár, R. Girshick, Masked autoencoders\nare scalable vision learners, in: Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2022, pp. 16000–16009.\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\ninformation processing systems 30 (2017).\n[14] A. Graves, S. Fernández, F. Gomez, J. Schmidhuber, Connectionist tem-\nporal classification: labelling unsegmented sequence data with recurrent\nneural networks, in: Proceedings of the 23rd international conference on\nMachine learning, 2006, pp. 369–376.\n[15] P. V oigtlaender, P. Doetsch, H. Ney, Handwriting recognition with large\nmultidimensional long short-term memory recurrent neural networks, in:\n2016 15th international conference on frontiers in handwriting recognition\n(ICFHR), IEEE, 2016, pp. 228–233.\n[16] C. Wigington, S. Stewart, B. Davis, B. Barrett, B. Price, S. Cohen, Data\naugmentation for recognition of handwritten words and lines using a cnn-\nlstm network, in: 2017 14th IAPR international conference on document\nanalysis and recognition (ICDAR), V ol. 1, IEEE, 2017, pp. 639–645.\n[17] J. Michael, R. Labahn, T. Grüning, J. Zöllner, Evaluating sequence-to-\nsequence models for handwritten text recognition, in: 2019 International\nConference on Document Analysis and Recognition (ICDAR), IEEE, 2019,\npp. 1286–1293.\n[18] M. Li, T. Lv, J. Chen, L. Cui, Y . Lu, D. Florencio, C. Zhang, Z. Li, F. Wei,\nTrocr: Transformer-based optical character recognition with pre-trained\nmodels, in: Proceedings of the AAAI Conference on Artificial Intelligence,\nV ol. 37, 2023, pp. 13094–13102.\n[19] C. Wick, J. Zöllner, T. Grüning, Transformer for handwritten text recog-\nnition using bidirectional post-decoding, in: International Conference on\nDocument Analysis and Recognition, Springer, 2021, pp. 112–126.\n[20] D. Coquenet, C. Chatelain, T. Paquet, Recurrence-free unconstrained hand-\nwritten text recognition using gated fully convolutional network, in: 2020\n17th International Conference on Frontiers in Handwriting Recognition\n(ICFHR), IEEE, 2020, pp. 19–24.\n[21] A. El-Yacoubi, M. Gilloux, R. Sabourin, C. Y . Suen, An hmm-based\napproach for off-line unconstrained handwritten word modeling and recog-\nnition, IEEE Transactions on Pattern Analysis and Machine Intelligence\n21 (8) (1999) 752–760.\n[22] B. Shi, X. Bai, C. Yao, An end-to-end trainable neural network for image-\nbased sequence recognition and its application to scene text recognition,\nIEEE transactions on pattern analysis and machine intelligence 39 (11)\n(2016) 2298–2304.\n[23] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog-\nnition, in: Proceedings of the IEEE conference on computer vision and\npattern recognition, 2016, pp. 770–778.\n[24] D. Coquenet, C. Chatelain, T. Paquet, Span: a simple predict & align net-\nwork for handwritten paragraph recognition, in: International Conference\non Document Analysis and Recognition, Springer, 2021, pp. 70–84.\n[25] F. Chollet, Xception: Deep learning with depthwise separable convolutions,\nin: Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 1251–1258.\n[26] P. Doetsch, A. Zeyer, H. Ney, Bidirectional decoder networks for attention-\nbased end-to-end offline handwriting recognition, in: 2016 15th Inter-\nnational Conference on Frontiers in Handwriting Recognition (ICFHR),\nIEEE, 2016, pp. 361–366.\n[27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,\nA. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large scale visual\nrecognition challenge, International journal of computer vision 115 (2015)\n211–252.\n8\n[28] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, Y . Wei, Deformable con-\nvolutional networks, in: Proceedings of the IEEE international conference\non computer vision, 2017, pp. 764–773.\n[29] X. Zhu, H. Hu, S. Lin, J. Dai, Deformable convnets v2: More deformable,\nbetter results, in: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2019, pp. 9308–9316.\n[30] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. Jégou,\nTraining data-efficient image transformers & distillation through attention,\nin: International conference on machine learning, PMLR, 2021, pp. 10347–\n10357.\n[31] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, P. Dollár, Designing\nnetwork design spaces, in: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2020, pp. 10428–10436.\n[32] Y . Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, M. Nadai, Efficient training\nof visual transformers with small datasets, Advances in Neural Information\nProcessing Systems 34 (2021) 23818–23830.\n[33] U.-V . Marti, H. Bunke, The iam-database: an english sentence database\nfor offline handwriting recognition, International Journal on Document\nAnalysis and Recognition 5 (2002) 39–46.\n[34] S. Cascianelli, V . Pippi, M. Maarand, M. Cornia, L. Baraldi, C. Kermor-\nvant, R. Cucchiara, The lam dataset: A novel benchmark for line-level\nhandwritten text recognition, in: 2022 26th International Conference on\nPattern Recognition (ICPR), IEEE, 2022, pp. 1506–1513.\n[35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An\nimage is worth 16x16 words: Transformers for image recognition at scale,\narXiv preprint arXiv:2010.11929 (2020).\n[36] J. Puigcerver, Are multidimensional recurrent layers really necessary for\nhandwritten text recognition?, in: 2017 14th IAPR international conference\non document analysis and recognition (ICDAR), V ol. 1, IEEE, 2017, pp.\n67–72.\n[37] I. Cojocaru, S. Cascianelli, L. Baraldi, M. Corsini, R. Cucchiara, Watch\nyour strokes: Improving handwritten text recognition with deformable con-\nvolutions, in: 2020 25th International Conference on Pattern Recognition\n(ICPR), IEEE, 2021, pp. 6096–6103.\n[38] T. Bluche, R. Messina, Gated convolutional recurrent neural networks for\nmultilingual handwriting recognition, in: 2017 14th IAPR international\nconference on document analysis and recognition (ICDAR), V ol. 1, IEEE,\n2017, pp. 646–651.\n[39] T. Bluche, J. Louradour, R. Messina, Scan, attend and read: End-to-\nend handwritten paragraph recognition with mdlstm attention, in: 2017\n14th IAPR international conference on document analysis and recognition\n(ICDAR), V ol. 1, IEEE, 2017, pp. 1050–1055.\n[40] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of\ndeep bidirectional transformers for language understanding, arXiv preprint\narXiv:1810.04805 (2018).\n[41] D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus), arXiv\npreprint arXiv:1606.08415 (2016).\n[42] J. L. Ba, J. R. Kiros, G. E. Hinton, Layer normalization, arXiv preprint\narXiv:1607.06450 (2016).\n[43] I. Loshchilov, F. Hutter, Decoupled weight decay regularization, arXiv\npreprint arXiv:1711.05101 (2017).\n[44] H. Bao, L. Dong, S. Piao, F. Wei, Beit: Bert pre-training of image trans-\nformers, arXiv preprint arXiv:2106.08254 (2021).\n[45] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, V . Stoyanov, Roberta: A robustly optimized bert pretrain-\ning approach, arXiv preprint arXiv:1907.11692 (2019).\n[46] S. Cascianelli, M. Cornia, L. Baraldi, R. Cucchiara, Boosting modern\nand historical handwritten text recognition with deformable convolutions,\nInternational Journal on Document Analysis and Recognition (IJDAR)\n25 (3) (2022) 207–217.\n[47] M. A. Islam, S. Jia, N. D. Bruce, How much position information do\nconvolutional neural networks encode?, arXiv preprint arXiv:2001.08248\n(2020).\n[48] L. Kang, P. Riba, M. Rusiñol, A. Fornés, M. Villegas, Pay attention to\nwhat you read: Non-recurrent handwritten text-line recognition, Pattern\nRecognition 129 (2022) 108766.\n[49] A. Graves, J. Schmidhuber, Framewise phoneme classification with bidi-\nrectional lstm and other neural network architectures, Neural networks\n18 (5-6) (2005) 602–610.\n[50] M. Yousef, K. F. Hussain, U. S. Mohammed, Accurate, data-efficient,\nunconstrained text recognition with convolutional neural networks, Pattern\nRecognition 108 (2020) 107482.\n[51] B. Li, Y . Hu, X. Nie, C. Han, X. Jiang, T. Guo, L. Liu, Dropkey for vision\ntransformer, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023, pp. 22700–22709.\n[52] K. Song, X. Tan, T. Qin, J. Lu, T.-Y . Liu, Mass: Masked se-\nquence to sequence pre-training for language generation, arXiv preprint\narXiv:1905.02450 (2019).\n[53] M. Joshi, D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, O. Levy, Spanbert:\nImproving pre-training by representing and predicting spans, Transactions\nof the association for computational linguistics 8 (2020) 64–77.\n[54] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-\nscale image recognition, arXiv preprint arXiv:1409.1556 (2014).\n[55] P. Foret, A. Kleiner, H. Mobahi, B. Neyshabur, Sharpness-aware minimiza-\ntion for efficiently improving generalization, in: International Conference\non Learning Representations (ICLR), 2020.\n[56] M. A. Souibgui, S. Biswas, A. Mafla, A. F. Biten, A. Fornés, Y . Kessentini,\nJ. Lladós, L. Gomez, D. Karatzas, Text-diae: A self-supervised degradation\ninvariant autoencoder for text recognition and document enhancement, in:\nproceedings of the AAAI conference on artificial intelligence, V ol. 37,\n2023, pp. 2330–2338.\n[57] M. Dhiaf, A. C. Rouhou, Y . Kessentini, S. B. Salem, Msdoctr-lite: A lite\ntransformer for full page multi-script handwriting recognition, Pattern\nRecognition Letters 169 (2023) 28–34.\n9\nAppendix\nThis appendix contains the following sections:\n• Appendix A: visual results on handwritten text recognition\ndatasets as mentioned in Introduction .\n• Appendix B: more details about our Convolutional Neural\nNetwork(CNN) backbone and ablations on both ResNet\nand VGG. This experiment was mentioned in Section 3.2\nof our paper.\n• Appendix C: more details about the span mask strategy.\nThis experiment was mentioned in Section 3.2of our paper.\n• Appendix D: training details about DeiT and DropKey.\nThis experiment was mentioned in Section 4.2of our paper.\n• Appendix E: some visualization results of attention maps\nas mentioned in Section 4.3of our paper .\nMethods IAM [33]\nTest CER Test WER\nResNet-18 4.7 14.9\nResNet-50 4.9 15.6\nVGG-16 7.2 22.1\nTable .10: Ablation study on using various CNN backbones on IAM [33]\ndataset. We reported the performance of the our approach using different\nbackbones and studied the effect of them. We use ResNet-18 as our final\nsolution.\nAppendix A. Visual results on the IAM [33], READ2016\n[11] and LAM [34] datasets.\nWe show our handwritten text recognition method’s visual re-\nsults on the IAM [33], READ2016 [11], and LAM [34] datasets.\nIAM [33] is a well-known offline handwriting benchmark\ndataset containing 6 482 images for training, 976 images for val-\nidation, and 2 915 images for testing. The image is in grayscale\nand the font has ligatures and some missing parts. Visual results\nare provided in Figure B.4.\nREAD2016 [11] consists of 8 349 train, 1 040 validation, and\n1 138 test images. The image contains a noisy background with\nsome blurry fonts. Visual results are provided in Figure B.5.\nLAM [34] is currently the largest line-level handwritten text\nrecognition dataset that contains 19 830 lines for training, 2\n470 lines for validation, and 3 523 lines for testing. The image\ncontains fonts with stains, some lines of text are skewed and\ninclude both upper and lower characters. Visual results are\nprovided in Figure B.6.\nFigure B.4: Visual results on IAM [33]\nFigure B.5: Visual results on READ2016 [11]\nFigure B.6: Visual results on LAM [34]\nAppendix B. CNN Backbones Ablation\nIn this study, we investigated the impact of different CNN\nbackbones on the overall model performance. We chose the most\nfundamental ResNet [23]and VGG [54]architectures as our CNN\nbackbones, consistent with the simple and easy-to-implement\nprinciples outlined in our paper. The performance of the pro-\nposed method is observed to be robust across various backbones.\nParticularly, ResNet-18 exhibits superior performance compared\nto other backbones.\nAppendix C. Span mask strategy\nThe details of our implementation of the span mask strategy\nare as follows: To achieve the designated mask ratio R (e.g.,\n10\nFigure C.7: Visualization of attention maps\n0.4 of L), we adopt an iterative process of sampling spans. In\neach iteration, we start by defining a maximum span length l\n(i.e., the number of interconnected tokens), and then randomly\nselect the starting point for each span. Noting that the maximum\nspan length is fixed. This means that the length of the sampled\nmasked segments remains the same for each iteration.\nAppendix D. Training details about DeiT [30]and DropKey\n[51]\nWe implemented it completely following the steps in Drop-\nKey [51], moving dropout operations ahead of attention matrix\ncalculation and setting the Key as the dropout unit, yielding a\ndropout-before-softmax scheme. And We set the drop ratio to\n0.1. In DeiT [30], each layer has a dimension of 768 and 6 heads\nas used in our approach. At the same time, we implemented\nDeiT with no distillation.\nAppendix E. Visualization results of attention maps\nIn Figure C.7, we present an extensive set of attention map\nvisualizations that offer valuable insights into the model’s be-\nhavior. We demarcate the region of interest in the original image\ncorresponding to the token under scrutiny using a red bounding\nbox. It is evident that when employing no mask and random\nmask strategies, the attention is highly localized, illuminating\nonly the regions that correspond to the annotated characters in\nthe original image. For instance, in the first visualization, the\nselected token corresponds to the letter ’o’ in the word ’of,’ and\nthe attention map distinctly highlights this specific region. This\nsuggests that, in these scenarios, each token is predominantly\nself-attentive. Conversely, when utilizing a span mask strategy,\nthere is a conspicuous expansion in the illuminated regions, in-\ndicating that the token now engages with a substantially broader\ncontextual landscape.\n11",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.5508537292480469
    },
    {
      "name": "Computer science",
      "score": 0.5470623970031738
    },
    {
      "name": "Transformer",
      "score": 0.5266509056091309
    },
    {
      "name": "Speech recognition",
      "score": 0.4901007115840912
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.47136273980140686
    },
    {
      "name": "Computer vision",
      "score": 0.4321148693561554
    },
    {
      "name": "Engineering",
      "score": 0.12652915716171265
    },
    {
      "name": "Electrical engineering",
      "score": 0.07953289151191711
    },
    {
      "name": "Voltage",
      "score": 0.07557815313339233
    }
  ]
}