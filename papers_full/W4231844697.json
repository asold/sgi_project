{
  "title": "Learning and Evaluating a Differentially Private Pre-trained Language Model",
  "url": "https://openalex.org/W4231844697",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1411875725",
      "name": "Shlomo Hoory",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2789635750",
      "name": "Amir Feder",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2110868788",
      "name": "Avichai Tendler",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A3212513922",
      "name": "Sofia Erell",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A4268837661",
      "name": "Alon Peled-Cohen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2734960329",
      "name": "Itay Laish",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A928169443",
      "name": "Hootan Nakhost",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A1979262825",
      "name": "Uri Stemmer",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A1017474696",
      "name": "Ayelet Benjamini",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A340644367",
      "name": "Avinatan Hassidim",
      "affiliations": [
        "Google (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A1874579918",
      "name": "Yossi Matias",
      "affiliations": [
        "Google (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6601211009",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2970055833",
    "https://openalex.org/W2152460337",
    "https://openalex.org/W3010607409",
    "https://openalex.org/W1985511977",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2594311007",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W3096738375",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2166434810",
    "https://openalex.org/W3025939866",
    "https://openalex.org/W2170540710",
    "https://openalex.org/W2949134592",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2889507104",
    "https://openalex.org/W2905810301",
    "https://openalex.org/W2027595342",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W2077217970",
    "https://openalex.org/W3102378604"
  ],
  "abstract": "Shlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell, Alon Peled-Cohen, Itay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, Yossi Matias. Findings of the Association for Computational Linguistics: EMNLP 2021. 2021.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1178–1189\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n1178\nLearning and Evaluating a Differentially Private Pre-trained Language\nModel\nShlomo Hoory∗, Amir Feder, Avichai Tendler, Alon Cohen, Soﬁa Erell,\nItay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini,\nAvinatan Hassidim and Yossi Matias\nGoogle\nTel Aviv, Israel\n{afeder,tendler,aloncohen,rovinsky}@google.com\nAbstract\nContextual language models have led to sig-\nniﬁcantly better results, especially when pre-\ntrained on the same data as the downstream\ntask. While this additional pre-training usu-\nally improves performance, it can lead to infor-\nmation leakage and therefore risks the privacy\nof individuals mentioned in the training data.\nOne method to guarantee the privacy of such\nindividuals is to train a differentially-private\nlanguage model, but this usually comes at the\nexpense of model performance. Also, in the\nabsence of a differentially private vocabulary\ntraining, it is not possible to modify the vo-\ncabulary to ﬁt the new data, which might fur-\nther degrade results. In this work we bridge\nthese gaps, and provide guidance to future re-\nsearchers and practitioners on how to improve\nprivacy while maintaining good model perfor-\nmance. We introduce a novel differentially\nprivate word-piece algorithm, which allows\ntraining a tailored domain-speciﬁc vocabulary\nwhile maintaining privacy. We then experi-\nment with entity extraction tasks from clinical\nnotes, and demonstrate how to train a differen-\ntially private pre-trained language model (i.e.,\nBERT) with a privacy guarantee ofϵ= 1.1 and\nwith only a small degradation in performance.\nFinally, as it is hard to tell given a privacy pa-\nrameter ϵ what was the effect on the trained\nrepresentation, we present experiments show-\ning that the trained model does not memorize\nprivate information.\n1 Introduction\nRecent advancements in natural language process-\ning (NLP), mainly the introduction of the trans-\nformer architecture and contextual language rep-\nresentations, have led to a surge in the perfor-\nmance and applicability of large language mod-\nels (Vaswani et al., 2017; Devlin et al., 2019).\nSuch models rely on pre-training on massive self-\nlabeled corpora to incorporate knowledge within\n∗Tel-Hai College, Israel. Work was done while at Google.\nthe language representation. Additionally, when\npresented with a new dataset and task, such models\noften gain from an additional pre-training stage,\nwhere they are trained to solve a language model-\ning task on the new training data.\nWhile the pre-training steps are crucial for good\nmodel performance on downstream tasks, it can\ncome at the expense of the privacy of the persons\nmentioned in the data. As these models learn to pre-\ndict words using their context, they often memorize\nindividual words and phrases. Such memorization\ncan lead to information leakage when using the\ntrained models or the language representation. This\nproblem is particularly acute in medical domains,\nwhere sensitive patient data might leak (Hartman\net al., 2020; Feder et al., 2020).\nOne solution for pre-training the model while\npreserving patients’ privacy is to train the model\nwith a differential privacy guarantee (Abadi et al.,\n2016b). Such guarantee is achieved through a train-\ning process which introduces random noise, allow-\ning the modeler to bound the effect an individual\nhas on the model. However, for a sufﬁciently small\nprivacy parameter ϵ, this usually comes at the ex-\npense of model performance. Also, differentially\nprivate training schemes were only shown to work\nfor recurrent language models, and not for more\nrecent systems that are based on the transformer\narchitecture (McMahan et al., 2018; Kerrigan et al.,\n2020).\nApart from their size (110M trainable parameters\nfor BERT), transformer-based language models in-\ntroduce an additional privacy concern. When using\npre-trained language models on new datasets, we\ncan often improve performance by learning a new\ndomain-speciﬁc vocabulary, and re-training the\nmodel with the new tokenizer (Section 5). Unfortu-\nnately, commonly used transformer-based models\nsuch as BERT rely on the WordPiece tokenization\nalgorithm (Wu et al., 2016b), which uses the dis-\ntribution of words in the data and can therefore\n1179\npotentially leak private information as well.\nFinally, even if we successfully train a contex-\ntual embedding model with a sufﬁciently small ϵ\nguarantee, it is hard to test and evaluate the re-\nsulting privacy-preserving properties of the model.\nOne also has difﬁculty understanding whether the\ndifferentially-private training procedure affected\nthe language representation other than by measur-\ning performance on a downstream task. For exam-\nple, it could be that other valuable information was\nalso lost during the differentially private training.\nIn this work, we provide a detailed solution\nto training a differentially-private vocabulary and\ncontextual embedding model, and to better under-\nstanding the resulting representation. We present a\nmethod for training BERT, a contextual embedding\nmodel, on medical data with a strong privacy guar-\nantee of ϵ= 1.1 in total (including the private Word-\nPiece and pre-training algorithms) and with only a\nsmall degradation in performance (Section 2.1). To\ndo that, we introduce the ﬁrst differentially private\nWordPiece algorithm, designed to generate a new\ndomain-speciﬁc vocabulary while maintaining user\nprivacy (Section 3.2). Following that, we success-\nfully generate a differentially private BERT model,\nwhich uses the new vocabulary to improve results\non the downstream tasks.\nPossibly the most major technical challenge in\npre-training a differentially-private contextual em-\nbedding model is the fact that the training batch\nsize has to be very large (128 K), all the while\ntraining on speciﬁc hardware (TPUs) in which the\nbatch size is limited. We overcome this obstacle\nby spreading each training batch over time during\nthe training process, along with other useful ma-\nnipulations we discuss in Section 2.1. Finally, after\ntraining the differentially-private BERT on clinical\nnotes, we follow common wisdom (Carlini et al.,\n2019) and provide privacy tests showing that infor-\nmation leakage has been prevented in this process\n(Section 5). We hope that this work will further im-\nprove user privacy, and will spur more theoretical\nand empirical research in the intersection of differ-\nential privacy and natural language processing.\n2 Previous Work\nSince the introduction of the differentially-private\nStochastic Gradient Descent (SGD) algorithm\n(Song et al., 2013; Abadi et al., 2016b), it is possi-\nble to train deep neural networks (DNN) with pri-\nvacy guarantees. Speciﬁcally, there have been sev-\neral attempts to train DNN-based language models\nwith such guarantees, though with mixed results in\nterms of performance on downstream tasks (McMa-\nhan et al., 2018; Kerrigan et al., 2020). To better\nunderstand the trade-offs between the performance\nand privacy of deep language models, we survey\nhere the literature on differentially-private training\nand on methods for measuring privacy in language\nmodels.\n2.1 Training Differentially-Private Models\nDifferential Privacy (DP; Dwork et al., 2006;\nDwork, 2011; Dwork et al., 2014) is a framework\nthat quantiﬁes the privacy leaked by some random-\nized algorithm accessing a private dataset, reader\nunfamiliar with DP, can consult the short introduc-\ntion in Appendix A. In the context of training a\nmachine learning model on private data, it enables\none to bound the potential privacy leakage when\ndeploying the model to the world.\nDeﬁnition 1 ((ϵ, δ)-DP). Given some ϵ, δ > 0, we\nsay that algorithm Ahas (ϵ, δ)-differential privacy,\nif for any two datasets D , D′ differing in a single\nelement and for all S ⊆Range(A), we have:\nPr[A(D) ∈S] ≤eϵPr[A(D′) ∈S] + δ.\nThe leading method for training models with\nsmall differential privacy parameters ϵ, δis the DP-\nSGD method introduced by Abadi et al. (2016b).\nThe method was subsequently incorporated into\nTensorﬂow’s privacy toolbox with improved pri-\nvacy analysis (Mironov, 2017; Mironov et al.,\n2019). The basic idea behind DP-SGD is to clip\nand add noise to the per-example gradients of the\nloss function during model training. The intuition\nis that such a mechanism guarantees that, for each\nstep, the inﬂuence of each example on the outcome\nis bounded.\nIn the context of NLP, there have been several at-\ntempts to train language models using the DP-SGD\nalgorithm. Speciﬁcally, McMahan et al. (2018) pre-\nsented a pipeline for training differentially-private\nlanguage models based on the recurrent neural net-\nwork (RNN) architecture. While successful on the\nRNN architecture, results on a ﬁne-tuned trans-\nformer, speciﬁcally GPT-2, were shown to be less\nsuccessful in preserving privacy without hurting\ntask performance (Kerrigan et al., 2020). In this\npaper, we present the ﬁrst, as far as we know, suc-\ncessfully trained differentially private BERT model,\nwith a strong privacy guarantee and with only a\nsmall decrease in downstream performance.\n1180\n2.2 Evaluating the Privacy of Language\nModels\nWhile differential privacy training provides privacy\nguarantees (in terms of the privacy parametersϵ, δ),\nit is often hard to evaluate the practical implication\nof such a guarantee. When evaluating language\nmodels it becomes even trickier, as private informa-\ntion might be encoded in speciﬁc phrases contained\nin the text, but it can also be implicitly contained in\nthe language model. In the context of clinical notes,\nfor example, information regarding the linguistic\nstyle of the doctor can be captured and predicted\nfrom linguistic cues in the text itself (Rosenthal\nand McKeown, 2011; Preo¸ tiuc-Pietro et al., 2015;\nCoavoux et al., 2018).\nSong and Raghunathan (2020) studied informa-\ntion leakage from language representations, and\npresented several methods for evaluating the pri-\nvacy preserving qualities of trained language mod-\nels. They provided a taxonomy of adversarial at-\ntacks, differing by the adversary’s access to model’s\ninternal state. Speciﬁcally, they deﬁned member-\nship attacks on language representation, which are\ndesigned to detect memorized information. In this\npaper, we build on the secret sharer membership\ntest, a method for quantitatively assessing the risk\nthat rare or unique training-data sequences are un-\nintentionally memorized by generative sequence\nmodels (Carlini et al., 2019). While not speciﬁcally\ndesigned for language models such as BERT, it ﬁts\nthe DP evaluation setup perfectly. Concretely, in\nthis test a secret sharer plants n identical occur-\nrences of a k-WordPiece token sequence into the\ntrain corpus. The sequence itself consists of i.i.d.\nrandom tokens where the secret is the middle token.\nThe model is then trained on the modiﬁed corpus\nand evaluated for each planted sequence by trying\nto predict the secret token.\nIn Section 5, we show that unlike the original\nBERT model, our trained DP-BERT model does\nnot memorize sequences of words introduced via\nthe secret sharer.\n3 Training Differentially Private\nContextual Language Models\nTraining differentially private language models\nbecomes exceedingly difﬁcult with model size.\nHence, attempting to train a transformer model\nsuch as BERT using the DP-SGD algorithm, with-\nout any modiﬁcations, is bound to result in a sig-\nniﬁcant performance degradation (Kerrigan et al.,\n2020). Moreover, as the WordPiece algorithm, the\nprocess that tokenizes the textual input of BERT,\nis not differentially private, re-training it to ﬁt a\ndomain-speciﬁc vocabulary will not guarantee that\nthere is no information leakage regardless of the\nDP-SGD training. In this section, we formulate the\nproblem of training a DP BERT model on medical\ntext, and explain the process of constructing a dif-\nferentially private vocabulary. We then discuss the\nimportance of parallel training and very large batch\nsizes in training such large language models, and\nprovide a method for sufﬁciently increasing such\ncrucial parameters.\n3.1 Problem Formulation\nWe choose to focus our DP training on entity extrac-\ntion (EE) tasks from medical text, speciﬁcally clini-\ncal notes. Clinical notes include medically relevant\ninformation regarding patients’ conditions, and are\noften used as training data for downstream machine\nlearning tasks (Esteva et al., 2019). However, they\ncan contain private information that might put pa-\ntients at risk (Feder et al., 2020; Hartman et al.,\n2020). For this reason, language models trained on\nsuch datasets must be able to learn domain-relevant\ninformation (such as medical jargon and doctors’\nwriting style) without memorizing private informa-\ntion (Lee et al., 2020).\nTo test our ability to train a DP language model\non clinical notes, we use a BERT model (Devlin\net al., 2019) with specialization to the medical do-\nmain. To this end, the public Wikipedia and Book-\nCorpus datasets (Zhu et al., 2015) used to train\nBERT were amended with the Medical Informa-\ntion Mart for Intensive Care III corpus (Johnson\net al., 2016, MIMIC-III) in order to improve per-\nformance on medical tasks.\nBefore introducing changes designed to guaran-\ntee privacy, let us review the procedure used to\nobtain the Medical BERT model. The available\nresources are the 3 billion word Wikipedia + Book-\nCorpus datasets, and the 712M word MIMIC-III\ncorpus. The training process consists of the follow-\ning three steps:\n(i) Build the vocabulary from the MIMIC-III cor-\npus.\n(ii) Train BERT from scratch on the Wikipedia +\nBookCorpus using the new vocabulary.\n(iii) Continue BERT’s training on the MIMIC-III\ncorpus.\n1181\nThe steps that are susceptible to leaking MIMIC-\nIII data are the ﬁrst, and the third. Therefore, by\nthe composability property of differential privacy\n(Dwork et al., 2014, Theorem 3.16), our problem\nreduces to providing algorithms with satisfactory\nDP guarantees for steps (i) and (iii) without causing\na signiﬁcant performance loss. We discuss these\nproblems in detail in the following two subsections.\n3.2 Constructing a differentially private\nvocabulary\nTransformer-based models commonly tokenize in-\nputs into sub-words using the WordPiece algorithm.\nThe WordPiece algorithm (Wu et al., 2016a) is a\ngeneral method for improving the generalization\nproperties of a language model by tokenizing based\non the most frequent combination of symbols rather\nthan words. While its efﬁcacy is undisputed, it can\nleak private information by memorizing certain to-\nkens in the training data. To prevent such leakage,\nwe modify this algorithm to satisfy DP. We do so\nby introducing noise to the word histogram used in\nits training process.\nThe WordPiece algorithm starts with construct-\ning the word histogram of the corpus. This his-\ntogram is then manipulated to obtain the Word-\nPiece output vocabulary through an iterative pro-\ncess which forms sub-words according to their like-\nlihood. Since DP is robust to post-processing, mak-\ning the input histogram DP is sufﬁcient to guarantee\na DP end-result vocabulary (Dwork et al., 2006).\nOur DP WordPiece algorithm therefore adds noise\nto the histogram with given privacy parameters and\nthen applies the standard WordPiece algorithm.\nThere exist techniques to generate histograms\nwith differential privacy, e.g. Korolova et al. (2009)\nand (Bun et al., 2019). The situation encountered in\nlanguage models is slightly different, since we wish\nto protect not the privacy of a single word in the\nhistogram, but of a larger entity such as an example\nspanning many words. In this work we guarantee\ndifferential privacy at the level of a single training\nexample, N = 256 words, to be consistent with\nthe differential privacy guarantee by the training\nprocess itself.\nGiven a textual dataset over the set of wordsX,\nwe partition the dataset into a sequence D of N-\nword tuples. For each tuple v, we deﬁne its word\nhistogram fv : X →R as:\nfv(x) =\n{\n1, if x ∈Supp(v)\n0, otherwise\nNote that this is not exactly the word histogram\nof the text, since each distinct word is counted\nexactly once, regardless of the number of times\nit appeared in the tuple. This heuristic is useful\nto get a better DP bound and describe below. It\ncan possibly reduce utility and somewhat change\nthe vocab obtained, since it is not the exact word\nhistogram.\nWe use fv to construct an (ϵ, δ)-DP histogram h\nusing the procedure described next. One should\nalso note that the construction holds for a general\nfv, not necessarily the one deﬁned above.\nGiven a collection of datasets D, where each\ndataset D ∈D is a sequence of tuples in XN, a\nfunction f : XN →RX, and some constants C, σ>\n0, we deﬁne a randomized function h : D→ RX\nby the following process:\n1. Set h′(D) = ∑\nv∈D\nf (v).\n2. For all coordinates x ∈Supp(h′) add Gaussian\nnoise N(0, σ2) to the x coordinate in h′(D).\n3. Clip h′ as follows to get h:\nh(D) =\n{\nh′(D), for h′(D) ≥C\n0, otherwise\nNow, using the above deﬁnitions, we can prove\nthat our newly modiﬁed WordPiece algorithm is\nindeed differentially private. Speciﬁcally, the fol-\nlowing theorem holds:\nTheorem 1. With the notations above, let k, m, δ>\n0, ϵ = k\nσ\n√\n2 log (2.5/δ) and C = m + σerf–1(1 –\nδ/2N).\nThen, if ∥f (v)∥2 < k, ∥f (v)∥∞ < m, and\nsupp(f (v)) ⊂supp(v) hold for all v ∈XN, then\nh is (ϵ, δ)-DP .\nProof. Given two neighboring datasetsD, D′ = D∪\n{v} where v ∈XN. We divide the coordinates of v\ninto two sets:\nFor elements in the vector v which already ap-\npear somewhere in D, the construction of h′ is\njust the Gaussian mechanism because the L2-norm\nbound on f , which is ( ϵ, δ/2)-DP as shown in\n(Dwork et al., 2014). Thereforeh is also (ϵ, δ/2)-DP,\nas post processing of h′.\nIf x is an element of X that appears in v but not\nin D, then h′(D)(x) = 0 and\nh′(D′)(x) = f (v)(x) + N(0, σ2) < m + N(0, σ)\n1182\nWhere in the last inequality we used the bound on\n∥f (v)∥∞. This will be clipped unless h′(D′)(x) >\nC = m+σerf–1(1–δ/2N) the probability of which is\nsmaller then Pr[N(0, σ2) > erf–1(1–δ/2N)] = δ/2N.\nBy the requirement on the support of f , there are at\nmost N such coordinates x, so by the union bound\nwe get a non-zero with probability < δ/2. Which\nmakes this part (0, δ/2)-DP.\nSo, summing up the contribution of both parts\nwe get (ϵ, δ)-differential privacy.\nWe apply the theorem with m = 1 as f is at\nmost 1 in all coordinates, and with k =\n√\nN, as\nthe maximal L2-norm is obtained for tuples of N-\ndistinct words and ∥(1, ... , 1)∥2 =\n√\nN. In this\nwork we used N = 256.\nCorollary 1. Let σ > 0, 0 < δ < 1.25 e–3/2, and\nlet C = 1 + σerf–1(1 – δ/N). Then, the above\nprocedure yields an (ϵ, δ)-DP histogram h, with\nϵ=\n√\nN\nσ\n√\n2 log (1.25/δ).\nThe theorem proves the corollary with slightly\nworse bounds: C = 1 + σerf–1(1 – δ/2N) and\nϵ=\n√\nN\nσ\n√\n2 log (2.5/δ). For the proof of the corol-\nlary as stated, which allows us to decrease ϵand\ntherefore improve the privacy guarantee, Appendix\nB.\nParameters for learning a DP-vocabulary In\nthis work we used corollary 1 with N = 256 and\nrequired δ= 10–9. We added a noise with σ= 200,\nas in the corollary we used C = 982.5 and ob-\ntained ϵ= 0.517 (denoted as ϵV in Section 5). We\napplied WordPiece on the DP-histogram, the result-\ning vocabulary had 20, 855 WordPieces, compared\nto 29, 157 when WordPiece was applied to the orig-\ninal histogram.\n3.3 Training a differentially private BERT\nEquipped with a DP trained vocabulary, we can\nnow train our language model. To train a differ-\nentially private contextual embedding model (i.e.\nBERT), we use the DP-SGD method supplied by\nthe TF privacy toolbox (see Section 2.1). The pa-\nrameters of the algorithm are the number of steps,\nbatch-size B, ℓ2-norm-clip C, and the noise mul-\ntiplier σ. To ﬁx notation, we formally deﬁne the\nDP-SGD step, as deﬁned in Abadi et al. (2016b, Al-\ngorithm 1). Given the per-example gradients of the\nloss function g1, ... , gB, the gradient ˜g for passing\nto apply_gradients is deﬁned by:\ngi = gi/ max(1,∥gi∥2/C), for all i; (1)\n˜g = 1\nB\n(∑\ni\ngi + N(0, σ2C2I)\n)\n. (2)\nThe most important parameter of the algorithm\nis the noise multiplier σ – increasing σ directly\ndecreases ϵ; i.e., increases the differential-privacy\nguarantee of the algorithm. On the other hand, it\nharms performance on the target data-set, and thus\na careful choice of σ is necessary to balance the\ntrade-off between privacy and performance. We\nchoose the noise σto be proportional to the square\nroot of the batch size B. This is done in order to\nmake the privacy guarantee oblivious to changes in\nthe batch size B (as one can observe from Eq. (2)).\nThe privacy guarantee is also affected by the num-\nber of training steps (or epochs), but this behavior\nis more gradual since ϵincreases near-linearly in\nthe range of interest. In our experience, the clip\nlevel C is of lesser importance and we ﬁx it to be\n0.01.\nFor any choice of parameters, we upper bound\nthe privacy parameter ϵ using the TF privacy\ntoolbox compute_dp_sgd_privacy function,\nwhere we also use the number of MIMIC examples\nN = 83M. We ﬁx privacy δ to be 10–8, which is\nsmaller than 1/N.\nThe effect of parallelism. In order to make the\ntraining run faster, we use TPUs 1 to parallelize\ntraining by splitting example batches to shards.\nThis mechanism is readily available through Ten-\nsorﬂow (TF; Abadi et al., 2016a), but its effect\nhas to be taken into account when computing the\nbounds on ϵ.\nIn order to understand this effect, let us ﬁrst\nreview the way we incorporate TF privacy into\nthe BERT training procedure. The change\nconsists of changing the loss computation code\nto compute the vector loss (per-example loss),\nand of wrapping the existing Adam weight\ndecay optimizer (Kingma and Ba, 2015), our\noptimizer of choice, by the DP optimizer using\nthe make_gaussian_optimizer_class\nmethod.\nThe subtle point lies in the second change,\nas the optimization is also wrapped by\nCrossShardOptimizer which handles\n1https://cloud.google.com/tpu/docs/\ntpus.\n1183\nthe sharded batching. Let B denote the unsharded\nbatch size, and P denote the number of parallel\nshards. For each batch, the examples are split\nbetween P independent instances of the TF privacy\noptimizer, each handling B/P examples. For each\nshard, the gradients are clipped, averaged and\nnoise is added by equations Eqs. (1) and (2).\nSubsequently, the CrossShardOptimizer\naverages the P shard gradients to obtain the single\ngradient to be passed to apply_gradients.\nTherefore, denoting thei-th gradient of shardj by\ngi,j, the gradient passed to apply_gradients\ncan be written as follows:\n˜g = 1\nP\n∑\nj\n[\n1\nB/P\n(∑\ni\ngi,j + N(0, σ2C2I)\n)]\n= 1\nB\n\n∑\ni,j\ngi,j + N(0, Pσ2C2I)\n\n. (3)\nThis implies that using noise multiplier σwith P\nshards is equivalent to an unsharded training with\nnoise multiplier σ\n√\nP. As computing an upper\nbound on ϵthrough TF privacy does not take paral-\nlelism into account, one must use σ\n√\nP as the noise\nmultiplier in order to get the correct result.\nAchieving larger batch sizes. As it quickly be-\ncame apparent, to successfully train a large trans-\nformer with DP-SGD, larger batch sizes are re-\nquired. However, usually batch size cannot in-\ncrease beyond a certain point because of mem-\nory considerations and limitation on the number of\navailable TPUs. With the resources available to us,\nfor example, we couldn’t get beyond parallelism of\nP = 256 with sharded batch size of 32, achieving\ntotal batch size B = 8192.\nWe chose to solve this problem by spreading the\nbatch in time, so apply_gradients is called\nonly once every T batches with the total average\ngradient. This is equivalent to increasing both P\nand B by a factor of T. With this method, the only\nlimit on T is processing time. From our experience,\nthe value of T = 32 is a reasonable choice, achiev-\ning parallelism of P = 256 ·32 and total batch size\nB of 128k with the above parameters.\nWe brieﬂy remark upon the implementation of\nthis mechanism. For every trainable variable, we\ncreated a variable with /grad_acc sufﬁx added\nto the original name. For each step, thetrain_op\neither accumulates the current gradients in the\nnew variables, or zeros the accumulator and calls\napply_gradients, depending on the current\nstep modulo T.\n4 Experimental Setup\nWe design our experiments to demonstrate the abil-\nity of the DP training scheme to achieve similar\nresults to the non-DP training scheme on the same\ndata. We focus on the medical domain as it has\nstrict privacy requirements and its language is dis-\ntinct enough so that additional pre-training should\nbe useful. We start by describing the data used\nfor the DP training and relevant implementation\ndetails. We then present the entity extraction task\nused for the supervised task training and evaluation.\nFinally, we discuss the relevant baselines, chosen to\ndemonstrate the efﬁcacy of the DP training scheme.\nPre-training data. For the DP pre-training, we\nsupplement the original training data used in Devlin\net al. (2019) with the MIMIC-III dataset, a com-\nmonly used collection of medical information that\ncontains more than 2 million distinct notes (John-\nson et al., 2016; Alsentzer et al., 2019). MIMIC-III\ncovers 38,597 distinct adult patients and 49,785\nhospital admissions between 2001 and 2012. The\nclinical notes in this dataset are widely used by\nNLP researchers for a variety of clinically-related\ntasks (Feder et al., 2020; Hartman et al., 2020), and\nwere previously used for pre-training BERT mod-\nels speciﬁcally for the medical domain (Alsentzer\net al., 2019).\nUsing the combined dataset, we train our DP-\nBERT model using the training scheme described\nin Section 3.\nEntity-extraction task. For the supervised task\ntraining, we used two datasets from the i2b2 Na-\ntional Center for Biomedical Computing for the\nNLP Shared Tasks Challenges: i2b2-2010 and i2b2-\n2011 (Uzuner et al., 2011). These datasets contain\nclinical notes tagged for concepts, assertions, and\nrelations (i2b2-2010 - 170 clinical notes, i2b2-2011\n- 424 clinical notes). In this task, patient reports\nare labeled with three concepts: test, treatment,\nand problem. The total number of entities in each\ncategory can be seen in Table 1.\nThe i2b2-2011 data is split to training (251 notes)\nand test (173 notes) sets. On i2b2-2010, we per-\nform 5-fold cross validation where each fold has\nrandom training (136 notes) and test (34 notes) sets.\n1184\nConcept i2b2-2010 i2b2-2011\nProblem 7, 073 11, 924\nTest 4, 608 8, 071\nTreatment 4, 844 8, 328\nTable 1: Number of Concept entities included in the\ni2b2 2010 and 2011 datasets, for each type of Concept\n(Problem, Test and Treatment).\nBaselines. We compare our differentially private\nBERT model, denoted as BERT-DP, to several non\nprivate baselines:\nBERT (Wikipedia + Books) We train a BERT-\nlarge model, as in Devlin et al. (2019), using\nthe default hyperparameters.\nBERT-M (Wikipedia + Books + MIMIC-III)\nWe supplement the original training from\nDevlin et al. (2019) with the MIMIC-III\nclinical notes corpus. In addition, we also\nuse a (non-differentially private) WordPiece\nvocabulary generated from MIMIC-III.\nBioBERT We use the training data presented in\nLee et al. (2020), and use it to train BERT. We\ntested version v1.1 which it trained using the\noriginal dataset + 1M PubMed abstracts.\nIn Section 5 we compare several differentially\nprivate models, discuss their differences and high-\nlight the effect of certain parameters (as discussed\nin Section 3) on the EE task performance.\n5 Results\nIn this section we empirically evaluate the trade-\noffs between a model’s privacy and its usefulness.\nPreviously, in Section 3, we have shown how to pre-\ntrain a contextual embedding model such as BERT\nwith any, possibly substantial, privacy guarantee.\nWe naturally expect that a stronger privacy guaran-\ntee would entail that less information is preserved\nduring pre-training, which in turn would degrade\nperformance on downstream tasks. Thus, we aim\nto ascertain the exact trade-off between these two\ngoals in order to be able to choose a model that has\nboth good performance and a satisfactory privacy\nguarantee.\nWe provide two sets of experiments to help bet-\nter understand this trade-off as well as to provide\npractitioners with tools to understand the effects\nof DP pre-training. First, we use the pre-trained\nDP model and ﬁne-tune it on the entity extraction\ntask on both i2b2-2010 and i2b2-2011, demonstrat-\ning the ability of the differentially private language\nmodel to beneﬁt from the pre-training step. Then,\nwe test the ability of the model to memorize private\ninformation and show that it is protected against\ncommonly used privacy attacks. Aggregating both\nresults, we argue that medically-relevant informa-\ntion is preserved in the DP model all the while\nprivate information is not revealed.\nFor all our model variants, unless explicitly\nstated otherwise, the parameters are as discussed\nin Section 3, with batch size B = 128k, noise multi-\nplier σ= 2.72, and 1M training steps.\n5.1 Preserving Useful Information\nFor our ﬁrst experiment, we pre-trained a DP BERT\nmodel, and then ﬁne-tuned it on an EE task over\nthe i2b2-2010 and i2b2-2011 datasets. We summa-\nrize our results in Table 2. As can be seen in the\ntable, the additional pre-training either on MIMIC-\nIII (BERT-M) or on PubMed (BioBERT) gives a\nsigniﬁcant boost in performance over the off-the-\nshelf BERT, increasing F1 performance from 76.3\nto 86.8 on i2b2-2010 and from 77.6 to 83.6 on i2b2-\n2011. Importantly, we observe that adding differen-\ntial privacy guaranties, using the hyperparameters\nand training procedure discussed in Section 3, de-\ngrades performance only slightly. Still, as expected,\nF1 performance decreases as privacy guaranties im-\nprove (ϵgets smaller), decreasing by 0.8 and 0.5 (in\nabsolute terms) in F1 performance on i2b2-2010\nand i2b2-2011, respectively. Indeed, the BERT-DP\nwith the smallest epsilon (ϵV + ϵT = 1.1) improves\nperformance by 7.4 on i2b2-2010 and 3.5 on i2b2-\n2011 (in absolute terms).\nModel ϵV ϵT i2b2-2010 i2b2-2011\nBERT ∞ ∞ 76.3 77.6\nBERT-M ∞ ∞ 86.8 83.6\nBioBERT ∞ ∞ 86.5 –\nBERT-DP 0.51 2.8 84.5 81.7\nBERT-DP 0.51 0.6 83.7 81.2\nTable 2: Results on the Medical Entity Extraction task\non both the i2b2-2010 and i2b2-2011 datasets. ϵ = ∞\ndenotes no differential privacy guarantee. BERT-DP\nwith the best privacy guarantee (ϵ= 1.1) highlighted in\nbold.\nIn addition, in Fig. 1 we evaluate the change in\nthe DP-SGD ϵ = ϵT and in the F1 score of the\ndownstream task as a function of the batch size, the\nnoise multiplier σ, and the number of pre-training\nepochs. The behavior in all three parameters is as\nexpected. Speciﬁcally, increasing σenables more\n1185\nFigure 1: Top to bottom - DP-SGD privacy parameterϵ(red) and test F1 score on the i2b2-2010 EE task (blue), as\na function of: noise multiplier σ; number of pre-training epochs; pre-training batch size.\nprivacy (lower ϵ), but worsens performance. Sim-\nilarly, with more pre-training epochs, the model\ngathers more information about the training data,\nand so obtain better F1 score but worse privacy\npreservation (higher ϵ). When increasing the batch\nsize without modifying the noise multiplier σpro-\nportionally, both ϵand the F1 increase. To summa-\nrize, based on the results in Fig. 1, we recommend\npractitioners interested in generating DP models to\nopt for very large batch sizes and train for as many\nepochs as their target ϵallows them.\n5.2 Forgetting Private Information\nFor our second batch of experiments, we follow\nCarlini et al. (2019) to test the model’s ability\nto memorize private information. We inject the\nMIMIC-III data set with “canaries”, where canary\nCk,p is a length k sequence of random word pieces\nthat is injected into a random location for each train-\ning example with probability p. For each canary,\none word piece is regarded as the secret, while the\nothers as hints. We evaluate a model trained on the\ninjected MIMIC-III data set on the same training\nexamples while masking the secret and using the\nmasked language model task to evaluate the true\nsecret rank. We measure how well the model mem-\norizes the secret by the exposure metric deﬁned as\nlog2(|vocab|) – log2(average secret rank).\nWe tested the HS, HSH, and HHSHH canary\nhint/secret patterns for different values of p on a\nDP model and a non-DP model. As can be seen\nfrom Fig. 2, even when the secret appears as much\nas 100K times in the data, the DP model performs\nsigniﬁcantly better than the non-DP model. This\nsuggests that the model learns through information\nthat helps it generalize rather than memorize the\ndataset in its entirety, which includes private and\npersonal information as well.\n6 Discussion and Future Work\nIn this paper, we have shown a procedure for learn-\ning and evaluating a differentially-private contex-\ntual language model. We have deﬁned the prob-\nlem of learning such a model with end-to-end pri-\nvacy guarantees and have discussed the pitfalls that\nmight lead to poor downstream performance. Al-\nlowing for vocabulary modiﬁcations, we have in-\ntroduced a novel WordPiece algorithm and proved\n1186\nFigure 2: Secret exposure as a function of the number of secret occurrences. Black lines denote models with DP\nϵ= 1, red lines models without DP ϵ= ∞. bs denotes batch size, step denotes step size and HHSHH/HSH/HS the\nhint pattern used.\nthat it is differentially-private. Then, to overcome\nthe difﬁculties associated with learning DP contex-\ntual language models, we have offered practical\nmeasures for circumventing them, most notably\nthrough vastly increasing batch sizes. Finally, to\nincrease the trust of the DP trained contextual lan-\nguage model, we have utilized a secret sharer eval-\nuation test and showed that our trained language\nmodel does not memorize private information.\nWhile these results are deﬁnitely encouraging,\nmore research is needed. Our results are conﬁned\nto the medical domain, where privacy needs are per-\nhaps most stringent. Showing the efﬁcacy of this\ntraining and evaluation pipeline on other domains\nwould certainly increase the trust in it. Addition-\nally, we have not fully explored potentially tighter\nbounds on our DP WordPiece algorithm. In future\nwork, we plan to provide more theoretical and em-\npirical support for end-to-end privacy guarantees.\nFinally, the observed performance gain due to\nthe vocabulary training presents an interesting ques-\ntion for the larger NLP community. Understanding\nthe importance of vocabulary vs. linguistic style\nwhen performing additional pre-training could im-\nprove the domain adaptation capabilities of existing\nNLP systems. In future work, we plan to expand\nour DP training to additional domains, allowing\nus to test the power of vocabulary modiﬁcations\nvia the DP WordPiece training in increasing across\ndomain performance.\nReferences\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S Cor-\nrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\net al. 2016a. Tensorﬂow: Large-scale machine learn-\ning on heterogeneous distributed systems. arXiv\npreprint arXiv:1603.04467.\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016b. Deep learning with differential\nprivacy. In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Se-\ncurity, pages 308–318.\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, W A Red-\nmond, and Matthew BA McDermott. 2019. Publicly\navailable clinical bert embeddings. NAACL HLT\n2019, page 72.\nMark Bun, Kobbi Nissim, and Uri Stemmer. 2019. Si-\nmultaneous private learning of multiple concepts. J.\nMach. Learn. Res., 20:94–1.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In 28th {USENIX} Security Sympo-\nsium ({USENIX} Security 19), pages 267–284.\nMaximin Coavoux, Shashi Narayan, and Shay B Co-\nhen. 2018. Privacy-preserving neural representa-\ntions of text. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1–10.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\n1187\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nCynthia Dwork. 2011. A ﬁrm foundation for pri-\nvate data analysis. Communications of the ACM ,\n54(1):86–95.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. 2006. Calibrating noise to sensitivity\nin private data analysis. In Theory of cryptography\nconference, pages 265–284. Springer.\nCynthia Dwork, Aaron Roth, et al. 2014. The algo-\nrithmic foundations of differential privacy. Founda-\ntions and Trends in Theoretical Computer Science ,\n9(3-4):211–407.\nAndre Esteva, Alexandre Robicquet, Bharath Ramsun-\ndar, V olodymyr Kuleshov, Mark DePristo, Katherine\nChou, Claire Cui, Greg Corrado, Sebastian Thrun,\nand Jeff Dean. 2019. A guide to deep learning in\nhealthcare. Nature medicine, 25(1):24–29.\nAmir Feder, Danny Vainstein, Roni Rosenfeld, Tzvika\nHartman, Avinatan Hassidim, and Yossi Matias.\n2020. Active deep learning to detect demographic\ntraits in free-form clinical notes. Journal of Biomed-\nical Informatics, 107:103436.\nTzvika Hartman, Michael D Howell, Jeff Dean,\nShlomo Hoory, Ronit Slyper, Itay Laish, Oren\nGilon, Danny Vainstein, Greg Corrado, Katherine\nChou, et al. 2020. Customization scenarios for de-\nidentiﬁcation of clinical notes. BMC medical infor-\nmatics and decision making, 20(1):1–9.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-Wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntiﬁc data, 3(1):1–9.\nGavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020.\nDifferentially private language models beneﬁt from\npublic pre-training. In Proceedings of the Second\nWorkshop on Privacy in NLP, pages 39–45.\nDiederik P Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. In ICLR\n(Poster).\nAleksandra Korolova, Krishnaram Kenthapadi, Nina\nMishra, and Alexandros Ntoulas. 2009. Releasing\nsearch queries and clicks privately. In WWW, pages\n171–180. ACM.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models. In International Confer-\nence on Learning Representations.\nIlya Mironov. 2017. R’enyi differential privacy. In\n2017 IEEE 30th Computer Security Foundations\nSymposium (CSF), pages 263–275. IEEE.\nIlya Mironov, Kunal Talwar, and Li Zhang. 2019.\nR’enyi differential privacy of the sampled gaussian\nmechanism. arXiv preprint arXiv:1908.10530.\nDaniel Preo¸ tiuc-Pietro, Vasileios Lampos, and Niko-\nlaos Aletras. 2015. An analysis of the user occupa-\ntional class through twitter content. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1754–1764.\nSara Rosenthal and Kathleen McKeown. 2011. Age\nprediction in blogs: A study of style, content, and\nonline behavior in pre-and post-social media genera-\ntions. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 763–772.\nCongzheng Song and Ananth Raghunathan. 2020. In-\nformation leakage in embedding models. In Pro-\nceedings of the 2020 ACM SIGSAC Conference\non Computer and Communications Security , pages\n377–390.\nShuang Song, Kamalika Chaudhuri, and Anand D Sar-\nwate. 2013. Stochastic gradient descent with dif-\nferentially private updates. In 2013 IEEE Global\nConference on Signal and Information Processing ,\npages 245–248. IEEE.\nÖzlem Uzuner, Brett R South, Shuying Shen, and\nScott L DuVall. 2011. 2010 i2b2/va challenge on\nconcepts, assertions, and relations in clinical text.\nJournal of the American Medical Informatics Asso-\nciation, 18(5):552–556.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nY . Wu, M. Schuster, Z. Chen, Q.V . Le, M. Norouzi,\nW. Macherey, M. Krikun, Y . Cao, Q. Gao,\nK. Macherey, and J. Klingner. 2016a. Google’s neu-\nral machine translation system: Bridging the gap\nbetween human and machine translation. arXiv\npreprint arXiv:1609.08144.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016b. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\n1188\nYukun Zhu, Ryan Kiros, Richard S Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In ICCV.\nA A short introduction to differential\nprivacy.\nThe results in this section are standard in differen-\ntial privacy (DP), and will be stated without proofs,\nfor more details and proofs consult e.g. Dwork\net al., 2006; Dwork, 2011; Dwork et al., 2014).\nA.1 Deﬁnition of DP\nSuppose we have a dataset which holds private in-\nformation about individuals. We wish to obtain\nsome information about the dataset, for example\ndescriptive statistics without revealing private in-\nformation about the individuals.\nNot revealing private information is informal,\nDP formalizes this concept by requiring that adding\nor removing any individual from a dataset, will\nnot change signiﬁcantly any probability computed\nfrom the information provided. The word \"prob-\nability\" suggests that DP makes sense only in the\nframework of randomized algorithms, that is why\ntypically in DP one adds noise to the algorithms.\nThe formal deﬁnition of DP goes as follows:\nDeﬁnition 2 (ϵ-DP). Given ϵ> 0, we say that the\nrandomized algorithm Ahas ϵ-DP , if for any two\ndatasets D, D′ differing in a single element and for\nall S ⊆Range(A), we have:\nPr[A(D) ∈S] ≤eϵPr[A(D′) ∈S].\nIn other words, the ratio between the probabil-\nities to obtain some result with or without the in-\ndividual, is bounded by a small factor. This deﬁni-\ntion is sometimes too strict, because the condition\nneeds to be satisﬁed even on very rare events. Most\nDP papers, including this one, works with an ap-\nproximate DP deﬁnition which allows the above\ndeﬁnition to fail with small probability δ, more\nformally:\nDeﬁnition 3 ((ϵ, δ)-DP). Given ϵ, δ > 0 , we say\nthat the randomized algorithm Ahas (ϵ, δ)-DP , if\nfor any two datasets D , D′ differing in a single\nelement and for all S ⊆Range(A), we have:\nPr[A(D) ∈S] ≤eϵPr[A(D′) ∈S] + δ.\nTypically δis taken smaller than 1/dataset-size,\nto avoid failures of the deﬁnition.\nA.2 Example: counting with DP.\nSuppose we have a group D of people and we wish\nﬁnd how many of them has a disease, we deﬁne\np(x) = 1 if the person x has the disease and p(x) = 0\notherwise.\nOne method is to release the count directly:\nA(D) =\n∑\nx∈D\np(x)\nThis method is not DP. Indeed, suppose A(D) = N,\nor Pr[A(D) = N] = 1. Let D′ be obtained by adding\na person with the disease to D, then A(D′) = N + 1,\nor or Pr[A(D) = N] = 0. Hence for any ϵ> 0:\n1 = Pr[A(D) ∈{N}] > eϵPr[A(D′) ∈{N}] = 0\nAnd the DP deﬁnition is not satisﬁed.\nTo obtain a DP version of this count, we can\nuse the Laplace mechanism. Consider the Laplace\ndistribution with density function: Lapb(x) =\n1\n2b exp(|x|\nb ) We can add noise to the above algorithm\nmaking it ϵ-DP as follows:\nA′(D) = A(D) + Lap1/ϵ(x) =\n∑\nx∈D\np(x) + Lap1/ϵ(x)\nIndeed for allD, D′, x and since |A(D)–A(D′)| ≤\n1:\nPr[A′(D) = x]\nPr[A′(D′) = x] = Pr[A(D) + Lap1/ϵ+ = x]\nPr[A(D′) + Lap1/ϵ = x] =\n=\nϵ\n2 exp(ϵ|x – A(D)|)\nϵ\n2 exp(ϵ|x – A(D′)| =\n= exp(ϵ(|x – A(D)| – |x – A(D′)|)) < exp(ϵ)\nA.3 Useful properties of DP.\nDP is robust to post processing, in other words any\nprocess applied to the result of a DP algorithm is\nstill DP. Formally:\nTheorem 2. Let Abe (ϵ, δ)-DP algorithm and let f\nbe any (possibly randomized) function on the range\nof A, then the composition f ◦Ais also (ϵ, δ)-DP .\nFor example, in this work we used the robustness\nto post processing when we stated that clipping the\nresult of the histogram was still DP, because the\noriginal histogram was DP.\nSuppose we apply multiple DP algorithms to\nthe dataset. Can we still say something about the\nprivacy loss in this case? DP behaves nicely with\nrespect to composition:\n1189\nTheorem 3. Suppose A1 ... Ak are all (ϵ, δ)-DP ,\nthen an adaptive composition of them is (kϵ, kδ)-\nDP .\nAdaptive in the above deﬁnition means that the\nalgorithm Ai can make choices based on the out-\ncomes of A1, ... Ai–1.\nFor example, in this work we used the composi-\ntion theorem when took a DP-algorithm to compute\nthe vocab and a separated DP algorithm to train the\nmodel, claiming that the entire process is DP.\nThere are more advanced composition theorem\nfor DP, but these are beyond the scope of this intro-\nduction.\nB A tighter bound on the differential\nprivacy of the vocab.\nIn the main text, we proved a theorem about differ-\nential privacy (DP) of histograms. In the proof we\nbounded separately the contributions of new words\nby an example, and already existing words. Here\nwe will provide stricter analysis for the case we\nused in this paper, by bounding both contributions\ntogether.\nGiven a textual dataset over the set of wordsX,\npartition the dataset into a sequence D of N-word\ntuples. For each tuple v, deﬁne its word histogram\nfv : X →R as:\nfv(x) =\n{\n1, if x ∈Supp(v)\n0, otherwise\nLet Ddenote a set of possible dataset of N-tuples.\nConstruct h : D→ RX as follows:\n1. Set h′(D)(x) = ∑\nv∈D\nfv(x).\n2. For all coordinates x ∈Supp(h′) add Gaussian\nnoise N(0, σ2) to the x coordinate in h′(D).\n3. Clip h′ as follows to get h:\nh(D) =\n{\nh′(D), for h′(D) ≥C\n0, otherwise\nThen we have:\nCorollary 2. Let σ > 0, 0 < δ < 1.25 e–3/2, and\nlet C = 1 + σerf–1(1 – δ/N). Then, the above\nprocedure yields an (ϵ, δ)-DP histogram h, with\nϵ=\n√\nN\nσ\n√\n2 log (1.25/δ).\nProof. Let D, D′ = D ∪{v} ∈ D. We note\nthat if there are ℓ elements x1, ... xℓ ∈ X in\nSupp(v) which are not in Supp(D), then when we\nrestrict fv to Supp(D), its norm can be bounded\nby ∥fv|X–{x1...xℓ}(x)∥2 ≤N – ℓ, where equality is\nachieved when the support of the restriction is a\nsingle element.\nBy (Dwork et al., 2014) We can therefore ob-\ntain ( ϵ, ∆1(ℓ))-DP for the restriction to supp(D)\nwith ϵ = N–ℓ\nσ\n√\n2 log (1.25/∆1(ℓ)) or ∆1(ℓ) =\n1.25exp(–1\n2 ( ϵσ\nN–ℓ)2).\nHence:\n∆1(ℓ) = 1.25(δ/1.25)( N\nN–ℓ )2\nFor each xi the probability to get non-zero count\nis smaller then δ/N. Therefore the part outside\nsupp(D) is (0, ∆2(ℓ)-DP with\n∆2(ℓ) = δℓ/N\nby the union bound.\nTherefore, for any 0 ≤ℓ≤N, we can bound the\nδ-term by:\n∆(ℓ) = ∆1(ℓ) + ∆2(ℓ) = 1.25(δ/1.25)( N\nN–ℓ )2\n+ δℓ/N\nTo simplify notations we denote y = N–ℓ\nN , in order\nto prove (ϵ, δ)-DP, it is enough to show that∆(y) ≤\nδfor all 0 < y ≤1, we have:\n∆(y) = 1.25(δ/1.25)\n1\nx2 + δ(1 – x)\nTaking the derivative:\n∆′(y) = 5( δ\n1.25 )1/y2\nln(1.25/δ)\n2y3 – δ\nAnd the second derivative:\n∆′′(y) = 5( δ\n1.25 )1/y2\nln(1.25/δ)(3y2 + 2ln(δ/1.25))\n2y6\nIf δ < 1.25 e–3/2, we have ∆′′(y) > 0 for 0 <\ny ≤1, we can also see that ∆(y = 1) = δ and\nlim\ny→0+∆(y) = δ, therefore ∆(y) ≤δfor 0 < y ≤1,\nand we proved (ϵ, δ)-DP.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.671711802482605
    },
    {
      "name": "Natural language processing",
      "score": 0.6481882333755493
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5555468201637268
    },
    {
      "name": "Computational linguistics",
      "score": 0.503041684627533
    },
    {
      "name": "Linguistics",
      "score": 0.3642445206642151
    },
    {
      "name": "Philosophy",
      "score": 0.06513562798500061
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210117425",
      "name": "Google (Israel)",
      "country": "IL"
    }
  ],
  "cited_by": 34
}