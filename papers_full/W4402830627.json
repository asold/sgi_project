{
  "title": "UTSRMorph: A Unified Transformer and Superresolution Network for Unsupervised Medical Image Registration",
  "url": "https://openalex.org/W4402830627",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2153518624",
      "name": "Runshi Zhang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2124969377",
      "name": "Hao Mo",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2117030371",
      "name": "Junchen Wang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2944483892",
      "name": "Bimeng Jie",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2114212131",
      "name": "Yang He",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A3168537253",
      "name": "Nenghao Jin",
      "affiliations": [
        "Chinese PLA General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1987767086",
      "name": "Liang Zhu",
      "affiliations": [
        "Chinese PLA General Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2115167851",
    "https://openalex.org/W4386365825",
    "https://openalex.org/W4387492269",
    "https://openalex.org/W2766992393",
    "https://openalex.org/W4295308634",
    "https://openalex.org/W2103857226",
    "https://openalex.org/W2016974693",
    "https://openalex.org/W4377711307",
    "https://openalex.org/W4360821277",
    "https://openalex.org/W4389180445",
    "https://openalex.org/W2787740020",
    "https://openalex.org/W2891631795",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W4391582541",
    "https://openalex.org/W4306914348",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4381597786",
    "https://openalex.org/W4379382445",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4226497331",
    "https://openalex.org/W3136762441",
    "https://openalex.org/W6794262402",
    "https://openalex.org/W4388848421",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W4386083034",
    "https://openalex.org/W4388505138",
    "https://openalex.org/W4379116955",
    "https://openalex.org/W4362601831",
    "https://openalex.org/W4320009770",
    "https://openalex.org/W4319878693",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W4385864519",
    "https://openalex.org/W4284683636",
    "https://openalex.org/W3199416604",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W2134478553",
    "https://openalex.org/W2150534249",
    "https://openalex.org/W4241074797",
    "https://openalex.org/W4225114096",
    "https://openalex.org/W2073366797",
    "https://openalex.org/W1970928383",
    "https://openalex.org/W3092446792",
    "https://openalex.org/W3204825373",
    "https://openalex.org/W4200629649",
    "https://openalex.org/W1983592655",
    "https://openalex.org/W2107956652"
  ],
  "abstract": "Complicated image registration is a key issue in medical image analysis, and deep learning-based methods have achieved better results than traditional methods. The methods include ConvNet-based and Transformer-based methods. Although ConvNets can effectively utilize local information to reduce redundancy via small neighborhood convolution, the limited receptive field results in the inability to capture global dependencies. Transformers can establish long-distance dependencies via a self-attention mechanism; however, the intense calculation of the relationships among all tokens leads to high redundancy. We propose a novel unsupervised image registration method named the unified Transformer and superresolution (UTSRMorph) network, which can enhance feature representation learning in the encoder and generate detailed displacement fields in the decoder to overcome these problems. We first propose a fusion attention block to integrate the advantages of ConvNets and Transformers, which inserts a ConvNet-based channel attention module into a multihead self-attention module. The overlapping attention block, a novel cross-attention method, uses overlapping windows to obtain abundant correlations with match information of a pair of images. Then, the blocks are flexibly stacked into a new powerful encoder. The decoder generation process of a high-resolution deformation displacement field from low-resolution features is considered as a superresolution process. Specifically, the superresolution module was employed to replace interpolation upsampling, which can overcome feature degradation. UTSRMorph was compared to state-of-the-art registration methods in the 3D brain MR (OASIS, IXI) and MR-CT datasets (abdomen, craniomaxillofacial). The qualitative and quantitative results indicate that UTSRMorph achieves relatively better performance. The code and datasets are publicly available at https://github.com/Runshi-Zhang/UTSRMorph.",
  "full_text": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020 1\nUTSRMorph: A Unified Transformer and\nSuperresolution Network for Unsupervised\nMedical Image Registration\nRunshi Zhang, Hao Mo, Junchen Wang, Member, IEEE, Bimeng Jie, Y ang He, Nenghao Jin, Liang Zhu\nAbstract— Complicated image registration is a key is-\nsue in medical image analysis, and deep learning-based\nmethods have achieved better results than traditional meth-\nods. The methods include ConvNet-based and Transformer-\nbased methods. Although ConvNets can effectively utilize\nlocal information to reduce redundancy via small neighbor-\nhood convolution, the limited receptive field results in the\ninability to capture global dependencies. Transformers can\nestablish long-distance dependencies via a self-attention\nmechanism; however, the intense calculation of the rela-\ntionships among all tokens leads to high redundancy. We\npropose a novel unsupervised image registration method\nnamed the unified Transformer and superresolution (UTSR-\nMorph) network, which can enhance feature representation\nlearning in the encoder and generate detailed displace-\nment fields in the decoder to overcome these problems.\nWe first propose a fusion attention block to integrate the\nadvantages of ConvNets and Transformers, which inserts a\nConvNet-based channel attention module into a multihead\nself-attention module. The overlapping attention block, a\nnovel cross-attention method, uses overlapping windows\nto obtain abundant correlations with match information of\na pair of images. Then, the blocks are flexibly stacked\ninto a new powerful encoder. The decoder generation pro-\ncess of a high-resolution deformation displacement field\nfrom low-resolution features is considered as a superres-\nolution process. Specifically, the superresolution module\nwas employed to replace interpolation upsampling, which\ncan overcome feature degradation. UTSRMorph was com-\npared to state-of-the-art registration methods in the 3D\nbrain MR (OASIS, IXI) and MR-CT datasets (abdomen, cran-\niomaxillofacial). The qualitative and quantitative results in-\ndicate that UTSRMorph achieves relatively better perfor-\nmance. The code and datasets are publicly available at\nhttps://github.com/Runshi-Zhang/UTSRMorph.\nIndex Terms— Deformable image registration, ConvNets,\nTransformer, Cross-attention, Superresolution.\nI. I NTRODUCTION\nThis work was supported by the National Key Research and De-\nvelopment Program of China (Grant No. 2022YFC2405401), the Nat-\nural Science Foundation of China (Grant No. 62173014 and Grant\nNo. U22A2051), the Natural Science foundation of Beijing Municipality\n(Grant No. L232037). (Corresponding author: Junchen Wang)\nJunchen Wang, Runshi Zhang, Hao Mo are with the\nSchool of Mechanical Engineering and Automation, Beihang\nUniversity, Beijing, China (e-mail: zhangrunshi@buaa.edu.cn,\nwangjunchen@buaa.edu.cn).\nBimeng Jie and Y ang He are with Peking University School and\nHospital of Stomatology, Beijing, China.\nNenghao Jin and Liang Zhu are with Department of Stomatology, The\nFirst Center of Chinese PLA General Hospital, Beijing, China.\nD\nEFORMABLE medical image registration [1] is a fun-\ndamental medical image analysis task. It can estimate\nthe dense nonlinear spatial correspondence between a moving\nimage and a fixed image, and then transforms them into a\ncommon coordinate system [2]. The information that is fused\nfrom mono-/multimodality images can assist surgeons in diag-\nnosing and performing operations, such as tumor invasion [3],\npreoperative planning [4], and intraoperative navigation [5].\nTraditional methods [6], [7] based on iterative computations\nview the optimization match problem with each pair of unseen\nimages as a variational problem that calculates a smooth point-\nto-point mapping between a moving image and a fixed image.\nIn traditional methods, the mapping model’s pattern is given\nas a priori (e.g., affine, spline, etc.), while in deep learning-\nbased methods the mapping is represented by learnable deep\nnetwork’s parameters, which makes the deep learning-based\nmethods have a good representation learning capability.\nWith rapid development, deep learning methods have pro-\nvided many effective technical programs for medical image\nanalysis tasks. In particular, convolutional neural networks\n(ConvNets) and Transformer networks have performed well\nin the medical image community, such as intelligent diagno-\nsis [8], lesion identification [9], and segmentation [10]. Deep\nlearning-based image registration methods [11], [12] have\nachieved better accuracy than traditional methods; additionally,\nthey are several orders of magnitude faster. The methods\ngenerate deformation fields by learning the feature repre-\nsentation of image registration pairs during training and the\ndeformation results can be obtained quickly during testing. The\ninitial medical image registration methods rely on supervised\nground-truth deformation fields, which are usually annotated\nby traditional registration methods. Thus, the accuracy of tradi-\ntional methods limits the performance of supervised methods,\ncausing deformation errors and inferior generalizability. G.\nBalakrishnan [11] proposed an unsupervised learning frame-\nwork based on ConvNets named V oxelMorph to generate a\ndeformation field to overcome this problem. Specifically, a\npair of images is input to the V oxelMorph’s UNet [13]. The\nfeature representation is extracted by the UNet’s encoder;\nthe decoder can combine the multiscale features using skip-\nconnection operations and interpolation upsampling layers to\ngenerate a high-resolution (HR) deformation field. Then, a\nspatial transformation network [14] can warp the moving\nimage to a fixed image by relying on the deformation field.\nResearchers have proposed several improved methods based\narXiv:2410.20348v1  [cs.CV]  27 Oct 2024\n2 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\non ConvNets [12], [15], [16]. Although ConvNets can effec-\ntively extract local information and reduce redundancy within\na small neighborhood, the limited receptive field is inherent.\nConvNets make it difficult for image registration methods to\nestablish long-range information dependency, which represents\nthe relations between one voxel and other far voxels.\nTransformer networks [17] are another essential deep learn-\ning method, and their self-attention mechanisms provide large\nreceptive fields to capture long-range spatial information. It\nestablishes long-distance dependencies between each token\nby calculating the similarity of query-key; thus, Transformer\ncould be suitable for registration tasks [18]. Vision Trans-\nformer (ViT) [19] initially utilized a Transformer in computer\nvision (CV), achieving outstanding performance and extensive\napplication. The input image is split into fixed-size patches\nas tokens, and a learned linear embedding layer is used to\nencode them. After adding a position embedding, a standard\nTransformer block is employed to calculate the relationship\nof each patch. The intense calculation of the similarity of all\ntokens leads to high redundancy [20]. Swin Transformer [21]\n(Swin-T) has significantly improved the computational effi-\nciency and feature representation of Transformers by using\nshifted windows in CV . It also provides unbound cross-window\ndesigns. In medical image registration, TransMorph [22] based\non Swin-T was proposed to obtain the displacement field,\nachieving better performance than V oxelMorph. It benefits\nfrom the feature representation of Swin-T, and TransMorph\nstill suffers from high redundancy. Therefore, combining the\nadvantages of ConvNets and ViTs is crucial to improving the\nrepresentation learning ability of image registration methods.\nIn this paper, we propose a lightweight unified Transformer\n(ConvNet and Transformer) and superresolution (SR) method\nfor medical image registration named UTSRMorph. UTSR-\nMorph uses and reconstructs encoder-decoder designs. The\nencoder comprises our proposed fusion attention blocks (FAB)\nand overlapping attention blocks (OAB). The encoder adopts\nthe advantages of ConvNets and Transformers to balance\nlong-distance information dependence and local information\nutilization. It fully extracts the point-to-point relationships of\nimage pairs through a cross-attention mechanism, enhancing\nits feature extraction ability. In the decoder, the SR module\nis employed to handle the feature degradation of the encoder\nand generate a smoother and more detailed deformation field.\nThe main contributions of this paper are as follows:\n(1) FAB, which combines the self-attention of ViTs and\nthe channel attention of ConvNets, is proposed to enhance\nthe feature representation learning ability. It can accelerate\noptimization while reducing the local redundancy of our\nnetwork based on Transformer.\n(2) OAB, which is different from the nonoverlapping Swin-\nT windows, are designed using a novel cross-attention. The\nabundant correlations with match information of image pairs\nare better integrated through cross-window connections.\n(3) The SR module is used to fuse multiscale features and\ngenerate detailed HR features. It provides much richer and\nmore meaningful feature representations from low-resolution\n(LR) features to the displacement field and reduces the com-\nputational cost.\nTo our knowledge, the lightweight modules which can fully\nintegrate the advantages of ConvNets and ViTs, and adopting\nthe SR approach in deformable medical image registration\nhave not been previously reported. Our proposed method is\ncompared to state-of-the-art (SOTA) methods in two 3D brain\nMR datasets (OASIS, IXI), abdomen MR-CT and our cran-\niomaxillofacial (CMF) tumor MR-CT datasets. Our method\nachieves outstanding performance on several evaluation met-\nrics and demonstrates its effectiveness.\nII. R ELATED WORK\nA. Deep Learning-based Image Registration\nDeep learning-based image registration methods have been\nextensively studied in previous works and consist of ConvNet-\nand Transformer-based methods. In recent years, ConvNet-\nbased methods, which have achieved comparable accuracy\nto traditional methods, have significantly reduced the time\ncost. G. Balakrishnan [11] presented an unsupervised learn-\ning framework (V oxelMorph) for deformable medical image\nregistration. This approach achieved significant breakthroughs\nin terms of registration accuracy and computational costs on\nmany datasets. In their subsequent work [12], organ segmen-\ntation information was used to improve V oxelMorph during\ntraining and significantly increased the registration accuracy.\nB. Kim [23] proposed a cycle-consistent method to retain\nthe original topology. This method can preserve the topology\nduring deformation via implicit regularization. H. Wang [15]\nrelied on the recursive deformable pyramid (RDP) network, a\npure convolutional pyramid, to handle large deformations.\nTransformer-based methods using the self-attention mech-\nanism can establish an interdependent relationship between\neach image pair window. J. Chen [24] adopted simple bridging\nof ViT and V-Net (ViT-V-Net) to increase the registration accu-\nracy of V oxelMorph. In their later work [22], the Swin-T block\nwas used as an encoder named TransMorph. Z. Chen [18]\nproposed TransMatch to input a pair of images separately\ninto the encoder branch and extract features independently.\nAdditionally, the query and key/value windows were sepa-\nrately generated, and a cross-attention mechanism was used\nto calculate the cross-window correlations. J. Zheng [25]\nproposed a novel motion separable backbone and residual\naligner module to address complicated motion patterns by\ncapturing separate motions and disentangling the predicted\ndeformation across multiple neighboring organs.\nIn summary, the above SOTA methods prove that the regis-\ntration accuracy can be improved by a modified encoder, which\nis constructed by ConvNet- or Transformer-based layers. Com-\nplicated upsampling fusion operations in the decoder are also\ncommonly employed in these methods; however, complex\ndesigns can lead to GPU memory explosion. Inspired by these\nmethods, the integration of local features and long-distance\ndependence by ConvNets and Transformers is proposed in this\npaper. Additionally, a lightweight SR module is first used to\nhandle complicated deformations. The modules can generate\nan HR displacement field and decrease model complexity. Our\nproposed method can increase feature representation learning\nability and limit GPU memory usage when inputting large\nimages.\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 3\nB. Combination of ConvNets and ViTs\nConvNets and ViTs have been used to improve representa-\ntion learning of vision backbones in recent works, such as po-\nsition embedding based on ConvNets [26] and replacing linear\nprojection with convolutional projection in self-attention [27].\nIn the CV community, K. Yuan [28] used convolutional\npatches stemming from input images to generate tokens, and a\ndepthwise convolution layer was employed to insert a feedfor-\nward network into a multihead self-attention (MSA) module.\nK. Li [20] proposed UniFormer, which adopted dynamic\nposition embedding and a multihead relation aggregator based\non ConvNets to handle redundancy and dependency. Similarly,\nX. Chen [29] proposed a method to combine ConvNet-based\nand self-attention modules. In deformation medical image\nregistration, ViT-V-Net [24] bridges ViT and ConvNets, yet\nsimple bridging in the encoder layers does not effectively\nutilize the advantages of both. In this paper, we combine\nConvNets and ViTs to reduce the training cost significantly\nand achieve effective representation learning ability.\nC. Cross-Attention Mechanism\nThe query, key, valueof the MSA module in Swin-T\nand ViTs are calculated separately from equal nonoverlap-\nping windows, while the cross-attention mechanism adopts\na more flexible window design. Typically, query and key-\nvalue originate from different feature maps and then generate\nadditional contextual information [30]–[33]. Z. Peng [33]\nproposed an augmented cross-attention unit in a Conformer,\nwhich calculates the cross-attention of the ConvNet and Trans-\nformer branch features and enhances the classification and\nlocalization accuracy. Y . Zheng [32] computed the cross-\nattention value between image patch features with context\nand structure information of the spatial relationship, achieving\nexcellent performance in histopathology whole slide images\n(WSI) analysis. In deformation medical image registration,\nTransMatch [18] used cross-windows from moving images\nand fixed images to calculate cross-attention values and then\nrealized explicit feature matching. In this paper, a different\ncross-attention mechanism was proposed and inserted into the\nencoder. Overlapping windows were used for each window\nfeature with a large receptive field to calculate abundant\ncorrelations with matching information.\nD. Superresolution Upsampling\nThe HR deformation displacement field is generated from\nLR features during the decoder, which is considered an SR\nprocess. The downsampling layer of the backbone inevitably\nleads to feature degradation when transforming HR features\nto LR features. This will be emphasized when restoring\nHR fine-grained features using an interpolation upsampling\nlayer, such as bicubic interpolation. Although interpolation-\nbased upsampling was applied in early SR because of its\nsimplicity, it usually generates much smoother results [34].\nThis may be detrimental to generating the deformation dis-\nplacement field details. Currently, learning-based upsampling\nis more popular. Transposed convolution (TransConv) leads to\ncrosshatch artifacts and fixed, redundant features because of\nzero padding [34]. To overcome this problem, ESPCN [35]\nwas proposed to upscale the LR feature into the HR feature\nusing a subpixel convolution layer. Specifically, the layer\ncontains a channel convolutional layer using an array of\nupscaling filters to obtain feature maps and a pixel-shuffle\nlayer that can rearrange them to an HR output. This approach\nhas been applied in many medical image SR tasks [36]–[38]. In\nthis paper, we propose an SR module to generate fine-grained\nfeatures and deformation fields for medical image registration.\nIII. M ETHOD\nThe framework for unsupervised medical image registration\nis described as [11], [12], [18], and the optimal displacement\nfield is written as\nbϕ = arg min\nϕ\nLsim(f, m◦ ϕ) +λLsmooth(ϕ), (1)\nwhere f and m are the fixed and moving images, respectively.\nϕ is the displacement field that maps m to f. m◦ϕ represents\nthe warped moving image using the displacement field. The\nloss function Lsim(·, ·) can measure the image similarity\nbetween the warped moving and fixed images. Lsmooth(·),\nwith a weight of λ, imposes regularization.\nThe key module in the registration framework is the network\nfor obtaining the displacement field, which is the proposed\nunified Transformer and superresolution (UTSRMorph) net-\nwork. In this section, UTSRMorph is introduced in detail,\nand the overall pipeline is shown in Fig.1. UTSRMorph also\nadopts an architecture design that has been widely used in\nprevious works [22]; the structure is based on UNet [13] and\nSwin-T [21]. The size of the moving and fixed images is\nH × W × D, and the input is 2 × H × W × D. Based on\nViT [19] and Swin-T, the patch partition splits a 3D volume\ninput into nonoverlapping patches treated as a ”token”. The\nresolution and feature dimension of each patch are P ×P ×P\nand 2 × P × P × P. Then, a linear embedding layer is used\nto map to a feature map with arbitrary dimensions ( C).\nz0 = [x1\npE; x2\npE; ...; xN\np E], (2)\nwhere xp ∈ RN×(2P3) is the patch, and the lower case p\nrepresents “patch”. P = 4 in this paper. N = HW D/P3 is\nthe resulting number of patches. E ∈ R2P3×C is a learnable\nlinear embedding layer, and the output is z0 ∈ RN×C.\nThen, four stages with several Transformer blocks are used\non these patch tokens, and the blocks of each stage consist\nof several FABs and an OAB. The linear embedding layer,\nFAB and OAB are referred to as ”stage 1”. In other stages\n(the i − th stage, i = 2, 3, 4), a patch merging layer is\nadded to concatenate the feature representation with 2 ×2 ×2\nneighboring patches. Then the number of tokens is reduced\n(2× downsampling of resolution) by 2 × 2 × 2 = 8to obtain\n8D−dimensional features 2(i+1)C. The features are further\ninput to a linear layer, and the output dimension is set to\n2(i−1)C. FAB uses local and global tokens to combine the\nadvantages of ConvNets for decreasing local redundancy and\nViTs for capturing long-range dependency via self-attention.\nOAB uses an overlapping window instead of the Swin-T\n4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nFig. 1. The unified Transformer and superresolution (UTSRMorph) network include four stages in the encoder and several SR modules in the\ndecoder. One stage consists of FABs and an OAB. The black dashed arrows represent that the moving image and fixed image are concatenated\nas input to the following two optional convolutional blocks.\nwindow to establish cross-window connections, enhancing the\nrepresentative learning ability. The features with unchanging\nresolution H/2(i+1) ×W/2(i+1) ×D/2(i+1) are calculated in\nthese blocks. Finally, four feature maps with the resolutions\nof C × H\n4 × W\n4 × D\n4 , 2C × H\n8 × W\n8 × D\n8 , 4C × H\n16 ×\nW\n16 × D\n16 , and 8C × H\n32 × W\n32 × D\n32 are obtained. Two optional\nconvolutional blocks are used to generate two feature maps\nwith HRs ( C\n2 × H\n2 × W\n2 × D\n2 , 16 × H × W × D) and then\nfully utilize the features with various resolutions. Although\nconvolutional blocks can slightly improve image registration\naccuracy according to [22], they consume considerable GPU\nmemory. The SR module is proposed in this paper to upsample\nfeatures and uses a skip connection to fuse the HR features\nof the encoder. The displacement field is obtained through the\nabove equation. The spatial transformation function [14], [22]\nuses the displacement field to warp the moving image and\ngenerate a registration result.\nIn the following subsection, we first elaborate on FAB and\nOAB in the encoder. Next, we detail how to upsample in the\nSR module of the decoder. Finally, the loss functions in this\nnetwork are introduced.\nA. Fusion Attention Block\nThe standard Swin-T architecture uses window-based self-\nattention to establish relationships between one window (to-\nken) and all other windows, and the shifted window provides\ncross-window connections of nonoverlapping windows. The\nMSA module treats different channel-wise features equally\nand calculates the relationship among the tokens, which can\nlead to high redundancy [20]. To overcome this problem,\nseveral previous works [20], [27], [28] introduced the desirable\nproperties of CNNs (shift, scale and distortion invariance)\nto ViTs. Therefore, we propose FAB to combine MSA and\nconvolutional channel attention (CA) into a standard Swin-\nT block. The CA module transforms the channel-wise global\nspatial information into channel statistics, which are utilized\nto rescale the feature map, hence suppressing redundant in-\nformation [39]. By combining CA with MSA into FAB, our\nmethod can capture channel-wise dependencies effectively\nwhile aggregating local and global features, thus reducing re-\ndundancy. Specifically, the MSA and CA are inserted between\ntwo LayerNorm (LN) layers of the Swin-T block. FAB uses\nthe window-based multihead self-attention (W-MSA) module\nand shifted window-based self-attention (SW-MSA) module\nalternately, similar to Swin-T, and the shift size is set to half\nof the window size. The standard CA module [39] allows the\nmodule to focus on more informative features of all tokens and\ncan extract the channel statistical information among channels.\nThe two FABs are formulated as\nbzl = W-MSA(LN(zl−1)) +αCA(LN(zl−1)) +zl−1, (3)\nzl = MLP(LN(bzl)) +bzl, (4)\nbzl+1 = SW-MSA(LN(zl)) +αCA(LN(zl)) +zl, (5)\nzl+1 = MLP(LN(bzl+1)) +bzl+1, (6)\nwhere bzl and zl are the features that are output by the (S)W-\nMSA and multilayer perceptron (MLP) modules for block l\nof each stage, respectively. α is a small multiplied weight\nof the CA module that regulates the optimization and visual\nrepresentation of FAB. The self-attention is formulated as\nAttention(Q, K, V) =SoftMax(QKT/\n√\nd + B)V, (7)\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 5\nFig. 2. Overlapping window partition of OAB.\nwhere Q, K, V∈ RPxPyPz×d are computed by linear map-\npings named query, key, value. d is the query/key dimen-\nsion. B ∈ RPxPyPz×PxPyPz is the relative position bias [40].\nPx, Py, Pz is the patch size, and PxPyPz represents the\nnumber of patches in a 3D feature. The range of the relative\nposition is [−P + 1, P − 1], P = [Px, Py, Pz]T; then, a bias\nmatrix bB ∈ R(2Px−1)×(2Py−1)×(2Pz−1) is parameterized, and\nthe values in B are taken from bB. Since the (S)W-MSA\nmodule also depends on the features of many token embed-\nding channels to calculate self-attention and enhance visual\nrepresentation, the features are directly computed in the CA\nmodule based on ConvNets, resulting in a high computational\ncost and information redundancy. To solve this problem, we\nuse two convolution layers to compress and recover the feature\nchannel in the CA module with a compression parameter β.\nB. Overlapping Attention Block\nThe cross-attention mechanism [18], [30]–[33] is always\nemployed in the context of Transformer architecture and\nallows more flexible multihead attention computations. It can\nbetter integrate abundant correlations with visual information\nthrough cross-window connections. Our proposed OAB is\nbased on the standard Swin-T and is described as follows:\nbzo = OA(LN(zl+1)) +zl+1, (8)\nzl+2 = MLP(LN(bzo)) +bzo, (9)\nwhere bzo is the output of the OA module. Specifically, Q and\nK/V come from nonoverlapping and overlapping windows,\nrespectively, as shown in Fig.2. IQ, IK, IV are calculated by a\nlinear map layer. IQ is partitioned using HWD\nPxPyPz\nlocal windows\nof size Px × Py × Pz, such as FAB, and then Q is generated,\nwhile HWD\nPxPyPz\noverlapping windows of size Po\nx × Po\ny × Po\nz\nare used to unfold IK, IV , obtaining K, V. The overlapping\nwindow size is defined as\nPo = (1 +ϵ) × P, (10)\nwhere ϵ is an overlapping parameter that can control the\ndegree of window overlap. P = [Px, Py, Pz]T and Po =\n[Po\nx , Po\ny , Po\nz ]T are the window sizes of query and key/value,\nrespectively. This overlapping operation can be specifically\ndescribed as the kernel size of the sliding partition with\nPo\nx ×Po\ny ×Po\nz , and the sliding stride with Px ×Py ×Pz is used\nto partition features. The features are padded to zero to ensure\nconsistent partition windows. The self-attention computing\nsimilarity is expressed as\nAttention(Q, K, V) =SoftMax(QKT/\n√\nd + Bo)V, (11)\nThe relative position bias Bo ∈ RPxPyPz×Po\nx Po\ny Po\nz changed\ncompared to that of FAB because of the different window par-\ntition methods. The range of the relative position is[−P+Po−\n1, 2Po − 3]; then, bB ∈ R(Px+Po\nx −1)×(Py+Po\ny −1)×(Pz+Po\nz −1) is\nparameterized, and the values in Bo are taken from bB. OAB\ncan enlarge the receptive field when calculating key/value\nand promote the utilization of effective information from\nquery, further reducing the optimization time cost and pro-\nviding better visual representation. Note that the local window\ncross-attention (LWCA) module proposed in [18] used a cross-\nattention mechanism in medical image registration. query and\nkey/value computed in the LWCA module are from different\nimages (fixed and moving images), respectively, while OAB\ncalculates them inside each window with a larger field.\nC. Superresolution Upsampling\nThe SR modules are employed to replace the interpola-\ntion upsampling layer and skip-connection operation to over-\ncome image degradation in the encoder. Several previous\nworks [36]–[38] in medical image analysis used the SR\nmodule to decrease subpixel location errors when the LR\nfeatures are upsampled to HR features. An SR module includes\na channel convolutional layer, a pixel-shuffle layer [35], a\nskip-connection operation and two convolutional layers. The\nchannel of the 3D feature map Ni+1 ∈ RC×H×W×D is\nmagnified by the channel convolutional layer by l3C times,\nand l is the upsampling factor. It is formulated as\nh = Conv(Ni+1), (12)\nwhere Conv is the channel convolutional layer and h is its out-\nput h ∈ Rl3C×H×W×D. Multiple features are rearranged by\nthe pixel-shuffle layer into HR features bh ∈ RC×lH×lW×lD.\nIt is shown in Fig.3 and is written as\nbh = PixelShuffle(h, l), (13)\nThe features in the encoder and HR features are concatenated\nvia the skip-connection operation, and the information is fully\nintegrated via two convolutional layers:\nNi = Conv(Conv(Cat(bh, Bi))), (14)\nwhere Bi is the feature obtained by OAB in the encoder. Ni\nshows the fused features.\nD. Loss Functions\nThe loss functions include three parts: The image similarity\nloss measures the similarity between a fixed image and a\nwarped moving image. The deformation field regularization\n6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nFig. 3. Schematic diagram of the pixel-shuffle layer in SR module.\nloss promotes a smoother deformation field. The improvement\nin the registration accuracy of auxiliary segmentation loss\nrelies on the segmentation information of the major organs.\nAccording to Eq. 1, the overall loss is formulated as\nL(f, m, ϕ) =Lsim(f, m◦ ϕ) +λLsmooth(ϕ)\n+γ{Lseg(fs, ms ◦ ϕ)}optional, (15)\nwhere fs and ms are the segmentation masks of the fixed and\nmoving images, respectively. λ and γ are the regularization\nparameter and auxiliary segmentation weight, respectively.\nNote that Lseg is optional.\n1) Image Similarity Loss:Lsim consists of a local normal-\nized cross-correlation metric (LNCC) and mutual information\n(MI) voxelwise between f and m◦ϕ. The LNCC is calculated\nas\nLNCC (f, m, ϕ) =\nX\np∈Ω\n\u0010P\npi △f(pi)([m ◦ ϕ](pi) − [ ˆm ◦ ϕ](p))\n\u00112\n\u0010P\npi\n△f2(pi)\n\u0011\u0010P\npi\n([m ◦ ϕ](pi) − [ ˆm ◦ ϕ](p))2\n\u0011,\n(16)\nwhere pi iterates over a local 3D cube centered at voxel p and\nthe cube length is 9 in these experiments. △f(pi) =f(pi) −\nˆf(p). ˆf(p) = 1\n93\nP\npi f(pi) is the image with local mean\nintensities and ˆm is same. Since a higher LNCC represents\na better registration accuracy, the loss function is Lsim =\n−LNCC .\nThe mutual information (MI) between two random variables\nX, Ycan be defined as follows,\nMI (X, Y) =\nZZ\np(x, y) log p(x, y)\np(x)p(y)dxdy, (17)\nwhere p(x, y), p(x), p(y) are probability density functions. MI\nis known for its difficulty of accurate calculation in continuous\nhigh-dimensional spaces. To address this, researchers have\nproposed various approaches to estimate and optimize its cal-\nculation. For instance, the Parzen window density estimation\nmethod [41] is utilized to derive the probability density and\nsubsequently calculate MI. MI is frequently adopted for cross-\nmodality-related tasks due to the relevance of their statistical\nfeatures.\n2) Deformation Field Regularization Loss:Optimizing Lsim\nmay cause ϕ acute deformation with unrealistic physical\nmotion. Therefore, a diffusion regularizer is defined as\nLsmooth(ϕ) =\nX\np∈Ω\n|| ▽u(p)||2, (18)\nwhere u is the spatial gradient of the displacement field.\n3) Auxiliary Segmentation Loss:The segmentation informa-\ntion can encourage image registration during training but is\nnot needed during testing. The pixel labels used for image\nsegmentation are annotated by professional surgeons or auto-\nmatic segmentation algorithms. The loss is formulated as\nLseg(fs, ms ◦ ϕ) =\n1 − 1\nK\nKX\nk=1\n2 P\np∈Ω fk\ns (p)[mk\ns ◦ ϕ](p)P\np∈Ω(fks (p))2 + P\np∈Ω([mks ◦ ϕ](p))2 , (19)\nwhere fk\ns and mk\ns are the binary mask of annotated struc-\nture/organ k ∈ [1, K] for f and m, respectively. Notably,\nour proposed UTSRMorph is an unsupervised framework and\nauxiliary information can be optionally included to improve\nregistration accuracy during training, such as segmentation\nlabels.\nIV. E XPERIMENTS\nA. Datasets\nWe evaluated our proposed method in datasets including\nbrain, abdomen and CMF tumor. (1) OASIS [42] contains 413\nT1W brain MR images and comes from the 2021 Learn2Reg\nchallenge [16] for interpatient registration. The original MR\nvolumes were preprocessed by the organizing committee using\nFreeSurfer [43] includes skull stripping, spatial normalization,\naffine transformations, and automatic structural segmentation.\nA total of 413 T1W brain MR images were split into 394 and\n19 images for the training set and validation set, respectively,\nwhich is similar to TransMorph [22]. Since the test set was\nnot available, the validation set was used for direct evalua-\ntion. The volumes were cropped to 160 × 192 × 224. 35\nanatomical structures were used as ground truths to measure\nthe registration accuracy. (2) IXI [23] contains 576 T1W brain\nMR images. The preprocessing operation was the same as\nthat for the OASIS dataset. The 576 T1 MR brain volumes\nwere split into 403, 58 and 115 for the training, validation\nand testing sets, respectively. The images were cropped to\n160 × 192 × 224. 30 annotated structures were used for\nevaluation. (3) Abdomen MR-CT [16], a benchmark dataset\nfor CT-MR registration in the Learn2Reg challenge, includes\nunpaired images of 40 MR images, 50 CT images for training\nand 8 MR-CT paired images for testing. The organizers have\nlabeled 3D segmentation masks for the liver, spleen, left\nand right kidney, which can provide auxiliary information\nto enhance registration accuracy during training. The target\narea for registration was cropped by the given regions of\ninterest (ROI) and padded to 192 × 160 × 192, and then\nall images were aligned with a template image through affine\ntransformation during preprocessing. (4) Our CMF tumor MR-\nCT dataset contains 28 image pairs. The age range was 23-88\nyears. The shapes of the CT images were 514 ×513×512, and\nthe spacings were 0.38×0.43×0.43 to 0.52×0.6×0.6 mm. The\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 7\nMR volumes ranged from 24 ×213×256 to 244 ×512×512,\nand the spacings ranged from 0.9 ×0.43×0.43 to 6 ×1.1×1.1\nmm. The regions of interest patches were cropped according to\nthe size of the MRI, then normalized, and affine transformation\nwas used to convert them to the same coordinate system. The\ninput size was 128 ×192×224. Since the facial tumors were\nsmall and the number of segmented organs was relatively\nsmall, 9 landmarks from our previous works [10], [44] were\nannotated by professional surgeons to evaluate registration\nperformance, including the anterior nasal spine (ANS), upper\nincisor midpoint (UI), menton (Me), lowest point of the\nsigmoid notch on the right (SN-R), lowest point of the sigmoid\nnotch on the left (SN-L), mandibular angle on the right (MA-\nR), mandibular angle on the left (MA-L), condyle on the\nright (Co-R), and condyle on the left (Co-L). In each training\niteration, we randomly selected two different volumes from\nthe training set as the fixed and moving images in the brain\nor abdomen datasets. The CMF tumor dataset was trained\nby a pair of MR-CT images. The annotated paired images\nwere input into the trained network during the testing stage.\nThe study was approved by the ethics committee of Peking\nUniversity School and Hospital of Stomatology (reference\nnumber: 2022-03).\nB. Evaluation Metrics\nIn this subsection, several evaluation metrics [22] are in-\ntroduced. (1) The Dice score (DSC) measures the overlap\nof masks with the same organ of warped moving and fixed\nimages. (2) The Hausdorff distance (HD95) describes the\nsimilarity of the anatomical segmentation between the fixed\nimage and the registration result. HD95 represents the 95th\npercentile. (3) The Jacobian matrix indicates the deformation\nfield quality. We calculated the percentage of voxels with\na nonpositive Jacobian determinant ( %|Jϕ ≤ 0|) or the\nstandard deviation of the logarithm of the Jacobian determinant\n(SDlogJ). (4) Target registration error (TRE) represents the Eu-\nclidean distance, which is calculated between the ground-truth\nand registered landmarks. (5) Wilcoxon rank-sum test [45] is\na standard procedure to test the equality of two distributions\nwhich is widely applied in statistics community. The advantage\nof Wilcoxon rank-sum test lies in its weak assumption on the\nunderlying data distribution. The method is used to evaluate\nthe significant differences between our proposed UTSRMorph\nand the baseline methods. (6) Time cost (s) represents the\ninference time cost for testing a pair of images by the models.\n(7) GPU Memory Usage (MB).\nC. Comparison Methods\nWe compared our method with several other SOTA regis-\ntration methods: (1) SyN [46], the publicly available Python\nlibrary ANTsPy was employed, and its transformation type\nwas set to ’SyNOnly’. The iteration numbers were set to\n(160, 80, 40). (2) V oxelMorph [12]. (3) ViT-V-Net [24], the\nbatch size is set to 1 in the abdomen MR-CT and OASIS\ndatasets with segmentation loss, and the batch size of other\ndatasets is set to 2. Note that in the GPU memory usage\ncomparison experiment for ViT-V-Net, the batch size is set to 1\nTABLE I\nTHE RESULTS OF ALL METHODS IN THE TEST SET OF THE IXI DATASET\nMethod DSC(%) HD95 % |Jθ| ≤0 Time Memory\nANTs(SyN) 64.45 ±15.19 6.36 ±3.98 0 113.12\nViT-V-Net 73.49 ±12.59 3.72 ±2.47 1.61 ±0.35 0.22 11423\nV oxelMorph 72.82 ±12.74 3.89 ±2.69 1.63 ±0.33 0.23 16028\nTransMorph 75.11 ±12.35 3.6 ±2.64 1.47 ±0.32 0.22 17667\nRDP 75.47 ±12.76 3.66 ±2.82 0.05 ±0.02 0.9 23911\nTransMatch 76.24 ±12.53 3.01±2.12 0.16±0.08 0.22 21985\nUTSRMorph 76.69±12.56 3.03±2.18 0.32 ±0.13 0.29 19146\nUTSRMorph-L 76.27 ±12.36 3.03 ±2.09 0.26 ±0.11 0.52 23187\nUTSRMorph-S 76.07 ±12.7 3.08 ±2.14 0.27 ±0.11 0.22 18552\nfor fair comparison. (4) LapIRN [47], [48], the latest improved\nmethod with conditional instance normalization is used. (5)\nConvexAdam [49], its feature extractor selects hand-crafted\nmodality independent neighbourhood descriptor (MIND) [50]\nfeatures and the total number of iterations is set to 100.\n(6) NiftyReg [51]. LapIRN, ConvexAdam and NiftyReg are\nonly evaluated in the OASIS and abdomen MR-CT datasets\nsince they were specially designed for the two datasets and\nobtained excellent performance in 2021 Learn2Reg challenge.\n(7) TransMorph [22]. (8) TransMatch [18], λ = 4, and the\nlearning rate was set to 0.0004. (9) RDP [15]. The number of\nRDP channels must be set to 12 in the OASIS, IXI and 10\nin the MR-CT datasets due to GPU memory limitations. All\nthe methods are trained: in the IXI dataset using the LNCC\nloss and L2 smoothed loss with λ = 1; in the abdomen MR-\nCT and CMF tumor MR-CT datasets using MI loss and L2\nsmoothed loss with λ = 1. Additionally, the segmentation loss\nis included in the abdomen MR-CT dataset with λ = γ = 1;\nin the OASIS dataset using the LNCC loss and L2 smoothed\nloss with and without the segmentation loss, λ = γ = 1. The\nbatch size is set to 1 and the learning rate is 0.0001.\nD. Implementation Details\nThree workstations with NVIDIA GeForceRTX 3090Ti or\nNVIDIA GeForceRTX 4090 GPUs were used to train different\ndatasets. The methods were built in Ubuntu 20.04, PyTorch\n1.13.1 and Python 3.8. The Adam optimizer was used to train\nthe methods, and the batch size was 1. The learning rates\nof OASIS and IXI were 0.0001 and 0.0004, respectively, and\nthe maximum number of epochs was 500. We used LNCC,\nLsmooth to train OASIS and IXI, and Lseg was optionally\nemployed in OASIS. λ, γwas 1, 1 in OASIS, and λ was\n4 in IXI. All methods were trained on the MR-CT datasets\nby MI loss. Lseg was used to train in the abdomen dataset,\nλ = γ = 1. The learning rates of the abdomen and CMF\ntumor MR-CT datasets were 0.0002 and 0.0001, respectively,\nand the training was limited to 200,000 and 100,000 itera-\ntions, respectively. The weighted factor α in FAB, channel\ncompression parameter β in the CA module and overlapping\nfactor ϵ in OAB are 0.01, 3 and 0.5, respectively. The window\nsize of OAB was 4, and the head number was 4. We proposed\ndifferent architecture hyperparameters for the method variants,\nincluding UTSRMorph-S, UTSRMorph, and UTSRMorph-L.\nThe embedding dimensions were 48, 96, and 128; the numbers\nof FABs in the third stage were 2, 4, and 18; and the numbers\n8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nTABLE II\nTHE RESULTS OF ALL METHODS IN THE TEST SET OF THE OASIS\nDATASET\nMethod DSC(%) HD95 SDlogJ Time Memory\nInitial 57.18 ±21.26 3.83 ±2.09\nANTs(SyN) 70.53 ±36.09 2.56 ±2.45 0.05 75\nViT-V-Net 79.56 ±16.15 2.12 ±2.08 0.11 ±0.01 1.17 10703\nV oxelMorph 78.65 ±16.8 2.24 ±2.02 0.13 ±0.01 1.14 10223\nNiftyReg 78.84 ±16.31 2.04 ±1.75 0.07 107.78\nLapIRN 80.71 ±15.19 1.94 ±1.9 0.07 1.2 12477\nConvexAdam 74.27 ±17.61 2.41 ±1.98 0.13 1.8 3677\nTransMorph 80.59 ±15.1 1.97 ±1.84 0.1 ±0.01 1.14 9811\nRDP 81.17 ±14.45 1.94±1.76 0.08 1.74 23275\nTransMatch 79.01 ±16.08 2.1 ±1.84 0.06 1.28 21922\nUTSRMorph 81.06 ±14.37 1.95 ±1.81 0.1 ±0.01 1.23 15710\nUTSRMorph-L 81.42±14.19 1.94±1.89 0.1±0.01 1.41 24139\nUTSRMorph-S 80.36 ±15.08 1.98 ±1.78 0.11 ±0.01 1.13 9693\nTo reduce GPU usage of TransMatch and maintain consistency during\ntraining, the shallowest convolutional block of TransMatch, TransMorph and\nUTSRMorph were abandoned.\nof FABs in the other stages were 2. The head numbers were\n{4, 4, 4, 4 }, {4, 4, 8, 8 } and {4, 4, 8, 16 }.\nE. Results\n1) Comparison with SOTA methods: The SOTA methods\nwith their proposed optimal training parameters were trained\nin the brain datasets (OASIS, IXI); the results are shown in\nTable I, II. Our proposed UTSRMorph achieved the optimal\nDSC and HD95 while having a lower SDlogJ, %|Jϕ ≤ 0|. This\nfinding indicates that UTSRMorph can generate high-quality,\ndetailed deformation fields and achieve excellent registration\nperformance. Moreover, the method demonstrated competi-\ntive results in terms of GPU memory usage and inference\ntime. Among the other comparison methods, the registration\nperformances of ViT-V-Net, TransMorph and TransMatch are\nsuperior to that of V oxelMorph due to the long-distance depen-\ndency of the Transformer, especially the feature representation\nlearning ability of Swin-T. The cross-attention calculation\nbetween a pair of input images occupies considerable GPU\nmemory in TransMatch, which causes the convolution blocks\nto be abandoned. This limits its performance. RDP, which\nuses a recursive pyramid strategy to focus on the deformation\nfusion block and upsampling layer in the decoder, achieved the\nclosest registration result to ours; however, the deformation\nfusion block specifically occupied GPU memory. According\nto the above methods, our method fully considers the feature\nextraction ability of the encoder for leveraging the advantages\nof ConvNets and Swin-T, and uses the SR module in the\nupsampling layer of the decoder to balance the smoothness\nand detail deformation field generating. Then, segmentation\nloss was added to train these models in OASIS; the results are\nshown in Table III. The auxiliary segmentation information\ncan promote global matching information correspondence by\nfacilitating local information matching of pivotal organs during\ntraining. Although it can significantly improve registration\naccuracy, the segmentation loss calculation for several different\ncategories also prominently increases computational resources,\nresulting in some methods with large GPU memory being out-\nof-commission. The detailed DSC distributions obtained with\nTABLE III\nTHE RESULTS OF ALL METHODS IN THE OASIS DATASET WITH\nSEGMENTATION LOSS\nMethod DSC(%) HD95 SDlogJ Time Memory\nViT-V-Net 84.48 ±11.78 1.66 ±1.93 0.12±0.01 1.11 22455\nV oxelMorph 84.18±12.19 1.67 ±1.75 0.12 ±0.02 1.18 20616\nTransMorph 85.86 ±10.63 1.46 ±1.43 0.12 ±0.02 1.14 21207\nUTSRMorph 86.28±10.22 1.42±1.4 0.13±0.02 1.23 23337\nRDP and TransMatch with segmentation loss were excluded from comparative\nmethods due to high GPU memory usage.\nFig. 4. Boxplots with Dice scores of various registration methods in the\nIXI dataset.\ndifferent methods for various organs are shown in Fig.4. The\noptimal DSC of most organs was acquired for UTSRMorph.\nThe visualization results for the IXI dataset are shown\nin Fig.5. A deformation field with local excessive distortion\nand missing detail was obtained by the traditional method,\nresulting in poor accuracy. The deformation field obtained by\nour method is generally smooth, behaves well, and maintains\nalmost no folding. The segmentation results after image reg-\nistration indicate that our method can consistently preserve\ninternal structures by balancing local deformation and global\nsmoothness using the advantages of ConvNets and ViTs,\nespecially for organs with smaller volumes.\nTABLE IV\nTHE RESULTS OF ALL METHODS IN THE TEST SET OF THE ABDOMEN\nMR-CT DATASET\nMethod DSC(%) HD95 SDlogJ Time Memory\nInitial 40.27 ±16.86\nAffine 57.24 ±19.42\nViT-V-Net 69.07 ±18.58 9.29 ±5.99 0.03 ±0.01 0.23 14838\nV oxelMorph 67.81 ±18.5 10.23 ±6.24 0.02 ±0.01 0.11 13975\nNiftyReg 72.61 ±22.89 8.54 ±8.34 0 117.8\nLapIRN 70.91 ±19.3 10.48 ±6.05 0.1 ±0.01 0.22 13457\nConvexAdam 77.16±19.12 7.63±7.63 0.07 0.39 3086\nTransMorph 70.97 ±20.68 10.64 ±7.96 0.04 ±0.01 0.52 19608\nRDP 76.77 ±22.54 7.81 ±6.02 0.05 0.26 23893\nTransMatch 70.45 ±20.74 11.12 ±8.96 0.02 ±0.01 0.14 23492\nUTSRMorph 75.22 ±18.7 9.72 ±7.77 0.03 ±0.01 0.11 17229\nUTSRMorph-L 73.7 ±19.31 9.81 ±7.62 0.03 ±0.01 0.17 23011\nUTSRMorph-S 73.18 ±19.24 9.57 ±7.86 0.02 0.09 13475\nThe convolutional blocks of TransMatch and UTSRMorph were abandoned.\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 9\nTABLE V\nTHE UNSUPERVISED RESULTS OF ALL METHODS IN THE TEST SET OF\nTHE ABDOMEN MR-CT DATASET\nMethod DSC(%) HD95 SDlogJ Time Memory\nViT-V-Net 65.59 ±19.74 10.15 ±7.11 0.03 ±0.01 0.23 14036\nV oxelMorph 65.71 ±19.73 10.42 ±6.99 0.02 ±0.01 0.11 13063\nTransMorph 68.76 ±20.54 9.92 ±7.24 0.03 ±0.01 0.52 18282\nRDP 64.81 ±19.99 10.43 ±7.09 0 0.26 22276\nTransMatch 68.75 ±19.53 9.99 ±7.16 0.02 ±0.01 0.14 21433\nUTSRMorph 69.89±19.39 9.58±7.06 0.02±0.01 0.11 16004\nThe convolutional blocks of TransMatch and UTSRMorph were abandoned.\nFig. 5. The visualization results of various registration methods in\nthe IXI dataset. The results include the raw image and ground-truth\nmask of the moving and fixed images, warped image and warped mask,\nestimated continuous and RGB deformations.The RGB deformation is\ngenerated by mapping each spatial dimension of the displacement field\nto a channel of the RGB color.\nFig. 6. Boxplots with Dice scores of various registration methods in the\nabdomen MR-CT dataset.\nTo achieve better performance, the segmentation loss was\nemployed to train the abdomen MR-CT dataset. The SOTA\nmethods were compared in the abdomen MR-CT dataset; the\nresults are shown in Table IV and Fig.6. The optimal perfor-\nmance is achieved by ConvexAdam. Our method achieved\nan accuracy which was second only to that of RDP and\nsignificantly greater than that of the other deep learn-based\nmethods. The unsupervised results are presented in Table V.\nAfter discarding the auxiliary segmentation information, the\nperformance of all unsupervised methods drops noticeably.\nOur method achieves better Dice scores, and surprisingly, the\nperformance of RDP decreases significantly. The evaluation\nmetrics of our datasets are shown in Table VI; our method\nachieved an optimal TRE index and generated relatively\nsmooth deformation fields. The visualization results of the\nTABLE VI\nTHE RESULTS OF ALL METHODS IN THE TEST SET OF THE\nCRANIOMAXILLOFACIAL TUMORS MR-CT DATASET\nMethod TRE (mm) SDlogJ Time Memory\nInitial 15.85 ±9.29\nViT-V-Net 6.03 ±3.22 0.02 ±0.01 0.77 13277\nV oxelMorph 5.9 ±3.43 0.01 ±0.01 0.77 12965\nTransMorph 5.63 ±3.15 0.01 ±0.01 0.75 13656\nRDP 5.61 ±3.25 0 0.93 22198\nTransMatch 5.77 ±3.1 0.01 ±0.01 1.32 20519\nUTSRMorph 5.54±3.33 0.01 0.81 15888\nUTSRMorph-L 5.71 ±3.05 0.01 ±0.01 0.9 21144\nUTSRMorph-S 5.72 ±3.53 0.01 0.77 12279\nThe shallowest convolutional block of TransMatch, TransMorph and UTSR-\nMorph were abandoned.\nFig. 7. The visualization results of various registration methods in our\nCMF tumor MR-CT dataset. The moving image is MR volume and the\nfixed image is CT volume. The ϕ is cropped to show that MR volume is\na subset of CT volume.\nmore detailed displacement field and warped moving image are\nshown in Fig.7. The local deformation in the deformation field\nof the RDP is more pronounced when adapting to larger shape\nchanges; the local spatial location in the displacement field is\nhighlighted. This causes local regional distortion. Our method\nutilizes local and global information to generate a smoother\ndeformation field; the local detail feature in a warped moving\nimage conforms to the fixed image.\n2) Ablation Experiment: To evaluate the effectiveness of\neach module in improving accuracy, we conducted ablation\nexperiments where we omitted the OAB, replaced the FAB\nwith Swin-T, and replaced the SR modules with interpolation\nTABLE VII\nTHE ABLATION EXPERIMENT OF ALL METHODS FOR PROPOSED\nMODULES\nDataset Lseg SR FAB OAB DSC HD95 SDlogJ\nOASIS ✔\n✘ ✘ ✘ 85.86±10.63 1.46 ±1.43 0.12 ±0.02\n✔ ✘ ✘ 85.98±10.53 1.46 ±1.44 0.12±0.01\n✔ ✔ ✘ 86.07±10.45 1.47 ±1.46 0.12 ±0.02\n✔ ✔ ✔ 86.28±10.22 1.42±1.4 0.13±0.02\nDataset Lseg SR FAB OAB DSC HD95 % |Jθ| ≤0\nIXI ✘\n✘ ✘ ✘ 75.11±12.35 3.6 ±2.64 1.47 ±0.32\n✔ ✘ ✘ 76.37±12.51 3.07 ±2.15 0.3±0.12\n✔ ✔ ✘ 76.38±12.57 3.05 ±2.2 0.32 ±0.14\n✔ ✔ ✔ 76.69±12.56 3.03±2.18 0.32±0.13\nDataset Lseg SR FAB OAB DSC HD95 SDlogJ\nAbdomen\nMR-CT ✔\n✘ ✘ ✘ 70.97±20.68 10.64 ±7.96 0.04 ±0.01\n✔ ✘ ✘ 72.6±20.31 10.29 ±7.65 0.02±0.01\n✔ ✔ ✘ 74.29±18.56 9.1±7.47 0.03±0.01\n✔ ✔ ✔ 75.22±18.7 9.72±7.77 0.03 ±0.01\n✔ represents this module used in UTSRMorph and ✘ indicates abandoned\nmodule.\n10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nFig. 8. The trend of change with Dice scores and loss in the test set of\nOASIS trained by segmentation loss. The curves named none, SR, FAB,\nOAB represent adding each module gradually, as shown in Table VII.\nupsampling layers. The results of the ablation experiments\nof UTSRMorph in different datasets are shown in Table VII.\nOur proposed SR, FAB and OAB can gradually increase the\nDSC and decrease the distortion of the deformation field. This\nfinding indicates that these modules can enhance the feature\nrepresentation ability and generate more accurate deformation\nfields, further improving the corresponding transformation\nbetween all points of a pair of images. The SR module\ndecreases %|Jθ| ≤0, which demonstrates that the module can\nimprove the smoothness of a displacement field and generate a\nmore detailed displacement field. Although FAB and OAB can\nsignificantly increase DSC, the rate of unrealistic deformation\nalso slightly increases. This proves its feature extraction and\nrepresentation learning ability. A typical change trend with\nDSCs and loss in the test set of OASIS trained by segmentation\nloss is shown in Fig.8. The curve between the number of\niterations and the loss decreases faster when our proposed\nSR, FAB and OAB are added. This indicates that aggregating\nConvNets and ViTs accelerates the error optimization speed\nsignificantly. ConvNets can reduce the redundancy of ViTs and\naccelerate optimization during training. The optimal DSCs ob-\nviously increase when OAB using the cross-attention module\nis added. It indicates the cross-window can integrate abundant\ncorrelations with visual information better.\nV. D ISCUSSION\nThe distributions of DSCs in the IXI dataset are shown in\nFig.4. Our UTSRMorph achieves the best registration perfor-\nmance across most organs, including thalamus, cerebellum-\nwhite-matter, ventralDC, caudate, hippocampus, 3rd-ventricle,\namygdala, CSF. In other organs, our method gains competitive\nresults comparable to TransMatch or RDP. Specifically, RDP\nobtains higher DSCs in organs with significant deformations,\nsuch as cerebellum-cortex and cerebral-cortex, highlighting\nits effectiveness in handling large deformations. TransMatch\nenhances registration performance in organs with complex\nstructures, such as the brain-stem and putamen, by leveraging\nmultiple features. Large-sized organs (such as cortex, ventricle,\nabdomen organs) frequently require significant deformations,\nwhile small-sized organs (such as thalamus, hippocampus,\nputamen) have complex relationships with surrounding tissues.\nHowever, our UTSRMorph can aggregate rich features ex-\ntracted by ConvNets and ViTs and enlarge the receptive field\nusing the overlapping attention modules, which can provide\na better balance between handling large deformations and\naddressing intricate local structures. The DSCs and registration\nperformance of choroid-plexus are worse in all methods.\nThe reason may be due to complex inter-attachment with\nsurrounding organs and diffuse morphological structures.\nThe distributions of DSCs in the abdomen MR-CT dataset\nare illustrated in Fig.6. The accuracy (especially for right-\nkidney) of the networks based entirely on deep learning\nis lower than the ConvexAdam, which utilizes hand-crafted\nMIND features and thin plate splines. The reason may be that\nthe learning-based approaches using segmentation loss was\nsubject to significant overfitting on the liver, spleen and left-\nkidney. The overfitting results led to unsatisfactory results on\nthe right-kidney. Our UTSRMorph demonstrates competitive\nresults comparable to RDP which excels in handling large\ndeformations. Except that, our method significantly outper-\nforms other learning-based networks. This finding indicates\nour network’s capability to effectively compensate for the large\ndeformation.\nThe statistical analysis of paired Wilcoxon rank-sum test in\nthe all datasets between UTSRMorph and other baseline meth-\nods for the DSCs is conducted. The p value of UTSRMorph\ncompared with other baseline methods in the IXI dataset is\np < 0.05. The p value in the OASIS dataset is p < 0.5\nexcept for RDP ( p = 0.28) and TransMorph ( p = 0.22).\nThe reason may be that the testing set of the OASIS dataset\nis small (19 samples) while the total number of the testing\nset in the IXI dataset is 115 samples. For the results in the\nOASIS dataset with segmentation loss, the performance of\nUTSRMorph is not significantly different from TransMorph\n(p = 0.11). The reason may be that the methods using seg-\nmentation loss was subject to significant overfitting. Although\nthe abdomen MR-CT and CMF tumor MR-CT datasets are not\nsuitable for statistical analysis because of their small sample\nsizes (8 samples and 4 samples, respectively), we still made\nthe statistical tests. The p values of LapIRN, ConvexAdam\nand RDP are 0.07, 0.41 and 0.09 in the abdomen MR-CT\ndataset trained with segmentation loss and the performance\nof UTSRMorph is significantly different from other methods.\nThe results in the abdomen MR-CT dataset show no significant\ndifference between all unsupervised methods when discarding\nthe auxiliary segmentation information. The Dice scores of\nUTSRMorph are not significantly different in the CMF tumor\nMR-CT datasets. Besides the impact of test data size, the\nerrors of the annotating landmarks, particularly in MR images\nwith large spacing (over 2mm), also play a significant role. In\nsubsequent work we will collect more MR data with smaller\nspacing and provide denser landmark annotations to improve\nthe effectiveness of TRE index. For most entries, the p value\nis less than 0.05, indicating that the DSCs improvement of our\nproposed method is significant.\nTo demonstrate the effectiveness of the SR module, we have\nconducted additional experiments as follows. We replaced the\noriginal interpolated upsampling layer with the SR module\nand investigated the performance improvement in TransMorph.\nThe compared results of the displacement field in the IXI\ndataset are visualized in Fig.9. These visualizations clearly\ndemonstrate that displacement fields generated by the SR\nmodule exhibit smoother deformations with no local folding of\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 11\nTABLE VIII\nTHE COMPARISON RESULTS OF DIFFERENT UPSAMPLING LAYERS IN IXI\nDATASET\nUpsampling type DSC(%) HD95 % |Jθ| ≤0\nCubic interpolation 76.43 ±12.57 3.07 ±2.18 0.4 ±0.1\nTransConv 76.56 ±12.26 3.05 ±2.18 0.4 ±0.1\nSR 76.69±12.56 3 .03±2.18 0 .3±0.1\nFig. 9. The visualization of the deformation field and the corresponding\nJacobian determinant for the same slices between using our SR mod-\nules and using the cubic interpolation upsampling layers for TransMorph\nin the IXI dataset.\nFig. 10. The visualization of the deformation field and the correspond-\ning Jacobian determinant for the same slices between using our SR\nmodules and using the TransConv layers for our UTSRMorph in the IXI\ndataset.\npatches. Moreover, a lower nonpositive Jacobian determinant\nfurther confirms the advantages of the SR module. This\nfinding indicates the SR module can enhance displacement\nsmoothness and gradually upscale the features to produce more\ndetailed displacement fields. We compared cubic interpolation,\nTransConv, and SR at the same time in our UTSRMorph and\nthe results are shown in Table VIII. The SR module slightly\noutperforms the others. The visualization results of TransConv\nand SR are shown in Fig.10. To measure the differences of\ndeformation field quality between our method and the cubic\ninterpolation upsampling layers in Fig.9, TransConv layers\nin Fig.10, the statistical test results of the Jacobian matrix\nindex are p < 10−3, p = 0.01. The results indicate that\nthe deformation field produced by the SR module achieves\nsmoother results compared to the other two upsampling layers.\nIn this paper, we have observed that SR, a common research\nfield for the ill-posed image inverse reconstruction, plays a\ncrucial role in enhancing registration performance. It has not\nbeen investigated in the image registration field and is a first\nattempt in this work. However, the pixel shuffle layer is subject\nto repeating artifacts as going deeper, whose parameters are\ndifficult to learn in deeper layers [34]. This may reduce the\naccuracy of deeper UTSRMorph-L. In the future work, we\nwill continue to focus on the SR to improve the accuracy of\nimage registration. We hope that integrating more SR concepts\ninto the deformable image registration community will not\nonly enhance the detail in displacement fields but also mitigate\nfeature degradation.\nVI. C ONCLUSION\nIn summary, we proposed UTSRMorph to generate a de-\nformation field for medical image registration tasks. It fully\nutilizes the advantages of ConvNets and ViTs to retain local\nfeatures while reducing the long-distance information redun-\ndancy of global dependency. Our method uses a cross-attention\nmechanism that relies on overlapping windows to enhance the\nrepresentation learning ability of the encoder. In addition, the\nSR module can improve the local deformation accuracy while\nensuring the smoothness of the generated deformation field.\nOur method achieved competitive results in four datasets. Ad-\nditionally, the experimental results validated its effectiveness\nin multimodal registration tasks.\nREFERENCES\n[1] A. Sotiras, C. Davatzikos, and N. Paragios, “Deformable medical image\nregistration: A survey,” IEEE Transactions on Medical Imaging, vol. 32,\nno. 7, pp. 1153–1190, 2013.\n[2] H. Xiao, X. Xue, M. Zhu, X. Jiang, Q. Xia, K. Chen, H. Li, L. Long,\nand K. Peng, “Deep learning-based lung image registration: A review,”\nComputers in Biology and Medicine , vol. 165, p. 107434, 2023.\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\nS0010482523008995\n[3] J. Dai, G. Dong, C. Zhang, W. He, L. Liu, T. Wang, Y . Jiang, W. Zhao,\nX. Zhao, Y . Xie, and X. Liang, “V olumetric tumor tracking from a\nsingle cone-beam x-ray projection image enabled by deep learning,”\nMedical Image Analysis, vol. 91, p. 102998, 2024. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S136184152300258X\n[4] J. Goerres, A. Uneri, M. Jacobson, B. Ramsay, T. De Silva, M. Ketcha,\nR. Han, A. Manbachi, S. V ogt, G. Kleinszig, J.-P. Wolinsky, G. Osgood,\nand J. H. Siewerdsen, “Planning, guidance, and quality assurance of\npelvic screw placement using deformable image registration,” PHYSICS\nIN MEDICINE AND BIOLOGY, vol. 62, no. 23, pp. 9018–9038, DEC\n7 2017.\n[5] P. Alvarez, M. Chabanas, S. Sikora, S. Rouz ´e, Y . Payan, and J.-\nL. Dillenseger, “Measurement and analysis of lobar lung deformation\nafter a change of patient position during video-assisted thoracoscopic\nsurgery,” IEEE Transactions on Biomedical Engineering, vol. 70, no. 3,\npp. 931–940, 2023.\n[6] T. Vercauteren, X. Pennec, A. Perchant, and N. Ayache, “Diffeomorphic\ndemons: Efficient non-parametric image registration,” NeuroImage,\nvol. 45, no. 1, Supplement 1, pp. S61–S72, 2009, mathematics in Brain\nImaging. [Online]. Available: https://www.sciencedirect.com/science/\narticle/pii/S1053811908011683\n[7] M. P. Heinrich, M. Jenkinson, M. Brady, and J. A. Schnabel, “Mrf-\nbased deformable registration and ventilation estimation of lung ct,”\nIEEE Transactions on Medical Imaging, vol. 32, no. 7, pp. 1239–1248,\n2013.\n[8] T. Jin, S. Pan, X. Li, and S. Chen, “Metadata and image features\nco-aware personalized federated learning for smart healthcare,” IEEE\nJournal of Biomedical and Health Informatics, vol. 27, no. 8, pp. 4110–\n4119, 2023.\n[9] R. Zhang, J. Wang, and C. Chen, “Automatic implant shape\ndesign for minimally invasive repair of pectus excavatum using\ndeep learning and shape registration,” Computers in Biology and\nMedicine, vol. 158, p. 106806, 2023. [Online]. Available: https:\n//www.sciencedirect.com/science/article/pii/S0010482523002718\n[10] R. Zhang, B. Jie, Y . He, L. Zhu, Z. Xie, Z. Liu, H. Mo, and J. Wang,\n“Craniomaxillofacial bone segmentation and landmark detection using\nsemantic segmentation networks and an unbiased heatmap,” IEEE Jour-\nnal of Biomedical and Health Informatics, vol. 28, no. 1, pp. 427–437,\n2024.\n12 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\n[11] G. Balakrishnan, A. Zhao, M. R. Sabuncu, A. V . Dalca, and J. Guttag,\n“An unsupervised learning model for deformable medical image regis-\ntration,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 9252–9260.\n[12] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V . Dalca,\n“V oxelmorph: A learning framework for deformable medical image\nregistration,” IEEE Transactions on Medical Imaging, vol. 38, no. 8,\npp. 1788–1800, 2019.\n[13] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in International Conference on\nMedical image computing and computer-assisted intervention. Springer,\n2015, pp. 234–241.\n[14] M. Jaderberg, K. Simonyan, A. Zisserman et al., “Spatial transformer\nnetworks,” Advances in neural information processing systems, vol. 28,\n2015.\n[15] H. Wang, D. Ni, and Y . Wang, “Recursive deformable pyramid network\nfor unsupervised medical image registration,” IEEE Transactions on\nMedical Imaging, pp. 1–1, 2024.\n[16] A. Hering, L. Hansen, T. C. W. Mok, and et.al., “Learn2reg: Com-\nprehensive multi-task medical image registration challenge, dataset and\nevaluation in the era of deep learning,” IEEE Transactions on Medical\nImaging, vol. 42, no. 3, pp. 697–712, 2023.\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[18] Z. Chen, Y . Zheng, and J. C. Gee, “Transmatch: A transformer-\nbased multilevel dual-stream feature matching network for unsupervised\ndeformable image registration,” IEEE Transactions on Medical Imaging,\nvol. 43, no. 1, pp. 15–27, 2024.\n[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly, J. Uszkoreit, and N. Houlsby, “An image is\nworth 16x16 words: Transformers for image recognition at\nscale,” CoRR, vol. abs/2010.11929, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2010.11929\n[20] K. Li, Y . Wang, J. Zhang, P. Gao, G. Song, Y . Liu, H. Li, and Y . Qiao,\n“Uniformer: Unifying convolution and self-attention for visual recogni-\ntion,” IEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 45, no. 10, pp. 12 581–12 600, 2023.\n[21] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” in 2021 IEEE/CVF International Conference on Computer\nVision (ICCV), 2021, pp. 9992–10 002.\n[22] J. Chen, E. C. Frey, Y . He, W. P. Segars, Y . Li, and Y . Du, “Transmorph:\nTransformer for unsupervised medical image registration,” Medical\nImage Analysis , vol. 82, p. 102615, 2022. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S1361841522002432\n[23] B. Kim, D. H. Kim, S. H. Park, J. Kim, J.-G. Lee, and J. C.\nYe, “Cyclemorph: Cycle consistent unsupervised deformable image\nregistration,” Medical Image Analysis , vol. 71, p. 102036, 2021.\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\nS1361841521000827\n[24] J. Chen, Y . He, E. C. Frey, Y . Li, and Y . Du, “Vit-v-net: Vision\ntransformer for unsupervised volumetric medical image registration,”\narXiv preprint arXiv:2104.06468, 2021.\n[25] J.-Q. Zheng, Z. Wang, B. Huang, N. H. Lim, and B. W. Papie ˙z,\n“Residual aligner-based network (ran): Motion-separable structure\nfor coarse-to-fine discontinuous deformable registration,” Medical\nImage Analysis , vol. 91, p. 103038, 2024. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S1361841523002980\n[26] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and\nB. Guo, “Cswin transformer: A general vision transformer backbone\nwith cross-shaped windows,” in 2022 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2022, pp. 12 114–12 124.\n[27] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and\nL. Zhang, “Cvt: Introducing convolutions to vision transformers,” in\n2021 IEEE/CVF International Conference on Computer Vision (ICCV),\n2021, pp. 22–31.\n[28] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu, “Incorporating\nconvolution designs into visual transformers,” in 2021 IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 2021, pp. 559–568.\n[29] X. Chen, X. Wang, J. Zhou, Y . Qiao, and C. Dong, “Activating\nmore pixels in image super-resolution transformer,” in 2023 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2023,\npp. 22 367–22 377.\n[30] D. Rong, Z. Zhao, Y . Wu, B. Ke, and B. Ni, “Prediction of myopia\neye axial elongation with orthokeratology treatment via dense i2i based\ncorneal topography change analysis,” IEEE Transactions on Medical\nImaging, vol. 43, no. 3, pp. 1149–1164, 2024.\n[31] F. Xue, Y . Li, D. Liu, Y . Xie, L. Wu, and R. Hong, “Lipformer: Learning\nto lipread unseen speakers based on visual-landmark transformers,”\nIEEE Transactions on Circuits and Systems for Video Technology ,\nvol. 33, no. 9, pp. 4507–4517, 2023.\n[32] Y . Zheng, J. Li, J. Shi, F. Xie, J. Huai, M. Cao, and Z. Jiang, “Kernel\nattention transformer for histopathology whole slide image analysis and\nassistant cancer diagnosis,” IEEE Transactions on Medical Imaging,\nvol. 42, no. 9, pp. 2726–2739, 2023.\n[33] Z. Peng, Z. Guo, W. Huang, Y . Wang, L. Xie, J. Jiao, Q. Tian, and\nQ. Ye, “Conformer: Local features coupling global representations for\nrecognition and detection,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 45, no. 8, pp. 9454–9468, 2023.\n[34] B. B. Moser, F. Raue, S. Frolov, S. Palacio, J. Hees, and A. Den-\ngel, “Hitchhiker’s guide to super-resolution: Introduction and recent\nadvances,” IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, vol. 45, no. 8, pp. 9862–9882, 2023.\n[35] W. Shi, J. Caballero, F. Husz ´ar, J. Totz, A. P. Aitken, R. Bishop,\nD. Rueckert, and Z. Wang, “Real-time single image and video super-\nresolution using an efficient sub-pixel convolutional neural network,” in\n2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2016, pp. 1874–1883.\n[36] H. Wang, Y . Xu, Q. Chen, R. Tong, Y .-W. Chen, H. Hu, and L. Lin,\n“Adaptive decomposition and shared weight volumetric transformer\nblocks for efficient patch-free 3d medical image segmentation,” IEEE\nJournal of Biomedical and Health Informatics, vol. 27, no. 10, pp. 4854–\n4865, 2023.\n[37] W. Chen, Y . Liu, J. Hu, and Y . Yuan, “Dynamic depth-aware network\nfor endoscopy super-resolution,”IEEE Journal of Biomedical and Health\nInformatics, vol. 26, no. 10, pp. 5189–5200, 2022.\n[38] J. Ma, S. Liu, S. Cheng, R. Chen, X. Liu, L. Chen, and S. Zeng,\n“Stsrnet: Self-texture transfer super-resolution and refocusing network,”\nIEEE Transactions on Medical Imaging, vol. 41, no. 2, pp. 383–393,\n2022.\n[39] Y . Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y . Fu, “Image super-\nresolution using very deep residual channel attention networks,” in\nComputer Vision – ECCV 2018, V . Ferrari, M. Hebert, C. Sminchisescu,\nand Y . Weiss, Eds. Cham: Springer International Publishing, 2018, pp.\n294–310.\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in ADVANCES\nIN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017),\nser. Advances in Neural Information Processing Systems, vol. 30, 2017,\n31st Annual Conference on Neural Information Processing Systems\n(NIPS), Long Beach, CA, DEC 04-09, 2017.\n[41] N. Kwak and C.-H. Choi, “Input feature selection by mutual information\nbased on parzen window,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 24, no. 12, pp. 1667–1671, 2002.\n[42] D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C. Morris,\nand R. L. Buckner, “Open Access Series of Imaging Studies (OASIS):\nCross-sectional MRI Data in Young, Middle Aged, Nondemented,\nand Demented Older Adults,” Journal of Cognitive Neuroscience ,\nvol. 19, no. 9, pp. 1498–1507, 09 2007. [Online]. Available:\nhttps://doi.org/10.1162/jocn.2007.19.9.1498\n[43] B. Fischl, “Freesurfer,” NeuroImage, vol. 62, no. 2, pp. 774–\n781, 2012, 20 YEARS OF fMRI. [Online]. Available: https:\n//www.sciencedirect.com/science/article/pii/S1053811912000389\n[44] B. Han, B. Jie, L. Zhou, T. Huang, R. Li, L. Ma, X. Zhang, Y . Zhang,\nY . He, and H. Liao, “Statistical and individual characteristics-based\nreconstruction for craniomaxillofacial surgery,” International Journal\nof Computer Assisted Radiology and Surgery, vol. 17, pp. 1155–1165,\n2022.\n[45] S. Datta and G. A. Satten, “Rank-sum tests for clustered data,” Journal\nof the American Statistical Association, vol. 100, no. 471, pp. 908–915,\n2005.\n[46] B. Avants, C. Epstein, M. Grossman, and J. Gee, “Symmetric\ndiffeomorphic image registration with cross-correlation: Evaluating\nautomated labeling of elderly and neurodegenerative brain,” Medical\nImage Analysis, vol. 12, no. 1, pp. 26–41, 2008, special Issue on\nThe Third International Workshop on Biomedical Image Registration\n– WBIR 2006. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S1361841507000606\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 13\n[47] T. C. Mok and A. C. Chung, “Large deformation diffeomorphic im-\nage registration with laplacian pyramid networks,” in Medical Image\nComputing and Computer Assisted Intervention–MICCAI 2020: 23rd\nInternational Conference, Lima, Peru, October 4–8, 2020, Proceedings,\nPart III 23. Springer, 2020, pp. 211–221.\n[48] ——, “Conditional deformable image registration with convolutional\nneural network,” in Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2021: 24th International Conference, Strasbourg,\nFrance, September 27–October 1, 2021, Proceedings, Part IV 24 .\nSpringer, 2021, pp. 35–45.\n[49] H. Siebert, L. Hansen, and M. P. Heinrich, “Fast 3d registration\nwith accurate optimisation and little learning for learn2reg 2021,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention. Springer, 2021, pp. 174–179.\n[50] M. P. Heinrich, M. Jenkinson, M. Bhushan, T. Matin, F. V .\nGleeson, S. M. Brady, and J. A. Schnabel, “Mind: Modality\nindependent neighbourhood descriptor for multi-modal deformable\nregistration,” Medical Image Analysis, vol. 16, no. 7, pp. 1423–1435,\n2012, special Issue on the 2011 Conference on Medical Image\nComputing and Computer Assisted Intervention. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S1361841512000643\n[51] M. Modat, G. R. Ridgway, Z. A. Taylor, M. Lehmann, J. Barnes, D. J.\nHawkes, N. C. Fox, and S. Ourselin, “Fast free-form deformation using\ngraphics processing units,” COMPUTER METHODS AND PROGRAMS\nIN BIOMEDICINE, vol. 98, no. 3, SI, pp. 278–284, JUN 2010, high-\nPerformance Medical Image Computing and Computer Aided Interven-\ntion Workshop (HP-MICCAI 2008), New York, NY , SEP 10, 2008.",
  "topic": "Image registration",
  "concepts": [
    {
      "name": "Image registration",
      "score": 0.7548413872718811
    },
    {
      "name": "Superresolution",
      "score": 0.6725791692733765
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6540627479553223
    },
    {
      "name": "Computer science",
      "score": 0.5722321271896362
    },
    {
      "name": "Medical imaging",
      "score": 0.5618001818656921
    },
    {
      "name": "Computer vision",
      "score": 0.5360617637634277
    },
    {
      "name": "Transformer",
      "score": 0.5211824774742126
    },
    {
      "name": "Image resolution",
      "score": 0.44542235136032104
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37377405166625977
    },
    {
      "name": "Image (mathematics)",
      "score": 0.35448893904685974
    },
    {
      "name": "Engineering",
      "score": 0.07144802808761597
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}