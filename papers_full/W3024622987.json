{
    "title": "Covid-twitter-bert: A natural language processing model to analyse covid-19 content on twitter",
    "url": "https://openalex.org/W3024622987",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2117219448",
            "name": "MÃ¼ller Martin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287262550",
            "name": "Salathe, Marcel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287422865",
            "name": "Kummervold, Per E",
            "affiliations": [
                "Twitter (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2992839908",
        "https://openalex.org/W2962707339",
        "https://openalex.org/W2952186591",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2973154071",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2922551710",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2773085311",
        "https://openalex.org/W2911489562"
    ],
    "abstract": "&lt;p&gt;In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based model, pretrained on a large corpus of Twitter messages on the topic of COVID-19. Our model shows a 10&ndash;30% marginal improvement compared to its base model, BERT-LARGE, on five different classification datasets. The largest improvements are on the target domain. Pretrained transformer models, such as CT-BERT, are trained on a specific target domain and can be used for a wide variety of natural language processing tasks, including classification, question-answering and chatbots. CT-BERT is optimised to be used on COVID-19 content, in particular from social media&lt;/p&gt;",
    "full_text": null
}