{
    "title": "Structural Guidance for Transformer Language Models",
    "url": "https://openalex.org/W3174281149",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A1975658276",
            "name": "Peng Qian",
            "affiliations": [
                "Institute of Cognitive and Brain Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2072731134",
            "name": "Tahira Naseem",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1982032445",
            "name": "Roger Lévy",
            "affiliations": [
                "Institute of Cognitive and Brain Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2129506641",
            "name": "Ramón Fernandez Astudillo",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2971351900",
        "https://openalex.org/W2962733492",
        "https://openalex.org/W3170110950",
        "https://openalex.org/W2921890305",
        "https://openalex.org/W2311095070",
        "https://openalex.org/W2962811598",
        "https://openalex.org/W4376522561",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2888882903",
        "https://openalex.org/W2150406842",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3093303082",
        "https://openalex.org/W4236838343",
        "https://openalex.org/W3113847915",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W3034552719",
        "https://openalex.org/W3034510440",
        "https://openalex.org/W2060535757",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3174413662",
        "https://openalex.org/W2963084773",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2038324640",
        "https://openalex.org/W4288795042",
        "https://openalex.org/W2798727047",
        "https://openalex.org/W2516255829",
        "https://openalex.org/W2970626985",
        "https://openalex.org/W3168987555",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3153543512",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2152463966",
        "https://openalex.org/W2971044268",
        "https://openalex.org/W2962788148",
        "https://openalex.org/W2888922637",
        "https://openalex.org/W2564486991",
        "https://openalex.org/W2918996109",
        "https://openalex.org/W2134495021",
        "https://openalex.org/W3023419341",
        "https://openalex.org/W3116510459",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W2913340405",
        "https://openalex.org/W3117738520",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2158899491",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1598003989",
        "https://openalex.org/W2970378492",
        "https://openalex.org/W2952230511",
        "https://openalex.org/W2768763386",
        "https://openalex.org/W3106209546"
    ],
    "abstract": "Peng Qian, Tahira Naseem, Roger Levy, Ramón Fernandez Astudillo. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Structural Guidance for Transformer Language Models\nPeng Qian1 Tahira Naseem2 Roger Levy1 Ram´on Fernandez Astudillo2\n1 Department of Brain and Cognitive Sciences, MIT 2 IBM Research\npqian@mit.edu tnaseem@us.ibm.com\nrplevy@mit.edu ramon.astudillo@ibm.com\nAbstract\nTransformer-based language models pre-\ntrained on large amounts of text data have\nproven remarkably successful in learning\ngeneric transferable linguistic representations.\nHere we study whether structural guidance\nleads to more human-like systematic linguistic\ngeneralization in Transformer language\nmodels without resorting to pre-training on\nvery large amounts of data. We explore two\ngeneral ideas. The “Generative Parsing” idea\njointly models the incremental parse and\nword sequence as part of the same sequence\nmodeling task. The “Structural Scaffold” idea\nguides the language model’s representation\nvia additional structure loss that separately\npredicts the incremental constituency parse.\nWe train the proposed models along with a\nvanilla Transformer language model baseline\non a 14 million-token and a 46 million-token\nsubset of the BLLIP dataset, and evaluate\nmodels’ syntactic generalization perfor-\nmances on SG Test Suites and sized BLiMP.\nExperiment results across two benchmarks\nsuggest converging evidence that generative\nstructural supervisions can induce more robust\nand humanlike linguistic generalization in\nTransformer language models without the\nneed for data intensive pre-training.\n1 Introduction\nPre-trained Transformer architectures have led to\nhuge progress in building more human-like lan-\nguage processing systems (Radford et al.; Devlin\net al., 2019; Brown et al., 2020, among others).\nThese models achieve impressive perplexity results\non language modelling datasets, perform well on\ngrammatical judgments (Warstadt et al., 2020), and\nprovide useful linguistic representations that ben-\neﬁt a wide range of downstream tasks. Probing\nanalyses also suggest that these models learn to im-\nplicitly encode syntactic information (Hewitt and\nManning, 2019; Clark et al., 2019) that may sup-\nport better linguistic generalization than recurrent\nneural network architectures (RNNs).\nHowever, the Transformer architecture (Vaswani\net al., 2017) is an interesting subject of study be-\nyond its success in transfer-learning settings. Trans-\nformer models lack the inductive biases of RNNs.\nRather than maintaining vector-valued state and\nupdating it in a recurrent manner, auto-regressive\nTransformer models encode all past decisions si-\nmultaneously at each inference step, thanks to a\nself-attention mechanism. The only notion of se-\nquence order is also given by position embeddings\nsummed to content embeddings in both input and\nauto-regressive signals.\nPrevious works have shown the advantage of\nstructural supervision in RNNs in learning to main-\ntain syntactic states and non-local dependencies\n(Kuncoro et al., 2018; Wilcox et al., 2019; Futrell\net al., 2019). It remains an open question whether\nTransformer language models can similarly beneﬁt\nfrom generative structural supervision, and what\nform of structural supervision would more effec-\ntively induce human-like syntactic generalization.\nThis work hypothesizes that the Transformer lan-\nguage model may beneﬁt from explicit generative\nstructural supervision to systematically generalize\nsyntactic knowledge. Here we explore two ma-\njor classes of structural guidance for Transformer\nlanguage models based on joint modeling of lan-\nguage and constituency parses. The “generative\nparsing as language modeling” approach builds a\nTransformer-parameterized model to learn to pre-\ndict actions that incrementally build constituency\ntrees along with terminal words, following prior\nwork on RNNs (Dyer et al., 2016; Choe and Char-\nniak, 2016). The “structural scaffolding” approach\nfollows the general idea of regularizing hidden rep-\nresentation through multi-task learning objective,\nwith prior success in various NLP tasks (Zhang\nS\nNP\nThe birds\nVP\nsang ADVP\n⟨BOS ⟩ NT(S) NT(NP) The birds REDUCE NT (VP) sang NT(ADVP ) ···\nw0 w1 w2 w3\ny0:1 y1:2 y2:3 y3:4\nw1 w2 w3\nw0 w1 w2\n(a) Vanilla language model\nNT(S) NT(NP) The birds REDUCE\n⟨BOS ⟩ NT(S) NT(NP) The birds\n(b) Parsing as Language Modelling\nw1 w2 w3\nw0 w1 w2\ny0:1 y1:2 y2:3\nw1 w2 w3\nw0 w1 w2\ny0:1 y1:2⟨PAD⟩\n(c) Language models with Structural Scaffold\nFigure 1: Top: Illustration of a partial constituency tree and corresponding transitions. Bottom: unidirectional\ntransformer language model (a) without explicit structural supervision, (b) for modelling generative action parsing\nsequence, and (c) with structural scaffold for predicting the local incremental parsing state.\nand Weiss, 2016; Søgaard and Goldberg, 2016;\nSwayamdipta et al., 2018).\nWe test these two approaches on two subsets of\nthe BLLIP dataset (Charniak et al., 2000) and evalu-\nate models’ syntactic generalization performances\non SG Test Suites (Hu et al., 2020) and a sam-\npled subset of the BLiMP Benchmark (Warstadt\net al., 2020). We show evidence that generative\nstructural supervision indeed induces more robust\nand human-like linguistic generalization in Trans-\nformer language models and explore the different\ntrade-offs involved in the presented methods.\n2 Models\nHere we explore joint modelling of structures and\nwords parametrized with Transformers by consid-\nering both a sentence W and its constituency parse\nY and modeling the joint distribution P(W, Y).\n2.1 Generative Parsing as Language\nModeling\nA language model can be described formally as a\nprobability distribution over strings of a language\nw1, ··· , wT , usually left-to-right factored.\np(W) =p(w1, ··· , wT ) =\nT∏\nt=1\np(wt |w<t) (1)\nThere are many possible approaches that can com-\nbine both language modeling and syntax model-\ning tasks. As long as both tasks share some of\nthe parameters they can be considered a case of\nmulti-task learning (Caruana, 1997). Of interest\nhere is the model proposed in Recurrent Neural\nNetwork Grammars (RNNGs; Dyer et al., 2016)\nand parsing as language model (LSTM-LM; Choe\nand Charniak, 2016). Both approaches model the\njoint distribution of words W and constituency tree\ncomponents Y as\np(Y, W) =p(a1, ··· , aR) =\nR∏\nt=1\np(at |a<t) (2)\nwhere at are transitions of a state machine that\ngenerates both the sentence and the tree. These\ntransitions are similar to the well-established transi-\ntion sets used for transition-based parsing (Earley,\n1970) but adapted to generate both text and parse\nsimultaneously. For the reminder of this work, we\nwill consider each at to be integer valued and in-\ndexing a dictionary of transitions. A transition a\ncan be a word w or a transition action that gener-\nates a component of the constituency tree y. The\nactions include non-terminal symbols that open and\nlabel a new constituent with the label x, indicated\nas NT(x), or a REDUCE action closing the closest\nopen constituent. An example of a partial parse tree\nand transitions can be found at the top of Figure 1.\nRNNG and LSTM-LM parametrize the same fac-\ntorization in Equation 2 in different ways. RNNG\nutilizes stack-LSTMs, which allow it to dynami-\ncally create representations for partial tree compo-\nnents by composition. The LSTM-LM, however,\nuses a ﬂat parametrization treating the transitions\nas a sequence in a conventional language model\nlearnt with an LSTM (Hochreiter and Schmidhu-\nber, 1997). It should also be noted that the LSTM-\nLM is designed as a parser, while RNNG is also\nused as a language model. In order to derive a lan-\nguage model from a joint model, it is is necessary\nto marginalize over all possible parse trees\np(W) =\n∑\nY ∈Y(W)\np(Y, W) (3)\nwhich is an intractable problem since there is an\nexponentially large number of possible trees. The\noriginal RNNG work (Dyer et al., 2016) proposes\nan approximate solution based on importance sam-\npling. In this work we use the word-synchronous\nbeam search approximation introduced in Stern\net al. (2017).\nThe marginalized likelihood language model in\nEquation 3 is desirable because it makes no statis-\ntical independence assumption between language\nand syntax and shares all parameters across both\ntasks, with the exception of action speciﬁc embed-\ndings. Particularly relevant for this work is the fact\nthat both word and non-word transitions are pre-\ndicted as language model output indiscriminately\nand are available at each prediction step through its\nhistory a<t.\nIn this work we propose to parametrize Eq 2\nwith a Transformer language model (Vaswani et al.,\n2017). This is equivalent to the ﬂat parametrization\nof the LSTM-LM but using a Transformer language\nmodel instead. Unlike LSTM-LM, which is a pars-\ning model, we derive from it a language model by\nmarginalization as in the RNNG. A Transformer\nlanguage model can be succinctly described as a\nneural network of vertically stacked layers where\nthe m-th layer is given by\nhm\n<t = FFm\n\nO ·\n\n\nAm\n1 (hm−1\n<t )\nAm\n2 (hm−1\n<t )\n···\nAm\nN (hm−1\n<t )\n\n\n\n. (4)\nHere hm−1\n<t ∈RH×t is the output of the previous\ndecoder layer for all previous predictions of the\nmodel at time step t and H is the size of the hid-\nden vector. The input to the ﬁrst layer i.e. h0\n<t\nare the embeddings of all previous transitions a<t\nconcatenated with a start symbol. Each embedding\nis the sum of both a content embedding, dictionary\nvector that is being indexed, and a position embed-\nding that encodes the absolute or relative position\nof each action in the sequence.\nFFm() is a feed-forward layer, Am\n1 () ···AM\nN ()\nare multiple self-attention heads and O ∈RH×H\nis a matrix multiplication performed on the con-\ncatenated output of the attention heads. Both the\nfeed-forward and the projection of N attention\nheads through O are wrapped around with residual,\ndropout and layer normalization operations that are\nhere removed for clarity.\nEach attention head comprises a simple inner\nproduct attention mechanism\nAm\nn (hm−1\n<t ) =V m\nn ·hm−1\n<t ·\nsoftmax\n(\n(Km\nn ·hm−1\n<t )T ·Qm\nn ·hm−1\n<t + M\n)\n(5)\nwhere V m\nn , Km\nn , Qm\nn ∈RH/N×H are value, key\nand query projection matrices respectively and the\nsoftmax operation is normalized over columns to\nsum to one. The matrix M∈{−∞ , 0}t×t is used\nto prevent the model from attending to future states\nduring training, enabling efﬁcient parallelization.\nIt is displayed here due to its relevance for the next\nsection.\nSimilarly to other models, to derive a distribution\nover all possible transitions, including words, non-\nterminal symbols and the REDUCE operation, we\ncan use a softmax together with an inner product\np(at |a<t) = softmax(EW∪Y ·hm\n<t)at (6)\nwhere EW∪Y are the embeddings for the joint vo-\ncabulary of words, non-terminals and REDUCE\ntransitions. Henceforth, we refer to this model as\nParsing as Language Model, or PLM for short.\nUnlike LSTMs or the RNNG, the Transformer\nhas direct access to all past decisions through self-\nattention and relies on position embeddings to en-\ncode word order. Thus, in principle, there is no\nstructural bias for the model to favor past deci-\nsions that are close in time to inform current pre-\ndiction. On one hand, this potential ability to use\nlong distance information can enable a less local,\nmore human like processing of language, but on\nthe other hand, it can also result in an additional\nlearning burden, especially if there is not sufﬁcient\nlearning data available. Also worth noting for the\nexperiments proposed here is that the total num-\nber of parameters of a typical Transformer greatly\nexceeds that of an LSTM or a RNNG model.\n2.2 Incorporating RNNG-like characteristics\nAs previously mentioned, unlike any of the other\nmodels, the RNNG is able to create partial tree rep-\nresentations by composition using stack-LSTMs.\nS\nNP\nThe birds ⟨BOS ⟩ NT(S) NT(NP) The birds\nBUFFER head\n⟨BOS ⟩ NT(S) NT(NP) The birds\nSTACK head\nFigure 2: Illustration of how the generated incremental\nconstituency parse is used to inform attention patterns\nin the structure-guided attention heads.\nThis changes the RNNG model structure dynami-\ncally as a function of the partial parse, a very de-\nsirable property to derive syntax-aware represen-\ntations. Moreover, the fact that Recurrent Neural\nNetworks such as LSTMs summarize all informa-\ntion about previous time steps on two hidden vec-\ntors, creates a bottleneck that forces the model to\nfocus on the local state. This is a situation where a\nsyntax-aware representation can provide additional\nvalue by enabling the local state to better encom-\npass past structures. We conjecture that a similarly\nconstrained local state might beneﬁt Transformer\nmodels in learning linguistic regularities, especially\nin a limited training data scenario.\nIn an attempt to capture a similar effect in the\nTransformer, we explore here the idea of masking\nsome attention heads to reﬂect the parser state as\nin the stack-Transformer (Astudillo et al., 2020).\nIn the stack-Transformer, two attention heads are\nspecialized to attend only to the contents of buffer\nand stack respectively for dependency and seman-\ntic parsing tasks. Here we choose to specialize\ntwo heads as well for each layer in Equation 4, as\ndepicted in Fig. 2. One attention head attends to\nthe contents of the last open constituent whereas\nanother head attends all other past decisions not\ninvolving that constituent. The rest of the heads are\nleft free as in the original Transformer architecture.\nTo constrain the attention heads, we only need to\nalter the mask Min Equation 5 to depend on head\nindex n and past actions Mn(a<t), which results\nin a negligible computation overhead.\nThis hard masking makes the model structure\nchange dynamically depending on the partial parse\nand it forces some heads to focus on the local syn-\ntactic state. Nevertheless, unlike the RNNG, it does\nnot create new representations of partial parses that\ncan be composed in a recurrent manner at each time\nstep, and some attention heads can still operate un-\nrestricted. We hypothesize that structure-aware at-\ntention mechanism may still help the model achieve\nbetter generalization. The symbolic representation\ninduces a strong inductive bias to how the model\nshould use the structure that it generates on the ﬂy.\nWe henceforth refer to this model PLM-mask.\n2.3 Scaffolding by Learning to Predict Local\nParse States\nGiven the strong coupling between the tasks, the\nmarginal likelihood Transformer language model\nof the previous section can be expected to be\nstrongly inﬂuenced by the additional syntax predic-\ntion task. This comes however at a big cost. First,\nsequences combine both words and non-terminal\nand reduce transitions, yielding longer sentences\nthan those of a normal language model R > T.\nFurthermore the approximated marginalization is\ncomputationally intensive and also introduces an\napproximation error.\nOne well-established regime that allows joint\nmodeling of tasks at a low complexity is that of the\nsyntactic scaffold (Zhang and Weiss, 2016; Søgaard\nand Goldberg, 2016; Swayamdipta et al., 2018).\nScaffolding adds an additional structure prediction\ntask at one of the layers of the model as a separate\nlayer and only during training. This is a minimally\nintrusive change since it just branches some hidden\nvector of the network and computes an additional\nloss. It also has no inﬂuence on test runtime and\navoids expensive steps such as marginalization.\nHowever, applying the idea of syntactic scaffold-\ning to our present scenario poses one difﬁculty. If\nwe use a standard language model predicting words\nw and predict the non-word symbols y separately,\nwe face the problem that the two sequences have\ndifferent lengths. To overcome this in a straight-\nforward way, we predict the n-gram of non-word\nactions yt:t+n(t) corresponding to the partial parse\nsynchronous with step t when we predict word wt.\nWe use a secondary softmax layer for this action\nn-gram prediction.\np(yt:t+n |y<t) = softmax(EY ∗\n·hm\n<t)yt:t+n (7)\nHere EY ∗\nis the vocabulary of all transition n-\ngrams excluding words found in the train corpus\nplus a blank symbol. Note that since Scaffolding\noperates only at train time, we do not need to worry\nabout generalization of these n-grams to test time.\nThe models are thus trained to minimize the loss\nfunction −log p(Y, W) where\np(Y, W) = ∏T\nt=1 p(wt |w<t)\n+ ∏T\nt=1 p(yt:t+n(t) |w<t) (8)\nThe scaffold can be set so that the synchronous\nnon-word action n-grams yt:t+n(t) are predicted ei-\nther before (Figure 1c, left) or after (Figure 1c,\nright) producing wt. We considered both vari-\nants in our experiments to empirically assess their\nimpact on performance. We refer to this model\nas Transformer Language Model with Syntactic\nScaffold, or ScLM in short, and its two versions\nScLM-past and ScLM-next, for past and next n-\ngram prediction.\n3 Experiments\n3.1 Model Training\nAll models, including the baseline vanilla language\nmodels (LM in short), the syntactic scaffold mod-\nels, and the generative parsing models, are based\non the same architecture of GPT-2 small (Radford\net al.) (117M parameters, 12 layers, H = 768) and\nuse the same BPE tokenizer, but with randomly\ninitialized weights. We believe this would give us\na fair comparison to pretrained GPT-2 as well, in\norder to evaluate whether structural guidance helps\nimprove sample efﬁciency. We implemented all the\nproposed models using Huggingface’s Transformer\npackage (Wolf et al., 2020)1.\nAs our goal here is to study whether structural\nguidance helps models learn robust humanlike gen-\neralization of syntactic knowledge, we train our\nmodel on the BLLIP dataset (Charniak et al., 2000),\nan English newswire style corpus used in Hu et al.\n(2020). This makes the results here more com-\nparable to the results reported in previous work,\nespecially with RNNGs. We train the proposed\nmodels and the baseline vanilla Transformer lan-\nguage models on BLLIP- MD, a 14 million-token\ncorpus, and BLLIP- LG, a 46 million-token corpus,\nboth of which are auto-parsed using a state-of-the-\nart constituency parser (Kitaev and Klein, 2018).\nWe used the parsed sentences to generate oracle\nparsing action sequence for PLM and PLM-mask.\nWe collected a list of word-synchronous parsing\n1Code available at https://github.com/IBM/\ntransformers-struct-guidance\naction sequences from the train and development\noracle of BLLIP- LG and use it to parametrize the\naction n-gram vocabulary of ScLMs trained on\nboth BLLIP- MD and BLLIP- LG. There are 3756\naction n-gram types from the corpora, including\none padding token and one blank token.\nAll models were trained with learning rate 10−5,\nAdamW optimizer, and minibatch of size 5. We\ntrained the models with multiple seeds within the\ncapacity of our resources, in order to accommodate\npotential variance. In total, there are three seeds of\nLM, four of ScLM-past, four of ScLM-next, three\nof PLM, and three of PLM-mask for BLLIP- MD,\nand the same number of seeds of each model type\nfor BLLIP- LG. Models were trained until conver-\ngence, as suggested by the loss of the development\nset during training.\n3.2 Targeted Syntactic Evaluation\nTo assess whether a trained model systematically\ngeneralizes its syntactic knowledge, we employ tar-\ngeted syntactic evaluation paradigm (Marvin and\nLinzen, 2018). Speciﬁcally, we measure models’\nperformance on two held-out test datasets, a collec-\ntion of syntactic generalization test suites from Hu\net al. (2020) and BLiMP Benchmark from Warstadt\net al. (2020). These two datasets cover a wide range\nof English syntactic phenomena.\nTests from Hu et al. (2020), which we refer\nas SG Test Suites, consist of hand-designed test\nsuites for evaluating ﬁne-grained syntactic gener-\nalization in incremental processing of a linguistic\ninput. The general method is to compare mod-\nels’ surprisalsp(continuation|preﬁx) of grammati-\ncal and ungrammatical continuations given certain\nsentence preﬁxes. We report the accuracy averaged\nacross SG test suites. BLiMP Benchmark features\nminimal pairs of a grammatical sentence W and\nan ungrammatical counterpart W∗. To evaluate a\nmodel on these minimal pairs, one simply com-\npares the likelihood of W and W∗assigned by the\nmodel.\nAs is implied by the evaluation methods, we\nneed to marginalize out the structure variables for\nPLM or PLM-mask models in order to estimate\nthe surprisal of a continuation, given a sentence\npreﬁx or the likelihood of a complete sentence.\nWe follow similar setup as in Futrell et al. (2019);\nWilcox et al. (2019) applying word-synchronous\nbeam search (Stern et al., 2017) to ﬁnd a list Yk of\nk incremental parses given a sentence preﬁx w<t.\nBLLIP-MD BLLIP-LG\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Accuracy\nModel Performance on SG Test Suites\nRNNG\nLM\nScLM-past\nScLM-next\nPLM\nPLM-mask\nGPT-2\nBLLIP-MD BLLIP-LG\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Accuracy\nModel Performance on BLiMP-10% Test Suites\nLM\nScLM-past\nScLM-next\nPLM\nPLM-mask\nGPT-2\nFigure 3: Comparing models’ overall accuracy across\ntest suites from SG Test Suites (top) and BLiMP-10%\n(bottom). RNNG performances are from Hu et al.\n(2020).\nWe then sum the joint probability p(w<t, y<t) over\nthe list of incremental parses given by the model to\napproximate the likelihood of p(w<t). We set the\nparse beam size to 100, word-synchronous beam\nsize k as 10, and fast track size of 5. Since the\nsearch process can be computationally intensive,\nthe large number of items in BLiMP benchmark\nposes a computational challenge. We therefore\nselect the ﬁrst 10% out of the 1000 items in each\nof the 67 tests of BLiMP Benchmark. We report\nthe accuracy over the 100 items and refer to this\ndown-sized BLiMP Benchmark as BLiMP-10%.\nWe compare models’ performance on the SG\nTest Suites and BLiMP-10% in Figure 3. Each bar\nshows a model’s performance averaged across mul-\ntiple seeds on a given benchmark, with each dot\nplotting the accuracy of a speciﬁc seed. Overall,\nsyntactic generalization performance improves as\nthe training data size increases from BLLIP- MD\n(14 million tokens) to BLLIP- LG (42 million to-\nkens). Models with structural guidance achieve\nhigher accuracy than the vanilla Transformer lan-\nguage model trained on the same set of raw text\ndata without explicit structural information. We\nalso include the results for the RNNGs taken from\nHu et al. (2020). RNNG lags behind all Trans-\nformer models by a large margin in average scores.\nWe also notice that among different forms of struc-\ntural guidance, generative parsing as language mod-\neling is the most effective in improving syntac-\ntic generalization performance against the baseline\ntransformer language models. We didn’t observe\nconsistent beneﬁts of adding dynamic masking\nmechanism to PLM. While scaffolding approach\nslightly improves vanilla Transformer language\nmodels, it still falls behind the best performance\nof the model trained with generative parsing. We\nhypothesize that our scaffold did not fully exploit\nthe compositional structure in the local parses by\nmodelling each action n-gram as a distinct type,\nwhile the generative parsing models only predict\nactions in a relatively small set of non-terminal ac-\ntion space, which might make it easier for PLM and\nPLM-mask to learn compositional generalization.\nWe leave it for future work to design new scaffolds\nthat can take advantage of the combinatorial nature\nof syntactic structure.\nFor completeness, we also ran the pre-trained\nGPT-2 model on the syntactic suites. This yielded\na score of 0.808 on the SG Test Suites and 0.827 on\nBLiMP-10% for the small version of pre-trained\nGPT-2. Among models trained on BLLIP- LG, the\naverage accuracy score on the SG Test Suites is\n0.723 for PLMs, 0.748 for PLM-masks, and 0.665\nfor LMs. Similar trend is observed on BLiMP-10%\nas well, where among models trained on BLLIP-\nLG the average accuracy is 0.751 for PLMs, 0.753\nfor PLM-masks, and 0.708 for LMs. The pro-\nposed PLM method is able to close the gap be-\ntween GPT-2 small and the same model trained\nwith BLLIP- LG by about half, while the improve-\nment for BLiMP is more modest but still signi-\nﬁcative. It remains an open question whether scal-\ning syntactic supervision to a larger dataset than\nBLLIP- LG would bring the generalization perfor-\nmance of PLM models closer to that of the pre-\ntrained GPT-2 model.\n3.2.1 Relationship between Perplexity and\nSyntactic Generalization Performance\nWe compare perplexity on the BLLIP held-out test\nset against syntactic generalization performance\nin Figure 4. Perplexities of PLM and PLM-mask\nmodels are computed setting the parse tree equal\nto the gold parse in Equation 3 to approximate the\nlikelihood. Note that, unlike Hu et al. (2020), all\n50 60\nWord-level Perplexity\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750SG Accuracy\nModel\nLM\nScLM-past\nScLM-next\nPLM\nPLM-mask\nCorpus\nBLLIP-MD\nBLLIP-LG\nCorpus\nBLLIP-MD\nBLLIP-LG\n50 60\nWord-level Perplexity\n0.68\n0.70\n0.72\n0.74\n0.76BLiMP-10% Accuracy\nModel\nLM\nScLM-past\nScLM-next\nPLM\nPLM-mask\nCorpus\nBLLIP-MD\nBLLIP-LG\nCorpus\nBLLIP-MD\nBLLIP-LG\nFigure 4: Comparison between model perplexity on\nBLLIP test data and syntactic generalization perfor-\nmance on SG Test Suites (top) and BLiMP-10% (bot-\ntom).\nour models use the same BPE vocabulary and word\ntokenization from GPT-2. The only exception are\nthe additional parsing actions in the vocabulary y.\nFrom Figure 4, both perplexity and syntactic gen-\neralization performance improve with dataset size.\nHowever, for both training dataset sizes, we see that\nstructural guidance can improve syntactic general-\nization. PLM models consistently perform better\nthan vanilla models. While all models achieve very\nsimilar perplexity results after being trained on a\nspeciﬁc dataset, their syntactic generalization per-\nformances differ dramatically.\n3.2.2 Effect of Structural Guidance on\nLearning Speciﬁc Syntactic Structures\nIn addition to comparing model’s aggregated per-\nformances, we also compare their generalization\nperformances in the clustered subsets of tests in SG\nTest Suites and BLiMP-10%. These subsets con-\nsist of several related tests that target speciﬁc type\nof syntactic phenomenon, such as NPI licensing,\nsubject-verb agreement, ﬁller-gap dependencies,\netc. We also include the results for the RNNGs\ntaken from Hu et al. (2020).\nResults in Figure 5 show converging evidence\nthat structural guidance in the form of generative\nparsing can robustly improve learning of subject-\nverb agreement and NPI licensing, and helps the\nmodel to better capture incremental processing phe-\nnomenon such as garden-path effects, but seems\nto slightly hurt the performance on gross syntactic\nstate. While overall the RNNG shows a poor per-\nformance this is mostly due to its very low scores\nfor licensing suites. Excluding these suites only\nthe RNNG shows a performance close to the PLM\nmodel, even outperforming it clearly for the gross\nsyntactic state suites. In this category and binding\nPLM variants seem inferior to all other models.\n4 Related Work\nMultitask learning (Caruana, 1997) has been ap-\nplied to a variety of NLP tasks with traditional\nmodeling approaches (Miller et al., 2000; Sutton\nand McCallum, 2005; Sutton et al., 2007) as well as\nmore recent neural models (Collobert et al., 2011;\nLi et al., 2020a). A recurring theme has been the\nuse of structure in the form of syntactic trees to\nbeneﬁt other NLP tasks. Among the early works\nexploring this direction, Punyakanok et al. (2008)\nshowed that syntactic parses can beneﬁt Semantic\nRole Labeling (SRL). Poon and Domingos (2009)\nextended this idea to induce ﬁrst-order logic repre-\nsentation in a unsupervised fashion, by clustering\nthe dependency structures. In both cases syntax\nforms part of a pipeline and is not strictly supervi-\nsion for the end task.\nThis trend continued with the rise of neural mod-\nels. Collobert et al. (2011) improved deep convolu-\ntion neural network for syntactic chunking models\nwith additional POS supervision. Zhang and Weiss\n(2016); Søgaard and Goldberg (2016) observe the\nbeneﬁts of POS supervision at different depths of a\nneural network model with impact on dependency\nparsing, tagging and CCG super tagging perfor-\nmance. He et al. (2019) perform a syntax-based\npruning of semantic roles, showing beneﬁts in a\nmultilingual setting. More recently, Sachan et al.\n(2020) incorporate a syntactic graph recurrent neu-\nral network into BERT models for better semantic\nrole labeling. However, their method shows little\nor no beneﬁt of syntax modeling for Named Entity\nRecognition and relation linking task. Neural ma-\nchine translation (Chen et al., 2018) and text gen-\neration (Li et al., 2020a) have also been shown to\nbeneﬁt from syntactic modeling. In a recent work,\nLi et al. (2020b) use syntactic modeling in BERT\nbased transformers to achieve performance gains\n0.05\n0.15\n0.25\n0.35\n0.45\n0.55\n0.65Accuracy\nLicensing (10 suites)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nLong-Distance Dependencies (8 suites)\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAgreement (3 suites)\nBLLIP-MD BLLIP-LG\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nGarden-Path Effects (6 suites)\nBLLIP-MD BLLIP-LG\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nGross Syntactic State (4 suites)\nBLLIP-MD BLLIP-LG\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nCenter Embedding (2 suites)\nModel Performance on Specific Clusters of SG Test Suites\nRNNG\nLM\nScLM-past\nScLM-next\nPLM\nPLM-mask\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nAnaphor Agreement (2 suites)\n0.55\n0.60\n0.65\n0.70\nArgument Structure (9 suites)\n0.60\n0.65\n0.70\n0.75\nBinding (7 suites)\n0.60\n0.65\n0.70\n0.75\nControl/Raising (5 suites)\n0.70\n0.75\n0.80\n0.85Accuracy\nDeterminer-Noun Agreement (8 suites)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nEllipsis (2 suites)\n0.60\n0.65\n0.70\n0.75\nFiller Gap (7 suites)\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nIrregular Forms (2 suites)\nBLLIP-MD BLLIP-LG\n0.40\n0.45\n0.50\n0.55\n0.60Accuracy\nIsland Effects (8 suites)\nBLLIP-MD BLLIP-LG\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nNPI Licensing (7 suites)\nBLLIP-MD BLLIP-LG\n0.55\n0.60\n0.65\n0.70\n0.75\nQuantifiers (4 suites)\nBLLIP-MD BLLIP-LG\n0.70\n0.75\n0.80\n0.85\nSubject-Verb Agreement (6 suites)\nModel Performance on Specific Clusters of BLiMP-10% Test Suites\nLM ScLM-past ScLM-next PLM PLM-mask\nFigure 5: Model performance comparison by speciﬁc linguistic phenomena clustered in SG Test Suites (top) and\nBLiMP-10% (bottom). RNNG performances are from Hu et al. (2020).\non several text classiﬁcation benchmarks. Other\nworks have found that structural supervision in the\nform of intermediate ﬁne-tuning (e.g., on CCG\nsuper tagging) is not helpful or even harmful (Pruk-\nsachatkun et al., 2020; Warstadt et al., 2019).\nThe focus of our work is on gauging the impact\nof joint modeling on syntactic generalization perfor-\nmance. In this direction, the work of Swayamdipta\net al. (2018) is close to the scaffolding version of\nour model. They predict multiple labels, extracted\nfrom syntactic information, as auxiliary task and\nshow positive effects on shallow semantic parsing\nand co-reference resolution. We use however a sin-\ngle feature, constituency parsing n-gram, which is\ncloser to prior work relying on Part-of-Speech in-\nformation. In addition, we explore impact of using\npreceding structure as feature vs postceding struc-\nture, which as shown plays a role in the learning\nprocess.\nIn terms of modeling objective and syntactic rep-\nresentations, our method is closest to the works of\nChoe and Charniak (2016); Dyer et al. (2016) that\njointly model syntax and language. A more recent\nwork from Peng et al. (2019) uses Rational Neural\nNetworks language model that can derive binary\nunlabeled constituents from attention weights and\ncan supervise the attention to attain a structural\ninductive bias. The proposed models show lower\nlanguage modeling perplexity compared to their\nstructure agnostic counterparts. We also extend\nhere the idea of syntax-aware language modeling\nto transformer-based language models.\nFinally, our approach relates to the other works\nthat propose ways of incorporating structural in-\nformation into Transformer-based models. This\nincludes the use of dependency or tree structure for\nconstraining self-attention patterns (Strubell et al.,\n2018; Wang et al., 2019; Zhang et al., 2020), guid-\ning cross-attention (Chen et al., 2018; Astudillo\net al., 2020), modelling syntactic distance (Du et al.,\n2020), using syntactic information to guide the\ncomputation ﬂow in the model (Shen et al., 2021),\nor through knowledge distillation (Kuncoro et al.,\n2020). Our structured masking in parsing as lan-\nguage modeling approach is close in spirit to the\nmethods that modify attention mechanism accord-\ning to syntactic connections (Astudillo et al., 2020);\nThis work, however, primarily aims to study the\nimpact of structural guidance on syntactic general-\nization. Therefore, we resort to simpler methods of\nincorporating structure to minimize the impact of\nmodeling intricacies.\n5 Conclusion\nOur work explores two forms of syntactic super-\nvision as structural guidance for Transformer lan-\nguage models. Experiments suggest that generative\nparsing approach can effectively improve system-\natic generalization of learned syntactic knowledge\nin small training data regime, while a naive syntac-\ntic scaffold approach does not improve the baseline\nto the same extent despite reduced computation\ncost at inference time. Future work may explore\nalternative structural guidance strategies that com-\nbine the best of both approaches.\nAcknowledgments\nThe authors would like to thank the anonymous\nreviewers for their helpful comments. This work\nwas supported by the MIT-IBM Watson AI Lab.\nReferences\nRam´on Fernandez Astudillo, Miguel Ballesteros,\nTahira Naseem, Austin Blodgett, and Radu Flo-\nrian. 2020. Transition-based parsing with stack-\ntransformers. page 1001–1007.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nRich Caruana. 1997. Multitask learning. Machine\nlearning, 28(1):41–75.\nEugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,\nJohn Hale, and Mark Johnson. 2000. Bllip 1987-89\nwsj corpus release 1. Linguistic Data Consortium,\nPhiladelphia, 36.\nKehai Chen, Rui Wang, Masao Utiyama, Eiichiro\nSumita, and Tiejun Zhao. 2018. Syntax-directed\nattention for neural machine translation. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 32.\nDo Kook Choe and Eugene Charniak. 2016. Parsing\nas language modeling. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2331–2336, Austin, Texas.\nAssociation for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of machine learning research ,\n12(ARTICLE):2493–2537.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWenyu Du, Zhouhan Lin, Yikang Shen, Timothy J.\nO’Donnell, Yoshua Bengio, and Yue Zhang. 2020.\nExploiting syntactic structure for better language\nmodeling: A syntactic distance approach. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6611–\n6628, Online. Association for Computational Lin-\nguistics.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 199–209, San Diego, California.\nAssociation for Computational Linguistics.\nJay Earley. 1970. An efﬁcient context-free parsing al-\ngorithm. Communications of the ACM , 13(2):94–\n102.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects: Representations of syntactic state. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 32–42, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nShexia He, Zuchao Li, and Hai Zhao. 2019. Syntax-\naware multilingual semantic role labeling. arXiv\npreprint arXiv:1909.00310.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 1725–1744, Online. Association for Compu-\ntational Linguistics.\nNikita Kitaev and Dan Klein. 2018. Constituency\nparsing with a self-attentive encoder. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), Melbourne, Australia. Association for Com-\nputational Linguistics.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1426–1436, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nAdhiguna Kuncoro, Lingpeng Kong, Daniel Fried,\nDani Yogatama, Laura Rimell, Chris Dyer, and Phil\nBlunsom. 2020. Syntactic structure distillation pre-\ntraining for bidirectional encoders. Transactions\nof the Association for Computational Linguistics ,\n8:776–794.\nYinghao Li, Rui Feng, Isaac Rehg, and Chao Zhang.\n2020a. Transformer-based neural text genera-\ntion with syntactic guidance. arXiv preprint\narXiv:2010.01737.\nZhongli Li, Qingyu Zhou, Chao Li, Ke Xu, and Yunbo\nCao. 2020b. Improving bert with syntax-aware local\nattention. arXiv preprint arXiv:2012.15150.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nScott Miller, Heidi Fox, Lance Ramshaw, and Ralph\nWeischedel. 2000. A novel use of statistical parsing\nto extract information from text. In 1st Meeting of\nthe North American Chapter of the Association for\nComputational Linguistics.\nHao Peng, Roy Schwartz, and Noah A. Smith. 2019.\nPaLM: A hybrid parser and language model. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3644–\n3651, Hong Kong, China. Association for Computa-\ntional Linguistics.\nHoifung Poon and Pedro Domingos. 2009. Unsu-\npervised semantic parsing. In Proceedings of the\n2009 conference on empirical methods in natural\nlanguage processing, pages 1–10.\nYada Pruksachatkun, Jason Phang, Haokun Liu,\nPhu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe\nPang, Clara Vania, Katharina Kann, and Samuel R\nBowman. 2020. Intermediate-task transfer learning\nwith pretrained models for natural language under-\nstanding: When and why does it work? arXiv\npreprint arXiv:2005.00628.\nVasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.\nThe importance of syntactic parsing and inference in\nsemantic role labeling. Computational Linguistics,\n34(2):257–287.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners.\nDevendra Singh Sachan, Yuhao Zhang, Peng Qi, and\nWilliam Hamilton. 2020. Do syntax trees help pre-\ntrained transformers extract information? arXiv\npreprint arXiv:2008.09084.\nYikang Shen, Shawn Tan, Alessandro Sordoni, Siva\nReddy, and Aaron Courville. 2021. Explicitly mod-\neling syntax in language models with incremental\nparsing and a dynamic oracle. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1660–1672, On-\nline. Association for Computational Linguistics.\nAnders Søgaard and Yoav Goldberg. 2016. Deep multi-\ntask learning with low level tasks supervised at lower\nlayers. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers) , pages 231–235, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nMitchell Stern, Daniel Fried, and Dan Klein. 2017. Ef-\nfective inference for generative neural parsing. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n1695–1700, Copenhagen, Denmark. Association for\nComputational Linguistics.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 5027–5038.\nCharles Sutton and Andrew McCallum. 2005. Joint\nparsing and semantic role labeling. Technical report,\nMASSACHUSETTS UNIV AMHERST DEPT OF\nCOMPUTER SCIENCE.\nCharles Sutton, Andrew McCallum, and Khashayar Ro-\nhanimanesh. 2007. Dynamic conditional random\nﬁelds: Factorized probabilistic models for labeling\nand segmenting sequence data. Journal of Machine\nLearning Research, 8(3).\nSwabha Swayamdipta, Sam Thomson, Kenton Lee,\nLuke Zettlemoyer, Chris Dyer, and Noah A. Smith.\n2018. Syntactic scaffolds for semantic structures.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 3772–3782, Brussels, Belgium. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nYaushian Wang, Hung-Yi Lee, and Yun-Nung Chen.\n2019. Tree transformer: Integrating tree structures\ninto self-attention. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1061–1070, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, et al. 2019. Investi-\ngating bert’s knowledge of language: Five analysis\nmethods with npis. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2870–2880.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the As-\nsociation for Computational Linguistics, 8:377–392.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019. Structural super-\nvision improves learning of non-local grammatical\ndependencies. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3302–3312, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nYuan Zhang and David Weiss. 2016. Stack-\npropagation: Improved representation learning for\nsyntax. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1557–1566, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nZhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng\nDuan, Hai Zhao, and Rui Wang. 2020. Sg-net: Syn-\ntax guided transformer for language representation.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence."
}