{
  "title": "Bimodal Speech Emotion Recognition Using Pre-Trained Language Models",
  "url": "https://openalex.org/W2992984162",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5017703745",
      "name": "Verena Heußer",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5009347823",
      "name": "Niklas Freymuth",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015363082",
      "name": "Stefan Constantin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5110453805",
      "name": "Alex Waibel",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1586759587",
    "https://openalex.org/W2117671523",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2753523903",
    "https://openalex.org/W2925618549",
    "https://openalex.org/W2143350951",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1965366140",
    "https://openalex.org/W2953739332",
    "https://openalex.org/W2747664154",
    "https://openalex.org/W2962770129",
    "https://openalex.org/W2797947982",
    "https://openalex.org/W2889462515",
    "https://openalex.org/W1501669607",
    "https://openalex.org/W2889169802",
    "https://openalex.org/W2972463723",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W2074788634",
    "https://openalex.org/W2898181186",
    "https://openalex.org/W2972451902",
    "https://openalex.org/W2888746944",
    "https://openalex.org/W2267835966",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2912728762",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2972498864",
    "https://openalex.org/W2889394771",
    "https://openalex.org/W2099341654",
    "https://openalex.org/W2803098682",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2003948868",
    "https://openalex.org/W2149628368",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2170784062",
    "https://openalex.org/W2986840739",
    "https://openalex.org/W2963631961",
    "https://openalex.org/W204722540",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2511566646",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2963119602",
    "https://openalex.org/W3098163860",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W3036267641",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3106003309",
    "https://openalex.org/W2046590632",
    "https://openalex.org/W2090777335",
    "https://openalex.org/W1596452078",
    "https://openalex.org/W1923034539",
    "https://openalex.org/W2952307697",
    "https://openalex.org/W2131204116",
    "https://openalex.org/W2752647243",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2963382687",
    "https://openalex.org/W2583643061",
    "https://openalex.org/W2145069607",
    "https://openalex.org/W2139511937",
    "https://openalex.org/W2750747353",
    "https://openalex.org/W2226884328",
    "https://openalex.org/W3098567156",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2973215447",
    "https://openalex.org/W2900898015",
    "https://openalex.org/W2975059944"
  ],
  "abstract": "Speech emotion recognition is a challenging task and an important step towards more natural human-machine interaction. We show that pre-trained language models can be fine-tuned for text emotion recognition, achieving an accuracy of 69.5% on Task 4A of SemEval 2017, improving upon the previous state of the art by over 3% absolute. We combine these language models with speech emotion recognition, achieving results of 73.5% accuracy when using provided transcriptions and speech data on a subset of four classes of the IEMOCAP dataset. The use of noise-induced transcriptions and speech data results in an accuracy of 71.4%. For our experiments, we created IEmoNet, a modular and adaptable bimodal framework for speech emotion recognition based on pre-trained language models. Lastly, we discuss the idea of using an emotional classifier as a reward for reinforcement learning as a step towards more successful and convenient human-machine interaction.",
  "full_text": "BIMODAL SPEECH EMOTION RECOGNITION USING PRE-TRAINED LANGUAGE\nMODELS\nVerena Heusser∗, Niklas Freymuth∗, Stefan Constantin, Alex Waibel\nKarlsruhe Institute of Technology\nInstitute for Anthropomatics and Robotics\n{verena.heusser, niklas.freymuth}@student.kit edu, {stefan.constantin, waibel}@kit.edu\nABSTRACT\nSpeech emotion recognition is a challenging task and an im-\nportant step towards more natural human-machine interac-\ntion. We show that pre-trained language models can be ﬁne-\ntuned for text emotion recognition, achieving an accuracy of\n69.5 %on Task 4A of SemEval 2017, improving upon the pre-\nvious state of the art by over 3 %absolute. We combine these\nlanguage models with speech emotion recognition, achieving\nresults of 73.5 %accuracy when using provided transcriptions\nand speech data on a subset of four classes of the IEMOCAP\ndataset. The use of noise-induced transcriptions and speech\ndata results in an accuracy of71.4 %. For our experiments, we\ncreated IEmoNet, a modular and adaptable bimodal frame-\nwork for speech emotion recognition based on pre-trained\nlanguage models. Lastly, we discuss the idea of using an emo-\ntional classiﬁer as a reward for reinforcement learning as a\nstep towards more successful and convenient human-machine\ninteraction.\nIndex Terms— Speech Emotion Recognition, Text Emo-\ntion Recognition, Bimodal Emotion Recognition, IEMOCAP,\nSelf Attention, Pre-trained Language Models\n1. INTRODUCTION\nEmotions are an important aspect of human behavior. They\ndo not only inﬂuence the reaction to our environment [1, 2],\nbut also actively change our perception of it [3] and some-\ntimes even contribute to how well we remember speciﬁc\nevents [4]. As such, they inﬂuence both human-human and\nhuman-machine interaction. However, in human-machine\ninteraction, emotions are often not at all or only scarcely\nconsidered. We propose to use automatically generated emo-\ntional feedback as a reward in Reinforcement Learning (RL).\nThis could be used to improve human-machine interaction,\nfor example when used with dialogue systems. In this paper,\nwe will focus on the tasks of automatic Text Emotion Recog-\nnition (TER) and automatic Speech Emotion Recognition\n(SER).\n∗equal contribution\nIt has been shown that combining textual features and a\nSER model into a multimodal system improves the overall\nclassiﬁcation performance [5, 6, 7, 8]. Additionally, the tasks\nof Automatic Speech Recognition (ASR) and language mod-\nelling have seen great improvements over the last year. This\nleads to a steady improvement in the quality of both tran-\nscripts and language models. Since most language models\nhave been shown to be able to solve a plethora of different\ntasks [9, 10], the combination of ASR and TER can be used\nto automatically transcribe a given utterance and use the lan-\nguage model for an emotion classiﬁcation. To show the ef-\nfectiveness of language models for TER, we ﬁne-tune both\nBERT [11] and XLNet [12], two self-attentive language mod-\nels, on Task 4A of SemEval 2017 [13].\nMotivated by these two recent advancements, we present\nIEmoNet (Interactive Emotion Network) which is a modular\nand adaptable bimodal SER framework based on pre-trained\nlanguage models. IEmoNetconsist of independently trainable\nASR, SER and TER sub-modules. They can be combined by\nﬁne-tuning only a few layers, omitting time-consuming re-\ntraining of the whole model. This allows us to utilize both tex-\ntual and paralinguistic features for emotion classiﬁcation. We\ntest our model on the ‘Text+Speech’ subtask of the Interac-\ntive Emotional dyadic MOtion CAPture (IEMOCAP) dataset\n[14]. Using 10-fold cross validation, IEmoNet achieves re-\nsults of 73.5 %when combining text and speech inputs. Using\nspeech and (simulated) automatically transcribed texts instead\nof the provided transcriptions results in 71.4 %accuracy.\n2. RELATED WORK\nIEmoNet is an approach to bimodal SER, meaning that it uses\ndifferent modalities to extract richer and more varied features.\nIn general, bi- and multimodal models combine modalities\nsuch as video recordings [15], more speciﬁc facial features\n[16], movements [17] and even EEG signals [18]. In our case\ntext, transcribed speech and speech are used, closely follow-\ning [7, 8, 19]. Previous research focused on the Time Delay\nNeural Network (TDNN) [20] and recurrent architectures like\nthe Long Short-Term Memory (LSTM) [21].\narXiv:1912.02610v1  [eess.AS]  29 Nov 2019\nRecently, ELMo [22], an LSTM-based pre-trained lan-\nguage model, has been successfully used to generate textual\nfeatures for multimodal emotion recognition [5]. We use pre-\ntrained language models based on Transformer encoder stacks\n[23] instead. These Transformer encoder stacks rely solely\non the self-attention mechanism [24] for contextualization.\nThey have shown great potential for ASR, SER and (emo-\ntional) language modelling individually, as will be shown in\nthe following sections.\n2.1. Speech Emotion Recognition\nSER has been studied for several decades [25], with early\nwork consisting of the classiﬁcation of a given utterance to\none of usually four distinct emotions based on prosodic fea-\ntures such as pitch, speech and intensity [26, 27]. While these\nrelatively simple features are still used today, deep neural ar-\nchitectures have led researchers to shift towards either more\nfeatures such as the IS09 emotional feature set [28], or more\nor less unprocessed audio/signal data as input [29, 30].\nTraining and evaluating SER models is challenging be-\ncause most datasets only include a few hours of spoken data,\nmaking it difﬁcult to train large neural networks without se-\nvere overﬁtting. Research on SER utilizes Mel Frequency\nCepstral Coefﬁcients (MFCCs), spectrograms and raw wave-\nforms as inputs, while the architectures vary from gated re-\ncurrent units [31] to a combination of LSTMs and Time De-\nlay Neural Networks (TDNNs) [32, 33]. Recently, the Trans-\nformer has also been used in SER, either directly [34] or as\na means of richer representations via predictive coding [35],\nachieving competitive results in both cases.\n2.2. Automatic Speech Recognition\nWhile ASR faces similar challenges as SER, research has\nshown that transfer between ASR and SER models is only\npossible in early layers of the respective models, meaning that\nboth tasks require their respective models to learn vastly dif-\nferent features [36].\nAs of today, a few architectures have shown to be well\nsuited for ASR, all of which make either direct or indirect\nuse of neural networks. Looking at the LibriSpeech corpus\n[37] as an example, Hybrid Models [38], Convolutional net-\nworks [39] and LSTMs [40, 41, 42] all achieve Word Error\nRates (WER) between 2.5 % and 3 % on the ’test-clean’ sub-\ntask when combined with a language model. Additionally,\nresearchers found that models based on the Transformer are\nalso competitive and very promising, especially in regards to\nmore complicated tasks [43, 44].\n2.3. Text Emotion Recognition\nTER is similar to sentiment analysis. Given an emotional\nmodel, usually Ekman’s Base Emotions [45], the task is to\nﬁnd the dominant emotion in a text. Common approaches are\neither keyword based or end-to-end. Keyword based methods\noperate on strong emotional words [46] or are based around\nmeta expressions like hashtags [47], emoticons [48], and\nemojis [49]. End-to-end architectures are mostly based on\nLSTMs and the attention mechanism [50, 51], allowing the\nmodels to correctly model long-term dependencies.\nHowever, with the introduction of attention-based models\nlike the Transformer [23] model and later BERT [11], gen-\neral language models are currently achieving great results on\nmost Natural Language Understanding tasks, including senti-\nment analysis. An example is the GLUE Benchmark [52], a\ncollection of various Natural Language Understanding tasks.\nSince GLUE’s release in 2018, BERT [11] and similar models\n[12, 53, 54, 55] gave way to over a dozen improvements over\nthe state of the art and eventually led to surpassing human\nperformance on this benchmark1.\nAdditionally, some of these language models are provided\nwith pre-trained models and weights, allowing for efﬁcient\nuse of resources via transfer learning. These models can be\nﬁne-tuned on a given task, which means that some or all of\nthe existing weights are trained on the new task for relatively\nfew iterations. This drastically reduces both training time and\nthe necessary amount of training samples, making it ideal for\nthe data-sparse ﬁeld of Emotion Recognition. Utilizing this,\nresearchers have achieved good results in both combining lan-\nguage models with LSTM architectures [56] and ﬁne-tuning\nthem on sentiment analysis tasks [57], showing that they are\nsuitable for the task at hand.\n2.4. Emotion in Reinforcement Learning\nIn the context of dialogue systems (and human-machine in-\nteraction in general) it seems rather important to integrate\nthe emotional state of the speaker into the overall decision\nmaking process of an agent. As of now, emotions have been\nused in both task-oriented [58] and non task-oriented dialogue\n[59], in both cases increasing the performance of the given\nsystem. Additionally, there has been a lot of research regard-\ning emotionally intelligent systems which implement and op-\nerate based on their own emotional state [60].\n3. REINFORCEMENT LEARNING\nUsing emotion recognition for reinforcement learning in di-\nalogue systems has the potential to greatly improve both the\nsystems usefulness and the users overall experience. For ex-\nample, a call center could track a caller’s anger [61] and an-\nnoyance [62] levels to see how satisﬁed they are with their\ncurrent call. In the case of a unsatisfactory calling experience,\nthe emotional reaction can then be interpreted as a negative\nreward and used to adjust the dialogue system, causing it to\n1See https://gluebenchmark.com/leaderboard (accessed\n2019-10-14)\nslightly change its behaviour and react more appropriately in\nfuture calls.\nHowever, there is an important distinction between emo-\ntion and mood [63, 64], the main difference being the\ntimescale of both concepts [65]. While emotion itself is\nshort-term, its effects on a person’s mood might persist for\na relatively long time. In the context of dialogue systems,\nthis means that a particularly good dialogue turn might be\nenough to cheer a person up and make them sound happier\nfor the rest of the interaction. This again generates more\n‘happy’ features for the next turns, causing the SER system\nto classify what would normally be ‘neutral’ as ‘happy’. To\ncounteract this effect of a ‘lingering’ mood, we propose to\nuse an approximated derivative of the emotional state instead\nof the state itself. For an emotion e and a dialogue history\nht at dialogue turn t, the output of the emotion recognition\nsystem can be seen as the posterior probability P(e|h). The\ndelta ∆P(e|ht) = P(e|ht) −P(e|ht−1) of each individual\nprobability can then be seen as the resulting change in its\ncorresponding emotion based on the last dialogue turn.\nAdditionally, certain messages are expected to have cer-\ntain emotional responses. Being the proverbial bearer of bad\nnews, for example, a dialogue system should not perceive an\nanticipated negative emotion as a negative reward. Mathemat-\nically, this can be seen as the difference between the probabil-\nities of the estimated emotional response Pest(e|ht) and the\nactual response P(e|ht) of the dialogue turn in question. Con-\nsidering mood, the unexpected change becomes ∆P(e|ht) −\n∆Pest(e|ht).\nSince a reward for reinforcement learning is usually\nbounded by a ﬁxed interval, a mapping f : E → [p, q]\nbetween the set of emotions E and their reward is necessary.\nIn our case, a negative emotion like ‘anger’ would be mapped\nto a negative value, the simplest case being −1. Taking this\ninto account, the actual reward rt used for reinforcement\nlearning would be\nrt =\n∑\ne∈E\nf(e) ·(∆P(e|ht) −∆Pest(e|ht)). (1)\nGiven that it is easier to give an estimated reward rather than\na set of changes in emotions, this can be simpliﬁed to\nrt =\n∑\ne∈E\nf(e) ·∆P(e|ht) −rest\nt , (2)\nwhere rest\nt is the estimated reward for dialogue turn t and\ncould be inferred from e.g. another neural network via re-\ngression.\n4. MODEL\nThe IEmoNet framework consists of ﬁve parts. A given ut-\nterance is ﬁrst forwarded into a pre-processing block, where\nboth ASR and SER features are extracted from the same raw\nsignal. These features are then fed into an ASR and a (neu-\nral) SER System respectively. The ASR System automatically\ngenerates a transcript of the utterance, which is then used as\nan input for the (also neural) TER System. Both the spoken\nand the TER system then provide their outputs as independent\nfeatures for the classiﬁcation block. Finally, the classiﬁcation\nblock outputs the detected emotion. An overview can be seen\nin Figure 1.\nFig. 1. The IEmoNet framework where each intermediate\nsystem (ASR, SER, TER) can be chosen and optimized indi-\nvidually.\nThis framework is very versatile and easily adaptable.\nThe intermediate systems, i.e. the ASR, SER and TER sys-\ntems, can be chosen and optimized independently, the only\nﬁxed variables being the input and the output size. Training\nIEmoNet is done in multiple steps. First the preprocessing\nblock extracts ASR and SER features. Depending on the\nused sub-systems, the ASR features can range from classic\nMFCCs+Deltas to augmented waveforms as used in [42]. For\nthe SER features either classical features such as pitch and\nprosody [26, 27], complex emotion recognition feature sets\n(for example the IS09 set [28]) or augmented waveforms [32]\nare plausible.\nAfter this, the utterances need to be transcribed to pro-\nvide textual data. We found that modern out-of-the-box ASR\nsystems generally provide high-quality transcriptions without\nthe need for additional ﬁne-tuning, allowing us to simply use\none of these systems to transcribe the given utterances. Note\nthat ﬁne-tuning an ASR system towards a given dataset would\nprobably provide slightly better transcriptions, leading to a\nslightly better performance overall. However, this comes at\nthe cost of signiﬁcantly more training time.\nThe SER and TER systems are trained individually on\ntheir parts of the given task, i.e. speech to emotion for SER\nand (possibly previously transcribed/generated) text to emo-\ntion for TER. For this, both models are extended by an aux-\niliary classiﬁcation block followed by a softmax layer, effec-\ntively creating two independent recognition systems.\nAfter this step is ﬁnished, we combine the SER and TER\nsystems by removing the classiﬁcation and softmax layers\nand replacing them with the classiﬁcation block, leaving their\nsecond-to-last layers as the next block’s input features. The\nclassiﬁcation block usually consists of a simple feedforward\nnetwork with a softmax classiﬁcation layer. However, more\nsohpisticated approaches employing the attention mechanism\nmight be better suited at aligning the two sets of features. The\nclassiﬁcation block is then trained while keeping the interme-\ndiate systems (ASR, SER and TER) unchanged. This is done\nto prevent potential overﬁtting due to huge relative differences\nin model size and structure.\nA big advantage of this modular setup is that any in-\ntermediate system can simply be replaced by one another,\nrequiring at worst the re-training of the classiﬁcation block\nand the extraction of new/other features in the pre-processing\nblock. This proves to be especially useful considering the\nongoing rapid development in both emotion recognition and\ndeep learning in general. IEmoNet can adapt to these im-\nprovements by upgrading an outdated sub-system to a newer\none without re-training all of the model.\n5. EXPERIMENTAL DETAILS / RESULTS\n5.1. Pre-trained Language Models\nThe performance of IEmoNet only relies on the ability of its\nintermediate systems to process sequence data. We found that\nusing pre-trained language models for the TER part drasti-\ncally improves performance while only taking very few train-\ning epochs to converge. For our experiments, we use BERT\n[11] and XLNet [12], two popular transformer-based general\nlanguage model that are available with pre-trained weights.\nThey both consist of either 24 (Large) or 12 (Base) Encoder\nTransformer blocks and are pre-trained on large resources of\ntextual data. For a detailed description of the Encoder Trans-\nformer block, see [23]. We use the case-sensitive (cased)\nModel Accuracy\nAll Neutral 0.483\nDataStories [50] 0.651\nBB twtr [51] 0.658\nLIA [68] 0.661\nNNEMBs [69] 0.664\nXLNet-Large 0.663 ±0.030\nBert-Base 0.681 ±0.007\nBert-Large 0.687 ±0.015\nXLNet-Base 0.695 ±0.007\nTable 1. Test accuracy on Task 4A of SemEval 2017. The\nbest models are in bold. We repeat each experiment ﬁve times\nand report the mean and standard deviation.\nversions of both models. Training BERT and XLNet on all\ntasks was conducted using the Adam-optimizer [66] with a\nlearning rate of 3e −5. We use the provided WordPiece em-\nbeddings for BERT and byte-pair encodings for XLNet. The\nsequence length is set to 48. Longer/Shorter inputs are trun-\ncated/padded. We add a fully-connected layer of 768 units\nwith Dropout [67] of 0.2 and L2-regularization of 0.03 be-\ntween the pre-trained models and the softmax layer. We use\nweighted cross-entropy as our loss function to counteract the\nimbalanced datasets. These hyperparameters were chosen ex-\nperimentally and work equally well on both SemEval and\nIEMOCAP.\n5.2. SemEval Experiments\nWe found that using pre-trained language models and ﬁne-\ntuning them on emotional datasets achieves good results. In\naddition, ﬁne-tuning instead of training from scratch signif-\nicantly reduces training time and resource usage. To show\nthe effectiveness of pre-trained language models for TER, we\nrun some tests on Task 4A of the SemEval 2017 competition\n[13], which is a 3-class sentiment analysis task. The input is a\ntweet (a twitter post containing no more than 280 characters),\nthe output is the predicted sentiment, in this case either ‘pos-\nitive’, ‘neutral’, or ‘negative’. We choose this task because of\nits similarity to TER. The sentiments may be roughly mapped\nto the emotions ‘happy’, ‘neutral’ and ‘sad’/‘angry’.\nThe dataset is hand-annotated and consists of about\n50.000 training and 12.000 test samples consisting of tweets\nfrom 2012 to 2017. In the training set, the data consists of\nroughly 39.5/44.8/15.6 % positive/neutral/negative tweets\nwhile the test set has a distribution of 19.3/48.3/32.3 % for\nthe respective classes. In both cases, neutral tweets are the\nmost common. However, the training data has a lot more pos-\nitive tweets while the test set is more shifted towards negative\nones. This causes the task to be more challenging because\nthe models need to detect negative sentiments with relatively\nlittle training data.\nWe experiment with both BERT and XLNet and ﬁnd XL-\nNet to be slightly superior in performance. The tweets were\nnot pre-processed.\nThe results can be seen in Table 1. We train each model\nﬁve times with different initial weights and report the mean\nand standard deviation for each model. Our best model\n(XLNet-Base) achieves an accuracy of 69.5 % after only one\nepoch of training. This is an improvement of over 3% ab-\nsolute over previous models, showing the effectiveness of\npre-trained models for this task.\n5.3. IEMOCAP Experiments\nThe IEMOCAP dataset provides 12 hours of emotional audio\ndialogues, recorded with ﬁve pairs of professional actors in\nboth scripted and spontaneous sessions. At least three anno-\ntators were asked to label each of the dialogues using Ekmans\nBase Emotions [45] and a variation of Russells Circumplex\nModel [70]. The inter-annotator agreement for the task was\n74.6 % (66.0 % for scripted and 83.1 % for spontaneous utter-\nances) [14]. Because of the scarcity of other emotions, we\nfollow common research [8, 33, 7] and only classify the emo-\ntions ‘happiness’, ‘sadness’, ‘anger’ and ‘neutral’. Our ﬁnal\ndataset contains 38.04 %neutral, 24.57 %angry, 24.14 %sad\nand 13.24 % happy samples. We split each dialogue into ut-\nterances from 3 to 15 seconds, with one utterance holding\nthe information of one speaker (i.e. mono audio data and\nthe respective transcription). Each utterance is then classiﬁed\nindividually and without context information. We use both\nscripted and spontaneous dialogues for all experiments.\nWe use 10-fold cross-validation with randomly shufﬂed\ndata with a ﬁxed random seed. Note that some researchers\n(e.g. [33, 5]) test their models in a speaker-exclusive setting\ninstead. They split the dataset by speakers, training on most\nspeakers and testing exclusively on the remaining few. We\ndenote the respective papers with a ‘*’. We train each model\nthree times with different initial weights and report the mean\nof all models and the mean of the three standard deviations.\nWe evaluate the models with respect to weighted and un-\nweighted accuracy. The Weighted Accuracy (W A) is given by\nthe number of correctly classiﬁed samples divided by the total\nnumber of samples. The Unweighted Accuracy (UA) corre-\nsponds to ﬁrst calculating the accuracy for each emotion class\nand then averaging by the number of samples per class.\n5.3.1. IEMOCAP Text Experiments\nAs described in Section 4, we train IEmoNet’s modalities\nindependently at ﬁrst. Motivated by their performance on\nthe SemEval task (see Section 5.2), we use both BERT and\nXLNet as TER models. We use the Base versions (i.e. 12\nTransformer Encoder Blocks) for all experiments because\nthey are roughly three times smaller than the large ones, mak-\ning them more feasible for real-time applications. To compare\nModel WA UA T\nTRE [8] 0.635 ±0.018 – P\nLSTM* [7] 0 .648 – P\nXLNet 0.694 ±0.015 0 .671 ±0.019 P\nBERT 0.709 ±0.015 0 .691 ±0.018 P\nTRE-ASR [8] 0.593 ±0.022 – A\nXLNet-A 0.668 ±0.020 0 .645 ±0.023 A\nBERT-A 0.685 ±0.017 0 .665 ±0.023 A\nTable 2. Weighted Accuracy (W A) and Unweighted Accu-\nracy (UA) for TER on the IEMOCAP dataset. Models without\ncross-validation are in cursive. Models with ‘*’ use speaker-\nexclusive testing. The T row denotes the Transcription. P\nstands for Provided and A for Automatic.\nthe theoretical and the practical performance, we train on both\nprovided and artiﬁcial ASR transcriptions [71, 72] with 6 %\nWER. The latter are used as an alternative to an ASR system\nand are conﬁgured to simulate typical transcription errors as\ndescribed in [71]. This WER was chosen to match the one\nreported in [8]. We use artiﬁcial noise instead of an ASR\nsystem due to time constraints to integrate an ASR of high\nquality.\nFor all text-related experiments, we omitted the use of a\nvalidation split, instead training on a ﬁxed number of seven\nepochs for all folds. The number seven was chosen because\nthe training loss usually stabilized after seven epochs. The re-\nsults can be seen in Table 2. Here, BERT slightly outperforms\nXLNet, achieving an accuracy of70.9 %on the provided tran-\nscripts and 68.5 % on the noisy ones. While our models’ per-\nformances still drops signiﬁcantly when using noisy instead\nof provided transcriptions, they seem to be more robust to this\nnoise than the one reported in [8]. We hypothesize that the ex-\ntensive pre-training on large amounts of textual data leads to\nthis increase in stability.\n5.3.2. IEMOCAP Speech Experiments\nFor the SER part, we train our models on the IS09 feature set\n[28], following previous research [34, 73]. The features were\nextracted using the openSMILE feature extraction tool [74]\nwith a frame size of300 frames and a stride of0.1, resulting in\nan overlap of 0.9 between two frames. We then pad/truncate\nthe sequences to a ﬁxed length of 384 frames.\nThe results in Table 3 are achieved using simple Attention-\nBiLSTM models. The models use a TDNN layer followed by\ntwo BiLSTM layers and an attention layer. The TDNN layer\nis used to map the inputs to the dimension of the BiLSTMs.\nThe output of the second BiLSTM layer is then fed into the at-\ntention layer. Then, the outputs of the second BiLSTM layer\nand the attention layer are concatenated and used as the input\nfor a softmax layer. Our best model, BiLSTM-128attDim,\nused a TDNN with ﬁlter size 16, 64 dimensional LSTMs\nModel WA UA\nIS09-Self-Att [34] 0.681 0.638\nCNN-LSTM* [33] 0.688 0.594\nAtt-TDNN/LSTM* [32] 0 .701 0 .607\nBiLSTM-64attDim 0.600 ±0.020 0 .531 ±0.020\nBiLSTM-128attdim 0.608 ±0.017 0 .526 ±0.013\nTable 3. Weighted Accuracy (W A) and Unweighted Accu-\nracy (UA) on SER on the IEMOCAP dataset. Models without\ncross-validation are in cursive. Models with ‘*’ use speaker-\nexclusive testing.\nModel WA UA T\nH-MM-4* [5] 0.717 – P\nMDRE [8] 0.718 ±0.019 – P\nAtt-align* [19] 0 .725 0 .709 P\nIEmoNet(XL) 0.715 ±0.015 0 .689 ±0.016 P\nIEmoNet(BE) 0.735 ±0.016 0 .710 ±0.016 P\nMDRE-ASR[8] 0.691 ±0.019 – A\nAtt-align-A* [19] 0 .704 0 .695 A\nIEmoNet(XL)-A 0.687 ±0.020 0 .657 ±0.020 A\nIEmoNet(BE)-A 0.714 ±0.017 0 .686 ±0.022 A\nTable 4. Weighted Accuracy (W A) and Unweighted Accu-\nracy (UA) on IEMOCAP using both speech and text data.\nModels without cross-validation are in cursive. Models with\n‘*’ use speaker-exclusive testing. The T row denotes the Tran-\nscription. P stands for Provided and A for Automatic.\n(in both directions) and self-attention with dimension 128.\nIt achieved an accuracy of 60.8 %. The second best model,\nBiLSTM-64attDim, uses the same conﬁguration, except for\nan attention dimension of 64, achieving 60.0 % weighted\naccuracy. Note that more sophisticated SER models have\nbeen shown to yield far better performance and would thus be\nbetter suited for a combined bimodal architecture. However,\nwe primarily want to showcase the capability of pre-trained\nlanguage models for bimodal Emotion Recognition, making\nthese basic models sufﬁcient for this purpose.\n5.3.3. IEMOCAP Bimodal Experiments\nWe combine our best individual models as described in Sec-\ntion 4. Again, each model is trained for a ﬁxed number\nof seven epochs. The results can be found in Table 4.\nIEmoNet(XL) uses the best XLNet models from Table 2\nas its text sub-system while IEmoNet(BE) uses the BERT\nmodels instead. Both combine them with the best SER model\nfrom Table 3. We also tried ﬁne-tuning the whole model\n(i.e. the classiﬁcation block, the TER and the SER parts\ntogether) after the seven epochs. However, the performance\nwas signiﬁcantly worse than the models that only trained the\nclassiﬁcation block. This is likely caused by the size of BERT\nand XLNet allowing them to easily dominate the classiﬁca-\ntion outcome. The best models achieve 73.5 % and 71.4 %\naccuracy for provided and automatic transcripts respectively.\n6. CONCLUSION / FUTURE WORK\nDetecting emotions in speech is a challenging task that marks\nan important step towards more natural and adaptive human-\ncomputer interaction. We show that self-attentive pre-trained\nlanguage models are well suited for text emotion recogni-\ntion and that the Transformer model in general works well\nfor bimodal Emotion Recognition. Our experiments result in\n69.5 % accuracy on Task 4A of SemEval 2017. We achieve\n73.5 % and on the ‘Text+Speech’ subtask of the IEMOCAP\ndataset. Using speech and (simulated) automatic transcrip-\ntions results in 71.4 % instead. The results on IEMOCAP\nare achieved by ﬁne-tuning BERT and combining it with a\ntraditional speech emotion recognition system. This is done\nusing IEmoNet, a modular framework that classiﬁes both the\nspeech data and a textual transcription in independent sub-\nmodules that can be switched out without pre-training the\nwhole model.\nWe suggest using Speech Emotion Recognition as a re-\nward for Reinforcement Learning. It could be used to improve\nthe quality of dialogue systems by allowing them to adjust\ntheir behaviour based on the emotional reactions of their di-\nalogue partner. We mention possible problems that arise due\nto different aspects of human emotion and propose a suitable\nreward function to circumvent them.\nFuture work will involve the implementation of our\nframework into a Reinforcement Learning agent as well as\nmore sophisticated combinations of Speech Emotion Recog-\nnition sub-modules. We will also repeat our experiments with\nan ASR system instead of artiﬁcial noise. Another possible\ndirection is to optimize pre-trained language models for emo-\ntional context, for example by adding an additional training\nstep before ﬁne-tuning them on a given task.\n7. ACKNOWLEDGEMENTS\nThe project on which this report is based was funded by the\nFederal Ministry of Education and Research (BMBF) of Ger-\nmany under the number 01IS18040A. The authors are respon-\nsible for the content of this publication.\n8. REFERENCES\n[1] Benedetto De Martino, Dharshan Kumaran, Ben Sey-\nmour, and Raymond J. Dolan, “Frames, biases, and ra-\ntional decision-making in the human brain,” Science,\nvol. 313, no. 5787, pp. 684–687, 2006.\n[2] Jeffrey Bouffard, “The inﬂuence of emotion on ratio-\nnal decision making in sexual aggression,” Journal of\nCriminal Justice, vol. 30, pp. 121–134, 02 2002.\n[3] Elizabeth A. Phelps, Sam Ling, and Marisa Carrasco,\n“Emotion facilitates perception and potentiates the per-\nceptual beneﬁts of attention,” Psychological Science,\nvol. 17, no. 4, pp. 292–299, 2006, PMID: 16623685.\n[4] Elizabeth A. Kensinger, “Negative emotion enhances\nmemory accuracy: Behavioral and neuroimaging evi-\ndence,” Current Directions in Psychological Science ,\nvol. 16, no. 4, pp. 213–218, 2007.\n[5] Gustavo Aguilar, Viktor Rozgic, Weiran Wang, and\nChao Wang, “Multimodal and multi-view models for\nemotion recognition,” in Proceedings of the 57th An-\nnual Meeting of the Association for Computational Lin-\nguistics, Florence, Italy, July 2019, pp. 991–1002, As-\nsociation for Computational Linguistics.\n[6] Jaejin Cho, Raghavendra Pappagari, Purva Kulkarni,\nJess Villalba, Yishay Carmiel, and Najim Dehak, “Deep\nneural networks for emotion recognition combining au-\ndio and transcripts,” in Proc. Interspeech 2018, 2018,\npp. 247–251.\n[7] Samarth Tripathi and Homayoon S. M. Beigi, “Multi-\nmodal emotion recognition on IEMOCAP dataset using\ndeep learning,” CoRR, vol. abs/1804.05788, 2018.\n[8] Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,\n“Multimodal speech emotion recognition using audio\nand text,” in 2018 IEEE Spoken Language Technology\nWorkshop (SLT). IEEE, 2018, pp. 112–118.\n[9] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever, “Language models\nare unsupervised multitask learners,” OpenAI Blog, vol.\n1, no. 8, 2019.\n[10] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao, “Multi-task deep neural networks for natu-\nral language understanding,” in Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, Florence, Italy, July 2019, pp. 4487–4496,\nAssociation for Computational Linguistics.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, “Bert: Pre-training of deep bidirec-\ntional transformers for language understanding,” inPro-\nceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 2019, pp. 4171–4186.\n[12] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le, “Xl-\nnet: Generalized autoregressive pretraining for lan-\nguage understanding,” in Advances in Neural Informa-\ntion Processing Systems 32, H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d´Alch´e-Buc, E. Fox, and R. Garnett,\nEds., pp. 5754–5764. Curran Associates, Inc., 2019.\n[13] Sara Rosenthal, Noura Farra, and Preslav Nakov,\n“SemEval-2017 task 4: Sentiment analysis in twit-\nter,” in Proceedings of the 11th International Work-\nshop on Semantic Evaluation (SemEval-2017), Vancou-\nver, Canada, Aug. 2017, pp. 502–518, Association for\nComputational Linguistics.\n[14] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jeannette N.\nChang, Sungbok Lee, and Shrikanth S. Narayanan,\n“Iemocap: interactive emotional dyadic motion capture\ndatabase,” Language Resources and Evaluation, vol. 42,\nno. 4, pp. 335, Nov 2008.\n[15] S. Poria, I. Chaturvedi, E. Cambria, and A. Hussain,\n“Convolutional mkl based multimodal emotion recog-\nnition and sentiment analysis,” in 2016 IEEE 16th In-\nternational Conference on Data Mining (ICDM) , Dec\n2016, pp. 439–448.\n[16] Carlos Busso, Zhigang Deng, Serdar Yildirim, Murtaza\nBulut, Chul Min Lee, Abe Kazemzadeh, Sungbok Lee,\nUlrich Neumann, and Shrikanth Narayanan, “Analysis\nof emotion recognition using facial expressions, speech\nand multimodal information,” in Proceedings of the\n6th International Conference on Multimodal Interfaces,\nNew York, NY , USA, 2004, ICMI ’04, pp. 205–211,\nACM.\n[17] Fatemeh Noroozi, Ciprian Corneanu, Dorota Kamiska,\nTomasz Sapiski, Sergio Escalera, and Gholamreza An-\nbarjafari, “Survey on emotional body gesture recogni-\ntion,” IEEE Transactions on Affective Computing , vol.\nPP, 01 2018.\n[18] M. Sreeshakthy and J. Preethi, “Classiﬁcation of human\nemotion from deap eeg signal using hybrid improved\nneural networks with cuckoo search,”BRAIN. Broad Re-\nsearch in Artiﬁcial Intelligence and Neuroscience , vol.\n6, no. 3-4, pp. 60–73, 2016.\n[19] Haiyang Xu, Hui Zhang, Kun Han, Yun Wang, Yiping\nPeng, and Xiangang Li, “Learning Alignment for Mul-\ntimodal Emotion Recognition from Speech,” in Proc.\nInterspeech 2019, 2019, pp. 3569–3573.\n[20] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and\nK. J. Lang, “Phoneme recognition using time-delay neu-\nral networks,” IEEE Transactions on Acoustics, Speech,\nand Signal Processing , vol. 37, no. 3, pp. 328–339,\nMarch 1989.\n[21] Sepp Hochreiter and J ¨urgen Schmidhuber, “Long short-\nterm memory,” Neural computation, vol. 9, no. 8, pp.\n1735–1780, 1997.\n[22] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer, “Deep contextualized word representa-\ntions,” in Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), New Orleans, Louisiana, June\n2018, pp. 2227–2237, Association for Computational\nLinguistics.\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin, “Attention is all you need,” in Ad-\nvances in neural information processing systems, 2017,\npp. 5998–6008.\n[24] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio, “Neural machine translation by jointly learning to\nalign and translate,” in 3rd International Conference\non Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceed-\nings, 2015.\n[25] Purnima Chandrasekar, Santosh V . Chapaneri, and\nDeepak J. Jayaswal, “Automatic speech emotion recog-\nnition: A survey,” 2014 International Conference\non Circuits, Systems, Communication and Information\nTechnology Applications (CSCITA), pp. 341–346, 2014.\n[26] Frank Dellaert, Thomas Polzin, and Alex Waibel, “Rec-\nognizing emotion in speech,” in Proceeding of Fourth\nInternational Conference on Spoken Language Process-\ning. ICSLP’96. IEEE, 1996, vol. 3, pp. 1970–1973.\n[27] Thomas Polzin and Alex Waibel, “Detecting emo-\ntions in speech,” Proceedings of the 2nd International\nConference on Cooperative Multimodal Communica-\ntion, CMC, 1 1998.\n[28] Bjrn Schuller, Stefan Steidl, and Anton Batliner, “A.:\nThe interspeech 2009 emotion challenge,” in In ISCA,\ned.: Proceedings of Interspeech, 2009, pp. 312–315.\n[29] Moataz El Ayadi, Mohamed S. Kamel, and Fakhri Kar-\nray, “Survey on speech emotion recognition: Features,\nclassiﬁcation schemes, and databases,” Pattern Recog-\nnition, vol. 44, no. 3, pp. 572 – 587, 2011.\n[30] Bj ¨orn W Schuller, “Speech emotion recognition: Two\ndecades in a nutshell, benchmarks, and ongoing trends,”\nCommunications of the ACM, vol. 61, no. 5, pp. 90–99,\n2018.\n[31] Kyunghyun Cho, Bart van Merri ¨enboer, Dzmitry Bah-\ndanau, and Yoshua Bengio, “On the properties of neural\nmachine translation: Encoder–decoder approaches,” in\nProceedings of SSST-8, Eighth Workshop on Syntax, Se-\nmantics and Structure in Statistical Translation , Doha,\nQatar, Oct. 2014, pp. 103–111, Association for Compu-\ntational Linguistics.\n[32] Mousmita Sarma, Pegah Ghahremani, Daniel Povey,\nNagendra Kumar Goel, Kandarpa Kumar Sarma, and\nNajim Dehak, “Emotion identiﬁcation from raw speech\nsignals using dnns,” in Proc. Interspeech 2018, 2018,\npp. 3097–3101.\n[33] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efﬁ-\ncient emotion recognition from speech using deep learn-\ning on spectrograms.,” in INTERSPEECH, 2017, pp.\n1089–1093.\n[34] Lorenzo Tarantino, Philip N Garner, and Alexandros\nLazaridis, “Self-attention for speech emotion recogni-\ntion,” Proc. Interspeech 2019, pp. 2578–2582, 2019.\n[35] Zheng Lian, Ya Li, Jianhua Tao, and Jian Huang, “Im-\nproving speech emotion recognition via transformer-\nbased predictive coding through transfer learning,”\narXiv preprint arXiv:1811.07691, 2018.\n[36] Haytham M. Fayek, Margaret Lech, and Lawrence\nCavedon, “On the correlation and transferability of fea-\ntures between automatic speech recognition and speech\nemotion recognition,” in Interspeech 2016, 2016, pp.\n3618–3622.\n[37] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-\njeev Khudanpur, “Librispeech: An ASR corpus based\non public domain audio books,” in 2015 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). apr 2015, IEEE.\n[38] Xuerui Yang, Jiwei Li, and Xi Zhou, “A novel\npyramidal-fsmn architecture with lattice-free mmi for\nspeech recognition,” CoRR, vol. abs/1810.11352, 2018.\n[39] Jason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan Leary,\nOleksii Kuchaiev, Jonathan M. Cohen, Huyen Nguyen,\nand Ravi Teja Gadde, “Jasper: An End-to-End Convo-\nlutional Neural Acoustic Model,” in Proc. Interspeech\n2019, 2019, pp. 71–75.\n[40] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, at-\ntend and spell: A neural network for large vocabulary\nconversational speech recognition,” in 2016 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), March 2016, pp. 4960–4964.\n[41] Kazuki Irie, Rohit Prabhavalkar, Anjuli Kannan, An-\ntoine Bruguier, David Rybach, and Patrick Nguyen, “On\nthe Choice of Modeling Unit for Sequence-to-Sequence\nSpeech Recognition,” in Proc. Interspeech 2019, 2019,\npp. 3800–3804.\n[42] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng\nChiu, Barret Zoph, Ekin D. Cubuk, and Quoc V . Le,\n“SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition,” in Proc. Inter-\nspeech 2019, 2019, pp. 2613–2617.\n[43] Linhao Dong, Shuang Xu, and Bo Xu, “Speech-\ntransformer: A no-recurrence sequence-to-sequence\nmodel for speech recognition,”2018 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 5884–5888, 2018.\n[44] Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues,\nMarkus Mller, and Alex Waibel, “Very Deep Self-\nAttention Networks for End-to-End Speech Recogni-\ntion,” in Proc. Interspeech 2019, 2019, pp. 66–70.\n[45] Paul Ekman, “Basic emotions,” Handbook of cognition\nand emotion, pp. 45–60, 1999.\n[46] Pollyanna Gonc ¸alves, Matheus Ara ´ujo, Fabr ´ıcio Ben-\nevenuto, and Meeyoung Cha, “Comparing and combin-\ning sentiment analysis methods,” in Proceedings of the\nFirst ACM Conference on Online Social Networks, New\nYork, NY , USA, 2013, COSN ’13, pp. 27–38, ACM.\n[47] Efthymios Kouloumpis, Theresa Wilson, and Jo-\nhanna D. Moore, “Twitter sentiment analysis: The good\nthe bad and the omg!,” in ICWSM, 2011.\n[48] Alexander Hogenboom, Daniella Bal, Flavius Frasin-\ncar, Malissa Bal, Franciska de Jong, and Uzay Kaymak,\n“Exploiting emoticons in sentiment analysis,” in Pro-\nceedings of the 28th Annual ACM Symposium on Ap-\nplied Computing, New York, NY , USA, 2013, SAC ’13,\npp. 703–710, ACM.\n[49] Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad Rah-\nwan, and Sune Lehmann, “Using millions of emoji oc-\ncurrences to learn any-domain representations for de-\ntecting sentiment, emotion and sarcasm,” in Proceed-\nings of the 2017 Conference on Empirical Methods in\nNatural Language Processing, Copenhagen, Denmark,\nSept. 2017, pp. 1615–1625, Association for Computa-\ntional Linguistics.\n[50] Christos Baziotis, Nikos Pelekis, and Christos Doulk-\neridis, “DataStories at SemEval-2017 task 4: Deep\nLSTM with attention for message-level and topic-based\nsentiment analysis,” in Proceedings of the 11th Inter-\nnational Workshop on Semantic Evaluation (SemEval-\n2017), Vancouver, Canada, Aug. 2017, pp. 747–754,\nAssociation for Computational Linguistics.\n[51] Mathieu Cliche, “BB twtr at SemEval-2017 task 4:\nTwitter sentiment analysis with CNNs and LSTMs,”\nin Proceedings of the 11th International Workshop\non Semantic Evaluation (SemEval-2017) , Vancouver,\nCanada, Aug. 2017, pp. 573–580, Association for Com-\nputational Linguistics.\n[52] Alex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman, “GLUE: A\nmulti-task benchmark and analysis platform for natural\nlanguage understanding,” in Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and Inter-\npreting Neural Networks for NLP , Brussels, Belgium,\nNov. 2018, pp. 353–355, Association for Computational\nLinguistics.\n[53] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov, “Roberta: A ro-\nbustly optimized bert pretraining approach,” 2019.\n[54] Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu, “Freelb: Enhanced adversarial\ntraining for language understanding,” 2019.\n[55] Zhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut, “Al-\nbert: A lite bert for self-supervised learning of language\nrepresentations,” 2019.\n[56] Chenyang Huang, Amine Trabelsi, and Osmar R.\nZa¨ıane, “ANA at semeval-2019 task 3: Contex-\ntual emotion detection in conversations through hier-\narchical lstms and BERT,” in Proceedings of the\n13th International Workshop on Semantic Evaluation,\nSemEval@NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 6-7, 2019, 2019, pp. 49–53.\n[57] Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu, “BERT\npost-training for review reading comprehension and\naspect-based sentiment analysis,” in Proceedings of\nthe 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2019, Minneapo-\nlis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short\nPapers), 2019, pp. 2324–2335.\n[58] Milan Gnjatovi ´c and Dietmar R ¨osner, “Adaptive dia-\nlogue management in the nimitek prototype system,” in\nPerception in Multimodal Dialogue Systems , Elisabeth\nAndr´e, Laila Dybkjær, Wolfgang Minker, Heiko Neu-\nmann, Roberto Pieraccini, and Michael Weber, Eds.,\nBerlin, Heidelberg, 2008, pp. 14–25, Springer Berlin\nHeidelberg.\n[59] Yuya Chiba, Takashi Nose, Taketo Kase, Mai Ya-\nmanaka, and Akinori Ito, “An analysis of the effect\nof emotional speech synthesis on non-task-oriented dia-\nlogue system,” in Proceedings of the 19th Annual SIG-\ndial Meeting on Discourse and Dialogue , Melbourne,\nAustralia, July 2018, pp. 371–375, Association for\nComputational Linguistics.\n[60] Thomas M. Moerland, Joost Broekens, and Catholijn M.\nJonker, “Emotion in reinforcement learning agents and\nrobots: a survey,” Machine Learning, vol. 107, no. 2,\npp. 443–480, Feb 2018.\n[61] F. Burkhardt, J. Ajmera, R. Englert, J. Stegmann, and\nW. Burleson, “Detecting anger in automated voice por-\ntal dialogs,” inProceedings of the Annual Conference of\nthe International Speech Communication Association,\nINTERSPEECH, 2006, vol. 2, pp. 1053–1056.\n[62] Jon Irastorza and Mar ´ıa In´es Torres, “Tracking the ex-\npression of annoyance in call centers,” in Cognitive In-\nfocommunications, Theory and Applications , pp. 131–\n151. Springer, 2019.\n[63] Chantal Berna, Siri Leknes, Emily A Holmes, Robert R\nEdwards, Guy M Goodwin, and Irene Tracey, “Induc-\ntion of depressed mood disrupts emotion regulation neu-\nrocircuitry and enhances pain unpleasantness,” Biologi-\ncal psychiatry, vol. 67, no. 11, pp. 1083–1090, 2010.\n[64] Sara C Sereno, Graham G Scott, Bo Yao, Elske J\nThaden, and Patrick J O’Donnell, “Emotion word pro-\ncessing: does mood make a difference?,” Frontiers in\npsychology, vol. 6, pp. 1191, 2015.\n[65] Andrew Lane, Christopher Beedie, and Peter Terry,\n“Distinctions between emotion and mood,” Cognition\nand Emotion, vol. 19, 09 2005.\n[66] Diederik Kingma and Jimmy Ba, “Adam: A method for\nstochastic optimization,” International Conference on\nLearning Representations, 12 2014.\n[67] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov, “Dropout: A\nsimple way to prevent neural networks from overﬁtting,”\nJ. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958, Jan.\n2014.\n[68] Mickael Rouvier, “LIA at SemEval-2017 task 4: An\nensemble of neural networks for sentiment classiﬁca-\ntion,” in Proceedings of the 11th International Work-\nshop on Semantic Evaluation (SemEval-2017), Vancou-\nver, Canada, Aug. 2017, pp. 760–765, Association for\nComputational Linguistics.\n[69] Yichun Yin, Yangqiu Song, and Ming Zhang,\n“NNEMBs at SemEval-2017 task 4: Neural twitter sen-\ntiment classiﬁcation: a simple ensemble method with\ndifferent embeddings,” inProceedings of the 11th Inter-\nnational Workshop on Semantic Evaluation (SemEval-\n2017), Vancouver, Canada, Aug. 2017, pp. 621–625,\nAssociation for Computational Linguistics.\n[70] James A Russell, “A circumplex model of affect.,”Jour-\nnal of personality and social psychology, vol. 39, no. 6,\npp. 1161, 1980.\n[71] Niehues-J. Waibel A. Sperber, M., “Toward robust neu-\nral machine translation for noisy input sequences,” in\nIn International Workshop on Spoken Language Trans-\nlation (IWSLT)., 2017.\n[72] Stefan Constantin, Jan Niehues, and Alex Waibel, “In-\ncremental processing of noisy user utterances in the spo-\nken language understanding task,” inProceedings of the\nFifth Workshop on Noisy User-generated Text (W-NUT),\n2019.\n[73] Gaetan Ramet, Philip N. Garner, Michael Baeriswyl,\nand Alexandros Lazaridis, “Context-aware attention\nmechanism for speech emotion recognition,” in IEEE\nWorkshop on Spoken Language Technology, Dec. 2018,\npp. 126–131.\n[74] Florian Eyben, Felix Weninger, Florian Gross, and\nBj¨orn Schuller, “Recent developments in opensmile, the\nmunich open-source multimedia feature extractor,” in\nProceedings of the 21st ACM International Conference\non Multimedia, New York, NY , USA, 2013, MM ’13,\npp. 835–838, ACM.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.810753583908081
    },
    {
      "name": "Speech recognition",
      "score": 0.6550606489181519
    },
    {
      "name": "Emotion recognition",
      "score": 0.6159154772758484
    },
    {
      "name": "Task (project management)",
      "score": 0.5589917302131653
    },
    {
      "name": "Natural language processing",
      "score": 0.5418662428855896
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5316284894943237
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5301122665405273
    },
    {
      "name": "Language model",
      "score": 0.4120691418647766
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}