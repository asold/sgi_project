{
  "title": "Foundation Models for Time Series Analysis: A Tutorial and Survey",
  "url": "https://openalex.org/W4393177791",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2129628882",
      "name": "Yuxuan Liang",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102909786",
      "name": "Haomin Wen",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2166739125",
      "name": "Yuqi Nie",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2103272554",
      "name": "Yushan Jiang",
      "affiliations": [
        "University of Connecticut"
      ]
    },
    {
      "id": "https://openalex.org/A1927467919",
      "name": "Ming Jin",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2114426781",
      "name": "Dongjin Song",
      "affiliations": [
        "University of Connecticut"
      ]
    },
    {
      "id": "https://openalex.org/A2278106503",
      "name": "Shirui Pan",
      "affiliations": [
        "Griffith University"
      ]
    },
    {
      "id": "https://openalex.org/A2141035074",
      "name": "Qingsong Wen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4391575085",
    "https://openalex.org/W3207999419",
    "https://openalex.org/W2979173158",
    "https://openalex.org/W3013563398",
    "https://openalex.org/W2971724044",
    "https://openalex.org/W4390874575",
    "https://openalex.org/W4290877193",
    "https://openalex.org/W4315588609",
    "https://openalex.org/W4390285500",
    "https://openalex.org/W4396758709",
    "https://openalex.org/W4309651348",
    "https://openalex.org/W4390872297",
    "https://openalex.org/W4323644256",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4283315029",
    "https://openalex.org/W4385567802",
    "https://openalex.org/W4390100390",
    "https://openalex.org/W4290876049",
    "https://openalex.org/W4385763767",
    "https://openalex.org/W4309651822",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W4385562572",
    "https://openalex.org/W4391987728",
    "https://openalex.org/W3199148273",
    "https://openalex.org/W3210060313",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W4385568228",
    "https://openalex.org/W4387573791"
  ],
  "abstract": "Time series analysis stands as a focal point within the data mining\\ncommunity, serving as a cornerstone for extracting valuable insights crucial to\\na myriad of real-world applications. Recent advances in Foundation Models (FMs)\\nhave fundamentally reshaped the paradigm of model design for time series\\nanalysis, boosting various downstream tasks in practice. These innovative\\napproaches often leverage pre-trained or fine-tuned FMs to harness generalized\\nknowledge tailored for time series analysis. This survey aims to furnish a\\ncomprehensive and up-to-date overview of FMs for time series analysis. While\\nprior surveys have predominantly focused on either application or pipeline\\naspects of FMs in time series analysis, they have often lacked an in-depth\\nunderstanding of the underlying mechanisms that elucidate why and how FMs\\nbenefit time series analysis. To address this gap, our survey adopts a\\nmethodology-centric classification, delineating various pivotal elements of\\ntime-series FMs, including model architectures, pre-training techniques,\\nadaptation methods, and data modalities. Overall, this survey serves to\\nconsolidate the latest advancements in FMs pertinent to time series analysis,\\naccentuating their theoretical underpinnings, recent strides in development,\\nand avenues for future exploration.\\n",
  "full_text": "Foundation Models for Time Series Analysis:\nA Tutorial and Survey\nYuxuan Liang\nThe Hong Kong University of Science\nand Technology (Guangzhou)\nyuxliang@outlook.com\nHaomin Wen\nBeijing Jiao Tong University &\nHKUST(GZ)\nwenhaomin@bjtu.edu.cn\nYuqi Nie\nPrinceton University\nynie@princeton.edu\nYushan Jiang\nUniversity of Connecticut\nyushan.jiang@uconn.edu\nMing Jin\nMonash University\nming.jin@monash.edu\nDongjin Song\nUniversity of Connecticut\ndongjin.song@uconn.edu\nShirui Pan\nGriffith University\ns.pan@griffith.edu.au\nQingsong Wen\nSquirrel AI, USA\nqingsongedu@gmail.com\nABSTRACT\nTime series analysis stands as a focal point within the data min-\ning community, serving as a cornerstone for extracting valuable\ninsights crucial to a myriad of real-world applications. Recent ad-\nvances in Foundation Models (FMs) have fundamentally reshaped\nthe paradigm of model design for time series analysis, boosting\nvarious downstream tasks in practice. These innovative approaches\noften leverage pre-trained or fine-tuned FMs to harness generalized\nknowledge tailored for time series analysis. This survey aims to\nfurnish a comprehensive and up-to-date overview of FMs for time\nseries analysis. While prior surveys have predominantly focused on\neither application or pipeline aspects of FMs in time series analysis,\nthey have often lacked an in-depth understanding of the underlying\nmechanisms that elucidate why and how FMs benefit time series\nanalysis. To address this gap, our survey adopts a methodology-\ncentric classification, delineating various pivotal elements of time-\nseries FMs, including model architectures, pre-training techniques,\nadaptation methods, and data modalities. Overall, this survey serves\nto consolidate the latest advancements in FMs pertinent to time\nseries analysis, accentuating their theoretical underpinnings, recent\nstrides in development, and avenues for future exploration.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíSpatial-temporal systems.\nKEYWORDS\nTime series, foundation model, deep learning\nQ. Wen is the corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0490-1/24/08. . . $15.00\nhttps://doi.org/10.1145/3637528.3671451\nFigure 1: Roadmaps of representative TSFMs.\nACM Reference Format:\nYuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin\nSong, Shirui Pan, and Qingsong Wen. 2024. Foundation Models for Time\nSeries Analysis: A Tutorial and Survey. In Proceedings of the 30th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining (KDD ‚Äô24),\nAugust 25‚Äì29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.\nhttps://doi.org/10.1145/3637528.3671451\n1 INTRODUCTION\nTime series data are characterized by their sequential order and\ntemporal dependencies, encapsulating valuable information about\nthe dynamics of diverse systems and processes [45, 86, 113]. Various\ntime series data (e.g., stock price, traffic flow, electricity) present\nunique challenges and opportunities for computational analysis,\neach requiring tailored approaches to effectively capture their in-\nherent properties. The analysis and understanding of time series\ndata is an important piece of data mining, facilitating crucial in-\nsights and decisions in many domains [46, 94], including finance\n[19, 108], healthcare [52, 62], cloud computing [116], environments\n[14, 70], energy [123], and urban computing [79, 88].\nIn recent years, the advancements of deep learning, especially\nthe transformer-based models [85], have revolutionized the field of\ntime series analysis [95]. Following the pioneering work [54], there\nhas been a surge in research interest exploring the application of\narXiv:2403.14735v3  [cs.LG]  18 Jun 2024\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuxuan Liang et al.\nTable 1: Comparison between our survey and related surveys.\n(Abbr: Taxonomy means the main taxonomy used in the\nsurvey. Standard means standard time series, Spatial means\nspatial time series, Others include trajectory and event).\nSurvey Taxonomy Standard Spatial Others\nJin et al. [48] Data ‚úî ‚úî ‚úò\nJiang et al. [45] Pipeline ‚úî ‚úò ‚úî\nZhang et al. [114] Pipeline ‚úî ‚úò ‚úò\nMiller et al. [66] Pipeline ‚úî ‚úò ‚úò\n(Ours) Methodology ‚úî ‚úî ‚úî\ntransformers for time series analysis [101, 118, 119]. The motiva-\ntion behind deep learning and transformers lies in their ability to\nautomatically learn comprehensive representations from raw data,\nthus capturing complex nonlinear relationships and temporal de-\npendencies without the need for manual feature engineering. Such\ncapability leads to significant performance improvements compared\nwith traditional statistical methods across numerous time series ap-\nplications. Foundation models (FMs), such as large language models\n(LLMs) in natural language processing (NLP) [117] and advanced\nmodels in computer vision (CV) [ 2], have emerged as powerful\nparadigms capable of achieving state-of-the-art performances in\ntheir respective fields. The success of these FMs can be attributed\nto their ability to leverage vast amounts of data to cultivate general-\npurpose representations , subsequently fine-tuning them, or even\ndeploying them directly in a zero-shot manner to excel across a\ndiverse spectrum of downstream tasks. This approach not only\neconomizes on the need for task-specific model development but\nalso encapsulates a broad understanding of the world, endowing\nthese models with exceptional versatility and efficiency [6, 49].\nInspired by the remarkable achievements of FMs in broad do-\nmains like CV and NLP, the concept of Time Series Foundation\nModels (TSFMs) has garnered attention as a promising direction\nfor time series analysis. TSFMs aim to harness the power of the\nfoundation model paradigm to develop generalized models profi-\ncient in understanding and forecasting time series data spanning\ndiverse domains. By capitalizing on large-scale time series datasets,\nTSFMs hold the promise of attaining superior performance on a\nspectrum of time series tasks, offering a unified framework that\ncan accelerate research and application developments in this field.\nDespite the promising prospects and rapid development of TSFMs,\na systematic analysis of TSFMs from a methodological standpoint\nhas been notably absent in prior literature. Existing studies, as de-\npicted in Table 1, have concentrated on either the data perspective\n[48] or the pipeline perspective [45] of TSFMs. To bridge this gap,\nthis survey aims to provide a comprehensive methodological anal-\nysis of foundation models for learning a variety of time series. This\nexamination will center on scrutinizing their model architectures ,\npre-training techniques , adaptation methods , and data modalities .\nThrough this endeavor, we seek to illuminate an overall picture of\ncore elements in TSFMs, thereby enhancing comprehension regard-\ning the rationale behind their efficacy and the mechanisms driving\ntheir substantial potential in time series analysis.\nIn contrast to previous surveys, this manuscript incorporates\nthe most extensive array of time series data types (see Table 1),\nspatial time series, as well as other types such as the trajectory\nand event. We further summarize the developmental roadmap of\ncurrent TSFMs in Figure 1, in order to foster further innovations\nand understanding in the dynamic and ever-evolving landscape of\nTSFMs. In short, our major contributions lie in three aspects:\n‚Ä¢Comprehensive and up-to-date survey. We offer a compre-\nhensive and up-to-date survey on foundation models for a wide\nspectrum of time series, encompassing standard time series, spa-\ntial time series, and other types (i.e., trajectories and events).\n‚Ä¢Novel methodology-centric taxonomy. We introduce a novel\ntaxonomy that offers a thorough analysis from a methodological\nstandpoint on TSFMs with the first shot, enabling a full under-\nstanding of the mechanism on why and how FMs can achieve\nadmirable performance in time series data.\n‚Ä¢Future research oppotunities. We discuss and highlight future\navenues for enhancing time series analysis using foundation\nmodels, urging researchers to delve deeper into this area.\n2 BACKGROUND\nFoundation Models. Foundation models (FMs), also known as\nlarge pre-trained models, are a class of deep models that are pre-\ntrained on vast amounts of data, thus equipped with a wide range of\ngeneral knowledge and patterns. To this end, these models serve as\na versatile starting point for various tasks across different domains.\nSpecifically, FMs can be fine-tuned or adapted to specific tasks with\nrelatively small amounts of task-specific data, showcasing remark-\nable flexibility and efficiency. In CV, FMs such as text-prompted\nmodel CLIP [73] and visual-prompted model SAM [51] have pro-\npelled advancements in image recognition, object detection, and\nmore. In NLP, FMs such as BERT [25] and GPT-3 [8] have revolu-\ntionized text understanding and generation tasks. Inspired by the\ngreat success of FMs in the above domains, this survey delves into\nthe utilization of these models in the realm of time series analysis.\nConcretely, we investigate TSFMs from a methodology perspec-\ntive: the components of foundation models encompass the data\nmodality, architecture, pre-training, and adaptation technicals: 1)\ndata modality refers to the type of data used for model training,\nfrom single modality such as time series, text, images, and audio\nto multimodality; 2) architecture refers to which deep neural net-\nwork is adopted as the backbone of FM, with Transformers [85, 95]\nbeing a popular choice for their ability to handle sequential data\neffectively; 3) Pre-training involves how to train the model on\nlarge, diverse datasets to gain a broad understanding of the data,\nusing supervised or self-supervised learning; 4) Adaptation, such as\nfine-tuning or few-shot learning, is employed to accommodate the\npre-trained FMs to specific tasks. This comprehensive framework\nof FMs, spanning from data modality to adaptation, facilitates the\nunderstanding of using them in time series analysis.\nCategories of Time Series. A time series is commonly described\nas an ordered sequence of data points. Figure 2 illustrates various\ntypes of time series discussed in this survey, including standard\ntime series, spatial time series, trajectories, and events. Note that\ntrajectories and events can be regarded as time series since each\ndata point is associated with a specific timestamp (and location),\nallowing for analysis using time series techniques such as anomaly\ndetection. These time series are formulated as follows.\nFoundation Models for Time Series Analysis: A Tutorial and Survey KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nTrajectory\nStandard Time SeriesTimeSpatial Time SeriesSpatial\nSpatial\nTime\nTimeEvent\nTime\n‚Ä¶‚Ä¶\nOther Time Series\nFigure 2: Illustration of various types of time series.\nDefinition 1 (Standard Time Series). The standard time series is\ndefined as a sequence of ùëá data points ordered by time. It can be\ndenoted by X = {x1,x2,¬∑¬∑¬∑ ,xùëá}‚àà Rùëá√óùê∑, where xùë° ‚ààRùê∑ is the\ndata point at time stepùë°, and ùê∑is the dimension of each data points.\nWhen ùê∑ = 1, X is referred to as a univariate time series, while\nùê∑ > 1, X is a multivariate time series.\nDefinition 2 (Spatial Time Series). It refers to a sequence of data\npoints with both temporal and spatial dimensions, which can be rep-\nresented by X= {X1,X2,¬∑¬∑¬∑ ,Xùëá}‚àà RùëÅ√óùëá√óùê∑, where Xùë° ‚ààRùëÅ√óùê∑\ndenotes the signals generated by ùëÅ sensors with each equipped\nwith ùê∑ features. Besides, the ùëÅ sensors are usually associated with\nspatial correlations, according to which the spatial time series can be\nfurther divided into two subtypes: i) spatio-temporal graph, when\nthe spatial correlation of those sensors is described by a graph ùê∫\nwith adjacent matrix A ‚ààRùëÅ√óùëÅ; ii) spatio-temporal raster, when\nsensors are distributed uniformly as a grid in geographical space.\nDefinition 3 (Trajectory). A trajectory is a sequence of times-\ntamped locations that describe the movements of an object in the\ngeographical space. It can be formulated as T= {(ùëô1,ùëô2,¬∑¬∑¬∑ ,ùëôùëá}‚àà\nRùëá√ó2, where ùëôùë° means the object‚Äôs location at time ùë°, represented\nby the two-dimensional coordinates, i.e., latitude and longitude.\nDefinition 4 (Event Sequence). An event sequence is a tempo-\nrally ordered set of events that describe the progression of actions\nor occurrences within a specific context. It can be formalized as\nE= {(ùëí1,ùë°1),(ùëí2,ùë°2),..., (ùëíùëõ,ùë°ùëõ)}, where ùëíùëñ is an event described\nby a predicate-argument structure that captures the nature of the\noccurrence, and ùë°ùëñ denotes the timestamp when ùëíùëñ occurs.\n3 TAXONOMY\nThe proposed taxonomy is illustrated in Figure 3, and the related\nworks can be found in Table 2. The proposed taxonomy unfolds\na structured and comprehensive classification to enhance the un-\nderstanding of foundation models on time series analysis. It is\norganized into four hierarchical levels, starting with the data cate-\ngory, followed by the model architecture, pre-training techniques,\nand finally, the application domain. Unlike previous taxonomies,\nours distinguishes itself by delving deeper into the foundation mod-\nels from the methodology perspective, with a keen focus on their\narchitectural designs, pre-training, and adaptation techniques. This\nmethod-centric view is pivotal for researchers, providing valuable\ninsights into the mechanisms of why and how foundation models\nshow great potential for time series analysis.\nDiving into the details of data categories, we classify the time\nseries data into three distinct types: standard time series, spatial\ntime series, and others, which encompass trajectory and event data.\nStandard time series data, characterized by their sequential order\nand temporal dependencies, form the backbone of traditional time\nseries analysis. Spatial time series data, on the other hand, introduce\nan additional layer of complexity by incorporating geographical\nor spatial information, making them crucial for applications in\nurban computing and environmental monitoring. Lastly, the ‚Äúothers‚Äù\ncategory, including trajectory and event data, represents diverse\ndatasets where time plays a critical role, such as the movement of\nobjects over time or the occurrence of specific events, offering a\nbroadened perspective on time series analysis.\nFrom the methodology perspective: i) regarding model architec-\nture, the proposed taxonomy highlights three primary categories:\nTransformer-based, non-Transformer-based, and diffusion-based\nmodels. Transformer-based models leverage self-attention mecha-\nnisms to capture long-range dependencies within time series, of-\nfering significant advantages in handling sequential data. Non-\ntransformer-based models, with their diverse architectures, cater\nto a wide range of time series tasks by efficiently processing tem-\nporal patterns. Diffusion-based models, a novel addition, employ\nstochastic processes to model the data generation process, present-\ning innovative solutions for time series analysis. ii) In terms of\npre-training techniques, the proposed taxonomy divides them into\nfully-supervised and self-supervised methods, the latter of which\nincludes contrastive, generative, and hybrid approaches. This clas-\nsification shows how different FMs are trained with or without\nlabels. iii) Adaptation strategies, such as zero-shot learning, prompt\nengineering, tokenization, and fine-tuning, further exemplify the\nversatility of FMs in customizing to specific time series applications.\n4 DATA PERSPECTIVE\nIn this section, we explore advancements in TSFMs from various\ndata perspectives: standard time series , spatial time series , and others.\nWe further categorize our discussion within each subsection into\ntask-oriented or general-purpose foundation models.\n4.1 Standard Time Series\nStandard time series possess diverse properties, including varying\nsampling rates and temporal patterns, which pose significant chal-\nlenges in developing relevant FMs. These models aim to identify\nuniversal patterns within extensive time series data from varied\nsources, either to enhance specific tasks or for broad analysis.\nMost of the existing attempts are in the category of task-oriented\nstandard time series foundation models. They leverage single or\nmultiple data modalities to craft robust models targeting particular\ntime series tasks, typically forecasting or classification. For models\ninvolved only in a single (i.e., time series) modality, they may either\nbe developed from scratch or on existing pre-trained models from\nother domains like large language or vision models [117].\nIn the first group, Lag-Llama [75] and TimeGPT-1 [35] represent\npioneering efforts as forecasting foundation models. Both models\nundergo pre-training on a vast collection of time series data span-\nning multiple domains. Lag-Llama employs a decoder-only trans-\nformer architecture, utilizing lags as covariates, whereas TimeGPT-\n1 features an encoder-decoder structure with several transformer\nlayers, facilitating efficient zero-shot forecasting. Another note-\nworthy contribution is TTMs [31], a recent endeavor in creating a\ndomain-agnostic forecasting model built upon TSMixer [30], which\nitself is pre-trained on diverse time series datasets from various\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuxuan Liang et al.\nFoundation Modelsfor Time Series\nStandard Time Series\nTransformer-based\nPre-trainedLLM, AM, VLM\nGeneral: Time-LLM [47], OFA [120], LLM4TS [11], PromptCast [102],TEMPO [10], LLMTime [36], Voice2Series [105], AutoTimes [63], UniTime [60]\nFinance: Yuet al.[108], Chenet al.[19], Xieet al.[100], Wimmeret al.[96]\nHealthcare: Liuet al.[62]\nSelf-supervised\nGenerative General:PatchTST [68], Moirai [97], Lag-Llama [75],TimeSiam [26], Timer [64], Daset al.[24], UniTS [34],TimeGPT-1 [35], Chronos [1], MTSMAE [84]\nContrastive General:TEST [83], TimeCLR [107]\nHealthcare:METS [52]\nHybrid General:SimMTM [27]\nFully-supervisedGeneral: TimeXer [90], UniTS [34]\nnon-Transformer-based(MLP RNN CNN)\nSelf-supervisedGenerative General:TSMixer [30]\nContrastive General:TF-C [115] , TS2Vec [111] , CLUDA [69]\nFully-supervisedGeneral: TTMs [31], TimesNet [98], RWKV-TS [40]\nDiffusion-based\nGeneral:TimeGrad [76] , D3VAE [55] , TransFusion [82] , ScoreGrad [104] ,Bilo≈°et al.[5] , Crabb√©et al.[23] , TimeDiff [80] , Wanget al.[89] , DiffTime [22]\nFinance:FTS-Diffusion [42]\nPower:DiffLoad [92]\nSpatial Time Series\nTransformer-based\nPre-trained LLMTransportation:ST-LLM [58], TPLLM [77]\nGeneral:GATGPT [18]\nSelf-supervisedGenerative\nTransportation:STEP [79]\nClimate:W-MAE [65], MetePFL [14], FengWu [13]\nGeneral:UniST [109]\nFully-supervisedTransportation:CPPBTR [29] , TFM [88]\nClimate:FourCastNet [70] , FedWing [15] , Pangu-Weather [3], ClimaX [67]\nnon-Transformer-based(MLP RNN CNN) Self-supervisedGeneral:SPGCL [53] , STGCL [61]\nDiffusion-based General:DiffSTG [93] , DSTPP [110] , DYffusion [9] , Yunet al.[112] , USTD [41] , PriSTI [59]\nOthers\nTransformer-based\nPre-trained LLMMobility:AuxMobLCast [103] , LLM-Mob [87]\nEvent:LAMP [81] , Gunjal & Durrettet al.[38]\nSelf-supervised\nGenerative Mobility:GTM [56]\nEvent:NYUTron [44] , GatorTron [106]\nContrastive Mobility:TrajCL [12]\nnon-Transformer-based(MLP RNN CNN) Self-supervised\nGenerative Mobility:Trembr [33]\nContrastive Mobility:MMTEC [57]\nHybrid Mobility:START [43]\nDiffusion-based Mobility:TrajGDM [21] , DiffTraj [122]\nFigure 3: A comprehensive taxonomy of TSFMs, categorized according to data and methodologies.\ndomains. Echoing Lag-Llama‚Äôs approach, TimesFM [24] emerges\nas a decoder-only model exhibiting strong zero-shot forecasting\ncapabilities. Concurrently, Moirai [97] introduces an approach with\nits masked encoder-based universal forecasting transformer, cou-\npled with a new pre-training dataset (LOTSA), containing 27 bil-\nlion observations from nine distinct domains. Additionally, the\nexploration extends to diffusion models like TimeGrad [ 76] and\nTransFusion [82], which primarily focus on optimizing a variational\nbound on data likelihood, transforming white noise into meaningful\nsamples of the target distribution.\nPre-training from scratch can be expensive, which has spurred\nthe development of alternative approaches that leverage pre-trained\nmodels from other domains, such as large language, vision, and\nacoustic models. For instance, LLM4TS [11] and TEMPO [10] suc-\ncessfully perform time series forecasting across various datasets by\nfine-tuning GPT-2 [74] backbones, predicated on the notion that\nLLMs can be adapted to process non-linguistic datasets by activat-\ning their inherent capabilities. Similarly, Voice2Series [105] engages\nin the synchronization of time series and acoustic data to harness\nthe classification prowess of an acoustic model for time series data.\nFoundation Models for Time Series Analysis: A Tutorial and Survey KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nAnother approach is presented by Wimmer et al. [96], who utilize\nvision-language models (VLMs) to predict market changes. Beyond\nfine-tuning existing models, a distinct methodology involves di-\nrect inference from LLMs for time series forecasting, showcasing\ncommendable zero-shot performance. A notable example of this\nis LLMTime [ 36], which introduces various strategies for effec-\ntively tokenizing time series data and transforming discrete token\ndistributions into flexible continuous value densities.\nBeyond approaches that focus solely on a single data modal-\nity of time series, there have been initiatives towards developing\nmulti-modal, task-oriented foundation models. A notable example\nis Time-LLM [47], which introduces a reprogramming framework\nto integrate textual and time series information, repurposing an\nexisting LLM into time series forecasters without additional compu-\ntational costs. In a similar vein, METS [52] employs a trainable ECG\nencoder alongside a frozen language model to process paired ECG\nand clinical reports. Further, there is emerging research on directly\nprompting LLMs for specific time series tasks. For instance, Prompt-\nCast [102] converts numerical inputs and outputs into prompts,\nframing the forecasting task as a sentence-to-sentence conversion\nto leverage language models directly for forecasting. Other studies,\nsuch as one involving LLMs prompted with historical stock price\ndata, company metadata, and past economic/financial news, aim to\nenhance stock return forecasting [108]. Another example combines\nGNNs with ChatGPT to predict stock movements [ 19], illustrat-\ning the diverse applications of these methodologies. Additional\nnoteworthy efforts include [100] and [62].\nNotably, recent efforts have been moved towards creating general-\npurpose, single-modality standard TSFMs. TS2Vec [111] is a pio-\nneering effort by introducing a universal framework for represent-\ning time series via contrastive learning. SimMTM [ 27] explores\ncross-domain applications, where pre-trained models via masked\ntime series modeling exhibit superior fine-tuning performance in\nforecasting and classification tasks. More recent works, such as\nTimer [64] and UniTS [34], further advance the field by facilitat-\ning general time series analysis through single, large-scale pre-\ntrained models. Moreover, there is a growing interest in adapting\npre-trained models, such as LLMs, for broad time series analysis.\nOFA [120] and TEST [ 83] exemplify this trend, though both ap-\nproaches necessitate end-to-end fine-tuning for specific tasks.\n4.2 Spatial Time Series\nIn complex real-world systems, time series data often display intri-\ncate spatial dependencies alongside temporal dynamics, manifest-\ning in forms such as spatio-temporal graphs and rasters. Similar to\nthe discussion in Sec. 4.1, research on spatial time series typically\nencompasses areas such as forecasting and classification. Unlike\nfoundation models for standard time series, most existing research\non spatial time series is still in its early stages, often characterized\nby being domain-specific, single-modality, and task-oriented. In\nthe following, we categorize related works into two specific data\nmodalities and discuss them in different subsections.\n4.2.1 Spatio-Temporal Graph. Most foundation models for spatio-\ntemporal graphs are task-oriented and only focused on graph data.\nIn the transportation sector, TFM [ 88] utilizes graph structures\nand algorithms to analyze the behavior and interactions within\ntransportation systems, showing promising results in urban traffic\nforecasting. ST-LLM [58] combines spatio-temporal information\nwith a partially frozen LLM to improve traffic predictions, while\nDiffSTG [93] applies denoising diffusion models to spatio-temporal\ngraphs for probabilistic traffic forecasting. Efforts towards domain-\nagnostic models include STEP [ 79], which links spatio-temporal\nGNNs with a pre-trained transformer for enhanced forecasting by\nlearning from extensive historical data. Similarly, STGCL [61] and\nSPGCL [53] explore the integration of contrastive learning into\nspatio-temporal graph forecasting, indicating its potential benefits.\nResearch on general-purpose models for spatio-temporal graphs is\nlimited. A notable example, USTD [41], introduces a unified model\nfor both forecasting and kriging tasks, employing an uncertainty-\naware diffusion approach to address diverse challenges effectively.\n4.2.2 Spatio-Temporal Raster. Spatio-temporal raster refers to a\ndata modality that captures and organizes spatial information over\nvarious time points in a grid-like format. This modality is primar-\nily utilized in climate foundation models. For instance, FourCast-\nNet [70] is a global, data-driven weather forecasting model deliver-\ning accurate short to medium-range predictions worldwide. Similar\nmodels, such as FengWu [ 13] and W-MAE [65], follow suit. No-\ntably, Pangu-Weather [4], which is trained on 39 years of global data,\nachieves superior deterministic forecasting outcomes across all eval-\nuated variables compared to leading numerical weather prediction\nsystems. On a different note, ClimaX [67] aims at general-purpose\nclimate foundation models, pre-trained with diverse datasets cover-\ning various variables, spatio-temporal scopes, and physical contexts.\nIt is designed for fine-tuning across a wide array of climate and\nweather-related tasks, such as forecasting, projection, and down-\nscaling, even for atmospheric variables and spatio-temporal scales\nnot encountered during its pre-training phase. However, there is\na scarce number of domain-agnostic models for spatio-temporal\nraster data. DYffusion [9], for example, capitalizes on the tempo-\nral dynamics inherent in raster data, integrating these dynamics\ndirectly with the model‚Äôs diffusion steps to create a stochastic, time-\nconditioned interpolator and forecasting network.\n4.3 Others\nIn addition to standard and spatial time series, various other types\nof data incorporate the temporal dimension, including trajecto-\nries, events, and clinical records. A majority of studies in this cat-\negory focus on trajectory data. For Transformer-based models,\nAuxMobLCast [103] fine-tunes pre-trained LLMs through mobility\nprompting and auxiliary POI Category classification to forecast hu-\nman mobility patterns, effectively bridging the gap between natural\nlanguage processing and temporal sequence prediction. LLM-Mob\n[87] encodes human mobility data into structured prompts that in-\nstruct LLMs to consider both long-term and short-term behavioral\npatterns, along with time-specific context, to generate accurate and\nexplainable predictions of future locations. For non-Transformer-\nbased models, Trembr [33] leverages auto-encoding techniques to\nextract road network and temporal information embedded in trajec-\ntories effectively. While START [43] introduces a hybrid approach\nto trajectory embedding learning by combining masked language\nmodel [25] and SimCLR [ 17] to enhance its learning capability.\nMore recently, GTM [56] separates trajectory features into three\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuxuan Liang et al.\n(a) Transformer-based(b) Non-Transformer-based(c) Diffusion-based\nFigure 4: Architectures of TSFMs.\ndomains, which can be masked and generated independently to\nmeet specific input and output requirements of a given task. Then,\nGTM is pre-trained by reconstructing densely sampled trajectories\nin an auto-regressive manner given re-sampled sparse counter-\nparts. For the diffusion-based model, DiffTraj [ 122] reconstructs\nand synthesizes geographic trajectories from white noise through\na conditioned reverse trajectory denoising process.\n5 METHODOLOGY PERSPECTIVE\nIn this section, we dissect TSFMs from a methodology perspective,\nfocusing on architecture and pipeline (including pre-train and adap-\ntation) intricacies. This discussion aims to elucidate the intricate\nmechanisms driving these models‚Äô efficacy and adaptability.\n5.1 Architecture\nAs shown in Figure 4, we first delve into the architecture of TSFMs,\nincluding Transformer-based models , non-Transformer-based nodels\nand diffusion-based models, focusing on the underlying mechanisms\nthat shape their capabilities, as well as how they could be applied\non various time series.\n5.1.1 Transformer-based Models. The architecture of FMs has seen\na significant convergence towards the Transformer [85], a model\narchitecture first introduced for NLP tasks. The core innovation of\nthe Transformer lies in its utilization of the attention mechanism,\nwhich allows the model to dynamically focus on different parts of\nthe input data. The attention function can be succinctly described\nas Attention(ùëÑ,ùêæ,ùëâ )= Softmax(ùëÑùêæùëá/\n‚àöÔ∏Å\nùëëùëò)ùëâ, where ùëÑ, ùêæ, and\nùëâ represent the queries, keys, and values matrices respectively,\neach with dimensions ùëá √óùëëùëò, and ùëëùëò serves as a scaling factor\nto moderate the dot products‚Äô magnitude. It is evident from the\nformula that the attention mechanism has the capability to learn\nglobal, long-range dependencies in data. This distinguishes it from\nprevious architectures, which were often limited by their local\nreceptive fields or dependency windows. Besides, the Transformer‚Äôs\ndesign is inherently friendly to parallelization, which allows for\nsignificant scalability, enabling the processing of large datasets\nand the construction of models with billions of parameters. Such\nscalability and efficiency in capturing intricate data patterns have\nled to the widespread adoption of the Transformer architecture\nbeyond its initial application in natural language processing (NLP)\n[25] to fields including computer vision (CV), speech, video, time\nseries (Table 2) and beyond.\nThe choice of foundation model framework remains debated\nin the realm of time series analysis, contrasting the trend towards\ndecoder-only models in natural language processing. Notable works\nin this area includes encoder-only [ 34, 68, 97], encoder-decoder\n[1, 26, 35], and decoder-only [24, 64, 75] models. Ansari et al. [1]\nanalyze the applicability of the encoder-decoder framework to\ndecoder-only models. Liu et al. [64] discuss that while the encoder-\nonly model is favored in time series forecasting for its effectiveness\non small datasets, the decoder-only architecture, with its strong\ngeneralization and capacity, could be preferred for large-scale time\nseries models. The diversity in the architectural choices underscores\nthe potential and necessity for further exploration within this field.\nIn terms of standard time series analysis, the Transformer archi-\ntecture leverages its sequence modeling capabilities to capture tem-\nporal dynamics. This includes either repurposing pretrained LLMs\nfor time series to leverage their preexisting sequence modeling\nstrengths [102], or directly using Transformer as a base for TSFMs,\ntraining from scratch to achieve models best suited for the specifics\nof time series data [35]. Besides, various techniques have been inno-\nvated to augment the functionality of Transformer models in time\nseries analysis comprehensively. A common practice in TSFMs seg-\nments time series into patches, which can effectively encapsulate\nlocal dynamics within input tokens [10, 11, 20, 24, 47, 68, 75, 97, 120].\nAnother critical design is the normalization layer, where reversible\ninstance normalization [50] techniques, standardizing data through\ninstance-specific mean and variance then reverting it at the output\nlayer, have found extensive application across the above models.\nMoreover, specialized approaches such as multi-resolution analy-\nsis, exemplified by Moirai [97] through the employment of vary-\ning patch sizes, and decomposition strategies, as implemented by\nTEMPO [10] via the separation of complex interactions into the\ntrend, seasonal, and residual components, have been shown to\nenhance model efficacy substantially.\nFor spatial time series, the attention mechanism is utilized to\nmodel both the spatial and temporal dependency. For instance, ST-\nLLM [58] employs a novel partially frozen attention strategy for\ntraffic prediction, leveraging spatial-temporal embeddings to cap-\nture the intricate dynamics of traffic data across space and time.\nConversely, other studies opt for independent modeling of spa-\ntial and temporal relationships. TFM [88] is a case in point, which\nemploys attention mechanisms within a dynamic graph encoder\nfor spatial modeling, integrating time encoding for temporal as-\npects, embodying principles of transformers in addressing traffic\nsystem‚Äôs spatial-temporal dependencies. Besides simultaneously\nmodeling spatial and temporal relationships, there exists an alterna-\ntive approach that augments the Transformer model with additional\nspatial models or external spatial information to enhance its capa-\nbilities in the temporal modeling of time series. An example of this\nis STEP [79], which uses unsupervised pre-trained TransFormer\nblocks to model temporal relationship from long-term history time\nseries, while applying a graph structure learner and spatio-temporal\nGNNs based on the representation of TransFormer blocks. Further-\nmore, the application of Transformer models extends to the domain\nof spatial-temporal prompt learning, as evidenced by initiatives\nsuch as MetePFL [14] and FedWing [15].\nIn addition to conventional time series data, the Transformer\narchitecture has demonstrated efficacy across a diverse array of\ntemporal datasets, such as trajectory and healthcare records, as sum-\nmarized in Figure 3. This expansion highlights the Transformer‚Äôs\nversatile capacity for temporal data analysis.\nFoundation Models for Time Series Analysis: A Tutorial and Survey KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\n5.1.2 Non-Transformer-based Models. Excluding the widespread\nadoption of Transformers, a diverse array of traditional pre-training\nmethods leveraged models such as Multi-Layer Perceptrons (MLPs)\n[31], Recurrent Neural Networks (RNNs) [33], and Convolutional\nNeural Networks (CNNs) [ 98] as the backbone for pre-training.\nThese models, each with their unique strengths, are notable for\ntheir effectiveness in both conventional and spatial time series data.\nMLPs and CNNs are both acclaimed for their capabilities in\nmodeling spatial and temporal data effectively. CNN-based archi-\ntectures, in particular, have garnered significant attention in self-\nsupervised learning for general time series representation, with\na notable emphasis on the usage of ResNet [ 27, 115] and dilated\nconvolution layers [69, 111] as foundational backbones. Those ap-\nproaches predominantly employ 1D convolutional operations. In\ncontrast, TimesNet [98] introduces a novel perspective by convert-\ning 1D time series data into 2D tensors, facilitating the adaptive\nidentification of multi-periodicity and the extraction of complex\ntemporal variations through the use of a parameter-efficient incep-\ntion block. MLP-based models, on the other hand, are lauded for\ntheir lightweight design, offering benefits in terms of reduced com-\nputational time and cost. TSMixer [30] and TTMs [31], as instances,\nboth claiming superior efficiency in memory usage and processing\nspeed while still delivering competitive performance.\nRNNs have been acknowledged for their proficiency in temporal\ndata modeling [33, 39]. Recently, there has been a resurgence of\ninterest in RNN architectures, which poses a compelling challenge\nto the prevailing Transformer-based models. This trend is driven\nby the quest for models that are not only more resource-efficient\nbut also adept at handling longer sequences through their inherent\nlinear complexity. A notable embodiment is the RWKV-TS [ 40],\nwhich leverages the RWKV [72], an RNN-type foundation model\narchitecture, demonstrating promising potential for general time\nseries analysis. This emerging trend presents a valuable opportunity\nfor time series research and applications.\n5.1.3 Diffusion-based Models. Diffusion-based foundation mod-\nels have gained prominence in CV [ 71, 78] and video [ 7] due to\ntheir proficiency in learning complex data distributions, yet their\nexploration in time series analysis remains nascent. These models\nfunction by gradually introducing and then reversing noise to data,\neffectively learning the generative process of original data through\nthe reverse diffusion process. This unique mechanism equips diffu-\nsion models with great potential to serve as versatile foundation\nmodels capable of tackling prediction, imputation, and anomaly\ndetection in time series.\nIn standard time series and other temporal data, diffusion models\npredict future states by capturing temporal dynamics, generating\nsmooth transitions from current to potential future states [76, 92].\nApplied to spatial time series, they extend this capability to model\nspatial correlations alongside temporal ones, providing insights\ninto the interplay between space and time, particularly beneficial\nin fields like traffic forecasting [93].\n5.2 Pipeline\nIn this part, we review TSFMs from the pipeline perspective, in-\ncluding diverse model acquisition and adaptation mechanisms.\n5.2.1 Pre-training. Pre-training is an initial and crucial step for\nbuilding TSFMs, since the knowledge learned in this phase enables\nthe models to generalize across different contexts and quickly adapt\nto various downstream tasks with minimal adaptations. On the\nother hand, the diverse nature of pre-training data (e.g., standard\ntime series, spatial time series, and trajectories), as well as the way\nthe data is used, lead to a wide spectrum of pre-training mechanisms\nwhen building and deploying foundation model. In this survey, we\npropose a new perspective mostly based on learning objectives in\nthe pre-training phase, to categorize existing methods for TSFMs.\nThese mechanisms include fully-supervised, self-supervised (gener-\native, contrastive, hybrid of generative and contrastive), and others.\nFully-supervised pre-training refers to the strategy where the\nfoundation model is initially trained on one or multiple large time se-\nries datasets with labels to capture the complex temporal dynamics\nand learn generalizable representations. TTMs [31] proposes a uni-\nversal time series foundation model supervised framework that is\nable to handle the heterogeneity of multiple time series datasets and\neffectively build the forecasting capability during pre-training, via\nthe design of multi-resolution enhancements (e.g., adaptive patch-\ning, data augmentation via downsampling, etc.) Fully-supervised\npre-training for TSFMs is particularly suited for scenarios where\nthere is sufficient labeled historical data. Moreover, this pre-training\ntechnique is more frequently used in some domain-specific applica-\ntions such as transportation [29, 88] and climate [3, 70], where the\nmodel can be directly tailored for downstream forecasting tasks\nwith the ease of minimal adaptations.\nWe categorize the generative pre-training strategy as a general\nmodeling of time series representations, including reconstruction\nand probabilistic modeling of time series inputs. In reconstruction-\nbased pre-training, an effective learning objective is to recover the\noriginal input space via masked autoencoding strategies [14, 79]. In\nthe probabilistic modeling methods, the latent representation space\nformed from temporal or spatial-temporal encoders is optimized to-\nward an estimated density via maximizing log-likelihood, based on\nwhich the forecasts can be sampled [76, 93]. Moreover, it is also ben-\neficial to leverage contrastive learning to enhance the robustness of\npre-training time series foundation models. The key is to construct\nand utilize the self-supervision signals by generating informative\npositive pairs as well as filtering out unsuitable negative pairs when\nperforming augmentation [61]. In addition to the aforementioned\ntwo self-supervised strategies, the efficacy of the hybrid variant\nhas also been validated, where the pre-trained model on fewer time\nseries data outperforms the supervised counterpart [43].\nIn general, self-supervised pre-training enables foundation mod-\nels to exploit the vast amounts of unlabeled time series data, pro-\nviding generic temporal knowledge that can be further fine-tuned\nfor specific downstream tasks. Compared with fully-supervised\npre-training, it provides a more generic and realistic solution for\nthe acquisition of a time series foundation model.\nNote that the above pre-training methods typically build the\nmodel from scratch and obtain the universal knowledge from data\nwith the same modality (i.e., time series). Nevertheless, recent ad-\nvancements in time series research have heightened the usage of\nLLMs [10, 11, 19, 36, 38, 47, 58, 62, 63, 81, 87, 100, 102, 103, 108, 120],\nVLMs [96], and AMs [ 105] that are pre-trained from other data\nmodalities (text sequence, image-text sequence, acoustic signals).\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuxuan Liang et al.\n5.2.2 Adaptation. The adaptation phase tailors TSFM to specific\ntasks or datasets, enhancing its performance on those tasks by lever-\naging the learned generic temporal knowledge. We partition prior\nmethods into four branches, including direct usage, fine-tuning,\nprompt engineering, and time series tokenization (see Figure 5).\n(a)Directusage(b)Tuning-based(c)Prompting-based(d)Tokenization-based\nTSFM\ninference\nForecastImputeDetect\nTSFM\ninference\nForecastImputeDetect\nFine-tune\n LLM\nForecastImputeDetect\nPromptEngineering\nInstruction\nTSFM/LLM\nForecastImputeDetect\nPrompting\nInstruction\nToken.\nFigure 5: Illustration of different adaptation techniques.\nDirect usage (also called zero-shot), means no further fine-tuning\non the target datasets, suggesting the sufficient capability of a\npre-trained model for downstream tasks. It can also indicate the\nhomogeneity between the pre-trained dataset and target dataset,\nespecially for some real-world applications where a foundation\nmodel is built to fulfill domain-specific tasks [4, 13].\nFine-tuning is a common strategy to adapt foundation models to\ntarget tasks. Based on the way the foundation model is used on the\ntarget dataset, there are three mainstream works: fine-tuning the\nwhole model [65, 67, 70] or specific components (e.g., training posi-\ntional embeddings and layer normalization, while keeping feedfor-\nward and attention layers frozen when fine-tuning LLMs) [11, 120],\nto directly infer results, or integrate foundation models as part of\nthe whole model [19, 52, 83, 103].\nPrompt engineering is more specialized in LLM-based TSFMs.\nThe prompt can be handcrafted with task-specific textual input and\ndirectly used to query the output for downstream prediction [87,\n102] or intermediate embedding as feature enhancement [103]. Be-\nsides, the prompt can also be parameterized vectors and end-to-end\nlearnable when optimizing the model on target datasets [10, 83]. In\ncomparison to static prompts, the use of trainable prompts enhances\nthe ability of LLMs to comprehend and match the context of given\ntime series inputs. For example, TEMPO [10] constructs a trainable\nprompt pool with distinct key-value pairs, and retrieves the most\nrepresentative prompt candidates with the highest similarity scores.\nTime series tokenization aims to effectively represent the time\nseries as embeddings, which is also more frequently adopted in\ntransformer-based architectures [ 10, 47, 68]. Common tokeniza-\ntion techniques include reversible instance normalization [50] that\nmitigates distribution shift, patching with channel independence\nstrategy that effectively and efficiently extracts the time series con-\ntext [68], as well as the joint usage of time series decomposition\nto explicitly represent explainable components [10] for the ease of\nsubsequent temporal modeling.\nIn addition to the main branches of adapting TSFMs, it is also\nworth noting that some fine-tuning strategies take real-world con-\nstraints into account. For example, the fine-tuning is performed in\na privacy-preserved manner [14, 16].\n5.3 Modality\nDuring the pre-training/adaptation of TSFMs, prior methods in-\nvolve single or multiple data modalities, where standard time series,\ntrajectory, raster, and text can be treated as different forms with\nunique domain perspectives. In this part, we review the data modal-\nities that are used in existing TSFMs across different domains.\n5.3.1 Single-modality. A majority of current TSFMs are constructed\nand tailored on the basis of single-modal data. Compared with\nmulti-modal methods, the single-modal time series modeling strat-\negy gains the advantages of inherent simplicity and bypasses the\nchallenges of handling modality gaps, yet frequently demonstrates\nexcellent empirical results across a wide range of real-world appli-\ncations, such as traffic [58, 79] and climate forecasting [13, 67].\n5.3.2 Multi-modality. However, the single-modal methods may\nnot encapsulate the full picture for several challenging downstream\ntasks in finance [19, 108] and healthcare domains [52, 62]. To cope\nwith this issue, there have been initiatives towards developing multi-\nmodal, task-oriented FMs, where additional information provides\nuseful information to enhance the model capability. In Chen et al. ,\nan external ChatGPT is queried to construct an evolving graph\nstructure representing companies, based on the analysis of news\nheadlines at specific time steps. As such, the inferred graph and\nstock prices are fed into the time series model (that uses GNN and\nLSTM for information propagation) to generate stock price move-\nment predictions. Another example in healthcare also demonstrated\nthe effectiveness of multi-modal medical context modeling, which\naligns the embedding of ECG (Electrocardiogram) and correspond-\ning medical text reports under a self-supervised contrastive learning\nframework and performs ECG classification. In general multi-modal\ntime series analysis, similar cross-modality alignment strategies\n(e.g., contrastive learning [ 83], reprogramming [ 47], token-wise\nprompting [63]) are adopted, where the multi-modal inputs are of-\nten the textual description of datasets and pre-training word embed-\nding from LLMs. As a notable example, Time-LLM [47] introduces\na reprogramming framework that aligns the language knowledge\nfrom pre-trained word embedding and time series information via\nlinear projection and multi-head attention, where the handcrafted\ndataset descriptions are also used to quired text token embeddings\nas prompts, which further enhances the embedding space and in-\nforms the LLM to comprehend the task contexts. As such, utilizing\nmulti-modal data facilitates the repurpose of the existing LLM into\ntime series forecasters without additional computational costs.\n6 CONCLUSION\nThe rapid development of FMs has revolutionized the research fields\nin different domains. In this survey, we provide a comprehensive\nand updated review of FMs specifically designed for time series anal-\nysis. A novel taxonomy is proposed from a methodology-centric\nperspective by classifying FMs based on key components including\nmodel architecture, pre-training technique, adaptation technique,\nand data modality. Our survey facilitates understanding the under-\nlying mechanism of applying the FMs to time series. Furthermore,\nwe believe that the consolidation of the latest advancements, as\nwell as the potential future direction (see Appendix), can inspire\nmore innovative works within the field of time series analysis.\nFoundation Models for Time Series Analysis: A Tutorial and Survey KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nACKNOWLEDGEMENTS\nThis work is mainly supported by the Guangzhou-HKUST(GZ)\nJoint Funding Program (No. 2024A03J0620). It is also funded by\nGuangzhou Municipal Science and Technology Project 2023A03J0011.\nREFERENCES\n[1] Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro\nMercado, Huibin Shen, Oleksandr Shchur, Syama Syndar Rangapuram, Sebas-\ntian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix,\nMichael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-\nSchneider, and Yuyang Wang. 2024. Chronos: Learning the Language of Time\nSeries. arXiv preprint arXiv:2403.07815 (2024).\n[2] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer,\nHisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan.\n2023. Foundational Models Defining a New Era in Vision: A Survey and Outlook.\narXiv preprint arXiv:2307.13721 (2023).\n[3] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian.\n2023. Accurate medium-range global weather forecasting with 3D neural net-\nworks. Nature (2023), 1‚Äì6.\n[4] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian.\n2023. Accurate medium-range global weather forecasting with 3D neural net-\nworks. Nature 619, 7970 (2023), 533‚Äì538.\n[5] Marin Bilo≈°, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, and Stephan\nG√ºnnemann. 2022. Modeling temporal data as continuous functions with process\ndiffusion. arXiv preprint arXiv:2211.02590 (2022).\n[6] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,\nSydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258 (2021).\n[7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David\nSchnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and\nAditya Ramesh. 2024. Video generation models as world simulators. (2024).\nhttps://openai.com/research/video-generation-models-as-world-simulators\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877‚Äì1901.\n[9] Salva R√ºhling Cachay, Bo Zhao, Hailey James, and Rose Yu. 2023. DYffusion:\nA Dynamics-informed Diffusion Model for Spatiotemporal Forecasting. arXiv\npreprint arXiv:2306.01984 (2023).\n[10] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye,\nand Yan Liu. 2023. TEMPO: Prompt-based Generative Pre-trained Transformer\nfor Time Series Forecasting. arXiv preprint arXiv:2310.04948 (2023).\n[11] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. 2023. LLM4TS: Two-Stage\nFine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. arXiv preprint\narXiv:2308.08469 (2023).\n[12] Yanchuan Chang, Jianzhong Qi, Yuxuan Liang, and Egemen Tanin. 2023. Con-\ntrastive Trajectory Similarity Learning with Dual-Feature Attention. In 2023\nIEEE 39th International Conference on Data Engineering (ICDE) . IEEE, 2933‚Äì2945.\n[13] Kang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, Xi\nChen, Leiming Ma, Tianning Zhang, Rui Su, et al. 2023. FengWu: Pushing the\nSkillful Global Medium-range Weather Forecast beyond 10 Days Lead. arXiv\npreprint arXiv:2304.02948 (2023).\n[14] Shengchao Chen, Guodong Long, Tao Shen, and Jing Jiang. 2023. Prompt\nFederated Learning for Weather Forecasting: Toward Foundation Models on\nMeteorological Data. In International Joint Conference on Artificial Intelligence .\n[15] Shengchao Chen, Guodong Long, Tao Shen, Tianyi Zhou, and Jing Jiang. 2023.\nSpatial-temporal Prompt Learning for Federated Weather Forecasting. arXiv\npreprint arXiv:2305.14244 (2023).\n[16] Shengchao Chen, Guodong Long, Tao Shen, Tianyi Zhou, and Jing Jiang.\n2023. Spatial-temporal Prompt Learning for Federated Weather Forecasting.\narXiv:2305.14244 [cs.LG]\n[17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.\nA Simple Framework for Contrastive Learning of Visual Representations. In\nICML, Vol. 119. 1597‚Äì1607.\n[18] Yakun Chen, Xianzhi Wang, and Guandong Xu. 2023. Gatgpt: A pre-trained large\nlanguage model with graph attention network for spatiotemporal imputation.\narXiv preprint arXiv:2311.14332 (2023).\n[19] Zihan Chen, Lei Nico Zheng, Cheng Lu, Jialu Yuan, and Di Zhu. 2023. ChatGPT\nInformed Graph Neural Network for Stock Movement Prediction.arXiv preprint\narXiv:2306.03763 (2023).\n[20] Jinguo Cheng, Chunwei Yang, Wanlin Cai, Yuxuan Liang, and Yuankai Wu.\n2024. NuwaTS: Mending Every Incomplete Time Series. arXiv preprint\narXiv:2405.15317 (2024).\n[21] Chen Chu, Hengcai Zhang, Peixiao Wang, and Feng Lu. 2024. Simulating\nhuman mobility with a trajectory generation framework based on diffusion\nmodel. International Journal of Geographical Information Science (2024), 1‚Äì32.\n[22] Andrea Coletta, Sriram Gopalakrishnan, Daniel Borrajo, and Svitlana Vyetrenko.\n2024. On the constrained time-series generation problem. Advances in Neural\nInformation Processing Systems 36 (2024).\n[23] Jonathan Crabb√©, Nicolas Huynh, Jan Stanczuk, and Mihaela van der Schaar.\n2024. Time Series Diffusion in the Frequency Domain. arXiv preprint\narXiv:2402.05933 (2024).\n[24] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. 2023. A\ndecoder-only foundation model for time-series forecasting. arXiv preprint\narXiv:2310.10688 (2023).\n[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nIn NAACL-HLT. 4171‚Äì4186.\n[26] Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, Jianmin\nWang, and Mingsheng Long. 2024. TimeSiam: A Pre-Training Framework for\nSiamese Time-Series Modeling. arXiv preprint arXiv:2402.02475 (2024).\n[27] Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Ming-\nsheng Long. 2023. SimMTM: A Simple Pre-Training Framework for Masked\nTime-Series Modeling. Advances in Neural Information Processing Systems (2023).\n[28] Yuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, and\nChongjun Wang. 2021. Adarnn: Adaptive learning and forecasting of time\nseries. In Proceedings of the 30th ACM international conference on information &\nknowledge management . 402‚Äì411.\n[29] Wenying Duan, Liu Jiang, Ning Wang, and Hong Rao. 2019. Pre-Trained Bidirec-\ntional Temporal Representation for Crowd Flows Prediction in Regular Region.\nIEEE Access 7 (2019), 143855‚Äì143865.\n[30] Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant\nKalagnanam. 2023. TSMixer: Lightweight MLP-Mixer Model for Multivariate\nTime Series Forecasting. arXiv preprint arXiv:2306.09364 (2023).\n[31] Vijay Ekambaram, Arindam Jati, Nam H Nguyen, Pankaj Dayama, Chandra\nReddy, Wesley M Gifford, and Jayant Kalagnanam. 2024. TTMs: Fast Multi-\nlevel Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of\nMultivariate Time Series. arXiv preprint arXiv:2401.03955 (2024).\n[32] Cheng Feng, Long Huang, and Denis Krompass. 2024. Only the Curve Shape\nMatters: Training Foundation Models for Zero-Shot Multivariate Time Series\nForecasting through Next Curve Shape Prediction. arXiv:2402.07570 [cs.LG]\n[33] Tao-Yang Fu and Wang-Chien Lee. 2020. Trembr: Exploring Road Networks\nfor Trajectory Representation Learning. ACM Trans. Intell. Syst. Technol. 11, 1\n(2020), 10:1‚Äì10:25.\n[34] Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros\nTsiligkaridis, and Marinka Zitnik. 2024. UniTS: Building a Unified Time Series\nModel. arXiv preprint arXiv:2403.00131 (2024).\n[35] Azul Garza and Max Mergenthaler-Canseco. 2023. TimeGPT-1. arXiv preprint\narXiv:2310.03589 (2023).\n[36] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. 2023. Large\nLanguage Models Are Zero-Shot Time Series Forecasters. Advances in Neural\nInformation Processing Systems (2023).\n[37] Albert Gu and Tri Dao. 2023. Mamba: Linear-Time Sequence Modeling with\nSelective State Spaces. arXiv:2312.00752 [cs.LG]\n[38] Anisha Gunjal and Greg Durrett. 2023. Drafting Event Schemas using Language\nModels. arXiv preprint arXiv:2305.14847 (2023).\n[39] Hansika Hewamalage, Christoph Bergmeir, and Kasun Bandara. 2021. Recurrent\nneural networks for time series forecasting: Current status and future directions.\nInternational Journal of Forecasting 37, 1 (2021), 388‚Äì427.\n[40] Haowen Hou and F Richard Yu. 2024. RWKV-TS: Beyond Traditional Recurrent\nNeural Network for Time Series Tasks. arXiv preprint arXiv:2401.09093 (2024).\n[41] Junfeng Hu, Xu Liu, Zhencheng Fan, Yuxuan Liang, and Roger Zimmermann.\n2023. Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal\nGraph Learning. arXiv:2310.17360 [cs.LG]\n[42] Hongbin Huang, Minghua Chen, and Xiao Qiao. 2024. Generative Learning for\nFinancial Time Series with Irregular and Scale-Invariant Patterns. InThe Twelfth\nInternational Conference on Learning Representations . https://openreview.net/\nforum?id=CdjnzWsQax\n[43] Jiawei Jiang, Dayan Pan, Houxing Ren, Xiaohan Jiang, Chao Li, and Jingyuan\nWang. 2022. Self-supervised Trajectory Representation Learning with Temporal\nRegularities and Travel Semantics. CoRR abs/2211.09510 (2022).\n[44] Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin,\nDuo Wang, Anas Abidin, Kevin Eaton, Howard Antony Riina, Ilya Laufer,\nPaawan Punjabi, et al . 2023. Health system-scale language models are all-\npurpose prediction engines. Nature (2023), 1‚Äì6.\n[45] Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, Yuriy\nNevmyvaka, and Dongjin Song. 2024. Empowering Time Series Analysis with\nLarge Language Models: A Survey. arXiv preprint arXiv:2402.03182 (2024).\n[46] Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Ge-\noffrey I Webb, Irwin King, and Shirui Pan. 2023. A survey on graph neural\nnetworks for time series: Forecasting, classification, imputation, and anomaly\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuxuan Liang et al.\ndetection. arXiv preprint arXiv:2307.03759 (2023).\n[47] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi,\nPin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. 2023. Time-LLM:\nTime series forecasting by reprogramming large language models.arXiv preprint\narXiv:2310.01728 (2023).\n[48] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang,\nJames Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. 2023. Large models for\ntime series and spatio-temporal data: A survey and outlook. arXiv preprint\narXiv:2310.10196 (2023).\n[49] Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jin-\ndong Wang, Shirui Pan, and Qingsong Wen. 2024. Position: What Can Large\nLanguage Models Tell Us about Time Series Analysis. InInternational Conference\non Machine Learning (ICML‚Äô24) .\n[50] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and\nJaegul Choo. 2021. Reversible instance normalization for accurate time-series\nforecasting against distribution shift. In International Conference on Learning\nRepresentations.\n[51] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\n2023. Segment anything. In Proceedings of the IEEE/CVF International Conference\non Computer Vision . 4015‚Äì4026.\n[52] Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. 2023. Frozen\nLanguage Model Helps ECG Zero-Shot Learning. In Medical Imaging with Deep\nLearning.\n[53] Rongfan Li, Ting Zhong, Xinke Jiang, Goce Trajcevski, Jin Wu, and Fan Zhou.\n2022. Mining spatio-temporal relations via self-paced graph contrastive learning.\nIn Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining . 936‚Äì944.\n[54] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\nand Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-\nneck of transformer on time series forecasting. Advances in neural information\nprocessing systems 32 (2019).\n[55] Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. 2022. Generative time series\nforecasting with diffusion, denoise, and disentanglement. Advances in Neural\nInformation Processing Systems 35 (2022), 23009‚Äì23022.\n[56] Yan Lin, Jilin Hu, Shengnan Guo, Bin Yang, Christian S. Jensen, Youfang Lin, and\nHuaiyu Wan. 2024. GTM: General Trajectory Modeling with Auto-regressive\nGeneration of Feature Domains. arXiv:2402.07232 [cs.LG]\n[57] Yan Lin, Huaiyu Wan, Shengnan Guo, Jilin Hu, Christian S. Jensen, and Youfang\nLin. 2023. Pre-training General Trajectory Embeddings with Maximum Multi-\nview Entropy Coding. arXiv:2207.14539 [cs.CV]\n[58] Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li, and\nRui Zhao. 2024. Spatial-temporal large language model for traffic prediction.\narXiv preprint arXiv:2401.10134 (2024).\n[59] Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and Yanjie Fu. 2023.\nPriSTI: A Conditional Diffusion Framework for Spatiotemporal Imputation.\narXiv preprint arXiv:2302.09746 (2023).\n[60] Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, and\nRoger Zimmermann. 2024. UniTime: A Language-Empowered Unified Model\nfor Cross-Domain Time Series Forecasting. arXiv:2310.09751 [cs.LG]\n[61] Xu Liu, Yuxuan Liang, Chao Huang, Yu Zheng, Bryan Hooi, and Roger Zimmer-\nmann. 2022. When do contrastive learning signals help spatio-temporal graph\nforecasting?. In Proceedings of the 30th International Conference on Advances in\nGeographic Information Systems . 1‚Äì12.\n[62] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine,\nJiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel.\n2023. Large Language Models are Few-Shot Health Learners. arXiv preprint\narXiv:2305.15525 (2023).\n[63] Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingsheng Long.\n2024. AutoTimes: Autoregressive Time Series Forecasters via Large Language\nModels. arXiv preprint arXiv:2402.02370 (2024).\n[64] Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and\nMingsheng Long. 2024. Timer: Transformers for Time Series Analysis at Scale.\narXiv preprint arXiv:2402.02368 (2024).\n[65] Xin Man, Chenghong Zhang, Changyu Li, and Jie Shao. 2023. W-MAE: Pre-\ntrained weather model with masked autoencoder for multi-variable weather\nforecasting. arXiv preprint arXiv:2304.08754 (2023).\n[66] John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas\nRana, I. Budak Arpinar, and Ninghao Liu. 2024. A Survey of Deep Learning and\nFoundation Models for Time Series Forecasting. arXiv:2401.13912 [cs.LG]\n[67] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and\nAditya Grover. 2023. ClimaX: A foundation model for weather and climate.\nInternational Conference on Machine Learning (2023).\n[68] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022.\nA time series is worth 64 words: Long-term forecasting with transformers.arXiv\npreprint arXiv:2211.14730 (2022).\n[69] Yilmazcan Ozyurt, Stefan Feuerriegel, and Ce Zhang. 2022. Contrastive\nlearning for unsupervised domain adaptation of time series. arXiv preprint\narXiv:2206.06243 (2022).\n[70] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh\nChattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li,\nKamyar Azizzadenesheli, et al. 2022. Fourcastnet: A global data-driven high-\nresolution weather model using adaptive fourier neural operators.arXiv preprint\narXiv:2202.11214 (2022).\n[71] William Peebles and Saining Xie. 2023. Scalable diffusion models with trans-\nformers. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 4195‚Äì4205.\n[72] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,\nHuanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV,\net al. 2023. Rwkv: Reinventing rnns for the transformer era. arXiv preprint\narXiv:2305.13048 (2023).\n[73] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand-\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\n2021. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning . PMLR, 8748‚Äì8763.\n[74] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019. Language models are unsupervised multitask learners.\nOpenAI blog 1, 8 (2019), 9.\n[75] Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George\nAdamopoulos, Rishika Bhagwatkar, Marin Bilo≈°, Hena Ghonia, Nadhir Vincent\nHassen, Anderson Schneider, et al. 2023. Lag-llama: Towards foundation models\nfor time series forecasting. arXiv preprint arXiv:2310.08278 (2023).\n[76] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Au-\ntoregressive denoising diffusion models for multivariate probabilistic time series\nforecasting. In International Conference on Machine Learning . PMLR, 8857‚Äì8868.\n[77] Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, and Zhiyong Cui.\n2024. TPLLM: A Traffic Prediction Framework Based on Pretrained Large\nLanguage Models. arXiv preprint arXiv:2403.02221 (2024).\n[78] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn\nOmmer. 2022. High-resolution image synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\n10684‚Äì10695.\n[79] Zezhi Shao, Zhao Zhang, Fei Wang, and Yongjun Xu. 2022. Pre-training en-\nhanced spatial-temporal graph neural network for multivariate time series\nforecasting. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining . 1567‚Äì1577.\n[80] Lifeng Shen and James Kwok. 2023. Non-autoregressive Conditional Diffusion\nModels for Time Series Prediction. arXiv preprint arXiv:2306.05043 (2023).\n[81] Xiaoming Shi, Siqiao Xue, Kangrui Wang, Fan Zhou, James Y Zhang, Jun Zhou,\nChenhao Tan, and Hongyuan Mei. 2023. Language Models Can Improve Event\nPrediction by Few-Shot Abductive Reasoning. InAdvances in Neural Information\nProcessing Systems .\n[82] Md Fahim Sikder, Resmi Ramachandranpillai, and Fredrik Heintz. 2023. Trans-\nfusion: generating long, high fidelity time series using diffusion models with\ntransformers. arXiv preprint arXiv:2307.12667 (2023).\n[83] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. 2023. TEST: Text\nPrototype Aligned Embedding to Activate LLM‚Äôs Ability for Time Series. arXiv\npreprint arXiv:2308.08241 (2023).\n[84] Peiwang Tang and Xianchao Zhang. 2022. MTSMAE: Masked Autoencoders for\nMultivariate Time-Series Forecasting. In 2022 IEEE 34th International Conference\non Tools with Artificial Intelligence (ICTAI) . IEEE, 982‚Äì989.\n[85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. In Advances in Neural Information Processing Systems 30 . 5998‚Äì6008.\n[86] Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, and\nQingsong Wen. 2024. Deep Learning for Multivariate Time Series Imputation:\nA Survey. arXiv preprint arXiv:2402.04059 (2024).\n[87] Xinglei Wang, Meng Fang, Zichao Zeng, and Tao Cheng. 2023. Where\nWould I Go Next? Large Language Models as Human Mobility Predictors.\narXiv:2308.15197 [cs.AI]\n[88] Xuhong Wang, Ding Wang, Liang Chen, and Yilun Lin. 2023. Building Trans-\nportation Foundation Model via Generative Graph Transformer. arXiv preprint\narXiv:2305.14826 (2023).\n[89] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang,\nZhengyang Zhou, and Yang Wang. 2023. An observed value consistent diffusion\nmodel for imputing missing values in multivariate time series. In Proceedings\nof the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining .\n2409‚Äì2418.\n[90] Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Yunzhong Qiu, Haoran\nZhang, Jianmin Wang, and Mingsheng Long. 2024. TimeXer: Empowering\nTransformers for Time Series Forecasting with Exogenous Variables. arXiv\npreprint arXiv:2402.19072 (2024).\n[91] Zepu Wang, Yuqi Nie, Peng Sun, Nam H Nguyen, John Mulvey, and H Vincent\nPoor. 2023. St-mlp: A cascaded spatio-temporal linear framework with channel-\nindependence strategy for traffic forecasting. arXiv preprint arXiv:2308.07496\n(2023).\nFoundation Models for Time Series Analysis: A Tutorial and Survey KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\n[92] Zhixian Wang, Qingsong Wen, Chaoli Zhang, Liang Sun, and Yi Wang. 2023.\nDiffLoad: uncertainty quantification in load forecasting with diffusion model.\narXiv preprint arXiv:2306.01001 (2023).\n[93] Haomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Qingsong Wen, Roger\nZimmermann, and Yuxuan Liang. 2023. DiffSTG: Probabilistic spatio-temporal\ngraph forecasting with denoising diffusion models. Inthe 31st ACM International\nConference on Advances in Geographic Information Systems . 1‚Äì12.\n[94] Qingsong Wen, Linxiao Yang, Tian Zhou, and Liang Sun. 2022. Robust time\nseries analysis and applications: An industrial perspective. In28th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining . 4836‚Äì4837.\n[95] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,\nand Liang Sun. 2023. Transformers in time series: A survey. In International\nJoint Conference on Artificial Intelligence(IJCAI) . 6778‚Äì6786.\n[96] Christopher Wimmer and Navid Rekabsaz. 2023. Leveraging vision-language\nmodels for granular market change prediction. arXiv preprint arXiv:2301.10166\n(2023).\n[97] Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese,\nand Doyen Sahoo. 2024. Unified training of universal time series forecasting\ntransformers. arXiv preprint arXiv:2402.02592 (2024).\n[98] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng\nLong. 2022. Timesnet: Temporal 2d-variation modeling for general time series\nanalysis. In The eleventh international conference on learning representations .\n[99] Yutong Xia, Yuxuan Liang, Haomin Wen, Xu Liu, Kun Wang, Zhengyang Zhou,\nand Roger Zimmermann. 2024. Deciphering spatio-temporal graph forecasting:\nA causal lens and treatment. Advances in Neural Information Processing Systems\n36 (2024).\n[100] Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023.\nThe Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal\nStock Movement Prediction Challenges. arXiv preprint arXiv:2304.05351 (2023).\n[101] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly\nTransformer: Time Series Anomaly Detection with Association Discrepancy. In\nInternational Conference on Learning Representations .\n[102] Hao Xue and Flora D Salim. 2022. PromptCast: A New Prompt-based Learning\nParadigm for Time Series Forecasting. arXiv preprint arXiv:2210.08964 (2022).\n[103] Hao Xue, Bhanu Prakash Voutharoja, and Flora D Salim. 2022. Leveraging\nlanguage foundation models for human mobility forecasting. In the 30th Inter-\nnational Conference on Advances in Geographic Information Systems . 1‚Äì9.\n[104] Tijin Yan, Hongwei Zhang, Tong Zhou, Yufeng Zhan, and Yuanqing Xia. 2021.\nScoreGrad: Multivariate probabilistic time series forecasting with continuous\nenergy-based generative models. arXiv preprint arXiv:2106.10121 (2021).\n[105] Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen. 2021. Voice2series:\nReprogramming acoustic models for time series classification. In International\nconference on machine learning . PMLR, 11808‚Äì11819.\n[106] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith,\nChristopher Parisien, Colin Compas, Cheryl Martin, Anthony B Costa, Mona G\nFlores, et al. 2022. A large language model for electronic health records. NPJ\nDigital Medicine 5, 1 (2022), 194.\n[107] Chin-Chia Michael Yeh, Xin Dai, Huiyuan Chen, Yan Zheng, Yujie Fan, Audrey\nDer, Vivian Lai, Zhongfang Zhuang, Junpeng Wang, Liang Wang, et al. [n. d.].\nToward a foundation model for time series data. In Proceedings of the 32nd ACM\nInternational Conference on Information and Knowledge Management .\n[108] Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, Zongyi Liu, and Yanbin Lu.\n2023. Temporal Data Meets LLM‚ÄìExplainable Financial Time Series Forecasting.\narXiv preprint arXiv:2306.11025 (2023).\n[109] Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, and Yong Li. 2024. UniST: A\nPrompt-Empowered Universal Model for Urban Spatio-Temporal Prediction.\narXiv:2402.11838 [cs.LG]\n[110] Yuan Yuan, Jingtao Ding, Chenyang Shao, Depeng Jin, and Yong Li. 2023. Spatio-\ntemporal Diffusion Point Processes. arXiv preprint arXiv:2305.12403 (2023).\n[111] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang,\nYunhai Tong, and Bixiong Xu. 2022. Ts2vec: Towards universal representation\nof time series. In Proceedings of the AAAI Conference on Artificial Intelligence ,\nVol. 36. 8980‚Äì8987.\n[112] Taeyoung Yun, Haewon Jung, and Jiwoo Son. 2023. Imputation as Inpainting:\nDiffusion models for SpatioTemporal Data Imputation. (2023).\n[113] Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu,\nJames Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al. 2023. Self-\nsupervised learning for time series analysis: Taxonomy, progress, and prospects.\narXiv preprint arXiv:2306.10125 (2023).\n[114] Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, and Jingbo Shang. 2024.\nLarge Language Models for Time Series: A Survey. arXiv:2402.01801 [cs.LG]\n[115] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. [n. d.].\nSelf-supervised contrastive pre-training for time series via time-frequency con-\nsistency. Advances in Neural Information Processing Systems 35 ([n. d.]).\n[116] Yingying Zhang, Zhengxiong Guan, Huajie Qian, Leili Xu, Hengbo Liu, Qing-\nsong Wen, Liang Sun, Junwei Jiang, Lunting Fan, and Min Ke. 2021. CloudRCA: A\nroot cause analysis framework for cloud computing platforms. In Proceedings of\nthe 30th ACM International Conference on Information & Knowledge Management .\n4373‚Äì4382.\n[117] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\n[118] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\nquence time-series forecasting. InProceedings of the AAAI conference on artificial\nintelligence, Vol. 35. 11106‚Äì11115.\n[119] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.\n2022. Fedformer: Frequency enhanced decomposed transformer for long-term\nseries forecasting. In International conference on machine learning . 27268‚Äì27286.\n[120] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. 2023. One Fits\nAll: Power General Time Series Analysis by Pretrained LM. Advances in Neural\nInformation Processing Systems (2023).\n[121] Zhengyang Zhou, Qihe Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang,\nYuxuan Liang, and Yang Wang. 2023. Maintaining the Status Quo: Capturing\nInvariant Relations for OOD Spatiotemporal Learning. In Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ‚Äô23) .\n3603‚Äì3614.\n[122] Yuanshao Zhu, Yongchao Ye, Shiyao Zhang, Xiangyu Zhao, and James Yu. 2024.\nDifftraj: Generating gps trajectory with diffusion probabilistic model. Advances\nin Neural Information Processing Systems 36 (2024).\n[123] Zhaoyang Zhu, Weiqi Chen, Rui Xia, Tian Zhou, Peisong Niu, Bingqing Peng,\nWenwei Wang, Hengbo Liu, Ziqing Ma, Xinyue Gu, et al. 2023. Energy fore-\ncasting with robust, flexible, and explainable machine learning algorithms. AI\nMagazine 44, 4 (2023), 377‚Äì393.\nA APPENDIX\nIn this section, we discuss the future research directions and oppor-\ntunities of TSFMs from the methodology perspective.\nIncooporating Multi-modalities. As illustrated in this sur-\nvey, a majority of current foundation models for time series are\ndeveloped based on a single modality. However, many real-world\ndynamic systems are coupled with various modalities (time series,\ntext, even image data). It would be a promising direction to leverage\nvarious modalities along with the time series in TSFM to learn more\ncomprehensive and generalized knowledge, therefore significantly\nboosting the performance of different downstream tasks.\nExploring more Efficient Architectures. Currently, the Trans-\nformer serves as the dominant architecture for building the foun-\ndation model. Though promising, Transformer-based foundation\nmodels have quadratic scaling with respect to the sequence length\ndue to their self-attention mechanism. This makes them compu-\ntationally expensive and memory-intensive for processing long\nsequences. Therefore, it is an interesting avenue for future study\nto explore more efficient FM backbone architectures, such as state-\nspace models Mamba [37].\nDeveloping more Effective Pipelines. Time series data has\nunique properties such as temporal distribution shift [28, 91, 121]\n(i.e., the data distribution will evolve over time) and causality (i.e.,\ncasual relationship can exist between different points in the time\nseries) [99]. Therefore, it would be another existing as well as chal-\nlenging future direction to develop TSFMs that can well address\nthe temporal distribution shift or have a powerful Interpretability\nfor downstream tasks.\nProtecting Privacy. Protecting privacy is an essential concern\nwhen training foundation models on diverse sources and modali-\nties of data, which raises potential risks of exposing sensitive in-\nformation. As such, one future direction is the development of\nrobust privacy-preserving techniques for training the TSFM from\nmulti-source datasets, as well as keeping the utility of the trained\nFMs. This may include the advancement of federated learning ap-\nproaches, where models can be trained across multiple decentral-\nized devices or servers without exchanging raw data.\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Yuxuan Liang et al.\nTable 2: Summary of representative foundation models tailored for time series data modeling. N/A means not not applicable\nCategory Method Modaility Pre-training Adaptation Domain Year\nStandard Time Series\nTransformer-based\nTime-LLM [47] Multi-Modality Pretrained LLM Prompt-engineering & Tokenization General 2023\nOFA [120] Single-Modality Pretrained LLM Fine-tuning General 2023\nPromptCast [102] Multi-Modality Pretrained LLM Prompt-engineering General 2022\nTEST [83] Multi-Modality Contrastive Prompt-engineering & Tokenization General 2023\nLLM4TS [11] Single-Modality Pretrained LLM Fine-tuning General 2023\nTEMPO [10] Single-Modality Pretrained LLM Fine-tuning & Prompt-engineering General 2023\nLLMTime [36] Single-Modality Pretrained LLM Zero-shot General 2023\nYuet al. [108] Multi-Modality Pretrained LLM Zero-shot Finance 2023\nChenet al. [19] Multi-Modality Pretrained LLM Zero-shot Finance 2023\nXieet al. [100] Multi-Modality Pretrained LLM Zero-shot Finance 2023\nWimmeret al. [96] Single-Modality Pretrained VLM Fine-tuning Finance 2023\nLiuet al. [62] Multi-Modality Pretrained LLM Prompt-engineering Healthcare 2023\nMETS [52] Multi-Modality Contrastive Zero-shot Healthcare 2023\nVoice2Series [105] Single-Modality Pretrained AM Tokenization General 2021\nPatchTST [68] Single-Modality Generative Fine-tuning General 2022\nMoirai [97] Single-Modality Generative Fine-tuning General 2024\nTimer [64] Single-Modality Generative Fine-tuning General 2024\nTimeSiam [26] Single-Modality Generative Fine-tuning General 2024\nTimeXer [90] Single-Modality Fully-supervised - General 2024\nAutoTimes [63] Multi-Modality Pretrained LLM Prompt-engineering & Tokenization General 2024\nLag-Llama [75] Single-Modality Generative Zero-shot & Fine-tuning General 2023\nDaset al. [24] Single-Modality Generative Zero-shot General 2023\nTimeGPT-1 [35] Single-Modality Generative Zero-shot & Fine-tuning General 2023\nUniTS [34] Single-Modality Fully-supervised & Generative Zero-shot & Prompt-engineering General 2024\nChronos [1] Single-Modality Generative Zero-shot General 2024\nSimMTM [27] Single-Modality Hybrid Fine-tuning General 2023\nMTSMAE [84] Single-Modality Generative Fine-tuning General 2022\nTimeCLR [107] Single-Modality Contrastive Fine-tuning General 2023\nUniTime [60] Single-Modality Pretrained LLM Fine-tuning General 2024\nGTT [32] Single-Modality Fully-supervised Zero-shot General 2024\nNon-Transformer-based\nTF-C [115] Single-Modality Contrastive Fine-tuning General 2022\nCLUDA [69] Single-Modality Contrastive Fine-tuning General 2022\nTS2Vec [111] Single-Modality Contrastive Fine-tuning General 2021\nTimesNet [98] Single-Modality Fully-supervised - General 2022\nTSMixer [30] Single-Modality Generative Fine-tuning General 2023\nTTMs [31] Single-Modality Fully-supervised Zero-shot & Fine-tuning & Prompt-engineering General 2024\nRWKV-TS [40] Single-Modality Fully-supervised - General 2024\nDiffsuion-based\nTimeGrad [76] Single-Modality Generative N/A General 2021\nD3VAE [55] Single-Modality Generative N/A General 2022\nTransFusion [82] Single-Modality Generative N/A General 2023\nScoreGrad [104] Single-Modality Generative N/A General 2021\nBilo≈° et al. [5] Single-Modality Generative N/A General 2022\nCrabb√© et al. [23] Single-Modality Generative N/A General 2024\nTimeDiff [80] Single-Modality Generative N/A General 2023\nWang et al. [89] Single-Modality Generative N/A General 2023\nDiffTime [22] Single-Modality Generative N/A General 2024\nFTS-Diffusion [42] Single-Modality Generative N/A Finance 2023\nDiffLoad [92] Single-Modality Generative N/A Power 2023\nSpatial Time Series\nTransformer-based\nCPPBTR [29] Single-Modality Fully-supervised Fine-tuning Transportation 2019\nTFM [88] (graph TF) Single-Modality Fully-supervised - Transportation 2023\nSTEP [79] Single-Modality Generative Fine-tuning Transportation 2022\nST-LLM [58] Single-Modality Pretrained LLM Fine-tuning Transportation 2024\nFourCastNet [70] Single-Modality Fully-supervised Fine-tuning Climate 2022\nMetePFL [14] Single-Modality Generative Federated Learning & Prompt-Learning Climate 2023\nFedWing [15] Single-Modality Fully-supervised Federated Learning & Prompt-Learning Climate 2023\nClimaX [67] Single-Modality Fully-supervised Fine-tuning Climate 2023\nFengWu [13] Single-Modality Generative Zero-shot Climate 2023\nPangu-Weather [3] Single-Modality Fully-supervised Zero-shot Climate 2023\nW-MAE [65] Single-Modality Generative Fine-tuning Climate 2023\nGATGPT [18] Single-Modality Pretrained LLM Fine-tuning General 2023\nTPLLM [77] Single-Modality Pretrained LLM Fine-tuning Transportation 2024\nUniST [109] Single-Modality Generative Fine-tuning General 2024\nNon-Tranformer-based SPGCL [53] Single-Modality Contrastive - General 2022\nSTGCL [61] Single-Modality Contrastive Fine-tuning General 2021\nDiffusion-based\nDiffSTG [93] Single-Modality Generative N/A General 2023\nDSTPP [110] Single-Modality Generative N/A General 2023\nDYffusion [9] Single-Modality Generative N/A General 2023\nYun et al. [112] Singe-Modality Generative N/A General 2023\nUSTD [41] Single-Modality Generative N/A General 2023\nPriSTI [59] Single-Modality Generative N/A General 2023\nOthers\nTransformer-based\nAuxMobLCast [103] Single-Modality Pretrained LLM Fine-tuning & Prompt-engineering Mobility 2022\nLLM-Mob [87] Single-Modality Pretrained LLM Prompt-engineering Mobility 2023\nTrajCL [12] Single-Modality Contrastive Fine-tuning Mobility 2023\nGTM [56] Single-Modality Generative Zero-shot Mobility 2024\nLAMP [81] Singe-Modality Pretrained LLM Prompt-engineering Event 2023\nGunjal & Durrett [38] Singe-Modality Pretrained LLM Prompt-engineering Event 2023\nNYUTron [44] Singe-Modality Generative Fine-tuning Event(Healthcare) 2023\nGatorTron [106] Singe-Modality Generative Fine-tuning Event(Healthcare ) 2022\nNon-Transformer-based\nMMTEC [57] Single-Modality Contrastive Fine-tuning Mobility 2022\nSTART [43] Single-Modality Hybrid Fine-tuning Mobility 2022\nTrembr [33] Single-Modality Generative Fine-tuning Mobility 2020\nDiffusion-based TrajGDM [21] Singe-Modality Generative N/A Mobility 2024\nDiffTraj [122] Singe-Modality Generative N/A Mobility 2024",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6504601240158081
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6344106197357178
    },
    {
      "name": "Cornerstone",
      "score": 0.6089908480644226
    },
    {
      "name": "Data science",
      "score": 0.5719631910324097
    },
    {
      "name": "Time series",
      "score": 0.5008816719055176
    },
    {
      "name": "Pipeline (software)",
      "score": 0.45687782764434814
    },
    {
      "name": "Bridging (networking)",
      "score": 0.45552265644073486
    },
    {
      "name": "Management science",
      "score": 0.3763137459754944
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2752421498298645
    },
    {
      "name": "Engineering",
      "score": 0.22078660130500793
    },
    {
      "name": "Machine learning",
      "score": 0.1903339922428131
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}