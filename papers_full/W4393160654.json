{
  "title": "Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection",
  "url": "https://openalex.org/W4393160654",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101230819",
      "name": "Beizhe Hu",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2024108457",
      "name": "Qiang Sheng",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2097699427",
      "name": "Juan Cao",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2120037429",
      "name": "Shi, Yuhui. (North Carolina State University, Raleigh, Nc)",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2079581369",
      "name": "Yang Li",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2794886163",
      "name": "Danding Wang",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2106317801",
      "name": "Peng Qi",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A5101230819",
      "name": "Beizhe Hu",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2024108457",
      "name": "Qiang Sheng",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2097699427",
      "name": "Juan Cao",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2120037429",
      "name": "Shi, Yuhui. (North Carolina State University, Raleigh, Nc)",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2079581369",
      "name": "Yang Li",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2794886163",
      "name": "Danding Wang",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106317801",
      "name": "Peng Qi",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3212591930",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4382323433",
    "https://openalex.org/W4307907065",
    "https://openalex.org/W4283452667",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W3119467012",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W4224310546",
    "https://openalex.org/W4221122068",
    "https://openalex.org/W4319653458",
    "https://openalex.org/W3035268925",
    "https://openalex.org/W6803182457",
    "https://openalex.org/W6966986966",
    "https://openalex.org/W2890801081",
    "https://openalex.org/W6779317912",
    "https://openalex.org/W3206237685",
    "https://openalex.org/W4221168025",
    "https://openalex.org/W3201124874",
    "https://openalex.org/W3031781733",
    "https://openalex.org/W2949813784",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W6793220154",
    "https://openalex.org/W2949431184",
    "https://openalex.org/W6910741466",
    "https://openalex.org/W4385565280",
    "https://openalex.org/W4224316507",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2809476703",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4383045293",
    "https://openalex.org/W4376654497",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3098829544",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4378506863",
    "https://openalex.org/W4389520264",
    "https://openalex.org/W4378465130",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W2951307134",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3152907744",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4386566461",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2742330194",
    "https://openalex.org/W3210022786",
    "https://openalex.org/W4321524280",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2991596147",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3034020579",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1522301498"
  ],
  "abstract": "Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good advisor for SLMs by providing multi-perspective instructive rationales. To instantiate this proposal, we design an adaptive rationale guidance network for fake news detection (ARG), in which SLMs selectively acquire insights on news analysis from the LLMs' rationales. We further derive a rationale-free version of ARG by distillation, namely ARG-D, which services cost-sensitive scenarios without inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and ARG-D outperform three types of baseline methods, including SLM-based, LLM-based, and combinations of small and large language models.",
  "full_text": "Bad Actor, Good Advisor:\nExploring the Role of Large Language Models in Fake News Detection\nBeizhe Hu1,2, Qiang Sheng1, Juan Cao1,2, Yuhui Shi1,2, Yang Li1,2, Danding Wang1, Peng Qi3\n1CAS Key Lab of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences\n3National University of Singapore\n{hubeizhe21s, shengqiang18z, caojuan, shiyuhui22s, liyang23s, wangdanding}@ict.ac.cn, pengqi.qp@gmail.com\nAbstract\nDetecting fake news requires both a delicate sense of di-\nverse clues and a profound understanding of the real-world\nbackground, which remains challenging for detectors based\non small language models (SLMs) due to their knowledge\nand capability limitations. Recent advances in large language\nmodels (LLMs) have shown remarkable performance in var-\nious tasks, but whether and how LLMs could help with fake\nnews detection remains underexplored. In this paper, we in-\nvestigate the potential of LLMs in fake news detection. First,\nwe conduct an empirical study and find that a sophisticated\nLLM such as GPT 3.5 could generally expose fake news and\nprovide desirable multi-perspective rationales but still under-\nperforms the basic SLM, fine-tuned BERT. Our subsequent\nanalysis attributes such a gap to the LLM’s inability to select\nand integrate rationales properly to conclude. Based on these\nfindings, we propose that current LLMs may not substitute\nfine-tuned SLMs in fake news detection but can be a good\nadvisor for SLMs by providing multi-perspective instructive\nrationales. To instantiate this proposal, we design an adaptive\nrationale guidance network for fake news detection (ARG),\nin which SLMs selectively acquire insights on news analysis\nfrom the LLMs’ rationales. We further derive a rationale-free\nversion of ARG by distillation, namely ARG-D, which ser-\nvices cost-sensitive scenarios without querying LLMs. Ex-\nperiments on two real-world datasets demonstrate that ARG\nand ARG-D outperform three types of baseline methods, in-\ncluding SLM-based, LLM-based, and combinations of small\nand large language models.\nIntroduction\nThe wide and fast spread of fake news online has posed real-\nworld threats in critical domains like politics (Fisher, Cox,\nand Hermann 2016), economy (CHEQ 2019), and public\nhealth (Naeem and Bhatti 2020). Among the countermea-\nsures to combat this issue, automatic fake news detection ,\nwhich aims at distinguishing inaccurate and intentionally\nmisleading news items from others automatically, has been a\npromising solution in practice (Shu et al. 2017; Roth 2022).\nThough much progress has been made (Hu et al. 2022a),\nunderstanding and characterizing fake news is still challeng-\ning for current models. This is caused by the complexity of\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nSmallLanguage Model\n[Label: FAKE] Detailed photos of Xiang Liu's tendon surgery exposed. Stop complaints and please show sympathy and blessings!\nLargeLanguage Model\nREAL\nThe answer is real.\n[News]\nLargeLanguage Model\n- Commonsense: Real surgery generally won’t be exposed…- Textual Description: The language is emotional and tries to attract audience…\n[News][Perspective-specific Prompting]+\n[News][Prompting]+\nSmallLanguage Model\n[News] Prediction: FAKE\n(a)\n(b)\nFigure 1: Illustration of the role of large language models\n(LLMs) in fake news detection. In this case, (a) the LLM\nfails to output correct judgment of news veracity but (b)\nhelps the small language model (SLM) judge correctly by\nproviding informative rationales.\nthe news-faking process: Fake news creators might manip-\nulate any part of the news, using diverse writing strategies\nand being driven by inscrutable underlying aims. Therefore,\nto maintain both effectiveness and universality for fake news\ndetection, an ideal method is required to have: 1) a delicate\nsense of diverse clues (e.g., style, facts, commonsense); and\n2) a profound understanding of the real-world background.\nRecent methods (Zhang et al. 2021; Kaliyar, Goswami,\nand Narang 2021; Mosallanezhad et al. 2022; Hu et al.\n2023) generally exploit pre-trained small language models\n(SLMs)1 like BERT (Devlin et al. 2019) and RoBERTa (Liu\net al. 2019) to understand news content and provide fun-\ndamental representation, plus optional social contexts (Shu\net al. 2019; Cui et al. 2022), knowledge bases (Popat et al.\n2018; Hu et al. 2022b), or news environment (Sheng et al.\n2022) as supplements. SLMs do bring improvements, but\ntheir knowledge and capability limitations also compromise\nfurther enhancement of fake news detectors. For example,\nBERT was pre-trained on text corpus like Wikipedia (De-\nvlin et al. 2019) and thus struggled to handle news items\nthat require knowledge not included (Sheng et al. 2021).\n1The academia lacks a consensus regarding the size boundary\nbetween small and large language models at present, but it is widely\naccepted that BERT (Devlin et al. 2019) and GPT-3 family (Brown\net al. 2020) are respectively small and large ones (Zhao et al. 2023).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22105\nAs a new alternative to SLMs, large language models\n(LLMs) (OpenAI 2022; Anthropic 2023; Touvron et al.\n2023), which are usually trained on the larger-scale corpus\nand aligned with human preferences, have shown impres-\nsive emergent abilities on various tasks (Wei et al. 2022a)\nand are considered promising as general task solvers (Ma\net al. 2023). However, the potential of LLMs in fake news\ndetection remains underexplored: 1) Can LLMs help detect\nfake news with their internal knowledge and capability? 2)\nWhat solution should we adopt to obtain better performance\nusing LLMs?\nTo answer these two questions, we first conduct a deep\ninvestigation of the effective role of LLMs in fake news\ndetection and attempt to provide a practical LLM-involved\nsolution. Unlike contemporary works (Pelrine et al. 2023;\nCaramancion 2023) which simply prompt LLMs to provide\npredictions with the task instruction, we conduct a detailed\nempirical study to mine LLMs’ potential. Specifically, we\nuse four typical prompting approaches (zero-shot/few-shot\nvanilla/chain-of-thought prompting) to ask the LLM to make\nveracity judgments of given news items (Figure 1(a)) and\nfind that even the best-performing LLM-based method still\nunderperforms task-specific fine-tuned SLMs. We then per-\nform an analysis of the LLM-generated explanatory ratio-\nnales and find that the LLM could provide reasonable and\ninformative rationales from several perspectives. By subse-\nquently inducing the LLM with perspective-specific prompts\nand performing rule-based ensembles of judgments, we find\nthat rationales indeed benefit fake news detection, and at-\ntribute the unsatisfying performance to the LLM’s inability\nto select and integrate rationales properly to conclude.\nBased on these findings, we propose that the current LLM\nmay not be a good substitute for the well-fine-tuned SLM but\ncould serve as a good advisor by providing instructive ratio-\nnales, as presented in Figure 1(b). To instantiate our pro-\nposal, we design the adaptive rationale guidance (ARG) net-\nwork for fake news detection, which bridges the small and\nlarge LMs by selectively injecting new insight about news\nanalysis from the large LM’s rationales to the small LM.\nThe ARG further derives the rationale-free ARG-D via dis-\ntillation for cost-sensitive scenarios with no need to query\nLLMs. Experiments on two real-world datasets show that\nARG and ARG-D outperform existing SLM/LLM-only and\ncombination methods. Our contributions are as follows:\n• Detailed investigation: We investigate the effective role\nof LLMs in fake news detection and find the LLM is bad\nat veracity judgment but good at analyzing contents;\n• Novel and practical solution: We design a novel ARG\nnetwork and its distilled version ARG-D that comple-\nments small and large LMs by selectively acquiring in-\nsights from LLM-generated rationales for SLMs, which\nhas shown superiority based on extensive experiments;\n• Useful resource: We construct a rationale collection\nfrom GPT-3.5 for fake news detection in two languages\n(Chinese and English) and make it publicly available to\nfacilitate further research.2\n2Code, data, and the extended version are available at https:\n//github.com/ICTMCG/ARG\n#\nChinese English\nTrain Val Test Train Val Test\nReal 2,331 1,172 1,137 2,878 1,030 1,024\nFake 2,873 779 814 1,006 244 234\nTotal 5,204 1,951 1,951 3,884 1,274 1,258\nTable 1: Statistics of the fake news detection datasets.\nLLM\nNewsTask Description\nPrediction\nNewsTask Description\nEliciting Sentence\n(e.g., “Let’s think step by step”)\nNewsTask Description\nNews-Label Pairs           1 N…\nNewsTask Description\nNews-Label-\nRationale Triplets 1 N…\nPrediction\nRationale\n(a) Zero-Shot Prompting\n(b) Zero-Shot CoT Prompting\n(c) Few-Shot Prompting\n(d) Few-Shot CoT Prompting\nfor (a) (c)\nfor (b) (d)\nFigure 2: Illustration of prompting approaches for LLMs.\nIs the LLM a Good Detector?\nIn this section, we evaluate the performance of the represen-\ntative LLM, i.e., GPT-3.5 in fake news detection to reveal\nits judgment capability. We exploit four typical prompting\napproaches and perform a comparison with the SLM (here,\nBERT) fine-tuned on this task. Formally, given a news item\nx, the model aims to predict whether x is fake or not.\nExperimental Settings\nDataset We employ the Chinese dataset Weibo21 (Nan\net al. 2021) and the English dataset GossipCop (Shu et al.\n2020) for evaluation. Following existing works (Zhu et al.\n2022; Mu, Bontcheva, and Aletras 2023), we preprocess the\ndatasets with deduplication and temporal data split to avoid\npossible performance overrating led by data leakage for the\nSLM. Table 1 presents the dataset statistics.\nLarge Language Model We evaluate GPT-3.5-turbo, the\nLLM developed by OpenAI and supporting the popular\nchatbot ChatGPT (OpenAI 2022), due to its representative-\nness and convenient calling. The large scale of parame-\nters makes task-specific fine-tuning almost impossible for\nLLMs, so we use the prompt learning paradigm, where an\nLLM learns tasks given prompts containing instructions or\nfew-shot demonstrations (Liu et al. 2023a). In detail, we uti-\nlize the following four typical prompting approaches to elicit\nthe potential of the LLM in fake news detection (Figure 2):\n• Zero-Shot Prompting constructs prompt only contain-\ning the task description and the given news. To make the\nresponse more proficient and decrease the refusal ratio,\nwe optionally adopt the role-playing technique when de-\nscribing our task (Liu et al. 2023b; Ramlochan 2023).\n• Zero-Shot CoT Prompting (Kojima et al. 2022) is\na simple and straightforward chain-of-thought (CoT)\nprompting approach to encourage the LLM to reason. In\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22106\nModel Usage Chinese English\nGPT-3.5-\nturbo\nZero-Shot 0.676 0.568\nZero-Shot CoT 0.677 0.666\nFew-Shot 0.725 0.697\nFew-Shot CoT 0.681 0.702\nBERT Fine-tuning 0.753 (+3.8%) 0.765 (+9.0%)\nTable 2: Performance in macro F1 of the large and small\nLMs. The best two results are bolded and underlined, respec-\ntively. The relative increases over the second-best results are\nshown in the brackets.\naddition to the elements in zero-shot prompting, it adds\nan eliciting sentence such as “Let’s think step by step. ”\n• Few-Shot Prompting (Brown et al. 2020) provides task-\nspecific prompts and several news-label examples as\ndemonstrations. After preliminary tests of {2,4,8}-shot\nsettings, we choose 4-shot prompting which includes two\nreal and two fake samples.\n• Few-Shot CoT Prompting (Wei et al. 2022b) not only\nprovides news-label examples but also demonstrates rea-\nsoning steps with prepared rationales. Here, we obtain\nthe provided rationale demonstrations from the correct\nand reasonable outputs of zero-shot CoT prompting.\nSmall Language Model We adopt the pre-trained small lan-\nguage models, BERT (Devlin et al. 2019) as the representa-\ntive, given its wide use in this task (Kaliyar, Goswami, and\nNarang 2021; Zhu et al. 2022; Sheng et al. 2022). Specifi-\ncally, we limit the maximum length of the text to 170 tokens\nand use chinese-bert-wwm-ext and bert-base-uncased from\nTransformers package (Wolf et al. 2020) for the Chinese and\nEnglish evaluation, respectively. We use Adam as the opti-\nmizer and do a grid search for the optimal learning rate. We\nreport the testing result on the best-validation checkpoint.\nComparison between Small and Large LMs\nTable 2 presents the performance of GPT-3.5-turbo with four\nprompting approaches and the fine-tuned BERT on the two\ndatasets. We observe that:\n1) Though the LLM is generally believed powerful, the\nLLM underperforms the fine-tuned SLM using all four\nprompting approaches. The SLM has a relative increase of\n3.8%∼11.3% in Chinese and 9.0%∼34.6% in English over\nthe LLM, indicating that the LLM lacks task-specific knowl-\nedge while the SLM learns during fine-tuning.\n2) Few-shot versions outperform zero-shot ones, suggest-\ning the importance of task samples. However, introducing\nseveral samples only narrows the gap with the SLM but does\nnot lead to surpassing.\n3) CoT prompting brings additional performance gain in\ngeneral, especially under the zero-shot setting on the English\ndataset (+17.3%). However, we also observe some cases\nwhere CoT leads to a decrease. This indicates that effective\nuse of rationales may require more careful design.\nOverall, given the LLM’s unsatisfying performance and\nhigher inference costs than the SLM, the current LLM has\nnot been a “good enough” detector to substitute task-specific\nSLMs in fake news detection.\nPerspective\nChinese English\nProportion macF1 Proportion macF1\nTextual Description 65% 0.706 71% 0.653\nNews: Everyone! Don’t buy cherries anymore: Cherries of this\nyear are infested with maggots, and nearly 100% are affected.\nLLM Rationale: ...The tone of the news is extremely urgent,\nseemingly trying to spread panic and anxiety.\nPrediction: Fake Ground Truth: Fake\nCommonsense 71% 0.698 60% 0.680\nNews: Huang, the chief of Du’an Civil Affairs Bureau, gets\nsubsistence allowances of 509 citizens, owns nine properties,\nand has six wives...\nLLM Rationale: ...The news content is extremely outra-\ngeous...Such a situation is incredibly rare in reality and even\ncould be thought impossible.\nPrediction: Fake Ground Truth: Fake\nFactuality 17% 0.629 24% 0.626\nNews: The 18th National Congress has approved that individ-\nuals who are at least 18 years old are now eligible to marry...\nLLM Rationale: First, the claim that Chinese individuals at\nleast 18 years old can register their marriage is real, as this is\nstipulated by Chinese law...\nPrediction: Real Ground Truth: Fake\nOthers 4% 0.649 8% 0.704\nTable 3: Analysis on different perspectives of LLM’s ratio-\nnales in the sample set, including the data ratio, LLM’s per-\nformance, and cases.\nAnalysis on the Rationales from the LLM\nThough the LLM is bad at news veracity judgment, we also\nnotice that the rationales generated through zero-shot CoT\nprompting exhibit a unique multi-perspective analytical ca-\npability that is challenging and rare for SLMs. For further\nexploration, we sample 500 samples from each of the two\ndatasets and manually categorize them according to the per-\nspectives from which the LLM performs the news analysis.\nStatistical results by perspectives and cases are presented in\nTable 3.3 We see that: 1) The LLM is capable of gener-\nating human-like rationales on news content from var-\nious perspectives, such as textual description, common-\nsense, and factuality, which meets the requirement of the\ndelicate sense of diverse clues and profound understanding\nof the real-world background in fake news detection. 2) The\ndetection performance on the subset using certain perspec-\ntives is higher than the zero-shot CoT result on the full test-\ning set. This indicates the potential of analysis by perspec-\ntives, though the coverage is moderate.3) The analysis from\nthe perspective of factuality leads to the performance lower\nthan average, indicating the unreliability of using the LLM\nfor factuality analysis based on its internal memorization.\nWe speculate this is caused by the hallucination issue (Ji\net al. 2023; Zhang et al. 2023).\n3Note that a sample may be analyzed from multiple perspec-\ntives and thus the sum of proportions might be larger than 100%.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22107\nModel Usage Chinese English\nGPT-3.5-turbo\nZero-Shot CoT 0.677 0.666\nfrom Perspective TD 0.667 0.611\nfrom Perspective CS 0.678 0.698\nBERT Fine-tuning 0.753 0.765\nEnsemble Majority V oting 0.735 0.724\nOracle V oting 0.908 0.878\nTable 4: Performance of the LLM using zero-shot CoT with\nperspective specified and other compared models. TD: Tex-\ntual description; CS: Commonsense.\nWe further investigate the LLM’s performance when\nasked to perform analysis from a specific perspective on the\nfull testing set (i.e., 100% coverage). 4 From the first group\nin Table 4, we see that the LLM’s judgment with single-\nperspective analysis elicited is still promising. Compared\nwith the comprehensive zero-shot CoT setting, the single-\nperspective-based LLM performs comparatively on the Chi-\nnese dataset and is better on the English dataset (for the com-\nmonsense perspective case). The results showcase that the\ninternal mechanism of the LLM to integrate the rationales\nfrom diverse perspectives is ineffective for fake news detec-\ntion, limiting the full use of rationales. In this case, com-\nbining the small and large LMs to complement each other\nis a promising solution: The former could benefit from the\nanalytical capability of the latter, while the latter could be\nenhanced by task-specific knowledge from the former.\nTo exhibit the advantages of this solution, we apply major-\nity voting and oracle voting (assuming the most ideal situa-\ntion where we trust the correctly judged model for each sam-\nple, if any) among the two single-perspective-based LLMs\nand the BERT. Results show that we are likely to gain a per-\nformance better than any LLM-/SLM-only methods men-\ntioned before if we could adaptively combine their advan-\ntages, i.e., the flexible task-specific learning of the SLM and\nthe informative rationale generated by the LLM. That is,\nthe LLM could be possibly a good advisor for the SLM\nby providing rationales, ultimately improving the perfor-\nmance of fake news detection.\nARG: Adaptive Rationale Guidance Network\nfor Fake News Detection\nBased on the above findings and discussion, we propose the\nadaptive rationale guidance (ARG) network for fake news\ndetection. Figure 3 overviews the ARG and its rationale-free\nversion ARG-D, for cost-sensitive scenarios. The objective\nof ARG is to empower small fake news detectors with the\nability to adaptively select useful rationales as references for\nfinal judgments. Given a news item x and its correspond-\ning LLM-generated rationales rt (textual description) and rc\n(commonsense), the ARG encodes the inputs using the SLM\nat first (Figure 3(a)). Subsequently, it builds news-rationale\n4We exclude the factuality to avoid the impacts of hallucina-\ntion. The eliciting sentence is “Let’s think from the perspective of\n[textual description/commonsense].”\ncollaboration via predicting the LLM’s judgment through\nthe rationale, enriching news-rationale feature interaction,\nand evaluating rationale usefulness (Figure 3(b)). The inter-\nactive features are finally aggregated with the news feature\nx for the final judgment of x being fake or not (Figure 3(c)).\nARG-D is derived from the ARG via distillation for scenar-\nios where the LLM is unavailable (Figure 3(d)).\nRepresentation\nWe employ two BERT models separately as the news and\nrationale encoder to obtain semantic representations. For the\ngiven news item x and two corresponding rationales rt and\nrc, the representations are X, Rt, and Rc, respectively.\nNews-Rationale Collaboration\nThe step of news-rationale collaboration aims at providing\na rich interaction between news and rationales and learning\nto adaptively select useful rationales as references, which\nis at the core of our design. To achieve such an aim, ARG\nincludes three modules, as detailed and exemplified using\nthe textual description rationale branch below:\nNews-Rationale Interaction To enable comprehensive\ninformation exchange between news and rationales, we\nintroduce a news-rationale interactor with a dual cross-\nattention mechanism to encourage feature interactions. The\ncross-attention can be described as:\nCA(Q, K, V) = softmax\n\u0010\nQ′ · K′/\n√\nd\n\u0011\nV′, (1)\nwhere Q′ = WQQ, K′ = WKK, and V′ = WVV. d is\nthe dimensionality. Given representations of the newsX and\nthe rationale Rt, the process is:\nft→x = AvgPool (CA(Rt, X, X)) , (2)\nfx→t = AvgPool (CA(X, Rt, Rt)) , (3)\nwhere AvgPool(·) is the average pooling over the token\nrepresentations outputted by cross-attention to obtain one-\nvector text representation f.\nLLM Judgement Prediction Understanding the judg-\nment hinted by the given rationale is a prerequisite for fully\nexploiting the information behind the rationale. To this end,\nwe construct the LLM judgment prediction task, whose re-\nquirement is to predict the LLM judgment of the news verac-\nity according to the given rationale. We expect this to deepen\nthe understanding of the rationale texts. For the textual de-\nscription rationale branch, we feed its representationRt into\nthe LLM judgment predictor, which is parametrized using a\nmulti-layer perception (MLP)5:\nˆmt = sigmoid(MLP(Rt)), Lpt = CE( ˆmt, mt), (4)\nwhere mt and ˆmt are respectively the LLM’s claimed judg-\nment and its prediction. The loss Lpt is a cross-entropy loss\nCE(ˆy, y) =−y log ˆy−(1−y) log(1−ˆy). The case is similar\nfor commonsense rationale Rc.\n5For brevity, we omit the subscripts of all independently\nparametrized MLPs.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22108\nRc\nRt\nNewsEncoder\nRationaleEncoder\nX\nft→xfx→t\n Rationale UsefulnessEvaluatorLetwt\nLpt\n Classifier\nfcls\nNews     Encoder\n Rationale-AwareFeature Simulator X\n Classifierfcls\nx\nAttention\nAttention\n(a) Representation(b) News-Rationale Collaboration\n(d) Distillation for Rationale-Free Model\nNews Item\nTextualDescriptionRationale\nCommonsenseRationale\nNews-Rationale Interactor\nLLM Judgment Predictor\nft→x’\nfc→xfx→c\n Rationale UsefulnessEvaluatorLecwc\nLpc\nNews-Rationale Interactor\nLLM Judgment Predictor\nfc→x’\nFeatureAggregator\nLLMRationales\n Vector/MatrixModuleLoss\nLce\n(c) Prediction\nNews Item Lced\n↑ ARG Network↓ ARG-D Network\nLkddistill from   fcls\nContent\nRationales\ninitialized frommodule in (c)initialized frommodule in (a)\nFigure 3: Overall architecture of our proposed adaptive rationale guidance (ARG) network and its rationale-free version ARG-\nD. In the ARG, the news item and LLM rationales are (a) respectively encoded into X and R∗(∗ ∈ {t, c}). Then the small and\nlarge LMs collaborate with each other via news-rationale feature interaction, LLM judgment prediction, and rationale usefulness\nevaluation. The obtained interactive features f′\n∗→x (∗ ∈ {t, c}). These features are finally aggregated with attentively pooled\nnews feature x for the final judgment. In the ARG-D, the news encoder and the attention module are preserved and the output\nof the rationale-aware feature simulator is supervised by the aggregated feature fcls for knowledge distillation.\nRationale Usefulness Evaluation The usefulness of ra-\ntionales from different perspectives varies across different\nnews items and improper integration may lead to perfor-\nmance degradation. To enable the model to adaptively se-\nlect appropriate rationale, we devise a rationale usefulness\nevaluation process, in which we assess the contributions of\ndifferent rationales and adjust their weights for subsequent\nveracity prediction. The process comprises two phases, i.e.,\nevaluation and reweighting. For evaluation, we input the\nnews-aware rationale vector fx→t into the rationale useful-\nness evaluator (parameterized by an MLP) to predict its use-\nfulness ut. Following the assumption that rationales leading\nto correct judgments are more useful, we use the judgment\ncorrectness as the rationale usefulness labels.\nˆut = sigmoid(MLP(fx→t)), Let = CE(ˆut, ut). (5)\nIn the reweighting phase, we input vectorfx→t into an MLP\nto obtain a weight numberwt, which is then used to reweight\nthe rationale-aware news vector ft→x. The procedure is as\nfollows:\nft→x\n′ = wt · ft→x. (6)\nWe also use attentive pooling to transform the representation\nmatrix X into a vector x.\nPrediction\nBased on the outputs from the last step, we now aggregate\nnews vector x and rationale-aware news vector f′\nt→x, f′\nc→x\nfor the final judgment. For news item x with label y ∈\n{0, 1}, we aggregate these vectors with different weights:\nfcls = wcls\nx · x + wcls\nt · f′\nt→x + wcls\nc · f′\nc→x, (7)\nwhere wcls\nx , wcls\nt and wcls\nc are learnable parameters ranging\nfrom 0 to 1. fcls is the fusion vector, which is then fed into\nthe MLP classifier for final prediction of news veracity:\nLce = CE(MLP(fcls), y). (8)\nThe total loss function is the weighted sum of the loss terms\nmentioned above:\nL = Lce + β1(Let + Lec) +β2(Lpt + Lpc), (9)\nwhere β1 and β2 are hyperparameters.\nDistillation for Rationale-Free Model\nThe ARG requires sending requests to the LLM for every\nprediction, which might not be affordable for cost-sensitive\nscenarios. Therefore, we attempt to build a rationale-free\nmodel, namely ARG-D, based on the trained ARG model via\nknowledge distillation (Hinton, Vinyals, and Dean 2015).\nThe basic idea is simulated and internalized the knowledge\nfrom rationales into a parametric module. As shown in Fig-\nure 3(d), we initialize the news encoder and classifier with\nthe corresponding modules in the ARG and train a rationale-\naware feature simulator (implemented with a multi-head\ntransformer block) and an attention module to internalize\nknowledge. Besides the cross-entropy loss Lce, we let the\nfeature fd\ncls to imitate fcls in the ARG, using the mean\nsquared estimation loss:\nLkd = MSE(fcls, fd\ncls). (10)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22109\nModel\nChinese English\nmacF1 Acc. F1 real F1fake macF1 Acc. F1 real F1fake\nG1: LLM-Only GPT-3.5-turbo 0.725 0.734 0.774 0.676 0.702 0.813 0.884 0.519\nG2: SLM-Only\nBaseline 0.753 0.754 0.769 0.737 0.765 0.862 0.916 0.615\nEANNT 0.754 0.756 0.773 0.736 0.763 0.864 0.918 0.608\nPublisher-Emo 0.761 0.763 0.784 0.738 0.766 0.868 0.920 0.611\nENDEF 0.765 0.766 0.779 0.751 0.768 0.865 0.918 0.618\nG3: LLM+SLM\nBaseline + Rationale 0.767 0.769 0.787 0.748 0.777 0.870 0.921 0.633\nSuperICL 0.757 0.759 0.779 0.734 0.736 0.864 0.920 0.551\nARG 0.784 0.786 0.804 0.764 0.790 0.878 0.926 0.653\n(Relative Impr. over Baseline) (+4.2%) (+4.3%) (+4.6%) (+3.8%) (+3.2%) (+1.8%) (+1.1%) (+6.3%)\nw/o LLM Judgment Predictor 0.773 0.774 0.789 0.756 0.786 0.880 0.928 0.645\nw/o Rationale Usefulness Evaluator 0.781 0.783 0.801 0.761 0.782 0.873 0.923 0.641\nw/o Predictor & Evaluator 0.769 0.770 0.782 0.756 0.780 0.874 0.923 0.637\nARG-D 0.771 0.772 0.785 0.756 0.778 0.870 0.921 0.634\n(Relative Impr. over Baseline) (+2.4%) (+2.3%) (+2.1%) (+2.6%) (+1.6%) (+0.9%) (+0.6%) (+3.2%)\nTable 5: Performance of the ARG and its variants and the LLM-only, SLM-only, LLM+SLM methods. The best two results in\nmacro F1 and accuracy are respectively bolded and underlined. For GPT-3.5-turbo, the best results in Table 2 are reported.\nEvaluation\nExperimental Settings\nBaselines We compare three groups of methods:\nG1 (LLM-Only): We list the performance of the best-\nperforming setting on each dataset in Table 2, i.e., few-shot\nin Chinese and few-shot CoT in English.\nG2 (SLM-Only) 6: 1) Baseline: The vanilla BERT-base\nmodel whose setting remains consistent with that in Sec-\ntion . 2) EANNT (Wang et al. 2018): A model that learns\neffective signals using auxiliary adversarial training, aiming\nat removing event-related features as much as possible. We\nused publication year as the label for the auxiliary task. 3)\nPublisher-Emo (Zhang et al. 2021): A model that fuses a\nseries of emotional features with textual features for fake\nnews detection. 4) ENDEF (Zhu et al. 2022): A model that\nremoves entity bias via causal learning for better generaliza-\ntion on distribution-shifted fake news data. All methods in\nthis group used the same BERT as the text encoder.\nG3 (LLM+SLM): 1) Baseline+Rationale: It concatenates\nfeatures from the news encoder and rationale encoder and\nfeeds them into an MLP for prediction. 2) SuperICL (Xu\net al. 2023): It exploits the SLM as a plug-in for the in-\ncontext learning of the LLM by injecting the prediction and\nthe confidence for each testing sample into the prompt.\nImplementation Details We use the same datasets intro-\nduced in Section and keep the setting the same in terms\nof the pre-trained model, learning rate, and optimization\nmethod. For the ARG-D network, the parameters of the\nnews encoder and classifier are derived from the ARG\nmodel. A four-head transformer block is implemented in the\nrationale-aware feature simulator. The weight of loss func-\ntions Let, Lpt, Lec, Lpc in the ARG and Lkd in the ARG-D\nare grid searched.\n6As this paper focuses on text-based news, we use the text-only\nvariant of the original EANN following (Sheng et al. 2021) and the\npublisher-emotion-only variant in (Zhang et al. 2021).\nPerformance Comparison and Ablation Study\nTable 5 presents the performance of our proposed ARG and\nits variants and the compared methods. From the results,\nwe observe that: 1) The ARG outperforms all other com-\npared methods in macro F1, demonstrating its effectiveness.\n2) The rationale-free ARG-D still outperforms all compared\nmethods except ARG and its variants, which shows the pos-\nitive impact of the distilled knowledge from ARG. 3) The\ntwo compared LLM+SLM methods exhibit different perfor-\nmance. The simple combination of features of news and ra-\ntionale yields a performance improvement, showing the use-\nfulness of our prompted rationales. SuperICL outperforms\nthe LLM-only method but fails to consistently outperform\nthe baseline SLM on the two datasets. We speculate that this\nis due to the complexity of our fake news detection task,\nwhere injecting prediction and confidence of an SLM does\nnot bring sufficient information. 4) We evaluate three abla-\ntion experiment groups to evaluate the effectiveness of dif-\nferent modules in the ARG network. From the result, we\ncan see that w/o LLM Judgement Predictor or w/o Ratio-\nnale Usefulness Evaluator both bring a significant decrease\nin ARG performance, highlighting the significance of these\ntwo structures. Besides, we found that even the weakest one\namong the variants of ARG still outperforms all other meth-\nods, which shows the importance of the news-rationale in-\nteraction structure we designed.\nResult Analysis\nTo investigate which part the additional gain of the ARG(-\nD) should be attributed to, we perform statistical analysis on\nthe additional correctly judged samples of ARG(-D) com-\npared with the vanilla BERT. From Figure 4, we observe\nthat: 1) The proportions of the overlapping samples between\nARG(-D) and the LLM are over 77%, indicating that the\nARG(-D) can exploit (and absorb) the valuable knowledge\nfor judgments from the LLM, even its performance is unsat-\nisfying. 2) The samples correctly judged by the LLM from\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22110\n77.9%\n20.4% 22.1%\n79.6%\n43.3% 45.3%\n15.5%\n20.9% 16.8%\n15.8%\n(a) right(ARG) – right(Baseline)\nLLM ARG\n✓ ✓\n✓ ✗\nTD CS\n✓ ✓\n✓ ✗\n✗ ✓\n(b) right(ARG- D) – right(Baseline)\nFigure 4: Statistics of additional correctly judged samples of\n(a) ARG and (b) ARG-D over the BERT baseline. right(·)\ndenotes samples correctly judged by the method(·). TD/CS:\nTextual description/commonsense perspective.\nP (0.23, 0.784)\nFigure 5: Performance as the shifting threshold changes.\nboth two perspectives contribute the most, suggesting more\ndiverse rationales may enhance the ARG(-D)’s training. 3)\n20.4% and 22.1% of correct judgments should be attributed\nto the model itself. We speculate that it produces some kinds\nof “new knowledge” based on the wrong judgments of the\ngiven knowledge.\nCost Analysis in Practice\nWe showcase a possible model-shifting strategy to balance\nthe performance and cost in practical systems. Inspired\nby Ma et al. (2023), we simulate the situation where we use\nthe more economic ARG-D by default but query the more\npowerful ARG for part of the data. As presented in Figure 5,\nby sending only 23% of the data (according to the confi-\ndence of ARG-D) to the ARG, we could achieve 0.784 in\nmacro F1, which is the same as the performance fully using\nthe ARG.\nRelated Work\nFake News Detection Fake news detection is generally for-\nmulated as a binary classification task between real and fake\nnews items. Research on this task could be roughly cate-\ngorized into two groups: social-context-based and content-\nbased methods. Methods in the first group aim at differ-\nentiating fake and real news during the diffusion proce-\ndure by observing the propagation patterns (Zhou and Za-\nfarani 2019), user feedback (Min et al. 2022), and so-\ncial networks (Nguyen et al. 2020). The second group fo-\ncuses on finding hints based on the given content, including\ntext (Przybyla 2020), images (Qi et al. 2021) and may re-\nquire extra assistance from knowledge bases (Popat et al.\n2018) and news environments (Sheng et al. 2022). Both two\ngroups of methods obtain textual representation from pre-\ntrained models like BERT as a convention but rarely con-\nsider its potential for fake news detection. We conducted an\nexploration in this paper by combining large and small LMs\nand obtained good improvement only using textual content.\nLLMs for Natural Language Understanding LLMs,\nthough mostly generative models, also have powerful nat-\nural language understanding (NLU) capabilities, especially\nin the few-shot in-context learning scenarios (Brown et al.\n2020). Recent works in this line focus on benchmarking the\nlatest LLM in NLU. Results show that LLMs may not have\ncomprehensive superiority compared with a well-trained\nsmall model in some types of NLU tasks (Zhong et al. 2023).\nOur results provide empirical findings in fake news detection\nwith only textual content as the input.\nConclusion and Discussion\nWe investigated if large LMs help in fake news detection\nand how to properly utilize their advantages for improving\nperformance. Results show that the large LM (GPT-3.5) un-\nderperforms the task-specific small LM (BERT), but could\nprovide informative rationales and complement small LMs\nin news understanding. Based on these findings, we designed\nthe ARG network to flexibly combine the respective advan-\ntages of small and large LMs and developed its rationale-free\nversion ARG-D for cost-sensitive scenarios. Experiments\nshowed the superiority of the ARG and ARG-D.\nDiscussion Our findings in fake news detection exemplify\nthe current barrier for LLMs to be competent in applica-\ntions closely related to the sophisticated real-world back-\nground. Though having superior analyzing capability, LLMs\nmay struggle to properly make full use of their internal ca-\npability. This suggests that “mining” their potential may re-\nquire novel prompting techniques and a deeper understand-\ning of its internal mechanism. We then identified the possi-\nbility of combining small and LLMs to earn additional im-\nprovement and provided a solution especially suitable for\nsituations where the better-performing models have to “se-\nlect good to learn” from worse ones. We expect our solution\nto be extended to other tasks and foster more effective and\ncost-friendly use of LLMs in the future.\nLimitations We identify the following limitations: 1) We\ndo not examine other well-known LLMs (e.g., Claude 7 and\nErnie Bot 8) due to the API unavailability for us when con-\nducting this research; 2) We only consider the perspectives\nsummarized from the LLM’s response and there might be\nother prompting perspectives based on a conceptualization\nframework of fake news; 3) Our best results still fall behind\nthe oracle voting integration of multi-perspective judgments\nin Table 4, indicating that rooms still exist in our line regard-\ning performance improvements.\n7https://claude.ai/\n8https://yiyan.baidu.com/\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22111\nAcknowledgements\nThe authors would like to thank the anonymous review-\ners for their insightful comments. This work is supported\nby the National Natural Science Foundation of China\n(62203425), the Zhejiang Provincial Key Research and De-\nvelopment Program of China (2021C01164), the Project\nof Chinese Academy of Sciences (E141020), the Post-\ndoctoral Fellowship Program of CPSF (GZC20232738)\n(GZC20232738) and the CIPSC-SMP-Zhipu.AI Large\nModel Cross-Disciplinary Fund. The corresponding author\nis Qiang Sheng.\nReferences\nAnthropic. 2023. Model Card and Evaluations for Claude\nModels. https://www-files.anthropic.com/production/\nimages/Model-Card-Claude-2.pdf. Accessed: 2023-08-13.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nAre Few-Shot Learners. In Advances in Neural Information\nProcessing Systems, 1877–1901. Curran Associates Inc.\nCaramancion, K. M. 2023. News Verifiers Showdown:\nA Comparative Performance Evaluation of ChatGPT 3.5,\nChatGPT 4.0, Bing AI, and Bard in News Fact-Checking.\narXiv preprint arXiv:2306.17176.\nCHEQ. 2019. The Economic Cost of Bad Actors\non the Internet. https://info.cheq.ai/hubfs/Research/THE\nECONOMIC COST Fake News final.pdf. Accessed: 2023-\n08-13.\nCui, J.; Kim, K.; Na, S. H.; and Shin, S. 2022. Meta-Path-\nbased Fake News Detection Leveraging Multi-level Social\nContext Information. In Proceedings of the 31st ACM Inter-\nnational Conference on Information & Knowledge Manage-\nment, 325–334. ACM.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. ACL.\nFisher, M.; Cox, J. W.; and Hermann, P. 2016. Pizzagate:\nFrom rumor, to hashtag, to gunfire in DC. The Washington\nPost.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distill-\ning the Knowledge in a Neural Network. arXiv preprint\narXiv:1503.02531.\nHu, B.; Sheng, Q.; Cao, J.; Zhu, Y .; Wang, D.; Wang, Z.;\nand Jin, Z. 2023. Learn over Past, Evolve for Future: Fore-\ncasting Temporal Trends for Fake News Detection. In Pro-\nceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 5: Industry Track), 116–\n125. ACL.\nHu, L.; Wei, S.; Zhao, Z.; and Wu, B. 2022a. Deep learning\nfor fake news detection: A comprehensive survey. AI Open,\n3: 133–155.\nHu, X.; Guo, Z.; Wu, G.; Liu, A.; Wen, L.; and Yu, P. 2022b.\nCHEF: A Pilot Chinese Dataset for Evidence-Based Fact-\nChecking. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 3362–\n3376. ACL.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\nBang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of Hal-\nlucination in Natural Language Generation. ACM Comput-\ning Surveys, 55: 1–38.\nKaliyar, R. K.; Goswami, A.; and Narang, P. 2021. Fake-\nBERT: Fake News Detection in Social Media with a BERT-\nbased Deep Learning Approach. Multimedia tools and ap-\nplications, 80(8): 11765–11788.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large Language Models are Zero-Shot Reason-\ners. In Advances in Neural Information Processing Systems,\nvolume 35, 22199–22213. Curran Associates, Inc.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2023a. Pre-train, prompt, and predict: A systematic sur-\nvey of prompting methods in natural language processing.\nACM Computing Surveys, 55(9): 1–35.\nLiu, Y .; Deng, G.; Xu, Z.; Li, Y .; Zheng, Y .; Zhang, Y .; Zhao,\nL.; Zhang, T.; and Liu, Y . 2023b. Jailbreaking ChatGPT via\nPrompt Engineering: An Empirical Study. arXiv preprint\narXiv:2305.13860.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv preprint arXiv:1907.11692.\nMa, Y .; Cao, Y .; Hong, Y .; and Sun, A. 2023. Large Lan-\nguage Model Is Not a Good Few-shot Information Extrac-\ntor, but a Good Reranker for Hard Samples! arXiv preprint\narXiv:2303.08559.\nMin, E.; Rong, Y .; Bian, Y .; Xu, T.; Zhao, P.; Huang, J.; and\nAnaniadou, S. 2022. Divide-and-Conquer: Post-User Inter-\naction Network for Fake News Detection on Social Media.\nIn Proceedings of the ACM Web Conference 2022, 1148–\n1158. ACM.\nMosallanezhad, A.; Karami, M.; Shu, K.; Mancenido, M. V .;\nand Liu, H. 2022. Domain Adaptive Fake News Detection\nvia Reinforcement Learning. In Proceedings of the ACM\nWeb Conference 2022, 3632–3640. ACM.\nMu, Y .; Bontcheva, K.; and Aletras, N. 2023. It’s about\nTime: Rethinking Evaluation on Rumor Detection Bench-\nmarks using Chronological Splits. In Findings of the Associ-\nation for Computational Linguistics: EACL 2023, 736–743.\nACL.\nNaeem, S. B.; and Bhatti, R. 2020. The COVID-19 ‘info-\ndemic’: a new front for information professionals. Health\nInformation & Libraries Journal, 37(3): 233–239.\nNan, Q.; Cao, J.; Zhu, Y .; Wang, Y .; and Li, J. 2021. MD-\nFEND: Multi-domain Fake News Detection. InProceedings\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22112\nof the 30th ACM International Conference on Information\nand Knowledge Management. ACM.\nNguyen, V .-H.; Sugiyama, K.; Nakov, P.; and Kan, M.-Y .\n2020. FANG: Leveraging Social Context for Fake News\nDetection Using Graph Representation. In Proceedings of\nthe 29th ACM International Conference on Information and\nKnowledge Management, 1165–1174. ACM.\nOpenAI. 2022. ChatGPT: Optimizing Language Models\nfor Dialogue. https://openai.com/blog/chatgpt/. Accessed:\n2023-08-13.\nPelrine, K.; Reksoprodjo, M.; Gupta, C.; Christoph, J.; and\nRabbany, R. 2023. Towards Reliable Misinformation Mit-\nigation: Generalization, Uncertainty, and GPT-4. arXiv\npreprint arXiv:2305.14928v1.\nPopat, K.; Mukherjee, S.; Yates, A.; and Weikum, G. 2018.\nDeClarE: Debunking Fake News and False Claims using\nEvidence-Aware Deep Learning. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Lan-\nguage Processing, 22–32. ACL.\nPrzybyla, P. 2020. Capturing the Style of Fake News. InPro-\nceedings of the AAAI Conference on Artificial Intelligence ,\nvolume 34, 490–497. AAAI Press.\nQi, P.; Cao, J.; Li, X.; Liu, H.; Sheng, Q.; Mi, X.; He, Q.;\nLv, Y .; Guo, C.; and Yu, Y . 2021. Improving Fake News\nDetection by Using an Entity-enhanced Framework to Fuse\nDiverse Multimodal Clues. In Proceedings of the 29th ACM\nInternational Conference on Multimedia, 1212–1220. ACM.\nRamlochan, S. 2023. Role-Playing in Large Language Mod-\nels like ChatGPT. https://www.promptengineering.org/role-\nplaying-in-large-language-models-like-chatgpt/. Accessed:\n2023-08-13.\nRoth, Y . 2022. The vast majority of content we take ac-\ntion on for misinformation is identified proactively. https:\n//twitter.com/yoyoel/status/1483094057471524867. Ac-\ncessed: 2023-08-13.\nSheng, Q.; Cao, J.; Zhang, X.; Li, R.; Wang, D.; and Zhu, Y .\n2022. Zoom Out and Observe: News Environment Percep-\ntion for Fake News Detection. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), 4543–4556. ACL.\nSheng, Q.; Zhang, X.; Cao, J.; and Zhong, L. 2021. Inte-\ngrating pattern-and fact-based fake news detection via model\npreference learning. In Proceedings of the 30th ACM inter-\nnational conference on information & knowledge manage-\nment, 1640–1650. ACM.\nShu, K.; Cui, L.; Wang, S.; Lee, D.; and Liu, H. 2019. dE-\nFEND: Explainable Fake News Detection. InProceedings of\nthe 25th ACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, 395–405. ACM.\nShu, K.; Mahudeswaran, D.; Wang, S.; Lee, D.; and Liu,\nH. 2020. FakeNewsNet: A Data Repository with News\nContent, Social Context and Spatiotemporal Information for\nStudying Fake News on Social Media.Big data, 8: 171–188.\nShu, K.; Sliva, A.; Wang, S.; Tang, J.; and Liu, H. 2017. Fake\nnews detection on social media: A data mining perspective.\nACM SIGKDD Explorations Newsletter, 19: 22–36.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv preprint arXiv:2302.13971.\nWang, Y .; Ma, F.; Jin, Z.; Yuan, Y .; Xun, G.; Jha, K.; Su, L.;\nand Gao, J. 2018. EANN: Event Adversarial Neural Net-\nworks for Multi-Modal Fake News Detection. In Proceed-\nings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 849–857. ACM.\nWei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Met-\nzler, D.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.;\nDean, J.; and Fedus, W. 2022a. Emergent Abilities of Large\nLanguage Models. Transactions on Machine Learning Re-\nsearch.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E.; Le, Q. V .; and Zhou, D. 2022b. Chain-\nof-Thought Prompting Elicits Reasoning in Large Language\nModels. In Advances in Neural Information Processing Sys-\ntems, volume 35, 24824–24837. Curran Associates, Inc.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu,\nJ.; Xu, C.; Le Scao, T.; Gugger, S.; Drame, M.; Lhoest, Q.;\nand Rush, A. 2020. Transformers: State-of-the-Art Natural\nLanguage Processing. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Process-\ning: System Demonstrations, 38–45. Online: ACL.\nXu, C.; Xu, Y .; Wang, S.; Liu, Y .; Zhu, C.; and McAuley, J.\n2023. Small Models are Valuable Plug-ins for Large Lan-\nguage Models. arXiv preprint arXiv:2305.08848.\nZhang, X.; Cao, J.; Li, X.; Sheng, Q.; Zhong, L.; and Shu,\nK. 2021. Mining Dual Emotion for Fake News Detection. In\nProceedings of the web conference 2021, 3465–3476. ACM.\nZhang, Y .; Li, Y .; Cui, L.; Cai, D.; Liu, L.; Fu, T.; Huang, X.;\nZhao, E.; Zhang, Y .; Chen, Y .; Wang, L.; Luu, A. T.; Bi, W.;\nShi, F.; and Shi, S. 2023. Siren’s Song in the AI Ocean: A\nSurvey on Hallucination in Large Language Models. arXiv\npreprint arXiv:2309.01219.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y .; Yang, C.;\nChen, Y .; Chen, Z.; Jiang, J.; Ren, R.; Li, Y .; Tang, X.; Liu,\nZ.; Liu, P.; Nie, J.-Y .; and Wen, J.-R. 2023. A Survey of\nLarge Language Models. arXiv preprint arXiv:2303.18223.\nZhong, Q.; Ding, L.; Liu, J.; Du, B.; and Tao, D.\n2023. Can ChatGPT Understand Too? A Comparative\nStudy on ChatGPT and Fine-tuned BERT. arXiv preprint\narXiv:2302.10198.\nZhou, X.; and Zafarani, R. 2019. Network-Based Fake News\nDetection: A Pattern-Driven Approach. ACM SIGKDD Ex-\nplorations Newsletter, 21(2): 48–60.\nZhu, Y .; Sheng, Q.; Cao, J.; Li, S.; Wang, D.; and Zhuang,\nF. 2022. Generalizing to the Future: Mitigating Entity Bias\nin Fake News Detection. In Proceedings of the 45th Inter-\nnational ACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, 2120–2125. ACM.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n22113",
  "topic": "Fake news",
  "concepts": [
    {
      "name": "Fake news",
      "score": 0.715219259262085
    },
    {
      "name": "Computer science",
      "score": 0.4670623242855072
    },
    {
      "name": "Psychology",
      "score": 0.4174034297466278
    },
    {
      "name": "Internet privacy",
      "score": 0.4171968102455139
    },
    {
      "name": "Data science",
      "score": 0.3319941759109497
    },
    {
      "name": "Sociology",
      "score": 0.3225420117378235
    }
  ]
}