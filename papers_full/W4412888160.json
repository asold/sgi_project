{
    "title": "GNN-RAG: Graph Neural Retrieval for Efficient Large Language Model Reasoning on Knowledge Graphs",
    "url": "https://openalex.org/W4412888160",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A3049394989",
            "name": "Costas Mavromatis",
            "affiliations": [
                "University of Minnesota System"
            ]
        },
        {
            "id": "https://openalex.org/A219814910",
            "name": "George Karypis",
            "affiliations": [
                "University of Minnesota System"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6631349028"
    ],
    "abstract": null,
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2025, pages 16682–16699\nJuly 27 - August 1, 2025 ©2025 Association for Computational Linguistics\nGNN-RAG: Graph Neural Retrieval for Efficient Large Language Model\nReasoning on Knowledge Graphs\nCostas Mavromatis\nUniversity of Minnesota\nmavro016@umn.edu\nGeorge Karypis\nUniversity of Minnesota\nkarypis@umn.edu\nAbstract\nRetrieval-augmented generation (RAG) in\nKnowledge Graph Question Answering\n(KGQA) enhances the context of Large\nLanguage Models (LLMs) by incorporating\ninformation retrieved from the Knowledge\nGraph (KG). Most recent approaches rely\non costly LLM calls to generate executable\nrelation paths or traverse the KG, which is\ninefficient in complex KGQA tasks, such\nas those involving multi-hop or multi-entity\nquestions. We introduce the GNN-RAG\nframework, which utilizes lightweight Graph\nNeural Networks (GNNs) for effective and\nefficient graph retrieval. The GNN learns to\nassign importance weights to nodes based\non their relevance to the question, as well\nas the relevance of their neighboring nodes.\nThis enables the framework to effectively\nhandle context from distant nodes in the graph,\nimproving retrieval performance. GNN-RAG\nretrieves the shortest paths connecting question\nentities to GNN answer candidates, providing\nthis information as context for the LLM.\nExperimental results show that GNN-RAG\nachieves effective retrieval on two widely\nused KGQA benchmarks (WebQSP and\nCWQ), outperforming or matching GPT-4\nperformance with a 7B tuned LLM. Ad-\nditionally, GNN-RAG excels on multi-hop\nand multi-entity questions outperforming\nLLM-based retrieval approaches by 8.9–15.5%\npoints at answer F1. Furthermore, it surpasses\nlong-context inference while using 9×\nfewer KG tokens. The code is provided in\nhttps://github.com/cmavro/GNN-RAG.\n1 Introduction\nLarge Language Models (LLMs) (Brown et al.,\n2020; Bommasani et al., 2021; Chowdhery et al.,\n2023) are the state-of-the-art models in many NLP\ntasks due to their remarkable ability to understand\nnatural language. The power of LLM comes from\npretraining in a large corpus of textual data to ob-\nMulti-Hop Multi-Entity\n40\n60\n80\n41.1\n46.9\n61.3 61.7\n70.2\n77.2\nAnswer F1 (%)\nNo RAG +KG-RAG +GNN-RAG\nFigure 1: Retrieval effect on multi-hop/entity KGQA.\nOur GNN-RAG outperforms existing KG-RAG meth-\nods by 8.9–15.5% points at F1 (ref: Table 2).\ntain general human knowledge (Kaplan et al., 2020;\nHoffmann et al., 2022). However, because pre-\ntraining is costly and time-consuming (Gururangan\net al., 2020), LLMs cannot easily adapt to new or\nin-domain knowledge and are prone to hallucina-\ntions (Zhang et al., 2023b).\nKnowledge Graphs (KGs) (Vrande ˇci´c and\nKrötzsch, 2014) store information in structured\nform that can be easily updated. KGs represent\nhuman-crafted factual knowledge in the form of\ntriplets (head, relation, tail) , e.g., <Jamaica →\nlanguage_spoken → English>, which collec-\ntively form a graph. In the case of KGs, the stored\nknowledge is updated by fact addition or removal.\nAs KGs capture complex interactions between\nstored entities, e.g. multi-hop relations, they are\nwidely used for knowledge-intensive tasks, such as\nQuestion Answering (QA) (Pan et al., 2024) with\nRetrieval-Augmented Generation (RAG) (Lewis\net al., 2020).\nRAG performance is highly dependent on the\nKG facts that are retrieved (Wu et al., 2023). The\nchallenge in KGQA is that KGs store complex\ngraph information, usually consisting of millions\nof facts, and retrieving the right information re-\nquires effective graph processing (Mavromatis and\nKarypis, 2022). Retrieval methods that rely on\noff-the-shelf NLP retrievers (Baek et al., 2023) or\nclassical graph algorithms (He et al., 2024) are lim-\n16682\nited as retrieval is not tailored for KGQA. Most\nrecent methods rely on LLMs for semantic pars-\ning (Luo et al., 2024a) or for traversing the KG (Sun\net al., 2024). However, LLMs can be ineffiecient\nin complex KGQA tasks, such as those involving\nmulti-hop or multi-entity questions. In such cases,\nleveraging context from distant nodes in the graph\nis crucial, and LLMs may struggle to process the\ncontext of the graph which expands exponentially\nat deeper levels (Liu et al., 2024a).\nTo address retrieval efficiency in complex\nKGQA, we present GNN-RAG, a graph neural\nframework which utilizes context from deeper parts\nof the graph, such as information from distant\nnodes, during retrieval. GNN-RAG relies on Graph\nNeural Networks (GNNs) (He et al., 2021; Mavro-\nmatis and Karypis, 2022), which are powerful\ngraph representation learners, able to handle the\ncomplex graph information stored in the KG. GNN-\nRAG consists of a graph neural phase, where the\nGNN learns to embed KG-specific semantics and\nto identify relevant nodes, given a question. In\nGNN-RAG, we implement deep (multi-layer) GNN\nmodels to leverage graph context. The GNN learns\nto assign importance weights to nodes based on\ntheir relevance to the question, as well as the rel-\nevance of their neighboring nodes. This enables\nthe framework to effectively handle context from\ndeeper parts of the graph, improving retrieval per-\nformance. At retrieval, the GNN scores answer\ncandidates for the given question, and the short-\nest paths that connect question entities and an-\nswer candidates are retrieved, which are verbalized\nand given as context to the LLM. Experimental re-\nsults show GNN-RAG’s superiority over competing\nRAG-based systems for KGQA by outperforming\nthem by up to 15.5% points at complex KGQA\nperformance (Figure 1). Furthermore, we show\nthe effectiveness of GNN-RAG in retrieval augmen-\ntation, while outperforming long-context retrieval\nusing 9×fewer KG tokens. Our contributions are\nsummarized below:\n• Framework: GNN-RAG utilizes GNN mod-\nels to leverage context from distant nodes in\nthe graph, while the LLM leverages its natu-\nral language processing ability for ultimate\nKGQA. Additionally, we propose retrieval\naugmentation and routing techniques that uti-\nlize GNN-RAG to enhance overall end-to-end\nefficiency.\n• Effectiveness: GNN-RAG achieves effec-\ntive performance on two widely used KGQA\nbenchmarks (WebQSP and CWQ), improving\ncomplex question answering by 8.9–15.5%\npoints at F1 (Figure 1).\n• Efficiency: GNN-RAG improves vanilla\nLLMs on KGQA performance without incur-\nring additional LLM calls as previous state-of-\nthe-art RAG systems for KGQA require. In\naddition, GNN-RAG outperforms or matches\nGPT-4 performance with a 7B tuned LLM,\nwhile outperforming long-context retrieval us-\ning 9×fewer KG tokens.\n2 Related Work\nKGQA Methods. KGQA methods fall into two\ncategories (Lan et al., 2022): (i) semantic parsing\n(SP) methods and (ii) information retrieval (IR)\nmethods. SP methods (Sun et al., 2020; Lan and\nJiang, 2020; Ye et al., 2022) learn to transform\nthe given question into a query of logical form,\ne.g., SPARQL query. The transformed query is\nthen executed over the KG to obtain the answers.\nHowever, SP methods require ground-truth logical\nqueries for training, which are time-consuming to\nannotate in practice, and may lead non-executable\nqueries due to syntactical or semantic errors (Das\net al., 2021; Yu et al., 2022). IR methods (Sun\net al., 2018, 2019; Zhang et al., 2022b) focus on\nthe weakly-supervised KGQA setting, where only\nquestion-answer pairs are given for training. IR\nmethods retrieve KG information, e.g., a KG sub-\ngraph (Zhang et al., 2022a), which is used as input\nduring KGQA reasoning. GNN-RAG falls in the IR\ncategory.\nGNNs & LMs. Combining GNNs with LMs has\nbeen the subject of a substantial body of existing lit-\nerature (Jin et al., 2023), with various applications\nranging from QA (Yasunaga et al., 2021; Wang\net al., 2021; Zhang et al., 2022c; Christmann et al.,\n2023; Tian et al., 2024; He et al., 2024; Zhang\net al., 2024a) to training LMs on graphs (Zhao\net al., 2022; Yasunaga et al., 2022; Huang et al.,\n2024). Such approaches seek to combine the nat-\nural language and graph reasoning into a single\nmodel by fusing latent GNN information with the\nLM. However, due to the modality mismatch of\nGNNs and LMs, fusing graph and natural language\ninformation is challenging for many knowledge-\nintensive tasks, even in supervised settings (Mavro-\nmatis et al., 2024). To alleviate this challenge,\nGNN-RAG divides KGQA in two stages. The GNN\n16683\nfirst retrieves useful information from the graph\nmodality, which is then converted into natural lan-\nguage for effective LLM reasoning.\nGraphRAG. GraphRAG usually refers to the\ngeneral approach of inserting verbalized graph in-\nformation at the context of LLMs (Peng et al., 2024;\nWei et al., 2024) or leveraging additional graph in-\nformation when retrieving context for RAG (Edge\net al., 2024; Gutiérrez et al., 2024). For instance,\nverbalizing graph information obtained by KGs has\nbeen widely applied in GraphRAG (Xie et al., 2022;\nBaek et al., 2023; Jiang et al., 2023a; Jin et al.,\n2024; Liu et al., 2024b; Zhao et al., 2024; Li et al.,\n2024a; Luo et al., 2024b). However, GraphRAG\nperformance downgrades when the graph informa-\ntion retrieved is noisy and irrelevant to the ques-\ntion (Wu et al., 2023; He et al., 2024). To improve\nretrieval in KGQA, GNN-RAG employs a graph\nneural framework, which tailors graph retrieval for\nthe KG at hand. By optimizing GNNs to iden-\ntify the right graph information for answering the\nquestions, GNN-RAG achieves superior retrieval\nperformance compared to existing approaches in\nKGQA.\n3 Problem Statement & Background\nKGQA. We are given a KG Gthat contains facts\nrepresented as (v,r,v ′), where vdenotes the head\nentity, v′denotes the tail entity, and ris the corre-\nsponding relation between the two entities. Given\nGand a natural language question q, the task of\nKGQA is to extract a set of entities {aq}∈G that\ncorrectly answer q. Following previous works (Lan\net al., 2022), question-answer pairs are given for\ntraining, but not the ground-truth paths that lead to\nthe answers.\nAs KGs usually contain millions of facts and\nnodes, a smaller question-specific subgraph Gq is\nretrieved for a question q. Entity linking identi-\nfies question entities {eq}and subgraph Gq is ex-\ntracted by following connections that start from\nthese question entities (Yih et al., 2015). Ideally,\nall correct answers for the question are contained in\nthe retrieved subgraph, {aq}∈G q. The retrieved\nsubgraph Gq along with the question qare used as\ninput to a KGQA model, which outputs the correct\nanswer(s). The prevailing KGQA methods studied\nare GNNs and LLMs.\nGNNs. KGQA can be regarded as a node classi-\nfication problem, where KG entities are classified\nas answers vs. non-answers for a given question.\nGNNs (Kipf and Welling, 2016; Veliˇckovi´c et al.,\n2017; Schlichtkrull et al., 2018) are powerful graph\nrepresentation learners suited for tasks such as node\nclassification. GNNs update the representation h(l)\nv\nof node vat layer lby aggregating messages m(l)\nvv′\nfrom each neighborv′. During KGQA, the message\npassing is also conditioned to the given question\nq (He et al., 2021). For readability purposes, we\npresent the following GNN update for KGQA,\nh(l)\nv = ψ\n(\nh(l−1)\nv ,\n∑\n(v,r,v′)∈Nv\nω(q,r) ·m(l)\nvv′\n)\n, (1)\nwhere function ω(·) is typically a LM that mea-\nsures how relevant relation r of fact (v,r,v ′) is\nto question q. Nv denotes the set of neighboring\ntriplets of node v. Neighbor messages m(l)\nvv′ are\naggregated by a sum-operator ∑ and function ψ(·)\ncombines representations from consecutive GNN\nlayers.\nPrevious GNN approaches perform query-to-KG\nsimilarity using pretrained encoders (function ωin\nEquation 1). However, such encoder models are\nnot as accurate as LLMs. GNN-RAG addresses this\nlimitation by employing an LLM as a post-retrieval\nstep to accurately select the relevant KG evidence.\nThe importance of GNN-RAG’s framework is ex-\ntensively validated in Section 6.\nLLM RAG . Retrieval-Augment Generation\n(RAG) is a method aiming to reduce LLM halluci-\nnations (Lewis et al., 2020). Given a query q, RAG\nretrieves relevant information (e.g, documents from\nthe given corpus), which is inserted as additional\ncontext cto the LLM’s input. In KGs, the context c\nconsists of graph information relevant to the ques-\ntion, such KG triplets, paths, or subgraphs. The\nretrieved graph information is first converted into\nnatural language so that it can be processed by the\nLLM. The input given to the LLM contains the\nKG factual information along with the question\nand a prompt. For instance, the input becomes\n“Knowledge: Jamaica →language_spoken →\nEnglish \\n Question: Which language do\nJamaican people speak?”, where the LLM has\naccess to KG information for answering the ques-\ntion.\nLandscape of KGQA methods . Figure 2\npresents the landscape of existing KGQA meth-\nods with respect to KG retrieval and reasoning.\nGNN-based methods, such as GraftNet (Sun et al.,\n2018), NSM (He et al., 2021), and ReaRev (Mavro-\nmatis and Karypis, 2022), reason over a dense KG\n16684\nGNN \nDense \nretrieval\nQuestion Question Prompt: \"Which of the\nfollowing relations are\nrelevant? \"\nLLM LLM \nPrompt: \"Which of the\nfollowing relations are\nrelevant? \"\nQuestion\nLLM \nQuestion LLM \nPrompt: \"Generate\nhelpful relation paths. \"\nPrompt: \"Given the retrieved knowledge,\ncan you answer the question? \" Question\nLLM Answer Answer Answer\nPrompt: \"Given the retrieved knowledge,\ncan you answer the question? \"\nRetrievalReasoning\nTextualize + RAG Textualize + RAG\nGNN-based KGQA LLM-based KGQA (ToG) LLM-based KGQA (RoG)\nFigure 2: The landscape of existing KGQA methods. GNN-based methods reason on dense subgraphs as they can\nhandle complex and multi-hop graph information. LLM-based methods employ the same LLM for both retrieval\nand reasoning due to its ability to understand natural language.\nsubgraph leveraging the GNN’s ability to handle\ncomplex graph information. Recent LLM-based\nmethods leverage the LLM’s power for both re-\ntrieval and reasoning (Gu et al., 2023). For instance,\nToG (Sun et al., 2024) uses the LLM to retrieve rel-\nevant facts hop-by-hop. RoG (Luo et al., 2024a)\nuses the LLM to generate plausible relation paths\nwhich are then queried on the KG to retrieve the\nrelevant information, similar to semantic parsing.\nToG utilizes an LLM to selectively expand and then\nprune the graph context. Meanwhile, RoG does not\nrely on any graph context. As a result, LLM-based\napproaches may overlook or incorrectly prune im-\nportant graph information for KGQA tasks.\n4 G NN-RAG\nWe present GNN-RAG, a new graph neural retrieval\nmethod for KGQA that leverages GNNs to improve\nretrieval performance when questions require com-\nplex graph information. GNN-RAG’s primary con-\ntribution lies in its use of GNNs for efficient graph\nretrieval, distinguishing it from previous methods\nthat rely on long-context retrieval based on text\nsimilarity or retrieval via LLMs. We provide the\noverall framework at inference time in Figure 3.\nFirst, the GNN processes a dense subgraph to uti-\nlize deep graph context and identify relevant an-\nswer nodes for a given question. Second, the short-\nest paths in the KG that connect question entities\nand GNN-based answers are extracted to represent\nuseful KG reasoning paths. The extracted paths are\nverbalized and given as context for LLM reasoning\nvia RAG. In our GNN-RAG framework, the GNN\nacts as a dense subgraph processor, while the LLM\nleverages its natural language processing ability for\ndownstream KGQA.\n4.1 GNN\nIn order to retrieve high-quality reasoning paths,\nwe prefer deep GNNs over other shallow KGQA\nmethods, e.g., embedding-based methods (Saxena\net al., 2020), due to their ability to handle deep\ngraph interactions. GNNs mark themselves as good\ncandidates for retrieval due to their architectural\nbenefit of exploring diverse reasoning paths (Choi\net al., 2024) that result in high answer recall.\nGNNs consist of Lupdates via Equation 1 (Lis\nhyperparameter), where the node embbeddings in\nthe subgraph Gq are updated to h(L)\nv ∈Rd, where d\nis the embedding dimension. In our framework, at\neach layer, the GNN assigns importance to nodes\nbased on their likelihood of forming a path to the\nanswer. We use their KG relations to assess their\nrelevance to the question. Unlike LLM-based KG\ntraversal, the GNN learns to aggregate informa-\ntion relevant to the question within the embedding\nspace.\nAt layer l+ 1, the GNN aggregates relation em-\nbeddings r,r ∈Nv, where Nv is the set of neigh-\nboring triplets of node v, based on their relevance\nto the question as\n∑\n(v,r,v′)∈Nv\np(l)\nv′ ·σ(qk ⊙r). (2)\nIn Equation 2, qk is a question embedding, ⊙is the\npairwise multiplication, σ(·) is a ReLU activation,\nand p(l)\nv′ is the node importance weight. Message\nmvv′ = σ(qk ⊙r) is activated if the corresponding\nrelation is relevant to the question, e.g., relation\n16685\nQ: \"Which language do Jamaican people speak?\"\nA: English, Jamaican English\nDense retrieval \nfor GNN GNN A: English, Jamaican English,\nFrench, Caribbean\nShortest\nPaths\nTextualize + RAG\nJamaica -> ofﬁcial_language -> English\nJamaica -> language_spoken -> Jamaican English\nJamaica -> close_to -> Haiti -> ofﬁcial_language -> French\nJamaica -> located_in -> Caribbean Sea\n\"Which language do Jamaican people speak?\"\nLLM A: English, Jamaican English\nRetrievalReasoning\nReasoning \nG NN-R AG\nLLM\nPrompt: \"Generate helpful relation paths.\"\n+RA \nUnion\nFigure 3: GNN-RAG: The GNN reasons over a dense subgraph to retrieve candidate answers, along with the\ncorresponding reasoning paths (shortest paths from question entities to answers). The retrieved reasoning paths\n–optionally combined with retrieval augmentation (RA)– are verbalized and given to the LLM for RAG.\n‘language_spoken’ is relevant to question ‘Which\nlanguage do Jamaican people speak?’.\nAs complex questions consist of multiple sub-\nquestions, we obtain Kdifferent question embed-\ndings qk,k ∈K, that capture different question\nparts (Qiu et al., 2020). Question embeddings qk\nand KG relation embeddings r are encoded via a\nshared pretrained LM (Jiang et al., 2023b), such as\nSBERT (Reimers and Gurevych, 2019). We obtain\nqk = γk\n(\nLM(q)\n)\n, r = γc\n(\nLM(r)\n)\n, (3)\nwhere γk is an attention-based pooling neural net-\nwork that attends to question tokens, and γc is the\n[CLS] token pooling. We provide the implementa-\ntion of these networks in Appendix A.\nWe implement a multi-head GNN layer which\naggregates neighboring messages based on differ-\nent question parts as\nh(l+1)\nv = ψ\n(\nh(l)\nv ,\n⏐⏐⏐⏐K\nk=1\n∑\n(v,r,v′)∈Nv\np(l)\nv′ ·σ(qk⊙r)\n)\n.\n(4)\nOperation ||K\nk=1 denotes the concatenation of K\nheads, and ψ(·) : R(K+1)d − →Rd is a multilayer\nperceptron that combines KG semantics relevant to\nthe question in the embedding space from consecu-\ntive layers.\nIn Equation 4, the importance of each node is\nmeasured by p(l)\nv ∈[0,1], which is the probability\nof visiting a node at layerl. In the first layer,p(0)\nv =\n1 if node vis a question entity v∈{eq}and p(0)\nv =\n0, otherwise. At each layer, the probability vector\nis updated based on the GNN node embeddings\nfollowed by a softmax(·) operation as\np(l+1) = softmax(H(l)w), (5)\nwhere w ∈Rd is a learnable vector. The GNN de-\nscribed above incorporates deep graph information\nup to Lhops from the question entities. At each\nlayer, it assigns importance to nodes based on their\nrelations relevance to the question, as well as the\nprobability of visiting their neighbor node in the\nprevious layer. This approach parallels LLM-based\nKG traversal (Sun et al., 2024), where, starting\nfrom the question entities, the LLM iteratively se-\nlects relations and neighboring nodes to explore.\nFurthermore, to enable iterative KG traversal we\nreset the probability vector p(l) by\np(l) =\n{\np(0) if l= L\n2\np(l) else. (6)\nHere, we impose the constraint thatLmust be even.\nAs shown in Equation 6, this mechanism allows us\nto re-evaluate node importance using deep graph\nembeddings H( L\n2 ), effectively restarting the rea-\nsoning process from the question entities (Mavro-\nmatis and Karypis, 2022). By default, we imple-\nment L= 6GNN layers, so the reasoning process\nrestarts from the question entities at layer l = 3.\nThis approach ensures that node embeddings cap-\nture deep graph information, enabling a revisited\nand refined KG traversal. Details on node embed-\nding initialization can be found in Appendix A.\n4.2 GNN Optimization and RAG\nGiven training question-answer pairs, the GNN is\ntrained via node classification, where nodes have\n16686\nlabel yv = 1if they belong to the answer set v ∈\n{aq}and yv = 0, otherwise. The GNN parameters\nare optimized via the KL-divergence loss so that\np(L)\nv is close to 1 if v∈{aq}, and zero otherwise.\nDuring inference, nodes with the highest proba-\nbility scores are identified as candidate answers. To\nfilter out noisy information, we select nodes whose\ncumulative probability exceeds a threshold of 0.95.\nSubsequently, the shortest paths connecting the\nquestion entities to the candidate answers (i.e., rea-\nsoning paths) are extracted. These reasoning paths\nare then verbalized using a predefined template and\nserve as input for the LLM-based RAG process.\nFor the downstream LLM, we opt to follow\nKGQA prompt tuning (Lin et al., 2023; Zhang et al.,\n2024b), where the Llama-7B-Chat model is trained\nto generate a list of answers for KGQA (Luo et al.,\n2024a), given the input reasoning paths. We pro-\nvide the details in Appendix A.\n4.3 G NN-RAG Augmentation and Routing\nGNN-RAG employs GNNs that are specialized in\nretrieving context from deeper parts of the graphs,\nsuch as context from distant nodes in the graph.\nHowever, the effectiveness of GNNs depends on\nthe training data, and they may not always gener-\nalize well to unseen contexts. In contrast, retrieval\nbased on natural language, such as using text em-\nbedding models or LLMs, leverages the pretraining\ndata of these models, enabling better generalization.\nTo address the generalization limitation of GNNs,\nwe propose the following two techniques.\nGNN-RAG+RA. First, retrieval augmentation\n(RA) combines information retrieved from KGs\nusing different approaches to enhance diversity\nand improve answer recall. GNN-RAG+RA com-\nbines GNN-RAG with LLM-based semantic pars-\ning, which specialized in question-relation match-\ning. We use RoG (Luo et al., 2024a) as the semantic\nparsing approach, which fine-tunes a 7B LLM to\ngenerate executable relation paths given a ques-\ntion. During inference, we take the union of the\nreasoning paths retrieved by the two retrievers.\nGNN-RAG+Route. Second, text-based ap-\nproaches (Baek et al., 2023) retrieve a large number\nof KG triplets by using text embedding similarity\nto the input question, subsequently utilizing long-\ncontext LLMs for KGQA. With access to a more\nextensive graph context, the LLM can provide more\naccurate responses. GNN-RAG+Route integrates\nGNN-RAG for fetching relevant graph information\nthat may not be captured through long-context re-\ntrieval alone. Specifically, the context retrieved by\nGNN-RAG is fed into the downstream LLM to gen-\nerate answers. If any of the generated responses\nare not included in the long-context, e.g., in the top-\nk = 100 triplets retrieved by text similarity, this\nindicates that GNN-RAG has provided additional\ndeep graph context that is beneficial for answer-\ning the question. In contrast, when the answers\nalign with the long-context information, the sys-\ntem routes the inference to long-context process-\ning, thereby capitalizing on the increased graph\ndata. For text embedding retrieval, we employ the\nSubgraphRAG method (Li et al., 2024b) which\nuses training data to fine-tune the retriever.\n5 Experimental Setup\nKGQA Datasets . We experiment with widely\nused KGQA benchmarks: WebQuestionsSP (We-\nbQSP) (Yih et al., 2015), Complex WebQues-\ntions 1.1 (CWQ) (Talmor and Berant, 2018), and\nMetaQA-3 (Zhang et al., 2018). WebQSP contains\n4,737 natural language questions that are answer-\nable using a subset Freebase KG (Bollacker et al.,\n2008). The questions require up to 2-hop reason-\ning within this KG. CWQ contains 34,699 total\ncomplex questions that require up to 4-hops of rea-\nsoning over the KG. MetaQA-3 consists of 3-hop\nquestions in the domain of WikiMovies (Miller\net al., 2016). We provide the detailed dataset statis-\ntics in Appendix C.\nImplementation & Evaluation. For subgraph\nretrieval, we use the linked entities and the pager-\nank algorithm to extract dense graph informa-\ntion (He et al., 2021). The default GNN imple-\nmentation is to use L = 6layers, K = 3decom-\nposed question embeddings, and iterative reasoning\n(Equation 6). For GNN training, we employ stan-\ndard training procedure and hyperparameters as in\nthe literature (Mavromatis and Karypis, 2022). As\nthe defualt downstream LLM, we use the Llama2-\nChat-7B model finetuned for KGQA (Luo et al.,\n2024a). For KGQA evaluation, we adopt Hit and\nF1 metrics. Hit measures if any of the true answers\nis found in the generated response, which is typ-\nically employed when evaluating LLMs. Hit is\nbased on exact match (Sun et al., 2024) rather than\nfuzzy matching as in Tan et al. (2023). F1 penalizes\nincorrect or missing answers. We also report H@1,\nwhich is the accuracy of the first predicted answer.\nFor retrieval evaluation, we use Hit@k, which eval-\nuates whether a correct answer is retrieved in the\n16687\nTable 1: Performance comparison of different methods\non the two KGQA benchmarks.\nType Method WebQSP CWQ\nHit F1 Hit F1\nEmbedding\nKV-Mem (Miller et al., 2016) 46.7 38.6 21.1 –EmbedKGQA (Saxena et al., 2020) 66.6 – – –TransferNet (Shi et al., 2021) 71.4 – 48.6 –Rigel (Sen et al., 2021) 73.3 – 48.7 –\nGNN\nGraftNet (Sun et al., 2018) 66.7 62.4 36.8 32.7PullNet (Sun et al., 2019) 68.1 – 45.9 –NSM (He et al., 2021) 68.7 62.8 47.6 42.4SR+NSM(+E2E) (Zhang et al., 2022a) 69.5 64.1 50.2 47.1NSM+h (He et al., 2021) 74.3 67.4 48.8 44.0SQALER (Atzeni et al., 2021) 76.1 – – –UniKGQA (Jiang et al., 2023b) 77.2 72.2 51.2 49.1ReaRev (Mavromatis and Karypis, 2022) 76.4 70.9 52.9 47.8ReaRev + LMSR 77.5 72.853.3 49.7\nLLM\nFlan-T5-xl (Chung et al., 2024) 31.0 – 14.7 –Alpaca-7B (Taori et al., 2023) 51.8 – 27.4 –LLaMA2-Chat-7B (Touvron et al., 2023) 64.4 – 34.6 –ChatGPT 66.8 – 39.9 –ChatGPT+CoT 75.6 – 48.9 –\nKG+LLM\nKAPING (Baek et al., 2023) 73.9 – – –KD-CoT (Wang et al., 2023) 68.6 52.5 55.7 –StructGPT (Jiang et al., 2023a) 72.6 – – –KB-BINDER (Li et al., 2023) 74.4 – – –ToG+Llama2-70B (Sun et al., 2024) 68.9 – 57.6 –ToG+ChatGPT (Sun et al., 2024) 76.2 – 58.9 –ToG+GPT-4 (Sun et al., 2024) 82.6 –69.5 –RoG-7B (Luo et al., 2024a) 85.770.8 62.6 56.2\nGNN+LLMG-Retriever (He et al., 2024) 70.1 – – –GNN-RAG 85.771.3 66.8 59.4GNN-RAG+RA 90.7 73.5 68.760.4\nKG-LCSubgraphRAG (Li et al., 2024b) 89.4 – 68.6 –GNN-RAG+Route 90.1 – 72.4 –GNN-RAG+RA+Route 91.0∗ – 73.3∗ –\nWe denote thebestandsecond-bestmethods, as well as the best∗method withlong-context (KG-LC).GNN-RAG, RoG, KD-CoT, and G-Retriever use 7B fine-tuned Llama2 models. KD-CoT employs ChatGPT as well. For KG-LC, methods use Llama-3.1-8B.\ntop-kretrieved nodes. Further experimental setup\ndetails are provided in Appendix C.\nCompeting Methods. We compare with SOTA\nGNN and LLM methods for KGQA (Mavromatis\nand Karypis, 2022; Li et al., 2023). We also include\nearlier embedding-based methods (Saxena et al.,\n2020) as well as zero-shot/few-shot LLMs (Taori\net al., 2023). We do not compare with seman-\ntic parsing methods (Yu et al., 2022; Gu et al.,\n2023) that use ground-truth SPARQL annotations\nfor training, which are difficult to obtain in practice.\nFurthermore, we compare GNN-RAG with LLM-\nbased retrieval (Luo et al., 2024a; Sun et al., 2024)\nand long-context (Li et al., 2024b) approaches in\nterms of efficiency and effectiveness.\n6 Results\nMain Results. Table 1 presents performance re-\nsults of different KGQA methods. The results show\nthat equipping LLMs with GNN-based retrieval en-\nhances KGQA performance compared to previous\napproaches (GNN+LLM vs. KG+LLM). Specif-\nically, GNN-RAG+RA outperforms RoG by 5.0–\n6.1% points at Hit, while it outperforms or matches\nTable 2: Performance analysis on multi-hop (hops≥2)\nand multi-entity (entities≥2) questions.\nMethod WebQSP(F1) CWQ(F1) MetaQA-3(H@1)multi-hop multi-entitymulti-hop multi-entitymulti-hop\nLLM (No RAG)48.4 61.5 33.7 32.3 29.7GNN 58.8 70.4 57.7 54.2 98.6\nRoG 63.3 65.1 59.3 58.3 84.8SubgraphRAG65.8 54.9 55.8 52.3 –\nGNN-RAG 69.8 82.3 68.2 64.8 98.6GNN-RAG+RA 71.1 88.8 69.3 65.6 98.6\nAbsolute Improv.+5.3 +23.7 +10.0 +7.3 +13.8\nTable 3: Comparison of different retrieval methods on\nCWQ. ‘#KG Tokens’ denotes the median number of KG\ntokens retrieved as context for the LLM.\nRetrieval Metrics KGQA\n#KG TokensHit@1(%) Hit@10(%) F1(%)\nRoG 201 25.9 54.5 56.2\nSubgraphRAG1,442 26.8 58.7 47.2\nGNN-RAG 114 52.9 64.1 59.4\nGNN-RAG+RA 362 52.9 71.1 60.4\nGNN – 52.9 63.8 47.8\nToG+GPT-4 performance, using an LLM with only\n7B parameters and much fewer LLM calls, while\nGNN-RAG can be deployed on a single 24GB\nGPU. GNN-RAG+RA outperforms ToG+ChatGPT\nby up to 14.5% points at Hit and the best per-\nforming GNN by 5.3–9.5% points at Hits@1 and\nby 0.7–10.7% points at F1. In long-context re-\ntrieval (KG-LC), GNN-RAG+Route outperforms\nSubgraphRAG by 3.5% points at Hit on CWQ,\nwhile being more efficient when queries are routed\nto GNN-RAG inference.\nComplex KGQA. Table 2 compares complex\nKGQA performance results on multi-hop questions,\nwhere answers are more than one hop away from\nthe question entities, and multi-entity questions,\nwhich have more than one question entities. GNN-\nRAG leverages GNNs to handle complex graph\ninformation and outperforms RoG (LLM-based re-\ntrieval) and SubgraphRAG (long-context retrieval)\nby 4.0–17.2% points at F1 on WebQSP, by 8.5–\n8.9% points at F1 on CWQ, and by 13.8% points at\nH@1 on MetaQA-3. In addition, GNN-RAG+RA\noffers an additional improvement by up to 6.5%\npoints at F1 over RoG. The results show that GNN-\nRAG is an effective retrieval method when the ques-\ntions involve complex graph information.\nRetrieval Results. Table 3 presents an eval-\nuation of the retrieval performance across differ-\nent graph retrieval methods, alongside their impact\non downstream KGQA performance. Based on\nthese results, we make the following observations:\nFirst, GNN-based retrieval is both more efficient\n16688\nTable 4: Ablation study on different GNN retrievers\nfor KGQA in terms of number of layers L, number of\nquestion embeddings K, and iterative reasoning.\nRetriever Setting WebQSP CWQ\nL K Hit H@1 F1 Hit H@1 F1\nDense Subgraph– – 70.2 68.7 54.3 47.1 45.5 41.9\nGNN 2 1 82.8 78.6 69.8 58.2 51.9 49.4\nGNN 3 1 85.0 79.6 70.4 58.5 52.5 50.1\nGNN 3 3 85.2 80.1 70.6 62.5 57.5 53.3\nGNN iterative6 3 85.7 80.6 71.3 66.8 61.7 59.4\n(in terms of the number of KG tokens) and more\neffective (in terms of F1 score) than both RoG and\nSubgraphRAG, particularly for complex questions\nsuch as those from CWQ. Second, GNN-based\nretrieval demonstrates superior performance, sur-\npassing RoG and SubgraphRAG by 26.1–27.0%\npoints in terms of Hit@1. Third, retrieval aug-\nmentation (denoted as GNN-RAG+RA) enhances\nboth Hit@kand KGQA F1 scores, as it enables the\ncombination of non-overlapping knowledge graph\ninformation from GNN-based and semantic pars-\ning (RoG) retrieval, leading to an increase in the\nnumber of input tokens and thereby improving the\nLLM’s contextual understanding. GNN-RAG+RA\nis more efficient than long-context alternatives as\nSubgraphRAG.\nGNN Ablation. In Table 4, we present an ab-\nlation study of GNN hyperparameters (number of\nlayers Land number of question embeddings K)\nand techniques (with or without iterative reasoning)\nfor GNN-RAG. The results indicate that shallow\nGNNs with fewer layers exhibit suboptimal perfor-\nmance. Additionally, increasing the number Kof\nquestion embeddings used leads to improvements,\nparticularly for complex questions (CWQ), which\nconsist of multiple subquestions. Furthermore, iter-\native GNN reasoning, as described in Equation 6,\nfurther enhances performance. This suggests that\nthe GNN effectively revisits node importance by\nincorporating deeper graph context.\nRetrieval Effect on LLMs . Table 5 presents\nperformance results of various LLMs using differ-\nent retrievers. GNN-RAG is the retrieval approach\nthat achieves the overall best performance. For in-\nstance, GNN-RAG improves ChatGPT by up to\n5.2% points at Hit over the best competing ap-\nproach . Moreover, GNN-RAG substantially im-\nproves the KGQA performance of weaker LLMs,\nsuch as Alpaca-7B and Flan-T5-xl. The improve-\nment over RoG is up to 13.2% points at Hit, while\nGNN-RAG outperforms Llama-70B approaches us-\nTable 5: Retrieval effect on performance (% Hit) using\nvarious LLMs.\nMethod WebQSP CWQ\nChatGPT 51.8 39.9\n+ ToG 76.2 58.9\n+ RoG 81.5 52.7\n+ SubgraphRAG 83.1 56.2\n+ GNN-RAG 85.3 64.1\nAlpaca-7B 51.8 27.4\n+ RoG 73.6 44.0\n+ GNN-RAG 76.2 54.5\nLlama2-Chat-7B 64.4 34.6\n+ RoG 84.8 56.4\n+ GNN-RAG 85.2 62.7\nLlama2/3.1-Chat-70B 57.4 39.1\n+ ToG 68.9 57.6\n+ SubgraphRAG 86.2 57.9\nFlan-T5-xl 31.0 14.7\n+ RoG 67.9 37.8\n+ GNN-RAG 74.5 51.0\nTable 6: GNN-RAG routing effect on efficiency and\nKGQA performance. We evaluate results on the routed\nsubset (CWQ-sub). ‘#Routed‘ denotes the number of\nrouted questions for efficient retrieval with GNN-RAG.\nCWQ-sub\n#Routed #LLM Calls Hit (%)\nRoG 0 4 59.6\nw/ GNN-RAGroute 2,377 (78%) 1 66.7\n#Routed #KG Tokens Hit (%)\nSubgraphRAG 0 1,400 41.4\nw/ GNN-RAGroute 970 (28%) 153 55.1\ning a lightweight Llama-7B model. The results\ndemonstrate that GNN-RAG can be integrated with\nother LLMs to improve their KGQA performance\nwithout retraining.\nRouting Efficiency. Table 6 presents results\ndemonstrating that GNN-RAG routing significantly\nreduces inference costs while enhancing the per-\nformance of KGQA. For RoG routing, we route to\nGNN-RAG if the GNN retrieves a multi-hop rea-\nsoning path. For SubgraphRAG routing, we route\nto GNN-RAG if any of the LLM response is not\npresent in the long-context. As the results indicate,\nGNN-RAG improves the performance of 78% of\nquestions by 7.1% points in Hit, while requiring\nonly a single LLM call, in contrast to RoG, which\nnecessitates 4 LLM calls. Furthermore, GNN-RAG\nenhances 28% of questions by 13.7% points rel-\native to SubgraphRAG, while retrieving approxi-\nmately 9×fewer KG tokens, leading to more effi-\ncient LLM inference.\n16689\nTable 7: Time latency across methods. LLM latency\ntime is measured on an A10G GPU with fp16 inference.\nRetrieval Generation Total\nTime (mins) Time (mins) Time (mins)\nRoG 11 31 42\nSubgraphRAG 0.1 58 58.1\nGNN-RAG 0.9 29 30\n6.1 Time Latency\nWe primarily measure #KG Tokens as a metric of\nefficiency, as the end-to-end (retrieval+generation)\nlatency is predominantly determined by the LLM\ngeneration. To provide more context, we present\nthe modularized latency cost of each method on\nWebQSP, using a 7B Llama model for generation\nin Table 7. As the results show, GNN-RAG is\nthe most efficient method in terms of end-to-end\nlatency.\nAppendix. Case studies are provided in Ap-\npendix B and further ablation studies in Ap-\npendix D.\n7 Conclusion\nWe introduce GNN-RAG, a novel graph neural\nmethod for enhancing RAG in KGQA with GNNs.\nOur contributions are the following. (1) Frame-\nwork: GNN-RAG tailors GNNs for KG retrieval\ndue to their ability to handle complex graph infor-\nmation. (2) Effectiveness: GNN-RAG achieves\nsuperior performance when multi-hop information\nis needed for KGQA. (3) Efficiency: GNN-RAG\nimproves vanilla LLMs on KGQA performance\nwithout incurring additional LLM calls as exist-\ning RAG systems for KGQA require. In addition,\nGNN-RAG outperforms long-context retrieval us-\ning 9×fewer KG tokens.\n8 Limitations\nGNN-RAG assumes that the KG subgraph, on\nwhich the GNN reasons, contains answer nodes.\nHowever, this may not be true for all questions\nor when errors in entity linking happen. In addi-\ntion, GNN-RAG employs simple prompting with\nthe shortest paths from question entities to candi-\ndate answers as context. As an extension, GNN-\nRAG can be combined with prompt optimiza-\ntion (Wen et al., 2023; Zhang et al., 2023a) so\nthat the LLM understands the graph better. More-\nover, the scope of our GNN-RAG contributions is\nto improve the retrieval results over the KG with-\nout specialized GNN-LLM interactions. However,\nthe GNN and the LLM could be coupled via itera-\ntive retrieval (Asai et al., 2023) to further improve\nKGQA.\nAcknowledgements\nThis work was supported in part by NSF (1447788,\n1704074, 1757916, 1834251, 1834332), Army Re-\nsearch Office (W911NF1810344), Intel Corp, and\nAmazon Web Services. Access to research and\ncomputing facilities was provided by the Minnesota\nSupercomputing Institute.\nReferences\nReid Andersen, Fan Chung, and Kevin Lang. 2006. Lo-\ncal graph partitioning using pagerank vectors. In\n2006 47th Annual IEEE Symposium on Foundations\nof Computer Science (FOCS’06).\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\narXiv preprint arXiv:2310.11511.\nMattia Atzeni, Jasmina Bogojeska, and Andreas Loukas.\n2021. Sqaler: Scaling question answering by decou-\npling multi-hop and logical reasoning. Advances in\nNeural Information Processing Systems.\nJinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023.\nKnowledge-augmented language model prompting\nfor zero-shot knowledge graph question answering.\narXiv preprint arXiv:2306.04136.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collabo-\nratively created graph database for structuring human\nknowledge. In Proceedings of the 2008 ACM SIG-\nMOD international conference on Management of\ndata, pages 1247–1250.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nHyeong Kyu Choi, Seunghun Lee, Jaewon Chu, and\nHyunwoo J Kim. 2024. Nutrea: Neural tree search\nfor context-guided multi-hop kgqa. Advances in Neu-\nral Information Processing Systems, 36.\n16690\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2023. Palm: Scaling language\nmodeling with pathways. Journal of Machine Learn-\ning Research, 24(240):1–113.\nPhilipp Christmann, Rishiraj Saha Roy, and Gerhard\nWeikum. 2023. Explainable conversational question\nanswering over heterogeneous sources via iterative\ngraph neural networks. In Proceedings of the 46th\ninternational ACM SIGIR conference on research\nand development in information retrieval, pages 643–\n653.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2024. Scaling instruction-finetuned language models.\nJournal of Machine Learning Research, 25(70):1–53.\nRajarshi Das, Manzil Zaheer, Dung Thai, Ameya\nGodbole, Ethan Perez, Jay-Yoon Lee, Lizhen Tan,\nLazaros Polymenakos, and Andrew McCallum.\n2021. Case-based reasoning for natural language\nqueries over knowledge bases. arXiv preprint\narXiv:2104.08762.\nDarren Edge, Ha Trinh, Newman Cheng, Joshua\nBradley, Alex Chao, Apurva Mody, Steven Truitt,\nand Jonathan Larson. 2024. From local to global: A\ngraph rag approach to query-focused summarization.\narXiv preprint arXiv:2404.16130.\nYu Gu, Xiang Deng, and Yu Su. 2023. Don’t gener-\nate, discriminate: A proposal for grounding language\nmodels to real-world environments. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nToronto, Canada. Association for Computational Lin-\nguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nBernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michi-\nhiro Yasunaga, and Yu Su. 2024. Hipporag: Neu-\nrobiologically inspired long-term memory for large\nlanguage models. arXiv preprint arXiv:2405.14831.\nGaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and\nJi-Rong Wen. 2021. Improving multi-hop knowledge\nbase question answering by learning intermediate\nsupervision signals. In Proceedings of the 14th ACM\ninternational conference on web search and data\nmining, pages 553–561.\nXiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla,\nThomas Laurent, Yann LeCun, Xavier Bresson, and\nBryan Hooi. 2024. G-retriever: Retrieval-augmented\ngeneration for textual graph understanding and ques-\ntion answering. arXiv preprint arXiv:2402.07630.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nXuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng\nBao, Quanjin Tao, Ziwei Chai, and Qi Zhu. 2024.\nCan gnn be good adapter for llms? arXiv preprint\narXiv:2402.12984.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023a. Struct-\ngpt: A general framework for large language model\nto reason over structured data. arXiv preprint\narXiv:2305.09645.\nJinhao Jiang, Kun Zhou, Wayne Xin Zhao, and Ji-Rong\nWen. 2023b. Unikgqa: Unified retrieval and reason-\ning for solving multi-hop question answering over\nknowledge graph. In International Conference on\nLearning Representations.\nBowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji,\nand Jiawei Han. 2023. Large language models on\ngraphs: A comprehensive survey. arXiv preprint\narXiv:2312.02783.\nBowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar\nRoy, Yu Zhang, Suhang Wang, Yu Meng, and Jiawei\nHan. 2024. Graph chain-of-thought: Augmenting\nlarge language models by reasoning on graphs. arXiv\npreprint arXiv:2404.07103.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nThomas N Kipf and Max Welling. 2016. Semi-\nsupervised classification with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907.\nYunshi Lan, Gaole He, Jinhao Jiang, Jing Jiang,\nWayne Xin Zhao, and Ji-Rong Wen. 2022. Complex\nknowledge base question answering: A survey. IEEE\nTransactions on Knowledge and Data Engineering.\nYunshi Lan and Jing Jiang. 2020. Query graph gen-\neration for answering multi-hop complex questions\nfrom knowledge bases. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 969–974. Association for Compu-\ntational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\n16691\nKun Li, Tianhua Zhang, Xixin Wu, Hongyin Luo, James\nGlass, and Helen Meng. 2024a. Decoding on graphs:\nFaithful and sound reasoning on knowledge graphs\nthrough generation of well-formed chains. arXiv\npreprint arXiv:2410.18415.\nMufei Li, Siqi Miao, and Pan Li. 2024b. Simple is effec-\ntive: The roles of graphs and large language models\nin knowledge-graph-based retrieval-augmented gen-\neration. arXiv preprint arXiv:2410.20724.\nTianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su,\nand Wenhu Chen. 2023. Few-shot in-context learning\non knowledge base question answering. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 6966–6980. Association for Computational\nLinguistics.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,\nMaria Lomeli, Rich James, Pedro Rodriguez, Jacob\nKahn, Gergely Szilvasy, Mike Lewis, et al. 2023.\nRa-dit: Retrieval-augmented dual instruction tuning.\narXiv preprint arXiv:2310.01352.\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024a. Lost in the middle: How language\nmodels use long contexts. TACL.\nYang Liu, Xiaobin Tian, Zequn Sun, and Wei Hu. 2024b.\nFinetuning generative large language models with dis-\ncrimination instructions for knowledge graph com-\npletion. arXiv preprint arXiv:2407.16127.\nLinhao Luo, Yuan-Fang Li, Gholamreza Haffari, and\nShirui Pan. 2024a. Reasoning on graphs: Faithful\nand interpretable large language model reasoning. In\nInternational Conference on Learning Representa-\ntions.\nLinhao Luo, Zicheng Zhao, Chen Gong, Gholam-\nreza Haffari, and Shirui Pan. 2024b. Graph-\nconstrained reasoning: Faithful reasoning on knowl-\nedge graphs with large language models. arXiv\npreprint arXiv:2410.13080.\nCostas Mavromatis and George Karypis. 2022. ReaRev:\nAdaptive reasoning for question answering over\nknowledge graphs. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n2447–2458, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nCostas Mavromatis, Petros Karypis, and George\nKarypis. 2024. Sempool: Simple, robust, and inter-\npretable kg pooling for enhancing language models.\nIn Pacific-Asia Conference on Knowledge Discovery\nand Data Mining, pages 154–166. Springer.\nAlexander Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason Weston.\n2016. Key-value memory networks for directly read-\ning documents. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing.\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Ji-\napu Wang, and Xindong Wu. 2024. Unifying large\nlanguage models and knowledge graphs: A roadmap.\nIEEE Transactions on Knowledge and Data Engi-\nneering.\nBoci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo,\nHaizhou Shi, Chuntao Hong, Yan Zhang, and Siliang\nTang. 2024. Graph retrieval-augmented generation:\nA survey. arXiv preprint arXiv:2408.08921.\nYunqi Qiu, Yuanzhuo Wang, Xiaolong Jin, and Kun\nZhang. 2020. Stepwise reasoning for multi-relation\nquestion answering over knowledge graph with weak\nsupervision. In Proceedings of the 13th international\nconference on web search and data mining , pages\n474–482.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP).\nApoorv Saxena, Aditay Tripathi, and Partha Talukdar.\n2020. Improving multi-hop question answering over\nknowledge graphs using knowledge base embeddings.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem,\nRianne Van Den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In The semantic web: 15th inter-\nnational conference, ESWC 2018, Heraklion, Crete,\nGreece, June 3–7, 2018, proceedings 15, pages 593–\n607. Springer.\nPriyanka Sen, Amir Saffari, and Armin Oliya. 2021.\nExpanding end-to-end question answering on differ-\nentiable knowledge graphs with intersection. arXiv\npreprint arXiv:2109.05808.\nJiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, and\nHanwang Zhang. 2021. Transfernet: An effec-\ntive and transparent framework for multi-hop ques-\ntion answering over relation graph. arXiv preprint\narXiv:2104.07302.\nHaitian Sun, Tania Bedrax-Weiss, and William W Co-\nhen. 2019. Pullnet: Open domain question answering\nwith iterative retrieval on knowledge bases and text.\narXiv preprint arXiv:1904.09537.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William Cohen.\n2018. Open domain question answering using early\nfusion of knowledge bases and text. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4231–4242.\nAssociation for Computational Linguistics.\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\nWang, Chen Lin, Yeyun Gong, Heung-Yeung Shum,\nand Jian Guo. 2024. Think-on-graph: Deep and\n16692\nresponsible reasoning of large language model with\nknowledge graph. In International Conference on\nLearning Representations.\nYawei Sun, Lingling Zhang, Gong Cheng, and Yuzhong\nQu. 2020. Sparqa: skeleton-based semantic pars-\ning for complex questions over knowledge bases. In\nProceedings of the AAAI conference on artificial in-\ntelligence, volume 34, pages 8952–8959.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nYiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu,\nYongrui Chen, and Guilin Qi. 2023. Can chatgpt\nreplace traditional kbqa models? an in-depth analysis\nof the question answering performance of the gpt llm\nfamily. In International Semantic Web Conference,\npages 348–367. Springer.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nYijun Tian, Huan Song, Zichen Wang, Haozhu Wang,\nZiqing Hu, Fang Wang, Nitesh V Chawla, and Pan-\npan Xu. 2024. Graph neural prompting with large\nlanguage models. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 38, pages\n19080–19088.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nThéo Trouillon, Johannes Welbl, Sebastian Riedel, Éric\nGaussier, and Guillaume Bouchard. 2016. Complex\nembeddings for simple link prediction. In Interna-\ntional conference on machine learning. PMLR.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio.\n2017. Graph attention networks. arXiv preprint\narXiv:1710.10903.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78–85.\nJunxing Wang, Xinyi Li, Zhen Tan, Xiang Zhao, and\nWeidong Xiao. 2021. Relation-aware bidirectional\npath reasoning for commonsense question answering.\nIn Proceedings of the 25th Conference on Computa-\ntional Natural Language Learning, pages 445–453.\nKeheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li,\nYunsen Xian, Chuantao Yin, Wenge Rong, and Zhang\nXiong. 2023. Knowledge-driven cot: Exploring faith-\nful reasoning in llms for knowledge-intensive ques-\ntion answering. arXiv preprint arXiv:2308.13259.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in neural\ninformation processing systems, 35:24824–24837.\nYanbin Wei, Qiushi Huang, James T Kwok, and\nYu Zhang. 2024. Kicgpt: Large language model\nwith knowledge in context for knowledge graph com-\npletion. arXiv preprint arXiv:2402.02389.\nYilin Wen, Zifeng Wang, and Jimeng Sun. 2023.\nMindmap: Knowledge graph prompting sparks graph\nof thoughts in large language models. arXiv preprint\narXiv:2308.09729.\nYike Wu, Nan Hu, Guilin Qi, Sheng Bi, Jie Ren, An-\nhuan Xie, and Wei Song. 2023. Retrieve-rewrite-\nanswer: A kg-to-text enhanced llms framework for\nknowledge graph question answering. arXiv preprint\narXiv:2309.11206.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang,\nVictor Zhong, Bailin Wang, Chengzu Li, Connor\nBoyle, Ansong Ni, Ziyu Yao, Dragomir Radev, Caim-\ning Xiong, Lingpeng Kong, Rui Zhang, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2022. Unified-\nskg: Unifying and multi-tasking structured knowl-\nedge grounding with text-to-text language models.\nEMNLP.\nMichihiro Yasunaga, Antoine Bosselut, Hongyu Ren,\nXikun Zhang, Christopher D Manning, Percy S\nLiang, and Jure Leskovec. 2022. Deep bidirectional\nlanguage-knowledge graph pretraining. Advances in\nNeural Information Processing Systems, 35:37309–\n37323.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. Qa-gnn: Rea-\nsoning with language models and knowledge graphs\nfor question answering. In North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nXi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou,\nand Caiming Xiong. 2022. RNG-KBQA: Generation\naugmented iterative ranking for knowledge base ques-\ntion answering. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6032–6043.\nAssociation for Computational Linguistics.\nScott Wen-tau Yih, Ming-Wei Chang, Xiaodong He,\nand Jianfeng Gao. 2015. Semantic parsing via staged\nquery graph generation: Question answering with\nknowledge base. In Proceedings of the Joint Con-\nference of the 53rd Annual Meeting of the ACL and\nthe 7th International Joint Conference on Natural\nLanguage Processing of the AFNLP.\n16693\nDonghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu,\nAlexander Hanbo Li, Jun Wang, Yiqun Hu, William\nWang, Zhiguo Wang, and Bing Xiang. 2022. Decaf:\nJoint decoding of answers and logical forms for ques-\ntion answering over knowledge bases. arXiv preprint\narXiv:2210.00063.\nGeng Zhang, Jin Liu, Guangyou Zhou, Kunsong Zhao,\nZhiwen Xie, and Bo Huang. 2024a. Question-\ndirected reasoning with relation-aware graph atten-\ntion network for complex question answering over\nknowledge graph. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing.\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie\nTang, Cuiping Li, and Hong Chen. 2022a. Sub-\ngraph retrieval enhanced model for multi-hop knowl-\nedge base question answering. arXiv preprint\narXiv:2202.13296.\nQinggang Zhang, Junnan Dong, Hao Chen, Xiao Huang,\nDaochen Zha, and Zailiang Yu. 2023a. Knowgpt:\nBlack-box knowledge injection for large language\nmodels. arXiv preprint arXiv:2312.06185.\nQixuan Zhang, Xinyi Weng, Guangyou Zhou, Yi Zhang,\nand Jimmy Xiangji Huang. 2022b. Arl: An adaptive\nreinforcement learning framework for complex ques-\ntion answering over knowledge base. Information\nProcessing & Management, 59(3):102933.\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng\nShen, Matei Zaharia, Ion Stoica, and Joseph E Gon-\nzalez. 2024b. Raft: Adapting language model to do-\nmain specific rag. arXiv preprint arXiv:2403.10131.\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga,\nHongyu Ren, Percy Liang, Christopher D Manning,\nand Jure Leskovec. 2022c. Greaselm: Graph rea-\nsoning enhanced language models. In International\nConference on Learning Representations.\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu,\nTingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang,\nYulong Chen, et al. 2023b. Siren’s song in the ai\nocean: a survey on hallucination in large language\nmodels. arXiv preprint arXiv:2309.01219.\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\nder J Smola, and Le Song. 2018. Variational reason-\ning for question answering with knowledge graph. In\nThirty-Second AAAI Conference on Artificial Intelli-\ngence.\nJianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian\nLiu, Rui Li, Xing Xie, and Jian Tang. 2022. Learning\non large-scale text-attributed graphs via variational\ninference. arXiv preprint arXiv:2210.14709.\nRuilin Zhao, Feng Zhao, Long Wang, Xianzhi Wang,\nand Guandong Xu. 2024. Kg-cot: Chain-of-thought\nprompting of large language models over knowl-\nedge graphs for knowledge-aware question answer-\ning. In Proceedings of the Thirty-Third International\nJoint Conference on Artificial Intelligence (IJCAI-\n24), pages 6642–6650. International Joint Confer-\nences on Artificial Intelligence.\nAppendix\nA G NN-RAG Implementation\nClassification layer: After LGNN layers, we ob-\ntain node representation matrix H(L) ∈R|V|×d.\nTo perform classification, we obtain the node prob-\nability matrix P = softmax(H(L)W), where\nW ∈Rd×1 is a learnable projection layer followed\nby softmax normalization. Answer nodes should\nhave larger probability pv ∈[0,1] than non-answer\nnodes.\nNode and relation embeddings : We use pre-\ntrained models, such as SBERT or other LMs,\nto encode relation embeddings. We obtain node\nembeddings by aggregating the adjacent relation\nembeddings of nodes, which has been shown\nto generalize better to new entities (He et al.,\n2021; Choi et al., 2024). The formula is h(0)\nv =\nReLU(∑\nr∈Nr(v) Wrr), where r is the relation\nembedding and W is learnable. During training,\nwe optimize the GNN parameters, but not the rela-\ntion embeddings obtained via the pretrained mod-\nels.\nQuestion Embeddings: As complex questions\nmight consist of multiple subquestions, we obtain\nKquestion embeddings to better capture different\nquestion parts (Qiu et al., 2020), as shown in Equa-\ntion 3. To capture multiple question’s contexts,\neach question representation qk ∈Rd,k ∈K, is\ninitialized by dynamically attending to different\nquestion’s tokens. First, we derive a representa-\ntion qj ∈Rd for each token jof the question and\na question representation, e.g., via CLS pooling,\nqc ∈Rd with pre-trained language models, such as\nSBERT. Equation 3 becomes\nqk = γk(LM(q)) =\n∑\nj\nak,jqj, (7)\nwhere j denotes is the j-th token position and\nak,j ∈ [0,1] is an attention weight. At each it-\neration k, weight ak,j is dynamically adjusted by\nencouraging attention to new question parts via:\nak,j = softmaxj(Wa( ˜qk ⊙qj) (8)\n˜qk = Wk(qk−1||qc||qk−1 ⊙qc||qc −qk−1),\n(9)\nwhere Wa ∈Rd×d and Wk ∈Rd×4d are learnable\nparameters.\nDownstream LLM: For the downstream LLM,\nwe opt to follow prompt tuning (Lin et al., 2023;\n16694\nZhang et al., 2024b), where the Llama-7B-Chat\nmodel is trained to generate to generate a list of\nanswers for KGQA. We follow RoG (Luo et al.,\n2024a) fine-tuning approach. The LLM is fine-\ntuned based on the training question-answer pairs\nto generate a list of correct answers, given the\nprompt:\n“Based on the reasoning paths, please\nanswer the given question.\\n Reasoning\nPaths: {Reasoning Paths} \\n Question:\n{Question}”.\nThe reasoning paths are verbalized as “{question\nentity} → {relation} → {entity} → ···\n→ {relation} → {answer entity} \\n” (see\nFigure 3).\nDuring training, the reasoning paths are the short-\nest paths from question entities to answer entities.\nDuring inference, the reasoning paths are obtained\nby GNN-RAG.\nB Case Studies\nFigure 4 illustrates two case studies from the CWQ\ndataset, showing how GNN-RAG improves LLM’s\nfaithfulness, i.e., how well the LLM follows the\nquestion’s instructions and uses the right informa-\ntion from the KG.\nIn both cases, GNN-RAG retrieves multi-hop\ninformation, which is necessary for answer-\ning the questions correctly. In the first case,\nGNN-RAG retrieves both crucial facts <Gilfoyle\n→ characters_that_have_lived_here →\nToronto> and <Toronto → province.capital\n→ Ontario> that are required to answer the ques-\ntion, unlike the KG-RAG baseline (RoG) that\nfetches only the first fact. In the second case, the\nKG-RAG baseline incorrectly retrieves informa-\ntion about <Erin Brockovich → person> and\nnot <Erin Brockovich → film_character>\nthat the question refers to. GNN-RAG uses\nGNNs to explore how <Erin Brockovich> and\n<Michael Renault Mageau> entities are re-\nlated in the KG, resulting into retrieving facts\nabout <Erin Brockovich → film_character>.\nThe retrieved facts include important information\n<films_with_this_crew_job → Consultant>.\nFigure 5 illustrates one case study from the\nWebQSP dataset, showing how RA (Section 4.3)\nimproves GNN-RAG. Initially, the GNN does\nnot retrieve helpful information due to its limi-\ntation to understand natural language, i.e., that\n<jurisdiction.bodies> usually “ make the\nTable 8: Datasets statistics. “avg.|Vq|” denotes average\nnumber of entities in subgraph, and “coverage” denotes\nthe ratio of at least one answer in subgraph.\nDatasets Train Dev Test avg.|Vq| coverage (%)\nWebQSP 2,848 250 1,639 1,429.8 94.9\nCWQ 27,639 3,519 3,531 1,305.8 79.3\nMetaQA-3114,196 14,274 14,274 497.9 99.0\nlaws”. GNN-RAG+RA retrieves the right infor-\nmation, helping the LLM answer the question cor-\nrectly.\nC Experimental Setup\nKGQA Datasets. We experiment with two widely\nused KGQA benchmarks: WebQuestionsSP (We-\nbQSP) (Yih et al., 2015), Complex WebQuestions\n1.1 (CWQ) (Talmor and Berant, 2018). We also\nexperiment with MetaQA-3 (Zhang et al., 2018)\ndataset. We provide the dataset statistics Table 8.\nWebQSP contains 4,737 natural language ques-\ntions that are answerable using a subset Freebase\nKG (Bollacker et al., 2008). This KG contains\n164.6 million facts and 24.9 million entities. The\nquestions require up to 2-hop reasoning within this\nKG. Specifically, the model needs to aggregate over\ntwo KG facts for 30% of the questions, to reason\nover constraints for 7% of the questions, and to\nuse a single KG fact for the rest of the questions.\nCWQ is generated from WebQSP by extending the\nquestion entities or adding constraints to answers,\nin order to construct more complex multi-hop ques-\ntions (34,689 in total). There are four types of\nquestions: composition (45%), conjunction (45%),\ncomparative (5%), and superlative (5%). The ques-\ntions require up to 4-hops of reasoning over the KG,\nwhich is the same KG as in WebQSP.MetaQA-3\nconsists of more than 100k 3-hop questions in the\ndomain of movies. The questions were constructed\nusing the KG provided by the WikiMovies (Miller\net al., 2016) dataset, with about 43k entities and\n135k triples. For MetaQA-3, we use 1,000 (1%) of\nthe training questions.\nImplementation. For subgraph retrieval, we use\nthe linked entities to the KG provided by (Yih et al.,\n2015) for WebQSP, by (Talmor and Berant, 2018)\nfor CWQ. We obtain dense subgraphs by (He et al.,\n2021). It runs the PageRank Nibble (Andersen\net al., 2006) (PRN) method starting from the linked\nentities to select the top- m(m = 2,000) entities\nto be included in the subgraph.\nFor GNN training, we employ standard train-\n16695\nQ: \"In which state did ﬁctional character Gilfoyle live?\"\nA: Ontario\nKG-RAG Gilfoyle -> ﬁctional_universe.ﬁctional_setting.characters_that_have_lived_here -> Toronto\nGNN-RAG \nGilfoyle -> ﬁctional_universe.ﬁctional_setting.characters_that_have_lived_here -> Toronto\nGilfoyle -> ﬁctional_universe.ﬁctional_character.place_of_birth -> Canada -\n> location.country.ﬁrst_level_divisions -> Ontario\nGilfoyle -> ﬁctional_universe.ﬁctional_setting.characters_that_have_lived_here -> Toronto -\n> location.province.capital -> Ontario\nLLM A: Toronto\nLLM A: Ontario\nQ: \"Who was the real Erin Brockovich featured in Michael Renault Mageau movie ?\"\nA: Consultant\nKG-RAG Erin Brockovich -> people.person.profession -> Environmentalist\nErin Brockovich -> people.person.profession -> Actor\nErin Brockovich -> people.person.profession -> Consultant\nGNN-RAG \nLLM A: Actor\nLLM A: Consultant\nErin Brockovich -> ﬁlm.ﬁlm.starring -> Julia Roberts ->\nﬁlm.ﬁlm_character.portrayed_in_ﬁlms -> Julia, the Waitress\nMichael Renault Mageau -> common.topic.notable_types -> Film Actor ->\ncommon.topic.notable_types -> Erin Brockovich\nMichael Renault Mageau -> ﬁlm.ﬁlm_crew_gig.crewmember -> m.0pxdvpl ->\nﬁlm.ﬁlm_job.ﬁlms_with_this_crew_job -> Consultant\nFigure 4: Two case studies that illustrate howGNN-RAG\nimproves the LLM’s faithfulness. In both cases, GNN-\nRAG retrieves multi-hop information that is necessary\nfor answering the complex questions.\nQ: \"Who made the laws in Canada?\"\nA: Parliament of Canada LLM\nA:\nParliament\nof Canada\nGNN-RAG\nCanada -> royalty.monarchy.kingdom ->\nElizabeth II\nCanada -> people.person.nationality -\n> WL Mackenzie King\n+ RA\n... +\nCanada ->\ngovernment.jurisdiction.bodies ->\nParliament of Canada\nFigure 5: One case study that illustrates the benefit of\nretrieval augmentation (RA). RA uses LLMs to fetch\nsemantically relevant KG information, which may have\nbeen missed by the GNN.\ning procedure and hyperparameters as in the lit-\nerature (Mavromatis and Karypis, 2022). As the\ndefualt downstream LLM, we use the Llama2-Chat-\n7B model finetuned for KGQA (Luo et al., 2024a).\nFor both training and inference, we use suggested\nhyperparameters, without performing further hy-\nperparameter search. Model selection is performed\nbased on the validation data. Experiments with\nGNNs were performed on a Nvidia Geforce RTX-\n3090 GPU over 128GB RAM machine. Exper-\niments with LLMs were performed on 4 A100\nGPUs connected via NVLink and 512 GB of mem-\nory. The experiments are implemented with Py-\nTorch.\nFor LLM prompting during KGQA, we use the\nfollowing prompt:\nBased on the reasoning paths, please\nanswer the given question. Please keep\nthe answer as simple as possible and\nreturn all the possible answers as a\nlist.\\n\nReasoning Paths: {Reasoning Paths} \\n\nQuestion: {Question}\nDuring GNN inference, each node in the sub-\ngraph is assigned a probability of being the correct\nanswer, which is normalized via softmax. To re-\ntrieve answer candidates, we sort the nodes based\non the their probability scores, and select the top\nnodes whose cumulative probability score is be-\nlow a threshold. We set the threshold to 0.95. To\nretrieve the shortest paths between the question en-\ntities and answer candidates for RAG, we use the\nNetworkX library1.\nCompeting Approaches.\nWe evaluate the following categories of methods:\n1. Embedding, 2. GNN, 3. LLM, 4. KG+LLM, 5.\nGNN+LLM, 6. Long-context (KG-LC).\n1https://networkx.org/\n16696\n1. KV-Mem (Miller et al., 2016) is a key-\nvalue memory network for KGQA. Embed-\nKGQA (Saxena et al., 2020) utilizes KG pre-\ntrained embeddings (Trouillon et al., 2016)\nto improve multi-hop reasoning. Transfer-\nNet (Shi et al., 2021) improves multi-hop rea-\nsoning over the relation set. Rigel (Sen et al.,\n2021) improves reasoning with questions of\nmultiple entities.\n2. GraftNet (Sun et al., 2018) uses a convolution-\nbased GNN (Kipf and Welling, 2016). Pull-\nNet (Sun et al., 2019) is built on top of\nGraftNet, but learns which nodes to retrieve\nvia selecting shortest paths to the answers.\nNSM (He et al., 2021) is the adaptation\nof GNNs for KGQA. NSM+h (He et al.,\n2021) improves NSM for multi-hop reasoning.\nSQALER (Atzeni et al., 2021) learns which\nrelations (facts) to retrieve during KGQA for\nGNN reasoning. Similarly, SR+NSM (Zhang\net al., 2022a) proposes a relation-path retrieval.\nUniKGQA (Jiang et al., 2023b) unifies the\ngraph retrieval and reasoning process with a\nsingle LM. ReaRev (Mavromatis and Karypis,\n2022) explores diverse reasoning paths in a\nmulti-stage manner.\n3. We experiment with instruction-tuned LLMs.\nFlan-T5 (Chung et al., 2024) is based on\nT5, while Aplaca (Taori et al., 2023) and\nLLaMA2-Chat (Touvron et al., 2023) are\nbased on LLaMA. ChatGPT 2 is a powerful\nclosed-source LLM that excels in many com-\nplex tasks. ChatGPT+CoT uses the chain-\nof-thought (Wei et al., 2022) prompt to im-\nprove the ChatGPT. We access ChatGPT\n‘gpt-3.5-turbo’ through its API (as of May\n2024).\n4. KD-CoT (Wang et al., 2023) enhances CoT\nprompting for LLMs with relevant knowledge\nfrom KGs. StructGPT (Jiang et al., 2023a)\nretrieves KG facts for RAG. KB-BINDER (Li\net al., 2023) enhances LLM reasoning by\ngenerating logical forms of the questions.\nToG (Sun et al., 2024) uses a powerful LLM\nto select relevant facts hop-by-hop. RoG (Luo\net al., 2024a) uses the LLM to generate rela-\ntion paths for better planning.\n2https://openai.com/blog/chatgpt\nTable 9: Performance analysis (F1) based on the num-\nber of maximum hops that connect question entities to\nanswer entities.\nMethod WebQSP CWQ\n1 hop 2 hop≥3 hop1 hop 2 hop≥3 hop\nRoG 73.4 63.3 – 50.4 60.7 40.0\nGNN-RAG 72.0 69.8 – 47.4 69.4 51.8\nGNN-RAG+RA 74.6 71.1 – 48.2 70.9 47.7\nTable 10: Performance analysis (F1) based on the num-\nber of answers (#Ans).\nMethod WebQSP CWQ#Ans=1 2≤#Ans≤4 5≤#Ans≤9 #Ans≥10#Ans=1 2≤#Ans≤4 5≤#Ans≤9 #Ans≥10RoG 67.89 79.39 75.04 58.3356.90 53.73 58.36 43.62GNN-RAG 71.24 76.30 74.06 56.2860.40 55.52 61.49 50.08GNN-RAG+RA71.16 82.31 77.78 57.7162.09 56.47 62.87 50.33\n5. G-Retriever (He et al., 2024) augments LLMs\nwith GNN-based prompt tuning.\n6. SubgraphRAG (Li et al., 2024b) uses a text\nencoder to encode KG triplets and trains an\nMLP classifier to select the topktriplets based\non the question.\nD Additional Experimental Results\nD.1 Question Analysis\nFollowing the case studies presented in Figure 4\nand Figure 5, we provide numerical results on how\nGNN-RAG improves multi-hop question answer-\ning and how retrieval augmentation (RA) enhances\nsimple hop questions. Table 9 summarizes these\nresults. GNN-RAG improves performance on multi-\nhop questions (≥2 hops) by 6.5–11.8% F1 points\nover RoG. Furthermore, RA improves performance\non single-hop questions by 0.8–2.6% F1 points\nover GNN-RAG.\nTable 10 presents results with respect to the num-\nber of correct answers. As shown, RA enhances\nGNN-RAG in almost all cases as it can fetch correct\nanswers that might have been missed by the GNN.\nD.2 Prompt Ablation\nWhen using RAG, LLM performance depends on\nthe prompts used. To ablate on the prompt impact,\nwe experiment with the following prompts:\nPrompt A:\n16697\nTable 11: Performance comparison (%Hit) based on\ndifferent input prompts.\nWebQSP CWQ\nPrompt A RoG 84.8 56.4\nGNN-RAG 86.8 62.9\nPrompt B RoG 84.3 55.2\nGNN-RAG 85.2 61.7\nPrompt C RoG 81.6 51.8\nGNN-RAG 84.4 59.4\nBased on the reasoning paths,\nplease answer the given question.\nPlease keep the answer as simple\nas possible and return all the\npossible answers as a list.\\n\nReasoning Paths: {Reasoning\nPaths} \\n\nQuestion: {Question}\nPrompt B:\nBased on the provided knowledge,\nplease answer the given question.\nPlease keep the answer as simple\nas possible and return all the\npossible answers as a list.\\n\nKnowledge: {Reasoning Paths} \\n\nQuestion: {Question}\nPrompt C:\nYour tasks is to use the\nfollowing facts and answer the\nquestion. Make sure that you use the\ninformation from the facts provided.\nPlease keep the answer as simple as\npossible and return all the possible\nanswers as a list.\\n\nThe facts are the following:\n{Reasoning Paths} \\n\nQuestion: {Question}\nWe provide the results based on different input\nprompts in Table 11. As the results indicate, GNN-\nRAG outperforms RoG in all cases, being robust at\nthe prompt selection.\nD.3 Effect of Training Data\nTraining Cost . GNN-RAG requires only fine-\ntuning the GNN for retrieval. The downstream\nLLM can be fine-tuned (our default implementa-\ntion) or not (as we experimented with in Table 5).\nFine-tuning the downstream LLM is memory-\nintensive. For example, if we use 2 A100-80G\nGPUs, 1 epoch of 30k training data requires more\nthan 12 hours. GNN training is much more effi-\ncient: On a GeForce RTX 3090, 1 epoch of 30k\ntraining data needs less than 15 minutes and less\nthan 8GB of GPU memory.\nData Size Impact. Fine-tuning the downstream\nLLM generally improves performance. In Table 14,\nwe compare LLaMa2-Chat-7B and LLaMa2-Chat-\n7B fine-tuned. As shown (Hit metric), GNN-RAG\ndemonstrates a more stable performance when\nswitching between the two LLMs. Specifically,\nGNN-RAG experiences a relatively small drop of\n0.5-5.0 points, whereas RoG suffers from a larger\nperformance degradation of 0.9-6.2 points under\nthe same conditions. CWQ has more data (27.6k)\nthan WebQSP (2.8k) and thus, performance im-\nprovement when using the tuned LLM is larger.\nIn Table 15, we provide results when we use 10k\ntraining data of CWQ when training the GNN. As\nshown, although GNN-RAG uses approximately 3x\nless data, it still outperforms RoG (which uses 30k\ndata from both CWQ and WebQSP for training).\nTable 12 compares performance of different\nmethods based on the training data used for training\nthe retriever and the KGQA model. For example,\nGNN-RAG trains a GNN model for retrieval and\nuses a LLM for KGQA, which can be fine-tuned or\nnot. As the results show, GNN-RAG outperforms\nthe competing methods (RoG and UniKGQA) by\neither fine-tuning the KGQA model or not, while it\nuses the same or less data for training its retriever.\nD.4 Graph Effect\nGNNs operate on dense subgraphs, which might\ninclude noisy information. A question that arises\nis whether removing irrelevant information from\nthe subgraph would improve GNN retrieval. We\nexperiment with SR (Zhang et al., 2022a), which\nlearns to prune question-irrelevant facts from the\nKG. As shown in Table 13, although SR can im-\nprove the GNN reasoning results – see row (a) vs.\n(b) at CWQ –, the retrieval effectiveness deteri-\norates; rows (c) and (d). After examination, we\nfound that the sparse subgraph may contain dis-\nconnected KG parts. In this case, GNN-RAG’s ex-\ntraction of the shortest paths fails, and GNN-RAG\nreturns empty KG information.\nD.5 Threshold Ablation\nAs an additional ablation study, we set the thresh-\nold θ, which controls the number of candidate an-\nswer nodes for entity selection, to 0.99 (retrieves\n16698\nTable 12: Performance results based on different train-\ning data.\nMethod WebQSP CWQ\nTraining Data (Retriever) Training Data (KGQA Model) HitTraining Data (Retriever) Training Data (KGQA Model) Hit\nUniKGQA WebQSP WebQSP 77.2 CWQ CWQ 51.2\nRoG WebQSP WebQSP 81.5 CWQ CWQ 59.1WebQSP+CWQ None 84.8 WebQSP+CWQ None 56.4WebQSP+CWQ WebQSP+CWQ 85.7WebQSP+CWQ WebQSP+CWQ 62.6\nGNN-RAG WebQSP None 86.8 CWQ None 62.9WebQSP WebQSP+CWQ 87.2 CWQ WebQSP+CWQ 66.8\nTable 13: Performance comparison on different sub-\ngraphs.\nRetriever KGQA Model WebQSP CWQ\nHit H@1 F1 Hit H@1 F1\na) Dense Subgraph (A) GNN – 77.5 72.8 – 52.7 49.1b) Sparse Subgraph (Zhang et al., 2022a) (B) GNN– 74.2 69.8 – 53.3 49.7\nc) GNN-RAG: (A) LLaMA2-Chat-7B (tuned)85.0 80.3 71.5 66.2 61.3 58.9d) GNN-RAG: (B) 83.4 78.9 69.8 60.6 55.6 53.3\nTable 14: Impact of LLM tuning.\nRetrieval LLM WebQSP CWQ\nRoG LLaMa2-Chat-7B (untuned) 84.8 56.4\nRoG LLaMa2-Chat-7B (tuned) 85.7 62.6\nGNN-RAG LLaMa2-Chat-7B (untuned) 85.2 62.7\nGNN-RAG LLaMa2-Chat-7B (tuned) 85.7 66.7\nTable 15: Number of training data impact on CWQ.\nRetrieval # Training Data CWQ Hit (%)\nRoG 30k 62.6\nGNN-RAG 27.6k 66.7\nGNN-RAG 10k 63.7\nmore candidate answers), to 0.95 (default), and\nto 0.75 (retrieves less candidate answers). GNN-\nRAG performance is shown in Table 16. Increas-\ning the threshold (0.99) to retrieve more context,\ncan further increase performance to 85.9%. Lower\nthreshold (0.75) might miss some answers and the\nperformance drops to 83.5%.\nD.6 Text Retrieval\nTable 17 presents retrieval performance against\nadditional text-based baselines. We conduct\nexperiments comparing GNN-RAG with triplet\nembedding similarity methods, Retrieve-Rewrite-\nAnswer (), and UniHGKR-base () (KG instruction,\ntop-50) on WebQSP. As Table 17 demonstrates,\nGNN-RAG outperforms these methods in both re-\ntrieval and downstream QA metrics.\nTable 16: Threshold θimpact for answer node selection\n(WebQSP Hit %).\nθ= 0.99 θ= 0.95 θ= 0.75\nGNN-RAG 85.9 85.7 83.8\nTable 17: GNN-RAG vs. text-based retrieval methods.\nRet. Hit@1 (%) Ret. Hit@10 (%) QA Hit (%)\nUniHGKR 24.5 56.0 76.3\nRet-Rewrite-Ans 50.7 85.9 81.1\nGNN-RAG 76.5 92.3 85.7\n16699"
}