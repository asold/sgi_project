{
  "title": "Detecting Tuberculosis-Consistent Findings in Lateral Chest X-Rays Using an Ensemble of CNNs and Vision Transformers",
  "url": "https://openalex.org/W4213421646",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5017208524",
      "name": "Sivaramakrishnan Rajaraman",
      "affiliations": [
        "National Institutes of Health",
        "United States National Library of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5026457059",
      "name": "Ghada Zamzmi",
      "affiliations": [
        "National Institutes of Health",
        "United States National Library of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5065629281",
      "name": "Les Folio",
      "affiliations": [
        "Moffitt Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A5073995883",
      "name": "Sameer Antani",
      "affiliations": [
        "National Institutes of Health",
        "United States National Library of Medicine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2016188383",
    "https://openalex.org/W3140854437",
    "https://openalex.org/W2912664121",
    "https://openalex.org/W1534477342",
    "https://openalex.org/W6735205496",
    "https://openalex.org/W2885134687",
    "https://openalex.org/W3181327235",
    "https://openalex.org/W6769508662",
    "https://openalex.org/W6657373244",
    "https://openalex.org/W6756612629",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W3005101931",
    "https://openalex.org/W6678190589",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W1904878066",
    "https://openalex.org/W2934946272",
    "https://openalex.org/W2608231518",
    "https://openalex.org/W3200177155",
    "https://openalex.org/W3211983116",
    "https://openalex.org/W2939788146",
    "https://openalex.org/W3004666248",
    "https://openalex.org/W6755733436",
    "https://openalex.org/W2891756914",
    "https://openalex.org/W2979322170",
    "https://openalex.org/W3036688711",
    "https://openalex.org/W3011825565",
    "https://openalex.org/W2076063813",
    "https://openalex.org/W6766263406",
    "https://openalex.org/W3207549851",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2792454083",
    "https://openalex.org/W2157203477",
    "https://openalex.org/W6768888876",
    "https://openalex.org/W2898891934",
    "https://openalex.org/W4232607484",
    "https://openalex.org/W3145444543",
    "https://openalex.org/W2028030186",
    "https://openalex.org/W3152157423",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2981630388",
    "https://openalex.org/W3101156210",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W4301597213",
    "https://openalex.org/W2119903037",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W2901573617",
    "https://openalex.org/W4300485340"
  ],
  "abstract": "Research on detecting Tuberculosis (TB) findings on chest radiographs (or Chest X-rays: CXR) using convolutional neural networks (CNNs) has demonstrated superior performance due to the emergence of publicly available, large-scale datasets with expert annotations and availability of scalable computational resources. However, these studies use only the frontal CXR projections, i.e., the posterior-anterior (PA), and the anterior-posterior (AP) views for analysis and decision-making. Lateral CXRs which are heretofore not studied help detect clinically suspected pulmonary TB, particularly in children. Further, Vision Transformers (ViTs) with built-in self-attention mechanisms have recently emerged as a viable alternative to the traditional CNNs. Although ViTs demonstrated notable performance in several medical image analysis tasks, potential limitations exist in terms of performance and computational efficiency, between the CNN and ViT models, necessitating a comprehensive analysis to select appropriate models for the problem under study. This study aims to detect TB-consistent findings in lateral CXRs by constructing an ensemble of the CNN and ViT models. Several models are trained on lateral CXR data extracted from two large public collections to transfer modality-specific knowledge and fine-tune them for detecting findings consistent with TB. We observed that the weighted averaging ensemble of the predictions of CNN and ViT models using the optimal weights computed with the Sequential Least-Squares Quadratic Programming method delivered significantly superior performance (MCC: 0.8136, 95% confidence intervals (CI): 0.7394, 0.8878, p &amp;lt; 0.05) compared to the individual models and other ensembles. We also interpreted the decisions of CNN and ViT models using class-selective relevance maps and attention maps, respectively, and combined them to highlight the discriminative image regions contributing to the final output. We observed that (i) the model accuracy is not related to disease region of interest (ROI) localization and (ii) the bitwise-AND of the heatmaps of the top-2-performing models delivered significantly superior ROI localization performance in terms of mean average precision [mAP@(0.1 0.6) = 0.1820, 95% CI: 0.0771,0.2869, p &amp;lt; 0.05], compared to other individual models and ensembles. The code is available at https://github.com/sivaramakrishnan-rajaraman/Ensemble-of-CNN-and-ViT-for-TB-detection-in-lateral-CXR .",
  "full_text": "Detecting Tuberculosis-Consistent\nFindings in Lateral Chest X-Rays\nUsing an Ensemble of CNNs and Vision\nTransformers\nSivaramakrishnan Rajaraman1*, Ghada Zamzmi1, Les R. Folio2 and Sameer Antani1\n1Computational Health Research Branch, National Library of Medicine, National Institutes of Health, Bethesda, MD,\nUnited States, 2Mofﬁtt Cancer Center, Tampa, FL, United States\nResearch on detecting Tuberculosis (TB)ﬁndings on chest radiographs (or Chest X-rays: CXR)\nusing convolutional neural networks (CNNs) has demonstrated superior performance due to\nthe emergence of publicly available, large-scale datasets with expert annotations and\navailability of scalable computational resources. However, these studies use only the\nfrontal CXR projections, i.e., the posterior-anterior (PA), and the anterior-posterior (AP)\nviews for analysis and decision-making. Lateral CXRs which are heretofore not studied\nhelp detect clinically suspected pulmonary TB, particularly in children. Further, Vision\nTransformers (ViTs) with built-in self-attention mechanisms have recently emerged as a\nviable alternative to the traditional CNN s. Although ViTs demonstrated notable\nperformance in several medical image analysis tasks, potential limitations exist in terms of\nperformance and computational efﬁciency, between the CNN and ViT models, necessitating a\ncomprehensive analysis to select appropriatemodels for the problem under study. This study\naims to detect TB-consistentﬁndings in lateral CXRs by constructing an ensemble of the CNN\nand ViT models. Several models are trained on lateral CXR data extracted from two large public\ncollections to transfer modality-speciﬁc knowledge andﬁne-tune them for detectingﬁndings\nconsistent with TB. We observed that the weighted averaging ensemble of the predictions of\nCNN and ViT models using the optimal weights computed with the Sequential Least-Squares\nQuadratic Programming method delivered signiﬁcantly superior performance (MCC: 0.8136,\n95% conﬁdence intervals (CI): 0.7394, 0.8878,p < 0.05) compared to the individual models\nand other ensembles. We also interpreted thedecisions of CNN and ViT models using class-\nselective relevance maps and attention maps, respectively, and combined them to highlight\nthe discriminative image regions contributing to theﬁnal output. We observed that (i) the model\naccuracy is not related to disease region of interest (ROI) localization and (ii) the bitwise-AND of\nthe heatmaps of the top-2-performing models delivered signiﬁcantly superior ROI localization\nperformance in terms of mean average precision [mAP@(0.1 0.6) = 0.1820, 95% CI:\n0.0771,0.2869,p < 0.05], compared to other individual models and ensembles. The code\nis available at https://github.com/sivaramakrishnan-rajaraman/Ensemble-of-CNN-and-ViT-\nfor-TB-detection-in-lateral-CXR.\nKeywords: chest radiographs, CNN, deep learning, tuberculosis classiﬁcation and localization, vision transformers,\nensemble learning, signiﬁcance analysis\nEdited by:\nTapas Si,\nBankura Unnayani Institute of\nEngineering, India\nReviewed by:\nUtpal Nandi,\nVidyasagar University, India\nWellington Pinheiro dos Santos,\nFederal University of Pernambuco,\nBrazil\n*Correspondence:\nSivaramakrishnan Rajaraman\nsivaramakrishnan.rajaraman@\nnih.gov\nSpecialty section:\nThis article was submitted to\nComputational Genomics,\na section of the journal\nFrontiers in Genetics\nReceived: 28 January 2022\nAccepted: 10 February 2022\nPublished: 24 February 2022\nCitation:\nRajaraman S, Zamzmi G, Folio LR and\nAntani S (2022) Detecting\nTuberculosis-Consistent Findings in\nLateral Chest X-Rays Using an\nEnsemble of CNNs and\nVision Transformers.\nFront. Genet. 13:864724.\ndoi: 10.3389/fgene.2022.864724\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 8647241\nORIGINAL RESEARCH\npublished: 24 February 2022\ndoi: 10.3389/fgene.2022.864724\n1 INTRODUCTION\nArtiﬁcial intelligence (AI) methods, particularly deep learning\n(DL)-based convolutional neural network (CNN) models, have\ndemonstrated remarkable performance in natural and medical\ncomputer vision applications (Schmidhuber, 2015). Considering\nchest-X-ray (CXR) analysis, CNN models have outperformed\nconventional machine learning (ML) methods for semantic\nsegmentation, classiﬁcation, and object detection, among other\ntasks (Wang et al., 2017; Irvin et al., 2019; Bustos et al., 2020).\nResearch on detecting Tuberculosis (TB)-consistentﬁndings\nin CXRs using DL methods has demonstrated superior\nperformance due to the emergence of publicly available, large-\nscale datasets with expert annotations and availability of scalable\ncomputational resources ( Jaeger et al., 2014 ; Lakhani and\nSundaram, 2017; Sivaramakrishnan et al., 2018 ; Pasa et al.,\n2019; Rajaraman and Antani, 2020). However, these studies\nonly use the frontal CXR projections, i.e., the posterior-\nanterior (PA), and the anterior-posterior (AP) views, for\nanalysis and decision-making. To the best of our knowledge,\nlateral CXR projections have, heretofore, not been used for AI\ndetection approaches to pulmonary diseases before this work.\nLateral CXR projections of children with clinically suspected\npulmonary TB, in addition to the conventional frontal\nprojections, are critical and showed an increase in the\ndetection sensitivity of enlarged lymph nodes by 1.8% and\nspeciﬁcity by 2.5% (Swingler et al., 2005). Further, the World\nHealth Organization (WHO) recommends the use of lateral CXR\nprojections to identify mediastinal or hilar lymphadenopathy\n(World Health Organization, 2016 ), especially in younger\nchildren with primary TB where a bacteriological conﬁrmation\nmight be challenging. As discussed in (Gaber et al., 2005), lateral\nCXRs provide useful spatial diagnostic information on the\nthoracic cage, pleura, lungs, pericardium, heart, mediastinum,\nand upper abdomen and help identify lymphadenopathy in\nchildren with primary TB (Gaber et al., 2005). Another study\n(Herrera Diaz et al., 2020 ) discusses the current national\nCanadian guidelines suggesting using lateral CXR projections\nfor TB screening upon admission to long-term care facilities.\nThese studies underscore the importance of using lateral CXR\nprojections as they carry useful information on disease\nmanifestation and progression; hence, this study aims to\nexplore these least studied types of CXR projection (the\nlateral) and propose a novel approach for detecting TB-\nconsistent ﬁndings.\nRecently, Vision Transformers (ViTs) (Zhai et al., 2021) with\nbuilt-in self-attention mechanisms have demonstrated\ncomparable performance to CNNs in natural and medical\nvisual recognition tasks, while requiring fewer computational\nresources. Several studies (Liu and Yin, 2021; Shome et al.,\n2021; Park et al., 2022 ) used ViTs to improve pulmonary\ndisease detection in frontal CXRs to detect manifestations\nconsistent with COVID-19 disease. Another study ( Duong\net al., 2021) used a ViT model to detect TB-consistentﬁndings\nin frontal CXRs and obtained an accuracy of 97.72%. The\npromising performance of ViT models in medical visual\nrecognition tasks is constrained by sparse data availability\n(Zhai et al., 2021 ). Unlike CNN models, ViT models lack\nintrinsic biases, i.e., the properties of translation equivariance,\nwhich is the similarity in processing different image parts\nregardless of their absolute position, and they do not consider\nthe relationship between the neighboring image pixels. Further,\nthe computational complexity of ViT models increases with the\ninput image resolution resulting in demand for a higher resource.\nIn contrast, CNN models have shown promising performance\neven with limited data due to their inherent inductive bias\ncharacteristics that help in convergence and generalization.\nHowever, CNN models do not encode the relative position of\ndifferent image features and may require large receptiveﬁelds to\nencode the combination of these features and capture long-range\ndependencies in an input image. This leads to increased\nconvolutional kernel sizes and subsequently the computational\ncomplexity (Alzubaidi et al., 2021). A potential solution could be\nto exploit the advantages of both models, i.e., CNNs and ViTs\ntoward decision-making for the task under study.\nSeveral ensemble methods including majority voting,\naveraging, weighted averaging, and stacking, have been studied\nfor medical visual recognition tasks ( Dietterich, 2000 ).\nConsidering CXR analysis, particularly TB detection, ensemble\nmethods have been widely used to improve performance in\nsemantic segmentation, classi ﬁcation, and object detection\ntasks (Hogeweg et al., 2010\n; Ding et al., 2017; Islam et al.,\n2017; Rajaraman et al., 2018a; Rajaraman and Antani, 2020).\nHowever, to the best of our knowledge, we are not aware of\nstudies that perform an ensemble of ViTs or an ensemble of both\nCNN and ViT models for disease detection, particularly detecting\nTB-consistent ﬁndings using lateral CXRs. The main\ncontribution of this work is a systematic approach that\nbeneﬁts from constructing ensembles of the best models from\nboth worlds (i.e., CNNs and ViTs) to detect TB-consistent\nﬁndings using lateral CXRs through reduced prediction\nvariance and improved performance.\nThe steps in this systematic study can be summarized as\nfollows: (i) First, ImageNet-pretrained CNN models, viz,\nVGG-16 ( Simonyan and Zisserman, 2015 ), DenseNet-121\n(Huang et al., 2017 ), and Ef ﬁcientNet-V2-B0 (Tan and Le,\n2021) and the ImageNet-pretrained ViT models, viz, ViT-B/\n16, ViT-B/32, ViT-L/16, and ViT-L/32 (Zhai et al., 2021) are\nretrained on a combined selection of publicly available lateral\nCXR collections (Rajpurkar et al., 2017; Bustos et al., 2020). This\nstep is performed to convert the weight layers speciﬁc to the\nlateral CXR modality and learn to classify normal and abnormal\nlateral CXRs; (ii) Next, the retrained models are used to transfer\nthe lateral CXR modality-speci ﬁc knowledge to improve\nperformance in the related task of classifying lateral CXRs as\nshowing no abnormalities or otherﬁndings that are consistent\nwith TB; (iii)The predictions of the top-K (K = 2, 3, 5, 7) models\nare combined using several ensemble methods such as majority\nvoting, simple averaging, and weighted averaging using the\noptimal weights derived with the Sequential Least-Squares\nQuadratic Programming (SLSQP) algorithm ( Gupta and\nGupta, 2018). We construct a “model-level” ensemble of the\nCNN and ViT models byﬂattening, concatenating the features\nfrom their deepest layers, and adding the classiﬁcation layers to\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 8647242\nRajaraman et al. Ensemble Models for Tuberculosis Detection\nclassify the lateral CXRs to their respective categories; (iv) We\nalso interpret CNN and ViT model decisions through the use of\nclass-selective relevance maps (CRM) (Kim et al., 2019) and\nattention maps, respectively, and construct an ensemble of\nthese heatmaps and attention maps using several ensemble\nmethods. Finally, we analyze and report statistical signiﬁcance\nin the results obtained using the individual models and their\nensembles using conﬁdence intervals (CIs) andp values.\n2 MATERIALS AND METHODS\n2.1 Datasets\nThe following publicly available datasets are used in this study:\nCheXpert CXR dataset: The authors in (Irvin et al., 2019)\nreleased a collection of frontal and lateral CXR projections,\nshowing normal lungs, and other pulmonary abnormalities.\nThe dataset contains 224,316 CXRs collected from 65,240\npatients at the Stanford University Hospital in California. The\nCXRs are labeled using a natural language processing (NLP)-\nbased automatic labeler for the presence of 14 thoracic\nabnormalities mentioned in radiological reports. The collection\nincludes 23,633 lateral CXRs manifesting various pulmonary\nabnormalities and 4,717 lateral CXRs showing no\nabnormalities. In this study, the lateral CXR projections are\nsplit at the patient level into 90/10 proportions for the train\nand test sets and are used during CXR modality-speci ﬁc\npretraining.\nPadChest CXR dataset: A collection of 160,000 frontal and\nlateral CXRs and their associated radiological reports are released\nby (Bustos et al., 2020). The collection includes normal and\nabnormal CXRs collected from 67,000 patients at the San Juan\nHospital in Spain. The CXR images are automatically labeled for\n174 radiographic ﬁndings, based on the Uni ﬁed Medical\nLanguage System (UMLS) terminology. The collection includes\n33,454 lateral CXRs manifesting several pulmonary abnormalities\nand 14,229 lateral CXRs showing no abnormalities. The abnormal\nlateral CXR collection also includes 530 CXRs collected from\npatients diagnosed with TB. The set of CXRs manifesting TB-\nconsistent ﬁndings and an equal number of lateral CXRs with no\nabnormalities are used during theﬁne-tuning. The ground truth\nannotations for the hold-out test set consisting of 53 images, and\nshowing ﬁndings that are consistent with TB, are provided by an\nexpert radiologist (with>30 years of experience). The radiologist\nused the web-based VGG Image Annotator tool (VIA, Oxford,\nEngland) (Dutta and Zisserman, 2019 ) to annotate the test\ncollection by manually setting boundary boxes for what is\nbelieved to be TB-consistent ﬁndings. Table 1 shows the\ndatasets, the numbers of images, and their respective patient-\nlevel train/test splits used in this study. The lateral CXR images\nfrom the PadChest and CheXpert collections are resized to 224 ×\n224 pixel dimensions to reduce computational overhead.\n2.2 Classiﬁcation Models\nThe following CNN and ViT Models are used in this study: (i)\nVGG-16 (Simonyan and Zisserman, 2015); (ii) DenseNet-121\n(Huang et al., 2017); (iii) EfﬁcientNet-V2-B0 (Tan and Le, 2021);\n(iv) ViT-Base (B)/16 (Zhai et al., 2021); (v) ViT-B/32 (Zhai et al.,\n2021); (vi) ViT-Large (L)/16 (Zhai et al., 2021); and (vii) ViT-L/32\n(Zhai et al., 2021). The CNN models are selected based on their\nsuperior performance in CXR-based visual recognition tasks\n(Wang et al., 2017; Rajaraman et al., 2018b; Irvin et al., 2019;\nRajaraman et al., 2020a). The numbers 16 and 32 in the ViT\nmodels denote the size of input image patches. The length of the\ninput image patch sequence is inversely proportional to the\nsquare of the patch size. Thus, the ViT models with smaller\npatch sizes are computationally more expensive (Zhai et al.,\n2021). Interested readers are referred to (Wang et al., 2017;\nRajaraman et al., 2018b; Irvin et al., 2019; Rajaraman et al.,\n2020a; Zhai et al., 2021) for a detailed description of these\nmodels’architecture.\n2.3 CXR Modality-Speciﬁc Pretraining,\nFine-Tuning, and Ensemble Learning\nDuring CXR modality-speciﬁc pretraining, the CNN models are\ninstantiated with their ImageNet pretrained weights, truncated at\ntheir optimal intermediate layers (Rajaraman et al., 2020b), and\nappended with the following layers: (i) a zero-padding (ZP) layer,\n(ii) a convolutional layer with 512ﬁlters, each of size 3 × 3, (iii) a\nglobal averaging pooling (GAP) layer; and (iv) aﬁnal dense layer\nwith two nodes and Softmax activation. The optimal intermediate\nlayers are identiﬁed from pilot analyses for the task under study.\nThe ViT models are instantiated with their pretrained weights\nlearned from a combined selection of ImageNet and\nImagenet21K datasets. These models are then truncated at the\noutput classiﬁcation token layer and appended with aﬂattening\nlayer and aﬁnal dense layer with two nodes to output prediction\nprobabilities. Figure 1shows the block diagram of models used in\nCXR modality-speciﬁc pretraining andﬁne-tuning stages.\nThe CNN and ViT models are then retrained on a combined\nselection of lateral CXRs from the CheXpert and PadChest\ndatasets ( Table 1 ). This process is called CXR modality-\nspeciﬁc pretraining and it is performed to impart CXR\nTABLE 1 |Datasets and their respective patient-level train/test splits. Data in parenthesis denotes the 90/10 train/test splits. A part of the lateral CXRs in the PadChest CXR\ncollection that show no abnormalities and those with TB-consistent manifestations are used forﬁne-tuning. The rest of the data from the PadChest and CheXpert lateral\nCXR collections are used for CXR modality-speciﬁc pretraining.\nDataset CXR modality-speci ﬁc pretraining Fine-tuning\nAbnormal Normal TB Normal\nPadChest 32923 (29631/3292) 13698 (12328/1370) 530 (477/53) 530 (477/53)\nCheXpert 23633 (21270/2363) 4717 (4245/472) - -\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 8647243\nRajaraman et al. Ensemble Models for Tuberculosis Detection\nmodality-speciﬁc knowledge to (i) coarsely learn the\ncharacteristics of normal and abnormal lateral CXRs and (ii)\nconvert the weight layers learned from natural images to the input\nCXR modality. The modality-speciﬁc pretrained CNN and ViT\nmodels are thenﬁne-tuned to classify the lateral CXRs as showing\nno abnormalities or otherﬁndings that are consistent with TB.\nThe datasets are split at the patient level into 90% for training and\n10% for testing during the CXR modality-speciﬁc pretraining and\nﬁnetuning stages as shown inTable 1. We allocated 10% of the\ntraining data for validation with aﬁxed seed. The training data is\naugmented using afﬁne transformations such as rotation (−5, +5),\nhorizontal ﬂipping, width, and height shifting (−5, +5), and\nnormalized so the image pixel values lie in the range (0, 1).\nDuring CXR modality-speciﬁc pretraining, the CNN and ViT\nmodels are trained for 100 epochs, using a stochastic gradient\ndescent (SGD) optimizer with an initial learning rate of 1e-2 and\nmomentum of 0.9, to minimize the categorical cross-entropy loss.\nWe used callbacks to store model checkpoints and reduced the\nlearning rate whenever the validation loss ceased to decrease. The\nbest-performing model, delivering the least validation loss at the\nend of the training epochs is stored to predict the hold-out test set.\nDuring ﬁne-tuning, the CXR modality-speciﬁc pretrained models\nare ﬁnetuned using the SGD optimizer with an initial learning\nrate of 1e-4 and momentum of 0.9. We used callbacks for early\nstopping and learning rate reduction. The best-performing\nmodel, delivering the least validation loss at the end of the\ntraining epochs is stored to predict the hold-out test set.\nThe top-K (K = 2, 3, 5, 7)ﬁne-tuned models that deliver\nsuperior performance with the hold-out test set are used to\nconstruct ensembles. We constructed “prediction-level” and\nFIGURE 1 |A systematic approach of training the models during CXR modality-speciﬁc pretraining andﬁne-tuning stages. (A) ViTs and (B) CNNs.\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 8647244\nRajaraman et al. Ensemble Models for Tuberculosis Detection\n“model-level” ensembles. At the prediction level, we used several\nensemble strategies such as majority voting, simple averaging,\nand SLSQP-based weighted averaging to combine the top-K\nmodel predictions. For SLSQP-based weighted averaging, we\ncomputed the optimal weights by minimizing the total\nlogarithmic loss using the SLSQP algorithm ( Gupta and\nGupta, 2018 ) to help convergence. For the model-level\nensemble, the top-K models are instantiated with their ﬁne-\ntuned weights. The ViT models are truncated at the ﬂatten\nlayer. The CNN models are truncated at their deepest\nconvolutional layer and added with aﬂatten layer. The output\nfrom the ﬂattened layers of the ViT and CNN models are then\nconcatenated and appended with theﬁnal dense layer to output\nclass probabilities. The weights of trainable layers are frozen and\nonly the ﬁnal dense layer is trained to output probabilities of\nclassifying the lateral CXRs into normal or TB categories. The\nmodel-level ensemble is trained using an SGD optimizer and an\ninitial learning rate of 1e-5. Callbacks are used to store model\ncheckpoints and reduce the learning rate whenever the validation\nperformance did not improve. The best-performing model with\nthe least validation loss is stored to predict the hold-out test set.\nFigure 2 illustrates the construction of model-level ensembles\nusing theﬁne-tuned CNN and ViT models. The performance of\nthe models during CXR modality-speci ﬁc pretraining, ﬁne-\ntuning, and ensemble learning are evaluated using the\nfollowing metrics: (i) accuracy; (ii) area under the receiver-\noperating-characteristic curve (AUROC); (iii) area under the\nprecision-recall curve (AUPRC); (iv) precision; (v) recall; (vi)\nF-score; (vii) Matthews correlation coef ﬁcient (MCC), (viii)\nDiagnostic Odds Ratio (DOR), and (ix) Cohen’s Kappa. These\nmetrics are expressed inEqs 1–11.\nAccuracy /equals\nTP + TN\nTP + TN + FP + FN (1)\nRecall /equals TP\nTP + FN (2)\nPrecision /equals TP\nTP + FP (3)\nF − score /equals 2× Precision × Recall\nPrecision + Recall (4)\nMCC /equals TP × TN − FP × FN\n((TP + FP)(TP + FN)(TN + FP)(TN + FN))1/2 (5)\nDOR /equals (TP × TN)\n(FP × FN) (6)\nPo /equals (TP + TN)\n(TP + FP + FN + TN) (7)\nPtrue /equals (TP + FN)(TP + FP)\n(TP + FP + FN + TN)2 (8)\nPfalse /equals (FP + TN)(FN + TN)\n(TP + FP + FN + TN)2 (9)\nPe /equals Ptrue + Pfalse (10)\nCohen’s Kappa/equals (Po − Pe)\n1 − Pe\n(11)\nHere, TP, TN, FP, and FN denote the true positive, true\nnegative, false positive, and false negative values, respectively.\nThe models are trained and evaluated using Tensorﬂow Keras\nversion 2.6.2 on a Linux system with NVIDIA GeForce GTX 1080\nTi GPU, and CUDA dependencies for GPU acceleration.\n2.4 Model Explainability\nDL models are often criticized for their“black box” behavior,\ni.e., lack of explanations toward their predictions. This lack of\nexplainability could be attributed to (i) their architectural\ndepth that may not allow decomposability into explainable\ncomponents and (ii) the presence of non-linear layers that\nperform complex data transformations and result in non-\ndeterministic behavior that adversely impacts clinical\nFIGURE 2 |A model-level ensemble constructed usingﬁne-tuned CNN\nand ViT models.\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 8647245\nRajaraman et al. Ensemble Models for Tuberculosis Detection\ninterpretations. Methods have been proposed (Selvaraju et al.,\n2017) to explain model predictions by highlighting\ndiscriminative parts of the image that causes the model to\nclassify the images to their respective categories. In this study,\nwe used class-selective relevance maps (CRM) ( Kim et al.,\n2019) to discriminate image regions used by the ﬁne-tuned\nCNN models to categorize the CXRs as showing TB-consistent\nﬁndings. It has been reported that the CRM-based\nvisualization ( Kim et al., 2019 )o u t p e r f o r m e dt h e\nconventional gradient-based class activation maps ( Selvaraju\net al., 2017) in interpreting model predictions.\nWe computed the attention maps from theﬁne-tuned ViT\nmodels using the attention rollout method discussed in (Zhai\net al., 2021). The steps involved in computing the attention map\nconsists of (i) getting the attention weights from each transformer\nblock, (ii) averaging the attention weights across all the heads, (iii)\nadding an identity matrix to the attention matrix to account for\nresidual connections, (iv) re-normalizing the weights and\nrecursively multiplying the weight matrices to mix the\nattention across tokens through all the layers, and (v)\ncomputing the attention from the output token to the input\nspace. The bounding box coordinates of the heatmaps and\nattention maps are computed as follows: (i) A difference\nbinary image is generated using the original input lateral CXR\nimage and the heatmap/attention map-overlaid image; (ii) the\npolygonal coordinates of the connected components in the binary\nimage are measured that gives the coordinates of the vertices and\nthat of the line segments making up the sides of the polygon, and\n(iii) a binary mask is generated from the polygon and the\ncoordinates are stored for further analysis. The delineated\nROIs are compared against the ground truth annotations\nprovided by the radiologist.\nFor evaluating localization performance, we used several\nensemble methods, such as simple averaging, SLSQP-based\nweighted averaging, and a bitwise-AND of the heatmaps and\nattention maps of top-K performing models. In simple averaging,\nthe heatmaps and attention maps obtained respectively using the\nCNN and ViT models are averaged to produce theﬁnal heatmap,\nhighlighting discriminative ROIs toward TB detection. In SLSQP-\nbased weighted averaging, the optimal weights obtained using the\nSLSQP method are used while averaging the heatmaps and\nattention maps. In a bitwise-AND ensemble, the heatmaps and\nattention maps are binarized and bitwise-ANDed. The\ncorresponding pixel in the ﬁnal heatmap is activated only if\nthere is complete agreement among activations in the candidate\nheatmaps and attention maps. The ROI localization performance\nof the constituent models and their ensembles is measured in\nterms of the mean average precision (mAP) metric. The mAP is\ncalculated by taking the mean precision over 11 IoU threshold\nvalues within the range [0.1, 0.6] at equal intervals of 0.05\n[denoted as mAP@[0.1, 0.6]] (GTUA et al., 2014).\n2.5 Statistical Signiﬁcance Analysis\nIt has been reported in (Diong et al., 2018) that 90–96% of the\nstudies published in scientiﬁc journals do not measure statistical\nsigniﬁcance in the reported results, casting doubt on algorithm\nreliability and conﬁdence. In this study, we analyzed statistical\nsigniﬁcance using the 95% conﬁdence intervals (CIs) for the MCC\nmetric measured as the Clopper–Pearson binomial CI interval.\nFor RoI localization, we measured the 95% CIs measured as the\nClopper–Pearson binomial CI interval for the mAP metric\nachieved by the individual models and their ensembles to\nreport statistical signi ﬁcance. The StatsModels and SciPy\nPython packages are used in this analysis. We obtained thep-\nvalue from the CIs using the methods reported in (Altman and\nBland, 2011). Considering the upper and lower limits of the 95%\nCI asu and l respectively, the standard error (SE) is measured as\ngiven in Eq. 12.\nSE /equals (u − l)(2×1 .96) (12)\nThe test statisticz is given byEq. 13\nz /equals\nDiff\nSE (13)\nHere, Diff denotes the estimated differences between the\nmodels for the measured metric.\nThe p-value is then calculated as given inEq 14.\np /equals exp( − 0.717 ×z − 0.416 ×z2) (14)\n3 RESULTS\n3.1 CXR Modality-Speciﬁc Pretraining and\nFine-Tuning\nRecall that the CNN and ViT models are instantiated with their\nImageNet-pretrained weights and retrained on a combined\nselection of lateral CXRs from the CheXpert and PadChest\ndatasets. The test performance achieved during CXR modality-\nspeciﬁc pretraining is shown in Table 2. From Table 2,w e\nobserved the following: (i) The training time for CNN models\nis comparatively small than ViT models. The EfﬁcientNet-V2-B0\nmodel took the least while the ViT-L/16 model took the most time\nfor training and convergence. (ii) The VGG-16 model\ndemonstrated superior performance in terms of accuracy,\nF-score, MCC, DOR, Kappa, AUROC, and AUPRC metrics.\nThe EfﬁcientNet-V2-B0 model demonstrated superior recall\nand ViT-B/32 demonstrated superior precision compared to\nother models. However, considering a balanced measure of\nprecision and recall, as provided by the MCC metric, the\nVGG-16 model demonstrated superior performance compared\nto other models. (iii) We observed that the 95% CIs obtained for\nthe MCC metric using the VGG-16 model are not signiﬁcantly\ndifferent (p > 0.05) from other models. Due to this lack of\nstatistical signiﬁcance, all modality-speciﬁc pretrained models\nare ﬁne-tuned to evaluate performance in the TB classiﬁcation\ntask. Table 3shows the performance achieved by theﬁne-tuned\nmodels that classify the lateral CXRs as showing no abnormalities\nor other abnormalities that are consistent with TB.\nThe following are observed fromTable 3: (i) The CNN models\ntook comparatively lesser time to converge than the ViT models.\nThis observation is analogous to CXR modality-speci ﬁc\npretraining. (ii) The DenseNet-121 model demonstrated\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 8647246\nRajaraman et al. Ensemble Models for Tuberculosis Detection\nsuperior performance in terms of accuracy, precision, F-score,\nMCC, DOR, Kappa, AUROC, and AUPRC metrics. The ViT-L/\n16 model demonstrated superior recall compared to other\nmodels. However, considering the MCC metric, the DenseNet-\n121 model demonstrated superior performance compared to\nother models. (iii) The 95% CIs for the MCC metric achieved\nTABLE 2 |Test performance achieved by the CNN and ViT models during lateral CXR modality-speciﬁc pretraining. The values in parenthesis denote the 95% CI measured\nas the Clopper–Pearson binomial interval for the MCC metric. Bold numerical values denote superior performance.\nModel Accuracy Recall Precision F MCC DOR Kappa AUROC AUPRC Training\ntime\n(seconds)\nViT-B/16 0.7747 0.7988 0.8913 0.8425 0.4596 (0.3647,0.5545) 9 0.4512 0.8276 0.9334 17582.14\nViT-B/32 0.7394 0.7151 0.9218 0.8054 0.4621 (0.3671,0.5571) 11 0.4293 0.8375 0.9375 10739.29\nViT-L/16 0.7678 0.7846 0.8946 0.8360 0.4555 (0.3606,0.5504) 9 0.4442 0.8276 0.9332 54949.73\nViT-L/32 0.7872 0.8324 0.8792 0.8552 0.4584 (0.3635,0.5533) 9 0.4560 0.8364 0.9373 28797.83\nEfﬁcientNet-V2-B0 0.7794 0.8391 0.8645 0.8516 0.4231 (0.3290,0.5172) 8 0.4223 0.8152 0.9281 2296.54\nVGG-16 0.8009 0.8361 0.8931 0.8637 0.4998 (0.4046,0.5950) 11 0.4960 0.8526 0.9441 9316.52\nDenseNet-121 0.7886 0.8230 0.8885 0.8545 0.4747 (0.3796,0.5698) 10 0.4701 0.8401 0.9393 7281.22\nTABLE 3 |Performance achieved by theﬁne-tuned models toward the TB classiﬁcation task. The values in parenthesis denote the 95% CI measured as the\nClopper–Pearson binomial interval for the MCC metric. Bold numerical values denote superior performance.\nModel Accuracy Recall Precision F MCC DOR Kappa AUROC AUPRC Training\ntime\n(seconds)\nViT-B/16 0.7642 0.6792 0.8182 0.7422 0.5361 (0.4411,0.6311) 12 0.5283 0.8548 0.8668 828.30\nViT-B/32 0.8302 0.7547 0.8889 0.8163 0.6680 (0.5783,0.7577) 30 0.6604 0.9227 0.9351 338.46\nViT-L/16 0.8302 0.8302 0.8302 0.8302 0.6604 (0.5702,0.7506) 24 0.6604 0.8943 0.9084 1539.06\nViT-L/32 0.7736 0.7170 0.8085 0.7600 0.5507 (0.4560,0.6454) 12 0.5472 0.8786 0.8911 574.24\nEfﬁcientNet-V2-B0 0.8019 0.6981 0.8810 0.77900 0.6172 (0.5246,0.7098) 22 0.6038 0.8896 0.9025 114.89\nVGG-16 0.8208 0.7358 0.8864 0.8041 0.6510 (0.5602,0.7418) 27 0.6415 0.9110 0.9219 267.40\nDenseNet-121 0.8585 0.8113 0.8958 0.8515 0.7202 (0.6347,0.8057) 41 0.7170 0.9288 0.9423 313.44\nFIGURE 3 |Performance curves achieved by the models used in this study. CXR modality-speciﬁc pretraining (VGG-16):(A) AUROC; (B) AUPRC; (C) Confusion\nmatrix. Fine-tuning (DenseNet-121):(D) AUROC; (E) AUPRC, and (F) Confusion matrix.\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 8647247\nRajaraman et al. Ensemble Models for Tuberculosis Detection\nby the DenseNet-121 model demonstrated a tighter error margin,\nhence higher precision, compared to other models. We observed\nthat the MCC metric achieved by the DenseNet-121 model is\nsigniﬁcantly superior to ViT-B/16 (p = 0.0001), ViT-L/32 (p =\n0.0002), and EfﬁcientNet-V2-B0 (p = 0.0183) models. We also\nobserved that the MCC metric achieved by the VGG-16 model is\nsigniﬁcantly superior to the ViT-B/16 (p = 0.0133) and ViT-L/32\n(p = 0.0304) models. These observations underscore the fact that\nthe CNN models delivered superior classiﬁcation performance\ncompared to the ViT models. Figure 3 shows the AUROC,\nAUPRC, and confusion matrices achieved by the VGG-16 and\nDenseNet-121 models during the CXR modality-speci ﬁc\npretraining and ﬁne-tuning stages, respectively. A no-skill\nclassiﬁer fails to discriminate between the classes and would\npredict a random or a constant class in all circumstances.\nThe ensemble of the top-K models (K = 2, 3, 5, 7) is\nconstructed to evaluate any improvement in classi ﬁcation\nperformance during ﬁne-tuning. Table 4 shows the\nperformance achieved using various ensemble methods\ndiscussed in this study. From Table 4, we observe that the\nperformance obtained through SLSQP-based weighted\naveraging is comparatively higher than other ensembles and\ntheir constituent models. This demonstrates that, unlike using\nequal weights, the use of optimal weights to combine the\npredictions of constituent models improved classi ﬁcation\nperformance. (ii) The SLSQP-based weighted averaging\n[optimal weights: (0.65, 0.35)] of the predictions of the top-2\nﬁne-tuned models, viz. DenseNet-121 and ViT-B/32 delivered\nsuperior performance in terms of accuracy, Kappa, and\nsigniﬁcantly superior performance in terms of the MCC metric\n(0.8136, 95% CI: (0.7394, 0.8878)) compared to its constituent\nmodels, viz. DenseNet-121 (p = 0.0137), and ViT-B/32 (p =\n0.0002). This ensemble also demonstrated signi ﬁcantly\nsuperior performance in terms of MCC metric compared to\nother models, viz. VGG-16 (p = 0.0001), EfﬁcientNet-V2-B0\n(p = 0.0001), ViT-B/16 (p = 0.0001), ViT-L/16 (p = 0.0001),\nTABLE 4 |Test performance obtained using prediction-level and model-level ensembles. The values in parenthesis denote 95% CI for the MCC metric measured as the\nClopper-Pearson binomial interval. Bold numerical values denote superior performance.\nEnsemble Models Accuracy Recall Precision F-score MCC DOR Kappa AUROC AUPRC Training\ntime\n(seconds)\nMajority voting Top-2 0.8774 0.8868 0.8704 0.8785 0.7549 (0.6730,0.8368) 51 0.7547 0.8774 0.9069 NA\nTop-3 0.8679 0.8302 0.898 0.8628 0.738 (0.6542,0.8218) 47 0.7358 0.8679 0.9065 NA\nTop-5 0.8585 0.7925 0.913 0.8485 0.7233 (0.6381,0.8085) 47 0.717 0.8585 0.9046 NA\nTop-7 0.8585 0.7925 0.913 0.8485 0.7233 (0.6381,0.8085) 47 0.717 0.8585 0.9046 NA\nSimple averaging Top-2 0.8679 0.8113 0.9149 0.86 0.7406 (0.6571,0.8241) 53 0.7358 0.9388 0.9525 NA\nTop-3 0.8491 0.8113 0.8776 0.8431 0.7001 (0.6128,0.7874) 34 0.6981 0.9377 0.9515 NA\nTop-5 0.8679 0.8113 0.9149 0.86 0.7406 (0.6571,0.8241) 53 0.7358 0.937 0.949 NA\nTop-7 0.8396 0.7925 0.875 0.8317 0.6823 (0.5936,0.7710) 30 0.6792 0.9313 0.9441 NA\nSLSQP-weighted averaging Top-2 0.9057 0.8679 0.9388 0.902 0.8136 (0.7394,0.8878) 110 0.8113 0.9409 0.9542 NA\nTop-3 0.9057 0.8868 0.9216 0.9039 0.8119 (0.7375,0.8863) 96 0.8113 0.9352 0.9492 NA\nTop-5 0.8962 0.8491 0.9375 0.8911 0.796 (0.7192,0.8728) 94 0.7925 0.9388 0.952 NA\nTop-7 0.9057 0.8679 0.9388 0.902 0.8136 (0.7394,0.8878) 110 0.8113 0.937 0.9503 NA\nModel-level Top-2 0.8962 0.8113 0.9773 0.8866 0.8041 (0.7285,0.8797) 223 0.7925 0.9491 0.9587 91.4263\nTop-3 0.8679 0.7736 0.9535 0.8542 0.7493 (0.6667,0.8319) 87 0.7358 0.9274 0.9433 418.05\nTop-5 0.8679 0.7736 0.9535 0.8542 0.7493 (0.6667,0.8319) 87 0.7358 0.9427 0.9525 555.088\nTop-7 0.8585 0.7547 0.9524 0.8421 0.7329 (0.6486,0.8172) 79 0.717 0.9366 0.9493 758.957\nFIGURE 4 |Performance curves achieved using SLSQP-based weighted averaging of the predictions of top-2ﬁne-tuned models, i.e., DenseNet-121, and ViT-B/\n32 models. (A) AUROC; (B) Confusion matrix, and(C) AUPRC.\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 8647248\nRajaraman et al. Ensemble Models for Tuberculosis Detection\nand ViT-L/32 (p = 0.0001) models. The model-level ensemble of\nthe top-2 ﬁne-tuned models, i.e., DenseNet-121 and ViT-B/32\ndemonstrated superior values for the DOR metric. Figure 4\nshows the AUROC, AUPRC, and confusion matrices achieved\nby the SLSQP-based weighted averaging of the predictions of the\ntop-2 ﬁne-tuned models.\n3.2 Evaluating TB-Consistent ROI\nLocalization Performance\nAs described inSection 2.4, we use CRMs and attention maps to\ninterpret the predictions of the CNN and ViT models,\nrespectively. The delineated ROIs are compared against the\nground truth annotations provided by the radiologist.Figure 5\nshows a sample lateral CXR with expert-annotated ROI\nconsistent with TB and the discriminative ROIs highlighted by\nthe ﬁne-tuned CNN and ViT models discussed in this study.\nTable 5shows the TB-consistent ROI localization performance in\nterms of mAP metric, achieved by the individual models.\nFurther, we constructed ensembles of the heatmaps of the top-\n2 models fromTable 5, viz. VGG-16 and DenseNet-121 models\nusing simple averaging, SLSQP-based weighted averaging, and\nbitwise-AND techniques. Figure 6 shows the box plots for the\nrange of mAP values achieved by the individual models and other\nensembles. Table 6 shows the TB-consistent ROI localization\nperformance achieved in terms of the mAP metric by the model\nensembles.\nFrom Figure 6 , we observe that the maximum, mean,\nmedian, the total range, and the inter-quartile range of the\nmAP values achieved with the Bitwise-AND ensemble is\nsigniﬁcantly higher (p < 0.05) than those obtained with the\nViT models and considerably higher than the averaging and\nweighted averaging ensembles. From Table 6, we observe that\nall ensemble methods demonstrated superior values for the\nm A Pm e t r i cc o m p a r e dt ot h ei n d i v i d u a lm o d e l s(Table 5). The\nbitwise-AND operation resulte d in superior values for the\nmAP metric compared to the constituent models, other\nmodels, and ensembles. The mAP metric achieved by the\nbitwise-AND ensemble is observed to be signi ﬁcantly\nsuperior to ViT-B/16, ViT-L/16, ViT-L/32 ( p = 0.0199),\nViT-B/32 (p = 0.0193), and EfﬁcientNet-V2-B0 ( p = 0.0014)\nmodels. This performance is followed by the SLSQP-based\nweighted averaging ensemble that demonstrated signiﬁcantly\nFIGURE 5 |TB-consistent ROI localization achieved using theﬁne-tuned models.(A) An instance of lateral CXR with expert-annotated ROI consistent with TB\n(shown with a red bounding box);(B) VGG-16; (C) DenseNet-121; (D) EfﬁcientNet-V2-B0; (E) ViT-B/16; (F) ViT-B/32; (G) ViT-L/16, and (H) ViT-L/32.\nTABLE 5 |TB-consistent ROI localization performance achieved by theﬁne-tuned\nCNN and ViT models. The values in parenthesis denote the 95% CI measured\nas the Clopper-Pearson binomial interval for the mAP metric. Bold numerical\nvalues denote superior performance.\nModel mAP@[0.1, 0.6]\nViT-B/16 0.0573 (0,0.1205)\nViT-B/32 0.0567 (0,0.1196)\nViT-L/16 0.0573 (0,0.1205)\nViT-L/32 0.0573 (0,0.1205)\nEfﬁcientNet-V2-B0 0.0690 (0.0001,0.1379)\nVGG-16 0.1283 (0.0374,0.2192)\nDenseNet-121 0.1052 (0.0218,0.1886)\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 8647249\nRajaraman et al. Ensemble Models for Tuberculosis Detection\nsuperior localization performance compared to ViT-B/16,\nViT-L/16, ViT-L/32 ( p = 0.0264), and Ef ﬁcientNet-V2-B0\n(p = 0.0029) models. Figure 7 shows a Bitwise-AND\nensemble of the heatmaps produced by the top-2 models,\nviz. VGG-16 and DenseNet-121 models, for instances of test\nimages.\n4 DISCUSSION\nFollowing ﬁndings from our pilot studies which are consistent\nwith prior observations [34], the ImageNet-pretrained CNNs\nwith their total depth and the ImageNet-pretrained ViT\nmodels demonstrated sub-optimal performance toward the\ntask of TB detection. Therefore, we truncated the\nImageNet-pretrained CNN models at their optimal\nintermediate layers, appended them with the classi ﬁcation\nlayers. Further, instead of using ImageNet weights learned\nf r o ms t o c kp h o t o g r a p h i ci m a g e sw et r a i n e dt h eC N Na n d\nViT models on a large-scale collection of lateral CXR data.\nThese CXR modality-speci ﬁc pretrained weights serve as a\npromising initialization to promote modality-speci ﬁc\nknowledge transfer and improved adaptation and\nperformance of the models in the relevant task of detecting\nTB-consistent manifestations.\nFrom our ﬁndings and evaluation results, we observe that\nthe ViT models demonstrate sub-optimal classi ﬁcation and\nROI localization performance and signiﬁcantly higher training\ntime, compared to the CNN-based DL models. Theseﬁndings\nconﬁrm our suspicion that these may be due to the lack of\nintrinsic inductive biases. On the other hand, CNN models\nshow superior performance at lower training times even with\nour limited dataset. Even though CheXpert and PadChest data\nsets have a cumulative of over 384,316 CXRs only 76,033\nlateral CXRs are found in them with only 530 lateral CXRs\n(0.13% of the total number of lateral CXRs) exhibiting\nmanifestations consistent with TB. This could be a\nsigniﬁcant factor in the sub-optimal performance exhibited\nby the ViT models. We improved both classiﬁcation and ROI\nlocalization performance, qualitatively and quantitatively,\nusing CXR modality-speci ﬁc training, ﬁne-tuning, and\nconstructing model ensembles. This performance\nimprovement with ensemble learning is consistent with the\nliterature (He et al., 2016; Rajaraman et al., 2018a; Rajaraman\net al., 2019).\nWe also show that classiﬁcation performance is not indicative\nof reliable disease prediction. For example, even though the\naverage classi ﬁcation performance of ViT models is\napproximately 80%, their average MAP score is only 5.7%\nwhich is evident from the visualization studies, examples of\nwhich are shown inFigures 5E– H. This underscores the need\nfor visualization of localized disease prediction regions to verify\nmodel credibility.\nRegarding the use of ensembles, weﬁnd in the literature a\nfrequent use of methods such as majority voting, simple\naveraging, and weighted averaging with equal eights.\nHowever, we show that using optimized weighting using\nFIGURE 6 |Box plots showing the range of mAP values obtained by the individual models and other ensembles.\nTABLE 6 |TB-consistent ROI localization performance achieved by the model\nensembles. The values in parenthesis denote the 95% CI measured as the\nexact Clopper-Pearson binomial interval for the mAP metric. Bold numerical values\ndenote superior performance.\nModel mAP@[0.1, 0.6]\nSimple averaging 0.1332 (0.0408,0.2256)\nSLSQP-weighted averaging 0.1742 (0.0711,0.2773)\nBitwise-AND 0.1820 (0.0771,0.2869)\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 86472410\nRajaraman et al. Ensemble Models for Tuberculosis Detection\nspecialized techniques, such as SLSQP, result in signiﬁcantly\nsuperior classiﬁcation performance, e.g., the SLSQP accuracy\nachieved with the top-2 models is 0.9057 compared to 0.8679\nfor simple averaging (p = 0.0001). Similar behavior is observed\nfor localization performance as well.\nOur study has the following limitations: (i) Lateral CXRs\nhelp con ﬁrm abnormal opaci ﬁcation spatial location,\nhowever, have more overlapping structures (e.g., shoulders\nincluding scapula and humeral heads), decreasing conspicuity\nrelative to frontal projections. Given that there are more\nfrontal projection CXRs available with TB manifestations,\nwe provide an avenue to explore the combination including\nlateral images that we believe will improve performance. (ii)\nThere are a very small number of lateral CXRs with TB-\nconsistent ﬁndings available for ﬁne-tuning the models which\nhave, very likely, affected the sub-par performance of ViT\nmodels as they demand more training data and training time\ndue to their functional characteristics. We expect that the\nperformance of the models would scale with increased data\nand appropriate empowerment of computational resources.\n(iii) There is also an imbalance in the number of left or right\nlateral CXRs in an already small dataset of 530 TB disease-\npositive images. On the positive side, through augmentation,\nensemble learning, and optimized weighting of model\npredictions, we were able to achieve a lateral-view agnostic\nperformance that was signi ﬁcantly high. However, it is\nimportant to consider that the anatomical view presented\nin a left lateral image is different from the one presented in the\nother. For clinical diagnostic or screening applications, it\nwould be necessary to train the classi ﬁer on these\ndifferences so that a reliable and robust interpretation of\nthe prediction can be obtained. Further, research is\nongoing in building combination model architectures like\nConViT (d’Ascoli et al., 2021) that combines characteristics\nof the CNN and ViT architectures toward improving\nperformance. Such models should be studied in future studies.\nDATA AVAILABILITY STATEMENT\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAUTHOR CONTRIBUTIONS\nSR: Conceptualization, Data curation, Formal analysis, Methodology,\nSoftware, Validation, Visualization, Writing — original draft,\nWriting— review and editing; GZ: Conceptualization,\nFIGURE 7 |A Bitwise-AND ensemble generated using the heatmaps produced by the top-2 performing models, viz. VGG-16 and DenseNet-121 models.(A) and\n(E) Sample test lateral CXRs with expert ground truth annotations (shown in red bounding box);(B) and (F) Heatmaps produced by the VGG-16 model;(C) and (G)\nHeatmaps produced by the DenseNet-121 model, and(D) and (H) Mask resulting from the Bitwise-AND operation of the heatmaps produced by the VGG-16 and\nDenseNet-121 models.\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 86472411\nRajaraman et al. Ensemble Models for Tuberculosis Detection\nMethodology, Writing— review and editing; LF: Data curation (lateral\nCXR annotations), Writing — review and editing; SA:\nConceptualization, Formal analysis, Funding acquisition,\nInvestigation, Project administration, Resources, Supervision,\nValidation, Writing— review and editing.\nFUNDING\nThis study is supported by the Intramural Research Program (IRP)\nof the National Library of Medicine (NLM) and the National\nInstitutes of Health (NIH).\nREFERENCES\nAltman, D. G., and Bland, J. M. (2011). How to Obtain the P Value from a\nConﬁdence Interval. BMJ 343, d2304. doi:10.1136/bmj.d2304\nAlzubaidi, L., Zhang, J., Humaidi, A. J., Al-Dujaili, A., Duan, Y., Al-Shamma, O.,\net al. (2021). Review of Deep Learning: Concepts, CNN Architectures,\nChallenges, Applications, Future Directions. J. Big Data 8, 53. doi:10.1186/\ns40537-021-00444-8\nBustos, A., Pertusa, A., Salinas, J.-M., and de la Iglesia-Vayá, M. (2020). PadChest:\nA Large Chest X-ray Image Dataset with Multi-Label Annotated Reports.Med.\nImage Anal. 66, 101797. doi:10.1016/j.media.2020.101797\nd’Ascoli, S., Touvron, H., Leavitt, M., Morcos, A., Biroli, G., and Sagun, L. (2021).\nConViT: Improving Vision Transformers with Soft Convolutional Inductive\nBiases. Available from: http://arxiv.org/abs/2103.10697.\nDietterich, T. G. (2000). Ensemble Methods in Machine Learning.Mult. Classif\nSyst. 1857, 1–15. doi:10.1007/3-540-45014-9_1\nDing, M., Antani, S., Jaeger, S., Xue, Z., Candemir, S., Kohli, M., et al. (2017).\n“Local-global Classi ﬁer Fusion for Screening Chest Radiographs,” in Proc.\nOf SPIE Medical Imaging 2017: Imaging Informatics for Healthcare,\nResearch, and Applications . Editors J. Z. Tessa and S. Cook. doi:10.1117/\n12.2252459\nD i o n g ,J . ,B u t l e r ,A .A . ,G a n d e v i a ,S .C . ,a n dH é r o u x ,M .E .( 2 0 1 8 ) .P o o r\nStatistical Reporting, Inadequate D ata Presentation and Spin Persist\nDespite Editorial Advice. PLoS One 13, e0202121. doi:10.1371/journal.\npone.0202121\nDuong, L. T., Le, N. H., Tran, T. B., Ngo, V. M., and Nguyen, P. T. (2021).\nDetection of Tuberculosis from Chest X-ray Images: Boosting the\nPerformance with Vision Transformer and Transfer Learning. Expert\nSyst. Appl. 184, 115519. doi:10.1016/j.eswa.2021.115519\nDutta, A., and Zisserman, A., 2019. TheVIA Annotation Software for Images,\nAudio and Video. in MM 2019 - Proceedings of the 27th ACM\nInternational Conference on Multimedia October 2019 2276 –2279.\ndoi:10.1145/3343031.3350535\nG a b e r ,K .A . ,M c G a v i n ,C .R . ,a n dW e l l s ,I .P .( 2 0 0 5 ) .L a t e r a lC h e s tX - r a yf o r\nPhysicians. J. R. Soc. Med.98, 310–312. doi:10.1258/jrsm.98.7.31010.1177/\n014107680509800705\nG t u a ,C o l l e g e s . ,A c a d e m y ,O . ,A c a d e m y ,O . ,A c a d e m y ,O . ,S c i e n c e ,A .C . ,\nTechnology, I., et al. (2014). “Microsoft COCO, ” in European Conference\non Computer Vision , 740–755.\nGupta, M., and Gupta, B. (2018).An Ensemble Model for Breast Cancer\nPrediction Using Sequential Least Squares Programming Method\n(SLSQP), Proceeding of the 2018 11th Int Conf Contemp Comput IC3\n2018, Aug. 2018, Noida, India. IEEE, 2–4. doi:10.1109/IC3.2018.8530572\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).Deep Residual Learning for Image\nRecognition, Proceedings of the 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2016, Las Vegas, NV, USA. IEEE,\n770–778. doi:10.1109/CVPR.2016.90\nHerrera Diaz, M., Haworth-Brockman, M., and Keynan, Y. (2020). Review of\nEvidence for Using Chest X-Rays for Active Tuberculosis Screening in Long-\nTerm Care in Canada.Front. Public Health8, 8. doi:10.3389/fpubh.2020.00016\nHogeweg, L., Mol, C., De Jong, P. A., Dawson, R., Ayles, H., and Van Ginneken, B.\n(2010). “Fusion of Local and Global Detection Systems to Detect Tuberculosis\nin Chest Radiographs,” in Lecture Notes in Computer Science (Including\nSubseries Lecture Notes in Arti ﬁcial Intelligence and Lecture Notes in\nBioinformatics, 650\n–657. doi:10.1007/978-3-642-15711-0_81\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2017).Densely\nConnected Convolutional Networks, Proceedings - 30th IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR, July 2017, Honolulu, HI,\nUSA. IEEE. doi:10.1109/CVPR.2017.243\nIrvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., et al. (2019).\nCheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and\nExpert Comparison. Proc. AAAI Conf. Artif. Intelligence33, 590–597. doi:10.\n1609/aaai.v33i01.3301590\nI s l a m ,M .T . ,A o w a l ,M .A . ,M i n h a z ,A .T . ,a n dA s h r a f ,K .( 2 0 1 7 ) .Abnormality Detection\nand Localization in Chest X-Rays Using Deep Convolutional Neural Networks.N e w\nYork, NY: arXiv. Available from: http://arxiv.org/abs/1705.09850.\nJ a e g e r ,S . ,C a n d e m i r ,S . ,A n t a n i ,S . ,W á n g ,Y .X . ,L u ,P .X . ,a n dT h o m a ,G .( 2 0 1 4 ) .T w o\nPublic Chest X-ray Datasets for Computer-Aided Screening of Pulmonary Diseases.\nQuant Imaging Med. Surg.4, 475–477. doi:10.3978/j.issn.2223-4292.2014.11.20\nKim, I., Rajaraman, S., and Antani, S. (2019). Visual Interpretation of\nConvolutional Neural Network Predictions in Classifying Medical Image\nModalities. Diagnostics 9, 38. doi:10.3390/diagnostics9020038\nLakhani, P., and Sundaram, B. (2017). Deep Learning at Chest Radiography:\nAutomated Classiﬁcation of Pulmonary Tuberculosis by Using Convolutional\nNeural Networks. Radiology 284, 574–582. doi:10.1148/radiol.2017162326\nLiu, C., and Yin, Q. (2021). Automatic Diagnosis of COVID-19 Using a Tailored\nTransformer-like Network.J. Phys. Conf. Ser.2010, 012175. doi:10.1088/1742-\n6596/2010/1/012175\nPark, S., Kim, G., Oh, Y., Seo, J. B., Lee, S. M., Kim, J. H., et al. (2022). Multi-task\nVision Transformer Using Low-Level Chest X-ray Feature Corpus for COVID-\n19 Diagnosis and Severity Quantiﬁcation. Med. Image Anal.75, 102299. doi:10.\n1016/j.media.2021.102299\nPasa, F., Golkov, V., Pfeiffer, F., Cremers, D., and Pfeiffer, D. (2019). Efﬁcient Deep\nNetwork Architectures for Fast Chest X-Ray Tuberculosis Screening and\nVisualization. Sci. Rep. 9, 6268. doi:10.1038/s41598-019-42557-4\nRajaraman, S., and Antani, S. K. (2020). Modality-Speci ﬁc Deep Learning\nModel Ensembles toward Improving TB Detection in Chest Radiographs.\nIEEE Access 8, 27318–27326. doi:10.1109/ACCESS.2020.2971257\nR a j a r a m a n ,S . ,C a n d e m i r ,S . ,X u e ,Z . ,A l d e r s o n ,P .O . ,K o h l i ,M . ,A b u y a ,J . ,e ta l .\n(2018).A Novel Stacked Generalization of Models for Improved TB\nDetection in Chest Radiographs, Conf Proc . Annu Int Conf IEEE Eng\nMed Biol Soc IEEE Eng Med Biol Soc Annu Conf, July 2018, Honolulu, HI,\nUSA. IEEE, 718–721. doi:10.1109/ EMBC.2018.8512337\nR a j a r a m a n ,S ,R a j a r a m a n ,S . ,C a n d e m i r ,S . ,K i m ,I . ,T h o m a ,G . ,A n t a n i ,S . ,e ta l .\n(2018). Visualization and Interpretation of Convolutional Neural Network\nPredictions in Detecting Pneumonia in Pediatric Chest Radiographs.Appl.\nSci. 8, 1715. doi:10.3390/app8101715\nRajaraman, S., Sornapudi, S., Kohli, M., and Antani, S. (2019).Assessment of an\nEnsemble of Machine Learning Models toward Abnormality Detection in Chest\nRadiographs, Proceedings of the Annual International Conference of the IEEE\nEngineering in Medicine and Biology Society, EMBS, July 2019, Berlin, Germany.\nIEEE. doi:10.1109/EMBC.2019.8856715\nR a j a r a m a n ,S . ,S i e g e l m a n ,J . ,A l d e r s o n ,P .O . ,F o l i o ,L .S . ,F o l i o ,L .R . ,a n d\nA n t a n i ,S .K .( 2 0 2 0 ) .I t e r a t i v e l yP r u n e dD e e pL e a r n i n gE n s e m b l e sf o r\nCOVID-19 Detection in Chest X-Rays. IEEE Access 8, 115041 –115050.\ndoi:10.1109/ACCESS.2020.3003810\nRajaraman, S., Kim, I., and Antani, S. K. (2020). Detection and Visualization of\nAbnormality in Chest Radiographs Using Modality-speciﬁc Convolutional\nNeural Network Ensembles. PeerJ 8, e8693. doi:10.7717/peerj.8693\nRajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., et al. (2017).\nCheXNet: Radiologist-Level Pneum onia Detection on Chest X-Rays with\nDeep Learning .\nSchmidhuber, J. (2015). Deep Learning in Neural Networks: An Overview.\nNeural Networks 61, 85–117. doi:10.1016/j. neunet.2014.09.003\nSelvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D.\n(2017).Grad-CAM: Visual Explanations from Deep Networks via Gradient-\nBased Localization, Proceedings of the IEEE International Conference on\nComputer Vision, 2017-Oct, Venice, Italy. IEEE, 618 –626. doi:10.1109/\nICCV.2017.74\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 86472412\nRajaraman et al. Ensemble Models for Tuberculosis Detection\nS h o m e ,D . ,K a r ,T . ,M o h a n t y ,S . ,T i w a r i ,P . ,M u h a m m a d ,K . ,A l t a m e e m ,A . ,\net al. (2021). Covid-transformer: Interpretable Covid-19 Detection Using\nVision Transformer for Healthcare. Int. J. Environ. Res. Public Health 18,\n11086. doi:10.3390/ijerph182111086\nSimonyan, K., and Zisserman, A. Very Deep Convolutional Networks for\nLarge-Scale Image Recognition. in Pr oceeding of the 3rd International\nConference on Learning Representations, ICLR 2015 - Conference Track\nProceedings, 2015,april 2015\nS i v a r a m a k r i s h n a n ,R . ,A n t a n i ,S . ,C a n d e m i r ,S . ,X u e ,Z . ,T h o m a ,G . ,A l d e r s o n ,\nP., et al. (2018). “Comparing Deep Learning Models for Population\nScreening Using Chest Radiography, ” in SPIE Medical Imaging\n(Houston, Texas, United States) : SPIE). doi:10.1117/12.2293140\nSwingler, G. H., Du Toit, G., Andronikou, S., Van Der Merwe, L., and Zar, H. J.\n(2005). Diagnostic Accuracy of Chest Radiography in Detecting Mediastinal\nLymphadenopathy in Suspected Pulmonary Tuberculosis. Arch. Dis. Child.\n90, 1153–1156. doi:10.1136/adc.2004.062315\nTan, M., and Le, Q. V. (2021).EfﬁcientNetV2: Smaller Models and Faster Training.\nAvailable at: http://arxiv.org/abs/2104.00298.\nW a n g ,X . ,P e n g ,Y . ,L u ,L . ,L u ,Z . ,B a g h e r i ,M . ,a n dS u m m e r s ,R .M .( 2 0 1 7 ) . C h e s t X - r a y 8 :\nHospital-Scale Chest X-ray Database and Benchmarks on Weakly-Supervised\nClassiﬁcation and Localization of Common Thorax Diseases, Proceeding of The\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017,\nHonolulu, HI, USA. IEEE, 1–19. doi:10.1109/CVPR.2017.369\nWorld Health Organization (2016). Chest Radiography in Tuberculosis .\nSwitzerland: World Heal Organ, 1–44. Available at: https://apps.who.int/iris/\nhandle/10665/252424.\nZhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.\n(2021). “The Ninth Annual Conference on Learning Representations (ICLR\n2021),” in A Virtual Conference Due to COVID-19 Pandemic, Vienna,\nMay 3–7.\nConﬂict of Interest: LF has two issued patents (no royalties since NIH and\nmilitary-owned) related to chest imaging: (i)“Radiographic marker that displays\nupright angle on portable x-rays,” US Patent 9,541,822 B2, and (ii)“Multigrayscale\nuniversal CT Window,” US Patent 8,406,493 B2.\nThe remaining authors declare that the research was conducted in the absence of\nany commercial orﬁnancial relationships that could be construed as a potential\nconﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations, or those of\nthe publisher, the editors, and the reviewers. Any product that may be evaluated in\nthis article, or claim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nCopyright © 2022 Rajaraman, Zamzmi, Folio and Antani. This is an open-access\narticle distributed under the terms of the Creative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in otherforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are credited and that the original publication in this\njournal is cited, in accordancewith accepted academic practice. No use, distribution or\nreproduction is permitted which does not comply with these terms.\nFrontiers in Genetics | www.frontiersin.org February 2022 | Volume 13 | Article 86472413\nRajaraman et al. Ensemble Models for Tuberculosis Detection",
  "topic": "Convolutional neural network",
  "concepts": [
    {
      "name": "Convolutional neural network",
      "score": 0.6570671200752258
    },
    {
      "name": "Computer science",
      "score": 0.6345346570014954
    },
    {
      "name": "Scalability",
      "score": 0.6178601384162903
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6083806753158569
    },
    {
      "name": "Ensemble learning",
      "score": 0.565081000328064
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5007400512695312
    },
    {
      "name": "Machine learning",
      "score": 0.49730852246284485
    },
    {
      "name": "Transformer",
      "score": 0.49595412611961365
    },
    {
      "name": "Database",
      "score": 0.09289872646331787
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1299303238",
      "name": "National Institutes of Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800548410",
      "name": "United States National Library of Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I3019308854",
      "name": "Moffitt Cancer Center",
      "country": "US"
    }
  ]
}