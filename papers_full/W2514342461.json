{
    "title": "Using Factored Word Representation in Neural Network Language Models",
    "url": "https://openalex.org/W2514342461",
    "year": 2016,
    "authors": [
        {
            "id": "https://openalex.org/A5046084081",
            "name": "Jan Niehues",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5025976094",
            "name": "Thanh-Le Ha",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5101688710",
            "name": "Eunah Cho",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5110453805",
            "name": "Alex Waibel",
            "affiliations": [
                "Karlsruhe Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2251098065",
        "https://openalex.org/W2095650036",
        "https://openalex.org/W2054533749",
        "https://openalex.org/W2517196708",
        "https://openalex.org/W2146574666",
        "https://openalex.org/W3209717902",
        "https://openalex.org/W2293139442",
        "https://openalex.org/W2250638379",
        "https://openalex.org/W2233968761",
        "https://openalex.org/W1970689298",
        "https://openalex.org/W2056250865",
        "https://openalex.org/W2952230511",
        "https://openalex.org/W2105245376",
        "https://openalex.org/W2129776742",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2058695628",
        "https://openalex.org/W2250375075",
        "https://openalex.org/W2113788796",
        "https://openalex.org/W2144725461",
        "https://openalex.org/W2406079600",
        "https://openalex.org/W2083460949",
        "https://openalex.org/W3204047770",
        "https://openalex.org/W2110168585",
        "https://openalex.org/W2251682575",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3204164289",
        "https://openalex.org/W2172609755",
        "https://openalex.org/W2108862644",
        "https://openalex.org/W2916864519",
        "https://openalex.org/W2250905272",
        "https://openalex.org/W2143719855"
    ],
    "abstract": "Neural network language and translation models have recently shown their great potentials in improving the performance of phrase-based machine translation.At the same time, word representations using different word factors have been translation quality and are part of many state-of-theart machine translation systems.used in many state-of-the-art machine translation systems, in order to support better translation quality.In this work, we combined these two ideas by investigating the combination of both techniques.By representing words in neural network language models using different factors, we were able to improve the models themselves as well as their impact on the overall machine translation performance.This is especially helpful for morphologically rich languages due to their large vocabulary size.Furthermore, it is easy to add additional knowledge, such as source side information, to the model.Using this model we improved the translation quality of a state-of-the-art phrasebased machine translation system by 0.7 BLEU points.We performed experiments on three language pairs for the news translation task of the WMT 2016 evaluation.",
    "full_text": "Proceedings of the First Conference on Machine Translation, V olume 1: Research Papers, pages 74–82,\nBerlin, Germany, August 11-12, 2016.c⃝2016 Association for Computational Linguistics\nUsing Factored Word Representation\nin Neural Network Language Models\nJan Niehues, Thanh-Le Ha, Eunah Cho and Alex Waibel\nInstitute for Anthropomatics\nKarlsruhe Institute of Technology, Germany\nfirstname.secondname@kit.edu\nAbstract\nNeural network language and translation\nmodels have recently shown their great po-\ntentials in improving the performance of\nphrase-based machine translation. At the\nsame time, word representations using dif-\nferent word factors have been translation\nquality and are part of many state-of-the-\nart machine translation systems. used in\nmany state-of-the-art machine translation\nsystems, in order to support better transla-\ntion quality.\nIn this work, we combined these two ideas\nby investigating the combination of both\ntechniques. By representing words in neu-\nral network language models using differ-\nent factors, we were able to improve the\nmodels themselves as well as their impact\non the overall machine translation perfor-\nmance. This is especially helpful for mor-\nphologically rich languages due to their\nlarge vocabulary size. Furthermore, it is\neasy to add additional knowledge, such as\nsource side information, to the model.\nUsing this model we improved the trans-\nlation quality of a state-of-the-art phrase-\nbased machine translation system by 0.7\nBLEU points. We performed experiments\non three language pairs for the news trans-\nlation task of the WMT 2016 evaluation.\n1 Introduction\nRecently, neural network models are deployed ex-\ntensively for better translation quality of statisti-\ncal machine translation (Le et al., 2011; Devlin et\nal., 2014). For the language model as well as for\nthe translation model, neural network-based mod-\nels showed improvements when used during de-\ncoding as well as when used in re-scoring.\nIn phrase-based machine translation (PBMT),\nword representation using different factors (Koehn\nand Hoang, 2007) are commonly used in state-\nof-the-art systems. Using Part-of-Speech (POS)\ninformation or automatic word clusters is es-\npecially important for morphologically rich lan-\nguages which often have a large vocabulary size.\nLanguage models based on these factors are able\nto consider longer context and therefore improve\nthe modelling of the overall structure. Further-\nmore, the POS information can be used to improve\nthe modelling of word agreement, which is often a\ndifﬁcult task when handling morphologically rich\nlanguages.\nUntil now, word factors have been used rela-\ntively limited in neural network models. Auto-\nmatic word classes have been used to structure the\noutput layer (Le et al., 2011) and as input in feed\nforward neural network language models (Niehues\nand Waibel, 2012).\nIn this work, we propose a multi-factor recur-\nrent neural network (RNN)-based language model\nthat is able to facilitate all available information\nabout the word in the input as well as in the out-\nput. We evaluated the technique using the surface\nform, POS-tag and automatic word clusters using\ndifferent cluster sizes.\nUsing this model, it is also possible to integrate\nsource side information into the model. By using\nthe model as a bilingual model, the probability of\nthe translation can be modelled and not only the\none of target sentence. As for the target side, we\nuse a factored representation for the words on the\nsource side.\nThe remaining of the paper is structured as fol-\nlowing: In the following section, we ﬁrst review\nthe related work. Afterwards, we will shortly de-\nscribe the RNN-based language model used in our\nexperiments. In Section 4, we will introduce the\nfactored RNN-based language model. In the next\n74\nsection, we will describe the experiments on the\nWMT 2016 data. Finally, we will end the paper\nwith a conclusion of the work.\n2 Related Work\nAdditional information about words, encoded as\nword factors, e.g. the lemma of word, POS\ntags, etc., is employed in state-of-the-art phrase-\nbased systems. (Koehn and Hoang, 2007) decom-\nposes the translation of factored representations\nto smaller mapping steps, which are modelled by\ntranslation probabilities from input factor to out-\nput factor or by generating probabilities of addi-\ntional output factors from existing output factors.\nThen those pre-computed probabilities are jointly\ncombined in the decoding process as a standard\ntranslation feature scores. In addition, language\nmodels using these word factors have shown to\nbe very helpful to improve the translation qual-\nity. In particular, the aligned-words, POS or word\nclasses are used in the framework of modern lan-\nguage models (Mediani et al., 2011; Wuebker et\nal., 2013).\nRecently, neural network language models have\nbeen considered to perform better than standard\nn-gram language models (Schwenk, 2007; Le et\nal., 2011). Especially the neural language models\nconstructed in recurrent architectures have shown\na great performance by allowing them to take a\nlonger context into account (Mikolov et al., 2010;\nSundermeyer et al., 2013).\nIn a different direction, there has been a great\ndeal of research on bringing not only target words\nbut also source words into the prediction process,\ninstead of predicting the next target word based on\nthe previous target words (Le et al., 2012; Devlin\net al., 2014; Ha et al., 2014).\nHowever, to the best of our knowledge, word\nfactors have been exploited in a relatively limited\nscope of neural network research. (Le et al., 2011;\nLe et al., 2012) use word classes to reduce the\noutput layer’s complexity of such networks, both\nin language and translation models. In the work\nof (Niehues and Waibel, 2012), their Restricted\nBoltzmann Machines language models also en-\ncode word classes as an additional input feature in\npredicting the next target word. (Tran et al., 2014)\nuse two separate feed forward networks to predict\nthe target word and its corresponding sufﬁxes with\nthe source words and target stem as input features.\nOur work exhibits several essential differences\nfrom theirs. Firstly, we leverage not only the target\nmorphological information but also word factors\nfrom both source and target sides in our models.\nFurthermore, we could use as many types of word\nfactors as we can provide. Thus, we are able to\nmake the most of the information encoded in those\nfactors for more accurate prediction.\n3 Recurrent Neural Network-based\nLanguage Models\nIn contrast to feed forward neural network-based\nlanguage models, recurrent neural network-based\nlanguage models are able to store arbitrary long\nword sequences. Thereby, they are able to directly\nmodel P(w|h) and no approximations by limiting\nthe history size are necessary. Recently, several\nauthors showed that RNN-based language models\ncould perform very well in phrase-based machine\ntranslation. (Mikolov et al., 2010; Sundermeyer et\nal., 2013)\nIn this work, we used the torch7 1 implementa-\ntion of an RNN-based language model (L ´eonard\net al., 2015). First, the words were mapped to\ntheir word embeddings. We used an input embed-\nding size of 100. Afterwards, we used two LSTM-\nbased layers. The ﬁrst has the size of the word\nembeddings and for the second we used a hidden\nsize of 200. Finally, the word probabilities were\ncalculated using a softmax layer.\nThe models were trained using stochastic gra-\ndient descent. The weights were updated using\nmini-batches with a batch size of 128. We used\na maximum epoch size of 1 million examples and\nselected the model with the lowest perplexity on\nthe development data.\n4 Factored Language Model\nWhen using factored representation of words,\nwords are no longer represented as indices in the\nneural network. Instead, they are represented a tu-\nples of indices w = (f1, . . . , fD), where D is the\nnumber of different factors used to describe the\nword. These factors can be the word itself, as well\nas the POS, automatic learned classes (Och, 1999)\nor other information about the word. Furthermore,\nwe can use different types of factors for the input\nand the output of the neural network.\n1http://torch.ch/\n75\nFigure 1: Factored RNN Layout\n4.1 Input Representation\nIn a ﬁrst step, we obtained a factored representa-\ntion for the input of the neural network. In the\nexperiments, we represented a word by its surface\nform, POS-tags and automatic word class, but the\nframework can be used for any number of word\nfactors. Although there are factored approaches\nfor n-gram based language models (Bilmes and\nKirchhoff, 2003), most n-gram language models\nonly use one factor. In contrast, in neural network\nbased language models, it is very easy to add ad-\nditional information as word factors. We can learn\ndifferent embeddings for each factor and represent\nthe word by concatenating the embeddings of sev-\neral factors. As shown in the bottom of Figure 1,\nwe ﬁrst project the different factors to the contin-\nuous factor embeddings. Afterwards, we concate-\nnate these embeddings into a word embedding.\nThe advantage of using several word factors is\nthat we can use different knowledge sources to\nrepresent a word. When a word occurs very rarely,\nthe learned embedding from its surface form might\nnot be helpful. The additional POS information,\nhowever, is very helpful. While using POS-based\nlanguage models in PBMT may lead to losing the\ninformation about high frequent words, in this ap-\nproach we can have access to all information by\nconcatenating the factor embeddings.\n4.2 Output Representation\nIn addition to use different factors in the input of\nthe neural network, we can also use different fac-\ntors on the output. In phrase-based machine trans-\nlation, n-gram language models based on POS-\ntags have been shown to be very successful for\nmorphologically rich languages.\nPorting this idea to neural network lan-\nguage models, we can not only train a\nmodel to predict the original word f1 given\nthe previous words in factor representation\nh = ( f1,1, . . . , f1,D), . . . ,(fi,1, . . . , fi,D), but\nalso train a model to predict he POS-tags (e.g. f2)\ngiven the history h.\nIn a ﬁrst step, we proposed to train individual\nmodels for all factors 1, . . . , Dgenerating proba-\nbilities P1, . . . , PD for every sentence. Then these\nprobabilities can be used as features for example\nin re-scoring of the phrase-based MT system.\nConsidering that it can be helpful to consider\nall factors of the word in the input, it can be also\nhelpful to jointly train the models for predicting\nthe different output factors. This is motivated by\nthe fact that multi-task learning has shown to be\nbeneﬁcial in several NLP tasks (Collobert et al.,\n2011). Predicting all output features jointly re-\nquires a modiﬁcation of the output layer of the\nRNN model. As shown in Figure 1, we replace the\nsingle mapping from the LSTM-layer to the soft-\nmax layer, by D mappings. Each mapping then\nlearns to project the LSTM-layer output to the fac-\ntored output probabilities. In the last layer, we use\nD different softmax units. In a similar way as the\nconventional network, the error between the out-\nput of the network and the reference is calculated\nduring training.\nUsing this network, we will no longer pre-\ndict the probability of one word factor Pd, d ∈\n{1, . . . D}, but D different probability distribu-\ntions P1, . . . , PD. In order to integrate this model\ninto the machine translation system we explored\ntwo different probabilities. First, we used only the\njoint probability P = ∏D\nd=1 Pd as a feature in the\nlog-linear combination. In addition, we also used\nthe joint probability as well as all individual prob-\nabilities Pd as features.\n4.3 Bilingual Model\nUsing the model presented before, it is possible to\nadd additional information to the model as well.\nOne example we explored in this work is to use\n76\nFigure 2: Bilingual Model\nthe model as a bilingual model (BM). Instead of\nusing only monolingual information by consider-\ning the previous target factors as input, we used\nsource factors additionally. Thereby, we can now\nmodel the probability of a word given the previ-\nous target words and information about the source\nsentence. So in this case we model the transla-\ntion probability and no longer the language model\nprobability.\nWhen predicting the target word wi+1 with its\nfactors fi+1,1, . . . , fi+1,D, the input to the RNN\nis the previous target word wi = fi,1, . . . , fi,D.\nUsing the alignment, we can ﬁnd the source word\nsa(i+1), which is aligned to the target word wi+1.\nWhen we add the features of source word\nsa(i+1) = (fs\na(i+1),1, . . . , fs\na(i+1),Ds )\nto the ones of the target word wi and create a new\nbilingual token\nbi = (fi+1,1, . . . , fi+1,D, fs\na(i+1),1, . . . , fs\na(i+1),Ds )\n, we now can predict the target word given the pre-\nvious target word and the aligned source word.\nIn the example in Figure 2, we would\ninsert (completed,VVD,87,ein,ART) to predict\n(a,DT,37).\nIn this case the number of input factors and out-\nput factors are no longer the same. In the input,\nwe have D + Ds input factors, while we have only\nD factors on the output of the network.\n5 Experiments\nWe evaluated the factored RNNLM on three dif-\nferent language pairs of the WMT 2016 News\nTranslation Task. In each language pair, we cre-\nated an n-best list using our phrase-based MT sys-\ntem and used the factored RNNLM as an addi-\ntional feature in rescoring. It is worth noting that\nthe POS and word class information are already\npresent during decoding of the baseline system by\nn-gram-based language models based on each of\nthese factors. First, we performed a detailed analy-\nsis on the English-Romanian task. In addition, we\nused the model in a German-English and English-\nGerman translation system. In all tasks, we used\nthe model in re-scoring of a PBMT system.\n5.1 System Description\nThe baseline system is an in-house implementa-\ntion of the phrase-based approach. The system\nused to generate n-best lists for the news tasks is\ntrained on all the available training corpora of the\nWMT 2015 Shared Translation task. The system\nuses a pre-reordering technique (Rottmann and\nV ogel, 2007; Niehues and Kolss, 2009; Herrmann\net al., 2013) and facilitates several translation and\nlanguage models. As shown in Table 1, we use\ntwo to three word-based language models and one\nto two cluster-based models using 50, 100 or 1,000\nclusters. The custers were trained as described\nin (Och, 1999). In addition, we used a POS-\nbased language model in the English-Romainian\nsystem and a bilingual language model (Niehues\net al., 2011) in English to German and German\nto English systems. The POS tags for English-\nRomanian were generated by the tagger described\nin (Ion et al., 2012) and the ones for German by\nRFTagger (Schmid and Laws, 2008).\nTable 1: Features\nEN-RO EN-DE DE-EN\nwordLM 2 3 3\nPOSLM 1 0 0\nclusterLM 2 1 2\nBiLM 0 1 1\n#features 22-23 20 22\nIn addition, we used discriminative word lex-\nica (Niehues and Waibel, 2013) during decoding\nand source discriminative word lexica in rescoring\n(Herrman et al., 2015).\nA full system description can be found in (Ha et\nal., 2016).\nThe German to English baseline system uses 20\nfeatures and the English to German systems uses\n22 features.\nThe English-Romanian system was optimized\non the ﬁrst part of news-dev2016 and the rescor-\ning was optimized on this set and a subset of 2,000\n77\nsentences from the SETimes corpus. This part of\nthe corpus was of course excluded for training the\nmodel. The system was tested on the second half\nof news-dev2016.\nThe English-German and German-English sys-\ntems were optimized on news-test2014 and also\nthe re-scoring was optimized on this data. We\ntested the system on news-test2015.\nFor English to Romanian and English to Ger-\nman we used an n-best List of 300 entries and\nfor German to English we used an n-best list with\n3,000 entries.\nFor decoding, for all language directions, the\nweights of the system were optimized using mini-\nmum error rate training (Och, 2003). The weights\nin the rescoring were optimized using the List-\nNet algorithm (Cao et al., 2007) as described in\n(Niehues et al., 2015).\nThe RNN-based language models for English to\nRomanian and German to English were trained on\nthe target side of the parallel training data. For En-\nglish to German, we trained the model and the Eu-\nroparl corpus and the News commentary corpus.\n5.2 English - Romanian\nIn the ﬁrst experiment on the English to Roma-\nnian task, we only used the scores of the RNN lan-\nguage models. The baseline system has a BLEU\nscore (Papineni et al., 2002) of 29.67. Using only\nthe language model instead of the 22 features, of\ncourse, leads to a lower performance, but we can\nsee clear difference between the different language\nmodels. All systems use a word vocabulary of 5K\nwords and we used four different factors. We used\nthe word surface form, the POS tags and word\nclusters using 100 and 1,000 classes.\nThe baseline model using words as input and\nwords as output reaches a BLEU score of 27.88.\nIf we instead represent the input words by factors,\nwe select entries from the n-best list that gener-\nates a BLEU score of 28.46. As done with the\nn-gram language models, we can also predict the\nother factors instead of the words themselves. In\nall cases, we use all four factors as input factors.\nAs shown in Table 2, all models except for the\none with 100 classes perform similarly, reaching\nup between 28.46 and 28.49. The language model\npredicting only 100 classes only reaches a BLEU\nscore of 28.23. It suggests that this number of\nclasses is too low to disambiguate the entries in\nthe n-best list.\nTable 2: English - Romanian Single Score\nInput Prediction Single\nWord Word 27.88\nAll factors Word 28.46\nAll factors POS 28.48\nAll factors 100 Cl. 28.23\nAll factors 1,000 Cl. 28.49\nAll factors All factors 28.54\nIf we predict all factors together and use then\nthe joint probability, we can reach the best BLEU\nscore of 28.54 as shown in the last line of the ta-\nble. This is 0.7 BLEU points better than the initial\nword based model.\nAfter evaluating the model as the only knowl-\nedge source, we also performed experiments using\nthe model in combination with the other models.\nWe evaluated the baseline and the best model in\nthree different conﬁguration in Table 3 using only\nthe joint probability. The three baseline conﬁgu-\nration differ in the models used during decoding.\nThereby, we are able to generate different n-best\nlists and test the models on different conditions.\nTable 3: English - Romanian Language Models\nModel Conf1 Conf2 Conf3\nBaseline 29.86 30.00 29.75\nLM 5K 29.79 29.84 29.73\nLM 50K 29.64 29.84 29.83\nFactored LM 5K 29.94 30.01 30.01\nFactored LM 50K 30.05 30.27 30.29\nIn Table 3, we tested the word-based and the\nfactored language model using a vocabulary of 5K\nand 50K words. Features from each model are\nused in addition to the features of the baseline sys-\ntem. As shown in the table, the word-based RNN\nlanguage models perform similarly, but both could\nnot improve over the baseline system. One possi-\nble reason for this is that we already use several\nlanguage models in the baseline model and they\nare partly trained on much larger data. While the\nRNN models are trained using only the target lan-\nguage model, one word-based language model is\ntrained on the Romanian common crawl corpus.\nFurthermore, the POS-based and word cluster lan-\nguage models use a 9-gram history and therefore,\ncan already model quite long dependencies.\nBut if we use a factored language model, we are\n78\nable to improve over the baseline system. Using\nthe additional information of the other word fac-\ntors, we are able to improve the bilingual model in\nall situations. The model using a surface word vo-\ncabulary of 5,000 words can improve by 0.1 to 0.3\nBLEU points. The model using a 50K vocabulary\ncan even improve by up to 0.6 BLEU points.\nTable 4: English - Romanian Bilingual Models\nModel Dev Test\nBaseline 40.12 29.75\n+ Factored LM 50K 40.87 30.17\n+ Factored BM 5K 41.11 30.44\n+ Factored BM 50K 41.16 30.57\nAfter analyzing the different language models,\nwe also evaluate how we can use the factored\nrepresentation to include source side information.\nThe results are summarized in Table 4. In these\nexperiments, we used not only the the joint proba-\nbility, but also the four individual probabilities as\nfeatures. Therefore, we will add ﬁve scores for\nevery model, since each model is added to its pre-\nvious conﬁguration in this experiment.\nExploiting all ﬁve probabilities of the lan-\nguage model brought us the similar improvement\nwe achieved using the joint probability from the\nmodel. On the test set, the improvements are\nslightly worse. When adding the model using\nsource side information based on a vocabulary of\n5K and 50K words, however, we get additional im-\nprovements. Adopting the both bilingual models\n(BM) along with a factored LM, we improved the\nBLEU score further leading up to the best score of\n30.57 for the test set.\n5.3 English - German\nIn addition to the experiments on English to Ro-\nmanian, we also evaluated the models on the task\nof translating English News to German. For the\nEnglish to German system, we use three factors\non the source side and four factors on the tar-\nget side. In English, we used the surface forms\nas well as automatic word cluster based on 100\nand 1,000 classes. On the target side, we used\nﬁne-graind POS-tags generated by the RFTagger\n(Schmid and Laws, 2008), in addition to the fac-\ntors for the source side.\nThe experiments using only the scores of the\nmodel are summarized in Table 5. In this exper-\niment, we analyzed a word based- and a factored\nTable 5: English - German Single Score\nModel Single\nLM 5K 20.92\nFactored LM 5K 21.69\nBM 5K 21.33\nFactored BM 5K 21.92\nlanguage models as well as bilingual models. As\ndescribed in section 4.3, the difference between\nthe language model and the bilingual model is that\nthe latter uses the source side information as addi-\ntional factor.\nUsing only the word-based language model we\nachieved a BLEU score of 20.92. Deploying a fac-\ntored language model instead, we can improve the\nBLEU score by 0.7 BLEU points to 21.69. While\nwe achieved a score of 21.33 BLEU points by us-\ning a proposed bilingual model, we improved the\nscore up to 21.92 BLEU points by adopting all fac-\ntors for the bilingual model.\nTable 6: English-German Language Model\nModel Conf1 Conf2\nBaseline 23.25 23.40\nFactored LM 5K 23.63 23.77\nFactored BM 5K 23.43 23.48\nIn addition to the analysis on the single model,\nwe also evaluated the model’s inﬂuence by com-\nbining the model with the baseline features. We\ntested the language model as well as the bilingual\nmodel on two different conﬁgurations. Adopting\nthe factored language model on top of the base-\nline features improved the translation quality by\naround 0.4 BLEU points for both conﬁgurations,\nas shown in Table 6. Although the bilingual model\ncould also improve the translation quality, it could\nnot outperform the factored language model. The\ncombination of the two models, LM and BM, did\nnot lead to further improvements. In summary,\nthe factored language model improved the BLEU\nscore by 0.4 points.\n5.4 German - English\nSimilar experiments were conducted on the Ger-\nman to English translation task. For this language\npair, we built models using a vocabulary size of\n5,000 words. The models cover word surface\nforms and two automatic word clusters, which are\n79\nbased on 100 and 1,000 word classes respectively.\nFirst, we will evaluate the performance of the sys-\ntem using only this model in rescoring. The results\nare summarized in Table 7.\nTable 7: German - English Single Score\nModel Single\nLM 5K 26.11\nFactored LM 5K 26.96\nBM 5K 26.77\nFactored BM 5K 26.81\nThe word based language model achieves a\nBLEU score 26.11. Extending the model to in-\nclude factors improves the BLEU score by 0.8\nBLEU points to 26.96. If we use a bilingual\nmodel, a word based model achieves a BLEU\nscore of 26.77 and the factored one a BLEU score\nof 26.81. Although the factored model performed\nbetter than the word-based models, in this case the\nbilingual model cannot outperform the language\nmodel.\nTable 8: German - English Language Model\nModel Single\nBaseline 29.33\n+ Factored BM 5K 29.51\n+ Factored LM 5K 29.66\nIn a last series of experiments, we used the\nscores combined with the baseline scores. The re-\nsults are shown in Table 8. In this language pair,\nwe can improve over the baseline system by using\nboth models. The ﬁnal BLEU score is 0.3 BLEU\npoints better than the initial system.\n6 Conclusion\nIn this paper, we presented a new approach to\nintegrate additional word information into a neu-\nral network language model. This model is es-\npecially promising for morphologically rich lan-\nguages. Due to their large vocabulary size, addi-\ntional information such as POS-tags are expected\nto model rare words effectively.\nRepresenting words using factors has been suc-\ncessfully deployed in many phrase-based machine\ntranslation systems. Inspired by this, we repre-\nsented each word in our neural network language\nmodel using factors, facilitating all available in-\nformation of the word. We showed that using the\nfactored neural network language models can im-\nprove the quality of a phrase-based machine trans-\nlation system, which already uses several factored\nlanguage models.\nIn addition, the presented framework allows an\neasy integration of source side information. By\nincorporating the alignment information to the\nsource side, we were able to model the translation\nprocess. In this model, the source words as well as\nthe target words can be represented by word fac-\ntors.\nUsing these techniques, we are able to im-\nprove the translation system on three different lan-\nguage pairs of the WMT 2016 evaluation. We\nperformed experiments on the English-Romanian,\nEnglish-German and German-English translation\ntask. The suggested technique yielded up to 0.7\nBLEU points of improvement on all three tasks.\nAcknowledgments\nThe project leading to this application has received\nfunding from the European Union’s Horizon 2020\nresearch and innovation programme under grant\nagreement n ◦ 645452. The research by Thanh-\nLe Ha was supported by Ministry of Science, Re-\nsearch and the Arts Baden-W¨urttemberg.\nReferences\nJeff A. Bilmes and Katrin Kirchhoff. 2003. Factored\nlanguage models and generalized parallel backoff.\nIn Proceedings of the 2003 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics on Human Language Technol-\nogy: Companion Volume of the Proceedings of HLT-\nNAACL 2003–short Papers - Volume 2, NAACL-\nShort ’03, pages 4–6, Stroudsburg, PA, USA. As-\nsociation for Computational Linguistics.\nZ. Cao, T. Qin, T.-Y . Liu, M.-F. Tsai, and H. Li. 2007.\nLearning to rank: from pairwise approach to listwise\napproach. In Proceedings of the 24th international\nconference on Machine learning, Icml ˆa07, pages\n129–136, New York, NY , USA. Acm.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.\n2011. Natural language processing (almost) from\nscratch. CoRR, abs/1103.0398.\nJ. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz,\nand J. Makhoul. 2014. Fast and robust neural\nnetwork joint models for statistical machine trans-\nlation. In Proceedings of the 52st Annual Meet-\ning of the Association for Computational Linguis-\ntics (ACL 2014), volume 1, pages 1370–1380, Balti-\nmore, Maryland, USA.\n80\nThanh-Le Ha, Jan Niehues, and Alex Waibel. 2014.\nLexical Translation Model Using a Deep Neural\nNetwork Architecture. In Proceedings of the 11th\nInternational Workshop on Spoken Language Trans-\nlation (IWSLT14), Lake Tahoe, CA, USA.\nThanh-Le Ha, Eunah Cho, Jan Niehues, Mohammed\nMediani, Matthias Sperber, Alexandre Allauzen,\nand yAlex Waibel. 2016. The karlsruhe institute\nof technology systems for the news translation task\nin wmt 2016. In Proceedings of the ACL 2016 First\nConference on Machine Translation (WMT2016).\nTeresa Herrman, Jan Niehues, and Alex Waibel. 2015.\nSource Discriminative Word Lexicon for Transla-\ntion Disambiguation. In Proceedings of the 12th In-\nternational Workshop on Spoken Language Transla-\ntion (IWSLT15), Danang, Vietnam.\nTeresa Herrmann, Jan Niehues, and Alex Waibel.\n2013. Combining Word Reordering Methods on\ndifferent Linguistic Abstraction Levels for Statisti-\ncal Machine Translation. In Proceedings of the Sev-\nenth Workshop on Syntax, Semantics and Structure\nin Statistical Translation, Altanta, Georgia, USA.\nRadu Ion, Elena Irimia, Dan Stefanescu, and Dan Tuﬁs.\n2012. Rombac: The romanian balanced annotated\ncorpus. In Nicoletta Calzolari (Conference Chair),\nKhalid Choukri, Thierry Declerck, Mehmet U?ur\nDo?an, Bente Maegaard, Joseph Mariani, Asun-\ncion Moreno, Jan Odijk, and Stelios Piperidis, ed-\nitors, Proceedings of the Eight International Con-\nference on Language Resources and Evaluation\n(LREC’12), Istanbul, Turkey, may. European Lan-\nguage Resources Association (ELRA).\nPhilipp Koehn and Hieu Hoang. 2007. Factored trans-\nlation models. In In Proceedings of the 2007 Joint\nConference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning (EMNLP-CoNLL, pages 868–876.\nHai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-\nLuc Gauvain, and Franc ¸ois Yvon. 2011. Structured\noutput layer neural network language model. InPro-\nceedings of the International Conference on Audio,\nSpeech and Signal Processing, pages 5524–5527.\nHai-Son Le, Alexandre Allauzen, and Franc ¸ois Yvon.\n2012. Continuous Space Translation Models with\nNeural Networks. In Proceedings of the 2012 con-\nference of the north american chapter of the asso-\nciation for computational linguistics: Human lan-\nguage technologies (NAACL), pages 39–48. Associ-\nation for Computational Linguistics.\nNicholas L´eonard, Sagar Waghmare, Yang Wang, and\nJin-Hwa Kim. 2015. rnn : Recurrent library for\ntorch. CoRR, abs/1511.07889.\nMohammed Mediani, Eunah Cho, Jan Niehues, Teresa\nHerrmann, and Alex Waibel. 2011. The KIT\nenglish-french translation systems for IWSLT 2011.\nIn 2011 International Workshop on Spoken Lan-\nguage Translation, IWSLT 2011, San Francisco, CA,\nUSA, December 8-9, 2011, pages 73–78.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH, volume 2, page 3.\nJan Niehues and Muntsin Kolss. 2009. A POS-Based\nModel for Long-Range Reorderings in SMT. In\nProceedings of the Fourth Workshop on Statistical\nMachine Translation (WMT 2009), Athens, Greece.\nJ. Niehues and A. Waibel. 2012. Continuous space\nlanguage models using restricted boltzmann ma-\nchines. In Proceedings of the Ninth International\nWorkshop on Spoken Language Translation (IWSLT\n2012), Hong Kong.\nJan Niehues and Alex Waibel. 2013. An MT Error-\nDriven Discriminative Word Lexicon using Sen-\ntence Structure Features. In Proceedings of the\nEighth Workshop on Statistical Machine Transla-\ntion, Soﬁa, Bulgaria.\nJan Niehues, Teresa Herrmann, Stephan V ogel, and\nAlex Waibel. 2011. Wider Context by Using Bilin-\ngual Language Models in Machine Translation. In\nSixth Workshop on Statistical Machine Translation\n(WMT 2011), Edinburgh, Scotland, United King-\ndom.\nJan Niehues, Quoc Khanh Do, Alexandre Allauzen,\nand Alex Waibel. 2015. Listnet-based MT Rescor-\ning. EMNLP 2015, page 248.\nFranz Josef Och. 1999. An Efﬁcient Method for Deter-\nmining Bilingual Word Classes. In Proceedings of\nthe Ninth Conference of the European Chapter of the\nAssociation for Computational Linguistics (EACL\n1999), Bergen, Norway.\nF.J. Och. 2003. Minimum error rate training in sta-\ntistical machine translation. In Proceedings of the\n41th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL 2003), Sapporo, Japa.\nK. Papineni, S. Roukos, T. Ward, and W.-jing Zhu.\n2002. Bleu: a method for automatic evaluation of\nmachine translation. In Proceedings of the 40th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL 2002), pages 311–318, Philadel-\nphia, Pennsylvania.\nKay Rottmann and Stephan V ogel. 2007. Word Re-\nordering in Statistical Machine Translation with a\nPOS-Based Distortion Model. In Proceedings of the\n11th International Conference on Theoretical and\nMethodological Issues in Machine Translation (TMI\n2007), Sk¨ovde, Sweden.\nHelmut Schmid and Florian Laws. 2008. Estimation\nof Conditional Probabilities with Decision Trees and\nan Application to Fine-Grained POS Tagging. In\nProceedings of the 22nd International Conference\n81\non Computational Linguistics, Manchester, United\nKingdom.\nHolger Schwenk. 2007. Continuous space language\nmodels. Computer Speech & Language, 21(3):492–\n518.\nMartin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,\nBen Freiberg, Ralf Schluter, and Hermann Ney.\n2013. Comparison of feedforward and recur-\nrent neural network language models. In Acous-\ntics, Speech and Signal Processing (ICASSP), 2013\nIEEE International Conference on, pages 8430–\n8434. IEEE.\nKe Tran, Arianna Bisazza, Christof Monz, et al. 2014.\nWord translation prediction for morphologically rich\nlanguages with bilingual neural networks. Associa-\ntion for Computational Linguistics.\nJoern Wuebker, Stephan Peitz, Felix Rietig, and Her-\nmann Ney. 2013. Improving statistical machine\ntranslation with word class models. In Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 1377–1381, Seattle, W A, USA, Oc-\ntober.\n82"
}