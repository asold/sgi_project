{
  "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias",
  "url": "https://openalex.org/W3175227919",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2746728558",
      "name": "Xu, Yufei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1980893091",
      "name": "Zhang Qi-ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1925561233",
      "name": "Zhang Jing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1960309444",
      "name": "Tao, Dacheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3139049060",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2143238378",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3155420132",
    "https://openalex.org/W2963402313",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W2103504761",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2963703618",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3165150763",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W2990205821",
    "https://openalex.org/W2145072179",
    "https://openalex.org/W2916743882",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3109241881",
    "https://openalex.org/W2109773745",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2546696630",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W2016163169",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2117228865",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W2556967412",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W1677409904",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2962850830",
    "https://openalex.org/W3163465952",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W8437397",
    "https://openalex.org/W3157914380",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3159337199",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2607041014",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W2470139095",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3158846111"
  ],
  "abstract": "Transformers have shown great potential in various computer vision tasks owing to their strong capability in modeling long-range dependency using the self-attention mechanism. Nevertheless, vision transformers treat an image as 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance. Alternatively, they require large-scale training data and longer training schedules to learn the IB implicitly. In this paper, we propose a novel Vision Transformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. Experiments on ImageNet as well as downstream tasks prove the superiority of ViTAE over the baseline transformer and concurrent works. Source code and pretrained models will be available at GitHub.",
  "full_text": "ViTAE: Vision Transformer Advanced by Exploring\nIntrinsic Inductive Bias\nYufei Xu1∗ Qiming Zhang1∗ Jing Zhang1 Dacheng Tao2,1\n1The University of Sydney, Australia,\n2JD Explore Academy, China\n{yuxu7116,qzha2506}@uni.sydney.edu.au, jing.zhang1@sydney.edu.au, dacheng.tao@gmail.com\nAbstract\nTransformers have shown great potential in various computer vision tasks owing to\ntheir strong capability in modeling long-range dependency using the self-attention\nmechanism. Nevertheless, vision transformers treat an image as 1D sequence of\nvisual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual\nstructures and dealing with scale variance. Alternatively, they require large-scale\ntraining data and longer training schedules to learn the IB implicitly. In this\npaper, we propose a new Vision Transformer Advanced by Exploring intrinsic IB\nfrom convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid\nreduction modules to downsample and embed the input image into tokens with\nrich multi-scale context by using multiple convolutions with different dilation\nrates. In this way, it acquires an intrinsic scale invariance IB and is able to learn\nrobust feature representation for objects at various scales. Moreover, in each\ntransformer layer, ViTAE has a convolution block in parallel to the multi-head self-\nattention module, whose features are fused and fed into the feed-forward network.\nConsequently, it has the intrinsic locality IB and is able to learn local features\nand global dependencies collaboratively. Experiments on ImageNet as well as\ndownstream tasks prove the superiority of ViTAE over the baseline transformer and\nconcurrent works. Source code and pretrained models will be available at code.\n1 Introduction\n68.7\n70.8\n71.7\n72.6\n74.2\n75.3\n68\n70\n72\n74ImageNet Top-1 Accuracy (%)\nEpochs\n100 200 300\n64.1\n68.1\n68.7\n67.5\n71.6\n72.6\n63\n65\n67\n69\n71\n73ImageNet Top-1 Accuracy (%)\nData Percentage (%)\nT2T\nViTAE\n20 60 100\nFigure 1: Comparison of data and training efﬁ-\nciency of T2T-ViT-7 and ViTAE-T on ImageNet.\nTransformers [79, 17, 40, 14, 46, 61] have shown\na domination trend in NLP studies owing to their\nstrong ability in modeling long-range dependen-\ncies by the self-attention mechanism [67, 81, 51].\nSuch success and good properties of transform-\ners has inspired following many works that apply\nthem in various computer vision tasks [ 19, 100,\n97, 80, 7]. Among them, ViT [19] is the pioneer-\ning pure transformer model that embeds images\ninto a sequence of visual tokens and models the\nglobal dependencies among them with stacked\ntransformer blocks. Although it achieves promis-\ning performance on image classiﬁcation, it re-\nquires large-scale training data and a longer train-\ning schedule. One important reason is that ViT\n∗Equal Contribution. Interns at JD Explore Academy.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2106.03348v4  [cs.CV]  24 Dec 2021\nlacks intrinsic inductive bias (IB) in modeling local visual structures (e.g., edges and corners) and\ndealing with objects at various scales like convolutions. Alternatively, ViT has to learn such IB\nimplicitly from large-scale data.\nUnlike vision transformers, Convolution Neural Networks (CNNs) naturally equip with the intrinsic\nIBs of scale-invariance and locality and still serve as prevalent backbones in vision tasks [ 26, 70,\n62, 8, 96]. The success of CNNs inspires us to explore intrinsic IBs in vision transformers. We\nstart by analyzing the above two IBs of CNNs, i.e., locality and scale-invariance. Convolution that\ncomputes local correlation among neighbor pixels is good at extracting local features such as edges\nand corners. Consequently, CNNs can provide plentiful low-level features at the shallow layers [94],\nwhich are then aggregated into high-level features progressively by a bulk of sequential convolutions\n[32, 68, 71]. Moreover, CNNs have a hierarchy structure to extract multi-scale features at different\nlayers [68, 38, 26]. Besides, intra-layer convolutions can also learn features at different scales by\nvarying their kernel sizes and dilation rates [25, 70, 8, 45, 96]. Consequently, scale-invariant feature\nrepresentation can be obtained via intra- or inter-layer feature fusion. Nevertheless, CNNs are not\nwell suited to model long-range dependencies 2, which is the key advantage of transformers. An\ninteresting question comes up: Can we improve vision transformers by leveraging the good properties\nof CNNs? Recently, DeiT [76] explores the idea of distilling knowledge from CNNs to transformers\nto facilitate training and improve the performance. However, it requires an off-the-shelf CNN model\nas the teacher and consumes extra training cost.\nDifferent from DeiT, we explicitly introduce intrinsic IBs into vision transformers by re-designing\nthe network structures in this paper. Current vision transformers always obtain tokens with single-\nscale context [19, 93, 80, 86, 47, 69, 77] and learn to adapt to objects at different scales from data.\nFor example, T2T-ViT [93] improves ViT by delicately generating tokens in a soft split manner.\nSpeciﬁcally, it uses a series of Tokens-to-Token transformation layers to aggregate single-scale\nneighboring contextual information and progressively structurizes the image to tokens. Motivated\nby the success of CNNs in dealing with scale variance, we explore a similar design in transformers,\ni.e., intra-layer convolutions with different receptive ﬁelds [70, 91], to embed multi-scale context\ninto tokens. Such a design allows tokens to carry useful features of objects at various scales, thereby\nnaturally having the intrinsic scale-invariance IB and explicitly facilitating transformers to learn\nscale-invariant features more efﬁciently from data. On the other hand, low-level local features are\nfundamental elements to generate high-level discriminative features. Although transformers can\nalso learn such features at shallow layers from data, they are not skilled as convolutions by design.\nRecently, [89, 43, 21] stack convolutions and attention layers sequentially and demonstrate that\nlocality is a reasonable compensation of global dependency. However, this serial structure ignores the\nglobal context during locality modeling (and vice versa). To avoid such a dilemma, we follow the\n“divide-and-conquer” idea and propose to model locality and long-range dependencies in parallel and\nthen fuse the features to account for both. In this way, we empower transformers to learn local and\nlong-range features within each block more effectively.\nTechnically, we propose a newVision Transformers Advanced by Exploring Intrinsic Inductive Bias\n(ViTAE), which is a combination of two types of basic cells, i.e., reduction cell (RC) and normal cell\n(NC). RCs are used to downsample and embed the input images into tokens with rich multi-scale\ncontext while NCs aim to jointly model locality and global dependencies in the token sequence.\nMoreover, these two types of cells share a simple basic structure, i.e., paralleled attention module and\nconvolutional layers followed by a feed-forward network (FFN). It is noteworthy that RC has an extra\npyramid reduction module with atrous convolutions of different dilation rates to embed multi-scale\ncontext into tokens. Following the setting in [93], we stack three reduction cells to reduce the spatial\nresolution by 1/16 and a series of NCs to learn discriminative features from data. ViTAE outperforms\nrepresentative vision transformers in terms of data efﬁciency and training efﬁciency (see Figure 1), as\nwell as classiﬁcation accuracy and generalization on downstream tasks.\nOur contributions are threefold. First, we explore two types of intrinsic IB in transformers, i.e.,\nscale invariance and locality, and demonstrate the effectiveness of this idea in improving the feature\nlearning ability of transformers. Second, we design a new transformer architecture named ViTAE\nbased on two new reduction and normal cells to intrinsically incorporate the above two IBs. The\nproposed ViTAE embeds multi-scale context into tokens and learns both local and long-range features\n2Despite projection in transformer can be viewed as 1 ×1 convolution [9], the term of convolution here\nrefers to those with larger kernels, e.g., 3 ×3, which are widely used in typical CNNs to extract spatial features.\n2\neffectively. Third, ViTAE outperforms representative vision transformers regarding classiﬁcation\naccuracy, data efﬁciency, training efﬁciency, and generalization on downstream tasks. ViTAE achieves\n75.3% and 82.0% top-1 accuracy on ImageNet with 4.8M and 23.6M parameters, respectively.\n2 Related Work\n2.1 CNNs with intrinsic IB\nCNNs have led to a series of breakthroughs in image classiﬁcation [38, 94, 26, 95, 87] and downstream\ncomputer vision tasks. The convolution operations in CNNs extract local features from the neighbor\npixels within the receptive ﬁeld determined by the kernel size [ 42]. Following the intuition that\nlocal pixels are more likely to be correlated in images [41], CNNs have the intrinsic IB in modeling\nlocality. In addition to the locality, another critical topic in visual tasks is scale-invariance, where\nmulti-scale features are needed to represent the objects at different scales effectively [49, 90]. For\nexample, to effectively learn features of large objects, a large receptive ﬁeld is needed by either using\nlarge convolution kernels [90, 91] or a series of convolution layers in deeper architectures [26, 32,\n68, 71]. To construct multi-scale feature representation, the classical idea is using image pyramid\n[8, 1, 55, 4, 39, 16], where features are hand-crafted or learned from a pyramid of images at different\nresolutions respectively [44, 8, 52, 63, 35, 3]. Accordingly, features from the small scale image\nmainly encode the large objects while features from the large scale image respond more to small\nobjects. In addition to the above inter-layer fusion way, another way is to aggregate multi-scale\ncontext by using multiple convolutions with different receptive ﬁelds within a single layer, i.e.,\nintra-layer fusion [96, 71, 70, 70, 72]. Either inter-layer fusion or intra-layer fusion empower CNNs\nan intrinsic IB in modeling scale-invariance. This paper introduces such an IB to vision transformers\nby following the intra-layer fusion idea and utilizing multiple convolutions with different dilation\nrates in the reduction cells to encode multi-scale context into each visual token.\n2.2 Vision transformers with learned IB\nViT [19] is the pioneering work that applies a pure transformer to vision tasks and achieves promising\nresults. However, since ViT lacks intrinsic inductive bias in modeling local visual structures, it indeed\nlearns the IB from amounts of data implicitly. Following works along this direction are to simplify\nthe model structures with fewer intrinsic IBs and directly learn them from large scale data [50, 74, 75,\n22, 18, 20, 27] which have achieved promising results and been studied actively. Another direction is\nto leverage the intrinsic IB from CNNs to facilitate the training of vision transformers, e.g., using less\ntraining data or shorter training schedules. For example, DeiT [76] proposes to distill knowledge from\nCNNs to transformers during training. However, it requires an off-the-shelf CNN model as a teacher,\nintroducing extra computation cost during training. Recently, some works try to introduce the intrinsic\nIB of CNNs into vision transformers explicitly [23, 58, 21, 43, 15, 89, 83, 92, 6, 47, 11]. For example,\n[43, 21, 83] stack convolutions and attention layers sequentially, resulting in a serial structure and\nmodeling the locality and global dependency accordingly. [ 80, 28] design sequential stage-wise\nstructures while [47, 33] apply attention within local windows. However, these serial structure may\nignore the global context during locality modeling (and vice versa). [ 88] establishes connection\nacross different scales at the cost of heavy computation. Instead, we follow the “divide-and-conquer”\nidea and propose to model locality and global dependencies simultaneously via a parallel structure\nwithin each transformer layer. Conformer [58], the most relevant concurrent work to us, employs\na unit to explore inter-block interactions between parallel convolution and transformer blocks. In\ncontrast, in ViTAE, the convolution and attention modules are designed to be complementary to each\nother within the transformer block. In addition, Conformer is not designed to have inherent scale\ninvariance IB.\n3 Methodology\n3.1 Revisit vision transformer\nWe ﬁrst give a brief review of vision transformer in this part. To adapt transformers to vision\ntasks, ViT [19] ﬁrst splits an image x ∈RH×W×C into tokens with a reduction ratio of p (i.e.,\nxt ∈R((H×W)/p2)×D), where H, W and C denote the height, width, and channel dimensions of\n3\nReduction Cell\nNormal Cell\nReduction Cell\nReduction Cell\nNormal Cell\nNormal Cell\nFeed Forward\nMulti-Head\nSelf-Attention\nGeLU\nLayer Norm\nLayer Norm\nC\nConv\nBN\nSiLU\nImg2Seq\nImg2Seq\nSeq2Img\nDifferent Dilation Rates\nCPosition Embedding ConcatElement-wise Add Feature Map\n× 2 × 2\nToken Class Token\nDog\nFeed Forward\nLayer Norm\nMulti-Head\nSelf-Attention\nConv\nBN\nSiLU\nLayer Norm\nSeq2Img\nImg2Seq\nLinear\nConv\nSiLU\nConv\nSiLU\nPyramid Reduction\nR/4 x W/4 R/8 x W/8 R/16 x W/16\nImg2Seq\nFigure 2: The structure of the proposed ViTAE. It is constructed by stacking three RCs and several\nNCs. Both types of cells share a simple basic structure, i.e., an MHSA module and a parallel\nconvolutional module followed by an FFN. In particular, RC has an extra pyramid reduction module\nusing atrous convolutions with different dilation rates to embed multi-scale context into tokens.\nthe input image, D = Cp2 denotes the token dimension. Then, an extra class token is concatenated\nto the visual tokens before adding position embeddings in an element-wise manner. The resulting\ntokens are fed into the following transformer layers. Each transformer layer is composed of two parts,\ni.e., a multi-head self-attention module (MHSA) and a feed forward network (FFN).\nMHSA Multi-head self-attention extends single-head self-attention (SHSA) by using different\nprojection matrices for each head. Speciﬁcally, the input tokens xt are ﬁrst projected to queries (Q),\nkeys (K) and values ( V ) using projection matrices, i.e., Q, K, V= xtWQ, xtQK, xtQV , where\nWQ/K/V ∈RD×D denotes the projection matrix for query, key, and value, respectively. Then, the\nself-attention operation is calculated as:\nAttention(Q, K, V) =softmax(QKT\n√\nD\n)V. (1)\nThis SHSA module is repeated for h times to formulate the MHSA module, where h is the number\nof heads. The output features of the h heads are concatenated along the channel dimension and\nformulate the output of the MHSA module.\nFFN FFN is placed on top of the MHSA module and applied to each token identically and separately.\nIt consists of two linear transformations with an activation function in between. Besides, a layer\nnormalization [2] and a shortcut are added before and aside from the MHSA and FFN, respectively.\n3.2 Overview architecture of ViTAE\nViTAE aims to introduce the intrinsic IB in CNNs to vision transformers. As shown in Figure 2,\nViTAE is composed of two types of cells, i.e., RCs and NCs. RCs are responsible for embedding\nmulti-scale context and local information into tokens, and NCs are used to further model the locality\nand long-range dependencies in the tokens. Taken an image x ∈RH×W×C as input, three RCs are\nused to gradually downsample x by 4×, 2×, and 2×, respectively. Thereby, the output tokens of\nthe RCs are of size [H/16, W/16, D] where D is the token dimension (64 in our experiments). The\noutput tokens of RCs are then ﬂattened as RHW/256×D, concatenated with the class token, and added\nby the sinusoid position encoding. Next, the tokens are fed into the following NCs, which keep the\nlength of the tokens. Finally, the prediction probability is obtained using a linear classiﬁcation layer\non the class token from the last NC.\n4\n3.3 Reduction cell\nInstead of directly splitting and ﬂatten images into visual tokens based on a linear image patch\nembedding layer, we devise the reduction cell to embed multi-scale context and local information\ninto visual tokens, which introduces the intrinsic scale-invariance and locality IBs from convolutions.\nTechnically, RC has two parallel branches responsible for modeling locality and long-range depen-\ndency, respectively, followed by an FFN for feature transformation. We denote the input feature of\nthe ith RC as fi ∈RHi×Wi×Di . The input of the ﬁrst RC is the image x. In the global dependencies\nbranch, fi is ﬁrstly fed into a Pyramid Reduction Module (PRM) to extract multi-scale context, i.e.,\nfms\ni ≜ PRMi(fi) =Cat([Convij(fi; sij, ri)|sij ∈Si, ri ∈R]), (2)\nwhere Convij(·) indicates the jth convolutional layer in the PRM ( PRMi(·)). It uses a dilation\nrate sij from the predeﬁned dilation rate set Si corresponding to the ith RC. Note that we use\nstride convolution to reduce the spatial dimension of features by a ratio ri from the predeﬁned\nreduction ratio set R. The conv features are concatenated along the channel dimension, i.e., fms\ni ∈\nR(Wi/p)×(Hi/p)×(|Si|D), where |Si|denotes the number of dilation rates in Si. fms\ni is then processed\nby an MHSA module to model long-range dependencies, i.e.,\nfg\ni = MHSA i(Img2Seq(fms\ni )), (3)\nwhere Img2Seq(·) is a simple reshape operation to ﬂatten the feature map to a 1D sequence. In this\nway, fg\ni embeds the multi-scale context in each token. In addition, we use a Parallel Convolutional\nModule (PCM) to embed local context within the tokens, which are fused with fg\ni as follows:\nflg\ni = fg\ni + PCM i(fi). (4)\nHere, PCM i(·) represents the PCM, which is composed of three stacked convolution layers and an\nImg2Seq(·) operation. It is noteworthy that the parallel convolution branch has the same spatial\ndownsampling ratio as the PRM by using stride convolutions. In this way, the token features can carry\nboth local and multi-scale context, implying that RC acquires the locality IB and scale-invariance IB\nby design. The fused tokens are then processed by the FFN, reshaped back to feature maps, and fed\ninto the following RC or NC, i.e.,\nfi+1 = Seq2Img(FFN i(flg\ni ) +flg\ni ), (5)\nwhere the Seq2Img(·) is a simple reshape operation to reshape a token sequence back to feature\nmaps. FFN i(·) represents the FFN in the ith RC. In our ViTAE, three RCs are stacked sequentially\nto gradually reduce the input image’s spatial dimension by 4×, 2×, and 2×, respectively. The feature\nmaps generated by the last RC are of a size of [H/16, W/16, D], which are then ﬂattened into visual\ntokens and fed into the following NCs.\n3.4 Normal cell\nAs shown in the bottom right part of Figure 2, NCs share a similar structure with the reduction cell\nexcept for the absence of the PRM. Due to the relatively small (1\n16 ×) spatial size of feature maps after\nRCs, it is unnecessary to use PRM in NCs. Givenf3 from the third RC, we ﬁrst concatenate it with the\nclass token tcls, and then add it to the positional encodings to get the input tokens t for the following\nNCs. Here we ignore the subscript for clarity since all NCs have an identical architecture but different\nlearnable weights. tcls is randomly initialized at the start of training and ﬁxed during the inference.\nSimilar to the RC, the tokens are fed into the MHSA module, i.e., tg = MHSA (t). Meanwhile, they\nare reshaped to 2D feature maps and fed into the PCM, i.e., tl = Img2Seq(PCM (Seq2Img(t))).\nNote that the class token is discarded in PCM because it has no spatial connections with other visual\ntokens. To further reduce the parameters in NCs, we use group convolutions in PCM. The features\nfrom MHSA and PCM are then fused via element-wise sum, i.e., tlg = tg + tl. Finally, tlg are fed\ninto the FFN to get the output features of NC, i.e., tnc = FFN (tlg) +tlg. Similar to ViT [19], we\napply layer normalization to the class token generated by the last NC and feed it to the classiﬁcation\nhead to get the ﬁnal classiﬁcation result.\n3.5 Model details\n5\nTable 1: Model details of two variants of ViTAE.\nModel\nReduction Cell Normal Cell Params Macs\nDilation Cells Heads Embed Cells (M) (G)\nViTAE-T [1, 2, 3, 4] ↓ 3 4 256 7 4.8 1.5\nViTAE-S [1, 2, 3, 4] ↓ 3 6 384 14 23.6 5.6\nWe use two variants of ViTAE in\nour experiments for a fair com-\nparison of other models with sim-\nilar model sizes. The details of\nthem are summarized in Table 1.\nIn the ﬁrst RC, the default convo-\nlution kernel size is 7 ×7 with a\nstride of 4 and dilation rates of S1 = [1, 2, 3, 4]. In the following two RCs, the convolution kernel\nsize is 3 ×3 with a stride of 2 and dilation rates of S2 = [1, 2, 3] and S3 = [1, 2], respectively. Since\nthe spatial dimension of tokens decreases, there is no need to use large kernels and dilation rates.\nPCM in both RCs and NCs comprises three convolutional layers with a kernel size of 3 ×3.\n4 Experiments\n4.1 Implementation details\nWe train and test the proposed ViTAE model on the standard ImageNet [38] dataset, which contains\nabout 1.3 million images and covers 1k classes. Unless explicitly stated, the image size during training\nis set to 224 ×224. We use the AdamW [48] optimizer with the cosine learning rate scheduler and\nuses the data augmentation strategy exactly the same as T2T [93] for a fair comparison, regarding the\ntraining strategies and the size of models. We use a batch size of 512 for training all our models and\nset the initial learning rate to be 5e-4. The results of our models can be found in Table 2, where all\nthe models are trained for 300 epochs on 8 V100 GPUs. The models are built on PyTorch [57] and\nTIMM [82].\n4.2 Comparison with the state-of-the-art\nWe compare our ViTAE with both CNN models and vision transformers with similar model sizes\nin Table 2. Both Top-1/5 accuracy and real Top-1 accuracy on the ImageNet validation set are\nreported. We categorize the methods into CNN models, vision transformers with learned IB, and\nvision transformers with introduced intrinsic IB. Compared with CNN models, our ViTAE-T achieves\na 75.3% Top-1 accuracy, which is better than ResNet-18 with more parameters. The real Top-1\naccuracy of the ViTAE model is 82.9%, which is comparable to ResNet-50 that has four more times\nof parameters than ours. Similarly, our ViTAE-S achieves 82.0% Top-1 accuracy with half of the\nparameters of ResNet-101 and ResNet-152, showing the superiority of learning both local and long-\nrange features from speciﬁc structures with corresponding intrinsic IBs by design. Similar phenomena\ncan also be observed when comparing ViTAE-T with MobileNetV1 [ 31] and MobileNetV2 [65],\nwhere ViTAE obtains better performance with fewer parameters. When compared with larger models\nwhich are searched according to NAS [73], our ViTAE-S achieves a similar performance when using\n384 ×384 images as input, which further shows the potential of vision transformers with intrinsic IB.\nIn addition, among the transformers with learned IB, ViT is the ﬁrst pure transformer model for\nvisual recognition. DeiT shares the same structure with ViT but uses different data augmentation and\ntraining strategies to facilitate the learning of transformers. DeiT\n⚗ denotes using an off-the-shelf\nCNN model as the teacher model to train DeiT, which introduces the intrinsic IB from CNN to\ntransformer implicitly in a knowledge distillation manner, showing better performance than the\nvanilla ViT on the ImageNet dataset. It is exciting to see that our ViTAE-T with fewer parameters\neven outperforms the distilled model DeiT\n⚗ , demonstrating the efﬁcacy of introducing intrinsic IBs\nin transformers by design. Besides, compared with other transformers with explicit intrinsic IB, our\nViTAE with fewer parameters also achieves comparable or better performance. For instance, ViTAE-T\nachieves comparable performance with LocalVit-T but has 1M fewer parameters, demonstrating the\nsuperiority of the proposed RCs and NCs in introducing intrinsic IBs.\n4.3 Ablation study\nWe use T2T-ViT [93] as our baseline model in the following ablation study of our ViTAE. As shown\nin Table 3, we investigate the hyper-parameter settings in RCs and NCs by isolating them separately.\nAll the models are trained for 100 epochs on ImageNet and follow the same training setting and data\naugmentation strategy as described in Section 4.1.\n6\nTable 2: Comparison of ViTAE and SOTA methods on the ImageNet validation set.\nType Model Params MACs Input ImageNet Real\n(M) (G) Size Top-1 Top-5 Top-1\nCNN\nResNet-18 [26] 11.7 3.6 224 70.3 86.7 77.3\nResNet-50 [26] 25.6 7.6 224 76.7 93.3 82.5\nResNet-101 [26] 44.5 15.2 224 78.3 94.1 83.7\nResNet-152 [26] 60.2 22.6 224 78.9 94.4 84.1\nEfﬁcientNet-B0 [73] 5.3 0.8 224 77.1 93.3 83.5\nEfﬁcientNet-B4 [73] 19.3 8.4 380 82.9 96.4 88.0\nMobileNetV1 [31] 4.3 0.6 224 72.3 - -\nMobileNetV2(1.4) [65] 6.9 0.6 224 74.7 - -\nRegNetY-600M [62] 6.1 1.2 224 75.5 - -\nRegNetY-4GF [62] 20.6 8.0 224 80.0 - 86.4\nRegNetY-8GF [62] 39.2 16.0 224 81.7 - 87.4\nTransformer\nDeiT-T [76] 5.7 2.6 224 72.2 91.1 80.6\nDeiT-T\n⚗ [76] 5.7 2.6 224 74.5 91.9 82.1\nLocalViT-T [43] 5.9 2.6 224 74.8 92.6 -\nLocalViT-T2T [43] 4.3 2.4 224 72.5 - -\nConT-Ti [89] 5.8 1.6 224 74.9 - -\nPiT-Ti [29] 4.9 1.4 224 73.0 - -\nT2T-ViT-7 [93] 4.3 1.2 224 71.7 90.9 79.7\nViTAE-T 4.8 1.5 224 75.3 92.7 82.9\nViTAE-T↑384 4.8 5.7 384 77.2 93.8 84.4\nCeiT-T [92] 6.4 2.4 224 76.4 93.4 83.6\nConViT-Ti [15] 6.0 2.0 224 73.1 - -\nCrossViT-Ti [6] 6.9 3.2 224 73.4 - -\nViTAE-6M 6.5 2.0 224 77.9 94.1 84.9\nPVT-T [80] 13.2 3.8 224 75.1 - -\nLocalViT-PVT [43] 13.5 9.6 224 78.2 94.2 -\nConViT-Ti+ [15] 10.0 4.0 224 76.7 - -\nPiT-XS [29] 10.6 2.8 224 78.1 - -\nConT-M [89] 19.2 6.2 224 80.2 - -\nViTAE-13M 13.2 3.4 224 81.0 95.4 86.8\nDeiT-S [76] 22.1 9.8 224 79.9 95.0 85.7\nDeiT-S\n⚗ [76] 22.1 9.8 224 81.2 95.4 86.8\nPVT-S [80] 24.5 7.6 224 79.8 -\nConformer-Ti [58] 23.5 5.2 224 81.3 - -\nSwin-T [47] 29.0 9.0 224 81.3 - -\nCeiT-S [92] 24.2 9.0 224 82.0 95.9 87.3\nCvT-13 [83] 20.0 9.0 224 81.6 - 86.7\nConViT-S [15] 27.0 10.8 224 81.3 - -\nCrossViT-S [6] 26.7 11.2 224 81.0 - -\nPiT-S [29] 23.5 4.8 224 80.9 - -\nTNT-S [23] 23.8 10.4 224 81.3 95.6 -\nTwins-PCPVT-S[10] 24.1 7.4 224 81.2 - -\nTwins-SVT-S [10] 24.0 5.6 224 81.7 - -\nT2T-ViT-14 [93] 21.5 5.2 224 81.5 95.7 86.8\nViTAE-S 23.6 5.6 224 82.0 95.9 87.0\nViTAE-S↑384 23.6 20.2 384 83.0 96.2 87.5\n7\nWe use✓ and ×to denote whether or not the corresponding module is enabled during the experiments.\nIf all columns under the RC and NC are marked ×as shown in the ﬁrst row, the model becomes the\nstandard T2T-ViT model. “Pre” indicates the output features of PCM and MHSA are fused before\nFFN while “Post” indicates a late fusion strategy correspondingly. “BN” indicates whether PCM uses\nBN after the convolutional layer or not. “×3” in the ﬁrst column denotes that the dilation rate set\nis the same in the three RCs. “ [1, 2, 3, 4] ↓” denotes using lower dilation rates in deeper RCs, i.e.,\nS1 = [1, 2, 3, 4], S2 = [1, 2, 3], S3 = [1, 2].\nTable 3: Ablation Study of RCs and NCs in our ViTAE.\n“Pre” indicates the output features of PCM and MHSA are\nfused before FFN while “Post” indicates a late fusion strategy\ncorrespondingly. “BN” indicates whether PCM uses BN or\nnot. “ [1, 2, 3, 4] ↓” denotes using smaller dilation rates in\ndeeper RCs, i.e., S1 = [1, 2, 3, 4], S2 = [1, 2, 3], S3 = [1, 2].\nReduction Cell Normal Cell\nTop-1Dilation (S1 ∼S3) PCM Pre Post BN\n× × × × × 68.7\n× × ✓ × × 69.1\n× × × ✓ × 69.0\n× × × ✓ ✓ 68.8\n× × ✓ × ✓ 69.9\n[1, 2] ×3 × × × × 69.5\n[1, 2, 3] ×3 × × × × 69.9\n[1, 2, 3, 4] ×3 × × × × 69.2\n[1, 2, 3, 4, 5] ×3 × × × × 68.9\n[1, 2, 3, 4] ↓ × × × × 69.8\n[1, 2, 3, 4] ↓ ✓ × × × 71.7\n[1, 2, 3, 4] ↓ ✓ ✓ × ✓ 72.6\nAs can be seen, using a pre-fusion\nstrategy and BN achieves the best\n69.9% Top-1 accuracy among other\nsettings. It is noteworthy that all the\nvariants of NC outperform the vanilla\nT2T-ViT, implying the effectiveness\nof PCM, which introduces the intrin-\nsic locality IB in transformers. It can\nalso be observed that BN plays an im-\nportant role in improving the model’s\nperformance as it can help to allevi-\nate the scale deviation between convo-\nlution’s and attention’s features. For\nthe RC, we ﬁrst investigate the impact\nof using different dilation rates in the\nPRM, as shown in the ﬁrst column.\nAs can be seen, using larger dilation\nrates ( e.g., 4 or 5) does not deliver\nbetter performance. We suspect that\nlarger dilation rates may lead to plain\nfeatures in the deeper RCs due to the\nsmaller resolution of feature maps. To\nvalidate the hypothesis, we use smaller dilation rates in deeper RCs as denoted by [1, 2, 3, 4] ↓. As\ncan be seen, it achieves comparable performance as [1, 2, 3]×. However, compared with [1, 2, 3, 4] ↓,\n[1, 2, 3]×increases the amount of parameters from 4.35M to 4.6M. Therefore, we select [1, 2, 3, 4] ↓\nas the default setting. In addition, after using PCM in the RC, it introduces the intrinsic locality IB,\nand the performance increases to 71.7% Top-1 accuracy. Finally, the combination of RCs and NCs\nachieves the best accuracy at 72.6%, demonstrating the complementarity between our RCs and NCs.\n4.4 Data efﬁciency and training efﬁciency\nTo validate the effectiveness of the introduced intrinsic IBs in improving data efﬁciency and training\nefﬁciency, we compare our ViTAE with T2T-ViT at different training settings: (a) training them\nusing 20%, 60%, and 100% ImageNet training set for equivalent 100 epochs on the full ImageNet\ntraining set, e.g., we employ 5 times epochs when using 20% data for training compared with using\n100% data; and (b) training them using the full ImageNet training set for 100, 200, and 300 epochs\nrespectively. The results are shown in Figure 1. As can be seen, ViTAE consistently outperforms\nthe T2T-ViT baseline by a large margin in terms of both data efﬁciency and training efﬁciency. For\nexample, ViTAE using only 20% training data achieves comparable performance with T2T-ViT\nusing all data. When 60% training data are used, ViTAE signiﬁcantly outperforms T2T-ViT using\nall data by about an absolute 3% accuracy. It is also noteworthy that ViTAE trained for only 100\nepochs has outperformed T2T-ViT trained for 300 epochs. After training ViTAE for 300 epochs, its\nperformance is signiﬁcantly boosted to 75.3% Top-1 accuracy. With the proposed RCs and NCs, the\ntransformer layers in our ViTAE only need to focus on modeling long-range dependencies, leaving the\nlocality and multi-scale context modeling to its convolution counterparts, i.e., PCM and PRM. Such\na “divide-and-conquer” strategy facilitates the training of vision transformers, making it possible to\nlearn more efﬁciently with less training data and fewer training epochs.\nTo further validate the data efﬁciency of ViTAE model, we train the ViTAE model from scratch\non the smaller datasets, i.e., Cifar10 and Cifar100. The results are summarized in Table 4. It can\nbe viewed that with only 1/7 number of epochs, the ViTAE-T model achieves better classiﬁcation\n8\nperformance on Cifar10 dataset, with far fewer parameters (4.8M v.s. 86M), which further conﬁrms\nViTAE model’s data efﬁciency.\nTable 4: Results of training from scratch on Cifar10/100.\nModel Params (M) Top-1 Acc Epochs Dataset\nDeiT-B 86.0 97.5 7000 Cifar10\nViTAE-T 4.8 97.7 1000 Cifar10\nViTAE-T 4.8 85.0 1000 Cifar100\n4.5 Generalization on downstream tasks\nTable 5: Generalization of ViTAE and SOTA methods on different downstream tasks.\nModel Params (M) Cifar10 Cifar100 iNat19 Cars Flowers Pets\nGraﬁt ResNet-50 [78] 25.6 - - 75.9 92.5 98.2 -\nEfﬁcientNet-B5 [73] 30 98.1 91.1 - - 98.5 -\nViT-B/16 [19] 86.5 98.1 87.1 - - 89.5 93.8\nViT-L/16 [19] 304.3 97.9 86.4 - - 89.7 93.6\nDeiT-B [76] 86.6 99.1 90.8 77.7 92.1 98.4 -\nT2T-ViT-14 [93] 21.5 98.3 88.4 - - - -\nViTAE-T 4.8 97.3 86.0 73.3 89.5 97.5 92.6\nViTAE-S 23.6 98.8 90.8 76.0 91.4 97.8 94.2\nWe further investigate the generalization of the proposed ViTAE models on downstream tasks by ﬁne-\ntuning them on the training sets of several ﬁne-grained classiﬁcation tasks3, including Flowers [53],\nCars [36], Pets [56], and iNaturalist19. We also ﬁne-tune the proposed ViTAE models on Cifar10 [37]\nand Cifar100 [37]. The results are shown in Table 5. It can be seen that ViTAE achieves SOTA\nperformance on most of the datasets using comparable or fewer parameters. These results demonstrate\nthat the good generalization ability of our ViTAE.\n4.6 Visual inspection of ViTAE\nTo further analyze the property of our ViTAE, we ﬁrst calculate the average attention distance of each\nlayer in ViTAE-T and the baseline T2T-ViT-7 on the ImageNet test set, respectively. The results are\nshown in Figure 3. It can be observed that with the usage of PCM, which focuses on modeling locality,\nthe transformer layers in the proposed NCs can better focus on modeling long-range dependencies,\nespecially in shallow layers. In the deep layers, the average attention distances of ViTAE-T and\nT2T-ViT-7 are almost the same since modeling long-range dependencies is much more important.\n40\n60\n80\n100\n0 1 2 3 4 5 6 7 8\nAttention Distance (pixel) Layers\nT2T-ViT ViTAE\nFigure 3: The average per-layer attention dis-\ntance of T2T-ViT-7 and our ViTAE-T.\nThese results conﬁrm the effectiveness of the adopted\n“divide-and-conquer” idea in the proposed ViTAE,\ni.e., introducing the intrinsic locality IB from con-\nvolutions into vision transformers makes it possible\nthat transformer layers only need to be responsible to\nlong-range dependencies, since locality can be well\nmodeled by convolutions in PCM.\nBesides, we apply Grad-CAM [66] on the MHSA’s\noutput in the last NC to qualitatively inspect ViTAE.\nThe visualization results are provided in Figure 4.\nCompared with the baseline T2T-ViT, our ViTAE\ncovers the single or multiple targets in the images\nmore precisely and attends less to the background. Moreover, ViTAE can better handle the scale\nvariance issue as shown in Figure 4(b). Namely, it can precisely cover the birds no matter they are\nin small, middle, or large size. Such observations demonstrate that introducing the intrinsic IBs of\nlocality and scale-invariance from convolutions to transformers helps ViTAE learn more discriminate\nfeatures than the pure transformers.\n3The performance of ViTAE on dense prediction tasks such as detection, segmentation, pose estimation, can\nbe found in the supplementary material.\n9\n(a) (b)\nInput\nT2T-ViT\nViTAE\nFigure 4: Visual inspection of T2T-ViT and ViTAE using Grad-CAM [66]. (a) Images containing\nmultiple or single objects and the heatmaps. (b) Images containing the same class of objects at\ndifferent scales and the heatmaps (Best viewed in color).\n5 Limitation and discussion\nIn this paper, we explore two types of IBs and incorporate them into transformers through the proposed\nreduction and normal cells. With the collaboration of these two cells, our ViTAE model achieves\nimpressive performance on the ImageNet with fast convergence and high data efﬁciency. Nevertheless,\ndue to computational resource constraints, we have not scaled the ViTAE model and train it on large-\nsize dataset, e.g., ImageNet-21K [38] and JFT-300M [30]. Although it remains unclear by now, we\nare optimistic about its scale property from the following preliminary evidence. As illustrated in\nFigure 2, our ViTAE model can be viewed as an intra-cell ensemble of complementary transformer\nlayers and convolution layers owing to the skip connection and parallel structure. According to the\nattention distance analysis shown in Figure 3, the ensemble nature enables the transformer layers\nand convolution layers to focus on what they are good at, i.e., modeling long-range dependencies\nand locality. Therefore, ViTAE is very likely to learn better feature representation from large-scale\ndata. Besides, we only study two typical IBs in this paper. More kinds of IBs such as constituting\nviewpoint invariance [64] can be explored in the future study.\n6 Conclusion\nIn this paper, we re-design the transformer block by proposing two basic cells (reduction cells and\nnormal cells) to incorporate two types of intrinsic inductive bias (IB) into transformers, i.e., locality\nand scale-invariance, resulting in a simple yet effective vision transformer architecture named ViTAE.\nExtensive experiments show that ViTAE outperforms representative vision transformers in various\nrespects including classiﬁcation accuracy, data efﬁciency, training efﬁciency, and generalization\nability on downstream tasks. We plan to scale ViTAE to the large or huge model size and train it on\nlarge-size datasets in the future study. In addition, other kinds of IBs will also be investigated. We\nhope that this study will provide valuable insights to the following studies of introducing intrinsic IB\ninto vision transformers and understanding the impact of intrinsic and learned IBs.\nAcknowledgement Dr. Jing Zhang is supported by the ARC project FL-170100117.\n10\nReferences\n[1] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden. Pyramid methods in image\nprocessing. RCA engineer, 29(6):33–41, 1984.\n[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n[3] H. Bay, T. Tuytelaars, and L. Van Gool. Surf: Speeded up robust features. In European conference on\ncomputer vision, pages 404–417. Springer, 2006.\n[4] P. J. Burt and E. H. Adelson. The laplacian pyramid as a compact image code. In Readings in computer\nvision, pages 671–679. Elsevier, 1987.\n[5] Z. Cai and N. Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 6154–6162, 2018.\n[6] C.-F. Chen, Q. Fan, and R. Panda. Crossvit: Cross-attention multi-scale vision transformer for image\nclassiﬁcation. arXiv preprint arXiv:2103.14899, 2021.\n[7] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao. Pre-trained image\nprocessing transformer. arXiv preprint arXiv:2012.00364, 2020.\n[8] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam. Rethinking atrous convolution for semantic image\nsegmentation. arXiv preprint arXiv:1706.05587, 2017.\n[9] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. arXiv\npreprint arXiv:2104.02057, 2021.\n[10] X. Chu, Z. Tian, Y . Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting spatial\nattention design in vision transformers. arXiv preprint arXiv:2104.13840, 2021.\n[11] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and C. Shen. Conditional positional encodings for\nvision transformers. arXiv preprint arXiv:2102.10882, 2021.\n[12] M. Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https:\n//github.com/open-mmlab/mmsegmentation, 2020.\n[13] M. Contributors. Openmmlab pose estimation toolbox and benchmark. https://github.com/\nopen-mmlab/mmpose, 2020.\n[14] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdinov. Transformer-xl: Attentive\nlanguage models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019.\n[15] S. d’Ascoli, H. Touvron, M. Leavitt, A. Morcos, G. Biroli, and L. Sagun. Convit: Improving vision\ntransformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697, 2021.\n[16] H. Demirel and G. Anbarjafari. Image resolution enhancement by using discrete and stationary wavelet\ndecomposition. IEEE transactions on image processing, 20(5):1458–1460, 2010.\n[17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[18] X. Ding, X. Zhang, J. Han, and G. Ding. Repmlp: Re-parameterizing convolutions into fully-connected\nlayers for image recognition. arXiv preprint arXiv:2105.01883, 2021.\n[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[20] A. El-Nouby, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin, I. Laptev, N. Neverova, G. Syn-\nnaeve, J. Verbeek, et al. Xcit: Cross-covariance image transformers. arXiv preprint arXiv:2106.09681,\n2021.\n[21] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. Jégou, and M. Douze. Levit: a vision\ntransformer in convnet’s clothing for faster inference.arXiv preprint arXiv:2104.01136, 2021.\n[22] M.-H. Guo, Z.-N. Liu, T.-J. Mu, and S.-M. Hu. Beyond self-attention: External attention using two linear\nlayers for visual tasks. arXiv preprint arXiv:2105.02358, 2021.\n[23] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021.\n[24] K. He, G. Gkioxari, P. Dollár, and R. Girshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961–2969, 2017.\n[25] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual\nrecognition. IEEE transactions on pattern analysis and machine intelligence, 37(9):1904–1916, 2015.\n[26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[27] L. He, Y . Dong, Y . Wang, D. Tao, and Z. Lin. Gauge equivariant transformer. InThirty-Fifth Conference\non Neural Information Processing Systems, 2021.\n[28] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh. Rethinking spatial dimensions of vision\ntransformers. In International Conference on Computer Vision (ICCV), 2021.\n[29] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh. Rethinking spatial dimensions of vision\ntransformers. arXiv preprint arXiv:2103.16302, 2021.\n[30] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning\nand Representation Learning Workshop, 2015.\n[31] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam.\nMobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint\narXiv:1704.04861, 2017.\n[32] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708,\n11\n2017.\n[33] Z. Huang, Y . Ben, G. Luo, P. Cheng, G. Yu, and B. Fu. Shufﬂe transformer: Rethinking spatial shufﬂe for\nvision transformer. arXiv preprint arXiv:2106.03650, 2021.\n[34] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. In International conference on machine learning, pages 448–456. PMLR, 2015.\n[35] Y . Ke and R. Sukthankar. Pca-sift: A more distinctive representation for local image descriptors. In\nProceedings of the IEEE conference on computer vision and pattern recognition, volume 2, pages II–II.\nIEEE, 2004.\n[36] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for ﬁne-grained categorization. In\n4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia,\n2013.\n[37] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n[38] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. Advances in neural information processing systems, 25:1097–1105, 2012.\n[39] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep laplacian pyramid networks for fast and accurate\nsuper-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 624–632, 2017.\n[40] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised\nlearning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n[41] Y . LeCun, Y . Bengio, et al. Convolutional networks for images, speech, and time series.The handbook of\nbrain theory and neural networks, 3361(10):1995, 1995.\n[42] Y . LeCun, Y . Bengio, and G. Hinton. Deep learning.nature, 521(7553):436–444, 2015.\n[43] Y . Li, K. Zhang, J. Cao, R. Timofte, and L. Van Gool. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707, 2021.\n[44] G. Lin, C. Shen, A. Van Den Hengel, and I. Reid. Efﬁcient piecewise training of deep structured models\nfor semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3194–3203, 2016.\n[45] T.-Y . Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for\nobject detection. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 2117–2125, 2017.\n[46] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov.\nRoberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n[47] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical\nvision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.\n[48] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on\nLearning Representations, 2018.\n[49] W. Luo, Y . Li, R. Urtasun, and R. S. Zemel. Understanding the effective receptive ﬁeld in deep\nconvolutional neural networks. InProceedings of the 30th International Conference on Neural Information\nProcessing Systems, volume 29, pages 4898–4906, 2016.\n[50] L. Melas-Kyriazi. Do you even need attention? a stack of feed-forward layers does surprisingly well on\nimagenet. arXiv: Computer Vision and Pattern Recognition, 2021.\n[51] H. Nam, J.-W. Ha, and J. Kim. Dual attention networks for multimodal reasoning and matching. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 299–307, 2017.\n[52] P. C. Ng and S. Henikoff. Sift: Predicting amino acid changes that affect protein function. Nucleic acids\nresearch, 31(13):3812–3814, 2003.\n[53] M.-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\nIndian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.\n[54] S. W. Oh, J.-Y . Lee, N. Xu, and S. J. Kim. Video object segmentation using space-time memory networks.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9226–9235, 2019.\n[55] H. Olkkonen and P. Pesola. Gaussian pyramid wavelet transform for multiresolution analysis of images.\nGraphical Models and Image Processing, 58(4):394–398, 1996.\n[56] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V . Jawahar. Cats and dogs. In IEEE Conference on\nComputer Vision and Pattern Recognition, 2012.\n[57] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In\nAdvances in Neural Information Processing Systems, volume 32, pages 8026–8037, 2019.\n[58] Z. Peng, W. Huang, S. Gu, L. Xie, Y . Wang, J. Jiao, and Q. Ye. Conformer: Local features coupling\nglobal representations for visual recognition. arXiv preprint arXiv:2105.03889, 2021.\n[59] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark\ndataset and evaluation methodology for video object segmentation. InProceedings of the IEEE conference\non computer vision and pattern recognition, pages 724–732, 2016.\n[60] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbeláez, A. Sorkine-Hornung, and L. Van Gool. The 2017 davis\nchallenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017.\n[61] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\n[62] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Dollár. Designing network design spaces. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 10428–10436,\n12\n2020.\n[63] E. Rublee, V . Rabaud, K. Konolige, and G. Bradski. Orb: An efﬁcient alternative to sift or surf. In\nProceedings of the IEEE international conference on computer vision, pages 2564–2571. Ieee, 2011.\n[64] S. Sabour, N. Frosst, and G. E. Hinton. Dynamic routing between capsules. arXiv preprint\narXiv:1710.09829, 2017.\n[65] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and\nlinear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4510–4520, 2018.\n[66] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual\nexplanations from deep networks via gradient-based localization. InProceedings of the IEEE international\nconference on computer vision, pages 618–626, 2017.\n[67] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations.arXiv preprint\narXiv:1803.02155, 2018.\n[68] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556, 2014.\n[69] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani. Bottleneck transformers for\nvisual recognition. arXiv preprint arXiv:2101.11605, 2021.\n[70] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. Alemi. Inception-v4, inception-resnet and the impact of\nresidual connections on learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 31, 2017.\n[71] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich.\nGoing deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1–9, 2015.\n[72] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for\ncomputer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 2818–2826, 2016.\n[73] M. Tan and Q. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks. In\nInternational Conference on Machine Learning, pages 6105–6114. PMLR, 2019.\n[74] I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, D. Keysers,\nJ. Uszkoreit, M. Lucic, and A. Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. arXiv preprint\narXiv:2105.01601, 2021.\n[75] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave, A. Joulin, G. Synnaeve,\nJ. Verbeek, and H. Jégou. Resmlp: Feedforward networks for image classiﬁcation with data-efﬁcient\ntraining. arXiv preprint arXiv:2105.03404, 2021.\n[76] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou. Training data-efﬁcient image\ntransformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.\n[77] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jégou. Going deeper with image transformers.\narXiv preprint arXiv:2103.17239, 2021.\n[78] H. Touvron, A. Sablayrolles, M. Douze, M. Cord, and H. Jégou. Graﬁt: Learning ﬁne-grained image\nrepresentations with coarse labels. arXiv preprint arXiv:2011.12982, 2020.\n[79] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems, volume 30, pages 5998–6008, 2017.\n[80] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without convolutions.arXiv preprint arXiv:2102.12122,\n2021.\n[81] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 7794–7803, 2018.\n[82] R. Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\n2019.\n[83] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang. Cvt: Introducing convolutions to\nvision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[84] B. Xiao, H. Wu, and Y . Wei. Simple baselines for human pose estimation and tracking. InProceedings of\nthe European Conference on Computer Vision (ECCV), September 2018.\n[85] T. Xiao, Y . Liu, B. Zhou, Y . Jiang, and J. Sun. Uniﬁed perceptual parsing for scene understanding. In\nProceedings of the European Conference on Computer Vision (ECCV), pages 418–434, 2018.\n[86] J. Xie, R. Zeng, Q. Wang, Z. Zhou, and P. Li. So-vit: Mind visual tokens for vision transformer. arXiv\npreprint arXiv:2104.10935, 2021.\n[87] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep neural\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n1492–1500, 2017.\n[88] W. Xu, Y . Xu, T. Chang, and Z. Tu. Co-scale conv-attentional image transformers. InProceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV), pages 9981–9990, October 2021.\n[89] H. Yan, Z. Li, W. Li, C. Wang, M. Wu, and C. Zhang. Contnet: Why not use convolution and transformer\nat the same time? arXiv preprint arXiv:2104.13497, 2021.\n[90] F. Yu and V . Koltun. Multi-scale context aggregation by dilated convolutions. InICLR 2016 : International\nConference on Learning Representations 2016, 2016.\n13\n[91] F. Yu, V . Koltun, and T. Funkhouser. Dilated residual networks. InProceedings of the IEEE conference\non computer vision and pattern recognition, pages 472–480, 2017.\n[92] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu. Incorporating convolution designs into visual\ntransformers. arXiv preprint arXiv:2103.11816, 2021.\n[93] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. E. Tay, J. Feng, and S. Yan. Tokens-to-token vit:\nTraining vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.\n[94] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European\nconference on computer vision, pages 818–833. Springer, 2014.\n[95] X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufﬂenet: An extremely efﬁcient convolutional neural network\nfor mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 6848–6856, 2018.\n[96] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing network. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 2881–2890, 2017.\n[97] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng, T. Xiang, P. H. S. Torr, and L. Zhang.\nRethinking semantic segmentation from a sequence-to-sequence perspective with transformers. arXiv\npreprint arXiv:2012.15840, 2020.\n[98] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba. Scene parsing through ade20k dataset.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633–641, 2017.\n[99] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba. Semantic understanding of\nscenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302–321, 2019.\n[100] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. Deformable detr: Deformable transformers for\nend-to-end object detection. In International Conference on Learning Representations, 2021.\n14\nA Appendix\nA.1 Results of other ViTAE variants\nTo make a fair comparison of our ViTAE model and other methods, we further design three more\nViTAE variants and present their results in Table 6. As can be seen, our ViTAE-T model achieves\n75.3% Top-1 accuracy on ImageNet [38] with 4.8M parameters, which outperforms other transformer\nmethods with even more than 5M parameters. With 6.5M parameters, ViTAE obtains 77.9% Top-1\naccuracy, outperforming ResNet-50 [26] with 1.2% absolute improvement and 3/4 less parameters.\nThese results demonstrate the potential of transformers with intrinsic IBs. Among vision transformers\nmodels, ViTAE-T outperforms DeiT-T\n⚗ [76] with similar parameters, while ViTAE-T does not\nrequire extra teacher models. Similarly, with 6.5M parameters, ViTAE-6M outperforms both trans-\nformers with learned IB [ 15] and transformers with intrinsic IB in a serial manner [ 92]. Similar\nphenomena can also be observed with the size of models increase, e.g., the ViTAE-S model achieves\nstate-of-the-art performance with fewer parameters.\nBesides, the classic vision transformer design is not well suited for downstream tasks like detection,\nsegmentation, pose estimation and etc. The stage-wise design can better adapt to the popular vision\nbackbones for these tasks. To fully explore the potential of the proposed RC and NC modules, we\nalso design the stage-wise variants of ViTAE model as shown in Table 7 and their classiﬁcation\nperformances are summarized in Table 6. The “NC Arrangement” means the number of NCs arranged\nafter each RC. We follow the ResNet [26] and Swin [47] experience to design ViTAE’s stage variants,\nwhere the spatial size are downsampled by 4, 2, 2, 2 in each stage, except for the ViTAE-T-Stage\nmodel, where we only adopts the ﬁrst three stages since the network is shallow. As shown in Table 6,\nthe stage-wise design can further improve the performance with fewer parameters.\nA.2 Performance on downstream tasks\nWe further validate the performance of the proposed ViTAE models on detection, segmentation, pose\nestimation, and video object segmentation.\nA.3 Object detection\nTo evaluate ViTAE’s performance on object detection and instance segmentation tasks, we adopt\nMask RCNN [24] and Cascade RCNN [5] as the detection framework, and ﬁnetune the models on\nCOCO 2017 dataset, which contains 118K training, 5K validation and 20K test-dev images. We\nadopt exactly the same training setting used in Swin [ 47], i.e., multi-scale training and AdamW\noptimizer. We compare the performance of ViTAE with the classic CNN backbone,i.e., ResNet [26],\nand the transformer structure, i.e., Swin [47]. The comparisons are conducted by simply replacing the\nbackbone while keeping other conﬁgurations unchanged. The results are summarized in Table 8. It\ncan be concluded that the ViTAE-S-Stage model can obtain the best performance on object detection\nand instance segmentation, with both frameworks.\nA.4 Semantic segmentation\nWe evaluate the semantic segmentation performance of the ViTAE model on ADE20K [ 98, 99].\nThe ADE20K dataset covers 150 semantic categories with 20K images for training and 2K images\nfor validation. We follow Swin’s [47] training and testing setting. We adopt UperNet [ 85] as the\nsegmentation framework and train the models for 160K iterations, with default setting used in\nmmsegmentation [12]. The results can be found in Table 8. It can be concluded that the ViTAE\nbackbone using 10M fewer parameters achieves better performance than ResNet-50 [26] and Swin-\nT [47] on segmentation.\nA.5 Pose estimation\nFor human pose estimation, we adopt the simple baseline [84] as the pose estimation framework and\ntest the ViTAE models’ performance on the COCO dataset. The experiment are conducted following\nthe default settings used in mmpose [13]. As shown in Table 8, the ViTAE-based model obtains an\nabsolute 2% mAP gain than ResNet [26] models with 7M parameters fewer.\nA.6 Video object segmentation\nFor VOS tasks, the STM [54] framework is adopted and we replace the backbone network with the\nViTAE-T-Stage model. Davis-2016 [59] and Davis-2017 [60] are used as the benchmark datasets. The\nﬁrst dataset contains 20 videos annotated with masks each for a single target object. The Davis-2017\ndataset is a multi-object extension of Davis-2016, with 59 objects in 30 videos. The training and\ntesting setting are the same as in STM [ 54]. With 29M parameters fewer, the ViTAE-based STM\nachieves an absolute 0.5 J&F scores improvement on Davis 2016 and 0.7 J&F scores improved on\n15\nTable 6: Comparison with SOTA methods.\nType Model Params MACs Input ImageNet Real\n(M) (G) Size Top-1 Top-5 Top-1\nCNN\nResNet-18 [26] 11.7 3.6 224 70.3 86.7 77.3\nResNet-50 [26] 25.6 7.6 224 76.7 93.3 82.5\nResNet-101 [26] 44.5 15.2 224 78.3 94.1 83.7\nResNet-152 [26] 60.2 22.6 224 78.9 94.4 84.1\nEfﬁcientNet-B0 [73] 5.3 0.8 224 77.1 93.3 83.5\nEfﬁcientNet-B4 [73] 19.3 8.4 380 82.9 96.4 88.0\nMobileNetV1 [31] 4.3 0.6 224 72.3 - -\nMobileNetV2(1.4) [65] 6.9 0.6 224 74.7 - -\nRegNetY-600M [62] 6.1 1.2 224 75.5 - -\nRegNetY-4GF [62] 20.6 8.0 224 80.0 - 86.4\nRegNetY-8GF [62] 39.2 16.0 224 81.7 - 87.4\nTransformer\nDeiT-T [76] 5.7 2.6 224 72.2 91.1 80.6\nDeiT-T\n⚗ [76] 5.7 2.6 224 74.5 91.9 82.1\nLocalViT-T [43] 5.9 2.6 224 74.8 92.6 -\nLocalViT-T2T [43] 4.3 2.4 224 72.5 - -\nConT-Ti [89] 5.8 1.6 224 74.9 - -\nPiT-Ti [29] 4.9 1.4 224 73.0 - -\nT2T-ViT-7 [93] 4.3 1.2 224 71.7 90.9 79.7\nViTAE-T 4.8 1.5 224 75.3 92.7 82.9\nViTAE-T-Stage 4.8 2.3 224 76.8 93.5 84.0\nCeiT-T [92] 6.4 2.4 224 76.4 93.4 83.6\nConViT-Ti [15] 6.0 2.0 224 73.1 - -\nCrossViT-Ti [6] 6.9 3.2 224 73.4 - -\nViTAE-6M 6.5 2.0 224 77.9 94.1 84.9\nPVT-T [80] 13.2 3.8 224 75.1 - -\nLocalViT-PVT [43] 13.5 9.6 224 78.2 94.2 -\nConViT-Ti+ [15] 10.0 4.0 224 76.7 - -\nPiT-XS [29] 10.6 2.8 224 78.1 - -\nConT-M [89] 19.2 6.2 224 80.2 - -\nViTAE-13M 13.2 3.4 224 81.0 95.4 86.8\nDeiT-S [76] 22.1 9.8 224 79.9 95.0 85.7\nDeiT-S\n⚗ [76] 22.1 9.8 224 81.2 95.4 86.8\nPVT-S [80] 24.5 7.6 224 79.8 -\nConformer-Ti [58] 23.5 5.2 224 81.3 - -\nSwin-T [47] 29.0 9.0 224 81.3 - -\nCeiT-S [92] 24.2 9.0 224 82.0 95.9 87.3\nCvT-13 [83] 20.0 9.0 224 81.6 - 86.7\nConViT-S [15] 27.0 10.8 224 81.3 - -\nCrossViT-S [6] 26.7 11.2 224 81.0 - -\nPiT-S [29] 23.5 4.8 224 80.9 - -\nTNT-S [23] 23.8 10.4 224 81.3 95.6 -\nTwins-PCPVT-S[10] 24.1 7.4 224 81.2 - -\nTwins-SVT-S [10] 24.0 5.6 224 81.7 - -\nT2T-ViT-14 [93] 21.5 5.2 224 81.5 95.7 86.8\nViTAE-S 23.6 5.6 224 82.0 95.9 87.0\nViTAE-S-Stage 19.2 6.0 224 82.2 96.0 87.4\nViT-B/16 [19] 86.5 18.7 384 77.9 - -\nViT-L/16 [19] 304.3 65.8 384 76.5 - -\nDeiT-B [76] 86.6 34.6 224 81.8 95.6 86.7\nPVT-M [80] 44.2 13.2 224 81.2 - -\nPVT-L [80] 61.4 19.6 224 81.7 - -\nConformer-S [58] 37.7 10.6 224 83.4 - -\nSwin-S [47] 50.0 17.4 224 83.0\nConT-B [89] 39.6 12.8 224 81.8 - -\nCvT-21 [83] 32.0 14.2 224 82.5 - 87.2\nConViT-S+ [15] 48.0 20.0 224 82.2 - -\nConViT-B [15] 86.0 34.0 224 82.4 - -\nConViT-B+ [15] 152.0 60.0 224 82.5 - -\nPiT-B [29] 73.8 25.0 224 82.0 - -\nTNT-B [23] 65.6 28.2 224 82.8 96.3 -\nT2T-ViT-19 [93] 39.2 8.9 224 81.9 95.7 86.9\nViTAE-B-Stage 48.5 13.8 224 83.6 96.4 87.9\n16\nTable 7: Model details of ViTAE variants.\nModel\nReduction Cell Normal Cell NC Params Macs\ndilation cells heads embed cells Arrangement (M) (G)\nViTAE-T [1, 2, 3, 4] ↓ 3 4 256 7 0, 0, 7 4.8 1.5\nViTAE-6M [1, 2, 3, 4] ↓ 3 4 256 10 0, 0, 10 6.5 2.0\nViTAE-13M [1, 2, 3, 4] ↓ 3 4 320 11 0, 0, 11 13.2 3.4\nViTAE-S [1, 2, 3, 4] ↓ 3 6 384 14 0, 0, 14 23.6 5.6\nViTAE-T-Stage [1, 2, 3, 4] ↓ 3 4 256 7 1, 1, 5 4.8 2.3\nViTAE-S-Stage [1, 2, 3, 4] ↓ 4 8 512 14 2, 2, 8, 2 19.2 6.0\nViTAE-B-Stage [1, 2, 3, 4] ↓ 4 8 768 17 2, 2, 11, 2 48.5 13.8\nTable 8: ViTAE on downstream tasks.\nDetection-COCO\nBackbone Method Lr Schd box mAP mask mAP params (M)\nResNet-50 [26] Mask RCNN [24] 1x 38.2 34.7 44\nSwin-T [47] Mask RCNN [24] 1x 43.7 39.8 48\nViTAE-S-Stage Mask RCNN [24] 1x 44.6 40.2 37\nResNet-50 [26] Cascade RCNN [5] 1x 41.2 35.9 82\nSwin-T [47] Cascade RCNN [5] 1x 48.1 41.7 86\nViTAE-S-Stage Cascade RCNN [5] 1x 48.9 42.0 75\nSegmentation-ADE20K\nBackbone Method Lr Schd mIoU mIoU(ms+ﬂip) params (M)\nSwin-T [47] UPerNet [85] 160K 44.5 45.8 60\nViTAE-S-Stage UPerNet [85] 160K 45.4 47.8 49\nPose-COCO\nBackbone Method InputSize mAP mAR params (M)\nResNet-50 [26] SimpleBaseline [84] 256x192 71.8 77.3 34\nViTAE-S-Stage SimpleBaseline [84] 256x192 73.7 79.0 27\nVOS-Davis2017\nBackbone Method J F J&F params (M)\nResNet-50 [26] STM [54] 79.2 84.3 81.8 39\nViTAE-T-Stage STM [54] 79.4 85.5 82.5 19\nVOS-Davis2016\nBackbone Method J F J&F params (M)\nResNet-50 [26] STM [54] 88.7 89.9 89.3 39\nViTAE-T-Stage STM [54] 89.2 90.4 89.8 19\nDavis 2017 dataset. It can be concluded that the intrinsic IB introduced by the RC and NC module\nindeed improves the generalization ability of backbone networks for various downstream tasks.\nA.7 More comparisons of data efﬁciency and training efﬁciency.\n64.1\n68.1\n68.7\n67.5\n71.6\n72.6\n59.7\n64.4 64.5\n59\n61\n63\n65\n67\n69\n71\n73ImageNet Top-1 Accuracy (%)\nData Percentage (%)\nT2T\nViTAE\nDeiT\n20 60 100\n68.7 \n70.8 \n71.7 \n72.6 \n74.2 \n75.3 \n64.5 \n70.0 \n72.2 \n64.0\n66.0\n68.0\n70.0\n72.0\n74.0ImageNet Top-1 Accuracy (%)\nEpochs\n100 200 300\nFigure 5: Comparisons of DeiT, T2T-ViT and\nViTAE in terms of data efﬁciency and training\nefﬁciency on ImageNet.\nBesides T2T-ViT [93] for the evaluation of the data\nefﬁciency and training efﬁciency, we further train\nDeiT [76] with 20%, 60% and 100% data for 100\nepochs and train it with 100% data for 100, 200, and\n300 epochs. Its results can be viewed in Figure 5. It\ncan be observed that, with inductive bias introduced,\nT2T-ViT achieves better performance with less data\nwhen compared with DeiT. Without loss of general-\nity, T2T-ViT outperforms DeiT with fewer training\nepochs, e.g., T2T-ViT with 20% data can perform\ncomparably to DeiT with 100% data. With more\nintrinsic inductive bias introduced, ViTAE outper-\nforms T2T-ViT with fewer data and fewer epochs.\nSuch observation conﬁrms that with proper intrinsic\ninductive bias, the training of transformer models\ncan be both data efﬁciency and training efﬁciency.\nA.8 Analysis of position embedding Table 9: ViTAE with different PE.\nSinusoid No Learnable\nTop-1 75.3 75.3 75.1As CNN can encode position information with\npadding [11], we further disable the position em-\n17\n(b) (d)\nDifferent Dilation Rates\nMulti-Head\nSelf-Attention\nLayer Norm\nC\nConv\nBN\nSiLU\nImg2Seq\n× 2\nMulti-Head\nSelf-Attention\nConv\nBN\nSiLU\nLayer Norm\nSeq2Img\nImg2Seq\nConv\nSiLU\nConv\nSiLU\nImg2Seq\n× 3\nMulti-Head\nSelf-Attention\nConv\nBN\nSiLU\nLayer Norm\nSeq2Img\nImg2Seq\nMulti-Head\nSelf-Attention\nLayer Norm\nSeq2Img\nImg2Seq\nGeLU\nC\nDifferent Dilation Rates\nSiLU\n× 2\n(a) (e)\n× 2\nMulti-Head\nSelf-Attention\nConv\nBN\nGeLU\nLayer Norm\nSeq2Img\nImg2Seq\nConv\nGeLU\n(c)\nFigure 6: Different structures used in the ablation study. (a) Origin ViTAE structure. (b) Adding\nanother BN in the PCM. (c) Replacing SiLU with GeLU in the PCM. (d) Replacing PCM with PRM.\n(e) Replacing GeLU with SiLU in the PRM.\nbedding and train the ViTAE model without position embedding for 300 epochs. The results are\nsummarized in Table 9. It can be seen that removing position embedding (PE) in the ViTAE model\ndoes not downgrade its performance. Such phenomena show that the PCM and PRM modules utilized\nin the ViTAE can aid the model in making sense of location information.\nA.9 More Ablation Studies\nTable 10: More ablation studies. (a), (b), (c), (d),\n(e) correspond to different structures in Figure 6.\n(a) (b) (c) (d) (e)\nTop-1 72.6 69.6 72.3 71.9 72.4\nTo further analyze the structure of our ViTAE\nmodel, we conduct more ablation studies related\nto the proposed reduction cells and normal cells.\nAs shown in Table 10 and Figure 6, we ﬁrst add\nanother batch normalization [34] in the PCM mod-\nule to make the PCM module has three exactly same convolution layers. However, such a structure\ndowngrades the performance by 3%. As there is layer normalization [ 2] before the input to the\nFFN module, the combination of batch normalization and layer normalization may conﬂict with\neach other, resulting in performance degradation. Another design choice we tried is replacing the\nPCM with PRM modules (Figure 6 (d)) to introduce both scale-invariance and locality in the parallel\nbranch. However, such a design shows a small drop in the performance, indicating that not only\nintroducing inductive bias is important in transformers, but also the way in which these inductive\nbiases are introduced also matters. What’s more, we test the activation function used in the PCM and\nPRM modules. By default, we adopt SiLU in PCM, following the design choice in pioneering CNN\nnetworks and GeLu in PRM following previous transformers’ design. By replacing the SiLU with\nGeLU (Figure 6 (c)), the performance drops a little. Similar phenomena can be observed when all\nPCM and PRM modules adopt SiLU as activation functions (Figure 6 (e)).\nA.10 More visual results.\nWe also provide more visual inspection results of ViTAE using Grad-CAM [66] in Figure 7 8 9 10 and\ncompare it with T2T-ViT [93] in Figure 11. It can be seen that our ViTAE can cover the targets more\nprecisely and compress the noise introduced by the complex background. Such phenomena conﬁrm\nthat with the introduced inductive bias, the ViTAE model can better adapt to targets in different\nsituations and thus achieves better performance on the vision tasks.\n18\nInput ViTAE Input ViTAE Input ViTAE\nFigure 7: More visual results of ViTAE.\n19\nInput ViTAE Input ViTAE Input ViTAE\nFigure 8: More visual results of ViTAE.\n20\nInput ViTAE Input ViTAE Input ViTAE\nFigure 9: More visual results of ViTAE.\n21\nInput ViTAE Input ViTAE Input ViTAE\nFigure 10: More visual results of ViTAE.\n22\nInput T2T-ViT ViTAE Input T2T-ViT ViTAE\nFigure 11: Visual comparison between ViTAE and T2T-ViT.\n23",
  "topic": "Locality",
  "concepts": [
    {
      "name": "Locality",
      "score": 0.740481972694397
    },
    {
      "name": "Transformer",
      "score": 0.7171396017074585
    },
    {
      "name": "Computer science",
      "score": 0.7029039859771729
    },
    {
      "name": "Dilation (metric space)",
      "score": 0.5565944910049438
    },
    {
      "name": "Inductive bias",
      "score": 0.5522518157958984
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5297209620475769
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3770979046821594
    },
    {
      "name": "Computer vision",
      "score": 0.3490654230117798
    },
    {
      "name": "Voltage",
      "score": 0.2237280011177063
    },
    {
      "name": "Electrical engineering",
      "score": 0.12208840250968933
    },
    {
      "name": "Multi-task learning",
      "score": 0.12190690636634827
    },
    {
      "name": "Mathematics",
      "score": 0.11425891518592834
    },
    {
      "name": "Engineering",
      "score": 0.09295392036437988
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}