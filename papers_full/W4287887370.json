{
  "title": "MT-Speech at SemEval-2022 Task 10: Incorporating Data Augmentation and Auxiliary Task with Cross-Lingual Pretrained Language Model for Structured Sentiment Analysis",
  "url": "https://openalex.org/W4287887370",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2100462851",
      "name": "Cong Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097086956",
      "name": "Jiansong Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2272073290",
      "name": "Cao Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1970297279",
      "name": "Fan Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3000025158",
      "name": "Wan Guanglu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284904311",
      "name": "Jinxiong Xia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3014501363",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287854669",
    "https://openalex.org/W2251900677",
    "https://openalex.org/W3175567752",
    "https://openalex.org/W3176138405",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W3103028291",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2901440799",
    "https://openalex.org/W4205184193",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2964310805",
    "https://openalex.org/W2014902591",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2301095666",
    "https://openalex.org/W2251648804",
    "https://openalex.org/W1495997854",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3215788354",
    "https://openalex.org/W2991376818",
    "https://openalex.org/W3105083537",
    "https://openalex.org/W4302068351",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4287822362",
    "https://openalex.org/W2155120841",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W4385414156",
    "https://openalex.org/W3037156693",
    "https://openalex.org/W2914120296"
  ],
  "abstract": "Sentiment analysis is a fundamental task, and structure sentiment analysis (SSA) is an important component of sentiment analysis. However, traditional SSA is suffering from some important issues: (1) lack of interactive knowledge of different languages; (2) small amount of annotation data or even no annotation data. To address the above problems, we incorporate data augment and auxiliary tasks within a cross-lingual pretrained language model into SSA. Specifically, we employ XLM-Roberta to enhance mutually interactive information when parallel data is available in the pretraining stage. Furthermore, we leverage two data augment strategies and auxiliary tasks to improve the performance on few-label data and zero-shot cross-lingual settings. Experiments demonstrate the effectiveness of our models. Our models rank first on the cross-lingual sub-task and rank second on the monolingual sub-task of SemEval-2022 task 10.",
  "full_text": "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 1329 - 1335\nJuly 14-15, 2022 ©2022 Association for Computational Linguistics\nMT-Speech at SemEval-2022 Task 10: Incorporating Data Augmentation\nand Auxiliary Task with Cross-Lingual Pretrained Language Model for\nStructured Sentiment Analysis\nCong Chen, Jiansong Chen, Cao Liu, Fan Yang, Guanglu Wan, Jinxiong Xia,\nMeituan, Beijing, China\n{chencong29, chenjiansong, liucao}@meituan.com\nAbstract\nStructured Sentiment Analysis (SSA) is an\nimportant component of sentiment analysis,\nwhich is a critical task in NLP. Traditional SSA\nmethods cannot capture the cross-lingual inter-\nactions between different language and there\nis insufficient annotated data, especially in the\ncross-lingual settings. In this paper, we use the\npre-trained language models with two auxiliary\ntasks and adopt data augmentation to address\nthe above problems. Specifically, we employ\nXLM-RoBERTa to capture the cross-lingual\nknowledge interactions and enhance the gener-\nalization in multilingual/cross-lingual settings.\nFurthermore, we leverage two data augmenta-\ntion techniques and propose two auxiliary tasks\nto improve the performance on the few-shot and\nzero-shot settings. Experiments demonstrate\nthat our model ranks first on the cross-lingual\nsub-task and second on the monolingual sub-\ntask of SemEval-2022 task 10.\n1 Introduction\nStructured Sentiment Analysis (SSA) is an impor-\ntant task in sentiment analysis (Barnes et al., 2021;\nLiu, 2012; Mitchell et al., 2013). The goal of SSA\nis to extract all opinion tuples from given texts.\nThe opinion tuple (h, t, e, p) consists of a holder\n(h) which expresses a polarity (p) towards a target\n(t) by a textual sentiment expression (e). Benefiting\na variety of business applications, such as human-\nmachine dialogue and recommendation systems,\nSSA has attracted much more attention from both\nacademia and industry (Pang et al., 2008; Mitchell\net al., 2013; Xu et al., 2020; Ovrelid et al., 2020;\nLi et al., 2019).\nThe mainstream method for SSA is to adopt a\npipeline approach by separately performing the\nsubtasks including holder extraction and target ex-\ntraction. However, such methods can not capture\ndependencies of multiple sub-tasks. To address\nthis problem, Barnes et al. (2021) leverages graph-\nbased dependency parsing to capture the depen-\ndencies among opinion tuples, where sentiment\nholders, targets and expressions are the nodes, and\nthe relations of them as the arcs. This model has\nobtained state-of-the-art performance on SSA.\nHowever, the aforementioned methods still suf-\nfer from some important issues. Firstly, the knowl-\nedge of the pre-trained language models (PLMs)\nhas not been fully exploited. In fact, the cross-\nlingual PLMs contain rich knowledge of the inter-\nactions among different languages. Secondly, the\nabove data-driven models rely on a large amount of\nannotation data, but there is insufficient or even no\nannotated data in the real scene. For example, in\nSemEval-2022 shared task 10 (Barnes et al., 2022),\nthe MultiBEU (Barnes et al., 2018) dataset has\nonly 1215 sentences and the MultiBCA (Barnes\net al., 2018) dataset have only 1341 sentences, and\nthere is no training data for the target language in\nthe cross-lingual setting, which heavily hinders the\nperformance on SSA.\nTo address the above problems, we propose a\nunified and end-to-end model for SSA, which per-\nforms data augmentation and adopts auxiliary tasks\nwith cross-lingual PLMs. Specifically, we employ\nXLM-RoBERTa (Conneau and Lample, 2019; Con-\nneau et al., 2019) as the backbone encoder to make\nuse of its multilingual/cross-lingual knowledge. To\nalleviate the problem of insufficiency or lack of\nannotated data, we adopt two data augmentation\nmethods: the one is to add in-domain annotated\ndata of the same task under the training stage, and\nthe other is to employ Masked Language Model\n(MLM) (Devlin et al., 2018) for generating sim-\nilar texts. Furthermore, in addition to predicting\neach tuple in the dependency parsing graph simul-\ntaneously, we add two auxiliary tasks: 1) sequence\nlabeling to predict the span of the holder / target\n/ expression, and 2) sentiment polarity classifica-\ntion. Note that both of them do not need additional\n1329\nMethods NoReCFine MultiBCA MultiBEU OpeNerEN OpeNerES MPQA DS Unis Average\nTop1 0.529(2) 0.728(1) 0.739(1) 0.760(2) 0.722(4) 0.447(1) 0.494(1) 0.631(1)\nTop2(Ours) 0.524(3) 0.728(1) 0.739(1) 0.763(1) 0.742(1) 0.416(2) 0.485(2) 0.628(2)\nTop3 0.533(1) 0.709(3) 0.715(3) 0.756(3) 0.732(3) 0.402(3) 0.463(3) 0.616(3)\nTop4 0.504(4) 0.681(6) 0.723(2) 0.747(4) 0.735(2) 0.375(5) 0.410(9) 0.596(4)\nTop5 0.483(8) 0.711(2) 0.681(6) 0.727(5) 0.686(7) 0.379(4) 0.373(13) 0.577(5)\nTable 1: Comparisons on monolingual evaluation leader board.\nMethods OpeNerES MultiBCA MultiBEU Average\nTop1(Ours) 0.644(1) 0.643(1) 0.632(1) 0.640(1)\nTop2 0.618(3) 0.562(7) 0.584(2) 0.588(2)\nTop3 0.628(2) 0.607(3) 0.527(4) 0.587(3)\nTop4 0.604(5) 0.596(4) 0.512(7) 0.571(4)\nTop5 0.589(6) 0.593(5) 0.516(6) 0.566(5)\nTable 2: Comparisons on cross-lingual evaluation leader board.\nannotations.\nWe conduct experiments on subtask 1 and sub-\ntask 2 of SemEval-2022 shared task on SSA. Ex-\nperimental results demonstrate that our method\noutperforms strong baselines. We rank first on\nthe cross-lingual sub-task and rank second on the\nmonolingual subtask in SemEval-2022 task 101.\nTo summarize, our contributions are as follows.\n• We leverage cross-lingual pre-trained lan-\nguage models to capture the interactive infor-\nmation knowledge among different languages.\n• We combine existing in-domain training data\nand produce new training data by MLM to\nalleviate the problem of insufficiency or lack\nof annotated data.\n• We propose two auxiliary tasks that do not\nrequire additional annotations to further im-\nprove the performance.\n• Experimental results demonstrate the effec-\ntiveness of our proposed model, and we rank\nfirst on the subtask 2 and rank second on the\nsubtask 1 on SemEval-2022 task 10.\n2 Method\nWe incorporate the dependency graph parsing\napproach (Barnes et al., 2021) into our model.\nThe general architecture is a pre-trained language\nmodel (e.g BERT (Devlin et al., 2018), RoBERTa\n(Liu et al., 2019), etc..) followed by a three-layers\nBiLSTMs (Schuster and Paliwal, 1997; Cross and\n1https://competitions.codalab.org/\ncompetitions/33556\nHuang, 2016) and the bilinear (Kiperwasser and\nGoldberg, 2016; Dozat and Manning, 2016) at-\ntention as the decoding component. Hence, we\ncan take advantage of the knowledge of large-\nscale PLMs (Vaswani et al., 2017; Radford et al.,\n2019; Raffel et al., 2020; Yang et al., 2019; Wolf\net al., 2019) and deep semantic dependency pars-\ning (Dozat and Manning, 2016; Oepen et al., 2020;\nKurtz et al., 2020).\n2.1 Encoder\nWe consider several state-of-the-art models as the\ncandidates of our model’s backbone, such as Mul-\ntilingual BERT (mBERT) (Devlin et al., 2018),\nXLM-RoBERTa (Conneau et al., 2019), and info-\nXLM(Chi et al., 2021). Particularly, we choose the\nXLM-RoBERTa backbone as the baseline. Because\nsubtask 1 is a multilingual problem and subtask 2\nis a cross-lingual zero-shot problem. They both\nbenefit from the Translation Language Modeling\n(TLM) objective in XLM-RoBERTa. The TLM\nand Masked Language Modeling (MLM) objec-\ntives in the XLM-family models perform better\nthan mBERT, which is simply trained on multi-\nlingual corpus with the MLM objective. Addi-\ntionally, XLM-RoBERTa is trained on more data,\nwhich makes it more robust. Another reason we\nchoose XLM-RoBERTa is that it is a large open-\nsource model for downstream applications. We\ndid not employ info-XLM as it is trained with the\nsentence-level classification objective, which is not\nsuited for this task.\n1330\nMethods MPQA DS Unis OpeNerEN OpeNerES MultiBCA MultiBEU NoReCFine Average\nw2v + BiLSTM 0.103 0.166 0.525 0.526 0.524 0.539 0.320 0.386\nmBERT 0.231 0.280 0.571 0.611 0.526 0.517 0.373 0.446\nmBERT +BiLSTM 0.266 0.285 0.621 0.614 0.619 0.589 0.386 0.483\nXLM-R +BiLSTM 0.332 0.357 0.705 0.654 0.669 0.650 0.481 0.550\nTable 3: Monolingual task performances with different encoders. All models use the same bilinear attention decoder.\nAll results are evaluated on the official released development set.\n2.2 Data Augmentation\nWe provide two data augmentation methods to fur-\nther boost the performance of our model. First is\nthe in-domain data enhancement (DA1) to better\nmake use of the data in different languages. The\nsecond is the MLM data augmentation (DA2).\n2.2.1 In-domain Data Enhancement\nWe combined different dataset that belong to\nthe same domain in the training phase, to help\nimprove generalization. Noticed that the four\ndatasets MultiBEU , MultiBCA, OpenerES , and\nOpenerEN (Agerri et al., 2013) are all from the\nhotel review corpus. We observe these datasets\nshare some common features even though they are\nof different languages. These languages share the\nsame or similar words for the same objects or con-\ncepts. For example, the \"hotel\" word in Catalan\nand Spanish are also \"hotel\", and in Basque it is a\nsimilar word \"hotela\". Besides, the people who use\nthese languages share the same sentiment polarity\ntendency on the hotel review domain. Combin-\ning the four language datasets together as a whole\ntraining set will improve the overall performance.\nWe additionally add the Portuguese hotel review\ndataset (BOTE-rehol)(Barros and Bona, 2021) and\nthe English laptops review dataset (RES14) (Pon-\ntiki et al., 2014; Xu et al., 2020) for extra training,\nwhich needs to be converted to the same format as\nthis task.\n2.2.2 Data Augmentation by Masked Word\nGeneration\nThe Masked Language Model corrupts the input\ntexts by randomly replacing the tokens with the\n[MASK] tokens, and predicts the original token\nat the [MASK] positions. For each sample with\nvalid opinion tuples, by randomly masking a small\nportion of the tokens in the text, we obtain a new\nsample whose meaning is similar as the original\nwith the same labels. Note that in this task we do\nnot mask the sentiment expression words as the\nPLMs may generate words of different polarities\nwhich are inconsistent with the original labels.\n2.3 Auxiliary Tasks\nSSA consists of structure prediction and sentiment\npolarity classification, and to handle these two tasks\nin an end-to-end manner is non-trivial. We propose\ntwo auxiliary tasks to provide more training signals\nto the model to better handle structure prediction\nand polarity classification. For structure predic-\ntion, we add a sequence labeling task to explic-\nitly predict the type (target, holder, or expression)\nof each token. For polarity classification, we add\nmore sentiment polarity classification data as ex-\ntra tasks. Specifically, we use the average pooling\nof the model’s BiLSTM hidden-states as sentence-\nlevel representations. The representation is fed to\na multilayer perceptron(MLP) for sentence-level\nsentiment polarity classification. The total loss is\nthe weighted sum of the main loss and the auxiliary\nlosses:\nL = Lp + (Ls + Lc)/2 (1)\nwhere Lp is the primary loss of the SSA task. Ls\nand Lc are the losses for sequence-labeling and\nclassification task, respectively.\n3 Experiments\n3.1 Experimental data\nWe use the officially released development set as\nthe test set, and randomly split the original training\nset into the training and development sets. We keep\nthe size of the split development sets the same as\nthe official released development set.\nWe transform the BOTE-rehol and RES14\ndatasets into the graph format and leave the opinion\ntuples’ holders empty since the two datasets do not\ncontain holder labels.\nFor each dataset, we convert the sentiment graph\nlabels to sequence labeling labels, which will be\nadded as an auxiliary task during training. Addi-\ntionally, for theMultiBCA and OpenerES datasets,\nwe make use of the Catalonia Independence Cor-\npus (CIC) (Zotova et al., 2020) as the extra training\n1331\nMethods MultiBCA MultiBEU OpeNerES OpeNerEN\nbaseline 0.685 0.650 0.654 0.712\nw / DA1 0.727 0.670 0.711 0.729\nTable 4: Monolingual performances of data augment (DA) on official released development set.\nMethods MultiBCA MultiBEU OpeNerES\nOpeNerEN 0.574 0.438 0.630\nw / DA1 0.600 0.550 0.620\nw / DA1-2 0.623 0.567 0.657\nTable 5: Cross-lingual performances of data augment\n(DA) on official released development set.\ndata of the polarity classification task.\n3.2 Implementation details\nWe adopt the head-first setting (Barnes et al., 2021)\nwhich sets the first token within each span as the\nhead of the span with all other tokens within that\nspan as dependents. The root node is represented\nby the first token within the sentiment expression.\nFor any word that is tokenized into a head-token\nfollowed by several sub-tokens, we set the head-\ntoken as the head and its following sub-tokens as\nthe dependents. We use holder, targ, exp-Positive,\nexp-Negative, exp-Neutral, Noneas labels to denote\ndifferent node types. The relation between each\nnode is expressed by the attention value between\nthe head-tokens of the nodes.\nWe try different combinations to get the best re-\nsults for different subtasks. With XLM-RoBERTa-\nlarge as the backbone, details combinations are\nlisted in Table 6 for the four hotel review\ndatasets (MultiBEU , MultiBCA , OpenerES and\nOpenerEN ). For DSUnis dataset (Toprak et al.,\n2010), we chose English RES14 dataset which also\nhas few holders elements as its in-domain dataset.\nAs most of the sentences in the two dataset are\nexpressed without holders elements.\nWhen generating new samples via MLM, for\neach sentence with at least one valid sentiment tu-\nple, we mask one position i at a time and feed\nthe masked sentence to the XLM-RoBERTa-large\nmodel. The PLM generates a word based on the\nhighest probability pi. We pick the top 5 most con-\nfident samples ranked by the PLM’s output prob-\nability Pi for i ∈ n, where n denotes the possible\nmasked positions. And set a threshold p as 0.85\nto filter out any samples with a probability lower\nthan the threshold. Repeated samples are not con-\nsidered valid. The generated samples are treated as\nsupplementary data to the original dataset.\nFor domain adaptation, we further pre-trained\nXLM-RoBERTa-large with all the data from the\nreleased datasets via Mask Language Modeling\n(MLM)(Devlin et al., 2018). We pick the best\ncheckpoint according to the lowest perplexity on\nthe development set.\n3.3 Overall Comparisons\nComparison Settings. Firstly, we compare our\nmodel with other participant teams on the leader\nboard of the structured sentiment competition. Ta-\nble 1 and Table 2 record the comparison results\nof the monolingual and cross-lingual evaluation,\nrespectively.\nComparison Results.(1) Our methods rank sec-\nond and first on the monolingual and cross-lingual\nevaluation, respectively, which demonstrates the ef-\nfectiveness of our proposed model. (2) Our model\nremarkably outperforms the top2 team in the cross-\nlingual subtask, which indicates our model has bet-\nter generalization on the zero-shot cross-lingual\nsettings.\n3.4 Effectiveness of Cross-lingual Pre-trained\nLanguage Model\nComparison Settings.To prove the effectiveness\nof XLM-RoBERTa2, a cross-lingual pre-trained\nlanguage model, we compare it with the follow-\ning baselines: 1) w2v + BiLSTM, BiLSTMs with\nword2vec (Mikolov et al., 2013) word embeddings;\n2) mBERT, the Multilingual BERT (Devlin et al.,\n2018); 3) mBERT + BiLSTM; 4) XLM-RoBERTa\n+ BiLSTM.\nComparison Results.(1) Table 3 demonstrates\nthat XLM-RoBERTa + BiLSTM obtains the best\nperformance among all of the benchmarks, and the\naverage score outperforms the strongest baseline\n(mBERT + BiLSTM) by 6.7%. It proves that our\nmodel has great generalization ability. (2) BiLSTM\ncan improve the performance by 3.7%, which in-\ndicates the BiLSTM layer can capture sequence\ninformation, which is beneficial to sequence encod-\ning (Cross and Huang, 2016).\n2We leverage the large version of XLM-RoBERTa to im-\nprove performances.\n1332\nMethods MultiBCA MultiBEU OpeNerES\nData combination OpeNerES\nMultiBEU\nOpeNerEN\nMultiBCA\nOpeNerES\nbote-rehol\nOpeNerEN\nMultiBCA\nTable 6: In domain data combination for cross-lingual evaluation. No target language data is participated in training\nand development.\nMethods MPQA DS Unis OpeNerEN OpeNerES MultiBCA MultiBEU NoReCFine\nbaseline 0.296 0.337 0.648 0.641 0.662 0.647 0.400\nw / Auxiliary-task 0.305 0.346 0.674 0.660 0.687 0.657 0.411\nTable 7: Performances on the official released development set with auxiliary tasks. We use RoBERTa-base (Liu\net al., 2019) for MPQA (Wiebe et al., 2005), DSUnis and OpeNerEN , bert-base-spanish-wwm-cased (Cañete et al.,\n2020) for OpeNerES , RoBERTa-base-ca (Armengol-Estapé et al., 2021) for MultiBCA, berteus-base-cased (Agerri\net al., 2020) for MultiBEU , and norwegian-RoBERTa-basehttps://huggingface.co/patrickvonplaten/\nnorwegian-roberta-base for NoReCFine (Øvrelid et al., 2020).\n3.5 Effectiveness of Data Augmentation\nComparison Settings. In order to demonstrate\nthe effectiveness of data augmentation, we utilize\nexisting training data for data augmentation (DA1)\nincluding MultiBEU MultiBCA, OpeNerES and\nOpeNerEN . Furthermore, we leverage MLM to\ngenerate new training data for data augmentation\n(DA2). We record the performance in Table 4 and\nTable 5, where \"w/\" means \"with\", and \"DA1-2\"\nmeans \"DA1 combined with DA2\".\nComparison Results.We can conclude the fol-\nlowing from Table 4 and Table 5: both DA1 and\nDA2 contribute to performance improvement, with\nperformance increases on almost every benchmark.\nSpecifically, the performance has remarkably im-\nproved in the cross-lingual settings, and data aug-\nmentation is more helpful on the few-shot and zero-\nshot settings.\n3.6 The Effectiveness of Auxiliary Tasks\nAs shown in Table 7, we leverage the auxiliary\ntasks including sequence labeling and sentiment\npolarity classification to improve the performances.\nWe can observe that the auxiliary tasks improve\nperformances on all of the datasets, which demon-\nstrate the effectiveness of the two auxiliary tasks.\n4 Conclusion\nThis paper studies the task of structured sentiment\nanalysis. In order to deal with the problems of\npoor interactions of different languages and lack\nof annotation data, we adopt the cross-lingual pre-\ntrained language model and adopt data augmen-\ntation and auxiliary tasks. Specifically, we em-\nploy XLM to capture the interactive information in\nthe pre-training stage. Furthermore, we leverage\ntwo data augmentation strategies and two auxil-\niary tasks to improve the performance for lack of\ntraining data. Experiments demonstrate the effec-\ntiveness of our models. Our models rank first on\nthe cross-lingual sub-task and rank second on the\nmonolingual sub-task of SemEval-2022 task 10.\nReferences\nRodrigo Agerri, Montse Cuadros, Sean Gaines, and\nGerman Rigau. 2013. OpeNER: Open polarity en-\nhanced named entity recognition. In Sociedad Es-\npañola para el Procesamiento del Lenguaje Natural,\nvolume 51, pages 215–218.\nRodrigo Agerri, Iñaki San Vicente, Jon Ander Cam-\npos, Ander Barrena, Xabier Saralegi, Aitor Soroa,\nand Eneko Agirre. 2020. Give your text represen-\ntation models some love: the case for basque. In\nProceedings of the 12th International Conference on\nLanguage Resources and Evaluation.\nJordi Armengol-Estapé, Casimiro Pio Carrino, Carlos\nRodriguez-Penagos, Ona de Gibert Bonet, Carme\nArmentano-Oller, Aitor Gonzalez-Agirre, Maite\nMelero, and Marta Villegas. 2021. Are multilin-\ngual models the best choice for moderately under-\nresourced languages? A comprehensive assessment\nfor Catalan. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, pages\n4933–4946, Online. Association for Computational\nLinguistics.\nJeremy Barnes, Toni Badia, and Patrik Lambert. 2018.\nMultiBooked: A corpus of Basque and Catalan hotel\nreviews annotated for aspect-level sentiment classifi-\ncation. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n1333\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nJeremy Barnes, Robin Kurtz, Stephan Oepen, Lilja\nØvrelid, and Erik Velldal. 2021. Structured sentiment\nanalysis as dependency graph parsing. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3387–3402, Online.\nAssociation for Computational Linguistics.\nJeremy Barnes, Oberländer Laura Ana Maria Kutuzov,\nAndrey and, Enrica Troiano, Jan Buchmann, Ro-\ndrigo Agerri, Lilja Øvrelid, Erik Velldal, and Stephan\nOepen. 2022. SemEval-2022 task 10: Structured sen-\ntiment analysis. In Proceedings of the 16th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2022), Seattle. Association for Computational Lin-\nguistics.\nJosé Meléndez Barros and Glauber De Bona. 2021. A\ndeep learning approach for aspect sentiment triplet\nextraction in portuguese. In Brazilian Conference on\nIntelligent Systems, pages 343–358. Springer.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge Pérez. 2020. Span-\nish pre-trained bert model and evaluation data. In\nPML4DC at ICLR 2020.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and M. Zhou. 2021. Infoxlm: An\ninformation-theoretic framework for cross-lingual\nlanguage model pre-training. In NAACL.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. Advances in\nneural information processing systems, 32.\nJames Cross and Liang Huang. 2016. Incremental pars-\ning with minimal features using bi-directional lstm.\nArXiv, abs/1606.06406.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nTimothy Dozat and Christopher D Manning. 2016.\nDeep biaffine attention for neural dependency pars-\ning. arXiv preprint arXiv:1611.01734.\nE. Kiperwasser and Yoav Goldberg. 2016. Simple and\naccurate dependency parsing using bidirectional lstm\nfeature representations. Transactions of the Associa-\ntion for Computational Linguistics, 4:313–327.\nRobin Kurtz, Stephan Oepen, and Marco Kuhlmann.\n2020. End-to-end negation resolution as graph pars-\ning. In IWPT.\nXin Li, Lidong Bing, Piji Li, and Wai Lam. 2019. A\nunified model for opinion target extraction and target\nsentiment prediction. ArXiv, abs/1811.05082.\nBing Liu. 2012. Sentiment analysis and opinion mining.\nSynthesis lectures on human language technologies,\n5(1):1–167.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTomas Mikolov, Kai Chen, Gregory S. Corrado, and\nJeffrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. In ICLR.\nMargaret Mitchell, Jacqui Aguilar, Theresa Wilson, and\nBenjamin Van Durme. 2013. Open domain targeted\nsentiment. In EMNLP.\nStephan Oepen, Omri Abend, Lasha Abzianidze, Johan\nBos, Jan Hajic, Daniel Hershcovich, Bin Li, Timo-\nthy J. O’Gorman, Nianwen Xue, and Daniel Zeman.\n2020. Mrp 2020: The second shared task on cross-\nframework and cross-lingual meaning representation\nparsing. In CONLL.\nLilja Ovrelid, Petter Maehlum, Jeremy Barnes, and Erik\nVelldal. 2020. A fine-grained sentiment dataset for\nnorwegian. In LREC.\nLilja Øvrelid, Petter Mæhlum, Jeremy Barnes, and Erik\nVelldal. 2020. A fine-grained sentiment dataset for\nNorwegian. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 5025–\n5033, Marseille, France. European Language Re-\nsources Association.\nBo Pang, Lillian Lee, et al. 2008. Opinion mining\nand sentiment analysis. Foundations and Trends® in\ninformation retrieval, 2(1–2):1–135.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos,\nHaris Papageorgiou, Ion Androutsopoulos, and\nSuresh Manandhar. 2014. Semeval-2014 task 4: As-\npect based sentiment analysis. In COLING 2014.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. ArXiv, abs/1910.10683.\nMike Schuster and Kuldip K Paliwal. 1997. Bidirec-\ntional recurrent neural networks. IEEE transactions\non Signal Processing, 45(11):2673–2681.\n1334\nCigdem Toprak, Niklas Jakob, and Iryna Gurevych.\n2010. Sentence and expression level annotation of\nopinions in user-generated discourse. In Proceedings\nof the 48th Annual Meeting of the Association for\nComputational Linguistics, pages 575–584, Uppsala,\nSweden. Association for Computational Linguistics.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. ArXiv, abs/1706.03762.\nJanyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.\nAnnotating expressions of opinions and emotions\nin language. Language Resources and Evaluation,\n39(2-3):165–210.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nLu Xu, Hao Li, Wei Lu, and Lidong Bing. 2020.\nPosition-aware tagging for aspect sentiment triplet\nextraction. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 2339–2349, Online. Association for\nComputational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS.\nElena Zotova, Rodrigo Agerri, Manuel Nunez, and Ger-\nman Rigau. 2020. Multilingual stance detection:\nThe catalonia independence corpus. arXiv preprint\narXiv:2004.00050.\n1335",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8903669714927673
    },
    {
      "name": "SemEval",
      "score": 0.8429255485534668
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.8145508766174316
    },
    {
      "name": "Natural language processing",
      "score": 0.6782020330429077
    },
    {
      "name": "Task (project management)",
      "score": 0.6643735766410828
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6090053915977478
    },
    {
      "name": "Annotation",
      "score": 0.6003466844558716
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5665397047996521
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5333739519119263
    },
    {
      "name": "Language model",
      "score": 0.47051161527633667
    },
    {
      "name": "Labeled data",
      "score": 0.4120883345603943
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": []
}