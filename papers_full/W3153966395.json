{
  "title": "Analyzing COVID-19 Tweets with Transformer-based Language Models",
  "url": "https://openalex.org/W3153966395",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4224696269",
      "name": "Feldman, Philip",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Tiwari, Sim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4305948043",
      "name": "Cheah, Charissa S. L.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224696271",
      "name": "Foulds, James R.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1542087761",
      "name": "Pan, Shimei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2035982357",
    "https://openalex.org/W1908686216",
    "https://openalex.org/W1981229576",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3047911967",
    "https://openalex.org/W2946119234",
    "https://openalex.org/W3090695844",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2964895772",
    "https://openalex.org/W3086249591"
  ],
  "abstract": "This paper describes a method for using Transformer-based Language Models (TLMs) to understand public opinion from social media posts. In this approach, we train a set of GPT models on several COVID-19 tweet corpora that reflect populations of users with distinctive views. We then use prompt-based queries to probe these models to reveal insights into the biases and opinions of the users. We demonstrate how this approach can be used to produce results which resemble polling the public on diverse social, political and public health issues. The results on the COVID-19 tweet data show that transformer language models are promising tools that can help us understand public opinions on social media at scale.",
  "full_text": "Analyzing COVID-19 Tweets with Transformer-based Language Models\nPhilip Feldman1, 2 Sim Tiwari2 Charissa S. L. Cheah2 James R. Foulds2 Shimei Pan2\n1 ASRC Federal, Beltsville, Maryland, USA\n2 University of Maryland Baltimore County, Baltimore, Maryland USA\n1philip.feldman@asrcfederal.com, 2{simt1, ccheah, jfoulds, shimei}@umbc.edu\nAbstract\nThis paper describes a method for using Transformer-based\nLanguage Models (TLMs) to understand public opinion from\nsocial media posts. In this approach, we train a set of GPT\nmodels on several COVID-19 tweet corpora that reﬂect pop-\nulations of users with distinctive views. We then use prompt-\nbased queries to probe these models to reveal insights into\nthe biases and opinions of the users. We demonstrate how\nthis approach can be used to produce results which resem-\nble polling the public on diverse social, political and public\nhealth issues. The results on the COVID-19 tweet data show\nthat transformer language models are promising tools that can\nhelp us understand public opinions on social media at scale.\nIntroduction\nLarge-scale research based on feedback from humans is\ndifﬁcult, and often relies on labor-intensive mechanisms\nsuch as polling, where statistically representative popula-\ntions will be surveyed using phone interviews, web surveys,\nand mixed-mode techniques. Often, for longitudinal stud-\nies, participants in a survey may need to be recontacted to\nupdate responses as a result of changing environments and\nevents (Fowler Jr 2013).\nAs social media has become ubiquitous, many attempts\nhave been made to determine public opinion by mining large\namount of social media data, often spanning years, which\nis available from online providers such as Twitter, Face-\nbook and Reddit, e.g. (Colleoni, Rozza, and Arvidsson 2014;\nSloan et al. 2015). Though social data can be mined in a va-\nriety of ways, answers to speciﬁc questions frequently can\nnot be obtained without expensive manual coding.\nThis may be ready to change with the emergence of large\ntransformer-based language models (TLMs) like the GPT\nseries (Radford et al. 2018) and BERT (Devlin et al. 2018).\nThese models are trained on massive text datasets such as\nBookCorpus, WebText and Wikipedia. They implement a\ntransformer-based deep neural network architecture which\nuses attention to allow the model to selectively focus on the\nsegments of the input text that are most useful in predicting\ntarget word tokens. A pre-trained GPT model can be used for\ngenerating texts as a function of the model and a sequence\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nof word tokens provided by users. We call the sequence of\nwords provided by users a “prompt” or a “probe”, which is\nspeciﬁcally designed to set up the theme/context for GPT to\ngenerate sentences. Since the model is not trained using any\nhand-crafted language rules, it effectively learns to generate\nnatural language by observing a large amount of text data. In\ndoing so, it captures semantic, syntactic, discourse and even\npragmatic regularities in language. GPT models were shown\nto generate text outputs often indistinguishable from that of\nhumans (Floridi and Chiriatti 2020).\nAs such, these models contain tremendous amounts of in-\nformation that can be used to answer questions about the\ncontent and knowledge encoded in the training text. Unfor-\ntunately, the knowledge captured in these TLMs is latent\n(e.g., millions of neural network model parameters), can be\ndifﬁcult to interpret. In this study, by using carefully con-\nstructed prompts/probes, we “poll” the model to gain access\nto the latent knowledge contained in the model in ways that\nare analogous to accessing the knowledge in a population by\nsurveying a random sample of individuals.\nWe can illustrate this process against a known ground\ntruth (the spatial relationships between countries) by polling\nthe GPT-31 repeatedly with the prompt “A short list of coun-\ntries that are nearest to , separated by commas:”. The\nprompt is initially seeded with a value such as “United\nStates”. Recursive responses create a graph, and a force-\ndirected layout approximately reconstructs the original\ncountry relationships2 (Figure 1).\nFigure 1: Central America reconstruction.\n1GPT-3 accessed using the OpenAI beta API\n2Full map at tinyurl.com/gptworldmap\narXiv:2104.10259v3  [cs.CL]  6 May 2021\nUsing this approach, we can create “maps” that approx-\nimate the local spatial relationships of ≈75% of current\ncountries. The presence or absence of a country is correlated\nwith population. This appears to be a bias in the model.\nIn this paper, we employ a similar approach that combines\na repeated-prompt based query method with data analysis\nand visualization to analyze the beliefs, biases and opinions\nof Twitter users in the context of the COVID-19 pandemic.\nWe describe our method for GPT model training and ﬁne\ntuning, determining probes, and analyzing and visualizing\nthe output from the model. Lastly, we discuss its limitations\nand implications for this approach to computational sociol-\nogy.\nRelated Work\nSince the introduction of the transformer model in 2017,\nTLMs have become a ﬁeld of study in themselves. The trans-\nformer uses self attention, where the model computes its\nown representation of its input and output (Vaswani et al.\n2017). So far, signiﬁcant research has been in increasing\nthe performance of these models, particularly as these sys-\ntems scale into the billions of parameters, e.g. (Radford\net al. 2019). Among them, BERT (Devlin et al. 2018) and\nGPT (Radford et al. 2018) are two of the most well known\nTLMs used widely in boosting the performance of diverse\nNLP applications.\nUnderstanding how and what kind of knowledge is stored\nin all those parameters is becoming a sub-ﬁeld in the\nstudy of TLMs. Among them, (Petroni et al. 2019) used\nprobes that present a query to the mode as a cloze state-\nment, where the model ﬁlls in a blank (e.g. “Twinkle\ntwinkle star”). Research is also being done on the\ncreation of effective prompts. Published results show that\nmining-based and paraphrasing approaches can increase ef-\nfectiveness in masked BERT prompts over manually created\nprompts (Jiang et al. 2020). In another study using GPT\nmodels ﬁne-tuned on descriptions of chess games, it was\nshown that models trained on a corpora of approximately\n23,000 chess games accurately replicated human gameplay\npatterns (Feldman 2020).\nUsing TLMs to evaluate social data is still nascent. A\nstudy by (Palakodety, KhudaBukhsh, and Carbonell 2020)\nused BERT ﬁne tuned on YouTube comments to gain insight\ninto community perception of the 2019 Indian election.\nLastly, we cannot ignore the potential weaponization of\nTLMs. OpenAI has shown that the GPT-3 can be “primed”\nusing “few-shot learning” (Brown et al. 2020). (McGufﬁe\nand Newhouse 2020) primed the GPT-3 using mass-shooter\nmanifestos with chilling results.\nDataset\nIn this study, we investigated the feasibility of em-\nploying GPT to analyze COVID-19 related tweets. Us-\ning the Twitter Search API, we collected tweets from\nthe USA if they included at least one of the identi-\nﬁed keywords/hashtags. As of this writing, the list of\ncase-insensitive keywords/hashtags include “coronavirus”,\n“covid19”, “sars-cov-2”, “pandemic”, “chinavirus”, “social\ndistancing”, “mask”, “ventilator”, “shelter in place” etc.\nTerms are reevaluated and updated monthly.\nSo far, we have retrieved a total of 18,703,707 tweets from\nNov. 2019 to the time of this writing. For this research, we\nconstructed three datasets to train three separate GPT mod-\nels based on three COVID-19 keywords: covid, sars-cov-2,\nand chinavirus. The most common term by Twitter users to\nrefer to the disease, was covid, and was thought to repre-\nsent the perspectives of the general public; sars-cov-2 was\nchosen for its greater use in science-related contexts; while\nchinavirus was chosen for its arguably racist connotations.\nTable 1 shows the size of each dataset. As one would ex-\npect from its more common usage, we collected approxi-\nmately 350 times more tweets associated with thecovid than\nthe other two tags. This would have some implications in\nthe behavior of the trained models, which we will describe\nin the ﬁne-tuning section. Please note the three datasets may\nbe overlapping (e.g., a tweet tagged with bothchinavirus and\ncovid). There are also many pandemic tweets not in any of\nthe datasets (e.g., those tagged with social distance.)\nTable 1: Tweet counts in each dataset\nchinavirus covid sars-cov-2\n14,950 7,015,582 25,768\nWe show three tweets from each dataset. These tweets\nwere posted at the beginning of April 2020, when the pan-\ndemic’s ﬁrst wave was starting to happen in the USA.\nchinavirus dataset: Just to remind everyone why so-\ncial healthcare is terrible...the UK has a 7% mortality\nrate of #ChinaVirus #WuhanVirus #coronavirus while\nAmerica is currently hovering around 1.7% of reported\ncases\ncovid dataset : This can’t be happening. These\nare our ﬁrst responders! If they aren’t protected\nour entire country is at risk. These doctors and\nnurses are on the front lines risking their life’s.\nWith a RAIN PANCHO?! #coronavirus #COVID\n#COVID2019 https://t.co/KWPVGuIgPW\nsars-cov-2 dataset: Currently, incubation period of\ncoronavirus, SARS-CoV-2, is considered to be 14 days.\nHealth authorities across countries are adopting a 14-\nday quarantine period, based @WHO guidelines.\nThese samples suggest qualitatively differing perspec-\ntives: the chinavirus appears reactionary and nationalistic;\nthe covid tweet is more emotional; while the sars-cov-2\ntweet is detailed and explicit.\nMethods\nIn this section, we describe how we customize a pre-trained\nGPT model with COVID-19 tweets and how we design\nprompts to setup the context and probe the models to reveal\nthe answers to our questions.\nModel Training and Fine Tuning\nFor this research, we employ the pre-trained GPT-2 model\nwith 12-layers, 768-hidden, 12-heads, 117M parameters\nhosted by Huggingface.com 3. We then ﬁne tune the pre-\ntrained GPT-2 model using the three COVID-19 tweet\ndatasets described above to produce three GPT models, one\nfor each dataset. We did not use the latest GPT-3 model as it\ndoes not allow us to retrain it with new data.\nTo demonstrate the behavior of these models, we use the\nbeginning of the same tweets that are shown in the previous\nsection as the prompts to generated three synthetic tweets. In\neach generated tweet, the prompt is shown in the brackets,\nand the generated content is shown in italics:\nChinavirus model:[Just to remind everyone why so-\ncial healthcare] doesn’t work. Stop being afraid. @re-\nalDonaldTrump @FLOTUS https://t.co/lFh9R1KdU\ncovid model: [This can’t be happening. These\nare our ﬁrst responders!] Please keep them in\nyour thoughts and prayers! #NursesCOVID19\nhttps://t.co/K8xnCfGfjC\nSars-cov-2 model: [Currently, incubation period of\ncoronavirus, SARS-CoV-2, is considered to be 14\ndays.] The duration of incubation is 12-14 days and the\nvirus can persist for up to 18 hours (depending on tem-\nperature and humidity).\nWe can see that each model has captured not only\nthe format of tweets (e.g., using mentions such as\n@realDonaldTrump, @FLOTUS or short URLs such as\nhttps://t.co/lFh9R1KdU) but also subjective and semantic\naspects of the original training data. As we can see, the\nmodels are not reconstructing the tweets. Rather, they are\nstochastically generating new text based on the prompts and\nlatent knowledge in the model. This property allows us to\nobtain responses to our queries which may reﬂect the beliefs\ntypical of the set of twitter users in the training data.\nPrompt design\nPrompts can be regarded as a form of conditioning or train-\ning, as discussed by (Brown et al. 2020). The length and\ncontent of the probe can be structured so that a desired re-\nsponse is more likely.\nFinding probes that resulted in useful responses was an in-\nteractive process, where we would develop ideas for probes\nand try them out on the models in small batches. For exam-\nple, the probe “Dr. Fauci is” allows the model to produce ad-\njectives (“likable”) adverbs (“very”), determiners (“a”), and\nverbs (“running”) as the next token. Changing the probe to\n“Dr. Fauci is a” constrains the next word to more likely to be\nan adjective or a noun. If we use the next nouns or adjectives\nafter the prompts as the responses to our inquiries, the probe\n“Dr. Fauci is a” may produce more direct answers.\nExample output from thecovid model is shown in Table 2.\nFor these relatively small models, we found that shorter\nprobes, typically 3 - 7 words would produce useful results.\n3huggingface.co/gpt2\nTable 2: Similar probes and different GPT outputs. We bold\nface the words if the ﬁrst nouns are extracted as the answers\nDr. Fauci is Dr. Fauci is a\nout of the spotlight hero in the war against COVID\nat it again dangerous man.\n100% correct medical genius\non top of everything liar . It was never about COVID19\nBy varying the prompts, we could tune the results to explore\nthe latent knowledge captured by GPT.\nFinally, we designed a set of prompts to probe the GPT\nmodel to reveal the Twitter public’s opinion/attitude toward\nvarious COVID-19 pandemic-related topics such as: whom\nto blame for the pandemic? How do people feel during the\npandemic? Is there any systematic bias towards certain de-\nmographic groups? For example, the prompt “During the\npandemic, [xxx] have been feeling” would be ﬁlled in with\nthe terms [Americans, Asians, Black folks] to compare the\nfeelings of different people during the pandemic.\nResponse/Answer Extraction\nThere are multiple ways to extract answers to different\nqueries from the GPT model. First, directly from the model.\nTransformer language models such as the GPT generate the\nnext token in a sequence based on a probabilistic distribu-\ntion computed by the model. We can directly use the out-\nput probability of a word given a prompt as the probability\nrelated to an answer. For example, given the prompt “Dr.\nFauci is a”, the model can directly output the probability of\nthe word “scientist” or “liar” appearing as the next word.\nSecond, extracting answers based on output sample anal-\nysis. For each prompt, we can generate a large number of\nrepresentative tweets using the model. We can then compute\nstatistics based on the generated samples. In this preliminary\nstudy, we adopted the second approach. We will explore the\nﬁrst approach in future work.\nSpeciﬁcally, for each prompt, each model generated 1,000\nsample responses. Each response was stored in a database,\nalong with parts-of-speech tags and a sentiment label (either\npositive or negative) automatically assigned by a sentiment\nanalyzer (Akbik et al. 2019). Statistics were computed from\nthe samples, focusing on the relationships between probes\nand models. Initial statistics were gathered on the ﬁrst words\ndirectly after each prompt in each response as they were gen-\nerated directly from the given probe, and are most likely to\nvary with and impacted by the probe. Since next word anal-\nysis is likely to produce high probable functional words that\ndo not carry speciﬁc meaning such as “the” and “has”, parts-\nof-speech tags were used to extract the ﬁrst noun (“virus”)\nor noun-noun combination (“chinese virus”), or adjective\n(“anxious”). Lastly, we computed the percentage of positive\ntweets in the 1000 samples generated per prompt per model.\nOne might ask why should we analyze synthetic tweets\ngenerated by GPT instead of the real tweets directly? For\nsome insight, we can look at Table 3 which shows the num-\nber of times each probe appears exactly in the 18 million real\ntweets we retrieved:\nTable 3: Probe frequency in 18 million COVID tweets\nProbe Count\nDonald Trump is a 1,423\nDr. Fauci is a 427\nThe pandemic was caused by 6\nFor the pandemic, we blame 0\nThough there are enough results to perform statistical\nanalysis related to Donald Trump and Dr. Fauci, the data\nis not sufﬁcient to support an analysis with statistical signif-\nicance for the COVID-19 causes and blames. Because trans-\nformer language models such as the GPT create tweets based\non statistically valid patterns (Feldman 2020), each synthetic\ntweet may encode information derived from a large num-\nber of real tweets that are syntactically and semantically re-\nlated. Further, in principle, the models can generate unlim-\nited datasets to facilitate robust data analysis.\nPreliminary Results\nIn this section, we present results that explore biases in the\ntweet corpora that each model was trained on.\nPolling the General Public on Twitter\nThe covid model was trained on the most data and more rep-\nresentative of mainstream Twitter users’ opinion/attitude. As\nsuch, it was more able to provide more granular responses to\nour probes. Our ﬁrst set of results will focus on the behavior\nof the covid model.\nPROBE: “The pandemic was caused by”Based on the\nnormalized count (i.e. percentage) of the ﬁrst nouns to ap-\npear after this probe in the 1000 generated samples, we list\nthe top ranked nouns in Table 4.\nTable 4: “The pandemic was caused by” top responses\nCause Percentage\nCoronavirus 31%\nVirus 11%\nChina 6%\nLab 2%\nAccident 2%\nThe vast majority of the responses (31%) attribute the\ncoronavirus explicitly, while an additional 11% referred to\nviruses in general. Typical tweets generated by the model\nafter the prompt “The pandemic was caused by” include a\nnovel coronavirus known as SARS-CoV-2and a virus known\nas SARS-CoV-2. Variations of these statements make up over\nhalf of the generated results ranging from fatalistic, a virus\nthat could not have been containedto conspiratorial, a novel\ncoronavirus that originated in a lab in Wuhan.\nThe next values appear to be more focused on human\ncauses. For example, “caused by” China, which had un-\nleashed COVID19 on the world and is responsible . Further\ndown this list align with conspiracy theories, where “the\npandemic was caused by” a lab accident. You can bet there\nwere dozens of other deaths, and “caused by” a ’geneticac-\ncident’ of the genes of aborted babies.\nPROBES: Blame for the pandemic To see if the mod-\nels would distinguish betweencause and blame, we tried the\nprobe “For the pandemic, we blame”. The most common re-\nsponse was to blame President Trump (Table 5). Responses\nTable 5: “For the pandemic, we blame” top responses\nCause Percentage\nTrump 35%\nChina 13.5%\nCOVID-19 5.7%\nMedia 5.3%\nGovernment 2.6%\nsuch as this one are common: “For the pandemic, we blame”\nTrump for the catastrophic response. Tweets that blamed the\nmedia often blamed the government as well: “we blame”the\nMedia and Government for not telling the truth.\nPROBES: How distinct groups are feelingTo extract the\npublic opinions about ethnic groups, we ran three probes,\neach of which began with “During the pandemic,”, and ﬁn-\nished with: 1) “Asians have been feeling” 2) “Black folk\nhave been feeling” 3) “Americans have been feeling.”\nFigure 2: Asian/Black/American feeling, top responses. X-\naxis is sorted by the normalized frequency (percentage) of\neach word appearing in the tweets generated by the covid\nmodel with Americans have been feeling as the prompt.\nThe top results are shown in Figures 2. Common among\nthe Asian and Black groups is the term “brunt”. In the Asian\nresults, tweets like “During the pandemic, Asian Americans\nhave been feeling the brunt of discrimination and harass-\nment.” are common. Alternatively, Black results emphasize\n“feeling the brunt of racism, discrimination, and police bru-\ntality”. All groups have substantial numbers of responses\nthat refer to isolation, with output like “have been feeling\nisolated, lonely & disconnected. ” However, the dominant\nterm in the American responses is “stressed” 4. For exam-\nple, “Americans have been feeling a lot more stressed and\nanxious about a new normal”, and “anxious, stressed, hope-\nless, and depressed ”. These generated texts indicate that\nthe model is presenting a more subjective feeling set of re-\nsponses for Americans in general, while ethnic sub-groups\nare feeling the brunt of external forces.\nPlease note that the above results are about “how groups\nfeel” based on the Twitter general public. To poll the feeling\nof each ethnic group directly, we would need to use a prompt\nlike “I am an Asian. I have been feeling”.\nPolling Different Populations\nIn addition to the “covid” model, we created models trained\non tweets containing the “chinavirus” and “sars-cov-2” tags.\nIn this section, we compare the outputs across the three mod-\nels, similar to polling to different sub-populations on Twitter.\nEach model generated 1,000 synthetic tweets for each probe,\nallowing for direct comparison between the generated data.\nDonald Trump and Dr. FauciOne of the most polarizing\nprompts that we found was “Dr. Fauci is a” 5. This created\ndistinct sets of responses for each of the models, as seen in\nFigure 3. The chinavirus model produced tweets such as “Dr.\nFauci is a liar and a demagogue #ChinaVirus .” Sorting by\nterm frequency based on the outputs of this model produces\nan opposed trend in the sars-cov-2 model. Linear regression\non each model’s term frequency clearly shows this interac-\ntion. The dominant terms produced by the sars-cov-2 model\nfor this prompt are professor, scientist, and physician. The\ngenerated content uses a more informational style, such as\n“Dr. Fauci is a professor and physician. He authored and\nco-authored several papers published on SARS-CoV-2”\nFigure 3: Fauci nouns extracted from different models,\nwhere the x-axis is sorted by their frequency computed from\nthe Twitter samples synthesized by the “chinavirus” model.\nTo examine the differences in emotional tone that these\nmodels produced with the “Dr. Fauci is a” probe, We ran an\nadditional probe, “Donald Trump is a”, and compared the\n4for this analysis the counts for “stressed”, “stress”, and “strain”\nwere combined\n5Thanks to Dr. Peterson of Montgomery College for the suggestion\nsentiment of the tweets synthesized from each probe across\nall three models, using an existing sentiment analyzer (Ak-\nbik et al. 2019). This is shown in Table 6:\nTable 6: Trump / Fauci Positive Sentiment\nProbe chinavirus covid sars-cov-2\nDr. Fauci is a 13.3% 33.1% 53.6%\nDonald Trump is a 44.4% 28.4% 27.1%\nWe see a similar pattern to that seen in Figure 3. In the\nresponse to the “Dr. Fauci is a” probe, the chinavirus model\ngenerates only 13% positive responses, while it generates\napproximately 45% positive text in response to “Donald\nTrump is a” (e.g. here is one such tweet produced by the\nmodel: “Donald Trump is agreat politician and a man of\nintegrity.”). The covid model falls between the other two\nmodels, particularly with respect to the Dr. Fauci probe. It\nis not signiﬁcantly different from the behavior of the sars-\ncov-2 model in response to the Donald Trump probe.\nPolling Over Time\nThe proposed method can also be used to poll opinions at\ndifferent times. We can have two different ways to poll the\nmodel over time. First, ﬁne tune each model with time sen-\nsitive data (e.g., ﬁne tune the model with pre-pandemic ver-\nsus during and post pandemic data); Second, we may use\ntime-sensitive prompts such as “in March, 2020, Americans\nhave been feeling”. In this study, we perform a coarse pre-\nand during-pandemic analysis. We therefore used the ﬁrst\nmethod.\nAs shown in Figure 4, we summarize the public sentiment\ntowards different demographic groups before and after the\npandemic. The GPT2-large model on the left was trained on\ngeneral web data before the pandemic while the other three\nmodels were ﬁne-tuned with the twitter covid data we col-\nlected during the entire course of the pandemic to date. The\nprobes used in the analysis include “[xxx] are like a” where\n[xxx] can be Americans, Hispanics, Asians, Blacks, Jews\nand Chinese. We use the GPT’s responses to these metaphors\nto assess the public sentiment towards different demographic\ngroups.\nAs shown in the chart, the sentiment in general is much\nmore positive before (outputs from 2019 GPT2-large) than\nduring the pandemic (outputs from all three pandemic-\nrelated models). This is true across all demographic groups\nwe considered in the experiment. Moreover, before the pan-\ndemic, the sentiment towards “Americans” is the most pos-\nitive while that towards ”Blacks” is the most negative. Dur-\ning the pandemic, the sentiment towards Chinese has turned\ndecisively negative. This is true across all three pandemic\nmodels. Inspecting tweets generated by different models to-\nwards the Chinese, the GPT2-large model generates tweets\nlike “We think Chinese are likea lot of other groups - very\nloyal to their own, have great energy”. However, during the\npandemic, the chinavirus model generates tweets like “ We\nthink Chinese are likea snake eating bats in a cauldron.\n#ChinaVirus”.\nFigure 4: Public sentiment towards different demographic\ngroups before and during the pandemic. The y-axis is the\npercentage of positive sentiment associated with the samples\ngenerated with each prompt by each model.\nDiscussion and Future Work\nPolling transformer language models have provided us with\na new lens to assess public attitude/opinions towards diverse\nsocial, political and public health issues. It is dynamic and\ncan be used to answer diverse questions. It is computation-\nally inexpensive and does not require any costly human an-\nnotated ground truth to train. It also can be used to support\nlongitudinally studies via either prompt design (e.g., using a\nprompt like “in January 2020”) or model tuning with time-\nappropriate data.\nPolling transformer language models is very different\nfrom real polling. For example, results from GPT, partic-\nularly the small models like chinavirus and sars-cov-2 are\nnoisy. In particular, individual tweets stochastically gener-\nated by GPT may be incorrect. It is important that we rely on\nstatistical patterns rather than individual tweets synthesized\nby these models to draw conclusions. In addition, prompt\ndesign is tricky. Small changes in prompts may result sig-\nniﬁcant changes in results (e.g., “Dr. Fauci is a” verus “Dr.\nFauci is”). Limitations of the TLMs themselves may also\nprevent them from providing accurate information. For ex-\nample, although humans can link affordances (I can walk in-\nside my house) and properties to recover information that is\noften left unsaid (the house is larger than me), TLMs strug-\ngle on such tasks (Forbes, Holtzman, and Choi 2019). TLMs\nare also vulnerable to negated and misprimed probes.\nSo far, we have only scratched the surface trying to probe\nand understand the latent knowledge captured in a trans-\nformer language model. To further this research, we plan\nto (a) develop a systematic approach for prompt design\nbased on a deeper understanding of the relationships be-\ntween prompts and responses as well as between prompts\nand context, (b) infer word/phrase-based probability directly\nbased on the token probability generated by the GPT, (3)\nimprove the NLP techniques used to extract answers from\nsynthesized tweets. In this preliminary study, we employed\nvery simple techniques such as extracting the ﬁrst nouns or\nadjectives, or using existing sentiment tools. With more so-\nphisticated syntactic analysis, we can extract more meaning-\nful answers from model responses.\nReferences\nAkbik, A.; Bergmann, T.; Blythe, D.; Rasul, K.; Schweter,\nS.; and V ollgraf, R. 2019. FLAIR: An easy-to-use frame-\nwork for state-of-the-art NLP. In NAACL 2019 Annual Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics (Demonstrations), 54–59.\nBrown, T. B.; et al. 2020. Language Models are Few-Shot\nLearners.\nColleoni, E.; Rozza, A.; and Arvidsson, A. 2014. Echo\nchamber or public sphere? Predicting political orientation\nand measuring political homophily in Twitter using big data.\nJournal of communication 64(2): 317–332.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805 .\nFeldman, P. 2020. Navigating Language Models with Syn-\nthetic Agents. arXiv preprint arXiv:2008.04162 .\nFloridi, L.; and Chiriatti, M. 2020. GPT-3: Its nature, scope,\nlimits, and consequences. Minds and Machines 30(4): 681–\n694.\nForbes, M.; Holtzman, A.; and Choi, Y . 2019. Do Neural\nLanguage Representations Learn Physical Commonsense?\narXiv preprint arXiv:1908.02899 .\nFowler Jr, F. J. 2013. Survey research methods. Sage publi-\ncations.\nJiang, Z.; Xu, F. F.; Araki, J.; and Neubig, G. 2020. How\ncan we know what language models know? Transactions of\nthe Association for Computational Linguistics 8: 423–438.\nMcGufﬁe, K.; and Newhouse, A. 2020. The radicalization\nrisks of GPT-3 and advanced neural language models.arXiv\npreprint arXiv:2009.06807 .\nPalakodety, S.; KhudaBukhsh, A. R.; and Carbonell, J. G.\n2020. Mining insights from large-scale corpora using ﬁne-\ntuned language models .\nPetroni, F.; Rockt¨aschel, T.; Lewis, P.; Bakhtin, A.; Wu, Y .;\nMiller, A. H.; and Riedel, S. 2019. Language models as\nknowledge bases? arXiv preprint arXiv:1909.01066 .\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training .\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI Blog 1(8).\nSloan, L.; Morgan, J.; Burnap, P.; and Williams, M. 2015.\nWho tweets? Deriving the demographic characteristics of\nage, occupation and social class from Twitter user meta-data.\nPloS one 10(3): e0115545.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. arXiv preprint arXiv:1706.03762 .\nThis paper is based upon work supported by the National\nScience Foundation under grant no. 2024124.",
  "topic": "Polling",
  "concepts": [
    {
      "name": "Polling",
      "score": 0.8538671731948853
    },
    {
      "name": "Transformer",
      "score": 0.7688493132591248
    },
    {
      "name": "Social media",
      "score": 0.7076371908187866
    },
    {
      "name": "Language model",
      "score": 0.6852360963821411
    },
    {
      "name": "Computer science",
      "score": 0.6606118679046631
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.6499254107475281
    },
    {
      "name": "Public opinion",
      "score": 0.5883870720863342
    },
    {
      "name": "Data science",
      "score": 0.4937613904476166
    },
    {
      "name": "Politics",
      "score": 0.4108259975910187
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34497058391571045
    },
    {
      "name": "Natural language processing",
      "score": 0.331756055355072
    },
    {
      "name": "World Wide Web",
      "score": 0.2935555577278137
    },
    {
      "name": "Political science",
      "score": 0.19647696614265442
    },
    {
      "name": "Engineering",
      "score": 0.11335647106170654
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.07837101817131042
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    }
  ],
  "institutions": []
}