{
    "title": "Beyond Search Engines: Can Large Language Models Improve Curriculum Development?",
    "url": "https://openalex.org/W4402480340",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5113064084",
            "name": "Moein Mohammad",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Hajiagha, Mohammadreza Molavi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226285591",
            "name": "Faraji, Abdolali",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226285590",
            "name": "Tavakoli, Mohammadreza",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3123362360",
            "name": "Kismihók Gábor",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Kismih\\`ok, G\\`abor",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3170985148",
        "https://openalex.org/W4200535651",
        "https://openalex.org/W4285086207",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W3089708241",
        "https://openalex.org/W4220765766",
        "https://openalex.org/W4213432396",
        "https://openalex.org/W2936695845"
    ],
    "abstract": null,
    "full_text": "Beyond Search Engines: Can Large Language\nModels Improve Curriculum Development?\nMohammad Moein1, Mohammadreza Molavi Hajiagha 1, Abdolali Faraji1,\nMohammadreza Tavakoli1, and G´ abor Kismih´ ok1\nTechnical Information Library, Hannover, Germany\n{mohammad.moein}@tib.de\nAbstract. While Online Learning is growing and becoming widespread,\nthe associated curricula often suffer from a lack of coverage and outdated\ncontent. In this regard, a key question is how to dynamically define the\ntopics that must be covered to thoroughly learn a subject (e.g., a course).\nLarge Language Models (LLMs) are considered candidates that can be\nused to address curriculum development challenges. Therefore, we de-\nveloped a framework and a novel dataset, built on YouTube, to evalu-\nate LLMs’ performance when it comes to generating learning topics for\nspecific courses. The experiment was conducted across over 100 courses\nand nearly 7,000 YouTube playlists in various subject areas. Our results\nindicate that GPT-4 can produce more accurate topics for the given\ncourses than extracted topics from YouTube video playlists in terms of\nBERTScore.\nKeywords: large language models· informal learning· curriculum devel-\nopment\n1 Introduction & Background\nThe growth of Online Learning has significantly impacted educational institu-\ntions and knowledge acquisition due to its ability to facilitate continual skill de-\nvelopment, especially in the post-pandemic world [3]. Nevertheless, online learn-\ning faces challenges in providing high-quality up-to-date educational contents\n[2]. To tackle these issues, Artificial Intelligence (AI) can be used to automate\nthe process of developing and maintaining high-quality up-to-date curricula [8].\nFor instance, [5] used the Latent Dirichlet Allocation (LDA) algorithm to ex-\ntract topics that must be covered for learning, based on the text of educational\nresources. While the developed methods are effective, they face challenges in\nterms of scalability (they only focus on limited educational areas) due to their\nreliance on heuristics or heavy computation [1].\nThe introduction of promising LLMs, such as ChatGPT [6], allows us to\nquery larger resource databases to improve the existing automatic curriculum\ndevelopment algorithms. Although the opportunities LLMs bring to education\nare significant, it is crucial to consider the associated risks such as generating\nlow-quality or factually wrong text [4]. Therefore, we propose a novel evaluation\narXiv:2412.07422v1  [cs.CY]  10 Dec 2024\n2 M. Moein et al.\nframework to assess the performance of LLMs in the curriculum development\ncontext, specifically in the task of answering the question: ”What topics need to\nbe covered (both as teachers and learners) to gain required up-to-date knowledge\nin a course?”\nTo do that, we evaluated LLMs efficacy to generate relevant topics on the\nsole basis of the course title, comparing to the baseline method derived from\nYouTube data across several educational areas. Our methodology is summarized\nas 1. Definition of target educational courses, 2. Constructing our dataset by\nquerying YouTube as a major learning platform covering a wide range of educa-\ntional areas [7], 3. Generation of learning topics using LLMs (i.e., GPT-3.5 and\nGPT-4), and 4. evaluating the LLM-generated learning topics against YouTube\nalternatives. Therefore, the key contribution of this paper is a structured, em-\npirical methodology to understand how LLMs can assist curriculum developers\nby automatically generating relevant up-to-date topics for courses. In addition,\nwe publish the collected dataset and LLM-generated topics to enable further\ninvestigations and make our approach reproducible.\n2 Method\nIn this section, we explain the details of our method and data collection pipeline\nusing GPT-4, GPT-3.5 and YouTube. Figure 1 illustrates the proposed frame-\nwork to assess the effectiveness of GPT-4 and GPT-3.5 in our defined task for\nmultiple areas. The proposed framework includes the following steps:\n(a) Compilation of education learning areas.We constructed the list of\ntarget learning areas by prompting GPT-4 to compile a list of 25 educa-\ntional areas, maximizing coverage across fields to avoid bias towards certain\nsubjects like engineering (Figure 1, part 1).\n(b) Course selection. For each generated learning area, GPT-4 proposed 4\ncourse titles (Figure 1, part 2).\n(c) Quality check.The data generated in the preceding steps was verified by\nfour human evaluators to ensure its validity.\n(d) YouTube playlist extraction.For each course title from the previous\nstep, we searched YouTube and extracted the top 10 recommended playlists.\nWe observed that some of the playlists did not include informative titles for\ntheir videos. For instance, a few playlists included titles likelecture 1, lecture\n2, etc. These playlists were removed from the dataset.\n(e) Building ground truth dataset.The list of video titles in each playlist\nserved as ground truth data (i.e., covered topics) as they should represent\ntopics in the course/playlist (Figure 1, part 3).\n(f) The baseline learning topics.Our objective was to establish baselines\nfor mapping courses with the learning topics they need to cover. We hy-\npothesized that in the ground truth courses, authors searched for guidance\nfrom related courses (i.e., other YouTube playlists) when determining the\nlearning topics for their courses. Therefore, we queried YouTube for each\nground truth course title to gather the top 3 related YouTube playlists as\nTitle Suppressed Due to Excessive Length 3\nData Science and Machine\nLearning\nPhilosophy and Ethics\nAreas Course1 2\n1. Ethics and Moral Philosophy\n2. Political Philosophy\nCourse Title\n1. Deep Reinforcement Learning\n2. Computer Vision and Image\nRecognition\nYoutube\n4\nGround Truth Curriculum\n3 5\nBert Score Youtbe score\nGPT 4 score\nGPT 3 score\nGPT-4\nGPT-3.5\nFig. 1.Data collection and evaluation pipeline: Ground truth building is illustrated\nin Parts 1-3; Part 4 shows the topic generation with GPT-4, GPT-3.5, and YouTube.\nFinally, model performance is assessed using BERTScore in Part 5.\nbaseline courses (Figure 1, part 4). The collected playlists that were not\naligned with the title of the ground truth courses were removed in this step.\nFurthermore, duplicated phrases were frequently observed within video titles\nof several playlists. Since exact repetitions do not constitute distinct learn-\ning topics and are redundant, they were filtered out to be aligned with the\nmain objective of the collected dataset. For example, the playlist titles were\nrepeated across all videos in a number of playlists. Therefore, n-grams with\nn≥3 were considered to identify and remove such duplicated phrases from\nvideo titles.\n(g) Generating LLM titles. GPT-4 and GPT-3.5 were prompted with the\nsame course titles (ground truth course titles) to generate AI-produced can-\ndidate topics (Figure 1, part 4). Both GPT-4 and GPT-3.5 were sampled\n3 times in order to control for uncertainty. This simulated the process of\nutilizing LLMs to develop an automatic curriculum (list of learning topics)\nfor a course. As an example, our prompt for Political Philosophy was:\nI am preparing to teach a course titled ’Political Philosophy’. Could you\nplease generate a list of topics suitable for teaching this course? The\ntopics should be in English and each one should be listed on a separate\nline.\n4 M. Moein et al.\n(h) Evaluation. The BERTScore [9] was used to evaluate the performance of\nLLM generated curricula (learning topics) in comparison with the YouTube\nbaseline curriculum (Figure 1, part 5). One of the key advantages ofBERTScore\nis its use of contextual embedding, which makes it robust to syntactic dif-\nferences [9]. For the comparison, the cosine similarity between the candi-\ndate embeddings (LLM generated curriculum) and the reference embeddings\n(YouTube baseline) was computed. The Bert-related computations were done\nby using the bert-score python package1 with a xlnet-base-cased model.\nApplying the above-mentioned steps produced a dataset containing 727 ground\ntruth playlists, 6631 baseline courses, and 6 (i.e., 3 for GPT-4 and 3 for GPT-3.5)\nsets of LLM-generated learning topics per ground truth playlist 2.\n3 Results and Discussion\nWe compared the performance of GPT-3.5 and GPT-4 generated learning top-\nics to YouTube video titles (in baseline courses) using BERTScore F1. Table 1\nsummarizes the average and standard deviation of BERTScore for each method.\nGPT-4 achieved slightly higher with an F1 score of 0 .30 ± 0.10 than YouTube\nwith a score of 0 .29 ± 0.11. GPT-3.5 performed worse than the other candidate\nmodels, which was expected since GPT-3.5 is a smaller model than GPT-4. Nev-\nertheless, both GPT models have better precision than YouTube. Essentially, this\nmeans that when GPT models generate topics, more of their topics are present\nin the ground truth dataset, compared to the baseline. But at the same time,\nGPT-4 misses some extra topics, which are relevant to the ground truth (lower\nrecall). Finally, we analyzed the performance of the models based on each area\nas illustrated in Figure 2. This confirms that, in general, GPT-4 is performing\nbetter than YouTube and GPT-3.53.\nTable 1.Average and Standard Deviation of BERTScore for candidate models.\nF1 Precision Recall\nCandidate\nGPT-3.5 0.27 ± 0.10 0.30 ± 0.12 0.25 ± 0.10\nGPT-4 0.30 ± 0.10 0.32 ± 0.12 0.29 ± 0.10\nYouTube 0.29 ± 0.11 0.29 ± 0.13 0.30 ± 0.13\nAlthough our results are promising, some limitations should be considered.\nWe acknowledge that considering more learning areas will provide a better es-\ntimation of the performance of LLMs in our target task. Also, the results are\n1 https://pypi.org/project/bert-score/\n2 The final dataset can be found from: https://bit.ly/raw dataset\n3 Detailed performance metric per area can be found from:\nhttps://bit.ly/topic model performance\nTitle Suppressed Due to Excessive Length 5\nData Science and Machine Learning\nProgramming and Development\nBusiness and Entrepreneurship\nHealthcare and Medicine\nFinance and Investment\nCreative Arts and Design\nLanguage Learning\nData Analytics and Visualization\nScience and Engineering\nEnvironmental Studies\nEducation and Teaching\nHealth and Wellness\nHistory and Archaeology\nCommunication and Public Speaking\nCulinary Arts and Chef Training\nPhilosophy and Ethics\nLiterature and Writing\nDPersonal Development and Self-Help\nCulinary Arts and Cooking\nMusic and Performing Arts\nIT and Software Development\nGeographic Information Systems (GIS)\nHuman Resources and Talent Management\nAutomotive Technology\nAerospace and Aviation\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nScore\ncandidate\ngpt-3\ngpt-4\nyoutube\nAverage F1 per Area\nFig. 2.Performance for each area. The graph shows the mean and 95% confidence\ninterval. Areas where GPT-4 outperformed YouTube are bold faced.\nlimited to using YouTube as ground truth, and not necessarily generalizable to\nother large repositories or resources of learning content. However, this is not\nthe limitation of the evaluation framework, but the dataset we constructed. It\nshould be noted that YouTube is a powerful opponent in our defined task as\nit uses more information (e.g., video content and description) beyond just the\nvideo title to offer related playlists (courses). Therefore, despite the potential\ninformation discrepancy, the LLMs achieved competing results and this shows\nthe potential of these LLMs when it comes to generating relevant topics for a\ncourse.\n4 Conclusion and Future Work\nIn this study, we present a framework and comprehensive dataset to explore\nLLMs’ capability in curriculum development, particularly in generating relevant\nlearning topics. Our defined task was finding the learning topics that should be\ncovered in a course. In 25 different learning areas and their including courses,\n6 M. Moein et al.\nGPT-4 models performed reasonably well in terms of F1 score which highlighted\nthe benefits of such models for curriculum development.\nWe recognize that certain decisions in this study, such as how we define educa-\ntional areas, might have valid alternatives. However, our focus was on presenting\nour framework as a case study. We aimed to demonstrate how YouTube (as a\npopular learning platform) can serve as an indicator for outdated curricula, and\nhow Large Language Models (LLMs) can offer a quick, effective update method\nin these situations. Further research, involving more diverse data sources, such as\nCoursera or university curricula, could improve the proposed dataset and refine\nour framework. Also, further investigations need to be carried out on prompt\nengineering (for example to provide more context) and hyperparameter tuning\nto investigate whether we can improve the LLMs’ performance.\nReferences\n1. Benmesbah, O., Lamia, M., Hafidi, M.: An improved constrained learning path\nadaptation problem based on genetic algorithm. Interactive Learning Environments\n31(6), 3595–3612 (Aug 2023). https://doi.org/10.1080/10494820.2021.1937659\n2. Dang, A., Khanra, S., Kagzi, M.: Barriers towards the continued usage of massive\nopen online courses: A case study in india. The International Journal of Management\nEducation 20(1), 100562 (Mar 2022). https://doi.org/10.1016/j.ijme.2021.100562\n3. Greenhow, C., Graham, C.R., Koehler, M.J.: Foundations of online learning: Chal-\nlenges and opportunities. Educational Psychologist 57(3), 131–147 (Jul 2022).\nhttps://doi.org/10.1080/00461520.2022.2090364\n4. Kasneci, E., Seßler, K., K¨ uchemann, S., Bannert, M., Dementieva, D., Fischer, F.,\nGasser, U., Groh, G., G¨ unnemann, S., H¨ ullermeier, E.: Chatgpt for good? on op-\nportunities and challenges of large language models for education. Learning and\nindividual differences 103, 102274 (2023)\n5. Molavi, M., Tavakoli, M., Kismih´ ok, G.: Extracting topics from open educational\nresources. In: Addressing Global Challenges and Quality Education: 15th European\nConference on Technology Enhanced Learning, EC-TEL 2020, Heidelberg, Germany,\nSeptember 14–18, 2020, Proceedings 15. pp. 455–460. Springer (2020)\n6. OpenAI: Chatgpt: Optimizing language models for dialogue (2022), https://\nopenai.com/blog/chatgpt, accessed: 2023-09-30\n7. Pires, F., Masanet, M.J., Tomasena, J.M., Scolari, C.A.: Learning with youtube:\nBeyond formal and informal through new actors, strategies and affordances. Con-\nvergence: The International Journal of Research into New Media Technologies28(3),\n838–853 (Jun 2022). https://doi.org/10.1177/13548565211020545\n8. Tavakoli, M., Faraji, A., Vrolijk, J., Molavi, M., Mol, S.T., Kismih´ ok, G.: An ai-\nbased open recommender system for personalized labor market driven education.\nAdvanced Engineering Informatics 52, 101508 (2022)\n9. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: Bertscore:\nEvaluating text generation with bert (arXiv:1904.09675) (Feb 2020).\nhttps://doi.org/10.48550/arXiv.1904.09675, http://arxiv.org/abs/1904.09675,\narXiv:1904.09675 [cs]"
}