{
  "title": "Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries",
  "url": "https://openalex.org/W3079786700",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5073408576",
      "name": "Benjamin Heinzerling",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101815181",
      "name": "Kentaro Inui",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2963184844",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2476140796",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2964075320",
    "https://openalex.org/W2852336278",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963606508",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2122865749",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2964084720",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W11298561",
    "https://openalex.org/W2888236192",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2043794661",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2091950909",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2165558283",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2904683980",
    "https://openalex.org/W3016309009",
    "https://openalex.org/W2110982843",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W205829674",
    "https://openalex.org/W2963356458",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W2987249037",
    "https://openalex.org/W2593864460",
    "https://openalex.org/W2964116313",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W1852412531",
    "https://openalex.org/W2963855739",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963290255",
    "https://openalex.org/W2963432357",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2760505947",
    "https://openalex.org/W2970070749",
    "https://openalex.org/W3083182073",
    "https://openalex.org/W2812869832",
    "https://openalex.org/W36903255",
    "https://openalex.org/W3104097132",
    "https://openalex.org/W2981852735"
  ],
  "abstract": "Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose single-token name is found in common LM vocabularies. Furthermore, the main benefit of this paradigm, namely querying the KB using a variety of natural language paraphrases, is underexplored so far. Here, we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to represent millions of entities and present a detailed case study on paraphrased querying of world knowledge in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.",
  "full_text": "Language Models as Knowledge Bases:\nOn Entity Representations, Storage Capacity, and Paraphrased Queries\nBenjamin Heinzerling1, 2 and Kentaro Inui2, 1\n1RIKEN AIP & 2Tohoku University\nbenjamin.heinzerling@riken.jp | inui@tohoku.ac.jp\nAbstract\nPretrained language models have been sug-\ngested as a possible alternative or comple-\nment to structured knowledge bases. However,\nthis emerging LM-as-KB paradigm has so far\nonly been considered in a very limited setting,\nwhich only allows handling 21k entities whose\nname is found in common LM vocabularies.\nFurthermore, a major beneﬁt of this paradigm,\ni.e., querying the KB using natural language\nparaphrases, is underexplored. Here we formu-\nlate two basic requirements for treating LMs\nas KBs: (i) the ability to store a large num-\nber facts involving a large number of entities\nand (ii) the ability to query stored facts. We\nexplore three entity representations that allow\nLMs to handle millions of entities and present\na detailed case study on paraphrased querying\nof facts stored in LMs, thereby providing a\nproof-of-concept that language models can in-\ndeed serve as knowledge bases.\n1 Introduction\nLanguage models (LMs) appear to memorize world\nknowledge facts during training. For example,\nBERT (Devlin et al., 2019) correctly answers\nthe query “Paris is the capital of [MASK]” with\n“France”. This observation prompted Petroni et al.\n(2019) to ask if LMs can serve as an alternative or\ncomplement to structured knowledge bases (KBs),\nthereby introducing the idea of treating LMs as\nKBs: During training, the LM encounters world\nknowledge facts expressed in its training data, some\nof which are stored in some form in the LM’s pa-\nrameters. After training, some of the stored facts\ncan be recovered from the LM’s parameters by\nmeans of a suitable natural language query (Fig. 1).\nA LM with such a “built-in” KB is useful for\nknowledge-intensive tasks (Petroni et al., 2020)\nand question answering (Roberts et al., 2020), and\ncould improve natural language interfaces to struc-\ntured data (Hendrix et al., 1978; Herzig et al., 2020).\nFigure 1: The LM-as-KB paradigm, introduced by\nPetroni et al. (2019). A LM memorizes factual state-\nments, which can be queried in natural language.\nHowever, this emerging LM-as-KB paradigm is\nfaced with several foundational questions.\nFirst question: KBs contain millions of entities,\nwhile LM vocabulary size usually does not exceed\n100k entries. How can millions of entities be rep-\nresented in LMs? Petroni et al. (2019) circum-\nvent this problem by only considering 21k enti-\nties whose canonical name corresponds to a single\ntoken in the LM’s vocabulary, e.g., entities like\n“France” or “Bert”, but not “United Kingdom” or\n“Sesame Street”. Hence, this approach cannot han-\ndle entities not contained in the vocabulary and a\nquery like “Bert is a character on [MASK]” is not\nanswerable in this simpliﬁed setting. To answer\nthis ﬁrst question, we compare three methods for\nscaling LM-as-KB to millions of entities:\n1. Symbolic representation, i.e., extending the\nLM vocabulary with entries for all entities;\n2. Surface form representation, i.e., each entity is\nrepresented by its subword-encoded canonical\nname, which is stored and queried by extend-\ning the LM with a sequence decoder; and\n3. Continuous representation, i.e., each entity is\nrepresented as an embedding.\nWe ﬁnd that, while all three entity representations\nallow LMs to store millions of world-knowledge\nfacts involving a large number of entities, each rep-\nresentation comes with different trade-offs: Sym-\narXiv:2008.09036v2  [cs.CL]  21 Apr 2021\nbolic representation allows the most accurate stor-\nage, but is computationally expensive and requires\nentity-linked training data. Surface representation\nis computationally efﬁcient and does not require\nentity-linked data, but is less accurate, especially\nfor longer entity names. Continuous representation\nalso requires entity-linked data, but is computation-\nally more efﬁcient than symbolic representation.\nSecond question: What is the capacity of LMs\nfor storing world knowledge? Can a LM store,\nsay, all relation triples contained in a KB like Wiki-\ndata (Vrande ˇci´c and Kr ¨otzsch, 2014)? Here we\nconduct experiments using synthetic data to study\nthe scaling behaviour of current LM architectures.\nVarying the number of trainable model parameters\nand recording the number of relation triples mem-\norized at a given accuracy level, we ﬁnd that, e.g.,\na Transformer (Vaswani et al., 2017) with 125 mil-\nlion parameters (12 layers of size 768), has the\ncapacity to memorize 1 million Wikidata relation\ntriples with 95 percent accuracy or 5 million rela-\ntion triples with 79 percent accuracy. Assuming\nlinear scaling, this ﬁnding suggests that larger LMs\nwith tens or hundreds of billions of parameters (Raf-\nfel et al., 2019; Brown et al., 2020) can be used to\nstore sizable parts, if not all, of a KB like Wikidata.\nThird question: How robustly is world knowl-\nedge stored in LMs? Is the LM able to recall a\nfact even if the query is slightly different than what\nwas memorized during training? For example, if\nthe LM memorized “Barack Obama was born in\nHawaii” during training, can it answer queries like\n“Barack Obama is from [MASK]” or “Where was\nBarack Obama born? [MASK]”? Here we conduct\nexperiments to measure how well the LM transfers\nknowledge from memorized statements to query\nvariants, both in a zero-shot setting in which the\nmodel is not exposed to the target query variant\nduring training, and a few shot setting, in which the\nmodel is ﬁnetuned on a small number of statements\ncontaining the target query variant. We observe\nzero-shot transfer in case of highly similar query\nvariants, and see successful few-shot transfer after\nﬁnetuning with 5 to 100 instances in case of less\nsimilar queries. This ability to handle soft, natu-\nral language queries, as opposed to hard, symbolic\nqueries in a language like SQL or SPARQL, is one\nof the key motivations for using LMs as KBs.\nContributions. We formulate two requirements\nfor treating LMs as KBs: (i) the ability to store\na large number of facts involving a large number\nof entities and (ii) the ability to query stored facts.\nAfter providing background on world knowledge in\nLMs (§2), we make the following contributions:1\n• A comparison of entity representations for\nscaling LM-as-KB to millions of entities (§3);\n• Empirical lower bounds on LM capacity for\nstoring world knowledge facts (§4); and\n• A controlled study of knowledge transfer from\nstored facts to paraphrased queries (§5).\nTerminology. In this work we are interested in\nstoring and retrieving world knowledge facts in\nand from a LM. World knowledge is knowledge\npertaining to entities, such as Barack Obama. A\nfact is a piece of world knowledge that can be ex-\npressed with a concise natural language statement,\nsuch as the English sentence Barack Obama was\nBorn in Hawaii , or with a relation triple, such\nas ⟨Barack Obama, wasBornIn, Hawaii⟩. A relation\ntriple, or relation for short, consists of asubject en-\ntity (Barack Obama), a predicate (wasBornIn), and\nan object entity (Hawaii). A knowledge base is a\nset of relations. Knowledge bases, such as Wiki-\ndata, typically contain thousands of predicates, mil-\nlions of entities, and billions of relations.\n2 World Knowledge in Language Models\nLarge pretrained LMs have been the driver of re-\ncent progress in natural language processing (Pe-\nters et al., 2018; Howard and Ruder, 2018; Rad-\nford et al., 2019; Devlin et al., 2019). While the\ntrend towards larger LMs is likely to continue (Raf-\nfel et al., 2019; Kaplan et al., 2020; Brown et al.,\n2020), it has limitations: (i) A model trained only\non text lacks grounding in perception and expe-\nrience and hence cannot learn meaning (Bender\nand Koller, 2020). (ii) Reporting bias leads to cer-\ntain knowledge rarely or never being expressed in\ntext. For example, a LM will easily learn to asso-\nciate the phrase “Barack Obama” with the phrase\n“U.S. President”, but might less likely learn that he\nis a “human being”, since the latter fact is rarely\nstated explicitly in text. In contrast, this type of\nknowledge is readily available in KBs. (iii) A large\nnumber of rare entities (Hoffart et al., 2014; Der-\nczynski et al., 2017; Ilievski et al., 2018) are, by\ndeﬁnition, rarely mentioned, making it difﬁcult for\n1Code available at:\nhttps://github.com/bheinzerling/lm-as-kb\nLMs to acquire knowledge about this long tail of\nentities from text alone.\nThese limitations have motivated efforts to ex-\nplicitly2 equip LMs with world knowledge. Table 2\n(Appx. A) situates these efforts on a spectrum from\npurely text-based LMs to representations of struc-\ntured KBs. Models based on text generation (Raf-\nfel et al., 2019; Roberts et al., 2020) and retrieval\n(Guu et al., 2020) have proven most successful in\nknowledge-intensive tasks. However, we argue that\nmodels which reify entities (Logan et al., 2019),\ni.e., models in which entities are “ﬁrst-class citi-\nzens” that can be directly predicted3, are a promis-\ning research direction, since the direct links into a\nKB can be seen as a form of grounding. This is one\nof our main motivations for considering symbolic\nand continuous entity representations.\n3 Entity Representations\nHow can millions of entities be represented in a\nLM? To answer our ﬁrst question, we compare\nthree types of entity representations: symbolic, sur-\nface form, and continuous.\nExperimental setup. We evaluate entity represen-\ntations by measuring how well they allow a LM\nto store and retrieve world knowledge facts. For\nexample, if the LM’s training data contains the\nstatement “Bert is a character on Sesame Street”,\nthe model should memorize this statement and re-\ncall the correct object Sesame Street when asked\nwith a query like “Bert is a character on [MASK].”\nSynthetic data. It is not a priori clear how many\nfacts a text from the LM’s training data, say, a\nWikipedia article, expresses. Since we want to\nprecisely measure how well a LM can store and\nretrieve facts, we create synthetic data by generat-\ning statements from KB relations and then train the\nmodel to memorize these statements. Using Wiki-\ndata as KB, we ﬁrst deﬁne two sets of entities: A\nsmaller set consisting of the top 1 million Wikidata\nentities according to node outdegree, and a larger\nset consisting of the roughly 6 million Wikidata en-\ntities that have an entry in the English Wikipedia.\nNext, we manually create templates for the 100\nmost frequent Wikidata predicates. For example,\nfor the predicate P19 (“place of birth”), we create\nthe template S was born in O and generate English\nstatements by ﬁlling the S and O slots with entities\n2As opposed to the LM acquiring world knowledge implic-\nitly as a side effect of its training objective.\n3As opposed to generating or retrieving a surface form\nwhich may or may not correspond to an entity.\n0 1M 2M 3M 4M 5M\nNumber of statements\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Memorization accuracy\nLSTM 1024\nLSTM 256\nRoBERTa-base\nRoBERTa-base without pretraining\nFigure 2: Accuracy of statement memorization with\nsymbolic representation of 1 million entities.\nfrom the sets deﬁned above for which this rela-\ntion holds.4 To make queries for an object unique\ngiven subject and predicate, we arbitrarily select\nexactly one fact if there are multiple objects and\ndiscard the other facts. This process yields 5 mil-\nlion statements involving up to 1 million entities,\nand 10 million statements involving up to 6 million\nentities. These statements then serve as training\ninstances, i.e., given the query “Barack Obama was\nborn in [MASK]”, the model should predict Hawaii.\nAs our goal is to store facts in a LM, there is no\ndistinction between training and test data.\nModels and training. We consider two common\nLM architectures: LSTMs (Hochreiter and Schmid-\nhuber, 1997) and Transformers (Vaswani et al.,\n2017). For LSTMs, we compare two conﬁgura-\ntions; a randomly initialized two-layer LSTM with\nlayers size 256 (LSTM 256) and one with layer size\n1024 (LSTM 1024). For Transformers, we com-\npare a pretrained RoBERTa-base (Liu et al., 2019),\nand RoBERTa without pretraining, i.e., a randomly\ninitialized Transformer of the same size. For con-\nsistent tokenization across all four models, we\nsubword-tokenize statements with the RoBERTa\ntokenizer. To store statements in a LM, we train\nuntil the model reaches 99 percent memorization\naccuracy, i.e., overﬁts the training data almost per-\nfectly, or stop early if accuracy does not improve\nfor 20 epochs. See Appx. D for training details.\n3.1 Symbolic Representation\nWith symbolic representation, each entity is repre-\nsented as an entry in the LM’s vocabulary. Predic-\ntion is done via masked language modeling (Devlin\net al., 2019), by encoding the query with the LM,\nprojecting the ﬁnal hidden state of the [MASK] to-\nken onto the vocabulary and then taking a softmax\n4Templates and sample of statements in Appx. B and C.\nover the vocabulary. As the results show (Fig. 2),\nsymbolic representation yields very high memo-\nrization accuracies with a vocabulary of 1 million\nentities. Randomly initialized RoBERTa-base with-\nout pretraining works best, memorizing 97 percent\nof 5 million statements correctly.\nUnfortunately, the softmax computation be-\ncomes prohibitively slow as the vocabulary size\nincreases (Morin and Bengio, 2005), making sym-\nbolic representation with a softmax over a vocabu-\nlary consisting of the full set of 6 million Wikipedia\nentities impractical. Imposing a hierarchy is a com-\nmon approach for dealing with large vocabularies,\nbut did not work well in this case (See Appx. F.1).\n3.2 Surface Form Representation\nWith surface form representation, each entity is\nrepresented by its canonical name. 5 Since this\nname generally consists of more than one token,\nwe cast memorizing statements and querying facts\nas a sequence-to-sequence task (Sutskever et al.,\n2014): Given the source sequence “Bert is a char-\nacter on [MASK]”, the model should generate the\ntarget sequence “Sesame Street”.6 To make models\nmemorize statements, we train until perplexity on\nthe training data reaches 1.0 or does not improve\nfor 20 epochs. For evaluation, we generate the tar-\nget sequence – i.e., the answer to a given query –\nvia a beam search with beam size 10. We measure\nperfect-match accuracy of the full entity name, i.e.,\nthere is no credit for partial token matches.\nThe four models under comparison are now\ntreated as sequence-to-sequence encoders and ex-\ntended with a decoder of the same size: LSTM\ndecoders for LSTM encoders ( LSTM2LSTM)\nand randomly initialized Transformers for Trans-\nformer encoders (RoBERTa2Transformer, Trans-\nformer2Transformer).\nUnlike symbolic representation, surface rep-\nresentation can handle the entire set of 6 mil-\nlion Wikipedia entities. As with symbolic rep-\nresentation, the randomly initialized Transformer\n(Fig. 3, dash-dotted red line) has the highest ca-\npacity, memorizing 10 million statements with\n90 percent accuracy. A pretrained encoder\n(RoBERTa2Transformer) appears to have a delete-\nrious effect, yielding lower accuracies than the ran-\ndomly initialized Transformer2Transformer. While\nthe larger LSTM2LSTM (layer size 1024) almost\n5We use English Wikidata labels as canonical names.\n6The [MASK] token is included since the target entity does\nnot always occur at the end of a statement.\n0 2M 4M 6M 8M 10M\nNumber of statements\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Memorization accuracy\nLSTM2LSTM (1024)\nLSTM2LSTM (256)\nRoBERTa2Transformer\nTransformer2Transformer\nFigure 3: Accuracy of statement memorization with ob-\nject entities represented by surface forms.\nFigure 4: Error analysis of statements memorized\nvia surface form representation. Correctly memorized\nstatements orange, wrong ones blue. Selected clusters\nare annotated with the statement’s object entity (green).\nFrequencies clipped to 200, jitter applied for clarity.\nmatches the performance of the best Transformer\nmodel, the smaller one (layer size 256) has insufﬁ-\ncient capacity, memorizing less than 50 percent of\n5 million statements.\nAnalysis of the Transformer2Transformer model\n(Fig. 4) reveals, perhaps unsurprisingly, that state-\nments involving infrequent, long entity mentions\nare difﬁcult to memorize.7 For example, the model\nfails to memorize most entity mentions that occur\nonly in one to ten statements and have a length of\n12 or more subwords (blue cluster, upper left).\n3.3 Continuous Representation\nWith continuous representation, an entity ei, i∈\n[1, Nentities] is represented by a d-dimensional em-\nbedding yi ∈Rd. After encoding a query with\nthe LM, prediction is performed by projecting the\nﬁnal hidden state corresponding to the [MASK]\n7We speculate that this drawback can be mitigated by short-\nening canonical names while ensuring a one-to-one mapping\nto entities, but leave this to future work.\n0 2M 4M 6M 8M 10M\nNumber of statements\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Memorization accuracy\nLSTM 1024\nLSTM 256\nRoBERTa-base\nRoBERTa-base without pretraining\nFigure 5: Accuracy of statement memorization with\ncontinuous entity representation.\nNo (195k mentions) Yes (805k mentions)\nRelation object memorized?\n100\n101\n102\n103\n104\nEntity mention frequency\nFigure 6: Error analysis of a sample of 1 million\nstatements memorized by a randomly initialized Trans-\nformer with continuous representation.\ntoken onto Rd, obtaining the predicted embedding\nˆ y∈Rd. We use ﬁxed, pretrained entity embed-\ndings and train with cosine lossL = 1−cos(ˆ y, yi).\nAt test time, the model prediction ˆ yis mapped\nto the closest pretrained entity embedding yi via\nnearest-neighbor search (Johnson et al., 2017).\nContinuous prediction with ﬁxed, pretrained\nembeddings. When training randomly initialized\nembeddings with a similarity objective, a degener-\nate solution is to make all embeddings the same,\ne.g., all-zero vectors. To prevent this, it is com-\nmon practice to use negative samples (Bordes et al.,\n2013). When using ﬁxed, pretrained embeddings\nas supervision signal, negative sampling is not nec-\nessary, since the target embeddings are not updated\nand therefore cannot become degenerate.\nWikidata embeddings. We train embeddings for 6\nmillion Wikidata entities using feature-speciﬁc au-\ntoencoders to encode entity features such as names,\naliases, description, entity types, and numeric at-\ntributes, following prior work on multi-modal KB\nembeddings (Pezeshkpour et al., 2018) and KB\nembeddings with autoencoders (Takahashi et al.,\n2018). Embedding training is detailed in Appx. E.\nResults. Fig. 5 shows memorization accuracies\nachieved with continuous representation. Like\nsurface representation, continuous representation\nscales to 6 million entities, and we see the same rel-\native order of models, but with overall lower accura-\ncies. RoBERTa without pretraining has the highest\ncapacity for storing world knowledge statements,\nmemorizing 67 percent of 10 million statements,\nwhile the small LSTM 256 model has the lowest ca-\npacity, memorizing 42 percent. Although far from\nfully understood, sequence-to-sequence architec-\ntures are relatively mature, with highly-optimized\ntoolkits and hyperparameter settings publicly avail-\nable (Ott et al., 2019). In contrast, prediction of\ncontinuous representations is still in an early stage\nof research (Kumar and Tsvetkov, 2019). We there-\nfore see these results as lower bounds for LM ca-\npacity with continuous representations.\nBy design, memorization with continuous repre-\nsentations does not rely on entity names, and hence,\nin contrast to surface form representation, does not\nlead to difﬁculties in handling entities with long\nnames. However, as with surface form representa-\ntion, infrequent entities are more difﬁcult to memo-\nrize than frequent ones. Most of the memorization\nerrors (Fig. 6, blue, left) involve infrequent enti-\nties with a median frequency of 3, while most of\nthe correctly memorized statements (orange, right)\ninvolve entities that occur more than 100 times.\n4 LM Capacity for Storing Facts\nWe now turn to the second question, how model\ncapacity scales with model size (Fig. 7, top). With\na 12-layer Transformer of layer size 96 or 192 (top\nsubﬁgure, solid red and dashed green lines), mem-\norization accuracy quickly drops as the number of\nfacts to memorize increases. Larger models can\nmemorize more facts, but accuracy drops rather\nquickly, e.g., to 65 percent of 3 million facts mem-\norized with a layer size of 384 (dotted orange line).\nAssuming a desired memorization accuracy of\n80 percent, we record the maximum number of\nfacts a model of a given size can memorize at this\nlevel (Fig. 7, bottom). For the model sizes consid-\nered here, storage capacity appears to scale linearly,\nwith a model of layer size 384 (55M parameters)\nstoring one million facts and a model of layer size\n960 (160M parameters) storing 7 million facts.\nApart from the number of facts to store, we hy-\npothesize that successful storage depends on two\nmore factors: the number of entities and the en-\n0 2M 4M 6M 8M 10M\nNumber of statements\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Memorization accuracy\nLayer size\n768\n384\n192\n96\n288\n(39M)\n384\n(55M)\n768\n(125M)\n960\n(160M)\nHidden layer size (#parameters)\n0.8M1.0M\n5.0M\n7.0MMax #facts memorizable\nFigure 7: Scaling of storage capacity with model size.\nMemorization accuracy decreases as the number of\nfacts grows (top). The maximum number of facts a\nmodel of a given layer size (parameter count) can mem-\norize with an accuracy of 80 percent increases linearly\n(bottom). All models are 12-layer Transformer with\ncontinuous representation of 6 million entities.\nAccuracy\nRepresentation 1M 6M\nSymbolic 0.97 n/a\nSurface 0.92 0.90\nContinuous 0.85 0.79\nTable 1: The number of entities (1M or 6M) impacts\nmemorization accuracy. The model is a 12-layer Trans-\nformer, layer size 768, memorizing 1 million facts.\ntropy of their distribution. As expected, a large\nnumber of entities makes memorization more difﬁ-\ncult (Table 1). The number of entities has a small\neffect with surface representation (2 percent drop),\nbut with continuous representation accuracy drops\nfrom 85 percent to 79 percent when the number of\nentities increases from 1 to 6 million. We also ob-\nserve an impact of the entity distribution (Appx. G),\nbut leave detailed analysis to future work.\n4.1 Storage Efﬁciency\nOur comparison of different entity representations\n(§3) does not control for the number of trainable\nmodel parameters. That is, we selected common ar-\n0 1M 2M 3M 4M 5M\nNumber of statements\n0.00\n0.02\n0.04\n0.06\n0.08Storage efficiency\ncontinuous\nsurface form\nsymbolic\nFigure 8: Storage efﬁciency with symbolic, surface\nform, and continuous representation of entities. In the\nsetting considered in this work, continuous representa-\ntion is most efﬁcient.\nchitectures, such as a Transformer with12 layers of\nsize 768, but made no effort to ensure that, e.g., the\nnumber of trainable parameters introduced by the\nsoftmax layer in a model with symbolic representa-\ntion matches the number of trainable parameters in-\ntroduced by the addition of a sequence-to-sequence\ndecoder component in a model with surface form\nrepresentation. In order to more fairly compare\nentity representations across models with differing\nnumbers of trainable parameters, we formulate the\nstorage efﬁciency of a model designed to memorize\nstatements:\nStorage efﬁciency = #statements ×accuracy\n#parameters\nThis measure expresses the intuition that a model is\nefﬁcient if it requires few parameters to memorize\na large number of statements with high accuracy.\nWhen quantifying efﬁciency with this measure, we\nﬁnd that continuous representation is the most efﬁ-\ncient (Figure 8) and hence use this form of entity\nrepresentation in the remainder of this work.\n5 Querying Stored Facts\nSo far, we saw that it is possible to store millions\nof facts in a LM, by ﬁnetuning the model to pre-\ndict the masked object of statements like Barack\nObama was born in [MASK]. However, given the\nlarge number of model parameters and training ef-\nfort, mere storage is not a compelling achievement:\nThe underlying relations, here ⟨Barack Obama, was-\nBornIn, Hawaii⟩, can be stored more compactly and\nwith perfect accuracy in a structured KB.\nOne of the potential beneﬁts of the LM-as-KB\nparadigm is the LM’s ability to handle paraphrases.\nIf the LM’s representation of the statement above is\nsufﬁciently similar to its representation of queries\nlike Barack Obama is from [MASK] or Where\nis Barack Obama from? [MASK] , this similarity\ncould allow transfer from the memorized statement\nto these unseen queries. Is this soft querying of\nfacts stored in a LM possible? We now conduct\na controlled experiment to answer this question,\nexpecting one of the following three outcomes:\n1. Rote memorization . The model memorizes\nstatements with little or no abstraction, so that even\nsmall, meaning-preserving changes to the query\nprevent the model from recalling the correct object.\n2. Generic association. The model memorizes\npairs of subject and object entities but disregards\nthe predicate. For example, a model might pre-\ndict Hawaii whenever the query contains the phrase\nBarack Obama, regardless of context. This patho-\nlogical behaviour could be especially prevalent if\nthe distribution of object entities co-occurring with\na given subject is dominated by one object.\n3. Fact memorization. The model memorizes\nfacts expressed in statements by forming abstrac-\ntions corresponding to entities and predicates. This\nallows retrieving a fact with a variety of queries.\nSections 3 and 4 already established that a model\nof sufﬁcient size can perform rote memorization of\nmillions of statements. We now design an experi-\nment to test whether LMs are capable of fact mem-\norization while taking care to distinguish this ca-\npability from generic association. Concretely, our\ngoal is to test if a LM that has memorized a state-\nment like Barack Obama was born in Hawaii can\nuse this knowledge to answer a query like Barack\nObama is from [MASK]. Conveniently,wasBornIn\nrelations are among the most frequent in Wikidata\nand hold for a diverse set of subject and object enti-\nties. This diversity of entities makes this predicate\na good candidate for our case study, since state-\nments involving a predicate with a less diverse set\nof subject or object entities are easier to memorize.8\nStatements and controls. We sample 100k state-\nments generated by the template “S was born in\nO”. To allow distinguishing if a model that mem-\norizes these 100k facts does so by generic assoca-\ntion or by fact memorization, we introduce control\nfacts. Given a fact ⟨S, P, O⟩, its control ⟨S, P’, O’⟩\ninvolves the same subject S, but a distinct predi-\ncate P’ and object O’. For example, a control for\n8For example, with the predicate isA and relations like\n⟨Barack Obama, isA, human⟩ the model would do well by\nalways predicting human if the subject mention matches a\nfrequent person name pattern such as “two capitalized words”.\nthe fact ⟨Albert Einstein, wasBornIn, Ulm⟩is the fact\n⟨Albert Einstein, diedIn, Princeton⟩. We add 100k\ncontrol statements generated from the template “S\ndied in O”’ and train RoBERTa-base to memorize\nall 200k statements with 98 percent accuracy. The\ncombination of statements and controls counters\ngeneric association: To correctly answer the query\n“Albert Einstein died in [MASK]”, the model needs\nto take into account the predicate, since two distinct\nobjects are associated with Albert Einstein.\nQuery variants. Next, we collect query variants,\nsuch as “S is from O” (row labels in Fig. 9, top).\nExpecting good transfer for variants that are sim-\nilar to the original statement, we include variants\nwith small changes, such as varying punctuation.\nAs more diverse variants, we select frequent rela-\ntion patterns, such as “S (b. 1970, O)”, from the\nGoogleRE Corpus (Google, 2013), as well as a\nquery in question form and queries with irrelevant\nor misleading distractors such as “S was born in\nO, but died somewhere else”. For each variant, we\ngenerate 100k queries by ﬁlling the S and O slots\nwith the same entity pairs as the original statements.\nTo balance statements and controls, we create con-\ntrol templates (row labels in Fig. 9, bottom) and\ngenerate a matching number of control statements.\nTransfer results. We evaluate knowledge transfer\nfrom memorized statements to query variants us-\ning RoBERTa-base (Fig. 9, top, left), measuring\naccuracy over the 100k statements generated with\na target query variant template. To measure the ef-\nfect of pretraining on transfer ability, we compare\nto RoBERTa-base without pretraining (Fig. 9, top,\nright). We consider zero-shot transfer without any\nﬁnetuning towards the target query variant, and a\nﬁnetuning setting, in which the LM is ﬁrst trained\nto memorize all 100k original statements and then\nﬁnetuned until it memorizes a small number of\nstatements in the target query format.9\nIn the zero-shot setting (leftmost column), even\nsmall changes to the query lead to a drop in fact re-\ncall: Adding an ellipsis (4th row) causes the model\nto answer 95% of queries correctly, a 3% drop from\n98% memorization of the original statements (ﬁrst\nrow). Adding an exclamation mark (5th row) re-\nsults in a 8% drop. For other paraphrases, e.g., S,\nwho is from O (7th row) and S is from O, zero-shot\ntransfer works only in 35% and 20% of cases, and\n9Our experiments, which test whether a LM can transfer\na memorized fact to given target paraphrases, are converse to\nthe probing setup by Jiang et al. (2020), which aims to ﬁnd\nthe best paraphrase for querying a given fact from a LM.\n0 5 10 25 50 100 250 500\nNumber of facts for finetuning to query variant\nS was born in O\nS was born in O .\nS was born at O\nS was born in O ...\nS was born in O !\nS , who was born in O\nS , who is from O\nS was born in O , but they did not die there\nS ( born in 1970 in O )\nS is from O\nWhere was S born ? O\nS ( b. 1970 , O )\nS was born in O , but died somewhere else\nIt is true that S was born in O\nIt is known that S was born in O\nAccording to Wikidata , S was born in O\nborn in O , S is a\nQuery variants\n0.98 0.98 0.97 0.93 0.98 0.98 0.98 0.98\n0.97 0.98 0.97 0.97 0.98 0.98 0.98 0.98\n0.98 0.98 0.98 0.96 0.97 0.98 0.98 0.98\n0.95 0.98 0.97 0.94 0.98 0.98 0.98 0.98\n0.90 0.98 0.98 0.96 0.96 0.98 0.98 0.98\n0.91 0.84 0.96 0.88 0.97 0.98 0.98 0.98\n0.35 0.97 0.96 0.97 0.98 0.97 0.98 0.98\n0.10 0.81 0.82 0.89 0.94 0.97 0.98 0.98\n0.13 0.39 0.85 0.92 0.98 0.98 0.98 0.98\n0.20 0.27 0.75 0.75 0.96 0.96 0.97 0.97\n0.32 0.20 0.45 0.45 0.64 0.91 0.97 0.98\n0.05 0.02 0.04 0.58 0.77 0.96 0.97 0.98\n0.03 0.02 0.02 0.23 0.94 0.97 0.98 0.98\n0.07 0.05 0.06 0.29 0.66 0.82 0.91 0.92\n0.08 0.05 0.16 0.12 0.57 0.79 0.91 0.94\n0.03 0.01 0.04 0.13 0.53 0.82 0.90 0.94\n0.08 0.04 0.03 0.03 0.33 0.45 0.83 0.90\nRoBERTa with pretraining\n0 5 10 25 50 100 250 500\nNumber of facts for finetuning to query variant\n0.98 0.97 0.98 0.97 0.98 0.98 0.98 0.98\n0.67 0.78 0.86 0.94 0.97 0.97 0.97 0.98\n0.96 0.95 0.96 0.94 0.98 0.98 0.98 0.98\n0.93 0.91 0.96 0.93 0.97 0.98 0.97 0.98\n0.92 0.87 0.95 0.92 0.97 0.98 0.98 0.98\n0.33 0.16 0.29 0.38 0.55 0.87 0.91 0.95\n0.03 0.01 0.02 0.02 0.03 0.40 0.77 0.92\n0.08 0.01 0.01 0.01 0.21 0.50 0.93 0.96\n0.34 0.11 0.27 0.43 0.60 0.77 0.94 0.97\n0.03 0.02 0.02 0.14 0.31 0.79 0.91 0.94\n0.02 0.01 0.01 0.01 0.01 0.01 0.02 0.04\n0.04 0.01 0.05 0.03 0.08 0.42 0.76 0.86\n0.07 0.00 0.01 0.04 0.09 0.29 0.81 0.85\n0.01 0.00 0.00 0.01 0.01 0.02 0.03 0.02\n0.01 0.00 0.00 0.01 0.01 0.01 0.02 0.02\n0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01\n0.03 0.01 0.01 0.01 0.01 0.02 0.03 0.04\nRoBERTa without pretraining\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy of recalling entity O\n0 5 10 25 50 100 250 500\nNumber of facts for finetuning to query variant\nS died in O\nS died in O ...\nS died at O\nS died in O .\nS died in O !\nS , who died in O\nS died in O , but they were not born there\nS died in O , but was born somewhere else\nS ( died 2010 in O )\nS ( d. 2010 , O )\nIt is true that S died in O\nIt is known that S died in O\nAccording to Wikidata , S died in O\nWhere did S die ? O\nAfter their death in O , S was\nS , who spent the last days of their life in O\nS spent the last days of their life in O\nQuery variants\n0.98 0.98 0.94 0.94 0.97 0.97 0.98 0.97\n0.96 0.97 0.94 0.93 0.97 0.97 0.98 0.98\n0.97 0.97 0.89 0.95 0.97 0.97 0.98 0.98\n0.91 0.97 0.96 0.96 0.97 0.97 0.97 0.97\n0.88 0.97 0.97 0.94 0.91 0.97 0.98 0.98\n0.60 0.76 0.91 0.95 0.95 0.94 0.97 0.97\n0.16 0.43 0.79 0.76 0.91 0.92 0.97 0.98\n0.18 0.17 0.78 0.84 0.75 0.96 0.98 0.97\n0.11 0.28 0.60 0.72 0.97 0.97 0.97 0.98\n0.02 0.00 0.00 0.00 0.94 0.96 0.97 0.98\n0.15 0.16 0.19 0.25 0.46 0.71 0.88 0.91\n0.19 0.13 0.07 0.21 0.50 0.64 0.82 0.91\n0.06 0.03 0.11 0.09 0.43 0.73 0.90 0.93\n0.01 0.00 0.00 0.01 0.02 0.08 0.94 0.96\n0.01 0.00 0.00 0.00 0.02 0.01 0.83 0.90\n0.02 0.00 0.02 0.00 0.01 0.00 0.07 0.97\n0.01 0.00 0.00 0.00 0.00 0.01 0.96 0.11\nRoBERTa with pretraining\n0 5 10 25 50 100 250 500\nNumber of facts for finetuning to query variant\n0.98 0.98 0.97 0.93 0.94 0.97 0.97 0.97\n0.92 0.92 0.94 0.93 0.91 0.96 0.97 0.98\n0.96 0.95 0.97 0.96 0.96 0.97 0.98 0.98\n0.64 0.82 0.92 0.83 0.96 0.95 0.96 0.98\n0.91 0.89 0.92 0.95 0.97 0.97 0.97 0.98\n0.37 0.22 0.25 0.40 0.34 0.70 0.90 0.93\n0.03 0.01 0.01 0.00 0.10 0.45 0.87 0.94\n0.02 0.01 0.01 0.02 0.03 0.05 0.32 0.84\n0.17 0.09 0.26 0.32 0.55 0.71 0.90 0.94\n0.07 0.01 0.04 0.12 0.13 0.47 0.68 0.87\n0.03 0.00 0.02 0.01 0.03 0.02 0.04 0.07\n0.03 0.01 0.03 0.02 0.04 0.03 0.04 0.07\n0.01 0.00 0.00 0.01 0.01 0.02 0.04 0.05\n0.03 0.01 0.02 0.02 0.04 0.03 0.05 0.11\n0.00 0.00 0.00 0.01 0.00 0.00 0.03 0.03\n0.03 0.02 0.01 0.02 0.03 0.08 0.27 0.60\n0.03 0.02 0.02 0.01 0.04 0.02 0.18 0.63\nRoBERTa without pretraining\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy of recalling entity O\nFigure 9: Transfer from memorized statements (top: “S was born in O”, bottom: “S died in O”) to query variants.\nthe question format (11th row) allows zero-shot\ntransfer with 32% accuracy. For the remaining\nparaphrases, e.g., those with parentheticals or the\ndistractor died, zero-shot transfer is poor, with ac-\ncuracies ranging from 3% to 13%.\nA clear trend is visible: transfer works best for\nsimilar statements and worst for dissimilar ones.\nTo quantify this trend, we compute a representation\nof a statement template by averaging over its 100k\nmean-pooled, LM-encoded statements, and then\nmeasure the Euclidean distance between the origi-\nnal template representation and the representation\nof a query variant template. Correlating Euclidean\ndistance and accuracy of zero-shot transfer obtains\na Pearson coefﬁcient of −0.68, indicating a strong\nnegative correlation. That is, transfer tends to work\nwell for paraphrased queries the LM deems similar\nto the originally memorized statement, but fails if\nthe LM’s representation of a query is too dissimilar\nto its representation of the original statement. This\ntrend is also reﬂected in the ﬁnetuning setting, with\nless similar variants requiring up to 500 instances\nuntil the model achieves 90 percent accuracy (last\nrow), while transfer to more similar variants works\nwell after ﬁnetuning on 5 to 50 target instances.\nWhen using RoBERTa without pretraining to\nmemorize statements, knowledge transfer to query\nvariants is much worse. While transfer still works\nfor the most similar variants (right, top rows), less\nsimilar variants require more ﬁnetuning compared\nto pretrained RoBERTa (right, middle rows). Trans-\nfer does not work for the least similar variants,\nwith accuracies as low as 1 to 4 percent even after\nﬁnetuning with 500 instances (right, bottom rows).\nSimilar results for control statements are shown in\nFig. 9 (bottom). We take these results as evidence\nthat pretraining enables LMs to handle paraphrased\nqueries and that LMs can memorize facts beyond\nmere rote memorization and generic association.\n6 Limitations and Conclusions\nLimitations. This work is not without limitations.\nWe only use one KB in our experiments. Arguably,\nas the largest publicly available source of world\nknowledge, Wikidata is the most promising re-\nsource for equipping LMs with such knowledge,\nbut attempts to store a KB with different struc-\nture might result in different outcomes, since some\ntypes of graphs are easier to memorize for a LM\nthan others (See Appx. G).\nWhile we use language like “ train a LM to\nmemorize statements” for simplicity throughout\nthis work, what we do in case of pretrained LMs\nis more akin to adaptive pretraining (Gururangan\net al., 2020). It is possible that integrating entity su-\npervision directly into LM pretraining (F´evry et al.,\n2020) allows more efﬁcient fact storage.\nOur analysis was focused on entity representa-\ntions and ignored the question of how to represent\nrelation predicates or entire relation triples. Here,\nrelation learning (Baldini Soares et al., 2019) and\nLM pretraining on fact-aligned corpora (Elsahar\net al., 2018) are avenues for future work.\nFinally, we formulated the LM-as-KB paradigm\nin terms of storing and retrieving relation triples.\nWhile structured KBs such as Wikidata consist of\nsuch triples and hence our experiments showing\nstorage and retrieval of triples LMs are sufﬁcient as\na proof-of-concept in principle, structured KBs al-\nlow more complex queries than the ones considered\nhere, such as 1-to-n relations, multihop inference,\nqueries involving numerical ranges, or facts quali-\nﬁed by time and location (Hoffart et al., 2013).\nConclusions. We gave a positive answer to Petroni\net al. (2019)‘s question if language models can\nserve as knowledge bases. Arguing that treating\nLMs as KBs requires representing a large number\nof entities, storing a large number of facts, and the\nability to query a fact with a variety of queries,\nwe showed that current LM architectures fulﬁll\nthese requirements when extended with a compo-\nnent for representing entities. In addition to the\nability to handle paraphrased queries, we envision\nfurther beneﬁts from the LM-as-KB paradigm. For\nexample, the fact-memorization and paraphrase-\nﬁnetuning setting introduced in Section 5 allows\nprecise control over which facts a LM learns.\n7 Acknowledgments\nWe thank the anonymous reviewers for helpful feed-\nback. This work was supported by a Google Fo-\ncused Research Award.\nReferences\nSungjin Ahn, Heeyoul Choi, Tanel P ¨arnamaa, and\nYoshua Bengio. 2016. A neural knowledge language\nmodel. CoRR, abs/1608.00318.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey\nLing, and Tom Kwiatkowski. 2019. Matching the\nblanks: Distributional similarity for relation learn-\ning. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2895–2905, Florence, Italy. Association for\nComputational Linguistics.\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5185–5198, Online. As-\nsociation for Computational Linguistics.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nJon Louis Bentley. 1975. Multidimensional binary\nsearch trees used for associative searching. Commu-\nnications of the ACM, 18(9):509–517.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA. Association for Computational Lin-\nguistics.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDur´an, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Proceedings of the 26th Interna-\ntional Conference on Neural Information Processing\nSystems - Volume 2, NIPS’13, page 2787–2795, Red\nHook, NY , USA. Curran Associates Inc.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. CoRR, abs/2005.14165.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1870–\n1879, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nSam Coppens, Miel Vander Sande, Ruben Verborgh,\nErik Mannens, and Rik Van de Walle. 2013. Rea-\nsoning over SPARQL. In LDOW.\nLeon Derczynski, Eric Nichols, Marieke van Erp, and\nNut Limsopatham. 2017. Results of the WNUT2017\nshared task on novel and emerging entity recogni-\ntion. In Proceedings of the 3rd Workshop on Noisy\nUser-generated Text, pages 140–147, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nTim Dettmers, Minervini Pasquale, Stenetorp Pon-\ntus, and Sebastian Riedel. 2018. Convolutional 2d\nknowledge graph embeddings. In Proceedings of\nthe 32th AAAI Conference on Artiﬁcial Intelligence ,\npages 1811–1818.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJeffrey L. Elman. 1990. Finding structure in time. Cog-\nnitive Science, 14(2):179–211.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-REx: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018) , Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nThibault F´evry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. CoRR, abs/2004.07202.\nMatthew Francis-Landau, Greg Durrett, and Dan Klein.\n2016. Capturing semantic similarity for entity link-\ning with convolutional neural networks. In Proceed-\nings of the 2016 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1256–1261, San Diego, California. Association for\nComputational Linguistics.\nDaniel Gillick, Sayali Kulkarni, Larry Lansing,\nAlessandro Presta, Jason Baldridge, Eugene Ie, and\nDiego Garcia-Olano. 2019. Learning dense repre-\nsentations for entity retrieval. In Proceedings of\nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL) , pages 528–537, Hong\nKong, China. Association for Computational Lin-\nguistics.\nGoogle. 2013. Google relation extraction corpus.\nhttps://ai.googleblog.com/2013/04/\n50000-lessons-on-how-to-read-relation.\nhtml. Accessed: 2020-10-07.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. CoRR,\nabs/2002.08909.\nGary G Hendrix, Earl D Sacerdoti, Daniel Sagalowicz,\nand Jonathan Slocum. 1978. Developing a natural\nlanguage interface to complex data. ACM Transac-\ntions on Database Systems (TODS), 3(2):105–147.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nM¨uller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4320–4333, Online. Association for\nComputational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Comput. ,\n9(8):1735–1780.\nJohannes Hoffart, Yasemin Altun, and Gerhard\nWeikum. 2014. Discovering emerging entities with\nambiguous names. In Proceedings of the 23rd inter-\nnational conference on World wide web, WWW 2014,\nSeoul, South Korea, pages 385–396.\nJohannes Hoffart, Fabian M Suchanek, Klaus\nBerberich, and Gerhard Weikum. 2013. Yago2: A\nspatially and temporally enhanced knowledge base\nfrom wikipedia. Artiﬁcial Intelligence, 194:28–61.\nJohannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-\ndino, Hagen F¨urstenau, Manfred Pinkal, Marc Span-\niol, Bilyana Taneva, Stefan Thater, and Gerhard\nWeikum. 2011. Robust disambiguation of named en-\ntities in text. In Proceedings of the 2011 Conference\non Empirical Methods in Natural Language Process-\ning, pages 782–792, Edinburgh, Scotland, UK. Asso-\nciation for Computational Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nFilip Ilievski, Piek V ossen, and Stefan Schlobach. 2018.\nSystematic study of long tail phenomena in entity\nlinking. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n664–674, Santa Fe, New Mexico, USA. Association\nfor Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2017.\nBillion-scale similarity search with gpus. CoRR,\nabs/1702.08734.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings .\nOpenReview.net.\nNikolaos Kolitsas, Octavian-Eugen Ganea, and\nThomas Hofmann. 2018. End-to-end neural entity\nlinking. In Proceedings of the 22nd Conference\non Computational Natural Language Learning ,\npages 519–529, Brussels, Belgium. Association for\nComputational Linguistics.\nSachin Kumar and Yulia Tsvetkov. 2019. V on Mises-\nFisher loss for training sequence to sequence models\nwith continuous outputs. In Proc. of ICLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt\nGardner, and Sameer Singh. 2019. Barack’s wife\nHillary: Using knowledge graphs for fact-aware lan-\nguage modeling. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5962–5971, Florence, Italy. Associa-\ntion for Computational Linguistics.\nFederico L ´opez, Benjamin Heinzerling, and Michael\nStrube. 2019. Fine-grained entity typing in hyper-\nbolic space. In Proceedings of the 4th Workshop\non Representation Learning for NLP (RepL4NLP-\n2019), pages 169–180, Florence, Italy. Association\nfor Computational Linguistics.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal probabilistic neural network language model. In\nAistats, volume 5, pages 246–252.\nMaximilian Nickel, V olker Tresp, and Hans-Peter\nKriegel. 2011. A three-way model for collective\nlearning on multi-relational data. In ICML, pages\n809–816. Omnipress.\nYusuke Oda, Philip Arthur, Graham Neubig, Koichiro\nYoshino, and Satoshi Nakamura. 2017. Neural ma-\nchine translation via binary code prediction. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 850–860, Vancouver, Canada. Asso-\nciation for Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena.\n2014. Deepwalk: Online learning of social represen-\ntations. In Proceedings of the 20th ACM SIGKDD\ninternational conference on Knowledge discovery\nand data mining, pages 701–710.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54, Hong Kong, China. Associ-\nation for Computational Linguistics.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vassilis Plachouras, Tim\nRockt¨aschel, and Sebastian Riedel. 2020. KILT: a\nbenchmark for knowledge intensive language tasks.\nCoRR, abs/2009.02252.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nPouya Pezeshkpour, Liyan Chen, and Sameer Singh.\n2018. Embedding multimodal relational data for\nknowledge base completion. In Proceedings of the\n2018 Conference on Empirical Methods in Natu-\nral Language Processing , pages 3208–3218, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Sch ¨utze.\n2019. E-BERT: Efﬁcient-yet-effective entity embed-\ndings for BERT. CoRR, abs/1911.03681.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nJonathan Raiman and Olivier Raiman. 2018. Deep-\ntype: Multilingual entity linking by neural type sys-\ntem evolution. In Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artiﬁcial In-\ntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artiﬁcial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 5406–5413. AAAI Press.\nSebastian Riedel, Limin Yao, Andrew McCallum, and\nBenjamin M. Marlin. 2013. Relation extraction with\nmatrix factorization and universal schemas. In Pro-\nceedings of the 2013 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n74–84, Atlanta, Georgia. Association for Computa-\ntional Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? CoRR, abs/2002.08910.\nClaude E. Shannon. 1948. A mathematical theory of\ncommunication. Bell Syst. Tech. J., 27(3):379–423.\nDaniil Sorokin and Iryna Gurevych. 2018. Model-\ning semantics with gated graph neural networks for\nknowledge base question answering. In Proceed-\nings of the 27th International Conference on Compu-\ntational Linguistics, pages 3306–3317. Association\nfor Computational Linguistics.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn Proceedings of the 27th International Conference\non Neural Information Processing Systems - Vol-\nume 2, NIPS’14, page 3104–3112, Cambridge, MA,\nUSA. MIT Press.\nRyo Takahashi, Ran Tian, and Kentaro Inui. 2018. In-\nterpretable and compositional relation learning by\njoint training with an autoencoder. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2148–2159, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nTh´eo Trouillon, Johannes Welbl, Sebastian Riedel,´Eric\nGaussier, and Guillaume Bouchard. 2016. Complex\nembeddings for simple link prediction. In Proceed-\nings of the 33rd International Conference on Inter-\nnational Conference on Machine Learning - Volume\n48, page 2071–2080. JMLR.org.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant,\nMatt Haberland, Tyler Reddy, David Courna-\npeau, Evgeni Burovski, Pearu Peterson, Warren\nWeckesser, Jonathan Bright, St ´efan J. van der Walt,\nMatthew Brett, Joshua Wilson, K. Jarrod Millman,\nNikolay Mayorov, Andrew R. J. Nelson, Eric Jones,\nRobert Kern, Eric Larson, CJ Carey, ˙Ilhan Po-\nlat, Yu Feng, Eric W. Moore, Jake Vand erPlas,\nDenis Laxalde, Josef Perktold, Robert Cimrman,\nIan Henriksen, E. A. Quintero, Charles R Harris,\nAnne M. Archibald, Ant ˆonio H. Ribeiro, Fabian Pe-\ndregosa, Paul van Mulbregt, and SciPy 1.0 Contribu-\ntors. 2020. SciPy 1.0: Fundamental Algorithms for\nScientiﬁc Computing in Python. Nature Methods,\n17:261–272.\nDenny Vrande ˇci´c and Markus Kr ¨otzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78–85.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2019. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. CoRR, abs/1912.09637.\nIkuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and\nYoshiyasu Takefuji. 2016. Joint learning of the em-\nbedding of words and entities for named entity dis-\nambiguation. In Proceedings of The 20th SIGNLL\nConference on Computational Natural Language\nLearning, pages 250–259. Association for Compu-\ntational Linguistics.\nZichao Yang, Phil Blunsom, Chris Dyer, and Wang\nLing. 2017. Reference-aware language models. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n1850–1859, Copenhagen, Denmark. Association for\nComputational Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nA Overview: world knowledge in natural language processing\nParadigm / Task Input Output Models and objectives\nLanguage modeling Text Text Next word prediction (Shannon, 1948; Elman, 1990; Bengio\net al., 2003), masked token prediction (Devlin et al., 2019)\nLM-as-KB? Text Text / single-token\nentity name\nClosed-book QA (LAMA probe, Petroni et al., 2019)\nSequence-to-sequence Text Text Text-to-text transformer (T5, Raffel et al., 2019), closed-book\nQA (Roberts et al., 2020)\nRetrieval Text Text, answer span Answer-span selection (Chen et al., 2017), retrieval-augmented\nLM (Guu et al., 2020), open-book QA\nEntity replacement Text, entity men-\ntion spans\nText Detecting replaced entity mentions (Xiong et al., 2019)\nEntity linking (EL) Text, entity men-\ntion spans\nTarget entity AIDA (Hoffart et al., 2011), neural EL (Francis-Landau et al.,\n2016; Kolitsas et al., 2018)\nEntity embeddings Text, entity men-\ntion spans\nEntity embeddings Joint embedding of entities and text (Yamada et al., 2016)\nLM with entity embed-\ndings\nText, linked en-\ntity mentions, en-\ntity embeddings\nText ERNIE (Zhang et al., 2019), E-BERT (Poerner et al., 2019)\nLM with integrated EL Text, entity embed-\ndings\nText KnowBert (Peters et al., 2019)\nLM-as-KB (this work) Natural language\nquery\nTarget entity Fact memorization, paraphrased queries, closed-book QA\nKnowledge-aware LM Text, knowledge\n(sub)graph\nTarget entity, text Neural Knowledge LM (Ahn et al., 2016), Reference-aware LM\n(Yang et al., 2017), Knowledge graph LM (Logan et al., 2019)\nSemantic parsing natural language\nquery\nmeaning represen-\ntation, target entity\nSEMPRE (Berant et al., 2013), GNNs for KBQA (Sorokin and\nGurevych, 2018)\nUniversal Schema relation triples, text\npatterns\nentity tuple and re-\nlation embeddings\nMatrix factorization (Riedel et al., 2013)\nKnowledge graph embed-\ndings\nrelation triples node and edge em-\nbeddings\nLink prediction; RESCAL (Nickel et al., 2011), TransE (Bor-\ndes et al., 2013), ComplexE (Trouillon et al., 2016), ConvE\n(Dettmers et al., 2018)\nGraph neural networks nodes, node fea-\ntures, edges\nnode embeddings DeepWalk (Perozzi et al., 2014), graph neural networks (Kipf\nand Welling, 2017)\nKnowledge graphs nodes, edges nodes, edges Storage and retrieval, SQL/SPARQL queries, symbolic reason-\ning (Coppens et al., 2013)\nTable 2: Approaches for using world knowledge in natural language processing, ranging from unstructured, purely\ntext-based approaches (top), over approaches that mix text and structured KBs to varying degrees (middle), to\napproaches operating on structured KBs (bottom).\nB Templates for generating English statements from Wikidata relations\nC Random sample of English statements generated from Wikidata relations\n• The Underfall Yard is followed by English Electric Part One\n• Gazi Beg is a child of Farrukh Yassar\n• 2011 European Rowing Championships is followed by 2012 European Rowing Championships\n• 2009 Yemeni tourist attacks is located in Shibam\n• George Best – A Tribute is performed by Peter Corry\n• Gamecock Media Group is owned by SouthPeak Games\n• 2017–18 Shefﬁeld Wednesday F.C. season is followed by 2018–19 Shefﬁeld Wednesday F.C. season\n• Nennslingen is located in or next to body of water Anlauter\n• 2013–14 Xavier Musketeers men’s basketball team is followed by 2014–15 Xavier Musketeers men’s basketball team\n• Shock to the System is a part of Cyberpunk\n• 1918–19 Ohio Bobcats men’s basketball team follows 1917–18 Ohio Bobcats men’s basketball team\n• Ramya Krishnan has the spouse Krishna Vamsi\n• The Cloud Minders follows The Way to Eden\n• Curve is followed by Somethingness\n• Austin Road is named after John Gardiner Austin\n• Dione juno has the parent taxon Dione\n• Spirit Bound Flesh is followed by The Wake\n• Sidnei da Silva has the given name Sidnei\n• In Memoriam is performed by Living Sacriﬁce\n• Tracks and Traces is followed by Live 1974\n• Grumman Gulfstream I is operated by Phoenix Air\n• Timeline of Quebec history has the part Timeline of Quebec history (1982–present)\n• Edwin C. Johnson held the position of Lieutenant Governor of Colorado\n• Here Comes the Summer follows Jimmy Jimmy\n• In Custody is screenwritten by Anita Desai\n• Bertie Charles Forbes is the father of Malcolm Forbes\n• The Mambo Kings has the cast member Helena Carroll\n• Carnival of Souls has the cast member Art Ellison\n• 1995–96 Philadelphia Flyers season is followed by 1996–97 Philadelphia Flyers season\n• John Harley is the father of Edward Harley, 5th Earl of Oxford and Earl Mortimer\n• Jane Fellowes, Baroness Fellowes has the spouse Robert Fellowes, Baron Fellowes\n• Francis of Assisi is buried in Basilica of San Francesco d’Assisi\n• 1990 Maharashtra Legislative Assembly election follows 1985 Maharashtra Legislative Assembly election\n• Makabana Airport is named after Makabana\n• Calvin Booth was born in Reynoldsburg\n• The Telltale Head is followed by Life on the Fast Lane\n• Alajos Keser ˝u is a sibling of Ferenc Keser˝u\n• Long An contains the administrative territorial entity Ch ˆau Th`anh\nD Hyperparameter settings and replicability statement\nE Embeddings of Wikidata entities\nFigure 10: Training embeddings of Wikidata entities with feature-speciﬁc autoencoders.\nWe train the embedding of a given Wikidata entity by collecting its features from, encoding each\nfeature to obtain a dense feature representation, and then concatenating feature representations. For textual\nfeatures, we use RoBERTa-base as encoder and train corresponding decoders in a standard sequence-to-\nsequence auto-encoding setup. For quantities, we select the 100 most common quantity types to obtain a\nﬁxed-sized representation and then follow a standard auto-encoding setup. Similarly we obtain a ﬁxed-size\nentity type representation by selecting the 1000 most common entity types. The concatenated feature-\nrepresentations are then compressed to embedding size d, using a separate autoencoder. Preliminary\nexperiments with embedding sizes d ∈{64, 128, 192, 256}showed similar memorization accuracies for\nall d, but faster convergence for smaller sizes. We set d = 64in our main experiments.\nID Template\nP31 S is an instance of O\nP106 S has the occupation O\nP17 S belongs to the country O\nP131 S is located in the administrative territorial entity O\nP27 S is citizen of O\nP47 S shares a border with O\nP19 S was born in O\nP161 S has the cast member O\nP421 S is located in time zone O\nP166 S received the award O\nP54 S is a member of the sports team O\nP20 S died in O\nP136 S has the genre O\nP69 S was educated at O\nP1412 S is a language spoken, written or signed in O\nP190 S is a twinned administrative body of O\nP641 S participates in the sport O\nP150 S contains the administrative territorial entity O\nP463 S is a member of O\nP735 S has the given name O\nP1343 S is described by source O\nP361 S is a part of O\nP159 the headquarters of S are located in O\nP1344 S is participant of O\nP495 S has the country of origin O\nP39 S held the position of O\nP910 S has the main category O\nP105 S has the taxon rank O\nP527 S has the part O\nP108 S is employed by O\nP279 S is a subclass of O\nP171 S has the parent taxon O\nP140 S has the religion O\nP407 S is in the O language\nP1303 S plays the instrument O\nP1411 S has been nominated for O\nP102 S is a member of political party O\nP3373 S is a sibling of O\nP1376 S is the capital of O\nP509 S died because of O\nP937 S works in O\nP264 S was produced by the record label O\nP119 S is buried in O\nP138 S is named after O\nP530 S has diplomatic relations with O\nP40 S is a child of O\nP155 S follows O\nP276 S is located in O\nP156 S is followed by O\nP36 S has the capital O\nP1196 S has the manner of death O\nP127 S is owned by O\nP101 S works in the ﬁeld O\nP607 S participated in the conﬂict O\nP364 S is a ﬁlm or TV show with the original language O\nP6379 S has works in the collection O\nP1346 S is a winner of the O\nP22 S is the father of O\nP137 S is operated by O\nP413 S plays the position O\nP26 S is spouse of O\nP1830 S is owner of O\nP1454 S has the legal form O\nP206 S is located in or next to body of water O\nP710 S is a participant of O\nID Template\nP1441 S is present in the work O\nP1532 S represents O when playing sport O\nP86 S was composed by O\nP840 S is set in the location O\nP172 S belongs to the ethnic group O\nP175 S is performed by O\nP57 S is directed by O\nP1889 S is different from O\nP162 S is produced by O\nP118 S belongs to the league O\nP58 S is screenwritten by O\nP551 S has the residence O\nP103 S has the native language O\nP2789 S connects with O\nP750 S has the distributor O\nP725 S is voiced by O\nP272 S is produced by the company O\nP112 S was founded by O\nP452 S belongs to the industrial sector O\nP81 S is connected to line O\nP97 S has noble title O\nP740 S formed in the location O\nP360 S is a list of O\nP793 S is associated with the signiﬁcant event O\nP915 S was ﬁlmed at O\nP410 S has military rank O\nP1001 S applies to the jurisdiction of O\nP30 S is located on the continent O\nP749 S has parent organization O\nP1435 S has heritage designation O\nP53 S belongs to the family of O\nP400 S was developed for the platform O\nP921 S has the main subject O\nP37 S has the ofﬁcial language O\nP734 S has the family name O\nTable 3: Templates used to generate English statements from Wikidata relations.\nEntity representation Architecture Hyper-param. Value\nSymbolic LSTM layers 2\nhidden size 256, 1024\ndropout 0.0\nlearning rate 0.001\nlr-scheduler plateau\noptimizer Adam\nTransformer model name RoBERTa-base\nlayers 12\nhidden size 768\nlearning rate 5e-5\nlr-scheduler plateau\noptimizer Adam\nSurface form LSTM layers (enc) 2\nhidden size (enc) 256, 1024\nlayers (dec) 2\nhidden size (dec) 256, 1024\nlearning rate 0.001\nlr-scheduler plateau\noptimizer Adam\nTransformer model name (enc) RoBERTa-base\nlayers (enc) 12\nhidden size (enc) 768\ndropout 0.0\nmodel name (dec) random init.\nlayers (dec) 12\nhidden size (dec) 768\nlearning rate 5e-4\nlr-scheduler inverse sqrt\noptimizer Adam\nContinuous LSTM layers 2\nhidden size 256, 1024\ndropout 0.0\nlearning rate 0.001\nlr-scheduler plateau\noptimizer Adam\nentity emb. dim 64\nentity emb. trainable no\nTransformer model name RoBERTa-base\nlayers 12\nhidden size 768\nlearning rate 5e-5\nlr-scheduler plateau\noptimizer Adam\nentity emb. dim 64\nentity emb. trainable no\nTable 4: Hyperparameter settings used in our experiments.\nF Things that didn’t work\nF.1 Hierarchical entity representation with\nbinary codes\nSince imposing a hierarchy is a common method\nfor dealing with large vocabulary sizes (Morin and\nBengio, 2005) in general, and large inventories of\nentities and entity types in particular (Raiman and\nRaiman, 2018; L ´opez et al., 2019), we created a\nhierarchy of all entities in Wikidata, using a given\nentity’s position in this hierarchy as training sig-\nnal. Speciﬁcally, we created the entity hierarchy\nby ﬁtting a KD-tree (Bentley, 1975; Virtanen et al.,\n2020) with leaf size 1 over pretrained entity embed-\ndings, thereby obtaining a binary partitioning of\nthe embedding space in which each ﬁnal partition\ncontains exactly one entity embedding. The path\nfrom the KD-tree’s root to a leaf can be represented\nas a binary code, which we use as training signal\n(Oda et al., 2017). Memorization accuracy of world\nknowledge facts with object entities represented in\nthe form of these binary codes was substantially\nlower compared to the three approaches described\nin the main part of this work.\nF.2 Training entity embeddings with negative\nsampling\nInstead of using ﬁxed, pretrained entity embed-\ndings as training signal, we experimented with\nrandomly initialized embeddings that are updated\nduring training, using between 1 and 50 in-batch\nnegative samples, which is a standard method in the\nknowledge base embedding literature (Bordes et al.,\n2013) and has been used successfully for entity re-\ntrieval (Gillick et al., 2019). However, compared\nto using ﬁxed, pretrained entity embeddings with-\nout negative sampling, we observed lower memo-\nrization accuracies and slower convergence in our\nexperiments.\nF.3 Updating pretrained entity embeddings\nduring training\nInstead of using ﬁxed entity embeddings, we tried\nupdating them during training with in-batch nega-\ntive sampling. This increased the number of train-\nable parameters, memory usage, and training time,\nbut did not lead to higher memorization accuracies.\nF.4 Continuous representation with\nEuclidean distance loss\nInstead of normalizing entity embeddings to the\nunit hypersphere and training with cosine loss,\nwe experimented with predicting the original pre-\ntrained entity embeddings and using the Euclidean\ndistance as loss. Compared to using spherical en-\ntity embeddings as prediction targets, we observed\nslower convergence and lower memorization accu-\nracies.\nG Impact of graph type on memorizability\nFigure 11: Impact of graph type on a model’s ability to memorize the graph. We consider two types of random\ngraphs, namely a uniform (Erdos-Renyi) graph, and a scale-free (Barabasi) graph. We interpret graph edges as\nrelation triples in a knowledge graph and train models to predict the relation object, given subject and predicate,\nuntil memorization accuracy reaches 99 percent. For a given number of model parameters, we gradually increase\nthe number of relation triples to memorizes and record the maximum number of relation triples memorized for\nthis number of parameters. We compare an LSTM, as well as a bilinear KB embedding (DistMult). For a given\nparameter budget, models are able to memorize more triples from a Erdos-Renyi graph (blue) than from a Barabasi\ngraph, indicating that the latter is more difﬁcult to memorize.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8279261589050293
    },
    {
      "name": "Complement (music)",
      "score": 0.7071389555931091
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.6985684633255005
    },
    {
      "name": "Security token",
      "score": 0.5502908825874329
    },
    {
      "name": "Natural language processing",
      "score": 0.4798583388328552
    },
    {
      "name": "Natural language",
      "score": 0.4563274383544922
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4129689037799835
    },
    {
      "name": "Question answering",
      "score": 0.4106976389884949
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Complementation",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Phenotype",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": []
}