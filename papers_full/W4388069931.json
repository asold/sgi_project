{
  "title": "Performance of Large Language Models (LLMs) in Providing Prostate Cancer Information",
  "url": "https://openalex.org/W4388069931",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1740246079",
      "name": "Ahmed Alasker",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4379314812",
      "name": "Seham Alsalamah",
      "affiliations": [
        "King Saud bin Abdulaziz University for Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5093148814",
      "name": "Nada Alshathri",
      "affiliations": [
        "King Saud bin Abdulaziz University for Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5111065042",
      "name": "Nura Almansour",
      "affiliations": [
        "King Saud bin Abdulaziz University for Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5093148815",
      "name": "Faris Alsalamah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2232343540",
      "name": "Mohammad Alghafees",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2741221034",
      "name": "Mohammad Alkhamees",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2799405527",
      "name": "Bader Alsaikhan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4378472917",
    "https://openalex.org/W4324304837",
    "https://openalex.org/W4319062614",
    "https://openalex.org/W2937483840",
    "https://openalex.org/W4295080856",
    "https://openalex.org/W2736868680",
    "https://openalex.org/W4366447635",
    "https://openalex.org/W4386117408",
    "https://openalex.org/W4386200227",
    "https://openalex.org/W4386019728",
    "https://openalex.org/W4385380523",
    "https://openalex.org/W4382774929",
    "https://openalex.org/W4386776401",
    "https://openalex.org/W4386753580"
  ],
  "abstract": "<title>Abstract</title> Prostate cancer, the second most common cancer in men worldwide, is highly complex regarding diagnosis and management. Hence, patients often seek knowledge through additional resources, including AI chatbots such as Generative Pre-trained Transformers (ChatGPT) and Google Bard. This study aimed to evaluate the performance of LLMs in providing educational content on prostate cancer. Common patient questions about prostate cancer were collected from reliable educational websites and evaluated for accuracy, comprehensiveness, readability, and stability by two independent board-certified urologists, with a third resolving discrepancies. Accuracy was measured on a 3-point scale, comprehensiveness on a 5-point Likert scale, and readability using the Flesch Reading Ease (FRE) Score and Flesch–Kincaid FK Grade Level. A total of 52 questions on general knowledge, diagnosis, treatment, and prevention of prostate cancer were provided to three LLMs. Although there was no significant difference in the overall accuracy of LLMs, ChatGPT demonstrated superiority among the LLMs in the context of general knowledge of prostate cancer (p = 0.018). ChatGPT Plus achieved higher overall comprehensiveness than ChatGPT and Bard (p = 0.028). For readability, Bard generated simpler sentences with the highest FRE score (54.7, p &lt; 0.001) and lowest FK Reading Level (10.2, p &lt; 0.001). ChatGPT and Bard generate accurate, understandable, and easily readable material on prostate cancer. These AI models might not replace healthcare professionals but can assist in patient education and guidance.",
  "full_text": "Page 1/18\nPerformance of Large Language Models (LLMs) in\nProviding Prostate Cancer Information\nAhmed Alasker \nMinistry of National Guard - Health Affairs\nSeham Alsalamah  (  seham1alslamh@gmail.com )\nKing Saud bin Abdulaziz University for Health Sciences\nNada Alshathri \nKing Saud bin Abdulaziz University for Health Sciences\nNura Almansour \nKing Saud bin Abdulaziz University for Health Sciences\nFaris Alsalamah \nMinistry of National Guard - Health Affairs\nMohammad Alghafees \nMinistry of National Guard - Health Affairs\nMohammad AlKhamees \nMinistry of National Guard - Health Affairs\nBader Alsaikhan \nMinistry of National Guard - Health Affairs\nArticle\nKeywords:\nPosted Date: October 31st, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3499451/v1\nLicense:     This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nPage 2/18\nAbstract\nProstate cancer, the second most common cancer in men worldwide, is highly complex regarding\ndiagnosis and management. Hence, patients often seek knowledge through additional resources,\nincluding AI chatbots such as Generative Pre-trained Transformers (ChatGPT) and Google Bard. This\nstudy aimed to evaluate the performance of LLMs in providing educational content on prostate cancer.\nCommon patient questions about prostate cancer were collected from reliable educational websites and\nevaluated for accuracy, comprehensiveness, readability, and stability by two independent board-certi\u0000ed\nurologists, with a third resolving discrepancies. Accuracy was measured on a 3-point scale,\ncomprehensiveness on a 5-point Likert scale, and readability using the Flesch Reading Ease (FRE) Score\nand Flesch–Kincaid FK Grade Level. A total of 52 questions on general knowledge, diagnosis, treatment,\nand prevention of prostate cancer were provided to three LLMs. Although there was no signi\u0000cant\ndifference in the overall accuracy of LLMs, ChatGPT demonstrated superiority among the LLMs in the\ncontext of general knowledge of prostate cancer (p = 0.018). ChatGPT Plus achieved higher overall\ncomprehensiveness than ChatGPT and Bard (p = 0.028). For readability, Bard generated simpler\nsentences with the highest FRE score (54.7, p < 0.001) and lowest FK Reading Level (10.2, p < 0.001).\nChatGPT and Bard generate accurate, understandable, and easily readable material on prostate cancer.\nThese AI models might not replace healthcare professionals but can assist in patient education and\nguidance.\nIntroduction\nLarge Language Models (LLMs) are deep learning algorithms that generate and present text from\ndatabases in a human-like fashion. Generative Pre-trained Transformers (ChatGPT) is a recent large\nlanguage arti\u0000cial intelligence (AI) model. [1] Even though ChatGPT was only recently introduced at the\nend of 2022, it has attracted much interest. ChatGPT can carry out a wide range of natural language\ntasks compared to the prior deep learning AI models. In addition, it can generate chatty responses to user\ninput that resemble human responses based on a wealth of data. [2] Therefore, ChatGPT has the\npotential to help people and communities make educated decisions about their health. [3] Nonetheless,\nChatGPT has shown imperfections in providing medical answers, mainly due to the outdated data from\nSeptember 2021 and before. [4] The current excitement and enthusiasm surrounding arti\u0000cial intelligence\n(AI) large language model chatbots has driven Google to experiment with conversational AI through Bard\nchatbot, released in 2023. It is powered by the Language Model for Dialogue Applications (LaMDA),\ninvented by Google in 2017.\nProstate cancer is the second most common cancer in men worldwide, with an estimated prevalence of\n43% in Saudi Arabia. [5, 6] Prostate cancer patients might present with localized symptoms or advanced\ndisease. Diagnosis of prostate cancer relies on digital rectal examination (DRE), prostate-speci\u0000c antigen\n(PSA) analysis, and prostate biopsy. Management options for prostate cancer are active surveillance,\nradiation therapy, and radical prostatectomy. More severe cases, such as relapses or metastasis, might\nrequire androgen deprivation therapy (ADT), salvage radiotherapy, and chemotherapy. [7] Due to the\nPage 3/18\ncomplexity of prostate cancer diagnosis and management, patients often seek knowledge through\nadditional resources such as AI chatbots; therefore, the performance of these LLMs in providing accurate,\nsu\u0000cient, and comprehensible information on prostate cancer must be evaluated.\nMethods\nCommon questions based on patient education were collected from trusted websites that provide\neducational material on prostate cancer, such as the American Society of Clinical Oncology (ASCO) or\nProstate Cancer UK, and provided to three LLMs (ChatGPT, ChatGPT plus, and Google Bard). The\nquestions targeted general knowledge, diagnosis, treatment, and prevention material on prostate cancer.\nThe factors used to assess the quality of responses were accuracy, comprehensiveness, patient\nreadability, and stability. All responses were generated and recorded on 31/July/2023. For generating text,\nwe used the ChatGPT-3, ChatGPT-4, and Google Bard, available at https://chat.openai.com/chat and\nhttps://bard.google.com/chat website.\nA 3-point scale was used for accuracy: one representing correct, two representing mixed with correct and\nincorrect/outdated data, and three representing completely incorrect. A 5-point Likert scale was used for\ncomprehensiveness of responses, with one for \"very comprehensive\" and \u0000ve for \"very Inadequate.\" For\nreadability, the output answers were analyzed for their sentences, words, syllables per word, and words\nper sentence. Moreover, the Flesch Reading Ease Score and Flesch–Kincaid Grade Level were calculated\nfor each text using the online calculator available at https://charactercalculator.com/\u0000esch-reading-ease/\nwebsite. A higher Flesch Reading Ease Score indicates an easily readable text, while the Flesch–Kincaid\nGrade Level demonstrates the grade-school level necessary to understand the text. [8] Due to the variety\nof responses generated for the same question by the LLMs, the stability of the output text was assessed\nfor a select number of questions. Stability was determined based on the subjective assessment of the\ntwo independent reviewers of whether the second and third answers were accurate compared to the \u0000rst\ngenerated answer. Three responses were generated for 30 questions, and the chat history was cleared\nafter each trial. Two experienced board-certi\u0000ed urologists worked independently to complete the ratings\naccording to the National Comprehensive Cancer Network (NCCN), American Urological Association\n(AUA), and European University Association (EUA) guidelines. [9–11] Discrepancies in grading and\nassessment among the two reviewers were independently reviewed and resolved by a blinded third board-\ncerti\u0000ed urologist.\nStatistical Analysis\nStatistical analysis was carried out using RStudio (R version 4.3.0). We expressed categorical variables\nas frequencies and percentages, including accuracy, comprehensiveness, readability, and stability. The\nstatistical differences between LLMs for those variables were assessed using Pearson's Chi-squared test\nor Fisher's exact test. We used median and interquartile range (IQR) to present numerical variables,\nincluding words, sentences, syllables, word/sentence, syllable/word, FRE score, and FK Reading levels. A\nPage 4/18\nKruskal-Wallis test was applied to explore the statistical differences between the three LLMs regarding\nthe numerical variables. Statistical signi\u0000cance was set at p < 0.05.\nResults\nA total of 52 questions were provided to three LLMs (ChatGPT, ChatGPT plus and Google Bard). For each\nLLM, nine questions were related to general knowledge (17.3%), \u0000ve questions about diagnosis (9.6%), 27\nquestions about treatment (51.9%), and 11 questions about screening and prevention (21.2%).\nAnalysis of the accuracy of different LLMs\nChatGPT achieved correct responses in 82.7% of cases, ChatGPT plus in 78.8%, and Google Bard in\n63.5%, with no signi\u0000cant difference in the overall accuracy between LLMs (p = 0.100). In the context of\ngeneral knowledge questions, there was a statistically signi\u0000cant difference in accuracy among the LLMs\n(p = 0.018, Fig. 1). ChatGPT correctly answered 88.9% of queries, ChatGPT plus 77.8%, and Google Bard\n22.2% (Fig. 2). The accuracy of the diagnosis-related responses showed no signi\u0000cant difference (p > \n0.999), with ChatGPT and Google Bard scoring 100% and ChatGPT plus at 80%. For treatment-related\nquestions, there were no signi\u0000cant differences in accuracy (p = 0.496), with ChatGPT achieving 77.8%\ncorrectness, ChatGPT plus 85.2%, and Google Bard 66.7%. Similarly, in the category of screening and\nprevention, there were no signi\u0000cant differences in accuracy (p = 0.884), with ChatGPT at 81.8%, ChatGPT\nPlus at 63.6%, and Google Bard at 72.7% (Table 1).\nPage 5/18\nTable 1\nAccuracy of different LLMs\nCharacteristic ChatGPT ChatGPT plus Google Bard p-value\nOverall (n = 52)       0.100\nCorrect 43 (82.7%) 41 (78.8%) 33 (63.5%)  \nMixed 8 (15.4%) 11 (21.2%) 17 (32.7%)  \nCompletely incorrect 1 (1.9%) 0 (0.0%) 2 (3.8%)  \nGeneral (n = 9)       0.018\nCorrect 8 (88.9%) 7 (77.8%) 2 (22.2%)  \nMixed 1 (11.1%) 2 (22.2%) 7 (77.8%)  \nCompletely incorrect 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nDiagnosis (n = 5)       > 0.999\nCorrect 5 (100.0%) 4 (80.0%) 5 (100.0%)  \nMixed 0 (0.0%) 1 (20.0%) 0 (0.0%)  \nCompletely incorrect 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nTreatment (n = 27)       0.496\nCorrect 21 (77.8%) 23 (85.2%) 18 (66.7%)  \nMixed 5 (18.5%) 4 (14.8%) 7 (25.9%)  \nCompletely incorrect 1 (3.7%) 0 (0.0%) 2 (7.4%)  \nScreening & Prevention (n = 11)       0.884\nCorrect 9 (81.8%) 7 (63.6%) 8 (72.7%)  \nMixed 2 (18.2%) 4 (36.4%) 3 (27.3%)  \nCompletely incorrect 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nAnalysis of the accuracy of different LLMs in all categories, General knowledge, Diagnosis, Treatment,and Screening & Prevention.\nAnalysis of the comprehensiveness of different LLMs\nThe overall comprehensiveness displayed statistically signi\u0000cant variations among the LLMs (p = 0.028).\nSpeci\u0000cally, ChatGPT Plus achieved a signi\u0000cantly higher proportion of comprehensive responses\n(67.3%) compared to ChatGPT (40.4%) and Google Bard (48.1%). However, no signi\u0000cant differences\nwere noted in the comprehensiveness of LLMs based on questions related to general knowledge,\ndiagnosis, treatment, and screening and prevention (Table 2).\nPage 6/18\nTable 2\nComprehensiveness of different LLMs\nCharacteristic ChatGPT ChatGPT plus Google Bard p-value\nOverall (n = 52)       0.028\nVery inadequate 0 (0.0%) 0 (0.0%) 2 (3.8%)  \nInadequate 19 (36.5%) 7 (13.5%) 13 (25.0%)  \nNeither comprehensive nor inadequate 12 (23.1%) 8 (15.4%) 11 (21.2%)  \nComprehensive 21 (40.4%) 35 (67.3%) 25 (48.1%)  \nVery comprehensive 0 (0.0%) 2 (3.8%) 1 (1.9%)  \nGeneral (n = 9)       0.520\nVery inadequate 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nInadequate 0 (0.0%) 0 (0.0%) 1 (11.1%)  \nNeither comprehensive nor inadequate 1 (11.1%) 0 (0.0%) 1 (11.1%)  \nComprehensive 8 (88.9%) 7 (77.8%) 7 (77.8%)  \nVery comprehensive 0 (0.0%) 2 (22.2%) 0 (0.0%)  \nDiagnosis (n = 5)       0.301\nVery inadequate 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nInadequate 3 (60.0%) 1 (20.0%) 1 (20.0%)  \nNeither comprehensive nor inadequate 1 (20.0%) 0 (0.0%) 0 (0.0%)  \nComprehensive 1 (20.0%) 4 (80.0%) 4 (80.0%)  \nVery comprehensive 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nTreatment (n = 27)       0.064\nVery inadequate 0 (0.0%) 0 (0.0%) 2 (7.4%)  \nInadequate 11 (40.7%) 5 (18.5%) 9 (33.3%)  \nNeither comprehensive nor inadequate 8 (29.6%) 4 (14.8%) 5 (18.5%)  \nComprehensive 8 (29.6%) 18 (66.7%) 10 (37.0%)  \nVery comprehensive 0 (0.0%) 0 (0.0%) 1 (3.7%)  \nScreening & Prevention (n = 11)       0.331\nAnalysis of the comprehensiveness of different LLMs in all categories, General knowledge, Diagnosis,Treatment, and Screening & Prevention.\nPage 7/18\nCharacteristic ChatGPT ChatGPT plus Google Bard p-value\nVery inadequate 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nInadequate 5 (45.5%) 1 (9.1%) 2 (18.2%)  \nNeither comprehensive nor inadequate 2 (18.2%) 4 (36.4%) 5 (45.5%)  \nComprehensive 4 (36.4%) 6 (54.5%) 4 (36.4%)  \nVery comprehensive 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nAnalysis of the comprehensiveness of different LLMs in all categories, General knowledge, Diagnosis,Treatment, and Screening & Prevention.\nAnalysis of the readability of different LLMs\nThe overall grade-level analysis revealed statistically signi\u0000cant variations among the LLMs (p < 0.001).\nSpeci\u0000cally, Google Bard displayed a signi\u0000cantly higher proportion of responses rated at the 10th to\n12th-grade level (34.6%) compared to ChatGPT (11.8%) and ChatGPT Plus (17.3%). Conversely, ChatGPT\ndemonstrated a signi\u0000cantly higher proportion of responses rated at the college level (61.5%) than\nGoogle Bard (36.5%). In the context of general knowledge about prostate cancer, ChatGPT exhibited more\ncollege-level responses (55.6%) compared to Google Bard (0.0%); however, the difference was not\nstatistically signi\u0000cant (p = 0.094). For diagnosis-related questions, the analysis yielded a signi\u0000cant\ndifference (p = 0.033), with Google Bard producing a higher proportion of 10th to 12th-grade responses\n(60.0%) compared to ChatGPT plus (20.0%) and ChatGPT (0.0%). In the treatment category, signi\u0000cant\ndifferences were observed (p < 0.001), with ChatGPT plus achieving a greater proportion of college-level\nresponses (70.4%) compared to ChatGPT (48.1%) and Google Bard (48.1%). Additionally, ChatGPT\ndisplayed more college graduate-level responses (44.4%) compared to ChatGPT Plus (29.6%) and Google\nBard (3.7%). In the context of screening and prevention, the difference between LLMs was not statistically\nsigni\u0000cant (Table 3).\nPage 8/18\nTable 3\nGrade level score of different LLMs\nCharacteristic ChatGPT ChatGPT plus Google Bard p-value\nOverall (n = 52)       < 0.001\n7th grade 0 (0.0%) 0 (0.0%) 2 (3.8%)  \n8th & 9th grade 2 (3.9%) 1 (1.9%) 12 (23.1%)  \n10th to 12th grade 6 (11.8%) 9 (17.3%) 18 (34.6%)  \nCollege 26 (51.0%) 32 (61.5%) 19 (36.5%)  \nCollege graduate 17 (33.3%) 10 (19.2%) 1 (1.9%)  \nGeneral (n = 9)       0.094\n7th grade 0 (0.0%) 0 (0.0%) 2 (22.2%)  \n8th & 9th grade 2 (22.2%) 1 (11.1%) 3 (33.3%)  \n10th to 12th grade 3 (33.3%) 3 (33.3%) 4 (44.4%)  \nCollege 2 (22.2%) 5 (55.6%) 0 (0.0%)  \nCollege graduate 2 (22.2%) 0 (0.0%) 0 (0.0%)  \nDiagnosis (n = 5)       0.033\n7th grade 0 (0.0%) 0 (0.0%) 0 (0.0%)  \n8th & 9th grade 0 (0.0%) 0 (0.0%) 2 (40.0%)  \n10th to 12th grade 0 (0.0%) 1 (20.0%) 3 (60.0%)  \nCollege 3 (75.0%) 3 (60.0%) 0 (0.0%)  \nCollege graduate 1 (25.0%) 1 (20.0%) 0 (0.0%)  \nTreatment (n = 27)       < 0.001\n7th grade 0 (0.0%) 0 (0.0%) 0 (0.0%)  \n8th & 9th grade 0 (0.0%) 0 (0.0%) 6 (22.2%)  \n10th to 12th grade 2 (7.4%) 0 (0.0%) 7 (25.9%)  \nCollege 13 (48.1%) 19 (70.4%) 13 (48.1%)  \nCollege graduate 12 (44.4%) 8 (29.6%) 1 (3.7%)  \nScreening & Prevention (n = 11)       0.235\nAnalysis of the Grade level score of different LLMs in all categories, General knowledge, Diagnosis,Treatment, and Screening & Prevention.\nPage 9/18\nCharacteristic ChatGPT ChatGPT plus Google Bard p-value\n7th grade 0 (0.0%) 0 (0.0%) 0 (0.0%)  \n8th & 9th grade 0 (0.0%) 0 (0.0%) 1 (9.1%)  \n10th to 12th grade 1 (9.1%) 5 (45.5%) 4 (36.4%)  \nCollege 8 (72.7%) 5 (45.5%) 6 (54.5%)  \nCollege graduate 2 (18.2%) 1 (9.1%) 0 (0.0%)  \nAnalysis of the Grade level score of different LLMs in all categories, General knowledge, Diagnosis,Treatment, and Screening & Prevention.\nFor the reading note, the overall analysis revealed statistically signi\u0000cant variations among the LLMs (p < \n0.001). Speci\u0000cally, Google Bard displayed a signi\u0000cantly lower proportion of responses categorized as\n\"Di\u0000cult to read\" (36.5%) compared to ChatGPT (51.0%) and ChatGPT plus (61.5%). In the \"Very di\u0000cult\nto read\" category, ChatGPT had a signi\u0000cantly higher proportion (33.3%) compared to Google Bard (1.9%)\nand ChatGPT Plus (19.2%). In the diagnosis context, there was a signi\u0000cant difference observed (p = \n0.044), with ChatGPT producing a higher proportion of \"Di\u0000cult to read\" responses (75.0%) compared to\nChatGPT plus (60.0%) and Google Bard (0.0%). In the treatment category, signi\u0000cant differences were\nobserved (p < 0.001), with ChatGPT plus achieving a greater proportion of \"Di\u0000cult to read\" responses\n(70.4%) compared to ChatGPT (48.1%) and Google Bard (48.1%). There was no statistical signi\u0000cance in\nthe screening and prevention context (p = 0.245, Table 4).\nPage 10/18\nTable 4\nAnalysis of the reading note of different LLMs\nCharacteristic ChatGPT ChatGPT plus Google Bard p-value\nOverall (n = 52)       < 0.001\nPlain English 2 (3.9%) 1 (1.9%) 12 (23.1%)  \nFairly easy to read 0 (0.0%) 0 (0.0%) 2 (3.8%)  \nDi\u0000cult to read 26 (51.0%) 32 (61.5%) 19 (36.5%)  \nFairly di\u0000cult to read 6 (11.8%) 9 (17.3%) 18 (34.6%)  \nVery di\u0000cult to read 17 (33.3%) 10 (19.2%) 1 (1.9%)  \nGeneral (n = 9)       0.105\nPlain English 2 (22.2%) 1 (11.1%) 3 (33.3%)  \nFairly easy to read 0 (0.0%) 0 (0.0%) 2 (22.2%)  \nDi\u0000cult to read 2 (22.2%) 5 (55.6%) 0 (0.0%)  \nFairly di\u0000cult to read 3 (33.3%) 3 (33.3%) 4 (44.4%)  \nVery di\u0000cult to read 2 (22.2%) 0 (0.0%) 0 (0.0%)  \nDiagnosis (n = 5)       0.044\nPlain English 0 (0.0%) 0 (0.0%) 2 (40.0%)  \nFairly easy to read 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nDi\u0000cult to read 3 (75.0%) 3 (60.0%) 0 (0.0%)  \nFairly di\u0000cult to read 0 (0.0%) 1 (20.0%) 3 (60.0%)  \nVery di\u0000cult to read 1 (25.0%) 1 (20.0%) 0 (0.0%)  \nTreatment (n = 27)       < 0.001\nPlain English 0 (0.0%) 0 (0.0%) 6 (22.2%)  \nFairly easy to read 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nDi\u0000cult to read 13 (48.1%) 19 (70.4%) 13 (48.1%)  \nFairly di\u0000cult to read 2 (7.4%) 0 (0.0%) 7 (25.9%)  \nVery di\u0000cult to read 12 (44.4%) 8 (29.6%) 1 (3.7%)  \nScreening & Prevention (n = 11)       0.245\nAnalysis of the reading notes of different LLMs in all categories, General knowledge, Diagnosis,Treatment, and Screening and prevention.\nPage 11/18\nCharacteristic ChatGPT ChatGPT plus Google Bard p-value\nPlain English 0 (0.0%) 0 (0.0%) 1 (9.1%)  \nFairly easy to read 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nDi\u0000cult to read 8 (72.7%) 5 (45.5%) 6 (54.5%)  \nFairly di\u0000cult to read 1 (9.1%) 5 (45.5%) 4 (36.4%)  \nVery di\u0000cult to read 2 (18.2%) 1 (9.1%) 0 (0.0%)  \nAnalysis of the reading notes of different LLMs in all categories, General knowledge, Diagnosis,Treatment, and Screening and prevention.\nNotably, signi\u0000cant differences were observed among the LLMs for all the continuous parameters,\nincluding words, sentences, syllables, word/sentence, syllable/word, FRE score, and FK Reading levels (p \n< 0.001 for all, Table 5). Firstly, when comparing the LLMs, ChatGPT exhibited the fewest words (197.0),\nfollowed by Google Bard (290.0), while ChatGPT plus had the most words (297.0). This trend suggests an\nincrease in the number of words from ChatGPT to ChatGPT plus to Google Bard. Secondly, in terms of\nsentences, ChatGPT had the lowest count (9.0), followed by ChatGPT Plus (15.5), and Google Bard had\nthe highest (16.5). This indicates a gradual increase in the number of sentences from ChatGPT to\nChatGPT plus to Google Bard.\nTable 5\nReadability of LLMs\nCharacteristic ChatGPT ChatGPT plus Google Bard p-value\nWords 197.0 (166.0–242.0) 297.0 (265.5–342.0) 290.0 (257.3–351.5) < 0.001\nSentences 9.0 (7.0–11.0) 15.5 (13.0–18.3) 16.5 (13.0–20.3) < 0.001\nSyllables 333.0 (289.5–411.5) 527.0 (458.8–574.3) 463.0 (404.0–551.8) < 0.001\nWord/sentence 22.4 (20.4–24.7) 19.2 (17.5–20.7) 18.3 (16.0–20.0) < 0.001\nSyllable/word 1.8 (1.7–1.8) 1.7 (1.7–1.8) 1.6 (1.5–1.7) < 0.001\nFRE Score 34.8 (28.7–45.0) 40.3 (33.4–45.8) 54.7 (46.0–60.2) < 0.001\nFKGL 14.0 (12.2–15.2) 12.3 (11.3–14.0) 10.2 (9.1–11.6) < 0.001\nAnalysis of the reading parameters and FRE and FKGL of different LLMs\nRegarding syllables, ChatGPT had the fewest (333.0), ChatGPT Plus had more (527.0), and Google Bard\nhad the most (463.0). This demonstrates a pattern of increasing syllables from ChatGPT to ChatGPT\nPlus to Google Bard. The word/sentence ratio followed a reverse pattern, with ChatGPT having the\nhighest ratio (22.4), followed by ChatGPT plus (19.2), and Google Bard with the lowest (18.3). Thus, the\ntrend is a decrease in the word/sentence ratio from ChatGPT to ChatGPT plus to Google Bard. Similarly,\nPage 12/18\nthe syllable/word ratio showed ChatGPT having the highest ratio (1.8), followed by ChatGPT plus (1.7),\nand Google Bard with the lowest (1.6). Lastly, in terms of readability, Google Bard had the highest FRE\nscore (54.7), ChatGPT Plus had a mid-range score (40.3), and ChatGPT had the lowest (34.8). For the FK\nReading Level, Google Bard had the lowest (10.2), ChatGPT Plus had an intermediate level (12.3), and\nChatGPT had the highest (14.0, Supplementary material).\nAnalysis of the stability of different LLMs\nThe analysis of stability was exclusively performed on ten questions in each LLM. These included three\ninquiries related to diagnosis, three to treatment, and four to screening and prevention. Inconsistency was\nonly detected in the response to one ChatGPT question about screening and prevention. There were no\nsigni\u0000cant differences in the stability of LLMs in terms of all domains (Table 6).\nTable 6\nStability of different LLMs\nCharacteristic ChatGPT ChatGPT plus Google Bard p-value\nOverall (n = 10)       > 0.999\nConsistent 9 (90.0%) 10 (100.0%) 10 (100.0%)  \nInconsistent 1 (10.0%) 0 (0.0%) 0 (0.0%)  \nDiagnosis (n = 3)        \nConsistent 3 (100.0%) 3 (100.0%) 3 (100.0%) NA\nInconsistent 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nTreatment (n = 3)        \nConsistent 3 (100.0%) 3 (100.0%) 3 (100.0%) NA\nInconsistent 0 (0.0%) 0 (0.0%) 0 (0.0%)  \nScreening & Prevention (n = 4)       > 0.999\nConsistent 3 (75.0%) 4 (100.0%) 4 (100.0%)  \nInconsistent 1 (25.0%) 0 (0.0%) 0 (0.0%)  \nAnalysis of the stability of different LLMs in all categories, General knowledge, Diagnosis, Treatment,and Screening and prevention.\nDiscussion\nThis study aimed to compare the performance of three LLMs in response to prostate cancer inquiries, and\nthe results demonstrated interesting variability in the criteria of accuracy, comprehensiveness, readability,\nand stability. Although the evaluation of the overall accuracy of LLMs showed no signi\u0000cant difference,\nChatGPT demonstrated superiority in most contexts. This \u0000nding aligns with previous studies that\nPage 13/18\nreached a similar conclusion, which showcases the capability of LLMs to provide accurate, but not\noptimal, answers to prostate cancer patients. [12, 13] For the general knowledge questions, unlike Google\nBard, which had poor levels of accuracy, ChatGPT exhibited more remarkable performance, signifying its\npotential as a valuable tool that aids in patient education. [12] Interestingly, in the context of treatment, all\nLLMs showed approximately close accuracy ranges with ChatGPT Plus in the lead. The similar\npercentages between ChatGPT and Bard in the context of therapy could be due to the focused approach\nto these inquiries, which requisite facts without the need for inference. This aligns with a previous study\nthat found that Google Bard had inferior diagnostic skills to physicians since it requires excellent clinical\nreasoning and inferential abilities. [14] When it came to the diagnosis, all LLMs had promising outcomes\nwith no signi\u0000cant differences, which presents the possibility of using LLMs in the realm of formulating\napproaches to aid physicians in their diagnosis. Lastly, similar to the previous domain, the screening and\nprevention domain also demonstrated ChatGPT plus preeminence with no signi\u0000cant overall differences\namong the three LLMs. This conciliates the general \u0000ndings observed in this study, which is that\nChatGPT is a superior model in its ability to provide accurate responses to patients.\nOur study proved a clear statistical difference between ChatGPT free, ChatGPT Plus, and Google Bard in\noverall comprehensiveness. Lim et al. evaluated the performance of ChatGPT free, ChatGPT plus, and\nGoogle Bard in generating comprehensive responses. They found no statistical difference between the\nthree LLM-Chatbots when comparing the comprehensiveness scores based on common queries\nanswered by the three bots. [15] Our study proved that ChatGPT Plus had the highest number of\ncomprehensive responses. On the other hand, Zhu et al. documented ChatGPT free as the LLM, which\ndemonstrated the superior performance of providing the highest proportion of comprehensive responses\nwith 95.45% comprehensiveness. [16] As reported by Xie et al., who compared the comprehensibility in\nproviding clinical guidance to junior doctors between three LLMs (including ChatGPT plus and Google\nBard), ChatGPT plus performed best in generating comprehensive responses. [17] This aligns with our\nstudy, which proved ChatGPT Plus was the highest-ranking LLM to generate comprehensive responses.\nGoogle Bard provided more easily readable answers, achieving higher FRE and lower FKGL scores and\ngenerating adequate, straightforward sentences. This \u0000nding aligns with several studies illustrating a\ncollege level of ChatGPT answers. [18, 19] For instance, Cocci et al. analyzed ChatGPT's responses to\nUrology case studies and found that ChatGPT achieved a college graduate reading level with median FRE\nand FKGL scores of 18 and 15.8, respectively. [18] Additionally, ChatGPT performed su\u0000ciently in\nproviding educational materials on dermatological diseases, with a 46.94 mean reading ease score. [19]\nConversely, Kianian et al. observed a lower FKGL of ChatGPT's responses (6.3 ± 1.2) than Bard's (10.5 ± \n0.8) when asked to generate educational information about uveitis. [20] ChatGPT scored an eighth-grade\nreadability level when generating output responses on radiology cases. [21] Moreover, Xie et al. evaluated\nthe readability of ChatGPT, Bard, and BingAI in generating answers about complex clinical scenarios.\nAmong the three LLMs, ChatGPT had the highest Flesch Reading Ease score. Nonetheless, Bard was a\nclose runner-up, and no signi\u0000cant difference was reported between the two. [17] In summary, although\nPage 14/18\nGhatGPT and Google Bard differ signi\u0000cantly in readability levels, both provide clear, understandable text\nwith a grade level suitable for patients seeking knowledge on prostate cancer.\nAlmost all generated answers were stable, except for one question within the \"screening and prevention\ndomain.\" Precisely, when asked, \"Should I get screened for prostate cancer?\" ChatGPT's 1st answer was\nless accurate than the second and third answers. Thus, it was labeled \"inconsistent\" for this question. It is\nimportant to note that only ten questions were tested for stability and compared across the three LLMs as\nthey are generally stable. In future studies, all inquiries should be tested and objectively evaluated in\nterms of their accuracy, comprehensiveness, and readability and determine the extent of their stability.\nAI chatbots have shown outstanding performance in providing precise, thorough information on prostate\ncancer. Nonetheless, even if AI can learn everything and anything about prostate cancer, it remains an\nobjective source of knowledge since it has never experienced the physical presence of treating such\ncases. This is described as the Knowledge Argument theory, in which the physical description of a\ndisease cannot replace the actual perceptual experience of treating it. There is a fundamental difference\nbetween knowing everything about prostate cancer and actually having the experience of treating\npatients and communicating their needs. Qualia is the philosophical term describing this subjective and\npersonal knowledge gained from physician-patient interactions, the empathy evoked from witnessing\npatients' suffering, and the tactile feedback experienced during physical examination or surgery. [21]\nSince these qualia are inaccessible to AI, it is impossible for AI to replace physicians in healthcare\neducation.\nLimitations\nWhile the study provided promising and insightful results, it had some limitations. First, although\nincorporating more questions would have clari\u0000ed statistical differences between the LLMs, this study\ncovered the most relevant, widely asked questions on prostate cancer. Furthermore, ChatGPT retrieves the\ndata from its knowledge base, which is only updated until September 2021. Finally, Google Bard\ndemonstrated a lack of information by refusing to answer one question, which might not have affected\nthe results. However, these limitations do not affect the reliability of this study's \u0000ndings. To our\nknowledge, this is the \u0000rst study to compare the performance of ChatGPT and Google Bard in the context\nof prostate cancer.\nConclusion\nIn conclusion, ChatGPT and Google Bard performed well in providing informational content on prostate\ncancer and might be helpful resources for patients and the general public. These study \u0000ndings\nemphasize the promising role of AI assistance in healthcare to improve patient's quality of life and\nenhance their education. Future studies should incorporate personalized inquiries and evaluate whether\nproviding more context would affect tested outcomes.\nPage 15/18\nDeclarations\nAcknowledgment\nnone\nAuthors Contribution:\nAA: conceptualization, methodology, investigation, formal analysis, writing—review and editing,\nvisualization, supervision; SS, NA, NA, FA: methodology, investigation, visualization, writing—original draft,\nvisualization; MA: methodology, investigation, formal analysis, visualization, writing—review and editing,\nvisualization; MA, BA: supervision, methodology, investigation, visualization, writing—review and editing,\nvisualization; All authors read and approved the \u0000nal manuscript.\nData Availability Statement \nThe data that support the \u0000ndings of this study are available on request from the corresponding author,\nSS.\nFunding source: none\nCon\u0000ict of interest: none\nReferences\n1. Gilson, A., et al. How does chatgpt perform on the united states medical licensing examination? the\nimplications of large language models for medical education and knowledge assessment. JMIR Med\nEduc. 9, e45312. (2023).\n2. Miao, J., Thongprayoon, C., & Cheungpasitporn, W. Assessing the accuracy of chatgpt on core\nquestions in glomerular disease. Kideny Int Rep. 8, 1657–1659 (2023).\n3. Biswas, S. Role of chat gpt in public health. Ann Biomed Eng. 51, 868–869 (2023).\n4. Sarraju, A., Bruemmer, D., Van Iterson, E., Cho, L., Rodriguez, F., & La\u0000n, L. Appropriateness of\ncardiovascular disease prevention recommendations obtained from a popular online chat-based\narti\u0000cial intelligence model. JAMA. 329, 842–844 (2023).\n5. Rawla, P. Epidemiology of prostate cancer. World J Oncol. 10, 63 (2019).\n\u0000. Alqahtani, W. S. et al. Epidemiology of cancer in Saudi Arabia thru 2010–2019: A systematic review\nwith constrained meta-analysis. AIMS Public Health. 7, 679 (2020).\n7. Sekhoacha, M. et al. Prostate cancer review: Genetics, diagnosis, treatment options, and alternative\napproaches. Molecules. 27, 5730 (2022).\n\u0000. Jindal, P., & MacDermid, J. C. Assessing reading levels of health information: uses and limitations of\n\u0000esch formula. Educ health. 30, 84–88 (2017).\nPage 16/18\n9. NCCN Guidelines. [cited 2023 Sept 26]. Available from: https://www.nccn.org/guidelines/guidelines-\ndetail?category=1&id=1459 .\n10. American Urological Association [Internet]. [cited 2023 Sept 26]. Available from:\nhttps://www.auanet.org/guidelines-and-quality/guidelines.\n11. European Association of Urology [Internet]. [cited 2023 Sept 26]. Available from:\nhttps://uroweb.org/guidelines.\n12. Zhu, L., Mou, W., Chen, R. Can the ChatGPT and other large language models with internet-connected\ndatabase solve the questions and concerns of patient with prostate cancer and help democratize\nmedical knowledge? J Transl Med. 21, 1–4 (2023).\n13. Pan, A. et al. Assessment of arti\u0000cial intelligence chatbot responses to top searched queries about\ncancer. JAMA oncol. (2023).\n14. Hirosawa, T., Mizuta, K., Harada, Y., & Shimizu, T. Comparative Evaluation of Diagnostic Accuracy\nBetween Google Bard and Physicians. Am J Med. 136, 1119–1123.e18 (2023).\n15. Lim, Z.W. et al. Benchmarking large language models' performances for myopia care: a comparative\nanalysis of ChatGPT-3.5, ChatGPT-4.0, and Google Bard. EBioMedicine. 95 (2023).\n1\u0000. Zhu, L., Mou, W., Chen, R. Can the ChatGPT and other large language models with internet-connected\ndatabase solve the questions and concerns of patient with prostate cancer and help democratize\nmedical knowledge? J Transl Med. 21, 296 (2023).\n17. Xie, Y., Seth, I., Hunter-Smith, D.J., Rozen, W.M. and Seifman, M.A. Investigating the impact of\ninnovative AI chatbot on post-pandemic medical education and clinical assistance: a comprehensive\nanalysis. ANZ J Surg. 10.1111/ans.18666 (2023).\n1\u0000. Cocci, A. et al. Quality of information and appropriateness of ChatGPT outputs for urology patients.\nProstate Cancer Prostatic Dis. 10.1038/s41391-023-00705-y (2023).\n19. Mondal, H., Mondal, S., Podder, I. Using chatgpt for writing articles for patients' education for\ndermatological diseases: a pilot study. Indian Dermatol Online J. 14, 482–486 (2023).\n20. Kianian, R., Sun, D., Crowell, E.L., Tsui, E. The use of large language models to generate education\nmaterials about uveitis. Ophthalmol Retina. 23, 2468–6530 (2023).\n21. Kuckelman, I.J. Assessing ai-powered patient education: a case study in radiology. Acad Radiol. 23,\n1076–6332 (2023).\n22. Nida-Rümelin M, O Conaill D. Qualia: The knowledge argument [Internet]. Stanford University; 2019\n[cited 2023 Oct 24]. Available from: https://plato.stanford.edu/entries/qualia-knowledge/#BasiIdea\nFigures\nPage 17/18\nFigure 1\nThe percentages of correct answers provided by each LLM\nFigure 2\nPage 18/18\nAnalysis of the accuracy of each LLM\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nSupplementarymaterial.docx",
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.8571563959121704
    },
    {
      "name": "Prostate cancer",
      "score": 0.6383864879608154
    },
    {
      "name": "Likert scale",
      "score": 0.5747726559638977
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5597772598266602
    },
    {
      "name": "Medicine",
      "score": 0.46791890263557434
    },
    {
      "name": "Cancer",
      "score": 0.4594728350639343
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4442768096923828
    },
    {
      "name": "Family medicine",
      "score": 0.37240737676620483
    },
    {
      "name": "Psychology",
      "score": 0.282395601272583
    },
    {
      "name": "Internal medicine",
      "score": 0.23071488738059998
    },
    {
      "name": "Computer science",
      "score": 0.21252459287643433
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185203018",
      "name": "King Saud bin Abdulaziz University for Health Sciences",
      "country": "SA"
    }
  ],
  "cited_by": 5
}