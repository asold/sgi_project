{
  "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers",
  "url": "https://openalex.org/W3035475042",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286962646",
      "name": "Yun, Chulhee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286981595",
      "name": "Chang, Yin-Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221384541",
      "name": "Bhojanapalli, Srinadh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2754832902",
      "name": "Rawat, Ankit Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221384543",
      "name": "Reddi, Sashank J.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156175939",
      "name": "Kumar Sanjiv",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1902237438",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3006610127",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2997753998",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2995446988",
    "https://openalex.org/W2964309636",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W2962749806",
    "https://openalex.org/W2970106668",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3012625278",
    "https://openalex.org/W2950858167",
    "https://openalex.org/W2995273672",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3124006463",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2964290105",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W2952744660",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W1539309091",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Recently, Transformer networks have redefined the state of the art in many NLP tasks. However, these models suffer from quadratic computational cost in the input sequence length $n$ to compute pairwise attention in each layer. This has prompted recent research into sparse Transformers that sparsify the connections in the attention layers. While empirically promising for long sequences, fundamental questions remain unanswered: Can sparse Transformers approximate any arbitrary sequence-to-sequence function, similar to their dense counterparts? How does the sparsity pattern and the sparsity level affect their performance? In this paper, we address these questions and provide a unifying framework that captures existing sparse attention models. We propose sufficient conditions under which we prove that a sparse attention model can universally approximate any sequence-to-sequence function. Surprisingly, our results show that sparse Transformers with only $O(n)$ connections per attention layer can approximate the same function class as the dense model with $n^2$ connections. Lastly, we present experiments comparing different patterns/levels of sparsity on standard NLP tasks.",
  "full_text": "O(n) Connections are Expressive Enough:\nUniversal Approximability of Sparse Transformers\nChulhee Yun\nMIT\nchulheey@mit.edu\nYin-Wen Chang\nGoogle Research NY\nyinwen@google.com\nSrinadh Bhojanapalli\nGoogle Research NY\nbsrinadh@google.com\nAnkit Singh Rawat\nGoogle Research NY\nankitsrawat@google.com\nSashank J. Reddi\nGoogle Research NY\nsashank@google.com\nSanjiv Kumar\nGoogle Research NY\nsanjivk@google.com\nAbstract\nRecently, Transformer networks have redeﬁned the state of the art in many NLP\ntasks. However, these models suffer from quadratic computational cost in the\ninput sequence length n to compute pairwise attention in each layer. This has\nprompted recent research into sparse Transformers that sparsify the connections\nin the attention layers. While empirically promising for long sequences, funda-\nmental questions remain unanswered: Can sparse Transformers approximate any\narbitrary sequence-to-sequence function, similar to their dense counterparts? How\ndoes the sparsity pattern and the sparsity level affect their performance? In this\npaper, we address these questions and provide a unifying framework that captures\nexisting sparse attention models. We propose sufﬁcient conditions under which we\nprove that a sparse attention model can universally approximate any sequence-to-\nsequence function. Surprisingly, our results show that sparse Transformers with\nonly O(n) connections per attention layer can approximate the same function class\nas the dense model with n2 connections. Lastly, we present experiments comparing\ndifferent patterns/levels of sparsity on standard NLP tasks.\n1 Introduction\nTransformer networks [28] and their variants [31] have played a key role in the recent advancement\nof the state of the art in many natural language processing tasks, such as machine translation [28],\nlanguage modeling [ 23, 24], and question answering [ 10, 17, 31]. The key component of these\nnetworks is the self-attention layer [ 1, 18], which updates the embeddings of the input tokens\nbased on their context. Naturally, the self-attention layer also plays the key role in the analysis of\nTransformers [3, 4, 12, 20, 33]; for example, Yun et al. [33] show that Transformers can approximate\nany continuous sequence-to-sequence functions (i.e., universal approximation), by proving that\nself-attention layers can compute contextual mappings of the input embeddings.\nOn the other hand, the self-attention layer is also the main bottleneck in scaling these models. It\ninvolves computation of pairwise inner products between input tokens, which results in quadratic\ncomputational complexity O(n2) in the length of the input sequence n. To mitigate this issue,\nresearchers have developed methods to sparsify the pairwise interactions/connections in self-attention\nlayers to reduce the computational complexity and/or improve model interpretability, and have shown\nsuccessful empirical results on tasks with long sequence lengths [ 2, 6, 8, 9, 11, 16, 22, 25, 26, 32,\n34, 35]. For example, Child et al. [6] propose sparse Transformers for sequence generation. One\nof the sparsity patterns considered in [6] is the STRIDED pattern, where the sparse attention layers\nalternate between two patterns: each token attends to only i) wlocal neighbors, and then ii) one after\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.04862v2  [cs.LG]  19 Dec 2020\nevery wtokens in a strided manner. By choosing w= O(√n), they propose sparse attention layers\nwith O(n3/2) connections and show improvements on both speed and performance over the dense\nTransformer.\nIn the existing results, the rule of thumb for designing sparsity patterns (e.g.,STRIDED ) is connectivity;\nthe intuition is that if each token can attend to the other tokens in multiple “hops,” then the resulting\nsparse Transformers do not lose much expressive power. However, there has been no formal\njustiﬁcation for this intuition. How does sparsifying the interaction in the self-attention layers affect\nthe model’s expressive power and ability to learn? What are the sparsity levels at which the model\nstill retains its rich expressive power, and how is it affected by the sparsity pattern? Such fundamental\nquestions about sparse attention models still remain unanswered.\n1.1 Summary of contributions\nIn this paper, we take the ﬁrst step towards a theoretical understanding of sparse Transformers.\n• We propose a uniﬁed framework to analyze sparse Transformers, which generalizes the existing\napproaches that sparsify attention layers (§ 3.1).\n• We propose a set of intuitive conditions on the sparsity pattern (Assumption 1) and the probability\nmap (Assumption 2). Then, in Theorem 1, we show that Sparse Transformers, of ﬁxed width\nand arbitrary depth, satisfying these conditions are universal approximators of any continuous\nsequence-to-sequence functions for any given ﬁxed sequence length (§ 3.2 and § 3.3).\n• We next show some examples of existing sparse Transformers [2, 6, 8, 9, 11, 34, 35] that satisfy\nthese conditions, and hence have universal approximability (§ 3.4). Surprisingly, we show that\nthere are sparse Transformers with only O(n) connections per self-attention layer (instead of n2)\nthat have enough expressive power to approximate arbitrary continuous functions (Corollary 2).\n• We report experimental results on standard NLP tasks using sparse Transformers, comparing\ndifferent sparsity patterns/levels (§ 5).\n2 Preliminaries and related works\nIn this section, we summarize the notation we will use throughout the paper, give a brief overview of\nTransformers, and then discuss existing efforts to sparsify the self-attention mechanism.\n2.1 Notation\nFor a positive integer a, we denote [a] = {1,2,...,a }. For any vector v ∈Rd, let vj denote its j-th\ncoordinate. For any matrix A ∈Rd×n, let Aj denote its j-th column, and ASdenote the submatrix\nconsisting of columns of A in an index set S⊆ [n]. We use ∥A∥p to denote the entry-wise ℓp norm\nof A. Let σS[·] be the softmax operator, which takes a matrix as input and applies softmax operation\nto each column of the matrix, which results in a column stochastic matrix.\n2.2 Transformers and their universal approximation power\nA Transformer network, consisting of multiple layers of Transformer blocks, implements a sequence-\nto-sequence function that maps Rd×n to Rd×n. A Transformer Block (TB) consists of two layers:\na self-attention layer and a token-wise feed-forward layer, and both layers have an identity skip\nconnection. More concretely, for an input X ∈Rd×n consisting of d-dimensional embeddings of n\ntokens, a Transformer block consists of the following two layers:\nAttn(X) = X + WO\n\n\nHead1(X)\n...\nHeadh(X)\n\n; Head i(X) = Wi\nVX ·σS[(Wi\nKX)TWi\nQX] (1a)\nTB(X) = Attn(X) + W2 ·ReLU(W1 ·Attn(X)), (1b)\nwhere WO ∈Rd×mh, Wi\nV,Wi\nK,Wi\nQ ∈Rm×d, W2 ∈Rd×r,and W1 ∈Rr×d. Although our\nanalysis and experiments rely on bias vectors, we omit those in (1) for simplicity.\nTo endow the network with information about the position of input tokens, it is common to add a\npositional embedding E ∈Rd×n to the input X before feeding it to the network. The positional\n2\nembedding E can be ﬁxed [28] or trainable [10]; we consider the latter. Using a trainable E, Th,m,r\nis deﬁned to be a class of functions of the form X ↦→t(X + E), where tis a composition of any\nnumber of Transformer blocks with hattention heads of head size m, and hidden layers of width r.\nThus, Th,m,r is a class of Transformers with a ﬁxed width while the depth can be arbitrary.\nFurther, let Fbe the class of continuous functions f : D →Rd×n deﬁned on any compact domain\nD ⊂Rd×n, where continuity is deﬁned with respect to the entry-wise ℓp norm (1 ≤p <∞).\nYun et al. [33, Theorem 3] show that T2,1,4 can universally approximate F. More precisely, for\nany f ∈F , ϵ >0 and 1 ≤p <∞, there exists a function g ∈T 2,1,4 such that dp(f,g) :=\n(\n∫\nD ∥f(X) −g(X)∥p\npdX)1/p ≤ϵ. Our goal in this paper is to study, in a similar manner, the\nexpressive power of sparse Transformers.\n2.3 Sparse Transformers\nAs seen in Eq. (1a), the self-attention layer involves computing the inner product between each pair of\ntokens, which we will refer to as theattention score matrix Ai := (Wi\nKX)TWi\nQX ∈Rn×n. This\nleads to quadratic computational complexity in n, which makes it expensive to apply Transformers to\ntasks with long sequence lengths. One popular approach to mitigate this problem is to sparsify the\nself-attention layers. We sub-classify sparse Transformers into three categories and summarize them\nbelow. For a more extensive summary, please see a recent survey [27].\nThe ﬁrst category reduces computation by making Ai sparse in a pre-determined manner. Each token\nin the sequence only attends to a ﬁxed smaller set of other tokens instead of the whole sequence\n[2, 6, 22]. In some papers, auxiliary tokens are added to improve connectivity between existing tokens\nwhile maintaining sparsity [11, 32]. One drawback of these approaches is that the sparsity pattern is\nindependent of input, so it cannot adapt to the data. To remedy this issue, [26] proposes to learn local\nattention span from data. In a concurrent paper, Zaheer et al. [34] propose the BIGBIRD sparsity\npattern which falls into this category. For BIGBIRD , the authors show its theoretical properties such\nas universal approximation and Turing completeness, as well as its superior empirical performance.\nWe note that our paper focuses on universal approximation for abroader class of sparse Transformers,\nby proposing a unifying framework to analyze them.\nThe second category studies making Ai sparse after the full Ai has been computed [8, 9, 35]. Here,\nthe focus is not on the computational gain via sparsity, because the full score matrix Ai has to be\ncomputed ﬁrst; rather, the goal here is to make attention layers more interpretable, as well as to\nimprove performance. This line of works modiﬁes σS in (1a) to other probability maps, by using\ntop-kelements or adopting sparser variants such as sparselin-gen or α-entmax [15, 21]. Compared to\nthe ﬁrst category, this approach has an advantage that sparsity patterns are adaptive to data.\nThe last category attempts to get the best of both worlds. This line of works tries to learn sparsity\npatterns from data using extra components predicting the connection between tokens, e.g., k-means\nclustering [25], LSTM [16], or locality-sensitive hashing [14]. This way, one can adaptively determine\nthe sparsity patterns before computing the score matrix. However, the drawback of this approach is\nthat one needs extra computation to train/run these additional components, which may be expensive.\n3 Universal approximation theorem for sparse Transformers\nIn this section, we derive a unifying framework to study sparse Transformers. We then propose a set\nof conditions on the sparse self-attention layers, and prove that the sparse Transformers satisfying\ntheses conditions are universal approximators of any continuous sequence-to-sequence functions.\nFinally, we show some examples of existing sparse Transformers that satisfy these conditions.\n3.1 A unifying framework for sparse Transformers\nWe modify the Transformer block in (1) to the following sparse Transformer block (STB):\nSAttnl(X) = X + WO\n\n\nSHead1,l(X)\n...\nSHeadh,l(X)\n\n, SHeadi,l(X)k = Wi\nVXAl\nk\n·ρ[(Wi\nKXAl\nk\n)TWi\nQXk]\nSTBl(X) = SAttnl(X) + W2 ·ReLU(W1 ·SAttnl(X)), (2)\n3\nwhere the sets Al\nk ⊆[n], for k ∈[n] and l ∈[p], deﬁne the psparsity patterns (formally deﬁned\nbelow), which are indexed by l∈[p]. Moreover, the parameter dimensions stay the same as in (1).\nNote that there are three main modiﬁcations from the dense Transformer.\n• (Cycling blocks) There are superscripts l ∈[p] added to the symbols such as SAttn. Unlike\ndense Transformers, some sparse Transformers cycle through pdifferent patterns. For example,\nthe STRIDED pattern [6] described in § 1 alternates between two different patterns, which\ncorresponds to p= 2. We add the superscript lto include such cases in our formulation. We\nassume that the layers in a sparse Transformer cycle through STB1,..., STBp.\n• (Sparsity patterns) Note that SHeadi,l(X)k denotes the k-th column of the i-th sparse attention\nhead. Unlike dense Transformers, the inner product of the k-th query vector Wi\nQXk is taken\nonly with Wi\nKXAl\nk\n, the key vectors of tokens in the set Al\nk ⊆[n]. Hence, instead of all n\ntokens, the k-th token computes attention scores with only tokens in Al\nk. For l∈[p], we refer to\nthe collection of the index sets {Al\nk}k∈[n], or simply {Al\nk}, as a sparsity pattern. As a result,\nSHeadi,l(X)k is a linear combination of columns in Wi\nVXAl\nk\n, rather than the whole sequence.\n• (Probability map) After computing the attention score matrix, the dense Transformer (1) uses the\nsoftmax operator σS to get a column stochastic matrix. In the sparse Transformers, we generalize\nσS to ρ. The probability map ρis any map that takes a matrix as input and outputs a column\nstochastic matrix.\nAs a sanity check, by choosing p = 1 , A1\nk = [ n] for all k ∈[n], and ρ = σS, we recover the\ndense Transformer (1). Note also that the sparse Transformer formulation covers the ﬁrst and\nsecond categories of existing results discussed in § 2.3. The ﬁrst category corresponds to choosing a\npredetermined sparsity pattern(s) {Al\nk}, while setting ρ= σS. The second category corresponds to\nopting for a probability map ρother than softmax σS, while maintaining A1\nk = [n] for all k∈[n].\nIn this paper, we assume for simplicity that all sparse attention heads SHead1,l,..., SHeadh,l in a\nsingle layer have identical sparsity patterns {Al\nk}. However, since our result only requires two sparse\nattention heads per layer (as we will see in Theorem 1), our result can be easily extended to the case\nthat allows multiple sparsity patterns in a single layer.\nSimilar to Th,m,r in § 2.2, we deﬁne the class of functions represented by sparse Transformers. We\nhide the dependence of this class on the sparsity patterns and probability map to simplify the notation.\nSTh,m,r := {X ↦→t(X + E) |tis a composition of cycling sparse Transformer blocks STBl,\neach with hheads of head size mand hidden layer size r,\nand positional embedding E ∈Rd×n is trainable}. (3)\n3.2 Conditions on sparsity patterns and probability map\nIn this section, we deﬁne a set of conditions on the sparsity patterns {Al\nk}and the probability map ρ\nthat ensures that the sparse Transformer universally approximate the function class F(cf. § 2.2).\nFor k∈[n] and the index sets {Al\nk}l∈[p], we deﬁne a sequence of sets {St\nk}t≥1 in a recursive way:\nS1\nk := A1\nk, St\nk :=\n⋃\nj∈A(t−1) mod p+1\nk\nSt−1\nj .\nThe set St\nk is the set of all tokens that the k-th token can directly/indirectly attend to, after tsparse\nattention layers with sparsity patterns cycling through {A1\nk},{A2\nk},..., {Ap\nk}. We now state our\nconditions on sparsity patterns.\nAssumption 1. The sparsity patterns {Al\nk}satisfy the following:\n1. For all k∈[n] and l∈[p], we have k∈Al\nk.\n2. There exists a permutation γ : [n] →[n] such that, for all i∈[n−1], γ(i) ∈⋃p\nl=1 Al\nγ(i+1).\n3. There exists a ﬁnite s∈N such that s= min{u|Su\nk = [n] for all k∈[n]}.\n4\nAssumption 1.1 is equivalent to saying that every token always attends to itself. Assumption 1.2 re-\nquires that there is a chain ofdirect connections that covers allntokens; note that the set⋃p\nl=1 Al\nγ(i+1)\nis the set of all tokens that the γ(i+ 1)-th token directly attends to. To elaborate more about the\nchain, consider a directed graph with nvertices corresponding to the ntokens. For any j ∈⋃p\nl=1 Al\nk,\nwe add a directed edge j →k. Given a graph constructed this way, Assumption 1.2 requires that\nthe graph has a Hamiltonian path γ(1) →γ(2) →···→ γ(n). Assumption 1.3 requires that after s\nsparse attention layers, every token can attend to all the other tokens, either directly or indirectly.\nAs we discuss in § 3.4, the statements in Assumption 1 are natural enough to be satisﬁed by many\nexisting sparsity patterns studied in the literature. In fact, Assumption 1.3 is necessary for universal\napproximation. If p= 1, n= 2, A1\n1 = {1}and A1\n2 = {1,2}, then the ﬁrst token never attends to\nthe second, so this sparse Transformer cannot approximate a function whose ﬁrst output token is\ndependent on both input tokens. The other two assumptions are required in parts of our proof, which\ninvolve “propagating information” over all the tokens in a sequential manner.\nWe now state the assumption on the probability map ρ[·]. For this, we deﬁne σH[·] to be the hardmax\noperator, which outputs the one-hot representation of the arg max entry for each column of the input\nmatrix. Since ρis a column-wise operator that outputs a column-stochastic matrix, we state the\nassumption for the operation of ρon a single column.\nAssumption 2. For any ζ >0 and η∈(0,1], ∃t> 0 such that, for any column input v satisfying\nvj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑\nj̸=j∗ρ[tv]j ≤η.\nAssumption 2 requires that, for inputs that have some margin between the unique maximum entry\nand the other entries, ρ[·] can closely approximate the behavior of the hardmax operator by scaling its\ninput by a positive factor t. This assumption is satisﬁed by softmax σS and other sparse variants such\nas sparselin-gen and α-entmax, as we show in § B of the supplementary material.\nIt is straightforward to check that the dense Transformer, which corresponds to p= 1, A1\nk = [n], and\nρ[·] = σS[·] in our framework, satisﬁes both Assumptions 1 and 2.\n3.3 Sparse Transformers are universal approximators\nThe key justifying intuition for adopting sparse attention layers is that, if each token can attend to the\nother tokens in multiple hops1, then these models do not lose too much expressive power. However,\nturning this intuition into a rigorous analysis is not straightforward. Moreover, recent results show\nthat limited width can render universal approximation impossible even with arbitrary depth [13, 19],\nhighlighting the challenges in analyzing sparse (limited “width”) Transformers.\nWe now state our main theorem, which shows that if the sparsity patterns{Al\nk}and the probability\nmap ρsatisfy Assumptions 1 and 2, sparse Transformers with h= 2 attention heads of size m= 1,\nand hidden layer width r = 4 are universal approximators of continuous sequence-to-sequence\nfunctions on any compact domain (recall that Fdenotes the class of such continuous functions).\nTheorem 1. Consider any f ∈F , and the class of sparse Transformers ST2,1,4 (cf. (3)) with\nthe underlying sparse attention layers satisfying Assumptions 1 and 2. Then, for any ϵ >0 and\n1 ≤p< ∞, there exists a function g∈ST 2,1,4 such that\ndp(f,g) :=\n(∫\nD\n∥f(X) −g(X)∥p\npdX\n)1/p\n≤ϵ.\nAs discussed earlier, dense Transformers satisfy Assumptions 1 and 2, which means that Theorem 1\nsubsumes the existing result [33] for dense Transformers. We note that the required h, m, and rin\nTheorem 1 are independent of d, n, or the sparsity patterns. We provide a high-level proof sketch of\nTheorem 1 in § 4.1. There, we also discuss how many layers are sufﬁcient for ϵ-approximation of f,\nand show that Theorem 1 requires only ptimes more self-attention layers than Yun et al. [33].\nWe would like to emphasize that Theorem 1 provides the ﬁrst formal evidence that well-designed\nsparse attention layers do not limit Transformer’s universal approximation power. In § 3.4, we\nshow a surprising fact that some existing sparse self-attention layers with only O(n) connections\n(as opposed to n2 in regular self-attention layers) retain enough expressive power to approximate\nF. Combined with the number of layers analyzed in § 4.1, this means that our analysis reduces the\n1Note that this corresponds to our Assumption 1.3.\n5\nconnections per layer from n2 to O(n), with only ptimes more attention layers. This advantage of\nsparse Transformers over their dense counterpart becomes even stronger with increasing sequence\nlength n, providing a theoretical support for the adoption of sparsity for tasks with long sequence\nlengths.\nOn a ﬁnal note, Theorem 1 views the sequence length nas a ﬁxed constant. Hence, our result does\nnot contradict a recent paper by Hahn [12] which studies the limitation of Transformers for varying n.\nAlso, our analysis applies to the encoder part of the Transformer network [28].\n3.4 Analysis of existing sparse Transformers\nBy Theorem 1, any sparse Transformer that satisﬁes our Assumptions 1 and 2 has universal approxi-\nmation ability. In this section, we give some examples of such sparse Transformers.\nChild et al. [6] propose two kinds of 2-step sparsity patterns (i.e., p= 2) for sequence generation\ntasks, namely STRIDED and FIXED patterns. We consider the extension of their auto-regressive\npatterns (i.e., attending only to past tokens) to the whole sequence. In the STRIDED pattern, a token\nﬁrst attends to its wneighbors and then attends to one token after every wtokens in a strided manner.\nThe sparsity pattern for the k-th token reads\nA1\nk = [n] ∩{k−⌈w/2⌉,...,k −1,k,k + 1,...,k + ⌊w/2⌋},\nA2\nk = [n] ∩{...,k −2w,k −w,k,k + w,k + 2w,... }.\n(4)\nIn the FIXED pattern, we divide the token into segments of length w. A token in a segment has access\nto other tokens in the same segment, and then the last tokens of the other segments:\nA1\nk = [n] ∩{⌈k/w⌉·w−w+ 1,..., ⌈k/w⌉·w}, A2\nk = [n] ∩({k}∪{w,2w,3w,... }) . (5)\nThe STRIDED and FIXED patterns satisfy both Assumption 1 and 2 for all values of w. Speciﬁcally,\nAssumption 1.3 holds with s= 2, because any token can directly/indirectly access all the tokens in\ntwo hops. As for Assumption 1.2, the identity permutation γ(i) = isufﬁces to satisfy the assumption\nfor both patterns. By choosing w = O(√n), sparse Transformers with the STRIDED and FIXED\npatterns achieve universal approximation power with O(n3/2) connections per attention layer.\nGuo et al. [11] consider the STAR sparsity pattern where they add an auxiliary relay token that attends\nto all the tokens, and the other tokens attend only to 2w neighboring tokens and the relay token.\nThere is only one sparsity pattern, so p= 1. The S TAR sparsity pattern can be written as\nA1\nk={n}∪\n{\n(i−1) mod (n−1) + 1 |i∈{k−w,...,k + w}\n}\nfor k∈[n−1], A1\nn=[n], (6)\nwhere w≥1. For any ﬁxed w, this sparse Transformer has O(n) connections per attention layer, and\nit satisﬁes both assumptions. Speciﬁcally, Assumption 1.2 is satisﬁed with the identity permutation,\ni.e., γ(i) = (i) for i∈[n]. Since any token can access other tokens within two hops, Assumption 1.3\nis satisﬁed with s = 2 . This demonstrates that O(n) connections per layer sufﬁce for sparse\nattention layers to have universal approximation power. One can similarly check that the sliding\nwindow sparsity patterns with/without global attention, proposed in Longformer [2], also satisfy the\nassumptions with O(n) connections. For the BIGBIRD sparsity pattern [34], it is also straightforward\nto check that a combination of its window attention and global attention satisﬁes Assumption 1 with\nO(n) connections. We state this interesting observation as a corollary below.\nCorollary 2. There exist sparse Transformers withO(n) connections per self-attention layer that\nare universal approximators in the sense of Theorem 1.\nRecall that another line of results that replaces softmax σS with sparse variants ρ[8, 9, 35] also ﬁts\ninto our formulation, with p = 1 and A1\nk = [n]. As we show in § B, these alternative ρ’s satisfy\nAssumption 2. Thus, by Theorem 1, these models also have the universal approximation property.\n4 Proof sketch and discussion\n4.1 Sketch of proof of Theorem 1\nNow, we sketch the proof of Theorem 1, which consists of three steps. Throughout the proof, we\nassume without loss of generality that D ⊂[0,1)d×n.\nStep 1. In the ﬁrst step, we approximate f ∈F with a piecewise constant function. Towards this,\nconsider a class of piecewise constant functionsF(δ) that map D to Rd×n, where δ >0 and δ−1 is an\n6\ninteger. Any function in F(δ) maps cubes of the form G+ [0,δ)d×n to matrices AG ∈Rd×n, where\nG ∈{0,δ,..., 1−δ}d×n. We approximate f with a function f ∈F(δ) such that dp(f,f) ≤ϵ/2, by\nchoosing small enough δ. We defer the statement and the proof to § C of the supplementary material.\nStep 2. We then approximate f ∈F(δ) with a sparse Transformer network with a slightly modiﬁed\narchitecture. In this architecture, we replace ReLU in the feed-forward layer with any piecewise\nlinear activation φ ∈Φ, where Φ denotes the class of (possibly discontinuous) piecewise linear\nfunctions with three pieces. We also replace ρin the sparse attention layer with the hardmax σH\noperator. We refer to the function class represented by the modiﬁed sparse Transformer as ST\nh,m,r\n.\nBy a careful construction, Lemma 3 shows that any f ∈F(δ) can be exactly represented by the\nmodiﬁed Transformer. To this end, we ﬁrst carefully choose the positional embedding E. We then\nquantize the inputs using feed-forward layers (Lemma 6), construct a contextual mapping using\nself-attention layers to map the quantized inputs to unique “ids” (Lemma 7), and then construct a\nvalue mapping with feed-forward layers to map the ids to desired output values (Lemma 8). See § D\nand § E in the supplementary material for details.\nLemma 3. For any f ∈F(δ), there exists g∈ST\n2,1,1\nsuch that f(X) = g(X) for all X ∈D.\nStep 3. The ﬁnal step is to approximate the function g ∈ ST\n2,1,1\nwith a sparse Transformer\ng ∈ ST2,1,4. This is done by approximating φ and σH with ReLU and ρ, respectively, while\ncarefully bounding the accumulation of errors introduced by the approximation. See § F in the\nsupplementary material for the details.\nLemma 4. For g∈ST\n2,1,1\nin Lemma 3, there exists g∈ST 2,1,4 such that dp(g,g) ≤ϵ/2.\nCombining these three steps, we establish that dp(f,g) ≤dp(f,f) + dp(f,g) + dp(g,g) ≤ϵ.\nHow many layers are sufﬁcient? In § D, Lemmas 6–8 show that we need dn\nδ sparse Transformer\nblocks (2) for quantization, p(n−1)\nδd + sfor the contextual mapping, and n\nδdn for the value mapping.\nRecall that pis from (2), sis from Assumption 1, and δis from Step 1 above. In comparison, § C of\n[33] shows that the dense counterpart requires dn\nδ , n\nδd + 1, and n\nδdn Transformer blocks (1) for the\nthree corresponding lemmas. Note two observations: 1) The value mapping dominates the depth,\nand its depth requirements are identical for the two cases; and 2) For contextual mappings (where the\nattention layers are used), we need roughly ptimes more layers for sparse models. Recall from § 3.4\nthat pis usually a small constant. These observations mean that sparse Transformers can achieve\nuniversal approximation using depth of the same order in d, nand δas the dense Transformers.\n4.2 Key challenges in the proof\nWhile the high level outline of the proof is similar to the one for dense Transformers [33], the proof\nin [33] crucially relies on having all connections for computing attention in each layer, which we\ndo not have in sparse Transformers. The sparsity in attention mechanism and the choice of general\nprobability map ρpose nontrivial challenges in the proof. We highlight the key differences below.\nEstablishing the Step 2 of the dense result [33] relies on constructing a contextual mapping using\nattention layers. A contextual mapping is a function that maps tokens in different sequences to unique\nvalues, thereby allowing Transformers to distinguish the same token appearing in different contexts.\nA crucial ingredient in the construction of such a mapping is a shift operation implemented with two\nattention heads in an attention layer. This shift operation involves each token taking the maximum\nand minimum over the entire sequence, which obviously cannot be done with sparse Transformers\nas it would require each token to attend to all the other tokens in the sequence. We circumvent this\nissue by carefully choosing the positional embedding E dependent on γ(cf. Assumption 1.2), and\nensuring that a similar shift operation is applied in a desired order even under sparsity.\nAs the ﬁnal phase of the contextual mapping in [33], a single attention layer shifts the entire sequence\nby the maximum over the sequence. Again, this cannot be directly implemented due to sparsity. Using\nAssumption 1.3, we instead prove that by stacking ssparse layers, one can successfully implement\na similar operation that shifts the entire sequence by the maximum over the whole sequence, up to\nsome controlled errors. This way, we overcome the difﬁculties posed by the sparsity and construct a\nnew version of contextual mappings. The details can be found in § E.2 of the supplementary material.\nMoreover, the proof of Step 3 in [33] uses the simple fact that softmax can approximate hardmax\narbitrarily closely. Since we do not restrict ourselves to softmax and generalize the probability map,\n7\nTable 1: Accuracy on the synthetic copying task. Percentages in parentheses mark the sparsity levels.\nSTRIDED FIXED STAR RANDOM\nDepth UNION\n(87%)\nMULTIHEAD\n(93%)\nSEQUENTIAL\n(93%)\nUNION\n(87%)\nMULTIHEAD\n(93%)\nSEQUENTIAL\n(93%) (87%) (90%)\n1-layer 0.82% 0.82% 0.80% 7.04% 0.76% 0.80% 1.53% 33.14%\n2-layer 100.00% 100.00% 81.24% 69.26% 56.45% 96.01% 29.70% 63.41%\n3-layer 100.00% 100.00% 100.00% 99.98% 99.08% 98.58% 42.18% 70.29%\n4-layer 100.00% 100.00% 100.00% 100.00% 99.64% 100.00% 83.57% 95.49%\na more careful argument is required. Since there are many layers in the network g, it turns out that\napproximating it with an original sparse Transformer in ST2,1,4 requires carefully controlling the\napproximation errors accumulated over layers. The proof of Lemma 4 in § F of the supplementary\nmaterial shows that this is indeed possible by utilizing Assumption 2.\n5 Experiments\nWe now present our experimental study comparing different design and implementation choices,\nincluding sparsity patterns and levels, on four tasks: i) a synthetic copying task, ii) language modeling,\niii) translation, and iv) GLUE tasks. Our goal is to understand the effect of such choices while\nemploying sparse Transformers to the tasks with small sequence lengths, complementing the existing\nresults for sparse Transformers on long sequence tasks.\n5.1 Experiment Settings\nWe consider four sparsity patterns: STRIDED (4), FIXED (5), STAR (6) and RANDOM . The ﬁrst three\npatterns are proposed in [6] and [11]; we test them for different values of w. In case of the RANDOM\npattern, given a sparsity level, we make connections uniformly at random. Following [6], STRIDED\nand FIXED patterns are tested for three different head conﬁgurations: i) SEQUENTIAL , where the\nsparse attention layers alternate between {A1\nk}and {A2\nk}, as described in the previous sections;\nii) UNION , where all sparse attention layers use the sparsity pattern{A1\nk∪A2\nk}; and iii) MULTIHEAD ,\nwhere half of the attention heads in every attention layer use {A1\nk}and the other half use {A2\nk}. Note\nthat, given the same sequence length, UNION is less sparse than the other two conﬁgurations. Thus,\nto ensure fair comparisons, we compare different conﬁgurations based on their sparsity levels.\nWe use maximum sequence length 256 in all our experiments, except 128 for GLUE tasks. For the\ncopying task, we experiment with only one sparse Transformer block (cf. Eq (2)), with varying\nnumbers of attention layers with 4 attention heads. For language modeling and translation, we use\nthe Tensor2Tensor [29] framework and employ 12-block and 6-block (respectively) Transformers\nwith 8 attention heads per block. For GLUE tasks, we experiment with the BERTBASE model. For\nmore details of the setup, see § G of the supplementary material.\n5.2 Results\nCopying task. We consider a synthetic copying task proposed in [ 14], where the input sequence\nhas the format 0s0s, where s is a 127 length sequence of symbols in [0,127]. The models have to\npredict (copy) the second part, given the ﬁrst half of the input. This task tests the ability of sparse\nTransformers to communicate the information. Table 1 presents the results for this task. Except for\nthe STAR and RANDOM patterns, we can see that the networks learn to copy the sequences with four\nsparse attention layers. One possible explanation for the bad performance of STAR is that, except for\nthe relay token, it only attends to local neighbors while the task requires to copy distant tokens.\nLanguage modeling. We conduct the language modeling experiments on the One Billion Word\nBenchmark [5] which has almost one billion tokens and a vocabulary of more than 800K unique\ntokens. In Figure 1a, we plot the perplexity against the sparsity level. We observe that the STRIDED\npattern and the STAR achieve the best performance across all sparsity levels. For both the STRIDED\nand FIXED patterns, the UNION conﬁguration shows the best performance.\nTranslation. For the translation task, we train the model on WMT18 English-Czech (en-cs) dataset\nand test it on the Newstest 2015 dataset. We plot the BLEU score against the sparsity level in\nFigure 1b. We apply the same sparsity pattern to both the encoder and the decoder. The STRIDED\n8\n(a) One Billion Benchmark\n (b) WMT en-cs\nFigure 1. Comparison of sparsity patterns and different head conﬁgurations on the One Billion\nBenchmark (a language modeling task) and WMT en-cs (a translation task). Note that the number of\nconnections in the attention layers goes down as we increase the sparsity level.\n(a) MNLI\n (b) XNLI\nFigure 2. Comparison of sparsity patterns and different head conﬁgurations on the MNLI and XNLI\n(sentence-pair classiﬁcation tasks), using the BERTBASE model.\nand FIXED patterns with UNION conﬁguration show the best scores, which are similar to the dense\nattention. The U NION conﬁguration is also the least sensitive to the sparsity levels.\nGLUE Tasks. We experiment with the BERTBASE model and report results on two sentence-pair\nclassiﬁcation tasks: MNLI [30] (Figure 2a) and XNLI [7] (Figure 2b). We plot the average accuracy\nof three runs on the dev set against the sparsity level. Additional results of the CoLA and MRPC\ntasks are reported in § H of the supplementary material.\nDiscussion. In all tasks, the RANDOM pattern performs worse than the deterministic patterns,\ndemonstrating the need for a careful design of sparsity patterns. Overall, our experiments suggest\nthat the design of the optimal sparsity patterns is heavily dependent on speciﬁc tasks. For example,\nthe STAR pattern shows the best performance on the language modeling task, while having trouble\nwith copying, translation, and BERT experiments. Among the three head conﬁgurations tested for\nSTRIDED and FIXED , the UNION performs the best in language modeling and translation but suffers\nin BERT tasks. In translation experiments, we see an interesting trend that the performance of\nMULTIHEAD conﬁguration improves as sparsity increases. We conjecture that this is due to the fact\nthat in STRIDED and FIXED , we have |A1\nk|= O(w) and |A2\nk|= O(n/w) (cf. Eqs (4) and (5)), so the\nsparsest choice of w= O(√n) is the one with the best “balance” between|A1\nk|and |A2\nk|.\n6 Conclusion\nRecently, sparse Transformers have received a lot of attention as they enable more efﬁcient/faster\nattention mechanisms for the tasks with very long sequence lengths. We take an initial step to provide\na theoretical understanding of these models. We provide a unifying framework that captures existing\nsparse attention models, and prove a universal approximation theorem for sparse Transformers\nwhich holds under intuitive conditions on sparsity patterns and probability maps. We also carry out\nexperiments comparing different sparsity patterns and levels on standard NLP tasks. We hope that\nthis work will shed light on the understanding of sparsity in attention layers, and provide guidance\nfor the design of sparse attention models.\n9\nBroader Impact\nThis work studies theoretical aspects of a class of widely used neural network models in NLP and\nrelated areas. Since we do not propose a new method nor a new dataset, we expect that the impact of\nthis work on ethical aspects and future societal consequences will be small, if any. Other than that,\nthis work brings new insights into the sparsity in attention models, hence may make an impact on the\nstudy of faster and more efﬁcient NLP models.\nAcknowledgments and Disclosure of Funding\nCY acknowledges partial support as a graduate Research Assistant from the NSF Grant (CAREER\n1846088). CY also acknowledges Korea Foundation for Advanced Studies for their support.\nReferences\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In International Conference on Learning Representations, 2015.\n[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document Transformer.\narXiv preprint arXiv:2004.05150, 2020.\n[3] Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.\nLow-rank bottleneck in multi-head attention models. arXiv preprint arXiv:2002.07028, 2020.\n[4] Gino Brunner, Yang Liu, Damián Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger\nWattenhofer. On identiﬁability in Transformers. arXiv preprint arXiv:1908.04211, 2019.\n[5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn.\nOne billion word benchmark for measuring progress in statistical language modeling. CoRR,\nabs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005.\n[6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse Transformers. arXiv preprint arXiv:1904.10509, 2019.\n[7] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger\nSchwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics, 2018.\n[8] Gonçalo M Correia, Vlad Niculae, and André FT Martins. Adaptively sparse Transformers.\narXiv preprint arXiv:1909.00015, 2019.\n[9] Baiyun Cui, Yingming Li, Ming Chen, and Zhongfei Zhang. Fine-tune BERT with sparse\nself-attention mechanism. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3539–3544, 2019.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional Transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[11] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang.\nStar-Transformer. arXiv preprint arXiv:1902.09113, 2019.\n[12] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions\nof the Association for Computational Linguistics, 8:156–171, 2020.\n[13] Jesse Johnson. Deep, skinny neural networks are not universal approximators. In International\nConference on Learning Representations, 2019.\n[14] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient Transformer.\narXiv preprint arXiv:2001.04451, 2020.\n10\n[15] Anirban Laha, Saneem Ahmed Chemmengath, Priyanka Agrawal, Mitesh Khapra, Karthik\nSankaranarayanan, and Harish G Ramaswamy. On controllable sparse alternatives to softmax.\nIn Advances in Neural Information Processing Systems, pages 6422–6432, 2018.\n[16] Xiaoya Li, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. Sac: Accelerating and\nstructuring self-attention via sparse adaptive connection. arXiv preprint arXiv:2003.09833,\n2020.\n[17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT\npretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n[18] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-\nbased neural machine translation. In Empirical Methods in Natural Language Processing\n(EMNLP), pages 1412–1421, Lisbon, Portugal, September 2015. Association for Computational\nLinguistics.\n[19] Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. Minimum width for universal approxi-\nmation. arXiv preprint arXiv:2006.08859, 2020.\n[20] Jorge Pérez, Javier Marinkovi´c, and Pablo Barceló. On the Turing completeness of modern\nneural network architectures. arXiv preprint arXiv:1901.03429, 2019.\n[21] Ben Peters, Vlad Niculae, and André FT Martins. Sparse sequence-to-sequence models. arXiv\npreprint arXiv:1905.05702, 2019.\n[22] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise\nself-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.\n[23] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. Technical Report, OpenAI, 2018.\n[24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. Technical Report, OpenAI, 2019.\n[25] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based\nsparse attention with routing Transformers. arXiv preprint arXiv:2003.05997, 2020.\n[26] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention\nspan in Transformers. arXiv preprint arXiv:1905.07799, 2019.\n[27] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey.\narXiv preprint arXiv:2009.06732, 2020.\n[28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\ntion Processing Systems, pages 5998–6008, 2017.\n[29] Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N Gomez, Stephan\nGouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, et al. Tensor2tensor for\nneural machine translation. arXiv preprint arXiv:1803.07416, 2018.\n[30] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) , pages 1112–1122. Association for Computational\nLinguistics, 2018. URL http://aclweb.org/anthology/N18-1101.\n[31] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V .\nLe. XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019.\n[32] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. Bp-Transformer: Modelling\nlong-range context via binary partitioning. arXiv preprint arXiv:1911.04070, 2019.\n11\n[33] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.\nAre Transformers universal approximators of sequence-to-sequence functions? In International\nConference on Learning Representations, 2020.\n[34] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for\nlonger sequences. arXiv preprint arXiv:2007.14062, 2020.\n[35] Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, and Xu Sun. Ex-\nplicit sparse Transformer: Concentrated attention through explicit selection. arXiv preprint\narXiv:1912.11637, 2019.\n[36] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\nwatching movies and reading books. In Proceedings of the IEEE international conference on\ncomputer vision, pages 19–27, 2015.\n12\nA Outline and notation\nThe supplementary material is organized as follows. First, § B proves that the softmax operator as\nwell as its sparse versions indeed satisfy Assumption 2. Next, § C provides formal statements of\nStep 1 in the proof sketch (§ 4.1). The outline of proof of Lemma 3 (Step 2 in the proof sketch) is\npresented in § D, followed by a separate section (§ E) proving the three key sublemmas in the proof.\nThe proof of Step 3, Lemma 4, is given in § F. Lastly, § G and § H present the detailed setup of our\nexperiments and additional experiment results, respectively.\nWe next review some of the notation and also introduce additional notation used throughout the\nsupplementary material. For a positive integer a, let [a] := {1,...,a }. For a,b,c ∈R where\nb−a> 0 is an integer multiple of c> 0, we write [a: c: b] := {a,a + c,a + 2c,...,b −c,b}. For\nany matrix A ∈Rd×n, let Aj denote its j-th column, and ASdenote the submatrix consisting of\ncolumns of A in the index set S⊆ [n]. We also use Ai,j to denote its (i,j)-th entry. Let 1 {·}be the\n0-1 indicator for an event. Let 1n ∈Rn be a vector whose components are all 1.\nB Sparse probability maps satisfy Assumption 2\nIn this section, we show that the softmax operatorσS as well as the probability maps ρused to replace\nsoftmax in the existing approaches, namely softmax with only top-kinputs [35], sparselin-gen [9],\nand α-entmax [8], all satisfy Assumption 2. We restate the assumption for reader’s convenience:\nAssumption 2. For any ζ >0 and η∈(0,1], ∃t> 0 such that, for any column input v satisfying\nvj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑\nj̸=j∗ρ[tv]j ≤η.\nAs in the assumption, we only consider the operation of these probability maps on a single vector, as\nthey are applied column-wise. For each of the probability maps, we will show that for any ζ >0 and\nη∈(0,1], we can choose t> 0 that satisﬁes the conditions of Assumption 2.\nB.1 Softmax & softmax with top- kinputs\nGiven an input vector v ∈Rn, the j-th coordinate of the output of softmax σS[v] is deﬁned as\nσS[v]j := exp(vj)∑n\ni=1 exp(vi).\nWe assume without loss of generality that the entry ofv is in decreasing order, where the ﬁrst two\nentries satisfy v1 −v2 ≥ζ. For any such ζ >0 and any 0 <η ≤1, our aim is to show the existence\nof t> 0 such that σS[tv]1 = exp(tv1)∑n\ni=1 exp(tvi) ≥1 −η. Then, ∑n\nj=2 σS[tv]j ≤ηfollows.\nNow, since vi ≤v1 −ζfor i∈[2 : n], note that\nσS[tv]1 = exp(tv1)∑n\ni=1 exp(tvi) ≥ exp(tv1)\nexp(tv1) + (n−1) exp(tv1 −tζ) = 1\n1 + (n−1) exp(−tζ).\nSince 1\n1+(n−1) exp(−tζ) is an increasing function in t >0, one can increase tsufﬁciently large to\nmake it greater than 1 −η.\nThe same argument holds for the softmax with top-kinputs, used in [35]. By the assumption on v,\nentries v1,...,v k are the top kcomponents. Thus,\nρ[tv]1 ≥ 1\n1 + (k−1) exp(−tζ) ≥1 −η\ncan be satisﬁed by choosing large enough t> 0.\nB.2 Sparselin-gen\nWe now consider the case where ρis sparselin-gen [15], which was used to sparsify the attention\nscore matrices in [9]. Given a regularization parameter λ∈[0,1), the sparselin-gen used in [9] is\ndeﬁned as\nρ[v] := arg min\np∈∆n−1\n∥p −v∥2 −λ∥p∥2 ,\n13\nwhere ∆n−1 := {p ∈Rn |p ≥0,∑n\ni=1 pi = 1}is the probability simplex. Then, the solution for\noptimization problem above can be written as\nρ[v]j = max\n{\n0,vj −τ(v)\n1 −λ\n}\n, for j ∈[n],\nwhere τ : Rn →R is a threshold function that chooses the threshold τ(v) such that ∑n\nj=1 ρ[v]j = 1.\nNow, assume without loss of generality that the entry of v is in decreasing order, where the ﬁrst two\nentries satisfy v1 −v2 ≥ζ. For any such ζ >0 and any 0 <η ≤1, our aim is to show the existence\nof t> 0 such that ρ[tv]1 ≥1 −η. This is done by choosing t= 1−η\nζ . To see this, notice that if vj’s\nare in decreasing order, then ρ[v]j are also in decreasing order. Now consider\nρ[tv]1 = max\n{\n0,tv1 −τ(tv)\n1 −λ\n}\n, ρ[tv]2 = max\n{\n0,tv2 −τ(tv)\n1 −λ\n}\n.\nIf ρ[tv]2 = 0, then ρ[tv]j = 0 for all j = 3,...,n , and ρ[tv]1 = 1 ≥1 −η. If ρ[tv]2 >0, then\nρ[tv]1 −ρ[tv]2 = tv1 −τ(tv)\n1 −λ −tv2 −τ(tv)\n1 −λ = t(v1 −v2)\n1 −λ ≥t(v1 −v2) ≥tζ = 1 −η.\nB.3 α-entmax\nNext, we consider the case where ρis α-entmax [21], which was used to sparsify the attention score\nmatrices in [8]. Given a parameter α≥1, the α-entmax is deﬁned as\nρ[v] := arg max\np∈∆n−1\npTv + Hα(v),\nwhere ∆n−1 is the probability simplex and Hα is the Tsallis continuous family of entropies\nHα(v) :=\n{\n1\nα(α−1)\n∑\njvj −vα\nj α> 1,\n−∑\njvjlog vj α= 1.\nAs shown in [8], the solution of α-entmax is equal to softmax if α= 1, and otherwise (α> 1) it is\ngiven in the form\nρ[v]j =\n[\nmax{0,(α−1)vj −τ(v)}\n] 1\nα−1 , for j ∈[n],\nwhere τ : Rn →R is a threshold function that chooses the threshold τ(v) such that ∑n\nj=1 ρ[v]j = 1.\nSince softmax (α= 1) is already covered above, we focus on α> 1.\nAgain, assume without loss of generality that the entry of v is in decreasing order, where the ﬁrst two\nentries satisfy v1 −v2 ≥ζ. For any such ζ >0 and any 0 <η ≤1, our aim is to show the existence\nof t> 0 such that ρ[tv]1 ≥1 −η. This is done by choosing t= 1/ζ(α−1).\nNote that (α−1)t(v1 −v2) ≥1 due to our choice of t. Then, we will show that with such a t,\nρ[tv]1 = 1 must hold. For the sake of contradiction, suppose not: ρ[tv]1 <1. Then, by monotonicity\nof ρ[tv]j, we have ρ[tv]2 >0. This means\nρ[tv]2 =\n[\n(α−1)tv2 −τ(tv)\n] 1\nα−1 >0,\nin particular, we have (α−1)tv2 −τ(tv) >0. However, recall that (α−1)t(v1 −v2) ≥1, which\nimplies (α−1)tv1 −τ(tv) >1. This results in\nρ[tv]1 =\n[\n(α−1)tv1 −τ(tv)\n] 1\nα−1 >1,\nthus contradicting ρ[tv]1 <1. Therefore, ρ[tv]1 = 1 must hold.\nC Details of the Step 1 in the proof sketch (§ 4.1)\nWe start by formally deﬁning the function class F(δ).\nF(δ) :=\n{\nZ ↦→\n∑\nG∈Gδ\nAG1\n{\nZ ∈G + [0,δ)d×n}\n|Z ∈D,AG ∈Rd×n\n}\n,\nwhere Gδ := {0,δ,..., 1 −δ}d×n. We now state and prove the lemma.\n14\nLemma 5. For any f ∈F and ϵ >0, there exists a small enough δ >0 such that there exists\nf ∈F(δ) such that dp(f,f) ≤ϵ/2.\nProof Since f : D → Rd×n is a continuous function on a compact domain, it is uniformly\ncontinuous. Also, continuity is deﬁned with respect to entry-wise ℓp norm which is equivalent to\nentry-wise ℓ∞norm, uniform continuity leads to\n∀ϵ> 0,∃δ >0 such that ∀X,Y ,∥X −Y ∥∞<δ =⇒ ∥f(X) −f(Y )∥p <ϵ/2.\nThen, suppose we create a set of cube grid pointsGδ := {0,δ,..., 1−δ}d×n, and deﬁne a piece-wise\nconstant approximation\nf(X) =\n∑\nG∈Gδ\nf(G)1\n{\nX ∈G + [0,δ)d×n}\n.\nNote that for any X ∈G + [0,δ)d×n we have ∥X −G∥∞<δ, so we have\nf(X) −f(X)\n\np = ∥f(X) −f(G)∥p <ϵ/2.\nThis implies that\ndp(f,f) =\n(∫\nD\nf(X) −f(X)\np\np\n)1/p\n≤ϵ/2,\nﬁnishing the proof of the lemma.\nD Proof of Lemma 3 (Step 2 in § 4.1)\nIn this section, we describe in further details how modiﬁed sparse Transformers (the class ST\n2,1,1\n)\nare able to exactly express arbitrary piecewise constant functions in F(δ). We show that we can\ncompute a contextual mapping of the entire input sequences without relying on dense self-attention\nlayers. The token-wise feed-forward layers then transform these contextual mappings to the desired\noutput sequence.\nTo give a high level summary of the proof, we want to show that given a piece-wise constant function\nf ∈F(δ), there exists a modiﬁed Transformer network g∈ST\n2,1,1\nthat exactly represents f. Recall\nﬁrst that the function class ST\n2,1,1\nhas an additive positional embedding matrix E ∈Rd×n that is\nadded to input before the input is fed to the network. We start by choosing the positional embedding\nE and construct a Transformer network that implements quantization of the input, contextual mapping\nof the quantized input, and value mapping of the context ids.\n1. Choose the positional embedding E according to γin Assumption 1.2. After addition, each\ncolumn of the input Xk + Ek are in disjoint intervals.\n2. Given the input X + E, a series of modiﬁed feed-forward layers quantizes it so that each\nentry of the quantized input has a value in {0,δ,...,n −δ}(Lemma 6).\n3. Next, a series of modiﬁed sparse self-attention layers takes the quantized input H and\nimplement a contextual mapping qsuch that, for different quantized input sequences H and\nH′, all the elements in q(H) and q(H′) are distinct (Lemma 7).\n4. Finally, a series of modiﬁed feed-forward layers maps each element in the context id q(H)\nto the desired output value of f ∈Fat the input X (Lemma 8).\nWe defer the proofs of Lemmas 6, 7, and 8 to a separate section: see § E.\nBefore discussing the details of each step, we note that although a Transformer network stacks\nself-attention and feed-forward layers in an alternate manner, we can use a series of arbitrary number\nof the same layers, thanks to skip connections. The outline of the proof is similar to [ 33], but key\ncomponent in their proof called selective shift operation relies on the fact that each token can attend\nto the entire sequence; this is not true in sparse Transformers, which poses a nontrivial challenge.\nWe overcome this issue by a more careful construction of the positional embedding E and sparse\nself-attention layers.\n15\nD.1 Choosing the positional embedding\nRecall from Assumption 1.2 that there exists a permutation γ : [n] →[n] such that for all i∈[n−1],\nγ(i) is one of the tokens that the γ(i+ 1)-th token directly attends to. Using this permutation γ, we\nchoose the columns of positional embedding E in the following way:\nEγ(1) = (n−1)1n, and Eγ(i) = (i−2)1n, for i∈[2 : n]\nAs a result, theγ(1)-th column ofX+E will be in the range[n−1,n)d, and similarlyXγ(i)+Eγ(i) ∈\n[i−2,i −1)d for i∈[2 : n]. This means that the entries corresponding to different tokens lie be in\ndisjoint intervals of the form [j,j + 1), where j ∈[0 : n−1].\nD.2 Quantization by feed-forward layers\nNote from the previous step that each entry of X + E must be in [0,n). Next, we quantize this\ninterval [0,n) of input using to a set of δ-grid points {0,δ,...,n −δ}. This allows us to deal with\nﬁnite set of values, which proves useful in the later stages of the proof. The next lemma shows that\nthe quantization can be carried out using a seried of the modiﬁed feed-forward layers.\nLemma 6. Consider a entry-wise quantization map gent\nq : R →R:\ngent\nq (t) =\n{kδ if kδ ≤t< (k+ 1)δ, k∈[0 : n/δ−1],\nt otherwise.\nThere exists a function gq : Rd×n ↦→Rd×n composed of dn\nδ token-wise feed-forward layers with\nr= 1 and an activation φ∈Φ, which implements the entry-wise quantization gent\nq to each entry of\nits input.\nD.3 Contextual mapping by sparse self-attention layers\nAfter the input X + E is quantized, the output of gq must be in the following set Hδ ⊂Rd×n:\nHδ := {G + E ∈Rd×n |G ∈Gδ},\nwhere Gδ := {0,δ,..., 1 −δ}d×n was deﬁned to be the δ-cubic grid points of [0,1)d×n. Using\nthis ﬁnite set of sequences, we construct a contextual mapping that maps each sequence in Hδ to\nunique numbers. Recall that the sparse attention layer has psparsity patterns that rotate in cycles, and\nAssumption 1.3 assumes that one token directly/indirectly access all the other tokens after ssuch\nsparse attention layers. We now state the lemma.\nLemma 7. Assume that n≥2, and δ−1 is an integer satisfying δ−1 ≥2. Suppose that the sparse\nself-attention layers (h = 2,m = 1) satisfy Assumption 1 and employ the hardmax σH operator,\nand that the positional embedding E was chosen as described in § D.1. Then, there exist a function\ngc : Rd×n →Rd×n composed of p(n−1)\nδd + ssparse self-attention layers, and a vector u ∈Rd, such\nthat q(H) := uTgc(H) satisﬁes the following properties:\n1. For any H ∈Hδ, the entries of q(H) are all distinct.\n2. For any H,H′∈Hδ such that H ̸= H′, all entries of q(H), q(H′) are distinct.\nThis contextual mapping maps each unique sequence/context into different context ids, enabling the\nnetwork to distinguish the same token appearing in different sequences.\nD.4 Value mapping by feed-forward layers\nAfter the contextual mapping, we use the token-wise feed-forward layers to map each different\ncontext ids to the desired output value of the target function f. More speciﬁcally, recall the function\ngc from Lemma 7. For any H ∈Hδ, we need to map the output gc(H) of Lemma 7 to the desired\nfunction value f(H −E) (recall that H is the quantized input after adding E to X, so we need\nto subtract E). This is done by implementing a token-wise value mapping using the feed-forward\nlayers.\n16\nLemma 8. There exists a function gv : Rd×n → Rd×n composed of n(1\nδ)dn token-wise feed-\nforward layers (r = 1) with an activation φ′∈Φ such that gv is deﬁned by a token-wise function\ngtkn\nv : Rd →Rd on each column,\ngv(Z) =\n[\ngtkn\nv (Z1) ··· gtkn\nv (Zn)\n]\n,\nwhere for all H ∈Hδ and k∈{1,...,n },\ngtkn\nv (gc(H)k) = f(H −E)k.\nD.5 Finishing the proof\nGiven Lemmas 6, 7, and 8, one can easily check that for any G ∈Gδ := {0,δ,..., 1 −δ}d×n and\nany input value X ∈G + [0,δ)d×n, we have\ngv ◦gc ◦gq(X + E) = gv ◦gc(G + E)\n=\n[\ngtkn\nv (gc(G + E)1) gtkn\nv (gc(G + E)2) ··· gtkn\nv (gc(G + E)n)\n]\n=\n[\nf(G)1 f(G)2 ··· f(G)n\n]\n= f(G) = f(X).\nTherefore, we have constructed a modiﬁed sparse Transformer networkg(X) := gv ◦gc ◦gq(X +E)\nthat satisﬁes g(X) = f(X) for all X ∈D, hence proving Lemma 3.\nE Proof of Lemmas 6, 7, and 8\nE.1 Proof of Lemma 6\nThe proof goes as follows. Using n\nδ token-wise feed-forward layers, we implement the quantization\nfunction gent\nq that quantizes the ﬁrst row of the input. Then we stack another n\nδ layers to quantize the\nsecond row, and so on.\nFor the ﬁrst row, we add n/δlayers of the following form, for k∈[0 : n/δ−1].\nZ ↦→Z + e(1)φ((e(1))TZ −kδ1T\nn), φ(t) =\n{0 t< 0 or t≥δ,\n−t 0 ≤t<δ,\nwhere e(1) ∈Rd is the ﬁrst canonical basis vector e(1) = (1,0,..., 0). Each layer quantizes Z1,:\nin [kδ,kδ + δ) to kδ, without modifying other intervals or other rows of Z. Note that the activation\nφis a piecewise linear function with three pieces; hence, φ∈Φ. Therefore, the layers satisfy the\ndeﬁnition of modiﬁed feed-forward layers. We can now repeat the same construction for the d−1\nremaining rows.\nE.2 Proof of Lemma 7\nIn order to construct a network gc that implements the contextual mapping, we ﬁrst introduce two\noperations referred to as the sparse selective shift operation and all-max-shift operation, implemented\nby at most two (modiﬁed) sparse attention heads of head size 1. Then, we proceed to stack layers\nimplementing the selective shift operations and all-max-shift operations, and prove that these layers\nmap input H ∈Hδ to unique context ids.\nE.2.1 Preliminaries\nSparse selective shift operation. Given any vector u ∈Rd, ﬁrst consider the following function\nimplementable with a sparse attention head with head size 1 and sparsity pattern {Al\nk}k∈[n]. For\nk∈[n], the function ψl : Rd×n →R1×n computes each of its output column in the following way:\nψl(Z; bQ)k := uTZAl\nk\nσH[(uTZAl\nk\n)T(uTZk −bQ)] =\n{\nmaxj∈Al\nk\nuTZj if uTZk >bQ,\nminj∈Al\nk\nuTZj if uTZk <bQ.\nOne can consider a sparse self-attention layer that consists of two such heads, with bQ <b′\nQ:\nΨl(Z; c,bQ,b′\nQ) := Z +\n[\nce(1) −ce(1)][ψl(Z; bQ)\nψl(Z; b′\nQ)\n]\n.\n17\nThe (1,k)-th entry of Ψl(Z; c,bQ,b′\nQ) reads\nΨl(Z; c,bQ,b′\nQ)1,k = Z1,k + c(ψl(Z; bQ)k −ψl(Z; b′\nQ)k)\n=\n{\nZ1,k + c(maxj∈Al\nk\nuTZj −minj∈Al\nk\nuTZj) if bQ <uTZk <b′\nQ,\nZ1,k if uTZk /∈[bQ,b′\nQ].\nThis means that for input columns Zk satisfying uTZk ∈(bQ,b′\nQ) only, Ψl shifts up the ﬁrst entry\nof Zk by the difference of maximum and minimum values of uTZj over the sparsity pattern j ∈Al\nk,\nwhile leaving other columns intact. By choosing bQ and b′\nQ properly, we can selectively modify\ncertain columns without touching other columns; we refer to this operation Ψl as the sparse selective\nshift operation, and we will see later that this is indeed the key ingredient of our proof.\nIn fact, this operation is a sparse version of the selective shift operation used in [ 33]. Since Al\nk is\nusually only a small subset of [n], one cannot calculate the maximum and minimum of uTZj over\nthe whole sequence, as done in [33]. Instead, we use Assumption 1.2 and a more careful choice of E\nto get around the restriction posed by sparsity.\nAll-max-shift operation. Suppose the input Z ∈Rd×n satisﬁes uTZ >0 entry-wise, for a vector\nu ∈Rd. Then, the all-max-shift operation Ωl : Rd×n →Rd×n is a sparse self-attention layer that\nconsists of one attention head:\nΩl(Z; c) = Z + ce(1)ψl(Z; 0).\nThe (1,k)-th entry of Ωl(Z; c) reads\nΩl(Z; c)1,k = Z1,k + cψl(Z; 0)k = Z1,k + cmax\nj∈Al\nk\nuTZj.\nSo, for each column k, the all-max-shift operation shifts up the ﬁrst entry of Zk by the maximum\nvalue of uTZj over the sparsity pattern j ∈Al\nk. Unlike the selective shift operation, the all-max-shift\noperation is applied to all the columns.\nColumn ids. Recall that the any input to this step is in\nHδ := {G + E ∈Rd×n |G ∈Gδ := [0 : δ: 1 −δ]d×n}.\nBecause of the way E is chosen according to the permutation γin Assumption 1.2, for any H ∈Hδ\nwe have\nHγ(1) ∈[n−1 : δ: n−δ]d,\nHγ(i) ∈[i−2 : δ: i−1 −δ]d for all i∈[2 : n].\nNow consider u := (1,δ−1,δ−2,...,δ −d+1). It is easy to check that for any H ∈Hδ, the map\nHk ↦→uTHk is one-to-one, and\nuTHγ(1) ∈\n[\n(n−1)\nd−1∑\ni=0\nδ−i : δ: (n−1)\nd−1∑\ni=0\nδ−i + δ−d+1 −δ\n]\n,\nuTHγ(i) ∈\n[\n(i−2)\nd−1∑\ni=0\nδ−i : δ: (i−2)\nd−1∑\ni=0\nδ−i + δ−d+1 −δ\n]\n, for i∈[2 : n].\n(7)\nHence, for each column Hk, the inner product uTHk is in an interval disjoint from the other columns.\nThus, uTHk can be thought as a “column id” that identiﬁes the column’s original input valueGk as\nwell as its position k. Note furthermore that for any H ∈Hδ,\nuTHγ(2) <uTHγ(3) <··· <uTHγ(n) <uTHγ(1). (8)\nE.2.2 Construction of layers\nGiven these preliminaries, we now describe our construction of gc. Recall from Assumption 1.2 that\nthe permutation γsatisﬁes γ(i−1) ∈⋃p\nl=1 Al\nγ(i) for i∈[2 : n]. From this, for i∈[2 : n] we let\n18\nli ∈[p] be any index such that γ(i−1) ∈Ali\nγ(i). For simplicity of notation, let zk := uTHk for\nk∈[n] and ∆ = ∑d−1\ni=0 δ−i.\nNext, starting from i= 2, we want to sequentially stack δ−d sparse selective shift operations\nΨli(·; δ−d,b −δ/2,b + δ/2),\nin increasing order of b∈\n[\n(i−2)∆ : δ: (i−2)∆ + δ−d+1 −δ\n]\n. That is, we want to add sparse\nattention layers with sparsity patterns Ali\nγ(i) that apply the selective shift operation to each possible\nvalue of zγ(i). Recall that the sparsity patterns have to cycle from A1\nk to Ap\nk, so we have to place\nother remaining p−1 sparsity patterns (whose indices are not li) in between the Ψli layers. This can\nbe done by setting all the other sparse attention layers to be the identity. This way, we stack a total of\npδ−d sparse attention layers for i= 2, another pδ−d for i= 3, and so on, up to i= n.\nAfter these layers, we further stack sall-max-shift operations. For i= 1,...,s , we add all-max-shift\noperations of the form\nΩ(i−1) mod p+1(·; 2snδ−nd−1).\nHere, the superscript (i−1) mod p+ 1 is there to make sure that we cycle through the sparsity\npatterns from 1 to p, until we stack slayers in total. This ﬁnishes the construction of our function gc\ncomposed of p(n−1)\nδd + ssparse self-attention layers.\nE.2.3 Selective shift operations\nWe now explain how these stacked self-attention layers implement a contextual mapping. This\nsubsection will consider the selective shift operations part; all-max-shift operations are described\nin the next subsection. Suppose that after the input H ∈Hδ is processed through the ﬁrst p(n−1)\nδd\nlayers, we get ˜H ∈Rd×n at the output. We will show at the end of this subsection that the map\nH ↦→uT˜Hγ(n) is a one-to-one map for column γ(n), so the selective shift operations compute a\n“unique id” for each possible input sequenceH ∈Hδ.\nFirst selective shift. First consider the ﬁrst pδ−d layers. Omitting layers that are identity, they\nare essentially selective shift operations Ψl2 (·; δ−d,b −δ/2,b + δ/2) for b ∈[0 : δ : δ−d+1 −δ].\nSince [0 : δ : δ−d+1 −δ] is the set of possible values of zγ(2), these layers perform selective shift\noperation on the γ(2)-th column without changing the other columns. Each possible value of Hγ(2)\nundergoes one and only shift operation (by the corresponding layer with b= uTHγ(2)), by which\nthe (1,γ(2))-th entry of the input is updated.\nRecall by Assumption 1.2 thatγ(1) ∈Al2\nγ(2), and that zγ(1) and zγ(2) are the maximum and minimum\nover the whole sequence z1,...,z n (see (8)). By Assumption 1.1 we also have γ(2) ∈Al2\nγ(2). Since\nboth γ(1) and γ(2) are in Al2\nγ(2), the maximum and minimum value ofzj := uTHj’s overj ∈Al2\nγ(2)\nare zγ(1) and zγ(2), respectively. Therefore, the (1,γ(2))-th entry of the input matrix is shifted up as\nfollows:\n˜H1,γ(2) := H1,γ(2) + δ−d(zγ(1) −zγ(2)).\nLet ˜Hγ(2) be the γ(2)-th column after the shift operation has shifted H1,γ(2) to ˜H1,γ(2). Then, deﬁne\n˜zγ(2) := uT˜Hγ(2) = zγ(2) + δ−d(zγ(1) −zγ(2)).\nNote that ˜zγ(2) >zγ(1) because\nzγ(2) + δ−d(zγ(1) −zγ(2)) >zγ(1) ⇔(δ−d −1)(zγ(1) −zγ(2)) >0,\nwhich is true. Therefore, ˜zγ(2) becomes the new maximum among the current values\nzγ(1),˜zγ(2),zγ(3),...,z γ(n), and the new minimum element is zγ(3).\nSecond selective shift. We now consider the nextpδ−d layers, which are essentially Ψl3 (·; δ−d,b−\nδ/2,b + δ/2) for b∈[∆ : δ: ∆ +δ−d+1 −δ]. They apply the shift operation to the γ(3)-th column.\nSince we have γ(2),γ(3) ∈Al3\nγ(3), the shift operation similarly yields\n˜zγ(3) := zγ(3) + δ−d(˜zγ(2) −zγ(3)) = zγ(3) + δ−d(zγ(2) −zγ(3)) + δ−2d(zγ(1) −zγ(2)).\n19\nWe can also show ˜zγ(3) >˜zγ(2), because\nzγ(3) + δ−d(˜zγ(2) −zγ(3)) >˜zγ(2) ⇔(δ−d −1)(˜zγ(2) −zγ(3)) >0.\nSo after this operation ˜zγ(3) and zγ(4) are the new maximum and minimum over the updated sequence\nzγ(1),˜zγ(2),˜zγ(3),zγ(4),...,z γ(n).\nRepeating the process. The same process continues. The next pδ−d layers shifts the γ(4)-th\ncolumns and results in ˜zγ(4) which is greater than ˜zγ(3). After the ﬁrst p(n−1)δ−d layers, all\ncolumns except γ(1)-th column have been shifted, resulting in zγ(1),˜zγ(2),..., ˜zγ(n) satisfying\n(n−1)∆ ≤zγ(1) <˜zγ(2) <··· <˜zγ(n). (9)\nLet us denote the output of the p(n−1)δ−d-th layer as ˜H.\nSelective shifts implement a one-to-one map. Next, we prove that the map from H ∈Hδ to\n˜zγ(n) := uT˜Hγ(n) = zγ(n) +\nn−1∑\ni=1\nδ−id(zγ(n−i) −zγ(n+1−i))\nis one-to-one. Recall that for each column Hk, the map Hk ↦→ uTHk =: zk is one-to-one.\nAlso, permutation of columns is one-to-one, which implies that it sufﬁces to show that the map[zγ(1) ... z γ(n)\n]\n↦→˜zγ(n) is one-to-one.\nSuppose we have two sequences\n[zγ(1) ... z γ(n)\n]\nand\n[z′\nγ(1) ... z ′\nγ(n)\n]\nthat map to the same\nvalue of ˜zγ(n) = ˜z′\nγ(n). Then,\n0 = ˜zγ(n) −˜z′\nγ(n) = zγ(n) −z′\nγ(n) +\nn−1∑\ni=1\nδ−id(zγ(n−i) −zγ(n+1−i) −z′\nγ(n−i) + z′\nγ(n+1−i)).\nSuppose zγ(n) ̸= z′\nγ(n). Since they both lie inside [(n−2)∆ : δ: (n−2)∆ + δ−d+1 −δ], we have\n−δ−d+1 + δ≤zγ(n) −z′\nγ(n) ≤δ−d+1 −δ.\nNote that all the terms other than zγ(n) −z′\nγ(n) are of “coarser resolution.” For example, the ﬁrst term\nδ−d(zγ(n−1) −zγ(n) −z′\nγ(n−1) + z′\nγ(n))\nin the summation can only take values 0,δ−d+1,−δ−d+1,2δ−d+1,−2δ−d+1,... , so it can never\ncancel the difference zγ(n) −z′\nγ(n) and make the sum ˜zγ(n) −˜z′\nγ(n) zero. This implies that zγ(n) =\nz′\nγ(n) must hold.\nNext, suppose zγ(n−1) ̸= z′\nγ(n−1). Since we have zγ(n) = z′\nγ(n),\n−δ−2d+1 <δ−d(zγ(n−1) −zγ(n) −z′\nγ(n−1) + z′\nγ(n)) = δ−d(zγ(n−1) −z′\nγ(n−1)) <δ−2d+1.\nBut similarly, any other terms in the summation have coarser resolution than δ−2d+1, so they cannot\ncancel the difference δ−d(zγ(n−1) −z′\nγ(n−1)). Thus zγ(n−1) = z′\nγ(n−1) must hold. Repeating the\nsame argument up to γ(1) proves that the two sequences must be equal:\n[zγ(1) ... z γ(n)\n]\n=[z′\nγ(1) ... z ′\nγ(n)\n]\n. This proves that the map H ↦→˜zγ(n) is one-to-one and ˜zγ(n) can be seen as\nthe unique id for the input sequence H ∈Hδ.\nE.2.4 All-max-shift operations\nNext, we explain the operation of the sall-max-shift layers. Recall from Assumption 1.3 that any\ntoken can attend to all the other tokens after ssteps, either directly or indirectly. Also recall from\nthe last subsection that the input to the ﬁrst all-max-shift layer is ˜H, and the maximum entry of\nuT˜H is ˜zγ(n), the unique id for input H. From the statement of Lemma 7, the output after the s\nall-max-shift operations for input H is denoted as gc(H). In this subsection, we show that through s\nall-max-shift operations, the maximum ˜zγ(n) will propagate to all tokens and be a “dominant” term,\nwhich determines the interval that uTgc(H) lies in. As a result, we can show Properties 7.1 and 7.2\nof gc at the end.\n20\nSome preliminaries. Note that the unique id ˜zγ(n) has the following upper bound:\n˜zγ(n) := zγ(n) +\nn−2∑\ni=1\nδ−id(zγ(n−i) −zγ(n+1−i)) + δ−(n−1)d(zγ(1) −zγ(2))\n≤zγ(n) + δ−d\nn−2∑\ni=1\n(zγ(n−i) −zγ(n+1−i)) + δ−(n−1)d(zγ(1) −zγ(2))\n= zγ(n) + δ−d(zγ(2) −zγ(n)) + δ−(n−1)d(zγ(1) −zγ(2))\n= δ−(n−1)dzγ(1) −(δ−(n−1)d −δ−d)zγ(2) −(δ−d −1)zγ(n)\n≤δ−(n−1)dzγ(1) ≤δ−(n−1)d((n−1)∆ + δ−d+1 −δ)\n≤δ−(n−1)d(n−1 + δ)(δ−d −1) ≤δ−nd −δ (10)\nwhere we used ∆ := ∑d−1\ni=0 δ−i = δ−d−1\nδ−1−1 ≤δ−d −1. A similar bound\n˜zγ(i) ≤nδ−id −δ (11)\nalso holds from a similar derivation. Next, recall from Assumption 1.3 the deﬁnitions\nS1\nk := A1\nk, St\nk :=\n⋃\nj∈A(t−1) mod p+1\nk\nSt−1\nj ,\nand that there exists s≥1 such that, for all k∈[n], Ss\nk = [n]. Finally, the following inequality will\nbe useful throughout: for any integer s≥1,\n(2s+ 1\n2s\n)\n≤\n(2s+ 1\n2s\n)2\n≤···≤\n(2s+ 1\n2s\n)s\n≤2. (12)\nLet us now describe the operation that the all-max-shift layers Ω(i−1) mod p+1(·; 2snδ−nd−1), i =\n1,...,s , carry out.\nFirst all-max-shift. The input to the ﬁrst all-max-shift layer is ˜H. Let the output of the layer be\nM1. Recall that uT˜H consists of values zγ(1),˜zγ(2),..., ˜zγ(n), which are all strictly greater than 0\nand strictly less than nδ−nd (by (10)). So, for each column k∈[n], the layer update reads\nM1\n1,k := ˜H1,k + 2snδ−nd−1 max\nj∈A1\nk\nuT˜Hj = ˜H1,k + 2snδ−nd−1uT˜Hj1\nk\n,\nwhere j1\nk := arg maxj∈A1\nk\nuT˜Hj. After the update, uTM1\nk is “dominated” by2snδ−nd−1uT˜Hj1\nk\n,\nmeaning that for any k,k′∈[n],\nuT˜Hj1\nk\n<uT˜Hj1\nk′ =⇒ uTMk <uTMk′.\nThis is because the minimum gap between different values of uT˜Hj1\nk\nis at least δ, and we have\nuT˜Hk <nδ−nd <2snδ−nd−1 ·δ,\nso if uT˜Hj1\nk\n<uT˜Hj1\nk′, that solely determines the order uTMk <uTMk′ because uT˜Hk cannot\nreverse it. Also, by the deﬁnition of j1\nk, for any index set B∈ [n] we have\nmax\ni∈B\nuT˜Hj1\ni\n= max\nj∈⋃\ni∈BA1\ni\nuT˜Hj. (13)\nIf s≥2, we move on to the second layer.\nSecond all-max-shift. At the second all-max-shift, we have sparsity patterns A1 mod p+1\nk . Let us\nthe output of this layer as M2. For each column k∈[n], the layer update reads\nM2\n1,k := M1\n1,k + 2snδ−nd−1 max\nj∈A1 mod p+1\nk\nuTM1\nj = M1\n1,k + 2snδ−nd−1uTM1\nj2\nk\n,\n21\nwhere j2\nk := arg maxj∈A1 mod p+1\nk\nuTM1\nj. If we look at the update more closely, we can apply (13)\nand get\nuTM2\nk = uT˜Hk + 2snδ−nd−1uT˜Hj1\nk\n+ 2snδ−nd−1(uT˜Hj2\nk\n+ 2snδ−nd−1 max\ni∈A1 mod p+1\nk\nuT˜Hj1\ni\n)\n= uT˜Hk + 2snδ−nd−1(uT˜Hj1\nk\n+ uT˜Hj2\nk\n) + (2snδ−nd−1)2 max\nj∈S2\nk\nuT˜Hj.\nAgain, the last term dominates the rest of the terms in uTM2\nk, because the minimum gap between\ndifferent values of maxj∈S2\nk\nuT˜Hj is at least δ, and\nuTM2\nk −(2snδ−nd−1)2 max\nj∈S2\nk\nuT˜Hj = uT˜Hk + 2snδ−nd−1(uT˜Hj1\nk\n+ uT˜Hj2\nk\n)\n<(1 + 4snδ−nd−1)nδ−nd ≤(1 + 4s)n2δ−2nd−1 ≤(2snδ−nd−1)2 ·δ= 4s2n2δ−2nd−1.\nThe last inequality holds due to inequality (12), because\n(2s+ 1\n2s\n)2\n≤2 ⇔1 + 4s≤4s2\nis true for s≥2.\nRemaining all-max-shifts. If s≥3, we move on to the third layer, which outputs M3. Similarly,\nwe can show that uTM3\nk is dominated by (2snδ−nd−1)3 maxj∈S3\nk\nuT˜Hj because the rest of the\nterms in uTM3\nk is strictly upper-bounded\nuTM3\nk −(2snδ−nd−1)3 max\nj∈S3\nk\nuT˜Hj <(1 + 3·2snδ−nd−1 + 3 ·(2snδ−nd−1)2)nδ−nd−1,\nwhich can then be shown to be smaller than (2snδ−nd−1)3 ·δ:\n(1 + 3·2snδ−nd−1 + 3·(2snδ−nd−1)2)nδ−nd ≤(1 + 6s+ 12s2)n3δ−3nd−2 ≤8s3n3δ−3nd−3 ·δ.\nThe last inequality is due to the fact that 1 + 6s+ 12s2 ≤8s3 for s≥3, which can derived from\n(12). Repeating this process, after all slayers we get Ms, and uTMs\nk is dominated by\n(2snδ−nd−1)smax\nj∈Ss\nk\nuT˜Hj = (2snδ−nd−1)smax\nj∈[n]\nuT˜Hj = (2snδ−nd−1)s˜zγ(n).\nThis is because the remaining terms in uTMs\nk can be strictly upper-bounded\nuTMs\nk −(2snδ−nd−1)s˜zγ(n) <\n(s−1∑\ni=0\n(s\ni\n)\n(2snδ−nd−1)i\n)\nnδ−nd,\nwhich is then dominated by the smallest difference possible in (2snδ−nd−1)s˜zγ(n):\n(s−1∑\ni=0\n(s\ni\n)\n(2snδ−nd−1)i\n)\nnδ−nd ≤\n(s−1∑\ni=0\n(s\ni\n)\n(2s)i\n)\n(nδ−nd−1)s−1nδ−nd\n= ((1 + 2s)s −(2s)s)(nδ−nd−1)s ·δ≤(2snδ−nd−1)s ·δ.\nThe last inequality used (1 + 2s)s −(2s)s ≤(2s)s, derived from (12).\nE.2.5 Verifying Properties 7.1 and 7.2\nAfter these all-max-shift operations, we deﬁne the output Ms of the last all-max-shift layers to be\nthe output of the function gc for input H, i.e., gc(H) := Ms.\nProperty 7.1 requires that for any H ∈Hδ, all the components uTgc(H) need to be distinct. This is\ntrue, because for each column of uTgc(H), we have\nuTgc(H)k mod 2snδ−nd = uT˜Hk.\n22\nThis is because anything added by the all-max-shift operations is an integer multiple of 2snδ−nd,\nand uT˜Hk <nδ −nd <2nδ−nd for all k. Recall that ˜H is the input matrix for the ﬁrst max-shift\noperation, and that the components of uT˜H are zγ(1),˜zγ(2),..., ˜zγ(n), which were shown to be\ndistinct by (9). Since uTgc(H)k produce distinct outputs for a mod operation, they themselves have\nto distinct. This proves Property 7.1.\nAlso, by the “domination” argument in the previous subsection, the outputgc(H) has the property\nthat for any column, uTgc(H)k lies inside an interval determined by ˜zγ(n), the unique id for the\ninput H:\nuTgc(H)k ∈\n[\n(2snδ−nd−1)s˜zγ(n),(2snδ−nd−1)s(˜zγ(n) + δ)\n)\n,\nand these intervals do not overlap because any different values of ˜zγ(n) must differ by at least δ. This\nmeans that for any input H,H′∈Hδ, the components in uTgc(H) and uTgc(H′) lie in disjoint\nintervals. Together with Property 7.1, this proves Property 7.2.\nE.3 Proof of Lemma 8\nTo prove this lemma, we implement a token-wise function that maps\ngtkn\nv (gc(H)k) = f(H −E)k,\nfor all H ∈Hδ and k ∈[n]. From the construction of Lemma 7, there are n|Hδ|= n\nδdn distinct\nvalues of uTgc(H)k, and different values of uTgc(H)k differ by at least δ. The implementation of\ngtkn\nv can be done by stacking feed-forward layers so that each layer maps one unique number to the\ncorresponding output column.\nMore precisely, choose any H ∈Hδ. For each of the nvalues of uTgc(H)k, we add one feed-\nforward layer of the form\nZ ↦→Z+(f(H−E)k−gc(H)k)φ′(uTZ−uTgc(H)k1T\nn), φ′(t) =\n{0 t< −δ/2 or t≥δ/2,\n1 −δ/2 ≤t<δ/ 2.\nThis layer updates any column j of its input Z that satisﬁes uTgc(H)k −δ/2 ≤ uTZj <\nuTgc(H)k + δ/2, without modifying any other columns that are out of this range.\nWe stack these layers for all possible values ofH ∈Hδ. After n\nδdn such layers, we get the desired\nfunction gv that satisﬁes\ngv(Z) =\n[\ngtkn\nv (Z1) ··· gtkn\nv (Zn)\n]\n,\nwhere for all H ∈Hδ and k∈[n],\ngtkn\nv (gc(H)k) = f(H −E)k.\nF Proof of Lemma 4 (Step 3 in § 4.1)\nIn this section, we describe how the modiﬁed sparse Transformer networkg∈ST\n2,1,1\nconstructed in\nLemma 3 can be approximated with an original sparse Transformer network g∈ST 2,1,4. Recall that\ngis a “modiﬁed” sparse Transformer network, which employ the hardmax σH operators in place of ρ\noperators in sparse self-attention layers and piecewise linear activations φ∈Φ instead of ReLUs in\nfeed-forward layers. The goal of this lemma is to approximate the functiong= gv ◦gc ◦gq ∈ST\n2,1,1\nwith a standard sparse Transformer g = ˜gv ◦˜gc ◦˜gq ∈ST 2,1,4 with accuracy dp(g,g) ≤ϵ/2. As\nthe construction of gconsists of three steps, we will approximate each of them step by step. The\nwhole intuition behind the proof is that as long as we are considering Lp approximation, we can\napproximate σH and φ∈Φ as closely as we want with ρand ReLUs, respectively. However, as the\nproof will show, controlling the aggregated error over layers is not a trivial job.\nF.1 Approximating the quantization function gq (Lemma 6)\nWe ﬁrst consider approximating gq from Lemma 6 with a standard feed-forward layer counterpart,\n˜gq. Recall from § E.1 that the modiﬁed feed-forward layers used in gq are of the form\nZ ↦→Z + e(i)φ((e(i))TZ −kδ1T\nn), φ(t) =\n{0 t< 0 or t≥δ,\n−t 0 ≤t<δ, (14)\n23\nfor i∈[d] and k ∈[0 : n/δ−1]. Note that the activation φ∈Φ can be closely approximated by\nthree ReLUs:\n˜φα(t) := −ReLU(t) + 1\nαReLU(t−(1 −α)δ) −1 −α\nα ReLU(t−δ)\n=\n\n\n\n0 t≤0 or t≥δ,\n−t 0 ≤t≤(1 −α)δ,\n1−α\nα (t−δ) (1 −α)δ≤t≤δ,\nwhere 0 < α <1. Note that ˜φα(t) = φ(t) except for an interval ((1 −α)δ,δ), and by shrinking\nα >0 this interval can be made arbitrarily small. Consider approximating the layers (14) with\nstandard feed-forward layers, by replacing φwith its approximation ˜φα. Let the resulting function be\n˜gq ∈ST 2,1,3.\nThen, it is easy to check that gq(X + E) = ˜gq(X + E) holds if all coordinates of X ∈[0,1)d×n\nare in the intervals of the form [kδ,(k+ 1 −α)δ] for some k ∈[0 : n/δ−1]; i.e., the intervals in\nwhich ˜φα perfectly approximates φ. The Lebesgue measure of the set of such inputs X is\n((1 −α)δ)nd × 1\nδnd = (1 −α)nd,\nand this can be made arbitrarily close to 1 by makingαsmall. As a result, “most” of the inputX ∈D\nsatisﬁes gq(X + E) = ˜gq(X + E) ∈Hδ, while a small fraction (of measure at most 1 −(1 −α)nd)\ncan map to some other values. For most of the remaining of the proof, we will consider the fraction\nof inputs mapped correctly to Hδ and bound their approximation error. We will come back to the\n1 −(1 −α)nd fraction at the end of the proof.\nF.2 Approximating the contextual mapping gc (Lemma 7)\nLet us now consider approximating the contextual mapping gc in Lemma 7, constructed using the\nhardmax σH operators, with the standard sparse self-attention layers employing ρoperator. We will\ncall the approximation ˜gc. Recall that ρsatisﬁes Assumption 2:\nAssumption 2. For any ζ >0 and η∈(0,1], ∃t> 0 such that, for any column input v satisfying\nvj∗−maxj̸=j∗vj ≥ζ(where j∗= arg maxjvj), we have ρ[tv]j∗ ≥1 −ηand ∑\nj̸=j∗ρ[tv]j ≤η.\nThis means that ρcan closely approximate σH in the sense that whenever the input vector v to the\nρoperator has a maximum element vj∗ by some margin ζ, then the j∗-th component of the output\nρ[tv] is close to 1, while the other components of ρ[tv] are close to 0.\nRecall that gc consists of two parts. The ﬁrst part is a composition of sparse selective shift operations,\nand the second is a composition of all-max-shift operations. We will ﬁrst examine how “errors” are\nintroduced when σH is replaced with ρin both operations, discuss how the errors accumulate, and\nshow how to choose the right ζand ηto control the errors in the approximation ˜gc.\nErrors introduced by ρ: Sparse selective shift operation. Recall that the key component in both\nthe selective shift operation and all-max-shift operation is the sparse attention head ψl(·), which\ncomputes its k-th column as the following:\nψl(Z; bQ)k := uTZAl\nk\nσH[(uTZAl\nk\n)T(uTZk −bQ)] =\n{\nmaxj∈Al\nk\nuTZj if uTZk >bQ,\nminj∈Al\nk\nuTZj if uTZk <bQ.\nNow suppose we replaced σH with ρsatisfying Assumption 2. Suppose each entry in uTZ differs\nat least by δ, which is true in the construction of gc. We choose ζ = δ/2 and some 0 <η <1, and\ncorresponding t> 0. Then, replace σH[·] with ρ[t·] and deﬁne\n˜ψl(Z; bQ)k := uTZAl\nk\nρ[t(uTZAl\nk\n)T(uTZk −bQ)].\nIf uTZk >bQ, it is easy to check that ˜ψl(Z; bQ)k satisﬁes\n(1 −η) max\nj∈Al\nk\nuTZj + η min\nj∈Al\nk\nuTZj ≤˜ψl(Z; bQ)k ≤max\nj∈Al\nk\nuTZj. (15)\n24\nSimilarly, if uTZk <bQ, we have\nmin\nj∈Al\nk\nuTZj ≤˜ψl(Z; bQ)k ≤(1 −η) min\nj∈Al\nk\nuTZj + ηmax\nj∈Al\nk\nuTZj.\nNow consider the approximate sparse selective shift operator ˜Ψl, implemented with ˜ψl. For bQ <b′\nQ,\nwe deﬁne\n˜Ψl(Z; c,bQ,b′\nQ) := Z +\n[\nce(1) −ce(1)]\n[\n˜ψl(Z; bQ)\n˜ψl(Z; b′\nQ)\n]\n.\nFor any column Zk satisfying bQ <uTZk <b′\nQ, we have\n(1 −2η)\n(\nmax\nj∈Al\nk\nuTZj −min\nj∈Al\nk\nuTZj\n)\n≤˜ψl(Z; bQ)k −˜ψl(Z; b′\nQ)k ≤max\nj∈Al\nk\nuTZj −min\nj∈Al\nk\nuTZj,\nand for any column Zk satisfying uTZk /∈[bQ,b′\nQ], we get\n|˜ψl(Z; bQ)k −˜ψl(Z; b′\nQ)k|≤ η\n(\nmax\nj∈Al\nk\nuTZj −min\nj∈Al\nk\nuTZj\n)\n.\nRecall that for the hardmax σH version, we had\nψl(Z; bQ)k −ψl(Z; b′\nQ)k =\n{\nmaxj∈Al\nk\nuTZj −minj∈Al\nk\nuTZj if bQ <uTZk <b′\nQ,\n0 if uTZk /∈[bQ,b′\nQ].\nFrom this observation, the approximation error ˜Ψl−Ψl of the selective shift operator on the (j,k)-th\nentry of the output can be bounded as follows:\n˜Ψl(Z; c,bQ,b′\nQ)j,k −Ψl(Z; c,bQ,b′\nQ)j,k ∈\n\n\n\n[−2cηDl\nk,0] if j = 1,uTZk ∈(bQ,b′\nQ),\n[−cηDl\nk,cηDl\nk] if j = 1,uTZk /∈[bQ,b′\nQ],\n{0} if j ̸= 1,\nwhere we used Dl\nk := maxj∈Al\nk\nuTZj −minj∈Al\nk\nuTZj for simplicity.\nErrors introduced by ρ: All-max-shift operation. Next, we examine the approximation error of\nthe all-max-shift operation introduced by replacement of σH with ρ. Let us deﬁne the approximate\nall-max-shift operation ˜Ωl:\n˜Ωl(Z; c) = Z + ce(1) ˜ψl(Z; 0).\nFrom (15), we can check that the approximation error ˜Ωl −Ωl of the all-max-shift operation is\nbounded as\n˜Ωl(Z; c)j,k −Ωl(Z; c)j,k ∈\n{[−cηDl\nk,0] if j = 1,\n{0} if j ̸= 1.\nErrors in selective shift operations. Given these approximation error bounds of single operations,\nwe now analyze the accumulation of errors through multiple layers. We ﬁrst consider the ﬁrst pδ−d\nself-attention layers in gc. Recall that they consist of selective shift layersΨl2 (·; δ−d,b−δ/2,b+δ/2)\nfor b∈[0 : δ: δ−d+1 −δ] and (p−1)δ−d identity layers. A natural way to approximate these layers\nwith standard self-attention layers is to use approximate layers ˜Ψl2 (·; δ−d,b −δ/2,b + δ/2), with\nsufﬁciently large t> 0. As we have seen above, there is no error introduced by ρexcept for the ﬁrst\nrow. Thus, we will analyze the approximation error of ˜Ψl2 (·; δ−d,b −δ/2,b + δ/2) for the ﬁrst row\nonly.\nLet us remind the readers how the ﬁrst selective shift operation (done by the ﬁrst pδ−d layers)\noriginally worked in gc. The input to gc is H, and we deﬁne zk := uTHk and ∆ = ∑d−1\ni=0 δ−i.\nRecall from Eqs. (7) and (8) in § E.2 that\n0 ≤zγ(2) <zγ(3) <··· <zγ(n) <zγ(1) ≤(n−1)∆ + δ−d+1 −δ <nδ−d\n25\nand zγ(2) ∈[0 : δ: δ−d+1 −δ], so zγ(2) will undergo the selective shift by one of the self-attention\nlayers, which updates the (1,γ(2))-th entry of the input. Let ˜Hγ(2) be the updated value of the\ncolumn and ˜zγ(2) := uT˜Hγ(2). The new sequence satisﬁes\n∆ ≤zγ(3) <··· <zγ(n) <zγ(1) <˜zγ(2) <nδ−2d,\nwhere the strict upper bound on ˜zγ(2) is from Eq. (11).\nIn case of the approximation ˜Ψl2 , we have seen that the error depends on the gap between maximum\nand minimum of uTZj’s, and this gap may grow larger as error accumulates; in the worst case, it\nmay grow exponentially. To see this, suppose a0 and b0 are the maximum and minimum value of\nuTZj’s, and they go through a selective shift operation, but they do not belong to the range of the\noperation (bQ,b′\nQ). Then, a0 and b0 will be updated to a1 and b1, which are bounded by\na1 ≤a0 + δ−dη(a0 −b0), b1 ≥b0 −δ−dη(a0 −b0).\nAfter the next layer, we get\na2 ≤a1 + δ−dη(a1 −b1) ≤a0 + δ−dη(a0 −b0) + δ−dη(1 + 2δ−dη)(a0 −b0),\nb2 ≥b1 −δ−dη(a1 −b1) ≥b0 −δ−dη(a0 −b0) −δ−dη(1 + 2δ−dη)(a0 −b0).\nSimilarly, after ksuch layers, we get\nak ≤a0 + (a0 −b0)δ−dη\nk−1∑\ni=0\n(1 + 2δ−dη)i,\nbk ≥b0 −(a0 −b0)δ−dη\nk−1∑\ni=0\n(1 + 2δ−dη)i,\nshowing that the gap ak −bk may grow exponentially in the worst case:\nak −bk ≤(1 + 2δ−dη)k(a0 −b0).\nIn the error-less case (σH), for any input sequence H, the maximum possible difference between\nmaximum and minimum of uTH is bounded above by nδ−d, and after one selective shift operation\nwas done on the γ(2)-th column, the difference is then bounded by nδ−2d. Therefore, the worst-case\npossible error introduced by ρ is bounded above by the sum of the worst-case errors calculated\nassuming that we started off with max-min difference nδ−2d. Using this observation, the error on\neach ﬁrst-row entry of the sequence after the ﬁrst pδ−d layers is bounded above by\n2nδ−2d ·δ−dη\nδ−d−1∑\ni=0\n(1 + 2δ−dη)i, (16)\nwhere a factor of 2 is introduced because when the selective shift operation is applied to the γ(2)-th\ncolumn, it may introduce an error which is twice the magnitude of the error introduced to the other\ncolumns. We want to make (16) smaller than δ\n8n. By Assumption 2, we can always choose t> 0 that\nsatisﬁes the assumption for\nζ = δ\n2, and η= 1\n2 δ2dlog\n(\n1 + δ2d˜δ\n8n2\n)\n>0, where ˜δ:= min\n{\nδ,21−1/pϵ\nn1/p\n}\n.\nUsing such t, we can control the total accumulated error by the ﬁrst pδ−d selective shift operations\nbelow\n˜δ\n8n:\n2nδ−2d ·δ−dη\nδ−d−1∑\ni=0\n(1 + 2δ−dη)i ≤2nδ−3dη(1 + 2δ−dη)δ−d\n−1\n(1 + 2δ−dη) −1\n= nδ−2d\n\n\n\n1 +\nlog\n(\n1 + δ2d˜δ\n8n2\n)\nδ−d\n\n\nδ−d\n−1\n\n≤nδ−2d\n(\nexp log\n(\n1 + δ2d˜δ\n8n2\n)\n−1\n)\n= nδ−2dδ2d˜δ\n8n2 =\n˜δ\n8n.\n26\nTherefore, after the ﬁrst pδ−d selective shift layers, the accumulated error for each entry of the ﬁrst\nrow is at most ˜δ/8n.\nWe can also apply similar arguments to the remaining selective shift layers. For example, for the j-th\nset of pδ−d selective shift layers where the operation is done on γ(j+ 1)-th column of the input, the\ngap between the maximum and the minimum, including the accumulated error from previous layers,\nis bounded above by nδ−(j+1)d. Therefore, for this set of layers, the maximum accumulated error is\nbounded by\n2nδ−(j+1)d ·δ−dη\nδ−d−1∑\ni=0\n(1 + 2δ−dη)i.\nSo, choosing t> 0 that satisﬁes Assumption 2 for η = δ\n2 and η = 1\n2 δ2dlog(1 + δ(j+1)d˜δ\n8n2 ), we can\ncontrol the accumulated error introduced by the pδ−d layers below δ\n8n:\n2nδ−(j+1)d ·δ−dη\nδ−d−1∑\ni=0\n(1 + 2δ−dη)i ≤2nδ−(j+2)dη(1 + 2δ−dη)δ−d\n−1\n(1 + 2δ−dη) −1\n≤nδ−(j+1)d\n\n\n\n1 +\nlog\n(\n1 + δ(j+1)d˜δ\n8n2\n)\nδ−d\n\n\nδ−d\n−1\n\n≤\n˜δ\n8n.\nIn total, the accumulated error by the ﬁrst p(n−1)/δd layers, which correspond to the selective shift\noperation part of the construction, is at most (n−1)˜δ\n8n ≤\n˜δ\n8 .\nErrors in all-max-shift operations. For all-max-shift operations, we approximate the hardmax\nσH all-max-shift operations Ωl(Z; nδ−nd) with its ρ-counterparts, ˜Ωl(Z; nδ−nd). We can similarly\nbound the accumulated error in the all-max-shift operations. Recall from § E.2 that during the\nwhole series of all-max-shift operations, the maximum entry in the sequence is upper-bounded by\n(2snδ−nd−1)snδ−nd and minimum entry is lower-bounded by(n−1)∆. Therefore, the gap between\nthe max and min elements, taking into consideration the errors from selective shift operations, is\nbounded from above by (2snδ−nd−1)snδ−nd. Then, using a similar argument as the select shift\noperation layers, the maximum error is bounded above by\n(2snδ−nd−1)snδ−nd ·nδ−ndη\ns−1∑\ni=0\n(1 + nδ−ndη)i,\nand we want to make it smaller than\n˜δ\n8 . By Assumption 2, we can always choose t> 0 that satisﬁes\nthe assumption for\nζ = δ\n2, and η= δnd\nsn log\n(\n1 + δs(nd+1)+nd˜δ\n2s+3ssns+1\n)\n>0.\nUsing such t, we can control the total accumulated error by the ﬁrst pδ−d selective shift operations\nbelow\n˜δ\n8 :\n(2snδ−nd−1)snδ−nd ·nδ−ndη\ns−1∑\ni=0\n(1 + nδ−ndη)i\n≤(2snδ−nd−1)snδ−nd ·nδ−ndη(1 + nδ−ndη)s −1\n(1 + nδ−ndη) −1\n= (2snδ−nd−1)snδ−nd\n\n\n\n1 +\nlog\n(\n1 + δs(nd+1)+nd˜δ\n2s+3ssns+1\n)\ns\n\n\ns\n−1\n\n\n≤(2snδ−nd−1)snδ−ndδs(nd+1)+nd˜δ\n2s+3ssns+1 =\n˜δ\n8.\n27\nSo far, we have analyzed the total accumulated error of approximating the contextual mapping\nfunction gc (constructed with hardmax σH) with an approximation ˜gc (constructed with ρ). We have\nseen that for any input H ∈Hδ, the approximation error can be controlled so that the error by\nthe selective shift operation part is at most ˜δ/8 and the all-max-shift operation part is at most ˜δ/8.\nTherefore, the total error of the (j,k)-th entry can be bounded as\n˜gc(H)j,k −gc(H)j,k ∈\n{\n[−\n˜δ\n4 ,\n˜δ\n4 ] j = 1,\n{0} j ̸= 1,\nfor any H ∈Hδ.\nF.3 Approximating the value mapping gv (Lemma 8)\nWe now consider the approximation of the value mappinggv with standard feed-forward layers. In\ngv, we implemented the function with layers of the form\nZ ↦→Z+(f(H−E)k−gc(H)k)φ′(uTZ−uTgc(H)k1T\nn), φ′(t) =\n{0 t< −δ/2 or t≥δ/2,\n1 −δ/2 ≤t<δ/ 2.\nSince the output of contextual mapping gc(H) and its approximation ˜gc(H) differ in only the ﬁrst\nrow and by ˜δ/4 ≤δ/4, one can approximate each layer in gv by replacing φ′with an approximation\n˜φ′, implementable with four ReLU’s:\n˜φ′(t) =\n\n\n\n0 t< −δ/2 or t≥δ/2,\n4\nδt+ 2 −δ/2 ≤t< −δ/4,\n1 −δ/4 ≤t<δ/ 4,\n−4\nδt+ 2 δ/4 ≤t<δ/ 2.\nLet ˜gv be the approximation of gv constructed this way. Because the error on ˜gc is bounded by ˜δ/4,\nthe error on the ﬁnal output ˜gv is also bounded by ˜δ/4. That is, for any H ∈Hδ,\n˜gv(˜gc(H))j,k −gv(gc(H))j,k ∈\n{\n[−\n˜δ\n4 ,\n˜δ\n4 ] j = 1,\n{0} j ̸= 1.\nHence, using ˜δ:= min\n{\nδ,21−1/pϵ\nn1/p\n}\n, we have\n∥˜gv(˜gc(H)) −gv(gc(H))∥p\np ≤n\n(˜δ\n4\n)p\n≤1\n2\n(ϵ\n2\n)p\n,\nfor all H ∈Hδ.\nF.4 Finishing the proof\nRecall from § F.1 that the approximated quantization function ˜gq maps most of the input X ∈D to\nH ∈Hδ, and a small fraction of them (of measure at most 1 −(1 −α)nd) to something else. Note\nnow that the original function g = gv ◦gc ◦gq and the approximation g = ˜gv ◦˜gc ◦˜gq are both\nbounded, so there is a global constant Bsuch chat ∥g(X + E) −g(X + E)∥p ≤Bfor all X ∈D.\nWe can divide the integral overD to two disjoint sets. The ﬁrst one D1 := {X ∈D |˜gq(X + E) ∈\nHδ}is the set of input X mapped to Hδ by ˜gq, and the other is its complement D2 = D \\D1.\ndp(g,g)p :=\n∫\nD\n∥g(X + E) −g(X + E)∥p\npdX\n=\n∫\nD1\n∥g(X + E) −g(X + E)∥p\npdX +\n∫\nD2\n∥g(X + E) −g(X + E)∥p\npdX\n≤1\n2\n(ϵ\n2\n)p\n+ (1 −(1 −α)nd)Bp.\nOne can make α close enough to 1 so that the second term is less than 1\n2\n(ϵ\n2\n)p\n. This makes\ndp(g,g) ≤ϵ/2, hence ﬁnishing the proof.\n28\nG Experimental setup\nG.1 Copying task\nWe generated the synthetic dataset for the copying task. The input sequence to the copying task has\nthe format 0s0s, where s is a 127 length sequence of symbols randomly sampled from the range of\n[0,127]. The training set contains 100K sequences, while the testing set contains 10K sequences.\nWe implement the copying task as a masked-LM [10] style prediction task by masking all the tokens in\nthe second half of the sequence. For the test examples, each masked token is predicted independently.\nFor the results reported in § 5, we experiment with bidirectional models, where each token can attend\nto both previous and future tokens.\nThe maximum sequence length is n= 256, and we use embedding dimension d= 256. The model\nhas 1 to 4 attention layers with h= 4 attention heads of size m= 64, followed by a feed-forward\nhidden layer of size r= 512. We train the model with the AdamW optimizer with weight decay and\nno dropout. We train the model using 3,000 warmup steps and a total of 500K training steps. The\nlearning rate is 1e−4. We use the batch size 1,024 on 8 TPUv3 chips.\nFor all sparsity patterns other than the RANDOM pattern, we choose the segment length wto be 16 for\nall patterns. This segment length results in the sparsest level for the STRIDED and FIXED patterns. In\nTable 1, we include the sparsity level as a reference. For this task, we report the prediction accuracy\nfor all the tokens.\nG.2 Language modeling\nFor the language modeling task, we train on the One Billion Word Benchmark [5] which contains\nalmost one billion tokens and a vocabulary of more than 800K tokens.\nWe use the Transformer model in the Tensor2Tensor framework [29]. We use a 12-block (cf. (2))\nTransformer, with embedding dimension d = 256, maximum sequence length n = 256, number\nof heads h= 8, head size m= 64, and feed-forward hidden layer size r = 1024. Since language\nmodeling task is auto-regressive (attending to only past tokens) in nature, we evaluate the (sparse)\nattention score matrices and mask them to be an upper-triangular matrix. We train the model with the\nAdafactor with weight decay. We train the model using 10K warmup steps and a total of 240K steps.\nWe use the batch size 4,096 on 8 TPUv2 chips.\nFor this task, we report the perplexity.\nG.3 Translation\nFor the translation task, we train on the WMT18 en-cs datasets (Europarl v7, Common Crawl\ncorpus, News Commentary v13, and CzEng), with a total of 15M pairs of sentences, and test on the\nnewstest2015 en-cs dataset, with 2,656 pairs.\nWe use the encoder-decoder architecture and apply the sparse attention on both encoder and decoder.\nWe use the Transformer model in the Tensor2Tensor framework [ 29] and the same setup as the\nlanguage modeling task, except for having 6 blocks in the Transformer networks, with head size\nm= 32 and having autoregressive patterns only in decoders.\nFor this task, we report the cased BLEU score.\nG.4 GLUE tasks\nFor the GLUE tasks, we use the pre-training and ﬁne-tuning framework [10]. Following Devlin et al.\n[10] we ﬁrst pre-train a BERTBASE model for 450K steps on the BooksCorpus [36] (800M words)\nand the English Wikipedia datasets (2,500M words). We later ﬁnetune the model on data from each\ntask separately. For each setting, we use the same sparsity pattern and head conﬁguration in both the\npre-training and the ﬁne-tuning stages. The sequence length is n= 128 in both stages.\nWe report the average accuracy of three runs on the dev set for all tasks. For each setting, we pre-train\na model and run ﬁne-tuning three times.\n29\nTable 2. Accuracy on the synthetic copying task when using an auto-regressive model. Percentages in\nparentheses mark the sparsity levels.\nSTRIDED FIXED STAR RANDOM\nDepth UNION\n(87%)\nMULTIHEAD\n(93%)\nSEQUENTIAL\n(93%)\nUNION\n(87%)\nMULTIHEAD\n(93%)\nSEQUENTIAL\n(93%) (87%) (90%)\n1-layer 0.79% 0.78% 0.78% 7.02% 7.04% 0.81% 0.77% 33.13%\n2-layer 12.40% 8.26% 1.57% 73.43% 13.24% 92.10% 12.32% 67.30%\n3-layer 94.50% 65.58% 60.88% 99.87% 70.82% 99.84% 14.03% 89.50%\n4-layer 100% 100% 98.40% 99.97% 99.16% 99.97% 31.19% 95.88%\n(a) WMT en-de\n (b) WMT de-en\nFigure 3. Comparison of sparsity patterns and different head conﬁgurations on the WMT de-en and\nen-de translation tasks.\n(a) CoLA\n (b) MRPC\nFigure 4. Comparison of sparsity patterns and different head conﬁgurations on the CoLA and MRPC\ntasks for the BERTBASE model.\nH Additional experimental results\nWe report additional experimental results in this section.\nH.1 Copying task\nWe include the results for the copying task using auto-regressive (unidirectional) models as in LM,\nwhere each token can only attend to previous tokens, in Table 2. In this case, the STAR pattern cannot\nattend to the last replay token. Indeed, the STAR pattern shows better performance when the model is\nbidirectional (cf. Table 1).\nH.2 Translation\nWe present experimental results of the translation tasks on the WMT English-German and German-\nEnglish datasets in Figure 3. We train on WMT18 (Europarl v7, Common Crawl corpus and News\nCommentary v13) and test on newstest 2015 datasets. The ﬁgures show similar trends to the results\non the WMT en-cs dataset in Figure 1b.\n30\nH.3 GLUE tasks\nFigure 4 presents the results comparing the sparsity patterns and the head conﬁgurations on the CoLA\nand MRPC tasks using the BERTBASE model. CoLA is a single-sentence classiﬁcation task, asking if\na sentence is a grammatical English sentence. MRPC is a sentence-pair classiﬁcation task, where\neach example is a pair of sentences and the label indicates whether the sentences are semantically\nequivalent.\n31",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7553133368492126
    },
    {
      "name": "Pairwise comparison",
      "score": 0.6305418014526367
    },
    {
      "name": "Computer science",
      "score": 0.606463611125946
    },
    {
      "name": "Quadratic equation",
      "score": 0.6059279441833496
    },
    {
      "name": "Sequence (biology)",
      "score": 0.570978581905365
    },
    {
      "name": "Algorithm",
      "score": 0.487884521484375
    },
    {
      "name": "Theoretical computer science",
      "score": 0.40271231532096863
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3494148254394531
    },
    {
      "name": "Mathematics",
      "score": 0.27469611167907715
    },
    {
      "name": "Voltage",
      "score": 0.0697157084941864
    },
    {
      "name": "Physics",
      "score": 0.05816715955734253
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 10
}