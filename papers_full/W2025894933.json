{
  "title": "Text Simplification Tools: Using Machine Learning to Discover Features that Identify Difficult Text",
  "url": "https://openalex.org/W2025894933",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2599013713",
      "name": "David Kauchak",
      "affiliations": [
        "Middlebury College"
      ]
    },
    {
      "id": "https://openalex.org/A317690994",
      "name": "Obay Mouradi",
      "affiliations": [
        "Claremont Graduate University"
      ]
    },
    {
      "id": "https://openalex.org/A1982910609",
      "name": "Christopher Pentoney",
      "affiliations": [
        "Claremont Graduate University"
      ]
    },
    {
      "id": "https://openalex.org/A2315761407",
      "name": "Gondy Leroy",
      "affiliations": [
        "Claremont Graduate University"
      ]
    },
    {
      "id": "https://openalex.org/A2599013713",
      "name": "David Kauchak",
      "affiliations": [
        "Middlebury College"
      ]
    },
    {
      "id": "https://openalex.org/A317690994",
      "name": "Obay Mouradi",
      "affiliations": [
        "Claremont Graduate University"
      ]
    },
    {
      "id": "https://openalex.org/A1982910609",
      "name": "Christopher Pentoney",
      "affiliations": [
        "Claremont Graduate University"
      ]
    },
    {
      "id": "https://openalex.org/A2315761407",
      "name": "Gondy Leroy",
      "affiliations": [
        "Claremont Graduate University",
        "University of Arizona"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6633661181",
    "https://openalex.org/W6602644681",
    "https://openalex.org/W2071099820",
    "https://openalex.org/W2066549963",
    "https://openalex.org/W1964424952",
    "https://openalex.org/W1659833910",
    "https://openalex.org/W6843671209",
    "https://openalex.org/W6704152115",
    "https://openalex.org/W2046121397",
    "https://openalex.org/W2119875902",
    "https://openalex.org/W2128573548",
    "https://openalex.org/W1790258399",
    "https://openalex.org/W6681158765",
    "https://openalex.org/W6674551184",
    "https://openalex.org/W6637463051",
    "https://openalex.org/W6602540667",
    "https://openalex.org/W6681837484",
    "https://openalex.org/W6680097875",
    "https://openalex.org/W2165578718",
    "https://openalex.org/W2125931430",
    "https://openalex.org/W1999061866",
    "https://openalex.org/W2019416425",
    "https://openalex.org/W6607749156",
    "https://openalex.org/W6604377103",
    "https://openalex.org/W2171575620",
    "https://openalex.org/W1996853800",
    "https://openalex.org/W2153081451",
    "https://openalex.org/W6632192099",
    "https://openalex.org/W1980754697",
    "https://openalex.org/W6668324382",
    "https://openalex.org/W2123109647",
    "https://openalex.org/W2083922802",
    "https://openalex.org/W1965883462",
    "https://openalex.org/W2023080089",
    "https://openalex.org/W2030334027",
    "https://openalex.org/W2038051713",
    "https://openalex.org/W2004483028",
    "https://openalex.org/W1919264805",
    "https://openalex.org/W6909306877",
    "https://openalex.org/W175146157",
    "https://openalex.org/W6680605462",
    "https://openalex.org/W2114286729",
    "https://openalex.org/W2127628633",
    "https://openalex.org/W6678852021",
    "https://openalex.org/W2098824882",
    "https://openalex.org/W6634328381",
    "https://openalex.org/W6696926454",
    "https://openalex.org/W2142580906",
    "https://openalex.org/W2126800506",
    "https://openalex.org/W1521626219",
    "https://openalex.org/W1704713987",
    "https://openalex.org/W190511925",
    "https://openalex.org/W2138238299",
    "https://openalex.org/W175030052",
    "https://openalex.org/W2070204206",
    "https://openalex.org/W2096533229",
    "https://openalex.org/W65111197",
    "https://openalex.org/W2147045128",
    "https://openalex.org/W2289836221",
    "https://openalex.org/W2344464929",
    "https://openalex.org/W2507992556",
    "https://openalex.org/W1574862351",
    "https://openalex.org/W1534215117",
    "https://openalex.org/W1566018662",
    "https://openalex.org/W109941524",
    "https://openalex.org/W2169818249",
    "https://openalex.org/W2133944470",
    "https://openalex.org/W60755955",
    "https://openalex.org/W3105605466"
  ],
  "abstract": "Although providing understandable information is a critical component in healthcare, few tools exist to help clinicians identify difficult sections in text. We systematically examine sixteen features for predicting the difficulty of health texts using six different machine learning algorithms. Three represent new features not previously examined: medical concept density, specificity (calculated using word-level depth in MeSH); and ambiguity (calculated using the number of UMLS Metathesaurus concepts associated with a word). We examine these features for a binary prediction task on 118,000 simple and difficult sentences from a sentence-aligned corpus. Using all features, random forests is the most accurate with 84% accuracy. Model analysis of the six models and a complementary ablation study shows that the specificity and ambiguity features are the strongest predictors (24% combined impact on accuracy). Notably, a training size study showed that even with a 1% sample (1,062 sentences) an accuracy of 80% can be achieved.",
  "full_text": "Text Simplification Tools: Using Machine Learning to  \nDiscover Features that Identify Difficult Text \n \nDavid Kauchak, PhD 1                   Obay Mouradi 2, Christopher Pentoney 2                  Gondy Leroy, PhD 3 2 \n         1Middlebury College                      2Claremont Graduate University                          3University of Arizona \n            Middlebury, VT                                      Claremont, CA                                                 T ucson, AZ \n       dkauchak@middlebury.edu   {obay.mouradi, ch ristopher.pentoney}@cgu.edu    gondyleroy@email.arizona.edu \n \nAbstract \nAlthough providing understandable information is \na critical component in healthcare, few tools exist to \nhelp clinicians identify diffic ult sections in text. We \nsystematically examine sixteen features for predicting \nthe difficulty of health texts using six different machine \nlearning algorithms. Three represent new features not \npreviously examined: medical concept density; \nspecificity (calculated using word-level depth in \nMeSH); and ambiguity (calculated using the number of \nUMLS Metathesaurus concepts associated with a \nword).  We examine these features for a binary \nprediction task on 118,000 simple and difficult \nsentences from a sentence-aligned corpus. Using all \nfeatures, random forests is the most accurate with 84% \naccuracy. Model analysis of the six models and a \ncomplementary ablation study shows that the \nspecificity and ambiguity features are the strongest \npredictors (24% combined impact on accuracy). \nNotably, a training size st udy showed that even with a \n1% sample (1,062 sentences) an accuracy of 80% can \nbe achieved. \n \n \n1. Introduction  \n \nLifespans continue to increase, survival rates for \nthose with many chronic diseases have drastically \nimproved, and more and mo re treatments are being \ndiscovered for a variety of illnesses.  However, \nhealthcare funding and the time availability of \npractitioners have not increased to match.  To combat \nthese conflicting views, improving patient health \nliteracy is becoming an increasingly important goal in \nhealthcare. Increased health literacy can improve \npreventive behaviors and access to suitable care by the \npopulation. It has been argued that for the Patient \nProtection and Affordable Care Act (ACA, a law that \ncame into effect in the US in 2010) to be successful, \nmore effort is needed to increase the health literacy of \nmillions of Americans [1]. Similarly, the Healthy \nPeople 2010 statement by the Department of Health \nand Human Services identified health literacy as an \nimportant national goal \n(http://www.healthypeople.gov/2010 ).   \nAn important factor necessary for improving health \nliteracy is providing suitable information that people \ncan understand. Kuijpers et al. [2] reviewed 18 studies \naiming to provide web-based, interactive patient \nempowerment tools for cancer patients. They found \nthat most tools included a strong educational \ncomponent. However, patients often do not understand \nthe information they are provided leading to \nsuboptimal health behaviors. Rudd [3] notes there is a \nmismatch between the skills of and demands on \npatients which can result in troublesome health \noutcomes and it is an ethical imperative to improve the \ninformation transfer.  Fincham [4] suggests that both \neducation of practitioners and outreach activities are \nnecessary to bridge the health literacy gap.  \nAlthough many methods for educating patients \nexist, text still remains one of the most cost-effective \nmethods of disseminating info rmation.  The challenge \nis that writing easy-to-understand text is difficult, and \nexisting tools aimed at simplif ying text have not been \nconvincingly shown to positively and significantly \naffect text understanding. While the intentions are \ngood, few existing approaches have been shown to \nincrease health literacy and improve health outcomes. \nWe advocate that evidence-based research is \nneeded to provide tools that support the clinical \npractitioners in writing easy to understand text and that \nsupport the patients in reading the information.  To this \nend, the long-term goal of our  project is to develop a \nwriting support tool for clinical practitioners that \nleverages modern technology. We use large \nvocabularies and machine learning to identify \nimportant traits and suggest easier alternatives.  We \nbelieve that the ability to provide appropriate text is \nbeneficial for individual patients as well as the \npopulation at large when they access documents \nprepared for the public. \nIn this paper, we focus on the problem of predicting \nthe difficulty of text and identifying the most important \ntext features contributing to that difficulty.  Such \nfeatures can be used to identify text sections requiring \n2014 47th Hawaii International Conference on System Science\n978-1-4799-2504-9/14 $31.00 © 2014 IEEE\nDOI 10.1109/HICSS.2014.330\n2616\n\nsimplification and then to  guide the simplification \nprocess [5, 6]. For example, Table 1 shows example \nsentences from a sentence aligned corpus of English \nWikipedia and Simple English Wikipedia.  The \nsentences generally convey the same meaning, but the \nsimple sentences are written more simply by including \nsimpler sentence structure, vocabulary and concepts.  \nWe explore a feature-based approach for predicting \ntext difficulty. Traditional readability formulas often \nutilize a very small number of features to predict the \ndifficulty of a text.  Instead, we utilize machine \nlearning approaches, which can integrate a much larger \nnumber of features that can capture a more varied \ncollection of text characteristics. In addition, the \nfeatures can be combined in a meaningful way by \nweighting based on usefulness.  By examining a \nfeature-based approach we can also understand which \nfeatures are most informative and can be used as part \nof future simplification tools. \nWe build upon previous approaches for predicting \ntext difficulty using machine learning in two key ways.  \nFirst, we introduce a number of new features including \nfeatures that capture the number of concepts used and \nthe specificity and ambiguity of the words used. To our \nknowledge, these features have not been previously \nexamined.  Second, we explore a broader range of \nmachine learning approaches to fully test the features \nin a broad range of settings and to examine how \ndifferent machine learning a pproaches perform in this \nproblem domain. \n \n2. Literature Review  \n \n2.1 Patient and Consumer Health Literacy \n \nMany researchers have focused on measuring \npatient health literacy so th at information provided to \nthe patient can be adjusted accordingly.  The Test of \nFunctional Health Literacy in Adults (TOFHLA) and \nits shortened version Shor t-TOFHLA (S-TOFHLA) [7, \n8] are among the most popul ar instruments. They \nrequire respondents to fill in the blanks in sentences by \nchoosing one of four word s. Another commonly used \ntest is the Rapid Estimate of Adult Literacy in \nMedicine (REALM) [9] which requires patients to read \nmedical terms out loud. A va riety of other tests have \nbeen developed, for exampl e, Chew [10] developed a \n3-item scale, validated using the S-TOFHLA. \nStudies evaluating the importance of health literacy \nin relation to health outcomes have shown a wide range \nof results. Al Sayah et al. [11] reviewed 24 studies \nwhere health literacy was measured as part of a \ndiabetes-related study. They report evidence for a \nrelationship between health literacy and diabetes \nknowledge and self-care but no evidence for a direct \nrelationship between health literacy and clinical \noutcomes. They reason that the existence of indirect \nrelationships, e.g., health literacy was related to \ncommunication quality with the healthcare provider, \nand the diversity of measurements used can explain \nthis lack of a strong direct relationship.  Similar results \nwere found for other medical conditions. For arthritis \npatients [12], health literacy was related to knowledge \nbut not directly to adherence to medicine. For patients \nwith hypertension, health l iteracy was not shown to \ncorrelate with adherence to treatment [13].  \nIn contrast, Omachi et al. [14] conducted structured \ntelephone interviews with 277 people. They reported \nthat lower health literacy was associated with worse \noutcomes for chronic obstructive pulmonary disease \n(COPD): worse COPD severity, higher helplessness, \nand higher likelihood of hospitalizations and \nemergency department visits. Sun et al.  [15] focused \non respiratory diseases and conducted a pathway model \nanalysis with the relationship between health literacy \nand behavior as one of the relations investigated. Based \nTable 1. Three example sentences pairs with the difficult (i.e. unsimplified) sentence and the \ncorresponding simple sentence. \nDifficult: \n \n \nSimple: \nMagnetic resonance imaging (MRI), or nucl ear magnetic resonance imaging (NMRI), or \nmagnetic resonance tomography (MRT) is a me dical imaging technique used in radiology \nto visualize detailed internal structures. \nMagnetic resonance imaging (MRI), or magnetic resonance imaging (NMRI), are machines \nthat doctors use to give a visual represen tation of soft tissue (flesh) inside the body. \n \nDifficult: \n \nSimple: \nPenicillin has since become the most widely us ed antibiotic to date, and is still used for \nmany gram-positive bacterial infections. \nPenicillin is a common antibiotic, used  to treat bacterial infections. \nDifficult: \nSimple: \nThe outer wall of the human heart is composed of three layers. \nThe heart has three layers. \n2617\non survey results from 3,222 respondents, they \nconclude that health literacy and prior knowledge are \nthe top determinants in health behavior. \n \n2.2 Text Readability  \n \nSimilar to health literacy, there are a variety \ninstruments for measuring the readability of text. The \ngoal of such instruments is to assign a rating indicating \nhow difficult an existing text is and indicate required \nliteracy levels needed to understand the information \npresented in a text. They are based on simple text \nsurface characteristics such as word and sentence \nlength, which are used as stand-ins for text complexity \n[16]. The most commonly used formulas are the \nFlesch-Kincaid grade level formulas [17], but others \nsuch as the Measure of Gobbledygook (SMOG), \nGunning-Fog index, DISCERN [18] and HON code \n(http://www.hon.ch/) are also used.  \nFor those with low health literacy, it is advocated \nthat text is rewritten in simple and plain language. \nWhile the Centers for Disease Control and Prevention \nprovide comprehensive advice in their guide for \ncreating easy-to-understand materials \n(http://www. cdc.gov/healthliteracy/pdf/Simply_Put.pdf\n), the need for easy-to use and efficient tools for \n(re)writing text result in a strong (over)reliance on \nreadability formulas. As a result, readability formulas \ncontinue to be used as the sole judgment tool to assess \ntext in a variety of settings  and topics. For example, \nthey are used to evaluate surveys [19], patient \ninformation leaflets provided by hospitals [20], or \nwebsite discussing a variety of topics such as ear tubes \n[21], speech and language difficulties [22], and \nnephrology articles on Wikipedia [23] among others. \nUnfortunately, the usability of readability formulas \nis limited and there is little evidence that the output of \nthese tools directly results in improved understanding \nby readers. Many studies re port a wide variety of \nresults demonstrating the difficulty of the problem. \nApplication of the formulas does not pinpoint the \ndifficult sections in a text and does not provide \nsuggested alternative writings. Furthermore, their \nratings have not convincingly been shown to correlate \nwith understanding, and in some cases, simplifying text \nusing the formulas negatively affects readability \nbecause writing style rather than content is changed \n[24].  Not surprisingly, incr easingly more concerns are \nraised about the effectiveness of these formulas for \nsimplifying consumer health texts [25].   \n \n2.3 Learned Readability Measures \n \nTo combat some of the drawbacks of static \nreadability formulas, recent work has explored learning \nreadability formulas from corpora with known \nreadability levels.  Features that may be associated \nwith text difficulty are extract ed and then used with the \nknown readability levels to learn a readability measure, \noften using machine learning methods.  By viewing the \nproblem as a computationa l learning problem, a much \nmore robust and exhaustive set of features can be \nsystematically explored, avoiding many of the \ndrawbacks of the existing static approaches.  These \nmodels have been shown to predict the readability \nlevels of text significantly better than static readability \nformulas [26].  The drawback  of these models is they \nrequire training data and ar e therefore domain specific \nbased on the training data [6]. \nIn addition to use as a prediction tool, learned \nreadability measures can also be a driver for \nidentifying features that are indicators of simplicity and \ncan be integrated in simplifi cation tools.  A variety of \nfeatures have been suggested. Most models include \nsome surface features similar to those used by the static \nformulas such as average word length or sentence \nlength [27, 28] along with othe r lexical features such as \noccurrence of words found in a simple lexicon [28].  \nSyntactic features including both part of speech and \nparse tree components have also been explored [28-\n30].  More importantly, higher level features that \ncapture more complex phenomena in the text such as \nthe occurrence of named entities [29] and language \nmodel scores [6, 31] have been incorporated into these \nmodels. \nBecause of the reliance of these approaches on \ntraining data annotated with readability level (or \ncollected from sources wher e different target reading \nlevels are known), different target audiences and \ndifferent text types have been examined.  Audiences \nhave included people with poor literacy [6], second \nlanguage learners [31] and people with cognitive \ndisabilities [32, 33].  Corpora used for training have \nincluded Wikipedia [28], books [27], magazines [29], \nand web pages [34]. \n \n2.4 Specificity and Ambiguity \n \nIn medicine, the plain language initiative \n(http://www.nih.gov/clearcommunication/plainlanguag\ne.htm) aims to provide in formation appropriate for \nconsumers and patients. Th is does not mean ‘dumbing \ndown’ text, but often require s use of non-medical terms \nsince most patients and consumers do not have a \nmedical background.  Intuitively,  for most readers, text \ncontaining increasingly more  technical and medical \nterms or words with different meanings in medicine, \nwill be increasingly difficult to understand. We aim to \ncapture this rationale with the development and \n2618\nevaluation of two new features: term specificity and \nterm ambiguity. \nWe view specificity as a measure of the technicality \nof a term in the medical domain.  For example, \ncompare the terms “heart” and “endocardium”. Both \nare terms related to the cardiovascular system and, in \nparticular the heart.  While “heart” is less specific, for \nmost people it is more accessible and more familiar.  A \nnumber of approaches have been previously suggested \nto measure word specificity including inverse \ndocument frequency [35], syntactic signals [36] and \nWordNet [37].  None of these approaches have been \nutilized for text readability nor were they designed for \nhealth-related concepts.  Our notion of specificity \nrelies on depth information within a word hierarchy of \nmedical terms.  Previous work has utilized similar \nword hierarchies in WordNet for text similarity [38], \nthough not to calculate word specificity and not for \nhealth-related content. \nWe view ambiguity as a measure of vagueness or \nuncertainty of the exact meaning of a term in the \nmedical domain, often referred to as semantic \nambiguity or lexical ambi guity. Semantic ambiguity \nhas been shown to result in slower response times, \nlonger processing times and longer fixation times [39].  \nThis delaying phenomena is sometimes referred to as \nthe “ambiguity disadvantage” [40].  This disadvantage \nis particularly prominent when the different possible \nmeanings of a term are sim ilar [41], as is common for \nmedical terms. Word ambiguity can also be a challenge \nfor many natural language processing applications, \nthough some progress has been made on automated \nword sense disambiguation [42, 43]. \n \n3. Methods \n \n3.1 Corpus \n \nMany corpora have been used to compare \nreadability formulas and to examine the effectiveness \nof features including compar ing blog versus medical \narticles [44], news articles [30], student magazine \narticles, and children’s books [27].  However, most of \nthese corpora consist of at most a few hundred example \ntexts.  We used a significantly larger data set consisting \nof 118,000 aligned sentence pairs collected from \nEnglish Wikipedia (http://en.wikipedia.org/) and \nSimple English Wikipedia \n(http://simple.wikipedia.org/), with the former \nrepresenting unsimplified sentences and the latter \nsimplified sentences [45].  Throughout the rest of this \npaper we will refer to those sentences/examples from \nEnglish Wikipedia as “difficult” and \nsentences/examples from Simple English Wikipedia as \n“simple”. \nThis corpus has a number of benefits for this study.  \nFirst, the size of the corpus allows for large-scale \nanalysis on a large variation of examples.  Second, the \ncorpus is sentence aligned and contains, for each \ndifficult sentence, a corre sponding simple sentence.  \nThis helps normalize for content and other variation \nbetween the simple and di fficult examples.  Third, \nwhen simplifying health-related articles it is important \nthat all text is understandable, not just the medical \nterms.  This corpus includes articles and terms for both \nmedical and non-medical terms and allows us to \nexamine characteristics of both in aggregate. \nWe used modern implemen tations of the machine \nlearning algorithms and a serv er with 16GB of memory \nand Intel Core i7-2600 CPU @ 3.40GHz processor.  \nHowever, running the full range of experiments \ninvolving 10-fold cross validation over multiple \ndifferent learning approaches is computationally \nintensive.  Therefore, the data set we examined \nconsisting of 118,000 sentence pairs was a random \nsubsample from the original data set [45], which \nconsisted of 137,000 pairs. Preliminary experiments \nshowed similar results for a number of the approaches \non the full data set. \n \n3.2 Text Features \n \nWe examined 16 features for use in predicting the \ndifficulty of text.  The features were extracted for each \nsentence in the corpus and ranged from surface \nfeatures, such as the number of words and characters, \nto aggregate features designed to model how \nambiguous the words in a sentence are.  Below we \noutline each feature. Some of these features have been \npreviously suggested such as surface features, part of \nspeech features and vocabulary features.  In this work, \nwe introduce a new collection of features based on the \nnumber of concepts in a sentence and two new \naggregate features, specificity and ambiguity. \n \nSurface features:  To capture basic text characteristics \nwe extract the number of characters and the number of \nwords.  \n \nPart-of-speech (POS) features:  The POS is \nautomatically tagged using the Natural Language \nToolkit (NLTK) [46] and then we count the number of \nnouns, adjectives, verbs and adverbs in the sentence, \neach as an individual feature.   We group together the \nremaining parts of speech into one feature and count \ntheir occurrence. Similar features have been used in \nprevious work for predicting sentence and document \nsimplicity [28 ]. \nVocabulary features: Previous work has shown that \ntext familiarity, as measured by frequency, has an \neffect on text simplicity [47, 48] .  Motivated by this, \n2619\nwe generated features to capture the general frequency \nof the words in the sentence.  For each word in the \nsentence we obtained the unigram frequency from the \nGoogle Web Corpus [49], which contains n-gram \ncounts from the web.  Using these counts, we included \nfeatures for the average, median and standard deviation \nfor the words in the sentence. In addition, we counted \nthe number of words not occurring in the 5000 most \nfrequent unigrams in the Google Web Corpus.  This \nfeature was designed to help capture the number of \nunfamiliar words. \n \nConcept Density features:  The number of concepts \ntalked about in a sentence and how related these \nconcepts are to each other can have an impact on the \ndifficulty of a sentence.  We use the Unified Medical \nLanguage System (UMLS), a resource provided by the \nNational Library of Medicine \n(http://www.nlm.nih.gov/research/umls/).  The UMLS \ncontains millions of health-related terms, which are \neach assigned to one or more concepts in the \nMetathesaurus. Each concept is assigned to one or \nmore semantic types in the Semantic Network.  \nFor each word in the sentence, we first filter out \ncommon concepts by ignoring words that occurred in \nthe Dale-Chall List [50], a lexicon of common words.  \nFor each remaining word that is a noun, adjective, verb \nor adverb we look up the word in the UMLS \nMetathesaurus.  To capture the concept density we \ncount how many of these remaining words are found in \nthe Metathesaurus and the number of different \nconcepts found (these will be different when two \nwords in the sentence are mapped to the same \nconcept).  To measure broader concept density we \ncount how many different semantic types are \nrepresented in the network, as identified by the UMLS \nsemantic network. \n \nSpecificity: For a given word, we hypothesize that one \nindicator of difficulty is how specific that word is.  To \nmeasure a word’s specificity we use the UMLS’s link \nto the Medical Subject Heading (MeSH), which \ncontains collections of term s arranged in a hierarchical \nstructure where height in the structure corresponds to \nthe level of specificity.  To calculate the specificity for \nthe sentence we sum the specificity level of each word \nin a sentence that we find in the MeSH database.  \nBecause the MeSH database was hand constructed and \nis limited in size and scope, if a word is not found in \nthe database but a synonym (as identified by the \nUMLS Metathesaurus) is, we use the specificity level \nof the synonym as the specificity level of the original \nword. \n \nAmbiguity: Another possible indicat or of difficulty is \nhow ambiguous a given word is.  To measure this, we \ncount how many different UMLS Metathesaurus \nconcepts a word is associated with.  This can be seen \nas analogous to counting the number of possible senses \nthat a word can have.  For a given sentence, we sum \nthe ambiguity of all of the words in the sentence.  \n \nTo normalize for varying sentence length, we \ndivide each feature value by the number of words in \nthe sentence and to avoid s cale bias, we normalize each \nfeature to be between 0 and 1 by dividing each feature \nby the maximum value found in the dataset for that \nfeature. \nTable 2 shows example feature values for the \ndifficult and simple sentences for our corpus.  The \ndifficult sentences tend to be longer than the simple \nsentences [28, 45] and, as has been seen in other data \nsets [47, 48], the difficult sentences tend to have more \nnouns, adjectives and adverbs while the simple \nsentences have more verbs.  The simple sentences tend \nto use higher frequency word s, as indicated by higher \nmedian frequency and the number of words used that \nwere ranked >5000.  Difficult sentences tended to use \nmore concepts and more semantic types.  Difficult \nsentences had higher specificity, which intuitively \ncorrelates with the use of more rare terms.  Similarly, \nsimple sentences tend to use words that were more \nambiguous, which tends to correlate with the use of \nmore frequent terms. \nTable 2. Average features values for the difficult \nand simple examples from the corpus. \nFeature Difficult Simple \nSurface \n Character count 0.058 0.048 \n Word Count 0.083 0.072 \nPart of speech \n Nouns 0.086 0.073 \n Adjectives 0.040 0.032 \n Verbs 0.086 0.081 \n Adverbs 0.034 0.028 \n Other 0.056 0.048 \nVocabulary \n Average frequency 0.318 0.316 \n Median frequency 0.018 0.026 \n Std. dev. of frequency  0.558 0.543 \n Frequency rank \n>5000  \n0.287 0.263 \nConcept density \n Concept count 0.045 0.036 \n Unique concept count 0.102 0.083 \n Semantic types 0.176 0.146 \nSpecificity 0.036 0.017 \nAmbiguity 0.029 0.041 \n \n2620\n3.3 Predicting Text Difficulty \n \nWe view the text difficulty prediction problem as a \nbinary classification probl em between simple and \ndifficult.  For each example, the sentences are \ntokenized and POS tagged.  We then extract the 16 \nfeatures described above. These features are passed to \nthe machine learning appro ach along with the binary \nlabel of either ‘simple’ (1) or ‘difficult’ (-1). \nIn practice, a finer-graine d classification would be \nuseful.  For many of the machine learning approaches, \nthis can be obtained by calibrating the output value or \nconfidence score [51] (e.g. the values output by a linear \nregression model can either be used directly to predict \ndifficulty or can be binned into discrete difficulty \nlevels).  However, since our main goal was to analyze \nthe usefulness of the different features we leave that for \nfuture investigation. \n \n3.4 Machine Learning Approaches \n \nTo understand how the features perform across \nmultiple different learning approaches, and to identify \nwhich classifiers work best  for this problem domain, \nwe investigated six machine learning approaches: \nrandom forests, decision trees, linear regression, Naïve \nbayes, K-nearest neighbors and support vector \nmachines (SVM).  The first five methods were run in R \n(random forests using the randomForest  package, \ndecision trees the trees package, linear regression using \nthe built-in functionality, Naïve Bayes using the e1071 \npackage and K-nearest neighbors using the class \npackage).  SVMs were run using SVMLight [52].  All \nclassifiers were run with their default parameter \nsetting. \n \n3.5 Experimental Setup \n \nTo evaluate the different approaches we used 10-fold \ncross-validation and randomly split the 118,000 \nexamples into ten, 90/10 splits.  Each model was then \ntrained on 90% and evaluated on the remaining 10% \nfor each of the ten splits.  The methods were evaluated \nusing accuracy of prediction on the test set. We use \nrandom assignment of labels as the baseline condition \nto compare the individual algorithms against. \n \n4. Results \n \n4.1 Classifier Performance \n \nTable 3 shows the classification accuracy for the six \nmachine learning methods averaged using 10-fold \ncross-validation over the 118,000 examples. All \napproaches achieve results that are much better than \nthe random baseline, with the random forest approach \nachieving the best results with an accuracy of 84.14%.  \nThis is better than the best previous results of 80.80% \n[28], which was achieved on a similar task, though not \nexactly the same examples were used.  All differences \nbetween the classifier accuracies for the ten folds are \nsignificantly different based on a t-test (p < 0.001) \nexcept for between linear regression and SVM. \nTo understand the types of mistakes that are being \nmade by the classifiers, Ta ble 4 shows the confusion \nmatrix for the first fold for the random forest classifier.  \nOverall, the classifier makes mistakes evenly between \nthe two classes.  Similar results were seen for the other \nclassifiers. \n \n4.2 Model Analysis \n \nFor some of the machine learning approaches we \ncan look at the weighting of the features to understand \nwhich features were most useful in making predictions.  \nThis is important for later tool development to ensure \nthat critical features are included. The analyses below \nrefer to models trained on the first split of the data, \nthough similar phenomena were seen for the other \nsplits.  In almost all cases, the specificity and \nambiguity features were the most predictive features \nfor the models. \n \nDecision Tree :  The decision tree model recursively \nsubdivides the data set by picking the features that best \nseparate the data. Therefore, the features that best \ndiscriminate between the classes are the features that \nare chosen earliest and appear higher up in the tree.  \nFor the decision tree learned, the most informative \nTable 3. Accuracy for the six different machine \nlearning approaches averaged over the 10 folds. \nLearning method Accuracy \nRandom Forest 84.14% \nDecision Tree 76.75% \nLinear Regression 74.62% \nSVM 74.48% \nK-nearest neighbors 63.82% \nNaïve Bayes 59.41% \nRandom 50.11% \n \nTable 4. Confusion matrix for the first fold for \nthe random forest classifier. \n             Predicted \n  Difficult Simple \nActual Difficult 4967 944 \n Simple 891 4998 \n2621\nfeature was specificity, at the top of the tree, and the \nsecond most informative was ambiguity, at the second \nlevel of the tree.  \n \nLinear Regression:  The linear regression model \ngenerates a prediction as a weighted linear combination \nof features.  Features with the largest absolute \nstandardized weights are therefore the most important.  \nThe three highest weighted features were character \ncount (-18.9), specificity (-14. 5) and ambiguity (9.5).  \nThe next largest feature w as weighted 5.9.  This \ncorrelates with the average features values from Table \n2 where length and specificity were higher for difficult \nsentences, while ambiguity was higher for simple \nsentences. \n \nSVM: We used a linear kernel for the SVM, so the \nseparating hyperplane is, like linear regression, a linear \nweighting of the features.  The three highest weighted \nfeatures were specificity (- 45.8), ambiguity (31.7), and \ncharacter count (-24.3).  The next largest feature had a \nweight of 4.8. \n \nNaïve Bayes:  The Naïve Bayes model learns a \ndistribution over features for each class.  By comparing \nthe probabilities associated with each feature between \nthe simple and difficult probability models we can see \nwhich features have the strongest disparity and \ntherefore the biggest impact.  The two features with the \nlargest disparity between th e classes were specificity, \nwhich was twice as probabl e for the difficult model, \nand ambiguity, which was 1.5 times as probable for the \nsimple model. \n \nThe random forest classifier is a combination of \nweighted decision trees and is therefore difficult to \nexamine feature importance.  K-nearest neighbors is a \nnon-parametric classifier and does not provide an \nexplicit model. \n \n4.3 Feature Ablation \n \nAs an additional experiment to understand the \nimpact of the different features we did an ablation \nstudy for the random forest classifier (the best \nperforming approach), calculating the accuracy when \nleaving each feature group out.  As with the previous \nmodel analysis, understanding the impact of each \nfeature and which features are redundant is important \nfor future tool development. An ablation study can \nshow the impact of omitting sp ecific features in a tool, \na common requirement when tools need to be fast and \neasy/small to install by users.  In addition, the study is \nneeded to confirm results from the model analysis. \nFor each of the feature groups, we calculated the \naverage accuracy over the 10-folds using all of the \nfeatures except the features in that feature group.  The \ndifference between the accuracy  of the classifiers with \nall of the features and the accuracy with that feature \ngroup excluded is an indicator of how impactful the \nfeature group is. Table 5 shows the results for the \nablation study. \nAs with the model analysis , the two most impactful \nfeatures were specificity and ambiguity.  When both \nare removed, the overall accuracy of the classifier \nreduces from 84.14% to 60.58%, a 23.56% absolute \nreduction in accuracy.  Even removed individually, \nspecificity and ambiguity each result in a larger \nreduction in performance than any of the other feature \ngroups.  Of the remaining features, the concept density \nfeature had the next most impact when removed and \nthe other groups had little impact on performance.  \nThat does not mean that these other feature groups are \nnot useful, only that they do not add additional \ninformation when taken in the context of the other \nfeatures.  Many of these features groups have been \nshown to be useful by them selves in other studies [27-\n29, 31]. \n \n4.4 Impact of Training Data Size \n \nIn many domains, such as healthcare, there are only \nsmall data sets available that either contain difficulty \nannotations or have different variants representing \ndifferent readability levels.  In addition, the data set we \nused only had a binary labeling for difficulty.  For \nmany applications a more fine-grained difficulty \nlabeling could be useful. Therefore, to understand how \nmuch data is required to obtain reasonable performance \nfor predicting text difficulty we trained the random \nforest classifier on increasi ng amount of training data \non the first fold. Figure 1 and Figure 2 show these \nTable 5. Ablation study using random forest \nclassifier.  Accuracies are averaged over ten folds \nusing all features except the feature group listed. \nFeature group \nexcluded \nAccuracy difference \nfrom all \nfeatures \nSurface 83.96%   0.18 \nPart of Speech 83.97%   0.17 \nVocabulary 83.72%   0.42 \nConcept Density 81.12%   3.02 \nSpecificity & \nAmbiguity \n60.58% 23.56 \nSpecificity 78.31%   5.83 \nAmbiguity 79.74%   4.40 \n2622\nresults for training sizes ra nging from 10% to 90% and \n1%-9% respectively.  Even with just 1% of the training \ndata (1062 examples) the classifier still achieves an \naccuracy of around 80%.  With 10% of the data \n(10,620 examples) the classifier achieves an accuracy \nof over 83.5%, which is within 1% absolute of the \nscore achieved using all of the training data.  These \nresults are encouraging for working on similar tasks in \nother domains. \n5. Conclusions \nBetter accessibility to health-related information for \npatients is important.  Unfortunately, for much of the \ninformation available there is currently a mismatch \nbetween patient education level and the difficulty level \nof the health documents available to them.  To help \nalleviate this, we are work ing on developing tools both \nto support patients in reading health-related texts and \nto assist content creators (e.g., doctors, practitioners, \npharmaceutical companies, hospitals) in creating text \nthat is more accessible. \nIn this paper, we explored 16 features for predicting \nthe difficulty of medical texts.   Identifying features that \ncorrelate with text difficulty can be useful for \nidentifying difficult text sections and for directing \nsimplifications.  Three of these features have not been \npreviously examined before  including: concept density, \nspecificity, and ambiguity.  Both specificity and \nambiguity were highly inform ative for predicting text \ndifficulty as seen from corpus statistics, model \nweighting across multiple machine learning \napproaches, and based on a feature ablation study using \nthe best performing approach (random forests).  \nSpecificity is positively corre lated with difficulty; text \nthat uses more specific medical terms tends to be more \ndifficult.  Ambiguity is negatively correlated with \ndifficulty; text that uses br oader terms, i.e. that have \nmore potential meanings, tends to be simpler. In this \npaper, we focused on applications within the health \ndomain, however, many of the features we explored \nare generally applicable.  Fo r future work, we plan to \ninvestigate the impact of th ese features as components \nof simplification tools.  \n \n6. Acknowledgements \n \nThis work was supported by the U.S. National \nLibrary of Medicine, NIH/NLM 1R03LM010902-01. \n \n7. References  \n[1] Somers, S.A., and Mahadevan, R., \"Health Literacy \nImplications of the Affordab le Care Act\", in (Editor, \n'ed.'^'eds.'): Book Health Literacy Implications of the \nAffordable Care Act, Center for Health Care \nStrategies, Inc., 2010 \n[2] Kuijpers, W., Groen, W.G., Aaronson, N.K., and \nHarten, W.H.V., \"A Systematic Review of Web-Based \nInterventions for Patient Empowerment and Physical \nActivity in Chronic Diseases: Relevance for Cancer \nSurvivors\", Journal of Medical Internet Research, \n15(2), 2013,  \n[3] Rudd, R.E., \"Needed Acti on in Health Literacy\", \nJournal of Health Psychology, 2013, pp. 1359-1053. \n[4] Fincham, J.E., \"The P ublic Health Importance of \nImproving Health Literacy\", American Journal of \nPharmaceutical Education, 77(3), 2013, pp. Article 41. \n[5] Leroy, G., Endicott, J., Mouradi, O., Kauchak, D., \nand Just, M., \"Improving Perceived and Actual Text \nDifficulty for Health Information Consumers Using \nSemi-Automated Methods\", AMIA Fall Symposium, \n2012, pp. 522-531. \n[6] Aluisio, S., Specia, L., Gasperin, C., and Scarton, \nC., \"Readability Assessment for Text Simplification\", \nProceedings of the NAACL HLT 2010 Fifth Workshop \non Innovative Use of NLP for Building Educational \nApplications, 2010, pp. 1-9. \n[7] Parker, R.M., Baker, D.W., Williams, M.V., and \nNurss, J.R., \"The Test of F unctional Health Literacy in \nAdults: A New Instrument for Measuring Patients’ \n \nFigure 1. Accuracy of th e random forest classifier \nfor increasing amounts of training data on the \nfirst fold. \n \n \nFigure 2. Accuracy of the random forest classifier \nfor smaller amounts of training data. \n2623\nLiteracy Skills\", Journal of General Internal Medicine, \n10(1995, pp. 537-541. \n[8] Nurss, J.R., Parker, R.M., Williams, M.V., and \nBaker, D.W., Test of Functional Health Literacy in \nAdults, Peppercorn Books & Press,  Hartford, MI,  \n1995. \n[9] Davis, T., Long, S., Jackson, R., Mayeaux, E., \nGeorge, R., Murphy, P., and Crouch, M., \"Rapid \nEstimate of Adult Literacy in Medicine: A Shortened \nScreening Instrument\", Fa mily Medicine, 25(6), 1993, \npp. 391-395. \n[10] Chew, L.D., Bradle, K. A., and Boyko, E.J., \"Brief \nQuestions to Identify Patients with Inadequate Health \nLiteracy\", Family Medicine, 36(8), 2004, pp. 588-594. \n[11] Sayah, F.A., Majumdar, S.R., Williams, B., \nRobertson, S., and Johnson, J.A., \"Health Literacy and \nHealth Outcomes in Diabetes:  A Systematic Review\", \nJournal of General Internal Medicine, 28(3), 2013, pp. \n444-452. \n[12] Quinlan, P., Price, K.O., Magid, S.K., Lyman, S., \nMandl, L.A., and Stone, P.W., \"The Relationship \namong Health Literacy, Health Knowledge, and \nAdherence to Treatment in Patients with Rheumatoid \nArthritis\", Hospital for Special Surgery Journal, \n9(2013, pp. 42-49. \n[13] Rr, I., and Ll., I., \"Examining the Association of \nHealth Literacy and Health Behaviors in African \nAmerican Older Adults: Does Health Literacy Affect \nAdherence to Antihypertensive Regimens?\", J \nGerontol Nurs., 39(3), 2013, pp. 22-32. \n[14] Omachi, T.A., Sarkar, U., Yelin, E.H., Blanc, \nP.D., and Katz, P.P., \"Lower Health Literacy Is \nAssociated with Poorer Health Status and Outcomes in \nChronic Obstructive Pulmonary Disease\", Journal of \nGeneral Internal Medicine, 28(1), 2013, pp. 74-81. \n[15] Sun, X., Shi, Y., Zeng, Q., Wang, Y., Du, W., \nWei, N., Xie, R., and Chang, C., \"Determinants of \nHealth Literacy and Health Behavior Regarding \nInfectious Respiratory Diseases: A Pathway Model\", \nBMC Public Health, 13(2013,  \n[16] Dubay, W.H., The Principles of Readability, \nImpact Information,  2004. \n[17] Wang, L.-W., Miller, M.J., Schmitt, M.R., and \nWen, F.K., \"Assessing Readability Formula \nDifferences with Written Heal th Information Materials: \nApplication, Results, and Recommendations\", \nResearch in Social & Administrative Pharmacy, (In \nPress)(2012,  \n[18] Chharnock, D., Sheppe rd, S., Needham, G., and \nGann, R., \"Discern: An Instrument for Judging the \nQuality of Written Consumer Health Information on \nTreatment Choices\", Epidemiology and Community \nHealth, 53(2), 1999, pp. 105-111. \n[19] Atcherson, S.R., Richbur g, C.M., Zraick, R.I., and \nGeorge, C.M., \"Readability of Questionnaires \nAssessing Listening Difficu lties Associated with \n(Central) Auditory Processing Disorders\", Language, \nspeech, and hearing services in schools, 44(1), 2013, \npp. 48-60. \n[20] Williamson, J.M.L., and Martinez, J.D., \"Analysis \nof Patient Information Leaf lets Provided by a District \nGeneral Hospital by the Flesch and Flesch-Kincaid \nMethod\", International Journal of Clinical Practice, \n64(13), 2013, pp. 1824. \n[21] Mckearney, T.C., and Mckearney, R.M., \"The \nQuality and Accuracy of Internet Information on the \nSubject of Ear Tubes\", International Journal of \nPediatric Otorhinolaryngology, 77(6), 2013, pp. 894-\n897. \n[22] \"An Assessment of the Quality of Information on \nStroke and Speech and Language Difficulty Web \nSites\", Journal of Information Science, 39(1), 2013, pp. \n113-125. \n[23] Thomas, G.R., Eng, L., Wolff, J.F.D., and Grover, \nS.C., \"An Evaluation of Wikipedia as a Resource for \nPatient Education in Nephrology\", Seminars in \nDialysis, 26(2), 2013, pp. 159-163. \n[24] Wang, Y., \"Automatic Recognition of Text \nDifficulty from Consumers H ealth Information\", 19th \nIEEE International Symposium on Computer-Based \nMedical Systems, 2006, pp. 131-136. \n[25] Gemoets, D., Rosemblat, G., Tse, T., and Logan, \nR., \"Assessing Readability of Consumer Health \nInformation: An Exploratory Study\", in (Editor, \n'ed.'^'eds.'): Book Assessi ng Readability of Consumer \nHealth Information: An Exploratory Study, 2004, pp. \n869-873. \n[26] Francois, T., and Miltsakaki, E., \"Do Nlp and \nMachine Learning Improve Traditional Readability \nFormulas?\", Proceedings of the First Workshop on \nPredicting and Improving Text Readability for target \nreader populations, 2012, pp. 49-57. \n[27] Ma, Y., Singh, R., Fosl er-Lussier, E., and Lofthus, \nR., \"Comparing Human Versus Automatic Feature \nExtraction for Fine-Grained  Elementary Readability \nAssessment\", First Workshop on Predicting and \nImproving Text Readability for target reader \npopulations, 2012, pp. 58--64. \n[28] Napoles, C., and Dredze, M., \"Learning Simple \nWikipedia: A Cogitation in Ascertaining Abecedarian \nLanguage\", Proceedings of the NAACL HLT 2010 \nWorkshop on Computational Linguistics and Writing: \nWriting Processes and Aut horing Aids, 2010, pp. 42-\n50. \n[29] Feng, L., Jansche, M., Huenerfauth, M., and \nElhadad, N., \"A Comparison of Features for Automatic \nReadability Assessment\", Proceedings of the 23rd \nInternational Conference on Computational \nLinguistics: Posters,  2010, pp. 276-284. \n2624\n[30] Pitler, E., and Nenkova, A., \"Revisiting \nReadability: A Unified Framework for Predicting Text \nQuality\", Proceedings of the Conference on Empirical \nMethods in Natural Language Processing, 2008, pp. \n186-195. \n[31] Schwarm, S.E., and Ostendorf, M., \"Reading \nLevel Assessment Using Support Vector Machines and \nStatistical Language Models\", Proceedings of the 43rd \nAnnual Meeting on Association for Computational \nLinguistics, 2005, pp. 523-530. \n[32] Feng, L., Elhadad, N., and Huenerfauth, M., \n\"Cognitively Motivated Features for Readability \nAssessment\", Proceedings of the 12th Conference of \nthe European Chapter of the Association for \nComputational Linguisti cs, 2009, pp. 229-237. \n[33] Roark, B., Mitchell, M., and Hollingshead, K., \n\"Syntactic Complexity Measures for Detecting Mild \nCognitive Impairment\", Proceedings of the Workshop \non BioNLP 2007: Biologica l, Translational, and \nClinical Language Processing, 2007, pp. 1-8. \n[34] Miltsakaki, E., and Troutt, A., \"Real-Time Web \nText Classification and Analysis of Reading \nDifficulty\", Proceedings of the Third Workshop on \nInnovative Use of NLP for Building Educational \nApplications, 2008, pp. 89-97. \n[35] Mihalcea, R., Corley, C., and Strapparava, C., \n\"Corpus-Based and Knowle dge-Based Measures of \nText Semantic Similarity\", Proceedings of the 21st \nnational conference on Artificial intelligence - Volume \n1, 2006, pp. 775-780. \n[36] Caraballo, S.A., and Charniak, E., \"Determining \nthe Specificity of Nouns from Text\", SIGDAT, 1999, \npp. 63-70. \n[37] Richardson, R., Smeat on, A.F., and Murphy, J., \n\"Using Wordnet as a Knowledge Base for Measuring \nSemantic Similarity between Words \", AICS, 1994 \n[38] Resnik, P., \"Semantic Similarity in a Taxonomy: \nAn Information-Based Measure and Its Application to \nProblems of Ambiguity in Natural Language\", Journal \nof Artificial Intelligence Research, 11(1999, pp. 95-\n130. \n[39] Rayner, K., and Duffy, S., \"Lexical Complexity \nand Fixation Times in Reading: Effects of Word \nFrequency, Verb Complexity, and Lexical Ambiguity\", \nMemory & Cognition, 14(3), 1986, pp. 191-201. \n[40] Pexman, P.M., Hino, Y., and Lupker, S.J., \n\"Semantic Ambiguity and the Process of Generating \nMeaning from Print\", Experimental Psychology: \nLearning, Memory and C ognition, 30(6), 2004, pp. \n1252-1270. \n[41] Rodd, J., Gaskell, G., and Marslen-Wilson, W., \n\"Making Sense of Semantic Ambiguity: Semantic \nCompetition in Lexical Access\", Journal of Memory \nand Language, 46(2), 2002, pp. 245-266. \n[42] Torres, S., and Gelbukh, A., \"Comparing \nSimilarity Measures for Original Wsd Lesk \nAlgorithm\", Centro de I nvestigación en Computación \n(CIC IPN), Unidad Profes ional Adolfo-López Mateos, \nAv. Juan de Dios Bátiz s/n and M. Othón de \nMendizábal, Zacatenco, México, Advances in \nComputer Science and Applications Research in \nComputing Science, 43(2009, pp. 155-166. \n[43] Stevenson, M., and Wilks, Y., \"Word Sense \nDisambiguation\", The Oxford Handbook of Comp. \nLinguistics, 2003, pp. 249-265. \n[44] Miller, T., Leroy, G., Chatterjee, S., Fan, J., and \nThoms, B., \"A Classifier to Evaluate Language \nSpecificity of Medical Documents\", Proceedings of the \n40th Annual Hawaii International Conference on \nSystem Sciences, 2007, pp. 134. \n[45] Coster, W., and Kauchak, D., \"Simple English \nWikipedia: A New Text Simplification Task\", \nProceedings of the 49th Annual Meeting of the \nAssociation for Computat ional Linguistics: Human \nLanguage Technologies: short papers - Volume 2, \n2011, pp. 665-669. \n[46] Bird, S., Klein, E., and Loper, E., Natural \nLanguage Processing with Py thon, O'Reilly Media, \nInc.,  2009. \n[47] Leroy, G., and Endicott, J.E., \"Combining Nlp \nwith Evidence-Based Methods to Find Text Metrics \nRelated to Perceived and Actual Text Difficulty\", 2nd \nACM SIGHIT International Health Informatics \nSymposium (ACM IHI 2012), 2012 \n[48] Leroy, G., and Endicott, J.E., \"Term Familiarity to \nIndicate Perceived and Actual  Difficulty of Text in \nMedical Digital Libraries\", International Conference \non Asia-Pacific Digital Libraries (ICADL 2011) - \nDigital Libraries -- for Culture Heritage, Knowledge \nDissemination, and Future Creation, 2011 \n[49] Brants, T., and Franz, A., \"Web 1t 5-Gram \nVersion 1\", in (Editor, 'ed.'^'eds.'): Book Web 1t 5-\nGram Version 1, Linguist ic Data Consortium, \nPhiladelphia, 2006 \n[50] Dale, E., and Chall, J.S., \"A Formula for \nPredicting Readability\", Educational Research \nBulletin, 27(1), 1948, pp. 11-20. \n[51] Niculescu-Mizil, A., a nd Caruana, R., \"Predicting \nGood Probabilities with Supervised Learning\", \nProceedings of the 22nd international conference on \nMachine learning, 2005, pp. 625-632. \n[52] Joachims, T., \"Making Large-Scale Support \nVector Machine Learning Practical\": Advances in \nKernel Methods, MIT Press, 1999, pp. 169-184. \n \n \n \n \n2625",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7909402847290039
    },
    {
      "name": "Ambiguity",
      "score": 0.7717215418815613
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6570529341697693
    },
    {
      "name": "Natural language processing",
      "score": 0.6452487111091614
    },
    {
      "name": "Word (group theory)",
      "score": 0.5659815073013306
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.5602662563323975
    },
    {
      "name": "Task (project management)",
      "score": 0.5394831895828247
    },
    {
      "name": "Sentence",
      "score": 0.5292529463768005
    },
    {
      "name": "Binary classification",
      "score": 0.5116605758666992
    },
    {
      "name": "Random forest",
      "score": 0.4322471022605896
    },
    {
      "name": "Sample (material)",
      "score": 0.4270361661911011
    },
    {
      "name": "Binary number",
      "score": 0.4159894585609436
    },
    {
      "name": "Machine learning",
      "score": 0.3583870530128479
    },
    {
      "name": "Information retrieval",
      "score": 0.33417385816574097
    },
    {
      "name": "Support vector machine",
      "score": 0.14573532342910767
    },
    {
      "name": "Mathematics",
      "score": 0.08031913638114929
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Arithmetic",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I195575238",
      "name": "Middlebury College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136722135",
      "name": "Claremont Graduate University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I138006243",
      "name": "University of Arizona",
      "country": "US"
    }
  ]
}