{
  "title": "The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction",
  "url": "https://openalex.org/W2948250930",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2556672858",
      "name": "Dimitris Alikaniotis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2073193502",
      "name": "Vipul Raheja",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2917881729",
    "https://openalex.org/W2252205254",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W2399359464",
    "https://openalex.org/W2401451642",
    "https://openalex.org/W2797371199",
    "https://openalex.org/W2124725212",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2759575900",
    "https://openalex.org/W2410156476",
    "https://openalex.org/W2964187553",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2164859977",
    "https://openalex.org/W2132337608",
    "https://openalex.org/W2251862950",
    "https://openalex.org/W2103018059",
    "https://openalex.org/W2741494657",
    "https://openalex.org/W2810035278",
    "https://openalex.org/W99445809",
    "https://openalex.org/W2250552659",
    "https://openalex.org/W2962901607",
    "https://openalex.org/W2470324779",
    "https://openalex.org/W2515384205",
    "https://openalex.org/W137061674",
    "https://openalex.org/W2251219521",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964258094",
    "https://openalex.org/W2589277916",
    "https://openalex.org/W2153013403",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2805583812",
    "https://openalex.org/W2251613956",
    "https://openalex.org/W2758774757",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2896668011",
    "https://openalex.org/W2321916036"
  ],
  "abstract": "Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.",
  "full_text": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 127–133\nFlorence, Italy, August 2, 2019.c⃝2019 Association for Computational Linguistics\n127\nThe Unreasonable Effectiveness of Transformer Language Models in\nGrammatical Error Correction\nDimitris Alikaniotis Vipul Raheja\nGrammarly\nNew York City, NY\nfirstname.lastname@grammarly.com\nAbstract\nRecent work on Grammatical Error Correc-\ntion (GEC) has highlighted the importance of\nlanguage modeling in that it is certainly pos-\nsible to achieve good performance by com-\nparing the probabilities of the proposed edits.\nAt the same time, advancements in language\nmodeling have managed to generate linguistic\noutput, which is almost indistinguishable from\nthat of human-generated text. In this paper,\nwe up the ante by exploring the potential of\nmore sophisticated language models in GEC\nand offer some key insights on their strengths\nand weaknesses. We show that, in line with\nrecent results in other NLP tasks, Transformer\narchitectures achieve consistently high perfor-\nmance and provide a competitive baseline for\nfuture machine learning models.\n1 Introduction\nTransformer models (Vaswani et al., 2017) trained\non large-scale language modeling datasets have re-\ncently proved to be a very effective means of rep-\nresenting the meaning of a sentence, being put\nto effective use in ﬁne-tuning both sentence-level\ntasks, such as the GLUE benchmark (Wang et al.,\n2018) and token-level tasks, such as Named Entity\nRecognition (Devlin et al., 2019). Recent work\nhas also found them to produce linguistically valid\nrepresentations (Goldberg, 2019), as well as to dis-\nplay excellent performance across multiple down-\nstream NLP tasks (e.g., Houlsby et al. 2019).\nIn this work, we explore how such models per-\nform in the task of Grammatical Error Correc-\ntion (GEC). While there is a substantial amount of\nwork on statistical (Rozovskaya and Roth, 2016;\nJunczys-Dowmunt and Grundkiewicz, 2014; Yan-\nnakoudakis et al., 2017) and neural (Ji et al.,\n2017; Xie et al., 2016; Yuan and Briscoe, 2016;\nChollampatt et al., 2016; Chollampatt and Ng,\n2017; Sakaguchi et al., 2017; Chollampatt and\nNg, 2018) machine translation methods for GEC,\nwe follow the approach of Bryant and Briscoe\n(2018) and explore how such models would fare\nin this task when treated as simple language mod-\nels. More speciﬁcally, Bryant and Briscoe (2018)\ntrain a 5-gram language model on the One Bil-\nlion Word Benchmark (Chelba et al., 2013) dataset\nand ﬁnd that it produces competitive baseline re-\nsults without any supervised training. In our work,\nwe extend this work by substituting the n-gram\nmodel for several publicly available implemen-\ntations of state-of-the-art Transformer language\nmodels trained on large linguistic corpora and as-\nsess their performance on GEC without any super-\nvised training. We ﬁnd that Transformer language\nmodels produce results on par with supervised ap-\nproaches providing a solid baseline system. This\nﬁnding is of particular importance in GEC, where\ndata collection and annotation requires substantial\nmanual effort.\n2 Related Work\nThe idea of using language models is quite funda-\nmental to the task of Grammatical Error Correc-\ntion, which has fed a substantial body of work over\nthe years. More recently, with the availability of\nweb-scale data powering the advances in language\nmodeling, among most of the other advances in\nNLP, a plethora of language-modeling based ap-\nproaches have been proposed for the GEC task.\nGamon et al. (2008); Matthieu Hermet and Sz-\npakowicz (2008) and Yi et al. (2008) were some of\nthe early works to successfully leverage language\nmodels trained on large amounts of web-scale data\ninto a GEC system, reinforcing the idea that sim-\nple models and a lot of data trump more elabo-\nrate models based on annotated data (Halevy et al.,\n2009).\nSince then, multiple works based on language-\n128\nmodels have been proposed for the GEC task\n(Park and Levy, 2011; Dahlmeier and Ng, 2012a),\neither relying entirely on LMs or using them\nfor ﬁne-tuning their systems. Many of the top-\nranked systems in the CoNLL-2013 and 2014\nGEC shared tasks (Ng et al., 2013, 2014), were\neither based on language models or had them as\nintegral parts of their systems (Kao et al., 2013;\nYoshimoto et al., 2013; Xing et al., 2013; Lee\nand Lee, 2014; Junczys-Dowmunt and Grund-\nkiewicz, 2014). LM-only approaches though took\na backseat and were only sporadically used after\nthe shared tasks, as Neural Machine Translation-\nbased approaches took over, but LMs remained\nan integral part of the GEC systems (Junczys-\nDowmunt and Grundkiewicz, 2016; Ji et al., 2017;\nXie et al., 2016; Junczys-Dowmunt et al., 2018;\nChollampatt and Ng, 2018). However, Bryant and\nBriscoe (2018) recently revived the idea, achiev-\ning competitive performance with the state-of-\nthe-art, demonstrating the effectiveness of the ap-\nproaches to the task without using any annotated\ndata for training.\n3 Methodology\nIn this work, we follow the setup from Bryant\nand Briscoe (2018) substituting the 5-gram lan-\nguage model for different language models based\non the Transformer architecture. Speciﬁcally, we\nuse Google’s BERT (Devlin et al., 2019) and Ope-\nnAI’s GPT (Radford et al., 2018) and GPT-2 (Rad-\nford et al., 2019).1 While all these are best thought\nof as language models in that they have been\ntrained to predict an element in a sequence, they\nuse slightly different objectives which does not\nmake them directly comparable. Speciﬁcally, GPT\nand GPT-2 have been trained with a classic lan-\nguage modeling objective, whereby they predict\nthe next word in a sequence, whereas BERT has\nbeen trained using a masked language modeling\nobjective in which the network attempts to predict\nmasked words in the sentence.\nWe extract the probability of a sentence from\nBERT, by iteratively masking every word in the\nsentence and then summing the log probabilities.\nWhile this approach is far from ideal, it has been\nshown (Wang and Cho, 2019) that it approximates\nthe log-likelihood of a sentence.\n1https://github.com/huggingface/\npytorch-pretrained-BERT/\nTest set Sent. Tokens Annot.\nCoNLL-2014 1,312 30k 2\nFCE 2,715 47k 1\nTable 1: Statistics for evaluation data\n3.1 Confusion sets\nSince our systems do not generate novel se-\nquences, we follow Bryant and Briscoe (2018) and\nuse simple heuristics to generate a confusion set\nof sentences that our language models score. For\nprepositions and determiners, the confusion set in-\ncludes the set of all prepositions and determiners\nplus an empty string ϵto remove unnecessary ad-\nditions. For morphological errors (e.g., past tense\nor pluralization), we use the Automatically Gener-\nated Inﬂection Database (AGID) which contains\ndifferent morphological forms for each word. 2\nHowever, we notice that due to the automatic gen-\neration, AGID contains errors that might prop-\nagate into our scoring. The problem with in-\ntroducing new errors and non-words is that they\nwould be interpreted as unknown words (hence-\nforth [UNK]s) from the model’s perspective. An\nunknown word in some context might give higher\nprobabilities to an erroneous sentence and cause\nthe model not to select the correct alternative. To\nremedy this issue, we generate a vocabulary from\nall the training sets and make sure that any pro-\nposed words which do not exist in the vocabulary\nare replaced by [UNK]s. Note that there is no rea-\nson to re-use the vocabulary of the training sets as\nany large English wordlist would achieve a simi-\nlar effect. Finally, for spelling mistakes, we, again,\nfollow Bryant and Briscoe (2018) and use CyHun-\nSpell3 to generate alternatives for non-words.\n3.2 Thresholding\nGiven that our confusion set is prone to errors (due\nto its automatic generation procedure) as well as\nthe fact that we cannot target all potential errors\n(e.g., insertions), we bias our method to prefer\nthe original sentence unless a much better the al-\nternative is found. We quantify this margin by\nimposing a threshold above which we accept a\ncandidate sentence as a better alternative. Con-\ncretely, let P(sc) be the probability of the can-\ndidate sentence and P(so) the probability of the\n2http://wordlist.aspell.net/other/\n3https://pypi.org/project/CyHunspell/\n129\nERRANT M2\nDataset System P R F 0.5 P R F 0.5\nCoNLL-2014\nFelice et al. (2014) † - - - 39.71 30.10 37.33\nYannakoudakis et al. (2017) - - - 58.79 30.63 49.66\nChollampatt and Ng (2017) - - - 62.74 32.96 53.14\nChollampatt and Ng (2018) - - - 65.49 33.14 54.79\nGe et al. (2018) - - - 74.12 36.30 61.34\nBryant and Briscoe (2018) 36.62 19.93 31.37 40.56 20.81 34.09\nBERT 33.27 27.14 31.83 35.69 27.99 33.83\nGPT-1 49.58 27.06 42.5 51.08 27.45 43.57\nGPT-2 57.73 24.75 45.58 58.51 24.9 46.08\nFCE\nYannakoudakis et al. (2017) - - - 65.03 32.45 54.15\nBryant and Briscoe (2018) 41.92 13.62 29.61 44.78 14.12 31.22\nBERT 29.56 34.67 30.46 31.97 35.01 32.53\nGPT-1 62.75 32.19 52.74 64.01 32.33 53.52\nGPT-2 61.91 33.47 52.92 62.64 33.74 53.48\nTable 2: Results of our Transformer-Language Model approach against similar approaches (Bryant and Briscoe,\n2018) and state-of-the-art on Grammatical Error Correction. For each of the datasets, we use the corresponding\ntest set, and we do not train our models on the corpora. As BERT, we report the best performing BERT model (12\nlayers, retaining uppercase characters). In the top part of each dataset, we report the scores of supervised methods\nand in the bottom the unsupervised ones. † denotes this system won the shared task competition.\noriginal sentence, then we accept the candidate if\nP(sc) > P(so) + τ, where τ is some threshold\nparameter which we ﬁt on each development set.\nNote that, practically, this parameter controls the\ntrade-off between precision and recall as higher\nτ values would mean that there is less chance of\nchanging the original sentence (i.e., higher preci-\nsion) and vice versa. We explore different values\nfor τ ∈ {0,2,4,6,8} by, as above, ﬁtting them on\nthe corresponding development set.4\n3.3 Search\nFinally, we perform greedy search to ﬁnd the best\nalternative sentence by iterating over each sen-\ntence multiple times, once for every position for\nwhich our heuristics found alternatives. If an alter-\nnative is selected for the target position, we update\nthe original sentence and proceed to the next po-\nsition. This pseudo-log-likelihood approximation\nmakes the problem of considering every permuta-\ntion more computationally tractable.\n4 Experiments\nWe evaluate our method and report results on two\nstandard publicly available datasets. Our evalua-\ntion is aimed to stay as true to Bryant and Briscoe\n(2018) as possible to ensure an even compari-\nson. Concretely, we use the test dataset from\n4Note that the probability of each sentence is in log space.\nthe CoNLL-2014 (Ng et al., 2014) shared task 5\nand the publicly available First Certiﬁcate in En-\nglish (FCE) (Yannakoudakis et al., 2011). Unfor-\ntunately, due to licensing issues, we were unable\nto obtain permission to use the JFLEG (Napoles\net al., 2017) corpus for evaluation. Note that in\nour method, we do not make use of the training\nsets commonly used with these datasets. However,\nwe use the development sets used by Bryant and\nBriscoe (2018) to tune the hyperparameter τ. The\nnumber of sentences and tokens for the datasets\nwe used can be found in Table 1.\nSimilar to Bryant and Briscoe (2018), we report\nresults on three metrics. We use the MaxMatch\n(M2) Precision, Recall and F 0.5 (Dahlmeier and\nNg, 2012b) and ERRANT Precision, Recall and\nF0.5 (Bryant et al., 2017).\n5 Results\nTable 2 presents the results of our method compar-\ning them against recent state-of-the-art supervised\nmodels and the simple n-gram language model\nused by Bryant and Briscoe (2018). Table 3 shows\nsome qualitative examples on how each model cor-\nrects two sentences pulled from the FCE along\nwith the gold annotations. The reported results\n5While we acknowledge the contemporaneous nature of\nthe BEA 2019 Shared Task on GEC and would have liked to\nreport results on the W&I+LOCNESS data, we could not do\nso because of license limitations.\n130\nSource It will start by a speech from the Director of the conference, followed by a meal.\nGold It will start with a speech by the Director of the conference, followed by a meal.\nBERT It will start with a speech from the Director of the conference, followed by a meal.\nGPT It will start by a speech from the Director of the conference, followed by a meal.\nGPT-2 It will start with a speech from the Director of the conference, followed by a meal.\nSource They all knows where the conference is and when.\nGold They all know where the conference is and when.\nBERT They all know where the conferencing is and when.\nGPT They all knows where the conference is and when.\nGPT-2 They all know where the conference is and when.\nTable 3: Source sentences along with the gold edits and the proposed candidates from each of our models.\ncome from the best performing hyperparameter τ\non each dataset. For BERT, we also explored dif-\nferent sizes (12 vs. 24 layers) and whether re-\ntaining uppercase characters helps in performance.\nThe best performing τ values were τ = 4 for\nCoNLL14 for all models; for the FCE dataset:\nBERT τ = 4, GPT τ = 8, and GPT-2 τ = 6.\nThe best ‘version,’ of BERT was thelarge, cased\n(i.e., retaining the lower- /uppercase distinction).\nA key result of Table 2 is that Transformer Lan-\nguage Models prove to be more than just a com-\npetitive baseline to legitimate Grammatical Er-\nror Correction systems on their own. Across the\nboard, Transformer Models are able to outperform\nthe simple n-gram model and even approach the\nperformance of supervised GEC systems.\n6 Discussion\nLooking at the performance of the two GPT mod-\nels more closely, we see that their performance\nis nearly identical with GPT-2 leading by a small\nmargin in the CoNLL14 dataset. Given that the\nversions we used share the same number of layers\n(12), we attribute GPT-2’s slight advantage to the\nfact that it was trained on considerably more data.\nAnother interesting result is that while BERT\nsurpasses the n-gram baseline overall, it achieves\nworse performance than the rest in terms of preci-\nsion and F 0.5 score. Considering its overall suc-\ncess at modeling NLP tasks, one might expect\nBERT to achieve better performance here. How-\never, as mentioned above, BERT is not truly a\nlanguage model in the sense that GPT and GPT-\n2 are but uses a quasi-language modeling objec-\ntive which could explain its degraded performance\nin this setting. Note that framing the task differ-\nently (e.g., by masking the preposition in a sen-\ntence and selecting the one with the highest proba-\nbility) might give the edge to BERT as it resembles\nthe way it was trained.\nIt is also worth mentioning that despite tun-\ning τ to each dataset, we do not explore different\nweights for different kinds of errors (e.g., penal-\nizing more spelling mistakes). Our key motivation\nwas to corroborate and extend the results of Bryant\nand Briscoe (2018) to current state-of-the-art lan-\nguage models which have been trained in several\nlanguages and show that these models are tough\nbaselines to beat for novel GEC systems.\nWhile the results of the Transformer language\nmodels shown in Table 2 demonstrate that they\nare a tough baseline to beat, it is worth noting that\nthe present approach is not without its limitations.\nWe believe that our methodology should not be\nconsidered a panacea to GEC. For instance, being\nbound by the confusion sets, our system (1) cannot\nhandle missing words (which make up about 20%\nof all errors), and (2) it is tuned to capture only a\nsubset of the possible mistakes a writer can make\n(closed class words).\nIt could be argued that since our system makes\nuse of a pre-deﬁned confusion set (even an auto-\nmatically generated one), it could not be consid-\nered as a fully unsupervised system. In principle,\nwe agree with that statement and we believe that\na system which uses, for example, corpus statis-\ntics to on-the-ﬂy generate a confusion set would\nbe a very interesting exercise and could yield sim-\nilar results. However, the present paper is con-\ncerned with highlighting the importance of lan-\nguage modeling in GEC and its potential in aid-\ning in low-resource languages where large parallel\ndatasets are unavailable, but such confusion sets\nare relatively easily obtainable.\n131\n7 Conclusion\nIn this work, we advanced on the foundational idea\nthat a simple language modeling-based approach\nto GEC with no annotated data can challenge the\nlatest neural and machine translation approaches\nthat rely on large quantities of annotated training\ndata. To this end, we improve on previous work\nby leveraging state-of-the-art language modeling\ntechniques and perform a thorough comparison of\nthree state-of-the-art Transformer language mod-\nels which in turn have been trained on data of the\norder of hundreds of millions of words. We ﬁnd\nthat merely using pre-trained, and publicly avail-\nable neural language models improves the perfor-\nmance by a signiﬁcant margin and comes within\nstriking distance of the state-of-the-art methods.\nThis work reinforces the strength and robust-\nness of language-model based methods for the\ntask of grammatical error correction. While recent\nstate-of-the-art GEC systems are pursuing NMT-\nbased models with huge amounts (millions of sen-\ntences) of annotated training data, approaches like\nthis which require no annotated training data pro-\nvide great value to researchers and developers\ninterested in building competitive GEC systems\n(e.g., in other languages) with limited annotated\ndata.\nAcknowledgements\nThe authors would like to thank Joel Tetreault\nfor his comments on earlier drafts as well as the\nanonymous reviewers for their helpful sugges-\ntions.\nReferences\nChristopher Bryant and Ted Briscoe. 2018. Language\nmodel based grammatical error correction without\nannotated training data. In Proceedings of the Thir-\nteenth Workshop on Innovative Use of NLP for\nBuilding Educational Applications, pages 247–253,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nChristopher Bryant, Mariano Felice, and Ted Briscoe.\n2017. Automatic annotation and evaluation of error\ntypes for grammatical error correction. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 793–805, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. Tech-\nnical report, Google.\nShamil Chollampatt and Hwee Tou Ng. 2017. Con-\nnecting the dots: Towards human-level grammatical\nerror correction. In Proceedings of the 12th Work-\nshop on Innovative Use of NLP for Building Edu-\ncational Applications, pages 327–333, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nShamil Chollampatt and Hwee Tou Ng. 2018. A multi-\nlayer convolutional encoder-decoder neural network\nfor grammatical error correction. In Proceedings of\nthe Thirty-Second AAAI Conference on Artiﬁcial In-\ntelligence.\nShamil Chollampatt, Kaveh Taghipour, and Hwee Tou\nNg. 2016. Neural network translation models for\ngrammatical error correction. In Proceedings of the\n25th International Joint Conference on Artiﬁcial In-\ntelligence, New York, USA.\nDaniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-\nsearch decoder for grammatical error correction. In\nProceedings of the 2012 Joint Conference on Empir-\nical Methods in Natural Language Processing and\nComputational Natural Language Learning , pages\n568–578, Jeju Island, Korea. Association for Com-\nputational Linguistics.\nDaniel Dahlmeier and Hwee Tou Ng. 2012b. Better\nevaluation for grammatical error correction. In Pro-\nceedings of the 2012 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n568–572, Montr ´eal, Canada. Association for Com-\nputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMariano Felice, Zheng Yuan, Øistein E. Andersen, He-\nlen Yannakoudakis, and Ekaterina Kochmar. 2014.\nGrammatical error correction using hybrid systems\nand type ﬁltering. In Proceedings of the Eigh-\nteenth Conference on Computational Natural Lan-\nguage Learning: Shared Task , pages 15–24, Bal-\ntimore, Maryland. Association for Computational\nLinguistics.\nMichael Gamon, Jianfeng Gao, Chris Brockett,\nAlexandre Klementiev, William B. Dolan, Dmitriy\nBelenko, and Lucy Vanderwende. 2008. Using\ncontextual speller techniques and language model-\ning for ESL error correction. In Proceedings of\nthe Third International Joint Conference on Natural\nLanguage Processing: Volume-I.\n132\nTao Ge, Furu Wei, and Ming Zhou. 2018. Reaching\nhuman-level performance in automatic grammati-\ncal error correction: An empirical study. CoRR,\nabs/1807.01270.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. CoRR, abs/1901.05287.\nA. Halevy, P. Norvig, and F. Pereira. 2009. The un-\nreasonable effectiveness of data. IEEE Intelligent\nSystems, 24(2):8–12.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799,\nLong Beach, California, USA. PMLR.\nJianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen\nGong, Steven Truong, and Jianfeng Gao. 2017. A\nnested attention neural hybrid model for grammati-\ncal error correction. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 753–\n762, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2014. The AMU system in the CoNLL-2014\nshared task: Grammatical error correction by data-\nintensive and feature-rich statistical machine trans-\nlation. In Proceedings of the Eighteenth Confer-\nence on Computational Natural Language Learn-\ning: Shared Task , pages 25–33, Baltimore, Mary-\nland. Association for Computational Linguistics.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2016. Phrase-based machine translation is state-of-\nthe-art for automatic grammatical error correction.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1546–1556, Austin, Texas. Association for Compu-\ntational Linguistics.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nShubha Guha, and Kenneth Heaﬁeld. 2018. Ap-\nproaching neural grammatical error correction as a\nlow-resource machine translation task. In Proceed-\nings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), pages 595–606, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nTing-hui Kao, Yu-wei Chang, Hsun-wen Chiu, Tzu-Hsi\nYen, Joanne Boisson, Jian-cheng Wu, and Jason S.\nChang. 2013. CoNLL-2013 shared task: Gram-\nmatical error correction NTHU system description.\nIn Proceedings of the Seventeenth Conference on\nComputational Natural Language Learning: Shared\nTask, pages 20–25, Soﬁa, Bulgaria. Association for\nComputational Linguistics.\nKyusong Lee and Gary Geunbae Lee. 2014.\nPOSTECH grammatical error correction system\nin the CoNLL-2014 shared task. In Proceedings\nof the Eighteenth Conference on Computational\nNatural Language Learning: Shared Task , pages\n65–73, Baltimore, Maryland. Association for\nComputational Linguistics.\nAlain Dsilets Matthieu Hermet and Stan Szpakow-\nicz. 2008. Using the web as a linguistic re-\nsource to automatically correct lexico-syntactic er-\nrors. In Proceedings of the Sixth International\nConference on Language Resources and Evaluation\n(LREC’08), Marrakech, Morocco. European Lan-\nguage Resources Association (ELRA).\nCourtney Napoles, Keisuke Sakaguchi, and Joel\nTetreault. 2017. Jﬂeg: A ﬂuency corpus and bench-\nmark for grammatical error correction. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers , pages 229–234, Valencia,\nSpain. Association for Computational Linguistics.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The CoNLL-2014 shared task\non grammatical error correction. In Proceedings of\nthe Eighteenth Conference on Computational Nat-\nural Language Learning: Shared Task , pages 1–\n14, Baltimore, Maryland. Association for Compu-\ntational Linguistics.\nHwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian\nHadiwinoto, and Joel Tetreault. 2013. The CoNLL-\n2013 shared task on grammatical error correction.\nIn Proceedings of the Seventeenth Conference on\nComputational Natural Language Learning: Shared\nTask, pages 1–12, Soﬁa, Bulgaria. Association for\nComputational Linguistics.\nY . Albert Park and Roger Levy. 2011. Automated\nwhole sentence grammar correction using a noisy\nchannel model. In Proceedings of the 49th Annual\nMeeting of the Association for Computational Lin-\nguistics: Human Language Technologies - Volume\n1, HLT ’11, pages 934–944, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAlla Rozovskaya and Dan Roth. 2016. Grammatical\nerror correction: Machine translation and classiﬁers.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2205–2215, Berlin, Germany.\nAssociation for Computational Linguistics.\n133\nKeisuke Sakaguchi, Matt Post, and Benjamin\nVan Durme. 2017. Grammatical error correction\nwith neural reinforcement learning. In Proceedings\nof the Eighth International Joint Conference on\nNatural Language Processing (Volume 2: Short\nPapers), pages 366–372, Taipei, Taiwan. Asian\nFederation of Natural Language Processing.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a markov ran-\ndom ﬁeld language model. CoRR, abs/1902.04094.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nZiang Xie, Anand Avati, Naveen Arivazhagan, Dan Ju-\nrafsky, and Andrew Y . Ng. 2016. Neural language\ncorrection with character-based attention. CoRR,\nabs/1603.09727.\nJunwen Xing, Longyue Wang, Derek F. Wong, Lidia S.\nChao, and Xiaodong Zeng. 2013. UM-checker: A\nhybrid system for English grammatical error cor-\nrection. In Proceedings of the Seventeenth Con-\nference on Computational Natural Language Learn-\ning: Shared Task, pages 34–42, Soﬁa, Bulgaria. As-\nsociation for Computational Linguistics.\nHelen Yannakoudakis, Ted Briscoe, and Ben Medlock.\n2011. A new dataset and method for automatically\ngrading ESOL texts. In Proceedings of the 49th An-\nnual Meeting of the Association for Computational\nLinguistics: Human Language Technologies , pages\n180–189, Portland, Oregon, USA. Association for\nComputational Linguistics.\nHelen Yannakoudakis, Marek Rei, Øistein E. Ander-\nsen, and Zheng Yuan. 2017. Neural sequence-\nlabelling models for grammatical error correction.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2795–2806, Copenhagen, Denmark. Association for\nComputational Linguistics.\nXing Yi, Jianfeng Gao, and William B. Dolan. 2008.\nA web-based English prooﬁng system for English\nas a second language users. In Proceedings of\nthe Third International Joint Conference on Natural\nLanguage Processing: Volume-II.\nIppei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,\nKeisuke Sakaguchi, Tomoya Mizumoto, Yuta\nHayashibe, Mamoru Komachi, and Yuji Matsumoto.\n2013. NAIST at 2013 CoNLL grammatical error\ncorrection shared task. In Proceedings of the Seven-\nteenth Conference on Computational Natural Lan-\nguage Learning: Shared Task , pages 26–33, Soﬁa,\nBulgaria. Association for Computational Linguis-\ntics.\nZheng Yuan and Ted Briscoe. 2016. Grammatical er-\nror correction using neural machine translation. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 380–386, San Diego, California. Association\nfor Computational Linguistics.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7753006219863892
    },
    {
      "name": "Computer science",
      "score": 0.7634508609771729
    },
    {
      "name": "Language model",
      "score": 0.6865249872207642
    },
    {
      "name": "Natural language processing",
      "score": 0.5471058487892151
    },
    {
      "name": "Artificial intelligence",
      "score": 0.540435254573822
    },
    {
      "name": "Strengths and weaknesses",
      "score": 0.47747111320495605
    },
    {
      "name": "Baseline (sea)",
      "score": 0.41428595781326294
    },
    {
      "name": "Machine learning",
      "score": 0.36813437938690186
    },
    {
      "name": "Engineering",
      "score": 0.09065210819244385
    },
    {
      "name": "Psychology",
      "score": 0.07139095664024353
    },
    {
      "name": "Voltage",
      "score": 0.06420651078224182
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I64189192",
      "name": "International Institute of Information Technology, Hyderabad",
      "country": "IN"
    }
  ],
  "cited_by": 4
}