{
  "title": "Conditional Set Generation with Transformers",
  "url": "https://openalex.org/W3040538742",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227877655",
      "name": "Kosiorek, Adam R.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222364528",
      "name": "Kim, Hyunjik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221389402",
      "name": "Rezende, Danilo J.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2170987079",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3098321015",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2970638480",
    "https://openalex.org/W2952433032",
    "https://openalex.org/W2992035660",
    "https://openalex.org/W2798979442",
    "https://openalex.org/W73302357",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W3102696055",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W2994687091",
    "https://openalex.org/W2963907629",
    "https://openalex.org/W3020787282",
    "https://openalex.org/W2963951231",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963716836",
    "https://openalex.org/W2784996692",
    "https://openalex.org/W2909878113"
  ],
  "abstract": "A set is an unordered collection of unique elements--and yet many machine learning models that generate sets impose an implicit or explicit ordering. Since model performance can depend on the choice of order, any particular ordering can lead to sub-optimal results. An alternative solution is to use a permutation-equivariant set generator, which does not specify an order-ing. An example of such a generator is the DeepSet Prediction Network (DSPN). We introduce the Transformer Set Prediction Network (TSPN), a flexible permutation-equivariant model for set prediction based on the transformer, that builds upon and outperforms DSPN in the quality of predicted set elements and in the accuracy of their predicted sizes. We test our model on MNIST-as-point-clouds (SET-MNIST) for point-cloud generation and on CLEVR for object detection.",
  "full_text": "Conditional Set Generation with Transformers\nAdam R. Kosiorek1 Hyunjik Kim1 Danilo J. Rezende1\nAbstract\nA set is an unordered collection of unique\nelements—and yet many machine learning mod-\nels that generate sets impose an implicit or explicit\nordering. Since model performance can depend\non the choice of ordering, any particular order-\ning can lead to sub-optimal results. An alterna-\ntive solution is to use a permutation-equivariant\nset generator, which does not specify an order-\ning. An example of such a generator is the Deep\nSet Prediction Network ( DSPN ). We introduce\nthe Transformer Set Prediction Network (TSPN ),\na ﬂexible permutation-equivariant model for set\nprediction based on the transformer, that builds\nupon and outperforms DSPN in the quality of pre-\ndicted set elements and in the accuracy of their\npredicted sizes. We test our model on MNIST -as-\npoint-clouds (SET-MNIST ) for point-cloud genera-\ntion and on CLEVR for object detection.\n1. Introduction\nIt is natural to reason about a group of objects as a set.\nTherefore many machine learning tasks involving predict-\ning objects or their properties can be cast as a set prediction\nproblem. These predictions are usually conditioned on some\ninput feature that can take the form of a vector, a matrix\nor a set. Some examples include predicting future states\nfor a group of molecules in a simulation (Noé et al., 2020),\nobject detection from images (Carion et al., 2020) and gen-\nerating correlated samples for sequential Monte Carlo in\nobject tracking (Zhu et al., 2020; Neiswanger et al., 2014).\nElements of a set are unordered, which brings about two\nchallenges that set prediction faces. First, the model must be\npermutation-equivariant; that is, the generation of a particu-\nlar permutation of the set elements must be equally probable\nto any other permutation. Second, training a generative\nmodel for sets typically involves comparing a predicted set\nagainst a ground-truth set. Since the result of this compari-\nson should not depend on the permutation of the elements\n1Deepmind, London, UK. Correspondence to: Adam R. Ko-\nsiorek <adamrk@google.com>.\nWorkshop on Object-Oriented Learning at ICML 2020. Copyright\n2020 by the author(s).\nof either set, the loss function used for training must be\npermutation-invariant. While it is possible to create a set\nmodel that violates either or both of these requirements,\nsuch a model has to learn to meet them, which is likely to\nresult in lower performance.\nPermutation equivariance imposes a constraint on the struc-\nture of the model (Bloem-Reddy & Teh, 2018; 2019), and\ntherefore sets are often treated as an ordered collection of\nitems, which allows using standard machine learning mod-\nels. For example assuming that a set has a ﬁxed size, we\ncan treat it as a tensor and turn set-prediction into multi-\nvariate regression (Achlioptas et al., 2018). If the ordering\nis ﬁxed but the size is not, we can treat set prediction as a\nsequence prediction problem (Vinyals et al., 2016). Both ap-\nproaches require using permutation-invariant loss functions\nto allow the model to learn a deterministic ordering policy\n(Eslami et al., 2016). However imposing such an ordering\ncan lead to a pathology that is commonly referred to as\nthe responsibility problem (Zhang et al., 2019; 2020); there\nexist points in the output set space where a small change\nin set space (as measured by a set loss) requires a large\nchange in the generative model’s output. This can lead to\nsub-optimal performance, as shown in Zhang et al. (2020).\nSome approaches choose to learn the ordering of set ele-\nments (Rezatoﬁghi et al., 2018), but this also suffers the\nsame problem as well as adding further complexity to the\nset prediction problem.\nRecently, Zhang et al. (2019) introduced the Deep Set\nPrediction Network (DSPN )—a model that generates sets\nin a permutation-equivariant manner using permutation-\ninvariant loss functions. DSPN relies on the observation that\nthe gradient of a permutation-invariant function is equivari-\nant with respect to the permutation of its inputs, also noticed\nby Papamakarios et al. (2019). DSPN uses this insight to\ngenerate a set by gradient-descent on a learned loss function\nwith respect to an initially-guessed set. DSPN has several\nlimitations, however. The functional form of the update step\nis limited, as the gradient information is only used to tans-\nlate the set elements. This, in turn, means that the method\ncan be computationally costly: not only is the backward\npass expensive, but many such passes might be needed to\narrive at an accurate prediction.\nIn this paper we develop the Transformer Set Prediction\nNetwork (TSPN ), where we replace the gradient-based up-\narXiv:2006.16841v2  [cs.CV]  1 Jul 2020\nConditional Set Generation with Transformers\ninput\nenc\ninit set\n(deterministic)\nenc\ngradd(    ,    )\nupdate #1\nenc\nupdate #K\nFigure 1.Deep Set Prediction Network (DSPN ) starts with a deter-\nministic set that is gradually changed into the desired prediction\nwith gradient descent on a learned loss function.\ninput\nenc\nrandom init\ntransformer\nprediction\nMLP N\nFigure 2.Our Transformer Set Prediction Network (TSPN ) extends\nDSPN to use more expressive permutation-equivariant set transfor-\nmations based on the Transformer (Vaswani et al., 2017). Addition-\nally, TSPN explicitly learns the set cardinality, which allows it to\ngeneralize to much larger sets.\ndates of DSPN with a Transformer (Vaswani et al., 2017),\nthat is also permutation-equivariant and learns to jointly up-\ndate the elements of the initial set. We make the following\ncontributions:\n• We show that the set-cardinality learning method of\nZhang et al. (2019) is prone to falling into local minima.\n• We thus introduce an alternative, principled method for\nlearning set cardinality.\n• We learn a distribution over the elements of the initial\nset (as opposed to DSPN that learns a ﬁxed initial set).\nThis allows one to directly generate sets with cardinality\ndetermined by the model (by sampling the correct number\nof points), and to dynamically change the size of the\ngenerated sets as test time.\n• We demonstrate that TSPN outperforms DSPN on condi-\ntional point-cloud generation and object-detection tasks.\nWe show that our model is not only more expressive than the\nDSPN , but can also generalize at test-time to sets of vastly\ndifferent cardinality than the sets encountered during train-\ning. We evaluate our model on auto-encoding SET-MNIST\n(LeCun et al., 2010) and on object detection on CLEVR\n(Johnson et al., 2017). We now proceed to describe DSPN\nand our method TSPN in detail, followed by experiments.\n2. Permutation-Equivariant Set Generation\n2.1. Permutation-Equivariant Generator\nThe Deep Set Prediction Network (DSPN ) iteratively trans-\nforms an initial set into the ﬁnal prediction, and the trans-\nformation is conditioned on some input. That is, given a\nconditioning y ∈Rdy and an initial set x :=\n{\nx0\ni\n}N\ni=1 of\nN points in Rd, DSPN iteratively applies a permutation-\nequivariant f to transform this initial set into the ﬁnal pre-\ndiction xK :=\n{\nxK\ni\n}N\ni=1 over Kiterations:\n{\nxK\ni\n}N\ni=1 = fK\n({\nx0\ni\n}N\ni=1,y\n)\n. (1)\nIn DSPN , the points x0\ni are initialised randomly as model\nparameters and learned, hence the model assumes ﬁxed set\ncardinality. For handling variable set sizes, each element\nof the predicted set is augmented with a presence variable\npi ∈ [0,1], which are transformed by f along with the\nxi and then thresholded to give the ﬁnal prediction. The\nground-truth sets are padded with all-zero vectors to the\nsame size. Note, however, that this mechanism does not\nallow to extrapolate to set sizes beyond the maximum size\nencountered in training.\nDSPN employs a permutation-invariant set encoder (e.g.\ndeep-sets (Zaheer et al., 2017), relation networks (Santoro\net al., 2017)) to produce a set embedding, and updates the\ninitial prediction using the gradients of an embedding loss,\narriving at a permutation-equivariant ﬁnal prediction.\nThis leads to the following set-generation procedure. Given\nthe input embedding h = input_encoder(y), an initial set1\nx0, and a permutation-invariant set_encoder, we can arrive\nat the ﬁnal prediction by performing gradient descent,\nˆhk = set_encoder\n(\nxk)\n, (2)\nxk = xk−1 −λ∇{x}d(h,ˆhk−1) for k= 1,...,K, (3)\nwith step size λ ∈ R+, distance function dand number of\niterations K. The ﬁnal prediction is xK. This set-generator\ncan be used as a decoder of an autoencoding framework\n(with a permutation-invariantinput_encoder) as well as for\nany conditional set-prediction task (e. g. in object-detection\nwhere y is an image and input_encoder is a CNN).\n2.2. Permutation-Invariant Loss\nThe model is trained using a set loss, i.e. a permutation-\ninvariant loss. Common choices are the Chamfer loss or the\n1We drop the explicit indication of cardinality to avoid clutter.\nConditional Set Generation with Transformers\nHungarian loss:\nLcham(A,B) =\n∑\na∈A\nmin\nb∈B\nd(a,b) +\n∑\nb∈B\nmin\na∈A\nd(a,b) (4)\nLhung(A,B) = min\nπ∈P\n∑\nai∈A\nd(ai,bπ(i)) , (5)\nwhere πis a permutation in the space of all possible permuta-\ntions P and dcan be any distance or loss function deﬁned on\npairs of set points. Note that the computational complexity\nof the Chamfer loss isO(N2) for sets of sizeN, whereas the\nHungarian loss is O(N3)—it uses the Hungarian algorithm\nto compute the optimal permutation, whose complexity is\nO(N3) (Bayati et al., 2008). Hence the Chamfer loss is\nsuitable for larger sets, and the Hungarian loss for smaller\nsets. For d, Zhang et al. (2019) use the Huber loss deﬁned\nas d(a,b) =∑\nimin\n(\n1/2(ai −bi)2,|ai −bi|−1/2\n)\n.\nRecall that in the implementation of DSPN , the ground truth\nset is padded with zero vectors so that all sets have the same\nsize. Padding a set Ato a ﬁxed size with constant elements\nˆaturns it into a multiset ˆA. A Multiset is a set that contains\nrepeated elements, that can be represented by an augmented\nset where each unique element is paired with its multiplicity.\nIf we use a multiset in its default form (i. e. with repeated\nelements) as the ground-truth in the Chamfer loss, then it is\nenough for the model to predict a set Bcontaining exactly\none element bi = ˆaequal to the repeated element of the\nset ˆAin order to account for all its repetitions in the ﬁrst\nterm of Equation (4). The remaining superﬂuous elements\nbj ∈Bpredicted by the model can match any other element\nin Awithout increasing the second term of Equation (4).\nThis implies that padding a ground-truth set of size N with\nM constant elements creates\n(N+1\nM−1\n)\npredictions that are\nall optimal and hence indistinguishable under the Chamfer\nloss. These predictions have a set size that varies from N to\nN + M −1, and hence the model is likely to fail to learn\nthe correct set cardinality—an effect clearly visible in our\nexperiments, c. f. Section 4.1 and Table 1.\nDSPN uses an additional regularization term Lrepr\n(\nh, ˆh\n)\n=\nd(h, ˆh) deﬁned as the distance between the model’s input\nfeatures and the encoding of the generated set, which im-\nproves the performance of the internal optimization loop.\n3. Size-Conditioned Set Generation with\nTransformers\nWe follow DSPN in that we generate sets by transform-\ning an initial set. However, we employ Transformers\n(Vaswani et al., 2017; Lee et al., 2019) as a learnable set-\ntransformation, which can readily account for any interac-\ntions between the set elements. This change implies that\nthe elements of the initial set can have a different (higher)\ndimensionality to the elements of the ﬁnal predicted set,\nincreasing the ﬂexibility of the model. Moreover, instead of\nconcatenating a presence variable to each set point, TSPN\nuses a multilayer perceptron (MLP ) to directly predict the\nnumber of points from the input embedding h, which then\nchooses the size N of the initial set {x}N\ni=1. Given this size,\nwe must generate the elements of the initial set such that\nthe ﬁnal predicted set is permutation-equivariant. One solu-\ntion is to sample the intial set from a permutation-invariant\ndistribution e. g. iid samples from a ﬁxed distribution , for\nwhich we choose N(α1,diag(β1)) with learnable α∈R,\nβ ∈R+. This makes the model stochastic, but also allows\none to choose an arbitrary size for the generated set, even\nto sizes that lie outside the range of set sizes encountered\nin training. Formally, let h be the input embedding; the\ngenerating process reads as follows:\nN = MLP (h) , (6)\nxi ∼N(α1,diag(β1)), i = 1,...,N, (7)\nˆxi = concatenate(xi,y) , (8)\n{x}= transformer ({ˆx}) . (9)\nWhile training TSPN , we use the ground-truth set-cardinality\nto instantiate the initial set, and we separately train the MLP\nby minimizing categorical cross-entropy with the ground-\ntruth set sizes. The cardinality-MLP is used only at test-time.\nNote that, in contrast to DSPN , TSPN does not require any\nadditional regularization terms applied to its representations.\nWe describe our work in relation to DSPN (Zhang et al.,\n2019), but we note that there are concurrent works that\nshare the ideas presented here. Both DETR (Carion et al.,\n2020) and Slot Attention (Locatello et al., 2020) use a vari-\nant of the transformer (Vaswani et al., 2017) for predicting a\nset of object properties. DETR uses an object-speciﬁc query\ninitialization and is, therefore, not equivariant to permuta-\ntions, similarly to (Zhang et al., 2019). Slot Attention is per-\nhaps the most similar to our work—transforms a randomly\nsampled point-cloud, same as TSPN , but it uses attention\nnormalized along the query axis instead the key axis.\n4. Experiments\nWe apply TSPN and our implementations of DSPN and a\nsize-conditioned DSPN (C-DSPN ) to two tasks: point-cloud\nprediction on SET-MNIST and object detection on CLEVR .\nPoint-cloud prediction is an autoencoding task, where we\nuse a set encoder to produce vector embeddings of point\nclouds of varying sizes, and explore the use of the above\nset prediction networks for reconstructing the point clouds\nconditioned on these embeddings. Object detection, instead,\nrequires predicting sets of bounding boxes conditioned on\nimage features. While generating point-clouds requires\npredicting large numbers of points, which are often assumed\nto be conditionally independent of each other, detecting\nobjects typically requires generating much smaller sets. Due\nConditional Set Generation with Transformers\nFigure 3.Set reconstructions (rows 1-5) and inputs (row 6) from\ndifferent models, annotated with the number of points (above).\nDSPN fails to learn set cardinality, see Section 2.2 for an explana-\ntion of why this happens. C- DSPN learns the cardinality well, but\nproduces reconstructions of lower ﬁdelity than TSPN .\nModel Chamfer\n[\n10−5]\n↓ Set Size RMSE ↓\nDSPN 8.2 ±0.66 165 ±54.7\nDSPN -BIG 8.5 ±1.83 182 ±29.2\nC-DSPN 8.92 ±0.78 0.3 ±0.13\nC-DSPN -BIG 9.63 ±1.17 0.3 ±0.09\nTSPN 5.42 ±0.15 0.8 ±0.08\nTable 1.SET-MNIST results, averaged over 5 runs. RMSE is root-\nmean-square error. Note that our DSPN results match the Chamfer\nloss reported in (Zhang et al., 2019). C-DSPN stands for the\nDSPN conditioned on the number of points. The -BIG variants\nuse wider layers with the same number of parameters as TSPN . For\nconditional models, sets were generated with the number of points\ninferred by the model.\nto possible overlaps between neighbouring bounding boxes,\nhowever, it is essential to take relations between different\nobjects into account. We implemented all models in JAX\n(Bradbury et al., 2018) and HAIKU (Hennigan et al., 2020)\nand run experiments on a single Tesla V100 GPU. Models\nare optimized withADAM (Kingma & Ba, 2015) with default\nβ1,β2 parameters. We used batch size = 32and trained all\nmodels for 100 epochs.\n4.1. Point-Cloud Generation on SET-MNIST\nWe convert MNIST into point clouds by thresholding pixel\nvalues with the mean value in the training set; then normal-\nize coordinates of remaining points to be in [0,1].\nWe use the same hyperparameters for DSPN as reported in\n(Zhang et al., 2019). C-DSPN uses the same settings, except\ninstead of presence variables pi it uses an MLP with one\nhidden layer of size 128 to predict the set size, as used in\nTSPN . TSPN uses a three-layer Transformer, with parameters\nshared between layers2. Each layer has 256 hidden units\nand 4 attention heads. The initial set is two-dimensional\n(same as the output), but it is linearly-projected into 256-\ndimensional space. The outputs of the transformer layers\nare kept at 256 dimensions, too, and we project them back\ninto 2 dimensions after the last layer. TSPN uses the same\ninput encoder as (C-)DSPN , that is a two-layerDEEPSET with\nFSPOOL pooling layer (Zhang et al., 2020). All models are\ntrained using the Chamfer loss with learning rate = 10−3.\nTable 1 shows quantitative results. Conditioning on the set\nsize does not improve the Chamfer loss for C-DSPN but it\ndoes signiﬁcantly improve the accuracy of set-size predic-\ntion. Further, replacing the decoder with the Transformer\n(TSPN ) leads to a signiﬁcant loss reduction with respect to\nDSPN . Figure 3 shows inputs set and model reconstructions.\nNote that in our experiments, DSPN almost always predicts\nthe same number of points (323), which is close to the max-\nimum number of points we used (here, 342). Interestingly,\nTSPN performs very well while extrapolating to much big-\nger sets than the ones encountered in training: Figure 5\nin Appendix A shows reconstructions, where we manually\nchange the desired set size up to 1000 points. This is in con-\ntrast to C-DSPN , whose performance decreases signiﬁcantly\nwhen we require it to generate a set whose size differs only\nslightly from the input, c. f. Figure 6 in Appendix A. We\nconjecture that this is caused by how FSPOOL handles sets\nof different sizes, which leads to incompatibility between\nembeddings of sets of different cardinality. This, in turn,\ncauses the distances in the latent space to be ill-deﬁned, and\nbreaks the internal gradient-based optimization.\n4.2. Object Detection on CLEVR\nCLEVR images consist of up to 10 rendered objects on plain\nbackgrounds. Following Zhang et al. (2019), we use the\nCLEVR dataset to test the efﬁcacy of our models for object\ndetection in a simple setting, which might pave the way for\nmore advanced object-based inference in future works. We\nuse the same hyperparameters for C-DSPN as Zhang et al.\n(2019). TSPN uses four layers with four attention heads and\n256 neurons each, without parameter sharing between layers.\nWe apply layer normalization (Xiong et al., 2020) before\nattention as in (Ba et al., 2016). The last transformer layer is\nfollowed by an MLP with a single hidden layer of All models\n2This gives 383k parameters compared to 190k for DSPN ,\nwhich shares parameters between its encoder and gradient-based\ndecoder. Not sharing parameters between layers does not improve\nresults, but signiﬁcantly increases parameter count. Sharing param-\neters and increasing layer width to match the number of parameters\ndoes not increase performance, either.\nConditional Set Generation with Transformers\nModel AP50 AP95 AP95 AP98 AP99 Set Size RMSE ↓ #Params\nDSPN 67.7 ±5.49 7.4 ±0.91 0.6 ±0.10 0.0 ±0.01 0.0 ±0.00 2.53 ±0.221 0.3 M\nC-DSPN 71.6 ±3.40 10.8 ±1.50 0.9 ±0.21 0.0 ±0.01 0.0 ±0.00 1.74 ±0.301 0.3 M\nTSPN 81.2 ±1.03 20.7 ±0.16 3.0 ±0.20 0.1 ±0.02 0.0 ±0.00 0.58 ±0.046 1.9 M\nTable 2.CLEVR object detection results averaged over 5 runs. These results are not directly comparable to the ones reported in Zhang et al.\n(2019), since all our models were trained using Chamfer loss. We subtract the resnet parameters (21.8 M) when reporting the number of\nmodel parameters.\nFigure 4.Object detection on CLEVR , with the number of detected\nobjects in the lower-left corner of each image. DSPN fails to detect\nthe correct number of objects. The ground-truth bounding boxes\nare approximate.\nuse a RESNET 34 (He et al., 2016) as the input encoder, and\nare trained with the Chamfer loss; DSPN -based models are\ntrained for 200 epochs with learning rate = 3×10−5; TSPN\nuses learning rate = 10−4 and is trained for 1200 epochs.\nLonger training for DSPN -based models lead to overﬁtitng\nand decreased validation performance. Note that DSPN in\n(Zhang et al., 2019) use the Hungarian loss (Equation (5)),\nwhich leads to better results than using Chamfer. We re-\nport the average precision scores at different thresholds and\nthe set size root-mean-square error in Table 2. Qualitative\nresults are available in Figure 4. We see that TSPN outper-\nforms C-DSPN and produces bounding boxes that are better\naligned with ground-truth, although these results are worse\nthan the ones reported in Zhang et al. (2019)—we expect\nimprovements when using the Hungarian loss as well.\n5. Conclusions\nWe introduced the Transformer Set Prediction Network\n(TSPN )—a transformer-based model for conditional set pre-\ndiction. TSPN infers the cardinality of the set, randomly\nsamples an initial set of the desired size and applies a trans-\nformer to generate the ﬁnal prediction. Set prediction in\nTSPN is permutation-equivariant, and the model can be ap-\nplied to any set-prediction tasks. Interesting directions in-\nclude scaling the model to large-scale point-clouds and ob-\nject detection (e. g. similar to Carion et al. (2020)), as well\nas turning this model into a generative model in either the\nVAE or GAN framework.\nAcknowledgements\nWe would like to thank George Papamakarios, Karl Stelzner,\nThomas Kipf, Teophane Weber and Yee Whye Teh for help-\nful discussions.\nReferences\nAchlioptas, P., Diamanti, O., Mitliagkas, I., and Guibas, L. J.\nLearning representations and generative models for 3d\npoint clouds. In International Conference on Machine\nLearning, 2018.\nBa, J., Kiros, J. R., and Hinton, G. E. Layer normalization.\nConditional Set Generation with Transformers\n2016, arXiv:1607.06450.\nBayati, M., Shah, D., and Sharma, M. Max-product for\nmaximum weight matching: Convergence, correctness,\nand lp duality. IEEE Transactions on Information Theory,\n2008.\nBloem-Reddy, B. and Teh, Y . W. Neural network models of\nexchangeable sequences. 2018.\nBloem-Reddy, B. and Teh, Y . W. Probabilistic symmetry\nand invariant neural networks. 2019, arXiv:1901.06082.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,\nC., Maclaurin, D., and Wanderman-Milne, S. JAX: com-\nposable transformations of Python+NumPy programs,\n2018. URL http://github.com/google/jax.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,\nA. M., and Zagoruyko, S. End-to-end object detection\nwith transformers. 2020, arXiv:2005.12872.\nEslami, S. M. A., Heess, N. M. O., Weber, T., Tassa, Y .,\nSzepesvari, D., Kavukcuoglu, K., and Hinton, G. E. At-\ntend, infer, repeat: Fast scene understanding with genera-\ntive models. In Neural Information Processing Systems,\n2016.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. 2016.\nHennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku:\nSonnet for JAX, 2020. URL http://github.com/\ndeepmind/dm-haiku.\nJohnson, J. E., Hariharan, B., van der Maaten, L., Fei-Fei,\nL., Zitnick, C. L., and Girshick, R. B. Clevr: A diagnostic\ndataset for compositional language and elementary visual\nreasoning. In Conference on Computer Vision and Pattern\nRecognition, pp. 1988–1997, 2017.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference on Learning\nRepresentations, 2015.\nLeCun, Y ., Cortes, C., and Burges, C. Mnist hand-\nwritten digit database. ATT Labs [Online]. Available:\nhttp://yann.lecun.com/exdb/mnist, 2, 2010.\nLee, J., Lee, Y ., Kim, J., Kosiorek, A., Choi, S., and Teh,\nY . W. Set transformer: A framework for attention-based\npermutation-invariant neural networks. In International\nConference on Machine Learning, 2019.\nLocatello, F., Weissenborn, D., Unterthiner, T., Mahen-\ndran, A., Heigold, G., Uszkoreit, J., Dosovitskiy, A.,\nand Kipf, T. Object-centric learning with slot atten-\ntion. 2020, arXiv:2006.15055. URL https://arxiv.\norg/abs/2006.15055.\nNeiswanger, W., Wood, F., and Xing, E. The dependent\ndirichlet process mixture of objects for detection-free\ntracking and object modeling. In Artiﬁcial Intelligence\nand Statistics, 2014.\nNoé, F., Tkatchenko, A., Müller, K.-R., and Clementi, C.\nMachine learning for molecular simulation. Annual re-\nview of physical chemistry, 2020.\nPapamakarios, G., Nalisnick, E. T., Rezende, D. J., Mo-\nhamed, S., and Lakshminarayanan, B. Normalizing\nﬂows for probabilistic modeling and inference. 2019,\narXiv:1912.02762.\nRezatoﬁghi, S. H., Kaskman, R., Motlagh, F. T., Shi, Q.,\nCremers, D., Leal-Taixé, L., and Reid, I. D. Deep perm-\nset net: Learn to predict sets with unknown permuta-\ntion and cardinality using deep neural networks. 2018,\narXiv:1805.00613.\nSantoro, A., Raposo, D., Barrett, D. G. T., Malinowski, M.,\nPascanu, R., Battaglia, P. W., and Lillicrap, T. P. A simple\nneural network module for relational reasoning. InNeural\nInformation Processing Systems, 2017.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\ntion is all you need. In Neural Information Processing\nSystems, 2017.\nVinyals, O., Bengio, S., and Kudlur, M. Order matters: Se-\nquence to sequence for sets. In International Conference\non Learning Representations, 2016.\nXiong, R., Yang, Y ., He, D., Zheng, K., xin Zheng, S., Xing,\nC., Zhang, H., Lan, Y ., Wang, L.-W., and Liu, T.-Y . On\nlayer normalization in the transformer architecture. 2020,\narXiv:2002.04745.\nZaheer, M., Kottur, S., Ravanbakhsh, S., Póczos, B.,\nSalakhutdinov, R., and Smola, A. J. Deep sets. In Neural\nInformation Processing Systems, 2017.\nZhang, Y ., Hare, J. S., and Prügel-Bennett, A. Deep set\nprediction networks. In Neural Information Processing\nSystems, 2019.\nZhang, Y ., Hare, J. S., and Prügel-Bennett, A. Fspool:\nLearning set representations with featurewise sort pooling.\nIn International Conference on Learning Representations,\n2020.\nZhu, M. X., Murphy, K., and Jonschkowski, R. Towards\ndifferentiable resampling. 2020, arXiv:2004.11938.\nConditional Set Generation with Transformers\nA. Set Size Extrapolation on SET-MNIST\n181 121 186 167 184 139 185\n1000\n900\n800\n700\n600\n500\n400\n300\n100\n200\nFigure 5.TSPN extrapolates to far bigger set sizes than encountered\nin training. Here, the model was trained with up to 342 points,\nand yet can generate sets of up to 1000 points. The bottom row\ncontains the ground-truth annotated with the number of points at\nthe bottom. This ﬁgure uses a smaller marker size than Figure 3,\nhence ground-truth appears less dense.\n209 200 270 192 172 112 175\n+5\n+4\n+3\n+2\n+1\n0\n-1\n-2\n-4\n-3\nFigure 6.The size-conditional DSPN fails to reconstruct sets when\nwe slightly change the output set size. Here, we take the size\nof the reconstructed set to be within [−4, 5] points of the input\nsize. As the size of the reconstruction strays from the original size,\nthe reconstruction quality quickly deteriorates. The bottom row\ncontains the input sets with their respective sizes.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5649291276931763
    },
    {
      "name": "Computer science",
      "score": 0.4344109296798706
    },
    {
      "name": "Electrical engineering",
      "score": 0.17367929220199585
    },
    {
      "name": "Engineering",
      "score": 0.15681111812591553
    },
    {
      "name": "Voltage",
      "score": 0.06510108709335327
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}