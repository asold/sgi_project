{
    "title": "A Proposal for a Language Model Based Cognitive Architecture",
    "url": "https://openalex.org/W4391117101",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5109606725",
            "name": "Kobe Knowles",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A2094470858",
            "name": "Michael Witbrock",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A2002900660",
            "name": "Gillian Dobbie",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A2896163244",
            "name": "Vithya Yogarajan",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A5109606725",
            "name": "Kobe Knowles",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2094470858",
            "name": "Michael Witbrock",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2002900660",
            "name": "Gillian Dobbie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2896163244",
            "name": "Vithya Yogarajan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6680406137",
        "https://openalex.org/W2082496272",
        "https://openalex.org/W4229019932",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W6743716967",
        "https://openalex.org/W6851024552",
        "https://openalex.org/W2883445328",
        "https://openalex.org/W2588121495",
        "https://openalex.org/W6614489337",
        "https://openalex.org/W2779578326",
        "https://openalex.org/W4378498706",
        "https://openalex.org/W2908261578",
        "https://openalex.org/W2968526727",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2905589099",
        "https://openalex.org/W2398031410",
        "https://openalex.org/W4312091890",
        "https://openalex.org/W4312091426",
        "https://openalex.org/W4312107563",
        "https://openalex.org/W2991046523",
        "https://openalex.org/W6849844328",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4378770584",
        "https://openalex.org/W4313483544",
        "https://openalex.org/W4362707064",
        "https://openalex.org/W4293768991",
        "https://openalex.org/W4385574286",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2947488356",
        "https://openalex.org/W4318908031",
        "https://openalex.org/W4385573504",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4378505261",
        "https://openalex.org/W4377130740",
        "https://openalex.org/W4378473969",
        "https://openalex.org/W4385570412",
        "https://openalex.org/W4377121468",
        "https://openalex.org/W4304194220",
        "https://openalex.org/W4385571076",
        "https://openalex.org/W4380136143",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4362508231",
        "https://openalex.org/W4385571985",
        "https://openalex.org/W2136518234",
        "https://openalex.org/W4303648904",
        "https://openalex.org/W4323706372",
        "https://openalex.org/W4376653732",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4378942694",
        "https://openalex.org/W4385572634",
        "https://openalex.org/W4380356267",
        "https://openalex.org/W4322760437",
        "https://openalex.org/W4307125074",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W2086282622",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W419646410",
        "https://openalex.org/W4319653585",
        "https://openalex.org/W3171668871",
        "https://openalex.org/W4221143046"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive performance on a wide variety of tasks. However, apparent limitations hinder their performance, especially on tasks that require multiple steps of reasoning or compositionality. Arguably, the primary sources of these limitations are the decoding strategy and how the models are trained. We propose, and provide a general description of, an architecture that combines LLMs and cognitive architectures, called Language Model based Cognitive Architecture (LMCA), to overcome these limitations. We draw an analogy between this architecture and \"fast\" and \"slow\" thinking in human cognition.",
    "full_text": "A Proposal for a Language Model Based Cognitive Architecture\nKobe Knowles, Michael Witbrock, Gillian Dobbie, Vithya Yogarajan\nNAOInstitute, Waipapa Taumata Rau - The University of Auckland, New Zealand\nkobe.knowles@auckland.ac.nz\nAbstract\nLarge Language Models (LLMs) have shown impressive per-\nformance on a wide variety of tasks. However, apparent lim-\nitations hinder their performance, especially on tasks that re-\nquire multiple steps of reasoning or compositionality. Ar-\nguably, the primary sources of these limitations are the de-\ncoding strategy and how the models are trained. We pro-\npose, and provide a general description of, an architecture that\ncombines LLMs and cognitive architectures, calledLanguage\nModel based Cognitive Architecture (LMCA), to overcome\nthese limitations. We draw an analogy between this architec-\nture and “fast” and “slow” thinking in human cognition.\nIntroduction\nLarge language models (LLMs) have shown impressive per-\nformance on many tasks, such as programming, reasoning,\ntranslation, and question answering, often surpassing human\nperformance (Anil et al. 2023; Bubeck et al. 2023; Katz et al.\n2023; OpenAI 2023). However, even within LLMs’ impres-\nsive capabilities, there are evident limitations: LLMs rea-\nson semantically, not symbolically (Tang et al. 2023); strug-\ngle with pure causal reasoning (Jin et al. 2023); fail to un-\nderstand simple identifier swaps in Python (Miceli Barone\net al. 2023); often perform poorly on tasks that require multi-\nstep reasoning and compositionality (Dziri et al. 2023); and\nproduce confabulations (OpenAI 2023). These weaknesses\nraise questions about whether artificial general intelligence\ncan be achieved with LLMs and whether they have a deep\nintuitive understanding.\nTwo apparent constraints on contemporary LLMs are (1)\nthe next-token prediction decoding strategy and (2) the lack\nof explicit high-level cognition, which would enable slow\nthinking (Kahneman 2011). In (1), the decoding strategy\ndoes not allow back-tracking from a token once it has been\nproduced, potentially limiting the LLM’s ability to solve\ncomplex problems that are not trivial to answer by just pre-\ndicting the next token. There have been attempts to remedy\nthis, including Tree-of-Thought (Long 2023), Self-Refine\n(Madaan et al. 2023), and V oyager (Wang et al. 2023a),\nwhich embed LLMs in a higher-level architecture that al-\nlows for iteration over the LLM’s output. Related is (2),\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nhigh-level cognition, which would allow more complex op-\nerations than just predicting the next token. Cognitive ar-\nchitectures’ working memory systems exemplify this exten-\nsion (Anderson et al. 2004; Laird, Lebiere, and Rosenbloom\n2017; Laird 2019).\nFast and slow thinking are terms associated with the dual-\nprocessing theory (Evans and Stanovich 2013; Wason and\nEvans 1974), commonly described as System 1 and System\n2, respectively (Kahneman 2011). These represent different\nmodes of thinking: System 1 is fast, biased, and intuitive;\nSystem 2 is slow, effortful, and involves symbolic reasoning.\nWhile concerns have been raised about its validity (Conway-\nSmith and West 2023; Kruglanski 2013), this theory still\nserves as a useful base in comparing modes of thinking.\nContemporary LLMs are more akin to a version of Sys-\ntem 1, whose output could be compared to a thought that\npops into a human’s mind. While we can prompt LLMs,\ne.g., with chain-of-thought prompting (Wei et al. 2023), they\nstill do not have the ability to slowly think about a solution\nlike human cognition—although, the importance of this can\nbe argued about; if correct choices are made early enough\nand with sufficient accuracy, during feed-forward next-token\nprediction, the performance could in principle be similar.\nThis consideration includes the ability to deliberate on dif-\nferent strategies for solving a problem, concentrate on solv-\ning a specific part of a problem, and backtrack from cur-\nrently generated text. There have been many approaches that\nattempt to add System 2 abilities to machine learning mod-\nels (Chen et al. 2019; Fabiano et al. 2023; Lin et al. 2023;\nMiech et al. 2021). While they implement limited aspects of\na higher-level cognition, they lack other critical components\nthat contribute to the multiple modes of thinking in human\ncognition.\nWe propose an architecture intended to give abilities as-\nsociated with human cognition to LLMs (i.e., System 2),\nspecifically by combining previously described AI cognitive\narchitectures (Anderson, Matessa, and Lebiere 1997; Laird,\nLebiere, and Rosenbloom 2017; Laird 2019) and LLMs.\nWe call this Language Model based Cognitive Architecture\n(LMCA); subsequent sections provide a brief background\nabout LLMs and cognitive architectures, detail our LMCA,\nand provide concluding remarks regarding its prospects.\nAAAI Fall Symposium Series (FSS-23)\n295\nBackground\nA transformer is an architecture that eschews recurrence\nby relying only on attention, allowing for increased paral-\nlelization and the ability to model long-range dependencies\n(Vaswani et al. 2017). Transformers consist of many stacked\nlayers, each consisting of a multi-head attention module\nand a point-wise feed-forward module. Transformers are the\nbuilding block of current language models (Devlin et al.\n2019; OpenAI 2023; Raffel et al. 2019). Language mod-\nels with parameters in the billions are referred to as large\nlanguage models (LLMs). Prominent LLMs include GPT-4\n(OpenAI 2023), PaLM 2 (Anil et al. 2023), and Llama (Tou-\nvron et al. 2023a,b).\nWhile their performance is impressive on many accounts\n(Bubeck et al. 2023; Katz et al. 2023), recent research puts\ninto question their level of understanding compared against\nhumans’ (Jin et al. 2023; Miceli Barone et al. 2023; Tang\net al. 2023). Many approaches exist to improve LLMs’ ca-\npabilities: in-context learning (Chen et al. 2022; Dong et al.\n2023; Lampinen et al. 2022; Wei et al. 2023; Wu et al. 2023;\nZhang, Feng, and Tan 2022), including chain-of-thought\nprompting (Shi et al. 2022; Wei et al. 2023; Zhang et al.\n2022), and instruction tuning (Jang et al. 2023; Longpre\net al. 2023; Ouyang et al. 2022; Peng et al. 2023; Wang et al.\n2023c,b; Xu, Shen, and Huang 2023; Ye et al. 2023). While\nthese approaches each have benefits, they have not yet raised\nthe level of understanding to that of humans.\nCognitive architectures support research in modelling the\nhuman mind in terms of the underlying mechanisms that\nproduce behaviour (Kotseruba and Tsotsos 2020; Laird,\nLebiere, and Rosenbloom 2017); they are software imple-\nmentations of general theories of intelligence (Laird 2022).\nKotseruba and Tsotsos (2020) categorise cognitive architec-\ntures based on their representation and type of information\nprocessing: symbolic, emergent, and hybrid. Symbolic cog-\nnitive architectures (Forbus and Hinrich 2017; Gore et al.\n2011) are those that represent concepts with symbols that are\nmanipulated by pre-defined instructions. They perform well\nat planning and formal reasoning but are not flexible and ro-\nbust. Emergent cognitive architectures (O’Reilly, Hazy, and\nHerd 2016; Rohrer 2011) are those associated with the con-\ncept of a neuron (biologically plausible or not); information\nis processed through the propagation of signals. They per-\nform well at learning and adaptability, and allow for enor-\nmously parallel processing, but are not transparent and per-\nform more poorly at logical inference than their symbolic\ncounterparts. Hybrid cognitive architectures (Laird 2019;\nRitter, Tehranchi, and Oury 2019; Sun 2017) combine ele-\nments of both symbolic and emergent architectures; some\nare more symbolic than emergent and vice versa.\nThe structure of a cognitive architecture generally con-\nsists of short-term and long-term memory, a motor mod-\nule, and a perception module (Anderson et al. 2004; Laird\n2019; Sun 2017). Short-term memory is usually called work-\ning memory, and long-term memory is split into declarative\nand procedural memory. Some procedure retrieves struc-\ntures from long-term memory and stores them in working\nmemory; alternatively, the perception module can add struc-\ntures into working memory. Operations can be applied to\nthe structures in working memory to create new structures,\nmodify structures, and remove structures. When appropri-\nate, the motor module produces an action to be executed in\nthe environment, given the current state of working mem-\nory. The Common Model of Cognition (Laird, Lebiere, and\nRosenbloom 2017) mirrors this characterisation of cognitive\narchitectures.\nLanguage Model Based Cognitive Architecture\nWe provide a general description of the proposed architec-\nture, LMCA, depicted in Figure 1, to be used in a textual\ninput-output setting. LMCA models the interplay between\nshort-term and long-term memory, with the objective of\ncompleting the input task. The types of tasks we consider are\nthose limited to the generation of a singular response, i.e.,\nthey are answerable. Long-term memory consists of mul-\ntiple modules: the Memory Module, Thought Module, and\nAction Module. Short-term “working” memory, stores struc-\ntures created by long-term memory modules. A Retrieval\nModule attends to structures in working memory, which are\ninput to the modules in long-term memory.\nWorking Memory and Retrieval\nWorking memory stores relevant structures for solving the\ncurrent task. A state in working memory is a snapshot of the\ncontents of working memory at a given time. We propose\nfive buffers in working memory: the Memory Buffer, Task\nBuffer, Thought Buffer, Struct Buffer, and Action Buffer.\nA structure in working memory is defined quite generally\nand differs depending on the working memory buffer; each\nbuffer has its own structure variant, some being more flex-\nible than others. The minimum requirements of a struc-\nture are that it has a natural language representation that\ncan be input to other modules (i.e., the Retrieval Module,\nMemory Module, Thought Module, and Action Module), a\nunique identifier, and an integer attention value represent-\ning whether the current structure is attended to. The imple-\nmentation of attention in the architecture could correspond\nto soft attention (a structure can be partially attended to) or\nhard attention (can either be fully attended to or not at all).\nThere could be an absolute hard cut-off value independent\nof other structures in working memory and a relative cut-\noff value dependent on other structures in working mem-\nory based on the context length of the modules in long-term\nmemory.\nThe Memory Buffer, Thought Buffer, and Action Buffer\nstore a history of outputs from the Memory Module,\nThought Module, and Action Module from long-term mem-\nory, respectively. The structures in these buffers will have a\ntime value (when each structure was created), an attention\nvalue, a unique identifier, and a natural language description\n(the output of their corresponding modules). Structures in\nthe Task Buffer are similar, only that there is an additional\nboolean value indicating if the structure corresponds to the\ninput task and a slot for an answer to the task to be inserted.\nA sub-task (a task that is not the input task) could be solved\nin a sub-working memory where the input task is the sub-\ntask.\n296\nThought Module\nWorking Memory\nMemory Module Action Module\nRetrieval Module\n(Attend to structures in \nworking memory)\nMemory \nBuffer\nTask\nBuffer\nThought \nBuffer\nAction \nBuffer\nStruct \nBuffer\nAction Queue\nAnswer to Input Task\nLong-Term Memory\nGenerated Thought\n(“conscious” awareness)\nAnswerable \nInput task\nFigure 1: The proposed LMCA in a text-only input-output setting. The architecture consists of three long-term memory modules\nrealised as language models and a retrieval module that attends to structures in working memory; working memory stores\nstructures that are iteratively added to by long-term memory over multiple time steps until an action is produced, prompting the\narchitecture to answer the input task. Each long-term memory module receives as input a textual representation of the attended-\nto structures in working memory. The Action Module produces actions which are appended to the Action Queue; actions fulfil\nthe purpose of creating and modifying existing structures in working memory, which are stored in five buffers.\nThe Struct Buffer, named after structs in C, consists of\nmore complex structures. A fluctuating number of variables\ncan be defined in any structure and these variables can have\none of several different data types. An operator is some-\nthing that modifies structures in working memory. Allowed\noperators, such as modifying a variable (i.e., incrementing\nan integer) or performing operations involving two or more\nstructures, can be defined for structures in the Struct Buffer.\nStructures are created by the Action Module.\nThe Retrieval Module has the sole purpose of attending\nto salient structures in working memory based on their rele-\nvance to the current action and is biased by recent thoughts\nand memories. If the action is to generate memories from the\nMemory Module, then salient structures in working mem-\nory should be attended to. The Retrieval Module could be\nrealised as a language model that has a large enough context\nlength to process all of the working memory at once, pro-\nducing an attention value for each structure, or instead out-\nputting in order the most salient structures autoregressively\nby referencing their identifiers.\nMemory Module\nThe Memory Module has the sole purpose of generating rel-\nevant memories given the attended-to structures of working\nmemory and is a form of declarative memory. It can be re-\nalised as a language model, a database of some sort, or a\ncombination of the two. A memory has varying levels of ab-\nstraction. At the lowest level of abstraction, we can gener-\nate past states of working memory directly. At higher levels\nof abstraction, we can generate subsets of previous working\nmemory states, based on what was attended to in that work-\ning memory state. At the highest level of abstraction, this\ncould be a natural language description of a previous work-\ning memory state, summarising the most important elements\nand their impacts on producing a solution.\nThought Module\nThe Thought Module’s role is to produce thoughts related\nto metacognition, given the attended-to element of working\nmemory, and is language model based. The thoughts could\nbe related to monitoring and regulation of cognition, a plan\non how to solve the current task, the next step in solving the\ntask, generating an answer to the current task, and generating\na new intermediary task that will help solve another task.\nThe thoughts are stored in the Thought Buffer in working\nmemory, where they can influence the next action, memory\ngenerated, and what is attended to in working memory.\n297\nAction Module\nThe Action Module’s purpose is to produce actions that can\nadd structures, modify structures, and remove structures in\nworking memory, and can determine when an output should\nbe generated for the currently attended-to task, given the re-\ntrieved structures from working memory. This module is a\nform of procedural memory. The Action Module will be a\nlanguage model where the decoding process is constrained\nto produce valid actions. Multiple actions can be proposed\nwith one call to the Action Module, where they will be added\nto the Action Buffer in working memory and appended to the\nAction Queue. The queue will be first-in, first-out; the ac-\ntions will be executed sequentially until the queue is empty.\nWhen the Action Queue is empty, the Action Module will\nbe tasked with producing more actions.\nPossible actions include generating a thought based on\nworking memory, generating memories based on working\nmemory, generating structures in the Struct Buffer, gener-\nating a new task through generating a thought, modifying\nstructures in the Struct Buffer in working memory via ap-\nplying operators to structures, and generating an answer to\na task. The action-generating process concludes when the\ninput task is answered.\nRealising Fast and Slow Thinking\nIn LMCA, fast thinking occurs when an action is generated\ninitially to complete a task. For example, when the task is\nto answer “What is 2+2?”, the Action Module should gener-\nate an action tasking the Thought Module to answer with-\nout other actions being applied to working memory. The\nfirst thought that the architecture produces is the answer to\nthe task’s question, i.e., 4; a fast, intuitive, and automatic\nthought similar to a thought a human would produce given\nthe same task. Slow thinking occurs when a way to com-\nplete the task cannot be found easily by the architecture.\nThoughts associated with planning, reasoning, and deliber-\nation are generated. To count as slow thinking, the thought\nprocess should be categorized by substantial effort and in-\nvolve many steps.\nTraining\nThe main challenge in realising LMCA is in training it. The\nMemory Module, Thought Module, Action Module and Re-\ntrieval Module are the four components with parameters that\nneed to be trained. 1 Initially, each module should be pre-\ntrained, i.e., each module should have some innate knowl-\nedge. The challenge will be acquiring data involving the\nstructures of working memory when solving a task and the\nassociated optimal outputs of each module. This includes\nthe optimal structures to retrieve from a working memory\nstate and the optimal memories, thoughts, and actions given\na working memory state.\nInstruction tuning (Wang et al. 2023b) is one approach\nthat can be used for training. A high-quality set of examples\nto train each module should be created. This entails con-\nstructing examples of the desired behaviour when solving a\n1If the Memory Module is realised as a database with no lan-\nguage model component then there are no parameters to train.\nproblem, i.e., ideal working memory states and the desired\noutputs of each module. Generating data, both manually and\nautomatically, is a challenge that needs to be overcome. An-\nother option is to pre-train the modules on other represen-\ntative tasks, ideally in a self-supervised manner. The mod-\nules can be trained further on the previously mentioned in-\nstruction tuning examples or in a multi-agent reinforcement\nlearning scenario (Nguyen, Nguyen, and Nahavandi 2020;\nOroojlooy and Hajinezhad 2023; Zhang, Yang, and Bas ¸ar\n2021).\nA key aspect of this architecture is its capacity for con-\ntinual learning, which depends on updating both long-term\nmemory—including the storage of experiences—and the\nRetrieval Module as the model acquires experience. One key\narea to consider supporting continual learning is utilising\nthought module states (its “thoughts”) to identify and reason\nabout errors made. Mechanisms are needed to store memo-\nries and update the parameters of the architecture given these\nthoughts, which includes the need to generate errors inter-\nnally. Also salient is how to identify and penalise incorrect\nthoughts, based on reflection and stored memories of previ-\nous thoughts.\nConclusion\nWe have proposed an architecture, LMCA, that aims to mit-\nigate the limitations of contemporary LLMs by combining\ntheir capabilities with structures drawn from previous work\non cognitive architectures in AI. In the context of fast and\nslow thinking, we aim to give LLMs slow thinking capa-\nbilities similar to human cognition, but in a fully trainable\nsetting. In LMCA, LLMs are prominent, being utilised in\nall four cognitive modules. Avenues to support training the\narchitecture are mentioned, along with considerations of im-\nportance and difficulty of generating data and the possibil-\nity of generating errors internally dictated by a generated\nthought. The next, and vital, step will be realising this archi-\ntecture in a software implementation.\nReferences\nAnderson, J. R.; Bothell, D.; Byrne, M. D.; Douglass, S.;\nLebiere, C.; and Qin, Y . 2004. An Integrated Theory of the\nMind. Psychological Review, 111(4): 1036–1060.\nAnderson, J. R.; Matessa, M.; and Lebiere, C. 1997. ACT-R:\nA Theory of Higher Level Cognition and Its Relation to Vi-\nsual Attention. Human–Computer Interaction, 12(4): 439–\n462.\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.;\nPassos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; Chu,\nE.; Clark, J. H.; El Shafey, L.; Huang, Y .; Meier-Hellstern,\nK.; Mishra, G.; Moreira, E.; Omernick, M.; Robinson, K.;\nRuder, S.; Tay, Y .; Xiao, K.; Xu, Y .; Zhang, Y .; Abrego,\nG. H.; Ahn, J.; Austin, J.; Barham, P.; Botha, J.; Bradbury, J.;\nBrahma, S.; Brooks, K.; Catasta, M.; Cheng, Y .; Cherry, C.;\nChoquette-Choo, C. A.; Chowdhery, A.; Crepy, C.; Dave, S.;\nDehghani, M.; Dev, S.; Devlin, J.; D ´ıaz, M.; Du, N.; Dyer,\nE.; Feinberg, V .; Feng, F.; Fienber, V .; Freitag, M.; Gar-\ncia, X.; Gehrmann, S.; Gonzalez, L.; Gur-Ari, G.; Hand, S.;\nHashemi, H.; Hou, L.; Howland, J.; Hu, A.; Hui, J.; Hurwitz,\n298\nJ.; Isard, M.; Ittycheriah, A.; Jagielski, M.; Jia, W.; Kenealy,\nK.; Krikun, M.; Kudugunta, S.; Lan, C.; Lee, K.; Lee, B.;\nLi, E.; Music Li; Li, W.; Li, Y .; Li, J.; Lim, H.; Lin, H.; Liu,\nZ.; Liu, F.; Maggioni, M.; Mahendru, A.; Maynez, J.; Misra,\nV .; Moussalem, M.; Nado, Z.; Nham, J.; Ni, E.; Nystrom,\nA.; Parrish, A.; Pellat, M.; Polacek, M.; Polozov, A.; Pope,\nR.; Qiao, S.; Reif, E.; Richter, B.; Riley, P.; Castro Ros,\nA.; Roy, A.; Saeta, B.; Samuel, R.; Shelby, R.; Slone, A.;\nSmilkov, D.; So, D. R.; Sohn, D.; Tokumine, S.; Valter, D.;\nVasudevan, V .; V odrahalli, K.; Wang, X.; Wang, P.; Wang,\nZ.; Wang, T.; Wieting, J.; Wu, Y .; Xu, K.; Xu, Y .; Xue, L.;\nYin, P.; Yu, J.; Zhang, Q.; Zheng, S.; Zheng, C.; Zhou, W.;\nZhou, D.; Petrov, S.; and Wu, Y . 2023. PaLM 2 Technical\nReport. arXiv:2305.10403.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early Experiments\nwith GPT-4. arXiv:2303.12712.\nChen, D.; Bai, Y .; Zhao, W.; Ament, S.; Gregoire, J. M.; and\nothers. 2019. Deep Reasoning Networks: Thinking Fast and\nSlow. arXiv:1906.00855.\nChen, M.; Du, J.; Pasunuru, R.; Mihaylov, T.; Iyer, S.; Stoy-\nanov, V .; and Kozareva, Z. 2022. Improving In-Context Few-\nShot Learning via Self-Supervised Training. In Proceedings\nof the 2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies, 3558–3573. Seattle, United States: As-\nsociation for Computational Linguistics.\nConway-Smith, B.; and West, R. L. 2023. Clarifying Sys-\ntem 1 & 2 Through the Common Model of Cognition.\narXiv:2305.10654.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nDong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun,\nX.; Xu, J.; Li, L.; and Sui, Z. 2023. A Survey on In-Context\nLearning. arXiv:2301.00234.\nDziri, N.; Lu, X.; Sclar, M.; Li, X. L.; Jian, L.; Lin, B. Y .;\nand others. 2023. Faith and Fate: Limits of Transformers on\nCompositionality. arXiv:2305.18654.\nEvans, J. S. B. T.; and Stanovich, K. E. 2013. Dual-Process\nTheories of Higher Cognition: Advancing the Debate. Per-\nspectives on Psychological Science: a Journal of the Asso-\nciation for Psychological Science, 8(3): 223–241.\nFabiano, F.; Pallagani, V .; Ganapini, M. B.; Horesh, L.;\nLoreggia, A.; Murugesan, K.; Rossi, F.; and Srivastava, B.\n2023. Fast and Slow Planning. arXiv:2303.04283.\nForbus, K. D.; and Hinrich, T. 2017. Analogy and Relational\nRepresentations in the Companion Cognitive Architecture.\nAI Magazine, 38(4): 34–42.\nGore, B. F.; Hooey, B. L.; Wickens, C. D.; Socash, C.;\nGosakan, M.; Gacy, M.; Brehon, M.; and Foyle, D. C. 2011.\nWorkload as a Performance Shaping Factor in MIDAS v5.\nIn 20th Behavior Representation in Modeling and Simula-\ntion (BRIMS) Conference.\nJang, J.; Kim, S.; Ye, S.; Kim, D.; Logeswaran, L.; Lee,\nM.; Lee, K.; and Seo, M. 2023. Exploring the Benefits of\nTraining Expert Language Models over Instruction Tuning.\narXiv:2302.03202.\nJin, Z.; Liu, J.; Lyu, Z.; Poff, S.; Sachan, M.; Mi-\nhalcea, R.; Diab, M.; and Sch ¨olkopf, B. 2023. Can\nLarge Language Models Infer Causation from Correlation?\narXiv:2306.05836.\nKahneman, D. 2011. Thinking, Fast and Slow. Farrar,\nStraus and Giroux.\nKatz, D. M.; Bommarito, M. J.; Gao, S.; and Arredondo, P.\n2023. GPT-4 Passes the Bar Exam.\nKotseruba, I.; and Tsotsos, J. K. 2020. 40 Years of Cog-\nnitive Architectures: Core Cognitive Abilities and Practical\nApplications. Artificial Intelligence Review, 53(1): 17–94.\nKruglanski, A. W. 2013. Only One? The Default Interven-\ntionist Perspective as a Unimodel—Commentary on Evans\n& Stanovich (2013). Perspectives on Psychological Science,\n8(3): 242–247. PMID: 26172966.\nLaird, J. E. 2019. The Soar Cognitive Architecture. MIT\nPress. ISBN 9780262538534.\nLaird, J. E. 2022. Introduction to Soar. arXiv:2205.03854.\nLaird, J. E.; Lebiere, C.; and Rosenbloom, P. S. 2017. A\nStandard Model of the Mind: Toward a Common Compu-\ntational Framework across Artificial Intelligence, Cognitive\nScience, Neuroscience, and Robotics. AI Magazine, 38(4):\n13–26.\nLampinen, A.; Dasgupta, I.; Chan, S.; Mathewson, K.;\nTessler, M.; Creswell, A.; McClelland, J.; Wang, J.; and\nHill, F. 2022. Can Language Models Learn from Explana-\ntions in Context? In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, 537–563. Abu Dhabi,\nUnited Arab Emirates: Association for Computational Lin-\nguistics.\nLin, B. Y .; Fu, Y .; Yang, K.; Ammanabrolu, P.; Brahman,\nF.; Huang, S.; Bhagavatula, C.; Choi, Y .; and Ren, X. 2023.\nSwiftSage: A Generative Agent with Fast and Slow Think-\ning for Complex Interactive Tasks. arXiv:2305.17390.\nLong, J. 2023. Large Language Model Guided Tree-of-\nThought. arXiv:2305.08291.\nLongpre, S.; Hou, L.; Vu, T.; Webson, A.; Chung, H. W.;\nTay, Y .; Zhou, D.; Le, Q. V .; Zoph, B.; Wei, J.; and Roberts,\nA. 2023. The Flan Collection: Designing Data and Methods\nfor Effective Instruction Tuning. arXiv:2301.13688.\nMadaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.;\nWiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang,\nY .; Welleck, S.; Majumder, B. P.; Gupta, S.; Yazdanbakhsh,\nA.; and Clark, P. 2023. Self-Refine: Iterative Refinement\nwith Self-Feedback. arXiv:2303.17651.\n299\nMiceli Barone, A. V .; Barez, F.; Cohen, S. B.; and Konstas,\nI. 2023. The Larger they are, the Harder they Fail: Lan-\nguage Models do not Recognize Identifier Swaps in Python.\nIn Findings of the Association for Computational Linguis-\ntics: ACL 2023, 272–292. Toronto, Canada: Association for\nComputational Linguistics.\nMiech, A.; Alayrac, J.-B.; Laptev, I.; Sivic, J.; and Zisser-\nman, A. 2021. Thinking Fast and Slow: Efficient Text-to-\nVisual Retrieval with Transformers. arXiv:2103.16553.\nNguyen, T. T.; Nguyen, N. D.; and Nahavandi, S. 2020.\nDeep Reinforcement Learning for Multiagent Systems: A\nReview of Challenges, Solutions, and Applications. IEEE\nTransactions on Cybernetics, 50(9): 3826–3839.\nOpenAI. 2023. GPT-4 Technical Report.arXiv:2303.08774.\nO’Reilly, R. C.; Hazy, T. E.; and Herd, S. A. 2016. The\nLeabra Cognitive Architecture: How to Play 20 Principles\nwith Nature. The Oxford Handbook of Cognitive Science,\n91: 91–116.\nOroojlooy, A.; and Hajinezhad, D. 2023. A Review of Coop-\nerative Multi-Agent Deep Reinforcement Learning. Applied\nIntelligence, 53(11): 13677–13722.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and\nLowe, R. 2022. Training Language Models to Follow In-\nstructions with Human Feedback. arXiv:2203.02155.\nPeng, B.; Li, C.; He, P.; Galley, M.; and Gao, J. 2023. In-\nstruction Tuning with GPT-4. arXiv:2304.03277.\nRaffel, C.; Shazeer, N. M.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. ArXiv, abs/1910.10683.\nRitter, F. E.; Tehranchi, F.; and Oury, J. D. 2019. ACT-R:\nA Cognitive Architecture for Modeling Cognition. Wiley\nInterdisciplinary Reviews. Cognitive Science, 10(3): e1488.\nRohrer, B. 2011. BECCA: Reintegrating AI for Natural\nWorld Interaction. AAAI Spring Symposium: Designing In-\ntelligent Robots.\nShi, F.; Suzgun, M.; Freitag, M.; Wang, X.; Srivats, S.;\nV osoughi, S.; Chung, H. W.; Tay, Y .; Ruder, S.; Zhou, D.;\nDas, D.; and Wei, J. 2022. Language Models are Multilin-\ngual Chain-of-Thought Reasoners. arXiv:2210.03057.\nSun, R. 2017. The CLARION Cognitive Architecture: To-\nward a Comprehensive Theory of the Mind. In Chipman, S.\nE. F., ed., The Oxford Handbook of Cognitive Science, vol-\nume 375, 117–133. New York, NY , US: Oxford University\nPress, xi.\nTang, X.; Zheng, Z.; Li, J.; Meng, F.; Zhu, S.-C.; Liang,\nY .; and Zhang, M. 2023. Large Language Models are In-\nContext Semantic Reasoners Rather than Symbolic Reason-\ners. arXiv:2305.14825.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lam-\nple, G. 2023a. LLaMA: Open and Efficient Foundation Lan-\nguage Models. arXiv:2302.13971.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\nrull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\nGao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\nS.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\nKloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\nSilva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\nB.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\nZarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\ndriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b.\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv:2307.09288.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention Is All You Need. arXiv [cs.CL].\nWang, G.; Xie, Y .; Jiang, Y .; Mandlekar, A.; Xiao, C.; Zhu,\nY .; Fan, L.; and Anandkumar, A. 2023a. V oyager: An\nOpen-Ended Embodied Agent with Large Language Mod-\nels. arXiv:2305.16291.\nWang, Y .; Ivison, H.; Dasigi, P.; Hessel, J.; Khot, T.; Chandu,\nK. R.; Wadden, D.; MacMillan, K.; Smith, N. A.; Beltagy,\nI.; and Hajishirzi, H. 2023b. How Far Can Camels Go? Ex-\nploring the State of Instruction Tuning on Open Resources.\narXiv:2306.04751.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2023c. Self-Instruct: Align-\ning Language Models with Self-Generated Instructions. In\nProceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n13484–13508. Toronto, Canada: Association for Computa-\ntional Linguistics.\nWason, P. C.; and Evans, J. S. T. B. T. 1974. Dual Processes\nin Reasoning? Cognition, 3(2): 141–154.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023. Chain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels. arXiv:2201.11903.\nWu, Z.; Wang, Y .; Ye, J.; and Kong, L. 2023. Self-Adaptive\nIn-Context Learning: An Information Compression Perspec-\ntive for In-Context Example Selection and Ordering. InPro-\nceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 1423–\n1436. Toronto, Canada: Association for Computational Lin-\nguistics.\nXu, Z.; Shen, Y .; and Huang, L. 2023. MultiInstruct: Im-\nproving Multi-Modal Zero-Shot Learning via Instruction\nTuning. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers), 11445–11465. Toronto, Canada: Association for\nComputational Linguistics.\nYe, S.; Hwang, H.; Yang, S.; Yun, H.; Kim, Y .; and Seo, M.\n2023. In-Context Instruction Learning. arXiv:2302.14691.\n300\nZhang, K.; Yang, Z.; and Bas ¸ar, T. 2021. Multi-Agent Re-\ninforcement Learning: A Selective Overview of Theories\nand Algorithms. In Vamvoudakis, K. G.; Wan, Y .; Lewis,\nF. L.; and Cansever, D., eds., Handbook of Reinforcement\nLearning and Control, 321–384. Cham: Springer Interna-\ntional Publishing. ISBN 9783030609900.\nZhang, Y .; Feng, S.; and Tan, C. 2022. Active Example Se-\nlection for In-Context Learning. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language\nProcessing, 9134–9148. Abu Dhabi, United Arab Emirates:\nAssociation for Computational Linguistics.\nZhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Auto-\nmatic Chain of Thought Prompting in Large Language Mod-\nels. arXiv:2210.03493.\n301"
}