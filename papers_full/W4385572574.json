{
  "title": "Better Language Models of Code through Self-Improvement",
  "url": "https://openalex.org/W4385572574",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5014650324",
      "name": "Hung Quoc To",
      "affiliations": [
        "Software (Spain)",
        "FPT University"
      ]
    },
    {
      "id": "https://openalex.org/A5062399934",
      "name": "Nghi Bui",
      "affiliations": [
        "McGill University",
        "Fulbright University Vietnam"
      ]
    },
    {
      "id": "https://openalex.org/A5010572058",
      "name": "Jin Guo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5089000736",
      "name": "Tien N. Nguyen",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4226485558",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3035214886",
    "https://openalex.org/W4284688961",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2414484917",
    "https://openalex.org/W2962936887",
    "https://openalex.org/W4285177871",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W648786980",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W3169008558",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4394638297",
    "https://openalex.org/W4221166942"
  ],
  "abstract": "Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a data augmentation framework using knowledge distillation. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to augment training data, which is then used for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs’ performance in sequence-generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 12994–13002\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nBetter Language Models of Code through Self-Improvement\nHung Quoc To♣∗, Nghi D. Q. Bui♦∗, Jin Guo♠, Tien N. Nguyen♢\n♣ FPT Software AI Center, ♦Department of Computer Science, Fulbright University, Viet Nam\n♠School of Computer Science, McGill University, Canada\n♢School of Engineering and Computer Science , The University of Texas at Dallas, USA\nhungtq29@fsoft.com.vn, dqnbui.2016@smu.edu.sg,\njguo@cs.mcgill.ca, tien.n.nguyen@utdallas.edu\nAbstract\nPre-trained language models for code (PLMCs)\nhave gained attention in recent research. These\nmodels are pre-trained on large-scale datasets\nusing multi-modal objectives. However, fine-\ntuning them requires extensive supervision and\nis limited by the size of the dataset provided.\nWe aim to improve this issue by proposing a\ndata augmentation framework using knowledge\ndistillation. Our framework utilizes knowl-\nedge gained during the pre-training and fine-\ntuning stage to augment training data, which\nis then used for the next step. We incorpo-\nrate this framework into the state-of-the-art lan-\nguage models, such as CodeT5, CodeBERT,\nand UnixCoder. The results show that our\nframework significantly improves PLMCs’ per-\nformance in sequence-generation tasks, such as\ncode summarization and code generation in the\nCodeXGLUE benchmark.\n1 Introduction\nPre-trained models for code (PLMCs), such as\nCodeBERT (Feng et al., 2020), PLBART (Ahmad\net al., 2021), CodeT5 (Wang et al., 2021), UniX-\nCoder (Guo et al., 2022), and DISCO (Ding et al.,\n2022), have become the foundation to solve many\npractical software engineering tasks such as code\nsummarization, code translation, program repair.\nThose PLMCs, like large language models (LLMs),\nare typically first pretrained on very large-scale\ndatasets with a variety of multi-modal objectives\nunder a self-supervised training style. They can\nthen be fine-tuned using task-specific datasets in a\nsupervised training style.\nWe hypothesise that, while fine-tuned models\nmay not achieve peak performance, PLMCs can\nproduce reasonable outputs that can be regarded\nas high quality data because they have been pre-\ntrained on large scale datasets, and that such data\n∗Equal contribution. Listing order is based on the alpha-\nbetical ordering of author surnames.\ncan be leveraged as additional high-quality training\ndata. Our framework utilizes the self-improvement\ncapability of PLMCs through an simple data aug-\nmentation step. This approach is particularly useful\nfor tasks involving code-related sequence genera-\ntion, such as code summarization and code genera-\ntion. Our method involves fine-tuning a PLMC on\na downstream dataset, allowing the model to gain\nknowledge about the task. The model then gener-\nates an augmented version of the original training\ndata, which are used to further fine-tuning. Our\nframework is similar to sequence-level knowledge\ndistillation (Kim and Rush, 2016), but our approach\nfocuses on improving model performance without\ncompressing the model by utilizing the same tech-\nnique.\nOur empirical evaluation results show that our\nframework significantly improves the state-of-the-\narts PLMCs, including CodeBERT, CodeT5, UniX-\nCoder with significant margins. In short, we sum-\nmarize our contributions as follows.\n• We present a simple self-improvement frame-\nwork and show how it can be easily adapted to\nPLMCs for the task of code-related sequence\ngeneration.\n• We conduct extensive evaluation on two tasks:\ncode summarization and code generation, and\ncompare it with the well-known, state-of-the-art\nPLMCs. The results show that our framework\nconsistently improvesover all PLMCs by a sig-\nnificant margin in those tasks.\n• We provide analysis and explanations on how uti-\nlizing a simple framework consistently improves\nthe performance of PLMCs.\nOur work is publicly available. 1\n1https://github.com/Fsoft-AIC/\nCode-LM-Self-Improvement\n12994\n... ...\nOriginal\ntrain dataset\nAugmented datasetData augmentation\n- Figure (2)\nFine-tuning Self-improving...\nFigure 1: Overall training pipeline.\nOriginal\ntrain dataset\nAugmented dataset\n...\nbeam search\nAdding to\nFigure 2: Demonstrating the data augmentation process\nin our work.\n2 Related Work\nExposure bias and hallucination in Sequence\nGeneration Tasks The exposure bias problem\nis regarded as the difference between the training\nand inference phases for auto-regressive sequence\ngeneration models. Previous work has attempted\nto reduce exposure bias in training phase (Bengio\net al., 2015; Ranzato et al., 2015; Wiseman and\nRush, 2016; Wang and Sennrich, 2020). In the\nsense that our self-improvement step involves train-\ning model on its own prediction, the exposure bias\nis close to our approach.\nCode understanding and generation Code\nlearning problems have recently emerged as one\nof the primary tasks for assessing the capability\nof language models. Most recent code models are\npretrained on multi-modal objectives before being\nfine-tuned on specific downstream tasks (Feng\net al., 2020; Ahmad et al., 2021; Wang et al., 2021;\nGuo et al., 2022; Ding et al., 2022).\nKnowledge Distillation Knowledge distillation\nis the process of transferring knowledge from a\nlarge unwieldy model or set of models to a single\nsmaller model that can be practically deployed un-\nder real-world constraints, and such smaller model\ncan usually keep the same performance or even\nbetter than the original model (Hinton et al., 2015;\nKim and Rush, 2016; Wang et al., 2020; Chen et al.,\n2020; Mukherjee et al., 2021). We perform an ad-\nditional self-improvement step to improve the orig-\ninal model without using external resources, our\nwork is relevant to knowledge distillation.\n3 Method\nAlgorithm 1Data Augmentation Process\nInput:\n• θfine−tuned, the fine-tuned model checkpoint\non a specific task T ∈ {code summarization,\ncode generation, etc. }.\n• D = {(xi,yi) ∣i = 1,n}, the train dataset\non which the θfine−tuned is fine-tuned.\n• Bk denotes the beamsearchalgorithm with\nbeam size of k. It returns a list of k best se-\nquences as prediction.\nOutput:\n• Augmented dataset ˜D\n1: procedure DATAAUGMENTATION PROCESS\n2: ˜D← ∅\n3: for eachdatapoint (xi,yi) ∈Ddo:\n4: LK ← Bk(Pθfine−tuned(y∣ xi))\n5: In other words, LK =\n[ˆyi1,ˆyi2,..., ˆyik]\n6: ˜yi ← argmax\nˆyij∈LK\n(sim(ˆyij,yi))\n7: Adding (xi,˜yi) → ˜D\n8: end for\n9: return ˜D\n10: end procedure\nOur method utilizes three distinct sets of model pa-\nrameters: θpre−trained, θfine−tuned, and θimproved.\nEach corresponds to the stage of the model parame-\nters after pre-trained, fine-tuned, and self-improved,\nrespectively. The model generates tokens in auto-\nregressive manner, progressing token-by-token.\nTypically, models are pretrained on large\nscale corpora, resulting in a pre-trained check-\npoint θpre−trained. These pre-trained models\nare subsequently fine-tuned on a targeted down-\nstream dataset D using a supervised learning ap-\nproach, yielding in a set of fine-tuned parameters\nθfine−tuned. Our investigation has revealed that\nmodel’s performance can be further enhanced if\nwe continue to fine-tuned these parameters on an\naugmented version of D. Figure 1 portrays our\nproposed self-improvement step within the broader\ntraining framework. This step encompasses a data\n12995\nModels Beam sizes Methods Ruby JavaScript Go Python Java PHP Overall\nRoBERTa 10 (Liu et al., 2019) 11.17 11.90 17.72 18.14 16.47 24.02 16.57\nPLBART 10 (Ahmad et al., 2021) 14.11 15.56 18.91 19.30 18.45 23.58 18.32\nPolyglotCodeBERT 10 (Ahmed and Devanbu, 2021) 14.75 15.80 18.77 18.71 20.11 26.23 19.06\nCodeBERT 1 Baseline 12.04 14.73 17.58 18.47 17.62 23.44 17.31\nSelf-Improved 12.40 15.44 18.52 19.09 18.31 24.46 18.04\n5 Baseline 12.31 15.76 18.12 19.09 18.37 24.85 18.08\nSelf-Improved 12.55 16.41 18.69 19.50 18.88 25.21 18.54\n10 Baseline 12.22 15.78 18.01 19.09 18.42 25.06 18.10\nSelf-Improved 12.52 16.39 18.66 19.50 18.92 25.28 18.54\nCodeT5 (base) 1 Baseline 14.80 15.57 19.57 19.86 19.87 25.33 19.17\nSelf-Improved 15.46 16.22 20.12 20.36 20.25 26.25 19.78\n5 Baseline 15.23 16.18 19.95 20.42 20.26 26.11 19.69\nSelf-Improved 15.60 16.51 20.26 20.59 20.44 26.46 19.97\n10 Baseline 15.29 16.18 19.95 20.42 20.26 26.10 19.70\nSelf-Improved 15.70 16.47 20.28 20.58 20.45 26.58 20.00\nUniXCoder (base) 1 Baseline 14.83 15.39 18.95 18.66 19.37 24.99 18.70\nSelf-Improved 15.36 15.83 19.42 19.13 20.04 26.05 19.31\n5 Baseline 15.17 15.93 19.52 19.25 20.18 26.45 19.42\nSelf-Improved 15.37 15.95 19.73 19.55 20.45 26.69 19.62\n10 Baseline 14.74 15.84 19.31 19.12 20.11 26.75 19.31\nSelf-Improved 15.37 15.96 19.73 19.56 20.44 26.79 19.64\nTable 1: Results on code summarization evaluated with smoothed BLUE-4. The “Overall” column presents average\nscores over six programming languages. The bolded numbers represent the best scores (higher is better) when\ncomparing between Baseline and Self-Improved for each model and each value of beam size.\naugmentation technique and an additional round\nof fine-tuning, in addition to the pre-traingn and\nfine-tuning paradigm.\nThe process of augmenting the dataset is illus-\ntrated in Figure 2. We provide a detailed algorithm\nfor this procedure in Algorithm 1. For each train-\ning pair of sequences (xi,yi)in the train dataset D,\nwe employ beam search to generate a list of K-best\npredictions LK. This list comprises kpredictions,\nwhere k represents the beam size. Subsequently,\nwe evaluate the similarity between each prediction\nˆyij and its corresponding ground truth sequence yi\nusing a similarity function sim based on BLEU\nscore. The prediction with highest similarity is\nthen selected ˜yi = argmaxˆyij∈LK (sim(ˆyij,yi)).\nFinally, we add the sequence pair (xi,˜yi) to a new\nempty dataset ˜D, which we refer as the augmented\ndataset. Essentially, the augmented dataset con-\ntains an equal number of datapoints as the original\ntraining dataset due to an one-by-one mapping dur-\ning augmetation. Moreover, the augnmentation\nprocess occurs offline, with each newly augmented\ndatapoint being saved to storage before being used\nfor training in the self-improving phase.\nThe subsequent step involves fine-tuning\nθfine−tuned on ˜Duntil convergence. This results in\na new set of model parameters denoted asθimproved.\nIt is important to note that the index j in ˆyij de-\nnotes the jth prediction in the beam, rather than the\njth token of the predicted sequence. Additionally,\nonly the training dataset Dis augmented, while the\nvalidation and test dataset remain unchanged for\nevaluation purpose.\n4 Experimental Setup\nOur goal is to show that for any of the fine-tuned\nmodel for a sequence generation task (F-PLMC),\nafter applying our self-improvement method (S-\nPLMC), the result improves.\nDataset and Downstream TasksTo achieve our\ngoal of enhancing the code-related sequence gener-\nation task, we selected code summarization and\ncode generation as our experimental areas. To\nevaluate these tasks, we utilized the CodeXGLUE\nbenchmark (Lu et al., 2021), which comprises var-\nious datasets for various code understanding and\ncode generation tasks. Specifically, we utilized the\ncode summarization and code generation datasets\nfrom CodeXGLUE and disregarded the other ones.\nThe statistics for each dataset is reported in Ap-\npendix.\nBaseline Models We chose CodeBERT (Feng\net al., 2020), CodeT5 (Wang et al., 2021), and\nUniXCoder (Guo et al., 2022) as baseline models.\nEach model represents a distinct neural architec-\nture style. CodeBERT abides to the Bidirectional\nTransformer architecture, which is a well-known\nPLMCs, despite the fact that it may not produce the\nbest results for the tasks in CodeXGLUE. CodeT5\n12996\nModels Beam sizes Methods EM BLEU CodeBLEU\nCodeGPT 10 (Lu et al., 2021) 20.10 32.79 35.98PLBART 10 (Ahmad et al., 2021) 18.75 36.69 38.52\nCodeT5 (base) 1 Baseline 21.75 39.00 41.64Self-Improved22.40 39.75 42.145 Baseline 21.10 40.67 43.59Self-Improved22.40 41.61 44.0610 Baseline 22.10 39.59 43.78Self-Improved22.35 41.85 44.49\nUniXCoder (base) 1 Baseline 21.50 38.28 40.85Self-Improved22.10 38.56 40.963 Baseline 22.05 37.53 40.11Self-Improved22.30 37.88 40.425 Baseline 22.00 37.18 39.78Self-Improved22.30 37.49 40.05\nTable 2: Results on code generation evaluated with EM,\nBLEU, and CodeBLEU. The bolded numbers represent\nthe best scores (higher is better for all metrics) when\ncomparing between Baseline and Self-Improved for\neach model and each value of beam size.\nand UniXCoder are the two PLMCs that achieve\nstate-of-the-arts performance on the CodeXGLUE\nbenchmark. CodeT5 is an encoder-decoder archi-\ntecture that follows the Seq2Seq learning style by\nfollowing T5. UniXCoder, on the other hand, is\na unified language model. It can behave as an\nencoder-only, decoder-only, or encoder-decoder\nmodel by modifying the masked attention matri-\nces inside each transformer layer. Note that while\nCodeT5 and UniXCoder are proposed to address\nboth code summarization and code generation,\nCodeBERT only considers the first problem in there\npaper. So we only consider CodeBERT for code\nsummarization in our work.\nEvaluation Metric We follow CodeXGLUE\nbenchmark in employing evaluation metrics for\neach task. Smoothed BLEU-4 (Lin and Och, 2004)\nis used as the evaluation metric for code summa-\nrization. For code generation, smoothed BLEU-4,\nCodeBLEU (Ren et al., 2020), and exact match\n(EM) are employed. We aim to improve all of these\nmetrics after apply our self-improvement method\ninto the PLMCs.\nImplementation We carefully selected the\ncheckpoints for CodeBERT2, CodeT5 3, and UniX-\nCoder 4 based on the corresponding tasks. It is\nimportant to note that not all of these methods have\nreleased fine-tuned checkpoints. CodeT5 stands\nout as the only model to have released checkpoints\nfor code summarization and code generation tasks.\nConversely, CodeBERT and UniXCoder only offer\n2https://github.com/microsoft/CodeBERT/tree/\nmaster/CodeBERT\n3https://github.com/salesforce/CodeT5\n4https://github.com/microsoft/CodeBERT/tree/\nmaster/UniXcoder\ntraining scripts. When checkpoints were not avail-\nable, we employed the provided training scripts to\nfine-tune the pretrained models until we obtained\nresults comparable to those reported in the original\nresearch. Additionally, CodeT5 and UniXCoder\nmay have different checkpoint options with vary-\ning model sizes, such as small, base, and large.\nWe selected the base version for both CodeT5 and\nUniXCoder. The same validation set is employed\nself-improving as in fine-tuning. The best check-\npoint is selected according to BLEU score evalu-\nated on this set among the training epochs.\n5 Evaluation Results\nThe results of our code summarization task are\npresented in Table 1. The \"Beam sizes\" column\nindicates the beam size used in the beam search\nalgorithm, while the \"Methods\" column indicates\nwhether or not our self-improved algorithm was\nutilized. We also included other models as refer-\nences to compare the relative improvement of our\nmodel. On average, we observed an average of 0.76\nBLUE score increase in performance across all lan-\nguages. This improvement was consistent across\nvarious beam sizes (1, 5, 10), which confirms the\neffectiveness of our self-improved approach across\na wide range of PLMCs. When comparing our\nmodel to other strong baselines, we found that our\nmethod improved the performance of CodeBERT\nfor JavaScript from 15.78 to 16.39, surpassing the\nperformance of PolyglotCodeBERT (15.80). This\nhighlights the benefit of our self-improved method\nin improving weak models. The results of our code\ngeneration study are presented in Table 2, the per-\nformance increase by 0.81 BLUE scores on average.\nWhen using EM and CodeBLEU, the improvement\nalso increases consistently.\nWhile conducting our experiments, it is impor-\ntant to note that we did not selectively choose the\nmost favorable random seed to optimize the per-\nformance of each entry. Instead, we utilized the\ndefault seed provided in each model repository to\nensure fairness and consistency. Our code summa-\nrization experiments encompassed six different pro-\ngramming languages, and both code summarization\nand generation experiments were evaluated using\nthree distinct beam sizes. In total, we conducted 60\nruns to gather comprehensive results. The numbers\nreported consistently demonstrate the improvement\nachieved in each individual run, thereby affirming\nthe robustness of the proposed method.\n12997\n0.25\n 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75\nr1: gap in performance across beam sizes (unit: BLEU)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1r2: gain in performance (unit: BLEU)\ny=0.362*x+0.414\nlanguage\nruby\njavascript\ngo\npython\njava\nphp\nmodel\nUniXCoder\nCodeBERT\nCodeT5\nFigure 3: Scatter plot visualizing performance gap (in\nBLEU score) infered by different beam sizes (i.e 10 and\n1) of θfine−tuned vs. performance gained (in BLEU\nscore) by θimproved infered with beam size of 1\n6 Ablation Study\nImprovement Study In this section, we examine\nthe factors that influence the improvement achieved\nby θimproved as compared to θfine−tuned through\ncode summarization. We define r1 as the differ-\nence in performance measured by BLEU between\ninferencing with a beam size of 10 and inferencing\nwith a beam size of 1. Additionally, we definer2 as\nthe improvement in BLEU when inferencing with\nthe same beam size of 1 between θfine−tuned and\nθimproved. By evaluating these values across a vari-\nety of beam sizes and programming languages in\nthe code summarization dataset, we are able to visu-\nalize the results in Figure 3. Additionally, we have\ncalculated the Pearson Correlation score, which\nis 0.77, indicating a strong correlation between r1\nand r2. Our analysis demonstrates that a larger r1\nis correlated with a better r2, suggesting that our\nmethod is more likely to yield better overall perfor-\nmance when r1 is large. We believe this insight is\na crucial finding as it provides a simple indicator\nof the model’s fully trained capability.\nCodeBLEU as Similarity Function Our pri-\nmary findings in code generation are presented\nusing BLEU as the similarity function. However,\nfor a more comprehensive assessment of the cor-\nrectness of the generated code, we consider Code-\nBLEU, which incorporates the specific characteris-\ntics of source code. CodeBLEU, therefore, aligns\nbetter with the objective of measuring similarity\nin data augmentation compared to BLEU, which\nrelies on n-gram matching. This section exam-\nines the impact of using CodeBLEU as a similarity\nBeam sizes sim(.) EM BLEU CodeBLEU\n1 BLEU 22.10 38.56 40.96\nCodeBLEU 21.45 38.67 41.35\n3 BLEU 22.30 37.88 40.42\nCodeBLEU 21.80 38.37 41.35\n5 BLEU 22.30 37.49 40.05\nCodeBLEU 21.80 38.02 40.56\nTable 3: Comparing BLEU and CodeBLEU as similarity\nfunction in Data Augmentation Process on UniXCoder.\nThe bolded numbers represent the best scores (higher is\nbetter for all metrics).\nfunction (CodeBLEU-augmentation) compared to\nBLEU (BLEU-augmentation) in the context of data\naugmentation for code generation.\nWe present the results of our UniXCoder self-\nimproving model with both BLEU-augmentation\nand CodeBLEU-augmentation in Table 3. The re-\nsults indicate that CodeBLEU-augmentation en-\nhances both BLEU and CodeBLEU scores com-\npared to BLEU-augmentation. This suggests that\nusing CodeBLEU as a similarity function improves\nthe generated code at a local level, encompass-\ning aspects such as fluency, semantics, and syntax.\nHowever, it does have a negative impact on exact\nmatch (EM). As code problems may not have a\nunique solution, when EM is used as an evaluation\nmetric, it should allow for a more lenient assess-\nment. Consequently, we argue that a slight decrease\nin EM would have minimal impact on the actual\ncorrectness of the generated solution. Thus, we\npropose placing greater emphasis on CodeBLEU\nas an evaluation metric for code generation.\n7 Conclusion\nWe introduced a self-improvement technique as a fi-\nnal fine-tuning step to enhance model performance.\nOur experiments showed that this method, when\napplied to popular pre-trained code models (Code-\nBERT, CodeT5, and UniXCoder), significantly im-\nproves performance on code summarization and\ncode generation tasks. We also provided insights\non when this method is most effective in improving\nPLMCs. We intend to implement our technique\nin larger-scale models and other tasks, and believe\nit is an efficient way to optimize the capabilities\nof any code language model without the need for\nextensive architecture modifications or large-scale\ndataset assembly. We leave all of these investiga-\ntions for the future.\n12998\nLimitations\nWe discuss a few limitations of our work. One lim-\nitation of Self-Improved is its complexity in usage.\nThe data augmentation process involves generating\npredictions for the entire training dataset with a\nlarge beam size, resulting in a time complexity of\nO(nk), where nis the train dataset size and kis\nthe beam size. Additionally, the fine-tuning step to\nderive θimproved also adds a significant amount of\ncomputational complexity. This limitation is dis-\ncussed in Section 6 to weigh the performance ben-\nefits of our method against the computational sacri-\nfices. Another limitation is that Self-Improved has\nonly been applied to encoder-decoder models in\nthis work. However, it is also applicable to other\ntypes of auto-regressive models, including encoder-\nonly models, which are commonly used for tasks\nsuch as code completion (Radford et al., 2019;\nLu et al., 2021; Guo et al., 2022). A few mod-\nels can be named are GPT models (Radford et al.,\n2019; Brown et al., 2020), CodeX (Chen et al.,\n2021), CodeGen (Nijkamp et al., 2022), etc. Fur-\nther research into these applications is left for fu-\nture work.\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2021. Unified pre-training for pro-\ngram understanding and generation. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2655–2668,\nOnline. Association for Computational Linguistics.\nToufique Ahmed and Premkumar T. Devanbu. 2021.\nMultilingual training for software engineering.\nCoRR, abs/2112.02043.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\nShazeer. 2015. Scheduled sampling for sequence\nprediction with recurrent neural networks. Advances\nin neural information processing systems, 28.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nTing Chen, Simon Kornblith, Kevin Swersky, Moham-\nmad Norouzi, and Geoffrey E. Hinton. 2020. Big\nself-supervised models are strong semi-supervised\nlearners. CoRR, abs/2006.10029.\nYangruibo Ding, Luca Buratti, Saurabh Pujar, Alessan-\ndro Morari, Baishakhi Ray, and Saikat Chakraborty.\n2022. Towards learning (dis)-similarity of source\ncode from program contrasts. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n6300–6312, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547, Online. Association for Computational\nLinguistics.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. UniXcoder: Unified cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 7212–7225, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.\nDistilling the knowledge in a neural network. In\nNIPS Deep Learning and Representation Learning\nWorkshop.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnet challenge: Evaluating the state of semantic\ncode search. CoRR, abs/1909.09436.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2018. Mapping language to code\nin programmatic context. CoRR, abs/1808.09588.\n12999\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1317–1327, Austin,\nTexas. Association for Computational Linguistics.\nChin-Yew Lin and Franz Josef Och. 2004. ORANGE: a\nmethod for evaluating automatic evaluation metrics\nfor machine translation. In COLING 2004: Pro-\nceedings of the 20th International Conference on\nComputational Linguistics, pages 501–507, Geneva,\nSwitzerland. COLING.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin B. Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Li-\ndong Zhou, Linjun Shou, Long Zhou, Michele Tu-\nfano, Ming Gong, Ming Zhou, Nan Duan, Neel Sun-\ndaresan, Shao Kun Deng, Shengyu Fu, and Shujie\nLiu. 2021. Codexglue: A machine learning bench-\nmark dataset for code understanding and generation.\nCoRR, abs/2102.04664.\nSubhabrata Mukherjee, Ahmed Hassan Awadallah,\nand Jianfeng Gao. 2021. Xtremedistiltransformers:\nTask transfer for task-agnostic distillation. CoRR,\nabs/2106.04563.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2015. Sequence level train-\ning with recurrent neural networks. arXiv preprint\narXiv:1511.06732.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu,\nDuyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio\nBlanco, and Shuai Ma. 2020. Codebleu: a method\nfor automatic evaluation of code synthesis. CoRR,\nabs/2009.10297.\nChaojun Wang and Rico Sennrich. 2020. On exposure\nbias, hallucination and domain shift in neural ma-\nchine translation. arXiv preprint arXiv:2005.03642.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. CoRR, abs/2002.10957.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696–8708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nSam Wiseman and Alexander M Rush. 2016. Sequence-\nto-sequence learning as beam-search optimization.\narXiv preprint arXiv:1606.02960.\nA Data statistics\nWe include the statstics for the data used in our\nexperiments. We access directly to CodeXGLUE\npage5 for downloading data. CodeXGLUE gath-\ners datasets from multiple sources for each down-\nstream task. For code summarization, the Code-\nSearchNet (Husain et al., 2019) are used with the\nnumber of examples for each train/dev/test are re-\nported in Table 4. The dataset comprise the code-\ntext pairs in 6 programming languages. CON-\nCODE (Iyer et al., 2018) dataset is employed as\nthe benchmark for code generatiom with statistics\nreported in Table 5. It contains pairs of Java mem-\nber function and natural language description Java\nlanguage.\nProgramming Language Training Dev Test\nPython 251,820 13,914 14,918\nPHP 241,241 12,982, 14,014\nGo 167,288 7,325 8,122\nJava 164,923 5,183 10,955\nJavaScript 58,025 3,885 3,291\nRuby 24,927 1,400 1,261\nTable 4: Statictisc of data used in code summarization\nSplit #Examples\nTrain 100,000\nDev 2,000\nTest 2,000\nTable 5: Statictisc of data used in code generation\nB Hyperparameter selection\nIn the fine-tuning phase, we maintain the model\nhyperparameter values as specified in the training\nscript provided by the official repositories. How-\never, we make a modification by increasing the\nbatch size to fully utilize the memory capacity of\na NVIDIA A100 80GB. In the self-improvement\nphase, we further decrease the learning rate by a\nfactor of 10.\n5https://github.com/microsoft/CodeXGLUE\n13000\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n8\n□\u0013 A2. Did you discuss any potential risks of your work?\n8\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. 4\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n13001\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4, Appendix D\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n13002",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8608022332191467
    },
    {
      "name": "Automatic summarization",
      "score": 0.8517001867294312
    },
    {
      "name": "Code (set theory)",
      "score": 0.6531690359115601
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6198731660842896
    },
    {
      "name": "Language model",
      "score": 0.6077395677566528
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5157137513160706
    },
    {
      "name": "Machine learning",
      "score": 0.5146108865737915
    },
    {
      "name": "Code generation",
      "score": 0.5025675296783447
    },
    {
      "name": "Natural language processing",
      "score": 0.41779449582099915
    },
    {
      "name": "Programming language",
      "score": 0.17491909861564636
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97750245",
      "name": "Software (Spain)",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I109689652",
      "name": "FPT University",
      "country": "VN"
    },
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210127305",
      "name": "Fulbright University Vietnam",
      "country": "VN"
    },
    {
      "id": "https://openalex.org/I162577319",
      "name": "The University of Texas at Dallas",
      "country": "US"
    }
  ],
  "cited_by": 4
}