{
  "title": "Integrating Query Context and User Context in an Information Retrieval Model Based on Expanded Language Modeling",
  "url": "https://openalex.org/W33269649",
  "year": 2012,
  "authors": [
    {
      "id": "https://openalex.org/A5002540201",
      "name": "Rachid Aknouche",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5056356349",
      "name": "Ounas Asfari",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5044238058",
      "name": "Fadila Bentayeb",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5080801111",
      "name": "Omar Boussaïd",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1997211290",
    "https://openalex.org/W2157036429",
    "https://openalex.org/W2144742206",
    "https://openalex.org/W2135175918",
    "https://openalex.org/W1580603475",
    "https://openalex.org/W4234816175",
    "https://openalex.org/W4250633719",
    "https://openalex.org/W2067438047",
    "https://openalex.org/W4233872535",
    "https://openalex.org/W2002659088",
    "https://openalex.org/W2061503185",
    "https://openalex.org/W2126808427",
    "https://openalex.org/W4243333943",
    "https://openalex.org/W1493526108",
    "https://openalex.org/W2161563551",
    "https://openalex.org/W2150608210",
    "https://openalex.org/W2092604030",
    "https://openalex.org/W2105981469",
    "https://openalex.org/W2095976990",
    "https://openalex.org/W4250909839",
    "https://openalex.org/W2068905009",
    "https://openalex.org/W1964348731",
    "https://openalex.org/W64144418",
    "https://openalex.org/W2405103513",
    "https://openalex.org/W1544000713",
    "https://openalex.org/W2169150382",
    "https://openalex.org/W2112175905",
    "https://openalex.org/W17896917",
    "https://openalex.org/W2623273758",
    "https://openalex.org/W1965657003",
    "https://openalex.org/W2102315706",
    "https://openalex.org/W3013757419",
    "https://openalex.org/W4285719527"
  ],
  "abstract": null,
  "full_text": "Integrating Query Context and User Context\nin an Information Retrieval Model\nBased on Expanded Language Modeling\nRachid Aknouche, Ounas Asfari, Fadila Bentayeb, and Omar Boussaid\nERIC Laboratory (Equipe de Recherche en Ingnierie des Connaissances)\n5 Avenue Pierre Mends France, 69676 Bron Cedex, France\n{Rachid.Aknouche,Ounas.Asfari,Fadila.Bentayeb,\nOmar.Boussaid}@univ-lyon2.fr\nhttp://eric.univ-lyon2.fr/\nAbstract. Access to relevant information adapted to the needs and the\ncontextof theuserisa real challenge. Theusercontext can beassimilated\nto all factors that can describe his intentions and perceptions of his sur-\nroundings. It is diﬃcult to ﬁnd a contextual information retrieval system\nthat takes into account all contextual factors. In this paper, both types of\ncontext user context and query context are integrated in an Information\nRetrieval (IR) model based on language modeling. Here, the query con-\ntext include the integration of linguistic and semantic knowledge about\nthe user query in order to explore the most exact understanding of user’s\ninformation needs. In addition, we consider one of the important factors\nof the user context, the user’s domain of interest or the interesting topic.\nA thematic algorithm is proposed to describe the user context. We as-\nsume that each topic can be characterized by a set of documents from\nthe experimented corpus. The documents of each topic are used to build\na statistical language model, which is then integrated to expand the orig-\ninal query model and to re-rank the retrieved documents. Our experi-\nments on the 20\nNewsgroup corpus show that the proposed contextual\napproach improves signiﬁcantly the retrieval eﬀectiveness compared to\nthe basic approach, which does not consider contextual factors.\n1 Introduction\nMost existing Information retrieval systems depend, in their retrieval decision,\nonly on queries and documents collections; information about actual users and\nsearch context is largely ignored, and consequently great numbers of irrelevant\nresults occur. Towards the optimal retrieval system, the system should exploit\nas much additional contextual information as possible to improve the retrieval\naccuracy, whenever this is available.\nContext is a broad notion in many ways. For instance, [11] deﬁne context as\nany information that can be used to characterize the situation of an entity. An\nentity is a person, place, or object that isconsidered relevant to the interaction\nbetween a user and an application, including the user and applications them-\nselves. The eﬀective use of contextual information in computing applications still\nG. Quirchmayr et al. (Eds.): CD-ARES 2012, LNCS 7465, pp. 244–258, 2012.\nc⃝ IFIP International Federation for Information Processing 2012\nIntegrating Query Context and User Context in an IR Model 245\nremains an open and challenging problem. Several researchers have tried over\nthe years to apply the context notion in information retrieval area; this will lead\nto the so-called contextual information retrieval systems which combine a set of\ntechnologies and knowledge on the query and the user, in order to deliver the\nmost appropriate answers according to the user’s information needs.\nAs information needs are generally expressed via queries and the query is\nalways formulated in a search context, contextual factors (such as the user’s\ndomain of interest, preferences, knowledge level, user task, etc.) have a strong\ninﬂuence on relevance judgments [16]. But it is diﬃcult to ﬁnd a contextual\ninformation retrieval system that takes into account all the available contextual\nfactors at the same time. Thus the context can be deﬁned as the combination\nof some contextual factors which may be more or less relevant according to the\nactual performed research. Indeed, in this paper, we try to consider two types\nof context, user context and query context. We think that these two contextual\nfactors can improve the information retrieval model.\nIn this paper, the user context is deﬁned by the user’s domain of interest or\nthe interesting topic. We propose a thematic algorithm to describe the prede-\nﬁned user’s topics which are characterized by a set of documents. Considering\nthe user’s interested topics allows providing more relevant results. The second\nconsidered contextual factor is the query context, which includes a linguistic\nand a semantic knowledge about the user query in order to explore the most ex-\nact understanding of user’s information needs. Thus, we extend the user query\nby related terms automatically by using the linguistic and semantic knowledge.\nAlso, we propose a framework based on language modeling approach in order to\nintegrate the two contextual factors.\nFor instance, if a user submits the query ”apple” into a Web search engine,\nknowing that user queries are generally shorts and contain words with several\nmeanings, there are diﬀerent topics in the top 20 documents selected by the\nsearch engine. Some users may be interested in documents dealing with ”apple”\nas ”fruit”, while other users may want documents related to Apple computers. In\norder to disambiguate this query, we can assign a set of topics with this query.\nFor example, we can assign the topics ”cooking”, ”fruit”o r” computer” with\nthe query ”apple”. This is the user’s domain of interest. In addition, to extend\nthe query ”apple” with the so-called query context, we can add concepts to this\nquery like: ”Computers”, ”Systems”, ”Macintosh”, etc.\nThe language models in information retrieval (IR) are used to compute the\nprobability of generating query q given a document D (i.e. compute:P(q|D));\nand the documents in the collectionC are ranked in descending order of this\nprobability. Several methods have been applied to compute this probability as\n[19]. In most approaches, the computation is conceptually decomposed into two\ndistinct steps: Estimating the document model and computing the query likeli-\nhood using the estimated document model, as in [21].\nIn this paper, we propose an approach to build a statistical language model\nthat extends the classic language modeling approach for information retrieval\nin order to integrate the two contextual factors. The extended model has been\n246 R. Aknouche et al.\ntestedbasedononeoftheco mmonIRtestcollections;20\n Newsgroupcorpus.The\nresults show that our contextual approach improves signiﬁcantly the retrieval\neﬀectiveness comparedto the basicapproach,which does notconsider contextual\nfactors. The rest of this paper is organized as follows: Section 2 introduces the\nstate of the art and some related works; Section 3 introduces our contextual\ninformation retrieval model based on language modeling; Section 4 shows the\nexperimental study and the evaluation of our approach. Finally, Section 5 gives\nthe conclusion and future work.\n2 Related Works\n2.1 Context in Information Retrieval\nSeveral contextual factors can be considered in Information Retrieval (IR) area\nin order to improve the information retrieval systems. In this section we review\nsome of studies in IR concerning the user context and query context, as long as\nwe take them into consideration to extend the language modeling approach for\nIR[3].\nUser Context. The user context can be assimilated to all factors that can\ndescribe his/her intentions and perceptions of his/her surroundings [22]. These\nfactors may cover various aspects: physical, social, personal, professional, tech-\nnical, task etc. Figure 1 shows these factors and examples for each one [18].\nHowever, the problems to be addressed include how to represent the context,\nhow to determine it at runtime, and how to use it to inﬂuence the activation of\nuser preferences. It is very diﬃcult to modeling all the contextual factors in one\nsystem, so the researchers often take into account some factors, as in [2], they\ndeﬁned the user context as the user’s current task together with his/her proﬁle.\nIn the contextual information retrieval, user context has a wide meaning based\non the user behavior; we can mention some of them in the following:\n– Visited Web pages [26]: Here, the user context is deﬁned as the information\nextracted by using the full browsing history of the user.\n– Recently accessed documents [5]: In this case, the user context is deﬁned\nas words which indicate a shift in context; these words are identiﬁed by\ninformation about the sequence of accessed documents. This is carried out\nby monitoring a user’s document access, generating a representation of the\nuser’s task context, indexing the consulted resources, andby presenting rec-\nommendations for other resources that were consulted in similar prior con-\ntexts.\n– Past queries and click-through data [25]: Several context-sensitive retrieval\nalgorithms are proposed based on statistical language models to combine the\npreceding queries and clicked document summaries with the current query\nfor better ranking of documents.\n– Recent selected items or purchases onproactive information systems [6].\nIntegrating Query Context and User Context in an IR Model 247\n– Information that is previously processed or accessed by the user via various\nforms: email, web page, desktop document, etc. Stuﬀ I’ve Seen SIS, [12].\n– Implicit feedback: Implicit feedback techniques often rely on monitoring the\nuser interaction with the retrieval system, and extract the apparently most\nrepresentative information related to what the user is aiming at [17].\nFig. 1. Context Model\nQuery Context. The notion of query context has been widely mentioned in\nmany studies of information retrieval like [4][9]. The objective is to use a va-\nriety of knowledge involving query to explore the most exact understanding of\nuser’s information needs. A query context will minimize the distance between\nthe information need,I, and the queryq,[ 1 ] .D i s t a n c e(I to q) is minimized by\nminimizing:\n– The lack of precision in the language used in the query terms. Lexicons\nwhich comprise the general vocabulary of a language can minimize this lack\nof precision in the language by identifying terms with minimal ambiguity.\n– The use of the wrong concepts in the query to represent the information\nneeds. Prior research suggests Ontology’s for doing that.\n– The lack of preferences in the query to constrain the concepts requested.\nThis lack can be minimized by using user proﬁles.\nThe query context, in other studies, is deﬁned as the elements that surround the\nquery, such as:\n– Text surrounding a query, Text highlighted by a user [13].\n– Surrounding elements in an XML retrieval application [15][24].\n– Broadcast news text for query-less systems [14].\n[10]exploitthequerycontextforpredictingtheuserintentasbeinginformational\nrelated to the content retrieval, navigational related to the web site retrieval or\ntransactional related to the online service retrieval.They construct the query\ncontext by associating it with ontologyconcepts from the ODP (Open Directory\nProject) taxonomy.\n248 R. Aknouche et al.\n2.2 Statistical Language Models\nA statistical language model is a probability distribution over word sequences,\nusing language models for information retrieval has been studied extensively,\nlike in [19][21][27], because of its solid theoretical background as well as its good\nempirical performance. The main idea of language models in IR is to order each\ndocument D in the collectionC according to their ability to generate the query\nq. Thus, it is the estimation of the generation probabilityP(q|D); Probability\nof a queryq given a documentD. Several diﬀerent methods have been applied\nto compute this conditional probability, such as the works of [21][8].\n2.3 Discussion\nUser query is an element that speciﬁes an information need, but the majori-\nties of these queries are short and ambiguous, and often fail to represent the\ninformation need. Many relevant termscan be absent from queries and terms\nincluded may be ambiguous, thus, queries must be processed to address more\nof the user’s intended requirements [2]. Typical solution includes expanding the\ninitial user query by adding relevant terms. In this study we will expand the\nquery representation by the query context which is deﬁned above.\nAs we mentioned previously, it is diﬃcult to consider all the available con-\ntextual factors. Thus, in this study, our deﬁnition of the user context is the\nuser’s interesting topic. Consequently, when we talk about the user context we\ntalk about the user’s interesting topics and taking into consideration the query\ncontext.\nThe language models for information retrieval have some limitations to cap-\nture the underlying semantics in a document due to their inability to handle\nthe long distance dependencies. Also queries are typically too short to provide\nenough contexts to precisely translate into a diﬀerent language. Thus, many\nirrelevant documents will be returned by using the standard language model\napproach for IR without integrating contextual factors.\nIn this paper, we will integrate the above two types of context within one\nframework based on language modeling. Each component contextual factor will\ndetermines a diﬀerent ranking score,and the ﬁnal document ranking combines\nall of them. This will be described in Section 3.\n3 Contextual Information Retrieval Model Based on\nLanguage Modeling\nIn this section, we present our approachto construct a statistical language model\ngiven user’s interested topics, user context, and considering the query expansion\nby using the linguistic and semantic processing.\nIntegrating Query Context and User Context in an IR Model 249\n3.1 Language Models for IR\nLet us consider a queryq = t1t2...tn, the generation probability is estimated as\nfollows:\nP(q | D)=\n∏\nt∈q\nP(t | θD)c(t;q)\n= P(t1 | θD)P(t2 | θD)...P(tn | θD)( 1 )\nwhere: c(t;q) Frequency of termt in query q. θD is a language model created\nfor a documentD. P(t | θD): The probability of term t in the document model.\nIn order to avoid zero probability by assigning a low probability to query terms\nti which are not observed in the documents of corpus, smoothing on document\nmodel is needed. The smoothing in IR is generally done by combining documents\nwith the corpus [27], thus:\nP(ti | θD)= λP(ti | θD)+(1 −λ)P(ti | θC )( 2 )\nwhere: λ is an interpolation parameter andθC the collection model. In the lan-\nguage modeling framework the similarity between a documentD and a queryq\n(a typical score function) can be also deﬁned by measuring theKullback-Leibler\n(KL-divergence) [19] as follows:\nScore(q,D)= −KL(θq ∥ θD)=\n∑\nt∈V\nP(t | θq)log P(t | θD)\nP(t | θq)\n=\n∑\nt∈V\nP(t | θq)log P(t | θD)−\n∑\nt∈V\nP(t | θq)log P(t | θq)\n∝\n∑\nt∈V\nP(t | θq)log P(t | θD)( 3 )\nWhere: θq is a language model for the queryq, generally estimated by relative\nfrequency of keywords in the query, andV the vocabulary. In the basic language\nmodeling approaches, the query model is estimated by Maximum Likelihood\nEstimation (MLE) without any smoothing [8].\nP(t | θq): The probability of term t in the query model.\nNote that the last simpliﬁcation is done because∑ P(t | θq)log P(t | θD)\ndepends only on the query, and does not aﬀect the documents ranking.\n3.2 General IR Model\nThe classic information retrieval systems (Non-context model) generate query\ndirectly based on similarity function or matching between the query and the\ndocuments, according to a few terms in the query. In fact, query is always formu-\nlated in a search context; contextual factors have a strong inﬂuence on relevance\njudgments. To improve retrieval eﬀectiveness, it is important to create a more\ncomplete query model that represents better the information need. In particular,\nall the related and presumed terms should be included in the query model. In\n250 R. Aknouche et al.\nthese cases, we construct the initial query model containing only the original\nterms, and a new contextual model containing the added terms. We generalize\nthis approach and integrate more models for the query.\nLet us use θ0\nq to denote the original query model, θs\nq to denote the query\ncontext model andθu\nq to denote the user context model.θ0\nq can be created by\nMLE (Maximum Likelihood Estimation ), as in [7]. We will describe the details\nto constructθs\nq and θu\nq in the following sections.\nGiventhesemodels,wecreatethefollowingﬁnalquerymodelbyinterpolation:\nP(t | θq)=\n∑\ni∈X\naiP(t | θi\nq)( 4 )\nwhere: X = {0,u,s } is the set of all component models.\nai (With ∑\ni∈X ai = 1) are their mixture weights.\nThus formula (3) becomes:\nScore(q,D)=\n∑\nt∈V\n∑\ni∈X\naiP(t | θi\nq)log P(t | θD)=\n∑\ni∈X\naiScorei(q,D)( 5 )\nWhere the score according to each component model is:\nScorei(q,D)=\n∑\nt∈V\nP(t | θi\nq)log P(t | θD)( 6 )\nLike that, the query model is enhancedby contextual factors. Now we have to\nconstruct both query context model and user context model and combine all\nmodels. We will describe that in the following sections.\n3.3 Constructing Query Context Model\nWe will use both a linguistic knowledge and a semantic knowledge to parse the\nuser query. Because linguistic knowledge does not capture the semantic rela-\ntionships between terms and semantic knowledge does not represent linguistic\nrelationships of the terms [1]. We useWordNet as the base of a linguistic knowl-\nedge. For the semantic knowledge we depend on ODP1(Open Directory Project)\nTaxonomy, it is one type of ontology.\nThe integration of linguistic and semantic knowledge about the user query\ninto one repository will produce the query context which can help to understand\nthe user query more accurately.\nThus the initial query q = t1t2...tn is parsed using WordNet in order to\nidentify the synonymous for each query term{tw1,tw2,...,t wk }.\nThe query and its synonymsqw are queried against the ODP taxonomy in\norder to extract a set of concepts{c1c2...cn } (with m ≥ n) that reﬂect the\nsemantic knowledge of the user query. The concepts of the terms setqw and their\nsub-concepts produce the query-contextCq = {c1c2...cn } . Thus the elements of\n1 ODP: Open Directory Project:www.dmoz.org\nIntegrating Query Context and User Context in an IR Model 251\nCq are the concepts extracted from the ODP taxonomy by querying the initial\nquery and its synonyms against it. For each termt, we select the concepts of\nonly ﬁrst ﬁve categories issued from ODP taxonomy.\nAmong the concepts of query contextCq, We consider only the shared con-\ncepts between at least two query terms, that means, a concept Ci ∈ Cq the\ncontext of the queryq if one of the following holds:\n– Ci and t ∈ q are the same,i.e. a direct relation.\n– Ci is a common concept between the concepts of at least two query terms\nor their synonymous.\nFor instance, if a user query is ”Java Language”, the query contextCq,i nt h i s\ncase, will be:< computers, programming, languages, Java, JavaScript >.T h u s ,\nthe corresponding query context modelθs\nq is then deﬁned as follows:\nP(t | θs\nq )=\n∑\nCi∈Cq\np(t | Ci)p(Ci | θ0\nq )( 7 )\nAccordingly, the scoreaccording to the query context model is deﬁned as follows:\nScores(q,D)=\n∑\nt∈V\nP(t | θs\nq )log P(t | θD)( 8 )\n3.4 Constructing User Context Model\nAs we previously mentioned, in this paper, the user context is deﬁned as the\nuser’s domain of interest or the interesting topic. We will depend on predeﬁned\ntopics, which are derived from the used corpus. To deﬁne these topics, we can\nuse our thematic algorithm, which will be discussed in the following (see Fig.2).\nWe suppose that the user can select his own topic from these predeﬁned topics\nby assigning one topic to his query.\nWe exploit a set of documents already classiﬁed in each topic. The documents\nof each topic can be identiﬁed using same thematic algorithm. Thus a language\nmodel, for each topic, can be extracted from the documents of this topic. To\nextract only the speciﬁc part of the topic and avoid the general terms in the\ntopic language model, we apply our thematic algorithm as follows (see Fig.2).\nA thematic unit refers to a fragment of a document D referring to one topic.\nThe steps are based on a thesaurus that includes all lexical indicators which\nare considered important for this segmentation process. They are created, in\nthis study, manually but to extend the thematic units coverage in a document,\nwe added synonyms and lexical forms that may have these units in the corpus.\nThe obtained text fragments are then grouped by the thematic unity. Seeking\ninformation related to the user context (term context) will be done from these\nfragments.\nNext maximum likelihood estimation is used on the documents of each topic\nto extract the topic language model.\nWe suppose that we have the following predeﬁned topics:TC\n1,TC 2,...,TC j ,\nand the user assign the topicTCj to his query. We can extract the user context\n252 R. Aknouche et al.\nFig. 2. A thematic algorithm\nlanguage model which is extracted from the documents of the topicTCj ,a s\nfollows (considering the smoothing):\nθu =a r g m a x\n∏\nD∈TC j\n∏\nt∈D\n[\nμP\n(\nt | θTC j\n)\n+(1 −μ)P (t | θC )\n]c(t;D)\n(9)\nWhere: c(t;D) is the occurrence of the termt in documentD.μ is a smoothing\nparameter. μ is ﬁxed to 0.5 in the experimentation, because, in our dataset, we\nhave considered the middle point as a smoothing parameter.TCj is the set of\ndocuments in the topicj. In the same method we can compute the set of user\ncontext models for all predeﬁned topics. When the user assigns one topic to his\nquery q manually, the related user context modelθs\nq has to be assigned to this\nquery q, and the score depending on this user context model (represented by the\nrelated topic) will be:\nScoreu(q,D)=\n∑\nt∈V\nP(t | θu\nq )log P(t | θD) (10)\n4 Experiments\nTo validate our approach, we will present an experimental study which is done\nby using a test collection, 20\n Newsgroups2 corpus. The objective of this ex-\nperimental study is to compare the results provided by using an information\nretrieval model on the dataset without considering the contextual factors with\nthose provided by a general information retrieval model considering the contex-\ntual factors.\nOur approach (including steps) is implemented in Java by using Eclipse envi-\nronment. The prototype use JWNL3 (Java WordNet Library), which is an API\nthat allows the access to the thesaurus WordNet to ﬁnd synonyms of query\n2 http://people.csail.mit.edu/jrennie/20Newsgroups/\n3 http://jwordnet.sourceforge.net/handbook.html\nIntegrating Query Context and User Context in an IR Model 253\nterms. For the semantic knowledge we depend on ODP Taxonomy which is free\nand open, everybody can contribute or re-use the dataset, which is available in\nRDF (structure and content are available separately), i.e., it can be re-used in\nother directory services. Also we used the Oracle RDBMS database to host: (1)\nthe thesaurus for terms synonyms, (2) the topics which are generated during the\nprocess of identiﬁcation and extraction,and (3) the relevance scores of returned\ndocuments. In order to facilitate the evaluation, we developed an interface that\nhelps users to enter their queries, to compute the evaluation criteria, and then\nto display the results which are ranked according to the degrees of relevance.\n4.1 Newsgroup Data Sets\nThe 20\nNewsgroup data set is a common benchmark collection of approximately\n20,000 newsgroupdocuments, partitioned nearly evenly across 20 diﬀerent news-\ngroups. This dataset was introduced by [20]. It has become a popular data set\nfor experiments in text applications of machine learning techniques, such as\ntext classiﬁcation. Over a period of time, 1000 articles were taken from each of\nthe newsgroups, which make this collection. The 20 topics are organized into\nbroader categories: computers, recreation, religion, science, for-sale and pol-\nitics. Some of the newsgroups are considered to have similar topics, such as\nthe rec.sport.baseball and rec.sport.hockey which both contain messages about\nsports,while othersarehighlyunrelated(e.g misc.forsale/ soc.religion.christian)\nnewsgroups. Table 1 shows the list of the 20 newsgroups, partitioned (more or\nless) according to subject matter. This dataset is also used by [23].\nMoreover,wepreprocessedthedatabyremovingstopwordsandalldocuments\nare stemmed using thePorter algorithm . The document-terms matrix is based\non language models and each document is represented as a vector of occurrence\nnumbers of the terms within it. The results of this preprocessing phase for the\ndataset before and after the classiﬁcation topics are presented in Table2.\nThe query execution phase, in our approach, returns a ranked list of docu-\nments that match the query. The experimentation on the 20\nNewsgroups collec-\ntion doesn’t provide the ability to compute the precision and recall metrics. In\nthis way our experiments were conducted with theLemur Toolkit 4 , which is a\nstandard platform to conduct experiments in information retrieval. The toolkit\nhas been used to carry out experiments onseveral diﬀerent aspects of language\nmodeling for ad-hoc retrieval. For example, it has been used to compare smooth-\ning strategies for document models, and query expansion methods to estimate\nquery models on standard TREC5 collections. We used the language models for\nall our retrieval tasks. All the other parameters were set at their default val-\nues. We remove allUseNet headers from the Newsgroup articles and we used\n20 queries, which are listed in Table 3, to retrieve results from this documents\ndataset. The queries vary from 1 term to 7 terms.\n4 http://www.lemurproject.com\n5 http://trec.nist.gov/\n254 R. Aknouche et al.\nTable 1. A list of the 20 Topics\n20\n Newsgroups dataset\ncomp.graphics\n rec.autos\n talk.politics.misc\n soc.religion.christia\ncomp.os.ms-windows.misc\n rec.motorcycles\n talk.politics.guns\n sci.crypt\ncomp.sys.ibm.pc.hardware\n rec.sport.baseball\n talk.politics.mideast\n sci.electronics\ncomp.sys.mac.hardware\n rec.sport.hockey\n talk.religion.misc\n sci.med\ncomp.windows.x\n misc.forsale\n alt.atheism\n sci.space\nTable 2. Pre-processing phase applied on the dataset\nCorpus\n docs\n stems\n Corpus\n docs\n stems\n20 news group\n 20017\n 192375\n ec.sport.baseball\n 1001\n 14000\nalt.atheism\n 1001\n 15618\n rec.sport.hockey\n 1001\n 15610\nccomp.graphics\n 1001\n 17731\n sci.crypt\n 1001\n 17436\ncomp.os.ms-windows.misc\n 1001\n 54511\n sci.electronics\n 1001\n 15622\ncomp.sys.ibm.pc.hardware\n 1001\n 16575\n sci.med\n 1001\n 19963\ncomp.sys.mac.hardware\n 1001\n 15011\n sci.space\n 1001\n 18432\ncomp.windows.x\n 1001\n 24915\n soc.religion.christian\n 1001\n 13915\nmisc.forsale\n 1001\n 17518\n talk.politics.guns\n 1001\n 20258\nrec.autos\n 1001\n 15415\n talk.politics.mideast\n 1001\n 20546\nrec.motorcycles\n 1001\n 15108\n talk.politics.misc\n 1001\n 17782\n4.2 Baseline\nClassic IR Baseline. As a baseline for comparison, for each dataset, we cre-\nated an index of all the documents usingLemur’s indexer. Figure 3 shows the\nindexation interface. Also we usedLemur’s retrieval engine to return a list of\nrelevant documents using the queries which aredescribed above.This is the stan-\ndard information retrieval setting. For instance, Figure 4 shows the document’s\nranking for the query ”athletics play ”.\nFig. 3. Lemur’s indexer\nIntegrating Query Context and User Context in an IR Model 255\nFig. 4. Lemur’s retrieval\nContextual Information Retrieval Baseline. We expanded the query with\nboth user context and query context. In the 20\nNewsgroup dataset, the topics\nnamesareconsideredasausercontext.The documents areindexed with Lemur’s\nindexer.Lemur’s retrieval engine wasused to performinformationretrievalusing\nthe expanded query.\nTable 3. The experimented queries list\nN\n Query\n N\n Query\n1\n Sport\n 11\n Logitech Bus Mouse adapter\n2\n athletics play\n 12\n Division Champions\n3\n Stanley Cup Champion: Vancouver Canucks\n13\n baseball fan\n4\n ordinary ISA card\n 14\n HD drive\n5\n East Timor\n 15\n System requirement\n6\n High speed analog-digital pc-board\n 16\n memory controller pc\n7\n Chicago Blackhawks\n 17\n league teams\n8\n Kevin Dineen play for the Miami Colons\n 18\n macintosh apple hardware\n9\n National league pitchers\n 19\n science cryptography\n10\n good defensive catcher\n 20\n society religion\n4.3 Results\nTo evaluate the performance of our approach we use the TREC evaluation soft-\nware to calculate the metrics. We useireval.pl Perl script which comes with the\nLemur toolkit distribution for interpreting the results of the programtrec\n eval.\nFigure 5 illustrates the precision/recallcurves ofthe IR classic and of our contex-\ntual retrieval model on the 20\nNewsgroup dataset. The results of our approach,\n256 R. Aknouche et al.\npresented by the curves, show signiﬁcantly improvement in measure of Preci-\nsion/Recall compared tothe classical model.\nThe improvement is precisely in the accuracy rate. It is obtained by using\nthe contextual model which expands the original query model and re-rank the\nreturned documents. Search engines involve query expansion to increase the\nquality of user search results. It is assumed that users do not always formulate\ntheir queries using the best terms. Using the general model which expands the\nuser query with the contextual factors will increase the recall at the expense of\nthe precision. This explains our motivation to consider the contextual factors\nand topic units in our approach.\nFig. 5. Precision/recall curves for 20\nNewsgroup corpus\n5C o n c l u s i o n\nIn order to improve the information retrieval, we considered, in this paper, two\ntypes of context, user context and query context. We proposed an approach\nbased on query expansion to provide a more eﬃcient retrieval process. The ini-\ntial query, the user context and the query context are integrated to create a new\nquery. A thematic algorithm is proposed to describe the user context and both\nlinguistic knowledge (WordNet) and semantic knowledge (ODP taxonomy) are\nused to represent the query context. The two contextual factors are integrated in\none framework based on language modeling. We proposed a new language mod-\neling approach for information retrieval that extends the existing language mod-\neling approach to build a language model in which various terms relationships\ncan be integrated. The integrated model has been tested on the 20\nNewsgroup\ncorpus. The results show that our contextual approach improves the retrieval ef-\nfectiveness compared to the basic approach, which does not consider contextual\nfactors. In the future, we plan to consider more contextual factors. The future\nobjective is to combine our approach with the On-line Analytical Processing\n(OLAP) systems.\nIntegrating Query Context and User Context in an IR Model 257\nReferences\n1. Asfari, O., Doan, B.-L., Bourda, Y., Sansonnet, J.-P.: A Context-Based Model\nfor Web Query Reformulation. In: Proceedings of the International Conference\non Knowledge Discovery and Information Retrieval, KDIR 2010, Spain, Valencia\n(2010)\n2. Asfari, O., Doan, B.-L., Bourda, Y., Sansonnet, J.-P.: Personalized Access to Con-\ntextual Information by using an Assistant for Query Reformulation. IARIA Jour-\nnal, IntSys 2011 4(34) (2011)\n3. Asfari, O.: Personnalisation et Adaptation de L’acc`es `a L’information Contextuelle\nen utilisant un Assistant Intelligent. PhD thesis, Universit´e Paris Sud - Paris XI,\ntel-00650115 - version 1 (September 19, 2011)\n4. Allan, J.: Challenges in information retrieval and language modeling. In: Work-\nshop Held at the Center for Intelligent Information Retrieval, University of Mas-\nsachusetts, Amherst, SIGIR Forum vol. 37(1), pp. 31–47 (2003)\n5. Bauer, T., Leake, D.: Real time user context modeling for information retrieval\nagents. In: CIKM 2001: Proceedings of the Tenth International Conference on\nInformation and Knowledge Management, pp. 568–570. ACM, Atlante (2001)\n6. Billsus, D., Hilbert, D., Maynes-Aminzade, D.: Improving proactive information\nsystems. In: IUI 2005: Proceedings of the 10th International Conference on Intel-\nligent User Interfaces, pp. 159–166. ACM, San Diego (2005)\n7. Bai, J., Nie, J., Bouchard, H., Cao, H.: Using Query Contexts in Information\nRetrieval. In: SIGIR 2007, Amsterdam, Netherlands, July 23-27 (2007)\n8. Bouchard, H., Nie, J.: Mod`eles de langue appliqu´es `a la recherche d’information\ncontextuelle. In: Proceedings of CORIA 2006 Conf. en Recherche d’Information et\nApplications, Lyon, pp. 213–224 (2006)\n9. Conesa, J., Storey, V.C., Sugumaran, V.: Using Semantic Knowledge to Improve\nWeb QueryProcessing. In:Kop, C., Fliedl, G., Mayr, H.C., M´etais, E. (eds.) NLDB\n2006. LNCS, vol. 3999, pp. 106–117. Springer, Heidelberg (2006)\n10. Daoud, M., Tamine, L., Duy, D., Boughanem, M.: Contextual Query Classiﬁcation\nFor Personalizing Informational Search. In: Web Information Systems Engineering,\nKerkennah Island, Sfax, Tunisia. ACM (Juin 2009)\n11. Dey, A.K., Abowd, G.D.: Toward a better understanding of context and context-\nawareness. In: Workshop on the What, Who, Where, When, and How of Context-\nAwareness (1999)\n12. Dumais, S., Cutrell, E., Cadiz, J.J., Jancke, G., Sarin, R., Robbins, D.C.:\n(Stuﬀ I’ve Seen): A system for personal information retrieval and re-use. In: Pro-\nceedings of 26th ACM SIGIR 2003, Toronto, pp. 72–79 (July 2003)\n13. Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G.,\nRuppin, E.: Placing search in context: the concept revisited. In: WWW, Hong\nKong (2001)\n14. Henzinger, M., Chang, B.-W., Milch, B., Brin, S.: Query-free news search. In: The\n12th International Conference on World Wide Web, Hungary (2003)\n15. Hlaoua, L., Boughanem, M.: Towards Contextual and Structural Relevance Feed-\nback in XML Retrieval. In: Beigbeder, M., Yee, W.G. (eds.) Workshop on Open\nSource Web Information Retrieval, Compi`egne, pp. 35–38 (2005)\n16. Ingwersen, P., J¨averlin, K.: Information retrieval in context. IRiX, ACM SIGIR\nForum 39(2), 31–39 (2005)\n17. Kelly, D., Teevan, J.: Implicit Feedback for Inferring User Preference: A Bibliog-\nraphy. SIGIR Forum 32(2), 18–28 (2003)\n258 R. Aknouche et al.\n18. Kofod-Petersen, A., Cassens, J.: Using Activity Theory to Model Context Aware-\nness. In: American Association for Artiﬁcial Intelligence, Berlin (2006)\n19. Laﬀerty, J., Zhai, C.: Language models, query models, and risk minimization for\ninformation retrieval. In: SIGIR 2001, The 24th ACM International Conference\non Research and Development in Information Retrieval, New York, pp. 111–119\n(2001)\n20. Lang, K.: NewsWeeder: learning to ﬁlter net news. In: The 12th International\nConference on Machine Learning, San Mateo, USA, pp. 331–339 (1995)\n21. Liu, X., Croft, W.B.: Statistical language modeling for information retrieval. In:\nCronin, B. (ed.) Annual Review of Information Science and Technology, ch. 1, vol.\n39 (2006)\n22. Mylonas, P., Vallet, D., Castells, P., Fernandez, M., Avrithis, Y.: Personalized\ninformation retrieval based on context and ontological knowledge. Knowledge En-\ngineering Review 23, 73–100 (2008)\n23. Ratinov, L., Roth, D., Srikumar, V.: Conceptual search and text categorization.\nTechnical Report UIUCDCS-R-2008-2932, UIUC, CS Dept. (2008)\n24. Sauvagnat, K., Boughanem, M., Chrisment, C.: Answering content and structure-\nbased queries on XML documents using relevance propagation. In: Information\nSystems, Num´ero Sp´ecial Special Issue, SPIRE 2004, vol. 31, pp. 621–635. Elsevier\n(2006)\n25. Shen, X., Tan, B., Zhai, C.: Context-sensitive information retrieval using implicit\nfeedback. In: SIGIR 2005: Proceedings of the 28th Annual International ACM\nSIGIR Conference on Research and Development in Information Retrieval, pp.\n43–50. ACM, Brazil (2005)\n26. Sugiyama, K., Hatano, K., Yoshikawa, M.: Adaptive Web Search Based on User\nProﬁle Constructed without Any Eﬀort from Users. In: WWW, New York, USA,\npp. 17–22 (2004)\n27. Zhai, C., Laﬀerty, J.: Model-based feedback in the language modeling approach to\ninformation retrieval. In: Proceedings of the CIKM 2001 Conference, pp. 403–410\n(2001)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.899649441242218
    },
    {
      "name": "Information retrieval",
      "score": 0.7266868948936462
    },
    {
      "name": "Query language",
      "score": 0.6640439033508301
    },
    {
      "name": "Query expansion",
      "score": 0.6324949860572815
    },
    {
      "name": "Context model",
      "score": 0.6091493368148804
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5449569821357727
    },
    {
      "name": "Language model",
      "score": 0.46756821870803833
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4620925784111023
    },
    {
      "name": "Web query classification",
      "score": 0.4541569948196411
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.4539085328578949
    },
    {
      "name": "RDF query language",
      "score": 0.4390195906162262
    },
    {
      "name": "Web search query",
      "score": 0.40420976281166077
    },
    {
      "name": "Natural language processing",
      "score": 0.3632204234600067
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2885287404060364
    },
    {
      "name": "Search engine",
      "score": 0.13778454065322876
    },
    {
      "name": "Programming language",
      "score": 0.07317399978637695
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}