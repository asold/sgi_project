{
    "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
    "url": "https://openalex.org/W4389524112",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5093152882",
            "name": "Gurusha Juneja",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2290175416",
            "name": "Subhabrata Dutta",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103349674",
            "name": "Soumen Chakrabarti",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2639070004",
            "name": "Sunny Manchanda",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2128156271",
            "name": "Tanmoy Chakraborty",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385573855",
        "https://openalex.org/W4318908878",
        "https://openalex.org/W4287393336",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4389519108",
        "https://openalex.org/W4303441863",
        "https://openalex.org/W4313304293",
        "https://openalex.org/W4378498603",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W4366565380",
        "https://openalex.org/W2962800603",
        "https://openalex.org/W3134642945",
        "https://openalex.org/W3048650942",
        "https://openalex.org/W4310629099",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4296605665"
    ],
    "abstract": "Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver’s capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3675–3691\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSmall Language Models Fine-tuned to Coordinate Larger Language\nModels improve Complex Reasoning\nGurusha Juneja∗\nIIT Delhi, India\nSubhabrata Dutta∗\nIIT Delhi, India\nSoumen Chakrabarti\nIIT Bombay, India\nSunny Manchhanda\nDYSL-AI, India\nTanmoy Chakraborty†\nIIT Delhi, India\nAbstract\nLarge Language Models (LLMs) prompted to\ngenerate chain-of-thought (CoT) exhibit im-\npressive reasoning capabilities. Recent at-\ntempts at prompt decomposition toward solv-\ning complex, multi-step reasoning problems\ndepend on the ability of the LLM to simulta-\nneously decompose and solve the problem. A\nsignificant disadvantage is that foundational\nLLMs are typically not available for fine-\ntuning, making adaptation computationally pro-\nhibitive. We believe (and demonstrate) that\nproblem decomposition and solution genera-\ntion are distinct capabilites, better addressed in\nseparate modules, than by one monolithic LLM.\nWe introduce DaSLaM, which uses a decomposi-\ntion generator to decompose complex problems\ninto subproblems that require fewer reasoning\nsteps. These subproblems are answered by a\nsolver. We use a relatively small (13B parame-\nters) LM as the decomposition generator, which\nwe train using policy gradient optimization to\ninteract with a solver LM (regarded as black-\nbox) and guide it through subproblems, thereby\nrendering our method solver-agnostic. Evalu-\nation on multiple different reasoning datasets\nreveal that with our method, a 175 billion pa-\nrameter LM (text-davinci-003) can produce\ncompetitive or even better performance, com-\npared to its orders-of-magnitude larger succes-\nsor, GPT-4. Additionally, we show thatDaSLaM\nis not limited by the solver’s capabilities as\na function of scale; e.g., solver LMs with di-\nverse sizes give significant performance im-\nprovement with our solver-agnostic decompo-\nsition technique. Exhaustive ablation studies\nevince the superiority of our modular finetuning\ntechnique over exorbitantly large decomposer\nLLMs, based on prompting alone.\n1 Introduction\nIn recent years, an astounding variety of text and\nNLP tasks have been accomplished by language\n∗Equal contribution as first author.\n†Correspondence: tanchak@iitd.ac.in\nmodels (LMs) (Devlin et al., 2019) — in essence,\nfitting continuous feature vectors to tokens and\nmodeling smooth conditional distributions over\nthousands of token positions with multi-task ob-\njectives. The next generation of large LMs (LLMs)\nsuch as T5, GPT4 and Bard (Raffel et al., 2020;\nOpenAI, 2023) developed protean capabilities, ex-\ntending to mathematical and logical ability, based\non prompting and in-context learning. Chain-of-\nthought (CoT) prompting has been a key enabler\n(Wei et al., 2022; Feng et al., 2023). LLMs can\nsolve middle-school word problems and equations\nreasonably well. It has also acquired the ability to\ninvoke specialized external tools such as Wolfram\nAlpha (Wolfram, 2023; Schick et al., 2023).\nRecent advances in LLMs have arisen largely\nfrom cleverly-engineered, ever-growing training\ndata, rather than any significant change in network\nstructure, which remains relatively regular, but with\nrapidly increasing network size and number of pa-\nrameters. One outcome of such giant monolithic\nLLMs is unsustainable levels of hardware and en-\nergy (Dhar, 2020) to train them. Meanwhile, neu-\nrologists and brain scientists have known, via fMRI\nscans, inter alia, that cerebral functions are special-\nized and spatially localized (Fedorenko and Varley,\n2016; Mahowald et al., 2023).\nMany recent complex reasoning challenges\nthrown at LLMs have a two-level character – the\ninput task needs to be decomposed into subtasks,\nthen the subtasks need to be solved, and finally,\nsubtask solutions have to be consolidated and com-\nbined to solve the input task. Existing approaches\nuse the same LLM to both decompose and solve\nthe task, sometimes in tangled and uninterpretable\nways. Because the sharing of an LLM across these\nfunctions cannot be closely controlled, very large\nmodels are needed for this double ability (decom-\npose and solve) to emerge.\nStaying entirely inside the LLM regime, and\navoiding the possibility of specialized tools, we\n3675\nFigure 1: Working example of DaSLaM on a mathematical reasoning question from the JEEBench dataset (Arora\net al., 2023). In this example, the solver LM is text-davinci-003. In step 1 , the solver is prompted to answer\nthe question (blue textbox) and it fails to answer correctly (red textbox). A problem decomposing LM generates\nsubproblems (violet textboxes) conditioned on the original question and the initial response of the solver in step\n2 . In step 3 , the solver answers these subproblems iteratively and appends to the prompt. Finally, the original\nproblem is appended to the prompt in step 4 , and the solver answers it correctly (green textbox).\nask a simple question – is it possible to offload the\nability of problem decomposition to a dedicated,\nrelatively smaller scale model, which is specialized\nand can act in synergy with any solver model of\nchoice? To incorporate flexibility and better gen-\neralization, an immediate requirement of such a\nsetup would be to enable a model-agnostic commu-\nnication between the decomposer and the solver.\nOur contributions. To study this research ques-\ntion, we develop DaSLaM (Decomposition And So-\nlution LAnguage Models), in which we separate\nthe decomposer from the solver, as shown in Fig-\nure 1. The solver LM can be conventionally trained\nor fine-tuned. In the illustration, when it answers\na question incorrectly, the decomposer LM takes\nover to produce sub-questions. The partial solu-\ntions are appended and resubmitted to the solver\nLM, which solves the question correctly. The de-\ncomposer LM regards the solver as a black box,\nand uses reinforcement learning (RL) to become a\nspecialized expert at decomposition, informed by\nthe solver’s mistakes.\nExtensive experiments with three reasoning\ndatasets (MATH, AQuA, and JEEBench) show that\nthe proposed specialization improves the perfor-\nmance of OpenAI GPT-3 text-davinci-003 to\noutperform GPT-3.5 and even begins to compete\nwith GPT-4, outperforming other similar methods.\nDaSLaM boosts text-davinci-003 from an exact\nmatch accuracy of 41.6 to 54.5 in zero-shot regime,\nwhich is 3.9 points higher than few-shot GPT-4.\nSimilarly, on Physics problems from JEEBench\ndataset, DaSLaM-augmented text-davinci-003\nscores only 0.58 points short of GPT-4 while out-\nperforming GPT-3.5. The decomposer LM in\nDaSLaM reduces decomposition errors, and general-\nizes well across diverse small-scale LMs. It is also\nmore robust in the face of difficult datasets, where\nthe solver gives near-random performance.\nThese results support our founding hypothesis\nthat heterogeneous functional specialization im-\nproves model efficiency and robustness of LLMs.\nA crucial findings from our experiments is that\nfinetuning the decomposer is much more powerful\nchoice than finetuning the solver. Moreover, a fine-\ntuned decomposer is largely superior compared to\nan orders of magnitude larger LLM prompted to\nact as a decomposer. Given the prohibitive cost of\nfinetuning LLMs like GPT 3, 3.5, or 4, we hope this\nmethod would provide us a promising direction to-\n3676\nwards future development of task-expert models.1\n2 Related Work\nEliciting superior reasoning abilities in LM through\nspecially designed prompts has found its popu-\nlarity through CoT prompting (Wei et al., 2022)\n– asking the LM to explain the reasoning steps\nimproves overall performance. Decomposing a\ncomplex reasoning requirement into multiple, sim-\nple steps results in superior reasoning capabili-\nties across modalities other than free-form natural\nlanguage text as well, e.g., reasoning over tabu-\nlar data (Ye et al., 2023), visual question answer-\ning (Lu et al., 2022), etc. These methods generally\nsolicit a single run of LM inference with no inter-\nmediate prompting interactions. Consequently, the\nLM often misses key reasoning steps or halluci-\nnates irrelevant ones.\nOn the other hand, a prototypical prompter, se-\nquentially interacting with the LM, has shown im-\npressive performance. Progressive Hint Prompting\n(Zheng et al., 2023) uses such a setting; first, the\nLM is asked to provide a base answer. The prompt\nthen uses the answer as a hint to the LM that pro-\ngressively guides it to the final answer. Zhou et al.\n(2023) followed a similar direction by breaking\ndown the problem itself. Their method, Least-to-\nmost prompting, asks the LM to generate simpler,\nrelated problems from a complex problem. The\nfinal solution to the original question is generated\nby the LM conditioned upon the solution of the sub-\nproblems. A major bottleneck then becomes the\nsolver’s ability to identify the critical subproblems.\nDecomposing a complex task and then solving each\ntask via multiple LLMs with their own in-context\nexamples have been attempted as well (Dua et al.,\n2022; Khot et al., 2023). Recently, Shridhar et al.\n(2022) explored subquestion generation from com-\nplex questions as means of distilling reasoning abil-\nities from larger LMs to smaller ones.\nOur proposed method, DaSLaM makes a depar-\nture from these mentioned approaches in three par-\nticular features: (i) we seek to separate out the de-\ncomposer from the solver to get rid of the solver’s\nlimitations affecting decomposition, (ii) the decom-\nposer acts as a plug-and-play module that can gen-\neralize to any solver, and (iii) the decomposition\nactuates with complete knowledge of the solver’s\nactions.\n1The codebase is given at: https://github.com/\nLCS2-IIITD/DaSLaM\n3 A General Overview of DaSLaM\nGiven an LM θ and a question Q as a sequence\nof tokens, a standard zero-shot prompting can be\ndescribed as,\nˆA= arg max\nA∈A\npθ(A|Q)\nwhere ˆAis the inferred answer, and Ais the set of\npossible answers (e.g., numerical values, multiple-\nchoice options, True/False, etc.). With a CoT\nprompting, the LM generates a sequence of tokens\nexplaining the stepsSto reach the answer given the\nquestion. The modified process can be described\nas,\nˆA= arg max\nA∈A\n[\npθ(A|S,Q) arg max\nS\npθ(S|Q)\n]\n(1)\nWhen answering Qrequires multistep reasoning,\none can conceptualize Sas a sequence of smaller\nsteps {S′\n1,S′\n2,··· ,S′\nn}such that the LM iteratively\nanswers a sequence of subproblems{Q′\n1,··· ,Q′\nn}\nto finally reach the desired answer to the original\nquestion. Eq. 1 can then be rewritten as,\nˆA= arg max\nA′n∈A\n∏\ni\npθ(A′\ni|S′\ni,Q′\ni) arg max\nS′\ni\npθ(S′\ni|Q′\ni) (2)\nwhere A′\ni is the answer to the subproblem Q′\ni. In\nthe usual regime of CoT, the subproblems Q′\ni are\nimplicit; the LM discovers them on its own and\ngenerates the reasoning steps and the answers. Due\nto the repeated multiplication in Eq. 2, any error\nin the initial stages quickly propagates along the\nchain of steps.\nIn DaSLaM, we seek to alleviate this problem by\noffloading the task of inferring the subproblems\n{Q′\ni}to a decomposer LM ϕ. For a more guided\nproblem decomposition, DaSLaM uses the answer\nand the steps generated by the solver LM, θin the\nnaive CoT regime as described in Eq. 1 to generate\na set of subproblems {ˆQi}i=1,...,n as follows:\nˆQi = arg max\nQ′\ni\npϕ(Q′\ni|{ˆQj : j ∈[1,i −1]},\nQ, ˆA0,S0), (3)\nfor i ∈[1,n], where ˆA0 and S0 are the initial an-\nswer and reasoning steps, respectively generated by\nθ. The solver LM θthen solves the subproblem set\none-by-one similar to Eq. 2. However, instead of\nseeking to generate the final answer as a response\nto the last subproblem ˆQn, we append the origi-\nnal question at the end and let θanswer it directly\ngiven the context generated by the subproblems,\ntheir answers, and the corresponding CoTs. The\nfour stages of workflow with DaSLaM, as described\n3677\nin Figure 1, can then be summarized as follows:\nˆA0 = arg max\nA∈A\n[\npθ(A|S0,Q) arg max\nS0\npθ(S0|Q)\n]\nˆQi = arg max\nQ′\ni\npϕ(Q′\ni|{ˆQj}j∈{1,i−1},Q, ˆA0,S0)\nˆAi = arg max\nAi∈A\n[\npθ(Ai|Si, ˆQi) arg max\nSi\npθ(Si|ˆQi)\n]\nˆA= arg max\nA∈A\n[\npθ(A|S,Q)\narg max\nS\npθ(S|{ˆAi,Si, ˆQi}i∈{1,N},Q)\n]\n(4)\n4 Learning to Decompose from Feedback\nTypical LMs are not suitable to reliably perform\nthe problem decomposition stage in Eq. 3. We\nseek to finetune the decomposer LM, ϕ for this\ntask. Specifically, we use the LLaMA 13 billion\nmodel. Instead of full LM-tuning, we use LoRA\nadapters (Hu et al., 2022) for parameter-efficient\ntraining. For brevity, we will denote the adapter-\naugmented LM as ϕ, although only the adapter\nweights are being updated while training. The\nwhole process of teaching the LM to perform the\nproblem decomposition comprises two successive\nstages: (i) subproblem construction from the origi-\nnal question and CoT, and (ii) policy optimization\nin conjunction with the solver LM. Details of the su-\npervised data curation required for these three steps\nare described in Section 5. Each data sample in\nthe supervised dataset Dcan be conceptualized as\na sequence of triplets ⟨Qgold,Sgold,Agold,Q′\ngold⟩,\nwhere Qgold denotes the original question, Sgold de-\nnotes reasoning steps in form of CoT,Agold denotes\nthe answer to the question, andQ′\ngold is a sequence\nof subproblems generated by decomposing Qgold\n(see Section 5 and Appendix A for further details\non supervised data curation process).\nThe first stage is straightforward with the decom-\nposer LM being finetuned using language modeling\nobjective. In the first stage, we seek to optimize the\nfollowing objective:\nmin\nϕ\n[−log(pϕ(Q′\ngold|Q,S))] (5)\nThis step is somewhat similar to instruction tuning,\nwhere an LM is asked to generate some text condi-\ntioned on a context, instead of following the usual\nsentence completion-style behavior of the LM. In-\ntuitively, the role of this stage is to align the LM to\nthe task of the decomposer.\nDecomposition via policy network. The previ-\nous stage inculcates the ability to decompose a\nproblem within ϕ. However, it is still blind to the\nactual errors made by the solver LM. In the next\nstage, we seek to make the decomposer LM work\nin synergy with any solver LM of choice. This\nsolver agnosticism restrains ϕto observe the inter-\nnal representations computed by θwhile solving\nthe problem. To handle this imposed blackbox\ncharacteristics of the solver, we resort to policy gra-\ndient optimization of ϕassuming θto be part of the\nenvironment. We formalize the setup as follows.\nElements of the environment: The solver LM θ\nconstitutes the core component of the environment.\nGiven a question Q, the solver-generated CoT S\nand the answer Aas sequences of tokens define the\nobservation of the policy.\nState and action space : We define the state\nspace as S. The initial state, s0 ∈S, is defined\nby the original question Qand the initial response\nfrom the solver, S0,A0, that are provided to the\ndecomposer LM ϕas input. A single timestep is\ndefined on generation of a single token, i.e., the\naction at at time t. Given the autoregressive na-\nture of LM, we define the state at t-th timestep as\nst = (st−1,{at−1}), i.e., the token generated at\nt−1 appended to the tokens generated till t−1.\nTrivially, the action space is the same as V, the\nvocabulary of ϕ.\nPolicy: The decomposer LM is conceptualized\nas a policy network πϕ : S− →V, i.e., it generates\na token given the inputs and the token generated\nhitherto, till the end of episode T. Inspired by the\nrecent success in Reinforcement Learning from Hu-\nman Feedback (RLHF) with autoregressive LMs,\nwe choose the Proximal Policy Optimization (PPO)\nalgorithm to train the policy πϕ(a|s). In a typical\nPPO setup, we define the advantage function as\nfollows:\nˆGt =\nT−t+1∑\ni=0\n(γλ)i[rt+i + γV(st+i+1) −V(st+i)] (6)\nwhere rt is the reward at step t, V(st) :S− →R is\nthe value function determining the reward associ-\nated to state st, and γ,λ are hyperparameters. We\nuse the policy model augmented with a randomly\ninitialized feedforward layer as the value function.\nA crucial component in on-policy learning regime\nis the reward function rt. While the end goal of the\nlearning is to have the solver answering correctly,\nthe decomposer LM should receive some incremen-\ntal signal aligned to its generation as well for it to\nstably converge to the optimal policy. With this\n3678\ngoal in mind, we construct the reward function as,\nrt = R1 + R2 + R3 + R4 + R5 (7)\nwhere R1 to R5 are defined as follows (see Ap-\npendix B for a detailed treatment on the reward\ncomputation method). Here cos-sim represents co-\nsine similarity. I(x) = 1if xis true is the indicator\nfunction.\nEntity coverage: R1 =\n|EQ′|\n|EQ|, where EQ′and EQ\nare the sets of distinct entities in the generated\nsubproblems and the original question, respec-\ntively.\nConsistency of answers to subproblems:\nR2 =\n∑\ni\n(\nI(ei = ˆei) + cos-sim(Q′\ni,Ai)\n)\n(8)\nwhere ˆeiis the entity whose value has been asked\nin the subproblem Q′\ni, and ei is the entity an-\nswered. This reward penalizes the decomposer\nLM for generating questions whose answers are\nnot consistent.\nOrder of operations: R3 = l\nm, where l is the\nnumber of operations matched in order between\nSand Sgold, and mis the total number of opera-\ntions in Sgold.\nCoT proximity: To ensure that the distance of rea-\nsoning produced by the model after prompting\nS to the gold reasoning Sgold is less than the\ndistance of reasoning produced without prompt\nS0 to the gold reasoning steps Sgold, we design\na reward based on the cosine similarity of each\nstep of Sgold. We break S and S0 at new-line\ntoken to form reasoning steps. At each step\nj, we compute c1j = cos-sim(Sj,Sj\ngold) and\nc2j = cos-sim(Sj\n0,Sj\ngold). The reward is\nR4 =\nm∑\nj=0\nI(c1j >c2j)c1j + I(c2j >c1j)(−1 −c2j),\n(9)\nCorrectness of final answer: R5=I( ˆA=Agold).\nNow, we can define the PPO objective as follows:\nmax\nϕ\n(\nEt\n[πϕ(at|st)\nπref(at|st)\nˆGt\n]\n−βEt[Kt]\n)\n(10)\nwhere πref is the reference model that is ini-\ntialized with supervised finetuned ϕ. Kt =\nKL[πref(·|st),πϕ(·|st)] is the KL-divergence be-\ntween the reference model and the policy model.\nThe resulting decomposer LMϕoptimized using\nthe above mentioned three stages of finetuning can\nthen be used with DaSLaM.\n5 Experiments\nTraining data curation. The training process of\nDaSLaM consists of two stages as mentioned previ-\nously. In the first stage, we require the subproblems\nalong with the reasoning steps for a given prob-\nlem. We use samples from four existing datasets —\nMATH (Hendrycks et al., 2021), AQuA (Ling et al.,\n2017), GSM8K (Cobbe et al., 2021), and Strate-\ngyQA (Geva et al., 2021). Each question in these\nfour datasets contains a question Qgold, a step-by-\nstep illustration of the reasoning process Sgold, and\nthe final answer Agold. We sample 7,000 examples\nfrom the training splits of these datasets and employ\nOpenAI’s text-davinci-003 model to generate\nthe corresponding subquestions. We provide the\nmodel with one-shot example illustrating how to\ndecompose a question into subquestions based on\nthe reasoning. In the second stage of training, we\nutilize the remaining training data from MATH and\nAQuA datasets to conduct the policy optimization\nsince this step does not require any supervised ex-\namples of subproblems.\nLMs used. We use LLaMA 13 billion (Touvron\net al., 2023) as the decomposer LM. For the solver\nLM, we primarily use text-davinci-003 (hence-\nforth, we denote it as GPT-3.5 for brevity). We\nalso experiment with the LLaMA 13 bilion and\nLLaMA 33 billion models as solvers to test the\nmodel-agnostic generalizability of DaSLaM.\nBaselines. We compare DaSLaM with four ex-\nisting methods of prompting: Chain-of-thought\nprompting (CoT) (Wei et al., 2022), Least-to-most\nprompting (L2M) (Zhou et al., 2023), Progressive\nHint Prompting (PHP) (Zheng et al., 2023), and,\nDemonstrate-Search-Predict (DSP) (Khattab et al.,\n2022a). The original setting of PHP requires an\n8-shot prompting; however, since all other methods\nincluding DaSLaM predict in the zero-shot setting,\nwe use PHP in 1-shot for a fairer comparison. Addi-\ntionally, we experiment with three ablation variants:\nDaSLaM-NF does not take the solver feedback into\naccount while generating the subproblems; Fine-\ntuned is the solver LM (LLaMA 13B in this case,\nwe could not finetune 33B variant due to compu-\ntational constraints) finetuned without any decom-\nposer; GPT-3.5 decomposer does away with the\nfinetuned LLaMA 13B decomposer and uses pre-\ntrained GPT-3.5 as the prompted decomposer.\nTest datasets. For evaluation purposes, we use\nthree datasets – MATH (Hendrycks et al., 2021),\nAQuA (Ling et al., 2017), and JEEBench (Arora\net al., 2023). For the first two datasets, only the\ntest splits are used during evaluation since their\n3679\nDataset Method\nCoT L2M PHP DSP GPT3.5 Decomposer DaSLaM-NF DaSLaM\nPnC 16.4 16.0 10.2 16.2 16.0 20.0 21.4\nNT 14.4 11.0 9.8 20.3 14.2 18.4 26.1\nALG 27.6 22.4 24.0 15.3 32.1 31.6 33.4\nI-ALG 16.4 16.8 10.0 17.0 18.4 20.8 24.8\nCalc. 14.0 14.58 14.28 18.8 12.0 15.1 18.2\nP-ALG 32.3 28.0 26.5 28.0 35.5 38.0 44.0\nGeom. 14.2 12.5 14.0 5.2 22.0 19.04 21.4\nAQuA 41.6 44.7 44.4 44.0 45.4 53.2 54.5\nTable 1: Performance comparison on MATH and AQuA datasets using GPT-3.5 as the solver LM. See Section 5 for\nabbreviations.\ntraining splits are used while finetuning the decom-\nposer. The MATH dataset contains mathematical\nproblems on multiple different domains. We report\nthe results on each of them separately and use the\nfollowing abbreviations – ALG, I-ALG, and P-\nALG for Algebra, Intermediate Algebra, and Pre-\nAlgebra, respectively; Calc for Calculus, Geom\nfor Geometry, PnC for Probability and Combina-\ntorics, NT for Number theory. From the JEEBench\ndataset, we use the problems in Physics ( Phy)\nand Mathematics (Math). Each of these two sub-\njects has three types of problems – single-answer\nmultiple-choice questions (MCQ), numerical prob-\nlems ( Num), and multi-answer multiple-choice\nquestions (Multi). For all these datasets, we use\nexact match criteria to evaluate the correctness of\nthe model-inferred answers. Details of training and\ninference hyperparameters and compute resource\nusage are provided in Appendix C.\n6 Experimental Results\nThe tasks used to evaluate the performance of\nDaSLaM contain questions that can be answered\neither of the three types – numerical, single correct\nanswer MCQ, and multiple correct answer MCQ.\nDaSLaM is better than pure promptingWe start\nwith DaSLaM augmented with GPT-3.5 as the solver\nLM on MATH and AQuA datasets (see Table 1).\nThe improvement achieved withDaSLaM prompting\ncompared to standard CoT is staggering across all\ntypes of problems in the MATH dataset: +11.7 on\nPre-Algebra, +8.4 on Intermediate Algebra, +7.7\non Number Theory, +7.2 on Geometry, +5.0 on\nProbability and Combinatorics, +5.8 on Algebra,\nand +4.2 on Calculus. The absolute improvement\nis even larger on the AQuA dataset, i.e.,+12.9 over\nCoT. It is noticeable that the effects of DaSLaM are\nstronger across tasks containing algebraic reason-\ning (AQuA, Pre- and Intermediate-Algebra, etc.)\ncompared to Probability and Combinatorics or Cal-\nculus, which require more implicit knowledge. The\nperformance gain achieved via DaSLaM is signif-\nicantly better compared to methods like L2M or\nPHP. The latter methods often fail to improve over\nstandard CoT (e.g., on Probability and combina-\ntorics, Number Theory, and Algebra problems,\nL2M shows a drop in accuracy). Even when im-\nproving over CoT, their improvement is meager\ncompared to DaSLaM. This trend entails our earlier\nargument in support of offloading the problem de-\ncomposition task to a specialized LM; methods that\nprompt the solver LM to decompose the problem\nlack the expertise achieved via dedicated finetuning\nin DaSLaM.\nFinetuned decomposer is essential. Despite\nbeing orders of magnitude smaller, a finetuned\nLLaMA 13B model delivers better performance\ncompared to GPT-3.5 as a decomposer (DaSLaM\nvs. GPT-3.5 generator in Table 1 and 2). This fur-\nther justifies our choice of separately finetuning the\ndecomposer and the added flexibility that it offers.\nIn fact, finetuning the decomposer is far effective\ncompared to finetuning the solver (DaSLaM vs\nFinetuned solver in Table 2).\nFeedback from the solver is important. In\nthe preceding paragraph, we attributed the superi-\nority of DaSLaM over other methods to the usage\nof a specialized LM for problem decomposition.\nHowever, manipulating the problem decomposi-\ntion upon feedback from the solver is also an im-\nportant factor here. None of the existing methods\ndoes so, and therefore, remains blind towards what\nreasoning (and possible errors) is followed by the\nsolver model. This is further manifested when we\n3680\nMethod Dataset\nPnC NT ALG iALG Geom Cal Palg AQuA\nLLaMA 13 billion\nCoT 2.05 4.0 3.12 2.4 3.2 2.08 5.0 17.7\nL2M 1.66 3.2 3.33 2.8 2.0 3.33 4.54 16.6\nFinetuned 2.8 3.6 3.57 3.2 4.1 3.05 6.04 19.4\nGPT3.5 Decomposer 2.05 5.0 4.68 2.8 2.08 4.0 6.66 20.4\nDaSLaM-NF 2.93 4.8 4.68 3.2 4.0 3.9 6.2 21.6\nDaSLaM 4.0 5.6 4.70 3.4 4.3 4.1 8.33 22.0\nLLaMA 33 billion\nCoT 2.4 4.16 4.54 3.7 4.0 4.0 5.2 20.0\nL2M 2.38 4.16 4.2 6.0 4.25 5.71 5.55 21.6\nDaSLaM-NF 3.2 5.83 5.6 5.6 5.1 5.71 5.2 22.5\nDaSLaM 4.0 7.36 9.09 6.02 5.3 6.03 8.44 26.8\nTable 2: Performance on\nMATH and AQuA with\nLLaMA 13 billion and\nLLaMA 33 billion as solvers.\nPHP is not reported as\none-shot PHP generated\nrandomly with both LLaMA\nvariants. DaSLaM provides\nconsistent improvement\nacross all the tasks while\nother baseline methods\nmostly fail.\nMethod Dataset\nPhy MCQ Math MCQ Phy Multi Math Multi Phy Num Math Num Phy Int Math Int\nCoT 33.33 21.9 6.25 12.0 3.03 1.69 12.5 20.0\nPHP 22.22 17.07 6.25 7.59 3.03 1.69 0* 4.0\nL2M 22.22 21.9 6.25 12.5 3.03 3.38 10.0 20.0\nDaSLaM-NF 20.8 31.7 7.5 10.12 3.03 3.38 12.5 16.0\nDaSLaM 55.55 36.5 18.75 16.0 6.06 10.16 22.5 24.0\nGPT-4 55.55 34.14 27.5 21.5 15.15 11.8 25.0 20.0\nTable 3: Performance comparison on the JEE benchmark dataset with GPT-3.5 as the solver LM. 0* signifies that\nthe model was not able to answer any problem in the task correctly.\ncompare DaSLaM with itself without the feedback\nmodule, DaSLaM-NF. While DaSLaM-NF is able to\nimprove upon basic CoT and other prompting meth-\nods, it falls short of a decomposer LM that has\naccess to the initial response of the solver.\nDaSLaM generalizes to smaller solvers. An im-\nportant aspect of a prompting method is its ability\nto work with LMs of different scales. Despite be-\ning finetuned with GPT-3.5 responses only,DaSLaM\nis able to improve upon the base performance of\nsmaller scale LLaMA models as well (see Table 2).\nL2M prompting generally fails with both LLaMA\n13 billion and 33 billion variants. On the other hand,\nDaSLaM, with or without feedback, almost doubles\nthe performance of the base CoT across multiple\ntasks of the MATH dataset. It shows substantial\nimprovement on AQuA as well. The importance\nof feedback from the solver LM usually manifests\nstrongly in proportion to the scale of the solver.\nDaSLaM generalizes to harder problems. Since\nthe decomposer LM ϕis trained using a subset of\nthe training data of MATH and AQuA, we opt for a\nharder (in terms of benchmark performance of dif-\nferent LMs) reasoning evaluation on the JEEBench\ndataset. Table 3 summarizes the performance of the\nbaselines and DaSLaM with GPT-3.5 as the solver\nLM on Mathematics and Physics questions of the\nJEEBench dataset. We notice that the superiority\nof DaSLaM manifests even more profoundly on this\ntask compared to the former ones. Both PHP and\nL2M prompting absolutely fail to improve upon\nbasic CoT prompting, often with a sharp fall in per-\nformance (e.g., Physics MCQ questions). On the\nother hand, DaSLaM boosts the LMs performance,\nvery often over 100% relative improvement (all\nthree types of problems in Physics and numeri-\ncal problems in Mathematics). Aggregated across\nquestion types, DaSLaM boosts the performance of\nGPT-3.5 to 22.420 in Physics and 22.07 in Math-\nematics. It is noteworthy that the same LM in\nits base setting performs near-random, i.e., 10.4\nand 10.7 in Physics and Mathematics, respectively,\nwhereas a random selection baseline gives scores\nof 9.6 and 10.3, respectively (Arora et al., 2023).\nFurthermore, GPT-3.5 with DaSLaM outperforms a\nbetter optimized candidate of the GPT series, GPT-\n3.5 on both these subjects (note that Arora et al.\n(2023) reported 18.9 and 15.7 scores with GPT-3.5\non Physics and Mathematics, respectively). Com-\nparison with GPT-4. The colossal compute used\nby GPT-4 makes the comparison with any of its\npredecessors like GPT-3.5 quite unfair. However,\nit is tempting to observe that DaSLaM boosts the\nperformance of GPT-3.5 often to the level of GPT-\n4. For example, on arithmetic problems of the\nAQuA dataset, DaSLaM surprisingly outperforms\nboth zero-shot and few-shot GPT-4 (40.6 and 50.4\nrespectively, compared to 54.5 with GPT-3.5 and\nDaSLaM). On MATH dataset, DaSLaM augmented\nGPT-3.5 scores an aggregate of 30.23, which is\nbetter than ChatGPT ( 26.4) and close to GPT-4\n(35.7). On JEEBench Mathematics problems, GPT-\n3681\nFigure 2: An example case study on a problem from the MATH dataset. GPT-3.5 is used as the solver LM with\nthree different methods of prompting – standard CoT, Least-to-most, and DaSLaM. Only DaSLaM is able to guide the\nmodel to the correct answer.\n4 comes up with an aggregate score of 23.1, which\nis pretty close to our 22.42. In Physics and Math\nMCQ questions, DaSLaM with GPT-3.5 outperforms\nGPT-4. These results definitely do not claim any\nassumed superiority of DaSLaM-boosted GPT-3.5\nover GPT-4 since there are multiple other cases that\nstate otherwise. Instead, we seek to demonstrate\nhow much leftover potential these LMs possess\nthat can be unleashed via our proposed method\nof feedback-guided automatic problem decomposi-\ntion.\n7 Case Study\nTo this point, we have compared the numbers pro-\nduced by DaSLaM-boosted models across different\ndatasets. While they provide an overall assess-\nment, deeper analyses are needed to comprehend\nthe actual reasoning steps adopted by these differ-\nent methods. Figure 2 shows the reasoning steps\ngenerated by GPT-3.5 given an example problem\nfrom the MATH dataset with three different prompt-\ning methods – vanilla CoT, L2M, andDaSLaM. Note\nthat DaSLaM uses the CoT output to decompose the\nproblem.\nBoth CoT and L2M end up with the model an-\nswering incorrectly. With CoT, the solver wrongly\nassumes that the given equation must have two real\nroots though it should not have any real roots. Also,\nit mistakes the value ofa2 as a. The effect is promi-\nnent in the subproblems generated by DaSLaM as it\nasks to find the value of aexplicitly. Furthermore,\nthe solver LM specifically announces that y≤0 to\nanswer the first subproblem generated by DaSLaM.\nThis helps to correct the reasoning about the sign\nof the discriminant.\nWith L2M, the confusion around the value of a\nand a2 persists, as the solver LM substitutes ain\nthe given equation by 49 (which is the value of a2)\ntwice in the answering process. Although it substi-\ntuted the correct value of aonce while answering\nthe second question, it is not explicitly declared\nlike in DaSLaM. We observe multiple similar failure\ncases with L2M. It is quite likely that prompting\nthe model to generate the final answer after each\nsubproblem accumulates the erroneous reasoning\nsteps that the model falls prey to.\nWith DaSLaM, the reasoning steps followed by\nthe solver remains robust throughout. It reaches\nthe final answer much earlier (third and fourth sub-\nproblems). In the final answer, the solver simply\n3682\nreiterates the steps that it earlier generated to an-\nswer the subproblems. This is a common behavior\nthat we observed across multiple problems from\nmultiple datasets. In Appendix D (see Figures 3\nand 4), we provide similar case studies on LLaMA\n13B and 33B with different prompting methods.\nWith reduced solver capacity, the difference be-\ntween Least-to-most and CoT generated reasoning\nsteps further diminishes with both leading to in-\ncorrect answers; DaSLaM, on the other hand, still\nguides the solver through correct steps.\nAn interesting observation can be made by com-\nparing how the solver behaves with CoT vs. with\nDaSLaM. With DaSLaM, we do not provide any new\nknowledge to the solver. Yet, the same model can\nrectify its errors made in CoT response. This may\npoint to the intuition that current LLMs are actu-\nally underutilized, and one can unfold even more\nimpressive performance with cleverly composed\nguidance.\nWe further provide case analyses with when\nDaSLaM fails to guide the model (GPT 3.5 in this\ncase) to successful final answers, in Appendix E.\nWhile we do not find any obvious pattern of er-\nrors, one can see that the decomposer generates\nquestions that are not readily answerable within\nthat context. DaSLaM does not use any method to\ntrace back the error or generate subproblems based\non the answers to the previous subproblems. This\nmight raise such issues where the subproblems gen-\nerated are not actually helping the solver in the\nright order.\n8 Conclusion\nWe challenged the design of ever-larger monolithic\nLLMs as homogeneous network structures, where\ndiverse aspects of problem decomposition and solu-\ntion are stored in a tangled and opaque manner. The\nformidable general-purpose problem-solving capa-\nbilities of LLMs are exceedingly resource-hungry,\ndependent on immense data engineering. Inspired\nby brain science, we took a first step toward hetero-\ngeneity — let two different LLMs evolve indepen-\ndently and adapt to their roles of decomposing and\nsolving complex reasoning problems. Through ex-\ntensive experiments on several benchmarks, we\nshowed that such a heterogeneous network can\nmatch or exceed some of the largest contemporary\nLLMs, at a much smaller parameter count.\nLimitations\nA potential limitation of DaSLaM, as with many sys-\ntem that uses an LLM-as-a-service API charging\nper token exchange, is the increased token usage\nbecause of the RL exploration. Asserting a token\nbudget on the decomposer LM is left as an avenue\nfor future exploration. Ideally, the decomposer LM\nshould seamlessly invoke solvers of many forms,\nsuch as retrievers (Khattab et al., 2022b) or mathe-\nmatical calculators (Schick et al., 2023; Wolfram,\n2023). Future work may extend DaSLaM to such\ntools. DaSLaM is limited to purely text-based sub-\nproblem decomposition; it is not possible at present\nto incorporate reasoning through other modalities\n(e.g., visual inputs for geometric reasoning) into\nDaSLaM in its current form.\nAcknowledgments\nThe authors acknowledge the financial support of\nDYSL-AI.\nReferences\nDaman Arora, Himanshu Gaurav Singh, and Mausam.\n2023. Have llms advanced enough? a challenging\nproblem solving benchmark for large language mod-\nels.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPayal Dhar. 2020. The carbon impact of artificial intel-\nligence. Nature Machine Intelligence, 2(8):423–425.\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt\nGardner. 2022. Successive Prompting for Decompos-\ning Complex Questions. In Empirical Methods in\nNatural Language Processing (EMNLP).\nEvelina Fedorenko and Rosemary A. Varley. 2016. Lan-\nguage and thought are not the same thing: evidence\nfrom neuroimaging and neurological patients. An-\nnals of the New York Academy of Sciences, 1369.\n3683\nGuhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye,\nDi He, and Liwei Wang. 2023. Towards revealing\nthe mystery behind chain of thought: a theoretical\nperspective.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022a. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp. arXiv preprint\narXiv:2212.14024.\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022b. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive NLP. arXiv preprint\narXiv:2212.14024.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2023. Decomposed prompting: A modular\napproach for solving complex tasks. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158–167, Vancouver,\nCanada. Association for Computational Linguistics.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. Advances in Neural Information\nProcessing Systems, 35:2507–2521.\nKyle Mahowald, Anna A. Ivanova, Idan A. Blank,\nNancy Kanwisher, Joshua B. Tenenbaum, and\nEvelina Fedorenko. 2023. Dissociating language\nand thought in large language models: a cognitive\nperspective.\nOpenAI. 2023. Gpt-4 technical report.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2022. Distilling multi-step reasoning ca-\npabilities of large language models into smaller mod-\nels via semantic decompositions. arXiv preprint\narXiv:2212.00193.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nStephen Wolfram. 2023. ChatGPT gets its “Wolfram\nsuperpowers”! (blog).\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei\nHuang, and Yongbin Li. 2023. Large language\nmodels are versatile decomposers: Decompose evi-\ndence and questions for table-based reasoning. arXiv\npreprint arXiv:2301.13808.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo\nLi, and Yu Li. 2023. Progressive-hint prompting\nimproves reasoning in large language models.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V Le, and Ed H.\nChi. 2023. Least-to-most prompting enables com-\nplex reasoning in large language models. In The\nEleventh International Conference on Learning Rep-\nresentations.\n3684\nSmall Language Models Fine-tuned to Coordinate Larger Language\nModels improve Complex Reasoning\n(Appendix)\nA Supervised Fine-tuning Dataset\nThe training data for supervised fine-tuning stage\nwas generated using text-davinci-003. Each\ndata point in the dataset consisted of a tuple\n⟨Qgold,Sgold,Q′\ngold⟩, where Qgold represents the\nreasoning question, Sgold represents the gold rea-\nsoning steps, andQ′\ngold represent the sub-questions\ngenerated by text-davinci-003 in a one-shot set-\nting. An example of a single data point is given\nbelow.\nQgold\nJohn borrowed 3 soccer boots from Jake, and forgot them\non the field, if peter came across a total of 15 boots on the\nfield Jake’s boots inclusive, and he took 4 boots at random,\nwhat is the probability that Jake’s boots were not amongst\nthe 4 taken?. (A) 12\n91 , (B) 3\n15 , (C) 12\n15 (D) 33\n91 , (E) 3\n91\nSgold\nSince Jake owns 3 of the boots, the subset from which the 4\nboots should be chosen are the 12 boots not owned by Jake\nfrom the universe of 15.The first boot can be one of the 12\nfrom the 15 with probability 12\n15 . The second boot can be\none of the 11 from the 14 remaining with probability 11\n14 .The\nthird boot can be one of the 10 from the 13 remaining with\nprobability 10\n13 . The fourth boot can be one of the 9 from the\n12 remaining with probability 9\n12 .The total probability will\nbe 12\n15 ·11\n14 ·10\n13 · 9\n12 . On cancellation, this comes to 33\n91\nQ′\ngold\n1. How many boots did Jake own?\n2. How many boots were on the field?\n3. How many boots did Peter take?\n4. What is the probability of choosing one of the 12 boots\nnot owned by Jake from the universe of 15?\n5. What is the probability of choosing the second, third\nand fourth boots not owned by Jake?\n6. What is the total probability?\nB Reward Calculation\nDuring policy gradient optimization of the decom-\nposer LM ϕ, rewards were provided in an incremen-\ntal fashion so that the generated sub-questions align\nwell with the initial question, the answer generated\nby the solver LM θand the sub-question solving\ncapabilities of θ. For this, the reward was designed\nas a combination of five terms. The definitions of\nthese terms are given in Section 4. Here we provide\nan example of the reward calculation.\nEntity coverage Reward ( R1). Here, we find\ndistinct nouns and numbers in the question and the\nsub-questions using the nltk library. The reward\nis calculated using R1 =\n|EQ′|\n|EQ|, where EQ′ and\nEQ are the sets of distinct nouns and numbers in\nthe sub-questions and questions, respectively. An\nexample of the same is shown below.\nQgold:\nEach good worker can paint my new house alone in 12\nhours. Each bad worker can paint my house alone in 36\nhours. I need my house painted in 3 hours. If I can only\nfind 3 good workers, how many bad workers must I also\nfind in order to have my house painted on time?\nQ′\ngold:\n1. How many goodworkers are needed to paint thehouse\nin 3 hours?\n2. How many bad workers are needed to paint the house\nin 3 hours?\n3. What is the total number of workers needed to paint\nthe house in 3 hours?\n|EQ′ |= 4 + 1 = 5 (11)\n|EQ′ |= 6 + 3 = 9 (12)\nR1 = |EQ′ |\n|EQ| = 5\n9 = 0.55 (13)\nConsistency of answers to subproblems re-\nward ( R2). To ensure that the sub-question is\nsuch that the solver LLM ϕ does not produce\nrandom answers, we compare the entity in the\nsolution to the entity asked in the subquestion\nalong with the cosine similarity of the embed-\ndings of sub-question and sub-answers. To find\nthe embeddings, we use the pre-trained model\nin the spacy library. R2 = ∑\ni(I(ei = ˆei) +\ncosine-similarity(Q′\ni,Ai)), where I(x) = 1if xis\ntrue, ˆei is entity whose value has been asked in the\nsubproblem Q′\ni, and ei is the entity answered. An\nexample for the reward calculation is given below.\n3685\nQ′\n1: How many good workers are needed to paint the house\nin 3 hours?\nA1: We can calculate the amount of work done in 3 hours\nby a good worker by dividing 12 hours by 3 hours. We get\n4 as the answer. We know that a bad worker can paint the\nhouse in 36 hours. We can calculate the amount of work\ndone in 3 hours by a bad worker by dividing 36 hours by 3\nhours. We get 12 as the answer. We need to find out how\nmany good workers are needed to paint the house in 3 hours.\nWe can calculate the number of good workers needed by\ndividing 12 by 4.\nAnswer: 3 good workers can paint the house in 3 hours.\ncos-sim(Q′\ni,Ai) = 0.70457435\nI(ei = ˆei) = 1\nR2 = 1 + 0.70457435 = 1.70457435\nOrder of operations reward (R3). To evaluate\nthe correctness of the mathematical solution pro-\nduced by the model, we compare the order of opera-\ntions. R3 = l\nm, where lis the number of operations\nmatched in order between S and Sgold, and mis\nthe total number of operations in Sgold. Below is\nan example of R3 calculation.\nSmodel:\nOriginal Price = x\nDiscounted Price = 0.78x\nPrice after coupon = 0.78x - 20\nPrice paid = 0.78x - 20 + 1.90\n0.78x - 20 + 1.90 = 0.5x\n0.28x = 21.90\nx = 78.21\nSgold : Let x be the original price of item\nDiscounted price = 0.78x\nPayment made by the customer after using the $20 coupon\n= 0.78x - 20\n0.78x - 20 = x/2 + 1.9\nx = 78.20\noperations in Smodel: [-, -, +, -, +]\noperations in Sgold: [-, -, /, +]\nR3 = 3\n4 = 0.75\nCoT Proximity reward (R4). As defined in Sec-\ntion 4, R4 =∑m\nj=0 I(c1j > c2j)c1 + I(c2j >\nc1j)(−1−c2j), where I(·) is the indicator function.\nHere we present an example to illustrate the same.\nSgold\n1. : We first consider the possible neighbors of the\nchief’s mother, one must be the chief, the other is\none of the remaining 5 natives. That native can, in\nturn, have one of 4 neighbors in addition to the chief’s\nmother\n2. Continuing around, there are 5! = 120possible ar-\nrangements\n3. The chief can be on his mother’s left or right, for a\ntotal of 2 ·120 = 240arrangements\n4. Alternatively, we know that the seating arrangement\nis the same if it is rotated, once the chief’s spot is\nchosen, the rotations are eliminated\n5. There are 2 ways to seat his wife and his mother, and\nthen there are 5 distinct positions for the remaining\n5 natives, which makes for 2 ·5! = 240 possible\narrangements\nS0:\n1. We can see that the chief must be seated between his\nmother and his wife.\n2. This means that there are 3 possible positions for the\nchief, and 5 possible positions for the other 6 people.\n3. Therefore, there are 3! ·5! = 1800 different seating\narrangements.\nS:\n1. The chief must sit between his mother and his wife.\n2. There are 8 people in the tribe.\n3. So, the chief’s mother and wife can be arranged in 2!\nways.\n4. The remaining 5 people can be arranged in 5! ways.\n5. Therefore, the total number of possible seating ar-\nrangements after removing rotation = 5! = 240\nCosine similarity calculation per step:\n1. c1j = 0.47673503, c2j = 0.44773823\n2. c1j = 0.45063934, c2j = 0.47917843\n3. c1j = 0.5173945, c2j = 0.20383504\n4. c1j = 0.46866685, c2j = 0\n5. c1j = 0.47825924, c2j = 0\nHence, R4 = 0 .47673503 + (−1 −0.47917843) +\n0.5173945 + 0.46866685 + 0.47825924 = 0.46187719\nCorrectness of final answer(R5), R5 = I( ˆA=\nAgold). This reward checks if the final answer\nmatches the gold answer. A negative example of\nthe same is given below.\n3686\nQgold:\nThree friends Alan, Roger and Peter attempt to answer a\nquestion on an exam. Alan randomly guesses the answer,\ngiving him a 1\n5 probability of guessing correctly. Roger\ncheats by looking at the paper of the student in front of him,\ngiving him a 2\n3 probability of answering correctly. And Peter\ndutifully performs the calculations, then marks the answer,\ngiving him a 5\n6 probability of a correct answer. What is the\nprobability that the question is answered correctly, but not\nvia cheating?\nSgold :\nProb(Alan) = 1\n5\nProb(Roger) without cheating = 2\n3 −1 =1\n3\nProb(Peter) = 5\n6\nTotal Probability = 1\n5 ·1\n3 ·5\n6 = 1\n18\nSmodel :\nAlan has a 1\n5 chance of getting the answer correct.\nRoger has a 2\n3 chance of getting the answer correct.\nPeter has a 5\n6 chance of getting the answer correct.\nThe probability that the question is answered correctly is\n1\n5 + 2\n3 + 5\n6 = 13\n12 .\nThe probability that the question is answered correctly, but\nnot via cheating is1 −(1\n5 + 2\n3 + 5\n6 ) = 1−13\n12 = −1\n12\nAgold: 1\n18\nAmodel: −1\n12\nR5 = 0\nC Hyperparameter Selection\nWe performed hyperparameter tuning on a subset\nof 250 examples from the training dataset. Based\non the improvement in accuracy and compute re-\nsources, we converged at the following values.\nFor the supervised fine-tuning stage, we used\nLoRA r= 16, LoRA α= 32, LoRA droput = 0.05.\nFor the RLMF stage, we finetuned the last 3 layers\nof LoRA adapters, using a batch size of 8, gradient\naccumulation steps=8, init kl coef=0.01, target=4.\nFor inference, we used the following generation\nparameters: temperature = 0.95, top p=0.18, pad\ntoken id = 0, do sample = False, number of beams\n= 1, maxi length = 2048.\nAll the models were implemented using Hug-\ngingface with PyTorch, and loaded in int_8 preci-\nsion. For solver-LLaMA 13B model, we instruc-\ntion finetuned the LLaMA13B model using the\nAlpaca dataset by stanford (Taori et al., 2023).\nFor solver-LLaMA 33 billion model, we used an\ninstruction-finetuned version of LLAMA-33 billion\nfrom Huggingface. To implement the PPO algo-\nrithm, we used the TRL library. We used a single\nNVIDIA A100 (80GB) GPU for all training and\ninference purposes.\nD Case Study on LLAMA Models\nSimilar to Figure 2, we performed example-wise\ncase study on the answering process of the LLaMA\nsolvers (13 billion and 33 billion models) with dif-\nferent prompting methods (see Figures 3 and 4).\nThe example problem was selected from the MATH\ndataset. As we can see, with relatively small scale\nsolvers like LLaMA, Least-to-most fails to add any\nimprovement over basic CoT prompting. DaSLaM\nis able to guide the model to the correct answer in\nboth cases.\n3687\nAsnranges over the positiveintegers, what is the maximumpossible value that the greatestcommon divisor of80n+ 49and18n+ 11can take?\nWhat is the greatest common divisor of8n+ 5and2n+ 1?The greatest common divisor of8n+ 5and2n+ 1is the greatest common divisor of8n+ 5and8n+ 5−2n−1 = 6n+ 4.The greatest common divisor of8n+ 5and2n+ 1is the greatest common divisor of8n+ 5and6n+ 4−2n−1 = 4n+.The greatest common divisor of8n+ 5and2n+ 1is the greatest common divisor of8n+ 5and4n+ 3−2n−1 = 2n+ 2.The greatest common divisor of8n+ 5and2n+ 1is the greatest common divisor of8n+ 5and2n+ 2−2n−1 = 1.Hence, The greatest common divisor of8n+ 5and2n+ 1is 1What is the greatest common divisor of18n+ 11and8n+ 5?The greatest common divisor of18n+ 11and8n+ 5is the greatest common divisor of8n+ 5and18n+ 11−8n−5 = 10n+ 6 =greatest common divisor of8n+ 5and10n+ 6−8n−5 = 2n+ 1 = 1What is the greatest common divisor of80n+ 49and18n+ 11?The greatest common divisor of80n+ 49and18n+ 11is the greatest common divisor of18n+ 11and80n+ 49−18n−11 = 62n+ 38is greatest common divisor of18n+ 11and62n+38−18n−11 = 44n+27is greatest common divisor of18n+11and44n+27−18n−11 =26n+ 16is greatest common divisor of18n+ 11and26n+ 16−18n−11 = 8n+ 5 = 1\nWhat is the greatest common divisor of80n+49and18n+11?The greatest common divisor of two numbers is the largestnumber that divides both of them. Therefore, the greatestcommon divisor of80n+49and18n+11is the largest numberthat divides both of them. The largest number that dividesboth of them is the largest number that divides80n+ 49andthe largest number that divides18n+ 11. The largest numberthat divides80n+ 49is80n+ 49itself. The largest numberthat divides18n+ 11is18n+ 11itself. Therefore, the greatestcommon divisor of80n+ 49and18n+ 11is80n+ 49and18n+ 11themselves.\nThe greatest common divisor of two numbers is the largestnumber that divides both of them. Therefore, the greatestcommon divisor of80n+49and18n+11is the largest numberthat divides both of them. The largest number that dividesboth of them is the largest number that divides80n+ 49andthe largest number that divides18n+ 11. The largest numberthat divides80n+ 49is80n+ 49itself. The largest numberthat divides18n+ 11is18n+ 11itself. Therefore, the greatestcommon divisor of80n+ 49and18n+ 11is80n+ 49and18n+ 11themselves.\nDaSLaM\nLeast to Most\nCOT\nFigure 3: A case study on LLAMA-13B.\nThe product of three consec-\nutive integers is 120. That\nproduct divided by the mean\nof the three integers is 24.\nWhat is the largest of the\nthree consecutive integers?\nWhat is the product of three consecutive integers?\nThe product of three consecutive integers is 120.\nWhat is the mean of the three integers?\nThe product of three consecutive integers is 120.\nThat product divided by the mean of the three integers is 24.\nSo, 120/mean = 24\nMean = 120/24 = 5\nWhat is the largest of the three consecutive integers?\nThe product of three consecutive integers is 120.\nThat product divided by the mean of the three integers is 24.\nSo, 120/mean = 24\nMean = 120/24 = 5\nSo, the three consecutive integers are 4, 5, 6.\nThe largest of the three consecutive integers is 6\nThe product of three consecutive integers is 120. That product\ndivided by the mean of the three integers is 24. The mean of\nthe three integers is the sum of the three integers divided by\n3. The sum of the three integers is 120. Therefore, the mean\nof the three integers is 40. The product of the three integers is\n120, so the product of the three integers divided by the mean\nof the three integers is 3. This means that the largest of the\nthree consecutive integers is 40.\nThe product of three consecutive integers is 120.\nThat product divided by the mean of the three integers is 24.\nThe mean of the three integers is the sum of the three integers\ndivided by 3.\nThe sum of the three integers is 120.\nTherefore, the mean of the three integers is 40.\nThe largest of the three consecutive integers is 40 + 24 = 64.\nDaSLaM\nLeast to Most\nCOT\nFigure 4: A case study on LLaMA-33B as solver model.\n3688\nE Failure cases with DaSLaM\nE.1 Case I\nQuestion A train running at a speed of 100\nmiles/hour, takes 10 hours to reach its destination.\nAfter covering quarter of the distance, it starts\nraining and the train has to be slowed to speed of\n75 miles/hour. What is the total journey duration?\nA)10 B)11.5 C)12.5 D)13.5 E)15\nInitial Answer The train covers the first quarter\nof the distance at a speed of 100 miles/hour.\nSo, the time taken to cover the first quarter of the\ndistance = (1/4) x 10 hours = 2.5 hours\nThe train covers the remaining three quarters of\nthe distance at a speed of 75 miles/hour.\nSo, the time taken to cover the remaining three\nquarters of the distance = (3/4) x 10 hours = 7.5\nhours\nTherefore, the total journey duration = 2.5 + 7.5 =\n10 hours\nSub problem 1 What is the total distance to be\ncovered?\nSub Answer 1 Let the total distance to be\ncovered be d.\nTime taken to cover the first quarter of the distance\n= (1/4)d/100 = 10/400 hours\nTime taken to cover the remaining distance =\n(3/4)d/75 = 300/400 hours\nTotal time taken = 10/400 + 300/400 = 10/4 hours\n= 2.5 hours\nSince the train takes 10 hours to reach its destina-\ntion,\nTotal distance to be covered = 10 x 100 = 1000\nmiles\nSub problem 2\nWhat is the speed of the train?\nSub Answer 2\nAfter covering quarter of the distance, the train has\nto be slowed to speed of 75 miles/hour.\nLet the remaining distance be x.\nTherefore, x = d - (d/4) = 1000 - (1000/4) = 750\nmiles.\nThe time taken to cover the remaining distance x\n= 750 miles at a speed of 75 miles/hour = (750/75)\nhours = 10 hours.\nTherefore, the speed of the train = x/time taken\n= 750/10 = 75 miles/hour.\nSub problem 3 What is the time taken to cover\nthe distance?\nSub Answer 3\nTime taken to cover the first quarter of the distance\n= (d/4)/100 hours = d/400 hours\nTime taken to cover the remaining distance =\n(3d/4)/75 hours = d/300 hours\nTotal time taken = d/400 + d/300 = (5d +\n4d)/1200 hours = 9d/1200 hours.\nPutting d=1000,\nFinal answer: The time taken to cover the dis-\ntance is 9 x 1000/1200 hours.\nSub problem 4 What is the total duration of the\njourney?\nSub Answer 4 Time taken to cover the first quarter\nof the distance = (d/4)/100 hours = d/400 hours\nTime taken to cover the remaining three quarters\nof the distance = (3d/4)/75 hours = 4d/300 hours\nTotal time taken = d/400 + 4d/300 = (5d +\n4d)/(400 + 300) = 9d/700 hours\nTotal duration of the journey = 9d/700 hours = 9\nhours and (d/700) hours = 9 hours and (100/700)\nhours = 9 hours and 14.29 minutes\nFinal Answer Step 1: The train covers the first\nquarter of the journey at a speed of 100 miles/hour.\nStep 2: The time taken to cover the first quarter\nof the journey is given by: Time = Distance/Speed\nTime = (1/4) x 100/100 Time = 1/4 hour Step 3:\nThe train covers the remaining three quarters of the\njourney at a speed of 75 miles/hour. Step 4: The\ntime taken to cover the remaining three quarters\nof the journey is given by: Time = Distance/Speed\nTime = (3/4) x 75/100 Time = 3/4 hour Step 5: The\ntotal duration of the journey is the sum of the time\ntaken to cover the first quarter and the remaining\nthree quarters of the journey. Total Time = 1/4 +\n3/4 Total Time = 4/4 Total Time = 1 hour Therefore,\nthe total duration of the journey is 1 hour.\n3689\nE.2 Case II\nQuestion Consider there is an staircase elevator\nand you are coming down. If you walk 20 steps\nand stop, then you reach bottom in 10 minutes. If\nyou walk 10 steps and stop, you reach to the ground\nin 20 minutes. What is the speed of the elevator?\nA)1 step/minute B)2 step/minute C)3 step/minute\nD)4 step/minute E)None of the above\nInitial Answer .\nSub Question 1 What is the total number of steps\nin the elevator?\n.\nSub Question 2 What is the speed of the elevator?\n.\nSub Question 3\nWhat is the time taken to cover 20 steps?\nSub Answer 3\nTime taken to cover 20 steps is 10 minutes\nSub Question 4\nWhat is the time taken to cover 10 steps?\nSub Answer 4\nTime taken to cover 10 steps is 20 minutes\nFinal Answer Let the speed of the elevator be x.\nTime taken to reach the bottom when walking 20\nsteps and stopping = 10 minutes\nTime taken to reach the bottom when walking 10\nsteps and stopping = 20 minutes\nTherefore,\n20x = 10\n10x = 20\nx = 2\nTherefore, the speed of the elevator is 2 step-\ns/minute.\nE.3 Case III\nQuestion Let −π\n6 <θ< −π\n12 . Suppose α1 and β1\nare the roots of the equation x2 −2xsec θ+ 1 = 0\nand α2 and β2 are the roots of the equation x2 +\n2xtan θ−1 = 0. If α1 > β1 and α2 > β2, then\nα1 + β2 equals\n(A) 2(sec θ−tan θ)\n(B) 2 secθ\n(C) −2 tanθ\n(D) 0\nInitial Answer\nWe can solve the equations for alpha1 and alpha2\nto get\nalpha1 = secθ+ tanθ\nalpha2 = −sec θ+ tanθ\nSince α1β1 and α2β2, we have\nsec θ+ tanθ> −sec θ\nsec θ+ tanθ> −tan θ\nAdding the two equations, we get\n2 secθ> 0\nTherefore, α1β2 = 2 secθ.\nThe correct answer is (B) 2 secθ.\nWhat is the equation of the line\nx2 −2xsec θ+ 1 = 0?\nGiven, −π\n6 < θ < −π\n12 We need to find the\nequation of the line x2 −2xsec θ+1 = 0We know\nthat the equation of a line is given by y= mx+ c\nNow, let us substitute the given equation in the\nequation of a line. x2 −2xsec θ+ 1 = 0\ny= mx+ c\nx2 −2xsec θ+ 1 =mx+ c\nmx+ c−x2 + 2xsec θ−1 = 0\nmx−x2 + 2xsec θ= 1\nComparing the coefficients of xon both sides,\nm= 2 secθ\nComparing the constant terms on both sides,\nc= 1\nTherefore, the equation of the line\nx2 −2xsec θ+ 1 = 0is given by\ny= 2 secθx+ 1\nHence, the answer is y= 2 secθx+ 1.\nWhat is the equation of the line\nx2 + 2xtan θ−1 = 0?\nGiven, −π\n6 <θ< −π\n12\nWe know that,\nsec θ= 1\ncos θ\ntan θ= sin θ\ncos θ\nTherefore,\nx2 −2xsec θ+ 1 =x2 −2x 1\ncos θ + 1 = 0\nx2 + 2xtan θ−1 =x2 + 2xsin θ\ncos θ −1 = 0\n3690\nHence, the equation of the linex2 +2xtan θ−1 =\n0 is x2 + 2xsin θ\ncos θ −1 = 0.\nFinal Answer: x2 + 2xsin θ\ncos θ −1 = 0\nWhat is the equation of the line\nx2 −2xsec θ + 1 = 0 minus the equation\nof the line x2 + 2xtan θ−1 = 0?\nThe equation of the line x2 −2xsec θ+ 1 = 0is\ngiven by x2 −2xsec θ+ 1 = 0\nThe equation of the line x2 + 2xtan θ−1 = 0is\ngiven by x2 + 2xtan θ−1 = 0\nSubtracting the two equations, we get\nx2 −2xsec θ+ 1−x2 −2xtan θ+ 1 = 0\nSimplifying the equation, we get\n−2x(sec θ+ tanθ) = 0\nTherefore, the equation of the line\nx2−2xsec θ+1 = 0minus the equation of the line\nx2 + 2xtan θ−1 = 0is −2x(sec θ+ tanθ) = 0\nHence, the answer is −2x(sec θ+ tanθ) = 0.\nWhat is the equation of the line\nx2 −2xsec θ + 1 = 0 plus the equation\nof the line x2\nGiven, −π\n6 <θ< −π\n12\nα1 and β1 are the roots of the equation\nx2 −2xsec θ+ 1 = 0\nα2 and β2 are the roots of the equation\nx2 + 2xtan θ−1 = 0\nα1 >β1 and α2 >β2\nWe need to find the value of α1 + β2\nLet us consider the equation x2 −2xsec θ+ 1 = 0\nα1 and β1 are the roots of the equation.\nTherefore,\nα1 + β1 = −2 secθ\nLet us consider the equation x2 + 2xtan θ−1 = 0\nα2 and β2 are the roots of the equation.\nTherefore,\nα2 + β2 = 2 tanθ\nNow,\nα1 + β2 = α1 + β2 + α2 + β2 −α2 −β2\n= α1 + β1 −α2 −β2\n= −2 secθ+ 2 tanθ\n= 2(secθ−tan θ)\nHence, the answer is (A) 2(sec θ−tan θ).\nFinal Answer Given,\n−π\n6 <θ< −π\n12\nα1 and β1 are the roots of the equation x2 −\n2xsec θ+ 1 = 0\nα2 and β2 are the roots of the equation x2 +\n2xtan θ−1 = 0\nα1 >β1 and α2 >β2\nWe need to find the value of α1 + β2\nLet us consider the equation x2 −2xsec θ+ 1 = 0\nα1 and β1 are the roots of the equation.\nTherefore,\nα1 + β1 = −2 secθ\nLet us consider the equation x2 + 2xtan θ−1 = 0\nα2 and β2 are the roots of the equation.\nTherefore,\nα2 + β2 = 2 tanθ\nNow,\nα1 + β2 = α1 + β2 + α2 + β2 −α2 −β2\n= α1 + β1 −α2 −β2\n= −2 secθ+ 2 tanθ\n= 2(secθ−tan θ)\nHence, the answer is (A) 2(sec θ−tan θ).\n3691"
}