{
  "title": "AMOM: Adaptive Masking over Masking for Conditional Masked Language Model",
  "url": "https://openalex.org/W4382317864",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2551593070",
      "name": "Yisheng Xiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226774090",
      "name": "Ruiyang Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A214709701",
      "name": "Lijun Wu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2126452540",
      "name": "Juntao Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1719067107",
      "name": "Tao Qin",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3205478007",
      "name": "Tie-Yan Liu",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2551593070",
      "name": "Yisheng Xiao",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2226774090",
      "name": "Ruiyang Xu",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2126452540",
      "name": "Juntao Li",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A3205478007",
      "name": "Tie-Yan Liu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2005452123",
      "name": "Min Zhang",
      "affiliations": [
        "Soochow University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2148190602",
    "https://openalex.org/W3113715281",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3168817639",
    "https://openalex.org/W4226069095",
    "https://openalex.org/W3214532454",
    "https://openalex.org/W2937808806",
    "https://openalex.org/W6746208923",
    "https://openalex.org/W3114620933",
    "https://openalex.org/W6762471145",
    "https://openalex.org/W3035289598",
    "https://openalex.org/W3093345276",
    "https://openalex.org/W3094341828",
    "https://openalex.org/W3206689743",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W3125507956",
    "https://openalex.org/W6719020539",
    "https://openalex.org/W2789163545",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W4221166896",
    "https://openalex.org/W6683854473",
    "https://openalex.org/W4287328846",
    "https://openalex.org/W6780674358",
    "https://openalex.org/W3015162217",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W3054488230",
    "https://openalex.org/W6728729416",
    "https://openalex.org/W3016440711",
    "https://openalex.org/W4225598930",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W6764548015",
    "https://openalex.org/W3175955332",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2986772082",
    "https://openalex.org/W3206889547",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2949644922",
    "https://openalex.org/W3000840023",
    "https://openalex.org/W2995999067",
    "https://openalex.org/W3022049116",
    "https://openalex.org/W4303874710",
    "https://openalex.org/W2160204597",
    "https://openalex.org/W2964093309",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385572691",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3174864715",
    "https://openalex.org/W3174255604",
    "https://openalex.org/W4281728254",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3135335819",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W4224280049",
    "https://openalex.org/W4287854594",
    "https://openalex.org/W3037611961",
    "https://openalex.org/W3081352735",
    "https://openalex.org/W3177172118",
    "https://openalex.org/W3126267552",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W3089659770",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W3211909097",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4280567072",
    "https://openalex.org/W4377079846",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287665582",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W3034730263",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W4312257461",
    "https://openalex.org/W3175665465",
    "https://openalex.org/W3168577192"
  ],
  "abstract": "Transformer-based autoregressive (AR) methods have achieved appealing performance for varied sequence-to-sequence generation tasks, e.g., neural machine translation, summarization, and code generation, but suffer from low inference efficiency. To speed up the inference stage, many non-autoregressive (NAR) strategies have been proposed in the past few years. Among them, the conditional masked language model (CMLM) is one of the most versatile frameworks, as it can support many different sequence generation scenarios and achieve very competitive performance on these tasks. In this paper, we further introduce a simple yet effective adaptive masking over masking strategy to enhance the refinement capability of the decoder and make the encoder optimization easier. Experiments on 3 different tasks (neural machine translation, summarization, and code generation) with 15 datasets in total confirm that our proposed simple method achieves significant performance improvement over the strong CMLM model. Surprisingly, our proposed model yields state-of-the-art performance on neural machine translation (34.62 BLEU on WMT16 EN to RO, 34.82 BLEU on WMT16 RO to EN, and 34.84 BLEU on IWSLT De to En) and even better performance than the AR Transformer on 7 benchmark datasets with at least 2.2x speedup. Our code is available at GitHub.",
  "full_text": "AMOM: Adaptive Masking over Masking for\nConditional Masked Language Model\nYisheng Xiao1, Ruiyang Xu1, Lijun Wu2, Juntao Li1*, Tao Qin2, Tie-Yan Liu2, Min Zhang1\n1Institute of Computer Science and Technology, Soochow University\n2Microsoft Research Asia\n{ysxiaoo, ryxu1}@stu.suda.edu.cn, {ljt, minzhang}@suda.edu.cn, {lijuwu, taoqin, tyliu}@microsoft.com\nAbstract\nTransformer-based autoregressive (AR) methods have\nachieved appealing performance for varied sequence-to-\nsequence generation tasks, e.g., neural machine translation,\nsummarization, and code generation, but suffer from low\ninference efficiency. To speed up the inference stage, many\nnon-autoregressive (NAR) strategies have been proposed in\nthe past few years. Among them, the conditional masked\nlanguage model (CMLM) is one of the most versatile frame-\nworks, as it can support many different sequence generation\nscenarios and achieve very competitive performance on these\ntasks. In this paper, we further introduce a simple yet effec-\ntive adaptive masking over masking strategy to enhance the\nrefinement capability of the decoder and make the encoder\noptimization easier. Experiments on 3 different tasks (neural\nmachine translation, summarization, and code generation)\nwith 15 datasets in total confirm that our proposed simple\nmethod achieves significant performance improvement\nover the strong CMLM model. Surprisingly, our proposed\nmodel yields state-of-the-art performance on neural machine\ntranslation (34.62 BLEU on WMT16 EN→RO, 34.82 BLEU\non WMT16 RO→EN, and 34.84 BLEU on IWSLT De→En)\nand even better performance than the AR Transformer on 7\nbenchmark datasets with at least 2.2× speedup. Our code is\navailable at GitHub1.\nIntroduction\nTransformer-based models (Vaswani et al. 2017) have been\nproven effective for various sequence to sequence gener-\nation tasks, such as machine translation (Wu et al. 2019;\nLiang et al. 2021), text summarization (Savelieva, Au-\nYeung, and Ramani 2020; Elsaid et al. 2022), dialogue sys-\ntems (Zhang et al. 2020; Ma et al. 2020), code genera-\ntion (Wang et al. 2020), etc. Despite the excellent perfor-\nmance of Transformer-based models, they usually adopt the\nautoregressive (AR) decoding paradigm in which the decod-\ning of a target sequence is decomposed into multi-step pre-\ndictions in left-to-right order, i.e., the next prediction is con-\nditioned on the previously generated part. Such an attribute\nincreases the inference time cost linearly with the target se-\nquence length, which is time-consuming for long sequences.\n*Corresponding Author\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n1https://github.com/amom-nar/AMOM\nTo alleviate this problem, many recent works explore non-\nautoregressive (NAR) methods (Gu et al. 2018; Qian et al.\n2021; Xiao et al. 2022) to predict a target sequence in paral-\nlel, which can dramatically increase inference speed. As the\ncost of increasing decoding speed, NAR models remove the\ninternal dependency of the target sequence and perform each\ndecoding prediction depending entirely upon the source/in-\nput sequence. Inevitably, the generation quality of NAR\nmethods falls behind their AR counterparts without target-\nside information in decoding (Gu et al. 2018).\nTo achieve a better trade-off between inference speedup\nand generation quality, the conditional masked language\nmodel (CMLM) (Ghazvininejad et al. 2019) has been pro-\nposed and has already become one of the most competitive\nand widely-used NAR frameworks, which exploits an iter-\native mask-predict decoding strategy. In the training stage,\nCMLM leverages a masked language model objective to\ngenerate the masked subset of the target sequence in par-\nallel conditioned on the source input and unmasked part in\ntarget sequence. During inference, CMLM first generates the\nwhole target sequence in parallel (the first iteration) and then\niteratively masks and predicts low-confidence tokens. Based\non CMLM, many recent works have achieved performance\nimprovements with advanced enhancement strategies from\ndifferent perspectives, e.g., improving the inference strat-\negy (Kasai et al. 2020a; Geng, Feng, and Qin 2021), benefit-\ning from the AT counterpart (Hao et al. 2021), training with\nbetter criterion (Marjan et al. 2020; Du, Tu, and Jiang 2021),\nintroducing self-correction mechanism (Huang, Perez, and\nV olkovs 2022) and pre-training (Li et al. 2022b).\nIn this paper, we further introduce a simple yet very effec-\ntive strategy to enhance the refinement capability of CMLM\nwithout changing the model structure and the inference al-\ngorithm, named adaptive masking over masking (AMOM).\nConcretely, we present two adaptive masking operations for\nboth the source and target sequence based on the conven-\ntional one-time masking in CMLM. The masking operation\nfor the source sequence can make the encoder optimization\neasier by adaptively masking a proportion of tokens based on\nthe masked target sequence. In contrast, the vanilla CMLM\nconstructs multiple masked target sequences for each source\nsequence in model training, making the encoder difficult to\nconverge (Guo, Xu, and Chen 2020). Another potential merit\nof the source-side masking is to improve the stability of the\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n13789\nCMLM model against different decoder inputs by prevent-\ning the internal co-adaptation (akin to dropout (Hinton et al.\n2012)). Moreover, cooperating it with the masking condi-\ntion of the target sentence can better improve the ability\nrather than fixed masking. Notice that JM-NAT (Guo, Xu,\nand Chen 2020) also explores the source-side masking op-\neration but has a clear difference from our strategy. It in-\ntroduces a BERT-like masked language model task on the\nencoder side to enhance the encoder training, whereas our\nadaptive strategy does not introduce any extra task and can\ndynamically capture target-side information. The target-side\nadaptive masking operation is presented to enhance the re-\nfinement process of CMLM, motivated by the masking ra-\ntio changes of the target sequence in different inference it-\nerations, which cannot be captured by the one-time mask-\ning. Simultaneously, unlike the adaptive target-side masking\nstrategy in GLAT (Qian et al. 2021) to achieve curriculum\nlearning, we design the masking strategy specially to en-\ncourage the model to perform steadily and conduct refine-\nments effectively. We focus on the promotion of each itera-\ntion rather than only enhancing the first iteration in GLAT.\nMore comparisons between our strategy and the counter-\nparts used in GLAT can be found in the experiments part.\nThough AMOM is simple, i.e., only two extra masking\noperations in model training, we find it is surprisingly effec-\ntive on different sequence generation tasks, including neu-\nral machine translation, summarization, and code genera-\ntion (15 datasets in total). It achieves state-of-the-art per-\nformance on multiple datasets based on the vanilla CMLM,\ne.g., 34.62 BLEU score on WMT16 EN→RO, 34.82 BLEU\non WMT16 RO→EN, and34.84 BLEU on IWSLT De→En.\nAMOM even performs better than the strong autoregressive\nTransformer on 7 datasets with at least 2.2× speedup.\nMethodology\nOur proposed adaptive masking over masking (AMOM)\nstrategy is a simple yet effective add-on for the conditional\nmasked language model (CMLM) (Ghazvininejad et al.\n2019) training, which comprises two adaptive masking op-\nerations for the encoder and decoder, respectively, to en-\nhance the encoder training and the refinement capability of\nCMLM. Specifically, we adopt the same encoder-decoder\narchitecture as the CMLM.\nConditional Masked Language Model\nA conditional masked language model feeds a source se-\nquence X to the encoder and a target sequence in which\npart of the tokens are masked by replacing them with the\n[mask] token to the decoder. The training objective of\nCMLM is to learn to predict the masked tokens Ymask in\nparallel given X and the unmasked tokens Yobs in the rest\npart of the target sequence, based on the assumption that all\ntarget tokens in Ymask are independent of each other, i.e.,\nthe prediction of each Ymask token is merely conditioned on\nX and Yobs. To eliminate the particularity ofYmask, CMLM\nsamples a different number of tokens each time as Ymask\nfrom the uniformly distributed number between one to the\ntarget length during training, rather than a fixed proportion\nof the target sequence. The training objective of CMLM is\nto maximize:\nLCMLM =\nX\nyt∈Ymask\nlog P(yt|Yobs, X; θ), (1)\nwhere θ denotes the trainable parameters of CMLM. Unlike\nAR methods that can automatically decide the decoding end\nby generating a special [EOS] (end of a sentence) token,\ntypical NAR methods require learning to predict the target\nlength in advance. CMLM adds a special token [LENGTH]\n(akin to the [cls] token in BERT) into its encoder to pre-\ndict the target length. During inference, given the input X\nand the predicted target length, CMLM executesk iterations\nof mask-predict operation (Ghazvininejad et al. 2019) to cre-\nate the final target sequence. At the first iteration, the CMLM\npredicts the entire Y in parallel fully depending onX. In the\nnext k − 1 iterations, CMLM repeatedly masks a specific\nnumber of low-confidence tokens generated from the last it-\neration and regenerates them in parallel.\nAdaptive X Masking\nBasically, CMLM leverages an encoder-decoder structure to\nachieve sequence to sequence generation, which requires the\nmutual cooperation between encoder and decoder. However,\nduring model training, each X will be paired with multiple\nYmask due to the uniform masking strategy of CMLM, mak-\ning the encoder optimization much harder than the decoder.\nGuo, Xu, and Chen also empirically prove that the conver-\ngence speed of the encoder is significant lower than the de-\ncoder. Another drawback of conditioning differentYmask on\nthe same X is the internal co-adaptation ofX, i.e., each pre-\ndiction of Ymask relies on the whole input sequence, making\nthe decoder less focused on the changes of decoder inputs.\nTo enhance the encoder training and address the above-\nmentioned flaws, we propose a simple yet effective adap-\ntive masking for inputX. Unlike previous research, our pro-\nposed adaptive X masking is included in the sequence to\nsequence generation task, and the number of masked tokens\nis coordinated with the number of masked Y tokens. More\nconcretely, given a training pair(X, Y) in CMLM, where Y\nwill be divided into Yobs and Ymask, the masking ratio α of\nY can be calculated as Nmask\nNobs+Nmask\n. Nobs and Nmask denote\nthe number of tokens in Yobs and Ymask, respectively. Then,\nwe introduce a mapping function φ(·) to decide the mask-\ning ratio of X based on the masking ratio in Y , i.e., we will\nrandomly mask φ(α) ∗ LX tokens in the source sequence,\nwhere LX denotes the length of the source sequence. Then\nthe training loss of CMLM with adaptive X masking can be\ncomputed as:\nLcmlm = −\nX\nyt∈Ymask\nlog P(yt|Yobs, ˆX; θ), (2)\nwhere ˆX refers to the input sequence with φ(α) ∗ LX to-\nkens being masked. We introduce different variations of φ\nin Table 5 and compare their performance.\nAdaptive Y Masking\nAs mentioned above, the superior performance of CMLM-\nbased methods comes from the iterative refinement process,\n13790\ni.e., the previously generated target sequence draft is re-\npeatedly polished by regenerating a specific number of low-\nconfidence tokens in the subsequent iterations. In seeing the\nself-correction nature of the refinement process, many recent\nworks introduce a correction objective in CMLM training to\nenhance its refinement capability e.g., SMART (Ghazvinine-\njad et al. 2020), CMLMC (Huang, Perez, and V olkovs 2022).\nUnlike these works that introduce extra training objectives\nand optimize the inference process of CMLM, we present\nan ultra-simple yet effective adaptive masking operation for\nY in model training without any change to the CMLM infer-\nence2. Our strategy is motivated by the quality improvement\nof predicted tokens along with the refinement iterations, i.e.,\nthe proportion of low-confidence tokens (for regeneration\nin each iteration) from Ymask will gradually decrease along\nwith the refinement iterations, resulting in a varied masking\nratio between Ymask and Yobs in the refinement process.\nTo capture the masking ratio changes in CMLM infer-\nence, we add another masking operation (adaptive Y mask-\ning) upon the one-time masking in the vanilla CMLM\nmodel. Specifically, for each training pair (X, Y), Y is di-\nvided into Yobs and Ymask. CMLM generates the masked\ntokens based on Yobs and X, where the generated result\nis denoted as ˆYmask to distinguish with Ymask. Then, we\ncompute the correctness ratio of predicted tokens in ˆYmask\nby comparing with target tokens in Ymask, formulated as\nβ = |ˆYmask=Ymask|\nNmask\n. Similar to adaptive X masking, we in-\ntroduce another mapping function ψ(·) to decide the mask-\ning proportion of ˆYmask and Yobs tokens. Different types of\nmapping function ψ(·) are experimented in Analysis, and\nmore details are given in Appendix. We assign a masking\nprobability of 1 − ψ(β) to each token in ˆYmask and a mask-\ning probability of ψ(β) to each token inYobs. As a result, the\nnewly masked tokens in the second time denoteY ′mask, and\nthe rest tokens will serve as a newY ′\nobs, for the next iteration.\nThe training loss of the new subset Y ′mask is computed the\nsame as the first-time masking in CMLM, formulated as:\nLaday = −\nX\nyt∈Y ′\nmask\nlog P(yt|Y ′\nobs, ˆX′; θ), (3)\nwhere ˆX′ refers to the input sequence with an adaptive\nmasking ratio of Y ′\nmask being masked.\nAMOM Training and Inference\nWe simply adopt two adaptive masking strategies based on\nthe original CMLM training process. The training objective\nof our proposed adaptive masking over masking (AMOM)\nis the simple combination of Lcmlm and Laday mentioned in\nEquation 2 and 3, formulated as:\nLAMOM = Lcmlm + Laday, (4)\nAs for inference, we utilize the same decoding strategy with\nCMLM. As mentioned above, we utilize a special token\n[LENGTH] in the encoder to predict the target length in\nadvance. Inevitably, there is a deviation between the pre-\ndicted length and the ground-truth length. Thus, we also\n2More comparisons are given in Appendix.\nconsider selecting the translation with the highest proba-\nbility with different target lengths to obtain better results.\nGiven the target length LY and the total number of refine-\nment iterations T, the model performs generation based on\nthe fully masked decoder input (i.e., empty Yobs) at the first\niteration. In the next T − 1 iterations, a specific number of\nlow-confidence tokens will be masked and re-generated. The\nnumber of masked tokens in each iteration can be computed\nas n = T−t\nT ∗ LY , where t denotes the current iteration\nnumber. Given the number of masked tokens, the model will\nselect them based on the output probability of each token,\nwhere tokens with the lowest probability will be masked,\nand their scores will be updated in the next iteration.\nExperiments\nTo evaluate our AMOM method and show its universal im-\npact on various sequence generation tasks, we conduct ex-\nperiments on natural machine translation, summarization,\nand code generation tasks.\nDatasets\nFor machine translation, we conduct experiments both on\nIWSLT and WMT datasets, which are widely used for\nNMT tasks. The datasets from IWSLT competitions con-\ntain 4 language pairs (170k pairs), see details in Table 2.\nFor WMT datasets, we choose two language pairs which\nare widely used in non-autoregressive machine translation\ntask, WMT16 English→Roman (0.6M pairs) and WMT14\nEnglish→German (4.5M pairs) tasks. Following previous\nworks on non-autoregressive machine translation, we ap-\nply sequence-level knowledge distillation (Kim and Rush\n2016; Zhou, Gu, and Neubig 2019) for all datasets. For\nWMT datasets, we use the same distilled data as the same as\nCMLM (Ghazvininejad et al. 2019). Then, we amalgamate\nthe raw and distilled data as our final training data, follow-\ning (Ding et al. 2020). For all IWSLT datasets, we train the\nteacher model with Transformersmall, and use the generated\nresults as the distilled data. Then, we train our AMOM on\ndistilled data. For summarization task, we use the XSUM\ndataset (Narayan, Cohen, and Lapata 2018) which contains\n204,045/11,332/11,334 online articles and single sentence\nsummary pairs from the British Broadcasting Corporation\nfor training/validation/test. We preprocess the dataset, fol-\nlowing (Lewis et al. 2020). For code generation task, we use\nPy150 dataset (Raychev, Bielik, and Vechev 2016) and use\nGitHub-Java dataset (Allamanis and Sutton 2013). We use\nthe Python official library tokenizer 3 and Javalang4 to split\nthe datasets into lines of codes. Then we use a sliding con-\ntext window to adopt 10-lines of code tokens as the source\nsentences and the next 4-lines as the target sentences. We\nfollow (Wang et al. 2020) to process the dataset to transform\nsome special tokens as [str] token (without bpe).\nSettings\nAll experiments are done using the Fairseq library (Ott\net al. 2019). Following previous settings (Ghazvininejad\n3https://docs.python.org/3/library/tokenize.html\n4https://github.com/c2nes/javalang\n13791\nModel Iterations WMT16 WMT14 SpeedupEN→RO RO→EN EN→DE DE→EN\nAR Transformer (Vaswani et al. 2017)* N 34.23 34.28 28.41 32.28 1.0x\nFull NAT\nNAT-FT\n(Gu et al. 2018) 1 27.29 29.06 17.69 21.47 15.6×\nAXE (Marjan et al. 2020) 1 31.54 30.75 23.53 - 15.3x\nOAXE (Du, Tu, and Jiang 2021) 1 33.3 32.4 26.1 - 15.3x\nGLAT (Qian et al. 2021) 1 32.87 33.51 26.55 31.02 15.3x\nFullyNAT (Gu and Kong 2021) 1 33.71 34.16 27.20 31.39 16.8x\nDSLP (Huang et al. 2022a) 1 34.17 34.60 27.02 31.61 14.8x\nDAT (Huang et al. 2022b) 1 - - 27.49 31.37 13.9x\nIterative\nRefine-NAT (Lee,\nMansimov, and Cho 2018) 10 27.11 30.19 21.61 25.48 1.5x\nLevenshteinNAR (Gu, Wang, and Zhao 2019) >7 33.02 - 27.73 - 4.0x\nDisCo (Kasai et al. 2020a) 3.1 33.25 33.22 27.34 - 3.5x\nCMLM-Based\nCMLM (Ghazvininejad et al.\n2019)* 10 33.46 33.83 27.21 31.03 2.3x\nSMART (Ghazvininejad et al. 2020) 10 33.85 33.53 27.65 31.27 1.7x\nJM-NAT (Guo, Xu, and Chen 2020) 10 33.52 33.72 27.69 32.24 -\nRDP (Ding et al. 2020) 10 33.7 - 27.8 - 1.5x\nLFR (Ding et al. 2021) 10 - 33.9 27.8 - 1.5x\nMvSR-NAT (Xie, Li, and Hu 2021) 10 33.38 33.56 27.39 31.18 3.8x\nCORR (Huang, Perez, and V olkovs 2022) 10 34.31 34.08 28.19 31.31 -\nCMLMC (Huang, Perez, and V olkovs 2022) 10 34.57 34.13 28.37 31.41 -\nOurs AMOM 10 34.62 34.82 27.57 31.67 2.3x\nTable 1: Results on 4 WMT machine translation tasks. “*” denotes the results of our implementations.\nModel En↔De En↔Fr En↔Zh En↔Es Avg Speedup\nTransformer 28.71/34.68 36.2/37.0 25.7/18.2 37.8/39.5 32.22 1.0x\nCMLM 27.77/33.87 35.2/35.0 26.0/17.9 37.1/39.0 31.48 2.2x\nAMOM 28.41/34.84 35.6/36.3 26.1/18.4 38.0/39.8 32.18 2.2x\nTable 2: Results on 8 IWSLT datasets. Numbers before and after “/” denote BLEU scores from and to English directions.\net al. 2019), we use the standard Transformer base config-\nuration on WMT datasets and standard Transformer small\nconfiguration on IWSLT datasets for both auto-regressive\nand non-autoregressive experiments. During AMOM train-\ning, we follow the hyper-parameters in CMLMC (Huang,\nPerez, and V olkovs 2022) for WMT14 En↔De and follow\nthe hyper-parameters of CMLM realization in Fairseq 5 for\nthe other datasets. During inference, we average the 5 best\ncheckpoints chosen by validation BLEU scores as our fi-\nnal model and set the length beam as 3/5 for IWSLT/WMT\ndatasets. For XSUM, we choose Transformer base with em-\nbedding dimension 768 and follow the training schedule ap-\nplied in NMT. During our training, we make a specific mod-\nification of the hyper-parameters referring to (Lewis et al.\n2020). During inference we follow the process in (Qi et al.\n2021), where the same consecutive tokens will be merged\nto avoid repeated n-gram tokens. For code generation tasks,\nwe choose Transformer base with embedding size 512 and\nfollow the original training schedule. We make a specific\nmodification of the hyper-parameters referring to (Liu et al.\n2022). For all datasets, we set the limits ratio of adaptive X\n5https://github.com/facebookresearch/fairseq/tree/main/\nexamples/nonautoregressive translation\nfrom 10%-30% and adaptiveY from 20%-80%, and select a\nlinear mapping function to decide the masking ratios. More\ndetails about training are presented in Appendix.\nMain Results\nNatural Machine Translation.Following previous works,\nwe evaluate the performance with BLEU (Papineni et al.\n2002) for WMT datasets and IWSLT En↔De dataset, and\nfor the other IWSLT datasets, we use SacreBLEU 6 (Post\n2018; Liang et al. 2021). Speedup is measured by LGPU\n1 fol-\nlowing the previous work (Kasai et al. 2020b; Gu and Kong\n2021; Helcl, Haddow, and Birch 2022). Table 2 presents\nthe results on 8 IWSLT datasets, we compare our AMOM\nwith original CMLM and strong Transformer (AR) base-\nline. First, a significant improvement can be found over the\noriginal CMLM on all datasets, with about 0.7 BLEU on\naverage. More excitingly, compared with the strong Trans-\nformer (AR) baseline, our AMOM has achieved better per-\nformance on five datasets, and only a tiny gap (0.04 BLEU)\nstill exists on average. We show our results in Table 1 for\nWMT datasets, we compare our approach with various itera-\ntive NAR models, including two popular fully NAR models.\n6https://github.com/mjpost/sacrebleu\n13792\nModel ROUGE-1 ROUGE-2 ROUGE-L\nTransformer 30.66 10.80 24.48\nWithout pretrain\nvanilla NAT 24.04 3.88 20.32\nInsertNAR 17.65 5.18 16.05\nLevenshitein 25.33 7.40 21.48\nDisco 26.85 6.86 21.72\nPOSPD 27.39 7.26 22.15\nCMLM* 25.80 6.31 20.45\nAMOM* 31.59 9.30 24.98\nWith pretrain\nBANG 34.71 11.71 29.16\nMIST 34.63 11.29 28.70\nELMER 37.30 13.17 29.92\nTable 3: Results on XSUM for the text summarization task.\n“*” denotes the results of our implementations.\nModel Python JA V A\nIter. BLEU ES Iter. BLEU ES\nCMLM 4 49.61 69.58 4 60.54 76.68\n10 53.44 70.42 10 62.82 77.24\nAMOM 4 50.57 70.22 4 62.86 76.61\n10 56.50 71.38 10 65.43 77.17\nTable 4: Results on Py150 and Github-Java dataset.\nWe re-run the experiments of CMLM with the same settings\nin AMOM to avoid inconsistency. After applying our sim-\nple yet effective methods to the traditional CMLM frame-\nwork, we achieved state-of-the-art (SOTA) BLEU score on\nWMT16 En→Ro (34.62) and Ro→En (34.82) with 10 iter-\nations. For the WMT14 En↔De dataset, AMOM also out-\nperforms most of the baselines on De→En (31.67). On the\nEn→De dataset, AMOM only gains 0.36 BLEU improve-\nment compared with CMLM and a comparable score com-\npared with strong CMLM-Based baselines. This might be\nbecause our adaptive X strategy hurts the performance in\nthe first iteration to some extent. Note that AMOM is com-\nplementary to other effective tricks applied in CMLM, and\nstronger results can be expected by combining our adaptive\nmasking strategies with their methods.\nSummarization. See Table 3, the performance is evaluated\nby ROUGE F1 score (Lin and Hovy 2002). Specifically,\nwe report the unigram ROUGE-1 and bigram ROUGE-2\noverlap to assess the informativeness, and the longest com-\nmon subsequence ROUGE-L score to assess the fluency.\nWe compare our AMOM with the original CMLM and sev-\neral NAR baseline models, including vanilla NAT (Gu et al.\n2018), InsertNAR (Stern et al. 2019), Levenshitein (Gu,\nWang, and Zhao 2019), Disco (Kasai et al. 2020a),\nPOSPD (Yang et al. 2021), CMLM (Ghazvininejad et al.\n2019), BANG (Qi et al. 2021), MIST (Jiang et al. 2021),\nELMER (Li et al. 2022a). Results show that AMOM outper-\nforms all other NAR models without pre-training. Since pre-\ntraining always benefits summarization task a lot, models\nwith pre-training achieve significant performance improve-\nments. Notice that AMOM can also be applied to the pre-\ntraining and finetune stage, we believe it also works to im-\nprove the performance.\nCode Generation.The performance is evaluated by BLEU\nand ES (Wang et al. 2020), which measure character-level\nedit similarity and n-gram level precision between the target\ncodes and generated codes, respectively. We also report the\nresults of different iterations in Table 4. Our AMOM outper-\nforms the original CMLM with different iterations and gains\nbetter improvements during refinements.\nAnalysis\nThe Mapping Function of AdaptiveX Masking. In this\nsubsection, we exhibit exhaustive experiments to explore en-\ncoder masking strategies and how to affect the model perfor-\nmance. In particular, we analyse the effects of different map-\nping functions, these strategies can utilize decoder masking\nratio αdec to obtain encoder masking ratio αenc:\n• φlinear: αenc = (b − a)αdec + a;\n• φconvex: αenc = (b − a)α2\ndec + b;\n• φconcvae: αenc = (a − b)α2\ndec + 2(b− a)αdec + b;\n• φladder: αenc = a − ⌈αdec\na−b+0.1 ⌉,\nwhere a and b are two hyper-parameters controlling the\nmasking limits, and the specific curves corresponding to the\nabove mapping function are presented in Appendix. The\nresults are shown in Table 5, and it is worth noting that\nthe above experiments are based on the CMLM model and\nIWSLT14 De→En dataset for clear contrast. Early experi-\nments show that encoder masking can boost the model per-\nformance, and at αenc = 0.2, the encoder masked model\nperforms best when using the fixed masking strategy, results\nare shown in Appendix. That is why we design the mapping\nfunction to limit the masking raio around 0.2. Firstly, we\ntake linear mapping functions as our priority. Fortunately,\nlinear mapping has been proved by comprehensive exper-\niments that it is indeed one of the most effective imple-\nmentations to boost the performance. Besides, the results\nare consistent with our intuition that the more tokens in\nY are masked, the few tokens in X should be masked to\nkeep the masking ratio balanced. We also have briefly tried\na few alternative implementations beyond linear mappings,\nbut without achieving further performance improvement.\nThe Effect of AdaptiveX Masking. We also compare our\nadaptive X masking strategy with several related works to\nfurther show its effectiveness. Since JM-NAT (Guo, Xu, and\nChen 2020) also introduces masking operation inX, we also\nconduct experiments to compare AMOM and their bert-like\nmasking. Also, they introduce an auxiliary MLM training\nobjective to improve the encoder, we further verify if this\ncan combine with AMOM, see Table 6. Notice that we keep\nthe decoder-side the same as vanilla CMLM (without adap-\ntive Y masking in AMOM and n-gram loss in JM-NAT)\nto make a fair comparison of encoder-side. Results show\nthat this MLM training objective can also improve AMOM\nslightly, but seems less related to our assumption and pur-\npose. Besides, we can find adaptive X outperforms the bert-\nlike masking for CMLM. Also, we find that the adaptive X\n13793\nStrategy αenc BLEU\nLinear\nφlinear(αdec, 0.25, 0.15) 34.20\nφlinear(αdec, 0.3, 0.1) 34.48\nφlinear(αdec, 0.35, 0.15) 34.30\nφlinear(αdec, 0.4, 0.1) 34.40\nφlinear(αdec, 0.1, 0.3) 33.64\nφlinear(αdec, 0.1, 0.4) 33.76\nConvex ψconvex(αdec, 0.3, 0.1) 33.55\nConcave ψconcave(αdec, 0.3, 0.1) 33.96\nLadder ψladder(αdec, 0.3, 0.1) 34.17\nTable 5: The BLEU scores of adaptive X masking strategy.\nMethod BLEU Method BLEU\nCMLM 33.87 CMLM 33.87\n+ adax 34.48 + mix cutoff 33.96\n+ adax+mlmloss 34.57 + span cutoff 33.93\n+ jm-nat 34.13 + random replace 34.13\n+ jm-nat+mlmloss 34.21 + random delete 33.95\nTable 6: Comparison between adaptive X masking and re-\nlated methods.\nmasking operation is similar to a data augmentation strategy\n(such as cutoff (Shen et al. 2020)), and specially designed\nto improve the refinements ability of CMLM. To better an-\nalyze them, we also compare adaptive X masking with sev-\neral common data augmentation strategies (including cut-\noff). Since fixed masking is similar to token cutoff, we con-\nduct experiments with span cutoff and mix cutoff. We also\ncompare with some other strategies (such as random delete,\nrandom replace). Results show that adaptiveX masking out-\nperforms all other operations onX, while various traditional\nstrategies can boost vanilla CMLM to some extent.\nThe Mapping Function of Adaptive Y Masking. We\nalso experiment with different masking strategies when ap-\nplied to the decoder side in a two-step training scheme.\nWe try same adaptive mapping function and denoted as\nψlinear, ψconvex, ψconcvae, and ψladder to obtain masking\nratio αdec. Specifically, we can calculate αdec based on ran-\ndomly sampled variable β which is correctness ratio pre-\ndicted by first step training as mentioned above : αdec =\nψlinear(β, a, b) = (b− a)β + a. Unlike the encoder mask-\ning mapping function, we choose a large masking ratio range\nbecause there exist various conditions of masking ratios and\ntokens confidence during inference. The schedule curves are\nalso shown in Appendix. Table 7 lists the results of several\nadaptive decoder masking strategies. Notice that we achieve\nall results here with a linear mappingφlinear(αdec, 0.3, 0.1)\nfor source-side masking. The simple linear mapping func-\ntion achieves the best performance, and the large masking\nratio range seems better. Besides, a high correctness ratio al-\nways indicates high token confidence, and then fewer tokens\nin ˆYmask will be masked in the next iteration. Our adaptive\nY masking strategy matches the inference strategy of the\noriginal CMLM.\nStrategy αdec BLEU\nLinear\nψlinear(β, 0.1, 0.9) 34.65\nψlinear(β, 0.2, 0.8) 34.84\nψlinear(β, 0.3, 0.7) 34.79\nψlinear(β, 0.2, 0.5) 34.62\nψlinear(β, 0.5, 0.8) 34.77\nψlinear(β, 0.8, 0.2) 34.61\nConvex ψconvex(β, 0.2, 0.8) 34.80\nConcave ψconcave(β, 0.2, 0.8) 34.59\nLadder ψladder(β, 0.2, 0.8) 34.75\nTable 7: The BLEU scores of adaptive Y masking strategy.\nMasking Strategy BLEU Masking Strategy BLEU\nAdaptive (Ours) 34.84 Uniform 34.53\n+ same ratio 34.65 Glancing 34.68\n+ 3 step 34.50 Glat 33.72\n+ exposure bias 34.79 Glat + fix-x (0.1) 33.64\n+ confidence-based 33.85 Glat + ada-x 33.35\nTable 8: Comparison of adaptive Y masking with different\nconstraints and related methods.\nThe Effect of AdaptiveY Masking. To better understand\nthe two-step training scheme and how to guide model train-\ning, we analyze the effect of different masking and training\nsettings, and notice that we all keep the uniform masking\nstrategy in the first step as the original CMLM. First, we\nuse uniform sampling to replace adaptive ψ sampling in the\nsecond masking step. Then we also keep the masking ra-\ntio αdec = β to verify whether the masking ratio is critical\nfor model training. Besides, we use an adaptive strategy to\ntrain three steps which simulate the multi-step inference sce-\nnarios. We also test the impact of whether recover ground\ntruth tokens or keep the predicted token in the second train-\ning step. Since the masking tokens are chosen by predic-\ntion confidence during inference, we also apply confidence-\nbased masking during training to further verify our adap-\ntive Y masking. Moreover, we also compare our adaptive\nY masking with the glancing masking strategy proposed in\nGLAT to improve the one-pass decoding. The results are\nshown in Table 8. We can observe that adaptive masking\noutperforms uniform masking in the second-step training,\nand the uniform masking seems to bring little improvements\ncompared with adaptive X masking (34.48). This also indi-\ncates that although AMOM may expand training expenses,\nadaptive Y masking is truly valuable, and the performance\nimprovements do not come from more updates. Moreover,\nresults also reflect that two-step refinements are enough for\nmodel training without the necessity for more steps. Besides,\nusing model prediction instead of ground truth can effec-\ntively reduce the problem of exposure bias, and introduc-\ning a confidence-based masking strategy does not bring im-\nprovements. Compared with GLAT, adopting the glancing\nmasking as the second step masking strategy also performs\nbetter than uniform masking but is inferior to our adaptive\n13794\n[0,10) [10,20) [20,30) [30,40) >= 40\nLENGTH\n20\n25\n30\n35BLEU\n[0,10) [10,20) [20,30) [30,40) >= 40\nLENGTH\n20\n25\n30\n35BLEU\nCMLM :   Iter.1\nAMOM :   Iter.1\nIter.10\nIter.10\nIter.N\nIter.N\nFigure 1: Comparison between different source language\nsentence length and decoding iterations.\nY masking. Besides, if we directly adopt glancing mask-\ning and one-step training the same as GLAT (Glat), the per-\nformance declines, and further combining it with encoder\nmasking even harms the performance. This indicates that our\nmethods play a different role compared with GLAT.\nMore Iterations for Long Sequence.For long source input\nsentences, it is almost impossible to obtain a fluent and rela-\ntively correct result for non-autoregressive machine transla-\ntion models. It often requires multiple iterations to refine the\ntranslation results. Therefore, the ability to refine is a cru-\ncial evaluation criterion for a model. First, we compare the\nBLEU scores of AMOM and CMLM in different iterations\nsteps, as shown in Appendix. We can see that the AMOM\noutperforms the CMLM model when the iterations step in-\ncreases, which proves that an adaptive masking strategy can\nenhance refinement ability. In addition, we make a compar-\nison of results with different source sentence length N and\ndifferent decoding iterations T on two two datasets (IWSLT\nDE→EN and WMT EN→RO). We split each dataset into\nfive segments according to sentence length and run infer-\nence three times according to different stepsN ∈ [1, 10, N].\nIn Figure 1, we present the improvements of more decod-\ning steps with different colours. Results show that AMOM\nexhibit significant gain than vanilla CMLM with more steps,\ne.g., although the performance of AMOM in Iter.1 is inferior\nthan CMLM, it all outperforms CMLM in Iter.10, especially\nfor long sentences. We can also find that long sentences of-\nten require more decoding steps, and AMOM perform better.\nRelated Work\nIterative-based Non-autoregressive Sequence Genera-\ntion. Non-autoregressive models have attracted an increas-\ning attention in recent years due to their efficient decod-\ning, but the improvements in decoding speed come at the\nexpense of generation quality. Thus, iterative-based non-\nautoregressive (NAR) models (Lee, Mansimov, and Cho\n2018; Gu, Wang, and Zhao 2019; Saharia et al. 2020; Geng,\nFeng, and Qin 2021; Lu, Meng, and Peng 2022) are pro-\nposed to achieve a better trade-off between the inference\nspeedup and generation quality. Lee, Mansimov, and Cho\nfirst propose the iterative model which aims refine the noised\ntarget sequence. Later, insertion and deletion operations are\nintroduced in each decoding iteration to create the final\ntranslation. Among these iterative NAR methods, the con-\nditional masked language model (CMLM) (Ghazvininejad\net al. 2019) is widely-used owing to its promising perfor-\nmance when using the mask-predict strategy. In particular,\nCMLM leverages the masked language model objective to\nguide model training and iteratively masks and predicts to-\nkens during inference. Many recently works have achieved\nperformance improvements based on CMLM (Guo, Xu, and\nChen 2020; Huang, Perez, and V olkovs 2022). Recently,\nSavinov et al. proposed step-unrolled denoising autoencoder\nwhich adopts denoising operation in each iteration.\nMasked Language Model. The masked language model\n(MLM) first introduced by BERT (Devlin et al. 2018) has\nbecome the essential component of various popular pre-\ntraining methods (Song et al. 2019; Liu et al. 2019; Dong\net al. 2019; Joshi et al. 2020; Li et al. 2022b; Xu, Van Durme,\nand Murray 2021). Its standard paradigm is to select some\ntokens in the source sequence by different strategies and then\nreplace them with a [mask] token, and then the model\nis trained to predict the masked tokens. Since the masking\nstrategy is significantly essential for these model, different\nmasking strategies are served as different learning methods.\nAs BERT is served as a single Transformer encoder and a\nmonolingual framework, there are limitations in various ap-\nplications, such as machine translation. Then much progress\nhas been made to extend the applications of masked lan-\nguage modeling strategy (Guo et al. 2020; Zhu et al. 2020;\nLi et al. 2022b). The CMLM-based non-autoregressive mod-\nels can also benefit from it by introducing a uniform mask-\ning strategy in training and a mask-predict decoding strat-\negy during inference (Ghazvininejad et al. 2019). However,\nonly few improvements on masking strategies are explored\nfor CMLM. In this work, we further design a simple yet ef-\nfective adaptive masking over masking method on both the\nencoder and decoder sides to enhance the CMLM training\nfor better refinement capability during inference.\nConclusion\nIn this paper, we present an adaptive masking over masking\n(AMOM) strategy to enhance the conditional masked lan-\nguage model (CMLM) for non-autoregressive sequence gen-\neration. Our AMOM only contains two masking operations\nin model training without modifying the model structure\nor changing the inference schedule. Extensive experiments\non different sequence generation tasks indicate our pro-\nposed AMOM can yield significant performance improve-\nment over the original CMLM model and even outperform\nthe strong autoregressive (Transformer) counterpart on 7\nNMT benchmark datasets and achieves SOTA performance\non WMT16 EN→RO, 34.82 BLEU on WMT16 RO→EN,\nand 34.84 BLEU on IWSLT De→En. Due to the limitation\nof computational resources, we only test our AMOM for the\nCMLM model. In the near future, we will design more el-\negant AMOM strategies and explore their effectiveness on\ndifferent NAR frameworks. We also will extend our AMOM\nto other types of masked language models, both in the pre-\ntraining and fine-tuning stages.\n13795\nAcknowledgments\nRuiyang Xu contributes equally with Yisheng Xiao. Jun-\ntao Li is the corresponding author. This work is sup-\nported by the National Science Foundation of China (NSFC\nNo. 62206194), the Natural Science Foundation of Jiangsu\nProvince, China (No. BK20220488), and the Project Funded\nby the Priority Academic Program Development of Jiangsu\nHigher Education Institutions. This work is also supported\nby Beijing Academy of Artificial Intelligence (BAAI).\nReferences\nAllamanis, M.; and Sutton, C. 2013. Mining source code\nrepositories at massive scale using language modeling. In\n2013 10th Working Conference on Mining Software Reposi-\ntories (MSR), 207–216. IEEE.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDing, L.; Wang, L.; Liu, X.; Wong, D. F.; Tao, D.; and Tu,\nZ. 2020. Understanding and Improving Lexical Choice in\nNon-Autoregressive Translation. In ICLR.\nDing, L.; Wang, L.; Liu, X.; Wong, D. F.; Tao, D.; and Tu,\nz. 2021. Rejuvenating Low-Frequency Words: Making the\nMost of Parallel Data in Non-Autoregressive Translation. In\nACL-IJCNLP, 3431–3441.\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y .;\nGao, J.; Zhou, M.; and Hon, H.-W. 2019. Unified language\nmodel pre-training for natural language understanding and\ngeneration. NeurIPS, 32.\nDu, C.; Tu, Z.; and Jiang, J. 2021. Order-agnostic cross en-\ntropy for non-autoregressive machine translation. In ICML,\n2849–2859. PMLR.\nElsaid, A.; Mohammed, A.; Fattouh, L.; and Sakre, M. 2022.\nA Comprehensive Review of Arabic Text summarization.\nIEEE Access.\nGeng, X.; Feng, X.; and Qin, B. 2021. Learning to Rewrite\nfor Non-Autoregressive Neural Machine Translation. In\nEMNLP, 3297–3308.\nGhazvininejad, M.; Levy, O.; Liu, Y .; and Zettlemoyer,\nL. 2019. Mask-Predict: Parallel Decoding of Conditional\nMasked Language Models. In EMNLP-IJCNLP, 6112–\n6121.\nGhazvininejad, M.; Levy, O.; Zettlemoyer; and Luke. 2020.\nSemi-autoregressive training improves mask-predict decod-\ning. arXiv preprint arXiv:2001.08785.\nGu, J.; Bradbury, J.; Xiong, C.; Li, V . O.; and Socher, R.\n2018. Non-Autoregressive Neural Machine Translation. In\nICLR.\nGu, J.; and Kong, X. 2021. Fully Non-autoregressive Neural\nMachine Translation: Tricks of the Trade. In Findings of\nACL-IJCNLP, 120–133.\nGu, J.; Wang, C.; and Zhao, J. 2019. Levenshtein Trans-\nformer. NeurIPS, 32: 11181–11191.\nGuo, J.; Xu, L.; and Chen, E. 2020. Jointly masked\nsequence-to-sequence model for non-autoregressive neural\nmachine translation. In ACL, 376–385.\nGuo, J.; Zhang, Z.; Xu, L.; Wei, H.-R.; Chen, B.; and Chen,\nE. 2020. Incorporating BERT into Parallel Sequence Decod-\ning with Adapters. In NeurIPS.\nHao, Y .; He, S.; Jiao, W.; Tu, Z.; Lyu, M.; and Wang,\nX. 2021. Multi-Task Learning with Shared Encoder for\nNon-Autoregressive Machine Translation. In NAACL-HLT,\n3989–3996.\nHelcl, J.; Haddow, B.; and Birch, A. 2022. Non-\nAutoregressive Machine Translation: It’s Not as Fast as it\nSeems. arXiv preprint arXiv:2205.01966.\nHinton, G. E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.;\nand Salakhutdinov, R. R. 2012. Improving neural networks\nby preventing co-adaptation of feature detectors. arXiv\npreprint arXiv:1207.0580.\nHuang, C.; Zhou, H.; Za ¨ıane, O. R.; Mou, L.; and Li, L.\n2022a. Non-autoregressive translation with layer-wise pre-\ndiction and deep supervision. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36, 10776–\n10784.\nHuang, F.; Zhou, H.; Liu, Y .; Li, H.; and Huang, M. 2022b.\nDirected Acyclic Transformer for Non-Autoregressive Ma-\nchine Translation. arXiv preprint arXiv:2205.07459.\nHuang, X. S.; Perez, F.; and V olkovs, M. 2022. Improv-\ning Non-Autoregressive Translation Models Without Distil-\nlation. In ICLR.\nJiang, T.; Huang, S.; Zhang, Z.; Wang, D.; Zhuang, F.; Wei,\nF.; Huang, H.; Zhang, L.; and Zhang, Q. 2021. Improving\nNon-autoregressive Generation with Mixup Training. arXiv\npreprint arXiv:2110.11115.\nJoshi, M.; Chen, D.; Liu, Y .; Weld, D. S.; Zettlemoyer, L.;\nand Levy, O. 2020. Spanbert: Improving pre-training by rep-\nresenting and predicting spans. Transactions of the Associ-\nation for Computational Linguistics, 8: 64–77.\nKasai, J.; Cross, J.; Ghazvininejad, M.; and Gu, J. 2020a.\nParallel machine translation with disentangled context trans-\nformer. arXiv preprint arXiv:2001.05136.\nKasai, J.; Pappas, N.; Peng, H.; Cross, J.; and Smith, N.\n2020b. Deep Encoder, Shallow Decoder: Reevaluating Non-\nautoregressive Machine Translation. In ICLR.\nKim, Y .; and Rush, A. M. 2016. Sequence-Level Knowledge\nDistillation. In EMNLP, 1317–1327.\nLee, J.; Mansimov, E.; and Cho, K. 2018. Deterministic\nNon-Autoregressive Neural Sequence Modeling by Iterative\nRefinement. In EMNLP, 1173–1182.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2020. BART: Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and Compre-\nhension. In ACL, 7871–7880.\nLi, J.; Tang, T.; Zhao, W. X.; Nie, J.-Y .; and Wen, J.-R.\n2022a. ELMER: A Non-Autoregressive Pre-trained Lan-\nguage Model for Efficient and Effective Text Generation.\narXiv preprint arXiv:2210.13304.\n13796\nLi, P.; Li, L.; Zhang, M.; Wu, M.; and Liu, Q. 2022b. Uni-\nversal Conditional Masked Language Pre-training for Neu-\nral Machine Translation. ACL.\nLiang, X.; Wu, L.; Li, J.; Wang, Y .; Meng, Q.; Qin, T.; Chen,\nW.; Zhang, M.; Liu, T.-Y .; et al. 2021. R-drop: regularized\ndropout for neural networks. NeurIPS, 34.\nLin, C.-Y .; and Hovy, E. 2002. Manual and automatic evalu-\nation of summaries. InProceedings of the ACL-02 Workshop\non Automatic Summarization, 45–51.\nLiu, F.; Fu, Z.; Li, G.; Jin, Z.; Liu, H.; and Hao, Y . 2022.\nNon-autoregressive Model for Full-line Code Completion.\narXiv preprint arXiv:2204.09877.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLu, S.; Meng, T.; and Peng, N. 2022. InsNet: An Efficient,\nFlexible, and Performant Insertion-based Text Generation\nModel. In Advances in Neural Information Processing Sys-\ntems.\nMa, Y .; Nguyen, K. L.; Xing, F. Z.; and Cambria, E. 2020. A\nsurvey on empathetic dialogue systems.Information Fusion,\n64: 50–70.\nMarjan, G.; Karpukhin, V .; Zettlemoyer, L.; and Levy, O.\n2020. Aligned cross entropy for non-autoregressive machine\ntranslation. In ICML, 3515–3523. PMLR.\nNarayan, S.; Cohen, S.; and Lapata, M. 2018. Don’t Give\nMe the Details, Just the Summary! Topic-Aware Convo-\nlutional Neural Networks for Extreme Summarization. In\nEMNLP, 1797–1807.\nOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;\nGrangier, D.; and Auli, M. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In NAACL-HLT.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine transla-\ntion. In ACL, 311–318.\nPost, M. 2018. A call for clarity in reporting BLEU scores.\narXiv preprint arXiv:1804.08771.\nQi, W.; Gong, Y .; Jiao, J.; Yan, Y .; Chen, W.; Liu, D.; Tang,\nK.; Li, H.; Chen, J.; Zhang, R.; et al. 2021. Bang: Bridging\nautoregressive and non-autoregressive generation with large\nscale pretraining. In ICML, 8630–8639. PMLR.\nQian, L.; Zhou, H.; Bao, Y .; Wang, M.; Qiu, L.; Zhang,\nW.; Yu, Y .; and Li, L. 2021. Glancing Transformer for\nNon-Autoregressive Neural Machine Translation. In ACL-\nIJCNLP, 1993–2003.\nRaychev, V .; Bielik, P.; and Vechev, M. 2016. Probabilistic\nmodel for code with decision trees.ACM SIGPLAN Notices,\n51(10): 731–747.\nSaharia, C.; Chan, W.; Saxena, S.; and Norouzi, M.\n2020. Non-Autoregressive Machine Translation with Latent\nAlignments. In EMNLP, 1098–1108.\nSavelieva, A.; Au-Yeung, B.; and Ramani, V . 2020. Abstrac-\ntive summarization of spoken and written instructions with\nBERT. arXiv preprint arXiv:2008.09676.\nSavinov, N.; Chung, J.; Binkowski, M.; Elsen, E.; and\nvan den Oord, A. 2021. Step-unrolled Denoising Autoen-\ncoders for Text Generation. In International Conference on\nLearning Representations.\nShen, D.; Zheng, M.; Shen, Y .; Qu, Y .; and Chen, W. 2020.\nA simple but tough-to-beat data augmentation approach\nfor natural language understanding and generation. arXiv\npreprint arXiv:2009.13818.\nSong, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y . 2019. Mass:\nMasked sequence to sequence pre-training for language gen-\neration. arXiv preprint arXiv:1905.02450.\nStern, M.; Chan, W.; Kiros, J.; and Uszkoreit, J. 2019. Inser-\ntion transformer: Flexible sequence generation via insertion\noperations. In ICML, 5976–5985. PMLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS, 5998–6008.\nWang, W.; Shen, S.; Li, G.; and Jin, Z. 2020. Towards Full-\nline Code Completion with Neural Language Models. arXiv\npreprint arXiv:2009.08603.\nWu, L.; Wang, Y .; Xia, Y .; Tian, F.; Gao, F.; Qin, T.; Lai,\nJ.; and Liu, T.-Y . 2019. Depth Growing for Neural Machine\nTranslation. In ACL, 5558–5563.\nXiao, Y .; Wu, L.; Guo, J.; Li, J.; Zhang, M.; Qin, T.; and\nLiu, T.-y. 2022. A Survey on Non-Autoregressive Gener-\nation for Neural Machine Translation and Beyond. arXiv\npreprint arXiv:2204.09269.\nXie, P.; Li, Z.; and Hu, X. 2021. MvSR-NAT: Multi-\nview Subset Regularization for Non-Autoregressive Ma-\nchine Translation. arXiv preprint arXiv:2108.08447.\nXu, H.; Van Durme, B.; and Murray, K. 2021. BERT,\nmBERT, or BiBERT? A Study on Contextualized Embed-\ndings for Neural Machine Translation. arXiv preprint\narXiv:2109.04588.\nYang, K.; Lei, W.; Liu, D.; Qi, W.; and Lv, J. 2021. POS-\nConstrained Parallel Decoding for Non-autoregressive Gen-\neration. In ACL, 5990–6000.\nZhang, Y .; Sun, S.; Galley, M.; Chen, Y .-C.; Brockett, C.;\nGao, X.; Gao, J.; Liu, J.; and Dolan, W. B. 2020. DI-\nALOGPT: Large-Scale Generative Pre-training for Conver-\nsational Response Generation. In ACL,System Demonstra-\ntions, 270–278.\nZhou, C.; Gu, J.; and Neubig, G. 2019. Understand-\ning Knowledge Distillation in Non-autoregressive Machine\nTranslation. In ICLR.\nZhu, J.; Xia, Y .; Wu, L.; He, D.; Qin, T.; Zhou, W.; Li, H.;\nand Liu, T.-Y . 2020. Incorporating bert into neural machine\ntranslation. arXiv preprint arXiv:2002.06823.\n13797",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8688206672668457
    },
    {
      "name": "Automatic summarization",
      "score": 0.8241422176361084
    },
    {
      "name": "Machine translation",
      "score": 0.7537726163864136
    },
    {
      "name": "Transformer",
      "score": 0.6265361905097961
    },
    {
      "name": "Language model",
      "score": 0.6079645752906799
    },
    {
      "name": "Inference",
      "score": 0.5764220356941223
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5048738121986389
    },
    {
      "name": "Autoregressive model",
      "score": 0.5037664771080017
    },
    {
      "name": "Masking (illustration)",
      "score": 0.49416449666023254
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.48802343010902405
    },
    {
      "name": "Speedup",
      "score": 0.4682126045227051
    },
    {
      "name": "Speech recognition",
      "score": 0.45588645339012146
    },
    {
      "name": "Encoder",
      "score": 0.4505121409893036
    },
    {
      "name": "Code (set theory)",
      "score": 0.43049827218055725
    },
    {
      "name": "Machine learning",
      "score": 0.3343082070350647
    },
    {
      "name": "Programming language",
      "score": 0.08610722422599792
    },
    {
      "name": "Parallel computing",
      "score": 0.07661992311477661
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}