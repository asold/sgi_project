{
    "title": "Multi-Manifold Attention for Vision Transformers",
    "url": "https://openalex.org/W4388240426",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2087629751",
            "name": "Dimitrios Konstantinidis",
            "affiliations": [
                "Information Technologies Institute",
                "Centre for Research and Technology Hellas"
            ]
        },
        {
            "id": "https://openalex.org/A3025430007",
            "name": "Ilias Papastratis",
            "affiliations": [
                "Centre for Research and Technology Hellas",
                "Information Technologies Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2943367408",
            "name": "Kosmas Dimitropoulos",
            "affiliations": [
                "Information Technologies Institute",
                "Centre for Research and Technology Hellas"
            ]
        },
        {
            "id": "https://openalex.org/A2143644221",
            "name": "Petros Daras",
            "affiliations": [
                "Information Technologies Institute",
                "Centre for Research and Technology Hellas"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1971662204",
        "https://openalex.org/W3137278571",
        "https://openalex.org/W6795062860",
        "https://openalex.org/W3136416617",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4312849330",
        "https://openalex.org/W4312785900",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W4214636423",
        "https://openalex.org/W6791705549",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W4313007769",
        "https://openalex.org/W6794345597",
        "https://openalex.org/W6779163297",
        "https://openalex.org/W6783944145",
        "https://openalex.org/W6771626834",
        "https://openalex.org/W6805937555",
        "https://openalex.org/W4312820606",
        "https://openalex.org/W4312291121",
        "https://openalex.org/W4312349930",
        "https://openalex.org/W4389977645",
        "https://openalex.org/W6797235774",
        "https://openalex.org/W4312960790",
        "https://openalex.org/W6810661123",
        "https://openalex.org/W6778939810",
        "https://openalex.org/W2515363767",
        "https://openalex.org/W2952200000",
        "https://openalex.org/W2970832409",
        "https://openalex.org/W6810997276",
        "https://openalex.org/W3034224401",
        "https://openalex.org/W3035037263",
        "https://openalex.org/W6810935401",
        "https://openalex.org/W3144630859",
        "https://openalex.org/W2759912964",
        "https://openalex.org/W2554962499",
        "https://openalex.org/W2902941042",
        "https://openalex.org/W2904518066",
        "https://openalex.org/W2890072975",
        "https://openalex.org/W3201625355",
        "https://openalex.org/W2948783659",
        "https://openalex.org/W3048942138",
        "https://openalex.org/W6792695861",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W6717452949",
        "https://openalex.org/W6640237361",
        "https://openalex.org/W2142040002",
        "https://openalex.org/W6753626425",
        "https://openalex.org/W6789913677",
        "https://openalex.org/W6787972765",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W2507296351",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W2963855133",
        "https://openalex.org/W6739622702",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2949736877",
        "https://openalex.org/W153185079",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4312443924",
        "https://openalex.org/W3159778524",
        "https://openalex.org/W2551176409",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W1928707779",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W3137963805",
        "https://openalex.org/W2622263826",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W4226467633",
        "https://openalex.org/W4226358102",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2886613400",
        "https://openalex.org/W2419919125",
        "https://openalex.org/W4200634271"
    ],
    "abstract": "Vision Transformers are very popular nowadays due to their state-of-the-art performance in several computer vision tasks, such as image classification and action recognition. Although their performance has been greatly enhanced through highly descriptive patch embeddings and hierarchical structures, there is still limited research on utilizing additional data representations so as to refine the self-attention map of a Transformer. To address this problem, a novel attention mechanism, called multi-manifold multi-head attention, is proposed in this work to substitute the vanilla self-attention of a Transformer. The proposed mechanism models the input space in three distinct manifolds, namely Euclidean, Symmetric Positive Definite and Grassmann, thus leveraging different statistical and geometrical properties of the input for the computation of a highly descriptive attention map. In this way, the proposed attention mechanism can guide a Vision Transformer to become more attentive towards important appearance, color and texture features of an image, leading to improved classification and segmentation results, as shown by the experimental results on well-known datasets.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nMulti-manifold Attention for Vision Transformers\nDIMITRIOS KONSTANTINIDIS, ILIAS PAPASTRATIS, KOSMAS DIMITROPOULOS, AND PETROS\nDARAS, (Senior Member, IEEE)\nInformation Technologies Institute, Centre for Research and Technology Hellas (CERTH), Thessaloniki, Greece\nCorresponding author: Dimitrios Konstantinidis (e-mail: dikonsta@iti.gr).\nThis work was supported from EC under grant agreement 101061548 ‘‘DAFNEplus: Decentralized platform for fair creative content\ndistribution empowering creators and communities though new digital distribution models based on digital tokens.’’\nABSTRACT Vision Transformers are very popular nowadays due to their state-of-the-art performance in\nseveral computer vision tasks, such as image classification and action recognition. Although their perfor-\nmance has been greatly enhanced through highly descriptive patch embeddings and hierarchical structures,\nthere is still limited research on utilizing additional data representations so as to refine the self-attention map\nof a Transformer. To address this problem, a novel attention mechanism, called multi-manifold multi-head\nattention, is proposed in this work to substitute the vanilla self-attention of a Transformer. The proposed\nmechanism models the input space in three distinct manifolds, namely Euclidean, Symmetric Positive\nDefinite and Grassmann, thus leveraging different statistical and geometrical properties of the input for the\ncomputation of a highly descriptive attention map. In this way, the proposed attention mechanism can guide\na Vision Transformer to become more attentive towards important appearance, color and texture features of\nan image, leading to improved classification and segmentation results, as shown by the experimental results\non well-known datasets.\nINDEX TERMS Attention, manifold, vision transformers, image classification, semantic segmentation.\nI. INTRODUCTION\nT\nRANSFORMER networks have been met with great\ninterest from the research community when they were\noriginally proposed for natural language processing in the pi-\noneering work of [1]. A Transformer is a network architecture\nthat relies on self-attention, which is an attention mechanism\nthat relates different positions of a single sequence in order to\ncompute a new representation of the sequence. Transformers\nare designed to handle sequential input data, however, unlike\nRecurrent Neural Networks (RNNs), they process the data in\nparallel since they receive the entire sequence as input. In this\nway, Transformers can extract both short- and long-term de-\npendencies between input and output, while simultaneously\nachieving increased parallelization and training speed with\nrespect to RNNs.\nRecently, Vision Transformers (ViTs) [2] have been intro-\nduced for computer vision tasks, leading to state-of-the-art\nperformance in well-known benchmark datasets due to their\nability to model short- and long-range relationships between\ndifferent image areas. However, ViTs lack the local inductive\nbiases of Convolutional Neural Networks (CNNs) and do not\nefficiently model local information [3]. To overcome such\nissues, recent works aim to increase local structure modelling\nby introducing convolutions to the ViTs [3]–[5], redesign\nFIGURE 1. With the modelling of input in the EuclideanEd , Grassmann\nG(p, d) and SPDSd\n++ manifolds and the computation of the respective\ndistance maps, the proposed MMA produces a fused attention map with\nhigh discriminative power.\nthe patch tokenization process and introduce local attention\nmechanisms [6]–[9] or adopt hierarchical structures similar\nto CNNs [6], [10], [11].\nHowever, most ViTs operate only in the Euclidean space\nof pixel intensity values, overlooking the fact that data rep-\nresentations in other manifolds can be beneficial to their per-\nformance. Additionally, the attention maps tend to be similar\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\nin the deeper layers of ViTs and their representation stops\nimproving [12], thus a need for more discriminative attention\nmaps is imperative. To this end, this work proposes multi-\nmanifold multi-head attention (MMA) that can be introduced\nto any ViT to improve the discriminative power of its attention\nmap. The main advantage of MMA over the vanilla self-\nattention is the use of three different manifolds, namely Eu-\nclidean, Symmetric Positive Definite (SPD) and Grassmann\nto model the input. Computing feature representations and\ndistances in manifolds with different statistical and geometri-\ncal properties enables a better modelling of the input through\nthe computation of an attention map that can better enhance\nthe important features of the input for a given computer vision\ntask, as shown in Fig. 1. More specifically, the contributions\nof this work are the following:\n• A novel self-attention mechanism, called MMA, is pro-\nposed to accurately model the underlying structure of\nthe input space through its transformation into feature\nrepresentations in three distinct manifolds.\n• The proposed MMA can be added to any Vision Trans-\nformer to better guide its attention towards important\nappearance, color and texture features of an image.\n• Extensive experimentation with various ViTs and on\ndifferent image classification and semantic segmenta-\ntion datasets verifies the performance improvements\nachieved with the proposed MMA.\nII. RELATED WORK\nA. VISION TRANSFORMERS\nDosovitskiy et al., in [2], were the first to propose ViT as a\npure Transformer backbone for image classification. In their\nwork, the input image is segmented into a series of non-\noverlapping image patches that are then projected into a linear\nembedding sequence. This sequence is concatenated with a\nlearnable positional encoding that holds information on the\nspatial location of patches in the image before being fed to\nthe Transformer for classification.\nSince then, ViTs have been heavily modified to achieve\nimproved performance on computer vision tasks [13]. Several\nViTs combine CNNs with Transformers to effectively lever-\nage the convolutional biases in images and enhance accuracy.\nBased on empirical observations that CNNs are better teach-\ners than Transformers, Touvron et al. employed a teacher-\nstudent strategy to transfer the inductive bias of the CNN\nteacher to the Transformer student through knowledge dis-\ntillation [14]. The authors in [3] proposed ConViT, in which\na parallel convolution branch was attached to the Transformer\nbranch to impose convolutional inductive biases via a Gated\nPositional Self-Attention that approximates the locality of\nconvolutional layers. Yuan et al.proposed the Convolution-\nenhanced Image Transformer that uses a convolutional mod-\nule to extract patch embeddings from low-level features and\na layer-wise attention to model long-range dependencies [5].\nTo improve local attention and enhance the feature ex-\ntraction capabilities of ViTs, Han et al. combined Trans-\nformer networks at patch and pixel levels to produce better\nrepresentations with rich local and global information [15].\nMeanwhile, the CSWin Transformer [9] achieves strong mod-\neling capabilities by performing self-attention in horizontal\nand vertical stripes in parallel, while A-ViT [16] adaptively\nadjusts the inference cost of ViTs for images of different\ncomplexity by reducing the number of processed tokens\nas inference proceeds. In a similar fashion, the authors in\n[17] combined locally-grouped self-attention and global sub-\nsampled attention in a Vision Transformer, achieving compet-\nitive image classification and object detection results. To fur-\nther reduce computational cost, several works approximated\nthe quadratic operations in self-attention by low-rank matri-\nces [18], positive orthogonal random features [19], locality-\nsensitive hashing [20] and memory-efficient coupling atten-\ntion maps [21]. On the other hand, the authors in [22] re-\nplaced the attention mechanism with a spatial pooling opera-\ntion, demonstrating that the resulted MetaFormer can achieve\nstate-of-the-art performance in several computer vision tasks.\nRealizing that ViTs need sufficiently large datasets to per-\nform well, Hassani et al. proposed three compact ViT ar-\nchitectures, namely ViT-Lite, Compact Vision Transformers\n(CVTs) and Compact Convolutional Transformers (CCTs)\n[4]. ViT-Lite is similar to the original ViT but with smaller\npatch sizing suitable for small datasets. CVTs expand on ViT-\nLite by pooling the sequential information from the Trans-\nformer, eliminating the need for the extra classification token.\nFinally, CCTs further expand on CVTs by adding convolu-\ntional blocks to the tokenization step, thus preserving local\ninformation while encoding relationships between patches,\nunlike the original ViT.\nRecognizing that a fixed resolution across the entire net-\nwork neglects fine-grained information and brings heavy\ncomputational costs, several works proposed hierarchical\nstructures for ViTs. Yuan et al. proposed the hierarchical\nstructuring of the patch embeddings through the combination\nof neighbouring embeddings into a single one [6]. Similarly,\nthe Token Pyramid Vision Transformer produces scale-aware\nsemantic features by processing tokens from various scales\ninto a single token to augment the representation [23]. Other\nViTs reduce the spatial dimensions of the output progres-\nsively, similarly to CNNs, using spatial-reduction attention\nor pooling layer [10], [11]. The Swin Transformer employs\nshifting windows in each layer to create hierarchical global\nand boundary features through cross-window interactions [7],\n[24], while the authors in [25] proposed CrossFormer that\nemploys a cross-scale embedding layer and a long-short dis-\ntance attention that builds dependencies among neighboring\nembeddings as well as embeddings that are far away from\neach other. Following a different strategy, the authors in [26]\nproposed the ViTAE architecture that employs convolution\nblocks in parallel to the multi-head self-attention modules\nto enable the Transformer network to better learn local fea-\ntures and global dependencies collaboratively. Whereas, the\nauthors in [27] proposed the use of deformable attention to\nmodel the relations among tokens under the guidance of the\nimportant regions in the feature maps, achieving very accurate\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\nresults on image classification and dense prediction tasks.\nFinally, UniFormer was proposed in [28] to combine the ad-\nvantages of 3D convolution and spatiotemporal self-attention,\nachieving a balance between computation efficiency and ac-\ncuracy. Different from traditional transformers, UniFormer\ncan tackle both spatiotemporal redundancy and dependency\nby learning local and global token affinity respectively in\nshallow and deep layers.\nTraditionally, ViTs process raw pixel intensities directly in\nthe Euclidean space without considering how different data\nrepresentations may affect their performance. The proposed\nwork improves local attention through the use of feature rep-\nresentations in different manifolds to create more descriptive\nattention maps.\nB. MANIFOLD BACKGROUND\nA manifold is a topological space that locally resembles a\nEuclidean space near each point [29]. Essentially, a manifold\nis a mapping from one space to another, allowing similar fea-\ntures to appear closer to each other, while dissimilar features\nmove further apart. Manifolds have been widely employed in\ncomputer vision tasks due to their ability to model different\nand complementary statistical and geometrical properties of\nfeatures that can be beneficial for a given task [23], [30],\n[31]. Two widely employed special types of manifolds used\nto describe image sets and videos in the literature are the SPD\nand Grassmann manifolds.\nHuang et al.utilized properties of the Riemmanian geome-\ntry on manifolds and proposed a new similarity method based\non SPD features for clustering tasks [30]. Yu et al.proposed\nthe contour covariance that lies on the SPD manifold as a\nregion descriptor for accurate image classification [32]. Sim-\nilarly, Chu et al.proposed the modelling of image sets with\ncovariance matrices for improved classification performance\n[33]. The importance and usefulness of feature modelling\non the SPD manifold can be further highlighted from the\ndesign of novel deep networks and network layers, such as\nVariational Autoencoders [34], LSTMs [35], GRUs [36] and\nmapping and pooling layers [37] to handle and learn from\nfeatures on the SPD manifold. The main drawback of employ-\ning covariance features in a deep learning framework is the\nnon-linearity of the SPD manifold that introduces challenging\noptimization problems [35].\nOn the other hand, Grassmannian geometry has been\nwidely employed in several computer vision tasks, such as\nimage classification and action recognition. Data represen-\ntations that lie on the Grassmann manifold include the Vec-\ntors of Locally Aggregated Descriptors (VLAD) that have\nbeen successfully employed for medical image classification\n[38] and the Linear Dynamic System (LDS) features that\nhave been employed for fire and smoke detection in video\nsequences [39] and skeletal sequence modelling for sign\nlanguage and action recognition [40], [41]. In a different\napproach, Dimou et al.introduced LDS features in a ResNet\narchitecture, achieving state-of-the-art performance in image\nclassification [42]. Finally, there are works on the clustering\nof image sets modelled in high-dimensional Grassmann man-\nifold through the projection to a low-dimensional space using\nan unsupervised dimensionality reduction algorithm based on\nNeighborhood Preserving Embeddings [43] or a new low rank\napproximation model that relies on the Double Nuclear norm\n[44].\nTo further leverage the discriminative power of feature rep-\nresentations in different manifolds, Wang et al.fused SPD and\nGrassmann manifold representations for clustering purposes,\nachieving state-of-the-art performance [45]. However, current\nViTs have largely overlooked alternative data representations,\ntypically focusing only on the Euclidean space of pixel inten-\nsities. To address this issue, this work proposes MMA that\ncombines feature representations in three distinctive mani-\nfolds to learn a highly descriptive attention map that can better\nidentify the important context of input images. By leveraging\nthe statistical properties of different manifolds, MMA can\nguide any Vision Transformer to better model the underlying\nstructure of the input space, leading to improved classification\nand segmentation results.\nIII. PROPOSED METHOD\nInspired by the need for more descriptive attention maps, this\nwork proposes MMA as a novel self-attention mechanism\nthat leverages the statistical and geometrical properties of\ndifferent manifolds to produce richer feature representations.\nIn contrast to the vanilla self-attention mechanism, MMA\nprojects the high-dimensional Euclidean space to the Grass-\nmann and SPD manifolds and computes the respective dis-\ntance maps on them, as shown in Fig. 1. Then, a fused atten-\ntion map is produced through the merging of the Euclidean,\nSPD and Grassmannian distance maps that is more attentive\ntowards important appearance, color and texture features of\nthe image, ultimately leading to more accurate results in\ndifferent computer vision tasks.\nA. PRELIMINARIES\nGiven an input image I ∈ RH×W ×C with height H, width W\nand C channels, a Vision Transformer divides the image into\nnon-overlapping rectangular patches of size P × P and then\nlinearly project them into the space of the hidden dimension\nD of the Transformer or employs a 2D convolution to perform\nboth image patch extraction and linear projection. The result\nis a vector of patch embeddings X ∈ RL×D, where L = HW\nP2\nis the sequence length.\nPosition embeddings are then added to the patch embed-\ndings to provide a sense of order in the input sequence,\nallowing ViT to model both the content of the patches and\ntheir spatial location with respect to other image patches. The\nmost common position embeddings are either vectors with\nsine and cosine frequencies [2] or learned embeddings [3],\n[46].\nThe embeddings are finally fed to the self-attention mech-\nanism that is the most important layer of a Transformer, re-\nsponsible for computing an output representation that is more\nattentive towards important features of the input. Vaswani\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\net al. proposed the multi-head self-attention (MHSA) that\nperforms different linear projections of the input at different\nsubspaces through parallel attention layers, called heads, con-\ncatenated together [47]. MHSA is computed as follows:\nQi = XW i\nq, Ki = XW i\nk , Vi = XW i\nv (1)\nSi = Attention(Qi, Ki, Vi), i = 1, 2, . . . ,h (2)\nMHSA(Q, K, V) = concat(S1, S2, . . . ,Sh)Wo (3)\nwhere h is the number of heads, Wo ∈ RD×D is the weight\nprojection matrix, d = D/h is the feature dimension in each\nhead, Si ∈ RL×L is the attention map of each head and\nWi\nq, Wi\nk , Wi\nv ∈ RD×d are the weight matrices for the\nquery, key and value vectors of each head i, respectively. The\nattention map of each head Si is equal to:\nSi = softmax (QiKT\ni√\nd )Vi (4)\nMHSA allows ViT to jointly attend to information from\ndifferent representation subspaces at different positions, en-\nabling it to gather more positional data because each head\nfocuses on various regions of the input and creating a more\ncomprehensive representation after the combination of the\nvectors.\nB. MULTI-MANIFOLD MULTI-HEAD ATTENTION (MMA)\nMMA can improve the performance of any ViT by replacing\nits vanilla self-attention mechanism. To achieve this, MMA\ntransforms the input sequence to points in the Euclidean, SPD\nand Grassmann manifolds and computes distances between\nthem in these manifolds in order to produce a fused attention\nmap of high discriminative power, as shown in Fig. 2. Next,\neach manifold is described in detail, along with how the\nindividual distance maps are computed and fused to form the\nrefined attention map.\n1) Euclidean Manifold\nGiven query Q ∈ RL×d and key K ∈ RL×d vectors, the\nEuclidean distance map of MMA is computed similarly to a\nvanilla Vision Transformer as:\nDE (Q, K) = QKT\n√dk\n(5)\nThe distance map DE ∈ Rh×L×L , with h representing the\nnumber of heads, expresses the similarity between query and\nkey vectors, with higher values denoting greater distances\nbetween the two vectors in the Euclidean manifold.\n2) SPD Manifold\nThe SPD manifold is a specific type of Riemann manifold\ncomposed of points expressed as square matrices M of size\nd × d and it is denoted as:\nSd\n++ = {M ∈ Rd×d : uT Mu > 0 ∀ u ∈ Rd − {0d }} (6)\nFIGURE 2. Proposed MMA architecture. The symbol⊗ denotes matrix\nmultiplication, whileBN is batch normalization.\nFor a matrix to be considered as point in a SPD mani-\nfold, it should be symmetrical and have positive eigenvalues.\nCovariance matrices possess such properties and thus they\ncan be considered points in a SPD manifold. Covariance\nmatrices have been widely employed in the literature to model\nappearance and texture features in computer vision tasks [32],\n[48]. As a result, the inclusion of covariance matrices in the\ncomputation of MMA is beneficial to the performance of a Vi-\nsion Transformer due to incorporating additional information\nabout the input, enhancing the discriminative power of the\noutput feature representation. Several metrics can be used to\nmeasure the distance between points in a SPD manifold [49],\nhowever, this work employs the Frobenius distance as it is\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\nnot restricted by the values of the elements in the covariance\nmatrices, unlike log-based distances.\nGiven query Q ∈ RL×d and key K ∈ RL×d vectors,\na learnable linear projection operation is initially employed\nto reduce the dimensionality of the vectors and improve the\ncomputational efficiency of the proposed approach. The pro-\njected query and key vectors can be defined as Qp, Kp ∈\nRL×s with s being the projection dimension. Afterwards, the\ncovariance matrices of these vectors are computed as:\nCQ = cov(Qp) = E[(Qp − E[Qp])(Qp − E[Qp])T ] (7)\nCK = cov(Kp) = E[(Kp − E[Kp])(Kp − E[Kp])T ] (8)\nDue to their properties, each of the matrices CQ, CK ∈\nRL×s×s describes a cluster of L covariance matrices that lie\nas points on the SPD manifold. The SPD distance between the\ni-th covariance matrix of query and the j-th covariance matrix\nof key is then calculated as:\nDij\nSPD(Ci\nQ, Cj\nK ) =\n||Ci\nQ − Cj\nK ||F\ns (9)\nwhere || · ||F denotes the Frobenius norm. Similar to the\nEuclidean distance map, DSPD ∈ Rh×L×L expresses the\nsimilarity between query and key vectors and quantifies the\ndistances between the two vectors in the SPD manifold.\n3) Grassmann Manifold\nThe Grassmann manifold is another well-known special type\nof Riemann manifold that embeds all p-dimensional linear\nsubspaces that lie in a d-dimensional Euclidean space. The\nGrassmann manifold, denoted as G(p, d), can be represented\nby the set of orthogonal matrices from the orthogonal group\nO(p) as follows:\nG(p, d) = {X ∈ Rd×p : XT X = Ip}/O(p), (10)\nwhere X represents any point on the Grassmann mani-\nfold. Grassmann manifolds have been commonly employed\nto model sequential and time-varying signals as any linear\ndynamic system can be easily transformed to a point in the\nGrassmann manifold [38], [40]. As a result, the transforma-\ntion of the input space to points in the Grassmann manifold\ncan provide to a Vision Transformer additional information\nregarding texture and color variations in an image patch,\nleading to enriched feature representations with high discrim-\ninative power.\nSeveral metrics have been defined to measure the distance\nbetween Grassmmanian points. The most common technique\nis to embed the manifold into the space of symmetric matrices\nwith the mapping Π : G(p, d) → Sym(d), Π(X) = XXT ,\nwhich is a one-to-one, continuous and differentiable mapping\n[50]. Moreover, to avoid the computation of the coordinates\nof all projected data and their pairwise distances as well as to\nimprove efficiency, the kernel form of the projection distance\n[51] is adopted.\nGiven query Q ∈ RL×d and key K ∈ RL×d vectors, the\ncovariance matrices of (7) and (8) are first obtained. Since\nthe orthogonality of the covariance matrices CQ and CK is\nnot guaranteed, a transformation is required to ensure that\nthe new matrices are orthogonal and thus represent points in\nthe Grassmann manifold. One of the most popular technique\nto achieve orthogonality is through QR decomposition [1],\nhowever this work employs the Cayley transform [52] due\nto its much smaller computational cost. According to this\ntransform, the covariance matrices CQ and CK are initially\ntransformed to the skew-symmetric matrices Cskew\nQ and Cskew\nK\nas follows:\nCskew\nQ = 1\n2(CQ − CT\nQ) (11)\nCskew\nK = 1\n2(CK − CT\nK ) (12)\nThen, the Cayley map is employed to the skew-symmetric\nmatrices to produce the following matrices:\nGQ = (Is + Cskew\nQ\n2 )(Is − Cskew\nQ\n2 )−1 (13)\nGK = (Is + Cskew\nK\n2 )(Is − Cskew\nK\n2 )−1 (14)\nThe matrices GQ, GK ∈ RL×s×s correspond to the repre-\nsentation of the query and key vectors in the Grassmann man-\nifold and describe clusters of L orthogonal matrices that lie\non the manifold. Finally, the projection distance is employed\nto calculate the scaled distance between the i-th orthogonal\nmatrix of query and the j-th orthogonal matrix of key in the\nGrassmann manifold as follows:\nDij\nG(Gi\nQ, Gj\nK ) =\n||Gi\nQ(Gi\nQ)T − Gj\nK (Gj\nK )T ||F\ns (15)\nwhere || · ||F denotes the Frobenius norm. As with the\nother manifold distance maps, DG ∈ Rh×L×L expresses the\nsimilarity between query and key vectors and quantifies the\ndistances between the two vectors in the Grassmann mani-\nfold.\n4) Fusion of Manifolds\nAfter the computation of the individual distance maps DE ,\nDSPD and DG ∈ Rh×L×L in each manifold, two setups,\ndenoted as early and late fusion, are proposed to derive the\noutput feature representation.\nEarly Fusion. In this setup, the three distance maps are\nconcatenated together and then a 3D convolution operation\nϱ is employed to learn an element-wise weight matrix to\nperform an effective mapping of the distances in the differ-\nent manifolds and generate the output feature representation.\nMore specifically, the three distance maps are concatenated\ntogether forming the map:\nDconcat = BN(concat(DE , DSPD, DG)) ∈ R3×L×L (16)\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\nwith BN denoting batch normalization. Afterwards, a\nweight matrix is computed to merge the different distance\nmaps in an optimal way:\nWD = softmax(LeakyRELU(ϱ(Dconcat ))) ∈ R3×L×L (17)\nThe weight matrix WD is responsible for computing ap-\npropriate weights in an element-wise manner to accurately\nmerge the different distances. Finally, the output feature rep-\nresentation V′ is computed as follows:\nV′ = softmax(sum(WD ⊗ Dconcat ))V ∈ RL×d (18)\nwhere the sum operation is applied to the first dimension\nof the product of the weight matrix and the concatenated\ndistance maps, effectively removing the first dimension.\nLate Fusion.In this setup, three parallel attention mecha-\nnisms are employed, each processing the input in a different\nmanifold, thus computing the manifold feature representa-\ntions V′\nE = softmax(DE )V, V′\nSPD = softmax(DSPD)V\nand V′\nG = softmax(DG)V for the Euclidean, SPD and\nGrassmann manifolds, respectively. Then, the output feature\nrepresentation V′ ∈ RL×d is equal to:\nV′ = L(concat(V′\nE , V′\nSPD, V′\nG)) (19)\nwhere L performs a linear projection from the size of the\nconcatenated feature representation 3d to the final size of the\nfeature representation d.\nIV. EXPERIMENTAL RESULTS\nA. IMPLEMENTATION DETAILS\nMMA was introduced in several state-of-the-art hierarchi-\ncal and compact ViTs to replace their vanilla self-attention\nmechanism. More specifically, the ViT-Lite-6/4, CVT-6/4 and\nCCT-7/3x2 models proposed in [4], the Swin-T model pro-\nposed in [7] and the UniFormer-S model proposed in [28]\nwere employed, giving rise to new models with the same\nname and the MMA-* prefix, when MMA is utilized. For\nthe Swin-T model, a patch size of 2, a window size of 4,\nan embedding dimension of 96, an mlp ratio of 2, depths\nof (2, 6, 4) and number of heads equal to (3, 6, 12) for the\ndifferent layers were selected so that the model can be trained\non the small datasets that was tested on.\nExperiments were conducted on 4 well-known image\nclassification datasets, namely C-10 [53], C-100 [53], T-\nImageNet [54] and ImageNet, as well as on a popular se-\nmantic segmentation dataset, namely ADE20K [55], [56].\nC-10 consists of 50K training and 10K test images equally\ndistributed among 10 classes, C-100 consists of 50K training\nand 10K test images equally distributed among 100 classes,\nT-ImageNet consists of 100K training and 10K validation\nimages equally distributed among 200 classes, while Ima-\ngeNet consists of more than 1.2M training and 50K valida-\ntion images distributed among 1000 classes. In T-ImageNet\nand ImageNet, the validation images were used to test the\nTABLE 1. Ablation study using the MMA-ViT-Lite-6/4 model with the early\nand late fusion of manifolds\nManifolds Accuracy (%)\nE SPD G Params (M) FLOPs (G) C-10 C-100\nEarly Fusion\n✓ 3.195 0.219 90.94 69.2\n✓ 3.197 0.214 90.49 70.38\n✓ 3.197 0.214 88.96 67.48\n✓ ✓ 3.197 0.221 91.89 72.41\n✓ ✓ 3.197 0.221 92.85 72.48\n✓ ✓ 3.197 0.215 92.03 71.35\n✓ ✓ ✓ 3.197 0.222 92.47 72.55\nLate Fusion\n✓ ✓ 3.59 0.252 91.43 71.71\n✓ ✓ 3.59 0.253 91.15 71.21\n✓ ✓ 3.59 0.246 89.3 68.25\n✓ ✓ ✓ 3.983 0.285 90.95 71.78\nperformance of ViTs. Finally, the Ade20K dataset consists of\naround 20K training and 2K test images depicting 150 classes\nof objects.\nAll experiments were run with a fixed batch size of 128\nand for 200 epochs for C-10 and C-100 and 300 epochs for\nT-ImageNet and ImageNet. For fair comparison, the imple-\nmentation of [4] was used for the experiments on C-10, C-\n100 and T-ImageNet and the implementation of [28] was used\nfor the experiments on ImageNet and ADE20K. As a result,\nthe AdamW optimizer [57] was used with a weight decay of\n0.01, a base learning rate of 5e−4 and a cosine learning rate\nscheduler that adjusts the learning rate during training [58]. A\nwarmup of 10 epochs was applied by increasing gradually the\nlearning rate from 0 to the initial value of the cosine learning\nrate scheduler [59]. Label smoothing [60] with a probability\nϵ = 0.1 was applied during training, where the true label is\nconsidered to have a probability of 1−ϵ and the probability ϵ\nis shared between the other classes. For data augmentation,\nAuto-Augment [61] was adopted to transform the training\ndata with adaptive learnable transformations, such as shift,\nrotation, and color jittering. Moreover, the Mixup strategy\n[62] was used to generate weighted combinations of random\nsample pairs from the training images.\nFinally, the projection size for the covariance matrices s\nwas set to 4, the 3D convolution layer for the computation\nof the weight matrix had a kernel of size 1 × 1 × 1 and the\nLeakyRELU function had a negative slope of 0.1. The code\nwas implemented in PyTorch and all tested transformers with\nand without the proposed MMA were trained from scratch on\na PC with 2 Nvidia 3090Ti gpus for fair comparison.\nB. ABLATION STUDY\nTo evaluate the contribution of each manifold to the perfor-\nmance of a Vision Transformer, experiments were conducted\nusing the MMA-ViT-Lite-6/4 model in the C-10 and C-100\ndatasets. In these experiments, all possible combinations of\nmanifold distance maps in early or late fusion setups were\nutilized and the results are presented in Table 1.\nFrom the results of Table 1, it can be concluded that when\nthe manifolds are utilized on their own, the Euclidean and\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\nTABLE 2. Comparative evaluation on C-10, C-100 and T-ImageNet. Parameters and FLOPs were computed for images of size 32× 32.\nAccuracy (%)\nMethod Params (M) FLOPs (G) C-10 C-100 T-ImageNet\nResNet-100 [63] 1.70 0.25 93.39 72.78 -\nResNet-164 [63] 1.73 0.26 94.54 75.67 -\nEfficientNet-B0 [64] 3.70 0.12 94.66 76.04 -\nLinformer [18] 3.96 0.28 92.45 70.87 -\nPerformer [19] 3.85 0.28 91.58 73.11 -\nReformer [20] 3.39 0.25 90.58 73.02 -\nCouplformer-7 [21] 3.85 0.28 93.44 74.53 -\nViT-Lite-6/4 [4] 3.195 0.219 90.94 69.2 49.18\nMMA-ViT-Lite-6/4 3.197 0.222 92.47 72.55 53.16\nCVT-6/4 [4] 3.195 0.216 92.58 72.25 51.45\nMMA-CVT-6/4 3.197 0.218 93.53 75.92 55.87\nSwin-T [7] 7.049 0.243 91.88 72.34 60.64\nMMA-Swin-T 7.05 0.246 92.94 73.7 61.57\nCCT-7/3×2 [4] 3.859 0.29 93.65 74.77 61.07\nMMA-CCT-7/3×2 3.861 0.293 94.74 77.5 64.41\nFIGURE 3. Class activations in C-100 using MMA-ViT-Lite-6/4. (a) Original\nimages and class activation maps using (b) Euclidean, (c) SPD, (d)\nGrassmann and (e) all three manifolds.\nSPD manifolds perform similarly and slightly better than\nthe Grassmann manifold. However, when two or more man-\nifolds are combined, the performance of MMA-ViT-Lite-6/4\nis significantly improved in both C-10 and C-100 datasets.\nThe best results are achieved when all three manifolds are\nemployed, as it is seen from the performance of MMA-ViT-\nLite-6/4 in the more challenging C-100 dataset. Noteworthy\nis the fact that the increase in the performance of MMA-\nViT-Lite-6/4 is simultaneously accompanied by a negligible\nincrease in the number of network parameters and a small\nincrease in the number of floating point operations (FLOPs).\nSimilar observations can be made when the late fusion of the\nmanifold representations is performed. These results verify\nthat the SPD and Grassmann manifolds contain different\nand supplementary information to the Euclidean manifold\nthrough the modelling of the appearance, color and texture\nvariations in images. This information can guide a Vision\nTransformer to become more attentive towards a better mod-\nelling of the underlying input space, enabling it to achieve\nimproved performance.\nOn the other hand, a comparison between early and late\nfusion of manifolds reveals that early fusion leads to superior\nperformance in both datasets, while utilizing almost 25%\nfewer network parameters and around 28% fewer FLOPs.\nThese results show that fusing the manifold distance maps\ninside the Transformer ensures the generation of a refined\nattention map with high discriminative power.\nTo further highlight the benefits of employing additional\nmanifolds, a visualization of class activation maps in a few\nimages from the C-100 dataset is illustrated in Fig. 3. It can\nbe observed that although the vanilla self-attention allows the\nmodel to pay attention on the ears of a kangaroo or the hump\nof a camel, the proposed MMA enables the model to become\nhighly attentive towards additional information, such as the\nentire body of the kangaroo and both the hump and the legs\nof a camel, thus increasing the confidence of the model in\nits classification results. For the rest of the experiments, it is\nassumed that the proposed MMA employs the early fusion of\nall three manifolds since this combination leads to the optimal\nperformance.\nC. IMAGE CLASSIFICATION ON SMALL DATASETS\nTo demonstrate the benefits of MMA, a comparative eval-\nuation of various hierarchical and compact ViTs with their\nvanilla self-attention and the proposed MMA in C-10, C-\n100 and T-ImageNet is performed. In addition, the selected\nTransformers are compared against other CNN networks and\nViTs and the results are presented in Table 2. The results\ndemonstrate that the MMA-enhanced ViTs achieve superior\nperformance in all datasets with respect to their original\nversions at the expense of a small increase in parameters\nand FLOPs. A comparison with other state-of-the-art deep\nnetworks shows that MMA-CCT-7/3x2 achieves the highest\nperformance in all datasets, outperforming other ViTs and\neven well-established CNN networks, such as ResNet-164\n[63] and EfficientNet-B0 [64].\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\nFIGURE 4. Class activations in T-ImageNet. (a) Original images and class activation maps using (b) CVT-6/4, (c) MMA-CVT-6/4, (d) Swin-T, (e) MMA-Swin-T,\n(f) CCT-7-3x2 and (g) MMA-CCT-7-3x2 models.\nFIGURE 5. Impact of MMA on tested ViT variants trained on C-100 and\nT-ImageNet in terms of model performance (MP) and generalization\nability (GA).\nFurthermore, Fig. 4 depicts the class activation maps for\nCVT-6/4, Swin-T and CCT-7/3x2, and their MMA-* coun-\nterparts. It can be deduced that the MMA-enhanced ViTs\nbecome more attentive towards significant parts of the object\nof interest, such as the legs of a spider, the arch rib of a\nbridge and the eyes and noise of a dog, thus leading to more\naccurate classification results. Additional conclusions can be\ndrawn from Fig. 5 that presents the impact of MMA on the\ntested ViTs in terms of model performance and generalization\nability. Irrespective of their network architecture, all ViTs are\nsignificantly improved with MMA through a decrease in the\nFIGURE 6. Visualization using t-SNE of outputs from (a) Swin-T, (b)\nMMA-Swin-T, (c) CCT-7-3x2 and (d) MMA-CCT-7-3x2 of 10 random classes\nof T-ImageNet.\ntraining and validation losses and an increase in the validation\naccuracy.\nFinally, Fig. 6 depicts the distribution of 10 random classes\nof T-ImageNet using the t-distributed stochastic neighbor\nembedding (t-SNE) algorithm [65] that is suitable for vi-\nsualizing high-dimensional data by applying nonlinear di-\nmensionality reduction. To this end, the feature vectors after\nthe head concatenation of (3) are employed as the high-\ndimensional input to the t-SNE algorithm, which then com-\nputes a two-dimensional output. From the distribution of the\n10 T-ImageNet classes in the 2D space, it can be observed that\nMMA leads to more compact classes (i.e., points of the same\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\nFIGURE 7. Activation maps from UniFormer-S (first row) and\nMMA-UniFormer-S (second row) for the classes (a) black widow spider,\n(b-c) mountain bike, (d) steel arch bridge and (e) warplane.\nTABLE 3. Comparative evaluation on ImageNet with images of size\n224 × 224.\nMethod Arch. Params (M) FLOPs (G) Top-1 acc.\nResNet-50 [66] 25.6 3.8 76.7\nEfficientNet-B0 [64] 5.3 0.4 77.1\nResNet-101 [66] CNN 44.5 7.6 78.3\nResNet-152 [66] 60.2 11.3 78.9\nConvNeXt-T [67] 28.0 4.5 82.1\nPVT-S [10] 24.5 3.8 79.8\nDeiT-S [14] 22.1 4.6 79.9\nTwins-PCPVT-S [17] 24.1 3.8 81.2\nSwin-T [7] 29.0 4.5 81.3\nConViT-S [3] 27.0 5.4 81.3\nPoolFormer-S36 [22] 30.8 5.0 81.4\nT2T-ViT-14 [6] 21.5 5.2 81.5\nCrossformer-T [25] 27.8 2.9 81.5\nTwins-SVT-S [17] ViT 24.0 2.9 81.7\nSwinV2-T [24] 28.0 5.9 81.8\nDAT-T [27] 28.3 4.6 82.0\nCeiT-S [5] 24.2 4.5 82.0\nViTAE-S [26] 23.6 5.6 82.0\nUniFormer-S [28] 21.5 3.6 82.3\nCrossformer-S [25] 30.7 4.9 82.5\nMMA-UniFormer-S 21.5 3.7 82.6\nclass closer to each other) and less stray points for all tested\nViTs. All these results verify the ability of MMA to guide any\nVision Transformer to accurately model the underlying input\nspace and produce highly descriptive output representations.\nThis is achieved through the fusion of different and com-\nplementary manifold representations that enables a Vision\nTransformer to model the important context of an image\nthrough a refined attention map of high discriminative power.\nAs a result, the proposed MMA can effectively substitute\nthe vanilla self-attention in Vision Transformers, significantly\nimproving their classification performance.\nD. IMAGE CLASSIFICATION ON IMAGENET\nTo demonstrate the ability of the proposed MMA to improve\nthe performance of Vision Transformers even on large and\nmore challenging image classification datasets, such as Ima-\ngeNet, we employed one of the state-of-the-art ViTs, namely\nUniFormer [28] and we replaced the attention mechanism of\nits small version (i.e., UniFormer-S) with the proposed MMA,\nthus obtaining the MMA-UniFormer-S version of the model.\nTable 3 presents the performance improvement achieved after\nthe modification, as well as performs a comparison against\nTABLE 4. Comparative evaluation on semantic segmentation using\nADE20K.\nBackbone Head Params (M) FLOPs (G) mIoU\nResNet50 [66] FPN 29 183 36.7\nPVT-S [10] FPN 28 161 39.8\nSwin-T [7] FPN 32 182 41.5\nDAT-T [27] FPN 32 198 42.6\nTwins-S [17] FPN 28 144 43.2\nTwins-PCPVT-S [17] FPN 28 162 44.3\nUniFormer-S [28] FPN 25 247 46.0\nDAT-S [27] FPN 53 320 46.1\nCrossFormer-S [25] FPN 34 210 46.4\nMMA-UniFormer-S FPN 25 259 46.9\nother state-of-the-art small-sized Vision Transformers.\nFrom the results, it can be seen that the MMA-UniFormer-\nS outperforms UniFormer-S by 0.3% on ImageNet, when\nthe proposed multi-manifold attention is introduced, manag-\ning to achieve a state-of-the-art performance on ImageNet\nwith an accuracy of 82.6%. The results verify the ability of\nthe proposed MMA to improve the performance of Vision\nTransformers in larger and more challenging datasets. A vi-\nsualization of the attention maps extracted from the original\nUniFormer-S and the modified MMA-UniFormer-S models\non a few images of ImageNet is presented in Fig. 7. This\nfigure further demonstrates that the use of additional mani-\nfolds in the attention mechanism of a Vision Transformer can\nlead to a more refined and descriptive attention map that can\nbetter describe the content of an image. Of notable importance\nare the cases of mountain bike identification (Fig. 7(b-c)), in\nwhich it can be seen that the MMA-UniFormer-S manages to\ncorrectly identify the object of interest, overcoming the mis-\nclassification errors performed by the UniFormer-S.\nE. SEMANTIC SEGMENTATION ON ADE20K\nTo further evaluate the ability of the proposed MMA mech-\nanism to improve the performance of Vision Transformers\non tasks different from image classification, we train and\nevaluate the MMA-UniFormer-S on semantic segmentation\nusing the challenging scene parsing ADE20K dataset. More\nspecifically, we employ the MMA-UniFormer-S, pre-trained\non ImageNet, as a backbone to the semantic FPN network\n[10] and we employ the training scheme of the UniFormer\nwith 80K iterations for fair comparison among the two mod-\nels.\nFrom Table 4, it can be seen that the introduction of MMA-\nUniFormer-S as a backbone to the semantic FPN network\ncan lead to significant improvements in the performance of\nthe model in semantic segmentation. More specifically, the\nuse of MMA-UniFormer-S leads to an improvement of 0.9%\nin mIoU with respect to UniFormer-S, thus leading the net-\nwork to achieve state-of-the-art performance with 46.9 mIoU,\nwhile retaining the smallest number of network parameters\n(25M) among other backbones. Finally, Fig. 8 visualizes a\nfew segmentation results from the ADE20K dataset. These\nresults further verify the higher accuracy achieved when the\nproposed MMA is introduced to the backbone model of a\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\nFIGURE 8. Semantic segmentation results in ADE20K. The first row depicts the original images, the second row shows the ground truth segmentation,\nwhile the third and fourth rows depict the segmentation results from the UniFormer-S + FPN and the MMA-UniFormer-S + FPN networks, respectively.\nsemantic segmentation network with the result being more\naccurately delineated objects. As a result, the proposed MMA\ncan lead to performance improvements in both image classi-\nfication and semantic segmentation with a minimal increase\nin network parameters and a slight increase in FLOPs.\nV. CONCLUSION\nThis work proposes MMA as a novel self-attention mecha-\nnism that is suitable for any Vision Transformer irrespective\nof its architecture. The motivation behind MMA lies in the\nuse of three distinct manifolds, namely Euclidean, SPD and\nGrassmann to model the input and produce a fused attention\nmap that can more accurately attend to the important con-\ntext of the input. Experimental results with hierarchical and\ncompact ViT variants on several image classification datasets\n(i.e., C-10, C-100, T-ImageNet and ImageNet) and a chal-\nlenging semantic segmentation dataset (i.e., ADE20K) verify\nthe effectiveness of MMA in producing highly descriptive\noutput representations and improving the performance of Vi-\nsion Transformers in both image classification and semantic\nsegmentation.\nREFERENCES\n[1] Åke Björck. Solving linear least squares problems by gram-schmidt\northogonalization. BIT Numerical Mathematics, 7(1):1–21, 1967.\n[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In ICLR, 2020.\n[3] Stéphane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos,\nGiulio Biroli, and Levent Sagun. Convit: Improving vision transformers\nwith soft convolutional inductive biases. In International Conference on\nMachine Learning, pages 2286–2296. PMLR, 2021.\n[4] Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen\nLi, and Humphrey Shi. Escaping the big data paradigm with compact\ntransformers. arXiv preprint arXiv:2104.05704, 2021.\n[5] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei\nWu. Incorporating convolution designs into visual transformers. In ICCV,\npages 579–588, October 2021.\n[6] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,\nFrancis E.H. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit:\nTraining vision transformers from scratch on imagenet. In ICCV, pages\n558–567, October 2021.\n[7] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen\nLin, and Baining Guo. Swin transformer: Hierarchical vision transformer\nusing shifted windows. In ICCV, pages 10012–10022, October 2021.\n[8] Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya, Jan Kautz, and\nPavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer.\nIn CVPR, pages 10809–10818, 2022.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\n[9] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang\nWang, Wenyu Liu, Gang Yu, and Chunhua Shen. Topformer: Token\npyramid transformer for mobile semantic segmentation. In CVPR, pages\n12083–12093, 2022.\n[10] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding\nLiang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer:\nA versatile backbone for dense prediction without convolutions. In ICCV,\npages 568–578, October 2021.\n[11] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk\nChoe, and Seong Joon Oh. Rethinking spatial dimensions of vision\ntransformers. In ICCV, pages 11936–11945, October 2021.\n[12] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian,\nZihang Jiang, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision\ntransformer. arXiv preprint arXiv:2103.11886, 2021.\n[13] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir,\nFahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A\nsurvey. ACM computing surveys (CSUR), 54(10s):1–41, 2022.\n[14] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexan-\ndre Sablayrolles, and Hervé Jégou. Training data-efficient image trans-\nformers & distillation through attention. In ICML, pages 10347–10357.\nPMLR, 2021.\n[15] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe\nWang. Transformer in transformer. Advances in Neural Information\nProcessing Systems, 34:15908–15919, 2021.\n[16] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai\nYu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general\nvision transformer backbone with cross-shaped windows. In CVPR, pages\n12124–12134, 2022.\n[17] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin\nWei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of\nspatial attention in vision transformers. Advances in Neural Information\nProcessing Systems, 34:9355–9366, 2021.\n[18] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao\nMa. Linformer: Self-attention with linear complexity. arXiv preprint\narXiv:2006.04768, 2020.\n[19] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan,\nXingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy\nDavis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with\nperformers. In ICLR, 2020.\n[20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The\nefficient transformer. In ICLR, 2019.\n[21] Hai Lan, Xihao Wang, and Xian Wei. Couplformer: Rethink-\ning vision transformer with coupling attention map. arXiv preprint\narXiv:2112.05425, 2021.\n[22] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang,\nJiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for\nvision. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 10819–10829, 2022.\n[23] Yao Ni, Piotr Koniusz, Richard Hartley, and Richard Nock. Manifold\nlearning benefits gans. In CVPR, pages 11265–11274, 2022.\n[24] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia\nNing, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling\nup capacity and resolution. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 12009–12019, 2022.\n[25] Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He,\nand Wei Liu. Crossformer: A versatile vision transformer hinging on cross-\nscale attention. In International Conference on Learning Representations,\n2021.\n[26] Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Vitae: Vision\ntransformer advanced by exploring intrinsic inductive bias. Advances in\nneural information processing systems, 34:28522–28535, 2021.\n[27] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision\ntransformer with deformable attention. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 4794–4803,\n2022.\n[28] Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li,\nand Yu Qiao. Uniformer: Unified transformer for efficient spatiotemporal\nrepresentation learning. pages 1–19, 2022.\n[29] Anastasis Kratsios and Ievgen Bilokopytov. Non-euclidean universal\napproximation. Advances in Neural Information Processing Systems,\n33:10635–10646, 2020.\n[30] Zhiwu Huang, Ruiping Wang, Xianqiu Li, Wenxian Liu, Shiguang Shan,\nLuc Van Gool, and Xilin Chen. Geometry-aware similarity learning on spd\nmanifolds for visual recognition. IEEE TCSVT, 28(10):2513–2523, 2017.\n[31] Xuan Son Nguyen, Luc Brun, Olivier Lézoray, and Sébastien Bougleux.\nA neural network based on spd manifold learning for skeleton-based hand\ngesture recognition. In CVPR, pages 12036–12045, 2019.\n[32] Xiaohan Yu, Shengwu Xiong, Yongsheng Gao, and Xiaohui Yuan. Contour\ncovariance: A fast descriptor for classification. In ICIP, pages 569–573.\nIEEE, 2019.\n[33] Li Chu, Rui Wang, and Xiao-Jun Wu. Collaborative representation for\nspd matrices with application to image-set classification. arXiv preprint\narXiv:2201.08962, 2022.\n[34] Nina Miolane and Susan Holmes. Learning weighted submanifolds with\nvariational autoencoders and riemannian variational autoencoders. In\nCVPR, pages 14503–14511, 2020.\n[35] Zhi Gao, Yuwei Wu, Yunde Jia, and Mehrtash Harandi. Learning to\noptimize on spd manifolds. In CVPR, pages 7700–7709, 2020.\n[36] Seungwoo Jeong, Wonjun Ko, Ahmad Wisnu Mulyadi, and Heung-Il Suk.\nEfficient continuous manifold learning for time series modeling. arXiv\npreprint arXiv:2112.03379, 2021.\n[37] Rui Wang, Xiao-Jun Wu, and Josef Kittler. Symnet: A simple symmetric\npositive definite manifold deep learning method for image set classifica-\ntion. IEEE Transactions on Neural Networks and Learning Systems, 2021.\n[38] Kosmas Dimitropoulos, Panagiotis Barmpoutis, Christina Zioga, Athana-\nsios Kamas, Kalliopi Patsiaoura, and Nikos Grammalidis. Grading of\ninvasive breast carcinoma through grassmannian vlad encoding. PloS one,\n12(9):e0185110, 2017.\n[39] Kosmas Dimitropoulos, Panagiotis Barmpoutis, Alexandros Kitsikidis, and\nNikos Grammalidis. Classification of multidimensional time-evolving data\nusing histograms of grassmannian points. TCSVT, 28(4):892–905, 2016.\n[40] Dimitrios Konstantinidis, Kosmas Dimitropoulos, and Petros Daras.\nSkeleton-based action recognition based on deep learning and grassman-\nnian pyramids. In EUSIPCO, pages 2045–2049. IEEE, 2018.\n[41] Dimitrios Konstantinidis, Kosmas Dimitropoulos, and Petros Daras. A\ndeep learning approach for analyzing video and skeletal features in sign\nlanguage recognition. In IEEE international conference on imaging sys-\ntems and techniques (IST), pages 1–6. IEEE, 2018.\n[42] Anastasios Dimou, Dimitrios Ataloglou, Kosmas Dimitropoulos, Federico\nAlvarez, and Petros Daras. Lds-inspired residual networks. TCSVT,\n29(8):2363–2375, 2018.\n[43] Dong Wei, Xiaobo Shen, Quansen Sun, Xizhan Gao, and Zhenwen Ren.\nNeighborhood preserving embedding on grassmann manifold for image-\nset analysis. Pattern Recognition, 122:108335, 2022.\n[44] Xinglin Piao, Yongli Hu, Junbin Gao, Yanfeng Sun, and Baocai Yin. Dou-\nble nuclear norm based low rank representation on grassmann manifolds\nfor clustering. In CVPR, pages 12075–12084, 2019.\n[45] Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Fujiao Ju, and Baocai\nYin. Adaptive fusion of heterogeneous manifolds for subspace clus-\ntering. IEEE Transactions on Neural Networks and Learning Systems,\n32(8):3484–3497, 2020.\n[46] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia\nXia, and Chunhua Shen. Conditional positional encodings for vision\ntransformers. arXiv preprint arXiv:2102.10882, 2021.\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In Advances in neural information processing systems,\npages 5998–6008, 2017.\n[48] Subhabrata Bhattacharya, Nasim Souly, and Mubarak Shah. Covariance of\nmotion and appearance featuresfor spatio temporal recognition tasks. arXiv\npreprint arXiv:1606.05355, 2016.\n[49] Raviteja Vemulapalli and David W Jacobs. Riemannian metric learning\nfor symmetric positive definite matrices. arXiv preprint arXiv:1501.02393,\n2015.\n[50] Mehrtash Harandi, Conrad Sanderson, Chunhua Shen, and Brian C Lovell.\nDictionary learning and sparse coding on grassmann manifolds: An extrin-\nsic solution. In ICCV, pages 3120–3127, 2013.\n[51] Jiayao Zhang, Guangxu Zhu, Robert W Heath Jr, and Kaibin Huang.\nGrassmannian learning: Embedding geometry awareness in shallow and\ndeep learning. arXiv preprint arXiv:1808.02229, 2018.\n[52] Asher Trockman and J Zico Kolter. Orthogonalizing convolutional layers\nwith the cayley transform. In International Conference on Learning\nRepresentations, 2020.\n[53] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of\nfeatures from tiny images. Technical report, University of Toronto, 2009.\n[54] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS\n231N, 7(7):3, 2015.\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nKonstantinidis et al.: Multi-manifold Attention for Vision Transformers\n[55] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and\nAntonio Torralba. Scene parsing through ade20k dataset. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages\n633–641, 2017.\n[56] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Semantic understanding of scenes through\nthe ade20k dataset. International Journal of Computer Vision, 127:302–\n321, 2019.\n[57] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\nIn ICLR, 2018.\n[58] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and\nMu Li. Bag of tricks for image classification with convolutional neural\nnetworks. In CVPR, pages 558–567, 2019.\n[59] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz\nWesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming\nHe. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv\npreprint arXiv:1706.02677, 2017.\n[60] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbig-\nniew Wojna. Rethinking the inception architecture for computer vision. In\nCVPR, pages 2818–2826, 2016.\n[61] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and\nQuoc V Le. Autoaugment: Learning augmentation strategies from data.\nIn CVPR, pages 113–123, 2019.\n[62] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz.\nmixup: Beyond empirical risk minimization. In ICLR, 2018.\n[63] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity\nmappings in deep residual networks. In ECCV, pages 630–645. Springer,\n2016.\n[64] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for\nconvolutional neural networks. In ICML, pages 6105–6114. PMLR, 2019.\n[65] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne.\nJournal of machine learning research, 9(11), 2008.\n[66] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–778, 2016.\n[67] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor\nDarrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages\n11976–11986, 2022.\nDIMITRIOS KONSTANTINIDIS received the\nB.S. degree in Electrical and Electronic Engineer-\ning from AUTH in 2009. He then received an\nAdvanced M.S. degree in in Artificial Intelligence\nfrom KU Leuven in 2012 and a PhD from the\nImperial College of London with the topic of moni-\ntoring urban changes from satellite images in 2017.\nHe is currently working at Information Technolo-\ngies Institute (ITI) of the Centre for Research and\nTechnology Hellas (CERTH) as a postdoctoral re-\nsearcher. His research interests lie in the fields of computer vision, image\nprocessing, machine learning and artificial intelligence.\nILIAS PAPASTRATISis a research assistant of the\nVisual Computing Lab (VCL) at the Information\nTechnologies Institute of CERTH. He received his\nB.S. degree in Electrical and Computer Engineer-\ning from University of Patras in 2018 and his M.S.\ndegree on Digital Media and Computational Intel-\nligence from Aristotle University of Thessaloniki\nin 2022. His main research interests lie in the\nfields of computer vision, machine/deep learning\napplications and robotics.\nKOSMAS DIMITROPOULOSis a Researcher of\nGrade C’ at Information Technologies Institute\n(ITI) of the Centre for Research and Technol-\nogy Hellas (CERTH), the academic director of\nAIMOVE Post-Master’s Programme for CERTH\n(AIMove – “Artificial Intelligence and Movement\nin Industries and Creation”) at MINES Paris-\nTech/PSL University and a visiting professor at the\ndepartment of Mechanical Engineering of Aristo-\ntle University of Thessaloniki (AUTH). He holds a\nB.S. degree in Electrical and Computer Engineering and a Ph.D. degree in\nApplied Informatics. His main research interests include multi-dimensional\ndata modelling and analysis, artificial intelligence, pattern recognition, hu-\nman computer interaction and virtual reality. Kosmas Dimitropoulos has been\ninvolved in several European and national research projects and he has served\nas a regular reviewer for a number of international journals and conferences.\nHe is Associate Editor for Frontiers in Robotics and AI.\nPETROS DARAS is a Research Director (Grade\nA’) of the Visual Computing Lab at the Informa-\ntion Technologies Institute of CERTH. He received\nthe B.S. degree in Electrical and Computer Engi-\nneering, the M.S. degree in Medical Informatics\nand the Ph.D. degree in Electrical and Computer\nEngineering all from the Aristotle University of\nThessaloniki, Greece in 1999, 2002 and 2005,\nrespectively. His main research interests include\n3D object recognition, indexing, classification and\nretrieval, real-time 3D reconstruction of dynamic scenes, compression and\ncoding of 3D meshes, bioinformatics and medical image processing. His\ninvolvement with those research areas has led to the co-authoring of 70 papers\nin refereed journals (of which 33 in IEEE Journals), 194 in international\nconferences, 35 book chapters and 4 books. He has been involved in more\nthan 59 projects, funded by the EC and the Greek Ministry of Research\nand Technology, having attracted as Researcher (July 2006 - May 2020):\n21.487.553,18 EUR. He is a Senior Member of IEEE and member of the\nTechnical Chamber of Greece. He served as chair of the IEEE Interest Group\non Image, Video and Mesh Coding (2012-2014) and as a Key member of the\nIEEE Interest Group on 3D Rendering, Processing and Communications of\nIEEE Multimedia (2010-today). He regularly acts as a reviewer/evaluator for\nthe European Commission; he served as coordinator of the Future Media and\n3D Internet - Task Force, the \"User Centric Media\" cluster, and member of\nthe \"3D Media\" cluster, the Future Internet Architectures (FIArch) group and\nthe Future Internet Assembly of the Networked Media unit of the EC.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329952\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}