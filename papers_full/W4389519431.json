{
  "title": "ALCUNA: Large Language Models Meet New Knowledge",
  "url": "https://openalex.org/W4389519431",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4320551213",
      "name": "Xunjian Yin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5059992938",
      "name": "Baizhou Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101284925",
      "name": "Xiao-jun Wan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W4385572901",
    "https://openalex.org/W3197499505",
    "https://openalex.org/W4205179624",
    "https://openalex.org/W4367860080",
    "https://openalex.org/W4389523675",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4385570777",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W4365601026",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3217756080",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W1654805921",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4311991782",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2074278824",
    "https://openalex.org/W2124282646",
    "https://openalex.org/W3154151289",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4309216591",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4319453112"
  ],
  "abstract": "With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models' capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs' abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model's understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1397–1414\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nALCUNA : Large Language Models Meet New Knowledge\nXunjian Yin∗and Baizhou Huang∗and Xiaojun Wan\nWangxuan Institute of Computer Technology, Peking University\nCenter for Data Science, Peking University\nThe MOE Key Laboratory of Computational Linguistics, Peking University\n{xjyin,hbz19,wanxiaojun}@pku.edu.cn\nAbstract\nWith the rapid development of NLP, large-scale\nlanguage models (LLMs) excel in various tasks\nacross multiple domains now. However, exist-\ning benchmarks may not adequately measure\nthese models’ capabilities, especially when\nfaced with new knowledge. In this paper, we ad-\ndress the lack of benchmarks to evaluate LLMs’\nability to handle new knowledge, an important\nand challenging aspect in the rapidly evolving\nworld. We propose an approach called Know-\nGen that generates new knowledge by altering\nexisting entity attributes and relationships, re-\nsulting in artificial entities that are distinct from\nreal-world entities. With KnowGen, we intro-\nduce a benchmark named ALCUNA to assess\nLLMs’ abilities in knowledge understanding,\ndifferentiation, and association. We benchmark\nseveral LLMs, reveals that their performance\nin face of new knowledge is not satisfactory,\nparticularly in reasoning between new and in-\nternal knowledge. We also explore the impact\nof entity similarity on the model’s understand-\ning of entity knowledge and the influence of\ncontextual entities. We appeal to the need for\ncaution when using LLMs in new scenarios or\nwith new knowledge, and hope that our bench-\nmarks can help drive the development of LLMs\nin face of new knowledge.\n1 Introduction\nLarge-scale language models (LLMs) have made\nimpressive progress in the last few years (Brown\net al., 2020; Ouyang et al., 2022; Touvron et al.,\n2023; OpenAI, 2023), which perform surprisingly\nwell on various tasks on various domains, to the\nextent that many traditional benchmarks (Thorne\net al., 2018; Wang et al., 2018) are no longer suffi-\ncient to measure the capabilities of LLMs. There-\nfore, some new benchmarks have been proposed\nto evaluate the ability of the model to solve more\ncomplex tasks such as college entrance exams, law\n*These authors contributed equally to this work.\nschool admission tests, math competitions and so\non (Hendrycks et al., 2021; Guo et al., 2023; Zhong\net al., 2023). LLMs also achieve promising results\non these benchmarks.\nHowever, it is surprising that there is not yet a\nbenchmark to evaluate the ability of large models\nin face of new knowledge, which is very important\nand challenging. Why is this evaluation important?\nFirstly, we are in a changing world, where models\nencounter new knowledge frequently in practice.\nAnd some work (Peng et al., 2023) is exploring\nretrieval methods to augment large models, which\nwill also cause the models to meet new knowledge\nfrequently. So of course we expect the model to\nperform well in such situations, because re-training\nthe model every time is very expensive and unre-\nalistic. Secondly, as Elangovan et al. (2021) men-\ntioned, the presence of overlap between the train-\ning and test data can lead to a misestimation of the\nmodel’s memory ability as generalization ability,\nespecially nowadays when LLMs are trained on an\nenormous amount of data. Whereas, evaluation on\nnew knowledge does not need to worry about such\ndata leakage, as new knowledge can usually lead\nto new data and thus more reliable and valuable\nassessment of the model’s ability.\nWhile such evaluation is important, it is chal-\nlenging to construct the benchmark. The reason\nis that it is difficult to ensure that the knowledge\ncontained in the benchmark is new for LLMs, since\ntraining data for some models are large and non-\npublicly available. Furthermore, it is also difficult\nto ensure that the knowledge used for benchmark-\ning will be not outdated and inefficient, as there\nare many LLMs that may soon include data from\nthe benchmark in their training. In summary, such\nbenchmark for new knowledge needs to exhibit\nthree basic characteristics: it contains enough new\nknowledge for sufficient evaluation (sufficient), the\nknowledge is new to all models (model-agnostic)\nand the knowledge can remain new for a long time\n1397\n(long-lasting).\nThere are several possible solutions to the above\nchallenges. One option is to always use the most\nupdated data such as the news of the day (temporal\nknowledge). However, this is both labor-intensive\nto race against the LLM training and unclear about\nthe lifetime of the proposed data. Another option is\nto keep the benchmark closed-source, with an au-\nthoritative committee managing the data and users\ncalling API when evaluating, thus preventing using\nthem for training LLMs . To reach this point further\nrequires community coordination.\nTo better address these challenges, we propose\nan approach to GENerate new KNOWledge conve-\nniently (KnowGen) based on the structured repre-\nsentation of existing entity knowledge by making\nreasonable changes to entity attributes and rela-\ntionships. There are differences and associations\nbetween artificial entities and existing entities. Par-\nticularly, we apply KnowGen with structured bi-\nological taxonomic information data to rationally\ncreate a group of organisms that do not exist in\nthe world. To test the model’s ability in face of\nnew knowledge, we construct a variety of ques-\ntions about these artificial entities that can examine\nthe model’s ability to understand new knowledge\n(Knowledge Understanding), distinguish between\nmodel’s internal knowledge and new knowledge\n(Knowledge Differentiation) and make multi-hop\nreasoning by linking model’s internal and new\nknowledge ( Knowledge Association ). We use\nthe ArtificialLy ConstrUcted kNowledge to Assess\nLLMs as a benchmark (ALCUNA ).\nWe evaluate and analyze several popular large\nmodels based on ALCUNA, including ChatGPT1,\nAlpaca, Vicuna, and ChatGLM with vanilla, CoT\n(Chain-of-Thought), Zero-Shot and Few-Shot set-\ntings (Brown et al., 2020; Kojima et al., 2023; Wei\net al., 2023). We find that neither ChatGPT nor\nother models perform very well in face of new\nknowledge. ChatGPT does a good job of under-\nstanding and differentiating new knowledge, but\nalmost all models fail to reason between the new\nknowledge and the internal knowledge. This re-\nminds us to remain cautious when large models\nencounter new scenarios and knowledge. In addi-\ntion, we explore the impact of entity similarity on\nthe model’s understanding of entity knowledge, the\nimpact of contextual entities, etc.\nThe contributions of our work are listed below:\n1https://chat.openai.com/chat\n1) we propose a new method KnowGen to generate\nnew knowledge for simulating real scenarios. 2)\nwe apply our method to produce an artificial bio-\nlogical entity knowledge dataset, ALCUNA, as a\nbenchmark for evaluating the performance of mod-\nels in face of new knowledge. 3) we evaluate and\nanalyze several popular large models and obtain\ninsightful observations and conclusions.\nOur benchmark has been released to the commu-\nnity to facilitate future research 2.\nAlgorithm 1: Knowledge Generation\ninput : One Class C\noutput :Property Set T(˜e) of ˜e\nep ←RandomSelect(C) ;\nEpsb ←sib(ep)\n// Get the triplet set for heredity, variation, dropout\nand extension\nTh\nR , Tv\nR , Td\nR ←RandomSplit(TR(ep))\nTh\nA , Tv\nA , Td\nA ←RandomSplit(TA(ep))\nTe ←RandomSample(T(Epsb))\n// Heredity and Dropout\nTR(˜e) ←TR(C) ∪Th\nR ;\nTA(˜e) ←TA(C) ∪Th\nA\n// Variation: replacing the object with an entity from\nthe same class\nfor (ep, r, e′) in Tv\nR do\nE′sb ←sib(e′)\ne′sb ←RandomSelect(E′sb)\nTR(˜e) ←TR(˜e) ∪{(˜e, r, e′sb)}\n// Variation: add gaussian noise to the value or copy\nfrom Epsb\nfor (ep, a, v) in Tv\nA do\nif isnum(v) then\n˜v ←v + N(0, v/10)\nelse\nepsb ←RandomSelect(Epsb)\n˜v ←GetValue(epsb, a)\nTA(˜e) ←TA(˜e) ∪{(˜e, a,˜v)}\n// Extension and get final property\nT(˜e) ←TA(˜e) ∪TR(˜e) ∪Te\n2 KnowGen: Knowledge Generation\nIn this section, we begin by presenting our inspira-\ntion, then formally introduce our knowledge gener-\nation method KnowGen.\n2.1 Inspiration\nAccording to the ontological form (Sowa, 1995;\nNoy et al., 2001), we can represent most knowledge\n2https://github.com/Arvid-pku/ALCUNA\n1398\nas entities, the classes to which they belong, their\nattributes and the relations between them. And in-\nspired by organisms: organisms can produce organ-\nisms with new properties naturally through heredity\nand variation. Can knowledge also be \"inherited\"\nand \"varied\" in a broader context?Different enti-\nties of the same class have different properties as\nwell as commonalities. Generally speaking, enti-\nties of the same class are similar to some extent,\nwhile they have some different properties. By anal-\nogy with biological terminology, we refer to this\ncharacteristic of knowledge of similar entities as\n\"hybridizability\". This inspires us to use different\nentities of the same class to fuse their properties,\nsimulating the process of biological inheritance and\nvariation, to generate new entity knowledge.\nIn the following we will formally define and\ndescribe how knowledge is \"inherited\" and \"varied\"\nin our approach.\n2.2 Knowledge Formalization\nBased on the design of the ontology, We repre-\nsent knowledge from the viewpoint of Entity. En-\ntity e ∈E is a distinct and identifiable object or\nindividual in the world. Each entity can possess\nvarious attributes a ∈A, which describe the prop-\nerties of the entity with a value v ∈ V. At the\nsame time, each entity can participate in relations\nr ∈R with other entities. Both the attributes and\nrelations of entity e can be represented as a set\nof property triplets: {(e, a, v)}= TA(e) ⊂ TA\nand {(e, r, e′)} = TR(e) ⊂ TR. Entities with\nsimilar characteristics may be aggregated into a\nclass C ⊂ E. For convenience, we denote the\nsame properties across all entities in class C as\nT(C) = TA(C) ∪TR(C). Without any special\ndescription, the triplet T(e) of entity e refers to its\nunique properties.\nFor example, Figure 1 shows an example in the\nform of such structured knowledge, where both\nAlpaca and Vicuna are existing entities belonging\nto the class Camels and Alcuna is an artificial entity\ngenerated by us. Alpaca has attributes such as\n\"Body mass\" and relations such as \"Eaten by\", and\ncan be formed into triplets such as (Alpaca, Body\nmass, 60kg) and (Alpaca, Eaten by, Cougar).\n2.3 Construction of New Entities\nFocusing on knowledge in the form of ontology, we\naim to construct artificial entities reasonably to ac-\ncelerate the process of new knowledge generation\nin the real world. When creating an artificial entity\nwithin a specific class, it must adhere to certain\ncommon properties shared by other entities in the\nsame class. Furthermore, it is essential for the arti-\nficial entity to possess some unique properties that\ndifferentiate it from existing entities. To address\nthese requirements for individuality and common-\nality, we propose a fast and effective method to\nconstruct an artificial entity that fuses attributes\nand relations of entities within the same class.\nInitially, we select an existing entity ep from a\nspecific class C to serve as the parent entityfor\nour artificial entity ˜e and consider other entities\nwithin the same class C as sibling entities of par-\nent, denoted by sib(ep) ={e1, ...en}. Our goal is\nto construct an artificial entity that exhibits high\nsimilarity to the parent entity and conforms to the\ncommonality of the class (heredity) while incorpo-\nrating properties from the sibling entities or reason-\nably changing property values (variation). As an\nexample, in Figure 1, the artificial entity Alcuna\ninherits the \"Diet\" and other attributes from the\nparent Alpaca, while the \"Body mass\" is varied.\nBesides the above operations of heredity and\nvariation, we construct new entities with additional\nextension and dropout of the properties of the new\nentities, in order to mimic human progressive cog-\nnitive processes of entities. As the example in\nFigure 1 shows, we extend the attribute of \"First\nappearance\" from Vicuna to Alcuna, and drop out\nthe \"Life span\" from the parent entity Alpaca.\nThe whole method can be seen in Algorithm 1.\nA detailed description of KnowGen with natural\nlanguage and expressions is shown in Appendix A.\nThe entities constructed in this way are not only\nreasonable but also convenient for us to assess the\nmodel’s cognitive ability to associate and differen-\ntiate new knowledge with the existing knowledge.\n2.4 Question Answering as Evaluation Task\nBased on the constructed artificial entities, a natural\nform of evaluation is to ask questions to LLMs in\nthe context of new knowledge. In specific, we lever-\nage an attribute triplet(˜e, a, v) for generating a one-\nhop question q(˜e, a, v) by specifically asking about\nthe object v, given the subject ˜e and attribute a.\nWith a chain of relation triplets TC = (˜e, r, e1) →\n(e1, r1, e2) →... →(eN−1, rN−1, eN ), we con-\nstruct a multi-hop question q(TC) asking about the\ntail object eN , given the head subject ˜e and rela-\ntions {r, r1, ..., rN−1}.\nWe propose that LLM requires the knowl-\nedge understanding, knowledge differentiation and\n1399\nBody mass: 60kgDiet: leavesLife span: 25.8yearsOther Properties…Alpaca\nBody mass: 47.5kgWeaning age: 7 monthsFirst appearance: middle Pleistocene age\nVicunaAlcuna\nCats\nCougar\nJaguarManed Wolf\nEaten byCompete withCamelsExisting\n?Body mass: 56.5kg [variation]Life Span [dropout]Diet: leaves [heredity]First appearance: middle Pleistocene age [extension]Other Properties inherited from Alpaca…ArtificialKnowledge Understanding (KU):What type of food is included in the diet of Alcuna? Answer: leavesIn which geological period did Alcuna first appear? Answer: middle Pleistocene age ...Knowledge Differentiation (KD):Does Alcuna have a body mass of 60kg?Answer: No.What's the average life span of Alcuna?Answer: I don’t know....Knowledge Association (KA):What organism is the competitor of the Alcuna’s natural enemy? Answer: Maned Wolf....Eaten byVariationHeredity\nFigure 1: Demonstration of ALCUNA , including heredity, variation, extension and dropout operations in KnowGen,\ngenerated artificial entity named Alcuna and three types of questions related to it.\nknowledge association abilities when faced with\nnew knowledge. To enable a more detailed assess-\nment, we design diverse categories of questions to\nevaluate each ability, as shown on the right side of\nFigure 1.\nSpecifically, we sample some attribute triplets in\nthe variation set Tv\nA (˜e) and the dropout set Td\nA(˜e)\nto create KD question set, which is ideally suited\nfor evaluating the knowledge differentiation abil-\nity of a model to distinguish between the parent\nentity ep existing in its internal knowledge and the\nnewly constructed artificial entity ˜e provided in the\ncontext.\nThe proficiency of LLMs in reasoning along\nmastered knowledge graph has been demonstrated\nin previous studies (Hu et al., 2022; Choudhary\nand Reddy, 2023). However, it remains uncertain\nwhether it can effectively establish connection be-\ntween newly encountered artificial entity and the\nexisting knowledge for multi-hop reasoning task.\nTo investigate this, we incorporate the relations of\nartificial entity to construct a new graph, which en-\ncompasses both existing entities and the artificial\nentity. We then perform a breadth-first-search on\nthe relation graph to identify a chain of relation\ntriplets TC with the artificial entity serving as root\n(e.g., [(Alcuna, Eaten by, Jaguar), (Jaguar, Com-\npete with, Maned Wolf)]), and then utilize the chain\nto generate a multi-hop question q(TC) (e.g., What\norganism is the competitor of the Alcuna’s natu-\nral enemy?). We group such questions into KA\nquestion set.\nFor the rest of the artificial entity’s property\ntriplets, we utilize them to evaluate the ability of\nremembering and understanding new knowledge\nby simply asking questions about the objects in\ntriplets. We group such questions into KU question\nset.\n3 A LCUNA : Our Benchmark\nWith our proposed method, one can create a large\namount of new entities quickly based on exist-\ning structured knowledge. A natural attempt is\nto construct new organisms on already discovered\nones, since the biological taxonomy is perfectly\nadapted to our proposed approach. Therefore, we\nutilize KnowGen to proposeALCUNA, a biological\ndataset for evaluating the ability of the model in\nface of new knowledge as shown in Figure 1.\n3.1 EOL Database\nWe utilize the structured data from the EOL3 (Ency-\nclopedia of Life) database (Parr et al., 2014) to pro-\nvide existing knowledge, which is an online, freely\naccessible database that aims to provide informa-\ntion about all known species on Earth. EOL orga-\nnizes all biological taxons into a taxonomic tree,\nin which each entity belongs to a class. The most\nintriguing feature of EOL is that it constructs rich\nstructured data in the form of key-value pairs for\neach biological entity, including taxonomic rank,\nattributes, relationships and information source. As\na whole, it contains 2404790 entities with a total\nof 13625612 properties consisted of 669 property\ntypes. The substantial volume of data, coupled with\n3https://eol.org/\n1400\nits well-organized format, renders EOL the most\nsuitable data source for constructing ALCUNA.\n3.2 Artificial Entity Construction Details\nEach time we select a class Cfrom the taxonomic\ntree of EOL and consider its members as entities.\nWe then divide them into one parent entityep and\nits siblings sib(ep) from which we construct the\nartificial entity. Since a high-level taxon is usually\nnot a specific organism, the properties it possesses\nmay be too homogeneous, so we screen out all\ntaxons belonging to kingdom, phylum and domain.\nIn the real world, the naming scheme of a newly\nfound creature usually incorporates the same prefix\nor suffix of other creatures of the same species. In\norder to mimic the real world scenario and consid-\nering the tokenization algorithms used in LLMs,\nwe firstly split the names of related existing enti-\nties (i.e. the parent entity and sibling entities of\nparent) into subwords 4. Then we randomly select\nnames of related entities, and for the i-th selected\nentity we choose its i-th subword. For example,\n\"ALCUNA\" is created from Alpaca and Vicuna.\n3.3 Template-based Question Generation\nGiven a property triplet (˜e, a, v) (one-hop setting)\nor a chain of property triplets TC = (˜e, r, e1) →\n(e1, r1, e2) →... →(eN−1, rN−1, eN ) (multi-hop\nsetting), we aim to generate natural language ques-\ntion asking about the tail object.\nWe leverage ChatGPT in the process of question\ngeneration to avoid expensive labor costs following\nPetroni et al. (2019). Specifically, we use ChatGPT\nto generate a question template with a placeholder\n[T] given only the relevant properties to avoid intro-\nducing too much model’s knowledge of a specific\nentity. Then we generate questions from the ques-\ntion template by replacing [T] with the name of\nhead subject. We generate five question templates\nfor each property group in form of multiple choice,\nfill-in-the-blank and Boolean questions. The details\nabout the prompts used for question generation and\nexamples are shown in Appendix B.2. To ensure\nthe quality of automatic question generation by this\nmethod, we randomly sample 100 questions each\nfor one-hop and multi-hop questions for human\nchecking. It turns out that for the generated one-\nhop questions, 98% are correct; for the multi-hop\nquestions, 95% are correct. It shows that this way\nof constructing questions is acceptable.\n4We utilize the tokenizer of GPT-2 for tokenization.\n0 5 10 15 20 >=25\n#Attributes\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n#Entities\n×102\n(a)\n0 1 2 3 4 >=5\n#Relations\n0.0\n0.5\n1.0\n1.5\n2.0\n#Entities\n×103\n(b)\nFigure 2: (a) The number of entities with different\ncounts of attributes. (b) The number of entities with\ndifferent counts of relations.\n3.4 Dataset Summary\nWith the previous steps, we constructed a dataset,\nALCUNA, for evaluating the ability of LLMs in\nface of new knowledge. The ALCUNA dataset\nconsists of a total of 84351 questions about 3554\nartificial entities. We ensure that the constructed\nartificial entities contain rich and unique attributes\nand relationships by filtering out parent entities\nwith less than three properties. Specifically, each\nartificial entity contains 11.75 property triples and\n25.39 siblings on average. The distribution of the\nnumber of property triplets is shown in Figure 2.\nWe organize the dataset in terms of questions,\nand for each question we collect the correspond-\ning property triplets as evidence and the relevant\nartificial entities’ information as new knowledge.\nWe divide all questions into three subsets, KU,\nKD and KA, as mentioned in Section 2.4 to mea-\nsure the corresponding capabilities of LLMs in a\nfine-grained manner. In specific, KU, KD and KA\ncontain 11316, 27186 and 15353 questions respec-\ntively. The details about question forms in the three\nsubsets are shown in Appendix B.1.\n4 Evaluation of LLMs\n4.1 LLMs Selected for Evaluation\nWe select several popular LLMs for evaluation and\nanalysis on our benchmarks, including ChatGPT,\nAlpaca-7B, Vicuna-13B and ChatGLM-6B. The\ndetailed description of our selected models can be\nfound in Appendix C.\n4.2 Evaluation Methods\nIn order to adapt the approach in the era of large\nmodels and to match the application scenarios in\npractice, we introduce two types of evaluation\nmethods: zero-shot and few-shot. We implement\nboth the vanilla and \"Chain-of-Thought\" (CoT) rea-\nsoning forms for zero-shot and few-shot setting.\n1401\nFor experiments in the zero-shot setting, our inputs\nare structured representations of new knowledge\nand questions to be answered by the model. For\nexperiments in the few-shot setting, we include sev-\neral examples of the same form together with the\nanswers, which we hope will help the model un-\nderstand. For the zero-shot CoT, we append \"Let’s\nthink step by step.\" at the end of questions, and\nfor the few-shot CoT, the reasoning process for the\nanswers of examples is also attached. Please refer\nto Appendix D for the detailed method description.\nAn example of prompt used in our experiment is\nshown in Table 12.\n4.3 Evaluation Metric\nSince our questions are in the form of multiple\nchoice, fill-in-the-blank or Boolean questions, the\ngolden answers are usually just one or a few words.\nTherefore, we determine the correctness of the\nmodel output by matching it with the answer (like\nAccuracy). Since there may be more than one possi-\nble answer to some questions, such as those asking\nabout the geographical distribution of entities, etc.,\nwe consider the answer to be correct as long as\nit matches one of the correct answers. This is a\nless stringent measurement, but as seen in Section\n5, most models still perform poorly. Using the\nproposed dataset, we evaluate each model’s ability\nwith each method for knowledge understanding,\nknowledge differentiation, and knowledge associa-\ntion, respectively. We report the average score on\nthe entire benchmark in each setting.\n4.4 Data Filtering\nSince there are differences of internal/existing\nknowledge of different models due to the differ-\nent model size and training data, we further fil-\nter the samples in our dataset for each model be-\nfore testing, based on previous work (Petroni et al.,\n2019), in order not to be influenced by the differ-\nence (which is not the theme of our paper) and to\ncompare the models’ performance in face of new\nknowledge in a more focused and fair way. For\nour method of filtering questions, please refer to\nAppendix E. We experiment and analyze the four\nmodels mentioned in Section 4.1 based on the fil-\ntered new knowledge, using the evaluation settings\nintroduced in Section 4.2.\n5 Result and Analysis\n5.1 Overall Results\nThe performance of the LLMs on our benchmark\nunder different settings is shown in Table 1. We can\nsee that ChatGPT has the best performance in all\nsettings, which is consistent with our usual beliefs.\nVicuna has the second best performance among all\nmodels. In terms of methods, the few-shot setting\nperforms better than the zero-shot setting overall,\nand CoT performs better than the vanilla form in\nmost cases.\nIn face of new knowledge, as seen in Table 1,\nLLMs do perform poorly except for ChatGPT on\nKU and KD experiments. Among all abilities,\nknowledge association is obviously the most diffi-\ncult for LLMs, and all of them have difficulty in\nrelating to their internal knowledge through new\nknowledge provided, and thus in making multi-hop\nreasoning correctly. The performance of knowl-\nedge understanding and knowledge differentiation\nis better than that of knowledge association, but yet\nnot satisfactory for most LLMs.\nIn summary, current LLMs perform relatively\npoorly in face of new knowledge, slightly better\nin knowledge understanding and knowledge dif-\nferentiation, and have more difficulty in reasoning\nacross new and existing knowledge. In order to\nhave a clearer view of models output, please refer\nto Appendix F for the analysis of models output.\nConsidering that it is expensive, slow and un-\nstable to call ChatGPT’s API, without loss of gen-\nerality, all the following comparison experiments\nfor analysis are conducted on three other models.\nIn addition, for convenience, the following analy-\nsis experiments are performed in the setting of the\nvanilla few-shot method, and structured input arti-\nficial entity knowledge, if not specifically stated.\n5.2 Impact of Entity Similarity\nIn this section we explore the effect of the similar-\nity between the artificial entity and the parent entity\non the model performance over the KD questions,\nwhich are designed to assess the model’s ability\nto distinguish between new knowledge and exist-\ning knowledge. Specifically, we explore attribute\nsimilarity and name similarity.\nThe More Similar, the More Confusing (unless\npowerful enough) We define the proportion of\noverlap of properties between entities as the prop-\nerty similarity of entities. As shown in Figure 3,\n1402\nChatGPT Alpaca Vicuna ChatGLM ChatGPT Alpaca Vicuna ChatGLM\nZero-Shot-Vanilla Zero-Shot-CoT\nKU 50.19 31.02 34.12 34.64 68.75 39.81 39.61 24.85\nKD 58.70 15.35 38.65 32.29 61.78 14.29 38.53 22.84\nKA 28.44 24.60 29.71 10.29 35.36 19.66 29.55 4.97\nAvg. 52.85 25.12 35.98 31.26 63.34 29.44 38.00 22.04\nFew-Shot-Vanilla Few-Shot-CoT\nKU 75.44 33.80 41.22 47.97 82.18 40.77 43.67 40.91\nKD 64.20 38.97 46.76 41.42 74.99 36.24 55.81 37.19\nKA 41.52 27.63 30.10 27.47 37.88 25.73 25.07 26.93\nAvg. 64.37 35.09 42.74 42.56 74.11 38.02 47.73 37.64\nTable 1: Performance of LLMs at our benchmark under different settings\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nProperty Similarity\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Avg Score\nalpaca_7b\nchatglm_6b\nchatgpt\nvicuna_13b\nFigure 3: Relationship between model performance on\nKD questions and the property similarity between the\nartificial entity and its parental entity.\nAlpaca Vicuna ChatGLM\nsimilar 38.74 47.23 41.61\nrandom 39.52 47.64 43.20\nTable 2: Results on KD questions of the entity with the\nsame property and different name.\nwe divide questions according to the property sim-\nilarity between the parental entities and the artifi-\ncial entities, and calculate the scores of models in\ndifferent similarity ranges on the KD problem re-\nspectively. We can find that the performance of the\nrobust ChatGPT to differentiate entities is almost\nindependent of the similarity. But other than it, all\nother models are affected by entity similarity. The\nmore similar the new entities are to the existing\nentities, the harder they are to differentiate them,\nwhich illustrates the flaws of LLMs. The further\nanalysis is shown in Appendix ??\nThe Name also Plays a Role Since each artificial\nentity is identified by its name, we think that the\nname might have some effect on the model’s abil-\nity to distinguish new knowledge, so we conducted\nexperiments on KD questions for comparison. We\nContext Alpaca Vicuna ChatGLM\nartificial 38.97 46.76 41.42\nw/ parent 37.09 35.04 24.71\nw/ irrelevant 37.12 37.17 35.00\nTable 3: Results on KD questions with different knowl-\nedge in context.\nassign two different names to artificial entities with\nthe same properties: one is a randomly generated\nsequence of characters (random), and the other is a\nrandom substitution of one character (similar) for\nthe name of its parent entity. The results of the\nexperiments on the KD questions are shown in Ta-\nble 2. We can find that the influence of names on\nmodel cognition does exist, and similar names are\nmore likely to cause confusion in the model. How-\never, the effect is not very large and both results\nare lower, which shows that the modeling of new\nentities by LLM is both derived from the names\nand influenced by the properties.\n5.3 Impact of Provided Knowledge\nTo further explore the performance of LLMs when\nnew knowledge is provided in the context, we vary\nthe knowledge content provided for the analysis\nexperiments.\nParent Entity Aggravates Confusion Accord-\ning to Section 5.2, the models suffer from a certain\ndegree of confusion when faced with new knowl-\nedge that overlaps in name or properties with inter-\nnal knowledge. To further verify this, we explicitly\nintroduce parent entities in the context. Specifi-\ncally, we conducted two comparison experiments:\n1) adding parent entity knowledge to the context;\n2) adding a random existing entity knowledge as\ncontrol variables. As shown in Table 3, the per-\nformance is affected by both parent and irrelevant\n1403\nContext Alpaca Vicuna ChatGLM\nartificial 27.63 30.1 27.47\nw/ chain 29.21 33.25 44.43\nw/ irrelevant 25.38 22.98 18.56\nTable 4: Results on KA questions with different knowl-\nedge in context.\nAlpaca Vicuna ChatGLM\nJSON NL JSON NL JSON NL\nKU 33.8 30.63 41.22 28.04 47.97 20.07\nKD 38.97 36.00 46.76 36.12 41.42 22.44\nKA 27.63 26.07 30.10 25.48 27.47 9.82\nAvg. 35.09 32.16 42.74 31.92 42.56 20.54\nTable 5: Results on ALCUNA with knowledge in JSON\nand natural language format (NL).\nentities in the context, which is consistent with pre-\nvious work (Shi et al., 2023). More significantly,\nfor Vicuna and ChatGLM models, the parent entity\nbrings more substantial performance degradation\ncompared to the irrelevant entity, again confirming\nthe confusion problem of existing large models in\nface of new knowledge.\nChain Entities are Key to Knowledge Associa-\ntion To more clearly analyze why all models per-\nforms poorly on the knowledge association prob-\nlem, we conduct two additional sets of experiments\non the KA questions: 1) adding knowledge about\nthe entities involved in the reasoning chain to the\ncontext. 2) randomly sampling the same number\nof entities to the context for a fair comparison. The\nfinal results are shown in Table 4. We can find\nthat the score of all models improves very much\nafter adding the information of the entities required\nfor inference, and the performance of all models\ndecreases after adding irrelevant entities. This also\nshows that the main problem is that LLMs really\ncannot make the association between new and ex-\nisting knowledge well, and not just the problem of\nnot being able to make reasoning.\nStructured Knowledge is Better Since our\nknowledge is represented structurally, the input\nof knowledge in our experiments is also structured,\nas shown in Table 12. To explore the effect of the\nform of the knowledge representation, we addition-\nally do comparison experiments with knowledge in\nnatural language form as input (NL). We use tem-\nplates to transform each attribute into a language\ndescription for input similar to the template-based\nquestion generation process in Section 3.3. As can\nbe seen from Table 5, all models generally perform\nbetter with the structured input setting (JSON). The\nmodels’ understanding of this structured text may\ncome from the code in the training data. This indi-\ncates that for this kind of high-density knowledge\ninput, a clear and structured representation is more\nhelpful for the model’s understanding.\n6 Assess New Models with A LCUNA\nIn this section, we discuss how to apply the pro-\nposed ALCUNA benchmark to other models. There\nare two different application scenarios of our bench-\nmark. First, if one wants to assess the knowledge\nunderstanding performance of different LLMs in\nthe face of new knowledge, ALCUNA can be di-\nrectly utilized for evaluation. On the other hand,\nif one wants to compare the knowledge differenti-\nation and association abilities, the different back-\nground knowledge inside the different models may\nlead to an unfair comparison. Therefore, we need\nto conduct an additional filtering on ALCUNA to\nensure the existing entities are possessed by all\nmodels, which will cause a shrinkage of our bench-\nmark. Despite this, the current benchmark has been\nfiltered on models such as Alpaca. A reasonable\nassumption is that the models that come after that\nwill be more and more powerful, so the resulting\nshrinkage won’t be very severe.\n7 Related Work\nLarge Language Models In recent years, sig-\nnificant advancements in Large Language Models\n(LLMs) like FLAN-T5(Chung et al., 2022), GPT-\n3(Brown et al., 2020), OPT(Zhang et al., 2022),\nLLama(Touvron et al., 2023) and GPT-4(OpenAI,\n2023) have led to exceptional performance in nat-\nural language processing tasks. At the same time,\nopen-source LLMs based on LLama and other fun-\ndamental models for further instruction fine-tuning\nhave emerged recently, such as Alpaca(Taori et al.,\n2023), Vicuna(Chiang et al., 2023), Koala(Geng\net al., 2023), ChatGLM(Du et al., 2022), etc.,\nwhich have also shown strong capabilities.\nThese models have shown breakthroughs on a\nvariety of tasks, and some have been applied as a\ncommercial product in daily work. Since the world\nis constantly changing, the ability of the models to\nperform when faced with new knowledge is critical.\nExisting Benchmarks Benchmarks, such as\nSQuAD(Rajpurkar et al., 2016), SNLI(Bowman\n1404\net al., 2015), GLUE(Wang et al., 2018), Super-\nGLUE (Wang et al., 2020), LAMBADA (Paperno\net al., 2016), etc., is essential for setting evaluation\ncriteria and tracking model performance. While\nsome traditional benchmarks like SQuAD and\nSNLI assess single-task abilities, GLUE and Su-\nperGLUE evaluate general language models across\nvarious NLP tasks. More recently, with rapid de-\nvelopment of LLMs, new benchmarks have been\nproposed to evaluate on more complex tasks such\nas college entrance exams, law school admission\ntests and so on (Hendrycks et al., 2021; Guo et al.,\n2023; Zhong et al., 2023). However, these bench-\nmarks are still based on existing knowledge, which\nis abundant in the training data of LLM.\nSome knowledge benchmarks (Onoe et al., 2021;\nMallen et al., 2023; Arodi et al., 2023) evaluate\nmodel’s ability of knowledge integration while they\neither only evaluate on small models which are very\ndifferent with LLMs (training data, ability, etc.) or\nsimply use knowledge that already exists. There-\nfore, benchmarks that assess LLMs’ ability with\nnew knowledge rather than existing knowledge are\nurgently needed.\nSource of New Knowledge Many works use tem-\nporal knowledge as a source of new knowledge to\nevaluate new knowledge behavior of LLMs (Lazari-\ndou et al., 2021; Agarwal and Nenkova, 2022; Jang\net al., 2023; Zaporojets et al., 2023). There is also\nsome work that uses entity or attribute substitution\nto create new knowledge (Longpre et al., 2022;\nZhou et al., 2023). A discussion of the differences\nand strengths and weaknesses of our work versus\nprior work is in Appendix I.\n8 Conclusion\nWe propose a new approach KnowGen to construct\nnew knowledge and build an artificial biological en-\ntity benchmark ALCUNA for evaluating the ability\nof LLMs faced with new knowledge. We test and\nanalyze several popular models with commonly\nused methods, and find some useful conclusions.\nOur proposed approach and benchmark can help\nthe development of more powerful LLMs that can\nunderstand, differentiate and reason across new and\nexisting knowledge. In the future, we expect that\nmore LLMs will be evaluated on our benchmark.\nMore benchmarks in other disciplines also can be\nconstructed based our method.\nLimitations\nAlthough the method we design can be used for\nthe construction of any knowledge that satisfies the\nontological representation, we have implemented\nit only on biological data. It is absolutely possible\nto use this approach to create new knowledge in\nother domains for evaluating LLMs. It is because\nKnowGen method for creating new knowledge only\nrequires some defined classes, and some entities in\nthe classes, entities with their own attributes, and\nconnections between the entities. Any knowledge\nbase with such a structure can be used to create\nnew knowledge with our KnowGen method. In the\nfuture, we hope that new knowledge datasets from\nmore domains will be available.\nWe evaluate only a few powerful models due to\nthe fact that some models are closed source or the\nnumber of parameters is too large. We expect that\nmore models can be measured using our bench-\nmark.\nEthics Statement\nThe knowledge of biological entities in our bench-\nmark is artificially constructed, so it does not exist\nin the real world. Therefore, for humans, it is neces-\nsary to be careful not to be misled by the knowledge\ninside; for models, the hazard of using our dataset\nfor training on model knowledge is unknown. This\nis in line with our expectation that we want AL-\nCUNA to be used as a new knowledge benchmark\nonly for the evaluation of model capabilities in face\nof new knowledge.\nAcknowledgements\nThis work was supported by National Key R&D\nProgram of China (2021YFF0901502), National\nScience Foundation of China (No. 62161160339),\nState Key Laboratory of Media Convergence Pro-\nduction Technology and Systems and Key Lab-\noratory of Science, Technology and Standard in\nPress Industry (Key Laboratory of Intelligent Press\nMedia Technology). We appreciate the anonymous\nreviewers for their helpful comments. Xiaojun Wan\nis the corresponding author.\nReferences\nOshin Agarwal and Ani Nenkova. 2022. Temporal ef-\nfects on pre-trained models for language processing\ntasks.\n1405\nAkshatha Arodi, Martin Pömsl, Kaheer Suleman, Adam\nTrischler, Alexandra Olteanu, and Jackie Chi Kit Che-\nung. 2023. The kitmus test: Evaluating knowledge\nintegration from multiple sources in natural language\nunderstanding systems.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nNurendra Choudhary and Chandan K. Reddy. 2023.\nComplex logical reasoning over knowledge graphs\nusing large language models.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335.\nAparna Elangovan, Jiayuan He, and Karin Verspoor.\n2021. Memorization vs. generalization: Quantifying\ndata leakage in nlp performance evaluation.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n2023. Koala: A dialogue model for academic re-\nsearch. Blog post.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How close is chatgpt to human experts?\ncomparison corpus, evaluation, and detection.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding.\nZiniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang,\nZiyi Yang, Chenguang Zhu, Kai-Wei Chang, and\nYizhou Sun. 2022. Empowering language models\nwith knowledge graph reasoning for question answer-\ning.\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2023. Temporalwiki: A lifelong bench-\nmark for training and evaluating ever-evolving lan-\nguage models.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nTomás Kociský, Sebastian Ruder, Dani Yogatama,\nKris Cao, Susannah Young, and Phil Blunsom. 2021.\nMind the gap: Assessing temporal generalization\nin neural language models. In Neural Information\nProcessing Systems.\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2022. Entity-based knowledge conflicts in question\nanswering.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work?\nNatalya F Noy, Deborah L McGuinness, et al. 2001.\nOntology development 101: A guide to creating your\nfirst ontology.\nYasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and\nGreg Durrett. 2021. Creak: A dataset for common-\nsense reasoning over entity knowledge.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\n1406\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nCynthia S Parr, Mr Nathan Wilson, Mr Patrick Leary,\nKatja S Schulz, Ms Kristen Lans, Ms Lisa Wal-\nley, Jennifer A Hammock, Mr Anthony Goddard,\nMr Jeremy Rice, Mr Marie Studer, et al. 2014. The\nencyclopedia of life v2: providing global access to\nknowledge about life on earth. Biodiversity data\njournal, (2).\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023. Check\nyour facts and try again: Improving large language\nmodels with external knowledge and automated feed-\nback.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases?\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Chi, Nathanael Schärli, and\nDenny Zhou. 2023. Large language models can be\neasily distracted by irrelevant context.\nJohn F. Sowa. 1995. Top-level ontological categories.\nInternational Journal of Human-Computer Studies,\n43(5):669–685.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2020. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nKlim Zaporojets, Lucie-Aimee Kaffee, Johannes Deleu,\nThomas Demeester, Chris Develder, and Isabelle Au-\ngenstein. 2023. Tempel: Linking dynamically evolv-\ning and newly emerging entities.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,\nShuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,\nand Nan Duan. 2023. Agieval: A human-centric\nbenchmark for evaluating foundation models.\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and\nMuhao Chen. 2023. Context-faithful prompting for\nlarge language models.\n1407\nA KnowGen Method\nHere, we will describe knowgen in the form of\nnatural language and math formulas.\nFollowing the symbolic representation in Sec-\ntion 2.2, we divide the property triplets T(e) =\nTA(e) ∪TR(e) of parent entity into three sets: re-\nmain, change and delete, denoted as T∗(e) =\nT∗\nA(e) ∪T∗\nR(e), ∗∈{ r, c, d}. We randomly sam-\nple some triplets from sibling entities as add set\nTa(˜e) = Ta\nA (˜e) ∪T a\nR (˜e). Subsequently, we fuse\nthe above four sets 5 to form the properties of ar-\ntificial entity. We retain Tr(e) for high similarity\nbetween the parent and artificial entities.\nTr\nA(˜e) ={(˜e, a, v)|(e, a, v) ∈T r\nA(e)}\nTr\nR(˜e) ={(˜e, r, e′)|(e, r, e′) ∈T r\nR(e)}\nWe modify the object of triplets in Tc(e) with\nsome perturbation to create uniqueness in the prop-\nerties of the artificial entity, but without introducing\noverly unusual values.\nTc\nA(˜e) ={(˜e, a,perturb(v))|(e, a, v) ∈T c\nA(e)}\nTc\nR(˜e) ={(˜e, r,perturb(e′))|(e, r, e′) ∈T c\nR(e)}\n, in which perturb() represents a perturbation func-\ntion that can modify values according to specific\nneeds. For numeric attribute valuev, we add a gaus-\nsian noise to it. For non-numeric attribute value v,\nwe use the object of the same attribute from sib-\nlings entities. For association object entity e′, we\nrandomly choose one of its siblings as the modified\nvalue.\nWe also include Ta(˜e) to ensure the commonal-\nity of entities within the specific class C.\nTa\nA (˜e) =RandomSample({(˜e, r, v)|\n(ei, r, v) ∈TA(ei), ei ∈Sib(˜e)}\nTa\nR (˜e) =RandomSample({(˜e, r, e′)|\n(ei, r, e′) ∈TR(ei), ei ∈Sib(˜e)}\nB Dataset Details\nB.1 Question Types Distribution\nAs mentioned in Section 2.4, we divide the AL-\nCUNA into three subsets. Each subset contains a\ncertain number of multiple choice, fill-in-the-blank,\n5We process these four collections in a manner that ensures\nno overlapping or shared properties between them, thereby\neliminating any potential contradictions.\nmultiple choice Boolean fill-in-the-blank\nKU 2459 4232 4625\nKD 3487 34497 19698\nKA 15353 0 0\nTotal 21299 38729 24323\nTable 6: Number of different forms of KU, KD and KA\nquestions.\nTotal\nChatGPT Vicuna Alpaca ChatGLM\nRefuse 20.01 17.3 20.69 17.45\nMulti 0.5 0.2 0.04 0.06\nWrong 79.49 82.5 79.27 82.49\nTotal 100.00 100.00 100.00 100.00\nTable 7: The percentage of categories of incorrect re-\nsponses from large models.\nand Boolean questions. The specific distribution of\nquestion types is shown in Table 6. Note that we\ngenerate only multiple choice questions for the DA\nquestion set. This is done for two reasons: one is\nthat the mutli-hop questions are too hard for current\nLLMs as shown in Table 1, and the other is that\nthere may be multiple answers to one multi-hop\nquestion, and to evaluate the results more accu-\nrately and avoid false negatives, we utilize choices\nto limit the answer space.\nB.2 Natural Language Question Generation\nWe prompt ChatGPT to generate question tem-\nplates given a property triplet in one-hop setting\nor a chain of property triplets in multi-hop setting.\nWe provide the prompt to generate Boolean, fill-in-\nthe-blank question templates under both settings in\nFigure 4, 5 and 6. We also show some generated\nquestion templates in Table 8. To generate multiple\nchoice questions, we simply append four choices\nto the corresponding fill-in-the-blank questions.\nIn order to guarantee the accuracy of the ques-\ntions generated through this approach, we select a\nrandom sample of 100 questions for both one-hop\nand multi-hop questions to be checked by humans.\nThe results indicate that over 98% of the one-hop\nquestions and over 95% of the multi-hop questions\ngenerated are accurate.\nC Introduction to Our Model We Select\nWe choose four representative and popular LLMs.\nChatGPT, is one of the most powerful models, but\nis close-source, so we can only call it through the\nAPI. The other three are all excellent open-source\n1408\nRelation Chain or Property Generated Templates\nprey on What are the animals that [T] prey on?\nWhat animals are preyed on by [T]?\ncingulum location Where is the cingulum located in the [T]’s mouth?\nWhat is the type of cingulum in the teeth of [T]?\nfrost free days How many frost free days are required for the growth of [T]?\nHow many frost-free days does the habitat of [T] have on average?\n(have host, co-roost with)What is the species that co-roosts with the host of [T]?\nWhat species shares a roosting habitat with the host of [T]?\n(parasitize, visit flowers of, eat)What is the food source of a species that feeds on the flowers visited by an organism parasitized by [T]?\nWhat is the food source of the species whose flowers are visited by an organism parasitized by ’[T]’?\nTable 8: Question templates generated from properties of entities or chains of reasoning.\nSystem: You are a powerful multi-hop question generator with biological knowledge. Users will provide a chain \nof RDF triples, and you will help write a question to ask the tail entity from the head entity. You shouldn't include \nbridge entities in the generated question. The question should only include the head entity [T], which is \nplaceholder. If you can't create a valid question, reply with \\\"[None]\\\" only. Don't reply with any explanation or \nother information.\n/* exemplars */\nUser: Given RDF triples ([T], compete with, x1), (x1, are eaten by, x2), (x2, are host of, x3), write a question to ask \nx3. Don't mention x1, x2, ... Write the possible question in natural English.\nAgent: Multi-hop question: Which entities serve as the habitats for the creatures that feed on the organisms \ncompeting with [T]?\nUser: Given RDF triples ([T], parasitize, x1), (x1, prey on, x2), write a question to ask x2. Don't mention x1, x2, ... \nWrite the possible question in natural English.\nAgent: Multi-hop question: What is the prey of an organism that is parasitized by [T]?\nUser: Given RDF triples ([T], eat, x1), (x1, compete with, x2), (x2, are pathogens of, x3), write a question to ask \nx3. Don't mention x1, x2, ... Write the possible question in natural English.\nAgent: Multi-hop question: Which species has pathogens that compete with species that are eaten by [T]?\n/* query */\nUser: Given RDF triples {$Triple chain}, write a question to ask {$Tail entity}. Don't mention x1, x2, ... Write the \npossible question in natural English.\nFigure 4: Prompt for generating multi-hop question templates.\nSystem: You are a powerful question generation model with biological knowledge. Given a biological taxon's \nproperty name and a set of possible values of the property. You need to generate a yes-no question template with \nplaceholders [T] and [V] about the given property satisfying the following constraints:\n1. Most important: the generated question template must contain [T] and [V].\n2. The placeholder [T] can be replaced with any taxons name.\n3. The placeholder [V] can be replaced with possible value of the given property. You can refer to the given value \nset for the format of possible property value. But don't use any value in the value set directly.\n4. Don't do the replacement, just provide the question template with [T] and [V] placeholders.\n5. Try to create the question even if the property is not a biological property. \n6. Don't reply with any explanation or other information.\n/* exemplars */\nUser: Property name: skeleton contains   Value set: ['chitin']\nAgent: Question template: Does the skeleton of [T] contain [V]?\nUser: Property name: body shape   Value set: ['cylinder’ , 'elongated', 'bilaterally symmetric', 'radially symmetric’]\nAgent: Question template: What's the body shape of [T]\nUser: Property name: litters per year   Value set: ['1.0 /year', '3.0 /year', '1.5 /year', '2.0 /year', '2.5 /year']\nAgent: Question template: Does one [T] produce [V] litters per year?\n/* query */\nUser: Property name: {$Property name}   Value set: {$Possible values of Property}\nFigure 5: Prompt for generating Boolean question templates.\n1409\nSystem: You are a powerful question generation model with biological knowledge. Given a biological taxon's \nproperty name. You need to generate a question template with a placeholder [T] about the given property so that \nthe placeholder [T] can be replaced with any taxons name. Try to create the question even if the property is not a \nbiological property. Don't reply with any explanation or other information.\n/* exemplars */\nUser: Property name: skeleton contains\nAgent: Question template: Which organic compound is a component of a [T]'s skeleton?\nUser: Property name: body shape\nAgent: Question template: What's the body shape of [T]\nUser: Property name: litters per year\nAgent: Question template: How many litters can [T] have per year?\n/* query */\nUser: Property name: {$Property name}\nFigure 6: Prompt for generating fill-in-the-blank question templates.\nmodels. Due to the limitation of GPU memory\nspace, we choose those open-source models rang-\ning from 6B-13B.\n• ChatGPT: ChatGPT is a sibling model to In-\nstructGPT (Ouyang et al., 2022), which is\ntrained on a vast instruction dataset and further\ntuned by reinforcement learning with human\nfeedbacks (RLHF).\n• Alpaca-7B: Alpaca is an open-source\ninstruction-following LLM trained for\nacademic purposes, which is fine-tuned from\nthe LLaMA 7B model (Touvron et al., 2023)\non 52K instruction-following demonstrations.\n• Vicuna-13B: Vicuna is trained by fine-tuning\nLLaMA using 70K conversations with Chat-\nGPT shared by users. A preliminary evalua-\ntion using GPT-4 as a judge shows that Vicuna-\n13B achieves more than 90% of the quality of\nChatGPT.\n• ChatGLM-6B: ChatGLM is also an open-\nsource instruction-following LLM, which is\nbased on General Language Model (Du et al.,\n2022) framework.\nWe download the above three open source models\nfrom Huggingface6, and thanks to the convenient\ndesign of the FastChat7 library, we unify the testing\nframework for all models and call them through\nthe API.\nWe also consider several models with 7B param-\neters for evaluation to compare the performance of\nmodels of same size, which may help to analyze the\n6https://huggingface.co/\n7https://github.com/lm-sys/FastChat\nimpact of different training processes on the abil-\nity in face of new knowledge. We select Llama2-\nChat-7B, Alpaca-7B, Vicuna-7B, and ChatGLM-\n7B specifically. The results are presented on Table\n9.\nD Introduction to Our Experiment\nMethod\nD.1 Zero/Few-shot Evaluation Setting\nThe zero-shot setting is where the model is given\nexplicit instructions to directly complete the mis-\nsion. This scenario evaluates the ability of the\noriginal model to solve the problem autonomously\nwithout training. In our benchmark, the input to\nthe zero-shot is the new knowledge of the artificial\nentity and a question to be asked about it.\nCompared to the zero-shot setting, in the few-\nshot setting the model is given several additional\nexamples from the same task as a reference. This\nallows evaluating the ability of the model to learn\nthe task quickly based on a limited number of sam-\nples, and is also consistent with practical situations\nwhere supervised training is not convenient. Ac-\ncording to the Min et al. (2022)’s study, in our\nbenchmark assessment, we provided 3 to 5 exam-\nples for each type of problem, expecting that it\nwould be sufficient to be able to demonstrate the\nlabeling space for this type of problem.\nD.2 Chain-of-Thought (CoT) Form\nFor both the zero-shot and few-shot evaluation set-\ntings, we add the design of the CoT form. For the\nzero-shot setting, we added the words \"Let’s think\nstep by step.\" at the end of the question, expecting\nthe model to output the thinking process, which can\nhelp LLM to reason about complex problems. For\n1410\nLlama2-Chat-7B Alpaca-7B Vicuna-7B ChatGLM-6B Llama2-Chat-7B Alpaca-7B Vicuna-7B ChatGLM-6B\nZero-Shot-Vanilla Zero-Shot-CoT\nKU 28.92 31.02 28.00 34.64 47.75 39.81 32.74 24.85\nKD 32.61 15.35 34.70 32.29 37.48 14.29 33.61 22.84\nKA 18.71 24.60 23.40 10.29 24.61 19.66 17.56 4.97\nAvg. 29.57 25.12 30.88 31.26 39.50 29.44 31.67 22.04\nFew-Shot-Vanilla Few-Shot-CoT\nKU 37.06 33.80 32.78 47.97 65.20 40.77 38.50 40.91\nKD 44.38 38.97 39.39 41.42 44.71 36.24 38.71 37.19\nKA 24.42 27.63 24.39 27.47 28.72 25.73 27.11 26.93\nAvg. 39.27 35.09 35.14 42.56 50.15 38.02 37.21 37.64\nTable 9: Performance of LLMs of 7B parameters at our benchmark under different settings.\nmultiple choice Boolean fill-in-the-blank\nKU 2025 3138 3814\nKD 2486 24592 12601\nKA 8757 0 0\nTotal 13268 27730 16415\nTable 10: Number of different forms of KU, KD and\nKA questions after filtering.\nthe few-shot setting, we add the thought process\nin the answer to each sample question shown to\ninspire the model.\nE Details of Our Approach to Filtering\nQuestions\nSpecifically, we retain only those artificial entities\nwhose parent entities could be perfectly recalled by\nthe model. In addition, since answering multi-hop\nquestions requires the model to make use of each\nsingle-hop knowledge, we then filter out any rea-\nsoning chains that contain knowledge that cannot\nbe correctly recalled by the model.\nThe method used for the above two filtering\nis to construct question templates for the knowl-\nedge involved, including attributes and relation-\nships, based on previous work (Petroni et al., 2019),\nand then to query the model using the few-shot set-\nting. We filter samples in our benchmark for every\nevaluated model to ensure that our questions are\nspecific to the ability about new knowledge, and\nthen select the intersection of filtered questions for\nfair experimentation and analysis. The number of\nquestions per category left is shown in Table 10.\nF Analysis about Models’ Output\nAn example of the output of the model is shown in\nTable 11. To better analyze the models’ responses,\nas shown in Table 7, we divide all the models’\nerror outputs into 3 categories, including rejecting\nresponses, answering multiple options, and other\nincorrect responses.\nWe can find that the percentage of answering\nmultiple options for all models is very small, which\nindicates that all models can understand and com-\nply with our requirements very well. In addition,\nsome of the questions are rejected by some mod-\nels, probably because some models recognizes that\nit cannot answer the corresponding question and\nresponds with \"I don’t know\" or \"I am sorry\".\nG Impact of Different Modifications\nAs shown in Section 5.2, models commonly strug-\ngle with knowledge differentiation when the arti-\nficial entity and the parent entity are similar. In\nthis section, we further conduct an ablation study\nto investigate the specific impact of different modi-\nfications (i.e. variation and dropout).\nTo ablate one type of modification, we recon-\nstruct artificial entities in the KD dataset. For each\nquestion in the original KD dataset, we have a par-\nent entity ep and the corresponding attribute a to be\nqueried. We then reconstruct several artificial enti-\nties by modifying one attribute of ep (except a) at a\ntime. From this, we obtain several artificial entities\nwith different similarity caused by the same type\nof modification. For a fair comparison, we only ex-\nperiment with ep with 10 attributes. We randomly\nsample 1000 parent entities and create a total of\n1000 ×10 = 10000new artificial entities. Finally,\nwe conduct experiments by querying models about\nthem with the same question about a.\nThe experiment results are presented in Table\n7. Different models exhibit almost the same trend\nunder both modifications. It is clear that dropout\nyields a stable improvement as the similarity de-\ncreases while the impact of variation is relatively\nweak and insignificant.\n1411\nQuestion: What organism is parasitized by an organism that feeds on Agabus ajax?\nAnswer: Rangifer tarandus\nModel’s Output: \"Thought process: Agabus ajax is a species of aquatic beetle that feeds on Acari,\nCulicidae, Daphnia magna, Daphnia pulex, and Ceratopogonidae. To answer the question, we need\nto identify an organism that is parasitized by an organism that feeds on Agabus ajax.\nFinal answer: Thala gorii\"]\nQuestion: Is Sorghum versinatum a type of hyperhydate?\nAnswer: Yes\nModel’s Output: \"ANSWER: Yes\"\nQuestion: What’s the preferred water depth range for Plana?\nAnswer: 500.0 cm\nModel’s Output: ANSWER: 500 cm\\n\\nIt is important to note that the information\nTable 11: Example of question and answer from our models.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSimilarity\n35\n40\n45\n50\n55\n60\n65Performance\nVariation\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSimilarity\nDropout\nModel\nChatGLM\nVicuna\nAlpaca\nChatGPT\nFigure 7: Relationship between model performance on KD questions and the property similarity between the\nartificial entity and its parental entity due to different levels of modification, i.e. variation and dropout.\nH Prompts in Experiments\nIn this section, we will show prompts used in our\nsettings. Since our questions contain Boolean, mul-\ntiple choice, and fill-in-the-blank forms, the rest of\nthe prompt is identical except for the restrictions\non the output format for each type of question that\nare targeted. Therefore, without loss of general-\nity, we provide the prompts for the fill-in-the-blank\nquestions as a demonstration. We present the CoT\nzero-shot prompt as an example in Table 12. The\nother methods of prompt are basically similar.\nI Source of New Knowledge\nTemporal Knowledge Using temporal knowl-\nedge as a source of new knowledge has the follow-\ning disadvantages:\n1. the expense of collecting data. With the con-\ntinuous emergence of new LLMs, the tempo-\nral knowledge used needs to be re-collected\neach time, requiring labor and resources to\nrace with the training of the LLM.\n2. uncertain validity and risk of information leak-\nage. Some LLMs do not announce the training\ndata they use, so it is not known whether the\ntemporal knowledge collected each time is\nstill valid or not.\n3. fairness of comparison. Since the range of\ntimestamps for training data of each model is\ndifferent, the new temporal knowledge is dif-\nferent for each model. Therefore, the test data\nused for each evaluation needs to be different,\nand it is uncertain whether such a comparison\nis fair.\nIn contrast, our proposed KnowGen method\nsolves the above problems. Since it is artificial\nknowledge, it will be valid for a long time and no\n1412\nYou are a powerful question-answering system with knowledge in the field of biology.\nUsers will provide some biological information along with a question.\nYour task is to combine the information provided by the user with your biological knowledge to\nanswer the question.\nIf you are unable to answer the question, simply respond with \"I don’t know.\"\nHere is the basic information about a taxon you can refer:\n###\n{\n\"name\": \"Bainvillevillea spinosa\",\n\"property\": {\n\"cellularity\": [\"multicellular\"],\n\"conservation status\": [\"least concern\"],\n\"geographic distribution\": [\"Ecuador\"],\n\"habitat\": [\"terrestrial\"],\n\"leaf complexity\": [\"compound\"],\n\"leaf morphology\": [\"broad\"],\n\"leaf sheddability\": [\"evergreen\"],\n\"plant growth form\": [\"branched\"],\n\"produces\": [\"oxygen\"],\n\"woodiness\": [\"woody\"]\n},\n\"rank\": \"species\"\n}\n###\nAnswer the following question a few words: What is the habitat of Bainvillevillea spinosa?\nDesired format: Thought process: <Thought process>, Final answer: [Final answer].\nLet’s think step by step.\nTable 12: Demonstration of the zero-shot prompt in the CoT form.\n1413\nmore effort is required to collect data repeatedly.\nMoreover, for all models, the knowledge is defi-\nnitely new, so we can also use the same test data to\nevaluate the models, which also ensures the validity\nand fairness of the evaluation.\nPrevious methods of entity and attribute substi-\ntutions As for the relationship to previous work\n(Longpre et al., 2022; Zhou et al., 2023), all of us\n\"construct\" knowledge, but our knowledge forms,\nconstruction purposes, and implementation meth-\nods are different.\n1. Knowledge form: The knowledge in Longpre\net al. (2022) and Zhou et al. (2023) is a state-\nment of a few sentences describing a simple\nfact. Whereas our knowledge is represented\nin ontological form, with complete properties,\nrelations and classes, which is more structured\nand complete.\n2. Purpose of construction: both of them are de-\nsigned to construct knowledge that contradicts\nthe internal knowledge of the model, as a way\nto make the model hallucinatory or unreliable\npredictions. Instead, our work is intended to\nsimulate the generation of new knowledge that\nis associated and consistent with the existing\nknowledge.\n3. Implementation: the methods of them are lim-\nited by the form of knowledge representation,\nand merely construct counterfactuals by re-\nplacing the name of the entity in the sentence\nwith the name of another existing entity. Our\nmethod, on the other hand, will create a com-\npletely new entity with a new name and which\nreasonably exists in the original knowledge\nsystem through operations such as heredity,\nvariation and dropout.\n1414",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6553870439529419
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6390319466590881
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5805801153182983
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.5415485501289368
    },
    {
      "name": "Data science",
      "score": 0.486695796251297
    },
    {
      "name": "Knowledge management",
      "score": 0.48253366351127625
    },
    {
      "name": "Knowledge base",
      "score": 0.46583500504493713
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34574806690216064
    },
    {
      "name": "Social science",
      "score": 0.07810869812965393
    },
    {
      "name": "Sociology",
      "score": 0.07206690311431885
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ],
  "cited_by": 5
}