{
    "title": "Optimizing Inference Performance of Transformers on CPUs",
    "url": "https://openalex.org/W3130954105",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3187115789",
            "name": "Dice, Dave",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3191866648",
            "name": "Kogan, Alex",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2073061372",
        "https://openalex.org/W3045733172",
        "https://openalex.org/W2967893498",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2963947383",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2931743911",
        "https://openalex.org/W3035408713",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3093091803",
        "https://openalex.org/W3104667152",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W3104613728",
        "https://openalex.org/W3101045333",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2945667196"
    ],
    "abstract": "The Transformer architecture revolutionized the field of natural language processing (NLP). Transformers-based models (e.g., BERT) power many important Web services, such as search, translation, question-answering, etc. While enormous research attention is paid to the training of those models, relatively little efforts are made to improve their inference performance. This paper comes to address this gap by presenting an empirical analysis of scalability and performance of inferencing a Transformer-based model on CPUs. Focusing on the highly popular BERT model, we identify key components of the Transformer architecture where the bulk of the computation happens, and propose three optimizations to speed them up. The optimizations are evaluated using the inference benchmark from HuggingFace, and are shown to achieve the speedup of up to x2.37. The considered optimizations do not require any changes to the implementation of the models nor affect their accuracy.",
    "full_text": "Optimizing Inference Performance of Transformers on CPUs\nDave Dice\nOracle Labs\nBurlington, MA, USA\ndave.dice@oracle.com\nAlex Kogan\nOracle Labs\nBurlington, MA, USA\nalex.kogan@oracle.com\nABSTRACT\nThe Transformer architecture revolutionized the field of natural\nlanguage processing (NLP). Transformers-based models (e.g., BERT)\npower many important Web services, such as search, translation,\nquestion-answering, etc. While enormous research attention is paid\nto the training of those models, relatively little efforts are made\nto improve their inference performance. This paper comes to ad-\ndress this gap by presenting an empirical analysis of scalability\nand performance of inferencing a Transformer-based model on\nCPUs. Focusing on the highly popular BERT model, we identify\nkey components of the Transformer architecture where the bulk\nof the computation happens, and propose three optimizations to\nspeed them up. The optimizations are evaluated using the infer-\nence benchmark from HuggingFace, and are shown to achieve the\nspeedup of up to x2.37. The considered optimizations do not require\nany changes to the implementation of the models nor affect their\naccuracy.\n1 INTRODUCTION\nThe introduction of the Transfomer architecture for deep neural\nnetworks (DNNs) by Vaswani et al. [26] has literally transformed\nthe field of NLP. It happened just a few years ago (in 2017, to be\nexact), and since then the field has exploded with an enormous wave\nof Transfomer-based models achieving state-of-the-art, and often\nsuper-human, performance on many NLP tasks, which just recently\nhave been considered unrealistically difficult to solve. BERT [ 5],\nRoBERTa [12], ALBERT [10], Transformer-XL [4] are only very\nfew examples in the vast sea of published models [35]. As of today,\nTransfomer-based models, and BERT in particular, power many\nimportant Web services, such as search [ 14, 15], translation and\ntext classification [11].\nThe big premise of the Transfomer-based models is that they\ncan be pre-trained on huge amounts of unlabeled data (such as all\nof Wikipedia or a book corpus), and later fine-tuned to a specific\ntask (e.g., question-answering) using just a small amount of labeled,\ndomain-specific data. To achieve high accuracy, those models fea-\nture millions (and, at times, billions) of parameters, and require\nlong and expensive training. As a result, numerous efforts have\nbeen made to optimize the training performance of those mod-\nels [7, 10, 12, 36]. At the same time, and despite the vast deployment\nof those models in practice, far less attention is paid to inference\nperformance. Furthermore, among the efforts that do target infer-\nence performance of Transformer-based models, many consider\nGPU or smartphone-based deployments [6, 29, 33, 38], even though\nin many practical settings the inference is done on small CPU-based\nsystems [11, 34].\nThis paper comes to address this gap by presenting an empirical\nanalysis of scalability and performance of inferencing Transfomer-\nbased models on CPUs. We identify the key component of the\nTransformer architecture where the bulk of the computation hap-\npens, namely, the matrix multiplication (matmul) operations, and\npropose three optimizations to speed them up.\nThe first optimization is based on the observation that the per-\nformance of the matmul operation is heavily impacted not only\nby the shape (dimensions) of the source matrices and the available\ncomputing resources (the number of worker threads), but also by\nwhether (at least) one of those matrices is provided in a transposed\nform. We propose a lightweight method to adaptively choose the\nappropriate form of source matrices for the inference, which re-\nsults in substantial performance improvement of the latter. The\nsecond optimization stems from the observation that an invocation\nof matmul operations in deep learning (DL) frameworks incurs\na significant sequential overhead, leading to the poor scalability\nof those operations. We analyze the source of the overhead, and\ndemonstrate how the scalability of matmul operations (and the\noverall inference performance) can be improved by reducing (some\nportion of) that overhead. Finally, the third optimization builds\non the realization that while performant matmul operations are\ntypically implemented by partitioning matrices into sub-matrices\nand carrying out the actual computation using highly optimized in-\nner kernels [8], the partitioning itself might be suboptimal and not\nfully utilize parameters of the underlying hardware (such as cache\ncapacity). We show how choosing different parameters for matrix\npartitioning results in faster matmul operations. We evaluate the\nefficacy of our optimizations using the industry-grade inference\nbenchmark from HuggingFace [32].\nWe note that prior work shows many factors impacting the in-\nference performance of DNN models [31], including the choice of\na DL framework, a math library, a thread pool library, availability\nof certain hardware features, such as the support for SIMD (single\ninstruction multiple data), etc. To make our analysis feasible, we\nmake several conscious choices when setting up our experimenta-\ntion environment, focusing on the inference performance of BERT\nimplemented in the widely used Pytorch framework [20] built with\nthe state-of-the-art oneDNN math library [18] (previously known\nas MKL-DNN) and run on an Intel Skylake processor-powered\nsystem (which supports AVX512 SIMD instructions). While the\nchosen setup is significant, we validate the generality of many of\nour findings in other variations of our setup such as with other\nTransformer-based models and math libraries. We also note that\ndespite the focus of our work being on NLP and Transformer-based\nmodels in particular, we believe our findings extend to any model\nin which matmul operations consume a significant portion of the\ninference time.\narXiv:2102.06621v3  [cs.CL]  22 Feb 2021\nDave Dice and Alex Kogan\nThe rest of the paper is organized as follows. We provide the\nrelevant background on Transformers and BERT in Section 2. The\nrelated work is discussed in Section 3. We describe our evaluation\nsetup in Section 4 and provide the analysis of the inference perfor-\nmance of BERT on CPUs in Section 5. Based on this analysis, we\ndescribe three optimizations for inference performance in Section 6.\nFinally, we conclude in Section 7 with a discussion of the results\nand some of the future directions.\n2 BACKGROUND: TRANSFOMER AND BERT\nThe Transformer architecture [26] is composed of two stacks of\nidentical layers; those stacks are called encoder and decoder. For\nthe purpose of this paper, we will focus on the encoder stack only,\nwhich is used exclusively in many actual Transformer-based models,\nincluding BERT. In fact, we will note upfront that BERTâ€™s model\narchitecture is almost identical to the Transformer encoder, only\ntweaking the number of layers, the activation function, etc. [5]. Also,\nwe note that BERT itself has multiple configurations that differ in\nthe various model hyper-parameters (e.g., the â€œbaseâ€ configuration\nfor BERT has12 layers while the â€œlargeâ€ one has24). Unless specified\notherwise, when we say BERT in this paper, we refer to to its â€œbaseâ€\nconfiguration [5].\nEach encoder layer has two sublayers, the first being amulti-head\nself-attention mechanismand the second being a position-wise fully-\nconnected feed-forward network. A residual connectionis employed\naround each of the sub-layers, followed by layer normalization.\nThe attention mechanism is at the heart of the Transformer ar-\nchitecture. For the purpose of this paper we will focus on the actual\ncomputations performed by this mechanism; the explanation of the\nintuition behind those computations can be found in many excel-\nlent sources [1, 16], including in the original paper [26]. Specifically,\nthe attention mechanism takes as an input three matrices Q, K and\nV and computes the output matrix:\nð´ð‘¡ð‘¡ð‘›(ð‘„,ð¾,ð‘‰ )= ð‘ ð‘œð‘“ð‘¡ð‘šð‘Žð‘¥ (ð‘„ð¾ð‘‡\nâˆš\nð‘‘ð‘˜\n)ð‘‰ (1)\nwhere ð‘‘ð‘˜ is the attention input dimension (64 for the BERT model).\nAs mentioned above, each self-attention sublayer includes multiple\nheads (12 for the BERT model). The computed function of this\nsublayer is given by the following expressions:\nð‘€ð‘¢ð‘™ð‘¡ð‘–ð»ð‘’ð‘Žð‘‘ (ð‘„,ð¾,ð‘‰ )= ð¶ð‘œð‘›ð‘ð‘Žð‘¡(â„Žð‘’ð‘Žð‘‘1,...,â„Žð‘’ð‘Žð‘‘ â„Ž)ð‘Šð‘‚\nwhere â„Žð‘’ð‘Žð‘‘ð‘– = ð´ð‘¡ð‘¡ð‘›(ð‘„ð‘Šð‘„\nð‘– ,ð¾ð‘Š ð¾\nð‘– ,ð‘‰ð‘Šð‘‰\nð‘– )\n(2)\nwhere ð‘Šð‘‚, ð‘Šð‘„\nð‘– , ð‘Šð¾\nð‘– and ð‘Šð‘‰\nð‘– are parameter matrices. Overall,\nthe computation of the multi-head self-attention requires 4 matrix\nmultiplications to create input token projections (the Q, K and V\nmatrices) and the projection of the concatenated output of all the\nmultiple heads. (We note that when Transformer is implemented\nin Pytorch, each of those multiplications are performed during the\ncomputation of the corresponding Linear modules.) In addition,\ntwo batched matrix multiplications are required to calculate the\nð´ð‘¡ð‘¡ð‘› function in Equation 1. Furthermore, the self-attention sub-\nlayer includes the invocation of softmax and layer normalization\noperations.\nAs for the fully-connected feed-forward sublayer, it consists of\ntwo linear transformations with an activation function in between:\nð¹ð¹ð‘ (ð‘¥)= ð´ð‘ð‘¡(ð‘¥ð‘Š1 +ð‘1)ð‘Š2 +ð‘2 (3)\nwhere ð‘Š1, ð‘1, ð‘Š2 and ð‘2 are weight and bias matrices, respectively\n(which are model parameters, one set for each layer) and ð´ð‘ð‘¡ is an\nactivation function, such as gelu [9]. While the inputs and outputs\nof the feed-forward sublayer have the same dimensions as the rest\nof the model (768, in case of BERT), the inner-layer has a larger di-\nmensionality (3072 for BERT). It is easy to see that the computation\nof the feed-forward sublayer requires two matrix multiplication\noperations (carried by two Linear modules in Pytorch), as well as\nan activation function and a layer normalization operation.\n3 RELATED WORK\nThere is a relatively small body of work we are aware of on opti-\nmizing inference performance of NLP models on CPUs. Ning et\nal. [15] describe their effort on accelerating BERT with ONNX Run-\ntime, an inference engine compatible with PyTorch and TensorFlow.\nThe idea is to fuse multiple operations in the computation graph\n(e.g., matrix multiplication, layer normalization and gelu) to reduce\nthe amount of overhead (e.g., memory copying) in invoking each\nelementary computation individually. They also experiment with re-\nducing the number of layers in BERT sacrificing (some) accuracy for\nhigher performance. In general, we note that the operation fusion\nis a known technique for optimizing inference performance, and is\northogonal to the optimizations described in this paper. Also, our\ntechniques aim for performing the given inference computations\nfaster, but without any change to the accuracy.\nWu et al. [34] describe another effort to optimize inference of\nBERT in Apache MXNet using the GluonNLP toolkit. They report\non speedups achieved by using the MKL math library in MXNet\nas well as from quantizing the model for better performance with\nlower precision. We note that we use MKL as one of the baseline\nconfigurations in our analysis, while model quantization is, once\nagain, orthogonal to the ideas discussed in this paper and may result\nin reduced accuracy.\nThere is an enormous effort on refining an/or replacing the\nattention mechanism with a more efficient alternative that re-\nquires less computation and/or allows scaling for longer sentences,\ne.g., [2, 3, 27, 37]. While most of those efforts are primarily con-\ncerned with speeding up training, they help inference directly or\nindirectly as well. Notably, one of the goals behind the knowledge\ndistillation effort [24, 25, 28], i.e., training a smaller model (student)\nto achieve a similar accuracy as a larger one (teacher), is reducing\nthe inference latency. Indeed, Le and Kaehler describe how they\nemploy distillation with quantization to speedup their deployment\nof BERT on CPUs [ 11]. We believe the optimizations described\nin this paper apply to most of such work. In particular, we show\nthe speedups achieved by our optimization for inferencing Distil-\nBert [24], a popular model that uses knowledge distillation, are\nsimilar to those of BERT.\nIn a broader context, Liu at et. [13] describe an approach called\nNeoCPU for optimizing CNN inference on CPUs. In addition to the\ncommon optimizations of operation fusion and inference simplifi-\ncation, NeoCPU manipulates the data layout flowing through the\nFebruary 23, 2021 â€¢ Copyright Oracle and or its affiliates\nOptimizing Inference Performance of Transformers on CPUs\n(a) Seq. length 8\n (b) Seq. length 64\n (c) Seq. length 384\nFigure 1: Performance breakdown for BERT by sub-layers and their components.\nmodel to minimize the overhead of transforming the data between\nvarious individual operations. Fang et al. [6] present TurboTrans-\nformers, a GPU-based serving system for Transformer models. They\ndescribe a number of optimizations targeting GPUs, such as mem-\nory management and batch reduction. Optimizing inference of NLP\nmodels on GPUs has been also the motivation behind the work by\nWang et. al [29]. At the same time, Wu et al. [33] describe opportu-\nnities and design challenges in enabling machine learning inference\non smartphones and other edge platforms.\n4 EVALUATION ENVIRONMENT\nIn this section, we describe the hardware and software setup for\nour experiments. We ran the experiments on an Intel-based system\nfeaturing two Intel Xeon Platinum 8167M processors with 26 hyper-\nthreaded cores each, and runs an Oracle Linux 7.8 OS. To avoid any\nnon-uniform memory access effects, we use the numactl utility to\nrestrict all our experiments to executing on and allocating memory\nfrom one socket only.\nOn the software side, we use Pytorch v1.6, a popular DL frame-\nwork. We compile Pytorch in the default configuration, which\nmeans that it employs MKL as its default math library, but also\nincludes support for oneDNN. While MKL is a closed-source library,\noneDNN is â€œan open-source cross-platform performance library\nof basic building blocks for deep learning applications\" [18], and,\nunless stated otherwise, we use the latter in our experiments.\nTo invoke oneDNN bindings, one needs to convert a given model\nas well as the input tensors into the so-called â€œmkldnnâ€ data format,\nwhich dictates how data is laid out in memory [19]. We note that\noneDNN bindings, however, are available for only a handful of DL\noperations, such as Linear and Convolution modules. In practice,\nthis means that for each such supported operation, Pytorch would\nseamlessly convert input tensors into the mkldnn format, apply\nthe corresponding operation and convert the output tensors back\ninto the default (â€œdenseâ€) format so they could be sent to other\noperations for which oneDNN bindings are not provided. Such\nconversions do not come for free, however, and involve memory\nlayout translations and memory copying. To avoid this overhead,\nwe extend the integration of oneDNN with Pytorch, adding the\nmissing bindings for various operations invoked by a typical Trans-\nformer model, such as the layer normalization, softmax and gelu\nactivation functions. The extension comprises of a few hundred\nlines of C++ and Python code. As we show in Section 6.1, the result-\ning setup performs on-par with or better than the default Pytorch\nconfiguration (which employs MKL).\nWe use the popular Transformers Python package (v3.0.2) from\nHuggingFace, which provides a state-of-the-art implementation\nof numerous Transformer-based NLP models implemented in Py-\ntorch (and Tensorflow) [ 32]. In addition, Transformers includes\nan easy-to-use inference benchmark, which we utilize heavily for\nour experiments. Furthermore, we utilize mcbench [30], the open-\nsourced suite of microbenchmarks, which includes, among other\nthings, microbenchmarks for evaluating the performance of matrix\nmultiplication operations when invoked directly through the C++\nAPI of the corresponding math libraries.\n5 INFERENCE PERFORMANCE ANALYSIS\nWe instrument the BERT model implemented in Transformers [32]\nand collect timing information for various sub-layers (multi-head at-\ntention, feed-forward) and modules (Linear, Softmax, etc.) compos-\ning the model while executing the Transformers inference bench-\nmark. We experiment with various input sequence lengths and vary\nthe number of threads from 1 to 16 (by setting theOMP_NUM_THREADS\nenvironment variable). We note that although our experimental ma-\nchine has more than 16 cores, we deliberately decided to focus on\nsmaller setups, as practical inference deployments typically include\na small number of cores [11, 34].\nFigure 1 presents the inference latency as well as the breakdown\nof runtime spent in two main sub-layers, attention and feed-forward\n(along with the small portion of time not associated with any of the\nsub-layers, which consists mostly of input embedding and pooling\nof the resulting tensor; this time is denoted as a red box titled\nâ€œOtherâ€). In addition, we break the time in the two sub-layers into\nmajor components. For the attention sub-layer, this is the time spent\nin self-attention (â€œselfâ€), the linear projection (â€œdenseâ€), the layer\nnormalization (â€œlayernormâ€) and the rest (â€œotherâ€). For the feed-\nforward sub-layer, this is the time spent in two linear projections\n(â€œdense1â€ and â€œdense2â€), the layer normalization (â€œlayernormâ€) and\nthe rest (â€œotherâ€), e.g., the gelu activation function.\nOverall, we see that the feed-forward sub-layer typically con-\nsumes more time than the attention sub-layer. Linear projections,\nFebruary 23, 2021 â€¢ Copyright Oracle and or its affiliates\nDave Dice and Alex Kogan\n(a) Seq. length 8\n (b) Seq. length 64\n (c) Seq. length 384\nFigure 2: Performance breakdown for BERT by modules.\nwhich ultimately translate into matmul operations, are responsible\nfor that. We do not break down the attention sub-layer for better\nreadability, yet the data shows that linear projections for the Q, K\nand V matrices consume the large share (50â€“75%) of its time as well.\nAlong with that, the attention sub-layer does include other mod-\nules, e.g., softmax, tensor transpose and reshaping operations, etc.,\nwhich explains why the time share of the attention sub-layer grows\nas we increase the number of threads. Specifically, as we show in\nSection 6.2, matmul operations, being carried out by carefully opti-\nmized math libraries (oneDNN, in this case), scale almost linearly\nwith the number of threads. At the same time, other operations (in-\ncluding layer norm, softmax and tensor reshaping) do not scale. As\nsuch, their relative portion grows while the portion of time spent in\nmatmul operations shrinks. The portion of non-scalable operation\nis larger in the attention sub-layer, hence its weight grows with\nthe number of threads. This growth is more tamed when the input\nsequence is large (cf. Figure 1 (c)) since the matmul operations are\ninvoked with larger matrices, thus consuming a larger portion of\ntime w.r.t. all other operations.\nWhen examining the total runtime (the black curve in Figure 1),\nwe note that the overall scalability of the inference latency is rela-\ntively low, and depends on the input sequence length. In particular,\nfor sequences of 8 tokens, we achieve the speedup of only x3.3\nwhen running with 16 threads versus 1 thread; the speedup goes up\nto x9.7 for sequences of 384 tokens. Better scalability with longer\nsequences is, once again, related to the scalability of matmul opera-\ntions in math libraries and the fact that larger sequences result in\nheavier matmul operations (with larger matrices) â€” reducing the\ntime spent in matmul operations when the number of threads in-\ncreases has a larger effect on the overall scalability of the inference\nlatency.\nIn Figure 2, we present a different way to breakdown the infer-\nence runtime, by the time spent in various models. While most\nmodule names are self-explanatory, we note that â€œbmmâ€ stands\nfor batched matrix multiplication, the operation at the heart of the\nmulti-head attention mechanism (there are two of those operations\nper each attention layer in BERT); \"other\" stands for the time spent\nin computations not included in the specific modules, such as the\ntime spent on transposing and reshaping tensors, input embedding,\npooling of the resulting tensor, etc.\nNot surprisingly, the vast majority of the inference time is spent\nin the Linear module, which in practice means matmul operations\n(the Linear module applies a linear transformation to the incom-\ning data, calculating a product of the input matrix with the stored\nweight matrix). This concurs the results in Figure 1. When summing\nup both â€œlinearâ€ and â€œbmmâ€ runtime portions, the matmul opera-\ntions consume between 66.2% and 91.5% of the total runtime. Note\nthat this portion decreases as we increase the number of threads.\nAs explained above, this is because matmul operations are executed\nby a math library (oneDNN), and are highly optimized and scale\nalmost linearly with the number of threads.\n6 OPTIMIZING INFERENCE PERFORMANCE\n6.1 Adaptive Linear Module\nThe results in Section 5 show that the key to improving inference\nperformance on CPU lies in reducing the time spent in matmul\noperations.\nFor the context, the API for a matmul operation allows invoking\nthat operation on two source matrices A and B (producing the\ndestination matrix C=AB) s.t. each of the source matrices can be\nprovided in the transposed form [17]. At the same time, the Pytorch\nLinear module stores the weight matrix in a transposed form, which\nmeans that, during inference, the input matrix (A) is always non-\ntransposed, while the weight matrix (B) is always transposed. We\nbelieve the Pytorch designers made this choice (to transpose the\nweight matrix) to achieve better performance for the backward pass\nduring training [21], a concern which is not relevant for inference1.\nOur experiments with the matmul microbenchmark from mcbench\nreveal an interesting observation. Figure 3 demonstrates the ratio\nbetween the time to compute matmul when both source matrices\nare non-transposed to the time to compute matmul when (only)\nthe second source matrix is transposed. In other words, ratio > 1\n(ratio < 1) corresponds to cases in which the former (latter, re-\nspectively) method is faster. The shape of the second matrix (B) is\nrepresented by the name of the corresponding data series, while\nthe shape of the first matrix (A) is given by the sequence length\nx first dimension of B. Note that the chosen three shapes are not\nincidental, and they correspond to the shapes of weight matrices\nused in Linear modules of the BERT model.\nMore concretely, Figure 3 (a)â€“(d) compare the performance of the\nmatmul operation in oneDNN across different numbers of threads.\nWe see that for shorter sequences, multiplying the non-transposed\n1We note that in Tensorflow, the weight matrix isalways given in the normal form to\nthe matmul operation.\nFebruary 23, 2021 â€¢ Copyright Oracle and or its affiliates\nOptimizing Inference Performance of Transformers on CPUs\n(a) #threads=1 (oneDNN)\n (b) #threads=2 (oneDNN)\n (c) #threads=4 (oneDNN)\n(d) #threads=16 (oneDNN)\n (e) #threads=16 (MKL)\n (f) #threads=16 (OpenBLAS)\nFigure 3: Matmul operation performance as a ratio between the time to multiply two non-transposed matrices and the time\nto multiply a non-transposed matrix by a transposed one.\nmatrices is almost always faster, and often results in substantial\nspeedups. For longer sequences, the picture is less clear â€“ one way\nof applying a mutmul operation is faster than the other for one\nshape but worse for another. In general, the faster way of applying a\nmatmul operation depends on the shape of the source matrices and\nthe number of threads used. We also confirmed that this observation\nis not unique to oneDNN, and is reproducible, at least to some extent,\nwith other math libraries. Figure 3 (e) and (f) show the results\nobtained with MKL and OpenBLAS libraries, respectively (for the\nlatter, we used the benchmark included in the library sources; for\nbrevity, we include only the result for 16 threads).\nOne may wonder about the reason for this performance differ-\nence. In oneDNN, much like in any math library for high-performance\nmatmul calculation [8], the matmul operation is coded in assem-\nbly, and each of the matmul variants (e.g., one with both source\nmatrices in the normal form vs. one in which the second matrix is\ntransposed) results in a different code path, which generates dif-\nferent memory access patterns. Based on the profiling information\nproduced by perf, we observe that given a certain configuration\n(i.e., the same source matrix shapes and the number of threads),\nboth variants have a similar number of L1 data cache misses, but\nthe faster variant has a lower number of L3 cache accesses. This\nsuggests that one reason for performance difference might be the\nbetter utilization of L2 cache by one variant over the other.\nGiven the results in Figure 3, we propose the following optimiza-\ntion for the Linear module. Each Linear module is augmented with\na transposeFlags array, specifying whether to use a transposed\nversion of the weights matrix for the forward pass (inference). Entry\nð‘–of the array corresponds to the sequence length of2ð‘–; the array has\n10 entries corresponding to the maximal length of512 tokens. When\ncreating a Linear module with the given weights shape [ð‘–ð‘›,ð‘œð‘¢ð‘¡],\nwe generate random matrices with the shape [2ð‘–,ð‘–ð‘›], for each\n0 â‰¤ð‘– < 10, and measure the time to perform a matmul operation\nwhen the weight matrix is transposed or not. Based on the result,\nwe set the corresponding entry transposeFlags[i]. During the\ninference time, given the input of shape [ð‘™ð‘’ð‘›ð‘”ð‘¡â„Ž,ð‘–ð‘›], we calculate\nð‘  = âŒŠlog(ð‘™ð‘’ð‘›ð‘”ð‘¡â„Ž)âŒ‹, and based on the flag in transposeFlags[s],\nperform the matmul operation with either weight matrix transposed\nor not.\nTo avoid the overhead of transposing the weight matrix during\ninference, we keep both variants of the weight matrix (transposed\nand non-transposed one). This doubles the memory footprint of\nthe Linear module. While it might not be a concern on some CPU\ndeployments, there are several ways to mitigate this drawback. First,\nsome shapes always prefer one form over the other, for all thread\ncounts (e.g., the shape 3072-768 in Figure 3 (a)â€“(d)). For this case,\nwe can keep only the relevant variant of the weight matrix. Second,\nthe length of the input can be known prior to the deployment of an\ninference server, e.g., in a farm of inference servers, certain servers\ncan be configured to handle input of a certain sequence length.\nOnce again, in this case we can keep only the relevant variant of\nthe weight matrix. Finally, if the input range is dynamic, one can\nstore one variant of the weight matrix and transpose on-demand.\nThe selection of the stored variant can be also dynamic and tuned\nbased on the actual input lengths seen during the runtime. All those\nmitigation ideas are left for the future work.\nWe note thattransposeFlags can be shared among Linear mod-\nules of the same shape. We use a key-value map (dictionary) to store\ntransposeFlags arrays where the [ð‘–ð‘›,ð‘œð‘¢ð‘¡]tuple of correspond-\ning Linear modules serves as a key. Thus, when initializing the\ntransposeFlags array, we query the dictionary first, and if such\na shape has been already profiled, we reuse the resulting array,\nskipping the profiling phase for that Linear module. For the BERT\nmodel, this optimization allows us to reduce the number of profiling\nFebruary 23, 2021 â€¢ Copyright Oracle and or its affiliates\nDave Dice and Alex Kogan\n#threads\nsequence length\n8 64 384\nonednn\nbase\nonednn\nnormal\nonednn\nalmo\nonednn\nbase\nonednn\nnormal\nonednn\nalmo\nonednn\nbase\nonednn\nnormal\nonednn\nalmo\nmkl\nbase\nmkl\nscript\n1 115 82 79 Â±1 (x1.46) 216 193 162 (x1.33) 884 Â±31 1009 807 (x1.10) 967 Â±15 861\n2 83 50 50 Â±1 (x1.66) 133 105 97 Â±1 (x1.37) 471 Â±29 512 413 (x1.14) 522 453\n4 51 34 34 (x1.50) 76 64 60 (x1.27) 259 Â±27 285 220 (x1.18) 302 Â±6 245\n8 35 27 27 (x1.30) 49 45 42 (x1.17) 135 148 128 (x1.05) 193 138 Â±2\n16 28 24 24 (x1.17) 36 34 32 (x1.12) 96 Â±14 97 82 (x1.17) 142 Â±2 83 Â±1\nTable 1: BERT-base inference latency (ms), for various sequence lengths. The numbers in () show the speedup of onednn-almo\nover onednn-base. The numbers after the Â±sign specify the standard deviation when it is larger than 1% of the mean.\nphases from 73 (6 Linear modules per each of the 12 self-attention\nlayers plus one for the input embedding) to 3 (one per each different\nshape). We emphasize that the profiling is run only once, during the\ninitialization of the model (and its corresponding Linear modules),\nand is not invoked during inference.\nTable 1 compares the performance of the HuggingFace inference\nbenchmark run on top of several Pytorch variants as described\nbelow. Each experiment is run five times in the same configuration,\nand mean results are presented. We also present the standard devi-\nation for the cases where it was relatively high. The variants we\nevaluate are mkl-base, which is the Pytorch version installed from\npip (and uses MKL); mkl-script, which is the base version run in\nthe torchscript mode (which creates â€œa serializable and optimizable\nmodels from PyTorch codeâ€ [22] and therefore is a recommended\nmode for inference [23]); onednn-base, which is the Pytorch ver-\nsion built from sources and uses oneDNN;onednn-normal, which\nis the onednn-base version in which the weight matrix is stored in\na normal (non-transposed) shape; and onednn-almo, which is the\nonednn-base version with the adaptive Linear module optimiza-\ntion. We note that the first two variants are included for reference\nonly, to demonstrate that they perform mostly on-par with (and, at\ntimes, worse than) onednn-base. Thus, we include them for one\ncase only, for brevity. We also note that the torchscript mode has a\nsmaller impact when applied to oneDNN-based variants, shaving\nabout 7-9 ms from the reported latency in each case. The qualita-\ntive comparison between oneDNN-based variants does not change,\nhowever (although, quantitatively, the torchscript mode leads to\neven larger speedups for the adaptive optimization). Therefore, we\ndo not include the torchscript mode results for those variants.\nThe improvements in the inference latency achieved by the adap-\ntive Linear module optimization, as can be seen in Table 1, correlate\nstrongly with the results in Figure 3. Specifically, higher speedups\nare achieved on shorter sequences and fewer number of threads,\nwhich are exactly the settings where the ratios depicted in Fig-\nure 3 are the highest. We also note the need for adaptivity â€“ while\nonednn-normal performs well on shorter sequences, its perfor-\nmance suffers on longer ones, which is exactly the settings in which,\naccording to Figure 3, multiplying the transposed weight matrix is\nfaster. The adaptive variant selects the correct shape in each case,\nperforming on-par or better than the other two oneDNN-based\nvariants.\nWe note that our findings extend to other Transformer-based\nmodels. For instance, Tables 2 and 3 present the results for RoBERTa\n#threads\nsequence length\n8 64 384\nonednn\nbase\nonednn\nalmo\nonednn\nbase\nonednn\nalmo\nonednn\nbase\nonednn\nalmo\n1 116 79 (x1.47) 216 162 (x1.33) 860 809 (x1.06)\n2 83 50 (x1.66) 133 97 (x1.37) 472 416 (x1.13)\n4 50 35 (x1.43) 76 60 (x1.27) 250 221 (x1.13)\n8 35 28 (x1.25) 49 41 (x1.20) 135 128 (x1.05)\n16 29 24 (x1.21) 36 32 (x1.12) 112 82 (x1.37)\nTable 2: RoBERTa inference latency (ms).\n#threads\nsequence length\n8 64 384\nonednn\nbase\nonednn\nalmo\nonednn\nbase\nonednn\nalmo\nonednn\nbase\nonednn\nalmo\n1 58 38 (x1.53) 108 81 (x1.33) 437 402 (x1.09)\n2 41 24 (x1.71) 67 49 (x1.37) 226 210 (x1.08)\n4 26 17 (x1.53) 39 30 (x1.30) 120 111 (x1.08)\n8 18 14 (x1.29) 25 21 (x1.19) 69 65 (x1.06)\n16 14 12 (x1.17) 18 16 (x1.12) 45 41 (x1.10)\nTable 3: DistilBERT inference latency (ms).\n#threads\nsequence length\n8 64 384\nonednn\nbase\nonednn\nalmo\nonednn\nbase\nonednn\nalmo\nonednn\nbase\nonednn\nalmo\n1 460 284 (x1.62) 811 786 (x1.03) 3134 3063 (x1.02)\n2 379 163 (x2.33) 558 475 (x1.17) 1717 1726 (x0.99)\n4 209 100 (x2.09) 300 299 (x1.00) 908 908 (x1.00)\n8 124 68 (x1.82) 171 203 (x0.84) 506 501 (x1.01)\n16 82 56 (x1.46) 109 120 (x0.91) 293 318 (x0.92)\nTable 4: BERT-large inference latency (ms).\nand DistilBERT, respectively, in their â€œbaseâ€ configurations, while\nTables 4 presents the results for BERT-large, the larger version of the\nBERT model [5]. For brevity, we include only the inference latencies\nfor onednn-base and onednn-almo variants. While the results\nfor RoBERTa and DistilBERT largely follow those for BERT-base,\nthe results for BERT-large are slightly different. This is because\nBERT-large uses a different number of hidden units (1024 vs. 768 in\nother models we have considered), and thus operates with matrices\nof different dimensions.\nFebruary 23, 2021 â€¢ Copyright Oracle and or its affiliates\nOptimizing Inference Performance of Transformers on CPUs\nFor BERT-large and short sentences, onednn-almo achieves\neven more impressive gains overonednn-base compared to BERT-\nbase, reaching the speedup of x2.33. For longer sentences, however,\nonednn-almo lags behind or performs on-par withonednn-base.\nWe identify the reason behind the performance regression as fol-\nlowing: The baseline performance of matmul operations established\nduring the profiling phase of the adaptive linear module optimiza-\ntion differs from the actual performance when the inference is\nexecuted. In other words, during profiling, we establish that using\nthe normal form of the weight matrix is faster than the transposed\none, yet when we run inference, using weights in the normal form\nends up being slower! As we expand in Section 6.3, we hypothesize\nthat this happens due to the poor fitting of matrix partitioning\nparameters in the math library (oneDNN) to hardware constraints,\nsuch as the L2 cache capacity.\n6.2 Reducing Sequential Overhead\nThe results in Table 1 (as well as in Figures 1 and 2) underline poor\nscalability of inference latency, especially for shorter sequences.\nThis is despite the fact that most of the inference time is spent\nin matmul operations (c.f. Figure 2), and those operations exhibit\nnearly linear scalability. The latter is demonstrated by the results\nfrom the mcbench microbenchmark shown in Figure 4 (a), in which\nwe measure the time to multiply two matrices of the shape [8,768]\nand [768,768] as we vary the number of threads. (These shapes\ncorrespond to the matmul operation invoked by linear projections\nin the attention sublayer of the BERT model when the inference is\nperformed on an input sequence of 8 tokens).\nTo shed more light on where the matmul operation cycles are\nspent during inference, we augmented Pytorch and oneDNN with\ntimestamps. We ran the following simple code that employed the\nLinear module only (rather than a full-fledge model) and thus al-\nlowed to focus on the performance of matmul operations:\n1 import torch\n2 from torch. utils import mkldnn as mkldnn_utils\n3 net = torch .nn.Linear (768, 768)\n4 net = mkldnn_utils.to_mkldnn(net)\n5 seq = torch .rand (8, 768). to_mkldnn()\n6 for i in range (0, 10000): net(seq)\nNote that the forward path through the Linear module above\ninvokes a matmul operation on two matrices of the same shape as\nthe ones used for the experiment in Figure 4 (a).\nWith the collected profiling information, we break down the\nphases through which the invocation of the forward pass of the\nLinear module goes, separating the time spent in the Python in-\nterpreter, Pytorch dispatcher (that directs the call to the oneDNN\nimplementation of the Linear model), oneDNN dispatcher (that\nselects the appropriate low-level matmul function), and finally, the\nmatmul (aka general matrix multiply, or GEMM) function itself.\nThe results are presented in the first (left) set of bars in Fig-\nure 4 (b). They show that, indeed, the time in the GEMM function\nscales with the number of threads. At the same time, the duration of\nthe rest of the computation phases does not change as the number\nof threads increases, implying that matmul operations in Pytorch\nincur significant sequential overhead. This overhead becomes even\nmore substantial when the matmul operation is applied on smaller\nmatrices and/or with a large number of threads. This, according\n#threads\nsequence length\n8 64 384\nonednn\nbase\nonednn\nalmo+sor\nonednn\nbase\nonednn\nalmo+sor\nonednn\nbase\nonednn\nalmo+sor\n1 115 78 (x1.47) 216 165 (x1.31) 884 806 (x1.10)\n2 83 48 (x1.73) 133 94 (x1.41) 471 412 (x1.14)\n4 51 32 (x1.59) 76 56 (x1.36) 259 215 (x1.20)\n8 35 25 (x1.40) 49 39 (x1.26) 135 123 (x1.10)\n16 28 22 (x1.27) 36 30 (x1.20) 96 78 (x1.23)\nTable 5: BERT-base inference latency (ms).\nto the Amdahlâ€™s law, explains the poor overall scalability of the\nmatmul operation.\nWe focus on the oneDNN dispatcher as a target for reducing the\nsequential overhead. The dispatcher validates the input parameters,\nidentifies the capabilities of the underlying architecture (e.g., the\ntype of supported AVX instructions, if any), the number of available\nthreads, etc. Based on this information, it iterates over the list of\navailable GEMM implementations and selects the first that is com-\npatible with the given set of requirements for the matmul operation.\nWhile this process is necessary for a correct behavior of oneDNN\nwith arbitrary input matrices (or, in general, input parameters), we\nobserve that during inference, which constrains the set of possible\ninputs to a few specific shapes, only one particular GEMM func-\ntion is called, at least when the number of threads is larger than\none2. Thus, when more than one thread is used, we implement an\noptimization where the dispatching process is reduced to call that\nfunction directly, skipping the validation logic described above.\nThe result of this optimization is shown in the second (right)\nset of bars in Figure 4 (b). We note that the time spent in the\noneDNN dispatcher is reduced substantially, leading to increas-\ning the speedup of x4.42 at 16 threads compared to a single thread\n(up from x3.4 without the optimization). Overall, the speedup is\nstill subpar to the one achieved with mcbench microbenchmark\n(cf. Figure 4 (a)) because the rest of the sequential overhead remains.\nThe effect of reducing the sequential overhead in matmul on\nthe inference performance is shown in Table 5. Here we present\nthe comparison between onednn-base and onednn-almo+sor,\nwhere the latter is the onednn-almo version with the sequential\noverhead reduction optimization described in this section applied.\nThe new optimization shaves another 3â€“12% from the inference\ntime, with larger gains recorded at larger thread counts and/or\nsmaller sequence lengths. This is expected since those are the set-\ntings where the sequential overhead has the most relative impact\non the duration of matmul operations.\n6.3 Modifying Matrix Partitioning\nHigh-performance math libraries, including oneDNN, perform ma-\ntrix multiplication by partitioning the arguments into sub-matrices\nof certain shape, which are then given to assembly-coded inner-\nkernels [8]. This design aims to amortize the cost of moving data\nacross adjacent memory layers, all while taking advantage of care-\nfully engineered inner-kernels. Hence, on a high level, the matrix\n2The oneDNN dispatcher may choose a different function when it detects that only a\nsingle thread is available.\nFebruary 23, 2021 â€¢ Copyright Oracle and or its affiliates\nDave Dice and Alex Kogan\n(a) oneDNN matmul performance\n (b) Break down of Pytorch Linear module performance\nFigure 4: Matmul performance when invoked directly though oneDNN API and through the Pytorch Linear module.\n#threads\nsequence length\n8 64 384\nonednn\nbase\nonednn\nalmo\nonednn\nalmo+sor\nonednn\nbase\nonednn\nalmo\nonednn\nalmo+sor\nonednn\nbase\nonednn\nalmo\nonednn\nalmo+sor\n1 460 288 (x1.60) 287 (x1.60) 811 579 (x1.40) 573 (x1.42) 3134 3003 (x1.04) 2954 Â±46 (x1.06)\n2 379 165 (x2.30) 160 (x2.37) 558 317 (x1.76) 311 (x1.79) 1717Â±22 1631 Â±39 (x1.05) 1606 Â±40 (x1.07)\n4 209 101 (x2.07) 96 Â±1 (x2.18) 300 191 (x1.57) 186 Â±2 (x1.61) 908 868 Â±15 (x1.05) 881 (x1.03)\n8 124 68 (x1.82) 63 (x1.97) 171 120 (x1.42) 114 (x1.50) 506 480 Â±7 (x1.05) 467 Â±19 (x1.08)\n16 82Â±1 55 (x1.49) 50 (x1.64) 109 87 Â±1 (x1.25) 82 (x1.33) 293Â±3 285 Â±12 (x1.03) 275 Â±8 (x1.07)\nTable 6: BERT-large inference latency (ms) with the modified matrix partitioning. The numbers in () show the speedup over\nonednn-base. The numbers after the Â±sign specify the standard deviation when it is larger than 1% of the mean.\nmultiplication operation can be expressed as the following triple-\nnested loop:\n1 for (p = 0; p < sizeK ; p+=BK)\n2 for ( i = 0; i < sizeM; i+=BM)\n3 for ( j= 0; j < sizeN; j+=BN)\n4 ð¶ð‘– ð‘—+=ð´ð‘–ð‘ ðµð‘ ð‘—\nVarious considerations take place when deciding on how to\npartition the matrices (i.e., set BK, BM and BN above), including\nthe size of the caches and TLB (translation-look-aside buffers), the\nshape and layout of source matrices, etc. [ 8]. Yet, while some of\nthose parameters are clearly hardware dependent, the oneDNN\nimplementation uses a set of constants to control the partitioning3.\nWe strongly believe that the regressions reported for BERT-large\nin Section 6.1 are the results of excessively conservative fitting of\nthose parameters to the actual hardware.\nAs evidence, we reduce one of the parameters (BK, from 384 to\n644) so that, effectively, the matrix multiplication is carried by a\nlarger number of iterations of the outermost loop, where each inner-\nkernel is activated on smaller sub-matrices that are more likely to\nfit into cache and, in general, reduce the amount of cache misses.\nThis has a highly positive effect on the inference performance, as\ndemonstrated in Table 6 with the results of the BERT-Large model,\n3e.g., see sgemm_nocopy_driver() in https://github .com/oneapi-src/oneDNN/\nblob/63c8b5ce84b0be266d1edad0420390f2e131cb29/src/cpu/x64/gemm/f32/\njit_avx512_common_gemm_f32.cpp#L1808-L1816\n4We note that while we tried a few other settings for matrix partitioning, a compre-\nhensive sensitivity analysis of partitioning parameters is a part of the future work.\n#threads\nsequence length\n8 64 384\nonednn\nbase\nonednn\nalmo+sor\nonednn\nbase\nonednn\nalmo+sor\nonednn\nbase\nonednn\nalmo+sor\n1 115 80 (x1.44) 216 150 (x1.44) 884 815 (x1.08)\n2 83 48 (x1.73) 133 84 (x1.58) 471 421 (x1.12)\n4 51 32 (x1.59) 76 53 (x1.43) 259 223 (x1.16)\n8 35 24 (x1.46) 49 37 (x1.32) 135 128 (x1.05)\n16 28 22 (x1.27) 36 30 (x1.20) 96 81 (x1.19)\nTable 7: BERT-base inference latency (ms) with the modified\nmatrix partitioning.\nwhich shows significant gains for onednn-almo over onednn-\nbase across most sequence lengths and thread counts. Performance\ncounters (reported by perf) show that with the patched version of\noneDNN, the number of last-level cache (LLC) misses is reduced.\nAlso, while both (patched and non-patched) versions report a similar\nnumber of instructions, the patched version uses significantly less\ncycles, yielding a higher IPC (instruction per cycle) ratio.\nThe results for BERT-base with the modified matrix partitioning\nare given in Table 7. They show that reducing the size of matrix\npartitions has a favorable effect on this model as well.\n7 DISCUSSION\nIn this paper we present the analysis of the inference performance\nfor BERT, one of the most prominent models for NLP based on\nFebruary 23, 2021 â€¢ Copyright Oracle and or its affiliates\nOptimizing Inference Performance of Transformers on CPUs\nthe Transfomer architecture, on CPU-based systems. The analy-\nsis demonstrates clearly that the way to speeding up inference\nlies through the optimization of the matmul operation. Based on\nthis observation, we investigate three optimizations for speeding\nup matmul operations, which collectively lead to the inference\nspeedup of up to x2.37 for Transfomer-based models over estab-\nlished baselines. The optimizations do not require any changes to\nthe implementation of those models, and they do not affect their\naccuracy. We further note that while the focus of our work has\nbeen the Transfomer architecture, our results are applicable to any\nmachine learning model in which matmul operations consume a\nsignificant portion of the inference time.\nOur work underscores the importance of the operation fusion as\na technique for optimizing computation during inference [13, 15].\nSuch fusion would reduce the amount of sequential overhead in\ninvoking individual operations (cf. Figure 4 (b)) and, in general,\nbring the scalability of high-level operations, such as the Linear\nmodule computation, closer to their low-level counterpart (cf. Fig-\nure 4 (a)). Furthermore, our work demonstrates that tuning matrix\npartitioning can lead to substantial matmul speedups. An adaptive\napproach similar to the one discussed in Section 6.1, but applied to\nthe matrix partitioning parameters, might be warranted.\nAnother related future direction is scaling primitive operations\nbeyond matmul. While matmul is responsible for the lionâ€™s share\nof the inference time, the portion of other operations grows as\nthe number of threads increases. For instance, for short sequences,\nthe share of the time spent in the layer normalization operation\ngrows from 2.9% for 1 thread to 12% for 16 threads (cf. Figure 2).\nParallelizing those operations and fusing them with matmul should\nprovide further improvement to the inference performance.\nREFERENCES\n[1] Jay Alammar. The Illustrated Transformer. http://jalammar.github.io/illustrated-\ntransformer/. Accessed: 01-07-21.\n[2] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-\ndocument transformer. CoRR, abs/2004.05150, 2020.\n[3] Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, and Niranjan Balasubra-\nmanian. Deformer: Decomposing pre-trained transformers for faster question\nanswering. In Proc. of Conference of the Association for Computational Linguistics\n(ACL), pages 4487â€“4497, 2020.\n[4] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and\nRuslan Salakhutdinov. Transformer-xl: Attentive language models beyond a\nfixed-length context. In Proc. of Conference of the Association for Computational\nLinguistics (ACL), pages 2978â€“2988, 2019.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-\ntraining of deep bidirectional transformers for language understanding. In Proc.\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, (NAACL-HLT), pages 4171â€“4186, 2019.\n[6] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbotransformers: An\nefficient GPU serving system for transformer models. CoRR, abs/2010.05680,\n2020.\n[7] Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tie-Yan Liu. Effi-\ncient training of BERT by progressively stacking. In Proceedings of International\nConference on Machine Learning (ICML), volume 97, pages 2337â€“2346, 2019.\n[8] Kazushige Goto and Robert A. van de Geijn. Anatomy of high-performance\nmatrix multiplication. ACM Trans. Math. Softw., 34(3), 2008.\n[9] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). CoRR,\nabs/1606.08415, 2016.\n[10] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learn-\ning of language representations. In Proc. of International Conference on Learning\nRepresentations, (ICLR), 2020.\n[11] Quoc N. Le and Kip Kaehler. How We Scaled Bert To Serve 1+ Billion Daily\nRequests on CPUs. https://robloxtechblog .com/how-we-scaled-bert-to-serve-1-\nbillion-daily-requests-on-cpus-d99be090db26. Published: 05-27-20, Accessed:\n01-06-21.\n[12] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.\n[13] Yizhi Liu, Yao Wang, Ruofei Yu, Mu Li, Vin Sharma, and Yida Wang. Optimizing\nCNN model inference on cpus. In Proc. of USENIX Annual Technical Conference\n(ATC), pages 1025â€“1040, 2019.\n[14] Pandu Nayak. Understanding searches better than ever before. https://\nblog.google/products/search/search-language-understanding-bert/. Published:\n10-25-19, Accessed: 01-06-21.\n[15] Emma Ning, Nathan Yan, Jeffrey Zhu, and Jason Li. Microsoft open\nsources breakthrough optimizations for transformer inference on gpu and\ncpu. https://cloudblogs .microsoft.com/opensource/2020/01/21/microsoft-onnx-\nopen-source-optimizations-transformer-inference-gpu-cpu/. Published: 01-20-\n20, Accessed: 01-06-21.\n[16] Harvard NLP. The Annotated Transformer. https://nlp .seas.harvard.edu/2018/\n04/03/attention.html. Accessed: 01-07-21.\n[17] oneAPI. BLAS functions. https://oneapi-src .github.io/oneDNN/\ngroup__dnnl__api__blas.html. Accessed: 01-07-21.\n[18] oneAPI. oneAPI Deep Neural Network Library (oneDNN). https://github .com/\noneapi-src/oneDNN. Accessed: 01-07-21.\n[19] oneAPI. Understanding Memory Formats. https://oneapi-src .github.io/oneDNN/\nunderstanding_memory_formats.html. Accessed: 01-07-21.\n[20] Pytorch. https://github .com/pytorch/pytorch. Accessed: 01-07-21.\n[21] Pytorch. Efficient forward pass in nn.Linear. https://github.com/pytorch/pytorch/\nissues/2159. Accessed: 01-07-21.\n[22] Pytorch. Torchscript. https://pytorch .org/docs/stable/jit.html. Accessed: 01-07-\n21.\n[23] Pytorch. Torchscript for Deployment. https://pytorch .org/tutorials/recipes/\ntorchscript_inference.html. Accessed: 01-07-21.\n[24] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distil-\nbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR,\nabs/1910.01108, 2019.\n[25] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation\nfor BERT model compression. In Kentaro Inui, Jing Jiang, Vincent Ng, and\nXiaojun Wan, editors, Proc. Conference on Empirical Methods in Natural Language\nProcessing, pages 4322â€“4331, 2019.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nIn Proc. of Conference on Neural Information Processing Systems (NIPS), pages\n5998â€“6008, 2017.\n[27] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer:\nSelf-attention with linear complexity. CoRR, abs/2006.04768, 2020.\n[28] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.\nMiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-\nTrained Transformers. In Proc. of Conference on Neural Information Processing\nSystems (NIPS), 2020.\n[29] Y. Wang, Q. Wang, and X. Chu. Energy-efficient inference service of transformer-\nbased deep learning models on gpus. In IEEE Conferences on Green Computing\nand Communications (GreenCom), pages 323â€“331, 2020.\n[30] Yu Emma Wang. Mille Crepe Bench: multi-layer performance analysis for deep\nlearning frameworks. https://github.com/Emma926/mcbench. Accessed: 12-29-\n20.\n[31] Yu Emma Wang, Carole-Jean Wu, Xiaodong Wang, Kim M. Hazelwood, and David\nBrooks. Exploiting parallelism opportunities with deep learning frameworks.\nCoRR, abs/1908.04705, 2019.\n[32] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and\nAlexander M. Rush. Transformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 38â€“45, October 2020.\n[33] Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury, Marat\nDukhan, Kim M. Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, Tommer Leyvand,\nHao Lu, Yang Lu, Lin Qiao, Brandon Reagen, Joe Spisak, Fei Sun, Andrew Tulloch,\nPeter Vajda, Xiaodong Wang, Yanghan Wang, Bram Wasti, Yiming Wu, Ran Xian,\nSungjoo Yoo, and Peizhao Zhang. Machine Learning at Facebook: Understanding\nInference at the Edge. In IEEE International Symposium on High Performance\nComputer Architecture (HPCA), pages 331â€“344, 2019.\n[34] Shufan Wu, Tao Lv, Pengxin Yuan, Patric Zhao, Jason Ye, and Haibin Lin. Opti-\nmization for BERT Inference Performance on CPU. https://medium .com/apache-\nmxnet/optimization-for-bert-inference-performance-on-cpu-3bb2413d376c.\nPublished: 09-12-19, Accessed: 01-06-21.\n[35] Patrick Xia, Shijie Wu, and Benjamin Van Durme. Which *BERT? A survey\norganizing contextualized encoders. In Proc. of Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 7516â€“7533, 2020.\n[36] Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bho-\njanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large\nFebruary 23, 2021 â€¢ Copyright Oracle and or its affiliates\nDave Dice and Alex Kogan\nbatch optimization for deep learning: Training BERT in 76 minutes. In Proc. of\nInternational Conference on Learning Representations (ICLR), 2020.\n[37] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\nAlberti, Santiago OntaÃ±Ã³n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. Big bird: Transformers for longer sequences. In Proc. of\nAdvances in Neural Information Processing Systems (NeurIPS), 2020.\n[38] Jeffrey Zhu. Bing delivers its largest improvement in search experience using\nAzure GPUs. https://azure .microsoft.com/en-us/blog/bing-delivers-its-largest-\nimprovement-in-search-experience-using-azure-gpus/. Published: 11-18-19,\nAccessed: 01-06-21.\nFebruary 23, 2021 â€¢ Copyright Oracle and or its affiliates"
}