{
    "title": "Forgetful Large Language Models: Lessons Learned from Using LLMs in Robot Programming",
    "url": "https://openalex.org/W4391117134",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5015249061",
            "name": "Juo-Tung Chen",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2119940187",
            "name": "Chien-Ming Huang",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A5015249061",
            "name": "Juo-Tung Chen",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2119940187",
            "name": "Chien-Ming Huang",
            "affiliations": [
                "Johns Hopkins University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6801784503",
        "https://openalex.org/W4383046944",
        "https://openalex.org/W4366734321",
        "https://openalex.org/W4200633400",
        "https://openalex.org/W4394828156",
        "https://openalex.org/W4385681252",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W3202764408",
        "https://openalex.org/W4385944467",
        "https://openalex.org/W4367860052"
    ],
    "abstract": "Large language models offer new ways of empowering people to program robot applications-namely, code generation via prompting. However, the code generated by LLMs is susceptible to errors. This work reports a preliminary exploration that empirically characterizes common errors produced by LLMs in robot programming. We categorize these errors into two phases: interpretation and execution. In this work, we focus on errors in execution and observe that they are caused by LLMs being “forgetful” of key information provided in user prompts. Based on this observation, we propose prompt engineering tactics designed to reduce errors in execution. We then demonstrate the effectiveness of these tactics with three language models: ChatGPT, Bard, and LLaMA-2. Finally, we discuss lessons learned from using LLMs in robot programming and call for the benchmarking of LLM-powered end-user development of robot applications.",
    "full_text": "Forgetful Large Language Models:\nLessons Learned from Using LLMs in Robot Programming\nJuo-Tung Chen, Chien-Ming Huang\nJohns Hopkins University, Baltimore, MD 21218, USA\njchen396@jhu.edu, chienming.huang@jhu.edu\nAbstract\nLarge language models offer new ways of empowering peo-\nple to program robot applications—namely, code generation\nvia prompting. However, the code generated by LLMs is sus-\nceptible to errors. This work reports a preliminary exploration\nthat empirically characterizes common errors produced by\nLLMs in robot programming. We categorize these errors into\ntwo phases: interpretation and execution. In this work, we fo-\ncus on errors in execution and observe that they are caused by\nLLMs being “forgetful” of key information provided in user\nprompts. Based on this observation, we propose prompt en-\ngineering tactics designed to reduce errors in execution. We\nthen demonstrate the effectiveness of these tactics with three\nlanguage models: ChatGPT, Bard, and LLaMA-2. Finally, we\ndiscuss lessons learned from using LLMs in robot program-\nming and call for the benchmarking of LLM-powered end-\nuser development of robot applications.\nIntroduction\nProgrammable robots have enabled a wide range of applica-\ntions, ranging from flexible automation to people-facing ser-\nvices. However, programming robot applications effectively\nrequires years of training and experience. The paradigm of\nend-user programming lowers the barriers to robot program-\nming (Ajaykumar, Steele, and Huang 2021) and empow-\ners end users to develop custom robot applications without\nsubstantial engineering training. The rise of large language\nmodels introduces new opportunities in this paradigm by of-\nfering a natural interface in which end users may program\nrobots (Vemprala et al. 2023).\nHowever, LLM-powered code generation is not error-free\ndue to its nondeterministic nature (Ouyang et al. 2023). De-\nspite extensive research efforts aimed at assessing the effec-\ntiveness and accuracy of LLM-based code generation tools,\ncertain limitations persist. For instance, these tools may pro-\nduce inconsistent and occasionally incorrect code outputs.\nExisting studies have employed approaches such as bench-\nmark evaluations (Liu et al. 2023a; Chen et al. 2021; Ham-\nmond Pearce et al. 2021) and systematic empirical assess-\nments (Liu et al. 2023b) to explore the capabilities of and\nchallenges in LLM-powered code generation. While these\ninvestigations have illuminated various errors and obstacles\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nthat may arise during the code generation process, they often\nfell short in providing comprehensive solutions to enhance\ncode generation stability and minimize the occurrence of er-\nrors.\nIt is worth noting that existing research often focuses pri-\nmarily on general benchmarking errors, aiming to identify\ncommon pitfalls and shortcomings in LLM-generated code;\ntherefore, these studies may not fully capture the specific\nnuances and intricacies of code specific to a specialized do-\nmain such as robotics. As a result, while such benchmark\nevaluations provide valuable insights into the overall perfor-\nmance of LLMs, they may not comprehensively address the\nunique challenges posed by code generation for robotic ap-\nplications.\nAs a step toward developing the empirical science of in-\ncorporating LLMs into robot programming processes, in\nthis work, we sought to explore two research questions: 1)\nWhat are the common errors produced by LLMs in end-\nuser robot programming?and 2) What practical strategies\ncan be employed to mitigate and reduce these errors?To\nground our exploration, we designed a sequential manipu-\nlation task (Figure 1) and tested three language models—\nChatGPT, Bard, and LLaMA-2—to assess their capabilities\nin generating code to complete the task.\nOur key findings are 1) LLMs are “forgetful” and do not\nconsider information provided in the system prompt as hard\nfact; 2) the forgetfulness of LLMs leads to errors in code ex-\necution; 3) in addition to execution errors, LLMs make vari-\nous errors (e.g., syntax errors, missing necessary libraries)\nthat cause failures in code interpretation; and 4) simple\nstrategies—such as reinforcing task constraints in the objec-\ntive prompt and extracting numerical task contexts from the\nsystem prompt and storing them in data structures—seem to\nnotably reduce execution errors caused by LLM forgetful-\nness.\nExperiment 1: Identifying Common Errors\nProgramming Task\nIn order to assess the code generation ability and perfor-\nmance of LLMs in robot programming, we set up a sequen-\ntial manipulation task. Our experimental setup includes a\nUR5 manipulator paired with a webcam for basic percep-\ntion via AR markers, allowing for the registration of task\nAAAI Fall Symposium Series (FSS-23)\n508\nHome Reach Grasp Pour Put back\nFigure 1: Sequential task execution by the robotic system. The five stages encompass homing, reaching the cylinder, grasping\nit, pouring its contents into a beaker, and returning the cylinder to its initial position.\nobjects into a virtual workspace for precise and accurate mo-\ntion planning via MoveIt. The sequential manipulation task\ninvolves the robot picking up a graduated cylinder and pour-\ning its contents into a beaker; this task is a common step in\nbiochemical lab tests1. The high-level procedure of the ma-\nnipulation task involves:\n1. Moving the robot to a home (neutral) position;\n2. Reaching out to the graduated cylinder;\n3. Grasping the graduated cylinder at its midpoint;\n4. Performing the pouring action (including moving to the\ntarget location and rotating the robot’s end effector); and\n5. Placing the cylinder back in its original position.\nBaseline Prompt\nA descriptive prompt is believed to enhance the quality of\nLLM-generated responses. It has been documented that a\nwell-constructed prompt should contain the following com-\nponents (Vemprala et al. 2023): constraints and require-\nments, environmental description, current state of the sys-\ntem, goals and objectives, description of the robotic API\nlibrary, and solution examples. Consequently, our baseline\nprompt is composed of four parts:system prompt, descrip-\ntion of robotic API library, solution example,and objec-\ntive prompt.See the appendix 2 for the full baseline prompt\nused in our experiments.\nSystem Prompt Here, we defined the role of the LLM and\nprovided it with task constraints and requirements. We addi-\ntionally included contextual details regarding the environ-\nment to alert the LLM to potential task objects.\nDescription of Robotic API LibraryWe provided a clear\nrundown of how each high-level function provided for the\nLLM should be used, along with useful reminders and con-\nventions. It is worth noting that by providing descriptive\nnames for all of the API functions, the LLM’s ability to\nunderstand the functional links between APIs may be en-\nhanced, which can facilitate the LLM to produce more desir-\nable outcomes for the given problem (Vemprala et al. 2023).\nSolution Example We provided an example solution to\nguide the LLM’s solution strategy and to (hopefully) prevent\nit from generating erroneous responses.\n1We envision the automation of several biochemical lab tests\nthrough custom robot applications so as to accelerate scientific ex-\nperimentation.\n2https://tinyurl.com/AAAI-Appendix\nObjective Prompt Here, we articulated the intended ob-\njective for the LLM to respond to while considering all\nprompts as outlined previously. Below is the objective\nprompt used in our experiments:\nPlease write a Python function to pick up a 25mL graduated\ncylinder at Marker 15 and pour its contents into a 500mL\nbeaker at Marker 7. After that, put the cylinder back to where\nit was.\nLarge Language Models\nIn our experiments, we used three language models: Chat-\nGPT (3.5-turbo-0613), Bard, and LLaMA-2 (13B parame-\nters). Given the stochastic nature of these LLMs, each model\nwas tested ten times while keeping the prompts and sequen-\ntial manipulation task the same across trials.\nFindings\nOur first experiment sought to understand common errors\nproduced by the three language models. To this end, we\nmanually characterized the observed errors, which can be\ngrouped roughly into two categories representing errors in\ndifferent phases of application development—errors in in-\nterpretation and errors in execution—as illustrated in Fig-\nure 2. We note that there may be errors in motion planning\nthat have nothing to do with LLM-generated code, which is\noutside the scope of this work.\nErrors in Interpretation Errors in this category cause\nfailures in code interpretation and include four different sub-\ntypes:\na. Name Error:This error type includes instances where\nreferences to variables or functions precede their defini-\ntion or initialization within the code (Figure 3).\nb. Syntax Error:Characterized by syntactically incorrect\ncode structures, this error type hinders the proper inter-\npretation of the generated code (Figure 4).\nc. Import Error:This error type typically indicates that the\ngenerated code does not include the necessary libraries\nfor code interpretation (Figure 5).\nd. ROS Error:Within the context of the Robot Operating\nSystem, this type of error surfaces due to the omission of\nROS node initialization or incorrect utilization of ROS\npackages, negatively impacting the overall communica-\ntion and coordination within the robotic system (Figure\n6).\n509\nEnd-User Developer\nLarge\nLanguage\nModel\nName\nError\nSyntax\nError\nImport\nError\nROS\nError\nFactual\nError\nPhysical\nError\nCode\nInterpreter\nPrompts Code Control commands\nMoveItErrors in\nmotion planning\nErrors in interpretation Errors in execution\nnot due to generated code\nFigure 2: Workflow and the emergence of potential errors in utilizing an LLM in robot programming.\n\n\n\n\nName error\n\n\n\n\n\n\n\n\n\n\nSyntax error\n\n\n\n\n\n\nImport error\n\n\n\n\n\n\n\nROS error\n\n\n\n\n\n\nFactual error\n\n\n\n\n\n\n\n\n\n\n\n# Move and place the 100mL beaker at marker 7 \n    move_and_place_object(\"beaker_100mL\", 7) \n    undefined function\n\n\n\n\n\n\n\n\n# Open the gripper to release the cylinder \n    lib.open_gripper( )  \n    unclosed parenthesis\n\nforgot to import libraries \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n# Initialize rospy node called gpt \nrospy.init_node('gpt')\n \n \n# Initialize function library \nlib = FunctionLib() \nunnecessary steps  \n(will result in task failure)\n  \n  ... \n    # Pour into beaker 500mL \n    lib.pour(\"beaker 500mL\") \n   \n    # Move above 0.1 meters the beaker's location \n    success = lib.go(beaker[0], beaker[1], beaker[2] + 0.1, \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Move down to pour the contents into the beaker \n    success = lib.go(beaker[0], beaker[1], beaker[2], \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n \n    # Move above 0.1 meters the cylinder's location \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2] + 0.1, \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Move down to put the cylinder back \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2], \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n    ...\n\n\n\nrospy.init_node('gpt')\nforgot to initialize rospy node \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n \n \n \n# initialize function library \nlib = FunctionLib()\n# Define the objects dimensions  \ncylinder_25mL_height = 0.065 \ncylinder_25mL_radius = 0.0125 \nbeaker_500mL_height = 0.25 \nbeaker_500mL_radius = 0.065\n fabricated numerical values \nFigure 3: Name error (using undefined functions).\n\n\n\n\nName error\n\n\n\n\n\n\n\n\n\n\nSyntax error\n\n\n\n\n\n\nImport error\n\n\n\n\n\n\n\nROS error\n\n\n\n\n\n\nFactual error\n\n\n\n\n\n\n\n\n\n\n\n# Move and place the 100mL beaker at marker 7 \n    move_and_place_object(\"beaker_100mL\", 7) \n    undefined function\n\n\n\n\n\n\n\n\n# Open the gripper to release the cylinder \n    lib.open_gripper( )  \n    unclosed parenthesis\n\nforgot to import libraries \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n# Initialize rospy node called gpt \nrospy.init_node('gpt')\n \n \n# Initialize function library \nlib = FunctionLib() \nunnecessary steps  \n(will result in task failure)\n  \n  ... \n    # Pour into beaker 500mL \n    lib.pour(\"beaker 500mL\") \n   \n    # Move above 0.1 meters the beaker's location \n    success = lib.go(beaker[0], beaker[1], beaker[2] + 0.1, \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Move down to pour the contents into the beaker \n    success = lib.go(beaker[0], beaker[1], beaker[2], \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n \n    # Move above 0.1 meters the cylinder's location \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2] + 0.1, \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Move down to put the cylinder back \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2], \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n    ...\n\n\n\nrospy.init_node('gpt')\nforgot to initialize rospy node \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n \n \n \n# initialize function library \nlib = FunctionLib()\n# Define the objects dimensions  \ncylinder_25mL_height = 0.065 \ncylinder_25mL_radius = 0.0125 \nbeaker_500mL_height = 0.25 \nbeaker_500mL_radius = 0.065\n fabricated numerical values \nFigure 4: Syntax error (syntactically incorrect).\nErrors in Execution Errors in this category cause fail-\nures in code execution—even though the code may be\ninterpretable—and include two types:\na. Factual Error:This error type indicates model halluci-\nnation; for example, instead of using numerical values\nthat the user provides in the system prompt to describe\nthe task objects, the model fabricates numbers, subse-\nquently causing errors in motion planning or execution\n(Figure 7).\nb. Physical Error:This error type includes errors that ul-\ntimately cause execution failures even if all other error\ntypes are not present. Examples include adding unneces-\nsary steps to the action sequence (Figure 8).\nThe two error categories—interpretationand execution—\ncall for different methods of error handling. Errors in inter-\npretation are typically caught by the program interpreter or\ncompiler, which displays error messages that help users ad-\ndress the errors in a more straightforward identification and\nrectification process (Inagaki et al. 2023). In contrast, er-\nrors in execution are less obvious, as they do not necessarily\ncause immediate code breakdown; these errors surface only\nwhen undesirable task outcomes are observed.\nOur experiment revealed varying patterns of error occur-\nrence across the three language models (Table 1). To our sur-\nprise, none of the three models successfully completed the\nintended task in any of the trials. This result underscores the\nchallenges involved in translating end-user prompts into ac-\n\n\n\n\nName error\n\n\n\n\n\n\n\n\n\n\nSyntax error\n\n\n\n\n\n\nImport error\n\n\n\n\n\n\n\nROS error\n\n\n\n\n\n\nFactual error\n\n\n\n\n\n\n\n\n\n\n\n# Move and place the 100mL beaker at marker 7 \n    move_and_place_object(\"beaker_100mL\", 7) \n    undefined function\n\n\n\n\n\n\n\n\n# Open the gripper to release the cylinder \n    lib.open_gripper( )  \n    unclosed parenthesis\n\nforgot to import libraries \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n# Initialize rospy node called gpt \nrospy.init_node('gpt')\n \n \n# Initialize function library \nlib = FunctionLib() \nunnecessary steps  \n(will result in task failure)\n  \n  ... \n    # Pour into beaker 500mL \n    lib.pour(\"beaker 500mL\") \n   \n    # Move above 0.1 meters the beaker's location \n    success = lib.go(beaker[0], beaker[1], beaker[2] + 0.1, \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Move down to pour the contents into the beaker \n    success = lib.go(beaker[0], beaker[1], beaker[2], \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n \n    # Move above 0.1 meters the cylinder's location \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2] + 0.1, \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Move down to put the cylinder back \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2], \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n    ...\n\n\n\nrospy.init_node('gpt')\nforgot to initialize rospy node \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n \n \n \n# initialize function library \nlib = FunctionLib()\n# Define the objects dimensions  \ncylinder_25mL_height = 0.065 \ncylinder_25mL_radius = 0.0125 \nbeaker_500mL_height = 0.25 \nbeaker_500mL_radius = 0.065\n fabricated numerical values \nFigure 5: Import error (oversight in importing necessary\nlibraries).\n\n\n\n\nName error\n\n\n\n\n\n\n\n\n\n\nSyntax error\n\n\n\n\n\n\nImport error\n\n\n\n\n\n\n\nROS error\n\n\n\n\n\n\nFactual error\n\n\n\n\n\n\n\n\n\n\n\n# Move and place the 100mL beaker at marker 7 \n    move_and_place_object(\"beaker_100mL\", 7) \n    undefined function\n\n\n\n\n\n\n\n\n# Open the gripper to release the cylinder \n    lib.open_gripper( )  \n    unclosed parenthesis\n\nforgot to import libraries \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n# Initialize rospy node called gpt \nrospy.init_node('gpt')\n \n \n# Initialize function library \nlib = FunctionLib() \nunnecessary steps  \n(will result in task failure)\n  \n  ... \n    # Pour into beaker 500mL \n    lib.pour(\"beaker 500mL\") \n   \n    # Move above 0.1 meters the beaker's location \n    success = lib.go(beaker[0], beaker[1], beaker[2] + 0.1, \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Move down to pour the contents into the beaker \n    success = lib.go(beaker[0], beaker[1], beaker[2], \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n \n    # Move above 0.1 meters the cylinder's location \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2] + 0.1, \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Move down to put the cylinder back \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2], \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n    ...\n\n\n\nrospy.init_node('gpt')\nforgot to initialize rospy node \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n \n \n \n# initialize function library \nlib = FunctionLib()\n# Define the objects dimensions  \ncylinder_25mL_height = 0.065 \ncylinder_25mL_radius = 0.0125 \nbeaker_500mL_height = 0.25 \nbeaker_500mL_radius = 0.065\n fabricated numerical values \nFigure 6: ROS error (omission of ROS node initialization).\ncurate and executable robot control code via LLMs. Further-\nmore, across the three models evaluated, factual and physi-\ncal errors were most common; the prevalence of these er-\nrors highlights a key limitation of LLM-based code genera-\ntion for end-user development of robot applications, which\nprompted us to explore practical strategies to reduce these\ntypes of execution errors.\nExperiment 2: Exploring Practical Strategies\nto Reduce Errors in Execution\nThis experiment studied strategies that might enhance an\nLLM’s ability to generate accurate and reliable code for\nrobotic applications. This experiment followed the same\nprotocol (e.g., same manipulation task, ten trials per lan-\nguage model) as the first experiment.\nPractical Strategies\nIn Experiment 1, we found that errors in execution may be\nattributed to the “forgetfulness” of LLMs; the models ap-\n510\nModel GPT 3.5 Bard LLaMA-2\nTrial Types of error Completion Types of error Completion Types of error Completion\n1 Factual, Physical No Factual, Physical No Factual, Physical No\n2 Factual, Physical No Factual, Physical No Factual, Physical No\n3 Factual, Physical No Factual, Physical No Factual, Physical No\n4 Factual, Physical No Factual, Physical No Factual, Physical No\n5 Factual, Physical No Factual, Physical No Factual, Physical No\n6 Factual, Physical, Import, ROS No Factual, Physical No Factual, Physical No\n7 Factual, Physical No Factual, Physical No Physical, Name No\n8 Factual, Physical, Import, ROS No Factual, Physical No Factual No\n9 Factual, Import, ROS No Factual, Physical No Factual, Physical No\n10 Factual, Physical, Import, ROS No Factual, Physical No Factual, Physical No\nTable 1: Common error identification experiment results.\n\n\n\n\nName error\n\n\n\n\n\n\n\n\n\n\nSyntax error\n\n\n\n\n\n\nImport error\n\n\n\n\n\n\n\nROS error\n\n\n\n\n\n\nFactual error\n\n\n\n\n\n\n\n\n\n\n\n# Move and place the 100mL beaker at marker 7 \n    move_and_place_object(\"beaker_100mL\", 7) \n    undefined function\n\n\n\n\n\n\n\n\n# Open the gripper to release the cylinder \n    lib.open_gripper( )  \n    unclosed parenthesis\n\nforgot to import libraries \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n# Initialize rospy node called gpt \nrospy.init_node('gpt')\n \n \n# Initialize function library \nlib = FunctionLib() \nunnecessary steps  \n(will result in task failure)\n  \n  ... \n    # Pour into beaker 500mL \n    lib.pour(\"beaker 500mL\") \n   \n    # Move above 0.1 meters the beaker's location \n    success = lib.go(beaker[0], beaker[1], beaker[2] + 0.1, \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Move down to pour the contents into the beaker \n    success = lib.go(beaker[0], beaker[1], beaker[2], \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n \n    # Move above 0.1 meters the cylinder's location \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2] + 0.1, \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Move down to put the cylinder back \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2], \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n    ...\n\n\n\nrospy.init_node('gpt')\nforgot to initialize rospy node \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n \n \n \n# initialize function library \nlib = FunctionLib()\n# Define the objects dimensions  \ncylinder_25mL_height = 0.065 \ncylinder_25mL_radius = 0.0125 \nbeaker_500mL_height = 0.25 \nbeaker_500mL_radius = 0.065\n fabricated numerical values \nFigure 7: Factual error (using fabricated numerical values\nto define the objects’ dimensions).\npear to “forget” the information provided in user prompts\nor do not treat the provided description as factual informa-\ntion to use in code generation. Therefore, we explored the\nfollowing strategies’ effectiveness in addressing the issue of\nforgetfulness:\n1. When prompts involve task/context information specified\nin numerical form, implement dedicated functions for re-\ntrieving precise, numerical data. (Figure 9)\n2. When dealing with intricate functions (like the pour func-\ntion in our experiment), reinforce key constraints in the\nobjective prompt to ensure more accurate and reliable\ncode generation. (Figure 10)\nIn addition to the these strategies, enhancing the clarity\nand specificity of the objective prompt by articulating its\nphysical implications or providing greater descriptive con-\ntext can also help curtail excessive divergence in LLM-\ngenerated code. Our implementations of these strategies are\nshown in Figures 9 and 10.\nFindings\nTable 2 shows the results of adopting the strategies proposed\nabove. Across all models, we observed a substantial increase\nin successful task completion and a decrease in the number\nof factual and physical errors. Specifically, ChatGPT was\nable to achieve a task completion rate of 60% and errors in\nexecution were reduced by 94.7% as compared to its results\nin Experiment 1. Bard achieved a similar success rate of 70%\nwith strategy implementation and the occurrence of factual\nand physical errors was reduced by 95%. However, LLaMA-\n2-13B only reached a task completion rate of 40% using the\nstrategies and factual and physical errors were reduced by\nonly 83.3%.\n\n\n\n\nName error\n\n\n\n\n\n\n\n\n\n\nSyntax error\n\n\n\n\n\n\nImport error\n\n\n\n\n\n\n\nROS error\n\n\n\n\n\n\nFactual error\n\n\n\n\n\n\n\n\n\n\n\n# Move and place the 100mL beaker at marker 7 \n    move_and_place_object(\"beaker_100mL\", 7) \n    undefined function\n\n\n\n\n\n\n\n\n# Open the gripper to release the cylinder \n    lib.open_gripper( )  \n    unclosed parenthesis\n\nforgot to import libraries \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n# Initialize rospy node called gpt \nrospy.init_node('gpt')\n \n \n# Initialize function library \nlib = FunctionLib() \nunnecessary steps  \n(will result in task failure)\n  \n  ... \n    # Pour into beaker 500mL \n    lib.pour(\"beaker 500mL\") \n   \n    # Move above 0.1 meters the beaker's location \n    success = lib.go(beaker[0], beaker[1], beaker[2] + 0.1, \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Move down to pour the contents into the beaker \n    success = lib.go(beaker[0], beaker[1], beaker[2], \n                     beaker[3], beaker[4], beaker[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n \n    # Move above 0.1 meters the cylinder's location \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2] + 0.1, \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Move down to put the cylinder back \n    success = lib.go(cylinder[0], cylinder[1], cylinder[2], \n                     cylinder[3], cylinder[4], cylinder[5]) \n   \n    # Open the gripper to release the cylinder \n    lib.open_gripper() \n    ...\n\n\n\nrospy.init_node('gpt')\nforgot to initialize rospy node \nimport rospy \nfrom Lib.ur5.FunctionLibrary import FunctionLib \n \n \n \n# initialize function library \nlib = FunctionLib()\n# Define the objects dimensions  \ncylinder_25mL_height = 0.065 \ncylinder_25mL_radius = 0.0125 \nbeaker_500mL_height = 0.25 \nbeaker_500mL_radius = 0.065\n fabricated numerical values \nFigure 8: Physical error (impractical physical action for the\ngiven task).\nDiscussion\nLessons Learned\nWhile promising, LLM-based code generation for end-user\ndevelopment of robot applications remains inconsistent,\nwhich is unsurprising given the intricate and probabilistic\ndesign of these models. This work highlights the importance\nof keeping users in the loop in application development.\nWe additionally determined that the success of LLM-\npowered code generation often hinges on the user’s ability\nto provide explicit and descriptive objective prompts; for in-\nstance, specifying detailed instructions such as “Place the\ncylinder back to its original position” yields more accurate\nresults than ambiguous directives like “Put it back.”\nFurthermore, we found that errors in execution primarily\nstem from the forgetfulness of LLMs, which causes them\nto overlook information supplied in prompts. Consequently,\n511\nModel GPT 3.5 Bard LLaMA-2\nTrial Types of error Completion Types of error Completion Types of error Completion\n1 None Yes None Yes None Yes\n2 None Yes None Yes Name No\n3 Import, ROS, Factual No Import, ROS, Factual No None Yes\n4 Name No Name No None Yes\n5 Import, ROS, Name No None Yes Name, Physical No\n6 None Yes None Yes Name, Physical No\n7 None Yes None Yes Name, Physical No\n8 Name No None Yes None Yes\n9 None Yes None Yes Name No\n10 None Yes None Yes Name No\nTable 2: Strategic Prompting Experiment Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsolution example prompt\n# Defining the objects dimensions \ncylinder_100mL_height = 0.245 \ncylinder_100mL_radius = 0.02 \nbeaker_1L_height = 0.15 \nbeaker_1L_radius = 0.065\n# Get the objects' dimensions by calling get_object_dimensions function \ncylinder_dims = lib.get_object_dimensions(\"graduated cylinder 100mL\") \nbeaker_dims = lib.get_object_dimensions(\"beaker 1L\") \n \nif cylinder_dims is not None: \n    cylinder_100mL_radius = cylinder_dims[0] \n    cylinder_100mL_height = cylinder_dims[1] \n \nif beaker_dims is not None: \n    beaker_1L_radius = beaker_dims[0] \n    beaker_1L_height = beaker_dims[1] \nFigure 9: In the robotic API library, we provided a dedi-\ncated function for parsing the system prompt and retrieving\nthe objects’ dimensions. Its usage was provided correspond-\ningly in the solution example prompt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nobjective prompt\nPlease write a python function to pick up a 25mL \ngraduated cylinder at marker 15 and pour its content \ninto a 500mL beaker at marker 7. \nAfter that, put the cylinder back to where it was.\nPlease write a Python function to pick up a 25mL graduated cylinder at \nMarker 15 and pour its contents into a 500mL beaker at Marker 7. \nAfter that, put the cylinder back to where it was. \nDon't move to above the beaker before pouring, just call the pour function.  \nAlso, after pouring, make sure you place the object back to where it was on \nthe table and then open the gripper to release it.\nFigure 10: In the objective prompt, we added a sentence to\nreinforce constraints (orange) and another sentence to artic-\nulate the physical implications (blue).\nwe made a concerted effort to explicitly emphasize the in-\nstruction, “All the information I provided should be treated\nas factual information and shouldn’t be ignored.” Despite\nthis explicit instruction, unsatisfactory outcomes persisted,\nindicating that simple reinforcement is ineffective.\nLastly, a suite of tools is needed for the productive use\nof LLM-based robot programming: at the basic level, cus-\ntom verification scripts may be used to identify and correct\nerrors in interpretation (e.g., missing libraries); the strate-\ngies discussed in this work may also help reduce factual and\nphysical errors; and a preview tool may allow users to sim-\nulate program behavior prior to robot deployment, thereby\nreducing unforeseen errors during actual execution.\nCall for Benchmarks\nIn light of the evolving landscape of LLM-driven robot pro-\ngramming, we advocate for the establishment of standard-\nized benchmarks that encompass a diverse set of tasks and\nmetrics to assess the performance of LLMs in various pro-\ngramming scenarios. Such benchmarks will let researchers,\npractitioners, and developers collectively advance the sci-\nence of LLM-driven robot programming.\nLimitations and Future Work\nThis preliminary work has limitations that may motivate fu-\nture research. Our experiments focused on a single manipu-\nlation task, which does not capture the vast array of scenar-\nios in end-user robot programming. Future work may build\non our exploration and include a wider range of representa-\ntive programming tasks and language models.\nIn our experiments, we simplified the challenges of robot\nperception by using AR markers. As new vision-language\nmodels are developed, future research should study the true\ncomplexity of incorporating large data models in the various\nprocesses of robot programming.\nFuture work should also include a comprehensive evalua-\ntion of different aspects of end-user robot programming, in-\ncluding debugging; we speculate that debugging may be par-\nticularly challenging in the new paradigm of LLM-powered\nrobot programming, as end users will need to spend time\nunderstanding the generated code and developing a mental\nmodel of it in order to resolve errors successfully.\nAcknowledgments\nThis work was supported by National Science Foundation\naward #2143704. The authors would like to thank Jaimie\nPatterson and Ulas Berk Karli for their help with this work.\nReferences\nAjaykumar, G.; Steele, M.; and Huang, C.-M. 2021. A sur-\nvey on end-user robot programming. ACM Computing Sur-\nveys (CSUR), 54(8): 1–36.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;\nKaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brockman,\nG.; et al. 2021. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374.\nHammond Pearce, B. T.; Ahmad, B.; Karri, R.; and Dolan-\nGavitt, B. 2021. Can openai codex and other large lan-\nguage models help us fix security bugs. arXiv preprint\narXiv:2112.02125.\n512\nInagaki, T.; Kato, A.; Takahashi, K.; Ozaki, H.; and Kanda,\nG. N. 2023. LLMs can generate robotic scripts from goal-\noriented instructions in biological laboratory automation.\narXiv preprint arXiv:2304.10267.\nLiu, J.; Xia, C. S.; Wang, Y .; and Zhang, L. 2023a. Is your\ncode generated by chatgpt really correct? rigorous evalua-\ntion of large language models for code generation. arXiv\npreprint arXiv:2305.01210.\nLiu, Z.; Tang, Y .; Luo, X.; Zhou, Y .; and Zhang, L. F.\n2023b. No Need to Lift a Finger Anymore? Assessing the\nQuality of Code Generation by ChatGPT. arXiv preprint\narXiv:2308.04838.\nOuyang, S.; Zhang, J. M.; Harman, M.; and Wang, M.\n2023. LLM is Like a Box of Chocolates: the Non-\ndeterminism of ChatGPT in Code Generation. arXiv\npreprint arXiv:2308.02828.\nVemprala, S.; Bonatti, R.; Bucker, A.; and Kapoor, A. 2023.\nChatgpt for robotics: Design principles and model abilities.\nMicrosoft Auton. Syst. Robot. Res, 2: 20.\n513"
}