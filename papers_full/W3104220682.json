{
    "title": "Long Document Ranking with Query-Directed Sparse Transformer",
    "url": "https://openalex.org/W3104220682",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A4275671831",
            "name": "Jyun-Yu Jiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2226924701",
            "name": "Chenyan Xiong",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2104325211",
            "name": "Chia-Jung Lee",
            "affiliations": [
                "Amazon (United States)",
                "Seattle University"
            ]
        },
        {
            "id": "https://openalex.org/A2009336559",
            "name": "Wei Wang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4295838474",
        "https://openalex.org/W2019509999",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3013936901",
        "https://openalex.org/W2536015822",
        "https://openalex.org/W4297662948",
        "https://openalex.org/W3130740619",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3089973398",
        "https://openalex.org/W3098417575",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3044284384",
        "https://openalex.org/W4236275144",
        "https://openalex.org/W2991671759",
        "https://openalex.org/W2782157559",
        "https://openalex.org/W2911997761",
        "https://openalex.org/W2949251082",
        "https://openalex.org/W2951534261",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2982596739",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2929285476",
        "https://openalex.org/W3004654429",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2909544278",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2648699835",
        "https://openalex.org/W3003186568",
        "https://openalex.org/W4300427681",
        "https://openalex.org/W2804032941",
        "https://openalex.org/W2964012472",
        "https://openalex.org/W4323654151",
        "https://openalex.org/W2970777192",
        "https://openalex.org/W2959673509",
        "https://openalex.org/W2783640434",
        "https://openalex.org/W2922386288",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4297567729",
        "https://openalex.org/W1990589796",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2070740689",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2945127593",
        "https://openalex.org/W3175111331",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W2923890923"
    ],
    "abstract": "The computing cost of transformer self-attention often necessitates breaking long documents to fit in pretrained models in document ranking tasks. In this paper, we design Query-Directed Sparse attention that induces IR-axiomatic structures in transformer self-attention. Our model, QDS-Transformer, enforces the principle properties desired in ranking: local contextualization, hierarchical representation, and query-oriented proximity matching, while it also enjoys efficiency from sparsity. Experiments on four fully supervised and few-shot TREC document ranking benchmarks demonstrate the consistent and robust advantage of QDS-Transformer over previous approaches, as they either retrofit long documents into BERT or use sparse attention without emphasizing IR principles. We further quantify the computing complexity and demonstrates that our sparse attention with TVM implementation is twice more efficient that the fully-connected self-attention. All source codes, trained model, and predictions of this work are available at https://github.com/hallogameboy/QDS-Transformer.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4594–4605\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n4594\nLong Document Ranking with Query-Directed Sparse Transformer\nJyun-Yu Jiang†, Chenyan Xiong‡, Chia-Jung Lee§and Wei Wang†\n†Department of Computer Science, University of California, Los Angeles, USA\n‡Microsoft Research AI, Redmond, USA\n§Amazon, Seattle, USA\n{jyunyu,weiwang}@cs.ucla.edu, chenyan.xiong@microsoft.com, cjlee@amazon.com\nAbstract\nThe computing cost of transformer self-\nattention often necessitates breaking long doc-\numents to ﬁt in pretrained models in docu-\nment ranking tasks. In this paper, we de-\nsign Query-Directed Sparse attention that in-\nduces IR-axiomatic structures in transformer\nself-attention. Our model, QDS-Transformer,\nenforces the principle properties desired in\nranking: local contextualization, hierarchi-\ncal representation, and query-oriented prox-\nimity matching, while it also enjoys efﬁ-\nciency from sparsity. Experiments on one\nfully supervised and three few-shot TREC\ndocument ranking benchmarks demonstrate\nthe consistent and robust advantage of QDS-\nTransformer over previous approaches, as they\neither retroﬁt long documents into BERT or\nuse sparse attention without emphasizing IR\nprinciples. We further quantify the computing\ncomplexity and demonstrates that our sparse\nattention with TVM implementation is twice\nmore efﬁcient that the fully-connected self-\nattention. All source codes, trained model,\nand predictions of this work are available\nat https://github.com/hallogameboy/\nQDS-Transformer.\n1 Introduction\nPre-trained Transformers such as BERT (Devlin\net al., 2019) effectively transfer language under-\nstanding to better relevance estimation in many\nsearch ranking tasks (Nogueira and Cho, 2019;\nNogueira et al., 2019; Yang et al., 2019). Nev-\nertheless, the effectiveness comes at the quadratic\ncost O(n2) in computing complexity corresponds\nto the text length n, prohibiting its direct appli-\ncation to long documents. Prior work adopts\nquick workarounds such as document truncation\nor splitting-and-pooling to retroﬁt the document\nranking task to pretrained transformers. Whilst\nthere have been successes with careful architecture\nFigure 1: An example illustration of the attention mech-\nanism used in Query-Directed Sparse Transformer.\ndesign, those bandit-solutions inevitably introduce\ninformation loss and create complicated system\npipelines.\nIntuitively, effective document ranking does not\nrequire fully connected self-attention between all\nquery and document terms. The relevance match-\ning between queries and documents often takes\nplace at text segments as opposed to individual\ntokens (Callan, 1994; Jiang et al., 2019), suggest-\ning that a document term may not need informa-\ntion thousands of words away (Metzler and Croft,\n2005; Child et al., 2019), and that not all document\nterms are useful to calculate the relevance to the\nquery (Xiong et al., 2017). The fully connected at-\ntention matrix includes many unlikely connections\nthat create efﬁciency debt in computing, inference\ntime, parameter size, and training convergence.\nThis paper presents Query-Directed Sparse\nTransformer (QDS-Transformer) for long docu-\nment ranking. In contrast to retroﬁtted solutions,\nQDS-Transformer fundamentally considers the de-\nsirable properties for assessing relevance by focus-\ning on attention paths that matter. Using sparse\nlocal attention (Child et al., 2019), our model re-\nmoves unnecessary connections between distant\n4595\ndocument tokens. Using global attention upon\nsentence boundaries, our model further incorpo-\nrates the hierarchical structures within documents.\nLast but not the least, we use global attention\non all query terms that direct the focus to the\nrelevance matches between query-document term\npairs. These three attention patterns in our Query-\nDirected Sparse attention, as illustrated in Figure 1,\npermit global dissemination of IR-axiomatic infor-\nmation while keeping computation compact and\nessential.\nIn our experiments with TREC Deep Learning\nTrack (Craswell et al., 2020) and three more few-\nshot document ranking benchmarks (Zhang et al.,\n2020), QDS-Transformer consistently improves\nthe standard retroﬁtting BERT ranking baselines\n(e.g., max-pooling on paragraphs) by 5% NDCG.\nIt also shows gains over more recent transformer\narchitectures that induces various sparse structures,\nincluding Sparse Transformer, Longformer, and\nTransformer-XH, as they were not designed to\nincorporate the essential information required in\ndocument ranking. In the meantime, we also\nthoroughly quantify the efﬁciency improvement\nfrom our query-directed sparsity, showing that with\nTVM support (Chen et al., 2018), different sparse\nattention patterns lead to variant training and infer-\nence speed up, and in general QDS-Transformer\nenjoys 200%+ speed up compared to vanilla BERT\non long documents.\nOur visualization also shows interesting learned\nattention patterns in QDS-Transformer. Similar to\nthe observation on BERT in NLP pipeline (Tenney\net al., 2019), in lower QDS-Transformer levels, the\nattention focuses more on learning the local inter-\nactions and document hierarchies, while in higher\nlayers the model focuses more on relevance match-\ning with the query terms. We also show examples\nthat QDS attention may center on the sole sentence\nthat directly answers the query, or may span across\nseveral sentences that cover different aspects of the\nquery, depending on the scope of the intent; this\nbrings the advantage of better interpretability based\non sparse attention.\n2 Related Work\nNeural models have demonstrated signiﬁcant ad-\nvances across various ranking tasks (Guo et al.,\n2019). Early approaches investigated diverse ways\nto capture relevance between queries and docu-\nments (Guo et al., 2016; Xiong et al., 2017; Dai\net al., 2018; Hui et al., 2017). And recently the\nstate-of-the-art in many text ranking tasks has been\ntaken by BERT or other pretrained language mod-\nels (Devlin et al., 2019; Nogueira et al., 2019;\nNogueira and Cho, 2019; Dai and Callan, 2019;\nYang et al., 2019; Craswell et al., 2020), when suf-\nﬁcient relevance labels are available for ﬁne-tuning\n(e.g., on MS MARCO (Bajaj et al., 2016)).\nThe improved effectiveness comes with the cost\nof computing efﬁciency with deep pretrained trans-\nformers, especially on long documents. This stim-\nulates studies investigating ways to retroﬁt long\ndocuments to BERT’s maximum sequence length\nlimits (512). A vanilla strategy is to truncate or\nsplit the documents: Dai and Callan (2019) applied\nBERT ranking on each passage segmented from\nthe document independently and explored differ-\nent ways to combine the passage ranking scores,\nusing the score of the ﬁrst passage (BERT-FirstP),\nthe best passage (BERT-MaxP) (also studied in\nYan et al. (2020)), or the sum of all passage scores\n(BERT-SumP).\nMore sophisticated approaches have also been\ndeveloped to introduce structures to transformer\nattentions. Transformer-XL employs recurrence\non a sequence of text pieces (Dai et al., 2019),\nTransformer-XH (Zhao et al., 2020) models a\ngroup of text sequences by linking them with eX-\ntra Hop attention paths, and Transformer Kernel\nLong (TKL) (Hofst ¨atter et al., 2020) uses a slid-\ning window over the document terms and matches\nthem with the query terms using matching ker-\nnels (Xiong et al., 2017).\nOn the efﬁciency front, Kitaev et al. (2020) pro-\nposed Reformer that employed locality-sensitive\nhashing and reversible residual layers to improve\nthe efﬁciency of Transformers. Child et al.\n(2019) introduced sparse transformers to reduce\nthe quadratic complexity to O(L\n√\nL) by applying\nsparse factorizations to the attention matrix, mak-\ning the use of self-attention possible for extremely\nlong sequences. Subsequent work (Sukhbaatar\net al., 2019; Correia et al., 2019) leverage a similar\nidea in a more adaptive way. Combining local win-\ndowed attention with a task motivated global atten-\ntion, Beltagy et al. (2020) presented Longformer\nwith an attention mechanism that scales linearly\nwith sequence length.\n3 Preliminaries on Document Ranking\nGiven a query qand a set of candidate documents\nD = {d}, the document ranking task is to pro-\nduce the ranking score f(q,d) for each candidate\n4596\nGlobal Attention\nLength- w Attention WindowQuery Attention\nLocal Attention\nLength- w Attention Window\nToken Embedding\nLayer\n1-st Transformer\nLayer\nL -th Transformer\nLayer\n( L \u0000 2)-th Transformer\nLayer\n( L \u0000 1)-th Transformer\nLayer\nLength- w Attention Window\n[SOS] [SOS] [SOS]\nRanking Score f ( q, d )\n[CLS] t\nq\n1 t\nq\n| q | t\nd\n1 t\nd\n| d |\n··· ··· ··· ···\nt\nd\nj\n······\nt\nd\nj +1t\nd\nj \u0000 1t\nd\ni \u0000 1 t\nd\ni +1t\nd\ni\n··· ··· ··· ··· ······\n··· ··· ··· ··· ··· ···\nQuery Tokens Document Tokens\n··· ··· ··· ·········\n··· ··· ··· ··· ······\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n··· ······\n...\nFully-connected Layer\nFigure 2: The overall schema of our proposed QDS-Transformer.\ndocument based on their relevance to the query.\nBERT Ranker. The standard way to leverage pre-\ntrained BERT in document ranking is to concate-\nnate the query and the document into one text se-\nquence, feed it into BERT layers, and then use\na linear layer on top of the last layer’s [CLS] to-\nken (Nogueira and Cho, 2019):\nf(q,d) =Linear(BERT([CLS] ◦q◦[SEP] ◦d)).\nThis BERT ranker can be ﬁne-tuned using rele-\nvance labels on (q,d) pairs, as simple as a classiﬁ-\ncation task, and has achieved strong performances\nin various text ranking benchmarks (Bajaj et al.,\n2016; Craswell et al., 2020).\nTransformer Layer . More speciﬁcally, let\n{t0,t1,...,t i,...,t n}be the tokens in the concate-\nnated q-dsequence, with query tokens t1:|q| ∈q\nand document tokens t|q|+1:n ∈|d|, considering\nspecial tokens being part of qor d. The l-th trans-\nformer layer in BERT takes the hidden represen-\ntations of previous layer (Hl−1), which is embed-\nding for l = 1, and produces a new Hl as fol-\nlows (Vaswani et al., 2017).\nHl = WF ( ˆHl), (1)\nˆHl = A·M ·VT , (2)\nA= 1, (3)\nM = softmax(Q·KT\n√dk\n), (4)\n(QT ; KT ; VT ) = (Wq; Wk; Wv) ·Hl−1. (5)\nIt ﬁrst passes the previous representations through\nthe self-attention mechanism, using three projec-\ntions (Eqn. 5), and then calculates the attention\nmatrix between all token pairs using their query-\nkey similarity (Eqn. 4, as in single-head formation).\nThe attention matrixMthen is used to fuse all other\ntokens’ representationV, to obtain the updated rep-\nresentation for each position (Eqn. 2). In the end,\nanother feed-foreword layer is used to obtain the\nﬁnal representation of this layer Hl (Eqn. 1).\nThe matrix Ais the n2 “adjencency” matrix in\nwhich each entry is one if there is an attention path\nbetween corresponding positions: Aij = 1means\nti queries the value of tj using the key of tj. In\nstandard transformer and BERT, the attention paths\nare fully connected thus A= 1.\nComputation Complexity. In each of the BERT\nlayers, all the feed-forward operations (Eqn. 1 and\n5) are applied to each individual token, leading\nto linear complexity w.r.t. text length n and the\nsquare of the hidden dimension size dim. The self-\nattention operation in Eqn. 2 and 4 calculates the\nattention strengths upon all token pairs, leading to\nsquared complexity w.r.t text length but linear of\nthe hidden dimension size.\nThe complexity of one transformer layer in\nBERT thus includes two components:\nO(dim2n)  \nFeedforward\n+ O(n2dim)  \nSelf-Attention\n. (6)\nThe hidden dimension size (dim) is 768 in BERT\nBase and 1024 in BERT Large (Devlin et al., 2019).\nWhen the text sequence is longer than 1000 or 2000\ntokens, which is often the case in document rank-\ning (Craswell et al., 2020), the self-attention part\nbecomes the main bottleneck in both computation\nand GPU memory. This leads to various retroﬁtted\nsolutions that adapted the document ranking tasks\nto standard BERT which takes at most 512 tokens\nper sequence (Dai and Callan, 2019; Yang et al.,\n2019; Yan et al., 2020; Nogueira et al., 2019).\n4597\n4 QDS-Transformer\nRecent research has shown that with sufﬁcient train-\ning and fully-connected self-attention, BERT learns\nattention patterns that capture meaningful struc-\ntures in language (Clark et al., 2019) or for speciﬁc\ntasks (Zhao et al., 2020). However, this is not yet\nthe case in long document ranking as computing\nbecomes the bottleneck.\nThis section ﬁrst presents how we overcome this\nbottleneck by injecting IR-speciﬁc inductive bias\nas sparse attention patterns. Then we discuss the\nefﬁcient implementation of sparse attention.\n4.1 Query-Directed Sparse Attention\nMathematically, inducing sparsity in self-attention\nis to modify the attention adjacency matrix Aby\nonly keeping connections that are meaningful for\nthe task. For document retrieval, we include two\ngroups of informative connections as sparse adja-\ncency matrices: local attentionand query-directed\nglobal attention.\n4.1.1 Local Attention\nIntuitively, it is unlikely that a token needs to see an-\nother token thousands of positions away to learn its\ncontextual representation, especially in the lower\ntransformer layers which are more about syntactic\nand less about long-range dependencies (Tenney\net al., 2019). We follow this intuition used in the\nSparse Transformer (Child et al., 2019) and deﬁne\nthe following local attention paths:\nAlocal[i,j] = 1,iff |i−j|≤ w/2. (7)\nIt only allows a token to see another token in each\ntransformer layer if the two are w/2 position away,\nwith wthe window size. The local attention serves\nas the backbone for many sparse transformer vari-\nations as it provides the basic local contextual in-\nformation (Correia et al., 2019; Sukhbaatar et al.,\n2019; Beltagy et al., 2020).\n4.1.2 Query-Directed Global Attention\nThe local attention itself does not fully capture the\nrelevance matches between the query and docu-\nments. We introduce several query-directed atten-\ntion patterns to incorporate inductive biases widely\nused in document representation and ranking.\nHierarchical Document Structures. A common\nintuition in document representation is to leverage\nthe hierarchical structures within documents, for\nexample, words, sentences, paragraphs, and sec-\ntions, and compose them into hierarchical attention\nnetworks (Yang et al., 2016). We use a two-level\nword-sentence-document hierarchy and inject this\nhierarchical structure by adding fully connected\nattention paths to all the sentences.\nSpeciﬁcally, we ﬁrst prepend a special token\n[SOS] (start-of-sentence) to each sentence in the\ndocument, and form the following attention con-\nnections:\nAsent[i,j] = 1, iff tj = [SOS]. (8)\nMatching with the Query. For retrieval tasks, ar-\nguably the most important principle is to capture\nthe semantic matching between queries and docu-\nments. Inducing this information is as simple as\nadding dedicated attention paths on query terms:\nAquery[i,j] = 1, iff ti ∈q. (9)\nIt allows each token to see all query terms so as to\nlearn query-dependent representations.\n4.2 Summary\nThe three attention patterns together form the\nquery-directed attention in QDS-Transformer:\nAQDS = Alocal ∪Asent ∪Aquery ∪A[CLS]. (10)\nWe also add the global attention between all other\ntokens and [CLS]. Keeping everything else stan-\ndard in BERT and using this query-directed sparse\nattention (AQDS) in place of the fully-connected\nself-attention (A), we obtain our QDS-Transformer\narchitecture as illustrated in Figure 2.\nInterestingly, QDS-Transformer also resembles\nvarious effective IR-Axioms developed in past\ndecades. For example, in QDS attention, a query\nterm mainly focuses on the [SOS] token through\nASent, while the [SOS] token recaps the proxim-\nity (Callan, 1994) matches locally around it through\nALocal. The local attention in the query part also\nresembles the effective phrase matches (Metzler\nand Croft, 2005) as the query term representations\nare contextualized using other query terms through\nALocal.\n4.3 Efﬁcient Sparsity Implementation\nOur query-directed sparse attention reduces the\nself-attention complexity from O(n2dim) to O(n·\ndim ·(w+ |q|+ |s|)), where the local window size\nw and query length |q|are constant to document\nlength, and the number of sentences is orders of\nmagnitude smaller.\nHowever, to implement this sparsity efﬁciently\non GPU is not that straightforward. Naively using\n4598\nAd-hoc Few-shot (avg. over 5 folds)\nTREC19 DLRB04 CW09-B CW12-B13\nTrain queries 367,013 150 120 60\nTrain qrels 384,597186,846 28,278 17,343\nDev queries 5,193 50 40 20\nDev qrels 519,300 62,282 9,426 5,781\nTest queries 43 50 40 20\nTest qrels 16,258 62,282 9,426 5,781\nTable 1: The statistics of the experimental datasets.\nfor-loops or masking the adjacency matrix Amay\nresult in even worse efﬁciency than the full self-\nattention in common deep learning frameworks.\nAn efﬁcient implementation of sparse operations\noften requires customized CUDA kernels, which\nare inconvenient and require expertise in low-level\nGPU operations (Child et al., 2019). Inspired by\nLongformer (Beltagy et al., 2020), we address this\nissue by implementing QDS-Transformer with Ten-\nsor Virtual Machine (TVM) (Chen et al., 2018).\nPrecisely, we implement custom CUDA kernels\nusing TVM to dynamically compile our attention\nmap AQDS into efﬁciency-optimized CUDA codes.\n5 Experimental Methodologies\nThis section discusses our experimental settings.\nTREC 2019 Deep Learning Track Benchmark .\nWe evaluate QDS-Transformer based on the doc-\nument ranking task from this recent TREC bench-\nmark (Craswell et al., 2020), speciﬁcally using\nthe reranking subtask to rerank top-100 BM25 re-\ntrieved documents. The ofﬁcial evaluation metric\nis NDCG@10 on the test set. We also report MAP\non test and MRR@10 on the development set.\nFew-shot Document Ranking Benchmarks .\nWe then evaluate the generalization ability\nof QDS-Transformer in the few-shot set-\nting (Zhang et al., 2020) using TREC datasets\nRobust04 (RB04), ClueWeb09-B (CW09), and\nClueWeb12-B13 (CW12), in which labels are\nmuch fewer than DL track. Our experimental\nsettings are consistent with prior work (Zhang\net al., 2020) in using the“MS MARCO Human\nLabels”. Speciﬁcally, neural rankers trained with\nMARCO labels are used as feature extractors to\nenrich TREC documents, which are then tested\nwith ﬁve-fold cross-validation (Dai et al., 2018).\nTable 1 summarizes the statistics of four datasets.\nWe describe more details about datasets and exper-\nimental settings in Appendix A.1.\nBaselines. Our baselines include multiple neural\nIR models and the best ofﬁcial TREC runs of single\nmodels. The main baselines cover:\n• Relying on BERT models, RoBERTa (FirstP)\nonly considers the ﬁrst paragraph, while\nRoBERTa (MaxP) encodes short paragraphs with\nBERT and combines them with a max-pooling\nlayer (Dai and Callan, 2019).\n• Transformer-XH (Zhao et al., 2020) retroﬁts data\npipelines to create independent sentences which\nare fed into BERT models, and aggregates them\nwith an extra-hop attention layer.\n• TK (Hofst¨atter et al., 2020) and TKL (Hofst¨atter\net al., 2020) apply BERT-based kernels to esti-\nmate the relevance over document tokens with\nfull attention.\n• Sparse-Transformer (Child et al., 2019) applies\nlength-wsparse local attention windows without\nconsidering query tokens.\n• Longformer also uses sparse local attention and\nadds global attention by prepending one special\ntoken respectively to the query and document,\nsame as in their (Beltagy et al., 2020) QA setup.\nFor ad-hoc retrieval, we also consider CO-\nPACRR (Hui et al., 2018) which employs CNNs\nwithout using pretrained NLM (non-PLM). Note\nthat IDST (Yan et al., 2020) is not comparable\nbecause it exploits external generators for docu-\nment expansion. For the few-shot learning task, we\nadditionally compare with SDM, RankSVM, Coor-\nAscent, and Conv-KNRM as reported in previous\nstudies (Xiong et al., 2017; Dai et al., 2018). More\ndetails of the baselines can be found in Appendix B.\nImplementation Details. We implement all meth-\nods with PyTorch (Paszke et al., 2019) and the Hug-\nging Face transformer library (Wolf et al., 2019),\nexcluding the baselines that have previously re-\nported their scores. For sparse attention, we imple-\nment it using TVM with a custom CUDA kernel in\nPyTorch (Chen et al., 2018). Models are optimized\nby the Adam optimizer (Kingma and Ba, 2014)\nwith a learning rate 10−5, (β1,β2) = (0.9,0.999),\nand a dropout rate 0.1. The dev set is used for\nhyperparameter tuning to decide the best model,\nwhich is then applied to the test set. We set the\nmaximum length of input sequences as 2,048. The\ndimension of the dense layer Fdense(·) in relevance\nestimation is 768, while the local attention window\nsize wis 128. All experiments are conducted on an\nNvidia DGX-1 server with 512 GB memory and\n8 Tesla V100 GPUs. Each method is limited to\naccess only one GPU for fair comparisons.\n4599\nTREC Deep Learning Track Document Ranking\nMethod Test Set Dev Set\nNDCG@10 MAPMRR@10\nBM25 0.488 0.234 0.252\nTREC Best Models\nBM25 (bm25tunedprf) 0.528 0.386 0.318\nTrad (srchvrsrun1) 0.561 0.349 0.306\nNon-PLM (TUW19-d3-re)0.644 0.271 0.401\nBERT (bm25expmarcomb) 0.646 0.424 0.352\nBaseline Models\nCO-PACRR 0.550 0.231 0.284\nTK 0.594 0.252 0.312\nTKL 0.644 0.277 0.329\nRoBERTa (FirstP) 0.588 0.233 0.278\nRoBERTa (MaxP) 0.630 0.246 0.320\nSparse Attention based Models\nSparse-Transformer 0.634 0.257 0.328\nLongformer-QA 0.627 0.255 0.326\nTransformer-XH 0.646 0.256 0.347\nQDS-Transformer 0.667 0.278 0.360\nTable 2: The ad-hoc retrieval performance of our ap-\nproach and baseline methods on the TREC-19 DL track\nbenchmark. Note that those baselines with higher MAP\nscores are all full retrieval and beneﬁted from addi-\ntional data engineering like query expansion.\n6 Evaluation Results\nThis section evaluates QDS-Transformer in its ef-\nfectiveness, attention patterns, and efﬁciency. We\nalso analyze the learned query-directed attention\nweights and show case studies.\n6.1 Retrieval Effectiveness\nTable 2 summarizes the retrieval effectiveness on\nthe TREC-19 DL benchmark. Table 3 shows the\nfew-shot performance on the three TREC datasets.\nQDS-Transformer consistently outperforms\nbaseline methods on all datasets in both experimen-\ntal settings. Note that the higher MAP scores from\nsome methods in TREC-19 DL is because they\nhave better ﬁrst stage retrieval and are not using the\nsame reranking setting. QDS-Transformer outper-\nforms the best BERT-based TREC run by 3.25%\nin NDCG@10 and is more effective than the con-\ncurrent sliding window approach, TKL. Moreover,\nQDS-Transformer outperforms RoBERTa (MaxP),\nwhich is the standard retroﬁtted method for BERT,\nby 6% in NDGG@10 while also being a uniﬁed\nframework.\nCompared with Sparse Transformers and\nLongformer-QA, QDS-Transformer provides more\nthan 5% improvement in nearly all datasets. The\nbest baseline is Transformer-XH, which creates\nstructural sparsity by breaking a document into\nsegments and introduces effective eXtra-hop at-\ntentions to jointly model the relevance of those\nMethod RB04 CW09 CW12NDCG ERR NDCG ERR NDCG ERR\nClassical IR; Cross ValidatedSDM 0.427 0.117 0.277 0.138 0.108 0.091RankSVM 0.420 n.a. 0.289 n.a. 0.121 0.092Coor-Ascent 0.427 n.a. 0.295 n.a. 0.121 0.095\nNeural IR; Trained on MS MARCO and then Cross Validated.Conv-KNRM 0.427 0.117 0.287 0.160 0.112 0.092RoBERTa (FirstP) 0.437 0.110 0.262 0.161 0.111 0.086RoBERTa (MaxP) 0.439 0.114 0.264 0.162 0.092 0.074Sparse-Transformer 0.449 0.119 0.274 0.173 0.119 0.094Longformer-QA 0.448 0.113 0.276 0.179 0.111 0.085Transformer-XH 0.450 0.123 0.283 0.179 0.107 0.080\nQDS-Transformer0.457 0.126 0.308 0.191 0.131 0.112\nTable 3: The few-shot learning retrieval performance of\ndifferent methods on three benchmark datasets. NDCG\nand ERR are at cut-off 20.\nMethod Attention TREC-19 DL Track\nQ Sent NDCG@10 MAP\nRoBERTa (MaxP) \u0013 \u0017 0.630 0.246\nSparse Transformer \u0017 \u0017 0.634 0.257\nLongFormer-QA \u0017 \u0017 0.627 0.255\nTransformer-XH \u0013 \u0013 0.646 0.256\nQDS-Transformer (S)\u0017 \u0013 0.633 0.244\nQDS-Transformer (Q)\u0013 \u0017 0.658 0.263\nQDS-Transformer \u0013 \u0013 0.667 0.278\nTable 4: The retrieval performance of different models\non the TREC-19 DL track benchmark dataset with dif-\nferent global attention patterns. Q and S indicate the\nusage of query and sentence global attention. Note that\nQDS-Transformer with no global attention is equiva-\nlent to Sparse-Transformer.\nsegments. While these methods show competitive\neffectiveness especially with our TVM implemen-\ntation, QDS-Transformer is consistently more ac-\ncurate through the query-directed sparse attention\npatterns in all evaluation settings.\n6.2 Effectiveness of Attention Patterns\nThis experiment studies the contribution of our\nquery-directed sparse attention patterns to QDS-\nTransformer’s effectiveness.\nTable 4 shows the ablation results of the three\nattention patterns in TREC-19 DL benchmark: lo-\ncal attention only (Alocal, Sparse Transformer), hi-\nerarchical attention on sentence only (Asent, QDS-\nTransformer (S)), and query-oriented attention only\n(Aquery, QDS-Transformer (Q)). All three sparse\nattention patterns contribute. As expected, query-\noriented attention is most effective to capture the\nrelevance match between query and documents.\nNote that the RoBERTa (MaxP) and Transformer-\nXH also attend to queries, but the attention is more\nlocalized as the document is broke into separated\ntext pieces and the query is concatenated with each\nof them. In comparison, QDS-Transformer mimics\nthe proximity matches and captures the global hier-\n4600\nWindow Size of Local Attention\n32 64 128 256 512 1024\nNDCG@10\n0.6\n0.61\n0.62\n0.63\n0.64\n0.65\n0.66\n0.67\nMAP@10\nNDCG@10\nMAP@10\nFigure 3: The performance of QDS-\nTransformer on TREC-19 DL track\ndataset with different local atten-\ntion window sizes w.\nLayer\n1 2 3 4 5 6 7 8 9 10 11 12\nAvg. Max Attention Weight\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n[CLS] tokens\n[SOS] tokens\nQuery tokens\nFigure 4: The average maximum at-\ntention scores to different types of\ntokens over Transformer layers.\nLayer\n1 2 3 4 5 6 7 8 9 10 11 12\nAvg. Attention Entropy\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n[CLS] tokens\n[SOS] tokens\nQuery tokens\nFigure 5: The average entropy\nscores of attention distributions for\ndifferent token types over Trans-\nformer layers.\nMethod Length Sparsity ms per q-d\nTrain Infer\nRoBERTa 1024 100% 391 100\nRoBERTa 2048 100% 799 205\nRoBERTa (FirstP) 512 100% 138 17\nRoBERTa (MaxP) 4*512 25% 305 55\nTransformer-XH 4*512 25% 309 54\nQDS-Transformer (128)512 30.84% 218 45\nQDS-Transformer (128)1024 18.72% 249 52\nQDS-Transformer (128)2048 8.97% 321 92\nLongformer-QA (128) 2048 4.70% 166 45\nSparse-Transformer (128)2048 4.56% 154 40\nQDS-Transformer (32)2048 6.70% 201 50\nQDS-Transformer (64)2048 8.97% 309 86\nQDS-Transformer (128)2048 13.53% 321 92\nQDS-Transformer (256)2048 22.64% 475 127\nQDS-Transformer (512)2048 40.88% 512 160\nQDS-Transformer (1024)2048 77.34% 629 195\nQDS-Transformer (Q) 2048 5.10% 316 108\nQDS-Transformer (S) 2048 8.57% 322 105\nWithout TVM Implementation\nSparse-Transformer (128)2048 4.56% 251 62\nQDS-Transformer (128)2048 13.53% 390 103\nTable 5: Efﬁciency Quantiﬁcation. The local attention\nwindow size is shown in parentheses. Q and S indicate\nthe usage of only query and sentence attention. Sparsity\nis compared with fully attention at same text length.\narchical structures in the document using dedicated\nattention from query terms to sentences.\nFigure 3 depicts the change in retrieval effec-\ntiveness by varying the local attention window\nsize. Both NDCG@10 and MAP@10 grow at a\nsteady pace starting from a window size of 32 and\npeak at 128, but no additional gain is observed\nwith bigger window sizes. The information from\na term 512 tokens away does not provide many\nsignals in relevance matching and is safely pruned\nin QDS-Transformer. Note that the dip at attention\nsize 1024 is because our model is initialized from\nRoBERTa which is only pretrained on 512 tokens.\n6.3 Model Efﬁciency\nThis experiment benchmarks the efﬁciency of dif-\nferent sparse attention patterns. Their training and\ninference time (ms per query-document pair, or\nMSpP) is shown in Table 5.\nRoBERTa on 2048 tokens is prohibitive; we only\nmeasured its time with random parameters as we\nwere not able to actually train it. Retroﬁtting was a\nnatural choice to leverage pretrained models.\nSparsity helps. Sparse-Transformer (128) is\nmuch faster than MaxP. Interestingly, its attention\nmatrix with only 4.56% non-zero entries leads to\non par efﬁciency with retroﬁtted solutions and also\nonly 5 times faster compared to full attention; this\nis due to the required cost involved in feed-forward.\nThis effect is also reﬂected in the efﬁciency of QDS-\nTransformer with different local window sizes.\nDifferent sparsity patterns dramatically inﬂuence\nthe optimization of TVM. Intuitively, patterns with\nmore regular shape would be easier to optimize\nthan more customized connections in TVM. For ex-\nample, the skipping patterns along sentence bound-\nary in QDS-Transformer (S) seems more forgiving\nthan the query-oriented attentions (Q). Compar-\ning efﬁciency with and without our TVM imple-\nmentation, the diagonal sparse shape in Sparse-\nTransformer is much better optimized.\nHow to better utilize the advantage from sparsity\nand structural inductive biases is perhaps a nec-\nessary future research direction in an era where\nmodels with fewer than one billion parameters are\nno longer considered large (Brown et al., 2020).\nMaking progress in this direction may need more\nclose collaborations between experts in application,\nmodeling, and infrastructure.\n4601\nQ1: 1037798 (who is robert gray) Q2: 1110199 (what is wiﬁ vs bluetooth)\ndocid: D3533931 docid: D1325409\nHeads 1,2,4,6,9,10,11,12: Head 01: Bluetooth’s low power consumption make it useful where power is limited.\nRobert Gray (title) Head 02: Wi-Fi appliances are often plugged into wall outlets to operate.\nHeads 3,5,7,8:\nRobert Gray, (born May 10, 1755,\nTiverton, R.I. died summer 1806, at\nsea near eastern U.S. coast), captain of\nthe ﬁrst U.S. ship to circumnavigate\nthe globe and explorer of the\nColumbia River.\nHead 07: The extremely low power requirements of the latest Bluetooth 4.0 standard allows wire-\nless connectivity to be added to devices powered only by watch batteries.\nHead 09: A Wi-Fi enabled network relies on a hub.\nHead 10: The advantages of using bluetooth from existing technology.\nHead 11: Wi-Fi is more suited to data-intensive activities such as streaming high-deﬁnition\nmovies, while Bluetooth is better suited to tasks such as transferring keyboard strokes to a computer.\nHead 12: The greater power of Wi-Fi network also means it can move data more quickly than\nBluetooth network.\nTable 6: Case study of two queries on the sentences with the highest attention weights in the last transformer layer\nover different heads for the [CLS] token.\nQ3: 1112341 (what is the daily life of thai people)\nQuery Token Sentence with the highest attention weight in the document D1641978\nlife Children are expected to show great respect for their parents, and they maintain close ties, even well into adulthood .\nthai Culture of Thailand (title)\nTable 7: Case study of the query 1112341 on the sentences in the document D1641978 with the highest attention\nweights among all heads from two query tokens. Note that we use attention weights in the third transformer layer.\n6.4 Learned Attention Weights\nThis experiment analyzes the learned attention\nweights in QDS-Transformer, using the approach\ndeveloped by Clark et al. (2019).\nFigure 4 illustrates the average maximum atten-\ntion weights of the three attention patterns used in\nour model. Interestingly, the model tends to implic-\nitly conduct hierarchical attention learning (Yang\net al., 2016), where lower layers focus on learning\nstructures and pay more attention to [SOS] tokens,\nwhile higher layers emphasize the relevance by at-\ntending to queries more. Attention on both types of\ntokens is consistently stronger than on the [CLS]\ntoken. The model is capturing the inductive biases\nemphasized by our sparse attention structures.\nFigure 5 shows the average entropy of the atten-\ntion weight distribution. Intuitively, lower layer\nattention tends to have high entropy and thus a\nvery broad view over many words, to create con-\ntextualized representations. The entropy of query\nand [SOS] are in general lower, as they focus on\ncapturing information needs and document struc-\ntures. The entropy of all three types of tokens rises\nagain in the last layer, implying that they may try\nto aggregate representation for the whole input.\n6.5 Case Study on Learned Attention Weights\nTable 6 shows a case study of sentences with the\nhighest attention weight from [CLS] in the last\nlayer for two example queries. For factoid query\nQ1, all heads center on precise sentences that can\ndirectly answer the query. For Q2 that is on the\nexploratory side, different attention heads exhibit\ndiverse patterns focusing on partial evidence that\ncan provide a broader understanding collectively.\nTable 7 depicts the other case study on learned\nattention weights of sentences from query tokens.\nWe adopt the third transformer layer, where sen-\ntences obtain more attention as shown in Figure 4,\nto emphasize signiﬁcant sentences for query to-\nkens. The results show query-directed attention can\ncapture sentences with different topics matched to\nindividual query tokens, thereby comprehending\nsophisticated document structure.\nThese ﬁndings suggest that QDS-Transformer\nhas an interesting potential to be applied to not\nonly retrieval but also the question-answering task\nin NLP, providing a generic and effective frame-\nwork, while also being interpretable with its the\nsparse structural attention connectivity. We further\nprovide an additional case study in Appendix C.\n7 Conclusions\nQDS-Transformer improves the efﬁciency and ef-\nfectiveness of pretrained transformers in long docu-\nment ranking using sparse attention structures. The\nsparsity is designed to capture the principal prop-\nerties (IR-Axioms) that are crucial for relevance\nmodeling: local contextualization, document struc-\ntures, and query-focused matching. In four TREC\ndocument ranking tasks with variant settings, QDS-\nTransformer consistently outperforms competitive\nbaselines that retroﬁt to BERT or use sparse atten-\ntion not designed for document ranking.\nOur experiments demonstrate the promising fu-\nture of joint optimization of structural domain\nknowledge and efﬁciency from sparsity, while its\ncurrent form is somewhat at the infancy stage. Our\n4602\nanalyses also indicate the potential of better inter-\npretability from sparse structures and more uniﬁed\nmodels for IR and QA.\nReferences\nZeynep Akkalyoncu Yilmaz, Shengjin Wang, and\nJimmy Lin. 2019. H2oloo at trec 2019: Combining\nsentence and document evidence in the deep learn-\ning track. In Proceedings of the Twenty-Eighth Text\nREtrieval Conference (TREC 2019).\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. Ms marco: A human generated machine\nreading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nJames P Callan. 1994. Passage-level evidence in\ndocument retrieval. In SIGIR’94, pages 302–310.\nSpringer.\nTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin\nZheng, Eddie Yan, Haichen Shen, Meghan Cowan,\nLeyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018.\n{TVM}: An automated end-to-end optimizing com-\npiler for deep learning. In 13th {USENIX}Sympo-\nsium on Operating Systems Design and Implementa-\ntion ({OSDI}18), pages 578–594.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. URL\nhttps://openai.com/blog/sparse-transformers.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does bert look\nat? an analysis of bert’s attention.\nGonc ¸alo M. Correia, Vlad Niculae, and Andr ´e F. T.\nMartins. 2019. Adaptively sparse transformers. In\nEMNLP.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\nCampos, and Ellen M V oorhees. 2020. Overview\nof the trec 2019 deep learning track. arXiv preprint\narXiv:2003.07820.\nZhuyun Dai and Jamie Callan. 2019. Deeper text un-\nderstanding for ir with contextual neural language\nmodeling. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, pages 985–988.\nZhuyun Dai, Chenyan Xiong, Jamie Callan, and\nZhiyuan Liu. 2018. Convolutional neural networks\nfor soft-matching n-grams in ad-hoc search. In Pro-\nceedings of the Eleventh ACM International Con-\nference on Web Search and Data Mining, page\n126–134.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nJiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce\nCroft. 2016. A deep relevance matching model\nfor ad-hoc retrieval. In Proceedings of the 25th\nACM International on Conference on Information\nand Knowledge Management. ACM.\nJiafeng Guo, Yixing Fan, Liang Pang, Liu Yang,\nQingyao Ai, Hamed Zamani, Chen Wu, W. Bruce\nCroft, and Xueqi Cheng. 2019. A deep look into\nneural ranking models for information retrieval.\nSebastian Hofst¨atter, Markus Zlabinger, and Allan Han-\nbury. 2019. Tu wien@ trec deep learning’19–simple\ncontextualization for re-ranking. arXiv preprint\narXiv:1912.01385.\nSebastian Hofst ¨atter, Markus Zlabinger, and Allan\nHanbury. 2020. Interpretable & time-budget-\nconstrained contextualization for re-ranking. arXiv\npreprint arXiv:2002.01854.\nSebastian Hofst ¨atter, Hamed Zamani, Bhaskar Mitra,\nNick Craswell, and Allan Hanbury. 2020. Local\nself-attention over long text for efﬁcient document\nretrieval. In SIGIR.\nKai Hui, Andrew Yates, Klaus Berberich, and Gerard\nDe Melo. 2018. Co-pacrr: A context-aware neu-\nral ir model for ad-hoc retrieval. In Proceedings of\nthe eleventh ACM international conference on web\nsearch and data mining, pages 279–287.\nKai Hui, Andrew Yates, Klaus Berberich, and Gerard\nde Melo. 2017. Pacrr: A position-aware neural ir\nmodel for relevance matching. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1049–1058.\n4603\nJyun-Yu Jiang, Mingyang Zhang, Cheng Li, Michael\nBendersky, Nadav Golbandi, and Marc Najork.\n2019. Semantic text matching for long-form docu-\nments. In The World Wide Web Conference, pages\n795–806.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. arXiv\npreprint arXiv:2001.04451.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nDonald Metzler and W Bruce Croft. 2005. A markov\nrandom ﬁeld model for term dependencies. In Pro-\nceedings of the 28th annual international ACM SI-\nGIR conference on Research and development in in-\nformation retrieval, pages 472–479.\nDonald Metzler and W Bruce Croft. 2007. Linear\nfeature-based models for information retrieval. In-\nformation Retrieval, 10(3):257–274.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Pas-\nsage re-ranking with bert. arXiv preprint\narXiv:1901.04085.\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\nJimmy Lin. 2019. Multi-stage document ranking\nwith bert. arXiv preprint arXiv:1910.14424.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems,\npages 8024–8035.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In ACL.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. arXiv\npreprint arXiv:1905.05950.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, et al. 2019. Transformers: State-of-the-\nart natural language processing. arXiv preprint\narXiv:1910.03771.\nChenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan\nLiu, and Russell Power. 2017. End-to-end neural\nad-hoc ranking with kernel pooling. In Proceedings\nof the 40th International ACM SIGIR conference on\nresearch and development in information retrieval,\npages 55–64.\nMing Yan, Chenliang Li, Chen Wu, Bin Bi, Wei\nWang, Jiangnan Xia, and Luo Si. 2020. Idst at\ntrec 2019 deep learning track: Deep cascade rank-\ning with generation-based document expansion and\npre-trained language modeling.\nPeilin Yang and Jimmy Lin. 2019. Reproducing and\ngeneralizing semantic term matching in axiomatic\ninformation retrieval. In European Conference on\nInformation Retrieval, pages 369–381. Springer.\nWei Yang, Haotian Zhang, and Jimmy Lin. 2019. Sim-\nple applications of bert for ad hoc document re-\ntrieval. arXiv preprint arXiv:1903.10972.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchi-\ncal attention networks for document classiﬁcation.\nIn Proceedings of the 2016 conference of the North\nAmerican chapter of the association for computa-\ntional linguistics: human language technologies ,\npages 1480–1489.\nKaitao Zhang, Chenyan Xiong, Zhenghao Liu, and\nZhiyuan Liu. 2020. Selective weak supervision for\nneural information retrieval. In Proceedings of The\nWeb Conference 2020, pages 474–485.\nChen Zhao, Chenyan Xiong, Corby Rosset, Xia\nSong, Paul Bennett, and Saurabh Tiwary. 2020.\nTransformer-xh: Multi-evidence reasoning with ex-\ntra hop attention. In International Conference on\nLearning Representations.\n4604\nAppendix\nA Experimental Details\nIn this section, we clarify the details about experi-\nmental datasets and experimental settings.\nA.1 Experimental Datasets\nTREC-19 DL Track Dataset . For ad-hoc re-\ntrieval, we adopt the TREC-19 DL track bench-\nmark as the experimental dataset with training, dev,\nand test sets. Training and dev sets consist of large-\nscale human relevance assessments derived from\nthe MS MARCO collection (Bajaj et al., 2016)\nwith no negative labels and sparse positive labels\nfor each query while relevance judgments in the\ntest sets are annotated by NIST judges.\nFew-shot Document Ranking Benchmarks. For\nfew-shot learning, three retrieval benchmark\ndatasets are utilized in our experiments, includ-\ning Robust04, ClueWeb09-B, and ClueWeb12-B13.\nRobust04 provides 249 queries from TREC Robust\ntrack 2014 with relevance labels. ClueWeb09-B\nincludes of 200 queries with relevance labels from\nTREC Web Track 2009-2012. ClueWeb12-B13\nconsists of 100 queries from TREC Web Track\n2013-2014 with relevance labels.\nNote that Table 1 in the paper summarizes the\nstatistics of four experimental datasets. Datasets of\nall benchmarks are publicly available. The TREC-\n19 DL track provides all dataset on its ofﬁcal web-\nsite1. The queries and relevance assessments of\nthree few-shot document ranking datasets can be\nfound at the TREC website2 while document col-\nlocations are also publicly available on the corre-\nsponding sites345 .\nA.2 Experimental Settings\nAd-hoc Retrieval. Experiments follow the pro-\ntocol of the TREC-19 deep learning track. Each\nmethod is trained with the training set. The model\nparameters can be further ﬁne-tuned with the dev\nset and the MRR@10 metric. The ﬁne-tuned model\nis ﬁnally applied to the test set for evaluation. Fol-\nlowing the ofﬁcial metrics, MRR@10 is used in\ndev set runs as labels are incomplete and shallow,\nwhile the test set is comprehensively evaluated us-\ning NDCG@10 and MAP@10.\n1https://microsoft.github.io/TREC-2019-Deep-Learning/\n2https://trec.nist.gov/\n3RB04: https://trec.nist.gov/data/qa/T8 QAdata/disks4 5.html\n4CW09: http://lemurproject.org/clueweb09/\n5CW12: https://lemurproject.org/clueweb12/\nMethod #Params Method #Params\nRoBERTa (FirstP) 124MRoBERTa (MaxP) 124M\nSparse-Transformer 149MLongformer-QA 149M\nTransformer-XH 128M QDS-Transformer 149M\nTable 8: Number of parameters for methods.\nFew-shot Document Ranking. All experimental\nsettings for few-shot learning are consistent with\nthe“MS MARCO Human Labels” setting in pre-\nvious studies (Zhang et al., 2020). Each method\nﬁrst trains a neural ranker on MARCO training\nlabels, which are identical as in the TREC DL\ntrack. The latent representations of trained models\nare then considered as features for a Coor-Ascent\nranker for low-label datasets using ﬁve-fold cross-\nvalidation (Dai and Callan, 2019; Dai et al., 2018)\nto rerank top-100 SDM retrieved results (Metzler\nand Croft, 2007). Standard metrics NDCG@20\nand ERR@20 are used to compare the different\napproaches. The results are reported by taking the\naverage of each test fold from the total 5 folds,\nwherein the rest 4 folds in each round are used as\ntraining and dev queries.\nHyperparameter Settings and Search. We adopt\nthe pretrained model for sparse attention (Beltagy\net al., 2020) and ﬁx all of the hidden dimension\nnumbers as 768 and the number of transformer\nlayers as 12. BERT-based models use RoBERTa\nas pretrained models (Liu et al., 2019). To hy-\nperparameter tuning, we search the local attention\nwindow size win {32, 64, 128, 256, 512, 1024 }\nwith the dev set and determine w = 128. Models\nare optimized by the Adam optimizer (Kingma and\nBa, 2014) with a learning rate 10−5, (β1,β2) =\n(0.9,0.999), and a dropout rate 0.1. Under the\nhyperparameter settings, the parameter numbers\nof our implemented methods are shown in Ta-\nble 8 summarizing the sizes of parameters based\non model.parameters() in PyTorch.\nA.3 Evaluation Scripts\nAll evaluation measures are computed by the ofﬁ-\ncial scripts. For ad-hoc retreival, we use trec eval6\nas the standard tool in the TREC community for\nevaluating ad-hoc retreival runs. This is also the\nofﬁcial setting of the TREC-19 deep learning track.\nFor few-shot document ranking, we use graded rele-\nvance assessment script (gdeval)7 as the evaluation\nscript measuring NDCG and ERR. Note that this\nsetting is consistent with previous studies (Zhang\net al., 2020; Dai and Callan, 2019).\n6https://github.com/usnistgov/trec eval\n7https://trec.nist.gov/data/web/10/gdeval.pl\n4605\nSentence in the document D2944963 for Q4: 833860 (what is the most popular food in switzerland) Top Query Token\nTop 10 Swiss foods with recipes (title) switzerland\nYou certainly won’t go hungry in Switzerland. food\nYou spear small cubes of bread onto long-stemmed forks and dip them into the hot cheese (taking care not to lose\nthe bread in the fondue).\nfood\nJamie Oliver has this easy cheese fondue recipe, and this ﬁve-star recipe has good reviews. popular\nTable 9: Case study of the query 833860 with the query tokens with the highest attention weights in the 10-th\ntransformer layer among all heads from the [SOS] tokens of sentences in the document D2944963.\nB Baseline Methods\nIn this section, we introduce each baseline method.\nTREC Best Runs.\n• bm25tuned prf (Yang and Lin, 2019) ﬁne-\ntunes the BM25 parameters with pseudo rel-\nevance feedback as the best BM25 based\nmethod in ofﬁcial runs.\n• srchvrs run1 is marked as the best traditional\nranking method among ofﬁcial runs (Craswell\net al., 2020).\n• TUW19-d3-re (Hofst¨atter et al., 2019) as the\nbest method without using non-pretrained lan-\nguage models (non-PLM) in ofﬁcial runs uti-\nlizes a transformer to encode both of the query\nand the document, thereby measuring interac-\ntions between terms and scoring the relevance.\n• bm25 expmarcomb (Akkalyoncu Yilmaz\net al., 2019) combines sentence-level and\ndocument-level relevance scores with a pre-\ntrained BERT model.\nClassical IR Methods.\n• SDM (Metzler and Croft, 2005) as a sequen-\ntial dependence model conducts ranking based\non the theory of probabilistic graphical mod-\nels. We obtain ranking results of SDM from\nprevious studies (Dai and Callan, 2019). SDM\nis not only treated as a baseline method but\nalso providing the candidate documents for\nreranking in the few-shot learning task.\n• Coor-Ascent (Metzler and Croft, 2007) is a\nlinear feature-based model for ranking. It\nis also considered as the trainer in few-shot\nlearning with representations from methods.\nNeural IR Methods.\n• CO-PACRR(Hui et al., 2018) utilizes CNNs\nto model query-document similarity matrices\nand provide a score using a max-pooling layer.\n• Conv-KNRM (Dai et al., 2018) applies\nCNNs to independently encode the query and\nthe document. The encoded representations\nare then integrated by a cross-matching layer,\nthereby deriving relevance scores.\nTransformer-based Methods.\n• TK (Hofst¨atter et al., 2020) and\nTKL (Hofst¨atter et al., 2020) apply\ntransformers to independently model the\nquery and document, thereby measuring term\ninteractions at the embedding level.\n• RoBERTa (FirstP) and RoBERTa\n(MaxP) (Dai and Callan, 2019) adapt\nlong-form documents by considering the ﬁrst\nparagraph and combining RoBERTa outputs\nwith max-pooling over paragraphs. Note that\neach paragraph is also attached with query\ntokens before being fed into the model.\n• Transformer-XH(Zhao et al., 2020) encodes\neach sentence independently and considers\ntheir relations with an extra-hop attention\nlayer. Note each sentence is also attached\nwith query tokens as the model input.\n• Sparse-Transformer(Child et al., 2019) sim-\nply uses sparse local attention to tackle the\nefﬁciency issue of transformers.\n• Longformer-QA (Beltagy et al., 2020) ex-\ntends Sparse-Transformer by attaching two\nglobal attention tokens to the query and the\ndocument as their settings for question answer-\ning. Note that their global attention would not\nconsider document structural information.\nC Additional Study on Attention Weights\nIn addition to attention from the classiﬁcation token\n[CLS] and query tokens as shown in Section 6.5,\nhere we analyze the attention from sentences. Ta-\nble 9 shows the query tokens with the highest atten-\ntion weights in the 10-th transformer layer among\nall head from the [SOS] tokens of sentences. Note\nthat the 10-th transformer layer indicates higher im-\nportance of query tokens as shown in Figre 4. The\nresults show that QDS-Transformer is capable of\ndirecting sentences to the tokens with matched top-\nics, thereby understanding sophisticated document\nstructure with different topics."
}