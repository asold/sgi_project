{
  "title": "TokenCut: Segmenting Objects in Images and Videos With Self-Supervised Transformer and Normalized Cut",
  "url": "https://openalex.org/W4294255647",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2309559497",
      "name": "Yangtao Wang",
      "affiliations": [
        "Université Grenoble Alpes",
        "Institut polytechnique de Grenoble",
        "Centre National de la Recherche Scientifique",
        "Laboratoire d'Informatique de Grenoble"
      ]
    },
    {
      "id": "https://openalex.org/A2102308878",
      "name": "Xi Shen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2041852224",
      "name": "Yuan Yuan",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095608055",
      "name": "Yu-Ming Du",
      "affiliations": [
        "École nationale des ponts et chaussées"
      ]
    },
    {
      "id": "https://openalex.org/A2222193198",
      "name": "Maomao Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2937147546",
      "name": "Shell Xu Hu",
      "affiliations": [
        "Samsung (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2183168386",
      "name": "James L. Crowley",
      "affiliations": [
        "Institut polytechnique de Grenoble",
        "Université Grenoble Alpes",
        "Laboratoire d'Informatique de Grenoble",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2109119983",
      "name": "Dominique Vaufreydaz",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Laboratoire d'Informatique de Grenoble",
        "Université Grenoble Alpes",
        "Institut polytechnique de Grenoble"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2963878474",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2895293811",
    "https://openalex.org/W6810265253",
    "https://openalex.org/W3167938559",
    "https://openalex.org/W6838789689",
    "https://openalex.org/W3203092180",
    "https://openalex.org/W6796634059",
    "https://openalex.org/W6767021658",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2737008123",
    "https://openalex.org/W6789782624",
    "https://openalex.org/W4221167012",
    "https://openalex.org/W2121947440",
    "https://openalex.org/W3204171527",
    "https://openalex.org/W2254462240",
    "https://openalex.org/W2161236525",
    "https://openalex.org/W2964352379",
    "https://openalex.org/W21025885",
    "https://openalex.org/W2047670868",
    "https://openalex.org/W2002781701",
    "https://openalex.org/W1920234547",
    "https://openalex.org/W2161185676",
    "https://openalex.org/W2979654309",
    "https://openalex.org/W1919709169",
    "https://openalex.org/W6796303064",
    "https://openalex.org/W3106670560",
    "https://openalex.org/W4239147634",
    "https://openalex.org/W3171581326",
    "https://openalex.org/W6789505266",
    "https://openalex.org/W3035725370",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2962749812",
    "https://openalex.org/W3035166710",
    "https://openalex.org/W2984303785",
    "https://openalex.org/W1966601141",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W2086052791",
    "https://openalex.org/W2963782415",
    "https://openalex.org/W6796761347",
    "https://openalex.org/W2033959528",
    "https://openalex.org/W2295160225",
    "https://openalex.org/W1963920598",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W2107200795",
    "https://openalex.org/W6799515669",
    "https://openalex.org/W2441099548",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W2963087201",
    "https://openalex.org/W3016163669",
    "https://openalex.org/W2807912089",
    "https://openalex.org/W3034896357",
    "https://openalex.org/W3109908659",
    "https://openalex.org/W1952794764",
    "https://openalex.org/W6803057574",
    "https://openalex.org/W2076756823",
    "https://openalex.org/W3070936185",
    "https://openalex.org/W2470139095",
    "https://openalex.org/W6675869821",
    "https://openalex.org/W4313150877",
    "https://openalex.org/W2138682569",
    "https://openalex.org/W2964028976",
    "https://openalex.org/W2294182682",
    "https://openalex.org/W2147253850",
    "https://openalex.org/W2114030927",
    "https://openalex.org/W2039313011",
    "https://openalex.org/W7746136",
    "https://openalex.org/W2740667773",
    "https://openalex.org/W2088049833",
    "https://openalex.org/W6686583229",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4312593919",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287760472",
    "https://openalex.org/W4288375254",
    "https://openalex.org/W2970642899",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4280490805",
    "https://openalex.org/W3130976481",
    "https://openalex.org/W2105628432",
    "https://openalex.org/W2952793010",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4287126729",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4287120947",
    "https://openalex.org/W4221167396",
    "https://openalex.org/W4297776992",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3209709214",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287210311",
    "https://openalex.org/W4287727117",
    "https://openalex.org/W3214096168",
    "https://openalex.org/W3160153466",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2186094539"
  ],
  "abstract": "In this paper, we describe a graph-based algorithm that uses the features obtained by a self-supervised transformer to detect and segment salient objects in images and videos. With this approach, the image patches that compose an image or video are organised into a fully connected graph, where the edge between each pair of patches is labeled with a similarity score between patches using features learned by the transformer. Detection and segmentation of salient objects is then formulated as a graph-cut problem and solved using the classical Normalized Cut algorithm. Despite the simplicity of this approach, it achieves state-of-the-art results on several common image and video detection and segmentation tasks. For unsupervised object discovery, this approach outperforms the competing approaches by a margin of 6.1%, 5.7%, and 2.6%, respectively, when tested with the VOC07, VOC12, and COCO20K datasets. For the unsupervised saliency detection task in images, this method improves the score for Intersection over Union (IoU) by 4.4%, 5.6% and 5.2%. When tested with the ECSSD, DUTS, and DUT-OMRON datasets, respectively, compared to current state-of-the-art techniques. This method also achieves competitive results for unsupervised video object segmentation tasks with the DAVIS, SegTV2, and FBMS datasets.",
  "full_text": "TOKEN CUT: S EGMENTING OBJECTS IN IMAGES AND VIDEOS\nWITH SELF -SUPERVISED TRANSFORMER AND NORMALIZED\nCUT\nAUTHOR VERSION\nYangtao Wang1∗, Xi Shen2∗, Yuan Yuan3, Yuming Du4, Maomao Li2, Shell Xu Hu5\nJames L. Crowley1, Dominique Vaufreydaz1\n1 Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\n2 Tencent AI Lab 3 MIT CSAIL 4 LIGM (UMR 8049) - Ecole des Ponts, UPE\n5 Samsung AI Center, Cambridge\nAbstract\nIn this paper, we describe a graph-based algorithm that\nuses the features obtained by a self-supervised trans-\nformer to detect and segment salient objects in images\nand videos. With this approach, the image patches that\ncompose an image or video are organised into a fully\nconnected graph, in which the edge between each pair\nof patches is labeled with a similarity score based on\nthe features learned by the transformer. Detection and\nsegmentation of salient objects can then be formulated\nas a graph-cut problem and solved using the classical\nNormalized Cut algorithm. Despite the simplicity of\nthis approach, it achieves state-of-the-art results on sev-\neral common image and video detection and segmenta-\ntion tasks. For unsupervised object discovery, this ap-\nproach outperforms the competing approaches by a mar-\ngin of 6.1%, 5.7%, and 2.6% when tested with the\nVOC07, VOC12, and COCO20K datasets. For the un-\nsupervised saliency detection task in images, this method\nimproves the score for Intersection over Union (IoU) by\n4.4%, 5.6% and 5.2%. When tested with the ECSSD,\nDUTS, and DUT-OMRON datasets. This method also\nachieves competitive results for unsupervised video ob-\nject segmentation tasks with the DA VIS, SegTV2, and\nFBMS datasets. Our implementation is available at\nhttps://www.m-psi.fr/Papers/TokenCut2022/.\n1 Introduction\nDetecting and segmenting salient objects in an image or\nvideo are fundamental problems in computer vision with\napplications in real-world vision systems for robotics,\nautonomous driving, traffic monitoring, manufacturing,\nand embodied artificial intelligence [18, 68, 69]. How-\never, current approaches rely on supervised learning re-\nquiring large data sets of high-quality, annotated training\n*Corresponding Author\n(a) Attention maps associated to different patches\n(b) A unified method for image and video segmentation.\nFigure 1: Attention maps associated with different\npatches highlight different regions of the object (Fig. 1a),\nwhich motivates us to build a unified graph-based solution\nfor unsupervised image and video segmentation (Fig. 1b).\ndata [34]. The high cost of this approach becomes even\nmore apparent when using transfer learning to adapt a pre-\ntrained object detector to a new application domain. Re-\nsearchers have attempted to overcome this barrier using\nactive learning [1, 48], semi-supervised learning [8, 36],\nand weakly-supervised learning [26, 43] with limited re-\nsults. In this paper, we report on results of an effort to\nuse features provided by transformers trained with self-\nsupervised learning, obviating the need for expensive an-\nnotated training data.\narXiv:2209.00383v3  [cs.CV]  5 Dec 2023\nAUTHOR VERSION\nVision transformers trained with self-supervised learning\n[6,14], such as DINO [6] and MAE [17,19,55] have been\nshown to outperform supervised training on downstream\ntasks. In particular, the attention maps associated with\npatches typically contain meaningful semantic informa-\ntion (Fig. 1a). For example, experiments with DINO [6]\nindicate that the attention maps of the class token high-\nlight salient object regions. However, such attention maps\nare noisy and cannot be directly used to detect or segment\nobjects.\nThe authors of LOST [49] have shown that the learned\nfeatures from DINO can be used to build a graph and seg-\nment objects using the inverse degrees of nodes. Specif-\nically, LOST employs a heuristic seed expansion strategy\nto accommodate noise and detect a single bounding box\nfor a foreground object. We have investigated whether\nsuch learned features can be used with a graph-based ap-\nproach to detect and segment salient objects in images and\nvideos (Fig. 1b), formulating the segmentation problem\nusing the classic normalised cut algorithm (Ncut) [45].\nIn this paper we describe TokenCut, a unified graph-based\napproach for image and video segmentation using fea-\ntures provided by self-supervised learning. The process-\ning pipeline for this approach, illustrated in Fig. 2, is com-\nposed of three steps: 1) graph construction, 2) graph cut,\n3) edge refinement. In the graph construction step, the\nalgorithm uses image patches as nodes and uses features\nprovided by self-supervised learning to describe the sim-\nilarity between pairs of nodes. For images, edges are la-\nbeled with a score for the similarity of patches based on\nlearned features for RGB appearance. For videos, edge\nlabels combine similarities of learned features for RGB\nappearance and optical flow.\nTo cut the graph, we rely on the classic normalized cut\n(Ncut) algorithm to group self-similar regions and delimit\nthe salient objects. We solve the graph-cut problem using\nspectral clustering with generalized eigen-decomposition.\nThe second smallest eigenvector provides a cutting so-\nlution indicating the likelihood that a token belongs to\na foreground object, which allows us to design a simple\npost-processing step to obtain a foreground mask. We also\nshow that standard algorithmns for edge-aware refine-\nment, such as Conditional Random Field [29] (CRF) and\nBilateral Solver [5] (BS) can be used to refine the masks\nfor detailed object boundary detection. This approach can\nbe considered as a run-time adaptation method, because\nthe model can be used to process an image or video with-\nout the need to retrain the model.\nDespite its simplicity, TokenCut significantly improves\nunsupervised saliency detection in images. Specifically,\nit achieves 77.7%, 62.8%. 61.9% mIoU on the EC-\nSSD [46], DUTS [65] and DUT-OMRON [73] respec-\ntively, and outperforms the previous state-of-the-art by a\nmargin of 4.4%, 5.6% and 5.2%. For unsupervised video\nsegmentation, TokenCut achieves competitive results on\nDA VIS [40], FBMS [39], SegTV2 [31]. Additionally, To-\nkenCut also obtains important improvement on unsuper-\nvised object discovery. For example, TokenCut outper-\nforms DSS [37], which is a concurrent work, by a margin\nof 6.1%, 5.7%, and 2.6% respectively on the VOC07 [15],\nVOC12 [16], COCO20K [34].\nIn summary, the main contributions of this paper are as\nfollows:\n• We describe TokenCut , a simple and unified approach\nto segment objects in images and videos that does not\nrequire human annotations for training. *\n• We show that TokenCut significantly outperforms pre-\nvious state-of-the-art methods unsupervised saliency\ndetection and unsupervised object discovery on images.\nAs a training-free method, TokenCut achieves compet-\nitive performance on unsupervised video segmentation\ncompared to the state-of-the-art methods.\n• We provide a detailed analysis on the TokenCut to vali-\ndate the design of the proposed approach.\n2 Related Work\nSelf-supervised vision transformers. ViT [14]\nhas shown that the transformer architecture [59] can be\neffective for computer vision tasks using supervised learn-\ning. Recently, many variants of ViT have been proposed\nto learn image encoders in a self-supervised manner.\nMoCo-V3 [7] demonstrates that using contrastive learning\non ViT can achieve strong results. DINO [6] shows that\ntransformers can be trained with self-distillation loss [20]\nand shows that the features learn by ViT contain explicit\ninformation useful for image semantic segmentation. In-\nspired by BERT, several approaches [4, 13, 19, 33] learn\nby missing token replacement, masking some tokens from\nthe input input and learning to recover the missing tokens\nin the output.\nUnsupervised object discovery. Given a group of\nimages, unsupervised object discovery seeks to discover\nand delimit similar objects that appear in multiple im-\nages. Early research [9,21,24,25,60] formulated the prob-\nlem using an hypothesis about the frequency of object oc-\ncurrences. Other researchers formulated object detection\nas an optimization problem over bounding box propos-\nals [11, 53, 61, 62] or as a ranking problem [63].\nRecently, LOST [49] significantly improved the state-\nof-the-art for unsupervised object discovery. LOST ex-\ntracts features using a self-supervised transformer based\non DINO [6] and designs a heuristic seed expansion strat-\negy to obtain a single object region. A concurrent work\nDSS [37] designs a weighted graph over patches using\nself-supervised transformer features as well as a KNN\nbased image matting algorithm using a color affinity ma-\ntrix. The eigen-decomposition of the affinity matrix is\ncomputed to obtain a coarse object mask. Following\n*The implementation is available at https://www.m-\npsi.fr/Papers/TokenCut2022/. An online demo is accessible\nat https://huggingface.co/spaces/yangtaowang/TokenCut (last\naccess May 2023).\n2\nAUTHOR VERSION\n(a) Graph Construction\n (b) Graph Cut\n (c) Edge Refinement\nFigure 2: An overview of the TokenCut approach. The algorithm constructs a fully connected graph in which the\nnodes are image patches and the edges are similarities between the image patches using transformer features. Object\nsegmentation is then solved using the Ncut algorithm [45]. Bi-partition of the graph using the second smallest eigen-\nvector allows to detect foreground object. A Bilateral Solver [5] (BS) or Conditional Random Field [29] (CRF) can be\nused for edge refinement.\nLOST [49], DSS also assumes that the foreground ob-\nject occupies a smaller region than the background. While\nDSS has a similar eigen-attention map as TokenCut, DSS\nis less able to detect large objects.\nAs with LOST, TokenCut also uses features obtained with\nself-supervised learning. However, rather than relying on\nthe attention map of some specific nodes, TokenCut forms\na fully connected graph of image tokens, with edges la-\nbeled with a similarity score between tokens based on\ntransformer features. The classical Ncut [45] algorithm\nis then used to detect and segment image objects.\nUnsupervised saliency detection. Unsupervised\nsaliency detection seeks to segment a salient object within\nan image. In this paper, we show that incorporating a sim-\nple post-processing step into TokenCut for unsupervised\nobject discovery can provide a strong baseline method for\nunsupervised saliency detection. Earlier works on Unsu-\npervised saliency detection [23, 32, 71, 79] use techniques\nsuch as color contrast [10], background priors [67], or\nsuper-pixels [32, 73]. More recently, unsupervised deep\nmodels [38, 77] have been used for saliency detection us-\ning noisy pseudo-labels generated from different hand-\ncrafted saliency methods. [64] shows that unsupervised\nGANs can differentiate between foreground and back-\nground pixels and generate high-quality saliency masks.\nSelfMask [47] use a spectral clustering method with self-\nsupervised features to group pixels into a set of candi-\ndate clusters. SelfMask is trained by selecting the salient\nmasks from the set of spectral clusters as pseudo-masks\nfor supervision using cluster voting scheme.\nUnsupervised video segmentation. Given an un-\nlabeled video, unsupervised video segmentation aims to\ngenerate pixel-level masks for the object of interest in\nthe video. Prior works segment objects by selecting\nsuper-pixels [28], learning flattened 3D object represen-\ntations [30], constructing an adversarial network to mask\na region such that the model can predict the optical flow\nof the masked region [75], or reconstructing the optical\nflow in a self-supervised manner [72], etc. DyStaB [74]\nfirst partitions the motion field by minimizing the tempo-\nral consistent mutual information and then uses the seg-\nments to learn the object detector, in which the models\nare jointly trained with a bootstrapping strategy. The de-\nformable sprites method (DeSprite) [76] trains a video\nauto-encoder to segment the object of interest by decom-\nposing the video into layers of persistent motion groups.\nIn contrast to these methods [72, 74, 75], our proposed\nmethod does not require prior training on videos. Com-\npared with methods [28, 30] that do not train on videos,\nour method achieves superior performance.\n3 Approach: TokenCut\nIn this section, we present TokenCut, a unified algorithm\nthat can be used to segment salient objects in an image\nor moving objects in a video. Our approach, illustrated\nin Fig. 2, is based on a graph where the nodes are vi-\nsual patches from either an image or a sequence of frames,\nand the edges are similarities between the features of the\nnodes based on the features provided by a visual trans-\nformer trained with self-supervised learning.\nThis section is organised as follows: we first briefly re-\nview vision transformers and the Normalized Cut algo-\nrithm in Section 3.1.1 and Section 3.1.2. We then describe\nthe TokenCut algorithm for object detection and segmen-\ntation in images and videos in Section 3.2.\n3.1 Background\n3.1.1 Vision Transformers\nThe Vision Transformer has been proposed in [14]. The\nkey idea is to process an image with transformer [59] ar-\n3\nAUTHOR VERSION\nchitectures using non-overlapping patches as tokens. For\nan image with sizeH×W, a vision transformer takes non-\noverlapping K × K image patches as inputs, resulting in\nN = HW/K 2 patches. Each patch is used as a token, de-\nscribed by a vector of numerical features that provide an\nembedding. An extra learnable token, denoted as a class\ntoken CLS, is used to represent the aggregated informa-\ntion of the entire set of patches. A positional encoding is\nadded to CLS token and the set of patch tokens, and the\nresulting vector is fed to a standard Vision Transformer\nwith self-attention [59] and layer normalization [2].\nThe Vision Transformer is composed of several stacked\nlayers of encoders, each with feed-forward networks and\nmultiple attention heads for self-attention, paralleled with\nskip connections. For the TokenCut algorithm, we use the\nVision Transformer, trained with self-supervised learning.\nWe extract latent features from the final layer as the input\nfeatures for TokenCut.\n3.1.2 Normalized Cut (Ncut)\nGraph partitioning. Given a graph G = ( V, E),\nwhere V and E are sets of nodes and edges respectively.\nE is the similarity matrix with Ei,j as the edge between\nthe i-node vi and the j-th node vj. Ncut [45] is proposed\nto partition the graph into two disjoint sets A and B. Dif-\nferent to standard graph cut, Ncut criterion considers both\nthe total dissimilarity betweenA and B as well as the total\nsimilarity within A and B. Precisely, we seek to minimize\nthe Ncut energy [45]:\nC(A, B)\nC(A, V) + C(A, B)\nC(B, V) , (1)\nwhere C measures the degree of similarity between two\nsets. , C(A, B) = P\nvi∈A,vj∈B Ei,j and C(A, V) is the\ntotal connection from nodes in A to all the nodes in the\ngraph.\nAs shown by [45], the equivalent form of optimization\nproblem in Eqn 1 can be expressed as:\nmin\nx\nE(x) = min\ny\nyT (D − E)y\nyT Dy , (2)\nwith the condition of y ∈ {1, −b}N , where b satisfies\nyT D1 = 0, where D is a diagonal matrix with di =P\nj Ei,j on its diagonal.\nNcut solution with the relaxed constraint. Tak-\ning z = D\n1\n2 y, Eqn 2 can be rewritten as:\nmin\nz\nzT D−1\n2 (D − E)D−1\n2 z\nzT z . (3)\nIndicating in [45], the formulation in Eqn 3 is equivalent\nto the Rayleigh quotient [58], which is equivalent to solve\nD−1\n2 (D − E)D−1\n2 z = λz, where D − E is the Lapla-\ncian matrix and known to be positive semidefinite [41].\nTherefore z0 = D\n1\n2 1 is an eigenvector associated to the\nsmallest eigenvalue λ = 0. According to the Rayleigh\nquotient [58], the second smallest eigenvector z1 is per-\npendicular to the smallest one ( z0) and can be used to\nminimize the energy in Eqn 3,\nz1 = argmin\nzT z0\nzT D−1\n2 (D − E)D−1\n2 z\nzT z .\nTaking z = D\n1\n2 y,\ny1 = argmin\nyT D1=0\nyT (D − E)y\nyT Dy .\nThus, the second smallest eigenvector of the generalized\neigensystem (D−E)y = λDy provides a solution to the\nNcut [45] problem.\n3.2 The TokenCut Algorithm\nThe TokenCut algorithm consists of three steps: (a) Graph\nConstruction, (b) Graph Cut, (c) Edge Refinement. An\noverview of the algorithm is shown in Fig. 2.\n3.2.1 Graph construction\nImage Graph. As described in Section 3.1.2, To-\nkenCut operates on a fully connected undirected graph\nG = (V, E), where vi represents the feature vectors of the\nnode vi. Each patch is linked to other patches by labeled\nedges, E. Edge labels represent a similarity score S.\nEi,j =\n\u001a1, if S(vi, vj) ≥ τ\nϵ, else , (4)\nwhere τ is a hyper-parameter and S(vi, vj) = vivj\n∥vi∥2∥vj∥2\nis the cosine similarity between features.ϵ is a small value\n10−5 to assure a fully connected graph. Note that the spa-\ntial location information has been implicitly included in\nthe features, which is achieved by positional encoding in\nthe transformer.\nVideo Graph. As with images, videos are pre-\nsented as a fully connected graph where the nodes V are\nvisual patches and the edges E are labeled with the simi-\nlarity between patches. However, for videos, similarity in-\ncludes a score based on both RGB appearance and a RGB\nrepresentation of optical flow computed between consec-\nutive frames [3]. The algorithm extracts a sequence of\nfeature vectors using a vision transformer as described in\nSection 3.1.1. Let vI\ni and vF\ni denote the feature of i-th im-\nage patch and flow patch respectively. Edges are labeled\nwith the average over the similarities between image fea-\nture and flow features, expressed as:\nEi,j =\n(\n1, if\nS(vI\ni ,vI\nj )+S(vF\ni ,vF\nj )\n2 ≥ τ\nϵ, else\n. (5)\n4\nAUTHOR VERSION\n(a) LOST Inverse\nDegree Attention\n(b) LOST\nDetection\n(c) DSS Eigen\nAttention\n(d) DSS\nDetection\n(e) Our Eigen\nAttention\n(f) Our\nDetection\nFigure 3: Visual results of unsupervised single object discovery on VOC12.In (a), we show the map of LOST [49]\ninverse degrees, which is used for detection (b). In (c), DSS [37] eigen attention map is shown for its detection in (d),\nnote that DSS is hard to detect large objects leading to an inverse foreground and background in eigen attention map.\nFor our approach, we illustrate the eigenvector in (e) and our detection in (f). Blue and Red bounding boxes indicate\nthe ground-truth and the predicted bounding boxes respectively.\nImage feature provide segmentation using appearance\nwhile flow features focus on segmentation with motion.\nWe provide a full analysis on the definition of edges in\nSection 4.5.\n3.2.2 Graph Cut\nThe Ncut algorithm is used to partition the fully connected\ngraph. Ncut computes the second smallest eigenvector of\nthe generalized eigensystem, as described in Section 3.1.2\nto highlight salient objects. We refer to this eigenvector\nas a measure for “eigen-attention”, and provide visual-\nizations of the attention map provided by this vector in\nSection 4. TokenCut uses eigen-attention to bi-partition\nthe graph, determines which partition belongs to the fore-\nground and then determines the nodes that belong to the\neach object region.\nBi-partition the graph. To partition the nodes into\ntwo disjoint sets, TokenCut uses the average value of\nthe second smallest eigenvector to cut the graph y1 =\n1\nN\nP\ni yi\n1. Formally, A = {vi|yi\n1 ≤ y1} and B =\n{vi|yi\n1 > y1}. Note that, we also explored the use of\nclassical clustering algorithms, such as K-means and EM,\nto cluster the second smallest eigenvector into 2 partitions.\nThe comparison is available in Section 4.5. Our experi-\nmens show that the average value generally provides bet-\nter results.\nForeground Determination. Given the two dis-\njoint sets of nodes, TokenCut selects the partition with\nthe maximum absolute value vmax as the foreground.\nIntuitively, the foreground object should be salient and\nthus less connected to the entire graph. In other words,\ndi < dj if vi belongs to the foreground while vj is the\nbackground token. Therefore, the eigenvector of the fore-\nground object should have a larger absolute value than the\nbackground region.\nSelect the object. In images, we are interested in\nsegmenting a single object. However, the foreground can\ncontain more than one salient object region. TokenCut se-\nlects the connected component in the foreground contain-\ning the maximum absolute valuevmax as the detected ob-\nject. In videos, as the goal is to segment objects based on\nboth motion and appearance, TokenCut takes the entire\nforeground region as the final output.\n3.2.3 Edge Refinement\nThe graph cut algorithm provides coarse masks of object\nregions due to the large size of transformer patches. The\nboundaries of such masks can be easily refined using stan-\ndard edge refinement technique. We have experimented\nwith off-the-shelf edge-aware post-processing techniques\nsuch as Bilateral Solver [5] (BS), Conditional Random\nField [29] (CRF) on top of the obtained coarse mask to\n5\nAUTHOR VERSION\nTable 1: Comparisons for unsupervised single object discovery . We compare TokenCut to state-of-the-art object\ndiscovery methods on VOC07 [15], VOC12 [16] and COCO20K [34,62] datasets. Model performances are evaluated\nwith CorLoc metric. “Inter-image Simi.” means the model leverages information from the entire dataset and explores\ninter-image similarities to localize objects.\nMethod Inter-image Simi. DINO [6] Feat. VOC07 [15] VOC12 [16] COCO20K [34, 62]\nSelective Search [49, 57] - 18.8 20.9 16.0\nEdgeBoxes [49, 80] - 31.1 31.6 28.8\nKim et al. [27, 49] ✓ - 43.9 46.4 35.1\nZhange et al. [49, 78] ✓ - 46.2 50.5 34.8\nDDT+ [49, 66] ✓ - 50.2 53.1 38.2\nrOSD [49, 62] ✓ - 54.5 55.3 48.5\nLOD [49, 63] ✓ - 53.6 55.1 48.5\nDINO-seg [6, 49] ViT-S/16 [14] 45.8 46.2 42.1\nLOST [49] ViT-S/16 [14] 61.9 64.0 50.7\nDSS [37] ViT-S/16 [14] 62.7 66.4 56.2\nTokenCut ViT-S/16 [14] 68.8 (↑6.1) 72.1 ( ↑5.7) 58.8 ( ↑2.6)\nLOD + CAD⋆ [49] ✓ - 56.3 61.6 52.7\nrOSD + CAD⋆ [49] ✓ - 58.3 62.3 53.0\nLOST + CAD⋆ [49] ViT-S/16 [14] 65.7 70.4 57.5\nTokenCut + CAD⋆ [49] ViT-S/16 [14] 71.4 (↑5.7) 75.3 ( ↑4.9) 62.6 ( ↑5.1)\n⋆ +CAD indicates to train a second stage class-agnostic detector with “pseudo-boxes” labels.\ngenerate more precise boundaries for the mask. We have\nfound that CRF usually provides the best results.\n4 Experiments\nWe evaluated the suitability of TokenCut for three\ntasks: unsupervised single object discovery, unsupervised\nsaliency detection and unsupervised video segmentation.\nWe present implementation details in Section 4.1. The\nresults of unsupervised single object discovery are shown\nin Section 4.2. The results for unsupervised saliency de-\ntection are presented in Section 4.3, and results for unsu-\npervised video segmentation in Section 4.4. We provide\nablation studies in Section 4.5.\n4.1 Implementation details\nModel details. For our experiments, we use the\nViT-S/16 model [14] trained with self-distillation loss\n(DINO) [6] to extract features of patches. Following [49],\nwe employ the key features of the last layer as the input\nfeatures v. Ablations on different features and ViT back-\nbones are provided in Tab. 5. We setτ = 0.2 for all image\ndatasets and τ = 0.3 for video datasets. The selection of\nτ is discussed in Section 4.5.\nAlgorithmic Cost. In terms of running time, our\nimplementation takes approximately 0.32 seconds to de-\ntect a bounding box for a salient object region in a sin-\ngle image with resolution 480 × 480 using a single GPU\nQUADRO RTX 8000. Obtainaing a coarse mask from 20\nframes of video with 320 x 576 resolution, requires an\naverage of 30 seconds with standard deviation of around\n4.5 seconds. Edge refinement the takes an additional 16.4\nseconds on average with a standard deviation 1.4 seconds.\nAs with single-frame graphs, the same video takes 0.93\nseconds in average to obtain the coarse mask with stan-\ndard deviation of 0.17 for all frames. The post processing\nstep cost 16.1 seconds with standard deviation of 1.4. For\nn tokens, the algorithmic complexity for building such a\ngraph is O(n2). Thus the average processing time grows\nwith the square of the number of frames in the video.\nOptical flow details. To generate optical flow,\nwe use two different approaches: RAFT [54] and\nARFlow [35]. The first one is supervised and the sec-\nond one is self-supervised. We extract the optical flow at\nthe original resolution of the image pairs, with the frame\ngaps n = 1 for DA VIS [40] and SegTV2 [31] dataset. For\nFBMS [39] we use n = 3 to compensate for the much\nslower rate of motion. This improves the optical flow\nquality as small pixel-level motions are hard to detect us-\ning off-the-shelf methods. Optical flow features are en-\ncoded as RGB values, using standard techniques for vi-\nsualization of optical flow [3]. This allows us to directly\nuse the pre-trained self-supervised transformers with op-\ntical flow encoded as RGB. Because of limits on available\ncomputational resources, we construct the video graph\nwith a maximum of 90 frames on the DA VIS dataset. For\nvideos longer than 90 frames, it is possible to aggregate\nresults using non-overlapping subgraphs with maximum\nvideo frames of 90.\n4.2 Unsupervised Single Object Discovery\nDatasets. TokenCut has been evaluated on three\ncommonly used benchmarks for unsupervised single ob-\nject discovery: VOC07 [15], VOC12 [16] and COCO20K\n[34, 62]. VOC07 and VOC12 contain 5011 and 11540\nimages respectively which belong to 20 categories.\n6\nAUTHOR VERSION\nTable 2: Comparisons for unsupervised saliency detection We compare TokenCut to state-of-the-art unsupervised\nsaliency detection methods on ECSSD [46], DUTS [65] and DUT-OMRON [73]. TokenCut achieves better results\ncompared with other competitive approaches.\nMethod ECSSD [46] DUTS [65] DUT-OMRON [73]maxFβ(%) IoU(%) Acc.(%) maxFβ(%) IoU(%) Acc.(%) maxFβ(%) IoU(%) Acc.(%)\nHS [71] 67.3 50.8 84.7 50.4 36.9 82.6 56.1 43.3 84.3wCtr [79] 68.4 51.7 86.2 52.2 39.2 83.5 54.1 41.6 83.8WSC [32] 68.3 49.8 85.2 52.8 38.4 86.2 52.3 38.7 86.5DeepUSPS [38] 58.4 44.0 79.5 42.5 30.5 77.3 41.4 30.5 77.9BigBiGAN [64] 78.2 67.2 89.9 60.8 49.8 87.8 54.9 45.3 85.6E-BigBiGAN [64] 79.7 68.4 90.6 62.4 51.1 88.2 56.3 46.4 86.0LOST [44, 49] 75.8 65.4 89.5 61.1 51.8 87.1 47.3 41.0 79.7LOST [44, 49]+BS [5] 83.7 72.3 91.6 69.7 57.2 88.7 57.8 48.9 81.8DSS [37] - 73.3 - - 51.4 - - 56.7 -\nTokenCut 80.3 71.2 91.8 67.2 57.6 90.3 60.0 53.3 88.0TokenCut + BS [5] 87.4 (↑3.7) 77.2 93.4 75.5 62.4 91.4 69.7 (↑11.9) 61.8 89.7TokenCut + CRF [29] 87.4 (↑3.7) 77.7 (↑4.4) 93.6 (↑2.0) 75.7 (↑6.0) 62.8 (↑5.6) 91.5 (↑2.8) 69.2 61.9 (↑5.2) 89.8 (↑8.0)\nCOCO20K consists of 19817 randomly chosen images\nfrom the COCO2014 dataset [34]. VOC07 and VOC12\nare commonly used to evaluate unsupervised object dis-\ncovery [11, 61–63, 66]. COCO20K is a popular bench-\nmark for a large scale evaluation [62].\nEvaluation metric. In line with previous re-\nsearch [11, 12, 50, 61–63, 66], we report performance us-\ning the CorLoc metric for precise localization. We use a\nsingle predicted bounding box for each image. For target\nimages, CorLoc is 1.0 if the intersection over union (IoU)\nscore between the predicted bounding box and the ground\ntruth bounding boxes is superior to 0.5.\nQuantitative Results. We evaluate the CorLoc\nscores in comparison with previous state-of-the-art single\nobject discovery methods [27, 49, 57, 62, 63, 66, 78, 80] on\nVOC07, VOC12, and COCO20K datasets. These meth-\nods can be roughly divided into two groups according\nto whether the model uses information from the entire\ndataset or explores inter-image similarities. Because of\nthe quadratic complexity of region comparison among\nimages, models with inter-image similarities are gener-\nally difficult to scale to larger datasets. The selective\nsearch [57], edge boxes [80], LOST [49] and TokenCut\ndo not require inter-image similarities and are thus much\nmore efficient. As shown in the Tab. 1, TokenCut con-\nsistently outperforms all the previous methods on all the\ndatasets by a large margin. Particularly, TokenCut ouper-\nforms DSS [37] by 6.1%, 5.7% and 2.6% for VOC07,\nVOC12 and COCO20K respectively using the same ViT-\nS/16 features.\nWe also list a set of results that includes using a second\nstage unsupervised training strategy to boost the perfor-\nmance. This is referred to as Class-Agnostic Detection\n(CAD) and proposed in LOST [49]. For this, we first com-\npute K-means on all the boxes produced by the first stage\nsingle object discovery model to obtain pseudo labels of\nthe bounding boxes. Then a classical Faster RCNN [42] is\ntrained on the pseudo labels. As shown in Tab. 1, Token-\nCut with CAD outperforms the state-of-the-art by 5.7%,\n4.9% and 5.1% on VOC07, VOC12 and COCO20k re-\nspectively.\nQualitative Results. In Fig. 3, we provide visual-\nization for LOST [49], DSS [37] and TokenCut*. For each\nmethod, we visualize the heatmap that is used to perform\nobject detection. For LOST, the detection is mainly based\non the map of inverse degree ( 1\ndi\n). For DSS, the heatmap\nis the attention map associated to the second eigenvec-\ntor. For TokenCut, we display the second smallest eigen-\nvector. The visual results demonstrate that TokenCut can\nextract a high quality segmentation for the salient object.\nCompared with LOST and DSS, TokenCut is able to ex-\ntract a more complete segmentation as can be seen in the\nfirst and the second samples in Fig. 3. In other cases, when\nLOST and DSS are unable to detect a large object, Token-\nCut can detect the object properly. Examples for this can\nbe seen in the third and fourth samples in Fig. 3.\nInternet Images. We further tested TokenCut on\nInternet images *. The results are in Fig 5. It can be\nseen that even though the input images have noisy back-\ngrounds, TokenCut can provide a precise attention map to\ncover the object and lead to an accurate prediction of the\nbounding box, demonstrates robustness of the method.\n4.3 Unsupervised Saliency detection\nDatasets. We validated the performance of\nTokenCutfor unsupervised Saliency detection us-\ning three datasets : Extended Complex Scene\nSaliency Dataset(ECSSD) [46], DUTS [65] and DUT-\nOMRON [73]. ECSSD contains 1 000 real-world images\nof complex scenes for testing. DUTS contains 10 553\ntrain and 5 019 test images. The training set is collected\nfrom the ImageNet detection train/val set. The test set is\ncollected from ImageNet test, and the SUN dataset [70].\nFollowing the previous work [44], we report the perfor-\nmance on the DUTS-test subset. DUT-OMRON [73]\ncontains 5 168 images of high quality natural images for\ntesting.\n*More visual results can be found in the project webpage.\n*We provide an online demo allowing to test Internet images.\n7\nAUTHOR VERSION\n(a) GT\n(b) TokenCut\n(c) TokenCut\n+ CRF\nFigure 4: Visual results of unsupervised segments on ECSSD [46]. In (a), we show the ground truth. (b) is TokenCut\ncoarse mask segmentation result. The performance of TokenCut + Bilateral Solver (BS) is presented in (c).\n(a) Input (b) Eigen Attention (c) Detection\nFigure 5: Visualization of images from the Internet.We\nshow the input images, our eigen attention, and final de-\ntection in (a), (b), and (c) respectively.\nEvaluation Metrics. We report three standard\nmetrics: F-measure, IoU and Accuracy. F-measure is\na standard measure for saliency detection, computed as\nFβ = (1+β2)Precision ×Recall\nβ2Precision +Recall , where the Precision and\nRecall are defined using a binarized predicted mask and\na ground truth mask. The maxFβ is the maximum value\nof 255 uniformly distributed binarization thresholds. Fol-\nlowing previous work [44, 64], we set β = 0.3 for con-\nsistency. IoU(Intersection over Union) score is computed\nTable 3: Comparisons for unsupervised video segmen-\ntation. We report Jaccard index and compare Token-\nCut to state-of-the-art unsupervised video segmentation\nmethods on DA VIS [40], FBMS [39] and SegTV2 [31].\nTokenCut achieves results that are similar to competing\napproaches.\nMethod Flow Training DA VIS [40] FBMS [39] SegTV2 [31]\nARP [28] CPM [22] 76.2 59.8 57.2ELM [30] Classic+NL [51] 61.8 61.6 -MG [72] RAFT [54] ✓ 68.3 53.1 58.2CIS [75] PWCNet [52]✓ 71.5 63.5 62DyStaB [74]⋆ PWCNet [52]✓ 80.0 73.2 74.2DeSprite [76]‡ RAFT [54]✓ 79.1 71.8 72.1\nTokenCut RAFT [54] 64.3 60.2 59.6TokenCut + BS [5]RAFT [54] 75.1 61.2 56.4TokenCut + CRF [29]RAFT [54] 76.7 66.6 61.6TokenCut ARFlow [35] 62.0 61.0 58.9TokenCut + BS [5]ARFlow [35] 73.1 64.7 54.6TokenCut + CRF [29]ARFlow [35] 74.4 69.0 60.8\n⋆: [74] is trained on DA VIS and evaluated on FBMS and\nSegTV2;\n‡: [76] is optimized for each video separately.\nbased on the binary predicted mask and the ground-truth,\nthe threshold is set to 0.5. Accuracy measures the pro-\nportion of pixels that have been correctly assigned to the\nobject/background. The binarization threshold is set to\n0.5 for masks.\nResults. Qualitative results are shown in Tab. 2.\nTokenCut significantly outperforms previous state-of-the-\nart methods. Adding BS [5] or CRF [29] refines the\nboundary of an object and further boosts the Token-\nCut performance, as can be seen in the visual results pre-\nsented in Fig. 4.\n4.4 Unsupervised Video Segmentation\nDatasets. We further evaluate TokenCut using\nthree commonly used datasets for unsupervised video seg-\nmentation: DA VIS [40], FBMS [39] and SegTV2 [31].\nDA VIS contains 50 high-resolution real-word videos,\nwhere 30 are for training and 20 are for validation. Pixel-\n8\nAUTHOR VERSION\n(a) GT\n(b) Ours\n(c) Ours + CRF\nFigure 6: Visual results of unsupervised video segmentation on DA VIS [40]. In (a), we show the ground truth\nsegmentation. For TokenCut, we illustrate its coarse mask in (b) and refinement results with CRF in (c).\nwise annotations are depicted for the principle moving ob-\nject within the scene for each frame. FBMS consists of 59\nmultiple moving object videos, providing 30 videos for\ntesting with a total of 720 annotation frames. SegTV2\ncontains 14 full pixel-level annotated video for multiple\nobjects segmentation. Following [72], we fuse the anno-\ntation of all moving objects into a single mask on FBMS\nand SegTV2 datasets for fair comparison.\nEvaluation metrics. We report performance using\nJaccard index. The Jaccard index measures the intersec-\ntion of union between an output segmentation M and the\ncorresponding ground-truth mask G, which has been for-\nmulated as J = |M∩G|\n|M∪G|.\nResults. We compare TokenCut to the state-of-the\nart unsupervised video segmentation results in Tab. 3. To-\nkenCut achieves competitive performances for this task.\nNote that DyStaB [74] must be trained on the entire\nDA VIS training set and uses the pretrained model for\nevaluation with the FBMS and SegTV2 datasets. De-\nSprite [76] learns an auto-encoder model to optimize on\neach individual video. In contrast, TokenCut does not re-\nquire training and generalizes well for all three datasets.\nVisual results are illustrated in Fig. 6, TokenCut can pre-\ncisely segment moving objects even in the case of chal-\nlenging occlusions. Adding CRF as a post-processing fur-\nther improves the boundary for segmented regions*.\n4.5 Analysis\nImpact of τ. In Tab. 4, we provide an analysis on\nτ defined in Eqn 4. The results indicate that the effects of\nvariations in τ value are not significant and that a suitable\nthreshold is τ = 0.2 for image input and τ = 0.3 for video\ninput.\n*The segmentation results of entire videos can be found in\nthe project webpage.\nTable 4: Analysis of τ. We report CorLoc for unsuper-\nvised single object discovery on VOC07 [15], VOC12\n[16] and COCO20K [34, 62] datasets, and Jacard index\non DA VIS [40].\nτ CorLoc Jaccard Index\nVOC07 [15] VOC12 [16] COCO20K [34, 62] DA VIS [40]\n0 67.4 71.3 56.1 70.7\n0.1 68.6 72.1 58.2 74.6\n0.2 68.8 72.1 58.8 75.8\n0.3 67.7 72.1 58.2 76.7\nTable 5: Analysis of different backbones. We re-\nport CorLoc for unsupervised single object discovery\non VOC07 [15], VOC12 [16] and COCO20K [34, 62]\ndatasets.\nMethod Backbone VOC07 [15] VOC12 [16] COCO20K [34, 62]\nLOST [49] DINO-S/16 [6, 14] 61.9 64.0 50.7TokenCutDeiT-S/16 [14, 56] 2.39 2.9 3.5TokenCutMoCoV3-S/16 [7, 14] 66.2 66.9 54.5TokenCutDINO-S/16 [6, 14]68.8 (↑6.9) 72.1 (↑8.1) 58.8 (↑8.1)\nLOST [49] DINO-S/8 [6, 14] 55.5 57.0 49.5TokenCutDINO-S/8 [6, 14] 67.3 (↑11.8) 71.6 (↑14.6) 60.7 (↑11.2)\nLOST [49] DINO-B/16 [6, 14] 60.1 63.3 50.0TokenCutMAE-B/16 [14, 19] 61.5 67.4 47.7TokenCutDINO-B/16 [6, 14] 68.8 (↑8.7) 72.4 (↑9.1) 59.0 (↑9.0)\nBackbones. In Tab. 5, we provide an ablation\nstudy with different transformer backbones. The“-S” and\n“-B” are ViT small [6, 14] and ViT base [6, 14] architec-\nture respectively. The “-16” and “-8” represents patch\nsizes 16 and 8 respectively. The “DeiT” is pre-trained\nsupervised transformer model. The “MoCoV3” [7] and\n“MAE” [19] are pre-trained self-supervised transformer\nmodel. We optimise τ for different backbones: τ is set\nto 0.3 for MoCov3 and MAE, while for DINO and Deit\nτ is set to 0.2. Several insights can be found: 1) To-\nkenCut is not suitable for supervised transformer models,\nwhile self-supervised transformers provide more powerful\n9\nAUTHOR VERSION\nTable 6: Analysis of different bi-partition methods. We\nreport CorLoc for unsupervised single object discovery\non VOC07 [15], VOC12 [16] and COCO20K [34, 62]\ndatasets.\nBi-partition VOC07 VOC12 COCO20K\nMean 68.8 72.1 58.8\nEnergy (Eqn 1) 67.3 69.7 -\nEM 63.0 65.7 59.3\nK-means 67.5 69.2 61.6\nTable 7: Analysis of video input. We report Jaccard in-\ndex for video segmentation on DA VIS [40], FBMS [39]\nand SegTV2 [31] with using input. “RGB + Flow” refers\nto using both video RGB frame and RGB representa-\ntion of optical flow as input to the vision transformer.\n“Mean Flow” indicates using mean of flow instead of flow\nfeatures extracted from vision transformer”. “RGB” and\n“Flow” present using only either RGB frames or optical\nflow as input. “CRF” indicates whether edge refinement\nstep using CRF [29] is computed.\nInput CRF DA VIS [40] FBMS [39] SegTV2 [31]\nGraph per frame\nRGB ✓ 62.4 67.2 61.0\nFlow ✓ 64.1 52.8 53.7\nRGB + Flow ✓ 76.4 63.2 64.4\nGraph per video\nRGB 51.8 58.4 59.3\nFlow 49.9 48.3 46.7\nRGB + Flow 64.3 60.2 59.6\nRGB ✓ 62.2 67.5 63.7\nFlow ✓ 63.1 50.2 50.2\nRGB + MeanFlow ✓ 37.5 23.3 15.7\nRGB + Flow ✓ 76.7 66.6 61.6\nfeatures allowing completing the task with TokenCut. 2)\nAs LOST [49] relies on a heuristic seeds expansion strat-\negy, the performance varies significantly using different\nbackbones. While our approach is more robust. More-\nover, as no training is required for TokenCut, it might be\na more straightforward evaluation for the self-supervised\ntransformers.\nBi-partition strategies. In Tab. 6, we study differ-\nent strategies to separate the nodes in into two groups\nusing the second smallest eigenvector. We consider\nthree natural methods: mean value (Mean), Expectation-\nMaximisation (EM), K-means clustering (K-means). We\nhave also tried to search for the splitting point based on the\nbest Ncut energy (Eqn 1). Note this approach is compu-\ntationally expensive due to the quadratic complexity. The\nresult suggests that the simple mean value as the splitting\npoint performs well for most cases.\nVideo input. We also study the impact of using\nRGB or optical Flow for video segmentation. Quantitative\nresults are presented in Tab. 7. We can see constructing\ngraph on the entire video is better than constructing the\n(a) RGB (b) Flow (c) RGB + Flow\nFigure 7: Visualization on DA VIS [40] using different\ninputs. We show segmentation results with RGB, Flow\nand RGB + Flow in (a), (b) and (c) respectively.\nDA VIS\nFBMS\nSegTV2\n(a) Frame t (b) Next frame (c) Flow\nFigure 8: Visual results of optical flow in DA VIS,\nFBMS, SegTV2 . In (a), we show Frame t. (b) is the\nfollowing frame (t + 3 for FBMS and t + 1 for SegTV2)\nused to compute the optical flow. (c) is the optical flow\nusing RAFT [54]. Note that optical flow in the DA VIS\ndataset has high quality, whereas the flow in the FBMS\nand SegTV2, can sometimes fail, as in the cases shown in\nrows 2 and 3.\ngraph per frame. We show an anaylsis by using the mean\nof optical flow feature and use it directly without feeding\ninto transformer. The results illustrate that constructing\nthe graph with RGB and RGB representation of the Flow\ntogether can significantly improve the performances over\nDA VIS [40]. On FBMS [39] and SegTV2 [31], due to\nthe low quality of optical flow, the motion of salient ob-\njects are not detected in the optical flow as can be seen\nfrom the fact that they are not visible in the RGB visu-\nalisation. Some failure cases are shown in Fig. 8. This\nfailure of optical flow to detect slow motion impedes the\ninference process for augmenting appearance with opti-\ncal flow features. The low quality of optical flow can be\nattributed to three factors: 1) small motion between two\n10\nAUTHOR VERSION\nTable 8: Analysis of video graph. We report Jaccard in-\ndex (J ) for video segmentation on DA VIS [40] with dif-\nferent video graphs. “single frame” represent creating the\ngraph for each frame separately.\nNodes Edges DA VIS ( J)\nVideo min(S(vIi ,vIj), S(vFi ,vFj )) 73.7\nVideo max(S(vIi ,vIj), S(vFi ,vFj )) 71.1\nVideo S(vI\ni,vI\nj)+S(vF\ni ,vF\nj )\n2 76.7\nSingle Frame S(vI\ni,vI\nj)+S(vF\ni ,vF\nj )\n2 76.4\nframes; 2) low quality of raw image, for instance sev-\neral examples in SegTV2, such as birdfall; 3) the absence\nof fine-tuning for the pre-trained optical flow model on\nthese three datasets. Using both RGB appearance and\nflow lead to a slight improvement before edge refinement,\nbut slightly worse results after edge refinement compared\nto using only RGB appearance. Some qualitative results\nare illustrated in Fig. 7. We can see how RGB frame and\noptical flow are complementary to each other: in the first\nrow, the target moving person shares semantically similar\nfeatures to other audiences and using only RGB frames\nwould produce a mask cover all the persons; in the sec-\nond row, the flow also has non-negligible values on the\nsurface of the river, thus using only flow leads to worse\nperformance.\nVideo graph. In Tab. 8, we provide an analysis for\ndifferent ways to construct graphs for video. For edges,\nwe also consider the minimum and maximum values be-\ntween the flow and RGB similarities. For nodes, a natural\nbaseline is to build a graph for each single frame. We can\nsee that the optimal choice is to use the average value of\nthe flow and RGB similarities (Eqn. 4) and build a graph\nfor an entire video.\n5 Discussion\nMulti-Object Segmentation. In the context of the\nUnsupervised Single Object Discovery task, the primary\nobjective is to identify the most salient object within a\ngiven image. Consequently, we only choose the largest\nconnected component in our approach. However, Token-\nCutcan identify more than one connected component in\nthe second smallest eigenvector when multiple objects are\npresent in the images. To illustrate this capability, we have\nincluded two examples in Fig 10. In Fig 11, we provide\nexamples when multiple objects are moving from differ-\nent directions. These results illustrate the robustness of\nour method.\nLimitations. Despite the good performance of the\nTokenCut proposal, it has several limitations. We show\nseveral failure cases in Fig. 9: i) As seen in the 1st row,\nTokenCut focuses on the largest salient part in the im-\nage, which may not be the desired object. ii) Similar to\nLOST [49], TokenCut assumes that a single salient object\n(a) LOST\nInverse Attn.\n(b) LOST\nDetection\n(c) Our Eigen\nAttention\n(d) Our\nDetection\nFigure 9: Failure cases on VOC12 (1st and 2nd row)\nand COCO (3rd row). LOST [49] mainly relies on the\nmap of inverse degrees (a) to perform detection (b). For\nour approach, we illustrate the eigenvector in (c) and our\ndetection in (d). Blue and Red bounding boxes indicate\nthe ground-truth and the predicted bounding boxes respec-\ntively.\n(a) Raw\nimage\n(b) Eigen\nAttention\n(c) Coarse\nmask\n(d) Fine\nmask\nFigure 10: Visual results of multiple objects in images.\nIn (a), we show the original image. TokenCut eigen Atten-\ntion is illustrated in (b). (c) is TokenCut coarse mask seg-\nmentation results before selecting the largest connected\ncomponent. (d) is TokenCut fine mask using bilateral\nsolver.\noccupies the foreground. If multiple overlapping objects\nare present in an image, both LOST and our approach\nwould fail to detect one of the object, as displayed in the\n2nd row. iii) For object detection, neither LOST norTo-\nkenCut can handle occlusion properly, as shown in the 3rd\nrow.\n6 Conclusion\nThis paper describes TokenCut, an unified and effective\napproach for both image and video object segmentation\nwithout the need for supervised learning. TokenCut uses\n11\nAUTHOR VERSION\n(a) Frame t (b) Frame t+5 (c) Frame t+10\nFigure 11: Visual results of multiple objects moving.\nWe visualize the case when multiple object moving from\ndifferent direction, TokenCut is also capable of segment-\ning the moving objects.\nfeatures from self-supervised transformers to constructs a\ngraph where nodes are patches and edges represent sim-\nilarities between patches. For videos, optical flow is in-\ncorporated to determine moving objects. We show that\nsalient objects can be directly detected and delimited us-\ning the Normalized Cut algorithm. We evaluated this ap-\nproach on unsupervised single object discovery, unsuper-\nvised saliency detection, and unsupervised video object\nsegmentation, demonstrating that TokenCut can provide a\nsignificant improvement over previous approaches. Our\nresults demonstrate that self-supervised transformers can\nprovide a rich and general set of features that may likely\nbe used for a variety of computer vision problems.\nAcknowledgment\nThis work has been partially supported by the MIAI Mul-\ntidisciplinary AI Institute at the Univ. Grenoble Alpes\n(MIAI@Grenoble Alpes - ANR-19-P3IA-0003), and by\nthe EU H2020 ICT48 project Humane AI Net under con-\ntract EU #952026.\nReferences\n[1] Hamed H Aghdam, Abel Gonzalez-Garcia, Joost\nvan de Weijer, and Antonio M L´opez. Active learn-\ning for deep detection neural networks. In ICCV,\n2019. 1\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. Layer normalization. arXiv, 2016. 4\n[3] Simon Baker, Daniel Scharstein, JP Lewis, Ste-\nfan Roth, Michael J Black, and Richard Szeliski.\nA database and evaluation methodology for optical\nflow. IJCV, 2011. 4, 6\n[4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert\npre-training of image transformers. arXiv, 2021. 2\n[5] Jonathan T Barron and Ben Poole. The fast bilateral\nsolver. In ECCV, 2016. 2, 3, 5, 7, 8\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e\nJ´egou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vi-\nsion transformers. In ICCV, 2021. 2, 6, 9\n[7] Xinlei Chen, Saining Xie, and Kaiming He. An em-\npirical study of training self-supervised vision trans-\nformers. In ICCV, 2021. 2, 9\n[8] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jing-\ndong Wang. Semi-supervised semantic segmenta-\ntion with cross pseudo supervision. In CVPR, 2021.\n1\n[9] Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang,\nand Jia-Bin Huang. Show, match and segment: Joint\nweakly supervised learning of semantic matching\nand object co-segmentation. PAMI, 2020. 2\n[10] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang,\nPhilip HS Torr, and Shi-Min Hu. Global contrast\nbased salient region detection. TPAMI, 2014. 3\n[11] Minsu Cho, Suha Kwak, Cordelia Schmid, and Jean\nPonce. Unsupervised object discovery and localiza-\ntion in the wild: Part-based matching with bottom-\nup region proposals. In CVPR, 2015. 2, 7\n[12] Thomas Deselaers, Bogdan Alexe, and Vittorio Fer-\nrari. Localizing objects while learning their appear-\nance. In ECCV, 2010. 7\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding.\nIn NAACL-HLT, 2018. 2\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for im-\nage recognition at scale. In ICLR, 2020. 2, 3, 6,\n9\n[15] M. Everingham, L. Van Gool, C. K. I. Williams,\nJ. Winn, and A. Zisserman. The PASCAL Visual\nObject Classes Challenge 2007 (VOC2007) Results.\nPASCAL VOC2007. 2, 6, 9, 10\n[16] M. Everingham, L. Van Gool, C. K. I. Williams,\nJ. Winn, and A. Zisserman. The PASCAL Visual\nObject Classes Challenge 2012 (VOC2012) Results.\nPASCAL VOC2012. 2, 6, 9, 10\n[17] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li,\nand Kaiming He. Masked autoencoders as spa-\ntiotemporal learners. In CVPR, 2022. 2\n[18] Andreas Geiger, Philip Lenz, Christoph Stiller, and\nRaquel Urtasun. Vision meets robotics: The kitti\ndataset. The International Journal of Robotics Re-\nsearch, 2013. 1\n[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,\nPiotr Doll ´ar, and Ross Girshick. Masked autoen-\ncoders are scalable vision learners. CVPR, 2022. 2,\n9\n[20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Dis-\ntilling the knowledge in a neural network. arXiv,\n2015. 2\n[21] Kuang-Jui Hsu, Yen-Yu Lin, Yung-Yu Chuang,\net al. Co-attention cnns for unsupervised object co-\nsegmentation. In IJCAI, 2018. 2\n12\nAUTHOR VERSION\n[22] Yinlin Hu, Rui Song, and Yunsong Li. Efficient\ncoarse-to-fine patchmatch for large displacement op-\ntical flow. In CVPR, 2016. 8\n[23] Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang\nWu, Nanning Zheng, and Shipeng Li. Salient object\ndetection: A discriminative regional feature integra-\ntion approach. In CVPR, 2013. 3\n[24] Armand Joulin, Francis Bach, and Jean Ponce. Dis-\ncriminative clustering for image co-segmentation. In\nCVPR, 2010. 2\n[25] Armand Joulin, Francis Bach, and Jean Ponce.\nMulti-class cosegmentation. In CVPR, 2012. 2\n[26] Tsung-Wei Ke, Jyh-Jing Hwang, and Stella X Yu.\nUniversal weakly supervised segmentation by pixel-\nto-segment contrastive learning. ICLR, 2021. 1\n[27] Gunhee Kim and Antonio Torralba. Unsupervised\ndetection of regions of interest using iterative link\nanalysis. In NeurIPS, 2009. 6, 7\n[28] Yeong Jun Koh and Chang-Su Kim. Primary object\nsegmentation in videos based on region augmenta-\ntion and reduction. In CVPR, 2017. 3, 8\n[29] Philipp Kr ¨ahenb¨uhl and Vladlen Koltun. Efficient\ninference in fully connected crfs with gaussian edge\npotentials. NIPS, 2011. 2, 3, 5, 7, 8, 10\n[30] Dong Lao and Ganesh Sundaramoorthi. Extending\nlayered models to 3d motion. In ECCV, 2018. 3, 8\n[31] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David\nTsai, and James M Rehg. Video segmentation by\ntracking many figure-ground segments. In ICCV,\n2013. 2, 6, 8, 10\n[32] Nianyi Li, Bilin Sun, and Jingyi Yu. A weighted\nsparse coding framework for saliency detection. In\nCVPR, 2015. 3, 7\n[33] Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li,\nYousong Zhu, Chaoyang Zhao, Rui Deng, Liwei\nWu, Rui Zhao, Ming Tang, et al. Mst: Masked self-\nsupervised transformer for visual representation. In\nNeurIPS, 2021. 2\n[34] Tsung-Yi Lin, Michael Maire, Serge Belongie,\nJames Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco:\nCommon objects in context. In ECCV, 2014. 1, 2,\n6, 7, 9, 10\n[35] Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu,\nYabiao Wang, Ying Tai, Donghao Luo, Chengjie\nWang, Jilin Li, and Feiyue Huang. Learning by\nanalogy: Reliable supervision from transformations\nfor unsupervised optical flow estimation. In CVPR,\n2020. 6, 8\n[36] Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen\nKuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt\nKira, and Peter Vajda. Unbiased teacher for semi-\nsupervised object detection. In ICLR, 2021. 1\n[37] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina,\nand Andrea Vedaldi. Deep spectral methods: A sur-\nprisingly strong baseline for unsupervised semantic\nsegmentation and localization. In CVPR, 2022. 2, 5,\n6, 7\n[38] Tam Nguyen, Maximilian Dax, Chaithanya Kumar\nMummadi, Nhung Ngo, Thi Hoai Phuong Nguyen,\nZhongyu Lou, and Thomas Brox. Deepusps: Deep\nrobust unsupervised saliency prediction via self-\nsupervision. In NeurIPS, 2019. 3, 7\n[39] Peter Ochs, Jitendra Malik, and Thomas Brox. Seg-\nmentation of moving objects by long term video\nanalysis. TPAMI, 2013. 2, 6, 8, 10\n[40] Federico Perazzi, Jordi Pont-Tuset, Brian\nMcWilliams, Luc Van Gool, Markus Gross,\nand Alexander Sorkine-Hornung. A benchmark\ndataset and evaluation methodology for video object\nsegmentation. In CVPR, 2016. 2, 6, 8, 9, 10, 11\n[41] Alex Pothen, Horst D Simon, and Kang-Pu Liou.\nPartitioning sparse matrices with eigenvectors of\ngraphs. SIAM journal on matrix analysis and ap-\nplications, 1990. 4\n[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. Faster r-cnn: Towards real-time object detec-\ntion with region proposal networks. NIPS, 2015. 7\n[43] Zhongzheng Ren, Zhiding Yu, Xiaodong Yang,\nMing-Yu Liu, Yong Jae Lee, Alexander G Schwing,\nand Jan Kautz. Instance-aware, context-focused, and\nmemory-efficient weakly supervised object detec-\ntion. In CVPR, 2020. 1\n[44] Xi Shen, Alexei A Efros, Armand Joulin, and Math-\nieu Aubry. Learning co-segmentation by segment\nswapping for retrieval and discovery. arXiv preprint\narXiv:2110.15904, 2021. 7, 8\n[45] Jianbo Shi and Jitendra Malik. Normalized cuts and\nimage segmentation. TPAMI, 2000. 2, 3, 4\n[46] Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. Hier-\narchical image saliency detection on extended cssd.\nTPAMI, 2015. 2, 7, 8\n[47] Gyungin Shin, Samuel Albanie, and Weidi Xie.\nUnsupervised salient object detection with spectral\ncluster voting. In CVPRW, 2022. 3\n[48] Yawar Siddiqui, Julien Valentin, and Matthias\nNießner. Viewal: Active learning with viewpoint en-\ntropy for semantic segmentation. In CVPR, 2020. 1\n[49] Oriane Sim ´eoni, Gilles Puy, Huy V . V o, Simon\nRoburin, Spyros Gidaris, Andrei Bursuc, Patrick\nP´erez, Renaud Marlet, and Jean Ponce. Localizing\nobjects with self-supervised transformers and no la-\nbels. In BMVC, 2021. 2, 3, 5, 6, 7, 9, 10, 11\n[50] Parthipan Siva, Chris Russell, Tao Xiang, and Lour-\ndes Agapito. Looking beyond the image: Unsuper-\nvised learning for object saliency and detection. In\nCVPR, 2013. 7\n[51] Deqing Sun, Stefan Roth, and Michael J Black. Se-\ncrets of optical flow estimation and their principles.\nIn CVPR, 2010. 8\n[52] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan\nKautz. Pwc-net: Cnns for optical flow using pyra-\nmid, warping, and cost volume. In CVPR, 2018. 8\n[53] Kevin Tang, Armand Joulin, Li-Jia Li, and Li Fei-\nFei. Co-localization in real-world images. In CVPR,\n2014. 2\n13\nAUTHOR VERSION\n[54] Zachary Teed and Jia Deng. Raft: Recurrent all-\npairs field transforms for optical flow. In ECCV,\n2020. 6, 8, 10\n[55] Zhan Tong, Yibing Song, Jue Wang, and Limin\nWang. Videomae: Masked autoencoders are data-\nefficient learners for self-supervised video pre-\ntraining. arXiv, 2022. 2\n[56] Hugo Touvron, Matthieu Cord, Matthijs Douze,\nFrancisco Massa, Alexandre Sablayrolles, and\nHerv´e J ´egou. Training data-efficient image trans-\nformers & distillation through attention. In ICML,\n2021. 9\n[57] Jasper RR Uijlings, Koen EA Van De Sande, Theo\nGevers, and Arnold WM Smeulders. Selective\nsearch for object recognition. IJCV, 2013. 6, 7\n[58] Charles F Van Loan and G Golub. Matrix compu-\ntations. The Johns Hopkins University Press, 1996.\n4\n[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, 2017. 2, 3, 4\n[60] Sara Vicente, Carsten Rother, and Vladimir Kol-\nmogorov. Object cosegmentation. In CVPR, 2011.\n2\n[61] Huy V V o, Francis Bach, Minsu Cho, Kai Han, Yann\nLeCun, Patrick P ´erez, and Jean Ponce. Unsuper-\nvised image matching and object discovery as op-\ntimization. In CVPR, 2019. 2, 7\n[62] Huy V V o, Patrick P ´erez, and Jean Ponce. Toward\nunsupervised, multi-object discovery in large-scale\nimage collections. In ECCV, 2020. 2, 6, 7, 9, 10\n[63] Huy V V o, Elena Sizikova, Cordelia Schmid, Patrick\nP´erez, and Jean Ponce. Large-scale unsupervised\nobject discovery. arXiv, 2021. 2, 6, 7\n[64] Andrey V oynov, Stanislav Morozov, and Artem\nBabenko. Object segmentation without labels with\nlarge-scale generative models. In ICML, 2021. 3, 7,\n8\n[65] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang\nFeng, Dong Wang, Baocai Yin, and Xiang Ruan.\nLearning to detect salient objects with image-level\nsupervision. In CVPR, 2017. 2, 7\n[66] Xiu-Shen Wei, Chen-Lin Zhang, Jianxin Wu, Chun-\nhua Shen, and Zhi-Hua Zhou. Unsupervised ob-\nject discovery and co-localization by deep descriptor\ntransformation. Pattern Recognition, 2019. 6, 7\n[67] Yichen Wei, Fang Wen, Wangjiang Zhu, and Jian\nSun. Geodesic saliency using background priors. In\nECCV, 2012. 3\n[68] Bichen Wu, Forrest Iandola, Peter H Jin, and Kurt\nKeutzer. Squeezedet: Unified, small, low power\nfully convolutional neural networks for real-time ob-\nject detection for autonomous driving. In CVPRW,\n2017. 1\n[69] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu,\nSerge Belongie, Jiebo Luo, Mihai Datcu, Marcello\nPelillo, and Liangpei Zhang. Dota: A large-scale\ndataset for object detection in aerial images. In\nCVPR, 2018. 1\n[70] Jianxiong Xiao, James Hays, Krista A. Ehinger,\nAude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In\nCVPR, 2010. 7\n[71] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hier-\narchical saliency detection. In CVPR, 2013. 3, 7\n[72] Charig Yang, Hala Lamdouar, Erika Lu, Andrew\nZisserman, and Weidi Xie. Self-supervised video\nobject segmentation by motion grouping. In CVPR,\n2021. 3, 8, 9\n[73] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan,\nand Ming-Hsuan Yang. Saliency detection via\ngraph-based manifold ranking. In CVPR, 2013. 2,\n3, 7\n[74] Yanchao Yang, Brian Lai, and Stefano Soatto. Dys-\ntab: Unsupervised object segmentation via dynamic-\nstatic bootstrapping. In CVPR, 2021. 3, 8, 9\n[75] Yanchao Yang, Antonio Loquercio, Davide Scara-\nmuzza, and Stefano Soatto. Unsupervised moving\nobject detection via contextual information separa-\ntion. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2019.\n3, 8\n[76] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo\nKanazawa, and Noah Snavely. Deformable sprites\nfor unsupervised video decomposition.CVPR, 2022.\n3, 8, 9\n[77] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash\nHarandi, and Richard Hartley. Deep unsupervised\nsaliency detection: A multiple noisy labeling per-\nspective. In CVPR, 2018. 3\n[78] Runsheng Zhang, Yaping Huang, Mengyang Pu,\nJian Zhang, Qingji Guan, Qi Zou, and Haibin Ling.\nObject discovery from a single unlabeled image by\nmining frequent itemsets with multi-scale features.\nTIP, 2020. 6, 7\n[79] Wangjiang Zhu, Shuang Liang, Yichen Wei, and\nJian Sun. Saliency optimization from robust back-\nground detection. In CVPR, 2014. 3, 7\n[80] C Lawrence Zitnick and Piotr Doll ´ar. Edge boxes:\nLocating object proposals from edges. In ECCV,\n2014. 6, 7\n14",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.8226572275161743
    },
    {
      "name": "Computer science",
      "score": 0.7205787897109985
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.6519036293029785
    },
    {
      "name": "Segmentation",
      "score": 0.6373949646949768
    },
    {
      "name": "Image segmentation",
      "score": 0.6011111736297607
    },
    {
      "name": "Object detection",
      "score": 0.508604884147644
    },
    {
      "name": "Computer vision",
      "score": 0.49393683671951294
    },
    {
      "name": "Transformer",
      "score": 0.4822845757007599
    },
    {
      "name": "Graph",
      "score": 0.4756954610347748
    },
    {
      "name": "Salient",
      "score": 0.4476076662540436
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.44521364569664
    },
    {
      "name": "Feature extraction",
      "score": 0.4121040403842926
    },
    {
      "name": "Machine learning",
      "score": 0.16152936220169067
    },
    {
      "name": "Theoretical computer science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}