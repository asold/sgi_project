{
  "title": "MTLB-STRUCT @PARSEME 2020: Capturing Unseen Multiword Expressions Using Multi-task Learning and Pre-trained Masked Language Models",
  "url": "https://openalex.org/W3095813143",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224033554",
      "name": "Taslimipoor, Shiva",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282665490",
      "name": "Bahaadini, Sara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226617663",
      "name": "Kochmar, Ekaterina",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2123215530",
    "https://openalex.org/W2970425711",
    "https://openalex.org/W2888274736",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2468996921",
    "https://openalex.org/W2963706742",
    "https://openalex.org/W3037738964",
    "https://openalex.org/W2889275302",
    "https://openalex.org/W2224490803",
    "https://openalex.org/W2970095260",
    "https://openalex.org/W146666485",
    "https://openalex.org/W2162051953",
    "https://openalex.org/W2740726595",
    "https://openalex.org/W2030904529",
    "https://openalex.org/W2934590479",
    "https://openalex.org/W2948384082",
    "https://openalex.org/W2889356955",
    "https://openalex.org/W2803798246",
    "https://openalex.org/W2507263433",
    "https://openalex.org/W2619927796",
    "https://openalex.org/W2889563872"
  ],
  "abstract": "This paper describes a semi-supervised system that jointly learns verbal multiword expressions (VMWEs) and dependency parse trees as an auxiliary task. The model benefits from pre-trained multilingual BERT. BERT hidden layers are shared among the two tasks and we introduce an additional linear layer to retrieve VMWE tags. The dependency parse tree prediction is modelled by a linear layer and a bilinear one plus a tree CRF on top of BERT. The system has participated in the open track of the PARSEME shared task 2020 and ranked first in terms of F1-score in identifying unseen VMWEs as well as VMWEs in general, averaged across all 14 languages.",
  "full_text": "MTLB-STRUCT @PARSEME 2020: Capturing Unseen Multiword\nExpressions Using Multi-task Learning and Pre-trained Masked\nLanguage Models\nShiva Taslimipoor\nALTA Institute\nUniversity of Cambridge, UK\nst797@cl.cam.ac.uk\nSara Bahaadini\nMicrosoft\nSunnyvale, CA, USA\nsabahaa@microsoft.com\nEkaterina Kochmar\nALTA Institute\nUniversity of Cambridge, UK\nek358@cl.cam.ac.uk\nAbstract\nThis paper describes a semi-supervised system that jointly learns verbal multiword expressions\n(VMWEs) and dependency parse trees as an auxiliary task. The model beneﬁts from pre-trained\nmultilingual BERT. BERT hidden layers are shared among the two tasks and we introduce an\nadditional linear layer to retrieve VMWE tags. The dependency parse tree prediction is modelled\nby a linear layer and a bilinear one plus a tree CRF on top of BERT. The system has participated\nin the open track of the PARSEME shared task 2020 and ranked ﬁrst in terms of F1-score in\nidentifying unseen VMWEs as well as VMWEs in general, averaged across all 14 languages.\n1 Introduction\nIn addition to other challenges in multiword expression (MWE) processing that were addressed in pre-\nvious work, such as non-compositionality (Salehi et al., 2014), discontinuity (Rohanian et al., 2019;\nWaszczuk, 2018), and syntactic variability (Pasquer et al., 2018), The PARSEME shared task edition\n1.21 has focused on another prominent challenge in detecting MWEs, namely detection of unseen MWEs.\nThe problem with unseen data is common for many NLP tasks. While rule-based and unsupervised ML\napproaches are less affected by unseen data, supervised ML techniques are often found to be prone to\noverﬁtting. In this respect, the introduction of language modelling objectives to be added to different\nNLP tasks and their effect on generalisation have shown promising results (Rei, 2017). Further improve-\nments brought by pre-trained language models made them a popular approach to a multitude of NLP\ntasks (Devlin et al., 2019). One particular advantage of such models is that they facilitate generalisation\nbeyond task-speciﬁc annotations (Pires et al., 2019).\nMWEs are inherent in all natural languages and distinguishable for their syntactic and semantic id-\niosyncracies (Baldwin and Kim, 2010; Fazly et al., 2009). Since language models are good at capturing\nsyntactic and semantic features, we believe they are a suitable approach for modelling MWEs. In par-\nticular, our system relies on BERT pre-trained language models (Devlin et al., 2019). Additionally, we\nrender the system semi-supervised by means of multi-task learning. The most promising feature to be\njointly learned with MWEs is dependency parse information (Constant and Nivre, 2016). Accordingly,\nwe ﬁne-tune BERT for two different objectives: MWE detection and dependency parsing. MWE learn-\ning is done via token classiﬁcation using a linear layer on top of BERT, and dependency parse trees\nare learned using dependency tree CRF network (Rush, 2020). Our experiments conﬁrm that this joint\nlearning architecture is effective for capturing MWEs in most languages represented in the shared task.2\n2 Related Work\nIn earlier systems, MWEs were extracted using pre-deﬁned patterns or statistical measures that either\nindicated associations among MWE components or (non-)compositionality of the expressions with re-\ngard to the components (Ramisch et al., 2010). For example, Cordeiro et al. (2016) employed such a\nsystem for identifying MWEs. While these models can be effective for some frequent MWEs, their main\n1http://hdl.handle.net/11234/1-3367\n2The code for the system and conﬁguration ﬁles for different languages are available at https://github.com/\nshivaat/MTLB-STRUCT/\narXiv:2011.02541v1  [cs.CL]  4 Nov 2020\ndisadvantage is that they capture MWE types (as opposed to tokens) and they are unable to take context\ninto account in running texts.\nThe use of supervised machine learning was facilitated by the availability of resources tagged for\nMWEs (Schneider et al., 2014; Savary et al., 2017; Ramisch et al., 2018). Al Saied et al. (2017) proposed\na transition-based system based on an arc-standard dependency parser (Nivre, 2004) which ranked ﬁrst in\nthe ﬁrst edition of PARSEME shared task on automatic identiﬁcation of verbal MWEs (VMWEs) (Savary\net al., 2017). Taslimipoor and Rohanian (2018) proposed a CNN-LSTM system which exploited fastText\nword representations and ranked ﬁrst in the open track of the PARSEME shared task edition 1.1 (Ramisch\net al., 2018). Previous systems such as TRA VERSAL (Waszczuk, 2018) (ranked ﬁrst in the closed track\nof the PARSEME shared task edition 1.1), and CRF-Seq/Dep (Moreau et al., 2018) employed tree CRF\nusing dependency parse features in non-deep learning settings. They showed strengths of this approach\nparticularly in the case of discontinuous VMWEs. In SHOMA (Taslimipoor and Rohanian, 2018), using\na linear-chain CRF layer on top of the CNN-biLSTM model did not result in improvements. In this work,\nwe use tree CRF, implemented as part of the Torch-Struct library (Rush, 2020), to model dependency\ntrees, and we show that when it is jointly trained with a transformer-based MWE detection system, it\nimproves MWE prediction for a number of languages.\nRecently, Savary et al. (2019) proposed that learning MWE lexicons in an unsupervised setting is\nan important step that can be used in combination with a supervised model, especially when the latter is\ntrained on a small amount of data. While we do not speciﬁcally learn MWE lexicons from external unan-\nnotated data, we believe that state-of-the-art pre-trained language representation models can capture cru-\ncial information about MWEs similar to other NLP phenomena (Peters et al., 2017). For instance, Peters\net al. (2017) showed how a semi-supervised system may beneﬁt from pre-trained language model-based\nembeddings for named entity recognition (NER) and chunking. The joint learning of MWEs and depen-\ndency parsing has been proved effective in Constant and Nivre (2016). They proposed an arc-standard\ntransition-based system which draws on a new representation that has two linguistic layers (a syntactic\ndependency tree and a forest of MWEs) sharing lexical nodes. The closest to our work is Taslimipoor\net al. (2019) where they have trained a multi-task neural network which jointly learns VMWEs and de-\npendency parsing on a small English dataset and uses ELMo pre-trained embeddings. Our work here is\ndifferent in that we ﬁne-tune the BERT architecture and we use a tree CRF for dependency parsing.\n3 System Description\nWe use pre-trained BERT for language representation (Devlin et al., 2019) as the basis for our neural\nnetwork. The BERT architecture is based on standard transformers involving self-attention layers of\nencoders and decoders. 3 What makes it different from other transformer-based pre-trained language\nrepresentation models is its capability in encoding the representation in a bidirectional way through a\nmasked language model schema. The reason that we choose BERT among other pre-trained models is\nthe availability of multi-lingual pre-trained BERT.4\nOur model is set up to learn MWEs and dependency trees simultaneously. BERT weights are shared\namong the two tasks. A fully connected layer that performs sequence tagging is added as the ﬁnal\nlayer for MWE objective. Parallel to that, linear layers and a dependency CRF module are introduced\nto perform structured prediction for dependency trees. 5 The whole model is trained in an end-to-end\nmanner. Figure 1 depicts the overall architecture of the system.\nWe use Torch-Struct (Rush, 2020) for dependency parsing where Tree CRF is implemented as a dis-\ntribution object. We ﬁrst apply a linear followed by a bilinear layer on BERT’s output to obtain the\nadjacency matrix structure of the dependency tree. The outputs from these layers are considered as log-\npotentials (l) for the CRF distribution. The distribution takes in log-potentials and converts them into\nprobabilities CRF(z; l) of a speciﬁc tree z. We query the distribution to predict over the set of trees\n3There are 12 layers (transformer blocks) following the implementation ofhttp://nlp.seas.harvard.edu/2018/\n04/03/attention.html, with the hidden dimension size of 768 and 12 attention heads.\n4https://huggingface.co/bert-base-multilingual-cased\n5In this work, we only focus on dependency arcs (tree structures) and we do not model dependency relation labels.\nw1w2w3wn … BERT\nMWE tagger\nTree CRF \ndependency tagger\nT1\nT2\nTn\nsoftmax\n: Bilinear node\nFigure 1: The overall architecture of the multi-task learning model with two branches on top of BERT.\nOne is a linear classiﬁer layer for MWE tagging and the other consists of a linear layer, a bilinear layer\nand a tree CRF dependency tagger.\nusing argmaxzCRF(z; l). The cost for updating the tree is based on the difference between the tree\nprobability and the gold standard dependency arcs.\nThe MWE classiﬁcation layer is optimised by cross-entropy between the ground truth MWE tags and\nthe predicted ones, while the cost for CRF is estimated using log probabilities over the tree structures.\nNote that log probabilities ( logprobs) for CRF are large negative values which should be maximised,\nso we multiply them by −1 to get the dependency loss values compatible with MWE ones: Lossdep =\n−logprobs. The overall loss function to be optimised by ADAM optimiser is a linear combination of the\ntwo losses, Lossmwe and Lossdep which are the losses for multi-word expression and dependency parse\ntree, respectively, with αbeing a constant value which is empirically set to 0.001 ≤ α≤ 0.01.\nLoss= Lossmwe + α∗ Lossdep (1)\n4 Experiments\nWe adapted the sequential labelling scheme of Rohanian et al. (2019) which is similar to IOB with the\ndifference that it introduces a new soft label o- for the tokens that are in between components of an\nMWE. We preserved MWE categories by sufﬁxing the label with the category name. In this case, the\nannotations for the idiomatic verbal expression (of type VID) in the sentence I would give this job a\ngo, would be: I[O] would[O] give[B-VID] this[o-VID] job[o-VID] a[I-VID] go[I-VID] with the labels\nshown as subscripts in brackets. 6\nIn the development phase of the shared task, we trained various conﬁgurations of our system and\nevaluated the performance on development sets. Speciﬁcally, we examined the performance of our model\nin two settings: (1) the model is back-propagated only based on Lossmwe (single-task), and (2) the\nlearning is based on the multi-task Loss (multi-task). We decided on the setting to be used for each\nlanguage separately based on the performance on development sets. We used bert-base-multilingual-\ncased as the pre-trained model for all languages.7 Due to lack of time and resources, we did not perform\nany extensive hyper-parameter search. We empirically chose learning rate 3 × 10−5 and batch size\n10 (except for GA where the selected batch size is 1). We trained the models for 10 epochs, and the\nmaximum lengths of sentences for training were chosen for each language separately based on the word\npiece tokenisation of multilingual BERT.8\nTable 1 shows results on the development sets. According to the shared task criteria, we report MWE-\nbased precision, recall and F1 measures for all VMWEs and unseen ones in particular. We also consider\nthe scores on the expressions which are syntactic variants of their occurrences in the training data useful\nto be reported. We chose the best setting for each language based on F1 scores on unseen VMWEs (in\nbold). The systems marked by * (in Table 1) are trained after the evaluation period; therefore, their scores\non test are not available in the ofﬁcial evaluation report. In the multi-task setting we tried two αvalues:\n1\n300 and 1\n700 . We used the value that worked best for each language ( 1\n300 for EL, RO, SV and TR, and 1\n700\n6Embedded MWEs can be detected only if the nested MWE is not part of the nesting one and their categories are different.\n7We tried uncased multilingual models, for FR and PL in particular, but we didn’t observe any improvements.\n8When tokenisation splits words into multiple pieces, we took the prediction for the ﬁrst piece as the prediction for the word.\nWe masked the rest in the learning process.\nSystem Global MWE-based Unseen MWE-based Variant\nP R F1 P R F1 F1\nDE single-task (bert German cased) 79.45 75.28 77.31 53.00 53.00 53.00 90.32\nmulti-task (bert German cased) 74.81 75.66 75.23 46.61 55.00 50.46 90.91\nsingle-task (bert multilingual) 73.06 74.16 73.61 45.45 55.00 49.77 86.96\nEL single-task 70.28 72.06 71.16 37.01 47.00 41.41 81.98\nmulti-task 72.38 72.38 72.38 40.68 48.00 44.04 82.30\nEU single-task 76.82 78.54 77.68 28.10 43.00 33.99 81.20\nmulti-task 79.04 80.22 79.63 29.08 41.00 34.02 83.78\nFR single-task 83.17 79.06 81.06 48.57 50.50 49.51 87.05\nmulti-task 81.53 80.00 80.76 44.92 52.48 48.40 87.46\n*single-task (camembert) 79.61 85.41 82.41 45.32 62.38 52.50* 91.16\nGA single-task 26.15 13.49 17.80 16.07 9.00 11.54 32.00\nmulti-task 25.00 14.29 18.18 18.18 12.00 14.46 25.00\nHE single-task 52.76 40.36 45.73 23.94 16.67 19.65 73.91\nmulti-task 57.14 38.55 46.04 31.15 18.63 23.31 58.54\nHI single-task 71.78 62.90 67.05 50.55 46.00 48.17 83.12\nmulti-task 64.09 62.37 63.22 39.62 42.00 30.78 87.50\nIT single-task 70.53 62.04 66.01 32.35 32.67 32.51 76.02\nmulti-task 71.84 61.42 66.22 29.90 28.71 29.29 78.90\nPL single-task 83.22 81.72 82.46 42.24 49.00 45.37 91.51\nmulti-task 83.92 80.14 81.99 42.59 46.00 44.23 90.30\nPT single-task 78.82 74.06 76.36 33.64 36.00 34.78 87.85\nmulti-task 80.11 73.05 76.42 40.40 41.00 40.59 84.08\nRO single-task 91.41 85.82 88.52 39.13 36.00 37.50 85.39\nmulti-task 91.07 86.06 88.50 39.53 34.00 36.56 85.29\nSV single-task 68.99 65.93 67.42 39.83 47.00 43.12 77.92\nmulti-task 70.37 70.37 70.37 41.13 51.00 45.54 81.53\nTR single-task 62.59 69.75 65.98 37.41 52.00 43.51 68.23\nmulti-task 66.06 69.48 67.73 43.31 55.00 48.46 66.43\n*multi-task (+ extra data) 67.89 70.84 69.33 45.08 55.00 49.55* 70.79\nZH single-task 72.39 73.21 72.80 59.13 60.18 59.65 71.43\nmulti-task 72.45 72.45 72.45 60.36 59.29 59.82 71.43\n*single-task (bert Chinese cased) 73.14 78.11 75.55 61.07 70.80 65.57* 71.43\n*multi-task (bert Chinese cased) 70.92 75.47 73.3 50.68 65.49 62.45 80.00\nTable 1: Global, Unseen and Variant MWE-based scores on validation datasets.\nfor DE, EU, FR, GA, HE, HI, IT, PL, PT and ZH). The best model for each language was trained on both\ntrain and dev sets. The results obtained on test data are reported in Section 5.\nAfter the evaluation period, we also ﬁne-tuned the dependency-CRF branch of the model on some\nportions of extra data for several lower-resource languages (e.g.GA, HI, HE and TR). We saw no notable\nimprovement except forTR as reported in Table 1 (multi-taks + extra data). We only ﬁne-tuned the model\nto learn unlabeled trees for dependency arcs, which are made available for additional data as part of the\nshared task. Due to being limited by the amount of computational power, we only partially used the extra\nunannotated data; therefore we leave the experiments on their effects to future work.\n5 Results and Analysis\nTable 2 shows the summary results of our system MTLB-STRUCT on the test sets. For each language,\nwe report the employed system (single or multi-task), the ratio of unseen data in the test set, global and\nunseen MWE-based F1 scores, and ﬁnally the system’s rank ( #) in the open track of the shared task. 9\nOur system is applied to all 14 languages and achieves the highest F1 score overall.\n9More detailed results (including precision and recall values, and token-based performance measures) are available on the\nshared task web page: http://multiword.sourceforge.net/PHITE.php?sitesig=CONF&page=CONF_02_\nMWE-LEX_2020___lb__COLING__rb__&subpage=CONF_50_Shared_task_results\nLang System Global Unseen Lang System Global Unseen\nunseen % F1 F1 # unseen % F1 F1 #\nDE single 37% 76.17 49.34 1 IT single 29% 63.76 20.81 3\nEL multi 31% 72.62 42.47 1 PL single 22% 81.02 39.94 2\nEU multi 15% 80.03 34.41 1 PT multi 24% 73.34 35.13 1\nFR single 22% 79.42 42.33 2 RO single 7% 90.46 34.02 2\nGA multi 69% 30.07 19.54 1 SV multi 31% 71.58 42.57 1\nHE multi 60% 48.30 19.59 1 TR multi 26% 69.46 43.66 2\nHI single 45% 73.62 53.11 1 ZH multi 38% 69.63 56.2 2\nOverall - - 70.14 38.53 1\nTable 2: The percentage of unseen expressions (unseen %), and Global and Unseen MWE-based F1\nresults for all languages (Lang) in test. Column # indicates the ranking of our system in the shared task.\nThe amount of MWEs seen in the training data is the largest contributing factor, as the percentage of\nseen-in-train gold MWEs is highly linearly correlated ( r = 0.90) with the global MWE-based F1 score\nacross the languages. We achieve the highest performance in terms of MWE-based F1 score on unseen\ndata for 8 out of 14 languages, with the largest gaps in performance observed on PT, where our system\noutperforms Seen2Unseen by 21.59 points, and on HI, where the gap between our system’s F1 and\nthat of Seen2Unseen equals 10.45 points. We note that our system works signiﬁcantly better than the\nsecond best systems for smaller datasets ( GA, HE, and HI) which also happen to have larger amount of\nunseen expressions. At the same time, TRAVIS-mono outperforms our system on FR, IT, PL, TR, and\nZH, with the largest gap of 5.68 points observed on FR.\nIn addition, our system’s performance is balanced across continuous and discontinuous MWEs, with\nthe exceptions of HI and TR, where discontinuous MWEs amount to 7% and 4% of all MWEs, respec-\ntively, and our system’s performance drops by as much as 30 F1 points compared to its performance\non continuous MWEs. The distinction between multi- and single-token MWEs is only applicable to 3\nlanguages, on two of which (DE and SV) our system achieves an F1 score above 0.80 on single tokens.\nFinally, the shared task data shows a wide diversity of VMWE categories present in different lan-\nguages: from just three in EU and TR up to eight in IT. Once again, we note that our system is applicable\nto detection of all categories: for instance, it achieves the highest F1 scores among all systems in iden-\ntiﬁcation of LS.ICV, a rare language-speciﬁc category of inherently clitic verbs used only in Italian.\nAt the same time, we identify LVC.cause, light-verb constructions with the verb adding a causative\nmeaning to the noun, as the most problematic category on which our system achieves comparatively\npoorer results, especially on DE, EL, FR, HI, PT, and SV.\nIt is worth noting that no language speciﬁc feature is used in our system and the authors were not\ninvolved in the creation of the datatsets. Overall, we note that our system is not only cross-lingual, but\nalso robust in terms of its performance and is capable of generalising to unseen MWEs.\n6 Conclusions and Future Work\nWe described MTLB-STRUCT, a semi-supervised system that is based on pre-trained BERT masked\nlanguage modelling and that jointly learns VMWE tags and dependency parse trees. The system ranked\nﬁrst in the open track of the PARSEME shared task - edition 1.2 and shows the overall state-of-the-\nart performance for detecting unseen VMWEs. In future, we plan to augment the dependency parsing\narchitecture to train on dependency relation categories (labels) as well as dependency arcs. We also plan\nto improve our system by making it more efﬁcient in order to train the dependency parsing module on\nthe extra available unannotated datasets.\nAcknowledgments\nThis paper reports on research supported by Cambridge Assessment, University of Cambridge. We are\ngrateful to the anonymous reviewers for their valuable feedback. We gratefully acknowledge the support\nof NVIDIA Corporation with the donation of the Titan V GPU used in this research.\nReferences\nHazem Al Saied, Matthieu Constant, and Marie Candito. 2017. The ATILF-LLF system for PARSEME shared\ntask: a transition-based verbal multiword expression tagger. In Proceedings of the 13th Workshop on Multiword\nExpressions (MWE 2017), pages 127–132.\nTimothy Baldwin and Su Nam Kim. 2010. Multiword expressions. In Handbook of Natural Language Processing,\nsecond edition., pages 267–292. CRC Press.\nMatthieu Constant and Joakim Nivre. 2016. A transition-based system for joint lexical and syntactic analysis.\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 161–171.\nSilvio Cordeiro, Carlos Ramisch, and Aline Villavicencio. 2016. UFRGS&LIF at SemEval-2016 task 10: rule-\nbased mwe identiﬁcation and predominant-supersense tagging. In Proceedings of the 10th International Work-\nshop on Semantic Evaluation (SemEval-2016), pages 910–917.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.\nAfsaneh Fazly, Paul Cook, and Suzanne Stevenson. 2009. Unsupervised type and token identiﬁcation of idiomatic\nexpressions. Computational Linguistics, 35(1):61–103.\nErwan Moreau, Ashjan Alsulaimani, Alfredo Maldonado, and Carl V ogel. 2018. CRF-Seq and CRF-DepTree at\nPARSEME shared task 2018: Detecting verbal mwes using sequential and dependency-based approaches. In\nProceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-\nMWE-CxG-2018), pages 241–247.\nJoakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the workshop on\nincremental parsing: Bringing engineering and cognition together, pages 50–57.\nCaroline Pasquer, Agata Savary, Jean-Yves Antoine, and Carlos Ramisch. 2018. Towards a variability measure\nfor multiword expressions. In Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages\n426–432, New Orleans, Louisiana, June. Association for Computational Linguistics.\nMatthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence\ntagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1756–1765.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual bert? arXiv preprint\narXiv:1906.01502.\nCarlos Ramisch, Aline Villavicencio, and Christian Boitet. 2010. mwetoolkit: a framework for multiword ex-\npression identiﬁcation. In Proceedings of the Seventh International Conference on Language Resources and\nEvaluation (LREC’10).\nCarlos Ramisch, Silvio Cordeiro, Agata Savary, Veronika Vincze, Verginica Mititelu, Archna Bhatia, Maja Buljan,\nMarie Candito, Polona Gantar, V oula Giouli, et al. 2018. Edition 1.1 of the PARSEME shared task on automatic\nidentiﬁcation of verbal multiword expressions. In the Joint Workshop on Linguistic Annotation, Multiword\nExpressions and Constructions (LAW-MWE-CxG-2018), pages 222–240.\nMarek Rei. 2017. Semi-supervised multitask learning for sequence labeling. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2121–2130,\nVancouver, Canada, July. Association for Computational Linguistics.\nOmid Rohanian, Shiva Taslimipoor, Samaneh Kouchaki, Le An Ha, and Ruslan Mitkov. 2019. Bridging the gap:\nAttending to discontinuity in identiﬁcation of multiword expressions. arXiv preprint arXiv:1902.10667.\nAlexander Rush. 2020. Torch-Struct: Deep structured prediction library. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics: System Demonstrations, pages 335–342, Online,\nJuly. Association for Computational Linguistics.\nBahar Salehi, Paul Cook, and Timothy Baldwin. 2014. Detecting non-compositional MWE components using\nWiktionary. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1792–1797, Doha, Qatar, October. Association for Computational Linguistics.\nAgata Savary, Carlos Ramisch, Silvio Cordeiro, Federico Sangati, Veronika Vincze, Behrang Qasemizadeh, Marie\nCandito, Fabienne Cap, V oula Giouli, Ivelina Stoyanova, et al. 2017. The PARSEME shared task on automatic\nidentiﬁcation of verbal multiword expressions. In Proceedings of the 13th Workshop on Multiword Expressions\n(MWE 2017), pages 31–47.\nAgata Savary, Silvio Ricardo Cordeiro, and Carlos Ramisch. 2019. Without lexicons, multiword expression\nidentiﬁcation will never ﬂy: A position statement. In Joint Workshop on Multiword Expressions and WordNet\n(MWE-WN 2019), pages 79–91. Association for Computational Linguistics.\nNathan Schneider, Emily Danchik, Chris Dyer, and Noah A Smith. 2014. Discriminative lexical semantic seg-\nmentation with gaps: running the mwe gamut. Transactions of the Association for Computational Linguistics,\n2:193–206.\nShiva Taslimipoor and Omid Rohanian. 2018. SHOMA at Parseme shared task on automatic identiﬁcation of\nVMWEs: Neural multiword expression tagging with high generalisation. arXiv preprint arXiv:1809.03056.\nShiva Taslimipoor, Omid Rohanian, and Le An Ha. 2019. Cross-lingual transfer learning and multitask learning\nfor capturing multiword expressions. In Proceedings of the Joint Workshop on Multiword Expressions and\nWordNet (MWE-WN 2019), pages 155–161.\nJakub Waszczuk. 2018. TRA VERSAL at PARSEME shared task 2018: Identiﬁcation of verbal multiword ex-\npressions using a discriminative tree-structured model. In Proceedings of the Joint Workshop on Linguistic\nAnnotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 275–282.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8147902488708496
    },
    {
      "name": "Task (project management)",
      "score": 0.7129704356193542
    },
    {
      "name": "Dependency (UML)",
      "score": 0.6894655227661133
    },
    {
      "name": "Natural language processing",
      "score": 0.6811694502830505
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6515835523605347
    },
    {
      "name": "Parsing",
      "score": 0.6181672215461731
    },
    {
      "name": "Tree (set theory)",
      "score": 0.5635986924171448
    },
    {
      "name": "struct",
      "score": 0.5199947953224182
    },
    {
      "name": "Dependency grammar",
      "score": 0.4759901762008667
    },
    {
      "name": "Layer (electronics)",
      "score": 0.46171092987060547
    },
    {
      "name": "Bilinear interpolation",
      "score": 0.4210858941078186
    },
    {
      "name": "Speech recognition",
      "score": 0.3634330630302429
    },
    {
      "name": "Programming language",
      "score": 0.11152538657188416
    },
    {
      "name": "Mathematics",
      "score": 0.08145421743392944
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    }
  ],
  "institutions": []
}