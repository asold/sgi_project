{
    "title": "An Open-Domain Avatar Chatbot by Exploiting a Large Language Model",
    "url": "https://openalex.org/W4389010543",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2133695853",
            "name": "Takato Yamazaki",
            "affiliations": [
                "Line Corporation (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2136112484",
            "name": "Tomoya Mizumoto",
            "affiliations": [
                "Line Corporation (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2483094709",
            "name": "Katsumasa Yoshikawa",
            "affiliations": [
                "Line Corporation (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A4282473158",
            "name": "Masaya Ohagi",
            "affiliations": [
                "Line Corporation (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2129120489",
            "name": "Toshiki Kawamoto",
            "affiliations": [
                "Line Corporation (Japan)"
            ]
        },
        {
            "id": "https://openalex.org/A2127963183",
            "name": "Toshinori Sato",
            "affiliations": [
                "Line Corporation (Japan)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4322718191",
        "https://openalex.org/W2577267675",
        "https://openalex.org/W4229506649",
        "https://openalex.org/W3170186840",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2041113851"
    ],
    "abstract": "With the ambition to create avatars capable of human-level casual conversation, we developed an open-domain avatar chatbot, situated in a virtual reality environment, that employs a large language model (LLM). Introducing the LLM posed several challenges for multimodal integration, such as developing techniques to align diverse outputs and avatar control, as well as addressing the issue of slow generation speed. To address these challenges, we integrated various external modules into our system. Our system is based on the award-winning model from the Dialogue System Live Competition 5. Through this work, we hope to stimulate discussions within the research community about the potential and challenges of multimodal dialogue systems enhanced with LLMs.",
    "full_text": "Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pages 428–432\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n428\nAn Open-Domain Avatar Chatbot\nby Exploiting a Large Language Model\nTakato Yamazaki, Tomoya Mizumoto, Katsumasa Yoshikawa,\nMasaya Ohagi, Toshiki Kawamoto, Toshinori Sato\nLINE Corporation\n{takato.yamazaki, tomoya.mizumoto}@linecorp.com\nAbstract\nWith the ambition to create avatars capable of\nhuman-level casual conversation, we developed\nan open-domain avatar chatbot, situated in a vir-\ntual reality environment, that employs a large\nlanguage model (LLM). Introducing the LLM\nposed several challenges for multimodal inte-\ngration, such as developing techniques to align\ndiverse outputs and avatar control, as well as ad-\ndressing the issue of slow generation speed. To\naddress these challenges, we integrated various\nexternal modules into our system. Our system\nis based on the award-winning model from the\nDialogue System Live Competition 5. Through\nthis work, we hope to stimulate discussions\nwithin the research community about the po-\ntential and challenges of multimodal dialogue\nsystems enhanced with LLMs.\n1 Introduction\nWe present a demonstration of an open-domain\navatar dialogue system that we have developed,\nwith the goal of facilitating natural, human-like\nconversations. With the advent of large language\nmodel (LLM) technologies such as LLaMA (Tou-\nvron et al., 2023) and ChatGPT (OpenAI, 2022),\nthe fluency of text-based dialogue systems has sig-\nnificantly improved. One of the next directions\nin this field involves dialogue systems that utilize\nvoice, facial expressions, and gestures through an\navatar, contributing to a more engaging and inter-\nactive conversation experience (Hyde et al., 2015).\nAs part of the efforts in dialogue system research,\nthe Dialogue System Live Competition 5 (DSLC5)\nwas held in Japan, a competition of avatar chat di-\nalogue systems (Higashinaka et al., 2022). It was\nhosted within the academic conference of dialogue\nsystems, where a large number of researchers eval-\nuate the demonstrations performed on the stage to\ndetermine their ranking. We developed a dialogue\nsystem based on the LLM for this competition (Ya-\nmazaki et al., 2022), and encountered a variety of\n'PVOE\u0001VTFGVM\u0001EBUB\u0001\nGSPN\u0001,OPXMFEHF\u0001%#\u0001\nTFBSDI\n,OPXMFEHF\u000e3FTQPOTF\u0001\n1SPNQU\n'PVOE\u0001VTFGVM\u0001QFSTPOB\u0001\nXJUI\u0001WFDUPS\u0001TFBSDI\n1FSTPOB\u000e3FTQPOTF\u0001\n1SPNQU\n5PQJD\u000e1SPWJEJOH\u0001\n1SPNQU\n#PSJOH\u00013FTQPOTF\u0001\n%FUFDUPS\n$POWFSTBUJPO\u000e\u0001\n$MPTVSF\u0001%FUFDUPS\n4FOUFODF\u000e\"QQFOEJOH\u0001\n1SPNQU\n1SPIJCJUFE\u00018PSE\u0001\n%FUFDUPS\n3FHFOFSBUF\n3FHFOFSBUF\n/P\n/P\n%FUFDUFE\n%FUFDUFE\n1BTTFE\n1BTTFE\n%FUFDUFE\n1SPNQU\u000e$SFBUJPO 1PTU\u000e1SPDFTT\n455\u0001&SSPS\u0001\n3FDPWFSZ\u00011SPNQU\n%FUFDUFE\u0001FSSPS\u0001JO\u0001\nTQFFDI\u0001SFDPHOJUJPO\n:FT\n/P\n(FOFSBM\u00011SPNQU\n6TFS\u0001VUUFSBODF\u0001XBT\u0001\nDPOUFOU\u000eMFTT\n5PYJD\u00013FTQPOTF\u0001\n%FUFDUPS\n1BTTFE\n3FHFOFSBUF\n%FUFDUFE\n1BTTFE\n:FT\n:FT\n:FT\n3FHFOFSBUF\n--.\n--.\n/P\n%JBMPH\u00014ZTUFN\n&NPUJPO\u0001$MBTTJGJFS1VODUVBUJPOJ[FS\n.PUJPO &YQSFTTJPO\nΰʔϧσϯϋϜελʔ΋͔Θ͍͍Ͱ͢ΑͶʂ\u0001\n͋ͷେ͖ͳ໨͕ͨ·Γ·ͤΜɻ\n͍ͬͯͨΜͰ͢Ͷɻ\u0001\n͍ͬͯ·͢ɻ\n1SPOVODJBUJPO\u0001)FMQFS\n\t4P\r\u0001ZPV\u0001VTFE\u0001UP\u0001IBWF\u0001B\u00014ZSJBO\u0001IBNTUFS\u000f\u0001\n*\u0001IBWF\u0001B\u0001(PMEFO\u0001IBNTUFS\u000f\n \t(PMEFO\u0001IBNTUFST\u0001BSF\u0001BEPSBCMF\r\u0001\nBSFO\bU\u0001UIFZ \u0001*\u0001KVTU\u0001DBO\bU\u0001SFTJTU\u0001UIFJS\u0001\nCJH\u0001FZFT\u000f\n 554\n455\n6TFS 4ZTUFN\n1FSTPOB\u0001&YUSBDUPS\n1FSTPOB\u0001%#\n,OPXMFEHF\u0001%#\n1FSTPOB\u0001%# 5POF\u0001\n1SPNQU\u0001$SFBUJPO &EJUJOH\u0001\u0007\u0001'JMUFSJOH--.\n%JBMPHVF\u00014ZTUFN\n&NPUJPO\u0001\"OBMZ[FS1VODUVBUJPOJ[FS\n.PUJPO\n1SPOVODJBUJPO\u0001)FMQFS\n*\u0001MPWF\u0001USBWFMJOH\u0001BOE\u0001*\bWF\u0001CFFO\u0001UP\u0001WBSJPVT\u0001\nQMBDFT\u0001XJUIJO\u0001+BQBO\u0001EP\u0001ZPV\u0001\nMJLF\u0001USBWFMJOH\u0001:PUTVCB\u000eTBO\n:FT\u0002\u0001*\u0001SFDFOUMZ\u0001XFOU\u0001UP\u0001,ZPUP\u0002\u00018F\u0001WJTJUFE\u0001\n'VTIJNJ\u0001*OBSJ\u00015BJTIB\u000f\u00015IF\u0001UIPVTBOE\u0001\nUPSJJ\u0001HBUFT\u0001XFSF\u0001BNB[JOH\u000f\n 554455\n6TFS 4ZTUFN\n1FSTPOB\u0001&YUSBDUPS\n1FSTPOB\u0001%#\n5POF\u0001\n3FHFOFSBUF\u0001XIFO\u0001GJMUFSFE\n,OPXMFEHF\u0001%#\n4P\r\u0001ZPV\u0001VTFE\u0001UP\u0001IBWF\u0001B\u00014ZSJBO\u0001IBNTUFS\u000f\u0001\n*\u0001IBWF\u0001B\u0001(PMEFO\u0001IBNTUFS\u000f\n(PMEFO\u0001IBNTUFST\u0001BSF\u0001BEPSBCMF\u0002\u0001*\u0001KVTU\u0001\nDBO\bU\u0001SFTJTU\u0001UIFJS\u0001CJH\u0001FZFT\u0002\n4ZTUFN\n6TFS\n)J\r\u0001*`N\u0001:PUTVCB\u0002\u00018IBU\u0001TIBMM\u0001XF\u0001UBML\u0001\nBCPVU\u0001UPEBZ \nʜ\n(SFFUJOH\u0001.PUJPO\n)BQQZ\u0001.PUJPO\u0001\u0007\u0001&YQSFTTJPO\nʜ\n5IBU\bT\u0001UPVHI\u000f\u0001\"SF\u0001ZPV\u0001PLBZ \u0001*\u0001BMTP\u0001NBLF\u0001\nNJTUBLFT\u0001PGUFO\r\u0001TP\u0001*\u0001LOPX\u0001IPX\u0001ZPV\u0001GFFM\u000f\u000f\u000f\n4BE\u0001.PUJPO\u0001\u0007\u0001&YQSFTTJPO\n6TFS\n*\u0001NBEF\u0001B\u0001CJH\u0001NJTUBLF\u0001BU\u0001XPSL\u0001ZFTUFSEBZ\r\u0001\nBOE\u0001*\bN\u0001GFFMJOH\u0001B\u0001CJU\u0001EPXO\u000f\n&YQSFTTJPO\nVR\nVR\nFigure 1: Sample interactions between users and our\navatar chatbot, translated from Japanese. The avatar, ac-\ncessible via VR headset or display, exhibits its emotions\nthrough motions and expressive facial cues.\nchallenges on integrating LLMs into a multimodal\ndialogue system. For instance, due to the real-time\nnature of spoken dialogue, it is essential to return\nsome form of response quickly, which is a chal-\nlenge when using computationally intensive LLMs.\nFurthermore, when the system involves an avatar,\nmethods of controlling the avatar’s facial expres-\nsions and motions present another challenge.\nWe strive to address such missing capabilities\nof an LLM-based dialogue system by integrating\nseveral external modules. Such modules encom-\npass the incorporation of filler phrases and thinking\nmotions during LLM’s computational time, task\nparallelization to speed up responses, and detection\nof errors in speech recognition. Simultaneously,\nwe paid close attention to the content of dialogue,\naspiring to create a system that allows users to en-\ngage in deep, prolonged, and safe interactions. As\na result, our system achieved the best human evalu-\nation results in the competition. However, among\n429\nthe metrics, the naturalness of avatar’s speaking\nstyle received the lowest score, indicating a need\nfor improvement on its motions and expressions.\nIn this demonstration, we present an avatar di-\nalogue system that improves from DSLC5. The\nimprovement includes addition of emotion recog-\nnizer to enhance naturalness of the avatar expres-\nsions and motions. Additionally, aiming to pro-\nvide a more immersive dialogue experience, we\noffer a system that allows conversation with an\navatar through a virtual reality (VR) headset. Al-\nthough the system is originally designed to respond\nin Japanese, we provide translated responses for\nEnglish speakers. Through this demo, we hope to\nstimulate discussion within the research commu-\nnity about the potential and challenges of integrat-\ning LLMs into multimodal dialogue systems.\n2 System Overview\nWe first provide a overview of the features of our\nproposed system, followed by a more detailed ex-\nplanation in the subsequent subsections. The sys-\ntem architecture is illustrated in Figure 2.\nInitially, the user’s vocal input is transcribed into\ntext utilizing a speech-to-text (STT) module. As the\nSTT results frequently lack punctuation, a module\ncalled the Punctuationizer is utilized to append\nperiod marks and question marks. The punctuated\ntext is then fed into the Dialogue System.\nThe Dialogue System leverages an LLM to gen-\nerate responses. To elicit more engaging responses\nfrom the LLM, a process termed Prompt Creation\nis performed beforehand, providing the model with\ncontextually rich information. Following the re-\nsponse generation, an Editing & Filteringphase is\nundertaken. During this phase, any responses that\nare dull or ethically inappropriate are identified and\neither edited or discarded as necessary.\nOnce the response text is determined, the system\nproceeds to control the avatar and text-to-speech\nsystem (TTS). The avatar’s motions and expres-\nsions are decided based on the outcome of the\nEmotion Analyzer. To help accurately reading\nKanji characters of Japanese, we add Pronunci-\nation Helperto convert into the phonetic script of\nHiragana.\n2.1 Dialogue System: Prompt Creation\nIn the Prompt Creation phase, the system utilizes\ndifferent types of few-shot prompts based on the\ngiven user inputs. All prompts are created based on\na common template, which includes instructions\nsuch as the system character’s profile, the current\ndate and time, and the manner of speech. Here, we\nintroduce a few of the prompts that we employ in\nour system.\nSTT Error Recovery Prompt There is a risk\nof LLM generating unintended responses when it\nreceives user utterances containing STT errors. To\nmitigate this, we implemented an STT error de-\ntector based on fine-tuned BERT (Devlin et al.,\n2019) with dialogue breakdown detection data (Hi-\ngashinaka et al., 2016). In cases where errors are\ndetected, the system discards the user input and\nemploys a prompt to inform the LLM about the in-\naudibility of the received utterance. Figure 3 shows\nan example shot of this prompt.\nKnowledge-Response Prompt In cases where a\nuser engages in a deep conversation on a specific\ntopic, specialized knowledge or the latest informa-\ntion not included in the LLM’s parameters might be\nrequired. Moreover, it is empirically known that the\nmore niche a topic is, the more likely the LLM is to\ngenerate dull responses or only refer to well-known\ntopics. Thus, satisfying users who wish to delve\ninto more core conversations is challenging. To ac-\ncommodate this, we introduced a search system for\ntopic-related knowledge from online sources (e.g.\nWikipedia) and inserts them into the prompt. This\nprompt is triggered when an effective knowledge\nsource is found through searching the database.\nPersona-Response Prompt The input length for\nthe LLM has a limit, and it is currently difficult to\nincorporate all past dialogues as input. However,\nmaintaining memory of past dialogues is crucial\nto achieving consistent conversations with the user.\nOur system is designed to maintain memories dur-\ning dialogues by storing and utilizing the personas\nof the user and the system itself. After each ut-\nterance by both speakers, persona sentences are\nobtained using a Persona Extractormodule which\nis also implemented with the LLM. These persona\nsentences are stored in a vector database and uti-\nlized in the prompt during the response generation.\n2.2 Dialogue System: Editing & Filtering\nWhile the responses generated by the LLM is flu-\nent, they sometimes lead to dull or stagnant con-\nversation, or even prematurely end the dialogue,\nresulting in a loss of user interest. To circumvent\nthese issues, we have implemented a Boring Re-\n430\n'PVOE\u0001VTFGVM\u0001EBUB\u0001\nGSPN\u0001,OPXMFEHF\u0001%#\u0001\nTFBSDI\n,OPXMFEHF\u000e3FTQPOTF\u0001\n1SPNQU\n'PVOE\u0001VTFGVM\u0001QFSTPOB\u0001\nXJUI\u0001WFDUPS\u0001TFBSDI\n1FSTPOB\u000e3FTQPOTF\u0001\n1SPNQU\n5PQJD\u000e1SPWJEJOH\u0001\n1SPNQU\n#PSJOH\u00013FTQPOTF\u0001\n%FUFDUPS\n$POWFSTBUJPO\u000e\u0001\n$MPTVSF\u0001%FUFDUPS\n4FOUFODF\u000e\"QQFOEJOH\u0001\n1SPNQU\n1SPIJCJUFE\u00018PSE\u0001\n%FUFDUPS\n3FHFOFSBUF\n3FHFOFSBUF\n/P\n/P\n%FUFDUFE\n%FUFDUFE\n1BTTFE\n1BTTFE\n%FUFDUFE\n1SPNQU\u000e$SFBUJPO 1PTU\u000e1SPDFTT\n455\u0001&SSPS\u0001\n3FDPWFSZ\u00011SPNQU\n%FUFDUFE\u0001FSSPS\u0001JO\u0001\nTQFFDI\u0001SFDPHOJUJPO\n:FT\n/P\n(FOFSBM\u00011SPNQU\n6TFS\u0001VUUFSBODF\u0001XBT\u0001\nDPOUFOU\u000eMFTT\n5PYJD\u00013FTQPOTF\u0001\n%FUFDUPS\n1BTTFE\n3FHFOFSBUF\n%FUFDUFE\n1BTTFE\n:FT\n:FT\n:FT\n3FHFOFSBUF\n--.\n--.\n/P\n%JBMPH\u00014ZTUFN\n&NPUJPO\u0001$MBTTJGJFS1VODUVBUJPOJ[FS\n.PUJPO &YQSFTTJPO\nΰʔϧσϯϋϜελʔ΋͔Θ͍͍Ͱ͢ΑͶʂ\u0001\n͋ͷେ͖ͳ໨͕ͨ·Γ·ͤΜɻ\n͍ͬͯͨΜͰ͢Ͷɻ\u0001\n͍ͬͯ·͢ɻ\n1SPOVODJBUJPO\u0001)FMQFS\n\t4P\r\u0001ZPV\u0001VTFE\u0001UP\u0001IBWF\u0001B\u00014ZSJBO\u0001IBNTUFS\u000f\u0001\n*\u0001IBWF\u0001B\u0001(PMEFO\u0001IBNTUFS\u000f\n \t(PMEFO\u0001IBNTUFST\u0001BSF\u0001BEPSBCMF\r\u0001\nBSFO\bU\u0001UIFZ \u0001*\u0001KVTU\u0001DBO\bU\u0001SFTJTU\u0001UIFJS\u0001\nCJH\u0001FZFT\u000f\n 554\n455\n6TFS 4ZTUFN\n1FSTPOB\u0001&YUSBDUPS\n1FSTPOB\u0001%#\n,OPXMFEHF\u0001%#\n1FSTPOB\u0001%# 5POF\u0001\n1SPNQU\u0001$SFBUJPO &EJUJOH\u0001\u0007\u0001'JMUFSJOH--.\n%JBMPHVF\u00014ZTUFN\n&NPUJPO\u0001\"OBMZ[FS1VODUVBUJPOJ[FS\n.PUJPO\n1SPOVODJBUJPO\u0001)FMQFS\n*\u0001MPWF\u0001USBWFMJOH\u0001BOE\u0001*\bWF\u0001CFFO\u0001UP\u0001WBSJPVT\u0001\nQMBDFT\u0001XJUIJO\u0001+BQBO\u0001EP\u0001ZPV\u0001\nMJLF\u0001USBWFMJOH\u0001:PUTVCB\u000eTBO\n:FT\u0002\u0001*\u0001SFDFOUMZ\u0001XFOU\u0001UP\u0001,ZPUP\u0002\u00018F\u0001WJTJUFE\u0001\n'VTIJNJ\u0001*OBSJ\u00015BJTIB\u000f\u00015IF\u0001UIPVTBOE\u0001\nUPSJJ\u0001HBUFT\u0001XFSF\u0001BNB[JOH\u000f\n 554455\n6TFS 4ZTUFN\n1FSTPOB\u0001&YUSBDUPS\n1FSTPOB\u0001%#\n5POF\u0001\n3FHFOFSBUF\u0001XIFO\u0001GJMUFSFE\n,OPXMFEHF\u0001%#\n4P\r\u0001ZPV\u0001VTFE\u0001UP\u0001IBWF\u0001B\u00014ZSJBO\u0001IBNTUFS\u000f\u0001\n*\u0001IBWF\u0001B\u0001(PMEFO\u0001IBNTUFS\u000f\n(PMEFO\u0001IBNTUFST\u0001BSF\u0001BEPSBCMF\u0002\u0001*\u0001KVTU\u0001\nDBO\bU\u0001SFTJTU\u0001UIFJS\u0001CJH\u0001FZFT\u0002\n4ZTUFN\n6TFS\n)J\r\u0001*`N\u0001:PUTVCB\u0002\u00018IBU\u0001TIBMM\u0001XF\u0001UBML\u0001\nBCPVU\u0001UPEBZ \nʜ\n(SFFUJOH\u0001.PUJPO\n)BQQZ\u0001.PUJPO\u0001\u0007\u0001&YQSFTTJPO\nʜ\n5IBU\bT\u0001UPVHI\u000f\u0001\"SF\u0001ZPV\u0001PLBZ \u0001*\u0001BMTP\u0001NBLF\u0001\nNJTUBLFT\u0001PGUFO\r\u0001TP\u0001*\u0001LOPX\u0001IPX\u0001ZPV\u0001GFFM\u000f\u000f\u000f\n4BE\u0001.PUJPO\u0001\u0007\u0001&YQSFTTJPO\n6TFS\n*\u0001NBEF\u0001B\u0001CJH\u0001NJTUBLF\u0001BU\u0001XPSL\u0001ZFTUFSEBZ\r\u0001\nBOE\u0001*\bN\u0001GFFMJOH\u0001B\u0001CJU\u0001EPXO\u000f\n&YQSFTTJPO\nVR\nVR\nFigure 2: System Overview\nSystem: What have you been interested in lately?\nUser: Well, I've actually started playing various\ngames recently.\nSystem: That's exciting! I'm a big fan of games\nmyself. What kind of games are you into?\nUser: ...\nSystem: (Hmm, I didn't quite catch the user's\nresponse...) Can you tell me about some other\ngames you enjoy playing?\nFigure 3: Example shot of an STT Error Recovery\nPrompt. The actual response is generated after the paren-\ntheses of the last utterance.\nsponse Filterand a Conversation-Closure Filter.\nBoth filters operate by identifying responses that\nare similar to manually collected boring and clos-\ning expressions. If either of these filters flags a\nresponse, the system will either add sentences us-\ning the LLM to enrich the content or revert back to\nthe Prompt Creation stage for regeneration.\nThere’s also a risk of generating ethically inap-\npropriate responses, making it unsafe to directly\nprovide the LLM’s outputs to the user. In order\nto achieve communication that is both secure and\nrespectful, we implemented Toxic-Response Filter.\nIt utilizes a classifier that has been fine-tuned with\nBERT using a Japanese harmful expression dataset\n(Kobayashi et al., 2023). In case where these filters\nflag a response, the system adds suitable instruc-\ntions to the prompt (e.g. “respond gently” or “ex-\npand on the topic”) and again reverts back to the\nPrompt Creation to regenerate.\n2.3 Response Timing\nIn spoken dialogues, promptly signaling under-\nstanding of the user’s speech is considered cru-\ncial for enabling a comfortable conversation. How-\never, the LLMs necessitate significant computa-\ntional time, requiring approximately 2 seconds for\neach generation in case of our system. This over-\nhead can lead to response delays, especially when\nregeneration is necessary.\nTo mitigate these potential sources of discomfort,\nour system employs concurrent operation of mul-\ntiple modules. For instance, we execute persona\nextraction while the system is speaking, enabling it\nto expedite response times. Additionally, we have\nintegrated the use of conversational fillers and an-\nimated motions during these waiting periods. By\nincorporating these elements, we aim to make the\ndelay less noticeable and align with the natural flow\nof human conversation.\n2.4 Emotion Analyzer and Avatar Control\nIn avatar dialogue systems, it is important to con-\ntrol the tone of the synthesized voice, as well as\nthe avatar’s facial expressions and motions, in ac-\ncordance with the content of the utterance. Our\nsystem performs emotion analysis on the responses\ngenerated by the Dialogue System to manage these\naspects. The analyzer employs a fine-tuned BERT\ntrained on the WRIME dataset (Kajiwara et al.,\n2021). It recognizes eight types of emotions,\nnamely joy, sadness, anticipation, surprise, anger,\nfear, disgust, and trust, on a strength scale ranging\nfrom 0 to 3.\nOur in-house developed TTS system called Co-\nharis can assign emotions such as “Happy” and\n“Sad”, and these are mapped with the output of the\nemotion analyzer. Similarly, the expressions and\nmotions of the avatar is also controlled based on the\nresults of emotion analyzer. The avatar displayed\nin our demonstration is a sample model provided\n431\nby the VRoid Hub1 service from pixiv Inc. This\nsample model comes with a variety of facial expres-\nsions such as “happy,” “sad,” “angry,” and so forth.\nMoreover, Mixamo 2 by Adobe Inc. offers a wide\nvariety of avatar motions, which also is utilized for\nexpressing emotions.\n2.5 User Interface\nThe user interface operates on a web browser. We\nuse the WebSpeechAPI of the browser for STT. The\navatar is displayed by controlling WebGL through\na library called three.js 3. We also provide a VR\ninterface using WebXR, allowing it to be displayed\nthrough a browser inside a VR headset.\n3 Evaluation\nWe present the results of our original system ob-\ntained during the final round of DSLC5. It is im-\nportant to note that the avatar and TTS used in the\ncompetition were provided by the organizers, and\nthus differ from the demonstration presented here.\nFurthermore, the evaluated system does not incor-\nporate avatar control based on the emotion analyzer,\nso the evaluation results should be considered as\nreference values. For the LLM, we utilized Hyper-\nCLOV A 82B (Kim et al., 2021), which was trained\non a Japanese corpus.\nIn the competition, the system engaged in a four-\nminute casual conversation in Japanese with an\navatar about two topics, after which the dialogue\nwas evaluated through a post-conversation survey.\nThe survey was conducted based on four aspects:\n“Natural: whether the dialogue was natural”, “Fol-\nlow: whether the system could respond appropri-\nately to the topic”, “Providing: whether the system\ncould provide new information related to the topic”,\nand “Style: whether the way of speaking, demon-\nstrated through voice, gestures, and facial expres-\nsions, was natural”. These evaluation criteria were\nset by the competition organizers as metrics for\nassessing “human-level conversational ability”. A\nfive-point Likert scale was used for the evaluation.\nTable 1 presents a comparison of our system\nwith the second-ranked system, which is a dialogue\nsystem leveraging a 1B-params language model\n(Moriya et al., 2022). Our system significantly out-\nperforms the second-ranked system in three met-\nrics: Natural, Follow, and Providing, indicating\n1https://hub.vroid.com/\n2https://www.mixamo.com/\n3https://threejs.org/\nNatural Follow Providing Style Total\n2nd 2.60 2.52 2.87 3.06 2.76\nOurs 4.00 4.00 3.89 3.54 3.86\nTable 1: Human evaluation results of the finals of\nDSLC5. “2nd” indicates the second place team.\nSpeaker Utterance\nUser Oh I see, you had a Djungarian hamster. I have\na golden hamster.\nSystem Golden hamsters are also cute. I can’t resist\ntheir big eyes. (Joy: 2)\nUser Yes, their eyes are really cute.\nSystem They’re so adorable, it makes me want to have\none again. By the way, have you named it?\n(Joy: 1 / Ant.: 1)\nUser Its name is Uchiko.\nSystem Oh, Uchiko, huh!? That’s a cool name! How\ndo your hamsters usually spend their time?\n(Joy: 1 / Sur.: 1)\nUser They usually live in their cage.\nSystem I see, indeed, it would be full of dangers if\nthey went outside. I wonder if they don’t get\nstressed just staying inside the house. (Fear: 3)\nTable 2: A sample dialogue translated from Japanese\nduring the preliminary stage, with the output of the\nEmotion Analyzer in the parentheses. “Ant.” and “Sur.”\ncorresponds to anticipation and surprise, respectively.\nThe number represents the strength of the emotion (0-3).\nhigh performance in dialogue content. As seen in\nthe dialogue examples shown in Table 2, our sys-\ntem successfully follows and expands on topics.\nHowever, the Style score is notably lower than the\nother metrics, indicating the need for further en-\nhancements in terms of avatar motions. We expect\nimprovements with the Emotion Analyzer, as in-\ndicated by the displayed output results in Table 2.\nThe emotion labels are accurately assigned, sug-\ngesting that they can be applied effectively to the\navatar’s facial expressions and motions.\n4 Conclusion\nIn conclusion, we developed an open-domain\navatar chatbot in a VR environment, leveraging a\nlarge language model (LLM). While encountering\nchallenges in multimodal integration, such as ad-\ndressing slow generation speed and controlling the\navatar, our system demonstrated promising results\nin Dialogue System Live Competition 5. Addition-\n432\nally, we attempted to improve the unnaturalness in\nthe avatar’s style of speaking, which was discov-\nered after the competition, by using the emotion\nanalyzer. We anticipate that this work will initiate\nmeaningful discussions among the research com-\nmunity regarding the potential and challenges of in-\ntegrating LLM into multimodal dialogue systems.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings\nof the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nRyuichiro Higashinaka, Kotaro Funakoshi, and Michimasa\nInaba. 2016. The dialogue breakdown detection challenge:\nTask description, datasets and evaluation metrics. In Proc.\nof The Tenth International Conference on Language Re-\nsources and Evaluation.\nRyuichiro Higashinaka, Tetsuro Takahashi, et al. 2022. Di-\nalogue system live competition 5 (in japanese). In JSAI\nSIG-SLUD, 96th Meeting, page 19. The Japanese Society\nfor Artificial Intelligence.\nJennifer Hyde, Elizabeth J Carter, et al. 2015. Using an in-\nteractive avatar’s facial expressiveness to increase persua-\nsiveness and socialness. In Proceedings of the 33rd Annual\nACM Conference on Human Factors in Computing Systems,\npages 1719–1728.\nTomoyuki Kajiwara, Chenhui Chu, Noriko Takemura, Yuta\nNakashima, and Hajime Nagahara. 2021. WRIME: A new\ndataset for emotional intensity estimation with subjective\nand objective annotations. In Proceedings of the 2021\nConference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language\nTechnologies, pages 2095–2104, Online. Association for\nComputational Linguistics.\nBoseop Kim, HyoungSeok Kim, et al. 2021. What changes\ncan large-scale language models bring? intensive study on\nHyperCLOV A: Billions-scale Korean generative pretrained\ntransformers. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pages\n3405–3424, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nKoga Kobayashi, Takato Yamazaki, et al. 2023. Proposal\nand evaluation of japanese harmful expression schema (in\njapanese). In Proceedings of the 29th Annual Conference\nof the Association for Natural Language Processing. Asso-\nciation for Natural Language Processing.\nShoji Moriya, Daiki Shiono, et al. 2022. aoba_v3 bot: A\nmulti-modal chit-chat dialogue system integrating diverse\nresponse generation models and rule-based approaches (in\njapanese). In JSAI SIG-SLUD, 96th Meeting. Japanese\nSociety for Artificial Intelligence.\nOpenAI. 2022. Introducing ChatGPT. https://openai.\ncom/blog/chatgpt.\nHugo Touvron, Thibaut Lavril, et al. 2023. Llama: Open\nand efficient foundation language models. arXiv preprint\narXiv:2302.13971.\nTakato Yamazaki, Toshiki Kawamoto, et al. 2022. An\nopen-domain spoken dialogue system using hyperclova\n(in japanese). In JSAI SIG-SLUD, 96th Meeting. Japanese\nSociety for Artificial Intelligence."
}