{
  "title": "Investigation of the structure-odor relationship using a Transformer model",
  "url": "https://openalex.org/W4313331689",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2105804749",
      "name": "Xiaofan Zheng",
      "affiliations": [
        "Kyushu University"
      ]
    },
    {
      "id": "https://openalex.org/A362757229",
      "name": "Yoichi Tomiura",
      "affiliations": [
        "Kyushu University"
      ]
    },
    {
      "id": "https://openalex.org/A2191732671",
      "name": "Kenshi Hayashi",
      "affiliations": [
        "Kyushu University"
      ]
    },
    {
      "id": "https://openalex.org/A2105804749",
      "name": "Xiaofan Zheng",
      "affiliations": [
        "Kyushu University"
      ]
    },
    {
      "id": "https://openalex.org/A362757229",
      "name": "Yoichi Tomiura",
      "affiliations": [
        "Kyushu University"
      ]
    },
    {
      "id": "https://openalex.org/A2191732671",
      "name": "Kenshi Hayashi",
      "affiliations": [
        "Kyushu University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2953273762",
    "https://openalex.org/W2527189750",
    "https://openalex.org/W2914757825",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W3152893301",
    "https://openalex.org/W2925177113",
    "https://openalex.org/W3133900975",
    "https://openalex.org/W2589330732",
    "https://openalex.org/W2761485135",
    "https://openalex.org/W2290847742",
    "https://openalex.org/W3092204459",
    "https://openalex.org/W4205765055",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6678156880",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W3030978062",
    "https://openalex.org/W2900090807",
    "https://openalex.org/W2507164429",
    "https://openalex.org/W3100157108",
    "https://openalex.org/W3103523530"
  ],
  "abstract": null,
  "full_text": "Zheng et al. Journal of Cheminformatics (2022) 14:88 \nhttps://doi.org/10.1186/s13321-022-00671-y\nRESEARCH\n© The Author(s) 2022, corrected publication 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 \nInternational License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To \nview a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver \n(http:// creat iveco mmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a \ncredit line to the data.\nOpen Access\nJournal of Cheminformatics\nInvestigation of the structure-odor \nrelationship using a Transformer model\nXiaofan Zheng1*, Yoichi Tomiura1 and Kenshi Hayashi2 \nAbstract \nThe relationships between molecular structures and their properties are subtle and complex, and the properties of \nodor are no exception. Molecules with similar structures, such as a molecule and its optical isomer, may have com-\npletely different odors, whereas molecules with completely distinct structures may have similar odors. Many works \nhave attempted to explain the molecular structure-odor relationship from chemical and data-driven perspectives. The \nTransformer model is widely used in natural language processing and computer vision, and the attention mechanism \nincluded in the Transformer model can identify relationships between inputs and outputs. In this paper, we describe \nthe construction of a Transformer model for predicting molecular properties and interpreting the prediction results. \nThe SMILES data of 100,000 molecules are collected and used to predict the existence of molecular substructures, and \nour proposed model achieves an F1 value of 0.98. The attention matrix is visualized to investigate the substructure \nannotation performance of the attention mechanism, and we find that certain atoms in the target substructures are \naccurately annotated. Finally, we collect 4462 molecules and their odor descriptors and use the proposed model to \ninfer 98 odor descriptors, obtaining an average F1 value of 0.33. For the 19 odor descriptors that achieved F1 values \ngreater than 0.45, we also attempt to summarize the relationship between the molecular substructures and odor \nquality through the attention matrix.\nKeywords Molecular structure-odor relation, Transformer model, Odor descriptor\nIntroduction\nSmell plays an important role in all aspects of life and is \nthus an important property of all compounds. The rela -\ntionship between molecular structure and odor quality is \nan essential research topic. Studies on this relationship \nmay lead to predictions of the odor of a molecule, odor \nsynthesis, and even the artificial synthesis of molecules \nwith specific odors. However, studying the odors of dif -\nferent substances is challenging. A previous study [1] \nshowed that molecules with similar structures may have \nvery different odors, while molecules with similar odors \nmay have completely distinct structures. In addition \nto the subtle relationship between molecular structure \nand odor, aspects such as sex, age, and disease history \ncan affect odor perception. Therefore, special training is \nrequired to label the odors of substances, which increases \nthe difficulty of labeling the odors of chemical com -\npounds. Thus, to date, the relationship between molecu -\nlar structure and odor remains difficult to specify.\nMachine learning has been applied in a wide range \nof fields, including physics and chemistry, and various \nmolecular structure property prediction methods have \nbeen proposed [2–5]. These methods can be divided into \nfeature-based methods and feature-free methods accord -\ning to the type of data that are input into the model. Fea -\nture-based methods take the generated fixed molecular \nfeatures (such as molecular fingerprints and molecular \nparameters) as model inputs and use various algorithms \n*Correspondence:\nXiaofan Zheng\nzheng.xiaofan.413@s.kyushu-u.ac.jp\n1 Graduate School of Information Science and Electrical Engineering, \nDepartment of Informatics, Kyushu University, Fukuoka, Japan\n2 Graduate School of Information Science and Electrical Engineering, \nDepartment of Electronics, Kyushu University, Fukuoka, Japan\nPage 2 of 16Zheng et al. Journal of Cheminformatics (2022) 14:88\n(e.g., random forest and support vector machines) to \npredict the molecular properties. Feature-free methods \npredict specific molecular properties by automatically \nextracting molecule features that are related to those \nproperties using methods such as graph neural networks \n[6] or graph kernels [7 ]. In addition to predicting molec -\nular properties such as water solubility and lipophilicity, \nfeature-free methods use artificial neural networks to pre-\ndict additional essential properties, such as the molecular \nenergy, dipole moment and molecular dynamics [8 , 9], \nallowing us to compute this information faster than using \ncomputational chemistry methods. In molecular property \nprediction, the interpretability of the model is particu -\nlarly important [10], as model interpretability allows us \nto investigate the relationship between molecular struc -\nture and different properties at the molecular, atomic, and \nsubatomic levels. Although feature-based methods use \nfixed features, the resulting model usually provides some \ninterpretability. In contrast, feature-free methods flexibly \nextract features according to the properties to be pre -\ndicted; however, the models are not often interpretable. \nTherefore, we aim to develop a feature-free method that \nallows interpretation of the extracted features.\nAt present, approximately 4000 odorants have been \nlabeled with their corresponding odor. The smells of \nodorants have been labeled with odor descriptors (ODs), \nsuch as ‘sweet, ’ ‘fruity, ’ and ‘green. ’ These data intro -\nduce the possibility of using data-driven approaches in \nmolecular structure-odor studies. Several studies have \nused machine learning methods for OD prediction. For \nexample, Keller et al. [11] used molecular parameters to \npredict the scores of 19 kinds of odors, achieving a cor -\nrelation coefficient of 0.55. In contrast to most studies \non OD prediction, this study attempted to predict scores \ncorresponding to ODs through regression rather than \nclassification, making it difficult to compare the results \nwith those of the studies mentioned below. Shang et  al. \n[12] predicted 10 ODs using molecular parameters, \nachieving an F1 value greater than 0.8. However, data \naugmentation was applied by synthesizing similar data \npoints based on the original dataset before dividing the \ndataset into training and test sets. Therefore, the test set \nwas essentially contaminated. Sanchez-Lengeling et  al. \n[13] combined two datasets and predicted 138 ODs using \na graph neural network (GNN) [3, 14], with the previous \noutput layer applied to cluster the ODs. Although the \naverage F1 value was 0.36, the clustering results showed \nthat the outputs in the last layer were closer to each other \nwhen the corresponding molecules were labeled with \nODs in similar categories. Chacko et  al. [15] used the \nsame dataset as Keller et al. [11] to predict the pleasant -\nness and intensity of odors, as well as two ODs (sweet \nand musky). The corresponding F1 values of the two ODs \non the test set were 0.84 and 0.69. The dataset used by \nChacko et al. contained 480 samples, and the ratio of the \ntraining set to the test set was 9:1. Thus, the results may \nnot be stable because of the small number of samples in \nthe test dataset. Debnath and Nakamoto [16] predicted \nthree ODs (fruity, green, and sweet) using the mass spec -\ntra of different molecules and achieved an average F1 \nvalue of 0.51.\nIn recent years, the Transformer model has been \nwidely used in image processing [17, 18] and natural lan-\nguage processing [19, 20] because of its flexible atten -\ntion mechanism. In addition to processing sentences and \nimages, the Transformer model can take more flexible \ninput forms (such as graphs) by using relative positional \nembedding [21, 22]. In terms of interpretability, the \nTransformer model results can naturally be interpreted \naccording to its attention mechanism. Several Trans -\nformer models for molecular property prediction have \nbeen developed in recent years. Karpov et  al. [23] used \nthe SMILES data of molecules in the form of strings as the \nmodel input and predicted various molecular properties, \nsuch as the melting and boiling points. When molecules \nare represented in nonstring forms, the relative posi -\ntional information between atoms must be used as one of \nthe inputs to the model. Maziarka et al. [24, 25] predicted \nmolecular properties by adding the relative positional \ninformation of the atoms to the attention matrix, and \nMaziarka et al. [26] used carefully designed functions to \nexpress the positional relationship between atoms based \non Maziarka et  al. [24]. Both of these works interpret \nthe model by visualizing the attention mechanism in the \nencoder. Hutchinson et al. [27] and Thölke [28] predicted \nseveral more essential properties, such as the molecular \nenergy, dipole moment, and molecular dynamics. They \nnot only used carefully designed functions to express the \npositional relationship between atoms but also computed \nthe outputs according to a more physical approach. For \nexample, they predicted the atomic forces by comput -\ning the derivative of the predicted atomic energies with \nrespect to the relative position.\nIn this research, we adopt a feature-free method and \nuse the Transformer model to predict ODs. We first pre -\ndict the existence of molecular substructures using the \nTransformer model and then evaluate the performance \nof the attention mechanism in terms of model interpret -\nability by visualizing the attention matrix. Finally, we use \nthe model to predict ODs and visualize the attention \nmatrices.\nThe main contributions of this study can be summa -\nrized as follows:\n• We finetune a Transformer model for predicting \nmolecular properties and interpreting the results.\nPage 3 of 16\nZheng et al. Journal of Cheminformatics (2022) 14:88\n \n• Experiments are conducted to predict the existence \nof various substructures and to investigate the inter -\npretability of the attention mechanism in the Trans -\nformer model.\n• The developed Transformer model is used to predict \nODs, and the attention matrix is visualized to iden -\ntify OD structural features.\nMethods and experiment\nModel\nThe original Transformer model [19] was developed for \nmachine translation and consists of an encoder and a \ndecoder. Each layer in the encoder contains one attention \nmodule, which can be regarded as a self-attention mech -\nanism through which each word in the input sentence \ninteracts with related words. Each layer in the decoder \ncontains two attention modules. The first attention mod -\nule is also a self-attention mechanism that enables the \nword that is currently being translated to communicate \nwith other translated words. The second attention mod -\nule is used to obtain information about the source lan -\nguage for the current word.\nA sentence is considered as a sequence of words. By \nadding position information as a positional embedding \nto the embedding of the input word, the original Trans -\nformer can consider the word order. Molecules are three-\ndimensional (3D) structures that are composed of atoms. \nThe relationship between atoms in a molecule cannot be \nrepresented by the positional embeddings used in the \noriginal Transformer because the bonds between atoms \nmust be represented.\nThe Molecular Attention Transformer (MAT) model \n[24] was developed to predict molecular properties such \nas water solubility and blood-brain barrier penetration. \nThe MAT model provides a creative solution for identify-\ning the relationship between atoms. As shown on the left \nside of Fig. 1, the MAT model replaces positional embed-\nding by adding adjacency and distance matrices to the \nattention matrix. The attention mechanism in the MAT \nmodel is formulated as\nwhere /afii98381 , /afii98382 , and /afii98383 are hyperparameters; Q, K, and V are \nthe query matrix, key matrix, and value matrix (as in the \noriginal Transformer); D and A are the distance matrix \nand adjacency matrix, respectively; and g(d) = exp(−d) \nis an elementwise function.\nIn this study, we propose a model based on the origi -\nnal Transformer and MAT models. We do not use more \ncomplex interatomic distance formulas or more distant \nneighborhood information as the direct inputs to the \nmodel, as used by Maziarka et al. [26]. Instead, we expect \nthe model to automatically learn more complex distance \nand adjacency relationships through multiple heads and \nmultiple encoder layers. The key features of the proposed \nmodel can be summarized as follows: (1) changes the \nattention calculation; (2) adds a decoder-like structure to \nthe model to improve interpretability; and (3) introduces \na contrastive loss function to the model.\nIn the MAT model, attention is calculated by sum -\nming the inner product between the atom attributes, \nadjacency matrix, and distance matrix. According to \nEq. (1), if the inner product between two atoms is large \nand these two atoms are far away from each other, infor -\nmation is exchanged between the two atoms, which is \n(1)\nAttention=\n(\n/afii98381 softmax\n(\nQK T\n√\ndk\n)\n+ /afii98382 g(D ) + /afii98383 A\n)\nV ,\nFig. 1 The details of the encoder and decoder-like modules are shown on the left and right. Encoder: The inputs to the encoder are embedded \natomic features (embedded af), the adjacency matrix, and the distance matrix. The outputs of the encoder are atomic attributes (attribute af). \nDecoder-like module: the inputs are vectors (embedded cls) corresponding to different outputs from the encoder\nPage 4 of 16Zheng et al. Journal of Cheminformatics (2022) 14:88\nunreasonable. In this study, we change Eq. (1) to the fol -\nlowing equation:\nQ adj is obtained by a linear transformation of the input to \nthe encoder layer ( Qadj= XW Q\nadj , where X is the input to \nthe encoder layer and W Q\nadj is a learnable parameter); \nQ dist , K adj , K dist , V adj , and V dist can be obtained in the \nsame way. On the basis of Eq. (2), message passing \nbetween two atoms based on their inner product value \noccurs only when the atoms are connected by a chemical \nbond or the atoms are close to each other.\nIn the MAT model, the output of the encoder is directly \npassed through a pooling layer before the molecular \nproperties are predicted by the fully connected layers. We \nadd a decoder-like module, similar to the original Trans -\nformer, to visualize the relationship between the atoms \nand outputs. The proposed model is shown in Fig.  2. In \nnatural language, the words in a sentence are related to \neach other. However, in most cases, ODs are not neces -\nsarily related to each other. Therefore, we use a decoder-\nlike module, namely, the Transformer decoder without \nthe self-attention mechanism, as shown on the right side \nof Fig. 1. The output of the transformer encoder is trans -\nmitted to this decoder-like module. As shown in Fig.  2, \nthe input to the decoder-like module is embedded cls i , \nwhich is obtained by passing a scalar of value 1 through \na single fully connected network. Thus, embedded cls i is a \n(2)\nAttention=\n(\nsoftmax\n(Q adjK T\nadj√\ndk\n)\n⊙ A\n)\nV adj\n+\n(\nsoftmax\n(\nQ distK T\ndist√\ndk\n)\n⊙ g(D )\n)\nV dist,\nlearnable input for target i . The attention in the decoder-\nlike module is computed by considering embedded cls i \nas the query and the outputs of the encoder as the key \nand value. This attention mechanism in the decoder-\nlike module is expected to obtain better predictions by \nemphasizing atoms that are related to the molecular \nproperties, thereby enabling the visualization of impor -\ntant substructures that affect the prediction results.\nThe contrastive loss function has been widely used in \nself-supervised learning in recent years [29, 30]. The \napplication of the contrastive loss to supervised learning \n[31] can also improve model performance. We directly \napply this contrastive loss to our model, and the defini -\ntion of the loss function is shown in Eq. (3).\nP(i) is a set that includes all samples whose labels are the \nsame as sample i, |•|  is a function that counts the number \nof elements in a set, zi is a feature vector with unit length \ncorresponding to sample i, A(i) is a set that includes all \nsamples in the batch except sample i, and τ is a hyper -\nparameter. The contrastive loss function brings feature \nvectors of samples with the same label closer while sepa -\nrating feature vectors of samples with different labels.\nExperiment\nWe conducted two experiments in this research. The \nfirst experiment aimed to predict whether the input mol -\necule has some specific substructure using our proposed \nTransformer model. The second experiment predicted \nODs using the proposed Transformer model.\n(3)\nLcontrastive=\n∑\ni∈batch\nLi =\n∑\ni∈batch\n−1\n|P(i)|\n∑\np∈P (i)\nlog exp(zizp /τ )∑\na∈A(i) exp(ziza /τ )\nFig. 2 Proposed model: the inputs are atomic features (af), there are multiple decoder-like modules for different targets, and there is a single fully \nconnected layer between attribute cls and the targets\nPage 5 of 16\nZheng et al. Journal of Cheminformatics (2022) 14:88\n \nWe used two different datasets for these two experi -\nments. We collected SMILES data for 100,000 molecules \nfrom ChEMBL [32] for the substructure predictions. The \nChEMBL database is a bioactive dataset covering more \nthan 2 million compounds, which ensures that we have \nsufficient data for substructure prediction. For the OD \nprediction experiment, we collected 4,240 odorants and \ntheir corresponding ODs from TheGoodScentsCompany \n[33]. Among the datasets that provide OD labels, The -\nGoodScentsCompany provides more data that is easier \nto obtain. In addition to odorants, we collected 222 mol -\necules that were annotated as odorless from TheGood -\nScentsCompany. RDKit with default settings was used \nto compute the atomic properties, adjacency matrices, \nand distance matrices of all molecules. For both datasets, \nwe removed molecules for which the distance matrix \ncould not be calculated and molecules with more than 60 \natoms. Finally, 98,324 and 4,365 samples were used in the \nsubstructure prediction and OD prediction experiments, \nrespectively. The model inputs were the atomic proper -\nties presented in Table  1. The code used for the experi -\nments can be found at [34].\nSubstructure prediction\nThe purpose of this experiment was to test the perfor -\nmance of the Transformer model in predicting the exist -\nence of substructures and to investigate the interpretation \nability of the model by visualizing the attention mecha -\nnism in the decoder-like module. We designed 24 sub -\nstructures and combinations of multiple substructures \nand predicted these substructures with our proposed \nmodel. Fig.  3 shows descriptions of the 24 substructures \nand the corresponding number of positive samples. Sub -\nstructures No. 0 to No. 11 are individual substructures, \nand substructures No. 12 to No. 23 are combinations of 2 \nor 3 substructures.\nThe 98,324 samples were divided into training and test \nsets at a ratio of 5:1. Because we have sufficient data in \nthis experiment and predicting the existence of sub -\nstructures is a relatively simple task, we did not consider \na wide range of hyperparameter settings. The hyperpa -\nrameter settings examined in this experiment are listed \nin Table 2. In addition to the parameters listed in Table 2, \nsimilar to the original Transformer, each encoder and \ndecoder layer includes an attention module and a two-\nlayer pointwise feedforward network with the same \nnumber of units as the dimension of the atomic attrib -\nutes, both of which end with a dropout layer with a rate \nof 0.1. Except for the layers used to convert the Q, K, and \nV matrices, which do not use the activation function, and \nthe final output layer, which uses the sigmoid activation \nfunction, the rest of the fully connected layers use ReLU \nas the activation function. The learning rate was set to \n7e-5 in this experiment.\nIn the multihead attention mechanism, each decoder \nlayer should contain multiple attention matrices. Hence, \nour visualization results are the sum of the attention \nmatrices of multiple heads. An example of visualizing the \nattention in the decoder-like module is shown in Fig.  4. \nThis figure shows the visualization results of a molecule \npredicted by a model with three decoder-like layers. The \nthree subfigures correspond to the attention mecha -\nnisms of the three decoder-like layers. The values in the \nattention matrices are indicated by the shade of the blue \ncircle covering each atom, with light blue indicating a \nsmall value and dark blue indicating a large value. (In the \ndefault RDKit settings, the atoms are identified by differ -\nent colors depending on the atomic number; for example, \n‘O, ’ ‘N, ’ and ‘S’ are written in red, blue, and yellow, respec-\ntively. Thus, the red character ‘O’ in Fig.  4 is not relevant \nto the attention.)\nIn addition to visualizing the attention matrices, we \nattempted to quantify the performance of the attention \nTable 1 Atomic features\nValues\nAtomic identity C, O, S, N, Cl, Na, P , F, Mg, I, Br, Zn, Fe, As, Ca, B, \nSi, K, Co, Cr, H, Al, others\nNumber of heavy neighbors 0, 1, 2, 3, 4, other\nNumber of hydrogen neighbors 0, 1, 2, 3, 4, other\nIs aromatic 0, 1\nIs in ring 0, 1\nHybridization type S, SP , SP2, SP3, SP3D, SP3D2, unspecified, other\nChirality CW, CCW, unspecified, other\nFormal charge 0, -1, 1, -2, 2, 3, 4, other\nExplicit valence 0, 1, 2, 3, 4, 5, 6, 7, other\nImplicit valence 0, 1, 2, 3, other\nPage 6 of 16Zheng et al. Journal of Cheminformatics (2022) 14:88\nmechanism in terms of identifying atoms related to predic-\ntions. For one molecule, we picked out the atoms contained \nin a target substructure and summed the corresponding \nattention values according to\nwhere T i is the set of atoms contained in the i-th target \nand attni(a) denotes the attention value of atom a when \npredicting the i-th target. (Note that each attention \n(4)\n∑\na∈T i\nattni(a),\nFig. 3 Details of the substructure experiment\nTable 2 Hyperparameters used in the substructure prediction \nexperiment\nSetting 1 Setting 2 Setting 3 Setting 4\nNumber of heads 12 12 12 12\nDimension of a single \nhead\n15 15 15 15\nNumber of encoder layers 6 6 6 6\nNumber of decoder layers 1 2 3 4\nAverage F1 value 0.985 0.970 0.976 0.966\nFig. 4 Visualization of a sample result\nPage 7 of 16\nZheng et al. Journal of Cheminformatics (2022) 14:88\n \nmatrix sums to 1 for all atoms; thus, the value of Eq. (4) \nshould be between 0 and 1, with larger values indicat -\ning that the attention mechanism better identifies atoms \nrelated to the target substructure.) In later experiments, \nwe visualized the sum of the attention matrices of all \ndecoder layers and all heads. Therefore, the sum of the \nattention values of all atoms in a molecule is the product \nof the number of heads ( nh ) and the number of decoder \nlayers ( ndc ). The performance of the attention mechanism \nin terms of identifying related atoms is evaluated as\nTo compute the sum of the attention values of the target \natoms, we calculated the variance of the attention val -\nues of the target atoms. A large variance indicates that \nthe attention mechanism tends to identify only some \nof the target atoms, while a small variance denotes that \nthe attention mechanism uniformly identifies the target \natoms. For each molecule, the variance is calculated as\nwhere |T i| is the number of atoms that belong to the \ntarget.\nOD prediction\nThe number of positive samples of most ODs is less \nthan 1/10 of the dataset, with ‘fruity’ having the most \npositive samples (1334). Moreover, 10 ODs have more \nthan 400 positive samples, namely, ‘fruity’ , ‘sweet’ , \n‘green’ , ‘floral’ , ‘woody’ , ‘herbaceous’ , ‘fresh’ , ‘fatty’ , ‘spicy’ , \nand ‘waxy’ . Among the ODs, 98 ODs had more than 50 \npositive samples. We predicted these 98 ODs in this \nexperiment.\nIn the OD prediction experiment, we compared the \nfollowing six models. Proposed model: a model with \nthe attention calculated using (2 ); MAT-attn: a model \nwith the attention calculated using Eq. (1 ); ADJ-only: \na model with the attention calculated using Eq. (7 ); \nDIST-only: a model with the attention calculated using \nEq. (8 ); Simplified decoder: the model shown in Fig.  5, \nwhich was created based on the proposed model by \nsimplifying the decoder-like module to a sum pool -\ning layer; MAT-model: the original MAT model. ADJ-\nonly and DIST-only were used to investigate the role \nof the adjacency and distance matrices. The simplified \ndecoder model was used to investigate the effect of the \ndecoder-like module.\n(5)\n∑\na∈T i attni(a)\nnh · ndc\n(6)1\n|T i|\n∑\na∈T i\n(\nattni(a) −\n∑\na′ ∈T i attni(a\n′\n)\n|T i|· nh · ndc\n)2\n,\nThe ratio of the training set to the test set was fixed at 5:1. \nThe hyperparameter settings used in this experiment are \nlisted in Table  3. In this experiment, class weights were \nused in the loss function, e.g., for each OD, the weight of \neach negative sample was 1, and the weight of a positive \nsample was equal to (number of all samples - number of \npositive samples)/number of positive samples.\nResults and discussion\nSubstructure prediction results\nThe best average F1 value for the 24 substructure pre -\ndiction experiment was 0.983, which was achieved with \n12 15-dimensional heads, six encoder layers, and one \ndecoder layer. The individual F1 values for the 24 sub -\nstructures were all greater than 0.9, as shown in Fig.  3. \nThe average F1 values for the other hyperparameter set -\ntings are listed in Table  2 and are generally very similar \nto one another. In summary, our proposed Transformer \nmodel can detect the existence of substructures and com-\nbinations of substructures.\nNext, we investigated the ability of the attention mech -\nanism to interpret the prediction results by visualizing \nthe attention matrix in the decoder-like module. We vis -\nualized only true positive (TP) samples (positive samples \nthat were predicted correctly). The visualization results \nof the model that achieved the best average F1 value \n(six encoder layers and one decoder layer) are shown in \nFig. 6. We visualize the results of 3 substructures in Fig. 6; \nthe visualizations of the other substructures show the \nsame trends as these 3 substructures. More TP results \ncorresponding to each substructure can be found at [34]. \nAccording to Fig.  6, for No. 1, the attention mechanism \nidentifies only part of the atoms in the target instead of \nall the atoms included in the target substructure. For sub-\nstructure No. 11, the attention mechanism identifies only \nO-O in the target and does not identify the single O. This \nresult shows that the attention mechanism does not iden-\ntify the atoms in the substructures that are similar to the \ntarget. For substructure No. 23, even molecules that con -\ntain only cCc are identified as positive, and the attention \nmechanism identifies both CC(C)C and cCc. This result \nshows that the attention mechanism can identify atoms \nin all composition substructures related to the target.\nFigure  6 shows that the attention mechanism clearly \nidentifies several atoms contained in the target sub -\nstructures. To investigate the role of the attention \n(7)Attention=\n(\nsoftmax\n(\nQK T\n√\ndk\n)\n⊙ A\n)\nV\n(8)Attention=\n(\nsoftmax\n(\nQK T\n√\ndk\n)\n⊙ g(D )\n)\nV\nPage 8 of 16Zheng et al. Journal of Cheminformatics (2022) 14:88\nmechanism in multiple decoder layers, we visualized \nmodels with two, three, and four decoder layers. Fig -\nure  7 shows the visualization results for each indi -\nvidual decoder layer. When the model has multiple \ndecoder layers, the attention mechanism in each \ndecoder layer can identify atoms related to the tar -\nget substructure, which inspired us to visualize the \nsum of the attention mechanisms in all decoder lay -\ners. Figure  8 shows the visualization results of the \nsummed attention, illustrating that models with \nFig. 5 Simplified decoder model\nTable 3 Hyperparameters used in the OD prediction experiment\nValues Optimal OD \nprediction \nsetting\nNumber of heads 6, 8, 10, 12 8\nDimension of a single head 30, 50 30\nNumber of encoder layers 5, 6, 7, 8 7\nNumber of decoder layers 1, 2 2\nτ in contrastive loss 0.3, 0.7, 1.0 0.7\nPage 9 of 16\nZheng et al. Journal of Cheminformatics (2022) 14:88\n \ndifferent numbers of decoder layers can accurately \nidentify atoms in the target substructures. Moreo -\nver, we used Eq. (5 ) to quantify the performance of \nthe attention mechanisms with different numbers \nof decoder layers. We calculated Eqs. (5 ) and (6 ) for \nall targets and all samples. The average values of Eq. \n(5) for all 16 substructures corresponding to the four \nhyperparameter settings in Table  2 are 0.704, 0.602, \n0.594, and 0.565, and the corresponding average val -\nues of Eq. (6 ) are 0.335, 0.204, 0.205, and 0.166. The \nattention mechanism in the model with one decoder \nlayer tends to identify the target substructures with \nlarger values and locates fewer atoms belonging to \nthe target. From the above results, we can draw the \nfollowing conclusions: (1) the attention mechanisms \nFig. 6 Visualization of the substructure prediction results\nPage 10 of 16Zheng et al. Journal of Cheminformatics (2022) 14:88\nin the decoder layers can be used to interpret the \nprediction results; (2) for TP samples, the attention \nmechanisms identify only certain atoms instead of all \natoms in the target substructures; and (3) for models \nwith multiple decoder layers, the sum of the atten -\ntion mechanisms in all decoder layers can identify the \natoms related to the predictions.\nFig. 7 Visualization of the attention in each decoder layer for models with two, three, and four decoder layers\nFig. 8 Visualization of the summed attention matrices\nTable 4 OD prediction results\nMacro F1 Micro F1\nProposed model 0.338 0.418\nADJ-only 0.333 0.417\nDIST-only 0.316 0.398\nMAT-attn 0.325 0.397\nSimplified decoder 0.310 0.395\nMAT model 0.264 0.351\nPage 11 of 16\nZheng et al. Journal of Cheminformatics (2022) 14:88\n \nOD prediction results\nThe hyperparameter settings used in the OD prediction \nexperiment and the optimal OD prediction settings are \npresented in Table  3. The results of our proposed model \nand the comparison model are shown in Table  4. The \nproposed model and the ADJ-only model achieve very \nsimilar results. Therefore, the attention values calculated \nby Eqs. (2) and (7) have similar effects on the results. \nWe expected to introduce the 3D structure information \nof the molecules through g(D) in Eq. (2); however, the \nexperimental results show that adding the distance infor -\nmation in this way does not enable the model to use the \n3D structure information. This finding may be because \nthere are relatively few samples, or the model itself may \nnot have the ability to learn 3D structural information \naccording to the distance matrix. The proposed model \nand the MAT-attn model obtain similar F1 results. \nTherefore, we conducted an approximate randomization \ntest to verify whether the differences between these two \nresults were meaningful. The p value was 0.009 when we \ncompared the proposed and MAT-attn models.\nThe best average F1 value was achieved by the model \nwith two decoder layers. Unlike the substructure pre -\ndiction experiment, visualizing the attention of the first \ndecoder layer shows that the attention mechanism tends \nto identify all atoms with similar values. This result may \nbe caused by having relatively few samples. In fact, the \nsame phenomenon was observed in the substructure \nprediction experiment when using the same number of \nsamples as in the OD prediction experiment. However, \neven if we increase the number of samples to approxi -\nmately 100,000 and perform the OD prediction experi -\nment, there may still be a tendency for the first encoder \nlayer attention mechanism to mark all atoms with similar \nvalues, it may be necessary to collect information about \nthe whole molecule to predict the odor, as a result of \nthe factors affecting the odor of a molecule being highly \ncomplex.\nRegarding attention visualization, we first visualized \nthe attention of the model that achieved the best F1 \nvalue. We visualized the attention of the second decoder-\nlike layer. More visualization results can be found at [34]. \nNineteen ODs obtained F1 values greater than 0.45. To \nensure that the visualization results are meaningful, \nwe visualize only these 19 ODs. Figure  9 shows several \nvisualization results of TP samples for ‘fruity’ , ‘musk’ , \n‘aldehydic’ and ‘fatty’ . For these four ODs, the attention \nmechanism tends to identify C(=O)O, carbon in a large \nring, C=O and long carbon chains, respectively. How -\never, for the remaining ODs, no obvious features are \nmarked in the corresponding positive samples.\nAccording to the substructure visualization experiment \nresults, the attention mechanism annotates only certain \natoms in the substructures instead of all related atoms. \nThe atoms in each substructure are randomly annotated \nby the attention mechanism; that is, the marked atoms \nvary depending on the model initialization. To determine \nthe substructures associated with the ODs, we repeat -\nedly trained the models with the same hyperparameter \nsettings and visualized the atoms that were frequently \nannotated by the attention mechanisms in the differ -\nent models. Specifically, we trained the models with the \nsame hyperparameters 100 times and created a counter \nfor each atom in each molecule in the samples. For each \nmodel, we then identified the top k atoms in a given mol -\necule with the largest attention values and increased the \ncounters corresponding to these k atoms by 1. Finally, we \ndetermined the atoms with counter values greater than n. \nFig. 9 Visualization of the OD prediction results\nPage 12 of 16Zheng et al. Journal of Cheminformatics (2022) 14:88\nFig. 10 Visualization results of TP samples in the OD prediction experiment when k = 5,n = 50\nPage 13 of 16\nZheng et al. Journal of Cheminformatics (2022) 14:88\n \nFig. 11 Visualization results of TN samples in the OD prediction experiment when k = 5,n = 50\nPage 14 of 16Zheng et al. Journal of Cheminformatics (2022) 14:88\nWe visualized the attention mechanisms of 100 models \nwith k = 5 and n = 50.\nSince we considered 100 models, when we visualized \nthe TP and TN (true negative samples, e.g., negative sam-\nples that were predicted correctly) samples, we chose \npositive samples that 90 of the 100 models predicted \nODs as positive and negative samples that 85 models pre-\ndicted as negative. Figure  10 shows the partial results of \nthe TP samples of 19 ODs. In Fig. 10, because k is limited \nto five atoms, the ‘fatty’ and ‘musk’ visualization results \nare not as good as those in Fig.  9. For the other ODs, \nwe can observe some clear features. According to the \nTP sample visualization results, we attempted to sum -\nmarize the feature substructures for each OD, and the \nsummary results are shown in the 4th column of Table  5. \nThe number of positive samples in the test set corre -\nsponding to the 19 ODs is shown in the second column \nof Table  5. (We note that an OD corresponds to multi -\nple feature substructures, and we summarize the features \nthat appear most frequently in the visualization results.) \nTable 5 Summary of visualization results\np F1 TP Substructure constraint TN Summary features\nFruity 242 0.636 Mainly annotates C(=O)O C(=O)O and no atoms of \nC(=O)O in a ring.\nMainly C(=O)OH C(=O)O\nSweet 208 0.509 Multiple structures, including \nC(=O)O\nC(=O)O Mainly C(=O)OH Multiple structures, including \nC(=O)O\nGreen 189 0.520 C=C, C=O C=O, C=C, CC(C)C and none \nof atoms in a ring\nCC(=C)C C=O and C=C without \nCC(=C)C\nFloral 147 0.541 Multiple substructures, \nincluding C(=O)O, C(=O), \nC with 3 carbon neighbors, \nc1ccccc1\nc1ccccc1C(=O)O or ‘A∼A(∼\nA)∼A’\nNo obvious features Multiple substructures, includ-\ning C(=O)O, C(=O), C with 3 \ncarbon neighbors, c1ccccc1\nWoody 107 0.517 CC(C)(C)C and three atoms of \nCC(C)(C)C in a ring\nCC(C)(C)C and three atoms of \nCC(C)(C)C in a ring\nOnly 3 molecules, and these \n3 samples are labeled by ODs \nsuch as ‘camphor’ and ‘earthy’\nCC(C)(C)C and three atoms of \nCC(C)(C)C in a ring\nFatty 78 0.475 Carbon chain, C=O, -OH ’C∼C∼C∼C∼ C ∼C∼C∼C ’,  \nwith each C having only two \nheavy neighbors\nTends to mark C(=O)O \nvaguely\nLong carbon chain\nRose 53 0.503 CC=C(C)C CC=C(C)C’ , c1ccccc1CCCC C=O at the end CC=C(C)C without C=O at \nthe end\nSulfurous 43 0.709 S S S=O S but not S=O\nMinty 32 0.466 CC(=C)C1CCCCC1 CC(=C)C1CCCCC1 Only 2 molecules, and they \nare labeled by ODs such as \n‘fresh’ and ‘herb’\nCC(=C)C1CCCCC1\nRoasted 36 0.470 ’[n,s,o]’ ’[n,s,o]’ Sometimes marks other \natoms instead of ’[n]’\nSubstructures related ’[n,s,o]’\nMeaty 36 0.591 ’[SH]’ , S ’[SH]’ , SS Tends to mark atoms on both \nsides of SS instead of SS\n’[SH]’ , SS and some neighbor-\ning substructures\nPineapple 28 0.467 C(=O)O C(=O)O and no atoms of \nC(=O)O in a ring\nDoes not mark C(=O)O May be a substructure con-\ntaining C(=O)O\nAldehydic 22 0.462 C=O, C=C C=O Mainly C(=O)O C(=O) but not C(=O)O\nPhenolic 24 0.484 c1ccccc1O c1ccccc1O Tends to mark atoms whose \nneighbor is an aromatic \ncarbon\nc1ccccc1O\nHoney 26 0.453 c1ccccc1 and C(=O)O c1ccccc1 and C(=O)O Marks c1ccccc1O vaguely Both c1ccccc1 and C(=O)O \nexist in molecule\nOrange 29 0.513 Carbon chain with C=O at \nthe end\n’C∼C∼C∼C∼ C ∼C=O’ , and \nnone of the atoms are in a \nring or have more than 3 \nheavy neighbors\nMost samples are labeled as \n‘fruity’ or ‘citrus’\nCarbon chain with C=O at \nthe end\nMusk 11 0.493 Ring with more than 10 \natoms\nRing with more than 10 \natoms\nToo few samples to sum-\nmarize\nRing with more than 10 atoms.\nCoconut 18 0.481 C(=O), with C in a ring C(=O), with C in a ring Tends to mark the end atoms \nthat connected to a carbon \nin a ring\nC(=O), with C in a ring and O \nat the end\nTerpene 9 0.533 Too few samples None Too few samples Too few samples\nPage 15 of 16\nZheng et al. Journal of Cheminformatics (2022) 14:88\n \nIn this experiment, we also visualized TN samples; in \nparticular, we visualized only TN samples containing \nthe feature substructures observed in the TP samples. \nThe feature substructures used to screen TN samples \nare shown in the 5th column of Table  5, and several vis -\nualization results are shown in Fig.  11 (the correspond -\ning OD labels of the molecule are also displayed). The \nfeatures of the TN samples are summarized in the 6th \ncolumn of Table  5. The TN sample visualizations show \nsome interesting results. For example, ‘woody’ has only \nthree TN samples under the corresponding substructure \nconstraints; however, the OD labels of these three sam -\nples are ODs such as ‘camphor’ and ‘earthy’ somewhat \nsimilar to ‘woody’ . Moreover, for ‘pineapple’ , the TP sam-\nple visualization results show that C(=O)O is frequently \nannotated. However, for TN samples, the attention \nmechanism does not identify C(=O)O and instead anno -\ntates the neighbor of the C(=O)O substructure; there -\nfore, we speculate that the region surrounding C(=O)O \nis also related to ‘pineapple’ . Finally, according to the fea-\ntures of the TP and TN samples, we drew some conclu -\nsions about the feature substructures of the 19 ODs, as \nshown in the last column of Table 5.\nRegarding the OD dataset, the amount of data is rel -\natively small when using a neural network to predict \nODs, and the consistency among the OD labels may \nalso be an issue. For example, ‘fruity’ is a comprehen -\nsive odor, and molecules that are labeled ‘berry’ , ‘apple’ , \netc., can be seen as ‘fruity’ . However, in the collected \ndata, some molecules that were labeled with fruit-like \nodors, such as ‘berry’ , had OD labels that did not con -\ntain ‘fruity’ . When we identified molecules containing \nfruit-like ODs (‘fruity’ , ‘citrus’ , ‘berry’ , ‘apple’ , ‘pineap -\nple’ , ‘orange’ , ‘pear’ , ‘melon’ , ‘banana’ , ‘lemon’ , ‘coconut’ , \n‘peach’ , ‘apricot’ , ‘cherry’ , ‘grape’ , ‘grapefruit’ , ‘plum’ , \n‘bergamot’ , ‘hawthorn’ , ‘jam’ , ‘mandarin’ , and ‘currant’) \nwith the label ‘fruity’ , the number of positive sam -\nples increased from 1334 to 1853, and the F1 value \nincreased from 0.636 to 0.744. In addition, Chacko \net  al. [15] inferred the two ODs ‘sweet’ and ‘musky’ \nwith the dataset in [35], which contains 480 samples, \nand achieved an F1 value of 0.81 for ‘sweet. ’ We also \ntrained and predicted ‘sweet’ odors with the dataset \ndescribed in [35]. However, the small sample size led \nto unstable results, with the F1 value ranging from \n0.71 to 0.91. Furthermore, we attempted to train the \nmodel with the dataset of 4462 samples used in this \nstudy and employed the dataset in [35] as the test set. \nThe resulting F1 value was 0.69, which is higher than \nthat in Table  5. We believe that this result is caused \nby the quality of the datasets. The dataset in [35] was \nconstructed by 55 people scoring the odor for each \nodorant, and we took the average score assigned by \nthese 55 individuals as the odorant label. The dataset \nused in this study was labeled by different people, and \nthe labeling standards may vary from person to per -\nson. The F1 score of ‘sweet’ was approximately 0.50 \nwith our datasets. This result may be influenced by the \nsmall number of samples and the lack of consistency \nin the labels across the large amount of collected data.\nConclusion\nIn this study, we used a machine learning approach to \ninvestigate the relationship between molecular structure \nand odor. We first built a Transformer model to predict \nthe molecular properties and interpret the prediction \nresults. We modified the attention calculation in the \nencoder based on the MAT model and used a decoder-\nlike module to interpret related substructures associated \nwith ODs. We applied the proposed model to predict \nsubstructures in molecules and investigated the role \nof the attention mechanisms in the decoder layers. The \nresults show that when we have a sufficient amount of \nsamples, the attention mechanisms can identify some, \nbut not all, of the atoms in the target substructures. \nThis result demonstrates that the prediction results can \nbe interpreted by visualizing the attention mechanism. \nFinally, we predicted 98 ODs with the proposed model \nand summarized the substructures associated with the \n19 ODs by visualizing the attention mechanism. With \nadditional odor labeling data, we expect to obtain bet -\nter F1 results and clearer attention visualization results, \nthereby enabling a better understanding of the relation -\nship between molecular structure and odor.\nAcknowledgements\nWe thank Stuart Jenkinson, PhD, from Edanz (https:// jp. edanz. com/ ac) for edit-\ning a draft of this manuscript.\nAuthor contributions\nXZ implemented the methods, analyzed the results and wrote the manuscript. \nYT designed the research plan, analyzed the results and wrote the manuscript. \nKH proposed the basic idea of odor synthesis by studying the odors of mol-\necules and provided knowledge of olfaction. All authors read and approved \nthe final manuscript.\nFunding\nThis work was supported by JSPS KAKENHI (Grant Number JP21K19796) and \nJST through the Establishment of University Fellowships Toward the Creation \nof Science Technology Innovation (Grant Number JPMJFS 2132).\nAvailability of data and materials\nThe code used to collect data from The Good Scents Company, the experi-\nmental code, and attention visualization results are provided at https:// github. \ncom/ zheng hah/ 0607\nPage 16 of 16Zheng et al. Journal of Cheminformatics (2022) 14:88\n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 13 August 2022   Accepted: 14 December 2022\nPublished: 29 December 2022\nReferences\n 1. Genva M, Kemene T, Deleu M, Lins L, Fauconnier M-L (2019) Is it possible \nto predict the odor of a molecule on the basis of its structure? Int J Mol \nSci. https:// doi. org/ 10. 3390/ ijms2 01230 18. Accessed on Dec 20 2022\n 2. Schütt K, Arbabzadah F, Chmiela S, Müller K-R, Tkatchenko A (2017) \nQuantum-chemical insights from deep tensor neural networks. Nat Com-\nmun. https:// doi. org/ 10. 1038/ ncomm s13890. Accessed on Dec 20 2022\n 3. Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE (2017) Neural mes-\nsage passing for quantum chemistry. In: International Conference on \nMachine Learning, PMLR, pp 1263–1272\n 4. Zheng S, Yan X, Yang Y, Xu J (2019) Identifying structure-property relation-\nships through smiles syntax analysis with self-attention mechanism. J \nChem Inf Model 59(2):914–923\n 5. Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, \nLeswing K, Pande V (2018) Moleculenet: a benchmark for molecular \nmachine learning. Chem Sci 9(2):513–530\n 6. Zhou J, Cui G, Hu S, Zhang Z, Yang C, Liu Z, Wang L, Li C, Sun M (2020) \nGraph neural networks: a review of methods and applications. AI Open \n1:57–81\n 7. Kriege NM, Johansson FD, Morris C (2020) A survey on graph kernels. \nAppl Netw Sci 5(1):1–42\n 8. Schütt K, Unke O, Gastegger M (2021) Equivariant message passing for \nthe prediction of tensorial properties and molecular spectra. In: Interna-\ntional Conference on Machine Learning, PMLR, pp 9377–9388\n 9. Klicpera J, Groß J, Günnemann S (2020) Directional message passing for \nmolecular graphs. arXiv preprint arXiv: 2003. 03123. Accessed on Dec 20 2022\n 10. Matveieva M, Polishchuk P (2021) Benchmarks for interpretation of QSAR \nmodels. J Cheminformatics 13(1):1–20\n 11. Keller A, Gerkin RC, Guan Y, Dhurandhar A, Turu G, Szalai B, Mainland \nJD, Ihara Y, Yu CW, Wolfinger R et al (2017) Predicting human olfac-\ntory perception from chemical features of odor molecules. Science \n355(6327):820–826\n 12. Shang L, Liu C, Tomiura Y, Hayashi K (2017) Machine-learning-based olfac-\ntometer: prediction of odor perception from physicochemical features of \nodorant molecules. Anal Chem 89(22):11999–12005\n 13. Sanchez-Lengeling B, Wei JN, Lee BK, Gerkin RC, Aspuru-Guzik A, \nWiltschko AB (2019) Machine learning for scent: learning generalizable \nperceptual representations of small molecules. arXiv preprint arXiv: 1910. \n10685. Accessed on Dec 20 2022\n 14. Kearnes S, McCloskey K, Berndl M, Pande V, Riley P (2016) Molecular graph \nconvolutions: moving beyond fingerprints. J Comput Aided Mol Des \n30(8):595–608\n 15. Chacko R, Jain D, Patwardhan M, Puri A, Karande S, Rai B (2020) Data \nbased predictive models for odor perception. Sci Rep 10(1):1–13\n 16. Debnath T, Nakamoto T (2022) Predicting individual perceptual scent \nimpression from imbalanced dataset using mass spectrum of odorant \nmolecules. Sci Rep 12(1):1–9\n 17. Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S (2020) \nEnd-to-end object detection with transformers. In: European Conference \non Computer Vision, Springer, pp 213–229\n 18. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner \nT, Dehghani M, Minderer M, Heigold G, Gelly S, et al (2020) An image is \nworth 16x16 words: transformers for image recognition at scale. arXiv \npreprint arXiv: 2010. 11929. Accessed on Dec 20 2022\n 19. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, \nPolosukhin I (2017) Attention is all you need. Adv Neural Inf Process Syst. \nhttps:// doi. org/ 10. 48550/ arXiv. 1706. 03762. Accessed on Dec 20 2022\n 20. Fan A, Lavril T, Grave E, Joulin A, Sukhbaatar S (2020) Addressing some \nlimitations of transformers with feedback memory. arXiv preprint arXiv: \n2002. 09402. Accessed on Dec 20 2022\n 21. Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov R (2019) Trans-\nformer-xl: attentive language models beyond a fixed-length context. \narXiv preprint arXiv: 1901. 02860. Accessed on Dec 20 2022\n 22. Huang Z, Liang D, Xu P , Xiang B (2020) Improve transformer models with \nbetter relative position embeddings. arXiv preprint arXiv: 2009. 13658. \nAccessed on Dec 20 2022\n 23. Karpov P , Godin G, Tetko IV (2020) Transformer-CNN: swiss knife for QSAR \nmodeling and interpretation. J Cheminformatics 12(1):1–12\n 24. Maziarka Ł, Danel T, Mucha S, Rataj K, Tabor J, Jastrzkebski S (2020) Mol-\necule attention transformer. arXiv preprint arXiv: 2002. 08264. Accessed on \nDec 20 2022\n 25. Maziarka Ł, Danel T, Mucha S, Rataj K, Tabor J, Jastrzębski S (2019) \nMolecule-augmented attention transformer. In: Workshop on Graph \nRepresentation Learning, Neural Information Processing Systems\n 26. Maziarka Ł, Majchrowski D, Danel T, Gaiński P , Tabor J, Podolak I, Morkisz \nP , Jastrzębski S (2021) Relative molecule self-attention transformer. arXiv \npreprint arXiv: 2110. 05841. Accessed on Dec 20 2022\n 27. Hutchinson MJ, Le Lan C, Zaidi S, Dupont E, Teh YW, Kim H (2021) \nLietransformer: Equivariant self-attention for lie groups. In: International \nConference on Machine Learning, PMLR, pp 4533–4543\n 28. Thölke P , De Fabritiis G (2022) Torchmd-net: equivariant transformers for \nneural network based molecular potentials. arXiv preprint arXiv: 2202. \n02541. Accessed on Dec 20 2022\n 29. Chen T, Kornblith S, Swersky K, Norouzi M, Hinton GE (2020) Big self-\nsupervised models are strong semi-supervised learners. Adv Neural Inf \nProcess Syst 33:22243–22255\n 30. Chen T, Kornblith S, Norouzi M, Hinton G (2020) A simple framework for \ncontrastive learning of visual representations. In: International Confer-\nence on Machine Learning, PMLR, pp 1597–1607.\n 31. Khosla P , Teterwak P , Wang C, Sarna A, Tian Y, Isola P , Maschinot A, Liu C, \nKrishnan D (2020) Supervised contrastive learning. Adv Neural Inf Process \nSyst 33:18661–18673\n 32. Mendez D, Gaulton A, Bento AP , Chambers J, De Veij M, Félix E, Magariños \nM, Mosquera J, Mutowo P , Nowotka M, Gordillo-Marañón M, Hunter F, \nJunco L, Mugumbate G, Rodriguez-Lopez M, Atkinson F, Bosc N, Radoux \nC, Segura-Cabrera A, Hersey A, Leach A (2018) ChEMBL: towards direct \ndeposition of bioassay data. Nucleic Acids Res 47(D1):930–940. https:// \ndoi. org/ 10. 1093/ nar/ gky10 75. Accessed on Dec 20 2022\n 33. The good scents company information system. http:// www. thego odsce \nntsco mpany. com/. Accessed on Dec 20 2022\n 34. GitHub. https:// github. com/ zheng hah/ 0607. Accessed on Dec 20 2022\n 35. Keller A, Vosshall LB (2016) Olfactory perception of chemically diverse \nmolecules. BMC Neurosci 17(1):1–17\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Odor",
  "concepts": [
    {
      "name": "Odor",
      "score": 0.8934151530265808
    },
    {
      "name": "Transformer",
      "score": 0.6342653036117554
    },
    {
      "name": "Computer science",
      "score": 0.6201494932174683
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47875040769577026
    },
    {
      "name": "Biological system",
      "score": 0.4506608545780182
    },
    {
      "name": "Data mining",
      "score": 0.3373482823371887
    },
    {
      "name": "Chemistry",
      "score": 0.25476527214050293
    },
    {
      "name": "Physics",
      "score": 0.13104590773582458
    },
    {
      "name": "Biology",
      "score": 0.07740676403045654
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}