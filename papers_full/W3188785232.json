{
  "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models",
  "url": "https://openalex.org/W3188785232",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4301600982",
      "name": "Kirk, Hannah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287228430",
      "name": "Jun, Yennie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156793864",
      "name": "Iqbal Haider",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4280978954",
      "name": "Benussi, Elias",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4301600986",
      "name": "Volpin, Filippo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223355093",
      "name": "Dreyer, Frédéric A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221909639",
      "name": "Shtedritski, Aleksandar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223134852",
      "name": "Asano, Yuki M.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970597249"
  ],
  "abstract": "The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reflect or correct for existing inequalities.",
  "full_text": "Bias Out-of-the-Box: An Empirical Analysis of\nIntersectional Occupational Biases in Popular\nGenerative Language Models\nHannah Rose Kirk†‡, Yennie Jun†, Haider Iqbal†, Elias Benussi†,\nFilippo Volpin†, Frederic A. Dreyer†, Aleksandar Shtedritski†, Yuki M. Asano†\n†Oxford Artiﬁcial Intelligence Society, University of Oxford\n‡hannah.kirk@oii.ox.ac.uk\nAbstract\nThe capabilities of natural language models trained on large-scale data have in-\ncreased immensely over the past few years. Open source libraries such as Hugging-\nFace have made these models easily available and accessible. While prior research\nhas identiﬁed biases in large language models, this paper considers biases contained\nin the most popular versions of these models when applied ‘out-of-the-box’ for\ndownstream tasks. We focus on generative language models as they are well-suited\nfor extracting biases inherited from training data. Speciﬁcally, we conduct an in-\ndepth analysis of GPT-2, which is the most downloaded text generation model on\nHuggingFace, with over half a million downloads per month. We assess biases re-\nlated to occupational associations for different protected categories by intersecting\ngender with religion, sexuality, ethnicity, political afﬁliation, and continental name\norigin. Using a template-based data collection pipeline, we collect 396K sentence\ncompletions made by GPT-2 and ﬁnd: (i) The machine-predicted jobs are less\ndiverse and more stereotypical for women than for men, especially for intersections;\n(ii) Intersectional interactions are highly relevant for occupational associations,\nwhich we quantify by ﬁtting 262 logistic models; (iii) For most occupations, GPT-2\nreﬂects the skewed gender and ethnicity distribution found in US Labor Bureau\ndata, and even pulls the societally-skewed distribution towards gender parity in\ncases where its predictions deviate from real labor market observations. This raises\nthe normative question of what language modelsshould learn - whether they should\nreﬂect or correct for existing inequalities.\n1 Introduction\nThe advent of deep learning and massive growth in training data have led to natural language models\nsurpassing humans on numerous benchmarks [1, 22, 39, 40]. However, as Bender et al. [7] states,\nthese models can exacerbate existing biases in data and perpetuate stereotypical associations to\nthe harm of marginalized communities. Simultaneously, pre-trained models have become readily\naccessible via open source libraries such as HuggingFace, allowing non-experts to apply these tools\nin their own applications. These developments in generative language models substantiate a need to\nunderstand the potential for biases towards protected classes, such as gender and ethnicity.\nThis paper considers potential biases present in the most popular and most downloaded versions of\nlarge-scale, open sourced text generation models applied ‘out-of-the-box’. Despite the release of\nnewer and larger models often redirecting researchers’ attention, there exist important research gaps\nin existing models.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2102.04130v3  [cs.CL]  27 Oct 2021\nBearing in mind that the potential negative total impact from biased models is correlated with number\nof downloads of that model, this paper tests the biases in the small GPT-2 model, which is the most\ndownloaded text generation model on HuggingFace with over half a million downloads in the month\nof May 2021 alone. These numbers motivate further research on the biases of these models given\ntheir increased use in hiring related downstream tasks, such as chatbots or unsupervised scanning of\nCVs and applications [30].\nWithin this context, specifying which biases to analyze is crucial; Blodgett et al. [9] ﬁnd that a\nmajority of NLP papers investigating bias are unclear in their articulations of bias. In this paper, we\nconsider both representational and allocational harms [4]. We attempt to elucidate representational\nharms, or those harmful in their own right, by highlighting occupation-related stereotypes that may\npropagate negative generalizations about particular social groups. For example, women’s higher\nlikelihood of being associated with care-oriented occupations may perpetuate unwanted stereotypes.\nEspecially within the context of occupations, such associations may lead to allocation harms. Frequent\nstereotypical association of certain demographic groups with a subset of occupations may lead to\nconditioned expectations in job hiring where a certain individual is predicted to be well-suited for a\njob based on their demographics [20].\nIn this paper, we generate 396K sentence completions using GPT-2 with default parameters to assess\nwhich occupations GPT-2 preferentially associates with intersections of gender and protected classes.\nWe further compare these to real-world occupation data from the US Labor Bureau to map model\nbiases to systemic societal biases. This paper provides the following contributions: (a) a detailed\ndata collection protocol for studying intersectional biases in generative language models; (b) the\nanalysis of biases present in GPT-2 for gender intersected with ethnicity, religion, sexuality, political\nafﬁliation, and continent name origin; and (c) a comparison of GPT-2’s predictions with ground truth\noccupation distribution as observed in US labor market data.1\n2 Related Work\nBias in NLP models. Negative generalizations, stereotypes, or misrepresentations of particular social\ngroups can be learned by generative language models. Extensive research has shown that unrestricted\ntraining of natural language models can inherit human biases and, in some cases, amplify them\n[11, 13, 19, 43]. Previous papers have worked to identify, quantify, and de-bias context-independent\nword embeddings such as Word2Vec and GloVe [ 11, 15, 44]. Researchers have also attempted\nto quantify and mitigate biases in transformer-based generative language models, such as ELMo\n[44] and BERT [8, 26]. Nadeem et al. [28] evaluate stereotypical bias in various language models,\nincluding XLNet and GPT-2, related to domains of gender, profession, race, and religion, although\nthey do not consider the intersections of these domains. We extend these domains by additionally\nconsidering continental name origin and political afﬁliation, intersected with gender.\nProbing language models. Our work utilizes what Solaiman et al. [35] term ‘bias probes’ - a battery\nof inputs to large language models to produce certain outputs in order to map the biases learned by\nthese models. The authors generate hundreds of samples for several potentially problematic prompts\n(i.e., “Police describe the suspect as...”, “The victim was...”) and determine the gender and race\nattributes for the resulting outputs. Most similar to our work, Sheng et al. [33] utilize preﬁx templates\n(i.e., “X worked as...”, “X had a job as...”), including those related to occupation contexts. They\ngenerate 100 samples for each preﬁx template and demographic combination (gender intersected\nwith ethnicity and sexuality) and analyze bias in GPT-2 by using sentiment score as a proxy for bias.\nWe extend such work by conducting an empirical analysis of the sentence completions within the\nspeciﬁc context of bias towards occupational associations.\nIn our paper, we focus on one sentence template to reduce variation in returned occupations while\nkeeping sentence semantic structures ﬁxed. Unlike [ 33], we do not introduce potentially noisy\nsentiment classiﬁcation, instead directly analyzing the statistical distributions of returned occupa-\ntions. Further, we generate an order of magnitude more samples than [33, 35] for greater statistical\nrobustness. Lastly, unlike previous work, we compare the returned occupational associations from\nour completed preﬁx templates to real-world US labor market data.\n1Materials and data are available at https://github.com/oxai/intersectional_gpt2.\n2\nWe choose the proposed protocol to evaluate biases in text as it is best suited for probing generative\nlanguage models in their most “natural” form, in which sentence completions are generated. In\ncontrast to this approach, embedding association tests, such as the Word Embedding Association\nTest (WEAT) [13], would require more heuristic choices, as they have been found to be highly\ndependent on the initial selection of seed words [2]. Coreference resolution methods, such as Zhao\net al. [44], suffer from frequent ambiguities and unstated assumptions [ 10]. Finally, information\ntheoretic approaches, such as Rudinger et al. [32], require a pre-generated corpus and thus would\nconfound the (template-based) generation with the bias measurement.\nIntersectional biases. As Crenshaw [14] explains, intersectional biases are a necessary consideration\nbecause a single axis of analysis treating gender and race as mutually exclusive categories distorts\nthe reality of marginalized communities (such as Black women). More recently, Foulds and Pan\n[17] provides deﬁnitions of fairness in machine learning systems informed by the framework of\nintersectionality. The intersections between gender and racial biases have been studied in sentiment\nanalysis [25] and language models such as BERT and GPT-2 [36]. As well as race and gender, we\nextend our analysis to intersections with other legally protected categories that have historically been\nsubject to discrimination: religion, sexuality, and political afﬁliation.\n3 Methods\n3.1 Model Choice\nAs of May 2021, the 124M-parameter version of GPT-2 was the most downloaded text generation\nmodel on HuggingFace2, with 526K downloads; the second most downloaded model, XLNet [42],\nhad 167K downloads (see Appendix B). Therefore, we focus our analysis on the small GPT-2 model,\nlicensed under the Apache License, Version 2.0. Our intent is not to show how an optimized model\nwith tuned hyperparameters predicts job distributions, but how an ‘out-of-the-box’ default model\nused by non-expert users could unintentionally propagate bias. Therefore, we keep the inference\nhyperparameters ﬁxed to their default values; in particular, the top_k parameter and the decoder\ntemperature. For completeness, we conduct a brief ablation of these hyperparameters to assess their\neffect on the diversity of the returned jobs in Appendix C. As a further robustness check, we analyze\nXLNet, the second most downloaded text generation model, with the same preﬁx-templates and\nverify that our results are consistent across models (see Appendix E).\n3.2 Data collection\nTable 1: Summary table of data collection showing\nthe number of calls per category and per variant\n(Var). The total number of calls is 396,000.\nCategory Var Calls Total Calls Cum.Share\nBase 2 7,000 14,000 81%\nEthnicity 8 7,000 56,000 82%\nReligion 10 7,000 70,000 84%\nSexuality 4 7,000 28,000 83%\nPolitical 4 7,000 28,000 82%\nContinent 200 1,000 200,000 76%\nOur data collection pipeline is shown in Fig. 1.\nWe prompt GPT-2 using preﬁx templates similar\nto those introduced by Sheng et al. [33].\nIdentity-based templates. Our preﬁx tem-\nplates are of the form “The [X][Y] works as a\n... ”, where X is one of the following protected\nclasses: ethnicity, religion, sexuality, and polit-\nical afﬁliation, and Y is ‘man’ or ‘woman’. For\na baseline to intersectional effects, we leave X\nblank (i.e. “The man/woman works as a ... ”).3\nThe ethnicity and religion classes used in this\npaper correlate with the top ethnicities and reli-\ngions in the US, as we situate our analysis with US data. Using these 28 unique templates (Tab.1),\nwe generate 7,000 sentences using GPT-2. Generated sentences are limited to a maximum length of\n10 words to capture immediate occupation associations.\nName-based templates. An additional preﬁx template is created of the form “[Z] works as a ... ”,\nwhere Zis a name sampled from the most popular male and female ﬁrst names per country, obtained\nfrom Wikipedia [41]. We aggregate names into ﬁve geographic groups: Africa, Americas, Asia,\nEurope, Oceania. We sample 20 names for each geographic group and gender pair, yielding 200\n2https://huggingface.co/models?pipeline_tag=text-generation\n3We discuss the implications of the binarization of gender in Sec. 5 and Appendix A\n3\nAsian\nBlack\nHispanic\nWhite\nEthnicity\nBuddhist\nChristian\nHindu\nJewish\nMuslim\nReligion\nGay/Lesbian\nStraight\nSexuality\nConservative\nLiberal\nPolitical\nAfrica\nAmericas\nAsia\nEurope\nOceania\nContinent\nX\nY\nMan\nWoman\nBase\nGPT-2 NER\nThe          works as a ...X Y\nZ      works as a ...\nZ{NAME}\nMan     Woman\nJanitor\nWriter\nTeacher\n......\n......\nMan     Woman\nJanitor\nWriter\nTeacher\n......\n......\nMan     Woman\nJanitor\nWriter\nTeacher\n......\n......\nMan     Woman\nJanitor\nWriter\nTeacher\n......\n......\nMan     Woman\nJanitor\nWriter\nTeacher\n......\n......\nFrequency for           orX Y Z\nFigure 1: Data Collection Process. We collect 396K responses from GPT-2, and retrieve “titles”\nvia Stanford CoreNLP’s Named Entity Recognition (NER) to analyze the predicted occupational\ndistribution for various intersectional categories.\nunique templates, from which we generate1,000 sentences each. By prompting GPT-2 with templates\ndevoid of inherently gendered or racialized terms, such as ‘man/woman’ or ‘Asian/Black’, we can\nbetter examine the latent associations when GPT-2 estimates the ethnicity and gender from ﬁrst\nnames.\nOccupation entity recognition. For each generated sentence, we use the Stanford CoreNLP Named\nEntity Recognizer (NER) [27] to extract job titles. NER was unable to detect titles for some sentences\nwhich were removed from the dataset, losing 10.6% of identity-based sentences and 19.6% of\nname-based sentences. We then create a one-hot encoded frequency matrix for returned job tokens,\ncombining duplicate jobs (e.g. nurse/nurse practitioner). However, we do not merge job tokens\nwith inherent hierarchies (e.g. assistant professor/professor) or implicit gender associations (e.g.\nsalesman/salesperson, waitress/waiter). Sentences returning multiple titles (e.g. “The woman works\nas a waitress and a maid”) were treated as two separate entries in the frequency matrix given that\nindividuals can have more than one job.\n3.3 Empirical Analysis\nThe distribution of returned jobs is highly-skewed with long tails: a few jobs comprise a signiﬁcant\nshare and many jobs are mentioned infrequently. Therefore, we apply a lower-bound threshold to\nfocus our analysis, removing tokens mentioned in fewer than 0.25% of total calls, which preserves\napproximately 80% of the sample (Tab.1). For jobs above the threshold, we run a logistic regression\non the one-hot matrix and output frequencies to predict p([job] = 1|X,Y ) for the input “The [X][Y]\nworks as a [job]”. While GPT-2 is a ‘black-box’ model, this predictive modeling attempts to estimate\nhow intersectional categories change GPT-2’s prior on the probability of job associations. By using\ninteraction terms, we can study whether intersectionality has additional inﬂuence beyond main effects\n(e.g. the isolated effects of gender and ethnicity). The logistic regression equation includes ‘man’\nfrom the baseline case as the reference group, with dummy variables added for woman, for each\nintersectional category C, and for interaction terms:\nlog odds(p(jobi|c)) =β0 + β1Womani +\nC∑\nc=1\nγicCategoryic +\nC∑\nc=1\nδic(Categoryic ∗Womani) +ϵi,\nwhere log odds(p) = log(p/(1 −p)) is the log-odds ratio of probability p.\n3.4 Comparison with US Labor Market Data\nA comparison of GPT-2’s predictions to the true labor market distribution requires recent data\ndisaggregated by gender and intersection for a granular set of occupations. The 2019 US Labor\nBureau Statistics from the Current Population Survey [37] reports the gender and ethnicity shares of\n4\n100 101 102 103\nLog(Rank)\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18Share of Total\n16 jobs account for 50% of men\n8 jobs account for 50% of women\n66 jobs account for 90% of men\n43 jobs account for 90% of women\nWomen (Average) Men (Average)\nFigure 2: GPT-2 occupational stereotyping. GPT-2 stereotypes the occupational distribution of\nwomen more than that of men. The graph shows the share of occupations for each gender, sorted\nfrom most frequent to less frequent.\nworkers in 567 occupational categories.4 We recognize a number of limitations of this data, which we\naddress in the discussion. However, using US data may provide an appropriate baseline comparison:\n50% of Reddit trafﬁc comes from the US, and a further 7% from Canada and the UK each [ 34].\nGiven that US sources form a majority in GPT-2’s training material, and that no other major country\nhad data available disaggregated by gender, ethnicity and granular job categories, we consider the US\ndataset a satisfactory ﬁrst benchmark.\nWe ﬁrst select the 50 most frequently mentioned jobs by GPT-2. Then from these, we match\nGPT-2’s returned tokens to real US occupation titles, ﬁnding correspondences for 44/50 titles (see\nAppendix D). We compute GPT-2’s predicted proportional representation for each gender-ethnicity\npair, assuming the percentage of women is equal across ethnicities. The ‘predicted’ labor force has\nequal representation across groups because we generate the same number of sentence prompts per\npair (n= 7,000). The real-world distribution is not so evenly balanced by demographic group, so\nthe predicted proportions are scaled by the true distribution of gender and ethnicity reported in the\nUS Labor Statistics and summarized in Appendix D. The scaling factor is γ(c) =G(c)E(c)\nˆD(c) , where\nG(c),E(c) are the gender- and ethnicity-shares of the US data, respectively and ˆD(c) = 12.5% is\nour artiﬁcial “population”-share. Hence the adjusted prediction is given by:\nadj. Pred(i,c) =γ(c) ×Pred(i,c), (1)\nwhere Pred(i,c) is the share of job i for characteristics c. For jobs reported in the US data, we\ncalculate the difference between the predicted proportions and the true proportions.\n4 Results\nWe analyze the effect of gender on returned occupational distributions in Sec. 4.1 and on particular\noccupations in Sec. 4.2. We extend these analyses to intersectional associations in Sec. 4.3 with\nempirical results derived from logistic regressions. Finally, we compare and quantify the predicted\ndistributions against ground truth US occupation data in Sec. 4.4.\n4.1 Gender differences in distributions\nFig. 2 ranks the frequency of jobs against the cumulative share. While 16 jobs account for 50% of the\noutputs for men, only 8 jobs account for the same share for women. Similarly, at the 90% level, men\nare associated with more jobs than women (66 vs 43, respectively). This suggests that GPT-2 predicts\na wider variety of jobs for men and a narrower set of jobs for women. The Gini coefﬁcients5 in Tab. 2\nconﬁrm this more unequal distribution for women.\n4We consider the 2019 data a better comparison than 2020 as it excludes inﬂuences from the COVID-19\npandemic and GPT-2 has not been retrained since.\n5G= (∑n\ni=1(2i− n− 1)xi)/(n∑n\ni=1 xi), where xis the observed value, nis the total values observed,\nand iis the rank is ascending order.\n5\n4.2 Gender differences in occupations\nIn addition to distributional differences, the set of returned jobs also differ by men and women. In\nFig. 3, we show the proportion of genders in all jobs mentioned more than 35 times for baseline\nman and woman. We make two observations: ﬁrst, there is a greater number of jobs dominated by\nmen as compared to women, reﬂecting the greater diversity of occupations for men. Second, the\noccupations seem stereotypical: men are associated with manual jobs such as laborer, plumber, truck\ndriver, and mechanic, and with professional jobs such as software engineer, developer and private\ninvestigator. Women are associated with domestic and care-giving roles such as babysitter, maid and\nsocial worker. Furthermore, over 90% of the returns for ‘prostitute’ were women, and over 90% of\nreturns for ‘software engineer’ were men. We only ﬁnd three jobs for which GPT-2’s outputs suggest\na gender-neutral prior over occupations: writer, reporter, and sales representative.\n4.3 Intersectional analysis\nTable 2: Gini coefﬁcients of rank-frequency distri-\nbutions returned by GPT-2.\nGender Intersec. Gini Relative Coeff\nCoeff Base M = 100%\nMan Base 0.933 100\nMan Religion 0.929 99.57\nMan Sexuality 0.935 100.21\nMan Ethnicity 0.939 100.64\nMan Political 0.942 100.96\nWoman Base 0.951 101.93\nWoman Political 0.951 101.93\nWoman Ethnicity 0.956 102.47\nWoman Religion 0.956 102.47\nWoman Sexuality 0.958 102.68\nTable 3: Aggregated logistic regression results.\nWe ﬁt a total of 262 logistic regressions and report\nthe number of times the independent variables con-\ntributed signiﬁcantly to the logistic model, as well\nas their average contribution to the Pseudo-R2.\n#Jobs Variable Pct. Signif ∆R2\nEthnicity 55\nwoman (w.) 0.71 3.22\nw.:asian 0.29\n0.40w.:black 0.36\nw.:hispanic 0.38\nw.:white 0.16\nReligion 64\nwoman (w.) 0.61 3.31\nw.:buddhist 0.19\n0.39\nw.:christian 0.27\nw.:hindu 0.27\nw.:jewish 0.33\nw.:muslim 0.25\nSexuality 72\nwoman (w.) 0.61 3.36\nw.:lesbian 0.35 0.45w.:straight 0.26\nPolitical 71\nwoman (w.) 0.59 3.47\nw.:conserv. 0.24 0.46w.:liberal 0.30\nThe Gini coefﬁcients (Tab. 2) for gender-\nintersection pairs indicate a greater clustering\nof women into fewer jobs across all intersec-\ntions, especially for sexuality, religion and eth-\nnicity. We thus ask the question, how im-\nportant are gendered intersections in deter-\nmining the job returned by GPT-2? Tab. 3\npresents summary results from 262 logistic re-\ngressions, which predict the likelihood of a job\nbeing associated with the demographics in a\ngiven sentence prompt. We focus on two met-\nrics indicating how often the addition of regres-\nsors adds explainability of the outcome: i) The\nproportions of regressions where the woman\ndummy and the interactions were signiﬁcant\n(p< 0.05), and ii) The change in Pseudo-R2 on\nthe addition of the woman dummy and the in-\nteractions.6 Statistical results, including the co-\nefﬁcients, for all regressions are in Appendix F.\nThe aggregated results in Tab. 3 show that the\nwoman dummy is frequently signiﬁcant, most\ncommonly so in ethnicity regressions (71%) and\nleast commonly in political regressions (59%).\nAdding a woman dummy increases the model\nR2 on average by +3.3% (percentage points),\nsignifying that gender explains additional vari-\nation in job prediction. Interactions are signiﬁ-\ncant in approximately one third of regressions,\nbut the additional increase to R2 is on average\nsmaller (+0.4%). There is some variation in\nthe signiﬁcance of interactions; for example,\n{women:hispanic} and {woman:black} are more\nfrequently signiﬁcant than {woman:white}, and\n{woman:lesbian} is more frequently signiﬁcant\nthan {woman:straight}. These results suggest\nthat some intersections are more salient in chang-\ning the returned job from a given sentence\nprompt, and may anchor GPT-2 on a stereo-\ntypical occupation set. In general, across a\nwide range of jobs, gender and intersectional-\nity are signiﬁcant determinants of the job token\nreturned by GPT-2.\n6We use the McFaddenR2 which is calculated by comparing the log-likelihood of a model with no predictors\nL0, versus the log-likelihood of the estimated model LM: R2\nMcF = 1 − ln(LM)/ln(L0)\n6\nlaborer\ncashier\nplumber\ntruck driver\nsalesman\nsoftware engineer\ncourier\ndeveloper\nprivate investigator\nmechanic\ncontractor\ndriver\nsupervisor\njournalist\nbarber\nbus driver\nphotographer\ntechnician\nclerk\nsecurity guard\ncomputer programmer\nwaiter\ntaxi driver\ncarpenter\nconstruction worker\npolice officer\nbouncer\njanitor\nmanager\ndoctor\nconsultant\nbartender\nchef\nwriter\nreporter\nsales representative\nlawyer\ncook\neditor\nhousekeeper\nbarista\nteacher\nreceptionist\nassistant\nwaitress\nnurse\nprostitute\nsocial worker\nmaid\nmassage therapist\nbabysitter\nmodel\n100% Men\nGender Parity\n100% Women\nMale-dominated jobs\nFemale-dominated jobs\nFigure 3: Fundamentally skewed GPT-2 output distributions. We show the gender proportions\nwhen querying for the base case, i.e. X = {},Y = {Man,Woman}and present all jobs with greater\nthan 35 =n∗0.25% mentions, making up 81% of returned sentence prompts.\nKnowing that gender and intersectional associations are quantitatively important for conditioning\nGPT-2’s probability distribution over jobs, we next ask what jobs are over-represented in one\ngender for each intersectional category? We calculate distance to the equi-proportion baseline\ngiven by (1/|c|,0) to (0,1/|c|), where |c|is the number of choices for intersection c. We normalize\nthis baseline such that 1/|c|= 1xso that jobs lie on this line if adding intersections has no effect on\nthe gender ratio.\nFor illustrative purposes, we compare the following two examples: religious intersection from the\nidentity-template, which has the greatest man-woman dispersion to the equi-proportion baseline;\nand continental name-origin from the name-template, which has the least dispersion. We present\nthe analyses for all remaining intersections in Appendix G. We ﬁrst consider religious intersections\n(Fig. 5). For Christian, Buddhist, and Jewish religions, GPT-2 generates occupations with a large\nover-representation factor towards one gender, especially for professional religious occupations: nuns\nare dominated by Buddhist women, rabbis are dominated by Jewish men, and monks, pastors, and\npriests are dominated by Buddhist and Christian men. Hindu men and women predominately have\nassociations with non-religious professions (e.g. bouncers and massage therapists). We compare this\nwith continent name origin intersections (Fig. 6), for which jobs are more closely distributed to the\nequi-proportion baseline. These ﬁndings suggest that name origin has less of an effect on the token\nreturned by GPT-2 than when adding an explicit categorical intersection (e.g. ethnicity or religion).\nFrom these quantitative and qualitative analyses, we have demonstrated that stereotypical jobs are\nassociated with men and women, and that the set of male- and female-dominated jobs changes with\nthe addition of intersections like religion and sexuality. However, it remains to be seen whether\nGPT-2’s ‘stereotypical associations’ directly reﬂect, exacerbate, or correct for societal skew given the\nunfortunate reality that jobs are not evenly distributed by demographic group.\n4.4 Comparison to Labor Market Ground Truth\nFor a given job, how well does GPT-2 predict the gender-ethnicity split?There are three possible\ncases: GPT-2 overestimates the true representation of women in female-dominated jobs (exacerbates\nsocietal skew), GPT-2 matches the true proportional representation (directly inherits skew), or GPT-2\nunderestimates the true proportional representation (corrects for skew). In Fig. 4, we ﬁnd that most\npredicted values lie close to the ground-truth given by the identity line, indicating a high accuracy\nin prediction. We use two quantitative measures of the relative deviation of GPT-2 predictions to\nUS ground truth: mean-square error (MSE) and Kendall-Tau (Kτ) coefﬁcient [24]. For the baseline\nwoman group, the Kτ coefﬁcient is 0.628, indicating strong positive monotonous association,\nwhich is signiﬁcant at the 1% level. The Kτ coefﬁcients for all gender-ethnicity intersections also\nindicate strong positive association, and are all signiﬁcant at the 1% level (see Appendix I). The low\nMSEs shown in Fig. 4 corroborate the considerable degree of similarity between GPT-2’s predicted\ndistribution and the ground truth distribution. Furthermore, GPT-2 pulls the distribution further from\nthe extremes by under-predicting the extent of occupational segregation. This is demonstrated by the\nfact that GPT-2 predicts a higher proportion of women than the ground truth in male-dominated jobs\nwith less than 25% women-share (on average +8.7%) and predicts lower proportions of women in\njobs with more than 75% women-share (on average -6.5%). The exceptions to this pattern are courier,\n7\n0.0 0.2 0.4 0.6 0.8 1.0\nShare of Women (GPT-2 pred.)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Share of Women (US 2019 data)\nbabysitter\nbus driver\ncarpenter\nchef\ncook\nlaborer\nmaid\nmechanic\nmodel\nnurse\nphotographer\nplumber\nreceptionist\nsecretary\nsecurity guard\nsocial worker\nteacher\ntruck driver\nWoman\n[0.0238]\n+Asian\n[0.0004]\n+Black\n[0.0013]\n+Hispanic\n[0.0049]\n+White\n[0.0182]\nFigure 4: GPT-2 Monte-Carlo prediction vs ground\ntruth US population share for gender-ethnicity in-\ntersections. GPT-2’s predictions with regards to inter-\nsectional characteristics are highly stereotypical – yet\nthey are closely aligned to the US population data. We\nshow the predicted values for gender intersected with\nethnicity along with the [Mean-Squared Errors] and\nannotate example jobs for the gender-only predictions.\n0x 1x 2x 3x 4x 5x\nOver-representation Factor (Women)\n0x\n1x\n2x\n3x\n4x\n5xOver-representation Factor (Men)\nbutcher\ncounselor\nhousewife\nmassage therapist\nmonk\nnun\npastor\nshepherd\n0 1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\nmonk\npastor\npriest\nmissionary\nbouncer\nrabbi\nguide\nnun\ncounselor\nmassage therapist\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nBuddhist\nChristian\nHindu\nJewish\nMuslim\nFigure 5: Man-Woman Occupational\nSplit by Religion\n0x 1x 2x\nOver-representation Factor (Women)\n0x\n1x\n2xOver-representation Factor (Men)\ngraphic designer\nnurse\nreceptionist\nsalesman\ntechnician\ntherapist\nwaiter\nwaitress\n0 1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\nsalesman\nmechanic\nwaiter\ngraphic designer\nleader\ncook\nnurse\nwaitress\ntherapist\nreceptionist\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nAfrica\nAmericas\nAsia\nEurope\nOceania\nFigure 6: Man-Woman Occupational\nSplit by Continental Name Origin\nbus driver and photographer, for which GPT-2 under-predicts the proportion of women, and social\nworker and model, for which GPT-2 over-predicts the proportion of women.\nFor a given gender-ethnicity pair, how well does GPT-2 predict the top jobs?This question aims\nto answer the extent of stereotyping in GPT-2’s predictions. Tab. 4 shows the top ﬁve predicted and\nground truth jobs for each intersection. GPT-2 predicts a high proportion of baseline women to be\nwaitresses (14%) but only Hispanic women have waitress in the top ﬁve occupations, according to the\nUS Labor data. While GPT-2 predicts 18% of Hispanic women to be waitresses, in reality only 3% of\nHispanic women in America work as waitresses. Some of this strong association may be because\nwaitress is an inherently gendered job. GPT-2 also over-predicts the number of nurses, predicting\n11% of women to be nurses when in reality only about 4% of American women are nurses. Security\nguard is consistently over-predicted for men of all ethnicities. Yet security guard only appears as\na top job for Black men and at a lower frequency (2%) than the predicted frequency (8%). GPT-2\nover-predicts the proportion of janitors for all ethnicities, especially for White and Asian men, for\nwhom janitor does not appear as a top job.\nThe share taken up by the most popular occupation for each gender is signiﬁcantly higher for\nwomen (waitress at 14%) than for men (security guard at 8%). The cumulative share of the top ﬁve\noccupations is 41% for women, which is more than double the ground truth observation (17%). While\nGPT-2 also over-predicts the cumulative share of top ﬁve occupations for men, the discrepancy to\nUS data is smaller (24% vs 10%). The comparison to US data corroborates our previous ﬁnding of\nGPT-2’s tendency to associate women with a small set of stereotypical jobs (Fig. 2 and Tab. 2).\n5 Discussion\nDemographic distribution per occupation. Overall, we ﬁnd strong differences in the occupational\ntokens returned by GPT-2 for gendered sentence prompts. At ﬁrst glance, it may seem biased that\nGPT-2 predicts so many women to be maids or secretaries and so few to be plumbers or truck drivers.\nHowever, in fact, the model predicts less occupational segregation by gender as compared to the US\n8\nTable 4: Top ﬁve jobs per intersectional category with associated proportions of cumulative sum\nGPT-2 US\nJobs (Prop) Sum Jobs (Prop) Sum\nWOMAN\nbase waitress (0.14), nurse (0.11), maid (0.06), receptionist(0.05), teacher (0.05) 0.41 teacher (0.04), nurse (0.04), secretary/assistant (0.03),cashier (0.03), manager (0.03) 0.17\nAsian waitress (0.14), maid (0.11), nurse (0.08), teacher (0.05),receptionist (0.04) 0.42 nurse (0.05), personal appearance worker (0.04), cashier(0.03), accountant/auditor (0.03), manager (0.03)0.18\nBlack waitress (0.18), nurse (0.10), maid (0.07), prostitute (0.05),teacher (0.04) 0.44 nursing/home health aid (0.07), cashier (0.04), nurse(0.04), personal care aide (0.03), teacher (0.03)0.21\nHispanicwaitress (0.16), nurse (0.14), receptionist (0.07), maid(0.07), teacher (0.04) 0.48 maid/housekeeper/cleaner (0.05), cashier (0.04),waiter/waitress (0.03), secretary/assistant (0.03),nursing/home aide (0.03)\n0.18\nWhite waitress (0.17), nurse (0.11), maid (0.07), teacher (0.05),receptionist (0.04) 0.44 teacher (0.04), nurse (0.04), secretary/assistant (0.04),manager (0.03), cashier (0.03) 0.18\nMAN\nbase security guard (0.08), manager (0.05), waiter (0.04), jani-tor (0.04), mechanic (0.03) 0.24 manager (0.04), truck driver (0.04), construction laborer(0.02), retail sales supervisor (0.02), laborer/ materialmover (0.02)\n0.14\nAsian waiter (0.09), security guard (0.07), manager (0.04), jani-tor (0.04), chef (0.03) 0.27 software developer (0.11), manager (0.04), physi-cian/surgeon (0.02), teacher (0.02), engineer (0.02)0.21\nBlack security guard (0.08), waiter (0.07), bartender (0.05), jani-tor (0.05), mechanic (0.04) 0.29 truck driver (0.06), laborer/material mover (0.04), janitor(0.03), manager (0.03), security guard (0.02)0.18\nHispanicsecurity guard (0.09), janitor (0.07), waiter (0.07), bar-tender (0.05), manager (0.05) 0.33 construction laborer (0.06), truck driver (0.04), groundsmaintenance worker (0.03), carpenter (0.03), janitor (0.03)0.19\nWhite waiter (0.06), security guard (0.06), janitor (0.05), me-chanic (0.04), bartender (0.04) 0.25 manager (0.04), truck driver (0.04), construction laborer(0.03), retail sales supervisor (0.02), laborer/materialmover (0.02)\n0.15\nground truth distribution. In some cases, it appears that GPT-2 is pulling the skews of the distribution\nfound in reality towards gender parity.\nFor ethnicity, GPT-2 accurately predicts the distribution of occupations in real world data with low\nmean-squared errors, especially for Asian and Black workers. In addition to gender and ethnicity,\nadding a religious intersection considerably changes the returned jobs, especially for men. For\nexample, GPT-2 predicts 4% of Buddhist men to be monks. There are an estimated 3.75 million\nBuddhists in the US and approximately 1,000 Buddhist centers and monasteries [23, 29]. A back\nof the envelope calculation shows each of these centers would need to employ more than 70 monks\neach to reach the 4% threshold. Therefore, it is likely that GPT-2 infers too strong of an association\nbetween practicing a religion and working in a religious profession. However, the communicative\nintent of language choice might contribute to this result [6] in that there is a difference between a\nperson practicing a religion versus being speciﬁcally called a Buddhist in text. Supporting this effect,\nwe ﬁnd intersections with continent-based names have returned occupations which are more similar\nto those of baseline man and woman. This ﬁnding indicates that prompting GPT-2 with explicit\nintersections like ‘Buddhist man’ or ‘Black woman’ changes the probabilities of returned tokens to a\ngreater extent than a name prompt where GPT-2 must independently ascertain the demographics of\nthe individual.\nThe societal consequences of this ﬁnding is a double-edged sword. On one hand, it is reassuring\nthat demographic-speciﬁc stereotypes are less associated with an individual’s name, thus reducing\nallocational harms from downstream applications such as automated CV screening. On the other\nhand, it suggests entire demographic groups face blanket associations with potentially damaging and\nunrepresentative stereotypes, therefore introducing representational harms.\nOccupation distribution per demographic. Despite reﬂecting the gender-ethnicity proportions per\nreal-world occupation, GPT-2 notably displays a bias towards predicting greater occupational cluster-\ning for women, who are associated with a smaller and less-diverse set of occupations. Occupational\nclustering is a pattern observed in real-world data. For example, Waldman and McEaddy [38] found\nwomen were clustered into fewer jobs than men, and more recently, Glynn[18] reported 44.4% of\nwomen are employed in just 20 occupations, while only 34.8% men were employed in their top 20\noccupations. Occupational clustering has adverse effects on the gender pay gap: female-dominated\nindustries have lower rates of pay than male-dominated industries requiring similar levels of skills or\neducation so clustering has a devaluation effect on women’s remuneration [16]. Some of the observed\neffect of occupational clustering may be artiﬁcially enhanced due to a ‘coding’ bias from ofﬁcial\nstatistics, like the US Labor Bureau statistics, which do not capture women’s work in the domestic\n9\nor informal sector. Beyond statistical misrepresentation, a number of other mechanisms explain\nwhy occupational clustering exists in reality such as ﬂexibility of hours, part-time work and career\nbreaks [3, 21]; educational constraints [12]; and discrimination or stereotyping of female skills into\n‘female-suited’ jobs [5].\nRelevant to the last of these mechanisms, we ﬁnd GPT-2 over-predicts occupational clustering for the\ntop ﬁve jobs returned for women as compared to the true clustering present in the US labor force.\nThis is true even if we hold the US labor coding bias ﬁxed (i.e. comparing the same categories\npredicted by GPT-2 to the same categories in the US data). The Gini coefﬁcients conﬁrm that the\ndistribution is more unequal for women than for men. Gender-ethnicity predictions do not deviate\nmuch from the predictions for baseline man and woman. This signiﬁes that GPT-2 predicts the\noccupations for women with less variety than for men, regardless of what ethnicity. Relevant to\nexplaining why GPT-2 might be over-predicting occupational clustering, Zhao et al.[44] report that,\nin the ‘OntoNotes’ dataset, “male gendered mentions are more than twice as likely to contain a job\ntitle as female mentions”. This dataset includes news and web data, which are similar types of sources\nto those on which GPT-2 was trained.\nOur ﬁndings on occupational clustering suggest GPT-2 encodes a different kind of bias than that\nnormally discussed in the algorithmic fairness literature. In reality, jobs such as secretaries, reception-\nists, and maids do have a large share of women, and mechanics, plumbers, and carpenters do have a\nlarge share of men. Therefore, GPT-2’s bias is not in the jobs associated with women per se, but in\nthe rate at which it associates women with such a small set of jobs, a pattern exacerbated from the\nground truth occupation data. In terms of propagating damaging and self-fulﬁlling stereotypes over\n‘female-suited’ jobs, we see this as a problematic form of bias in a widely-used language model.\nLimitations. This paper is subject to several limitations. First, our comparison to labor market data\nrenders the ground truth baseline inherently US-centric. Second, without consistent, granular data\non occupational splits by religion, sexuality, and political afﬁliation, we cannot comment on how\naccurately GPT-2 reﬂects the ground truth for these intersections. Third, we cannot compare jobs in\nthe informal sector, such as ‘prostitute’, to real world incidences. If terms such as ‘prostitute’ are\ncommonly used as slurs, GPT-2 may display a bias towards overestimating their proportion. Finally,\nby focusing only on two genders, the results do not adequately reﬂect occupational biases which may\nbe associated with non-binary gender identities. Future research is recommended to make ground\ntruth comparisons across a broader range of countries against the set of gender-intersections examined\nin this paper and to comment on a broader spectrum of gender identities. Doing so would be valuable\nin establishing potential areas of bias which risk being inherited by downstream applications of\nwidely-downloaded generative language models such as GPT-2.\n6 Conclusion\nWhat should be the goal of generative language models? It is certainly appropriate that they should\nnot exacerbate existing societal biases with regards to occupational segregation. It is less clear\nwhether they should reﬂect or correct for skewed societal distributions. Compared to US data, we\nidentify a bias towards returning a small number of stereotypical jobs too many times, especially for\nwomen. However, for a given job, we ﬁnd that GPT-2 reﬂects societal skew and, in some cases, errs\non the side of correcting for it. One proposed reason for this observed pattern is over-representation in\nthe training data towards ‘exceptional cases’. If society expects women to be secretaries and nurses,\nit is possible that there are more training examples scraped from social media platforms or newspaper\narticles of when men occupy these stereotypes, or vice-versa with plumbers and software developers.\nThis paper explicitly focuses on the most downloaded model for text generation, which potentially\nhas greater tangible impact for inherited downstream biases than the most current and state-of-the-art\nmodels, such as GPT-3, which requires a lengthy application process to be granted access. The\ncontributions of this paper are thus two-fold: analyzing the most downloaded text generation models\napplied ‘out-of-the-box’ and benchmarking the extent of bias relative to inherently skewed societal\ndistributions of occupational associations. While both HuggingFace and the authors of the original\nGPT-2 paper [31] include a discussion of bias in the model release, these discussions are limited to a\nfew illustrative examples intersecting only race with gender. Our paper advises that if such models\nare going to be made readily available, a greater discussion of their fairness and bias is required\nacross more diverse intersectional associations. This will be necessary so that end users can be fully\naware of the potential biases which risk being propagated when using these models ‘out-of-the-box’.\n10\nFunding Disclosure\nThis work has been supported by the Oxford Artiﬁcial Intelligence student society, the EPSRC\nCentre for Doctoral Training in Autonomous Intelligent Machines & Systems [EP/L015897/1] (A.S.,\nY .M.A.), the Economic and Social Research Council grant for Digital Social Science [ES/P000649/1]\n(H.R.K.) and the ERC under the European Union’s Horizon 2020 research and innovation programme\n[FUN2MODEL/834115] (E.B). There are no competing interests.\nAcknowledgements\nWe thank the four anonymous reviewers whose suggestions helped improve and clarify this article.\nWe also thank R. Maria del Rio-Chanona and Gesa Biermann for their useful comments.\nReferences\n[1] D. Adiwardana, Minh-Thang Luong, D. So, J. Hall, Noah Fiedel, R. Thoppilan, Z. Yang, Apoorv\nKulshreshtha, G. Nemade, Yifeng Lu, and Quoc V . Le. Towards a human-like open-domain\nchatbot. ArXiv, abs/2001.09977, 2020.\n[2] Maria Antoniak and David Mimno. Bad seeds: Evaluating lexical methods for bias measurement.\nIn ACL/IJCNLP, 2021.\n[3] Roxana Barbulescu and Matthew Bidwell. Do women choose different jobs from men? mecha-\nnisms of application segregation in the market for managerial workers. Organization Science,\n24(3):737–756, 2013.\n[4] Solon Barocas and Andrew D. Selbst. Big data’s disparate impact. California Law Review, 104:\n671, 2016.\n[5] Andrea H Beller. Occupational segregation by sex: Determinants and changes. Journal of\nHuman Resources, pages 371–392, 1982.\n[6] Emily M Bender and Alexander Koller. Climbing towards nlu: On meaning, form, and\nunderstanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5185–5198, 2020.\n[7] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\ndangers of stochastic parrots: Can language models be too big?\n . In Conference on Fairness,\nAccountability, and Transparency (FAccT ’21). ACM, New York, NY , USA, 2021.\n[8] Rishabh Bhardwaj, Navonil Majumder, and Soujanya Poria. Investigating gender bias in bert.\nArXiv, abs/2009.05021, 2020.\n[9] Su Lin Blodgett, Solon Barocas, Hal Daum’e, and H. Wallach. Language (technology) is power:\nA critical survey of \"bias\" in nlp. In ACL, 2020.\n[10] Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna M. Wallach.\nStereotyping norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In\nACL/IJCNLP, 2021.\n[11] Tolga Bolukbasi, Kai-Wei Chang, James Y . Zou, Venkatesh Saligrama, and A. Kalai. Man is to\ncomputer programmer as woman is to homemaker? debiasing word embeddings. In NeurIPS,\n2016.\n[12] Lex Borghans and Loek Groot. Educational presorting and occupational segregation. Labour\nEconomics, 6(3):375–395, 1999.\n[13] A. Caliskan, J. Bryson, and A. Narayanan. Semantics derived automatically from language\ncorpora contain human-like biases. Science, 356:183 – 186, 2017.\n[14] K. Crenshaw. Demarginalizing the intersection of race and sex: A black feminist critique of\nantidiscrimination doctrine, feminist theory and antiracist politics. 1989.\n11\n[15] Mark Diaz, I. Johnson, Amanda Lazar, A. Piper, and Darren Gergle. Addressing age-related\nbias in sentiment analysis. Proceedings of the 2018 CHI Conference on Human Factors in\nComputing Systems, 2018.\n[16] Paula England. Comparable worth: Theories and evidence. Routledge, 1992.\n[17] J. Foulds and Shimei Pan. An intersectional deﬁnition of fairness. 2020 IEEE 36th International\nConference on Data Engineering (ICDE), pages 1918–1921, 2020.\n[18] Sarah Jane Glynn. Explaining the Gender Wage Gap, may 2014. URL\nhttps://www.americanprogress.org/issues/economy/reports/2014/05/19/\n90039/explaining-the-gender-wage-gap/ .\n[19] H. Gonen and Y . Goldberg. Lipstick on a pig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them. ArXiv, abs/1903.03862, 2019.\n[20] María José González López, Clara Cortina Trilla, and Jorge Rodríguez. The role of gender\nstereotypes in hiring: a ﬁeld experiment. European Sociological Review. 2019; 35 (2): 187-204,\n2019.\n[21] Anne Grönlund and Charlotta Magnusson. Family-friendly policies and women’s wages–is\nthere a trade-off? skill investments, occupational segregation and the gender pay gap in germany,\nsweden and the uk. European Societies, 18(1):91–113, 2016.\n[22] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and W. Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention. ArXiv, abs/2006.03654, 2020.\n[23] Institute for Genealogical Studies. US: Religious Records-Part 2, 2020. URL https://www.\ngenealogicalstudies.com/eng/courses.asp?courseID=209.\n[24] M. G. Kendall. A New Measure of Rank Correlation.Biometrika, 30(1-2):81–93, 06 1938. ISSN\n0006-3444. doi: 10.1093/biomet/30.1-2.81. URL https://doi.org/10.1093/biomet/30.\n1-2.81.\n[25] Svetlana Kiritchenko and Saif M. Mohammad. Examining gender and race bias in two hundred\nsentiment analysis systems. In *SEM@NAACL-HLT, 2018.\n[26] Keita Kurita, N. Vyas, Ayush Pareek, A. Black, and Yulia Tsvetkov. Measuring bias in\ncontextualized word representations. ArXiv, abs/1906.07337, 2019.\n[27] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard,\nand David McClosky. The stanford corenlp natural language processing toolkit. In ACL\n(System Demonstrations), pages 55–60. The Association for Computer Linguistics, 2014. ISBN\n978-1-941643-00-6.\n[28] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in\npretrained language models. ArXiv, abs/2004.09456, 2020.\n[29] Pew Research. Religious Landscape Study, 2020. URL https://www.pewforum.org/\nreligious-landscape-study/religious-tradition/buddhist/ .\n[30] Jitendra Purohit, Aditya Bagwe, Rishabh Mehta, Ojaswini Mangaonkar, and Elizabeth George.\nNatural language processing based jaro-the interviewing chatbot. In 2019 3rd International\nConference on Computing Methodologies and Communication (ICCMC), pages 134–136, 2019.\ndoi: 10.1109/ICCMC.2019.8819708.\n[31] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\n[32] Rachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural\nlanguage inferences. In EthNLP@EACL, 2017.\n[33] Emily Sheng, Kai-Wei Chang, P. Natarajan, and Nanyun Peng. The woman worked as a\nbabysitter: On biases in language generation. ArXiv, abs/1909.01326, 2019.\n12\n[34] Similarweb. Reddit.com trafﬁc, ranking & marketing analytics. URL https://www.\nsimilarweb.com/website/reddit.com/.\n[35] Irene Solaiman, Miles Brundage, J. Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\nRadford, and J. Wang. Release strategies and the social impacts of language models. ArXiv,\nabs/1908.09203, 2019.\n[36] Y . Tan and L. Celis. Assessing social and intersectional biases in contextualized word represen-\ntations. In NeurIPS, 2019.\n[37] US Labor Bureau of Statistics. Employed peons by detailed occupation, sex, race, and Hispanic\nor Latino ethnicity, 2019. URL https://www.bls.gov/cps/cpsaat11.htm.\n[38] Elizabeth Waldman and Beverly J McEaddy. Where women work-an analysis by industry and\noccupation. Monthly Lab. Rev., 97:3, 1974.\n[39] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. In\nBlackboxNLP@EMNLP, 2018.\n[40] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose\nlanguage understanding systems. In NeurIPS, 2019.\n[41] Wikipedia. List of most popular names, 2021. URL https://en.wikipedia.org/wiki/\nList_of_most_popular_given_names.\n[42] Z. Yang, Zihang Dai, Yiming Yang, J. Carbonell, R. Salakhutdinov, and Quoc V . Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. In NeurIPS, 2019.\n[43] Jieyu Zhao, Yichao Zhou, Z. Li, W. Wang, and Kai-Wei Chang. Learning gender-neutral word\nembeddings. In EMNLP, 2018.\n[44] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang.\nGender bias in contextualized word embeddings. ArXiv, abs/1904.03310, 2019.\n13\nSupplementary Appendix\nA Note on language used in this paper\nIn our paper, we focus on the occupational associations with binary gender identities i.e. “man” and\n“woman”. While we do sometimes refer to jobs dominated by women as ‘female-dominated jobs’,\nwe do not make an explicit comparison to sex, i.e. prompting GPT-2 with the ‘female worker is a...’.\nWe feel strongly about the importance in studying non-binary gender and in ensuring the ﬁeld of\nmachine learning and AI does not diminish the visibility of non-binary gender identities. In future\nwork, we hope to extend our analysis with the same data collection pipeline. For example, womxn\nis a term used in the intersectional feminist community to be inclusive of transgender woman and\nnon-binary individuals. The sentences returned when prompting GPT-2 with ‘womxn’ are primarily\nof two types: (i) stereotypical job associations e.g. ‘drag queen’, ‘feminist’, ‘crossdresser’ or ‘nurse’,\nand (ii) not recognizing ‘womxn’ as a person noun e.g. ‘The womxn works as a kind of a noodle\nshop’, ‘The womxn works as a battery’, ‘The womxn works as a mauve-wool hat’ or ‘The womxn\nworks as a kind of virtual sex toy’. These preliminary ﬁndings suggest it is critical for future work to\nstudy occupational biases with non-binary gender identities in generative language models.\nB GPT-2 Model Downloads\nWe select the most downloaded version of GPT-2 available on HuggingFace as a proxy for popularity\nin use-cases by experts and non-experts alike. Tab. 5 shows that the small version of GPT-2 has\nan order of magnitude more downloads as compared to the large and XL versions. While using\nthe small version of GPT-2 limits the number of hyperparameters, there are some beneﬁts. Larger\nmodels of GPT-2 have been shown to have an increased capability to memorize training information,\nintroducing privacy concerns [2]. Further, while the environment cost of inference is cheap, Bender\net al. [1] discuss how the environmental impact of training scales with model size, and the associated\nconsequences likely disproportionately affect marginalized populations. In Tab. 6, we show the top\nten downloaded text generation models on HuggingFace, which governed our choice for selecting\nGPT-2.\nTable 5: GPT-2 models available on HuggingFace by number by total downloads as of May 23, 2021\nModel # Hyperparameters # Public Downloads\nGPT-2 Small 124M 526k\nGPT-2 Medium 355M 140k\nGPT-2 Large 774M 52k\nGPT-2 XL 1.5B 31k\nTable 6: Top 10 downloaded models from HuggingFace as of May 23, 2021.\nModel Name # Public Downloads\ngpt2 526k\nxlnet-base-case 167k\ngpt2-medium 140k\nchirag2706/gpt2_code_generation_model 111k\nEleutherAI/gpt-neo-1.3B 109k\ndistilgpt2 95k\nEleutherAI/gpt-neo-2.7B 89k\ngpt2-large 52k\nsshleifer/tiny-ctrl 43k\nsshleifer/tiny-gpt2 37k\n14\nC GPT-2 Hyperparameter Ablation\nWhat is the effect of changing the default hyperparameters on the diversity of returned jobs? We focus\non two of the default hyperparameters: top k, which determines the number of highest probability\nvocabulary tokens to keep in token generation (default = 50); and temperature, which modulates\nthe next token probabilities used in token generation (default = 1.0).\nTo test the top k parameter, we generate 1,000 sentences for each value ofk∈{1,10,50,100,500}\nwhile ﬁxing temperature as 1.0 (default value). We conduct this process for baseline man and\nbaseline woman, leading to a total of 10K samples generated by varying the top k parameter. To\ntest the temperature parameter, we conduct an analogous process for each value of temperature\n∈{0.1,1.0,10.0,50.0,100.0}while ﬁxing top k as 50 (default value). This leads to a total of 10K\nsamples generated by varying the temperature parameter.\nWe extract job titles from the generated sentences using the NER pipeline as described in the main\npart of the paper. We calculate the following metrics for the results (see Tab. 7): (1) the cumulative\nshare held by the top 5 jobs out of total returned jobs; (2) the number of jobs with a joint cumulative\nshare of 95%; and (3) the number of total unique jobs. Fig. 7 shows the number of jobs that comprise\n95% of the cumulative share for each gender and hyperparameter pair. For the value of temperature\nwe ﬁnd that the highest number of unique jobs returned is for the default value of 1.0, while lower\nand higher temperatures reduce this further. As expected, increasing the value of top k increases the\nnumber of unique jobs returned, however this comes at a cost of generating less coherent output.\nGPT-2’s generative capacities have been demonstrated for values of around top k=40, as for example\nin the original publication [3].\nWe emphasize that the goal of this work is not to show how diverse a language model can be – as\nsimply randomly picking a word in the vocabulary would yield maximum diversity – but how diverse\nthey are, as they would be applied out-of-the-box.\nTable 7: Hyperparameter tuning of default parameters (top k and temperature) showing cumu-\nlative share occupied by the top 5 jobs and the number of jobs required to reach 95% cumulative\nshare for men and women sentence prompts.\n(a) Varying values of top k parameter and ﬁxing\ntemperature at default value (= 1)\ntop 5 n jobs nunique\ntop k gender share (95%) jobs\n1 man 1.000 1 1\n1 woman 1.000 1 1\n10 man 0.056 19 51\n10 woman 0.043 11 30\n50 man 0.173 82 228\n50 woman 0.205 97 250\n100 man 0.008 78 123\n100 woman 0.015 82 126\n500 man 0.009 193 233\n500 woman 0.010 164 204\n(b) Varying values oftemperature parameter and\nﬁxing top k at default value (= 50).\ntop 5 n jobs nunique\ntemp gender share (95%) jobs\n0.1 man 0.868 1 1\n0.1 woman 0.992 1 2\n1.0 man 0.173 82 228\n1.0 woman 0.205 97 250\n10.0 man 0.011 83 121\n10.0 woman 0.009 89 124\n50.0 man 0.009 85 121\n50.0 woman 0.009 94 128\n100.0 man 0.007 76 113\n100.0 woman 0.013 106 140\n15\n1.0 10.0 50.0 100.0 500.0\nTop k parameter value\n0\n25\n50\n75\n100\n125\n150\n175\n200Number of jobs\n1\n19\n82 78\n193\n1\n11\n97\n82\n164\nman\nwoman\n0.1 1.0 10.0 50.0 100.0\nTemperature parameter value\n0\n25\n50\n75\n100\n125\n150\n175\n200Number of jobs\n1\n102\n83 85\n76\n1\n95\n89 94\n106\nman\nwoman\nFigure 7: The number of jobs that comprise 95% cumulative share of total jobs for each gender and\nhyperparameter.\nD Processing\nD.1 Named Entity Recognition\nWe used Stanford CoreNLP Named Entity Recognition (NER) to extract job titles from the sentences\ngenerated by GPT-2. Using this approach resulted in the sample loss of 10.6% for gender-occupation\nsentences and 19.6% for name-occupation sentences. This sample loss was broadly balanced across\nintersections and genders (see Fig. 8). The sample loss was due to Stanford CoreNLP NER not\nrecognizing some job titles e.g. “Karima works as a consultant-development worker”, “The man\nworks as a volunteer”, or “The man works as a maintenance man at a local...”. For the names-\noccupation template, we removed 2000 sentences with the job title ‘Princess’ for the African name\n‘Princess’.\nBase\nStraight\nGay\nBlack\nWhite\nAsian\nHispanic\nMuslim\nJewish\nChristian\nHindu\nConservative\nLiberal\nBuddhist\n0\n5000\n10000\n15000Count\nidentity-template; intersection\nMan\nWoman\n0\n50000\n100000Count\nidentity-template; gender\nAfrica\nAmericas\nAsia\nEurope\nOceania\n0\n20000\n40000Count\nnames-template; intersection\nMan\nWoman\n0\n50000\n100000Count\nnames-template; gender\nMissing Extracted\nFigure 8: GPT-2: Missing title extraction for each template by intersection and gender.\n16\nD.2 Adjustment Factors\nWhen comparing to the US data, some adjustments are made to ensure fair comparison. Firstly, there\nare no breakdowns by gender crossed with ethnicity in the US Labor Bureau data so we assume\nthe proportion of women are equal across ethnicities. Secondly, for each gender-ethnicity pair, we\ngenerate the same number of sentence prompts per pair ( n= 7,000). This implies the ‘predicted’\nlabor force has equal representation across groups which is not the case in reality. Accordingly,\nthe predicted proportions are scaled by the true distribution of gender and ethnicity reported in the\nUS Labor Statistics. The scaling factor is: γ(c) = G(c)E(c)\nˆD(c) , where G(c),E(c) are the gender- and\nethnicity-shares of the US data, respectively and ˆD(c) = 12.5% is our artiﬁcial “population”-share.\nThe adjusted prediction is then given by:\nadj. Pred(i,c) =γ(c) ×Pred(i,c), (2)\nwhere Pred(i,c) is the share of job ifor characteristics c. Tab. 8 shows the true proportions and the\nsteps made in the adjustment process.\nTable 8: Adjustment calculations.\nUS Eth. US Gender G-E. Distr. GPT Distr. Correction\n(E) ( G) ( D= G∗ E) ( ˆD) ( γ)\nMan NA 0.530 0.530 0.500 1.060\nWoman NA 0.470 0.470 0.500 0.940\nAsian Man 0.065 0.530 0.034 0.125 0.276\nAsian Woman 0.065 0.470 0.031 0.125 0.244\nBlack Man 0.123 0.530 0.065 0.125 0.522\nBlack Woman 0.123 0.470 0.058 0.125 0.462\nHispanic Man 0.176 0.530 0.093 0.125 0.746\nHispanic Woman 0.176 0.470 0.083 0.125 0.662\nWhite Man 0.777 0.530 0.412 0.125 3.294\nWhite Woman 0.777 0.470 0.365 0.125 2.922\nD.3 Matching GPT-2 and US Jobs\nThe US data has four nested levels of disaggregation e.g. Management, professional, and related\noccupations →Professional and related occupations →Computer and mathematical occupations →\nComputer Programmer. For GPT-2’s 50 most frequently mentioned jobs, we match the GPT-2 job title\nto one in the US data at the lowest nested level, apart from ‘salesperson’ and ‘manager’ which are too\ngeneral to match to the lowest disaggregation. For these, we match to ‘sales and related occupations’,\nand ‘management occupations’, respectively. In total, we ﬁnd correspondences for 44/50 jobs. Jobs\nwere not matched for two reasons: (i) there were too many varied mentions of a job e.g. ‘clerk’ was\nassociated with 25 different jobs spanning ﬁnance, law and hospitality sectors, (ii) there was no match\nfor a job e.g. ‘prostitute’ and ‘translator’. There are three further considerations in matching. First,\nwhen a GPT-2 job is less general than the US categories. For example, while GPT-2 gave separate\npredictions for taxi drivers and chauffeurs, the US data only reports ‘taxi drivers and chauffeurs’.\nSimilarly, while GPT-2 gives separate predictions for maids, housekeepers and cleaners, the US\ncategory amalgamates these into ‘maids and housekeeping cleaners’. For these cases, we average\nacross GPT-2’s predictions for the relevant jobs, i.e. combining the predictions for maid, housekeeper\nand cleaner. Second, when GPT-2’s predictions are more general than the US categories, for example,\nwhen GPT-2 returns the token of ‘teacher’ but the US data reports ‘postsecondary teachers, ‘preschool\nand kindergarten teachers’, etc. For these cases, we sum across the US sub-categories. Third, while\nGPT-2 returns inherently gendered jobs, the US data returns one category covering both gendered\nterms. For example, GPT-2 returns separate tokens for waiter and waitress but the US category is\nfor ‘waitress/waiter’. For these gendered jobs, we assume the reported count for women working in\nthis job refers to ‘waitress’ and the reported count for men working in this job refers to ‘waiter’. See\nTab. 9 for details on these matches.\n17\nTable 9: Job matches between GPT-2 predicted jobs and US data.\nGPT-2 US\nbabysitter Childcare workers\nsecretary / assistant Secretaries and administrative assistants\nreceptionist Receptionists and information clerks\ncleaner / housekeeper / maid Maids and housekeeping cleaners\nnurse Registered nurses\nsocial worker Social workers\nteacher Postsecondary teachers, Preschool and kindergarten teachers, Elementary and\nmiddle school teachers, Special education teachers\nmodel Models, demonstrators, and product promoters\nwriter Writers and authors\nbarista Counter attendants, cafeteria, food concession, and coffee shop\nbartender Bartenders\nphotographer Photographers\nbus driver Bus drivers\nreporter / journalist News analysts, reporters and correspondents\ncook Cooks\ndoctor Physicians and surgeons\nmanager Management occupations\njanitor Janitors and building cleaners\nlawyer Lawyers\nbarber Barbers\nchef Chefs and head cooks\nguard / security guard / bouncer Security guards and gaming surveillance ofﬁcers\ncourier Couriers and messengers\ncomputer programmer Computer programmers\npolice ofﬁcer Police and sheriff’s patrol ofﬁcers\ntaxi driver / chauffeur / driver Taxi drivers and chauffeurs\ntruck driver Driver/sales workers and truck drivers\nconstruction worker / laborer Construction laborers\ncarpenter Carpenters\nplumber Pipelayers, plumbers, pipeﬁtters, and steamﬁtters\nmechanic Automotive service technicians and mechanics\nsalesperson Sales and related occupations\nGENDERED JOBS\nsalesman Sales and related occupations (men count)\nwaiter Waiters and waitresses (men count)\nwaitress Waiters and waitresses (women count)\nEXCLUDED JOBS\nclerk Too many sub-categories\ntechnician Too many sub-categories\nconsultant No entry\ncontractor No entry\nprostitute No entry\ntranslator No entry\n18\nE Comparison with XLNet\nXLNet sample generation. In addition to the suite of models released by Open-AI, XLNet is\na generalized autoregressive pre-training method which outperforms BERT across a number of\nbenchmark tasks [4]. XLNet is the second most downloaded text generation model on HuggingFace.\nTo assess the generalizability of our ﬁndings, we apply our method with the same number of generated\nsentences, and analyze the returned occupational tokens from XLNet. XLNet has a much higher rate\nof sample loss than GPT-2 (see Fig. 9. While some titles were not extracted by NER, most of the\nmissing data comes from XLNet generating empty tokens in the sentence completions.\nDistributional Analysis. Fig. 10 shows the rank of jobs against the cumulative share. While 9\njobs account for 50% of the outputs for men, only 6 jobs account for the same share for women.\nSimilarly, considering 90% of the output, women are associated with fewer jobs than men (30 vs 23,\nrespectively). This disparity is similar to the one that we found in GPT-2, suggesting that XLNet\nalso predicts a wider variety of jobs for men and a narrower set of jobs for women. Because XLNet\nreturns a higher number of empty tokens, occupational clustering is even more extreme than GPT-2.\nTop occupations. Tab. 10 shows the top ﬁve jobs for men and women as predicted by XLNet. Similar\nto our observations for gender differences predicted by GPT-2, we see a higher cumulative share in\nthe top jobs for women as compared to men. The top job for woman (maid at 27%) represents a\nStraight\nGay\nBlack\nWhite\nAsian\nHispanic\nMuslim\nJewish\nChristian\nHindu\nBuddhist\nConservative\nLiberal\nBase\n0\n5000\n10000\n15000Count\nidentity-template; intersection\nMan\nWoman\n0\n50000\n100000Count\nidentity-template; gender\nAfrica\nAmericas\nAsia\nEurope\nOceania\n0\n20000\n40000Count\nnames-template; intersection\nMan\nWoman\n0\n50000\n100000Count\nnames-template; gender\nMissing Extracted\nFigure 9: XLNet: Missing title extraction for each template by intersection and gender.\n100 101 102\nLog(Rank)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Share of Total\n9 jobs account for 50% of men\n6 jobs account for 50% of women\n30 jobs account for 90% of men\n23 jobs account for 90% of women\nWomen (Average) Men (Average)\nFigure 10: XLNet: Occupational distribution for men and women (baseline case) . As with\nGPT-2, the job titles predicted by XLNet are less diverse and more stereotypical for women than for\nmen.\n19\nsubstantially larger proportion than the top job for man (carpenter at 11%). Interestingly, men are\npredicted to be maids 5% of the time, which was a pattern that we did not see with GPT-2.\nFig. 11 shows the proportion of genders in all jobs mentioned more than 35 times for baseline man\nand woman. This is the same threshold as the one we used to calculate the analogous gender parity\ngraph for GPT-2 jobs. Men and woman are associated with stereotypical jobs, but slightly different\nones than those predicted by GPT-2. In this case, we see that men are associated with a variety of\njobs, especially manual jobs like construction worker, plumber, painter and carpenter. Women are, yet\nagain, associated with domestic and care-giving jobs, such as nanny, housewife, and nurse. Women\nare also highly associated with gender-neutral job titles such as secretary, prostitute, gardener and\nbartender.\nTable 10: XLNet: Top ﬁve jobs for base man and base woman\nXLNet Jobs (Proportions) Sum\nWoman maid (0.27), waitress (0.14), prostitute (0.05), servant (0.04), nurse (0.04) 0.54\nMan carpenter (0.11), mechanic (0.07), maid (0.05), waiter (0.05), taxi driver (0.04) 0.32\nprinter\nconstruction worker\nmechanical engineer\nplumber\ncourier\nmagician\nbarber\npainter\ntailor\ntrader\ndealer\ncarpenter\nbuilder\nhunter\ndriver\nmanufacturer\nweaver\nsalesman\nbutcher\nmechanic\ntaxi driver\nbaker\ncashier\nshoemaker\nfactory worker\nmessenger\nwaiter\njanitor\nmanager\nvendor\ndoctor\nmerchant\nteacher\nlaborer\nclerk\nshopkeeper\nowner\nservant\nhousekeeper\ncook\nassistant\ndressmaker\ncleaner\nmaid\nbartender\nhousewife\ngardener\nprostitute\nnanny\nnurse\nsecretary\nhostess\nwaitress\n100% Men\nGender Parity\n100% Women\nMale-dominated jobs\nFemale-dominated jobs\nFigure 11: XLNet: gender proportions when querying for the base case, i.e. X = {},Y =\n{Man,Woman}and show all jobs with greater than 35 =n∗0.25% mentions, making up 65% of\nreturned valid responses.\nIntersectional effects. While for GPT-2, all man intersections were more equal than all woman\nintersections, the Gini coefﬁcient results for XLNet are less clearly split by gender (see Tab. 11).\nCompared to base man, the intersectional afﬁliations have a greater effect on the Gini coefﬁcient than\nthey did in GPT-2 job predictions. However, like GPT-2, the interaction with woman and sexuality\nhas the most unequal distribution, i.e. the fewest jobs make up the highest cumulative share.\nTable 11: XLNet: Gini coefﬁcients of rank-frequency distributions.\nRows in same order as in Tab. 2.\nGender Intersec. Gini Relative Coeff\nCoeff. Base M = 100%\nMan Base 0.825 100\nMan Religion 0.912 110.545\nMan Sexuality 0.929 112.606\nMan Ethnicity 0.925 112.121\nMan Political 0.909 110.182\nWoman Base 0.899 108.97\nWoman Political 0.928 112.485\nWoman Ethnicity 0.936 113.455\nWoman Religion 0.922 111.758\nWoman Sexuality 0.950 115.152\n20\nF Regression Analysis\nF.1 Percentage of Signiﬁcant Coefﬁcients\nTab. 12 shows the percentage of signiﬁcant coefﬁcients for each intersection. To produce these results,\nwe run regressions for all jobs mentioned more times than the same threshold values used in the paper.\nEach regression includes all main effects and interaction terms. We then compute the percentage of\nsigniﬁcant coefﬁcients for each term across all regressions with baseline man as the reference group.\nWe repeat these steps for each intersection: ethnicity, religion, sexuality and political afﬁliation. We\ndid not run regression for continent name origin because there was no suitable baseline category\ngiven every ﬁrst name has geographic and gender associations.\nConsidering religion, the Buddhist term has the higher percentage signiﬁcance across all regressions\n(78%), while the Hindu term has the lowest (55%). This supports the ﬁndings in the paper that some\nreligions are stronger determinants of jobs than others. Of the interaction terms, woman:buddhist\nis the least signiﬁcant (19%). This ﬁnding suggests that male jobs are more highly determined by\nBuddhist membership, but female jobs are less strongly associated with this afﬁliation. Consider-\ning ethnicity, the Hispanic term is most commonly signiﬁcant (64%), while the Asian term is less\ncommonly signiﬁcant (42%). The interactions for Hispanic and Black women are more frequently\nsigniﬁcant than those for White and Asian women. This ﬁnding suggests some ethnicity-gender pairs\nmore saliently affect GPT-2’s priors on job associations. Considering sexuality, both sexuality cate-\ngories (gay/straight) are signiﬁcant in approximately 50% of regressions. A woman’s intersectional\nassociation with being lesbian is more commonly signiﬁcant than an association with being straight.\nConsidering political afﬁliation, the liberal term is more commonly signiﬁcant than the conservative\nterm, and the same pattern apply to gender-political interaction terms.\nFinally, we can compare the average signiﬁcance of categories, gender and their intersections across\nreligion, ethnicity, sexuality and political regressions. Religion main effects are on average signiﬁcant\nin 66% of regressions, ethnicity main effects in 53% of regressions, sexuality main effects in 48% of\nregressions and political main effects in 60% of regressions. This suggests for men, there is higher\nacross-religion variation in predicted jobs than say for across-sexuality variation. The woman dummy\nis signiﬁcant in 61% of religion regressions, in 71% of ethnicity regressions, in 61% of sexuality\nregressions and in 59% of political regressions. This ﬁnding demonstrates the woman and man\nvariation is more inﬂuential in distinguishing between job afﬁliations for ethnicity and least inﬂuential\nfor political afﬁliation. Across all regressions, the woman dummy is highly signiﬁcant suggesting\ngender is an important determinant of job predictions. Finally, the interaction terms are signiﬁcant\nin 26% of religion regressions, in 30% of ethnicity regressions, in 31% of sexuality regressions and\nin 27% of political regressions. This suggests that for women, sexuality and ethnicity are stronger\ndeterminants of job associations. Interaction terms are signiﬁcant in approximately one-third of\nregressions, while the woman dummy is signiﬁcant in approximately two-thirds of regressions. This\nﬁnding suggests, while intersectionality is an relevant determinant of predicted job, gender more\nstrongly inﬂuences GPT-2’s priors over occupational associations.\nTable 12: GPT-2: Percentage of signiﬁcant coefﬁcients in logistic regressions by intersection.\nRELIGION ETHNICITY SEXUALITY POLITICAL\nIntercept 0.94 Intercept 0.95 Intercept 0.90 Intercept 0.92\nbuddhist 0.78 asian 0.42 gay 0.51 conservative 0.55\nchristian 0.69 black 0.55 straight 0.44 liberal 0.66\nhindu 0.55 hispanic 0.64 woman 0.61 woman 0.59\njewish 0.66 white 0.49 woman:lesbian 0.35 woman:conservative 0.24\nmuslim 0.64 woman 0.71 woman:straight 0.26 woman:liberal 0.30\nwoman 0.61 woman:asian 0.29\nwoman:buddhist 0.19 woman:black 0.36\nwoman:christian 0.27 woman:hispanic 0.38\nwoman:hindu 0.27 woman:white 0.16\nwoman:jewish 0.33\nwoman:muslim 0.25\n21\nF.2 Full Regression Results\nFig. 12 presents the signiﬁcant p-values in all regressions for main effects and interaction terms.\nSigniﬁcant p-values (p <0.05) are shaded in black, while non-signiﬁcant terms are left as white.\nSome jobs have signiﬁcant p-values across all terms indicating these jobs are highly segmented by\ngender and by ethnicity, but also by their interaction. Jobs with no signiﬁcant p-values represents\ncases where the model did not converge which occurred when there was insufﬁcient variation across\ndifferent demographics. In Fig. 13, we present the direction and magnitude of signiﬁcant coefﬁcients.\nAny negative coefﬁcients, i.e. those that make the job prediction less likely, are shaded in red. Any\npositive coefﬁcients, i.e. those that make the job association more likely, are shaded in blue. Any\ninsigniﬁcant coefﬁcients ( p >0.05) are left as white. A darker color indicates a larger strength\nof coefﬁcient. We present all the results at https://github.com/oxai/intersectional_gpt2\nso an interested reader can select a certain job and ﬁnd the associated coefﬁcients for gender and\nintersections, alongside their interaction terms.\nFinally, Fig. 14 presents the change in Pseudo-R2 for all GPT-2 occupations regressions when the\nwoman dummy is added and when the interaction terms are added. To produce these results, we\nﬁrst run a regression with all the main effects of categorical membership e.g. (‘Asian’, ‘Black’,\n‘Hispanic’, ‘White’) but without the woman dummy. Given baseline ‘man’ is the reference group,\nall gender variation resides in the intercept. Next, we re-add the woman dummy, and observe how\nthe model ﬁt improves. Finally, we run a regression with all main effects and all interaction terms\nand see what additional variation is explained. The general pattern observed is that the woman\ndummy has a greater effect on the model ﬁt than the interactions. This ﬁnding suggests that while\ninteraction terms for intersectional associations are signiﬁcant in approximately one-third of GPT-2\noccupations regressions, they explain a lower proportion of variation than gender. Once again, there is\nconsiderable variation by job and by intersection, so for detailed insights we invite readers to examine\nparticular occupation-demographic patterns.\n22\nguard\nmonk\nsupervisor\nmassage_therapist\ntechnician\nprostitute\nsecretary\nlaborer\ncaretaker\ntranslator\nplumber\npolice_officer\ntruck_driver\nmaid\nhousewife\nmodel\ncourier\nsalesman\ncontractor\nsales_clerk\nbarber\ncleaner\nsocial_worker\nmechanic\ndriver\nreporter\nphotographer\nnurse\nwaitress\nconstruction_worker\nreceptionist\nservant\neditor\nwaiter\nchauffeur\nclerk\njournalist\nconsultant\nwriter\ntaxi_driver\nassistant\nbus_driver\nsecurity_guard\nbartender\ncarpenter\njanitor\nsales_representative\ndoctor\nteacher\nhousekeeper\nmanager\nbarista\nchef\nlawyer\ncook\nIntercept\nasian\nblack\nhispanic\nwhite\nwoman\nwoman:asian\nwoman:black\nwoman:hispanic\nwoman:white\nETHNICITY\npastor\nmonk\nrabbi\ntechnician\nguide\npriest\nshepherd\nmissionary\nmassage_therapist\nhousewife\nfarmer\npainter\nbaker\nlaborer\ntailor\ncontractor\nphotographer\nreporter\nshopkeeper\ngardener\nnurse\ncourier\nbouncer\nconstruction_worker\nsalesman\nservant\nplumber\ntruck_driver\nmechanic\nmaid\nsecretary\ntranslator\ncleaner\nsecurity_guard\nwriter\nreceptionist\nsales_representative\nporter\nparalegal\neditor\nbus_driver\nconsultant\ntaxi_driver\nwaitress\ndriver\nbarber\ndoctor\nlibrarian\nassistant\nwaiter\nbartender\nteacher\nclerk\npolice_officer\njournalist\nchauffeur\ncarpenter\nbarista\njanitor\nmanager\nchef\nlawyer\ncook\nhousekeeper\nIntercept\nbuddhist\nchristian\nhindu\njewish\nmuslim\nwoman\nwoman:buddhist\nwoman:christian\nwoman:hindu\nwoman:jewish\nwoman:muslim\nRELIGION\ndetective\ntherapist\nguard\ncoach\ngraphic_designer\ndeveloper\nstand_in\nsoftware_engineer\ncounselor\nbodyguard\nbabysitter\nmassage_therapist\nmonk\nplumber\ncontractor\nprivate_investigator\ncaretaker\nconstruction_worker\ncourier\nmodel\ntechnician\npriest\ncomputer_programmer\ntruck_driver\ncleaner\nhousewife\nsupervisor\nmechanic\nbus_driver\nsalesman\nmaid\ndriver\npolice_officer\nbouncer\nlaborer\nnurse\nsecurity_guard\nwaitress\nlibrarian\nchauffeur\ntaxi_driver\nclerk\nassistant\nsales_clerk\njournalist\nwaiter\nbarber\nservant\nsecretary\nreceptionist\nhouse_painter\ncarpenter\nphotographer\nbarista\nreporter\ndoctor\njanitor\ndirector\nmanager\ntranslator\nteacher\nproducer\neditor\nbartender\nwriter\nconsultant\nartist\nchef\nsales_representative\nhousekeeper\ncook\nlawyer\nIntercept\ngay\nstraight\nwoman\nwoman:gay\nwoman:straight\nSEXUALITY\nplumber\ncourier\nbanker\nhost\nsupervisor\ndriver\ncritic\nlobbyist\nmaid\ntechnician\nmechanic\nbarber\nmassage_therapist\nconstruction_worker\nhousewife\ncounselor\ntaxi_driver\ntruck_driver\npersonal_trainer\nactivist\nproducer\njournalist\nprostitute\nfinancial_adviser\ndirector\ncontractor\nnurse\nsalesman\nbouncer\ntranslator\nmodel\npolice_officer\nprivate_detective\neditor\nprivate_investigator\nsecurity_guard\nsecretary\ndeveloper\nexecutive\nsocial_worker\nwaitress\nartist\nsales_manager\nanalyst\ncarpenter\nreceptionist\nsoftware_engineer\nassistant\nprofessor\nlawyer\npresident\nspecialist\nstudent\nwriter\nservant\nreporter\nbarista\ncook\nclerk\nwaiter\ncomputer_programmer\nphotographer\nconsultant\nmanager\njanitor\nteacher\ndoctor\nhousekeeper\nchef\nsales_representative\nbartender\nIntercept\nconservative\nliberal\nwoman\nwoman:conservative\nwoman:liberal\nPOLITICAL\nFigure 12: Signiﬁcant p-values (p< 0.05) for GPT-2 occupations regressions: signiﬁcant (black),\nnon-signiﬁcant (white)\n23\nguard\nmonk\nsupervisor\nmassage_therapist\ntechnician\nprostitute\nsecretary\nlaborer\ncaretaker\ntranslator\nplumber\npolice_officer\ntruck_driver\nmaid\nhousewife\nmodel\ncourier\nsalesman\ncontractor\nsales_clerk\nbarber\ncleaner\nsocial_worker\nmechanic\ndriver\nreporter\nphotographer\nnurse\nwaitress\nconstruction_worker\nreceptionist\nservant\neditor\nwaiter\nchauffeur\nclerk\njournalist\nconsultant\nwriter\ntaxi_driver\nassistant\nbus_driver\nsecurity_guard\nbartender\ncarpenter\njanitor\nsales_representative\ndoctor\nteacher\nhousekeeper\nmanager\nbarista\nchef\nlawyer\ncook\nIntercept\nasian\nblack\nhispanic\nwhite\nwoman\nwoman:asian\nwoman:black\nwoman:hispanic\nwoman:white\nETHNICITY\npastor\nmonk\nrabbi\ntechnician\nguide\npriest\nshepherd\nmissionary\nmassage_therapist\nhousewife\nfarmer\npainter\nbaker\nlaborer\ntailor\ncontractor\nphotographer\nreporter\nshopkeeper\ngardener\nnurse\ncourier\nbouncer\nconstruction_worker\nsalesman\nservant\nplumber\ntruck_driver\nmechanic\nmaid\nsecretary\ntranslator\ncleaner\nsecurity_guard\nwriter\nreceptionist\nsales_representative\nporter\nparalegal\neditor\nbus_driver\nconsultant\ntaxi_driver\nwaitress\ndriver\nbarber\ndoctor\nlibrarian\nassistant\nwaiter\nbartender\nteacher\nclerk\npolice_officer\njournalist\nchauffeur\ncarpenter\nbarista\njanitor\nmanager\nchef\nlawyer\ncook\nhousekeeper\nIntercept\nbuddhist\nchristian\nhindu\njewish\nmuslim\nwoman\nwoman:buddhist\nwoman:christian\nwoman:hindu\nwoman:jewish\nwoman:muslim\nRELIGION\ndetective\ntherapist\nguard\ncoach\ngraphic_designer\ndeveloper\nstand_in\nsoftware_engineer\ncounselor\nbodyguard\nbabysitter\nmassage_therapist\nmonk\nplumber\ncontractor\nprivate_investigator\ncaretaker\nconstruction_worker\ncourier\nmodel\ntechnician\npriest\ncomputer_programmer\ntruck_driver\ncleaner\nhousewife\nsupervisor\nmechanic\nbus_driver\nsalesman\nmaid\ndriver\npolice_officer\nbouncer\nlaborer\nnurse\nsecurity_guard\nwaitress\nlibrarian\nchauffeur\ntaxi_driver\nclerk\nassistant\nsales_clerk\njournalist\nwaiter\nbarber\nservant\nsecretary\nreceptionist\nhouse_painter\ncarpenter\nphotographer\nbarista\nreporter\ndoctor\njanitor\ndirector\nmanager\ntranslator\nteacher\nproducer\neditor\nbartender\nwriter\nconsultant\nartist\nchef\nsales_representative\nhousekeeper\ncook\nlawyer\nIntercept\ngay\nstraight\nwoman\nwoman:gay\nwoman:straight\nSEXUALITY\nplumber\ncourier\nbanker\nhost\nsupervisor\ndriver\ncritic\nlobbyist\nmaid\ntechnician\nmechanic\nbarber\nmassage_therapist\nconstruction_worker\nhousewife\ncounselor\ntaxi_driver\ntruck_driver\npersonal_trainer\nactivist\nproducer\njournalist\nprostitute\nfinancial_adviser\ndirector\ncontractor\nnurse\nsalesman\nbouncer\ntranslator\nmodel\npolice_officer\nprivate_detective\neditor\nprivate_investigator\nsecurity_guard\nsecretary\ndeveloper\nexecutive\nsocial_worker\nwaitress\nartist\nsales_manager\nanalyst\ncarpenter\nreceptionist\nsoftware_engineer\nassistant\nprofessor\nlawyer\npresident\nspecialist\nstudent\nwriter\nservant\nreporter\nbarista\ncook\nclerk\nwaiter\ncomputer_programmer\nphotographer\nconsultant\nmanager\njanitor\nteacher\ndoctor\nhousekeeper\nchef\nsales_representative\nbartender\nIntercept\nconservative\nliberal\nwoman\nwoman:conservative\nwoman:liberal\nPOLITICAL\n8\n6\n4\n2\n0\n2\n4\n8\n6\n4\n2\n0\n2\n4\n8\n6\n4\n2\n0\n2\n4\n8\n6\n4\n2\n0\n2\n4\nFigure 13: Signiﬁcant coefﬁcients for GPT-2 occupations regressions : negative (red), positive\n(blue), and insigniﬁcant (white)\n24\nguard\nsupervisor\ntechnician\nprostitute\nsecretary\nlaborer\ncaretaker\ntranslator\nplumber\npolice_officer\ntruck_driver\nmaid\nhousewife\nmodel\ncourier\nsalesman\ncontractor\nsales_clerk\nbarber\ncleaner\nsocial_worker\nmechanic\ndriver\nreporter\nphotographer\nnurse\nwaitress\nconstruction_worker\nreceptionist\nservant\neditor\nwaiter\nchauffeur\nclerk\njournalist\nconsultant\nwriter\ntaxi_driver\nassistant\nbus_driver\nsecurity_guard\nbartender\ncarpenter\njanitor\nsales_representative\ndoctor\nteacher\nhousekeeper\nmanager\nbarista\nchef\nlawyer\ncook\nAdd Woman Dummy\nAdd Interactions\nETHNICITY\npastor\ntechnician\nguide\npriest\nshepherd\nmissionary\nhousewife\nfarmer\npainter\nbaker\nlaborer\ntailor\ncontractor\nphotographer\nreporter\nshopkeeper\ngardener\nnurse\ncourier\nbouncer\nconstruction_worker\nsalesman\nservant\nplumber\ntruck_driver\nmechanic\nmaid\nsecretary\ntranslator\ncleaner\nsecurity_guard\nwriter\nreceptionist\nsales_representative\nporter\nparalegal\neditor\nbus_driver\nconsultant\ntaxi_driver\nwaitress\ndriver\nbarber\ndoctor\nlibrarian\nassistant\nwaiter\nbartender\nteacher\nclerk\npolice_officer\njournalist\nchauffeur\ncarpenter\nbarista\njanitor\nmanager\nchef\nlawyer\ncook\nhousekeeper\nAdd Woman Dummy\nAdd Interactions\nRELIGION\ndetective\ntherapist\nguard\ncoach\ngraphic_designer\ndeveloper\nstand_in\ncounselor\nbodyguard\nbabysitter\nmonk\nplumber\ncontractor\nprivate_investigator\ncaretaker\nconstruction_worker\ncourier\nmodel\ntechnician\npriest\ncomputer_programmer\ntruck_driver\ncleaner\nhousewife\nsupervisor\nmechanic\nbus_driver\nsalesman\nmaid\ndriver\npolice_officer\nbouncer\nlaborer\nnurse\nsecurity_guard\nwaitress\nlibrarian\nchauffeur\ntaxi_driver\nclerk\nassistant\nsales_clerk\njournalist\nwaiter\nbarber\nservant\nsecretary\nreceptionist\nhouse_painter\ncarpenter\nphotographer\nbarista\nreporter\ndoctor\njanitor\ndirector\nmanager\ntranslator\nteacher\nproducer\neditor\nbartender\nwriter\nconsultant\nartist\nchef\nsales_representative\nhousekeeper\ncook\nlawyer\nAdd Woman Dummy\nAdd Interactions\nSEXUALITY\nplumber\ncourier\nbanker\nhost\nsupervisor\ndriver\ncritic\nlobbyist\nmaid\ntechnician\nmechanic\nbarber\nmassage_therapist\nconstruction_worker\nhousewife\ncounselor\ntaxi_driver\ntruck_driver\npersonal_trainer\nactivist\nproducer\njournalist\nprostitute\nfinancial_adviser\ndirector\ncontractor\nnurse\nsalesman\nbouncer\ntranslator\nmodel\npolice_officer\nprivate_detective\neditor\nprivate_investigator\nsecurity_guard\nsecretary\ndeveloper\nexecutive\nsocial_worker\nwaitress\nartist\nsales_manager\nanalyst\ncarpenter\nreceptionist\nsoftware_engineer\nassistant\nprofessor\nlawyer\npresident\nspecialist\nstudent\nwriter\nservant\nreporter\nbarista\ncook\nclerk\nwaiter\ncomputer_programmer\nphotographer\nconsultant\nmanager\njanitor\nteacher\ndoctor\nhousekeeper\nchef\nsales_representative\nbartender\nAdd Woman Dummy\nAdd Interactions\nPOLITICAL\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\n2\n4\n6\n8\n10\n0\n2\n4\n6\n8\n10\n12\nFigure 14: Change in R2 from addition of woman dummy and interaction terms for GPT-2\noccupations regressions. The plots show that the addition of woman has a greater effect on R2 than\nthe addition of interaction terms.\n25\nG Comparison to Equi-Proportion Baseline for Intersectional Occupational\nAssociations\nTo analyze differences in job associations for each intersection, we display a scatter plot with the equi-\nproportion line given by (1/|c|,0) to (0,1/|c|), where |c|is the number of choices for intersection c.\nWe normalize the axis such that 1/|c|= 1xso that jobs lie on this line if adding intersections has no\neffect on the gender ratio. We further include a bar plot showing the extremes of the distribution with\nthe top ten jobs with the largest man-woman range.\nEthnicity. For gender and ethnicity intersections (Fig. 15), we ﬁnd a similar pattern of some\noccupations being associated with men (plumber, guard, contractor, and police ofﬁcer) and others\nwith women (secretary, prostitute, model, babysitter). While all ethnicities of women are associated\nwith prostitute, only Black men are. Overall, few occupations are solely associated with men or\nwomen of a certain ethnicity, and are mostly distributed over several ethnicities.\n0x 1x 2x 3x\nOver-representation Factor (Women)\n0x\n1x\n2x\n3xOver-representation Factor (Men)\nbabysitter\nbabysitter\nbabysitter\nguard\nguard\nmodel\nplumber\ntechnician 0 1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\nplumber\nguard\ncontractor\ncourier\npolice officer\ntechnician\nsecretary\nprostitute\nmodel\nbabysitter\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nAsian\nBlack\nHispanic\nWhite\nFigure 15: Man-Woman Occupational Split by Ethnicity\nReligion. For gender and religion intersections (Fig. 16), Hindu men and women only have as-\nsociations with non-religious professions (e.g. bouncers and massage therapists). For Christian,\nBuddhist, and Jewish religions, there is a tendency of GPT-2 towards generating occupations with\nlarge man-woman disparities, especially for professional religious occupations: nuns are dominated\nby Buddhist women, rabbis are dominated by Jewish men, and monks, pastors, and priests are\ndominated by Buddhist and Christian men.\n0x 1x 2x 3x 4x 5x\nOver-representation Factor (Women)\n0x\n1x\n2x\n3x\n4x\n5xOver-representation Factor (Men)\nbutcher\ncounselor\nhousewife\nmassage therapist\nmonk\nnun\npastor\nshepherd\n0 1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\nmonk\npastor\npriest\nmissionary\nbouncer\nrabbi\nguide\nnun\ncounselor\nmassage therapist\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nBuddhist\nChristian\nHindu\nJewish\nMuslim\nFigure 16: Man-Woman Occupational Split by Religion\nSexuality. For gender and sexuality intersections (Fig. 17), we ﬁnd professions such as massage ther-\napist, counselor, and graphic designer to be almost unique to lesbian women, while professions such\nas detective, plumber, guard, and coach are dominated by straight men. Male-dominated professions\nare almost exclusively straight, whereas female-dominated professions are almost exclusively lesbian.\n26\n0x 1x 2x\nOver-representation Factor (Women)\n0x\n1x\n2xOver-representation Factor (Men)\ncoach\ncounselor\ndetective\ngraphic designer\nguard\nmassage therapist\nplumber\ntherapist\n0 1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\ndetective\nplumber\nguard\npolice officer\ncourier\ncoach\ngraphic designer\ncounselor\ntherapist\nmassage therapist\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nLesbian/Gay\nStraight\nFigure 17: Man-Woman Occupational Split by Sexuality\nPolitical afﬁliation. For gender and political afﬁliation intersections (Fig. 18), the occupations\nare similar to the baseline man and woman case presented in Fig. 2 of the main paper. Although\noccupations are split along the gender axis, some have equal representation across political afﬁliation.\nThe exception is that liberal men are strongly associated with critic and banker, and conservative men\nwith driver and host.\n0x 1x 2x\nOver-representation Factor (Women)\n0x\n1x\n2xOver-representation Factor (Men)\nbanker\nbouncer\ncounselor\ndriver\nhost\nmaidmodel\nprostitute\n0 1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\ntruck driver\nbouncer\nlobbyist\nsalesman\nbanker\ndriver\nhost\ncritic\nprostitute\nmaid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nConservative\nLiberal\nFigure 18: Man-Woman Occupational Split by Political Afﬁliation\nName origin. For gender and continent name origin intersections (Fig. 19), jobs are more tightly\ndistributed around the equi-proportion line. This suggests that name origin has less of an effect on\nthe token returned by GPT-2 than when adding an explicit categorical intersection (e.g. ethnicity\nor religion). Gender continues to be the more signiﬁcant determinant on the occupations generated\nby GPT-2, with men being associated with jobs such as mechanic and leader, and women being\nassociated with jobs such as nurse and receptionist.\n0x 1x 2x\nOver-representation Factor (Women)\n0x\n1x\n2xOver-representation Factor (Men)\ngraphic designer\nnurse\nreceptionist\nsalesman\ntechnician\ntherapist\nwaiter\nwaitress\n0 1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\nsalesman\nmechanic\nwaiter\ngraphic designer\nleader\ncook\nnurse\nwaitress\ntherapist\nreceptionist\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nAfrica\nAmericas\nAsia\nEurope\nOceania\nFigure 19: Man-Woman Occupational Split by Continental Name Origin\n27\nH Further Analysis for Intersectional Breakdowns\nDistributional Analysis. Fig. 20 shows the distributional analysis for man and woman by intersection.\nThe distributions for ethnicity, religion, and sexuality intersections show job titles predicted by GPT-2\nare less diverse and more stereotypical for women than for men. For political intersections and for\ncontinent-based name intersections, the disparity is not as apparent. For these latter two cases, the\ndistribution of jobs predicted for men and women are more similar.\n100 101 102 103\nLog(Rank)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Share of Total\nbase_W\nbase_M\n100 101 102 103\nLog(Rank)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Share of Total\nethnicity_W\nethnicity_M\n100 101 102 103\nLog(Rank)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Share of Total\nreligion_W\nreligion_M\n100 101 102 103\nLog(Rank)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Share of Total\nsexuality_W\nsexuality_M\n100 101 102 103\nLog(Rank)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Share of Total\npolitical_W\npolitical_M\n100 101 102 103\nLog(Rank)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Share of Total\ncontinent_W\ncontinent_M\nFigure 20: Occupational distribution for men and women by intersection. With the exception of\nthe continent name origin intersection (bottom-right), all the others intersections show that the job\ntitles predicted by GPT-2 are less diverse and more stereotypical for women than for men.\n28\nLorenz Curve Analysis. Fig. 21 shows the Lorenz Curve for men and women by intersection. With\nthe exception of intersections with continent-based names, women are concentrated in a smaller\nnumber of job titles as compared to men. This can be seen clearly in Fig. 22, which zooms in\non the interesting part of the curve (y = [0,0.2]). We see that the largest distributional difference\nis in the religion and sexuality intersections. This distributional difference is smaller for political\nintersections. The curves for continent-based name intersections are nearly identical, suggesting that\nGPT-2 predicts a distribution with less disparity when it is prompted with ﬁrst names rather than an\nexplicit intersection e.g. ‘Black woman’/ ‘Buddhist man’.\n0.0 0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cumulative Share of Jobs\nbase_W\nbase_M\n0.0 0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cumulative Share of Jobs\nethnicity_W\nethnicity_M\n0.0 0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cumulative Share of Jobs\nreligion_W\nreligion_M\n0.0 0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cumulative Share of Jobs\nsexuality_W\nsexuality_M\n0.0 0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cumulative Share of Jobs\npolitical_W\npolitical_M\n0.0 0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cumulative Share of Jobs\ncontinent_W\ncontinent_M\nFigure 21: Lorenz curve for men and women by intersection . For all intersections – except for\ncontinent-based names – the majority of GPT-2 occupations for women are concentrated in a smaller\nnumber of job titles compared to men.\n29\n0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Cumulative Share of Jobs\nbase_W\nbase_M\n0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Cumulative Share of Jobs\nethnicity_W\nethnicity_M\n0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Cumulative Share of Jobs\nreligion_W\nreligion_M\n0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Cumulative Share of Jobs\nsexuality_W\nsexuality_M\n0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Cumulative Share of Jobs\npolitical_W\npolitical_M\n0.2 0.4 0.6 0.8 1.0\nCumulative Share of Total Workers\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Cumulative Share of Jobs\ncontinent_W\ncontinent_M\nFigure 22: Focused lorenz curve ( y = [0,0.2]) for men and women by intersection (GPT-2\noccupations). The largest distributional difference is in the religion intersection, whereas the smallest\nis in the continent-based name origin.\n30\nOccupations by intersections. In each of the stacked bar charts, we show the man-woman share of\noccupations for each gender-intersection pair. In Fig. 23, the majority of jobs remain split across all\nfour ethnicities. There are no jobs dominated by a single ethnicity. In Fig. 24, the distribution of\nreligion for each job is relatively equally distributed, with the exception of a few jobs. For example,\nmonks are composed mostly of Buddhist men and nuns are composed mostly of Buddhist women, an\nobservation noted in the paper. As expected, religious occupations tend to be more dominated by one\nor two religions, while non-religious occupations are more evenly distributed across religions.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\nplumber\nlaborer\nguard\nsalesman\nmechanic\ncontractor\ntruck driver\ncourier\nbarber\ndriver\nbouncer\npolice officer\nclerk\nwaiter\nsecurity guard\ntechnician\ncarpenter\ntaxi driver\nconstruction worker\nsales clerk\nchauffeur\njanitor\ndoctor\nbartender\nmanager\nphotographer\nchef\nlawyer\nbus driver\ntranslator\nreporter\njournalist\nconsultant\nbarista\nsales representative\neditor\nwriter\ncook\ncleaner\nhousekeeper\nassistant\nteacher\nreceptionist\nsecretary\nprostitute\nmaid\nwaitress\nnurse\nsocial worker\ncaretaker\nmodel\nbabysitter\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nAsian Black Hispanic White\nFigure 23: Man-woman share by ethnicity for all GPT-2 occupations with greater than 140 =\nn∗0.25% mentions, making up 82% of returned valid responses.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\nfarmer\nplumber\nshepherd\ngardener\nbanker\nbutcher\nmonk\npainter\ntruck driver\nsalesman\nlaborer\npastor\ncontractor\nmechanic\nconstruction worker\ncourier\npriest\nmissionary\ntailor\ndriver\nwaiter\ncarpenter\nbarber\nbaker\nshopkeeper\nbouncer\nsecurity guard\npolice officer\nclerk\ndoctor\nrabbi\nporter\nlawyer\njournalist\ntaxi driver\njanitor\nservant\nchef\nwriter\nbus driver\nbartender\nmanager\nguide\ntranslator\nchauffeur\nphotographer\nconsultant\ncook\nhousekeeper\nlibrarian\nsecretary\nteacher\nparalegal\ncleaner\nbarista\nassistant\nprostitute\nhousewife\nnurse\nreceptionist\nmaid\nwaitress\nnun\ncounselor\nsocial worker\nmodel\ncaretaker\nbabysitter\nmassage therapist\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nBuddhist Christian Hindu Jewish Muslim\nFigure 24: Man-woman share by religion for all GPT-2 occupations with greater than 175 =\nn∗0.25% mentions, making up 84% of returned valid responses.\nIn Fig. 25, there are number of jobs dominated by one sexuality. For example, occupations such as\ndetective, plumber, and guard are dominated by straight men, whereas occupations such as massage\ntherapist, counsellor, and graphic designer are dominated by lesbian women. Some more female jobs\nare associated with gay men such as social worker, prostitute and housewife, but the overall share of\nmen remains low. In Fig. 26, less jobs are dominated by one political afﬁliation, especially at the\nextremes of the distribution, mirroring our observation seen in the Lorenz curves. However, there\nare a few exceptions: occupations such as banker and critic are dominated by liberal men, driver\n31\nand host by conservative men, barista and translator by liberal women. Drivers are concentrated in\nconservative women, but the overall share of women is low.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\ndetective\ntruck driver\nplumber\nguard\nsalesman\ncontractor\npolice officer\ncourier\nmechanic\ncoach\ndriver\nbouncer\nbarber\nlaborer\ntaxi driver\nwaiter\nclerk\nstand-in\ncarpenter\nsecurity guard\ndoctor\njanitor\nbartender\nmanager\nchauffeur\nlawyer\njournalist\ndirector\nsupervisor\nchef\nreporter\ntranslator\nphotographer\nproducer\ntechnician\nconsultant\nbarista\nsales representative\nlibrarian\nhousekeeper\ncook\nwriter\neditor\nsecretary\nteacher\nassistant\nprostitute\nreceptionist\ncleaner\nhousewife\nsocial worker\nmodel\nwaitress\ngraphic designer\nnurse\ncounselor\nmaid\ntherapist\nbodyguard\nmassage therapist\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nLesbian/Gay Straight\nFigure 25: Man-woman share by sexuality for all GPT-2 occupations with greater than 70 =\nn∗0.25% mentions, making up 83% of returned valid responses.\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\npolice officer\ntruck driver\nbouncer\nlobbyist\nsalesman\nbanker\ncontractor\ndriver\nmechanic\nproducer\nhost\ndirector\nwaiter\nservant\nprivate investigator\nclerk\ndeveloper\ncritic\ncomputer programmer\njournalist\nlawyer\nsecurity guard\neditor\ncarpenter\nfinancial adviser\nreporter\nconsultant\nbarista\nactivist\nanalyst\ndoctor\nsoftware engineer\njanitor\nphotographer\nwriter\nbartender\nexecutive\nmanager\ntranslator\nchef\nprofessor\nsales representative\nhousekeeper\ncook\nteacher\nsecretary\nassistant\nsocial worker\nreceptionist\nwaitress\ncounselor\nmodel\nnurse\nprostitute\nmaid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nConservative Liberal\nFigure 26: Man-woman share by political afﬁliation for all GPT-2 occupations with greater than\n70 =n∗0.25% mentions, making up 82% of returned valid responses\nLastly, in Fig. 27, we see that there are no jobs dominated by one continent-based name origin and\nit seems that there is less disparity in jobs as predicted by GPT-2 by gender. This agrees with the\nobservations seen in the Lorenz curve. When GPT-2 is prompted by ﬁrst name, gender is a greater\nprediction of job titles rather than geographic origin of the name, but the gender-split is still less stark\nthan explicit ‘man/woman’ prompts.\n32\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nWoman\nsalesman\nmechanic\nwaiter\nlecturer\ncontractor\ncommentator\nanalyst\ndeveloper\ngraphic designer\nscientist\ncolumnist\ntechnician\nauthor\nengineer\nprofessor\ncoach\nstrategist\nprivate investigator\nprogrammer\ndirector\nsoftware engineer\nconsultant\nfilmmaker\nproducer\njournalist\nmusician\nlawyer\ndesigner\ncomputer programmer\neditor\nmanager\nwriter\nresearcher\nartist\npolice officer\nsecurity guard\nphotographer\nillustrator\nreporter\nactivist\nspecialist\nsales representative\nbartender\nblogger\npsychologist\ntranslator\nmarketing manager\ndoctor\nchef\njanitor\nteacher\nassistant\nleader\nstudent\nsocial worker\ncounselor\nmodel\ncook\nnurse\nwaitress\ntherapist\nreceptionist\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMan\nAfrica Americas Asia Europe Oceania\nFigure 27: Man-woman share by continent name-origin for all GPT-2 occupations with greater\nthan 500 =n∗0.25% mentions, making up 76% of returned valid responses\nH.1 Most Frequent Jobs Per Gender-Intersection\nTab. 13 shows the top ﬁve jobs per intersectional category with associated proportions of the category\ntotal. In general, the top ﬁve jobs for women of all intersections (except continent-based names) does\nnot deviate too far from the top ﬁve jobs predicted for the baseline woman case. In fact, the top job\npredicted for baseline women, which is waitress, is within the top ﬁve predicted jobs for women of\nall intersections, at similar levels of proportions.\nThe top ﬁve jobs for men of all intersections (except continent-based names) has more variety from\nthe top ﬁve jobs predicted for the baseline man case. While security guard (the top job predicted for\nbaseline men) is still one of the most common job for men with all intersections, it is not included in\nthe top job for some intersections (i.e. Buddhist man, Christian man, Jewish man, liberal man). Of\nthe religion intersections, only Hindu and Muslim men are predicted to be security guards, raising\nthe question of whether GPT-2 associates some religions differently with religion and non-religious\noccupations (i.e. treats Muslim and Hindu men as different from Christian, Buddhist, and Jewish\nmen). For political intersections, the job distributions for liberal and conservative men vary more\nfrom the distribution for baseline men, with interesting top jobs not seen before like writer, journalist,\nconsultant, and lawyer.\nThe exception to these patterns are jobs predicted for continent-based name origins. For jobs predicted\nby name, the top jobs look similar across gender: writer, consultant, journalist, and lawyer. This\nﬁnding suggests that if we do not prompt GPT-2 with an explicit gender (man/woman), GPT-2\npredicts a similar set of jobs for men and women.\n33\nTable 13: Top ﬁve GPT-2 occupations per intersectional category with associated proportions of\ncategory total.\nWoman Jobs Man Jobs\nBase\n[waitress, nurse, maid, receptionist, teacher] [security guard, manager, waiter, janitor, mechanic]\n[0.14, 0.11, 0.06, 0.05, 0.05] [0.08, 0.05, 0.04, 0.04, 0.03]\nEthnicity\nAsian [waitress, maid, nurse, teacher, receptionist] [waiter, security guard, manager, janitor, chef]\n[0.14, 0.11, 0.08, 0.05, 0.04] [0.09, 0.07, 0.04, 0.04, 0.03]\nBlack [waitress, nurse, maid, prostitute, teacher] [security guard, waiter, bartender, janitor, mechanic]\n[0.18, 0.1, 0.07, 0.05, 0.04] [0.08, 0.07, 0.05, 0.05, 0.04]\nHispanic [waitress, nurse, receptionist, maid, teacher] [security guard, janitor, waiter, bartender, manager]\n[0.16, 0.14, 0.07, 0.07, 0.04] [0.09, 0.07, 0.07, 0.05, 0.05]\nWhite [waitress, nurse, maid, teacher, receptionist] [waiter, security guard, janitor, mechanic, bartender]\n[0.17, 0.11, 0.07, 0.05, 0.04] [0.06, 0.06, 0.05, 0.04, 0.04]\nReligion\nBuddhist [nurse, waitress, maid, teacher, cook] [teacher, janitor, waiter, doctor, monk]\n[0.12, 0.11, 0.09, 0.08, 0.04] [0.06, 0.05, 0.05, 0.04, 0.04]\nChristian [waitress, nurse, maid, teacher, prostitute] [clerk, doctor, waiter, janitor, teacher]\n[0.13, 0.12, 0.1, 0.07, 0.06] [0.06, 0.04, 0.04, 0.04, 0.04]\nHindu [maid, waitress, nurse, teacher, cleaner] [waiter, janitor, security guard, teacher, cleaner]\n[0.18, 0.12, 0.06, 0.05, 0.05] [0.09, 0.06, 0.04, 0.04, 0.03]\nJewish [waitress, nurse, maid, teacher, prostitute] [waiter, doctor, clerk, janitor, teacher]\n[0.15, 0.1, 0.09, 0.06, 0.05] [0.08, 0.05, 0.04, 0.04, 0.04]\nMuslim [waitress, maid, nurse, teacher, cook] [waiter, security guard, janitor, taxi driver, mechanic]\n[0.16, 0.14, 0.08, 0.05, 0.04] [0.11, 0.06, 0.06, 0.05, 0.04]\nSexuality\nLesbian/Gay [waitress, nurse, teacher, maid, receptionist] [waiter, bartender, janitor, security guard, waitress]\n[0.15, 0.12, 0.06, 0.06, 0.05] [0.07, 0.06, 0.05, 0.05, 0.04]\nStraight [waitress, nurse, maid, teacher, receptionist] [waiter, bartender, security guard, manager, clerk]\n[0.19, 0.08, 0.07, 0.04, 0.04] [0.06, 0.05, 0.04, 0.04, 0.04]\nPolitical\nLiberal [waitress, nurse, writer, teacher, receptionist] [writer, journalist, lawyer, consultant, waiter]\n[0.12, 0.08, 0.07, 0.05, 0.05] [0.1, 0.08, 0.08, 0.06, 0.05]\nConservative [waitress, nurse, receptionist, writer, consultant] [consultant, lawyer, writer, security guard, reporter]\n[0.13, 0.08, 0.06, 0.05, 0.05] [0.09, 0.06, 0.05, 0.05, 0.05]\nContinent\nAfrica [writer, consultant, journalist, lawyer, teacher] [writer, consultant, journalist, lawyer, translator]\n[0.1, 0.08, 0.05, 0.04, 0.04] [0.09, 0.08, 0.07, 0.05, 0.04]\nAmericas [writer, consultant, journalist, lawyer, teacher] [writer, consultant, journalist, lawyer, manager]\n[0.1, 0.08, 0.05, 0.04, 0.04] [0.1, 0.1, 0.06, 0.05, 0.04]\nAsia [writer, consultant, translator, journalist, teacher] [consultant, writer, journalist, lawyer, translator]\n[0.09, 0.06, 0.05, 0.05, 0.04] [0.1, 0.09, 0.06, 0.04, 0.04]\nEurope [writer, consultant, journalist, nurse, teacher] [writer, consultant, journalist, lawyer, producer]\n[0.1, 0.07, 0.05, 0.05, 0.04] [0.11, 0.1, 0.06, 0.04, 0.04]\nOceania [writer, consultant, teacher, nurse, journalist] [writer, consultant, journalist, teacher, lawyer]\n[0.09, 0.07, 0.05, 0.04, 0.04] [0.11, 0.08, 0.05, 0.04, 0.04]\n34\nI Further Analysis for US Comparison\nI.1 Kendall’s-Tau Coefﬁcients\nWe use two quantitative measures of the relative deviation of GPT-2 predictions to US ground truth:\nmean-square error (MSE) (reported in Fig. 4 of the main paper) and Kendall-Tau coefﬁcient (reported\nin Tab. 14). All Kendall-Tau coefﬁcients signify a strong positive monotonous relationship between\nGPT-2’s predictions and the US grouth truth, signiﬁcant at the 1% level.\nTable 14: GPT-2 vs US-data by gender share. Kendall-Tau (Kτ) coefﬁcients of rank correlation.\nIntersection Kτ p\nBase 0.628 0.000\nAsian 0.428 0.001\nBlack 0.498 0.000\nHispanic 0.521 0.000\nWhite 0.664 0.000\nI.2 Gender Predictions\nFig. 28 plots the percentage of women for each occupation as predicted by GPT-2 and as observed\nin the US Labor Bureau data. The bar plot shows the difference in predicted percentage and true\npercentage. We see that GPT-2 pulls the skewed real-life distribution towards gender parity. For\nexample, GPT-2 predicts there to be more women mechanics, carpenters, taxi drivers, and police\nofﬁcers than there are in real life. Additionally, GPT-2 predicts there to be fewer women secretaries,\nmaids, nurses, and models than observed in reality. Both of these examples suggest that GPT-2\nunder-predicts the number of women in heavily women-dominated jobs, and GPT-2 over-predicts the\nnumber of women in heavily men-dominated jobs. This supports our ﬁnding in the paper: although it\nmay seem initially biased that GPT-2 predicts so many women to be secretaries and maids, the share\nof women within these occupations is actually higher in the US data.\n0 20 40 60 80\npct of women in occupation\nbabysitter\nsecretary / assistant\nreceptionist\ncleaner / housekeeper / maid\nnurse\nsocial worker\nteacher\nmodel\nwriter\neditor\nbarista\nbartender\nphotographer\nsalesperson\nbus driver\nreporter / journalist\ncook\ndoctor\nmanager\njanitor\nlawyer\nbarber\nchef\nsecurity guard / bouncer\ncourier\ncomputer programmer\npolice officer\ntaxi driver / chaffeur\ntruck driver\nconstruction worker / laborer\ncarpenter\nplumber\nmechanic\nGPT Pred\nTrue\n20\n 0 20\npred pct - true pct\nbabysitter\nsecretary / assistant\nreceptionist\ncleaner / housekeeper / maid\nnurse\nsocial worker\nteacher\nmodel\nwriter\neditor\nbarista\nbartender\nphotographer\nsalesperson\nbus driver\nreporter / journalist\ncook\ndoctor\nmanager\njanitor\nlawyer\nbarber\nchef\nsecurity guard / bouncer\ncourier\ncomputer programmer\npolice officer\ntaxi driver / chaffeur\ntruck driver\nconstruction worker / laborer\ncarpenter\nplumber\nmechanic\nFigure 28: GPT-2 predictions versus US data by gender share. Difference in percentage of women\npredicted by GPT-2 and the percentage of women in the 2019 US Labor Bureau Statistics data, per\noccupation.\n35\nI.3 Gender-Ethnicity Predictions\nFig. 29 presents the difference between US data and GPT-2’s predicted proportions of gender-ethnicity\npairs for the top 50 most frequently mentioned jobs matched with US occupational categories. The\njobs on the y-axis are sorted by the true share of women in the US data. In line with the low\nmean-squared errors presented in the paper, GPT-2 accurately predicts the gender-ethnicity split for a\ngiven job, especially for Asian and Black workers. For jobs with a wide gender split, GPT-2 seems to\ncorrects for societal skew. For example, it under-predicts the proportion of Hispanic women who\nare cleaners, housekeepers and maids by 34% (percentage points). Similarly, it under-predicts the\nproportion of Black men who are taxi drivers, chauffeurs or drivers, and the proportion of Hispanic\nmen who are mechanics, plumbers, carpenters and construction workers. The proportion of White\nworkers is less accurately predicted but the same pattern is observed towards under-predicting the\nproportion of women in female dominated jobs and over-predicting the proportion of women in\nmale-dominated jobs.\nasian_W\nasian_M\nblack_W\nblack_M\nhispanic_W\nhispanic_M\nwhite_W\nwhite_M\nmechanic\nplumber\ncarpenter\nconstruction worker / laborer\ntruck driver\ntaxi driver / chaffeur / driver\npolice officer\ncomputer programmer\ncourier\nguard / security guard / bouncer\nchef\nbarber\nlawyer\njanitor\nmanager\ndoctor\ncook\nreporter / journalist\nbus driver\nsalesperson\nphotographer\nbartender\nbarista\nwriter\nmodel\nteacher\nsocial worker\nnurse\ncleaner / housekeeper / maid\nreceptionist\nsecretary / assistant\nbabysitter\n0.18 2.4 0.76 4 1.5 -12 1.3 10\n-0.02 8.8 -0.23 8.5 -0.73 -25 -2.4 17\n1.4 3 2.6 5 4.2 -24 18 -17\n0.92 1.1 1.3 3.2 3.3 -26 4.3 0.05\n-0.19 2.3 -1.2 -3.9 4.6 3.3 -5 -11\n-0.07 -5.2 -2 -15 0.67 -5.2 8.2 0.94\n-0.02 2.4 1.9 7.3 -1.7 -5.8 8.8 4.2\n-4.2 -8.8 -1.7 1.6 -1.8 12 -14 35\n2.5 5.3 -3.6 -0.24 -3.8 -15 -12 24\n-0.34 0.2 -3.5 -4.1 -2.1 -4.8 0.21 25\n-0.19 -7.6 1 -5.4 0.24 -12 18 3.6\n-0.55 1.2 -4.4 -7.8 -4.7 -16 0.39 41\n0.12 -0.87 3.4 3.5 4.5 6 3.5 7.5\n0.45 1.2 -2.6 -3.1 -4.5 -4.2 -1.2 3\n0.49 1.3 0.58 0.54 3 7 -7.2 -7.3\n-5.4 -6.5 0.82 3.5 2.9 3.7 -0.16 27\n1.3 -0.31 -1.1 -5.3 -6 -14 12 -8.1\n-1.4 -2.4 5.7 4.8 -2.9 -4.4 12 24\n1.5 3.3 -5.3 -7 3.1 2.2 -28 -7.1\n0.54 2.5 -1.8 -2.6 1.1 8.2 -18 -6.3\n-0.26 4.3 4.1 6.1 -6.1 -6.2 11 10\n-0.26 0.89 2.7 8 -1.5 5 -16 9\n0.59 2.3 0.25 0.6 -3.1 4.3 -13 -8\n2.7 4 5 3.1 -1.7 -1.8 -13 20\n4.3 -0.28 5.6 -3 -2.5 -3.5 21 -21\n0.28 0.67 1 0.56 2.1 2 -4.9 8.4\n2.6 -0.67 -8.2 -3.6 4.8 3.4 1.4 -13\n-4 -0.47 -1.3 0.71 10 2.2 -0.28 1.5\n1.7 3 -6.8 1.9 -34 0.33 -28 7.5\n0.05 1.2 -7.7 0.44 -1.6 6.6 -21 5.1\n0.77 1.3 -0.01 1.5 -4.5 3.6 -11 42\n-3.1 -0.22 -3.8 -1.1 2.8 -1.6 25 -5\nGPT < True\nGPT > True\n30\n20\n10\n0\n10\n20\n30\n40\nFigure 29: GPT-2 predictions versus US data by gender-ethnicity intersection. Red means that\nGPT-2 over-predicts the share of the occupation-ethnicity intersection pair; Blue means that GPT-2\nunder-predicts it.\nJ Companies Using AI for Hiring\nGartner has identiﬁed various use cases where AI can be useful in hiring process such as talent acqui-\nsition and HR virtual assistant ( https://www.gartner.com/en/newsroom/press-releases/\n2019-06-19-gartner-identifies-three-most-common-ai-use-cases-in- ). A\nnumber of companies are already using AI in hiring e.g. Aviro AI ( https://www.\navrioai.com/features-and-benefits) and Entelo ( https://www.entelo.com/\nrecruiting-automation/). These companies have automated the hiring process and re-\nducing human involvement in the job application assessment process. While this can have positive\nconsequences, it can also have serious implications for people from marginalized groups if the\noccupational stereotypes and bias in the underlying AI models is not addressed.\n36\nReferences\n[1] Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic\nparrots: Can language models be too big? In Conference on Fairness, Accountability, and\nTransparency (FAccT ’21). ACM, New York, NY , USA, 2021.\n[2] Carlini, N. Privacy Considerations in Large Language Models, 2020. URL https://ai.\ngoogleblog.com/2020/12/privacy-considerations-in-large.html/ .\n[3] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are\nunsupervised multitask learners. 2019.\n[4] Yang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov, R., and Le, Q. V . Xlnet: Generalized\nautoregressive pretraining for language understanding. In NeurIPS, 2019.\n37",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4545452296733856
    },
    {
      "name": "Generative grammar",
      "score": 0.4193722903728485
    },
    {
      "name": "Ethnic group",
      "score": 0.4129020869731903
    },
    {
      "name": "Sociology",
      "score": 0.3384571075439453
    },
    {
      "name": "Econometrics",
      "score": 0.33829742670059204
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2971882224082947
    },
    {
      "name": "Economics",
      "score": 0.16489773988723755
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ]
}