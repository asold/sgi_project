{
  "title": "A Systematic Review of Large Language Models in Medical Specialties: Applications, Challenges and Future Directions",
  "url": "https://openalex.org/W4409497426",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2222106110",
      "name": "Asma Musabah Alkalbani",
      "affiliations": [
        "Macquarie University"
      ]
    },
    {
      "id": "https://openalex.org/A3206177180",
      "name": "Ahmed Salim Alrawahi",
      "affiliations": [
        "University of Applied Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2113021274",
      "name": "Ahmad Salah",
      "affiliations": [
        "University of Applied Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2792996044",
      "name": "Venus Haghighi",
      "affiliations": [
        "Macquarie University"
      ]
    },
    {
      "id": "https://openalex.org/A2096237007",
      "name": "Yang Zhang",
      "affiliations": [
        "Macquarie University"
      ]
    },
    {
      "id": "https://openalex.org/A2429645631",
      "name": "Salam Alkindi",
      "affiliations": [
        "Sultan Qaboos University"
      ]
    },
    {
      "id": "https://openalex.org/A1740996049",
      "name": "Quan Z. Sheng",
      "affiliations": [
        "Macquarie University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4385827193",
    "https://openalex.org/W4390388749",
    "https://openalex.org/W4311762778",
    "https://openalex.org/W4380997513",
    "https://openalex.org/W4388014051",
    "https://openalex.org/W4387653139",
    "https://openalex.org/W4378711631",
    "https://openalex.org/W4393318738",
    "https://openalex.org/W4398223047",
    "https://openalex.org/W4404402066",
    "https://openalex.org/W4401374389",
    "https://openalex.org/W4399701665",
    "https://openalex.org/W4403087472",
    "https://openalex.org/W6609523181",
    "https://openalex.org/W3205551384",
    "https://openalex.org/W4220660761",
    "https://openalex.org/W3211439143",
    "https://openalex.org/W4392164233",
    "https://openalex.org/W4385476863",
    "https://openalex.org/W4385568225",
    "https://openalex.org/W4389061936",
    "https://openalex.org/W4389179097",
    "https://openalex.org/W4386898509",
    "https://openalex.org/W4389505505",
    "https://openalex.org/W4387564414",
    "https://openalex.org/W4388869624",
    "https://openalex.org/W3193158708",
    "https://openalex.org/W4205403018",
    "https://openalex.org/W4297839701",
    "https://openalex.org/W4383749364",
    "https://openalex.org/W4392450439",
    "https://openalex.org/W4387157142",
    "https://openalex.org/W4383483317",
    "https://openalex.org/W4283321006",
    "https://openalex.org/W4225758372",
    "https://openalex.org/W3121419306",
    "https://openalex.org/W3211844088",
    "https://openalex.org/W4389574959",
    "https://openalex.org/W4390240044",
    "https://openalex.org/W4386023761",
    "https://openalex.org/W4386173035",
    "https://openalex.org/W4381838911",
    "https://openalex.org/W4220989182",
    "https://openalex.org/W4385827730",
    "https://openalex.org/W4386448461",
    "https://openalex.org/W4392883453",
    "https://openalex.org/W4384499300",
    "https://openalex.org/W3100221118",
    "https://openalex.org/W3214654535",
    "https://openalex.org/W4384282814",
    "https://openalex.org/W4388346870",
    "https://openalex.org/W4385442373",
    "https://openalex.org/W4391613414",
    "https://openalex.org/W4389513553",
    "https://openalex.org/W4364374141",
    "https://openalex.org/W3207838037",
    "https://openalex.org/W4285020859",
    "https://openalex.org/W4392186538",
    "https://openalex.org/W3125915762",
    "https://openalex.org/W4388282759",
    "https://openalex.org/W4210898640",
    "https://openalex.org/W4220986946",
    "https://openalex.org/W3128169560",
    "https://openalex.org/W4389792145",
    "https://openalex.org/W4386257899",
    "https://openalex.org/W4366769280",
    "https://openalex.org/W4392818216",
    "https://openalex.org/W4388738157",
    "https://openalex.org/W3209254806",
    "https://openalex.org/W4224065439",
    "https://openalex.org/W4386272371",
    "https://openalex.org/W4388190222",
    "https://openalex.org/W4390064615",
    "https://openalex.org/W4388608412",
    "https://openalex.org/W4310858512",
    "https://openalex.org/W4386335674",
    "https://openalex.org/W4386248120",
    "https://openalex.org/W3210307996",
    "https://openalex.org/W4327666499",
    "https://openalex.org/W4318263284",
    "https://openalex.org/W4281560212",
    "https://openalex.org/W3168553196",
    "https://openalex.org/W4387326738",
    "https://openalex.org/W4390975604",
    "https://openalex.org/W4388169129",
    "https://openalex.org/W4386002582",
    "https://openalex.org/W4383371705",
    "https://openalex.org/W4221109207",
    "https://openalex.org/W4385667643",
    "https://openalex.org/W4389377916",
    "https://openalex.org/W4392542153",
    "https://openalex.org/W4388975763",
    "https://openalex.org/W4386151807",
    "https://openalex.org/W4389045841",
    "https://openalex.org/W3178751578",
    "https://openalex.org/W4389210014",
    "https://openalex.org/W4385250451",
    "https://openalex.org/W4391224472",
    "https://openalex.org/W4386701122"
  ],
  "abstract": "<title>Abstract</title> <bold>Background: </bold>Large Language Models (LLMs) are one of the artificial intelligence (AI) technologies used to understand and generate text, summarize information, and comprehend contextual cues. LLMs have been increasingly used by researchers in various medical applications, but their effectiveness and limitations are still uncertain, especially across various medical specialties. <bold>Objective: </bold>This review evaluates recent literature on how LLMs are utilized in research studies across 19 medical specialties. It also explores the challenges involved and suggests areas for future research focus. <bold>Methods:</bold> Two researchers performed literature searches in PubMed, Web of Science and Scopus to identify published literature from January 2021 to March 2024. The studies included the usage of LLM on performing medical tasks. Data was extracted and analyzed by five reviewers. To assess risk of bias, quality assessment was performed using the revised tool for the quality assessment of artificial intelligence-centered diagnostic accuracy studies (QUADAS-AI). <bold>Results:</bold> Results were synthesized through categorical analysis of evaluation metrics, impact types, and validation approaches across medical specialties. <underline>A total of 84</underline> studies were included in this review and mainly originated from two countries; USA (35/84) and China (16/84). Although reviewed LLM applications spread across 19 medical specialties, multi-specialty applications were demonstrated in 22 studies. Various aims for using LLMs include clinical natural language processing (31/84), supporting medical decision (20/84), medical education (15/84), diagnoses (15/84), patient management and patient engagement (3/84). GPT-based and BERT-based LLMs are most used in (83/84) studies. Despite reported positive impacts such as improved efficiency and diagnostic accuracy, challenges related to reliability, accuracy and ethics remain. The overall risk of bias was low in 72 studies, high in 11 studies and not clear in 3 studies. <bold>Conclusion:</bold> GPT-based and BERT-based LLMs dominate medical specialty applications, with over 98.8% of reviewed studies using these models. Despite their potential benefits in medical process efficiency and diagnostics, a key finding from challenges regarding accuracy was the substantial variability in performance among the LLMs. For instance, LLMs' accuracy ranged from 3% in diagnostic support to over 90% in some clinical NLP tasks. Heterogeneity in the utilization of LLMs across diverse medical tasks and contexts prevented meaningful meta-analysis, as the studies lacked standardized methodologies, outcome measures, and implementation approaches. Therefore, room for improvement remains wide for developing domain-specific LLMs using medical data and establishing validation standards to ensure reliability and effectiveness.",
  "full_text": "A Systematic Review of Large Language Models in\nMedical Specialties: Applications, Challenges and\nFuture Directions\nAsma Musabah Alkalbani \nMacquarie University\nAhmed Salim Alrawahi \nUniversity of Technology and Applied Sciences\nAhmad Salah \nUniversity of Technology and Applied Sciences\nVenus Haghighi \nMacquarie University\nYang Zhang \nMacquarie University\nSalam Alkindi \nSultan Qaboos University\nQuan Z Sheng \nMacquarie University\nArticle\nKeywords: Arti\u0000cial intelligence (AI), clinical decision support systems, large language models (LLMs),\nclinical NLP , medical specialties\nPosted Date: April 16th, 2025\nDOI: https://doi.org/10.21203/rs.3.rs-5128451/v2\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: The authors declare no competing interests.\nA Systematic Review of Large Language Models in Medical \nSpecialties: Applications, Challenges and Future \nDirections  \n \n  \nBackground: Large Language Models (LLMs) are one of the artificial intelligence (AI) \ntechnologies used to understand and generate text, summarize  information, and \ncomprehend contextual cues. LLMs have been increasingly used by researchers in \nvarious medical applications, but their effectiveness and limi tations are still \nuncertain, especially across various medical specialties.   \nObjective: This review evaluates recent literature on how LLMs are utilized in \nresearch studies across 19 medical specialties. It also explores the ch allenges \ninvolved and suggests areas for future research focus.   \nMethods: Two researchers performed literature searches in PubMed, Web of Science and \nScopus to identify published literature from January 2021 to March 2024. The studies  \nincluded the usage of LLM on performing medical tasks.   Data was extracted and analyzed \nby five reviewers. To assess risk of bias,  quality assessment was  performed using the \nrevised tool for the quality assessment of artificial intelligence ce ntered diagnostic \naccuracy studies (QUADAS-AI).  \nResults: Results were synthesized through categorical analysis of evaluation metrics, \nimpact types, and validation approaches across medical specialties. A  total of 84 \nstudies were included in this review and mainly originated f rom two countries; USA \n(35/84) and China (16/84). Although reviewed LLM applic ations spread across 19 \nmedical specialties, multi-specialty applications were demonstrated in 22 studies. \nVarious aims for using LLMs include clinical natural language proce ssing (31/84), \nsupporting medical decision (20/84), medical education (15/84), diagnoses (15/84), \npatient management and patient engagement (3/84). GPT-based and BERT-based \nLLMs are most used in (83/84) studies. Despite reported positive im pacts such as \nimproved efficiency and diagnostic accuracy, challenges relat ed to reliability, \naccuracy and ethics remain. The overall risk of bias was low in 7 2 studies, high in 11 \nstudies and not clear in 3 studies.  \nConclusion: GPT-based and BERT-based LLMs dominate medical specialty \napplications, with over 98.8% of reviewed studies using these models. Despite their \npotential benefits in medical process efficiency and diagnost ics, a key finding from \nchallenges regarding accuracy was the substantial variability i n performance among \nthe LLMs. For instance, LLMs' accuracy ranged from 3% in diagnostic support to over \n90% in some clinical NLP tasks. Heterogeneity in the utilization of LLMs across \ndiverse medical tasks and contexts prevented meaningful meta-an alysis, as the \nstudies lacked standardized methodologies, outcome measures, and implementation \napproaches.  Therefore, room for improvement remains wide for developing domain-\nspecific LLMs using medical data and establishing validation stand ards to ensure \nreliability and effectiveness.  \nTrial registration: PROSPERO (CRD42024561381); \nhttps://www.crd.york.ac.uk/prospero/display_record.php?RecordID=561381  \n  \nKeywords: Artificial intelligence (AI); clinical decision support systems; large \nlanguage models (LLMs); clinical NLP; medical specialties.   \n  \nIntroduction  \nRapid advancements in natural language processing (NLP) influenc e the domain of \nArtificial Intelligence (AI) and its subfield of Large Language Models (LLMs) (e.g., GPT \nand BERT) [1], [2]. LLMs can be defined as deep neural netwo rk models with \ntransformer-based architectures trained on large text corpora, cap able of \nunderstanding context, generating human-like text, and pe rforming complex \nlanguage tasks. The model size specifies its complexity and processin g capabilities \nand is a varying number from millions to billions of parameters, t herefore called \n“large”.  LLMs utilize complex Deep Learning (DL) architectures to  autonomously \nlearn complex linguistic patterns and semantics from unprece dented training \ndatasets making them distinct from other approaches that rely heavily on predefined \nrules and feature engineering. This enables LLMs to demonstrate hu man-like \ncapabilities in understanding and generating text, summari zing information, and \ncomprehend contextual cues with remarkable accuracy. These capa bilities have \nattracted growing interest in applying LLMs to healthcare and medicine.  \nWith applications ranging from drug discovery and development , clinical decision \nsupport [3] to medical education and licensing [4], LLMs hold significant potential for \nvarious medical specialties by improving diagnostic accuracy [5 ], optimizing \ndownstream tasks [6], and enhancing patient care [7]. LLMs have also th e ability to \ncontinuously learn from ever-evolving medical knowledge that ensure s their \nadaptability and relevance in dynamic medical environments,  thereby promising \nsustained development in clinical practice.  \nDespite increasing applications of LLMs in many medical specialties, li terature \nreported challenges and negative impacts on employing LLMs in medical practices \nand processes. These included accuracy issues [8], ethical concerns [9], inconsistent \nreliability of LLMs [5], their lack of clinical knowledge [4] and their inability to handle \nspecific tasks [10]. Reported challenges pertained to all LLMs i ncluding general-\npurpose and clinically trained LLMs. This necessitates a rigorous evaluation of LLMs' \nperformance and impact across different medical disciplines. Several systematic \nreviews on the use of LLMs for medical applications exist [11], [12]. Existing reviews \nare limited to the use of LLMs in mainly one medical specialty, missing the evaluation \nof the LLMs' effectiveness, limited or missing discussion on LLMs imp act and \nchallenges associated with the use of LLMs across various medical domains.   \nExisting systematic reviews can be categorized into three types based on their study \nfocus: 1) studies focusing on a very narrow medical field (a singl e specialty), 2) \nstudies focusing on a single task performed by LLMs, and 3) studies reviewing several \nLLMs across different medical specialties. All of the three classes can focus on a single \nLLM (e.g., GPT) or several LLMs. Examples of the first type, which reviewed LLM tasks \nwithin a single medical specialty, include [13], which studied th e performance of \nLLMs in Orthopedics, and [14], which systematically reviewed the ab ility of GPT-4 \nand Bard to pass the Fundamentals of Robotic Surgery (FRS) didactic test. The second \ntype focuses on a single task performed by a single LLM, as in [1 5], which reviewed \ndifferent versions of ChatGPT in medical licensing examinations worldwide,  not \nlimited to a specific specialty. An example of the third type is [16], where the authors \nreviewed the various applications of GPT across multiple medical spe cialties. The \ncurrent study distinguishes itself from existing systematic re views in several ways. \nFirst, its coverage extends to March 2024, while other studi es have a shorter time \nspan, such as [16], only covered periods up to September 2023. Seco nd, this study \nfocuses exclusively on top-tier journal publications, specifically WoS Q1  journals, to \nensure a focus on the highest quality papers.  \nThis systematic review considers both proprietary (e.g., GPT) and  open source (e.g., \nBERT) LLMs. This study aims to 1) assess the current uses of L LMs in medical \nspecialties and reported accuracy based on original studies, 2) explore the most \ncommon challenges of LLMs in these medical specialties, and 3) identify potential \nfuture applications of LLMs in medical specialties. The quality of the studies included \nwill be assessed for potential biases.  \nMethods  \nThis systematic review was conducted following the Preferred Reporting Items for \nSystematic Reviews and Meta-Analyses statement (PRISMA [17]). The PRISMA flow \ndiagram of the included studies is presented in Figure 1 nd its checklist is listed in \nAppendix 4. The protocol review was also registered at PROSPERO \n(CRD42024561381) and is available via \nhttps://www.crd.york.ac.uk/prospero/display_record.php?RecordID=561381. We \nassess existing literature that has LLMs implemented into a medical specialty of one \nof the applications (Diagnoses, Medical Education, Decision Support, Clinical NLP).  \n  \n  \nFigure 1. PRISMA flow diagram.  \n  \n  \nSearch Strategy   \nA comprehensive search was performed by searching PubMed, Web of Science (WoS), \nand Scopus for journal articles in English published from 2021  to March 2024. The \nsearch query was ((\"large language models\" OR LLMs OR GPT-2 OR GPT -3 OR GPT-\n3.5 OR GPT-4 OR ChatGPT OR BERT OR LAMDA OR RoBERTa OR \"Turing- NLG\" OR \n\"Turing Bletchley\" OR XLNet OR \"Transformer-XL\" OR ERNIE OR ELECT RA OR \n\nALBERT) AND (clinical OR disease OR healthcare OR medical OR patient OR diagnosis \nOR therapy OR treatment OR surgical OR oncology OR cardiology OR neurology OR \npsychological OR immunology OR genomics OR bioinformatics OR “public hea lth\")), \nas explained in Textbox 1 and detailed in Appendix 6. All re trieved records were \ninitially screened by two investigators (Alkalbani and Salah) to ensure duplicates \nwere removed and eligibility criteria were met. Discrepanc ies were either resolved \nby discussion and consensus or by a third investigator (Alrawahi).    \n  \nTextbox 1. Categories and terms applied in the search queries.  \nSearch concepts combined using “AND”  \nLarge Language models  \n Medical Specialties   \nSearch terms combined using “OR”  \n Different LLMs names and generic terms of LLMs  \n Different medical specialties as classified in [18]  \nInclusion and Exclusion Criteria   \nThis review contained all peer-reviewed journal articles that  studied the application \nof LLMs in medical specialties. The title and abstract of all retr ieved records were \nrigorously screened. The inclusion criteria included any peer-reviewed journal article \nthat employed one or more LLM to a medical specialty application . The exclusion \ncriteria included 1) non-English publications, 2) no LLM is used, 3) other types of \njournal articles than original research (reviews, editorials, r eports, commentaries, \nletters, viewpoints, reflections, notes). To further assess high-qual ity research only, \nany study that is 4) not published in a Q1 WoS journal is excluded.  \nData Extraction  \nThe main outcomes of this systematic review were to identify the medical specialties \nwhere LLMs are utilized and assess the LLMs' effectiveness and their challenges. Data \nextracted and recorded into the following study characteristics: au thors, year of \npublication, country of affiliation, objective, LLM model, med ical specialty, \napplication (Decision Support, Diagnoses, Medical Education, and Cl inical NLP) \n(available in Appendix 1), and evaluation metrics (available in Appendix 2), Data was \nindependently extracted by four reviewers (Alkalbani, Salah,  Haghighi, and Zhang). \nDiscrepancies were discussed and resolved either by consensus or by a fifth reviewer \n(Alrawahi).  \n  \nQuality assessment  \nOne of the inclusion criteria in this review was Q1 WoS journal articles to distinguish \nstudies based on the quality of their publication source. We modified QUADAS-AI [19] \nto evaluate the risk of bias of included studies in this review. The heterogeneity of \nspecialties and the rapid development of LLMs also posed c hallenges in establishing \nconsistent quality criteria across studies and therefore, we could not perform a meta-\nanalysis. The modified tool is presented in Appendix 5.  \nResults  \n  \nStudy Selection \n \nA total of 6806 records were retrieved during the initial search. 2095 duplicates were \nremoved. A further 4,448 records were removed after screening w hile 6 records \ncould not be retrieved. 257 publications were fully reviewed  for potential inclusion \nof which 84 studies only met the inclusion criteria and were included in this review.  \n  \nStudy Characteristics  \n \nThe characteristics of the included studies are analyzed in Append ix 1, which \nprovides a detailed of all reviewed studies. There was an upward trend in publication \nrates from 2021 onwards where 41/84 publications were published in 2023. This \nsuggests increasing interest in using LLMs for medical specialt ies. The analysis \nshowed that about 33% of the included studies were conducted by institutes located \nin North America (USA= 35, Canada=2), while around 31% were  affiliated with \ninstitutions in Asia (China=16, Korea=5, India=2, Japan=2, Taiwan=2, Singapore=2, \nUAE=1). Around 23% of the studies were European affiliated (Germany=6, Italy=5, \nUK=4, France=2, Spain=2, Finland=1 Greece=1, Portugal=1, Net herlands=1, \nSerbia=1). A limited number of studies were affiliate d with Australia (n= 3, 3%) and \nSouth America (Brazil=1, 1%). These distributions highlighted the globa l interest in \nutilizing LLM in the medical field.   \n  \n   \nMedical Specialties  \nThe included studies were categorized into one of 19 medical specialtie s as per \nspecialty profiles of the Association of American Medical Col leges [18]. Several \nstudies that crossed specialty boundaries or addressed broad medical practices (e.g., \ntherapy recommendations spanning ophthalmology, orthopedics, and  dermatology) \nwere classified as multi-specialty and included in this study , as long as they met the \nother inclusion criteria (peer-reviewed, Q1 WoS journal, original research).   \nMore studies (26%, n=22) used LLMs in multi-specialty than in  any other individual \nspecialty. The specialties included were Surgery (approx. 1 2%, n= 10), Neurology \n(approx. 10%, n= 8), Radiology (approx. 10%, n= 8), Emergency Medicine (7%, n=6), \nPsychiatry (7%, n=6), Epidemiology (approx. 6%, n= 5), Infectious Diseases (approx. \n5%, n= 4), Pathology (approx. 4%, n= 3), Oncology (2%, n=2) , Cardiology (1%, n=1), \nEndocrinology (1%, n=1), ENT (1%, n=1), Geriatrics (1%, n=1),  Immunology (1%, \nn=1), Medical Genetics and Genomics (1%, n=1), Ophthalmology (1 %, n=1), \nPediatrics (1%, n=1), Urology (1%, n=1), and Nephrology (1%, n=1).   \nApplication of LLMs  \nWe identified five main applications of the LLMs investigated i n the literature \nincluding clinical NLP (approx. 37%, n= 31), decision support (appr ox. 24%, n= 20), \nmedical education (approx. 18%, n= 15), diagnoses (approx. 18%, n= 15), and patient \nmanagement and engagement (approx. 4%, n= 3).  \n  \nLLMs used \nStudies either used single LLM (82%, n=69) or multiple LLMs (18%, n=15). Over half \nof the studies used GPT-Based LLMs (51%, n=43), (approx48%, n= 40) u sed Bert-\nBased LLMs while (approx. 10%, n= 8) only used other type of LLMs.  \n  \nType of LLMs  \nThe performance and reliability of LLMs depend significantly  on training and fine-\ntuning. Reviewed studies used one of three types of LLMs. 1) Gener al-Purpose are \ntrained on large general-purpose datasets rather than specific medical dataset s. 2) \nFine-tuned LLMs are pre-trained general-purpose LLMs but further trained in specific \ntasks or domains using relevant smaller datasets [20]. 3) Medical-specifi c are LLMs \ndeveloped by training the model from scratch on specific medical d atasets derived \nfrom clinical notes, electronic health records (EHRs), and medical liter ature [21]. \nMost of the included studies used out-of-the-box general-purpose LLMs (57%, n=48), \nfine-tuned LLMs (approx. 36%, n= 30) or trained their  LLMs from scratch as in \nMedical-specific LLMs (7%, n=6).  \n  \nReported Impact  \nMost of the included studies reported impacts of applying LLMs to medical specialties. \nAll studies reported one or more positive impacts. Positive impact included improved \ndiagnosis accuracy (approx. 24%, n=20), enhanced medical processe s efficiency \n(approx. 29%, n=24), assisted in better decision making (21%, n=18), supported \nmedical education (15%, n= 13), and enhanced patient care (approx. 11%, n= 9). \nReported negative impacts were accuracy issues (approx. 24%, n= 20), inconsistent \nreliability (51%, n=43), lack of clinical knowledge (approx. 13%, n= 11), ethical issues \n(approx. 4%, n=3), inability to handle certain tasks (7%, n=6 ) while no negative \nimpacts reported in (1%, n=1) of the reviewed studies.  \n  \nPerformance Evaluation  \nSeveral evaluation metrics of LLM performance were reported. Each  study used at \nleast one evaluation metric. Over half of the included studies (approx. 55%, n= 46) \nprovided accuracy related metrics, F1-score was reported in (25%,  n=21) while \nprecision and recall were used in (13%, n=11) and (approx. 12%, n= 10) respectively. \nOther evaluation measurements reported were AUC-ROC (approx. 11 %, n= 9) and \nLikert scales (approx. 10%, n= 8). Several evaluation metrics are not included in Table \n1 due to their limited usage across the reviewed papers and to m aintain the table's \nconciseness given the large number of potential metrics. Detailed evaluation metrics \nare presented in Appendix 2. The studies reviewed were very heterogeneous in terms \nof medical specialties and LLM applications demonstrated; therefore, pooled analysis \nof specificity and sensitivity cannot be done. Each of the studi es has been conducted \nwith different datasets, metrics, and evaluation methodologi es best suited for its \ncontext; hence, conducting such analyses risks compromising integrity and accuracy. \nA descriptive synthesis has thus been carried out to provide an overview.  \nThe performance of LLMs on medical tasks was evaluated across 84  studies, \ncategorized by their comparison approach: no comparison, compa rison to existing \ntools/algorithms, and comparison to human professionals. A significant  portion \n(27/84) presented LLM results without any comparative benc hmark. Among the 34 \nstudies comparing LLMs to other tools/algorithms, a clear majority (29/34) \ndemonstrated superior LLM performance, while 3 showed lower performance, and 2 \nachieved similar performance. In the 23 studies directly comparing LL Ms to human \nprofessionals, a different pattern is noticed: 11 favored human expertise, 6 indicated \nsuperior LLM performance, 5 showed equivalent performan ce, and 1 reported an \nuncertain comparison. This analysis reveals a complex landscape wher e LLMs \ndemonstrate considerable promise in some medical tasks, outperfor ming existing \ncomputational methods, but human expertise remains superior  in a substantial \nnumber of cases. Collectively, across all comparative studies (n=84), LLMs performed \nbetter in 35 studies, worse in 14 studies, achieved the same level of performance in 7 \nstudies, and the comparison was uncertain in 1 instance. While a summary of the \nLLMs’ accuracy of the included reviewed studies is provided in Table 1, a \ncomprehensive analysis results are presented in Appendix 2. \n \nTable 1: Tasks of LLMs in Medicine: Summary of Performance Analysis and Key Findings. \nApplication Task \nExamples \nDataset \nCharacteristics \nAccuracy \nrange \nKey findings \nMedical \nEducation \nPerformanc\ne in \ndifferent \nmedical \nexams \nOfficial/ \npractice \nquestion sets \n(100-861 \nquestions) \n29.4%-\n90% \n  \nGPT-4 significantly \noutperforms other LLMs \n[22], [23], [24], [25] \nHuman experts still \noutperform LLMs [26], \n[27].  \nDiagnostic \nAssistance \nDisease \ndiagnosis \nbased on \nvarious \ninputs \nsingle to \nhundreds of \nquestions/case\ns \n3%-94% \n  \nPrompt design \nsignificantly impacts AI \ndiagnostic performance \n[28]. \nLLMs are promising for \nprimary screening but \nnot complex cases [29], \n[30]. \nSignificant safety \nconcerns exist for using \ncurrent LLMs in real-\ntime. [28]  \nClinical \ndecision \nsupport \nSuggesting \nclinical \nrisks, \noutcomes, \n15 cases to \n84K \npatients/quest\nions \n27%-60%\n \n  \n- Performance is \nhighly model-\ndependent [31], [32], \n[33]. \nand \nrecommend\nations \n- LLMs show potential \nin assisting decision \nsupport but often \nlack reliability, \naccuracy, and \nreasoning of human \nexperts [27].  \n \nPatient \nManageme\nnt & \nEngagemen\nt \nAnswering \npatient \nquestions \nnotes/reports \n(100-6600) \nsets of 10-40 \nquestions \n56% - \n80% \n- LLMs have the \npotential to answer \nmedical questions \nbut patients are \nchallenged by \nmedical terms [34], \n[35] \n- LLMs often fabricate \ncitations and \nhallucinate [36]  \n- Overall information \nquality is lower \nhuman expert [37].  \nClinical \nNLP \n- \nGenerating \nconcise \nsummaries \n- identifying \nclinical \nentities \n(problems, \ntreatments, \ntests, \nmedication\ns) from \nunstructure\nd text \n- Classifying \nsentences/\nnotes based \non content \nclinical notes \n(100s-10Ks) \n clinical trial \nprotocols \n(180). \n71- >90% -  Fine-tuning can \nimprove LLMs \nperformance \nsubstantially [38], \n[39], [40]. \n- Handling long \ndocuments, complex \nreasoning, and model \nhallucinations limit \nclinical extraction \ntasks [41]. \n \nValidation Approach  \nThe performance of LLMs of the studies included in this review was e ither validated \n(82%, n= 69) or not validated (approx. 18%, n=15). The pe rformance of LLMs was \ncompared against the performance of medical professionals or validated by hu man \nexperts (33%, n= 28) or compared against other tools including othe r LLMs or \nalgorithms (approx. 49%, n=41).  \n  \nQuality assessment Results  \nAlrawahi assessed the quality and the risk of bias using the modified tool, while Salah \nvalidated the assessment with disagreements resolved through conse nsus \ndiscussions. Each of the four domains was evaluated for risk of bias as low, high, or \nunclear. The overall risk of bias was low in 72 studies, high in 11 studies and not clear \nin 1 study. The detailed results are presented in Appendix 3.  \n  \nDiscussion  \nPrincipal Findings  \nOverview and Scope of the Review  \nThis review comprehensively evaluates the effectiveness, impa cts, and challenges of \nutilized LLMs across several medical specialties by reviewing the most relevant high-\nquality (SCIE-indexed Q1 journals) studies from Web of Science, PubMed and Scopus \ndatabases. While there have been many reviews on LLM applicatio ns in healthcare \nand medical applications since 2021, only a few systematic reviews exist with a focus \non evaluating the impact of LLM applications in medical specialti es. Two reviews \nfocused on evaluating the use of ChatGPT in Gastroenterology [42], [43] while others \nassessed the performance of ChatGPT in medical examinations [44], [45], [46], [47]. \nThis systematic review, therefore, stands out as a comprehensive survey and analysis \nof the included studies to 1) assess the performance of LLMs in medi cal applications \nbased on the reported evaluation metrics such as F1-score, 2) i dentify the evidence \nthat supports or hinders the use of LLMs in medical domains ind ividually, and 3) \nidentify future directions of applying LLMs in medical specialties. The findings of this \nreview suggest that LLMs showed high acceptable accuracy in m ost of the reported \napplications. However, some concerns have been raised, such as hallucinations and \nfalse information [48].  \nThis review showed an upward trend in publications between 2021 and 2024 focused \non the application of LLMs in medical specialties. This correlates w ith the major \nrelease and widespread use of new and advanced LLMs and r eflects the increasing \nmaturity of LLMs especially ChatGPT, BERT, and domain-specific dri ven models to \nperform medical-related tasks. The maturity and advancements of LLMs are expected \nto continue expanding the role of LLMs in medical applications.     \n  \nPerformance Comparison Across Specialties  \n22 reviewed studies utilized LLMs in one or more specialties reflecting wide adoption \nacross different domains in medicine. The application reported in  [8] provided \ntherapy recommendations across multiple specialties: ophthalmology, orthopedics,  \nand dermatology. Although this suggests the flexibility of L LMs' potential to support \nmulti-specialty and found no significant difference in harmfulness ratings across the \nthree specialties, but quality scores varied on specific questions. Surgery, Neurology, \nand Radiology were leading specialties with 10, 8 and 8 st udies, respectively. In \nneurosurgery, GPT-4 excelled on written board questions [49] but  required caution \nfor patient information [37]. In orthopedics, resident perfor mance surpassed GPT-4 \non exam questions [50] while LLMs showed promise for improvin g radiology report \nreadability [51] and answering general patient queries [52].  \nLLMs can improve aspects in surgical education [49], assist in extrac ting evidence \n[53] and diagnosis from radiology reports [54], [55], while aidi ng in interpreting \nneurological data [56]. Emerging applications in psychiatry, emer gency medicine, \nand epidemiology are increasing. LLMs may improve the understandi ng and \ndiagnosis of psychiatric disorders [57]. LLMs can also help in prioritizing patient care \nby rapidly and correctly classifying patients in triage [58], [59]. However, these are \nadvanced medical specialties whose accuracy, privacy and ethical concerns should be \nconsidered [60], [61].   \n  \nKey Areas of Utilization of LLMs \n31 of the studies included in this review focused on the ap plication of clinical NLP. \nThis highlights the potential of LLM applications to analyze large volumes of \nunstructured data including clinical or nursing notes, medical records, and patient \nrecords from different medical specialties [62]. The capabili ty of LLMs to handle \nunstructured data can also facilitate their conversion to structured data and integrate \nit into electronic medical records systems and clinical decision support systems [63] \nwhich was reported in 20 reviewed studies indicating the growing interest in LLMs \nto assist medical staff. However, the performance of LLMs appears highly dependent \non the type and structure of the input medical text. LLMs per form differently when \nprocessing structured EHR data [64], [65], [66] semi-structured radi ology or \npathology reports [21], [55], unstructured clinical notes [67], medical exam questions \n[49], patient-generated text/queries [68], or scientific liter ature [69]. Challenges \narise with long documents requiring segmentation or specializ ed architectures [37], \nand performance can degrade when encountering variability in documentation styles \nor formats [70]. This suggests that LLM applicability is not unif orm across all forms \nof medical text. Therefore, careful attention to the quality of  input data should be \nconsidered when applying LLMs to clinical NLP and decision support systems. \nMedical education and diagnoses received similar focus, recog nizing the potential of \nLLMs to enhance learning and training experiences and assist in diagnostic tasks. The \nsmall number of publications in patient management and eng agement showed \nlimited focus on these applications reflecting either technical challen ges in \nimplementing LLMs or untapped potential for these applications.         \n  \nDominant LLMs and Training Approaches  \nDespite the GPT and BERT being general-purposed LLMs, they collectively dominated \nthe applications in medical specialties suggesting the leverage thei r capabilities \nwithout the need to train LLMs from scratch (trained entirely on medical datasets) or \nminimally fine-tune them. Both are perceived positively for  their ease of use and \nutility in streamlining tasks such as drafting administrative le tters [71], generating \ninitial summaries [72], or providing general medical information [35], [73].  \nThe limited presence of other LLMs indicates the maturity and a ccessibility of GPT \nand BERT in comparison to emerging LLMs. Comparative studies of ten indicate \nperformance improvements with newer model versions, such as GPT-4 over GPT-3.5 \n[49], [74], although the choice of optimal model and training strat egy appears task- \nand data-dependent [75], [76]. This rapid evolution highlig hts the fast pace of \ndevelopment in LLM capabilities but also underscores that performance benchmarks \nlast for a short time, necessitating continuous re-evaluation as LLM s are updated \n[27].  \nAnother consideration is the computational requirements of GPT and  BERT and \navailable resources that favor many medical applications to use both LL Ms rather \nthan emerging ones. The dominance of GPT and BERT LLMs also impact ed how \napplications technically utilized LLMs. Most applications (4 8/84) used pre-trained \ngeneral-purpose models demonstrating the accessibility of ready L LMs to a wide \nrange of medical applications without the need for extensive com putational \nresources or training on specific datasets. Fine-tuned models were applied in (30/84) \nstudies reflecting the keen interest to customize general-pu rpose LLMs (BERT) to \nspecific domain-specific tasks including automated clinical notes  [77], extracting \nknowledge from clinical notes [67], and understanding mental health disorders [78]. \nSeveral studies also highlight the potential and limitations of zero- shot or few-shot \nlearning approaches, particularly leveraging GPT [79], [80], [8 1]. Notably, prompt-\nbased fine-tuning emerges as a technique capable of achieving  competitive \nperformance with significantly reduced training data [55].  \n  \nAlthough developing and training LLMs from scratch optimizes t he model for \nadvanced medical applications, the limited number of applications relying on training \nfrom scratch demonstrated the complexity of computational and data  requirements. \nThis makes it less viable for medical researchers and professionals without adequate \ncomputing and expertise resources. However, emerging applications exist  in \ndiagnostics [82], genomics data mining [83] and interpreting me dical reports \n[84]. Such domain-specific LLMs (e.g., CancerBERT in [48], medBERT .de in [85], \nSurgicBERTa in [81] frequently demonstrate superior performance com pared to \ngeneral-purpose LLMs [75], suggesting that specific medical language benefits from \ntargeted training.  \nWhile the majority of studies focus on English, many investigat e LLMs performance \nin other languages [4], [85], [86], [87], [88], [89]. These studies r eveal variable \nperformance and specific challenges such as worse performance of GPT- 3.5 on a \nChinese national medical licensing exam compared to its English translati on [4], the \nlack of adequate lexical resources and annotated corpora in Serbian [5], and the need \nfor domain-specific German LLMs [85]. These challenges indicate that LLM \neffectiveness is not uniform across languages and is often constra ined by the \navailability of high-quality, language-specific medical traini ng data and resources \n[88].           \n  \nImpact Assessment  \n  \nThe identified positive impact of using LLMs across medical specialties demonstrates \ntheir active contributions beyond theory. The majority of reported advantages \nrevolve around improving medical process efficiency and advan cing diagnostic \naccuracy. This suggests the potential of LLMs' contributions to routine tasks including \noptimizing administrative procedures and downstream tasks to allow medical \nprofessionals to focus more on patient care. Positive LLMs' impact  on diagnostics \nreflects their capabilities of augmenting human expertise, potentially leading to faster \nand more accurate diagnoses [90]. However, a balance has to be main tained where \nLLMs are studied to enhance not replace human expertise in di agnostics. Further, \nLLMs also provide valuable assistance to medical practitioners in clinical decis ion-\nmaking by offering evidence-based recommendations, judgm ent, and analytics [91]. \nA notable impact was also described for supporting medical education a nd patient \nengagement indicating the potential of LLMs to provide and fac ilitate interactive \nlearning experiences for both medical professionals and patie nts. LLMs have been \nproven to pass different medical examinations [9] reflecting the ir credibility and \naccuracy as educational tools. Despite patient care enhancement being the  lowest-\nreported benefit, this may imply an indirect impact of LLMs th rough improved \ndiagnostic accuracy and process efficiency. Ultimately, this revie w did not conduct a \nmeta-analysis due to the heterogeneous nature of the compiled studies.  \nDifferent significant concerns were also reported by most of the reviewed studies. 43 \nstudies reported inconsistent reliability where LLMs may fail t o maintain the same \nlevel of performance, produce different output for the same inp ut, or have different \nquality output. This poses major challenges for integrating LLMs in to medical \ninformation systems where reliability is essential and consequen tly may reduce the \nadoption and trust of LLMs due to the potential risks. Another 20 studies stated one \nor more forms of accuracy issues including unclear diagnoses and missing essential \ntreatment recommendations [8]. This shows the severity of implications on p atient \nsafety and treatments and therefore, training the LLMs with diverse and high-quality \nmedical datasets and continuing the validation in real-world med ical scenarios are \ncrucial to ensure accuracy and consistent reliability. The reported  lack of clinical \nknowledge is expected due to the reliance on general-purpose LLMs that m ight not \nbe adequately trained to perform medical-related tasks. Ethical co ncerns can be \ndirect or associated with other considerations as well. They p ose major challenges \ndue to the evolving nature of the technology including patient privacy, transparency, \nmodel bias, and data security.  \nEvaluation Metrics and Performance Assessment  \nThe evaluation metrics are crucial to assess the performance, reliability, and safety of \nLLMs. For instance, in the context of machine learning, an ev aluation metric for a \nclassification task is different from that of a clustering task. I n this review, the most \ncommonly used evaluation metrics are accuracy, F1-score, precision, and recall in as \nshown in Appendix 2 presenting classification problems. Most studies aim to evaluate \nthe overall correctness of LLM outputs. This reflects the simplicity of metrics and its \ninterpretability facilitating the communication of results to the non-technical medical \nresearch community. However, over-reliance on accuracy alone may not capture the \nperformance evaluation of LLMs correctly, especially in the case of imbalanced \ndatasets. Therefore, the F1-score and other metrics are used to p rovide a balanced \nevaluation. This is particularly important in medical contexts w here both false \npositives and false negatives can have significant consequences.  \nOur synthesis of the reviewed studies reveals critical qualitativ e limitations that \nimpact the reliability of LLMs in medical settings. Factual inacc uracies and \nhallucinations in LLM outputs are observed frequently [92]. Th ese range from \nmisinterpretations of clinical data to the fabrication of information or references [52]. \nFactors contributing to these issues include the complexity and specificit y of the \nprompt [64], the quality and representativeness of the training data [67], and \nlimitations in processing higher-order clinical reasonin g [93], temporal reasoning \n[41], handling negation [94], and poor understanding of different  medical \nterminologies [92]. Although mitigation strategies such as promp t engineering [95], \nhybrid model architectures [94], data augmentation [55], an d rule-based post-\nprocessing [96] are explored, human oversight and validation is decisive to ensure \nclinical safety and accuracy [37]. These findings suggest that while LLMs can enhance \nefficiency, their current limitations necessitate caution, p articularly in complex \nmedical tasks.  \nOnly 21 studies utilized the F1-score evaluation metric as the second most used \nmetric. While AUC-ROC usage is associated with the evaluation of LLM s in \nclassification tasks, a 5-point Likert scale is interestingly used to incorporate human-\nexpert judgment in measuring the clarity, accuracy, and complete ness of AI-\ngenerated clinical summaries in comparison to human-generated  ones [97]. This \nreview also identified several evaluation metrics used for  specific tasks, including \nclustering medical records using the silhouette metric [78], gatheri ng a range of \nopinions about the performance of LLMs [97], [98], assessing readability of generated \ntext using Flesch-Kincaid [37] and FRE [99]. The variety of evaluation metrics reflects \nthe wide spectrum of LLM applications in medical specialties, wh ile the correctness \nof the LLMs remains the dominant one. We highlight the importance of  human \ninterpretability in medical AI applications, i.e., the 5-point Likert scale, which is not a \ncommon evaluation metric.  \nValidation Approaches   \nThe validation of the methodologies in the reviewed studies rev ealed a thorough \nevaluation of LLMs from multiple perspectives, including comparin g them to both \nhuman experts and other tools. 44 studies benchmark their per formance against \nexisting tools while 34 studies compared their LLMs' relativ e performance against \nmedical experts. This suggests a rigorous benchmark where the LL Ms' potential is \nassessed to match human expert performance. However, evaluations by  clinical \nexperts consistently raise significant concerns regarding the re liability, factual \naccuracy, and safety of LLMs output [58]. For instance, readability assessments \nfrequently find LLM generated content unsuitable for patient e ducation due to high \ncomplexity [100] where evaluations by laymen rate LLMs resp onses more favorably \nthan experts [28]. These discrepancies may reflect differing standards or awareness \nof clinical details. Moreover, human clinical expertise is esse ntial not just for \nvalidating LLM outputs but throughout the development and ap plication pipeline. \nExperts are crucial for curating and annotating training  data [101], designing \neffective and clinically relevant prompts [80], defining annot ation schemas [87], \ningesting domain knowledge [79], interpreting specific or amb iguous LLM outputs \n[41], and ensuring outputs align with clinical standards [97].  \nTherefore, there is a need for more standardized validation methods in medical LLM \nresearch due to the diversity of approaches. It is noteworthy that se veral reviewed \nstudies did not validate their methodologies. This lack of comp arative validation \ncould be attributed to the novelty of this research direction, wh ere standardized \nvalidation methods are yet to be established. Such findings unde rscore the evolving \nnature of LLM applications in medicine and the ongoing challenges i n developing \nrobust evaluation frameworks.  \nFuture Directions  \nFuture LLM applications should consider several key areas requiring future research \nto fully realize the potential of LLMs in medicine. Continued  effort is needed to \nenhance LLMs capabilities, particularly through domain-specific pre-training or fine-\ntuning using high-quality medical corpora. Addressing the scarcit y of large-scale, \nhigh-quality, annotated medical datasets accessible for resea rch is important. The \ndevelopment of novel LLMs is needed to better suit clinical data complexities, such as \nhandling long temporal sequences or multimodal inputs. Improving reliab ility and \nsafety should be prioritized to enhance LLM performance consist ency, which will \nimprove the trust and adoption of LLMs across various specialties. Thi s can be \nachieved by developing methods to mitigate factual errors, reduce harmful \nhallucinations, and address inherent biases.   \nThe current focus on multi-specialty applications must be maintain ed and improved \nto effectively produce comprehensive output from various medical dom ains [102]. \nThis should be aided by expanding research effort to explor e low-represented \nspecialties in this review including ophthalmology, cardiology, en docrinology, \nurology, immunology, geriatrics, and nephrology or in not presented specialties such \nas dermatology, anesthesiology, gynecology, obstetrics, and hem atology. This can \nuncover new horizons for LLM applications across a wider range o f medical \nspecialties.  Dermatology could benefit from multimodal LLMs th at combine visual \nimage analysis with text-based reasoning to improve diagnostic accuracy for  skin \nconditions. LLMs could optimize anesthesiology medication dosing an d monitoring \nprotocols based on patient-specific factors. For gynecology and obs tetrics, \napplications in prenatal risk assessment and personalized pregna ncy monitoring \nrepresent promising directions. Hematology applications could f ocus on rare blood \ndisorder identification and the development of personalized treatment protocols.  \nWhile several LLMs excel in specific medical examinations, other s did not perform \nwell. Future research may focus on developing fine-tuned L LMs to aid medical \neducation and training, particularly for license examinations, board examinations, \nand medical knowledge assessments. The same can be considered for p atient \nmanagement, engagement, and communication where LLMs can be util ized to \npersonalize patient care to individual or group level.  \nBeyond specialty-specific applications, we identified three cr oss-cutting application \nareas with exceptional potential: (1) multilingual LLMs to ad dress healthcare \nrequirements in non-English speaking populations; (2) LLM-powered c linical \ndecision support systems that integrate with existing electronic heal th records for \nreal-time guidance; and (3) patient-oriented LLMs designed  specifically for health \nliteracy improvement and treatment adherence support.  \nFinally, the establishment of robust, standardized evaluation  methodologies and \nbenchmarks specific to medical applications is crucial for transparently assessing and \ncomparing LLMs performance. The evaluation of LLM application s in medical \nspecialties requires longitudinal studies to study the long -term impact of LLMs on \npatients, medical professionals, and healthcare environments. The assessme nt \nshould be extended to measure the adaptability and trust of L LMs to existing and \nevolving medical practices and knowledge.  \n \n  \nLimitations  \nA limitation of the current systematic review is the extreme heterogen eity of the \nstudies included, since there is considerable variation by medical  specialty, dataset, \nmetric, and methodology of evaluation applied in specific contexts. The diversity \nfound did not allow for meta-analysis without giving way to the in tegrity and \naccuracy of the results; thus, it needs a descriptive synthesis of the results. \nFurthermore, analysis with regard to specificity and sensitivity cannot be done, which \nis another limitation of this study. To address this in future r esearch, it is \nrecommended to develop standardized evaluation frameworks specific to LLM \nevaluation in healthcare. Additionally, establishing benchmark datasets for each \nmedical specialty to enable direct and transparent comparison of different LLMs. \nMoreover, implementing mixed methods that combine quantitati ve performance \nmetrics with qualitative assessment of clinical utility to provide a more \ncomprehensive evaluation of LLM applications in medicine. Finall y, meta-analyses \nmay be conducted within more homogeneous subsets of studies to provid e more \nrobust conclusions.  \nDue to the massive number of existing LLMs, the proposed search strategy involved \nthe combination of broad, generic terms (like \"large language mod els\") and specific, \nwell-known LLMs (e.g., ChatGPT and BERT) to achieve a balan ce between \ncomprehensiveness and specificity in our retrieval. We recogni ze that the rapidly \nchanging field of LLMs means that new LLMs also constantly e merge, which may \nmean our list of explicitly named models was not exhaustive. Futur e systematic \nreviews might consider refreshing their existing search strate gies in a sequential \nmanner, while employing the use of generic terms to capture the larger literature.  \n  \nConclusions  \nThis systematic review demonstrated that GPT-based and BERT-base d models are \nthe most used models in medical specialties studies. Applications reporte d include \nclinical NLP, supporting clinical decision, medical education, diagnoses, patient \nmanagement and engagement. Applications were based mainly on gener al-purpose \nand fine-tuned LLMs while fewer medical-specific ones e xisted due to the complex \nand costly requirements to build domain-specific ones. Comparing the negative \nimpact with the advantages of using LLMs for medical applications p resents a \ncomplex landscape where a balance is required to leverage LLMs' capab ilities while \nreducing the limitations. It also highlighted the urgent need to establish a validation \nstandard in LLM research, a crucial issue that needs immediat e attention. Our study \nsuggested that a validation standard is one of the main challenges  in LLM research. \nWe believe that a collaboration between medical and compute r science researchers \nwill significantly enhance LLM's capabilities in medical specialties.  Future LLM \nresearch in the clinical domain will be directed to the most  useful multimodal \narchitecture, fusing data types for more sophisticated medical tasks, while enhancing \nreliability. The development of specialty-specific LLMs should th erefore concentrate \non the utilization of credible medical data for fine-tuning a nd the most advanced \ntransfer learning techniques.  \nAcknowledgements  \nThis study was funded by Ministry of Higher Education, Resear ch and Innovation, \nOman as a part of the Block Funding Program grant number BFP /RGP/ICT/22/445. \nThe funder played no role in study design, data collection, analysis and interpretation \nof data, or the writing of this manuscript.  \n  \nAvailability of data  \nThe datasets used and/or analyzed during the current study available from the \ncorresponding author on reasonable request.  \nAuthors' Contributions  \nAlkalbani conceptualized the study, Alkalbani, Salah and Alrawahi designed the study. \nAlkalbani, Salah, Venus, and Yang extracted the data. Alk albani, Salah, and Alrawahi \nanalyzed the data and wrote the manuscript. Alkindi and Sheng reviewed and edited \nthe manuscript.  \nConflicts of Interest  \nnone declared  \n  \nMultimedia Appendix 1  \nCharacteristics of the reviewed studies \nMultimedia Appendix 2  \nDetailed LLM performance evaluation metrics. \nMultimedia Appendix 3  \nRisk of Bias Assessment. \nMultimedia Appendix 4  \nPRISMA (Preferred Reporting Items for a Systematic Review and Meta-Analyses) \n2020 checklist.  \nMultimedia Appendix 5  \nModified QUADAS-AI framework  \nMultimedia Appendix 6  \nQuery strings of PubMed, Web of Science, Scopus.  \nReferences \n  [1] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, and others, “Improv ing \nlanguage understanding by generative pre-training,” 2018. \n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-trainin g of deep \nbidirectional transformers for language understanding,” arXiv preprint \narXiv:1810.04805, 2018. \n[3] S. Sandmann, S. Riepenhausen, L. Plagwitz, and J. Varghese, “Systematic \nanalysis of ChatGPT, Google search and Llama 2 for clinical decision support tasks,” \nNature Communications, vol. 15, no. 1, p. 2050, 2024. \n[4] X. Wang et al., “ChatGPT Performs on the Chinese National Medical Licensing \nExamination,” Journal of Medical Systems, vol. 47, no. 1, Aug. 2023, doi: \n10.1007/s10916-023-01961-0. \n[5] J. M. de Oliveira, R. S. Antunes, and C. A. da Costa, “SOAP classifie r for free-text \nclinical notes with domain-specific pre-trained language models,” Expert Systems \nwith Applications, vol. 245, p. 123046, Jul. 2024, doi: 10.1016/j.eswa.2023.123046. \n[6] Y. Kim, J.-H. Kim, Y.-M. Kim, S. Song, and H. J. Joo, “Predicting me dical \nspecialty from text based on a domain-specific pre-trained BERT,” International \nJournal of Medical Informatics, vol. 170, p. 104956, Feb. 2023, doi: \n10.1016/j.ijmedinf.2022.104956. \n[7] H. L. Walker et al., “Reliability of Medical Information Provided by ChatGPT: \nAssessment Against Clinical Guidelines and Patient Information Quality Instrument,” \nJournal of Medical Internet Research, vol. 25, p. e47479, Jun. 2023, doi: \n10.2196/47479. \n[8] T. I. Wilhelm, J. Roos, and R. Kaczmarczyk, “Large Language Models for \nTherapy Recommendations Across 3 Clinical Specialties: Comparative Study,” \nJournal of Medical Internet Research, vol. 25, p. e49324, Oct. 2023, doi: \n10.2196/49324. \n[9] A. Mihalache, R. S. Huang, M. M. Popovic, and R. H. Muni, “ChatGPT- 4: An \nassessment of an upgraded artificial intelligence chatbot in the United States \nMedical Licensing Examination,” Medical Teacher, vol. 46, no. 3, pp. 366–372, Oct. \n2023, doi: 10.1080/0142159x.2023.2249588. \n[10] L. Ilias and D. Askounis, “Context-aware attention layers coupled with \noptimal transport domain adaptation and multimodal fusion methods for \nrecognizing dementia from spontaneous speech,” Knowledge-Based Systems, vol. \n277, p. 110834, Oct. 2023, doi: 10.1016/j.knosys.2023.110834. \n[11] Y. Artsi, V. Sorin, E. Konen, B. S. Glicksberg, G. Nadkarni, and  E. Klang, “Large \nlanguage models for generating medical examinations: systematic review,” BMC \nMedical Education, vol. 24, no. 1, p. 354, 2024. \n[12] S. M. Pressman, S. Borna, C. A. Gomez-Cabello, S. A. Haider, C. R . Haider, and A. \nJ. Forte, “Clinical and Surgical Applications of Large Language Models: A Systematic \nReview,” Journal of Clinical Medicine, vol. 13, no. 11, p. 3041, 2024. \n[13] C. Zhang et al., “Examining the Role of Large Language Models in \nOrthopedics: Systematic Review,” Journal of Medical Internet Research, vol. 26, p. \ne59607, 2024. \n[14] A. Moglia, K. Georgiou, P. Cerveri, L. Mainardi, R. M. Satava,  and A. Cuschieri, \n“Large language models in healthcare: from a systematic review on medical \nexaminations to a comparative analysis on fundamentals of robotic surgery online \ntest,” Artificial Intelligence Review, vol. 57, no. 9, p. 231, 2024. \n[15] M. Liu et al., “Performance of ChatGPT across different versions in medical \nlicensing examinations worldwide: systematic review and meta-analysis,” Journal of \nMedical Internet Research, vol. 26, p. e60807, 2024. \n[16] L. Wang et al., “Applications and Concerns of ChatGPT and Other \nConversational Large Language Models in Health Care: Systematic Review,” Journal \nof Medical Internet Research, vol. 26, p. e22769, 2024. \n[17] M. J. Page et al., “The PRISMA 2020 statement: an updated guideline for \nreporting systematic reviews,” bmj, vol. 372, 2021. \n[18] “Specialty Profiles — careersinmedicine.aamc.org.” [Online]. Avail able: \nhttps://careersinmedicine.aamc.org/explore-options/specialty-profiles \n[19] V. Sounderajah et al., “A quality assessment tool for artificial intelligence-\ncentered diagnostic test accuracy studies: QUADAS-AI,” Nature medicine, vol. 27, no. \n10, pp. 1663–1665, 2021. \n[20] Y. Zhang, K. Chen, Y. Weng, Z. Chen, J. Zhang, and R. Hubbar d, “An intelligent \nearly warning system of analyzing Twitter data using machine learning on COVID-\n19 surveillance in the US,” Expert Systems with Applications, vol. 198, p. 116882, Jul. \n2022, doi: 10.1016/j.eswa.2022.116882. \n[21] J. R. Mitchell et al., “A Question-and-Answer System to Extract Data From \nFree-Text Oncological Pathology Reports (CancerBERT Network): Development \nStudy,” Journal of Medical Internet Research, vol. 24, no. 3, p. e27210, Mar. 2022, doi: \n10.2196/27210. \n[22] D. Li et al., “Comparing the performance of <scp>ChatGPT GPT</scp>‐4, \nBard, and Llama‐2 in the Taiwan Psychiatric Licensing Examination and in \ndifferential diagnosis with multi‐center psychiatrists,” Psychiatry and Clinical \nNeurosciences, vol. 78, no. 6, pp. 347–352, Feb. 2024, doi: 10.1111/pcn.13656. \n[23] N. F. Ayoub, Y.-J. Lee, D. Grimm, and V. Divi, “Head-to-head compa rison of \nChatGPT versus Google search for medical knowledge acquisition,” Otolaryngology–\nHead and Neck Surgery, vol. 170, no. 6, pp. 1484–1491, 2024. \n[24] H. Wang, W. Wu, Z. Dou, L. He, and L. Yang, “Performance and explorat ion of \nChatGPT in medical examination, records and education in Chinese: Pave the way \nfor medical AI,” International Journal of Medical Informatics, vol. 177, p. 105173, Sep. \n2023, doi: 10.1016/j.ijmedinf.2023.105173. \n[25] C. Wang, S. Liu, A. Li, and J. Liu, “Text Dialogue Analy sis for Primary Screening \nof Mild Cognitive Impairment: Development and Validation Study,” Journal of \nMedical Internet Research, vol. 25, p. e51501, Dec. 2023, doi: 10.2196/51501. \n[26] M. L. Oon, N. L. Syn, C. L. Tan, K. Tan, and S. Ng, “Bridging  bytes and biopsies: \nA comparative analysis of ChatGPT and histopathologists in pathology diagnosis and \ncollaborative potential,” Histopathology, vol. 84, no. 4, pp. 601–613, Nov. 2023, doi: \n10.1111/his.15100. \n[27] J. Y. Yun, D. J. Kim, N. Lee, and E. K. Kim, “A comprehensive evaluation of  \nChatGPT consultation quality for augmentation mammoplasty: A comparative \nanalysis between plastic surgeons and laypersons,” International Journal of Medical \nInformatics, vol. 179, p. 105219, Nov. 2023, doi: 10.1016/j.ijmedinf.2023.105219. \n[28] T. Scquizzato et al., “Testing ChatGPT ability to answer laypeople questions \nabout cardiac arrest and cardiopulmonary resuscitation,” Resuscitation, vol. 194, p. \n110077, Jan. 2024, doi: 10.1016/j.resuscitation.2023.110077. \n[29] A. Maillard et al., “Can Chatbot Artificial Intelligence Replace Infectious \nDiseases Physicians in the Management of Bloodstream Infections? A Prospective \nCohort Study,” Clinical Infectious Diseases, vol. 78, no. 4, pp. 825–832, Oct. 2023, doi: \n10.1093/cid/ciad632. \n[30] S. Bushuven et al., “‘ChatGPT, can you help me save my child’s life?’-\nDiagnostic Accuracy and Supportive Capabilities to lay rescuers by ChatGPT in \nprehospital Basic Life Support and Paediatric Advanced Life Support cases–an in-\nsilico analysis,” Journal of Medical Systems, vol. 47, no. 1, p. 123, 2023. \n[31] A. Mulyar, O. Uzuner, and B. McInnes, “MT-clinical BERT: scaling c linical \ninformation extraction with multitask learning,” Journal of the American Medical \nInformatics Association, vol. 28, no. 10, pp. 2108–2115, Aug. 2021, doi: \n10.1093/jamia/ocab126. \n[32] X. Cai, S. Liu, J. Han, L. Yang, Z. Liu, and T. Liu, “Chestxra ybert: A pretrained \nlanguage model for chest radiology report summarization,” IEEE Transactions on \nMultimedia, vol. 25, pp. 845–855, 2021. \n[33] S. Y. Wang, J. Huang, H. Hwang, W. Hu, S. Tao, and T. Hernande z-Boussard, \n“Leveraging weak supervision to perform named entity recognition in electronic \nhealth records progress notes to identify the ophthalmology exam,” International \njournal of medical informatics, vol. 167, p. 104864, 2022. \n[34] V. Hristidis, N. Ruggiano, E. L. Brown, S. R. R. Ganta, and S.  Stewart, “ChatGPT \nvs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison \nof Results,” Journal of Medical Internet Research, vol. 25, p. e48966, Jul. 2023, doi: \n10.2196/48966. \n[35] Y. K. Ghanem et al., “Dr. Google to Dr. ChatGPT: assessing the content and \nquality of artificial intelligence-generated medical information on appendicitis,” \nSurgical Endoscopy, vol. 38, no. 5, pp. 2887–2893, Mar. 2024, doi: 10.1007/s00464-\n024-10739-5. \n[36] G. Parker and M. J. Spoelma, “A chat about bipolar disorder,” Bipolar \nDisorders, vol. 26, no. 3, pp. 249–254, Sep. 2023, doi: 10.1111/bdi.13379. \n[37] A. Mishra et al., “Exploring the Intersection of Artificial Intelligence and \nNeurosurgery: Let us be Cautious With ChatGPT,” Neurosurgery, vol. 93, no. 6, pp. \n1366–1373, Jul. 2023, doi: 10.1227/neu.0000000000002598. \n[38] J. Shang, X. Tang, and Y. Sun, “PhaTYP: predicting the lifestyle  for \nbacteriophages using BERT,” Briefings in Bioinformatics, vol. 24, no. 1, p. bbac487, \n2023. \n[39] Y. Zhang et al., “HLAB: learning the BiLSTM features from the ProtBert-\nencoded proteins for the class I HLA-peptide binding prediction,” Briefings in \nBioinformatics, vol. 23, no. 5, p. bbac173, 2022. \n[40] A. Z. Klein, A. Magge, K. O’Connor, J. I. Flores Amaro, D. Weissenbacher,  and G. \nGonzalez Hernandez, “Toward using Twitter for tracking COVID-19: a natural \nlanguage processing pipeline and exploratory data set,” Journal of medical Internet \nresearch, vol. 23, no. 1, p. e25314, 2021. \n[41] B. Percha, K. Pisapati, C. Gao, and H. Schmidt, “Natural langu age inference for \ncuration of structured clinical registries from unstructured text,” Journal of the \nAmerican Medical Informatics Association, vol. 29, no. 1, pp. 97–108, Nov. 2021, doi: \n10.1093/jamia/ocab243. \n[42] M. Wong et al., “Review of emerging trends and projection of future \ndevelopments in large language models research in ophthalmology,” British Journal \nof Ophthalmology, 2023. \n[43] E. Klang, A. Sourosh, G. N. Nadkarni, K. Sharif, and A. Lahat, “Evaluat ing the \nrole of ChatGPT in gastroenterology: a comprehensive systematic review of \napplications, benefits, and limitations,” Therapeutic Advances in Gastroenterology, \nvol. 16, p. 17562848231218618, 2023. \n[44] H. A. Younis et al., “A Systematic Review and Meta-Analysis of Artificial \nIntelligence Tools in Medicine and Healthcare: Applications, Considerations, \nLimitations, Motivation and Challenges,” Diagnostics, vol. 14, no. 1, 2024, [Online]. \nAvailable: \nhttps://www.mdpi.com/2075-4418/14/1/109 \n[45] G. Levin, N. Horesh, Y. Brezinov, and R. Meyer, “Performance of Cha tGPT in \nmedical examinations: A systematic review and a meta-analysis,” BJOG: An \nInternational Journal of Obstetrics & Gynaecology, vol. 131, no. 3, pp. 378–380, 2024. \n[46] N. Schopow, G. Osterhoff, and D. Baur, “Applications of the natural language \nprocessing tool ChatGPT in clinical practice: comparative study and augmented \nsystematic review,” JMIR Medical Informatics, vol. 11, p. e48933, 2023. \n[47] H.-J. Kao, T.-W. Chien, W.-C. Wang, W. Chou, and J. C. Chow, “Assessi ng \nChatGPT’s capacity for clinical decision support in pediatrics: A comparative study \nwith pediatricians using KIDMAP of Rasch analysis,” Medicine, vol. 102, no. 25, p. \ne34068, 2023. \n[48] S. Zhou, N. Wang, L. Wang, H. Liu, and R. Zhang, “CancerBERT: a cancer \ndomain-specific language model for extracting breast cancer phenotypes from \nelectronic health records,” Journal of the American Medical Informatics Association, \nvol. 29, no. 7, pp. 1208–1216, Mar. 2022, doi: 10.1093/jamia/ocac040. \n[49] R. Ali et al., “Performance of ChatGPT and GPT-4 on Neurosurgery Written \nBoard Examinations,” Neurosurgery, vol. 93, no. 6, pp. 1353–1365, Aug. 2023, doi: \n10.1227/neu.0000000000002632. \n[50] P. A. Massey, C. Montgomery, and A. S. Zhang, “Comparison of ChatGP T–3.5, \nChatGPT-4, and orthopaedic resident performance on orthopaedic assessment \nexaminations,” JAAOS-Journal of the American Academy of Orthopaedic Surgeons, vol. \n31, no. 23, pp. 1173–1179, 2023. \n[51] J. J. Butler et al., “From technical to understandable: Artificial Intelligence \nLarge Language Models improve the readability of knee radiology reports,” Knee \nSurgery, Sports Traumatology, Arthroscopy, vol. 32, no. 5, pp. 1077–1086, Mar. 2024, \ndoi: 10.1002/ksa.12133. \n[52] A. P. Mika, J. R. Martin, S. M. Engstrom, G. G. Polkowski, and J. M.  Wilson, \n“Assessing ChatGPT Responses to Common Patient Questions Regarding Total Hip \nArthroplasty,” Journal of Bone and Joint Surgery, vol. 105, no. 19, pp. 1519–1526, Jul. \n2023, doi: 10.2106/jbjs.23.00209. \n[53] H. Liu et al., “Use of BERT (Bidirectional Encoder Representations from \nTransformers)-Based Deep Learning Method for Extracting Evidences in Chinese \nRadiology Reports: Development of a Computer-Aided Liver Cancer Diagnosis \nFramework,” Journal of Medical Internet Research, vol. 23, no. 1, p. e19689, Jan. \n2021, doi: 10.2196/19689. \n[54] S. Datta and K. Roberts, “Fine-grained spatial information ext raction in \nradiology as two-turn question answering,” International journal of medical \ninformatics, vol. 158, p. 104628, 2022. \n[55] R. S. Y. C. Tan et al., “Inferring cancer disease response from radiology reports \nusing large language models with data augmentation and prompting,” Journal of the \nAmerican Medical Informatics Association, vol. 30, no. 10, pp. 1657–1664, Jul. 2023, \ndoi: 10.1093/jamia/ocad133. \n[56] E. Mohamad, C. Boutoleau-Bretonnière, and G. Chapelet, “ChatGPT’s  Dance \nwith Neuropsychological Data: a case study in Alzheimer’s Disease,” Ageing Research \nReviews, p. 102117, 2023. \n[57] N. Otsuka et al., “Diagnosing psychiatric disorders from history of present \nillness using a large‐scale linguistic model,” Psychiatry and Clinical Neurosciences, \nvol. 77, no. 11, pp. 597–604, Sep. 2023, doi: 10.1111/pcn.13580. \n[58] A. Zaboli, F. Brigo, S. Sibilio, M. Mian, and G. Turcato, “Human int elligence \nversus Chat-GPT: who performs better in correctly classifying patients in triage?,” \nThe American Journal of Emergency Medicine, vol. 79, pp. 44–47, May 2024, doi: \n10.1016/j.ajem.2024.02.008. \n[59] S. Lee et al., “Deep learning-based natural language processing for detecting \nmedical symptoms and histories in emergency patient triage,” The American Journal \nof Emergency Medicine, vol. 77, pp. 29–38, Mar. 2024, doi: \n10.1016/j.ajem.2023.11.063. \n[60] A. CB, K. Mahesh, and N. Sanda, “Ontology-based semantic data \ninterestingness using BERT models,” Connection Science, vol. 35, no. 1, Apr. 2023, \ndoi: 10.1080/09540091.2023.2190499. \n[61] Y. Qiao, X. Zhu, and H. Gong, “BERT-Kcr: prediction of lysine crotonyla tion \nsites by a transfer learning method with pre-trained BERT models,” Bioinformatics, \nvol. 38, no. 3, pp. 648–654, Oct. 2021, doi: 10.1093/bioinformatics/btab712. \n[62] D. Vithanage, Y. Zhu, Z. Zhang, C. Deng, M. Yin, and P. Yu, “Extra cting \nSymptoms of Agitation in Dementia from Free-Text Nursing Notes Using Advanced \nNatural Language Processing,” in MEDINFO 2023—The Future Is Accessible, IOS \nPress, 2024, pp. 700–704. \n[63] Q. Guo, S. Cao, and Z. Yi, “A medical question answering system usin g large \nlanguage models and knowledge graphs,” International Journal of Intelligent \nSystems, vol. 37, no. 11, pp. 8548–8564, Jul. 2022, doi: 10.1002/int.22955. \n[64] A. Acharya et al., “Clinical risk prediction using language models: benefits and \nconsiderations,” Journal of the American Medical Informatics Association, Feb. 2024, \ndoi: 10.1093/jamia/ocae030. \n[65] Y.-P. Chen, Y.-H. Lo, F. Lai, and C.-H. Huang, “Disease Concept-Embe dding \nBased on the Self-Supervised Method for Medical Information Extraction from \nElectronic Health Records and Disease Retrieval: Algorithm Development and \nValidation Study,” Journal of Medical Internet Research, vol. 23, no. 1, p. e25113, Jan. \n2021, doi: 10.2196/25113. \n[66] T. Haze, R. Kawano, H. Takase, S. Suzuki, N. Hirawa, and K. Tamu ra, “Influence \non the accuracy in ChatGPT: Differences in the amount of information per medical \nfield,” International Journal of Medical Informatics, vol. 180, p. 105283, Dec. 2023, \ndoi: 10.1016/j.ijmedinf.2023.105283. \n[67] K. Xie et al., “Extracting seizure frequency from epilepsy clinic notes: a \nmachine reading approach to natural language processing,” Journal of the American \nMedical Informatics Association, vol. 29, no. 5, pp. 873–881, Feb. 2022, doi: \n10.1093/jamia/ocac018. \n[68] B. Alkouz, Z. Al Aghbari, M. A. Al-Garadi, and A. Sarker, “Deepluen za: Deep \nlearning for influenza detection from Twitter,” Expert Systems with Applications, vol. \n198, p. 116845, Jul. 2022, doi: 10.1016/j.eswa.2022.116845. \n[69] Z.-H. Lu, J. X. Wang, and X. Li, “Revealing opinions for COVID- 19 questions \nusing a context retriever, opinion aggregator, and question-answering model: Model \ndevelopment study,” Journal of medical Internet research, vol. 23, no. 3, p. e22860, \n2021. \n[70] D. Truhn et al., “Extracting structured information from unstructured \nhistopathology reports using generative pre‐trained transformer 4 \n(<scp>GPT</scp>‐4),” The Journal of Pathology, vol. 262, no. 3, pp. 310–319, Dec. \n2023, doi: 10.1002/path.6232. \n[71] C. Karakas, D. Brock, and A. Lakhotia, “Leveraging ChatGPT in the Pe diatric \nNeurology Clinic: Practical Considerations for Use to Improve Efficiency and \nOutcomes,” Pediatric Neurology, vol. 148, pp. 157–163, Nov. 2023, doi: \n10.1016/j.pediatrneurol.2023.08.035. \n[72] S. Liu et al., “Using AI-generated suggestions from ChatGPT to optimize \nclinical decision support,” Journal of the American Medical Informatics Association, \nvol. 30, no. 7, pp. 1237–1245, Apr. 2023, doi: 10.1093/jamia/ocad072. \n[73] Z. Xue, Y. Zhang, W. Gan, H. Wang, G. She, and X. Zheng, “Quality a nd \nDependability of ChatGPT and DingXiangYuan Forums for Remote Orthopedic \nConsultations: Comparative Analysis,” Journal of Medical Internet Research, vol. 26, \np. e50882, 2024. \n[74] G. Wang et al., “Potential and Limitations of ChatGPT 3.5 and 4.0 as a Source \nof COVID-19 Information: Comprehensive Comparative Analysis of Generative and \nAuthoritative Information,” Journal of Medical Internet Research, vol. 25, p. e49771, \nDec. 2023, doi: 10.2196/49771. \n[75] S. Ji, M. Hölttä, and P. Marttinen, “Does the magic of BERT apply to medi cal \ncode assignment? A quantitative study,” Computers in Biology and Medicine, vol. 139, \np. 104998, Dec. 2021, doi: 10.1016/j.compbiomed.2021.104998. \n[76] K. Nimmi, B. Janet, A. K. Selvan, and N. Sivakumaran, “Pre-trai ned ensemble \nmodel for identification of emotion during COVID-19 based on emergency response \nsupport system dataset,” Applied Soft Computing, vol. 122, p. 108842, Jun. 2022, doi: \n10.1016/j.asoc.2022.108842. \n[77] V. C. Hartman, S. S. Bapat, M. G. Weiner, B. B. Navi, E. T. Sholle, and T.  R. \nCampion, “A method to automate the discharge summary hospital course for \nneurology patients,” Journal of the American Medical Informatics Association, vol. 30, \nno. 12, pp. 1995–2003, Aug. 2023, doi: 10.1093/jamia/ocad177. \n[78] S. Kim, J. Cha, D. Kim, and E. Park, “Understanding Mental H ealth Issues in \nDifferent Subdomains of Social Networking Services: Computational Analysis of \nText-Based Reddit Posts,” Journal of Medical Internet Research, vol. 25, p. e49074, \nNov. 2023, doi: 10.2196/49074. \n[79] D. Hu, B. Liu, X. Zhu, X. Lu, and N. Wu, “Zero-shot information  extraction from \nradiological reports using ChatGPT,” International Journal of Medical Informatics, \nvol. 183, p. 105321, Mar. 2024, doi: 10.1016/j.ijmedinf.2023.105321. \n[80] S. Datta et al., “AutoCriteria: a generalizable clinical trial eligibility criteria \nextraction system powered by large language models,” Journal of the American \nMedical Informatics Association, vol. 31, no. 2, pp. 375–385, Nov. 2023, doi: \n10.1093/jamia/ocad218. \n[81] M. Bombieri, M. Rospocher, S. P. Ponzetto, and P. Fiorini, “Mac hine \nunderstanding surgical actions from intervention procedure textbooks,” Computers \nin Biology and Medicine, vol. 152, p. 106415, Jan. 2023, doi: \n10.1016/j.compbiomed.2022.106415. \n[82] E. K. E. Laison et al., “Identifying Potential Lyme Disease Cases Using Self-\nReported Worldwide Tweets: Deep Learning Modeling Approach Enhanced With \nSentimental Words Through Emojis,” Journal of Medical Internet Research, vol. 25, p. \ne47014, Oct. 2023, doi: 10.2196/47014. \n[83] X.-F. Wang, C.-Q. Yu, Z.-H. You, Y. Qiao, Z.-W. Li, and W.-Z. Huang, “A n efficient \ncircRNA-miRNA interaction prediction model by combining biological text mining \nand wavelet diffusion-based sparse network structure embedding,” Computers in \nBiology and Medicine, vol. 165, p. 107421, Oct. 2023, doi: \n10.1016/j.compbiomed.2023.107421. \n[84] Z. Lu et al., “Natural language processing and machine learning methods to \ncharacterize unstructured patient-reported outcomes: validation study,” Journal of \nMedical Internet Research, vol. 23, no. 11, p. e26777, 2021. \n[85] K. K. Bressem et al., “medBERT.de: A comprehensive German BERT model for \nthe medical domain,” Expert Systems with Applications, vol. 237, p. 121598, Mar. \n2024, doi: 10.1016/j.eswa.2023.121598. \n[86] M. Chizhikova et al., “CARES: A Corpus for classification of Spanish \nRadiological reports,” Computers in Biology and Medicine, vol. 154, p. 106581, Mar. \n2023, doi: 10.1016/j.compbiomed.2023.106581. \n[87] A. Kaplar, M. Stošović, A. Kaplar, V. Brković, R. Naumo vić, and A. Kovačević, \n“Evaluation of clinical named entity recognition methods for Serbian electronic \nhealth records,” International Journal of Medical Informatics, vol. 164, p. 104805, \nAug. 2022, doi: 10.1016/j.ijmedinf.2022.104805. \n[88] S. Karthikeyan, A. G. S. de Herrera, F. Doctor, and A. Mirza, “ An OCR Post-\nCorrection Approach Using Deep Learning for Processing Medical Reports,” IEEE \nTransactions on Circuits and Systems for Video Technology, vol. 32, no. 5, pp. 2574–\n2581, May 2022, doi: 10.1109/tcsvt.2021.3087641. \n[89] M. Homburg et al., “A Natural Language Processing Model for COVID-19 \nDetection Based on Dutch General Practice Electronic Health Records by Using \nBidirectional Encoder Representations From Transformers: Development and \nValidation Study,” Journal of Medical Internet Research, vol. 25, p. e49944, 2023. \n[90] Â. Fonseca, A. Ferreira, L. Ribeiro, S. Moreira, and C. Duque,  “Embracing the \nfuture—is artificial intelligence already better? A comparative study of artificial \nintelligence performance in diagnostic accuracy and decision‐making,” European \nJournal of Neurology, vol. 31, no. 4, Jan. 2024, doi: 10.1111/ene.16195. \n[91] R. K. Gan, J. C. Ogbodo, Y. Z. Wee, A. Z. Gan, and P. A. González, “Performan ce \nof Google bard and ChatGPT in mass casualty incidents triage,” The American Journal \nof Emergency Medicine, vol. 75, pp. 72–78, Jan. 2024, doi: \n10.1016/j.ajem.2023.10.034. \n[92] L. Caruccio, S. Cirillo, G. Polese, G. Solimando, S. Sundaramurthy, an d G. \nTortora, “Can ChatGPT provide intelligent diagnoses? A comparative study between \npredictive models and ChatGPT to define a new medical diagnostic bot,” Expert \nSystems with Applications, vol. 235, p. 121186, Jan. 2024, doi: \n10.1016/j.eswa.2023.121186. \n[93] R. Cuthbert and A. I. Simpson, “Artificial intelligence in orthop aedics: can chat \ngenerative pre-trained transformer (ChatGPT) pass section 1 of the fellowship of the \nroyal college of surgeons (trauma & orthopaedics) examination?,” Postgraduate \nMedical Journal, vol. 99, no. 1176, pp. 1110–1114, 2023. \n[94] S. Fu et al., “A hybrid model to identify fall occurrence from electronic health \nrecords,” International Journal of Medical Informatics, vol. 162, p. 104736, Jun. 2022, \ndoi: 10.1016/j.ijmedinf.2022.104736. \n[95] J. Kaarre et al., “Exploring the potential of ChatGPT as a supplementary tool \nfor providing orthopaedic information,” Knee Surgery, Sports Traumatology, \nArthroscopy, vol. 31, no. 11, pp. 5190–5198, Aug. 2023, doi: 10.1007/s00167-023-\n07529-2. \n[96] J. Liu et al., “OpenDeID Pipeline for Unstructured Electronic Health Record \nText Notes Based on Rules and Transformers: Deidentification Algorithm \nDevelopment and Validation Study,” Journal of Medical Internet Research, vol. 25, p. \ne48145, Dec. 2023, doi: 10.2196/48145. \n[97] S. Liu et al., “Why do users override alerts? Utilizing large language model to \nsummarize comments and optimize clinical decision support,” Journal of the \nAmerican Medical Informatics Association, vol. 31, no. 6, pp. 1388–1396, Mar. 2024, \ndoi: 10.1093/jamia/ocae041. \n[98] H. Song et al., “Evaluating the Performance of Different Large Language \nModels on Health Consultation and Patient Education in Urolithiasis,” Journal of \nMedical Systems, vol. 47, no. 1, Nov. 2023, doi: 10.1007/s10916-023-02021-3. \n[99] J. R. Bellinger, J. S. De La Chapa, M. W. Kwak, G. A. Ramos, D. Morrison,  and B. \nW. Kesser, “BPPV Information on Google VersusAI (ChatGPT),” Otolaryngology–Head \nand Neck Surgery, vol. 170, no. 6, pp. 1504–1511, Aug. 2023, doi: 10.1002/ohn.506. \n[100] D. J. Campbell et al., “Evaluating ChatGPT responses on thyroid nodules for \npatient education,” Thyroid, vol. 34, no. 3, pp. 371–377, 2024. \n[101] J. Li et al., “Are synthetic clinical notes useful for real natural language \nprocessing tasks: A case study on clinical entity recognition,” Journal of the American \nMedical Informatics Association, vol. 28, no. 10, pp. 2193–2201, Jul. 2021, doi: \n10.1093/jamia/ocab112. \n[102] P. Tsoutsanis and A. Tsoutsanis, “Evaluation of Large language model \nperformance on the Multi-Specialty Recruitment Assessment (MSRA) exam,” \nComputers in Biology and Medicine, vol. 168, p. 107794, Jan. 2024, doi: \n10.1016/j.compbiomed.2023.107794. \n \nAbbreviations  \nAI: artificial intelligence  \nBERT: bidirectional encoder representations from transformers  \nCNN: convolutional neural network  \nLLM: large language model  \nPRISMA: preferred reporting items for a systematic review and meta-analyses  \nPROSPERO: international prospective register of systematic reviews  \nQUADAS-AI: a quality assessment tool for artificial intelligence-centered diagnostic \ntest accuracy studies. \nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nappendixtable011.docx\nappendixtable021.docx\nappendixtable031.docx\nappendix.docx\n7055510530601SP1.docx\n7055510530591SP1.docx",
  "topic": "Scopus",
  "concepts": [
    {
      "name": "Scopus",
      "score": 0.5495197176933289
    },
    {
      "name": "Medical diagnosis",
      "score": 0.5099109411239624
    },
    {
      "name": "Specialty",
      "score": 0.4874565601348877
    },
    {
      "name": "MEDLINE",
      "score": 0.4413071274757385
    },
    {
      "name": "English language",
      "score": 0.4247468113899231
    },
    {
      "name": "Psychology",
      "score": 0.41951417922973633
    },
    {
      "name": "Medical education",
      "score": 0.39253854751586914
    },
    {
      "name": "Medicine",
      "score": 0.33553457260131836
    },
    {
      "name": "Pathology",
      "score": 0.214569091796875
    },
    {
      "name": "Political science",
      "score": 0.20785683393478394
    },
    {
      "name": "Psychiatry",
      "score": 0.14418166875839233
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}