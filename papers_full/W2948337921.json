{
    "title": "Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains",
    "url": "https://openalex.org/W2948337921",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5084377683",
            "name": "Samira Abnar",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5087195265",
            "name": "Lisa Beinborn",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5109855717",
            "name": "Rochelle Choenni",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5007928903",
            "name": "Willem Zuidema",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2517394272",
        "https://openalex.org/W2062240844",
        "https://openalex.org/W2009284521",
        "https://openalex.org/W2782213998",
        "https://openalex.org/W2151591509",
        "https://openalex.org/W2168217710",
        "https://openalex.org/W2951682322",
        "https://openalex.org/W3022154375",
        "https://openalex.org/W2344975321",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2946296745",
        "https://openalex.org/W2115630963",
        "https://openalex.org/W2898482221",
        "https://openalex.org/W2963299036",
        "https://openalex.org/W2511680308",
        "https://openalex.org/W2252057887",
        "https://openalex.org/W1992570774",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2963692430",
        "https://openalex.org/W2963951265",
        "https://openalex.org/W2963337368",
        "https://openalex.org/W2563767109",
        "https://openalex.org/W2891177506",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963472233",
        "https://openalex.org/W2042003045",
        "https://openalex.org/W2508661145",
        "https://openalex.org/W1878625794",
        "https://openalex.org/W2564636125",
        "https://openalex.org/W2803461564",
        "https://openalex.org/W2170167574",
        "https://openalex.org/W2759601380",
        "https://openalex.org/W2964222268"
    ],
    "abstract": "In this paper, we define and apply representational stability analysis (ReStA), an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis (RSA) in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second (or higher) layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al. (2014) contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing.",
    "full_text": "Blackbox meets blackbox: Representational Similarity and Stability\nAnalysis of Neural Language Models and Brains\nSamira Abnar Lisa Beinborn Rochelle Choenni Willem Zuidema\nInstitute for Logic, Language and Computation\nUniversity of Amsterdam\n{abnar,l.beinborn}@uva.nl,rochelle.choenni@student.uva.nl,zuidema@uva.nl\nAbstract\nIn this paper, we deﬁne and apply represen-\ntational stability analysis (ReStA), an intu-\nitive way of analyzing neural language mod-\nels. ReStA is a variant of the popular repre-\nsentational similarity analysis (RSA) in cog-\nnitive neuroscience. While RSA can be used\nto compare representations in models, model\ncomponents, and human brains, ReStA com-\npares instances of the same model, while sys-\ntematically varying single model parameter.\nUsing ReStA, we study four recent and suc-\ncessful neural language models, and evaluate\nhow sensitive their internal representations are\nto the amount of prior context. Using RSA,\nwe perform a systematic study of how similar\nthe representational spaces in the ﬁrst and sec-\nond (or higher) layers of these models are to\neach other and to patterns of activation in the\nhuman brain. Our results reveal surprisingly\nstrong differences between language models,\nand give insights into where the deep linguis-\ntic processing, that integrates information over\nmultiple sentences, is happening in these mod-\nels. The combination of ReStA and RSA on\nmodels and brains allows us to start address-\ning the important question of what kind of lin-\nguistic processes we can hope to observe in\nfMRI brain imaging data. In particular, our\nresults suggest that the data on story reading\nfrom Wehbe et al. (2014) contains a signal of\nshallow linguistic processing, but show no ev-\nidence on the more interesting deep linguistic\nprocessing.\n1 Representational Similarity\nRepresentational similarity analysis (RSA) is a\ntechnique which allows us to compare heteroge-\nneous representational spaces (Laakso and Cot-\ntrell, 2000). It is very common in cognitive neuro-\nscience because it allows researchers to study the\nrelation between patterns of activation in the brain\nand representations of stimuli in a computational\nmodel (Kriegeskorte et al., 2008). The key idea\nis simple: instead of directly trying to map mod-\nels to brains, we ﬁrst construct two similarity ma-\ntrices that record how similar brain responses are\nto each other for different stimuli, and how simi-\nlar the computational model’s representations for\neach stimulus are to each other. The representa-\ntional similarity score is then deﬁned as the simi-\nlarity (typically: Pearson’s correlation) of the two\nsimilarity matrices (or equivalently: the similarity\nof two distance matrices).\nRSA can also be applied to deep learning mod-\nels (Laakso and Cottrell, 2000; Dharmaretnam and\nFyshe, 2018; Alvarez-Melis and Jaakkola, 2018;\nWang et al., 2018; Chrupała and Alishahi, 2019).\nIn this paper, we present a large-scale study and\ncomparison of both neural language models and\nfMRI data from brain imaging experiments with\nhuman subjects, using RSA. However, we extend\nstandard RSA using an approach we call Repre-\nsentational Stability Analysis (ReStA). The idea is\nagain simple: we apply RSA to compare instances\nof the same model, while systematically varying a\nmodel parameter.\nWe focus on a single parameter: the length of\nthe prior context presented to the model. Vary-\ning the amount of context allows us to quantify\nthe degree of context-dependence of different neu-\nral language models, and different components\nof those models. If internal representations are\nsimilarly organized regardless of how much addi-\ntional context is presented to the model, context-\ndependence is low. If, on the other hand, repre-\nsentations change with each additional amount of\ncontext included, context-dependence is high. Us-\ning this approach, we ﬁnd intriguing differences\nbetween some recent, successful neural language\nmodels (GoogleLM, ELMO, BERT and the Uni-\nversal Sentence Encoder; Table 1), and between\nthe ﬁrst and deeper layers of those models.\nContext-dependence, in turn, gives us a handle\non an important question in the research that tries\narXiv:1906.01539v2  [cs.AI]  5 Jun 2019\nModel Objective Corpus Rep.Dim. Architecture\nGloVe (Pennington et al., 2014) Predicting\nco-occurrence probabilities Wikipedia 300 Bag of words\nELMO (Peters et al., 2018) Bidirectional\nLanguage Modelling 1B benchmark 1,024 BiLSTM\nGoogleLM (Jozefowicz et al., 2016) Language Modelling 1B benchmark 1,024 LSTM\nUniSentEnc. (Cer et al., 2018) Skip-Thought/ClassiﬁcationVariety of web sources\n/ SNLI 512 Transformer Encoder\nBERT (base) (Devlin et al., 2019)Masked Language Modelling\n/ Next Sent. Pred.\nBooksCorpus\n/ English Wikipedia 768 Transformer Encoder\nTable 1: Details of the third party computational models used in this paper, including a brief characterization of\nthe optimization objective, the training corpus, and the dimensionality of representations we extract from them.\nto link neural language models to brain activation:\nwhich aspects of language processing in the brain\ncan we hope to observe in fMRI data using NLP\nand machine learning tools?\n2 Bridging NLP Models and\nNeurolinguistics\nAn important motivation behind our work is to\ncontribute to answering a big question in computa-\ntional linguistics: how do we establish a relation-\nship between NLP models and data on the human\nbrain activation while they process language? Pio-\nneering work of Mitchell et al. (2008) showed that\ntechniques from distributional semantics could be\nused to predict and decode brain activation. In the\ndecade since that paper, many efforts have been re-\nported using brain data to evaluate computational\nmodels, or using NLP models to build predic-\ntive models of the human brain, or both (Murphy\net al., 2012; Wehbe et al., 2014a; Ruan et al., 2016;\nSøgaard, 2016; Xu et al., 2016; Fyshe et al., 2014;\nBingel et al., 2016; Bulat et al., 2017; Abnar et al.,\n2018; Pereira et al., 2018; Huth et al., 2016).\nMost of that work is focused on lexical rep-\nresentations, reporting promising results for con-\ncrete nouns, presented in isolation. More recently\nresearchers have tried to adapt the methodology\nto address words in context, in sentence and story\nprocessing tasks. Pereira et al. (2018), for in-\nstance, used a bag of words model of sentence\nmeaning to decode sentences from brain activa-\ntion. Wehbe et al. (2014b); Qian et al. (2016) use\nthe internal states of LSTMs trained for language\nmodelling for encoding. Jain and Huth (2018) re-\nport that the higher layers of the LSTM are better\nat predicting the activation of brain regions that are\nknown for higher level language functions (a ﬁnd-\ning seemingly at odds with results from section5).\nIn this effort, however, we run into a number\nof major conceptual, methodological and techni-\ncal challenges. Most importantly: how do we\ndetermine what we are really observing in the\nbrain data? Are we really seeing signatures of lin-\nguistic processes, or just neural correlates of gen-\neral cognitive processes evoked by a correct un-\nderstanding of the linguistic input? How do we\nadequately control for alternative explanations of\nthe observed correlations? And how do we deal\nwith the intricate temporal dynamics and the over-\nwhelmingly high dimensionality of the brain, and\nthe very indirect, delayed and/or coarse measure-\nments that neuroimaging gives us of the processes\nin the brain? Merely demonstrating a correlation\nbetween two black boxes is clearly not sufﬁcient.\nWe argue that experiments to ﬁnd the model\nbest correlated with brain activations should be\naccompanied by efforts for interpreting the inter-\nnal representations and operations of the models.\nApplying ReStA for the prior context parameter\ngives us a way to roughly characterize the depth\nof linguistic processing in different language mod-\nels and different components of these models. If\na model component only tracks the lexical se-\nmantics of the current word, the representations\nit forms should not be sensitive to the amount of\nprior context. On the other hand, If a model com-\nponent tracks long-distance syntactic dependen-\nBlock Words Unique words Sentences Sent Length Scans\n1 1583 553 115 11 326\n2 1711 560 163 8 338\n3 1411 461 134 8 265\n4 1853 583 177 8 366\nTable 2: Statistics of the Harry Potter dataset.\nFigure 1: Alignment of the words in the story and the brain vectors. Each fMRI scan lasts for 2 seconds during which the\nsubject is reading four words sequentially. Delay is the amount of time in seconds between the time the ﬁrst of the four word is\nshown to the subject and when the fMRI scan is started to be taken.\ncies, semantic polarity, named entities, topics or\nstory arcs, resolves anaphora or builds up situa-\ntion models, its representations will be different\nwhenever different amounts of prior context are\navailable. Hence, in this paper, we will interpret\ncontext-dependence as an imperfect but useful sig-\nnature of deep linguistic processing.\n3 Models and Data\nIn this section, we explain the language encod-\ning models we study in our experiments and the\ndataset from which we get the language stimuli\nand their corresponding brain data.\n3.1 Neural Language Models\nWe study language models with different archi-\ntectures trained with different objective functions\n(see Table 1). As a word level embedding model,\nwe use GloVe (Pennington et al., 2014). We con-\nsider a sentence as a bag of words and take the\naverage of the GloVe embeddings of its individual\nwords.\nWe employ two high performing LSTM based\nlanguage models: ELMO (Peters et al., 2018)\nand GoogleLM (Jozefowicz et al., 2016). Both\nof these models have two LSTM layers; how-\never, ELMO uses bidirectional LSTM layers,\nwhereas in the GoogleLM the LSTM layers are\nuni-directional. From these models, we take the\ninternal states of each of the LSTM layers as two\ndifferent representation spaces.\nIn our comparisons, we also use BERT and\nthe Universal Sentence Encoder (UniSentEnc), as\nTransformer based models. BERT is trained on\nmasked language modelling and next sentence\nprediction tasks (Devlin et al., 2019) while the\nUniversal Sentence Encoder is trained on a differ-\nent objective than language modelling. The pa-\nrameters of this model are optimized with respect\nto different language tasks such that it can better\nencode the meaning of complete sentences. These\ntwo models do not have the recurrent inductive\nbias of LSTMs, and hence the representations they\nlearn can be completely different.\nTo study how and where the models integrate\ninformation over time, we modify the amount of\ncontext provided to the models to obtain the con-\ntextualized word representations. We do this at the\nsentence level. Thus, for the context length of 0,\nwe only feed the target words to the models; For\ncontext length 1 we feed all the previous words\nin the current sentence to the models. For context\nlength i where i > 1, in addition to the current\nsentence we feed all the words in the last i sen-\ntences. We operate on the sentence level to feed\nthe model with independently meaningful pieces\nof text.\nFrom prior work, we expect a relation between\nthe depth of the layers and the level of abstraction\nof their representations. We study this intuition\nhere empirically by analyzing the different layers\nof the models, and we focus on the ﬁrst and last\nlayers. Note that the last layer corresponds to the\nsecond layer for the LSTM architectures, but to the\n12th layer for Bert.\n3.2 Brain Data\nWe compare the representations of our model to\nhuman brain activations captured while reading a\nstory. We use the dataset by (Wehbe et al., 2014a)\nwhich consists of the fMRI scans of 8 participants\nreading chapter 9 of Harry Potter and the Sor-\ncerer’s stone(Rowling, 1998).1\nThe story was presented to the participants word\n1The data is available athttp://www.cs.cmu.edu/\n˜fmri/plosone/. Further information on the pre-\nprocessing steps is described in the supplementary material.\n(a) GoogleLM\n(b) ELMO\n(c) BERT\nFigure 2: RSA between different layers of each model given\ndifferent context length in terms of number of previous sen-\ntences over the story words. In these plots, for example\nL1 c3 means representation from layer 1, when the context\nlength is 3 sentences including the current sentence. When\nc = 0, the model only sees the current words and whenc = 1\nthe model sees current sentence up to the target word. Here\ndarker means more similar. The values are averaged over the\nfour story blocks and the standard deviation of all the values\nacross the four blocks are below 0.002.\nby word on a screen in four continuous blocks. 2\nEach word was displayed for 0.5 seconds and an\nfMRI scan was taken every 2 seconds. Figure 1 vi-\nsualizes an example for the beginning of the chap-\nter. More detailed statistical information about the\nstimuli can be found in Table 2.\nBrain Regions The fMRI data contains acti-\nvation values for approximately 40,000 voxels\nper scan, each reﬂecting the oxygen usage (the\n“BOLD response”) in approximately 3mm3 of\nbrain tissue. To obtain the brain representa-\ntions, we ﬂatten the 3D fMRI images into vectors\nthereby ignoring the spatial relationships between\nthe voxels. We do this either for the whole brain,\nor for speciﬁc regions separately. Not all of the\nscanned voxels are related to language process-\ning, but the changes in activity might be associated\nwith other cognitive processes like, for example,\nthe noise perception in the scanner. A common\nreduction method is to restrict the brain response\nto voxels that fall within a pre-selected set of re-\ngions. In our analysis, we only include the vox-\nels from the top k regions that are most similar\nacross different subjects given the same stimuli.\nWe heuristically set the value of k to 16 based on\nthe distribution of the similarity scores.3\nDelay An important point to consider when\ndealing with fMRI data is the hemodynamic re-\nsponse delay: from the time neurons start ﬁring,\nit takes 4 to 6 seconds until the Bold response\nreaches its peak (Buckner, 1998). This means that\nfrom the time a stimulus is presented to a subject,\nit takes approximately5 seconds before we can ob-\nserve its response in the fMRI scan of the brain.\nWe account for this delay by varying the alignment\nbetween stimuli and scans. If we apply a delay of\n0 seconds, scan 3 in the example would be applied\nto the sequence boy he hated more, Figure 1. With\na delay of 2 seconds, it is aligned to the previous\nstimulus he would meet a and a delay of 4 would\nresult in alignment withHarry had never believed.\n2The story chapter is split into four almost equal length\nblocks, each reﬂecting approximately 12 minutes of measure-\nments. Each block is presented to the participant in one con-\ntinuous trial, and experimental blocks are separated by pauses\nfor the subjects.\n3We sort the brain regions based on their cross-subject\nsimilarities for different stimuli and pick a threshold value\nbased when there is a relatively big jump in the similarity\nscores.\nFigure 3: RSA across models\n4 Analyzing Neural Language Models\nIn this section, we present the results of ap-\nplying ReStA, Representational Stability Analy-\nsis, to three different language encoding mod-\nels, GoogleLM, ELMO and BERT. We investigate\nwhat type of information is captured in the learned\nrepresentations without making any explicit as-\nsumptions. Next, we apply standard RSA to, ﬁrst,\ninvestigate the relations between different compo-\nnents of the language encoding models, and sec-\nond to study the alignment of these components\nwith the activity patterns in the human brain.4\n4.1 Representational Stability Analysis\nWe deﬁne the Representational Stability as the\nsimilarity between the representations obtained\nfrom a model, when a single condition is changed,\ni.e. increase in context length. We use RSA\nto measure the similarity between the representa-\ntional spaces. And to compute RSA we use cosine\nsimilarity to measure the intra-space similarities\nand use Pearson correlation to quantify the sim-\nilarities across representational spaces.\nIn Figure 2 the representations of the differ-\nent layers given different context lengths are com-\npared for GoogleLM, ELMO and BERT. The val-\nues under the diagonal of these plots indicate\nthe ReStA when the varying condition is context\nlength. This is measured as RSA(Lk−ci, Lk−cj ),\nwhere k is the layer id and ci and cj are differ-\n4We made the code that reproduces all the experi-\nments publicly available at {https://github.com/\nsamiraabnar/Bridge}\nent conditions which in this case indicate differ-\nent context lengths. We have depicted the trends\nof how the ReStA changes for different context\nlength in Figures 4a and 4b.\nEffect of depth As we can see in Figure 2 and\nmore clearly in Figure 5, for the LSTM based\nmodels, we observe a higher degree of similarity\nbetween the two layers (∼0.75 and ∼0.80) com-\npared to BERT ( ∼0.35). This can be partly ex-\nplained by the higher number of layers in BERT,\ni.e the ﬁrst and last layer are further apart. More-\nover, the relation between the ﬁrst and last layers is\nalmost the same for all context lengths and for all\nthese three models the two layers are most similar\nwhen provided with the same amount of context.\nContext sensitivity Next, we analyse the sensi-\ntivity of different layers of each model to context\nlength. In Figures 4a and 2, we see that for both\nLSTM based models, GoogleLM and ELMO, the\nﬁrst layer, L0, is less sensitive to the changes in\nthe context length compared to the last layer, L1,\ni.e. the representations are not affected anymore\nby increasing the context length to more than 3\nsentences. A hierarchical encoding mechanism,\nwhere the ﬁrst layer is responsible for encoding\nthe local context and the second(last) layer is en-\ncoding more global information, can justify these\nresults.\nWe can see in Figure 4a, that the sensitivity to\nthe context length is more signiﬁcant in the Trans-\nformer based models compared to LSTM based\nmodels. In these models, the difference in the rep-\nresentations at different context lengths does not\nfade away as the context length increases but the\nrate of the changes becomes constant. As illus-\ntrated in Figures 4a and 2c we observe that in\nBERT, regardless of the current context length,\nadding more context leads to different represen-\ntations. In addition, in this model, the represen-\ntations from the ﬁrst layer, L0 are more context-\ndependent than those from the last layer, L11.\nSince in self-attention layers, there is a direct con-\nnection between the representations at different\npositions, the higher degree of sensitivity to con-\ntext length is not surprising. This is evidence that,\nfor computing the representations of each position\nin the input, the representations from all positions,\nno matter how far they are, are in fact taken into\naccount. We speculate that the last layer of BERT\nis less sensitive to context could be that in higher\n0 1 2 3 4 5 6\n0.2\n0.4\n0.6\n0.8\n1\nContext Length\nGoogleLM(L0) GoogleLM(L1) Elmo(L0) Elmo(L1) BERT(L0) BERT(L02) UniSentEnc GloVe\n(a) Context Sensitivity (RSA(Lk ci,Lk ci+1))\n0 1 2 3 4 5\n0\n0.2\nContext Length\n(b) Changes in Context Sensitivity (δRSA(Lk ci,Lk ci+1))\nFigure 4: Changes in RSA by increasing context length. (a) Shows how the amount of difference in the representational spaces\nchanges by increasing the context length. (b) Shows for all models that we study, regardless of whether and how much their\nrepresentations change by increasing context length, the amount of difference becomes almost constant after context length of\n3 sentences. Note that in (b), we have scaled the plot and removed some of the models to increase the readability.\n0 1 2 3 4 5 6 7\n0.2\n0.4\n0.6\n0.8\n1\nContext Length\nGoogleLM(L0→L1) Elmo(L0→L1)\nBERT(L0→L02) BERT(L0→L1)\nFigure 5: Layer similarities ( RSA(Lk ci, Lk+1 ci). Here\nwe show how increasing context length affects the similarity\nbetween different layers of the models.)\nlayers, the representations correspond to more ab-\nstract meanings, and the representational space be-\ncomes denser than the lower layers.\n4.2 RSA across Models\nIn the second step, we study whether the compu-\ntational models have learned inherently different\nrepresentational spaces. According to representa-\ntional similarity scores, among the models that we\nstudy, shown in Figure 3, UniSentEnc seems to\nlearn very different representations from ELMO,\nGoogleLM and BERT. While BERT and UniSen-\ntEnc are both Transformer based models, the rep-\nresentational space of BERT is more similar to\nthe representations from ELMO and GoogleLM\nthat are LSTM based models. This can be due\nto the fact that ELMO, GoogleLM and BERT are\ntrained with language modelling objectives, while\nUniSentEnc is trained on skip-thought and clas-\nsiﬁcation tasks and this could indicate the effect\nof the training objective on the representational\nspaces.\n5 The Relation between the Models and\nthe Activity Patterns in Human Brains\nFigure 7 shows the similarity of different computa-\ntional representation spaces with brain representa-\ntions, with respect to different amounts of context\nprovided to the models, averaged over all human\nsubjects. Due to the hemodynamic response delay,\nwe expect to see the peak in similarities after about\n4s delay. As we can see in Figure 6, the highest\nRSA for all models is at Delay = 4s, the rank-\ning of the models based on their similarities with\nbrain representations is the same for all amounts\nof delay. Interestingly, the performances of these\nmodels on the NLP tasks are not correlated with\ntheir similarity with the brain representations (but\nnote the overall low correlations). The represen-\ntations learned by LSTM based models are most\nsimilar to the brain data, and for both ELMO and\nGoogleLM the representations from lower layers,\nL0, have higher similarity scores compared to the\nhigher layers, L1. Interestingly, for UniSentEnc,\nBERT(L11) and also GoogleLM( L1), increasing\nthe context length, which usually boosts the per-\nformance of language encoding models in lan-\nguage understanding tasks (Wang and Cho, 2016),\nleads to lower similarity with brain representa-\ntions. It seems that the way these models integrate\nthe context information, pushes the representation\nfurther away from the brain representations. This\ncould mean: (1) These models are doing fairly\nwell at encoding the local context, but not at a\nmore global level. Alternatively, (2) The informa-\ntion about the more global aspects of the meaning\nis not encoded in the brain representations.\nGoogleLM(L0) GoogleLM(L1) ELMO(L0) ELMO(L1) BERT(L0) BERT(L11) UniSentEnc GloVe\n0.1\n0.2\n0.3\n0.4\nDelay=-4 Delay=-2 Delay=0 Delay=2 Delay=4 Delay=6 Delay=8 Delay=10\nFigure 6: Representational similarity of the models and brains averaged over all subjects and the four blocks at different time\ndelays after the human subjects have read the target words, when the context provided to the models is three sentences. Here\nthe delay is increasing from left to right and the error bars indicate the standard deviation across different blocks.\n0 1 2 3 4 5 6 7\n0.2\n0.3\n0.4\nContext Length\nGoogleLM(L0) GoogleLM(L1) Elmo(L0) Elmo(L1)\nBert(L0) Bert(L11) UniSentEnc GloVe\nFigure 7 : Similarity of the representations from different\nlayers of different models, given different amount of context\nwith brain representations, averaged over all subjects. Note\nthat the average RSA of brains of different human subjects is\nabout 0.55\nDifferent Segments of the StoryIf during train-\ning the models are only trained on full sentences, it\nmight be the case that the quality of their represen-\ntations, when given complete sentences, is signiﬁ-\ncantly better than when provided with incomplete\nsentences. On the other hand, the representation\nof sentences in the brain might also be more reli-\nable when the full sentence is read. To take this\ninto account, we look at the similarities of each of\nthe models with brain representations, only at the\nsteps in the story where an end of a sentence token\nis reached. Figure 8a presents the results. We see\nthat in this case, the similarity of all the models\nwith brain representations increases slightly, but\nthis could be because of the reduced dimension-\nality of the similarity matrix, and we see that the\ngeneral patterns stay similar.\nIn Figure 8b we observe that at the story seg-\nments where a name of a character is mentioned,\nthe patterns of similarities change a bit, e.g. the\nlast layer of BERT is less similar to the brain repre-\nsentations compared the ﬁrst layer of BERT, when\nan intermediate amount of context is provided to\nthe model. This ﬁnding is difﬁcult to interpret, but\nwarrants further research.\nDifferent Regions of the Brain We looked at\nthe similarity scores of the computational repre-\nsentations with the representations at different re-\ngions of the brain. This is illustrated in Figure 9\nfor subject 4 as an example. We observe that the\npatterns of RSA of different models are very sim-\nilar across different brain regions, i.e. the scores\nscale for all regions almost similarly across dif-\nferent models. Despite the low correlations be-\ntween the models and the brain activation, we ﬁnd\nthat all the models are consistently best aligned\nwith the regions in the Left Anterior Temporal\nLobe (LATL). This region is known for seman-\ntic and sometimes syntactic processing of lan-\nguage (Westerlund and Pylkk ¨anen, 2014; Bemis\nand Pylkk¨anen, 2011; Leffel et al., 2014). We also\nﬁnd some correlation with the Left Parietal Lobe,\nwhich is not known to be responsible for language\nprocessing. We also computed the average RSA\nbetween different brain regions for the eight sub-\njects, both within and across subjects, and ﬁnd that\nthe different regions of a single brain are more\nsimilar (RSA = 0.4) than the same regions of dif-\nferent brains ( RSA = 0.12). These are counter-\nintuitive ﬁndings that warrant further investiga-\ntion. If brain functions involved in story compre-\nhension are spatially localized and brains are or-\nganized similarly across individuals, we would ex-\npect the same regions from different subjects to be\nmore similar than different regions from the same\nsubject.\nPredictive Approach Besides, RSA, we can use\na predictive approach to see which regions of the\nbrain are more predictable, given the representa-\ntions from a computational model. In the predic-\ntive approach, we train a linear regression model\n0 1 2 3 4 5 6 7\n0.3\n0.4\n0.5\n0.6\nContext Length\nGoogleLM(L0) GoogleLM(L0) Elmo(L0) Elmo(L0) BERT(L0) BERT(L01) UniSentEnc GloVe\n(a) Complete sentences\n0 1 2 3 4 5 6 7\n0.5\n0.6\n0.7\nContext Length\n(b) Mentions of story character\nFigure 8: Similarity of the computational representations with brain representations at different segments of the story.\nto predict the brain activity patterns at different\nsteps of the story. This way, we can obtain more\nﬁne-grained insights into which parts of the model\ncontribute more to which regions in the brain.\nIn Figure 10, we show the results of using rep-\nresentations obtained from GoogleLM(L0) to pre-\ndict brain activity patterns of different subjects.\nSimilar to the results we obtained from RSA, the\neffect of hemodynamic response delay is clearly\nvisible here. One of the difﬁculties of employ-\ning a predictive approach is to train a regression\nmodel for such high dimensions and with so little\ndata. Hence, if the performance of the prediction\nis low, it is hard to tell if it is because we are not\nable to train a good regression model or because\nthere is no correlation between the two models.\nTo overcome this challenge, one solution could be\nto ﬁrst use RSA to reduce the search space and\nthen employ predictive modelling to gain more\nﬁne-grained insights. We postpone further anal-\nysis with the predictive approach to future studies.\n6 Discussion and Conclusion\nIn this paper, we employ a representational simi-\nlarity metric to compare the representations from\nthe language encoding models with the brain ac-\ntivity patterns, i.e. measure the alignment between\nthe brain activation patterns and activations of the\ninternal state of the models. The main advantage\nof RSA is that it treats both the brain and the model\nas a blackbox; it does not need to know how brains\nor models represent objects, words or sentences,\nbut only how similar representations are to each\nother. For N stimuli considered, the analysis only\ncompares 1\n2 N(N −1) pairs of pairwise similari-\nties (assuming similarities are symmetric), regard-\nless of the dimensionality of two representational\nspaces. This bottleneck brings many advantages\nincluding computational efﬁciency, reuse of the\nsimilarity matrices in multiple comparisons, and\nnot having to worry about how to map representa-\ntions of very different nature to each other. It also\nbrings important limitations and inevitable infor-\nmation loss, e.g. standard RSA, assumes all fea-\ntures of the representational spaces to have equal\ncontributions.\nOne of our contributions in this paper is the in-\ntroduction of ReStA, which uses RSA to measure\nthe stability of the representations from the mod-\nels when an input condition such as context length\nis changed. Comparing the representational sim-\nilarity of different layers of different models, we\nﬁnd that both architectural differences and differ-\nent training objectives have a noticeable impact on\nthe representations learned by the models and the\nway they change under different conditions. We\nsee a clear difference in the sensitivity to context\nsize between L0 and L1 in the LSTM based mod-\nels. This means, in line with results from previ-\nous work using different methods (e.g., Giulianelli\net al., 2018), that the L1 component integrates in-\nformation over time steps while L0 does not.\nUsing brain data to evaluate the representations\nlearned at different layers of each of the lan-\nguage encoding models, we ﬁnd that layers of\nthe LSTM based models achieve higher similar-\nity score with brain data compared to single word\nrepresentation models like GloVe and the Trans-\nformer based models. This observation could\nshow that the learning biases of the LSTM based\nlanguage models are closer to what happens in the\nhuman brain. Zooming into the results, we see that\nwhile changing the conditions of the inputs to the\nmodels has a signiﬁcant impact on the representa-\ntions they compute and their performance on NLP\ntasks (Khandelwal et al., 2018), these changes do\nnot get reﬂected in their alignment with the brain\nrepresentations.\nFinally, evaluating computational models of\nlanguage processing with brain imaging data for\n(a) GoogleLM (L0)\n(b) GoogleLM (L1)\n(c) ELMO (L0)\n(d) ELMO (L1)\n(e) UniSentEnc\n(f) BERT (L0)\n(g) BERT (L11)\n(h) GloVe\nFigure 9: RSA of representations learned at different layers\nof different models with representations at different regions\nof Subject4’s brain which is chosen randomly (the code ac-\ncompanying this paper can be used to generate the plots for\nthe other subjects). In order to emphasize the difference of\nthe similarity of each model with different brain regions, the\ncolor bar is scaled independently for each model. The darkest\nregion for all models is the Left Anterior Temporal Lobe.\n0 2 4 6 8 10\n0.4\n0.6\n0.8\n1 ·10−2\nDelay\nFigure 10: Predictive power of representations learned by\nGoogle LM (L0 Cinf) for brain representations in terms of\nexplained variance (each subject in a different color).\na task such as “story reading” is hard, because\nof the inherent issues in the brain data and also\nthe complexity of the task (Beinborn et al., 2019).\nBoth the RSA framework and the predictive mod-\nelling approach make it possible to make a bridge\nbetween these black boxes, neural network mod-\nels for language processing on the one hand and\nthe human brain on the other. And while each of\nthese approaches has its beneﬁts and limitations,\nthey might provide us with complementary infor-\nmation. Hence, it is invaluable to look at both of\nthem.\nIn our experiments, we observe more similari-\nties between representations learned by some ar-\nchitectures and brain representations. However,\ncaution is required when interpreting these results,\nas the representational similarity between all mod-\nels and the brain images remains very low. We\nplan to perform further analysis on various (big-\nger) datasets to get a better interpretation of what\nis happening in both the brain and these computa-\ntional models.\n7 Acknowledgement\nWe thank Dieuwke Hupkes, Arnold Kochari, the\nLanguage in Interaction BQ1 team, and the anony-\nmous reviewers for useful comments on the re-\nsearch described here and earlier versions of this\npaper. The work presented here was funded by the\nNetherlands Organization for Scientiﬁc Research\n(NWO), through a Gravitation Grant 024.001.006\nto the Language in Interaction Consortium.\nReferences\nSamira Abnar, Rasyan Ahmed, Max Mijnheer, and\nWillem Zuidema. 2018. Experiential, distributional\nand dependency-based word embeddings have com-\nplementary roles in decoding brain activity. In Pro-\nceedings of the 8th Workshop on Cognitive Modeling\nand Computational Linguistics (CMCL 2018), pages\n57–66. Association for Computational Linguistics.\nAlexandre Abraham, Fabian Pedregosa, Michael Eick-\nenberg, Philippe Gervais, Andreas Mueller, Jean\nKossaiﬁ, Alexandre Gramfort, Bertrand Thirion, and\nGa¨el Varoquaux. 2014. Machine learning for neu-\nroimaging with scikit-learn. Frontiers in neuroin-\nformatics, 8:14.\nDavid Alvarez-Melis and Tommi Jaakkola. 2018.\nGromov-Wasserstein alignment of word embedding\nspaces. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1881–1890, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nLisa Beinborn, Samira Abnar, and Rochelle Choenni.\n2019. Robust evaluation of language-brain encod-\ning experiments. International Journal of Computa-\ntional Linguistics and Applications, to appear.\nDouglas K Bemis and Liina Pylkk ¨anen. 2011. Sim-\nple composition: A magnetoencephalography inves-\ntigation into the comprehension of minimal linguis-\ntic phrases. Journal of Neuroscience , 31(8):2801–\n2814.\nJoachim Bingel, Maria Barrett, and Anders Søgaard.\n2016. Extracting token-level signals of syntactic\nprocessing from fMRI - with an application to PoS\ninduction. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 747–755. Associa-\ntion for Computational Linguistics.\nRandy L Buckner. 1998. Event-related fMRI and the\nhemodynamic response. Human brain mapping ,\n6(5-6):373–377.\nLuana Bulat, Stephen Clark, and Ekaterina Shutova.\n2017. Speaking, seeing, understanding: Correlating\nsemantic models with conceptual representation in\nthe brain. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1081–1091. Association for Com-\nputational Linguistics.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Univer-\nsal sentence encoder for english. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2018: Sys-\ntem Demonstrations, Brussels, Belgium, October 31\n- November 4, 2018, pages 169–174.\nGrzegorz Chrupała and Afra Alishahi. 2019. Correlat-\ning neural and symbolic representations of language.\nIn Proceedings of the Annual Meeting of the Associ-\nation for Computational Linguistics (ACL 2019).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics - Human Language Technologies\n(NAACL HLT 2019).\nDhanush Dharmaretnam and Alona Fyshe. 2018. The\nemergence of semantics in neural network represen-\ntations of visual information. In Proceedings of\nConference of the North American Chapter of the\nAssociation for Computational Linguistics - Human\nLanguage Technologies (NAACL HLT 2018).\nAlona Fyshe, Partha P Talukdar, Brian Murphy, and\nTom M Mitchell. 2014. Interpretable semantic vec-\ntors from a joint model of brain-and text-based\nmeaning. In Proceedings of the conference. Associ-\nation for Computational Linguistics. Meeting (ACL\n2014), volume 2014, page 489. NIH Public Access.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to investi-\ngate and improve how language models track agree-\nment information. In 1st BlackBoxNLP workshop at\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP 2019).\nAlexander G Huth, Wendy A de Heer, Thomas L Grif-\nﬁths, Fr ´ed´eric E Theunissen, and Jack L Gallant.\n2016. Natural speech reveals the semantic maps that\ntile human cerebral cortex. Nature, 532(7600):453.\nShailee Jain and Alexander G. Huth. 2018. Incorpo-\nrating context into language encoding models for\nfMRI. In Proceedings of the 32Nd International\nConference on Neural Information Processing Sys-\ntems, NIPS’18, pages 6629–6638, USA. Curran As-\nsociates Inc.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. CoRR.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Juraf-\nsky. 2018. Sharp nearby, fuzzy far away: How neu-\nral language models use context. In Proceedings of\nthe 56th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2018, Melbourne, Aus-\ntralia, July 15-20, 2018, Volume 1: Long Papers ,\npages 284–294.\nNikolaus Kriegeskorte, Marieke Mur, and Peter A Ban-\ndettini. 2008. Representational similarity analysis-\nconnecting the branches of systems neuroscience.\nFrontiers in systems neuroscience, 2:4.\nAarre Laakso and Garrison Cottrell. 2000. Content\nand cluster analysis: assessing representational sim-\nilarity in neural systems. Philosophical psychology,\n13(1):47–76.\nTimothy Leffel, Miriam Lauter, Masha Westerlund,\nand Liina Pylkk ¨anen. 2014. Restrictive vs. non-\nrestrictive composition: a magnetoencephalogra-\nphy study. Language, cognition and neuroscience ,\n29(10):1191–1204.\nTom M Mitchell, Svetlana V Shinkareva, Andrew Carl-\nson, Kai-Min Chang, Vicente L Malave, Robert A\nMason, and Marcel Adam Just. 2008. Predicting\nhuman brain activity associated with the meanings\nof nouns. science, 320(5880):1191–1195.\nBrian Murphy, Partha Talukdar, and Tom Mitchell.\n2012. Selecting corpus-semantic models for neu-\nrolinguistic decoding. In Proceedings of the First\nJoint Conference on Lexical and Computational Se-\nmantics, SemEval ’12, pages 114–123, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the conference on\nempirical methods in natural language processing\n(EMNLP 2014), pages 1532–1543.\nFrancisco Pereira, Bin Lou, Brianna Pritchett, Samuel\nRitter, Samuel J Gershman, Nancy Kanwisher,\nMatthew Botvinick, and Evelina Fedorenko. 2018.\nToward a universal decoder of linguistic meaning\nfrom brain activation. Nature communications ,\n9(1):963.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL HLT 2018, pages 2227–2237.\nPeng Qian, Xipeng Qiu, and Xuanjing Huang. 2016.\nBridging lstm architecture and the neural dynam-\nics during reading. In Proceedings of International\nJoint Conferences on Artiﬁcial Intelligence Organi-\nzation (IJCAI 2016).\nJ. K. Rowling. 1998. Harry Potter And the Sorcerer’s\nStone. Arthur A. Levine Books.\nYu-Ping Ruan, Zhen-Hua Ling, and Yu Hu. 2016. Ex-\nploring semantic representation in brain activity us-\ning word embeddings. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP 2016), pages 669–679.\nAnders Søgaard. 2016. Evaluating word embeddings\nwith fMRI and eye-tracking. In Proceedings of the\n1st Workshop on Evaluating Vector-Space Repre-\nsentations for NLP, pages 116–121. Association for\nComputational Linguistics.\nLiwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue\nWu, Kun He, and John Hopcroft. 2018. Towards un-\nderstanding learning representations: To what extent\ndo different neural networks learn the same repre-\nsentation. In Advances in Neural Information Pro-\ncessing Systems, pages 9606–9615.\nTian Wang and Kyunghyun Cho. 2016. Larger-context\nlanguage modelling with recurrent neural network.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2016.\nLeila Wehbe, Brian Murphy, Partha Talukdar, Alona\nFyshe, Aaditya Ramdas, and Tom Mitchell. 2014a.\nSimultaneously uncovering the patterns of brain re-\ngions involved in different story reading subpro-\ncesses. in press.\nLeila Wehbe, Ashish Vaswani, Kevin Knight, and\nTom M. Mitchell. 2014b. Aligning context-based\nstatistical models of language with brain activity\nduring reading. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP 2014).\nMasha Westerlund and Liina Pylkk ¨anen. 2014. The\nrole of the left anterior temporal lobe in semantic\ncomposition vs. semantic memory. Neuropsycholo-\ngia, 57:59–70.\nHaoyan Xu, Brian Murphy, and Alona Fyshe. 2016.\nBrainbench: A brain-image test suite for distri-\nbutional semantic models. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2016), pages 2017–\n2021.\n8 Supplementary Material\n8.1 Preprocessing Brain Images\nBesides the cognitive process of interest, other factors like the physiological processes in the bodies of\nthe human subjects or technical features of the MRI-machine and scanning environment may inﬂuence\nthe fMRI measurements. An important issue is therefore how to preprocess the data to ﬁlter out those\nirrelevant effects adequately.\nDetrending. We normalise the brain activations in two steps: we scale the activation values by subtract-\ning the per-voxel mean activation. We also experiment with a more elaborate preprocessing procedure,\nimplemented in the nilearn.signal.clean Python library. Detrending is a popular strategy in\ncognitive neuroscience (Abraham et al., 2014), that removes the linear trend, applies a high pass ﬁltering\nwith 0.005 Hz, and standardises the vectors.\nVoxel selection. To reduce the noise and remove the voxels which their activation is not related to the\nstory reading task, we apply two steps for selecting the voxels. In the ﬁrst step, we remove all the constant\nvoxels. These are the brain regions in which the activation does not change at all during the scanning\nexperiment. Next, we compare the similarity of different regions of the brain for all eight subjects and\nselect those regions that their activations over the different segments of the story are most similar among\nthe different subjects. To do this, we rank the regions based on the average of the similarity scores and\nthen selected the top 16 regions. After applying this voxel selection strategy, we have approximately\n10000 voxels for each subject.\nIn our experiments, we do not model the spatial dependency of the voxels. Thus, after the preprocess-\ning steps, we ﬂatten the 3D fMRI images into vectors with the size of the total number of the voxels.\n8.2 Representational Similarity Across Different Layers of Different Models\nFigure 11: RSA of different layers of different models for different context length. In this plot, for exampleELMO 0 c1 means\nrepresentation from layer 1 of ELMO, when the context length is 1 sentences."
}