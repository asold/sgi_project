{
  "title": "Unsupervised Out-of-Domain Detection via Pre-trained Transformers",
  "url": "https://openalex.org/W3175534941",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2105317100",
      "name": "Keyang Xu",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2895963235",
      "name": "Tongzheng Ren",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2107899045",
      "name": "Shikun Zhang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2765681163",
      "name": "Yihao Feng",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2803697594",
    "https://openalex.org/W2398119937",
    "https://openalex.org/W2137233422",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W3115311694",
    "https://openalex.org/W3103708104",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2899682268",
    "https://openalex.org/W2951883849",
    "https://openalex.org/W2963344330",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3161024824",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2060918050",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2963909453",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1970088130",
    "https://openalex.org/W1493526108",
    "https://openalex.org/W2531327146",
    "https://openalex.org/W3106241909",
    "https://openalex.org/W3094476119",
    "https://openalex.org/W2786088545",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2132870739",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2904981516",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963693742",
    "https://openalex.org/W2986193249"
  ],
  "abstract": "Keyang Xu, Tongzheng Ren, Shikun Zhang, Yihao Feng, Caiming Xiong. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 1052‚Äì1061\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n1052\nUnsupervised Out-of-Domain Detection via Pre-trained Transformers\nKeyang Xu1, Tongzheng Ren2, Shikun Zhang3, Yihao Feng2, Caiming Xiong4\n1 Columbia University 2 University of Texas at Austin\n3 Carnegie Mellon University 4 Salesforce Research\nkx2155@columbia.edu shikunz@cs.cmu.edu\n{tzren, yihao}@cs.utexas.edu cxiong@salesforce.com\nAbstract\nDeployed real-world machine learning appli-\ncations are often subject to uncontrolled and\neven potentially malicious inputs. Those out-\nof-domain inputs can lead to unpredictable\noutputs and sometimes catastrophic safety is-\nsues. Prior studies on out-of-domain detec-\ntion require in-domain task labels and are lim-\nited to supervised classiÔ¨Åcation scenarios. Our\nwork tackles the problem of detecting out-of-\ndomain samples with only unsupervised in-\ndomain data. We utilize the latent represen-\ntations of pre-trained transformers and pro-\npose a simple yet effective method to trans-\nform features across all layers to construct out-\nof-domain detectors efÔ¨Åciently. Two domain-\nspeciÔ¨Åc Ô¨Åne-tuning approaches are further pro-\nposed to boost detection accuracy. Our em-\npirical evaluations of related methods on two\ndatasets validate that our method greatly im-\nproves out-of-domain detection ability in a\nmore general scenario.1\n1 Introduction\nDeep neural networks, despite achieving good per-\nformance on many challenging tasks, can make\noverconÔ¨Ådent predictions for completely irrelevant\nand out-of-domain (OOD) inputs, leading to sig-\nniÔ¨Åcant AI safety issues (Hendrycks and Gimpel,\n2017). Detecting out-of-domain inputs is a funda-\nmental task for trustworthy AI applications in real-\nworld use cases, because those applications are of-\nten subject to ill-deÔ¨Åned queries or even potentially\nmalicious inputs. Prior work on out-of-domain de-\ntection (e.g., Hendrycks and Gimpel, 2017; Lee\net al., 2018; Liang et al., 2018; Hendrycks et al.,\n2019, 2020; Xu et al., 2020) mostly requires in-\ndomain task labels, limiting its usage to super-\nvised classiÔ¨Åcation. However, deployed applica-\n1Code is available at https://github.com/rivercold/\nBERT-unsupervised-OOD.\ntions rarely receive controlled inputs and are sus-\nceptible to an ever-evolving set of user inputs that\nare scarcely labeled. For example, for many non-\nclassiÔ¨Åcation tasks, such as summarization or topic\nmodeling, there are no available classiÔ¨Åers or task\nlabels, which limits the practical usage of recently\nproposed out-of-domain detection methods. There-\nfore, it is natural to ask the following question:\nCan we detect out-of-domain samples using only\nunsupervised data without any in-domain labels?\nWe regard the out-of-domain detection problem\nas checking whether the given test samples are\ndrawn from the same distribution that generates\nthe in-domain samples, which requires a weaker\nassumption than prior work (e.g., Lee et al., 2018;\nHendrycks et al., 2020). We suppose that there are\nonly in-domain samples, which allows us to under-\nstand the properties of data itself regardless of tasks.\nTherefore, methods developed for this problem are\nmore applicable than task-speciÔ¨Åc ones and can be\nfurther adapted to tasks where no classiÔ¨Åcation la-\nbels are present, such as active learning or transfer\nlearning.\nTo solve the problem, we utilize the latent em-\nbeddings of pre-trained transformers (e.g., Vaswani\net al., 2017; Devlin et al., 2019; Liu et al., 2019) to\nrepresent the input data, which allow us to apply\nclassical OOD detection methods such as one-class\nsupport vector machines (Sch√∂lkopf et al., 2001)\nor support vector data description (Tax and Duin,\n2004) on them.\nHowever, the best practice on how to extract\nfeatures from BERT is usually task-speciÔ¨Åc. For\nsupervised classiÔ¨Åcation, we can represent the text\nsequence using the hidden state of [CLS] token\nfrom the top layer. Meanwhile BERT‚Äôs interme-\ndiate layers also capture rich linguistic informa-\ntion that may outperform the top layer for speciÔ¨Åc\nNLP tasks. By performing probing tasks on each\nlayer, Jawahar et al. (2019) suggest bottom layers\n1053\nof BERT capture more surface features, middle lay-\ners focus more on syntax and semantic features are\nwell represented by top ones.\nAs no prior knowledge about OOD samples is\nusually provided in practice, deciding which layer\nof features is the most effective for OOD detec-\ntion is itself non-trivial. Some OOD samples may\njust contain a few out-of-vocabulary words; while\nothers are OOD due to their syntax or semantics.\nBased on the observations above, this paper stud-\nies how to leverage all-layer features from a pre-\ntrained transformer for OOD detection in an unsu-\npervised manner. Our contributions are three-fold:\n‚Ä¢By analyzing all layers of (Ro)BERT(a) mod-\nels, we empirically validate that it is hard to extract\nfeatures from a certain layer that work well for any\nOOD datasets.\n‚Ä¢We propose a computationally efÔ¨Åcient way to\ntransform all-layer features of a pre-trained trans-\nformer into a low-dimension one. We empirically\nvalidate that the proposed method outperforms\nbaselines that use one-layer features or by simple\naggregations of all layers.\n‚Ä¢We propose two different techniques for Ô¨Åne-\ntuning a pre-trained transformer to further improve\nits capability of detecting OOD data.\n2 Problem Setup\nAssume that we have a collection of text inputs\nDn := {xi}n\ni=1, we want to construct an out-of-\ndomain detector that takes an unseen new input u\nand determines whether uuucomes from the same\ndistribution that generates Dn. We adopt a more\npractical setting where we have no prior knowl-\nedge of what out-of-domain inputs look like. In\nthis case, training a domain classiÔ¨Åer directly is\nnot feasible. The out-of-domain detector can be\ndescribed mathematically as:\ng(uuu,œµ) =\n{\nTrue if I(uuu) ‚â§œµ,\nFalse if I(uuu) >œµ,\nwhere I(¬∑) denotes the anomaly score function, and\nœµis a chosen threshold to ensure that the true posi-\ntive rate is at a certain level (e.g.,95%) (Hendrycks\nand Gimpel, 2017; Liang et al., 2018; Lee et al.,\n2018). The OOD detection problem boils down to\ndesigning I(¬∑) such that it assigns in-domain inputs\nlower scores than out-of-domain inputs.\nThere are two different scenarios, considering\nif we have any in-domain labels for data xi ‚ààDn.\nHere we deÔ¨Åne in-domain labels as any speciÔ¨Åc\nsupervised task labels, such as sentiments, intents\nor topics of the text.\nWith in-domain labels Suppose that we have\nmulti-class label yi ‚àà[K] and Dn = {(xxxi,yi)}n\ni=1.\nGiven a classiÔ¨Åer htrained with Dn, we can use\nmaximum calibrated softmax probability with tem-\nperature scaling as the anomaly score (Liang et al.,\n2018; Hinton et al., 2015):\nI(xxx) := ‚àímax\ni‚àà[K]\nexp (hi(xxx)/T)‚àëK\nj=1 exp (hj(xxx)/T)\n,\nwhere hi(xxx) is the output logits of the multi-class\nclassiÔ¨Åer, and T is the temperature that is selected\nsuch that the true positive rate is at a given rate\n(e.g., 95% in Liang et al. (2018)). This method is\nknown as Maximum Softmax Probability (MSP),\nwhich requires multi-class labels to train a classiÔ¨Åer\nand thus limits its application in practice. We argue\nthat requiring in-domain labels is a less practical\nscenario for OOD detection and will not be further\ndiscussed it in this paper.\nWithout in-domain labels The setting of no in-\ndomain labels is our major focus. Under this as-\nsumptin, the models we can obtain in hand are usu-\nally not classiÔ¨Åers, but feature extractors instead.\nThen it is natural to resort to classic outlier detec-\ntion methods like one-class support vector machine\n(Sch√∂lkopf et al., 2001), support vector data de-\nscription (Tax and Duin, 2004) or kernel density\nestimation (KDE) for estimating the support or the\ndensity of the in-domain data distribution.\nWhen applying such methods to text data, the\nmajor focus of prior work is to design a good\nnetwork structure or learning objectives (Ruff\net al., 2018). Instead, in this paper we mainly fo-\ncus on how to obtain good representations from\npre-trained transformers and design new anomaly\nscores without modifying its structure, while still\nobtaining good OOD detection performance.\n3 Model and Feature Learning\nBERT and its variants such as RoBERTa (e.g., De-\nvlin et al., 2019; Liu et al., 2019) are pre-trained\non large-scale public data (denoted as Dpub) us-\ning self-supervised tasks, such as language model\nand next sentence prediction. These models show\n1054\nmo ve 100  d oll ar s fro m m y s avi ngs to my ch ecki ng\nle t me  know how to make  a vacation r e que st\nIs rice ok after 3 days in the refrigerator\ncan you tell me a joke about politicians\ntell the miles it will take to get to las vegas from \nsan die go\n‚Ä¶\nTraining Set Dn\nTest Sample\nhow  are my sports teams doing\n‚Ä¶\nx1\n[cls]\nx2\ntell\nx14\nsan\nx15\ndiego\nFeed\nForward\nSelf-\nAttention\nz1\nz2\nz14\nz15\n‚Ä¶\n‚Ä¶\nùëì1\n( ƒâ1 , ‚àë1 )\nFeed\nForward\nSelf-\nAttention\nz1\nz2\nz13\nz14\n‚Ä¶\nùëì2\n( ƒâ2 , ‚àë2 )\nFeed\nForward\nSelf-\nAttention\nz1\nz2\nz13\nz14\n‚Ä¶\nùëìL\n( ƒâL , ‚àëL )\nFeed\nForward\nSelf-\nAttention\nz1\nz2\nz13\nz14\n‚Ä¶ Feed\nForward\nSelf-\nAttention\nz1\nz2\nz13\nz14\n‚Ä¶‚Ä¶\nx1\n[cls]\nx2\nhow\nx6\nteams\nx7\ndoing\nFeed\nForward\nSelf-\nAttention\nz1\nz2\nz6\nz7\n‚Ä¶\nM1 M2 ML\nOC-SVM\nAnomaly \nScore\n‚Ä¶\nt1\n[cls]\nt2\nt14\nt15\nBCAD\nFine-tuning\nIMLM \nFine-tuning\n‚Ä¶\nÀÜ ÀÜ ÀÜ\nFigure 1: An overview of using Mahalanobis distance features (MDF) extracted from a pre-trained transformer f\nto detect out-of-domain data. We estimate mean ÀÜcl and covariance matrix ÀÜŒ£l for each layer of f by samples from\nan unsupervised training setDn; and then extract MDF ofDn to optimize a OC-SVM. Given an unseen test sample,\nits feature M is extracted using ÀÜcl and ÀÜŒ£l and then fed into OC-SVM for an anomaly score. Two domain-speciÔ¨Åc\nÔ¨Åne-tuning methods, IMLM and BCAD, can be further applied to BERT to boost detection accuracy.\npromising results when transferred to tasks in other\ndomains. We aim to leverage features obtained\nfrom pre-trained transformers to construct OOD\ndetectors in lieu of in-domain labels in Dn.\n3.1 BERT features for OOD detection\nAfter pretraining, we can obtain a BERT/RoBERTa\nmodel f with Llayers. We denote f‚Ñì(x) ‚ààRd as\nthe d-dimensional feature embeddings correspond-\ning to the ‚Ñì-th layer for input x, and f(xxx) is the\noverall representation using all layers of f. We\nexplore the following methods to extract BERT\nfeatures to construct OOD detectors.\nFeatures from the ‚Ñì-th layer f‚Ñì Options to ex-\ntract fl(xxx) include using the hidden states of [CLS]\ntoken or averaging all contextualized token embed-\ndings at the ‚Ñì-layer. Then we can directly construct\nan OOD detector based on features from f‚Ñì of each\ninput xxx in Dn using existing pure sample based\nmethods, such as one-class support vector machine\n(OC-SVM).2\nFeatures from all layers Using BERT features\nfrom only one layer might not be sufÔ¨Åcient, as\nprior work (Jawahar et al., 2019) has explored that\ndifferent layers of BERT capture distinct linguis-\ntic properties, e.g., lower-level features capturing\nlexical properties, middle layers representing syn-\ntactic properties, and semantic properties surfacing\nin higher layers. The effects of BERT features\nfrom different layers on detecting OOD data are\n2It is also possible to use other related one-class classiÔ¨Åca-\ntion methods, such as Isolation Forest. However, in practice\nwe Ô¨Ånd OC-SVM works the best and we use it in our empirical\nevaluations.\nyet to be investigated. One straightforward way that\nleverages all Llayers is to concatenate all layer-\nwise features f‚Ñì(xxx), which has no information loss.\nHowever, this solution is computationally expen-\nsive and thus hard to optimize OC-SVM or kernel\nbased methods. Another solution is to perform ag-\ngregation likes max- or mean-pooling along the fea-\nture dimension across all layers, sacriÔ¨Åcing some\ninformation in exchange for efÔ¨Åciency.\nIn this paper, we propose a simple yet effective\nmethod (described below) to use latent representa-\ntions from all layers of a pre-trained transformer\nand can automatically decide features from which\nlayers are important. Besides, this method is com-\nputationally efÔ¨Åcient, only requiring us to solve a\nlow-dimensional constrained convex optimization.\nMahalanobis distance as features (MDF) for\nall layers Support Vector Data Description\n(SVDD) (Tax and Duin, 2004) is a technique re-\nlated to OC-SVM where a hypersphere is used to\nseparate the data instead of a hyperplane. However,\nthe features provided by deep models may not be\nseparable by hyperspheres. We focus on a general-\nization of the hypersphere called hyper-ellipsoid to\naccount for such surface shapes.\nSuppose that we use the concatenated features\nfrom all layers Œ¶(xxx) = [ f1(xxx),...,f L(xxx)]‚ä§ ‚àà\nRd¬∑L and consider the following optimization\nproblem to Ô¨Ånd the hyper-ellipsoid, which is\nsimilar to the optimization formula of SVDD:\nmin\nR,c,Œ£,Œæ\n1\n2‚à•Œ£‚à•2\nFr +\n(\nR2 + 1\nŒΩn\n‚àë\ni\nŒæi\n)\n,\ns.t.‚à•Œ¶(xi) ‚àíc‚à•2\nŒ£‚àí1 ‚â§R2 +Œæi , Œæi ‚â•0,‚àÄi, (1)\n1055\nwhere Œ¶ is the feature map, c is the center\nof the hyper-ellipsoid, and Œ£ is a symmetric\npositive deÔ¨Ånite matrix that reÔ¨Çects the shape\nof the ellipsoid. And R reÔ¨Çects the volume of\nthe hyper-ellipsoid. 3 Here we also introduce\na regularization term 1\n2 ‚à•Œ£‚à•2\nFr to constrain the\ncomplexity of Œ£. If Œ£ = I, then the optimization\nproblem is identical to one-class SVDD.\nSolving Eq (1) exactly can be difÔ¨Åcult, since it in-\nvolves Ô¨Ånding the optimalŒ£ of shape D√óD, where\nD= d¬∑Lis the dimension of the features. For the\nconcatenated features Œ¶(x), Dcan be tens of thou-\nsands or even hundreds of thousands, which makes\nthe exact solution computationally intractable. To\ntackle the problem, we consider asimple and com-\nputationally efÔ¨Åcient approximation of the solu-\ntion, which can be useful in practice.\nFirst, we decompose the feature space into sev-\neral subspaces, based on the features from different\nlayers, i.e., assume Œ£ is a block diagonal matrix,\nand Œ£‚Ñì reÔ¨Çects the shape of feature distribution at\nlayer ‚Ñì. By a straightforward calculation, we have:\n‚à•Œ¶(x) ‚àíc‚à•2\nŒ£‚àí1 =\nL‚àë\n‚Ñì=1\n‚à•f‚Ñì(x) ‚àíc‚Ñì‚à•2\nŒ£‚àí1\n‚Ñì\n,\nwhere we decompose the center c to be the\ncenter of each layer c = [ c1,..., cL]‚ä§. Still,\noptimizing c‚Ñì and Œ£‚Ñì can be difÔ¨Åcult since\nthe dimension of f‚Ñì(x) can be high. Based on\nthe intuition that c‚Ñì and Œ£‚Ñì should not deviate\nfrom the empirical mean and covariance estima-\ntion ÀÜc‚Ñì and ÀÜŒ£‚Ñì from the training data, we can\nreplace c and Œ£‚Ñì with the following approximation:\nc‚Ñì ‚âàÀÜc‚Ñì = 1\nn\nn‚àë\ni=1\n[f‚Ñì(xi)] ,\nŒ£‚Ñì ‚âà\nÀÜŒ£‚Ñì\nw‚Ñì\n= 1\n(n‚àí1)w‚Ñì\nn‚àë\ni=1\n(f‚Ñì(xi) ‚àíÀÜc‚Ñì)(f‚Ñì(xi) ‚àíÀÜc‚Ñì)‚ä§,\nwhere w‚Ñì is a layer-dependent constant. Now\nwe only need to Ô¨Ånd proper {w‚Ñì}L\n‚Ñì=1 as well\nas the corresponding R and {Œæi}n\ni=1, which is a\nlow-dimension optimization problem that only\nscales linearly with the number of layer L. We\nfurther deÔ¨Åne:\nM‚Ñì(xi) = (f‚Ñì(xi) ‚àíÀÜc‚Ñì)‚ä§ÀÜŒ£‚àí1\n‚Ñì (f‚Ñì(xi) ‚àíÀÜc‚Ñì) ,\n3We can further assume ‚à•Œ£‚à• = 1, where the norm can\nbe the operator norm or Frobenius norm, which can give the\ndeÔ¨Ånition of the hyper-ellipsoid with unique Œ£ and R.\nwhere the square root of M‚Ñì(xxxi) is also referred to\nas the Mahalanobis distance of the features of data\nxi from layer ‚Ñì. Assume w = [w1,...,w L]‚ä§ ‚àà\nRL and M(x) = [ M1(x),...,M L(x)]‚ä§ ‚ààRL,\nthen we have:\n‚à•Œ¶(x) ‚àíc‚à•2\nŒ£‚àí1 = ‚ü®w,M(x)‚ü©.\nAs ‚à•Œ£‚à•2\nFr = ‚àëL\n‚Ñì=1\n‚à•ÀÜŒ£l‚à•2\nFr\nw2\n‚Ñì\nis not convex w.r.t w,\nwe instead minimize ‚àí1\n2 ‚à•w‚à•2\n2, which has a similar\nregularization effect on Œ£ (as we don‚Äôt want‚à•w‚à•2\nto be small, which can make ‚à•Œ£‚à•Fr very large). So\nthe Ô¨Ånal optimization problem to solve is:\nmin\nR,w,Œæ\n‚àí1\n2‚à•w‚à•2\n2 + R2 + 1\nŒΩn\n‚àë\ni\nŒæi,\ns.t. ‚ü®w,M(xi)‚ü©‚â§ R2 + Œæi,Œæi ‚â•0 ,‚àÄi, (2)\nwhich in fact is a one-class SVM with a linear\nkernel, with Mahalanobis distance of each layers\nas features (MDF), and it can be simply solved with\nthe standard convex optimization. We illustrate our\nproposed algorithm in Figure 1.\nRemark Note that the optimization in Eq (2) is\nnot identical as that in Eq (1), since we are using\nempirical sample mean {ÀÜc‚Ñì}L\n‚Ñì=1 and covariance\n{ÀÜŒ£‚Ñì/w‚Ñì}L\n‚Ñì=1 to replace the original parameters c\nand Œ£ in Eq (1), which are hard to optimize when\nthe dimension of the concatenated features Œ¶(x)\nis high. Also, our approximation from Eq (1) to\nEq (2) is different from the known result that when\nŒ¶(x) is the inÔ¨Ånite-dimensional feature map of the\nwidely used Gaussian RBF kernels, OC-SVM and\nSVDD are equivalent and asymptotically consis-\ntent density estimators (Tsybakov et al., 1997; Vert\net al., 2006). In our case, Œ¶(x) is the concatenated\nfeatures from all layers of pre-trained transform-\ners, which makes our approximation fundamentally\ndifferent from prior work.\n3.2 Feature Ô¨Åne-tuning\nWe can also Ô¨Åne-tune the pre-trained transformer\nf on the unsupervised in-domain dataset Dn so\nthat f(xxx) can better represent the distribution of\nDn. We explore two domain-speciÔ¨Åc Ô¨Åne-tuning\napproaches.\nIn-domain masked language modeling (IMLM)\nGururangan et al. (2020) Ô¨Ånd that domain-adaptive\nmasked language modeling (Devlin et al., 2019)\nwould improve supervised classiÔ¨Åcation capabil-\nity of BERT when it is transferred to that domain.\n1056\nCross-corpus Examples (SST)\nType Source Text\nIn-Domain SST if you love reading and or poetry , then by all means check it out\nIn-Domain SST there ‚Äôs no disguising this as one of the worst Ô¨Ålms of the summer\nOut-of-Domain RTE capital punishment is a deterrent to crime\nOut-of-Domain SNLI a crowd of people are sitting in seats in a sports ground bleachers\nOut-of-Domain Multi30K a trailer drives down a red brick road\nCross-intent Examples (CLINIC150)\nType Intent Text\nIn-Domain Transfer move 100 dollars from my savings to my checking\nIn-Domain PTO Request let me know how to make a vacation request\nIn-Domain Food Last is rice ok after 3 days in the refrigerator\nIn-Domain Tell Joke can you tell me a joke about politicians\nOut-of-Domain ‚Äî how are my sports teams doing\nOut-of-Domain ‚Äî create a contact labeled mom\nOut-of-Domain ‚Äî what‚Äôs the extended zipcode for my address\nTable 1: Examples of in-domain/out-of-domain samples for SST and CLINIC150. The source labels for SST and\nthe intent labels for CLINIC150 are here just for illustration and are not included in Dn. None of the above OOD\nsamples are provided in training as well.\nSimilarly, we can do MLM on Dn and argue this\nwould make the features of Dn concentrate, bring-\ning beneÔ¨Åts to downstream OOD detection.\nBinary classiÔ¨Åcation with auxiliary dataset\n(BCAD) Another way of Ô¨Åne-tuning the model\nf is to use the public dataset Dpub that pretrains it.\nWe consider the training data in Dn as in-domain\npositive samples and data in the public dataset\nDpub as OOD negative samples. We add a new\nclassiÔ¨Åcation layer on top offand update this layer\ntogether with all parameters of f by performing a\nbinary classiÔ¨Åcation task. In practice, we only need\na small subset of Dpub, denoted as ÀúDpub, for Ô¨Åne-\ntuning. Since ÀúDpub is publicly available and has no\nlabels, we do not violate the unsupervised setting.\nÀúDpub does not provide any information about the\nOOD samples at test time as well.\nBesides, the added classiÔ¨Åcation layer can actu-\nally be applied for OOD detection using the MSP\nmethod, and this is exactly the setting of zero-shot\nclassiÔ¨Åcation, which we use as a baseline for com-\nparison in our experiments.\n4 Experiments\nDatasets We consider two distinct datasets for\nexperiments, where one is to regard text from un-\nseen corpora as OOD, and the other one is to detect\nclass-level OOD samples within the same corpus.\n‚Ä¢ Cross-corpus dataset (SST) We follow the\nexperimental setting in Hendrycks et al. (2020),\nby providing in-domain Dn with the original train-\ning set of SST dataset (Socher et al., 2013) and\nconsidering samples from four other datasets (i.e.,\n20 Newsgroups (Lang, 1995), English-German\nMulti30K (Elliott et al., 2016), RTE (Dagan et al.,\n2005) and SNLI (Bowman et al., 2015)) as OOD\ndata. For evaluation, we use the original test data\nof SST as in-domain positives and randomly pick\n500 samples from each of the four datasets as OOD\nnegatives. We do not include any sentiment labels\nfrom SST to Dn for training.\n‚Ä¢ Cross-intent dataset (CLINIC150) This is\na crowdsourced dialog dataset (Larson et al., 2019),\nincluding in-domain queries covering 150 intents\nand out-of-domain queries that do not fall within\nany of the 150 intents. We use all 15,000 queries\nthat are originally in its training data as in-domain\nsamples but discard their intent labels. For eval-\nuation, we mix the 4,500 unseen in-domain test\nqueries with 1,000 out-of-domain queries and wish\nto separate two sets by their anomaly scores.\nExamples taken from the two datasets can be\nfound in Table 1. Note that for both datasets, only\nthe in-domain samples are used for training, and\nthe source/intent labels are not used in our experi-\nments.\nEvaluation metrics We rank all test samples by\ntheir anomaly scores and follow Liang et al. (2018)\nto report four different metrics, namely, Area Un-\nder the Receiver Operating Characteristic Curve\n(AUROC), Detection Accuracy ( DTACC), and\n1057\nArea under the Precision-Recall curve (AUPR) for\nin-domain and out-of-domain testing sentences re-\nspectively, denoted by AUIN and AUOUT.\nModel conÔ¨Ågurations We evaluate all methods\nwith both BERT and RoBERTa (base models with\n768 latent dimensions and 12 layers).\nChoice of ÀúDpub for BCAD We adopt the\nBooksCorpus (Zhu et al., 2015) and English\nWikipedia, which are the sources used in common\nby BERT and RoBERTa for pre-training. We split\nparagraphs into sentences and sample ÀúDpub to have\nthe same size as Dn for BCAD.\nBaselines To examine the effectiveness of our\nnewly proposed anomaly score based on MDF that\nutilizes the representations of all layers, we com-\npare it with the following baselines.\n‚Ä¢(Ro)BERT(a)-Single layer: It uses f‚Ñì(xxx) men-\ntioned above. We iterate all 12 layers and detailed\nresults of each layer are discussed in Section 5.1.\n‚Ä¢(Ro)BERT(a)-Mean pooling: we construct all-\nlayer representation by averaging all f‚Ñì(xxx), which\nhas 768 dimensions.\n‚Ä¢(Ro)BERT(a)-Max pooling: we aggregate all\nlayers by picking largest values along each feature\ndimension and get a 768-dimension vector.\n‚Ä¢(Ro)BERT(a)-Euclidean distance as features\n(EDF): we replace Mahalanobis distance with Eu-\nclidean distance and still obtain a 12-dimension\nvector.\n‚Ä¢TF-IDF: we extract TF-IDF features and adopt\nSVD to reduce high-dimensional features to 100\ndimensions for computational efÔ¨Åciency.\nAll of the above methods extract features as the\ninput to OC-SVM to compute anomaly scores.\n‚Ä¢BCAD + MSP: It performs zero-shot classiÔ¨Å-\ncation after BCAD Ô¨Åne-tuning, as discussed in Sec-\ntion 3. The temperature scaling is tuned to achieve\nthe best result. This method is not applicable when\nno ÀúDpub is provided.\n5 Results and Discussions\nIn this section, we present the results for our exper-\niments and summarize our Ô¨Åndings.\n5.1 Using single-layer feature f‚Ñì(x)\nTable 2 shows results obtained from using the\n[CLS] embedding or averaging token embeddings\nLayer SST CLINIC150\nBERT RoBERTa BERT RoBERTa\nCLS A VG CLS A VG CLS A VG CLS A VG\n12 92.7 81.7 89.8 87.8 61.5 60.2 53.4 51.6\n11 88.8 66.3 88.8 68.8 57.3 59.0 51.6 55.5\n10 87.7 52.1 79.6 68.4 56.6 55.4 53.8 56.2\n9 85.5 50.7 84.2 67.2 56.8 56.5 58.3 56.5\n8 82.9 57.6 78.7 67.7 61.6 55.8 58.9 56.0\n7 85.8 59.2 83.6 67.5 62.3 63.0 57.5 56.4\n6 76.4 61.9 73.0 67.8 58.2 62.3 55.5 56.7\n5 74.2 58.2 63.5 67.2 56.3 62.8 56.2 57.1\n4 66.7 67.4 70.0 69.8 61.9 60.9 52.7 57.8\n3 65.8 67.5 62.9 69.3 54.3 59.4 51.0 58.5\n2 62.6 63.2 75.7 68.8 60.4 58.6 55.6 59.9\n1 68.1 63.5 70.0 71.0 60.9 64.6 55.6 58.5\nTable 2: The AUROC scores of OOD detection\non the SST/CLINIC150 dataset for each layer of\nBERT/RoBERTa. CLS denotes using the hidden state\nof the [CLS] token and A VG represents averaging all to-\nken embeddings in the same layer. Layer 12 indicates\nthe top layer and layer 1 is the bottom layer right af-\nter the word embedding layer. The best result for each\ncolumn is marked in bold.\n(A VG) at each layer of (Ro)BERT(a) models in the\ncross-corpus and the cross-intent dataset.\nWe observe that detecting cross-intent OOD sam-\nples in CLINIC150 is more challenging than that of\ncross-dataset OOD data in SST. This is mainly be-\ncause the OOD samples in CLINIC150 are sorted\nby humans and the differences between intents can\nbe subtle. We will further compare the performance\nof these two settings in Figure 2.\nThe best f‚Ñì(xxx) for OOD is dataset-speciÔ¨Åc For\nthe cross-corpus dataset (SST), we Ô¨Ånd that the\nbest results come from the top layer of both\n(Ro)BERT(a). However, for the cross-intent dataset\n(CLINIC150), the middle layers perform the best\nwhen using [CLS], while the bottom layers achieve\nthe best results with A VG. This indicates that OOD\ndistributions are not simply based on certain types\nof linguistic features and the strategy of choos-\ning f‚Ñì(xxx) is dataset-speciÔ¨Åc; for some dataset, se-\nmantic features play a more important role, while\nsometimes we need to focus on syntactic or lexical\nfeatures. This validates the assumption that it is\nbeneÔ¨Åcial to fully utilize all layers of the hidden\nrepresentations from pre-trained transformers to\ndetect OOD instances.\nWe Ô¨Ånd using f‚Ñì(xxx) of BERT is generally better\n1058\nSST CLINIC150\n#feats AUROC DTACC AUIN AUOUT AUROC DTACC AUIN AUOUT\nBERT-Single layer (best) 768 92.7 85.8 93.4 91.7 64.6 60.9 88.4 26.7\nRoBERTa-Single layer (best) 768 89.8 91.5 79.2 93.8 59.9 57.6 86.8 22.7\nBERT + Mean-Pooling 768 81.8 76.5 77.2 82.8 62.9 59.9 87.0 27.9\nBERT + Max-Pooling 768 67.2 66.1 64.2 59.4 63.0 60.0 88.0 25.8\nRoBERTa + Mean-Pooling 768 91.0 92.3 80.9 94.5 57.1 56.2 85.7 20.5\nRoBERTa + Max-Pooling 768 93.2 91.9 89.3 95.1 54.9 54.4 84.8 19.4\nBERT + EDF 12 90.1 84.8 92.8 84.2 55.3 55.2 84.3 20.3\nBERT + MDF 12 93.3 87.5 94.9 89.1 76.7 71.1 93.4 38.2\nBERT + IMLM + MDF 12 93.6 88.1 97.5 89.4 77.8 72.2 93.8 39.1\nBERT + BCAD + MDF 12 97.0 94.5 98.0 94.8 81.2 74.5 94.6 47.4\nBERT + IMLM + BCAD + MDF 12 98.1 95.4 98.7 95.9 82.1 75.6 95.0 47.6\nRoBERTa + EDF 12 99.5 95.8 99.5 99.4 56.9 56.9 86.3 19.6\nRoBERTa + MDF 12 99.8 97.7 99.8 99.8 78.6 71.9 93.8 42.6\nRoBERTa + IMLM + MDF 12 99.9 97.8 99.8 99.8 80.1 73.1 94.5 44.9\nRoBERTa + BCAD + MDF 12 99.2 96.6 99.4 98.7 80.5 72.9 94.3 49.4\nRoBERTa + IMLM + BCAD + MDF12 99.9 98.6 99.9 99.9 84.4 76.7 95.4 59.9\nTF-IDF + SVD 100 78.0 72.0 78.2 73.2 58.5 56.5 86.2 21.8\nBERT + BCAD + MSP - 68.5 69.0 61.5 65.4 68.3 63.5 89.7 34.1\nRoBERTa + BCAD + MSP - 73.7 69.3 69.0 75.3 62.1 59.6 85.9 27.8\nTable 3: OOD detection performance on SST and CLINIC 150 for all models. OC-SVM is used for computing\nanomaly scores except MSP, and its parameters size is #feats. For (Ro)BERT(a)+Single-layer, the best results in\nTable 2 are reported. For all MDF-based model, we only report results of A VG as sequence representation at each\nlayer due to space limit. Larger values of all four metrics indicate better performances. The best result for each\nmetric is marked in bold.\nthan RoBERTa, especially with [CLS]. We guess\nnext sentence prediction may cause this, which pre-\ntrains on [CLS] and is exclusive for BERT.\nIn later sections, (Ro)BERT(a)-Single layer will\nrefer to the best one in Table 2.\n5.2 Overall OOD detection performance\nWe report the empirical results of OOD detection\nin Table 3 and the following observations.\nPre-trained transformers produce good feature\nrepresentations Methods using single-layer fea-\nture f‚Ñì outperforms frequency-based features (TF-\nIDF) and zero-shot classiÔ¨Åcation (MSP), which val-\nidates the strong representation capability granted\nby self-supervised pre-training.\nSimple aggregations of all layers are not so ef-\nfective The results of max-pooling and mean-\npolling are not very promising. Even though we\nobserve an absolute 0.5% boost in SST using max-\npooling, using the best single layer actually outper-\nforms those simple aggregations in CLINIC150.\nMDF is more effective MDF consistently out-\nperforms methods that directly use features f‚Ñì(xxx),\nsimple aggregations of f‚Ñì(xxx), or TF-IDF features\non all four metrics. In terms of AUROC, MDF out-\nperforms the best single-layer of (Ro)BERT(a) by\nabsolute 7.1% on SST and 14.0% on CLINIC150.\nMDF also performs better than EDF. Note that\nEuclidean distance is a special case of Mahalanobis\ndistance when the covariance is an identity matrix.\nEmpirically, the features generated by neural mod-\nels are not invariant across all dimensions; and\nthe comparison between MDF and EDF validates\nSVDD with a hyper-ellipsoid is better than a hyper-\nsphere.\nMDF is more efÔ¨Åcient in training OC-SVM\nNotice that our approach is also more computa-\ntionally efÔ¨Åcient when obtaining optimal www and\nRRR since the optimization is performed on a new\ntransformed low dimensional data space (d= 12\nis number of layers in f). See column #feats in\nTable 3 for detailed comparisons.\n1059\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nMSP\nSingle Layer\nMDF\nMDF+IMLM+BCAD\n0 10 20 30 40 50\nAnomaly score\nIn-domain\nOut-of-domain\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nMSP\nSingle Layer\nMDF\nMDF+IMLM+BCAD\n5 10 15 20 25\nAnomaly score\nIn-domain\nOut-of-domain\n(a) ROC Curve on SST (b) I(x) on SST (c) ROC Curve on CLINIC150 (d) I(x) on CLINIC150\nFigure 2: (a): ROC curves on the SST dataset. (b): Distribution of anomaly scores generated by IMLM + BCAD + MDF. Both\nÔ¨Ågures are based on the BERT model. (c): ROC curves on the CLINIC150 dataset. (d): Distribution of anomaly scores generated\nby IMLM + BCAD + MDF.\nSentence GT TF-IDF Single MDF\n(a) is a visa necessary for traveling to south africa In In In In\n(b) can you tell me who sells dixie paper plates Out In Out Out\n(c) can you tell me how to solve simple algebraic equations with one variableOut Out In Out\n(d) what oil is best for chicken Out In In In\nTable 4: Examples of CLINIC150 with predictions from three models, which is ‚ÄúIn‚Äù when sample‚Äôs anomaly score is lower\nthan 25th percentile and ‚ÄúOut‚Äù when larger than 75th percentile. GT is the ground truth and Single stands for BERT-Single.\nFine-tuning techniques improve performance\nFrom Table 3, we can see both MILM and BCAD\nimprove OOD detection performance when incor-\nporated with MDF separately. The overall best de-\ntecting performance is achieved by MILM + BCAD\n+ MDF, combining both proposed Ô¨Åne-tuning meth-\nods with MDF.\nWe also Ô¨Ånd that RoBERTa outperforms BERT\nwhen using MDF, even though features from a sin-\ngle layer prefers BERT in Table 2.\n5.3 Visualizations\nWe plot the ROC curves of four different anomaly\nscores on SST in Figure 2 (a) and on CLINIC150\nin Figure 2 (c), conÔ¨Årming that our proposed MDF\nand two Ô¨Åne-tuning techniques improve the ability\nin detecting OOD samples. We also present the\ndistributions of anomaly scores I(x) generated by\nour best method in Figure 2 (b) for SST and in\nFigure 2 (d) for CLINIC150. For SST, the OOD de-\ntector can clearly separate I(x) of in-domain and\nout-domain samples, and the in-domain scores are\ndensely concentrated on the low-score region. Al-\nthough for CLINIC150, we do observe some OOD\nsamples mixing with in-domain ones, accounting\nfor the gap of metric scores between two datasets.\n5.4 Case Studies\nWe present some examples from CLINIC150 to-\ngether with their corresponding predictions by TF-\nIDF, BERT-single layer and MDF methods in Ta-\nble 4. TF-IDF predicts false positives for examples\n(b) and (d) because most of the words in the exam-\nple test query are seen in the training set, like ‚Äúi\nwould like you to buy me some paper plates‚Äù (in-\ntent: order), ‚Äúi need to know how long to cook\nchicken for‚Äù (intent: cooking time) and etc. BERT-\nsingle layer learns the syntax of ‚Äúcan you tell me\nhow to ...‚Äù, which is frequently seen in the train-\ning data, but it fails to discern that the semantic\nmeaning is out-of-domain. For example (d), all\nmodels make the mistake, potentially associating it\nwith the intent: recipe (‚Äúi need to Ô¨Ånd a good way\nto make chicken soup‚Äù or ‚Äúwhat‚Äôs the best way to\nmake chicken stir fry‚Äù).\n6 Related Work\nOut-of-domain detection is essentially an important\ncomponent for trustworthy machine learning appli-\ncations. There are two lines of work proposed to\nperform out-of-domain detection. One is to tackle\nthe problem in speciÔ¨Åc multi-class classiÔ¨Åcation\ntasks, where well-trained classiÔ¨Åers are utilized to\ndesign anomaly scores (e.g., Hendrycks and Gim-\npel, 2017; Liang et al., 2018; Lee et al., 2018; Card\net al., 2019; Hendrycks et al., 2020; Xu et al., 2020),\nThose methods can only be useful when multi-class\nlabels are available, which limits their application\nin more general domains. Our proposed work goes\nbeyond this limitation and can utilize large amounts\nof unsupervised data.\nAnother line of work is based on support esti-\nmation or density estimation, which assumes that\nthe in-domain data is in speciÔ¨Åc support or from\nthe high density region (Sch√∂lkopf et al., 2001; Tax\n1060\nand Duin, 2004). In principle, our work is closely\nrelated to this line of work. Besides, Zhai et al.\n(2016); Ruff et al. (2018); Zong et al. (2018) also\nleverage the features of neural networks, though\nthese methods require designing speciÔ¨Åc network\nstructures for different data. Our work circumvents\nthe issues of prior work by designing a computa-\ntionally efÔ¨Åcient method that leverages the power-\nful representations of pre-trained transformers.\nFinally, the Ô¨Åne-tuning techniques we use to im-\nprove the representation of data are closely related\nto unsupervised pre-training for transformers (De-\nvlin et al., 2019; Yang et al., 2019), and recently\nproposed contrastive learning (e.g., He et al., 2020;\nChen et al., 2020). Lately, Gururangan et al. (2020)\ndiscover that performing pre-training (MLM) on\nthe target domain with unlabeled data can also help\nto improve downstream classiÔ¨Åcation performance.\nTo the best of our knowledge, our method is the\nÔ¨Årst to incorporate transformers and pre-training\ntechniques to improve out-of-domain detection.\n7 Conclusion\nWe study the problem of detecting out-of-domain\nsamples with unsupervised in-domain data, which\nis a more general setting for out-of-domain detec-\ntion. We propose a simple yet effective method us-\ning Mahalanobis distance as features, which signif-\nicantly improves the detection ability and reduces\ncomputational cost in learning the detector. Two\ndomain-adaptive Ô¨Åne-tuning techniques are further\nexplored to boost the detection performance.\nIn the future, we are interested in deploying our\nOOD method to real-world applications, such as\ndetecting unseen new classes for incremental few-\nshot learning (Zhang et al., 2020; Xia et al., 2021)\nor Ô¨Åltering OOD samples in data augmentations.\nAcknowledgments\nWe would like to thank the anonymous reviewers\nfor their valuable feedback and comments.\nReferences\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2015, pages 632‚Äì642.\nDallas Card, Michael Zhang, and Noah A Smith. 2019.\nDeep weighted averaging classiÔ¨Åers. In Proceedings\nof the Conference on Fairness, Accountability, and\nTransparency, FAT* 2019, pages 369‚Äì378.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A simple framework\nfor contrastive learning of visual representations. In\nProceedings of the 37th International Conference on\nMachine Learning, ICML 2020, pages 1597‚Äì1607.\nI. Dagan, Oren Glickman, and B. Magnini. 2005. The\npascal recognising textual entailment challenge. In\nMachine Learning Challenges, Evaluating Predic-\ntive Uncertainty, Visual Object ClassiÔ¨Åcation and\nRecognizing Textual Entailment, First PASCAL Ma-\nchine Learning Challenges Workshop, MLCW 2005.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2019, pages 4171‚Äì4186.\nDesmond Elliott, S. Frank, K. Sima‚Äôan, and Lucia Spe-\ncia. 2016. Multi30k: Multilingual english-german\nimage descriptions. In Proceedings of the 5th Work-\nshop on Vision and Language.\nSuchin Gururangan, Ana Marasovi ¬¥c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don‚Äôt stop pretraining:\nAdapt language models to domains and tasks.\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL\n2020, pages 8342‚Äì8360.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020. Momentum contrast for un-\nsupervised visual representation learning. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, pages 9729‚Äì9738.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline\nfor detecting misclassiÔ¨Åed and out-of-distribution\nexamples in neural networks. In 5th International\nConference on Learning Representations, ICLR\n2017.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, pages 2744‚Äì2751.\nDan Hendrycks, Mantas Mazeika, and Thomas Diet-\nterich. 2019. Deep anomaly detection with outlier\nexposure. In 7th International Conference on Learn-\ning Representations, ICLR 2019.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\n1061\nGanesh Jawahar, Beno√Æt Sagot, and Djam√© Seddah.\n2019. What does bert learn about the structure of\nlanguage? In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, pages 3651‚Äì3657.\nK. Lang. 1995. Newsweeder: Learning to Ô¨Ålter net-\nnews. In Machine Learning, Proceedings of the\nTwelfth International Conference on Machine Learn-\ning, pages 331‚Äì339.\nStefan Larson, Anish Mahendran, Joseph J. Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K. Kummerfeld, Kevin Leach, Michael A.\nLaurenzano, Lingjia Tang, and Jason Mars. 2019.\nAn evaluation dataset for intent classiÔ¨Åcation and\nout-of-scope prediction. In Proceedings of the 2019\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2019, pages 1311‚Äì1316.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018. A simple uniÔ¨Åed framework for detecting out-\nof-distribution samples and adversarial attacks. In\nAdvances in Neural Information Processing Systems,\nNeurIPS 2018, pages 7167‚Äì7177.\nShiyu Liang, Yixuan Li, and Rayadurgam Srikant.\n2018. Enhancing the reliability of out-of-\ndistribution image detection in neural networks. In\n6th International Conference on Learning Represen-\ntations, ICLR 2018.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLukas Ruff, Robert Vandermeulen, Nico Goernitz, Lu-\ncas Deecke, Shoaib Ahmed Siddiqui, Alexander\nBinder, Emmanuel M√ºller, and Marius Kloft. 2018.\nDeep one-class classiÔ¨Åcation. In Proceedings of the\n35th International Conference on Machine Learning,\nICML 2018, pages 4393‚Äì4402.\nBernhard Sch√∂lkopf, John C Platt, John Shawe-Taylor,\nAlex J Smola, and Robert C Williamson. 2001. Es-\ntimating the support of a high-dimensional distribu-\ntion. Neural computation, 13(7):1443‚Äì1471.\nR. Socher, Alex Perelygin, J. Wu, Jason Chuang,\nChristopher D. Manning, A. Ng, and Christopher\nPotts. 2013. Recursive deep models for semantic\ncompositionality over a sentiment treebank. In Pro-\nceedings of the 2013 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2013,\npages 1631‚Äì1642.\nDavid MJ Tax and Robert PW Duin. 2004. Sup-\nport vector data description. Machine learning ,\n54(1):45‚Äì66.\nAlexandre B Tsybakov et al. 1997. On nonparametric\nestimation of density level sets. The Annals of Statis-\ntics, 25(3):948‚Äì969.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nR√©gis Vert, Jean-Philippe Vert, and Bernhard\nSch√∂lkopf. 2006. Consistency and convergence\nrates of one-class svms and related algorithms.\nJournal of Machine Learning Research, 7(5).\nCongying Xia, Wenpeng Yin, Yihao Feng, and Philip\nYu. 2021. Incremental few-shot text classiÔ¨Åcation\nwith multi-round new classes: Formulation, dataset\nand system. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, pages 1351‚Äì1360.\nHong Xu, Keqing He, Yuanmeng Yan, Sihong Liu, Zi-\njun Liu, and Weiran Xu. 2020. A deep generative\ndistance-based classiÔ¨Åer for out-of-domain detection\nwith mahalanobis space. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 1452‚Äì1460.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754‚Äì5764.\nShuangfei Zhai, Yu Cheng, Weining Lu, and Zhongfei\nZhang. 2016. Deep structured energy based mod-\nels for anomaly detection. In Proceedings of The\n33rd International Conference on Machine Learn-\ning, pages 1100‚Äì1109.\nJian-Guo Zhang, Kazuma Hashimoto, Wenhao Liu,\nChien-Sheng Wu, Yao Wan, Philip S Yu, Richard\nSocher, and Caiming Xiong. 2020. Discriminative\nnearest neighbor few-shot intent detection by trans-\nferring natural language inference. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, pages\n5064‚Äì5082.\nY . Zhu, Ryan Kiros, R. Zemel, R. Salakhutdinov, R. Ur-\ntasun, A. Torralba, and S. Fidler. 2015. Aligning\nbooks and movies: Towards story-like visual ex-\nplanations by watching movies and reading books.\n2015 IEEE International Conference on Computer\nVision, ICCV 2015, pages 19‚Äì27.\nBo Zong, Qi Song, Martin Renqiang Min, Wei Cheng,\nCristian Lumezanu, Daeki Cho, and Haifeng Chen.\n2018. Deep autoencoding gaussian mixture model\nfor unsupervised anomaly detection. In 6th Interna-\ntional Conference on Learning Representations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6090239882469177
    },
    {
      "name": "Zh√†ng",
      "score": 0.6073068380355835
    },
    {
      "name": "Natural language processing",
      "score": 0.526719331741333
    },
    {
      "name": "Computational linguistics",
      "score": 0.4830368459224701
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4825281798839569
    },
    {
      "name": "Transformer",
      "score": 0.43052539229393005
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4236147403717041
    },
    {
      "name": "Engineering",
      "score": 0.1878228783607483
    },
    {
      "name": "History",
      "score": 0.123872309923172
    },
    {
      "name": "China",
      "score": 0.11352729797363281
    },
    {
      "name": "Mathematics",
      "score": 0.10801267623901367
    },
    {
      "name": "Electrical engineering",
      "score": 0.09426137804985046
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    }
  ],
  "cited_by": 25
}