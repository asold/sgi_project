{
  "title": "Token Prediction as Implicit Classification to Identify LLM-Generated Text",
  "url": "https://openalex.org/W4389523998",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5074021977",
      "name": "Yutian Chen",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5101816947",
      "name": "Hao Kang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5091957289",
      "name": "Vivian Zhai",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5091957290",
      "name": "Liangze Li",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5102775511",
      "name": "Rita Singh",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5113017615",
      "name": "Bhiksha Raj",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W4383815588",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W3114326827",
    "https://openalex.org/W4318351452",
    "https://openalex.org/W3174432697",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4317553041"
  ],
  "abstract": "This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13112‚Äì13120\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nToken Prediction as Implicit Classification to Identify LLM-Generated Text\nYutian Chen‚Ä†, Hao Kang‚Ä†, Yiyan Zhai, Liangze Li, Rita Singh, Bhiksha Raj\nCarnegie Mellon University\nPittsburgh, PA 15213\n{yutianch, haok, yiyanz, liangzel, rsingh, bhiksha}@andrew.cmu.edu\nAbstract\nThis paper introduces a novel approach for\nidentifying the possible large language mod-\nels (LLMs) involved in text generation. Instead\nof adding an additional classification layer to a\nbase LM, we reframe the classification task\nas a next-token prediction task and directly\nfine-tune the base LM to perform it. We uti-\nlize the Text-to-Text Transfer Transformer (T5)\nmodel as the backbone for our experiments. We\ncompared our approach to the more direct ap-\nproach of utilizing hidden states for classifica-\ntion. Evaluation shows the exceptional perfor-\nmance of our method in the text classification\ntask, highlighting its simplicity and efficiency.\nFurthermore, interpretability studies on the fea-\ntures extracted by our model reveal its ability\nto differentiate distinctive writing styles among\nvarious LLMs even in the absence of an explicit\nclassifier. We also collected a dataset named\nOpenLLMText, containing approximately 340k\ntext samples from human and LLMs, including\nGPT3.5, PaLM, LLaMA, and GPT2.\n1 Introduction\nIn recent years, generative LLMs have gained\nrecognition for their impressive ability to produce\ncoherent language across different domains. Con-\nsequently, detecting machine-generated text has\nbecome increasingly vital, especially when ensur-\ning the authenticity of information is critical, such\nas legal proceedings.\nTraditionally, techniques like logistic regression\nand support vector machines (SVM) have been\nused for detection tasks, as explained by Jawahar\net al. (2020). The analysis of textual features like\nperplexity is proven to be effective as well (Wu\net al., 2023). Recent advancements have introduced\nthe use of language model itself to detect generated\ntext, such as the AI Text Classifier released by\nOpenAI (2023) and Solaiman et al. (2019).\nHowever, the exponential growth in the num-\nber of parameters from hundreds of millions to\n‚Ä†Two authors contribute equally to this work.\n‚ãÜThe implementation of the classification model, training\nprocess, and dataset collection is publicly available on https:\n//github.com/MarkChenYutian/T5-Sentinel-public\nhundreds of billions has significantly improved\nthe text generation quality, presenting an unprece-\ndented challenge to the detection task. To over-\ncome this challenge, we propose using the inherent\nnext-token prediction capability of the base LM for\ndetection task, aiming not just to determine whether\nor not the text is generated but also to identify its\nsource.\n2 Related Work\n2.1 Generated Text Detection\nLearning-based approaches to machine-generated\ntext detection can be broadly classified into two\ncategories: unsupervised learning and supervised\nlearning. Unsupervised learning includes GLTR de-\nveloped by Gehrmann et al. (2019) that uses linguis-\ntic features like top-kwords to identify generated\ntext. Another unsupervised approach, DetectGPT\nby Mitchell et al. (2023), employs a perturbation-\nbased method by generating a modifications of the\ntext via a pre-trained language model and then com-\nparing log probabilities of original and perturbed\nsamples. Supervised Learning includes GROVER\n(Zellers et al., 2020) that extracts the final hidden\nstate and uses a linear layer for prediction in a dis-\ncriminative setting. Energy-based models (Bakhtin\net al., 2019) have also been investigated for dis-\ncriminating between text from different sources.\nSolaiman et al. (2019) fine-tuned RoBERTa model\non GPT-2 generated text, resulting in an accuracy\nof 91% on GPT2-1B in GPT2-Output dataset.\n2.2 Text-to-Text Transfer Transformer\nText-to-Text Transfer Transformer (T5) (Raffel\net al., 2020) has gained recognition due to its sim-\nplicity by converting all text-based language prob-\nlems into a text-to-text format. Raffel et al. and\nJiang et al. (2021) have shown that T5 out-performs\nBERT-based models on various natural language\nprocessing tasks.\nHowever, prior approaches have not emphasized\nthe use of T5 in the task of distinguishing the lan-\nguage model responsible for text generation. Fur-\nthermore, existing approaches have not directly\nleveraged the next-token prediction capability of\n13112\nthe model for this particular task. Our approach\nadvances the field by choosing T5 model as the\nbase LM and using its next-token prediction ca-\npability to improve the accuracy and efficiency of\ndistinguishing the origin of the text.\n3 Dataset\n3.1 Data Collection\nThe dataset we collected, named OpenLLMText,\nconsists of approximately 340,000 text samples\nfrom five sources: Human, GPT3.5 (Brown et al.,\n2020), PaLM (Chowdhery et al., 2022), LLaMA-\n7B (Touvron et al., 2023), and GPT2-1B (GPT2 ex-\ntra large) (Radford et al., 2019). The OpenLLMText\ndataset, along with the response collected from\nOpenAI & GPTZero, is publicly available on Zen-\nodo.1\nHuman text samples are obtained from the\nOpenWebText dataset collected by Gokaslan and\nCohen (2019). GPT2-1B text samples stem from\nGPT2-Output dataset released by OpenAI (2019).\nAs for GPT3.5 and PaLM, the text samples are col-\nlected with prompt ‚ÄúRephrase the following para-\ngraph by paragraph: [Human_Sample]‚Äù. But in-\nstructing LLaMA-7B to rephrase human text sam-\nples is ineffective, due to the lack of fine-tuning\nfor instruction following of LLaMA-7B. Hence,\nwe provided the first 75 tokens from the human\nsamples as context to LLaMA-7B and obtained the\ntext completion as the output. For further details,\nincluding the temperature and sampling method for\neach source, please refer to the table 3 in Appendix\nA.\nWe partitioned the OpenLLMText into train\n(76%), validation (12%) and test (12%) subsets.\nThe detailed breakdown is listed in Table 4 in Ap-\npendix A.\n3.2 Data Preprocessing\nWe noticed stylistic differences among different\nmodels. For instance, LLaMA generates \\\\n for\nnewline character instead of \\n as in other sources.\nTo address the inconsistency, we followed a similar\napproach as Guo et al. (2023) to remove direct indi-\ncator strings and transliterate indicator characters.\n3.3 Dataset Analysis\nTo avoid potential bias and shortcuts that can\nbe learned by model unexpectedly, we analyzed\n1OpenLLMText can be downloaded at: https://zenodo.\norg/records/8285326\nùëÉ Human = , ùëÉ GPT3.5 = , ùëÉ PaLM = , ‚ãØ \nùê∏1 ùê∏2\nSelf-Attention\nFeed-forward MLP\nùê∏ùëÅ ùê∏<ùë†>\n‚ãØ\nEncoder Block\n‚ãØ\n√ó 6\nMasked Multi-head Attention\nMasked Multi-head Attention\nDecoder Block \nFeed Forward\n‚ãØ\nNext-token\nProbability Distribution\n<extra_id0> <extra_id1>\n√ó 6\n<extra_id2>\nT5-small Backbone\nFigure 1: T5-Sentinel model architecture\nthe distribution of length, punctuation, tokens\nand word classes in OpenLLMText across different\nsources. Results indicate that no significant bias\nexists between different sources. For detailed anal-\nysis and visualization, please refer to the Appendix\nA.\n4 Method\nOur approach can be formulated as follows. Let Œ£\nrepresent the set of all tokens. The base LM can be\ninterpreted as a function LM : Œ£‚àó√óŒ£ ‚ÜíR. Given\na string s ‚ààŒ£‚àóand a token œÉ ‚ààŒ£, LM(s,œÉ) esti-\nmates the probability of the next token beingœÉ. Let\nY denote the set of labels in OpenLLMText, which\ncontains ‚ÄúHuman‚Äù, ‚ÄúGPT-3.5‚Äù, etc. We establish\na bijection f : Y ‚ÜíY, where Y ‚äÇŒ£ acts as a\nproxy for the labels. By doing so, we reformulate\nthe multi-class classification task Œ£‚àó‚ÜíY into a\nnext-token prediction task Œ£‚àó ‚ÜíY. Hence, the\nmulti-class classification task can be solved directly\nusing LM:\nÀÜy= f‚àí1\n(\narg max\ny‚ààY\nLM(s,y)\n)\n(1)\n4.1 T5-Sentinel\nT5-Sentinel is the implementation of our approach\nusing T5 model. Unlike previous learning-based\napproaches where final hidden states are extracted\nand passed through a separate classifier (Solaiman\net al., 2019; Guo et al., 2023), T5-Sentinel directly\nrelies on the capability of the T5 model to predict\nthe conditional probability of next token. In other\nwords, we train the weight and embedding of the\nT5 model and encode the classification problem\ninto a sequence-to-sequence completion task as\nshown in Figure 1.\nWe use reserved tokens that do not exist in the\ntext dataset as Y. During fine-tuning, we use the\n13113\n√ó 6\nMLP\n‚Ä¶\n‚Ä¶\n512\n512\nùëÉ Human =  ,  ùëÉ GPT3.5 = , ‚ãØ\nT5-small Backbone\nEncoder Block Decoder Block√ó 6\nùê∏1 ùê∏2 ùê∏<ùë†>\n‚ãØ ùê∏<ùëÉùê¥ùê∑>\nFigure 2: T5-Hidden model architecture\nHuman GPT3.5 PaLM LLaMA GPT2\nPredicted\nHuman\nGPT3.5\nPaLM\nLLaMA\nGPT2\nActual\n6148 150 717 17 335\n29 6989 357 2 8\n152 158 7027 3 60\n113 42 59 6211 162\n65 9 46 0 7265\nFigure 3: Confusion matrix of T5-Sentinel\nAdamW optimizer (Loshchilov and Hutter, 2017)\nwith a mini-batch size of 128. The learning rate is\n1 √ó10‚àí4 with weight decay of 5 √ó10‚àí5, and we\ntrain for 15 epochs.\n4.2 T5-Hidden\nTo evaluate the effectiveness of not using an ad-\nditional classifier while accomplishing the same\nclassification task, we also fine-tuned the T5 model\nwith a classifier attached, denoted T5-Hidden.\nAs illustrated with Figure 2, the classifier in T5-\nHidden uses the final hidden state from the decoder\nblock of the T5 model and computes the probabil-\nity for each label after taking a softmax over its\noutput layer. T5-Hidden is trained under identical\nconfiguration as T5-Sentinel.\n5 Evaluation\nT5-Sentinel and T5-Hidden are evaluated on the\ntest subset of OpenLLMText dataset with receiver\noperating characteristic (ROC) curve, area under\nROC curve (AUC) and F1 score.\n5.1 Multi-Class Classification\nWe breakdown the evaluation on multi-class classi-\nfication, i.e., identify the specific LLM responsible\nfor text generation, into one-vs-rest classification\nfor each label.\nAs presented in Table 5, T5-Sentinel achieves\na superior weighted F1 score of 0.931 compared\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nHuman\nGPT3.5\nPaLM\nLLaMA\nGPT2-XL\nFigure 4: ROC curves for T5-Sentinel for each one-vs-\nrest classification task\n10 3\n 10 2\n 10 1\n 100\nFalse Positive Rate\n10 3\n10 2\n10 1\n100\nFalse Negative Rate\nHuman\nGPT3.5\nPaLM\nLLaMA\nGPT2-XL\nFigure 5: DET curves for T5-Sentinel for each one-vs-\nrest classification task\nto 0.833 of T5-Hidden, both under the probability\nthreshold of 0.5. The confusion matrix for T5-\nSentinel is presented in Figure 3. To illustrate the\nperformance under different probability threshold,\nwe plot the ROC curves and Detection Error Trade-\noff (DET) curves on each one-vs-rest task in figure\n4 and 5 respectively.\n5.2 Human-LLMs Binary Classification\nFor generated text detection task, we compare\nT5-Sentinel against T5-Hidden and two widely-\nadopted baseline classifiers, the AI text detector by\nOpenAI and ZeroGPT.\nFigure 6 displays the ROC curves obtained from\nour experiments and the detailed performance met-\nrics such as AUC, accuracy, and F1 score are sum-\nmarized in Table 1. Additionally, we compare\nthe performance of each classifier on the gener-\nation text detection subtask for each LLM source\nin OpenLLMText, as shown in Table 2. Notably, T5-\nSentinel outperforms the baseline across all sub-\ntasks in terms of AUC, accuracy, and F1 score.\n6 Interpretability Study\nOur interpretability studies, including a dataset\nablation study and integrated gradient analysis,\n13114\nAUC Accuracy F1 Recall Precision\nOpenAI 0.795 0.434 0.415 0.985 0.263\nZeroGPT 0.533 0.336 0.134 0.839 0.148\nT5-Hidden 0.924 0.894 0.766 0.849 0.698\nT5-Sentinel 0.965 0.956 0.886 0.832 0.946\nTable 1: Evaluation result for T5-Sentinel and T5-Hidden on Human-LLM binary classification problem comparing\nto that of baselines OpenAI (2023); ZeroGPT (2023) on test susbet of OpenLLMText dataset.\nTask Human v. GPT3.5 Human v. PaLM Human v. LLaMA Human v. GPT2\nMetric AUC Acc F1 AUC Acc F1 AUC Acc F1 AUC Acc F1\nOpenAI .761 .569 .694 .829 .659 .743 .676 .573 .709 .901 .768 .809\nZeroGPT .576 .493 .555 .735 .662 .649 .367 .375 .519 .435 .382 .504\nSolaiman et al. .501 .499 .005 .508 .501 .013 .524 .533 .027 .870 .748 .666\nT5-Hidden\nstd\n.971\n.011\n.922\n.022\n.916\n.026\n.964\n.020\n.914\n.035\n.908\n.033\n.806\n.062\n.746\n.084\n.779\n.077\n.965\n.019\n.910\n.024\n.903\n.017\nT5-Sentinel .970 .914 .906 .962 .906 .898 .964 .903 .901 .965 .912 .904\nTable 2: Evaluation results for T5-Sentinel, T5-Hidden and baselines on each specific human-to-LLM binary\nclassification task. For T5-Hidden model, we also tested with 5 random initializations and report the standard\ndeviation of metrics under each task in italic.\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nOpenAI\nZeroGPT\nT5-Sentinel\nT5-Hidden\nFigure 6: ROC curves for OpenAI classifier, ZeroGPT,\nT5-Hidden and the proposed T5-Sentinel on test subset\nof OpenLLMText\nshow that T5-Sentinel does not rely on unexpected\nshortcuts. Additionally, we employ t-distributed\nStochastic Neighbor Embedding (t-SNE) projec-\ntion (van der Maaten and Hinton, 2008) on the hid-\nden states of the last decoder block of T5-Sentinel.\nThe resulted t-SNE plot, shown in Figure 7, demon-\nstrates the model‚Äôs ability to distinguish textual\ncontents from different sources, corroborating the\nevaluation results discussed earlier. For compari-\nson, we also plotted the t-SNE plot of T5-Hidden\non the test subset of OpenLLMText in figure 8, re-\nsults show that the T5-Sentinel cannot distinguish\nLLaMA with other source of sample correctly. This\naligns with the evaluation reported in table 2.\n80\n 60\n 40\n 20\n 0 20 40 60\n80\n60\n40\n20\n0\n20\n40\n60\nHuman\nGPT3.5\nPaLM\nLLaMA\nGPT2-XL\nFigure 7: t-SNE plot for T5-Sentinel on test subset of\nOpenLLMText dataset under perplexity of 100\n80\n 60\n 40\n 20\n 0 20 40 60\n60\n40\n20\n0\n20\n40\n60\nHuman\nGPT3.5\nPaLM\nLLaMA\nGPT2-XL\nFigure 8: t-SNE plot for T5-Hidden on test subset of\nOpenLLMText dataset under perplexity of 100\n13115\n6.1 Dataset Ablation Study\nAblation study is conducted on the OpenLLMText\ndataset to further investigate which feature is uti-\nlized by the T5-Sentinel to make classification.\nWe design 4 different cleaning configurations on\nOpenLLMText: i) compress consecutive newline\ncharacters to one; ii) transliterate Unicode charac-\nters to ASCII characters2; iii) remove all punctu-\nation; iv) cast all characters to lower case. Eval-\nuation results for each one-vs-rest binary classifi-\ncation task in Table 6 shows that T5-Sentinel is\nquite robust to perturbations in the input text. For\nablation configuration i), ii) and iv), the AUC and\nF1 score are almost identical. However, the perfor-\nmance drop significantly under condition iii) (with\n‚àÜAUC ‚âà‚àí0.3).\nTo prove that T5-Sentinel is not overfitting to\nspecific punctuation, we independently remove\neach punctuation in ASCII from input text and\nevaluated the performance of model on each one-\nvs-rest classification task. Results show that only\nthe removal of period and comma cause significant\nperformance degradation (shown in Table 6). This\ncan be due to the fact that T5-Sentinel is utilizing\nsyntax structure of input sample to distinguish text\nfrom human, GPT3.5 and PaLM instead of overfit-\nting on these two punctuation. In section 6.2, we\nconfirm this hypothesis with an integrated gradient\nanalysis.\n6.2 Integrated Gradient Analysis\nThe integrated gradient method, proposed by Sun-\ndararajan et al. (2017), is a robust tool for attribut-\ning the prediction of a neural network to the input\nfeatures. Here, we apply the integrated gradient\nmethod on the word embedding of input text sam-\nple and calculated the integrated gradient of each\ntoken using the following formula:\nIG(x) = x‚àíx0\nm\nm‚àë\ni=0\n‚àÇL(T5(x0 + i\nm (x‚àíx0)),y)\n‚àÇx\n(2)\nwhere x0 denotes the word embedding of the input\ntext same length as xbut filled with <pad> token,\nwhich is considered as a baseline input.\nThe visualization tool we developed uses equa-\ntion 2 with m= 100 to calculate the integrated gra-\ndient of each token and show the attribution of each\ntoken in the prediction made by T5-Sentinel model.\n2Implemented with Python package Unidecode, https:\n//pypi.org/project/Unidecode/\nSome samples for visualization can be found in\nappendix D.\nWe notice the existence of substantial gradients\non non-punctuation tokens, especially on syntax\nstructures like clauses (Sample 2, Appendix D) and\nsemantic structures like repetitive verbs (Sample\n4, Appendix D), indicating that the gradients are\nnot exclusively overfitting on punctuation tokens.\nRather, the drop in performance of the model with-\nout punctuation appears to stem from the fact that\nthe removal of punctuation disrupts the overall se-\nmantic structure within the text that the model has\ncorrectly learned.\n7 Conclusion\nIn conclusion, this paper demonstrates the effec-\ntiveness of involving next-token prediction in iden-\ntifying possible LLMs that generate the text. We\nmake contributions by collecting and releasing the\nOpenLLMText dataset, transferring the classifica-\ntion task into the next-token prediction task, con-\nducting experiments with T5 model to create the\nT5-Sentinel, and providing insight on the differ-\nences of writing styles between LLMs through\ninterpretability studies. In addition, we provide\ncompelling evidence that our approach surpasses\nT5-Hidden and other existing detectors. As it elim-\ninates the requirement for an explicit classifier, our\napproach stands out for its efficiency, simplicity,\nand practicality.\nLimitations\nThe OpenLLMText dataset we collected is based\non the OpenWebText dataset. The original\nOpenWebText dataset collects human written En-\nglish content from Reddit, an online discussion\nwebsite mainly used in North America. Hence, the\nentries from human in dataset may bias towards\nnative English speakers‚Äô wording and tone. This\nmight lead to a degraded performance when the\ndetector trained on OpenLLMText dataset is given\nhuman-written text from non-native English speak-\ners. This tendency to misclassify non-native En-\nglish writing as machine-generated is also men-\ntioned by Liang et al. (2023).\nReferences\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng,\nMarc‚ÄôAurelio Ranzato, and Arthur Szlam. 2019.\nReal or fake? learning to discriminate machine from\nhuman generated text.\n13116\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nSebastian Gehrmann, Hendrik Strobelt, and Alexan-\nder M. Rush. 2019. Gltr: Statistical detection and\nvisualization of generated text.\nAaron Gokaslan and Vanya Cohen. 2019. Open-\nwebtext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How close is chatgpt to human experts?\ncomparison corpus, evaluation, and detection.\nGanesh Jawahar, Muhammad Abdul-Mageed, and Laks\nV . S. Lakshmanan. 2020. Automatic detection of\nmachine generated text: A critical survey. CoRR,\nabs/2011.01314.\nKelvin Jiang, Ronak Pradeep, and Jimmy Lin. 2021. Ex-\nploring listwise evidence reasoning with t5 for fact\nverification. https://aclanthology.org/2021.\nacl-short.51.\nWeixin Liang, Mert Yuksekgonul, Yining Mao, Eric\nWu, and James Zou. 2023. Gpt detectors are biased\nagainst non-native english writers.\nIlya Loshchilov and Frank Hutter. 2017. Fixing\nweight decay regularization in adam. CoRR,\nabs/1711.05101.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky,\nChristopher D. Manning, and Chelsea Finn. 2023.\nDetectgpt: Zero-shot machine-generated text detec-\ntion using probability curvature.\nOpenAI. 2019. Gpt2-output dataset. https://github.\ncom/openai/gpt-2-output-dataset .\nOpenAI. 2023. https://beta.openai.com/\nai-text-classifier.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Radford,\nGretchen Krueger, Jong Wook Kim, Sarah Kreps,\nMiles McCain, Alex Newhouse, Jason Blazakis, Kris\nMcGuffie, and Jasmine Wang. 2019. Release strate-\ngies and the social impacts of language models.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of Machine\nLearning Research, 9:2579‚Äì2605.\nKangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng,\nand Tat-Seng Chua. 2023. Llmdet: A large language\nmodels detection tool.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2020. Defending against neural fake\nnews.\nZeroGPT. 2023. AI Detector. https://www.zerogpt.\ncom.\nA Dataset\nA.1 Length Distribution\nThe length distribution of text sample from each\nsource is presented in Figure 9. Since we trun-\ncated the text to first 512 tokens during training\nand evaluation, the actual length distribution for\neach source received by the classifier is shown in\nFigure 10, which is approximately the same across\nvarious sources.\n13117\n0.00%\n0.25%\n0.50%\nHuman\nGPT3.5\nPaLM\nLLaMA\nGPT2\n0.00%\n0.25%\n0.50%\n0.00%\n0.25%\n0.50%\n0.00%\n0.25%\n0.50%\n0 500 1000 1500 2000 2500\n0.00%\n0.25%\n0.50% Frequency\nSample Length in OpenLLMT ext dataset (# T oken)\nFigure 9: Distribution of sample length measured by the\nnumber of tokens in the OpenLLMText dataset.\nA.2 Punctuation Distribution\nFigure 11 shows the distribution of top-40 ASCII\npunctuation in OpenLLMText dataset. For most of\nthe punctuation, all LLMs tend to generate them\nwith similar frequency. However, PaLM does tend\nto generate ‚Äú*‚Äù more frequently than other sources.\nHowever, further experiments on dataset clean-\ning (indicated in 6 in Appendix C) show that T5-\nSentinel is not relying on this feature to identify\nPaLM generated text.\nA.3 Token Distribution\nThe distribution of most commonly seen tokens\nfrom each source is presented in Figure 12. It is\nworth noting that while the GPT2 source lacks sin-\ngle quotation marks and double quotation marks,\nthe overall token distributions from all sources ex-\nhibit a consistent pattern.\nA.4 Word-Class Distribution\nFigure 13 displays the word-class distribution like\nnoun, adjective and others for each source in\nOpenLLMText dataset. The distribution is almost\nidentical across all sources.\nB Evaluation\nThe detailed evaluation results for human-to-LLM\nbinary classification tasks and one-to-rest binary\nclassification tasks are separately listed in Table 2\nand 5.\nC Dataset Ablation Study\nTable 6 has shown the performance of T5-Sentinel\nunder different dataset cleaning methods.\n0.0%\n10.0% Human\nGPT3.5\nPaLM\nLLaMA\nGPT2\n0.0%\n10.0%\n0.0%\n10.0%\n0.0%\n10.0%\n0 100 200 300 400 500\n0.0%\n10.0% Frequency\nSample Length received by T5-Sentinel (# T oken)\nFigure 10: Distribution of sample length measured by\nthe number of tokens actually received by T5-Sentinel\nduring training, validation and test.\n0.00%\n1.00% Human\nGPT3.5\nPaLM\nLLaMA\nGPT2\n0.00%\n1.00%\n0.00%\n1.00%\n0.00%\n1.00%\n. , - ' \" : ) ( / ? $ ! ] [ ; %_@* &=#>+ | <{ } \\ ~^ `\n0.00%\n1.00% Frequency\nMost common punctuation in OpenLLMT ext dataset\nFigure 11: Distribution of ASCII Punctuations in\nOpenLLMText\n0.00%2.50%\nhas\nthis\n\"\nyou\ning\nhe\nfrom\nby\n:\nat\nhave\nare\nbe\nI\nas\nwas\nThe\nwith\nd\nit\ne\nt\non\n'\nfor\nis\n-\nthat\nin\nand\nof\nto\na\ns\n.\n,\nthe\nHuman\n0.00%2.50%\nGPT3.5\n0.00%2.50%\nPaLM\n0.00%2.50%\nLLaMA\n0.00%2.50%\nGPT2\nMost common tokens in OpenLLMT ext dataset\nFrequency\nFigure 12: Distribution of tokens in OpenLLMText\n13118\nSource Dataset/Tool Generation Method Temp Top- p\nHuman OpenWebText dataset - - -\nGPT3.5 OpenAI‚Äôs gpt-3.5-turbo API Rephrase human samples 1 1\nPaLM text-bison-001 API Rephrase human samples 0.4 0 .95\nLLaMA LLaMA-7B model Text completion 0.95 0 .95\nGPT2 GPT2-Output dataset Random hidden state 1 1\nTable 3: Sources and details of text samples in OpenLLMText\nSubset Human GPT3.5 PaLM LLaMA-7B GPT2-1B Total\ntrain 51205 51360 46525 50099 65079 264268\nvalid 10412 10468 3485 9305 10468 44138\ntest 7367 7385 7400 6587 7385 36124\nTotal 68984 69213 57410 65991 82932 344530\nTable 4: Number of entries from each source in each subset of OpenLLMText.\nTask AUC Accuracy F1\nSentinel Hidden Sentinel Hidden Sentinel Hidden\nHuman v. Rest 0.965 0.965 0.956 0.894 0.886 0.766\nGPT3.5 v. Rest 0.989 0.989 0.979 0.980 0.949 0.950\nPaLM v. Rest 0.984 0.984 0.957 0.947 0.901 0.881\nLLaMA-7B v. Rest 0.989 0.989 0.989 0.899 0.969 0.616\nGPT2-1B v. Rest 0.995 0.995 0.981 0.969 0.955 0.929\nAverage (weighted) 0.984 0.984 0.972 0.939 0.931 0.833\nTable 5: Evaluation result for each one-vs-rest classification task for T5-Sentinel and T5-Hidden on OpenLLMText\ntest subset. Accuracy, and F1-score are calculated under probability threshold of 0.5.\nOne-vs-rest Human GPT3.5 PaLM LLaMA GPT2\nMetric AUC F1 AUC F1 AUC F1 AUC F1 AUC F1\nOriginal .965 .886 .989 .949 .984 .901 .989 .969 .995 .955\nNewline .965 .886 .989 .949 .984 .901 .989 .969 .995 .955\n‚ÜìUnicode .947 .832 .987 .941 .983 .895 .981 .946 .988 .907\n‚ÜìPunc .775 .493 .590 .096 .679 .120 .974 .880 .942 .729\n‚Üì. .918 .661 .877 .645 .886 .619 .993 .954 .986 .882\n‚Üì, .946 .784 .954 .861 .931 .794 .993 .974 .991 .922\n? .967 .890 .989 .949 .984 .904 .989 .968 .995 .955\n! .966 .888 .989 .949 .984 .903 .988 .968 .995 .954\n: .966 .889 .989 .949 .984 .905 .988 .965 .995 .952\n‚Äô .969 .884 .988 .948 .983 .903 .989 .968 .994 .946\n\" .966 .881 .988 .947 .983 .901 .985 .961 .995 .951\n* .964 .881 .988 .946 .978 .891 .989 .968 .995 .953\nLower .966 .863 .984 .928 .973 .889 .987 .962 .989 .914\nTable 6: Evaluation results of T5-Sentinel on OpenLLMText dataset under different ablation configurations. ‚ÄúNew-\nline‚Äù, ‚ÄúUnicode‚Äù, ‚ÄúLower‚Äù and ‚ÄúPunc‚Äù stands for the cleaning configuration i) to iv) respectively. Each nested row\nunder ‚ÄúPunc‚Äù represents removing that specific punctuation. ‚Üìmeans the accuracy drop by a considerable amount.\nBold and italic represents the worst and second-worst entries in that column.\n13119\n0%\n50% Human\nGPT3.5\nPaLM\nLLaMA\nGPT2\n0%\n50%\n0%\n50%\n0%\n50%\nNOUN\nADJ VERBNUM ADV ADP\nX\nCONJPRT\n.\nDETPRON\n0%\n50% Frequency\nWord classes distribution in OpenLLMT ext dataset\nFigure 13: Distribution of word classes inOpenLLMText\nD Integrated Graident Samples\nSome samples of integrated gradient results are\npresented below. Brighter background indicates a\nhigher integrated gradient value on the token and\nmeaning that specific token contributes more on\nfinal prediction result. Sample 1 - 5 are randomly\nchosen from the test set of OpenLLMText dataset.\nSample 1. Label: Human, Predicted: Human\nOut going US President Barack Obama said\nthat he did not expect President - e lect Donald\nTrump to follow his administration ‚Äô s blueprint\ns in dealing with Russia , yet hoped that Trump\nwould \" stand up \" to Moscow . \" My hope is\nthat the president - e lect coming in takes a\nsimilarly constructive approach , . . .However ,\nhe repeated allegations that Russia had engaged\nin cyber attack s against the US . Although US\nintelligence officials blame d Russia for cyber\nattack s on the Democratic National Committee\n, they have not provided any substantial proof\nto the\n(. . . Truncated)\nSample 2. Label: GPT3.5, Predicted: GPT3.5\nBarack Obama has stated that he hopes Presi-\ndent - e lect Donald Trump will confront Russia\n, despite not expecting him to follow the cur-\nrent administration ‚Äô s policies . . . .W hilst\nhe wished Russia well and acknowledged it as\nan important partner to the world , Obama ex-\npressed hope for Trump ‚Äô s success \" not just by\nits own people but by people around the world\n\". Obama commented that not everything that\nhad worked for Trump\n(. . . Truncated)\nSample 3. Label: PaLM, Predicted: PaLM\nHTC has had a tough year . Revenue is down\n, it lost a patent suit to Apple , and it d re\nw criticism for pulling the plug on Je lly Bean\nupdates for some of its phones . The company\nneeds a win , and it ‚Äô s hoping that the D roid\nDNA will be it . The DNA is a high - end phone\nwith a 5- inch 10 80 p display , the same quad\n- core Snapdragon chip as the Opti mus G and\nNex us 4, and 2 GB of RAM . It ‚Äô s a powerful\nphone with a beautiful screen , but there are\nsome trade off s . The first trade off is battery\nlife . The DNA ‚Äô s battery is smaller than the\nbatteries in the Opti mus G and Galaxy S III ,\nand it doesn ‚Äô t last as long .\n(. . . Truncated)\nSample 4. Label: LLaMA, Predicted: LLaMA\n. . .Barack Obama s aid he did not expect P\nresident - e lect Donald Trump to follow his\nadministration ‚Äô s s blueprint s in dealing with\nRussia , yet hoped that Tru mp would \" stand up\n\" to Moscow . speaking during a joint press ap-\npearance in White House after meeting Trump\n. The president also said that while Americans\nare concerned about Russian interference in to\nlast year ‚Äô s election campaign and what it might\nmean for the f u ture of democracy , there was\nno evidence that votes were rig ged by outside\nactors at any point . said Obama during a joint\npress appearance in White House after meeting\nTrump . Out going US President Barack O b\na mas aid that while Americans are concerned\nabout Russian interference in to last year\n(... Truncated)\nSample 5. Label: GPT2, Predicted: GPT2\nGene ric and other online electric curb side me-\nter sizes There have been 11 50 comparison s\nbetween generic sets of in line -3 - gau ge , sin-\ngle - phase and Can ten n a - type electric curb\nside meters in Ontario over the past 20 years\n(19 75 - 28 ), total ling 2 2.3 M km . Here are\nsamples of current (10 - year average ) home\nelectric curb side meters from selected suppli-\ners . All currently available meters have a 1 \"\nrestriction and are marked with the size in deci\nmal format and a code of 1 or PF\n(. . . Truncated)\n13120",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.8352668881416321
    },
    {
      "name": "Computer science",
      "score": 0.7540677785873413
    },
    {
      "name": "Classifier (UML)",
      "score": 0.7005029916763306
    },
    {
      "name": "Artificial intelligence",
      "score": 0.597899317741394
    },
    {
      "name": "Security token",
      "score": 0.5968372821807861
    },
    {
      "name": "Transformer",
      "score": 0.5271560549736023
    },
    {
      "name": "Task (project management)",
      "score": 0.441715806722641
    },
    {
      "name": "Natural language processing",
      "score": 0.4254254996776581
    },
    {
      "name": "Machine learning",
      "score": 0.39972057938575745
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 15
}