{
    "title": "A Differentiable Language Model Adversarial Attack on Text Classifiers",
    "url": "https://openalex.org/W3184599456",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2979338862",
            "name": "Ivan Fursov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2318676791",
            "name": "Alexey Zaytsev",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3183222683",
            "name": "Pavel Burnyshev",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2312684179",
            "name": "Ekaterina Dmitrieva",
            "affiliations": [
                "National Research University Higher School of Economics"
            ]
        },
        {
            "id": "https://openalex.org/A2805756856",
            "name": "Nikita Klyuchnikov",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2296311086",
            "name": "Andrey Kravchenko",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2951265132",
            "name": "Ekaterina Artemova",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2978444901",
            "name": "Evgenia Komleva",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2187681102",
            "name": "Evgeny Burnaev",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963178695",
        "https://openalex.org/W2962700793",
        "https://openalex.org/W2963809642",
        "https://openalex.org/W3015001695",
        "https://openalex.org/W6772640148",
        "https://openalex.org/W3105604018",
        "https://openalex.org/W2908442265",
        "https://openalex.org/W6729756640",
        "https://openalex.org/W6741419198",
        "https://openalex.org/W6737220129",
        "https://openalex.org/W2799194071",
        "https://openalex.org/W2963083752",
        "https://openalex.org/W6775067480",
        "https://openalex.org/W6774596189",
        "https://openalex.org/W6763636181",
        "https://openalex.org/W2962982907",
        "https://openalex.org/W6637162671",
        "https://openalex.org/W6640425456",
        "https://openalex.org/W2966149470",
        "https://openalex.org/W2963834268",
        "https://openalex.org/W2962782614",
        "https://openalex.org/W3023553115",
        "https://openalex.org/W3116387889",
        "https://openalex.org/W3101449015",
        "https://openalex.org/W3104423855",
        "https://openalex.org/W4288953700",
        "https://openalex.org/W2950733407",
        "https://openalex.org/W6765039553",
        "https://openalex.org/W2926587947",
        "https://openalex.org/W2963823140",
        "https://openalex.org/W6752083044",
        "https://openalex.org/W2996851481",
        "https://openalex.org/W2963100531",
        "https://openalex.org/W2890719433",
        "https://openalex.org/W3037045905",
        "https://openalex.org/W3035441470",
        "https://openalex.org/W2947415936",
        "https://openalex.org/W2998277219",
        "https://openalex.org/W3034397670",
        "https://openalex.org/W6782926819",
        "https://openalex.org/W6734716764",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6753579488",
        "https://openalex.org/W6638523607",
        "https://openalex.org/W6729448088",
        "https://openalex.org/W2949676527",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W3027211216",
        "https://openalex.org/W2905526464",
        "https://openalex.org/W2962818281",
        "https://openalex.org/W2949128310",
        "https://openalex.org/W6685053522",
        "https://openalex.org/W6773410612",
        "https://openalex.org/W6691459498",
        "https://openalex.org/W2163455955",
        "https://openalex.org/W6768492035",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W6755007375",
        "https://openalex.org/W2998604177",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2803831897",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2964153729",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3010186808",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2963600562",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W3087231533",
        "https://openalex.org/W3013520104",
        "https://openalex.org/W2963207607",
        "https://openalex.org/W3099126561",
        "https://openalex.org/W2915238810",
        "https://openalex.org/W2890969459",
        "https://openalex.org/W2950606982",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2552767274",
        "https://openalex.org/W1945616565",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2885318751",
        "https://openalex.org/W2735135478",
        "https://openalex.org/W4288594304",
        "https://openalex.org/W3100202075",
        "https://openalex.org/W3012613466",
        "https://openalex.org/W3004786215",
        "https://openalex.org/W2609368435",
        "https://openalex.org/W2932893307",
        "https://openalex.org/W2963389226",
        "https://openalex.org/W2963859254",
        "https://openalex.org/W3013371788"
    ],
    "abstract": "Transformer models play a crucial role in state of the art solutions to problems arising in the field of natural language processing (NLP). They have billions of parameters and are typically considered as black boxes. Robustness of huge Transformer-based models for NLP is an important question due to their wide adoption. One way to understand and improve robustness of these models is an exploration of an adversarial attack scenario: check if a small perturbation of an input invisible to a human eye can fool a model. Due to the discrete nature of textual data, gradient-based adversarial methods, widely used in computer vision, are not applicable per se. The standard strategy to overcome this issue is to develop token-level transformations, which do not take the whole sentence into account. The semantic meaning and grammatical correctness of the sentence are often lost in such approaches In this paper, we propose a new black-box sentence-level attack. Our method fine-tunes a pre-trained language model to generate adversarial examples. A proposed differentiable loss function depends on a substitute classifier score and an approximate edit distance computed via a deep learning model. We show that the proposed attack outperforms competitors on a diverse set of NLP problems for both computed metrics and human evaluation. Moreover, due to the usage of the fine-tuned language model, the generated adversarial examples are hard to detect, thus current models are not robust. Hence, it is difficult to defend from the proposed attack, which is not the case for others. Our attack demonstrates the highest decrease of classification accuracy on all datasets(on AG news: 0.95 without attack, 0.89 under SamplingFool attack, 0.82 under DILMA attack).",
    "full_text": "Received November 25, 2021, accepted January 9, 2022, date of publication February 11, 2022, date of current version February 17, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3148413\nA Differentiable Language Model Adversarial\nAttack on Text Classifiers\nIVAN FURSOV1, ALEXEY ZAYTSEV1, PAVEL BURNYSHEV\n 1,2, EKATERINA DMITRIEVA3,\nNIKITA KLYUCHNIKOV1, ANDREY KRAVCHENKO4, EKATERINA ARTEMOVA\n 2,3,\nEVGENIA KOMLEVA\n 1, AND EVGENY BURNAEV\n1,5\n1Skolkovo Institute of Science and Technology, 121205 Moscow, Russia\n2Huawei Noah’s Ark Laboratory, 620073 Moscow, Russia\n3HSE University, 101000 Moscow, Russia\n4University of Oxford, Oxford OX1 2JD, U.K.\n5Artiﬁcial Intelligence Research Institute (AIRI), 103001 Moscow, Russia\nCorresponding author: Alexey Zaytsev (a.zaytsev@skoltech.ru)\nThis work was supported by the Analytical Center through the RF Government (Subsidy Agreement 000000D730321P5Q0002) under\nGrant 70-2021-00145.\nABSTRACT Transformer models play a crucial role in state of the art solutions to problems arising in the\nﬁeld of natural language processing (NLP). They have billions of parameters and are typically considered\nas black boxes. Robustness of huge Transformer-based models for NLP is an important question due to\ntheir wide adoption. One way to understand and improve robustness of these models is an exploration of an\nadversarial attack scenario: check if a small perturbation of an input invisible to a human eye can fool a model.\nDue to the discrete nature of textual data, gradient-based adversarial methods, widely used in computer\nvision, are not applicable per se. The standard strategy to overcome this issue is to develop token-level\ntransformations, which do not take the whole sentence into account. The semantic meaning and grammatical\ncorrectness of the sentence are often lost in such approaches In this paper, we propose a new black-box\nsentence-level attack. Our method ﬁne-tunes a pre-trained language model to generate adversarial examples.\nA proposed differentiable loss function depends on a substitute classiﬁer score and an approximate edit\ndistance computed via a deep learning model. We show that the proposed attack outperforms competitors\non a diverse set of NLP problems for both computed metrics and human evaluation. Moreover, due to the\nusage of the ﬁne-tuned language model, the generated adversarial examples are hard to detect, thus current\nmodels are not robust. Hence, it is difﬁcult to defend from the proposed attack, which is not the case for\nothers. Our attack demonstrates the highest decrease of classiﬁcation accuracy on all datasets(on AG news:\n0.95 without attack, 0.89 under SamplingFool attack, 0.82 under DILMA attack).\nINDEX TERMS Deep learning, natural language processing, adversarial attack, generative model.\nI. INTRODUCTION\nFor the last decade deep learning has been an extremely active\nand growing area of research. It provides solutions for various\nproblems in computer vision and natural language process-\ning. Whilst useful, deep learning models are hard to explain,\nas they have billions of parameters and hence are black boxes\nwith no robustness guarantees. Adversarial attacks [1] in all\napplication areas including computer vision [2], [3], natural\nlanguage processing [4]–[6], and graphs [7] seek to reveal\nnon-robustness of deep learning models.\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Alicia Fornés\n.\nAn adversarial attack on a text classiﬁcation model per-\nturbs the input sentence in such a way that the deep learning\nmodel is fooled, while the perturbations adhere to certain con-\nstraints, utilising semantic similarity, morphology or gram-\nmar patterns. The deep learning model then misclassiﬁes the\nperturbed sentence, whilst for a human it is evident that the\nsentence’s class remains the same [8].\nUnlike for images and their pixels, for textual data it is\nnot possible to estimate the derivatives of class probabilities\nwith respect to input tokens due to the discrete nature of the\nlanguage and its vocabulary. Although a sentence or a word\nrepresentation can lie in a continuous space, the token itself\ncan not be altered slightly to get to the neighbouring point.\nThis turns partial derivatives useless.\n17966 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 10, 2022\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\nFIGURE 1. The workflow of an adversarial attack. With a generative\nmodel we create a sample of possibly adversarial sequences. Then,\nwe select the best one via a substitute model. This example is adversarial\nwith high probability: a genuine model gives a wrong answer for it, whilst\nit is also similar to the initial sequence.\nMany adversarial attacks that accept the initial space of\ntokens as input attempt to modify these sequences using\noperations like addition, replacement, or switching of\ntokens [9]–[11]. Searching for the best modiﬁcation can be\nstated as a discrete optimisation problem, which often appears\nto be computationally hard and is solved by random or\ngreedy search heuristics in practice [11]. Otherwise gradient\noptimisation techniques can be leveraged in the embedding\nspace [12]–[14]. This approach has a bottleneck: all informa-\ntion contained in a sentence has to be squeezed into a single\nsentence embedding.\nTo alleviate these problems we propose a new differen-\ntiable adversarial attack model, which beneﬁts from lever-\naging pre-trained language models, DILMA (DIfferentiable\nLanguage Model Attack). A general outline of our approach\nis presented in Figure 1. The proposed model for an adversar-\nial attack has two regimes. Both regimes work in a black-box\nmodel without knowledge of gradients of an attack, while the\nsecond regime uses a substitute (surrogate [15]) model output\nto make attack more efﬁcient.\nThe ﬁrst regime is a random sampling from a pre-\ntrained language model that produces adversarial examples\nby chance, whilst constraining discrepancy between an origi-\nnal and a modiﬁed sequence. The second regime is a targeted\nattack that modiﬁes the language model by optimising the\nloss with two terms related to misclassiﬁcation by the target\nmodel and a discrepancy between an initial sequence and\nits adversarial counterpart. After the generation of a set of\nsequences we select the most adversarial sequence according\nto the substitute model scores.\nThus, we expect that by construction the generated adver-\nsarial examples will fool a deep learning model, but will\nremain semantically close to the initial sequence. For the ﬁrst\nregime, we don’t need to ﬁne-tune a language model, whilst\nin some cases we will require to sample many sequences\nto ﬁnd an adversarial one. For the second regime, the\nmodel ﬁne-tuning requires some computational time, but\nafter the parameters are ﬁxed our approach produces adver-\nsarial examples with high success rate.\nFor the training phase in the second regime, we use a\nseparately trained differentiable version of the Levenshtein\ndistance [16] and the Gumbel-Softmax heuristic to pass the\nderivatives through our sequence generation layers. As our\nloss is differentiable, we can adopt any gradient-based adver-\nsarial attack. The number of hyperparameters in our method\nis small, as we want our attack to be easily adopted to new\ndatasets and problems. The training and inference procedure\nis summarised in Figure 2. Examples of sentences generated\nvia our attack DILMA are presented in Table 1.\nTo summarise, the main contributions of this work are the\nfollowing.\n• We propose a new black-box adversarial attack DILMA\non classiﬁer models for texts. The attack is based on a\nmasked language model (MLM) and a differentiable loss\nfunction to optimise during an attack. The model utilises\na substitute classiﬁer.\n• Our DILMA relies on ﬁne-tuning parameters of an\nMLM by optimising the weighted sum of two differ-\nentiable terms: one is based on a surrogate distance\nbetween the source text and its adversarial version, and\nanother is a substitute classiﬁer model score (Sec. III).\nHence, we adopt a generative model framework for the\ntask of the generation of adversarial examples.\n• We apply DILMA to various NLP sentence classiﬁca-\ntion tasks and receive superior results in comparison to\nother methods (Sec. IV).\n• We show that a particular advantage of DILMA, when\ncompared to other approaches, lies in the resistance to\ncommon defense strategies. The vast majority of exist-\ning approaches fail to fool models after they defend\nthemselves (Sec. V).\n• We provide a framework to extensively evaluate adver-\nsarial attacks for sentence classiﬁcation and conduct\na thorough evaluation of DILMA and related attacks.\nAdversarial training and adversarial detection must be\nan essential part of the adversarial attack evaluation on\ntextual data in support to human evaluation (Sec. V-D).\nII. RELATED WORK\nThere exist adversarial attacks for different types of data: the\nmost mainstream being image data [17], [18], graph data [19],\nand sequences [20]. The latter work is one of the ﬁrst publica-\ntions on generation of adversarial attacks for natural language\nprocessing, such as texts. It identiﬁes two main challenges for\nthe task: a discrete space of possible objects and a complex\ndeﬁnition of a semantically coherent sequence.\nThere is a well-established categorisation of textual attacks\naccording to the perturbation level. Character-level attacks\ninclude replacing some of the characters with visually similar\nones [21]. HotFlip uses gradient-based decisions to swap,\nremove, or insert characters [11]. [22] creates sub-word\nperturbations to craft non-standard English adversaries and\nmitigates the cultural biases of machine translation models.\nToken-levelattacks replace a token (mostly at the word level)\nin a given sentence, based on a salience score and following a\nspeciﬁc synonym replacement strategy. [23] chooses the best\nsynonym replacement from WordNet to generate a sentence,\nclose to the classiﬁer’s decision boundary. [24] and [25]\nutilise BERT’s masked model to generate the substitution to\nthe target word in multiple ways. The Metropolis-Hastings\nVOLUME 10, 2022 17967\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\nTABLE 1. Attack samples for the PWWS attack (top performing according to our evaluation) and our attack DILMA. DILMA can provide more diverse\nadversarial sequences with meanings similar to that of the initial sentence.\nFIGURE 2. The training phase of the DILMA architecture consists of the\nfollowing steps.Step 1: obtain logitsP from a pre-trained Language\nModel (LM) generator for inputx. Step 2: samplex′ from the multinomial\ndistribution P using the Gumbel-Softmax estimator. To improve\ngeneration quality we can sample many times.Step 3: obtain the\nsubstitute probabilityC(x′) and approximate the edit distanceDL(x′,x).\nStep 4: calculate the loss, do a backward pass.Step 5: update parameters\nof the LM using the computed gradients.\nalgorithm improves sampling from a constrained distribution,\nallowing to substitute a word in a sentence which belongs to\na desired class [26]. Sentence-level attacks aim to generate a\nnew adversarial instance from scratch with the help of para-\nphrasing models [27], back translation [28] or competitive\ndialogue agents [29].\nAnother way to categorise textual adversarial attacks\naccounts for the amount of information received from the\nvictim target model. White box attacks have full access\nto the victim model and its gradients. Generating adver-\nsarial examples guided by the training loss is intractable\ndue to the discrete nature of textual data. Instead greedy\nmethods [30] or the Gumbel trick [31] are used to make\ncomputations tractable. Black boxattacks have access only\nto the model outputs. Victim’s scores may help to estimate\nword salience [32]. Black box attacks can emulate white\nbox attacks. For example, DistFlip [33] is a faster version of\nHotFlip with a similar accuracy.\nBlind attacks do not have access to the target model\nat all and can be seen as a form of text augmenta-\ntion. Such attacks include stop word removal, dictionary-\nbased word replacement, and rule-based insertion of\nnegation [34].\nMethods to defend against attacks exploit adversarial sta-\nbility training [35] and robust word and character represen-\ntations [36]. Back-off strategies to recognise intentionally\nreplaced and corrupted words can detect perturbed inputs\nbefore actually passing them through a classiﬁer [37].\nCommon tasks requiring defence from adversarial attacks\nby adversarial training, include classiﬁcation, sentence pair\nproblems, such as natural language inference (NLI) [32], and\nmachine translation [38], [39].\nFrom a technical point of view, two open source frame-\nworks for textual adversarial attacks, adversarial training\nand text augmentation, namely, OpenAttack [40] and\nTextAttack [6], facilitate the research in the area and help\nto conduct fast and correct comparison.\nFrom the current state of the art, we see a lack of effec-\ntive ways to generate adversarial categorical sequences and\ndefend from such attacks. Existing approaches use previous\ngenerations of LMs based on recurrent architectures, stay at a\ntoken level, or use V AEs, despite known limitations of these\nmodels for modelling NLP data [41], [42].\n17968 VOLUME 10, 2022\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\nIII. METHODS\nA. GENERAL DESCRIPTION OF THE APPROACH\nWe generate adversarial examples using two consecutive\ncomponents: a masked language model (MLM) with param-\neters θ that provides for an input sequence x, conditional\ndistribution pθ(x′|x), and a sampler from this distribution such\nthat x′∼pθ(x′|x). Thus, we can generate sequences x′by a\nconsecutive application of the MLM and the sampler.\nWe can use a pre-trained MLM to generate probably\nadversarial sequences. But to make sampling more efﬁcient,\nwe optimise MLM parameters θ with respect to a pro-\nposed differentiable loss function that forces the MLM to\ngenerate semantically similar but adversarial examples. The\nloss function consists of two terms: the ﬁrst term corre-\nsponds to a substitute classiﬁer C(x′) that outputs a prob-\nability of belonging to a target class, and the second term\ncorresponds to a Deep Levenshtein distance DL(x,x′) that\napproximates the edit distance between sequences. Thus,\nby minimising the loss function, we expect to get low scores\nfor a substitute classiﬁer, fooling the model, but keep seman-\ntics similar by minimising an approximation of Levenshtein\ndistance between an initial sequence and an adversarial\none.\nThe general scheme of our approach is given in Fig. 2.\nWe start with the description of the LM and the sampler in\nSubsection III-B. We continue with the description of the\nloss function in Subsection III-C. The formal description of\nour algorithm is given in Subsection III-D. In later subsec-\ntions III-E and III-F, we provide more details on the use of\nthe target and substitute classiﬁers and the Deep Levenshtein\nmodel correspondingly.\nB. MASKED LANGUAGE MODEL\nThe MLM is a model that takes a sequence of tokens\n(e.g. words) as input x ={x 1,..., xt }and outputs logits for\ntokens P = {p1,..., pt } ∈Rd·t for each index 1,..., t,\nwhere d is the size of the dictionary of tokens. In this work\nwe use a transformer architecture as the LM [42]. We pre-\ntrain a transformer encoder [42] in a BERT [43] manner from\nscratch using the available data.\nThe sampler is deﬁned as follows: the MLM learns a\nprobability distribution over sequences, so we can sample\na new sequence x′ = {x′\n1,..., x′\nt }based on the vectors of\ntoken logits P. In this work we use Straight-Through Gumbel\nEstimator ST (P) : P → x′ for sampling [44]. To get the\nactual probabilities qij from logits pij, we use a softmax\nwith temperature τ [45], where τ > 1, which produces\na softer probability distribution over classes. As τ → ∞,\nthe original distribution approaches the uniform distribution.\nIf τ → 0, then sampling from it reduces to arg max\nsampling.\nOur ﬁrst method SamplingFool samples sequences from\nthe categorical distribution with qij probabilities. The method\nis similar to the random search algorithm and serves as a\nbaseline.\nC. LOSS FUNCTION\nThe Straight-Through Gumbel sampling allows a propagation\nof the derivatives through the softmax layer. Hence, we opti-\nmise parameters θ of our MLM to improve the quality of\ngenerated adversarial examples.\nLet Ct\ny(x) be the probability of the target classiﬁer to\npredict the considered class y. A loss function takes two terms\ninto account: the ﬁrst term estimates the probability score\ndrop (1−Ct\ny(x′)) of the target classiﬁer, the second term repre-\nsents the edit distance DL(x,x′) between the initial sequence\nx and the generated sequence x′. We should maximise the\nprobability drop and minimise the edit distance, so it is as\nclose to 1 as possible.\nIn the black-box scenario we do not have access to the\ntrue classiﬁer score, so we use a substitute classiﬁer score\nCy(x′) ≈ Ct\ny(x′). As a differentiable alternative to edit\ndistance, we use the Deep Levenshtein model proposed\nin [16] — a deep learning model that approximates the edit\ndistance: DL(x,x′) ≈D(x,x′).\nThis way, the loss function becomes:\nL(x′,x,y) =β(1 −DL(x′,x))2 −log(1 −Cy(x′)), (1)\nwhere Cy(x′) is the probability of the true class y for sequence\nx′and β is a weighting coefﬁcient. Thus, we penalise cases\nwhen a modiﬁcation in more than one token is needed in\norder to get x′from x. Since we focus on non-target attacks,\nthe Cy(x′) component is included in the loss. The smaller the\nprobability of an attacked class, the smaller the loss.\nWe estimate derivatives of L(x′,x,y) in a way similar\nto [46]. Using these derivatives, a backward pass updates the\nweights θof the MLM. We ﬁnd that updating the whole set of\nparameters θ is not the best strategy, and a better alternative\nis to update only the last linear layer and the last layer of the\ngenerator.\nWe consider two DILMA options: initial DILMA that\nminimises only the classiﬁer score (the second term) during\nthe attack and DILMA with DLthat takes into account the\napproximate Deep Levenshtein edit distance.\nD. DILMA ALGORITHM\nNow we are ready to group the introduced components into a\nformal algorithm with the architecture depicted in Fig. 2.\nGiven a sequence x, the DILMA attack performs the fol-\nlowing steps at iterations i =1,2,..., k starting from the\nparameters of a pre-trained MLM θ0 =θ and running for k\niterations.\nStep 1. Pass the sequence x through the pre-trained MLM.\nObtain logits P =LMθi−1 (x).\nStep 2. Sample an adversarial sequence x′from the logits\nusing the Gumbel-Softmax Estimator.\nStep 3. Calculate the probability Cy(x′) and the Deep Lev-\nenshtein distance DL(x′,x). Calculate the loss value\nL(x′,x,y) (1).\nStep 4. Do a backward pass to update the LM’s weights θi−1\nusing gradient descent and get the new weights θi.\nVOLUME 10, 2022 17969\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\nStep 5. Obtain an adversarial sequence x′\ni by sampling based\non the softmax with a selected temperature.\nThe algorithm chooses which tokens should be replaced.\nThe classiﬁcation component changes the sequence in a\ndirection where the probability score Cy(x′) is low, and the\nDeep Levenshtein distance keeps the generated sequence\nclose to the original one.\nWe obtain a set of m sampled adversarial sequences on each\niteration of the algorithm. The last sequence in this set is not\nalways the best one. Therefore among generated sequences\nthat are adversarial w.r.t. the substitute classiﬁer Cy(x′) we\nchoose x′\nopt with the lowest Word Error Rate (WER), esti-\nmated with respect to the initial sequence x. If all examples\ndon’t change the model prediction w.r.t. Cy(x′), then we select\nx′\nopt with the smallest target class score, estimated by the\nsubstitute classiﬁer.\nWe choose the hyperparameter set achieving the best NAD\nscore with the Optuna framework [47].\nE. CLASSIFICATION MODEL\nIn all experiments we use two classiﬁers: a target classiﬁer\nCt (x) that we attack and a substitute classiﬁer C(x) that\nprovides differentiable substitute classiﬁer scores.\nThe target classiﬁer is RoBerta [48]. The substitute classi-\nﬁer is the LSTM model. The hidden size is 150, the dropout\nrate is 0.3.\nThe substitute classiﬁer has access only to 50% of the data,\nwhilst the target classiﬁer uses the entire dataset. We split the\ndataset into two parts keeping the class balance similar for\neach part.\nF. THE DEEP LEVENSHTEIN MODEL\nThe differentiable version of the edit distance allows gradient-\nbased updates of parameters. Following the Deep Lev-\nenshtein approach [16], we train a deep learning model\nDL(x,x′) to estimate the Word Error Rate (WER) between\ntwo sequences x and x′. We treat WER as the word-level\nLevenshtein distance.\nFollowing [49], the Deep Levenshtein model receives two\nsequences (x,y). It encodes them into dense representations\nzx =E(x) and zy =E(y) of ﬁxed length l using the shared\nencoder. Then it concatenates the representations and the\nabsolute difference between them in a vector (z x,zy,|zx−zy|)\nof length 3l . At the end the model uses a fully-connected layer\nto predict WER. To estimate the parameters of the encoder\nand the fully connected layer we use the L2 loss between\nthe true and the predicted WER values. We form a training\nsample of size of about two million data points by sampling\npairs of sequences and their modiﬁcations from the training\ndata.\nIV. EXPERIMENTS\nThe datasets and the source code are published online 1.\n1The code is available at https://anonymous.4open.science/r/4cf31d59-\n9fe9-4854-ba53-7be1f9f6be7d/\nA. COMPETING APPROACHES\nWe compared our approach to other popular approaches\ndescribed below. By selecting the methods among the ones\navailable in the literature, we target the diversity of proposed\napproaches and focus on the top performers for each direc-\ntion. We also aim to cover attacks with different trade-offs\nbetween the desired word error rate (WER) and the classiﬁer\nscore drop. Whilst this trade-off for some attacks is a result\nof the selection of hyperparameteres, the exact conﬁguration\nis not possible, so we focus on such hyperparameters that\nmaximise the NAD score.\nHotFlip [11] selects the best token to change, given an\napproximation of partial derivatives for all tokens and all\nelements of the dictionary. To change multiple tokens HotFlip\nselects the sequence of changes via a beam search.\nTextbugger [50] works at a symbol level and tries\nto replace symbols in words to generate new adversarial\nsequences using derivative values for possible replacements.\nDeepWordBug [51] is a greedy replace-one-word heuris-\ntic scoring based on an estimate of importance of a word to\na RNN model with each replacement being a character-swap\nattack. DeepWordBug is a black box attack.\nPWWS [52] is a greedy synonym-swap method, which\ntakes into account the saliencies of the words and the effec-\ntiveness of their replacement. Replacements of the words are\nmade with the help of the WordNet dictionary.\nB. DATASETS\nWe conducted experiments on four open NLP datasets for\ndifferent tasks such as text classiﬁcation, intent prediction,\nand sentiment analysis. The characteristics of these datasets\nare presented in Table 2.\nThe AG News corpus(AG) [53] consists of news articles\non the web from the AG corpus. There are four classes: World,\nSports, Business, and Sci/Tech. Both training and test sets\nare perfectly balanced. The Dialogue State Tracking Chal-\nlenge dataset (DSTC) is a special processed dataset related\nto dialogue system tasks. The standard DSTC8 dataset [54]\nwas adopted to the intent prediction task by extracting most\nintent-interpreted sentences from dialogues. The Stanford\nSentiment Treebank (SST-2) [55] contains phrases with\nﬁne-grained sentiment labels in the parse trees of 11, 855 sen-\ntences from ﬁlm reviews. The Rotten Tomatoes dataset\n(RT) [56] is a ﬁlm-review dataset of sentences with positive\nor negative sentiment labels.\nC. NAD METRIC\nIn order to create an adversarial attack, changes must be\napplied to the initial sequence. A change can be done either\nby inserting, deleting, or replacing a token in some position\nin the original sequence. In the WER calculation, the cost\nof any change to the sequence made by insertion, deletion,\nor replacement is 1. Therefore, we consider the adversarial\nsequence to be perfect if WER =1, and the target classiﬁer\noutput has changed. For the classiﬁcation task, Normalised\n17970 VOLUME 10, 2022\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\nTABLE 2. Statistics of the datasets.\nTABLE 3. The NAD metric before/after adversarial training on 5,000 examples. We want to maximise the NAD metric for both cases. The best values are\nin bold, the second best values are underscored. DILMA is resistant to adversarial training as well as sampling fool.\nTABLE 4. The attack success rate and WER The best values are inbold, the second best values are underscored. DILMA shows the highest attack success\nrate among all datasets.\nAccuracy Drop (NAD) is calculated in the following way:\nNAD(A) =1\nN\nN∑\ni=1\n1{Ct (xi) ̸=Ct (x′\ni))}\nWER(xi,x′\ni) ,\nwhere x′ =A(x) is the output of an adversarial generation\nalgorithm for the input sequence x, Ct (x) is the label assigned\nby the target classiﬁcation model, and WER(x′,x) is the\nWord Error Rate. The highest value of NAD is achieved\nwhen WER(x′\ni,xi) =1 and C(xi) ̸= C(x′\ni) for all i. Here\nwe assume that adversaries produce distinct sequences and\nWER(xi,x′\ni) ≥1.\nV. EVALUATION AND COMPARISON\nA. ADVERSARIAL ATTACK EVALUATION\nResults for the considered methods are presented in Table 3.\nWe demonstrate not only the quality of attacks on the initial\ntarget classiﬁer, but also after its re-training with additional\nadversarial samples added to the training set. After re-training\nthe initial target classiﬁer, competitor attacks cannot provide\nreasonable results, whilst our methods perform signiﬁcantly\nbetter before and after retraining for most of the datasets.\nWe provide additional attack quality metrics in supplemen-\ntary materials.\nB. ADDITIONAL METRICS FOR EVALUATION\nWe provide two additional metrics to judge the quality of\nour and competitors’ attacks: accuracy of target models\nchanging after attacks and probability difference – a dif-\nference between true model scores before and after an\nattack. Higher values for probability differencesmean that\nan attack is more successful in affecting the target model\ndecision.\nDiscriminator training [57] is another defence strategy\nwe followed. We trained an additional discriminator on\n10,000 samples of the original sequences and adversarial\nexamples. The discriminator detects whether an example\nis normal or adversarial. The discriminator has an LSTM\narchitecture with ﬁne-tuning of GLoVE embeddings and was\ntrained for 50 epochs using the negative log-likelihood loss\nwith early stopping based on validation scores.\nWe provide all these scores in Table 5. As we can see\nour methods are top performer with respect to the obtained\naccuracy scores and probability differences. Due to smaller\nand more careful changes, HotFlip is harder to detect by a\ndiscriminator. However, ROC AUC scores are close to a ROC\nAUC score of a random classiﬁer for two of four datasets for\nour attacks.\nC. LINGUISTIC EVALUATION\nTo measure the linguistic features of the generated adversarial\nsentences we calculated several metrics, which can be divided\ninto two groups: showing how far the generated samples\nare from the original ones and demonstrating the linguistic\nquality of adversarial examples.\nVOLUME 10, 2022 17971\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\nTABLE 5. Additional performance scores before (first row) and after different attacks. Our methods are initalic. The best values are inbold, the second\nbest values are underscored.\nTABLE 6. Obtained diversity metrics Dist-2 and Ent-2 after an attack.↑ / ↓ shows if a metric was increased or decreased. Results for the initial sequences\nare in the first row. All attacks increase the diversity compared to the initial sentences, whilst maintaining semantics of generated adversarial examples.\nThese results show that the generated sentences are not\nvery different from the original ones. The same conclusion\nalso follows from the morphological and syntactic analyses.\nWe evaluate a similarity of part-of-speech (POS) anno-\ntations between original and adversarial sentences using the\nJaccard index (the intersection of two sets over the union\nof two sets) between sets containing the POS tags of words\nfrom the original A and adversarial B sentences. Duplicated\nPOS-tags in the sets are allowed. In the same way, the similar-\nity of syntax annotationsis estimated as the Jaccard index\nof dependency relation tags. We use the Stanza toolkit [58]\nfor POS-tagging and dependency parsing. The detailed report\nabout morphological and syntactic similarities can be found\nin the Appendix.\nThe diversity across the generated sentences was eval-\nuated by two measures. The ﬁrst measure, Dist-k, is the\ntotal number of distinct k-grams divided by the total num-\nber of produced tokens in all the generated sentences [59].\nThe second measure, Ent-k [60] considers that infrequent k-\ngrams contribute more to diversity than frequent ones. Table 6\npresents the results for k =2. We choose this value as being\nmore indicative to estimate the diversity. A table for other\nvalues of k can be found in the Appendix. As we can see,\npresented methods preserve the lexical diversity.\nD. HUMAN EVALUATION\nWe conducted a human evaluation to understand how com-\nprehensive DILMA adversarial perturbations are and to\ncompare DILMA to other approaches. We used a sample from\nthe SST-2 dataset of size 200, perturbed with ﬁve different\nattacks.\nWe recruited crowd workers from a crowdsourcing plat-\nform to estimate the accuracy of the methods we compare.\nGiven a sentence and the list of classes, the workers were\nasked to deﬁne a class label. We used original sentences to\ncontrol the workers’ performance and estimated the results on\nadversarial sentences. Each sentence was shown to 5 workers.\nWe used the majority vote as the ﬁnal label. For all different\nattacks we achieve similar performances at about 0.70 ±0.02,\nwith HotFlip scoring 0.72 and DILMA scoring 0.7.\nVI. CONCLUSION\nConstructing adversarial attacks for natural language pro-\ncessing is a challenging problem due to the discrete nature\nof input data and non-differentiability of the loss function.\nOur idea is to combine sampling from a masked language\nmodel (MLM) with tuning of its parameters to produce truly\nadversarial examples. To tune parameters of the MLM we use\na loss function based on two differentiable surrogates – for\na distance between sequences and for an attacked classiﬁer.\nThis results in the proposed DILMA with DL approach. If we\nonly sample from the MLM, we obtain a simple baseline\nSamplingFool.\nIn order to estimate the efﬁciency of adversarial attacks\non categorical sequences we have proposed a metric com-\nbining WER and the accuracy of the target classiﬁer. For\n17972 VOLUME 10, 2022\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\nTABLE 7. Word error rate (WER) of attacks. Our methods are initalic.\nSmaller values are better. The best values are inbold, the second best\nvalues are underscored.\nthe considered set of diverse NLP datasets, our approaches\ndemonstrate a good quality outperforming other approaches.\nThe second term in the loss function related to the Deep\nLevenshtein model is important, as it improves performance\nscores. Moreover, in contrast to competing methods, our\napproaches win over common strategies used to defend from\nadversarial attacks. Human and linguistic evaluation also\nshow the adequacy of the proposed attacks.\nVII. SUPPLEMENTARY MATERIALS\nA. INTRODUCTION\nSupplementary materials consist of the following subsection:\n• In subsection VII-B we present additional metrics to\nassess the overall quality of attacks.\n• In subsection VII-C we show whether our attack can be\ndetected in a simple way.\n• In subsection VII-E we present additional linguistic met-\nrics to assess the diversity and similarity to the initial\nsequence for the sequences generated by adversarial\nattacks.\n• In subsection VII-F we make notes about complexity of\nour method and competing approaches.\nB. ADDITIONAL METRICS ON OVERALL ATTACK QUALITY\nWord Error Rate(WER) derives from the Levenshtein dis-\ntance, working at the word level instead of the phoneme level.\nIt is the ratio of the number of changes required to get one\ntoken sequence from another divided by the total number of\ntokens. We use words as tokens in this case.\nWER =S +D +I\nN ,\nwhere S is the number of substitutions, D is the number of\ndeletions, I is the number of insertions, and N is the number\nof words in the reference. The mean results are presented in\nTable 7. We see that whilst our attacks change more words\nthan others, they can preserve the original meanings of sen-\ntences according to our other experiments.\nC. ADDITIONAL EXPERIMENT ON ADVERSARIAL\nDETECTION\nWe conducted an additional experiment on the detection of\nadversarial examples. We train an adversarial discriminator:\na classiﬁer that tries to distinguish adversarial and normal\nTABLE 8. ROC AUC scores (↓) for adversarial discriminator as a\ncountermeasure against adversarial attacks: a binary classification\n‘‘adversary vs non-adversary’’ . Our methods are initalic. The best values\nare inbold, the second best values are underscored\n. Only successful\nattacks are considered.\nsequences for different attacks. Our discriminator is a bidi-\nrectional LSTM with GloVe embeddings. We run the training\nphase for samples of about 1, 000 examples with the number\nbeing slightly different for different attacks. The stopping\ncriterion was an increase in the loss for a test sample.\nIn Table 8, we report ROC AUC values for this classiﬁer.\nAs we want our attack to be undetectable, low ROC AUC\nvalues certify that our attack is hard to detect via a model\nof decent quality. We see, that across different datasets the\nperformance of different attacks differ, while among alterna-\ntive to ours approach, we observe either weak overall per-\nformance (HotFlip) or high detection rate by a discriminator\n(DeepWordBug, PWWS, TextBugger).\nD. ADDITIONAL LINGUISTIC METRICS ON ATTACK\nQUALITY\nSemantic, morphological, and syntactic similarities show\nhow far is the generated sentence from the original one from\nthe linguistic point of view. Detailed results are presented in\nTable 9.\nE. ADDITIONAL LINGUISTIC METRICS ON ATTACK\nQUALITY\nSemantic, morphological, and syntactic similarities show\nhow far is the generated sentence from the original one from\nthe linguistic point of view. Detailed results are presented in\nTable 9.\nThe diversity of the generated sentences was evaluated\nby Dist-k and Ent-k metrics. Dist-k is the total number of\ndistinct k-grams divided by the total number of produced\ntokens in all of the generated sentences. Ent-k is the analogue\nof entropy and considers that infrequent k-grams contribute\nmore to diversity than frequent ones. We consider these met-\nrics for different values of k to get broader evaluation of the\ndiversity of generated sequences. The results are in Table 10\nfor Dist −k and in Table 11 for Ent −k.\nF. ADDITIONAL EXPERIMENT DETAILS\nThe DILMA attack performs 30 iterations on each sample.\nThe output is chosen based on the probability drop of the tar-\nget model. Competing approaches do not strictly restrict the\nnumber of attack iterations. Still, they use a list of constraints\nVOLUME 10, 2022 17973\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\nTABLE 9. Semantic (Sem), morphological (Mor), and syntactic (Syn) similarities between original and generated sentences. Our methods are emphasised\nin italic. The values related to closest texts are inbold, the second closest values are underscored.\nTABLE 10. Dist-k results after an attack. Results before an attack are in the first row. Our methods are emphasised initalic.\nTABLE 11. Ent-k results after an attack. Results before an attack are in the first row. Our methods are emphasised initalic.\nas a stopping mechanism: not allowing the attack to modify\none word twice, not allowing the attack to alter a word from\nstoplist, stopping if the Levenshtein distance between source\nand perturbed sequence crosses a threshold.\nVIII. CONCLUSION\nThe provided metric values support our claims from the main\narticle about a decent quality of our attacks DILMA, DILMA\nwith DL and SamplingFool compared to the attacks on NLP\nmodels presented currently in the literature.\nREFERENCES\n[1] X. Yuan, P. He, Q. Zhu, and X. Li, ‘‘Adversarial examples: Attacks\nand defenses for deep learning,’’ IEEE Trans. Neural Netw. Learn. Syst.,\nvol. 30, no. 9, pp. 2805–2824, Sep. 2019.\n[2] N. Akhtar and A. Mian, ‘‘Threat of adversarial attacks on deep learning\nin computer vision: A survey,’’ IEEE Access, vol. 6, pp. 14410–14430,\n2018.\n[3] I. Oseledets and V . Khrulkov, ‘‘Art of singular vectors and universal\nadversarial perturbations,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., Jun. 2018, pp. 8562–8570.\n[4] W. E. Zhang, Q. Z. Sheng, A. Alhazmi, and C. Li, ‘‘Adversarial attacks\non deep learning models in natural language processing: A survey,’’ 2019,\narXiv:1901.06796.\n[5] W. Wang, R. Wang, L. Wang, Z. Wang, and A. Ye, ‘‘Towards a robust deep\nneural network in texts: A survey,’’ 2019, arXiv:1902.07285.\n[6] J. Morris, E. Liﬂand, J. Y . Yoo, J. Grigsby, D. Jin, and Y . Qi, ‘‘TextAttack:\nA framework for adversarial attacks, data augmentation, and adversarial\ntraining in NLP,’’ in Proc. Conf. Empirical Methods Natural Lang. Pro-\ncess., Syst. Demonstrations, 2020, pp. 119–126.\n[7] L. Sun, Y . Dou, C. Yang, J. Wang, P. S. Yu, L. He, and B. Li, ‘‘Adversarial\nattack and defense on graph data: A survey,’’ 2018, arXiv:1812.10528.\n[8] A. Kurakin, I. Goodfellow, and S. Bengio, ‘‘Adversarial machine learning\nat scale,’’ 2016, arXiv:1611.01236.\n[9] S. Samanta and S. Mehta, ‘‘Towards crafting text adversarial samples,’’\n2017, arXiv:1707.02812.\n[10] B. Liang, H. Li, M. Su, P. Bian, X. Li, and W. Shi, ‘‘Deep text classiﬁcation\ncan be fooled,’’ 2017, arXiv:1704.08006.\n[11] J. Ebrahimi, A. Rao, D. Lowd, and D. Dou, ‘‘HotFlip: White-box adversar-\nial examples for text classiﬁcation,’’ in Proc. 56th Annu. Meeting Assoc.\nComput. Linguistics, vol. 2, 2018, pp. 31–36.\n[12] M. Sato, J. Suzuki, H. Shindo, and Y . Matsumoto, ‘‘Interpretable adversar-\nial perturbation in input embedding space for text,’’ in Proc. 27th Int. Joint\nConf. Artif. Intell., Jul. 2018, pp. 4323–4330.\n[13] Y . Ren, J. Lin, S. Tang, J. Zhou, S. Yang, Y . Qi, and X. Ren, ‘‘Generating\nnatural language adversarial examples on a large scale with generative\nmodels,’’ 2020, arXiv:2003.10388.\n[14] I. Fursov, A. Zaytsev, N. Kluchnikov, A. Kravchenko, and E. Burnaev,\n‘‘Gradient-based adversarial attacks on categorical sequence models via\ntraversing an embedded world,’’ 2020, arXiv:2003.04173.\n[15] Z. Yan, Y . Guo, and C. Zhang, ‘‘Subspace attack: Exploiting promising\nsubspaces for query-efﬁcient black-box attacks,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., 2019, pp. 3825–3834.\n[16] S. Moon, L. Neves, and V . Carvalho, ‘‘Multimodal named entity recog-\nnition for short social media posts,’’ in Proc. Conf. North Amer. Chapter\nAssoc. Comput. Linguistics, Hum. Lang. Technol., (Long Papers), vol. 1,\n2018, pp. 852–860.\n17974 VOLUME 10, 2022\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\n[17] C. Szegedy, W. Zaremba, I. Sutskever, J. B. Estrach, D. Erhan,\nI. Goodfellow, and R. Fergus, ‘‘Intriguing properties of neural networks,’’\nin Proc. ICLR, 2014, pp. 1–10.\n[18] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‘‘Explaining and harnessing\nadversarial examples,’’ 2014, arXiv:1412.6572.\n[19] D. Zügner, A. Akbarnejad, and S. Günnemann, ‘‘Adversarial attacks on\nneural networks for graph data,’’ in Proc. 28th Int. Joint Conf. Artif. Intell.,\nAug. 2019, pp. 2847–2856.\n[20] N. Papernot, P. McDaniel, A. Swami, and R. Harang, ‘‘Crafting adversarial\ninput sequences for recurrent neural networks,’’ in Proc. MILCOM IEEE\nMil. Commun. Conf., Nov. 2016, pp. 49–54.\n[21] S. Eger, G. G. Şahin, A. Rücklé, J.-U. Lee, C. Schulz, M. Mesgar,\nK. Swarnkar, E. Simpson, and I. Gurevych, ‘‘Text processing like humans\ndo: Visually attacking and shielding,’’ in Proc. Conf. North, 2019,\npp. 1634–1647.\n[22] S. Tan, S. Joty, M.-Y . Kan, and R. Socher, ‘‘It’s Morphin’ time! Combating\nlinguistic discrimination with inﬂectional perturbations,’’ in Proc. 58th\nAnnu. Meeting Assoc. Comput. Linguistics, 2020, pp. 2920–2935.\n[23] Z. Meng and R. Wattenhofer, ‘‘A geometry-inspired attack for generating\nnatural language adversarial examples,’’ in Proc. 28th Int. Conf. Comput.\nLinguistics, 2020, pp. 6679–6689.\n[24] L. Li, R. Ma, Q. Guo, X. Xue, and X. Qiu, ‘‘BERT-ATTACK: Adversarial\nattack against BERT using BERT,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process. (EMNLP), 2020, pp. 6193–6202.\n[25] S. Garg and G. Ramakrishnan, ‘‘BAE: BERT-based adversarial examples\nfor text classiﬁcation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2020, pp. 6174–6181.\n[26] H. Zhang, H. Zhou, N. Miao, and L. Li, ‘‘Generating ﬂuent adversarial\nexamples for natural languages,’’ in Proc. 57th Annu. Meeting Assoc.\nComput. Linguistics, 2019, pp. 5564–5569.\n[27] W. C. Gan and H. T. Ng, ‘‘Improving the robustness of question answering\nsystems to question paraphrasing,’’ in Proc. 57th Annu. Meeting Assoc.\nComput. Linguistics, 2019, pp. 6065–6075.\n[28] Y . Zhang, J. Baldridge, and L. He, ‘‘Paws: Paraphrase adversaries from\nword scrambling,’’ in Proc. Conf. North Amer. Chapter Assoc. Comput.\nLinguistics, Human Lang. Technol. (Long and Short Papers), vol. 1, 2019,\npp. 1298–1308.\n[29] M. Cheng, W. Wei, and C.-J. Hsieh, ‘‘Evaluating and enhancing the robust-\nness of dialogue systems: A case study on a negotiation agent,’’ in Proc.\nConf. North, 2019, pp. 3325–3335.\n[30] Y . Cheng, L. Jiang, and W. Macherey, ‘‘Robust neural machine translation\nwith doubly adversarial inputs,’’ 2019, arXiv:1906.02443.\n[31] P. Yang, J. Chen, C.-J. Hsieh, J.-L. Wang, and M. I. Jordan, ‘‘Greedy attack\nand gumbel attack: Generating adversarial examples for discrete data,’’\nJ. Mach. Learn. Res., vol. 21, no. 43, pp. 1–36, 2020.\n[32] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, ‘‘Is bert really robust? A strong\nbaseline for natural language attack on text classiﬁcation and entailment,’’\nin Proc. AAAI Conf. Artif. Intell., vol. 34, no. 5, 2020, pp. 8018–8025.\n[33] Y . Gil, Y . Chai, O. Gorodissky, and J. Berant, ‘‘White-to-black: Efﬁcient\ndistillation of black-box adversarial attacks,’’ in Proc. Conf. North, 2019,\npp. 1373–1379.\n[34] T. Niu and M. Bansal, ‘‘Adversarial over-sensitivity and over-stability\nstrategies for dialogue models,’’ in Proc. 22nd Conf. Comput. Natural\nLang. Learn., 2018, pp. 486–496.\n[35] H. Liu, Y . Zhang, Y . Wang, Z. Lin, and Y . Chen, ‘‘Joint character-level word\nembedding and adversarial stability training to defend adversarial text,’’ in\nProc. AAAI Conf. Artif. Intell., vol. 34, 2020, pp. 8384–8391.\n[36] E. Jones, R. Jia, A. Raghunathan, and P. Liang, ‘‘Robust encodings: A\nframework for combating adversarial typos,’’ in Proc. 58th Annu. Meeting\nAssoc. Comput. Linguistics, 2020, pp. 2752–2765.\n[37] D. Pruthi, B. Dhingra, and Z. C. Lipton, ‘‘Combating adversarial mis-\nspellings with robust word recognition,’’ in Proc. 57th Annu. Meeting\nAssoc. Comput. Linguistics, 2019, pp. 5582–5591.\n[38] M. Cheng, J. Yi, P.-Y . Chen, H. Zhang, and C.-J. Hsieh, ‘‘Seq2sick: Eval-\nuating the robustness of sequence-to-sequence models with adversarial\nexamples,’’ inProc. AAAI Conf. Artif. Intell., vol. 34, 2020, pp. 3601–3608.\n[39] W. Zou, S. Huang, J. Xie, X. Dai, and J. Chen, ‘‘A reinforced generation of\nadversarial examples for neural machine translation,’’ in Proc. 58th Annu.\nMeeting Assoc. Comput. Linguistics, 2020, pp. 3486–3497.\n[40] G. Zeng, F. Qi, Q. Zhou, T. Zhang, Z. Ma, B. Hou, Y . Zang, Z. Liu, and\nM. Sun, ‘‘OpenAttack: An open-source textual adversarial attack toolkit,’’\n2020, arXiv:2009.09191.\n[41] Z. Yang, Z. Hu, R. Salakhutdinov, and T. Berg-Kirkpatrick, ‘‘Improved\nvariational autoencoders for text modeling using dilated convolutions,’’ in\nICML, pp. 3881–3890, 2017.\n[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. NeurIPS,\n2017, pp. 5998–6008.\n[43] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘Bert: Pre-training of\ndeep bidirectional transformers for language understanding,’’ 2018.\n[44] E. Jang, S. Gu, and B. Poole, ‘‘Categorical reparametrization with gumble-\nsoftmax,’’ in Proc. Int. Conf. Learn. Represent. (ICLR), 2017.\n[45] G. Hinton, O. Vinyals, and J. Dean, ‘‘Distilling the knowledge in a neural\nnetwork,’’ Tech. Rep., 2015.\n[46] E. Jang, S. Gu, and B. Poole, ‘‘Categorical reparameterization with\ngumbel-softmax,’’ Tech. Rep., 2016.\n[47] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, ‘‘Optuna: A\nnext-generation hyperparameter optimization framework,’’ in Proc. 25th\nACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Jul. 2019,\npp. 2623–2631.\n[48] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘Roberta: A robustly optimized bert\npretraining approach,’’ Tech. Rep., 2019.\n[49] X. Dai, X. Yan, K. Zhou, Y . Wang, H. Yang, and J. Cheng, ‘‘Convolutional\nembedding for edit distance,’’ in Proc. 43rd Int. ACM SIGIR Conf. Res.\nDevelop. Inf. Retr., Jul. 2020, pp. 599–608.\n[50] J. Li, S. Ji, T. Du, B. Li, and T. Wang, ‘‘TextBugger: Generating adversarial\ntext against real-world applications,’’ in Proc. Netw. Distrib. Syst. Secur.\nSymp., 2019, pp. 1–15.\n[51] J. Gao, J. Lanchantin, M. L. Soffa, and Y . Qi, ‘‘Black-box generation of\nadversarial text sequences to evade deep learning classiﬁers,’’ in Proc.\nIEEE Secur. Privacy Workshops (SPW), May 2018, pp. 50–56.\n[52] S. Ren, Y . Deng, K. He, and W. Che, ‘‘Generating natural language adver-\nsarial examples through probability weighted word saliency,’’ in Proc. 57th\nAnnu. Meeting Assoc. Comput. Linguistics, 2019, pp. 1085–1097.\n[53] X. Zhang, J. Zhao, and Y . LeCun, ‘‘Character-level convolutional networks\nfor text classiﬁcation,’’ Tech. Rep., 2015.\n[54] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, and P. Khaitan, ‘‘Schema-\nguided dialogue state tracking task at DSTC8,’’ 2020, arXiv:2002.01359.\n[55] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng,\nand C. Potts, ‘‘Recursive deep models for semantic compositionality over\na sentiment treebank,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess., 2013, pp. 1631–1642.\n[56] B. Pang and L. Lee, ‘‘Seeing stars: Exploiting class relationships for\nsentiment categorization with respect to rating scales,’’ in Proc. 43rd Annu.\nMeeting Assoc. Comput. Linguistics (ACL), 2005, pp. 115–124.\n[57] H. Xu, Y . Ma, H. Liu, D. Deb, H. Liu, J. Tang, and A. K. Jain, ‘‘Adver-\nsarial attacks and defenses in images, graphs and text: A review,’’ 2019,\narXiv:1909.08072.\n[58] P. Qi, Y . Zhang, Y . Zhang, J. Bolton, and C. D. Manning, ‘‘Stanza: A Python\nnatural language processing toolkit for many human languages,’’ in Proc.\n58th Annu. Meeting Assoc. Comput. Linguistics, Syst. Demonstrations,\n2020, pp. 101–108.\n[59] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, ‘‘A diversity-promoting\nobjective function for neural conversation models,’’ in Proc. Conf. North\nAmer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol., 2016,\npp. 110–119.\n[60] Y . Zhang, M. Galley, J. Gao, Z. Gan, X. Li, C. Brockett, and B. Dolan,\n‘‘Generating informative and diverse conversational responses via adver-\nsarial information maximization,’’ in Proc. Adv. Neural Inf. Process. Syst.,\n2018, pp. 1815–1825.\nIVAN FURSOVwas born in Chelyabinsk, Russia.\nHe received the master’s degree from the Skolkovo\nInstitute of Science and Technology, in 2020. He is\ncurrently a Deep Learning Research Engineer at\nTinkoff and continues to work on new approaches\nin adversarial attacks on NLP models. In his mas-\nter’s thesis, he proposed a new adversarial attack\non sequence classiﬁers.\nVOLUME 10, 2022 17975\nI. Fursovet al.: Differentiable Language Model Adversarial Attack on Text Classifiers\nALEXEY ZAYTSEVwas born in Kharkiv, Ukraine.\nHe graduated from the MIPT, in 2012. He received\nthe Ph.D. degree in mathematics from the IITP\nRAS, in 2017. He is currently an Assistant Pro-\nfessor at the Skoltech. His research interests\ninclude development of new methods for sequen-\ntial data, Bayesian optimization, and embeddings\nfor weakly structured data. In his master’s thesis,\nhe proposed a modiﬁcation of Bayesian approach\nfor linear regression that allows an automated\nfeature selection.\nPAVEL BURNYSHEVwas born in Perm, Russia.\nHe graduated from the MIPT, in 2020. He is cur-\nrently pursuing the Master of Science degree with\nthe Skolkovo Institute of Science and Technology.\nHe is also a Data Scientist at the NLP Depart-\nment, Huawei, and works on adversarial attacks\nfor machine translation.\nEKATERINA DMITRIEVA is currently pursuing\nthe Ph.D. degree with the CS Faculty, HSE Univer-\nsity. Her research interests include semantic pars-\ning, in particular text2SQL models and adversarial\nattacks.\nNIKITA KLYUCHNIKOV received the M.Sc.\ndegree in information science and technology from\nthe Skolkovo Institute of Science and Technol-\nogy, the M.Sc. degree in applied mathematics and\nphysics from the Moscow Institute of Physics\nand Technology, in 2016, and the Ph.D. degree\nin computational and data science and engineer-\ning from the Skolkovo Institute of Science and\nTechnology, in 2021. His main research interests\ninclude machine learning, Bayesian optimization,\nand their industrial applications.\nANDREY KRAVCHENKO is currently a\nResearcher at the University of Oxford and the\nSkolkovo Institute of Science and Technology.\nHis Ph.D. research was at the intersection of\nmachine learning and unstructured data extraction.\nHe also played a signiﬁcant role in the DIADEM\nproject, which produced state-of-the art research in\nthe ﬁeld of large-scale fully automated web data\nextraction. His current research interests include\nthe theory and application of anomaly detection in\nbig data using sequences and graphs, and in particular, the development of\nefﬁcient machine learning algorithms based on the embedding of vectors.\nHe also works on exploring the broader connection between black-box\nmachine learning models and knowledge-based systems, with a particular\nfocus on knowledge graphs.\nEKATERINA ARTEMOVA graduated from HSE\nUniversity. She received the Ph.D. degree from the\nInstitute of System Analysis, RAS. She is currently\na Postdoctoral Researcher at the CS Faculty, HSE\nUniversity, and advises the Noah Ark’s NLP Team\non advanced research topics. She focuses on NLU\ntasks, ranging from ToD systems to IE and creating\nnew datasets.\nEVGENIA KOMLEVAgraduated from the MIPT,\nin 2021. She is currently pursuing the master’s\ndegree in data science with the Skolkovo Institute\nof Science and Technology. She is also working on\nNLP problems at ABBYY and plans to continue\nher research on adversarial attacks.\nEVGENY BURNAEV received the M.Sc. degree\nfrom the Moscow Institute of Physics and Tech-\nnology, in 2006, and the Ph.D. degree from the\nInstitute for Information Transmission Problems,\nin 2008. He is currently an Associate Professor\nat the Skolkovo Institute of Science and Tech-\nnology, Moscow, Russia. His research interests\ninclude Gaussian processes for multi-ﬁdelity sur-\nrogate modeling and optimization, deep learning\nfor 3D data analysis and manifold learning, and\non-line learning for prediction and anomaly detection.\n17976 VOLUME 10, 2022"
}