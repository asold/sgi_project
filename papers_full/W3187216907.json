{
  "title": "DPT: Deformable Patch-based Transformer for Visual Recognition",
  "url": "https://openalex.org/W3187216907",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2001718419",
      "name": "Chen Zhi-yang",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A3173613335",
      "name": "Zhu, Yousong",
      "affiliations": [
        "Shandong Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2223418435",
      "name": "Zhao, Chaoyang",
      "affiliations": [
        "Shandong Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2628365124",
      "name": "HU Guosheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103026051",
      "name": "Zeng Wei",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2664671475",
      "name": "Wang, Jinqiao",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2108897329",
      "name": "Tang Ming",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shandong Institute of Automation"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2982220924",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2737725206",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2966926453",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W3203974803",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3092739317",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3172509117"
  ],
  "abstract": "Transformer has achieved great success in computer vision, while how to split patches in an image remains a problem. Existing methods usually use a fixed-size patch embedding which might destroy the semantics of objects. To address this problem, we propose a new Deformable Patch (DePatch) module which learns to adaptively split the images into patches with different positions and scales in a data-driven way rather than using predefined fixed patches. In this way, our method can well preserve the semantics in patches. The DePatch module can work as a plug-and-play module, which can easily be incorporated into different transformers to achieve an end-to-end training. We term this DePatch-embedded transformer as Deformable Patch-based Transformer (DPT) and conduct extensive evaluations of DPT on image classification and object detection. Results show DPT can achieve 81.9% top-1 accuracy on ImageNet classification, and 43.7% box mAP with RetinaNet, 44.3% with Mask R-CNN on MSCOCO object detection. Code has been made available at: https://github.com/CASIA-IVA-Lab/DPT .",
  "full_text": "DPT: Deformable Patch-based Transformer for Visual\nRecognition\nZhiyang Chen1,2, Yousong Zhu1, Chaoyang Zhao1âˆ—, Guosheng Hu3, Wei Zeng4, Jinqiao Wang1,2,\nMing Tang1\n1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n3AnyVision, Belfast, UK\n4Peking University, Beijing, China\n{zhiyang.chen,yousong.zhu,chaoyang.chao,jqwang,tangm}@nlpr.ia.ac.cn\nhuguosheng100@gmail.com,weizeng@pku.edu.cn\nABSTRACT\nTransformer has achieved great success in computer vision, while\nhow to split patches in an image remains a problem. Existing meth-\nods usually use a fixed-size patch embedding which might destroy\nthe semantics of objects. To address this problem, we propose a new\nDeformable Patch (DePatch) module which learns to adaptively\nsplit the images into patches with different positions and scales in\na data-driven way rather than using predefined fixed patches. In\nthis way, our method can well preserve the semantics in patches.\nThe DePatch module can work as a plug-and-play module, which\ncan easily be incorporated into different transformers to achieve\nan end-to-end training. We term this DePatch-embedded trans-\nformer as Deformable Patch-based Transformer (DPT) and conduct\nextensive evaluations of DPT on image classification and object\ndetection. Results show DPT can achieve 81.9% top-1 accuracy on\nImageNet classification, and 43.7% box mAP with RetinaNet, 44.3%\nwith Mask R-CNN on MSCOCO object detection. Code has been\nmade available at: https://github.com/CASIA-IVA-Lab/DPT.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Computer vision; Image rep-\nresentations; Object recognition ; Object detection .\nKEYWORDS\nvision transformer; deformable patch; image classification; object\ndetection\nACM Reference Format:\nZhiyang Chen1,2, Yousong Zhu1, Chaoyang Zhao1âˆ—, Guosheng Hu3, Wei\nZeng4, Jinqiao Wang1,2, Ming Tang1. 2021. DPT: Deformable Patch-based\nTransformer for Visual Recognition. In Proceedings of the 29th ACM Inter-\nnational Conference on Multimedia (MM â€™21), October 20â€“24, 2021, Virtual\nEvent, China. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/\n3474085.3475467\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM â€™21, October 20â€“24, 2021, Virtual Event, China\nÂ© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8651-7/21/10. . . $15.00\nhttps://doi.org/10.1145/3474085.3475467\ntail claw claw head\n(a) Original patch embedding module.\ntail claw claw head\n(b) DePatch.\nFigure 1: An example of vanilla patch splitting and our de-\nformable way. (a) Original patch embedding module divides\nthe image in a fixed way. It sometimes destroys the seman-\ntics of objects. (b) Our DePatch splits the image into patches\nin a deformable way with learnable positions and scales.\n(Better viewed in color)\n1 INTRODUCTION\nRecently, transformer [25] has made significant progress in natual\nlanguage process (NLP) and speech recognition. It has gradually be-\ncome the prevailing method for sequence modeling tasks. Inspired\nby this, some studies have successfully applied transformer in com-\nputer vision, and achieved promising performance on image classifi-\ncation [17, 24], object detection [2, 35], and semantic segmentation\n[33]. Similar to NLP, transformer usually divides the input image\ninto a sequence of fixed-size patches (e.g. 16 Ã—16) [7, 24, 26, 30],\nand models the context relationships between different patches\nthrough multi-head self-attention. Compared to convolution neural\nnetworks (CNNs), transformer can effectively capture long-range\ndependency inside the sequence, and the features extracted contain\nmore semantic information.\nAlthough transformer has tasted sweetness in vision tasks, there\nare still many aspects to be improved. DeiT [24] exploits data aug-\nmentation and knowledge distillation to learn visual transformer in\nâˆ—Corresponding author.\narXiv:2107.14467v1  [cs.CV]  30 Jul 2021\na data-efficient manner. T2T-ViT [30] decomposes the patch embed-\nding module by recursively aggregating neighboring tokens for bet-\nter local representation. TNT [10] maintains a fine-grained branch\nto better model local details inside a patch. PVT [ 26] transforms\nthe architecture into four stages, and generates feature pyramid for\ndense prediction. These works use a fixed-size patch embedding\nunder an implicit assumption that the fixed image split design is\nsuitable for all images. However, such â€˜hardâ€™ patch split may bring\ntwo problems: (1) Collapse of local structures in an image. As shown\nin Figure 1(a), a regular patch (16 Ã—16) is always hard to capture\nthe complete object-related local structure, since objects are with\nvarious scales in different images. (2) Semantic inconsistency across\nimages. The same object in different images might have different\ngeometric variations (scale, rotation, etc). The fixed way of splitting\nimages will potentially capture inconsistent information for one\nobject in different images. As discussed, these fixed patches can\npotentially destroy the semantic information, leading to degraded\nperformance.\nTo address the aforementioned problems, in this paper, we pro-\npose a new module, called DePatch, which divides images in a\ndeformable way. In this way, we can well preserve the semantics\nin one patch, reducing the semantics destruction caused by im-\nage splitting. To achieve this, we learn the offset and scale of each\npatch in feature map space. The offset and scale are learned based\non the input feature map and are generated for each patch as il-\nlustrated in Figure 1(b). The proposed module is lightweight and\nintroduces a very small number of parameters and computations.\nMore importantly, it can work as a plug-and-play module which can\neasily be incorporated into other transformer architectures. A trans-\nformer with DePatch module is named Deformable Patch-based\nTransformer, DPT. In this work, we integrate DePatch module to\nPyramid Vision Transformer (PVT) [26] to verify its efficacy since\nPVT achieves state-of-the-art performance in pixel-level prediction\ntasks like object detection and semantic segmentation. With de-\nformable patch adjustment, DPT generates complete, robust and\ndiscriminative features for each patch based on local contextual\nstructures. Therefore it can not only achieve high performance on\nclassification tasks, but also outperform other methods on tasks\nwhich highly depend on local features, e.g. object detection. Our\nmethod achieves 2.3% improvements on ImageNet classification,\nand improves box mAP by 2.8%/3.5% with RetinaNet and Mask\nR-CNN framework for MSCOCO object detection compared to its\nconterpart, PVT-Tiny.\nOur main contributions can be summarized as:\nâ€¢We introduce a new adaptive patch embedding module, De-\nPatch. DePatch can adjust the position and scale of each\npatch based on the input image, and effectively preserve se-\nmantics in one patch, reducing semantics destruction caused\nby image splitting.\nâ€¢Our DePatch is lightweight and can work as a plug-and-play\nmodule integrated into different transformers, leading to a\nDeformable Patch-based Transformer (DPT). In this work,\nwe incorporate DePatch into PVT to verify the efficacy of\nDPT.\nâ€¢We conduct extensive experiments on image classification\nand object detection. For example, our module improves\ntop-1 accuracy by 2.3% on ImageNet classification, and also\ngains 2.8%/3.5% improvements for both RetinaNet and Mask\nR-CNN detectors under the tiny configuration.\n2 RELATED WORK\n2.1 Vision Transformer\nTransformer [25] has been the mainstream approach for NLP tasks.\nIt uses self-attention to capture long-range dependence within the\nwhole sequence, and achieves state-of-the-art performance. This\nidea is applied into computer vision firstly by Non-Local block and\nits variants [1, 14, 27]. Recently, there appear a large amount of\nworks building pure vision transformers without convolution layers.\nViT [7] is as far as we know the first work in this trend. It achieves\ncomparable results with traditional CNN architectures with the help\nof large training data. DeiT [24] uses complex training schedules\nand knowledge distillation to improve performance trained on\nImageNet only.\nCurrent works focus on combining the advantages of transformer\nand CNN in order to capture better local information. This object\nis obtained by combining convolution blocks and self-attention\nlayers together [3, 22], maintaining high-resolution feature maps\n[10, 17, 26], adding parameters biased for locality [ 6, 9] or re-\ndesigning the brute-force patch-embedding module [30]. Though\nlarge improvements achieved, most architectures split the input\nimage with a fixed pattern, without awareness of the input content\nand geometric variations. Our DPT can modify the position and\nscale of each patch in an adaptive way. To the best of our knowledge,\nour model is the first vision transformer that do patch splitting in a\ndata-specific way.\n2.2 Deformable-Related Work\nModifying fixed pattern into an adaptive way is a common idea to\nimprove performance. There have been a great number of works\nhelp models focus on important features and adopt geometric varia-\ntions in computer vision. All related works fall into two categories,\nattention-based methods [1, 13, 14, 27] and offset-based methods\n[5, 8, 28, 34, 35]. We mainly review offset-based ones.\nOffset-based methods predict offsets to explicitly direct impor-\ntant locations. This idea bears some similarity with region proposal\nnetwork in object detection [11, 20]. Unlike our task, region pro-\nposal network uses supervision of bounding box annotations. In\nimage classification task, there are also some works explicitly learn-\ning positions of the important regions for better performance [8]\nor faster inference [28]. The learning process are merely guided\nby cross-entropy loss and final accuracy. Deformable convolution\n[5, 34] is the work most similar to ours. It predicts an offset for each\npixel of the convolution kernel, while the predicted regions in our\nmethod are more regular ones. Irregular patches are not compatible\nin vision transformers. Deformable-DETR [35] applies deformable\noperations in the self-attention layers and cross-attention layers\nof DETR. However, its main purpose is to accelerate training, and\nDeformable-DETR still relies on feature maps extracted from CNNs.\nAs far as we know, our work is the first to apply deformable op-\nerations in a pure vision transformer architecture. We focus on\nadjusting the position and scale of each patch, therefore extracting\nfeatures better maintaining local structures. Our module can work\nas a plug-and-play module, and is compatible for various vision\ntransformer architectures.\n3 METHOD\n3.1 Preliminaries: Vision Transformer\nVision transformer is composed of three parts, a patch embedding\nmodule, multi-head self-attention blocks and feed-forward multi-\nlayer perceptrons (MLP). The network starts with the patch embed-\nding module which transforms the input image into a sequence of\ntokens, and then alternately stacks multi-head self-attention blocks\nand MLPs to obtain the final representation. We mainly elaborate\non the patch embedding module in this section, and then have a\nquick review over the multi-head self-attention.\nPatch embedding module divides images into patches with fixed\nsize and positions, and embeds each of them with a linear layer. We\ndenote the input image or feature map as ğ‘¨ âˆˆRğ»Ã—ğ‘ŠÃ—ğ¶. For sim-\nplicity, we assume ğ» = ğ‘Š. Previous works split ğ‘¨ into a sequence\nof ğ‘ patches with sizeğ‘ Ã—ğ‘ (ğ‘  = âŒˆğ»/\nâˆš\nğ‘âŒ‰). The sequence is denoted\nas {ğ’›(ğ‘–)}1â‰¤ğ‘–â‰¤ğ‘.\nTo better interpret the patch splitting process, we reformulate the\npatch embedding module. Each patchğ’›(ğ‘–)can be seen as a represen-\ntation for a rectangle region of the input image. We denote its center\ncoordinate as (ğ‘¥(ğ‘–)\nğ‘ğ‘¡ ,ğ‘¦(ğ‘–)\nğ‘ğ‘¡ ). Since patch size is fixed, the left-top cor-\nner and right-bottom corner are determined as(ğ‘¥(ğ‘–)\nğ‘ğ‘¡ âˆ’ğ‘ /2,ğ‘¦(ğ‘–)\nğ‘ğ‘¡ âˆ’ğ‘ /2)\nand (ğ‘¥(ğ‘–)\nğ‘ğ‘¡ +ğ‘ /2,ğ‘¦(ğ‘–)\nğ‘ğ‘¡ +ğ‘ /2). There are ğ‘ Ã—ğ‘ pixels inside this region,\ntheir coordinates are represented by Â®ğ’‘(ğ‘–,ğ‘—)= (ğ‘(ğ‘–,ğ‘—)\nğ‘¥ ,ğ‘(ğ‘–,ğ‘—)\nğ‘¦ ). All co-\nordinates Â®ğ’‘(ğ‘–,ğ‘—)are integers themselves. The features at these pixels\nare denoted as {Ëœğ’‚(ğ‘–,ğ‘—)}1â‰¤ğ‘—â‰¤ğ‘. These features are then flattened and\nprocessed by a linear layer to get representation for the new patch,\nas shown in Eq. (1).\nğ’›(ğ‘–)= ğ‘¾ğ‘ğ‘ğ‘¡ğ‘â„ Â·ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡{Ëœğ’‚(ğ‘–,1),..., Ëœğ’‚(ğ‘–,ğ‘ Ã—ğ‘ )}+ğ’ƒğ‘ğ‘ğ‘¡ğ‘â„ (1)\nMulti-head self-attention module aggregates relative informa-\ntion over the whole input sequence, giving each token a global\nview. This module learns three groups of representative features\nfor each head, query ( ğ‘¸â„ âˆˆRğ‘Ã—ğ‘‘), key (ğ‘²â„ âˆˆRğ‘Ã—ğ‘‘) and value\n(ğ‘½â„ âˆˆRğ‘Ã—ğ‘‘). ğ‘¸â„ and ğ‘²â„ are multiplied to obtain the attention map\nğ‘¨ğ’•ğ’•ğ’ â„, which represents similarity between different patches. The\nattention map is used as the weights to sum up ğ‘½â„. Independent re-\nsults are calculated for different heads to get more variant features.\nResults from all heads are then concatenated and transformed to\nbecome new representations ğ’ â€².\nğ‘¨ğ’•ğ’•ğ’ â„ = ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘¸â„ğ‘²ğ‘‡\nâ„/\nâˆš\nğ‘‘) (2)\nğ’ â€²= ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡{ğ‘¨ğ’•ğ’•ğ’ 1ğ‘½1,..., ğ‘¨ğ’•ğ’•ğ’ ğ»ğ‘½ğ»}ğ‘¾ğ‘ğ‘Ÿğ‘œğ‘— +ğ’ƒğ‘ğ‘Ÿğ‘œğ‘— (3)\n3.2 DePatch Module\nThe patch embedding process described in 3.1 is fixed and inflexible.\nPositions (ğ‘¥(ğ‘–)\nğ‘ğ‘¡ ,ğ‘¦(ğ‘–)\nğ‘ğ‘¡ )and size ğ‘  are fixed, therefore the rectangle\nregion is unchangable for each patch. The feature for each patch is\ndirectly represented with its inside pixels. In order to better locate\nimportant structures and handle geometric deformation, we loosen\nthese constraints to develop our deformable patch embedding mod-\nule, DePatch.\nflatten\nLinear Layer\n(a) Vanilla patch embedding module in PVT\nLinear Layer\nğ›¿ğ›¿ğ›¿ğ›¿, ğ›¿ğ›¿ğ›¿ğ›¿\nğ‘¤ğ‘¤\nâ„\nprediction\nflatten\nsample ğ‘˜ğ‘˜ Ã— ğ‘˜ğ‘˜ points\noffsets scales\n(b) DePatch module ( ğ‘˜ = 3)\nFigure 2: DePatch module instruction. Offsets and scales are\npredicted with local features, and new embeddings are ob-\ntained by bilinear interpolation.\nFirstly, we turn the location and the scale of each patch into\npredicted parameters based on input contents. As for the location,\nwe predict an offset (ğ›¿ğ‘¥,ğ›¿ğ‘¦ )allowing it to shift around the original\ncenter. As for the scale, we simply replace the fixed patch size ğ‘ \nwith predictable ğ‘ â„ and ğ‘ ğ‘¤. In this way, we can determine a new\nrectangle region, and denote its left-top corner as(ğ‘¥1,ğ‘¦1)and right-\nbottom corner as (ğ‘¥2,ğ‘¦2). For clarity, we omit the superscript (ğ‘–).\nWe emphasize that (ğ›¿ğ‘¥,ğ›¿ğ‘¦,ğ‘  ğ‘¤,ğ‘ â„)can be different even for patches\nin a single image.\nğ‘¥1 = ğ‘¥ğ‘ğ‘¡ +ğ›¿ğ‘¥ âˆ’ğ‘ ğ‘¤\n2 , ğ‘¦1 = ğ‘¦ğ‘ğ‘¡ +ğ›¿ğ‘¦âˆ’ğ‘ â„\n2\nğ‘¥2 = ğ‘¥ğ‘ğ‘¡ +ğ›¿ğ‘¥ +ğ‘ ğ‘¤\n2 , ğ‘¦2 = ğ‘¦ğ‘ğ‘¡ +ğ›¿ğ‘¦+ğ‘ â„\n2\n(4)\nAs shown in Figure 2, we add a new branch to predict these pa-\nrameters. Based on the input feature map, we predict(ğ›¿ğ‘¥,ğ›¿ğ‘¦,ğ‘  ğ‘¤,ğ‘ â„)\ndensely for all patches first, and then embed them with predicted\nregions. Offsets and scales are predicted with Eq. (5) and (6). ğ’‡ğ‘(Â·)\ncan be any feature extractor, and here we use a single linear layer.\nAfter that, ğ‘¾ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ and ğ‘¾ğ‘ ğ‘ğ‘ğ‘™ğ‘’ are followed to predict offsets and\nscales. At the beginning of training, these weights are initialized to\nzero. ğ’ƒğ‘ ğ‘ğ‘ğ‘™ğ‘’ is initialized to make sure each patch focuses on exactly\nthe same rectangle region as the original model.\nğœ¹ğ’™, ğœ¹ğ’š = ğ‘‡ğ‘ğ‘›â„(ğ‘¾ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ Â·ğ’‡ğ‘(ğ‘¨)) (5)\nğ’”ğ‘¤,ğ’”â„ = ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘‡ğ‘ğ‘›â„(ğ‘¾ğ‘ ğ‘ğ‘ğ‘™ğ‘’ Â·ğ’‡ğ‘(ğ‘¨)+ğ’ƒğ‘ ğ‘ğ‘ğ‘™ğ‘’)) (6)\nAfter the rectangle region is determined, we extract the feature\nfor each patch. The main problem is that regions are with different\nsizes, and the predicted coordinates are usually fractional. We solve\nthis problem in a sampling-and-interpolation manner. Given the\nrectangle coordinates (ğ‘¥1,ğ‘¦1)and (ğ‘¥2,ğ‘¦2), we sample ğ‘˜Ã—ğ‘˜ points\nuniformly inside the region, ğ‘˜is a super-parameter for our method,\nwhich will be ablated in 4.3. Each sampling location is denoted\nas Â®ğ’‘(ğ‘—) = (ğ‘(ğ‘—)\nğ‘¥ ,ğ‘(ğ‘—)\nğ‘¦ )for any 1 â‰¤ğ‘— â‰¤ğ‘˜ Ã—ğ‘˜. The features of all\nPatch Emb\nEncoder\nStage 1\nPatch Emb\nEncoder\nStage 2\nPatch Emb\nEncoder\nStage 3\nPatch Emb\nEncoder\nStage 4\nPatch Emb\nEncoder\nStage 1\nDePatch\nEncoder\nStage 2\nDePatch\nEncoder\nStage 3\nDePatch\nEncoder\nStage 4\nğ¹ğ¹4: ğ»ğ»\n32 Ã— ğ‘Šğ‘Š\n32 Ã— ğ¶ğ¶4\nğ¹ğ¹3: ğ»ğ»\n16 Ã— ğ‘Šğ‘Š\n16 Ã— ğ¶ğ¶3\nğ¹ğ¹1: ğ»ğ»\n4 Ã— ğ‘Šğ‘Š\n4 Ã— ğ¶ğ¶1\nğ¹ğ¹2: ğ»ğ»\n8 Ã— ğ‘Šğ‘Š\n8 Ã— ğ¶ğ¶2\nFigure 3: Left: Original PVT architecture. Right: DPT,\nEquipped with our DePatch module.\nsampled points {Ëœğ’‚(ğ‘—)}1â‰¤ğ‘—â‰¤ğ‘˜Ã—ğ‘˜ are then flattened and processed in\na linear layer to generate patch embedding, as in Eq. (7).\nğ’›(ğ‘–)= ğ‘¾ Â·ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡{Ëœğ’‚(1),..., Ëœğ’‚(ğ‘˜Ã—ğ‘˜)}+ğ’ƒ (7)\nThe index of sampled points is mostly fractional. Assume that\nwe intend to extract feature at point (ğ‘ğ‘¥,ğ‘ğ‘¦). Its corresponding\nfeature is obtained via bilinear interpolation as\nğ‘¨(ğ‘ğ‘¥,ğ‘ğ‘¦)=\nâˆ‘ï¸\nğ‘ğ‘¥ ,ğ‘ğ‘¦\nğº(ğ‘ğ‘¥,ğ‘ğ‘¦;ğ‘ğ‘¥,ğ‘ğ‘¦)Â· ğ‘¨(ğ‘ğ‘¥,ğ‘ğ‘¦) (8)\nğº(ğ‘ğ‘¥,ğ‘ğ‘¦;ğ‘ğ‘¥,ğ‘ğ‘¦)= ğ‘šğ‘ğ‘¥(0,1âˆ’|ğ‘ğ‘¥âˆ’ğ‘ğ‘¥|)Â·ğ‘šğ‘ğ‘¥(0,1âˆ’|ğ‘ğ‘¦âˆ’ğ‘ğ‘¦|) (9)\nIn Eq. (8), ğº(Â·)is the bilinear interpolation kernel all over the\nintegral spatial locations. It only becomes non-zero at the four\nlocations close to (ğ‘ğ‘¥,ğ‘ğ‘¦). Therefore, it can be computed quickly\nwith few multiply-adds.\n3.3 Overall Architecture\nDePatch is a self-adaptive module to change positions and scales\nof patches. As DePatch can work as a plug-and-play module, we\ncan easily incorporate DePatch into various vision transformers.\nBecause of the superiority and generality, we choose PVT as our\nbase model. PVT has four stages with feature maps of decreasing\nscales. It utilizes spatial-reduction attention to reduce cost on high\nresolution feature maps. For detailed information please refer to\n[26]. Our model is denoted as DPT. It is built by replacing the\npatch embedding modules at the beginning of stage 2, 3 and 4\nwith DePatch, while keeping other configurations unchanged. The\noverall architecture is shown in Figure 3.\n4 EXPERIMENTS\nIn this section, we conduct image classification experiments on\nImageNet [21] and object detection experiments on COCO [ 16].\nAfter that, some ablation studies are provided then for further\nanalysis.\n4.1 Image Classification\nExperiment Settings We use ImageNet [21] for image classifica-\ntion experiments. The ImageNet dataset consists of 1.28M images\nfor training and 50K for validation. These images belong to 1000\ncategories. We report top-1 error on validation set for compari-\nson. The images are resized into 256 Ã—256 and randomly cropped\ninto 224 Ã—224 for training. Advanced data augmentation methods\nincluding Mixup [32], CutMix [31], label smoothing [23] and Rand-\nAugment [4] are utilized. Our models are trained with the batch\nsize of 1024 for 300 epochs and optimized by AdamW [ 19] with\ninitial learning rate of 1 Ã—10âˆ’3 and cosine schedule [18]. Weight\ndecay is set to 0.05 for non-bias parameters. All these settings keep\nthe same with original PVT [26] for fair comparison.\nResults As shown in Table 1, our smallest DPT-Tiny achieves\n77.4% top-1 accuracy, which is 2.3% higher than the corresponding\nbaseline PVT model. The best result is achieved by our medium\none. It achieves 81.9% top-1 accuracy, and even outperforms models\nwith much larger costs like PVT-Large and catch up with DeiT-Base.\nAs for CNN-based models, DPT-Small outperforms the popular\nResNet50 by 4.9%. Our models achieve better results than both\nCNN-based and transformer-based models, and outperform them\nby a large margin.\n4.2 Object Detection\nExperiment Settings Our experiments for object detection are\nconducted on COCO [16], a large-scale detection benchmark. We\nset train2017 split with 118K images as our training set, and val2017\nsplit with 5K images for validation. Mean Average Precision (mAP)\nis used as our evaluation metric. Following [26], we evaluate our\nDPT backbones on three prevailing frameworks, RetinaNet [ 15],\nMask R-CNN [ 11] and DETR [ 2]. We load ImageNet pretrained\nweights to initialize the backbone. Our models are trained with the\nbatch size of 16 and optimized by AdamW [19] with initial learning\nrate 1 Ã—10âˆ’4. As to RetinaNet and Mask R-CNN, we report results\nwith both 1x and multi-scale 3x train schedules (12 and 36 epochs).\nFor 1x schedule, images are resized so that the shorter edge has\n800 pixels and the longer edge does not exceed 1333 pixels. For\nmulti-scale training, the shorter edge is resized within the range of\n[640, 800]. As for DETR, the model is trained for 50 epochs with\nrandom flip and random scale.\nResults We compare DPT to PVT [26] and standard ResNe(X)t\n[12, 29]. The comparison is shown in Table 2. As for RetinaNet, DPT-\nSmall significantly outperforms PVT-Small by 2.1% and Resnet50\nTable 1: Results on ImageNet Classification.\nMethod #Param (M) FLOPs (G) Top-1 Acc(%)\nResNet18 [12] 11.7 1.8 69.8\nDeiT-Tiny [24] 5.7 1.3 72.2\nPVT-Tiny [26] 13.2 1.9 75.1\nDPT-Tiny (ours) 15.2 2.1 77.4\nResNet50 [12] 25.6 4.1 76.1\nDeiT-Small [24] 22.1 4.6 79.9\nT2T-ViT-14 [30] 21.4 5.2 80.6\nPVT-Small [26] 24.5 3.8 79.8\nDPT-Small (ours) 26.4 4.0 81.0\nResNet101 [12] 44.7 7.9 77.4\nX101-32x4d [29] 44.2 8.0 78.8\nX101-64x4d [29] 83.5 15.6 79.6\nViT-Base [7] 86.6 17.6 77.9\nDeiT-Base [24] 86.6 17.6 81.8\nT2T-ViT-19 [30] 39.0 8.0 81.2\nPVT-Medium [26] 44.2 6.7 81.2\nPVT-Large [26] 61.4 9.8 81.7\nDPT-Medium (ours) 46.1 6.9 81.9\nby 6.2% mAP at a comparable computational cost, which indicates\nthat DPT provides more discriminative features for target objects\nin images. With our DePatch module, each patch is aware of its\nneighboring content, and extracts crucial information needed for\ndifferent locations. Moreover, with 3Ã—training schedule and multi-\nscale training, RetinaNet+DPT-Medium achieves 43.7% mAP. It\noutperforms PVT-Medium and ResNet101 by a large margin, and\neven achieves better performance than PVT-Large model, but with\nmore than 20% cost reduced.\nResults for Mask R-CNN are similar. Our DPT-Small model\nachieves 43.1% box mAP and 39.9% mask mAP under 1Ã—schedule,\noutperforming PVT-Small by 2.7% and 2.1%. With3Ã—training sched-\nule and multi-scale training, Mask R-CNN+DPT-Small achieves the\nbest result with 44.4% box mAP and 41.0% mask mAP.\nDETR is a latest framework for object detection. It requires a\nlong trained schedule (e.g. 500 epochs). We only validate our model\nwith a shorter schedule (50 epochs). According to Table 4, our DPT-\nSmall achieves 37.7% box mAP, outperforming PVT-Small by 3.0%\nand ResNet50 by 5.4%. Therefore we conclude that DPT is also\ncompatible with transformer-based detectors.\n4.3 Ablation Studies\nEffect of module position There are four patch embedding mod-\nules in PVT. The first directly operates on the input image, and\nthe rest are inserted at the beginning of the following stages. We\nperform detailed experiments to illustrate where we should add\nDePatch.\nSince raw images contain little semantic information, it is diffi-\ncult for the first module to predict offsets and scales beyond its own\nregion. Therefore, we only attempt to replace the rest three patch\nembedding modules. The results are shown in Table 5. The improve-\nments obtained by stage 2, 3 and 4 are 0.3%, 1.0%, and 1.5%. The more\nstage2\n2 Ã— 2 points\n4 Ã— 4 points\n3 Ã— 3 points\n(a) Scale distribution at stage 2\nstage3\n2 Ã— 2 points\n4 Ã— 4 points\n3 Ã— 3 points\n(b) Scale distribution at stage 3\n2 Ã— 2 points\n4 Ã— 4 points\n3 Ã— 3 points\n(c) Scale distribution at stage 4\nFigure 4: Region scale learned by DPT-Tiny with different\nnumber of sampling points ğ‘˜ Ã—ğ‘˜. We illustrate statistics in\nstage 2, 3 and 4. Scale is measured by the edge size of the\nregion.\npatch embedding modules we replace, larger improvement it brings.\nAccording to the results, replacing all patch embedding modules\nin stage 2, 3 and 4 achieves the best performance. It outperforms\nbaseline PVT model by 1.5%, with only 0.86M parameters increased.\nIn the following experiments, it will be our default configuration\nto replace all three patch embedding modules.\nEffect of number of sampling points We experiment to show\nhow many points we should sample in one predicted region. Sam-\npling more points slightly increases FLOPs, but also has a stronger\nlearning ability to capture features from a larger region. The results\nare shown in Table 6. Increasing sampling points from 2 Ã—2 to\n3 Ã—3 provides another 0.8% improvement, while further increas-\ning it to 4 Ã—4 only improves by 0.2%. Since sampling 4 Ã—4 points\nonly gets marginal improvement. We take theğ‘˜ = 3 as the default\nconfiguration in following experiments.\nWe claim that sampling more points will benefits DPT with\nstronger ability to extract features from larger area. We show how\nthe distributions of predicted scales change during training with dif-\nferent number of sampling points in Figure 4. Although we initialize\nthe region scales strictly the same as that in PVT (ğ‘ğ‘ğ‘¡ğ‘â„_ğ‘ ğ‘–ğ‘§ğ‘’ = 2),\nDePatch can learn to expand on its own. This phenomenon accords\nto the common sense in CNN, that enlarging the receptive field\nTable 2: Object detection performance on MS COCO (RetinaNet)\nRetinaNet 1Ã— RetinaNet 3Ã—+ MS\nBackbone #Param(M) ğ‘šğ´ğ‘ƒ ğ´ğ‘ƒ 50 ğ´ğ‘ƒ75 ğ´ğ‘ƒğ‘† ğ´ğ‘ƒğ‘€ ğ´ğ‘ƒğ¿ ğ´ğ‘ƒ ğ´ğ‘ƒ 50 ğ´ğ‘ƒ75 ğ´ğ‘ƒğ‘† ğ´ğ‘ƒğ‘€ ğ´ğ‘ƒğ¿\nResNet18 [12] 21.3 31.8 49.6 33.6 16.3 34.3 43.2 35.4 53.9 37.6 19.5 38.2 46.8\nPVT-Tiny [26] 23.0 36.7 56.9 38.9 22.6 38.8 50.0 39.4 59.8 42.0 25.5 42.0 52.1\nDPT-Tiny (ours) 24.9 39.5 60.4 41.8 23.7 43.2 52.2 41.2 62.0 44.0 25.7 44.6 53.9\nResNet50 [12] 37.7 36.3 55.3 38.6 19.3 40.0 48.8 39.0 58.4 41.8 22.4 42.8 51.6\nPVT-Small [26] 34.2 40.4 61.3 43.0 25.0 42.9 55.7 42.2 62.7 45.0 26.2 45.2 57.2\nDPT-Small (ours) 36.1 42.5 63.6 45.3 26.2 45.7 56.9 43.3 64.0 46.5 27.8 46.3 58.5\nResNet101 [12] 56.7 38.5 57.8 41.2 21.4 42.6 51.1 40.9 60.1 44.0 23.7 45.0 53.8\nResNeXt101-32x4d [29] 56.4 39.9 59.6 42.7 22.3 44.2 52.5 41.4 61.0 44.3 23.9 45.5 53.7\nResNeXt101-64x4d [29] 95.5 41.0 60.9 44.0 23.9 45.2 54.0 41.8 61.5 44.4 25.2 45.4 54.6\nPVT-Medium [26] 53.9 41.9 63.1 44.3 25.0 44.9 57.6 43.2 63.8 46.1 27.3 46.3 58.9\nPVT-Large [26] 71.1 42.6 63.7 45.4 25.8 46.0 58.4 43.4 63.6 46.1 26.1 46.0 59.5\nDPT-Medium (ours) 55.9 43.3 64.6 45.9 27.2 46.7 58.6 43.7 64.6 46.4 27.2 47.0 58.4\nTable 3: Object detection performance on MS COCO (Mask R-CNN)\nMask R-CNN 1Ã— Mask R-CNN 3Ã—+ MS\nBackbone #Param(M) ğ‘šğ´ğ‘ƒğ‘ ğ´ğ‘ƒğ‘\n50 ğ´ğ‘ƒğ‘\n75 ğ‘šğ´ğ‘ƒğ‘š ğ´ğ‘ƒğ‘š\n50 ğ´ğ‘ƒğ‘š\n75 ğ‘šğ´ğ‘ƒğ‘ ğ´ğ‘ƒğ‘\n50 ğ´ğ‘ƒğ‘\n75 ğ‘šğ´ğ‘ƒğ‘š ğ´ğ‘ƒğ‘š\n50 ğ´ğ‘ƒğ‘š\n75\nResNet18 [12] 31.2 34.0 54.0 36.7 31.2 51.0 32.7 36.9 57.1 40.0 33.6 53.9 35.7\nPVT-Tiny [26] 32.9 36.7 59.2 39.3 35.1 56.7 37.3 39.8 62.2 43.0 37.4 59.3 39.9\nDPT-Tiny (ours) 34.8 40.2 62.8 43.8 37.7 59.8 40.4 42.2 64.4 46.1 39.4 61.5 42.3\nResNet50 [12] 44.2 38.0 58.6 41.4 34.4 55.1 36.7 41.0 61.7 44.9 37.1 58.4 40.1\nPVT-Small [26] 44.1 40.4 62.9 43.8 37.8 60.1 40.3 43.0 65.3 46.9 39.9 62.5 42.8\nDPT-Small (ours) 46.1 43.1 65.7 47.2 39.9 62.9 43.0 44.4 66.5 48.9 41.0 63.6 44.2\nResNet101 [12] 63.2 40.4 61.1 44.2 36.4 57.7 38.8 42.8 63.2 47.1 38.5 60.1 41.3\nResNeXt101-32x4d [29] 62.8 41.9 62.5 45.9 37.5 59.4 40.2 44.0 64.4 48.0 39.2 61.4 41.9\nResNeXt101-64x4d [29] 101.9 42.8 63.8 47.3 38.4 60.6 41.3 44.4 64.9 48.8 39.7 61.9 42.6\nPVT-Medium [26] 63.9 42.0 64.4 45.6 39.0 61.6 42.1 44.2 66.0 48.2 40.5 63.1 43.5\nPVT-Large [26] 81.0 42.9 65.0 46.6 39.5 61.9 42.5 44.5 66.0 48.3 40.7 63.4 43.7\nDPT-Medium (ours) 65.8 43.8 66.2 48.3 40.3 63.1 43.4 44.3 65.6 48.8 40.7 63.1 44.1\nTable 4: Object detection performance on MS COCO (DETR\nwith 50 epochs)\nBackbone ğ‘šğ´ğ‘ƒ ğ´ğ‘ƒ 50 ğ´ğ‘ƒ75 ğ´ğ‘ƒğ‘† ğ´ğ‘ƒğ‘€ ğ´ğ‘ƒğ¿\nResNet50 [12] 32.3 53.9 32.3 10.7 33.8 53.0\nPVT-Small [26] 34.7 55.7 35.4 12.0 36.4 56.7\nDPT-Small (ours) 37.7 59.2 38.8 15.0 40.3 58.5\nTable 5: Effect of module position ( ğ‘˜ = 2)\nStage 2 Stage 3 Stage 4 #Params(M) Top-1 Acc(%)\n13.23 75.1\nâœ“ 13.26 75.4\nâœ“ âœ“ 13.43 76.1\nâœ“ âœ“ âœ“ 14.09 76.6\nTable 6: Effect of number of sampling points\nSampling points #Params(M) Flops(G) Top-1 Acc(%)\nBaseline 13.23 1.94 75.1\n2 Ã—2 14.09 2.03 76.6\n3 Ã—3 15.15 2.14 77.4\n4 Ã—4 16.64 2.30 77.6\nTable 7: Decouple of predicting offsets and scales\nOffsets Scales Top-1 Acc(%)\n75.1\nâœ“ 76.6\nâœ“ âœ“ 77.4\nTable 8: Top-1 accuracy (%) with short training schedule\nMethod 150 epochs 300 epochs\nPVT-Tiny [26] 73.1 75.1\nDPT-Tiny (Ours) 76.2 77.4\n(a)\n (b)\nFigure 5: Training curve for both DPT-Tiny and PVT-Tiny.\nbenefits the model. The distributions of scales for ğ‘˜ = 2 converge\nearly with low variance. Sampling2Ã—2 points is unable to represent\nany larger regions, hence it limits the capacity of the our module\nfor understanding images with heavy geometric deformation. The\nstatistics do not differ much for ğ‘˜ = 3 and 4 except in stage 2. We\nassume that sampling more points will achieve even better per-\nformance, while it is not worth additional cost. Designing a more\nsophisticated spatial pyramid cound be another way to improve\nour method. We leave it as our future work.\nDecouple offsets and scales DePatch learns both the offset\nand scale for each patch. Offsets are predicted to shift the patches\ntowards more important regions, and scales are for better main-\ntaining local structures. They both facilitates the performance of\nour model. We decouple these two factors in Table 7 in order to\nsee how each single one influences our model. When scales are\nnot predicted, the shape of all rectangle regions is fixed the same\nas patches in original PVT. Only predicting offsets can improve\n1.5% above baseline, and another 0.8% is obtained by predicting\nscales. We claim that both offsets and scales are important for our\nself-adaptive patch embedding module.\nAnalysis for fast convergence DePatch module is able to ad-\njust the patches to a proper shape for each image. Adequate patches\nmaintain important local structures, and features are learned more\nefficiently. Therefore the whole network can learn at a faster speed.\nWe draw the training curve for both our DPT-Tiny and PVT-Tiny\nin Figure 5. The training loss and test accuracy do converge faster\nin first few epochs.\nBased on this phenomenon, we expect that our module can\nalleviate the requirement of long training schedule. We prove it\nby simply reducing training epochs by half. As shown in Table 8,\nDPT-Tiny trained with only 150 epochs outperforms a fully-trained\nPVT-Tiny by 1.1%, and the performance degradation caused by a\nshorter schedule is only 1.2%, which is much smaller than original\nPVT-Tiny. This indicates that our DePatch module can significantly\naccelerate training for vision transformers, which would benefit\nfurther research.\nTable 9: Effect of initialization for ğ‘Šğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ and ğ‘Šğ‘ ğ‘ğ‘ğ‘™ğ‘’\nInitialization Top-1 Acc(%)\nTruncated normal 77.36\nZero init 77.39\nEffect of parameter initialization As stated in 3.2, we initial-\nize ğ‘Šğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ and ğ‘Šğ‘ ğ‘ğ‘ğ‘™ğ‘’ to zero. We shown in Table 9 that initializa-\ntion methods have little impact on final performance. We take zero\ninitialization in the all experiments.\n4.4 Visualization\nWe illustrate offsets learned by DePatch in Figure 6. The visualiza-\ntion shows that patches predicted by DePatch are well-located to\ncapture important features. DePatch has more obvious impacts at\nthe edge of foreground objects. It encourages the patches outside to\nshift a bit towards the object, thus covering more critical areas than\nnormal patches. When there are more than one object appearing\nin the image, patches would adjust their positions to the closest\none (the two whales in Figure 6(b)). This attribute would be more\ncrucial for object detection, since different patches can be more rep-\nresentative for different objects. Therefore, the detector can better\nlocate and classify all the objects with more related features. The\npredicted scales are also influenced by richness of local context,\nsuch as edges or corners. It becomes small when it needs to focus\non subtle details (the beak of the bird in Figure 6(d)), and large if\nmore context is needed (homogenous area of the dogâ€™s stomach\nFigure 6(h)). The high variance of offsets and scales indicates strong\nself-adaptability of our method.\n5 CONCLUSION\nIn this paper, we introduce DePatch, a deformable module to split\npatches. It encourages the network to extract patch information\nfrom object-related regions and make our model insensitive to\ngeometric deformation. This module can work as a plug-and-play\nmodule and improve various vision transformers. We also build a\ntransformer with DePatch module, named Deformable Patch-based\nTransformer, DPT. Extensive experiments on image classification\nand object detection indicate that DPT can extract better features\nand outperform CNN-based models and other vision transformers.\nDePatch can be utilized in other vision transformers as well as other\ndownstream tasks to improve their performance. Our model is the\nfirst work to modify vision transformer in a data-dependant way.\nWe hope our idea could serve as a good starting point for future\nstudies.\nACKNOWLEDGMENTS\nThis work was supported by the Research and Development Projects\nin the Key Areas of Guangdong Province (No.2020B010165001)\nand National Natural Science Foundation of China under Grants\nNo.61772527, No.61976210, No.62002357, No.61876086, No.61806200,\nNo.62002356, and No.61633002.\n(a)\n (b)\n (c)\n (d)\n(e)\n (f)\n (g)\n (h)\nFigure 6: Visualization of learned patches and their offsets with our DPT-Small at stage 4. Our method can adaptively adjust\nthe position and scale for each patch based on the input content.\nREFERENCES\n[1] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. 2019. Gcnet: Non-\nlocal networks meet squeeze-excitation networks and beyond. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision Workshops . 0â€“0.\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In European Conference on Computer Vision . Springer, 213â€“229.\n[3] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. 2021. Do We\nReally Need Explicit Position Encodings for Vision Transformers? arXiv preprint\narXiv:2102.10882 (2021).\n[4] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. 2020. Randaugment:\nPractical automated data augmentation with a reduced search space. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops. 702â€“703.\n[5] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen\nWei. 2017. Deformable convolutional networks. In Proceedings of the IEEE inter-\nnational conference on computer vision . 764â€“773.\n[6] StÃ©phane dâ€™Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli,\nand Levent Sagun. 2021. ConViT: Improving Vision Transformers with Soft\nConvolutional Inductive Biases. arXiv preprint arXiv:2103.10697 (2021).\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[8] Jianlong Fu, Heliang Zheng, and Tao Mei. 2017. Look closer to see better: Recur-\nrent attention convolutional neural network for fine-grained image recognition.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition .\n4438â€“4446.\n[9] Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai, and Hongsheng Li. 2021.\nFast convergence of detr with spatially modulated co-attention. arXiv preprint\narXiv:2101.07448 (2021).\n[10] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.\n2021. Transformer in transformer. arXiv preprint arXiv:2103.00112 (2021).\n[11] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. 2017. Mask r-cnn.\nIn Proceedings of the IEEE international conference on computer vision . 2961â€“2969.\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition . 770â€“778.\n[13] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. InProceed-\nings of the IEEE conference on computer vision and pattern recognition . 7132â€“7141.\n[14] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and\nWenyu Liu. 2019. Ccnet: Criss-cross attention for semantic segmentation. In\nProceedings of the IEEE/CVF International Conference on Computer Vision . 603â€“612.\n[15] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. 2017.\nFocal loss for dense object detection. In Proceedings of the IEEE international\nconference on computer vision . 2980â€“2988.\n[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. Microsoft coco: Common\nobjects in context. In European conference on computer vision . Springer, 740â€“755.\n[17] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer\nusing Shifted Windows. arXiv preprint arXiv:2103.14030 (2021).\n[18] Ilya Loshchilov and Frank Hutter. 2016. Sgdr: Stochastic gradient descent with\nwarm restarts. arXiv preprint arXiv:1608.03983 (2016).\n[19] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\narXiv preprint arXiv:1711.05101 (2017).\n[20] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:\nTowards real-time object detection with region proposal networks.arXiv preprint\narXiv:1506.01497 (2015).\n[21] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.\n2015. Imagenet large scale visual recognition challenge. International journal of\ncomputer vision 115, 3 (2015), 211â€“252.\n[22] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and\nAshish Vaswani. 2021. Bottleneck transformers for visual recognition. arXiv\npreprint arXiv:2101.11605 (2021).\n[23] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew\nWojna. 2016. Rethinking the inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vision and pattern recognition .\n2818â€“2826.\n[24] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, and HervÃ© JÃ©gou. 2020. Training data-efficient image transformers\n& distillation through attention. arXiv preprint arXiv:2012.12877 (2020).\n[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762 (2017).\n[26] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong\nLu, Ping Luo, and Ling Shao. 2021. Pyramid vision transformer: A versatile back-\nbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122\n(2021).\n[27] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local\nneural networks. In Proceedings of the IEEE conference on computer vision and\npattern recognition . 7794â€“7803.\n[28] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, and Gao Huang. 2020.\nGlance and focus: a dynamic approach to reducing spatial redundancy in image\nclassification. arXiv preprint arXiv:2010.05300 (2020).\n[29] Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He. 2017.\nAggregated residual transformations for deep neural networks. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition . 1492â€“1500.\n[30] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. 2021. Tokens-to-token vit: Training vision transformers\nfrom scratch on imagenet. arXiv preprint arXiv:2101.11986 (2021).\n[31] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo. 2019. Cutmix: Regularization strategy to train strong classifiers\nwith localizable features. In Proceedings of the IEEE/CVF International Conference\non Computer Vision . 6023â€“6032.\n[32] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2017.\nmixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412\n(2017).\n[33] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao\nWang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al . 2020. Re-\nthinking Semantic Segmentation from a Sequence-to-Sequence Perspective with\nTransformers. arXiv preprint arXiv:2012.15840 (2020).\n[34] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. 2019. Deformable convnets\nv2: More deformable, better results. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 9308â€“9316.\n[35] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. 2020.\nDeformable DETR: Deformable Transformers for End-to-End Object Detection.\narXiv preprint arXiv:2010.04159 (2020).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7832099199295044
    },
    {
      "name": "Transformer",
      "score": 0.7641615867614746
    },
    {
      "name": "Embedding",
      "score": 0.7350845336914062
    },
    {
      "name": "Artificial intelligence",
      "score": 0.637882649898529
    },
    {
      "name": "Computer vision",
      "score": 0.5909866690635681
    },
    {
      "name": "Object detection",
      "score": 0.4507666230201721
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3625950813293457
    },
    {
      "name": "Engineering",
      "score": 0.07288137078285217
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210112150",
      "name": "Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210094879",
      "name": "Shandong Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ],
  "cited_by": 99
}