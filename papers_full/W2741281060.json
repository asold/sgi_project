{
    "title": "Compressed Nonparametric Language Modelling",
    "url": "https://openalex.org/W2741281060",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A5086032589",
            "name": "Ehsan Shareghi",
            "affiliations": [
                "Australian Regenerative Medicine Institute",
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A5081525024",
            "name": "Gholamreza Haffari",
            "affiliations": [
                "Australian Regenerative Medicine Institute",
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A5078530959",
            "name": "Trevor Cohn",
            "affiliations": [
                "The University of Melbourne"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963766645",
        "https://openalex.org/W22168010",
        "https://openalex.org/W2044905690",
        "https://openalex.org/W2161488606",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W2250653840",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W2158266063",
        "https://openalex.org/W2952380754",
        "https://openalex.org/W2115529864",
        "https://openalex.org/W2533248932",
        "https://openalex.org/W2113430702",
        "https://openalex.org/W2131924950",
        "https://openalex.org/W2041445031",
        "https://openalex.org/W1549037892",
        "https://openalex.org/W2158874082",
        "https://openalex.org/W2567188764",
        "https://openalex.org/W2005902041",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W2038339694",
        "https://openalex.org/W2250988012",
        "https://openalex.org/W2087309226",
        "https://openalex.org/W2154099718"
    ],
    "abstract": "Hierarchical Pitman-Yor Process priors are compelling for learning language models, outperforming point-estimate based methods. However, these models remain unpopular due to computational and statistical inference issues, such as memory and time usage, as well as poor mixing of sampler. In this work we propose a novel framework which represents the HPYP model compactly using compressed suffix trees. Then, we develop an efficient approximate inference scheme in this framework that has a much lower memory footprint compared to full HPYP and is fast in the inference time. The experimental results illustrate that our model can be built on significantly larger datasets compared to previous HPYP models, while being several orders of magnitudes smaller, fast for training and inference, and outperforming the perplexity of the state-of-the-art Modified Kneser-Ney count-based LM smoothing by up to 15%.",
    "full_text": "Compressed Nonparametric Language Modelling\nEhsan Shareghi,| Gholamreza Haffari,| Trevor Cohn\n| Faculty of Information Technology, Monash University\n Computing and Information Systems, The University of Melbourne\nﬁrst.last@fmonash.edu, unimelb.edu.aug\nAbstract\nHierarchical Pitman-Yor Process priors are com-\npelling for learning language models, outperform-\ning point-estimate based methods. However, these\nmodels remain unpopular due to computational and\nstatistical inference issues, such as memory and\ntime usage, as well as poor mixing of sampler. In\nthis work we propose a novel framework which\nrepresents the HPYP model compactly using com-\npressed sufﬁx trees. Then, we develop an efﬁcient\napproximate inference scheme in this framework\nthat has a much lower memory footprint compared\nto full HPYP and is fast in the inference time. The\nexperimental results illustrate that our model can\nbe built on signiﬁcantly larger datasets compared\nto previous HPYP models, while being several or-\nders of magnitudes smaller, fast for training and\ninference, and outperforming the perplexity of the\nstate-of-the-art Modiﬁed Kneser-Ney count-based\nLM smoothing by up to 15%.\n1 Introduction\nStatistical Language Models (LM) are the key components\nof many tasks in Natural Language Processing, such as Sta-\ntistical Machine Translation, and Speech Recognition. Con-\nventional LMs are n-gram models which apply the Markov\nassumption to approximate the probability of a sequence wN\n1\nas,\nP(wN\n1 ) =\nNY\ni=1\nP(wijwi\u00001\n1 ) \u0019\nNY\ni=1\nP(wijwi\u00001\ni\u0000n+1): (1)\nSeveral smoothing techniques have been proposed to address\nthe statistical sparsity issue in computation of each condi-\ntional probability term. The widely used smoothing tech-\nniques for LM are Kneser-Ney (KN)[Kneser and Ney, 1995],\nand its extension Modiﬁed Kneser-Ney (MKN) [Chen and\nGoodman, 1999]. The intuition behind KN, MKN and their\nextensions [Shareghi et al., 2016a] is to adjust the original\ndistribution to assign non-zero probability to unseen or rare\nevents. This is achieved by re-allocating the probability mass\nin an interpolative procedure via absolute discounting.\nIt turns out that the Bayesian generalisation of KN family\nof smoothing is the Hierarchical Pitman-Yor Process (HPYP)\nLM [Teh, 2006a], which was originally developed for ﬁnite-\norder LM [Teh, 2006b], and was extended as the Sequence\nMemoizer (SM) [Wood et al., 2011] to model inﬁnite-order\nLMs. While capturing the long range dependency via HPYP\nimproves the estimation of conditional probabilities, these\ntypes of models remain impractical due to several computa-\ntional and learning challenges, namely large model size (data\nstructure representing the model, and the number of parame-\nters), long training and test time, and poor sampler mixing.\nIn this paper we address aforementioned issues; inspired\nby the recent advances in using compressed data structures in\nLM [Shareghi et al., 2015; 2016b] our model is built on top\nof a compressed sufﬁx tree (CST) [Ohlebusch et al., 2010].\nIn the training step, only the CST representation of text is\nconstructed, allowing for a very fast training, while proposing\nan efﬁcient approximate inference algorithm for the test time.\nMixing issue is avoided via careful sampler initialisation and\ndesign.\nThe empirical results show that our proposed approxima-\ntion of HPYP is richer than KN and MKN, and is much more\nefﬁcient in learning and inference phase compared to full\nHPYP. Compared with 10-gram KN and MKN models, our\n1-gram model consistently improves the perplexity by up to\n15%. Our compressed framework allows us to train on large\ncollection of text, i.e. 100\u0002larger than the largest dataset\nused in HPYP LMs [Wood et al., 2011] while having several\norders of magnitudes smaller memory footprint and support-\ning fast and efﬁcient inference.\n2 Interpolative Language Models\nConventional interpolative smoothing techniques in LM fol-\nlow a general form,\nP(wiju) = c(uwi) \u0000d\nc(u) + \r(u;d)\nc(u)\n\u0016P(wij\u0019(u));\nwhere, u = wi\u00001\ni\u0000n+1 is called the context, \u0019(u) is u with its\nleast recent symbol dropped, and c and d are the count and\nabsolute discount, while \r is the mass allocated to the lower\nlevel of the interpolation. Interpolative smoothing assumes\nthat P(wju), the conditional distribution of a word w in the\ncontext u, is similar to and hence smoothed by that of sufﬁx\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n2701\nk u Pk\nKN(wiju;dk) Pk\nHPYP(wiju;\u0011u)\nn u 1\nc(uwi)\u0000dk\nc(u) + dkN1+(u \u0001)\nc(u) Pk\u00001\nKN (wij\u0019(u);dk\u00001) nu\nwi\u0000dutu\nwi\nnu: + \u0012u + \u0012u+ dutu\n:\nnu: +\u0012u Pk\u00001\nHPYP(wij\u0019(u);\u0011\u0019(u))\nn-1 \u0019(u1) N1+(\u0001uwi)\u0000dk\nN1+(\u0001u \u0001) + dkN1+(u \u0001)\nN1+(\u0001u \u0001) Pk\u00001\nKN (wij\u0019(u);dk\u00001) nu\nwi\u0000dutu\nwi\nnu: +\u0012u + \u0012u+dutu\n:\nnu: +\u0012u Pk\u00001\nHPYP(wij\u0019(u);\u0011\u0019(u))\n... ... ... ...\n1 \" N1+(\u0001uwi)\u0000dk\nN1+(\u0001u \u0001) + dkN1+(u \u0001)\nN1+(\u0001u \u0001)\n1\nj\u001bj\nnu\nwi\u0000dutu\nwi\nnu:+\u0012u + \u0012u+dutu\n:\nnu: +\u0012u\n1\nj\u001bj\nTable 1: One-to-One mapping of interpolative smoothing under KN and HPYP. In Pk\nKN column : N1+(u \u0001) = jfw : c(\nuw) > 0gj, and\nsimilarly N1+(\u0001u) and N1+(\u0001u \u0001) are deﬁned. In Pk\nHPYP column : nu\nwi =\n(uwi) and nu\n: = c(u) when k = n. Also nu\n: = P\nw2\u001bu nu\nw,\nand tu\n: is deﬁned similarly, and \u0011u = fdu;\u0012u;fnu\nw;tu\nwgw2\u001bug.\nk= 1;u = \"\nk= 2;u = \u0019(\u0019(u1))\nk= 3;u = \u0019(u1)\nk= 4;u = u1 u2 u3\n\u00192\n...\n\u00162\n...\nFigure 1: An example of a interpolative LM of depth 3. Moving\nfrom leaf node u1 towards the root, corresponds to moving from\nthe top row towards the bottom row in Table 1. Nodes sharing a\nparent correspond to identical sequences except for their least recent\nsymbol, for example a partial sequence assignment to the nodes is\n\u0019(\u0019(u1)) =c, \u00191(u1) =bc, u1 = abc, u2 = bbc, u3 = dbc.\nof the context P(wj\u0019(u)). This recursive smoothing stops at\nthe unigram level where the conditioning context is empty,\nu = \" (see the left panel of Table 1). In what follows, we\nprovide a brief overview of hierarchical Pitmon-Yor Process\nLM and its relationship with KN.\n2.1 Hierarchical Pitman-Yor Process (HPYP) LM\nWe start by describing the Pitman-Yor process (PYP; [Pit-\nman and Yor, 1997 ]), used as a prior over LM parameters.\nPYP(d;\u0012;H ) is a distribution over the space of probabil-\nity distributions with three parameters: a base distribution H\nwhich is the expected value of a draw from a PYP, the con-\ncentration parameter \u0000d < \u0012which controls the variation\nof draws from PYP around H, and the discount parameter\n0 \u0014d <1 which controls the heavy-tailness of the sampled\ndistributions. PYP is an appropriate prior for LMs to capture\npower-law behaviour prevalent in the natural language[Gold-\nwater et al., 2011].\nTo illustate the use of the PYP prior, we conside as the like-\nlihood a simple unigram LM G, from which the words of a\ntext are generated. The Chinese restaurant process (CRP) is\na metaphor that allows generating words from a PYP without\ndirectly dealing with the LM G itself by integrating it out.\nConsider a restaurant where customers are seated on differ-\nent tables, and each table is served one dish. To make the\nanalogy, the restaurant corresponds to the LM, the customers\nare the text token needed to be generated, and the dishes are\nthe words. Let tw denote the number of tables serving the\nsame dish w in the restaurant, nw denote the total number\nof customers seated on these tables, and t: = P\nwtw and\nn: = P\nwnw to be the total number of tables and customers,\nrespectively. Generating the next word from the LM is done\nby sending a customer to the restaurant which either (i) sits on\nan existing table serving the dish wwith probability propor-\ntional to nw\u0000dtw, or (2) sits on a new table with probability\nproportional to \u0012+ dt: and orders a dish from the base distri-\nbution H. The probability of the next word wis thus\nP(wj\u0011) = nw \u0000dtw\nn: + \u0012 + \u0012+ dt:\nn: + \u0012P(wjH):\nwhere \u0011 = fd;\u0012; fnw;twgw2\u001bg, and \u001b is the vocabulary.\nNote that fnw;twgw2\u001b form the sufﬁcient statistics for gen-\nerating the next word.\nIn a HPYP LM, the distribution Gu of words following a\ncontext u has a PYP(du;\u0012u;G\u0019(u)) prior,\nGu jdu;\u0012u;G\u0019(u) \u0018 PYP(du;\u0012u;G\u0019(u)):\nwhere the base distribution G\u0019(u) itself has a PYP prior. This\ninduces a hierarchy among these context-conditioned distri-\nbutions (see Figure 1) tying the base distribution of each node\nof the hierarchy to the distribution of its parent. Note that u\nrefers to both a context (a sequence of words) and its corre-\nsponding node in the HPYP tree. The distribution at the root\nof the hierarchy G\" corresponds to the empty context \":\nG\" jd\";\u0012\";U \u0018PYP(d\";\u0012\";U)\nwhere the base distribution Uis the uniform distribution over\nthe vocabulary \u001b.\nHaving a HPYP LM, the next word is generated by inte-\ngrating out all of the distributions corresponding to the nodes\nof the tree. This is achieved by hierarchical Chinese restau-\nrant process (HCRP), whereby a customer is sent to a restau-\nrant of a context from which a word needs to be generated. In\ncase the customer is seated on a new table, a new customer is\nsent to the parent restaurant for ordering the dish. The number\nof customers sent to the parent is orchestrated via the concen-\ntration and discount parameters. The following constraints\nhold for fnu\nw;tu\nwgw2\u001bu across the tree nodes:\n8w2\u001bu : 0 <tu\nw \u0014nu\nw (2)\n8w2\u001bu : nu\nw =\nX\n   2children(u)\nt   \nw (3)\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n2702\nb\nbc#bbc\n#$\nbc#\n$\nabc#bbc\n#$\nc#\n9\nc#bbc\n#$\nc#$\n7\nabc#bbc\n#$\n#$\n8\n0\n#\nbbc\n#$\n5\n6\nbc#$\n#\nbbc\n#$\n4\n1\n2\n3\nSA\n9\n8\n0\n4\n1\n5\n6\n2\n7\n3\nBWT\n#\nc\n$\nc\n#\n#\nb\na\nb\nb\n(a)\nFigure 2: (top) Sufﬁx Tree, (bottom) Sufﬁx Array and Burrows-\nWheeler Transformation for “#abc#bbc#$”. In (top), labels cor-\nrespond to concatenation of edge labels from the root to each node\nand digits stored on leaves and in SA, (bottom), correspond to the\nstarting index of the sufﬁxes in text.\nwhere \u001bu is the subset of \u001bobserved after the context u. Ty-\ning the distributions as a hierarchy simulates the smoothing\nprocess by adjusting the base distributions of each level re-\ncursively. Given f\u0011ugu2HPYP, u 2HPYP denoting all nodes\nin HPYP, the predictive probability of a word win a context\nu is computed recursively as PHPYP(wju) in Table 1.\nNote the close similarity in formulation of KN smoothing\nand the HPYP as illustrated in Table 1; indeed we recover KN\nsmoothing from HPYP by assuming f\u0012u = 0gu2HPYP and\nftu\nw = 1gu2HPYP\nw2\u001bu . In what follows, we exploit this property to\ncompactly represent the HPYP LM.\n3 Compressed HPYP LM\nThe child-parent relationship between distributions in HPYP\nforms a context tree which can be represented as a sufﬁx\ntrie, or more compactly as a sufﬁx tree [Wood et al., 2011].\nIn practice sufﬁx trees require at best 20jTj bytes of space,\nwhere T denotes the text, making them impractical for any-\nthing but small data.\nHence, even storing the structure of the HPYP model with-\nout storing parameters f\u0011ugu2HPYP is impractical for large\ndatasets. This was possibly the case given that the largest\ndataset that HPYP LMs were built on small corpora including\nonly a few million words [Teh, 2006b; Gasthaus et al., 2010;\nWood et al., 2011]. However, with the availability of datasets\nof orders of magnitudes larger size [Buck et al., 2014;\nParker et al., 2011], it is crucial to improve the scalability\nof HPYP LMs. In the following, we brieﬂy introduce a set of\ncompressed data structures and operations, and then illustrate\nhow a HPYP LM can be made scalable using these tools.\n3.1 Compressed Sufﬁx Tree\nA Sufﬁx Tree (ST)[Weiner, 1973] of a string Twith alphabet\n\u001bis a tree of jTj+ 1 leaves, where a path from the root to a\nleaf corresponds to a sufﬁx of T. Each leaf holds a number\nindicating the staring position of the sufﬁx in T while leaves\nare lexicographically ordered, see Figure 2(top). The search\nfor any sequence u in T corresponds to ﬁnding the node vin\nST such that u is a preﬁx of the concatenation of the path la-\nbels from the root to v. While sufﬁx trees offer O(juj) search\nDEFINITION OPERATION COMPLEXITY\nsearch(u) bw-search O(jujlog j\u001bj)\nc(u) size O(1)\nN1+(u\u0001) deg O(1)\nN1+(\u0001u) int-sym O(N1 +(\u0001u) logj\u001bj)\nN1+(\u0001u\u0001) deg+int-sym N1+(\u0001u)O(1)+O(N1+(\u0001u) logj\u001bj)\nT\nable 2: Key CST operations, deﬁnitions, and time complexities.\nThe four operations at the bottom assume the node matching u is\ngiven via a backward-search (bw-search).\ncomplexity, in practice they require 20jTj bytes.\nA Sufﬁx Array (SA) [Manber and Myers, 1993 ] of T is\nan array of sorted sufﬁxes of T, where SA[i] holds a same\nnumber as i-th leaf in ST, see Figure 2(bottom). Search in\nSA translates to binary search to ﬁnd the corresponding range\nthat spans over all substrings that haveu as their preﬁx, and is\nO(jujlog jTj). Constructing SA takes 4-8jTj bytes in prac-\ntice, which compared to jTjlog j\u001bjbits required to store T\nmakes both ST and SA impractical to use for large data.\nA Compressed Sufﬁx Array (CSA) exploits text compress-\nibility and provides the same functionality as SA but in\nspace equal to bzip2 compressed T in practice. We use\nthe FM-Index [Ferragina et al., 2008] that utilizes the text\ncompressibility by using the Burrows-Wheeler transforma-\ntion (BWT) [Burrows and Wheeler, 1994] of the text, which\nis deﬁned as BWT[i ] = [ SA[i] \u00001 mod jTj] as illustrated\nin Figure 2(bottom). Searching for a sequence in BWT is\ndone in reverse order (called backward-search) and requires\nO(jujlog j\u001bj). Similarly, a Compressed Sufﬁx Tree (CST)\nsimulates ST and is built on CSA by storing extra bits to store\nthe shape of the tree and path labels [Ohlebusch et al., 2010].\nFor details see [Shareghi et al., 2015].\nTable 2 illustrates the key operations on CSA and CST\nalong with their complexities. The backward-search looks-\nup the CSA span covering the given sequence and returns the\n[lb;rb] of the nodevmatching it, and thesize operation counts\nthe number of leaves of the CST subtree rooted atvusing the\nlength of the returned range. The degree operation returns the\nnumber of types completing a sequence to its right. The most\nexpensive operation is the interval-symbols, which for a con-\ntext \u0019(u) returns its corresponding set of children and each\nchild’s corresponding node’s range using the BWT [Schnat-\ntinger et al., 2010]. Figure 2(bottom) shows the interval sym-\nbol procedure for ﬁnding the children of “\u0019(u) = bc”: First a\nsearch is done to ﬁnd the corresponding range for “bc” in SA,\nhighlighted in gray, then the corresponding cells on BWT are\nlooked up to ﬁnd fb;agas the two possible completions of\n“bc” to its left, and then their corresponding nodes are located\nefﬁciently, as illustrated by dashed arrows.\n3.2 Compressed HPYP LM\nWe make use of a CST to represent HPYP LM compactly,\nresulting in a model which is less than the size of the\ntext itself. This is inspired by the use of CSTs to repre-\nsent KN family of smoothing, and the fact that the struc-\nture of the hierarchy representing both models are exactly\nthe same. The basic idea for compressed KN-smoothed\nLMs is to extract the required counts directly from a CST\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n2703\nrepresentation of the text on-the-ﬂy [Shareghi et al., 2015;\n2016b]; Table 2 covers the required quantities together with\nthe CST operations and their time complexities.\nThe avid readers might notice that the key difference be-\ntween KN and HPYP is in the fnu\nw;tu\nwgu2HPYP\nw2\u001bu . While it is\npossible to store vectors fnu\nw;tu\nwgw2\u001bu for each node u of\nHPYP, it adds a signiﬁcant load to the memory usage. This is\none of the key computational issues of the inference approach\ntaken in sequence memoizer [Wood et al., 2011], which sam-\nples these vectors in the training time and stores them before\nusing them in testing. We address the issue by moving the\nsampling to the test time, hence skipping the need to store\nthese samples.\nTo complete our model description, we need to cover the\ndiscount and concentration parameters. In our model, the dis-\ncount parameters are set to Kneser-Ney discounts and tied\nbased on the context size juj, while each distribution uses its\nown separate concentration parameter. Our decision for ﬁx-\ning the discount parameters was to avoid the cost of sampling\nthem during the inference. Also, the range for the discount\nparameters are very ﬁne-grained making the gain in sampling\nthem very negligible1. We show in the inference section how\nﬁxing the discounts allows us to develop an efﬁcient sampler.\n4 Fast Approximate Inference for HPYP LM\nThe inference in a HPYP LM translates into computing the\npredictive probability of a word w, in the context u. This\ncorresponds to integrating out all the latent prior distributions\nand is deﬁned as the following intractable integral,\nP(wju) =\nZ\nP(wju;\u0011)P(\u0011)d(\u0011) (4)\nand is approximated using samples for \u0011. Here \u0011 =\nf\u0011ugu2HPYP, and P(wju;\u0011) is deﬁned as in Table 1 (right\npanel). While there are different approaches to generate sam-\nples, they are designed to run in the training phase and require\nexplicit storage of sampled quantities. This amounts to a sig-\nniﬁcant memory load which we avoid by skipping it. Con-\nsequently, it is required to generate the samples in the test\nphase, but the existing sampling algorithms [Gasthaus and\nTeh, 2010] generate samples across all nodes in HPYP which\nis too slow to ﬁt our purpose. Instead, we present a novel\nsampler, designed for the query phase of LM, which results\nin a much lower memory usage, is fast, and avoids mixing\nissues inherited in the full samplers.\nLet us assume the path from the root to the node matching\nthe context u of a given query “uwi” and denote the set of\ndistributions along the path by \r+, and all the other nodes of\nthe HPYP by \r\u0000. For example, for the query P(w = cju =\nab), \r+ = fGab;Gb;G\"gand \r\u0000= fGuju 2HPYP^Gu =2\n\r+g. Then, the three computational solutions involved in our\nproposed approximate inference scheme are:\nBranch Sampling While a full HPYP sampler, i.e. in\nSM [Wood et al., 2011], samples on \r+ [\r\u0000, in here only the\n\r+ distributions and only the type matching the query word\n1Our analysis shows no improvements of perplexities when dis-\ncounts were sampled, while it made the inference step slower.\nwi are selected for sampling, ﬁxing \r\u0000at their initialization\nwhich is KN. This means at any given state of sampling, we\nhave ftu\nw = 1gu2\r\u0000\nw2\u001bu and allows for a fast inference in the test\ntime while reducing the size of the sampling space exponen-\ntially hence reducing the risk of poor mixing.\nForgetting Samples Samples on \r+ are generated during\nthe test phase, used for approximating the predictive prob-\nability and then forgotten immediately. In this process, for\nany given query the state of HPYP will be set to KN at the\nbeginning of the sampling process. This keeps the memory\nusage of the training and inference phase close, and roughly\nmatching the size of the compressed text.\nRange Shrinking The key quantities in the sampling phase\nare tu\nw. In practice, the range 0 < tu\nw \u0014 nu\nw can poten-\ntially be very large, making the sampling very slow. Instead,\nwe follow a non-uniform sampling by shrinking the range to\n1 \u0014tu\nw \u0014minfM;nu\nwg(Here M = 10). The motivation\nhere is based on the key difference between KN and MKN\nwhich is mainly in the discount range. In MKN, which typ-\nically outperforms KN, discounts are larger than 1 and our\nempirical analysis on several datasizes and languages illus-\ntrate the range in practice is [0;3].2 This effect to some de-\ngree is replicated in HPYP when the discounts 0 \u0014du < 1\nare multiplied by tu\nw. Shrinking the sampling range keeps the\nHPYP distributions at each level of HPYP close to their cor-\nresponding KN (and MKN) counterparts, while the concen-\ntration parameter allows the distributions to have more ﬂexi-\nbility in capturing the desired distribution.\nThe ﬁrst two components allow fast inference while keep-\ning the memory usage of our approach to be several orders of\nmagnitudes smaller than the SM, hence making our approach\ncomputationally practical for large data regime. The third\ncomponent seeks to take advantage of the best component of\nMKN smoothing, while avoiding the mixing issue that occurs\nfor the inﬁnite HPYP case. In the following two subsections,\nwe provide the statistical underpinning of the sampling, and\nshow how it can be done under CST mechanics.\n4.1 Sampling\nWe sample using the joint distribution P(f\u0011ugu2HPYP),\nY\nw\nH(:)t\"\nw\nY\nu\n \n(\u0012ujdu)tu\n:\n(\u0012uj1)nu\n:\nY\nw\nSdu(nu\nw;tu\nw)\n!\n(5)\nwhere (ajb)c is the Pochhammer 3 symbol, and Sd(n;t) is\nthe generalized Stirling number of kind(\u00001;\u0000d;0) [Hsu and\nShiue, 1998].\nThe joint distribution in eqn. 5 allows efﬁcient sampling for\ntu\nw and nu\nw in the hierarchy, starting from the data level and\ngoing up in the hierarchy. The only expensive computation\nis for the Stirling numbers which are cached as KN discounts\nare used. We use the exact recursive formulation of Stirling\nnumbers [Buntine and Hutter, 2012] and switch to asymptotic\napproximation4 when tor nare large, i.e. \u00158000.\n2In theory, the MKN discounts can be as large as c(uwi).\n3(ajb)c = a(a+ 1\u0002b):::(a+ (c\u00001) \u0002b)\n4Using the Stirling’s approximation for factorials.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n2704\nAlgorithm 1 Gibbs Sampler for \u00112\r+\n1: function SAMPLER (w;k;n;\r +;~S;M )\n2: u\u0003 \r+\nk ; \u0019(u\u0003)  \r+\nk\u00001\n3: tu\u0003\nw  0; nu\u0003\nw  n\n4: ~Q null\n5: if nu\u0003\nw 6= 1 then\n6: while tu\u0003\nw \u0014minfM;nu\u0003\nw gdo\n7: if u\u00036= \"then\n8: F /P(tu\u0003\nw j:::) .eqn.7\n9: else\n10: F /P(t\"\nwj:::) .eqn.8\n11: ~Q (F;tu\u0003\nw )\n12: tu\u0003\nw  tu\u0003\nw + 1\n13: tu\u0003\nw  sample from( ~Q)\n14: else\n15: tu\u0003\nw  1\n16: \u0012u\u0003\n sample \u0012u\n17: ~S  (u\u0003;nu\u0003\nw ;tu\u0003\nw ;\u0012u\u0003\n)\n18: if u\u00036= \"then\n19: n\u0019(u\u0003)\nw  update(n\u0019(u\u0003)\nw ;tu\u0003\nw ) .eqn.6\n20: SAMPLER (w;k \u00001;n\u0019(u\u0003)\nw ;\r+;~S;M )\n21: return(~S)\nFor each Gu 2\r+, except the leaf level 5, the nu\nw’s will\nbe sampled jointly as t (u)\nw ’s are sampled, where  (u) 2\nchildren(u). Starting from the leaf level of the hierarchy, the\nnu\nw’s are read from the data, hence ﬁxed andtu\nw’s are sampled\nwhile satisfying the constraints in eqn.2, and eqn.3. Given a\nsampled tu\u0003\nw at the leaf level u\u0003, the n\u0019(u\u0003)\nw is updated as,\nn\u0019(u\u0003)\nw = tu\u0003\nw +\nX\n 2children(\u0019(u\u0003))^ 6=u\u0003\nt \nw: (6)\nThe conditional probability of the sampled tu\u0003\nw from eqn. 5\nfor the non-root levels while ﬁxing all the independent vari-\nables, P(tu\u0003\nw j:::), is proportional to,\n(\u0012u\u0003\njdu\u0003\n)tu\u0003\n:\n(\u0012\u0019(u\u0003)j1)P\n 2children(\u0019(u\u0003))\nt \n:\nSdu\u0003 (nu\u0003\nw ;tu\u0003\nw )Sd\u0019(u\u0003) (n\u0019(u\u0003)\nw ;t\u0019(u\u0003)\nw ) (7)\nwhere tu\u0003\n: = tu\u0003\nw + P\nv6=wtu\u0003\nv , and for the root level,\nP(t\"\nwj:::) /H(:)t\"\nw(\u0012\"jd\")t\"\n:Sd\"(n\"\nw;t\"\nw): (8)\nGiven sampled tu\u0003\nw ;nu\u0003\nw for a context u, the concentration\nparameter \u0012u\u0003\nis sampled via auxiliary variables [Teh et al.,\n2012] using a Gamma(a,b) prior. Algorithm 1 illustrates the\nsampling procedure for collecting a single set of samples\nalong \r+. The algorithm starts from the leaf level and moves\nup on the \r+ branch, sampling tu\nw and n\u0019(u)\nw jointly. The in-\ndex kdenotes the level on the extracted branch, and matches\nthe kin Table1. Given a query, this process is repeated mul-\ntiple times along the \r+ branch.\n5The leaf level is where the data is observed (ﬁrst row of Table 1).\ncontext\nε\nb\nab\nfull\nc\nbc\nabc\nbackward-search \ndirection on CST\ninterpolation & sampling \ndirection on HPYP\nFigure 3: Direction of search, interpolation, and sampling for“abc”.\nQUANTITY COMPLEXITY\ntu\u0003\n: O(1)\nP\n 2children(\u0019(u\u0003))\nt \n: N1+(\n\u0001\u0019(u\u0003))O(1)+O(N1+(\u0001\u0019(u\u0003)) logj\u001bj)\nn\u0019(u\u0003)\nw N1+(\u0001\u0019(u\u0003)w)+O(N1+(\u0001\u0019(u\u0003)w) logj\u001bj)\nTable 3: Complexities of computing critical sampling quantities.\n4.2 Sampling under CST mechanics\nSampling under CST involves two passes in the opposite di-\nrections, see Figure 3. Given a query, one pass starts from\nthe last word of the query and grows the pattern to its left\none word at a time. This pass collects all the required nodes\nin \r+, and identiﬁes the required fragmentations, while al-\nlowing to reuse spans during the backward-search, instead of\noperating a fresh search over the full CST span. Once all the\nrequired nodes are extracted, a second pass in the opposite\ndirection on the \r+ branch, samples t;n, and \u0012. The core of\nsampler relies on eqn. 7 and eqn. 8, which we describe in be-\nlow. Given a sampled tu\u0003\nw for a context u\u0003and word w, tu\u0003\n:\nis deﬁned as (N1+(u\u0003\n\u0001) \u00001) + (tu\u0003\nw ) which\nassumes a table\nper word type for all the words occurring after u except for\nthe word w for which tu\u0003\nw is sampled. This translates into a\ndegree(u) call to compute N1+(u\u0003\n\u0001) and is done in\nconstant\ntime. Computing the other main quantities is more expensive\nand involves interval-symbols operation. The children nodes\nof u\u0003, and \u0019(u\u0003)ware extracted via interval-symbols. Then,\ntheir table and count statistics are computed,\nX\n 2children(\u0019(u\u0003))\nt \n: = tu\u0003\n: +\nX\n 2children(\u0019(u\u0003))^ 6=u\u0003\nt \n:\nn\u0019(u\u0003)\nw = tu\u0003\nw +\nX\n 2children(\u0019(u\u0003)w)^ 6=u\u0003\nt \nw;\nwhere for each  , except for u\u0003, a degree operation is called,\nand tu\u0003\n: is computed as mentioned before. Table 3 illustrates\nthe complexity of these computations.\n5 Experiments\nWe report the perplexity of KN, MKN, SM, and our approach\nCN using the Finnish (FI), Spanish (ES), German (DE), En-\nglish (EN), French (FR), portions of the Europarl v7 [Koehn,\n2005] corpus, as well as 250MiB, 500MiB, 1,2,4, and 8GiB\nchunks of English Common Crawl corpus[Buck et al., 2014].\nThe data was tokenized, sentence split, and the XML markup\ndiscarded. As test sets, we used newstest-2014 for all lan-\nguages except Spanish, for which we used newstest-2013. To\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n2705\nPERP\nLEXITY\ntokens (M) n=10 n=1\nTRAI\nN TEST KN MKN SM CN\nEU-DE 54:93 0: 06 1810 1694 1598 1543\nEU-FI 40:47 0: 02 5570 5344 4833 4756\nEU-FR 66:79 0: 08 1328 1191 1090 1048\nEU-ES 62:06 0:07 444 416 440 377\nEU-EN 61:30 0:07 921 844 806 725\n125MiB 32:52 0 :07 333\n329 328 289\n250MiB 65:01 0: 07 299 295 300 283\n1GiB 201:52 0: 07 246 242 251 224\n2GiB 403:47 0: 07 223 219 — 209\n4GiB 807:71 0: 07 204 200 — 190\n8GiB 1617:27 0: 07 184 181 — 174\nTable 4: Data statistics, and perplexities of Kneser-Ney (KN), Mod-\niﬁed Kneser-Ney (MKN), Sequence Memoizer (SM), and Com-\npressed Nonparametric (CN) on different datasets. The empty cells\nfor 2,4,8 GiB are due to SM exceeding the 180GiB memory budget.\navoid the effect of differences in handling Out-of-V ocabulary\nwords in measuring the perplexities, we used a closed vo-\ncabulary setup. To measure the KN and MKN perplexities\nwe used the SRILM [Stolcke, 2002 ] toolkit. And to ver-\nify the comparability, we forced the KN assumptions on our\nmodel and closely matched (difference \u00141) the perplexity\nnumbers reported by SRILM. For benchmarking the memory\nand time usage of CN against SM, we used the English lan-\nguage datasets varying in size, from 125MiB to 8GiB chunks\nof Common Crawl. All experiments are done on a single core\non Intel Xeon E5-2667 3.2GHz and 180GiB of RAM.\nPerplexity As illustrated in Table 4, our approach (CN)\nconsistently outperforms MKN perplexities by a margin of\nup to 15%. To test against full HPYP inference, we com-\npared against the available implementation of SM.6 Although\nCN is initialised by KN and samples from the conditional\nP(\r+j\r\u0000), it is consistently better than SM. We speculate\nthis is due to poor mixing of SM over the full sampling space\ninvolving HPYP tree nodes, and proper mixing of our fast\ninference method over the smaller sampling space involving\nonly nodes on a single branch \r+. The difference between\nperplexities across multiple runs of our model were negligi-\nble. Comparing SM with KN and MKN reveals a surprising\nresult on some datasets: SM does worse, or is only marginally\nbetter regardless of the number of burn-in, or samples.7\nMemory and Time As demonstrated in Figure 4, the mem-\nory used by SM is several orders of magnitudes larger than\nthe size of the text and the size of our model in both train-\ning and test (query). For instance, on 250MiB dataset CN\nused (297MiB,108MiB) in training and test, compared with\n6https://github.com/jgasthaus/libPLUMP\n7This doesn’t verify the comparison reported in [Wood et al.,\n2011]. We noticed a critical decision in their experimental setup:\nwhile setting a threshold to replace low frequency words with a sin-\ngle token is a popular approach in text processing, in the KN and\nMKN LM this will cause a range of discount parameters to be zero,\neliminating the effect of smoothing and making KN and MKN per-\nform worse than their full potential.\n125M 250M 1G 2G 4G 8G\n1\n2\n4\n8\n16\n32\n64\n100Memory (GiB)\nDate Size\nTime (sec)\n1e+01 1e+05 1e+09\nTraining\n125M 250M 1G 2G 4G 8G\n1\n2\n4\n8\n16\n32\n64\n100Memory (GiB)\nData Size\nTime (sec)\n1e+01 1e+05 1e+09\nLoading+Query\nSM\nCN\nMemory\nTime\nFigure 4: Time (right Y-axis) and Memory (left Y-axis) usage of SM\nand CN in training and query phases on various data sizes (X-axis).\nSM could not be run on 2 \u0014GiB due to our memory budget.\n(16GiB,16GiB) of SM when it stores only 1 set of HPYP\nsamples. This made it impossible for us to run SM on larger\n(\u00152GiB) datasets, or with more samples. In terms of time\nusage in the training step, our approach is several times faster,\ni.e., (40\u0002,67\u0002) on (125MiB,250MiB) datasets. Noting that\nSM tends to get slower on larger datasizes, taking more than\n14 days to train on 1GiB dataset, while we only required less\nthan 2 hours. In the test time we are on average around 24\u0002\nslower noting that SM tends to get slower on larger datasizes,\ni.e. from 27\u0002faster on 125MiB to 21\u0002on 250MiB. Infer-\nence only with 5 samples along each branch and no burn-\nin, affected the perplexities of our approach up to 2% while\nmaking the average test speed only 4\u0002slower than SM. Our\napproach on ”load+query” carries roughly a similar pattern\nexcluding the load time. On 1GiB, our query is only 1:8\u0002\nslower than SM, and we are 2:3\u0002faster on load+query. A\nsigniﬁcant result which is due to smaller model size, and ef-\nﬁcient inference mechanism of our CST-based framework.\n6 Conclusion\nIn this paper we proposed a framework based on compressed\nsufﬁx trees to represent inﬁnite-order hierarchical Bayesian\nlanguage models compactly, while developing a fast and\nmemory-efﬁcient approximate inference scheme. Compared\nwith the existing HPYP LMs our approach has several orders\nof magnitudes lower memory footprint allowing us to apply\nit on (100\u0002) larger data sizes than the largest data used by\nHPYP LM. This is achieved by avoiding potential mixing is-\nsues, while consistently outperforming the Kneser-Ney fam-\nily of smoothings by a signiﬁcant margin.\nAs our future work, we would like to speedup the inference\nvia approximating Stirling numbers using a separate model to\navoid its expensive recursion cost during sampling, as well as\nexploring continuous space approximations of HPYP.\nAcknowledgments\nThis research was supported by the National ICT Australia\n(NICTA). The ﬁrst author would like to thank Wray Buntine\nfor fruitful discussions about sampling in HPYP, and Philip\nChan for the support to run the experiments on Monash Ad-\nvanced Research Computing Hybrid (MonARCH) servers.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n2706\nReferences\n[Buck et al., 2014] Christian Buck, Kenneth Heaﬁeld, and\nBas van Ooyen. N-gram counts and language models from\nthe common crawl. In Proceedings of the Language Re-\nsources and Evaluation Conference, 2014.\n[Buntine and Hutter, 2012] Wray Buntine and Marcus Hut-\nter. A bayesian view of the Poisson-Dirichlet process.\narXiv preprint arXiv:1007.0296, 2012.\n[Burrows and Wheeler, 1994] Michael Burrows and David\nWheeler. A block sorting lossless data compression al-\ngorithm. Technical Report 124, Digital Equipment Corpo-\nration Systems Research Center, 1994.\n[Chen and Goodman, 1999] Stanley F Chen and Joshua\nGoodman. An empirical study of smoothing techniques\nfor language modeling. Computer Speech & Language,\n13(4):359–393, 1999.\n[Ferragina et al., 2008] Paolo Ferragina, Rodrigo Gonz ´alez,\nGonzalo Navarro, and Rossano Venturini. Compressed\ntext indexes: From theory to practice. ACM J. of Exp.\nAlgorithmics, 13, 2008.\n[Gasthaus and Teh, 2010] Jan Gasthaus and Yee W. Teh. Im-\nprovements to the sequence memoizer. In Advances in\nNeural Information Processing Systems 23, pages 685–\n693, 2010.\n[Gasthaus et al., 2010] Jan Gasthaus, Frank Wood, and\nYee Whye Teh. Lossless compression based on the se-\nquence memoizer. In 2010 Data Compression Conference\n(DCC 2010), pages 337–345, 2010.\n[Goldwater et al., 2011] Sharon Goldwater, Thomas L Grif-\nﬁths, and Mark Johnson. Producing power-law distribu-\ntions and damping word frequencies with two-stage lan-\nguage models. Journal of Machine Learning Research,\n12(Jul):2335–2382, 2011.\n[Hsu and Shiue, 1998] Leetsch C Hsu and Peter Jau-Shyong\nShiue. A uniﬁed approach to generalized stirling numbers.\nAdvances in Applied Mathematics, 20(3):366–384, 1998.\n[Kneser and Ney, 1995] Reinhard Kneser and Hermann Ney.\nImproved backing-off for m-gram language modeling. In\nProceedings of IEEE International Conference on Acous-\ntics, Speech and Signal Processing, volume 1, pages 181–\n184, 1995.\n[Koehn, 2005] Philipp Koehn. Europarl: A parallel corpus\nfor statistical machine translation. In Proceedings of the\nMachine Translation summit, 2005.\n[Manber and Myers, 1993] Udi Manber and Eugene W. My-\ners. Sufﬁx arrays: A new method for on-line string\nsearches. SIAM Journal on Computing, 22(5):935–948,\n1993.\n[Ohlebusch et al., 2010] Enno Ohlebusch, Johannes Fischer,\nand Simon Gog. CST++. In Proceedings of the Interna-\ntional Symposium on String Processing and Information\nRetrieval, 2010.\n[Parker et al., 2011] Robert Parker, David Graff, Junbo\nKong, Ke Chen, and Kazuaki Maeda. English gigaword\nﬁfth edition. Linguistic Data Consortium, (LDC2011T07),\n2011.\n[Pitman and Yor, 1997] Jim Pitman and Marc Yor. The two-\nparameter Poisson-Dirichlet distribution derived from a\nstable subordinator. The Annals of Probability, pages 855–\n900, 1997.\n[Schnattinger et al., 2010] Thomas Schnattinger, Enno\nOhlebusch, and Simon Gog. Bidirectional search in a\nstring with wavelet trees. In Proceedings of the Annual\nSymposium on Combinatorial Pattern Matching, 2010.\n[Shareghi et al., 2015] Ehsan Shareghi, Matthias Petri, Gho-\nlamreza Haffari, and Trevor Cohn. Compact, efﬁcient and\nunlimited capacity: Language modeling with compressed\nsufﬁx trees. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, 2015.\n[Shareghi et al., 2016a] Ehsan Shareghi, Trevor Cohn, and\nGholamreza Haffari. Richer interpolative smoothing based\non modiﬁed kneser-ney language modeling. In Proceed-\nings of the Conference on Empirical Methods in Natural\nLanguage Processing, 2016.\n[Shareghi et al., 2016b] Ehsan Shareghi, Matthias Petri,\nGholamreza Haffari, and Trevor Cohn. Fast, small and\nexact: Inﬁnite-order language modelling with compressed\nsufﬁx trees. Transactions of the Association for Computa-\ntional Linguistics, 4:477–490, 2016.\n[Stolcke, 2002] Andreas Stolcke. SRILM–an extensible lan-\nguage modeling toolkit. In Proceedings of the Interna-\ntional Conference of Spoken Language Processing, 2002.\n[Teh et al., 2012] Yee Whye Teh, Michael I Jordan,\nMatthew J Beal, and David M Blei. Hierarchical dirichlet\nprocesses. Journal of the american statistical association,\n2012.\n[Teh, 2006a] Yee Whye Teh. A Bayesian interpretation of\ninterpolated Kneser-Ney. Technical report, NUS School\nof Computing, 2006.\n[Teh, 2006b] Yee Whye Teh. A hierarchical Bayesian lan-\nguage model based on Pitman-Yor processes. In Proceed-\nings of the Annual Meeting of the Association for Compu-\ntational Linguistics, 2006.\n[Weiner, 1973] Peter Weiner. Linear pattern matching algo-\nrithms. In Proceedings of the Annual Symposium Switch-\ning and Automata Theory, 1973.\n[Wood et al., 2011] Frank Wood, Jan Gasthaus, C ´edric Ar-\nchambeau, Lancelot James, and Yee Whye Teh. The\nsequence memoizer. Communications of the ACM,\n54(2):91–98, 2011.\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)\n2707"
}