{
  "title": "LLMDet: A Third Party Large Language Models Generated Text Detection Tool",
  "url": "https://openalex.org/W4389520029",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2915682003",
      "name": "Kangxi Wu",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1974895339",
      "name": "Liang Pang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106143704",
      "name": "Huawei Shen",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2106240991",
      "name": "Xueqi Cheng",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2730258684",
      "name": "Tat-Seng Chua",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288076474",
    "https://openalex.org/W4315498228",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W2768348081",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W3172226021",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W4290803104",
    "https://openalex.org/W4318149317",
    "https://openalex.org/W3173978205",
    "https://openalex.org/W2286300105",
    "https://openalex.org/W4364387756",
    "https://openalex.org/W3046357466",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3139815689",
    "https://openalex.org/W4390873635",
    "https://openalex.org/W4324299222",
    "https://openalex.org/W2957997671",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3101891351",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3156309620",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W4288334893",
    "https://openalex.org/W4318351452",
    "https://openalex.org/W4312091692",
    "https://openalex.org/W4384918448"
  ],
  "abstract": "Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. Therefore, we propose LLMDet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and others. In LLMDet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each LLM. By jointly analyzing the proxy perplexities of LLMs, we can determine the source of the generated text. Experimental results show that LLMDet yields impressive detection performance while ensuring speed and security, achieving 98.54% precision and about Ã— 5.0 faster for recognizing human-authored text. Additionally, LLMDet can effortlessly extend its detection capabilities to a new open-source model. We will provide an open-source tool at https://github.com/TrustedLLM/LLMDet.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2113â€“2133\nDecember 6-10, 2023 Â©2023 Association for Computational Linguistics\nLLMDet: A Third Party Large Language Models\nGenerated Text Detection Tool\nKangxi Wu1,2, Liang Pang1âˆ—, Huawei Shen1,2, Xueqi Cheng1,2, Tat-Seng Chua3\n1 Institute of Computing Technology, Chinese Academy of Sciences\n2 University of Chinese Academy of Sciences\n3 Sea-NExT Joint Lab, National University of Singapore\n{wukangxi22s, pangliang, shenhuawei, cxq}@ict.ac.cn\ndcscts@nus.edu.sg\nAbstract\nGenerated texts from large language models\n(LLMs) are remarkably close to high-quality\nhuman-authored text, raising concerns about\ntheir potential misuse in spreading false in-\nformation and academic misconduct. Conse-\nquently, there is an urgent need for a highly\npractical detection tool capable of accurately\nidentifying the source of a given text. However,\nexisting detection tools typically rely on access\nto LLMs and can only differentiate between\nmachine-generated and human-authored text,\nfailing to meet the requirements of fine-grained\ntracing, intermediary judgment, and rapid de-\ntection. Therefore, we propose LLMDet, a\nmodel-specific, secure, efficient, and extend-\nable detection tool, that can source text from\nspecific LLMs, such as GPT-2, OPT, LLaMA,\nand others. In LLMDet, we record the next-\ntoken probabilities of salient n-gram as fea-\ntures to calculate proxy perplexity for each\nLLM. By jointly analyzing the proxy perplexi-\nties of LLMs, we can determine the source of\nthe generated text. Experimental results show\nthat LLMDet yields impressive detection per-\nformance while ensuring speed and security,\nachieving 98.54% precision and about Ã—5.0\nfaster for recognizing human-authored text. Ad-\nditionally, LLMDet can effortlessly extend its\ndetection capabilities to a new open-source\nmodel. We will provide an open-source tool at\nhttps://github.com/TrustedLLM/LLMDet.\n1 Introduction\nRecently, the emergence of ChatGPT1 has heralded\na \"Cambrian Explosion\" for generative large lan-\nguage models (LLMs). GPT-4 (OpenAI, 2023),\nBard2, PaLM-2 (Anil et al., 2023), and other LLMs\nfrom internet companies are currently flourish-\ning, while open-source communities are witness-\ning a proliferation of open-source models like\nâˆ—Corresponding author\n1https://openai.com/product/chatgpt\n2https://bard.google.com\nLLaMA (Touvron et al., 2023a), OPT (Liu et al.,\n2021), ChatGLM (Du et al., 2022). These mod-\nels are capable of generating coherent, fluent, and\nmeaningful text. However, the formidable text gen-\neration capabilities of generative language models\nhave also raised concerns about their potential mis-\nuse in domains such as phishing, spreading false\ninformation, and academic fraud. Additionally,\nwith the application of products like ChatGPT, the\nfuture abundance of machine-generated text data\nhas the potential to contaminate genuine human-\ngenerated data (Hataya et al., 2022), altering the\ndata ecosystem of the real world.\nAccordingly, the study of practical content gen-\neration detection tools has attracted widespread\nattention from the community. Recently, the pri-\nmary focus of research is on approaching the text\ndetection problem as a binary classification task\nto distinguish machine-generated text and human-\nauthored text, making it hard to assign responsibil-\nity to a specific model or its provider. Nevertheless,\nWatermarking (Kirchenbauer et al., 2023) meth-\nods necessitate altering the text generation process,\nleading to a compromise in the quality of the gen-\nerated content. Techniques like GPT-zero3, Detect-\nGPT (Mitchell et al., 2023), and the classifier in\nOpenAI (OpenAI, 2023) require access to the de-\nployed model, thereby resulting in high cost and\nintractability for third parties.\nThus, a practical LLM detection tool should\npossess the following capabilities, which are also\nthe objectives of our method: Specificity: Merely\nfocusing on identifying human and machine-\ngenerated text is insufficient for duty attribution.\nThere is a pressing need for the ability to recognize\nthe specific model responsible for generating the\ntext. Safety: Ensuring model security and mitigat-\ning potential risks require a detection method that\ndoes not require accessing model parameters. This\nneed is particularly urgent for commercial mod-\n3https://gptzero.me\n2113\nels. Efficiency: With the increasing demand for\ndetection and the exponential growth of models,\nit is crucial to develop detection algorithms that\nhave low resource and low latency requirements.\nExtendibility: The detection tool should inher-\nently possess the capacity to seamlessly accommo-\ndate emerging model paradigms. This capability\nplays a pivotal role in refining the detection ecosys-\ntem and effectively addressing the ever-expanding\nvariety of LLMs.\nGuided by the aforementioned capabilities, we\npropose a pragmatic third-party detection method\ncalled LLMDet. Our approach is inspired by the\nobservation that perplexity serves as a reliable\nsignal for distinguishing the source of generated\ntext, a finding that has been validated in previous\nwork (Solaiman et al., 2019; Jansen et al., 2022;\nMitchell et al., 2023). However, directly calculat-\ning perplexity requires access to LLMs, which com-\npromises both safety and efficiency. In LLMDet,\nwe address this challenge by capturing the next\ntoken probabilities of prominent n-gram in texts\nas priors. This enables us to efficiently compute\na proxy perplexity for each LLM. By comprehen-\nsively analyzing the proxy perplexities of LLMs,\nwe can accurately trace the specific language model\nresponsible for generating the text. Notably, our\nmethod eliminates the need to access the model\nat the detection end, ensuring the security of pa-\nrameters in large-scale models. It also offers the\npotential for seamless integration with emerging\nopen-source models, as well as proprietary models\nunder appropriate licensing. These factors con-\ntribute to the widespread adoption of our approach.\nLLMDet exhibits outstanding overall detection\nperformance, with an F1-Macro score of 88.14%\nand near-perfect results for R@2, indicating that\nhighly ranked predictions cover the correct labels\nfor the majority of instances. Particularly notable\nis its exceptional discriminative ability in human\ntext, LLaMA-generated text, and BART-generated\ntext. In terms of detection efficiency, LLMDet\nsignificantly outperforms other similar methods\nsuch as fine-tuned RoBERTa, GPT-zero4, Detect-\nGPT (Mitchell et al., 2023), and True-PPL with\nrespect to speed. And, it has very low resource re-\nquirements, as text detection can be accomplished\nsolely on a CPU, enabling easy accessibility for a\nwider range of users. Additionally, when tested on\nperturbated text data, LLMDet produces satisfac-\n4https://gptzero.me\ntory detection results, demonstrating its robustness\nand adaptability.\n2 Related Work\nThe existing methods for detecting generated text\ncan be broadly categorized into two types: black-\nbox and white-box detection (Tang et al., 2023).\n2.1 Black-box Detection\nBlack-box detection methods can be further di-\nvided into three main branches: statistical learn-\ning methods, supervised learning methods, and\nunsupervised learning methods. Traditional ap-\nproaches utilize statistical metrics such as entropy,\nperplexity, andn-gram frequency for text classifica-\ntion (Gehrmann et al., 2019; FrÃ¶hling and Zubiaga,\n2021).\nCompared to statistical learning methods, su-\npervised learning methods are more commonly\nused in text detection. These works leverage text\nfeatures to train a supervised classification model\nspecifically designed for the detection of machine-\ngenerated text (Bakhtin et al., 2019; Uchendu et al.,\n2020; Fagni et al., 2021; OpenAI, 2023).\nHowever, the study conducted by (Uchendu\net al., 2020; Chakraborty et al., 2023) demonstrates\nthat a limitation of supervised models is the po-\ntential occurrence of overfitting within the domain,\nresulting in poor detection performance outside the\ndomain.\nTo address the limitations of supervised learning\nmethods, unsupervised learning methods such as\nDetectGPT (Mitchell et al., 2023) and GPT-Zero\nhave been developed. These approaches utilize\nchecks on perplexity and burstiness in the text to\ndetermine whether it is artificially generated or au-\nthored by a human.\n2.2 White-box Detection\nWhite-box detection methods require full access\nto LLMs, thereby enabling control over the gen-\neration behavior of the model or embedding wa-\ntermark within the generated text (Abdelnabi and\nFritz, 2021; Ueoka et al., 2021; Dai et al., 2022).\nThis enables the tracking and detection of machine-\ngenerated text within white-box settings.\nThe current state-of-the-art approach, as pro-\nposed by (Kirchenbauer et al., 2023), partitions\nthe modelâ€™s vocabulary into whitelist and black-\nlist tokens when predicting the next token given\na prompt. During text generation, the goal is to\n2114\nproduce whitelist tokens as much as possible, effec-\ntively creating a strong watermark. Third parties\ncan determine if the text is machine-generated by\nanalyzing the frequency of whitelist tokens within\nthe text. While watermarking methods offer ro-\nbustness and interpretability, they can compromise\nthe quality of the generated text and may not be\nhighly practical in certain scenarios (Sadasivan\net al., 2023).\n3 Motivation\nA practical LLMs detection method should possess\nthe characteristics of being specific, secure, effi-\ncient, and extensible, which serve as the intention\nfor developing our third-party detection tool.\nSpecificity: The field of LLMs constantly\nevolves, indicating that a sole focus on identifying\nhuman and machine-generated text is insufficient\nto meet detection requirements. From the perspec-\ntive of copyright protection for works generated\nby artificial intelligence (Aplin and Pasqualetto,\n2019), an ideal detection tool should be capable of\nidentifying the specific language model responsible\nfor generating the text, thereby exerting a lasting\nimpact on intellectual property rights protection.\nSafety: The majority of existing detection meth-\nods require accessing or modifying model parame-\nters, which is deemed unacceptable for commercial\nmodels. Once the model is loaded, it represents a\nfinancial loss for the owner and can also expose\nthe model to potential attacks (Kurita et al., 2020).\nHence, considering the security of the model, it is\ndesirable to minimize the need for model loading\nduring the detection process.\nEfficiency: With the growing number of users\nutilizing large-scale models, the future of text de-\ntection is poised for exponential expansion in terms\nof demand and user base. For instance, in the realm\nof education, there is a significant need for text de-\ntection to combat cheating and plagiarism (Cotton\net al.), despite often constrained hardware condi-\ntions. This poses a formidable challenge to exist-\ning detection methods. Hence, the pursuit of rapid\nand resource-efficient approaches has become a\npivotal direction in developing efficient detection\nalgorithms.\nExtendibility: As for multi-model generated\ntext detection approaches, it is crucial to seamlessly\nadapt to emerging model paradigms and extend de-\ntection capabilities to new models. This is because\nan excellent detection tool is not static but needs\nto keep up with technological advancements and\ncontinuously enhance its own detection ecosystem\nto address the challenges posed by new models.\n4 LLMDet\nCombining the aforementioned motivations, we\nintroduce LLMDet, a text detection tool capable\nof identifying the sources from which the text was\ngenerated, such as Human, LLaMA, OPT, or others.\nThe overall framework of the system is illustrated\nin Figure 1 and consists of two main components:\nDictionary Construction (see Â§ 4.1) and Text De-\ntection (see Â§ 4.2).\nThe construction of the dictionary is performed\noffline by us or provided by the model owner, ensur-\ning its independence from external systems. This\nensures the fulfillment of the four characteristics\nproposed for our detection tool in Â§ 3. The text de-\ntection component can be distributed to tool users,\nallowing third-party detection without requiring the\npossession of the model. For the specific algorithm,\nplease refer to Appendix A.\n4.1 Dictionary Construction\nDrawing from previous detection works, such as\nDetectGPT (Mitchell et al., 2023) and GPT-Zero5,\nperplexity has shown promising results in detecting\nmachine-generated text. Therefore, we consider uti-\nlizing perplexity as a measurement of identifying\nthe generated text from different LLMs. However,\ncalculating the actual perplexity requires access\nto LLMs, which goes against the safety and effi-\nciency characteristics of the practical LLMs detec-\ntion method.\nPerplexity is a measure used to evaluate the per-\nformance of language models. Specifically, it is the\nexponential average of the negative log-likelihood\nof a sequence generated by the model. The per-\nplexity score is calculated based on the probability\nof generating the next word, given all the previous\nwords in the sequence, e.g. p(xi, x<i). In order\nto calculate the perplexity of text without access-\ning the model, we need approximate p(xi, x<i) by\nreplacing x<i with a n-gram , thus a dictionary\nshould be constructed, with n-gram as keys and the\nnext token probabilities as values. This dictionary\nserves as prior information during the detection pro-\ncess, allowing us to compute the proxy perplexity\nof the text instead of the true perplexity. The con-\nstruction process can be divided into three steps:\n5https://gptzero.me\n2115\nPrompt\nSampling\nLLM is\nGPT2\nOPT\nGPT2 Text\nCollection\nOPT Text\nCollection\nLLaMA\nLLaMA Text\nCollection\nWord\nStatistic n-grams\nText Detection\nNext Token Probability Sampling\nDictionary Construction\nText Collection\nYou are\nWhat isâ€¦\nPrompti\nâ€¦\n1. Generated Text Samples 2. Word Frequency Statistic\nWord\nStatistic n-grams\nWord\nStatistic n-grams\n2-grams 3-grams 4-grams\n3. Next Token Probability Sampling\nGPT2\nOPT\nLLaMA\nâ€¦\nGPT2 Dictionary\nKey V alue\nOPT Dictionary\nLLaMA Dictionary\n0.93 Information Retrieval\n0.88 Artificial Intelligent\n0.85 Large Language Model\n0.77 As a dialogue agent\n0.66 I donâ€™tknow what\nâ€¦\nInformation\n0.45 Retrieval\n0.32 Technology\n0.13 Seeking\n0.02 ratio\nâ€¦\nLarge\nLanguage\n0.35 Model\n0.32 Models\n0.13 AI\n0.02 course\nâ€¦\nâ€¦\nS\nt\no\nr\ne\nd\n4. Proxy Perplexity\nEstimation\nGPT2 Proxy\nPerplexity\nOPT Proxy\nPerplexity\nLLaMA Proxy\nPerplexity\nâ€¦ â€¦\nâˆ’1\nð‘¡ à·\nð‘–=0\nð‘¡\nlogð‘(ð‘¥ð‘–|ð‘›-ð‘”ð‘Ÿð‘Žð‘š)\n5. Result Ranking\nRanker\nP Model\n0.4 GPT2\n0.2 OPT\n0.1 Human\n0.1 â€¦\nFigure 1: The detailed processes of the proposed tool LLMDet. It contains two main phases, dictionary construction\nand text detection. The dictionary construction phase is carried out offline by us or provided by the model holder,\nindependent of external systems. The text detection phase can be accessed by the tool user who, as a third party,\nperforms text detection without holding the model.\n1) Generated Text Sampling: Due to the ab-\nsence of readily available model-generated text\ndata, it is necessary to collect a sufficient number\nof corresponding generated texts for each model.\nWe provide a prompt dataset and, for each model,\nrandomly sample an equal number of prompts. We\nuse these prompts to generate corresponding texts\nand collect the required text data.\n2) Word Frequency Statistics: In this phase,\nwe first utilize the generated texts collected in the\nprevious step to perform n-gram word frequency\nstatistics (Pang et al., 2016). The n-gram range\nfrom 2-gram to n-gram. Subsequently, we select\nthe top-k n-gram based on their frequency.\n3) Next Token Probability Sampling: In this\nphase, we use each n-gram s obtained from word\nfrequency statistics as samples. We input the first\nn âˆ’1 token s[1:nâˆ’1] into the corresponding gener-\native models for predicting next-token probabili-\nties pw = [pw\n1 , . . . , pw\n|W|], where |W|is the size of\nvocabulary. Subsequently, we sample the top- K\nwords based on next-token probabilities. For n-\ngram with different values of n, the optimal value\nof K for top-K sampling may vary.\nWe should consider the optimal values, the de-\ngree of n-gram, the number of n-gram k, and the\nnumber of next token probabilities K from two\naspects: detection performance and storage cost.\nIn terms of detection performance, the larger n,\nk, and K may improve the detection performance\nof LLMDet, as this enables the proxy perplexity to\napproximate the true perplexity.\nIn terms of storage cost, due to the data type\nof the sampling probabilities being Float64 and n-\ngram being a string, a significant amount of storage\nspace is required, e.g. O(nkK). If n is set to 4, k\nis set to 100,000 (much smaller than the number of\n4-gram), and K is set to 10,000 (most vocabulary\nsize is larger than that), we need almost 22GB to\nstore only probabilities for one model. Thus, we\nhave to reduce the storage in practical use. The\nreduction can be considered in two folds, 1) se-\nlect a suitable n, k and K, 2) reduce Float64 to\nFloat16 and represent n-gram as Int16. We find\nthat does not significantly affect LLMDet, while it\nreduces storage costs by approximately 11 times\nabout 0.5GB.\nIn the end, we constructed an n-gram and proba-\nbility dictionary for each LLM, which was utilized\nfor calculating proxy perplexity. The above three\nsteps are repeated on GPT-2 (Radford et al., 2019),\nOPT (Liu et al., 2021), UniLM (Dong et al., 2019),\nLLaMA (Touvron et al., 2023a), BART (Lewis\net al., 2019), T5 (Raffel et al., 2020), Bloom (Scao\net al., 2022) and GPT-neo (Black et al., 2022), re-\nspectively.\n4.2 Text Detection\nIn Â§ 4.1, we have obtained the dictionary ofn-gram\nand their probabilities. Therefore, we can use the\ncorresponding dictionary of each model as prior in-\nformation for third-party detection to calculate the\nproxy perplexity of the text being detected on each\nmodel. Immediately after, by inputting the proxy\nperplexity as a feature into a trained text classifier,\nwe can obtain the corresponding detection results.\n4.2.1 Proxy Perplexity Estimating\nDuring text detection, for the input text X, our\ninitial task is to estimate the proxy perplexity of\n2116\nthis text across various large language models as a\nvector of feature information.\nTaking the estimation of proxy perplexity on\nModelm as an example, we begin by tokeniz-\ning the input text X to obtain its sequence X =\n[x1, x2, ..., xt], assuming the length of the tok-\nenized sequence is denoted as t.\nThen, the proxy perplexity of the sequence X\non Modelm can be mathematically represented by\nthe following function, denoted as Proxy_PPL:\nProxy_PPL(X) =âˆ’1\nt\ntâˆ‘\ni=0\nlog p (xi |n-gram) . (1)\nMore specifically, log p (xi |n-gram) represents\nthe logarithmic likelihood of the i-th token, condi-\ntioned on the preceding tokens x<i matching the\nn-gram in the dictionary of Modelm. The like-\nlihood probability p (xi |n-gram) corresponds to\nthe value associated with the matching n-gram in\nthe dictionary.\nSimilarly, by repeating the above procedure on\nother models, we can obtain the proxy perplex-\nity of the detection text on the respective mod-\nels. These proxy perplexities constitute the fea-\nture information vector for detection, denoted\nas F = [Proxy_PPL1,Proxy_PPL2, ...,Proxy_PPLc], sub-\nscript c denotes the number of LLMs.\n4.2.2 Result Ranking\nBefore result ranking, we initially estimate the\nproxy perplexity of the generated texts from each\nlanguage model and human-generated texts. This\nestimation allows us to obtain a separate feature in-\nformation vector for each text. Subsequently, these\nvectors are employed to train a text detector.\nNext, we input the feature information vectors\nF, obtained during the proxy perplexity estimation\nphase, of the text to be detected into the trained text\ndetector for result prediction, yielding a prediction\nresult, such as for a given Modeli, the probability\nis denoted as pi. It is important to note that we\ndenote the probability of Human as p0.\nHowever, due to the fact that the text detector is\ntrained based on perplexity as a feature, it is not\nsensitive to the length information of the detected\ntext, resulting in suboptimal detection performance\nfor some short texts. Therefore, it is necessary to\napply a smoothing technique to the probabilities of\nthe detection results in order to enhance the success\nrate of detecting short texts. The smoothing process\nis denoted as,\nËœpi = log (pi) +1\nL log\n( 1\nc + 1\n)\n, (2)\nwith L is the length of the text to be detected, c\ndenotes the number of LLMs.\nFinally, we apply softmax to the smoothed prob-\nabilities to obtain [ Ë†p0, Ë†p1, ...,Ë†pc]. Consequently, the\ndetection results are transformed into the proba-\nbility of Modeli is Ë†pi. Subsequently, the detection\nresults are sorted based on the magnitude of the\nprobability values in the result dictionary, yielding\nthe final detection outcome,\n[ Ë†p0, Ë†p1, ...,Ë†pc] =softmax ([ Ëœp0, Ëœp1, ...,Ëœpc]) . (3)\n5 Experiments\nWe conduct experiments based on proxy perplex-\nity and true perplexity according to the methods\nproposed in Â§ 4. By comparing the performance\nof the text detectors based on fine-tuned RoBERTa,\nproxy perplexity, and ture perplexity, we find that\nour proposed method outperforms existing meth-\nods in terms of detection efficiency, security, and\nscalability while ensuring the performance of the\ndetector.\n5.1 Datasets\nIn our experiments, we use Wikipedia paragraphs\nfrom the SQuAD context (Rajpurkar et al., 2016)\nand news articles from the Xsum (Narayan et al.,\n2018) dataset for extraction. We extract the first 5\nphrases of each text data to form a prompt dataset.\nDuring the text generation phase, for each LLM,\nwe randomly select 32,000 data samples from the\nprompt dataset as input and have the model gen-\nerate corresponding text. The generated text from\neach model is evenly split into two parts: 16,000\nsamples for the statistical dataset and 16,000 sam-\nples for the validation dataset. The statistical\ndataset is used for n-gram frequency counting. The\nvalidation dataset from LLMs, along with 16,000\nsamples collected from HC3 (Guo et al., 2023) as\nhuman-generated text, form a combined dataset for\nthe training and validation of text detectors.\n5.2 Metrics\nTo evaluate the ability of the detector to distin-\nguish between text generated by different LLMs\nand human-written text, we employ precision (P),\nrecall (R), and F1 score to assess the discriminative\n2117\nTable 1: Experimental results of text detector based on FT-RoBERTa(Fine-tuned RoBERTa), True-PPL(True\nPerplexity), and LLMDet(Proxy Perplexity). Their detection environments are respectively GPU-V100 32GB, CPU,\nand CPU.\nMetric Method Label of Text Source\nHuman GPT-2 OPT UniLM LLaMA BART T5 BLOOM GPT-Neo\nP(%)â†‘\nFT-RoBERTa 90.82 85.61 53.35 91.67 44.62 100.00 73.95 70.80 42.08\nTrue-PPL 97.97 98.54 98.25 79.02 98.54 98.94 89.77 94.41 97.09\nLLMDet 98.54 76.09 79.08 90.81 95.61 97.55 86.86 84.67 84.45\nR(%)â†‘\nFT-RoBERTa 94.00 58.05 81.48 73.41 95.24 94.18 27.98 27.09 18.22\nTrue-PPL 98.99 95.92 95.70 82.25 98.46 99.73 88.44 94.29 97.61\nLLMDet 99.00 78.13 73.88 91.74 97.30 98.41 87.56 83.08 83.90\nF1(%)â†‘\nFT-RoBERTa 92.38 69.19 64.48 81.53 60.77 97.00 40.60 39.19 25.13\nTrue-PPL 98.48 97.22 96.96 80.60 98.85 99.34 89.10 94.35 97.35\nLLMDet 98.77 77.09 76.39 91.27 96.44 97.98 87.21 83.87 84.18\nTable 2: Comparison of overall performance between text detectors based on true perplexity, fine-tuned RoBERTa,\nand proxy perplexity. Note: Ratio = (Macro-F1/Macro-F1True _PPL ) Â·(Time/Time Ture _PPL )\nMethod Macro-P(%) â†‘ Macro-R(%)â†‘ Macro-F1(%)â†‘ Time(s)â†“ Ratio â†‘\nTrue-PPL 94.72 94.60 94.65 46410.15 Ã—1.00\nFine-tuned RoBERTa 72.19 63.30 63.40 41799.76 Ã—0.74\nLLMDet 88.19 88.13 88.14 8678.76 Ã—4.97\nperformance of the text detector on each of LLMs\nand human-generated text. Additionally, F1-Macro,\nR@1, R@2, and R@3 metrics are used to analyze\nthe overall performance of the detector,\nF1i = 2PiRi\nPi + Ri\n, F1-Macro =\nâˆ‘N\ni=1 F1i\nN , (4)\nR@k =\nâˆ‘M\nj=1 IGjâˆˆKj\nM , (5)\nwhere Pi, Ri and F1i respectively represent the\nprecision, recall, and F1 score of Modeli. N de-\nnotes the total number of categories, M represents\nthe number of texts being tested. Gj represents\nthe ground label of Text j, Kj refers to the top-k\ncategories with the highest probabilities in the pre-\ndicted results, IGjâˆˆKj takes the value of 1 when\nGj âˆˆKj, and 0 otherwise.\n5.3 Research Quesitons\nBased on the characteristics and assumptions of\nour proposed detection tool in Â§ 3, we formulate\nfour research questions regarding LLMDet.\nâ€¢ RQ1: Can perplexity-based methods trace the\nsource of text from certain LLM?\nâ€¢ RQ2: How significant is the impact of the proxy\nperplexity-based approach on detection perfor-\nmance?\nâ€¢ RQ3: Can LLMDet achieve the expected level\nof efficiency compared to existing methods?\nâ€¢ RQ4: How is the extendibility of LLMDet\ndemonstrated?\n5.4 Experiments & Results\nWe conducted experimental verification for the\naforementioned raised questions.\nFor Specificity (RQ1) : We first compute the\ntrue perplexity of the combined datasets con-\nstructed in Â§ 5.1 on GPT-2, GPT-2-Large, OPT-\n1.3B, OPT-2.7B, UniLM, LLaMA-7B, BART, T5-\nBase, Bloom-650M and GPT-Neo-2.7B models.\nSubsequently, we joint these perplexity values to\ntrain a text classifier based on LightGBM (Ke et al.,\n2017).\nThe classifier is then tested, and the results are\npresented in Table 1. We observe that the text\ndetector based on true perplexity achieved excel-\nlent detection success rates when confronted with\ntexts generated by different models, with the ex-\nception of the generated texts by UniLM. Despite\n2118\nthe comparatively lower detection performance\nfor UniLM-generated texts, the F1 score reaches\n80.60%, which is significantly higher than random\nguessing. These experimental results robustly vali-\ndate the applicability of perplexity as a distinguish-\ning metric for models that identify specific sources\nof text.\nFor Safety (RQ2) : We utilize the statistical\ndatasets generated on GPT-2, GPT-2-Large, OPT-\n1.3B, OPT-2.7B, UniLM, LLaMA-7B, BART, T5-\nBase, Bloom-650M, and GPT-Neo-2.7B, as men-\ntioned in the Â§ 5.1, to construct dictionaries for\neach model using the method described in the Â§ 4.1.\nThen, we employ these dictionaries to calculate\nthe proxy perplexity of the combined dataset as\nfeatures for training a text classifier based on Light-\nGBM (Ke et al., 2017).\nThe classifier is then tested, and the results are\npresented in Table 1. Our proposed method based\non proxy perplexity achieves comparable results\nto the text detector based on real perplexity on\nHuman, LLaMA-generated, and BART-generated\ntexts, with detection success rates exceeding 95%.\nAdditionally, our method outperforms the true\nperplexity-based detector when it comes to de-\ntecting UniLM-generated texts. Furthermore, the\nF1 score for detecting texts from other sources is\nat least 76.39%, significantly higher than random\nguessing. Based on the confusion matrix in Figure\n2, it can be observed that there is a tendency for the\ntext generated by GPT-2 and OPT to be easily con-\nfused with each other, while text generated by T5,\nBloom, and GPT-Neo also exhibit a tendency to be\neasily confused. Although the overall performance\nis not as high as the real perplexity-based text clas-\nsifier, our proposed method does not require model\naccess during detection and offers advantages such\nas speed, scalability, and enhanced security.\nTo assess the comprehensive detection capability\nof the detector, we compute the F1-Macro, R@1,\nR@2 and R@3 values. From Table 2, it is evident\nthat our proposed method achieves an R@2 value\nof 98.00%. This indicates that, among the top two\ntext sources with the highest predicted probabilities,\nthere is typically one source that corresponds to the\ntrue source of the text.\nFor Efficiency (RQ3): In order to compare the\nefficiency of various methods, in addition to the\nmain experiment in Table 2, we also conduct tests\nusing the same set of 1000 texts to measure the\nefficiency required for detection in GPT-Zero, De-\nHumanGPT-2 OPT UniLMLLaMABART\nT5\nBloomGPT-Neo\nPrediction Label\nHuman\nGPT-2\nOPT\nUniLM\nLLaMA\nBART\nT5\nBloom\nGPT-Neo Ground Truth Label\n0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.00 0.77 0.15 0.02 0.04 0.00 0.01 0.00 0.00\n0.01 0.24 0.69 0.02 0.04 0.00 0.01 0.00 0.00\n0.01 0.04 0.01 0.90 0.02 0.01 0.02 0.00 0.00\n0.00 0.01 0.01 0.01 0.97 0.00 0.00 0.00 0.00\n0.00 0.00 0.00 0.00 0.00 0.98 0.02 0.00 0.00\n0.00 0.00 0.00 0.02 0.00 0.03 0.85 0.05 0.05\n0.00 0.00 0.00 0.00 0.00 0.00 0.06 0.81 0.12\n0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.13 0.81\n0.0\n0.2\n0.4\n0.6\n0.8\nFigure 2: The confusion matrix of the detection per-\nformed by LLMDet.\nTable 3: The detection time of GPT-Zero, De-\ntectGPT, and LLMDet on a dataset of 1000 texts.\nNote: Ratio = ( Accuracy/AccuracyTrue _PPL ) Â·\n(Time/Time Ture _PPL )\nMethod Accuracy(%)â†‘ Time(s)â†“ Ratio (True-PPL)â†‘\nGPT-Zero 86.56 2376.87 Ã—0.46\nDetectGPT 92.67 14354.61 Ã—0.08\nTrue-PPL 94.87 1199.11 Ã—1.00\nLLMDet 88.19 224.53 Ã—4.96\ntectGPT, True-PPL, and LLMDet. In terms of re-\nsource requirements, both DetectGPT and True-\nPPL methods are run on a V100-SXM-32GB, GPT-\nZero utilizes its API for detection on a GPU, while\nLLMDet only requires a GPU for the completion\nof the detection process.\nBased on the efficiency analysis in Table 2 and\nTable 3, it can be observed that LLMDet outper-\nforms other detection methods significantly. Fur-\nthermore, in terms of resource requirements, our\napproach exhibits the lowest demands. Conse-\nquently, our detection tool demonstrates a substan-\ntially higher efficiency compared to other methods,\nmaking it more aligned with future detection needs.\nFor Extendibility (RQ4): To illustrate the ex-\ntendibility of the LLMDet method, we expand\nits detection capability from one model to eight.\nSpecifically, We sequentially add the LLM model\ninto our LLMDet tool in the following sequence:\nGPT-2, LLaMA, OPT, UniLM, BART, T5, Bloom,\nand GPT-Neo. Thereby, continuously extending the\ndetection capability to these models. Additionally,\nwith each expansion, we retrain the text detector\n(LightGBM) and assessed the resultant changes in\noverall performance.\nFrom Figure 3, it can be observed that during the\nexpansion of LLMDet, there is only a slight fluc-\ntuation in the value of F1-Macro, which remains\n2119\n1 2 3 4 5 6 7 8\nThe number of LLMs\n65\n70\n75\n80\n85\n90\n95\n100F1-Macro(%)\n+GPT-2\n+LLaMA\n+OPT +UniLM\n+BART +T5 +Bloom+GPT-Neo\nFigure 3: The impact of sequentially adding the LLM\ninto LLMDet on the comprehensive detection perfor-\nmance measured by F1-Macro.\nconsistently around 85%. Therefore, it can be con-\ncluded that in the future, LLMDet can be easily\nexpanded to a new model with sightly performance\naffection.\nIn addition, in order to explore the performance\nchanges of LLMDet when using newer and larger\nLLM, we also conducted additional experiments.\nThe detailed experimental steps and results can be\nseen in Appendix B.\n6 Analysis\nIn this section, we conduct several additional exper-\niments to facilitate a more comprehensive analysis\nof LLMDet. Firstly, we verify the detection ro-\nbustness of LLMDet. Subsequently, we investigate\nthe impact of n-gram in dictionary construction on\nthe detection performance of LLMDet. Finally, we\nexplore the influence of the top-k of the next token\nsamples in dictionary construction on the detection\nperformance of LLMDet.\n6.1 The Robustness Testing of Detector\nMany LLMs can change their probability of the\nnext token via different methods, for example,\nchanging hyperparameters like temperature, or\neven updating weight by fine-tuning. Furthermore,\ngenerated text may encounter deliberate perturba-\ntion, such as random deletions. It is worth consid-\nering the robustness of this method in these situa-\ntions.\nFor hyperparameter changes, we use the ap-\nproach outlined in Â§ 5.1 of this article to generate\n16,000 text instances using LLaMA-7B at tempera-\ntures of 0.1, 0.4, 0.7, and 1.0 respectively.\nFor random deletion, we use the approach out-\nlined in Â§ 5.1 to generate 16,000 text instances\nusing LLaMA-7B. For the generated text, we set\nthe deletion rates at 0.1, 0.3, and 0.5, respectively,\nsubsequently introducing corresponding perturbed\ntexts by randomly removing words from the text\naccording to these specified rates.\nFor weight updates, we employ the approach\noutlined in Â§ 5.1 to generate 16,000 text instances\nusing the Vicuna-7B, an instruction fine-tuned ver-\nsion of LLaMA-7B.\nThese text instances are then utilized as test\ndata to assess the robustness of LLMDet, and the\nexperimental outcomes are presented in Table 4.\nLLMDet exhibits strong robustness against certain\ntypes of perturbations in the text, such as random\ndeletions, slight weight updates in the generative\nmodel, and adjustments to temperature settings.\nFor more analysis of experimental results, please\nsee Appendix C.\n6.2 The Influence of N-gram\nWe compute the proxy perplexity of each model for\nthe combined dataset in the Â§ 4.1 using dictionaries\nbuilt on 2-gram, 3-gram, and 4-gram, respectively.\nBy jointly analyzing the proxy perplexities to train\nand test the text classifier using the LightGBM. It\nshould be noted that (n-1)-gram are a subset of\nn-gram. Based on the results shown in Table 5, it\ncan be observed that the overall detection perfor-\nmance of text within the domain does not increase\nsignificantly as the value of n increases, but rather\nexhibits a slight improvement. Considering that\nthe number of n-gram increases exponentially as n\nincreases, we only consider 4-gram in LLMDet.\n6.3 Next Token Top- K Sampling\nThe construction of the dictionary incurs significant\nstorage overhead due to the necessity of storing the\ntop-K probabilities along with their correspond-\ning n-gram, presenting a challenge to our method.\nConsequently, determining the optimal value of\nK requires a comprehensive consideration of both\ndetection performance and storage costs.\nIn order to gain a more intuitive understanding\nof the impact of the K value on the detection per-\nformance of LLMDet, while keeping the number\nof 2-gram, we solely vary theK value and examine\nthe changes in F1-Macro of LLMDet across differ-\nent K values. The result is presented in Figure 4.\nWe observe that as the value of K increases,\nthe detection performance of LLMDet gradually\nimproves. However, the performance improvement\n2120\nTable 4: The detection performance of LLMDet in three scenarios: temperature changes, random deletion, and\nweight updates.\nMetric (%) Temperature Delete Ratio Weight Update\n0.1 0.4 0.7 1.0 0.1 0.3 0.5 Fine-tuned LLaMA(Vicuna)\nR@1(Accuracy)â†‘ 91.23 92.05 93.02 94.37 90.06 89.31 87.80 97.78\nR@2â†‘ 97.55 97.48 97.48 98.06 97.07 99.12 99.53 99.07\nR@3â†‘ 99.46 99.33 99.24 99.33 99.52 99.53 99.82 99.39\nTable 5: The impact of the value of n in n-gram on the\noverall detection performance.\nMetric (%) 2-gram 3-gram 4-gram\nF1-Macro â†‘ 87.44 87.79 88.14\nR@1 â†‘ 89.61 90.18 89.51\nR@2 â†‘ 97.84 98.04 98.00\nR@3 â†‘ 99.58 99.56 99.64\n500 1000 1500 2000\nThe K of top-K samples in Next Token Sampling\n40\n60\n80\n100F1-Macro(%)\nFigure 4: The impact of the K value in top-K sampling\nof 2-gram on the detection performance of LLMDet.\nbecomes less pronounced after K reaches 1500.\nNonetheless, the corresponding storage overhead\nstill increases linearly. Therefore, considering the\noverall trade-off between detection performance\nand storage cost, we recommend adopting a top-\n2000 sampling for 2-gram. For 3-gram and 4-gram,\ntheir quantities are immense. Therefore, following\nthe completion of similar experimental analyses,\nwe employ a top-100 sampling for these n-gram .\n7 Conclusions and Future Work\nIn the era dominated by machine-generated text,\nthere is a growing need for an efficient and se-\ncure detection tool. However, existing detection\nmethods typically require interaction with language\nmodels, which inherently compromises speed and\nsecurity. Our proposed detection tool, LLMDet,\novercomes these limitations by leveraging pre-\nmined prior probability information to compute\nproxy perplexity, ensuring both speed and secu-\nrity in the detection process. Additionally, our\nmethod enables text tracking, allowing for the iden-\ntification of the underlying language model from\nwhich the text originates. Importantly, our detec-\ntion tool can be continuously enhanced by expand-\ning to new open-source LLMs, enabling ongoing\nimprovements.\nIn the future, we aim to further refine our de-\ntection tool. Firstly, we will improve the dictio-\nnaries used to compute proxy perplexity, thereby\nenhancing the detection performance. Secondly,\nfor closed-source models, we are unable to build\ntheir corresponding dictionaries. To mitigate it to\nsome extent, we have considered two possible ap-\nproaches:\n1) In the process of implementing LLMDet, we\noffer not only detection capabilities but also an ex-\ntensible interface for closed-source model owners.\nDetails about this implementation can be found in\nAlgorithm 1 of Appendix A. The extended inter-\nface aims to secure the model effectively without\ncompromising the interests of the model owners.\nThrough this approach, we hope to encourage more\nclosed-source model owners to participate and con-\ntribute to the continuous improvement of the detec-\ntion ecosystem of LLMDet.\n2) We have also explored using statistical tech-\nniques to estimate the next-token probability in pro-\nprietary commercial models. However, due to lim-\nited data volume, achieving the anticipated results\nhas been challenging. Additionally, generating a\nsignificant amount of statistical data comes with\nconsiderable costs. As a result, we have included\nthis approach on our list of future work items.\nFurthermore, the distillation method is a valu-\nable avenue for future exploration. We will cer-\ntainly consider it in our future research endeavors.\nLimitations\nOne of the limitations of the current LLMDet is its\nrestriction to detecting English text, thus unable to\ndetect text in other languages. In the future, we can\n2121\nextend our approach to encompass models for other\nlanguages, thereby equipping it with the capability\nto detect text in diverse languages.\nFurthermore, at present, the number of models\ndetectable by LLMDet is limited. We will expand\nthe capabilities of our detection tool to encompass\na broader range of models, providing more possi-\nbilities for text tracing and attribution.\nEthics Statement\nWe honor and support the ethical guidelines of\nEMNLP. This paper primarily focuses on the detec-\ntion of text generated by LLMs, aiming to construct\na detection tool suitable for the user base from var-\nious domains. The tool is designed to efficiently\nand securely perform text detection to prevent the\nmisuse of generated text. Overall, our approach ex-\nhibits advantages over previous methods in terms\nof efficiency and granularity of detection, making\nthis work meaningful. Additionally, the datasets\nused in this study are sourced from previously pub-\nlished works and do not involve any privacy or\nethical concerns.\nAcknowledgements\nThis work was supported by the National Key\nR&D Program of China (2022YFB3103700,\n2022YFB3103704), the National Natural Science\nFoundation of China (NSFC) under Grants No.\n62276248, and the Youth Innovation Promotion\nAssociation CAS under Grants No. 2023111.\nReferences\nSahar Abdelnabi and Mario Fritz. 2021. Adversarial wa-\ntermarking transformer: Towards tracing text prove-\nnance with data hiding. In 2021 IEEE Symposium on\nSecurity and Privacy (SP), pages 121â€“140. IEEE.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, and Sia-\nmak Shakeri et al. 2023. Palm 2 technical report.\nTanya Aplin and Giulia Pasqualetto. 2019. Artificial\nintelligence and copyright protection. Chapter in\nRosa Maria Ballardini, Petri KuoppamÃ¤ki, Olli PitkÃ¤-\nnen (eds) Regulating Industrial Internet Through\nIPR, Data Protection and Competition Law (Kluwer,\n2019).\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng,\nMarcâ€™Aurelio Ranzato, and Arthur Szlam. 2019.\nReal or fake? learning to discriminate machine from\nhuman generated text.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2022. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow,\n2021. URL: https://doi. org/10.5281/zenodo ,\n5297715.\nSouradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu,\nBang An, Dinesh Manocha, and Furong Huang. 2023.\nOn the possibilities of ai-generated text detection.\nDebby RE Cotton, Peter A Cotton, and J Reuben Ship-\nway. Chatting and cheating: Ensuring academic in-\ntegrity in the era of chatgpt.\nLong Dai, Jiarong Mao, Xuefeng Fan, and Xiaoyi Zhou.\n2022. Deephider: A multi-module and invisibility\nwatermarking scheme for language model. arXiv\npreprint arXiv:2208.04676.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. Advances in neural information process-\ning systems, 32.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320â€“335.\nTiziano Fagni, Fabrizio Falchi, Margherita Gambini, An-\ntonio Martella, and Maurizio Tesconi. 2021. Tweep-\nfake: About detecting deepfake tweets. Plos one,\n16(5):e0251415.\nLeon FrÃ¶hling and Arkaitz Zubiaga. 2021. Feature-\nbased detection of automated language models: tack-\nling gpt-2, gpt-3 and grover.PeerJ Computer Science,\n7:e443.\nSebastian Gehrmann, Hendrik Strobelt, and Alexan-\nder M. Rush. 2019. Gltr: Statistical detection and\nvisualization of generated text.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How close is chatgpt to human experts?\ncomparison corpus, evaluation, and detection. arXiv\npreprint arxiv:2301.07597.\nRyuichiro Hataya, Han Bao, and Hiromi Arai. 2022.\nWill large-scale generative models corrupt future\ndatasets? arXiv preprint arXiv:2211.08095.\nTim Jansen, Yangling Tong, Victoria Zevallos, and Pe-\ndro Ortiz Suarez. 2022. Perplexed by quality: A\nperplexity-based method for adult and harmful con-\ntent detection in multilingual heterogeneous web data.\narXiv preprint arXiv:2212.10440.\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang,\nWei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.\n2122\n2017. Lightgbm: A highly efficient gradient boost-\ning decision tree. Advances in neural information\nprocessing systems, 30.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen,\nJonathan Katz, Ian Miers, and Tom Goldstein. 2023.\nA watermark for large language models.\nKeita Kurita, Paul Michel, and Graham Neubig. 2020.\nWeight poisoning attacks on pretrained models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2793â€“\n2806, Online. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nJing Liu, Xinxin Zhu, Fei Liu, Longteng Guo, Zijia\nZhao, Mingzhen Sun, Weining Wang, Hanqing Lu,\nShiyu Zhou, Jiajun Zhang, et al. 2021. Opt: Omni-\nperception pre-trainer for cross-modal understanding\nand generation. arXiv preprint arXiv:2107.00249.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky,\nChristopher D. Manning, and Chelsea Finn. 2023.\nDetectgpt: Zero-shot machine-generated text detec-\ntion using probability curvature.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Donâ€™t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797â€“1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nLiang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengx-\nian Wan, and Xueqi Cheng. 2016. Text matching\nas image recognition. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 30.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485â€“5551.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383â€“2392, Austin,\nTexas. Association for Computational Linguistics.\nVinu Sankar Sadasivan, Aounon Kumar, Sriram Bala-\nsubramanian, Wenxiao Wang, and Soheil Feizi. 2023.\nCan ai-generated text be reliably detected?\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili Â´c, Daniel Hesslow, Roman\nCastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon,\nMatthias GallÃ©, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Rad-\nford, Gretchen Krueger, Jong Wook Kim, Sarah\nKreps, et al. 2019. Release strategies and the so-\ncial impacts of language models. arXiv preprint\narXiv:1908.09203.\nRuixiang Tang, Yu-Neng Chuang, and Xia Hu. 2023.\nThe science of detecting llm-generated texts.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,\nBaptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,\nRashi Rungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian, Xi-\naoqing Ellen Tan, Binh Tang, Ross Taylor, Adina\nWilliams, Jian Xiang Kuan, Puxin Xu, and Zheng\nYan. 2023b. Llama 2: Open foundation and fine-\ntuned chat models.\nAdaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee.\n2020. Authorship attribution for neural text gener-\nation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 8384â€“8395, Online. Association for\nComputational Linguistics.\nHonai Ueoka, Yugo Murawaki, and Sadao Kuro-\nhashi. 2021. Frustratingly easy edit-based linguistic\nsteganography with a masked language model. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5486â€“5492, Online. Association for Computa-\ntional Linguistics.\n2123\nA Algorithm of LLMDet\nFor the detailed implementation process of\nLLMDetd, please refer to the pseudocode provided\nbelow. Algorithm 1 is a dictionary construction al-\ngorithm that is completed offline by us or provided\nto the model holder independently of external sys-\ntems. Algorithm 2 will be provided to users as a\nthird-party tool.\nAlgorithm 1: Dictionary Construction\nInput : A prompt dataset P\nA large language model M\nOutput :A dictionary D for M\n// Step1: Generate text samples\nProcedure GenerationText(M, P)\n// T is a generation text set\nT â†âˆ… ;\nfor x in P do\n// Use M to generate text\nt â†M.generate(x) ;\nT.append(t) ;\nend\nreturn T\n// Step2: Word frequency statistic\nProcedure WordStatistic(T, n)\n// Do counter for text sequence\nto get top-k n-gram\nn-gram â†CountNgram(T, n, k) ;\nreturn n-gram\n// Step3: Next token probability\nsampling\nProcedure NextTokenSampling(M, P, K)\n// DM stores information for\nmodel M\nDM â†âˆ… is empty list;\nT â†GenerationText(M, P) ;\nfor n in {2, 3, 4}do\nDn is an empty dictionary ;\nn-gram â†WordStatistic(T, n) ;\nfor s in n-gram do\npw â†M.next_token(s[1:nâˆ’1]) ;\nDn.add({s[1:nâˆ’1] : pw\n[1:K]}) ;\nend\nDM .append(Dn) ;\nend\nreturn DM\nAlgorithm 2: Text Detection\nInput : A piece of text t for detecting\nA list D = [DM0 , . . . , DMc ] for c\nLLMs and DM0 denotes human\nOutput :A detection result R\n// Step4: Proxy perplexity\nestimation\nProcedure ProxyPerplexity(t, DM )\n// Ngram () generate one text\nspan in t with length n\nfor s, n in Ngram (t) do\nGet Dn from DM ;\nif s[1:nâˆ’1] in Dn then\np â†- log(Dn.index(s[1:nâˆ’1]));\nProxy_PPL += p ;\nend\nend\nreturn Proxy_PPL\n// Step5: Result ranking\nProcedure Rank(t, D)\nfor i in {0, . . . , c}do\nPPLi\nâ†ProxyPerplexity(t, DMi );\nend\np â†Classifier([PPL0, . . . ,PPLc]) ;\nËœp â†smooth(p) ;\nË†p â†softmat(Ëœp) ;\nR â†sort(Ë†p) ;\nreturn R\nB LLMDet Using Newer and Larger\nLLM\nIn order to explore whether the gap between proxy\nperplexity (our method) and true perplexity be-\ncomes more apparent as the size of LLMs increases,\nwe conduct additional experiments. We replace\nLLaMA-7B with LLaMA2-13B (Touvron et al.,\n2023b) while keeping all other experimental set-\ntings the same as in the original paper. The detailed\nexperimental results are shown in Table 6 and Ta-\nble 7.\nFrom the experimental results, when we re-\nplace the original LLM with a better-performing\nand larger-scale LLM, such as replacing LLaMA-\n7B with LLaMA2-13B, the detection performance\nremains essentially consistent with the original\nperformance. This indicates that when a better-\nperforming and larger-size LLM is used, the perfor-\nmance gap between proxy perplexity (our method)\n2124\nTable 6: Experimental results of text detector based on proxy perplexity with LLaMA-7B, and proxy perplexity\nwith LLaMA2-13B.\nMetric (%) Model Size Label of Text Source\nHuman GPT-2 OPT UniLM LLaMA BART T5 BLOOM GPT-Neo\nPâ†‘\nLLMDet with LLaMA-7B 98.54 76.09 79.08 90.81 95.61 97.55 86.86 84.67 84.45\nLLMDet with LLaMA2-13B 98.45 74.67 79.97 91.47 95.70 97.46 86.64 83.15 84.60\nTrue perplexity 97.97 98.54 98.25 79.02 98.54 98.94 89.77 94.41 97.09\nRâ†‘\nLLMDet with LLaMA-7B 99.00 78.13 73.88 91.74 97.30 98.41 87.56 83.08 83.90\nLLMDet with LLaMA2-13B 99.04 79.07 72.64 90.53 97.49 98.11 86.99 83.15 83.98\nTrue perplexity 98.99 95.92 95.70 82.25 98.46 99.73 88.44 94.29 97.61\nF1â†‘\nLLMDet with LLaMA-7B 98.77 77.09 76.39 91.27 96.44 97.98 87.21 83.87 84.18\nLLMDet with LLaMA2-13B 98.74 76.80 76.13 91.00 96.58 97.78 86.82 83.47 84.29\nTrue perplexity 98.48 97.22 96.96 80.60 98.85 99.34 89.10 94.35 97.35\nTable 7: Comparison of overall performance between text detectors based on proxy perplexity with LLaMA-7B and\nproxy perplexity with LLaMA2-13B.\nMethod Macro-F1(%) â†‘ R1(ACC)(%) â†‘ R2(%) â†‘ R3(%) â†‘\nproxy perplexity with LLaMA-7B 88.14 89.51 98.00 99.64\nproxy perplexity with LLaMA2-13B 87.96 89.30 98.07 99.69\nTrue perplexity 94.65 95.54 99.33 99.80\nand true perplexity does not become more obvious.\nC Additional Analysis for Robustness\nTesting\nFrom Table 4, it can be observed that as the tem-\nperature increases, the accuracy of text generation\ndetection improves.\nRegarding this phenomenon, what we need to\nclarify is that our method calculates proxy perplex-\nity by building a dictionary based on the probability\nof sampling the next token. When calculating the\nprobability of the next token, we directly use the\nsoftmax with a default temperature of 1.0. When\nthe temperature of LLM is set to 1.0, the generated\ntext actually conforms more closely to the probabil-\nity distribution of the next token in the dictionary\nwe have constructed. At this point, the calculated\nproxy perplexity is closer to the true perplexity, re-\nsulting in higher detection accuracy. Therefore, we\ncan observe that when the temperature is higher,\nthe text distribution generated by the LLM is closer\nto the probability distribution of the next token in\nthe dictionary, leading to higher detection accuracy.\nD Sample of n-gram for each LLM\nSpecific examples of 2-gram, 3-gram, and 4-gram\nfor each LLM can be referred to in the tables. Ta-\nble 8 shows the samples for GPT-2. Table 9 shows\nthe samples for OPT. Table 10 shows the samples\nfor LLaMA. Table 11 shows the samples for T5.\nTable 12 shows the samples for UniLM. Table 13\nshows the samples for BART. Table 14 shows the\nsamples for GPT-Neo. Table 15 shows the samples\nfor Bloom.\n2125\nTable 8: Samples of n-gram for GPT-2.\n2-gram 3-gram 4-gram\n(â€™notâ€™, â€™normalâ€™) (â€™onâ€™, â€™aâ€™, â€™robustâ€™) (â€™toâ€™, â€™theâ€™, â€™allegedâ€™, â€™healthâ€™)\n(â€™rapeâ€™, â€™herâ€™) (â€™makeâ€™, â€™thoseâ€™, â€™customerâ€™) (â€™onâ€™, â€™paceâ€™, â€™forâ€™, â€™anâ€™)\n(â€™toâ€™, â€™agreeâ€™) (â€™mentalâ€™, â€™healthâ€™, â€™clinicâ€™) (â€™receptionâ€™, â€™atâ€™, â€™theâ€™, â€™BBCâ€™)\n(â€™politicalâ€™, â€™campaigningâ€™) (â€™towerâ€™, â€™alsoâ€™, â€™featuresâ€™) (â€™theâ€™, â€™ruleâ€™, â€™toâ€™, â€™meanâ€™)\n(â€™withâ€™, â€™patientsâ€™) (â€™lostâ€™, â€™bothâ€™, â€™matchesâ€™) (â€™toâ€™, â€™getâ€™, â€™awayâ€™, â€™withâ€™)\n(â€™theâ€™, â€™Commonâ€™) (â€™ofâ€™, â€™soccerâ€™, \"â€™\") (â€™sheâ€™, â€™wasâ€™, â€™gâ€™, â€™oredâ€™)\n(â€™privateâ€™, â€™menâ€™) (â€™haveâ€™, â€™beenâ€™, â€™accusedâ€™) (â€™theâ€™, â€™areaâ€™, â€™.]â€™, â€™Câ€™)\n(â€™wereâ€™, â€™victoriousâ€™) (â€™formsâ€™, â€™ofâ€™, â€™disciplineâ€™) (â€™worldâ€™, â€™economicâ€™, â€™crisisâ€™, â€™,â€™)\n(â€™youngâ€™, â€™teamâ€™) (â€™manâ€™, â€™whoâ€™, â€™mayâ€™) (â€™toâ€™, â€™Argentinaâ€™, â€™)â€™, â€™onâ€™)\n(â€™saidâ€™, â€™Seâ€™) (â€™guessingâ€™, â€™youâ€™, â€™mightâ€™) (â€™peopleâ€™, â€™whoâ€™, â€™areâ€™, â€™loyalâ€™)\n(â€™orâ€™, â€™suspectsâ€™) (â€™theâ€™, â€™courtâ€™, â€™ruledâ€™) (â€™partnersâ€™, â€™.â€™, â€™Câ€™, â€™Gâ€™)\n(â€™soundâ€™, â€™energyâ€™) (â€™serviceâ€™, â€™.â€™, â€™Iâ€™) (â€™ourâ€™, â€™communityâ€™, â€™fromâ€™, â€™thisâ€™)\n(â€™sheâ€™, â€™beatâ€™) (â€™physicalâ€™, â€™.â€™, â€™Ifâ€™) (â€™trainâ€™, â€™youngâ€™, â€™boysâ€™, â€™aboutâ€™)\n(â€™politicalâ€™, â€™careerâ€™) (â€™thatâ€™, â€™Mexicansâ€™, â€™whoâ€™) (â€™theâ€™, â€™problemâ€™, â€™.â€™, â€™\"â€™)\n(â€™notâ€™, â€™proposeâ€™) (â€™oneâ€™, â€™.â€™, â€™Weâ€™) (â€™toâ€™, â€™Barbâ€™, â€™adosâ€™, â€™onâ€™)\n(â€™todayâ€™, â€™acceptedâ€™) (â€™shareâ€™, â€™ofâ€™, â€™onlineâ€™) (â€™themâ€™, â€™.â€™, â€™Iâ€™, \"â€™m\")\n(â€™wasâ€™, â€™slowâ€™) (â€™wasâ€™, â€™walkingâ€™, â€™withâ€™) (â€™ofâ€™, â€™theâ€™, â€™UEFAâ€™, â€™presidentialâ€™)\n(â€™receiveâ€™, â€™governmentâ€™) (â€™theâ€™, â€™largestâ€™, â€™everâ€™) (â€™questionedâ€™, â€™forâ€™, â€™aâ€™, â€™weekâ€™)\n(â€™vâ€™, â€™Arsenalâ€™) (â€™theâ€™, â€™sharkâ€™, â€™,â€™) (â€™thinkâ€™, â€™thisâ€™, â€™isâ€™, â€™incorrectâ€™)\n(â€™theâ€™, â€™notesâ€™) (â€™toâ€™, â€™haveâ€™, â€™abusedâ€™) (â€™whoâ€™, â€™hadâ€™, â€™beenâ€™, â€™shotâ€™)\n(â€™smoothâ€™, â€™andâ€™) (â€™inquestâ€™, â€™isâ€™, â€™nowâ€™) (â€™otherâ€™, â€™animalsâ€™, â€™thatâ€™, â€™haveâ€™)\n(â€™theirâ€™, â€™viewsâ€™) (â€™meetâ€™, â€™Presidentâ€™, â€™Tsâ€™) (â€™saleâ€™, â€™onâ€™, â€™theâ€™, â€™marketâ€™)\n(â€™powerfulâ€™, â€™laserâ€™) (â€™inâ€™, â€™theâ€™, â€™processionâ€™) (â€™struckâ€™, â€™ourâ€™, â€™cityâ€™, â€™.â€™)\n(â€™perspectiveâ€™, â€™\"â€™) (â€™thatâ€™, â€™isâ€™, â€™litteredâ€™) (â€™onâ€™, â€™Skyâ€™, â€™Sportsâ€™, â€™1â€™)\n(â€™wasâ€™, â€™introducedâ€™) (â€™ownâ€™, â€™rightâ€™, â€™whichâ€™) (â€™ofâ€™, â€™theâ€™, â€™filmâ€™, â€™hasâ€™)\n(â€™saidâ€™, â€™Theresaâ€™) (â€™justiceâ€™, â€™isâ€™, â€™fairâ€™) (â€™thatâ€™, â€™isâ€™, â€™madeâ€™, â€™forâ€™)\n(â€™trainâ€™, â€™.â€™) (â€™itâ€™, â€™wasâ€™, â€™beforeâ€™) (â€™whereâ€™, â€™Richardâ€™, â€™IIIâ€™, â€™wasâ€™)\n(â€™wasâ€™, â€™initiallyâ€™) (â€™toâ€™, â€™getâ€™, â€™landâ€™) (â€™teachingâ€™, â€™childrenâ€™, â€™aboutâ€™, â€™\"â€™)\n(â€™toâ€™, â€™rectâ€™) (â€™hasâ€™, â€™remainedâ€™, â€™silentâ€™) (â€™winnerâ€™, â€™willâ€™, â€™receiveâ€™, â€™aâ€™)\n(â€™nowâ€™, â€™noâ€™) (â€™matchâ€™, â€™Câ€™, â€™-â€™) (â€™theâ€™, â€™stateâ€™, â€™governmentâ€™, â€™dissolvedâ€™)\n(â€™theirâ€™, â€™tuitionâ€™) (â€™inâ€™, â€™theseâ€™, â€™jobsâ€™) (â€™theâ€™, â€™moneyâ€™, â€™,â€™, â€™peopleâ€™)\n(â€™seasâ€™, â€™offâ€™) (â€™installedâ€™, â€™withâ€™, â€™properâ€™) (â€™theâ€™, â€™Irishâ€™, â€™Cupâ€™, â€™inâ€™)\n(â€™playâ€™, â€™Missâ€™) (â€™seasonâ€™, â€™atâ€™, â€™Villaâ€™) (â€™onâ€™, â€™theâ€™, â€™Rightsâ€™, â€™ofâ€™)\n(â€™thenâ€™, â€™punchedâ€™) (â€™sameâ€™, â€™venueâ€™, â€™(â€™) (â€™shoreâ€™, â€™lineâ€™, â€™.â€™, â€™Itâ€™)\n(â€™patientsâ€™, â€™livingâ€™) (â€™whenâ€™, â€™aâ€™, â€™reportâ€™) (â€™productionâ€™, â€™companyâ€™, â€™.â€™, â€™Câ€™)\n(â€™notâ€™, â€™rightâ€™) (â€™thatâ€™, â€™Mrsâ€™, â€™Matâ€™) (â€™playâ€™, â€™aâ€™, â€™movieâ€™, â€™,â€™)\n(â€™spinâ€™, â€™aâ€™) (â€™greatâ€™, â€™workâ€™, â€™weâ€™) (â€™rememberâ€™, â€™thatâ€™, â€™midâ€™, â€™-â€™)\n(â€™usualâ€™, â€™\"â€™) (â€™theâ€™, â€™groupâ€™, â€™ofâ€™) (â€™toâ€™, â€™sayâ€™, â€™thatâ€™, â€™yourâ€™)\n(â€™pushâ€™, â€™theirâ€™) (â€™forâ€™, â€™allâ€™, â€™involvedâ€™) (â€™takenâ€™, â€™itâ€™, â€™downâ€™, â€™.â€™)\n(â€™zooâ€™, â€™orâ€™) (â€™iâ€™, â€™hâ€™, â€™lfâ€™) (â€™thatâ€™, â€™Eâ€™, â€™Onâ€™, â€™continuesâ€™)\n(â€™wasâ€™, â€™talkingâ€™) (â€™onâ€™, â€™allâ€™, â€™hisâ€™) (â€™toâ€™, â€™aâ€™, â€™harmfulâ€™, â€™retributionâ€™)\n(â€™obviousâ€™, â€™:â€™) (â€™timeâ€™, â€™.â€™, â€™Theyâ€™) (â€™stillâ€™, â€™inâ€™, â€™collegeâ€™, â€™inâ€™)\n(â€™saidâ€™, â€™Pacâ€™) (â€™newâ€™, \"â€™s\", â€™ocialâ€™) (â€™violenceâ€™, â€™inâ€™, â€™theâ€™, â€™gayâ€™)\n(â€™overflowâ€™, â€™laneâ€™) (â€™theâ€™, â€™pubâ€™, â€™inâ€™) (â€™withâ€™, â€™himâ€™, â€™.â€™, â€™Hisâ€™)\n(â€™theâ€™, â€™Gibraltarâ€™) (â€™letâ€™, â€™outâ€™, â€™\"â€™) (â€™whereâ€™, â€™theyâ€™, â€™trainâ€™, â€™forâ€™)\n(â€™showedâ€™, â€™greatâ€™) (â€™wasâ€™, â€™releasedâ€™, â€™onâ€™) (â€™ourâ€™, â€™financesâ€™, â€™.â€™, â€™Weâ€™)\n(â€™thatâ€™, â€™yearâ€™) (â€™theâ€™, â€™coupleâ€™, â€™withâ€™) (â€™toâ€™, â€™ensureâ€™, â€™theirâ€™, â€™successâ€™)\n(â€™yoursâ€™, â€™,â€™) (â€™isâ€™, â€™atâ€™, â€™.\"â€™) (â€™theâ€™, â€™stolenâ€™, â€™carâ€™, â€™,â€™)\n(â€™ofâ€™, â€™investmentsâ€™) (â€™haveâ€™, â€™obtainedâ€™, â€™aâ€™) (â€™toâ€™, â€™preventâ€™, â€™futureâ€™, â€™attacksâ€™)\n(â€™otherâ€™, â€™hintsâ€™) (â€™toâ€™, â€™letâ€™, â€™itâ€™) (â€™ourâ€™, â€™seriesâ€™, â€™andâ€™, â€™willâ€™)\n2126\nTable 9: Samples of n-gram for OPT.\n2-gram 3-gram 4-gram\n(â€™specificâ€™, â€™typesâ€™) (â€™fromâ€™, â€™Samâ€™, â€™Jonesâ€™) (â€™withinâ€™, â€™theâ€™, â€™openingâ€™, â€™fewâ€™)\n(â€™withâ€™, â€™paymentsâ€™) (â€™industryâ€™, â€™toâ€™, â€™getâ€™) (â€™theâ€™, â€™servicesâ€™, â€™ofâ€™, â€™theâ€™)\n(â€™producerâ€™, â€™forâ€™) (â€™thenâ€™, â€™theâ€™, â€™Countyâ€™) (â€™whetherâ€™, â€™theâ€™, â€™policeâ€™, â€™watchdogâ€™)\n(â€™wasâ€™, â€™nakedâ€™) (â€™sabâ€™, â€™erâ€™, â€™instructorâ€™) (â€™sportâ€™, â€™interestingâ€™, â€™toâ€™, â€™mostâ€™)\n(â€™placeâ€™, â€™playâ€™) (â€™theâ€™, â€™economicâ€™, â€™impactâ€™) (â€™showsâ€™, â€™thatâ€™, â€™thereâ€™, \"â€™s\")\n(â€™visibleâ€™, â€™ethnicâ€™) (â€™theâ€™, â€™surgeonâ€™, â€™actuallyâ€™) (â€™racialâ€™, â€™discriminationâ€™, â€™.â€™, â€™Theâ€™)\n(â€™trainedâ€™, â€™byâ€™) (â€™isâ€™, â€™aâ€™, â€™wickedâ€™) (â€™muchâ€™, â€™betterâ€™, â€™atâ€™, â€™thisâ€™)\n(â€™placedâ€™, â€™aâ€™) (â€™tenâ€™, â€™yearsâ€™, â€™.â€™) (â€™ofâ€™, â€™leavingâ€™, â€™theâ€™, â€™clubâ€™)\n(â€™responsibilityâ€™, â€™\"â€™) (â€™oneâ€™, â€™byâ€™, â€™aâ€™) (â€™threadâ€™, â€™.â€™, â€™Câ€™, â€™Câ€™)\n(â€™situationâ€™, â€™fromâ€™) (â€™itâ€™, â€™wasâ€™, â€™Lawrenceâ€™) (â€™shipâ€™, â€™isâ€™, â€™.â€™, â€™Iâ€™)\n(â€™theirâ€™, â€™bestâ€™) (â€™readâ€™, â€™aâ€™, â€™particularâ€™) (â€™oldâ€™, â€™keepâ€™, â€™isâ€™, â€™theâ€™)\n(â€™recommendâ€™, â€™buyingâ€™) (â€™forâ€™, â€™oneâ€™, â€™gameâ€™) (â€™protectedâ€™, â€™fromâ€™, â€™overâ€™, â€™fâ€™)\n(â€™seenâ€™, â€™sinceâ€™) (â€™veryâ€™, â€™strongâ€™, â€™turnoutâ€™) (â€™momentâ€™, â€™theâ€™, â€™drillâ€™, â€™sergeantâ€™)\n(â€™yearâ€™, â€™byâ€™) (â€™heâ€™, â€™isâ€™, â€™aâ€™) (â€™thingsâ€™, â€™.â€™, â€™Andâ€™, â€™theyâ€™)\n(â€™ofâ€™, â€™Mosulâ€™) (â€™privateâ€™, â€™collegeâ€™, â€™shouldâ€™) (â€™startedâ€™, â€™collaboratingâ€™, â€™toâ€™, â€™solveâ€™)\n(â€™recentâ€™, â€™videoâ€™) (â€™thenâ€™, â€™comeâ€™, â€™backâ€™) (â€™theâ€™, â€™Guardianâ€™, â€™lastâ€™, â€™yearâ€™)\n(â€™oâ€™, â€™lympâ€™) (â€™idiotâ€™, â€™.â€™, â€™Thisâ€™) (â€™otherâ€™, â€™candidatesâ€™, â€™.â€™, â€™Câ€™)\n(â€™toâ€™, â€™Selâ€™) (â€™onâ€™, â€™toâ€™, â€™herâ€™) (â€™touristâ€™, â€™wasâ€™, â€™missingâ€™, â€™,â€™)\n(â€™requestsâ€™, â€™toâ€™) (â€™plâ€™, â€™umesâ€™, â€™wasâ€™) (â€™outfitâ€™, â€™forâ€™, â€™aâ€™, â€™weddingâ€™)\n(â€™thatâ€™, â€™fourâ€™) (â€™materialâ€™, â€™.â€™, â€™Heâ€™) (â€™whoâ€™, â€™Marâ€™, â€™lowâ€™, â€™eâ€™)\n(â€™patternâ€™, â€™.â€™) (â€™doctorâ€™, â€™prescribedâ€™, â€™meâ€™) (â€™seenâ€™, â€™usedâ€™, â€™onâ€™, â€™theâ€™)\n(â€™ultimateâ€™, â€™opportunâ€™) (â€™toâ€™, â€™winâ€™, â€™promotionâ€™) (â€™managedâ€™, â€™toâ€™, â€™maintainâ€™, â€™myâ€™)\n(â€™youâ€™, â€™ifâ€™) (â€™gotâ€™, â€™noâ€™, â€™problemâ€™) (â€™ofâ€™, â€™theâ€™, â€™Londonâ€™, â€™bombingsâ€™)\n(â€™teamâ€™, â€™playersâ€™) (â€™publishedâ€™, â€™inâ€™, â€™Februaryâ€™) (â€™runâ€™, â€™.â€™, â€™Theâ€™, â€™problemâ€™)\n(â€™quickerâ€™, â€™.â€™) (â€™wantâ€™, â€™toâ€™, â€™moveâ€™) (â€™thisâ€™, â€™movieâ€™, â€™isâ€™, â€™completeâ€™)\n(â€™systemsâ€™, â€™\"â€™) (â€™offenceâ€™, â€™ofâ€™, â€™\"â€™) (â€™theâ€™, â€™frontâ€™, â€™doorâ€™, â€™toâ€™)\n(â€™teamâ€™, â€™cultureâ€™) (â€™withoutâ€™, â€™underminingâ€™, â€™theâ€™) (â€™pageâ€™, â€™ofâ€™, â€™theâ€™, â€™Hongâ€™)\n(â€™sonâ€™, â€™awayâ€™) (â€™eventâ€™, â€™onâ€™, â€™Februaryâ€™) (â€™vandalismâ€™, â€™withoutâ€™, â€™theâ€™, â€™peopleâ€™)\n(â€™themselvesâ€™, â€™forâ€™) (â€™notâ€™, â€™officiallyâ€™, â€™recognisedâ€™) (â€™theâ€™, â€™energyâ€™, â€™sectorâ€™, â€™inâ€™)\n(â€™yourâ€™, â€™schoolâ€™) (â€™thanâ€™, â€™aâ€™, â€™typicalâ€™) (â€™naturalâ€™, â€™selectionâ€™, â€™.â€™, â€™Sometimesâ€™)\n(â€™windingâ€™, â€™roadsâ€™) (â€™firstâ€™, â€™.â€™, â€™Ifâ€™) (â€™theâ€™, â€™sameâ€™, â€™stadiumâ€™, â€™andâ€™)\n(â€™ofâ€™, â€™Idlibâ€™) (â€™effortâ€™, â€™fromâ€™, â€™Câ€™) (â€™sellingâ€™, â€™informationâ€™, â€™regardingâ€™, â€™aâ€™)\n(â€™ofâ€™, â€™sevenâ€™) (â€™progressedâ€™, â€™throughâ€™, â€™theâ€™) (â€™limitâ€™, â€™,â€™, â€™especiallyâ€™, â€™inâ€™)\n(â€™subsequentlyâ€™, â€™chargedâ€™) (â€™theâ€™, â€™Bluesâ€™, â€™.â€™) (â€™thatâ€™, â€™.â€™, â€™Iâ€™, â€™justâ€™)\n(â€™siteâ€™, â€™AHâ€™) (â€™tigersâ€™, â€™inâ€™, â€™captivityâ€™) (â€™someâ€™, â€™sortâ€™, â€™,â€™, â€™atâ€™)\n(â€™performâ€™, â€™tasksâ€™) (â€™glâ€™, â€™idingâ€™, â€™.â€™) (â€™oneâ€™, â€™isâ€™, â€™theâ€™, â€™mostâ€™)\n(â€™toâ€™, â€™theseâ€™) (â€™thisâ€™, â€™hasâ€™, â€™notâ€™) (â€™toâ€™, â€™Deputyâ€™, â€™Firstâ€™, â€™Ministerâ€™)\n(â€™rememberâ€™, â€™whoâ€™) (â€™hostsâ€™, â€™Japanâ€™, â€™,â€™) (â€™listsâ€™, â€™andâ€™, â€™theâ€™, â€™factâ€™)\n(â€™participateâ€™, â€™andâ€™) (â€™signedâ€™, â€™20â€™, â€™-â€™) (â€™theyâ€™, \"â€™re\", â€™oldâ€™, â€™enoughâ€™)\n(â€™workedâ€™, â€™becauseâ€™) (â€™termâ€™, â€™\"â€™, â€™lessâ€™) (â€™payâ€™, â€™aâ€™, â€™taxâ€™, â€™inâ€™)\n(â€™tooâ€™, â€™ifâ€™) (â€™excessiveâ€™, â€™forceâ€™, â€™againstâ€™) (â€™theirâ€™, â€™furyâ€™, â€™andâ€™, â€™carnageâ€™)\n(â€™otherwiseâ€™, â€™damagedâ€™) (â€™makeâ€™, â€™theâ€™, â€™matchâ€™) (â€™wereâ€™, â€™toâ€™, â€™acknowledgeâ€™, â€™himâ€™)\n(â€™onâ€™, â€™Ukrainianâ€™) (â€™toâ€™, â€™Europeâ€™, â€™,â€™) (â€™nowâ€™, â€™.â€™, â€™Iâ€™, â€™donâ€™)\n(â€™smackâ€™, â€™inâ€™) (â€™whichâ€™, â€™wonâ€™, â€™theâ€™) (â€™thatâ€™, â€™theâ€™, â€™collisionâ€™, â€™wasâ€™)\n(â€™lâ€™, â€™[â€™) (â€™thinkâ€™, â€™heâ€™, \"â€™d\") (â€™repeatâ€™, â€™ofâ€™, â€™theâ€™, â€™previousâ€™)\n(â€™ownerâ€™, â€™didâ€™) (â€™noâ€™, â€™andâ€™, â€™theyâ€™) (â€™thenâ€™, â€™,â€™, â€™ofâ€™, â€™courseâ€™)\n(â€™singleâ€™, â€™runâ€™) (â€™ofâ€™, â€™glassâ€™, â€™andâ€™) (â€™manâ€™, â€™whoâ€™, â€™beatâ€™, â€™theâ€™)\n(â€™wasâ€™, â€™pullingâ€™) (â€™howâ€™, â€™hisâ€™, â€™teamâ€™) (â€™ofâ€™, â€™someoneâ€™, â€™onâ€™, â€™thatâ€™)\n(â€™theâ€™, â€™influenzaâ€™) (â€™ofâ€™, â€™dataâ€™, â€™thatâ€™) (â€™whenâ€™, â€™theâ€™, â€™victimâ€™, â€™wasâ€™)\n(â€™ofâ€™, â€™pressureâ€™) (â€™friendâ€™, â€™whoâ€™, â€™suffersâ€™) (â€™saidâ€™, â€™captainâ€™, â€™Jamieâ€™, â€™Stirâ€™)\n2127\nTable 10: Samples of n-gram for LLaMA.\n2-gram 3-gram 4-gram\n(â€™treesâ€™, â€™.â€™) (â€™particularâ€™, â€™authorâ€™, â€™.â€™) (â€™whichâ€™, â€™affectâ€™, â€™millionsâ€™, â€™ofâ€™)\n(â€™tallâ€™, â€™manâ€™) (â€™togetherâ€™, â€™asâ€™, â€™aâ€™) (â€™â€œâ€™, â€™personâ€™, â€™alâ€™, â€™â€â€™)\n(â€™taxâ€™, â€™ingâ€™) (â€™oneâ€™, â€™handâ€™, â€™,â€™) (â€™toâ€™, â€™putâ€™, â€™itsâ€™, â€™footâ€™)\n(â€™secondâ€™, â€™Localâ€™) (â€™onlyâ€™, â€™makeâ€™, â€™anâ€™) (â€™typeâ€™, â€™ofâ€™, â€™aircraftâ€™, â€™thatâ€™)\n(â€™publicâ€™, â€™meetingâ€™) (â€™thereâ€™, â€™wereâ€™, â€™noneâ€™) (â€™workâ€™, â€™forâ€™, â€™aâ€™, â€™machineâ€™)\n(â€™whenâ€™, â€™Johnâ€™) (â€™whenâ€™, â€™theâ€™, â€™organizationâ€™) (â€™wouldâ€™, â€™meanâ€™, â€™Eâ€™, â€™4â€™)\n(â€™toâ€™, â€™Australianâ€™) (â€™thisâ€™, â€™situationâ€™, â€™,â€â€™) (â€™twoâ€™, â€™Jewishâ€™, â€™womenâ€™, â€™inâ€™)\n(â€™servicesâ€™, â€™duringâ€™) (â€™theirâ€™, â€™complaâ€™, â€™intsâ€™) (â€™theâ€™, â€™Mastersâ€™, â€™inâ€™, â€™hisâ€™)\n(â€™theâ€™, â€™ITâ€™) (â€™professionalâ€™, â€™Hollywoodâ€™, â€™setâ€™) (â€™toâ€™, â€™followâ€™, â€™theâ€™, â€™nextâ€™)\n(â€™womanâ€™, â€™onâ€™) (â€™theâ€™, â€™streetsâ€™, â€™.â€™) (â€™walkingâ€™, â€™alongâ€™, â€™theâ€™, â€™edgeâ€™)\n(â€™whileâ€™, â€™Gâ€™) (â€™strongâ€™, â€™squadâ€™, â€™,â€™) (â€™willâ€™, â€™useâ€™, â€™theâ€™, â€™Eâ€™)\n(â€™reportsâ€™, â€™makeâ€™) (â€™perâ€™, â€™dayâ€™, â€™.â€™) (â€™theâ€™, â€™injuredâ€™, â€™wereâ€™, â€™childrenâ€™)\n(â€™signedâ€™, â€™Arâ€™) (â€™touchedâ€™, â€™asâ€™, â€™aâ€™) (â€™theâ€™, â€™Titleâ€™, â€™VIIâ€™, â€™ofâ€™)\n(â€™workersâ€™, â€™aboveâ€™) (â€™toâ€™, â€™lightâ€™, â€™afterâ€™) (â€™toâ€™, â€™Sonâ€™, â€™arâ€™, â€™Quâ€™)\n(â€™requirementâ€™, â€™doesâ€™) (â€™tentâ€™, â€™thatâ€™, â€™heâ€™) (â€™theâ€™, â€™resourcesâ€™, â€™toâ€™, â€™respondâ€™)\n(â€™someâ€™, â€™thousandsâ€™) (â€™openingâ€™, â€™tookâ€™, â€™placeâ€™) (â€™volunteâ€™, â€™eringâ€™, â€™atâ€™, â€™herâ€™)\n(â€™theâ€™, â€™biggerâ€™) (â€™playâ€™, â€™,â€™, â€™Henryâ€™) (â€™whichâ€™, â€™areâ€™, â€™responsibleâ€™, â€™forâ€™)\n(â€™studyâ€™, â€™butâ€™) (â€™tieâ€™, â€™againstâ€™, â€™Dâ€™) (â€™showsâ€™, â€™insâ€™, â€™urâ€™, â€™ersâ€™)\n(â€™thenâ€™, â€™returnâ€™) (â€™officialsâ€™, â€™inâ€™, â€™Pyâ€™) (â€™toâ€™, â€™currentâ€™, â€™Healthâ€™, â€™Secretaryâ€™)\n(â€™theâ€™, â€™pursâ€™) (â€™ofâ€™, â€™negativeâ€™, â€™economicâ€™) (â€™wayâ€™, â€™toâ€™, â€™Newâ€™, â€™Hopeâ€™)\n(â€™theâ€™, â€™tallâ€™) (â€™shouldâ€™, â€™considerâ€™, â€™anâ€™) (â€™toâ€™, â€™theâ€™, â€™aircraftâ€™, â€™,â€™)\n(â€™remainâ€™, â€™unclearâ€™) (â€™yearâ€™, â€™reveâ€™, â€™alsâ€™) (â€™signsâ€™, â€™toâ€™, â€™betterâ€™, â€™representâ€™)\n(â€™strongerâ€™, â€™startâ€™) (â€™thatâ€™, â€™librariesâ€™, â€™offerâ€™) (â€™theâ€™, â€™Australianâ€™, â€™Governmentâ€™, â€™hasâ€™)\n(â€™thatâ€™, â€™whileâ€™) (â€™onâ€™, â€™Russiaâ€™, â€™,â€™) (â€™underâ€™, â€™neâ€™, â€™athâ€™, â€™.â€™)\n(â€™withâ€™, â€™Nigerâ€™) (â€™theâ€™, â€™superâ€™, â€™visorâ€™) (â€™weekâ€™, â€™inâ€™, â€™theâ€™, â€™Uâ€™)\n(â€™sheâ€™, â€™holdsâ€™) (â€™râ€™, â€™idesâ€™, â€™.â€™) (â€™theâ€™, â€™tripâ€™, â€™toâ€™, â€™Sâ€™)\n(â€™spotâ€™, â€™whereâ€™) (â€™sectorâ€™, â€™,â€™, â€™toâ€™) (â€™shootingâ€™, â€™onâ€™, â€™Sundayâ€™, â€™morningâ€™)\n(â€™spotâ€™, â€™,\"â€™) (â€™suâ€™, â€™ckerâ€™, â€™pâ€™) (â€™theâ€™, â€™discoveryâ€™, â€™ofâ€™, â€™theâ€™)\n(â€™takeâ€™, â€™responsibilityâ€™) (â€™putâ€™, â€™anâ€™, â€™endâ€™) (â€™toâ€™, â€™aâ€™, â€™Liberalâ€™, â€™Demâ€™)\n(â€™weekâ€™, â€™sinceâ€™) (â€™useâ€™, â€™theâ€™, â€™artâ€™) (â€™toâ€™, â€™takeâ€™, â€™actionâ€™, â€™.â€™)\n(â€™whatâ€™, â€™actionsâ€™) (â€™someâ€™, â€™newâ€™, â€™peopleâ€™) (â€™wentâ€™, â€™wrongâ€™, â€™.â€™, â€™Iâ€™)\n(â€™weekâ€™, â€™announcedâ€™) (â€™teamâ€™, â€™callâ€™, â€™-â€™) (â€™youngâ€™, â€™womanâ€™, â€™whileâ€™, â€™sheâ€™)\n(â€™wereâ€™, â€™splitâ€™) (â€™ofâ€™, â€™CRâ€™, â€™PFâ€™) (â€™withâ€™, â€™clubsâ€™, â€™suchâ€™, â€™asâ€™)\n(â€™sensitiveâ€™, â€™.â€™) (â€™provinceâ€™, â€™ofâ€™, â€™Galâ€™) (â€™totalâ€™, â€™,â€™, â€™approximatelyâ€™, â€)\n(â€™theâ€™, â€™stemâ€™) (â€™sloâ€™, â€™ppyâ€™, â€™atâ€™) (â€™theseâ€™, â€™suspectâ€™, â€™sâ€™, â€™areâ€™)\n(â€™withâ€™, â€™credâ€™) (â€™whenâ€™, â€™sheâ€™, â€™foughtâ€™) (â€™theâ€™, â€™poolâ€™, â€™fâ€™, â€™encingâ€™)\n(â€™regardingâ€™, â€™Islamâ€™) (â€™payâ€™, â€™riseâ€™, â€™theyâ€™) (â€™theâ€™, â€™processâ€™, â€™isâ€™, â€™thatâ€™)\n(â€™thisâ€™, â€™beforeâ€™) (â€™toâ€™, â€™theâ€™, â€™bandsâ€™) (â€™visitorsâ€™, â€™aâ€™, â€™yearâ€™, â€™,â€™)\n(â€™theirâ€™, â€™handsâ€™) (â€™ourâ€™, â€™operationsâ€™, â€™andâ€™) (â€™toâ€™, â€™Glâ€™, â€™oucâ€™, â€™estersâ€™)\n(â€™suppâ€™, â€™lierâ€™) (â€™underlyingâ€™, â€™collâ€™, â€™aterâ€™) (â€™withâ€™, â€, â€™1â€™, â€™9â€™)\n(â€™shootâ€™, â€™andâ€™) (â€™nowâ€™, â€™,\"â€™, â€™sheâ€™) (â€™survâ€™, â€™iveâ€™, â€™â€â€™, â€™andâ€™)\n(â€™recoveryâ€™, â€™.â€™) (â€™ofâ€™, â€™debâ€™, â€™tsâ€™) (â€™wasâ€™, â€™stillâ€™, â€™veryâ€™, â€™tightâ€™)\n(â€™toâ€™, â€™migrâ€™) (â€™teamâ€™, â€™thatâ€™, â€™tookâ€™) (â€™taxâ€™, â€™billâ€™, â€™everâ€™, â€™handedâ€™)\n(â€™strugglingâ€™, â€™thisâ€™) (â€™remainingâ€™, â€™toâ€™, â€™takeâ€™) (â€™wildâ€™, â€™fâ€™, â€™iresâ€™, â€™thatâ€™)\n(â€™summarâ€™, â€™izeâ€™) (â€™ofâ€™, â€™theâ€™, â€™delâ€™) (â€™vehicleâ€™, â€™andâ€™, â€™aâ€™, â€™pedâ€™)\n(â€™societyâ€™, â€™whichâ€™) (â€™theâ€™, â€™problemâ€™, â€™beforeâ€™) (â€™simpleâ€™, â€™conceptâ€™, â€™.â€™, â€™Wellâ€™)\n(â€™settlementâ€™, â€™withâ€™) (â€™whoseâ€™, â€™clientâ€™, â€™diedâ€™) (â€™toâ€™, â€™drivingâ€™, â€™anâ€™, â€™olderâ€™)\n(â€™thatâ€™, â€™facâ€™) (â€™yardâ€™, â€™linesâ€™, â€™andâ€™) (â€™whichâ€™, â€™heâ€™, â€™highlightâ€™, â€™edâ€™)\n(â€™wasâ€™, â€™smartâ€™) (â€™recipâ€™, â€™ientâ€™, â€™willâ€™) (â€™thrâ€™, â€™illingâ€™, â€™clâ€™, â€™ashâ€™)\n(â€™â€˜â€™, â€™Kâ€™) (â€™seriesâ€™, â€™,â€™, â€™winningâ€™) (â€™villageâ€™, â€™ofâ€™, â€™Vâ€™, â€™ranâ€™)\n2128\nTable 11: Samples of n-gram for T5.\n2-gram 3-gram 4-gram\n(â€™withâ€™, â€™Billâ€™) (â€™veryâ€™, â€™impressedâ€™, â€™.â€™) (â€™thereâ€™, â€™isâ€™, â€™nothingâ€™, â€™inâ€™)\n(â€™severalâ€™, â€™bandsâ€™) (â€™withâ€™, â€™Jamaicaâ€™, â€™nâ€™) (â€™wonâ€™, â€™theâ€™, â€™bronzeâ€™, â€™atâ€™)\n(â€™thatâ€™, â€™recognizedâ€™) (â€™theâ€™, â€™tourâ€™, â€™,â€™) (â€™wasâ€™, â€™whatâ€™, â€™Iâ€™, â€™wasâ€™)\n(â€™trayâ€™, â€™byâ€™) (â€™packagedâ€™, â€™andâ€™, â€™transportedâ€™) (â€™theâ€™, â€™Caâ€™, â€™ffeâ€™, â€™Nâ€™)\n(â€™toâ€™, â€™1977â€™) (â€™theâ€™, â€™productsâ€™, â€™andâ€™) (â€™yearâ€™, â€™toâ€™, â€™assessâ€™, â€™theâ€™)\n(â€™stunningâ€™, â€™terraceâ€™) (â€™ofâ€™, â€™assetsâ€™, â€™includingâ€™) (â€™worldâ€™, â€™.â€™, â€™Landâ€™, â€™sâ€™)\n(â€™shockedâ€™, â€™thatâ€™) (â€™toâ€™, â€™theâ€™, â€™Superâ€™) (â€™toâ€™, â€, â€™achievingâ€™, â€™thatâ€™)\n(â€™purchaseâ€™, â€™Abdulâ€™) (â€™ruralâ€™, â€™.â€™, â€™Theâ€™) (â€™theirâ€™, â€™homeâ€™, â€™gamesâ€™, â€™.â€™)\n(â€™rightâ€™, â€™whileâ€™) (â€™theyâ€™, â€™hopeâ€™, â€™toâ€™) (â€™withâ€™, â€™himâ€™, â€™wasâ€™, â€™theâ€™)\n(â€™trainingâ€™, â€™afterâ€™) (â€™ourâ€™, â€™mouthâ€™, â€™sâ€™) (â€™worthâ€™, â€, â€™mentioningâ€™, â€™thatâ€™)\n(â€™receivedâ€™, â€) (â€™thenâ€™, â€™toâ€™, â€™takeâ€™) (â€™toâ€™, â€™basicâ€™, â€™healthâ€™, â€™careâ€™)\n(â€™respiratoryâ€™, â€™syndromeâ€™) (â€™supportâ€™, â€™theâ€™, â€™communityâ€™) (â€™timeâ€™, â€™.â€™, â€™Theâ€™, â€™companyâ€™)\n(â€™visitorsâ€™, â€™gotâ€™) (â€™oneâ€™, â€™shotâ€™, â€™onâ€™) (â€™toâ€™, â€™winâ€™, â€™,â€™, \"â€™\")\n(â€™understoodâ€™, â€™theâ€™) (â€™weddingâ€™, â€™sâ€™, â€™,â€™) (â€™wouldâ€™, â€™scoreâ€™, â€™inâ€™, â€™theâ€™)\n(â€™softwareâ€™, â€™providesâ€™) (â€™oppositionâ€™, â€™ofâ€™, â€) (â€™withâ€™, â€™businessâ€™, â€™activityâ€™, â€™upâ€™)\n(â€™showâ€™, â€™anyâ€™) (â€™thenâ€™, â€, â€™withdrawnâ€™) (â€™wrongâ€™, â€™onâ€™, â€™Uâ€™, â€™.â€™)\n(â€™speaksâ€™, â€™onâ€™) (â€™outâ€™, â€™howâ€™, â€™youâ€™) (â€™wasâ€™, â€™firstâ€™, â€™introducedâ€™, â€™inâ€™)\n(â€™stillâ€™, â€™outstandingâ€™) (â€™theâ€™, â€™finalâ€™, â€™spotâ€™) (â€™thatâ€™, â€, â€™aâ€™, â€™Germanâ€™)\n(â€™trialâ€™, â€™mayâ€™) (â€™perâ€™, â€™100â€™, â€™.â€™) (â€™toâ€™, â€™2003â€™, â€™andâ€™, â€™wasâ€™)\n(â€™theirâ€™, â€™medicalâ€™) (â€™regionâ€™, â€™,â€™, â€™whileâ€™) (â€™theyâ€™, â€™cameâ€™, â€™intoâ€™, â€)\n(â€™useâ€™, â€™appropriateâ€™) (â€™writerâ€™, â€™forâ€™, â€™iTunesâ€™) (â€™weâ€™, â€™haveâ€™, â€™signedâ€™, â€™theâ€™)\n(â€™theâ€™, â€™eventsâ€™) (â€™whoâ€™, â€™areâ€™, â€™assignedâ€™) (â€™wasteâ€™, â€™mustâ€™, â€™beâ€™, â€™removedâ€™)\n(â€™proposedâ€™, â€™locationâ€™) (â€™theâ€™, â€™Syrianâ€™, â€™peopleâ€™) (â€™whichâ€™, â€™isâ€™, â€™fundedâ€™, â€™byâ€™)\n(â€™wayâ€™, â€™ourâ€™) (â€™soldâ€™, â€™inâ€™, â€™1986â€™) (â€™theâ€™, â€™Britishâ€™, â€™governmentâ€™, â€™isâ€™)\n(â€™workingâ€™, â€™pitchesâ€™) (â€™reportâ€™, â€™hisâ€™, â€™disappearâ€™) (â€™theâ€™, â€™anteriorâ€™, â€™anteriorâ€™, â€)\n(â€™sixthâ€™, â€™minutesâ€™) (â€™whichâ€™, â€™wasâ€™, â€™discoveredâ€™) (â€™veryâ€™, â€™difficultâ€™, â€™jobâ€™, â€™,\"â€™)\n(â€™thirstâ€™, â€™.â€™) (â€™wasâ€™, â€™givenâ€™, â€™bailâ€™) (â€™wasâ€™, â€™toâ€™, â€™beâ€™, â€)\n(â€™successorâ€™, â€™.â€™) (â€™theâ€™, â€™experienceâ€™, â€™andâ€™) (â€™troopsâ€™, â€™areâ€™, â€™transferredâ€™, â€™toâ€™)\n(â€™weâ€™, â€™allâ€™) (â€™toâ€™, â€™theâ€™, â€™leftâ€™) (â€™whatâ€™, â€™weâ€™, â€™didâ€™, â€™.â€™)\n(â€™wantâ€™, â€™meâ€™) (â€™underâ€™, â€™theâ€™, â€™followingâ€™) (â€™toâ€™, â€™provideâ€™, â€™forâ€™, â€™theirâ€™)\n(â€™returnedâ€™, â€™withâ€™) (â€™thatâ€™, â€™Mrâ€™, â€™Croâ€™) (â€™theâ€™, â€™mathâ€™, â€™isâ€™, â€™onlyâ€™)\n(â€™wasâ€™, â€™watchâ€™) (â€™stationâ€™, â€™inâ€™, â€™Londonâ€™) (â€™trackâ€™, â€™.â€™, â€™Theâ€™, â€™Sanâ€™)\n(â€™theirâ€™, â€™facesâ€™) (â€™seekâ€™, â€™toâ€™, â€™promoteâ€™) (â€™websiteâ€™, â€™.â€™, â€™Mrâ€™, â€™Burkeâ€™)\n(â€™shotâ€™, â€™pastâ€™) (â€™officeâ€™, â€™closedâ€™, â€™.â€™) (â€™theâ€™, â€™firstâ€™, â€™ladyâ€™, â€™isâ€™)\n(â€™trainerâ€™, â€™Danâ€™) (â€™thatâ€™, â€™Waltâ€™, â€™Disneyâ€™) (â€™wasâ€™, â€™clearedâ€™, â€™ofâ€™, â€™allâ€™)\n(â€™theâ€™, â€™lowerâ€™) (â€™seemedâ€™, â€™likeâ€™, â€™itâ€™) (â€™workâ€™, â€™dueâ€™, â€™toâ€™, â€)\n(â€™theâ€™, â€™Caribbeanâ€™) (â€™veryâ€™, â€™fewâ€™, â€™peopleâ€™) (â€™toâ€™, â€™useâ€™, â€™itsâ€™, â€™ownâ€™)\n(â€™wildlifeâ€™, â€™groupsâ€™) (â€™unhealthyâ€™, â€™fatâ€™, â€™fatâ€™) (â€™whileâ€™, â€™,â€™, â€™butâ€™, â€™theâ€™)\n(â€™theâ€™, â€™Beeâ€™) (â€™ofâ€™, â€™theâ€™, â€™heavyâ€™) (â€™withâ€™, â€™theâ€™, â€™Securitiesâ€™, â€™andâ€™)\n(â€™yourâ€™, â€™advancedâ€™) (â€™strongâ€™, â€™transportâ€™, â€™networkâ€™) (â€™thatâ€™, â€™anâ€™, â€™arrestâ€™, â€™hasâ€™)\n(â€™winningâ€™, â€™playerâ€™) (â€™soâ€™, â€™unfortunatelyâ€™, â€™noâ€™) (â€™wasâ€™, â€, â€™largelyâ€™, â€™usedâ€™)\n(â€™successfulâ€™, â€™?â€™) (â€™â€œâ€™, â€™Unâ€™, â€™derâ€™) (â€™thereâ€™, \"â€™\", â€™sâ€™, â€™moreâ€™)\n(â€™wereâ€™, â€™disappointedâ€™) (â€™wasâ€™, â€™drivingâ€™, â€™,â€™) (â€™theâ€™, â€, â€™iâ€™, â€™Genâ€™)\n(â€™whyâ€™, â€™anâ€™) (â€™projectâ€™, â€™hasâ€™, â€™raisedâ€™) (â€™theâ€™, â€™bestâ€™, â€™knownâ€™, â€™playâ€™)\n(â€™representedâ€™, â€™herâ€™) (â€™opportunityâ€™, â€™toâ€™, â€™helpâ€™) (â€™thenâ€™, â€™chargedâ€™, â€, â€™aâ€™)\n(â€™promoteâ€™, â€™investmentâ€™) (â€™ourâ€™, â€™productsâ€™, â€™andâ€™) (â€™theâ€™, â€™drugâ€™, â€™onâ€™, â€™theâ€™)\n(â€™soldierâ€™, â€™andâ€™) (â€™plungeâ€™, â€™inâ€™, â€™theâ€™) (â€™themâ€™, â€™fromâ€™, â€™becomingâ€™, â€™theâ€™)\n(â€™startedâ€™, â€™shootingâ€™) (â€™orâ€™, â€™deleteâ€™, â€™anyâ€™) (â€™toâ€™, â€™tellâ€™, â€™.â€™, â€™Itâ€™)\n(â€™visitingâ€™, â€™theâ€™) (â€™ofâ€™, â€™myâ€™, â€™wayâ€™) (â€™whichâ€™, â€™ranâ€™, â€™fromâ€™, â€™12â€™)\n(â€™questionsâ€™, â€™.\"â€™) (â€™raisedâ€™, â€™theâ€™, â€™limitâ€™) (â€™thatâ€™, â€™Scotlandâ€™, â€™canâ€™, â€™winâ€™)\n2129\nTable 12: Samples of n-gram for UniLM.\n2-gram 3-gram 4-gram\n(â€™ourâ€™, â€™hardâ€™) (â€™sâ€™, â€™aâ€™, â€™hoâ€™) (â€™ministerâ€™, â€™forâ€™, â€™immigrationâ€™, â€™.â€™)\n(â€™theâ€™, â€™jâ€™) (â€™fâ€™, â€™aâ€™, â€™.â€™) (â€™willâ€™, â€™helpâ€™, â€™toâ€™, â€™diâ€™)\n(â€™partsâ€™, â€™\"â€™) (â€™backâ€™, â€™aroundâ€™, â€™theâ€™) (â€™underâ€™, â€™pâ€™, â€™riâ€™, â€™vilâ€™)\n(â€™leadâ€™, â€™withâ€™) (â€™windowâ€™, â€™,â€™, â€™toâ€™) (â€™wasâ€™, â€™transferredâ€™, â€™toâ€™, â€™qâ€™)\n(â€™naâ€™, â€™ggâ€™) (â€™toâ€™, â€™mâ€™, â€™eastâ€™) (â€™hazeâ€™, â€™lnâ€™, â€™utâ€™, â€™conâ€™)\n(â€™meetsâ€™, â€™atâ€™) (â€™wayâ€™, â€™youâ€™, â€™wouldâ€™) (â€™kâ€™, â€™haâ€™, â€™kaâ€™, â€™gâ€™)\n(â€™litâ€™, â€™huaâ€™) (â€™archâ€™, â€™erâ€™, â€™,â€™) (â€™thatâ€™, â€™mâ€™, â€™ccaâ€™, â€™beâ€™)\n(â€™sheâ€™, â€™fellâ€™) (â€™andâ€™, â€™legsâ€™, â€™,â€™) (â€™onâ€™, â€™theâ€™, â€™futureâ€™, â€™.â€™)\n(â€™thirdâ€™, â€™nightâ€™) (â€™hangedâ€™, â€™,â€™, â€™andâ€™) (â€™râ€™, â€™ajâ€™, â€™eshâ€™, â€™.â€™)\n(â€™playâ€™, â€™themâ€™) (â€™littleâ€™, â€™fluâ€™, â€™steredâ€™) (â€™strangersâ€™, â€™\"â€™, â€™wasâ€™, â€™releasedâ€™)\n(â€™oldestâ€™, â€™andâ€™) (â€™gâ€™, â€™araâ€™, â€™vâ€™) (â€™suspendedâ€™, â€™hisâ€™, â€™journalistâ€™, â€™conductâ€™)\n(â€™informalâ€™, â€™economyâ€™) (â€™earningsâ€™, â€™wereâ€™, â€™affectedâ€™) (â€™scoredâ€™, â€™aâ€™, â€™fewâ€™, â€™goalsâ€™)\n(â€™swordâ€™, â€™thatâ€™) (â€™roundâ€™, â€™stoneâ€™, â€™coâ€™) (â€™heâ€™, â€™wasâ€™, â€™talkingâ€™, â€™aboutâ€™)\n(â€™greatâ€™, â€™areâ€™) (â€™kaâ€™, â€™wasakiâ€™, â€™fromâ€™) (â€™parkedâ€™, â€™hisâ€™, â€™carâ€™, â€™inâ€™)\n(â€™whichâ€™, â€™featuresâ€™) (â€™superâ€™, â€™shâ€™, â€™owâ€™) (â€™nationalâ€™, â€™teamâ€™, â€™,â€™, â€™winningâ€™)\n(â€™responsibilityâ€™, â€™isâ€™) (â€™modernâ€™, â€™\"â€™, â€™dâ€™) (â€™outâ€™, â€™herâ€™, â€™jâ€™, â€™anâ€™)\n(â€™wasâ€™, â€™convictedâ€™) (â€™andâ€™, â€™investmentâ€™, â€™.â€™) (â€™wouldâ€™, â€™seeâ€™, â€™herâ€™, â€™fatherâ€™)\n(â€™grandparentsâ€™, â€™.â€™) (â€™divisionâ€™, â€™wasâ€™, â€™neverâ€™) (â€™sayâ€™, â€™itâ€™, \"â€™\", â€™sâ€™)\n(â€™sheâ€™, â€™getsâ€™) (â€™factoâ€™, â€™\"â€™, â€™protectorâ€™) (â€™theâ€™, â€™midâ€™, â€™-â€™, â€™18thâ€™)\n(â€™showingâ€™, â€™theâ€™) (â€™nakâ€™, â€™aâ€™, â€™,â€™) (â€™theyâ€™, â€â€™, â€™reâ€™, â€™spaâ€™)\n(â€™isâ€™, â€™reallyâ€™) (â€™24â€™, â€™deâ€™, â€™câ€™) (â€™inâ€™, â€™thievesâ€™, â€â€™, â€™denâ€™)\n(â€™thankâ€™, â€™goodnessâ€™) (â€™changesâ€™, â€™toâ€™, â€™theâ€™) (â€™theâ€™, â€™throatâ€™, â€™ofâ€™, â€™kaâ€™)\n(â€™smallerâ€™, â€™bathroomâ€™) (â€™newâ€™, â€™estimateâ€™, â€™.â€™) (â€™toâ€™, â€™theâ€™, â€™hazeâ€™, â€™lnâ€™)\n(â€™mainâ€™, â€™reasonâ€™) (â€™gâ€™, â€™baâ€™, â€™,â€™) (â€™theâ€™, â€™titleâ€™, â€™isâ€™, â€™initiallyâ€™)\n(â€™notâ€™, â€™conâ€™) (â€™herâ€™, â€™painâ€™, â€™.â€™) (â€™ransomâ€™, â€™wareâ€™, â€™.â€™, â€™itâ€™)\n(â€™teamâ€™, â€™,â€™) (â€™herâ€™, â€™wereâ€™, â€™atâ€™) (â€™nakâ€™, â€™hirâ€™, â€™niâ€™, â€™saâ€™)\n(â€™myâ€™, â€™earâ€™) (â€™thatâ€™, â€™theâ€™, â€™heroâ€™) (â€™ofâ€™, â€™theâ€™, â€™followingâ€™, â€™:â€™)\n(â€™parkingâ€™, â€™lotsâ€™) (â€™conâ€™, â€™niâ€™, â€™torâ€™) (â€™putâ€™, â€™pressureâ€™, â€™onâ€™, â€™peopleâ€™)\n(â€™inâ€™, â€™terrorâ€™) (â€™thatâ€™, â€™theâ€™, â€™ukâ€™) (â€™sâ€™, â€™aâ€™, â€™littleâ€™, â€™moreâ€™)\n(â€™nâ€™, â€™wyâ€™) (â€™theâ€™, â€™officialâ€™, â€™wâ€™) (â€™hoâ€™, â€™istedâ€™, â€™itselfâ€™, â€™intoâ€™)\n(â€™reâ€™, â€™journalistsâ€™) (â€™theâ€™, â€™crowdâ€™, â€™wasâ€™) (â€™inâ€™, â€™theirâ€™, â€™49â€™, â€™-â€™)\n(â€™nativeâ€™, â€™townâ€™) (â€™-â€™, â€™blackâ€™, â€™boyâ€™) (â€™sureâ€™, â€™whatâ€™, â€™heâ€™, â€™meantâ€™)\n(â€™protectâ€™, â€™.â€™) (â€™mostâ€™, â€™kâ€™, â€™usaâ€™) (â€™saâ€™, â€™ssaâ€™, â€™wasâ€™, â€™aâ€™)\n(â€™happyâ€™, â€™featuresâ€™) (â€™performedâ€™, â€™\"â€™, â€™mâ€™) (â€™haveâ€™, â€™usâ€™, â€™hereâ€™, â€™yetâ€™)\n(â€™theyâ€™, â€™turnedâ€™) (â€™hereâ€™, â€™andâ€™, â€™youâ€™) (â€™hotâ€™, â€™,â€™, â€™â€â€™, â€™sâ€™)\n(â€™soâ€™, â€™dâ€™) (â€™publishedâ€™, â€™inâ€™, â€™1977â€™) (â€™theâ€™, â€™floorâ€™, â€™.â€™, â€™mâ€™)\n(â€™siâ€™, â€™unâ€™) (â€™26â€™, â€™/â€™, â€™22â€™) (â€™heâ€™, â€™paâ€™, â€™,â€™, â€™andâ€™)\n(â€™noâ€™, â€™organisedâ€™) (â€™themâ€™, â€™asâ€™, â€™independentâ€™) (â€™girlâ€™, â€™whoâ€™, â€™heâ€™, â€™wouldâ€™)\n(â€™uâ€™, â€™buâ€™) (â€™intoâ€™, â€™theâ€™, â€™waterâ€™) (â€™withâ€™, â€™aâ€™, â€™fellowâ€™, â€™râ€™)\n(â€™herâ€™, â€™albumâ€™) (â€™onâ€™, â€™planâ€™, â€™ktâ€™) (â€™wonâ€™, â€™everyâ€™, â€™tournamentâ€™, â€™.â€™)\n(â€™vâ€™, â€™ilâ€™) (â€™libraryâ€™, â€™andâ€™, â€™archivesâ€™) (â€™verdictâ€™, â€™wasâ€™, â€™overturnedâ€™, â€™byâ€™)\n(â€™partnerâ€™, â€™\"â€™) (â€™brâ€™, â€™idâ€™, â€™siâ€™) (â€™seeâ€™, â€™theâ€™, â€™lastâ€™, â€™oneâ€™)\n(â€™respectivelyâ€™, â€™.â€™) (â€™cafeâ€™, â€™sâ€™, â€™,â€™) (â€™orderâ€™, â€™ofâ€™, â€™theâ€™, â€™taâ€™)\n(â€™secondâ€™, â€™fastestâ€™) (â€™terrorâ€™, â€™.â€™, â€™theâ€™) (â€™theyâ€™, â€™movedâ€™, â€™intoâ€™, â€™aâ€™)\n(â€™marryâ€™, â€™menâ€™) (â€™moneyâ€™, â€™forâ€™, â€™lifeâ€™) (â€™sayingâ€™, â€™,â€™, â€™\"â€™, â€™itâ€™)\n(â€™myâ€™, â€™ladyâ€™) (â€™areâ€™, â€™bâ€™, â€™onâ€™) (â€™innocentâ€™, â€™peopleâ€™, â€™.â€™, â€™oâ€™)\n(â€™wereâ€™, â€™foundâ€™) (â€™hadâ€™, â€™takenâ€™, â€™herâ€™) (â€™wouldâ€™, â€™notâ€™, â€™bookâ€™, â€™theâ€™)\n(â€™sixthâ€™, â€™,â€™) (â€™withâ€™, â€™aâ€™, â€™videoâ€™) (â€™theâ€™, â€™albumâ€™, â€™\"â€™, â€™isâ€™)\n(â€™fundâ€™, â€™hasâ€™) (â€™mitâ€™, â€™eâ€™, â€™chtâ€™) (â€™theâ€™, â€™northâ€™, â€™kâ€™, â€™oreâ€™)\n(â€™yearsâ€™, â€™andâ€™) (â€™youâ€™, â€™whatâ€™, â€™.â€™) (â€™hadâ€™, â€™killedâ€™, â€™herâ€™, â€™.â€™)\n2130\nTable 13: Samples of n-gram for BART.\n2-gram 3-gram 4-gram\n(â€™travellersâ€™, â€™wereâ€™) (â€™searchâ€™, â€™isâ€™, â€™beginningâ€™) (â€™whoâ€™, â€™wasâ€™, â€™inâ€™, â€™theâ€™)\n(â€™tossedâ€™, â€™.â€™) (â€™televisionâ€™, â€™Newsâ€™, â€™mediaâ€™) (â€™upâ€™, â€™\"â€™, â€™......â€™, â€™Familyâ€™)\n(â€™targetedâ€™, â€™onlineâ€™) (â€™sheâ€™, â€™toldâ€™, â€™herâ€™) (â€™themâ€™, â€™Theyâ€™, â€™foughtâ€™, â€™theyâ€™)\n(â€™toâ€™, â€™tellâ€™) (â€™teamâ€™, â€™theâ€™, â€™teamâ€™) (â€™wasâ€™, â€™aâ€™, â€™...â€™, â€™Arâ€™)\n(â€™twoâ€™, â€™wonâ€™) (â€™theâ€™, â€™Empireâ€™, â€™\"â€™) (â€™villageâ€™, â€™todayâ€™, â€™!â€™, â€™Articleâ€™)\n(â€™wasâ€™, â€™bulliedâ€™) (â€™surviveâ€™, â€™andâ€™, â€™thriveâ€™) (â€™willâ€™, â€™providedâ€™, â€™masterâ€™, â€™classesâ€™)\n(â€™transactionâ€™, â€™detailsâ€™) (â€™theâ€™, â€™reasonâ€™, â€™forâ€™) (â€™wereâ€™, â€™citedâ€™, â€™includingâ€™, â€™:â€™)\n(â€™troubleâ€™, â€™Officialsâ€™) (â€™wouldâ€™, â€™chooseâ€™, â€™thisâ€™) (â€™thenâ€™, â€™heâ€™, â€™bornâ€™, â€™hereâ€™)\n(â€™victimâ€™, â€™leftâ€™) (â€™theâ€™, â€™strongâ€™, â€™manâ€™) (â€™withâ€™, â€™forceâ€™, â€™,â€™, â€™theyâ€™)\n(â€™useâ€™, â€™Thereâ€™) (â€™GÂ¦â€™, â€™Deanâ€™, â€™Andrewsâ€™) (â€™wasâ€™, â€™jailedâ€™, â€™inâ€™, â€™2010â€™)\n(â€™teamâ€™, â€™......â€™) (â€™whileâ€™, â€™hisâ€™, â€™commentsâ€™) (â€™victoriesâ€™, â€™beingâ€™, â€™scoredâ€™, â€™.â€™)\n(â€™Aâ€™, â€™Schoolâ€™) (â€™theâ€™, â€™reportâ€™, â€™includesâ€™) (â€™theyâ€™, â€™startâ€™, â€™againâ€™, â€™Afterâ€™)\n(â€™wrongâ€™, â€™......â€™) (â€™wereâ€™, â€™.....â€™, â€™theyâ€™) (â€™theâ€™, â€™suspectsâ€™, â€™.â€™, â€™Theâ€™)\n(â€™worldsâ€™, â€™.â€™) (â€™theyâ€™, â€™areâ€™, â€™workingâ€™) (â€™thisâ€™, â€™youthfulâ€™, â€™groupâ€™, â€™ofâ€™)\n(â€™toâ€™, â€™abolishâ€™) (â€™stillâ€™, â€™isâ€™, â€™aâ€™) (â€™wereâ€™, â€™falseâ€™, â€™claimsâ€™, â€™thatâ€™)\n(â€™wasâ€™, â€™amongâ€™) (â€™universeâ€™, â€™.â€™, â€™Theâ€™) (â€™toâ€™, â€™explainâ€™, â€™whyâ€™, â€™??â€™)\n(â€™toâ€™, â€™Chadâ€™) (â€™wasâ€™, â€™sentencedâ€™, â€™forâ€™) (â€™trainâ€™, â€™operatorsâ€™, â€™Theâ€™, â€™trainâ€™)\n(â€™tookâ€™, â€™4â€™) (â€™walkâ€™, â€™outâ€™, â€™endâ€™) (â€™theâ€™, â€™wholeâ€™, â€™seriesâ€™, â€™isâ€™)\n(â€™teamâ€™, â€™Behindâ€™) (â€™thenâ€™, â€™heâ€™, â€™andâ€™) (â€™wishesâ€™, â€™toâ€™, â€™remainâ€™, â€™anonymousâ€™)\n(â€™visitorâ€™, â€™stâ€™) (â€™theâ€™, â€™.â€™, â€™Plâ€™) (â€™theeâ€™, â€™toâ€™, â€™theâ€™, â€™numberâ€™)\n(â€™thatâ€™, â€™Northâ€™) (â€™whoâ€™, â€™isâ€™, â€™basedâ€™) (â€™unbeatenâ€™, â€™andâ€™, â€™theâ€™, â€™victoryâ€™)\n(â€™standâ€™, â€™-â€™) (â€™theâ€™, â€™firstâ€™, â€™....â€™) (â€™wantâ€™, â€™themâ€™, â€™!â€™, â€™Theyâ€™)\n(â€™yourâ€™, â€™thingâ€™) (â€™thousandâ€™, â€™peopleâ€™, â€™missingâ€™) (â€™yearsâ€™, â€™agoâ€™, â€™Theâ€™, â€™strikeâ€™)\n(â€™theâ€™, â€™Greekâ€™) (â€™wasâ€™, â€™hesâ€™, â€™overâ€™) (â€™theâ€™, â€™slotâ€™, â€™...â€™, â€™Brianâ€™)\n(â€™Isâ€™, â€™Myâ€™) (â€™weatherâ€™, â€™forecastâ€™, â€™keepsâ€™) (â€™wasâ€™, â€™atâ€™, â€™boardâ€™, â€™.â€™)\n(â€™theyâ€™, â€™!!!â€™) (â€™singersâ€™, â€™andâ€™, â€™designersâ€™) (â€™whichâ€™, â€™standâ€™, â€™Hideâ€™, â€™Transcriptâ€™)\n(â€™wheelâ€™, â€™isâ€™) (â€™willâ€™, â€™mergeâ€™, â€™itâ€™) (â€™theâ€™, â€™universityâ€™, â€™.â€™, â€™GÂ¦â€™)\n(â€™towerâ€™, â€™houseâ€™) (â€™toâ€™, â€™sayingâ€™, â€™goodbyeâ€™) (â€™theâ€™, â€™serviceâ€™, â€™thatâ€™, â€™doesâ€™)\n(â€™theâ€™, â€™digâ€™) (â€™.â€™, â€™Assistantâ€™, â€™managerâ€™) (â€™theirâ€™, â€™websiteâ€™, â€™|â€™, â€™TVâ€™)\n(â€™verifiedâ€™, â€™.â€™) (â€™throughâ€™, â€™itsâ€™, â€™shareâ€™) (â€™whoâ€™, â€™wereâ€™, â€™sackedâ€™, â€™inâ€™)\n(â€™theâ€™, â€™bastardâ€™) (â€™sellingâ€™, â€™propertiesâ€™, â€™,â€™) (â€™toâ€™, â€™passingâ€™, â€™theâ€™, â€™markâ€™)\n(â€™targetedâ€™, â€™itâ€™) (â€™sureâ€™, â€™.â€™, â€™Especiallyâ€™) (â€™yetâ€™, â€™.......â€™, â€™Andâ€™, â€™......â€™)\n(â€™weeksâ€™, â€™laterâ€™) (â€™speakâ€™, â€™isâ€™, â€™sureâ€™) (â€™thisâ€™, â€™clubâ€™, â€™:â€™, â€™Articleâ€™)\n(â€™summerâ€™, â€™.â€™) (â€™whoâ€™, â€™wasâ€™, â€™82â€™) (â€™wasâ€™, â€™likelyâ€™, â€™disappointedâ€™, â€™inâ€™)\n(â€™thenâ€™, â€™isâ€™) (â€™wasâ€™, â€™Andâ€™, â€™Thatâ€™) (â€™usâ€™, â€™:â€™, â€™)â€™, â€™??â€™)\n(â€™theâ€™, â€™Samâ€™) (â€™whoâ€™, â€™cameâ€™, â€™toâ€™) (â€™theyâ€™, â€™sayâ€™, â€™:â€™, â€™Turnâ€™)\n(â€™teamâ€™, â€™isâ€™) (â€™theâ€™, â€™raceâ€™, â€™wasâ€™) (â€™whoâ€™, â€™theâ€™, â€™artistâ€™, â€™Theâ€™)\n(â€™topicâ€™, â€™-â€™) (â€™servicesâ€™, â€™areâ€™, â€™resumedâ€™) (â€™theâ€™, â€™southâ€™, â€™Crewâ€™, â€™smâ€™)\n(â€™spottedâ€™, â€™nearbyâ€™) (â€™todayâ€™, â€™:â€™, â€™Jackâ€™) (â€™withâ€™, â€™aâ€™, â€™golfâ€™, â€™clubâ€™)\n(â€™understandableâ€™, â€™toâ€™) (â€™sheâ€™, â€™neverâ€™, â€™heardâ€™) (â€™GÂ¦â€™, â€™Theâ€™, â€™Unâ€™, â€™ailâ€™)\n(â€™victimâ€™, â€™refusedâ€™) (â€™withâ€™, â€™theâ€™, â€™ageâ€™) (â€™whileâ€™, â€™....â€™, â€™.\",â€™, â€™asâ€™)\n(â€™AÂ·â€™, â€™advertisementâ€™) (â€™survivesâ€™, â€™militaryâ€™, â€™testsâ€™) (â€™withâ€™, â€™womenâ€™, â€™areâ€™, â€™isâ€™)\n(â€™strifeâ€™, â€™GÂ¦â€™) (â€™toâ€™, â€™gainâ€™, â€™insightsâ€™) (â€™trialâ€™, â€™wasâ€™, â€™strikeâ€™, â€™afterâ€™)\n(â€™traderâ€™, â€™Bryanâ€™) (â€™theâ€™, â€™holidaysâ€™, â€™willâ€™) (â€™whereâ€™, â€™areâ€™, â€™youâ€™, â€™?,â€™)\n(â€™templesâ€™, â€™Fromâ€™) (â€™spaceâ€™, â€™Theâ€™, â€™policyâ€™) (â€™unavailableâ€™, â€™.â€™, â€™forâ€™, â€™Advertisementâ€™)\n(â€™surprisedâ€™, â€™andâ€™) (â€™theâ€™, â€™itemsâ€™, â€™Theyâ€™) (â€™wasâ€™, â€™attackedâ€™, â€™sheâ€™, â€™wasâ€™)\n(â€™widespreadâ€™, â€™asâ€™) (â€™startsâ€™, â€™!â€™, â€™Whenâ€™) (â€™GÂ¦â€™, â€™Theâ€™, â€™formerâ€™, â€™...â€™)\n(â€™twoâ€™, â€™titlesâ€™) (â€™unionâ€™, â€™andâ€™, â€™toâ€™) (â€™wereâ€™, â€™thereâ€™, â€™wereâ€™, â€™someâ€™)\n(â€™whoseâ€™, â€™whoseâ€™) (â€™toâ€™, â€™changeâ€™, â€™Aâ€™) (â€™websiteâ€™, â€™saidâ€™, â€™:â€™, â€™advertisementâ€™)\n(â€™thanâ€™, â€™liveâ€™) (â€™yâ€™, â€™rsâ€™, â€™.â€™) (â€™thenâ€™, â€™travelsâ€™, â€™Câ€™, â€™henâ€™)\n2131\nTable 14: Samples of n-gram for GPT-Neo.\n2-gram 3-gram 4-gram\n(â€™whereâ€™, â€™aboutâ€™) (â€™theâ€™, â€™bagâ€™, â€™andâ€™) (â€™transferredâ€™, â€™toâ€™, â€™theâ€™, â€™Aucklandâ€™)\n(â€™theseâ€™, â€™talksâ€™) (â€™thisâ€™, â€™contextâ€™, â€™thatâ€™) (â€™toldâ€™, â€™thatâ€™, â€™theyâ€™, â€™hadâ€™)\n(â€™ticketâ€™, â€™.â€™) (â€™withâ€™, â€™Hellâ€™, â€™fireâ€™) (â€™lâ€™, â€™Bulâ€™, â€™gerâ€™, â€™Gâ€™)\n(â€™weekâ€™, â€™:â€™) (â€™toâ€™, â€™schoolsâ€™, â€™thatâ€™) (â€™touchâ€™, â€™ofâ€™, â€™theâ€™, â€™nightâ€™)\n(â€™theâ€™, â€™dronesâ€™) (â€™toâ€™, â€™anyoneâ€™, â€™inâ€™) (â€™willâ€™, â€™beâ€™, â€™drawnâ€™, â€™inâ€™)\n(â€™thingsâ€™, â€™happenâ€™) (â€™thenâ€™, â€™grabbedâ€™, â€™Mâ€™) (â€™wasâ€™, â€™proposedâ€™, â€™byâ€™, â€™Swedishâ€™)\n(â€™toâ€™, â€™exposeâ€™) (â€™toâ€™, â€™sitâ€™, â€™onâ€™) (â€™withâ€™, â€™someâ€™, â€™describingâ€™, â€™itâ€™)\n(â€™theâ€™, â€™grapheneâ€™) (â€™togetherâ€™, â€™onâ€™, â€™thisâ€™) (â€™whyâ€™, â€™moreâ€™, â€™needsâ€™, â€™toâ€™)\n(â€™withâ€™, â€™girlsâ€™) (â€™theâ€™, â€™storeâ€™, â€™wasâ€™) (â€™veryâ€™, â€™powerfulâ€™, â€™andâ€™, â€™personalâ€™)\n(â€™whereâ€™, â€™residentsâ€™) (â€™lâ€™, â€™violâ€™, â€™ationâ€™) (â€™waysâ€™, â€™thatâ€™, â€™participantsâ€™, â€™canâ€™)\n(â€™theirâ€™, â€™complianceâ€™) (â€™theâ€™, â€™earlyâ€™, â€™universeâ€™) (â€™yourâ€™, â€™countryâ€™, â€™andâ€™, â€™youâ€™)\n(â€™toâ€™, â€™gettingâ€™) (â€™withâ€™, â€™hisâ€™, â€™abilityâ€™) (â€™uphillâ€™, â€™battleâ€™, â€™toâ€™, â€™getâ€™)\n(â€™thenâ€™, â€™headedâ€™) (â€™withâ€™, â€™someoneâ€™, â€™thatâ€™) (â€™whateverâ€™, â€™Iâ€™, â€™hadâ€™, â€™toâ€™)\n(â€™underminedâ€™, â€™.\"â€™) (â€™wasâ€™, â€™gâ€™, â€™ustingâ€™) (â€™universeâ€™, â€™todayâ€™, â€™isâ€™, â€™drivenâ€™)\n(â€™unconsciousâ€™, â€™whenâ€™) (â€™toldâ€™, â€™prosecutorsâ€™, â€™thatâ€™) (â€™Lâ€™, â€™sâ€™, â€™firstâ€™, â€™cashâ€™)\n(â€™theâ€™, â€™strokeâ€™) (â€™toâ€™, â€™persuadeâ€™, â€™Sâ€™) (â€™withâ€™, â€™menâ€™, â€™andâ€™, â€™hadâ€™)\n(â€™worksâ€™, â€™aheadâ€™) (â€™theâ€™, â€™endâ€™, â€™usersâ€™) (â€™Lâ€™, â€™sâ€™, â€™NFLâ€™, â€™coverageâ€™)\n(â€™tryingâ€™, â€™severalâ€™) (â€™uniqueâ€™, â€™toâ€™, â€™humansâ€™) (â€™yearâ€™, â€™,â€™, â€™afterâ€™, â€™whichâ€™)\n(â€™touristsâ€™, â€™awayâ€™) (â€™theâ€™, â€™regenerationâ€™, â€™ofâ€™) (â€™Lâ€™, â€™sâ€™, â€™otherâ€™, â€™fiveâ€™)\n(â€™withâ€™, â€™64â€™) (â€™toâ€™, â€™memorâ€™, â€™iseâ€™) (â€™translationâ€™, â€™,â€™, â€™theâ€™, â€™performanceâ€™)\n(â€™wasâ€™, â€™sidelinedâ€™) (â€™veryâ€™, â€™unusualâ€™, â€™caseâ€™) (â€™willâ€™, â€™beâ€™, â€™rolledâ€™, â€™outâ€™)\n(â€™toâ€™, â€™bedâ€™) (â€™Aâ€™, â€™23â€™, â€™millionâ€™) (â€™wasâ€™, â€™condemnedâ€™, â€™byâ€™, â€™theâ€™)\n(â€™lâ€™, â€™Secretâ€™) (â€™timeâ€™, â€™inâ€™, â€™recentâ€™) (â€™whoâ€™, â€™haveâ€™, â€™beenâ€™, â€™ravagedâ€™)\n(â€™lâ€™, â€™Listâ€™) (â€™theyâ€™, â€™areâ€™, â€™inâ€™) (â€™youngâ€™, â€™playersâ€™, â€™GKâ€™, â€™Wadeâ€™)\n(â€™whyâ€™, â€™notâ€™) (â€™veryâ€™, â€™sickâ€™, â€™lyâ€™) (â€™topâ€™, â€™fiveâ€™, â€™momentsâ€™, â€™.â€™)\n(â€™toâ€™, â€™bowâ€™) (â€™usâ€™, â€™,â€™, â€™becauseâ€™) (â€™workâ€™, â€™.â€™, â€™Hisâ€™, â€™angerâ€™)\n(â€™whichâ€™, â€™alreadyâ€™) (â€™transâ€™, â€™peopleâ€™, â€™canâ€™) (â€™visionâ€™, â€™toâ€™, â€™lifeâ€™, â€™,\"â€™)\n(â€™withâ€™, â€™Artâ€™) (â€™wereâ€™, â€™workingâ€™, â€™towardsâ€™) (â€™towardâ€™, â€™theâ€™, â€™portâ€™, â€™icoâ€™)\n(â€™twiceâ€™, â€™aboutâ€™) (â€™wayâ€™, â€™ofâ€™, â€™narrowingâ€™) (â€™whichâ€™, â€™sawâ€™, â€™formerâ€™, â€™ownerâ€™)\n(â€™underâ€™, â€™arrestâ€™) (â€™theâ€™, â€™embassyâ€™, \"â€™s\") (â€™trunkâ€™, â€™ofâ€™, â€™herâ€™, â€™carâ€™)\n(â€™whereverâ€™, â€™Iâ€™) (â€™theirâ€™, â€™resistanceâ€™, â€™overâ€™) (â€™violateâ€™, â€™theâ€™, â€™UNâ€™, â€™Charterâ€™)\n(â€™usedâ€™, â€™Photoshopâ€™) (â€™AEâ€™, â€™80â€™, â€™bnâ€™) (â€™wasâ€™, â€™hijackedâ€™, â€™byâ€™, â€™aâ€™)\n(â€™theâ€™, â€™traditionâ€™) (â€™Lâ€™, â€™theyâ€™, â€™saidâ€™) (â€™wasâ€™, â€™managedâ€™, â€™byâ€™, â€™theâ€™)\n(â€™wardâ€™, â€™electingâ€™) (â€™theâ€™, â€™outcomeâ€™, â€™theâ€™) (â€™whoâ€™, â€™feltâ€™, â€™thatâ€™, â€™theâ€™)\n(â€™zooâ€™, â€™thenâ€™) (â€™weâ€™, \"â€™re\", â€™frustratedâ€™) (â€™usingâ€™, â€™aâ€™, â€™rapidâ€™, â€™HIVâ€™)\n(â€™wroteâ€™, â€™thatâ€™) (â€™thisâ€™, â€™VIPâ€™, â€™clubâ€™) (â€™wasâ€™, â€™bornâ€™, â€™inâ€™, â€™1971â€™)\n(â€™visaâ€™, â€™programâ€™) (â€™Lâ€™, â€™theâ€™, â€™officerâ€™) (â€™tryingâ€™, â€™toâ€™, â€™developâ€™, â€™nuclearâ€™)\n(â€™lâ€™, â€™streâ€™) (â€™Lâ€™, â€™sâ€™, â€™thenâ€™) (â€™wereâ€™, â€™alsoâ€™, â€™upâ€™, â€™byâ€™)\n(â€™usuallyâ€™, â€™accompaniedâ€™) (â€™usedâ€™, â€™byâ€™, â€™Democraticâ€™) (â€™usâ€™, â€™forâ€™, â€™aâ€™, â€™newâ€™)\n(â€™withâ€™, â€™developersâ€™) (â€™theâ€™, â€™slainâ€™, â€™soldierâ€™) (â€™wasâ€™, â€™invitedâ€™, â€™toâ€™, â€™anâ€™)\n(â€™useâ€™, â€™aheadâ€™) (â€™withoutâ€™, â€™startingâ€™, â€™quarterbackâ€™) (â€™lâ€™, â€™Theâ€™, â€™BCCâ€™, â€™Iâ€™)\n(â€™wealthyâ€™, â€™neighborhoodâ€™) (â€™weâ€™, â€™canâ€™, â€™talkâ€™) (â€™wasâ€™, â€™proudâ€™, â€™toâ€™, â€™representâ€™)\n(â€™Lâ€™, â€™Filâ€™) (â€™theâ€™, â€™heatedâ€™, â€™swimmingâ€™) (â€™wasâ€™, â€™alsoâ€™, â€™aâ€™, â€™featuredâ€™)\n(â€™underâ€™, â€™thisâ€™) (â€™theâ€™, â€™cocaineâ€™, â€™,â€™) (â€™withâ€™, â€™yourâ€™, â€™messageâ€™, â€™toâ€™)\n(â€™wasâ€™, â€™waitingâ€™) (â€™wordâ€™, â€™.â€™, â€™Heâ€™) (â€™Lâ€™, â€™asâ€™, â€™theâ€™, â€™Boeingâ€™)\n(â€™lâ€™, â€™Râ€™) (â€™toâ€™, â€™keepâ€™, â€™usingâ€™) (â€™winâ€™, â€™overâ€™, â€™Sportingâ€™, â€™.â€™)\n(â€™willâ€™, â€™forgiveâ€™) (â€™womanâ€™, â€™,â€™, â€™Iâ€™) (â€™totalâ€™, â€™assetsâ€™, â€™,â€™, â€™asâ€™)\n(â€™theâ€™, â€™grooveâ€™) (â€™worldâ€™, â€™ofâ€™, â€™actingâ€™) (â€™Lâ€™, â€™saidâ€™, â€™Johnâ€™, â€™Fâ€™)\n(â€™trialsâ€™, â€™inâ€™) (â€™wellâ€™, â€™asâ€™, â€™deliveringâ€™) (â€™wasâ€™, â€™theâ€™, â€™secondâ€™, â€™mostâ€™)\n(â€™turningâ€™, â€™hisâ€™) (â€™theâ€™, â€™Worldâ€™, â€™Retailâ€™) (â€™wentâ€™, â€™toâ€™, â€™Oldâ€™, â€™Traffordâ€™)\n2132\nTable 15: Samples of n-gram for Bloom.\n2-gram 3-gram 4-gram\n(â€™theâ€™, â€™tribesâ€™) (â€™theâ€™, â€™dataâ€™, â€™optimâ€™) (â€™theyâ€™, â€™areâ€™, â€™notâ€™, â€™legallyâ€™)\n(â€™testâ€™, \"â€™s\") (â€™theâ€™, â€™policeâ€™, â€™gotâ€™) (â€™toâ€™, â€™aâ€™, â€™weakâ€™, â€™eningâ€™)\n(â€™variousâ€™, â€™colorsâ€™) (â€™theâ€™, â€™humanâ€™, â€™systemâ€™) (â€™workâ€™, â€™menâ€™, â€™wereâ€™, â€™allowedâ€™)\n(â€™ticketsâ€™, â€™forâ€™) (â€™taggedâ€™, â€™Aâ€™, â€™idanâ€™) (â€™wereâ€™, â€™alsoâ€™, â€™usedâ€™, â€™forâ€™)\n(â€™thanâ€™, â€™Rpâ€™) (â€™theoryâ€™, â€™ofâ€™, â€™anâ€™) (â€™wayâ€™, â€™weâ€™, â€™thinkâ€™, â€™aboutâ€™)\n(â€™stateâ€™, â€™mightâ€™) (â€™theirâ€™, â€™recentâ€™, â€™poorâ€™) (â€™twoâ€™, â€™appearancesâ€™, â€™forâ€™, â€™Portugalâ€™)\n(â€™theatreâ€™, â€™specialâ€™) (â€™wereâ€™, â€™theâ€™, â€™Redsâ€™) (â€™timeâ€™, â€™thatâ€™, â€™theyâ€™, â€™haveâ€™)\n(â€™strategicâ€™, â€™planningâ€™) (â€™usedâ€™, â€™forâ€™, â€™advertisingâ€™) (â€™Glâ€™, â€™Womenâ€™, â€™withâ€™, â€™Photâ€™)\n(â€™wereâ€™, â€™83â€™) (â€™upâ€™, â€™theâ€™, â€™Blueâ€™) (â€™unclearâ€™, â€™whetherâ€™, â€™orâ€™, â€™notâ€™)\n(â€™theâ€™, â€™Generalâ€™) (â€™theâ€™, â€™sameâ€™, â€™pricingâ€™) (â€™usingâ€™, â€™thisâ€™, â€™againâ€™, â€™.â€™)\n(â€™wroteâ€™, â€™isâ€™) (â€™totalâ€™, â€™annualâ€™, â€™revenueâ€™) (â€™whoâ€™, â€™isâ€™, â€™stillâ€™, â€™chairmanâ€™)\n(â€™wifeâ€™, â€™metâ€™) (â€™testsâ€™, â€™andâ€™, â€™normalâ€™) (â€™toolâ€™, â€™toâ€™, â€™educâ€™, â€™ateâ€™)\n(â€™studentâ€™, â€™assaultedâ€™) (â€™transportâ€™, â€™ingâ€™, â€™USâ€™) (â€™unitâ€™, â€™inâ€™, â€™theâ€™, â€™nationâ€™)\n(â€™specialâ€™, â€™equipmentâ€™) (â€™surroundingâ€™, â€™itâ€™, â€™.â€™) (â€™trialâ€™, â€™courtâ€™, â€™,â€™, â€™heâ€™)\n(â€™vesselâ€™, â€™withâ€™) (â€™suiteâ€™, â€™.â€™, â€™Theâ€™) (â€™wasâ€™, â€™aâ€™, â€™handsomeâ€™, â€™youngâ€™)\n(â€™topâ€™, â€™employersâ€™) (â€™useâ€™, â€™inâ€™, â€™thisâ€™) (â€™thinkâ€™, â€™heâ€™, â€™GLâ€™, â€™llâ€™)\n(â€™undoubtedlyâ€™, â€™haveâ€™) (â€™theâ€™, â€™Houseâ€™, â€™Financeâ€™) (â€™treatmentâ€™, â€™ofâ€™, â€™anyâ€™, â€™individualâ€™)\n(â€™typeâ€™, â€™atâ€™) (â€™wereâ€™, â€™arrestedâ€™, â€™byâ€™) (â€™womanâ€™, â€™claimingâ€™, â€™sheâ€™, â€™wasâ€™)\n(â€™theâ€™, â€™fiscalâ€™) (â€™staffâ€™, â€™.â€™, â€™Câ€™) (â€™willâ€™, â€™returnâ€™, â€™fromâ€™, â€™theâ€™)\n(â€™withâ€™, â€™obsâ€™) (â€™wasâ€™, â€™aâ€™, â€™bankâ€™) (â€™youâ€™, â€™canâ€™, â€™hearâ€™, â€™meâ€™)\n(â€™weddingâ€™, â€™planâ€™) (â€™visitorsâ€™, â€™.â€™, â€™Theseâ€™) (â€™useâ€™, â€™theirâ€™, â€™violenceâ€™, â€™.â€™)\n(â€™tumorâ€™, â€™andâ€™) (â€™theâ€™, â€™brainâ€™, â€™(â€™) (â€™wasâ€™, â€™mentallyâ€™, â€™illâ€™, â€™afterâ€™)\n(â€™smallâ€™, â€™feeâ€™) (â€™withâ€™, â€™newâ€™, â€™talentâ€™) (â€™theseâ€™, â€™materialsâ€™, â€™.â€™, â€™Theâ€™)\n(â€™unnecessaryâ€™, â€™burdenâ€™) (â€™thatâ€™, â€™isâ€™, â€™distinguishedâ€™) (â€™toâ€™, â€™theâ€™, â€™Detroitâ€™, â€™Pistâ€™)\n(â€™yetâ€™, â€™reviewedâ€™) (â€™soâ€™, â€™impressedâ€™, â€™withâ€™) (â€™thinkâ€™, â€™thereâ€™, â€™isâ€™, â€™veryâ€™)\n(â€™yarnâ€™, â€™havingâ€™) (â€™withâ€™, â€™Jonâ€™, â€™ahâ€™) (â€™wasâ€™, â€™thenâ€™, â€™advisedâ€™, â€™thatâ€™)\n(â€™wonderâ€™, â€™overâ€™) (â€™technologyâ€™, â€™andâ€™, â€™financialâ€™) (â€™whichâ€™, â€™isâ€™, â€™theirâ€™, â€™honeyâ€™)\n(â€™stopâ€™, â€™;â€™) (â€™someoneâ€™, â€™enjoyâ€™, â€™sâ€™) (â€™whichâ€™, â€™isâ€™, â€™widelyâ€™, â€™regardedâ€™)\n(â€™tennisâ€™, â€™gameâ€™) (â€™upâ€™, â€™overâ€™, â€™4â€™) (â€™toâ€™, â€™beâ€™, â€™muchâ€™, â€™ofâ€™)\n(â€™theâ€™, â€™microâ€™) (â€™successfulâ€™, â€™filmsâ€™, â€™everâ€™) (â€™toâ€™, â€™writeâ€™, â€™theâ€™, â€™articleâ€™)\n(â€™threeâ€™, â€™aspectsâ€™) (â€™theâ€™, â€™Americanâ€™, â€™Barâ€™) (â€™yearsâ€™, â€™.â€™, â€™Leâ€™, â€™ighâ€™)\n(â€™websiteâ€™, â€™,â€™) (â€™willâ€™, â€™haveâ€™, â€™lotsâ€™) (â€™usedâ€™, â€™toâ€™, â€™identifyâ€™, â€™newâ€™)\n(â€™willâ€™, â€™riseâ€™) (â€™theâ€™, â€™pandemicâ€™, â€™continuesâ€™) (â€™toâ€™, â€™myâ€™, â€™daughterâ€™, â€™GLâ€™)\n(â€™yourâ€™, â€™Câ€™) (â€™waveâ€™, â€™hadâ€™, â€™reachedâ€™) (â€™toâ€™, â€™worryâ€™, â€™aboutâ€™, â€™myâ€™)\n(â€™thatâ€™, â€™orphâ€™) (â€™theâ€™, â€™oppositionâ€™, â€™playâ€™) (â€™wasâ€™, â€™aâ€™, â€™shotâ€™, â€™fromâ€™)\n(â€™westâ€™, â€™-â€™) (â€™statementsâ€™, â€™thatâ€™, â€™areâ€™) (â€™topâ€™, â€™3â€™, â€™,â€™, â€™andâ€™)\n(â€™willâ€™, â€™stateâ€™) (â€™unemployedâ€™, â€™wasâ€™, â€™aroundâ€™) (â€™toâ€™, â€™doâ€™, â€™aâ€™, â€™lotâ€™)\n(â€™yourâ€™, â€™pocketâ€™) (â€™toâ€™, â€™dataâ€™, â€™compiledâ€™) (â€™weightâ€™, â€™ofâ€™, â€™theâ€™, â€™animalâ€™)\n(â€™theâ€™, â€™Andesâ€™) (â€™withâ€™, â€™aâ€™, â€™projectâ€™) (â€™toâ€™, â€™keepâ€™, â€™ourâ€™, â€™stateâ€™)\n(â€™trainâ€™, â€™hadâ€™) (â€™wouldâ€™, â€™notâ€™, â€™backâ€™) (â€™variesâ€™, â€™fromâ€™, â€™0â€™, â€™.â€™)\n(â€™trustedâ€™, â€™.â€™) (â€™theâ€™, â€™Summerâ€™, â€™Olympicsâ€™) (â€™upâ€™, â€™aâ€™, â€™newâ€™, â€™officeâ€™)\n(â€™willâ€™, â€™knowâ€™) (â€™theâ€™, â€™Britishâ€™, â€™offâ€™) (â€™theyâ€™, â€™haveâ€™, â€™lookedâ€™, â€™toâ€™)\n(â€™timeâ€™, â€™variesâ€™) (â€™theâ€™, â€™stormâ€™, â€™hadâ€™) (â€™toâ€™, â€™fallâ€™, â€™asleepâ€™, â€™inâ€™)\n(â€™trainingâ€™, â€™(â€™) (â€™violenceâ€™, â€™,â€™, â€™exploitationâ€™) (â€™vesselâ€™, â€™whichâ€™, â€™wasâ€™, â€™responsibleâ€™)\n(â€™whoâ€™, â€™ranâ€™) (â€™wasâ€™, â€™Glâ€™, â€™perfectâ€™) (â€™toâ€™, â€™theâ€™, â€™Governorâ€™, â€™forâ€™)\n(â€™thatâ€™, â€™treatyâ€™) (â€™substanceâ€™, â€™useâ€™, â€™,â€™) (â€™toâ€™, â€™theâ€™, â€™Dallasâ€™, â€™Cowâ€™)\n(â€™toâ€™, â€™2014â€™) (â€™twoâ€™, â€™humanâ€™, â€™Bâ€™) (â€™varietyâ€™, â€™ofâ€™, â€™insightâ€™, â€™intoâ€™)\n(â€™storeâ€™, â€™pageâ€™) (â€™toâ€™, â€™assistâ€™, â€™Liverpoolâ€™) (â€™videoâ€™, â€™playbackâ€™, â€™.â€™, â€™Theâ€™)\n(â€™theyâ€™, â€™satisfiedâ€™) (â€™togetherâ€™, â€™forâ€™, â€™nearlyâ€™) (â€™veryâ€™, â€™strongâ€™, â€™teamâ€™, â€™.â€™)\n(â€™theâ€™, â€™effectiveâ€™) (â€™wellâ€™, â€™supportedâ€™, â€™byâ€™) (â€™whereâ€™, â€™weâ€™, â€™needâ€™, â€™toâ€™)\n2133",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7675231695175171
    },
    {
      "name": "Proxy (statistics)",
      "score": 0.6651323437690735
    },
    {
      "name": "Security token",
      "score": 0.6577226519584656
    },
    {
      "name": "Perplexity",
      "score": 0.6457979679107666
    },
    {
      "name": "Open source",
      "score": 0.600757360458374
    },
    {
      "name": "Tracing",
      "score": 0.5414668321609497
    },
    {
      "name": "Salient",
      "score": 0.5414296984672546
    },
    {
      "name": "Language model",
      "score": 0.43525606393814087
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.43282806873321533
    },
    {
      "name": "Insider threat",
      "score": 0.42167600989341736
    },
    {
      "name": "Interpreter",
      "score": 0.41780099272727966
    },
    {
      "name": "Natural language processing",
      "score": 0.35192355513572693
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3477456271648407
    },
    {
      "name": "Data science",
      "score": 0.34663331508636475
    },
    {
      "name": "Computer security",
      "score": 0.3338685631752014
    },
    {
      "name": "Data mining",
      "score": 0.32053256034851074
    },
    {
      "name": "Machine learning",
      "score": 0.2812918424606323
    },
    {
      "name": "Insider",
      "score": 0.19172728061676025
    },
    {
      "name": "Programming language",
      "score": 0.18911302089691162
    },
    {
      "name": "Software",
      "score": 0.1450251042842865
    },
    {
      "name": "Political science",
      "score": 0.09095436334609985
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210090176",
      "name": "Institute of Computing Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ]
}