{
  "title": "T-CVAE: Transformer-Based Conditioned Variational Autoencoder for Story Completion",
  "url": "https://openalex.org/W2964669873",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2159238107",
      "name": "Tianming Wang",
      "affiliations": [
        "Peking University",
        "University of Computer Studies Yangon"
      ]
    },
    {
      "id": "https://openalex.org/A2101284925",
      "name": "Xiao-jun Wan",
      "affiliations": [
        "University of Computer Studies Yangon",
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2593696076",
    "https://openalex.org/W2898900547",
    "https://openalex.org/W2888213795",
    "https://openalex.org/W2950037544",
    "https://openalex.org/W2621430944",
    "https://openalex.org/W2952053069",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2029846146",
    "https://openalex.org/W2158794898",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2952243835",
    "https://openalex.org/W2517226069",
    "https://openalex.org/W3022187094",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2831850043",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W1909320841",
    "https://openalex.org/W1787105636",
    "https://openalex.org/W2160467647",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2005814556",
    "https://openalex.org/W25648700",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W2963993699",
    "https://openalex.org/W2889002152",
    "https://openalex.org/W2966584111",
    "https://openalex.org/W2000900121",
    "https://openalex.org/W2605246398",
    "https://openalex.org/W2758815496",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W2798664956",
    "https://openalex.org/W2581637843"
  ],
  "abstract": "Story completion is a very challenging task of generating the missing plot for an incomplete story, which requires not only understanding but also inference of the given contextual clues. In this paper, we present a novel conditional variational autoencoder based on Transformer for missing plot generation. Our model uses shared attention layers for encoder and decoder, which make the most of the contextual clues, and a latent variable for learning the distribution of coherent story plots. Through drawing samples from the learned distribution, diverse reasonable plots can be generated. Both automatic and manual evaluations show that our model generates better story plots than state-of-the-art models in terms of readability, diversity and coherence.",
  "full_text": "T-CV AE: Transformer-Based Conditioned Variational Autoencoder for Story\nCompletion\nTianming Wangand Xiaojun Wan\nInstitute of Computer Science and Technology, Peking University\nThe MOE Key Laboratory of Computational Linguistics, Peking University\nfwangtm, wanxiaojung@pku.edu.cn\nAbstract\nStory completion is a very challenging task of gen-\nerating the missing plot for an incomplete story,\nwhich requires not only understanding but also in-\nference of the given contextual clues. In this pa-\nper, we present a novel conditional variational au-\ntoencoder based on Transformer for missing plot\ngeneration. Our model uses shared attention lay-\ners for encoder and decoder, which make the most\nof the contextual clues, and a latent variable for\nlearning the distribution of coherent story plots.\nThrough drawing samples from the learned distri-\nbution, diverse reasonable plots can be generated.\nBoth automatic and manual evaluations show that\nour model generates better story plots than state-\nof-the-art models in terms of readability, diversity\nand coherence.\n1 Introduction\nStory completion is a task of generating the missing plot for\nan incomplete story. It is a big challenge in machine com-\nprehension and natural language generation, related to story\nunderstanding and generation [Winograd, 1972; Black and\nBower, 1980]. This task requires machine to ﬁrst understand\nwhat happens in the given story and then infer and write\nwhat would happen in the missing part. It involves two as-\npects: understanding and generation. Story understanding in-\ncludes identifying persona [Bamman et al., 2014], narratives\nschema construction [Chambers and Jurafsky, 2009 ] and so\non. Generation is the next step based on understanding, re-\ngarded as making inference based on clues in the given story.\nA good generated story plot should be meaningful and coher-\nent with the context. Moreover, the incontinuity of the input\ntext makes the understanding and generation more difﬁcult.\nA recently proposed commonsense stories corpus named\nROCStories [Mostafazadeh et al., 2016a] provides a suitable\ndataset for the story completion task. The stories consist of\nﬁve sentences that reﬂect causal and temporal commonsense\nrelations of daily events. Based on this corpus, we deﬁne our\ntask as follows: given any four sentences of a story, our goal\nis to generate the missing sentence, which is regarded as the\nmissing plot, to complete this story. Many previous works\nfocus on selecting or generating a reasonable ending for an\nGiven Story:  My Dad loves chocolate chip cookies.  \n_______________. I decided I would learn how to make them. I \nmade my first batch the other day. My Dad was very surprised and \nquite happy! \nGold standard: My Mom doesn't like to make cookies because \nthey take too long. \nNon-coherent: He has been making them all week.   \nGeneric or dull: He always ate them. \n \nFigure 1: An example incomplete story with different generated\nplots.\nincomplete story [Guan et al., 2018; Li et al., 2018; Chen\net al., 2018]. These tasks are the specialization of our story\ncompletion task and thus prior approaches are not suitable\nfor generating the beginning or middle plot of the story. In\naddition, they tend to generate generic and non-coherent plot.\nFigure 1 shows an example.\nTo address the issues above, we propose a novel\nTransformer-based Conditional Variational AutoEncoder\nmodel (T-CV AE) for story completion. We abandon the\nRNN/CNN architecture and use the Transformer [Vaswani et\nal., 2017 ], which is a stacked attention architecture, as the\nbasis of our model. We adopt a modiﬁed Transformer with\nshared self-attention layers in our model. The shared self-\nattention layer allows decoder to attend to the encoder state\nand the decoder state at the same time. The encoder and de-\ncoder are put in the same stack so that information can be\npassed in every attention layer. This modiﬁcation helps the\nmodel make the most of the contextual clues. Upon this mod-\niﬁed Transformer, we further build a conditional variational\nautoencoder model for improving the diversity and coherence\nof the answer. A latent variable is used for learning the dis-\ntribution of coherent story plots and then it is incorporated in\nthe decoder state by a combination layer. Through drawing\nsamples from the learned distribution, our model can gener-\nate story plots of higher quality.\nWe perform experiments on the benchmark ROCStories\ndataset. Our model strongly outperforms prior methods and\nachieves the state-of-the-art performance. Both automatic\nand manual evaluations show that our model generates better\nstory plots in terms of readability, diversity and coherence.\nOur model also outperforms the state-of-the-art model on the\nstory ending generation task. We further study an interesting\nphenomenon that the scores of neural models on automatic\nmetrics vary when the position of missing plot in story varies,\nand we attribute the reason to the structure of human-written\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5233\nstories. Our contribution can be summarized as follows:\n\u000fTo the best of our knowledge, this is the ﬁrst at-\ntempt to address the story completion task of gen-\nerating missing plots in any position and we pro-\npose a novel Transformer-based conditional variational\nautoencoder(T-CV AE) for this task. Our code is avail-\nable at https://github.com/sodawater/T-CV AE.\n\u000fOur model achieves the state-of-the-art performance and\nboth automatic and manual evaluations show that our\nmodel can generate better story plots in terms of read-\nability, diversity and coherence.\n\u000fWe study the difference of generating story plots in dif-\nferent positions.\n2 Related Work\n2.1 Story Understanding\nSeveral lines of research have been done in the ﬁeld of story\nunderstanding. Early works focus on learning the represen-\ntation of narratives [Schank and Abelson, 1977; Chambers\nand Jurafsky, 2008]. Narrative plots understanding [Goyal et\nal., 2010] and character understanding [Bamman et al., 2014]\nhave also been studied. Recent works attempt to tackle the\nstoy-cloze task proposed by [Mostafazadeh et al., 2016a],\nwhich requires to select a correct ending from two candi-\ndates given a story context. Feature-based classiﬁcation mod-\nels [Mostafazadeh et al., 2016b; Chaturvediet al., 2017] mea-\nsure the coherence between candidates and the given story\ncontext from aspects of sentiment and topic. Neural network\nmodels have also been applied to this task[Chen et al., 2018].\n2.2 Story Generation\nIn story generation, most previous automatic story generation\nworks are limited to selecting a sequence of events that meet\na set of criteria and then generating a story based on the se-\nquence [Li et al., 2013; Martin et al., 2018]. These systems\nare considered as story planning systems. Recent researches\nfocus on generating coherent and ﬂuent stories about a given\ntopic. These models generate stories based on skeleton[Xu et\nal., 2018], storyline [Yaoet al., 2018] and premise [Fan et al.,\n2018]. The above story-cloze task has also been expanded to\na generation task that requires to generate a reasonable end-\ning for a given story. Model based on adversarial learning[Li\net al., 2018] and model leveraging external structured knowl-\nedge [Guan et al., 2018] have been proposed for addressing\nthe task, and the latter achieves the state of the art perfor-\nmance.\n2.3 Conditional Variational Autoencoder\nThe variational autoencoder [Kingma and Welling, 2013;\nRezende et al., 2014] is one of the most popular frameworks\nfor generation. The basic idea of V AE is to encode the input\ninto a probability distribution zand apply a decoder to recon-\nstruct the input using samples z. Conditional variational au-\ntoencoder(CV AE) is a modiﬁcation of V AE to generate text or\nimage conditioned certain given attributes. V AE/CV AE has\nbeen widely used and explored in text generation, especially\ndialog generation: V AE conditioned on dual encoder [Cao\nand Clark, 2017 ], hierarchical V AE [Serban et al., 2017 ],\nknowledge-guided CV AE[Zhao et al., 2017] and so on.\n3 Our Approach\nOur model is a Transformer-based conditional variational au-\ntoencoder, which can generate diverse and coherent story\nplots. We begin by formulating the story completion task.\nThen our Transformer model with shared self-attention lay-\ners will be introduced, which is also the basis of T-CV AE.\nFinally we will describe our T-CV AE model that incorporates\na latent variable for encoding coherent story plots. Figure 2\nshows the overall architecture of our model.\n3.1 Problem Formulation\nThe story completion task can be formulated as follows:\ngiven an incomplete story consisting of M \u00001 sentences\nx = fs1;:::;s k\u00001;sk+1;:::;s Mg, where si = wi\n1wi\n2:::wi\nni\nrepresents the i-th sentence containing ni words and k rep-\nresents the position of the missing sentence in the story, our\ngoal is to generate a one-sentence plot which is coherent with\nthe given context. The model is trained to maximize the prob-\nability p(yjx), where yis the gold plot.\n3.2 Our Transformer\nOur model is adapted from the Transformer, whose over-\nall architecture is composed of a stack of L multi-head at-\ntention layers and point-wise, fully connected feed-forward\nnetwork for both the encoder and the decoder. We omit\nthe background description and follow the formula and no-\ntations proposed by [Vaswani et al., 2017 ] in this pa-\nper. We denote queries, keys and values for attention as Q,\nK and V and multi-head attention as MultiHead(Q;K;V ),\nfeedfoward networks as FFN(x) and layer normalization as\nLayerNorm(x).\nInput Representation\nOur input representation is different from the original Trans-\nformer, since the input text in our task is not continuous. We\nuse a similar idea proposed in [Devlin et al., 2018], where the\ninput representation of a given word is constructed by con-\ncatenating the word, segment and position embeddings:\nIRwi\nj\n= WEwi\nj\n\bSEi \bPEj (1)\nwhere IRwi\nj\nis the input representation of j-th word in i-th\nsentence, WEwi\nj\nis the word embedding of wi\nj, SEi is the\nsegment embedding of i-th sentence and PEj is the posi-\ntion embedding of j-th word. For convenience, we denote\nthe packages of a set of input representations for encoder and\ndecoder as IRE and IRD respectively.\nShared Attention Layers\nThe original Transformer has separated encoder stack and de-\ncoder stack and their self-attention layers are independent. It\nis suitable for machine translation since source language and\ntarget language have different distributions. It is better to rep-\nresent them in different spaces. But in our task, the missing\nplot to be generated is a part of a story and representing it in\nthe same space as the given context could make the completed\nstory more coherent.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5234\nWord \nembedding\nSegment \nembedding\nPosition \nembedding\n S1\nMy   cookies. \n My    long. \ny\n My    happy!\nSM\n<GO> My\nMulti-Head Self Attention Masked Multi-Head \nAttention\n.\nAdd & Norm\nV K Q\nAdd & Norm\nQKV\nFeed Forward\nAdd & Norm\nFeed Forward\nAdd & Norm\nMulti-Head Attention\nc\nQ K V\nKL\n Combination\nLinear & Softmax\ny\nMy   long. <EOS>\nPosterior \nNet\nPrior Net\nEncoder & Decoder \nStack\nsharing \nparameters\nMLP\nLinear z\nz \nsharing \nparameters\nL ×\nFigure 2: Architecture of our T-CV AE model. Both prior net and the posterior net are built upon the encoder, and the posterior net takes an\nextra input y which is enclosed by a dashed line. In training phase, latent variable z fed to the combination layer is derived by the posterior\nnet, which is connected by the dashed line; in inference phase, the prior net is used for replacing the posterior net to derive latent variable z\n0\n,\nwhich is connected by solid line. The reparametrization trick is used to obtain samples of latent variable either from z while training or z\n0\nwhile inferring.\nTo better capture contextual clues, we propose shared at-\ntention layers for the encoder and the decoder. It not only\nmeans that the attention layers in the encoder and the decoder\nshare the same parameters, but also allows the decoder to at-\ntend to the encoder state and the decoder state at the same\ntime. In this way, information can pass between the encoder\nand the decoder in every layer.\nSpecially, we denote the input and output of thel-th layer in\nthe encoder and the decoder as El\nin, El\nout and Dl\nin, Dl\nout re-\nspectively. Particularly,E1\nin = IREWe and D1\nin = IRDWe,\nwhere We 2R3demb\u0002dmodel is parameter matrix, demb is the\ndimension of embedding and dmodel is the dimension of hid-\nden layers in the model. Then for encoder, the input of multi-\nhead self-attention in the encoder is the same as that in the\noriginal Transformer.\nEl\nin = El\u00001\nout\nA= MultiHead(El\nin;El\nin;El\nin)\nB = LayerNorm(A+ El\nin)\nEl\nout = LayerNorm(FFN (B) +B)\n(2)\nFor decoder, the inputs K and V for attention layers are the\ncombination of El\nin and Dl\nin. Speciﬁcally,\nDl\nin = Dl\u00001\nout\nA= MultiHead(Dl\nin;[El\nin; Dl\nin];[El\nin; Dl\nin])\nB = LayerNorm(A+ Dl\nin)\nDl\nout = LayerNorm(FFN (B) +B)\n(3)\nSimilar to the original Transformer, we use a masking in the\ndecoder to ensure that the attention and prediction for position\njcan depend only on the known words at positions preceding\nj.\nWe also share the point-wise, fully connected layers of the\nencoder and the decoder. The Transformer with shared self-\nattention layers is the basis of T-CV AE, and it can handle the\ncompletion task too. We directly use the linear transformation\nand the softmax function to convert the ﬁnal output of the\ndecoder so that it can predict word probabilities and generate\nwords.\nOt = DL\nout;tWo + bo\nPt = softmax(Ot)\n(4)\nwhere DL\nout;t is the ﬁnal decoder output at time-step t, Wo 2\nRdmodel\u0002dvocab and bo 2Rdvocab are parameters, and dvocab\nis vocabulary size. Pt is the probability distribution of the\nword to be generated at time-step t.\n3.3 T-CV AE\nUpon the Transformer, we further build T-CV AE which uses\na latent variable for learning the distribution of the coherent\nstory plots. In T-CV AE, the missing plotyis generated con-\nditioned on the given incomplete story xand a diversity and\ncoherence promoting latent variablezwhich captures the dis-\ntribution of the plots. We deﬁne the conditional distribution\np(yjx) =\nR\nzp(yjx;z)p(zjx)dz and our goal is to use neural\nnetworks to approximate p(zjx) and p(yjx;z). We refer to\np(zjx) as the prior netand p(yjx;z) as the plot generator.\nSince the integration over zis intractable, we therefore ap-\nply variational inference and optimize the corresponding evi-\ndence lower bound (ELBO):\nlog p(yjx) = log\nZ\nz\np(yjx;z)p(zjx)dz\n\u0015Eq(zjx;y)[log p(yjx;z)]\n\u0000DKL(q(zjx;y)jjp(zjx))\n(5)\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5235\nwhere q(zjx;y) is the posterior net (i.e. recognition net) to\napproximate the true posterior distribution of the latent vari-\nable z, and DKL(jj) denotes the KL-divergence. We assume\nthat zfollows multivariate Gaussian distribution with a diag-\nonal covariance matrix.\nModel Details\nFigure 2 demonstrate an overview of our model T-CV AE and\nthe pipeline of the training and inference procedures. In T-\nCV AE, the prior net and the posterior net are both built upon\nthe encoder of the modiﬁed Transformer.\nThe posterior net encodes both the given incomplete story\nxand the missing ploty. Since we assume zfollows isotropic\nGaussian distribution, q(zjx;y) \u0018 N(\u0016;\u001b2I) and then we\nhave\nh= MultiHead(c;EL\nout(x; y);EL\nout(x; y))\u0014\n\u0016\nlog(\u001b2)\n\u0015\n= hWq + bq\n(6)\nwhere cis a context vector(random initialized), which is re-\ngarded as a single query for the multi-head attention to get\nthe representation of the story h. EL\nout(x; y) stands for the ﬁ-\nnal outputs of the encoder when taking both xand yas input,\nWq 2Rdmodel\u0002dz and bq 2Rdz are parameters and dz is the\ndimension of latent variable.\nThe prior net only encodes the given story x. Similarly,\np\u0012(zjx) \u0018N(\u0016\n0\n;\u001b\n02I) and we have\nh\n0\n= MultiHead(c;EL\nout(x);EL\nout(x))\n\u0014\n\u0016\n0\nlog(\u001b\n02)\n\u0015\n= MLPp(h\n0\n)\n(7)\nwhere MLPp is a multi-layer perceptron.\nDifferent from the RNN-based CV AE, we do not use the\nlatent variable zto initialize the state of the decoder. Instead,\nwe incorporate it to the decoder state by a combination layer.\nCt = tanh([z;DL\nout;t]Wc)\nOt = CtWo + bo\nPt = softmax(Ot)\n(8)\nwhere Wc 2Rdmodel\u0002dmodel is parameter matrix. Ct is the\noutput of the combination layer at time-step tand is further\nfed to linear transformation and softmax layer to get the prob-\nability distribution.\nTraining Details\nOur model is trained similarly to [Zhao et al., 2017]. Opti-\nmizing Eq(6) consists two parts: maximizing the probability\nof reconstructing y, which can push the predictions made by\nthe posterior net and the plot generator closer to the ground\ntruth; minimizing the KL-divergence between the posterior\ndistribution and the prior distribution of z, which can push\nthe prior net to produce a reasonable probability distribution\nwhen the ground truth is no longer available. KL annealing\nis used during training, which increases the weight of the KL\nterm from 0 to 1 gradually.\n4 Experiment\nWe perform experiments on the ROCStory dataset for evalu-\nating models. The dataset is randomly split by 8:1:1 to get\nthe training, validation and test datasets with 78529, 9817\nand 9816 stories respectively. For each story, we randomly\nchoose one sentence at any position of the story as the target\nto be generated.\n4.1 Baselines\nWe compare our models with the following baselines:\nSeq2Seq. We implement a bidirectional-LSTM with atten-\ntion mechanism as a baseline. We concatenate the scope em-\nbedding and the word embedding as the input of the encoder.\nHLSTM. The story is encoded by a hierarchical LSTM: a\nword-level LSTM for encoding each sentence and a sentence-\nlevel LSTM for connecting four sentences.\nCV AE. We implement a LSTM-based CV AE model, in\nwhich the initial state of the decoder is the combination of\na latent variable and the ﬁnal state of the encoder.\nTransformer. The original Transformer [Vaswani et al.,\n2017] is also compared. The same input representation as\nour model is fed to the encoder.\nIE+MSA. [Guan et al., 2018] proposed a model using in-\ncremental encoding scheme and incorporated external struc-\ntured commonsense knowledge for generating endings for the\nincomplete stories. It achieves state-of-the-art performance\non the story ending generation task. We use the released code\n1 for training and testing on our dataset. Note that the model\ncan only be used for comparison on story ending generation.\n4.2 Parameter Settings\nWe set our model parameters based on preliminary experi-\nments on the development data. For all models including\nbaselines, dmodel is set to 512 and demb is set to 300. For\nTransformer models, the head of attention H is set to 8 and\nthe number of Transformer blocks L is set to 6. The num-\nber of LSTM layers is set to 2. For V AE models,dz is set to\n64 and the annealing step is set to 20000. We apply dropout\nto the output of each sub-layer in Transformer blocks. We\nuse a rate Pdrop = 0:15 for all models. We use the Adam\nOptimizer with an initial learning rate of 10\u00004, momentum\n\f1 = 0:9,\f2 = 0:99and weight decay \u000f= 10\u00009. The batch\nsize is set to 64. We use greedy search for all models and\ninitialize them with 300-dimensional Glove word vectors.\n4.3 Metric\nWe conduct both the automatic evaluation and manual evalu-\nation on the test set.\nBLEU, B1, B2, B3. The word-overlap score against gold-\nstandard story plot is widely used in many story generation\nworks. BLEU [Papineni et al., 2002] in this paper refers to the\ndefault BLEU-4, but we also report on other n-gram scores\n(B1, B2, B3).\n1https://github.com/JianGuanTHU/StoryEndGen\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5236\nMethods BLEU% B1% B2% B3% D1% D2% AdverSuc% Gram Logic\nHuman - - - - 7.15 42.98 92.30 2.84 2.80\nSeq2Seq 2.90 27.41 10.56 5.20 2.69 15.95 80.97 2.59 1.69\nHLSTM 2.31 25.70 9.04 4.26 2.63 14.80 72.46 2.49 1.65\nCV AE 3.03 27.73 10.79 5.40 2.72 16.32 81.18 2.52 1.90\nTransformer 3.05 27.53 10.70 5.31 2.93 16.75 82.51 2.63 1.92\nOur(T-CV AE) 4.25 29.33 12.75 6.96 3.63 23.46 87.54 2.71 2.13\nTable 1: Comparison results on the story completion task\nMethods BLEU% D1% D2% AdverSuc%\nOur(T-CV AE) 4.25 3.63 23.46 87.54\n-CV AE 3.98 3.50 21.40 86.22\n-Shared 3.56 3.05 18.79 84.83\n-Shared, -CV AE 3.05 2.93 16.75 82.51\nTable 2: Ablation study on story completion. -CV AE means only\nusing Transformer and -Shared means using seperated attention lay-\ners.\nD1, D2. The proportions of distinct unigrams and bigrams\nin the outputs [Li et al., 2015] are common metrics to evalu-\nate the diversity of generated results.\nAdverSuc. Considering there might exist many reasonable\nplots for a single incomplete story, BLEU score might be one-\nsided when only one reference is provided. Adversarial Suc-\ncess is the fraction of instances in which a model is capable\nof fooling the evaluator[Li et al., 2017], which can reﬂect the\nquality of the generated answer. We use a pre-trained coher-\nence model as the evaluator. We treat original human-written\nstories as positive examples and stories consisting of a ran-\ndom sentence(chosen from another story) as negative exam-\nples. We use BERT as the coherence discriminator.2\nGram & Logic. We also use two metrics - grammaticality\n(Gram) and logicality (Logic) for manual evaluation. Gram\nis used to evaluate whether the generated story plot is natural\nand ﬂuent while Logic for evaluating whether the plot is rea-\nsonable and coherent with the story. The score ranges from\n1 to 3. 1 means bad, 2 means okay and 3 means good. We\nemploy crowdsourced judges on Amazon Mechanical Turk to\nprovide evaluations for a random sample of 100 items. Each\nincomplete story is given and 3 judges are asked to grade the\nresults. The ﬁnal scores are averaged across different judges\nand stories.\n4.4 Results and Analysis\nAutomatic and Manual Evaluation\nTable 1 presents both automatic and manual evaluation results\nover different metrics. We can see that our T-CV AE model\nstrongly outperforms other baselines. Among all prior meth-\nods, Transformer has the best performance. Our T-CV AE\nmodel achieves the state-of-the-art scores on all automatic\nmetrics, which improves the state of art from3:05% to 4:25%\non BLEU, 2:93% to 3:63% on D1, 16:75% to 23:45% on D2\nand 82:51% to 87:54% on AdverSuc. Higher BLEU score in-\ndicates that the plot generated by our model are more close to\n2The pre-trained BERT achieves a 95% accuracy at the classiﬁ-\ncation task on validation dataset.\nthe gold standard answer than others. Noted that all methods\nget a much higher score on BLEU-1 than BLEU-2, BLEU-3,\nand it means unigrams(especially pronouns and prepositions)\nare much easier to be matched than bigrams and trigrams. All\nthe methods have high scores on AdverSuc because the neg-\native examples for training are not strong. Our model ranks\nonly second to human and it shows that our model can fool\nthe automatic evaluator than any prior method. Moreover, our\nmethod is also signiﬁcantly better than the baseline models on\nD1 and D2, which indicates that our model can generate more\ndiverse and non-generic plots for incomplete stories.\nOur model also outperforms the baseline models on man-\nual metrics, and it achieves2:71 and 2:13 on Gram and Logic\nrespectively. The results show that the our model can generate\nmore coherent and readable plots than baselines. We further\ndo t-test on manual evaluation results for our T-CV AE model\nand the orginial Transformer, and p-values are 0:0452 and\n0:00012 on Gram and Logic respectively, which indicates that\nour T-CV AE model is signiﬁcantly better than Transformer.\nThe Kappa measuring inter-rater agreement is 0.52, which\nimplies a moderate agreement.\nAblation Study\nTable 2 shows the results of ablation study on automatic met-\nrics. Without shared attention layer, the performance of our\nmodel drops by 0:69% on BLEU, 0:58% on D1, 4:67% on\nD2 and 2:69% on AdverSuc respectively, which indicates its\neffectiveness. These scores also drop when latent variable\nis removed, which means using latent variable for learning\nthe distribution can help generate more coherent and diverse\nplots. Removing both shared attention layer and latent vari-\nable, the model degrades to the standard Transformer and\nachieves the lowest score.\nPosition Study\nWe also compare our method with the prior method which\nachieves the state-of-the-art score on the story ending gener-\nation task. In Table 3, we can see that our T-CV AE model\nstrongly outperforms IE+MSA on all metrics. Comparing\nwith the results in Table 1, we see that models achieve a lower\nscore on D1 and D2 but a higher score on AdverSuc, which\nindicates that models tend to generate a generic plot for story\nending generation. We can also see that all these models\nachieve a much lower BLEU score on story ending genera-\ntion. We guess that the difﬁculty of generating plots varies\nfrom position to position.\nWe further study this phenomenon and compare the BLEU\nscores of generating plots in different position k. The re-\nsults are shown in Figure 3. We can clearly see that BLEU\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5237\nMethods BLEU% B1% B2% B3% D1% D2% AdverSuc% Gram Logic\nIE+MSA 1.73 24.43 8.21 3.50 1.85 9.87 83.08 2.57 1.60\nOur(T-CV AE) 2.61 25.74 9.87 4.80 3.05 18.86 88.92 2.73 1.97\nTable 3: Comparison results on the story ending generation task\nGiven story 1 Martin hated storms. . Martin scampered to a nearby tree to take cover. He began to beg God to\npreserve his life. Just at that moment, the clouds parted and Martin felt relieved !\nSeq2Seq One day, he heard a loud noise.\nTransformer He was afraid of storms.\nOurs One day, a big storm came and hit him.\nHuman One day Martin was working in the ﬁelds when a sudden storm arose.\nGiven story 2 . I discovered him last week. His songs were innovative and funny. I sat there and listened to him all\nday long. I decided to buy his albums when they are released.\nSeq2Seq I love music.\nTransformer My friend is a rap star.\nOurs My friend is a musician.\nHuman My new favorite youtube musician is Nicky.\nGiven story 3 When I was younger I played basketball in a local league. I was n’t very good but I was very tall. One day I accidentally scored a basket\nfor the enemy team! Somehow I thought we were on the other side of the court. .\nSeq2Seq I was so happy.\nTransformer i was so sad that i did n’t have to play basketball anymore.\nIE+MSA We ended up winning the tournament.\nOurs I was so upset that I quit.\nHuman My team laughed it off since it was n’t a big deal.\nTable 4: Case Study.\n/s49 /s50 /s51 /s52 /s53\n/s49\n/s50\n/s51\n/s52\n/s53\n/s54\n/s55\n/s56/s66 /s76 /s69 /s85 /s40 /s37 /s41\n/s107\n/s32 /s101 /s113 /s50 /s101 /s113\n/s32 /s72  /s84 /s77\n/s32 /s67 /s86 /s65 /s69\n/s32 /s84 /s114 /s97 /s110 /s115 /s102 /s111 /s114 /s109 /s101 /s114\n/s32 /s79ˇ/s114 /s40 /s84 /s45 /s67 /s86 /s65 /s69 /s41\nFigure 3: BLEU scores of different models on generating k-th sen-\ntence\nscore goes down askincreases and it drops signiﬁcantly from\nk = 1to k = 2. Our T-CV AE model achieves a BLEU score\nof 7:45% when k = 1, drops 3:51% when kcomes to 2 and\nonly achieves2:61% on generating the last sentence, i.e. story\nending. For Seq2Seq method, it achieves 5:22% on generat-\ning the beginning of story and only 1:99% on ending. To\nexplain this phenomenon, we analyze the structure of stories\nin this dataset and ﬁnd that plots become more complex when\nstory progresses. In other words, starting plot is simple and\ngeneric, paves the way for the follow-ups; subsequent plots\nbecome more speciﬁc and informative, which are hard to pre-\ndict. In Figure 3, there is another interesting phenomenon\nthat all the BLEU scores of baseline methods rise again when\nkgoes from 4 to 5. We attribute the reason to the continuity\nof the input. The given text are continuous when k = 5, and\nseparated encoder can learn a better representation compared\nwith the case k = 4. Sharing attention layer and putting the\nencoder and decoder in the same stack enable our model to\nhandle discontinuous input text better.\nCase Study\nWe present some examples of generated story plots in Table\n4. We can see our model generates more coherent and rea-\nsonable plots than other baselines. But compared with the\nhuman-written plots, plots generated by our model still have\nsome deﬁciencies in informativeness and coherence.\nIn example 1, both Transformer and our model ﬁnd the\nkeyword “storms”. But the plot generated by Transformer is\nnon-coherent and dull. Story 1 has a progressive structure that\nwe mentioned in above sections: the ending is more speciﬁc\nand informative than the beginning. This is very common in\nthis dataset. In example 2, all the methods generate starting\nplots about music but the answer generated by Seq2Seq is\nbad. In example 3, our model generates a generic but rea-\nsonable ending while all the baseline methods generate non-\ncoherent endings. In general, neural models tend to generate\ngeneric and dull plots like “I was happy” , “It was fun”. It is\nalso difﬁcult for our model to completely overcome this.\n5 Conclusion\nWe investigate the problem of generating the missing story\nplot at any position for an incomplete story. Our proposed T-\nCV AE model strongly outperforms prior methods. We evalu-\nate models on both automatic and manual metrics and results\nshow that our model can generate plots with better coherence\nand diversity. We further study the difﬁculty of generating\nplots in different positions. Our future work will focus on\nstory completion and story generation task in open domain.\nAcknowledgments\nThis work was supported by National Natural Science Foun-\ndation of China (61772036) and Key Laboratory of Science,\nTechnology and Standard in Press Industry (Key Laboratory\nof Intelligent Press Media Technology). We thank the anony-\nmous reviewers for their helpful comments. Xiaojun Wan is\nthe corresponding author.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5238\nReferences\n[Bamman et al., 2014] David Bamman, Brendan O’Connor,\nand Noah A Smith. Learning latent personas of ﬁlm char-\nacters. In ACL, page 352, 2014.\n[Black and Bower, 1980] John B Black and Gordon H\nBower. Story understanding as problem-solving. Poetics,\n9(1-3):223–250, 1980.\n[Cao and Clark, 2017] Kris Cao and Stephen Clark. Latent\nvariable dialogue models and their diversity. In EACL,\nShort Papers, volume 2, pages 182–187, 2017.\n[Chambers and Jurafsky, 2008] Nathanael Chambers and\nDaniel Jurafsky. Unsupervised learning of narrative event\nchains. In ACL, volume 94305, pages 789–797, 2008.\n[Chambers and Jurafsky, 2009] Nathanael Chambers and\nDan Jurafsky. Unsupervised learning of narrative schemas\nand their participants. In ACL-IJCNLP 2009: Volume\n2-Volume 2, pages 602–610, 2009.\n[Chaturvedi et al., 2017] Snigdha Chaturvedi, Haoruo Peng,\nand Dan Roth. Story comprehension for predicting what\nhappens next. In EMNLP, pages 1603–1614, 2017.\n[Chen et al., 2018] Jiaao Chen, Jianshu Chen, and Zhou Yu.\nIncorporating structured commonsense knowledge in story\ncompletion. arXiv preprint arXiv:1811.00625, 2018.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Fan et al., 2018] Angela Fan, Mike Lewis, and Yann\nDauphin. Hierarchical neural story generation. In ACL,\nvolume 1, pages 889–898, 2018.\n[Goyal et al., 2010] Amit Goyal, Ellen Riloff, and Hal\nDaum´e III. Automatically producing plot unit represen-\ntations for narrative text. In EMNLP, pages 77–86, 2010.\n[Guan et al., 2018] Jian Guan, Yansen Wang, and Minlie\nHuang. Story ending generation with incremental en-\ncoding and commonsense knowledge. arXiv preprint\narXiv:1808.10113, 2018.\n[Kingma and Welling, 2013] Diederik P Kingma and Max\nWelling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\n[Li et al., 2013] Boyang Li, Stephen Lee-Urban, George\nJohnston, and Mark Riedl. Story generation with crowd-\nsourced plot graphs. In AAAI, 2013.\n[Li et al., 2015] Jiwei Li, Michel Galley, Chris Brockett,\nJianfeng Gao, and Bill Dolan. A diversity-promoting ob-\njective function for neural conversation models. arXiv\npreprint arXiv:1510.03055, 2015.\n[Li et al., 2017] Jiwei Li, Will Monroe, Tianlin Shi,\nS˙ebastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial\nlearning for neural dialogue generation. In EMNLP, pages\n2157–2169, 2017.\n[Li et al., 2018] Zhongyang Li, Xiao Ding, and Ting Liu.\nGenerating reasonable and diversiﬁed story ending using\nsequence to sequence model with adversarial training. In\nCOLING, pages 1033–1043, 2018.\n[Martin et al., 2018] Lara J Martin, Prithviraj Am-\nmanabrolu, Xinyu Wang, William Hancock, Shruti\nSingh, Brent Harrison, and Mark O Riedl. Event represen-\ntations for automated story generation with deep neural\nnets. In AAAI, 2018.\n[Mostafazadeh et al., 2016a] Nasrin Mostafazadeh,\nNathanael Chambers, Xiaodong He, Devi Parikh,\nDhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and\nJames Allen. A corpus and cloze evaluation for deeper\nunderstanding of commonsense stories. In NAACL-HLT,\npages 839–849, 2016.\n[Mostafazadeh et al., 2016b] Nasrin Mostafazadeh, Lucy\nVanderwende, Wen-tau Yih, Pushmeet Kohli, and James\nAllen. Story cloze evaluator: Vector space representation\nevaluation by predicting what happens next. In Proceed-\nings of the 1st Workshop on Evaluating Vector-Space Rep-\nresentations for NLP, pages 24–29, 2016.\n[Papineni et al., 2002] Kishore Papineni, Salim Roukos,\nTodd Ward, and Wei-Jing Zhu. Bleu: a method for au-\ntomatic evaluation of machine translation. In ACL, pages\n311–318, 2002.\n[Rezende et al., 2014] Danilo Jimenez Rezende, Shakir Mo-\nhamed, and Daan Wierstra. Stochastic backpropagation\nand approximate inference in deep generative models. In\nICML, pages 1278–1286, 2014.\n[Schank and Abelson, 1977] RC Schank and R Abelson.\nScript, plans, goals and understanding: An inquiry into\nhuman knowledge structures. 1977.\n[Serban et al., 2017] Iulian Vlad Serban, Alessandro Sor-\ndoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C\nCourville, and Yoshua Bengio. A hierarchical latent vari-\nable encoder-decoder model for generating dialogues. In\nAAAI, pages 3295–3301, 2017.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, pages 5998–6008, 2017.\n[Winograd, 1972] Terry Winograd. Understanding natural\nlanguage. Cognitive psychology, 3(1):1–191, 1972.\n[Xu et al., 2018] Jingjing Xu, Xuancheng Ren, Yi Zhang,\nQi Zeng, Xiaoyan Cai, and Xu Sun. A skeleton-based\nmodel for promoting coherence among sentences in narra-\ntive story generation. In EMNLP, pages 4306–4315, 2018.\n[Yao et al., 2018] Lili Yao, Nanyun Peng, Weischedel Ralph,\nKevin Knight, Dongyan Zhao, and Rui Yan. Plan-\nand-write: Towards better automatic storytelling. arXiv\npreprint arXiv:1811.05701, 2018.\n[Zhao et al., 2017] Tiancheng Zhao, Ran Zhao, and Maxine\nEskenazi. Learning discourse-level diversity for neural di-\nalog models using conditional variational autoencoders. In\nACL, volume 1, pages 654–664, 2017.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5239",
  "topic": "Autoencoder",
  "concepts": [
    {
      "name": "Autoencoder",
      "score": 0.7607468366622925
    },
    {
      "name": "Readability",
      "score": 0.6281167268753052
    },
    {
      "name": "Computer science",
      "score": 0.6160167455673218
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5684102177619934
    },
    {
      "name": "Inference",
      "score": 0.5599387288093567
    },
    {
      "name": "Transformer",
      "score": 0.5551029443740845
    },
    {
      "name": "Latent variable",
      "score": 0.5264652371406555
    },
    {
      "name": "Plot (graphics)",
      "score": 0.5134080648422241
    },
    {
      "name": "Missing data",
      "score": 0.508609414100647
    },
    {
      "name": "Encoder",
      "score": 0.48942938446998596
    },
    {
      "name": "Machine learning",
      "score": 0.42482930421829224
    },
    {
      "name": "Natural language processing",
      "score": 0.39430803060531616
    },
    {
      "name": "Mathematics",
      "score": 0.23042932152748108
    },
    {
      "name": "Artificial neural network",
      "score": 0.2259262204170227
    },
    {
      "name": "Statistics",
      "score": 0.15275803208351135
    },
    {
      "name": "Engineering",
      "score": 0.12164711952209473
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}