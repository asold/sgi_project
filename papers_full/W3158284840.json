{
    "title": "HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation ofHands and Object in Interaction",
    "url": "https://openalex.org/W3158284840",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2955361574",
            "name": "Shreyas Hampali",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2145538302",
            "name": "Sayan Sarkar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2517120913",
            "name": "Mahdi Rad",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A206624101",
            "name": "Vincent Lepetit",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2520346623",
        "https://openalex.org/W3094927600",
        "https://openalex.org/W3175199633",
        "https://openalex.org/W2051264744",
        "https://openalex.org/W2972487609",
        "https://openalex.org/W3034479523",
        "https://openalex.org/W2469784314",
        "https://openalex.org/W3092774272",
        "https://openalex.org/W3098312772",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W2968722025",
        "https://openalex.org/W3119997354",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2998273692",
        "https://openalex.org/W3108516375",
        "https://openalex.org/W2150457612",
        "https://openalex.org/W2153169563",
        "https://openalex.org/W2330643967",
        "https://openalex.org/W3109929115",
        "https://openalex.org/W2124419806",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W2963995996",
        "https://openalex.org/W3128632815",
        "https://openalex.org/W2962811204",
        "https://openalex.org/W2963188159",
        "https://openalex.org/W2948343307",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3048921023",
        "https://openalex.org/W2210697964",
        "https://openalex.org/W2768683308",
        "https://openalex.org/W3099926824",
        "https://openalex.org/W2605973302",
        "https://openalex.org/W2798581336",
        "https://openalex.org/W3034470433",
        "https://openalex.org/W2227547437",
        "https://openalex.org/W2798637590",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2979577579",
        "https://openalex.org/W2964211001",
        "https://openalex.org/W2768466711",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963601560",
        "https://openalex.org/W3108094035",
        "https://openalex.org/W2962926199",
        "https://openalex.org/W2981514602",
        "https://openalex.org/W3113327026",
        "https://openalex.org/W3107620906",
        "https://openalex.org/W3035467087",
        "https://openalex.org/W3107825842",
        "https://openalex.org/W3100052745",
        "https://openalex.org/W3127246939",
        "https://openalex.org/W3021322563",
        "https://openalex.org/W2949924544",
        "https://openalex.org/W2963873475"
    ],
    "abstract": "We propose a robust and accurate method for estimating the 3D poses of two hands in close interaction from a single color image. This is a very challenging problem, as large occlusions and many confusions between the joints may happen. Our method starts by extracting a set of potential 2D locations for the joints of both hands as extrema of a heatmap. We do not require that all locations correctly correspond to a joint, not that all the joints are detected. We use appearance and spatial encodings of these locations as input to a transformer, and leverage the attention mechanisms to sort out the correct configuration of the joints and output the 3D poses of both hands. Our approach thus allies the recognition power of a Transformer to the accuracy of heatmap-based methods. We also show it can be extended to estimate the 3D pose of an object manipulated by one or two hands. We evaluate our approach on the recent and challenging InterHand2.6M and HO-3D datasets. We obtain 17% improvement over the baseline. Moreover, we introduce the first dataset made of action sequences of two hands manipulating an object fully annotated in 3D and will make it publicly available.",
    "full_text": "Keypoint Transformer: Solving Joint Identification in\nChallenging Hands and Object Interactions for Accurate 3D Pose Estimation\nShreyas Hampali(1), Sayan Deb Sarkar(1), Mahdi Rad(1), Vincent Lepetit(2,1)\n(1)Institute for Computer Graphics and Vision, Graz University of Technology, Graz, Austria\n(2)LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vall´ee, France\n{<firstname>.<lastname>}@icg.tugraz.at, vincent.lepetit@enpc.fr\nProject Website: https://www.tugraz.at/index.php?id=57823\nAbstract\nWe propose a robust and accurate method for estimat-\ning the 3D poses of two hands in close interaction from a\nsingle color image. This is a very challenging problem, as\nlarge occlusions and many confusions between the joints\nmay happen. State-of-the-art methods solve this problem by\nregressing a heatmap for each joint, which requires solv-\ning two problems simultaneously: localizing the joints and\nrecognizing them. In this work, we propose to separate\nthese tasks by relying on a CNN to first localize joints as 2D\nkeypoints, and on self-attention between the CNN features\nat these keypoints to associate them with the correspond-\ning hand joint. The resulting architecture, which we call\n“Keypoint Transformer”, is highly efficient as it achieves\nstate-of-the-art performance with roughly half the number\nof model parameters on the InterHand2.6M dataset. We\nalso show it can be easily extended to estimate the 3D pose\nof an object manipulated by one or two hands with high\nperformance. Moreover, we created a new dataset of more\nthan 75,000 images of two hands manipulating an object\nfully annotated in 3D and will make it publicly available.\n1. Introduction\n3D hand pose estimation has the potential to make vir-\ntual reality, augmented reality, and interaction with com-\nputers and robots much more intuitive. Recently, signifi-\ncant progress has been made for single-hand pose estima-\ntion using depth maps and even single RGB images. Be-\ning able to deal with RGB images is particularly attrac-\ntive as it does not require a power-hungry active sensor.\nMany approaches have been proposed, mostly based on di-\nrect prediction with different convolutional network archi-\ntectures [17, 21, 36, 43, 52, 57, 70] of the 3D joint locations\nor angles, or relying on rendering for fine pose estimation\nand tracking [2, 14, 39, 48, 58].\nFigure 1. Our approach accurately predicts 3D hand and object\nposes from a single RGB image in challenging scenarios including\ncomplex hand interactions (top) and 2 hands interacting with an\nobject where the hands can be severely occluded (bottom). The\nbottom example is from the H 2O-3D dataset we also introduce in\nthis paper, which contains challenging, fully and accurately 3D-\nannotated, video sequences of two hands manipulating objects.\nIn contrast to single-hand pose estimation, two-hand\npose estimation has received much less attention. This\nproblem is indeed significantly harder: The appearance sim-\nilarities between the joints of the two hands make their iden-\ntification extremely challenging. Moreover, in close inter-\naction, some of the joints of a hand are likely to be occluded\nby the other hand or itself. Thus, first detecting the left\nand right hands and then independently predicting their 3D\nposes [15, 43] performs poorly in close interaction scenar-\nios. Bottom-up approaches [37,62] directly estimate the 2D\njoint locations and their depths using one heatmap per joint.\nHowever, as shown in Fig. 2, the similarity in appearances\nInput Heatmap from [37] Right hand pose Right hand pose\nimage for the recovered recovered\nRight Index Tip by [37] by our method\nFigure 2. Similar appearances between joints and partial oc-\nclusions make previous methods prone to failure. The Inter-\nNet state-of-the-art method [37] predicts a heatmap for each joint\nbut the predicted heatmaps can become ambiguous, resulting in\nfailures when predicting the hand pose (the hand on the back in\nthis example). Our approach explicitly models the relationship be-\ntween keypoints resulting in more accurate poses. More examples\ncan be found in the supplementary material.\nof the joints and severe occlusions degrade the quality of\nheatmaps failing to localize the joints accurately. More re-\ncent works [12,25,66] have attempted to alleviate this prob-\nlem by exploiting joint-segmentation, joint-visibility, or by\nadding more refinement layers increasing the overall com-\nplexity of the network. By exploiting only keypoints, our\nmethod outperforms these methods by a large margin with\na significantly smaller model.\nAs shown in Fig. 3, instead of aiming to localize and rec-\nognize the hand joints simultaneously, we estimate the 3D\nposes of the hands in three stages: (1) We first detect “key-\npoints”, which are potential joint locations in the image, by\npredicting a single heatmap. These keypoints do not have to\nexactly match all the hand joints: The 3D poses we predict\nare still correct if some joints are not detected as keypoints,\nand if some keypoints do not correspond to joints. (2) Then,\nwe associate the keypoints with the corresponding joint or\nto the background in the case of false positives, on the ba-\nsis of the keypoint locations and their image features. This\nis done for all the keypoints simultaneously to exploit mu-\ntual constraints, using the self-attention mechanism. (3) Fi-\nnally, we predict the 3D hand poses using a cross-attention\nmodule, which selects keypoints associated with each of the\nhand joints. Our approach is agnostic to the parameteriza-\ntion of the pose and we consider three different hand pose\nrepresentations.\nOur architecture, which we call “Keypoint Trans-\nformer”, is therefore designed to explicitly disambiguate\nthe identity of the keypoints and performs very well even\non complex configurations. Fig. 1 shows its output on two\nchallenging examples, using the MANO [49] mesh as the\noutput representation. Our architecture is related to the “De-\ntection Transformer” (DETR) [8] architecture. DETR uses\nall the spatial features from a low-resolution CNN feature\nmap, combined with learned location queries to detect ob-\njects in an image. The high computational complexity of\nthe Transformer restricts DETR from using higher resolu-\ntion CNN feature maps. As we show in our experiments,\nusing the DETR-style architecture for hand pose estimation\nresults in lower accuracy and we hypothesize that this is\ndue to the use of lower resolution feature maps and features\nfrom the entire image.\nWe train and evaluate our architecture on the recent In-\nterHand2.6M hand-hand [37] and HO-3D hand-object [14]\ninteraction datasets. We also introduce a challenging dataset\nof videos with two hands interacting with an object with\ncomplete and accurate 3D annotations without markers.\nThis dataset is based on the work of [14], and we call it\nH2O-3D. Our method achieves state-of-the-art performance\non existing hand-interaction datasets and serves as a strong\nbaseline for the H 2O-3D dataset. Our experiments show\nthat on InterHand2.6M, our method achieves state-of-the-\nart performance with roughly half the number of model pa-\nrameters. We carry out several ablation studies and compare\nwith strong baselines to prove the efficacy of our approach.\n2. Related Work\nMany approaches have already been proposed for hand\nor object pose estimation from either RGB images or depth\nmaps. Here we focus mainly on works that estimate hand\nposes during hand-hand or hand-object interactions. We\nalso discuss recent advances in Transformer architectures\nfor computer vision as they are highly relevant to our work.\n2.1. Interacting Hand Pose Estimation\nHand pose estimation methods can be broadly classified\nas generative, discriminative, or hybrid approaches. Gen-\nerative methods [14, 30, 40–42, 60] fit a parametric hand\nmodel to an observed image or depth map by minimizing\na fitting error under some constraints. Discriminative meth-\nods [5, 16, 17, 23, 37, 43, 57, 71] mostly directly predict the\nhand pose from a single frame. Generative methods often\nrely heavily on tracking and are prone to drift whereas dis-\ncriminative methods tend to generalize poorly to unseen im-\nages [1]. Hybrid approaches [4, 7, 15, 38, 51, 53, 55, 56, 59,\n62,63] try to combine the best of these two worlds by using\ndiscriminative methods to detect visual cues in the image\nfollowed by model fitting.\nEarlier methods [30,40,41] for generative hand pose esti-\nmation during interaction used complex optimization meth-\nods to fit a parametric hand model to RGBD data. [14] pro-\nposed multi-frame optimization to fit hand and object mod-\nels to RGBD data from multiple RGBD cameras. Genera-\ntive methods alone often lose tracking during close interac-\ntions or occlusions and are hence combined with discrimi-\nnative methods to guide the optimization.\n[4, 59] detect the fingertips as discriminative points and\nused them in the optimization along with a collision term\nand physical modelling. Recently, [51] proposed high-\nfidelity hand surface tracking of hand-hand interactions in\na multi-view setup where the regressed 3D hand joint loca-\ntions were used for initializing the tracking. [7,15,38,43,62]\ncompute dense features or keypoints from a single RGB or\ndepth image and fit a hand model [49] to these estimates\nwith physical constraints and joint angle constraints. Fully\ndiscriminative methods [16, 17, 37, 57] jointly estimate the\n3D joint locations or hand model parameters of both the in-\nteracting hands or the interacting hand and the object by\nincorporating contacts and inter-penetrations in the train-\ning. [23] estimates the hand-object surface using implicit\nrepresentation that naturally allows modelling of the con-\ntact regions between hand and object. [12, 25] improve the\naccuracy of 3D pose estimation during hand-hand inter-\naction scenarios by incorporating joint-visibility and part-\nsegmentation cues, whereas [66] utilize refinement layers\nto iteratively refine the estimated poses.\nBy contrast with the above mentioned approaches de-\nsigned specifically for hand-hand or hand-object interaction\nscenarios, we propose in this work a unified discrimina-\ntive approach for all hand interaction scenarios. Further,\nmany previous discriminative methods perform poorly dur-\ning close hand interactions due to similarity in appearance\nof the joints. In this work, we model relationship between\nall detected joints in the image resulting in more accurate\npose estimation while keeping the model complexity low.\nThe success of discriminative methods depend on the\nvariability of training data and several hand interaction\ndatasets have been proposed. [13] first provided a marker-\nbased hand-object interaction dataset using RGBD cameras.\n[14, 71] and [17] respectively proposed real and synthetic\nhand-object interaction dataset with a single hand manipu-\nlating an object, while [29] recently developed a two-hands\nand object interaction dataset. [5] proposed single and two-\nhand object interaction dataset using infrared camera for\ncontact annotations. [37] developed a large-scale two-hand\ninteraction dataset using semi-automatic annotation process\nwith many close interactions. [54] used MoCap to obtain\npose of full body, hand, and object during interaction and\nused it to generate realistic grasp on unseen objects.\nIn this work, we also introduce a challenging two-hands-\nand-object interaction dataset which we created using the\noptimization method of [14]. Our dataset is made of videos\nof two hands from different subjects manipulating an object\nfrom the YCB dataset [64], annotated with the 3D poses of\nthe hands and the object. Our architecture already performs\nwell on this dataset and constitutes a strong baseline.\n2.2. Transformers in Computer Vision\nTransformers have recently been increasingly gaining\npopularity for vision related problems [24]. Features are of-\nten extracted from a CNN backbone and different architec-\ntures have been proposed to solve object detection [8, 69],\nimage classification [11], pose estimation [6,20,32,33] and\nlow-level image tasks [28, 65]. We refer the reader to [24]\nfor a detailed survey.\n[8, 69] proposed to combine a CNN backbone with a\nTransformer to detect objects in an image. [32] proposed\nto reconstruct the vertices of a single human body or hand\nfrom an RGB image using multiple Transformer encoder\nlayers and achieved state-of-the-art performance. [33] im-\nproved [32] by using graph convolutions along with a Trans-\nformer encoder. [20] estimated a 3D pose from hand point-\ncloud data using a Transformer encoder-decoder architec-\nture and proposed to generate query embeddings from input\npoint-cloud instead of learning them as in [8, 69]. While\nthese works are aimed at single hand pose estimation and\ntheir extension to two hands is non-trivial, our architecture\nis designed to estimate single and two-hands poses along\nwith the object pose during hand-object interaction from the\ninput RGB image.\nIn a closely related work, [6] solves the multi-person 2D\npose estimation problem using detected keypoints and per-\nson centers, by associating the joint keypoints to the correct\nperson center using attention. There are however several\nkey differences: In our case, the hand centers get very close\nto each other during close interaction, and the approach\nin [6] would not be transferable. More importantly, hand\njoints are much more ambiguous than “body joints” as they\nlook very similar to each other. Our method is also robust to\nundetected and falsely detected keypoints as we show in our\ndiscussions, while [6] cannot handle undetected keypoints.\nFurther, we show that, by randomly sampling keypoints on\nthe object, we can easily extend our method to 3D object\npose estimation during hand-object interactions.\n3. Method\nAs shown in Fig. 3, our architecture first detects key-\npoints that are likely to correspond to the 2D locations of\nhand joints and encodes them as input to the keypoint-joint\nassociation stage. The keypoints are encoded with their\nspatial locations and the image features at these locations.\nThe self-attention layers in the Keypoint Transformer dis-\nambiguate the keypoints and associates them with differ-\nent joint types and a background class. The (single) cross-\nattention layer then selects these “identity-aware keypoints”\nto predict root-joint-relative pose parameters of both hands,\nplus additional parameters such as the translation between\nthe hands and hand shape parameters.\nWe detail below the keypoint detection and encoding\nstep, how we use the Keypoint Transformer to predict the\nhands poses, the representations we considered for the 3D\nhand poses, and the loss used for training. We also explain\nhow our approach can be extended to object pose estimation\nduring hand-object interaction scenarios.\n  Nkpt  x 1\n  Nkpt x 256\nxM\nFeature map\n2D keypoint \nlocations\nFeatures\nInput image\nSingle-Channel \nKeypoints heatmap\nMulti-Head Self-\nAttention\nSample & \nConcat\nK V QFFN\nNkpt\nNkpt\n  Nkpt x Nfeat\n~\nNkpt  x 224\n~\nSpatial \nPositional \nEncoding\n  Nkpt  x 256\n  Nkpt  x 32\nU-Net\nKeypoint- Joint Association\nMatMul\nSoftMax\nMatMul\n  Nkpt x 1\nFFN\n  256  x 1\n3D prediction for joint j\nQj\nLearned \nQuery for \nJoint j\n  Nkpt  X 256\nPose EstimationKeypoint  Sampling\n  256  x 1\nW\nFigure 3. Overview of our approach. We detect keypoints which are potential locations of joints and encode them with CNN image\nfeatures and spatial embedding (Section 3.1). From this information, the self-attention module creates context-aware keypoint features\nwhich are essential for associating each keypoint with the corresponding joint (Section 3.2). A cross-attention module finally predicts for\neach joint (using learned queries) the values required for computing the hand poses (Section 3.3). The exact nature of these values depends\non the chosen representation of the hand pose (Section A). Not all the keypoints have to correspond to a joint and not all the joints have to\nbe detected as keypoints, which makes our approach very robust but still accurate as it relies on keypoints (as discussed in Section 2).\n3.1. Keypoint Detection and Encoding\nGiven the input image, we first extract keypoints that\nare likely to correspond to 2D hand joint locations. To do\nthis, we predict a single-channel heatmap H from the in-\nput image using a standard U-Net [50] architecture, and we\nkeep its local maximums using a non-differentiable, non-\nmaximum suppression operation. At this stage, we do not\nattempt to recognize which keypoint corresponds to which\njoint as it is a difficult task, and the predicted heatmap has\nonly one channel. In practice, we keep a maximum ofNhand\nkeypoints, with Nhand = 64 , while the number of hand\njoints is 42 in total for 2 hands. The 2D keypoint locations\nare normalized to [0, 1] range.\nThe ground truth heatmap H∗ is obtained by applying\na 2D Gaussian kernel of variance σ at each of the ground\ntruth 2D joint locations and the U-Net is trained to predict\nthe heatmap by minimizing the L2 loss.\nWe compute for each detected keypoint an appearance\nand spatial encoding to represent the keypoints as input to\nthe next stage. As shown in Fig. 3, for the appearance part,\nwe extract image features from the decoder of the U-Net\nnetwork. More exactly, we sample feature maps at mul-\ntiple layers of the U-Net decoder at the normalized key-\npoint locations using bilinear interpolation and concatenate\nthem to form a 3968-D feature vector, which is then reduced\ndown to a 224-D encoding vector using a 3-layer MLP. For\nthe spatial encoding, we obtain 32-D sine positional encod-\ning similar to [8] corresponding to the 2D location of the\nkeypoint. We finally concatenate the appearance and spa-\ntial encodings to form a 256-D vector representation of the\nkeypoint. The keypoint detector is pre-trained before fine-\ntuning it jointly with the rest of the pipeline.\n3.2. Keypoint-Joint Association\nFor each keypoint Ki, we have now an encoding vector\nFi ∈ R256. We use these vectors as input to the multi-\nlayer, multi-head self-attention module with NSA layers.\nThe self-attention [61] helps to model the relationship be-\ntween the keypoints and create global context-aware feature\nGi ∈ R256, for each keypoint. Such context-aware features\nare necessary to associate the keypoints with different joint\ntypes using a “keypoint-joint association” loss we denote\nLKI. As a result of LKI, the keypoint features also now en-\ncode the joint identity information along with the localized\nCNN image features.\nThe identity of keypoint k is defined by (hk, jk), where\nhk is the hand identity (left or right) and jk is the joint in-\ndex. We also use an additional ‘background’ identity for\nkeypoints that are falsely detected. The keypoint identity is\npredicted using a feed-forward network (FFN) consisting of\na 2-layer MLP, a linear projection layer and a softmax layer.\nWe use the cross-entropy loss for LKI:\nLKI =\nX\ni\nCE((hi, ji), (h∗\ni , j∗\ni )) , (1)\nwhere (h∗\ni , j∗\ni ) are the ground truth identities and CE de-\nnotes the cross-entropy loss. To obtain the ground truth\nidentity for the detected keypoints, we associate them at\ntraining time with the closest reprojection of a ground truth\n3D joint, if the distance is below a threshold γ. If there\nare no reprojected joints within a distance of γ, the key-\npoint is assigned to the background class. We empirically\nset γ = 3 pixels in our experiments. Similar to [8], the\nkeypoint identities are predicted after each layer of self-\nattention module using FFNs with shared weights and the\nloss is applied to predictions of each layer.\nThe prediction can result in multiple keypoints assigned\nto the same joint identity and some keypoints assigned to\nthe background class. As we discuss in Section 5, the key-\npoints associated to the background are ignored, while all\nthe keypoints associated with a given joint are considered\nfor estimating the pose of the corresponding joint by the\ncross-attention module.\n3.3. Pose Estimation from Identity-Aware Key-\npoints\nThe keypoint-joint association loss enables the keypoint\nfeatures to also encode joint identity information along with\nthe image features and spatial embeddings. We use a sin-\ngle cross-attention layer with learned joint queries to pre-\ndict which keypoint(s) match the queried joint identities.\nThe cross-attention operation [61] for a learned joint query\nQj ∈ R256 and features {Gi}i is given by\nCA\n\u0000\nQj, G\n\u0001\n= softmax\n \nQT\nj WKG\n16\n!\n(WV G)T , (2)\nwhere G is a matrix whose columns contain feature vectors\n{Gi}i, and WK and WV are learnable matrices of dimen-\nsion 256 × 256. Similar to [61], the cross-attention features\nare added to Qj to create a residual connection. The result-\ning features are transformed by a 3-layer MLP to map them\nto the pose space.\nThe number of joint queries depend on the pose repre-\nsentation. We consider 3 different representations and de-\nscribe them in Section A and the suppl. mat. For example,\nwe use 21 joint queries for each of the 21 joints per hand\nwhen using 2.5D pose representation. Along with the joint\nqueries, one for each joint of the two hands, we use an addi-\ntional learned query to predict the relative translationTL→R\nbetween the hands, and the 10-D MANO hand shape param-\neters β. These are learned using the L1 loss. The MANO\nshape parameters are useful when predicting the pose using\nthe MANO joint angle representation.\n3.4. Hand Pose Representations and Losses\nWe consider three main hand pose representations: 3D\njoint locations, 2.5D [21, 37] joint locations, and MANO\njoint angles [49]. Previous methods [16, 17, 22, 46, 48] have\nnoted that regressing model parameters such as joint an-\ngles is less accurate in terms of joint error than regress-\ning the joint locations directly. However, regressing MANO\njoint angles provides access to the complete hand mesh re-\nquired for modeling contacts and interpenetration during in-\nteractions [5, 17, 54] or for learning in a weakly supervised\nsetup [3,16,27], which could be interesting for future exten-\nsion of our method. As we show later in our experiments,\nthe Keypoint Transformer enables the MANO joint angle\nrepresentation to achieve competitive performance when\nObject Keypoints\nHand Keypoints\n(a) (c)(b) (d)\nFigure 4. Keypoint detection for hands and object. We train a\nU-net decoder to predict (b) a heatmap for all the joints together\nand (c) a segmentation map for the object from each we extract\nkeypoints at random locations.\ncompared to the joint location representation. We follow\nstandard practice for these three representations. We detail\nthem and their corresponding losses in the supplementary\nmaterial for completeness.\n3.5. Object Pose Estimation\nOur method generalizes easily to predict the 3D pose of\nan object together with 3D poses of hands. As shown in\nFig. 4, along with the heatmap for the hand keypoints, we\nalso predict a segmentation map of the object by adding an\nadditional prediction head to the U-Net decoder. We then\nrandomly select Nobj = 20 points from this segmentation\nmap and refer to them as ‘object keypoints’. We also tried\nestimating the heatmap of 2D reprojections of fixed points\non the object mesh and selecting their local maximums as\nobject keypoints and obtained similar results.\nWe encode the appearance and spatial locations of the\nobject keypoints in a 256-D vector, exactly like the hand\nkeypoints. Collectively, these keypoint encodings cover the\nobject appearance, allowing us to predict the 3D rotation\nand translation of the object. The encodings of Nobj object\nkeypoints and Nhand hand keypoints are provided together\nto the self-attention module.\nAlong with the hand keypoint identities (hk, jk) and the\nbackground identity described in Section 3.2, we rely on an\nadditional identity for the object. During the keypoint asso-\nciation stage, all the keypoints originating from the object\nare associated with the ‘object’ identity, allowing the cross-\nattention module to only attend to object keypoints when\nestimating the object pose. Along with the joint queries that\nestimate the hand pose, we consider 2 additional queries in\nthe cross-attention module and predict the 3D object rota-\ntion and 3D object translation relative to the right hand in a\nmanner similar to that of hand pose. The object rotation is\nparameterized using the method proposed in [68].\nWe use a symmetry-aware object corner loss similar to\n[45] to train the network:\nLobj-pose = min\nR∈S\n1\n8\n8X\ni=1\n||P · Bi − P∗ · R · Bi||2\n2 , (3)\nwhere P and P∗ denote the estimated and ground-truth ob-\nject poses, Bi denotes the ith corner of the 3D bounding box\nfor the object in rest pose, and S is the set of rotation matri-\nces which, when applied to the object, does not change its\nappearance.\n3.6. End-to-End Training\nWe train our architecture end-end by minimizing the sum\nof the losses introduced above:\nL = LH + LKI + LT + Lhand-pose + Lobj-pose , (4)\nwhere Lhand-pose is the loss on the hand poses (detailed in\nthe suppl. mat.) and LT is the L1 loss for relative transla-\ntion between the two hands. Note that the keypoint detector\nis pretrained before training the entire network end-to-end.\nMore optimization details are also given in the suppl. mat.\n4. Evaluation\nWe evaluated our method on three challenging hand\ninteraction datasets: InterHand2.6M, HO-3D, and our\nH2O-3D dataset we introduce with this paper. We discuss\nthem below.\n4.1. InterHand2.6M\nTraining and test sets. InterHand2.6M [37] is a recently\npublished two-hand interaction dataset with many challeng-\ning poses. It was annotated semi-automatically and contains\n1.36M train images and 849K test images.\nMetrics. We report the Mean Per Joint Position Er-\nror (MPJPE) and the Mean Relative-Root Position Er-\nror (MRRPE) to evaluate the root-relative hand pose and\nthe translation between the hands respectively, as in [37].\nBaselines. We consider two Transformer-based baseline\narchitectures. The first baseline (‘CNN+SA’) provides the\nlow-resolution (32× downsampled) CNN feature maps af-\nter flattening along the spatial dimensions as input to the\nTransformer encoder containing self-attention (SA) mod-\nules. The output tokens of the encoder are concatenated\nand the pose is predicted using an MLP. The second base-\nline (‘CNN+SA+CA’) is more similar to DETR [8], where\nMPJPE (mm) MRRPE\nSingle Hand Two Hands All (mm)\nCNN+SA 13.53 16.87 15.31 33.84\nCNN+SA+CA (DETR [8]) 12.81 15.94 14.48 32.87\nInterNet [37] 12.16 16.02 14.22 32.57\nOurs 10.99 14.34 12.78 29.63\nDong et al. [25] - - 12.08 -\nOurs 9.10 11.98 11.30 21.89\nFan et al. [12] 11.32 15.57 - 30.51\nOurs 11.08 15.33 13.41 30.87\nTable 1. Comparison with 2 baselines and the state-of-the-art\nmethods on InterHand2.6M [37]. We compare with [12, 25, 37]\nusing the different train/test splits reported in their works.\nthe low-resolution CNN feature maps are provided to the\nTransformer encoder-decoder architecture. The Trans-\nformer decoder contains SA and cross-attention (CA) mod-\nules. The queries in the decoder are learnt and the pose is\npredicted using FFN similar to our Keypoint Transformer.\nWe provide more details about the baselines in the suppl.\nmat. These baselines help to understand the importance of\nkeypoint sampling and selection for pose estimation.\nResults. Table 1 compares the accuracy of our method\nwith the state-of-the-art method InterNet [37], and the two\nbaselines, when using the 2.5D pose representation. Our\nmethod achieves 10% higher accuracy than InterNet, which\nis a CNN-based architecture, and 16% and 12% higher ac-\ncuracy than the two baselines, respectively. The higher ac-\ncuracy of ‘CNN+SA+CA’ w.r.t ‘CNN+SA’ baseline demon-\nstrates that soft-selection of image features by the decoder\nimproves the accuracy. Further, the higher accuracy (12%)\nof our Keypoint-Transformer w.r.t the ‘CNN+SA+CA’ ar-\nchitecture shows that use of keypoint features for pose es-\ntimation instead of features from the entire image increases\nthe overall accuracy.\nWe compare our method with [25] and [12] using their\ntrain and test splits. [12, 25] use per-joint heatmaps coupled\nwith joint visibility and segmentation guided features to im-\nprove the accuracy of the pose estimation, thus resulting in\nthe model complexity that is higher than InterNet [37]. Our\nmethod with a model complexity same as [37] (see Sec-\ntion 5) still outperforms these state-of-the-art methods. We\nshow qualitative results in Fig. 5 and in the suppl. mat.\n4.2. HO-3D\nTraining and test sets. The HO-3D [14] dataset contains\nautomatically annotated hand-object interaction sequences\nof a right hand and an object from the YCB [64] dataset.\nIt contains 66K training images and 11K test images. We\nconsider only objects seen in the training set for evaluation.\nMetrics. As in [14], we report the mean joint error\nafter scale-translation alignment of the root joint and the\narea-under-the-curve (AUC) metrics to evaluate the hand\npose. The object pose is computed w.r.t to the hand frame\nof reference and is evaluated using the standard Maximum\nSymmetry-Aware Surface Distance (MSSD) [19], as it con-\nsiders the symmetricity of objects.\nResults. We use 3D joint representation to estimate the\nhand pose. Table 2 compares the accuracy of the pro-\nposed hand-object pose estimation method with other ap-\nproaches. Keypoint Transformer performs significantly bet-\nter than previous methods [14,16,17]. As [14,16,17] do not\nconsider symmetricity of objects during training and eval-\nuation, we also report our results in a similar setting. We\nshow qualitative results in Fig. 6. Please refer to supplemen-\ntary material for quantitative comparison with more recent\nworks.\nFigure 5. Qualitative results on InterHand2.6M [37]. Our method obtains accurate poses of hands during complex interactions. We\nshow the estimated MANO model from a different view.\n  \nH2O-3D HO-3D\n  \nH2O-3D HO-3D\nHO-3D H2O-3D\nFigure 6. Qualitative results for our method on the H 2O-3D and HO-3D datasets . Our method recovers poses even under large\nocclusions by the object and achieves state-of-the-art results on HO-3D while serving as a strong baseline for our new dataset H 2O-3D.\nNote that some objects (columns 2&4) are considered to be rotationally symmetric along the z-axis.\nCamera\nIntrinsics\nImage\nCrop\nJoint\nError\nMean Joint\nAUC\nMSSD (Object\nPose Error)\n[14] Yes Yes 3.04 0.49 -\n[17] No Yes 3.18 0.46 -\n[16] Yes No 3.69 0.37 11.99\nOurs No Yes 2.57 0.54 7.02\nTable 2. Accuracy of our method on the HO-3D dataset for\nhand and object pose estimation. Our method outperforms pre-\nvious methods by a large margin.\n4.3. H2O-3D\nTraining and test sets. We introduce a dataset named\nH2O-3D comprising sequences of two hands manipulating\nan object automatically annotated with the 3D poses of the\nhands and the object, by extending the work of [14] to con-\nsider two hands. Figs. 1 and 6 show some images. Five dif-\nferent subjects manipulate 10 different objects from YCB\nusing both hands with a functional intent. We captured\n60’998 training images and 15’342 test images using a 5\nRGBD cameras multi-view setup. The H 2O-3D test set\ncontains 7 objects seen in the training set and 1 unseen ob-\nject. More details are provided in the supplementary mate-\nrial. H 2O-3D is significantly more challenging than previ-\nous hand interaction datasets as there are many large occlu-\nsions between the hands and the objects.\nMetrics and Results We use the 3D joint representa-\ntion for the hand pose and evaluate the accuracy using the\nMPJPE and MRRPE metrics (see Section 4.1) for the hand\nand the MSSD metric for the object (see Section 4.2). De-\ntails about the angle of symmetry for different objects con-\nsidered during training and evaluation is provided in the\nsuppl. mat. Due to large occlusions of the object by the\nhands, a portion of images are unsuitable for object pose\nestimation. We identify these images as the ones whose\nground truth object segmentation area is less than 2% of the\ncropped image area and exclude them from the object pose\nestimation during training and evaluation. We also used the\nHO-3D train split and mirrored the images randomly during\ntraining to obtain right hand- and left hand-only images, to\nlater combine with the training set of H2O-3D.\nOur method achieves an MPJPE of 3.09 cm and an MR-\nRPE of 8.28 cm on this dataset. Due to large occlusions by\nthe object, estimating the translation between the hands is\nmore challenging and the MRRPE is about 2.5 times worse\nthan on InterHands2.6M which does not contain objects.\nOn objects, our method achieves MSSD values of 7.96 cm.\nWe provide object-specific MSSD values in the supplemen-\ntary material. Fig. 6 shows qualitative results.\n  \nKpts. Heatmap Output PoseInput Image Left-Index MCPLeft-Index DIP\nFigure 7. Visualizing the cross-attention weights for two joint\nqueries of the left hand. The radius of the red circles are pro-\nportional to the weights. When the joint is occluded like the DIP\njoint on the second row, nearby visible keypoints are selected by\nthe attention mechanism for pose estimation.\n  \n(a)\n(b)\n(c)\n(d)\nFigure 8. Robustness to noisy keypoints. In this example, we\nadded noisy keypoints around the middle finger PIP joint. Most of\nthe noisy keypoints are predicted to belong to background class (in\nred), while some are associated with the PIP joint (in blue). The\nnoisy keypoints associated with the PIP joint have all higher cross-\nattention weights (c) and are considered for final pose estimation.\n  \nw/o\nw/\nOutput PoseIndex DIP Index MCPInput Image\nFigure 9. Cross-attention with and without the keypoint-joint\nassociation loss LKI. LKI makes the keypoints ‘identity-aware’,\nresulting in higher accuracy.\n5. Discussion\nWe report here the results of experiments we perform us-\ning InterHand2.6M (V0.0) to understand better our method.\nVisualization of cross-attention. We visualize the cross-\nattention weights in Fig. 7 for two joint queries of the left-\nindex finger. When the joint is not occluded, as in the first\nrow, each joint query attends to the keypoint whose loca-\ntion coincides with the corresponding joint location in the\nimage. In other words, local image features at the loca-\ntion of the joint are used to estimate the pose of that joint.\nWe believe this property of using local image features helps\nin achieving higher accuracy than other CNN-based ap-\nproaches [16, 17, 37]. In the second row of Fig. 7, the left\nindex finger is occluded except for the MCP joint and no\nkeypoints are detected for the invisible joints. The cross-\nattention module selects nearby visible keypoints resulting\nCamera\nIntrinsics\nMPJPE (mm) MRRPE\n(mm)Single Hand Two Hands All\n3D No 12.42 17.08 14.76 33.14\n2.5D Yes 11.73 17.69 14.73 34.40\nθ No 15.36 20.61 18.01 37.91\nTable 3. Accuracy obtained with the 3 different pose represen-\ntations.\nNCA= 1, VaryingNSA NSA= 6, VaryingNCA [37]0 3 6 1 3 6\nSingle Hand 12.34 11.77 11.24 11.24 11.14 11.08 12.63\nTwo Hands 16.93 15.55 15.44 15.44 15.35 15.33 17.36\nTable 4. 3D pose accuracy (MPJPE, in mm) for different num-\nbers of self-attention (NSA) and cross-attention (NCA) layers.\nResnet-18 Resnet-34 Resnet-50 [37] (Resnet-50)\nTotal Params 28M 38M 48M 48M\nSingle Hand 11.67 11.99 11.28 12.63\nTwo Hands 16.78 16.41 15.32 17.36\nTable 5. 3D pose accuracy (MPJPE, in mm) for different back-\nbones.\nin a more global-level feature for estimating the joint pose.\nRobustness to noisy keypoints. To demonstrate the ro-\nbustness of our method, we added incorrect keypoints\naround the detected keypoints. As shown in Fig. 8 and sup-\nplementary material, most of these keypoints are labeled as\nbackground and all the keypoints that are assigned to the\nsame joint are considered equally for the pose estimation.\nImportance of the keypoint-joint association loss LKI.\nLKI helps the cross-attention module to select the appro-\npriate features for pose estimation as visualized in Fig. 9.\nFurther, LKI improves MPJPE by 10% (17.08 mm v/s 18.91\nmm) and MRRPE by 15% (33.14 mm v/s 38.96 mm) on\ninteracting hand images.\nAccuracy with different pose representations. Table 3\ncompares the accuracy of the 3 hand pose representations\nthat we consider. While the accuracy of the 3D and 2.5D\nrepresentations are similar, the joint angle representation re-\nsults in lower accuracy, in line with the observation from\nprevious works [16, 17, 22, 46, 48].\nEffect of the number of self-attention (SA) and cross-\nattention (CA) layers. Table 4 reports the MPJPE with\ndifferent combinations of SA and CA layers. Even in the\nabsence of any SA layers, our method outperforms [37].\nAdding more CA layers has little effect on the accuracy.\nEffect of the number of parameters. Table 5 reports the\nMPJPE for different CNN backbones. While larger back-\nbones improve the accuracy, our method outperforms [37]\neven with a Resnet-18 backbone with approximately half\nthe total number of parameters.\n6. Conclusion\nWe showed that, by integrating a keypoint detector into\na Transformer architecture, we could predict 3D poses of\nhands and objects from very challenging images, in a much\nmore accurate way than a standard Transformer architecture\ndoes. As we rely on keypoints, we believe that our approach\nis more general and could be applied to other problems,\nsuch as human and other articulated objects pose prediction\nand object category pose prediction [47].\nAcknowledgments. This work was supported by the\nChristian Doppler Laboratory for Semantic 3D Computer\nVision, funded in part by Qualcomm Inc, and Chistera\nIPalm.\nSupplementary Material\nIn this supplementary material, we discuss the limita-\ntions of our method, provide more details about the experi-\nments and also show several qualitative results and compar-\nisons. We also refer the reader to theSupplementary Video\nfor visualization of results on different action sequences.\nA. Hand Pose Representations and Losses\nWe detail the three possible representations mentioned in\nSection 3.4 of the paper. We assume 21 3D-joint locations\nper hand as in the MANO [49] model. The losses for each\nof the 3 representations are summarized in Table 6.\n3D representation. In this representation, each joint j\nis associated with a parent-relative joint vector V (j) =\nJ3D(j) − J3D(p(j)), where J3D is the 3D joint location and\np(j) refers to the parent joint index of joint j. We estimate\n20 joint vectors per hand using 20 joint queries, one for each\nskeletal bone (40 queries for two hands), from which we can\ncompute the root-relative 3D location, Jr\n3D of each joint by\nsimple accumulation. The advantage of this representation\nis that it defines the hand pose relative to its root without\nrequiring knowledge of the camera intrinsics.\n2.5D representation [21, 37]. In this representation, each\njoint is parameterised by its 2D location J2D, and the dif-\nference ∆Zp between its depth and the depth of its par-\nent joint. The camera intrinsics matrix K and the absolute\ndepth Zroot of the root joint (the wrist) [37] or the scale\nof the hand [21] are then required to reconstruct the 3D\npose of the hand in camera coordinate system as J3D =\nK−1 · (Zroot + ∆Zr) ·\n\u0002\nJ2Dx , J2Dy , 1\n\u0003T\n, where ∆Zr is the\nroot-relative depth of the joint computed from its predicted\n∆Zp and the predicted ∆Zp for its parents. J2Dx , J2Dy are\nthe predicted x and y coordinates of J2D.\nWhen using this representation, we also predict the root\ndepth Zroot separately using RootNet [35] as in [37]. Each\njoint query estimates the J2D and ∆Zr for that joint and we\nrequire a total of 21 joint queries (42 for two hands), one for\neach joint location to estimate the 2.5D pose per hand.\nMANO joint angles, θ [49]. In this representation, each\n3D hand pose is represented by 16 3D joint angles in the\nhand kinematic tree and is estimated using 16 joint queries\nRepresentation Lhand−pose\n3D P\nj||V(j)−V(j)∗||1+P\nj||Jr3D(j)−Jr∗\n3D(j)||1\n2.5D P\nj||J2D(j)−J∗2D(j)||1+P\nj|∆Zr(j)−∆Zr∗\n(j)|\nθ P\nj||Jr3D(j)−Jr∗\n3D(j)||1+P\nj||θ(j)−θ∗(j)||1\nTable 6. Hand pose losses for different pose representations. x∗\ndenotes the ground-truth values for variable x and x(j) the value\nof x at joint j.\nper hand, one for each joint. The MANO hand shape param-\neter is estimated along with the relative translation between\nthe hands using an additional query. Given the predicted 3D\njoint angles θ for each hand and the shape parameters β, it\nis possible to compute the root-relative 3D joint locations,\nJr\n3D of each hand.\nB. Comparison with state-of-the-art on HO3D\nDataset\nWe compare the performance our method with several\nother methods on the HO-3D(V2) and HO-3D(V3) datasets\nand show the results in Tab. 7 and Tab. 8, respectively. On\nHO-3D(V2), the performance of our method is very close to\nthe HandOccNet [44], which achieves the highest accuracy\nwhen considering the scale-translation aligned MPJPE met-\nric. However, HandOccNet achieves higher accuracy when\nconsidering the procrustes aligned MPJPE metric. On the\nHO-3D(V3) dataset, our method performs worse than Arti-\nBoost [31], which uses additional training data.\nC. Method Limitations\nThough our method results in accurate poses during in-\nteractions, the results are sometimes not plausible as we\ndo not model contacts and interpenetration [5, 17, 23] be-\ntween hands and objects. Further, during highly complex\nand severely occluded hand interactions as we show in the\nlast row of Fig. 18, our method fails to obtain reasonable\nhand poses. We believe these problems can be tackled in\nthe future by incorporating temporal information and phys-\nical modeling into our architecture.\nD. Hand-Object Pose Estimation Pipeline\nIn Fig. 10, we show the complete pipeline of our\nKeypoint-Transformer architecture for estimating poses of\ntwo hands and object during interaction.\nE. Implementation details\nThe encoder of our U-Net [50] is based on ResNet-\n50 [18] architecture while a series of upsampling and con-\nvolutional layers with skip connections forms the U-Net\ndecoder. We use 256 ×256 pixels as input image resolu-\ntion, 128×128 pixels as heatmap resolution, and set the 2D\nGaussian kernel variance, σ to 1.25 during training. The\nMethod Pose Repr. Joint Error\n(scale and trans.\nalign.) in cms\nJoint Error AUC\n(scale and trans.\nalign.)\nJoint Error\n(Procrustes\nalign.) in cms\nJoint Error AUC\n(Procrustes\nalign.)\nMesh Error\n(Procrustes\nalign.) in cms\nMesh Error AUC\n(Procrustes\nalign.)\nMesh Error\nF-Score\n@5mm\nMesh Error\nF-Score\n@15mm\nMETRO [32] Mesh 2.89 0.504 1.04 0.792 1.11 0.779 0.484 0.946\nLiu et al. [34] Joint angle 3.17 0.463 0.99 0.803 0.95 0.810 0.528 0.956\nHandOccNet [44] Joint angle 2.40 0.557 0.91 0.819 0.88 0.819 0.564 0.963\nI2L-MeshNet [36] Mesh 2.60 0.529 1.12 0.775 1.39 0.722 0.409 0.932\nPose2Mesh [10] Mesh 3.33 0.480 1.25 0.754 1.27 0.749 0.441 0.909\nI2UV-HandNet [9] Mesh - - 0.99 0.804 1.01 0.799 0.500 0.943\nZheng et al. [67] 2.5D 2.51 0.541 - - - - - -\nHampali et al. [14] 3D 3.04 0.494 1.07 0.788 1.06 0.790 0.506 0.942\nHasson et al. [17] Joint angle 3.18 0.461 1.10 0.780 1.12 0.777 0.464 0.939\nHasson et al. [16] Joint angle 3.69 0.369 1.14 0.773 1.14 0.773 0.428 0.932\nArtiBoost [31] 2.5D 2.53 0.532 1.14 0.773 1.09 0.782 0.488 0.944\nOurs-ResNet18 Joint angle 3.11 0.459 1.10 0.780 1.13 0.774 0.444 0.935\nOurs-ResNet50 3D 2.57 0.553 1.08 0.786 - - - -\nTable 7. Comparison with state-of-the-art methods on HO-3D V2 dataset.\nMethod Pose Repr. Joint Error\n(scale and trans.\nalign.) in cms\nJoint Error AUC\n(scale and trans.\nalign.)\nJoint Error\n(Procrustes\nalign.) in cms\nJoint Error AUC\n(Procrustes\nalign.)\nMesh Error\n(Procrustes\nalign.) in cms\nMesh Error AUC\n(Procrustes\nalign.)\nMesh Error\nF-Score\n@5mm\nMesh Error\nF-Score\n@15mm\nArtiBoost [31] 2.5D 2.34 0.565 1.08 0.785 1.04 0.792 0.507 0.946\nOurs-ResNet50 3D 2.48 0.575 1.09 0.785 - - - -\nTable 8. Comparison with state-of-the-art methods on HO-3D V3 dataset. Note that ArtiBoost [31] uses additional training data which is\nnot used in our method.\n256×256 pixel input image patch is loosely cropped around\nthe hand and object. We use Adam [26] optimizer with a\nlearning rate of 10-4 and 10-5 for the attention modules and\nCNN backbone, respectively. The network is trained for 50\nepochs on 3 Titan V GPUs with a total batch size of 78 and\nuses on-line augmentation techniques such as rotation, scale\nand mirroring during training.\nF. Baseline Architectures\nWe detail here the two baselines, ‘CNN+SA’ and\n‘CNN+SA+CA’ considered in Section 4.1 of the main pa-\nper. Figures 11 and 12 show their architectures. We used\n256 × 256 cropped images as input to the CNN resulting in\na feature map of spatial dimensions 8 × 8 and 2048 chan-\nnels. The features are flattened along the spatial dimensions\nand the 64 features are converted to 224 dimensions using 3\nMLP layers. These features are then concatenated with 32-\nD positional embeddings resulting in 256-D features and are\nprovided to the Transformer encoder. The networks were\ntrained to output the 2.5D pose representation for 50 epochs\non 3 Titan V GPUs with a batch size of 78. The joint queries\nin ‘CNN+SA+CA’ are learned in a similar way as for our\nKeypoint Transformer.\nG. Robustness to Noisy Keypoints\nWe show more examples to demonstrate the robustness\nof our method to noisy keypoints. We consider two scenar-\nios, adding noisy keypoints to the set of detected keypoints,\nand randomly removing some keypoints from the set of de-\ntected keypoints. We show results in Figures 13 and 14,\nrespectively. The number of detected keypoints for these\ncases were 48 and we added 30 additional noisy keypoints\nfor the former scenario and retained only 30 keypoints for\nthe latter scenario.\nH. H2O-3D Dataset\nOur dataset contains sequences of two hands interact-\ning with an object, captured on a multi-view setup with 5\nRGBD cameras. We collected data from six different sub-\njects and considered ten objects from the YCB dataset with\neach subject manipulating the object with a functional in-\ntent. The dataset is automatically annotated with 3D poses\nof hands and objects using the optimization method of [14].\nThe dataset contains 60’998 training images and 15’342 test\nimages from 17 different multi-view sequences in total. As\nexplained in the main paper, we only consider 9’098 images\nfrom the set of 15’342 test images for object pose evalua-\ntion as the objects in the remaining images are barely visible\ndue to occlusion by the hands. We show some sample an-\nnotations from the dataset in Fig. 15. Table 9 shows the\nlist of YCB objects and their axis and angle of symmetry\nconsidered during our training and evaluation.\nH.1. Per-Object MSSD Values with Keypoint Trans-\nformer\nTable 10 shows the accuracy of the object poses esti-\nmated by our Keypoint Transformer on the H2O-3D dataset\nusing the MSSD metric as described in Section 4.3 of the\nmain paper.\n  Nkpt  x 1\n  Nkpt x 256\nxM\nFeature map\n2D keypoint \nlocations\nFeatures\nInput image\nMulti-Head Self-\nAttention\nSample & \nConcat\nK V QFFN\nNkpt\nNkpt\n  Nkpt x Nfeat\n~\nNkpt  x 224~\nSpatial \nPositional \nEncoding\n  Nkpt  x 256\n  Nkpt  x 32\nU-Net\nKeypoint- Joint Association\nMatMul\nSoftMax\nMatMul\n  Nkpt x 1\nFFN\n  256  x 1\n3D prediction\nQj\nLearned Query \nfor {Joint j,  \nobject rot, object \ntrans}\n  Nkpt  X 256\nPose EstimationKeypoint  Sampling\n  256  x 1\nW\nSingle-Channel hand \nKeypoints heatmap\nObject Segmentation\nSelect random \nlocations on object \nsegmentation\nNon-Maximum \nSuppression Object\nFigure 10. Pipeline for hands and object pose estimation. The object keypoints are selected by randomly sampling 2D locations on the\nobject segmentation map regressed by the U-Net (Section 3.5 of main paper). The hand keypoints are selected from the single-channel\nkeypoints heatmap, also regressed by the U-Net (Section 3.1 of main paper). Each of the detected keypoints are encoded using CNN image\nfeatures and spatial embedding. The keypoints are associated with one of the 42 hand joints (21 joints per hand), the object class or the\nbackground class in the keypoint-joint association stage (Section 3.2 of main paper). The object rotation and translation w.r.t the right\nhand is estimated in the pose estimation stage using 2 different learned object queries, while the pose of each hand-joint is estimated using\nper-joint learned queries (Section 3.3 of main paper).\nTransformer \nEncoder\nCNN MLP 2.5D \nPose\nTransformer \nEncoder\nCNN MLP 2.5D \nPose\nTransformer \nDecoder\nLearned Joint \nQueries\nConcat \nFeatures\nFigure 11. The ‘CNN+SA’ baseline architecture.\nTransformer \nEncoder\nCNN MLP 2.5D \nPose\nTransformer \nEncoder\nCNN MLP 2.5D \nPose\nTransformer \nDecoder\nLearned Joint \nQueries\nConcat \nFeatures\nFigure 12. The ‘CNN+SA+CA’ baseline architecture.\nI. Qualitative Results and Comparisons\nWe provide here more qualitative results on HO-3D,\nH2O-3D and InterHand2.6M.\nI.1. HO-3D and H2O-3D Qualitative Results\nFig. 16 shows qualitative results on H2O-3D and HO-3D.\nNote that as we do not model contacts and interpenetration\nbetween hands and object, our method sometimes results in\nimplausible poses as we show in the last example of Fig. 16.\nI.2. InterHand2.6M Qualitative Results\nFig. 17 compares the estimated poses using the InterNet\nmethod from [37] and our proposed approach. As noted in\nSection 1 and Table 3 of the main paper, purely CNN-based\nInput Image\nDetected \nKeypoints\nAdditional Noisy \nKeypoints\nEstimated \nPose\nEstimated \nPose\nFigure 13. Effect of adding additional noisy keypoints. Our\nmethod predicts accurate poses even with noisy keypoints.\napproaches do not explicitly model the relationship between\nimage features of joints and tend to confuse joints during\ncomplex interactions. Our method performs well during\ncomplex interactions and strong occlusions (see last row of\nFig. 17).\nWe show more qualitative results using the MANO an-\ngle representation in Fig. 18. Our retrieved poses are very\nsimilar to ground-truth poses. As we show in the last row of\nFig. 18, our method fails during scenarios where the hand is\nseverely occluded during complex interaction.\nInput Image\nDetected \nKeypoints\nEstimated \nPose\nSubset of  Keypoints \nused for Pose \nEstimation\nEstimated \nPose\nFigure 14. Effect of using a subset of detected keypoints for pose\nestimation. We consider only 30 of the 48 detected keypoints for\npose estimation and still estimate an accurate pose.\nJ. Attention Visualization\nIn Fig. 19, we show more visualization of the cross-\nattention weights for three different joint queries. More\nspecifically, the cross-attention weights represent the multi-\nObject Axis Angle\nMustard Bottle Z 180o\nBleach Cleanser Z 180o\nCracker Box Z 180o\nSugar Box Z 180o\nPotted Meat Can Z 180o\nBowl Z ∞\nMug Z ∞\nPitcher Base Z ∞\nBanana - -\nPower Drill - -\nTable 9. H 2O-3D objects and their axis and angle of symme-\ntry considered during training and evaluation with our Keypoint\nTransformer.\nObject MSSD (cm)\nBleach Cleanser 7.7\nMug 6.5\nBanana 9.8\nPitcher Base 7.9\nBowl 7.8\nScissors 13.5\nPower Drill 8.5\nAll 7.9\nTable 10. Object pose estimation accuracy of our Keypoint Trans-\nformer on the H2O-3D dataset.\nplicative factor on each of the keypoint features for a given\njoint query. We observe that the cross-attention learns to se-\nlect keypoint(s) from respective joint location for each joint\nquery when the joint is visible. For occluded joints, features\nfrom nearby visible joints are selected.\nReferences\n[1] Anil Armagan, Guillermo Garcia-Hernando, Seungryul\nBaek, Shreyas Hampali, Mahdi Rad, Zhaohui Zhang,\nShipeng Xie, Ming-xiu Chen, Boshen Zhang, F. Xiong, Yang\nXiao, Zhiguo Cao, Junsong Yuan, Pengfei Ren, Weiting\nHuang, Haifeng Sun, Marek Hr ´uz, Jakub Kanis, Zdenek\nKrnoul, Qingfu Wan, Shile Li, Linlin Yang, Dongheui Lee,\nAngela Yao, Weiguo Zhou, Sijia Mei, Yunhui Liu, Adrian\nSpurr, Umar Iqbal, Pavlo Molchanov, Philippe Weinzaepfel,\nRomain Br´egier, Gr´egory Rogez, Vincent Lepetit, and Tae-\nKyun Kim. Measuring Generalisation to Unseen Viewpoints,\nArticulations, Shapes and Objects for 3D Hand Pose Estima-\ntion Under Hand-Object Interaction. In European Confer-\nence on Computer Vision, 2020. 2\n[2] Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Push-\ning the Envelope for RGB-Based Dense 3D Hand Pose Es-\ntimation via Neural Rendering. In Conference on Computer\nVision and Pattern Recognition, 2019. 1\n[3] Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim.\nWeakly-Supervised Domain Adaptation via GAN and Mesh\nModel for Estimating 3D Hand Poses Interacting Objects.\nIn Conference on Computer Vision and Pattern Recognition,\n2020. 5\n[4] Luca Ballan, Aparna Taneja, Juergen Gall, Luc Van Gool,\nand Marc Pollefeys. Motion Capture of Hands in Action Us-\ning Discriminative Salient Points. In European Conference\non Computer Vision, 2012. 2\n[5] Samarth Brahmbhatt, Chengcheng Tang, Christopher D.\nTwigg, Charles C. Kemp, and James Hays. ContactPose:\nA Dataset of Grasps with Object Contact and Hand Pose. In\nEuropean Conference on Computer Vision, 2020. 2, 3, 5, 9\n[6] Guillem Braso, Nikita Kister, and Laura Leal-Taix ´e. The\nCenter of Attention: Center-Keypoint Grouping Attention\nfor Multi-Person Pose Estimation. In International Confer-\nence on Computer Vision, 2021. 3\n[7] Zhe Cao, Ilija Radosavovic, Angjoo Kanazawa, and Jitendra\nMalik. Reconstructing Hand-Object Interactions in the Wild.\nIn arXiv Preprint, 2020. 2, 3\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-End Object Detection with Transformers. In European\nConference on Computer Vision, 2020. 2, 3, 4, 6\n[9] Ping Chen, Yujin Chen, Dong Yang, Fangyin Wu, Qin Li,\nQingpei Xia, and Yong Tan. I2uv-handnet: Image-to-uv pre-\ndiction network for accurate and high-fidelity 3d hand mesh\nmodeling. In International Conference on Computer Vision,\n2021. 10\n[10] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee.\nPose2mesh: Graph convolutional network for 3d human pose\nand mesh recovery from a 2d human pose. InEuropean Con-\nference on Computer Vision (ECCV), 2020. 10\nFigure 15. Samples from H 2O-3D dataset. Our dataset contains sequences with complex actions performed by both hands on YCB [64]\nobjects.\nFigure 16. Qualitative results on H 2O-3D and HO-3D [14]. Our method obtains state-of-the-art results on HO-3D while predicting\nreasonable results on H2O-3D. The last example is a failure case where the predicted relative translations are inaccurate.\nInput Image InterNet [37] 2D Pose InterNet [37] 3D Pose Ours 2D Pose Ours 3D Pose\nFigure 17. Qualitative comparison between InterNet [37] and our proposed method. Our method outputs more accurate poses even during\nstrong occlusions. Red circles indicate regions where InterNet results are inaccurate.\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image Is\nWorth 16x16 Words: Transformers for Image Recognition at\nScale. In arXiv Preprint, 2020. 3\n[12] Zicong Fan, Adrian Spurr, Muhammed Kocabas, Siyu Tang,\nMichael Black, and Otmar Hilliges. Learning to Disam-\nbiguate Strongly Interacting Hands via Probabilistic Per-\nPixel Part Segmentation. In International Conference on 3D\nVision, 2021. 2, 3, 6\n[13] Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul\nBaek, and Tae-Kyun Kim. First-Person Hand Action Bench-\nmark with RGB-D Videos and 3D Hand Pose Annotations.\nIn Conference on Computer Vision and Pattern Recognition,\n2018. 3\nInput Image\nOurs View 1\nOurs View 2\nGround-truth \nView 1\nGround-truth\nView 2\nFigure 18. Qualitative results of our method on InterHand2.6M [37] compared to ground-truth poses. Our method predicts accurate poses\nin most scenarios. The last row shows a failure case where our method cannot recover the accurate pose due to complex pose and severe\nocclusion.\nInput Image\nKeypoints \nHeatmap\nRight –  \nIndex Tip\nRight – \nMiddle PIP\nRight –   \nPinky MCP Output Pose\nFigure 19. Attention visualization for 3 joint queries. Each joint query attends to the image feature from the respective joint location.\n[14] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vin-\ncent Lepetit. HOnnotate: A Method for 3D Annotation of\nHand and Object Poses. In Conference on Computer Vision\nand Pattern Recognition, 2020. 1, 2, 3, 6, 7, 10, 13\n[15] Shangchen Han, Beibei Liu, R. Cabezas, Christopher D.\nTwigg, Peizhao Zhang, Jeff Petkau, Tsz-Ho Yu, Chun-Jung\nTai, Muzaffer Akbay, Zheng Wang, Asaf Nitzan, Gang\nDong, Yuting Ye, Lingling Tao, Chengde Wan, and Robert\nWang. MEgATrack: Monochrome Egocentric Articulated\nHand-Tracking for Virtual Reality. IEEE Transactions on\nRobotics and Automation, 39, 2020. 1, 2, 3\n[16] Yana Hasson, Bugra Tekin, Federica Bogo, Ivan Laptev,\nMarc Pollefeys, and Cordelia Schmid. Leveraging Pho-\ntometric Consistency over Time for Sparsely Supervised\nHand-Object Reconstruction. In Conference on Computer\nVision and Pattern Recognition, 2020. 2, 3, 5, 6, 7, 8, 10\n[17] Yana Hasson, G ¨ul Varol, Dimitrios Tzionas, Igor Kale-\nvatykh, Michael J. Black, Ivan Laptev, and Cordelia Schmid.\nLearning Joint Reconstruction of Hands and Manipulated\nObjects. In Conference on Computer Vision and Pattern\nRecognition, 2019. 1, 2, 3, 5, 6, 7, 8, 9, 10\n[18] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep\nResidual Learning for Image Recognition. In Conference on\nComputer Vision and Pattern Recognition, 2016. 9\n[19] Tom ´as Hodan, Martin Sundermeyer, Bertram Drost, Yann\nLabb´e, Eric Brachmann, Frank Michel, Carsten Rother, and\nJiri Matas. BOP Challenge 2020 on 6D Object Localization.\nIn Computer Vision - ECCV 2020 Workshops - Glasgow, UK,\nAugust 23-28, 2020, Proceedings, Part II, 2020. 6\n[20] Lin Huang, Jianchao Tan, Ji Liu, and Junsong Yuan. Hand-\nTransformer: Non-Autoregressive Structured Modeling for\n3D Hand Pose Estimation. InEuropean Conference on Com-\nputer Vision, 2020. 3\n[21] Umar Iqbal, Pavlo Molchanov, Thomas Breuel, Juergen Gall,\nand Jan Kautz. Hand Pose Estimation via Latent 2.5D\nHeatmap Regression. In European Conference on Computer\nVision, 2018. 1, 5, 9\n[22] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\nJitendra Malik. End-to-End Recovery of Human Shape and\nPose. In Conference on Computer Vision and Pattern Recog-\nnition, 2018. 5, 8\n[23] Korrawe Karunratanakul, Jinlong Yang, Yan Zhang,\nMichael J. Black, Krikamol Muandet, and Siyu Tang. Grasp-\ning Field: Learning Implicit Representations for Human\nGrasps. In International Conference on 3D Vision , 2020.\n2, 3, 9\n[24] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Khan, and Mubarak Shah. Trans-\nformers in Vision: A Survey. In arXiv Preprint, 2021. 3\n[25] Dong Uk Kim, Kwang In Kim, and Seungryul Baek. End-\nto-End Detection and Pose Estimation of Two Interacting\nHands. In International Conference on Computer Vision ,\n2021. 2, 3, 6\n[26] Diederik P. Kingma and Jimmy Ba. Adam: A Method for\nStochastic Optimization. In International Conference for\nLearning Representations, 2015. 10\n[27] Dominik Kulon, Riza Alp G ¨uler, Iasonnas Kokkinos,\nMichael Bronstein, and Stefanos Zafeiriou. Weakly-\nSupervised Mesh-Convolutional Hand Reconstruction in the\nWild. In Conference on Computer Vision and Pattern Recog-\nnition, 2020. 5\n[28] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner.\nColorization Transformer. In arXiv Preprint, 2021. 3\n[29] Taein Kwon, Bugra Tekin, Jan St ¨uhmer, Federica Bogo, and\nMarc Pollefeys. H2O: Two Hands Manipulating Objects for\nFirst Person Interaction Recognition. In International Con-\nference on Computer Vision, 2021. 3\n[30] Nikolaos Kyriazis and Antonis A. Argyros. Scalable 3D\nTracking of Multiple Interacting Objects. In Conference on\nComputer Vision and Pattern Recognition, 2014. 2\n[31] Kailin Li, Lixin Yang, Xinyu Zhan, Jun Lv, Wenqiang Xu,\nJiefeng Li, and Cewu Lu. ArtiBoost: Boosting articulated\n3d hand-object pose estimation via online exploration and\nsynthesis. In Conference on Computer Vision and Pattern\nRecognition, 2022. 9, 10\n[32] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-End Hu-\nman Pose and Mesh Reconstruction with Transformers. In\nConference on Computer Vision and Pattern Recognition ,\n2021. 3, 10\n[33] Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh\nGraphormer. In International Conference on Computer Vi-\nsion, 2021. 3\n[34] Shaowei Liu, Hanwen Jiang, Jiarui Xu, Sifei Liu, and Xiao-\nlong Wang. Semi-Supervised 3D Hand-Object Poses Estima-\ntion with Interactions in Time. In Conference on Computer\nVision and Pattern Recognition, 2021. 10\n[35] Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee.\nCamera Distance-Aware Top-Down Approach for 3D Multi-\nPerson Pose Estimation from a Single RGB Image. In Inter-\nnational Conference on Computer Vision, 2019. 9\n[36] Gyeongsik Moon and Kyoung Mu Lee. I2L-MeshNet:\nImage-to-Lixel Prediction Network for Accurate 3D Human\nPose and Mesh Estimation from a Single RGB Image. In\nEuropean Conference on Computer Vision, 2020. 1, 10\n[37] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori,\nand Kyoung Mu Lee. InterHand2.6M: A Dataset and Base-\nline for 3D Interacting Hand Pose Estimation from a Single\nRGB Image. In European Conference on Computer Vision,\n2020. 1, 2, 3, 5, 6, 7, 8, 9, 11, 14, 15\n[38] Franziska Mueller, Micah Davis, Florian Bernard, Olek-\nsandr Sotnychenko, Mickeal Verschoor, Miguel Otaduy, Dan\nCasas, and Christian Theobalt. Real-Time Pose and Shape\nReconstruction of Two Interacting Hands with a Single\nDepth Camera. IEEE Transactions on Robotics and Automa-\ntion, 38, 2019. 2, 3\n[39] Markus Oberweger, Paul Wohlhart, and Vincent Lepetit.\nTraining a Feedback Loop for Hand Pose Estimation. In In-\nternational Conference on Computer Vision, 2015. 1\n[40] Iason Oikonomidis, Nikolaos Kyriazis, and Antonis A. Ar-\ngyros. Full DOF Tracking of a Hand Interacting with an\nObject by Modeling Occlusions and Physical Constraints. In\nInternational Conference on Computer Vision, 2011. 2\n[41] I. Oikonomidis, Nikolaos Kyriazis, and Antonis A. Argyros.\nTracking the Articulated Motion of Two Strongly Interact-\ning Hands. In Conference on Computer Vision and Pattern\nRecognition, 2012. 2\n[42] Paschalis Panteleris, Nikolaos Kyriazis, and Antonis A. Ar-\ngyros. 3D Tracking of Human Hands in Interaction with Un-\nknown Objects. In British Machine Vision Conference, 2015.\n2\n[43] Paschalis Panteleris, Iason Oikonomidis, and Antonis A. Ar-\ngyros. Using a Single RGB Frame for Real Time 3D Hand\nPose Estimation in the Wild. In IEEE Winter Conference on\nApplications of Computer Vision, 2018. 1, 2, 3\n[44] JoonKyu Park, Yeonguk Oh, Gyeongsik Moon, Hongsuk\nChoi, and Kyoung Mu Lee. HandOccNet: Occlusion-Robust\n3D Hand Mesh Estimation Network. InConference on Com-\nputer Vision and Pattern Recognition, 2022. 9, 10\n[45] Kiru Park, Timothy Patten, and Markus Vincze. Pix2Pose:\nPixel-Wise Coordinate Regression of Objects for 6D Pose\nEstimation. In International Conference on Computer Vi-\nsion, 2019. 5\n[46] Georgios Pavlakos, Nikos Kolotouros, and Kostas Dani-\nilidis. TexturePose: Supervising Human Mesh Estimation\nwith Texture Consistency. In International Conference on\nComputer Vision, 2019. 5, 8\n[47] Georgios Pavlakos, Xiaowei Zhou, Aaron Chan, Konstanti-\nnos G Derpanis, and Kostas Daniilidis. 6-dof object pose\nfrom semantic keypoints. In International Conference on\nRobotics and Automation (ICRA), 2017. 9\n[48] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas\nDaniilidis. Learning to Estimate 3D Human Pose and Shape\nfrom a Single Color Image. In Conference on Computer Vi-\nsion and Pattern Recognition, 2018. 1, 5, 8\n[49] Javier Romero, Dimitrios Tzionas, and Michael J. Black.\nEMbodied Hands: Modeling and Capturing Hands and Bod-\nies Together. IEEE Transactions on Robotics and Automa-\ntion, 36, 2017. 2, 3, 5, 9\n[50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional Networks for Biomedical Image Seg-\nmentation. In Conference on Medical Image Computing and\nComputer Assisted Intervention, 2015. 4, 9\n[51] Breannan Smith, Chenglei Wu, He Wen, Patrick Peluse,\nYaser Sheikh, Jessica Hodgins, and Takaaki Shiratori. Con-\nstraining Dense Hand Surface Tracking with Elasticity.IEEE\nTransactions on Robotics and Automation, 39, 2020. 2\n[52] Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Otmar Hilliges,\nand Jan Kautz. Weakly Supervised 3D Hand Pose Estimation\nvia Biomechanical Constraints. In European Conference on\nComputer Vision, 2020. 1\n[53] Srinath Sridhar, Franziska Mueller, Michael Zollh ¨ofer, Dan\nCasas, Antti Oulasvirta, and Christian Theobalt. Real-Time\nJoint Tracking of a Hand Manipulating an Object from RGB-\nD Input. In European Conference on Computer Vision, 2016.\n2\n[54] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dim-\nitrios Tzionas. GRAB: A Dataset of Whole-Body Human\nGrasping of Objects. In European Conference on Computer\nVision, 2020. 3, 5\n[55] Jonathan Taylor, Lucas Bordeaux, Thomas Cashman, Bob\nCorish, Cem Keskin, Toby Sharp, Eduardo Soto, David\nSweeney, Julien Valentin, Benjamin Luff, Arran Topalian,\nErroll Wood, Sameh Khamis, Pushmeet Kohli, Shahram\nIzadi, Richard Banks, Andrew Fitzgibbon, and Jamie Shot-\nton. Efficient and Precise Interactive Hand Tracking through\nJoint, Continuous Optimization of Pose and Correspon-\ndences. IEEE Transactions on Robotics and Automation, 35,\n2016. 2\n[56] Jonathan Taylor, Vladimir Tankovich, Danhang Tang, Cem\nKeskin, David Kim, Philip Davidson, Adarsh Kowdle, and\nShahram Izadi. Articulated Distance Fields for Ultra-Fast\nTracking of Hands Interacting. IEEE Transactions on\nRobotics and Automation, 36, 2017. 2\n[57] Bugra Tekin, Federica Bogo, and Marc Pollefeys. H+O: Uni-\nfied Egocentric Recognition of 3D Hand-Object Poses and\nInteractions. In Conference on Computer Vision and Pattern\nRecognition, 2019. 1, 2, 3\n[58] Hsiao-Yu Tung, Hsiao-Wei Tung, Ersin Yumer, and Kate-\nrina Fragkiadaki. Self-Supervised Learning of Motion Cap-\nture. In Advances in Neural Information Processing Systems,\n2017. 1\n[59] Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo\nAponte, Marc Pollefeys, and Juergen Gall. Capturing Hands\nin Action Using Discriminative Salient Points and Physics\nSimulation. International Journal of Computer Vision, 118,\n2016. 2\n[60] Dimitrios Tzionas and Juergen Gall. 3D Object Reconstruc-\ntion from Hand-Object Interactions. In International Con-\nference on Computer Vision, 2015. 2\n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention Is All You Need. In Advances in Neu-\nral Information Processing Systems, 2017. 4, 5\n[62] Jiayi Wang, Franziska Mueller, Florian Bernard, Suzanne\nSorli, Oleksandr Sotnychenko, Neng Qian, Miguel Otaduy,\nDan Casas, and Christian Theobalt. RGB2Hands: Real-Time\nTracking of 3D Hand Interactions from Monocular RGB\nVideo. IEEE Transactions on Robotics and Automation, 39,\n2020. 1, 2, 3\n[63] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular\nTotal Capture: Posing Face, Body, and Hands in the Wild.\nIn Conference on Computer Vision and Pattern Recognition,\n2019. 2\n[64] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and\nDieter Fox. PoseCNN: A Convolutional Neural Network for\n6D Object Pose Estimation in Cluttered Scenes. Science,\n2018. 3, 6, 13\n[65] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning Texture Transformer Network for Image\nSuper-Resolution. In Conference on Computer Vision and\nPattern Recognition, 2020. 3\n[66] Baowen Zhang, Yangang Wang, Xiaoming Deng, Yinda\nZhang, Ping Tan, Cuixia Ma, and Hongan Wang. Interact-\ning Two-Hand 3D Pose and Shape Reconstruction from Sin-\ngle Color Image. In International Conference on Computer\nVision, 2021. 2, 3\n[67] Xiaozheng Zheng, Pengfei Ren, Haifeng Sun, Jingyu Wang,\nQi Qi, and Jianxin Liao. Joint-aware regression: Rethinking\nregression-based method for 3d hand pose estimation. In\nBritish Machine Vision Conference, 2021. 10\n[68] Y . Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao\nLi. On the Continuity of Rotation Representations in Neural\nNetworks. In Conference on Computer Vision and Pattern\nRecognition, 2019. 5\n[69] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable DETR: Deformable Transform-\ners for End-to-End Object Detection. In International Con-\nference for Learning Representations, 2021. 3\n[70] Christian Zimmermann and Thomas Brox. Learning to Es-\ntimate 3D Hand Pose from Single RGB Images. In Interna-\ntional Conference on Computer Vision, 2017. 1\n[71] Christian Zimmermann, Duygu Ceylan, Jimei Yang,\nBryan C. Russell, Max Argus, and Thomas Brox. Frei-\nHAND: A Dataset for Markerless Capture of Hand Pose and\nShape from Single RGB Images. In International Confer-\nence on Computer Vision, 2019. 2, 3"
}