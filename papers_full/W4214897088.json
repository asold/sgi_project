{
  "title": "AI and Ethics When Human Beings Collaborate With AI Agents",
  "url": "https://openalex.org/W4214897088",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2134101386",
      "name": "José J. Cañas",
      "affiliations": [
        "Universidad de Granada"
      ]
    },
    {
      "id": "https://openalex.org/A2134101386",
      "name": "José J. Cañas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4231317190",
    "https://openalex.org/W6804231597",
    "https://openalex.org/W393180982",
    "https://openalex.org/W2146948159",
    "https://openalex.org/W2603256381",
    "https://openalex.org/W2553338792",
    "https://openalex.org/W1965008419",
    "https://openalex.org/W2962564927",
    "https://openalex.org/W2568367294",
    "https://openalex.org/W2594910663",
    "https://openalex.org/W2023097517",
    "https://openalex.org/W2016151598",
    "https://openalex.org/W4243342770",
    "https://openalex.org/W2141373701",
    "https://openalex.org/W4248588746",
    "https://openalex.org/W6754482593",
    "https://openalex.org/W6740092872",
    "https://openalex.org/W2063052894",
    "https://openalex.org/W2123179704",
    "https://openalex.org/W6677763084",
    "https://openalex.org/W2063649921",
    "https://openalex.org/W2622273525",
    "https://openalex.org/W2909271637",
    "https://openalex.org/W2133394319",
    "https://openalex.org/W2811094992",
    "https://openalex.org/W2965631297",
    "https://openalex.org/W4237513721",
    "https://openalex.org/W1730934115",
    "https://openalex.org/W565376517",
    "https://openalex.org/W4205266736",
    "https://openalex.org/W2110171129",
    "https://openalex.org/W1481252446",
    "https://openalex.org/W3216137408",
    "https://openalex.org/W4205491507",
    "https://openalex.org/W2726428977",
    "https://openalex.org/W2614657806",
    "https://openalex.org/W4206981357",
    "https://openalex.org/W4301309063",
    "https://openalex.org/W1933657216"
  ],
  "abstract": "The relationship between a human being and an AI system has to be considered as a collaborative process between two agents during the performance of an activity. When there is a collaboration between two people, a fundamental characteristic of that collaboration is that there is co-supervision, with each agent supervising the actions of the other. Such supervision ensures that the activity achieves its objectives, but it also means that responsibility for the consequences of the activity is shared. If there is no co-supervision, neither collaborator can be held co-responsible for the actions of the other. When the collaboration is between a person and an AI system, co-supervision is also necessary to ensure that the objectives of the activity are achieved, but this also means that there is co-responsibility for the consequences of the activities. Therefore, if each agent's responsibility for the consequences of the activity depends on the effectiveness and efficiency of the supervision that that agent performs over the other agent's actions, it will be necessary to take into account the way in which that supervision is carried out and the factors on which it depends. In the case of the human supervision of the actions of an AI system, there is a wealth of psychological research that can help us to establish cognitive and non-cognitive boundaries and their relationship to the responsibility of humans collaborating with AI systems. There is also psychological research on how an external observer supervises and evaluates human actions. This research can be used to programme AI systems in such a way that the boundaries of responsibility for AI systems can be established. In this article, we will describe some examples of how such research on the task of supervising the actions of another agent can be used to establish lines of shared responsibility between a human being and an AI system. The article will conclude by proposing that we should develop a methodology for assessing responsibility based on the results of the collaboration between a human being and an AI agent during the performance of one common activity.",
  "full_text": "CONCEPTUAL ANALYSIS\npublished: 04 March 2022\ndoi: 10.3389/fpsyg.2022.836650\nFrontiers in Psychology | www.frontiersin.org 1 March 2022 | Volume 13 | Article 836650\nEdited by:\nRebekah Ann Rousi,\nUniversity of Vaasa, Finland\nReviewed by:\nRafael Ignacio Madrid,\nFundació per a la Universitat Oberta\nde Catalunya, Spain\nSonia Adelé,\nUniversité Gustave Eiffel, France\n*Correspondence:\nJosé J. Cañas\ndelagado@ugr.es\nSpecialty section:\nThis article was submitted to\nHuman-Media Interaction,\na section of the journal\nFrontiers in Psychology\nReceived: 15 December 2021\nAccepted: 09 February 2022\nPublished: 04 March 2022\nCitation:\nCañas JJ (2022) AI and Ethics When\nHuman Beings Collaborate With AI\nAgents. Front. Psychol. 13:836650.\ndoi: 10.3389/fpsyg.2022.836650\nAI and Ethics When Human Beings\nCollaborate With AI Agents\nJosé J. Cañas*\nMind, Brain and Behaviour Research Centre, University of Granada, Granada, Spain\nThe relationship between a human being and an AI system has to be considered as\na collaborative process between two agents during the perfo rmance of an activity.\nWhen there is a collaboration between two people, a fundament al characteristic of that\ncollaboration is that there is co-supervision, with each ag ent supervising the actions of\nthe other. Such supervision ensures that the activity achie ves its objectives, but it also\nmeans that responsibility for the consequences of the activ ity is shared. If there is no\nco-supervision, neither collaborator can be held co-respo nsible for the actions of the\nother. When the collaboration is between a person and an AI sys tem, co-supervision is\nalso necessary to ensure that the objectives of the activity are achieved, but this also\nmeans that there is co-responsibility for the consequences of the activities. Therefore,\nif each agent’s responsibility for the consequences of the a ctivity depends on the\neffectiveness and efﬁciency of the supervision that that ag ent performs over the other\nagent’s actions, it will be necessary to take into account th e way in which that supervision\nis carried out and the factors on which it depends. In the case of the human supervision\nof the actions of an AI system, there is a wealth of psychologi cal research that can\nhelp us to establish cognitive and non-cognitive boundarie s and their relationship to\nthe responsibility of humans collaborating with AI systems . There is also psychological\nresearch on how an external observer supervises and evaluat es human actions. This\nresearch can be used to programme AI systems in such a way that the boundaries of\nresponsibility for AI systems can be established. In this ar ticle, we will describe some\nexamples of how such research on the task of supervising the a ctions of another agent\ncan be used to establish lines of shared responsibility betw een a human being and an AI\nsystem. The article will conclude by proposing that we shoul d develop a methodology\nfor assessing responsibility based on the results of the col laboration between a human\nbeing and an AI agent during the performance of one common act ivity.\nKeywords: AI, ethics, agent collaboration, human-AI interaction, human factors\nINTRODUCTION\nMost approaches to the ethics of AI have been made to answer quest ions about the ethical principles\nthat should guide the development of AI systems. Thus, much th ought and research has been given\nto what can be designed, to the purpose for which it should be des igned, to the limits that should be\nplaced on the design of AI so that these systems do not cause har m to humans, and so on (\nStahl et al.,\n2017). We might call this approach the “ethics of design goals.” Howe ver, we also need to address\nanother very important aspect of ethics and AI: the responsibil ities of the people and the AI systems\nCañas AI and Ethics as Collaboration Between Agents\nwith which they interact, and the consequences of their\njoint actions. We have to recognize that, by using the term\n“intelligence, ” we are assuming that AI systems are agents\nthat “collaborate” with other human and artiﬁcial agents in\nperforming a task. Therefore, the ethical implications of the\nresults of this collaborative task have to be considered fro m the\npoint of view of the collaboration between the agents, and the\nagents’ common responsibilities. Following this approach, wh at\nwe have to do is to ask ourselves what the ethical responsibilit ies\nare for a good or bad result of individual actions in the conte xt\nof a system in which AI and human agents jointly collaborate.\nThis approach would be along the lines of what has been called\n“collective responsibility” (\nFrench and Wettstein, 2006 ).\nThis is the philosophical approach taken by authors such as\nFloridi (2016). This author has explored the ethical issues related\nto the outcomes of joint actions taken by several interactin g\nagents in the performance of a task. He speaks of “distributed\nmoral responsibility, ” which is the consequence of “distrib uted\nmoral actions.” On these issues, Floridi identiﬁes two main\npoints of view that can be taken to address distributed moral\nresponsibility. One view is what he calls “agent-oriented et hics, ”\nin which the emphasis is on the actions of each individual agen t\nand the interest is in the individual development, social wel fare,\nand ultimate salvation of each agent. From this point of view, one\nshould speak of the individual responsibility of each agent fo r the\nresults of their individual actions.\nAnother point of view is what this author calls “patient-\noriented ethics, ” which is concerned with the ultimate welf are\nof the system. The ﬁnal results of the individual actions of\nindividual agents within a system in which they collaborate have\nconsequences for the environment in which the system is loca ted.\nThe word “patient” that Floridi uses refers to the entity who\nreceives the eﬀects of the overall actions of the system. We\ncan think of a system such as a hospital in which many agents\n(healthcare workers with diﬀerent specialities) collaborat e and in\nwhich there is one external agent, the patient, who receives t he\neﬀects of the collaborative actions of all the healthcare wor kers.\nTo give another example related to AI, we can imagine that\nwe have a car that is driven jointly by an intelligent system\ninstalled in it and by a human driver. When there is an acciden t,\nthe agent-oriented approach to ethics would be concerned with\nanalyzing the ethical responsibilities of the individual ac tors,\nwho in this case are the artiﬁcial intelligence system in the\ncar and the human driver. However, patient-based ethics woul d\nbe concerned with the ethical responsibilities of the system\nconsisting of the car and the human driver as a whole, with\nrespect to the consequences of the system’s actions on the\nsurrounding environment (e.g., pedestrians).\nWhat is important is that an agent-oriented ethics aims to\nanalyse the morality of the individual actions of agents to w hom\nintentionality is attributed. In the example of the car, when\nanalyzing an accident we analyse the morality of the actions\nof the individual agents (the AI system and the human driver)\nindividually. We assume that the accident (the morally negat ive\nevent) could have come from the morally negative actions of\none of the agents, or of both of the agents separately. Thus, th e\nengineers designing the car’s AI system will try to ensure tha t\nthe actions of the AI system follow ethical criteria, but wit hout\ntaking into account the possible actions of the other agent, t he\nhuman driver. In the same way, driving teachers will train dr ivers\nto drive according to the ethical rules by which a human being\nmust be governed, but without considering how the AI agent is\ndesigned to act.\nHowever, in patient-oriented ethics (e.g., when looking at r oad\naccidents with a focus on pedestrians) we can do a “backwards”\nanalysis in which, when the system (the car with the human\ndriver and the AI system) performs a morally negative action\n(e.g., hitting a pedestrian), we assume that the individual a ctions\nwere not morally negative when considered by themselves. The\nconsequences of the morally negative action of the system\nwere the result of a wrong interaction of the two agents that\ncollaborate in the task of driving. For example, the human\ndriver could misinterpret the action of the intelligent syst em\nand act incorrectly because of this misinterpretation. There fore,\nthe responsibility does not lie with one of the individual agen ts\n(whom we can assume had good intentionality) but with the bad\ndesign of the interaction.\nBefore going any further, it is necessary to deﬁne three\nconcepts that tend to be confused because, although in English\nthe words for these concepts have diﬀerent meanings, the\nconcepts are all related. The confusion may be due to the fact\nthat in other languages such as Spanish or French there is\nonly one word used to refer to all three concepts together. The\nthree concepts are accountability, responsibility, and liab ility.\nThe (\nEncyclopaedia Britannica, 2021 ) deﬁnes accountability as\nthe “principle according to which a person or institution is\nresponsible for a set of duties and can be required to give\nan account of their fulﬁllment to an authority that is in a\nposition to issue rewards or punishment.” Thus, accountabili ty\nfundamentally means being able to give an explanation for one ’s\nbehavior. Therefore, when a person and an AI agent collaborat e\nin an activity, each of them is accountable if they are able to give\nan explanation of their individual actions, but we can also speak\nof the accountability of the joint system formed by the two ag ents,\nthe human being and the AI agent, to the extent that they are ab le\nto explain their joint behavior.\nA concept related to accountability is responsibility which,\naccording to the (\nEncyclopaedia Britannica, 2021 ), is “the\ntechnical term that was preferred to indicate the duty that per sons\nin public authority had to ‘respond’ in their conduct and actio ns\nas public oﬃcials.” A term related to responsibility in the leg al\nenvironment is liability, which the Encyclopaedia Britannic a\ndeﬁnes as the concept “preferred to indicate that by doing a\ncertain action (or entering into a certain contract) a person has\nput himself under an obligation and is therefore answerable for\nthe consequences following from that action (or from enterin g\ninto that contract).”\nThis clariﬁcation is important because in other languages\nthere is only one word for all three concepts. That word\nin Spanish, for example, is responsabilidad. This often causes\nspeakers of languages other than English to use the English\nword “responsibility” without clarifying whether they mean\naccountability, responsibility, or liability, creating con fusion in\nthe discussion of ethics and AI. What we are interested in her e\nFrontiers in Psychology | www.frontiersin.org 2 March 2022 | Volume 13 | Article 836650\nCañas AI and Ethics as Collaboration Between Agents\nis accountability and, to a lesser extent, responsibility, bu t we are\nnot interested in liability, which is a concept that is of inte rest\nonly in the legal sphere. If we are talking about an accident\ninvolving a car with a human driver and an AI system, we are\nnot interested in the legal consequences of that accident. W hat\nwe are interested in here is the extent to which the two agents , the\nhuman driver and the AI system installed in the car, can explai n\ntheir behavior and be held “jointly” responsible for it.\nOur interest in this article is therefore in the ethics of the\ncollaboration between an AI system and a human agent. This\nis the more common point of view of human factors and\nergonomics. For human factors specialists, the interest is i n the\ndesign of the interaction between an AI agent that interacts with a\nhuman being in a joint activity (such as driving). That inter action\nshould be designed while thinking that what is important is\nthe design of the interaction and not the design of each agent\nseparately when they interact in the joint activity (i.e., th e design\nin the case of the AI system and the training in the case of\nthe human driver). Of course, in this design of the interacti on\nwe cannot forget that the AI system has to be designed to take\ninto account the characteristics of the human being. Howeve r,\nthe system also has to be designed to take into account the\nevidence that the characteristics and functioning of the hu man\ncognitive system are not independent of their conditions of\ninteraction (\nCañas, 2021 ). We should also not forget that, from\nthe point of view of ergonomics and human factors, the interes t\nlies in explaining the accountability and, to a certain exten t,\nthe responsibility for the results of the interaction betwee n an\nAI system and a human being. The liability for the results of\nthat interaction is outside the interest of human factors an d\nergonomics specialists.\nIn the following sections of this article we will begin by\ndeﬁning interaction as a collaboration between agents. Nex t, we\nwill brieﬂy point out the ethical implications of this deﬁniti on\nof interaction. These ethical implications will require an an alysis\nof the cognitive and non-cognitive components of collaborat ion\nbetween agents, which we introduce in the following sections\nof the article. Finally, we will point out the implications of t his\nproposal for the design of AI systems.\nINTERACTION AS COLLABORATION\nTraditionally, it has been considered that when we are\nintroducing an automatic machine into an activity, what we a re\ndoing is assigning functions that were previously performed by\npeople to a machine that will now be responsible for them.\nIn other words, we are redistributing the functions that people\nand machines perform in the activity. For this reason, since t he\nbeginning of ergonomic studies on automation we have been\nusing the term “function distribution, ” so much so that many\nhuman factors specialists consider the terms automation and\nfunction distribution to be synonymous (\nParasuraman et al.,\n2000).\nThis traditional way of conceiving the introduction of a\nmachine as a way of assigning functions to the machine in\nthe activity has been useful in many sectors. However, this\nview is now considered incomplete. The main reason for this\nincompleteness is that it is basically a way of considering\nmachines only as tools in activities where the subjects are h uman\nbeings. It should be noted that the term “function sharing” w as\ncoined when machines were essentially taking over some of the\nmanual functions for tasks. However, with the assignment of\n“some intelligence” (cognitive function) to machines, the term\n“inter-agent collaboration” began to be considered to be mor e\nappropriate for referring to a situation in which humans and\nmachines are part of the subject of the activity and share both\nmanual and mental functions. In the activity there may still be\nmachine tools, but some machines with cognitive functions a nd\na certain level of intelligence can no longer be called tools but\nshould now be called “subjects collaborating with human sub jects\nin the performance of the task” (\nPhillips et al., 2011 ).\nThis reasoning leads us to consider that what we should\ndo now is to replace the term “distribution of functions”\nby the term “collaboration between intelligent machines an d\npeople” in order to take into account the fact that people and\nintelligent machines each take charge of certain functions and\nshare others. In this way we can understand the evidence that\nalready exists that shows that the consequences of an agent’s\nactions depend on the collaboration between agents.\nSkitka et al.\n(2000) pointed out that errors of omission might occur when\na person fails to respond to a problem that arises in a system\nbecause the automatic agent fails to detect the problem or fai ls\nto communicate it. There are also errors of commission when\na person follows the recommendations given by the automatic\nagent despite contradictory information being available fr om\nother sources. These authors suggested that errors of omiss ion\nmay be due to a decrease in the level of vigilance over what\nthe automated system is doing, and errors of commission migh t\nbe due to a combination of a failure to heed the advice of the\nautomated system and an erroneous belief in the superiority o f\nthe automated system’s judgement.\nTherefore, if we are to address the ethical issues that resul t\nfrom the introduction of intelligent systems to human activ ities,\nwe must analyse how the collaboration between intelligent ag ents\nshould be designed and what the ethics of such a collaboratio n\nare. The shift from the point of view of the ethics of two\ninteracting agents, where one is the subject of the activity and\none is the tool, to the point of view of the ethics of two agents\ncollaborating in the performance of the activity requires us to\nrethink how we address the ethical issues of the relationshi p\nbetween human beings and AI.\nETHICS OF COLLABORATION BETWEEN\nINTELLIGENT AGENTS\nTherefore, the question we should ask ourselves now is how\nthe collaboration of people and AI agents should be designed\nin order for the outcome of such collaboration to be ethicall y\ncorrect. To answer this question, the ﬁrst thing we must reco gnize\nis that, as with any collaboration between agents, the actio ns\nof one agent are not independent of the actions of the other\nFrontiers in Psychology | www.frontiersin.org 3 March 2022 | Volume 13 | Article 836650\nCañas AI and Ethics as Collaboration Between Agents\nagent. The second question is to identify the cognitive and n on-\ncognitive components of this collaboration. In any collabor ation\nbetween intelligent agents there are two types of component,\none that we might call the cognitive type and the other that we\nmight call the non-cognitive type. Although the behavior of a n\nagent, whether human or artiﬁcial, is the result of one uniqu e\nsystem that processes what is in the environment and responds\nto the demands imposed on it, it is clear that there are some\ncomponents of this behavior that could be explained by the way\nin which information from the environment is processed and\nothers that are related to the motivation or personality of th e\nagents. We call the ﬁrst components “cognitive” and deﬁne the m\nas those components of the system, whether human or artiﬁcial,\nthat perceive and process (analyse, elaborate on, memorize, e tc.)\nthe information. The latter are called “non-cognitive, ” an d are\nthose factors that inﬂuence the behavior of an agent but do no t\nprocess information from the environment. These factors sho uld\nbe taken into account because when one agent collaborates wi th\nanother in an activity, it is important that each perceives and\nanalyses the information and behavior of the other, but it is\nalso necessary that both agents are motivated to collaborat e, that\nthe collaboration does not provoke negative emotions for eit her\nof them, that there is mutual trust in the collaboration, and so\non. Therefore, taking into account this distinction betwee n the\ncognitive and the non-cognitive components of collaboratio n\nbetween agents, we will start with the cognitive components a nd\nthen address the non-cognitive components in the subsequent\nsection. However, in addressing them separately, we should n ot\nforget that this distinction is only useful as a scientiﬁc me thod\nfor analyzing the agents’ behavior. The distinction betwee n\ncognitive and non-cognitive components does not have a reali ty\nindependent of the work of the scientist analyzing the agents’\nbehavior. It is evident, for example, that emotions have to\nbe explained by considering how the agent’s intelligent syst em\nperceives the world.\nCOGNITIVE COMPONENTS OF\nCOLLABORATION BETWEEN HUMAN AND\nAI AGENTS\nIt is evident that the interdependence between the agents’ ac tions\nimplies that one agent must understand the actions of the othe r.\nIf an agent does not understand the actions of another agent w ith\nwhom he or she is collaborating, he or she may act in a wrong\nway, causing the outcome of the joint actions to be ethically\nwrong. The two main factors that aﬀect our understanding of th e\nthoughts and actions of the actors with whom we work are: (1)\neach actor must pay attention to what the other is doing; and (2 )\neach agent must correctly interpret what the other agent is do ing.\nWe might start by analyzing the communication in the\ndirection of the machine to the person. At all intermediate le vels\nof automation, the person has to know what the machine is\ndoing. To do this, the person must “monitor” the actions of\nthe machine. However, as\nStanton (2019) has recently reminded\nus, people have a great deal of trouble monitoring, and this\nmay be one of the big problems with automation today. In\ninterviews recently conducted by\nKyriakidis et al. (2019) with\n12 experts in ergonomics and human factors, the interviewees\nalmost unanimously stated that the main problem we have in\nmaking automation live up to the expectations it is creating\nis that we humans are not very good at monitoring processes.\nFor this reason, it is worth spending some time considering the\nexplanations of why monitoring is so diﬃcult for humans. Since\nmonitoring is so essential in the collaboration between peopl e\nand machines in the design of automation, we must ﬁnd solutio ns\nto ensure that a failure to monitor a smart machine does not le ad\nto a joint action that is ethically wrong.\nThis poor ability of humans to monitor what a machine\nis doing has led ergonomics and human factors specialists to\ninvestigate the design and use of alarms that warn of adverse\nevents such as incorrect machine actions. Thinking about the\ncollaboration between a person and an AI system, we can ask\nourselves what we expect the person to do when hearing the\nalarm generated by an AI system. As one example, we could\nexpect the alarm to alert the person that something is wrong and\nthat they are required to take control of the activity. For ex ample,\nin an automatic car when an alarm is heard indicating that the\ncar is drifting oﬀ the road, we would expect the alarm to be\naccompanied by the deactivation of the automatic driving and by\nthe driver taking control of the car to correct the course and avoid\nthe hazard. Conversely, we could have an automatic car in whic h\nthe alarm serves to warn the driver that something is wrong bu t\nthe car itself attempts to correct the error, with the driver o nly\nbeing required to monitor what the car is doing. This monitor ing\ncould be done by pressing one button, such as a green button,\nif the driver sees that the car is indeed doing the right thing to\ncorrect the error, or another button, such as a red one, if the driver\nsees that what the car is doing is not the right thing and so the car\nshould try to do something else. Thus, in the ﬁrst case the ala rm\nmeans that the driver has to start driving, and in the second c ase\nthe driver has to monitor what the car is still doing.\nMishler and\nChen (2018) called the ﬁrst type of alarm a “direct response” and\nthe second an “indirect response, ” and carried out experiment al\nresearch to compare how long it takes a driver to react to each\nand how well he or she does so. The results showed that the dire ct\nresponse was faster and more correct than the indirect respons e.\nIt seems that the indirect response of letting the car remain i n\nautomatic mode but requiring the driver to tell the car wheth er it\nis doing right or wrong requires more mental resources that t ake\nlonger and are more prone to error than the direct response of\nletting the driver start driving immediately by deactivati ng the\nautomatic system. Thus, in alarms generated by an intellige nt\nsystem we have a good example of how the actions of the human\nagent depend on the design of the non-human intelligent agent .\nUnderstanding what the machine is doing is also very\nimportant for good collaboration. An example can be found in\nrecent research conducted by\nChiou et al. (2021) . These authors\nstudied how people interact with robots designed for search and\nrescue operations. Such robots are currently gaining importa nce\nbecause they have great advantages, including the fact that their\nuse prevents people from having to take risks in search and rescue\noperations. The people using them can stay in safe places while\nthe robots reach the risk areas.\nFrontiers in Psychology | www.frontiersin.org 4 March 2022 | Volume 13 | Article 836650\nCañas AI and Ethics as Collaboration Between Agents\nThe use of these robots can be considered as an example\nof an activity that is carried out in collaboration between a\nperson (the robot operator) and an intelligent device. In this\nactivity it is essential that there is good communication be tween\nthe robot and the operator; if that communication fails, the\nintentions of the operator or the robot may be good, but the\nend result may be wrong. The communication between the robot\nand the person operating the robot will have a positive eﬀect if\nthey share situational awareness (\nEndsley and Jones, 2001 ). By\nsituational awareness we mean “the perception of the elements\nin the environment within a volume of time and space, the\nunderstanding of their signiﬁcance and the projection of the ir\nstate into the near future” (\nEndsley, 1995, p. 36). We can see that\nwhen several agents are collaborating in an activity, each m ust\nhave the situational awareness that is necessary to perform t he\nactivities for which he or she is responsible. However, all th e\ncollaborating agents need to have shared situational aware ness\namong themselves in order to understand the situation as a wh ole\nand the actions that the other agents are taking. This is why w e\nspeak of individual situational awareness and shared situat ional\nawareness. Without such shared situational awareness, it i s\npossible for one agent, with their own individual, and therefo re\npartial, situational awareness, to carry out an action that i s in\ncontradiction with the actions of another agent.\nIn order to create this shared situational awareness, there\nneeds to be good communication between the actors. Included\nin this good communication are the explanations that one agen t\ngives to the others about what she/he or it is doing and why\nshe/he or it is doing it. According to\nChiou et al. (2021) , there are\nfour types of communication between a robot and an operator\nwhen we are considering the type of explanation that the robot\ngives for its actions. First, we have the situation where the robot\nis designed always to give explanations, without the operator\nneeding to ask for them. Secondly, we could have a robot that\nonly gives explanations when the operator asks for them. In thi s\nsituation, the operator is left to decide for himself/herself when\nto ask for explanations, or, in the third type of communication ,\nhe/she could be trained to ask for explanations in a convenien t\nway. In the fourth type of communication, which serves as a poin t\nof comparison, the robot never gives explanations.\nThe researchers found that when the robot gives many\nexplanations that are not necessary, the performance of the\nrobot–person team is no better. Unnecessary explanations caus e\na higher mental load on the operator, which may cause her/him\nto perform her/his actions badly. Therefore, joint performan ce is\naﬀected by poor robot design. Even if the robot has been designe d\nto act well and explain everything to the operator, such a desig n\nresults in worse collaboration and thus in a joint performanc e\nthat may lead to errors. These results clearly indicate that t he\nmorality of the individual actions of the agents taken separa tely\ndoes not imply an improvement in the morality of the actions of\nthe robot–human operator system.\nTherefore, for the collaboration between the person and the\nautomatic system to occur in an optimal way, it is necessary\nthat the person observes what the automatic system does and\nunderstands it. However, in the same way as with collaborati on\nbetween human agents, good collaboration also requires tha t\nthe automatic system observes the person’s mental state and\nbehavior. The fundamental reason why we need the automatic\nsystem to observe the person’s state of mind and behavior is\nthat it must be able to adapt to them. Since the mid-1950s,\nwhen people began to talk about the distribution of functions,\nit became clear that this distribution could not be ﬁxed, but\ninstead was dynamic. For this reason, in more recent years, t he\nterm “adaptive automatic systems” has come to be used. An\nadaptive system is deﬁned as a system that can change the type\nof automation depending on the situation or state of mind and\nbehavior of the person.\nThe ﬁrst thing a machine must have is a way of recording the\nperson’s actions and interpreting their behavior and, if nece ssary,\ntheir thoughts and mental state. These sensors can be of vario us\ntypes and serve various purposes. First, there are motion sensors .\nMotion sensors should serve not only to detect the movements\nof the person interacting with the machine, but also to detect\nthe movements of the people in the environment in which that\nperson and that machine are acting. This would be the case with\nan automatic car with a driver in charge of certain aspects of\ndriving, when it is driving on a street where there are pedestr ians.\nVision is the fundamental way in which a person detects the\nmovement of objects and people in her/his environment. For thi s\nreason, automated machines are being equipped with image and\nvideo recording equipment that is analyzed by software desig ned\nto detect movement. Great advances are being made in this ﬁel d\nusing artiﬁcial intelligence methods known as “deep learni ng”\nmethods (\nZhang et al., 2020 ).\nA person’s emotional state is very important in interpreting\nhis or her behavior. Emotions are factors that aﬀect a person’s\ndecision making and actions. Let us think again about the vehi cle\nsituation. All drivers know that our emotional state inﬂuen ces\nthe decisions we make about overtaking, increasing speed, an d\nso on. For this reason, the machine must be able to recognize\nthe emotional state of the person. One option for detecting the\nemotional state of a person is the use of facial expression analy sis\nsoftware.\nMunoz-de-Escalona and Cañas (2017) have shown\nthat emotions detected from the analysis of facial expressio ns\ncan be used to predict how well a task is performed. Another\noption is pupil diameter. It has been known for many years\nthat pupil diameter is sensitive to the level of arousal caused\nby the person’s emotional state. For this reason, research in to\nhow people interact with machines is using systems that measure\nvariations in pupil diameter to analyse a person’s emotional\nprocessing. The experience of an emotion requires a thought\nprocess. For this reason, we consider emotion to be a cognitive\nfactor of collaboration (\nLazarus, 1982).\nFinally, a topic that is being investigated in the ﬁeld of\nintelligent systems design is what has been called “theory o f\nmind” (Carruthers and Smith, 1996; Frith and Frith, 1999 ). There\nare two ways of understanding what is meant by a theory of mind .\nOne way is to assume that we all have an “unscientiﬁc” idea, ba sed\non our experience and our general knowledge of the world, of\nhow the human mind works, in order to predict how the people\nwith whom we interact will behave. Another way to understand\nwhat a theory of mind means is to assume that when we interact\nwith other people we have the ability to simulate the minds\nFrontiers in Psychology | www.frontiersin.org 5 March 2022 | Volume 13 | Article 836650\nCañas AI and Ethics as Collaboration Between Agents\nof those people and thus predict their behavior. This mental\nsimulation of the minds of others is done with the knowledge\nthat we believe we have of the workings of our own minds. It is\nclear that a theory of mind includes beliefs about ethically correct\nbehavior. The question we must ask in the context of ethics is\nwhether it is possible to design AI systems that have a theory o f\nmind that includes the structure of the human mind and how it\nfunctions, and whether people can attribute a mind to AI system s\n(\nWinﬁeld, 2018).\nNON-COGNITIVE COMPONENTS OF\nCOLLABORATION BETWEEN HUMAN AND\nAI AGENTS\nThere are many non-cognitive components of agent\ncollaboration, including social cues, security, responsib ility,\nautonomy, and trust (\nEtemad-Sajadi et al., 2022 ). We can take\nas an example the last of these, trust. A fundamental aspect\nthat is currently receiving a great deal of attention in the a rea\nof automation is that of human trust in the automated system.\nIn the words of\nParasuraman and Riley (1997) , trust plays a\nfundamental role in the disposition of the human being in the\nautomatic systems with which he collaborates in an activity ,\nespecially in situations of uncertainty. Researchers such as\nLee\nand See (2004) have shown that when the reliability of the\nautomatic system falls below 90% (i.e., it fails more than 10 %\nof the time), humans stop trusting the automatic system and\nstop collaborating with it, which aﬀects the eﬀectiveness of t he\nactivity they are carrying out.\nIt is obvious that this eﬀect of human trust in the proper\nfunctioning of the automatic system cannot be explained if we\nthink of the automation of an activity as a simple distributio n of\nfunctions between the human being and the automatic machine .\nTrust is a factor of social relations. When we talk about trus t we\nare talking about collaboration between social agents. We c ould\nsay that if we are going to let a Tesla car drive for us, it is bec ause\nwe trust that it will not crash into us or run over a pedestrian. It\nis not enough to say that we have assigned one or more function s\nto the car: we need to think that, by letting it drive for us, we are\nalso trusting it to do this well, just as if we were letting som eone\nelse drive for us. That person would need to have our trust that\nhe or she would drive well, otherwise we would not let him or he r\ndrive for us.\nThere are diﬀerent deﬁnitions of the term trust that are\nrelevant to our analysis of automation in human activity. Ea ch\ndeﬁnition emphasizes diﬀerent aspects and relates the concept\nof trust to diﬀerent types of automated systems and contexts of\ninteraction with them. For example, there is a widely accepted\ndeﬁnition proposed by\nMayer et al. (1995) that states that trust\nis the willingness of one party to be vulnerable to the actions of\nanother party, based on the expectation that the other party will\nperform a particular action, regardless of the ability to cont rol or\nmonitor that other party.\nIt is clear that there are several important aspects to this\ndeﬁnition. The ﬁrst aspect is the “willingness” of one party to be\nvulnerable to the actions of the other. Simply put, it can be sai d\nthat a person voluntarily admits that the actions of another m ay\nharm and not always beneﬁt him or her. Therefore, this means\nthat the person who trusts runs a risk of being exposed to negativ e\nsituations. The second aspect is that the trusting person admi ts\nthat he/she may not have control over the actions of the person\nhe/she trusts.\nSheridan (2002), one of the most important researchers on the\nsubject of automation, points out that we can consider trust bo th\nas an eﬀect and as a cause. As an eﬀect, it refers to how we perceive\nthe reliability of the system. If the system does not fail, if it lets us\nknow what it is doing, if it does what it does with procedures th at\nare familiar to us, if we can predict what it will do in the futur e,\nand so on, we will say that the system gives us conﬁdence. As\na cause, trust in a system will cause us to behave toward it in a\ncertain way. If we do not trust the system, we will try to avoid\nit. This is what often happens when we disconnect an alarm that\noften fails. Because we do not trust it, we switch it oﬀ.\nThis research has found that the reliability of alarms in ale rting\nus to real problems is very important when we talk about trust. I t\nhas been shown that if the reliability of an alarm falls below 90%,\nthat is, if it fails more than 10% of the time, people tend to stop\npaying attention to it. They may even switch it oﬀ. If the person\nnotices that every time the alarm goes oﬀ there really is a prob lem\nwith the automation, he or she will tend to pay attention to the\nalarm every time. On the other hand, when the alarm is trigger ed\nby mistake and not because there is something going on to whic h\nthe person has to pay attention, we have the problem for which we\nuse a phrase taken from the children’s story, “the cry wolf eﬀec t”\n(\nWickens et al., 2009 ).\nThe problem arises after a person experiences the failure of\nthe alarm system several times. The failure of the alarm syst em\nis what is called, in the terminology of signal detection the ory,\na false alarm. The experience of interacting with a machine\nthat has emitted several false alarms leads the person to fail t o\nrespond to the alarm when it is correctly triggered. Therefor e, the\nphenomenon that we call the “cry wolf” eﬀect occurs when the\nperson does not respond to a true alarm or responds late because\nof their experience of many false alarms.\nIMPROVING COLLABORATION SO THAT IT\nIS ETHICALLY CORRECT (ACTS\nETHICALLY)\nThe question now arises as to how we improve collaboration so\nthat the results of the activity are ethically correct (the r esults are\nethically positive, and ethically negative results are avoi ded). To\nanswer that question, we have two options. One is for the agent s\nto receive some kind of training to learn how the other agent\nbehaves. In this option, the engineers designing the AI do not\nneed it to behave like a person. It is only necessary for the pers on\nto learn how the AI behaves, or the other way round. Another\noption for engineers is to design the AI to behave as a person\nwould behave. This second option is the one reported by, for\nexample,\nKadar et al. (2017) . After analyzing some accidents in\nrecent years in which poor interaction between human agents\nand intelligent automated agents can be identiﬁed (the Alvi a\nFrontiers in Psychology | www.frontiersin.org 6 March 2022 | Volume 13 | Article 836650\nCañas AI and Ethics as Collaboration Between Agents\ntrain crash on the route from Madrid to Santiago de Compostela ,\nthe Air France Flight 447 crash and the Asiana Airlines Flight\n214 crash), the authors concluded that these accidents occu rred\nlargely because the artiﬁcial intelligence control strate gies for\nthe automated vehicle control were not the same as or similar\nto those used by human controllers and/or were not used in\na similar way. Humans use control strategies based on how\nwe perceive the environment. From the pioneering research\nof\nGibson (1950, 1979) , we know that people perceive the\nenvironment on the basis of the invariants in it. For example,\ndrivers control their speed through braking, based on the rat e of\noptic expansion (\nLee, 1976 ). However, in AI, kinetic parameters\nof movements, instead of perceptual invariants, are used to de sign\ncontrol strategies. These diﬀerent control strategies can l ead to\na lack of understanding between human agents and AI systems,\nand this lack of understanding can be the cause of accidents.\nDISCUSSION\nIf the ethics of the actions of collaborative activities inv olving\nhumans and AI systems are to be explained by the characteristi cs\nof that collaboration, it is clear that the methodology for\nanalyzing the ethical issues of those activities must have a basis\nin the analysis of the characteristics of that collaboratio n. Even\nif the actions of each actor are ethically correct in themsel ves,\ntaken together in the context of the collaboration those act ions\nmay lead to ethically incorrect consequences. Therefore, t he aim\nshould be to analyse the ethics of collaboration and not the\nethics of the individual actions of each agent, whether huma n or\nartiﬁcial, separately.\nWe can therefore propose a number of principles to underpin\nthis methodology. First, we must analyse whether the design\nof the collaboration ensures that there is good monitoring b y\neach agent of the other’s actions. If this monitoring is not w ell-\ndesigned, it may happen that the actions of one agent are ignore d\nby the other agent. In that case, even if the actions of each ag ent\nare ethically correct in themselves, taken together they mi ght lead\nto ethically incorrect consequences.\nSecondly, We should clarify that our proposal to consider the\ndesign of AI systems and the ethical issues in such design mus t be\ndistinguished from other proposals that are currently receiv ing\nmuch attention. One of these proposals has been referred to\nby cognitive scientists as “extended cognition.” This idea was\npopularized by (\nClark et al., 1998 ) and means that automation\nis explained as an extension of the human mind that provides the\nhuman mind with cognitive capabilities that it does not posses s\nwithout automatic machines. In its most extreme version, th is\nidea would mean treating automatic machines as an extension\nof the human brain. It would be something like considering th at\nautomatic machines are prostheses that are installed in our b rains\nto allow us to perform cognitive activities that we could not d o\nwith them. In its less external version, the idea means consi dering\nthat automatic machines are cognitive artifacts (\nHutchins, 1999)\nor cognitive extenders ( Hernández-Orallo and Vold, 2019 ) that\nattach themselves to us to perform cognitive activities in su ch\na way that if they disappear we are not able to perform such\nactivities. However, in no case do they become an extension o f\nour nervous system because they are independent entities. It is\nthis independence that makes us consider them as collaboratin g\nagents. Automatic machines cannot be extensions of our brai n,\nespecially since they are increasingly capable of performing m ore\ncomplex tasks without the intervention of our brains.\nThis idea of extended cognition has to be considered in\nthe sense that Hutchins (2000) proposed some years ago when\ntalking about how cognitive activities are performed in ever yday\nlife. He coined the term “distributed cognition” to refer to t he\nobvious fact that when we perform a cognitive activity such as ,\nfor example, memorizing and recalling a fact, we create artif acts\nthat help us to perform it. Consider, for example, how we now\nremember almost no phone numbers because they are stored\nin our mobile phone’s address book. It is clear that the mobile\nphone book can be considered as an agent that collaborates\nwith us to memorize and remember phone numbers, but it is\nnot part of our brain. We could give our diary the ability to\nremember that we have to call a person on a certain day at a\ncertain time, which would give it a certain autonomy from us\nand allow it to organize our actions, but that does not mean tha t\nthis diary is part of our brain—it means that we have given a\ncognitive function to another agent with whom we collaborat e.\nThis way of understanding cognitive activities, as proposed b y\nHutchins, simply means that the activity in which people and\nmachines collaborate is an activity in which the collaborat ion\nmeans that cognitive activity is distributed between human a nd\nnon-human agents. In no way does it mean that machines\nbecome an extension of our brains.\nOur proposal is also not what has been called “the human-\nmachine symbiosis approach” to human–machine interaction. I n\nthis approach, collaboration between humans and machines is a\nclosed relationship that is mutually beneﬁcial (\nLicklider, 1960 ).\nThis symbiosis can take many forms, depending on the type of\nmachine that is collaborating with humans. The collaboratio n\ncan be dependent if we consider that machines are mere\npre-programmed mechanisms that do not possess intelligence.\nHowever, in our proposal the collaboration is between agents\nthat can be considered intelligent. Therefore, from the eth ical\npoint of view, our proposal goes beyond the symbiosis approach\nas originally proposed. There may be symbiosis between two\nagents that are not at the same level of intelligence, but wha t we\nare now proposing is a symbiosis in which the collaboration is\nbetween two intelligent agents, and that proposal has importan t\nethical consequences.\nIn conclusion, we should apply the concepts of accountability\nand responsibility in the context of the collaboration betwee n\nhumans and AI systems, and design AI accordingly. What this\nmeans is that the actions of an agent have to be understood as\ndependent on the collaboration with other agents. In this way , the\nmorality of the system’s actions will depend on the morality of an\nindividual agent’s actions, but also on the collaboration be tween\nthe agents. An agent might misinterpret the actions of the othe r\nagent and act accordingly, causing the outcome of the action s of\nthe AI–human system to be morally reprehensible. For example,\nwhen an intelligent car and its human driver hit a pedestrian w e\ncould talk about a morally reprehensible event, but this could be\nFrontiers in Psychology | www.frontiersin.org 7 March 2022 | Volume 13 | Article 836650\nCañas AI and Ethics as Collaboration Between Agents\nthe result of: (1) the driver or the AI system performing a mora lly\nreprehensible action (deciding to ignore a traﬃc light), or ( 2) the\ndriver misinterpreting the AI system, thinking that it is goi ng to\nstop the car and doing nothing. There is extensive research a bout\nhuman collaboration that could be applied to this objective.\nThe idea presented in this article can contribute to achievin g\nthe objectives of the ongoing social debate on AI and ethics. For\nexample, understanding the relationship between ethics and AI\nfrom the point of view of collaboration between intelligent a gents\nwill contribute to the development and application of a strong\nsafety and security practice as deﬁned by\nGoogle (2022). This idea\ncan also contribute to the discussion on the ethical principle s to\nbe followed in the development of AI according to the ethical\nguidelines for reliable AI deﬁned by the\nEuropean Commission\n(2022). For example, one of these principles is that human beings\nshould be free to make vital decisions for themselves. Howeve r,\nfor this principle to be fulﬁlled, it is necessary that freedom o f\naction be understood in the context of collaboration betwee n\nintelligent agents.\nAUTHOR CONTRIBUTIONS\nThe author conﬁrms being the sole contributor of this work an d\nhas approved it for publication.\nREFERENCES\nCañas, J. J. (2021). “The human mind and engineering models, ” in International\nConference on Human–Computer Interaction (Cham: Springer), 197–208.\ndoi: 10.1007/978-3-030-77431-8_12\nCarruthers, P., and Smith, P. (1996). Theories of Theories of Mind. Cambridge:\nCambridge University Press. doi: 10.1017/CBO9780511597985\nChiou, E. K., Demir, M., Buchanan, V., Corral, C. C., Endsley, M. R. , Lematta, G.\nJ., et al. (2021). Towards human–robot teaming: tradeoﬀs of explanat ion-based\ncommunication strategies in a virtual search and rescue task. Int. J. Soc. Robot.\ndoi: 10.1007/s12369-021-00834-1. [Epub ahead of print].\nClark,., A., and Chalmers, D. (1998). The extended mind. Analysis 58, 7–19.\ndoi: 10.1093/analys/58.1.7\nEncyclopaedia Britannica (2021). Available online at: https://ww w.britannica.com/\ntopic/accountability (accessed November, 2021).\nEndsley, M. (1995). Toward a theory of situation awareness in dyn amic systems.\nHuman Factor37, 32–64. doi: 10.1518/001872095779049543\nEndsley, M. R., and Jones, W. M. (2001). “A model of inter and int ra team\nsituation awareness: Implications for design, training and measu rement, ” in\nNew Trends in Cooperative Activities: Understanding System Dynamics in\nComplex Environments, eds M. McNeese, E. Salas, and M. Endsley (Santa\nMonica, CA: Human Factors and Ergonomics Society), 1–24.\nEtemad-Sajadi, R., Soussan, A., and Schöpfer, T. (2022). How et hical issues raised\nby human–robot interaction can impact the intention to use the robo t? Int. J.\nSoc. Robot.doi: 10.1007/s12369-021-00857-8. [Epub ahead of print].\nEuropean Commission (2022). Available online at: https://digital-s trategy.ec.\neuropa.eu/en/library/ethics-guidelines-trustworthy-ai (acces sed April, 2019).\nFloridi, L. (2016). Faultless responsibility: on the nature and alloca tion\nof moral responsibility for distributed moral actions. Philos.\nTrans. R. Soc. London A 374, 20160112. doi: 10.1098/rsta.201\n6.0112\nFrench, P. A., and Wettstein, H. K. (2006). Shared Intentions and Collective\nResponsibility. Boston, MA: Blackwell Publishing.\nFrith, C. D., and Frith, U. (1999). Interacting minds – a biological basis. Science\n286, 1692–1695. doi: 10.1126/science.286.5445.1692\nGibson, J. (1950). Perception of the Visual World. Boston, MA: Houghton Miﬄin.\nGibson, J. (1979). The Ecological Approach to Visual Perception. Boston, MA:\nHoughton Miﬄin.\nGoogle (2022). Available online at: https://ai.google/principles/ ( accessed February,\n2022).\nHernández-Orallo, J., and Vold, K. (2019). “AI extenders: the ethi cal and societal\nimplications of humans cognitively extended by AI, ” in Proceedings of the\n2019 AAAI/ACM Conference on AI, Ethics, and Society(Honolulu), 507–513.\ndoi: 10.1145/3306618.3314238\nHutchins, E. (1999). “Cognitive artifacts, ” inThe MIT Encyclopedia of the Cognitive\nSciences, eds R. A. Wilson and F. Keil (MIT Press), 126–127.\nHutchins, E. (2000). “Distributed cognition, ” in International Encyclopedia of the\nSocial and Behavioral Sciences, edsN. J. Smelser and P. B. Baltes (Elsevier\nScience), 138.\nKadar, E. E., Köszeghy, A., and Virk, G. S. (2017). “Safety and ethical concerns in\nmixed human–robot control of vehicles, ” in, A World With Robots,eds M. I.\nA. Ferreira, J. S. Sequeira, M. O. Tokhi, E. E. Kadar, and G. S. Virk (Springer),\n135–144. doi: 10.1007/978-3-319-46667-5_10\nKyriakidis, M., de Winter, J. C., Stanton, N., Bellet, T., van Are m, B., Brookhuis,\nK., et al. (2019). A human factors perspective on automated driving . Theor. Iss.\nErgon. Sci.20, 223–249. doi: 10.1080/1463922X.2017.1293187\nLazarus, R. S. (1982). Thoughts on the relations between emotion and cognition.\nAm. Psychol.37, 1019–1024. doi: 10.1037/0003-066X.37.9.1019\nLee, D. N. (1976). A theory of visual control of braking based on i nformation about\ntime-to-collision. Perception 5, 437–459. doi: 10.1068/p050437\nLee, J. D., and See, K. A. (2004). Trust in automation: designing for appropriate\nreliance. Human Factors46, 50–80. doi: 10.1518/hfes.46.1.50.30392\nLicklider, J. C. (1960). Man–computer symbiosis. IRE Trans. Human Factors Electr.\n1, 4–11. doi: 10.1109/THFE2.1960.4503259\nMayer, R. C., Davis, J. H., and Schoorman, F. D. (1995). An integra tive model of\norganizational trust. Acad. Manag. Rev.20, 709–734. doi: 10.2307/258792\nMishler, S., and Chen, J. (2018). “Eﬀect of response method on driv er responses\nto auditory warnings in simulated semi-autonomous driving, ” in Proceedings\nof the Human Factors and Ergonomics Society Annual Meeting, V ol. 62\n(Los Angeles, CA), 1934–1938. doi: 10.1177/15419312186214 39\nMunoz-de-Escalona, E., and Cañas, J. J. (2017). “Online measuri ng of available\nresources, ” inH-Workload 2017: The First International Symposium on Human\nMental Work(Dublin: Springer).\nParasuraman, R., and Riley, V. (1997). Humans and automation: use, mi suse,\ndisuse, abuse. Human Factors 39, 230–253. doi: 10.1518/00187209777854\n3886\nParasuraman, R., Sheridan, T. B., and Wickens, C. D. (2000). A mode l for types and\nlevels of human interaction with automation. IEEE Trans. Syst. Man Cybernet.\nA 30, 286–297. doi: 10.1109/3468.844354\nPhillips, E., Ososky, S., Grove, J., and Jentsch, F. (2011). “From tools to teammates:\ntoward the development of appropriate mental models for intelligent robots, ”\nin Proceedings of the Human Factors and Ergonomics Society An nual\nMeeting, Vol. 55 (Los Angeles, CA), 1491–1495. doi: 10.1177/10711813115\n51310\nSheridan, T. B. (2002). Humans and Automation: System Design and Research\nIssues. Santa Monica, CA: Wiley/Human Factors and Ergonomics Society.\nSkitka, L. J., Mosier, K., and Burdick, M. D. (2000). Accounta bility and automation\nbias. Int. J. Human Comp. Stud. 52, 701–717. doi: 10.1006/ijhc.1999.\n0349\nStahl, B. C., Obach, M., Yaghmaei, E., Ikonen, V., Chatﬁeld, K., and Brem,\nA. (2017). The responsible research and innovation (RRI) maturity model:\nlinking theory and practice. Sustainability 9, 1036–1019. doi: 10.3390/su906\n1036\nStanton, N. A. (2019). Thematic issue: driving automation and a utonomy.\nTheor. Iss. Ergon. Sci. 20, 1–7. doi: 10.1080/1463922X.2018.15\n41112\nWickens, C. D., Rice, S., Keller, D., Hutchins, S., Hughes, J., an d Clayton, K.\n(2009). False alerts in air traﬃc control conﬂict alerting system: Is there\nFrontiers in Psychology | www.frontiersin.org 8 March 2022 | Volume 13 | Article 836650\nCañas AI and Ethics as Collaboration Between Agents\na “cry wolf” eﬀect? Human Factors 51, 446–462. doi: 10.1177/0018720809\n344720\nWinﬁeld, A. F. T. (2018). Experiments in artiﬁcial theory of mind: fro m safety to\nstory-telling. Front. Robot. AI5, 75. doi: 10.3389/frobt.2018.00075\nZhang, Y., Zhang, M., Cui, Y., and Zhang, D. (2020). Detectio n and tracking\nof human track and ﬁeld motion targets based on deep learning.\nMultimedia Tools Appl. 79, 9543–9563. doi: 10.1007/s11042-019-0\n8035-9\nConﬂict of Interest: The author declares that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be c onstrued as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their aﬃliated organizat ions, or those of\nthe publisher, the editors and the reviewers. Any product that may b e evaluated in\nthis article, or claim that may be made by its manufacturer, is not gua ranteed or\nendorsed by the publisher.\nCopyright © 2022 Cañas. This is an open-access article distributed under the terms\nof the Creative Commons Attribution License (CC BY). The use, distribution or\nreproduction in other forums is permitted, provided the original author(s) and the\ncopyright owner(s) are credited and that the original publication in this journal\nis cited, in accordance with accepted academic practice. Nouse, distribution or\nreproduction is permitted which does not comply with these terms.\nFrontiers in Psychology | www.frontiersin.org 9 March 2022 | Volume 13 | Article 836650",
  "topic": "Process (computing)",
  "concepts": [
    {
      "name": "Process (computing)",
      "score": 0.6469223499298096
    },
    {
      "name": "Cognition",
      "score": 0.6046337485313416
    },
    {
      "name": "Psychology",
      "score": 0.5800065994262695
    },
    {
      "name": "Knowledge management",
      "score": 0.3985237181186676
    },
    {
      "name": "Social psychology",
      "score": 0.344401478767395
    },
    {
      "name": "Process management",
      "score": 0.3237149119377136
    },
    {
      "name": "Computer science",
      "score": 0.271506667137146
    },
    {
      "name": "Business",
      "score": 0.2634217143058777
    },
    {
      "name": "Neuroscience",
      "score": 0.10220611095428467
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173304897",
      "name": "Universidad de Granada",
      "country": "ES"
    }
  ],
  "cited_by": 34
}