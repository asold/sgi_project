{
  "title": "Enhancing Machine-Generated Text Detection: Adversarial Fine-Tuning of Pre-Trained Language Models",
  "url": "https://openalex.org/W4396667547",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2108704199",
      "name": "Dong Hee Lee",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2654054947",
      "name": "Beakcheol Jang",
      "affiliations": [
        "Yonsei University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6854866820",
    "https://openalex.org/W6852800892",
    "https://openalex.org/W3131189267",
    "https://openalex.org/W4383751019",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4385572879",
    "https://openalex.org/W4200594764",
    "https://openalex.org/W3013843954",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6767101625",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6848670183",
    "https://openalex.org/W6853626865",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6776403474",
    "https://openalex.org/W3114326827",
    "https://openalex.org/W4389518647",
    "https://openalex.org/W6764055196",
    "https://openalex.org/W3101891351",
    "https://openalex.org/W3046357466",
    "https://openalex.org/W6763240421",
    "https://openalex.org/W6857292341",
    "https://openalex.org/W6640425456",
    "https://openalex.org/W2243397390",
    "https://openalex.org/W2765424254",
    "https://openalex.org/W6725794477",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W2963767194",
    "https://openalex.org/W6730161283",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6771917389",
    "https://openalex.org/W3187071356",
    "https://openalex.org/W6776215604",
    "https://openalex.org/W6779411778",
    "https://openalex.org/W6785006811",
    "https://openalex.org/W6804006442",
    "https://openalex.org/W4288781262",
    "https://openalex.org/W6849274154",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W6676984168",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W4383987308",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3212179433",
    "https://openalex.org/W4318351452",
    "https://openalex.org/W4387559712",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3171847983",
    "https://openalex.org/W4300996741",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W3017003177",
    "https://openalex.org/W3103557498",
    "https://openalex.org/W4288334893",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035204084"
  ],
  "abstract": "Advances in large language models (LLMs) have revolutionized the natural language processing field. However, the text generated by LLMs can result in various issues, such as fake news, misinformation, and social media spam. In addition, detecting machine-generated text is becoming increasingly difficult because it produces text that resembles human writing. We propose a new method for effectively detecting machine-generated text by applying adversarial training (AT) to pre-trained language models (PLMs), such as Bidirectional Encoder Representations from Transformers (BERT). We generated adversarial examples that appeared to have been modified by humans and applied them to the PLMs to improve the model&#x2019;s detection capabilities. The proposed method was validated on various datasets and experiments. It showed improved performance compared to traditional fine-tuning methods, with an average reduction in the probability of misclassification of machine-generated text by about 10&#x0025;. We demonstrated the robustness of the model when generated with input tokens of different lengths and under different training data ratios. We suggested future research directions for applying AT to different languages and language model types. This study opens new possibilities for applying AT to the problem of machine-generated text detection and classification and contributes to building more effective detection models.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8284091353416443
    },
    {
      "name": "Adversarial system",
      "score": 0.7204539179801941
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6931073069572449
    },
    {
      "name": "Language model",
      "score": 0.631654679775238
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5999276638031006
    },
    {
      "name": "Machine learning",
      "score": 0.5665563344955444
    },
    {
      "name": "Transformer",
      "score": 0.5070676803588867
    },
    {
      "name": "Natural language processing",
      "score": 0.4970283806324005
    },
    {
      "name": "Encoder",
      "score": 0.4929239749908447
    },
    {
      "name": "Adversarial machine learning",
      "score": 0.4161379933357239
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I193775966",
      "name": "Yonsei University",
      "country": "KR"
    }
  ]
}