{
  "title": "CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained Vision-Language Model",
  "url": "https://openalex.org/W4378508557",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100694827",
      "name": "Shuai Zhao",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5100354377",
      "name": "Xiaohan Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5043617790",
      "name": "Linchao Zhu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5090036167",
      "name": "Ruijie Quan",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5005421447",
      "name": "Yi Yang",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3134095442",
    "https://openalex.org/W3003711889",
    "https://openalex.org/W3204479434",
    "https://openalex.org/W4378501777",
    "https://openalex.org/W3101411491",
    "https://openalex.org/W1981283549",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W4287203292",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W3113987534",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W4308021352",
    "https://openalex.org/W4287281076",
    "https://openalex.org/W2962790387",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4386185423",
    "https://openalex.org/W1998042868",
    "https://openalex.org/W2294053032",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2343052201",
    "https://openalex.org/W4295830359",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W4285606530",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4394625882",
    "https://openalex.org/W4384918183",
    "https://openalex.org/W4287114362",
    "https://openalex.org/W4367365797",
    "https://openalex.org/W2962965870",
    "https://openalex.org/W3200351427",
    "https://openalex.org/W3122930709",
    "https://openalex.org/W4386065575",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W4295246343",
    "https://openalex.org/W3212456749",
    "https://openalex.org/W4292825881",
    "https://openalex.org/W4312928771",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2146835493",
    "https://openalex.org/W4387294596",
    "https://openalex.org/W3181158454",
    "https://openalex.org/W2997749585",
    "https://openalex.org/W4293868248",
    "https://openalex.org/W4378770953",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2144554289",
    "https://openalex.org/W3179897446",
    "https://openalex.org/W4282961290",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W2810983211",
    "https://openalex.org/W4386598393",
    "https://openalex.org/W3035449864",
    "https://openalex.org/W4285192809",
    "https://openalex.org/W3102564565",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W2964065044",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4281686064",
    "https://openalex.org/W4281643269",
    "https://openalex.org/W4382202677",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W4386790226",
    "https://openalex.org/W3003921261",
    "https://openalex.org/W2750938222",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W4286906902",
    "https://openalex.org/W4225562651",
    "https://openalex.org/W4226177592",
    "https://openalex.org/W3153469116",
    "https://openalex.org/W3003868038",
    "https://openalex.org/W1491389626",
    "https://openalex.org/W2008806374",
    "https://openalex.org/W4312879041",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4283821822",
    "https://openalex.org/W2963233387",
    "https://openalex.org/W4285191490",
    "https://openalex.org/W1971822075",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W3082397598",
    "https://openalex.org/W4393147969",
    "https://openalex.org/W3202415716",
    "https://openalex.org/W4387968089",
    "https://openalex.org/W4294691145",
    "https://openalex.org/W4390871731",
    "https://openalex.org/W3003218881",
    "https://openalex.org/W3181186176",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W4387968234",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W4393212620",
    "https://openalex.org/W2095705004"
  ],
  "abstract": "Pre-trained vision-language models~(VLMs) are the de-facto foundation models for various downstream tasks. However, scene text recognition methods still prefer backbones pre-trained on a single modality, namely, the visual modality, despite the potential of VLMs to serve as powerful scene text readers. For example, CLIP can robustly identify regular (horizontal) and irregular (rotated, curved, blurred, or occluded) text in images. With such merits, we transform CLIP into a scene text reader and introduce CLIP4STR, a simple yet effective STR method built upon image and text encoders of CLIP. It has two encoder-decoder branches: a visual branch and a cross-modal branch. The visual branch provides an initial prediction based on the visual feature, and the cross-modal branch refines this prediction by addressing the discrepancy between the visual feature and text semantics. To fully leverage the capabilities of both branches, we design a dual predict-and-refine decoding scheme for inference. We scale CLIP4STR in terms of the model size, pre-training data, and training data, achieving state-of-the-art performance on 13 STR benchmarks. Additionally, a comprehensive empirical study is provided to enhance the understanding of the adaptation of CLIP to STR. Our method establishes a simple yet strong baseline for future STR research with VLMs.",
  "full_text": "1\nCLIP4STR: A Simple Baseline for Scene Text\nRecognition with Pre-trained Vision-Language\nModel\nShuai Zhao, Ruijie Quan B, Linchao Zhu, Yi Yang Senior Member, IEEE\nAbstract—Pre-trained vision-language models (VLMs) are the\nde-facto foundation models for various downstream tasks. How-\never, scene text recognition methods still prefer backbones pre-\ntrained on a single modality, namely, the visual modality, despite\nthe potential of VLMs to serve as powerful scene text readers.\nFor example, CLIP can robustly identify regular (horizontal) and\nirregular (rotated, curved, blurred, or occluded) text in images.\nWith such merits, we transform CLIP into a scene text reader\nand introduce CLIP4STR, a simple yet effective STR method\nbuilt upon image and text encoders of CLIP. It has two encoder-\ndecoder branches: a visual branch and a cross-modal branch. The\nvisual branch provides an initial prediction based on the visual\nfeature, and the cross-modal branch refines this prediction by\naddressing the discrepancy between the visual feature and text\nsemantics. To fully leverage the capabilities of both branches, we\ndesign a dual predict-and-refine decoding scheme for inference.\nWe scale CLIP4STR in terms of the model size, pre-training data,\nand training data, achieving state-of-the-art performance on 13\nSTR benchmarks. Additionally, a comprehensive empirical study\nis provided to enhance the understanding of the adaptation of\nCLIP to STR. Our method establishes a simple yet strong baseline\nfor future STR research with VLMs.\nIndex Terms—Vision-Language Model, Scene Text Recogni-\ntion, CLIP\nI. I NTRODUCTION\nV\nISION-LANGUAGE models (VLMs) pre-trained on\nweb-scale data like CLIP [1] and ALIGN [2] shows re-\nmarkable zero-shot capacity across different tasks. Researchers\nalso successfully transfer the knowledge from pre-trained\nVLMs to diverse tasks in a zero-shot or fine-tuning manner,\ne.g., visual question answering [3], information retrieval [4],\n[5], referring expression comprehension [6], and image cap-\ntioning [7]. VLM is widely recognized as a foundational model\nand an important component of artificial intelligence [8].\nScene text recognition (STR) is a critical technique and an\nessential process in many vision and language applications,\ne.g., document analysis, autonomous driving, and augmented\nreality. Similar to the aforementioned cross-modal tasks, STR\ninvolves two different modalities: image and text. However,\nThis work was partially supported by the Earth System Big Data Platform\nof the School of Earth Sciences, Zhejiang University. Corresponding author:\nRuijie Quan\nShuai Zhao is with the ReLER Lab, Australian Artificial Intelligence\nInstitute, University of Technology Sydney, Ultimo, NSW 2007, Aus-\ntralia. Part of this work is done during an internship at Baidu Inc. E-\nmail: zhaoshuaimcc@gmail.com. Linchao Zhu, Ruijie Quan, Yi Yang are\nwith ReLER Lab, CCAI, Zhejiang University, Zhejiang, China. E-mail:\n{zhulinchao, quanruijie, yangyics }@zju.edu.cn.\nnormal image\nImages\n Grad-CAM\nairplane\nautomobile\nbird\ncat 98%\ndeer\ndog\nfrog\nhorse\nship\ntruck\nProbabilities\n+ horizontal text\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse 98%\nship\ntruck\n+ rotated text\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse 99%\nship\ntruck\n+ curved text\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse 98%\nship\ntruck\n+ blurred text\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse 98%\nship\ntruck\n+ occluded text\n 0.0 0.2 0.4 0.6 0.8 1.0\nairplane\nautomobile\nbird\ncat 46%\ndeer\ndog\nfrog\nhorse 50%\nship\ntruck\nFig. 1: Zero-shot classification results of CLIP-ViT-B/32 .\nCLIP can perceive and understand text in images, even for\nirregular text with noise, rotation, and occlusion. CLIP is\npotentially a powerful scene text recognition expert.\nunlike the popularity of pre-trained VLMs in other cross-\nmodal tasks, STR methods still tend to rely on backbones\npre-trained on single-modality data [9], [10], [11], [12]. In\nthis work, we show that VLM pre-trained on image-text pairs\npossess strong scene text perception abilities, making them\nsuperior choices as STR backbones.\nSTR methods generally struggle with irregular text like\nrotated, curved, blurred, or occluded text [13], [14]. However,\nirregular text is prevalent in real-life scenarios [15], [16],\nmaking it necessary for STR models to effectively handle\nthese challenging cases. Interestingly, we observe that the\nVLM ( e.g., CLIP [1]) can robustly perceive irregular text in\narXiv:2305.14014v4  [cs.CV]  24 Dec 2024\n2\ncocacola \ndelocated \nImage Grad-CAM T ext \nfinish \nbridgestone \nmarriott \nstarbucks \nFig. 2: Attention of CLIP-ViT-B/32 for STR images .\nnatural images. In Fig. 1, we put different text stickers on\na natural image, use CLIP to classify it 1, and visualize the\nattention of CLIP via Grad-CAM [19]. It is evident that CLIP\npays high attention to the text sticker and accurately under-\nstands the meaning of the word, regardless of text variations 2.\nCLIP is trained on massive natural images collected from\nthe web, and its text perception ability may come from the\nnatural images containing scene texts [20]. Will CLIP perceive\nthe text in common STR images [21], [22], [16], which are\ncropped from a natural image? Fig. 2 presents the visualization\nresults of CLIP-ViT-B/32 for STR images. Although the text\nin these STR images is occluded, curved, blurred, and rotated,\nCLIP can still perceive them. From Fig. 1&2, we can see\nCLIP possesses an exceptional capability to perceive and\ncomprehend various text in images. This is exactly the desired\nquality for a robust STR backbone.\nIn this work, we aim to leverage the text perception ca-\npability of CLIP for STR and build a strong baseline for\nfuture STR research with VLMs. To this end, we introduce\nCLIP4STR, a simple yet effective STR framework built upon\nCLIP. CLIP4STR consists of two encoder-decoder branches:\nthe visual branch and the cross-modal branch. The image and\ntext encoders inherit from CLIP, while the decoders employ\nthe transformer decoder [23]. To enable the decoder to delve\ndeep into word structures (dependency relationship among\ncharacters in a word), we incorporate the permuted sequence\nmodeling technique proposed by PARSeq [24]. This allows\nthe decoder to perform sequence modeling of characters in\n1The class categories are from CIFAR-10 [17]. The experiment is inspired\nby Stanislav Fort [18].\n2This phenomenon, where CLIP focuses on the text while disregarding\nthe natural object, is also known as typographic attacks [20]. Neurons in\nCLIP image encoders can simultaneously perceive both visual and text signals\nassociated with the same concept, such as an image or typographic text of\nSpiderman. This ability may stem from the training images containing scene\ntexts in the large training data.\narbitrary orders without relying on specific sequence order\nassumptions. During training, the visual branch provides an\ninitial prediction based on the visual feature, which is then\nrefined by the cross-modal branch to address possible discrep-\nancies between the visual feature and text semantics of the\nprediction. The cross-modal branch functions as a semantic-\naware spell checker, similar to modern STR methods [9], [12].\nFor inference, we design a dual predict-and-refine decoding\nscheme to fully utilize the capabilities of both encoder-decoder\nbranches for improved character recognition.\nWe scale CLIP4STR across different model sizes, pre-\ntraining data, and training data to investigate the effective-\nness of large-scale pre-trained VLMs as STR backbones.\nCLIP4STR achieves state-of-the-art performance on 13 STR\nbenchmarks, encompassing regular and irregular text. Ad-\nditionally, we present a comprehensive empirical study on\nadapting CLIP to STR. CLIP4STR provides a simple yet\nstrong baseline for future STR research with VLMs.\nII. R ELATED WORK\nA. Vision-Language Models and Its Application\nLarge-scale pre-trained vision-language models learning un-\nder language supervision such as CLIP [1], ALIGN [2], and\nFlorence [25] demonstrate excellent generalization abilities.\nThis encourages researchers to transfer the knowledge of these\npre-trained VLMs to different downstream tasks in a fine-\ntuning or zero-shot fashion. For instance, [4], [26], [27] tune\nCLIP on videos and make CLIP specialized in text-video\nretrieval, CLIPScore [7] uses CLIP to evaluate the quality\nof generated image captions, and [28], [29] use CLIP as\nthe reward model during test time or training. The wide\napplication of VLMs also facilitates the research on differ-\nent pre-training models, e.g., ERNIE-ViLG [30], CoCa [31],\nOFA [32], DeCLIP [33], FILIP [34], and ALBEF [35]. Re-\nsearchers also explore the power of scaling up the data, e.g.,\nCOYO-700M [36] and LAION-5B [37]. Generally, more data\nbrings more power for large VLMs [38].\nVLMs pre-trained on large-scale image-text pairs possess\nmany fascinating attributes [1], [20], [39]. For instance, some\nneurons in CLIP can perceive the visual and text signals\ncorresponding to the same concept. [20] finds particular neu-\nrons in CLIP-RN50 ×4 respond to both photos of Spiderman\nand the text ‘‘spider’’ in an image. This also leads to\nTypographic Attacks, namely, VLMs focus on the text rather\nthan natural objects in an image as shown in Figure 1. In this\nwork, we leverage the text perception ability of multi-modal\nneurons and make CLIP specialize in scene text recognition.\nB. Scene Text Recognition\nScene text recognition methods can be broadly divided into\ntwo categories: context-free and context-aware. Context-free\nSTR methods only utilize the visual features of images, such as\nCTC-based [40] methods [41], [42], [43], [10], segmentation-\nbased methods [44], [45], [46], and attention-based methods\nwith an encoder-decoder mechanism [47], [48]. Since context-\nfree STR methods lack the understanding of text semantics,\nthey are less robust against occluded or incomplete text.\n3\ntext encoder\nimage encoder image decoder\ncross-modal decoder\nbr ii ad \nvisual prediction\nC\nﬁnal prediction\nbread br ii ad \nvisual prediction\nfrozen\nC concatenate\nstop gradient\nCLIP\nFig. 3: The framework of CLIP4STR . It has a visual branch and a cross-modal branch. The cross-modal branch refines the\nprediction of the visual branch for the final output. The text encoder is partially frozen.\nContext-aware STR methods are the mainstream approach,\nleveraging text semantics to enhance recognition performance.\nFor example, ABINet [9], LevOCR [49], MATRN [50], and\nTrOCR [12] incorporate an external language model to capture\ntext semantics. Other methods achieve similar goals with\nbuilt-in modules, such as RNN [51], [52], GRU [53], trans-\nformer [54], [24], [55]. The context information is interpreted\nas the relations of textual primitives by Zhang et al . [56],\nwho proposes a relational contrastive self-supervised learning\nSTR framework. Besides the context-free and context-aware\nmethods, some efforts aim to enhance the explainability of\nSTR. For instance, STRExp [57] utilizes local individual\ncharacter explanations to deepen the understanding of STR\nmethods. Moreover, training data plays a vital role in STR.\nTraditionally, synthetic data [58], [59] are used for training due\nto the ease of generating a large number of samples. However,\nrecent research suggests that using realistic training data can\nlead to better outcomes compared to synthetic data [60], [24],\n[61], [62]. Motivated by these findings, we primarily employ\nrealistic training data in this work.\nThe success of VLMs also spreads to the STR area. For\nexample, TrOCR [12] adopts separate pre-trained language\nand vision models plus post-pretraining on STR data in an\nauto-regressive manner [63], MATRN [50] uses a popular\nmulti-modal fusion manner in VLMs such as ALBEF [35] and\nViLT [64]. CLIPTER [65] enhances the character recognition\nperformance by utilizing the CLIP features extracted from\nthe global image. CLIP-OCR [66] leverages both visual and\nlinguistic knowledge from CLIP through feature distillation.\nIn contrast, we directly transfer CLIP to a robust scene text\nreader, eliminating the need for CLIP features from the global\nimage or employing an additional CLIP model as a teacher for\nthe STR reader. We hope our method can be a strong baseline\nfor future STR research with VLMs.\nIII. M ETHOD\nA. Preliminary\nBefore illustrating the framework of CLIP4STR, we first in-\ntroduce CLIP [1] and the permuted sequence modeling (PSM)\ntechnique proposed by PARSeq [24]. CLIP serves as the\nbackbone, and the PSM is used for sequence modeling.\n1) CLIP: CLIP consists of a text encoder and an image\nencoder. CLIP is pre-trained on 400 million image-text pairs\nusing contrastive learning. The text and image features from\nTABLE I: Examples of attention mask M. The sequences\nwith [B] and [E] represent the input context and output se-\nquence, respectively. The entry Mi,j = −∞ (negative infinity)\nindicates that the dependency of output i on input context j\nis removed.\n[B] y1 y2 y3\ny1 0 −∞ −∞ −∞\ny2 0 0 −∞ −∞\ny3 0 0 0 −∞\n[E] 0 0 0 0\n(a) AR mask\n[B] y1 y2 y3\ny1 0 −∞ 0 0\ny2 0 0 −∞ 0\ny3 0 0 0 −∞\n[E] 0 0 0 0\n(b) cloze mask\n[B] y1 y2 y3\ny1 0 −∞ 0 0\ny2 0 −∞ −∞ −∞\ny3 0 −∞ 0 −∞\n[E] 0 0 0 0\n(c) random mask\nCLIP are aligned in a joint image-text embedding space. i)\nThe image encoder of CLIP is a vision transformer (ViT) [69].\nGiven an image, ViT introduces a visual tokenizer (convolu-\ntion) to convert non-overlapped image patches into a discrete\nsequence. A [CLASS] token is then prepended to the beginning\nof the image sequence. Initially, CLIP image encoder only\nreturns the feature of the [ CLASS] token, but in this work,\nwe return features of all tokens. The rationale behind this\nchoice is that character-level recognition requires fine-grained\ndetail, and local features from all patches are necessary. These\nfeatures are normalized and linearly projected into the joint\nimage-text embedding space. ii) The text encoder of CLIP\nis a transformer encoder [23], [70]. The text tokenizer is a\nlower-cased byte pair encoding – BPE [71] with vocabulary\nsize 49,152. The beginning and end of the text sequence are\npadded with [ SOS] and [ EOS] tokens, respectively. Linguistic\nfeatures of all tokens are utilized for character recognition.\nThese features are also normalized and linearly projected into\nthe joint image-text embedding space.\n2) Permuted sequence modeling: Traditionally, STR meth-\nods use a left-to-right or right-to-left order to model character\nsequences [9]. However, the characters in a word do not\nstrictly follow such directional dependencies. For instance, to\npredict the letter “ o” in the word “ model”, it is sufficient to\nconsider only the context “ m_de” rather than relying solely\non the left-to-right context “ m_” or the right-to-left context\n“led_”. The dependencies between characters in a word can\ntake various forms. To encourage the STR method to ex-\nplore these structural relationships within words, PARSeq [24]\nintroduces a permuted sequence modeling (PSM) technique.\nThis technique uses a random attention mask M for attention\n4\nposition query\nMulti-Head Attention\nchar tokenizer\n[B] [P]\npostion embedding\nrandom\nattention\nmask\nMulti-Head Attention MLP Linear\ninput context\nb r a de\nCLIP visual or cross-modal feature\nb r a de [E] [P]\nCross Entropy Loss\nprediction\nGT\nFig. 4: The decoder of CLIP4STR . [B], [E], and [P] are the beginning, end, and padding tokens, respectively. ‘ [··· ]’ in\nprediction represents the ignored outputs. Layer normalization [67] and dropout [68] in the decoder are ignored.\noperations [23] to generate random dependency relationships\nbetween the input context and the output. Table I illustrates\nthree examples of mask M. We will delve further into this\nmechanism in §III-C.\nB. Encoder\nThe framework of CLIP4STR is illustrated in Fig. 3.\nCLIP4STR employs a dual encoder-decoder design, consist-\ning of a visual branch and a cross-modal branch. The text\nand image encoders utilize the architectures and pre-trained\nweights from CLIP. The visual branch generates an initial\nprediction based on the visual features extracted by the image\nencoder. Subsequently, the cross-modal branch refines the\ninitial prediction by addressing the discrepancy between the\nvisual features and the textual semantics of the prediction.\nSince the image and text features are aligned in a joint image-\ntext embedding space during pre-training, it becomes easy to\nidentify this discrepancy. The cross-modal branch acts as a\nsemantic-aware spell checker.\nThe text encoder is partially frozen. This freezing operation\nretains the learned text understanding ability of the language\nmodel and reduces training costs. It is a common practice in\ntransfer learning of large language models [72]. In contrast, the\nvisual branch is fully trainable due to the domain gap between\nSTR data (cropped word images) and CLIP training data\n(collected from the web, often natural images). Additionally,\nwe block the gradient flow from the cross-modal decoder to\nthe visual encoder to enable autonomous learning of the visual\nbranch, resulting in improved refined cross-modal predictions.\nFor the text encoder g(·) and the image encoder h(·), given\nthe input text t and image x, the text, image, and cross-modal\nfeatures are computed as:\nFt = g(t) ∈ RLt×D, (1)\nFi = h(x) ∈ RLi×D, (2)\nFc = [FT\ni FT\nt ]T ∈ RLc×D, (3)\nwhere Lt represents the text sequence length, Li is the\nsequence length of image tokens, D denotes the dimension\nof the joint image-text embedding space, and the cross-modal\nsequence length Lc = Li + Lt.\nC. Decoder\nThe decoder aims to extract the character information from\nthe visual feature Fi or cross-modal feature Fc. The decoder\nframework is shown in Fig. 4. It adopts the design of the\ntransformer decoder [23] plus the PSM technique mentioned\nin § III-A2, enabling a predicted character to have arbitrary\ndependencies on the input context during training.\nThe visual and cross-modal decoders have the same archi-\ntecture but differ in the input. They receive the following\ninputs: a learnable position query p ∈ RN×D, an input\ncontext c ∈ RN×D, and a randomly generated attention\nmask M ∈RN×N . N represents the length of characters.\nThe decoder outputs the prediction y ∈ RN×C, where C\nis the number of character classes. The decoding stage can\nbe denoted as: y = DEC(p, c, M, F). The first Multi-Head\nAttention (MHA) in Fig. 4 performs context-position attention:\nm1 = softmax(pcT\n√\nD\n+ M)c + p. (4)\nThe second MHA focuses on feature-position attention:\nm2 = softmax(m1FT\n√\nD\n)F + m1. (5)\nFor simplicity, we ignore the input and output linear trans-\nformations in the attention operations of Eq. (4) and Eq. (5).\nThen m2 ∈ RN×D is used for the final prediction y:\ny = Linear(MLP(m2) +m2). (6)\nDuring training, the output of the decoder depends on the\nrandomly permuted input context. This encourages the decoder\nto analyze the word structure beyond the traditional left-\nto-right or right-to-left sequence modeling assumptions [9].\nThe inclusion of a random attention mask M in Eq.(4)\nenables this capability [24]. Table I presents examples of\ngenerated attention masks, including a left-to-right auto-\nregressive (AR) mask, a cloze mask, and a random mask.\nFollowing PARSeq [24], we employ K = 6masks per input\ncontext during training. The first two masks are left-to-right\nand right-to-left masks, and others are randomly generated.\nCLIP4STR is optimized to minimize the sum of cross-entropy\nlosses (CE(·)) of the visual branch and the cross-modal branch:\nL = CE(yi, ˆy) +CE(y, ˆy), (7)\nwhere ˆy, yi, and y indicate ground truth, prediction of the\nvisual branch, and prediction of the cross-modal branch.\n5\nAlgorithm 1: Inference decoding scheme (§A)\nInput: image x, image encoder h(·) and decoder Deci(·),\ntext encoder g(·), cross-modal decoder Decc(·), AR\nmask Ma, cloze mask Mc, image and cross-modal\nposition query pi and pc, context c = 0 ∈ RN×D,\nchar and text tokenizer CTK(·) and TTK(·), iterative\nrefinement times Ti\nOutput: prediction y\n// c1,· denote the 1st row\n1 c1,· ← CTK([B]);\n2 Fi ← h(x);\n// autoregressive visual decode\n3 yi ← 0;\n4 for k ← 1 to N − 1 do\n5 yi\nk,· ← Deci(pi\nk,·, c1:k,·, Ma\n1:k,1:k, Fi);\n6 ck+1,· ← CTK(yi\nk,·);\n7 end\n// autoregressive cross-modal decode\n8 Fc ← [FT\ni g(TTK(yi))T ]T ;\n9 y ← 0;\n10 for k ← 1 to N − 1 do\n11 yk,· ← Decc(pc\nk,·, c1:k,·, Ma\n1:k,1:k, Fc);\n12 ck+1,· ← CTK(yk,·);\n13 end\n// refinement with cloze mask\n14 for k ← 1 to Ti do\n15 c ← [CTK([B])T CTK(yi\n1:N−1,·)T ]T ;\n16 yi ← Deci(pi, c, Mc, Fi);\n17 Fc ← [FT\ni g(TTK(yi))T ]T ;\n18 c ← [CTK([B])T CTK(y1:N−1,·)T ]T ;\n19 y ← Decc(pc, c, Mc, Fc);\n20 end\n1) Decoding scheme: CLIP4STR consists of two branches:\na visual branch and a cross-modal branch. To fully exploit the\ncapacity of both branches, we design a dual predict-and-refine\ndecoding scheme for inference, inspired by previous STR\nmethods [9], [24]. Alg. 1 illustrates the decoding process. The\nvisual branch first performs autoregressive decoding, where the\nfuture output depends on previous predictions. Subsequently,\nthe cross-modal branch addresses possible discrepancies be-\ntween the visual feature and the text semantics of the visual\nprediction, aiming to improve recognition accuracy. This pro-\ncess is also autoregressive. Finally, the previous predictions\nare utilized as the input context for refining the output in a\ncloze-filling manner. The refinement process can be iterative.\nAfter iterative refinement, the output of the cross-modal branch\nserves as the final prediction.\nIV. E XPERIMENT\nA. Experimental Details\nWe instantiate CLIP4STR with CLIP-ViT-B/16, CLIP-ViT-\nL/14, and CLIP-ViT-H/14 [38]. Table II presents the main\nhyper-parameters of CLIP4STR. A reproduction of CLIP4STR\nis at https://github.com/VamosC/CLIP4STR.\nTest benchmarks The evaluation benchmarks include\nIIIT5K [74], CUTE80 [75], Street View Text (SVT) [76], SVT-\nPerspective (SVTP) [77], ICDAR 2013 (IC13) [21], ICDAR\n2015 (IC15) [22], and three occluded datasets – HOST,\nTABLE II: Model sizes and optimization hyper-parameter .\nThe learning rate for CLIP encoders is 8.4e-5 × batch\n512 [73]. For\nmodels trained from scratch (decoders), the learning rate is\nmultiplied by 19.0. Params is the total parameters in a model,\nand non-trainable parameters in three models are 44.3M,\n80.5M, and 126M, respectively. Training time is measured on\n8 NVIDIA RTX A6000 GPUs.\nModel Params Train Data Batch Epochs Time\nCLIP4STR-B 158M Real(3.3M) 1024 16 12.8h\nCLIP4STR-L 446M Real(3.3M) 1024 10 23.4h\nCLIP4STR-H 1B RBU(6.5M) 1024 4 48.0h\nWOST [78], and OCTT [14]. Additionally, we utilize 3 re-\ncent large benchmarks: COCO-Text (low-resolution, occluded\ntext) [79], ArT (curved and rotated text) [15], and Uber-\nText (vertical and rotated text) [16].\nTraining dataset 1) MJ+SJ : MJSynth (MJ, 9M sam-\nples) [58] and SynthText (ST, 6.9M samples) [59]. 2)\nReal(3.3M): COCO-Text (COCO) [79], RCTW17 [80], Uber-\nText (Uber) [16], ArT [15], LSVT [81], MLT19 [82],\nReCTS [83], TextOCR [84], Open Images [85] annotations\nfrom the OpenVINO toolkit [86]. These real datasets have\n3.3M images in total. 3) RBU(6.5M) : A dataset provided\nby [62]. It combines the Real(3.3M), benchmark datsets (train-\ning data of SVT, IIIT5K, IC13, and IC15), and part of\nUnion14M-L [61].\nLearning strategies We apply a warm up and cosine learning\nrate decay policy. The batch size is kept to be close to 1024.\nFor large models, this is achieved by gradient accumulation.\nFor synthetic data, we train CLIP4STR-B for 6 epochs and\nCLIP4STR-L for 5 epochs. For RBU(6.5M) data, we train\n11, 5, and 4 epochs for CLIP4STR-B, CLIP4STR-L, and\nCLIP4STR-H, respectively. AdamW [87] optimizer is adopted\nwith a weight decay value 0.2. All experiments are performed\nwith mixed precision [88].\nData and label processing RandAugment [97] excludes\nsharpness and invert is used with layer depth 3 and magnitude\n5. The image size is 224×224. The sequence length of the text\nencoder is 16. The maximum length of the character sequence\nis 25. Considering an extra [B] or [E] token, we set N = 26.\nDuring training, the number of character classes C = 94, i.e.,\nmixed-case alphanumeric characters and punctuation marks\nare recognized. During inference, we only use a lowercase\nalphanumeric charset, i.e., C = 36. The iterative refinement\ntimes Ti = 1. The evaluation metric is word accuracy.\nB. Comparison to State-of-the-art\nWe compare CLIP4STR with previous SOTA methods on 10\ncommon STR benchmarks in Table III. CLIP4STR surpasses\nthe previous methods by a significant margin, achieving new\nSOTA performance. Notably, CLIP4STR performs exception-\nally well on irregular text datasets, such as IC15 (incidental\nscene text), SVTP (perspective scene text), CUTE (curved\ntext line images), HOST (heavily occluded scene text), and\nWOST (weakly occluded scene text). This aligns with the\nexamples shown in Fig. 1&2 and supports our motivation for\n6\nTABLE III: Word accuracy on 10 common benchmarks. The best and second-best results are highlighted. Benchmark\ndatasets (B) - SVT, IIIT5K, IC13, and IC15. ‘N/A’ for not applicable. ♯Reproduced by PARSeq [24].\nMethod Pre-train Data Train Data IIIT5K SVT IC13 IC15 IC15 SVTP CUTE HOST WOST OCTT\n3,000 647 1,015 1,811 2,077 645 288 2,416 2,416 1,911\n0-shot CLIP [1] WIT 400M [1] N/A 90.0 – – – – – – – – –\nSRN [89] ImageNet-1K MJ+ST 94.8 91.5 – 82.7 – 85.1 87.8 – – –\nTextScanner [45] N/A MJ+ST 95.7 92.7 94.9 – 83.5 84.8 91.6 – – –\nRCEED [90] N/A MJ+ST+B 94.9 91.8 – – 82.2 83.6 91.7 – – –\nTRBA [60] N/A MJ+ST 92.1 88.9 – 86.0 – 89.3 89.2 – – –\nVisionLAN [78] From Scratch MJ+ST 95.8 91.7 – 83.7 – 86.0 88.5 50.3 70.3 –\nABINet [9] WikiText-103 MJ+ST 96.2 93.5 – 86.0 – 89.3 89.2 – – –\nViTSTR-B [10] ImageNet-1K MJ+ST 88.4 87.7 92.4 78.5 72.6 81.8 81.3 – – –\nLevOCR [49] WikiText-103 MJ+ST 96.6 92.9 – 86.4 – 88.1 91.7 – – –\nMATRN [50] WikiText-103 MJ+ST 96.6 95.0 95.8 86.6 82.8 90.6 93.5 – – –\nPETR [91] N/A MJ+ST 95.8 92.4 97.0 83.3 – 86.2 89.9 – – –\nDiG-ViT-B [11] Textimages-33M MJ+ST 96.7 94.6 96.9 87.1 – 91.0 91.3 74.9 82.3 –\nPARSeqA [24] From Scratch MJ+ST 97.0 93.6 96.2 86.5 82.9 88.9 92.2 – – –\nTrOCRLarge [12] Textlines-684M MJ+ST+B 94.1 96.1 97.3 88.1 84.1 93.0 95.1 – – –\nSIGAT [92] ImageNet-1K MJ+ST 96.6 95.1 96.8 86.6 83.0 90.5 93.1 – – –\nCLIP-OCR [66] From Scratch MJ+ST 97.3 94.7 – 87.2 – 89.9 93.1 – – –\nLISTER-B [93] N/A MJ+ST 96.9 93.8 – 87.2 – 87.5 93.1 – – –\nCLIPTER [65] N/A Real(1.5M) – 96.6 – – 85.9 – – – – –\nDiG-ViT-B [11] Textimages-33M Real(2.8M) 97.6 96.5 97.6 88.9 – 92.9 96.5 62.8 79.7 –\nCCD-ViT-B [94] Textimages-33M Real(2.8M) 98.0 97.8 98.3 91.6 – 96.1 98.3 – – –\nViTSTR-S [10] ♯ ImageNet-1K Real(3.3M) 97.9 96.0 97.8 89.0 87.5 91.5 96.2 64.5 77.9 64.2\nABINet [9] ♯ From Scratch Real(3.3M) 98.6 98.2 98.0 90.5 88.7 94.1 97.2 72.2 85.0 70.1\nPARSeqA [24] From Scratch Real(3.3M) 99.1 97.9 98.4 90.7 89.6 95.7 98.3 74.4 85.4 73.1\nMAERec-B [61] Union14M-U Union14M-L 98.5 97.8 98.1 – 89.5 94.4 98.6 – – –\nCLIP4STR-B\nWIT 400M\nMJ+ST 97.7 95.2 96.1 87.6 84.2 91.3 95.5 79.8 87.0 57.1\nCLIP4STR-L MJ+ST 98.0 95.2 96.9 87.7 84.5 93.3 95.1 82.7 88.8 59.2\nCLIP4STR-B Real(3.3M) 99.2 98.3 98.3 91.4 90.6 97.2 99.3 77.5 87.5 81.8\nCLIP4STR-L Real(3.3M) 99.5 98.5 98.5 91.3 90.8 97.4 99.0 79.8 89.2 84.9\nCLIP4STR-B\nDataComp-1B [95]\nReal(3.3M) 99.4 98.6 98.3 90.8 90.3 97.8 99.0 77.6 87.9 83.1\nCLIP4STR-B RBU(6.5M) 99.5 98.3 98.6 91.4 91.1 98.0 99.0 79.3 88.8 83.5\nCLIP4STR-L RBU(6.5M) 99.6 98.6 99.0 91.9 91.4 98.1 99.7 81.1 90.6 85.9\nCLIP4STR-H DFN-5B [96] RBU(6.5M) 99.5 99.1 98.9 91.7 91.0 98.0 99.0 82.6 90.9 86.5\nadapting CLIP as a scene text reader, as CLIP demonstrates\nrobust identification of regular and irregular text. CLIP4STR\nexhibits excellent reading ability on occluded datasets, sur-\npassing the previous SOTA by 7.8% and 5.4% in the best\ncase on HOST and WOST, respectively. This ability can be\nattributed to the pre-trained text encoder and cross-modal de-\ncoder, which can infer missing characters using text semantics\nor visual features. The performance of CLIP4STR is also much\nbetter than CLIP-OCR [66] and CLIPTER [65], both of which\nwork in a similar direction as CLIP4STR. This demonstrates\nthat directly transferring CLIP into a STR reader is more\neffective than the distillation method [66] or utilizing CLIP\nfeatures of the global image as auxiliary context [65].\nIn addition to the small-scale common benchmarks, we\nalso evaluate CLIP4STR on 3 larger and more challenging\nbenchmarks. These benchmarks primarily consist of irregular\ntexts with various shapes, low-resolution images, rotation, etc.\nThe results, shown in Table IV, further demonstrate the strong\ngeneralization ability of CLIP4STR. CLIP4STR substantially\noutperforms the previous SOTA methods on these three large\nand challenging benchmarks. At the same time, we observe\nthat scaling CLIP4STR to 1B parameters does not bring much\nimprovement in performance. CLIP4STR-L is comparable to\nCLIP4STR-H in most cases, while CLIP4STR-H is superior\nin recognizing occluded characters (WOST, HOST, OCTT).\nTABLE IV: Word accuracy on 3 large benchmarks .\n♯Reproduced by PARSeq [24].\nMethod Train COCO ArT Uber\ndata 9,825 35,149 80,551\nViTSTR-S [10] ♯ MJ+ST 56.4 66.1 37.6\nTRBA [60] ♯ MJ+ST 61.4 68.2 38.0\nABINet [9] ♯ MJ+ST 57.1 65.4 34.9\nPARSeq A [24] MJ+ST 64.0 70.7 42.0\nMPSTR A [98] MJ+ST 64.5 69.9 42.8\nCLIP-OCR [66] MJ+ST 66.5 70.5 42.4\nCLIP4STR-B MJ+ST 66.3 72.8 43.4\nCLIP4STR-L MJ+ST 67.0 73.7 44.5\nDiG-ViT-B [11] Real(2.8M) 75.8 – –\nViTSTR-S [10] ♯ Real(3.3M) 73.6 81.0 78.2\nTRBA [60] ♯ Real(3.3M) 77.5 82.5 81.2\nABINet [9] ♯ Real(3.3M) 76.5 81.2 71.2\nPARSeq A [24] Real(3.3M) 79.8 84.5 84.1\nMPSTR A [98] Real(3.3M) 80.3 84.4 84.9\nCLIP4STR-B Real(3.3M) 81.1 85.8 86.8\nCLIP4STR-L Real(3.3M) 81.9 85.9 87.6\nCLIP4STR-B RBU(6.5M) 81.3 85.8 92.1\nCLIP4STR-L RBU(6.5M) 82.7 86.4 92.2\nCLIP4STR-H RBU(6.5M) 83.0 86.4 91.7\nV. E MPIRICAL STUDY\nThis section presents our empirical study on adapting CLIP\nto STR. Without mention, the models are all trained on 3.3M\n7\nTABLE V: Ablation study of different components of\nCLIP4STR. PSM is short for the permuted sequence mod-\neling technique [24]. Recipe represents the training recipe for\nCLIP4STR in §IV-A. Cross denotes the cross-modal branch.\n[CLASS] with a ✓mark means the decoders only use the\n[CLASS] and [EOS] of CLIP encoders rather than features\nof all tokens (refer to §III-A).\nReference Method Avg.\nABINet [9] 89.1\nPARSeqA [24] (previous SOTA) 89.9\nBase PSM ViT-B Recipe Cross [CLASS] ViT-L Avg.\n✓ 89.2\n✓ ✓ 89.9\n✓ ✓ ✓ 90.0\n✓ ✓ ✓ ✓ 90.8\n✓ ✗ ✓ ✓ 90.0\n✓ ✓ ✓ ✓ ✓ ✓ 90.6\n✓ ✓ ✓ ✓ ✓ ✗ 91.2\n✓ ✓ ✓ ✓ ✓ ✗ ✓ 91.9\nreal data, and the IC15 dataset here contains 2,077 samples.\nThe average accuracy reported in this section is calculated over\nthe first 9 benchmarks (14,315 samples) in Table III.\nA. Ablation Study of CLIP4STR\nTable III&IV show that CLIP4STR achieves SOTA perfor-\nmance on 11 STR benchmarks. What are the sources of this\nhigh performance? We conduct ablation studies of different\ncomponents in Table V, starting with the visual branch in\nFig. 3 as the baseline (accuracy 89.2%). The encoder is a ViT-\nS without pre-training. Then we apply the permuted sequence\nmodeling (PSM) technique [24] to the visual decoder and\nfollow the training recipe of PARSeq: 4 ×8 patch size, the\nsame learning rate for the encoder and decoder, and 20 training\nepochs. This brings a 0.7% improvement in accuracy. Next,\nwe replace the encoder with the image encoder of CLIP-\nViT-B/16. However, no significant gain is observed without\nadaptations. To unleash the potential of CLIP, we adjust the\ntraining recipe: using 16 ×16 patch size, a small learning rate\nfor CLIP encoders, a relatively large learning rate for decoders,\nand fewer training epochs — 16 (§IV-A). The learning rate\nis searched automatically by Ray [99], and the best number\nof training epochs is decided by manual test. CLIP makes\nthe model converge easier and faster, so the training recipe\nshould change accordingly. To better grasp the contribution of\nPSM, we conducted an experiment where we removed PSM\nand achieved an accuracy of 90.0%.\nAlthough the performance of the visual branch is already\nvery high (90.8%), the cross-modal branch further improves\nthe accuracy by 0.4%, demonstrating its effectiveness. It is\nworth noting that utilizing CLIP features of all patches is\ncrucial for character recognition. Only using the [CLASS]\nand [EOS] results in inferior performance – 90.6%. Moreover,\nthe use of a large model — CLIP-ViT-L/14 further increases\nthe accuracy by 0.7%. The large CLIP-ViT-L/14 converges\nfaster than CLIP-ViT-B/16 for STR. It only requires 10 epochs\nof training on the Real(3.3M) data, much less than the training\nepochs of CLIP-ViT-B/16.\nTABLE VI: Freezing options in CLIP4STR-B . #Params\nmeans the number of learnable parameters of encoders in\nCLIP4STR-B. One decoder in CLIP4STR-B has 4.3M param-\neters. token means we only use pre-trained token embed-\ndings of CLIP text encoder as text features.\nFrozen Layers #Params IC15 WOST HOST COCO UberImage Text\n0 0 149 M 90.8 87.5 76.4 80.8 87.0\n0 3 114 M 90.4 88.1 76.9 81.2 86.8\n0 6 104 M 90.6 87.5 77.5 81.1 86.8\n0 9 95 M 90.3 86.8 74.9 80.9 86.3\n0 12 86 M 90.3 86.1 74.9 80.9 86.4\n0 token 86 M 90.7 87.3 77.0 80.9 86.7\n0 6 95 M 90.6 87.5 77.5 81.1 86.8\n3 6 84 M 90.4 88.5 76.5 81.3 86.4\n6 6 62 M 89.5 86.7 72.8 80.3 83.8\n9 6 41 M 87.8 80.0 64.0 75.3 72.8\n12 6 19 M 61.2 55.8 40.4 49.5 20.6\nB. Parameter Freezing Options\nIn CLIP4STR, we freeze half of the layers in the CLIP\ntext encoder, which is a common practice when transferring a\nlarge language model to new tasks [72]. Table VI illustrates the\ninfluence of different parameter freezing options. The results\nindicate that freezing the language model has a lesser impact\ncompared to freezing the image model. Despite using the\nfixed pre-trained token embeddings of the CLIP text encoder,\nthe system can still achieve satisfactory performance. This\ndemonstrates that semantic understanding in STR is relatively\neasier compared to general language understanding. In STR,\ntext mainly consists of words and phrases, which simplifies\nthe task compared to the general language case. On the other\nhand, freezing the image models has a significant impact\non performance. The substantial domain gap between the\ndata in STR and the pre-trained data of the CLIP image\nencoder possibly contributes to this discrepancy. CLIP is pre-\ntrained on web images, which are primarily natural images.\nIn contrast, the scene text recognition data comprises cropped\nword images. Such a disparity may necessitate a fully trainable\nimage encoder in CLIP4STR to bridge the domain gap.\nC. Comparison to Single-modality Pre-trained Model\nIn previous empirical studies, we see the effectiveness\nof CLIP as a STR backbone. Is VLM better than models\npre-trained on single-modality data? To further clarify this\nquestion, Table VII presents the results of replacing the visual\nencoder in Fig. 3 with a random initialized ViT, an ImageNet-\n1K [100] pre-trained ViT via DeiT [101] 3, and an ImageNet-\n21K pre-trained ViT provided by Ridnik et al. [102]. The\ntraining schedules including the learning rate and training\nepochs are kept the same as CLIP4STR. In Table VII, the\nImageNet pre-trained models even perform worse than the\nmodel trained from scratch. Previous works also support\nthis finding. PARSeq [24] trains its vision transformer from\nscratch rather than using a pre-trained model. TrOCR [12]\nuses pre-trained transformers from DeiT [101], BEiT [103],\n3https://github.com/facebookresearch/deit\n8\nTABLE VII: Different pre-training strategies . #Params\nmeans the learnable parameters in the visual encoder. For\na fair comparison, only the results of the visual branch in\nCLIP4STR-B are shown.\nPre-train #Params IC15 WOST HOST COCO Uber\nScratch 86 M 90.1 84.9 74.8 80.7 86.6\nImageNet-1K 86 M 89.7 82.7 68.7 80.0 84.0\nImageNet-21K 86 M 89.3 83.1 69.1 79.6 82.9\nImage-text pairs 86 M 90.3 87.4 76.3 80.9 86.6\nTABLE VIII: Parameter-efficient adaptations . #Params\nmeans the learnable parameters in the visual encoder. r is\nthe feature reduction ratio in LST. Here we only show the\nresults of the visual branch in CLIP4STR-B, and the cross-\nmodal branch is ignored.\nMethod #Params IC15 WOST HOST COCO Uber\nFrozen 0 60.9 54.8 39.9 48.9 20.1\nCLIP-Adapter 262 K 63.6 57.2 41.1 50.9 22.7\nLST ( r = 4) 4.1M 88.2 82.8 66.1 77.1 78.7\nLST ( r = 2) 13.1M 89.6 86.0 70.8 79.6 80.6\nFine-tune 86 M 90.3 87.4 76.3 80.9 86.6\nand RoBERTa [104], but it still post-pretrains them on 684M\ntextlines from publicly available PDF files on the Internet.\nImageNet classification pre-training does not align well with\nSTR. Classifying objects in an image does not help the model\nlearn specific information about the text within the image.\nFor example, two images – one of a cat and one of a dog\n– both containing the text “ park” cause the model to learn\ncontradictory information about the same text. In contrast, the\nvision encoders in CLIP can accurately perceive text signals\ndue to the presence of multi-modal neurons [20] (§II-A),\nmaking CLIP a strong backbone for STR.\nD. Parameter-efficient Adaptations\nCLIP4STR fine-tunes the whole pre-trained CLIP model\nto transfer the knowledge of CLIP to the STR task. Besides\nsuch a fully fine-tuning manner, the parameter-efficient fine-\ntuning (PEFT) methods for large pre-trained models are also\npopular. For example, CoOp [105] only trains several learnable\nprefix prompts for efficiency, and CLIP-Adapter [106] incorpo-\nrates tunable linear layers on top of frozen VLMs. These PEFT\nmethods achieve pretty good performance on a few tasks, so\nwe wonder if such PEFT methods work for STR.\nWe test CLIP with two PEFT methods in this work,\ni.e., CLIP-Adapter [106] and Ladder Side-Tuning (LST)\nadapter [107]. Fig. 5 shows the design of the two adapters.\nCLIP-Adapter adds two linear layers on the top of the frozen\npre-trained VLM. We use the same architecture as [106]: a\nresidual addition ratio λ = 0.2, which means that the original\nCLIP feature is multiplied by 0.8. Ladder Side-Tuning (LST)\nuses a ladder side network as shown in Fig. 5. Following [107],\nwe use the structure-pruned [108] CLIP model as the ladder\nside network. The CLIP features are downsampled by a factor\nof 1/r before entering the ladder side network to reduce the\ncomputation cost, and then upsampled by a factor of r before\ninput input \noutput output \nfrozen \nbackbone \nnew \nparameters \nFig. 5: CLIP-Adapter (left) and LST (right) .\noutput to match the original feature dimension. We also use\nthe layer-dropping strategy in LST, which connects only the\nlayers [2, 4, 6, 8, 10, 12] to the ladder side network, namely, the\ndepth of LST is 6. This reduces the training cost.\nThe results of using the two adapters with CLIP in STR\nare presented in Table VIII. CLIP-Adapter outperforms the\nfrozen model but falls short of the performance achieved by\nthe fully fine-tuned model. The addition of a few learnable\nparameters on top of the CLIP model alone is insufficient to\nbridge the domain gap between scene text data and the pre-\ntraining data of CLIP. On the other hand, LST achieves notably\nimproved performance but still lags behind the fine-tuned\nmodel. However, when the parameters of LST are increased, it\napproaches the performance of the fine-tuned model. Overall,\nLST can serve as an alternative option when computational\nresources are limited for training.\nE. Inference Time\nDespite the good performance, adapting the pre-trained\nCLIP model introduces extra training and inference costs due\nto its large size. Table IX presents the inference time of\nCLIP4STR. The large transformer models slow down the in-\nference speed of CLIP4STR. However, using a large ViT does\nnot always improve accuracy, as Table VII shows, because\nof different pre-training strategies. The cross-modal branch\nalso increases the inference time, but slightly (0.49ms), since\nthe input sequence length of the text encoder is small (16, as\nexplained in §IV-A). Moreover, we can reduce the inference\ntime of the cross-modal branch by replacing line 10 ∼13 in\nAlg. 1 with\ny ← Decc(pc, c, Ma, Fc). (8)\nEq. (8) uses the prediction of the visual branch as the input\ncontext instead of the previous prediction of the cross-modal\n9\nTABLE IX: Inference time of CLIP4STR. AR stands for\nautoregressive decoding, and cloze stands for cloze-filling\ndecoding manner (refer to Table I). Iter. is the number\nof refinement steps during decoding. Time is the average\ninference time per sample on a single NVIDIA A100 40GB.\nMethod Backbone Decode Iter. Avg. Time (ms)\nABINet [9] ResNet-45 Cloze 1 89.1 1.30\nPARSeq [24] ViT-S AR 1 89.9 1.32\nPARSeq [24] ViT-B AR 1 90.0 2.81\nCLIP4STR-B (Visual) ViT-B Cloze 1 89.8 2.73\nCLIP4STR-B (Visual) ViT-B AR 1 90.8 3.03\nCLIP4STR-B (Cross) ViT-B AR 1 91.2 3.52\nCLIP4STR-B (Cross) ViT-B AR + Eq. (8) 1 91.1 3.41\nCLIP4STR-B (Cross) ViT-B AR 2 91.2 3.72\nCLIP4STR-B (Cross) ViT-B AR 3 91.2 3.85\nCLIP4STR-L (Cross) ViT-L AR 1 91.9 6.52\nTABLE X: Word accuracy on cleaned benchmarks . Mis-\nlabeled samples in blue benchmarks are cleaned by Yang et\nal. [98]. All methods are trained on 3.3M real samples. The\nbest results are highlighted.\nMethod IIIT5K SVT IC13 IC15 IC15 SVTP CUTE\n3,000 647 1,015 1,811 2,077 645 288\nABINet [9]♯ 98.6 97.8 98.0 93.2 91.4 94.7 97.2\nPARSeqA [24] 98.9 97.5 98.5 93.8 92.6 95.7 98.6\nMPSTRA [98] 99.2 98.5 98.3 93.9 92.7 96.1 99.0\nCLIP4STR-B 99.2 97.8 98.4 94.1 93.3 97.4 99.3\nCLIP4STR-L 99.4 97.8 98.6 94.0 93.5 97.4 99.0\nbranch, avoiding repeated runs of the cross-modal decoder.\nHowever, this slightly decreases the performance. The ViT-\nL backbone also increases the inference time. Clearly, for\nCLIP4STR, there is a trade-off between recognition accuracy\nand inference speed. Besides, Table IX also shows that more\niterative refinement times (a large Ti at line 14 in Alg. 1)\nwill not bring further improvement in accuracy, so we just set\nTi =1 in practice.\nF . Qualitative results\nFig. 6 shows qualitative results of CLIP4STR on IC15 (in-\ncidental text), SVTP (perspective text), CUTE (curved text),\nand HOST (heavily occluded). CLIP4STR can robustly read\nscene text that is curved, occluded, blurred, or rotated, showing\nits great robustness. Meanwhile, we find that CLIP4STR has\na strong ability to complement incomplete characters. In the\nlast two cases in Fig. 6, CLIP4STR predicts an additional\n“n” character. This capability may stem from the semantic\nunderstanding of the pre-trained CLIP model. However, the\naccuracy of this complement is uncertain, and we currently\ncannot control this behavior in CLIP4STR.\nG. Results on Cleaned Benchmarks\nRecently, Yang et al . [98] correct the ground truth of\nmislabeled samples and present cleaned versions of IIIT5K,\nSVT, IC13, IC15, SVTP, and CUTE. Table X shows the results\nof CLIP4STR on these cleaned benchmarks. CLIP4STR still\nachieves SOTA performance on these cleaned benchmarks.\nDelocated \nGABBANA UNIVERSAL \nVISA \nImages Prediction Images Prediction \nCarlsberg \n State \nFINISH \n BREWER Y \nIT ALIAN \n SCOTTISH \nCINERAMA \n CINERAMA \nMarriott \n HIGGINS \nCAUTION \n Expert \nn venting \n comi n \nFig. 6: Qualitative results of CLIP4STR-B .\nVI. C ONCLUSION\nWe present CLIP4STR, a method that leverages CLIP for\nSTR. It has a dual encoder-decoder architecture: a visual\nbranch for initial prediction and a cross-modal branch for\nrefinement. CLIP4STR achieves state-of-the-art results on 13\nSTR benchmarks, showing that CLIP is a powerful scene text\nreader and that vision-language pre-training benefits STR. We\nalso conduct a comprehensive empirical study to explain how\nCLIP adapts to STR. We hope CLIP4STR can serve as a\nsimple but strong baseline for future STR research with VLMs.\nAPPENDIX A\nDETAIL EXPLANATION OF THE INFERENCE PROCESS\nHere we provide an explanation of the inference process in\nAlg. 1. Given an image x, the initial step involves obtaining\nthe image feature Fi ← h(x). This image feature Fi is\nthen forwarded to the visual decoder to generate the visual\nprediction yi, with the blank context (denoted as token [B])\nserving as the initial condition (line 1). Subsequently, the\nvisual decoder operates in an autoregressive manner, utilizing\nprevious predictions as context for subsequent ones (lines 4-\n7). Once the prediction is obtained from the visual branch, the\nlinguistic feature is derived by inputting the visual prediction\nyi along with the cross-model feature Fc into the text encoder\n(line 8). Similar to the decoding process of the visual decoder,\nthe cross-modal decoder generates predictions y in an au-\ntoregressive fashion (lines 10–13). Upon acquiring predictions\n10\nTABLE XI: Comparison with autoregressive pre-training\nmethods. Rang et al . [62] train CLIP4STR on RBEU-\nSyn(23.8M). The best and second-best results are highlighted.\nMethod Pre-train IIIT5K SVT IC13 IC15 IC15 SVTP CUTE\n3,000 647 1,015 1,811 2,077 645 288\nTrOCR [12] Textlines-684M 94.1 96.1 97.3 88.1 84.1 93.0 95.1\nDTrOCR [55] Textlines-6B 99.6 98.9 99.4 93.5 93.2 98.6 99.1\nCLIP4STR-B [62] WIT-400M 99.0 98.8 – 92.3 – 97.8 99.7\nCLIP4STR-L [62] WIT-400M 99.1 98.6 – 92.6 – 98.1 99.7\nCLIP4STR-B DataComp-1B 99.5 98.3 98.6 91.4 91.1 98.0 99.0\nCLIP4STR-L DataComp-1B 99.6 98.6 99.0 91.9 91.4 98.1 99.7\nCLIP4STR-H DFN-5B 99.5 99.1 98.9 91.7 91.0 98.0 99.0\nyi and y, they are employed to update the context c during\nthe refinement process (lines 15 and 18). Notably, while the\ndecoder previously produced yi and y in an autoregressive\nmanner, a different approach is adopted in lines 14–20, where\na cloze mask is utilized. This entails providing information\nabout other characters in the word as context when predicting\na certain character. For further insights into the workings of\nautoregressive and cloze masks, please refer to Table I.\nAPPENDIX B\nDISCUSSION WITH AUTOREGRESSIVE PRE-TRAINING\nAnother pre-training task for STR is autoregressive lan-\nguage modeling, such as TrOCR [12] and DTrOCR [55].\nThese models take the image as input and are optimized\nby predicting the next tokens based on the previous context\nduring pre-training, similar to the GPT language models [109].\nTable XI presents a comparison between CLIP4STR and\nautoregressive pre-training methods. DTrOCR, pre-trained on\n6B textlines, surpasses CLIP4STR on IC13, IC15, and SVTP,\ndemonstrating the effectiveness of large-scale autoregressive\npre-training. However, the difference between performance\non these three benchmarks is trivial, and CLIP4STR per-\nforms better on III5K, SVT, and CUTE. Overall, CLIP4STR\nand DTrOCR are two comparable methods. In such a case,\nCLIP4STR has two additional merits to be a more practical\nSTR method: 1) Numerous large-scale pre-trained VLMs are\npublicly available, eliminating the cost of pre-training for\nCLIP4STR. Additionally, the cost of transferring CLIP into\na STR reader is affordable, as shown in Table II. In contrast,\nthe cost of pre-training DTrOCR on 6 billion textlines is pro-\nhibitive. 2) CLIP4STR is open-sourced and easy to reproduce,\nwhile DTrOCR is closed-sourced. Moreover, CLIP4STR offers\na thorough empirical study on adapting CLIP to STR, which\nis valuable for subsequent STR methods based on VLMs.\nACKNOWLEDGMENTS\nThank Chao Liang for maintaining the code at https:\n//github.com/VamosC/CLIP4STR. The unique iden-\ntifier [110] for papers of Shuai is quickstep drudge con-\nsent wackiness mangle unspoiled childish exploring antennae\nagony embassy starved.\nREFERENCES\n[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable\nvisual models from natural language supervision,” in ICML, 2021, pp.\n8748–8763.\n[2] C. Jia, Y . Yang, Y . Xia, Y . Chen, Z. Parekh, H. Pham, Q. V . Le,\nY . Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language\nrepresentation learning with noisy text supervision,” in ICML, 2021,\npp. 4904–4916.\n[3] H. Song, L. Dong, W. Zhang, T. Liu, and F. Wei, “CLIP models are\nfew-shot learners: Empirical studies on VQA and visual entailment,”\nin ACL, 2022, pp. 6088–6100.\n[4] H. Luo, L. Ji, M. Zhong, Y . Chen, W. Lei, N. Duan, and T. Li,\n“Clip4clip: An empirical study of clip for end to end video clip retrieval\nand captioning,” Neurocomputing, vol. 508, pp. 293–304, 2022.\n[5] X. Yang, L. Zhu, X. Wang, and Y . Yang, “Dgl: Dynamic global-local\nprompt tuning for text-video retrieval,” in AAAI, vol. 38, no. 7, 2024,\npp. 6540–6548.\n[6] S. Subramanian, W. Merrill, T. Darrell, M. Gardner, S. Singh, and\nA. Rohrbach, “Reclip: A strong zero-shot baseline for referring ex-\npression comprehension,” in ACL, 2022, pp. 5198–5215.\n[7] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y . Choi, “Clipscore:\nA reference-free evaluation metric for image captioning,” in EMNLP,\n2021, pp. 7514–7528.\n[8] N. Fei, Z. Lu, Y . Gao, G. Yang, Y . Huo, J. Wen, H. Lu, R. Song,\nX. Gao, T. Xiang et al. , “Towards artificial general intelligence via a\nmultimodal foundation model,” Nature Communications, vol. 13, no. 1,\np. 3094, 2022.\n[9] S. Fang, H. Xie, Y . Wang, Z. Mao, and Y . Zhang, “Read like humans:\nAutonomous, bidirectional and iterative language modeling for scene\ntext recognition,” in CVPR, 2021, pp. 7098–7107.\n[10] R. Atienza, “Vision transformer for fast and efficient scene text\nrecognition,” in ICDAR, 2021, pp. 319–334.\n[11] M. Yang, M. Liao, P. Lu, J. Wang, S. Zhu, H. Luo, Q. Tian, and X. Bai,\n“Reading and writing: Discriminative and generative modeling for self-\nsupervised text recognition,” in ACM MM, 2022, pp. 4214–4223.\n[12] M. Li, T. Lv, J. Chen, L. Cui, Y . Lu, D. Florencio, C. Zhang, Z. Li, and\nF. Wei, “Trocr: Transformer-based optical character recognition with\npre-trained models,” in AAAI, vol. 37, no. 11, 2023, pp. 13 094–13 102.\n[13] S. Long, X. He, and C. Yao, “Scene text detection and recognition:\nThe deep learning era,” IJCV, vol. 129, no. 1, pp. 161–184, 2021.\n[14] Z. Raisi and J. Zelek, “Occluded text detection and recognition in the\nwild,” in CRV, 2022, pp. 140–150.\n[15] C. K. Chng, Y . Liu, Y . Sun, C. C. Ng, C. Luo, Z. Ni, C. Fang, S. Zhang,\nJ. Han, E. Ding et al., “Icdar2019 robust reading challenge on arbitrary-\nshaped text-rrc-art,” in ICDAR, 2019, pp. 1571–1576.\n[16] Y . Zhang, L. Gueguen, I. Zharkov, P. Zhang, K. Seifert, and B. Kadlec,\n“Uber-text: A large-scale dataset for optical character recognition from\nstreet-level imagery,” inSUNw: Scene Understanding Workshop-CVPR,\nvol. 2017, 2017, p. 5.\n[17] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features\nfrom tiny images,” Technical Report, 2009.\n[18] S. Fort, March 2021. [Online]. Available: https://stanislavfort.github.\nio/2021/03/05/OpenAI CLIP stickers and adversarial examples.html\n[19] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, “Grad-cam: visual explanations from deep networks via\ngradient-based localization,” IJCV, vol. 128, pp. 336–359, 2020.\n[20] G. Goh, N. Cammarata, C. V oss, S. Carter, M. Petrov, L. Schubert,\nA. Radford, and C. Olah, “Multimodal neurons in artificial neural\nnetworks,” Distill, vol. 6, no. 3, p. e30, 2021.\n[21] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Bigorda, S. R.\nMestre, J. Mas, D. F. Mota, J. A. Almazan, and L. P. De Las Heras,\n“Icdar 2013 robust reading competition,” in ICDAR, 2013, pp. 1484–\n1493.\n[22] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. K. Ghosh, A. D.\nBagdanov, M. Iwamura, J. Matas, L. Neumann, V . R. Chandrasekhar,\nS. Lu, F. Shafait, S. Uchida, and E. Valveny, “ICDAR 2015 competition\non robust reading,” in ICDAR, 2015, pp. 1156–1160.\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nNeurIPS, 2017, p. 5998–6008.\n[24] D. Bautista and R. Atienza, “Scene text recognition with permuted\nautoregressive sequence models,” in ECCV, 2022, pp. 178–196.\n[25] L. Yuan, D. Chen, Y .-L. Chen, N. Codella, X. Dai, J. Gao, H. Hu,\nX. Huang, B. Li, C. Li et al., “Florence: A new foundation model for\ncomputer vision,” arXiv preprint arXiv:2111.11432 , 2021.\n[26] S. Zhao, L. Zhu, X. Wang, and Y . Yang, “Centerclip: Token clustering\nfor efficient text-video retrieval,” in SIGIR, 2022, pp. 970–981.\n[27] X. Wang, L. Zhu, Z. Zheng, M. Xu, and Y . Yang, “Align and tell:\nBoosting text-video retrieval with local alignment and fine-grained\nsupervision,” T-MM, vol. 25, pp. 6079–6089, 2022.\n11\n[28] S. Zhao, X. Wang, L. Zhu, and Y . Yang, “Test-time adaptation with\nCLIP reward for zero-shot generalization in vision-language models,”\nin ICLR, 2024.\n[29] J. Cho, S. Yoon, A. Kale, F. Dernoncourt, T. Bui, and M. Bansal,\n“Fine-grained image captioning with clip reward,” arXiv preprint\narXiv:2205.13115, 2022.\n[30] H. Zhang, W. Yin, Y . Fang, L. Li, B. Duan, Z. Wu, Y . Sun,\nH. Tian, H. Wu, and H. Wang, “Ernie-vilg: Unified generative pre-\ntraining for bidirectional vision-language generation,” arXiv preprint\narXiv:2112.15283, 2021.\n[31] J. Yu, Z. Wang, V . Vasudevan, L. Yeung, M. Seyedhosseini, and\nY . Wu, “Coca: Contrastive captioners are image-text foundation mod-\nels.” TMLR, vol. 2022, 2022.\n[32] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou,\nand H. Yang, “OFA: unifying architectures, tasks, and modalities\nthrough a simple sequence-to-sequence learning framework,” in ICML,\n2022, pp. 23 318–23 340.\n[33] Y . Li, F. Liang, L. Zhao, Y . Cui, W. Ouyang, J. Shao, F. Yu, and J. Yan,\n“Supervision exists everywhere: A data efficient contrastive language-\nimage pre-training paradigm,” arXiv preprint arXiv:2110.05208, 2021.\n[34] L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li,\nX. Jiang, and C. Xu, “FILIP: fine-grained interactive language-image\npre-training,” in ICLR. OpenReview.net, 2022.\n[35] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi,\n“Align before fuse: Vision and language representation learning with\nmomentum distillation,” NeurIPS, vol. 34, pp. 9694–9705, 2021.\n[36] M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim, “Coyo-700m:\nImage-text pair dataset,” https://github.com/kakaobrain/coyo-dataset,\n2022.\n[37] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman,\nM. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al. ,\n“Laion-5b: An open large-scale dataset for training next generation\nimage-text models,” NeurIPS, vol. 35, pp. 25 278–25 294, 2022.\n[38] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini,\nR. Taori, A. Dave, V . Shankar, H. Namkoong, J. Miller, H. Hajishirzi,\nA. Farhadi, and L. Schmidt, “Openclip,” Jul. 2021.\n[39] S. Shen, L. H. Li, H. Tan, M. Bansal, A. Rohrbach, K. Chang, Z. Yao,\nand K. Keutzer, “How much can CLIP benefit vision-and-language\ntasks?” in ICLR. OpenReview.net, 2022.\n[40] A. Graves, S. Fern ´andez, F. J. Gomez, and J. Schmidhuber, “Con-\nnectionist temporal classification: labelling unsegmented sequence data\nwith recurrent neural networks,” in ICML, 2006, pp. 369–376.\n[41] P. He, W. Huang, Y . Qiao, C. C. Loy, and X. Tang, “Reading scene\ntext in deep convolutional sequences,” in AAAI, vol. 30, no. 1, 2016.\n[42] B. Shi, X. Bai, and C. Yao, “An end-to-end trainable neural network\nfor image-based sequence recognition and its application to scene text\nrecognition,” T-PAMI, vol. 39, no. 11, pp. 2298–2304, 2016.\n[43] F. Borisyuk, A. Gordo, and V . Sivakumar, “Rosetta: Large scale system\nfor text detection and recognition in images,” in ACM SIGKDD, 2018,\npp. 71–79.\n[44] M. Liao, J. Zhang, Z. Wan, F. Xie, J. Liang, P. Lyu, C. Yao, and X. Bai,\n“Scene text recognition from two-dimensional perspective,” in AAAI,\nvol. 33, no. 01, 2019, pp. 8714–8721.\n[45] Z. Wan, M. He, H. Chen, X. Bai, and C. Yao, “Textscanner: Reading\ncharacters in order for robust scene text recognition,” in AAAI, vol. 34,\nno. 07, 2020, pp. 12 120–12 127.\n[46] L. Zhao, Z. Wu, X. Wu, G. Wilsbacher, and S. Wang, “Background-\ninsensitive scene text recognition with text semantic segmentation,” in\nECCV, 2022, pp. 163–182.\n[47] Z. Cheng, F. Bai, Y . Xu, G. Zheng, S. Pu, and S. Zhou, “Focusing\nattention: Towards accurate text recognition in natural images,” in\nICCV, 2017, pp. 5076–5084.\n[48] B. Shi, M. Yang, X. Wang, P. Lyu, C. Yao, and X. Bai, “ASTER: an\nattentional scene text recognizer with flexible rectification,” T-PAMI,\nvol. 41, no. 9, pp. 2035–2048, 2019.\n[49] C. Da, P. Wang, and C. Yao, “Levenshtein OCR,” in ECCV, 2022, pp.\n322–338.\n[50] B. Na, Y . Kim, and S. Park, “Multi-modal text recognition networks:\nInteractive enhancements between visual and semantic features,” in\nECCV, 2022, pp. 446–463.\n[51] C. Lee and S. Osindero, “Recursive recurrent nets with attention\nmodeling for OCR in the wild,” in CVPR, 2016, pp. 2231–2239.\n[52] Y . Gao, Y . Chen, J. Wang, and H. Lu, “Semi-supervised scene text\nrecognition,” T-IP, vol. 30, pp. 3005–3016, 2021.\n[53] P. Dai, H. Zhang, and X. Cao, “Sloan: Scale-adaptive orientation\nattention network for scene text recognition,” T-IP, vol. 30, pp. 1687–\n1701, 2020.\n[54] F. Sheng, Z. Chen, and B. Xu, “Nrtr: A no-recurrence sequence-to-\nsequence model for scene text recognition,” in ICDAR, 2019, pp. 781–\n786.\n[55] M. Fujitake, “Dtrocr: Decoder-only transformer for optical character\nrecognition,” in WACV, 2024, pp. 8025–8035.\n[56] J. Zhang, T. Lin, Y . Xu, K. Chen, and R. Zhang, “Relational contrastive\nlearning for scene text recognition,” inACM MM, 2023, pp. 5764–5775.\n[57] M. V . Ty and R. Atienza, “Scene text recognition models explainability\nusing local features,” in ICIP, 2023, pp. 645–649.\n[58] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman, “Synthetic\ndata and artificial neural networks for natural scene text recognition,”\narXiv preprint arXiv:1406.2227 , 2014.\n[59] A. Gupta, A. Vedaldi, and A. Zisserman, “Synthetic data for text\nlocalisation in natural images,” in CVPR, 2016, pp. 2315–2324.\n[60] J. Baek, Y . Matsui, and K. Aizawa, “What if we only use real datasets\nfor scene text recognition? toward scene text recognition with fewer\nlabels,” in CVPR, 2021, pp. 3113–3122.\n[61] Q. Jiang, J. Wang, D. Peng, C. Liu, and L. Jin, “Revisiting scene text\nrecognition: A data perspective,” in ICCV, 2023, pp. 20 543–20 554.\n[62] M. Rang, Z. Bi, C. Liu, Y . Wang, and K. Han, “An empirical study of\nscaling law for ocr,” in CVPR, 2024.\n[63] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[64] W. Kim, B. Son, and I. Kim, “Vilt: Vision-and-language transformer\nwithout convolution or region supervision,” in ICML, 2021, pp. 5583–\n5594.\n[65] A. Aberdam, D. Bensa ¨ıd, A. Golts, R. Ganz, O. Nuriel, R. Tichauer,\nS. Mazor, and R. Litman, “Clipter: Looking at the bigger picture in\nscene text recognition,” in ICCV, 2023, pp. 21 706–21 717.\n[66] Z. Wang, H. Xie, Y . Wang, J. Xu, B. Zhang, and Y . Zhang, “Symmet-\nrical linguistic feature distillation with clip for scene text recognition,”\nin ACM MM, 2023, pp. 509–518.\n[67] J. Lei Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” ArXiv\ne-prints, pp. arXiv–1607, 2016.\n[68] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: a simple way to prevent neural networks from\noverfitting,” JMLR, vol. 15, no. 1, pp. 1929–1958, 2014.\n[69] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” in ICLR. OpenReview.net,\n2021.\n[70] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training\nof deep bidirectional transformers for language understanding,” in\nNAACL-HLT, 2019.\n[71] R. Sennrich, “Neural machine translation of rare words with subword\nunits,” arXiv preprint arXiv:1508.07909 , 2015.\n[72] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson,\nK. Lenc, A. Mensch, K. Millican, M. Reynolds et al. , “Flamingo: a\nvisual language model for few-shot learning,” NeurIPS, vol. 35, pp.\n23 716–23 736, 2022.\n[73] P. Goyal, “Accurate, large minibatch sg d: training imagenet in 1 hour,”\narXiv preprint arXiv:1706.02677 , 2017.\n[74] A. Mishra, K. Alahari, and C. V . Jawahar, “Scene text recognition using\nhigher order language priors,” in BMVC, 2012.\n[75] A. Risnumawan, P. Shivakumara, C. S. Chan, and C. L. Tan, “A robust\narbitrary text detection system for natural scene images,” Expert Syst.\nAppl., vol. 41, no. 18, pp. 8027–8048, 2014.\n[76] K. Wang, B. Babenko, and S. J. Belongie, “End-to-end scene text\nrecognition,” in ICCV, 2011, pp. 1457–1464.\n[77] T. Q. Phan, P. Shivakumara, S. Tian, and C. L. Tan, “Recognizing\ntext with perspective distortion in natural scenes,” in ICCV, 2013, pp.\n569–576.\n[78] Y . Wang, H. Xie, S. Fang, J. Wang, S. Zhu, and Y . Zhang, “From two\nto one: A new scene text recognizer with visual language modeling\nnetwork,” in ICCV, 2021, pp. 14 194–14 203.\n[79] A. Veit, T. Matera, L. Neumann, J. Matas, and S. Belongie, “Coco-text:\nDataset and benchmark for text detection and recognition in natural\nimages,” arXiv preprint arXiv:1601.07140 , 2016.\n[80] B. Shi, C. Yao, M. Liao, M. Yang, P. Xu, L. Cui, S. Belongie, S. Lu,\nand X. Bai, “Icdar2017 competition on reading chinese text in the wild\n(rctw-17),” in ICDAR, vol. 1, 2017, pp. 1429–1434.\n[81] Y . Sun, Z. Ni, C.-K. Chng, Y . Liu, C. Luo, C. C. Ng, J. Han, E. Ding,\nJ. Liu, D. Karatzas et al., “Icdar 2019 competition on large-scale street\nview text with partial labeling-rrc-lsvt,” in ICDAR, 2019, pp. 1557–\n1562.\n12\n[82] N. Nayef, Y . Patel, M. Busta, P. N. Chowdhury, D. Karatzas, W. Khlif,\nJ. Matas, U. Pal, J.-C. Burie, C.-l. Liu et al., “Icdar2019 robust reading\nchallenge on multi-lingual scene text detection and recognition—rrc-\nmlt-2019,” in ICDAR, 2019, pp. 1582–1587.\n[83] R. Zhang, Y . Zhou, Q. Jiang, Q. Song, N. Li, K. Zhou, L. Wang,\nD. Wang, M. Liao, M. Yanget al., “Icdar 2019 robust reading challenge\non reading chinese text on signboard,” in ICDAR, 2019, pp. 1577–1581.\n[84] A. Singh, G. Pang, M. Toh, J. Huang, W. Galuba, and T. Hassner, “Tex-\ntocr: Towards large-scale end-to-end reasoning for arbitrary-shaped\nscene text,” in CVPR, 2021, pp. 8802–8812.\n[85] I. Krasin, T. Duerig, N. Alldrin, V . Ferrari, S. Abu-El-Haija,\nA. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit, S. Be-\nlongie, V . Gomes, A. Gupta, C. Sun, G. Chechik, D. Cai, Z. Feng,\nD. Narayanan, and K. Murphy, “Openimages: A public dataset for\nlarge-scale multi-label and multi-class image classification.” Dataset\navailable from https://github.com/openimages, 2017.\n[86] I. Krylov, S. Nosov, and V . Sovrasov, “Open images v5 text annotation\nand yet another mask text spotter,” in ACML, 2021, pp. 379–389.\n[87] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\nin ICLR, 2019.\n[88] P. Micikevicius, S. Narang, J. Alben, G. F. Diamos, E. Elsen, D. Garc´ıa,\nB. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu,\n“Mixed precision training,” in ICLR, 2018.\n[89] D. Yu, X. Li, C. Zhang, T. Liu, J. Han, J. Liu, and E. Ding, “Towards\naccurate scene text recognition with semantic reasoning networks,” in\nCVPR, 2020, pp. 12 113–12 122.\n[90] M. Cui, W. Wang, J. Zhang, and L. Wang, “Representation and correla-\ntion enhanced encoder-decoder framework for scene text recognition,”\nin ICDAR, 2021, pp. 156–170.\n[91] Y . Wang, H. Xie, S. Fang, M. Xing, J. Wang, S. Zhu, and Y . Zhang,\n“Petr: Rethinking the capability of transformer-based language model\nin scene text recognition,” T-IP, vol. 31, pp. 5585–5598, 2022.\n[92] T. Guan, C. Gu, J. Tu, X. Yang, Q. Feng, Y . Zhao, and W. Shen, “Self-\nsupervised implicit glyph attention for text recognition,” in CVPR,\n2023, pp. 15 285–15 294.\n[93] C. Cheng, P. Wang, C. Da, Q. Zheng, and C. Yao, “Lister: Neighbor\ndecoding for length-insensitive scene text recognition,” in ICCV, 2023,\npp. 19 541–19 551.\n[94] T. Guan, W. Shen, X. Yang, Q. Feng, Z. Jiang, and X. Yang, “Self-\nsupervised character-to-character distillation for text recognition,” in\nICCV, 2023, pp. 19 473–19 484.\n[95] S. Y . Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen,\nR. Marten, M. Wortsman, D. Ghosh, J. Zhang et al. , “Datacomp: In\nsearch of the next generation of multimodal datasets,” NeurIPS, vol. 36,\n2024.\n[96] A. Fang, A. M. Jose, A. Jain, L. Schmidt, A. Toshev, and V . Shankar,\n“Data filtering networks,” arXiv preprint arXiv:2309.17425 , 2023.\n[97] E. D. Cubuk, B. Zoph, J. Shlens, and Q. Le, “Randaugment: Practical\nautomated data augmentation with a reduced search space,” inNeurIPS,\n2020, pp. 702–703.\n[98] X. Yang, Z. Qiao, J. Wei, D. Yang, and Y . Zhou, “Masked and\npermuted implicit context learning for scene text recognition,” IEEE\nSignal Processing Letters , vol. 31, pp. 964–968, 2023.\n[99] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang,\nM. Elibol, Z. Yang, W. Paul, M. I. Jordan et al. , “Ray: A distributed\nframework for emerging {AI} applications,” in OSDI, 2018, pp. 561–\n577.\n[100] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and\nL. Fei-Fei, “Imagenet large scale visual recognition challenge,” IJCV,\npp. 211–252, 2015.\n[101] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efficient image transformers & distillation\nthrough attention,” in ICML, 2021, pp. 10 347–10 357.\n[102] T. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor, “Imagenet-\n21k pretraining for the masses,” NeurIPS, 2021. [Online]. Available:\nhttps://github.com/Alibaba-MIIL/ImageNet21K\n[103] H. Bao, L. Dong, S. Piao, and F. Wei, “Beit: BERT pre-training of\nimage transformers,” in ICLR, 2022.\n[104] Y . Liu, “Roberta: A robustly optimized bert pretraining approach,”\narXiv preprint arXiv:1907.11692 , 2019.\n[105] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Learning to prompt for\nvision-language models,” IJCV, vol. 130, no. 9, pp. 2337–2348, 2022.\n[106] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y . Zhang, H. Li, and\nY . Qiao, “Clip-adapter: Better vision-language models with feature\nadapters,” IJCV, vol. 132, no. 2, pp. 581–595, 2024.\n[107] Y .-L. Sung, J. Cho, and M. Bansal, “Lst: Ladder side-tuning for\nparameter and memory efficient transfer learning,” NeurIPS, vol. 35,\npp. 12 991–13 005, 2022.\n[108] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, “Pruning\nfilters for efficient convnets,” in ICLR. OpenReview.net, 2017.\n[109] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , “Language\nmodels are few-shot learners,” NeurIPS, vol. 33, pp. 1877–1901, 2020.\n[110] S. Zhao, L. Zhu, R. Quan, and Y . Yang, “Protecting copyrighted\nmaterial with unique identifiers in large language model training,”\n2024. [Online]. Available: https://arxiv.org/abs/2403.15740\nShuai Zhao received the MEng degree in computer\nscience from Zhejiang University, Hangzhou, China,\nin 2020, and the BEng degree in Automation from\nHuazhong University of Science & Technology,\nWuhan, China, in 2017. He is currently pursuing a\nPhD degree in computer science at the University of\nTechnology Sydney, Sydney, Australia. His research\ninterests include computer vision and machine learn-\ning.\nRuijie Quan received the PhD degree from the Uni-\nversity of Technology Sydney (UTS), Sydney, Aus-\ntralia, in 2022. He is currently a postdoc researcher\nwith Zhejiang University, China. His research in-\nterests include deep learning and its applications to\ncomputer vision.\nLinchao Zhu received the BE degree from Zhejiang\nUniversity, China, in 2015, and the PhD degree in\ncomputer science from the University of Technology\nSydney, Australia, in 2019. He is currently a ZJU100\nYoung Professor with the College of Computer Sci-\nence, Zhejiang University. His research interests are\nvideo analysis, physics-informed neural networks,\nand large language models.\nYi Yang (Senior Member, IEEE) received the PhD\ndegree from Zhejiang University, Hangzhou, China,\nin 2010. He is currently a professor with Zhejiang\nUniversity. He was a professor with the Univer-\nsity of Technology Sydney. He was a post-doctoral\nresearcher at the School of Computer Science at\nCarnegie Mellon University. His current research in-\nterests include machine learning and its applications\nto multimedia content analysis and computer vision,\nsuch as multimedia retrieval and video content un-\nderstanding.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7806510925292969
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6327201724052429
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.609216570854187
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5277436375617981
    },
    {
      "name": "Encoder",
      "score": 0.5147781372070312
    },
    {
      "name": "Inference",
      "score": 0.5120442509651184
    },
    {
      "name": "Modal",
      "score": 0.5030519366264343
    },
    {
      "name": "Baseline (sea)",
      "score": 0.45529675483703613
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.41720470786094666
    },
    {
      "name": "Computer vision",
      "score": 0.38685935735702515
    },
    {
      "name": "Natural language processing",
      "score": 0.3785131573677063
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37290406227111816
    },
    {
      "name": "Speech recognition",
      "score": 0.3402310311794281
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    }
  ],
  "cited_by": 6
}