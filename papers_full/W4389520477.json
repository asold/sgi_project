{
  "title": "On the Calibration of Large Language Models and Alignment",
  "url": "https://openalex.org/W4389520477",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5113053510",
      "name": "Chiwei Zhu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A3035318654",
      "name": "Ben-Feng Xu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2029296201",
      "name": "Quan Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2152909678",
      "name": "Yongdong Zhang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2154925013",
      "name": "Zhendong Mao",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4313451803",
    "https://openalex.org/W4366736258",
    "https://openalex.org/W4385570738",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4378465439",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4288091035",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4378473736",
    "https://openalex.org/W2963266575",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4378945695",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W4288482469",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4389518686",
    "https://openalex.org/W585236412",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389519585",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4361188845",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4300772090",
    "https://openalex.org/W2531327146",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W2047634553",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4321594373",
    "https://openalex.org/W2933254221",
    "https://openalex.org/W3100452485",
    "https://openalex.org/W1826524928",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W4378509427",
    "https://openalex.org/W2254249950",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W1599263113",
    "https://openalex.org/W4285298351",
    "https://openalex.org/W4287252513",
    "https://openalex.org/W3199958362",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2047635155"
  ],
  "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9778–9795\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nOn the Calibration of Large Language Models and Alignment\nChiwei Zhu1, Benfeng Xu1∗, Quan Wang2, Yongdong Zhang1, Zhendong Mao1\n1University of Science and Technology of China, Hefei, China\n2MOE Key Laboratory of Trustworthy Distributed Computing and Service,\nBeijing University of Posts and Telecommunications, Beijing, China\ntanz@mail.ustc.edu.cn, benfeng@mail.ustc.edu.cn\nwangquan@bupt.edu.cn, zhyd73@ustc.edu.cn, zdmao@ustc.edu.cn\nAbstract\nAs large language models attract increasing\nattention and find widespread application, con-\ncurrent challenges of reliability also arise at\nthe same time. Confidence calibration, an ef-\nfective analysis method for gauging the relia-\nbility of deep models, serves as a crucial tool\nfor assessing and improving their reliability.\nHowever, such investigation has been compara-\ntively underexplored. In this work, we conduct\na systematic examination of the calibration of\naligned language models throughout the entire\nconstruction process, including pretraining and\nalignment training. At each stage, we inves-\ntigate how different training settings, such as\nparameter scales and training data, affect model\ncalibration. To thoroughly assess model cali-\nbration, we evaluate models on three most con-\ncerned aspects: generation, factuality and un-\nderstanding. Our work sheds light on whether\npopular LLMs are well-calibrated and how the\ntraining process influences model calibration.\n1 Introduction\nLarge Language Models (LLMs) like GPT-\n3 (Brown et al., 2020), PaLM (Chowdhery\net al., 2022), and GPT4 (OpenAI, 2023), fol-\nlowed by many open-source replications including\nLLaMA (Touvron et al., 2023a), Pythia (Biderman\net al., 2023) are revolutionizing the paradigm and\nre-shaping the expectation of modern natural lan-\nguage processing. When further trained with align-\nment treatment (Ouyang et al., 2022; Bai et al.,\n2022), these LLMs further exhibit impressive ca-\npability in responding to generalized human in-\nstructions, which implies their potential as general-\npurpose intelligent assistants and this has since at-\ntract considerable attention in the field and around\nthe world.\nAs LLMs find more diverse applications and ex-\nert widespread influence, it becomes increasingly\n∗Corresponding author: Benfeng Xu.\nPretrainingModelParametersTrainingDynamics\nAlignmentTrainingData TrainingDynamicsTrainingAlgorithms\nLanguageModeling\nFactuality\nLanguageUnderstanding\nFigure 1: Scope of investigations in this paper.\nimperative to ensure their reliability and faithful-\nness, particularly in fields such as healthcare (Kung\net al., 2023) and law (Huang et al., 2023). These are\ndomains where inaccurate predictions can lead to\nsignificant, potentially severe challenges. However,\ndue to the intrinsic autoregressive mechanism and\ncomplex system structures, the behaviours of these\nmodels can not be easily attributed or interpreted.\nConfidence calibration is an effective method to\nestimate a model’s awareness of its uncertainty, and\nit helps enhance our understanding and assurance\nof the trustworthiness of deep models. Generally,\nit associates model output confidence, i.e. probabil-\nity, with ground truth correctness likelihood (Guo\net al., 2017) and informs the user to what extent\nthe outputs should be trusted, even though they\nmay not always be correct. Intuitively, for ex-\nample, given 100 predictions in a classification\ntask which are produced by a classifier and each of\nthem is assigned 0.8 confidence, we expect 80 of\nthem to be correctly classified for a well-calibrated\nclassifier. As a consequence, better calibration of\nLLMs could significantly extend their usability. In\nearly meteorology, calibration was noted as valid-\nity (Miller, 1962) or reliability (Murphy, 1973),\nindicating the trustworthiness of forecasters. Well\ncalibrated probabilities can provide extra informa-\ntion for users to decide whether to trust the model’s\nprediction, particularly for modern neural networks\nwhose decisions are harder to interpret (Guo et al.,\n2017). Studies have also pointed out that calibra-\ntion is helpful to reduce hallucination in language\n9778\nmodels (Xiao and Wang, 2021; Tian et al., 2020).\nPrevious works have shown that pre-trained lan-\nguage models can generate well-calibrated predic-\ntions (Desai and Durrett, 2020; Kadavath et al.,\n2022). However, these works mainly concentrate\non vanilla language models, while the aligned lan-\nguage models receive less focus. A newly proposed\nwork evaluates calibration of some aligned models\nby prompting them to verbalize confidence in the\ntoken space (Tian et al., 2023), but it mainly studies\nblack-box models, whose training process is not\navailable, and thus can not provide insight into how\nmodel calibration is affected by different factors\nin the alignment training process. To conclude, a\nsystematical study on the calibration of aligned lan-\nguage models is still missing, and our work aims\nto fill this gap.\nIn this work, we study the calibration of aligned\nlanguage models in the entire building cycle and\nprovide evidence on how to achieve decent model\ncalibration. An overview of the scheme of out study\nis at Figure 1. Following the training process of\naligned language models, we study model calibra-\ntion in pre-training stage and alignment training\nstage respectively. In each stage, we reveal how\nmodel calibration changes when using different\ntraining settings. For pre-training stage, we ex-\namine the effect of parameter scale and training\ndynamics (steps). For alignment training stage, we\nstudy the effect of instruction tuning and RLHF,\nin which instruction tuning is further scrutinized\nby changing instruction datasets, training methods\nand also training dynamics.\nBesides the understanding and generating abil-\nity, factual faithfulness and reasoning capability\nare two widely considered issues with large lan-\nguage models (Du et al., 2023). We also follow this\npath to study models’ calibration when applied to\ndifferent tasks. For this purpose, we design three\ntasks for each of the stages above. (1) To evalu-\nate model calibration on common text generation,\nwe use Causal Language Modeling (CLM) task,\nwhich is also the objective of pre-training stage.\n(2) To study model calibration on factuality, we de-\nsigned a facts generation task where the models are\nasked to generate fact-related content. (3) To study\nmodel calibration on reasoning, we use multi-task\nlanguage understanding task, where questions and\npossible options are provided and models are asked\nto select the most probable one.\nThrough extensive experiments and analysis, we\narrive at the following findings.\nFor pretraining of LLMs:\n• Larger Parameter Scales\n/MVRightArrow: Improve mod-\nels’ calibration.\n• Longer Training Dynamics\n/MVRightArrow: Also benefit\ncalibration accuracy.\nFor alignment of LLMs:\n• Instruction Tuning\n/MVRightArrow: Deteriorates models’\ncalibration.\n• Synthetic Data\n/MVRightArrow: Exacerbates the harmful\neffect of instruction tuning.\n• Parameter-efficient Fine-tuning /MVRightArrow: Effec-\ntive regularization for restraining calibration\nerror.\n• RLHF /MVRightArrow: Help maintaining calibration ac-\ncuracy.\nFor different tasks:\n• In pre-training: Improvement in calibration\naccuracy is more significant on fact genera-\ntion task or language understanding tasks\nthan language modeling task.\n• In alignment training: Calibration accuracy\nevolves consistently across different down-\nstream tasks including fact generation, lan-\nguage understanding or vanilla language mod-\neling.\nWe believe these conclusions as well as detailed\nexperiments can take us a step further towards un-\nderstanding large language models, especially the\nintrinsic mechanism of their calibration behaviour.\nOur experimental results also provide us with some\npossible solutions to improve calibration, includ-\ning increasing model scales and employing param-\neter efficient tuning methods. Besides, diversity\nguided instruction data construction may also be\nvery promising. Hopefully these findings can shed\nlight on future works to construct more factual and\ntrustworthy assistants.\n2 Related Work\nAligned Large Language Models are large lan-\nguage models that are specially trained to follow\nhuman’s intents or instructions. Large language\n9779\nmodels are proved to have the ability of complet-\ning some downstream tasks without any gradient\nupdating (Brown et al., 2020). To better make use\nof such ability, many researches have found that\ninstruction following models can be constructed by\nfine-tuning models with instruction-response pairs,\nwhich is called instruction tuning (Weller et al.,\n2020; Mishra et al., 2022; Sanh et al., 2022; Wei\net al., 2022a; Xu et al., 2023b). While these models\ncan understand human instructions and make rea-\nsonable responses, they often produce unexpected\nresults like lies, made-up facts, biased or toxic texts\nand so on. To better align models with human in-\ntents, reinforcement learning with human feedback\nis introduced to the training of large language mod-\nels (Ouyang et al., 2022). Though instruction tun-\ning and RLHF can significantly improve the mod-\nels’ ability of interacting with humans, how they\ninfluence the calibration of large language models\nhave not been researched on.\nConfidence calibration is a concerned prob-\nlem for classification models. A large amount\nof works have studied the calibration of statisti-\ncal machine learning systems and the methods to\nimprove their calibration (DeGroot and Fienberg,\n1983; Palmer et al., 2008; Yang and Thompson,\n2010). Later, calibration of neural networks have\nalso been researched on (Hendrycks and Gimpel,\n2016; Nguyen and O’Connor, 2015; Nixon et al.,\n2019; Minderer et al., 2021). Guo et al. (2017)\npoints out that modern neural networks are not as\ncalibrated as their ancestors and proposes a temper-\nature scaling methods to calibrate neural networks.\nIn natural language processing field, calibration of\ntransformer-based language models are evaluated\namong different tasks, including machine trans-\nlation (Kumar and Sarawagi, 2019), QA (Jiang\net al., 2021) and selective prediction (Varshney\net al., 2022). Recently, large-scale generative lan-\nguage models are receiving growing attention, and\nsome works have examined calibration of these\nmodels (Srivastava et al., 2022; Kuhn et al., 2023;\nTian et al., 2023). There are also works improving\ncalibration of large language models, for example,\nXu et al. (2023a) propose kNN Prompting to effec-\ntively mitigate calibration errors in in-context learn-\ning. However, as mentioned before, these works\neither concentrate on vanilla language models or\nstudy black-box models. We study models calibra-\ntion in their whole life cycles from pre-training to\nalignment training, where our main contributions\nFigure 2: Reliability diagram for a Pythia-70m model.\nlie in.\nAnalysis of large language models. Understand-\ning various aspects of LLMs through theoretical\nor empirical approaches have long been an impor-\ntant interests for NLP scholars. Many works have\ndemonstrated the scaling law of LLMs in different\nscenarios w.r.t. model scales, data size and com-\nputational costs (Kaplan et al., 2020; Rae et al.,\n2022; Xia et al., 2023). Wei et al. (2022b) defines\nand reveals the emergent abilities of large language\nmodels. Liang et al. (2022) proposes a holistic eval-\nuation framework named HELM to analyze large\nlanguage models on their capabilities, limitations,\nand potential risks. Hallucination and factuality\nalso draw a lot of attention (McKenna et al., 2023;\nZheng et al., 2023), but they do not take a further\nstep towards the intrinsic mechanism while merely\nexplore the verification on the surface. Differently,\nthis paper provides a formal and systematical anal-\nysis on calibration behaviour of LLMs and their\nalignment treatment.\n3 Definitions\nIn this section we formally define some basic con-\ncepts in our work, including confidence calibration,\nreliability diagram and expected calibration error.\nConfidence calibration is the main objective we are\nstudying in this work and the other two are tools\nwe use to evaluate model calibration. Our selection\nof tools follows previous work (Guo et al., 2017).\nConfidence Calibration. Given a supervised\nmulti-class classification scenario, where we have\ninput x, label y ∈Y = {1, 2...K}, model predic-\ntion y′∈Y and confidence p′∈[0, 1]. A model is\n9780\nperfectly calibrated, if we can get\nP(y′= y|p′= p) =p, ∀p ∈[0, 1]\nfor any input x (Guo et al., 2017). In another word,\nthe more confident a model is, the chance of its\nprediction being the same as ground truth should\nbe higher. It should be noted thatP(y′= y|p′= p)\ncan not be calculated with finite number of samples,\nso calibration is often evaluated by some statisitcal\napproximations.\nReliability Diagram is a kind of visualized eval-\nuation of confidence calibration (DeGroot and Fien-\nberg, 1983), which plots prediction accuracy as a\nfunction of confidence (e.g. Figure 2).\nTo evaluate the calibration with a model with\na finite set of samples, we divide confidence in-\nterval [0, 1] into M bins with equal length (1/M)\nand group model predictions into these bins ac-\ncording to their prediction confidence. Let Bm be\nthe set of indices of samples which fall into the\ninterval (m−1\nM , m\nM ], then for each interval bin, we\ncan calculate corresponding accuracy and average\nconfidence as follows:\nAcc(Bm) = 1\n|Bm|\n∑\ni∈Bm\n1 ( ˆyi = yi),\nConf (Bm) = 1\n|Bm|\n∑\ni∈Bm\nˆpi,\nwhere ˆyi and yi are the prediction class and ground\ntruth of the ith sample. 1 is the indicator function\nwhich produces 1 if ˆyi = yi otherwise 0. ˆpi is\nthe prediction confidence (probability) of the ith\nsample.\nGiven Acc(Bm) and Conf (Bm), we can draw\nthe reliability diagram for a model. For a per-\nfectly calibrated model, we will have Acc(Bm) =\nConf (Bm) for all m, so its reliability diagram will\nbe y = x. Obviously, the nearer a curve is to the di-\nagonal, the better calibration it represents. Though\nperfect calibration is impossible, we normally hope\na model is well calibrated. Note that since reliabil-\nity diagram do not contain the number of samples,\nsometimes it could be insufficient to represent true\ncalibration of a model when some bins only contain\nvery few samples.\nExpected Calibration Error (ECE). As relia-\nbility diagram is more like a qualitative evaluation\nwhich depicts model calibration in different confi-\ndence intervals, we hope to get a quantitative scalar\nmetric which can reflect overall calibration level\nof a model. Expected Calibration Error is such\na quantitative measurement of calibration (Naeini\net al., 2015).\nFor a set of N samples, we also divide confi-\ndence intervals into M bins and get Acc(Bm) and\nConf (Bm) in the same way as we did when draw-\ning a reliability diagram. Then ECE is calculated\nas follows:\nECE =\nM∑\nm=1\n|Bm|\nN |Acc(Bm) −Conf (Bm)|\nECE represents confidence error averaged on\nsamples and obviously lower ECE means better\ncalibration. We set m = 10when measuring cal-\nibration with the tools above following previous\nworks (Guo et al., 2017; Desai and Durrett, 2020;\nHe et al., 2023).\n4 Calibration Evaluation Tasks and Data\nAs mentioned before, we evaluate model calibra-\ntion on three tasks considering language models’\nability of understanding, factuality and reasoning.\nIn this section, we in detail introduce for each task\nhow the evaluation is conducted and the datasets\nchosen for evaluation.\nCausal Language Modeling is the task of pre-\ndicting the next token for a given sequence, which\nis also the pre-training objective of causal language\nmodels. For a test sequence, we randomly sample\na position in the sequence. Then this sequence is\nfed into models to generate a token corresonding\nto the position. If the predicted token is the same\nas the one in original sentence, we count it as a\ntrue positive. In such way we can get generation\naccuracy and confidence of test dataset, and then\nevaluate model calibration with reliability diagram\nand ECE metric. We use development and test set\nof the PILE dataset (Gao et al., 2020) in this task.\nThe PILE dataset is a large-scale English text cor-\npus which is frequetly used in the pre-training of\nlarge language models.\nFacts Generation is the task aimed at evaluating\nmodels’ memory on factual knowledge. The task\nis mostly the same as causal language modeling in\nform , except that we utilize entity linking data, in\nwhich texts are labeled with entity spans. We let\nmodels generate the first token of entities. We only\ntake the first token of entities into account as when\nthe first token is correctly generated, there is high\n9781\nFigure 3: Model calibration of different parameter scales.\nchance that the whole entity can also be recovered.\nIn facts generation task, we use an enitity linking\ndataset T-REx (Elsahar et al., 2018), which includes\nentity-labeled texts extracted from Wikipedia pages.\nFor T-REx and the PILE dataset, we randomly draw\n100k samples as our evaluation set.\nMulti-task Language Understanding is a task\nwhere models are given questions across different\nfields with multiple answer options, which is de-\nsigned for testing the understanding and reasoning\nability of a language model. We mainly focus on\nquestions with a single correct answer. Follow-\ning MMLU benchmark (Hendrycks et al., 2021),\nwe concatenate 5 in-context samples ahead of the\nquestions and designed prompts to constrain mod-\nels to respond with answer options (i.e. ’ABCD’).\nWe choose MMLU benchmark (Hendrycks et al.,\n2021) as our evaluation data, which covers single-\nchoice questions in 57 subjects across STEM, the\nhumanities, the social sciences and so on.\n5 Calibration in Pre-training Stage\nIn this section, we study the effect of parameter\nscales and training dynamics in pre-training stage\nto models’ calibration.\n5.1 Experimental Setups\nWe choose Pythia as our base model (Biderman\net al., 2023). Pythia is a suite of transformer-based,\nauto-regressive language models designed for sci-\nentific research. It contains 8 models whose scales\nrange from 70m to 12B parameters and for each\nof the scale it provides 154 checkpoints including\n143 checkpoints saved every 1,000 training steps\n(i.e. 1 epoch) and 11 checkpoints trained for less\nthan 1,000 steps. All of these models are trained\non exactly the same data—the PILE (Gao et al.,\n2020) dataset in the same order. For parameter\nscale study, we experiment on models with all 8\nscales. As for training dynamics, we choose Pythia-\n1B4 considering time and computational cost, and\nuse 2n∗1, 000(n = 1, 2...) steps checkpoints (up to\nstep143,000) for our study. We also include check-\npoints of step256 and step512 in our experiments\nto observe the behavior of under-fitted models.\n5.2 Parameter Scales\nFigure 3 shows the experimental results of parame-\nter scales on three tasks. Generally, larger models\nproduce better calibrated results while the level\nof such effect is diverse among tasks. We find\nthat models with all parameter scales can produce\nwell-calibrated predictions on CLM task, with ECE\nlower than 0.1. Also, parameter scales only mildly\naffect model calibration on the CLM task, where\ndifference is minor between smallest and largest\nmodel. This might because CLM task is the same\nas the pre-training objective, where large scale and\ndiverse corpus makes it hard for models to be over-\nconfident when generating common texts. On facts\ngeneration task, model performance on both cali-\nbration and accuracy shows a stronger positive cor-\nrelation with parameter scales. Results on MMLU\n(Figure 3-e and 3-f) seems very messy, but we can\nstill observe some meaningful patterns here. All\n9782\nFigure 4: Model calibration of different training dynamics.\nmodels perform poorly on language understand-\ning task, with accuracies only slightly better than\nrandom choice. As Pythia models have not been\ntrained to follow certain instructions, it is hard for\nthem to understand these knowledge-demanding\nquestions in the MMLU dataset. However, while\naccuracies from all models are similar, ECE de-\ncreases monotonically as the model scale increases.\nMoreover, we found that as the parameter scale\nincreases, confidence distribution of model output\ngradually shrinks to a smaller and lower interval\n(see Appendix B), which might indicate that al-\nthough larger models still can not solve these prob-\nlems, they are more aware of their own capability\nthan small ones. To further verify our conclusions,\nwe conduct same experiments on 4 more models,\nLLaMA (Touvron et al., 2023a), LLaMA-2 (Tou-\nvron et al., 2023b), FLAN-T5 (Chung et al., 2022)\nand OPT (Zhang et al., 2022). Results are presented\nin Appendix C.1. We can see that our conclusions\nholds most of the time, where LLaMA2, FLAN-T5\nand OPT show monotonic improvement with in-\ncreasing model scale while there are outliers in the\nresults of LLaMA. This demonstrates that factors\nother than scale may influence the trend, and LLMs\nshould be examined using more continuously sam-\npled checkpoints to reach a more robust conclusion,\nwhich we leave for future works.\n5.3 Training Dynamics\nOn the whole, the effect of training dynamics fol-\nlows the same pattern with that of parameter scales\nbut there are a few unique observations can be\npointed out (See Figure 4). On CLM task, we\ncan observe apparent improvement in both accu-\nracy and calibration in the very early stage of pre-\ntraining. However, though accuracy keeps growing\nas the training goes on, ECE stabilize at a low\nlevel for the rest of the training process, which\nmeans that under-fitted models can also be well-\ncalibrated. Training dynamics also show a stronger\nimpact on facts generation task. It can be seen\nthat models trained for less than 1 epoch behave\nextremely poor. In this stage, models are near to\nrandomly initialized parameters which can not gen-\nerate a reasonable probability distribution, thus the\naccuracy is almost zero for all confidence intervals.\nResults on MMLU dataset is similar to that of pa-\nrameter scales, with accuracy barely grows while\ncalibration keep improving. Note that we observe\nan increase of ECE in step143,000 (see Figure 4-f),\nwhich may be an sign of over-fitting. As Pythia\ndoes not provide checkpoints with further steps, we\nkeep this as a simple hypothesis.\n6 Calibration in Alignment Stage\nAlignment training is divided into two sub-stages,\ninstruction tuning and RLHF. In this section we\nstudy how model calibration changes during these\nprocess. As the models are fine-tuned on instruc-\ntions in this stage, we add instruction prompt for\nall three tasks (see Table 1). Figure 5 shows the\nECE level of fine-tuned models on three tasks when\nusing different alignment training settings. We also\nreport accuracy results in Appendix C.2, where the\n9783\nFigure 5: Model calibration using different alignment training settings.\nTasks Prompts\nCausal Language Modeling Finish this sentence:\nFacts Generation Finish this Wikipedia description:\nMulti-task Language Understandign The following are multiple choice questions (with answers) about {...}.\nTable 1: Prompts of different tasks in Alignment Training Stage\naccuracy performance generally remains stable and\nonly fluctuates with different datasets and training\nmethods.\n6.1 Experimental Setups\nIn alignment training stage, we use LLaMA-7B\nas our base model (Touvron et al., 2023a). Com-\nplete training hyper-parameters can be found in\nAppendix A.\nInstruction Tuning. We leverage open-source\nGPT-generated and human-labeled data in instruc-\ntion tuning. Alpaca (Taori et al., 2023) contains\n52k pairs of instructions and responses generated\nin the style of self-instruct (Wang et al., 2022) us-\ning OpenAI GPT model. OpenAssistant Conversa-\ntions1, which we denote as OA, is a human-labeled\nassistant-style conversation corpus. We extract all\nsingle-turn conversations written in English from\nOA, resulting in 11k pairs of instructions. Consid-\nering fairness, we sample the datasets to the same\nsize when comparing effect of instruction tuning.\nWe follow setups of Stanford-Alpaca2 and Alpaca-\nLoRA3 for direct fine-tuning and LoRA training\nrespectively. For each experiment group, we train\nthe model for 3 epochs.\nRLHF training contains two parts, reward model\ntraining and reinforcement learning. Training of\nreward models often needs ranked response data,\nwhere responses from different language models to\n1https://huggingface.co/datasets/\nOpenAssistant/oasst1\n2https://github.com/tatsu-lab/stanford_alpaca\n3https://github.com/tloen/alpaca-lora\nthe same instructions are collected and ranked by\nhuman annotators or another language model. We\nuse open-source LM-ranked responses data (Peng\net al., 2023), where GPT-4, GPT-3.5 and OPT-IML\ngenerate responses to Alpaca instructions and these\nresponses are ranked by GPT-4. We use LLaMA as\nour reward model and conduct PPO training on top\nof previously instruction tuned model with Alpca\ndata. We also use LoRA in RLHF training process\nto lower computational costs. We perform RLHF\ntraining with Huggingface TRL Library4.\n6.2 Instruction Tuning\nWe find that instruction tuning generally weakens\nthe calibration of language models. Such impact\nchanges when using different instruction tuning\nsettings.\nTraining Data. As can be seen in Figure 5, direct\nfine-tuning with Alpaca does the most harm to cal-\nibration while models fine-tuned with OA dataset\nperform better. Note that in MMLU dataset, the\nmodel trained with OA is even better calibrated\nthan LLaMA in the first epoch, which might be-\ncause of the capability gain in following instruc-\ntions. However, the ECE level rapidly becomes\nworse in later epochs, which represents that degen-\neration of calibration is a result of fitting process\nand OA is less likely to cause such degeneration.\nWe presume such behavior is related to the diver-\nsity of datasets. Such presumption is mostly based\non intuition where samples in OA datasets display\nstrong personal or emotional characteristics while\n4https://huggingface.co/docs/trl/index\n9784\ninstructions in Alpaca are much more homoge-\nneous. To further look into the difference of Alpaca\nand OA, we compare semantic diversity of their re-\nsponses. We use MPNET (Song et al., 2020) to ex-\ntract sentence features of responses in both datasets\nand visualize these features with t-SNE (Van der\nMaaten and Hinton, 2008), see Figure 6. Results\nshow that semantic features of OA dataset are more\nevenly distributed while those of Alpaca tend to\nbe dense and clustered, which means the former\nis more diverse in semantics. We attribute this\ndifference of diversity to how the datasets are con-\nstructed. Alpaca is a synthetic corpus generated\nin a self-instruct way, where a small set of human-\nwritten instruction data containing 175 seed tasks\nare fed into GPT-3 and augmented to 52k scale. In\nthis case, Alpaca will contain a lot of instruction\ndata with similar format and content for each seed\ntask, and fine-tuning with such clustered dataset\nconsequently leads to a worse calibration. On the\nother hand, OA is created by crowd-sourcing where\nthousands of volunteers are asked to submit their\nown instruction data, which makes the dataset more\ndiverse in tasks and text styles, thus do less harm\nto model calibration.\nTraining Methods. Parameter efficient tuning is\na type of training methods that keep the pre-trained\nweight unchanged and only train a small set of extra\nparameters. Although these methods are originally\ndesigned to train large models with lower resources,\nthey may be able to improve calibration by reduc-\ning catastrophic forgetting (He et al., 2023). We\ncompare calibration of models trained on Alpaca\nand OA datasets with full fine-tuning and LoRA\ntuning. Results exhibit that in all three tasks, model\ntrained with LoRA performs better in calibration\nthan those are directly fine-tuned. Besides, it can\nbe noticed that deterioration in calibration is slight\nfor model trained with LoRA (often in a level of\n0.001) when training epochs increase, while the\nfull fine-tuned model becomes visibly worse. Such\nobservations proves that LoRA can mitigate the\ncalibration degeneration in the instruction tuning\nprocess. In MMLU dataset we observe that behav-\niors of models trained on Alpaca with LoRA are\nsimilar to those trained with OA, where calibration\nimproves compared to LLaMA in the first epoch.\nThis may also indicate that LoRA is helpful in re-\nduce the harmful effect of instruction tuning and\nimprove model calibration.\nFigure 6: Sentence feature distributions of Alpaca and\nOA dataset (Each point is a response sentence).\nTraining Dynamics. In almost all instruction tun-\ning experimental groups, models trained for more\nsteps behave worse in calibration, which indicates\nthat models calibration are affected severely by the\nsmall instruction dataset. Note that we obtained\ncontradictory observation (model calibration im-\nproves when trained longer) in the pre-training\nstage where the model is trained with large-scale\nand diverse corpora, thus we anticipate that im-\nprove the scale and diversity of instruction data can\nalso help improve calibration.\n6.3 RLHF\nThe last groups of bars in each chart of Figure 5\nshow ECE level of models trained with RLHF for\nthree epochs. We can see that compared to the\ninstruction-tuned model, i.e. the 3rd epoch model\ntrained with Alpaca, there is no significant degen-\neration in ECE after RLHF training. Moreover,\nmodels’ calibration do not deteriorate as RLHF last\nfor more epochs. This indicates that when applied\nto models that already have been trained with in-\nstruction data, RLHF might not do further harm to\nmodel calibration.\n7 Discussion\nConfidence calibration is helpful for building hon-\nest large language models in two ways. Firstly,\nconfidence calibration is closely related to the un-\ncertainty of language models, which is leveraged in\nmany approaches like self-consistency to improve\nmodel performance (Wang et al., 2023). Studying\nand improving model calibration will provide fur-\nther evidence to these methods and inspire more\nuncertainty-based techniques.\n8 Conclusions\nIn this work we systematically study the calibration\nof aligned large language models. We designed\n9785\nthorough experiments evaluating the model cali-\nbration with different training settings and reveal\nhow model calibration is affected by pre-training\nand alignment training process. In pre-training, we\nfind that model calibration improves as parameter\nscales and training dynamics increases. In align-\nment training stage, experimental results show that\ninstruction tuning damages model calibration sig-\nnificantly and ill-distributed synthetic data does\nmore harm. Such harm will increase when fine-\ntuning process lasts longer while can be remediated\nby using parameter efficient training methods like\nLoRA. In the mean time, we surprisingly find that\nRLHF has little impact on calibration of instruction\ntuned models.\nLimitations\nThere are two main limitations in this work. The\nfirst one is that we can not carry out fine-grained\nexperiments on larger and better-performing mod-\nels like LLaMA and LLaMA-2, as they only pro-\nvide limited number of variants on scale (e.g.\n7B/30B/70B for LLaMA-2) and do not provide\ncheckpoints for different training dynamics. More\ndetailed and rigorous conclusions can be drawn if\nfiner-grained model variants are available. The sec-\nond one is that our observations and conclusions\ncan be further explored, like mining the relation of\nthese observations to the mathematical theory of\nconfidence calibration and proving our conclusions\ntheoretically. We leave such in-depth exploration\nfor future works.\nAcknowledgements\nWe would like to thank all the reviewers sincerely\nfor their valuable advice to improve this work. This\nresearch is supported by National Science Fund for\nExcellent Young Scholars under Grant 62222212\nand the General Program of National Natural Sci-\nence Foundation of China under Grant 62376033.\nReferences\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. 2023. Pythia:\nA suite for analyzing large language models across\ntraining and scaling.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nMorris H DeGroot and Stephen E Fienberg. 1983. The\ncomparison and evaluation of forecasters. Journal of\nthe Royal Statistical Society: Series D (The Statisti-\ncian), 32(1-2):12–22.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B.\nTenenbaum, and Igor Mordatch. 2023. Improving\nfactuality and reasoning in language models through\nmultiagent debate.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-REx: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The pile: An\n800gb dataset of diverse text for language modeling.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International conference on machine learn-\ning, pages 1321–1330. PMLR.\n9786\nGuande He, Jianfei Chen, and Jun Zhu. 2023. Pre-\nserving pre-trained features helps calibrate fine-tuned\nlanguage models. In The Eleventh International Con-\nference on Learning Representations.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding.\nDan Hendrycks and Kevin Gimpel. 2016. A baseline\nfor detecting misclassified and out-of-distribution\nexamples in neural networks. arXiv preprint\narXiv:1610.02136.\nQuzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang,\nCong Jiang, Zhibin Chen, Zirui Wu, and Yansong\nFeng. 2023. Lawyer llama technical report.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language models\nfor question answering. Transactions of the Associa-\ntion for Computational Linguistics, 9:962–977.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort,\nDeep Ganguli, Danny Hernandez, Josh Jacobson,\nJackson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language models (mostly) know what\nthey know.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\nAviral Kumar and Sunita Sarawagi. 2019. Calibration\nof encoder decoder models for neural machine trans-\nlation.\nTiffany H Kung, Morgan Cheatham, Arielle Medenilla,\nCzarina Sillos, Lorie De Leon, Camille Elepaño,\nMaria Madriaga, Rimel Aggabao, Giezel Diaz-\nCandido, James Maningo, et al. 2023. Performance\nof chatgpt on usmle: Potential for ai-assisted medical\neducation using large language models. PLoS digital\nhealth, 2(2):e0000198.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan Kim,\nNeel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models.\nNick McKenna, Tianyi Li, Liang Cheng, Moham-\nmad Javad Hosseini, Mark Johnson, and Mark Steed-\nman. 2023. Sources of hallucination by large lan-\nguage models on inference tasks.\nRobert G Miller. 1962. Statistical prediction by discrim-\ninant analysis. In Statistical prediction by discrimi-\nnant analysis, pages 1–54. Springer.\nMatthias Minderer, Josip Djolonga, Rob Romijnders,\nFrances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin\nTran, and Mario Lucic. 2021. Revisiting the calibra-\ntion of modern neural networks. Advances in Neural\nInformation Processing Systems, 34:15682–15694.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nAllan H Murphy. 1973. A new vector partition of the\nprobability score. Journal of Applied Meteorology\nand Climatology, 12(4):595–600.\nMahdi Pakdaman Naeini, Gregory Cooper, and Milos\nHauskrecht. 2015. Obtaining well calibrated proba-\nbilities using bayesian binning. In Proceedings of the\nAAAI conference on artificial intelligence, volume 29.\nKhanh Nguyen and Brendan O’Connor. 2015. Pos-\nterior calibration and exploratory analysis for nat-\nural language processing models. arXiv preprint\narXiv:1508.05154.\nJeremy Nixon, Michael W Dusenberry, Linchuan Zhang,\nGhassen Jerfel, and Dustin Tran. 2019. Measuring\ncalibration in deep learning. In CVPR workshops,\nvolume 2.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nTN Palmer, FJ Doblas-Reyes, Antje Weisheimer, and\nMJ Rodwell. 2008. Toward seamless prediction: Cal-\nibration of climate change projections using seasonal\nforecasts. Bulletin of the American Meteorological\nSociety, 89(4):459–470.\n9787\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Tali Bers, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R. Brown, Adam Santoro, Aditya Gupta, and\nAdrià Garriga-Alonso et al. 2022. Beyond the imita-\ntion game: Quantifying and extrapolating the capa-\nbilities of language models.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit\nSharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,\nand Christopher D. Manning. 2023. Just ask for cali-\nbration: Strategies for eliciting calibrated confidence\nscores from language models fine-tuned with human\nfeedback.\nRan Tian, Shashi Narayan, Thibault Sellam, and\nAnkur P. Parikh. 2020. Sticking to the facts: Confi-\ndent decoding for faithful data-to-text generation.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nNeeraj Varshney, Swaroop Mishra, and Chitta Baral.\n2022. Investigating selective prediction approaches\nacross several tasks in iid, ood, and adversarial set-\ntings.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2023. Self-consistency improves chain\nof thought reasoning in language models.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022. Self-instruct: Aligning language\nmodel with self generated instructions.\n9788\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022b. Emer-\ngent abilities of large language models.\nOrion Weller, Nicholas Lourie, Matt Gardner, and\nMatthew E. Peters. 2020. Learning from task de-\nscriptions. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1361–1375, Online. Association for\nComputational Linguistics.\nMengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Vic-\ntoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke\nZettlemoyer, and Ves Stoyanov. 2023. Training tra-\njectories of language models across scales.\nYijun Xiao and William Yang Wang. 2021. On halluci-\nnation and predictive uncertainty in conditional lan-\nguage generation. arXiv preprint arXiv:2103.15025.\nBenfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu,\nQiaoqiao She, and Yongdong Zhang. 2023a. $k$NN\nprompting: Beyond-context learning with calibration-\nfree nearest neighbor inference. In The Eleventh In-\nternational Conference on Learning Representations.\nBenfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang\nZhou, Yongdong Zhang, and Zhendong Mao. 2023b.\nExpertprompting: Instructing large language models\nto be distinguished experts.\nHuiqin Yang and Carl Thompson. 2010. Nurses’ risk\nassessment judgements: A confidence calibration\nstudy. Journal of Advanced Nursing, 66(12):2751–\n2760.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n2023. Why does chatgpt fall short in providing truth-\nful answers?\n9789\nA Hyper-parameters\nWe list all used hyper-parameters in Table 2 and\nTable 3.\nParameters Values\nDirect Fine-tune\nnums of gpu 4\nepochs 3\nbatch size per gpu 4\ngradient accumulation 8\ntotal batch size 4 ∗4 ∗8 = 128\nmax sequence length 2048\nlearning rate 2e-5\nwarmup ratio 0.03\nlr scheduler cosine\nLoRA\nnums of gpu 4\nepochs 3\nbatch size per gpu 16\ngradient accumulation 2\ntotal batch size 4 ∗16 ∗2 = 128\nmax sequence length 2048\nlearning rate 3e-4\nwarmup steps 100\nlora r 8\nlora alpha 32\nlora dropout 0.1\nlora target modules [q_proj, v_proj]\nlr scheduler linear\nTable 2: Hyper-parameters of instruction tuning.\nParameters Values\nReward Model\nnums of gpu 4\nepochs 2\nbatch size per gpu 8\ngradient accumulation 1\ntotal batch size 4 ∗8 ∗1 = 32\nmax sequence length 2048\nlearning rate 2e-5\nlora r 8\nlora alpha 32\nlora dropout 0.1\nlr scheduler cosine\nRLHF\nnums of gpu 4\nepochs 3\nbatch size 8\ngradientaccumulation 8\noutput max length 128\nlearning rate 1.4e-5\nlora r 16\nlora alpha 32\nlora dropout 0.05\nTable 3: Hyper-parameters of RLHF.\n9790\nB MMLU Confidence Distribution\nFigure 7 shows the confidence distribution of out-\nputs of Pythia models from 70m to 12B. As is\nexplained in Section 5.2, the ranges of the distri-\nbutions tend to become smaller for larger models.\n9791\nFigure 7: Confidence distribution of model outputs of different scales on MMLU Dataset.\n9792\nC Supplementary Experimental results\nC.1 Calibration of Other Models\nFigure 8-11 show supplementary results about\nparameter scales for other 4 models (LLaMA,\nLLaMA-2, FLAN-T5 and OPT), where we can\ndraw similar conclusions with that of Section 5.2.\nFigure 8: Calibration results of LLaMA.\nFigure 9: Calibration results of LLaMA-2.\n9793\nFigure 10: Calibration results of FLAN-T5.\nFigure 11: Calibration results of OPT.\n9794\nC.2 Accuracy Results in Alignment Stage\nFigure 12 shows the accuracy of different model\noutputs on the the three datasets. As can be seen\nin the figure, accuracy generally keeps stable\nand only fluctuates with different training data.\nFigure 12: Model accuracy using different alignment training settings.\n9795",
  "topic": "Calibration",
  "concepts": [
    {
      "name": "Calibration",
      "score": 0.8028602004051208
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.7889519929885864
    },
    {
      "name": "Computer science",
      "score": 0.7391867637634277
    },
    {
      "name": "Process (computing)",
      "score": 0.6286441087722778
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49471303820610046
    },
    {
      "name": "Machine learning",
      "score": 0.47231078147888184
    },
    {
      "name": "Reliability engineering",
      "score": 0.4701186418533325
    },
    {
      "name": "Language model",
      "score": 0.4172067940235138
    },
    {
      "name": "Natural language processing",
      "score": 0.4125273823738098
    },
    {
      "name": "Data mining",
      "score": 0.3255928158760071
    },
    {
      "name": "Statistics",
      "score": 0.16476580500602722
    },
    {
      "name": "Engineering",
      "score": 0.1252157986164093
    },
    {
      "name": "Mathematics",
      "score": 0.08434528112411499
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    }
  ]
}