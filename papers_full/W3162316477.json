{
  "title": "Are Convolutional Neural Networks or Transformers more like human vision?",
  "url": "https://openalex.org/W3162316477",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4282319613",
      "name": "Tuli, Shikhar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202192972",
      "name": "Dasgupta, Ishita",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287214774",
      "name": "Grant, Erin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221661171",
      "name": "Griffiths, Thomas L.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3099206234",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2902617128",
    "https://openalex.org/W3104962541",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2903867357",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2910992787",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2550553598",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W2066380143",
    "https://openalex.org/W3105558400",
    "https://openalex.org/W2221625691",
    "https://openalex.org/W2895013038"
  ],
  "abstract": "Modern machine learning models for computer vision exceed humans in accuracy on specific visual recognition tasks, notably on datasets like ImageNet. However, high accuracy can be achieved in many ways. The particular decision function found by a machine learning system is determined not only by the data to which the system is exposed, but also the inductive biases of the model, which are typically harder to characterize. In this work, we follow a recent trend of in-depth behavioral analyses of neural network models that go beyond accuracy as an evaluation metric by looking at patterns of errors. Our focus is on comparing a suite of standard Convolutional Neural Networks (CNNs) and a recently-proposed attention-based network, the Vision Transformer (ViT), which relaxes the translation-invariance constraint of CNNs and therefore represents a model with a weaker set of inductive biases. Attention-based networks have previously been shown to achieve higher accuracy than CNNs on vision tasks, and we demonstrate, using new metrics for examining error consistency with more granularity, that their errors are also more consistent with those of humans. These results have implications both for building more human-like vision models, as well as for understanding visual object recognition in humans.",
  "full_text": "Are Convolutional Neural Networks or Transformers more like human vision?\nShikhar Tuli (stuli@princeton.edu)\nDepartment of Electrical and Computer Engineering, Princeton University\nIshita Dasgupta (idg@google.com)\nDeepMind, New York\nErin Grant (eringrant@berkeley.edu)\nDepartment of Electrical Engineering and Computer Sciences, UC Berkeley\nThomas L. Grifﬁths (tomg@princeton.edu)\nDepartments of Psychology and Computer Science, Princeton University\nAbstract\nModern machine learning models for computer vision exceed\nhumans in accuracy on speciﬁc visual recognition tasks, no-\ntably on datasets like ImageNet. However, high accuracy can\nbe achieved in many ways. The particular decision function\nfound by a machine learning system is determined not only by\nthe data to which the system is exposed, but also the inductive\nbiases of the model, which are typically harder to characterize.\nIn this work, we follow a recent trend of in-depth behavioral\nanalyses of neural network models that go beyond accuracy\nas an evaluation metric by looking at patterns of errors. Our\nfocus is on comparing a suite of standard Convolutional Neu-\nral Networks (CNNs) and a recently-proposed attention-based\nnetwork, the Vision Transformer (ViT), which relaxes the\ntranslation-invariance constraint of CNNs and therefore repre-\nsents a model with a weaker set of inductive biases. Attention-\nbased networks have previously been shown to achieve higher\naccuracy than CNNs on vision tasks, and we demonstrate, us-\ning new metrics for examining error consistency with more\ngranularity, that their errors are also more consistent with those\nof humans. These results have implications both for building\nmore human-like vision models, as well as for understanding\nvisual object recognition in humans.\nIntroduction\nConvolutional Neural Networks (CNNs) are currently the\nde-facto standard for many computer vision tasks, includ-\ning object detection (Ren et al., 2015), image classiﬁca-\ntion (Krizhevsky et al., 2012), segmentation (Girshick et al.,\n2014), facial recognition (Schroff et al., 2015), and caption-\ning (L. Chen et al., 2017). The inductive bias in CNNs was\ninspired by the primate visual system, and their layer acti-\nvations have been used to explain neural activations therein\n(Yamins et al., 2014). A large amount of recent work has gone\ninto understanding the representations and strategies learned\nby CNNs trained on popular datasets like ImageNet (Geirhos\net al., 2020; Hermann et al., 2020). Much of this takes the\nform of behavioral analysis; i.e., analyzing model classiﬁca-\ntions to gain insight into the underlying representations. A\nkey ﬁnding from this work is that networks tend to classify\nimages by texture rather than by shape (Baker et al., 2018).\nOn the other hand, humans preferentially use shape informa-\ntion for classiﬁcation (Kucker et al., 2019). For example,\nCNNs struggle to recognize sketches that preserve the shape\nrather than texture, while these can easily be classiﬁed by hu-\nmans.\nFig. 1: Error-consistency\nstimuli (Geirhos et al.,\n2019): (left) Original\nimage from ImageNet, and\n(right) a textured transform.\nThe analysis of the relationship between human vision and\nneural networks has been signiﬁcantly improved by the avail-\nability of diagnostic datasets (Geirhos et al., 2019; Navon,\n1977). For example, Geirhos et al. (2019) present Stylized\nImageNet where the texture from one class can be applied\nto an image from another class (preserving shape; see Fig-\nure 1). Model performance on this dataset allows us to de-\ntermine whether a model is biased toward shape or texture.\nAnother related line of behavioral analyses instead considers\nerror consistency on standard datasets rather than testing on\nspecially designed datasets (Geirhos et al., 2020).\nRecent developments in machine learning suggest, how-\never, that convolutions maybe not be necessary for computer\nvision. New Transformer architectures have been success-\nfully used for vision-based tasks (Vaswani et al., 2017). These\ndo not have the architectural inductive bias toward local spa-\ntial structure that convolutions provide. Instead, they are\nbased entirely on ﬂexible (learned) allocation of attention.\nWhile the success of Transformers has been most extensively\ndemonstrated in language (Devlin et al., 2019), their appli-\ncation to vision tasks have also outperformed state-of-the-art\nCNNs (M. Chen et al., 2020; Dosovitskiy et al., 2021).\nIn this paper, we compare one of these attention-based\nmodels, the Vision Transformer (ViT) (Dosovitskiy et al.,\n2021) to standard CNNs as well as humans, on a visual cate-\ngorization task. We focus on understanding whether CNNs or\nViTs are more “human-like” in their classiﬁcation behavior.\nConvolution vs. Attention\nConvolutional Neural Networks marked the advent of deep\nlearning as a powerful and scalable approach (Krizhevsky et\nal., 2012) by demonstrating state of-the-art performance on\nlarge-scale image classiﬁcation datasets like ImageNet. Con-\nvolutional layers convolve the input and pass its result to the\nnext layer (see Figure 2(a)). This hard-codes a sense of trans-\narXiv:2105.07197v2  [cs.CV]  1 Jul 2021\nlational invariance—each patch in an image is processed by\nthe same weights. This is similar to the response of a neuron\nin the visual cortex to a speciﬁc stimulus (Lecun et al., 1998).\nBy training the weights of these convolutional ﬁlters, CNNs\ncan learn representations of images for every speciﬁc class\nand have been shown to have many parallels to processing in\nthe visual cortex (Yamins et al., 2014). These inductive biases\nallowed CNNs to vastly outperform fully connected networks\non vision tasks. However, such local connectivity can lead to\nloss of global context; for example, it can encourage a bias\ntowards classifying on the basis of texture rather than shape\n(Hermann et al., 2020). Some approaches to address this are\ntraining on augmented versions of images and incorporating\ntop-down information (Cao et al., 2015).\nTransformer models offer another approach (Vaswani et\nal., 2017). The primary backbone of a Transformer is self-\nattention. This mechanism permits us to contextually up-\nweight the relevance of certain information. This can be used\nto implement local receptive ﬁelds—previous work shows\nthat multi-head self-attention layers (like the ones we use) can\nperform like a convolution layer (Cordonnier et al., 2020).\nHowever, Transformers are much more ﬂexible and are not\nbound to always use convolutions. This ﬂexibility has led to\ntheir great success in natural language processing, where one\nmight have to attend to information at various distances away\nfrom the current word. They have recently been successful in\nthe vision domain as well (M. Chen et al., 2020; Dosovitskiy\net al., 2021). In this paper, we investigate whether this added\nﬂexibility allows Transformers to give more human-like rep-\nresentations than CNNs.\nMeasuring Error Consistency\nA central problem in machine learning and artiﬁcial intel-\nligence research, as well as in cognitive science and be-\nhavioural neuroscience, is to establish whether two decision\nmakers (be they humans or AI models) use the same strat-\negy to solve a given task. Most comparisons across systems\nonly consider their accuracy on the task. However, there are\nmany ways to achieve the same average accuracy on a test\nset. First, two systems can differ in which stimuli they fail\nto classify correctly, which is not captured by accuracy met-\nrics. Second, while there is only one way to be right, there\nare many ways to be wrong—systems can also vary system-\natically in how they misclassify stimuli. We consider various\nmeasures of these differences below.\nError overlap. First, we consider how to measure the sim-\nilarity of two systems in terms of which stimuli they tend to\nmisclassify. As a ﬁrst pass, we can simply consider how many\nof the decisions down to individual trials are identical (either\nboth correct or both incorrect). We call this the observed er-\nror overlap. This is given bycobsi, j = ei, j\nn where ei, j is how of-\nten the two systems “agree”;i.e., how often they both classify\ncorrectly or both classify incorrectly. This metric increases\nas the accuracies of the systems improve, since the number of\nClass\nFlattening \npatches and positional embedding Attention units (×N)\nConvolution units (×M)\nMLP\nHead Class\nMLP\nHead\n(a)\n(b)\nGlobal information\npassed to next layer\nLocal receptive /f_ield\nFig. 2: Bird’s eye view of (a) convolutional and (b) attention-\nbased networks.\noverlapping correct decisions will increase.\nCorrecting for accuracy using Cohen’sκ. Consider a sys-\ntem that at each trial gets it right with probability pcorrect and\ngets it wrong otherwise. This amounts to taking i.i.d. samples\nfrom a binomial with parameter pcorrect. Two such models\nwill have higher observed error overlap as pcorrect (the av-\nerage accuracy) increases; i.e., they will have a higher error\noverlap expected by chance. This is calculated by comparing\nindependent binomial observers i and j with their accuracies\nas the respective probabilities: cexpi, j = pi pj + (1 −pi)(1 −\npj). The expected overlap can be used to normalize the ob-\nserved error overlap, giving a measure of error consistency\nknown as Cohen’sκ :\nκi, j =\ncobsi, j −cexpi, j\n1 −cexpi, j\nCohen’sκ has been used in previous research comparing\nhumans and neural networks (Geirhos et al., 2020). However,\nit does not take into account what the system misclassiﬁes an\nimage as when making an error—it simply considers whether\nor not the classiﬁcation was correct. It is also difﬁcult to in-\nterpret where the similarities and differences across systems\ncome from.\nMore Granular Investigation of Misclassiﬁcations\nWe can compare the decisions made by two classiﬁers, with-\nout loss of information, by comparing each’s confusion ma-\ntrix, a table that accumulates the true vs. predicted class of\neach decision made by the classiﬁer. However, this is a very\nhigh dimensional object; e.g., ImageNet contains 1000 differ-\nent ﬁne-grained classes, giving a confusion matrix with 10 6\nelements. This matrix will also be very sparsely populated as\nmost off-diagonal terms will be zero. Further, collecting ad-\nequate human data to populate the corresponding confusion\nmatrix for human decisions is difﬁcult.\n(a) (b)\n (c) (d)\nκ κ\nFig. 3: (a) Class-wise and (b) inter-class JS distance vs. Cohen’s κ on the Stylized ImageNet (SIN) dataset. Class-wise JS\ndistance vs. (c) ImageNet top-1 errors and (d) SIN top-1 error.\nOne solution is to cluster the classes into higher-level cate-\ngories, for example, by using the WordNet hierarchy (Miller,\n1995); this gives 16 so-called “entry-level” categories, viz.\nairplane, bear, bicycle, bird, boat, bottle, car, cat, chair, clock,\ndog, elephant, keyboard, knife, oven and truck (Geirhos et al.,\n2019)). To evaluate these ImageNet-trained models on these\n16 classes, we collected the class probabilities estimated by\nthe models and mapped these to the 16 entry-level categories\nby summing over the probabilities for the ImageNet classes\nbelonging to each category, producing a 16 ×16 confusion\nmatrix for each model.\nWe can use this confusion matrix to generate various met-\nrics of comparison between two classiﬁers. These measures\nare more ﬂexible than the Cohen’s κ metric introduced pre-\nviously, since they capture information aboutwhat misclassi-\nﬁed elements are misclassiﬁed as what. This opens the door\nto more sophisticated analyses that take into account cluster\nstructure in the confusion matrix. For example, misclassify-\ning a car as a truck might be a more “human-like” error than\nmisclassifying it as a dog.\nConcretely, we generate a probability distribution of errors\nover C classes by computing the number of times elements\nfrom each class are misclassiﬁed and normalizing with the\nnet number of errors made. In particular, to get a probability\ndistribution of errors, p ∈∆C (where ∆C is the C-dimensional\nprobability simplex), we normalize the error terms for every\nclass:\npi = ei\n∑C\ni ei\n, ∀i ∈{1,2, . . . ,C},\nwhere ei is a count of errors deﬁned for a given system.\nWe then compute the Jensen-Shannon (JS) distance be-\ntween these distributions, given by\nJS(p,q) =\n√\nD(p ∥m) +D(q ∥m)\n2\nwhere m is the point-wise mean of two probability distribu-\ntions p and q (i.e., mi = (pi + qi)/2, p and q being the prob-\nability distributions of errors of the two systems), and D is\nthe Kullback-Leibler divergence. The JS distance is a sym-\nmetrized and smoothed version of the Kullback-Liebler di-\nvergence,\nD(p ∥q) =∑\ni\npilog pi\nqi\nA lower JS distance implies classiﬁers with high error con-\nsistency. This measure, unlike Cohen’s κ, is only concerned\nwith similarities in which examples tend to be misclassiﬁed,\nand is unaffected by the overall accuracy of the classiﬁer\n(c.f., Geirhos et al., 2020). In the next section, we compute\nthese distances to human classiﬁcation behavior in convolu-\ntional and Transformer models, showing how these can yield\nmore information than existing measures like Cohen’s κ. In\nwhat immediately follows, we deﬁne two variants of the JS\ndistance that are less and more granular.\nClass-wise JS distance. To produce an error-consistency\nmetric that is very close to Cohen’s κ, we collapse columns\n(predicted labels) of the confusion matrix, and compute the\naccumulated error for 16 true classes as:\nei = ∑\nj\nCMi, j, ∀j ̸= i\nwhere CM is the confusion matrix for the given system in\nquestion. In this context, the class-wise JS distance compares\nwhich classes were misclassiﬁed, for a given number of out-\nput classes (16 in this case).\nInter-class JS distance. We can also use the confusion ma-\ntrix to compute more ﬁne-grained measures. In particular,\nwe can directly compute the distances between the full dis-\ntribution of errors giving a 240-dimensional inter-class error\nCohen's Kappa JS distance\n(class-wise)\nJS distance\n(inter-class)\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\n0.18Cohen's Kappa\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nJS distance\nViT-B/32\nResNet-50\nAlexNet\nVGG-16\nGoogleNet\nFig. 4: Error consistency results on SIN dataset.\ndistribution (i.e., p ∈∆240 corresponding to the off-diagonal\nentries of the 16 ×16 confusion matrix) by taking the error\ncounts to be the off-diagonal elements of the confusion ma-\ntrix:\nei j = CMi, j, ∀j ̸= i\nIn this context, the inter-class JS distance compares what\nclasses were misclassiﬁed as what.\nAn interesting ﬁnding is that, instead of a strong correla-\ntion shown by class-wise JS in Figure 3(a), Figure 3(b) sug-\ngests that there is no correlation of inter-class JS distance with\nCohen’sκ implying that this metric gives insight beyond Co-\nhen’sκ in measuring error-consistency with humans.\nMethods\nWe analyze error consistency for different algorithms—the\nmost popular CNN, i.e., ResNet (Kolesnikov et al., 2020),\nand the recently proposed attention-based Vision Transformer\n(ViT) (Dosovitskiy et al., 2021). The ViT and ResNet mod-\nels used were pre-trained on ImageNet-21K (also known as\nthe “Full ImageNet, Fall 2011 release”) and ILSVRC-2012\ndatasets (Russakovsky et al., 2015). The ViT models used\ninclude ViT-B/16, ViT-B/32, ViT-L/16 and ViT-L/32 and the\nResNet model used is BiT-M-R50x1 1. We test these on a\nspecially designed diagnostic dataset, the Stylized ImageNet\ndataset where cue-conﬂict between texture and shape are gen-\nerated by texture-based style transfer (Geirhos et al., 2019) 2.\nAll results are reported with 95% conﬁdence intervals on\ncross-validated test error.\nResults\nFirstly, we see that the class-wise JS distance is closely\n(inversely) correlated with Cohen’s κ when evaluated on a\n1Trained models available at https://console.cloud\n.google.com/storage/browser/vit models/ and https://\nconsole.cloud.google.com/storage/browser/bit models/\n2Data available at https://github.com/rgeirhos/texture\n-vs-shape/tree/master/stimuli\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0\nFraction of 'texture' decisions\nFraction of 'shape' decisions\nShape categories\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\nResNet−50\nAlexNet\nVGG−16\nGoogLeNet\nViT−B_16\nViT−L_32\nHumans (avg.)\nFig. 5: Shape bias for different networks for the SIN dataset\n(Geirhos et al., 2019). Vertical lines indicate averages.\nrange of models on the Stylized ImageNet (SIN) dataset (Fig-\nure 3(a)). We also see that this measure does not have sig-\nniﬁcant correlation with accuracy on the training task (Fig-\nure 3(c)), reinforcing that error consistency on diagnostic\ndatasets like SIN can be independent of accuracy on the train-\ning task. However, when comparing this measure with the\naccuracy on SIN (Figure 3(d)), we can see that a lower JS\ndistance with human errors implies a higher accuracy on SIN;\ni.e., a higher shape bias (see the “Shape Bias” section below).\nFigure 4 presents the comparison of different error consis-\ntency metrics for the considered models on the SIN dataset.\nWe ﬁrst compare Cohen’s κ across ResNet and ViT. In this\nﬁrst comparison of the error consistency between Transform-\ners and CNNs, we ﬁnd that ViT is more consistent with hu-\nmans than ResNet. We then compare our new JS distance\nmeasures. We ﬁrst consider the class-wise distance. Higher\nCohen κ and lower JS distance each indicate greater error\nconsistency. We plot the JS distance in decreasing magnitude\nto visually highlight similarities in the pattern with Cohen’s\nκ. We ﬁnd that the pattern repeated for Cohen’s κ is repli-\ncated with the class-wise JS distances; i.e., that ViT is more\nhuman-like than CNNs. Finally, we consider the distance be-\ntween the full joint distributions or the inter-class JS distance\nfor 240 error types. We note the surprising ﬁnding that the\ninter-class JS distance for ViT is higher than for ResNet.\nCohen's Kappa JS distance\n(class-wise)\nJS distance\n(inter-class)\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\n0.18Cohen's Kappa\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nJS distance\nViT-B/32\nViT-B/32(ft.)\nResNet-50\nResNet-50(ft.)\nFig. 6: Error consistency results for SIN dataset before and\nafter ﬁne-tuning.\nShape Bias\nCNNs have been shown to have a stronger dependence on\ntexture rather than shape when categorizing visual objects\n(Baker et al., 2018), while humans tend to have the opposite\npreference. Here, we examine how ViTs and CNNs compare\non this shape bias. The shape bias has been deﬁned as the\npercentage of the time the model correctly predicts the shape\nfor trials on which either shape or texture prediction is cor-\nrect (Geirhos et al., 2020). We tested this by evaluating per-\nformance on the SIN dataset. This dataset contains images\nwhere the shape and texture of the object in each image con-\nﬂict. With this dataset, we can test if a system classiﬁes on\nthe basis of shape or texture.\nWe use the same models as in the previous section, trained\nin the same way. We then analyze their test performance\non the SIN dataset. We ﬁrst collect all the trials in which\nthe classiﬁer label matches either the true texture or the true\nshape of the object. We then check what fraction of these got\nthe shape right vs. the texture right. The results for this test\nare presented in Figure 5. Small bar plots on the right in-\ndicate accuracy (answer corresponds to either correct texture\nor shape category). We see that ViT has a higher shape bias\nthan traditional CNNs. This goes part of the way toward ex-\nplaining the higher error consistency on the class-wise mea-\nsures from the previous section with humans who primarily\ncategorize objects by shape rather than texture. However, it\nseems at odds with the ﬁnding that when considering the full\nerror distribution, ResNet is more human-like than ViT. Try-\ning to understand this, we note that the shape bias analysis\nonly considers the cases where either the shape or texture is\ncorrectly predicted. It only includes misclassiﬁcations that\nmatched the image’s true texture. It therefore doesn’t contain\nmost of the misclassiﬁcations that the full error distribution\nreﬂects. Hence, in the full error distribution, ResNet indeed\noutperforms ViT and this could only be revealed by the inter-\nclass JS distance.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0\nFraction of 'texture' decisions\nFraction of 'shape' decisions\nShape categories\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\nResNet−50\nResNet−50(ft.)\nViT−B_32\nViT−B_32(ft.)\nHumans (avg.)\nFig. 7: Shape bias for ResNet and ViT before and after ﬁne-\ntuning. Vertical lines indicate averages.\nFine-tuning with Augmented Data\nWe have so far only considered the performance of models\nthat have been trained on the same ﬁxed computer visions\ndatasets. However, the structure of the data observed can sig-\nniﬁcantly alter the representations learned. Brendel & Bethge\n(2019) show that ImageNet can be largely be solved using\nonly local information, indicating that a texture bias is an eco-\nlogically rational heuristic when solving this dataset. Geirhos\net al. (2019) show that training on a dataset where textures\nare forced to be uninformative by changing the texture of Im-\nageNet object to those of randomly chosen paintings leads to\na shape bias almost as high as in humans. Hermann et al.\n(2020) show that simpler data augmentations also play a sig-\nniﬁcant role in learning a shape bias. In particular, naturalis-\ntic data augmentation involving color distortion, noise, and\nblur substantially decreases texture bias, whereas random-\ncrop augmentation increases texture bias in ImageNet-trained\nCNNs (Hermann et al., 2020). These ﬁndings highlight the\ncrucial role of training data in the representations learned. In\nthis section, we examine how data augmentations affect the\nrepresentations learned by these systems, in particular, how\nthis ﬁne-tuning affects their similarity to human behavior.\nMethods\nAmong the attention- and convolution-based models as iden-\ntiﬁed previously, we train the smallest ViT model (ViT-B/32)\nand the smallest ResNet (BiT-M-R50x1). Further works\nTraining on augmented data\nFig. 8: Shape bias and ImageNet accuracy of ViT and ResNet\nwith ﬁne-tuning over augmented data.\ncould look at the effect of the number of trainable parame-\nters while training models of both architectures.\nWe use augmentations presented in T. Chen et al. (2020)\nand Hermann et al. (2020): rotation ( ±90◦, 180◦randomly),\nrandom cutout (rectangles of size 2 ×2 px to half the image\nwidth), Sobel ﬁltering, Gaussian blur (kernel size = 3×3 px),\ncolor distortion (color jitter with probability 80% and color\ndrop with probability 20%) and Gaussian noise (standard de-\nviation of 0.196 for normalized image). These augmentations\nare applied to the ImageNet dataset, and are then used to ﬁne-\ntune the models. For ViT, we used cosine step decay and\ntrained for 100 epochs with an initial learning rate of 0.3.\nFor ResNet, we used linear step decay and trained for 5000\nepochs with an initial learning rate of 0.03. An appropriate\ntraining recipe was chosen based on the network architectures\nand their hyper-parameters used for the pre-training (Doso-\nvitskiy et al., 2021; Kolesnikov et al., 2020). Further details\nabout hyper-parameter tuning, along with the source code for\ntraining and the experiments implemented, can be found at:\nhttps://github.com/shikhartuli/cnn txf bias.\nError Consistency\nWe repeat the error consistency analyses above with these\nﬁne-tuned models. These results are reported in Figure 6. We\nﬁnd that ﬁne-tuning made ResNet less human-like in terms\nof error consistency (signiﬁcant differences in Cohen’sκ and\nthe inter-class JS distance, a non-signiﬁcant trend in the class-\nwise JS distance). This is surprising, since these augmenta-\ntions have been found to increase shape bias (see next section)\nand one would expect that increased shape bias would mean\ngreater error consistency with humans. On the other hand, we\nﬁnd that ViT does not signiﬁcantly change in its error con-\nsistency with ﬁne-tuning, and in fact trends (not statistically\nsigniﬁcantly) towards in the opposite direction than ResNet,\nin particular, towards improved error consistency.\nShape Bias\nWe repeat the shape-bias analyses above with these ﬁne-tuned\nmodels, results in Figure 7. Consistent with previous ﬁndings\nin Hermann et al. (2020), we see that ResNet increases its\nshape bias after ﬁne-tuning. We ﬁnd that ViT also increases\nits shape bias after ﬁne-tuning.\nEffect on Accuracy\nFine-tuning changes the representations used and we have\nstudied how this affects ML systems’ similarity to human\nclassiﬁcation behavior. However, it remains to be seen how\nthis ﬁne-tuning affects accuracy on the original training task.\nWe analyze this in Figure 8. We see that training on aug-\nmented data increases shape bias and decreases ImageNet ac-\ncuracy slightly, as is corroborated by previous works (Her-\nmann et al., 2020). The decrease in accuracy for ResNet is\nmore signiﬁcant than for ViT.\nConclusion\nIn this work, we explore the extent to which different vi-\nsion models correlate with human vision from an error-\nconsistency point-of-view. We see that recently proposed\nTransformer networks not only outperform CNNs on accu-\nracy for image classiﬁcation tasks, but also have higher shape\nbias and are largely more consistent with human errors. We\nexplore this consistency with new metrics that go beyond the\npreviously proposed Cohen’s κ. Further, we ﬁne-tuned two\nmodels—a Transformer and a traditional Convolutional Neu-\nral Network (CNN)—on augmented datasets to ﬁnd that this\nincreases shape bias in both CNNs and Transformers. We\nobserve that Transformers maintain their accuracy while also\ngaining equivalently in their shape bias when compared to\nCNNs. This could possibly be explained by the nature of at-\ntention models that permits focus on the part of the image that\nis important for the given task and neglect the otherwise noisy\nbackground to make predictions.\nMany more tests can still be performed on Transformer\nmodels. For example, ViTs could be compared with iGPT\n(M. Chen et al., 2020) to see how the architecture within this\nfamily could affect shape and texture biases. This could help\nus formulate architectural “features” that help in modelling\nbetter brain-like networks. Additionally, the JS metric we in-\ntroduce can be used to analyze the kinds of errors made in\nmany other ways. For example, we can also gauge “concept-\nlevel” similarity between model misclassiﬁcations (like dogs\nfor cats and not trucks). This could also help probe into the\npremise that humans could be using not just shape/texture but\nalso “concepts” for classiﬁcation (Speer et al., 2017). This\nwould not be possible with a scalar metric like Cohen’s κ.\nFurther, by applying these human error-consistency metrics\nas part of the training loss, we could get a model nearer to\n“human-like strategy”. This could also contribute towards\nsimplifying or regularizing these models, reducing the com-\nputational cost of training.\nAcknowledgements\nThis work was supported by the Defense Advanced Research\nProjects Agency (DARPA) under the Lifelong Learning Ma-\nchines (L2M) program via grant number HR001117S0016\nand by the National Science Foundation (NSF) under grant\nnumber 1718550.\nReferences\nBaker, N., Lu, H., Erlikhman, G., & Kellman, P. J. (2018).\nDeep convolutional networks do not classify based on\nglobal object shape. PLoS computational biology, 14(12),\ne1006613.\nBrendel, W., & Bethge, M. (2019). Approximating CNNs\nwith Bag-of-local-Features models works surprisingly well\non ImageNet. In Proceedings of the International Confer-\nence on Learning Representations (ICLR).\nCao, C., Liu, X., Yang, Y ., Yu, Y ., Wang, J., Wang, Z., . . .\nothers (2015). Look and think twice: Capturing top-\ndown visual attention with feedback convolutional neural\nnetworks. In Proceedings of the International Conference\non Computer Vision (ICCV) (pp. 2956–2964).\nChen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., &\nChua, T.-S. (2017). SCA-CNN: Spatial and channel-wise\nattention in convolutional networks for image captioning.\nIn Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) (pp. 5659–5667).\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Dhariwal,\nP., . . . Sutskever, I. (2020). Generative pretraining from\npixels. In Proceedings of the International Conference on\nMachine Learning (ICML).\nChen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A\nsimple framework for contrastive learning of visual repre-\nsentations. In International Conference on Machine Learn-\ning (ICML) (pp. 1597–1607).\nCordonnier, J.-B., Loukas, A., & Jaggi, M. (2020). On the re-\nlationship between self-attention and convolutional layers.\nIn Proceedings of the International Conference on Learn-\ning Representations (ICLR).\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019,\nJune). BERT: Pre-training of deep bidirectional transform-\ners for language understanding. In Proceedings of the An-\nnual Meeting of the Association for Computational Lin-\nguistics (ACL) (pp. 4171–4186).\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D.,\nZhai, X., Unterthiner, T., . . . Houlsby, N. (2021). An image\nis worth 16x16 words: Transformers for image recognition\nat scale. In Proceedings of the International Conference on\nLearning Representations (ICLR).\nGeirhos, R., Meding, K., & Wichmann, F. A. (2020). Beyond\naccuracy: Quantifying trial-by-trial behaviour of CNNs\nand humans by measuring error consistency. In Advances\nin Neural Information Processing Systems.\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wich-\nmann, F. A., & Brendel, W. (2019). ImageNet-trained\nCNNs are biased towards texture; Increasing shape bias\nimproves accuracy and robustness. In Proceedings of\nthe International Conference on Learning Representations\n(ICLR).\nGirshick, R., Donahue, J., Darrell, T., & Malik, J. (2014).\nRich feature hierarchies for accurate object detection and\nsemantic segmentation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR) (pp. 580–587).\nHermann, K., Chen, T., & Kornblith, S. (2020). The ori-\ngins and prevalence of texture bias in convolutional neural\nnetworks. In Advances in Neural Information Processing\nSystems (pp. 19000–19015).\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J.,\nGelly, S., & Houlsby, N. (2020). Big Transfer (BiT): Gen-\neral visual representation learning. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV).\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Im-\nagenet classiﬁcation with deep convolutional neural net-\nworks. In Advances in Neural Information Processing Sys-\ntems (pp. 1097–1105).\nKucker, S. C., Samuelson, L. K., Perry, L. K., Yoshida, H.,\nColunga, E., Lorenz, M. G., & Smith, L. B. (2019). Re-\nproducibility and a unifying explanation: Lessons from the\nshape bias. Infant Behavior and Development , 54, 156–\n165.\nLecun, Y ., Bottou, L., Bengio, Y ., & Haffner, P. (1998).\nGradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11), 2278-2324.\nMiller, G. A. (1995). WordNet: A lexical database for En-\nglish. Communications of the ACM, 38(11), 39–41.\nNavon, D. (1977). Forest before trees: The precedence of\nglobal features in visual perception. Cognitive psychology,\n9(3), 353–383.\nRen, S., He, K., Girshick, R., & Sun, J. (2015). Faster\nR-CNN: Towards real-time object detection with Region\nRroposal Networks. In Advances in Neural Information\nProcessing Systems (pp. 91–99).\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,\nMa, S., . . . Fei-Fei, L. (2015). ImageNet Large Scale Vi-\nsual Recognition Challenge. International Journal of Com-\nputer Vision (IJCV), 115(3), 211-252.\nSchroff, F., Kalenichenko, D., & Philbin, J. (2015). FaceNet:\nA uniﬁed embedding for face recognition and clustering. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) (p. 815-823).\nSpeer, R., Chin, J., & Havasi, C. (2017). ConceptNet 5.5:\nAn open multilingual graph of general knowledge. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence\n(p. 4444–4451).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N., . . . Polosukhin, I. (2017). Attention is all\nyou need. In Advances in Neural Information Processing\nSystems.\nYamins, D. L. K., Hong, H., Cadieu, C. F., Solomon, E. A.,\nSeibert, D., & DiCarlo, J. J. (2014). Performance-\noptimized hierarchical models predict neural responses in\nhigher visual cortex. Proceedings of the National Academy\nof Sciences, 111(23), 8619–8624.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7598980665206909
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6840548515319824
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6496875286102295
    },
    {
      "name": "Machine learning",
      "score": 0.6082504987716675
    },
    {
      "name": "Transformer",
      "score": 0.5305676460266113
    },
    {
      "name": "Artificial neural network",
      "score": 0.523779571056366
    },
    {
      "name": "Suite",
      "score": 0.4573471248149872
    },
    {
      "name": "Inductive bias",
      "score": 0.452256441116333
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.43751460313796997
    },
    {
      "name": "Deep learning",
      "score": 0.418023943901062
    },
    {
      "name": "Multi-task learning",
      "score": 0.1308014988899231
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ],
  "cited_by": 21
}