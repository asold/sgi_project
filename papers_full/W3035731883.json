{
    "title": "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks",
    "url": "https://openalex.org/W3035731883",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5072082849",
            "name": "Yekun Chai",
            "affiliations": [
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5011998179",
            "name": "Shuo Jin",
            "affiliations": [
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5010013507",
            "name": "Xinwen Hou",
            "affiliations": [
                "Chinese Academy of Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963088785",
        "https://openalex.org/W2907502844",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4295224299",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W2964189376",
        "https://openalex.org/W2767286248",
        "https://openalex.org/W2963344337",
        "https://openalex.org/W2956480774",
        "https://openalex.org/W2964213727",
        "https://openalex.org/W2970327629",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W4320930577",
        "https://openalex.org/W2964302946",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2982316857",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4394643672",
        "https://openalex.org/W2773734397",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2194775991"
    ],
    "abstract": "Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6887–6900\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n6887\nHighway Transformer: Self-Gating Enhanced Self-Attentive Networks\nYekun Chai† Shuo Jin‡ Xinwen Hou†\n†Institute of Automation, Chinese Academy of Sciences\n‡University of Pittsburgh\nchaiyekun@gmail.com shj42@pitt.edu\nAbstract\nSelf-attention mechanisms have made striking\nstate-of-the-art (SOTA) progress in various se-\nquence learning tasks, standing on the multi-\nheaded dot product attention by attending to\nall the global contexts at different locations.\nThrough a pseudo information highway , we\nintroduce a gated component self-dependency\nunits (SDU) that incorporates LSTM-styled\ngating units to replenish internal semantic im-\nportance within the multi-dimensional latent\nspace of individual representations. The sub-\nsidiary content-based SDU gates allow for the\ninformation ﬂow of modulated latent embed-\ndings through skipped connections, leading to\na clear margin of convergence speed with gra-\ndient descent algorithms. We may unveil the\nrole of gating mechanism to aid in the context-\nbased Transformer modules, with hypothesiz-\ning that SDU gates, especially on shallow lay-\ners, could push it faster to step towards subop-\ntimal points during the optimization process.\n1 Introduction\nSelf-attention mechanism has lately attracted ex-\ntensive interests due to its remarkable achievement\non a wide range of sequence modeling applications,\nincluding natural language processing such as neu-\nral machine translation (Vaswani et al., 2017; Ott\net al., 2018; Shaw et al., 2018), language model-\ning (LM) (Dai et al., 2019; Al-Rfou et al., 2019),\nself-supervised pretraining (Radford et al., 2018;\nDevlin et al., 2018; Lan et al., 2019); image genera-\ntion (Parmar et al., 2018); deep reinforcement learn-\ning (Zambaldi et al., 2018; Vinyals et al., 2019),\netc.\nHolding the great promise of deep neural net-\nworks in language and images, Transformer capi-\ntalizes on the stacked multi-headed self-attention\nmechanism based on the conventional encoder-\ndecoder architecture in a sequence-to-sequence\n(seq2seq) manner to learn the global soft signals\nwithout explicit recurrence mechanism. Multi-head\ndot product attention (MHDPA) not only underpins\nthe parallel training of multiple heads but captures\nlong-term dependencies across an arbitrarily long\ndistance within the same context. In which sepa-\nrated multiple heads independently draw sub-level\nattentions within the latent semantic sub-space of\na ﬁxed dimension, where different heads are pre-\nsumed to signal different meaning aspects implic-\nitly (Vaswani et al., 2017). Additionally, residual\nconnections between layers allow the deep tandem\nstack of multiple identical modules by impeding\ndegradation problem during training (He et al.,\n2016). Thus Transformer architectures take the\nplace of Recurrent Neural Networks (RNNs), es-\npecially Long Short-Term Memory (LSTM) net-\nworks (Hochreiter and Schmidhuber, 1997) to be\nthe model solution to learning sequential data.\nRecently, there have been plenty of works con-\ntending that gating mechanisms could play a vital\nrole or even entirely substitute RNNs or Transform-\ners to model language sequences. Dauphin et al.\n(2017) ﬁrstly claimed that non-recurrent networks\nare also highly competitive with conventional RNN-\ndominated models in LM. They proposed the hi-\nerarchical gated temporal convolution neural net-\nworks (CNNs) with Gated Linear Units (GLU) to\nreplace the recurrent connections in RNNs and\nachieved strong performance with faster training\nspeed. Gehring et al. (2017) integrated absolute\npositional embedding, multi-step attention, GLU,\nand residual connections into entirely convolutional\nmodels to outperform strong LSTM models in\nNMT and abstractive summarization tasks. Wu\net al. (2019) applied dynamic convolutions using\nshared softmax-normalized ﬁlters of depth-wise\non GLU-regulated inputs within a ﬁxed reception\nﬁeld rather than global contexts, challenging the\ncommon self-attention-dominated intuition.\n6888\nHowever, all of the models, as mentioned earlier,\nadopt stacked CNNs rather than self-attention net-\nworks (SAN) to attend to the global contexts. It is\nwell-known that CNNs are good at learning local-\nregion features rather than long-term dependency,\nwhile SANs are adept in attending global depen-\ndencies. Context-based self-attention can capture\nthe importance of relative relations under a valid\ncontext and is thus location-unaware. It focuses on\nthe object-wise attention distributions between any\ntwo words but ignores the fundamental importance\nof feature-wise information.\nIntuitionally, people need to consider not only\nthe global contextual dependency but the mean-\ning of individual words to comprehend the read-\ning materials better. Grounding on this, we apply\nself-gating approaches on Transformer blocks for\nseq2seq modeling that combines gating units with\nskip-connections and Transformers to jointly take\ninto account both the inner feature-wise importance\nand the relation-aware content-based attention dis-\ntribution.\nWe adopt the self-dependency gating approach\nto intrinsically draw a binary importance ratio of\nitself and decide how much information of each\nfeature to retain or remove. Our key contributions\nare:\n•to illustrate that our self-dependency units on\nshallow Transformer layers could expedite the\nconvergence speed during both the training\nand validation process without hyperparame-\nter tuning.\n•to support the claim that Transformer layers\nin different depth attend to information of\ndifferent aspects, wherein bottom layers fo-\ncus on local-range encodings. It substantiates\nthe argument that the bottom layers of SAN\ncan learn more in local contexts (Yang et al.,\n2018).\n•to empirically prove that self-gating mecha-\nnisms are complementary to recurrence mech-\nanisms in R-Transformer and Transformer-XL\ncomponents.\n2 Preliminaries\nThis section brieﬂy introduces the related back-\nground of Transformer and Highway Networks.\nSAN has been dominant in most SOTA sequence\nlearning models, whose basic components consist\nof stacked Transformers modules. We conduct\ncomparison experiments on the Transformer and\nits two variants, Transformer-XL (Dai et al., 2019)\nand R-Transformer (Wang et al., 2019).\n2.1 Multi-head Dot Product Attention\nScaled dot product attention (DPA) (Vaswani et al.,\n2017) computes global attention weights between\npairs within the context across an arbitrarily long\ndistance, which could allow the simultaneous train-\ning and space-saving, impeding the drawbacks of\nsequential dependency of RNNs.\nGiven the input word representation X ∈\nRL×dh, where Lis the sequence length, dis the\ninput dimension of each head and his the number\nof attention heads, DPA uses the linear projection\nto acquire the query Q, key K and value V. De-\nnoting splitted inputs for i-th head as Xi ∈RL×d,\nwhere i ∈{1,··· ,h}, single-head self-attention\ncan be calculated as:\nQi,Ki,Vi = XiWq,XiWk,XiWv (1)\nheadi = softmax\n(\nd−1/2QiK⊤\ni\n)\nVi (2)\nwhere learnable weights {Wq,Wk,Wv}∈ Rd×d,\nd−1/2 is a scaling factor to prevent the effect of\nlarge values. In LM tasks, attention weights be-\nfore softmax function are masked to only attend to\nhistory sequences.\nMHDPA (Fig 1a) linearly projects the single\nDPA into hheads and performs attention operation\nin parallel, to jointly learn different semantic mean-\nings of different subspaces (Vaswani et al., 2017).\nMHDPA can be calculated as:\nAtt(Q,K,V) = [head1 ◦···◦ headh] Wo (3)\nwhere ◦denotes the concatenation of hdifferent\nheads, Wo ∈Rdh×dh is the trainable weight.\n2.2 Transformer\nAbsolute Positional Encoding Transformer ap-\nplies sinusoidal timing signal as the absolute posi-\ntional encoding (PE) and directly element-wise add\nthe dense word embeddings E ∈RL×dh on the PE\nbefore feeding into Transformer modules:\nPE(pos,2i) = sin( pos\n100002i/d) (4)\nPE(pos,2i+1) = cos( pos\n100002i/d) (5)\nX = E + PE(E) (6)\nwhere ‘pos’ indicates the position of sequences,i\ndenotes the order along the embedding dimension.\n6889\nGiven input representations X, Transformer\ncomponents with a sternward Layer Normalization\n(LN) is:\nU = LN(X + Att(Q,K,V) (7)\nFFN(U) = FF\n(\nReLU(FF(U))\n)\n(8)\nO = LN(U + FFN(U)) (9)\nwhere Eq. 8 indicates the position-wise feed-\nforward networks (FFN), O ∈RL×dh represents\nthe output of transformer layer. FF denotes the\nfeed-forward fully-connected layer, ReLU is used\nas the non-linear activate function.\n2.3 Transformer-XL\nTransformer-XL (Dai et al., 2019) injected rela-\ntive PE and segment-level recurrence to provide\nhistorical information for LM tasks.\nRelative Positional Encoding Transformer-XL\ndecomposed the dot product calculation of\nMHDPA, merged terms with similar meanings of\npositional bias, and reduced trainable weights with\nglobal positional semantics. It incorporated partial\ntrainable parameters of relative sinusoidal PE in\nthe MHDPA operation.\nThe Relative PE Arel of Transformer-XL is:\na= Q⊤K (10)\nb= Q⊤Wk,R R (11)\nc= u⊤K (12)\nd= v⊤Wk,R R (13)\nArel(Q,K) = a+ b+ c+ d (14)\nwhere Wk,R ∈ Rd×d, {u,v} ∈Rd are train-\nable parameters. For each two positions i,j in\nthe segment, R is sinusoidal encoding matrices be-\ntween relative position i−j. The terms a,b,c,d in\nthe Eq. 10, 11, 12, 13 represent the content-based\naddressing, content-dependent positional biases,\nglobal biases between different positions and the\nglobal positional biases, respectively.\nSegment-level Recurrence In Transformer-XL,\nthe previous hidden states are cached and reused to\ninject the history information and attend to contexts\nbeyond a ﬁxed length through multi-layer stacks.\nThe MHDPA is computed as:\nMn−1\nτ =\nstop gradient\n  \nSG(Xn−1\nτ−1) ◦Xn−1\nτ\n(15)\nQ,K,V = Xn−1\nτ Wq, Mn−1\nτ Wk, Mn−1\nτ Wv\n(16)\nDPA(Q,K,V) = Arel(Q,K)V (17)\nwherein the key and value Mn−1\nτ concatenate the\nprevious memory Xn−1\nτ−1 with the current segment\ninputs Xn−1\nτ for the τ-th segment in the n-th layer,\nSGmeans no backpropagation through the tensor.\n2.4 R-Transformer\nR-Transformer (Wang et al., 2019) employed short-\nrange RNNs, termed localRNNs, to capture the\npositional information without explicit PEs. local-\nRNNs take the recurrent connections within a local\ncontext, and shift right with one position at each\ntime step. It can be seen as applying the RNN cells,\nsuch as LSTM, on the same receptive ﬁelds as the\nconvolutional ﬁlters along the sequence direction.\nX = localRNN(E) (18)\nO = Transformer-layer(X) (19)\nNone of the above Transformer models explicitly\nconsider the essential feature-wise information. We\naugment several gated units on the Transformer\nblock of the models above and empirically illustrate\nthe effectiveness of gating units on convergence\nacceleration.\n2.5 Highway Networks\nLet we deﬁne the non-linear transforms as H, T\nand C, Highway Network (Srivastava et al., 2015)\nis deﬁned as:\nO = H(X) ⊙T(X) + X ⊙C(X) (20)\nwhere T(·) and C(·) denote transform and carry\ngates to control the input transformation,⊙denotes\nthe Hadamard product.\n3 Gating Architecture\nLSTM-styled gate units have been proven to be ef-\nfective on sequence learning tasks (Dauphin et al.,\n2017; Gehring et al., 2017; Wu et al., 2019). We\nspontaneously wonder whether such gating mech-\nanisms could help when augmenting the Trans-\nformer components.\n6890\nX\nLinear\nMat Mul\nScale\nSoftmax\nLinear Linear\nMat Mul\nLinear\n(a) Multi-head dot product self-attention\nLinear\nX\nLinear\ngate\npoint-wise \nproduct\nZ (b) Self-dependency units\nX\nMHDPASDU\nadd & norm\nFFNSDU\nadd & norm\nU (c) SDU-augmented Transformer block\nFigure 1: Illustration of MHDPA, SDU and SDU-enhanced Transformer block.\n3.1 Self-Dependency Units\nSimilar to GLU (Dauphin et al., 2017) that adopts\nthe inputs as sigmoidal gates, we apply the Self-\nDependency Units (SDU) by taking full inputs\nas their respective self gates and computing the\nelement-wise product upon themselves (Fig 1b).\nT(X) = Ψ(XW1 + b1) (21)\nSDU(X) = T(X) ⊙(XW2 + b2) (22)\nwhere T(X) indicates the transform gate, Ψ is\nthe gate function that conﬁne the linear projec-\ntion into a ﬁxed range, {W1,W2}∈ Rd×d and\n{b1,b2}∈ Rd are trainable parameters.\nThe element-wise gating function Ψ takes\nsigmoidal-curve functions to regulate the point-\nwise weights within a ﬁxed region, which have\na side effect of relative normalization. Speciﬁcally,\nthe sigmoid function σ(x) = 1 /(1 + exp(−x))\nand its rescaled version tanh(x) = 2 σ(2x) −\n1,where x∈R.\nWe interpret the tanh function as an update gate,\nwhich can restrict the importance range into be-\ntween -1 and 1, while the σ function bears a re-\nsemblance to the input gate in LSTMs to modulate\nhow much information to retain at the feature-wise\nlevel.\n3.2 Pseudo-highway Connection\nMHDPA computes the multi-headed pairwise atten-\ntion along the sequence dimension by measuring\nthe distance between each word. It might overlook\nthe fundamental importance of individual features.\nRather than replacing MHDPA as gating and con-\nvolution operations in dynamic convolutions (Wu\net al., 2019), we simply add a new branch of inputs\nto enrich the representations of residual connected\nMHDPA with augmented gating-modiﬁed encod-\nings. The gated units are also supplemented on\nFFN modules to provide additional self-adaptive\ninformation ﬂow ( Fig 1c).\nFrom other perspectives, SDU can be considered\nas a self-dependency non-linear activation func-\ntion with dynamic adaptation. The self-gating aug-\nmented Transformer module is calculated as:\nU = LN\n(\nX + Att(Q,K,V)\n+ SDU(X)\n) (23)\nO = LN\n(\nU + FFN(U) + SDU(U)\n)\n(24)\nwhere U and O represent the intermediate repre-\nsentation and outputs.\nPseudo-highway Transformer When we take σ\ngate as Ψ, we can have the similar format as high-\nway networks:\n∇[f(X) ⊙σ(g(X))] =\ntransform gate\n  \nσ(g(X)) ⊙∇f(X)\n+\ncarry gate\n  (\n1 −σ(g(X))\n)(\nσ(g(X)) ⊙f(X)\n)\n(25)\nwhere the σ(.) can be seen as the transform gate,\nwhile (1 −σ(.)) can be seen as the carry gate. This\ncould be regarded as a form of highway networks.\n3.3 Variant Gated Connections\nHighway Gate Similar to the highway net-\nworks (Srivastava et al., 2015), let T(X) signal\nthe transform gate and (1 −T(X)) be the carry\ngate, we have the highway-network-like structures\nby regulating the encoding f(X) with transform\ngate and controling X with carry gate. This is quite\n6891\nsimilar to highway networks:\nT(X) = σ(XW1 + b1) (26)\nf(X) = XW2 + b2 (27)\no(X) = (1 −T(X)) ⊙X\n+ T(X) ⊙f(X) (28)\nU = LN\n(\no(X) + Att(Q,K,V)\n)\n(29)\nwhere Eq. 28 is the element-wise summation of\nhighway networks, o(·) represents the intermediate\noutput.\nGated MHDPA Similar to previous highway\ngates, we can apply the carry gate and transform\ngate on the attention and FFN units respectively.\nThus we have:\no(X) = (1 −T(X)) ⊙Att(Q,K,V)\n+ T(X) ⊙f(X) (30)\nU = LN\n(\no(X) + X\n)\n(31)\nSuch gates can be regarded as dynamically adjust-\ning the information ﬂow between the feature-wise\nrepresentations and SANs (Eq. 30).\n4 Experiments and Results\nWe apply the gating mentioned above on Trans-\nformer variants described in section 2 on LM tasks\nand respectively make comparisons in terms of both\nthe convergence process and the ﬁnal performance.\nFor fairness, we apply SDU components based on\nthe same hyperparameters as the original paper 1.\nOur code is available2.\n4.1 vs. Transformer / R-Transformer\nWe ﬁrst evaluate the gating units on the Penn Tree-\nBank (PTB) LM task. The SDU gates are added on\nEq. 7, 9 for each Transformer block. All models in\nthis section are trained on single NVIDIA Titan Xp\nGPU.\n4.1.1 Char-level PTB\nHyperparameter and Training The gated com-\nponents are evaluated on character-level PTB LM\ntasks (see Appendix A.1 for hyperparameter set-\ntings). The loss and bit per character (bpc) provide\nthe metrics to evaluate the trained models. All\nmodels are trained with 100 epochs.\n1Some results of baselines are slightly lower than those re-\nported in original papers using the code obtained from authors\nbut are within the limits of experimental error and variance.\n2https://github.com/cyk1337/\nHighway-Transformer\n0 20 40 60 80 100\nEpochs\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75loss\nTraining and validation loss\n+tanh SDU train loss\n+tanh SDU val loss\nTransformer-L3 train loss\nTransformer-L3 val loss\n+sigmoid SDU train loss\n+sigmoid SDU val loss\n0 20 40 60 80 100\nEpochs\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0bpc\nTraining and validation bpc\n+tanh SDU train bpc\n+tanh SDU val bpc\nTransformer-L3 train bpc\nTransformer-L3 val bpc\n+sigmoid SDU train bpc\n+sigmoid SDU val bpc\nFigure 2: The 3-layer Transformer’s curve of train-\ning and evaluation performance on character-level PTB\nLM.\n0 20 40 60 80 100\nEpochs\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8loss\nTraining and validation loss\n+tanh SDU val loss\n+tanh SDU train loss\nR-Transformer-L3 train loss\nR-Transformer-L3 val loss\n+sigmoid SDU val loss\n+sigmoid SDU train loss\n0 20 40 60 80 100\nEpochs\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8bpc\nTraining and validation bpc\n+tanh SDU val bpc\n+tanh SDU train bpc\nR-Transformer-L3 val bpc\nR-Transformer-L3 train bpc\n+sigmoid SDU val bpc\n+sigmoid SDU train bpc\nFigure 3: The 3-layer RT’s curve of training and evalu-\nation performance on character-leval PTB LM task.\nResults of Transformer As shown in Table 1,\nall the gating-enhanced models conspicuously sur-\npass the performance of the loss and perplexity\nover the baseline on both training and validating\nset, revealing the positive inﬂuence of self-gating\nunits in supporting Transformer blocks. Further-\nmore, Fig. 2 presents the beneﬁcial effect of gating\nunits in accelerating the convergence process in\nboth training and evaluation set by a clear margin,\nvalidating the accumulative effect that our gating\nunits bring out. In which SDUs with tanh gates\n(8.76% improvement) outperform the counterpart\nwith sigmoid gates (8.2% improvement) in terms\nof the ﬁnal perplexity on the test set.\nmodel eval loss eval ppl test loss test ppl\nT-L3 1.068 1.541 1.036 1.495\n+σSDU 0.9776 1.410 ⇓ 0.950 1.371 ⇓\n+tanhSDU 0.9714 1.401 ⇓ 0.945 1.364 ⇓\nTable 1: Performance of 3 Layer Transformers and\nSDU components on char-level PTB LM task. The best\nperformance is marked bold.\nResults of RT It can be seen in Fig. 3 that supple-\nmenting SDUs can increase the speed of the conver-\ngence process of training and evaluation, strength-\nening our previous claim. As for the ﬁnal perplexity\non the test set, σ-gate SDUs could achieve better\nthan baselines while tanh-gate SDUs perform a\nbit worse, as shown in Table 2. The inﬂuence of\nσ-gate SDUs might be owing to that σ function\ncompresses the input into the dense non-zero ratios\nwithin (0,1) and results in stable variation range.\nIn contrast, the zero-centered property and possibly\n6892\nzeroed values oftanh may cause the corresponding\nunits easier to be trapped into the premature conver-\ngence during the training process. Besides, σgates\nhave been empirically proved to be more stable\nthan tanh gates in the follow-up experiments.\nmodel eval loss eval ppl test loss test ppl\nRT-L3 0.8896 1.283 0.867 1.250\n+tanhSDU 0.9096 1.312 0.883 1.274\n+σSDU 0.8863 1.279 ⇓ 0.863 1.245 ⇓\nTable 2: Performance of 3 Layer R-Transformers and\nSDU components on char-level PTB LM task.\n4.1.2 Word-level PTB\nHyperparameter and Training We compare\nthe performance between 3-layer Transformer and\nR-Transformer (RT) with and without SDU gating\nunits. Appendix A.1 illustrates the hyperparame-\nter setup. All experiments are conducted with 100\nepochs, and the loss and perplexity (ppl) values on\nthe development set serve as evaluation metrics.\n0 20 40 60 80 100\nEpochs\n4.5\n5.0\n5.5\n6.0\n6.5loss\nTraining and validation loss\n+tanh SDU train loss\n+tanh SDU val loss\nTransformer-L3 train loss\nTransformer-L3 val loss\n+sigmoid SDU train loss\n+sigmoid SDU val loss\n0 20 40 60 80 100\nEpochs\n100\n200\n300\n400\n500\n600\n700ppl\nTraining and validation ppl\n+tanh SDU train ppl\n+tanh SDU val ppl\nTransformer-L3 train ppl\nTransformer-L3 val ppl\n+sigmoid SDU train ppl\n+sigmoid SDU val ppl\nFigure 4: Loss and perplexity of 3-layerTransformers\non the word-level PTB training and validation set.\nmodel eval loss eval ppl test loss test ppl\nT-L3 4.937 139.4 4.87 130.43\n+σSDU 4.934 138.9 ⇓ 4.87 130.30 ⇓\n+tanhSDU 5.001 148.5 4.94 139.53\nTable 3: Performance of 3-layer basicTransformer(T-\nL3) and SDU components on word-level PTB LM.\nResults of Transformer Figure 4 shows a no-\nticeable downward trend on the evaluation perfor-\nmance (i.e., the validation loss and perplexity) of\nthe attention model with tanh and sigmoid func-\ntions over the beginning 30 epochs, again indicat-\ning the convergence acceleration effect of our gated\nunits. Also, σ-gate enhanced models outmatches\nthe baseline on the test perplexity, but models with\ntanh gates reach into a plateau untimely. As for\nthe training curves, Transformers with SDUs have\nseen a remarkably sharper fall in comparison with\nthe baseline model over all the training period.\nResults of RT As in Fig. 5 and Table 4, models\nwith SDUs entirely surpass the performance of the\nbaseline involving both the convergence speed and\nperplexity on the test set. Similar to the word-\nlevel R-Transformer, tanh-gate SDUs behave a bit\nbetter than the counterpart with sigmoid gates, both\nshowing stable curvatures of convergence.\nmodel eval loss eval ppl test loss test ppl\nRT-L3 4.58 97.63 4.53 92.31\n+σSDU 4.53 92.91 ⇓ 4.48 87.88 ⇓\n+tanhSDU 4.50 89.97 ⇓ 4.44 84.92 ⇓\nTable 4: The performance of 3-layer R-Transformers\n(RT-L3) and SDU components on word-level PTB LM.\n0 20 40 60 80 100\nEpochs\n4.0\n4.5\n5.0\n5.5\n6.0loss\nTraining and validation loss\n+tanh SDU train loss\n+tanh SDU val loss\n+sigmoid SDU train loss\n+sigmoid SDU val loss\nR-transformer train loss\nR-transformer val loss\n0 20 40 60 80 100\nEpochs\n50\n100\n150\n200\n250\n300\n350\n400ppl\nTraining and validation ppl\n+tanh SDU train ppl\n+tanh SDU val ppl\n+sigmoid SDU train ppl\n+sigmoid SDU val ppl\nR-transformer train ppl\nR-transformer val ppl\nFigure 5: Loss and perplexity of 3-layer RT on the\nword-level PTB training and validation sets.\n4.2 Sub-total\nTo sum up, gating units have empirically expe-\ndited the convergence of Transformer blocks due to\nthe enrichment of self-regulated features with skip-\nconnections. It can be seen that σ-gate presents the\nstability to bear a hand to reach the plateau without\nhurting the test performance, but tanh-gate seems\nto be task- and data-dependent and could be better\nthan σ-gate SDUs in some circumstances. We can\nsee that our proposed gated units are complemen-\ntary to the recurrent connections in RNNs and can\nboost the performance based onlocalRNN-encoded\nrepresentations.\nIn the following experiment, we check whether\nit is necessary to apply gates on all the layers and\nprobe the effect of SDU variants (i.e., “ highway\ngate” and “gate MHDPA”). Due to the small size of\nPTB, we experiment on a larger LM datasetenwik8\nand adopt the impressive Transformer-XL, one of\nthe vital variant structures used in XLNet (Yang\net al., 2019).\n6893\n4.3 vs. Transformer-XL\nHyperparameter See Appendix A.3 for detailed\nhyperparameter settings.\n4.3.1 Results of 6-layer Transformer-XL\nIt is noticeable that Transformer-XL models with\ndifferent gating variants all outperform the base-\nline with different margins in terms of both perfor-\nmance and convergence speed, as shown in Table 5.\nFig. 6 shows that SDUs beneﬁt the convergence\nand validation performance compared with base-\nlines. Among which σ-gate SDUs ranked top by\nachieving 3.1% improvement of bpc on the dev set,\nfollowed by gates with tanh, gated MHDPA, high-\nway gate with 2.7%, 1.8%, 1.7% advance respec-\ntively. We attribute such improvements to the aug-\nmented reﬁned representations learned by our gated\nunits, preventing the basic self-attention blocks\nfrom purely considering the contextual dependency.\nIt is also illustrated that SDUs do not conﬂict with\nrecurrence mechanisms in Transformer-XL.\n0\n10000 20000 30000 40000\nsteps\n1.0\n1.5\n2.0\n2.5loss\nTraining loss\nL6-Transformer-XL\n+tanh SDU\n+sigmoid SDU\n0\n10000 20000 30000 40000\nsteps\n1.5\n2.0\n2.5\n3.0\n3.5bpc\nTraining bpc\nL6-Transformer-XL\n+tanh SDU\n+sigmoid SDU\n0\n10000 20000 30000 40000\nsteps\n1.0\n1.2\n1.4\n1.6\n1.8loss\nValidation loss\nL6-Transformer-XL \n+tanh SDU \n+sigmoid SDU \n0\n10000 20000 30000 40000\nsteps\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50bpc\nValidation bpc\nL6-Transformer-XL \n+tanh SDU \n+sigmoid SDU \nFigure 6: The comparison between 6-layer\nTransformer-XL with adding different SDUs.\n4.3.2 Ablation Study\n6-layer Transformer-XL To probe whether it is\nrequired to augment SDUs on each Transformer\nlayer, we supplement gates on layer 1-3, layer 3-6,\nand layer 1-6 but removing gates on FFN compo-\nnents (denoted “\\FFN”) as in Table 5 (see Fig. 8 in\nAppendix B for detailed convergence curvatures).\nWe ﬁnd that supplementing tanh-gates on the bot-\ntom three layers contribute most to the overall per-\nformance while tanh-gates on the top three layers\ncould hinder the test set performance. Low-level\nTransformer blocks can capture the information\nfrom localness while top layers usually focus on the\nglobal long-range dependency (Yang et al., 2018).\nmodel eval loss eval bpc test loss test bpc\nL6-XL 0.8843 1.276 0.86 1.24339\n+tanhSDU 0.8602 1.241 ⇓ 0.84 1.21424⇓\n+σSDU 0.8577 1.237⇓ 0.84 1.21123 ⇓\n+highway gate 0.8692 1.254⇓ 0.85 1.22177 ⇓\n+gated MHDPA 0.8682 1.253⇓ 0.85 1.22398 ⇓\nAblation study\n+tanhL1-6\\FFN 0.8720 1.258⇓ 0.85 1.22866 ⇓\n+tanhL1-3 0.8660 1.249⇓ 0.85 1.22039 ⇓\n+tanhL3-6 0.8852 1.277 ⇓ 0.86 1.24420 ⇓\n+σL1-6\\FFN 0.8752 1.263 ⇓ 0.85 1.23332 ⇓\n+σL1-3 0.8792 1.268 ⇓ 0.86 1.23589 ⇓\n+σL3-6 0.8843 1.276 ⇓ 0.86 1.24261 ⇓\nTable 5: Results of 6-layer Transformer-XL(L6-XL)\nand augmented SDUs with different settings. “+σ” L1-\n6\\FFN represents adding σ-SDUs on MHDPAs of 1-st\nto 6-th layers but not on FFN sublayers.\nThus gates on bottom layers could aid in learn-\ning syntax and superﬁcial representations to some\nextent. It also indicates that our gates may be bene-\nﬁcial for the encoding of low-level ﬁne-granularity\nrepresentations rather than semantic meaning regu-\nlation on high-level layers.\n12-layer Transformer-XL Previous experi-\nments are all conducted on shallow models and\nillustrate the positive effects. To investigate the\nperformance on deep stacked models, we further\nextend our trials to 12-layer Transformer-XL.\nAll hyperparameters are the same as 6-layer\nTransformer-XL, as shown in Appendix A.3. Each\nModel is trained 400k steps for more than 100\nhours on 4 x GeForce 2080Ti GPUs in parallel.\n0\n50000100000150000200000250000300000350000400000\nsteps\n0.8\n1.0\n1.2loss\nTraining loss\nbaseline\n+tanh L1-2\n0\n50000100000150000200000250000300000350000400000\nsteps\n1.00\n1.25\n1.50\n1.75bpc\nTraining bpc\nbaseline\n+tanh L1-2\n0\n50000100000150000200000250000300000350000400000\nsteps\n0.8\n0.9\n1.0loss\nValidation loss\nbaseline \n+tanh L1-2 \n0\n50000100000150000200000250000300000350000400000\nsteps\n1.2\n1.4bpc\nValidation bpc\nbaseline \n+tanh L1-2 \nFigure 7: The comparison between 12-layer\nTransformer-XL with and without tanh gated\nunits on bottom two layers.\nThe experimental results illustrate that SDU\ncomponents have contributed to expediting the con-\nvergence during training (see Fig. 9 and 10 in Ap-\npendix C for details). But supplementing gated\nunits on each Transformer block could encounter\nthe premature convergence phenomenon. It is also\nobserved that adding the bottom few layers with\ngated units could strengthen the convergence pro-\n6894\nmodel eval loss eval bpc test loss test bpc\nL12-XL 0.7554 1.090 0.74 1.07160\nAblation study\n+tanhL1-12 0.7919 1.143 0.78 1.12797\n+tanhL1-6 0.7623 1.100 0.75 1.08234\n+tanhL1-3 0.7558 1.090 0.74 1.07140 ⇓\n+tanhL1-2 0.7548 1.089⇓ 0.74 1.06904⇓\n+tanhL1 0.7549 1.089⇓ 0.74 1.06960 ⇓\n+tanhL6-12 0.7572 1.092 0.74 1.07313\n+tanh\\FFN 0.7734 1.116 0.76 1.09920\n+σL1-12 0.7752 1.118 0.77 1.10462\n+σL1-6 0.7635 1.101 0.75 1.08283\n+σL1-3 0.7580 1.094 0.74 1.07383\n+σL1-2 0.7552 1.090 0.74 1.07148 ⇓\n+σL1 0.7557 1.090 0.74 1.07157 ⇓\n+σL6-12 0.7585 1.094 0.75 1.07607\n+σ\\FFN 0.7647 1.103 0.75 1.08652\n+highway gate 0.7784 1.120 0.77 1.10922\n+gated MHDPA 0.7741 1.117 0.76 1.10292\nTable 6: Final results of 12-layer Transformer-XL\n(XL-L12) and augmented SDUs with different settings.\ncess without impeding the ﬁnal performance, as\nshown in Table 6. It is observed from Fig. 7 that\ntanh-gates on the bottom two layers promote the\nconvergence process and further improve the bpc\nperformance on the dev and test set.\nInterestingly, the performance does not follow a\npositive correlation with the increase of gated layer\nnumbers. We can see that enriching the bottom 2\nlayers with tanh and σ gated functions (denoted\n“+tanh L1-2” and “+σL1-2” in Table 6) could im-\npressively beneﬁt for the convergence on both train-\ning and evaluation process and even marginally in-\ncrease the ﬁnal test bpc (see Fig. 9 and Fig. 10 in\nAppendix C for details). Therefore, the lower lay-\ners beneﬁt more from our proposed gated units than\nhigher layers, again illustrating that SDUs could en-\nhance feature-wise information on shallow layers\nof deep-stacked Transformer components.\n4.4 Gating Mechanism Analysis\nIt can be concluded that gating units could boost\nthe convergence, especially on low-level layers.\nEnhancing the bottom layers of deep-stacked mod-\nels may result in faster convergence of optimiza-\ntion. This may be owing to that SDU gates can\nenrich the original representations with adaptive\nself-dependency encodings. The ﬁnal hidden state\ncan be regarded as a revised representation that\nincorporating additional self-attentive features.\nMeanwhile, we ﬁnd that supplementing SDU\ngates does not increase much of the time cost in\ncomparison with baselines. Instead, the total run-\nning time of each experimental setting is quite simi-\nlar. We summarize the training time costs of 6-layer\nTransformer-XL as table .7.\nmodel time cost (hour)\nxl-L6 21.16\n+tanhSDU 21.45\n+σSDU 21.87\n+ highway gate 21.93\n+gated MHDPA 21.10\nTable 7: Summary of training time costs of 6-layer\nTransformer-XL.\nIt is argued that low-level transformers learn the\nlocal-region information while high-level layers\npay more attention to global dependencies (Yang\net al., 2018). Our experimental results could ver-\nify that gated representation on bottom layers can\nstrengthen the performance by introducing addi-\ntional gated encodings on localness.\nFurther, the visualization of learned gate bias pa-\nrameters of 6-layer and 12-layer models, as shown\nin Fig. 11 in Appendix D.1, presenting the layer\nseparation with the increase of layer depth. It has\nseamlessly veriﬁed our previous hypothesis that\nSDU on shallow layers could promote the learning\nprocess and attend to different information with top\nlayers. The scatter plot of Fig. 12 in Appendix D.2\nindicates that gates on different sublayers learn\nfrom different aspects in the identical representa-\ntion space.\nSDUs calculate the output by regulating the in-\nformation ﬂow of inputs conditioned on themselves.\nGiven the hidden dimension of d, the additional\ncost of trainable parameters on each SDU unit in\nour experiments is O(2d(d+ 1)). Meanwhile, con-\nvolutions along the sequence direction can substi-\ntute fully-connected feedforward SDU to curtail\nthe extra parameter cost. Such gating units equip\ngood scalability to attach to different Transformer\nstructures with only minor modiﬁcation of imple-\nmentation.\nThe gradient of our SDU components is:\n∇[f(x) ⊙Ψ(g(x))] = ∇f(x) ⊙Ψ(g(x)) (32)\n+ f(x) ⊙∇Ψ(g(x)) (33)\nwhere f,g are linear projections and Ψ takes tanh\nor σfunction. The addition operation of two terms\nprovides an unimpeded information ﬂow, which\ncan be regarded as a multiplicative skip connec-\ntion (Dauphin et al., 2017) while the second term is\nusually vanishing due to the derivative of the gating\n6895\nfunction Ψ. Based on the experimental results, we\nhypothesize that it could accelerate the optimiza-\ntion process to move towards a local minimum.\n5 Related Work\nIn recent years, there have been plenty of works\nadopting gating units into CNNs to help learn\nsequential information. Dauphin et al. (2017)\nproposed stacked gated CNNs by incorporating\nGLUs into the 1-dimensional convolution opera-\ntion, achieving the competitive results in compar-\nison to recurrent models on LM tasks. Based on\nthis, Gehring et al. (2017) augmented the attention\nmechanism together with GLUs on the convolu-\ntional structures, also surpassing the deep LSTMs\non NMT tasks. Recently, dynamic convolutions\nwere used to replace MHDPA components in Trans-\nformers entirely and also get the impressive results\non the WMT-14 dataset (Wu et al., 2019).\nAmounts of works employed gating mecha-\nnisms to modulate self-attention sublayers. Gated-\nAttention Reader (Dhingra et al., 2016) introduced\ngated attention by computing gates on the query\nencoding to interact with document representations\nfor reading comprehension. Zhang et al. (2018)\nreplaced the ﬁrst layer of Transformer decoding\nstacks with an average attention layer by comput-\ning forget gates using averaged preceding contex-\ntual encodings to regulate the current state infor-\nmation. Distance-based SAN (Im and Cho, 2017)\nand DiSAN (Shen et al., 2018) put a fusion gate\nto aggregate the representations after the multi-\ndimensional self-attention block for natural lan-\nguage inference. Lai et al. (2019) proposed a gated\nself-attention memory network with aggregated in-\nteractions between input sequences and context\nvectors for answer selection of question answering.\nNotably, our SDU bears a resemblance to the\nactivation Swish (Ramachandran et al., 2017) in\nterms of the equation format. Both of them use\nthe sigmoidal function and self-gating mechanism.\nHowever, Swish controls the input gated on itself in\na tandem way while the proposed SDU applies the\ngate after a linear projection and performs using a\nshunt connection in Transformer stacks.\n6 Conclusion and Future Work\nGating-enhanced architecture enjoys both the ad-\nvantage of MHDPA and self-regulated gating mech-\nanism, allowing for the pseudo-highway informa-\ntion ﬂow for better convergence by elastically intro-\nducing a few trainable parameters. It outperforms\nor matches the performance of common Trans-\nformer variants without hyperparameter tuning. It\nis empirically proved that self-gating units on shal-\nlow layers could provide more internal represen-\ntations of importance and signiﬁcantly beneﬁt for\nconvergence. This also supports the argument that\ndifferent levels of Transformer components attend\nto different semantic aspects while lower levels pay\nmore attention to local regions. In the future, it\nis necessary to interpret the semantics that Trans-\nformer layers in different depths can convey, which\nis beneﬁcial for the computing-efﬁciency.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2019. Character-level lan-\nguage modeling with deeper self-attention. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 33, pages 3159–3166.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nYann N Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2017. Language modeling with\ngated convolutional networks. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 933–941. JMLR. org.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBhuwan Dhingra, Hanxiao Liu, Zhilin Yang,\nWilliam W Cohen, and Ruslan Salakhutdinov.\n2016. Gated-attention readers for text comprehen-\nsion. arXiv preprint arXiv:1606.01549.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1243–1252. JMLR. org.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\n6896\nJinbae Im and Sungzoon Cho. 2017. Distance-based\nself-attention network for natural language infer-\nence. arXiv preprint arXiv:1712.02047.\nTuan Lai, Quan Hung Tran, Trung Bui, and Daisuke\nKihara. 2019. A gated self-attention memory\nnetwork for answer selection. arXiv preprint\narXiv:1909.09696.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. arXiv preprint arXiv:1806.00187.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image transformer. arXiv preprint\narXiv:1802.05751.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nPrajit Ramachandran, Barret Zoph, and Quoc V Le.\n2017. Searching for activation functions. arXiv\npreprint arXiv:1710.05941.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. arXiv preprint arXiv:1803.02155.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nShirui Pan, and Chengqi Zhang. 2018. Disan: Di-\nrectional self-attention network for rnn/cnn-free lan-\nguage understanding. In Thirty-Second AAAI Con-\nference on Artiﬁcial Intelligence.\nRupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\nSchmidhuber. 2015. Highway networks. arXiv\npreprint arXiv:1505.00387.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czar-\nnecki, Micha ¨el Mathieu, Andrew Dudzik, Juny-\noung Chung, David H Choi, Richard Powell, Timo\nEwalds, Petko Georgiev, et al. 2019. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement\nlearning. Nature, pages 1–5.\nZhiwei Wang, Yao Ma, Zitao Liu, and Jiliang\nTang. 2019. R-transformer: Recurrent neural\nnetwork enhanced transformer. arXiv preprint\narXiv:1907.05572.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. arXiv\npreprint arXiv:1901.10430.\nBaosong Yang, Zhaopeng Tu, Derek F Wong, Fandong\nMeng, Lidia S Chao, and Tong Zhang. 2018. Mod-\neling localness for self-attention networks. arXiv\npreprint arXiv:1810.10182.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nVinicius Zambaldi, David Raposo, Adam Santoro, Vic-\ntor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls,\nDavid Reichert, Timothy Lillicrap, Edward Lock-\nhart, et al. 2018. Deep reinforcement learning with\nrelational inductive biases.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accel-\nerating neural transformer via an average attention\nnetwork. arXiv preprint arXiv:1805.00631.\n6897\nA Experimental Setup Details\nA.1 Hyperparameter Settings for RT on\nChar-level PTB\nFor RT on char-level PTB, we adopt the batch\nsize of 16, gradient clipping with maximum L2\nnorm of 0.15, layer number of 3, hidden dimen-\nsion of 512, the sequence length of 400 in char-\nlevel, the dropout rate for sublayer connection of\n0.15, 8 heads for MHDPA, the initial learning\nrate of 2, SGD optimizer with linear decay, layer\nnumber of 3 in both Transformer and RT models.\nWeights are initialized with uniform distribution\nw ∼U(−0.1,0.1) and biases are all initialized as\n0s. The size of GRU cells in localRNNs is set to 7\nin RT.\nA.2 Hyperparameter Settings for RT on\nWord-level PTB\nWe use the dropout rates of 0.35 and 0.15 for sub-\nlayer connections and word embeddings, the initial\nlearning rate of 2, gradient clipping with the max-\nimum L2 norm of 0.35, the hidden dimension of\n128, 8-head attention, sequence length of 80 in both\nTransformer and RT. The weights are initialized\nwith uniform distribution U(−0.01,0.01), and the\nbiases are constant 0s. The optimizer is stochastic\ngradient descent (SGD) with annealed decay. The\nlocalRNN context size for LSTM cells is set to 9\nin RT.\nA.3 Hyperparameter Settings for\nTransformer-XL on enwik8\nWe use layer number of 6, 8 heads for MHDPA\nwith hidden size 64 for each head, hidden size of\n2,048 in FFN components, the dropout rate of 0.1 in\nFFN, embedding size of 512, learning rate 0.00025,\nmemory length of 512, batch size of 22, Adam op-\ntimizer without the warm-up strategy. We initialize\nweights under the Gaussian N(0,1) and biases as\n0s.\nB Experimental Results of 6-layer\nTransformer-XL\nFig 8 displays all the experimental curvatures with\ndifferent SDU settings on 6-layer Transformer-XL.\n0\n5000 10000 15000 20000 25000 30000 35000 40000\nsteps\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50loss\nTraining loss\n+gated MHDPA\n+sigmoid SDU \\FFN\n+tanh SDU L3-6\n+tanh SDU L1-3\nL6-Transformer-XL\n+highway gate\n+tanh SDU\n+tanh SDU \\FFN\n+sigmoid SDU L3-6\n+sigmoid SDU L1-3\n+sigmoid SDU\n0\n5000 10000 15000 20000 25000 30000 35000 40000\nsteps\n1.5\n2.0\n2.5\n3.0\n3.5bpc\nTraining bpc\n+gated MHDPA\n+sigmoid SDU \\FFN\n+tanh SDU L3-6\n+tanh SDU L1-3\nL6-Transformer-XL\n+highway gate\n+tanh SDU\n+tanh SDU \\FFN\n+sigmoid SDU L3-6\n+sigmoid SDU L1-3\n+sigmoid SDU\n0\n5000 10000 15000 20000 25000 30000 35000 40000\nsteps\n1.0\n1.2\n1.4\n1.6\n1.8loss\nValidation loss\n+gated MHDPA \n+sigmoid SDU \\FFN \n+tanh SDU L3-6 \n+tanh SDU L1-3 \nL6-Transformer-XL \n+highway gate \n+tanh SDU \n+tanh SDU \\FFN \n+sigmoid SDU L3-6 \n+sigmoid SDU L1-3 \n+sigmoid SDU \n0\n5000 10000 15000 20000 25000 30000 35000 40000\nsteps\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6bpc\nValidation bpc\n+gated MHDPA \n+sigmoid SDU \\FFN \n+tanh SDU L3-6 \n+tanh SDU L1-3 \nL6-Transformer-XL \n+highway gate \n+tanh SDU \n+tanh SDU \\FFN \n+sigmoid SDU L3-6 \n+sigmoid SDU L1-3 \n+sigmoid SDU \nFigure 8: The performance of 6-layer Transformer-\nXL experiments with various settings of gated units.\nC Experimental Results of 12-layer\nTransformer-XL\nC.1 Transformer-XL v.s. +tanh Gates\nFig. 9 shows the curve of tanh-gate enhanced\nTransformer-XL during the training and evalua-\ntion process. Adding tanh-gates on the ﬁrst few\nlayers greatly boost the convergence performance\nin both the training and evaluation process. Among\nwhich “+tanh L1-2” presents a rapid convergence\ntrend and marginally outperforms the baseline per-\nformance.\n0\n50000100000150000200000250000300000350000400000\nsteps\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3loss\nTraining loss\n+tanh L1\n+tanh L1-12\n+tanh L1-12\\FFN\nbaseline\n+tanh L1-2\n+tanh L1-3\n+tanh L6-12\n+tanh L1-6\n0\n50000100000150000200000250000300000350000400000\nsteps\n1.0\n1.2\n1.4\n1.6\n1.8bpc\nTraining bpc\n+tanh L1\n+tanh L1-12\n+tanh L1-12\\FFN\nbaseline\n+tanh L1-2\n+tanh L1-3\n+tanh L6-12\n+tanh L1-6\n0\n50000100000150000200000250000300000350000400000\nsteps\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05loss\nValidation loss\n+tanh L1 \n+tanh L1-12 \n+tanh L1-12\\FFN \nbaseline \n+tanh L1-2 \n+tanh L1-3 \n+tanh L6-12 \n+tanh L1-6 \n0\n50000100000150000200000250000300000350000400000\nsteps\n1.1\n1.2\n1.3\n1.4\n1.5bpc\nValidation bpc\n+tanh L1 \n+tanh L1-12 \n+tanh L1-12\\FFN \nbaseline \n+tanh L1-2 \n+tanh L1-3 \n+tanh L6-12 \n+tanh L1-6 \nFigure 9: The performance of 12-layer Transformer-\nXL experiments augmenting tanh gated units.\n6898\nC.2 Transformer-XL v.s. +sigmoid Gates\nFig. 10 illustrates the performance of Transformer-\nXL augmented with σ gates. The sigmoid-gated\nTransformer-XL has showed a similar trend as\ntanh gates in Fig. 9.\n0\n50000100000150000200000250000300000350000400000\nsteps\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3loss\nTraining loss\n+sigmoid L1-6\n+sigmoid L0\n+sigmoid L6-12\nbaseline\n+sigmoid L1-12\\FFN\n+sigmoid L1-2\n+sigmoid L1-3\n+sigmoid L1-12\n0\n50000100000150000200000250000300000350000400000\nsteps\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8bpc\nTraining bpc\n+sigmoid L1-6\n+sigmoid L0\n+sigmoid L6-12\nbaseline\n+sigmoid L1-12\\FFN\n+sigmoid L1-2\n+sigmoid L1-3\n+sigmoid L1-12\n0\n50000100000150000200000250000300000350000400000\nsteps\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05loss\nValidation loss\n+sigmoid L1-6 \n+sigmoid L0 \n+sigmoid L6-12 \nbaseline \n+sigmoid L1-12\\FFN \n+sigmoid L1-2 \n+sigmoid L1-3 \n+sigmoid L1-12 \n0\n50000100000150000200000250000300000350000400000\nsteps\n1.1\n1.2\n1.3\n1.4\n1.5bpc\nValidation bpc\n+sigmoid L1-6 \n+sigmoid L0 \n+sigmoid L6-12 \nbaseline \n+sigmoid L1-12\\FFN \n+sigmoid L1-2 \n+sigmoid L1-3 \n+sigmoid L1-12 \nFigure 10: The performance of 12-layer Transformer-\nXL experiments augmenting σgated units.\nD Plot of Gate Biases of Transformer-XL\nD.1 Heatmap Visualization\nFig 11 witnesses the visualization of learned biases,\nwhich are all initialized as zeros at the beginning.\nObviously, the trainable biases of SDU gates per-\nform quite different between on MHDPA and FFN\nsublayers as in Fig. 11a, 11c for 6-layer models\nand Fig. 11b, 11d for 12-layer models. Also, the\ngate biases are similarly distributed on all of the\n6 layers, as in Fig. 11e, while showing the layer\nseparation on the bottom few transformer layers\nas shown in Fig. 11f. This also veriﬁes the experi-\nmental evidence that SDU gates on 6-layer models\nall positively inﬂuence the ﬁnal test performance,\nbut those only on the previous few layers of 12-\nlayer transformers could have better results on both\nconvergence speed and the ﬁnal test bpc.\nD.2 Scatter Visualization\nFig. 12 illustrates the uniform distribution on both\n6-layer and 12-layer Transformer-XL models. Due\nto the existence of residual connections, the rep-\nresentation space can be seen as the same. Hence\nthe evenly distributed gate biases may learn from\ndifferent aspects accordingly, which also matches\nour common intuition.\n6899\na1a2a3a4a5a6\nSDU gate bias on MHDPA of 6-layer models\n0.06\n0.03\n0.00\n0.03\n0.06\n0.09\n(a) Plot of gate biases on MHDPA of 6-layer models.\na1a2a3a4a5a6a7a8a9a10a11a12\nSDU gate bias on both MHDPA of 12-layer models\n0.4\n0.0\n0.4\n0.8\n1.2\n1.6\n (b) Plot of gate biases on MHDPA of 12-layer models.\nb1b2b3b4b5b6\nSDU gate bias on FFN of 6-layer models\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\n(c) Plot of gate biases on FFN of 6-layer models.\nb1b2b3b4b5b6b7b8b9b10b11b12\nSDU gate bias on both FFN of 12-layer models\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n (d) Plot of gate biases on FFN of 12-layer models.\na1b1a2b2a3b3a4b4a5b5a6b6\nSDU gate bias on both MHDPA and FFN of 6-layer models\n0.06\n0.03\n0.00\n0.03\n0.06\n0.09\n(e) Plot of gate biases on all sublayers of 6-layer models.\na1\nb1\na2\nb2\na3\nb3\na4\nb4\na5\nb5\na6\nb6\na7\nb7\na8\nb8\na9\nb9\na10\nb10\na11\nb11\na12\nb12\nSDU gate bias on both MHDPA and FFN of 12-layer models\n0.4\n0.0\n0.4\n0.8\n1.2\n1.6\n (f) Plot of gate biases on all sublayers of 12-layer models.\nFigure 11: The heatmap visualization of learnable biases (i.e., b1 in Eq. 21) on σ gate units of 6-layer (left\ncolumn) and 12-layer (right column) Transformer-XL models, where vertical axises represent the layer number\nof our models, and “a1” and “b3” denote the 1-st MHDPA sublayer and 3-rd FFN sublayer, respectively. All gate\nbiases are initialized as 0s with 512 dimension of each.\n6900\n200\n 100\n 0 100 200 300\n200\n100\n0\n100\n200\n300\nlayer1-SA\nlayer2-SA\nlayer3-SA\nlayer4-SA\nlayer5-SA\nlayer6-SA\nlayer1-FFN\nlayer2-FFN\nlayer3-FFN\nlayer4-FFN\nlayer5-FFN\nlayer6-FFN\nCluster of gate biases on 6L transformer-XL\n(a) Plot of bias distributions on 6-layer models.\n600\n 400\n 200\n 0 200 400\n600\n400\n200\n0\n200\n400\n600\nlayer1-SA\nlayer2-SA\nlayer3-SA\nlayer4-SA\nlayer5-SA\nlayer6-SA\nlayer7-SA\nlayer8-SA\nlayer9-SA\nlayer10-SA\nlayer11-SA\nlayer12-SA\nlayer1-FFN\nlayer2-FFN\nlayer3-FFN\nlayer4-FFN\nlayer5-FFN\nlayer6-FFN\nlayer7-FFN\nlayer8-FFN\nlayer9-FFN\nlayer10-FFN\nlayer11-FFN\nlayer12-FFN\nCluster of gate biases on 12L transformer-XL\n(b) Plot of bias distributions on 12-layer models.\nFigure 12: Scatter visualization of SDU gate biases on 6-layer and 12-layer Transformer-XL, where “layer2-SA”\ndenotes the gate bias on 2-nd self-attention sublayer. We employ t-Distributed Stochastic Neighbor Embedding\n(t-SNE) to reduce the dimension from 512 to 2."
}