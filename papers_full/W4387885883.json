{
    "title": "Vision Transformer Model for Predicting the Severity of Diabetic Retinopathy in Fundus Photography-Based Retina Images",
    "url": "https://openalex.org/W4387885883",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A142813711",
            "name": "Waleed Nazih",
            "affiliations": [
                "Prince Sattam Bin Abdulaziz University"
            ]
        },
        {
            "id": "https://openalex.org/A2794553179",
            "name": "Ahmad O. Aseeri",
            "affiliations": [
                "Prince Sattam Bin Abdulaziz University"
            ]
        },
        {
            "id": "https://openalex.org/A4380110631",
            "name": "Osama Youssef Atallah",
            "affiliations": [
                "Alexandria University"
            ]
        },
        {
            "id": "https://openalex.org/A4202303019",
            "name": "Shaker El-Sappagh",
            "affiliations": [
                "Suez University",
                "Galala University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4317207118",
        "https://openalex.org/W3175311671",
        "https://openalex.org/W4319597459",
        "https://openalex.org/W2999301272",
        "https://openalex.org/W2966820236",
        "https://openalex.org/W6800217721",
        "https://openalex.org/W2974361506",
        "https://openalex.org/W4220898925",
        "https://openalex.org/W4312093785",
        "https://openalex.org/W3128220181",
        "https://openalex.org/W4226244379",
        "https://openalex.org/W4362466198",
        "https://openalex.org/W2234307896",
        "https://openalex.org/W4308784174",
        "https://openalex.org/W3014353624",
        "https://openalex.org/W3164230720",
        "https://openalex.org/W2769713325",
        "https://openalex.org/W3168997536",
        "https://openalex.org/W2828862258",
        "https://openalex.org/W4280488276",
        "https://openalex.org/W2107871626",
        "https://openalex.org/W1995023030",
        "https://openalex.org/W1982890488",
        "https://openalex.org/W1969496006",
        "https://openalex.org/W2948685905",
        "https://openalex.org/W4224437581",
        "https://openalex.org/W2127322315",
        "https://openalex.org/W3035941285",
        "https://openalex.org/W4313827473",
        "https://openalex.org/W4318953389",
        "https://openalex.org/W2998557912",
        "https://openalex.org/W4293163051",
        "https://openalex.org/W6629711637",
        "https://openalex.org/W3125069671",
        "https://openalex.org/W2757894858",
        "https://openalex.org/W2997090102",
        "https://openalex.org/W3174384244",
        "https://openalex.org/W4313011320",
        "https://openalex.org/W3135447553",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W4220862900",
        "https://openalex.org/W3022092973",
        "https://openalex.org/W3020619771",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2942586440",
        "https://openalex.org/W3081111549",
        "https://openalex.org/W6767164110",
        "https://openalex.org/W1966206262",
        "https://openalex.org/W6713134421",
        "https://openalex.org/W2972869264",
        "https://openalex.org/W2887873758",
        "https://openalex.org/W3207619483",
        "https://openalex.org/W4327972770",
        "https://openalex.org/W4313524270",
        "https://openalex.org/W2535333834",
        "https://openalex.org/W4318773815",
        "https://openalex.org/W4224030538",
        "https://openalex.org/W4376273098",
        "https://openalex.org/W4224641883",
        "https://openalex.org/W4200585179",
        "https://openalex.org/W3160633377",
        "https://openalex.org/W4206481248",
        "https://openalex.org/W6848335571",
        "https://openalex.org/W2908723574",
        "https://openalex.org/W3206815816",
        "https://openalex.org/W3167044851",
        "https://openalex.org/W3094648158",
        "https://openalex.org/W3182950653",
        "https://openalex.org/W4212984122",
        "https://openalex.org/W3204146347",
        "https://openalex.org/W3192646885",
        "https://openalex.org/W2979927758",
        "https://openalex.org/W2934420704",
        "https://openalex.org/W4362603432",
        "https://openalex.org/W4226512186",
        "https://openalex.org/W2968917279",
        "https://openalex.org/W2948210185",
        "https://openalex.org/W4313531407",
        "https://openalex.org/W2953384591",
        "https://openalex.org/W3098558149",
        "https://openalex.org/W4300715911",
        "https://openalex.org/W4292713924",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4287022992"
    ],
    "abstract": "Diabetic Retinopathy (DR) is a result of prolonged diabetes with poor blood sugar management. It causes vision problems and blindness due to the deformation of the human retina. Recently, DR has become a crucial medical problem that affects the health and life of people. Diagnosis of DR can be done manually by ophthalmologists, but this is cumbersome and time consuming especially in the current overloaded physician&#x2019;s environment. The early detection and prevention of DR, a severe complication of diabetes that can lead to blindness, require an automatic, accurate, and personalized machine learning-based method. Various deep learning algorithms, particularly convolutional neural networks (CNNs), have been investigated for detecting different stages of DR. Recently, transformers have proved their capabilities in natural language processing. Vision transformers (ViTs) are extensions of these models to capture long-range dependencies in images, which achieved better results than CNN models. However, ViT always needs huge datasets to learn properly, and this condition reduced its applicability in DR domain. Recently, a new real-world and large fundus image dataset called fine-grained annotated diabetic retinopathy (FGADR) has been released which supported the application of ViT in DR diagnosis domain. The literature has not explored FGADR to optimize ViT models. In this paper, we propose a novel ViT based deep learning pipeline for detecting the severity stages of DR based on fundus photography-based retina images. The model has been built using FGADR dataset. The model has been optimized using a new optimizer called AdamW to detect the global context of images. Because FGADR is an imbalanced dataset, we combine several techniques for handling this issue including the usage of F1-score as the optimization metric, data augmentation, class weights, label smoothing, and focal loss. Extensive experiments have been conducted to explore the role of ViT with different data balancing techniques to detect DR. In addition, the proposed model has been compared with the state-of-the-art CNN algorithms such as ResNet50, Incep-tionV3, and VGG19. The adopted model was able to capture the crucial features of retinal images to understand DR severity better. It achieved superior results compared to other CNN and baseline ViT models (i.e., 0.825, 0.825, 0.826, 0.964, 0.825, 0.825, and 0.956 for F1-score, accuracy, balanced accuracy, AUC, precision, recall, specificity, respectively). The results of the proposed ViT model were quite encouraging to be applied in real medical environment for assisting physicians to make accurate, personalized, and timely decisions.",
    "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000. \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nVision Transformer Model for Predicting the \nSeverity of Diabetic Retinopathy in Fundus \nPhotography-Based Retina Images \nWaleed Nazih1,2, Ahmad O. Aseeri1*, Osama Youssef Atallah3, Shaker El-Sappagh4,5 \n1Department of Computer Science, College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, Al Kharj 11942, Saudi Arabia \n2Department of Computer Science, Cairo Higher Institute for Engineering, Computer Science and Management, Cairo 11865, Egypt \n3Department of Biomedical Engineering, Medical Research Institute, Alexandria University, El-Hadra Bahry, Alexandria 21561, Egypt; ohassan@alexu.edu.eg  \n4Faculty of Computer Science and Engineering, Galala University, Suez 435611, Egypt; sh.elsappagh@gmail.com (S.E.) \n5Information Systems Department, Faculty of Computers and Artificial Intelligence, Benha University, Banha 13518, Egypt. \nCorresponding author: Ahmad O. Aseeri (e-mail: a.aseeri@psau.edu.sa). \nâ€œThe authors extend their appreciation to the Deputyship for Research & Innovation, Ministry of Education in Saudi Arabia for funding this \nresearch work through the project number (IF-PSAU-2022/01/19574)â€  \n \nABSTRACT Diabetic Retinopathy (DR) is a result of prolonged diabetes with poor blood sugar \nmanagement. It causes vision problems and blindness due to the deformation of the human retina. Recently, \nDR has become a crucial medical problem that affects the health and life of people. Diagnosis of DR can be \ndone manually by ophthalmologists, but this is cumbersome and time consuming especially in the current \noverloaded physicianâ€™s environment. The early detection and prevention  of DR, a severe complication of \ndiabetes that can lead to blindness, require an automatic, accurate, and personalized machine learning-based \nmethod. Various deep learning algorithms, particularly convolutional neural networks (CNNs), have been \ninvestigated for detecting different stages of DR. Recently, transformers have proved their capabilities in \nnatural language processing. Vision transformers (ViTs) are extensions of these models to capture long-range \ndependencies in images, which achieved better results than CNN models. However, ViT always needs huge \ndatasets to learn properly, and this condition reduced its applicability in DR domain. Recently, a new real -\nworld and large fundus image dataset called fine-grained annotated diabetic retinopathy (FGADR) has been \nreleased which supported the application of ViT in DR diagnosis domain. The literature has not explored \nFGADR to optimize ViT models. In this paper, we propose a novel ViT based deep learning pipeline for \ndetecting the severity stages of DR based on fundus photography-based retina images. The model has been \nbuilt using FGADR dataset. The model has been optimized using a new optimizer called AdamW to detect \nthe global context of images. Because FGADR is an imbalanced dataset, we combine several tech niques for \nhandling this issue including the usage of F1 -score as the optimization metric, data augmentation, class \nweights, label smoothing, and focal loss. Extensive experiments have been conducted to explore the role of \nViT with different data balancing  techniques to detect DR. In addition, the proposed model has been \ncompared with the state -of-the-art CNN algorithms such as ResNet50, Incep -tionV3, and VGG19. The \nadopted model was able to capture the crucial features of retinal images to understand DR se verity better. It \nachieved superior results compared to other CNN and baseline ViT models (i.e., 0.825, 0.825, 0.826, 0.964, \n0.825, 0.825, and 0.956 for F1 -score, accuracy, balanced accuracy, AUC, precision, recall, specificity, \nrespectively). The results of the proposed ViT model were quite encouraging to be applied in real medical \nenvironment for assisting physicians to make accurate, personalized, and timely decisions. \nINDEX TERMS deep learning, vision transformer, diabetic retinopathy, machine learning, disease \ndiagnosis.  \nI. INTRODUCTION \nThe global prevalence of diabetes has seen a significant \nincrease in recent years, with estimates suggesting a rise \nfrom 9.3% (463 million) in 2019 to 10.2% (578 million) by \n2030 and 10.9% (700 million) by 2045 [1]. The associated \nhealthcare expenditure has also risen by 316% over the last \n15 years, reaching a staggering 966 billion dollars [2]. \nDiabetes is a major cause of several debilitating conditions, \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \nincluding blindness, heart attacks, and kidney failure [3]. \nOne of the complications that arise from prolonged diabetes \nis DR, which involves damage to the blood vessels behind \nthe retina and may cause vision loss if not detected early \n[4] [5]. It affects approximately 30 to 40% of individuals \nwith diabetes worldwide, with over 100 million people \nliving with DR [6]. The condition becomes more prevalent \nin patients who have had diabetes for over 20 years, \naccounting for 80% of such individuals and contributi ng to \n12% of new blindness cases, and a major cause of vision \nloss and blindness for those aged between 20 and 74 years \n[7]. \nBy 2040, IDF Diabetes Atlas is expecting that one \nin three diabetes patients will develop DR [8]. Worldwide, \npatients with DR are expected to grow from 126.6 million \nin 2010 to 191.0 million by 2030, and the number of \npatients with vision -threatening DR is  expected to increase \nif no suitable action is taken [9]. According to WHO, DR \nis estimated to account for 4.8% of the number of cases of \nblindness worldwide [10]. To avoid the complications of \nDR, the early detection of the disease is crucial to sustain \nthe patient's vision effectively [11]. Currently, physicians \nmanually investigate the fundus images of eyes to measure \nthe severity of DR. However, these diagnostic procedures \nare difficult, error -prone, expensive, and time consuming \n[5][11][12]. In additio n, because it needs much time of \nalready overloaded physicians, many patients do not \nreceive medical care in a timely manner which results in a \nsevere DR state for many cases. Fortunately, much of the \nvisual loss from DR is preventable if it is coupled wit h an \nearly assessment and screening [6]. Hence, it is crucial to \naddress this issue by providing an automatic, accurate, \naccessible, and cheap methodology for the early detection \nand grading of the disease [13].  \nMachine learning (ML) and deep learning (DL) \ntechniques have been used to handle this medical problem \nbased mainly on the fundus images which visually record \nthe present ophthalmic appearance of a personâ€™s retina [4]. \nRetinal blood vessel segmentation, les ion segmentation, \nand DR classification are the regular steps for DR \ndetection. This classification problem is mainly formulated \nas a binary classification task (i.e., DR or normal retina) \nwhich is called DR detection. The grading of DR stage \nconsists of a nnotating the infected parts and determining \nthe types of infection: mild, moderate, or severe. This task \nis usually formulated as a multiclass classification task \n[14]. Using ML models, several studies have been \nconducted for DR classification and grading . In [15], \nauthors proposed the so called â€œtetragonal local octa pattern \n(T-LOP) featuresâ€, which is a new method for representing \nfeatures of the fundus images. Gayathri et al., [16] utilized \nsupport vector machine, random forest, and decision tree to \nclassify DR, and Washburn [17] used Gabor wavelet \nmethod with AdaBoost classifier to grade DR. Recently, \nSelvachandran et al., [7] provided a comprehensive survey \nabout DR detection techniques.  \nDL has more advanced architectures such as \nconvolutional neural networks (CNNs) that are able to \nautomatically extract deep and spatial features from images \n[18]. No independent features extraction and feature \nselection steps are needed with DL algorithms because \nthese techniques can learn deep representations from \nimages. Xu et al. [19] employed a CNN model using the \nKaggle EyePACS dataset for classifying retinal fundus \nimages. Stochastic gradient descent was used as the \noptimizer, and data augmentation te chniques such as image \nresizing, rotation, flipping, shearing, and translation were \napplied to increase the diversity of the images. The dataset \nwas split into training and testing sets, with 800 and 200 \nimages, respectively. Kazakh -British et al., [20] co mbined \na simple CNN model with an anisotropic diffusion filter to \nautomatically classify DR. Following the same method of \nbuilding simple DL models, in [21] the authors combined \nthe CNN architectures with the Wiener filter and OTSU for \nthe segmentation to perform binary classification for DR \ndetection. On the other way, transfer learning has been used \nto use pretrained deep model for better feature extraction. \nIn [22], Rego used InceptionV3 to detect DR using RGB \nand textures features. Pamadi et al., [23] built both binomial \nand multinomial classification of fundus images by \nutilizing MobileNetV2, Saranya et al. [24] used DenseNet -\n121 model to detect DR from fundus images, and different \narchitectures of EfficientNet were investigated in [25]. \nSome studies pr oposed hybrid models by using DL model \nfor feature extraction and regular ML model for \nclassification. For example, Boral and Thorat [26] utilized \nInceptionV3 for representation learning and support vector \nmachine for DR classification. Ensemble models, al so \ncalled Hybrid CNN architecture, are well -known \ntechniques to build robust and accurate classifiers. Jiang et \nal. [27] proposed an ensemble model for DR detection.  The \nensemble utilized three CNN models. In addition, the \nAdaboost was used for efficient integration of these DL \nmodelsâ€™ outputs using learned weights. The image \npreprocessing steps were resizing (i.e., 520, 520, 3), \nrotation, translation, mirroring, contrast, sharpness, and \nbrightness. In [28], Kaushik et al., built a stacking ensemble \nmodel consisting of three CNN models for DR \nclassification. Zhang et al., [29] proposed an ensemble \nmodel consisting of three CNN architectures (i.e., \nInceptionV3, Xception, and InceptionResNetV2) to detect \nDR, and an ensemble of ResNet50 + DenseNet169 + \nDenseNet201 to grade DR. This study consedered a set of \npreprocess steps to improve the quality of the images. On \nthe other hand, Bellemo et al., [30] proposed an ensemble \nwith VGGNet + ResNet, and Xie et al., [31] proposed an \nensmeble with VGGNet + ResNet + Dens eNet for DR \ngrading without doing any preprocessing steps.  \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017 1 \n \n \nNormal retinal  \n \nDiabetic retinal  \nFIGURE 1. Examples of diabetic retinopathy retina. The left image is a normal retina, and right is a DR-4 retina. \nDR is graded into five stages based on the International \nClinical Diabetic Retinopathy (ICDR) scale [32]: no \napparent retinopathy, mild nonproliferative diabetic \nretinopathy (NPDR), moderate NPDR, severe NPDR, and \nproliferative DR (PDR) [33]. Figure 1 prov ides visual \nexamples comparing a normal retina with one affected by \ndiabetic retinopathy, highlighting multiple lesions. In [34], \na simple CNN model has been explored to grade DR after \napplying a green channel filter on fundus images. Luo et \nal., [35] prop osed MVDRNet by combining a VGG -16 \nmodel with attention mechanisms. The study utilized a \ndataset containing multi -view fundus images. Araujo et al., \n[36] proposed a CNN model cased GRADUATE. It was an \nuncertainty-aware DL model that could give a \npathologically explainable description to support its \ndecisions. Gayathri et al., [16] proposed hybrid multipath \nCNN model for deep representation learning for DR \ngrading and combine the features extractor with three \ndifferent ML classifiers including random forest,  SVM, and \nJ48. Shaik and Cherukuri [37] proposed â€œHinge Attention \nNetwork (HA -Net)â€ which combined VGG -16 for spatial \nrepresentation learning and multiple attention stages for \nDR severity grading. Li et al., [38] proposed a semi -\nsupervised auto -encoder graph network (SAGN) where the \nautoencoder was used for feature extraction, radial bases \nfunction was used to calculate neighbor correlations, and \nthe graph CNN was used to grade DR. Wang et al. [39] \ncompared InceptionV3, AlexNet, and VGG -16 for DR \ngrading, a nd found that InceptionV3 achieved the best \naccuracy. ML and DL models are trained using publicly \navailable datasets [4] such as DRIVE [40], EyePACS [41], \nAPTOS [42], STARE [43], DIARETDB [44], HEIMED \n[45], ROC [46], Messidor [47], e -ophtha [48], DDR [49],  \nFGADR [33], DeepDRiD [50], IDRiD [51], and RFMiD \n[52]. The main problems of these data are the severe data \nimbalance and poor image quality. Han et al. [53] proposed \na CNN model called a category weighted network to \naddress data imbalance at the model lev el. The study \nemployed relation weighted labels instead of one -hot labels \nto preserve the distance relationship between labels. Using \nthe DDR and APTOS datasets, the model achieved high \nkappa scores and accuracy for DR grading. Tummala et al., \n[12] proposed an EfficientNetV2 -based deep ensemble \nmodel for automated estimation of fundus images quality. \nThe model achieved a test accuracy of 75% for the quality \nenstimation using the DeepDRiD dataset. For more \ninformation about the role of deep learning in detec tion and \nstaging of DR, readers are guided to these survey studies \n[13][54][5][55][7][11][56][57][58].  \nMost studies in computer vision rely on CNN \narchitectures due to their ability to encode spatial \nequivariance through convolutional layers, enabling deep \nvisual representations. However, transformer -based \narchitectures utilizing self -attention mechanisms, such as \nvision transformers (ViTs), have shown superior \nperformance to standard convolutions in various computer \nvision applications [55], despite their increased \ncomputational requirements [59]. ViT offers several \nadvantages over CNNs including: (1) ViT c an capture \nlonger-range dependencies among pixels than CNNs, (2) \nViT has a built -in saliency mechanism which supports \nmodel to concentrate more on specific focus points during \nprocessing, and (3) ViT defends better against adversarial \nattacks compared to C NNs. It's worth noting that the \nflexible CNN architectures support the scaling of depth \nfrom a few to hundreds of layers, a flexibility that is not \npresent in ViT. The ViT model was first introduced by \nDosovitskiy et al. in 2020 [60], adapting the attentio n \nmechanism from text -based data to images. ViT converts \nan input image into a sequence of patches, which are then \nprocessed by a vision encoder and fed into a multilayer \nperceptron (MLP) for classification. Each patch is mapped \nto a latent vector using tr ansformer layers, incorporating \npositional embeddings. Transformer encoders, consisting \nof multi-head self-attention (MSA) and MLP modules, are \nemployed to aggregate global information and propagate \nfeatures across layers. The resulting image representatio n \nis used for classification. ViT has been explored in many \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \ndomains and it outperformed CNN [61][62]. In addition, it \nhas been successfully applied to detect and grade DR in  \nvarious studies [63][64][65][66]. Kumar et al., [67] \ncompared three DL architectures including Transformer -\nbased network (i.e., Swin-Transformer and Vision -\nTransformer (ViT)), CNN (i.e., EfficientNet and ResNet), \nand multi -layered perceptron (MLPMixer) for DR \ndetection. They discovered that models based on \ntransformer architectures had better accuracy than these. \nYu et al., [65] pr oposed multiple instances learning VIT \n(MIL-ViT) DL model. The training has been done in two \nstages: first, pretraining on a large fundus image dataset, \nand second fine -tuning on the downstream task for DR \ndetection. Performance has been evaluated using \nAPTOS2019 and RFMiD2020 datasets which show that \nMIL-ViT achieved better results than CNN. Zhang et al., \n[68] proposed the TC -Net image segmentation framework \nby combining both CNN and ViT. For locality -aware \nperspective, an encoder -decoder CNN model is used  to dig \nout local information using the convolution operations. For \nlong-range dependencies, a ViT model is used to focus on \nthe global context. In addition, dynamic cyclical focal loss \nwas used to address the class imbalance. The TC -Net \nachieved mean pixe l accuracy of 0.517 and 0.69 9 on the \nDDR and IDRiD, respectively. Adak et al., [69] proposed \nan ensemble of transformers models where four models \nhave been integrated to determine the degree of DR \nseverity. Gu et al., [70] proposed a DR grading model based \non an integrated model of vi sion transformer and  residual \nattention. The proposed model has two main blocks (1) \nfeature extraction block based on transformer that can pay \nmore attention to retinal hemorrhage and exudate areas, and \n(2) grading prediction block based on residual attention that \ncan capture different spatial regions for different classes. \nFor more studies about transformers in medical images, \nreaders are guided to these recent studies [71][72]. Most of \nthe current DR diagnosis systems do not achieve \nsatisfactory performance, and there is room  for \nimprovements in the literature. The output of the proposed \nmodel can be translated into a set of new knowledge that \ncan be translated into medical practices for physicians in \nhospitals and medical centers. In [82][83], the authors \ndiscussed the role of translational medicine concept which \nis the â€œeffective translation of the  new knowledge, \nmechanisms, and techniques generated by  advances in \nbasic science research into new approaches for  prevention, \ndiagnosis, and treatment of disease.â€ These studies \nhighlighted the crucial role of AI in applying translational  \nmedicine which improves healthcare decisions. The main \ncontributions of this work can be summarized as follows.  \nâ€¢ Adaptation of Vision Transformer (ViT) for the \nautomated classification of diabetic retinopathy, \ntailoring the model specifically for this purpose.  \nâ€¢ Utilizes a dataset comprising high -quality \nresolution images, annotated by three \nophthalmologists, and containing a substantial \nnumber of images (1842 in total).  \nâ€¢ Addresses the challenge of imbalanced datasets by \nemploying image augmentation techniques and \nassigning appropriate loss weights.  \nâ€¢ The proposed model is validated on unseen test \ndata and compared against other state -of-the-art \nmodels, establishing its performance and \neffectiveness in comparison to existing \napproaches. \nThe remainder of the paper is organized as follows. Section \n2 presents the methodology of the study and the proposed \nmodel. Section 3 represents the results of the study and \ndiscussion about findings. Finally, conclusion and future \nwork are drawn in sectio n 4. \nII. MATERIALS AND METHODS \nIn this section, we describe the dataset and the proposed model \narchitecture. We explain the resulting modelâ€™s \nhyperparameters, and we specify the used evaluation metrics. \nA. Data Description \nThe development of computer-aided diagnosis systems for DR \nfaces significant challenges related to the availability and \nquality of training data. While there are some publicly \naccessible DR databases, most of them lack detailed \nannotations and only provide  image-level labels, which are \noften unreliable. To address these limitations, it is essential to \nhave datasets that include both pixel -level lesion masks and \nimage-level severity grading. Among the available datasets, \nthe fine-grained annotated diabetic retinopathy FGADR and \nIDRiD datasets fulfill these requirements. The IDRiD dataset, \nalthough containing only 81 images with segmented lesions, \ndoes not provide a sufficiently large dataset for our purposes. \nTherefore, the FGADR dataset, consisting of 1,842 images \nwith six segmented lesions, is considered the most suitable \nchoice. The FGADR dataset includes both pixel-level lesion \nannotations and image-level grading labels. The severity of \nDR is graded according to the international protocol [32] as \nshown in Table 1. In addition, the annotations in the FGADR \ndataset were performed by three ophthalmologists, ensuring \nreliable and consistent grading labels. Additionally, the \nimages and masks in the FGADR dataset are already cropped, \nfocusing on the retina area, eliminating unnecessary black \nTABLE 1: A DESCRIPTION OF THE DATASET. \nDR Scale Description Number of Images \n0 no retinopathy 101 \n1 \nmild non-proliferative DR \n(NPDR) \n212 \n2 moderate NPDR 595 \n3 severe NPDR 647 \n4 proliferative DR 287 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nareas in the image margins. Moreover, all images in the dataset \nhave the same dimensions of 1280 Ã— 1280, eliminating the \nneed for padding and ensuring consistency in the dataset. Zhou \n[33] performed a benchmark study based on this dataset, but \nthey provide d just a preliminary result which could be \nimproved using other hybrid techniques, new DL architectures \nlike ViT, and extra data preparation steps. During \nexperimentation, we followed the data usage agreement of \nZhou et al., [33] and all the experiments we re carried out in \naccordance with relevant guidelines and regulations. FGADR \nis available from the corresponding author on reasonable \nrequest [33]. Note that all experiments were carried out based \non the relevant guidelines and regulations, and we followed \nthe protocols mentioned by the data releasing organization in \ntheir respective licenses. \nB. Proposed ViT Framework \nIn this section, we discuss the proposed ViT framework for \nDR grading. The proposed framework has the four main steps \nof (1) image augmentation, (2) image normalization, (3) data \nsplitting, (4) data balancing, and (5) ViT model training and \nvalidation. \n1) Image Augmentation \nSix augmentation steps have been taken to increase the size of \nthe dataset. Figure 2 shows examples of the classes and \naugmented versions from these images. \nFlip vertical and horizontal: With probabilities p_hf and p_vf. \nTranspose: By swapping the height and width dimension. \nRandom rotation: this technique is consistently applied, where \nthe rotation angle Î± is randomly determined within the \nspecified range defined by Î¸, i.e., Î± âˆˆ[-Î¸, Î¸], with \nÎ±={90,180,270}. \nRandom brightness adjustment: This is pixel-level adjustment. \nWe applied this transformation 40% of the time with a random \nfactor Î²âˆˆ[Î²min, Î²max] for Î²=0 results in a complete black image, \nÎ²=1 results in the original image unchanged, and Î²>1 increases \nthe brightness by the Î² factor. \nRandom contrast adjustment: This is pixel -level adjustment. \nWe applied this transformation with a probability of 40% \nbased on a factor k âˆˆ [kmin, kmax] for k=0 results in a solid gray \nimage, k=1 results in the original image unchanged, and k>1 \nincreases the contrast by the k factor. \nRandom saturation adjustment: This is pixel-level adjustment \nwith probability 40%. It adjusts the saturation of RGB images \nby a random factor Î² randomly picked in the interval Î²âˆˆ[Î²min, \nÎ²max]. This process involves converting RGB images into a \nfloating-point representation, followed by a conversion to the \nHSV color space. An offset is applied to the saturation \nchannel, and then the image is converted back to the RGB \ncolor space. Finally, the image is restored to its original data \ntype. \n \n2) Image Normalization \nData normalization is an important pre -processing step. It \nensures that the pixel values come from the Gaussian \ndistribution. Neural networks rely on gradient calculations and \ntry to learn how weighty a feature or a pixel should be in \ndetermining the class of an image. Normalized pixel values \nhelp the gradient calculations to stay consistent and not get so \nlarge which slows down or prevents the network conversion. \nAs a result, normalization of pixel values (intensity) of images \nis recommended to avoid the influence of high frequency noise \nand low noise, accelerate the training process, make the \ntraining more stable, and to give the same weight to different \nfeatures (pixels) which results in more accurate model \nweights. Normalization is performed by subtracting the mean \nÎ¼ from each pixel. The resulting values are then divided by the \nstandard deviation Ïƒ, which is computed from all pixel values. \nMore formally, the normalized value z(x)=((x -Î¼))â„Ïƒ. The \nresulting distribution resembles the Gaussian function \ncentered at zero. Because pixel numbers must be positive, the \nscale of the data is [0, 1]. \n \n3) Data Splitting \nThe dataset has been split into 80%/10%/10% for \ntraining/validation/testing. Training dataset is used to optimize \nour proposed ViT model, model validation is based on a \nseparate 10% of the data, and testing or generalization \nperformance is based on 10% of unseen dataset. \n \n4) Data balancing and balanced training \nAs shown in Table 1, FGADR is an imbalance dataset. \nTraining an DL model based on imbalanced dataset results in \na biased model towards the majority class. The model could \nconsider the minority class examples as outliers and \ncompletely concentrate on the ma jority class. However, the \nminority class is always the positive class that needs full \nattention in the modelâ€™s learning process. Handling this \nproblem involves many techniques including dataset or model \napproaches. The most common dataset-based techniques are \nsampling-based techniques and data augmentation. In our \nstudy, we apply different data augmentation techniques, see \nSection 2.2.1. Model based approaches include several \ntechniques [53]. In our paper, we combine several techniques \nincluding the usage of F1-score as the optimization metric, \nclass weights, label smoothing, and focal loss. The F1 -score \nwas chosen over accuracy for two main reasons [73]. The F1-\nscore is particularly suitable for datasets with imbalanced \ndistributions of classes. Imbalanced  datasets often have \nminority classes, such as the one at hand. Accuracy may be \nbiased towards the more frequent classes, resulting in lower \nclassification accuracy for the less frequent classes. This score \nmitigates this bias and provides a more reliable evaluation. By \nconsidering the F1-score, the evaluation of the ViT model's \ndetection performance considers the model's ability to balance \nprecision and recall, as well as its overall discriminative power \nin differentiating between different classes in the dataset. The \ndefault weighting for classes in a classification task 1 for every \nclass. This uniform mechanism is not suitable for the  \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n \n \n \n \n \n \n \n \n \n \n \n \nimbalanced dataset. Class weighting is a technique to give \nhigh weight to the minority class (i.e., positive class). In our \nstudy, we give a weight to every class such that it is \nproportional to the number of examples in that class, i.e., \nweight of class x = total # samples/(# classes*# samples in \nclass x). The model loss is calculated by comparing the model \n \nClass \n \nExample of original training sample \n \nAn augmented version \n \nNo retinopathy (0) \n  \n \nNPDR (1) \n  \n \nModerate NPDR (2) \n  \n \nSevere NPDR (3) \n  \n \nProliferative DR (4) \n  \nFigure 2: Examples from train set for images from every class and example of its augmented version. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \noutput ğ‘¦Ì‚ and the label y of the corresponding image, and y is \nusually encoded by one-hot code. Conventional one-hot code \ncontains only two values, 0 and 1, which assumes that the \ndistances between all classes are equal. This creates a problem, \nespecially in the multi-classification tasks because it ignores \nthe logical relationship between the classes. For example, in \nthe five-class DR classification task, the distance between No \nretinopathy and NPDR, and between Moderate NPDR and \nSevere NPDR differed considerably. Label smoothing [74] is \na technique used in classification tasks to prevent the model \nfrom becoming overconfident. Instead of assigning a hard \nlabel of 0 or 1 to the true class, label smoothing assigns a \nsmoothed label value between 0 and 1 for each class. \nLin et al., [2] proposed the focal loss to force customized \nbalance among classes in imbalanced datasets, which \ndemonstrates superior performance [3]. Focal loss is a better \nalternative for cross-entropy. Focal loss concentrates on the \nexamples that the model gets wrong predictions rather than the \nones that it can confidently predict. This makes the model \ngradually enhance its predictions on hard examp les \nusing down weighting. Down weighting is an approach that \nreduces the influence of easy examples of the majority class \non the loss function which results in a trained model that gives \nmore attention to hard examples. A modulating factor is added \nto the cross-entropy loss to implement the focal loss, \nâˆ’ âˆ‘ ğ›¼ğ‘– (ğ‘– âˆ’ ğ‘ğ‘– )ğ›¾ logğ‘(ğ‘ğ‘– )ğ‘›\nğ‘–=1  and Î³ is the focusing and Î± is the \nweighing hyperparameters. \n \n5) ViT model training  \nViT, a network architecture with high capacity, shares \nsimilarities with transformers employed in language \nprocessing. However, ViT adopts self -attention instead of \nconvolution to gather information across different locations. \nWithin ViT, two essential components play crucial roles: (1) \nmultiheaded self -attention, which facilitates the early \naggregation of global information, and (2) residual \nconnections, which effectively propagate features from lower \nlayers to higher layers. The local multiheaded self -attention \nconcept in convolutional neural networks (CNNs), derived \nfrom the structure of convolutional receptive fields, combines \nCNNs with self-attention or applies Transformers to smaller-\nsize images [4]. The fundamental concept behind ViT  is to \ntransform the input image into a sequence of image patches, \nakin to textual words, and extract features using a vision \nencoder. These features are then fed into a multilayer \nperceptron (MLP), as illustrated in Figure 3. \nâ€¢ The input image ğ¼ğ‘§ of size 224 Ã— 224 is converted \ninto a sequence of flattened patches ğ‘¥ğ‘\nğ‘– , ğ‘– =\n1,2, â€¦ , ğ‘›ğ‘. All patches have the same size of \nğ‘¤ğ‘ Ã— ğ‘¤ğ‘ Ã— ğ‘ğ‘, ğ‘ğ‘ is the number of channels in ğ¼ğ‘§, \nğ‘¤ğ‘ = 64 is imperially chosen and ğ‘›ğ‘ = (\n256\n64 )\n2\n=\n16, and as fundus images are RGB, ğ‘ğ‘ = 3.   \nâ€¢ Each patch ğ‘¥ğ‘\nğ‘–  is flattened and mapped to a D -\ndimensional latent vector (patch embedding ğ‘§0) by \ntransformer layers using a trainable linear projection. \nğ‘§0 = [ğ‘¥ğ‘ğ‘™ğ‘ğ‘ ğ‘ ; ğ‘¥ğ‘\n1ğ”¼; ğ‘¥ğ‘\n2ğ”¼; ğ‘¥ğ‘\n3ğ”¼; â€¦ ; ğ‘¥ğ‘\nğ‘›ğ‘\nğ”¼] + ğ”¼ğ‘ğ‘œğ‘ , ğ”¼ âˆˆ\nâ„ğ‘¤ğ‘Ã—ğ‘¤ğ‘Ã—ğ‘ğ‘Ã—ğ· is the patch embedding projection, \nğ”¼ğ‘ğ‘œğ‘  âˆˆ â„(ğ‘›ğ‘+1)Ã—ğ· is the position embeddings added \nto patch embeddings to preserve the positional \ninformation of patches, ğ‘¥ğ‘ğ‘™ğ‘ğ‘ ğ‘  = ğ‘§0\n0 is a learnable \nembedding [5]. At this stage, the patch images are \nmapped to the embedding space with positional \ninformation. \nâ€¢ Add a sequence of transformer encoders [6]. \nTransformer encoder (TE) has a well -known \narchitecture, see Figure 4, which includes two blocks \nğ´ğ‘   and ğ¹ğ‘› containing MSA (Multi -head Self -\nAttention) and MLP modules, respectively [6]. Layer \nnormalization (ğ¿ğ‘) and residual connection are used \nbefore and after each of these modules, respectively. \nFormally, ğ‘§ğ‘™\nâ€² = ğ‘€ğ‘†ğ´(ğ¿ğ‘(ğ‘§ğ‘™âˆ’1)) + ğ‘§ğ‘™âˆ’1, ğ‘§ğ‘™ =\nğ‘€ğ¿ğ‘ƒ(ğ¿ğ‘(ğ‘§ğ‘™\nâ€²)) + ğ‘§ğ‘™\nâ€², ğ‘™ = 1,2, â€¦ , ğ¿, and ğ¿ is the total \nnumber of transformer blocks. In this context, the \nMLP module consists of two layers with 4D and D \nneurons, respectively, employing the GELU \n(Gaussian Error Linear Unit) activation function. \nâ€¢ MSA with h heads is the core component of the \ntransformer encoder. Each head ğ‘– âˆˆ {1,2, â€¦ , â„} of \nMSA includes a scaled dot-product attention [6]. A \nhead ğ‘– calculates a tuple comprising query ğ‘„ğ‘– , key \nğ¾ğ‘–, and value ğ‘‰ğ‘–  [6], as follows. ğ‘„ğ‘– = ğ‘‹ğ‘Šğ‘„\nğ‘– , ğ¾ğ‘– =\nğ‘‹ğ‘Šğ¾\nğ‘– , and ğ‘‰ğ‘– = ğ‘‹ğ‘Šğ‘‰\nğ‘–, ğ‘‹ is the input embedding, and \nğ‘Šğ‘„, ğ‘Šğ¾, ğ‘Šğ‘‰ are the weight matrices used in the \nlinear transformation process. \nâ€¢ The resulting tuple (ğ‘„, ğ¾, ğ‘‰) is used as input to the \nscaled dot -product attention which calculates the \nattention required to pay to the input image patches. \nFormally, ğ‘†ğ´(ğ‘„, ğ¾, ğ‘‰ ) = ğœ“ (\nğ‘„ğ¾ğ‘‡\nâˆšğ·â„\n) ğ‘‰, ğœ“ is SoftMax \nfunction, and ğ·â„ =\nğ·\nâ„. \nâ€¢ The outcomes of scaled dot-product attentions across \nall heads are concatenated in MSA, ğ‘€ğ‘†ğ´(ğ‘„, ğ¾, ğ‘‰) =\n [ğ‘†ğ´1; ğ‘†ğ´2; â€¦ ; ğ‘†ğ´â„ ]ğ‘Šğ¿, ğ‘Šğ¿ is a weight matrix. \nâ€¢ Additional transformer encoder blocks can be \nincorporated into the system. Following the \napplication of multiple blocks, the < ğ‘ğ‘™ğ‘ğ‘ ğ‘  > token \naccumulates contextual information. The resulting \nstate of the learnable embedding from the \nTransformer encoder, denoted as (ğ‘§ğ¿\n0), serves as the \nimage representation. This image representation, \ndenoted as y, is obtained by applying layer \nnormalization to representation ğ‘¦ =  ğ¿ğ‘(ğ‘§ğ¿\n0). \n \nAs illustrated in Figure 3, an MLP head is introduced, \ncomprising a hidden layer consisting of 128 neurons. The  \noutput layer of the MLP consists of five neurons, utilizing \nthe SoftMax function to generate a probability \ndistribution for determining the severity level of DR. \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nFigure 3: Proposed ViT architecture; MLP is for multilayer perceptron. \n \n \n \nFigure 4: Internal architecture of a transformer encoder (TE) [69]. \n \nC. Performance Evaluation \nFour standard metrics were used to evaluate the resulting \nclassifiers including accuracy, balanced accuracy, specificity,  \nprecision, recall, AUC, and F1 -score, where TP is the true \npositive, TN is the true negative, FP is the false positive, FN is \nthe false negative, and TPR (true positive rate) =\nTP (TP + FN)â„  (see Equations 1â€“7). \n \nAccuracy = TP + TN\nTP + TN + FP + FN                                    (1) \n \nPrecision = TP/(TP + FP)                                               (2) \n \nRecall = TP\nTP + FN                                                                 (3) \n \nF1 âˆ’ Score = 2 âˆ— Precision âˆ— Recall\nPrecision + Recall                                 (4) \nSpecificity (true negative rate) =  ğ‘‡ğ‘\nğ‘‡ğ‘ + ğ¹ğ‘ƒ                (5) \nBalanced accuracy =   ğ‘‡ğ‘ƒğ‘… + ğ‘‡ğ‘‡ğ‘ğ‘…\n2                               (6) \n \nAUC = ğ‘‡ğ‘ƒ\n2 âˆ— (ğ‘‡ğ‘ƒ + ğ¹ğ‘) + ğ‘‡ğ‘\n2 âˆ— (ğ¹ğ‘ƒ + ğ‘‡ğ‘)                        (7) \n \nIII. RESULTS AND DISCUSSION \n \nThis section focuses on conducting a series of experiments to \noptimize the architecture of the proposed ViT model and attain \noptimal performance. Furthermore, a comparison is made \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nbetween the performance of the proposed model and several \nstate-of-the-art classification models. Various experiments \nwere performed, involving different training optimizers and \nhyperparameters, aiming to identify the most suitable \nconfiguration. \nA. Experimental setup \nAll experiments were carried out on the Google Colab \nplatform, utilizing a Tesla V100 with 16 GB of memory and \n12 GB RAM. The implementation of the experiments \ninvolved the utilization of Keras and TensorFlow libraries [7]. \nImages of FGADR dataset are downsized into 224 Ã—  224 \nand the dataset was split into three parts, 80% for training \ndataset, 10% for validation dataset to fine -tune the modelâ€™s \nhyperparameters, and 10% for testing dataset to test the \ngeneralization performance of the proposed model. Results are \nreported in terms of the testing performance. \n       In the training process of the ViT models, several \nconsiderations were considered to optimize the trainable \nparameters and achieve the highest performance. To minimize \nthe loss, the training process employed the minibatch \noptimization approach. This approach involves dividing the \ntraining data into smaller batches or subsets, in this case, a \nbatch size of 32 training samples was used. The model's \n \nTABLE 2: TWO ARCHITECTURES OF VIT MODEL WITH DIFFERENT SIZES. \nModel Layers Hidden Size MLP size # of Heads Params \nViT-Base 12 768 3072 12 86M \nViT-Large 24 1024 4096 16 307M \nTABLE 3: COMPARING PERFORMANCES OF VIT-BASE AND VIT-LARGE MODELS WITH 16 AND 32 IMAGE PATCHES. \nModel, patch size F1-Score Accuracy Balanced Accuracy AUC Precision Recall Specificity \nViT-Base, 16 0.616 0.616 0.597 0.841 0.616 0.616 0.9 \nViT-Base, 32 0.789 0.789 0.791 0.951 0.789 0.789 0.947 \nViT-Large, 16 0.678 0.678 0.679 0.913 0.678 0.678 0.919 \nViT-Large, 32 0.804 0.804 0.805 0.959 0.804 0.804 0.951 \nparameters were updated based on the average gradient \ncalculated from each batch, which helps in reducing the \ncomputational burden and memory requirements compared to \nprocessing the entire dataset at once. Cross -entropy loss is \nused in the training process. It is commonly used in multi-class \nclassification problems because it effectively measures the \ndissimilarity between the predicted probabilities and the true \nclass labels. By minimizing this loss, the model learned to \nassign higher probabilities to the correct classes, improving its \noverall accuracy. The model's trainable parameters, such as \nweights and biases, are tuned using the RAdam optimizer [8]. \nThe number of epochs was 100 epochs. Learning rates from \n1e-4 to 4e-4 were tried, and the learning rate of 3e-4 resulted \nin the best performance. \nB. Experiment 1: ViT model and patch size selection \nThe first experiment was conducted to choose between two \nvariants of ViT architectures, i.e., ViT-Base and ViT-Large. \nBoth model variants are pretrained using ImageNet21k and \nImageNet2012 datasets. ViT-Base is a base model with 86 \nmillion parameters, and ViT-Large is a larger model with 307 \nmillion parameters. The architecture information of the two \nViT variants is shown in Table 2. We added three dense layers \nto train the classifier (i.e., 128 units, 64 units, 32 units). Two \nbatch normalization layers have been added, one before the \nfirst dense layer and another layer after the first dense layer. \nThe GeLu activation function has been used in all internal \nlayers and SoftMax has been used in the extra output layer.  \nTable 3 shows the performance of different ViT architectures \nusing different batch sizes. We tested the performance of each \nmodel using 16 Ã— 16 and 32 Ã— 32 patches. The ViT-Large \nmodel with 32 Ã— 32 patches exhibited the best performance, \nas indicated in Table 3 (i.e., 0.804, 0.804, 0.805, 0.959, 0.804, \n0.804, and 0.951 for F1 -score, accuracy, balanced accuracy, \nAUC, precision, recall, specificity, respectively). Because \nViT-Large has a more complex architecture, larger batch size \nachieved better results. \nOverfitting has been prevented using several regularization \ntechniques including dropout and L2 regularization. \nConsequently, this model (i.e., ViT-Large) was selected for \nfurther analysis , and all subsequent experiments were \nconducted using this model. Several fine-tuning experiments \nof ViT-Large model, especially the optimizer using the Radam \nalgorithm, were carried out in order to enhance the \nperformance of ViT -Large model and to beat the reported \nperformance of the same problem at hand (i.e., 0.81 F1-score) \n[9]. \nC. Experiment 2: Weight decay selection for the large \nViT model \nOptimizer selection is a crucial step for training a deep \nlearning model. Best optimizer finds the best weights quickly. \nDifferent optimizers have been explored. For example, Adam \nis a well-known optimizer which has been used to optimize the \ninitial model . Based on the performance of the resulting \nmodel, another variant of Adam optimizer, i.e., AdamW, was \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nexplored and utilized in this experiment [10]. RAdam is \nknown for its fast convergence rate and lower memory \nrequirements. AdamW includes regularization which is a \nmodified version of L2 regularization. In the formulation of \nL2 regularization, the weight-decay is added to the gradient \nupdate term, whilst in the formulation of AdamW, the weight-\ndecay is decoupled from the gradient update term. Several \ndifferent values of weight-decay parameter were explored in \nthis experiment, as shown in Table 4. In addition, the learning \nrate determines the step size at which the model's parameters \nare updated during optimization. Learning rates from 1e-4 to \n4e-4 were explored, where the learning rate of 3e-4 resulted in \nthe best performance. \nThe highest achieved result was an F1-Score of 0.783 with a \nweight decay value of 1e -4. None of the experiments \nsurpassed the best outcome reported in experiment 1, where \nthe F1-Score was 0.804. Therefore, RAdam was employed in \nall subsequent experiments due to its superior performance. \nD. Experiment 3: Selection of the dropout and \nregularization \nLarge models such as ViT are prone to overfitting. In the next \ntwo experiments, dropout and L2 regularization techniques \nwere utilized to improve the model performance and ensure \noverfitting prevention. Three and two layers of dropout were \nadded to the ViT model with different dropout values as \nshown Table 5. In addition, Table 6 shows the results of an \nexperiment to select the best regularization parameter. L2 \nregularization was utilized with two different values. \nIntroducing two layers of dropout with a value of 0.2 yielded \nthe best F1-Score of 0.759. As it can be noticed, because the \nproposed model is based on the transfer learning where the \nlayers of the ViT are frozen, a small probability is needed for \nthe dropout layer to provide the best results. L2 regularization \nwith a val ue of 0.001 resulted in an F1 -Score of 0.726, see \nTable 6. However, none of these techniques improved upon \nthe model performance observed in experiment 1 (i.e., F1 -\nScore = 0.804).  The exploration of the above hyperparameters \nindicates that the ViT model is  well trained to solve our \nmedical problem and indicates that the architecture of the \nadded extra dense layers has minor impact on the performance \nof the overall model. In other words, the extracted deep \nrepresentation from the ViT model is good enough to train \nsimple classifier.  \n \nTABLE 4: PERFORMANCES OF VIT-LARGE MODEL WITH ADAMW OPTIMIZER. \nWeight-Decay F1-Score Accuracy Balanced Accuracy AUC Precision Recall Specificity \n1e-3 0.675 0.675 0.676 0.923 0.675 0.675 0.919 \n2e-3 0.620 0.620 0.620 0.884 0.620 0.620 0.905 \n1e-4 0.783 0.783 0.785 0.958 0.783 0.783 0.946 \n5e-5 0.777 0.777 0.779 0.957 0.777 0.777 0.944 \n2e-4 0.723 0.723 0.724 0.940 0.723 0.723 0.930 \n3e-4 0.747 0.747 0.748 0.951 0.747 0.747 0.937 \n4e-4 0.735 0.735 0.737 0.943 0.735 0.735 0.934 \nTABLE 5: COMPARING PERFORMANCES OF VIT-LARGE MODEL WITH RADAM OPTIMIZER AND DROPOUT. \n# layers, \ndropout F1-Score Accuracy Balanced Accuracy AUC Precision Recall Specificity \n3, 0.3 0.669 0.669 0.671 0.92 0.669 0.669 0.92 \n2, 0.1 0.744 0.744 0.746 0.946 0.744 0.744 0.936 \n2, 0.2 0.759 0.759 0.761 0.946 0.759 0.759 0.94 \n2, 0.5 0.627 0.627 0.629 0.898 0.627 0.627 0.907 \nTABLE 6: COMPARING PERFORMANCES OF VIT-LARGE MODEL WITH RADAM OPTIMIZER AND \nREGULARIZATION. \nL2 Regularization F1-Score Accuracy Balanced Accuracy AUC Precision Recall Specificity \n0.1 0.624 0.624 0.625 0.907 0.624 0.624 0.906 \n0.001 0.726 0.726 0.728 0.942 0.726 0.726 0.931 \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nTABLE 7: COMPARING PERFORMANCES OF VIT-LARGE MODEL WITH LABEL SMOOTHING. \nLabel \nSmoothing F1-Score Accuracy Balanced Accuracy AUC Precision Recall Specificity \n0.0 0.732 0.732 0.734 0.948 0.732 0.732 0.933 \n0.1 0.738 0.738 0.740 0.944 0.738 0.738 0.935 \n0.4 0.729 0.729 0.731 0.938 0.729 0.729 0.932 \nTABLE 8: COMPARING PERFORMANCES OF VIT-LARGE MODEL WITH CLASS WEIGHTS. \nClass Weights F1-Score Acc Balanced Accuracy AUC Precision Recall Specificity \n2.0, 1.5, 1.0, 1.0, 1.5 0.783 0.783 0.784 0.963 0.783 0.783 0.946 \n2.0, 2.0, 1.0, 1.0, 2.0 0.765 0.765 0.765 0.953 0.765 0.765 0.941 \n3.64, 1.73, 0.62, 0.57, 1.28 0.783 0.783 0.783 0.956 0.783 0.783 0.946 \n3.64, 1.73, 0.91, 0.86, 1.28 0.825 0.825 0.826 0.964 0.825 0.825 0.956 \n3.65, 1.75, 1.0, 0.95, 1.25 0.798 0.798 0.799 0.961 0.798 0.798 0.950 \n3.64, 1.73, 1.0, 0.86, 1.28 0.801 0.801 0.802 0.962 0.801 0.801 0.950 \nE. Experiment 4: Robustness improvement using \nlabel smoothing \nLabel smoothing [11] is a regularization technique commonly \nused in classification tasks to prevent the model from \nbecoming overconfident in its predictions. Instead of \nassigning a hard label of 0 or 1 to the true class, label \nsmoothing assigns a smoothed label value between 0 and 1. \nThis encourages the model to learn more robust and \ngeneralizable representations. Different values of label \nsmoothing were tried in this experiment  as shown in Table 7. \n \nFigure 5: Confusion matrix of proposed ViT model with  \nfinetuned class weights on the test dataset. \n \nWhen employing label smoothing with a value of 0.1, the best \nF1-score achieved in this experiment was 0.738. This result \nwas inferior to the best outcome reported in experiment 1, \nwhich utilized label smoothing with a value of 0.2. Previous \nexperiments did not achieve better results than the best \noutcome obtained in experiment 1, which reported an F1 score \nof 0.804. Consequently, in subsequent experiments, different \ntechniques were employed to address the class imbalance \nwithin the FGADR dataset, as illustrated in Table 1. This class \nimbalance presents challenges during training, as the model \nmay exhibit bias towards the majority classes and encounter \ndifficulties in accurately predicting the minority classes. \nFurthermore, we made modifications to the training process \nwith the objective of maximizing the  F1 score of validation \ndatasets, as opposed to minimizing validation loss in previous \nexperiments. \nF. Experiment 5: Class weight for dataset balancing \nOne approach to address class imbalance is using class \nweighting. Class weights assign different weights to each class \nbased on their prevalence in the dataset. By assigning higher \nweights to the minority class and lower weights to the majority \nclass, the model is encouraged to pay more attention to the \nminority class during training, thus mitigating the impact of \nclass imbalance. Manual weights were tried in the beginning \nwhich assign every class a weight value between 1 and 2. \nThen, automatic weights were  calculated according the \nequation of ğ‘ğ‘™ğ‘ğ‘ ğ‘  ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ =  ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ /\n(ğ‘›ğ‘¢ğ‘š ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘’ğ‘  âˆ— ğ‘ğ‘™ğ‘ğ‘ ğ‘  ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ ). The best result was \nachieved using a manually fine-tuned version of the automatic \nweights which achieved F1 -score of 0.825 which surpassed \nthe best reported performance (i.e., 0.81 F1-Score), as shown \nin Table 8. \nTwo versions of manual weight adjustment (i.e., class weights \nranging from 1 to 2) yielded F1 -Scores of 0.783 and 0.765, \nrespectively. However, automatic weights performed better.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nAutomatic weights were calculated based on the number of \nsamples in each class (i.e., 3.64, 1.73, 0.62, 0.57, 1.28), and \nthey achieved the same F1-Score as the manual weights. These \nweights were fine-tuned to pay more attention to the minority \nclasses during training. One of these trials (weights: 3.64, 1.73, \n0.91, 0.86, 1.28) surpassed the best F1 -Score reported in \nexperiment 1 (F1-Score = 0.804) and the accuracy reported in \n(accuracy = 0.810) [12]. The confusion matrix of the ViT \nmodel using these weights is depicted in Figure 5. \nG. Experiment 6: Two-phase ViT pre-training for \ndataset balancing \nThe most recent experiment aimed to tackle the imbalanced \ndataset by employing a two-phase ViT pre-training approach \n[13]. In this approach, the ViT model was initially pre-trained \nusing a large fundus database (i.e., transfer step), resulting in \nsubstantial improvements in model performance when fine -\ntuned for the downstream retinal disease classification task \n(i.e., adapta tion step). For the initial step, a preprocessed \nversion of the Kaggle EyePACS 1 dataset was utilized. This \ncopy exclusively consisted of the training dataset, comprising \n35,126 labeled images. Moreover, all images were resized to \n1024 Ã— 1024 and cropped to remove significant amounts of \nblack space. The images were sourced from EyePACS and \ncaptured under diverse conditions by various devices at \nmultiple primary care sites across California and other \nlocations. Each subject contributed two images of their left and \nright eyes, both with the same resolution. A clinician assessed \neach image for the presence of DR, employing a scale of 0â€“4. \nConstructing ViT models using this approach necessitates \nsubstantial computational resources, resulting in only a limited \nnumber of ViT models being built using the EyePACS dataset. \nThese models were subsequently trained on the FGADR \ndataset to enhance classification performance. One of these \nmodels achieved an improved F1 -Score of 0.828 on the \nFGADR dataset. \nH. Experiment 7: Focal loss for dataset balancing \nFocal loss was tried in this experiment with different alpha and \ngamma combinations. By using focal loss, the model can \neffectively focus more on the minority class and mitigate the \ndominant influence of the majority class.  \nAs illustrated in Table 9, the highest F1 -Score obtained was \n0.792 with an alpha of 0.25 and a gamma of 2. This suggests \nthat utilizing focal loss with the ViT model did not result in a \nbetter F1-Score. \n \nTABLE 9: PERFORMANCE OF VIT-LARGE MODEL WITH RADAM OPTIMIZER AND FOCAL LOSS. \nAlpha, \nGamma F1-score Accuracy Balanced Accuracy AUC Precision Recall Specificity \n0.25, 2 0.786 0.786 0.787 0.960 0.786 0.786 0.947 \n0.5, 2 0.786 0.786 0.787 0.960 0.786 0.786 0.947 \n0.75, 3 0.750 0.750 0.752 0.953 0.750 0.750 0.938 \n0.05, 0.5 0.744 0.744 0.746 0.949 0.744 0.744 0.936 \nTABLE 10: COMPARING OF THE PROPOSED VIT MODEL WITH OTHER CNN MODELS. \nModel F1-Score Accuracy Balanced Accuracy AUC Precision Recall Specificity \nMobileNetV3Large 0.536 0.536 0.538 0.838 0.536 0.536 0.884 \nInceptionV3 0.680 0.680 0.680 0.887 0.680 0.680 0.920 \nConvNeXtLarge 0.689 0.689 0.691 0.891 0.689 0.689 0.922 \nVGG19 0.700 0.700 0.700 0.893 0.700 0.700 0.925 \nResNet50 0.708 0.708 0.710 0.904 0.708 0.708 0.927 \nDenseNet201 0.771 0.771 0.772 0.944 0.771 0.771 0.942 \nSegmentation and \nDensenet121 [12] \n- 0.810 - - - 0.810 0.980 \nSegmentation and \nInceptionV3 [12] \n- 0.810 - - - 0.842 \n \n0.990 \nProposed ViT 0.825 0.825 0.826 0.964 0.825 0.825 0.956 \n \n \n \n \n1 https://www.kaggle.com/datasets/mariaherrerot/eyepacspreprocess \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n \nFigure 6: Comparison between different ViT pipelines. \nI. Experiment 8: Comparison with other models \nThe proposed models were compared with other models, as \nshown in Table 10. Additionally, a comparison was made with \nother published result s that employed the FGADR dataset \n[12]. The proposed model demonstrated superior performance \nin terms of F1 -Score, while models incorporating \nsegmentation in addition to classification achieved better \nspecificity. \nA summary of the results from the most significant fine-tuned \nViT models is presented in Figure 6. The large ViT model with \na patch size of 32 achieved the highest results among the four \nViT variants presented in experiment 1 and was used as the \nbenchmark for sub-sequent experiments. The utilization of the \nAdamW optimizer and focal loss did not surpass the ViT \nmodel, while fine -tuned class weights outperformed the \nbenchmark ViT, achieving an F1-Score of 0.825. As shown in \nFigure 6, the proposed ViT model ou tperformed the \nperformance of the aforementioned CNN models. The \nproposed model outperformed VGG19 by 0.125, 0.125, 0.126, \n0.071, 0.125, 0.125, and 0.031 for F1 -score, accuracy, \nbalanced accuracy, AUC, precision, recall, and specificity, \nrespectively. It o utperformed ResNet50 by 0.117, 0.117, \n0.116, 0.06, 0.117, 0.117, and 0.029 for F1 -score, accuracy, \nbalanced accuracy, AUC, precision, recall, and specificity, \nrespectively. In addition, it outperformed DenseNet201 by \n0.054, 0.054, 0.054, 0.02, 0.054, 0.054, and 0.014 for F1 -\nscore, accuracy, balanced accuracy, AUC, precision, recall, \nand specificity, respectively. \nThe resulting mode is more efficient than CNN and all the \nliterature studies. We added an extra step in the application of \nvision transformers by applying it in this new domain and \nachieving state-of-the-art results. The resulting model can be \napplied in real medical environments to provide accurate and \nfast assistance to ophthalmologists. Note that our deep \nlearning model is based directly on the ViT transformer \nnetwork architecture. We did not retrain the model from \nscratch, but we used the transfer learn ing technique to reuse \nthe pretrained ViT model with its pre -learned weights. We \noptimized a set of dense layers which have been used to train \nthe classifier for predicting the severity of diabetic retinopathy. \nThis process has been followed by huge litera ture studies in \nthe domain of medical image analysis [81]. The current study \nproved the suitability of transformer models to analyze \nmedical images.  \nAt this point, we optimized a robust DL model, but model \ntrustworthy needs an extra step of explainability. Decision \nvisualization is an important technique to highlight the \nimportant regions in the image where the model has used to \nmake its decision. The attention mechanism in ViT is a critical \ncomponent that enables these models to process and \nunderstand visual data. It involves multi -head self-attention, \nwhere input image patches are transformed into Query, Key, \nand Value vectors. Attention scores are computed based on the \nrelevance of patches to each other, and a weighted sum of \nValues captures informative relationships. This process is \nrepeated across multiple attention heads and layers. Figure 7 \nshows, from left to right, an input image, ViT output attention \nas heatmap, and the input image masked by ViT attention. \nLighter regions have more attention scores than darker regions \nand therefore they are more relevant to the final disease \ndiagnosis. We have a survey for the recent advances in model \nexplainability [ 84]. In the future, we will explore the \napplication of these techniques to enhance the \nunderstandability of the deep learning models.  \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n  \nFigure 7: Visualization of attention map for an input image. \n \nIV. CONCLUSION  \nIn this study, we explored the problem of automated severity \nstage detection of DR from fundus images. We proposed a \nvision transformer deep learning pipeline to capture long -\nrange dependencies in images. The study used the transfer \nlearning technique to train a large vision model on a relatively \nsmall dataset. For experimentation, the model has been trained \nusing the new real -world FGADR dataset. We checked the \nperformance of the proposed model using the original \nimbalanced data. Then, we improved the model performance \nwith collection of data and algorithm -based balancing \ntechniques. The proposed ViT model achieved superior results \nthan other baseline ViT and state -of-the-art CNN \narchitectures. To conclude, this study explored (1) the role of \nlearning the long-term spatial dependencies in medical images \nusing transformers to improve the performance of disease \ndetection, (2) the role of data balancing techniques (i.e., data \ncentric and algorithmic) to train more stable and accurate \nmodels, (3) the role of hyp erparameter optimization to \nimprove the performance of large models with frozen weights \nof early layers, and (3) the role of transfer learning to simplify \nthe model optimization process. Our ViT model achieved \nsuperior results compared to the CNN and ViT b aseline \narchitectures (i.e., 0.825, 0.825, 0.826, 0.964, 0.825, 0.825, \nand 0.956 for F1 -score, accuracy, balanced accuracy, AUC, \nprecision, recall, specificity, respectively). It is important to \nstudy the tradeoff between the model performance and its \ncomplexity. In the future, we will investigate the role of model \ncomplexity to enhance the results. This will need the testing of \nViT architectures in different experimental environments and \nwith different datasets. We will explore alternative approaches \nfor c ompressing transformers and developing lightweight \nmodels. We will improve the robustness of the model by \ntesting its performance using external validation and \nmeasuring its uncertainty. Additionally, we will investigate \nthe performance of ViT across different numbers of layers and \nheads. As our ViT model did not perform any lesion \nsegmentation, we will explore differ segmentation techniques \nfor fundus images to improve the detection of DR and to \nimprove the understandability of the resulting decision by \nproviding visual explainability of the model decision. Before \nAI model deployment, it must be tested regarding its fairness \nagainst any bias in data or algorithm, the robustness against \nadversarial attacks, the capability to protect the privacy of \npatient, the stability and robustness, and the ability to quantify \nand enhance modelâ€™s uncertainty. The extension of our model \nto be trustworthy will be considered in a separate future study. \nACKNOWLEDGMENT \nThe authors extend their appreciation to the Deputyship for \nResearch & Innovation, Ministry of Education in Saudi Arabia \nfor funding this research work through the project number (IF-\nPSAU-2022/01/19574). \nREFERENCES \n[1] P. Saeedi et al., â€œGlobal and regional diabetes prevalence \nestimates for 2019 and projections for 2030 and 2045: \nResults from the International Diabetes Federation \nDiabetes Atlas, 9th edition,â€ Diabetes Res. Clin. Pract., \nvol. 157, p. 107843, Nov. 2019, doi: \n10.1016/J.DIABRES.2019.107843. \n[2] I. D. Atlas, â€œIDF Diabetes Atlas,â€ 2023. \nhttps://diabetesatlas.org/. \n[3] WHO, â€œDiabetes,â€ 2023. https://www.who.int/news-\nroom/fact-sheets/detail/diabetes. \n[4] W. L. Alyoubi, W. M. Shalash, and M. F. Abulkhair, \nâ€œDiabetic retinopathy detection through deep learning \ntechniques: A review,â€ Informatics Med. Unlocked, vol. \n20, p. 100377, 2020. \n[5] P. Uppamma and S. Bhattacharya, â€œDeep Learning and \nMedical Image Processing Techniques for Diabetic \nRetinopathy: A Survey of Applications, Challenges, and \nFuture Trends,â€ J. Healthc. Eng., vol. 2023, 2023. \n[6] T.-E. Tan and T. Y. Wong, â€œDiabetic retinopathy: \nLooking forward to 2030,â€ Front. Endocrinol. \n(Lausanne)., vol. 13, 2022. \n[7] G. Selvachandran, S. G. Quek, R. Paramesran, W. Ding, \nand L. H. Son, â€œDevelopments in the detection of diabetic \nretinopathy: a state-of-the-art review of computer-aided \ndiagnosis and machine learning methods,â€ Artif. Intell. \nRev., vol. 56, no. 2, pp. 915â€“964, 2023. \n[8] â€œIDF Diabetes Atlas 9th Edition,â€ 9th edition, 2023. \nhttps://diabetesatlas.org/atlas/ninth-edition/. \n[9] Y. Zheng, M. He, and N. Congdon, â€œThe worldwide \nepidemic of diabetic retinopathy,â€ Indian J. Ophthalmol., \nvol. 60, no. 5, p. 428, 2012. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \n[10] D. S. W. Ting, G. C. M. Cheung, and T. Y. Wong, \nâ€œDiabetic retinopathy: global prevalence, major risk \nfactors, screening practices and public health challenges: \na review,â€ Clin. Experiment. Ophthalmol., vol. 44, no. 4, \npp. 260â€“277, 2016. \n[11] M. Z. Atwany, A. H. Sahyoun, and M. Yaqub, â€œDeep \nlearning techniques for diabetic retinopathy \nclassification: A survey,â€ IEEE Access, 2022. \n[12] S. Tummala, V. S. G. Thadikemalla, S. Kadry, M. Sharaf, \nand H. T. Rauf, â€œEfficientNetV2 Based Ensemble Model \nfor Quality Estimation of Diabetic Retinopathy Images \nfrom DeepDRiD,â€ Diagnostics, vol. 13, no. 4, p. 622, \n2023. \n[13] A. Sebastian, O. Elharrouss, S. Al-Maadeed, and N. \nAlmaadeed, â€œA Survey on Deep-Learning-Based \nDiabetic Retinopathy Classification,â€ Diagnostics, vol. \n13, no. 3, p. 345, 2023. \n[14] C. Raja and L. Balaji, â€œAn automatic detection of blood \nvessel in retinal images using convolution neural network \nfor diabetic retinopathy detection,â€ Pattern Recognit. \nImage Anal., vol. 29, pp. 533â€“545, 2019. \n[15] T. Nazir, A. Irtaza, Z. Shabbir, A. Javed, U. Akram, and \nM. T. Mahmood, â€œDiabetic retinopathy detection through \nnovel tetragonal local octa patterns and extreme learning \nmachines,â€ Artif. Intell. Med., vol. 99, p. 101695, 2019. \n[16] S. Gayathri, V. P. Gopi, and P. Palanisamy, â€œDiabetic \nretinopathy classification based on multipath CNN and \nmachine learning classifiers,â€ Phys. Eng. Sci. Med., vol. \n44, no. 3, pp. 639â€“653, 2021. \n[17] P. S. Washburn, â€œInvestigation of severity level of \ndiabetic retinopathy using adaboost classifier algorithm,â€ \nMater. Today Proc., vol. 33, pp. 3037â€“3042, 2020. \n[18] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, â€œA survey \nof convolutional neural networks: analysis, applications, \nand prospects,â€ IEEE Trans. neural networks Learn. \nSyst., 2021. \n[19] K. Xu, D. Feng, and H. Mi, â€œDeep convolutional neural \nnetwork-based early automated detection of diabetic \nretinopathy using fundus image,â€ Molecules, vol. 22, no. \n12, p. 2054, 2017. \n[20] N. S. P. Kazakh-British, A. A. Pak, and D. Abdullina, \nâ€œAutomatic detection of blood vessels and classification \nin retinal images for diabetic retinopathy diagnosis with \napplication of convolution neural network,â€ in \nProceedings of the 2018 international conference on \nsensors, signal and image processing, 2018, pp. 60â€“63. \n[21] B. K. Anoop, â€œBinary classification of DR-diabetic \nretinopathy using CNN with fundus colour images,â€ \nMater. Today Proc., vol. 58, pp. 212â€“216, 2022. \n[22] S. RÃªgo, M. Dutra-Medeiros, F. Soares, and M. \nMonteiro-Soares, â€œScreening for diabetic retinopathy \nusing an automated diagnostic system based on deep \nlearning: diagnostic accuracy assessment,â€ \nOphthalmologica, vol. 244, no. 3, pp. 250â€“257, 2021. \n[23] A. M. Pamadi, A. Ravishankar, P. A. Nithya, G. Jahnavi, \nand S. Kathavate, â€œDiabetic Retinopathy Detection using \nMobileNetV2 Architecture,â€ in 2022 International \nConference on Smart Technologies and Systems for Next \nGeneration Computing (ICSTSN), 2022, pp. 1â€“5. \n[24] P. Saranya, S. K. Devi, and B. Bharanidharan, â€œDetection \nof Diabetic Retinopathy in Retinal Fundus Images using \nDenseNet based Deep Learning Model,â€ in 2022 \nInternational Mobile and Embedded Technology \nConference (MECON), 2022, pp. 268â€“272. \n[25] W. Mudaser, P. Padungweang, P. Mongkolnam, and P. \nLavangnananda, â€œDiabetic Retinopathy Classification \nwith pre-trained Image Enhancement Model,â€ in 2021 \nIEEE 12th Annual Ubiquitous Computing, Electronics & \nMobile Communication Conference (UEMCON), 2021, \npp. 629â€“632. \n[26] Y. S. Boral and S. S. Thorat, â€œClassification of diabetic \nretinopathy based on hybrid neural network,â€ in 2021 5th \nInternational Conference on Computing Methodologies \nand Communication (ICCMC), 2021, pp. 1354â€“1358. \n[27] H. Jiang, K. Yang, M. Gao, D. Zhang, H. Ma, and W. \nQian, â€œAn interpretable ensemble deep learning model \nfor diabetic retinopathy disease classification,â€ in 2019 \n41st annual international conference of the IEEE \nengineering in medicine and biology society (EMBC), \n2019, pp. 2045â€“2048. \n[28] H. Kaushik, D. Singh, M. Kaur, H. Alshazly, A. Zaguia, \nand H. Hamam, â€œDiabetic retinopathy diagnosis from \nfundus images using stacked generalization of deep \nmodels,â€ IEEE Access, vol. 9, pp. 108276â€“108292, 2021. \n[29] W. Zhang et al., â€œAutomated identification and grading \nsystem of diabetic retinopathy using deep neural \nnetworks,â€ Knowledge-Based Syst., vol. 175, pp. 12â€“25, \n2019. \n[30] V. Bellemo et al., â€œArtificial intelligence using deep \nlearning to screen for referable and vision-threatening \ndiabetic retinopathy in Africa: a clinical validation \nstudy,â€ Lancet Digit. Heal., vol. 1, no. 1, pp. e35â€“e44, \n2019. \n[31] Y. Xie et al., â€œArtificial intelligence for \nteleophthalmology-based diabetic retinopathy screening \nin a national programme: an economic analysis modelling \nstudy,â€ Lancet Digit. Heal., vol. 2, no. 5, pp. e240â€“e249, \n2020. \n[32] C. P. Wilkinson et al., â€œProposed international clinical \ndiabetic retinopathy and diabetic macular edema disease \nseverity scales,â€ Ophthalmology, vol. 110, no. 9, pp. \n1677â€“1682, 2003. \n[33] Y. Zhou, B. Wang, L. Huang, S. Cui, and L. Shao, â€œA \nBenchmark for Studying Diabetic Retinopathy: \nSegmentation, Grading, and Transferability,â€ IEEE \nTrans. Med. Imaging, vol. 40, no. 3, pp. 818â€“828, 2021, \ndoi: 10.1109/TMI.2020.3037771. \n[34] C. Harshitha, A. Asha, J. L. S. Pushkala, R. N. S. \nAnogini, and C. Karthikeyan, â€œPredicting the stages of \ndiabetic retinopathy using deep learning,â€ in 2021 6th \ninternational conference on inventive computation \ntechnologies (ICICT), 2021, pp. 1â€“6. \n[35] X. Luo et al., â€œMVDRNet: Multi-view diabetic \nretinopathy detection by combining DCNNs and attention \nmechanisms,â€ Pattern Recognit., vol. 120, p. 108104, \n2021. \n[36] T. AraÃºjo et al., â€œGRADUATE: Uncertainty-aware deep \nlearning-based diabetic retinopathy grading in eye fundus \nimages,â€ Med. Image Anal., vol. 63, p. 101715, 2020. \n[37] N. S. Shaik and T. K. Cherukuri, â€œHinge attention \nnetwork: A joint model for diabetic retinopathy severity \ngrading,â€ Appl. Intell., vol. 52, no. 13, pp. 15105â€“15121, \n2022. \n[38] Y. Li, Z. Song, S. Kang, S. Jung, and W. Kang, â€œSemi-\nsupervised auto-encoder graph network for diabetic \nretinopathy grading,â€ IEEE Access, vol. 9, pp. 140759â€“\n140767, 2021. \n[39] X. Wang, Y. Lu, Y. Wang, and W.-B. Chen, â€œDiabetic \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nretinopathy stage classification using convolutional \nneural networks,â€ in 2018 IEEE International Conference \non Information Reuse and Integration (IRI), 2018, pp. \n465â€“471. \n[40] A. Asad, A. T. Azar, N. El-Bendary, and A. E. \nHassaanien, â€œAnt colony based feature selection \nheuristics for retinal vessel segmentation,â€ arXiv Prepr. \narXiv1403.1735, 2014. \n[41] U. Kaggle San Francisco, CA, â€œDiabetic Retinopathy \nDetection EYEPACS Dataset.â€ . \n[42] U. APTOS, Atlanta, GA, â€œAPTOS 2019 Blindness \nDetection.â€ . \n[43] U. San Diego, CA, â€œThe STARE Project, Shiley Eye \nCenter,â€ 2004. . \n[44] I. and U. Kauppi, Tomi and Kalesnykiene, Valentina and \nKamarainen, Joni-Kristian and Lensu, Lasse and Sorri, \nâ€œDIARETDB0: Evaluation database and methodology for \ndiabetic retinopathy algorithms,â€ Mach. Vis. Pattern \nRecognit. Res. Group, Lappeenranta Univ. Technol. \nFinl., vol. 73, pp. 1â€“17, 2006. \n[45] L. Giancardo et al., â€œExudate-based diabetic macular \nedema detection in fundus images using publicly \navailable datasets,â€ Med. Image Anal., vol. 16, no. 1, pp. \n216â€“226, 2012. \n[46] M. Niemeijer et al., â€œRetinopathy online challenge: \nautomatic detection of microaneurysms in digital color \nfundus photographs,â€ IEEE Trans. Med. Imaging, vol. \n29, no. 1, pp. 185â€“195, 2009. \n[47] E. DecenciÃ¨re et al., â€œFeedback on a publicly distributed \nimage database: the Messidor database,â€ Image Anal. \nStereol., vol. 33, no. 3, pp. 231â€“234, 2014. \n[48] E. Decenciere et al., â€œTeleOphta: Machine learning and \nimage processing methods for teleophthalmology,â€ Irbm, \nvol. 34, no. 2, pp. 196â€“203, 2013. \n[49] T. Li, Y. Gao, K. Wang, S. Guo, H. Liu, and H. Kang, \nâ€œDiagnostic assessment of deep learning algorithms for \ndiabetic retinopathy screening,â€ Inf. Sci. (Ny)., vol. 501, \npp. 511â€“522, 2019. \n[50] R. Liu et al., â€œDeepdrid: Diabetic retinopathyâ€”grading \nand image quality estimation challenge,â€ Patterns, vol. 3, \nno. 6, p. 100512, 2022. \n[51] P. Porwal et al., â€œIndian diabetic retinopathy image \ndataset (IDRiD): a database for diabetic retinopathy \nscreening research,â€ Data, vol. 3, no. 3, p. 25, 2018. \n[52] S. Pachade et al., â€œRetinal fundus multi-disease image \ndataset (rfmid): A dataset for multi-disease detection \nresearch,â€ Data, vol. 6, no. 2, p. 14, 2021. \n[53] Z. Han, B. Yang, S. Deng, Z. Li, and Z. Tong, â€œCategory \nweighted network and relation weighted label for diabetic \nretinopathy screening,â€ Comput. Biol. Med., vol. 152, p. \n106408, 2023. \n[54] S. Atasever, N. AzgÄ±noglu, D. S. TerzÄ±, and R. TerzÄ±, â€œA \ncomprehensive survey of deep learning research on \nmedical image analysis with focus on transfer learning,â€ \nClin. Imaging, 2022. \n[55] A. Solano, K. N. Dietrich, M. MartÃ­nez-Sober, R. \nBarranquero-CardeÃ±osa, J. Vila-TomÃ¡s, and P. \nHernÃ¡ndez-CÃ¡mara, â€œDeep Learning Architectures for \nDiagnosis of Diabetic Retinopathy,â€ Appl. Sci., vol. 13, \nno. 7, p. 4445, 2023. \n[56] M. M. Islam, H.-C. Yang, T. N. Poly, W.-S. Jian, and Y.-\nC. J. Li, â€œDeep learning algorithms for detection of \ndiabetic retinopathy in retinal fundus photographs: A \nsystematic review and meta-analysis,â€ Comput. Methods \nPrograms Biomed., vol. 191, p. 105320, 2020. \n[57] N. Tsiknakis et al., â€œDeep learning for diabetic \nretinopathy detection and classification based on fundus \nimages: A review,â€ Comput. Biol. Med., vol. 135, p. \n104599, 2021. \n[58] D. Das, S. K. Biswas, and S. Bandyopadhyay, â€œA critical \nreview on diagnosis of diabetic retinopathy using \nmachine learning and deep learning,â€ Multimed. Tools \nAppl., vol. 81, no. 18, pp. 25613â€“25655, 2022. \n[59] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. \nDosovitskiy, â€œDo vision transformers see like \nconvolutional neural networks?,â€ Adv. Neural Inf. \nProcess. Syst., vol. 34, pp. 12116â€“12128, 2021. \n[60] A. Dosovitskiy et al., â€œAn image is worth 16x16 words: \nTransformers for image recognition at scale,â€ arXiv \nPrepr. arXiv2010.11929, 2020. \n[61] Y. Liu et al., â€œA survey of visual transformers,â€ IEEE \nTrans. Neural Networks Learn. Syst., 2023. \n[62] F. Shamshad et al., â€œTransformers in medical imaging: A \nsurvey,â€ Med. Image Anal., p. 102802, 2023. \n[63] R. Sun, Y. Li, T. Zhang, Z. Mao, F. Wu, and Y. Zhang, \nâ€œLesion-aware transformers for diabetic retinopathy \ngrading,â€ in Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition, 2021, pp. \n10938â€“10947. \n[64] S. A. Kamran, K. F. Hossain, A. Tavakkoli, S. L. \nZuckerbrod, and S. A. Baker, â€œVtgan: Semi-supervised \nretinal image synthesis and disease prediction using \nvision transformers,â€ in Proceedings of the IEEE/CVF \nInternational Conference on Computer Vision, 2021, pp. \n3235â€“3245. \n[65] S. Yu et al., â€œMil-vt: Multiple instance learning enhanced \nvision transformer for fundus image classification,â€ in \nMedical Image Computing and Computer Assisted \nInterventionâ€“MICCAI 2021: 24th International \nConference, Strasbourg, France, September 27â€“October \n1, 2021, Proceedings, Part VIII 24, 2021, pp. 45â€“54. \n[66] A. Papadopoulos, F. Topouzis, and A. Delopoulos, â€œAn \ninterpretable multiple-instance approach for the detection \nof referable diabetic retinopathy in fundus images,â€ Sci. \nRep., vol. 11, no. 1, pp. 1â€“15, 2021. \n[67] N. S. Kumar and B. R. Karthikeyan, â€œDiabetic \nRetinopathy Detection using CNN, Transformer and \nMLP based Architectures,â€ in 2021 International \nSymposium on Intelligent Signal Processing and \nCommunication Systems (ISPACS), 2021, pp. 1â€“2. \n[68] Z. Zhang, G. Sun, K. Zheng, J.-K. Yang, X. Zhu, and Y. \nLi, â€œTC-Net: A joint learning framework based on CNN \nand vision transformer for multi-lesion medical images \nsegmentation,â€ Comput. Biol. Med., p. 106967, 2023. \n[69] C. Adak, T. Karkera, S. Chattopadhyay, and M. Saqib, \nâ€œDetecting Severity of Diabetic Retinopathy from Fundus \nImages using Ensembled Transformers,â€ arXiv Prepr. \narXiv2301.00973, 2023. \n[70] Z. Gu, Y. Li, Z. Wang, J. Kan, J. Shu, and Q. Wang, \nâ€œClassification of Diabetic Retinopathy Severity in \nFundus Images Using the Vision Transformer and \nResidual Attention,â€ Comput. Intell. Neurosci., vol. 2023, \n2023. \n[71] A. Parvaiz, M. A. Khalid, R. Zafar, H. Ameer, M. Ali, \nand M. M. Fraz, â€œVision transformers in medical \ncomputer visionâ€”A contemplative retrospection,â€ Eng. \nAppl. Artif. Intell., vol. 122, p. 106126, 2023. \n[72] E. Z. Ye, J. Ye, and E. H. Ye, â€œApplications of Vision \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 VOLUME XX, 2017 \nTransformers in Retinal Imaging: A Systematic Review,â€ \n2023. \n[73] G. M. Weiss, H. He, and Y. Ma, â€œFoundations of \nImbalanced Learning. Imbalanced Learning: \nFoundations, Algorithms, and Applications. Hoboken.â€ \nNJ, USA: John Wiley & Sons, 2013. \n[74] R. MÃ¼ller, S. Kornblith, and G. E. Hinton, â€œWhen does \nlabel smoothing help?,â€ Adv. Neural Inf. Process. Syst., \nvol. 32, 2019. \n[75] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. DollÃ¡r, \nâ€œFocal loss for dense object detection,â€ in Proceedings of \nthe IEEE international conference on computer vision, \n2017, pp. 2980â€“2988. \n[76] M. Abadi et al., â€œTensorflow: a system for large-scale \nmachine learning.,â€ in Osdi, 2016, vol. 16, no. 2016, pp. \n265â€“283. \n[77] L. Liu et al., â€œOn the variance of the adaptive learning \nrate and beyond,â€ arXiv Prepr. arXiv1908.03265, 2019. \n[78] I. Loshchilov and F. Hutter, â€œDecoupled weight decay \nregularization,â€ arXiv Prepr. arXiv1711.05101, 2017. \n[79] H. Gandhi, K. Agrawal, U. Oza, and P. Kumar, â€œDiabetic \nRetinopathy Classification Using Pixel-Level Lesion \nSegmentation,â€ in Futuristic Trends in Networks and \nComputing Technologies: Select Proceedings of Fourth \nInternational Conference on FTNCT 2021, 2022, pp. \n405â€“417. \n[80] S. Garg, T. Vu, and A. Moschitti, â€œTanda: Transfer and \nadapt pre-trained transformer models for answer sentence \nselection,â€ in Proceedings of the AAAI conference on \nartificial intelligence, 2020, vol. 34, no. 05, pp. 7780â€“\n7788. \n[81]  K. He et al., â€œTransformers in medical image analysis,â€ \nIntell. Med., vol. 3, no. 1, pp. 59â€“78, 2023. \n[82] Mediouni, M., Madiouni, R., Gardner, M. and Vaughan, \nN., 2020. Translational medicine: challenges and new \northopaedic vision (Mediouni-Model). Current \northopaedic practice, 31(2), pp.196-200. \n[83] Mediouni, M., R. Schlatterer, D., Madry, H., Cucchiarini, \nM. and Rai, B., 2018. A review of translational medicine. \nThe future paradigm: how can we connect the orthopedic \ndots better?. Current medical research and opinion, 34(7), \npp.1217-1229. \n[84] El-Sappagh, S., Alonso, J.M., Islam, S.R., Sultan, A.M. \nand Kwak, K.S., 2021. A multilayer multimodal detection \nand prediction model based on explainable artificial \nintelligence for Alzheimerâ€™s disease. Scientific reports, \n11(1), p.2660. \n \n \nWaleed Nazih  received B .Sc. and M.Sc. degrees \nfrom the Faculty of Computers and Information, \nCairo University, Egypt. In 2021, he received a Ph.D. \nfrom the Faculty of Computers and Information \nSciences, Ain Shams University, Egypt. He is \ncurrently working as a computer science lecturer with \nthe Department of Computer Science, at Prince \nSattam Bin Abdulaziz University, Saudi Arabia. His \nresearch interests include machine learning, \nhealthcare, voice over IP (VoIP) security, natural \nlanguage processing (NLP), and Arabic language resources. \n \nAhmad O. Aseeri  is currently an Assistant \nProfessor at the Department of Computer \nScience, College of Computer Engineering and \nSciences, Prince Sattam bin Abdulaziz \nUniversity, Saudi Arabia. He received his \nbachelorâ€™s degree in computing from King Saud \nUniversity, Saudi A rabia; M.Sc. in Computer \nScience from University of Wisconsin -\nMilwaukee, United States; and Ph.D. in \nComputer Science from Texas Tech University, United States. Dr. Aseeriâ€™s \nresearch interest is in the field of Artificial Intelligence, having the primary \nfocus in machine learning-based optimizations and vulnerability analysis for \nresource-constraint IoTs and Physical Unclonable Functions. Dr Aseeri has \nalso research works in optimization and modeling methods for healthcare \napplications, time series forecasting, and data mining with direct application \nto clustering techniques, including K - means and bisecting memory -aware \nK-means for big data. \n \nOsama Youssef Atallah  received his B.Eng. \nand Ph.D. degrees from Faculty of Computing \nSciences and Engineering, De Montfort \nUniversity (Leicester), UK. He is currently \nworking as a Senior lecturer/researcher at the \nDepartment of Biomedical Engineering, Medical \nResearch Institut e, Alexandria University, \nAlexandria Egypt. His research interests include \napplied machine learning/deep learning, natural \nlanguage processing, and applied machine \nlearning/deep learning for medical image diagnosis. \n \nShaker El -Sappagh received the bachelorâ€™s \ndegree in computer science from Information \nSystems Department, Faculty of Computers and \nInformation, Cairo University, Egypt, in 1997, \nand the masterâ€™s degree from the same university \nin 2007. He received the Ph.D. degrees in \ncomputer science from Information Systems \nDepartment, Faculty of Computers and \nInformation, Mansura University, Mansura, \nEgypt in 2015. In 2003, he joined the Department \nof Information Systems, Faculty of Computers \nand Information, Minia University, Egypt as a \nteaching assistant. Since June 2016, he has been \nwith the Department of Information Systems, Faculty of computers and \nInformation, Benha University as an assistant professor. He worked as a \nresearch professor at the UWB Wireless Communications Research Center \nin the Department of Information and Communic ation Engineering at Inha \nUniversity, South Korea for three years (2018 -2020).  He worked as a \nresearch professor at the Centro Singular de InvestigaciÃ³n en TecnoloxÃ­as \nIntelixentes (CiTIUS), Universidade de  Santiago de Compostela, Santiago \nde Compostela, Spain for one year (2021). Now, he is an associate professor \nat Galala University, Egypt since 2021. He is also a senior researcher at the \nCollege of Computing and Informatics, Sungkyunkwan University, South  \nKorea since 2021. He has publications in clinical decision support systems \nand semantic intelligence. His current research interests include machine \nlearning, medical informatics, (fuzzy) ontology engineering, distributed and \nhybrid clinical decision supp ort systems, semantic data modeling, fuzzy \nexpert systems, and cloud computing. He is a reviewer in many journals, \nand he is very interested in the diseasesâ€™ diagnosis and treatment research. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3326528\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}