{
  "title": "On the Explainability of Natural Language Processing Deep Models",
  "url": "https://openalex.org/W4285798540",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5005954815",
      "name": "Julia El Zini",
      "affiliations": [
        "American University of Beirut"
      ]
    },
    {
      "id": "https://openalex.org/A5008382926",
      "name": "Mariette Awad",
      "affiliations": [
        "American University of Beirut"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962807820",
    "https://openalex.org/W2963143606",
    "https://openalex.org/W2964118342",
    "https://openalex.org/W2972548215",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W2998991762",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2949227999",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W2887533108",
    "https://openalex.org/W2964072618",
    "https://openalex.org/W2972535098",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W1985912981",
    "https://openalex.org/W3090395639",
    "https://openalex.org/W3036453007",
    "https://openalex.org/W3035503910",
    "https://openalex.org/W2741040846",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W2809925683",
    "https://openalex.org/W1732154880",
    "https://openalex.org/W2936032120",
    "https://openalex.org/W2970959491",
    "https://openalex.org/W1503259811",
    "https://openalex.org/W2140610559",
    "https://openalex.org/W2105767494",
    "https://openalex.org/W175897666",
    "https://openalex.org/W2963847595",
    "https://openalex.org/W2962772482",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2952768212",
    "https://openalex.org/W6771862290",
    "https://openalex.org/W1831449718",
    "https://openalex.org/W2979949198",
    "https://openalex.org/W3005600385",
    "https://openalex.org/W2963224792",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2908082115",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2292919134",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2760327630",
    "https://openalex.org/W2963233086",
    "https://openalex.org/W3159434682",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W2598104261",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2962863107",
    "https://openalex.org/W2890353432",
    "https://openalex.org/W2251491951",
    "https://openalex.org/W1915485278",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2963095307",
    "https://openalex.org/W3035281110",
    "https://openalex.org/W2657631929",
    "https://openalex.org/W2964142373",
    "https://openalex.org/W2978238437",
    "https://openalex.org/W2951684477",
    "https://openalex.org/W2757667583",
    "https://openalex.org/W2497040301",
    "https://openalex.org/W2963020213",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3035563045",
    "https://openalex.org/W6777449117",
    "https://openalex.org/W2963039693",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W2788403449",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2118463056",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3116103312",
    "https://openalex.org/W2517063656",
    "https://openalex.org/W2945976633",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W2963123635",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W3007590609",
    "https://openalex.org/W2964027067",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2755111576",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2962911926",
    "https://openalex.org/W2977162702",
    "https://openalex.org/W2939556020",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W3048585918",
    "https://openalex.org/W2551814208",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2962886429",
    "https://openalex.org/W2963656855",
    "https://openalex.org/W2963954913",
    "https://openalex.org/W2970155250",
    "https://openalex.org/W2963082289",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W2926555354",
    "https://openalex.org/W2889624842",
    "https://openalex.org/W3152893301",
    "https://openalex.org/W1795234945",
    "https://openalex.org/W3102818708",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2889436406",
    "https://openalex.org/W4288366297",
    "https://openalex.org/W2608239929",
    "https://openalex.org/W2612364175",
    "https://openalex.org/W2559655401",
    "https://openalex.org/W2951025380",
    "https://openalex.org/W4289704137",
    "https://openalex.org/W4300756893",
    "https://openalex.org/W3082925502",
    "https://openalex.org/W4206236515",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2971051967",
    "https://openalex.org/W2996251235",
    "https://openalex.org/W3173813266",
    "https://openalex.org/W2962958286",
    "https://openalex.org/W4285674619",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W4297801963",
    "https://openalex.org/W4288335579",
    "https://openalex.org/W3012815759",
    "https://openalex.org/W4288103164",
    "https://openalex.org/W2852714836",
    "https://openalex.org/W4288375898",
    "https://openalex.org/W2963365341",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963715038",
    "https://openalex.org/W2769099080",
    "https://openalex.org/W4287548204",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4297734170",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3117696238",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2153332911",
    "https://openalex.org/W4303105513",
    "https://openalex.org/W2576410866",
    "https://openalex.org/W2150165932",
    "https://openalex.org/W4288620981",
    "https://openalex.org/W2489487449",
    "https://openalex.org/W810147176",
    "https://openalex.org/W4287195293"
  ],
  "abstract": "Despite their success, deep networks are used as black-box models with outputs that are not easily explainable during the learning and the prediction phases. This lack of interpretability is significantly limiting the adoption of such models in domains where decisions are critical such as the medical and legal fields. Recently, researchers have been interested in developing methods that help explain individual decisions and decipher the hidden representations of machine learning models in general and deep networks specifically. While there has been a recent explosion of work on Explainable Artificial Intelligence (ExAI) on deep models that operate on imagery and tabular data, textual datasets present new challenges to the ExAI community. Such challenges can be attributed to the lack of input structure in textual data, the use of word embeddings that add to the opacity of the models and the difficulty of the visualization of the inner workings of deep models when they are trained on textual data. Lately, methods have been developed to address the aforementioned challenges and present satisfactory explanations on Natural Language Processing (NLP) models. However, such methods are yet to be studied in a comprehensive framework where common challenges are properly stated and rigorous evaluation practices and metrics are proposed. Motivated to democratize ExAI methods in the NLP field, we present in this work a survey that studies model-agnostic as well as model-specific explainability methods on NLP models. Such methods can either develop inherently interpretable NLP models or operate on pre-trained models in a post hoc manner. We make this distinction and we further decompose the methods into three categories according to what they explain: (1) word embeddings (input level), (2) inner workings of NLP models (processing level), and (3) models‚Äô decisions (output level). We also detail the different evaluation approaches interpretability methods in the NLP field. Finally, we present a case-study on the well-known neural machine translation in an appendix, and we propose promising future research directions for ExAI in the NLP field.",
  "full_text": "On the Explainability of Natural Language\nProcessing Deep Models\nJULIA EL ZINI and MARIETTE AWAD, Department of Electrical and Com-\nputer Engineering, American University of Beirut.\nDespite their success, deep networks are used as black-box models with outputs that are not easily\nexplainable during the learning and the prediction phases. This lack of interpretability is significantly\nlimiting the adoption of such models in domains where decisions are critical such as the medical\nand legal fields. Recently, researchers have been interested in developing methods that help explain\nindividual decisions and decipher the hidden representations of machine learning models in general and\ndeep networks specifically. While there has been a recent explosion of work on Explainable Artificial\nIntelligence (ExAI) on deep models that operate on imagery and tabular data, textual datasets present\nnew challenges to the ExAI community. Such challenges can be attributed to the lack of input structure\nin textual data, the use of word embeddings that add to the opacity of the models and the difficulty of\nthe visualization of the inner workings of deep models when they are trained on textual data.\nLately, methods have been developed to address the aforementioned challenges and present satisfac-\ntory explanations on Natural Language Processing (NLP) models. However, such methods are yet to be\nstudied in a comprehensive framework where common challenges are properly stated and rigorous\nevaluation practices and metrics are proposed.\nMotivated to democratize ExAI methods in the NLP field, we present in this work a survey that studies\nmodel-agnostic as well as model-specific explainability methods on NLP models. Such methods can either\ndevelop inherently interpretable NLP models or operate on pre-trained models in apost-hoc manner. We\nmake this distinction and we further decompose the methods into three categories according to what\nthey explain: (1) word embeddings (input-level), (2) inner workings of NLP models (processing-level) and\n(3) models‚Äô decisions (output-level). We also detail the different evaluation approaches interpretability\nmethods in the NLP field. Finally, we present a case-study on the well-known neural machine translation\nin an appendix and we propose promising future research directions for ExAI in the NLP field.\nAuthors‚Äô address: Julia El Zini, jwe04@aub.edu.lb; Mariette Awad, mariette.awad@aub.edu.lb, Department of\nElectrical and Computer Engineering, American University of Beirut., P.O. Box 11-0236, Riad El Solh, Beirut,\nLebanon, 1107-2020.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted\nwithout fee provided that copies are not made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. Copyrights for components of this work owned by others\nthan ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on\nservers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from\npermissions@acm.org.\n¬© 2022 Association for Computing Machinery.\nXXXX-XXXX/2022/10-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: October 2022.\narXiv:2210.06929v1  [cs.CL]  13 Oct 2022\n2 ‚Ä¢ Julia El Zini and Mariette Awad\nCCS Concepts: ‚Ä¢ Computing methodologies ‚ÜíNatural language processing; Machine learning;\nArtificial intelligence; ‚Ä¢ Machine learning ‚ÜíNeural machine translation .\nAdditional Key Words and Phrases: ExAI, NLP, Language Models, Transformers, Neural Machine\nTranslation, Transparent Embedding Models, Explaining Decisions\nACM Reference Format:\nJulia El Zini and Mariette Awad. 2022. On the Explainability of Natural Language Processing Deep\nModels. 1, 1 (October 2022), 37 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nEver since their introduction, deep learning (DL) models are revolutionizing several NLP\napplications ranging from machine translation [8, 113, 119] to text summarization [107] and\nquestion answering [25, 121, 122]. AI-powered systems that mainly use DL models are reaching\nhighly accurate predictions that surpass human performances in some cases [100]. However,\ndue to their non-linear multilayered structure, deep networks are often seen as black-box\nmodels that achieve high performances; but in an opaque manner. Researchers and practitioners\noften question how credible these predictions are if the reasoning behind them is a highly non-\nlinear enigma that cannot be easily deciphered. This black-box nature of DL models gives rise\nto several criticisms of their non-transparent predictions. Transparency and interpretability\nare thus needed to establish user trust when black-box DL achieves a performance comparable\nto that of the human. Besides, weak DL models entail interpretability to investigate failure\ncases and direct the researchers in the proper paths [27]. The need for interpretability is even\nmore pronounced when DL models beat human performance where explanations can serve as\na machine teaching framework for the human to make better decisions. For instance, if the\ndecision-making system of Alpha Go [100] that beat the world champion in the Go game was\ntransparent, some creative moves can be taught to humans to help them learn the game or\neven extend their mental capabilities.\nTransparency is not only needed on the prediction level; some situations require DL models\nthat achieve explainability during the learning phases. For instance, DL models which learn\nfrom curated datasets might engender bias that is not easy to detect requiring a higher ex-\nplainability level [71]. Interpretability of the hidden representations of these networks and the\nunderstanding of the predictions of the whole category would suggest whether some protected\nattributes are affecting the predictions in a biased manner.\nRecently, the ExAI field has attracted researchers to develop explainability methods for black-\nbox deep networks [63, 88, 98] in general. While some of these methods can be directly applied\nto NLP models [6, 63, 88], others are specific to imagery datasets or Convolutional Neural\nNetworks (CNNs) that are not very suitable for neural machine understanding tasks [21, 30, 66,\n98]. It is worth mentioning that explainability and interpretability are used interchangeably in\nthis work. For definitions and terminologies, readers are referred to [68].\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 3\ndatasets or Convolutional Neural Networks (CNNs) that are not very suitable for neural\nmachine understanding tasks [21, 30, 66, 98]. We focus our survey on the interpretability of\nlanguage models or deep networks that operate on textual data. We make the distinction\nbetween the methods that develop inherently interpretable models and those that operate in\na post-hoc manner. We also make the distinction betweenmodel-specific and model-agnostic\nmethods and the level at which each method operates: input- (or embedding), processing- and\nprediction-level. Throughout this work, we use the terms explainability and interpretability\ninterchangeably. For more details on definitions and terminologies, we refer readers to [68].\nWe make the distinction between the methods that develop inherently interpretable models\nand those that operate in a post-hoc manner. We also make the distinction between model-\nspecific and model-agnostic methods and the level at which each method operates: input- (or\nembedding), processing- and prediction-level.\nAlthough interpretability is a relatively new research track in AI, different surveys have\nbeen compiled to highlight specific interpretability aspects [28, 37, 70, 74, 85]. Recently, [22]\naddressed interpretability of NLP models in their survey while focusing on explaining model‚Äôs\ndecision. Unlike [22], this work is not only limited to ExAI methods on the decision level.\nOur work here contributes to the community the first survey on the (1) interpretability of\nword embeddings models which constitute a crucial part of NLP, (2) the inner representations\nof NLP networks, and (3) the transformer models which presented a great debate on the\ninterpretability of their attention mechanisms. In addition to that, we survey existing work on\nthe explainability of individual model decisions with insights about research challenges and\nopportunities in that field. We also highlight different empirical setups, metrics, and datasets\nthat NLP researchers rely on to evaluate their ExAI methods.\nThe rest of this paper is organized as follows: Section 2 presents the challenges that general\nExAI methods face with textual datasets or NLP models and Section 3 presents the related\nsurveys to ExAI. Then, Section 4 presents the terminology used in this paper with respect to the\nthree proposed dimensions. The rest of the sections are focused on the interpretability of NLP\nmodels on three levels: the input, the processing, and the output. Section 5 surveys existing work\non the interpretability of word embeddings serving as inputs to NLP models, Section 6 and 7\npresent the interpretability methods applied to the inner representations of Recurrent Neural\nNetworks (RNNs) and transformers, with their debatable attention mechanisms, respectively\nduring the processing phase and Section 8 focuses on the explanations of individual predictions\nor outputs. Finally, we end with some concluding remarks and future directions in Sections 10\nand 9 while presenting a case study on neural machine translation in an appendix.\n2 CHALLENGES IN EXAI ON NLP MODELS\nIn the traditional learning framework, textual datasets present many challenges such as\npolysemy, sarcasm, slang, cultural effect and ambiguity. Explainability methods proliferate\nthese challenges. In what follows, we identify some text-specific challenges that hamper the\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n4 ‚Ä¢ Julia El Zini and Mariette Awad\napplication of general ExAI methods into NLP models. Those challenges motivate the necessity\nof addressing ExAI in NLP independently from other ExAI techniques.\nFirst, some of the explainability methods provide their explanations in terms of specific\ninput features, such as pixels. These features are not as straightforward in textual datasets. The\nmajority of NLP models operate on embeddings that are opaque representations, as opposed to\npixels or numerical values. Providing explanations in terms of specific embedding dimensions\nwill not have a practical implication for the explanations. Additionally, the inner workings\nof deep networks trained on text cannot be easily visualized as opposed to visual networks.\nFurther processing on the inner encodings is required to dissect the learned knowledge.\nWhen a model‚Äôs decision is explained by the words that contributed to the prediction, further\nrefinement is needed. For instance, explaining decisions in terms of the input features can be\neasily formulated when the input is numerical or imagery where decisions can be reflected\nby clear features or pixels. This task becomes challenging with text where the syntactic\nand semantic features cannot be easily dissected in the input to interdependently serve as\nexplanations. After all, a word is a fusion of syntax, semantics, and previous context. When\nproviding the explanation as a set of words, one can thus inquire if the explanation model is\nattending to the part-of-speech tag, the entity, the meaning, or the accumulated context.\nLong-term dependencies, on the other hand, add to the challenges of ExAI methods for\nNLP models. Namely, textual explanations might not always be a set of consecutive words\nbut words with longer dependencies as opposed to neighboring pixels in images. Finally,\nmulti-lingual support for deep models language models specifically introduces new challenges\nto explainability where the encoded language semantics and context need further deciphering.\n3 RELATED SURVEYS\nGiven the infancy of the ExAI field, especially in the context of NLP models, there is a handful\nof surveys that describe its terminology, taxonomy, different methods, and evaluation frame-\nworks. Doshi et al. introduce the taxonomy of ExAI in [28] while setting the common ground\nfor rigorous evaluation of interpretability of machine learning models through application-\ngrounded, human-grounded, and functionally-grounded settings. Montavon et al. [70] focus on\nactivation maximization techniques, sensitivity analysis, Taylor decomposition, and relevance\npropagation. Their work is specific to post-hoc interpretability methods and does not discuss\ninherently transparent models. A subset of the interpretability methods is also surveyed in\n[85], where a comprehensive study is presented covering activation maximization, network in-\nversion, deconvolutional neural networks, and network dissection based visualization applied\non imagery datasets. Similarly, Guidotti et al. [39] focus on decision rules, features importance,\nsaliency masks, sensitivity analysis, partial dependence plot, prototype selection, and neurons\nactivation methods while mainly studying image and tabular datasets.\nIn [37], a brief survey that discusses linear proxy models, decision trees, automatic rules, and\nsaliency maps, is presented. This survey studies a few approaches that areinherently explainable\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 5\nFig. 1. Summary of surveys on ExAI aggregated by year, citation number, evaluation and type of\nconsidered data. The thickness of the bar reflects the scope of the presented work categorized into two\ngroups: broad and narrow. The thick bars represent broad scopes and the thin bars the narrower ones .\nsuch as attention networks, and disentangled representations that are designed to generate\nexplanations. Later, Nguyen et al. [74] discuss how the activation maximization approaches\nevolved. Their survey is limited to imagery datasets and is very specific to optimization\ntechniques in activation maximization. The survey of [110] is also specific to explainability\nmethods in the medical field, which implies signals and imagery datasets. Very recently, [23]\nintroduce the opportunities and challenges of ExAI while discussing the limitations of certain\nmethods and [84] present ExAI methods applied to reinforcement learning settings.\nThe majority of the surveys in the literature are either brief [29, 37] or are focused on imagery\ndatasets [23, 74, 85]. It is not until late 2020 that a survey on ExAI integrated with NLP models\nhas been introduced in [22]. In this survey, Danilevsky et al. categorized the explanation types\nand methods while focusing on visualizing techniques and presenting some gaps and research\ndirection in the ExAI-NLP area. A few months later, a survey on explanation-based human\ndebugging of NLP models[56] has been released. The survey targets explanatory debugging\nand human-in-the-loop debugging systems.\nWhile both papers, [22] and [56], targeted a similar framework; there are clear differences\nbetween the scopes, depth, and breadth. Our work is the first to survey existing work on the\nexplainability of word embeddings and the attention mechanism. Moreover, [22] focuses on\nthe interpretability methods that explain individual predictions and [56] focuses on interactive\nhuman-in-the-loop learning whereas our work is broader. We instead report on literature\nthat interprets the knowledge encoded by language models. Finally, [22] claimed that only\nfour papers are found in the literature targeting global explanations which are defined as the\nstudy of the predictive process independently of any particular input. While we acknowledge\nthe difficulty of finding such papers due to proper tagging with ExAI keywords, we dedicate\nthree sections (5, 6 and 7) to survey over 50 papers, that are missed by [22] and we highlight\nattempts to understand how NLP models process inputs and the information they encode.\nFigure 1 visualizes existing surveys since 2017 and highlights the broadness of their scope,\nthe data types that they study, their impact on the ExAI community (reflected by their citation\nnumber), and whether evaluation methods are included.\n4 TERMINOLOGY\nIn this survey, we study interpretability methods on NLP models over the thehow, the what, and\nthe which dimensions. Our first dimension is specific tohow the models are being explained, i.e.\nby design or in a post-hoc manner. The former methods develop models from scratch, such that\nthey are inherently interpretable while the latter methods explain pretrained black-box models.\nAlthough interpretable models are inherently transparent, they require modifying the model‚Äôs\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n6 ‚Ä¢ Julia El Zini and Mariette Awad\nFig. 2. The dimensions studied in this work within terminology used in the ExAI framework and a\nvisual comparison to the work of [22]. Missing arrows between consecutive dimensions reflect the\ninfeasibility of the integration of the proposed dimensions.\narchitecture and retraining huge models. A recent study [96], outlines the difference between\nboth frameworks and highlights the areas where developing explainability methods for black-\nbox models should be avoided in high-stake decision-making environments. In the second\ndimension, we categorize the interpretability attempts into three categories according to what\nthey explain. The first category interprets the word embeddings which is on the input level\nof most of the deep NLP networks. The second category interprets the inner representations\nof RNNs and transformers which is on the processing level of the DL. Finally, the third\ncategory interprets individual model decisions with respect to specific input features or neuron\nactivations that are on the output level of the networks. The third dimension addresses the\nquestion of which models are being interpreted by making the distinction between explanation\nmethods that are model-agnostic, i.e. can operate on any machine learning model, and those that\nare model-specific, i.e. tailored for specific architectures. Figure 2 highlights the contributions\nof this paper in terms of these three dimensions within the terminology used in the ExAI\nframework and highlights the difference between our work and [22].\n5 INTERPRETING WORD EMBEDDINGS\nIn this section, we discuss the explainability of word embeddings which constitute the input\nto the majority of NLP models. Word embedding models are the result of various optimization\nmethods and deep structures that represent words as low-dimensional dense continuous\nvectors [67, 79]. Although effective at encoding semantic and syntactic information, those\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 7\nvectors are notoriously hard to interpret. Additionally, they show little resemblance to the\ntheories of lexical semantics, which are based on familiar, discrete classes and relations.\nInterpretability of word embeddings is not only essential for developing transparent models\nbut it is desirable for lexicon induction, efficiency, and fairness [ 31]. For instance, if inter-\npretable, semantically or syntactically similar words can be easily extracted in lexicon induction.\nAdditionally, the evaluation embeddings would be transparent by examining the information\nencoded by the individual vectors. Interpretability has also a computational advantage on\nmany classification tasks where irrelevant dimensions can be disregarded. Finally, fairness can\nbe ensured by removing dimensions encoding protected attributes such as gender or race [12].\nRecently, different approaches have been suggested to improve the interpretability of word\nembeddings. Such methods, discussed next, either rely on visualization [65], impose sparsity\n[34, 73, 105] and dimension-specific constraints [128], use rotation techniques [32, 76, 95] or\nincorporate external knowledge [33, 49] and contextual information [80, 81, 108] to derive\nmore interpretable embeddings. Other approaches rely on neighborhood analysis to quantify\ninterpretable characteristics of a given embedding model[92]. Such analysis allows to dissect\nthe information that a given word embedding encodes and to explain the correlation between\nthe embedding model on different tasks. All these approaches are model-specific and the\ndistinction between post-hoc and inherent interpretability is later highlighted in Table 2.\n5.1 Sparsification of Embedding Spaces\nDense word embeddings cannot solely provide meaningful representations. For instance, in\ndense word embeddings, the representation of a word ùë§ is a dense vector of small positive or\nnegative values spreading several hundred dimensions. Those dimensions are also active (i.e.\nhaving non-zero values) for words of different types and domains. Consequently, an active\ndimension in a dense word embedding model cannot imply a specific semantic or syntactic\nform. Sparsifying word embedding models can thus map dimensions to meaning or syntax\nmaking embeddings more explainable. To illustrate this concept, we visualize in Figure 3,\n5 words for 5 randomly chosen dimensions from a dense (SVD300) and sparse (NNSE1000)\nembedding models as derived by [73]. We observe that the words in each dimension are more\nsemantically coherent for the sparse embedding model and thereby more interpretable.\nMurphy et al. [73] used a matrix factorization algorithm, Non-Negative Sparse Encoding\n(NNSE), to derive the first distributional model that satisfies sparsity, effectiveness and in-\nterpretability. The authors decompose the input representation ùëø ‚ààRùëö√óùëõ into two matrices,\nùë® ‚ààRùëö√óùëò and ùë´ ‚ààRùëò√óùëõ subject to sparsity and non-negativity constraints. Their problem is\nto find ùë®,ùë´ that minimize√çùëö\nùëñ=1 ||ùëøùëñ,: ‚àíùë®ùëñ,: √óùë´||2 +ùúÜ||ùë®ùëñ,:||1, subject to ùë®ùëñ,ùëó ‚â•0 and ùë´ùëñ,:ùë´T\nùëñ,: ‚â§1.\nThe optimization in the NNSE method requires heavy memory usage and cannot effectively\ndeal with streaming text data. Consequently, an online approach to deriving an interpretable\nword embedding model (OIWE) is proposed in [64]. OIWE uses projected gradient descent to\napply non-negative constraints on non-negative methods such as Skip-Gram. An unconstrained\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n8 ‚Ä¢ Julia El Zini and Mariette Awad\nFig. 3. Sets of 5 words for 5 randomly chosen dimensions from a dense (SVD300) and sparse (NNSE1000)\nembdding model as in [73]. Better semantic coherence is observed in the sparse NNSE1000 model.\noptimization problem is later proposed in [34] to transform any distributed representation\ninto sparse or binary vectors. Using sparse coding, each input vector ùëøùëñ is represented as a\nsparse linear combination of basis vectors. Using online adaptive gradient descent, longer and\nsparser vectors are derived as more interpretable ‚Äúovercomplete‚Äù representations.\nLater, in [104], the authors employed de-noising ùëò-sparse auto-encoder to obtain an inter-\npretable transformation of input embeddings, SParse Interpretable Neural Embeddings (SPINE).\nInput embeddings are projected into a new space ‚ààRùëö√óùë£ where embeddings are both sparse\nand non-negative. De-noising ùëò-sparse auto-encoder is used to train the model by minimizing\nthe combination of the reconstruction loss, the average sparsity loss, and the partial sparsity\nloss over the data set while capturing the sparsity constraints.\nDeriving the embedding models, discussed so far, relies on optimization methods that mainly\noperate in ùúñ-accurate regimes. Operating in a post-hoc manner might cause an accumulation\nof inaccuracies. Some methods introduce sparsity as a post-processing step. For instance, Sun\net al. [105] directly apply the sparsity constraint while computing the word embeddings of\nWord2Vec. This is achieved by introducing the l1 regularizer and employing regularized dual\naveraging to produce sparse representations. Additionally, Panigrahi et al. [75] developed an\nunsupervised method to generate Word2Sense where each dimension encodes a fine-grained\nsense. Word2Sense embeddings are probability distributions over senses where the probabilities\nof each word and sense are estimated and the Jensen Shannon divergence [35] is then used to\ncompute word similarities to eliminate redundant senses.\n5.2 Rotation of Embedding Spaces\nFrom a linear algebra perspective, having a transparent basis, every embedding vector can\nbe explained as a combination of understandable concepts. To identify these interpretable\ndimensions (i.e. basis), word spaces can be rotated while preserving the encoded information.\nRothe et al. [95] formulated this rotation as a decomposition of an embedding space into two\ncomponents: an interpretable orthogonal subspace and a ‚Äúremainder‚Äù subspace. The resulting\nsub-spaces and their orthogonal complements form the basis for an embedding calculus that\nsupports certain operations. The goal becomes to find an orthogonal matrix that transforms\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 9\nthe embedding space into an interpretable one with fewer dimensions. Different lexicons such\nas opinion, POS, and emotion are used to train the transformation matrix to minimize the\ndistance between lexicon pairs and words with identical labels. Consequently, the authors\nextended the ‚Äúking - man + woman=queen‚Äù analogy into operations like ‚Äú-1√óhate = love‚Äù.\nIn [76], interpretability is induced by factor rotation that reforms the word embedding\nmatrix to have a simple structure by a linear transformation. The rotation encourages each\nrow and column (word vector and dimension, respectively) to have a few large values. More\nspecifically, the rotation is a post-processing step that computes a rotated matrix minimizing\nthe rotation criterion. The latter was introduced in [20] by forcing a low complexity on the\nrows and the columns of the rotated matrix and is minimized using the gradient projection\nalgorithm. This approach can be seen as a combination of sparsification and rotation where\nembedding models are rotated while encouraging low complexities in the values. Likewise, [32]\ncomputes orthogonal transformations by randomly sampling word vectors and their target\nbefore deriving the orthogonal Procrustes closed-form solution.\nThe approaches discussed above enhance the explainability of existing model embeddings.\nHowever, the derived explanations are simple linear algebraic analogies, i.e. at the level of\nsimple grammatical relations such as negation, grammatical gender, and prefix derivation. In [1],\nthe explainability of embeddings was addressed from a different angle. Instead of making the\nembedding models more explainable, Allen et al. [1] targeted well-known linear relationships\nand derived their probabilistically-grounded interpretations. Specifically, the authors aim at\nexplaining why the embeddings often satisfy ùë§ùëè‚àó= ùë§ùëé ‚àó‚àíùë§ùëé +ùë§ùëè when word embeddings\nare trained using only word co-occurrence. This is achieved by first defining paraphrasing\nas ‚Äúan equivalence drawn between words and word sets by reference to the distributions\nthey induce over words around them‚Äù. Then, the authors show that paraphrasing determines\nlinear relationships hold whenever the embeddings factorize point-wise mutual information.\nFinally, the linear relationship follows when analogies between words are interpreted as word\ntransformations sharing the parameters. The mathematical definitions and proofs provided in\nthe paper establish the first rigorous explanation of the embeddings‚Äô linear relationships.\n5.3 Integrating External Knowledge\nThe previous two methods discussed the interpretability of word embeddings without the incor-\nporation of external knowledge. However, existing lexicons and ontologies help in building em-\nbeddings spaces that can, by construction, reflect the semantic and syntactic relationships and\nthat are thus more interpretable. For instance, Faruqui et al. [33] enhance the interpretability of\nembeddings by fine-tuning the embedding model using relational information from semantic\nlexicons. This is achieved by proposing a graph-based learning framework, ‚Äúretrofitting‚Äù, for\nincorporating lexical relational resources. Given an un-directed graph Œ© = (ùëâ,ùê∏ )of semantic\nrelationships and a word embedding model ÀÜùë∏ = (ÀÜùëû1,..., ÀÜùëûùëõ), the goal is to learn a refined\nembedding model, ùë∏ = (ùëû1,...,ùëû ùëõ), such that its column vectors (word embeddings) are close\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n10 ‚Ä¢ Julia El Zini and Mariette Awad\nin distance to their counterparts in ÀÜùë∏ and to their adjacents underŒ©. The optimization problem\nis thus to minimize √çùëõ\nùëñ=1\n\u0002\nùõºùëñ||ùëûùëñ ‚àíÀÜùëûùëñ||2 +√ç\n(ùëñ,ùëó)‚ààùê∏ ùõΩùëñùëó ||ùëûùëñ ‚àíùëûùëó||2\u0003\nThe embeddings are first trained\nindependently of the semantic lexicons and then through ‚Äúretrofitting‚Äù. ùë∏ is computed by\nsolving a system of linear equations using an efficient iterative updating method.\nIn some particular domains, knowledge graphs go beyond syntax and semantics and develop\nknowledge basis on higher-level categorical and scientific relations. The medical domain is a\nperfect illustration where [49], for instance, incorporates the rich categorical and taxonomic\nknowledge in the biomedical domain to leverage the interpretability of medical embeddings.\nThe authors learn a transformation matrix that transforms the word embeddings to a new\ninterpretable space according to the biomedical taxonomy while retaining its expressive\nfeatures. Similarly, Pelevina et al. [77] utilize an ego-network to transform word embeddings\ninto sense vectors. The authors define an ego-network as a set single nodes, ego, along with\nthe nodes they are connected to, alter. After learning word embeddings and building a graph\nof nearest neighbors based on vector similarities, word senses are induced by clustering the\nego-network. Such word senses can be effectively used in word sense disambiguation.\nAlthough based on a sound mathematical formulation, the previously discussed approaches\ndo not explicitly show the practicality of the derived explainable embeddings in tasks such as\nde-biasing the embedding models. Instead, [128] addressed the gender-neutral word embed-\ndings problem and aimed at eliminating gender influence while preserving essential gender\ninformation in specific dimensions. This was done by categorizing all the words into male-\ndefinition, female-definition, and gender-neutral according to WordNet definitions. Then,\nembeddings were learned by minimizing using stochastic gradient descent a combination of (1)\nthe word proximities (2) the negative distance between words in the female and male-definition\nseed words, and (3) the difference between female words and their male counterpart.\n5.4 Contextualized Embeddings\nThe word embeddings that we discussed so far are functions where every word, regardless of\nwhether it has more than one meaning, has a unique embedding vector. Peters et al. [81], pro-\nposed an embedding model, where words are represented by contextualized vectors that model\nthe syntactic and semantic characteristics of a word as well as disambiguation across linguistic\ncontexts, i.e., in the case of polysemy. Disambiguation is possible due to the formulation of the\nword representation as a function of the entire sentence in pre-trained bidirectional language\nmodels. While such models are discussed in Section 7, we focus here on the explainability of\ntheir embedding layer rather than the full model.\nThe work of [80] empirically ‚Äúdissects‚Äù the contextualized embeddings by evaluating their\nperformance in a suite of four NLP tasks. The authors investigate the intrinsic properties of\ncontextual vectors that are independent of the NLP model and architecture details by studying\nhow semantic and syntactic information is modeled throughout the network‚Äôs depth. The\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 11\nresults show that morphology is encoded in the word embedding layer, the local syntax in the\nearly contextual layers, and semantic information in the deeper layers.\nTenney et al. [109] studied the way sentence structure is modeled across a range of syntactic,\nsemantic, local, and long-range phenomena in contextualized embeddings. Through edge\nprobing, the authors showed that such embeddings outperform their non-contextualized\ncounterparts on syntactic tasks and not on semantic tasks, which shows that they are better\nat encoding syntax than higher-level semantics. Moreover, contextualized representations\nare able to encode long-term linguistic information which helps disambiguate longer-range\nsyntactic dependencies relations and structures. Contextualized embeddings are also studied\nwithin the gender bias framework in [ 127]. The analysis showed that some training data\ncontains significantly more male than female entities which gets reflected in embeddings that\nsystematically encode gender information in an unequal manner.\n5.5 Evaluating Embeddings Interpretability\nThe research work described above attempts at improving the interpretability of word embed-\ndings. The question that can be immediately asked targets the evaluation of the interpretability\nof the obtained embedding spaces. While a common rigorous evaluation method, that is based\non well-designed bench-marking datasets, is yet to be developed, researchers are currently\nutilizing a set of interesting experiments summarized in this section.\nThe interpretability of the sparsification methods is commonly evaluated through word in-\ntrusion which seeks to quantify how coherent the dimensions of a learned word representation\nare. The experiment goes as follows: from the learned representation, a dimension ùëñ is chosen\nand the vocabulary words are then ranked according to the variance of their values. The four\nhighest-ranked words are then chosen across each dimension and a word from the bottom\nhalf of the list is then added as an ‚Äúintruder‚Äù that human judges are asked to identify. The\nprecision of the human judges across different state-of-the-art sparse embeddings is reported\nin Table 1. In [ 105], the word intrusion task is extended and a new evaluation metric that\ndoes not require any human assessment. Given that the intruder word should be different\nfrom the top four words while those latter words should be similar, the ratio of the distance\nbetween the intruder word and top words to the distance between the top words is thus used\nas a quantitative metric. The higher the ratio is, the more interpretable the embedding state is.\nNNSE [73] SPOWV [34] SPINE [104] Word2Sense [75]\n92.33 41.75 74.83 75.3\nTable 1. Precision on the word intrusion task on the state-of-the-art sparse embedding spaces\nAdditionally, Rothe et al. [95] used the cosine similarity to decide whether two words are\nsynonyms or antonyms which reflects the level of interpretability of word connotations. The\ncosine similarity between embeddings was also used in polarity spectrum creation where the\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n12 ‚Ä¢ Julia El Zini and Mariette Awad\nmain task is to predict the spectrum of a word in a certain query. The morphological analogy\nwas also used to test how well these embeddings can encode the POS tags in the computed\nsubspaces. These experiments test the quality of a subspace rather than its interpretability.\nThe work in [33] evaluates the interpretability of embeddings as their ability in capturing se-\nmantic and syntactic aspects. Such tasks include the word similarity and the synonym-selection\ntests which are aligned with [95] discussed above. The syntactic relations test evaluates how\nwell the embeddings can encode relations of the form ‚Äúa is to b as c is to d‚Äù. Sentiment analysis\ntests the knowledge encoded in these representations. While these tests better target the\ninterpretability of the representations in terms of their semantic and syntactic relations, they\ndo not explicitly reflect the interpretability of certain dimensions.\nPeters et al. [81] dissect the word representations in bi-directional models and evaluate their\ninterpretability in terms of the quality of the interpretation on tasks such as semantic role\nlabeling, and constituency parsing, and named entity recognition. Later, this set of experiments\nis extended in [80] to include question answering, sentiment analysis, and textual entailment.\n5.6 Discussion\nThe evaluation methods discussed above mainly reflect the quality of embeddings rather than\ntheir interpretability. For instance, evaluating the embeddings on a POS tagging task shows\nhow well they encode some syntax properties without reflecting the exact dimension, or the set\nof dimensions, where such properties are encoded. In other words, the majority of experiments\nreport an overall performance on some NLP tasks without addressing the alignment between\na dimension and a particular context or syntactic aspect.\nSome interesting experiments are reported in [34, 73, 75, 104]. Murphy et al. [73] qualita-\ntively assess the interpretability of dimensions of the NNSE embeddings by investigating the\ndominating dimensions in the NNSE representations. Similarly, Faruqui et al. [34] consider\nthe dimension interpretability and qualitatively assessed whether the top-ranking words for a\nparticular dimension display semantic or syntactic groupings. [104] extend this experiment\nto SPINE and qualitatively compare the interpretability of SPINE to that of SPOW with two\nbaseline embedding models, word2vec and Glove. The authors examine the top participating di-\nmensions for some sampled words and study the top words from the participating dimensions.\nSimilar experiments are conducted by [75] in word2sense.\nThese experiments are paving the way towards a rigorous evaluation of embeddings‚Äô ex-\nplainability but are still lacking the commonality and objectivity aspects. Finally, the majority\nof these methods operate on embedding models and are thus model-specific. However, some\nof the obtained models are inherently interpretable; whereas others need further processing.\nTable 2 summarizes the discussed methods within the dimensions discussed in this work while\nhighlighting their reliance on common evaluation schemes and whether they need existing\nembedding spaces. Figure 4 reflects the interest in the interpretability of word embeddings\nby showing the number of papers we surveyed within each category since 2012. One can see\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 13\nthat the sparsification of embeddings was one of the earliest methods in the field. After the\nsuccess of bidirectional transformer models in 2017, contextualized embeddings are attracting\nresearchers to study their interpretability. Interest in other methods seems to be consistent\nand a general interest in the interpretability of word embeddings seems to be increasing.\nMethod Needs Existing Common Posthoc / References\nEmbedding? Evaluation Inherently\nSparsification Inherently [34, 64, 73] [104]\nx Inherently [75, 105]\nRotation x Inherently [1, 32, 76, 95]\nIntegrating External Knowledge x Inherently [33, 49, 77, 128]\nContextualized Embeddings x Post-hoc [80, 109, 127]\nInherently [81]\nTable 2. Summary of existing work on the interpretability of word embeddings\nFig. 4. Number of the surveyed papers in each method reflecting the interest since 2012.\n6 WHAT DO RNNS LEARN?\nAfter discussing the interpretability on the input level, we focus next on the processing\nlevel. Hence, we dissect the inner representations of RNNs. These methods either implement\ninherently interpretable RNN architectures or try to interpret existing RNN architectures in a\npost-hoc manner. Both approaches however operate in a model-specific way.\n6.1 Post-hoc Interpretation\nMost of the post-hoc interpretation methods try to dissect hidden knowledge in trained deep\nnetworks from syntax and semantic lens. One such lens is compositionality, defined in [57], as\na way to understand how such networks build sentence semantics from individual words. The\nauthors develop strategies based on heatmap and t-sne visualization by plotting unit values to\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n14 ‚Ä¢ Julia El Zini and Mariette Awad\nvisualize negation, intensification, and concessive clauses. Similar to [51], but using the unit‚Äôs\nsalience, the authors are able to compute the amount that each input contributes to the seman-\ntics of a sentence. These strategies show that Long Short-Term Memory Networks (LSTMs)\nare able to perform well due to their ability to maintain a sharp focus on essential keywords.\nMoreover, sentiment analysis tasks are sensitive to some dimensions in an embedding. More\nimportantly, neural models in NLP are able to learn the properties of local compositionality in\na subtle way while respecting asymmetry as in negation.\nFocusing on syntax, Blevins et al. [11] develop a set of experiments that investigate how the\ninternal representations in deep RNNs capture soft, hierarchical syntactic information without\nexplicit supervision. First, the authors extract word-level representations produced by layers\nof the RNN trained on tasks such as dependency parsing, machine translation, and language\nmodeling. A classifier is then trained to predict the POS tag and the parent, grand-parent,\nand great-grandparent constituent labels of that word. Another classifier is trained to check\nwhether a dependency arc exists between two words. The evaluation of these classifiers shows\nthat the representations learned by RNNs encode syntax beyond the explicit information\nencountered during training. The results demonstrate that the word representations produced\nin RNNs at different depths are highly correlated with syntactic features. In particular, deeper\nlayers are shown to capture higher-level syntax notions.\nThese findings agree with [111], where recurrent architectures are compared to non-recurrent\nones for their ability to model hierarchical syntax. Experiments on subject-verb agreement\nand logical inference show that recurrent architectures, LSTM specifically, are notably more\nrobust and present better generalization guarantees when longer sequences are encountered.\nThe same quest is applied to LSTMs in [ 60] to study how syntactic structures are encoded.\nThrough number agreement in English subject-verb dependencies experiments, the authors\nshow that LSTMs are able to capture a considerable amount of grammatical structures, but more\nexpressive architectures may be required to reduce errors in particularly complex sentences.\nNumber agreement is also studied later by [ 40] by including nonsensical sentences where\nRNNs cannot rely on semantic or lexical cues and by comparing to human intuition to show\nthat long-distance agreements are reliably encoded in an RNN. In [ 53], the role of context\nin LSTMs is analyzed through ablation studies by shuffling, replacing, and dropping prior\ncontext words. LSTMs were found to be able to use about 200 tokens of context on average.\nHowever, LSTMs struggle to distinguish between nearby context from distant history. More\ninterestingly, the model pays attention to the order of words only in the recently-processed\nsentence. LSTMs‚Äô predictions have been empirically explored in [52] along with the LSTMs‚Äô\nlearned representations. Specifically, long-range dependencies are investigated in character-\nlevel language to seek cells that identify high-level patterns such as line lengths, brackets,\nand quotes. Karpathy et al. [52] study ùëõ-gram model to conclude that LSTMs‚Äô performance\nimproves on characters that require long-range reasoning.\nThe discussed approaches address particular semantic and syntactic structures and study\nwhere and how these structures are encoded. Nonetheless, these approaches do not study how\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 15\nthe input is processed through these inner encodings. Very recently, Hou et al. [45] suggested\na novel interpretation framework inspired by computation theory. The authors are the first to\ndraw the analogy between Finite State Automata (FSA) and an interpretable inner mechanism\nfor sequential data processing. In their work, FSAs are learned from trained RNN models\nto achieve better interpretability of the latter. When inputting several sequences to an RNN\nmodel, the hidden state points are gathered, and similar ones are clustered. The FSA states are\nthe clusters, and the transitions between the states arise when one item of the input sequence\nis processed. Aside from the novel connection with FSA, an interesting advantage of [45] is its\nability to provide global explanations in classification tasks.\n6.2 Inherently Interpretable Models\nInherently interpretable RNNs are trained in an explainable way by adding transparency\nconstraints [118] and exploring tree and graph-like structures [59, 106]. In [118], the authors\ndevelop an inherently interpretable RNN, SISTA-RNN, based on the sequential iterative soft-\nthresholding algorithm and the idea of deep unfolding [43]. By allowing the RNN parameters\nto be seen as the parameters of a probabilistic model, the weights and outputs can retain their\nmeaning. Traditionally, given input-output training pairs (ùíôùëñ,ùíöùëñ)ùëñ=1,...,ùëÅ and model parameters\nùúΩ, RNNs learn ùúΩ according to the following optimization problem: min\nùúΩ\n√ç\nùëñ ùëì(ùíöùëñ, ÀÜùíöùëñ)subject\nto ÀÜùíöùëñ = ùëîùúÉ(ùíôùëñ),ùëñ = 1,...,ùëÅ , where ùëì is the loss function and ùëîis the conventional black-box\nRNN. By changing the optimization constraint to ÀÜùíöùëñ = ‚ÑéùúÉ(ùíôùëñ),ùëñ = 1,...,ùëÅ and ‚ÑéùúΩ (ùíôùëñ) =\narg min\nùëß\nPùúÉ(ùíõ,ùíôùëñ)with ‚Ñéattempting to solve an optimization problem PùúΩ that corresponds to a\nprincipled probabilistic model, the parameters ùúΩ become interpretable. SISTA-RNN is proved\nto achieve a higher performance on a sequential compressive sensing task.\nTai et al. [106] improved the semantic representation of LSTMs by building a semantic tree\non semantic topologies. Tree-LSTMs are similar to the traditional LSTM units but differ in\nthe gating vector and memory cell update that are dependent on the state of the children\n(tree notion). A forget gate per child allows Tree-LSTM to select the information in each child.\nLater, Liang et al. [59] learn interpretable representation following a wise graph construction\nparadigm. Their structure-evolving LSTM first considers each data element as a separate node\nin a graph. Then, nodes with high compatibility are recursively merged to form richer and\nmore interpretable encodings. Similarly, graph LSTMs [ 78] are formulated to incorporate\nlinguistic analysis. This approach is shown to encode richer linguistic knowledge improving\nthe performance of relation extraction [41, 78, 126, 129]. Although both LSTMs outperformed\nexisting systems, their evaluation remains under-investigated. For instance, [106] evaluate\ntheir LSTMs on sentiment analysis and semantic relatedness tasks. Moreover, the effectiveness\nof structure-evolving LSTMs is mainly evaluated on a non-NLP-related image segmentation\ntask without thorough testing on other modalities.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n16 ‚Ä¢ Julia El Zini and Mariette Awad\nNeural module networks jointly train deep ‚Äúmodules‚Äù for visual question answering offer-\ning visual explanations for an answer [ 4, 46]. While such networks are not fully within a\ntextual modality, i.e. they accept the question as text and an image as a reference, we briefly\ndiscuss them in this survey for completeness purposes. Neural module networks learn to parse\nquestions as executable modules to understand well synthetic visual QA domains [42]. The\nquestions are analyzed in a semantic parser before determining the basic computational units\nthat contribute to the answer [4]. Andreas et al. [3] learn the parameters for these modules\njointly via reinforcement learning on (world, question, answer) triples. Gupta et al. [42] extend\nthis approach and introduce modules that reason over a paragraph of text through symbolic\nreasoning over numbers and dates. Furthermore, an unsupervised auxiliary loss is suggested\nto extract arguments explaining specific events in the text.\n6.3 Discussion\nTable 3 summarizes the aforementioned approaches while highlighting their interpretability\nand evaluation methods. While [57] is inspired by a non-text-specific interpretability approach,\nthe rest of the methods are designed to suit NLP models. One can clearly see that a common\nevaluation is not established yet, some methods can be subjective [40] while others rely on\nhuman-annotated data [11, 106]. Such annotations are not directly related to explainability;\nthey however reflect some semantic or syntactic aspects such as subject-verb agreement or\nPOS tags that. Although a good performance can imply that the specific semantic or syntactic\naspect is indeed encoded in the model, it cannot directly imply explainability. Localization of\nthe encoded aspect in the model as well as input analysis in terms of these aspects are further\nneeded to achieve pure model transparency.\n7 WHAT DO TRANSFORMERS LEARN?\nAfter discussing the interpretability of general RNNs, we consider the popular transformer\nmodels [114]. Such models are based on attention mechanisms to handle ordered sequences\nof data. Transformers follow the encoder-decoder structure using stacked multi-head self-\nattention and fully connected layers. Bidirectional Encoder Representations from Transformers\n(BERT) [25] and GPT-2 [86], have been trained on huge text corpora and are currently used\nas state-of-the-art NLP models after fine-tuning on specific tasks. Different approaches have\nbeen studied to understand the inner dynamics of transformer models and to visualize the\nattention weights in order to better understand how they process input and why they do it so\nwell. All the discussed approaches are model-specific and operate in a post-hoc manner.\n7.1 Visualization of Transformers\nThe Explainability of transformers has been extensively addressed from a visualization per-\nspective by developing tools that allow the user to interact with such models to understand\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 17\nReference Inherently/\nPost-hoc\nModel Method Evaluation Method\n[52] (2015) Post-hoc LSTM Identification of high-level\npatterns\nReasoning onùëõ‚àígram mod-\nels, Subject-verb agreement\n[57] (2016) Post-hoc LSTM Heatmap T-SNE visualization\n[60] (2016) Post-hoc LSTM Probing Subject-verb agreement\nand logical inference\n[11] (2018) Post-hoc RNN Correspondence between net-\nwork depth and syntactic\ndepth\nPOS tag and dependency\nclassification\n[40] (2018) Post-hoc RNN Comparison to human intu-\nition\nSubject-verb agreement\n[53] (2018) Post-hoc LSTM Ablation Context localization\n[53] (2018) Post-hoc LSTM Ablation Context localization\n[106] (2015) Inherently LSTM Integration of semantic\ntopologies\nSentiment analysis and re-\nlation classification\n[118] (2016) Inherently RNN Sequential iterative soft-\nthresholding algorithm and\ndeep unfolding\nSequential compressive\nsensing task\n[59] (2017) Inherently LSTM Integration of knowledge\ngraphs\nSemantic object parsing\n[4] (2016) Inherently Module\nNetworks\nReinforcement Learning Visual question answering\n[46] (2017) Inherently Module\nNetworks\nJoint learning Visual question answering\n[42] (2019) Inherently Module\nNetworks\nUnsupervised auxiliary loss Reasoning over text\n[41, 78, 126,\n129] (2017-\n2020)\nInherently LSTM Encoding of richer linguistic\nknowledge\nRelation Extraction\nTable 3. Summary of interpretability attempts at understanding the inner representations of RNNs\ntheir inner mechanisms. For instance, the work of [54] presents an interactive tool to visu-\nalize attention and provides an interface to dynamically adjust the search tree and attention\nweights. Similarly, in [62], the authors proposed a flexible visualization library to visually\nanalyze models for natural language inference and machine comprehension that relies on\na perturbation-driven exploration strategy. Also, in [101], SEQ2SEQ-VIS, visual analytics is\npresented for sequence-to-sequence model debugging by visualizing the five stages of a seq2seq\nmodel: encoder, decoder, attention, prediction, and beam search. SEQ2SEQ-VIS also describes\nthe knowledge that the model has learned by relying on transitions of latent states and their\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n18 ‚Ä¢ Julia El Zini and Mariette Awad\nrelated neighbors. Finally, SEQ2SEQ-VIS provides an interactive method to manipulate the\nmodel internally and observe the impact on the output.\nExBERT [44] is another interactive visualization tool that uses linguistic annotations, mask-\ning, and nearest neighbor search to provide insights into contextual representations learned in\ntransformer models in three main components: (1) the attention, (2) the corpus and (3) the\nsummary view. Similarly, Vig [115] presents a tool to visualize BERT and GPT-2 models at\nthree different granularity levels: the attention-head level, the model level, and the neuron\nlevel. Table 4 summarizes these methods and the specific models they are applied on.\nReference Method Model\n[54] (2017) Visualization of attention weights Neural machine translation models\n[62] (2018) Perturbation-driven Natural language inference and ma-\nchine comprehension models\n[101] (2018) Visualization of encoder, decoder, atten-\ntion, prediction, and beam search\nSequence-to-sequence models\n[44] (2019) Linguistic annotations, masking, and near-\nest neighbor search\nBERT\n[115] (2019) Visualization on the level of attention-\nhead, model, neuron\nBERT and GPT-2\nTable 4. Summary of transformer visualization methods\n7.2 Is the Attention Mechanism Inherently Interpretable?\nControversy has accompanied attention mechanisms since their introduction. While some at-\ntention weights can provide reliable explanations [72]; some researchers showed that attention\ndistributions are not easily interpretable and require further processing [13, 47].\nIn an attempt to investigate these controversies, Vashishth et al. [112] manually analyzed\nattention mechanisms on several NLP tasks. The experiments showed that attention weights\nare interpretable indeed and are correlated with feature importance measures capturing several\nlinguistic notions. Similar findings were obtained by [72] on medical tasks where attention\nweights were efficient in the selection of most relevant segments in a medical document.\nThese methods, however, are specific to particular domains and linguistic notions and\nmight not be easily extendable to higher-level knowledge structures. Hence, a parallel line of\nwork attempted at proving that ‚Äúattention is not explanation‚Äù by considering more general\ncorrelations experiments. For instance, [47] show that there is no frequent correlation between\nthe attention weights and feature importance methods. Moreover, the authors identify attention\ndistributions that yield equivalent predictions through correlation computation.\nPrior to these researches, [ 91] focused on the reasoning capabilities of transformers by\nconsidering the recognizing textual entailment task. For this purpose, the attention patterns\nare visualized on hand-picked validation samples. Word-by-word attention is shown to resolve\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 19\nsynonym relationships, match multi-word expressions to single words, and ignore irrelevant\nparts. Moreover, when a deeper semantics level or common-sense knowledge connects two\nsentences, attention seems to be capable of capturing the semantics. However, when two\nsentences are not related, attention seems to be dominated by the last output vector.\nThe authors in [116] study the interaction between the syntax and the attention weights\nin GPT-2 [86]. The authors investigate the alignment between syntactic dependency and\nattention weights. The authors aggregate over the corpus the percentage of total attention of a\ngiven head that attends to tokens belonging to the given POS tag with syntactic features. The\nsame aggregation method is used to test the alignment between attention and dependency by\ncomputing the proportion of attention between tokens that are components of dependency\nrelations. To explore how long- and short-distance relations are captured, the number of tokens\nspanned by each head is computed, then attention dispersion is computed based on entropy.\nThe heatmap shows that most POS tags are disproportionately targeted by one or more heads.\nAttention heads that focus on one POS tags vary according to layer depths: determiners are\ntargeted in the early layers, while proper nouns are targeted in deeper layers. Additionally,\nthe alignment between attention and dependency relations is strongest in the middle layers.\nHeads in the early layers tend to focus on position rather than content, whereas attention\nheads in deeper layers target specific constructs. Finally, deeper layers capture longer-distance\nrelationships. A moderate correlation is found between the distance and entropy of attention\nand the attention distance is negatively correlated with dependency alignment.\nA common limitation of the discussed approaches is the lack of a unified definition of the\nexplainability of attention. Formal explainability definition in NLP, or the lack thereof, is\naddressed in [13], where identifiability of the attention weights is defined as their ability to\nbe uniquely identified from the attention head‚Äôs output. The study of identifiability is done\non attention weights and token contextualized embeddings and the aggregation of context\ninto hidden tokens. Input tokens are shown to retain their identity whereas the information\nidentity gradually decreases with depth. Pruthi et al. [83] manipulate the attention weights to\nwhitewash problematic tokens in explanations that affect the model fairness or accountability.\nThis is achieved by diminishing the weight assigned to these impermissible tokens.\nTable 5 summarizes the controversy around the attention weights while highlighting the\nmethods and the findings of each work. In summary, attention weights are moreinherently\ninterpretable than the parameters of general deep networks. However, some aspects of inter-\npretability need further processing and investigation of the attention weights.\n7.3 Interpretability of BERT\nBidirectional training is introduced in BERT [ 25] revolutionizing the training of language\nmodels. This section discusses the interpretability methods that study the why and the how\nsuch training works so well. Such dissection aids in understanding BERT [48, 108], its weights\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n20 ‚Ä¢ Julia El Zini and Mariette Awad\nReference Method Interpretable? Finding\n[72] (2018) Selection of most relevant\nsegments\nAbility of attention mechanism to iden-\ntify meaningful explanations\n[112] (2019) Analysis of weights on text\nclassification and text gener-\nation tasks\nHigh correlation between attention\nweights and feature importance of lin-\nguistic features\n[91] (2015) Weights visualization on rec-\nognizing textual entailement\nX Dominance of the last output vector\nover attention in some cases\n[27] (2017) Layer-wise relevance propa-\ngation (LRP) [6]\nX Importance of LRP to further interpret\nthe attention weights and the internal\nworkings of transformers\n[116] (2019) Alignment between syntac-\ntic dependency and attention\nthrough visualization and ag-\ngregation\nX (1) Disproportionality between heads\ntargeting POS, (2) Capturing of longer-\ndistance relationships by deeper layers\nand (3) Moderate correlation between\ndistance and entropy of attention\n[47] (2019) Correlation and counterfactu-\nals\nX No frequent correlation between atten-\ntion weights and gradient-based mea-\nsures of feature importance\n[13] (2019) Aggregation of context into\nhidden tokens\nX Preservation of the token identifiability\nthroughout the model and decrease of\ninformation identifiability with depth\n[83] (2020) Diminishing attention\nweights of impermissible\ntokens\nX Attention-based explanations can be\ndeceived especially within the fairness\ncontext\nTable 5. Summary of the literature discussing if the attention weights are inherently interpretable.\n[19, 87] and limitations [93]. More interestingly, it improves the performance of query retrieving\nby automatically discovering better prompts to use to retrieve and combine more accurate\nanswers as in [50]. The approaches discussed next describe the inner workings of BERT models,\ntheir attention weights, and methods used to dissect their inner knowledge.\nThe work of [ 87] can be considered as a general evaluation scheme for BERT attention\nweights. The authors proposed several methods to analyze the linguistic information in BERT.\nFirst, heatmaps of attention weights are explored to find linguistic patterns. Second, a maximum\nspanning tree is constructed for each sentence to check if the syntactic dependencies between\ntokens have been learned by the network. Sequence labeling tasks are also used to measure how\nimportant the learned features are for different tasks. Finally, the encoder weights of a high-\nresource language pair are used to initialize a low-resource language pair in order to assess\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 21\nthe generality of the learned features. These methods lead to insightful conclusions. Attention\nis shown to follow four different patterns: paying attention to the word itself, to the adjacent\nwords, and to the end of the sentence. Early layers tend to focus on short dependencies while\nhigher ones focus on long dependencies. As expected, training on larger datasets can induce\nbetter syntactic relationships, and syntactic dependencies get to be significantly encoded in at\nleast one attention head in each layer of the model. Moreover, early layers encode syntactic\ninformation, whereas semantic information is encoded in the upper layers, and starting from\nthe third layer, information about the input length starts to vanish.\nA study more focused on the attention heads is presented in [19]. By means of aggregation,\nmost of the heads are found to put little attention on the current token, whereas there is\na considerable amount of heads that heavily attend to adjacent tokens, especially in the\nearly layers. Moreover, the authors speculate that, when the head‚Äôs attention function is not\napplicable, the head attends to the end of the sentence token. Thus, gradient-based measures\nof feature importance are applied to show that attending to some end-of-sentence token\ndoes not have a substantial impact on BERT‚Äôs output. In order to study whether attention\nheads span a few words or attend broadly over many words, the average entropy of each\nhead‚Äôs attention distribution is computed. The results show that in the early layers, attention\nheads have widespread attention. To study the alignment between syntactic dependencies and\nattention weights, attention maps are extracted from BERT. Evaluation of the direction and\nthe prediction of the head shows that certain attention heads specialize in certain dependency\nrelationships. Finally, the coreference resolution test, where the antecedent selection accuracy\nis computed, shows that BERT heads achieve reasonable co-reference resolution performance.\nWhile [19] and [87] are focused on the attention heads and weights, Tenney et al. [ 108]\ndevelop a layer-based approach to interpret the encoded knowledge in BERT‚Äôs layers. Tenney et\nal. study the traditional NLP steps that BERT follows to investigate where linguistic information\nis formed. The authors employ probing techniques to understand the interactions within BERT\nbetter and to study at which layers the BERT network can resolve syntactic and semantic\nstructures. It is worth mentioning that the edge probing technique, introduced in [108], aims\nat quantifying the degree to which linguistic structures can be extracted from a pre-trained\nencoder. Two metrics are defined: a scalar mixing weight and a cumulative scoring. The former\nspecifies the most relevant combination of layers when a probing classifier is tested on BERT,\nwhereas the latter quantifies the score improvement on a probing task when a particular layer\nis considered. The results show that the traditional NLP steps are implicitly followed in order\nby the network: POS tags are processed earliest, then constituents, dependencies, semantic\nroles, and coreference are processed in the deep layers. In accordance with previous work,\nbasic syntactic information is encoded in the early network layers, while higher-level semantic\ninformation appears in deeper layers. Moreover, syntactic information is more localizable in a\nfew layers, whereas semantic information is generally spread across the network.\nJawhar et al. [48] address the same question as [108] and attempt at unpacking the elements\nof language structure learned by the layers of BERT. The authors also use probing techniques\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n22 ‚Ä¢ Julia El Zini and Mariette Awad\nto show phrasal representation in the lower layers of BERT is responsible for phrase-level\ninformation while richer hierarchy can be found in intermediate layers with surface features\nat the bottom, syntactic features in the middle, and semantic features at the top. Finally, the\ndeepest layers encode long-distance dependency such as subject-verb agreement. Jawhar et al.\n[48] also show that similar to tree-like structures, the linguistic information is encoded in a\nclassical compositional way.\nBERT‚Äôs ability to encode factual knowledge is studied by [82, 90] through linguistic tasks\nsuch as facts about entities, common sense, and general question answering. BERT is shown\nto encode relational knowledge without any fine-tuning and without access to structured\ndata like other NLP methods. This suggests that language models trained on huge corpora\ncan serve as an alternative to traditional knowledge bases [82]. Roberts et al. [90] consider\nfinetuning language models (such as BERT) to study their ability to encode and retrieve\nknowledge through natural language queries. The authors found that language models are\nable to perform well on factual tasks but are expensive to train and more opaque than shallow\nmethods. Moreover, they don‚Äôt provide any guarantees that knowledge can be updated or\nremoved over the course of training.\nReaders are referred to [93] where Rogers et al. survey existing work that dissects how and\nwhere BERT encodes semantic, syntactic, and word knowledge. The dissection is done via\nself-attention heads and throughout BERT layers. Moreover, [93] surveys modifications to\nBERT‚Äôs training objectives and architecture, describe the over-parameterization issue in BERT\ntraining, and report some of the compression approaches and future research directions.\n7.4 Discussion\nDue to their reliance on the attention mechanism, the interpretation of transformers is less\nchallenging than RNNs. In view of the way they are designed, attention weights are relatively\nmore interpretable than the conventional deep networks weights. This design made the evalu-\nation of transformers‚Äô inner workings more feasible by visualizing their weights and hidden\nrepresentations [54, 101, 115]. The controversy that accompanied the attention mechanisms\ntrying to address the extent to which the attention weights are explainable can lead to the\nfollowing conclusion. Relative to general deep networks weights, attention weights can be\nthought of as more interpretable. However, solely, their ability to provide full transparency\nor meaningful explanations is questionable. Further processing is needed to achieve proper\ntransparency of transformer models especially for long-distance temporal relations. More\nspecifically, when the task in hand is not a simple classification but a more complex task\nsuch as translation, question answering and natural language inference, attention weights\nmight not offer the desired interpretability [27, 47]. Mohankumar et al. [69] argue that when\nthe attention distribution is computed on input representations that are very similar to each\nother, they cannot provide very meaningful explanations. For this purpose, the authors of\n[69] diversify the hidden representations over which the distribution are computed for more\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 23\nfaithful explanations. Moreover, attention weights become less interpretable in deep layers\n[13, 116]. Hence, if the explanation at deeper layers requires further processing especially that\noutput vectors might dominate the attention weights as in [91].\nMoreover, similar to RNNs (Section 6), the evaluation of interpretability methods on trans-\nformers is achieved by designing methods that test specific semantic or syntactic aspect\nthrough alignment [116], correlations [47] and general explainability methods such as LRP [6]\nand weight visualization [91]. The first building block in the common explainability framework\nof transformers is presented in [13]. However, their definition and metric need to be extended\nto involve other syntactic and semantic explainability aspects discussed in this section. Finally,\ndeveloping only post-hoc interpretation methods on transformers can be explained by the\nfact that such models are already famous and retraining them is very computationally heavy.\nTraining BERT model on huge corpora, for instance, requires the same energy that five cars\nconsume on average during their lifetime according to [102].\n8 EXPLAINING MODEL‚ÄôS DECISIONS\nWhile the previous section opens the deepblack-box models to understand their representations,\nthis section focuses on highlighting the evidence supporting their decisions. We categorize\nthese approaches into post-hoc and inherently interpretations.\n8.1 Post-hoc Interpretation\nThese methods, consider a pre-trained model and analyze how such a model process a textual\ninput before producing a decision. When the model is black-box the interpretability method is\nmodel-agnostic. If some assumptions are made on the architecture, the method ismodel-specific.\n8.1.1 Model-agnostic Explanations. Given a black-box model ùëì : ùëã ‚Ü¶‚Üíùëå and an input ùë•, the\ngoal of model-agnostic interpretation methods is to explain the individual prediction ùëì(ùë•). The\nmajority of these methods rely on perturbing ùë• according to some distribution ùê∑ùë•.\nIn [88], Ribeiro et al. present LIME, one of the first state-of-the-art explainability algorithms,\nthat approximatesany classifier or regressor locally with an interpretable model. LIME presents\nthe interpretation ùëîfor the user in terms of comprehensible explanations such as bag-of-words.\nFormally, LIME minimizesL(ùëì,ùëî,ùúã )+Œ©(ùëî), where L(ùëì,ùëî,ùúã ), a measure of how unfaithful the\nexplanation ùëîis to the original modelùëì with the unfaithfulness is computed in a locality defined\nby a proximity measure ùúã and Œ©(ùëî), the complexity of the explanation ùëî. The explanation\nis created by approximating ùëì locally by an interpretable one. L(ùëì,ùëî,ùúã )is approximated by\nsampling around an input ùë• according to ùúãùë•, performing perturbations on the input then\nexplaining linearly, respecting thus local faithfulness.\nLater in 2018, Ribeiro et al. [89] argue that the coverage of the explanations generated by\nLIME is not clear. For instance ‚Äúnot‚Äù, ‚Äúgood‚Äù could be an explanation of negative sentiment and\n‚Äúnot‚Äù, ‚Äúbad‚Äù could be that of a positive one. Thus, the generated explanation does not clearly\nstate when the word ‚Äúnot‚Äù has a positive/negative influence on the sentiment. Ribeiro et al.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n24 ‚Ä¢ Julia El Zini and Mariette Awad\n[89] define ‚Äúanchors‚Äù as if-then rules to generate model agnostic-explanations. A perturbation\nis applied by replacing ‚Äúabsent‚Äù tokens with random words having the same POS tag drawn\naccording to an embedding similarity-based probability distribution. Experiments on visual\nquestion answering, and textual and image classification show that the anchors model enhances\nthe precision of explanations with less effort to understand and apply.\nA landmark in the timeline of the ExAI method is achieved when Lundberg and Lee theoret-\nically showed that many interpretability methods and metrics can be unified in one approach\nthat exploits game theoretical concepts in [63]. Inspired by the Shapley value of game theory,\nSHAP values are proposed as a measure of the feature importance for a model‚Äôs prediction.\nSHAP is proved to be a unified measure that different methods such as LIME [88], Deep LIFT\n[99] and layer-wise relevance propagation [6] tried to approximate in the literature. Later, Chen\net al. [16] extend SHAP and proposed the L-Shapley and C-Shapley measure by exploiting the\nunderlying graph structure to reduce the number of model evaluations. In [2], explanations of\nblack-box models are formulated as groups of input-output tokens causally related. Explana-\ntions are generated by querying with perturbed inputs and solving a partitioning problem to\nselect the relevant components. Variational auto-encoders are used to derive meaningful input\nperturbations.\nWe draw the reader‚Äôs attention to the fact that the approaches discussed above [2, 63, 88, 89]\nare not exclusive to textual data. While other similar explainability methods exist [7, 63, 103],\nthey are not validated on NLP tasks, thus beyond the scope of this work.\n8.1.2 Model-specific Explanations. Instead of considering black-box models, K√°d√°r et al. [51]\ninterpret specific recurrent architectures in RNN models by quantifying the contribution of each\ninput to the encoding of a GRU architecture. Their omission score is computed by measuring\nthe salience of each word ùë†ùëñ in a sentence ùë†1:ùëõ by observing how much the representation\nof the sentence when omitting the word ùë†ùëñ would deviate. The results show the sensitivity\nto the information structure of a sentence and selective attention to lexical, semantic, and\ngrammatical relations. Similarly, Arras et al. [5] adapt the Layer-wise Relevance Propagation\n(LPR) method of [6] to explain the predictions of accumulators and gated interactions in the\nLSTM architecture in particular.\n[51] and [5] exploit general ExAI methods, such as perturbation and layer propagation, to\nexplain NLP models. Although such methods do not require human annotation or intervention,\nthey might be computationally expensive. Prior to that, researchers developed methods that\nimitate the thinking process of human beings by relying on human annotations to derive\nexplanations. Those attempts are grouped under the ‚Äúrationalization‚Äù framework that aims at\nexplaining why a specific instance belongs to a category by extracting a ‚Äúrationale‚Äù along with\nthe network annotation. According to [55], a ‚Äúrationale‚Äù can be defined as ‚Äúsub-sets of the\nwords from the input text that satisfy two key properties. First, the selected words represent\nshort and coherent pieces of text (e.g., phrases) and, second, the selected words must alone\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 25\nsuffice for prediction as a substitute of the original text. ‚Äù Rationales are proved to help to learn\ndomain-invariant representations that can induce machine attention [9].\nIn [124], rationale-annotated data is exploited to aid learning by providing the learning\nalgorithm (Support Vector Machines, SVMs, in this case) with hints as to which features of\nthe input the algorithm should attend. Later, Zhang et al. [125], extended the idea to CNNs\nby incorporating rationales for text classification in a Rationale-Augmented CNN (RA-CNN).\nRA-CNN computes a document-level vector representation by summing over its constituent\nsentence vectors weighted by the likelihood that the sentence is a rationale in support of\nthe most likely class. Adversarial learning is utilized in [ 123] to improve the performance\nof rationale extraction so as not to leave any useful information out of the selection. The\noutcome is also incorporated in [ 123] into the selection process to improve the predictive\naccuracy through more comprehensive rationale extraction. Differentiable binary variables\nare introduced in [10] to further improve the rationale extraction by augmenting the objective\nwith re-parameterized gradient estimates. Game-theoretic concepts are incorporated in [14] to\nderive a rationalization criterion that approximates finding causal features without highlighting\nspurious correlations between inputs and outputs.\nA common benchmark to evaluate the rationalization attempts is presented in the ERASER\nframework of [ 26]. Other approaches include, but are not limited to, contrastive textual\nexplanations [94], representation erasure [58] and information theoretical measures [15].\n8.2 Inherently Interpretable Models\nWhile the previous interpretability methods work on pre-trained models, the methods discussed\nin this section develop models that can classify textual data while explaining their particular\ndecisions. In other words, for a textual inputùë•the models discussed next can output a decision\n(class) ùë¶and an explanation ùê∏formulated differently in each reference. Using the terminology\nof this work, we further cluster these methods in model-agnostic and model-specific groups.\n8.2.1 Model-agnostic Explanations. As an extension to the previously discussed ‚Äúrationaliza-\ntion‚Äù attempts [124, 125], Lei et al. [55] propose an automated approach to extract rationales\nfrom the text as subsets of the text words that are coherent and short but lead to the same\npredictions. For this purpose, the authors train a generator and encoder that learn the classi-\nfication as well as the explanation. While the former specifies a distribution over subsets of\nwords as candidate rationales, and the latter processes them for prediction. This is achieved by\n(1) forcing the produced rationale, ùëß (set of words) to be sufficient as a replacement for the\ninput text ùë• in predicting the output ùë¶, i.e. ||enc(ùëß,ùë•)‚àíùë¶||small (2) while maintaining short,\ni.e. small ||ùëß||, and coherent rationales. Coherency is satisfied by encouraging words to form\nmeaningful phrases (consecutive words) rather than sets of isolated words. Hence, the sum\nof |ùëßùë° ‚àíùëßùë°‚àí1|is minimized. Then, doubly stochastic gradient descent is used to minimize the\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n26 ‚Ä¢ Julia El Zini and Mariette Awad\nfollowing objective over training instances of the form (ùë•,ùë¶):\n‚àëÔ∏Å\n(ùë•,ùë¶)\nEùëß‚âàgen(ùë•)\n\u0014\n||enc(ùëß,ùë•)‚àíùë¶||2\n2 +ùúÜ1||ùëß||+ ùúÜ2\n‚àëÔ∏Å\nùë°\n|ùëßùë° ‚àíùëßùë°‚àí1|\n\u0015\n(1)\n8.2.2 Model-specific Explanations. Liu et al. [61] consider local explanations for particular\ninputs but present a generative explanation framework that learns to generate fine-grained\nexplanations inherently, while making classification decisions in a model-agnostic manner. The\nprediction component is composed of a (1) text encoder that takes the input text sequenceùëÜand\nencodes it in a representation vectorùë£ùëí and (2) a category predictorùëÉthat outputs the category\ncorresponding toùë£ùëí along with its probability distribution. The explanation component consists\nof an explanation generator that takes ùë£ùëí and generates fine-grained explanations ùëíùëê, as a set\nof words that explain the model‚Äôs decision. Fine-grained explanations can be thought of as\nattribute-specific rationales. In other words, in sentiment analysis applications, if a product\nhas three attributes: quality, practicality, and price, the review ‚Äùthe product has good quality\nwith a low price‚Äù, can be explained by ‚Äúlow‚Äù as a fine-grained explanation for the price and\n‚Äúhigh‚Äù as a fine-grained explanation for the quality. In [61], those explanations ùëíùëê are provided\nin two datasets collected by the authors of the work.\nTraining consists of minimizing a combination of two-loss factors: the classification loss and\nthe explanation loss. To avoid cases where the generative explanations are independent of the\npredicted overall decision, the authors define an explanation factor that helps build stronger\ncorrelations between the explanations and the predictions. More specifically, a classifier ùê∂\nis trained to predict the category from the explanations and not the original input text. ùê∂ is\nthen used to provide more robust guidance for the text encoder to leverage the generation\nprocess by generating a more informative representation vector ùë£ùëí. Experiments show that the\nexplanation factor enhances the performance of the base predictor model.\nReasoning over Knowledge Graph [97] presents a promising way to explain NLP systems\nin a structured way in the form of < ùë†ùëúùë¢ùëüùëêùëí_ùëíùëõùë°ùëñùë°ùë¶,ùëüùëíùëôùëéùë°ùëñùëúùëõ,ùë°ùëéùëüùëîùëíùë° _ùëíùëõùë°ùëñùë°ùë¶ > [117]. Graph\nreasoning has been addressed as a variational inference problem [17], random-walk search\n[36] and a Markov Decision Processes [ 24, 120]. Recently, knowledge graph reasoning has\nattracted researchers in the NLP and ExAI communities. Readers are referred to [18] where its\nbasic concept, definitions, and methods are surveyed.\n8.3 Discussion\nTo the best of our knowledge, inherently interpretable models for explaining individual de-\ncisions of NLP models are restricted to the work of [ 55, 61]. This can be explained by the\nfact that current language models require large computational costs in terms of time and\ncomputational resources. Modifying the architecture to an explainable one and retraining the\ncurrent high-performance models will be expensive.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 27\nOn the evaluation level, since model-agnostic approaches were not specific to textual data,\ntheir evaluation was general and not specific to NLP cases. For instance, LIME‚Äôs evaluation\nfor textual data was done qualitatively by getting insights on a model trained to differentiate\n‚ÄúChristianity‚Äù from ‚ÄúAtheism‚Äù on a subset of the 20 newsgroup dataset [ 88]. On the other\nhand, SHAP [ 63] is not explicitly tested on NLP tasks and [ 89]‚Äôs anchors were tested on\ntabular datasets. Although a common evaluation framework is not followed with the model-\nspecific approaches, textual data is used to evaluate the described explainability methods. This\nevaluation is inspired by sensitivity analysis where the sentence representation is monitored\nto evaluate how much deviation is observed when the words outputted by the explainability\nmethod are omitted [5, 51]. Regarding the inherently interpretable models, one can clearly see\nthat both methods [55, 61] rely on the encoder-decoder design to generate explanations. [55]\nevaluate their approach on multi-aspect sentiment analysis and compare their explanations to\nmanually annotated test cases. Although [61] have similar motivation, they do not compare\nto [55]. The authors report instead a BLEU score for their generated explanations. They also\nreport the performance of a classifier on the fine-grained explanation instead of the initial\ninput and employ a human evaluation framework.\n9 DISCUSSION AND FUTURE DIRECTIONS\nTo better understand the interest in ExAI in NLP, we consider the titles of the papers that\nwe reference in this work and we visualize word frequencies in Figure 5. One can see that\ndeep networks , representation or embedding methods and attention models are very frequent\nin our referenced work. The importance of visualizations or visual clarification techniques\nin explaining deep models can be also inferred from the frequencies. The figure also hints\nat the fact that ExAI in NLP in the referenced work is mostly focused on understanding the\ninner workings of the underlying models rather than understanding a particular output of\nclassification. Additionally, to reflect the interest in ExAI in general within the machine learning\ncommunity, we show the top 7 conferences and journals referenced in this work in Figure 6.\nThe lack of journals that study or survey ExAI methods in general, and in NLP, in particular,\nis reflected in the figure. Researchers are publishing their ExAI methods and discoveries in\ngeneral AI conferences or conferences that focus on linguistics and natural language.\nFurthermore, Figure 7 shows how the research interest in some ExAI categories is progress-\ning since 2012. For instance, The interpretability of word embeddings has attracted researchers\nsince 2015, when the concern about bias in the machine learning model has emerged after\nGoogle Photos application tagged a black woman as Gorilla [38]. The introduction of transform-\ners in 2017 has also encouraged researchers to study the magic behind their state-of-the-art\nperformance on different NLP tasks. Moreover, the unprecedented breakthroughs in NMT in\n2016 [119] encouraged research work on ExAI applied to NMT models afterward.\nAlthough recent years are witnessing significant growth in ExAI methods applied to NLP\nmodels, these methods are not fulfilling their potential yet. For instance, the evaluation of\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n28 ‚Ä¢ Julia El Zini and Mariette Awad\nFig. 5. Most frequent terms used in the titles of the publications referenced in this work\nFig. 6. Top 7 conferences and journals referenced in this work\nFig. 7. Timeline for ExAI in NLP\nExAI methods is lacking a unique testing framework where metrics and datasets are well-\ndesigned. In the assessment of ExAI‚Äôs current methods, different NLP tasks such as POS\ntagging, word intrusion, and correlation experiments are explored. However, these tasks are\npaper-specific and they are not used to compare different methods. This brings to the front\nthe need for a common evaluation framework where human-labeled datasets are well defined\nand NLP tasks are described within the syntactic or semantic aspect that they aim to explain.\nAdditionally, an area that is yet to be explored by researchers is the quantitative assessment\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 29\nof the interpretability of the embedding spaces and the effectiveness of each dimension in\nsemantically and syntactically encoding a particular concept.\nMoreover, the majority of the work studied in this survey post-process NLP models to explain\ntheir decisions and inner workings. Inherently interpretable NLP models are under-examined\ndue to the fact that retraining a model after modifying its architecture is very expensive and\nmight not achieve the same performance. Exploring explainable designs for NLP models can\nthus be one of the main subjects for future work. Another direction for future work is the\nlevel at which explanations for language models are provided. So far, the explanations are\nprovided as either the contribution of individual words to the decision or the layer/neuron at\nwhich syntax or semantics are encoded. However, text analysis is a multi-step process: after\nextracting information from textual unstructured data, analysis is applied to the extracted\ninformation to reach knowledge. Wisdom and logic come at the highest level of semantics.\nThus, explanations provided on the individual input words will be ignoring the hierarchy of\nthe text understanding process which will affect their efficacy.\n10 CONCLUSION\nThis work presents the first comprehensive survey on explainability methods in the NLP field\nthat combines ExAI methods on the input-, processing- and output levels. According to the\nassumptions made on explained models, we make the distinction between model-agnostic and\nmodel-specific methods. We further distinguish between explanation in a post-hoc manner and\nexplanation that results in inherently interpretable models.\nWe present different attempts to interpret word embedding models that are recently serving\nas inputs to almost every NLP network. Some of those methods rely on altering the embedding\nspace by imposing a sparsity constraint or applying a rotation transformation while others\nintegrate external knowledge bases and ontologies or rely on bidirectional language models to\nderive contextualized embeddings. Moreover, we survey existing work on the interpretation\nof hidden representations of NLP models, general RNNs, and transformers in terms of human-\nunderstandable concepts. We discuss the debate over the interpretability of attention weights\nand we derive the following conclusion: attention weights are relatively more inherently\ninterpretable than traditional models‚Äô parameters but further analysis is required for complete\ntransparency in attention-based models. Additionally, we present the research work that\nexplains a particular model decision inspired by general ExAI methods or designed specifically\nfor NLP models. We also discuss different visualization platforms that present user-friendly\nexplainability schemes based on perturbations and attention weights.\nFigure 8 summarizes the work done on the interpretability of word embeddings, inner\nworkings of RNNs and transformers, the model‚Äôs decision, and the different visualization\nmethods while highlighting the interconnections between the different methods.\nTo date, there is no common evaluation ground for explainability methods on NLP models.\nIn this work, we shed the light on different empirical setups, datasets, and metrics that can be\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n30 ‚Ä¢ Julia El Zini and Mariette Awad\nFig. 8. References for Sections 5, 6, 7 visualized over similarity of scopes.\ndesigned to assess the performance of ExAI in NLP. These setups are aggregated according to\nwhat the corresponding ExAI method is addressing: embeddings, inner workings, or model‚Äôs\ndecisions. We further examine these evaluation methods to discriminate between evaluations\nthat explicitly assess the interpretability and those that are of qualitative nature or need further\nanalysis to extract insights useful from an explainability perspective. Finally, we discuss the\nlimitations of existing ExAI methods while highlighting research areas that can possibly be\nthe focus of researchers in their future exploration.\nREFERENCES\n[1] Carl Allen and Timothy Hospedales. 2019. Analogies Explained: Towards Understanding Word Embeddings.\nIn International Conference on Machine Learning . 223‚Äì231.\n[2] David Alvarez-Melis and Tommi S Jaakkola. 2017. A causal framework for explaining the predictions of\nblack-box sequence-to-sequence models. In EMNLP.\n[3] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Learning to Compose Neural\nNetworks for Question Answering. In Proceedings of the 2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies . 1545‚Äì1554.\n[4] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition . 39‚Äì48.\n[5] Leila Arras, Jos√© Arjona-Medina, Michael Widrich, Gr√©goire Montavon, Michael Gillhofer, Klaus-Robert\nM√ºller, Sepp Hochreiter, and Wojciech Samek. 2019. Explaining and interpreting LSTMs. In Explainable ai:\nInterpreting, explaining and visualizing deep learning . Springer, 211‚Äì238.\n[6] Sebastian Bach, Alexander Binder, Gr√©goire Montavon, Frederick Klauschen, Klaus-Robert M√ºller, and\nWojciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 31\npropagation. PloS one 10, 7 (2015).\n[7] David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, and Klaus-Robert\nM√ºller. 2010. How to explain individual classification decisions. The Journal of Machine Learning Research\n11 (2010), 1803‚Äì1831.\n[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473 (2014).\n[9] Yujia Bao, Shiyu Chang, Mo Yu, and Regina Barzilay. 2018. Deriving Machine Attention from Human\nRationales. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing .\n1903‚Äì1913.\n[10] Joost Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Predictions with Differentiable\nBinary Variables. In 57th Annual Meeting of the Association for Computational Linguistics . ACL Anthology,\n2963‚Äì2977.\n[11] Terra Blevins, Omer Levy, and Luke Zettlemoyer. 2018. Deep RNNs Encode Soft Hierarchical Syntax. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). 14‚Äì19.\n[12] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to\ncomputer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural\ninformation processing systems . 4349‚Äì4357.\n[13] Gino Brunner, Yang Liu, Dami√°n Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer.\n[n.d.]. ON IDENTIFIABILITY IN TRANSFORMERS. ([n. d.]).\n[14] Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. 2020. Invariant rationalization. In International\nConference on Machine Learning . PMLR, 1448‚Äì1458.\n[15] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. 2018. Learning to explain: An information-\ntheoretic perspective on model interpretation. In International Conference on Machine Learning . PMLR,\n883‚Äì892.\n[16] Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. 2018. L-shapley and c-shapley: Efficient\nmodel interpretation for structured data. arXiv preprint arXiv:1808.02610 (2018).\n[17] Wenhu Chen, Wenhan Xiong, Xifeng Yan, and William Yang Wang. 2018. Variational Knowledge Graph\nReasoning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers) . 1823‚Äì1832.\n[18] Xiaojun Chen, Shengbin Jia, and Yang Xiang. 2020. A review: Knowledge reasoning over knowledge graph.\nExpert Systems with Applications 141 (2020), 112948.\n[19] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What Does BERT Look\nat? An Analysis of BERT‚Äôs Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP . 276‚Äì286.\n[20] Charles B Crawford and George A Ferguson. 1970. A general rotation criterion and its use in orthogonal\nrotation. Psychometrika 35, 3 (1970), 321‚Äì332.\n[21] Piotr Dabkowski and Yarin Gal. 2017. Real time image saliency for black box classifiers. In Advances in\nNeural Information Processing Systems . 6967‚Äì6976.\n[22] Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. 2020. A\nSurvey of the State of Explainable AI for Natural Language Processing. arXiv preprint arXiv:2010.00711\n(2020).\n[23] Arun Das and Paul Rad. 2020. Opportunities and challenges in explainable artificial intelligence (xai): A\nsurvey. arXiv preprint arXiv:2006.11371 (2020).\n[24] Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy,\nAlex Smola, and Andrew McCallum. 2018. Go for a Walk and Arrive at the Answer: Reasoning Over Paths\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n32 ‚Ä¢ Julia El Zini and Mariette Awad\nin Knowledge Bases using Reinforcement Learning. InInternational Conference on Learning Representations .\n[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) . 4171‚Äì4186.\n[26] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. In Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics . 4443‚Äì4458.\n[27] Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Visualizing and understanding neural ma-\nchine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) . 1150‚Äì1159.\n[28] Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning.\narXiv preprint arXiv:1702.08608 (2017).\n[29] Filip Karlo Do≈°iloviƒá, Mario Brƒçiƒá, and Nikica Hlupiƒá. 2018. Explainable artificial intelligence: A sur-\nvey. In 2018 41st International convention on information and communication technology, electronics and\nmicroelectronics (MIPRO). IEEE, 0210‚Äì0215.\n[30] Alexey Dosovitskiy and Thomas Brox. 2015. Inverting convolutional networks with convolutional networks.\narXiv preprint arXiv:1506.02753 4 (2015).\n[31] Philipp Dufter and Hinrich Sch√ºtze. 2019. Analytical methods for interpretable ultradense word embeddings.\narXiv preprint arXiv:1904.08654 (2019).\n[32] Kawin Ethayarajh. 2019. Rotate King to get Queen: Word Relationships as Orthogonal Transformations in\nEmbedding Space. In EMNLP-IJCNLP. 3494‚Äì3499.\n[33] Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A Smith. 2015.\nRetrofitting Word Vectors to Semantic Lexicons. InProceedings of the 2015 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies . 1606‚Äì1615.\n[34] Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah A Smith. 2015. Sparse Overcomplete\nWord Vector Representations. In Proceedings of the 53rd Annual Meeting of the ACL and the 7th IJCNLP\n(Volume 1: Long Papers) . 1491‚Äì1500.\n[35] Bent Fuglede and Flemming Topsoe. 2004. Jensen-Shannon divergence and Hilbert space embedding. In\nInternational Symposium onInformation Theory, 2004. ISIT 2004. Proceedings. IEEE, 31.\n[36] Matt Gardner, Partha Talukdar, Jayant Krishnamurthy, and Tom Mitchell. 2014. Incorporating vector space\nsimilarity in random walk inference over knowledge bases. InProceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP) . 397‚Äì406.\n[37] Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining\nexplanations: An overview of interpretability of machine learning. In2018 IEEE 5th International Conference\non data science and advanced analytics (DSAA) . IEEE, 80‚Äì89.\n[38] Loren Grush. 2015 (accessed September 28, 2020). Google engineer apologizes after Photos app tags two black\npeople as gorillas . https://www.theverge.com/2015/7/1/8880363/google-apologizes-photos-app-tags-two-\nblack-people-gorillas\n[39] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi.\n2018. A survey of methods for explaining black box models. ACM computing surveys (CSUR) 51, 5 (2018),\n1‚Äì42.\n[40] Kristina Gulordava, Piotr Bojanowski, √âdouard Grave, Tal Linzen, and Marco Baroni. 2018. Colorless Green\nRecurrent Networks Dream Hierarchically. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers). 1195‚Äì1205.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 33\n[41] Zhijiang Guo, Yan Zhang, and Wei Lu. 2019. Attention Guided Graph Convolutional Networks for Relation\nExtraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\n241‚Äì251.\n[42] Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2019. Neural module networks for\nreasoning over text. arXiv preprint arXiv:1912.04971 (2019).\n[43] John R Hershey, Jonathan Le Roux, and Felix Weninger. 2014. Deep unfolding: Model-based inspiration of\nnovel deep architectures. arXiv preprint arXiv:1409.2574 (2014).\n[44] Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. 2019. exbert: A visual analysis tool to explore\nlearned representations in transformers models. arXiv preprint arXiv:1910.05276 (2019).\n[45] Bo-Jian Hou and Zhi-Hua Zhou. 2020. Learning With Interpretable Structure From Gated RNN. IEEE\nTransactions on Neural Networks and Learning Systems (2020).\n[46] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. 2017. Learning to\nreason: End-to-end module networks for visual question answering. In Proceedings of the IEEE International\nConference on Computer Vision . 804‚Äì813.\n[47] Sarthak Jain and Byron C Wallace. 2019. Attention is not Explanation. InProceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) . 3543‚Äì3556.\n[48] Ganesh Jawahar, Beno√Æt Sagot, and Djam√© Seddah. 2019. What does BERT learn about the structure of\nlanguage?. In ACL 2019-57th Annual Meeting of the Association for Computational Linguistics .\n[49] Kishlay Jha, Yaqing Wang, Guangxu Xun, and Aidong Zhang. 2018. Interpretable Word Embeddings for\nMedical Domain. In 2018 IEEE International Conference on Data Mining (ICDM) . IEEE, 1061‚Äì1066.\n[50] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language\nmodels know? Transactions of the Association for Computational Linguistics 8 (2020), 423‚Äì438.\n[51] √Åkos K√°d√°r, Grzegorz Chrupa≈Ça, and Afra Alishahi. 2017. Representation of linguistic form and function in\nrecurrent neural networks. Computational Linguistics 43, 4 (2017), 761‚Äì780.\n[52] Andrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015. Visualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078 (2015).\n[53] Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp Nearby, Fuzzy Far Away: How\nNeural Language Models Use Context. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) . 284‚Äì294.\n[54] Jaesong Lee, Joong-Hwi Shin, and Jun-Seok Kim. 2017. Interactive visualization and manipulation of\nattention-based neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations . 121‚Äì126.\n[55] Tao Lei, Regina Barzilay, and Tommi S Jaakkola. 2016. Rationalizing Neural Predictions. In EMNLP.\n[56] Piyawat Lertvittayakumjorn and Francesca Toni. 2021. Explanation-Based Human Debugging of NLP\nModels: A Survey. arXiv preprint arXiv:2104.15135 (2021).\n[57] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2016. Visualizing and Understanding Neural Models\nin NLP. In Proceedings of NAACL-HLT . 681‚Äì691.\n[58] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding neural networks through representation\nerasure. arXiv preprint arXiv:1612.08220 (2016).\n[59] Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, and Eric P Xing. 2017. Interpretable\nstructure-evolving LSTM. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition .\n1010‚Äì1019.\n[60] Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-\nsensitive dependencies. Transactions of the Association for Computational Linguistics 4 (2016), 521‚Äì535.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n34 ‚Ä¢ Julia El Zini and Mariette Awad\n[61] Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards Explainable NLP: A Generative Explana-\ntion Framework for Text Classification. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics . 5570‚Äì5581.\n[62] Shusen Liu, Tao Li, Zhimin Li, Vivek Srikumar, Valerio Pascucci, and Peer-Timo Bremer. 2018. Visual\ninterrogation of attention-based models for natural language inference and machine comprehension . Technical\nReport. Lawrence Livermore National Lab.(LLNL), Livermore, CA (United States).\n[63] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Advances\nin neural information processing systems . 4765‚Äì4774.\n[64] Hongyin Luo, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2015. Online learning of interpretable word\nembeddings. In EMNLP. 1687‚Äì1692.\n[65] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine\nlearning research 9, Nov (2008), 2579‚Äì2605.\n[66] Aravindh Mahendran and Andrea Vedaldi. 2015. Understanding deep image representations by inverting\nthem. In Proceedings of the IEEE conference on computer vision and pattern recognition . 5188‚Äì5196.\n[67] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations\nin vector space. arXiv preprint arXiv:1301.3781 (2013).\n[68] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences.Artificial Intelligence\n267 (2019), 1‚Äì38.\n[69] Akash Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, Mitesh M Khapra, Balaji Vasan Srinivasan,\nand Balaraman Ravindran. 2020. Towards Transparent and Explainable Attention Models. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics . 4206‚Äì4216.\n[70] Gr√©goire Montavon, Wojciech Samek, and Klaus-Robert M√ºller. 2018. Methods for interpreting and\nunderstanding deep neural networks. Digital Signal Processing 73 (2018), 1‚Äì15.\n[71] Raymond Mooney. 1996. Comparative Experiments on Disambiguating Word Senses: An Illustration of the\nRole of Bias in Machine Learning. In EMNLP.\n[72] James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, and Jacob Eisenstein. 2018. Explainable\nPrediction of Medical Codes from Clinical Text. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers). 1101‚Äì1111.\n[73] Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012. Learning effective and interpretable semantic\nmodels using non-negative sparse embedding. In Proceedings of COLING 2012 . 1933‚Äì1950.\n[74] Anh Nguyen, Jason Yosinski, and Jeff Clune. 2019. Understanding neural networks via feature visualization:\nA survey. In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning . Springer, 55‚Äì76.\n[75] Abhishek Panigrahi, Harsha Vardhan Simhadri, and Chiranjib Bhattacharyya. 2019. Word2Sense: Sparse In-\nterpretable Word Embeddings. InProceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics. 5692‚Äì5705.\n[76] Sungjoon Park, JinYeong Bak, and Alice Oh. 2017. Rotated word vector representations and their inter-\npretability. In EMNLP. 401‚Äì411.\n[77] Maria Pelevina, Nikolay Arefyev, Chris Biemann, and Alexander Panchenko. 2016. Making Sense of Word\nEmbeddings. In Proceedings of the 1st Workshop on Representation Learning for NLP . 174‚Äì183.\n[78] Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence\nn-ary relation extraction with graph lstms. Transactions of the Association for Computational Linguistics 5\n(2017), 101‚Äì115.\n[79] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language processing\n(EMNLP). 1532‚Äì1543.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 35\n[80] Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018. Dissecting Contextual Word\nEmbeddings: Architecture and Representation. In Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing . 1499‚Äì1509.\n[81] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT . 2227‚Äì2237.\n[82] Fabio Petroni, Tim Rockt√§schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander\nMiller. 2019. Language Models as Knowledge Bases?. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) . 2463‚Äì2473.\n[83] Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C Lipton. 2020. Learning to\nDeceive with Attention-Based Explanations. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics . 4782‚Äì4793.\n[84] Erika Puiutta and Eric Veith. 2020. Explainable Reinforcement Learning: A Survey. arXiv preprint\narXiv:2005.06247 (2020).\n[85] Zhuwei Qin, Fuxun Yu, Chenchen Liu, and Xiang Chen. 2018. How convolutional neural networks see\nthe world‚ÄîA survey of convolutional neural network visualization methods. Mathematical Foundations of\nComputing 1, 2 (2018), 149.\n[86] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI Blog 1, 8 (2019), 9.\n[87] Alessandro Raganato and J√∂rg Tiedemann. 2018. An analysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP . 287‚Äì297.\n[88] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should i trust you?\" Explaining the\npredictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge\ndiscovery and data mining . 1135‚Äì1144.\n[89] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision model-agnostic\nexplanations. In Thirty-Second AAAI Conference on Artificial Intelligence .\n[90] Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How Much Knowledge Can You Pack into the\nParameters of a Language Model?. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) . 5418‚Äì5426.\n[91] Tim Rockt√§schel, Edward Grefenstette, Karl Moritz Hermann, Tom√°≈° Koƒçisk`y, and Phil Blunsom. 2015.\nReasoning about entailment with neural attention. arXiv preprint arXiv:1509.06664 (2015).\n[92] Anna Rogers, Shashwath Hosur Ananthakrishna, and Anna Rumshisky. 2018. What‚Äôs in your embedding,\nand how it predicts task performance. In Proceedings of the 27th International Conference on Computational\nLinguistics. 2690‚Äì2703.\n[93] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for Computational Linguistics 8 (2020), 842‚Äì866.\n[94] Alexis Ross, Ana Marasoviƒá, and Matthew E Peters. 2020. Explaining nlp models via minimal contrastive\nediting (mice). arXiv preprint arXiv:2012.13985 (2020).\n[95] Sascha Rothe and Hinrich Sch√ºtze. 2016. Word embedding calculus in meaningful ultradense subspaces. In\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers). 512‚Äì517.\n[96] Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use\ninterpretable models instead. Nature Machine Intelligence 1, 5 (2019), 206‚Äì215.\n[97] Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for if-then\n, Vol. 1, No. 1, Article . Publication date: October 2022.\n36 ‚Ä¢ Julia El Zini and Mariette Awad\nreasoning. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3027‚Äì3035.\n[98] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and\nDhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In\nProceedings of the IEEE international conference on computer vision . 618‚Äì626.\n[99] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through\npropagating activation differences. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70. JMLR. org, 3145‚Äì3153.\n[100] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian\nSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game of\nGo with deep neural networks and tree search. nature 529, 7587 (2016), 484‚Äì489.\n[101] Hendrik Strobelt, Sebastian Gehrmann, Michael Behrisch, Adam Perer, Hanspeter Pfister, and Alexander M\nRush. 2018. Seq2seq-vis: A visual debugging tool for sequence-to-sequence models. IEEE transactions on\nvisualization and computer graphics 25, 1 (2018), 353‚Äì363.\n[102] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep\nlearning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n[103] Erik Strumbelj and Igor Kononenko. 2010. An efficient explanation of individual classifications using game\ntheory. The Journal of Machine Learning Research 11 (2010), 1‚Äì18.\n[104] Anant Subramanian, Danish Pruthi, Harsh Jhamtani, Taylor Berg-Kirkpatrick, and Eduard Hovy. 2018.\nSpine: Sparse interpretable neural embeddings. In Thirty-Second AAAI Conference on Artificial Intelligence .\n[105] Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, and Xueqi Cheng. 2016. Sparse word embeddings using l1\nregularized online learning. In Proceedings of the Twenty-Fifth IJCAI . AAAI Press, 2915‚Äì2921.\n[106] Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved Semantic Representations\nFrom Tree-Structured Long Short-Term Memory Networks. InProceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) . 1556‚Äì1566.\n[107] Oguzhan Tas and Farzad Kiyani. 2007. A survey automatic text summarization. PressAcademia Procedia 5,\n1 (2007), 205‚Äì213.\n[108] Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics . 4593‚Äì4601.\n[109] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al. 2019. What do you learn from context? probing for\nsentence structure in contextualized word representations. arXiv preprint arXiv:1905.06316 (2019).\n[110] Erico Tjoa and Cuntai Guan. 2019. A survey on explainable artificial intelligence (XAI): towards medical\nXAI. CoRR abs/1907.07374 (2019).\n[111] Ke M Tran, Arianna Bisazza, and Christof Monz. 2018. The Importance of Being Recurrent for Modeling\nHierarchical Structure. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing. 4731‚Äì4736.\n[112] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. 2019. Attention inter-\npretability across nlp tasks. arXiv preprint arXiv:1909.11218 (2019).\n[113] Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan Gomez, Stephan Gouws, Llion Jones,\n≈Åukasz Kaiser, Nal Kalchbrenner, Niki Parmar, et al. 2018. Tensor2Tensor for Neural Machine Translation.\nIn Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1:\nResearch Track). 193‚Äì199.\n[114] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems .\n5998‚Äì6008.\n, Vol. 1, No. 1, Article . Publication date: October 2022.\nOn the Explainability of Natural Language Processing Deep Models ‚Ä¢ 37\n[115] Jesse Vig. 2019. Visualizing Attention in Transformer-Based Language Representation Models. arXiv\npreprint arXiv:1904.02679 (2019).\n[116] Jesse Vig and Yonatan Belinkov. 2019. Analyzing the Structure of Attention in a Transformer Language\nModel. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP . 63‚Äì76.\n[117] Cunxiang Wang, Jinhang Wu, Luxin Liu, and Yue Zhang. 2020. Commonsense Knowledge Graph Reasoning\nby Selection or Generation? Why? arXiv preprint arXiv:2008.05925 (2020).\n[118] Scott Wisdom, Thomas Powers, James Pitton, and Les Atlas. 2016. Interpretable recurrent neural networks\nusing sequential sparse recovery. arXiv preprint arXiv:1611.07252 (2016).\n[119] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al . 2016. Google‚Äôs neural machine translation system:\nBridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 (2016).\n[120] Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. DeepPath: A Reinforcement Learning Method\nfor Knowledge Graph Reasoning. In Proceedings of the 2017 Conference on Empirical Methods in Natural\nLanguage Processing . 564‚Äì573.\n[121] Huijuan Xu and Kate Saenko. 2016. Ask, attend and answer: Exploring question-guided spatial attention\nfor visual question answering. In European Conference on Computer Vision . Springer, 451‚Äì466.\n[122] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016. Stacked attention networks for\nimage question answering. In Proceedings of the IEEE CVPR . 21‚Äì29.\n[123] Mo Yu, Shiyu Chang, Yang Zhang, and Tommi Jaakkola. 2019. Rethinking Cooperative Rationalization:\nIntrospective Extraction and Complement Control. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) . 4094‚Äì4103.\n[124] Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using ‚Äúannotator rationales‚Äù to improve machine\nlearning for text categorization. In Human language technologies 2007: The conference of the North American\nchapter of the association for computational linguistics; proceedings of the main conference . 260‚Äì267.\n[125] Ye Zhang, Iain Marshall, and Byron C Wallace. 2016. Rationale-augmented convolutional neural networks\nfor text classification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.\nConference on Empirical Methods in Natural Language Processing , Vol. 2016. NIH Public Access, 795.\n[126] Yue Zhang and Jie Yang. 2018. Chinese NER Using Lattice LSTM. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers) . 1554‚Äì1564.\n[127] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender\nBias in Contextualized Word Embeddings. arXiv:1904.03310 [cs.CL]\n[128] Jieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-Wei Chang. 2018. Learning Gender-Neutral Word\nEmbeddings. In EMNLP. 4847‚Äì4853.\n[129] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng\nLi, and Maosong Sun. 2020. Graph neural networks: A review of methods and applications. AI Open 1\n(2020), 57‚Äì81.\n, Vol. 1, No. 1, Article . Publication date: October 2022.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8876930475234985
    },
    {
      "name": "Interpretability",
      "score": 0.8805590867996216
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7720245122909546
    },
    {
      "name": "Field (mathematics)",
      "score": 0.6573255658149719
    },
    {
      "name": "Natural language processing",
      "score": 0.6218699216842651
    },
    {
      "name": "Machine translation",
      "score": 0.5392346382141113
    },
    {
      "name": "Deep learning",
      "score": 0.5148066878318787
    },
    {
      "name": "Machine learning",
      "score": 0.4441927373409271
    },
    {
      "name": "Word (group theory)",
      "score": 0.4343221187591553
    },
    {
      "name": "Language model",
      "score": 0.417322039604187
    },
    {
      "name": "Linguistics",
      "score": 0.08084976673126221
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98635879",
      "name": "American University of Beirut",
      "country": "LB"
    }
  ],
  "cited_by": 89
}