{
  "title": "Accuracy of latest large language models in answering multiple choice questions in dentistry: A comparative study",
  "url": "https://openalex.org/W4406960022",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4220250659",
      "name": "Huy Cong Nguyen",
      "affiliations": [
        "Phenikaa University"
      ]
    },
    {
      "id": null,
      "name": "Hai Phong Dang",
      "affiliations": [
        "Phenikaa University"
      ]
    },
    {
      "id": "https://openalex.org/A2100930758",
      "name": "Thuy Linh Nguyen",
      "affiliations": [
        "Phenikaa University"
      ]
    },
    {
      "id": "https://openalex.org/A2064303628",
      "name": "Viet Hoang",
      "affiliations": [
        "Van Lang University"
      ]
    },
    {
      "id": "https://openalex.org/A2103038690",
      "name": "Viet Anh Nguyen",
      "affiliations": [
        "Phenikaa University"
      ]
    },
    {
      "id": "https://openalex.org/A4220250659",
      "name": "Huy Cong Nguyen",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Hai Phong Dang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100930758",
      "name": "Thuy Linh Nguyen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2064303628",
      "name": "Viet Hoang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103038690",
      "name": "Viet Anh Nguyen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4362601804",
    "https://openalex.org/W4392363583",
    "https://openalex.org/W4389559003",
    "https://openalex.org/W4407152919",
    "https://openalex.org/W4389396369",
    "https://openalex.org/W4399567363",
    "https://openalex.org/W4387472364",
    "https://openalex.org/W4392894031",
    "https://openalex.org/W4391027805",
    "https://openalex.org/W4386460012",
    "https://openalex.org/W4399994575",
    "https://openalex.org/W4387393039",
    "https://openalex.org/W4296024375",
    "https://openalex.org/W3130719814",
    "https://openalex.org/W4400520863",
    "https://openalex.org/W4367672983",
    "https://openalex.org/W4401735946",
    "https://openalex.org/W4393069045",
    "https://openalex.org/W4392702964",
    "https://openalex.org/W4403869417",
    "https://openalex.org/W4392746026",
    "https://openalex.org/W4403515983",
    "https://openalex.org/W4401346464",
    "https://openalex.org/W4400525805",
    "https://openalex.org/W2116810060"
  ],
  "abstract": "Objectives This study aims to evaluate the performance of the latest large language models (LLMs) in answering dental multiple choice questions (MCQs), including both text-based and image-based questions. Material and methods A total of 1490 MCQs from two board review books for the United States National Board Dental Examination were selected. This study evaluated six of the latest LLMs as of August 2024, including ChatGPT 4.0 omni (OpenAI), Gemini Advanced 1.5 Pro (Google), Copilot Pro with GPT-4 Turbo (Microsoft), Claude 3.5 Sonnet (Anthropic), Mistral Large 2 (Mistral AI), and Llama 3.1 405b (Meta). Ï‡ 2 tests were performed to determine whether there were significant differences in the percentages of correct answers among LLMs for both the total sample and each discipline (p &lt; 0.05). Results Significant differences were observed in the percentage of accurate answers among the six LLMs across text-based questions, image-based questions, and the total sample (p&lt;0.001). For the total sample, Copilot (85.5%), Claude (84.0%), and ChatGPT (83.8%) demonstrated the highest accuracy, followed by Mistral (78.3%) and Gemini (77.1%), with Llama (72.4%) exhibiting the lowest. Conclusions Newer versions of LLMs demonstrate superior performance in answering dental MCQs compared to earlier versions. Copilot, Claude, and ChatGPT achieved high accuracy on text-based questions and low accuracy on image-based questions. LLMs capable of handling image-based questions demonstrated superior performance compared to LLMs limited to text-based questions. Clinical relevance Dental clinicians and students should prioritize the most up-to-date LLMs when supporting their learning, clinical practice, and research.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4695076644420624
    },
    {
      "name": "Dental research",
      "score": 0.4408060908317566
    },
    {
      "name": "MEDLINE",
      "score": 0.4143398702144623
    },
    {
      "name": "Natural language processing",
      "score": 0.3961893320083618
    },
    {
      "name": "Dentistry",
      "score": 0.3625732660293579
    },
    {
      "name": "Data science",
      "score": 0.35073429346084595
    },
    {
      "name": "Medicine",
      "score": 0.34004098176956177
    },
    {
      "name": "Biology",
      "score": 0.19967761635780334
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210124651",
      "name": "Phenikaa University",
      "country": "VN"
    },
    {
      "id": "https://openalex.org/I4210123993",
      "name": "Van Lang University",
      "country": "VN"
    }
  ]
}