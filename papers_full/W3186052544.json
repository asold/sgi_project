{
  "title": "DIRECT : A Transformer-based Model for Decompiled Identifier Renaming",
  "url": "https://openalex.org/W3186052544",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2908548519",
      "name": "Vikram Nitin",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2753056537",
      "name": "Anthony Saieva",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2166284654",
      "name": "Baishakhi Ray",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2664521734",
      "name": "Gail Kaiser",
      "affiliations": [
        "Columbia University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3126089539",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2889467844",
    "https://openalex.org/W3000168638",
    "https://openalex.org/W2995210680",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2892187814",
    "https://openalex.org/W3080730902",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2795422216",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3105735055",
    "https://openalex.org/W3008568373",
    "https://openalex.org/W3103753836",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W4394659000",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963935794",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2509067210",
    "https://openalex.org/W2915149747",
    "https://openalex.org/W2142772656",
    "https://openalex.org/W2949644922"
  ],
  "abstract": "Decompiling binary executables to high-level code is an important step in reverse engineering scenarios, such as malware analysis and legacy code maintenance. However, the generated high-level code is difficult to understand since the original variable names are lost. In this paper, we leverage transformer models to reconstruct the original variable names from decompiled code. Inherent differences between code and natural language present certain challenges in applying conventional transformer-based architectures to variable name recovery. We propose DIRECT, a novel transformer-based architecture customized specifically for the task at hand. We evaluate our model on a dataset of decompiled functions and find that DIRECT outperforms the previous state-of-the-art model by up to 20%. We also present ablation studies evaluating the impact of each of our modifications. We make the source code of DIRECT available to encourage reproducible research.",
  "full_text": "Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021), pages 48–57\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n48\nDIRECT : A Transformer-based Model for\nDecompiled Variable Name Recovery\nVikram Nitin ∗ Anthony Saieva ∗ Baishakhi Ray Gail Kaiser\nDepartment of Computer Science, Columbia University\nvikram.nitin@columbia.edu, {ant,rayb,kaiser}@cs.columbia.edu\nAbstract\nDecompiling binary executables to high-level\ncode is an important step in reverse engineer-\ning scenarios, such as malware analysis and\nlegacy code maintenance. However, the gen-\nerated high-level code is difﬁcult to under-\nstand since the original variable names are\nlost. In this paper, we leverage transformer\nmodels to reconstruct the original variable\nnames from decompiled code. Inherent dif-\nferences between code and natural language\npresent certain challenges in applying conven-\ntional transformer-based architectures to vari-\nable name recovery. We propose DIRECT,\na novel transformer-based architecture cus-\ntomized speciﬁcally for the task at hand. We\nevaluate our model on a dataset of decompiled\nfunctions and ﬁnd that DIRECT outperforms\nthe previous state-of-the-art model by up to\n20%. We also present ablation studies evalu-\nating the impact of each of our modiﬁcations.\nWe make the source code of DIRECT available\nto encourage reproducible research.\n1 Introduction\nProprietary software often comes in binary form,\nmaking it difﬁcult to comprehend its functionality,\nas many high-level code abstractions (e.g., mean-\ningful variable names, code structures, etc.) are\nlost when source code is compiled to binaries. To\nextract meaningful information from binaries, soft-\nware analysts typically use reverse engineering that\nconverts binary executables into another form that\ncan be more easily comprehended ( ˇDurﬁna et al.,\n2013). Reverse engineering is often applied in bi-\nnary code inspection, legacy software maintenance,\nmalware analysis, and cyber forensics. For exam-\nple, reverse engineering uncovered the rebirth of\nZeUS malware variants during the coronavirus pan-\ndemic of 2020 (Osborne, 2020).\n∗ Equal contribution\nvoid __fastcall add_match(char *a1) {\n// ... var declarations omitted ...\nv1 = ( i n t)(a1 - 1);\nw h i l e ( 1 ) {\nv3 = *(unsigned __int8 *)(v1++ + 1);\nv2 = v3;\ni f ( !v3 ) break ;\nv4 = v2 > 0x7F;\ni f ( v2 != 127 )\nv4 = v2 > 0x1F;\ni f ( !v4 ) {\nfree(a1);\nreturn ;\n}\n}\n// ... some code omitted ...\n}\nFigure 1: Real world Hex-Rays decompilation. Recon-\nstructed source differs signiﬁcantly from original, and\nit is hard to deduce original developers’ intentions.\nTraditionally, the primary reverse engineering\ntools are disassemblers, which extract assembly\ninstructions from a binary executable. However,\nin recent years, decompilers like Ghidra (Ghidra)\nand Hex Rays (Hex-Rays) have become practical\nand popular. They produce a source code-like ap-\nproximation of the binary code as shown in Figure\n1. While these tools can retrieve the approximate\ncode structure, they introduce variable names that\nhave no semantic meaning, drastically reducing\ncode readability and comprehensibility (Katz et al.,\n2018; Hu et al., 2018; Hayati et al., 2018).\nIn recent years, Machine Learning-based mod-\nels have shown promise in recovering lost variable\nnames from decompiled code using a frequency-\nbased model (He et al., 2018) or LSTMs (La-\ncomis et al., 2019). However, variables in source\ncode are not independent of each other and often\nhave hidden long-range dependencies. LSTMs and\nfrequency-based models are not well-suited to cap-\nture such dependencies. Since transformer-based\nmodels can more effectively capture long-range\ndependencies (Vaswani et al., 2017), in this work\nwe explore transformer-based models to recover\nvariable names from decompiled code.\n49\nTransformers are popular in natural language\nprocessing (Vaswani et al., 2017; Devlin et al.,\n2019; Yang et al., 2019). However, code differs\nfrom natural language in many signiﬁcant ways\n(Allamanis et al., 2018; Ding et al., 2020), hence\nvanilla transformer architectures need modiﬁca-\ntions for practical application to the task of variable\nrecovery. Consider the following problems:\nUnknown number of tokens to be predicted:\nTransformers that capture bidirectional context usu-\nally predict a known number of tokens, but to make\nthe vocabulary size manageable, identiﬁers must\nbe split into subtokens. For example, an identiﬁer\nlike my var could be split up as three subtokens -\n“my”, “ ”, and “var”. Each identiﬁer can be com-\nprised of an arbitrary number of subtokens, and the\nmodel needs to access the information contained\nin the entire sequence while predicting the name\nfor an identiﬁer. To deal with this problem, we use\nan encoder-decoder transformer architecture as in\n(Ahmad et al., 2021).\nSyntactic constraints: Unlike natural language,\ncode’s strict syntax requires that a variable assigned\na name at one occurrence in the prediction must be\nthe same at all other occurrences. For example, if\na decompiled identiﬁer name v1 appears on line\n3 and line 100, the predicted name must be the\nsame on both lines. We propose a novel algorithm\nthat uses the joint probability over sequences to\npredict variable name identiﬁers while still obeying\nconstraints imposed by the code syntax.\nToken Non-uniformity: While training a\nmodel for natural language, all tokens are usu-\nally given equal importance (Vaswani et al., 2017).\nHowever, for semantic understanding of code the\nidentiﬁer tokens are more important than those to-\nkens that are built into the language syntax. For ex-\nample, a variable name like “click count” pro-\nvides much more sematic information than a key-\nword like ”while”. We propose a token weighting\nscheme specially crafted for the variable name re-\ncovery problem.\nCode sequences are long: Adaptations of NLP\ntechniques to code often consider functions analo-\ngous to sentences. Traditional transformers limit\nthe maximum sequence size to a few hundred to-\nkens. While this restriction rarely presents a prob-\nlem dealing with sentences, many functions are\nmuch longer. For example, the longest function in\nour benchmark dataset (Section 4.1) is over 4000\ntokens long. To handle longer functions we propose\na mechanism to break long sequences into smaller\npieces and recombine their individual predictions\nwhile still obeying code’s syntactic constraints.\nPutting all these together, we propose DIRECT\n(Decompiled Identiﬁer Renaming Engine using\nContextual Transformers), the ﬁrst transformer-\nbased model built specially for variable recovery\nfrom decompiled binaries. We compare DIRECT\nto DIRE (Lacomis et al., 2019), the state of the art\nin variable name recovery on a benchmark dataset\nand show that DIRECT improves on the baseline\nby 20%. We also evaluate the individual impact of\neach of our speciﬁc adaptations by performing a\nseries of ablation studies. We provide the source\ncode for DIRECT 1 in the hope that it will prove to\nbe a useful tool for other researchers.\n2 Related Work\nVariable Name Recovery: DIRE (Lacomis et al.,\n2019), compared to in the evaluation, performs the\nsame task as DIRECT but uses traditional LSTMs\ncombined with GGNNs. DIRECT uses DIRE’s to-\nkenizer as is, our innovations replace DIRE’s bidi-\nrectional LSTM with our task-speciﬁc transformer\narchitecture. Prior to DIRE, Debin (He et al., 2018)\nrepresented the prior state of the art using decision\ntree-based modeling.\nType Inference: Debin also attempted to re-\ncover type information – which is a different prob-\nlem. Typilus (Allamanis et al., 2020) is a new\nGGNN-based approach for type inference.\nFunction Name Recovery: An orthogonal de-\ncompilation problem is function name recovery.\nFunction names are usually left in executables’\nmetadata, by default, but in malware these sym-\nbols are probably stripped. Recent work by Ar-\ntuso et al. (Artuso et al., 2020) has shown trans-\nformers are highly applicable to this task and the\npre-training/ﬁne-tuning paradigm has a place in\ncode analysis, but they limit their experiments\nto function names. Other work like David et al.\n(David et al., 2020) uses LSTM architectures to\nencode API call sequences as function proﬁles and\nlearned the function names commonly associated\nwith those call sequences.\nTransformers for Filling-in Blanks: Filling in\nblanks in an input sequence necessitates a model\nthat can capture bidirectional context. BERT’s\npre-training objective (Devlin et al., 2019) solves\n1https://github.com/DIRECT-team/\nDIRECT-nlp4prog\n50\n512-Token\nPieces\nv1\t:\t'my_var'\nv2\t:\t'end'\na1\t:\t'count'\nv1\t:\t'n'\nv2\t:\t'end'\na2':\t'len'\nCombine\nFinal\nPredictions\nvoid func ( int v1, char...\n...\nfunc\n... \nvoid ... \n... \nConf : 0.7\nConf : 0.3\nv1\t:\t'my_var'\na1\t:\t'count'\nv2\t:\t'end'\na2\t:\t'len'\n\t\t.....\t\nvoid\nBERT\nEncoder\nfunc\n (\n int\nmy_\n var\n </s>\n,\nlong int a2 = 5;\nv1 ++;\n...\nint\n... \nlong ... \nlong\n int\nBERT Decoder\nmy_\n var\nFigure 2: Our state-of-the-art variable renaming model, DIRECT. DIRECT breaks the function into pieces, passes\neach piece through a BERT encoder and decoder, and combines the predictions of all the pieces. For simplicity,\nwe have omitted the advanced prediction algorithm (Algorithm 1; Figure 3) in this diagram. For more details, refer\nto Section 3.2.\nthis problem by reconstructing random masked to-\nkens. SpanBERT (Joshi et al., 2020) focuses on\ncontiguous spans of masked tokens with a modi-\nﬁed pre-training objective. These methods require\nthe location and length of each blank to be known\nin advance, but Insertion Transformers (Stern and\nUszkoreit, 2019) solve for variable-length blanks\nwithout explicitly controlling insertion.\nBlank Language Models (Shen et al., 2020) solve\nfor ﬁxed blanks with variable length with a special\nblank character that the model can predict and feed\nback in a loop. Another architecture that solves the\nsame problem is BART (Lewis et al., 2020). Sim-\nilar to us, BART uses a BERT encoder and a left-\nright decoder to perform arbitrary transformations\non the input. However both these approaches can-\nnot be directly applied to variable renaming without\nmodiﬁcation to guarantee that multiple blanks have\nthe same prediction.\nDecompilers: There are two decompilers used\nin practice. One is Hex-Rays (Hex-Rays), from\nwhich the training set was built, and the other is the\nopen-source Ghidra platform (Ghidra), which both\nfail to make meaningful efforts at reconstructing\nvariable names without debugging information. A\nresearch compiler DREAM++ (Yakdan et al., 2016)\nfunction signature heuristics to generate meaning-\nful variable names, but does not apply ML models.\nAdapting ML to SE tasks: Recent works like\n(Rahman et al., 2019) and (Ding et al., 2020) have\nalso investigated the difﬁculties of applying ML\nmodels from other disciplines directly to software\nengineering tasks.\n3 Design\nFigure 2 provides an overview of DIRECT. In this\nsection we detail each of the problems we encoun-\ntered and the design decision solutions.\n3.1 Encoding/Decoding\nTransformers are traditionally used to predict entire\nsequences; however in our problem setting most\ntokens are ﬁxed. Therefore we need to adapt trans-\nformers from predicting entire sequences to pre-\ndicting individual tokens based on the ﬁxed tokens.\nWhile making a prediction on an occurrence of\na particular variable, the model should ideally have\naccess to the information contained in the entire\ninput sequence. The naive solution is to use a bidi-\nrectional transformer that with a Masked Language\nModel (MLM) training scheme, such as BERT.\nHowever by design, an MLM is designed to predict\nthe same number of tokens as in the input sequence.\nIn our case, because of subtokenization, the pre-\ndicted subsequences can be of unknown length.\nAdapting an MLM transformer to solve this prob-\nlem is non-trivial.\nThe next option is to use a transformer as a\n51\nsequence-to-sequence language model to predict\nthe immediate next token given all the preceding\ntokens. One could feed the entire sequence until a\nvariable is reached, start generating tokens one at\na time in an autoregressive manner, and stop when\na special end token is predicted. However such a\nmodel cannot use bidirectional context while mak-\ning a prediction, and can only leverage the part of\nthe sequence that precedes each variable.\nWe propose to use an encoder-decoder setup,\nas in (Vaswani et al., 2017). The transformer en-\ncoder embeds each input token, and the sequential\ndecoder attends over these encoder embeddings\nwhile making predictions one token at a time. So\nalthough we give the decoder only the portion of\nthe input sequence that precedes the variable of in-\nterest, it also has access to the entire input sequence\nthrough the encoder embeddings.\nOf course, this still leaves open the question of\nhow to constrain multiple instances of the same\nvariable to have the same prediction. While we\nwill present a better solution to this problem in\nSection 3.2, a good ﬁrst approximation is to simply\nuse the prediction at the ﬁrst occurrence of the\nvariable we are interested in. We hypothesize that\nsince the encoder-decoder model has access to the\nentire sentence while making a prediction for each\noccurrence, one cannot do drastically better than\nthis simple approximation.\n3.2 Advanced Prediction Algorithm\nEffective sequence modeling requires not only mak-\ning predictions, but also predictions that ﬁt the\nproblem setting (Ding et al., 2020). Semantic pre-\nserving identiﬁer renaming mandates that once a\nvariable has been renamed it must have the same\nvalue at each occurrence. This additional constraint\nposes a challenge for vanilla transformers since\nthey predict each token independently in traditional\nlanguage modeling. Exhaustively searching the tar-\nget vocabulary space is computationally intractable,\nso we narrow the search space with a specialized\nprediction algorithm that ﬁts the problem setting.\nAt the variable’s ﬁrst occurrence, we make m\npredictions for its name, each of which leads to a\ndifferent sequence of variable name assignments.\nThroughout our algorithm, we maintain the top k\nsequences only. Thus at the ﬁrst occurrence of a\nvariable, we generate m ×k possible sequences,\nand pick the top k. In practice, we use m = k.\nAt later occurrences of a variable, we update the\nscores of the existing predictions, thus maintaining\nthe list of k sequences. This is where our algorithm\ndiffers from standard beam search. Note that the\npredictions made at the ﬁrst occurrence of a vari-\nable constrain its predictions at further occurrences,\nbut choosing a large k mitigates this problem.\nThis procedure, “Advanced prediction”, is\nshown in Figure 3 for the case when k = 2. Algo-\nrithm 1 describes it in detail. In our experiments,\nwe observed that choosing k = 5was optimal.\nAlgorithm 1 Advanced Prediction\n1: Input : A sequence of decompiler output to-\nkens S, and a model M\n2: Output : S with predicted names\n3: gen ←[[ ]], probs ←[1]\n4: for tok ∈S do\n5: if tok is not a variable then\n6: for seq ∈gen do\n7: seq.append(tok)\n8: continue\n9: if tok has been seen before then\n10: for j ∈1...len(gen) do\n11: n ←current pred of tok in gen[j]\n12: p ←prob assigned to n by M at\nthe current position\n13: gen[j] ←gen[j] +p\n14: probs[j] ←probs[j] ×p\n15: else\n16: for j ∈1...len(gen) do\n17: Using beam search over sub-\ntokens with M, ﬁnd the top k\npossibilities for the name of tok\n18: Let the names be n1, ..., nk and\ntheir probabilities be p1, ..., pk\n19: Replace gen[j] with\n(gen[j] +n1), ...,(gen[j] +nk)\n20: Replace probs[j] with\n(probs[j] ·p1), ...,(probs[j] ·pk)\n21: Sort gen and probs in desc. order of probs\n22: gen ←gen[1 :k]\n23: probs ←probs[1 :k]\n24: return gen[1]\n52\nint v1 ; char v2 ;* ++v1\n Decoder\ncount\nage0.9\n0.1\nint\n ;\n char v2 ;\n* ++v1\nage\nint\n ;\n char v2 ;\n* ++v1\ncount\n Decoder\naddr\nname0.95\n0.05\n Decoder\ndesc\nstr0.8\n0.2\n0.9\n0.1\nint\n ;\n char\n name\n ;\n* ++v1\nage\nint\n ;\n char\n str\n ;\n* ++v1\ncount\n Decoder\nage 0.85\n Decoder\ncount 0.9\n0.9 x 0.95\n0.9 x 0.05\n0.1 x 0.8\n0.1 x 0.2\n0.86\n0.08\n0.86 x 0.85\n0.08 x 0.9\nFigure 3: Advanced Prediction with k = 2. The de-\ncoder takes as input the portion of the sequence that\nprecedes the variable being predicted. Our algorithm\ndiffers from standard beam search in the prediction of\nthe second occurrence ofv1. Rather than generate mul-\ntiple predictions for v1, the algorithm simply updates\nthe scores of the existing predictions in order to obey\nthe syntactic constraints of code.\n3.3 Identiﬁer Token Coefﬁcient\nA typical transformer treats all tokens identically\nwhen computing the loss function during pre-\ntraining and ﬁne-tuning. Code differs from natural\nlanguage in the grammar requires the majority of\nthe tokens. The only opportunity for the program-\nmer to inject semantic meaning into the source\ncode text is through identiﬁers, which makes this\nproblem compelling in the ﬁrst place. The model\nshould therefore treat identiﬁer tokens differently.\nWe implement this concept by training with a\ncustom loss function as shown in Figure 4. Tra-\nditional NLP architectures predict the entire se-\nquence, and then train on a loss function by aver-\naging the error uniformly across all tokens. Our\ncustom weighting scheme places increased signif-\nicance on prediction of identiﬁers, using a mask\nwhich increases the loss 50-fold for identiﬁers as\ncompared to all other tokens. We expect that this\nidentiﬁer token coefﬁcient (ITC) hyper parameter\ncould be tuned in the future for better performance.\nPredicting the identiﬁers and ignoring the rest\nof the characters in the sequence would result in a\nmodel that doesn’t learn the context surrounding\nthe identiﬁer which informs the prediction.\nv1 \n- \n( \n( \nint \n) \na1 \n- \n1 \n; \n) \nT ransformer \nBased \nPrediction \nLOSS \nPredicted \nSequence \nIdentifier \nT oken \nCoefficients \nInput \nSequence \nlen \n- \n( \n( \nint \n) \nmy \n- \n2 \n; \n) \n_ \nrec \nlen \n- \n( \n( \nchar \n) \nmy \n- \n1 \n; \n) \n_ \nvar \nNLL 50 \n1 \n1 \n1 \n1 \n1 \n50 \n1 \n1 \n1 \n1 \n50 \n50 \nActual \nSequence \n+ X \nFigure 4: Identiﬁer Token Coefﬁcient loss function.\nThe Negative Log Likelihood (NLL) loss is computed\nfor each token, and a weighted sum taken to compute\nthe loss.\n3.4 Splitting and Merging Mechanism\nAnother inherent difference between code and nat-\nural language when considering sequence to se-\nquence modeling is the length of the sequence. Dis-\ncussion of natural language modeling overlooks\nthis aspect since sentences rarely exceed 200 to-\nkens. In code however functions are signiﬁcantly\nlonger so the ML models must support sequences\nof arbitrary length. In fact our benchmark dataset\ncontains a small number of sequences with length\ngreater than 2000. With respect to identiﬁer re-\ncovery, longer sequences mean more variables to\nrecover, multiple usages per variable, and more\nopportunity for errors. This poses a problem for\ntransformers as traditional transformer based archi-\ntectures, like BERT, require a maximum sequence\nlength set in advance. Furthermore since attention\nmust be trained across all tokens, the memory us-\nage increases quadratically with sequence length.\nIn order to use our model for arbitrary sequence\nlengths, we developed a novel splitting and joint\nprediction mechanism. As described in Figure 2\nwe divide the sequence into multiple chunks of\n512 tokens upon which the model predicts. A sin-\ngle variable can have a different prediction in each\nchunk we combine these predictions using the pre-\ndiction at the ﬁrst chunk in which a variable occurs.\nWe also tried using the chunk with the highest\nconﬁdence, but we found that this did not perform\nas well. We suspect this is because the probabili-\nties are less than one, and multiplications with each\nsuccessive variable only decrease the probability of\nthe entire sequence. Hence smaller pieces with per-\nhaps just one or two occurrences of a variable will\n53\nbe more conﬁdent in their predictions despite hav-\ning less information. One could impose a penalty\nfor pieces with fewer variables, but we defer this\nanalysis to future work.\nOther transformer variants can handle sequences\narbitrary lengths like XLNet (Yang et al., 2019),\nand we expect these advanced models will handle\nthis issue as well as present new challenges. We\nagain leave these endeavors to future work.\n3.5 DIRECT\nUsing the techniques from the previous sections,\nwe put it all together to get DIRECT, a state-of-\nthe-art variable renaming system. Given an input\nsequence, DIRECT splits it into pieces of length\nat most 512 each (default BERT architecture), and\nputs each piece through a BERT encoder and a\nBERT decoder with advanced prediction (Algo-\nrithm 1). Different predictions across pieces for the\nsame variable are combined by taking the predic-\ntion of the ﬁrst piece in the sequence that contains\nthe variable. Figure 2 depicts the entire model.\n4 Experimental Setup\n4.1 Data\nWe use the dataset provided by DIRE (Lacomis\net al., 2019). It was generated using C binaries\nfrom Github, which were then decompiled using\nIda’s Hex-Rays decompilation plugin (Hex-Rays).\nThe training data set consists of 1,011,049 func-\ntions, with a median of 16 variables per function,\na median of 4 unique variables per function, and\na median sequence length of 150 subtokens. We\nfollow DIRE and use Sentencepiece (Kudo and\nRichardson, 2018) to split the functions into subto-\nkens.\nWe use only the “Body-not-in-train” subset for\nthe validation and test data. They consist of 23662\nand 24862 examples, respectively.\n4.2 Metrics\nWe deﬁne accuracy as an exact match between the\noriginal variable name as determined by the debug\ninformation mapping, and the name predicted by\nDIRECT. We also examine the edit distance be-\ntween predicted names and true names, and use the\nedit distance per number of characters (the charac-\nter error rate) as our metric as in DIRE (Lacomis\net al., 2019) to capture success of partial matches.\nWe also measure the Jaccard similarity which is\nthe ratio of the number of overlapping n-grams\nbetween two sequences to the total number of n-\ngrams contained in them. We use n=1, so that each\nword is treated as a set of its constituent charac-\nters. There are some instances when decompiler\nvariables have no corresponding true name. These\nare ignored from all metrics.\n4.3 Pre-training Procedure\nWe pre-train one BERT model using the standard\nMLM task on source sequences directly from the\ndecompiler output (with the dummy variable names\nfrom the decompiler). We call this the BERT en-\ncoder. Similarly we pre-train another BERT model\nusing MLM on target sentences (with the true vari-\nable names), and call this the BERT decoder. Both\nmodels used 4 attention heads, 6 hidden layers,\nand a hidden embedding size of 256. We trained\nthe encoder and decoder for 220k and 140k steps,\nrespectively, using a batch size of 128 sequences.\nWhile masking tokens, we do not differentiate be-\ntween variable and non-variable tokens since we\nwant the model to learn the complete structure of\nthe code sequences. We also used the standard op-\ntimization techniques employed by BERT (Devlin\net al., 2019), wherein an Adam optimizer is used\nwith a variable learning rate. The learning rate in-\ncreases linearly from 0 to 10−4 over the ”warm-up”\nperiod of 40k iterations, and then decreases linearly\nfrom 10−4 to 0 at the end of pre-training.\n4.4 Fine-tuning Procedure\nAfter reviewing our proof of concept experiments\nwe trained our best conﬁguration for 85 epochs to\nproduce the DIRECT prototype. We follow the\nsame convention as DIRE (Lacomis et al., 2019),\nwhereby the number of sequences per batch is vari-\nable, but the total number of tokens in the batch\nis ﬁxed to deﬁne the size of the batch. We used\na batch size of 4096 tokens per batch. We used a\nlearning rate of 1e-4 for the ﬁrst 10 epochs, 0.3e-4\nfor the next 10, and 1e-5 thereafter.\n5 Results\n5.1 DIRECT Evaluation\nIn order to evaluate the effectiveness of DIRECT,\nwe compare its performance against that of DIRE\non our test dataset. The results are shown in Table\n1. We observe that DIRECT achieves an increase\nof 7.1 percentage points in accuracy over DIRE,\nwhich is a relative increase of 19.9%. We obtained\nall DIRE results by re-running the authors’ code on\n54\nModel Accuracy (%) ↑ Top-5 Accuracy (%) ↑ CER ↓ Jaccard Dist ↓\nDIRE 35.8 41.5 .664 .537\nDIRECT 42.8 49.3 .663 .501\nImprovement 20% 19% .2% 6.5%\nTable 1: Test Accuracy, Top-5 Accuracy (computed by taking the top 5 predictions for each sequence and using\nthe predictions of variables contained in these sequences), Character Error Rate and Jaccard distance of DIRE vs\nDIRECT. DIRECT outperforms DIRE on all four metrics. DIRE results are reproduced by re-running the authors’\ncode on our dataset.\nFigure 5: A visualization of the attention weights of\nthe trained decoder while predicting variables. Darker\nrepresents larger weights. The variable subtokens that\nare being predicted are boxed . For more details, refer\nto Section 5.\nthe dataset, rather than simply using the numbers\nfrom their paper.\n5.2 Qualitative Analysis\nWe also perform some qualitative inspection of\nthe attention weights of the trained model to un-\nderstand what information it is using to make its\ninferences. An example of this is shown in Figure\n5 where the predicted identiﬁer is outlined in black.\nThe attentions shown are the weights used while\npredicting a name for the variable shown in a box,\naveraged over all attention heads at the last layer\nof the decoder.\nWe observe that when making a prediction on\nthe ﬁrst occurrence of a variable, the decoder model\npays attention mainly to the function header, more\nspeciﬁcally the return type and function name.\nHowever for later occurrences of the same vari-\nable, although it does look at the function header\n0-50 51-100101-150151-200201-300301-400401-500501-750751-10001000+\nLength of Sequence\n0\n10\n20\n30\n40\n50\n60Prediction Accuracy (%)\nDIRECT Accuracy\nDIRE Accuracy\n% of Corpus\nFigure 6: Variation of Accuracy of DIRECT and DIRE\nwith length. The spike in DIRE’s performance for the\nlast two categories with very few examples is likely to\nbe an anomaly and not representative of its true perfor-\nmance on sequences of those lengths. Note that this is\non the validation set.\nto some extent, it relies chieﬂy on its predictions\nfor earlier instances of the same variable.\n5.3 Performance on Long Sequences\nThe graph in Figure 6 shows the accuracy of DI-\nRECT on sequences of various lengths. As we\ncross the 500 token mark, and the splitting tech-\nnique takes over, there is a steep drop in accuracy.\nThis problem is mirrored in DIRE’s accuracy al-\nthough not quite as steeply. Still for sequences of\nlength less than 512 tokens DIRECT has a improve-\nment of 10 percentage points over DIRE (48.9%\nvs. 38.8%). DIRE has high accuracy in the longest\ntwo sets of sequences, but this is likely an anomaly\ncaused by insufﬁcient samples sizes.\nOther transformer based variants address this\nsequence issue such as XLNet (Yang et al., 2019),\nand we expect these advanced models will handle\nthis issue as well as present new challenges. We\nagain leave these endeavors to future work.\n55\nModel Accuracy (%) ↑ CER ↓\nUniform token weighting 30.0 .80\nWeighting identiﬁers only 33.7 .76\nITC weighting scheme 34.4 .75\nTable 2: Validation accuracy and Character Error Rate for various token weighting schemes. Prediction is done\nusing the “ﬁrst prediction” strategy. All the models are trained for 15 epochs. Refer to Section 3.3 for more details.\nModel Accuracy (%) ↑ CER ↓\nFirst pred 34.4 .75\nAdvanced pred 34.6 .75\nTable 3: Validation accuracy and Character Error Rate\nfor advanced prediction versus ﬁrst prediction. Both\nmodels are trained for 15 epochs. Refer to Section 3.2\nfor more details.\nModel Accuracy (%) CER\nDecoder Only 19.6 .97\nEncoder-Decoder 34.4 .75\nTable 4: Validation accuracy and Character Error Rate\nfor our encoder-decoder model versus a decoder-only\nmodel. Both models are trained for 15 epochs. Refer\nto Section 3.1 for more details.\n5.4 Ablation Studies\nIn this section, we evaluate the impact of each of\nour design choices. We train all the models for 15\nepochs and evaluate them on the validation set.\n5.4.1 Encoder-Decoder Architecture\nTable 4 shows the performance of our encoder-\ndecoder model vs a decoder-only model (a single\ntransformer, operating as an autoregressive lan-\nguage model) using the prediction at the ﬁrst occur-\nrence of each variable. As we can see, the decoder-\nonly model does signiﬁcantly worse, which is ex-\npected since it has access only to a part of the func-\ntion while making a prediction at the ﬁrst instance\nof a variable.\n5.4.2 Advanced Prediction Algorithm\nTable 3 compares the results of advanced prediction\nwith “ﬁrst prediction”, i.e., taking the prediction at\nthe ﬁrst occurrence of a variable. We observe that\nadvanced prediction improves the performance of\nour encoder-decoder model by a small amount.\nThis could be explained by our observation in\nSection 5.2 that the model seems to rely its earlier\npredictions while predicting the name of a particu-\nlar variable. Later predictions of a variable refer to\nthe value assigned at the ﬁrst prediction, and so the\nprediction of a variable seldom changes from what\nwas predicted at the ﬁrst instance.\n5.4.3 Identiﬁer Token Coefﬁcient\nWe compare the performance of three different\ntoken weighting schemes in the loss function -\nweighting all tokens uniformly, weighting accord-\ning to ITC (as described in Section 3.3), and weight-\ning the identiﬁers only while ignoring the rest of\nthe tokens.\nAs seen in Table 2, ITC shows a 4.4% increase in\naccuracy relative to the uniform weighting scheme,\nwithout hyperparameter tuning of the coefﬁcient.\nAs expected the model that ignores the surrounding\ntokens in the loss function performs worse. This\nis because the model doesn’t effectively learn the\ncontext surrounding the identiﬁers, resulting in a\ndecrease in accuracy by 0.7 percentage points.\n6 Conclusion and Future Work\nThe problem of variable name reconstruction poses\ncertain challenges for traditional transformer-based\nmodels. Speciﬁcally, the variable length of the\nprediction target, the constraints imposed by code\nsyntax, architecture limitations that make long\nsequences difﬁcult, and the task speciﬁc non-\nuniformity of token signiﬁcance. In this work, we\ndeveloped a series of solutions to address these is-\nsues, namely 1) an encoding/decoding scheme to\nhandle arbitrary sub-token length prediction, 2) a\nspecialized prediction algorithm, 3) a customized\nidentiﬁer token coefﬁcient weighting scheme, and\n4) a splitting and combining algorithm for stan-\ndard transformers to handle sequences of arbitrary\nlength. In addition to empirical studies evaluating\nthe effectiveness of each of these techniques, we\nalso combined them to create DIRECT, a practical\nopen-sourced identiﬁer renaming engine. We eval-\nuated DIRECT using a standard benchmark dataset\nagainst the state of the art, DIRE (Lacomis et al.,\n2019), and found that DIRECT provides a 20%\nimprovement. We hope that in addition to an open-\nsourced tool, this work functions as a roadmap for\n56\nother researchers trying to solve the types of prob-\nlems we encountered when adapting transformer-\nbased models to code analysis tasks. Future work\ncould leverage the Abstract Syntax Tree (AST) of\neach function, and employ new transformer archi-\ntectures like XLNet (Yang et al., 2019) to avoid\nsplitting up the input while handling longer se-\nquences. Our approach might also improve the\nresults of other code analysis tasks like type infer-\nence, function re-naming, docstring prediction, and\nfunction boundary identiﬁcation.\n7 Acknowledgements\nThis work was supported in part by DARPA\nN6600121C4018, NSF CCF-1815494, NSF CNS-\n1563555, NSF CCF-1845893, NSF CCF-1822965,\nNSF CNS-1842456. We thank the anonymous re-\nviewers for their helpful feedback. We would also\nlike to thank Suman Jana for his generous provision\nof computing resources.\nReferences\nWasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Uniﬁed pre-training\nfor program understanding and generation. arXiv\npreprint arXiv:2103.06333.\nMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu,\nand Charles Sutton. 2018. A survey of machine\nlearning for big code and naturalness. ACM Com-\nputing Surveys (CSUR), 51(4):1–37.\nMiltiadis Allamanis, Earl T Barr, Soline Ducousso,\nand Zheng Gao. 2020. Typilus: Neural Type Hints.\nIn 41st ACM SIGPLAN Conference on Program-\nming Language Design and Implementation (PLDI),\npages 91–105.\nFiorella Artuso, Giuseppe Antonio Di Luna, Luca\nMassarelli, and Leonardo Querzoni. 2020. In\nNomine Function: Naming Functions in Stripped\nBinaries with Neural Networks. arXiv preprint\narXiv:1912.07946.\nYaniv David, Uri Alon, and Eran Yahav. 2020. Neu-\nral Reverse Engineering of Stripped Binaries. arXiv\npreprint arXiv:1902.09122.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In 17th Annual Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies (NAACL-HLT), pages 4171–4186.\nYangruibo Ding, Baishakhi Ray, Premkumar Devanbu,\nand Vincent J Hellendoorn. 2020. Patching as trans-\nlation: the data and the metaphor. In 2020 35th\nIEEE/ACM International Conference on Automated\nSoftware Engineering (ASE), pages 275–286. IEEE.\nGhidra. 2021. GHIDRA, A software reverse engineer-\ning (SRE) suite of tools developed by NSA’s Re-\nsearch Directorate in support of the Cybersecurity\nmission. https://ghidra-sre.org/.\nShirley Anugrah Hayati, Raphael Olivier, Pravalika Av-\nvaru, Pengcheng Yin, Anthony Tomasic, and Gra-\nham Neubig. 2018. Retrieval-based neural code gen-\neration. Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing.\nJingxuan He, Pesho Ivanov, Petar Tsankov, Veselin\nRaychev, and Martin Vechev. 2018. Debin: Pre-\ndicting Debug Information in Stripped Binaries. In\nACM SIGSAC Conference on Computer and Com-\nmunications Security, pages 1667–1680.\nHex-Rays. 2021. Hex-Rays Decompiler. https://\nhex-rays.com/decompiler/.\nXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin.\n2018. Deep code comment generation. In 2018\nIEEE/ACM 26th International Conference on Pro-\ngram Comprehension (ICPC) , pages 200–20010.\nIEEE.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanbert: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nDeborah S Katz, Jason Ruchti, and Eric Schulte. 2018.\nUsing recurrent neural networks for decompilation.\nIn 2018 IEEE 25th International Conference on\nSoftware Analysis, Evolution and Reengineering\n(SANER), pages 346–356. IEEE.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nJeremy Lacomis, Pengcheng Yin, Edward J. Schwartz,\nMiltiadis Allamanis, Claire Le Goues, Graham Neu-\nbig, and Bogdan Vasilescu. 2019. DIRE: A Neural\nApproach to Decompiled Identiﬁer Naming. In 34th\nIEEE/ACM International Conference on Automated\nSoftware Engineering (ASE), pages 628—-639.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nCharlie Osborne. 2020. Zeus sphinx revamped as\ncoronavirus relief payment attack wave continues.\nhttps://tinyurl.com/ka2t6k2r.\n57\nMd Masudur Rahman, Saikat Chakraborty, Gail Kaiser,\nand Baishakhi Ray. 2019. Toward Optimal Selection\nof Information Retrieval Models for Software Engi-\nneering Tasks. In 19th International Working Con-\nference on Source Code Analysis and Manipulation\n(SCAM), pages 127–138.\nTianxiao Shen, Victor Quach, Regina Barzilay, and\nTommi Jaakkola. 2020. Blank Language Models.\narXiv preprint arXiv:2002.03079.\nKiros Stern, Chan and Uszkoreit. 2019. Insertion\ntransformer: Flexible sequence generation via inser-\ntion operations. https://arxiv.org/abs/1902.\n03249.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In 31st Conference on Neural Informa-\ntion Processing Systems (NIPS).\nKhaled Yakdan, Sergej Dechand, Elmar Gerhards-\nPadilla, and Matthew Smith. 2016. Helping Johnny\nto Analyze Malware: A Usability-Optimized De-\ncompiler and Malware Analysis User Study. In\nIEEE Symposium on Security and Privacy (S&P) ,\npages 158–177.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In 33rd Conference on\nNeural Information Processing Systems (NIPS).\nLuk´as ˇDurﬁna, Jakub Kˇroustek, and Petr Zemek. 2013.\nPsybot malware: A step-by-step decompilation case\nstudy. In 2013 20th Working Conference on Reverse\nEngineering (WCRE), pages 449–456.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8007194995880127
    },
    {
      "name": "Executable",
      "score": 0.657859206199646
    },
    {
      "name": "Identifier",
      "score": 0.5303633213043213
    },
    {
      "name": "Source code",
      "score": 0.5150715708732605
    },
    {
      "name": "Transformer",
      "score": 0.5075815916061401
    },
    {
      "name": "Source lines of code",
      "score": 0.5047179460525513
    },
    {
      "name": "Programming language",
      "score": 0.4795765280723572
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.42607322335243225
    },
    {
      "name": "KPI-driven code analysis",
      "score": 0.4252914488315582
    },
    {
      "name": "Static program analysis",
      "score": 0.27337658405303955
    },
    {
      "name": "Artificial intelligence",
      "score": 0.24960368871688843
    },
    {
      "name": "Software",
      "score": 0.15522560477256775
    },
    {
      "name": "Engineering",
      "score": 0.12686893343925476
    },
    {
      "name": "Software development",
      "score": 0.11545655131340027
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    }
  ]
}