{
  "title": "Retrieval-Augmented Generation: Methods, Applications and Challenges",
  "url": "https://openalex.org/W4409778133",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2101998124",
      "name": "Yicheng Liu",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W7065023868",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2558203065",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2287889828"
  ],
  "abstract": "The Retrieval-Augmented Generation (RAG) has been proven to have a promising approach. It can address the limitations of purely generative models in knowledge-intensive tasks caused by their reliance on static, pre-trained knowledge. RAG addresses these challenges by integrating a retrieval mechanism with a generative model, enabling dynamic access to external knowledge sources during the generation process. This paper presents a comprehensive study of the RAG framework, focusing on its architecture, training strategies, and applications. The framework combines a dense passage retriever (DPR) with a sequence-to-sequence generator (GPT-3.5-turbo), jointly optimized in an end-to-end manner to retrieve and utilize relevant knowledge effectively. This paper evaluates RAG on MS MARCO, demonstrating its superiority over state-of-the-art purely generative models and traditional retrieval-based systems. Experimental results show that RAG achieves significant improvements in factual accuracy, relevance, and interpretability, as measured by metrics such as term frequencyinverse document frequency, bidirectional encoder representation from transformer Score, and Q-Bilingual Evaluation Understudy-1.",
  "full_text": " \n \nRetrieval-Augmented Generation: Methods, Applications and \nChallenges \nYicheng Liu \nFaculty of Science, National University of Singapore, Singapore \ne0952418@u.nus.edu \nAbstract: The Retrieval-Augmented Generation (RAG) has been proven to have a promising \napproach. It can address the limitations of purely generative models in knowledge -intensive \ntasks caused by their reliance on static, pre -trained knowledge. RAG addresses these \nchallenges by integrating a retrieval mechanism with a generative model, enabling dynamic \naccess to external knowledge sources during the generation process.  This paper presents a \ncomprehensive study of the RAG framework, focusing on its architecture, training strategies, \nand applications. The framework combines a dense passage retriever (DPR) with a sequence-\nto-sequence generator (GPT-3.5-turbo), jointly optimized in an end-to-end manner to retrieve \nand utilize relevant knowledge effectively. This paper eva luates RAG on MS MARCO, \ndemonstrating its superiority over state -of-the-art purely generative models and traditional \nretrieval-based systems. Experimental results show that RAG achieves significant \nimprovements in factual accuracy, relevance, and interpret ability, as measured by metrics \nsuch as term frequency –inverse document frequency, bidirectional encoder representation \nfrom transformer Score, and Q-Bilingual Evaluation Understudy-1. \nKeywords: Retrieval-Augmented Generation, Dense Passage Retrieval, Large-scale language \nModel, Term Frequency -Inverse Document Frequency , Bidirectional  Encoder \nRepresentations from Transformers \n1. Introduction \nPre-trained large-scale language models (LLMs) such as ChatGPT, Gemini, and DeepSeek have been \nproven to have great abilities to acquire extensive and profound knowledge from the dataset. They \ncan function as a parameterized implicit knowledge base without the need for external memory, \nrelying on the vast amount of pre-trained knowledge stored within their parameters [1]. Despite these \nadvantages, LLMs may produce a phenomenon called “hallucinations”, especially for time-sensitive \nqueries due to their relia nce on static knowledge bases. Furthermore, the LLMs are a black box, \nmaking it harder to verify the reliability of the outputs [2]. \nTo address these problems, retrieval -augmented generation (RAG) has emerged as a promising \nparadigm that combines the advantages of retrieval -based methods and generative models. RAG \nintegrates a retrieval mechanism, dynamically extract ing relevant information from external \nknowledge sources. By leveraging external knowledge, RAG not only enhances the factual accuracy \nand timeliness of generated text but also improves interpretability by providing explicit references to \nthe retrieved documents. \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n© 2025 The Authors. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0 (https://creativecommons.org/licenses/by/4.0/). \n99 \n \n \nDespite its potential, the development and deployment of RAG systems face several challenges. \nFirst, the retrieval process must be both efficient and accurate, ensuring that the most relevant \ndocuments are selected from potentially massive knowledge corpor a. Second, the generative model \nmust effectively integrate retrieved information into its responses, balancing the need for factual \ncorrectness with the ability to generate fluent and contextually appropriate text. Finally, the end -to-\nend training of RAG s ystems requires careful optimization to ensure that both the retriever and \ngenerator components work collaboratively. \nThis paper tests a RAG framework that dynamically integrates external knowledge retrieval with \ntext generation. The approach leverages a dense passage retriever and a sequence -to-sequence \ngenerator, jointly optimized in an end -to-end manner to enhance the accuracy and releva nce of \ngenerated responses. Through extensive experiments on benchmarks such as MS MARCO, this paper \ndemonstrates the effectiveness of RAG in improving factual correctness and interpretability for \nknowledge-intensive tasks. \n2. Related Work \nThe development of RAG builds upon advancements in two key areas: (1) generative models that \nproduce fluent and contextually relevant text, and (2) retrieval -based systems that extract precise \ninformation from external knowledge sources. Below, this paper reviews the relevant literature in \nthese domains and highlights the evolution of hybrid approaches that combine retrieval and \ngeneration. \n2.1. Generative Models \nGenerative models have achieved remarkable success in natural language processing (NLP) tasks, \nsuch as text generation, summarization, and dialogue systems. However, they face limitations in \nhandling knowledge-intensive tasks due to their reliance on stati c, pre-trained knowledge and their \ntendency to generate factually incorrect or outdated information, a phenomenon known as \n\"hallucination.\" \nTo address these limitations, sequence-to-sequence models like T5 and BART have been proposed. \nWhile they exhibit improved flexibility and performance, they still struggle with dynamically \nincorporating external knowledge, especially for queries requiring up-to-date or domain -specific \ninformation. \n2.2. Retrieval-Based Systems \nRetrieval-based systems aim to provide accurate and interpretable answers by extracting relevant \ninformation from external knowledge sources. Traditional approaches, such as BM25 and TF -IDF, \nrely on term-based matching to retrieve documents or passages. Wh ile effective for simple queries, \nthese methods often fail to capture semantic relationships between the query and the retrieved content. \nMore recent approaches, such as Dense Passage Retrieval (DPR), leverage dense vector \nrepresentations to improve retrieval accuracy. This approach has demonstrated significant \nimprovements in tasks like open-domain question answering (QA). However, retrieval-based systems \nare inherently limited by their inability to generate novel or synthesized responses, as they can only \nreturn existing content from the knowledge corpus. \n2.3. Hybrid Approaches: Combining Retrieval and Generation \nThe limitations of purely generative and retrieval-based systems have motivated the development of \nhybrid approaches that combine the strengths of both paradigms. Early attempts, such as REALM \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n100 \n \n \nand ORQA, introduced retrieval-augmented pre-training to jointly optimize retrieval and generation \ntasks [3-5]. These models demonstrated the potential of integrating external knowledge into \ngenerative models but were limited by their two -stage training pipelines and computational \ninefficiency. \nThis paper’s work builds on these advancements by proposing RAG, a framework that seamlessly \nintegrates retrieval and generation in an end-to-end manner. Unlike previous approaches, RAG jointly \noptimizes the retriever and generator components, allowing the model to dyn amically retrieve and \nutilize relevant knowledge while maintaining the flexibility and fluency of generative models. \n3. Methods \nIn this section, t his paper presents the architecture and implementation details of RAG framework, \nwhich integrates a retrieval mechanism with a generative model to dynamically access external \nknowledge and generate contextually appropriate responses. The evaluation focuses on three  key \naspects: \nThis paper first assesses the performance of RAG in comparison to purely generative models and \ntraditional retrieval-based systems, aiming to highlight the advantages of combining retrieval and \ngeneration capabilities. Then this paper tests the effectivene ss of RAG in handling knowledge -\nintensive tasks, such as open-domain question answering and dialogue generation, to demonstrate its \nability to leverage external knowledge for improved accuracy and relevance. Finally, this paper \nanalyzes the key factors that contribute to RAG's performance, including the interaction between the \nretriever and generator components, to provide insights into the framework's strengths and potential \nareas for improvement. In this paper, the RAG-Sequence as the RAG method. \n3.1. Models \nThe framework consists of two main components, a retriever that retrieves the most similar paragraph \nfrom an external knowledge base and a generator that combines the retrieved paragraph with the \nquestion query and generates a fluent response. \nFor each question query, the model uses the corresponding paragraph to generate the complete \nsequence. For a given input query q , the retriever retrieves a set of relevant passages P =\n{p1, p2, ⋯ , pk} from a large -scale knowledge corpus. Then the generator takes the query q and the \nretrieved passage P as input, and generates an output response r. \nThe retriever and generator are jointly optimized in an end-to-end manner, allowing the model to \nlearn how to effectively retrieve and utilize external knowledge. \n3.2. Retriever: Dense Passage Retrieval (DPR) \nThe retriever identifies and retrieves the most relevant document from a knowledge corpus. T his \npaper employs a DPR based on a dual-encoder architecture [6]: \nFor query encoder, this paper encodes the input query q into a dense vector q, and uses BERT as \nthe transformer-based encoder. For passage encoder, this paper encodes each passage pi into a dense \nvector pi using the same separate transformer -based encoder as the query encoder. For similarity \nscoring, this paper computes the similarity of a passage pi between the query q as the dot product of \ntheir embeddings: \n score(q, pi)  =  q⊺pi (1) \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n101 \n \n \n3.3. Generator: GPT-3.5-Turbo \nThe generator is responsible for synthesizing the retrieved documents into a coherent response. This \npaper applies GPT-3.5-turbo, a sequence-to-sequence (Seq2Seq) architecture based on a pre-trained \nlanguage model [7]. This paper concatenates the question query q and retrieved passage , together. \nGPT-3.5-turbo was pre -trained with a denoising objective and a variety of noise functions. It has \nachieved state-of-the-art results across a range of generation tasks and outperforms models of similar \nsize. \n4. Experiments \nTo evaluate the performance of RAG, t his paper  experiments with a wide range of knowledge -\nintensive questions. For the experiments, this paper uses the related websites from the dataset. Each \nwebsite is split into disjoint 100 -word chunks. During training, t his paper retrieves the most similar \nchunk for each query. This paper now discusses experimental details for each task. \n4.1. Open-domain Question Answering \nOpen-domain question answering (OpenQA) is a key benchmark for evaluating the ability of models \nto retrieve and generate accurate answers from large-scale knowledge sources. In this study, this paper \nevaluates the RAG framework on the MS MARCO dataset, a w idely used benchmark for OpenQA \ntasks. Below, this paper describes the dataset, experimental setup, baseline models, and results. \nMS MARCO (Microsoft Machine Reading Comprehension) is a large -scale dataset designed for \nquestion answering and passage-ranking tasks [8]. It consists of real-world user queries sourced from \nBing search logs, paired with human-generated answers and relevant passages from web documents.  \nFor the experiments, this paper focuses on the question -answering task, where the goal is to \ngenerate concise and accurate answers using the provided passages.  \n4.2. Evaluation \nTo comprehensively evaluate the performance of the RAG framework, this paper employs a \ncombination of traditional and task-specific metrics. this paper introduces TF-IDF cosine similarity, \nBERT-based cosine similarity, and Q-BLEU-1 to assess the quality of the generated responses. Below, \ndetails will be described: \nTF-IDF (Term Frequency -Inverse Document Frequency) is a classic information retrieval \ntechnique that measures the importance of a word in a document relative to a corpus  [9]. It is \ncalculated as: \n TF − IDF(t, d) = TF(t, d) × IDF(t) (2) \nThis paper uses TF-IDF to compute the cosine similarity between the generated response and the \nreference answer, providing a measure of their lexical overlap and relevance. Both the generated \nresponse and the reference answer are represented as TF -IDF vect ors, where each dimension \ncorresponds to a unique word in the corpus, weighted by its TF -IDF score. The similarity between \nthe two vectors is computed as the cosine of the angle between them: \n similarityTF−IDF =\nvgen∙vref\n∥vgen∥∥vref∥ (3) \nWhere vgen  is the TF -IDF vector for generated response and vref  is the TF -IDF vector for \nreference answer. \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n102 \n \n \nBERT-based cosine similarity leverages contextualized embeddings to measure the semantic \nsimilarity between the generated response and the reference answer [10]. The generated response and \nreference answer are encoded into dense vector representations using a BERT model. The similarity \nbetween the two embeddings is computed as: \n similarityBERT =\negen∙eref\n∥egen∥∥eref∥ (4) \nWhere egen is the BERT embedding for generated response and eref is the BERT embedding for \nreference answer. \nBLEU-1 is a variant of the traditional BLEU (Bilingual Evaluation Understudy) metric, adapted \nfor question -answering and knowledge -intensive tasks  [11]. Unlike standard BLEU, Q -BLEU-1 \nfocuses on n -gram overlap between generated and reference texts. Q -BLEU-n incorporates a \nquestion-aware weighting mechanism to better align with the semantic relevance of the generated \ncontent. Specifically: Unigram Precisi on: Q -BLEU-1 calculates the precision of unigrams (single \nwords) in the generated response that match the reference answer, weighted by their relevance to the \ninput question. Question-Answer Alignment: The metric assigns higher weights to unigrams that are \nsemantically aligned with the question, ensuring that the evaluation focuses on the most contextually \nimportant aspects of the response. \nThe Q-BLEU-1 score is computed as: \n Q − BLEU − 1 =\nΣω∈responseweight(ω)∙match(ω,reference)\nΣω∈responseweight(ω)  (5) \nwhere weight(ω) is the question -aware weight of word ω, and match (ω, reference) is a binary \nindicator of whether ω appears in the reference answer. \nThe combination of TF -IDF, BERT Score, and Q -BLEU-1 provides a multi -faceted evaluation \nframework: \nTF-IDF ensures that the retriever provides high -quality input to the generator.  BERT Score \nevaluates the semantic quality and fluency of the generated responses.  R-BLEU-1 provides a \nlightweight measure of lexical overlap, ensuring that the generated text aligns with the reference at a \nsurface level. \nThe differences among the three metrics are shown in Table 1. \nTable 1: Role in Evaluation and Complementary Value of Metrics \nMetric Role in Evaluation Complementary Value \nTF-IDF Assess retrieval relevance Ensures high-quality input for generation \nBERT Evaluate semantic quality of \ngeneration Captures deep semantic understanding \nQ-BLEU-1 Measure lexical overlap Provides efficient surface-level evaluation \n \nTogether, these metrics address the key aspects of RAG's performance, including retrieval \nrelevance, semantic accuracy, and lexical alignment, while mitigating the limitations of individual \nmetrics. \n5. Results \nIn this section, this paper presents the results of the experiments comparing the performance of the \nplain (non-RAG) model and the RAG model across three similarity metrics:  BERT-based semantic \nsimilarity, BLEU score, and TF-IDF-based lexical importance matching. The analysis focuses on the \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n103 \n \n \nability of both models to generate answers that are semantically and lexically aligned with the ground \ntruth. \n5.1. Overall Performance \nIn this part, this paper applies three metrics mentioned above to evaluate the performance of RAG, \ncomparing with the plain method. For each metric, this paper use a figure to compare the scores \nbetween plain and RAG method. In the figure, the coordinate of the point is (plain score, RAG score). \nThen y=x is drawn in the figure to compare the scores.  \n5.1.1. TF-IDF Similarity Scores \nThe TFIDF similarity scores exhibit more variation compared to BERT and BLEU. As seen in Figure \n1, there are some differences in similarity between the \"plain\" and \"RAG\" methods. While both \nmethods show relatively low similarity values, there are a few ins tances where the \"RAG\" method \nyields higher scores, such as in sample 4, where the \"plain\" score is 0.000, but the \"RAG\" score rises \nto 0.0427. These results suggest that the TF -IDF method, which is based on term frequency and \ninverse document frequency, might benefit slightly from the RAG process, although the overall effect \nis still limited. \n \nFigure 1: Similarity Comparison for TF-IDF (Photo/Picture credit: Original). \n5.1.2. BERT Similarity Scores \nThe BERT -based similarity scores, which measure the semantic alignment between generated \nanswers and ground truth, range from -1 (completely dissimilar) to 1 (completely similar). The \nmajority of scores for both models fall within the range of 0.7 to 1, in dicating that most generated \nanswers are semantically close to the ground truth. However, there are instances of negative values \n(e.g., plain: -0.55374306, RAG: -0.31060708), suggesting that both models occasionally generate \nanswers that are completely unrelated to the correct answers (Figure 2). \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n104 \n \n \n \nFigure 2: Similarity Comparison for BERT (Photo/Picture credit: Original). \n5.1.3. Q-BLEU-1 Similarity Scores \nThe BLEU scores, however, show a different pattern. BLEU's similarity scores are mostly close to \nzero for both the \"plain\" and \"RAG\" methods (in Figure 3), indicating that this metric does not capture \nsignificant similarity between the compared texts in th e majority of the samples. But for non -zero \npoints, in most cases (about 87% cases), RAG achieves higher BLEU scores than the plain model. \n \nFigure 3: Similarity Comparison for Q-BLEU-1 (Photo/Picture credit: Original). \n5.2. Statistical Analysis \nTo quantify the performance differences, this paper computed the mean, and standard deviation of \nthe similarity scores and the rate of cases RAG outperforms the plain method for the RAG model. To \ncompare the performance of RAG method under different metrics, this paper shows the result in Table \n2. \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n105 \n \n \nTable 2: Mean, Std, and Rate of cases RAG Outperforms Plain Method \n Mean for RAG Std Dev for RAG Rate of cases RAG outperforms plain \nmethod \nTF-IDF 0.1788 0.1654 63% \nBERT 0.8381 0.2608 54% \nQ-BLEU-1 0.0254 0.0484 86% \n6. Challenges & Future Work \n6.1. Current Challenges \n6.1.1. Retrieval Quality and Efficiency \nThe performance of RAG heavily depends on the quality of the retrieved documents. Inaccurate or \nirrelevant retrievals can lead to suboptimal or incorrect generation results. Additionally, retrieving \nfrom large-scale knowledge corpora (e.g., Wikipedia) can be computationally expensive, especially \nfor real-time applications [12]. \nFuture work could explore more advanced retrieval techniques, such as hierarchical retrieval or \nactive retrieval strategies, to improve both accuracy and efficiency. Additionally, integrating multi -\nmodal retrieval (e.g., combining text, images, and structu red data)  [13,14] could enhance the \nrelevance of retrieved content. \n6.1.2. Multi-Hop Reasoning \nMany knowledge -intensive tasks, such as complex question answering, require multi -hop \nreasoning—combining information from multiple documents to derive the correct answer. Current \nRAG systems often struggle with such tasks due to limitations in their ability to reason across multiple \nretrieved documents [15]. \nDeveloping models that explicitly support multi -hop reasoning, such as graph -based retrieval or \niterative retrieval -generation pipelines, could address this challenge. Additionally, incorporating \nreinforcement learning to optimize retrieval and generation for multi -hop tasks may improve \nperformance. \n6.1.3. Knowledge Coverage and Timeliness \nRAG relies on external knowledge corpora, which may lack coverage for niche domains or fail to \ninclude up-to-date information. This limits the model's ability to handle queries requiring specialized \nor real-time knowledge. \nExpanding the knowledge corpus to include domain-specific sources (e.g., medical journals, legal \ndatabases) and integrating dynamic updates (e.g., live data feeds) could improve coverage and \ntimeliness. Additionally, leveraging techniques like continual learning could help RAG systems adapt \nto evolving knowledge. \n6.2. Future Directions \nCurrent RAG systems typically retrieve documents once at the beginning of the generation process. \nHowever, dynamically retrieving documents during generation (e.g., based on intermediate outputs) \ncould improve relevance and accuracy. Active retrieval strategies, where the model decides when and \nwhat to retrieve, could further enhance performance.  Extending RAG to handle multi -modal inputs \n(e.g., text, images, videos) could enable more applications, such as generating responses based on \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n106 \n \n \nvisual or audio context  [13]. This would require advancements in multi -modal retrieval and \ngeneration techniques. \nDeploying RAG in resource -constrained environments (e.g., edge devices) requires reducing its \ncomputational and memory footprint. Techniques like model distillation, quantization, and efficient \nretrieval algorithms could make RAG more accessible for real-world applications. \nIncorporating human feedback into the retrieval and generation process could improve the quality \nand reliability of RAG systems. For example, users could validate retrieved documents or refine \ngenerated responses, enabling the model to learn from human expertise. \nAs RAG systems are deployed in sensitive domains (e.g., healthcare, and education), addressing \nethical concerns such as bias, fairness, and privacy becomes critical. Future research should focus on \ndeveloping frameworks to ensure responsible use of RAG technology. \n7. Conclusion \nThis paper presented a study of RAG, a framework that combines the advantages of retrieval -based \nmethods and generative models to address the limitations of purely generative models in knowledge-\nintensive tasks. This paper’s work demonstrates that RAG effe ctively leverages external knowledge \nto enhance the accuracy, relevance, and interpretability of generated text, while maintaining the \nflexibility and fluency of state-of-the-art language models. \nThrough extensive experiments on MS MARCO, this paper showed that RAG outperforms both \npurely generative models (GPT -3.5-turbo) by evaluating three metrics, including TF -IDF, BERT \nScore and Q-BLEU-1. The ablation studies further highlighted the importance of end-to-end training \nand the synergistic interaction between the retriever and generator components. \nDespite its promising results, RAG faces several challenges, including retrieval efficiency, multi-\nhop reasoning, and knowledge coverage. Addressing these limitations will be critical for advancing \nthe capabilities of retrieval -augmented systems. Future wo rk should explore dynamic retrieval \nstrategies, multi -modal integration, and lightweight deployment techniques to make RAG more \nscalable and adaptable to real-world applications. \nIn conclusion, RAG represents a significant step forward in bridging the gap between retrieval and \ngeneration, offering a powerful framework for knowledge -intensive NLP tasks. This paper believes \nthat these findings will inspire further research in this di rection, ultimately leading to more robust, \ninterpretable, and trustworthy AI systems. \nReferences \n[1] Roberts, A., Raffel, C., & Shazeer, N. (2020). How much knowledge can you pack into the parameters of a language \nmodel?. arXiv preprint arXiv:2002.08910. \n[2] Marcus, G. (2020). The next decade in AI: four steps towards robust artificial intelligence. arXiv preprint \narXiv:2002.06177.  \n[3] Glendinning, I. (2013). Comparison of policies for academic integrity in higher education across the European \nUnion.Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming -Wei Chang. REALM:  Retrieval-\naugmented language model pre-training. ArXiv, abs/2002.08909, 2020. \n[4] Lee, K., Chang, M. W., & Toutanova, K. (2019). Latent retrieval for weakly supervised open domain question \nanswering. arXiv preprint arXiv:1906.00300. \n[5] Karpukhin, V., Oguz, B., Min, S., Lewis, P. S., Wu, L., Edunov, S., ... & Yih, W. T. (2020, November). Dense Passage \nRetrieval for Open-Domain Question Answering. In EMNLP (1) (pp. 6769-6781). \n[6] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language \nmodels to follow instructions with human feedback. Advances in neural information processing systems, 35, 27730-\n27744. \n[7] Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., & Deng, L. (2016). Ms marco: A human -\ngenerated machine reading comprehension dataset. \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n107 \n \n \n[8] Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., & Weston, J. (2018). Wizard of wikipedia: Knowledge-powered \nconversational agents. arXiv preprint arXiv:1811.01241. \n[9] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019, June). Bert: Pre -training of deep bidirectional \ntransformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of \nthe association for computational linguistics: human language technologies, volume 1 (long and short papers) (pp. \n4171-4186). \n[10] Nema, P., & Khapra, M. M. (2018). Towards a better metric for evaluating question generation systems. arXiv \npreprint arXiv:1808.10192. \n[11] Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., & Shazeer, N. (2018). Generating wikipedia by \nsummarizing long sequences. arXiv preprint arXiv:1801.10198. \n[12] Faghri, F., Fleet, D. J., Kiros, J. R., & Fidler, S. (2017). Vse++: Improving visual-semantic embeddings with hard \nnegatives. arXiv preprint arXiv:1707.05612. \n[13] Wang, L., Li, Y., & Lazebnik, S. (2016). Learning deep structure-preserving image-text embeddings. In Proceedings \nof the IEEE conference on computer vision and pattern recognition (pp. 5005 -5013). \n[14] Lin, X. V., Socher, R., & Xiong, C. (2018). Multi -hop knowledge graph reasoning with reward shaping. arXiv \npreprint arXiv:1808.10568. \n[15] Weston, J., Chopra, S., & Bordes, A. (2014). Memory networks. arXiv preprint arXiv:1410.3916.  \nProceedings of MSS 2025 Symposium: Automation and Smart Technologies in Petroleum Engineering \nDOI: 10.54254/2755-2721/142/2025.KL22312 \n108 ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5258296728134155
    },
    {
      "name": "Information retrieval",
      "score": 0.34709978103637695
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ],
  "cited_by": 1
}