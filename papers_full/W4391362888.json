{
    "title": "A Two-Stage Attention-Based Hierarchical Transformer for Turbofan Engine Remaining Useful Life Prediction",
    "url": "https://openalex.org/W4391362888",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5083056696",
            "name": "Zhengyang Fan",
            "affiliations": [
                "George Mason University"
            ]
        },
        {
            "id": "https://openalex.org/A5036747126",
            "name": "Wanru Li",
            "affiliations": [
                "George Mason University"
            ]
        },
        {
            "id": "https://openalex.org/A5019345566",
            "name": "Kuo‐Chu Chang",
            "affiliations": [
                "George Mason University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1982275278",
        "https://openalex.org/W2108898839",
        "https://openalex.org/W2292044413",
        "https://openalex.org/W1753777452",
        "https://openalex.org/W1579040661",
        "https://openalex.org/W2152411594",
        "https://openalex.org/W1976146618",
        "https://openalex.org/W2000774724",
        "https://openalex.org/W2325344880",
        "https://openalex.org/W3181794117",
        "https://openalex.org/W3136300763",
        "https://openalex.org/W2406882687",
        "https://openalex.org/W4386715033",
        "https://openalex.org/W2059401976",
        "https://openalex.org/W3175127519",
        "https://openalex.org/W3005147174",
        "https://openalex.org/W3185585276",
        "https://openalex.org/W3126272279",
        "https://openalex.org/W4386820292",
        "https://openalex.org/W3012833929",
        "https://openalex.org/W3138886298",
        "https://openalex.org/W3120284962",
        "https://openalex.org/W3035429418",
        "https://openalex.org/W3100533721",
        "https://openalex.org/W4385429279",
        "https://openalex.org/W4311387174",
        "https://openalex.org/W3048894909",
        "https://openalex.org/W4311358050",
        "https://openalex.org/W3112478554",
        "https://openalex.org/W4283014531",
        "https://openalex.org/W2944676531",
        "https://openalex.org/W3137613462",
        "https://openalex.org/W4360770771",
        "https://openalex.org/W4386760021",
        "https://openalex.org/W4384937305",
        "https://openalex.org/W6854382130",
        "https://openalex.org/W4387341255",
        "https://openalex.org/W4389149897",
        "https://openalex.org/W6797282723",
        "https://openalex.org/W4285266752",
        "https://openalex.org/W4312560592",
        "https://openalex.org/W3146366485",
        "https://openalex.org/W2120841219",
        "https://openalex.org/W2013821261",
        "https://openalex.org/W2594845301",
        "https://openalex.org/W2811131765",
        "https://openalex.org/W6801894677",
        "https://openalex.org/W6839418665",
        "https://openalex.org/W2016210396",
        "https://openalex.org/W2910660149",
        "https://openalex.org/W2772084711",
        "https://openalex.org/W6788158879",
        "https://openalex.org/W3037995823",
        "https://openalex.org/W3014146531",
        "https://openalex.org/W3176877591",
        "https://openalex.org/W4388096928",
        "https://openalex.org/W4362009348",
        "https://openalex.org/W3200811867",
        "https://openalex.org/W3116796810",
        "https://openalex.org/W4383753167",
        "https://openalex.org/W4285186957",
        "https://openalex.org/W3173407600"
    ],
    "abstract": "The accurate estimation of the remaining useful life (RUL) for aircraft engines is essential for ensuring safety and uninterrupted operations in the aviation industry. Numerous investigations have leveraged the success of the attention-based Transformer architecture in sequence modeling tasks, particularly in its application to RUL prediction. These studies primarily focus on utilizing onboard sensor readings as input predictors. While various Transformer-based approaches have demonstrated improvement in RUL predictions, their exclusive focus on temporal attention within multivariate time series sensor readings, without considering sensor-wise attention, raises concerns about potential inaccuracies in RUL predictions. To address this concern, our paper proposes a novel solution in the form of a two-stage attention-based hierarchical Transformer (STAR) framework. This approach incorporates a two-stage attention mechanism, systematically addressing both temporal and sensor-wise attentions. Furthermore, we enhance the STAR RUL prediction framework by integrating hierarchical encoder–decoder structures to capture valuable information across different time scales. By conducting extensive numerical experiments with the CMAPSS datasets, we demonstrate that our proposed STAR framework significantly outperforms the current state-of-the-art models for RUL prediction.",
    "full_text": null
}