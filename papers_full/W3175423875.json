{
    "title": "SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining",
    "url": "https://openalex.org/W3175423875",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2124400109",
            "name": "Taolin Zhang",
            "affiliations": [
                "East China Normal University",
                "Alibaba Group (United States)",
                "Shanghai Key Laboratory of Trustworthy Computing"
            ]
        },
        {
            "id": "https://openalex.org/A3173445773",
            "name": "Zerui Cai",
            "affiliations": [
                "East China Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2108884846",
            "name": "Chengyu Wang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2114674632",
            "name": "Minghui Qiu",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2162411816",
            "name": "Bite Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2115656599",
            "name": "Xiaofeng He",
            "affiliations": [
                "East China Normal University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963895422",
        "https://openalex.org/W3004611733",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2997012196",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W3210120707",
        "https://openalex.org/W3105892552",
        "https://openalex.org/W3034987253",
        "https://openalex.org/W2161914416",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W2123402141",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W2983102021",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3114916066",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4287829318",
        "https://openalex.org/W3035053871",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2939876107",
        "https://openalex.org/W3100809613",
        "https://openalex.org/W2963386218",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W2468628149",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2735585131",
        "https://openalex.org/W1854214752",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W3102725307",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W41404523",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3083494020",
        "https://openalex.org/W2604314403",
        "https://openalex.org/W2100664567",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2184957013",
        "https://openalex.org/W3010108619",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2740205663",
        "https://openalex.org/W3081505754",
        "https://openalex.org/W2158139315",
        "https://openalex.org/W3046375318"
    ],
    "abstract": "Taolin Zhang, Zerui Cai, Chengyu Wang, Minghui Qiu, Bite Yang, Xiaofeng He. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 5882–5893\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5882\nSMedBERT: A Knowledge-Enhanced Pre-trained Language Model with\nStructured Semantics for Medical Text Mining\nTaolin Zhang1,2,3, Zerui Cai4, Chengyu Wang2, Minghui Qiu2\nBite Yang5, Xiaofeng He4,6∗\n1 School of Software Engineering, East China Normal University 2 Alibaba Group\n3 Shanghai Key Laboratory of Trsustworthy Computing\n4 School of Computer Science and Technology, East China Normal University5 DXY\n6 Shanghai Research Institute for Intelligent Autonomous Systems\nzhangtl0519@gmail.com, zrcai flow@126.com, yangbt@dxy.cn\n{chengyu.wcy, minghui.qmh}@alibaba-inc.com, hexf@cs.ecnu.edu.cn\nAbstract\nRecently, the performance of Pre-trained Lan-\nguage Models (PLMs) has been signiﬁcantly\nimproved by injecting knowledge facts to en-\nhance their abilities of language understanding.\nFor medical domains, the background knowl-\nedge sources are especially useful, due to the\nmassive medical terms and their complicated\nrelations are difﬁcult to understand in text. In\nthis work, we introduce SMedBERT, a med-\nical PLM trained on large-scale medical cor-\npora, incorporating deep structured semantics\nknowledge from neighbours of linked-entity.\nIn SMedBERT, the mention-neighbour hybrid\nattention is proposed to learn heterogeneous-\nentity information, which infuses the semantic\nrepresentations of entity types into the homo-\ngeneous neighbouring entity structure. Apart\nfrom knowledge integration as external fea-\ntures, we propose to employ the neighbors of\nlinked-entities in the knowledge graph as addi-\ntional global contexts of text mentions, allow-\ning them to communicate via shared neighbors,\nthus enrich their semantic representations. Ex-\nperiments demonstrate that SMedBERT signif-\nicantly outperforms strong baselines in various\nknowledge-intensive Chinese medical tasks. It\nalso improves the performance of other tasks\nsuch as question answering, question matching\nand natural language inference.1\n1 Introduction\nPre-trained Language Models (PLMs) learn effec-\ntive context representations with self-supervised\ntasks, spotlighting in various NLP tasks (Wang\net al., 2019a; Nan et al., 2020; Liu et al., 2020a). In\naddition, Knowledge-Enhanced PLMs (KEPLMs)\n(Zhang et al., 2019; Liu et al., 2020b; Wang et al.,\n2019b) further beneﬁt language understanding by\n∗Corresponding author.\n1The code and pre-trained models will be available at\nhttps://github.com/MatNLP/SMedBERT.\nఉᆆ⯲\n( sore throat) \n⣬⯻∈ \n(novel coronavirus) \nSymptom Cause of disease \nલ੮᝕ḉ \n(respiratory \ninfection) \nDisease \nલ੮㔲ਾᖷ \n(respiratory \nsyndrome) \nSymptom-Disease \nCause-Disease \nEntity Type Relation \nCOVID-19 \n\u0003,QSXW\u00037H[W\u001d  䓡։ ਇ✣ θఉᆆ⯲ θ㞯⌱ ᱥ᝕ḉ ⣬⯻∈ (COVID-19 )Ⲻ⯽⣬Ⱦ\n (Fever , sore throat , and diarrhea  are symptoms of novel coronavirus  (COVID-19 ).) \n1HLJKERULQJ\u0003(QWLW\\\u0003IURP\u0003';<\u0010.* \n/LQNHG\u0010(QWLW\\ \n㛰⛄ \n(pneumonia) \n᝕ḉ〇\n(infectious \nDepartment)\nSymptom-Symptom Medical Department \nਇ✝ \n(fever) \nCause-Department \nFigure 1: Example of neighboring entity information\nin medical text. (Best viewed in color)\ngrounding these PLMs with high-quality, human-\ncurated knowledge facts, which are difﬁcult to learn\nfrom raw texts.\nIn the literatures, a majority of KEPLMs (Zhang\net al., 2020a; Hayashi et al., 2020; Sun et al.,\n2020) inject information of entities corresponding\nto mention-spans from Knowledge Graphs (KGs)\ninto contextual representations. However, those\nKEPLMs only utilize linked-entity in the KGs as\nauxiliary information, which pay little attention\nto the neighboring structured semantics informa-\ntion of the entity linked with text mentions. In\nthe medical context, there exist complicated do-\nmain knowledge such as relations and medical facts\namong medical terms (Rotmensch et al., 2017; Li\net al., 2020), which are difﬁcult to model using\nprevious approaches. To address this issue, we\nconsider leveraging structured semantics knowl-\nedge in medical KGs from the two aspects. (1)\nRich semantic information from neighboring struc-\ntures of linked-entities, such as entity types and\nrelations, are highly useful for medical text under-\nstanding. As in Figure 1, “新型冠状病毒” (novel\ncoronavirus) can be the cause of many diseases,\nsuch as “肺炎” (pneumonia) and “ 呼吸综合征”\n5883\n(respiratory syndrome). 2 (2) Additionally, we\nleverage neighbors of linked-entity as global “con-\ntexts” to complement plain-text contexts used in\n(Mikolov et al., 2013a; Pennington et al., 2014).\nThe structure knowledge contained in neighbour-\ning entities can act as the “knowledge bridge” be-\ntween mention-spans, facilitating the interaction of\ndifferent mention representations. Hence, PLMs\ncan learn better representations for rare medical\nterms.\nIn this paper, we introduce SMedBERT, a KE-\nPLM pre-trained over large-scale medical corpora\nand medical KGs. To the best of our knowledge,\nSMedBERT is the ﬁrst PLM with structured se-\nmantics knowledge injected in the medical do-\nmain. Speciﬁcally, the contributions of SMedBERT\nmainly include two modules:\nMention-neighbor Hybrid Attention: We fuse\nthe embeddings of the node and type of linked-\nentity neighbors into contextual target mention rep-\nresentations. The type-level and node-level atten-\ntions help to learn the importance of entity types\nand the neighbors of linked-entity, respectively, in\norder to reduce the knowledge noise injected into\nthe model. The type-level attention transforms the\nhomogeneous node-level attention into a heteroge-\nneous learning process of neighboring entities.\nMention-neighbor Context Modeling: We pro-\npose two novel self-supervised learning tasks for\npromoting interaction between mention-span and\ncorresponding global context, namely masked\nneighbor modeling and masked mention modeling.\nThe former enriches the representations of “con-\ntext” neighboring entities based on the well trained\n“target word” mention-span, while the latter focuses\non gathering those information back from neighbor-\ning entities to the masked target like low-frequency\nmention-span which is poorly represented (Turian\net al., 2010).\nIn the experiments, we compare SMedBERT\nagainst various strong baselines, including main-\nstream KEPLMs pre-trained over our medical re-\nsources. The underlying medical NLP tasks in-\nclude: named entity recognition, relation extrac-\ntion, question answering, question matching and\nnatural language inference. The results show that\nSMedBERT consistently outperforms all the base-\nlines on these tasks.\n2Although we focus on Chinese medical PLMs here. The\nproposed method can be easily adapted to other languages,\nwhich is beyond the scope of this work.\n2 Related Work\nPLMs in the Open Domain. PLMs have gained\nmuch attention recently, proving successful for\nboosting the performance of various NLP tasks\n(Qiu et al., 2020). Early works on PLMs focus\non feature-based approaches to transform words\ninto distributed representations (Collobert and We-\nston, 2008; Mikolov et al., 2013b; Pennington et al.,\n2014; Peters et al., 2018). BERT (Devlin et al.,\n2019) (as well as its robustly optimized version\nRoBERTa (Liu et al., 2019b)) employs bidirec-\ntional transformer encoders (Vaswani et al., 2017)\nand self-supervised tasks to generate context-aware\ntoken representations. Further improvement of per-\nformances mostly based on the following three\ntypes of techniques, including self-supervised tasks\n(Joshi et al., 2020), transformer encoder architec-\ntures (Yang et al., 2019) and multi-task learning\n(Liu et al., 2019a).\nKnowledge-Enhanced PLMs. As existing BERT-\nlike models only learn knowledge from plain cor-\npora, various works have investigated how to in-\ncorporate knowledge facts to enhance the lan-\nguage understanding abilities of PLMs. KEPLMs\nare mainly divided into the following three types.\n(1) Knowledge-enhanced by Entity Embedding:\nERNIE-THU (Zhang et al., 2019) and KnowBERT\n(Peters et al., 2019) inject linked-entity as hetero-\ngeneous features learned by KG embedding algo-\nrithms such as TransE (Bordes et al., 2013). (2)\nKnowledge-enhanced by Entity Description: E-\nBERT (Zhang et al., 2020a) and KEPLER (Wang\net al., 2019b) add extra description text of entities to\nenhance semantic representation. (3) Knowledge-\nenhanced by Triplet Sentence: K-BERT (Liu et al.,\n2020b) and CoLAKE (Sun et al., 2020) convert\ntriplets into sentences and insert them into the train-\ning corpora without pre-trained embedding. Pre-\nvious studies on KG embedding (Nguyen et al.,\n2016; Schlichtkrull et al., 2018) have shown that\nutilizing the surrounding facts of entity can obtain\nmore informative embedding, which is the focus of\nour work.\nPLMs in the Medical Domain. PLMs in the med-\nical domain can be generally divided into three\ncategories. (1) BioBERT (Lee et al., 2020), Blue-\nBERT (Peng et al., 2019), SCIBERT (Beltagy et al.,\n2019) and ClinicalBert (Huang et al., 2019) ap-\nply continual learning on medical domain texts,\nsuch as PubMed abstracts, PMC full-text articles\nand MIMIC-III clinical notes. (2) PubMedBERT\n5884\nDƵůƚŝͲ,ĞĂĚ\u0003 \n^ĞůĨͲ\u0004ƚƚĞŶƚŝŽŶ \n&ĞĞĚ\u0003&ŽƌǁĂƌĚ\u0003 \n>ĂǇĞƌ \ndŽŬĞŶ\u0003/ŶƉƵƚ\u0003 \nDǆ \ndͲ\u001cŶĐŽĚĞƌ \n<Ͳ\u001cŶĐŽĚĞƌ \nEǆ \nDƵůƚŝͲ,ĞĂĚ\u0003 \n^ĞůĨͲ\u0004ƚƚĞŶƚŝŽŶ \n,ǇďƌŝĚ\u0003\u0004ƚƚĞŶƚŝŽŶ\u0003 \nEĞƚǁŽƌŬ \n,ĞƚĞƌŽŐĞŶĞŽƵƐ\u0003/ŶĨŽƌŵĂƚŝŽŶ\u0003&ƵƐŝŽŶ \nWƌĞͲƚƌĂŝŶŝŶŐ\u0003 \ndĂƐŬƐ \nD>D\u0003 DĞŶƚŝŽŶͲŶĞŝŐŚďŽƌ\u0003\u0012ŽŶƚĞǆƚ\u0003DŽĚĞůŝŶŐ \n;ĂͿ ^DĞĚ\u0011\u001cZd\u0003\u0004ƌĐŚŝƚĞĐƚƵƌĞ \n<ŶŽǁůĞĚŐĞ\u0003/ŶƉƵƚ \nDĞĚŝĐĂů\u0003<' \nᯠߐ\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003 ⯵ ∂\n\u0003˄\u0012Ks/\u0018Ͳϭϵ ˅\nEĞŝŐŚďŽƌŝŶŐ \n\u001cŶƚŝƚŝĞƐ \ndǇƉĞ EŽĚĞ \nEŽĚĞ \nW\u001cWZ \ndǇƉĞ \nJ\nÊ ,ǇďƌŝĚ\u0003\u0004ƚƚ͘\u0003Θ\u0003/ŶĨŽƌ͘\u0003&ƵƐŝŽŶ\u0003DŽĚƵůĞƐ \nÊ DĞŶƚŝŽŶͲŶĞŝŐŚďŽƌ \u0003\u0012ŽŶƚĞǆƚ\u0003DŽĚĞůŝŶŐ \n༃ DĂƐŬĞĚ\u0003EĞŝŐŚďŽƌ\u0003DŽĚĞůŝŶŐ \n\u0014U\n\u0016U\u0015U\nŬŶŽǁůĞĚŐĞ\u0003ĐŽŶƚĞǆƚ ŬŶŽǁůĞĚŐĞ\u0003ĐŽŶƚĞǆƚ \n\u0017U΀D\u0004^<΁ \nĸDĂƐŬĞĚ\u0003DĞŶƚŝŽŶ\u0003DŽĚĞůŝŶŐ \nW>DƐ ƚŽŬĞŶƐ \n\u0014PO\n\u0016PO\n\u0015PO\nFigure 2: Model overview of SMedBERT. The left part is our model architecture and the right part is the details of\nour model including hybrid attention network and mention-neighbor context modeling pre-training tasks.\n(Gu et al., 2020) learns weights from scratch using\nPubMed data to obtain an in-domain vocabulary,\nalleviating the out-of-vocabulary (OOV) problem.\nThis training paradigm needs the support of large-\nscale domain data and resources. (3) Some other\nPLMs use domain self-supervised tasks for pre-\ntraining. For example, MC-BERT (Zhang et al.,\n2020b) masks Chinese medical entities and phrases\nto learn complex structures and concepts. Disease-\nBERT (He et al., 2020) leverages the medical terms\nand its category as the labels to pre-train the model.\nIn this paper, we utilize both domain corpora and\nneighboring entity triplets of mentions to enhance\nthe learning of medical language representations.\n3 The SMedBERT Model\n3.1 Notations and Model Overview\nIn the PLM, we denote the hidden feature of each\ntoken {w1,...,w N}as {h1,h2,...,h N}where N\nis the maximum input sequence length and the total\nnumber of pre-training samples as M. Let E be\nthe set of mention-span em in the training corpora.\nFurthermore, the medical KG consists of the enti-\nties set Eand the relations set R. The triplet set\nis S = {(h,r,t) |h∈E,r ∈R,t ∈E}, where h\nis the head entity with relation rto the tail entity\nt. The embeddings of entities and relations trained\non KG by TransR (Lin et al., 2015) are represented\nas Γent and Γrel, respectively. The neighboring\nentity set recalled from KG by em is denoted as\nNem = {e1\nm,e2\nm,...,e K\nm}where Kis the threshold\nof our PEPR algorithm. We denote the number of\nentities in the KG as Z. The dimensions of the hid-\nden representation in PLM and the KG embeddings\nare d1 and d2, respectively.\nThe main architecture of the our model is shown\nin Figure 2. SMedBERT mainly includes three\ncomponents: (1) Top-K entity sorting determine\nwhich K neighbour entities to use for each men-\ntion. (2) Mention-neighbor hybrid attention aims\nto infuse the structured semantics knowledge into\nencoder layers, which includes type attention,\nnode attention and gated position infusion module.\n(3) Mention-neighbor context modeling includes\nmasked neighbor modeling and masked mention\nmodeling aims to promote mentions to leverage\nand interact with neighbour entities.\n3.2 Top-K Entity Sorting\nPrevious research shows that simple neighboring\nentity expansion may induce knowledge noises dur-\ning PLM training (Wang et al., 2019a). In order\nto recall the most important neighboring entity set\nfrom the KG for each mention, we extend the Per-\nsonalized PageRank (PPR) (Page et al., 1999) algo-\nrithm to ﬁlter out trivial entities. 3 Recall that the it-\nerative process in PPR isVi = (1−α)A·Vi−1+αP\nwhere A is the normalized adjacency matrix, α\nis the damping factor, P is uniformly distributed\njump probability vector, andV is the iterative score\nvector for each entity.\nPEPR speciﬁcally focuses on learning the weight\nfor the target mention span in each iteration. It\n3We name our algorithm to be Personalized Entity PageR-\nank, abbreviated as PEPR.\n5885\nassigns the span em a higher jump probability 1 in\nP with the remaining as 1\nZ. It also uses the entity\nfrequency to initialize the score vector V:\nVem =\n{ tem\nT em ∈E\n1\nM em /∈E\n(1)\nwhere T is the sum of frequencies of all entities.\ntem is the frequency of em in the corpora. After\nsorting, we select the top-Kentity set Nem.\n3.3 Mention-neighbor Hybrid Attention\nBesides the embeddings of neighboring entities,\nSMedBERT integrates the type information of med-\nical entities to further enhance semantic representa-\ntions of mention-span.\n3.3.1 Neighboring Entity Type Attention\nDifferent types of neighboring entities may have\ndifferent impacts. Given a speciﬁc mention-span\nem, we compute the neighboring entity type atten-\ntion. Concretely, we calculate hidden representa-\ntion of each entity type τ as hτ = ∑\neim∈Eτm\nheim.\nEτ\nm are neighboring entities of em with the same\ntype τ and heim = Γent\n(\nei\nm\n)\n∈Rd2 .\nh′\nem = LN(σ(fsp(hi,...,h j) Wbe)) (2)\nwhere fsp is the self-attentive pooling (Lin et al.,\n2017) to generate the mention-span representation\nhem ∈Rd1 and the (hi,hi+1,...,h j) is the hid-\nden representation of tokens (wi,wi+1,...,w j) in\nmention-span em trained by PLMs. h′\nem ∈Rd2\nis obtained by σ(·) non-linear activation function\nGELU (Hendrycks and Gimpel, 2016) and the\nlearnable projection matrix Wbe ∈Rd1×d2 . LNis\nthe LayerNorm function (Ba et al., 2016). Then, we\ncalculate the each type attention weight using the\ntype representation hτ ∈Rd2 and the transformed\nmention-span representation h′\nem:\nα′\nτ = tanh\n(\nh′\nemWt + hτWt′\n)\nWa (3)\nwhere Wt ∈Rd2×d2 , Wt′ ∈Rd2×d2 and Wa ∈\nRd2×1. Finally, the neighboring entity type atten-\ntion weights ατ are obtained by normalizing the\nattention score α′\nτ among all entity types T.\n3.3.2 Neighboring Entity Node Attention\nApart from entity type information, different\nneighboring entities also have different inﬂuences.\nSpeciﬁcally, we devise the neighboring entity node\nattention to capture the different semantic inﬂu-\nences from neighboring entities to the target men-\ntion span and reduce the effect of noises. We cal-\nculate the entity node attention using the mention-\nspan representation h′\nem and neighboring entities\nrepresentation heim with entity type τ as:\nβ′\nemeim\n=\n(\nh′\nemWq\n)(\nheimWk\n)T\n√d2\nατ (4)\nβemeim =\nexp\n(\nβ′\nemeim\n)\n∑\neim∈Nem\nexp\n(\nβ′\nemeim\n) (5)\nwhere Wq ∈Rd2×d2 and Wk ∈Rd2×d2 are the\nattention weight matrices.\nThe representations of all neighboring entities in\nNem are aggregated to ¯h′\nem ∈Rd2 :\nˆh′\nem =\n∑\neim∈Nem\nβemeim\n(\nheimWv + bv\n)\n(6)\n¯h′\nem = LN\n(\nˆh′\nem +\n(\nσ\n(\nˆh′\nemWl1 + bl1\n)\nWl2\n))\n(7)\nwhere Wv ∈ Rd2×d2 , Wl1 ∈ Rd2×4d2 , Wl2 ∈\nR4d2×d2 . bv ∈Rd2 and bl1 ∈R4d2 are the bias\nvectors. ¯h′\nem is the mention-neighbor representa-\ntion from hybrid attention module.\n3.3.3 Gated Position Infusion\nKnowledge-injected representations may divert the\ntexts from its original meanings. We further reduce\nknowledge noises via gated position infusion:\nh′\nemf = σ\n([¯h′\nem ∥h′\nem\n]\nWmf + bmf\n)\n(8)\n˜h′\nemf = LN(h′\nemfWbp + bbp) (9)\nwhere Wmf ∈R2d2×2d2 , Wbp ∈R2d2×d1 , bmf ∈\nR2d2 , bbp ∈Rd1 . h′\nemf ∈R2d2 is the span-level\ninfusion representation. “∥” means concatenation\noperation. ˜h′\nemf ∈ Rd1 is the ﬁnal knowledge-\ninjected representation for mention em. We gener-\nate the output token representation hif by 4:\ngi = tanh\n(([\nhi ∥˜h′\nemf\n])\nWug + bug\n)\n(10)\nhif = σ\n(([\nhi ∥gi ∗˜h′\nemf\n])\nWex + bex\n)\n+ hi\n(11)\nwhere Wug, Wex ∈R2d1×d1 . bug,bex ∈Rd1 . “∗”\nmeans element-wise multiplication.\n4We ﬁnd that restricting the knowledge infusion position\nto tokens is helpful to improve performance.\n5886\n3.4 Mention-neighbor Context Modeling\nTo fully exploit the structured semantics knowl-\nedge in KG, we further introduce two novel self-\nsupervised pre-training tasks, namely Masked\nNeighbor Modeling (MNeM) and Masked Men-\ntion Modeling (MMeM).\n3.4.1 Masked Neighbor Modeling\nFormally, let rbe the relation between the mention-\nspan em and a neighboring entity ei\nm:\nhmf = LN(σ(fsp(hif,...,h jf) Wsa)) (12)\nwhere hmf is the mention-span hidden fea-\ntures based on the tokens hidden representation(\nhif,h(i+1)f,...,h jf\n)\n. hr = Γ rel(r) ∈Rd2 is\nthe relation rrepresentation and Wsa ∈Rd1×d2 is\na learnable projection matrix. The goal of MNeM\nis leveraging the structured semantics in surround-\ning entities while reserving the knowledge of re-\nlations between entities. Considering the object\nfunctions of skip-gram with negative sampling\n(SGNS) (Mikolov et al., 2013a) and score func-\ntion of TransR (Lin et al., 2015):\nLS = log fs(w,c) + k·Ecn∼PD[log fs(w,−cn)]\n(13)\nftr(h,r,t) =∥hMr + r−tMr ∥ (14)\nwhere the win LS is the target word of contextc. fs\nis the compatibility function measuring how well\nthe target word is ﬁtted into the context. Inspired by\nSGNS, following the general energy-based frame-\nwork (LeCun et al., 2006), we treat mention-spans\nin corpora as “target words”, and neighbors of cor-\nresponding entities in KG as “contexts” to pro-\nvide additional global contexts. We employ the\nSampled-Softmax (Jean et al., 2015) as the crite-\nrion LMNeM for the mention-span em:\n∑\nNem\nlog exp(fs(θ))\nexp(fs(θ)) + K·Een∼Q(en)[exp(fs(θ′))]\n(15)\nwhere θdenotes the triplet (em,r,ei\nm), ei\nm ∈Nem.\nθ′is the negative triplets (em,r,en), and en is neg-\native entity sampled with Q(ei\nm) detailed in Ap-\npendix B. To keep the knowledge of relations be-\ntween entities, we deﬁne the compatibility function\nas:\nfs\n(\nem,r,ei\nm\n)\n= hmfMr + hr\n||hmfMr + hr||·(heimMr)T\n||heimMr||µ\n(16)\nwhere µis a scale factor. Assuming the norms of\nboth hmfMr + hr and heimMr are 1,we have:\nfs\n(\nem,r,ei\nm\n)\n= µ ⇐⇒ftr(hmf,hr,heim) = 0\n(17)\nwhich indicates the proposedfsis equivalence with\nftr. Because |henMr |needs to be calculated for\neach en, the computation of the score function fs\nis costly. Hence, we transform part of the formula\nfs as follows:\n(hmfMr + hr) ·(henMr)T =\n[\nhmf 1\n][ Mr\nhr\n][ Mr\nhr\n]T [\nhen 0\n]T\n=\n[\nhmf 1\n]\nMPr\n[\nhen 0\n]T\n(18)\nIn this way, we eliminate computation of transform-\ning each hen. Finally, to compensate the offset in-\ntroduced by the negative sampling function Q(ei\nm)\n(Jean et al., 2015), we complement fs(em,r,ei\nm)\nas:\n[\nhmf 1\n]\nMPr\n∥\n[\nhmf 1\n]\nMPr ∥·\n[ heim 0 ]\n∥heim ∥ µ−µlog Q(ei\nm)\n(19)\n3.4.2 Masked Mention Modeling\nIn contrast to MNeM, MMeM transfers the seman-\ntic information in neighboring entities back to the\nmasked mention em.\nYm = LN(σ(fsp(hip,...,h jp) Wsa)) (20)\nwhere Ym is the ground-truth representation of em\nand hip = Γ p(wi) ∈Rd2 . Γp is the pre-trained\nembedding of BERT in our medical corpora. The\nmention-span representation obtained by our model\nis hmf. For a sample s, the loss of MMeMLMMeM\nis calculated via Mean-Squared Error:\nLMMeM =\nMs∑\nmi\n∥hmif −Ymi ∥2 (21)\nwhere Ms is the set of mentions of sample s.\n3.5 Training Objective\nIn SMedBERT, the training objectives mainly con-\nsist of three parts, including the self-supervised\nloss proposed in previous works and the mention-\nneighbor context modeling loss proposed in our\nwork. Our model can be applied to medical text\npre-training directly in different languages as long\n5887\nas high-quality medical KGs can be obtained. The\ntotal loss is as follows:\nLtotal = LEX + λ1LMNeM + λ2LMMeM (22)\nwhere LEX is the sum of sentence-order predic-\ntion (SOP) (Lan et al., 2020) and masked language\nmodeling. λ1 and λ2 are the hyperparameters.\n4 Experiments\n4.1 Data Source\nPre-training Data. The pre-training corpora after\npre-processing contains 5,937,695 text segments\nwith 3,028,224,412 tokens (4.9 GB). The KGs em-\nbedding trained by TransR (Lin et al., 2015) on\ntwo trusted data sources, including the Symptom-\nIn-Chinese from OpenKG5 and DXY-KG6 contain-\ning 139,572 and 152,508 entities, respectively. The\nnumber of triplets in the two KGs are 1,007,818\nand 3,764,711. The pre-training corpora and the\nKGs are further described in Appendix A.1.\nTask Data. We use four large-scale datasets in\nChineseBLUE (Zhang et al., 2020b) to evaluate\nour model, which are benchmark of Chinese med-\nical NLP tasks. Additionally, we test models on\nfour datasets from real application scenarios pro-\nvided by DXY company 7 and CHIP 8, i.e., Named\nEntity Recognition (DXY-NER), Relation Extrac-\ntion (DXY-RE, CHIP-RE) and Question Answer\n(WebMedQA (He et al., 2019)). For other informa-\ntion of the downstream datasets, we refer readers\nto Appendix A.2.\n4.2 Baselines\nIn this work, we compare SMedBERT with general\nPLMs, domain-speciﬁc PLMs and KEPLMs with\nknowledge embedding injected, pre-trained on our\nChinese medical corpora:\nGeneral PLMs: We use three Chinese BERT-style\nmodels, namely BERT-base (Devlin et al., 2019),\nBERT-wwm (Cui et al., 2019) and RoBERTa (Liu\net al., 2019b). All the weights are initialized from\n(Cui et al., 2020).\nDomain-speciﬁc PLMs: As very few PLMs in the\nChinese medical domain are available, we consider\nthe following models. MC-BERT (Zhang et al.,\n5http://www.openkg.cn/dataset/\nsymptom-in-chinese\n6https://portal.dxy.cn/\n7https://auth.dxy.cn/accounts/login\n8http://www.cips-chip.org.cn:8088/home\nModel D1 D2 D3\nSGNS-char-med 27.21% 27.16% 21.72%\nSGNS-word-med 24.64% 24.95% 20.37%\nGLOVE-char-med 27.24% 27.12% 21.91%\nGLOVE-word-med 24.41% 23.89% 20.56%\nBERT-open 29.79% 29.41% 21.83%\nBERT-wwm-open 29.75% 29.55% 21.97%\nRoBERTa-open 30.84% 30.56% 21.98%\nMC-BERT 30.63% 30.34% 22.65%\nBioBERT-zh 30.84% 30.69% 22.71%\nERNIE-med 30.97% 30.78% 22.99%\nKnowBERT-med 30.95% 30.77% 23.07%\nSMedBERT 31.81% 32.14% 24.08%\nTable 1: Results of unsupervised semantic similarity\ntask. “med” refers to models continually pre-trained on\nmedical corpora, and “open” means open-domain cor-\npora. “char’ and “word” refer to the token granularity\nof input samples.\n2020b) is pre-trained over a Chinese medical cor-\npora via masking different granularity tokens. We\nalso pre-train BERT using our corpora, denoted as\nBioBERT-zh.\nKEPLMs: We employ two SOTA KEPLMs con-\ntinually pre-trained on our medical corpora as our\nbaseline models, including ERNIE-THU (Zhang\net al., 2019) and KnowBERT (Peters et al., 2019).\nFor a fair comparison, KEPLMs use other addi-\ntional resources rather than the KG embedding are\nexcluded (See Section 2), and all the baseline KE-\nPLMs are injected by the same KG embedding.\nThe detailed parameter settings and training pro-\ncedure are in Appendix B.\n4.3 Intrinsic Evaluation\nTo evaluate the semantic representation ability of\nSMedBERT, we design an unsupervised semantic\nsimilarity task. Speciﬁcally, we extract all entities\npairs with equivalence relations in KGs as positive\npairs. For each positive pair, we use one of the\nentity as query entity while the other as positive\ncandidate, which is used to sample other entities\nas negative candidates. We denote this dataset as\nD1. Besides, the entities in the same positive pair\noften have many neighbours in common. We select\npositive pairs with large proportions of common\nneighbours as D2. Additionally, to verify the abil-\nity of SMedBERT of enhancing the low-frequency\nmention representation, we extract all positive pairs\nthat with at least one low-frequency mention as D3.\nThere are totally 359,358, 272,320 and 41,583 sam-\nples for D1, D2, D3 respectively. We describe the\n5888\nNamed Entity Recognition Relation Extraction\nModel cMedQANER DXY-NER Average CHIP-RE DXY-RE Average\nDev Test Dev Test Test Test Dev Test Test\nBERT-open 80.69% 83.12% 79.12% 79.03% 81.08% 85.86% 94.18% 94.13% 90.00%\nBERT-wwm-open 80.52% 83.07% 79.48% 79.29% 81.18% 86.01% 94.35% 94.38% 90.20%\nRoBERT-open 80.92% 83.29% 79.27% 79.33% 81.31% 86.19% 94.64% 94.66% 90.43%\nBioBERT-zh 80.72% 83.38% 79.52% 79.45% 81.42% 86.12% 94.54% 94.64% 90.38%\nMC-BERT 81.02% 83.46% 79.79% 79.59% 81.53% 86.09% 94.74% 94.73% 90.41%\nKnowBERT-med 81.29% 83.75% 80.86% 80.44% 82.10% 86.27% 95.05% 94.97% 90.62%\nERNIE-med 81.22% 83.87% 80.82% 80.87% 82.37% 86.25% 94.98% 94.91% 90.58%\nSMedBERT 82.23% 84.75% 83.06% 82.94% 83.85% 86.95% 95.73% 95.89% 91.42%\nTable 2: Performance of Named Entity Recognition (NER) and Relation Extraction (RE) tasks in terms of F1. The\nDevelopment data of CHIP-RE is unreleased in public dataset.\nQuestion Answering Question Matching Natural Lang. Infer.\nModel cMedQA WebMedQA Average cMedQQ cMedNLI\nDev Test Dev Test Test Dev Test Dev Test\nBERT-open 72.99% 73.82% 77.20% 79.72% 76.77% 86.74% 86.72% 95.52% 95.66%\nBERT-wwm-open 72.03% 72.96% 77.06% 79.68% 76.32% 86.98% 86.82% 95.53% 95.78%\nRoBERT-open 72.22% 73.18% 77.18% 79.57% 76.38% 87.24% 86.97% 95.87% 96.11%\nBioBERT-zh 74.32% 75.12% 78.04% 80.45% 77.79% 87.30% 87.06% 95.89% 96.04%\nMC-BERT 74.40% 74.46% 77.85% 80.54% 77.50% 87.17% 87.01% 95.81% 96.06%\nKnowBERT-med 74.38% 75.25% 78.20% 80.67% 77.96% 87.25% 87.14% 95.96% 96.03%\nERNIE-med 74.37% 75.22% 77.93% 80.56% 77.89% 87.34% 87.20% 96.02% 96.25%\nSMedBERT 75.06% 76.04% 79.26% 81.68% 78.86% 88.13% 88.09% 96.64% 96.88%\nTable 3: Performance of Question Answering (QA), Question Matching (QM) and Natural Language Inference\n(NLI) tasks. The metric of the QA task is Acc@1 and those of QM and NLI are F1.\ndetails of collecting data and embedding words\nin Appendix C. In this experiments, we compare\nSMedBERT with three types of models: classical\nword embedding methods (SGNS (Mikolov et al.,\n2013a), GLOVE (Pennington et al., 2014)), PLMs\nand KEPLMs. We compute the similarity between\nthe representation of query entities and all the other\nentities, retrieving the most similar one. The evalu-\nation metric is top-1 accuracy (Acc@1).\nExperiment results are shown in Table 1. From\nthe results, we observe that: (1) SMedBERT greatly\noutperforms all baselines especially on the dataset\nD2 (+1.36%) , where most positive pairs have\nmany shared neighbours, demonstrating that ability\nof SMedBERT to utilize semantic information from\nthe global context. (2) In dataset D3, SMedBERT\nimprove the performance signiﬁcantly (+1.01%),\nindicating our model is effective to enhance the\nrepresentation of low-frequency mentions.\n4.4 Results of Downstream Tasks\nWe ﬁrst evaluate our model in NER and RE tasks\nthat are closely related to entities in the input texts.\nTable 2 shows the performances on medical NER\nand RE tasks. In NER and RE tasks, we can ob-\nserve from the results: (1) Compared with PLMs\ntrained in open-domain corpora, KEPLMs with\nmedical corpora and knowledge facts achieve bet-\nter results. (2) The performance of SMedBERT is\ngreatly improved compared with the strongest base-\nline in two NER datasets (+0.88%, +2.07%), and\n(+0.68%, +0.92%) on RE tasks. We also evaluate\nSMedBERT on QA, QM and NLI tasks and the\nperformance is shown in Table 3. We can observe\nthat SMedBERT improve the performance consis-\ntently on these datasets (+0.90% on QA, +0.89%\non QM and +0.63% on NLI) . In general, it can\nbe seen from Table 2 and Table 3 that injecting the\ndomain knowledge especially the structured seman-\ntics knowledge can improve the result greatly.\n4.5 Inﬂuence of Entity Hit Ratio\nIn this experiment, we explore the model perfor-\nmance in NER and RE tasks with different entity hit\nratios, which control the proportions of knowledge-\nenhanced mention-spans in the samples. The aver-\n5889\nFigure 3: Entity hit ratio results of SMedBERT and\nERNIE in NER and RE tasks.\nFigure 4: The inﬂuence of different K values in results.\nage number of mention-spans in samples is about\n40. Figure 3 illustrates the performance of SMed-\nBERT and ERNIE-med (Zhang et al., 2019). From\nthe result, we can observe that: (1) The perfor-\nmance improves signiﬁcantly at the beginning and\nthen keeps stable as the hit ratio increases, prov-\ning the heterogeneous knowledge is beneﬁcial to\nimprove the ability of language understanding and\nindicating too much knowledge facts are unhelpful\nto further improve model performance due to the\nknowledge noise (Liu et al., 2020b). (2) Compared\nwith previous approaches, our SMedBERT model\nimproves performance greatly and more stable.\n4.6 Inﬂuence of Neighboring Entity Number\nWe further evaluate the model performance under\ndifferent K over the test set of DXY-NER and\nDXY-RE. Figure 4 shows the the model result with\nK = {5,10,20,30}. In our settings, the SMed-\nBERT can achieve the best performance in differ-\nent tasks around K = 10. The results of SMed-\nBERT show that the model performance increasing\nﬁrst and then decreasing with the increasing of\nK. This phenomenon also indicates the knowledge\nnoise problem that injecting too much knowledge\nof neighboring entities may hurt the performance.\n4.7 Ablation Study\nIn Table 4, we choose three important model com-\nponents for our ablation study and report the test\nModel D5 D6 D7 D8\nSMedBERT 84.75% 82.94% 86.95% 95.89%\nERNIE-med 83.87% 80.87% 86.25% 94.91%\n- Type Att. 84.25% 81.99% 86.61% 95.29%\n- Hybrid Att. 83.71% 80.85% 86.46% 95.20%\n- Know. Loss 84.31% 82.12% 86.50% 95.43%\nTable 4: Ablation study of SMedBERT on four datasets\n(testing set). Due to the space limitation, we use the ab-\nbreviations “D5”, “D6”, “D7”, and “D8” to represent\nthe cMedQANER, DXY-NER, CHIP-RE, and DXY-\nRE datasets respectively.\nset performance on four datasets of NER and RE\ntasks that are closely related to entities. Speciﬁ-\ncally, the three model components are neighboring\nentity type attention, the whole hybrid attention\nmodule, and mention-neighbor context modeling\nrespectively, which includes two masked language\nmodel loss LMNeM and LMMeM.\nFrom the result, we can observe that: (1) With-\nout any of the three mechanisms, our model per-\nformance can also perform competitively with the\nstrong baseline ERNIE-med (Zhang et al., 2019).\n(2) Note that after removing the hybrid attention\nmodule, the performance of our model has the\ngreatest decline, which indicates that injecting rich\nheterogeneous knowledge of neighboring entities\nis effective.\n5 Conclusion\nIn this work, we address medical text mining tasks\nwith the structured semantics KEPLM proposed\nnamed SMedBERT. Accordingly, we inject entity\ntype semantic information of neighboring entities\ninto node attention mechanism via heterogeneous\nfeature learning process. Moreover, we treat the\nneighboring entity structures as additional global\ncontexts to predict the masked candidate entities\nbased on mention-spans and vice versa. The exper-\nimental results show the signiﬁcant improvement\nof our model on various medical NLP tasks and\nthe intrinsic evaluation. There are two research di-\nrections that can be further explored: (1) Injecting\ndeeper knowledge by using “farther neighboring”\nentities as contexts; (2) Further enhancing Chinese\nmedical long-tail entity semantic representation.\nAcknowledgements\nWe would like to thank anonymous reviewers for\ntheir valuable comments. This work is supported by\n5890\nthe National Key Research and Development Pro-\ngram of China under Grant No. 2016YFB1000904,\nand Alibaba Group through Alibaba Research In-\ntern Program.\nReferences\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.\nHinton. 2016. Layer normalization. CoRR,\nabs/1607.06450.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientiﬁc text. In\nEMNLP, pages 3613–3618.\nAntoine Bordes, Nicolas Usunier, Alberto Garc ´ıa-\nDur´an, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In NIPS, pages 2787–2795.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: deep\nneural networks with multitask learning. In ICML,\npages 160–167.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for chinese natural language process-\ning. In EMNLP, pages 657–668.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nBERT. CoRR, abs/1906.08101.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL, pages 4171–4186.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lu-\ncas, Naoto Usuyama, Xiaodong Liu, Tristan Nau-\nmann, Jianfeng Gao, and Hoifung Poon. 2020.\nDomain-speciﬁc language model pretraining for\nbiomedical natural language processing. CoRR,\nabs/2007.15779.\nHiroaki Hayashi, Zecong Hu, Chenyan Xiong, and Gra-\nham Neubig. 2020. Latent relation language models.\nIn AAAI, pages 7911–7918.\nJunqing He, Mingming Fu, and Manshu Tu. 2019. Ap-\nplying deep matching networks to chinese medical\nquestion answering: a study and a dataset. BMC\nMedical Informatics Decis. Mak., 19-S(2):91–100.\nYun He, Ziwei Zhu, Yin Zhang, Qin Chen, and James\nCaverlee. 2020. Infusing disease knowledge into\nBERT for health question answering, medical in-\nference and disease name recognition. In EMNLP,\npages 4604–4614.\nDan Hendrycks and Kevin Gimpel. 2016. Gaussian er-\nror linear units (gelus). arXiv:1606.08415.\nKexin Huang, Jaan Altosaar, and Rajesh Ran-\nganath. 2019. Clinicalbert: Modeling clinical\nnotes and predicting hospital readmission. CoRR,\nabs/1904.05342.\nPaul Jaccard. 1912. The distribution of the ﬂora in the\nalpine zone. New Phydvtologist, 11(2):37–50.\nS´ebastien Jean, KyungHyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015. On using very large tar-\nget vocabulary for neural machine translation. In\nACL, pages 1–10.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanbert: Improving pre-training by representing\nand predicting spans. Trans. Assoc. Comput. Lin-\nguistics, 8:64–77.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In ICLR.\nYann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato,\nand F Huang. 2006. A tutorial on energy-based\nlearning. Predicting structured data, 1(0).\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. Biobert: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinform., 36(4):1234–\n1240.\nLinfeng Li, Peng Wang, Jun Yan, Yao Wang, Simin\nLi, Jinpeng Jiang, Zhe Sun, Buzhou Tang, Tsung-\nHui Chang, Shenghui Wang, and Yuting Liu. 2020.\nReal-world data medical knowledge graph: con-\nstruction and applications. Artif. Intell. Medicine ,\n103:101817.\nYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and\nXuan Zhu. 2015. Learning entity and relation em-\nbeddings for knowledge graph completion. In AAAI,\npages 2181–2187.\nZhouhan Lin, Minwei Feng, C´ıcero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. In ICLR.\nDayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng\nChen, Daxin Jiang, Jiancheng Lv, and Nan Duan.\n2020a. Rikinet: Reading wikipedia pages for nat-\nural question answering. In ACL, pages 6762–6771.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020b. K-BERT:\nenabling language representation with knowledge\ngraph. In AAAI, pages 2901–2908.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks\nfor natural language understanding. In ACL, pages\n4487–4496.\n5891\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nTom´as Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013a. Efﬁcient estimation of word represen-\ntations in vector space. In ICLR.\nTom´as Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013b. Distributed rep-\nresentations of words and phrases and their compo-\nsitionality. In NIPS, pages 3111–3119.\nGuoshun Nan, Zhijiang Guo, Ivan Sekulic, and Wei Lu.\n2020. Reasoning with latent structure reﬁnement for\ndocument-level relation extraction. In ACL, pages\n1546–1557.\nDat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark\nJohnson. 2016. Neighborhood mixture model for\nknowledge base completion. In CoNLL, pages 40–\n50.\nLawrence Page, Sergey Brin, Rajeev Motwani, and\nTerry Winograd. 1999. The pagerank citation rank-\ning: Bringing order to the web. Technical Report\n1999-66, Stanford InfoLab.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of BERT and elmo on ten\nbenchmarking datasets. In BioNLP, pages 58–65.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP, pages 1532–1543.\nMatthew E. Peters, Mark Neumann, Robert L. Logan\nIV , Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced contex-\ntual word representations. In EMNLP, pages 43–54.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL, pages 2227–2237.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nCoRR, abs/2003.08271.\nMaya Rotmensch, Yoni Halpern, Abdulhakim Tlimat,\nSteven Horng, and David Sontag. 2017. Learning\na health knowledge graph from electronic medical\nrecords. Scientiﬁc reports, 7(1):1–11.\nMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter\nBloem, Rianne van den Berg, Ivan Titov, and Max\nWelling. 2018. Modeling relational data with graph\nconvolutional networks. In ESWC, pages 593–607.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo,\nYaru Hu, Xuanjing Huang, and Zheng Zhang. 2020.\nColake: Contextualized language and knowledge\nembedding. In COLING, pages 3660–3670.\nJoseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-\ngio. 2010. Word representations: A simple and gen-\neral method for semi-supervised learning. In ACL,\npages 384–394.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nXiaoyan Wang, Pavan Kapanipathi, Ryan Musa,\nMo Yu, Kartik Talamadupula, Ibrahim Abdelaziz,\nMaria Chang, Achille Fokoue, Bassem Makni,\nNicholas Mattei, and Michael Witbrock. 2019a. Im-\nproving natural language inference using external\nknowledge in the science questions domain. In\nAAAI, pages 7208–7215.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juanzi Li, and Jian Tang. 2019b. KE-\nPLER: A uniﬁed model for knowledge embedding\nand pre-trained language representation. CoRR,\nabs/1911.06136.\nWilliam E Winkler. 1990. String comparator metrics\nand enhanced decision rules in the fellegi-sunter\nmodel of record linkage.\nLiang Xu, Xuanwei Zhang, and Qianqian Dong.\n2020. Cluecorpus2020: A large-scale chinese\ncorpus for pre-training language model. CoRR,\nabs/2003.01355.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NIPS, pages 5754–\n5764.\nDenghui Zhang, Zixuan Yuan, Yanchi Liu, Zuohui Fu,\nFuzhen Zhuang, Pengyang Wang, Haifeng Chen,\nand Hui Xiong. 2020a. E-BERT: A phrase and\nproduct knowledge enhanced language model for e-\ncommerce. CoRR, abs/2009.02835.\nNingyu Zhang, Qianghuai Jia, Kangping Yin, Liang\nDong, Feng Gao, and Nengwei Hua. 2020b.\nConceptualized representation learning for chinese\nbiomedical text mining. CoRR, abs/2008.10813.\nSheng Zhang, Xin Zhang, Hui Wang, Jiajun Cheng, Pei\nLi, and Zhaoyun Ding. 2017. Chinese medical ques-\ntion answer matching using end-to-end character-\nlevel multi-scale cnns. Applied Sciences, 7(8):767.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: en-\nhanced language representation with informative en-\ntities. In ACL, pages 1441–1451.\n5892\nA Data Source\nA.1 Pre-training Data\nA.1.1 Training Corpora\nThe pre-training corpora is crawled from DXY\nBBS (Bulletin Board System) 9, which is a very\npopular Chinese social network for doctors, med-\nical institutions, life scientists, and medical prac-\ntitioners. The BBS has more than 30 channels,\nwhich contains 18 forums and 130 ﬁne-grained\ngroups, covering most of the medical domains. For\nour pre-training purpose, we crawl texts from chan-\nnels about clinical medicine, pharmacology, public\nhealth and consulting. For text pre-processing, we\nmainly follow the methods of (Xu et al., 2020). Ad-\nditionally, (1) we remove all URLs, HTML tags,\ne-mail addresses, and all tokens except characters,\ndigits, and punctuation (2) all documents shorter\nthan 256 are discard, while documents longer than\n512 are cut into shorter text segments.\nA.1.2 Knowledge Graph\nThe DXY knowledge graph is construed by ex-\ntracting structured text from DXY website 10,\nwhich includes information of diseases, drugs\nand hospitals edited by certiﬁed medical experts,\nthus the quality of the KG is guaranteed. The\nKG is mainly disease-centered, including totally\n3,764,711 triples, 152.508 unique entities, and 44\ntypes of relations. The details of Symptom-In-\nChinese from OpenKG is available 11. We ﬁnally\nget 26 types of entities, 274,163 unique entities, 56\ntypes of relations, and 4,390,726 triples after the\nfusion of the two KGs.\nA.2 Task Data\nWe choose the four large-scale datasets in Chine-\nseBlue tasks (Zhang et al., 2020b) while others\nare ignored due to the limitation of datasets size,\nwhich are cMedQANER, cMedQQ, cMedQNLI\nand cMedQA. WebMedQA (He et al., 2019) is\na real-world Chinese medical question answering\ndataset and CHIP-RE dataset are collected from\nonline health consultancy websites. Note that since\nboth the WebMedQA and cMedQA datasets are\nvery large while we have many baselines to be\ncompared, we randomly sample the ofﬁcial train-\ning set, development set and test set respectively\n9https://www.dxy.cn/bbs/newweb/pc/home\n10https://portal.dxy.cn/\n11http://openkg.cn/dataset/\nsymptom-in-chinese\nto form their corresponding smaller version for ex-\nperiments. DXY-NER and DXY-RE are datasets\nfrom real medical application scenarios provided\nby a prestigious Chinese medical company. The\nDXY-NER contains 22 unique entity types and 56\nrelation types in the DXY-RE. These two datasets\nare collected from the medical forum of DXY and\nbooks in the medical domain. Annotators are se-\nlected from junior and senior students with clinical\nmedical background. In the process of quality con-\ntrol, the two datasets are annotated twice by differ-\nent groups of annotators. An expert with medical\nbackground performs quality check manually again\nwhen annotated results are inconsistent, whereas\nperform sampling quality check when results are\nconsistent. Table 5 shows the datasets size of our\nexperiments.\nB Model Settings and Training Details\nHyper-parameters. d1=768, d2=200, K=10, µ\n=10, λ1=2, λ2=4.\nModel Details. We align the all mention-spans\nto the entity in KG by exact match for compar-\nison purpose with ENIRE-THU (Zhang et al.,\n2019). The negative sampling function is deﬁned\nas Q(ei\nm) =\nteim\nCeim\n, where Ceim is the sum of fre-\nquency of all mentions with the same type of ei\nm.\nThe Mention-neighbor Hybrid Attention module is\ninserted after the tenth transformer encoder layer\nto compare with KnowBERT (Peters et al., 2019),\nwhile we perform the Mention-neighbor Context\nModeling based on the output of BERT encoder.\nWe use all the base-version PLMs in the experi-\nments. The size of SMedBERT is 474MB while\n393MB of that are components of BERT, and the\nadded 81MB is mostly of the KG embedding. Re-\nsults are presented in average with 5 random runs\nwith different random seeds and the same hyper-\nparameters.\nTraining Procedure. We strictly follow the orig-\ninally pre-training process and parameter setting\nof other KEPLMs. We only adapt their publicly\navailable code from English to Chinese and use\nthe knowledge embedding trained on our medical\nKG. To have a fair comparison, the pre-training\nprocessing of SMedBERT is mostly set based on\nENIRE-THU (Zhang et al., 2019) without layer-\nspecial learning rates in KnowBERT (Peters et al.,\n2019). We only pre-train SMedBERT on the col-\nlected medical data for 1 epoch. In pre-training\n5893\nThe Dataset Size in Our Experiments\nDataset Train Dev Test Task Metric\ncMedQANER\n(Zhang et al., 2020b)\n1,673 175 215 NER F1\ncMedQQ\n(Zhang et al., 2020b)\n16,071 1,793 1,935 QM F1\ncMedQNLI\n(Zhang et al., 2020b)\n80,950 9,065 9,969 NLI F1\ncMedQA\n(Zhang et al., 2017)\n186,771 46,600 46,600 QA Acc@1\nWebMedQA\n(He et al., 2019)\n252,850 31,605 31,655 QA Acc@1\nCHIP-RE ∗ 43,649 - 10,622 RE F1\nDXY-NER 34,224 8,576 8,592 NER F1\nDXY-RE 141,696 35,456 35,794 RE F1\n∗CHIP-RE dataset is released in CHIP 2020. (http://cips-chip.org.cn/2020/eval2)\nTable 5: The statistical data and metric of eight datasets used in our SMedBERT model.\nprocess, the learning rate is set to 5e−5 and batch\nsize is 512 with the max sequence length is 512.\nFor ﬁne-tuning, we ﬁnd the following ranges of\npossible values work well, i.e., batch size is{8,16},\nlearning rate (AdamW) is {2e−5, 4e−5, 6e−5}and\nthe number of epochs is {2,3,4}. Pre-training\nSMedBERT takes about 36 hours per epoch on\n2 NVIDIA GeForce RTX 3090 GPUs.\nC Data and Embedding of Unsupervised\nSemantic Similarity\nSince the KGs used in this paper is a directed\ngraph, we ﬁrst transform the directed ”等价关系”\n(equivalence relations) pairs to undirected pairs\nand discard the duplicated pairs. For each posi-\ntive pairs, we use head and tail as query respec-\ntively and sample the negative candidates based\non the other. Speciﬁcally, we randomly select 19\nnegative entities with the same type and has a Jaro-\nWinkle similarity (Winkler, 1990) bigger 0.6 with\nthe ground-truth entity. We select from all samples\nin Dataset-1 with positive pairs that the neighbours\nsets of head and tail entity have Jaccard Index (Jac-\ncard, 1912) no less than 0.75 and at least 3 common\nelement to construct the Dataset-2. For Dataset-3,\nwe count the frequency of all entity mentions in pre-\ntraining corpora, and treat mentions with frequency\nno more than 200 as low-frequency mentions.\nClassic Word Representation Embedding:\nWe train the character-level and word-level em-\nbedding using SGNS (Mikolov et al., 2013a) and\nGLOVE (Pennington et al., 2014) model respec-\ntively on our medical corpora with open-source\ntoolkits12. We average the character embedding for\nall tokens in the mention to get the character-level\nrepresentation. However, since some mentions are\nvery rare in the corpora for word-level representa-\ntion, we use the character-level representation as\ntheir word-level representation.\nBERT-like Representation Embedding: We\nextract the token hidden features of the last layer\nand average the representations of the input tokens\nexcept [CLS] and [SEP] tag, to get a vector for\neach entity.\nSimilarity Measure: We try using the inverse of\nL2-distance and cosine similarity as measurement,\nand we ﬁnd that cosine similarity always perform\nbetter. Hence, we report all experiment results\nunder the cosine similarity metric.\n12SGNS: https://github.com/JuGyang/\nword2vec-SGNS.\nGlove: https://github.com/stanfordnlp/\nGloVe"
}