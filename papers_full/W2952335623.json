{
  "title": "Specialized Language Models using Dialogue Predictions",
  "url": "https://openalex.org/W2952335623",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5053619454",
      "name": "C. Popovici",
      "affiliations": [
        "National Institute for Research and Development in Informatics - ICI Bucharest"
      ]
    },
    {
      "id": "https://openalex.org/A5060649349",
      "name": "Paolo Baggia",
      "affiliations": [
        "Telecom Italia Lab"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1538585905",
    "https://openalex.org/W3720092",
    "https://openalex.org/W109558583",
    "https://openalex.org/W2137692244",
    "https://openalex.org/W2950492505",
    "https://openalex.org/W1504944830",
    "https://openalex.org/W2158310889",
    "https://openalex.org/W2096312440"
  ],
  "abstract": "This paper analyses language modeling in spoken dialogue systems for accessing a database. The use of several language models obtained by exploiting dialogue predictions gives better results than the use of a single model for the whole dialogue interaction. For this reason several models have been created, each one for a specific system question, such as the request or the confirmation of a parameter. The use of dialogue-dependent language models increases the performance both at the recognition and at the understanding level, especially on answers to system requests. Moreover other methods to increase performance, like automatic clustering of vocabulary words or the use of better acoustic models during recognition, does not affect the improvements given by dialogue-dependent language models. The system used in our experiments is Dialogos, the Italian spoken dialogue system used for accessing railway timetable information over the telephone. The experiments were carried out on a large corpus of dialogues collected using Dialogos.",
  "full_text": "arXiv:cmp-lg/9612002v2  13 Dec 1996\nSPECIALIZED LANGUAGE MODELS USING DIALOGUE PREDICTIONS\nCosmin Popovici Paolo Baggia\nICI - Institutul de Cercetari in Informatica\nBd. M. Averescu, 8-10\nBucuresti (Romania)\nCSELT - Centro Studi e Laboratori\nTelecomunicazioni\nVia G. Reiss Romoli, 274\nI-10148 Torino (Italy)\nbaggia@cselt.stet.it\nABSTRACT\nThis paper analyses language modeling in spoken\ndialogue systems for accessing a database. The use of\nseveral language models obtained by exploiting dialogue\npredictions gives better results than the use of a single\nmodel for the whole dialogue interaction. For this reason\nseveral models have been created, each one for a speciﬁc\nsystem question, such as the request or the conﬁrmation\nof a parameter.\nThe use of dialogue-dependent language models in-\ncreases the performance both at the recognition and at\nthe understanding level, especially on answers to sys-\ntem requests. Moreover using other methods to increase\nperformances, like automatic clustering of vocabulary-\nwords or the use of better acoustic models during recog-\nnition, does not aﬀect the improvements given by dia-\nlogue-dependent language models.\nThe system used in our experiments is Dialogos, the\nItalian spoken dialogue system used for accessing rail-\nway timetable information over the telephone. The ex-\nperiments were carried out on a large corpus of dialogues\ncollected using Dialogos.\n1. INTRODUCTION\nIn a spoken dialogue system (SDS) a method to improve\nspeech recognition and speech understanding is to use\ncontextual knowledge as a constraint, both at the recog-\nnition and at the parsing level [1].\nCarter [2] shows that clustering the sentences of the\ntraining corpus into subcorpora on the basis of the crite-\nrion of minimizing entropy, improves n-gram based lan-\nguage models. We propose that the splitting of a corpus\nacquired from a SDS should be done according to the di-\nalogue point in which an utterance was given. On these\nsubcorpora a set of more speciﬁc n-gram based language\nmodels was trained. This work extends the previous one\ndescribed in [3], where ﬁrst insights into the usefulness\nof dialogue predictions were given on a corpus acquired\nwith an earlier version of the dialogue system, see [4].\nOur use of dialogue prediction is similar to the static\nprediction described in [5] and is related to the dialogue-\nstep dependent models in [6], the diﬀerence being that\nwe also measured performance at the understanding le-\nvls el.\nOther methods to improve SDS performances in con-\njunction with the use of dialogue predictions were tested.\nThe work developed in [7] was exploited and the vocab-\nulary-words (VW) were clustered automatically. Fur-\nther improvement was obtained using acoustic models\ntrained on a larger training-set of domain speciﬁc utter-\nances. It’s remarkable that even in those cases the im-\nprovements given by dialogue-dependent language mod-\nels were not aﬀected.\n2. THE SYSTEM USED FOR THE\nACQUISITION\nDialogos is an all-software, completely integrated, di-\nalogue system which runs very close to real-time on a\nDEC Alpha, except for the telephonic interface and text-\nto-speech synthesizer which are run from a PC equipped\nwith a D41E Dialogic board.\nThe acoustical front-end performs feature extraction\nand acoustic-phonetic decoding. The recognition mod-\nule is based on a frame-synchronous Viterbi decoding,\nwhere the acoustic matching is performed by a pho-\nnetic neural network [8]. The vocabulary of Dialogos\ncontains 3,471 words, clustered in 358 classes. 348 of\nthem contain a single word, while the remaining 10\nclasses contain semantically important words, such as\ncity names (2,983 words), station names (33 words),\nnumbers (76 words), months, week days, and so on.\nDuring the recognition, a class-based bigram language\nmodel is used. It was trained on 30,000 sentences. The\ntraining data of the language models was partially de-\nrived from a previous trial of SDS applied to the same\ndomain, but for the most part (86%) it was manually\ncreated.\nThe linguistic processor starts from the best-decoded\nsequence, and it performs a multi-step robust partial\nparsing, which is an improvement of [9]. It accepts par-\ntial solutions on the basis of their coherence with re-\nspect to the parser’s linguistic knowledge and generates\na task- oriented semantic caseframe.\nTo interpret a new utterance in the on-going interac-\ntion, the dialogue module (DM) takes into account the\nlinguistic history and the active focus. This mechanism\nallows the DM to identify linguistic references, ﬁnd out\nthe correct context to apply to utterance interpretation,\nand decide if an utterance causes a shift or restriction\nof focus [10]. The result of the contextual interpreta-\ntion is the choice of a proper dialogue act (DA), such\nas the request or the conﬁrmation of a parameter, and\nthe generation of a system answer. The DM makes use\nof pragmatic expectations about what the user would\nprobably say in a certain dialogue state. On the ba-\nsis of these contextual based expectations the DM can\ngenerate predictions.\nS1> Where are you leaving from and going to?\n< request : departure\ncity, arrivalcity >\nU1> From Turin to Milan.\nS2> Are you leaving from Turin for Milan?\n< confirm : departure\ncity, arrivalcity >\nU2> Yes tomorrow at about eight o’clock.\nS3> Do you want to leave tomorrow at about eight\no’clock?\n< confirm : departure\ndate, arrivaltime >\nU3> Yes.\nS4> I have found two connections . . . Do you want\nother information about these connections?\n< confirm : connection\ninformation >\nU4> No thanks.\nS5> Thank you for the call. Good-bye.\nFigure 1: Example of a dialogue interaction.\nUsing Dialogos a corpus 1 of near 2,000 dialogues for\na total of 19,697 utterances was acquired. A dialogue\nexample is shown in Figure 1, where for each system sen-\ntence ( Si >) the DA and the parameters are given. This\ninformation can also be used for predicting a more spe-\nciﬁc language model which better represents the syntac-\ntic, semantic, and contextual constraints of the future\nuser’s answer.\n3. PREDICTIONS\nThe concept of prediction constitutes the guessing of a\nfuture action and it is commonly used to obtain con-\nstraints in a certain point of a dialogue. In an infor-\nmation inquiry system the knowledge to estimate the\nsubset of user’s DA already exists. In the VERMOBIL\nsystem [11], for instance, a special module estimates the\nset of DAs in the next user utterance and a stochastic\nrecovery is done when the prediction fails. In our system\na certain point in a dialogue is identiﬁed by the question\nthat the user is replying to, i.e. the DA of the system\ngenerated sentence, which is called in the following dia-\nlogue prediction (DP).\nAt the recognition level, we make use of the informa-\ntion that the DM can provide, by creating speciﬁc LMs\nfor each DP. The most speciﬁc LM is obtained from a\ntraining-set which only contains replies given in a cer-\ntain DP. However, some questions very rarely appear\nand for them the information contained in the training\nDB is not enough to obtain a robust LM.\n3.1. QUESTION CLASSIFICATION\nThe system questions were classiﬁed in a natural way.\nAt ﬁrst they were divided into groups according to the\ntype of DA: request for ( Ri) and conﬁrmation of ( Ci)\na parameter i, and listing of train information ( Info ).\nThen these groups were separated into DAs involving\none or more parameters, and, ﬁnally, a distinction was\nmade between the diﬀerent parameters dealt with by\n1A part of this corpus collected from 493 naive users\n(1,363 dialogues, 13,123 utterances) is reported in [12], where\nthe evaluation results of the system are given.\nthe questions, such as departure city ( p), arrival city\n(a), departure time ( t), and departure date ( d). For\nexample, Cp is the conﬁrmation of the departure city,\nRt the request of the departure time, and Rp & Ra\nthe request of both the departure and the arrival cities\nthrough a single sentence. In Figure 2 the various classes\nare shown together with the frequencies of occurrence in\nthe acquired corpus.\nBearing in mind these distinctions, a speciﬁc train-\ning set for each class was obtained. The utterances of\na speciﬁc training-set include all the instances of dif-\nferent user’s answers in that point of the dialogue, for\ninstance in the Cp training-set there are both positive\nand negative conﬁrmations.\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nCp\nCp & Ca\nCa\nCt\nCd\nInfo\nRp\nRs & Ra\nRa\nRt\nRd\n% of utterances\nFigure 2: Relative frequencies of the classes.\n3.2. CREATION OF THE MODELS\nAfter obtaining the training-sets for each speciﬁc class,\ndiﬀerent models were created with the same algorithm\nused for a single context-independent model. All the re-\nsults presented in this paper were obtained using both\na bigram model during the acoustic decoding and a tri-\ngram one for the rescoring of the 25 n-best sequences.\n4. EXPERIMENTAL RESULTS\nWe carried out two sets of experiments using either a\nsingle model for all utterances or a set of speciﬁc models\nthat takes into account the predictions described before.\nBoth the context-independent and the specialized mod-\nels were trained on the same material, 15,575 user utter-\nances, and tested on 2,040 ones. The two sets were dis-\njunctive. Performance is measured at both recognition\nand understanding levels. Recognition performance is\nmeasured in terms of sentence accuracy (SA) and word\naccuracy (W A), and understanding one in terms of sen-\ntence understanding (SU 2) and concept accuracy (CA).\n2SU is obtained comparing for each sentence the case-\nframe generated by the parser with a manually corrected one.\nThe CA takes into account substitution, insertion, and dele-\ntion of concepts, i.e. attibute-value pairs in the caseframe.\nThe CA formula is similar to the W A one, see [13].\n4.1. SINGLE CONTEXT-INDEPENDENT\nMODELS\nTable 1 shows the comparison of the performance of the\nLM used during the acquisition (baseline) and a sin-\ngle dialogue-independent LM obtained with the whole\ntraining-set (ALL\nINT). The baseline model was mainly\ntrained on manually created data, which some of them\nare unusual in a dialog interaction, and so this model\nshows a poor level of speciﬁcity. The ALL\nINT model,\non the other hand, is far more speciﬁc, as it only includes\nutterances occurred through the user dialogues, and so\nit reﬂects the distribution of the utterances in a real\nsetting. Both at the recognition and the understanding\nlevels the ALL\nINT model gives a better performance.\nSA W A SU CA\nbaseline 69.4 68.8 76.1 66.4\nALL INT 70.9 71.1 77.6 68.5\nALL PRED 71.2 73.1 79.4 72.2\nFINAL 71.5 73.4 79.8 72.5\nTable 1: Results of single models and models with DP.\n4.2. LANGUAGE MODELS WITH\nDIALOGUE PREDICTIONS\nA set of two models with DP were tested. The ﬁrst\none, ALL\nPRED, was created as described in Section\n3.2. Another one, FINAL, takes for each class the best\nbetween the single model (ALL\nINT) and the model\nwith DP (ALL PRED), according to the SU metric. For\nclasses containing a few utterances the ALL INT model\nwas preferable, for instance, in the class “conﬁrmation\nof departure city” (Cp), so in this case it was selected.\nThe results for the models with DP are also given in\nTable 1. They show that the use of DP almost dou-\nble the improvement obtained with the ALL\nINT model\nalone. The error rate reduction between ALL INT and\nFINAL is near 10% for W A and SU, and over 20% for\nCA. These improvements are encouraging because they\ncompare favorably with the ones reported in [6].\nThe improvements became clearer if we separate the\ntest utterances into requests for and conﬁrmations of\na parameter, as shown in Table 2. Through the use\nof DP (the FINAL model) a general improvement for\nthe request utterances of 2-4% was achieved. This was\nslightly reduced for the conﬁrmations, because about\n70% of them are utterances of only one word (”Yes”,\n”No”, ”Okay”, and so on), which are always correctly\nrecognized.\nSA W A SU CA\nrequest ALL INT 60.8 74.6 67.4 60.6\nrequest FINAL 62.8 78.9 71.3 66.3\nconﬁrm ALL INT 77.3 71.9 84.6 76.5\nconﬁrm FINAL 76.9 71.3 85.4 78.1\nTable 2: Results for requests and conﬁrmations.\n5. PREDICTIONS VS. OTHER\nIMPROVEMENTS\nIt is interesting to test if the increment of performance\nbrought by the use of DP is aﬀected by the use of other\nmethods. Two methods were tested, such as: the auto-\nmatic clustering of vocabulary words (ACVW) and the\nuse of acoustic models trained on a larger set of domain\nspeciﬁc utterances.\n5.1. LANGUAGE MODELS WITH\nAUTOMATIC CLUSTERING OF\nVOCABULARY WORDS\nWord clustering is commonly used to reduce number of\nparameters of a LM. This could increase the statistical\nrobustness and reduce the size of the model itself. At\nﬁrst, most of the classes (348 from 358) had one single\nword, and these classes were clustered again in auto-\nmatic way using Maximum likelihood method 3, as de-\nscribed in [7]. The ﬁnal number of classes was 120. Two\nmodels FINAL-clust, and ALL\nINT-clust were trained\non the same database as FINAL, and ALL INT de-\nscribed above, but the word classiﬁcation was changed\nfrom 358 to 120 classes.\n5.2. USE OF MORE SPECIFIC ACOUSTIC\nMODELS\nAll experimental results till now, have used an acoustic\nmodel (M1) trained on a set of two DBs. The ﬁrst is\na domain independent one, which contains phonetically\nbalanced data produced by 1,136 speakers, 4,875 utter-\nances (with an average length of 6 words) and 3,653 iso-\nlated words. The second one is domain dependent, and\nit includes 3,580 utterances (with an average length of\n2 words) from 270 speakers. It came from an older SDS\nacquisition. A new acoustic model (M2) was created by\nadding 13,929 utterances (with an average length of 2\nwords), from the corpus described in Section 2, to the\ndomain dependent DB part of M1.\n5.3. FINAL COMPARISON\nTable 3 shows W A and SU results for the LMs with au-\ntoclassiﬁcation using both M1 and M2 acoustic models.\nAutoclassﬁcation only (M1 columns) improved both the\nsingle model and the DP one, compared to the results\nin Table 1, and, as expected, the M2 acoustic models\nfurtherly increment the recognition and understanding\nresults. In any case these improvements does not alter\nthe advantage obtained by the use of DP.\nThe diagram in Figure 3 represents the error rate re-\nduction values between ALL\nINT and FINAL LMs, for\nthree diﬀerent experimental settings, which are: with-\nout ACVW using M1 (-clust/M1); with ACVW still us-\ning M1 (+clust/M1); and with ACVW but using M2\n(+clust/M2). The diagram shows clearly that in each\ncase the LMs which use DP give better recognition and\n3In [7] several clustering methods were compared through\nthe perplexity values and they gave similar results. In this\nwork the choice of the best automatic clustering method was\nmade experimentally.\nW A SU\nM1 M2 M1 M2\nALL INT-clust 71.9 73.8 79.0 81.4\nFINAL-clust 73.4 75.6 80.8 83.5\nTable 3: Comparison between models with ACVW.\n/G30\n/G35\n/G31/G30\n/G31/G35\n/G32/G30\n/G32/G35\n/G33/G30\n/G2d/G63/G6c/G75/G73/G74/G20/G20/G20\n/G4d /G31\n/G2b/G63/G6c/G75/G73/G74/G20/G20\n/G4d /G31/G20\n/G2b/G63/G6c/G75/G73/G74/G20/G20\n/G4d /G32/G20\n/G45 /G72 /G72 /G6f /G72 /G20 /G72 /G65 /G64 /G75 /G63 /G74 /G69 /G6f /G6e /G20 /G28 /G25 \n/G29 \n/G43/G41\n/G53/G55\n/G57 /G41\n/G53/G41\nFigure 3: Error reduction among all the experimental\nsettings.\nunderstanding results (all the error rate reduction val-\nues are positive). Its also remarkable that the use of\nDP, in conjunction with other methods, could even in-\ncrease the improvement. All the values of +clust/M2\nare greater then the -clust/M1 ones, so for SU it goes\nfrom 10.9% (for -clust/M1) to 12.7% (for +clust/M2),\nand from 22.9% to 28.7% for CA. However in +clust/M1\nthe error reduction is the smallest, because the ACVW\nimprove above all the single model (ALL\nINT-clust).\n6. CONCLUSIONS\nIt has been shown that more speciﬁc models (created ex-\nclusively with replies given at a certain point of the dia-\nlogue) improve globally the performance of SDS. On the\nother hand, in some cases the speciﬁc models are not ro-\nbust enough (i.e. very rare, but appropriate utterances).\nThe trade-oﬀ between speciﬁcity and robustness should\nbe better studied in future. The improvement of the\nperformance for requests suggests a proportional general\nimprovement of the whole system, because it implies a\nhigher number of positive replies to the following conﬁr-\nmation and the reduction of the number of turns in the\ndialogue for some unnecessary recovery. Moreover the\nuse of DP is useful in conjunction with other methods,\nsuch as the autoclassiﬁcation of vocabulary words and\nthe use of more speciﬁc acoustic models. These kind of\ndialogue-dependent LMs have been already integrated\ninto Dialogos system.\n7. REFERENCES\n[1] Young, S., “The MINDS system: using context and\ndialogue to enhance speech recognition”, in Proc.\nof DARPA Conf , 1989, pp. 131–136.\n[2] Carter, D., “Improving Language Models by Clus-\ntering Training Sentences”, in Proc. of ANLP 94 ,\nStuttgart, 1994.\n[3] Gerbino, E., P. Baggia, E. Giachin, C. Rullent,\n“Analysis and Evaluation of Spontaneous Speech\nUtterances in Focused Dialogue Context”, in Proc.\nof ESCA Workshop , Vigs, 1995, pp. 185–188.\n[4] Baggia, P., E. Gerbino, E. Giachin, C. Rul-\nlent, “Experiences of spontaneous speech in-\nteraction with a dialogue system”, in Proc.\nof CRIM/FORWISS Workshop , Mnchen, 1994,\npp. 241–248.\n[5] Andry, F., “Static and Dynamic Predictions: A\nMethod to Improve Speech Understanding in Co-\noperative Dialogues”, in Proc. of ICSLP 92, Banﬀ ,\n1992, pp. 639–642.\n[6] Eckert, W., F. Gallwitz, H. Niemann, “Combin-\ning Stochastic and Linguistic Language Models for\nRecognition of Spontaneous Speech”, in Proc. of\nICASSP 96 , Atlanta, 1996, vol. 1, pp. 423–427.\n[7] Moisa, L., E. Giachin, “Automatic Clustering\nof Words for Probabilistic Language Models”, in\nProc. of EUROSPEECH–95 , Madrid, 1995, vol. 2,\npp. 1249–1253.\n[8] Gemello, R., D. Albesano, F. Mana, R. Cancelliere,\n“Recurrent Network Automata for Speech Recog-\nnition: A Summary of Recent Work”, in Proc.\nof IEEE Workshop on NN for Signal Processing ,\nErmioni, Greece, 1994.\n[9] Baggia, P., C. Rullent, “Partial Parsing as a Ro-\nbust Parsing Technique”, in Proc of ICASSP-93 ,\nMinneapolis, 1993, vol. 2, pp. 123–126.\n[10] Gerbino, E., M. Danieli, “Managing Dialogue in\na Continuous Speech Understanding System”, in\nProc. of EUROSPEECH–93 , Berlin, 1993, vol. 3,\npp. 1661–1664.\n[11] Reithinger, N., E. Maier, “Utilizing Statistical Dia-\nlogue Act Processing in Vermobil”, in Proc. of ACL\n95, Cambridge, MA, 1995.\n[12] Albesano, D., P. Baggia, M. Danieli, R. Gemello,\nE. Gerbino, C. Rullent, “Dialogos: A Robust Sys-\ntem for Human-Machine Spoken Dialogue on the\nTelephone”, these Proceedings.\n[13] Boros, M., W. Eckert, F. Gallwitz, G. Grz,\nG. Hanrieder, H. Niemann, “Towards Understand-\ning Spontaneous Speech: Word Accuray vs. Con-\ncept Accuracy”, in Proc. of ICSLP 96 , Philadephia,\n1996, vol 2, pp 1009–1012.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.810215950012207
    },
    {
      "name": "Language model",
      "score": 0.6974251866340637
    },
    {
      "name": "Vocabulary",
      "score": 0.6482106447219849
    },
    {
      "name": "Natural language processing",
      "score": 0.6416603326797485
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5736781358718872
    },
    {
      "name": "Cluster analysis",
      "score": 0.5627944469451904
    },
    {
      "name": "Spoken language",
      "score": 0.5296297669410706
    },
    {
      "name": "Language understanding",
      "score": 0.5164518356323242
    },
    {
      "name": "Speech recognition",
      "score": 0.3689038157463074
    },
    {
      "name": "Linguistics",
      "score": 0.28286099433898926
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}