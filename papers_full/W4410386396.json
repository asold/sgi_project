{
  "title": "Large language models in oncology: a review",
  "url": "https://openalex.org/W4410386396",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2102071759",
      "name": "David Chen",
      "affiliations": [
        "University of Toronto",
        "Princess Margaret Cancer Centre"
      ]
    },
    {
      "id": "https://openalex.org/A5075414597",
      "name": "Rod Parsa",
      "affiliations": [
        "Princess Margaret Cancer Centre",
        "McMaster University"
      ]
    },
    {
      "id": "https://openalex.org/A2196946318",
      "name": "Karl Swanson",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A4318222423",
      "name": "John Jose Nunez",
      "affiliations": [
        "BC Cancer Agency",
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A2555178028",
      "name": "Andrew Critch",
      "affiliations": [
        "Machine Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2089952979",
      "name": "Danielle S Bitterman",
      "affiliations": [
        "Boston Children's Hospital",
        "Harvard University",
        "Mass General Brigham",
        "Dana-Farber Cancer Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2563214622",
      "name": "Fei-Fei Liu",
      "affiliations": [
        "University of Toronto",
        "Princess Margaret Cancer Centre"
      ]
    },
    {
      "id": "https://openalex.org/A2173471273",
      "name": "Srinivas Raman",
      "affiliations": [
        "Spinal Cord Injury BC",
        "Princess Margaret Cancer Centre",
        "University of Toronto",
        "BC Cancer Agency"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4383737134",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3141847762",
    "https://openalex.org/W2345195116",
    "https://openalex.org/W4404173762",
    "https://openalex.org/W4367595583",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W6872166091",
    "https://openalex.org/W4397003633",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4394867807",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4393867901",
    "https://openalex.org/W4399774223",
    "https://openalex.org/W4405992235",
    "https://openalex.org/W4402909345",
    "https://openalex.org/W4391258615",
    "https://openalex.org/W4392407790",
    "https://openalex.org/W4391112015",
    "https://openalex.org/W4391653030",
    "https://openalex.org/W4388931647",
    "https://openalex.org/W4401351347",
    "https://openalex.org/W4400202790",
    "https://openalex.org/W4401752707",
    "https://openalex.org/W2956228567",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W4307968550",
    "https://openalex.org/W4393524206",
    "https://openalex.org/W4391175984",
    "https://openalex.org/W4389780040",
    "https://openalex.org/W4386117070",
    "https://openalex.org/W4392285697",
    "https://openalex.org/W4395002012",
    "https://openalex.org/W4399153193",
    "https://openalex.org/W4378783467",
    "https://openalex.org/W4388033695",
    "https://openalex.org/W4403681525",
    "https://openalex.org/W4388759569",
    "https://openalex.org/W4365511237",
    "https://openalex.org/W4390745503",
    "https://openalex.org/W4285006978",
    "https://openalex.org/W4288686885",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4396977150",
    "https://openalex.org/W4386117408",
    "https://openalex.org/W4395678627",
    "https://openalex.org/W4380423243",
    "https://openalex.org/W4384944902",
    "https://openalex.org/W4381611207",
    "https://openalex.org/W4399365598",
    "https://openalex.org/W4394844693",
    "https://openalex.org/W4392986561",
    "https://openalex.org/W4388792797",
    "https://openalex.org/W4390722831",
    "https://openalex.org/W4403028548",
    "https://openalex.org/W4380575774",
    "https://openalex.org/W4400307914",
    "https://openalex.org/W4402545529",
    "https://openalex.org/W4378214315",
    "https://openalex.org/W4399281044",
    "https://openalex.org/W4385682300",
    "https://openalex.org/W2142149055",
    "https://openalex.org/W4400571418",
    "https://openalex.org/W4399206216",
    "https://openalex.org/W4366835631",
    "https://openalex.org/W4393054030",
    "https://openalex.org/W4392599487",
    "https://openalex.org/W4395026179",
    "https://openalex.org/W6862537813",
    "https://openalex.org/W4388115165",
    "https://openalex.org/W4391362878",
    "https://openalex.org/W4387821331",
    "https://openalex.org/W4407965332",
    "https://openalex.org/W4393122385",
    "https://openalex.org/W4404485589",
    "https://openalex.org/W4404354416",
    "https://openalex.org/W4403983018",
    "https://openalex.org/W3001762024",
    "https://openalex.org/W4389991792",
    "https://openalex.org/W4390940921",
    "https://openalex.org/W4404900783",
    "https://openalex.org/W6851736839",
    "https://openalex.org/W4210670486",
    "https://openalex.org/W2954907832",
    "https://openalex.org/W4400974494",
    "https://openalex.org/W3136552952",
    "https://openalex.org/W4386907175",
    "https://openalex.org/W4392100955",
    "https://openalex.org/W4402906848",
    "https://openalex.org/W4366683734"
  ],
  "abstract": "Large language models (LLMs) have demonstrated emergent human-like capabilities in natural language processing, leading to enthusiasm about their integration in healthcare environments. In oncology, where synthesising complex, multimodal data is essential, LLMs offer a promising avenue for supporting clinical decision-making, enhancing patient care, and accelerating research. This narrative review aims to highlight the current state of LLMs in medicine; applications of LLMs in oncology for clinicians, patients, and translational research; and future research directions. Clinician-facing LLMs enable clinical decision support and enable automated data extraction from electronic health records and literature to inform decision-making. Patient-facing LLMs offer the potential for disseminating accessible cancer information and psychosocial support. However, LLMs face limitations that must be addressed before clinical adoption, including risks of hallucinations, poor generalisation, ethical concerns, and scope integration. We propose the incorporation of LLMs within compound artificial intelligence systems to facilitate adoption and efficiency in oncology. This narrative review serves as a non-technical primer for clinicians to understand, evaluate, and participate as active users who can inform the design and iterative improvement of LLM technologies deployed in oncology settings. While LLMs are not intended to replace oncologists, they can serve as powerful tools to augment clinical expertise and patient-centred care, reinforcing their role as a valuable adjunct in the evolving landscape of oncology.",
  "full_text": "1\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nOpen access \nLarge language models in oncology: \na review\nDavid Chen,1,2 Rod Parsa,1,3 Karl Swanson,4 John- Jose Nunez,5,6 Andrew Critch,7 \nDanielle S Bitterman,8,9,10 Fei- Fei Liu,11,12 Srinivas Raman    1,2,13\nTo cite: Chen D, Parsa R, \nSwanson K, et al . Large \nlanguage models in oncology: \na review. BMJ Oncology \n2025;4:e000759. doi:10.1136/\nbmjonc-2025-000759\n ► Additional supplemental \nmaterial is published online only. \nTo view, please visit the journal \nonline (https:// doi. org/ 10. 1136/ \nbmjonc- 2025- 000759).\nReceived 23 January 2025\nAccepted 24 April 2025\nFor numbered affiliations see \nend of article.\nCorrespondence to\nDr Srinivas Raman;  \n srinivas. raman@ bccancer. bc. ca\nReview\n© Author(s) (or their \nemployer(s)) 2025. Re- use \npermitted under CC BY- NC. No \ncommercial re- use. See rights \nand permissions. Published by \nBMJ Group.\nABSTRACT\nLarge language models (LLMs) have demonstrated \nemergent human- like capabilities in natural language \nprocessing, leading to enthusiasm about their integration \nin healthcare environments. In oncology, where \nsynthesising complex, multimodal data is essential, LLMs \noffer a promising avenue for supporting clinical decision- \nmaking, enhancing patient care, and accelerating research. \nThis narrative review aims to highlight the current state \nof LLMs in medicine; applications of LLMs in oncology for \nclinicians, patients, and translational research; and future \nresearch directions. Clinician- facing LLMs enable clinical \ndecision support and enable automated data extraction \nfrom electronic health records and literature to inform \ndecision- making. Patient- facing LLMs offer the potential \nfor disseminating accessible cancer information and \npsychosocial support. However, LLMs face limitations that \nmust be addressed before clinical adoption, including risks \nof hallucinations, poor generalisation, ethical concerns, \nand scope integration. We propose the incorporation of \nLLMs within compound artificial intelligence systems to \nfacilitate adoption and efficiency in oncology. This narrative \nreview serves as a non- technical primer for clinicians to \nunderstand, evaluate, and participate as active users who \ncan inform the design and iterative improvement of LLM \ntechnologies deployed in oncology settings. While LLMs \nare not intended to replace oncologists, they can serve as \npowerful tools to augment clinical expertise and patient- \ncentred care, reinforcing their role as a valuable adjunct in \nthe evolving landscape of oncology.\nCURRENT STATE OF LARGE LANGUAGE MODELS IN \nMEDICINE\nIntroduction\nLarge language models (LLMs) are artifi-\ncial intelligence (AI) systems focused on the \ngeneration of natural language. The field of \noncology is well- positioned to benefit from the \nincorporation of LLM technologies, especially \ngiven its emphasis on the synthesis of diverse \ndata types such as clinical, imaging, labora-\ntory and genomic reports integrated with \nthe psychosocial elements of patient- centred \nmedicine. This narrative clinical primer aims \nto provide a background for the application of \nLLMs in cancer care and lay the groundwork \nfor their adoption in clinical oncology. Our \nnarrative review adopts a practical approach \nby offering step- by- step examples of LLM tool \nintegration into clinical oncology workflows \nas well as discussion of contemporary trends \nincluding compound AI systems with human- \nin- the- loop designs, multi- modal LLMs, and \nemergent regulatory frameworks to fill the \ngap between conceptual overviews and clin-\nical realities. We identified relevant studies \nand potential applications by conducting a \ncomprehensive but non- systematic search of \nacademic databases (PubMed, MEDLINE, \nEMBASE, Google Scholar) using variations \nof the keywords ‘large language model’, \n‘generative artificial intelligence’, ‘oncology’ \nand ‘cancer’. We also cross- referenced bibli-\nographies of retrieved articles and drew on \nexpert clinical and AI knowledge within the \nauthor team to ensure coverage of emerging \nand notable studies within the scope of large \nlanguage model development and applica-\ntions in oncology.\nHistory of NLP and LLMs\nNatural language processing (NLP) describes \nthe computer- aided analysis that enables \ncomprehension and generation of human \nlanguage. In the early 2000s, the first iter -\nation of language generators employed \nstatistical models that estimated the likeli-\nhood of the next word in a sequence, based \non frequency of occurrence in the training \ndata.1 To leverage the scale of large natural \nlanguage datasets such as unstructured \ntext in electronic health records (EHRs), \nmachine learning- based NLP approaches \nused mathematical models to extract high- \nlevel patterns from data to make inferences. \nThe evolution of natural language processing \nfrom rule- based algorithms to contemporary \nlarge language models is shown as a timeline \nin figure 1.\nDeep learning, inspired by biological neural \nnetworks, refers to a sub- field of machine \nlearning models that learn high- level patterns \nfrom input data to mimic human- like data \nprocessing. Transformer- based LLMs, such as \nBERT2 and RoBERTa3 in the late 2010s as well \nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n2\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview Open access\nas GPT in 2020, 4 were critical developments in NLP that \nexhibited human- like performance in sentiment analysis, \nquestion answering, feature extraction, language trans-\nlation and text summarisation. In clinical contexts, this \nability to ingest significant volumes of textual information \nwas first applied to electronic medical records (EMRs); \nLLMs’ ability to process and analyse free text elements \nenabled clinicians to rapidly create structured datasets, \nstreamline clinical coding, mine academic literature and \ndevelop predictive models based on extractable patient \nfeatures.5 Oncology- specific applications of NLP models \nfocused initially on case identification (ie, identifying \npast patients with specific or rare medical characteristics \nnot captured by structured data fields), enhanced cancer \nstaging and codification of staging parameters and the \nidentification of specific clinical outcomes. 6 As conversa-\ntional capabilities improved, NLP research in oncology \nturned towards patient- facing applications, including \ncancer screening campaigns and patient education after \ndiagnosis.7 The significance of these advances in NLP, \nand their contribution to human advancement, was most \nrecently underscored by the Nobel Committee’s decision \nto award the 2024 Nobel Prize in Physics to Drs Hopfield \nand Hinton, pioneers in the development of the artificial \nneural networks employed in modern LLMs.\nLLMs recently entered the public consciousness with \nOpenAI’s 2022 release of ChatGPT 3.5, an AI chatbot \nbased on the GPT LLM that is credited as the fastest- \ngrowing consumer software application in history.8 Today, \nthere are three common categories of clinical LLMs: (1) \nzero- shot, generalist LLMs that can perform diverse NLP \ntasks with no pre- training, (2) fine- tuned LLMs that have \nbeen trained on custom medical datasets to perform \nspecialised tasks and (3) LLMs equipped with in- context \nlearning or retrieval- augmented generation techniques \nto enable more accurate recall of medical information \nfrom a knowledge base.\nDevelopment and validation of clinical LLMs\nStandard workflows for the development of clinical \nLLMs involve model selection, fine- tuning for domain- \nspecific tasks, validation in clinical contexts and deploy-\nment in real- world settings as summarised in figure 2. \nFirst, clinical LLM applications are typically built using \nfoundational LLMs developed by academic and industry \ndevelopers, including Google (Gemini), Meta (Llama), \nOpenAI (GPT) and Anthropic (Claude). 9 Some of the \nmost popular LLMs are open- source (eg, Llama2 by \nMeta), allowing users to modify their underlying architec-\nture, while others remain proprietary and allow limited \ninteractions through an application programming inter -\nface (eg, GPT- 4 by OpenAI).\nSecond, domain- specific LLMs are developed through \ntransfer learning, where foundational LLMs are fine- \ntuned or trained on specialised datasets, such as clinical \nnotes and EHRs, to function for specialised tasks. Exam-\nples of clinical- focused LLMs include Google’s Med- \nPaLM, which focuses on providing high- quality responses \nto medical questions,10 and ClinicalBERT, a model which \npredicts hospital readmission within 30 days by analysing \nclinical text.11 Furthermore, a recent study by Wang et al \ndemonstrated superior performance of fine- tuned LLMs \nin the field of radiation oncology, where they outper -\nformed baseline foundational LLMs on tasks related to \ntreatment regimen generation, treatment modality selec-\ntion and ICD- 10 code generation.12 Likewise, Ferber et al \ndemonstrated the superior performance of fine- tuned \nLLMs compared with baseline foundational LLMs in \nthe field of medical oncology when assessed on manage-\nment guidelines of pancreatic, colorectal and hepatocel-\nlular cancers.13 LLM outputs can be further optimised by \nproviding model outputs with a few examples through \nfew- shot learning as well as rewarded to steer LLMs \ntowards more truthful and less toxic outputs based on \nhuman feedback through reinforcement learning.14\nFigure 1 Timeline of Natural language processing development in medicine.\nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n3\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview\nOpen access\nThird, LLMs are commonly internally validated \nthrough one of three classes of metrics on a human-  \nannotated benchmark dataset, including multiple-  \nclassification (classification of text into multiple \ngroups), token- similarity (similarity of generated text \nwith reference text) and question- answering (identi-\nfying the answer to a specific question). 15 However, \nthese methods do not capture real- world clinical effi-\ncacy, and external validation against expert oncologist \ndecision- making remains essential. Extrinsic evalua-\ntions of LLM performance have included compari-\nsons against trained healthcare professionals across \ntest scores or standard of care, measures of clinical \nefficiency or subjective ratings of performance, 16 \nwith recent recommendations that models be stress-  \ntested via exposure. to diverse clinical scenarios and \npatient populations to ensure generalisability before \nreal- world deployment. 17 Indeed, a recent cross-  \nsectional study analysing eight LLMs demonstrated \nan 85% accuracy rate on examination- style multiple \nchoice questions from the American Society of \nOncology; however, 81.8% of the incorrect questions \nwere rated as having a medium to high likelihood \nof moderate to severe harm. 18 Another validation \nstudy focused on molecular tumour boards found \nthat LLMs offered equivalent treatment recommen-\ndations to clinicians 25% of the time, with a further \n37.5% of recommendations as plausible alternative \ntreatments. In generating these recommendations, \nhowever, 17% of articles referenced by LLMs were \nhallucinations, reinforcing the need for clinician \nsupervision. 19 Most recently, researchers have been \nworking towards standardising the human evaluation \nof LLMs in healthcare. For example, the QUEST \nframework proposed by Tam was developed through \na systematic review of prior evaluation guidelines \nand addresses gaps in reliability, generalisability and \napplicability of these guidelines across a variety of \nmedical specialties. 20\nAPPLICATIONS OF LARGE LANGUAGE MODELS IN ONCOLOGY\nClinician decision support\nEmergent themes of clinician- facing applications of LLMs \nin oncology include serving as clinical decision support \ntools for diagnosis, screening and prevention, treatment \nand management, and automated data extraction and \nprocessing for clinician review. 9 Common themes and \nhighlighted examples of potential LLM applications in \noncology across a diverse set of application scenarios \nare shown in figure 3 and online supplemental eTable 1, \nrespectively. In figure 3, we outline three principal themes \nillustrating how LLMs are being applied in oncology: (1) \nclinician- facing applications for diagnosis, prognosis, \nscreening, management, data extraction and reference \ninformation; (2) patient- facing applications for psycho-\nsocial support, communication and reference informa-\ntion; and (3) research facilitation such as screening of \ntrial eligibility, literature evidence synthesis and precision \nmedicine. Notably, this figure highlights the breadth of \npotential LLM applications grouped by distinct applica-\ntion domains that can help address the multifaceted chal-\nlenges in modern oncology care.\nDiagnosis\nLLM- enabled tools have the potential to identify useful \npatterns from both text- only and multi- modal inputs to \nrecommend clinical diagnoses. In an early evaluation \nof LLM diagnostic accuracy, Wang et al found GPT- 4 \nperformed well in generated report structure and clarity \nbut performed worse than physicians in diagnostic accu-\nracy21 when tested on 109 ultrasound text descriptions of \nthyroid cancer. However, the Turing test evaluation found \nthat physicians believed that 71% of GPT- generated \nreports were likely physician- generated, suggesting that \nGPT and physician- generated reports are largely indis-\ntinguishable. Compared with gold- standard clinician \nannotations, LLMs have shown promising diagnostic \nperformance in exam- style, text- based assessments across \nseveral tumour sites including dermatological (85% accu-\nracy),22 bone (87% accuracy), 23 oropharyngeal (71% \nFigure 2 Development and validation process of clinical large language model applications.\nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n4\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview Open access\naccuracy)24 and neurological cancers (50% accuracy). 25 \nNotably, the advent of multi- modal LLMs that integrate \nimage processing capabilities is uniquely positioned to \nanalyse both clinical images, including photographs, 22 \nand multiple radiologic modalities including MRI, 26 \nCT27 and ultrasound. 28 While most studies demonstrate \nhuman- like performance, there remain concerns about \ngeneralisation such as variable diagnostic performance \nacross different skin tones in melanoma, 22 suggesting \nthat the accuracy in minority patient demographics and \nrare diseases should be evaluated with caution. Similar \nconcerns arise in underrepresented cancer subtypes, \nwhere training data scarcity may lead to decreased model \nperformance. Oncologists can use LLM predictions to aid \ndiagnosis but should be cautious about their interpret-\nability and alignment with clinical judgement. However, it \nis important to note that these findings are derived from \nearly- stage pilot studies in controlled settings and require \nfurther validation, especially to assess performance consis-\ntency across diverse patient populations, heterogenous \npractice settings and various cancer diagnoses and stages.\nExisting AI- driven approaches in radiology and histo-\npathology have demonstrated high diagnostic accuracy \nfor tasks like tumour localisation and malignancy classi-\nfication, often through convolutional neural networks or \nother deep learning architectures that directly analyse \nimaging data. 29 LLM- based solutions can complement \nthese image- centric models by parsing clinical notes, \nradiology reports or pathology descriptors, providing \na structured synopsis of relevant clinical factors that \ncan enhance diagnostic workflows and communication \nbetween radiologists, pathologists and oncologists (60). \nIn this way, LLMs may ultimately function synergistically \nwith established computer vision algorithms to bridge the \ngap between raw imaging data and patient- level clinical \ndecision- making (61). However, multi- modal LLMs are \nstill in early development, and rigorous testing is needed \nto evaluate their ability to integrate imaging and text data \nreliably across diverse patient populations and clinical \nsettings.\nScreening and prevention\nThe integration of LLMs into screening and prevention \nefforts represents a nascent but rapidly evolving area of \nresearch, focusing on text- based knowledge synthesis and \nguideline- based recommendation.30 While most AI- based \nscreening applications to date have emphasised image \nanalysis for earlier detection of lesions or nodules, LLMs \noffer potential in complementary domains like patient risk \nstratification from EHRs, automated reminder systems for \nat- risk populations and the generation of patient- specific \npreventative measures. 31 These text- driven functional-\nities could be especially valuable for oncologists seeking \nto optimise large- scale screening programmes or adapt \nguidelines to individual patient risk profiles.\nFigure 3 Themes of large language model applications in oncology.\nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n5\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview\nOpen access\nThe utility of LLMs in augmenting decisions related \nto screening and prevention has been investigated in \nprostate (85% and 100% accuracy for 30 easy and hard \nprostate cancer screening questions),32 colorectal (100% \naccuracy in 20 colorectal cancer screening questions), 33 \nbreast, ovarian and lung cancer (83% accuracy across \n15 select- all- that- apply pan- cancer screening scenarios) \ncontexts.34 Although these standardised screening evalu-\nations show high accuracy, they are based on predefined \nquestion sets rather than real- world clinical scenarios. \nChiarelli et al tested the reliability of GPT when queried \nwith three prompt variations, showing that there was no \ndifference in accuracy but noted that systematic evalua-\ntions of reliability are warranted given the probabilistic \nnature of LLMs. 32 Despite attempts to ground LLM in \nevidence- based knowledge such as PubMedBERT and \nMed- PaLM, oncologists should validate that LLM- based \nscreening recommendations coupled with explanations \nalign with established clinical guidelines and generalise \nwhen integrated into patient screening programmes.\nTreatment and management\nWhile LLMs can suggest treatment options aligned with \nestablished guidelines, studies have found that they may \nalso propose non- concordant treatments, requiring \nhuman oversight to align management with patient pref-\nerences and evidence- based guidelines. 35 Marchi et al \nfound that ChatGPT- 3.5 provided accurate suggestions \nfor primary treatment (85.3% accuracy, 100% sensitivity) \nand adjuvant treatment (96% accuracy, 100% sensitivity) \nfor 68 head and neck cancer cases according to NCCN \nconsensus expert- driven guidelines for cancer manage-\nment.36 High sensitivity in treatment recommendations \nunderscores the comprehensive nature of LLM outputs \nbut may lead to over- inclusive lists that require oncologist \njudgement to refine. For example, in another study, Chen \net al found that across 104 prompts for 26 pan- cancer \ndiagnoses, GPT- 3.5 provided at least 1 NCCN- concordant \ntreatment in 98% of scenarios but also recommended \nnon- concordant treatments in 34.3% of scenarios.35 Given \nthe occurrence of non- guideline- concordant recommen-\ndations, it is critical to underscore that LLM outputs \nshould complement—but not replace—human clinical \njudgement while future research continues to identify \nand address the knowledge deficiencies of LLM tools in \nclinical settings. For the oncologist, this means that LLMs \ncan generate a differential list of treatments for future \nevaluation of guideline concordance and patient prefer -\nence, but cannot replace human decision- making. LLMs \nin specialised oncology tasks have shown mixed perfor -\nmance, with examples in the literature demonstrating \nthat LLMs prescribed chemotherapy protocols with inap-\npropriate dosing (56% accuracy) 37 and were subject to \nhallucinations when recommending management for \nimmune- related adverse events (44% accuracy). 38 From \na medical lens, LLMs may fail to consider important \nfactors such as a patient’s comorbid conditions or psycho- \nsocio- economic factors that may contraindicate certain \nregimens in practice. Similarly, while an LLM might \nproduce a seemingly appropriate surgical recommenda-\ntion, only a trained surgeon or multidisciplinary tumour \nboard can balance tumour resectability, patient prefer -\nences, anatomical complexities and the associated risks \nunique to a specific patient. From a psychosocial lens in \npalliative scenarios, an LLM’s suggestions may overlook \nfamily dynamics or cultural values—factors that carry \nsubstantial weight in deciding care goals. These exam-\nples illustrate how human judgement, guided by clinical \nexpertise and empathy, remains central to comprehen-\nsive patient- centred care.\nOne of the most exciting applications of LLMs in \noncology is the recommendation of treatments in \ncomplex settings outside of established clinical practice \nguidelines. These tasks may be ideally suited for LLMs \ngiven their ability to process vast amounts of medical liter-\nature and patient data to identify patterns that may not \nbe apparent to human experts, but useful for generating \nnovel treatment recommendations in complex and rare \ncancer cases. In complex breast39 and colorectal40 cancer \nsettings, studies have reported 70% and 87% concor -\ndance of LLM- generated treatments with tumour board \nrecommendations. Likewise, Chen et al found that LLM- \ngenerated diagnosis and treatment recommendations \nof 79 clinical oncology cases with images achieved up to \n72% accuracy.41 However, inaccurate referencing to estab-\nlished guidelines and generation of medically inaccurate \noutputs with confidence contribute to poor autonomous \nactionability of LLM- generated recommendations42\nData extraction and processing\nTo enable synthesis of patient data for molecular tumour \nboards, oncologists may use LLMs to extract key tumour \nattributes rather than manually extracting this data from \nthe patient EHR.43 Preston et al demonstrated that LLM- \nbased data extraction of tumour attributes, including \ntumour site and the widely- adopted TNM cancer staging \nclassification, can achieve 94–99% AUROC performance \nand generalise across multiple health systems and state \nregistries.43 Notably, LLM- enabled data extractions for \nwell- defined categories, such as TNM stage, can even \ncorrect human errors on expert review.43\nBeyond clinical features, automated extraction of social \nand behavioural determinants from clinical data 44 can \nbe applied to address several humanistic elements of the \ncancer patient experience, including identifying at- risk \npatients who lack advance directives, surrogate decision- \nmakers and decision capacity, 45 and recommending \nonline resources to address psychosocial needs.46 Instead \nof prompting LLMs to generate ‘black box’ predictions \nfrom clinical data, oncologists can prompt LLMs to \nextract important data points from large- scale clinical \ntext, allowing oncologists to prioritise expert synthesis \nof medical knowledge and patient care over non- patient \nfacing, administrative tasks. While the high performance \nin structured data extraction is encouraging, variability \nin EHR systems and documentation practices across \nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n6\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview Open access\ninstitutions may affect extraction performance in external \nsettings, motivating the need for robust, multi- centre eval-\nuations to confirm generalisability in real- world settings.\nPatient-facing applications\nThe familiarity and accessibility of chatbot LLMs with near- \nhuman levels of language competency underscore their \npotential as patient- facing health information resources \nand supportive management tools to help address patient \neducational and psychosocial factors of cancer care.\nHealth information resource\nThe accurate performance of conversational chatbots on \nstandardised benchmarks of medical competency, such \nas the USMLE, 47 and common patient queries about \ncancer48 49  suggests that LLM applications may serve as \na readily accessible, supplementary patient resource for \ncancer information. Beyond responding to clinician- level \nqueries, cross- sectional studies of ChatGPT- 4 reported \nhigh accuracy and alignment to oncologists or guidelines \nif available, when tested on general patient- level questions \nabout genetic counselling,50 breast,51 lung,52 colon53 and \npancreatic54 cancers. Moreover, pilot evaluations of LLMs \nfor language translation55 and biomedical text simplifica-\ntion56 are emergent research directions of clinical LLMs \nthat can facilitate patient education in oncology. LLMs \ncan provide oncology knowledge as an accessible patient \nresource.\nHowever, we caution that the variable medical accuracy \nacross various cancers and topics,44 risks of misinterpreta-\ntion, oversimplification of complex medical information, \npropagation of outdated or non- personalised advice and \ndecreased readability of chatbot- generated responses \ncompared with physicians may collectively pose serious \nrisks to deploying patient- facing LLMs in real- world \nsettings until effective safeguards for accuracy and misin-\nformation are implemented. Despite the positive results of \nthese pilot studies, oncologists should explain to patients \nthat LLM tools may (1) generate unreliable and outdated \ninformation that can lead to harm, (2) fail to personalise \nrecommendations to the individual patient, (3) harbour \ninherent biases based on their training datasets and (4) \nprovide limited protections to personal health informa-\ntion privacy and security that have yet to be systematically \nregulated.\nSupportive management\nConversational LLMs, known as chatbots, may act as a \ncomplementary agent for psychosocial and emotional \nsupport in oncology. Chatbots can provide empathetic \nresponses to online patient questions about general \nmedicine16 and cancer- specific48 topics in non- inferiority \nevaluations compared with physicians, supporting their \nuse in generating empathetic template responses under \nclinician oversight when integrated into patient health \nportals.57 LLM tools also pose potential to improve \npatient communication during post- treatment care, such \nas improving dialogue rates for patients experiencing \noncological aphasia.58 Combining LLM- enabled language \ncompetencies with hardware, such as assistive robots with \nfunctional language and physical capabilities, is a prom-\nising development towards more human- like levels of \nemotional connection.59\nOncologists should establish clear protocols defining \nthe scope of chatbot use that encompass two major compo-\nnents: development- focused guidelines and patient- facing \nusage guidelines. From a development perspective, these \nprotocols can inform model creators and industry part-\nners about the clinical and ethical parameters expected \nin an oncology setting, helping to ensure that chatbot \nfeatures—such as data handling, language style and \nmanagement plans—are compatible with current stan-\ndards of care and privacy regulations. In parallel, usage \nguidelines aimed at clinicians and patients will clarify the \nchatbot’s intended purposes (eg, providing supplemen-\ntary educational information, screening for psychosocial \nsupport needs or summarising care instructions), limita-\ntions (eg, lack of personalised medical advice, potential \nfor erroneous responses) and recommended follow- up \nactions. This dual approach enables a coordinated \neffort to shape the chatbot’s capabilities during devel-\nopment while also providing straightforward guidelines \nthat support safe, consistent and beneficial interactions \nbetween patients, oncology teams and AI- based tools.\nPatient perceptions of LLMs\nRecent studies show that patients often perceive AI- gener-\nated health advice, such as from ChatGPT, as helpful and \nempathetic, with evidence that users sometimes rate these \nresponses even more favourably than physician- written \nanswers.60 However, research indicates that patients have \nonly moderately high trust in chatbot responses—Nov \net al (2023) report average trust scores of around 3.4 \nout of 5, with confidence falling as question complexity \nincreases.61 Platt et al (2024) similarly found that patients’ \ncomfort using ChatGPT for healthcare queries was below \nmid- range, highlighting accuracy and privacy concerns.62 \nA notable risk factor is that lay users cannot always iden-\ntify when an LLM’s advice is inaccurate or outdated, \nunderscoring the potential for harm if chatbots are used \nwithout adequate oversight.60 Nevertheless, public surveys \nof online users suggest that, despite these reservations, \nmembers of the public, including patients, show willing-\nness to adopt AI health tools in the future if privacy, safety \nand transparency standards are better established.63 64\nFacilitating and synthesising oncology research\nAutomated processing of unstructured text is a unique \ncompetency of LLMs that can be used to facilitate transla-\ntional research in oncology. For example, LLMs can struc-\nture clinical trial eligibility criteria for cancer patients that \nachieve moderate performance compared with physician \nrecommendations, with mixed reports of both high false \npositives65 and high false negatives. 66 Similar to medical \ntumour board recommendations, LLMs applied to clin-\nical trial recommendation should be used to generate an \nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n7\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview\nOpen access\ninclusive selection of potentially eligible trials for oncolo-\ngists to prioritise in their final recommendation.\nTranslation of oncology clinical trial results into action-\nable clinical recommendations requires expert synthesis of \nscientific knowledge prone to time lag between discovery \nand implementation. 67 To address this problem, LLM \nsystems such as SEETrials have demonstrated proficiency \n(96% specificity, 94% sensitivity) in automated extraction \nof intervention outcomes associated with cancer trials \nreported in conference abstracts, 68 enabling oncologists \nto glean early insights into the safety and efficacy of novel \ninterventions.\nApplied to precision oncology, LLMs have seen success \nin automating data extraction of driver mutations and \nclinical data from EHRs to evaluate the prognostic value \nof these mutations and functional effects.69 Likewise, liter-\nature mining by LLMs may be useful as a research tool \nfor drug synergy predictions applied to complex cancer \npatient scenarios.70 Academic oncologists can stay up to \ndate on advancements in LLM applications by engaging \nwith emerging LLM in oncology research, attending \ninterdisciplinary conferences and collaborating with AI \nexperts to safely and effectively integrate these tools into \nmodern oncology practice (table 1).\nLIMITATIONS AND FUTURE DIRECTIONS OF LARGE LANGUAGE \nMODELS\nTechnical limitations\nThe implementation of LLMs in medicine is limited both \nby AI- intrinsic and clinical workflow challenges. Training \nand testing models on sparse, incompletely labelled \ndatasets risks generating insights that fail to generalise \nto broad use cases. Furthermore, LLM- based models are \nprone to generating convincing ‘hallucinations’, content \nthat is entirely nonsensical or unfaithful to the provided \nsource content,9 which must be either actively detected \nor accounted for by healthcare providers. For example, \nChen et al reported a 12.5% hallucination rate by LLM \nchatbots asked to generate cancer treatment informa-\ntion.35 As a result, there has been increasing focus on \nthe development of LLM safeguards that prevent the \ngeneration of health disinformation. 71 Other studies \nhave found that LLM chatbots may provide inconsistent \nresponses to the same question asked several times, 72 \nraising questions about their reliability and reproduc-\nibility. Finally, most modern LLM chatbots are trained on \nfixed time windows—for example, ChatGPT 3.5’s initial \nrelease was trained on data up to September 2021. This \ntraining method may exclude more recent advances and \nrisks generating outdated responses, especially in rapidly \nevolving fields such as oncology.\nEthical limitations\nCollaborations between international institutions, as \nevidenced by a partnership between the WHO and \nthe European Parliament, have garnered interest in \nproducing ethical guidelines and frameworks for the \napplication of AI in healthcare. 73 These frameworks \nemphasise the preservation of patient autonomy, tech-\nnological transparency, accountability and inclusiveness. \nUntil international standards are formalised, significant \ndiscussion has focused on adherence to existing national \nstandards, such as the US Health Insurance Portability \nand Accountability Act (HIPAA). Models that employ \nidentifiable patient information may risk inadvertently \nstoring or disclosing sensitive information in violation of \nHIPAA regulations or may be vulnerable to cybersecurity \nbreaches.74 Furthermore, the use of identifiable patient \ninformation in the pre- training process may violate prin-\nciples around informed consent and rights- of- data, espe-\ncially if previously anonymised data can be re- identified.\nKapsali et al have highlighted discrepancies between \nthe aforementioned principles and ChatGPT’s features,75 \npointing to its black- box technology and insufficient \ndocumentation as causes for concern. Unsurprisingly, it \nTable 1 How large language models are being used in oncology\nDomain Application\nOncologist  ► Generate differential diagnoses based on patient clinical notes and data for oncologist review\n ► Prognosticate patient based on risk and survival as a supportive tool\n ► Provide cancer screening information based on established guidelines\n ► Generate treatment recommendations for oncologist review\n ► Extraction of key cancer attributes from clinical text to inform clinical decision- making\n ► Generating summaries of clinical notes, consultations and diagnostic reports\nCancer Patient  ► Patient health information resource with clinician oversight\n ► Language translation\n ► Biomedical text simplification\n ► Psychosocial and emotional support and counselling\nCancer Research  ► Clinical trial matching for eligible cancer patients\n ► Extraction of trial outcomes from literature for clinician education\n ► Extraction of mutation and clinical data for precision oncology\n ► Literature mining for drug synergies\nAdministration  ► Draft work communications and patient messages\n ► Transcribe and summarise patient and clinician meetings\n ► Generate pre- filled administrative paperwork\n ► Copy- edit and format administrative paperwork\nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n8\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview Open access\nhas been shown that patients may prefer human judge-\nment and expertise over AI- generated recommenda-\ntions,76 especially when existing legal frameworks around \nliability and medical malpractice fail to fully address \nAI- driven outcomes.77 Indeed, emerging research has \nrevealed the potential for LLMs to perpetuate societal \nbiases, such as race, even without the explicit input of \ndemographic data. 78 A recent systematic review high-\nlighted the prevalence of gender and racial bias in \nmedical LLMs, describing model outputs that leaned \non stereotypical gender roles, used gendered language \nand underrepresented women while overvaluing male \ncompetence. Mitigation strategies that limit these biases \nhave relied primarily on prompt engineering methods \nwith varying effectiveness, and there exists a need for \nstandardised metrics that systematically reduce bias in \nall stages of model development and implementation. 79 \nGiven the historic and ongoing issues with diversity, as \nevidenced by clinical trial participation for example, 80 \nthere exists an imperative for cancer researchers to inter-\nrogate oncology- focused LLMs for data- driven biases.\nResource limitations\nAlthough the economic challenges of LLM deployment \nin healthcare systems remain underexplored, efforts have \nbeen made to estimate the computational, energy and \nfinancial costs associated with model development and \nimplementation.81 The cost burden of LLMs in medicine \nis based on model training and fine- tuning, integration \ninto existing electronic health systems, input data types \nand latency requirements. Carbon footprint estimates \nhave been inferred at each lifecycle point of LLMs—\npretraining, fine- tuning, and inference—with the infer -\nence stage dominating the long- term environmental \nimpact.82 For example, a single query of a fine- tuned \nGPT- 3 model uses 0.04 kW- h of electricity per 100 pages \nof generated text, 82 a power consumption that could \nrapidly balloon when scaled across thousands of patients \nthat each warrant numerous clinician queries to medical \nrecords. However, there have been countervailing debates \non the economic implications of LLM deployment, with \nsome researchers proposing that the expected cost effi-\nciencies and sustainable practices conferred by automa-\ntion far outweigh the negative impacts. 83 Cancer care in \nparticular is likely to benefit from these efficiencies, given \nthe longstanding capacity constraints as the number of \ncancer patients outpaces the number of clinicians avail-\nable to support them.84\nFuture directions\nEmerging directions in LLM implementation will involve \nadvances in technology and model complexity, cohe-\nsive regulatory and standardisation frameworks, greater \nemphasis placed on inclusivity and equity and the incor -\nporation of clinician and patient feedback into develop-\nment cycles to better align model outputs with desired \noutcomes (table 2).\nRecent research has proposed a paradigm shift from \nincreasing resource usage towards designing specialised \ncomponent tools that work together as a compound AI \nsystem.85 Roadmaps for the design of compound AI \nsystems in oncology can be informed by previous system \ndesigns used for chemical synthesis 86 and geometry \ntheorem proofs. 87 There exists additional potential for \nmultimodal AI models that integrate oncology- focused \nmodels in collaboration with other disciplines, such \nas radiology and pathology, with potential to stream-\nline tasks such as summarisation, patient education, \ndifferential diagnosis generation and interdisciplinary \ncollaboration. 88 In the era of precision medicine, the \nintegration of multimodal datasets which combine \ntextual data from medical records, oncology clinic \nvisits, multidisciplinary discussions, genomic pathology \nreports and imaging findings is likely to enhance patient- \nspecific recommendations. Fine- tuning techniques \nsuch as prompt engineering have also shown partic-\nular promise; prompts that provide additional clinical \ncontext have been shown to generate treatment plans \nTable 2 Limitations to large language model adoption in oncology and potential solutions\nLimitations Potential solutions\nTechnical  ► Comprehensive data labelling requirements that employ diverse clinical and patient data\n ► Continually shifting training window that captures new studies and advancements as they are released\n ► LLM safeguards that detect and prevent hallucinations and health disinformation\n ► Development of AI reliability metrics to track output consistency\nEthical  ► Development of consensus ethical frameworks around the use of AI in clinical contexts\n ► Inclusion of both patient and clinician feedback on a continual basis, both into regulatory frameworks and model \ndevelopment\n ► Adoption of open- source and transparent development, along with clear documentation, to avoid the perception of a \nblack- box technology\n ► Continual research and benchmarking of societal biases found in LLM inputs and outputs\n ► Anonymisation of all patient information by LLMs to preserve privacy and security\nEconomic  ► Careful consideration of build vs buy options for institutions considering LLM deployment\n ► Investment into sustainable energy options that fuel LLM energy consumption while minimising carbon footprint\n ► Judicious use of LLM model queries, limited to use cases where it improves clinical outcomes\n ► Comprehensive accounting of the cost efficiencies conferred by LLM deployment (eg, human resources)\nAI, artificial intelligence; LLM, Large language model.\nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n9\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview\nOpen access\nin concordance with cancer care guidelines. 89 From \na model evaluation perspective, some are proposing \nmore realistic evaluation frameworks using agent- based \nmodelling to create AI structured clinical examinations \n(‘AI- SCE’) that test varying degrees of self- governance \nin dynamic environments. 17\nDeployment of AI in healthcare settings has also engen-\ndered ongoing discussion around maximising benefit and \nminimising risk through standardised regulation. While \ninternationally recognised governance mechanisms \nfor AI in healthcare do not currently exist, 90 there has \nbeen increasing consensus in focused areas of interest. \nFor instance, a diverse set of academic, industry, funding \nagency and publishers has proposed the implementa-\ntion of Findable, Accessible, Interoperable and Reusable \nData Principles to define good data stewardship practices \nand facilitate data sharing that may be adopted in the \nprecision oncology community.91 Standardised reporting \nguidelines for biomedical- focused LLM research, such as \nTRIPOD+LLM for primary research involving LLMs, 92 \nQUEST for human evaluation of LLMs20 or CONSORT- AI \nfor clinical trials involving AI,93 aim to improve the consis-\ntency, reliability and verifiability of future advancements.\nPatient- centric regulations for patient privacy, medical \nmalpractice and informed consent lag behind technical \ninnovation. To date, this has only been addressed within \nthe confines of individual partnerships (eg, Google’s \nHIPAA- compliant generative AI at the Mayo Clinic) and \nnot at scale. With time, the adoption of widespread data \nsharing and ethics frameworks will permit existing models \nto train on large, open- source and more representative \ndatasets while considering important principles of data \nprivacy and right to use, intellectual property and risk of \nharm. This will in turn enable the development of accu-\nrate, purpose- built LLMs for cancer- specific applications, \nboth via open- source collaborations (eg, RadOnc- GPT, \nCancerGPT)70 94  and industry- sponsored offerings (eg, \nCareIntellect by GE Healthcare, Watson for Oncology by \nIBM and Intellispace Oncology by Phillips).\nThe design of human- in- the- loop training cycles, where \nLLMs are fine- tuned by engineers with clinician feed-\nback, can optimise LLM outputs that are more clinically \nuseful to the oncology care team. Explicit and implicit \npatient feedback may help LLMs better align outputs with \nthe unique psychosocial experiences of cancer patients. \nExplicit feedback involves reports from patient users after \ninteractions with the LLM application, such as numer -\nical scores or binary ratings of the text output from the \ntool. Implicit feedback involves indirect reports from \npatient users based on user interactions and behaviour \npatterns with the LLM application, such as monitoring \nuser reactions to LLM outputs through engagement \ntime or characteristics of follow- up queries. The design \nof oncology LLM applications requires consideration \nof both emotional and cognitive empathy in order to \naddress the psychosocial demands of the cancer patient \nexperience and prioritise the clinical competencies that \nimpact patient clinical outcomes.\nCONCLUSION\nLLMs have the potential to impact all aspects of cancer \ncare due to their human- like ability to understand and \ngenerate natural language. Clinician and patient- facing \napplications of LLMs in oncology, ranging from diagnosis, \nmanagement and emotional support, serve as promising \ndirections of LLM research in oncology. Coupled with \nemergent multi- modal capabilities and integration into \ncompound AI systems, state- of- the- art LLM applications \nare well- positioned to move towards addressing clin-\nical and translational research challenges in oncology. \nHowever, there remain several limitations of LLM deploy-\nment in clinical practice, including medical accuracy, \nprivacy and ethics, which remain to be systematically \naddressed in order to facilitate their widespread adop-\ntion. Validation of LLM applications should demonstrate \nsufficient benefit in real- world clinical settings necessary \nto prioritise patient care outcomes in oncology. While \nthe mixed performance of LLMs across oncology- related \ncompetencies may suggest that oncologists will not be \nreplaced by AI solutions anytime soon, LLM- based tools \nmay serve as useful clinician decision support and patient- \nfacing management tools under clinician oversight.\nAuthor affiliations\n1Radiation Medicine Program, Princess Margaret Hospital Cancer Centre, Toronto, \nOntario, Canada\n2Temerty Faculty of Medicine, University of Toronto, Toronto, Ontario, Canada\n3Michael G. DeGroote School of Medicine, McMaster University, Stockholm, Ontario, \nCanada\n4Department of Medicine, University of California–San Francisco, San Francisco, \nCalifornia, USA\n5Department of Psychiatry, University of British Columbia, Vancouver, British \nColumbia, Canada\n6BC Cancer Agency, Vancouver, British Columbia, Canada\n7Center for Human- Compatible Artificial Intelligence, Department of Electrical \nEngineering and Computer Sciences, UC Berkeley, Berkeley, California, USA\n8Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, Harvard \nMedical School, Boston, Massachusetts, USA\n9Department of Radiation Oncology, Dana- Farber Cancer Institute, Boston, \nMassachusetts, USA\n10Computational Health Informatics Program, Boston Children’s Hospital, Boston, \nMassachusetts, USA\n11Radiation Medicine Program, Princess Margaret Hospital, Toronto, Ontario, Canada\n12Department of Radiation Oncology, University of Toronto, Toronto, Ontario, Canada\n13Radiation Oncology, BC Cancer - Vancouver, Vancouver, British Columbia, Canada\nContributors DC, RP and SR conceptualised, analysed the data, interpreted the \ndata, drafted the manuscript and edited the manuscript. KS, JN, DB and FL edited \nthe manuscript. DC and SR are the guarantors of the manuscript.\nFunding This work was partially supported by the CARO CROF studentship and the \nRobert L. Tundermann and Christine E. Couturier philanthropic funds.\nCompeting interests None declared.\nPatient and public involvement Patients and/or the public were not involved in \nthe design, conduct, reporting or dissemination plans of this research.\nPatient consent for publication Not applicable.\nEthics approval Not applicable.\nProvenance and peer review Not commissioned; externally peer reviewed.\nData availability statement Data are available upon reasonable request.\nSupplemental material This content has been supplied by the author(s). It has \nnot been vetted by BMJ Publishing Group Limited (BMJ) and may not have been \nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n10\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview Open access\npeer- reviewed. Any opinions or recommendations discussed are solely those \nof the author(s) and are not endorsed by BMJ. BMJ disclaims all liability and \nresponsibility arising from any reliance placed on the content. Where the content \nincludes any translated material, BMJ does not warrant the accuracy and reliability \nof the translations (including but not limited to local regulations, clinical guidelines, \nterminology, drug names and drug dosages), and is not responsible for any error \nand/or omissions arising from translation and adaptation or otherwise.\nOpen access This is an open access article distributed in accordance with the \nCreative Commons Attribution Non Commercial (CC BY- NC 4.0) license, which \npermits others to distribute, remix, adapt, build upon this work non- commercially, \nand license their derivative works on different terms, provided the original work is \nproperly cited, appropriate credit is given, any changes made indicated, and the use \nis non- commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.\nORCID iD\nSrinivas Raman http://orcid.org/0000-0001-5688-9628\nREFERENCES\n 1 Hadi MU, tashi qasem al, Qureshi R, et al. A survey on large \nlanguage models: applications, challenges, limitations, and practical \nusage. [Preprint] 2023. \n 2 Devlin J, Chang MW, Lee K, et al. BERT: pre- training of deep \nbidirectional transformers for language understanding. [Preprint] \n2019. Available: http://arxiv.org/abs/1810.04805\n 3 Liu Y , Ott M, Goyal N, et al. RoBERTa: a robustly optimized bert \npretraining approach. [Preprint] 2019. Available: http://arxiv.org/abs/ \n1907.11692\n 4 Brown TB, Mann B, Ryder N, et al. Language models are few- shot \nlearners. [Preprint] 2020. Available: http://arxiv.org/abs/2005.14165\n 5 Locke S, Bashall A, Al- Adely S, et al. Natural language processing \nin medicine: A review. Trends in Anaesthesia and Critical Care \n2021;38:4–9. \n 6 Yim W- W, Yetisgen M, Harris WP , et al. Natural language processing \nin oncology: a review. JAMA Oncol 2016;2:797–804. \n 7 Raynaud C, Wu D, Levy J, et al. Patients facing large language \nmodels in oncology: a narrative review. JCO Clin Cancer Inform \n2024;8:e2400149. \n 8 Wu T, He S, Liu J, et al. A brief overview of chatgpt: the history, \nstatus quo and potential future development. IEEE/CAA J Autom \nSinica 2023;10:1122–36. \n 9 Thirunavukarasu AJ, Ting DSJ, Elangovan K, et al. Large language \nmodels in medicine. Nat Med 2023;29:1930–40. \n 10 Singhal K, Azizi S, Tu T, et al. Large language models encode clinical \nknowledge. Nature 2023;620:172–80. \n 11 Huang K, Altosaar J, Ranganath R. Clinicalbert: modeling clinical \nnotes and predicting hospital readmission. [Preprint] 2020. \n 12 Wang P , Liu Z, Li Y , et al. Fine- Tuning Large Language Models for \nRadiation Oncology, A Specialized Health Care Domain. Int J Radiat \nOncol Biol Phy 2024;120:e664. \n 13 Ferber D, Wiest IC, Wölflein G, et al. GPT- 4 for Information retrieval \nand comparison of medical oncology guidelines. NEJM AI 2024;1. \n 14 Ouyang L, Wu J, Jiang X, et al. Training language models to follow \ninstructions with human feedback. [Preprint] 2022. \n 15 Hu T, Zhou XH. Unveiling llm evaluation focused on metrics: \nchallenges and solutions. [Preprint] 2024. Available: http://arxiv.org/ \nabs/2404.09135\n 16 Ayers JW, Poliak A, Dredze M, et al. Comparing physician and \nartificial intelligence chatbot responses to patient questions posted \nto a public social media forum. JAMA Intern Med 2023;183:589–96. \n 17 Mehandru N, Miao BY , Almaraz ER, et al. Evaluating large language \nmodels as agents in the clinic. NPJ Digit Med 2024;7:84. \n 18 Longwell JB, Hirsch I, Binder F , et al. Performance of large language \nmodels on medical oncology examination questions. JAMA Netw \nOpen 2024;7:e2417641. \n 19 Berman E, Sundberg Malek H, Bitzer M, et al. Retrieval \naugmented therapy suggestion for molecular tumor boards: \nalgorithmic development and validation study. J Med Internet Res \n2025;27:e64364. \n 20 Tam TYC, Sivarajkumar S, Kapoor S, et al. A framework for human \nevaluation of large language models in healthcare derived from \nliterature review. NPJ Digit Med 2024;7:258. \n 21 Wang Z, Zhang Z, Traverso A, et al. Assessing the role of GPT- 4 \nin thyroid ultrasound diagnosis and treatment recommendations: \nenhancing interpretability with a chain of thought approach. Quant \nImaging Med Surg 2024;14:1602–15. \n 22 Cirone K, Akrout M, Abid L, et al. Assessing the utility of multimodal \nlarge language models (gpt- 4 vision and large language and vision \nassistant) in identifying melanoma across different skin tones. JMIR \nDermatol 2024;7:e55508. \n 23 Yang F , Yan D, Wang Z. Large- Scale assessment of ChatGPT’s \nperformance in benign and malignant bone tumors imaging report \ndiagnosis and its potential for clinical applications. J Bone Oncol \n2024;44:100525. \n 24 Sievert M, Aubreville M, Mueller SK, et al. Diagnosis of malignancy \nin oropharyngeal confocal laser endomicroscopy using GPT 4.0 with \nvision. Eur Arch Otorhinolaryngol 2024;281:2115–22. \n 25 Horiuchi D, Tatekawa H, Shimono T, et al. Accuracy of ChatGPT \ngenerated diagnosis from patient’s medical history and imaging \nfindings in neuroradiology cases. Neuroradiology 2024;66:73–9. \n 26 Rajendran P , Chen Y , Qiu L, et al. Auto- delineation of Treatment \nTarget Volume for Radiation Therapy Using Large Language \nModel- Aided Multimodal Learning. Int J Radiat Oncol Biol Phy \n2025;121:230–40. \n 27 Sun D, Hadjiiski L, Gormley J, et al. Outcome Prediction Using \nMulti- Modal Information: Integrating Large Language Model- \nExtracted Clinical Information and Image Analysis. Cancers (Basel) \n2024;16:2402. \n 28 Guo Y , Wan Z. Performance evaluation of multimodal large language \nmodels (LLaVA and GPT- 4- based chatGPT) in medical image \nclassification tasks. 2024 IEEE 12th International Conference on \nHealthcare Informatics (ICHI), IEEE; 541–3. Orlando, FL, USA. \n 29 Campanella G, Hanna MG, Geneslaw L, et al. Clinical- grade \ncomputational pathology using weakly supervised deep learning on \nwhole slide images. Nat Med 2019;25:1301–9. \n 30 Topol EJ. High- performance medicine: the convergence of human \nand artificial intelligence. Nat Med 2019;25:44–56. \n 31 Koh D- M, Papanikolaou N, Bick U, et al. Artificial intelligence \nand machine learning in cancer imaging. Commun Med (Lond) \n2022;2:133. \n 32 Chiarelli G, Stephens A, Finati M, et al. Adequacy of prostate \ncancer prevention and screening recommendations provided by an \nartificial intelligence- powered large language model. Int Urol Nephrol \n2024;56:2589–95. \n 33 Atarere J, Naqvi H, Haas C, et al. Applicability of Online Chat- Based \nArtificial Intelligence Models to Colorectal Cancer Screening. Dig Dis \nSci 2024;69:791–7. \n 34 Nguyen D, Swanson D, Newbury A, et al. Evaluation of ChatGPT \nand Google Bard Using Prompt Engineering in Cancer Screening \nAlgorithms. Acad Radiol 2024;31:1799–804. \n 35 Chen S, Kann BH, Foote MB, et al. Use of Artificial Intelligence \nChatbots for Cancer Treatment Information. JAMA Oncol \n2023;9:1459–62. \n 36 Marchi F , Bellini E, Iandelli A, et al. Exploring the landscape of \nAI- assisted decision- making in head and neck cancer treatment: a \ncomparative analysis of NCCN guidelines and ChatGPT responses. \nEur Arch Otorhinolaryngol 2024;281:2123–36. \n 37 Erdat EC, Yalciner M, Urun Y . n.d. Accuracy and usability of artificial \nintelligence chatbot generated chemotherapy protocols. Future \nOncol:1–6. \n 38 Burnette H, Pabani A, von Itzstein MS, et al. Use of artificial \nintelligence chatbots in clinical management of immune- related \nadverse events. J Immunother Cancer 2024;12:e008599. \n 39 Sorin V, Klang E, Sklair- Levy M, et al. Large language model \n(ChatGPT) as a support tool for breast tumor board. NPJ Breast \nCancer 2023;9:44. \n 40 Choo JM, Ryu HS, Kim JS, et al. Conversational artificial intelligence \n(chatGPTTM) in the management of complex colorectal cancer \npatients: early experience. ANZ J Surg 2024;94:356–61. \n 41 Chen D, Huang RS, Jomy J, et al. Performance of Multimodal \nArtificial Intelligence Chatbots Evaluated on Clinical Oncology Cases. \nJAMA Netw Open 2024;7:e2437711. \n 42 Benary M, Wang XD, Schmidt M, et al. Leveraging Large Language \nModels for Decision Support in Personalized Oncology. JAMA Netw \nOpen 2023;6:e2343689. \n 43 Preston S, Wei M, Rao R, et al. Toward structuring real- world data: \nDeep learning for extracting oncology information from clinical text \nwith patient- level supervision. Patterns (N Y) 2023;4:100726. \n 44 Guevara M, Chen S, Thomas S, et al. Large language models to \nidentify social determinants of health in electronic health records. \nNPJ Digit Med 2024;7:6. \n 45 Song J, Topaz M, Landau AY , et al. Using natural language \nprocessing to identify acute care patients who lack advance \ndirectives, decisional capacity, and surrogate decision makers. PLoS \nONE 2022;17:e0270220. \n 46 Leung YW, Park B, Heo R, et al. Providing Care Beyond Therapy \nSessions With a Natural Language Processing- Based Recommender \nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n11\nChen D, et al. BMJ Oncology 2025;4:e000759. doi:10.1136/bmjonc-2025-000759\nReview\nOpen access\nSystem That Identifies Cancer Patients Who Experience \nPsychosocial Challenges and Provides Self- care Support: Pilot \nStudy. JMIR Cancer 2022;8:e35893. \n 47 Gilson A, Safranek CW, Huang T, et al. How Does ChatGPT Perform \non the United States Medical Licensing Examination (USMLE)? The \nImplications of Large Language Models for Medical Education and \nKnowledge Assessment. JMIR Med Educ 2023;9:e45312. \n 48 Chen D, Parsa R, Hope A, et al. Physician and Artificial Intelligence \nChatbot Responses to Cancer Questions From Social Media. JAMA \nOncol 2024;10:956. \n 49 Pan A, Musheyev D, Bockelman D, et al. Assessment of Artificial \nIntelligence Chatbot Responses to Top Searched Queries About \nCancer. JAMA Oncol 2023;9:1437–40. \n 50 Patel JM, Hermann CE, Growdon WB, et al. ChatGPT accurately \nperforms genetic counseling for gynecologic cancers. Gynecol Oncol \n2024;183:115–9. \n 51 Liu HY , Alessandri Bonetti M, De Lorenzi F , et al. Consulting the \nDigital Doctor: Google Versus ChatGPT as Sources of Information \non Breast Implant- Associated Anaplastic Large Cell Lymphoma and \nBreast Implant Illness. Aesth Plast Surg 2024;48:590–607. \n 52 Rahsepar AA, Tavakoli N, Kim GHJ, et al. How AI Responds to \nCommon Lung Cancer Questions: ChatGPT vs Google Bard. \nRadiology 2023;307:e230922. \n 53 Emile SH, Horesh N, Freund M, et al. How appropriate are answers \nof online chat- based artificial intelligence (ChatGPT) to common \nquestions on colon cancer? Surgery 2023;174:1273–5. \n 54 Moazzam Z, Cloyd J, Lima HA, et al. Quality of ChatGPT Responses \nto Questions Related to Pancreatic Cancer and its Surgical Care. \nAnn Surg Oncol 2023;30:6284–6. \n 55 NLLB Team. Scaling neural machine translation to 200 languages. \nNature 2024;630:841–6. \n 56 Swanson K, He S, Calvano J, et al. Biomedical text readability after \nhypernym substitution with fine- tuned large language models. PLOS \nDigit Health 2024;3:e0000489. \n 57 Garcia P , Ma SP , Shah S, et al. Artificial Intelligence–Generated \nDraft Replies to Patient Inbox Messages. JAMA Netw Open \n2024;7:e243201. \n 58 Zeng Y , Tang Q, Chen S, et al. Integration of a large language \nmodel with augmentative and alternative communication tool \nfor oncological aphasia rehabilitation. Asia Pac J Oncol Nurs \n2024;11:100344. \n 59 Kim CY , Lee CP , Mutlu B. Understanding large- language model (llm)- \npowered human- robot interaction. 2024. \n 60 Armbruster J, Bussmann F , Rothhaas C, et al. “Doctor ChatGPT, Can \nYou Help Me?” The Patient’s Perspective: Cross- Sectional Study. J \nMed Internet Res 2024;26:e58831. \n 61 Nov O, Singh N, Mann D. Putting chatgpt’s medical advice to the \n(turing) test: survey study. JMIR Med Educ 2023;9:e46939. \n 62 Platt J, Nong P , Smiddy R, et al. Public comfort with the use of \nChatGPT and expectations for healthcare. J Am Med Inform Assoc \n2024;31:1976–82. \n 63 Chen SY , Kuo HY , Chang SH. Perceptions of ChatGPT in healthcare: \nusefulness, trust, and risk. Front Public Health 2024;12:1457131. \n 64 Choudhury A, Shamszare H. Investigating the Impact of User Trust \non the Adoption and Use of ChatGPT: Survey Analysis. J Med \nInternet Res 2023;25:e47184. \n 65 Hung T, Kuperman G, Sherman EJ, et al. Performance of a trained \nlarge language model to provide clinical trial recommendation in a \nhead and neck cancer population. JCO 2024;42:11081. \n 66 Wong C, Zhang S, Gu Y , et al. Scaling clinical trial matching using \nlarge language models: a case study in oncology. [Preprint] 2023. \nAvailable: http://arxiv.org/abs/2308.02180\n 67 Morris ZS, Wooding S, Grant J. The answer is 17 years, what is the \nquestion: understanding time lags in translational research. J R Soc \nMed 2011;104:510–20. \n 68 Lee K, Paek H, Huang L- C, et al. Seetrials: leveraging large language \nmodels for safety and efficacy extraction in oncology clinical trials. \nSSRN [Preprint] 2024. \n 69 Elsamahy EA, Ahmed AE, Shoala T, et al. Deep- GenMut: Automated \ngenetic mutation classification in oncology: A deep learning \ncomparative study. Heliyon 2024;10:e32279. \n 70 Li T, Shetty S, Kamath A, et al. CancerGPT for few shot drug pair \nsynergy prediction using large pretrained language models. npj Digit \nMed 2024;7:40. \n 71 Menz BD, Kuderer NM, Bacchi S, et al. Current safeguards, risk \nmitigation, and transparency measures of large language models \nagainst the generation of health disinformation: repeated cross \nsectional analysis. BMJ 2024;384:e078538. \n 72 Funk PF , Hoch CC, Knoedler S, et al. ChatGPT’s Response \nConsistency: A Study on Repeated Queries of Medical Examination \nQuestions. EJIHPE 2024;14:657–68. \n 73 World Health Organization. Ethics and governance of artificial \nintelligence for health: WHO guidance. 1st edn. 2021. Available: \nhttps://www.who.int/publications/i/item/9789240029200\n 74 Ong JCL, Chang S- H, William W, et al. Ethical and regulatory \nchallenges of large language models in medicine. Lancet Digit Health \n2024;6:e428–32. \n 75 Kapsali MZ, Livanis E, Tsalikidis C, et al. Ethical Concerns About \nChatGPT in Healthcare: A Useful Tool or the Tombstone of Original \nand Reflective Thinking? Cureus 2024;16:e54759. \n 76 Rodler S, Kopliku R, Ulrich D, et al. Patients’ Trust in Artificial \nIntelligence- based Decision- making for Localized Prostate Cancer: \nResults from a Prospective Trial. Eur Urol Focus 2024;10:654–61. \n 77 Shumway DO, Hartman HJ. Medical malpractice liability in large \nlanguage model artificial intelligence: legal review and policy \nrecommendations. J Osteopath Med 2024;124:287–90. \n 78 Omiye JA, Lester JC, Spichak S, et al. Large language models \npropagate race- based medicine. NPJ Digit Med 2023;6:195. \n 79 Omar M, Sorin V, Agbareia R, et al. Evaluating and addressing \ndemographic disparities in medical large language models: a \nsystematic review. Int J Equity Health 2025;24:57. \n 80 Choradia N, Karzai F , Nipp R, et al. Increasing diversity in clinical \ntrials: demographic trends at the National Cancer Institute, 2005- \n2020. J Natl Cancer Inst 2024;116:1063–71. \n 81 Klang E, Apakama D, Abbott EE, et al. A strategy for cost- effective \nlarge language model use at health system- scale. NPJ Digit Med \n2024;7:320. \n 82 Kleinig O, Sinhal S, Khurram R, et al. Environmental impact of large \nlanguage models in medicine. Intern Med J 2024;54:2083–6. \n 83 Ren S, Tomlinson B, Black RW, et al. Reconciling the contrasting \nnarratives on the environmental impact of large language models. Sci \nRep 2024;14:26310. \n 84 Shulman LN, Sheldon LK, Benz EJ. The Future of Cancer Care in the \nUnited States—Overcoming Workforce Capacity Limitations. JAMA \nOncol 2020;6:327. \n 85 Zaharia M, Om K, Chen L, et al. The shift from models to compound \nAI systems. n.d. Available: https://bair.berkeley.edu/blog/2024/02/18/ \ncompound-ai-systems/\n 86 Boiko DA, MacKnight R, Kline B, et al. Autonomous chemical \nresearch with large language models. Nature New Biol \n2023;624:570–8. \n 87 Trinh TH, Wu Y , Le QV, et al. Solving olympiad geometry without \nhuman demonstrations. Nature 2024;625:476–82. \n 88 Shen Y , Xu Y , Ma J, et al. Multi- modal large language models in \nradiology: principles, applications, and potential. Abdom Radiol \n2024;50:2745–57. \n 89 Schulte B. Capacity of ChatGPT to Identify Guideline- Based \nTreatments for Advanced Solid Tumors. Cureus 2023;15:e37938. \n 90 Morley J, Murphy L, Mishra A, et al. Governing Data and \nArtificial Intelligence for Health Care: Developing an International \nUnderstanding. JMIR Form Res 2022;6:e31623. \n 91 Vesteghem C, Brøndum RF , Sønderkær M, et al. Implementing the \nFAIR Data Principles in precision oncology: review of supporting \ninitiatives. Brief Bioinform 2020;21:936–45. \n 92 Gallifant J, Afshar M, Ameen S, et al. The tripod- llm statement: a \ntargeted guideline for reporting large language models use. medRxiv \n2024. \n 93 Liu X, Cruz Rivera S, Moher D, et al. Reporting guidelines for clinical \ntrial reports for interventions involving artificial intelligence: the \nCONSORT- AI extension. Nat Med 2020;26:1364–74. \n 94 Liu Z, Wang P , Li Y , et al. RadOnc- gpt: a large language model for \nradiation oncology. [Preprint] 2023. \nBMJ Oncology: first published as 10.1136/bmjonc-2025-000759 on 15 May 2025. Downloaded from https://bmjoncology.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n",
  "topic": "Psychosocial",
  "concepts": [
    {
      "name": "Psychosocial",
      "score": 0.5746516585350037
    },
    {
      "name": "Enthusiasm",
      "score": 0.5401581525802612
    },
    {
      "name": "Narrative",
      "score": 0.4665524661540985
    },
    {
      "name": "Oncology",
      "score": 0.44329163432121277
    },
    {
      "name": "Health care",
      "score": 0.42741188406944275
    },
    {
      "name": "Medicine",
      "score": 0.4107590615749359
    },
    {
      "name": "Internal medicine",
      "score": 0.3923184275627136
    },
    {
      "name": "Psychology",
      "score": 0.2950989902019501
    },
    {
      "name": "Political science",
      "score": 0.21103188395500183
    },
    {
      "name": "Psychiatry",
      "score": 0.16256234049797058
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802654988",
      "name": "Princess Margaret Cancer Centre",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I98251732",
      "name": "McMaster University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I180670191",
      "name": "University of California, San Francisco",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1289530486",
      "name": "BC Cancer Agency",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1288882113",
      "name": "Boston Children's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210117453",
      "name": "Dana-Farber Cancer Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I48633490",
      "name": "Mass General Brigham",
      "country": "US"
    }
  ],
  "cited_by": 2
}