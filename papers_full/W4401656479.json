{
  "title": "Scaling, but not instruction tuning, increases large language models’ alignment with language processing in the human brain",
  "url": "https://openalex.org/W4401656479",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101366865",
      "name": "Changjiang Gao",
      "affiliations": [
        "Nanjing University of Science and Technology",
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5108151025",
      "name": "Zhengwu Ma",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100427076",
      "name": "Jiajun Chen",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100435506",
      "name": "Ping Li",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A5102865824",
      "name": "Shujian Huang",
      "affiliations": [
        "Nanjing University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5079800748",
      "name": "Jixing Li",
      "affiliations": [
        "City University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4377866072",
    "https://openalex.org/W4312107867",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4212828284",
    "https://openalex.org/W2626804490",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2070586582",
    "https://openalex.org/W4241074797",
    "https://openalex.org/W4402684140",
    "https://openalex.org/W1983208069",
    "https://openalex.org/W3217436924",
    "https://openalex.org/W2135595031",
    "https://openalex.org/W3004619146",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W4403651116",
    "https://openalex.org/W2344975321",
    "https://openalex.org/W2792056048",
    "https://openalex.org/W4401042840",
    "https://openalex.org/W4385573487",
    "https://openalex.org/W3175306105",
    "https://openalex.org/W2156964236",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W2135894974",
    "https://openalex.org/W4304465621",
    "https://openalex.org/W4366999450",
    "https://openalex.org/W4361766487",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4386616609",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3210923133",
    "https://openalex.org/W4392283598",
    "https://openalex.org/W4298144575",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W4398250357",
    "https://openalex.org/W4242031388",
    "https://openalex.org/W4389142554",
    "https://openalex.org/W4389519222",
    "https://openalex.org/W4285731152",
    "https://openalex.org/W4220949944",
    "https://openalex.org/W2963636937",
    "https://openalex.org/W2151591509"
  ],
  "abstract": "Abstract Transformer-based large language models (LLMs) have significantly advanced our understanding of meaning representation in the human brain. However, increasingly large LLMs have been questioned as valid cognitive models due to their extensive training data and their ability to access context thousands of words long. In this study, we investigated whether instruction tuning, another core technique in recent LLMs beyond mere scaling, can enhance models’ ability to capture linguistic information in the human brain. We evaluated the self-attention of base and fine-tuned LLMs of different sizes against human eye movement and functional magnetic resonance imaging (fMRI) activity patterns during naturalistic reading. We show that scaling has a greater impact than instruction tuning on model-brain alignment, reinforcing the scaling law in brain encoding performance. These finding have significant implications for understanding the cognitive plausibility of LLMs and their role in studying naturalistic language comprehension.",
  "full_text": null,
  "topic": "Comprehension",
  "concepts": [
    {
      "name": "Comprehension",
      "score": 0.5876468420028687
    },
    {
      "name": "Computer science",
      "score": 0.5655279159545898
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4974959194660187
    },
    {
      "name": "Natural language",
      "score": 0.4375916123390198
    },
    {
      "name": "Natural language processing",
      "score": 0.4342286288738251
    },
    {
      "name": "Linguistics",
      "score": 0.41935741901397705
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32255059480667114
    },
    {
      "name": "Programming language",
      "score": 0.13270869851112366
    },
    {
      "name": "Philosophy",
      "score": 0.09002193808555603
    },
    {
      "name": "History",
      "score": 0.08338633179664612
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36399199",
      "name": "Nanjing University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I168719708",
      "name": "City University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    }
  ]
}