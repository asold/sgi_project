{
    "title": "Do Transformer Modifications Transfer Across Implementations and Applications?",
    "url": "https://openalex.org/W3133264589",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2490038776",
            "name": "Sharan Narang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2104689355",
            "name": "Hyung Won Chung",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2738935859",
            "name": "Yi Tay",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2789854853",
            "name": "Liam Fedus",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2892311493",
            "name": "Thibault Févry",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2981509194",
            "name": "Michael Matena",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3023535622",
            "name": "Karishma Malkan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1023462809",
            "name": "Noah Fiedel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2496873187",
            "name": "Noam Shazeer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2115082539",
            "name": "Zhen-zhong Lan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2313799619",
            "name": "Yan-Qi Zhou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1963853028",
            "name": "Wei Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2068489588",
            "name": "Nan Ding",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1978239266",
            "name": "Jake Marcus",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099832880",
            "name": "Adam Roberts",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2001631461",
            "name": "Colin Raffel",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3023690688",
        "https://openalex.org/W2952191002",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2981040094",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W2519314406",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W2963351145",
        "https://openalex.org/W3040573126",
        "https://openalex.org/W2789541106",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2079656678",
        "https://openalex.org/W2963285578",
        "https://openalex.org/W2963454111",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W2963120839",
        "https://openalex.org/W2888520903",
        "https://openalex.org/W2949454572",
        "https://openalex.org/W3033188311",
        "https://openalex.org/W2767321762",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W2767286248",
        "https://openalex.org/W2754517384",
        "https://openalex.org/W2960955628",
        "https://openalex.org/W2257408573",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2962911098",
        "https://openalex.org/W2907121943",
        "https://openalex.org/W4297747548",
        "https://openalex.org/W3010768098",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2252136820",
        "https://openalex.org/W2965658867",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W3093517588",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2963034893",
        "https://openalex.org/W2176412452",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2885185669",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2123045220",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W2900096133",
        "https://openalex.org/W4288284003",
        "https://openalex.org/W4293569541",
        "https://openalex.org/W4287391717",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W3021293129",
        "https://openalex.org/W104184427",
        "https://openalex.org/W3005700362",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3026997957",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W4287867774",
        "https://openalex.org/W2952355681",
        "https://openalex.org/W2784823820",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W4320930577",
        "https://openalex.org/W2263490141",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2267186426",
        "https://openalex.org/W2963574252",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3100107515",
        "https://openalex.org/W4297781872",
        "https://openalex.org/W3000779003",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2964081807",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W4301581299",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W1799366690",
        "https://openalex.org/W3006439205",
        "https://openalex.org/W3126822054",
        "https://openalex.org/W2963537482",
        "https://openalex.org/W4287900772",
        "https://openalex.org/W4287633642",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2912521296",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3102892879"
    ],
    "abstract": "Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, Colin Raffel. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5758–5773\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n5758\nDo Transformer Modiﬁcations Transfer Across Implementations\nand Applications?\nSharan Narang∗ Hyung Won Chung Yi Tay William Fedus\nThibault Fevry† Michael Matena† Karishma Malkan† Noah Fiedel\nNoam Shazeer Zhenzhong Lan † Yanqi Zhou Wei Li\nNan Ding Jake Marcus Adam Roberts Colin Raﬀel †\nAbstract\nThe research community has proposed co-\npious modiﬁcations to the Transformer ar-\nchitecture since it was introduced over\nthree years ago, relatively few of which\nhave seen widespread adoption. In this\npaper, we comprehensively evaluate many\nof these modiﬁcations in a shared exper-\nimental setting that covers most of the\ncommon uses of the Transformer in natu-\nral language processing. Surprisingly, we\nﬁnd that most modiﬁcations do not mean-\ningfully improve performance. Further-\nmore, most of the Transformer variants we\nfound beneﬁcial were either developed in\nthe same codebase that we used or are rel-\natively minor changes. We conjecture that\nperformance improvements may strongly\ndepend on implementation details and cor-\nrespondingly make some recommendations\nfor improving the generality of experimen-\ntal results.\n1 Introduction\nMuch of the empirical success of deep learn-\ning can be attributed to advances in meth-\nods for building and training neural net-\nworks. These advances include improved op-\ntimizers (Sutskever et al., 2013; Hinton et al.,\n2012; Kingma and Ba, 2014; Shazeer and\nStern, 2018a), regularization schemes (Srivas-\ntava et al., 2014; Zhang et al., 2017; Neelakan-\ntan et al., 2015), and model architectures (He\net al., 2016; Hochreiter and Schmidhuber, 1997;\nVaswani et al., 2017). An aspiration underly-\ning much of this work is that an improvement\nto a particular machine learning pipeline will\nyield equal-or-better performance on any task\nthat the pipeline is applicable to. For example,\nresidual connections in convolutional networks\n(He et al., 2016) are designed to ideally improve\n∗Correspondence to sharannarang@google.com\n†Work completed while at Google\nperformance on any task where these models\nare applicable (image classiﬁcation, semantic\nsegmentation, etc.). In practice, when propos-\ning a new improvement, it is impossible to test\nit on every applicable downstream task, so re-\nsearchers must select a few representative tasks\nto evaluate it on. However, the proposals that\nare ultimately adopted by the research com-\nmunity and practitioners tend to be those that\nreliably improve performance across a wide\nvariety of tasks “in the wild”.\nThe Transformer architecture (Vaswani\net al., 2017) is an example of a seminal im-\nprovement in the ﬁeld of deep learning. Cur-\nrently, the Transformer is the de facto archi-\ntecture of choice for processing sequential data\nand is starting to be applied to vision appli-\ncations (e.g. Dosovitskiy et al. (2020)). Since\nbeing introduced three years ago, many modi-\nﬁcations to the Transformer architecture have\nbeen proposed. However, the most widely-used\napplications of the Transformer architecture\n(e.g. Devlin et al. (2018); Yang et al. (2019);\nRadford et al. (2018); Raﬀel et al. (2019)) incor-\nporate few of these modiﬁcations. Instead, the\nstandard practice is to use a slightly-modiﬁed\nversion of the originally-proposed Transformer.\nOne possible explanation for this is that the\noriginally-proposed Transformer architecture\nwas near-perfect, and there wasn’t much that\ncould be done to improve it. This is in contrast\nto, for example, convolutional neural networks,\nwhich have continually evolved over the past\nfew decades (e.g. the replacement of pooling\nwith striding (Springenberg et al., 2014), fully-\nconnected layers with convolutional layers (Lin\net al., 2013), the addition of normalization\n(Ioﬀe and Szegedy, 2015) and residual connec-\ntions (He et al., 2016), etc.). Another possible\nexplanation is that the modiﬁcations proposed\nto the Transformer do not “generalize” across\n5759\napplications, i.e. the modiﬁcations only help\non the limited experimental setting considered\nwhen the modiﬁcation was proposed, and/or\nrely on speciﬁc details that are not common\nacross implementations of the Transformer.\nThe main goal of this paper is to try to\ndetermine why most modiﬁcations proposed\nto the Transformer have not seen widespread\nadoption. To answer this question, we reimple-\nmented and evaluated a wide variety of Trans-\nformer variants on a suite of tasks that Trans-\nformers are commonly applied to. Our main\nﬁnding is that many Transformer modiﬁcations\ndo not result in improved performance in our\nexperimental setting. Moreover, those variants\nthat did yield better performance tended to\nbe those that were quite small changes and/or\nwere developed in the codebase where we car-\nried out our evaluation. This suggests to us the\npossibility that Transformer modiﬁcations ex-\nhibit a surprising lack of generalization across\ndiﬀerent implementations and tasks.\n2 Modiﬁcations\nIn this section, we enumerate all of the archi-\ntectural modiﬁcations we consider. For a de-\nscription of the Transformer architecture, refer\nto the appendix D.\nDue to space constraints, we are seldom able\nto thoroughly deﬁne each speciﬁc modiﬁcation.\nMoreover, we limit our study to the encoder-\ndecoder architecture. Please refer to the origi-\nnal sources for each modiﬁcation for additional\ndetails.\n2.1 Activations\nWe consider various activation functions to re-\nplace the ReLU in the feedforward network\nblock. The activation functions that we ex-\nplored are: (1) GeLU (Hendrycks and Gimpel,\n2016), (2) Swish (Ramachandran et al., 2017),\n(3) Exponential Linear Units (ELU) (Clevert\net al., 2015), (4) Scaled exponential linear units\n(SeLU) (Klambauer et al., 2017), (5) Sigmoid\nand (6) Softplus. We also explore “Gated Lin-\near Unit” (GLU) variants (Dauphin et al., 2017;\nShazeer, 2020) which compose two linear trans-\nformations together in an element-wise fashion,\ni.e. F1(x) ⊙σ(F2(x)) where σ is an activation\nfunction and F1 and F2 are separate learned\naﬃne transformations. We explore modifying\nσ to be sigmoid activations (denoted as GLU),\nReLU activations (denoted as ReGLU), GeLU\nactivations (denoted as GeGLU) or to be a\nstandard linear transformation (no activation,\ndenoted as LiGLU).\n2.2 Normalization\nWe explored “RMS (root-mean-square) norm”\n(Zhang and Sennrich, 2019) as an alternative\nto layer normalization as well as the Rezero\n(Bachlechner et al., 2020) initialization scheme,\nincluding combining Rezero with Layer Norm\nand RMS Norm. We also explored the Fixup\n(Zhang et al., 2019) initialization scheme which\ntries to solve the vanishing/exploding gradient\nproblem by rescaling the initializations.\n2.3 Depth\nWe explored the trade-oﬀs between the width of\nthe feedforward subblocks ( dﬀ) and depth ( L).\nIn order to ensure fair comparison, we scale dﬀ\nand the number of heads ( H) in order to keep\nthe total number of parameters constant when\nchanging the depth.\n2.4 Embeddings\nThe Transformer model includes multiple\nweight matrices of shape of dmodel ×dvocab:\none at the input of the encoder, one at the\ninput of the decoder, and one at the output\nof the decoder. Chung et al. (2021) showed\nthe beneﬁts of untying the embeddings for the\nencoder-only models. We extend the analy-\nsis and explore various ways of sharing these\nparameters: tying only encoder input and de-\ncoder input embeddings, tying only decoder\ninput and output embeddings, and untying all\nthe embeddings.\nIn addition, we explored factorizing the em-\nbedding matrix into two smaller matrices (Lan\net al., 2019). In other words, the embed-\nding matrix of size [ dmodel,dvocab] is factored\ninto [dmodel,dinner] and [dinner,dvocab]. We tried\nboth untied and tied decoder embeddings while\nencoder and decoder embeddings are shared.\nThe last technique we explored for the em-\nbeddings is the “Adaptive input embeddings”\nby Baevski and Auli (2019). Vocabulary items\nare clustered based on their frequencies. A\ncluster with more frequent ones has a larger\nembedding dimension. The embedding vec-\ntors are projected to the same dimension and\nconcatenated.\n5760\n2.5 Parameter sharing\nWe also explored sharing the parameters of the\nTransformer layers inspired by the “ALBERT”\nmodel of Lan et al. (2020). Each subblock\n(e.g., self-attention) has a unique set of weights\nshared across all l layers. Following Lan et al.\n(2020), we factorized the embeddings (denoted\nas “Factorized embeddings”) in addition to the\nparameter sharing. Note that these models\nhave untied softmax and vocabulary embed-\ndings in the decoder; we also tried tying them\n(denoted as “Shared embeddings”). Finally,\nwe experimented with applying the parameter\nsharing to the encoder and decoder separately.\n2.6 Softmax\nOur work considers variations to the softmax\ncomputation that produces the ﬁnal probabil-\nity distribution as computed by the last layer\nembedding. Adaptive softmax (Joulin et al.,\n2017) uses the natural imbalance in word dis-\ntributions (Zipf, 1949) to form clusters in a\nhierarchical model, which minimizes compu-\ntation time. In the original implementation,\neach cluster is permitted to have a diﬀerent\ncapacity and the size of the representations\nfor rare words is reduced via a projection ma-\ntrix. We consider the original variant, as well\nas a version that ablates the projection opera-\ntion. Mixture of Softmaxes (MoS) (Yang et al.,\n2017) improves the expressiveness of a single\nsoftmax operation by instead computing a lin-\near combination over softmaxes, each weighted\nby learned coeﬃcients.\n2.7 Architectures\nTransparent Attention One type of atten-\ntion variant we experiment with is Transparent\nAttention (Bapna et al., 2018). Transparent\nattention (Bapna et al., 2018) creates weighted\nresidual connections along encoder depth to\nfacilitate gradient ﬂow. In appendix A, we\nexperiment with additional attention variants.\nEvolved Transformer The Evolved\nTransformer (So et al., 2019) was designed\nvia evolution-based architecture search (Real\net al., 2019) where the initial population was\nseeded with the original Transformer. The\nsearch space generalizes the one followed in\nNASNet (Zoph et al., 2018), but extended to\nbe able to represent the Transformer.\nSynthesizer variants We explore the fac-\ntorized, dense, and random Synthesizer vari-\nants from Tay et al. (2020), where self-attention\nis replaced with “synthetic attention” patterns.\nWe denote “plus” when dot product attention\nis additively combined with the synthetic at-\ntention and plus alpha to denote when a scalar\nα is used to interpolate between synthetic and\ndot product attention.\nFunnel Transformer Funnel Trans-\nformer progressively reduces the sequence\nlength in order to eﬃciently encode the input\nsequence (Dai et al., 2020). We only applied\nthis reduction to the encoder.\nLightweight and Dynamic convolu-\ntions Lightweight convolution (Wu et al.,\n2019) is a special case of a depth-wise convolu-\ntion. It shares the weights of every subsequent\nnumber of m channels where m is a hyperpa-\nrameter and normalizes the weights across the\nﬁlter dimension. For a Transformer model, the\ndepth dimension corresponds to dmodel. Dy-\nnamic convolution (Wu et al., 2019) uses ker-\nnels that are functions of the input at the cur-\nrent time step. Following Wu et al. (2019), we\ncompute the kernels as a simple linear function\nof the layer input.\nSparse Expert Transformers Mixture\nof Experts (MoE) Transformer (Shazeer et al.,\n2018; Lepikhin et al., 2020) and Switch Trans-\nformer (Fedus et al., 2021) both replace the\nfeedforward network with sparsely activated\nexperts layers. The result is an example of\nadaptive computation where parameters (ex-\npert FFNs) are selected for each speciﬁc token.\nThis provides a way of scaling up the parame-\nter count of a model independently from the\nFLOPs required for a forward pass. Some vari-\nants in Fedus et al. (2021) consider sparse self-\nattention layers as well, but we only consider\nthe primary variant here.\nProduct Key Memory Similar to the\nexpert model designs, product key memory net-\nworks (Lample et al., 2019) process inputs adap-\ntively, selecting sparse values. In contrast, the\nmechanism of sparse computation isn’t done\nvia learned routing, but instead by an eﬃcient\nk-nearest neighbor weighted sum.\nUniversal Transformer Similar to block\nsharing, the Universal Transformer (Dehghani\net al., 2018) applies the same Transformer\n5761\n“block” over and over again to the input se-\nquence. However, instead of applying it a ﬁxed\nnumber of times, it recurrently reﬁnes the rep-\nresentation for each token until a halting mech-\nanism (based on Adaptive Computation Time\n(Graves, 2016)) is triggered.\n3 Experiments\nIn order to study the impact of each of the\nmodiﬁcations described in section 2, we con-\nduct a systematic study by comparing a base-\nline model to each modiﬁcation while hold-\ning the task, hyperparameters, optimizer, and\neither the parameter count or FLOP budget\n(ﬂoating point operations per second) constant.\nWe use the original Transformer model as our\nbaseline model with two modiﬁcations: First,\nwe apply layer normalization before the self-\nattention and feedforward blocks instead of\nafter. This small change has been unanimously\nadopted by all current Transformer implemen-\ntations because it leads to more eﬀective train-\ning (Baevski and Auli, 2019; Xiong et al., 2020).\nSecondly, we use relative attention with shared\nbiases (as used in Raﬀel et al. (2019)) instead of\nsinusoidal positional embeddings, which makes\nit easier to train the model. Our baseline model\nis a standard encoder-decoder with 12 layers in\nthe encoder and decoder. The feedforward net-\nwork in each layer consists of a dense layer with\ndimension of dﬀ = 3072. All attention mecha-\nnisms have 12 heads and “key” and “value” ma-\ntrices have a dimension of dkv = 64. All other\nsublayers have a dimension of dmodel = 768 re-\nsulting in 223 million parameters in the model.\nWe refer to this model as the “Vanilla Trans-\nformer”.\nWe consider two experimental settings for\nevaluating the performance of each modiﬁca-\ntion: Transfer learning based on the T5 (Raﬀel\net al., 2019) and supervised machine translation\non the WMT’14 English-German translation.\nFor transfer learning, we copy the methodol-\nogy used by the T5 model, proposed in Raﬀel\net al. (2019). For full details of this experimen-\ntal setup, please refer to Raﬀel et al. (2019).\nWe pre-train encoder-decoder models in a self-\nsupervised manner using the “span corruption”\nmasked language modeling objective (Taylor,\n1953; Fedus et al., 2018; Devlin et al., 2018) on\nthe C4 dataset. We run experiments on version\n2.3.1 of the C4 dataset available in TensorFlow\nDatasets1. We pre-train each architecture vari-\nant for 524,288 steps with batches of 65,536 to-\nkens. As in T5, we use Adafactor (Shazeer and\nStern, 2018b) for optimization and an inverse\nsquare root learning rate schedule during pre-\ntraining. We use a maximum sequence length\nof 512 for both the inputs and targets during\npre-training. To evaluate the performance of\npre-trained models, we compute the perplexity\non a held-out portion of the C4 dataset for each\npre-trained model, with the expectation that\nimprovements in perplexity will correlate with\nperformance on ﬁne-tuned tasks. To capture\nthe inter-run variance on these models, we run\neach model 5 times for 65 ,536 steps ( 1\n8 th of\nthe total pre-training steps). We report the\nmean and standard deviation of the loss (log\nperplexity) on held-out data of these ﬁve ex-\nperiments and also report the ﬁnal loss at the\nend of pre-training (524,288 steps). We do not\nuse any regularization during pre-training.\nIn the transfer learning setting, after pre-\ntraining we ﬁne-tune each model on three dif-\nferent tasks: the SuperGLUE (Wang et al.,\n2019) natural language understanding meta-\nbenchmark, the XSum (Narayan et al., 2018)\nabstractive summarization dataset, and the\nclosed-book variant (Roberts et al., 2020) of the\nWebQuestions (Berant et al., 2013) question-\nanswering task. With these tasks, we hope to\ncapture a broad variety of NLP problems in-\ncluding language understanding and classiﬁca-\ntion, language generation, and knowledge inter-\nnalization. For SuperGLUE and XSum, each\nmodel is ﬁne-tuned for 262 ,144 steps. Since\nthe WebQuestions dataset is much smaller, we\nﬁne-tune the model for only 30 ,000 steps. We\nuse a constant learning rate of 0 .0005 with a\nlinear warm-up of 20 ,000 steps. Similar to\npre-training, each batch contains 65 ,536 to-\nkens. We save a checkpoint every 2 ,500 steps\n(1,000 steps for WebQuestions) and report re-\nsults on the model checkpoint corresponding\nto the highest validation performance. We use\na dropout of 0 .1 during ﬁne-tuning for all the\ntasks. All results are reported on the valida-\ntion split of each dataset. For SuperGLUE, we\nreport the average score across all tasks in the\n1https://www.tensorflow.org/datasets/\ncatalog/c4\n5762\nbenchmark. We report ROUGE-2 (Lin, 2004)\nfor XSum and accuracy for WebQuestions.\nFor supervised training on the WMT’14 En-\nglish to German translation task (Bojar et al.,\n2014), we use the same model and batch size as\nfor the transfer learning setting. We train for\na total of 150,000 steps. We use the same data\nsplits as were used in (Vaswani et al., 2017) and\nreport the BLEU score of the highest-scoring\ncheckpoint on the validation set. We use a vo-\ncabulary of 37,000 tokens learned by Byte Pair\nEncoding (Sennrich et al., 2016) for supervised\ntraining as opposed to 32,000 tokens (created\nusing SentencePiece (Kudo and Richardson,\n2018)) for the transfer learning experiments.\nTo compare the eﬃciency of the model, we\nalso report the total number of parameters,\nthe total number of ﬂoating point operations,\nand the measured steps per second in the pre-\ntraining experiments. Reporting these param-\neters can help us understand the trade-oﬀ be-\ntween quality and eﬃciency. For each architec-\ntural modiﬁcation, we attempt to keep either\nthe parameter count or total operations in the\nmodel approximately the same to perform a\nfair comparison with the baseline model.\nAll hyperparameters are held constant for\neach architectural variant across pre-training\nand ﬁne-tuning. However, we found that cer-\ntain architectural (Rezero and Fixup) vari-\nants achieved signiﬁcantly lower negative log\nperplexity than the baseline model with the\nAdafactor optimizer. Therefore, we use the\nAdam optimizer (Kingma and Ba, 2014) for\nthese variants. For pre-training, we use an in-\nverse square root learning rate schedule with a\nlinear warm-up of 4,000 steps. For ﬁne-tuning,\nwe use a constant learning rate of 5 e−5 with\na linear warm-up of 20 ,000 steps. We provide\ndetails of certain modiﬁcations in appendix B.\nAll experiments are run using the T5 library\n2 on “slices” of Cloud TPU Pods. All model\nvariants are implemented in the Mesh Tensor-\nFlow library (Shazeer et al., 2018).\n3.1 Results\nThe results for all model variants are shown in\ntable 1. The vanilla Transformer achieves a Su-\nperGLUE average of 70.97 and a BLEU score\n2https://github.com/google-research/\ntext-to-text-transfer-transformer\nof 26.62 on WMT14 EnDe. This is comparable\nwith the scores achieved by the equivalently-\nsized T5-Base model Raﬀel et al. (2019) and\nsimilarly-sized Transformer-Big from Vaswani\net al. (2017), which conﬁrms that our base-\nline is reasonable. As mentioned earlier, each\nvariant has approximately the same number of\nparameters or total operations as the vanilla\nTransformer, with the following exceptions: For\nthe Universal Transformer, the total number\nof operations is approximately 4 ×the base-\nline model. Since the Universal Transformer\nmodel is already signiﬁcantly smaller than the\nbaseline model, it would not be fair to shrink\nthe model even further to match the number\nof operations with the baseline. Product key\nmemories (Lample et al., 2019) should only\nslightly increase FLOPs over the vanilla Trans-\nformer, but the total number of operations is\nartiﬁcially extremely high due to an ineﬃcient\nimplementation in Mesh Tensorﬂow.\nWe ﬁnd that several activation functions im-\nprove performance over the ReLU activation.\nSpeciﬁcally, SwiGLU and GeGLU improve per-\nformance on pre-training, ﬁne-tuning, and su-\npervised training without sacriﬁcing any ef-\nﬁciency in terms of speed. Replacing layer\nnormalization with RMS normalization yields\nimprovements while also improving training\nspeed. Our experiments with varying the depth\nof the model indicate that deeper models tend\nto outperform shallower ones with a ﬁxed pa-\nrameter count. However, these deeper mod-\nels are also more compute-intensive and there-\nfore slower than their shallower counterparts.\nSharing of parameters across layers tends to\nhurt performance. Interestingly, untying the\nencoder/decoder embeddings improve perfor-\nmance with only a modest increase in param-\neter count. Using mixture of softmaxes does\nimprove performance but is almost 40% slower\nthan the vanilla Transformer.\nAmong the diﬀerent architectures, we ﬁnd\nthat two of the synthesizer variants are beneﬁ-\ncial. Switch Transformer, mixture of experts,\nand product key memories all improve perfor-\nmance with signiﬁcantly more parameters than\nthe baseline model. However, these implemen-\ntations only use a subset of the parameters\nduring each step, so they are roughly equiva-\nlent to the vanilla Transformer in total number\n5763\nof operations. Surprisingly, all the other archi-\ntecture variants generally performed poorly.\nOverall, we found that most of the beneﬁcial\nmodiﬁcations conferred improvements across\npre-training, ﬁne-tuning, and supervised train-\ning, though a few variants (e.g. transparent\nattention, Synthesizer-random, ﬁxup) harmed\nperformance for transfer learning but not for\nWMT’14 EnDe. The modiﬁcations that led to\nsigniﬁcant improvements tended to fall into one\nof three buckets: relatively minor changes (i.e.,\nactivation functions, normalization and untying\nembedding matrices); those that increase pa-\nrameter count (i.e., Switch Transformer, prod-\nuct key memory) or are slower (i.e., mixture\nof softmaxes, deeper models); or those that\nwere originally invented in the Mesh Tensor-\nFlow codebase that we use for our experiments\n(i.e., mixture of experts, switch Transformer,\nsynthesizer). To further ensure the correct-\nness of the various architecture modiﬁcations,\nwe reached out to authors of 12 techniques to\nreview our implementation and provide their\nfeedback and received responses from 6 of them.\nAll of the authors who responded conﬁrmed\nthat our re-implementation was correct.\n3.2 Impact of hyperparameter tuning\nIt is a well-established fact in deep learning\nthat hyperparameters (and even random seeds\n(Dodge et al., 2020)) may have a huge impact\non model quality. In our experiments, we in-\ntentionally kept hyperparameter ﬁxed in order\nto measure whether a given modiﬁcation im-\nproves performance regardless of hyperparame-\nter settings. Given that this may be an overly\nidealistic constraint, we present a case study\nof trying to improve one of the model variants\nby tuning its hyperparameters. We selected\nUniversal Transformers (UT) (Dehghani et al.,\n2018) because it was claimed to achieve bet-\nter results than the vanilla Transformer, and\nthe UT has a relatively large number of hy-\nperparameters that we can adjust. Using our\nstandard hyperparameters, we obtain a loss of\n2.40 after training for 65 ,536 steps. Bearing in\nmind that our vanilla Transformer obtains a\nloss of 2.182 after the same amount of training,\nour goal was to at least achieve comparable\nperformance using the UT.\nTo this end, we swept over 25 model conﬁgu-\nrations, varying the number of recurrent steps\nand the gating/transition functions in the UT.\nWe also varied non-model-speciﬁc hyperparam-\neters including the learning rate schedule and\ndmodel. Over these 25 sweeps, only 2 managed\nto outperform the initial results. The only set-\ntings that worked were the result of reducing\nthe number of recurrent steps (from 16 to 2)\nand slightly increasing the model size. In the\nend, we managed to achieve an improvement of\n2.40 →2.265 (or 6% relative). While this is sig-\nniﬁcant, many other hyperparameter settings\nfailed to produce good results, and we were ulti-\nmately unable to match the performance of the\nvanilla Transformer. This exercise illustrates\nthe challenge of tuning these models.\n3.3 Correlation of perplexity and task\nperformance\nIn order to understand the relationship between\npre-training performance and ﬁne-tuned task\nquality, we investigate the correlation between\nperplexity and quality on each task. As shown\nin ﬁg. 1, quality on all three tasks seem to be\ncorrelated with pre-training perplexity, though\nthe correlation is surprisingly weak given past\nresults suggesting a stronger relationship (Adi-\nwardana et al., 2020). Interestingly, the perfor-\nmance on SuperGLUE (Spearman’s ρ= 0.87)\nand XSum (Spearman’s ρ= 0.80) seems to be\nhighly correlated with the pre-training perplex-\nity, whereas the performance on WebQuestions\n(Spearman’s ρ= 0.69) has a somewhat lower\ncorrelation. This may indicate that classiﬁca-\ntion and generation tasks beneﬁt more from\nimprovements in perplexity than knowledge-\nintensive tasks like question answering.\n4 Conjectures and\nRecommendations\nAs discussed above, we were surprised to ﬁnd\nthat so few of the architectural modiﬁcations\nproduced improvements in the settings we con-\nsidered. This largely contrasts the experiments\nincluded in the original papers that proposed\neach modiﬁcation. We broadly grouped the\nmodiﬁcations that actually did improve per-\nformance as either 1) being relatively simple\n(e.g. a change in activation function), 2) being\ndeveloped in the same codebase where we ran\nexperiments (e.g. the Synthesizer variants (Tay\net al., 2020)), or 3) incurring an increase in\nparameter count or FLOPs (e.g. the Switch\n5764\nModel Params Ops Step/s Early loss Final loss SGLUE XSum WebQWMT EnDe\nVanilla Transformer 223M 11.1T 3.50 2 .182±0.005 1 .838 71 .66 17 .78 23 .02 26.62\nGeLU 223 M 11.1T 3.58 2 .179±0.003 1 .838 75.79 17.86 25.13 26.47\nSwish 223 M 11.1T 3.62 2 .186±0.003 1 .847 73.77 17.74 24.34 26.75\nELU 223 M 11.1T 3.56 2 .270±0.007 1 .932 67 .83 16 .73 23 .02 26.08\nGLU 223 M 11.1T 3.59 2 .174±0.003 1.814 74.20 17.42 24.34 27.12\nGeGLU 223 M 11.1T 3.55 2 .130±0.006 1.792 75.96 18.27 24.87 26.87\nReGLU 223 M 11.1T 3.57 2 .145±0.004 1.803 76.17 18.36 24.87 27.02\nSeLU 223 M 11.1T 3.55 2 .315±0.004 1 .948 68 .76 16 .76 22 .75 25.99\nSwiGLU 223 M 11.1T 3.53 2 .127±0.003 1.789 76.00 18.20 24.34 27.02\nLiGLU 223 M 11.1T 3.59 2 .149±0.005 1.798 75.34 17.97 24.34 26.53\nSigmoid 223 M 11.1T 3.63 2 .291±0.019 1 .867 74.31 17.51 23 .02 26.30\nSoftplus 223 M 11.1T 3.47 2 .207±0.011 1 .850 72.45 17.65 24.34 26.89\nRMS Norm 223 M 11.1T 3.68 2 .167±0.008 1.821 75.45 17.94 24.07 27.14\nRezero 223 M 11.1T 3.51 2 .262±0.003 1 .939 61 .69 15 .64 20 .90 26.37\nRezero + LayerNorm 223M 11.1T 3.26 2 .223±0.006 1 .858 70 .42 17 .58 23 .02 26.29\nRezero + RMS Norm 223M 11.1T 3.34 2 .221±0.009 1 .875 70 .33 17 .32 23 .02 26.19\nFixup 223 M 11.1T 2.95 2 .382±0.012 2 .067 58 .56 14 .42 23 .02 26.31\n24 layers,dﬀ = 1536,H= 6 224M 11.1T 3.33 2 .200±0.007 1 .843 74.89 17.75 25.13 26.89\n18 layers,dﬀ = 2048,H= 8 223M 11.1T 3.38 2 .185±0.005 1.831 76.45 16.83 24.34 27.10\n8 layers,dﬀ = 4608,H= 18 223M 11.1T 3.69 2 .190±0.005 1 .847 74.58 17.69 23.28 26.85\n6 layers,dﬀ = 6144,H= 24 223M 11.1T 3.70 2 .201±0.010 1 .857 73.55 17.59 24.60 26.66\nBlock sharing 65 M 11.1T 3.91 2 .497±0.037 2 .164 64 .50 14 .53 21 .96 25.48\n+ Factorized embeddings 45M 9.4T 4.21 2 .631±0.305 2 .183 60 .84 14 .00 19 .84 25.27\n+ Factorized & shared em-\nbeddings\n20M 9.1T 4.37 2 .907±0.313 2 .385 53 .95 11 .37 19 .84 25.19\nEncoder only block sharing170M 11.1T 3.68 2 .298±0.023 1 .929 69 .60 16 .23 23 .02 26.23\nDecoder only block sharing144M 11.1T 3.70 2 .352±0.029 2 .082 67 .93 16 .13 23.81 26.08\nFactorized Embedding 227M 9.4T 3.80 2 .208±0.006 1 .855 70 .41 15 .92 22 .75 26.50\nFactorized & shared embed-\ndings\n202M 9.1T 3.92 2 .320±0.010 1 .952 68 .69 16 .33 22 .22 26.44\nTied encoder/decoder in-\nput embeddings\n248M 11.1T 3.55 2 .192±0.002 1 .840 71.70 17.72 24.34 26.49\nTied decoder input and out-\nput embeddings\n248M 11.1T 3.57 2 .187±0.007 1.827 74.86 17.74 24.87 26.67\nUntied embeddings 273 M 11.1T 3.53 2 .195±0.005 1.834 72.99 17.58 23.28 26.48\nAdaptive input embeddings204M 9.2T 3.55 2 .250±0.002 1 .899 66 .57 16 .21 24.07 26.66\nAdaptive softmax 204 M 9.2T 3.60 2 .364±0.005 1 .982 72.91 16.67 21 .16 25.56\nAdaptive softmax without\nprojection\n223M 10.8T 3.43 2 .229±0.009 1 .914 71.82 17.10 23 .02 25.72\nMixture of softmaxes 232M 16.3T 2.24 2 .227±0.017 1.821 76.77 17.62 22 .75 26.82\nTransparent attention 223M 11.1T 3.33 2 .181±0.014 1 .874 54 .31 10 .40 21 .16 26.80\nDynamic convolution 257M 11.8T 2.65 2 .403±0.009 2 .047 58 .30 12 .67 21 .16 17.03\nLightweight convolution 224M 10.4T 4.07 2 .370±0.010 1 .989 63 .07 14 .86 23 .02 24.73\nEvolved Transformer 217M 9.9T 3.09 2 .220±0.003 1 .863 73.67 10.76 24.07 26.58\nSynthesizer (dense) 224 M 11.4T 3.47 2 .334±0.021 1 .962 61 .03 14 .27 16 .14 26.63\nSynthesizer (dense plus) 243M 12.6T 3.22 2 .191±0.010 1 .840 73.98 16.96 23.81 26.71\nSynthesizer (dense plus al-\npha)\n243M 12.6T 3.01 2 .180±0.007 1.828 74.25 17.02 23.28 26.61\nSynthesizer (factorized) 207M 10.1T 3.94 2 .341±0.017 1 .968 62 .78 15 .39 23.55 26.42\nSynthesizer (random) 254M 10.1T 4.08 2 .326±0.012 2 .009 54 .27 10 .35 19 .56 26.44\nSynthesizer (random plus) 292M 12.0T 3.63 2 .189±0.004 1 .842 73.32 17.04 24.87 26.43\nSynthesizer (random plus\nalpha)\n292M 12.0T 3.42 2 .186±0.007 1.828 75.24 17.08 24.08 26.39\nUniversal Transformer 84M 40.0T 0.88 2 .406±0.036 2 .053 70 .13 14 .09 19 .05 23.91\nMixture of experts 648 M 11.7T 3.20 2 .148±0.006 1.785 74.55 18.13 24.08 26.94\nSwitch Transformer 1100M 11.7T 3.18 2 .135±0.007 1.758 75.38 18.02 26.19 26.81\nFunnel Transformer 223M 1.9T 4.30 2 .288±0.008 1 .918 67 .34 16 .26 22 .75 23.20\nWeighted Transformer 280M 71.0T 0.59 2 .378±0.021 1 .989 69 .04 16 .98 23 .02 26.30\nProduct key memory 421M 386.6T 0.25 2 .155±0.003 1.798 75.16 17.04 23.55 26.73\nTable 1: Results for all architecture variants. The baseline model is the vanilla Transformer with relative\nattention. The early loss represents the mean and standard deviation of perplexity at 65 ,536 steps. The\nﬁnal perplexity is reported at the end of pre-training (524 ,288 steps). SGLUE refers to SuperGLUE and\nWebQ refers to WebQuestions dataset. We report average, ROUGE-2, accuracy, and BLEU score for\nSuperGLUE, XSum, WebQuestions, and WMT EnDe, respectively, on the validation sets.Note: Results\non WMT English to German are reported without any pre-training. The scores which outperform\nthe vanilla Transformer are highlighted in boldface.\n5765\n2.2\n 2.1\n 2.0\n 1.9\n 1.8\n 1.7\nPre-training loss (neg log perplexity)\n50\n55\n60\n65\n70\n75\n80Average score\n(a) SuperGLUE\n2.2\n 2.1\n 2.0\n 1.9\n 1.8\n 1.7\nPre-training loss (neg log perplexity)\n10\n12\n14\n16\n18\n20ROUGE-2 (b) XSum\n2.2\n 2.1\n 2.0\n 1.9\n 1.8\n 1.7\nPre-training loss (neg log perplexity)\n15\n18\n21\n24\n27Accuracy (c) WebQuestions\nFigure 1: Relationship between perplexity and ﬁne-tuned task quality. The x-axis measures the pre-\ntraining perplexity and the y-axis measures the score for each task, with each point representing an\narchitecture variant. The dashed line shows baseline performance and the gray line is the line of best ﬁt.\nTransformer (Fedus et al., 2021) or Universal\nTransformer (Dehghani et al., 2018)). Other\nmodiﬁcations that don’t ﬁt into one of these cat-\negories generally didn’t improve performance.\nThere are various possible explanations as to\nwhy our results bore out the way they did:\n1. The Mesh TensorFlow codebase and imple-\nmentation are just so diﬀerent than standard\npractice that most architectural modiﬁcations\ndo not work. We believe this is unlikely due to\nthe fact that the Mesh TensorFlow Transformer\nimplementation was created by one of the co-\nauthors of the original Transformer paper and\nhas been used to attain state-of-the-art results\n(e.g. Raﬀel et al. (2019); Roberts et al. (2020);\nKhashabi et al. (2020); Kale (2020); Nogueira\net al. (2020); Narang et al. (2020); Xue et al.\n(2020); Fedus et al. (2021), etc.).\n2. The tasks we consider are non-standard or\ndo not match the set of tasks used to vet the\nmodiﬁcations in the ﬁrst place . The Trans-\nformer model is used for a variety of NLP prob-\nlems including classiﬁcation and generation\ntasks. We included transfer learning experi-\nments on SuperGLUE, XSum, and WebQues-\ntions and supervised training on WMT’14\nEnDe, which covers the majority of use-cases.\n3. Not tuning hyperparameters handicapped\nother methods . While per-modiﬁcation tun-\ning might improve results (as veriﬁed in sec-\ntion 3.2), we argue that truly useful improve-\nments to the Transformer should be reasonably\nhyperparameter-agnostic. Further, if hyperpa-\nrameter sensitivity was the issue, it would be\nlikely that a least a few of the compared meth-\nods “got lucky” with the hyperparameters, but\nvery few modiﬁcations produced a boost.\n4. We implemented many of the modiﬁcations\nincorrectly. To rule out this possibility, we\ncorresponded with many of the creators of the\nmodiﬁcations we considered, who conﬁrmed\nthe correctness in all cases.\n5. Modiﬁcations to the Transfomer architecture\noften do not transfer across implementations\nand applications.\nFollowing the above rationale, we believe the\nﬁnal option is a plausible explanation for our\nresults. This possibility is supported by the\nfact that few of the modiﬁcations we consider in\nthis paper have seen widespread adoption – if\nthey transferred easily across implementations\nand applications, they would likely have been\nmore widely adopted.\nGiven this sober take, we conclude our pa-\nper with some suggestions as to how to ensure\nthe robustness of improvements for future ar-\nchitectural modiﬁcations. First, when propos-\ning a new modiﬁcation, try it out in multi-\nple completely disparate codebases. Given the\nproliferation of Transformer implementations\n(e.g. Wolf et al. (2019); Shazeer et al. (2018);\nVaswani et al. (2018), etc.), this should be\nstraightforward. Second, apply it to a wide\nvariety of downstream applications, including\ntransfer learning, supervised learning, and lan-\nguage modeling – and, possibly, include do-\nmains beyond NLP too (e.g., computer vision\n(Dosovitskiy et al., 2020)). Third, when eval-\nuating performance in diﬀerent implementa-\ntions and on diﬀerent tasks, keep hyperparam-\neters ﬁxed as much as possible, or at least\nattempt to measure the robustness of the mod-\niﬁcations to changes in hyperparameters. Fi-\nnally, best-practice reporting of results should\ninclude mean and standard deviation across\nmultiple trials, or at least avoid cherry-picking\n5766\nthe best run (Dodge et al., 2020; Henderson\net al., 2018). With these guidelines in mind,\nwe hope future work on architectural modiﬁ-\ncations to the Transformer will be more likely\nto see widespread adoption and improve the\nperformance of this powerful architecture.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R\nSo, Jamie Hall, Noah Fiedel, Romal Thop-\npilan, Zi Yang, Apoorv Kulshreshtha, Gau-\nrav Nemade, Yifeng Lu, et al. 2020. To-\nwards a human-like open-domain chatbot.arXiv\npreprint arXiv:2001.09977.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoﬀrey E.\nHinton. 2016. Layer normalization. arXiv\npreprint arXiv:1607.06450.\nThomas Bachlechner, Bodhisattwa Prasad Ma-\njumder, Huanru Henry Mao, Garrison W Cot-\ntrell, and Julian McAuley. 2020. Rezero is\nall you need: Fast convergence at large depth.\narXiv preprint arXiv:2003.04887 .\nAlexei Baevski and Michael Auli. 2019. Adaptive\ninput representations for neural language mod-\neling. In International Conference on Learning\nRepresentations.\nAnkur Bapna, Mia Xu Chen, Orhan Firat,\nYuan Cao, and Yonghui Wu. 2018. Train-\ning deeper neural machine translation mod-\nels with transparent attention. arXiv preprint\narXiv:1808.07561.\nJonathan Berant, Andrew Chou, Roy Frostig, and\nPercy Liang. 2013. Semantic parsing on Free-\nbase from question-answer pairs. In Proceedings\nof the 2013 Conference on Empirical Methods in\nNatural Language Processing, pages 1533–1544,\nSeattle, Washington, USA. Association for Com-\nputational Linguistics.\nOndrej Bojar, Christian Buck, Christian Feder-\nmann, Barry Haddow, Philipp Koehn, Johannes\nLeveling, Christof Monz, Pavel Pecina, Matt\nPost, Herve Saint-Amand, Radu Soricut, Lucia\nSpecia, and Ale s Tamchyna. 2014. Findings of\nthe 2014 workshop on statistical machine trans-\nlation. In Proceedings of the Ninth Workshop\non Statistical Machine Translation , pages 12–\n58, Baltimore, Maryland, USA. Association for\nComputational Linguistics.\nJianpeng Cheng, Li Dong, and Mirella Lapata.\n2016. Long short-term memory-networks for ma-\nchine reading. arXiv preprint arXiv:1601.06733.\nHyung Won Chung, Thibault Fevry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2021. Re-\nthinking embedding coupling in pre-trained lan-\nguage models. In International Conference on\nLearning Representations.\nDjork-Arn´ e Clevert, Thomas Unterthiner, and\nSepp Hochreiter. 2015. Fast and accurate deep\nnetwork learning by exponential linear units\n(elus). arXiv preprint arXiv:1511.07289 .\nZihang Dai, Guokun Lai, Yiming Yang, and\nQuoc V. Le. 2020. Funnel-Transformer: Fil-\ntering out Sequential Redundancy for Eﬃcient\nLanguage Processing. arXiv e-prints , page\narXiv:2006.03236.\nYann N Dauphin, Angela Fan, Michael Auli, and\nDavid Grangier. 2017. Language modeling with\ngated convolutional networks. In International\nconference on machine learning , pages 933–941.\nPMLR.\nMostafa Dehghani, Stephan Gouws, Oriol\nVinyals, Jakob Uszkoreit, and  Lukasz Kaiser.\n2018. Universal transformers. arXiv preprint\narXiv:1807.03819.\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. 2018. BERT: Pre-\ntraining of deep bidirectional transformers\nfor language understanding. arXiv preprint\narXiv:1810.04805.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early\nstopping.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain\nGelly, et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929 .\nWilliam Fedus, Ian Goodfellow, and Andrew M\nDai. 2018. Maskgan: better text generation via\nﬁlling in the . arXiv preprint arXiv:1801.07736 .\nWilliam Fedus, Barret Zoph, and Noam Shazeer.\n2021. Switch transformers: Scaling to trillion\nparameter models with simple and eﬃcient spar-\nsity. arXiv preprint arXiv:2101.03961 .\nAlex Graves. 2016. Adaptive computation time\nfor recurrent neural networks. arXiv preprint\narXiv:1603.08983.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. 2016. Deep residual learning for image\nrecognition. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition,\npages 770–778.\n5767\nPeter Henderson, Riashat Islam, Philip Bachman,\nJoelle Pineau, Doina Precup, and David Meger.\n2018. Deep reinforcement learning that matters.\nIn Proceedings of the AAAI Conference on Arti-\nﬁcial Intelligence, volume 32.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nGeoﬀrey Hinton, Nitish Srivastava, and Kevin\nSwersky. 2012. Lecture 6a: Overview of mini-\nbatch gradient descent.\nSepp Hochreiter and J¨ urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nSergey Ioﬀe and Christian Szegedy. 2015. Batch\nnormalization: Accelerating deep network train-\ning by reducing internal covariate shift. InInter-\nnational conference on machine learning , pages\n448–456. PMLR.\nArmand Joulin, Moustapha Ciss´ e, David Grang-\nier, Herv´ e J´ egou, et al. 2017. Eﬃcient softmax\napproximation for gpus. In International Con-\nference on Machine Learning , pages 1302–1310.\nPMLR.\nMihir Kale. 2020. Text-to-text pre-training\nfor data-to-text tasks. arXiv preprint\narXiv:2005.10433.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and\nHannaneh Hajishirzi. 2020. UniﬁedQA: Cross-\ning format boundaries with a single QA sys-\ntem. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020 , pages 1896–\n1907, Online. Association for Computational\nLinguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. arXiv\npreprint arXiv:1412.6980.\nG¨ unter Klambauer, Thomas Unterthiner, An-\ndreas Mayr, and Sepp Hochreiter. 2017. Self-\nnormalizing neural networks. arXiv preprint\narXiv:1706.02515.\nTaku Kudo and John Richardson. 2018. Sentence-\nPiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text\nprocessing. arXiv preprint arXiv:1808.06226 .\nGuillaume Lample, Alexandre Sablayrolles,\nMarc’Aurelio Ranzato, Ludovic Denoyer, and\nHerv´ e J´ egou. 2019. Large memory layers with\nproduct keys. arXiv preprint arXiv:1907.05242 .\nZhenzhong Lan, Mingda Chen, Sebastian Good-\nman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. 2019. Albert: A lite bert for self-\nsupervised learning of language representations.\narXiv preprint arXiv:1909.11942 .\nZhenzhong Lan, Mingda Chen, Sebastian Good-\nman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. 2020. ALBERT: A Lite BERT for self-\nsupervised learning of language representations.\nIn International Conference on Learning Repre-\nsentations.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong\nXu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng\nChen. 2020. Gshard: Scaling giant models with\nconditional computation and automatic shard-\ning. arXiv preprint arXiv:2006.16668 .\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summa-\nrization Branches Out , pages 74–81, Barcelona,\nSpain. Association for Computational Linguis-\ntics.\nMin Lin, Qiang Chen, and Shuicheng Yan.\n2013. Network in network. arXiv preprint\narXiv:1312.4400.\nSharan Narang, Colin Raﬀel, Katherine Lee, Adam\nRoberts, Noah Fiedel, and Karishma Malkan.\n2020. WT5?! Training text-to-text models\nto explain their predictions. arXiv preprint\narXiv:2004.14546.\nShashi Narayan, Shay B. Cohen, and Mirella La-\npata. 2018. Don’t give me the details, just the\nsummary! topic-aware convolutional neural net-\nworks for extreme summarization. In Proceed-\nings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1797–\n1807, Brussels, Belgium. Association for Compu-\ntational Linguistics.\nArvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya\nSutskever, Lukasz Kaiser, Karol Kurach, and\nJames Martens. 2015. Adding gradient noise im-\nproves learning for very deep networks. arXiv\npreprint arXiv:1511.06807.\nRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep,\nand Jimmy Lin. 2020. Document ranking with\na pretrained sequence-to-sequence model. In\nFindings of the Association for Computational\nLinguistics: EMNLP 2020 , pages 708–718, On-\nline. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training.\nColin Raﬀel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. 2019. Ex-\nploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683.\nPrajit Ramachandran, Barret Zoph, and Quoc V\nLe. 2017. Searching for activation functions.\narXiv preprint arXiv:1710.05941 .\n5768\nEsteban Real, Alok Aggarwal, Yanping Huang,\nand Quoc V Le. 2019. Regularized evolution\nfor image classiﬁer architecture search. In Pro-\nceedings of the aaai conference on artiﬁcial in-\ntelligence, volume 33, pages 4780–4789.\nAdam Roberts, Colin Raﬀel, and Noam Shazeer.\n2020. How much knowledge can you pack into\nthe parameters of a language model? arXiv\npreprint arXiv:2002.08910.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016. Neural machine translation of rare\nwords with subword units. In Proceedings of the\n54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1715–1725, Berlin, Germany.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position repre-\nsentations. arXiv preprint arXiv:1803.02155 .\nNoam Shazeer. 2020. Glu variants improve trans-\nformer. arXiv preprint arXiv:2002.05202 .\nNoam Shazeer, Youlong Cheng, Niki Parmar,\nDustin Tran, Ashish Vaswani, Penporn Koanan-\ntakool, Peter Hawkins, HyoukJoong Lee, Ming-\nsheng Hong, Cliﬀ Young, et al. 2018. Mesh-\ntensorﬂow: Deep learning for supercomputers.\narXiv preprint arXiv:1811.02084 .\nNoam Shazeer and Mitchell Stern. 2018a. Adafac-\ntor: Adaptive learning rates with sublinear\nmemory cost. In International Conference on\nMachine Learning, pages 4596–4604. PMLR.\nNoam Shazeer and Mitchell Stern. 2018b. Adafac-\ntor: Adaptive learning rates with sublinear\nmemory cost. CoRR, abs/1804.04235.\nDavid So, Quoc Le, and Chen Liang. 2019. The\nevolved transformer. In International Confer-\nence on Machine Learning , pages 5877–5886.\nPMLR.\nJost Tobias Springenberg, Alexey Dosovitskiy,\nThomas Brox, and Martin Riedmiller. 2014.\nStriving for simplicity: The all convolutional net.\narXiv preprint arXiv:1412.6806 .\nNitish Srivastava, Geoﬀrey Hinton, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: a simple way\nto prevent neural networks from overﬁtting.\nThe journal of machine learning research ,\n15(1):1929–1958.\nIlya Sutskever, James Martens, George Dahl, and\nGeoﬀrey Hinton. 2013. On the importance of\ninitialization and momentum in deep learning.\nIn International conference on machine learning,\npages 1139–1147. PMLR.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng\nJuan, Zhe Zhao, and Che Zheng. 2020. Synthe-\nsizer: Rethinking self-attention in transformer\nmodels. arXiv preprint arXiv:2005.00743 .\nWilson L Taylor. 1953. “cloze procedure”: A new\ntool for measuring readability. Journalism quar-\nterly, 30(4):415–433.\nAshish Vaswani, Samy Bengio, Eugene Brevdo,\nFrancois Chollet, Aidan N Gomez, Stephan\nGouws, Llion Jones,  Lukasz Kaiser, Nal Kalch-\nbrenner, Niki Parmar, et al. 2018. Tensor2tensor\nfor neural machine translation. arXiv preprint\narXiv:1803.07416.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n Lukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Advances in neural in-\nformation processing systems, pages 5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel Bowman. 2019. Su-\nperGLUE: A stickier benchmark for general-\npurpose language understanding systems. In\nAdvances in Neural Information Processing Sys-\ntems, volume 32, pages 3266–3280. Curran Asso-\nciates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh,\nJulien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R´ emi Louf, Mor-\ngan Funtowicz, et al. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language pro-\ncessing. arXiv preprint arXiv:1910.03771 .\nFelix Wu, Angela Fan, Alexei Baevski, Yann\nDauphin, and Michael Auli. 2019. Pay less at-\ntention with lightweight and dynamic convolu-\ntions. In International Conference on Learning\nRepresentations.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020.\nOn layer normalization in the transformer archi-\ntecture.\nLinting Xue, Noah Constant, Adam Roberts,\nMihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raﬀel. 2020. mt5: A\nmassively multilingual pre-trained text-to-text\ntransformer. arXiv preprint arXiv:2010.11934 .\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov,\nand William W Cohen. 2017. Breaking the\nsoftmax bottleneck: A high-rank rnn language\nmodel. arXiv preprint arXiv:1711.03953 .\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime\nCarbonell, Ruslan Salakhutdinov, and Quoc V.\nLe. 2019. XLNet: Generalized autoregressive\npretraining for language understanding. arXiv\npreprint arXiv:1906.08237.\n5769\nBiao Zhang and Rico Sennrich. 2019. Root mean\nsquare layer normalization. arXiv preprint\narXiv:1910.07467.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin,\nand David Lopez-Paz. 2017. mixup: Beyond\nempirical risk minimization. arXiv preprint\narXiv:1710.09412.\nHongyi Zhang, Yann N Dauphin, and Tengyu\nMa. 2019. Fixup initialization: Residual learn-\ning without normalization. arXiv preprint\narXiv:1901.09321.\nGeorge Kingsley Zipf. 1949. Human behavior and\nthe principle of least eﬀort .\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens,\nand Quoc V Le. 2018. Learning transferable ar-\nchitectures for scalable image recognition. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 8697–8710.\n5770\nA Experiments with positional\nembeddings\nWe also conducted a study of architectural\nvariants using learned positional embeddings\n(Vaswani et al., 2017) in the baseline model in-\nstead of relative attention. Besides this change,\nthe experimental setup remains the same (as\ndescribed in section 3). The weighted Trans-\nformer architecture doesn’t reliably converge\nusing positional embeddings, so we do not re-\nport results using this architecture.\nIn addition to the modiﬁcations described\nin section 2, we also experiment with varia-\ntions in attention. Sinusoidal positional em-\nbeddings (Vaswani et al., 2017) were proposed\nin the original Transformer to inject informa-\ntion of the order of the sequence into what was\notherwise a set-operation transformation. Rel-\native attention (Shaw et al., 2018) replaced the\nabsolute position embeddings by those based\non relative distance between tokens (clipped to\na maximum distance hyperparameter k). The\nMeshTensorﬂow code base (Shazeer et al., 2018)\nintroduces two changes to relative attention.\nIn these changes, a bias is added to the self-\nattention logits (eq. 8) before multiplication\nwith values, where the bias may be optionally\nshared across self-attention layers.\nThe results from this study are shown in\ntable 2. Similar to relative attention, the\nonly modiﬁcations that result in improvements\nare relatively minor modiﬁcations (e.g. activa-\ntion function andnormalization), ineﬃcient in\nterms of parameter count or FLOPs (e.g. the\nSwitch Transformer) or were invented in the\nsame codebase that we used (e.g. Synthesizer).\nArchitectures with relative attention outper-\nform those with positional embedding by a\nsigniﬁcant margin. Interestingly, certain archi-\ntectures (Mixture of Softmaxes, tied decoder\ninput and output embeddings) outperformed\nthe vanilla Transformer with relative attention\nperform worse than the vanilla Transformer\nin this setup. Also, the absolute ﬁne-tuned\nperformance is worse for almost all the models\ncompared with their relative attention counter-\nparts.\nB Implementation details for\nmodiﬁcations\nFor factorized embedding, we use an inner di-\nmension of 128 for models with and without\nblock sharing of parameters.\nIn adaptive input embedding experiments,\nwe use three clusters of size 2500, 6000, and\n23,628. For experiments with adaptive soft-\nmax, we split the third cluster into two clus-\nters of 23,500 and 128. Since we used a larger\nvocabulary (see section 3) for the supervised\ntraining on the WMT’14, we use the same num-\nber of clusters with the same relative cluster\nsizes.\nWe experimented with 10 and 15 softmaxes\nfor the mixture of softmax models. In the\npaper, we only report results for the model\nwith 15 softmaxes since it performs better.\nFor Lightweight and Dynamic convolutions,\nwe use one-dimensional kernel with width 9.\nThe depth of the kernel is determined depend-\ning on whether it is depthwise-convolution or\nvanilla convolution in which case its depth is\ndmodel. For Universal Transformer, we use num-\nber of recurrent steps of 24 and halting thresh-\nold of 0.5. We use 32 experts in the Mixture\nof Experts experiments.\nIn PKM experiments, we use knn= 32, 128\nkeys and 512 memory slots. In our experiments,\nwe introduce a product key memory network\nbefore the last layer in the decoder.\nIn the Funnel Transformer experiments, we\nuse mean pooling with 3 blocks in the en-\ncoder. The input sequence is pooled after ev-\nery 4 layers in the funnel Transformer. In the\nweighted Transformer, we freeze the weights\nof the branched attention module for the last\n20,000 steps of pre-training.\nC Reproducing the original\nTransformer experiments\nVaswani et al. (2017) reported the BLEU score\nof 25.8 (Table 3 of their paper) when evalu-\nated on the dev set without checkpoint aver-\naging. We ran a replication experiment with\nthe same Transformer-Base architecture and\nachieved 25.52. With this, we believe that\nour Transformer codebase closely replicates the\noriginal one. Additionally, the baseline trans-\nformer model in our paper is comparable to\nthe Transformer-Big model from Vaswani et al.\n5771\n(2017). The Transformer-Big model achieves a\nBLEU score of 26.4 (Table 3 of their paper) on\nthe validation set of the WMT EnDe transla-\ntion task. Our baseline model achieves a BLEU\nscore of 26.62 on the same validation set which\nis marginally better than the results reported\nin the original paper.\nD Transformer Background\nIn this section, we give a brief description of the\noriginal Transformer architecture. We primar-\nily include this description so that we can refer\nback to speciﬁc components as we introduce\ndiﬀerent modiﬁcations. For a more in-depth\ndescription of the Transformer architecture, re-\nfer to the original paper (Vaswani et al., 2017)\nor follow-up tutorials3,4 .\nIn this work, we solely experiment with\n“encoder-decoder” Transformers, which ingest\nan input sequence of tokens and produce an\noutput sequence conditioned on the input. We\ndenote the tokens of the input sequence as\nx[1],x[2],...,x [T] and the target sequence as\ny[1],y[2],...,y [U]. The encoder ﬁrst embeds\neach entry in the input sequence using the em-\nbedding matrix E ∈Rdvocab×dmodel and adds a\nposition encoding p as follows:\nhe,0[t] = E[x[t]] + p[t]\nwhere p[t] ∈Rdmodel is a “position embedding”.\nIn the original Transformer, this position em-\nbedding is computed as\np[t,i] =\n\n\n\nsin\n(\nt\n100002i/dmodel\n)\nieven\ncos\n(\nt\n100002i/dmodel\n)\niodd\n(1)\nIn general, we will use he,l and hd,l to denote\nthe output of the lth layer block of the encoder\nand decoder, respectively. For simplicity, we re-\nfer to the embeddings as if they are the output\nof a “zeroth” layer block.\nEach layer block in the encoder comprises a\nmulti-headed self-attention mechanism (Cheng\net al., 2016) followed by a position-wise\ndense/nonlinearity/dense feedforward network.\nBoth of these “subblocks” include a residual\n3http://nlp.seas.harvard.edu/2018/04/03/\nattention.html\n4http://jalammar.github.io/\nillustrated-transformer/\nconnection (He et al., 2016) and layer normal-\nization (Ba et al., 2016). Layer normaliza-\ntion is deﬁned as an operation over a sequence\nh[1],...,h [T] as\nµ[t] = 1\ndmodel\ndmodel∑\ni=1\nh[t,i] (2)\nσ[t] =\n√ 1\ndmodel\ndmodel∑\ni=1\n(h[t,i] −µ[t])2\n(3)\nLayerNorm(h)[t] = γ\nσ[t] ⊙(h[t,i] −µ[t]) + β\n(4)\nwhere ⊙indicates elementwise multiplication\nand γ,β ∈Rdmodel are learned parameters that\nare unique to each instance of layer normaliza-\ntion.\nHead h in the multi-headed self-attention of\nlayer l produces, at timestep t,\nqe,l,h[t] = he,l−1[t]Qe,l,h (5)\nke,l,h[t] = he,l−1[t]Ke,l,h (6)\nve,l,h[t] = he,l−1[t]Ve,l,h (7)\nae,l,h = softmax\n(qe,l,h[t]ke,l,h[t]⊤\n√dk\n)\nve,l,h[t]\n(8)\nwhere Qe,l,h ∈Rdmodel×dk, Ke,l,h ∈Rdmodel×dk,\nand Ve,l,h ∈Rdmodel×dv are the “query”, “key”,\nand “value” projection matrices, respectively.\nThe self-attention outputs ae,l,h for all H heads\nare then concatenated and projected against\nthe matrix Oe,l ∈ RHdv×dmodel along with a\nresidual connection and layer normalization as\nfollows:\nse,l[t] = LayerNorm\n\n\n\n\nae,l,1[t]\n...\nae,l,H[t]\n\nOe,l + he,l−1[t]\n\n\n(9)\nThe output of the multi-headed self-\nattention mechanism is then passed through a\nfeedforward network that operates on each se-\nquence element independently. Speciﬁcally, the\nfeedforward network consists of a projection, a\nReLU nonlinearity, and another projection as\nfollows:\nfe,l[t] = max(0,se,l[t]We,l,1 +be,l,1)We,l,2 +be,l,2\n(10)\n5772\nwhere We,l,1 ∈Rdmodel×dﬀ ,be,l,1 ∈Rdﬀ ,We,l,1 ∈\nRdﬀ×dmodel and be,l,1 ∈Rdmodel . The output of\nthe feedforward network is then combined with\nthe subblock’s input via a residual connection\nand layer normalization:\nhe,l = LayerNorm(se,l + fe,l) (11)\nOverall, the decoder is structured sim-\nilarly to the encoder, with the following\nchanges: First, the self-attention mechanisms\nare “causal” which prevents the decoder from\nlooking at future items from the target se-\nquence when it is fed in during training. This\nis achieved by constructing an “attention mask”\nM ∈RU×U that zeros out attention entries\nthat are nonpermissable; speciﬁcally replacing\nthe operation in eq. (8) with\nM[i,j] =\n{\n0, i ≤j\n−∞, i>j (12)\nad,l,h = softmax\n(qd,l,h[t]kd,l,h[t]⊤\n√dk\n+ M\n)\nvd,l,h[t]\n(13)\nwhere the d subscript denotes activations and\nparameters for the decoder. Second, the layer\nblocks in the decoder contain an encoder-\ndecoder attention mechanism after the self-\nattention mechanism and before the feedfor-\nward network. Speciﬁcally, encoder-decoder\nattention computes\nq′\nd,l,h[t] = sd,l[t]Q′\nd,l,h (14)\nk′\nd,l,h[t] = he,L[t]K′\nd,l,h (15)\nv′\nd,l,h[t] = he,L[t]V′\nd,l,h (16)\na′\nd,l,h = softmax\n(\nq′\nd,l,h[t]k′\nd,l,h[t]⊤\n√dk\n)\nv′\nd,l,h[t]\n(17)\nThe activations from each head a′\nd,l,h are then\nfed into the residual/layer norm block (eq. (9))\nand the feedforward network (eq. (10)) as usual.\nAt the output of the ﬁnal layer of the decoder,\neach entry in the sequence of activations hd,L\nis projected via an output logit matrix G ∈\nRdmodel×dvocab.\n5773\nModel Params Ops Step/s Early loss Final loss SGLUE XSum WebQ\nVanilla Transformer 223 M 11.1T 3.90 2 .245±0.005 1 .865 69 .72 16 .94 24 .60\nGeLU 223 M 11.1T 3.88 2 .220±0.005 1.863 70.36 17.10 23.28\nSwish 223 M 11.1T 3.93 2 .234±0.005 1 .865 69.60 17.07 24.34\nELU 223 M 11.1T 3.86 2 .333±0.013 1 .942 64 .30 16 .21 24 .07\nGLU 223 M 11.1T 3.88 2 .212±0.005 1.834 70.43 17.42 24.34\nGeGLU 223 M 11.1T 3.85 2 .172±0.010 1.807 72.36 17.69 24.87\nReGLU 223 M 11.1T 3.87 2 .190±0.008 1.832 70.63 17.38 21.96\nSeLU 223 M 11.1T 3.84 2 .372±0.016 1 .967 64 .68 16 .00 23 .28\nSwiGLU 223 M 11.1T 3.82 2 .168±0.006 1.806 70.90 17.51 25.13\nLiGLU 223 M 11.1T 3.88 2 .180±0.002 1.816 71.23 17.55 24.60\nSigmoid 223 M 11.1T 3.94 2 .947±1.152 1 .908 69 .36 16 .64 23 .02\nSoftplus 223 M 11.1T 3.77 2 .324±0.032 1 .885 68 .99 16 .92 21 .96\nRMS Norm 223 M 11.1T 3.99 2 .209±0.008 1.856 69.11 16 .90 23 .55\nRezero 223 M 11.1T 4.14 3 .180±0.719 2 .506 54 .01 6 .44 20 .90\nRezero + LayerNorm 223 M 11.1T 3.78 2 .229±0.006 1 .902 64 .75 16 .40 23 .02\nRezero + RMS Norm 223 M 11.1T 3.90 2 .306±0.016 1 .948 59 .86 15 .66 23 .02\nFixup 223 M 11.1T 3.32 2 .473±0.014 2 .236 57 .98 12 .51 23 .28\n24 layers,dﬀ = 1536,H= 6 224 M 11.1T 3.12 2 .260±0.014 1 .874 70.59 17.11 23.02\n18 layers,dﬀ = 2048,H= 8 223 M 11.1T 3.27 2 .268±0.037 1 .878 70.40 16.87 23 .02\n8 layers,dﬀ = 4608,H= 18 223 M 11.1T 3.61 2 .243±0.003 1 .871 68 .67 17.03 23.55\n6 layers,dﬀ = 6144,H= 24 223 M 11.1T 3.59 2 .250±0.004 1 .882 68 .08 16 .93 23 .81\nBlock sharing 65 M 11.1T 4.03 2 .777±0.019 2 .237 63 .06 13 .89 21 .96\n+ Factorized embeddings 45 M 9.4T 4.35 2 .670±0.178 2 .205 57 .17 12 .13 20 .11\n+ Factorized & Shared embeddings20M 9.1T 4.49 2 .874±0.059 2 .362 57 .46 11 .78 19 .58\nEncoder only block sharing 170 M 11.1T 3.80 2 .399±0.008 2 .016 64 .08 14 .74 21 .69\nDecoder only block sharing 144 M 11.1T 3.92 2 .542±0.067 2 .048 69.95 16.01 21 .96\nFactorized Embedding 227 M 9.4T 3.97 2 .273±0.019 1 .886 68 .91 16 .41 21 .43\nFactorized & shared embeddings 202M 9.1T 4.08 2 .387±0.006 2 .018 69.93 16.07 21 .96\nTied encoder/decoder input embed-\ndings\n248M 11.1T 3.86 2 .254±0.008 1 .872 68 .34 16 .60 22 .75\nTied decoder input and output em-\nbeddings\n248M 11.1T 3.86 2 .262±0.006 1 .871 69 .48 16 .85 23 .28\nUntied embeddings 273 M 11.1T 3.83 2 .265±0.013 1 .872 67 .99 16 .66 23 .02\nAdaptive input embeddings 204 M 9.2T 4.15 2 .321±0.006 1 .934 69 .20 16 .69 21 .96\nAdaptive softmax 204 M 9.2T 4.21 2 .425±0.005 2 .009 67 .71 15 .74 20 .11\nAdaptive softmax without projection223M 10.8T 3.97 2 .357±0.009 1 .937 68 .68 16 .45 22 .75\nMixture of softmaxes 232 M 16.3T 2.50 3 .112±1.169 1.843 70.70 16.78 22 .75\nRelative attention with bias 223M 11.3T 3.49 2 .197±0.005 1.832 74.06 17.63 24.87\nRelative attention with shared bias 223M 11.3T 3.57 2 .194±0.006 1.840 74.14 17.62 24.34\nRelative position representation 223M 11.1T 3.10 2 .189±0.008 1.838 74.26 17.67 24.07\nSinusoidal positional encoding 223M 11.1T 3.91 2 .278±0.032 1 .906 69.76 16.25 22 .75\nTransparent attention 223 M 11.1T 3.61 2 .244±0.013 1 .949 53 .77 6 .39 15 .08\nDynamic convolution 257 M 11.8T 2.65 2 .405±0.007 2 .038 55 .16 10 .25 4 .50\nLightweight convolution 224 M 10.4T 4.05 2 .356±0.006 1 .990 61 .32 14 .08 24 .08\nEvolved Transformer 217 M 9.7T 3.11 2 .233±0.004 1 .890 67 .88 16 .40 24 .08\nSynthesizer (dense) 224 M 11.4T 3.61 2 .339±0.019 1 .965 61 .02 14 .48 18 .25\nSynthesizer (dense plus) 243 M 12.6T 3.34 2 .200±0.008 1.832 74.16 16.96 24.87\nSynthesizer (dense plus alpha) 243M 12.6T 3.11 2 .204±0.005 1.846 75.18 16.94 24.60\nSynthesizer (factorized) 207 M 10.1T 4.10 2 .629±0.573 1 .964 61 .76 15 .44 22 .49\nSynthesizer (random) 254 M 10.1T 4.26 2 .458±0.167 1 .972 64 .61 15 .39 23 .02\nSynthesizer (random plus) 292 M 12.0T 3.79 2 .202±0.010 1.849 76.84 17.04 23.02\nSynthesizer (random plus alpha) 292M 12.0T 3.55 2 .212±0.013 1.856 75.02 17.08 24.87\nUniversal Transformer 84 M 40.0T 0.88 2 .443±0.022 2 .111 60 .54 12 .02 17 .73\nMixture of experts 648 M 11.7T 3.20 2 .194±0.008 1.846 68.82 17.12 24.87\nSwitch Transformer 1100 M 11.8T 3.41 2 .175±0.005 1.775 72.21 17.78 24.87\nFunnel Transformer 223 M 1.9T 4.83 2 .291±0.008 1 .925 67 .11 16 .33 21 .64\nProduct key memory 421 M 386.6T 0.25 2 .212±0.007 1.821 69.62 16 .58 24 .08\nTable 2: Pre-training and ﬁne-tuning results for all architecture variants with learned positional embed-\ndings. The early loss represents the mean and standard deviation of perplexity at 65 ,536 steps. The\nﬁnal perplexity is reported at the end of pre-training (524 ,288 steps). SGLUE refers to SuperGLUE and\nWebQ refers to WebQuestions dataset. We report average, ROUGE-2, and accuracy for SuperGLUE,\nXSum, and WebQuestions, respectively, on the validation sets. The scores which outperform the vanilla\nTransformer are highlighted in boldface."
}