{
  "title": "TCM-GPT: Efficient pre-training of large language models for domain adaptation in Traditional Chinese Medicine",
  "url": "https://openalex.org/W4399206078",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2146282143",
      "name": "Guoxing Yang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2101016562",
      "name": "Liu Xiaohong",
      "affiliations": [
        "CRUK Lung Cancer Centre of Excellence",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2344202409",
      "name": "Jianyu Shi",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2128849231",
      "name": "Zan Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2099641897",
      "name": "Guangyu Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2146282143",
      "name": "Guoxing Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101016562",
      "name": "Liu Xiaohong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2344202409",
      "name": "Jianyu Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128849231",
      "name": "Zan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099641897",
      "name": "Guangyu Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3198659451",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2011550183",
    "https://openalex.org/W3014279233",
    "https://openalex.org/W1991862576",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W6804076021",
    "https://openalex.org/W3207715300",
    "https://openalex.org/W3103978933",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4381253519",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3039201763",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4313483736",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W4385572697",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4311642023"
  ],
  "abstract": "Pre-training and fine-tuning have emerged as a promising paradigm across various natural language processing (NLP) tasks. The effectiveness of pretrained large language models (LLM) has witnessed further enhancement, holding potential for applications in the field of medicine, particularly in the context of Traditional Chinese Medicine (TCM). However, the application of these general models to specific domains often yields suboptimal results, primarily due to challenges like lack of domain knowledge, unique objectives, and computational efficiency. Furthermore, their effectiveness in specialized domains, such as Traditional Chinese Medicine, requires comprehensive evaluation.To address the above issues, we propose a novel domain specific TCMDA (TCM Domain Adaptation) approach, efficient pre-training with domain-specific corpus. Specifically, we first construct a large TCM-specific corpus, TCM-Corpus-1B, by identifying domain keywords and retrieving from general corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained modelâ€™s weights and uses rank decomposition matrices to efficiently train specific dense layers for pre-training and fine-tuning, efficiently aligning the model with TCM-related tasks, namely TCM-GPT-7B. We further conducted extensive experiments on two TCM tasks, including TCM examination and TCM diagnosis. TCM-GPT-7B archived the best performance across both datasets, outperforming other models by relative increments of 17% and 12% in accuracy, respectively. To the best of our knowledge, our study represents the pioneering validation of domain adaptation of a large language model with 7 billion parameters in TCM domain. We will release both TCM-Corpus-1B and TCM-GPT-7B model once accepted to facilitate interdisciplinary development in TCM and NLP, serving as the foundation for further study.",
  "full_text": null,
  "topic": "Adaptation (eye)",
  "concepts": [
    {
      "name": "Adaptation (eye)",
      "score": 0.7159487009048462
    },
    {
      "name": "Traditional Chinese medicine",
      "score": 0.5800154209136963
    },
    {
      "name": "Domain adaptation",
      "score": 0.5692201852798462
    },
    {
      "name": "Training (meteorology)",
      "score": 0.5538418292999268
    },
    {
      "name": "Computer science",
      "score": 0.47337883710861206
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4721921682357788
    },
    {
      "name": "Psychology",
      "score": 0.27965497970581055
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2777533531188965
    },
    {
      "name": "Medicine",
      "score": 0.2637486457824707
    },
    {
      "name": "Alternative medicine",
      "score": 0.18683084845542908
    },
    {
      "name": "Geography",
      "score": 0.11527058482170105
    },
    {
      "name": "Neuroscience",
      "score": 0.09813332557678223
    },
    {
      "name": "Mathematics",
      "score": 0.0902990996837616
    },
    {
      "name": "Pathology",
      "score": 0.0731254518032074
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210147892",
      "name": "CRUK Lung Cancer Centre of Excellence",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    }
  ]
}