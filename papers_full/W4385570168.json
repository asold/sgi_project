{
    "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection",
    "url": "https://openalex.org/W4385570168",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5065688288",
            "name": "Rheeya Uppaal",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5085263462",
            "name": "Junjie Hu",
            "affiliations": [
                null,
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A5100443469",
            "name": "Yixuan Li",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4206339000",
        "https://openalex.org/W3035507081",
        "https://openalex.org/W3202559014",
        "https://openalex.org/W2531327146",
        "https://openalex.org/W3092527263",
        "https://openalex.org/W1493526108",
        "https://openalex.org/W4301785137",
        "https://openalex.org/W2758425594",
        "https://openalex.org/W3105975529",
        "https://openalex.org/W4287552504",
        "https://openalex.org/W2034978228",
        "https://openalex.org/W3196675050",
        "https://openalex.org/W2618169590",
        "https://openalex.org/W1540596182",
        "https://openalex.org/W2963909453",
        "https://openalex.org/W2119880843",
        "https://openalex.org/W4287812705",
        "https://openalex.org/W4287614078",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3175976195",
        "https://openalex.org/W3174540647",
        "https://openalex.org/W3216820735",
        "https://openalex.org/W4221149036",
        "https://openalex.org/W2986193249",
        "https://openalex.org/W2101946573",
        "https://openalex.org/W4221143264",
        "https://openalex.org/W3104939451",
        "https://openalex.org/W4223977507",
        "https://openalex.org/W4281261683",
        "https://openalex.org/W3173182565",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W3114610051",
        "https://openalex.org/W3034408878",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W1932198206",
        "https://openalex.org/W3034640977",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3035441651",
        "https://openalex.org/W4312284582",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W4220902914",
        "https://openalex.org/W4205725534",
        "https://openalex.org/W4286889105",
        "https://openalex.org/W4237277263",
        "https://openalex.org/W3175204457",
        "https://openalex.org/W2329847998",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3121064530",
        "https://openalex.org/W3159630167",
        "https://openalex.org/W2923014074"
    ],
    "abstract": "Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Fine-tuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of directly leveraging pre-trained language models for OOD detection, without any model fine-tuning on the ID data. We compare the approach with several competitive fine-tuning objectives, and offer new insights under various types of distributional shifts. Extensive experiments demonstrate near-perfect OOD detection performance (with 0% FPR95 in many cases), strongly outperforming the fine-tuned counterpart.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 12813–12832\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nIs Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for\nOut-of-Domain Detection\nRheeya Uppaal1 Junjie Hu1,2 Yixuan Li1\n1Department of Computer Sciences,\n2Department of Biostatistics and Medical Informatics\nUniversity of Wisconsin-Madison\n{uppaal, jhu, sharonli}@cs.wisc.edu\nAbstract\nOut-of-distribution (OOD) detection is a criti-\ncal task for reliable predictions over text. Fine-\ntuning with pre-trained language models has\nbeen a de facto procedure to derive OOD detec-\ntors with respect to in-distribution (ID) data.\nDespite its common use, the understanding\nof the role of fine-tuning and its necessity for\nOOD detection is largely unexplored. In this\npaper, we raise the question: is fine-tuning nec-\nessary for OOD detection? We present a study\ninvestigating the efficacy of directly leveraging\npre-trained language models for OOD detec-\ntion, without any model fine-tuning on the ID\ndata. We compare the approach with several\ncompetitive fine-tuning objectives, and offer\nnew insights under various types of distribu-\ntional shifts. Extensive evaluations on 8 di-\nverse ID-OOD dataset pairs demonstrate near-\nperfect OOD detection performance (with 0%\nFPR95 in many cases), strongly outperform-\ning its fine-tuned counterparts. We show that\nusing distance-based detection methods, pre-\ntrained language models are near-perfect OOD\ndetectors when the distribution shift involves a\ndomain change. Furthermore, we study the ef-\nfect of fine-tuning on OOD detection and iden-\ntify how to balance ID accuracy with OOD\ndetection performance. Our code is publically\navailable1.\n1 Introduction\nDespite recent successes, high-performing pre-\ntrained language models are still fragile under dis-\ntribution shifts, making their applications to the\nreal world challenging (Ribeiro et al., 2020). In\nmost real-world settings, the train and test distri-\nbutions are often not independent and identically\ndistributed. Furthermore, test distributions are of-\nten non-stationary and can change over time. The\nproblem of out-of-distribution (OOD) detection ad-\ndresses the identification of anomalous data, en-\nabling the model to abstain from prediction when it\n1https://github.com/Uppaal/lm-ood\nis not supposed to. This is especially important for\nhigh-risk settings like financial and medical appli-\ncations, where unreliable predictions could incur\ngreat costs (Ulmer et al., 2020; Zhang et al., 2021).\nIn literature, a de facto procedure is to fine-tune\na pre-trained language model on the in-distribution\n(ID) data2, and then derive the OOD detector based\non the adapted model (Zhou et al., 2021; Hendrycks\net al., 2020; Xu et al., 2021). The fine-tuned model\nis hypothesized to produce embeddings that are\ncustomized to the ID data. Thus, prior work fo-\ncuses on the design of fine-tuning and expects the\nadapted representations to be more useful for OOD\ndetection. Despite its common use, the understand-\ning of the role of fine-tuning and its necessity for\nOOD detection is largely lacking in the field.\nMotivated by this, we revisit the common pro-\ncedure and raise the unexplored question: is fine-\ntuning necessary at all, for OOD detection ? To\nanswer this question, we introduce a simple and\neffective procedure for OOD detection, which does\nnot require any model fine-tuning on the ID data.\nSpecifically, we explore distance-based metrics for\ndetection, which measure the relative distances of\nsamples in the representation space of a pre-trained\nlanguage model. The operating hypothesis is that\nembeddings of ID samples are closer to each other\nthan the OOD sample embeddings. To the best of\nour knowledge, we are the first to explore distance-\nbased OOD detection methods directly on a pre-\ntrained language model, rather than the fine-tuned\nmodels adopted in previous works.\nWe show that our method based on a pre-trained\nlanguage model achieves near-perfect performance\nin detecting out-of-domain shifts, favorably outper-\nforming its fine-tuned counterparts. For example,\nfor 20NewsGroups (ID) vs. RTE (OOD), OOD\ndetection with the best fine-tuning loss (Khosla\net al., 2020) yields an FPR95 of 24.8%, while a pre-\n2Note that the ID data is defined w.r.t. the downstream\ndataset of interest, not the pre-training data.\n12813\ntrained language model can perfectly detectRTE as\nOOD with 0% FPR95. For comprehensive evalua-\ntions, we experiment on 8 diverse ID-OOD dataset\npairs spanning semantic and background shifts, and\nshow that the strong performance of using the pre-\ntrained model holds consistently. To better under-\nstand the strong performance, we further show that\npre-trained models display strongly separated do-\nmain clusters, both qualitatively and quantitatively.\nThe strong separation of domain clusters leads to\nthe efficacy of distance-based OOD detection.\nEven further, we systematically compare differ-\nent fine-tuning objectives, and interestingly observe\nthat the performance of distance-based OOD detec-\ntion declines over the course of fine-tuning across\nall objectives, despite the increase in ID classifi-\ncation accuracy. To this end, we provide new in-\nsights that early stopping (Yao et al., 2007) can be\na promising solution, if one desires a good trade-\noff between OOD detection and ID classification\nperformance.\nOur contributions can be summarized as follows:\n1. We propose a simple and effective method\nfor zero-shot3 OOD detection, leveraging pre-\ntrained language models without fine-tuning\non the ID data. Extensive experiments demon-\nstrate its near-perfect performance (with 0%\nFPR95 in most cases), favorably outperform-\ning its fine-tuned counterparts.\n2. We conduct a comprehensive study to under-\nstand fine-tuning objectives and their impact\non OOD detection. We offer new insights on\ntheir efficacy under various types of distribu-\ntion shifts.\n3. We perform qualitative and quantitative analy-\nsis on the embedding characteristics, explain-\ning the strong performance of using a pre-\ntrained language model for OOD detection.\n2 Preliminaries\nOOD Detection For a supervised multi-class clas-\nsification task, the labeled training dataset Din =\n{(xi,yi)}N\ni=1 consists of samples from the joint\ndistribution PXY, where Xis the input space and\nY= {1,··· ,C}is the label space. Given a test-\ntime sample x′, OOD detection aims to identify\nwhether x′is in-distribution (ID) Pin or not, where\nPin is the marginal of PXY on X. Formally, we\n3We use the term “zero-shot” to refer to a setting where no\n(ID or OOD) data is used to update the model parameters.\ndenote the OOD detector as a binary function map-\nping G(x′) : X→{ in,out}.\nTypes of Distribution Shifts Arora et al. (2021)\ncategorize OOD samples by the type of distribu-\ntion shift they exhibit in NLP problems. According\nto Ren et al. (2019), the representations h(x) can\nbe decomposed into two independent and disjoint\ncomponents—semantic features and background\nfeatures. Semantic features are discriminative and\nstrongly correlated with labels for prediction, while\nbackground features contain population-level statis-\ntics and are invariant across labels.\nBased on the type of features in OOD samples,\nthe distribution shift is categorized assemantic shift\nor background shift. An example of the semantic\nshift is the open-set classification problem that en-\ncounters novel classes at test time (Scheirer et al.,\n2012), where the semantic of x′is outside the sup-\nport of Y. Background shift is often seen when the\ndomain or style of texts changes in the input space\nXwhile Yremains the same (Pavlick and Tetreault,\n2016). We comprehensively consider both types of\nshifts later in our experiments in Section 4.\n3 Methodology\nIn Section 3.1, we start by introducing OOD de-\ntection with pre-trained language models, which\ndoes not require any model fine-tuning on the ID\ndataset. We further consider OOD detection with\nmodel fine-tuning in Section 3.2.\n3.1 OOD Detection with Pre-trained Models\nWe consider a pre-trained language model back-\nbone h: X→ Rd, which encodes an input x to a\nd-dimensional text embedding h(x).\nThe goal of OOD detection is to identify samples\nthat do not belong to Pin. Note that the ID data\nis defined w.r.t. the downstream dataset Din of\ninterest, instead of the pre-training data. Different\nfrom prior works, there is no fine-tuning/training\non the ID samples, and the setup is thus labelled as\nzero-shot OOD detection.\nWe formulate the zero-shot OOD detector as a\nbinary function mapping:\nGλ(x; h) =\n{\nin if S(x; h) ≥λ\nout if S(x; h) <λ , (1)\nwhere S(x; h) is the OOD scoring function, and λ\nis the threshold. By convention, λis chosen so that\n12814\na high fraction of ID data (e.g., 95%) is above the\nthreshold. We describe S(x; h) in details next.\nWe employ distance-based methods for zero-\nshot OOD detection, which measure the relative\ndistances of samples in representation space. To\nthe best of our knowledge, we are the first to use\ndistance-based OOD detection directly with a pre-\ntrained language model, while previous works use\nmodels adapted to the ID data. The operating hy-\npothesis is that the embeddings of ID samples are\ncloser to each other than the OOD sample embed-\ndings. Modeling the learned representation space\nas a mixture of multivariate Gaussians, Lee et al.\n(2018) used the Maximum Mahalanobis distance\n(Mahalanobis, 2018) to all class centroids as the\nscore for OOD detection:\nSMaha(x; h) = min\nc∈Y\n(h(x) −µc)⊤\nΣ−1 (h(x) −µc) ,\nwhere Σ is the covariance matrix and µc is the\nmean embedding of class c. Both Σ and µc are\nestimated on the ID embeddings extracted from the\npre-trained language model h(·).\nUsing Mahalanobis distance for OOD detection\nrequires some distributional assumptions on the\nrepresentation space. This is circumvented through\nnon-parametric density estimation using nearest\nneighbors (Sun et al., 2022). The distance between\na query point and its k-th nearest neighbor in the\nID data is used for OOD detection:\nSkNN(x,h) = −∥z −zk∥2,\nwhere z and zk are the L2 normalized embeddings,\nfor the query point x and its k-th nearest neighbor.\nIn Section 5, we evaluate zero-shot OOD detection\nperformance using both parametric (Maha) and\nnon-parametric (KNN) distance functions.\n3.2 OOD Detection with Fine-tuning\nIn contrast to the zero-shot OOD detection setup,\nan alternative strategy is to fine-tune the model\non the ID dataset Din and then perform OOD de-\ntection w.r.t. the fine-tuned model. In what fol-\nlows, we comprehensively consider three different\nfine-tuning objectives: (1) cross-entropy loss, (2)\ntask-adaptive pretraining loss, and (3) supervised\ncontrastive loss.\nCross-Entropy (CE) The cross-entropy loss is\nwidely used for training neural networks, making it\nan ideal baseline for our study. Given a pre-trained\nmodel, we fine-tune with the CE loss:\nLCE = 1\nN\nN∑\ni=1\n−log efy(xi;θ)\n∑C\nj=1 efj(xi;θ)\nwhere fy is the logit output corresponding to the\nground truth label y, and θis the parameterization\nof the neural network.\nTask-adaptive Pretraining (TAPT) Gururangan\net al. (2020) show that multi-phase adaptive pre-\ntraining boosts downstream task performance of\npre-trained language models. They introduce Task\nAdaptive Pre-Training (TAPT), which involves ex-\ntending the unsupervised pre-training process (us-\ning the masked language modeling objective (Ken-\nton and Toutanova, 2019)) with data for the down-\nstream task, before fine-tuning to the same task\nusing cross-entropy. TAPT improves generaliza-\ntion capabilities by providing a strong initialization\nfor fine-tuning, and to the best of our knowledge,\nTAPT has not been used in the setting of OOD\ndetection prior to our work.\nSupervised Contrastive Learning (SupCon) By\nleveraging information on labels and increasing the\nnumber of positive pairs during contrastive train-\ning, SupCon (Khosla et al., 2020) has been shown\nto consistently outperform cross-entropy on large-\nscale classification tasks (Gunel et al., 2020). The\nobjective encourages embeddings of a class to be\nhighly separated from other classes, boosting the\nperformance of OOD detection on text classifica-\ntion tasks (Zhou et al., 2021). Formally,\nLSupCon = −\nN∑\ni=1\n1\nN|P(i)|\n∑\np∈P(i)\nlog exp(z⊤\ni zp/τ)∑\na∈A(i) exp (z⊤\ni za/τ),\nwhere P(i) is the set of anchor instances from the\nsame class as xi, A(i) is the set of all anchor in-\nstances, zi is the L2 normalized sentence embed-\nding for xi, and τ is the temperature.\nAfter fine-tuning, OOD detection is performed\nusing a similar procedure as Equation 1, except that\nthe scoring function S(x; h) is calculated using\nthe fine-tuned model. While our primary focus is\ndistance-based detection, we additionally consider\ntwo common output-based methods—maximum\n12815\nSettings ID OOD\nOoD: Semantic Shift20NewsGroupsSST-2, MNLI, RTE, Multi30KIMDB, NewsCategory, CLINC150OoD: Background ShiftIMDB SST-2\nSame Domain ShiftNewsCategory-ID NewsCategory-OOD\nTable 1: Settings of ID-OOD dataset pairs\nsoftmax probability (MSP) (Hendrycks and Gim-\npel, 2017) and energy score (Liu et al., 2020). They\nderive OOD scores from the confidence or logits\nfrom the classification head of the model.\n4 Experimental Setup\nDatasets We adopt the benchmark in Hendrycks\net al. (2020) and Zhou et al. (2021), examining 9\ndiverse ID-OOD dataset pairs. Specifically, we\nuse the IMDB dataset (Maas et al., 2011) and\nSST-2 (Socher et al., 2013) on sentiment analy-\nsis, the 20NewsGroups (20NG) dataset (Lang,\n1995) on topic classification, the RTE (Wang\net al., 2018) and MNLI (Williams et al., 2018)\non natural language inference, the English side of\nMulti30k (Elliott et al., 2016) on machine trans-\nlation, the cross-intent dataset CLINC150 (Larson\net al., 2019), and the NewsCategory multiclass\nclassification dataset (Misra, 2018). Details of the\ndata preparation are described in Appendix A.\nWith these datasets, we examine two main set-\ntings: out-of-domain (OoD) shift where ID and\nOOD examples come from different datasets (i.e.,\ndomains), and same-domain (SD) shift where ID\nand OOD examples come from the same domain\nbut have disjoint sets of classes. In the OoD set-\nting, we further categorize the ID-OOD pairs into\nthe semantic shift and background shift. Particu-\nlarly, IMDB and SST-2 are both sentiment anal-\nysis datasets that have the same set of classes but\nconsist of examples from different domains. In\nthe same-domain setting, we split the NewsCat-\negory dataset, where we make disjoint sets of\nclasses as ID and OOD (Appendix A).\nModels We use RoBERTa (Liu et al., 2019), which\nis a commonly used pre-trained language model\nlike BERT (Kenton and Toutanova, 2019). Both\nmodels have been used in prior work on OOD detec-\ntion (Podolskiy et al., 2021; Hendrycks et al., 2020),\nbut we choose RoBERTa as the diverse data it is\npre-trained on has been shown to make it stronger\nfor OOD detection (Zhou et al., 2021; Podolskiy\net al., 2021; Hendrycks et al., 2020). We use em-\nbeddings of the beginning-of-sentence (BOS) token\nas the sentence representation, and compare this\nto alternate approaches in Appendix C. Following\nZhou et al. (2021), we fine-tune RoBERTa-base on\ndownstream datasets for 10 epochs. For SupCon,\nwe use a joint objective with Cross Entropy, with\nweight α = 2 to the SupCon loss. For TAPT, we\npre-train the model for 3 epochs on the ID data.\nFor distance-based OOD detection, we use sen-\ntence embeddings from the penultimate layer. We\nfine-tune all layers using Adam, with batch size 4,\nlearning rate 10−5, and weight decay 0.01. Further\ndetails of implementation and configurations are in\nAppendix G.\nEvaluation Metrics We report the following stan-\ndard metrics: (1) the false positive rate ( FPR95)\nof OOD samples when the true positive rate of ID\nsamples is at 95%, (2) the area under the receiver\noperating characteristic curve (AUROC), (3) the\narea under the precision-recall curve (AUPR), and\n(4) ID classification accuracy (ID ACC).\n5 Results and Analysis\n5.1 Out-of-domain detection with pre-trained\nlanguage models is near perfect\nTable 2 shows the pre-trained model outperform-\ning all its fine-tuned variants in the out-of-domain\nshift setting, and achieving near-perfect OOD de-\ntection on all ID-OOD pairs considered. In addition\nto comparisons with three fine-tuning objectives,\nwe also compare with a competitive baseline pro-\nposed by Zhou et al. (2021), which fine-tunes a\nmodel with a novel contrastive objective. Taking\n20NewsGroups (ID) vs. RTE (OOD) as an ex-\nample, OOD detection with the best fine-tuning\nstrategy (i.e., SupCon) yields an FPR95 of 24.8%.\nIn sharp contrast, zero-shot OOD detection using\nthe pre-trained language model can perfectly detect\nRTE as OOD with 0% FPR95. We investigate\nsame-domain shift in-depth later in Section 5.3.\nFigure 1 sheds some light on the strong perfor-\nmance of pre-trained language models for out-of-\ndomain detection. In the leftmost figure, we ob-\nserve that large pre-trained language models create\nseparate domain clusters of sentence embeddings\nfor ID and OOD data, matching the findings of Aha-\nroni and Goldberg (2020). The strong separation of\nclusters boosts the performance of distance-based\nOOD detection. In contrast, fine-tuning induces a\nmodel to divide a single domain cluster into mul-\ntiple class clusters. When a fine-tuned model en-\ncounters an OOD datapoint, it attempts to classify\n12816\nKNN(non-parametric) Mahalanobis(parametric)\nID→OOD Pair Training AUROC↑ AUPR (In)↑ AUPR (Out)↑ FPR95↓ AUROC↑ AUPR (In)↑ AUPR (Out)↑ FPR95↓\nOut-of-Domain: Semantic Shift\nZhou et al. 0.935 0.982 0.664 0.713 0.978 0.994 0.865 0.015\nCE 0.973 0.991 0.923 0.155 0.981 0.994 0.942 0.087\n20NG→SST-2 TAPT 0.969 0.990 0.903 0.169 0.981 0.994 0.939 0.088\nSupCon 0.969 0.990 0.909 0.180 0.980 0.994 0.943 0.094\nPre-trained1.000 1.000 1.000 0.000 1.000 1.000 1.000 0.000\nZhou et al. 0.935 0.929 0.950 0.718 0.964 0.955 0.978 0.224\nCE 0.954 0.898 0.984 0.263 0.968 0.925 0.989 0.166\n20NG→MNLI TAPT 0.950 0.887 0.982 0.263 0.964 0.910 0.988 0.175\nSupCon 0.954 0.899 0.984 0.265 0.970 0.932 0.990 0.156\nPre-trained1.000 0.999 1.000 0.000 1.000 0.999 1.000 0.000\nZhou et al. 0.934 0.972 0.780 0.594 0.956 0.981 0.860 0.312\nCE 0.922 0.958 0.858 0.410 0.945 0.970 0.902 0.285\n20NG→RTE TAPT 0.898 0.942 0.822 0.455 0.919 0.952 0.869 0.352\nSupCon 0.923 0.959 0.858 0.393 0.952 0.975 0.914 0.248\nPre-trained1.000 1.000 0.999 0.000 1.000 1.000 0.999 0.000\nZhou et al. 0.954 0.823 0.993 0.261 0.969 0.867 0.996 0.144\nCE 0.951 0.804 0.993 0.292 0.961 0.817 0.995 0.206\n20NG→IMDB TAPT 0.955 0.797 0.994 0.227 0.965 0.804 0.995 0.159\nSupCon 0.958 0.826 0.994 0.234 0.970 0.852 0.996 0.150\nPre-trained0.988 0.970 0.998 0.019 0.990 0.975 0.998 0.012\nZhou et al. 0.932 0.977 0.708 0.851 0.980 0.993 0.888 0.005\nCE 0.949 0.976 0.898 0.264 0.962 0.982 0.920 0.175\n20NG→Multi30K TAPT 0.940 0.970 0.886 0.258 0.956 0.978 0.922 0.167\nSupCon 0.937 0.969 0.887 0.294 0.955 0.977 0.918 0.201\nPre-trained1.000 1.000 1.000 0.000 1.000 1.000 1.000 0.000\nZhou et al. 0.928 0.921 0.937 0.765 0.955 0.948 0.969 0.383\nCE 0.939 0.877 0.977 0.339 0.957 0.905 0.984 0.234\n20NG→NewsCategory TAPT 0.931 0.853 0.973 0.343 0.947 0.874 0.981 0.243\nSupCon 0.938 0.877 0.976 0.354 0.962 0.919 0.986 0.219\nPre-trained1.000 0.999 1.000 0.000 1.000 0.999 1.000 0.000\nZhou et al. 0.952 0.992 0.601 0.388 0.988 0.998 0.870 0.005\nCE 0.953 0.991 0.816 0.247 0.964 0.993 0.844 0.189\n20NG→CLINC150 TAPT 0.944 0.989 0.769 0.296 0.959 0.992 0.830 0.213\nSupCon 0.940 0.988 0.761 0.343 0.957 0.992 0.821 0.230\nPre-trained1.000 1.000 1.000 0.000 1.000 1.000 1.000 0.000\nOut-of-Domain: Background Shift\nCE 0.865 0.994 0.147 0.741 0.893 0.996 0.231 0.618\nIMDB→SST-2 TAPT 0.857 0.994 0.137 0.746 0.877 0.995 0.172 0.683\nSupCon 0.838 0.993 0.119 0.824 0.865 0.995 0.149 0.800\nPre-trained0.967 0.999 0.582 0.210 0.996 1.000 0.860 0.004\nSame Domain Shift\nCE 0.925 0.922 0.933 0.465 0.877 0.815 0.912 0.467\nNewsCategory-ID→ TAPT 0.918 0.917 0.924 0.513 0.876 0.822 0.907 0.502\nNewsCategory-OOD SupCon 0.925 0.922 0.933 0.465 0.877 0.815 0.912 0.467\nPre-trained0.816 0.839 0.806 0.845 0.550 0.458 0.628 0.939\nTable 2: Comparison of OOD detection performance of pre-trained and fine-tuned models. Pre-trained language\nmodels are near-perfect OOD detectors in the out-of-domain setting, but worst in the same-domain shift setting.\nit by mapping it to one of the existing ID class clus-\nters. However, due to the distributional difference\nof the datapoint, the model is unable to perfectly\nmap such a point and OOD points end up in the\nspace between the ID class clusters most similar to\nit. Fine-tuned representations of the data thus make\ndistance-based OOD detection more challenging.\n5.2 What’s the best way of fine-tuning for\nOOD detection?\nWhile pre-trained models show strong out-of-\ndomain detection performance, they lack the classi-\nfication ability on the ID dataset. This is expected\nsince the models are not optimized for the down-\nstream classification task. Thus, we raise the next\nquestion: How can we fine-tune the model to ac-\ncurately classify ID data while having reasonable\nOOD detection performance?\nTo answer this question, we comprehensively\ncompare three fine-tuning objectives ( c.f. Sec-\ntion 3.2), coupled with different OOD detection\nmethods. Figure 2 depicts the effect of fine-tuning\nfor OOD detection, for both semantic shift (top:\n20NewsGroups vs. RTE) and background shift\n(middle: IMDB vs. SST-2). We highlight three\nkey observations: (1) For distance-based methods,\n12817\nFigure 1: Comparison of data representations from the penultimate layer of pre-trained and fine-tuned models.\nFrom left to right: (1) Pre-trained model, (2) Fine-tuning with Cross-Entropy (CE), (3) Fine-tuning with TAPT,\nand (4) Fine-tuning with SupCon. The ID dataset, 20NewsGroups, is shown in maroon, while the OOD datasets\nRTE and SST-2 are in yellow and purple respectively. The pretrained model represents each domain as a separate\ncluster, strengthening distance-based OOD performance. Fine-tuning encourages the model to learn class-specific\nclusters, making distance based OOD detection more challenging.\nthe OOD detection performance worsens as the\nnumber of fine-tuning epochs increases, highlight-\ning that early stopping is the key to strong OOD\ndetection performance. For example, on 20News-\nGroups (ID) vs. RTE (OOD), the model trained\nwith TAPT for 1 epoch yields an AUROC of 95.5%\n(with Mahalanobis), which declines to 91.9% af-\nter 10 epochs of fine-tuning. To the best of our\nknowledge, we are the first to show the importance\nof early stopping on fine-tuning language models\nfor distance-based OOD detection. (2) Irrespective\nof the fine-tuning objectives, distance-based OOD\ndetection methods consistently outperform output-\nbased methods, particularly MSP using softmax\nconfidence (Hendrycks and Gimpel, 2017) and en-\nergy score using logits (Liu et al., 2020). (3) Under\nsemantic shift, out-of-domain detection using any\nof the three fine-tuning objectives displays simi-\nlar performance on most ID-OOD pairs, bearing a\nlarge gap w.r.t.the pre-trained language model.\nLinear Probing is Suboptimal To perform clas-\nsification while preserving the OOD detection per-\nformance of a pre-trained model, one possible so-\nlution is linear probing (Alain and Bengio, 2016),\ni.e., fine-tuning the classification head to the down-\nstream task, while keeping the weights of the pre-\ntrained model backbone unchanged. However, in\nFigure 6 (Appendix), we show that linear prob-\ning does not yield competitive classification per-\nformance. In particular, we observe the strongest\nfine-tuning objective (TAPT) only obtains an ID\naccuracy of 61% after 100 epochs of fine-tuning,\ncompared to full network fine-tuning where an ac-\ncuracy of 86% is achieved in 10 epochs.\n5.3 Investigation on same-domain data shifts\nIn this subsection, we further investigate a more\nchallenging type of data shift, where the test sam-\nples are from the same domain and thus can be\ndistributionally very close to the ID data. This\nis in contrast to our evaluations in Sections 5.1\nand 5.2, where the OOD samples are from different\ndomains. To simulate same-domain shifts, we split\nthe NewsCategory dataset into two sets with dis-\njoint classes: one for ID, and another for OOD. The\ndomain for both sets of classes is identical, while\nthe semantic label sets are different. The allocation\nof classes is described in Table 5 (Appendix A).\nFigure 2 (bottom) shows the effect of fine-tuning\nfor detection in this challenging setup of same-\ndomain shifts. A salient observation is that fine-\ntuning consistently improves OOD detection perfor-\nmance, across all training objectives. To better un-\nderstand why the pre-trained model underperforms\nin this case, in Figure 3, we plot feature represen-\ntations, before and after fine-tuning, respectively.\nAs seen in the left of Figure 3, when both ID and\nOOD data are sampled from the same domain, their\nembeddings are highly overlapping. This explains\nthe suboptimal performance of directly employing\nembeddings from the pre-trained language model.\nIn contrast, fine-tuning creates stronger separability\nbetween ID and OOD data. Table 3 quantitatively\nconfirms that fine-tuning leads to stronger ID-OOD\nseparability (c.f. Equation 2).\n5.4 Deeper look at embedding quality\nWe quantitatively measure the embeddings pro-\nduced by both pre-trained and fine-tuned language\nmodels. We adopt the following three metrics as\n12818\nFigure 2: Effect of fine-tuning on ID accuracy and OOD detection performance, across different objectives and\ndetection methods. From left to right: (1) ID Accuracy, AUROC with (2) CE, (2) TAPT, and (3) SupCon losses.\nFrom top to bottom: OoD semantic shift, OoD background shift, and same-domain (SD) shift. The X-axis shows the\nnumber of fine-tuning epochs, with ‘0’ indicating the pre-trained model. The Y-axis shows either the ID accuracy or\nthe AUROC. Actual values can be found in Appendix D.\nFigure 3: Comparison of data representations in the\npenultimate layer of pre-trained vs. fine-tuned mod-\nels for same-domain data shifts. Here we split the\nNewsCategory dataset into two parts with disjoint\nclasses: one for ID, and another for OOD. ID data is\nshown in blue, while OOD data is in yellow. Left: Pre-\ntrained model. Right: Fine-tuned with cross-entropy\nloss. Fine-tuning encourages the model to separate the\nembeddings into individual class clusters.\nin Ming et al. (2023): (1) inter-class dispersion,\nwhich is the average cosine similarity among pair-\nwise class centroids, (2) intra-class compactness,\nwhich measures the average cosine similarity be-\ntween each feature embedding and its correspond-\ning class centroid, and (3) ID-OOD separability,\nwhich functions as a measure of domain gap be-\nTraining ID-OOD Separability ↑\nCE 12.235\nTAPT 12.489\nSupCon 7.549\nPre-trained 0.138\nTable 3: Effect of fine-tuning on ID-OOD separability,\nfor same-domain (SD) shift with the NewsCategory\ndataset. Fine-tuning for a single epoch helps separate\noverlapping ID and OOD data into dispersed clusters.\ntween ID and OOD. Formally,\nDisp.(↑) = 1\nC\nC∑\ni=1\n1\nC−1\nC∑\nj=1\nµi ·µj1 {i̸= j}\nComp.(↓) = 1\nC\nC∑\nj=1\n1\nN\nN∑\ni=1\nzi ·µj1 {yi = j}\nSep.(↑) = 1\n|Dtestout |\n∑\nx′∈Dtest\nout\nmax\nj∈Y\nzx′ ·µj\n− 1\n|Dtest\nin |\n∑\nx∈Dtest\nin\nmax\nj∈Y\nzx ·µj,\n(2)\nwhere µi is the average of embeddings for samples\nin class i, and z is the L2 normalized embedding.\n12819\nID Objective ID Accuracy↑ Dispersion↑ Compactness↓(in degree) (in degree)\nCE 0.791 90.994 19.57520NewsGroupsTAPT 0.807 91.753 18.902SupCon 0.763 89.354 21.987Pre-trained 0.053 1.514 4.326\nCE 0.938 87.041 21.787IMDB TAPT 0.940 76.871 15.894SupCon 0.928 135.550 19.245Pre-trained 0.500 0.636 6.058\nCE 0.745 88.701 33.878NewsCategoryTAPT 0.756 88.216 33.509SupCon 0.667 63.392 30.793Pre-trained 0.050 3.086 9.210\nTable 4: Quality of ID embeddings generated by pre-\ntrained and fine-tuned models, quantified by accuracy\non the ID test set, inter-class dispersion, and intra-\nclass compactness. The fine-tuned models show well-\nseparated and compact class clusters, while the pre-\ntrained model shows a single domain cluster, a sub-\noptimal setting for downstream classification. Fine-\ntuned models are trained for a single epoch.\nTable 4 shows us that fine-tuning encourages the\nmodel to embed the data into well-separated class\nclusters with high inter-class dispersion (measured\nin angular degrees). In contrast, the pre-trained\nmodel represents the entire domain as a homo-\ngeneous cluster containing data from all classes.\nInterestingly, the pre-trained model displays the\nstrongest compactness, indicating the closeness\namong ID data points in the original representa-\ntion space. Note that the ID accuracy is random\nfor the pre-trained model, which is expected. Dis-\npersion and compactness monotonically improve\nthrough fine-tuning, further indicating that fine-\ntuning encourages the model to project the data\ninto well-separated and compact class-wise clus-\nters. However, Figure 4 shows us that while fine-\ntuning improves ID-OOD separability for the same-\ndomain shift, it has less impact on out-of-domain\nshifts. (Actual values and results for other objec-\ntives can be found in Appendix D.) This trend also\nechos our previous observations in Section 5.2 and\nSection 5.3, on OOD detection performance.\n6 Related Work\nThe problem of OOD detection is different from do-\nmain adaptation (Ramponi and Plank, 2020), where\na model is trained to generalize to a known tar-\nget domain with the same label space. It is also\ndifferent from selective prediction where a model\nabstains only when its confidence is low, irrespec-\ntive of domain (El-Yaniv et al., 2010; Geifman and\nEl-Yaniv, 2017; Kamath et al., 2020).\nFigure 4: Effect of fine-tuning (w/ SupCon loss) on the\nID-OOD separability. The X-axis shows the number\nof fine-tuning epochs, and the Y-axis shows ID-OOD\nseparability (in angular degrees).\nOOD Detection Methods A popular baseline is\nthe calibration method Maximum Softmax Proba-\nbility (MSP) (Hendrycks and Gimpel, 2017), that\ndirectly uses maximum class probability produced\nby the logits of a trained classifier. However, pre-\ndictive confidence has been shown to be undesir-\nably high for OOD samples, making MSP ineffec-\ntive (Nguyen et al., 2015; Wei et al., 2022; Shen\net al., 2021). Liu et al. (2020) propose using en-\nergy score for OOD detection, which better dis-\ntinguishes in- and out-of-distribution samples than\nsoftmax scores. ReAct (Sun et al., 2021) improves\nthe energy score by introducing a rectified activa-\ntion, which reduces model overconfidence in OOD\ndata. Sun and Li (2022) utilize logit sparsifica-\ntion to enhance the vanilla energy score. More\nrecently, detection methods that utilize distances\nof samples in representation space, have risen as a\npromising class of OOD detection methods in both\nthe vision (Mandelbaum and Weinshall, 2017; Lee\net al., 2018; Sun et al., 2022; Ming et al., 2023) and\nmulti-modal (Ming et al., 2022) regimes.\nOOD Detection in NLP In the realm of NLP,\nmodel confidence using sentence embeddings has\nbeen shown to be a strong baseline with pre-trained\ntransformers (Hendrycks et al., 2020; Desai and\nDurrett, 2020). Contrastive learning (Khosla et al.,\n2020; Gao et al., 2021; Jin et al., 2022) minimizes\nintra-class variance, leading to stronger OOD de-\ntection, especially in low data regimes (Zeng et al.,\n2021), and with Mahalanobis distance (Zhou et al.,\n2021; Podolskiy et al., 2021). Detection perfor-\nmance has also been strengthened using data aug-\n12820\nmentation (Chen and Yu, 2021; Rawat et al., 2021),\ndiscriminative training (Zhan et al., 2021), mutual\ninformation maximization (Nimah et al., 2021), en-\nsembles (Li et al., 2021) and prototypical networks\nin the few-shot setup (Tan et al., 2019). While most\nprevious works perform fine-tuning on the ID data,\nwe provide a comprehensive understanding on di-\nrectly using the pre-trained model for zero-shot\nOOD detection.\nPre-trained vs Fine-tuned Pre-trained language\nmodels have been shown to learn implicit sentence\nrepresentations, forming unsupervised domain clus-\nters (Aharoni and Goldberg, 2020). Andreassen\net al. (2021) and Kumar et al. (2021) showed that\nfine-tuning distorts pre-trained features, worsening\naccuracy on OOD generalization. However, to the\nbest of our knowledge, we are the first to explore\nthe effect of directly using pre-trained language\nmodels for OOD detection. Related to our work,\nMing et al. (2022) show that pre-trained models\ncan be used for zero-shot OOD detection. Differ-\nent from ours, they perform OOD detection in the\nmulti-modal space and calculate distances between\nthe visual and textual representations.\n7 Conclusion\nIn this paper, we explore the simple and effective\nsetting of zero-shot OOD detection with pre-trained\nlangage models. Our work departs from prior lit-\nerature that typically requires fine-tuning on the\nID data. Extensive evaluations demonstrate that\npre-trained models are near-perfect for OOD de-\ntection when the test data comes from a different\ndomain. We additionally investigate the effect of\nfine-tuning on OOD detection, and identify strate-\ngies to achieve both strong OOD detection perfor-\nmance and ID accuracy. We perform both quali-\ntative and quantitative analysis on the embedding\ncharacteristics, explaining the strong performance\nof our method. We hope our work will inspire fu-\nture work to the strong promise of using pre-trained\nmodels for OOD detection.\nEthical Considerations\nOur project aims to improve the reliability and\nsafety of large language models, which can be frag-\nile under distribution shift (Ribeiro et al., 2020) and\nincur great costs (Ulmer et al., 2020; Zhang et al.,\n2021). By properly flagging anomalous data, our\nmethod can lead to direct benefits and societal im-\npacts, particularly for safety-critical applications.\nFrom a user’s perspective, our method can help\nimprove trust in the language models. Our study\ndoes not involve any human subjects or violation\nof legal compliance. We do not anticipate any po-\ntentially harmful consequences to our work. As\ndetailed in Appendix A, all of our experiments are\nconducted using publicly available datasets. Our\ncode has been released for reproducibility. Through\nour study and releasing our code, we hope to raise\nstronger research and societal awareness toward the\nproblem of out-of-distribution detection in natural\nlanguage processing.\nLimitations\nWe provide a comprehensive study on the efficacy\nof leveraging pre-trained language models for zero-\nshot OOD detection. Our method is thus limited\nto the setting of abstaining from prediction on all\nOOD data. This is more conservative than selec-\ntive prediction, where the model must make pre-\ndictions over as many ID & OOD points as possi-\nble while maintaining high accuracy. Despite this,\nOOD detection has lower risks to high-risk and\nsafety-critical applications, where rare and anoma-\nlous data is more reasonably flagged to the expert.\nWe believe our work provides new values and in-\nsights to the research community, especially on\nsafe handling of distributional shifts when deploy-\ning pre-trained language models.\nAs discussed in our Ethical Considerations, the\nOOD detection problem is of significant use in\nhigh-risk settings, and should be incorporated into\nproduction-level pipelines. However, for the same\nreason, the OOD detection models must be also\nreliable to avoid any risk to the downstream appli-\ncations.\nAcknowledgements\nLi is supported in part by the AFOSR Young Inves-\ntigator Award under No. FA9550-23-1-0184; UL\nResearch Institutes through the Center for Advanc-\ning Safety of Machine Intelligence; Philanthropic\nFund from SFF; and faculty research awards from\nGoogle, Meta, and Amazon. Hu is supported in\npart by a gift fund from ProtagoLabs. Any opin-\nions, findings, conclusions, or recommendations\nexpressed in this material are those of the authors\nand do not necessarily reflect the views, policies,\nor endorsements either expressed or implied, of the\nsponsors. We would like to thank Yifei Ming and\nthe anonymous reviewers for helpful comments.\n12821\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7747–\n7763.\nGuillaume Alain and Yoshua Bengio. 2016. Under-\nstanding intermediate layers using linear classifier\nprobes. arXiv preprint arXiv:1610.01644.\nAnders Johan Andreassen, Yasaman Bahri, Behnam\nNeyshabur, and Rebecca Roelofs. 2021. The evo-\nlution of out-of-distribution robustness throughout\nfine-tuning. Transactions on Machine Learning Re-\nsearch.\nUdit Arora, William Huang, and He He. 2021. Types\nof out-of-distribution texts and how to detect them.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n10687–10701.\nDerek Chen and Zhou Yu. 2021. Gold: Improving\nout-of-scope detection in dialogues using data aug-\nmentation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 429–442.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 295–302.\nRan El-Yaniv et al. 2010. On the foundations of noise-\nfree selective classification. Journal of Machine\nLearning Research, 11(5).\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lu-\ncia Specia. 2016. Multi30k: Multilingual english-\ngerman image descriptions. In Proceedings of the\n5th Workshop on Vision and Language, pages 70–74.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6894–6910.\nYonatan Geifman and Ran El-Yaniv. 2017. Selective\nclassification for deep neural networks. Advances in\nneural information processing systems, 30.\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Veselin\nStoyanov. 2020. Supervised contrastive learning for\npre-trained language model fine-tuning. In Interna-\ntional Conference on Learning Representations.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline\nfor detecting misclassified and out-of-distribution ex-\namples in neural networks. In 5th International Con-\nference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference Track\nProceedings.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2744–2751.\nDi Jin, Shuyang Gao, Seokhwan Kim, Yang Liu,\nand Dilek Hakkani-Tür. 2022. Towards textual\nout-of-domain detection without in-domain labels.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 30:1386–1395.\nAmita Kamath, Robin Jia, and Percy Liang. 2020. Se-\nlective question answering under domain shift. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5684–\n5696.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of NAACL-HLT, pages 4171–4186.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\npervised contrastive learning. Advances in Neural\nInformation Processing Systems, 33:18661–18673.\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew\nJones, Tengyu Ma, and Percy Liang. 2021. Fine-\ntuning can distort pretrained features and underper-\nform out-of-distribution. In International Conference\non Learning Representations.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nKen Lang. 1995. Newsweeder: Learning to filter net-\nnews. In Machine Learning Proceedings 1995, pages\n331–339. Elsevier.\nStefan Larson, Anish Mahendran, Joseph J Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K Kummerfeld, Kevin Leach, Michael A\nLaurenzano, Lingjia Tang, et al. 2019. An evaluation\ndataset for intent classification and out-of-scope pre-\ndiction. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n1311–1316.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018. A simple unified framework for detecting out-\nof-distribution samples and adversarial attacks. Ad-\nvances in neural information processing systems, 31.\n12822\nXiaoya Li, Jiwei Li, Xiaofei Sun, Chun Fan, Tianwei\nZhang, Fei Wu, Yuxian Meng, and Jun Zhang. 2021.\nkfolden: k-fold ensemble for out-of-distribution\ndetection-fold ensemble for out-of-distribution de-\ntection. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3102–3115.\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan\nLi. 2020. Energy-based out-of-distribution detection.\nAdvances in Neural Information Processing Systems,\n33:21464–21475.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the associ-\nation for computational linguistics: Human language\ntechnologies, pages 142–150.\nPrasanta Chandra Mahalanobis. 2018. On the gener-\nalized distance in statistics. Sankhy¯a: The Indian\nJournal of Statistics, Series A (2008-), 80:S1–S7.\nAmit Mandelbaum and Daphna Weinshall. 2017.\nDistance-based confidence score for neural network\nclassifiers. arXiv preprint arXiv:1709.09844.\nYifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun,\nWei Li, and Yixuan Li. 2022. Delving into out-of-\ndistribution detection with vision-language represen-\ntations. In Advances in Neural Information Process-\ning Systems.\nYifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li.\n2023. How to exploit hyperspherical embeddings for\nout-of-distribution detection? In Proceedings of the\nInternational Conference on Learning Representa-\ntions.\nRishabh Misra. 2018. News category dataset. DOI:\nDOI: https://doi. org/10.13140/RG, 2(20331.18729).\nAnh Nguyen, Jason Yosinski, and Jeff Clune. 2015.\nDeep neural networks are easily fooled: High con-\nfidence predictions for unrecognizable images. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 427–436.\nIftitahu Nimah, Meng Fang, Vlado Menkovski, and\nMykola Pechenizkiy. 2021. Protoinfomax: Prototyp-\nical networks with mutual information maximization\nfor out-of-domain detection. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2021,\npages 1606–1617.\nEllie Pavlick and Joel Tetreault. 2016. An empiri-\ncal analysis of formality in online communication.\nTransactions of the Association for Computational\nLinguistics, 4:61–74.\nAlexander Podolskiy, Dmitry Lipin, Andrey Bout, Eka-\nterina Artemova, and Irina Piontkovskaya. 2021. Re-\nvisiting mahalanobis distance for transformer-based\nout-of-domain detection. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 35,\npages 13675–13682.\nAlan Ramponi and Barbara Plank. 2020. Neural un-\nsupervised domain adaptation in nlp—a survey. In\nProceedings of the 28th International Conference on\nComputational Linguistics, pages 6838–6855.\nMrinal Rawat, Ramya Hebbalaguppe, and Lovekesh\nVig. 2021. Pnpood: Out-of-distribution detection for\ntext classification via plug andplay data augmentation.\narXiv preprint arXiv:2111.00506.\nJie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan\nPoplin, Mark Depristo, Joshua Dillon, and Balaji\nLakshminarayanan. 2019. Likelihood ratios for out-\nof-distribution detection. Advances in neural infor-\nmation processing systems, 32.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Behav-\nioral testing of nlp models with checklist. InProceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4902–4912.\nWalter J Scheirer, Anderson de Rezende Rocha,\nArchana Sapkota, and Terrance E Boult. 2012. To-\nward open set recognition. IEEE transactions on pat-\ntern analysis and machine intelligence, 35(7):1757–\n1772.\nYilin Shen, Yen-Chang Hsu, Avik Ray, and Hongxia\nJin. 2021. Enhancing the generalization for intent\nclassification and out-of-domain detection in slu. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2443–\n2453.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nYiyou Sun, Chuan Guo, and Yixuan Li. 2021. React:\nOut-of-distribution detection with rectified activa-\ntions. In Advances in Neural Information Processing\nSystems.\nYiyou Sun and Yixuan Li. 2022. Dice: Leveraging\nsparsification for out-of-distribution detection. In\nEuropean Conference on Computer Vision.\nYiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li.\n2022. Out-of-distribution detection with deep nearest\nneighbors. In International Conference on Machine\nLearning (ICML). PMLR.\n12823\nMing Tan, Yang Yu, Haoyu Wang, Dakuo Wang, Saloni\nPotdar, Shiyu Chang, and Mo Yu. 2019. Out-of-\ndomain detection for low-resource text classification\ntasks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3566–3572.\nDennis Ulmer, Lotta Meijerink, and Giovanni Cinà.\n2020. Trust issues: Uncertainty estimation does\nnot enable reliable ood detection on medical tabu-\nlar data. In Machine Learning for Health , pages\n341–354. PMLR.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nFeng Wang and Huaping Liu. 2021. Understanding\nthe behaviour of contrastive loss. In Proceedings of\nthe IEEE/CVF conference on computer vision and\npattern recognition, pages 2495–2504.\nHongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng,\nBo An, and Yixuan Li. 2022. Mitigating neural\nnetwork overconfidence with logit normalization.\nIn International Conference on Machine Learning\n(ICML). PMLR.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122.\nKeyang Xu, Tongzheng Ren, Shikun Zhang, Yihao\nFeng, and Caiming Xiong. 2021. Unsupervised out-\nof-domain detection via pre-trained transformers. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1052–\n1061.\nYuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.\n2007. On early stopping in gradient descent learning.\nConstructive Approximation, 26(2):289–315.\nZhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu,\nYanan Wu, Hong Xu, Huixing Jiang, and Weiran Xu.\n2021. Modeling discriminative representations for\nout-of-domain detection with supervised contrastive\nlearning. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 870–878.\nLi-Ming Zhan, Haowen Liang, Bo Liu, Lu Fan, Xiao-\nMing Wu, and Albert YS Lam. 2021. Out-of-scope\nintent detection with self-supervision and discrimi-\nnative training. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 3521–3532.\nOliver Zhang, Jean-Benoit Delbrouck, and Daniel L\nRubin. 2021. Out of distribution detection for med-\nical images. In Uncertainty for Safe Utilization of\nMachine Learning in Medical Imaging, and Perina-\ntal Imaging, Placental and Preterm Image Analysis,\npages 102–111. Springer.\nWenxuan Zhou, Fangyu Liu, and Muhao Chen. 2021.\nContrastive out-of-distribution detection for pre-\ntrained transformers. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1100–1111.\n12824\nA Preparation of Evaluation Benchmarks\nFor ID data, we use the train splits of the IMDB\ndataset on sentiment analysis (Maas et al., 2011),\nand the 20NewsGroups dataset on topic clas-\nsification (Lang, 1995). For OOD data, we use\nthe test splits of IMDB and 20NewsGroups, as\nwell as the test splits from the sentiment classifi-\ncation dataset SST-2 (Socher et al., 2013), Natu-\nral Language Inference datasets RTE (Wang et al.,\n2018) and MNLI (Williams et al., 2018), the En-\nglish source side of machine translation dataset\nMulti30k (Elliott et al., 2016), and the cross in-\ntent dataset CLINC150 (Larson et al., 2019). For\nMNLI, we use both the matched and mismatched\ntest sets. For Multi30k, we combine the flickr\n2016 English test set, mscoco 2017 English test set,\nand filckr 2018 English test. For CLINC150, we\nuse the ‘out of scope’ class as the test set.\nInspired by Arora et al. (2021), we evaluate the\ndetection performance under same-domain shift us-\ning the NewsCategory (Misra, 2018) dataset.\nWe create two disjoint sets of classes, used as\nID and OOD respectively. The domain for both\nsets of classes is identical, while the label sets dif-\nfer. Notably, the NewsCategory dataset con-\ntains classes with similar semantics, for example\n‘Arts’ and ‘Arts & Culture’. To ensure the seman-\ntic distinction between the ID and OOD classes,\nwe categorize semantically similar classes to be\nentirely in either ID or OOD sets. The allocation\nof classes is summarized in Table 5. The dataset\nalso has a strong class imbalance, so we sample\ndata points according to a multinomial distribution,\nfollowing Lample and Conneau (2019). Figure 5\nshows the class frequencies before and after sam-\npling.\nMore statistics about each dataset is available\nin Table 6. The listed datasets are intended for\nresearch purposes only. We do not make any com-\nmercial use of them.\nB Ablation on the Effect of Layers\nThe RoBERTa architecture consists of a backbone\nof multiple transformer layers, followed by a task-\nspecific head on top. For the classification task,\nthis task-specific head consists of a dense layer fol-\nlowed by a classification projection layer. Zhou\net al. (2021) use the features from after the dense\nlayer for OOD detection. Instead, we use the fea-\ntures from before this layer. Table 7 shows the\nOOD detection performance using the representa-\nID Classes OOD Classes\nPolitics Style & Beauty\nThe Worldpost Style\nWorldpost Arts\nWorld News Arts & Culture\nImpact Culture & Arts\nCrime Food & Drink\nMedia Taste\nBusiness College\nMoney Education\nFifty Science\nGood News Tech\nQueer V oices Sports\nBlack V oices Wellness\nWomen Healthy Living\nLatino V oices Travel\nReligion Home & Living\nWeird News Parenting\nParents\nWeddings\nDivorce\nEntertainment\nComedy\nEnvironment\nGreen\nTable 5: Division of classes in the NewsCategory\ndataset into disjoint ID and OOD sets.\ntions from after the dense layer. Table 7 displays a\nworse performance than our main results in Table 2,\nwhere the representations from before the dense\nlayer are used. Using the representations from be-\nfore the task-specific head also makes zero-shot\nOOD detection possible, where the task-specific\nhead is randomly initialized, but weights from the\nbackbone of the pre-trained model are used.\nC Generation of Sequence Embeddings\nOur experiments in the main paper use sentence em-\nbeddings obtained from the beginning-of-sentence\n(BOS) token. This practice is standard for most\nBERT-like models, including RoBERTa, which we\nuse for our experiments. Prior work has also shown\nthat using the average of all token embeddings can\nlead to the formation of similar domain-based clus-\nters (Aharoni and Goldberg, 2020).\nIn this section, we compare this approach with\nthe alternate approach of obtaining sequence em-\nbeddings as the average of all token embeddings in\nthe sequence. Table 8 shows that both approaches\nyield almost identical performance on the OOD\ndetection task.\n12825\nDataset Domain Language License Statistics\nTrain Val Test\nIMDB Large Movie Review Dataset English Unknown 25,000 25,000 50,000\n20NewsGroupsNews Articles English Unknown 11314 2000 5532\nSST-2 Movie Reviews English cc-by-4.0 67349 872 1821\nRTE News and Wikipedia text English cc-by-4.0 2490 277 3000\nMNLI Open American National Corpus English cc-by-4.0 392702 19647 19643\nMulti30k Flickr30K, MSCOCO English, German Custom (research-only, non-commercial) N/A N/A 2532\nCLINC150 Intent Classification English cc-by-3.0 15000 3000 1000\nNewsCategoryHuffPost English CC0: Public Domain 64856 4053 17968\nTable 6: Artifacts used in our study. The dataset statistics report the values used in our study. For example, the\nvalues of the NewsCategory dataset are reported after sampling.\nKNN(non-parametric) Mahalanobis(parametric)\nID→OOD Pair Training AUROC↑ AUPR (In)↑ AUPR (Out)↑ FPR95↓ AUROC↑ AUPR (In)↑ AUPR (Out)↑ FPR95↓\nOut-of-Domain: Semantic Shift\nCE 0.967 0.989 0.907 0.193 0.973 0.991 0.918 0.154\n20NG→SST-2 TAPT 0.962 0.988 0.885 0.226 0.971 0.990 0.911 0.164\nSupCon 0.962 0.987 0.889 0.230 0.971 0.990 0.917 0.159\nCE 0.946 0.884 0.981 0.311 0.955 0.900 0.984 0.250\n20NG→MNLI TAPT 0.942 0.875 0.980 0.314 0.952 0.887 0.983 0.253\nSupCon 0.946 0.884 0.981 0.311 0.957 0.904 0.985 0.246\nCE 0.912 0.953 0.839 0.445 0.927 0.960 0.870 0.373\n20NG→RTE TAPT 0.889 0.938 0.806 0.507 0.902 0.944 0.836 0.430\nSupCon 0.911 0.953 0.837 0.445 0.932 0.964 0.879 0.347\nCE 0.943 0.786 0.992 0.339 0.951 0.790 0.993 0.279\n20NG→IMDB TAPT 0.947 0.778 0.993 0.283 0.956 0.782 0.994 0.212\nSupCon 0.952 0.808 0.993 0.277 0.961 0.822 0.995 0.212\nCE 0.941 0.972 0.882 0.296 0.950 0.976 0.895 0.254\n20NG→Multi30K TAPT 0.932 0.967 0.870 0.313 0.942 0.971 0.891 0.247\nSupCon 0.928 0.964 0.869 0.331 0.940 0.970 0.892 0.274\nCE 0.932 0.864 0.974 0.375 0.941 0.878 0.978 0.324\n20NG→NewsCategory TAPT 0.924 0.844 0.971 0.384 0.933 0.852 0.975 0.326\nSupCon 0.929 0.861 0.973 0.396 0.944 0.886 0.979 0.319\nCE 0.946 0.990 0.783 0.285 0.952 0.991 0.800 0.255\n20NG→CLINC150 TAPT 0.935 0.987 0.739 0.343 0.945 0.989 0.774 0.280\nSupCon 0.932 0.987 0.732 0.372 0.943 0.989 0.770 0.319\nOut-of-Domain: Background Shift\nCE 0.856 0.994 0.135 0.784 0.877 0.995 0.171 0.738\nIMDB→SST-2 TAPT 0.852 0.994 0.130 0.765 0.867 0.995 0.136 0.760\nSupCon 0.833 0.993 0.105 0.840 0.859 0.994 0.128 0.834\nSame Domain Shift\nNewsCategory-ID→ CE 0.924 0.924 0.930 0.499 0.887 0.837 0.914 0.490\nNewsCategory-OOD TAPT 0.920 0.920 0.925 0.520 0.881 0.830 0.910 0.501\nSupCon 0.927 0.925 0.935 0.464 0.878 0.817 0.912 0.475\nTable 7: Comparison of fine-tuning objectives with distance-based methods, using the representations from after the\ndense layer and before the classification projection layer.\nD Detailed Performance of Fine-tuning\nfor OOD Detection\nTable 9 summarizes the epoch-wise performance\nwhen fine-tuning on ID data, for the setting of OoD\nsemantic shift. Table 10 shows the same for OoD\nbackground shift, while Table 11 shows this for\nsame-domain (SD) shift.\nE Effect of Temperature in SupCon\nContrastive loss is shown to be a hardness-aware\nloss function, penalizing hard negative samples by\nreducing tolerance to them (Wang and Liu, 2021).\nThe temperature τ has been shown to control the\ntolerance to negative samples. As seen in Figure\n7, low temperature leads to a uniform distribu-\ntion with high separability in the learnt embedding\nspace, but this can reduce tolerance to semanti-\ncally similar samples, breaking underlying seman-\ntic structure. The temperature must be set optimally\nto balance the ‘uniformity-tolerance’ trade-off, hav-\ning some tolerance to semantically similar exam-\nples. When IMDB is ID, we find OOD detection to\nbe optimal at τ = 0.7, since the two classes of the\n12826\nFigure 5: Class frequencies of the NewsCategory\ndataset. The original frequencies in blue show a strong\nclass imbalance, while the modified frequencies in or-\nange are more balanced.\nOOD Embedding AUROC (kNN)↑ FPR (kNN)↓ AUROC (kNN)↑ FPR (kNN)↓\nSST-2 Avg 1.000 1.000 1.000 0.000BOS 1.000 1.000 1.000 0.000\nMNLI Avg 1.000 0.999 1.000 0.000BOS 1.000 0.999 1.000 0.000\nRTE Avg 0.999 0.999 0.997 0.000BOS 1.000 1.000 0.999 0.000\nIMDB Avg 0.986 0.973 0.997 0.008BOS 0.988 0.970 0.998 0.019\nMulti30K Avg 1.000 1.000 1.000 0.000BOS 1.000 1.000 1.000 0.000\nNewsCategory Avg 1.000 0.999 1.000 0.000BOS 1.000 0.999 1.000 0.000\nCLINC150 Avg 1.000 1.000 1.000 0.000BOS 1.000 1.000 1.000 0.000\nTable 8: Comparison of methods to generate sequence\nembeddings. In the OoD Semantic Shift setting, where\n20NewsGroups is the ID dataset, the performance be-\ntween Avg (averaging all token embeddings to get the\nsequence embedding) and BOS (using the first token\nembedding as the sequence embedding) are almost iden-\ntical.\ndataset share semantic similarities. However, with\nthe 20NewsGroups topic classification task, we\nfind a lower value of τ = 0.1 to be optimal. This\nis because a larger number of ID classes requires a\nstronger uniformity in the learnt distribution, and\nthe weaker semantic similarities between classes\nassures that this uniformity does not hurt perfor-\nmance.\nTables 14, 16 and 15 show the effects of varying\nthe temperature parameter τ in the SupCon loss,\non OOD detection, in the settings of OoD semantic\nshift, OoD background shift and same-domain shift.\nAll models are fine-tuned for 10 epochs.\nF Effect of k\nFigure 8 shows us that k = 1 is consistently\nthe optimal k for kNN, across fine-tuning objec-\ntives and distribution shifts. The detection per-\nFigure 6: ID accuracy with linear probing instead of\nfine-tuning, with 20NewsGroups. In comparison to\nfine-tuning with TAPT, where the accuracy after 10\nepochs is 86%, linear probing with TAPT achieves an\naccuracy of about only 61% after 100 epochs.\nFigure 7: Effect of the temperature τ on representa-\ntions trained with the SupCon loss. The ID data is\n20NewsGroups. Left: τ = 0.1. Right: τ = 0.7.\nformance remains strong until k reaches the ID\nclass size, which is between 400 and 600 for\n20NewsGroups. After this point, the nearest\nneighbour for an ID and OOD point will both be\noutside the nearest ID class cluster, making both\ndistances more comparable and harder to distin-\nguish. With pre-trained models, the performance\nremains strong as there is no concept of class clus-\nters and a single domain cluster is instead present.\nG Details on Implementation\nWe use RoBERTa from the HuggingFace library4,\nand use PyTorch to train our models. Hyperparame-\nter search is performed through a grid search. Apart\nfrom the default parameters in the trainer module\nfrom HuggingFace, our selected hyperparameters\nare listed in Table 13.\n4https://github.com/huggingface/\ntransformers\n12827\nTraining Epoch ID Accuracy↑ Dispersion↑ Compactness↓ ID-OOD MSP Energy KNN MahalanobisSeparability↑ AUROC↑ FPR95↓ AUROC↑ FPR95↓ AUROC↑ FPR95↓ AUROC↑ FPR95↓\n1 0.791 89.777 24.303 26.594 0.757 0.687 0.849 0.432 0.934 0.332 0.961 0.2212 0.823 90.632 22.508 26.595 0.790 0.656 0.855 0.421 0.925 0.373 0.956 0.2473 0.840 91.439 20.312 28.570 0.808 0.638 0.864 0.426 0.931 0.344 0.957 0.2294 0.851 91.934 18.293 29.259 0.816 0.658 0.859 0.432 0.931 0.356 0.958 0.238CE 5 0.843 91.643 17.757 29.247 0.808 0.672 0.854 0.450 0.928 0.367 0.953 0.2436 0.855 91.966 16.464 29.579 0.824 0.655 0.855 0.437 0.922 0.380 0.946 0.2627 0.856 92.097 16.210 29.064 0.832 0.691 0.862 0.459 0.919 0.422 0.942 0.2778 0.859 92.170 15.122 28.968 0.829 0.695 0.854 0.472 0.920 0.413 0.945 0.2909 0.858 92.211 14.745 30.084 0.841 0.653 0.863 0.448 0.925 0.393 0.946 0.27410 0.858 92.232 14.261 29.733 0.833 0.684 0.853 0.469 0.922 0.410 0.945 0.285\n1 0.807 90.555 23.987 27.595 0.785 0.646 0.861 0.403 0.929 0.326 0.955 0.2392 0.840 91.058 21.600 27.174 0.784 0.662 0.852 0.418 0.916 0.351 0.942 0.2643 0.841 91.473 20.052 29.920 0.823 0.610 0.875 0.386 0.931 0.323 0.948 0.2504 0.842 91.517 18.602 27.894 0.798 0.677 0.845 0.456 0.910 0.379 0.932 0.293TAPT 5 0.851 91.766 17.315 27.091 0.814 0.680 0.849 0.473 0.909 0.395 0.928 0.3136 0.852 91.916 16.551 28.467 0.819 0.666 0.844 0.487 0.908 0.421 0.926 0.3307 0.857 92.016 15.881 25.505 0.803 0.712 0.824 0.541 0.893 0.486 0.913 0.3938 0.860 92.122 14.934 26.382 0.799 0.701 0.820 0.516 0.897 0.457 0.918 0.3649 0.856 92.149 14.602 26.829 0.808 0.691 0.828 0.508 0.897 0.463 0.918 0.36010 0.861 92.211 14.364 27.151 0.807 0.695 0.826 0.493 0.898 0.455 0.919 0.352\n1 0.763 87.389 26.510 26.239 0.771 0.622 0.866 0.404 0.936 0.327 0.970 0.1802 0.820 89.348 23.556 27.233 0.771 0.661 0.851 0.438 0.935 0.333 0.967 0.2063 0.838 90.452 21.171 26.267 0.760 0.710 0.832 0.487 0.928 0.350 0.962 0.2304 0.842 90.874 20.170 28.124 0.796 0.660 0.859 0.410 0.927 0.343 0.960 0.206SupCon 5 0.851 91.295 18.608 28.033 0.815 0.649 0.865 0.412 0.921 0.382 0.954 0.2726 0.852 91.342 18.493 30.519 0.832 0.616 0.883 0.370 0.934 0.304 0.960 0.2067 0.855 91.736 17.224 28.144 0.818 0.711 0.863 0.448 0.922 0.375 0.954 0.2488 0.853 91.828 16.390 28.809 0.825 0.676 0.863 0.441 0.921 0.386 0.950 0.2539 0.857 91.977 15.999 28.812 0.832 0.666 0.869 0.452 0.922 0.390 0.952 0.24710 0.862 92.016 15.624 28.713 0.833 0.683 0.869 0.447 0.923 0.393 0.952 0.248\nTable 9: Effect of fine-tuning by various objectives on OOD detection performance. With 20NewsGroups as ID\nand RTE as OOD, this ID-OOD pair exhibits a out-of-domain semantic shift.\nTraining Epoch ID Accuracy↑ Dispersion↑ Compactness↓ ID-OOD MSP Energy KNN MahalanobisSeparability↑ AUROC↑ FPR95↓ AUROC↑ FPR95↓ AUROC↑ FPR95↓ AUROC↑ FPR95↓\n1 0.938 87.041 21.787 8.437 0.699 0.868 0.675 0.873 0.894 0.432 0.951 0.2542 0.937 81.117 20.439 5.936 0.677 0.894 0.676 0.921 0.896 0.429 0.947 0.2953 0.937 97.130 18.534 10.150 0.767 0.852 0.765 0.856 0.866 0.539 0.931 0.3444 0.938 99.677 16.615 11.517 0.735 0.841 0.746 0.839 0.865 0.613 0.901 0.490CE 5 0.927 114.249 15.839 11.704 0.719 0.881 0.734 0.882 0.850 0.625 0.896 0.4786 0.936 111.093 15.514 10.819 0.743 0.853 0.748 0.854 0.831 0.671 0.886 0.5417 0.938 122.309 14.283 14.760 0.745 0.829 0.752 0.826 0.860 0.679 0.889 0.5718 0.938 124.571 14.686 15.711 0.784 0.811 0.793 0.812 0.872 0.674 0.899 0.5569 0.941 130.242 13.908 16.455 0.787 0.805 0.798 0.806 0.872 0.713 0.898 0.59610 0.939 130.285 14.314 15.770 0.781 0.813 0.794 0.813 0.865 0.741 0.893 0.618\n1 0.940 76.871 15.894 7.455 0.733 0.830 0.708 0.838 0.902 0.414 0.966 0.1662 0.943 82.230 15.106 10.080 0.805 0.808 0.803 0.820 0.918 0.418 0.960 0.2423 0.937 89.350 14.646 10.831 0.814 0.782 0.810 0.789 0.867 0.650 0.916 0.5134 0.938 100.884 13.629 11.705 0.810 0.792 0.802 0.795 0.866 0.644 0.898 0.583TAPT 5 0.940 116.726 12.179 12.610 0.790 0.820 0.781 0.820 0.863 0.679 0.887 0.5956 0.940 117.262 11.048 11.496 0.770 0.829 0.773 0.831 0.861 0.641 0.890 0.5337 0.940 119.857 10.796 13.009 0.789 0.806 0.789 0.810 0.870 0.634 0.901 0.5198 0.942 127.375 10.332 14.030 0.808 0.799 0.811 0.797 0.859 0.680 0.875 0.6139 0.944 134.293 8.886 14.992 0.787 0.792 0.791 0.790 0.859 0.738 0.881 0.68210 0.943 134.601 9.060 15.340 0.797 0.794 0.801 0.795 0.857 0.746 0.877 0.683\n1 0.928 135.550 19.245 11.282 0.669 0.869 0.667 0.876 0.855 0.600 0.930 0.3812 0.927 133.438 18.591 10.494 0.682 0.865 0.674 0.891 0.809 0.592 0.903 0.4233 0.929 148.985 13.544 9.218 0.708 0.872 0.698 0.882 0.807 0.696 0.876 0.6214 0.937 158.041 8.588 12.908 0.742 0.842 0.736 0.842 0.846 0.726 0.884 0.666SupCon 5 0.935 161.662 7.455 13.168 0.711 0.854 0.725 0.853 0.849 0.711 0.876 0.6396 0.937 163.736 6.264 11.734 0.752 0.865 0.732 0.865 0.849 0.742 0.877 0.6987 0.936 164.397 5.306 9.679 0.688 0.868 0.678 0.868 0.849 0.775 0.877 0.7448 0.938 167.184 4.434 9.826 0.749 0.850 0.726 0.852 0.842 0.793 0.870 0.7749 0.938 167.316 4.306 8.397 0.727 0.858 0.745 0.859 0.841 0.815 0.868 0.78710 0.938 167.586 4.182 8.259 0.720 0.851 0.736 0.851 0.838 0.824 0.865 0.800\nTable 10: Effect of fine-tuning by various objectives on OOD detection performance. With IMDB as ID and SST-2\nas OOD, this ID-OOD pair exhibits a out-of-domain background shift.\nComputations The RoBERTa base model has\napproximately 125 million parameters, including\nthose of the classification head. On a single\nNVIDIA GeForce RTX 2080 Ti GPU, training\nthe model for 10 epochs takes approximately 8-\n12 hours, and OOD detection for a single dataset\ntakes approximately 15 minutes. Over the scale of\nour experiments, we have used about 200 hours of\nGPU training time.\nMultiple Runs Following the protocol in Arora\net al. (2021), we report results over a single run.\n12828\nTraining Epoch ID Accuracy↑ Dispersion↑ Compactness↓ ID-OOD MSP Energy KNN MahalanobisSeparability↑ AUROC↑ FPR95↓ AUROC↑ FPR95↓ AUROC↑ FPR95↓ AUROC↑ FPR95↓\n1 0.745 86.386 38.342 13.311 0.739 0.794 0.810 0.705 0.927 0.481 0.829 0.6262 0.804 87.198 35.562 14.676 0.733 0.787 0.810 0.692 0.929 0.475 0.847 0.6093 0.842 89.052 33.008 17.263 0.749 0.770 0.819 0.636 0.934 0.446 0.867 0.5474 0.860 89.508 30.364 18.668 0.750 0.780 0.822 0.629 0.933 0.446 0.878 0.520CE 5 0.872 91.260 29.191 18.844 0.794 0.752 0.842 0.603 0.927 0.473 0.872 0.5256 0.878 90.918 27.667 19.017 0.798 0.736 0.834 0.607 0.921 0.495 0.865 0.5157 0.884 91.440 25.515 21.154 0.821 0.706 0.855 0.549 0.927 0.469 0.885 0.4758 0.888 91.601 24.952 21.588 0.830 0.700 0.858 0.555 0.925 0.500 0.885 0.4759 0.890 91.885 24.063 21.728 0.837 0.693 0.862 0.548 0.924 0.499 0.884 0.47410 0.890 91.969 23.580 22.184 0.844 0.676 0.866 0.541 0.924 0.489 0.887 0.479\n1 0.756 85.080 38.572 13.219 0.737 0.800 0.794 0.750 0.924 0.500 0.832 0.6312 0.825 87.712 35.636 15.552 0.734 0.782 0.811 0.678 0.928 0.493 0.854 0.5873 0.852 89.502 33.618 18.240 0.780 0.728 0.835 0.609 0.933 0.438 0.874 0.5084 0.874 89.802 31.870 18.473 0.777 0.754 0.828 0.601 0.926 0.463 0.869 0.523TAPT 5 0.886 91.409 29.624 18.564 0.792 0.737 0.830 0.830 0.917 0.518 0.855 0.5736 0.882 91.537 28.103 19.632 0.812 0.723 0.841 0.587 0.918 0.523 0.863 0.5317 0.891 91.683 26.551 20.700 0.823 0.711 0.853 0.559 0.924 0.486 0.875 0.5038 0.889 91.731 25.830 20.536 0.829 0.694 0.851 0.574 0.918 0.515 0.869 0.5249 0.888 91.874 25.309 21.490 0.835 0.683 0.858 0.563 0.920 0.494 0.878 0.48910 0.890 91.969 24.302 21.409 0.839 0.686 0.858 0.556 0.918 0.513 0.875 0.502\n1 0.667 69.588 36.713 9.288 0.734 0.796 0.786 0.726 0.922 0.510 0.820 0.6562 0.750 75.252 34.277 11.627 0.748 0.742 0.808 0.669 0.926 0.496 0.827 0.6193 0.803 79.054 31.839 13.914 0.738 0.771 0.806 0.674 0.935 0.437 0.856 0.5614 0.822 82.853 29.858 15.612 0.741 0.769 0.807 0.652 0.931 0.445 0.856 0.555SupCon 5 0.847 84.920 28.296 17.149 0.748 0.774 0.803 0.638 0.929 0.452 0.863 0.5206 0.868 88.327 26.281 18.311 0.774 0.757 0.808 0.637 0.923 0.470 0.863 0.5247 0.869 89.118 24.956 19.524 0.790 0.747 0.823 0.587 0.926 0.462 0.872 0.5008 0.882 89.527 24.449 20.277 0.794 0.722 0.827 0.584 0.927 0.449 0.874 0.4719 0.884 90.408 23.481 20.775 0.813 0.711 0.836 0.581 0.924 0.473 0.873 0.46710 0.884 90.487 23.106 21.220 0.821 0.697 0.842 0.568 0.925 0.465 0.877 0.465\nTable 11: Effect of fine-tuning by various objectives on OOD detection performance. Using subsets of the\nNewsCategory as ID and OOD, this ID-OOD pair exhibits a same-domain shift.\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\n1.000\n1.100\n1 10 50 100 200 300 400 500 600 1k 2k\nAUROC\nk\nPretrained (kNN) Pretrained (k-avg NN)\nCE (kNN) CE (k-avg NN)\nTAPT (kNN) TAPT (k-avg NN)\nSupCon (kNN) SupCon (k-avg NN)\n0.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\n1.000\n1 10 50 100 200 300 400 500 600 1k 2k\nFPR95\nk\nPretrained (kNN) Pretrained (k-avg NN)\nCE (kNN) CE (k-avg NN)\nTAPT (kNN) TAPT (k-avg NN)\nSupCon (kNN) SupCon (k-avg NN)\nFigure 8: Effect of kin OOD detection using kNN, for the OoD semantic shift setting (20NewsGroups→RTE).\nLeft: AUROC. Right: FPR95.\nKNN(non-parametric) Mahalanobis(parametric)\nID→OOD Pair Training AUROC↑ AUPR (In)↑ AUPR (Out)↑ FPR95↓ AUROC↑ AUPR (In)↑ AUPR (Out)↑ FPR95↓\nOut-of-Domain: Semantic Shift\n20NG→SST-2 CE 0.973 0.991 0.923 0.155 0.981 0.994 0.942 0.087\nTAPT 0.969 0.990 0.903 0.169 0.981 0.994 0.939 0.088\nSupCon 0.969 0.990 0.909 0.180 0.980 0.994 0.943 0.094\nPre-trained1.000 1.000 1.000 0.000 1.000 1.000 1.000 0.000\n20NG→RTE CE 0.922 0.958 0.858 0.410 0.945 0.970 0.902 0.285\nTAPT 0.898 0.942 0.822 0.455 0.919 0.952 0.869 0.352\nSupCon 0.923 0.959 0.858 0.393 0.952 0.975 0.914 0.248\nPre-trained1.000 1.000 0.999 0.000 1.000 1.000 0.999 0.000\nTable 12: Comparison of OOD detection performance of pre-trained and fine-tuned models, averaged over 3 runs.\n12829\nHyperparameter Value\nBatch size 4\nLearning rate 1e-5\nWeight decay 0.01\nMaximum sequence length 256\nNumber of pre-training epochs (for TAPT) 3\nContrastive loss weight (for SupCon) 2.0\nCE loss weight (for SupCon) 1.0\nTemperature (for SupCon) 0.1 or 0.7 ( ∗)\nTable 13: Hyperparameters used in our study. (∗) Values\ndepend on the dataset.\nτ ID Acc. MSP Energy KNN MahalanobisAUROC↑ FPR95↓ AUROC↑ FPR95↓ AUROC↑ FPR95AUROC↑ FPR95↓\n0.1 0.851 0.830 0.662 0.868 0.413 0.913 0.413 0.930 0.3490.2 0.850 0.826 0.635 0.851 0.422 0.910 0.426 0.932 0.3160.3 0.855 0.839 0.650 0.864 0.447 0.913 0.448 0.933 0.3420.4 0.853 0.817 0.671 0.836 0.486 0.905 0.470 0.925 0.3730.5 0.853 0.822 0.645 0.844 0.441 0.904 0.434 0.921 0.3470.6 0.852 0.816 0.649 0.836 0.475 0.901 0.453 0.918 0.3640.7 0.853 0.805 0.683 0.822 0.518 0.887 0.495 0.903 0.4170.8 0.854 0.805 0.673 0.827 0.506 0.903 0.468 0.920 0.3940.9 0.854 0.818 0.668 0.840 0.483 0.902 0.483 0.920 0.3991 0.853 0.799 0.706 0.814 0.509 0.894 0.489 0.912 0.400\nTable 14: Effect of the temperature τ in SupCon fine-\ntuning, on OOD detection, for OoD semantic shift\n(20NewsGroups→RTE).\nHowever, in Table 12 we show results of a sub-\nset of experiments averaged over 3 runs. There is\nno significant difference between the results in Ta-\nble 12 and Table 2, indicating that our experiments\nare stable across runs. Therefore, for the sake of\ncomputational resources and time, we stick to the\nsingle-run practice in our experiments.\nτ ID Acc. MSP Energy KNN MahalanobisAUROC↑ FPR95↓ AUROC↑ FPR95↓ AUROC↑ FPR95AUROC↑ FPR95↓\n0.1 0.939 0.788 0.833 0.728 0.836 0.842 0.750 0.866 0.7500.2 0.940 0.682 0.850 0.642 0.852 0.819 0.812 0.844 0.7960.3 0.941 0.725 0.835 0.732 0.834 0.832 0.814 0.856 0.7920.4 0.939 0.751 0.859 0.721 0.861 0.822 0.835 0.845 0.8120.5 0.940 0.784 0.842 0.758 0.837 0.826 0.825 0.849 0.7960.6 0.939 0.768 0.818 0.719 0.820 0.829 0.797 0.855 0.7760.7 0.938 0.720 0.851 0.736 0.851 0.833 0.833 0.859 0.8340.8 0.940 0.775 0.828 0.651 0.826 0.823 0.820 0.841 0.8060.9 0.939 0.757 0.891 0.652 0.889 0.861 0.829 0.876 0.8111 0.939 0.738 0.857 0.748 0.857 0.809 0.835 0.840 0.822\nTable 15: Effect of the temperature τ in SupCon fine-\ntuning, on OOD detection, for OoD background shift\n(IMDB→SST-2).\nτ ID Acc. MSP Energy KNN MahalanobisAUROC↑ FPR95↓ AUROC↑ FPR95↓ AUROC↑ FPR95AUROC↑ FPR95↓\n0.1 0.888 0.817 0.700 0.842 0.570 0.927 0.470 0.877 0.4780.2 0.885 0.825 0.681 0.835 0.592 0.922 0.509 0.878 0.5100.3 0.879 0.802 0.733 0.817 0.600 0.922 0.502 0.866 0.5250.4 0.889 0.815 0.670 0.809 0.594 0.922 0.522 0.874 0.5240.5 0.822 0.706 0.818 0.749 0.747 0.913 0.576 0.821 0.6620.6 0.890 0.794 0.713 0.796 0.641 0.919 0.561 0.871 0.5630.7 0.891 0.811 0.694 0.804 0.609 0.921 0.534 0.876 0.5380.8 0.892 0.814 0.697 0.812 0.602 0.922 0.534 0.879 0.5250.9 0.847 0.730 0.798 0.747 0.714 0.909 0.606 0.818 0.6771 0.888 0.817 0.706 0.819 0.611 0.920 0.534 0.875 0.541\nTable 16: Effect of the temperature τ in SupCon fine-\ntuning, on OOD detection, for same-domain shift with\nthe NewsCategory dataset.\n12830\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations section in end of main paper.\n□\u0013 A2. Did you discuss any potential risks of your work?\nEthical Considerations and Limitations sections in end of main paper.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1 contains all our main claims.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4 and Appendix A\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix A\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix A\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe do not make use of any sensitive data, so there is no requirement to anonymize it.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nAppendix A\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix A\nC □\u0013 Did you run computational experiments?\nSection 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix G\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n12831\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4 and Appendix G\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nAppendix G\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix G\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n12832"
}