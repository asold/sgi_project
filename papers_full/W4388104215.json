{
    "title": "From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models",
    "url": "https://openalex.org/W4388104215",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A21019295",
            "name": "Adrian David Cheok",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2479475222",
            "name": "Emma Yann Zhang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A21019295",
            "name": "Adrian David Cheok",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2479475222",
            "name": "Emma Yann Zhang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2007321142",
        "https://openalex.org/W2077574412",
        "https://openalex.org/W2086699924",
        "https://openalex.org/W2125838338",
        "https://openalex.org/W2128084896",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2948978827",
        "https://openalex.org/W2765811365",
        "https://openalex.org/W2770173563",
        "https://openalex.org/W2954996726",
        "https://openalex.org/W2786599352",
        "https://openalex.org/W2743780012",
        "https://openalex.org/W2963017889",
        "https://openalex.org/W3130295820",
        "https://openalex.org/W3050693196",
        "https://openalex.org/W2889300381",
        "https://openalex.org/W4212911677",
        "https://openalex.org/W2784996692",
        "https://openalex.org/W2173520492",
        "https://openalex.org/W2951029718",
        "https://openalex.org/W2963751193",
        "https://openalex.org/W2913323966",
        "https://openalex.org/W1815076433",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4362679631",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3195830874",
        "https://openalex.org/W3161120562",
        "https://openalex.org/W4286521091",
        "https://openalex.org/W4316116392",
        "https://openalex.org/W3190572136",
        "https://openalex.org/W3128513378",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W4362603432",
        "https://openalex.org/W4362519158",
        "https://openalex.org/W4229036808",
        "https://openalex.org/W2126160338",
        "https://openalex.org/W2148948619",
        "https://openalex.org/W4240245920",
        "https://openalex.org/W2001771035",
        "https://openalex.org/W3101684541",
        "https://openalex.org/W1980661496",
        "https://openalex.org/W4952878",
        "https://openalex.org/W2944851425",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W4319166060",
        "https://openalex.org/W4281763794",
        "https://openalex.org/W4385573518",
        "https://openalex.org/W2994747431",
        "https://openalex.org/W2774000609",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2973379954",
        "https://openalex.org/W3005700362",
        "https://openalex.org/W3171293857",
        "https://openalex.org/W2594475271",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W3176196997",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4283157303",
        "https://openalex.org/W3166961312",
        "https://openalex.org/W3126767825",
        "https://openalex.org/W4212985843"
    ],
    "abstract": "Generative transformers have revolutionized the realm of artificial intelligence, particularly in the domain of natural language processing. This paper embarks on a historical journey, tracing the roots of computational theory with Alan Turing and culminating in the sophisticated generative transformer architectures of today. Through a blend of review, history, and tutorial, we aim to provide a holistic understanding of these models, emphasizing their significance, underlying mechanisms, and vast applications. The tutorial segment offers a hands-on approach, guiding readers through the intricacies of building a basic generative transformer model. As we navigate this transformative landscape, we also shed light on challenges, ethical considerations, and future prospects in the world of generative models.",
    "full_text": "Adrian David Cheok and Emma Yann Zhang\nFrom Turing to Transformers: A Comprehensive Review and \nTutorial on the Evolution and Capabilities of Generative \nModels\nCopyright: 2023 © the Author(s). Text is available under a Creative Commons Attribution 4.0 International license. More information in our Publishing Policy.\nhttps://doi.org/10.32388/3NTOLQ.2\nOct 30, 2023\nPreprint v2\nFrom Turing to Transformers: A Comprehensive\nReview and Tutorial on the Evolution and\nApplications of Generative Transformer Models\nAdrian David Cheok and Emma Yann Zhang\nOctober 30, 2023\nAbstract\nGenerative transformers have revolutionized the realm of artificial in-\ntelligence, particularly in the domain of natural language processing. This\npaper embarks on a historical journey, tracing the roots of computational\ntheory with Alan Turing and culminating in the sophisticated generative\ntransformer architectures of today. Through a blend of review, history,\nand tutorial, we aim to provide a holistic understanding of these models,\nemphasizing their significance, underlying mechanisms, and vast applica-\ntions. The tutorial segment offers a hands-on approach, guiding readers\nthrough the intricacies of building a basic generative transformer model.\nAs we navigate this transformative landscape, we also shed light on chal-\nlenges, ethical considerations, and future prospects in the world of gener-\native models.\nKeywords: generative transformers; large language models; genera-\ntive models; Alan Turing; artificial intelligence; machine learning; neural\nnetwork; natural language processing\n1 Introduction\n1.1 Background and significance of generative models in\nAI\nGenerative models have emerged as a cornerstone in the realm of artificial in-\ntelligence (AI). At their core, these models are designed to generate new data\nsamples that are similar to the input data they have been trained on. This\ncapability has profound implications, enabling machines to create, imagine, and\nreplicate complex patterns observed in the real world.\nThe inception of generative models can be traced back to the early days of\nAI, where the foundational work of Alan Turing laid the groundwork for the\nevolution of generative models and the broader field of AI. Following Turing’s\npioneering contributions, the field witnessed the emergence of simple algorithms\n1\ndesigned to mimic and reproduce sequential data. An exemplar of this era is\nthe Hidden Markov Models (HMM) proposed by Leonard Baum in a series of\nseminal papers published in the late 1960s [6, 7, 8]. These models were ground-\nbreaking for their time, providing a probabilistic framework to understand and\npredict sequences. The most notable application of HMMs was in the realm of\nspeech recognition [10], where they became a foundational component, enabling\nsystems to decode and understand human speech with increasing accuracy.\nThe introduction of Recurrent Neural Networks (RNNs) in 1982 by John\nHopfield [9] and Long Short-Term Memory (LSTM) networks in 1997 by Hochre-\niter and Schmidhuber [12] marked significant advancements in the field. RNNs\nbrought the ability to remember previous inputs in handling sequential data,\nwhile LSTMs addressed the challenges of long-term dependencies, making them\npivotal for tasks like time series prediction, speech recognition, and natural\nlanguage processing. Together, they set foundational standards for modern\ngenerative AI models handling sequences.\nHowever, with the advent of deep learning and the proliferation of neural\nnetworks, the potential and capabilities of generative models have expanded\nexponentially. Neural-based generative models, such as Variational Autoen-\ncoders (VAEs) [34, 15] introduced in 2013 and Generative Adversarial Networks\n(GANs) [28, 17] introduced in the following year, have showcased the ability to\ngenerate high-fidelity new data samples based on training data, ranging from\nimages to text and even music.\nThe significance of generative models in AI is multifaceted. Firstly, they\nplay a pivotal role in unsupervised learning, where labeled data is scarce or\nunavailable. By learning the underlying distribution of the data, generative\nmodels can produce new samples, aiding in tasks like data augmentation [23,\n38], anomaly detection [33], and image denoising [32, 42]. Secondly, the creative\npotential of these models has been harnessed in various domains, from image [22,\n57, 46, 60], video and music generation to drug discovery [51, 40] and virtual\nreality [66, 27]. The ability of machines to generate novel and coherent content\nhas opened up avenues previously deemed exclusive to human creativity.\nFurthermore, generative models serve as powerful tools for understanding\nand interpreting complex data distributions. They provide insights into the\nstructure and relationships within the data, enabling researchers and practition-\ners to uncover hidden patterns, correlations, and features [19]. This interpreta-\ntive power is especially valuable in domains like biology [31], finance [39], and\nclimate science [36], where understanding data intricacies can lead to ground-\nbreaking discoveries.\nGenerative models stand as a testament to the advancements and possibil-\nities within AI. Their ability to create, interpret, and innovate has not only\nbroadened the horizons of machine learning but has also reshaped our under-\nstanding of intelligence and creativity.\n2\n1.2 The rise of transformer architectures\nWhile Variational Autoencoders (VAEs) and Generative Adversarial Networks\n(GANs) have significantly advanced the field of generative AI, another monu-\nmental shift in the deep learning landscape emerged with the introduction of\nthe transformer architecture. Presented in the seminal paper ”Attention is All\nYou Need” by a team of Google researchers led by Vaswani in 2017 [26], trans-\nformers have redefined the benchmarks in a multitude of tasks, particularly in\nnatural language processing (NLP).\nThe transformer’s innovation lies in its self-attention mechanism, which al-\nlows it to weigh the significance of different parts of an input sequence, be it\nwords in a sentence or pixels in an image. This mechanism enables the model to\ncapture long-range dependencies and intricate relationships in the data, over-\ncoming the limitations of previous architectures like Recurrent Neural Networks\n(RNNs) and Long Short-Term Memory (LSTM) networks. RNNs and LSTMs,\nwhile effective in handling sequential data, often struggled with long sequences\ndue to issues like vanishing and exploding gradients [16]. Transformers, with\ntheir parallel processing capabilities and attention mechanisms, alleviated these\nchallenges.\nThe success of the transformer architecture was not immediate but became\nevident with the introduction of large language models like BERT (Bidirectional\nEncoder Representations from Transformers) and GPT (Generative Pre-trained\nTransformer). BERT, developed by researchers at Google, demonstrated the\npower of transformers in understanding the context of words in a sentence by\nconsidering both left and right contexts in all layers [29]. This bidirectional\napproach led to state-of-the-art results in several NLP tasks, from question\nanswering to sentiment analysis [58]. On the other hand, OpenAI’s GPT show-\ncased the generative capabilities of transformers [74], producing human-like text\nand achieving remarkable performance in tasks like machine translation [78] and\ntext summarization [75] without task-specific training data.\nThe transformer’s versatility extends beyond NLP. Vision Transformer (ViT)\n[48], an adaptation of the architecture for image classification tasks, has shown\nthat transformers can rival, if not surpass, the performance of traditional con-\nvolutional neural networks (CNNs) in computer vision tasks[56, 68]. This cross-\ndomain applicability underscores the transformer’s potential and its founda-\ntional role in modern AI.\nAnother driving factor behind the rise of transformers is the ever-growing\ncomputational power and the availability of large-scale datasets. Training trans-\nformer models, especially large ones, requires significant computational resources.\nThe feasibility of training such models has been made possible due to advance-\nments in GPU and TPU technologies [67], coupled with the availability of vast\namounts of data to train on. The combination of innovative architecture and\ncomputational prowess has led to the development of models with billions or\neven trillions of parameters, pushing the boundaries of what machines can gen-\nerate to new heights.\nGenerative AI models have undergone significant transformations since their\n3\n1936: Turing machines\nA theoretical framework that\nprovided the foundation for un-\nderstanding computation and\nalgorithmic processes.1950: Turing test\nThe first practical measure for\nmachine intelligence. 1964: ELIZA\nThe first chatbot that simulates\nconversations with a human.1966: Hidden Markov\nModel\nAn early statistical model that\npredicts sequential data.\n1982: Recurrent neural net-\nwork\nA popular model for handling\nsequential data with memory\nretaining capabilities.\n1997: LSTM\nSolves vanishing gradient prob-\nlem of RNN, allowing it to pro-\ncess longer sequences of data. 2014: Generative adversar-\nial network\nA framework that can gener-\nate new data based on training\ndataset.\n2017: Transformers\nBased on the attention mech-\nanism, a scalable and efficient\narchitecture that sets the stan-\ndard for large language models.\n2018: GPT-1, BERT\nOpenAI introduces GPT-1 with\n117 million parameters. Google\nintroduces BERT.2019: GPT-2\nImproved text generation with\n1.5 billion parameters. 2020: GPT-3\nAn updated model with 175\nbillion parameters and abilities\nto translate languages, write\nessays and generate codes.\n2021: DALL-E\nA model that generates high-\nquality images from textual de-\nscriptions. 2022: ChatGPT\nA conversational interface that\nset new standards for natural,\ncoherent, and context-aware in-\nteractions in generative models.\n2023: GPT-4, LLaMA\nOpenAI releases GPT-4 with\n1.76 trillion parameters. Meta\nintroduces the LLaMA and\nLLaMA 2 models.\nFigure 1: A timeline illustrating key milestones in the development of generative\nAI, from Turing Machines to GPT-4.\n4\ninception, with each milestone contributing to the capabilities we see today.\nFrom the foundational Turing machines to the latest GPT-4 and LLaMA mod-\nels, the journey of generative AI has been marked by groundbreaking advance-\nments. A detailed timeline capturing these key milestones is presented to offer\na comprehensive overview of the field’s evolution (Figure 1).”\n1.3 Purpose and structure of the paper\nThe fast growth in artificial intelligence, especially with recent technologies like\ngenerative models and transformers, highlights the need for a comprehensive\nstudy that spans both their historical development and current applications.\nThe primary objective of this paper is to provide readers with a holistic under-\nstanding of the evolution, significance, architecture, and capabilities of genera-\ntive transformers, contextualized within the broader landscape of AI.\nOur motivation for this paper is informed by the existing body of work on\ntransformer-based models and generative AI. While there are several compre-\nhensive reviews, each focuses on specific aspects of the topic. For example,\nGozalo-Brizuela and Garrido-Merchan [76] concentrate on the taxonomy and\nindustrial implications of large generative models, providing a compilation of\npopular generative models organized into various categories such as text-to-text,\ntext-to-image, and text-to-audio. Lin et al.[65] present an exhaustive review of\nvarious transformer variants, their architectural modifications, and applications.\nAdditionally, there are survey papers that focus on the use of transformers for\nspecific tasks such as natural language processing [55, 50], computer vision [62,\n64, 80, 73], time series analysis and forecasting [70, 72], among others. These\nexisting reviews are invaluable, but our paper aims to provide a more compre-\nhensive overview that bridges these specialized areas.\nWhile these papers offer valuable insights, there is a gap in the literature\nfor a resource that combines a historical review, a hands-on tutorial, and a\nforward-looking perspective on generative transformer models. Our paper aims\nto fill this void, serving as a comprehensive guide for newcomers and seasoned\nresearchers alike. The historical review section helps readers understand how\ngenerative AI has developed and progressed in the wider context of AI. Mean-\nwhile, our practical tutorial guides readers through the foundational concepts\nand practical implementations, equipping them to build their own generative\ntransformer models. We offer a unique blend of theoretical understanding and\npractical know-how, setting our work apart from existing reviews. Addition-\nally, we strive to provide a unique balance between explaining the historical\nevolution, technical aspects, and applications of transformers. This makes our\npaper a go-to source for researchers and professionals seeking a wholesome un-\nderstanding and knowledge of transformers.\nThe structure of the paper, which is designed to guide the reader through a\nlogical progression, is as follows:\n• Historical Evolution: We embark on a journey tracing the roots of\ncomputational theory, starting with the foundational concepts introduced\n5\nby Alan Turing. This section provides a backdrop, setting the stage for the\nemergence of neural networks, the challenges they faced, and the eventual\nrise of transformer architectures.\n• Tutorial on Generative Transformers: Transitioning from theory to\npractice, this section offers a practical approach to understanding the in-\ntricacies of generative transformers. Readers will gain insights into the\narchitecture, training methodologies, and best practices, supplemented\nwith code snippets and practical examples.\n• Applications and Challenges: Building upon the foundational knowl-\nedge, we delve into the myriad applications of generative transformers,\nhighlighting their impact across various domains. Concurrently, we ad-\ndress the challenges and ethical considerations associated with their use,\nfostering a balanced perspective.\n• Conclusion and Future Directions: The paper concludes with a re-\nflection on the current state of generative transformers, their potential\ntrajectory, and the exciting possibilities they hold for the future of AI.\nIn essence, this paper endeavors to be more than just a review or a tutorial,\nit aspires to be a comprehensive guide, weaving together history, theory, prac-\ntice, and prospects, providing readers with a panoramic view of the world of\ngenerative transformers.\n2 Historical Evolution\nThe development of computational theory and artificial intelligence has been\nshaped by pioneering figures, innovative ideas, and transformative discoveries.\nCentral to this narrative is Alan Turing, whose unparalleled contributions laid\nthe foundations for modern computation and the subsequent emergence of AI.\nThis section delves deeper into Turing’s groundbreaking work, and the lasting\nlegacy that continues to shape the digital age.\n2.1 Turing Machines and the Foundations of Computation\nOne of Turing’s major contributions was the idea of the Turing machine pro-\nposed in his 1936 paper titled ”On Computable Numbers, with an Applica-\ntion to the Entscheidungsproblem.” [2] This abstract machine was a simple but\npowerful theoretical construct that was designed to perform computations by\nmanipulating symbols on an infinite tape based on a set of rules. The infinite\ntape is divided into discrete cells, each cell can contain a symbol from a finite\nalphabet, and the machine itself has a ”head” that can read and write symbols\non the tape and move left or right. The machine’s behavior is dictated by a set\nof transition rules, which determine its actions based on the current state and\nthe symbol being read. In essence, the Turing machine is a rule-based system\n6\nthat manipulates symbols on a tape, embodying the fundamental operations of\nreading, writing, and transitioning between states.\nWhile the concept might seem rudimentary, the implications of the Turing\nmachine are profound. Turing demonstrated that this simple device, with its set\nof rules and operations, could compute any function that is computable, given\nenough time and tape. This assertion, known as the Church-Turing thesis [11]\n(independently proposed by Alonzo Church in his paper titled ”An Unsolvable\nProblem of Elementary Number Theory” also published in 1936 [1]), posits that\nany function computable by an algorithm can be computed by a Turing machine.\nThis thesis, though not proven, has stood the test of time, with no evidence to\nthe contrary. It serves as a foundational pillar in computer science, defining the\nboundaries of what is computable.\nWorld War II saw Turing’s theoretical concept manifest in tangible, real-\nworld applications. Stationed at Bletchley Park, Britain’s cryptographic hub,\nTuring played a key role in deciphering the Enigma code used by the German\nmilitary. Turing helped develop a machine called the Bombe, which expedited\nthe decryption process of Enigma-encrypted messages [18]. This secret work\nwas crucial for the Allies’ success and showed how computer science could have\na major impact on real-world events.\nAfter World War II, Turing turned his attention to the development of elec-\ntronic computers. He was instrumental in the design of the Automatic Com-\nputing Engine (ACE) [3], one of the earliest computer models capable of storing\nprograms. This showed Turing’s forward-thinking approach to the digital age.\nBeyond computing, he also delved into the nature of intelligence and how it\ncould be replicated in machines.\nThe Turing machine’s significance transcended its immediate mathematical\nimplications. The true brilliance of Turing’s insight, however, lies in the con-\ncept of universal computation. Turing’s subsequent proposition of a Universal\nTuring Machine (UTM)—a machine capable of simulating any other Turing ma-\nchine given the right input and rules—was a revolutionary idea [2]. Given a\ndescription of a Turing machine and its input encoded on the tape, the UTM\ncould replicate the behavior of that machine. This meta-level of computation\nwas groundbreaking. It suggested that a single, general-purpose machine could\nbe designed to perform any computational task, eliminating the need for task-\nspecific machines. The UTM was a harbinger of modern computers, devices\nthat can be reprogrammed to execute a wide array of tasks.\nThe implications of universal computation extend beyond mere hardware.\nIt challenges our understanding of intelligence and consciousness. If the human\nbrain, with its intricate neural networks and synaptic connections, operates on\ncomputational principles, then could it be simulated by a Turing machine? This\nquestion, which blurs the lines between philosophy, neuroscience, and computer\nscience, remains one of the most intriguing and debated topics in the field of\nartificial intelligence.\n7\n2.1.1 Turing’s impact on artificial intelligence and machine learning\nAlan Turing’s influence on the fields of artificial intelligence (AI) and machine\nlearning (ML) is both profound and pervasive. While Turing is often lauded for\nhis foundational contributions to computational theory, his vision and insights\ninto the realm of machine intelligence have played a pivotal role in shaping the\ntrajectory of AI and ML.\nHis 1950 paper, ”Computing Machinery and Intelligence,” [4] introduced the\nfamous Turing Test as a practical measure of machine intelligence. Alan Turing\nintroduced the Turing Test within the context of an ”Imitation Game,” involving\na man, a woman, and a judge as players. They communicate electronically from\nseparate rooms, and the goal of the judge is to identify who is the woman.\nThe man aims to deceive the judge into thinking he is the woman, while the\nwoman assists the judge. Turing then adapts this game into his famous test by\nreplacing the man with a machine, aiming to deceive the questioner in the same\nway. Although the original game focused on gender identification, this aspect\nis often overlooked in later discussions of the Turing Test.\nIn this work, Turing posed the provocative question: ”Can machines think?”\nRather than delving into the philosophical intricacies of defining ”thinking,”\nTuring proposed a pragmatic criterion for machine intelligence: if a machine\ncould engage in a conversation with a human, indistinguishably from another\nhuman, it would be deemed intelligent. This criterion, while straightforward,\nsparked widespread debate and research, laying the foundation for the field of\nartificial intelligence.\nThe Turing Test, in many ways, encapsulated the essence of AI — the quest\nto create machines that can mimic, replicate, or even surpass human cognitive\nabilities. It set a benchmark, a gold standard for machine intelligence, challeng-\ning researchers and scientists to build systems that could ”think” and ”reason”\nlike humans. While the test itself has been critiqued and refined over the years,\nits underlying philosophy remains central to AI: the aspiration to understand\nand emulate human intelligence.\nBeyond the Turing Test, Turing’s insights into neural networks and the po-\ntential of machine learning were visionary. In a lesser-known report written\nin 1948, titled ”Intelligent Machinery,” [13] Turing delved into the idea of ma-\nchines learning from experience. He envisioned a scenario where machines could\nbe trained, much like a human child, through a process of education. Turing\npostulated the use of what he termed ”B-type unorganized machines,” which\nbear a striking resemblance to modern neural networks. These machines, as\nTuring described, would be trained, rather than explicitly programmed, to per-\nform tasks. Although in its infancy at the time, this idea signaled the rise of\nmachine learning, where algorithms learn from data rather than being explicitly\nprogrammed.\nTuring’s exploration of morphogenesis, the biological process that causes\norganisms to develop their shape, further showcased his interdisciplinary ge-\nnius [5]. In his work on reaction-diffusion systems, Turing demonstrated how\nsimple mathematical models could give rise to complex patterns observed in\n8\nnature. This work, while primarily biological in its focus, has profound im-\nplications for AI and ML. It underscores the potential of simple algorithms to\ngenerate complex, emergent behavior, a principle central to neural networks and\ndeep learning.\nAlan Turing’s impact on artificial intelligence and machine learning is im-\nmeasurable. His vision of machine intelligence, his pioneering insights into neu-\nral networks, and his interdisciplinary approach to problem-solving have left an\nindelible mark on the field. As we navigate the intricate landscape of modern\nAI, with its deep neural networks, generative models, and transformers, it is\nimperative to recognize and honor Turing’s legacy. His work serves as a beacon,\nilluminating the path forward, reminding us of the possibilities, challenges, and\nthe profound potential of machines that can ”think.”\n2.1.2 From Turing’s Foundations to Generative Transformers\nThe journey from Alan Turing’s foundational concepts to the sophisticated\nrealm of generative transformers is a testament to the evolution of computa-\ntional theory and its application in artificial intelligence. While at first glance\nTuring’s work and generative transformers might seem worlds apart, a closer\nexamination reveals a direct lineage and influence.\nAlan Turing’s conceptualization of the Turing machine provided the bedrock\nfor understanding computation. His idea of a machine that could simulate\nany algorithm, given the right set of instructions, laid the groundwork for the\nconcept of universal computation. This idea, that a single machine could be\nreprogrammed to perform a myriad of tasks, is the precursor to the modern\nnotion of general-purpose computing systems.\nFast forward to the advent of neural networks, which Turing had touched\nupon in his lesser-known works. These networks, inspired by the human brain’s\ninterconnected neurons, were designed to learn from data. The foundational\nidea was that, rather than being explicitly programmed to perform a task, these\nnetworks would ”learn” by adjusting their internal parameters based on the data\nthey were exposed to. Turing’s vision of machines learning from experience\nresonates deeply with the principles of neural networks.\nGenerative transformers, a cutting-edge development in the AI landscape,\nare an extension of these neural networks. Transformers, with their self-attention\nmechanisms, are designed to weigh the significance of different parts of an input\nsequence, capturing intricate relationships within the data. The ”generative”\naspect of these models allows them to produce new, previously unseen data\nsamples based on their training.\nDrawing a direct link, Turing’s Universal Turing Machine can be seen as an\nearly, abstract representation of what generative transformers aim to achieve in\na more specialized domain. Just as the Universal Turing Machine could simu-\nlate any other Turing machine, given the right input and set of rules, generative\ntransformers aim to generate any plausible data sample, given the right train-\ning and context. The universality of Turing’s machine finds its parallel in the\nversatility of generative transformers.\n9\nFurthermore, Turing’s exploration into machine learning, the idea of ma-\nchines learning from data rather than explicit programming, is the very essence\nof generative transformers. These models are trained on vast datasets, learning\npatterns, structures, and nuances, which they then use to generate new content.\nThe bridge between Turing’s early insights into machine learning and the capa-\nbilities of generative transformers is a direct one, showcasing the evolution of a\nconcept from its theoretical inception to its practical application.\nWhile Alan Turing might not have directly worked on generative transform-\ners, his foundational concepts, vision of machine learning, and the principles he\nlaid down have directly influenced and shaped their development. The journey\nfrom Turing machines to generative transformers is a testament to the enduring\nlegacy of Turing’s genius and the continual evolution of artificial intelligence.\n2.2 Early Neural Networks and Language Models\nThe realm of artificial intelligence has witnessed a plethora of innovations and\nadvancements, with neural networks standing at the forefront of this revolu-\ntion. These computational models, inspired by the intricate web of neurons in\nthe human brain, have paved the way for sophisticated language models that\ncan understand, generate, and manipulate human language with unprecedented\naccuracy.\n2.2.1 Introduction to Neural Networks\nNeural networks [21, 14], at their core, are a set of algorithms designed to\nrecognize patterns. They interpret sensory data through a kind of machine per-\nception, labeling, and clustering of raw input. These algorithms loosely mirror\nthe way a human brain operates, thus the nomenclature ”neural networks.”\nA basic neural network consists of layers of interconnected nodes or ”neu-\nrons.” Each connection between neurons has an associated weight, which is\nadjusted during training. The fundamental equation governing the output y of\na neuron is given by:\ny = f\n X\ni\nwixi + b\n!\n(1)\nwhere xi are the input values, wi are the weights, b is a bias term, and f is\nan activation function.\nThe activation function introduces non-linearity into the model, allowing\nit to learn from error and make adjustments, which is essential for learning\ncomplex patterns. One of the commonly used activation functions is the sigmoid\nfunction, defined as:\nf(z) = 1\n1 + e−z (2)\n10\nNeural networks typically consist of an input layer, one or more hidden\nlayers, and an output layer. The depth and complexity of a network, often\nreferred to as its ”architecture,” determine its capacity to learn from data.\n2.2.2 Evolution of Recurrent Neural Networks (RNNs)\nWhile traditional neural networks have proven effective for a wide range of tasks,\nthey possess inherent limitations when dealing with sequential data. This is\nwhere Recurrent Neural Networks (RNNs) come into play. RNNs are designed to\nrecognize patterns in sequences of data, such as time series or natural language.\nThe fundamental difference between RNNs and traditional neural networks\nlies in the former’s ability to retain memory of previous inputs in its internal\nstate. This is achieved by introducing loops in the network, allowing information\nto persist.\nThe output of an RNN at time t, denoted ht, is computed as:\nht = f (Whhht−1 + Wxhxt + b) (3)\nwhere Whh and Wxh are weight matrices, xt is the input at time t, and ht−1\nis the output from the previous timestep.\nWhile RNNs are powerful, they suffer from challenges like the vanishing and\nexploding gradient problems, especially when dealing with long sequences [16].\nThis makes them less effective in capturing long-term dependencies in the data.\n2.2.3 Long Short-Term Memory (LSTM) Networks\nTo address the vanishing gradient problem of RNNs, Long Short-Term Memory\n(LSTM) networks were introduced. LSTMs, a special kind of RNN, are designed\nto remember information for extended periods [41].\nThe core idea behind LSTMs is the cell state, a horizontal line running\nthrough the entire chain of repeating modules in the LSTM. The cell state can\ncarry information from earlier time steps to later ones, mitigating the memory\nissues faced by traditional RNNs.\nLSTMs introduce three gates:\n1. **Forget Gate**: It decides what information from the cell state should\nbe thrown away or kept. Mathematically, the forget gate ft is given by:\nft = σ(Wf · [ht−1, xt] + bf ) (4)\n2. **Input Gate**: It updates the cell state with new information. The\ninput gate it and the candidate values ˜Ct are computed as:\nit = σ(Wi · [ht−1, xt] + bi) (5)\n˜Ct = tanh(WC · [ht−1, xt] + bC) (6)\n3. **Output Gate**: It determines the output based on the cell state and\nthe input. The output ht is given by:\n11\nht = ot × tanh(Ct) (7)\nwhere ot is the output gate, defined as:\not = σ(Wo · [ht−1, xt] + bo) (8)\nLSTMs, with their ability to capture long-term dependencies and mitigate\nthe challenges faced by traditional RNNs, have paved the way for advancements\nin sequence modeling, particularly in the domain of natural language processing.\n2.3 The Advent of Transformers\nIn the ever-evolving landscape of artificial intelligence and machine learning,\nthe transformer architecture stands out as a significant leap forward, especially\nin the domain of natural language processing. Introduced in the seminal paper\n”Attention Is All You Need” by Vaswani et al. [26], transformers have revolu-\ntionized the way we approach sequence-to-sequence tasks. This section aims to\ndemystify the transformer architecture, breaking it down into its core compo-\nnents and principles.\n2.3.1 Introduction to the Transformer Architecture\nAt a high level, the transformer is a type of neural network architecture de-\nsigned to handle sequential data, making it particularly well-suited for tasks\nlike language translation, text generation, and more. Unlike its predecessors,\nsuch as RNNs and LSTMs, which process data in order, transformers leverage\na mechanism called ”attention” to draw global dependencies between input and\noutput.\nThe Attention Mechanism:\nThe heart of the transformer architecture is the attention mechanism. In\nessence, attention allows the model to focus on different parts of the input\nsequence when producing an output sequence, much like how humans pay at-\ntention to specific words when understanding a sentence.\nMathematically, the attention score for a given queryq and key k is computed\nas:\nAttention(q, k) = exp(score(q, k))P\nk′ exp(score(q, k′)) (9)\nwhere score is a function that calculates the relevance of the key k to the\nquery q. The output of the attention mechanism is a weighted sum of values,\nwhere the weights are the attention scores.\nThe Transformer Architecture:\nThe transformer model consists of an encoder and a decoder. Each of these\nis composed of multiple layers of attention and feed-forward neural networks.\n12\nThe encoder takes in a sequence of embeddings (representations of input\ntokens) and processes them through its layers. The decoder then generates the\noutput sequence, leveraging both its internal layers and the encoder’s output.\nOne of the distinguishing features of transformers is the use of ”multi-head\nattention,” which allows the model to focus on different parts of the input\nsimultaneously, capturing various aspects of the information.\nWhy Transformers?\n• Parallelization: Unlike RNNs, which process sequences step-by-step,\ntransformers can process all tokens in parallel, leading to faster training\ntimes.\n• Long-range Dependencies : The attention mechanism enables trans-\nformers to capture relationships between tokens, regardless of their dis-\ntance in the sequence.\n• Scalability: Transformers are highly scalable, making them suitable for\nlarge datasets and complex tasks.\nThe transformer architecture, with its innovative attention mechanism and\nparallel processing capabilities, has set new benchmarks in the field of machine\nlearning. Its ability to capture intricate patterns and relationships in sequen-\ntial data has paved the way for state-of-the-art models in natural language\nprocessing, making tasks like real-time translation, text summarization, and\nquestion-answering more accurate and efficient.\n2.4 Attention Mechanism: The Heart of Transformers\nThe attention mechanism, a pivotal innovation in the realm of deep learning,\nhas transformed the way we approach sequence-to-sequence tasks in natural\nlanguage processing. Serving as the cornerstone of the transformer architecture,\nattention allows models to dynamically focus on different parts of the input\ndata, capturing intricate relationships and dependencies. This section aims\nto elucidate the principles and mathematics behind the attention mechanism,\nshedding light on its significance in the transformer architecture.\n2.4.1 Conceptual Overview of Attention\nIn traditional sequence-to-sequence models, such as RNNs and LSTMs, infor-\nmation from the entire input sequence is compressed into a fixed-size context\nvector, which is then used to generate the output sequence. This approach,\nwhile effective for short sequences, struggles with longer sequences as the con-\ntext vector becomes a bottleneck, unable to capture all the nuances of the input\ndata.\nThe attention mechanism addresses this challenge by allowing the model to\n”attend” to different parts of the input sequence dynamically, based on the cur-\nrent context. Instead of relying on a single context vector, the model computes\n13\na weighted sum of all input vectors, where the weights represent the ”attention\nscores.”\n2.4.2 Mathematics of Attention\nThe core of the attention mechanism is the computation of attention scores.\nGiven a query q and a set of key-value pairs ( k, v), the attention score for a\nspecific key k is computed as:\nscore(q, k) = qT k (10)\nThe attention weights, which determine how much focus should be given to\neach key-value pair, are computed using a softmax function:\nAttention(q, k) = exp(score(q, k))P\nk′ exp(score(q, k′)) (11)\nThe output of the attention mechanism is a weighted sum of the values:\noutput =\nX\ni\nAttention(q, ki)vi (12)\nQuery\nKey\nValue\nscore Attention weights\nWeighted sum\nOutput\nFigure 2: Schematic representation of the attention mechanism.\nAs depicted in Figure 2, the attention mechanism computes scores based on\nthe query and keys, derives attention weights, and produces an output based on\na weighted sum of values.\n2.4.3 Significance in Transformers\nIn the transformer architecture, attention is not just a supplementary feature;\nit’s the core component. Transformers employ a variant called ”multi-head\n14\nattention,” which runs multiple attention mechanisms in parallel, capturing dif-\nferent types of relationships in the data.\nThe attention mechanism’s ability to focus on different parts of the input\nsequence, irrespective of their position, empowers transformers to handle long-\nrange dependencies, making them particularly effective for tasks like language\ntranslation, text summarization, and more.\nFurthermore, the self-attention mechanism, a special case where the query,\nkey, and value are all derived from the same input, enables transformers to weigh\nthe significance of different parts of the input relative to a specific position.\nThis is crucial for understanding context and semantics in natural language\nprocessing tasks.\n2.5 Generative Transformers and Their Significance\nGenerative transformers have emerged as a groundbreaking advancement in the\ndomain of artificial intelligence, particularly in natural language processing and\ngeneration. These models, characterized by their ability to generate coherent\nand contextually relevant sequences of text, have set new benchmarks in var-\nious tasks, from text completion to story generation. This section introduces\nthe notable generative models available, including the GPT series and other\nsignificant contributions in this domain.\n2.5.1 GPT (Generative Pre-trained Transformer) Series\nThe GPT series, developed by OpenAI, fully demonstrates the power and po-\ntential of generative transformers. Built upon the transformer architecture,\nthe GPT models leverage the attention mechanism to understand and generate\nhuman-like text. The GPT series has seen rapid evolution, with each iteration\nbringing enhanced capabilities and performance.\nGPT-1: The first in the series, GPT-1 [30], was released in 2018. It laid the\nfoundation for subsequent models. With 117 million parameters, it showcased\nthe potential of transformers in generating coherent paragraphs of text.\nGPT-2: Released in 2019, GPT-2 [35] increased its parameters to 1.5 billion.\nIts ability to generate entire articles, answer questions, and even write poetry\ngarnered significant attention from the research community and the public alike.\nGPT-3: GPT-3 [45] has 175 billion parameters. Its capabilities extend\nbeyond mere text generation; it can translate languages, write essays, create\npoetry, and even generate code.\nGPT-4: The most recent model from OpenAI, GPT-4 [79], consists a stag-\ngering 1.76 trillion parameters, positioning it among the most advanced language\nmodels currently available. Leveraging advanced deep learning methodologies,\nit surpasses the capabilities of its forerunner, GPT-3. Remarkably, GPT-4 can\nhandle up to 25,000 words simultaneously, a capacity eightfold greater than\nGPT-3. Furthermore, GPT-4 is versatile in accepting both text and image\nprompts, allowing users to define tasks across vision and language domains.\n15\nA notable improvement in GPT-4 is its reduced propensity for hallucinations\ncompared to earlier versions.\n2.5.2 Other Notable Generative Transformer Models\nBeyond the GPT series, the landscape of generative transformers is rich and\ndiverse, with several models making significant contributions to the field.\nBERT (Bidirectional Encoder Representations from Transform-\ners): Developed by Google, BERT [29] revolutionized the way we approach\nnatural language understanding tasks. Unlike GPT, which is generative, BERT\nis discriminative, designed to predict missing words in a sentence. Its bidirec-\ntional nature allows it to capture context from both the left and the right of\na word, leading to superior performance in tasks like question-answering and\nsentiment analysis.\nLLaMA: LLaMA [81] is an auto-regressive language model built on the\ntransformer architecture, introduced by Meta. In February 2023, Meta unveiled\nthe initial version of LLaMA, boasting 65 billion parameters and adept at nu-\nmerous generative AI functions. By July 2023, LLaMA 2 was launched with\nthree distinct model sizes: 7, 13, and 70 billion parameters.\nLaMDA: LaMDA [69] is a specialized family of transformer-based neural\nlanguage models for dialog applications developed by Google in 2022. With\nup to 137 billion parameters and pre-training on 1.56 trillion words of public\ndialog and web text, LaMDA aims to address two key challenges: safety and\nfactual grounding. The model incorporates fine-tuning and external knowledge\nconsultation to improve its safety metrics, ensuring responses align with human\nvalues and avoid harmful or biased suggestions. For factual grounding, LaMDA\nemploys external knowledge sources like information retrieval systems and cal-\nculators to generate responses that are not just plausible but also factually\naccurate. The model shows promise in various domains, including education\nand content recommendations, offering a balanced blend of quality, safety, and\nfactual integrity.\n3 Tutorial on Generative Transformers\nIn this section, we delve into a hands-on tutorial on generative transformers,\nguiding readers through the foundational concepts and practical implementa-\ntions. By the end of this tutorial, readers should have a clear understanding\nof the transformer architecture and be equipped to build their own generative\ntransformer models.\n3.1 Basics of the Transformer Architecture\nThe transformer architecture, introduced by Vaswani et al. in their seminal\npaper ”Attention Is All You Need” [26], has become the backbone of many\nstate-of-the-art models in natural language processing. Let’s break down its\ncore components.\n16\n3.1.1 Overview\nAs depicted in Fig. 3, the transformer consists of an encoder and a decoder. The\nencoder processes the input sequence, and the decoder generates the output\nsequence. Both the encoder and decoder are composed of multiple layers of\nattention mechanisms and feed-forward neural networks.\nEncoder DecoderFeatures\nInput Output\nFigure 3: Expanded schematic representation of the transformer architecture\nwith a smaller Features block.\n3.1.2 Attention Mechanism\nAs previously discussed, the attention mechanism allows the model to focus on\ndifferent parts of the input sequence when producing an output. The mechanism\ncomputes attention scores based on queries, keys, and values.\nMathematical Representation:\nGiven a query q, key k, and value v, the attention output is computed as:\nAttention(q, k, v) = softmax\n\u0012q · kT\n√dk\n\u0013\nv (13)\nwhere dk is the dimension of the key.\nCode Snippet:\nThe following Python code snippet demonstrates how to implement this\nattention mechanism using PyTorch:\nimport torch\nimport torch.nn.functional as F\ndef scaled_dot_product_attention(q, k, v):\nmatmul_qk = torch.matmul(q, k.transpose(-2, -1))\nd_k = q.size(-1) ** 0.5\nscaled_attention_logits = matmul_qk / d_k\nattention_weights = F.softmax(scaled_attention_logits, dim=-1)\noutput = torch.matmul(attention_weights, v)\nreturn output, attention_weights\nIn this code snippet, q, k, and v are the query, key, and value tensors,\nrespectively. The function scaled dot product attention computes the attention\noutput according to Equation 13.\n17\n3.1.3 Multi-Head Attention\nInstead of using a single set of attention weights, the transformer uses multiple\nsets, allowing it to focus on different parts of the input simultaneously. This is\nknown as multi-head attention.\nCode Snippet:\nclass MultiHeadAttention(nn.Module):\ndef __init__(self, d_model, num_heads):\nsuper(MultiHeadAttention, self).__init__()\nself.num_heads = num_heads\n# Dimension of the model\nself.d_model = d_model\n# Depth of each attention head\nself.depth = d_model\n# Linear layer for creating query, key and value matrix\nself.wq = nn.Linear(d_model, d_model)\nself.wk = nn.Linear(d_model, d_model)\nself.wv = nn.Linear(d_model, d_model)\n# Final linear layer to produce the output\nself.dense = nn.Linear(d_model, d_model)\nFigure 4: PyTorch implementation of multi-head attention.\n3.1.4 Feed-Forward Neural Networks\nEach transformer layer contains a feed-forward neural network, applied inde-\npendently to each position.\nCode Snippet:\nclass PointWiseFeedForwardNetwork(nn.Module):\ndef __init__(self, d_model, dff):\nsuper(PointWiseFeedForwardNetwork, self).__init__()\nself.fc1 = nn.Linear(d_model, dff)\nself.fc2 = nn.Linear(dff, d_model)\n...\nFigure 5: PyTorch implementation of point-wise feed-forward network.\nEach method and its body are indented with a tab or four spaces, which\nis the standard Python indentation. This makes the code easier to read and\nunderstand.\n18\n3.1.5 Self-attention Mechanism\nThe self-attention mechanism is a variant of the attention mechanism where the\ninput sequence itself serves as the queries, keys, and values. This allows the\ntransformer to weigh the significance of different parts of the input relative to\na specific position, crucial for understanding context and semantics.\nMathematical Representation:\nGiven an input sequence X, the queries Q, keys K, and values V are derived\nas:\nQ = XWQ, K = XWK, V = XWV (14)\nwhere WQ, WK, and WV are weight matrices. The self-attention output is\nthen computed using the attention formula:\nSelfAttention(Q, K, V) = softmax\n\u0012QKT\n√dk\n\u0013\nV (15)\n3.1.6 Positional Encoding\nTransformers, by design, do not have a built-in notion of sequence order. To\nprovide the model with positional information, we inject positional encodings to\nthe input embeddings. These encodings are added to the embeddings to ensure\nthe model can make use of the sequence’s order.\nMathematical Representation:\nThe positional encodings are computed using sine and cosine functions:\nPE(pos,2i) = sin\n\u0010 pos\n100002i/dmodel\n\u0011\n(16)\nPE(pos,2i+1) = cos\n\u0010 pos\n100002i/dmodel\n\u0011\n(17)\nwhere pos is the position and i is the dimension.\n3.1.7 Multi-head Attention\nMulti-head attention is an extension of the attention mechanism, allowing the\nmodel to focus on different parts of the input simultaneously. By running mul-\ntiple attention mechanisms in parallel, the model can capture various types of\nrelationships in the data.\nMathematical Representation:\nGiven queries Q, keys K, and values V , the multi-head attention output is\ncomputed as:\nMultiHead(Q, K, V) = Concat(head1, . . . ,headh)WO (18)\nwhere each head is computed as:\nheadi = Attention(QWQi, KWKi, V WV i) (19)\n19\nand WQi, WKi, WV i, and WO are weight matrices.\nHead 1\nHead 2\nHead h\nConcat Dense Layer Output\nFigure 6: Schematic representation of multi-head attention.\nFigure 6 showcases the multi-head attention mechanism, where multiple at-\ntention heads operate in parallel, and their outputs are concatenated and passed\nthrough a dense layer to produce the final output.\nUnderstanding the intricacies of the transformer architecture, from the self-\nattention mechanism to multi-head attention, is crucial for harnessing its full\npotential. By delving into the mathematical foundations and practical imple-\nmentations, one can build powerful models capable of handling a wide range of\ntasks in natural language processing.\n3.1.8 Encoder and Decoder modules\nThe Transformer architecture consists of an encoder and a decoder, each made\nup of multiple layers. Here, we’ll walk through the implementation of these\nmodules.\nEncoder Module:\nThe encoder module consists of multiple encoder layers, each containing\nmulti-head attention and feed-forward neural networks.\nCode Snippet for Encoder:\nimport torch.nn as nn\nclass EncoderLayer(nn.Module):\n20\ndef __init__(self, d_model, num_heads):\nsuper(EncoderLayer, self).__init__()\nself.mha = MultiHeadAttention(d_model, num_heads)\nself.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n# Layer normalization and dropout layers can be added here\ndef forward(self, x):\nattn_output = self.mha(x, x, x)\nout1 = x + attn_output # Add & Norm\nffn_output = self.ffn(out1)\nout2 = out1 + ffn_output # Add & Norm\nreturn out2\nDecoder Module:\nThe decoder module is similar to the encoder but has an additional multi-\nhead attention layer to attend to the encoder’s output.\nCode Snippet for Decoder:\nclass DecoderLayer(nn.Module):\ndef __init__(self, d_model, num_heads):\nsuper(DecoderLayer, self).__init__()\nself.mha1 = MultiHeadAttention(d_model, num_heads)\nself.mha2 = MultiHeadAttention(d_model, num_heads)\nself.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n# Layer normalization and dropout layers can be added here\ndef forward(self, x, enc_output):\nattn1 = self.mha1(x, x, x)\nout1 = x + attn1 # Add & Norm\nattn2 = self.mha2(out1, enc_output, enc_output)\nout2 = out1 + attn2 # Add & Norm\nffn_output = self.ffn(out2)\nout3 = out2 + ffn_output # Add & Norm\nreturn out3\nIn these code snippets, ‘MultiHeadAttention‘ and ‘PointWiseFeedForward-\nNetwork‘ are custom classes that you would define based on your specific needs\nfor multi-head attention and point-wise feed-forward networks, respectively.\n3.2 Building a Simple Generative Transformer\nBuilding a generative transformer from scratch involves several steps, from data\npreprocessing to model training and text generation. In this section, we’ll walk\nthrough each of these steps, providing a comprehensive guide to constructing\nyour own generative transformer.\n21\n3.2.1 Data Preprocessing and Tokenization\nBefore feeding data into the model, it’s essential to preprocess and tokenize it.\nTokenization involves converting raw text into a sequence of tokens, which can\nbe words, subwords, or characters.\nTokenization:\nUsing popular libraries like the HuggingFace’s ‘transformers‘, tokenization\ncan be achieved as:\nfrom transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(’gpt2-medium’)\ntokens = tokenizer.encode(\"Hello, world!\")\nFigure 7: Tokenizing text using GPT-2 tokenizer.\n3.2.2 Defining the Transformer Model\nAssuming you’ve already defined the EncoderLayer and DecoderLayer classes,\nyou can define the complete Transformer model as follows:\nclass Transformer(nn.Module):\ndef __init__(self, d_model, num_heads, num_layers):\nsuper(Transformer, self).__init__()\nself.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_layers)])\nself.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_layers)])\ndef forward(self, src, tgt):\nenc_output = src\nfor layer in self.encoder:\nenc_output = layer(enc_output)\ndec_output = tgt\nfor layer in self.decoder:\ndec_output = layer(dec_output, enc_output)\nreturn dec_output\nBuilding a generative transformer, while complex, is made accessible with\nmodern libraries and tools. By understanding the steps involved, from data\npreprocessing to model training and generation, one can harness the power of\ntransformers for a wide range of applications.\n3.3 Advanced Techniques and Best Practices\nWhile the foundational concepts and basic implementations provide a solid\nstarting point, mastering generative transformers requires a deeper understand-\ning of advanced techniques and best practices. This section offers insights\n22\ninto improving generation quality, handling long sequences, memory issues, and\nleveraging fine-tuning and transfer learning [82].\n3.3.1 Techniques for Improving Generation Quality\nAchieving high-quality text generation necessitates a combination of model ar-\nchitecture tweaks, training strategies, and post-processing methods.\nTemperature Sampling:\nBy adjusting the temperature during sampling, one can control the random-\nness of the generated text [71]. A lower temperature makes the output more\ndeterministic, while a higher value introduces randomness.\npi = e\nzi\nT\nP\nj e\nzj\nT\n(20)\nwhere pi is the adjusted probability, zi is the original probability, and T is\nthe temperature.\nTop-k and Top-p Sampling:\nInstead of sampling from the entire distribution, one can restrict the sam-\npling pool to the top-k tokens or those tokens that have a cumulative probability\ngreater than a threshold p [63].\nGradient Clipping:\nTo prevent exploding gradients during training, gradient clipping can be\nemployed, ensuring the gradients remain within a defined range [43].\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\nFigure 8: Gradient clipping in PyTorch.\n3.3.2 Handling Long Sequences and Memory Issues\nTransformers, by design, have quadratic complexity with respect to sequence\nlength. This can lead to memory issues for long sequences.\nGradient Accumulation:\nInstead of updating the model weights after every batch, gradients can be\naccumulated over multiple batches, effectively simulating a larger batch size\nwithout the memory overhead [25].\nModel Parallelism:\nFor models with billions of parameters, distributing the model across multi-\nple GPUs can alleviate memory constraints [37].\nGradient Checkpointing:\nThis technique involves storing intermediate activations during the forward\npass and recomputing them during the backward pass, reducing memory usage\nat the cost of increased computation.\n23\n3.3.3 Fine-tuning and Transfer Learning\nTransfer learning, the practice of leveraging pre-trained models on new tasks,\nhas proven highly effective in the NLP domain.\nFine-tuning:\nOnce a model is pre-trained on a large corpus, it can be fine-tuned on a\nsmaller, task-specific dataset. This approach often yields superior results com-\npared to training from scratch [44, 47].\nfrom transformers import GPT2ForSequenceClassification\nmodel = GPT2ForSequenceClassification.from_pretrained(’gpt2-medium’)\n# Fine-tuning code here\nFigure 9: Fine-tuning a pre-trained GPT-2 model.\nAdapters:\nInstead of fine-tuning the entire model, adapters allow for training only a\nsmall portion of the model, introducing task-specific parameters without altering\nthe pre-trained weights [54].\nMastering generative transformers goes beyond understanding the basics. By\nincorporating advanced techniques and best practices, one can achieve state-of-\nthe-art performance, handle large models and sequences efficiently, and adapt\npre-trained models to new tasks with ease. As the field of NLP continues to\nevolve, staying abreast of these practices ensures robust and high-quality model\ndeployments.\n4 Applications and Use Cases\nGenerative transformers, with their unparalleled capability to understand and\ngenerate human-like text, have found applications across a myriad of domains [76].\nThis section provides an in-depth exploration of some of the most prominent\napplications, shedding light on the transformative impact of these models on\nvarious industries.\n4.1 Text Generation for Creative Writing\nThe realm of creative writing, traditionally seen as the bastion of human cre-\nativity, has witnessed significant advancements with the advent of generative\ntransformers. These models, trained on vast corpora of literature, can produce\ntext that mirrors the style, tone, and complexity of human authors.\nNovel and Short Story Generation: GPT-3 and its successors have been\nemployed to generate entire novels or assist authors by suggesting plot twists,\ncharacter developments, and dialogues. The generated content, while sometimes\nrequiring human oversight, exhibits creativity and coherence.\n24\nPoetry and Song Lyrics: The nuanced and abstract nature of poetry and\nsong lyrics poses a challenge for traditional models. However, generative trans-\nformers, with their deep understanding of context, have been used to produce\nverses that resonate with human emotions and experiences.\n4.2 Chatbots and Conversational Agents\nThe rise of digital communication has spurred the demand for intelligent chat-\nbots and conversational agents. Generative transformers, with their ability to\ngenerate contextually relevant and coherent responses, stand at the forefront of\nthis revolution. One of the most prominent examples of a conversational agent\nbuilt on generative transformer architecture is ChatGPT, developed by Ope-\nnAI. ChatGPT reached 100 million monthly active users just two months after\nlaunching, making it the fastest-growing application in history.\nCustomer Support: Businesses employ transformer-based chatbots to\nhandle customer queries, complaints, and feedback. These chatbots can un-\nderstand the context, provide accurate information, and even escalate issues\nwhen necessary.\nPersonal Assistants: Digital personal assistants, like Siri and Alexa, are\nintegrating transformer models to enhance their conversational capabilities,\nmaking interactions more natural and context-aware.\n4.3 Code Generation and Programming Assistance\nThe software development landscape is undergoing a paradigm shift with the\nintroduction of transformer models capable of understanding and generating\ncode. These models assist developers by suggesting code snippets, detecting\nbugs, and even generating entire functions or modules.\nCode Completion: Integrated Development Environments (IDEs) are in-\ncorporating transformers to provide real-time code completion suggestions, en-\nhancing developer productivity.\nBug Detection and Fixing: Transformers can be trained to detect anoma-\nlies in code and suggest potential fixes, reducing debugging time and ensuring\nmore robust software.\n4.4 Other Notable Applications\nBeyond the aforementioned domains, generative transformers have found appli-\ncations in diverse areas:\nTranslation: While traditional machine translation models have limita-\ntions, transformers can produce translations that consider the broader context,\nresulting in more accurate and idiomatic outputs.\nSummarization: Generative transformers can read lengthy articles or doc-\numents and produce concise summaries, retaining the core information and in-\ntent.\n25\nGaming: In the gaming industry, transformers are used to generate di-\nalogues, plotlines, and even assist in game design by suggesting scenarios or\ncharacter backstories.\nThe applications of generative transformers are vast and continually ex-\npanding. As research progresses and models become more sophisticated, it is\nanticipated that their integration into various domains will become even more\nprofound.\n5 Challenges and Limitations\nWhile generative transformers have showcased remarkable capabilities, they are\nnot devoid of challenges and limitations. This section delves into some of the\nmost pressing concerns surrounding these models, from interpretability issues\nto ethical dilemmas and computational constraints.\n5.1 Model Interpretability\nDeep learning models, especially those with millions or billions of parameters\nlike generative transformers, are often criticized for being ”black boxes.” Un-\nderstanding why a model made a particular decision can be elusive [24].\nAttention Maps: One approach to interpretability is visualizing attention\nmaps [26, 20]. These maps show which parts of the input the model focused\non when producing an output. Attention maps are generated by the attention\nmechanism that computes a set of attention scores, which can be visualized as\na heatmap.\nAttention maps serve as a tool for interpreting transformer models in NLP\nby providing insights into various aspects of text processing. They help in\nanalyzing the roles of words in sentences, identifying key topics, evaluating text\nquality, and detecting errors or biases. However, while attention maps provide\ninsights, they don’t offer a complete understanding of the model’s decision-\nmaking process.\nMathematical Analysis: Efforts are being made to develop mathematical\ntools and frameworks to dissect the inner workings of transformers [52, 53]. Yet,\na comprehensive understanding remains a research frontier.\n5.2 Hallucination in Text Generation\nGenerative transformers are sometimes susceptible to generating text that, while\ncoherent and grammatically correct, is factually incorrect or nonsensical. This\nphenomenon is commonly referred to as hallucination. Ji et al. conducted a\ncomprehensive survey of the issue of hallucination in natural language genera-\ntion (NLG) [77].\nThe causes of hallucination are multifaceted and can vary. They may include\ninadequate training data, which limits the model’s understanding of the subject\nmatter. Overfitting to the training set is another common issue, where the\n26\nmodel learns the noise in the data rather than the actual pattern. Additionally,\nhigh model complexity leading to over-parameterization can also contribute to\nhallucination.\nAddressing the issue of hallucination involves multiple strategies. One ap-\nproach is to fine-tune the model on a more specific dataset that is closely aligned\nwith the task at hand. Another strategy involves incorporating external knowl-\nedge bases that can fact-check the generated text in real-time. Ensemble meth-\nods, which combine the outputs of multiple models, can also be used to validate\nthe generated text and reduce the likelihood of hallucination.\nEfforts are underway to quantify the degree of hallucination in generated\ntext. Although a standard measure has yet to be established, one simplistic\nway to quantify it is through the Hallucination Score, defined as the ratio of\nthe number of hallucinated tokens to the total number of generated tokens, as\nshown in Equation 21.\nHallucination Score = Number of hallucinated tokens\nTotal number of generated tokens (21)\n5.3 Ethical Considerations in Text Generation\nGenerative transformers, with their ability to produce human-like text, raise\nseveral ethical concerns [61].\nMisinformation and Fake News: There’s potential for these models to\ngenerate misleading or false information, which can be weaponized to spread\nmisinformation.\nBias and Fairness: Transformers, being trained on vast internet datasets,\ncan inherit and perpetuate biases present in the data [59]. Addressing this\nrequires careful dataset curation and post-hoc bias mitigation techniques.\nBias =\nPn\ni=1(Pmodel(xi) − Ptrue(xi))\nn (22)\nWhere Pmodel is the model’s prediction, Ptrue is the true distribution, and n\nis the number of samples.\n5.4 Computational Requirements and Environmental Im-\npact\nTraining a large language model demands significant computational resources.\nFor example, the GPT-3 model with 175 billion parameters would require 3.14e23\nFLOPS for training, translating to 355 GPU-years and a cost of $4.6 million\non a V100 GPU [49]. Memory is another bottleneck; the model’s 175 billion\nparameters would need 700GB of memory, far exceeding the capacity of a single\nGPU. To manage these challenges, OpenAI used model parallelism techniques\nand trained the models on a high-bandwidth cluster. As language models grow\nin size, model parallelism is becoming increasingly essential for research.\n27\nEnergy Consumption: The energy required to train state-of-the-art mod-\nels can be equivalent to the carbon footprint of multiple car lifetimes. This raises\nenvironmental concerns.\nExclusivity: The computational demands mean that only well-funded or-\nganizations can train the most advanced models, leading to concerns about the\ndemocratization of AI.\nWhile generative transformers offer immense potential, it’s crucial to ad-\ndress their challenges and limitations. Balancing the pursuit of state-of-the-art\nperformance with ethical, environmental, and computational considerations is\nparamount for the sustainable and responsible advancement of the field.\n6 Future Directions and Conclusion\nAs we reflect upon the journey of generative transformers, from their founda-\ntional roots with Alan Turing to their current state-of-the-art capabilities, it\nbecomes evident that we stand on the cusp of a transformative era in artificial\nintelligence.\n6.1 The Future of Generative Transformers\nGenerative transformers, having already revolutionized numerous domains, are\npoised to further push the boundaries of what machines can achieve. With\nadvancements in model architectures, training techniques, and hardware capa-\nbilities, we can anticipate models that not only understand and generate human-\nlike text but also exhibit creativity, reasoning, and perhaps even a semblance of\nconsciousness.\nBeyond Text: The future might see transformers that seamlessly integrate\nmultiple modalities – text, image, sound, and more – offering a holistic under-\nstanding of the world and generating content that transcends the limitations of\ncurrent models.\n6.2 Potential Areas of Research and Development\nThe way forward is full of opportunities for exploration and innovation. As the\nfield of generative transformers continues to evolve, there are numerous avenues\nfor research and development that remain unexplored or underexplored.\nModel Efficiency: As models grow in size, research into making them more\nefficient, both in terms of computational requirements and energy consumption,\nwill be paramount.\nEthical AI: With the power of these models comes the responsibility of\nensuring their ethical use. Research into bias mitigation, fairness, and trans-\nparency will play a crucial role in shaping the future of generative transformers.\nInterdisciplinary Integration: The fusion of AI with fields like neuro-\nscience, cognitive science, and even philosophy could lead to breakthroughs\nthat redefine our understanding of intelligence, both artificial and natural.\n28\nReferences\n[1] Paul Bernays. “Alonzo Church. An unsolvable problem of elementary\nnumber theory. American journal of mathematics, vol. 58 (1936), pp. 345–\n363.” In: The Journal of Symbolic Logic 1.2 (1936), pp. 73–74.\n[2] Alan Mathison Turing et al. “On computable numbers, with an application\nto the Entscheidungsproblem”. In: J. of Math 58.345-363 (1936), p. 5.\n[3] Alan M Turing et al. “Proposed electronic calculator”. In: National Phys-\nical Laboratory (1946).\n[4] Computing Machinery. “Computing machinery and intelligence-AM Tur-\ning”. In: Mind 59.236 (1950), p. 433.\n[5] Alan Mathison Turing. “The chemical basis of morphogenesis”. In: Philo-\nsophical Transactions of the Royal Society of London. Series B, Biological\nSciences 237.641 (1952), pp. 37–72.\n[6] Leonard E Baum and Ted Petrie. “Statistical inference for probabilistic\nfunctions of finite state Markov chains”. In: The annals of mathematical\nstatistics 37.6 (1966), pp. 1554–1563.\n[7] Leonard E Baum and John Alonzo Eagon. “An inequality with applica-\ntions to statistical estimation for probabilistic functions of Markov pro-\ncesses and to a model for ecology”. In: (1967).\n[8] Leonard E Baum et al. “A maximization technique occurring in the statis-\ntical analysis of probabilistic functions of Markov chains”. In: The annals\nof mathematical statistics 41.1 (1970), pp. 164–171.\n[9] John J Hopfield. “Neural networks and physical systems with emergent\ncollective computational abilities.” In:Proceedings of the national academy\nof sciences 79.8 (1982), pp. 2554–2558.\n[10] Lawrence R Rabiner. “A tutorial on hidden Markov models and selected\napplications in speech recognition”. In: Proceedings of the IEEE 77.2\n(1989), pp. 257–286.\n[11] B Jack Copeland. “The church-turing thesis”. In: (1997).\n[12] Sepp Hochreiter and J¨ urgen Schmidhuber. “Long short-term memory”.\nIn: Neural computation 9.8 (1997), pp. 1735–1780.\n[13] Alan Turing. “Intelligent machinery (1948)”. In: B. Jack Copeland (2004),\np. 395.\n[14] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and\nmachine learning. Vol. 4. 4. Springer, 2006.\n[15] Diederik P Kingma and Max Welling. “Auto-encoding variational bayes”.\nIn: arXiv preprint arXiv:1312.6114 (2013).\n[16] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. “On the difficulty\nof training recurrent neural networks”. In: International conference on\nmachine learning. Pmlr. 2013, pp. 1310–1318.\n29\n[17] Ian Goodfellow et al. “Generative adversarial nets”. In: Advances in neural\ninformation processing systems 27 (2014).\n[18] Andrew Hodges. Alan Turing: The Enigma: The Book That Inspired the\nFilm” The Imitation Game” . Princeton University Press, 2014.\n[19] Alec Radford, Luke Metz, and Soumith Chintala. “Unsupervised represen-\ntation learning with deep convolutional generative adversarial networks”.\nIn: arXiv preprint arXiv:1511.06434 (2015).\n[20] Kelvin Xu et al. “Show, attend and tell: Neural image caption generation\nwith visual attention”. In: International conference on machine learning .\nPMLR. 2015, pp. 2048–2057.\n[21] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT\npress, 2016.\n[22] Aaron van den Oord et al. “Wavenet: A generative model for raw audio”.\nIn: arXiv preprint arXiv:1609.03499 (2016).\n[23] Antreas Antoniou, Amos Storkey, and Harrison Edwards. “Data augmen-\ntation generative adversarial networks”. In:arXiv preprint arXiv:1711.04340\n(2017).\n[24] Finale Doshi-Velez and Been Kim. “Towards a rigorous science of inter-\npretable machine learning”. In: arXiv preprint arXiv:1702.08608 (2017).\n[25] Yujun Lin et al. “Deep gradient compression: Reducing the communica-\ntion bandwidth for distributed training”. In:arXiv preprint arXiv:1712.01887\n(2017).\n[26] Ashish Vaswani et al. “Attention is all you need”. In: Advances in neural\ninformation processing systems 30 (2017).\n[27] Panos Achlioptas et al. “Learning representations and generative models\nfor 3d point clouds”. In: International conference on machine learning .\nPMLR. 2018, pp. 40–49.\n[28] Antonia Creswell et al. “Generative adversarial networks: An overview”.\nIn: IEEE signal processing magazine 35.1 (2018), pp. 53–65.\n[29] Jacob Devlin et al. “Bert: Pre-training of deep bidirectional transformers\nfor language understanding”. In: arXiv preprint arXiv:1810.04805 (2018).\n[30] Alec Radford et al. “Improving language understanding by generative pre-\ntraining”. In: (2018).\n[31] Gregory P Way and Casey S Greene. “Extracting a biologically relevant\nlatent space from cancer transcriptomes with variational autoencoders”.\nIn: PACIFIC SYMPOSIUM on BIOCOMPUTING 2018: Proceedings of\nthe Pacific Symposium . World Scientific. 2018, pp. 80–91.\n[32] Qingsong Yang et al. “Low-dose CT image denoising using a generative\nadversarial network with Wasserstein distance and perceptual loss”. In:\nIEEE transactions on medical imaging 37.6 (2018), pp. 1348–1357.\n30\n[33] Lucas Deecke et al. “Image anomaly detection with generative adversarial\nnetworks”. In: Machine Learning and Knowledge Discovery in Databases:\nEuropean Conference, ECML PKDD 2018, Dublin, Ireland, September\n10–14, 2018, Proceedings, Part I 18 . Springer. 2019, pp. 3–17.\n[34] Diederik P Kingma, Max Welling, et al. “An introduction to variational\nautoencoders”. In: Foundations and Trends® in Machine Learning 12.4\n(2019), pp. 307–392.\n[35] Alec Radford et al. “Language models are unsupervised multitask learn-\ners”. In: OpenAI blog 1.8 (2019), p. 9.\n[36] Markus Reichstein et al. “Deep learning and process understanding for\ndata-driven Earth system science”. In: Nature 566.7743 (2019), pp. 195–\n204.\n[37] Mohammad Shoeybi et al. “Megatron-lm: Training multi-billion parame-\nter language models using model parallelism”. In:arXiv preprint arXiv:1909.08053\n(2019).\n[38] Connor Shorten and Taghi M Khoshgoftaar. “A survey on image data\naugmentation for deep learning”. In: Journal of big data 6.1 (2019), pp. 1–\n48.\n[39] Justin Sirignano and Rama Cont. “Universal features of price formation\nin financial markets: perspectives from deep learning”. In: Quantitative\nFinance 19.9 (2019), pp. 1449–1459.\n[40] Natalie Stephenson et al. “Survey of machine learning techniques in drug\ndiscovery”. In: Current drug metabolism 20.3 (2019), pp. 185–193.\n[41] Yong Yu et al. “A review of recurrent neural networks: LSTM cells and\nnetwork architectures”. In: Neural computation 31.7 (2019), pp. 1235–\n1270.\n[42] He Zhang, Vishwanath Sindagi, and Vishal M Patel. “Image de-raining\nusing a conditional generative adversarial network”. In:IEEE transactions\non circuits and systems for video technology 30.11 (2019), pp. 3943–3956.\n[43] Jingzhao Zhang et al. “Why gradient clipping accelerates training: A the-\noretical justification for adaptivity”. In: arXiv preprint arXiv:1905.11881\n(2019).\n[44] Daniel M Ziegler et al. “Fine-tuning language models from human prefer-\nences”. In: arXiv preprint arXiv:1909.08593 (2019).\n[45] Tom Brown et al. “Language models are few-shot learners”. In: Advances\nin neural information processing systems 33 (2020), pp. 1877–1901.\n[46] Prafulla Dhariwal et al. “Jukebox: A generative model for music”. In:\narXiv preprint arXiv:2005.00341 (2020).\n[47] Jesse Dodge et al. “Fine-tuning pretrained language models: Weight ini-\ntializations, data orders, and early stopping”. In:arXiv preprint arXiv:2002.06305\n(2020).\n31\n[48] Alexey Dosovitskiy et al. “An image is worth 16x16 words: Transform-\ners for image recognition at scale”. In: arXiv preprint arXiv:2010.11929\n(2020).\n[49] Chuan Li. “OpenAI’s GPT-3 Language Model: A Technical Overview”.\nIn: Lambda Labs Blog (2020). Accessed: [Your Access Date Here]. url:\nhttps://lambdalabs.com/blog/demystifying-gpt-3.\n[50] Francisca Adoma Acheampong, Henry Nunoo-Mensah, and Wenyu Chen.\n“Transformer models for text-based emotion detection: a review of BERT-\nbased approaches”. In: Artificial Intelligence Review (2021), pp. 1–41.\n[51] Yuemin Bian and Xiang-Qun Xie. “Generative chemistry: drug discovery\nwith deep learning generative models”. In: Journal of Molecular Modeling\n27 (2021), pp. 1–18.\n[52] Hila Chefer, Shir Gur, and Lior Wolf. “Transformer interpretability be-\nyond attention visualization”. In: Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition . 2021, pp. 782–791.\n[53] Nelson Elhage et al. “A mathematical framework for transformer circuits”.\nIn: Transformer Circuits Thread 1 (2021).\n[54] Ruidan He et al. “On the effectiveness of adapter-based tuning for pre-\ntrained language model adaptation”. In: arXiv preprint arXiv:2106.03164\n(2021).\n[55] Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha.\n“Ammus: A survey of transformer-based pretrained models in natural lan-\nguage processing”. In: arXiv preprint arXiv:2108.05542 (2021).\n[56] Maithra Raghu et al. “Do Vision Transformers See Like Convolutional\nNeural Networks?” In: CoRR abs/2108.08810 (2021). arXiv: 2108.08810.\nurl: https://arxiv.org/abs/2108.08810.\n[57] Aditya Ramesh et al. “Zero-shot text-to-image generation”. In: Interna-\ntional Conference on Machine Learning . PMLR. 2021, pp. 8821–8831.\n[58] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. “A primer in BERTol-\nogy: What we know about how BERT works”. In: Transactions of the\nAssociation for Computational Linguistics 8 (2021), pp. 842–866.\n[59] Andrew Silva, Pradyumna Tambwekar, and Matthew Gombolay. “To-\nwards a comprehensive understanding and accurate evaluation of societal\nbiases in pre-trained transformers”. In: Proceedings of the 2021 Confer-\nence of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies. 2021, pp. 2383–2389.\n[60] Eva Cetinic and James She. “Understanding and creating art with AI:\nReview and outlook”. In: ACM Transactions on Multimedia Computing,\nCommunications, and Applications (TOMM) 18.2 (2022), pp. 1–22.\n[61] Deep Ganguli et al. “Predictability and surprise in large generative mod-\nels”. In: Proceedings of the 2022 ACM Conference on Fairness, Account-\nability, and Transparency. 2022, pp. 1747–1764.\n32\n[62] Kai Han et al. “A survey on vision transformer”. In: IEEE transactions\non pattern analysis and machine intelligence 45.1 (2022), pp. 87–110.\n[63] John Hewitt, Christopher D Manning, and Percy Liang. “Truncation sam-\npling as language model desmoothing”. In:arXiv preprint arXiv:2210.15191\n(2022).\n[64] Salman Khan et al. “Transformers in vision: A survey”. In: ACM comput-\ning surveys (CSUR) 54.10s (2022), pp. 1–41.\n[65] Tianyang Lin et al. “A survey of transformers”. In: AI Open (2022).\n[66] Daniel Martin et al. “Scangan360: A generative model of realistic scan-\npaths for 360 images”. In: IEEE Transactions on Visualization and Com-\nputer Graphics 28.5 (2022), pp. 2003–2013.\n[67] Goran S Nikoli´ c et al. “A survey of three types of processing units: CPU,\nGPU and TPU”. In: 2022 57th International Scientific Conference on In-\nformation, Communication and Energy Systems and Technologies (ICEST).\nIEEE. 2022, pp. 1–6.\n[68] Sayak Paul and Pin-Yu Chen. “Vision transformers are robust learners”.\nIn: Proceedings of the AAAI conference on Artificial Intelligence . Vol. 36.\n2. 2022, pp. 2071–2081.\n[69] Romal Thoppilan et al. “Lamda: Language models for dialog applica-\ntions”. In: arXiv preprint arXiv:2201.08239 (2022).\n[70] Qingsong Wen et al. “Transformers in time series: A survey”. In: arXiv\npreprint arXiv:2202.07125 (2022).\n[71] Frank F Xu et al. “A systematic evaluation of large language models of\ncode”. In: Proceedings of the 6th ACM SIGPLAN International Sympo-\nsium on Machine Programming . 2022, pp. 1–10.\n[72] Sabeen Ahmed et al. “Transformers in time-series analysis: A tutorial”.\nIn: Circuits, Systems, and Signal Processing (2023), pp. 1–34.\n[73] Abdulaziz Amer Aleissaee et al. “Transformers in remote sensing: A sur-\nvey”. In: Remote Sensing 15.7 (2023), p. 1860.\n[74] S´ ebastien Bubeck et al. “Sparks of artificial general intelligence: Early\nexperiments with gpt-4”. In: arXiv preprint arXiv:2303.12712 (2023).\n[75] Mingqi Gao et al. “Human-like summarization evaluation with chatgpt”.\nIn: arXiv preprint arXiv:2304.02554 (2023).\n[76] Roberto Gozalo-Brizuela and Eduardo C Garrido-Merchan. “ChatGPT\nis not all you need. A State of the Art Review of large Generative AI\nmodels”. In: arXiv preprint arXiv:2301.04655 (2023).\n[77] Ziwei Ji et al. “Survey of hallucination in natural language generation”.\nIn: ACM Computing Surveys 55.12 (2023), pp. 1–38.\n[78] Wenxiang Jiao et al. “Is ChatGPT a good translator? A preliminary\nstudy”. In: arXiv preprint arXiv:2301.08745 (2023).\n33\n[79] OpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL].\n[80] Fahad Shamshad et al. “Transformers in medical imaging: A survey”. In:\nMedical Image Analysis (2023), p. 102802.\n[81] Hugo Touvron et al. “Llama: Open and efficient foundation language mod-\nels”. In: arXiv preprint arXiv:2302.13971 (2023).\n[82] Bohan Zhuang et al. “A survey on efficient training of transformers”. In:\narXiv preprint arXiv:2302.01107 (2023).\n34"
}