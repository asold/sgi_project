{
  "title": "Learning Video Representations using Contrastive Bidirectional Transformer",
  "url": "https://openalex.org/W2975357369",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A249285529",
      "name": "Sun Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214330933",
      "name": "Baradel, Fabien",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2337523870",
      "name": "Murphy, Kevin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2565758713",
      "name": "Schmid, Cordelia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2963542293",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2511428026",
    "https://openalex.org/W2926645869",
    "https://openalex.org/W2941818575",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2922303317",
    "https://openalex.org/W2122476475",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W2949759968",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964345931",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W2784025607",
    "https://openalex.org/W2056120433",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2962870068",
    "https://openalex.org/W2964037671",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2944828972",
    "https://openalex.org/W2984008963",
    "https://openalex.org/W2799247651",
    "https://openalex.org/W2126579184",
    "https://openalex.org/W2099614498",
    "https://openalex.org/W2962865004",
    "https://openalex.org/W2952633803",
    "https://openalex.org/W2949517790",
    "https://openalex.org/W2796207103",
    "https://openalex.org/W2794481829",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2937170809",
    "https://openalex.org/W2955874753",
    "https://openalex.org/W2964700958",
    "https://openalex.org/W2960747818",
    "https://openalex.org/W2972780057",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2948242301",
    "https://openalex.org/W2789198060",
    "https://openalex.org/W2963426332",
    "https://openalex.org/W2963631366",
    "https://openalex.org/W2618799552",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W2795410839",
    "https://openalex.org/W2783047733",
    "https://openalex.org/W2962795934",
    "https://openalex.org/W219040644",
    "https://openalex.org/W2487442924",
    "https://openalex.org/W2109698606",
    "https://openalex.org/W2962688385",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2962756039",
    "https://openalex.org/W2968880719",
    "https://openalex.org/W2773514261",
    "https://openalex.org/W2235034809",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2949099979"
  ],
  "abstract": "This paper proposes a self-supervised learning approach for video features that results in significantly improved performance on downstream tasks (such as video classification, captioning and segmentation) compared to existing methods. Our method extends the BERT model for text sequences to the case of sequences of real-valued feature vectors, by replacing the softmax loss with noise contrastive estimation (NCE). We also show how to learn representations from sequences of visual features and sequences of words derived from ASR (automatic speech recognition), and show that such cross-modal training (when possible) helps even more.",
  "full_text": "Under review\nLEARNING VIDEO REPRESENTATIONS USING\nCONTRASTIVE BIDIRECTIONAL TRANSFORMER\nChen Sun1 Fabien Baradel1,2 Kevin Murphy1 Cordelia Schmid1\n1Google Research 2Univ. Lyon, INSA-Lyon, CNRS, LIRIS\n{chensun,fbaradel,kpmurphy,cordelias}@google.com\nABSTRACT\nThis paper proposes a self-supervised learning approach for video features that\nresults in signiÔ¨Åcantly improved performance on downstream tasks (such as video\nclassiÔ¨Åcation, captioning and segmentation) compared to existing methods. Our\nmethod extends the BERT model for text sequences to the case of sequences of\nreal-valued feature vectors, by replacing the softmax loss with noise contrastive\nestimation (NCE). We also show how to learn representations from sequences\nof visual features and sequences of words derived from ASR (automatic speech\nrecognition), and show that such cross-modal training (when possible) helps even\nmore.\n1 I NTRODUCTION\nRecently there has been a lot of progress in self-supervised representation learning for textual\nsequences, followed by supervised Ô¨Åne-tuning (using small labeled datasets) of shallow (often linear)\ndecoders on various downstream NLP tasks, such as sentiment classiÔ¨Åcation. In this paper, we\nbuild on this work and propose a new method for self-supervised representation learning for videos,\noptionally accompanied by speech transcripts generated by automatic speech recognition (ASR). We\nshow that Ô¨Åne-tuning linear decoders together with our self-supervised video representations, can\nachieve state of the art results on various supervised tasks, including video classiÔ¨Åcation, segmentation\nand captioning.\nOur approach builds on the popular BERT (Bidirectional Encoder Representations from Transformers)\nmodel (Devlin et al., 2018) for text. This uses the Transformer architecture (Vaswani et al., 2017) to\nencode long sentences, and trains the model using the ‚Äúmasked language modeling‚Äù (MLM) training\nobjective, in which the model must predict the missing words given their bidirectional context. The\nMLM loss requires that each token in the sequence be discrete. The VideoBERT model of (Sun et al.,\n2019a) therefore applied vector quantization (VQ) to video frames before passing them (along with\noptional ASR tokens) to the BERT model. Unfortunately, VQ loses Ô¨Åne-grained information that is\noften critical for downstream tasks. More recently, several papers (e.g., VilBERT (Lu et al., 2019)\nand LXMERT (Tan & Bansal, 2019)) proposed to address this limitation by directly measuring the\nvisual similarity between frames using pre-trained visual encoders.\nIn this paper, we propose a way to train bidirectional transformer models on sequences of real-\nvalued vectors (e.g., video frames), x1:T , using noise contrastive estimation (NCE), without needing\npre-trained visual encoders. We call our method ‚ÄúContastive Bidirectional Transformer‚Äù or CBT.\nWe also develop a method that combines x1:T with an optional sequence of discrete tokens, y1:T‚Ä≤\n(e.g., derived from ASR). In contrast to the VideoBERT paper (Sun et al., 2019a), we provide a\n‚Äúlightweight‚Äù way of combining these signals after training each modality separately. In particular, we\npropose a cross-modal transformer to maximize the mutual information betweenx1:T and y1:T‚Ä≤ at the\nsequence level (rather than at the frame level). This method is robust to small misalignments between\nthe sequences (e.g., if the words at time t do not exactly correspond to what is visually present in\nframe t).\nWe demonstrate the effectiveness of the proposed approach for learning short-term visual representa-\ntions, as well as longer term temporal representations. For visual representations, we encode each\nwindow of K frames using a 3D convolutional neural network S3D (Xie et al., 2018), and then pass\n1\narXiv:1906.05743v2  [cs.LG]  27 Sep 2019\nUnder review\nWord\nembedding BERT\nCBTS3D\nCross-modal\nTransformer\nClassifier\nClassifier\nCaptioning\nTemporal\nSegmentation\nLcross\nLvisual\nLclass\nLseq\nLseq\nLclass\nHowTo(U)\nHowTo(U)\nKinetics(U)\nCOIN(L)\nActNet(L)\n50Salads(L)\nBreakfast(L)\nYouCook2(L)\nUCF101(L)\nHMDB51(L)\nùë¶\"‚à∂$%\nùë•\"‚à∂$ ùëí(\n\"‚à∂$\nùëí)\n\"‚à∂$%\n‚Ñé)\n\"‚à∂$%\n‚Ñé(\n\"‚à∂$\n‚Ñé()\n\"‚à∂$+$%\n‚Ñé(\n\"‚à∂$\n‚Ñé(\n\"‚à∂$\n‚Ñé(\n\"‚à∂$\n‚Ñé(\n\"‚à∂$\nùëí(\n\"‚à∂$\nASR\nVideo\n¬´ Let‚Äôs go open up our camper door ¬ª\nDownstream Tasks\nPre-training\nFigure 1: Summary of our method for training and evaluation. The blocks above the line are\npre-trained in an self-supervised way. The solid blocks represent the BERT language model, which is\npre-trained on web text and frozen (see section 3.1). The black CBT visual block is trained using\nNCE loss on unlabeled HowTo or Kinetics videos (see section 3.2). The red cross-modal transformer\nis trained using cross-modal loss on HowTo with ASR (see section 3.3). The components below the\nline are trained in a supervised way on various tasks. The purple block is trained for next action\nprediction on ActivityNet, Breakfast, and 50Salads (see section 4.2). The blue block is trained for\nvideo classiÔ¨Åcation on UCF101 and HMDB501 (see section 4.1). The green blocks are trained\non captioning and video segmentation tasks, which are described in the supplementary material\n(section 6.1 and section 6.2). Lseq refers to cross-entropy sequence loss.\nthis sequence of features to the CBT model for self-supervised pretraining with the NCE loss on the\nunlabeled Kinetics dataset (Kay et al., 2017). We then Ô¨Åne-tune a linear classiÔ¨Åer for video classiÔ¨Å-\ncation on UCF101 and HMDB51. We show that our method outperforms previous state-of-the-art\nself-supervised methods by large margins (UCF101 from 75.7% to 79.5% and HMDB51 from 35.7%\nto 44.6%).\nFor temporal representations, we encode each window of K frames using a S3D network that is\npretrained on Kinetics, and then ‚Äúfrozen‚Äù. We then pass this sequence of features to the CBT model\nfor self-supervised pretraining with the NCE loss on the unlabeled HowTo100M dataset (Miech\net al., 2019b). We also evaluate the effects of running ASR on HowTo100M, and passing this to our\ncross-modal transformer as an additional signal. We then Ô¨Åne-tune various shallow decoders for a\nvariety of tasks, including video classiÔ¨Åcation, segmentation and captioning. We show large gains\ncompared to previous methods, especially when we use cross-modal pretraining.\nSee Ô¨Åg. 1 for a summary of our training method and evaluation protocol.\n2 R ELATED WORK\nVideo representations. Most existing work on learning video representations, such as (Simonyan\n& Zisserman, 2014; Varol et al., 2018; Carreira & Zisserman, 2017; Xie et al., 2018; Tran et al.,\n2014), only captures a few seconds of video. Long-term context can be encoded by recurrent neural\nnetworks (Abu Farha et al., 2018; Sun et al., 2019b), graph convolutional networks (Zhang et al.,\n2019), or long-term feature banks (Wu et al., 2019), but these are all supervised methods. Some recent\nwork have been done on learning self-supervised video representation (V ondrick et al., 2016; Wang\n& Gupta, 2015; Misra et al., 2016; Sermanet et al., 2018; Han et al., 2019; Xu et al., 2019; Wang\net al., 2019a) by deÔ¨Åning pretext tasks such as ordering (Lee et al., 2017), rotation (Jing et al., 2018),\n2\nUnder review\ntemporal cycle consistency (Wang et al., 2019b; Dwibedi et al., 2019) or colorization (V ondrick et al.,\n2018) but similar to their supervised counterparts they capture only few seconds.\nSelf-supervised context modeling. Recently, there has between a lot of work on self-supervised\ncontext modeling for language representations (Peters et al., 2018; Radford et al., 2019; Devlin\net al., 2018). In particular, the BERT model, which stands for Bidirectional Encoder Representations\nfrom Transformers (Devlin et al., 2018), pre-trains deep bidirectional representations by jointly\nconditioning on both left and right context in all layers. The pre-trained BERT representations can be\nÔ¨Åne-tuned with just one additional output layer to create state-of-the-art models for a wide range of\nNLP tasks, such as question answering and linguistic entailment. Our representation builds on this\napproach and adapts it to continuous video data by using a contrastive loss.\nMutual information estimation and maximization.For representation learning, a signal encoder\ncan be trained to maximize the mutual information (MI) between the input signal and its encoded\noutputs, or the encoded outputs of the signal and its context (see e.g., (Belghazi et al., 2018; Hjelm\net al., 2019; Oord et al., 2018; Tian et al., 2019). In particular, contrastive predictive coding\n(CPC) (Oord et al., 2018) uses noise contrastive estimation (Gutmann & Hyv√§rinen, 2010) to\nmaximize the MI lower bound. Unlike CPC, which relies on auto regressive models to encode\ncontext, we use BERT to encode bidirectional context within each sequence, and across different\nmodalities.\nCross-modal learning. The multi-modality of video is a rich source of information for self-\nsupervised learning of video representations. Since videos contain both visual and audio signals that\nare roughly synchronized, the two modalities can supervised each other, as explored in prior work\nsuch as (Aytar et al., 2016; Owens et al., 2016b;a; Zhao et al., 2018). Another common form of weak\nsupervision is based on video and language, where language is either obtained by automatic speech\nrecognition (ASR) or from additional textual description. Language can be leveraged by Ô¨Ånding a\njoint embedding space for both visual and textual modalities or by learning an alignment between the\ntwo modalities (Miech et al., 2018; Alayrac et al., 2016; Zhou et al., 2018a).\nRecently, several concurrent approaches (Sun et al., 2019a; Lu et al., 2019; Li et al., 2019a; Su et al.,\n2019; Tan & Bansal, 2019; Li et al., 2019b) generalize the BERT architecture and MLM objective to\nlearn visual-linguistic representations. They assume the visual representations to be Ô¨Åxed and given\nby supervised pre-trained visual encoders, and deÔ¨Åne the visual MLM objective in terms of visual\nsimilarities (e.g. via vector quantization or measuring L2 distance) between the original and predicted\nvisual representations. To the best of our knowledge, our proposed CBT is the Ô¨Årst to demonstrate\nthe effectiveness of BERT-style pre-training in a fully self-supervised way for video.\n3 M ETHOD\nWe Ô¨Årst give an overview of the BERT model for learning from sequences of words,y1:T‚Ä≤ . We then\ndiscuss an extension to the case of sequences of video frames, x1:T . Finally, we discuss how to learn\nfrom both kinds of data, even when not perfectly aligned.\n3.1 T HE BERT MODEL\nThe BERT model (Devlin et al., 2018) takes in a sequence of discrete tokens, y1:T‚Ä≤ , where yt ‚àà\n{1, . . . , K}, embeds each one into a dense vector, ey\nt ‚ààD, and then emits a sequence of dense output\nvectors, hy\nt ‚ààDy, which are computed using a transformer (Vaswani et al., 2017). The output\nsequence captures local and global semantic information about the input sequence.\nThe main training objective for BERT is to minimize the pseudo negative log likelihood, deÔ¨Åned by\nLbert = ‚àíEy‚àºD\nT‚àë\nt=1\nlog p(yt|y‚àít) (1)\nwhere y‚àít is the sequence of all words except the t‚Äôth, and\np(yt|y‚àít) = exp(eT\nt ÀÜet)‚àëK\nk=1 exp(fenc(k)T ÀÜet)\n(2)\n3\nUnder review\nHere fenc(k) is an embedding lookup table for token k, et = fenc(yt) is the embedding for the token\nat t, e‚àít = [fenc(yl) : l < t,0, fenc(yl) : l > t] is the embedding sequence for the context, and\nÀÜet = gcontext(e‚àít) is a multi-layer multi-headed transformer network that takes a T √óD feature\nmatrix as input (masked at location t) and returns a matrix of the same size.\n3.2 T HE CBT MODEL\nThe BERT model requires a Ô¨Åxed discrete vocabulary. However, for images and videos, the inputs\nare real-valued vectors. We propose to use the softmax version of the noise contrastive estimation\n(NCE) loss (Jozefowicz et al., 2016), which has the form\nLvisual = ‚àíEx‚àºD\n‚àë\nt\nlog NCE(xt|x‚àít) (3)\nwhere\nNCE(xt|x‚àít) = exp(eT\nt ÀÜet)\nexp(eT\nt ÀÜet) +‚àë\nj‚ààneg(t) exp(eT\nj ÀÜet) (4)\nwhere et = fenc(xt) is the output of a 3D CNN applied to a small window around frame t (we use\nthe S3D model from (Xie et al., 2018)), ÀÜet = gcontext(e‚àít) is the output of a visual transformer, and\nneg(t) is a set of (indices of) \"negative examples\" (in practice we use all the other frames from the\nsame minibatch as frame t).\nIntuitively the NCE loss encourages the model to learn to identify the correct frame (given the\ncontext) compared to a set of negative distractors. More formally, it can be shown that the NCE\nloss maximizes (a lower bound on) the mutual information (MI) between xt and x‚àít (see e.g.,\n(Oord et al., 2018; Poole et al., 2019)). This loss has been used in other work on self-supervised\nvisual representation learning, e.g., in the deep infomax (DIM) (Hjelm et al., 2019) and contrastive\npredictive coding (CPC) (Oord et al., 2018) papers. In DIM, the context predictor uses a CNN\napplied to neighboring patches in the same image, and in CPC, the context predictor uses a causal\nautoregressive model applied to \"earlier\" patches in the same image. In our CBT method, the context\npredictor is a bidirectional transformer applied to video frames.\n3.3 T HE CROSS -MODAL CBT MODEL\nIn this section we show how to learn useful representations from sequences of continuous visual\nfeatures (from video) and sequences of discrete words (from ASR). More precisely, assume we\nhave two sequences, x = x1:T representing video, and y = y1:T‚Ä≤ , representing ASR. Note that the\nsequences may not be perfectly aligned, since a person may speak about things at time t that are not\nvisible in the frame at time t. Therefore it does not make sense to try to maximize the MI between xt\nand yt at the frame level. Instead, we try to maximize MI between x and y at the sequence level.\nTo do this, we Ô¨Årst encode each sequence using CBT and BERT to get hx\n1:T = CBT(x1:T ) and\nhy\n1:T‚Ä≤ = BERT(y1:T‚Ä≤ ), as shown in Ô¨Åg. 1. We then concatenate these sequences and pass them to\na shallow cross-modal transformer to produce hxy\n1:T+T‚Ä≤ . Finally, we pass this to a shallow MLP to\ncompute an MI-like score MI(x, y) =f(hxy\n1:T+T‚Ä≤ ). (Here f() extracts the features from hxy\n0 , but\nit could also use average pooling.) This model is trained using Lcross = ‚àíE(x,y)‚àºDlog NCE(y|x),\nwhere\nNCE(y|x) = MI(x, y)\nMI(x, y) +‚àë\ny‚Ä≤‚ààNeg(y) MI(x, y‚Ä≤) (5)\nwhere Neg(y) is a set of ASR sequences not associated with video x.\nNote that our cross-modal training assumes there is something in common between the video and\ntext streams. In practice this means we have to focus on certain kinds of videos, such as instructional\nvideos, in which the spoken words and the visual content are \"about\" the same thing. By contrast,\narbitrary videos may contain speech content that is unrelated to the visual frames (e.g., imagine a\nconversation between two characters in a drama or soap opera).\n4\nUnder review\n3.4 O VERALL MODEL\nOur overall model has three components: one transformer (BERT) that takes discrete ASR tokens,\none transformer that takes continuous video features, and a third transformer to estimate mutual\ninformation between the two modalities. We jointly train the model by optimizing:\nLcbt = wbertLbert + wvisualLvisual + wcrossLcross (6)\nWe Ô¨Åx wbert = 0, since we use a pre-trained (frozen) BERT model for ASR. We setwvisual = 1, and\neither set wcross = 1or wcross = 0, depending on whether we use cross-modal training or not.\n4 E XPERIMENTS\nIn this section we conduct experiments to study the usefulness of the representations learned by our\nCBT model for various downstream tasks, including action anticipation, video captioning and action\nsegmentation. We also consider ablations to our model, such as turning cross-modal training on or\noff, varying the size of the visual transformer, and varying the amount of unlabeled pre-training data.\n4.1 L EARNING SELF -SUPERVISED VISUAL REPRESENTATIONS\nIn this section we evaluate self-supervised visual representation learning on the downstream task\nof action recognition. Existing methods use various proxy tasks to pre-train feature extractors in a\nself-supervised way, and then use supervised learning to train linear classiÔ¨Åers on top of these frozen\nrepresentations, or Ô¨Åne-tune the classiÔ¨Åer plus feature extractor end-to-end. We follow the same\nprotocol.\nExperimental setup. We follow the standard practice from recent works (Han et al., 2019; Jing\net al., 2018; Wang et al., 2019a) by pre-training our model (S3D feature extractor followed by CBT)\non unlabeled RGB-only Kinetics (Kay et al., 2017) videos. Kinetics is the largest action recognition\ndataset containing 500k short clip videos (about 10 seconds long) for 600 human actions classes.\nWe take 1 minute sliding windows from the original YouTube videos they are selected from. We\nthen use the (average pooled) S3D features as input to a linear classiÔ¨Åer, and train the classiÔ¨Åer on\nvarious datasets. For evaluation, we use UCF101 (Soomro et al., 2012), which contains 13,320 videos\nfrom 101 human actions, and HMDB51 (Kuehne et al., 2011), which contains 7,000 videos from 51\nclasses. For both dataset we report the action recognition test accuracy averaged over the 3 standard\ntrain/test splits.\nTo pre-train our CBT model, we use a curriculum learning strategy, by Ô¨Årst pre-training the S3D\nfeature extractor on unsupervised clips using the loss proposed in the 3DRotNet paper (Jing et al.,\n2018) on 16 consecutive frames. We then jointly Ô¨Åne-tune the last blocks of S3D ( Mixed5b and\nMixed5c) with the visual transformer using the CBT visual loss. We observed that this strategy\ngave us better results on downstream tasks compared to pre-training from scratch using CBT; it also\nsaves memory and computation, which allows us to use longer sequences.\nDuring pre-training, we set the number of visual transformer layers to be 2, number of attention\nheads to be 4, and hidden unit size to be 768. We randomly take 60-second sliding windows from the\nKinetics videos, and break them into sequences of 1.5-second clips. We randomly mask out 6 out of\nthe 40 possible locations. We resize the video frames to 112 by 112 before encoding them with S3D\nto save memory. The model is trained for 500K iterations with batch size of 128 and learning rate of\n1e-5.\nComparison of pre-training strategies. In Table 1(Left) we compare our way of pre-training\nthe S3D model (i.e., using CBT visual loss) to existing approaches. In particular, we consider the\nShufÔ¨Çe&Learn (Misra et al., 2016) and 3DRotNet (Jing et al., 2018) proxy tasks. We reimplement\nthe two methods using S3D CNN, and pre-train them on the same Kinetics data. We also consider\nrandom initialization. We report classiÔ¨Åcation results on UCF101 and HMDB51 using frozen features\nand Ô¨Åne-tuned features passed to a linear classiÔ¨Åer. We see that our method outperforms existing\ntraining methods by a large margin.\nComparison to existing methods.Table 1(Right) compares the results of our method to various\nstate-of-the art self-supervised methods. (We only report the results of Ô¨Åne-tuning, which are better\n5\nUnder review\nFine-tuned Frozen\nMethod UCF101 HMDB51UCF101 HMDB51\nRandom 63.3 29.7 25.7 11.5\nShufÔ¨Çe&Learn‚àó 68.7 35.8 26.5 12.6\n3DRotNet‚àó 75.3 40.0 47.7 24.8\nCBT 79.5 44.5 54.0 29.5\nMethod DatasetUCF101 HMDB51\nShufÔ¨Çe&Learn (Misra et al., 2016)UCF101 50.2 18.1OPN (Lee et al., 2017)UCF101 59.6 23.8ClipOrder (Xu et al., 2019)UCF101 72.4 30.9Wang et al. (2019a) Kinetics 61.2 33.43DRotNet (Jing et al., 2018)Kinetics 66.0 37.1DPC (Han et al., 2019)Kinetics 75.7 35.7CBT Kinetics 79.5 44.6\nTable 1: Self-supervised action recognition accuracy. (Left) We show the effect of different pre-\ntraining strategies on our model. ‚àóare our reimplementations. (Right) Comparison to the state of the\nart on UCF101 and HMDB51. (We report the performances from the original papers.)\nfor all methods than using frozen features.) Note that the methods differ both in architecture and\ntraining objective. First we compare against 2DCNN approaches ShufÔ¨Çe&Learn (Misra et al., 2016)\nand OPN (Lee et al., 2017). Our method outperforms both by a very large margin. This can be\nexplained by the fact that our backbone is a 3DCNN architecture, which is much more powerful\nthan 2D CNNs for video action recognition. Next we compare against approaches using 3DCNN\narchitectures similar to our S3D. We also outperform all of these methods by a very large margin,\nand even beat the most recent approach, DPC (Han et al., 2019), by 3.8 points on UCF101 and 8.9\npoints on HMDB51. We believe this is due to the better contextual features that we are able to learn\nby using the transformer model and NCE loss.\n4.2 L EARNING SELF -SUPERVISED TEMPORAL REPRESENTATIONS\nIn this section, we consider self-supervised training of representations from long videos, followed by\nsupervised Ô¨Åne-tuning on various downstream tasks. To avoid running out of memory, we pre-train\nthe S3D model on the task of classifying (short) Kinetics videos. We then freeze this feature extractor,\nand focus on learning longer-term temporal representations using the self-supervised CBT model.\nThat is, we precompute short term representations ex\nt = fenc(xt) for all videos using S3D, and focus\non learning global representations hx\n1:T = CBT(ex\n1:T ).\nFor the self-supervised pre-training, we use unlabeled videos from the HowTo100M dataset (Miech\net al., 2019b). This contains ‚àº1M instructional videos (details below), so the speech is informative\nabout the vision. Therefore, we also run ASR on this dataset and use cross-modal training to compute\nhxy\n1:T = CrossTransformer(hx\n1:T , hy\n1:T‚Ä≤ ), where hy\n1:T‚Ä≤ = BERT(y1:T‚Ä≤ ) is the output of a pretrained\nBERT model. To evaluate the performance of the pre-trained features (both visual and cross-modal),\nwe consider three tasks: \"action anticipation\" (i.e, predicting the next action label to occur given\nsome temporal preÔ¨Åx); video captioning (see section 6.1 in supplementary), and video segmentation\n(see section 6.2 in supplementary).\nDetails on self-supervised pre-training.We pre-train our model on HowTo100M (Miech et al.,\n2019b). This is a new large-scale dataset of 1.22M narrated instructional videos available on YouTube\nand covers among 23k different visual tasks. The average duration of a video is 6.5 minutes and there\nare on average 110 clips per video.\nTo extract visual features, we resize the videos to be 224 by 224, and compute visual features over\nsliding windows of 1.5 seconds (30 frames at 20 FPS) using an S3D network (Xie et al., 2018)\npre-trained on the Kinetics dataset (Kay et al., 2017). We take the feature embeddings from the Ô¨Ånal\nMixed5c block of the S3D network before the classiÔ¨Åcation layer, and average pool the features\nspatio-temporally to get vectors of size 1024. We follow the same strategy for extracting visual\nfeatures on the downstream tasks. The visual features are not Ô¨Åne-tuned during pre-training or when\napplied to downstream tasks.\nTo extract text features, we convert the audio track into sentences by calling the YouTube ASR API,\nfollowed by an off-the-shelf LSTM-based language model to add punctuation, thus converting the\nstream of words into a stream of sentences. We then follow the standard preprocessing steps from\nBERT (Devlin et al., 2018), and use WordPieces tokenization with the same vocabulary of 30,000\ntokens. To encode ASR tokens, we take the pre-trained BERT-base architecture, which has 12 layers\nof Transformers, each of which has 768 hidden units and 12 attention heads.\n6\nUnder review\nTo construct paired inputs to pre-train CBT with the cross-modal objective, we iterate over all the\nsentence segments in the HowTo100M dataset, and concatenate short segments until they reach\nthe maximal length of 48 tokens. We then retrieve up to 48 visual features (72 seconds) starting at\nthe same locations in videos. We mask out 6 out of 48 features randomly. For both the video and\ncross-modal transformers, we set the total hidden units per layer to 768. We Ô¨Åx the number of layers\nto 1 for the cross-modal transformer and explore the optimal number of layers and attention heads for\nthe video transformer. Their weights are randomly initialized.\nFor pre-training the CBT model on HowTo100M, we use 32 Cloud TPUs and a total batch size of\n128. We use the Adam optimizer with an initial learning rate of 1e-5 and a linear decay learning rate\nschedule. The model is trained for 2 million iterations, which takes around 2 days.\nDetails on supervised evaluation.We evaluate the pre-trained temporal representations by transfer\nlearning to downstream tasks. We Ô¨Årst focus on action anticipation, whose goal is to predict the future\nactions by observing video sequences preceding them. In the supplementary, we also present results\non video captioning and action segmentation.\nFor the action anticipation task, we follow the standard setup described from recent work (Abu Farha\net al., 2018; Miech et al., 2019a). We consider three datasets: the Breakfast dataset (Kuehne et al.,\n2014) is composed of 1712 cooking videos and contains 48 Ô¨Åne-grained action classes; the 50Salads\ndataset (Stein & McKenna, 2013) contains 50 cooking videos with 17 Ô¨Åne-grained action classes; and\nthe ActivityNet 200 dataset (Heilbron et al., 2015) contains 19994 YouTube videos with 200 human\naction classes (beyond the cooking and instructional domains). The inputs are video segments up to\nTc seconds before the actual action starts, and the outputs are categorical labels. For comparison with\nprevious approaches, we set Tc = 1for Breakfast and 50Salads, and Tc = 5for ActivityNet.\nFor all experiments except for ablation on sequence lengths, we Ô¨Åx the input sequence length to be\n72 seconds (corresponding to 48 sliding windows), features for videos shorter than 72 seconds are\nzero-padded. The outputs of CBT are transformed features with the same length, we take the output\nfeature at the last non-padded position to represent the whole sequence, and put a linear classiÔ¨Åer\non top to predict the future actions. We jointly Ô¨Åne-tune the weights of visual transformers with the\nlinear classiÔ¨Åer. The text transformers and cross-modal transformers are not used during Ô¨Åne-tuning\nsince only visual inputs are available. We train our model for 5 epochs using a batch size of 32 with\nthe Adam optimizer and an initial learning rate of 1e-3. We report the top-1 accuracy on the test sets\nfor Breakfast and 50Salads, and on the validation set for ActivityNet.\nComparison to existing methods.Table 2 (Left) compares to existing methods. First we compare\nto two existing self-supervised approaches, namely V ondrick et al. (2016) and Sun et al. (2019a).\nOur approach outperforms both by a very large margin. The difference with VideoBERT (Sun et al.\n(2019a)), which also relies on a BERT model, can be explained by the fact that it quantizes the visual\nfeatures into tokens and, hence loses discriminative power. Next we compare to some recent methods\nthat train deep classiÔ¨Åers end-to-end, namely (Abu Farha et al., 2018) and (Miech et al., 2019a). We\noutperform both by a large margin.\nMethod Self-superBkfstSaladsActNet\nV ondrick et al. (2016)Y 8.1 6.2 -Abu Farha et al. (2018)N 30.1 30.1 -Sun et al. (2019a)Y 9.1 5.5 -Miech et al. (2019a)N 32.3 - 54.8\nCBT Y 32.7 39.1 59.8\nWindow Bkfst Salads ActNet(in sec.)AvgPool LSTM CBTAvgPool LSTM CBTAvgPool LSTM CBT\n1.5 20.3 - - 30.2 - - 42.9 - -15 23.5 24.2 26.3 36.8 29.7 36.6 54.2 54.2 54.430 24.2 25.6 29.8 34.3 33.9 37.8 52.9 57.1 57.545 22.7 26.2 30.8 32.2 35.3 38.5 50.7 57.3 58.872 21.9 27.0 32.7 26.7 35.6 39.1 46.2 57.3 59.8\nTable 2: Action anticipation accuracy. (Left) Comparison to the state of the art on Breakfast, 50\nSaldads, ActivityNet. Self-super = Y means the model was pre-trained in a self-supervised way, and\nthen Ô¨Åne-tuned using a linear classiÔ¨Åer. Self-super = N means the model is trained end-to-end on\nthe speciÔ¨Åc task. (Right) Comparison with the average pooling and LSTM baselines on 50Salads\nBreakfast, 50Salads and ActivityNet. We vary the observation window lengths (sec.)\nEffect of video length.In Table 2 (Right) we show the impact of the length of the training videos on\nthe performance. We compare with two baselines, average pooling (AvgPool) and LSTM (Hochreiter\n& Schmidhuber, 1997). The AvgPool baseline simply computes the average of all input visual\nfeatures over time. The LSTM baseline takes the same sequence of S3D features but recurrently\nupdates its hidden states over time. The Ô¨Ånal hidden state is used for classiÔ¨Åcation. We adjust the\n7\nUnder review\nhidden unit size of LSTM to make its number of parameters comparable to CBT. We can see that CBT\nsigniÔ¨Åcantly outperforms the two baselines on all three datasets. Moreover, we can see that as the\nobserved video length increases, the performance of CBT monotonically increases, while LSTM and\nAvgPool either plateaus or decreases. These results indicate that CBT is better at modeling long-term\ntemporal context.\nData size (%) Cross-modalBkfst Salads ActNet\n0 x 28.8 35.1 57.410 ‚úì 29.9 37.3 58.325 ‚úì 30.2 37.3 58.450 ‚úì 30.0 37.4 58.575 ‚úì 31.4 38.5 59.3100 x 29.9 37.6 57.6100 ‚úì 32.7 39.1 59.8\nL A Bkfst Salads ActNet\n1 4 29.6 34.89 59.3\n2 4 32.7 39.1 59.8\n4 4 32.7 38.9 59.2\n8 4 23.2 6.1 58.0\n16 4 9.17 5.8 57.5\nL A Bkfst Salads ActNet\n2 1 30.9 37.5 58.2\n2 2 31.8 36.3 58.3\n2 4 32.7 38.9 59.4\n2 8 30.9 39.9 58.9\n2 16 31.8 39.8 57.8\nTable 3: Ablation study on the action anticipation task. We show accuracy on Breakfast, 50Salads\nand ActivityNet. (Left) Impact of the percentage of HowTo100M videos used, and the cross-modal\nobjective during pre-training. 0% corresponds to no pretraining, ie. using random weights. (Middle,\nRight) Impact of the number of layers (L) and attention heads (A) for the visual transformers.\nEffect of dataset size and cross-modal training. In Table 3 (Left), we study the impact of pre-\ntraining data set. As expected, pre-training with more examples leads to higher performance on all\nthree benchmarks. We also study the impact of cross-modal training. We see this helps signÔ¨Åcantly,\nespecially on the smaller datasets (Breakfast and Salads).\nEffect of model size.In Table 3 (Middle) and (Right), we study the impact of the number of\nlayers (L) and the number of attention heads (A) for the visual transformer. Not surprisingly, model\nperformance initially increases, but surprisingly, it then starts to decrease, in contrast to the case of\nNLP-BERT. We conjecture that this is because our unlabeled pre-training set is much smaller than\nused by the NLP-BERT model. Fortunately, our technique is quite scalable, since we can train the\nvideo representations on top of S3D features using relatively shallow transformers ‚Äî our visual\ntransformer only has 15M parameters, whereas the BERT NLP transformer has 110M parameters.\nApplications to other tasks.In Table 4 we show the results of using of our learned temporal\nrepresentation for video captioning and action segmentation. See section 6.1 and section 6.2 in the\nsupplementary for details.\nMethod BLEU-4 METEOR ROUGE-L CIDEr\nZhou et al. (2018c) 4.38 11.55 27.44 0.38\nS3D 3.24 9.52 26.09 0.31\nVideoBERT 4.33 11.94 28.80 0.55\nCBT 5.12(¬±0.02) 12.97(¬±0.05) 30.44(¬±0.08) 0.64(¬±0.00)\nMethod Frame Acc.\nTang et al. (2019) 25.8\nRichard et al. (2018)21.2\nDing & Xu (2018) 34.3\nCBT 53.9\nTable 4: (Left) Video captioning results on the YouCook2 dataset (Zhou et al., 2018b). We compare\nwith previous state-of-the-art methods by Zhou et al. (2018c) and Sun et al. (2019a), the caption\ndecoder of all methods share the same architecture, the main difference comes from the visual encoder.\n(Right) Action segmentation results on the COIN dataset (Tang et al., 2019). A linear classiÔ¨Åer is\napplied on the sequence of CBT output features for dense frame labeling. We compare with previous\nstate-of-the-art methods using the standard frame accuracy metric.\n5 C ONCLUSION\nWe have shown how to extend the BERT model to learn representations from video in a self-supervised\nway, without needing vector quantization or pre-trained visual features. We have also shown how\nto extend this to the cross-modal setting, when ASR is available. Finally, we demonstrated that our\nmethod learns features that are far more useful than existing self-supervised methods for a variety\nof downstream video tasks, such as classiÔ¨Åcation, captioning and segmentation. We believe that the\nsimplicity and modularity of our method will let us scale to much larger unlabeled video datasets,\nwhich we hope will let us Ô¨Ånally surpass supervised video pretraining (e.g., on Kinetics), just as other\nmethods (e.g., CPC++ (H√©naff et al., 2019)) have recently surpassed supervised image pretraining\n(on ImageNet).\n8\nUnder review\nREFERENCES\nYazan Abu Farha, Alexander Richard, and Juergen Gall. When will you do what? - anticipating temporal\noccurrences of activities. In CVPR, 2018.\nJean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien.\nUnsupervised learning from narrated instruction videos. In CVPR, 2016.\nYusuf Aytar, Carl V ondrick, and Antonio Torralba. Soundnet: Learning sound representations from unlabeled\nvideo. In NeurIPS, 2016.\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and\nR Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018.\nJ. Carreira and A. Zisserman. Quo vadis, action recognition? A new model and the Kinetics dataset. In CVPR,\n2017.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nLi Ding and Chenliang Xu. Weakly-supervised action segmentation with iterative soft boundary assignment. In\nCVPR, 2018.\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal\ncycle-consistency learning. In CVPR, 2019.\nMichael Gutmann and Aapo Hyv√§rinen. Noise-contrastive estimation: A new estimation principle for unnormal-\nized statistical models. In AISTATS, 2010.\nTengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding.\narXiv preprint arXiv:1909.04656, 2019.\nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale\nvideo benchmark for human activity understanding. In CVPR, 2015.\nOlivier J H√©naff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efÔ¨Åcient image\nrecognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and Yoshua Bengio.\nLearning deep representations by mutual information estimation and maximization. In ICLR, 2019.\nSepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation, 1997.\nLonglong Jing, Xiaodong Yang, Jingen Liu, and Yingli Tian. Self-supervised spatiotemporal feature learning via\nvideo rotation prediction, 2018.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of\nlanguage modeling. arXiv preprint arXiv:1602.02410, 2016.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio\nViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint\narXiv:1705.06950, 2017.\nH. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: A large video database for human motion\nrecognition. In ICCV, 2011.\nHilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and semantics of\ngoal-directed human activities. In CVPR, 2014.\nHsin-Ying Lee, Jia-Bin Huang, Maneesh Kumar Singh, and Ming-Hsuan Yang. Unsupervised representation\nlearning by sorting sequences. In ICCV, 2017.\nGen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and\nlanguage by cross-modal pre-training. arXiv preprint arXiv:1908.06066, 2019a.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and\nperformant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019b.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic represen-\ntations for vision-and-language tasks. In NeurIPS, 2019.\n9\nUnder review\nAntoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heteroge-\nneous data. arXiv preprint arXiv:1804.02516, 2018.\nAntoine Miech, Ivan Laptev, Josef Sivic, Heng Wang, Lorenzo Torresani, and Du Tran. Leveraging the present\nto anticipate the future in videos. In CVPRW, June 2019a.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.\nHowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In\nICCV, 2019b.\nIshan Misra, C. Lawrence Zitnick, and Martial Hebert. ShufÔ¨Çe and learn: Unsupervised learning using temporal\norder veriÔ¨Åcation. In ECCV, 2016.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding.\nIn NeurIPS, 2018.\nAndrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H Adelson, and William T Freeman.\nVisually indicated sounds. In CVPR, 2016a.\nAndrew Owens, Jiajun Wu, Josh H McDermott, William T Freeman, and Antonio Torralba. Ambient sound\nprovides supervision for visual learning. In ECCV, 2016b.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL, 2018.\nBen Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On variational bounds\nof mutual information. In ICML, 2019.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\nAlexander Richard, Hilde Kuehne, Ahsan Iqbal, and Juergen Gall. Neuralnetwork-viterbi: A framework for\nweakly supervised video learning. In CVPR, 2018.\nPierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and\nGoogle Brain. Time-contrastive networks: Self-supervised learning from video. In ICRA, 2018.\nKaren Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos.\nIn NeurIPS, 2014.\nK. Soomro, A. Zamir, and M. Shah. UCF101: A dataset of 101 human actions classes from videos in the wild.\nTechnical Report CRCV-TR-12-01, 2012.\nSebastian Stein and Stephen J. McKenna. Combining embedded accelerometers with computer vision for\nrecognizing food preparation activities. In UbiComp, 2013.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic\nvisual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT: A joint model for\nvideo and language representation learning. In ICCV, 2019a.\nChen Sun, Abhinav Shrivastava, Carl V ondrick, Rahul Sukthankar, Kevin Murphy, and Cordelia Schmid.\nRelational action forecasting. In CVPR, 2019b.\nHao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490, 2019.\nYansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou.\nCOIN: A large-scale dataset for comprehensive instructional video analysis. In CVPR, 2019.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint\narXiv:1906.05849, 2019.\nDu Tran, Lubomir D Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. C3d: generic features for\nvideo analysis. arXiv preprint arXiv:1412.0767, 2014.\nG√ºl Varol, Ivan Laptev, and Cordelia Schmid. Long-term temporal convolutions for action recognition. TPAMI,\n2018.\n10\nUnder review\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\nCarl V ondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from unlabeled\nvideo. In CVPR, 2016.\nCarl Martin V ondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking\nemerges by colorizing videos. In ECCV, 2018.\nJiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, and Wei Liu. Self-supervised spatio-\ntemporal representation learning for videos by predicting motion and appearance statistics. In CVPR, 2019a.\nXiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV,\n2015.\nXiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of time.\nIn CVPR, 2019b.\nChao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Kr√§henb√ºhl, and Ross B. Girshick.\nLong-term feature banks for detailed video understanding. In CVPR, 2019.\nSaining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature\nlearning for video understanding. In ECCV, 2018.\nDejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal\nlearning via video clip order prediction. In CVPR, 2019.\nYubo Zhang, Pavel Tokmakov, Cordelia Schmid, and Martial Hebert. A structured model for action detection. In\nCVPR, 2019.\nHang Zhao, Chuang Gan, Andrew Rouditchenko, Carl V ondrick, Josh McDermott, and Antonio Torralba. The\nsound of pixels. In ECCV, 2018.\nLuowei Zhou, Nathan Louis, and Jason J Corso. Weakly-supervised video object grounding from text by loss\nweighting and object interaction. In BMVC, 2018a.\nLuowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instruc-\ntional videos. In AAAI, 2018b.\nLuowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher, and Caiming Xiong. End-to-end dense video\ncaptioning with masked transformer. In CVPR, 2018c.\n11\nUnder review\n6 S UPPLEMENTARY MATERIALS\n6.1 V IDEO CAPTIONING\nIn this section, we apply our model to video captioning.\nDataset. We pretrain our model on HowTo100M, and then use its features as input to a captioning\nmodel (details below) which is trained on the YouCook2 dataset (Zhou et al., 2018b). This contains\n2000 Youtube videos of an average length of 5.26 minutes for a total of 176 hours. The annotations\nconsist of segmentation boundaries and captions with on average 7.7 segments per video and 8.8\nwords per caption. We made sure that there is no overlap between the videos from our pre-training\ndatasets and YouCook2.\nModel. We follow the experimental setup from (Zhou et al., 2018c), where the ground truth video\nsegmentations from YouCook2 are used to train a supervised model mapping video segments to\ncaptions. Our captioning model is a transformer with 2 layers and a hidden layer of size 128. During\ntraining we set the dropout probability to 0.4. We train our model for 10K iterations using batch size\nof 128 with the Adam optimizer and an initial learning rate of 1e-4. We report BLEU, METEOR and\nROUGE metrics on the validation set.\nComparison to other methods. Table 4 shows our results. We outperform a simple baseline\ncomputed using average-pooled S3D features. We also outperform the approach of Zhou et al.\n(2018c) and VideoBERT Sun et al. (2019a) on all reported metrics. The comparison to VideoBERT\nis particularly interesting. The gains suggest that removing the quantization of video features is\nimportant for obtaining a Ô¨Åne-grained video representation. We also observe that the difference\nbetween CBT and VideoBERT is smaller for YouCook2 than for Breakfast and 50Salads action\nanticipation task, possibly because the YouCook2 dataset set is more similar to the cooking videos\nused for pre-training by VideoBERT.\n6.2 A CTION SEGMENTATION\nIn this section, we apply our model to the task of temporal action segmentation.\nDataset. We pretrain our model on HowTo100M and then use its features as input to a linear\nclassiÔ¨Åer (details below) which is trained on the COIN dataset Tang et al. (2019). This contains\n11827 instructional Youtube videos of an average length of 2.36 minutes. The annotations consist of\nsegment boundaries and class label. On average there are 3.91 segments per video each of which\nlasts 14.9 seconds. There are in total 779 classes.\nModel. We extract video features using S3D and feed the sequence to the visual transformer. We use\na Ô¨Åxed size of 72 seconds and use zero-padding for shorter sequences. The overall clip is represented\nby its associated output embedding of size 768. This preprocessing step is frozen. We feed the\nfeatures to a linear classiÔ¨Åer, which we train or model for 100K iterations using batch size of 32 with\nthe Adam optmizer and initial learning rate of 1e-3. At test time we operate on a long video with a\nsliding window of 72 seconds.\nComparison to existing approaches.In Table 4 we compare CBT against various state of the art\napproaches using the frame acuracy as metric, including (Tang et al., 2019), (Richard et al., 2018)\nand (Ding & Xu, 2018). We outperform them by a large margin state (+19.6 points).\n12",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.8959925174713135
    },
    {
      "name": "Computer science",
      "score": 0.7697255611419678
    },
    {
      "name": "Transformer",
      "score": 0.7576894760131836
    },
    {
      "name": "Closed captioning",
      "score": 0.7407742142677307
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5626742839813232
    },
    {
      "name": "Feature learning",
      "score": 0.5608350038528442
    },
    {
      "name": "Segmentation",
      "score": 0.5604990720748901
    },
    {
      "name": "Speech recognition",
      "score": 0.535407543182373
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4676387310028076
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.375685453414917
    },
    {
      "name": "Natural language processing",
      "score": 0.3755130171775818
    },
    {
      "name": "Artificial neural network",
      "score": 0.2162560224533081
    },
    {
      "name": "Image (mathematics)",
      "score": 0.14258360862731934
    },
    {
      "name": "Engineering",
      "score": 0.08306372165679932
    },
    {
      "name": "Linguistics",
      "score": 0.068502277135849
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}