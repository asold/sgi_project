{
    "title": "Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?",
    "url": "https://openalex.org/W3184030040",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5054612798",
            "name": "James A. Michaelov",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A5070598549",
            "name": "Megan D. Bardolph",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A5075447203",
            "name": "Seana Coulson",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A5043344696",
            "name": "Benjamin Bergen",
            "affiliations": [
                "University of California, San Diego"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2119728020",
        "https://openalex.org/W3103816537",
        "https://openalex.org/W2041221140",
        "https://openalex.org/W2418330948",
        "https://openalex.org/W2756894032",
        "https://openalex.org/W1596515083",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W2470606929",
        "https://openalex.org/W2004845106",
        "https://openalex.org/W2141138276",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2997086627",
        "https://openalex.org/W2150104820",
        "https://openalex.org/W2974597461",
        "https://openalex.org/W2888625409",
        "https://openalex.org/W2971754107",
        "https://openalex.org/W2160580906",
        "https://openalex.org/W3027353876",
        "https://openalex.org/W1999686134",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2787685818",
        "https://openalex.org/W2338040292",
        "https://openalex.org/W2092183144",
        "https://openalex.org/W1984050661",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W2101850707",
        "https://openalex.org/W2612205435",
        "https://openalex.org/W3098068882",
        "https://openalex.org/W2000387713",
        "https://openalex.org/W2105837493",
        "https://openalex.org/W2092435433",
        "https://openalex.org/W3134106875"
    ],
    "abstract": "Despite being designed for performance rather than cognitive plausibility, transformer language models have been found to be better at predicting metrics used to assess human language comprehension than language models with other architectures, such as recurrent neural networks. Based on how well they predict the N400, a neural signal associated with processing difficulty, we propose and provide evidence for one possible explanation - their predictions are affected by the preceding context in a way analogous to the effect of semantic facilitation in humans.",
    "full_text": "Different kinds of cognitive plausibility: why are transformers better than RNNs\nat predicting N400 amplitude?\nJames A. Michaelov (j1michae@ucsd.edu)\nMegan D. Bardolph (mbardolph@ucsd.edu)\nSeana Coulson (scoulson@ucsd.edu)\nBenjamin K. Bergen (bkbergen@ucsd.edu)\nDepartment of Cognitive Science, University of California, San Diego\n9500 Gilman Dr, La Jolla, CA 92093, USA\nAbstract\nDespite being designed for performance rather than cognitive\nplausibility, transformer language models have been found to\nbe better at predicting metrics used to assess human language\ncomprehension than language models with other architectures,\nsuch as recurrent neural networks. Based on how well they\npredict the N400, a neural signal associated with processing\ndifﬁculty, we propose and provide evidence for one possible\nexplanation—their predictions are affected by the preceding\ncontext in a way analogous to the effect of semantic facilitation\nin humans.\nKeywords: Language Comprehension; Electroencephalogra-\nphy (EEG); Neural Networks; Machine Learning; Cognitive\nArchitectures\nIntroduction\nNeural language models (NLMs) are valuable tools in under-\nstanding human language comprehension because they learn\nto predict language based on the surface-level statistics of lan-\nguage alone. As such, they are inherently models both of\nwhat can be predicted and what can be learned about lan-\nguage based only on linguistic input. For this reason, they\ncan be used to test hypotheses about how such knowledge\nmay be used in the human language comprehension system\nthat would be impossible to test experimentally.\nSince the early days of their implementation, recurrent neu-\nral network language models (RNN-LMs) have been used to\ninvestigate human cognition (Elman, 1990). However, the re-\ncent development of the transformer network (Vaswani et al.,\n2017) has largely overshadowed them in the ﬁeld of machine\nlearning. Research has shown that language model perfor-\nmance (operationalized as perplexity) tends to correlate with\nhow human-like RNN-LMs are in their processing of lan-\nguage (e.g., Aurnhammer & Frank, 2019). Thus, the fact that\ntransformer language models (T-LMs) perform better than the\npreviously state-of-the-art RNN-LMs despite their vastly dif-\nferent architectures suggests that they may be a viable alter-\nnative model of human language processing.\nIt is therefore unsurprising that in the last year, researchers\ninterested in the cognition of language have compared RNN-\nLMs and T-LMs in terms of how well they capture human\nlinguistic behavior (Merkx & Frank, 2020; Ettinger, 2020;\nMisra, Ettinger, & Rayz, 2020; Wilcox, Gauthier, Hu, Qian,\n& Levy, 2020; Eisape, Zaslavsky, & Levy, 2020). To the best\nof our knowledge, only one (currently unpublished) study has\nattempted to investigate whether RNN-LMs or T-LMs are\nbetter for predicting the N400, a neural response reﬂecting\nsemantic retrieval demands. In their study, Merkx and Frank\n(2020) ﬁnd that the surprisal of T-LMs ﬁt the human N400\ndata better than the surprisal of RNN-LMs. This is somewhat\nunexpected because, as Merkx and Frank (2020) note, intu-\nitively, RNN-LMs more closely match what we believe about\nthe human language comprehension system—they process\nlanguage word-by-word and have a limited ‘working mem-\nory’.\nHere we ask whether the success T-LMs evince in predict-\ning N400 amplitude (Merkx & Frank, 2020) is connected to\nother ﬁndings; for example, they appear to show semantic\npriming effects and, like the N400 (Nieuwland & Kuperberg,\n2008), they are rather insensitive to negation (Ettinger, 2020;\nMisra et al., 2020). To do so, we investigate whether the pre-\ndictions of T-LMs show an analog of semantic priming phe-\nnomena that have been argued to impact N400 amplitude to a\ngreater or lesser extent (e.g., Brouwer, Fitz, & Hoeks, 2012;\nLau, Holcomb, & Kuperberg, 2013). If this is the case, it\nwould suggest that T-LMs may be analogous to the human\nlanguage comprehension system in a different way to RNN-\nLMs, and may offer an insight into the relationship between\nthese phenomena. Such a result would also indicate that we\nneed to update our conception of what makes a computational\nlanguage model more or less cognitively plausible.\nBackground\nThe N400 (Kutas & Hillyard, 1980) is a negative deﬂection\nin the event-related brain potential (ERP), peaking roughly\n400ms after the presentation of a stimulus. It is thought\nto index processing difﬁculty—if the preceding context ac-\ntivates semantic content associated with an upcoming word,\nthe word is easier to process, and thus elicits a reduced am-\nplitude N400. Recent accounts have hypothesized that the\nN400 speciﬁcally indexes the extent to which the upcoming\nword was not expected; a prediction error not affected by the\nstrength of failed predictions (Van Petten & Luka, 2012; Luke\n& Christianson, 2016; DeLong & Kutas, 2020; Kuperberg,\nBrothers, & Wlotko, 2020).\nThe surprisal of an NLM towards a word is a clear con-\nceptual analog of the N400—surprisal is the negative log-\narithm of the probability of an upcoming word given its\ncontext. NLM surprisal signiﬁcantly predicts N400 ampli-\ntude, beating other metrics derived from NLMs (Frank, Ot-\narXiv:2107.09648v1  [cs.CL]  20 Jul 2021\nten, Galli, & Vigliocco, 2015; Aurnhammer & Frank, 2019;\nMerkx & Frank, 2020). Additionally, on more ﬁne-grained\nanalysis, surprisal appears to behave analogously to N400\namplitude—in many cases, experimental manipulations that\nimpact N400 amplitude affect surprisal values in an analo-\ngous fashion (Michaelov & Bergen, 2020).\nHowever, while surprisal is a good model of the extent to\nwhich a word is predicted in the context of a sentence, N400\namplitude is also modulated by other factors (Kutas & Fed-\nermeier, 2011; Kuperberg et al., 2020). One key ﬁnding is\nthat the N400 to a target word is less negative in amplitude\nwhen it follows a semantically related word than an unre-\nlated one (Kutas & Van Petten, 1988; Kutas & Federmeier,\n2011). Additionally, the N400 response to a word is reduced\nif it is semantically related to the most predictable upcom-\ning word—for example, the word monopoly in “Checkmate, ”\nRosaline announced with glee. She was getting to be really\ngood at monopolyelicits a less negative N400 than the word\nfootball by virtue of being more semantically related to the\nbest completion, chess (Federmeier & Kutas, 1999). The\nN400 response to a word is also reduced if it is semantically\nrelated to the previous words in the utterance—for example,\nthere is no difference in N400 response between the wordeat\nin for breakfast the boys would only eat...and for breakfast\nthe eggs would only eat..., despite its semantic implausibility\nin the second clause (Kuperberg, Sitnikova, Caplan, & Hol-\ncomb, 2003).\nThese effects have led some researchers to argue that the\nN400 can be explained by spreading activation, where words\nin the preceding context partially activate semantically-\nrelated upcoming words, regardless of whether they are ap-\npropriate completions to the sentence (Brouwer et al., 2012).\nThis ‘bag-of-words’ approach to semantic pre-activation\n(Kuperberg, 2016) has also been reﬂected in computational\nmodeling of the N400. Several researchers have used the\ncosine distance between the word embeddings of the target\nword and those for the preceding words to reﬂect the kind\nof semantic similarity that may lead to facilitated processing\n(Parviz, Johnson, Johnson, & Brock, 2011; Ettinger, Feld-\nman, Resnik, & Phillips, 2016; Frank & Willems, 2017). This\napproach is also used to control for confounding effects of\nsemantic similarity when explicitly investigating prediction\n(Kuperberg et al., 2020).\nThe present study has two main aims. First, to directly\ncompare and quantify how well semantic facilitation (as op-\nerationalized by cosine distance between embeddings) and\nprediction (as operationalized by surprisal) predict N400 am-\nplitude. Second, to identify the extent to which the two are\ncorrelated and how this varies by language model architec-\nture, with the hope that this will inform why T-LMs have\nbeen found to predict N400 amplitude better than RNN-LMs\ndespite their apparent cognitive implausibility.\nExperiment 1: RNN-LM vs. T-LM surprisal\nModeling approach and details\nThis experiment follows the same general approach as pre-\nvious research investigating surprisal as a predictor of N400\namplitude (Frank et al., 2015; Aurnhammer & Frank, 2019;\nMerkx & Frank, 2020), namely, comparing recorded ERP\ndata to NLM surprisal for the same set of stimuli. We use\nstimuli from an ERP study whose data have been previously\npresented (Bardolph, Van Petten, & Coulson, 2018). These\nstimuli were run through two NLMs, one RNN-LM and one\nT-LM, and the predicted probability of target words was col-\nlected. This predicted probability was then transformed into\nsurprisal, where the surprisal S of a word wi is the nega-\ntive logarithm of its probability given its preceding context\nw1...wi−1, as shown in (1).\nS(wi) =−logP(wi|w1...wi−1) (1)\nThis surprisal was then used as a predictor in a linear\nmixed-effects model to predict by-trial, by-electrode ampli-\ntude from the original ERP study.\nTwo NLMs were used. The RNN-LM was the BIG\nLSTM+CNN I NPUTS model (Jozefowicz, Vinyals, Schus-\nter, Shazeer, & Wu, 2016), henceforth the JRNN. The T-LM\nused was GPT-2 (Radford et al., 2019). Both models are very\nlarge—the JRNN has roughly 1 billion parameters, while\nGPT-2 has roughly 1.5 billion. One area in which the two dif-\nfer is that the JRNN has a vocabulary size of roughly 800,000,\nwhile GPT-2 has a vocabulary size of roughly 50,000. Addi-\ntionally, while the JRNN was trained on approximately one\nbillion words, GPT-2 was trained on a dataset an order of\nmagnitude larger.\nOriginal ERP Study\nThe original study (Bardolph et al., 2018) used stimuli\nadapted from previous work (Thornhill & Van Petten, 2012).\nThere were 290 sentence frames with target words in one of\nfour conditions, for a total of 1160 sentences.\nThe conditions were the following. The B EST COMPLE -\nTION was the completion with the highest cloze probability\n(Taylor, 1957; cloze = 0 .458 ±0.261). The cloze probabil-\nity of a word is the proportion of participants in a norming\nstudy that ﬁlled in a gap in the sentence with that word. R E-\nLATED completions were low-cloze (0 .043 ±0.058) words\nthat are semantically related to the best completion. U NRE -\nLATED completions are low-cloze (0.024±0.037) words that\nare semantically unrelated to the best completion. I MPLAU -\nSIBLE completions are completions that were semantically\nimplausible with a cloze of zero. The conditions can be il-\nlustrated with the following example: It’s hard to admit when\none iswrong (BEST COMPLETION ) / incorrect (RELATED to\nbest completion) / lonely (UNRELATED to best completion) /\nscreened (IMPLAUSIBLE ).\nAs expected from previous research, Bardolph et al. (2018)\nfound that the B EST COMPLETION elicited the lowest-\n0\n500\n1000\n1500\n2000\nPredictor + ROI Predictor x ROI\nFixed effects\nFit improvement (baseline AIC − AIC)\nPredictor\nCloze\nJRNN Surprisal\nGPT−2 Surprisal\nFigure 1: The improvement in AIC for each model compared\nto the baseline model with only ROI as a main effect. The\n‘Predictor + ROI’ models have main effects of the predic-\ntor and ROI, while the ‘Predictor x ROI’ model also includes\ntheir interaction.\namplitude N400, followed by R ELATED , U NRELATED , and\nIMPLAUSIBLE completions, in order of increasing amplitude.\nIn the study, 44 healthy adult experimental participants\nread sentences in English one word at a time. EEG was\nrecorded from 29 scalp sites. In the present study, the mean\namplitude at each site over the 300-500ms time period (the\ncanonical N400 time-frame) was calculated for each elec-\ntrode in each trial. These mean amplitude measurements thus\nserved as the outcome measures in the regression models de-\nscribed below.\nResults\nLinear mixed-effects models were used to predict N400 am-\nplitude. All models included region of interest (ROI; Pre-\nfrontal, Fronto-central, Central, Posterior, Left Temporal,\nRight Temporal) as a ﬁxed effect and Subject, Sentence\nFrame, and Electrode as random intercepts (more complex\nrandom effects structures led to models that did not converge\nor had singular ﬁts).\nWe evaluated the statistical signiﬁcance of each predictor\nusing likelihood ratio tests between nested models. All re-\nported p-values are corrected for multiple comparisons based\non the false discovery rate (Benjamini & Yekutieli, 2001; R\nCore Team, 2020). Adding the ﬁxed effect of surprisal to\na null model (a linear mixed-effects model with only ROI\nas a ﬁxed effect and the aforementioned random intercepts)\nsigniﬁcantly improved model ﬁt (JRNN: χ2(1) =1483, p <\n0.0001; GPT-2: χ2(1) =1752.5, p < 0.0001). Further adding\nthe interaction of surprisal and ROI also signiﬁcantly im-\nproved the model ﬁt (JRNN: χ2(5) = 467.82, p < 0.0001;\nGPT-2: χ2(5) =628.84, p < 0.0001).\nA comparison of AICs (Figure 1) shows that for equivalent\nmodels, GPT-2 surprisal ﬁts N400 amplitude more closely.\nFor comparison, we also include the AIC values for equiva-\nlent linear mixed-effects models with cloze probability as the\nmain predictor. While it may be unsurprising that cloze is a\nworse predictor of N400 amplitude than surprisal in this study\ndue to the fact that the RELATED and UNRELATED conditions\nwere matched for cloze, as far as we are aware, this is the ﬁrst\ntime that a corpus-derived metric has out-performed cloze as\na predictor of human processing difﬁculty (see Brothers &\nKuperberg, 2021, for discussion). To investigate the differ-\nence in AICs further, we compared the predictions of the best\nmodels (i.e. those including the Predictor x ROI interaction)\nto the real N400 in a held-out dataset of roughly 15% of the\ntotal data (46,280 measurements).\nFigure 2 shows both the true and predicted amplitudes\nin the 300-500ms time window for electrodes in the Cen-\ntral and Posterior ROIs (the canonical N400 ROIs). As\ncan be seen, the N400 amplitudes predicted by GPT-2 sur-\nprisal are closer to the true amplitudes than those pre-\ndicted by JRNN surprisal in 3 out of 4 of the condi-\ntions: B EST COMPLETION (One-tailed t-test testing whether\nN400 amplitude predicted by the GPT-2 surprisal model <\nN400 amplitude predicted by the JRNN surprisal model:\nt(10198) =5.9324, p < 0.0001), RELATED (GPT-2< JRNN:\nt(9105.3) =3.3548, p = 0.0019), and IMPLAUSIBLE (GPT-2\n> JRNN: t(9363.7) =−7.4523, p < 0.0001). While there is\na difference in means in the expected direction for U NRE -\nLATED completions, the difference is not signiﬁcant (GPT-2\n> JRNN: t(9695.5) =−1.2441, p = 0.4584).\nWe further tested whether the models successfully pre-\ndicted the differences between conditions by running one-\ntailed t-tests between the predicted amplitudes for condi-\ntions closest in mean predicted amplitude (using the val-\nues for the Central and Posterior ROIs, as shown in Fig-\nure 2). GPT-2 surprisal successfully predicts that R ELATED\nwords elicit a higher-amplitude N400 than B EST COM-\nPLETION s ( t(9601.2) = 18.466, p < 0.0001), that U NRE -\nLATED words elicit a higher-amplitude N400 than R ELATED\nwords ( t(9393.7) = 5.8936, p < 0.0001), and I MPLAUSI -\nBLE words elicit a higher-amplitude N400 than UNRELATED\nwords (t(9536.7) =47.936, p < 0.0001). On the other hand,\nwhile JRNN surprisal successfully predicts that R ELATED\nwords will elicit a higher-amplitude N400 than B EST COM-\nPLETIONS (t(9593.2) = 15.871, p < 0.0001), and that I M-\nPLAUSIBLE words elicit a higher-amplitude N400 than U N-\nRELATED words ( t(9507.8) = 40.43, p < 0.0001), it does\nnot predict that U NRELATED completions elicit a higher-\namplitude N400 than R ELATED completions ( t(9392.6) =\n1.2691, p = 0.4584), which was observed in the ERP data\nand was successfully predicted by GPT-2 surprisal. In fact,\nwhile its predictions are closer, in terms of predicting signiﬁ-\ncant differences between conditions, JRNN surprisal does no\nbetter than cloze (Predicted B EST COMPLETION N400 am-\nplitude < predicted RELATED N400 amplitude: t(9651.3) =\n38.472, p < 0.0001; RELATED < UNRELATED : t(9384.9) =\n0.5747, p = 1; U NRELATED < IMPLAUSIBLE : t(9528.4) =\n3.7758, p = 0.0004).\n−1\n0\n1\nBest Completion Related Unrelated Implausible\nCondition\nAmplitude (microvolts)\nMetric\nPredicted (Cloze)\nPredicted (JRNN Surprisal)\nPredicted (GPT−2 Surprisal)\nTrue N400\nFigure 2: True and predicted amplitudes in the 300-500ms\ntimeframe for electrodes in the Central and Posterior ROIs.\nError bars indicate standard error. A more negative amplitude\nrepresents a higher-amplitude N400. It should be noted that\nthe y-axis is reversed, following convention.\nDiscussion\nThere are several key results from this study. First, sur-\nprisal and its interaction with scalp ROI are signiﬁcant pre-\ndictors of N400 amplitude, replicating previous results (Frank\net al., 2015; Aurnhammer & Frank, 2019; Merkx & Frank,\n2020). Second, we replicate the ﬁnding that the surprisal of\nT-LMs better predicts N400 amplitude than that of RNN-LMs\n(Merkx & Frank, 2020). Finally, we ﬁnd that while GPT-\n2 surprisal successfully predicts differences in N400 due to\nexperimental manipulation—with B EST COMPLETION elic-\niting the most reduced N400, and R ELATED , U NRELATED ,\nand I MPLAUSIBLE completions each eliciting increasingly\nmore negative (less positive) N400s—JRNN surprisal fails\nto distinguish between R ELATED and U NRELATED comple-\ntions. This difﬁculty for the JRNN to predict the difference\nbetween the two is consistent with the ﬁndings of Michaelov\nand Bergen (2020).\nWhy does GPT-2 surprisal ﬁt N400 amplitude better in\nboth models? It is likely that there are multiple reasons for\nthis, some of which may involve the simple fact of larger size\nand larger training data. It is also important to note that it is\npossible that the better predictions of GPT-2 surprisal are due\nto different factors across the experimental conditions.\nHowever, one architecture-related possibility is suggested\nby our replication of the ﬁnding that JRNN surprisal can\nstruggle to predict the difference between the R ELATED and\nUNELATED conditions for some sets of stimuli (Michaelov &\nBergen, 2020) and our novel ﬁnding that this is not the case\nwith GPT-2 surprisal. As can be seen in Figure 2, the inability\nof JRNN surprisal to predict the difference between the two\nseems to be mostly driven by overestimating the N400 am-\nplitude for the RELATED condition. This is something which\nis improved upon by using GPT-2 surprisal for prediction. It\nis important to note that as explained previously, the effect of\nsemantic relatedness on N400 amplitude has been previously\nhypothesized to involve spreading activation or some other\nform of shallow semantic facilitation.\nThis is crucial because as Merkx and Frank (2020) note,\none of the key differences between T-LMs and RNN-LMs is\nthat T-LMs do not have the same memory bottleneck as RNN-\nLMs—they have direct access to all previous words in the\nsequence. RNN-LMs, on the other hand, only have one cur-\nrent state, which is adjusted with each new input. Thus, this\nincreased memory capacity—the capacity to ‘remember’ ex-\nactly which words precede the current word—means that it is\npossible for the network to use speciﬁc previous words in pre-\ndicting the next word, and that these could independently (or\nin a bag-of-words fashion) semantically facilitate predictions.\nThis may also surface as susceptibility to priming—previous\nwork has found that T-LMs are likely to repeat words that that\nthey have already seen, and their predictions can be seman-\ntically primed by presenting them with an individual prime\nword (Misra et al., 2020).\nExperiment 2: Quantifying semantic\nfacilitation\nIn Experiment 2, we investigate whether the surprisal of GPT-\n2 incorporates something roughly analogous to spreading ac-\ntivation. That is, the ﬁnding that words are at least partly\npredicted because they have been activated by the semantics\nof previous words in the sequence. We do so by comparing\nthe extent to which surprisal in each of the two models cor-\nrelates with estimates of semantic similarity from each of the\ntwo models. A higher correlation would indicate the model is\nmore biased towards predicting words that are semantically\nrelated to preceding words in the utterance, i.e., that it ex-\nhibits behavior akin to that commonly attributed to semantic\nspreading activation in humans.\nMethod\nThe high-level process for calculating semantic similarity in\nthe NLMs was similar to that used for calculating surprisal.\nStimuli from the same experiment (Bardolph et al., 2018)\nwere run through the NLMs, and the activation states of the\nmodel were recorded. For this study, however, we used the\ncontext-free word embeddings for each of the two NLMs\n(JRNN and GPT-2). As in previous work, we calculated the\nmean embeddings of all words preceding the target word,\nand calculated the cosine distance between this and the target\nword embedding. We then compared the extent to which co-\nsine distance and surprisal were correlated for each network.\nResults and Discussion\nAs can be seen in Figure 3, cosine similarity and surprisal are\nsubstantially more correlated for GPT-2 ( r = −0.480) than\nfor the JRNN (r = −0.200), supporting the idea that semantic\nFigure 3: Cosine similarity and surprisal in the JRNN and\nGPT-2. r is Pearson’s correlation coefﬁcient.\nsimilarity is more directly correlated with surprisal in GPT-2\nthan the JRNN.\nTo the best of our knowledge the approach here is novel.\nMoreover, it is valuable because it is self-contained. We uti-\nlize each model’s own semantic representations to evaluate\nthe degree of semantic spreading activation. Since our own\nsemantic representations are the only ones we have access to\nduring language comprehension, the approach can be seen as\ncognitively plausible. Further, it can easily be applied to any\nlanguage model and any data set, without necessarily requir-\ning carefully constructed stimuli.\nExperiment 3: Testing the implicit semantic\nfacilitation account\nExperiment 1 showed that GPT-2 surprisal is a better predic-\ntor of N400 amplitude than JRNN surprisal is. Experiment\n2 showed that GPT-2 surprisal is more highly correlated with\nsemantic similarity than JRNN surprisal is. In Experiment 3,\nwe directly test whether the latter ﬁnding explains the former,\nthat is, whether the fact that GPT-2 surprisal is more corre-\nlated with semantic similarity (as operationalized by cosine\nsimilarity) leads to its better prediction of N400 amplitude.\nMethod\nTo do this, we investigated the extent to which each NLM’s\nsurprisal and cosine similarity metrics explain different pro-\nportions of the variance in N400 amplitude. If it is indeed the\ncase that GPT-2 surprisal predicts N400 amplitude better than\nJRNN surprisal because its surprisal is more correlated with\ncosine similarity, then we should expect that adding cosine\nsimilarity as a predictor to a linear mixed-effects model with\nGPT-2 surprisal as a predictor should lead to less improve-\nment than adding cosine similarity to the equivalent JRNN\nsurprisal model.\nResults\nWe tested this hypothesis by running likelihood ratio tests\ncomparing the previous best models (including surprisal,\nROI, and their interaction as ﬁxed effects) with models that\nalso included cosine similarity as a predictor and larger mod-\nels that included both cosine similarity and its interaction\nwith ROI as predictors. We found that each of these sig-\nniﬁcantly improved the JRNN model ﬁt (cosine similarity:\nχ2(1) =92.782, p < 0.0001; cosine similarity x ROI:χ2(6) =\n93.594, p < 0.0001), but neither improved GPT-2 model ﬁt\n(cosine similarity: χ2(1) =0.1524, p = 1; cosine similarity x\nROI: χ2(6) =5.7346, p = 1).\nDiscussion\nThe results show that cosine similarity explains additional\nvariance in N400 amplitude beyond what is explained by\nJRNN surprisal. However, this is not the case with GPT-2\nsurprisal. The results, therefore, provide evidence that it is\nnot only the case that GPT-2 surprisal is a better predictor of\nN400 amplitude and more correlated with semantic similarity\nthan JRNN surprisal, but that the two are related. Speciﬁcally,\nthey provide evidence for the hypothesis that it is the fact that\nGPT-2 surprisal correlates better with semantic similarity that\nmakes it better at predicting N400 amplitude.\nGeneral Discussion\nOur results replicate and build upon previous work. As in\nprevious work, we ﬁnd that the surprisal of both RNN-LMs\nand T-LMs signiﬁcantly predicts N400 amplitude, that T-LM\nsurprisal is a better predictor than RNN-LM surprisal, and\nthat JRNN surprisal struggles to predict the difference be-\ntween words that are semantically related and unrelated to\nthe highest-cloze completion (Frank et al., 2015; Aurnham-\nmer & Frank, 2019; Michaelov & Bergen, 2020; Merkx &\nFrank, 2020).\nOur ﬁrst novel ﬁnding is that GPT-2 surprisal can better\npredict the amplitude of N400s elicited by words that are se-\nmantically related to the highest-cloze completions than can\nJRNN surprisal. This allows GPT-2 surprisal to successfully\ndistinguish between low-cloze words that are semantically re-\nlated and unrelated to the best completion where JRNN sur-\nprisal struggles.\nOur second and more important ﬁnding is that GPT-2 sur-\nprisal is more (inversely) correlated with GPT-2 semantic\nsimilarity than JRNN surprisal is with JRNN semantic sim-\nilarity. We hypothesize that this is due to the difference in\narchitecture—with access to the previous words in a given\ninput, T-LMs are able to predict the next word based on any\nof these words, while RNN-LMs are limited to predict based\non their single recurrent state, which may be multi-layered,\nbut nonetheless does not store individual previous words ex-\nplicitly. This is in concord with recent ﬁndings showing\nthat BERT, another T-LM, is susceptible to priming (Ettinger,\n2020; Misra et al., 2020). This suggests that ‘bag-of-words’\nsemantic spreading activation, while not the whole story be-\nhind the neurocognitive system or systems underlying the\nN400 response, may still play a part in it, and thus, in the\nwhole language comprehension system.\nAs a computational modeling study, the results of our ex-\nperiments do not directly demonstrate a speciﬁc way in which\nlanguage comprehension is implemented in the brain. How-\never, they do demonstrate that it is not necessary to posit\nseparate systems to explain the fact that N400 amplitude\nis affected both by how predictable an upcoming word is\nand the prior occurrence of semantically or associatively re-\nlated words in the context. While this is been previously\nshown with other (more elaborate) modeling approaches (e.g.\nRabovsky, Hansen, & McClelland, 2018), we show directly\nthat lexical prediction could in principle implicitly incorpo-\nrate semantic relatedness or similarity.\nIt should be noted, however, that we do not provide evi-\ndence that the the same system must underlie both kinds of\nN400 response. This is still an open research question. The\none fMRI-based study on the topic, for example, suggests that\nthe two may occur in distinct areas (Frank & Willems, 2017).\nBy contrast, the ERP-based work on the topic suggests that\nwhile there are differences between the effects of semantic\nfacilitation and prediction that might be used to dissociate\nthe two, the N400 response to each exhibit a very similar\ntime course and topography (Kutas, 1993; Van Petten, 1993;\nLau et al., 2013; Broderick, Anderson, Di Liberto, Crosse, &\nLalor, 2018).\nAnother interesting question raised by our results is how to\ndetermine the cognitive plausibility of an NLM. As noted by\nMerkx and Frank (2020), intuitively, the RNN-LM architec-\nture appears more cognitively plausible as a model of lan-\nguage comprehension than the T-LM. This is due to what\nKeller (2010) refers to as the distance-based memory cost\nof plausible language models—they have limited ‘working\nmemory’, and like humans, struggle with long-distance lin-\nguistic phenomena (e.g. long-range dependencies). This is\nsomething inherent in the architecture of RNN-LMs, even\nthose with features such as long short-term memory (LSTM)\nthat help them remember and forget necessary input. Trans-\nformers, on the other hand, have perfect memory for their\nentire context window (1024 tokens in GPT-2).\nHowever, as discussed, some aspects of language compre-\nhension may involve facilitation based on semantic similar-\nity, and as we demonstrate in Experiment 2, this is a feature\nthat appears to be more present in GPT-2, and combined with\nthe ﬁndings of Ettinger and colleagues (Ettinger, 2020; Misra\net al., 2020), the present study suggests that this may be a\nwidespread feature of T-LMs in general. Therefore, it ap-\npears that some aspects of human language comprehension,\nspeciﬁcally, those associated with language processing with\nlimited working memory, may be better modeled with RNN-\nLM surprisal; while those that involve more shallow seman-\ntic facilitation may be better modeled with T-LMs surprisal.\nThis may help to explain why there are conﬂicting results re-\ngarding which is better for modeling reading time (Merkx &\nFrank, 2020; Wilcox et al., 2020; Eisape et al., 2020).\nUnderstanding the differences between the two model ar-\nchitectures and how these relate to different aspects of human\nlanguage comprehension may thus not only help us improve\nour language models, but also offer insight into the neurocog-\nnitive systems involved in language.\nAcknowledgments\nThis research was partially funded by a 2020-2021 CARTA\n(Center for Academic Research and Training in Anthro-\npogeny) Fellowship awarded to James Michaelov.\nReferences\nAurnhammer, C., & Frank, S. L. (2019). Evaluating\ninformation-theoretic measures of word prediction in natu-\nralistic sentence reading. Neuropsychologia, 134, 107198.\nBardolph, M., Van Petten, C., & Coulson, S. (2018). Single\nTrial EEG Data Reveals Sensitivity to Conceptual Expec-\ntations (N400) and Integrative Demands (LPC). In Twelfth\nAnnual Meeting of the Society for the Neurobiology of Lan-\nguage. Quebec City, Canada.\nBenjamini, Y ., & Yekutieli, D. (2001). The control of the\nfalse discovery rate in multiple testing under dependency.\nAnnals of statistics, 1165–1188.\nBroderick, M. P., Anderson, A. J., Di Liberto, G. M., Crosse,\nM. J., & Lalor, E. C. (2018). Electrophysiological Corre-\nlates of Semantic Dissimilarity Reﬂect the Comprehension\nof Natural, Narrative Speech. Current Biology, 28(5), 803-\n809.e3.\nBrothers, T., & Kuperberg, G. R. (2021, February). Word\npredictability effects are linear, not logarithmic: Implica-\ntions for probabilistic models of sentence comprehension.\nJournal of Memory and Language, 116, 104174. doi:\n10.1016/j.jml.2020.104174\nBrouwer, H., Fitz, H., & Hoeks, J. (2012). Getting real about\nSemantic Illusions: Rethinking the functional role of the\nP600 in language comprehension. Brain Research, 1446,\n127–143.\nDeLong, K. A., & Kutas, M. (2020). Comprehending sur-\nprising sentences: Sensitivity of post-N400 positivities to\ncontextual congruity and semantic relatedness. Language,\nCognition and Neuroscience, 0(0), 1–20.\nEisape, T., Zaslavsky, N., & Levy, R. (2020). Cloze Dis-\ntillation Improves Psychometric Predictive Power. In Pro-\nceedings of the 24th Conference on Computational Natural\nLanguage Learning (pp. 609–619). Online: Association\nfor Computational Linguistics.\nElman, J. L. (1990). Finding Structure in Time. Cognitive\nScience, 14(2), 179–211.\nEttinger, A. (2020). What BERT Is Not: Lessons from a\nNew Suite of Psycholinguistic Diagnostics for Language\nModels. Transactions of the Association for Computational\nLinguistics, 8, 34–48.\nEttinger, A., Feldman, N., Resnik, P., & Phillips, C. (2016).\nModeling N400 amplitude using vector space models of\nword representation. In Proceedings of the 38th Annual\nConference of the Cognitive Science Society.Philadelphia,\nUSA.\nFedermeier, K. D., & Kutas, M. (1999). A Rose by Any\nOther Name: Long-Term Memory Structure and Sentence\nProcessing. Journal of Memory and Language, 41(4), 469–\n495.\nFrank, S. L., Otten, L. J., Galli, G., & Vigliocco, G. (2015).\nThe ERP response to the amount of information conveyed\nby words in sentences. Brain and Language, 140, 1–11.\nFrank, S. L., & Willems, R. M. (2017). Word predictabil-\nity and semantic similarity show distinct patterns of brain\nactivity during language comprehension. Language, Cog-\nnition and Neuroscience, 32(9), 1192–1203.\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., & Wu,\nY . (2016). Exploring the Limits of Language Modeling.\narXiv:1602.02410 [cs].\nKeller, F. (2010, July). Cognitively Plausible Models of Hu-\nman Language Processing. InProceedings of the ACL 2010\nConference Short Papers(pp. 60–67). Uppsala, Sweden:\nAssociation for Computational Linguistics.\nKuperberg, G. R. (2016). Separate streams or probabilistic\ninference? What the N400 can tell us about the compre-\nhension of events. Language, Cognition and Neuroscience,\n31(5), 602–616.\nKuperberg, G. R., Brothers, T., & Wlotko, E. W. (2020). A\nTale of Two Positivities and the N400: Distinct Neural Sig-\nnatures Are Evoked by Conﬁrmed and Violated Predictions\nat Different Levels of Representation. Journal of Cognitive\nNeuroscience, 32(1), 12–35.\nKuperberg, G. R., Sitnikova, T., Caplan, D., & Holcomb,\nP. J. (2003, June). Electrophysiological distinctions\nin processing conceptual relationships within simple sen-\ntences. Cognitive Brain Research, 17(1), 117–129. doi:\n10.1016/S0926-6410(03)00086-7\nKutas, M. (1993). In the company of other words: Electro-\nphysiological evidence for single-word and sentence con-\ntext effects. Language and Cognitive Processes, 8(4), 533–\n572.\nKutas, M., & Federmeier, K. D. (2011). Thirty Years and\nCounting: Finding Meaning in the N400 Component of the\nEvent-Related Brain Potential (ERP). Annual Review of\nPsychology, 62(1), 621–647.\nKutas, M., & Hillyard, S. A. (1980). Reading senseless sen-\ntences: Brain potentials reﬂect semantic incongruity. Sci-\nence, 207(4427), 203–205.\nKutas, M., & Van Petten, C. (1988). Event-related brain po-\ntential studies of language. Advances in psychophysiology,\n3, 139–187.\nLau, E. F., Holcomb, P. J., & Kuperberg, G. R. (2013). Dis-\nsociating N400 Effects of Prediction from Association in\nSingle-word Contexts. Journal of Cognitive Neuroscience,\n25(3), 484–502.\nLuke, S. G., & Christianson, K. (2016). Limits on lexical\nprediction during reading. Cognitive Psychology, 88, 22–\n60.\nMerkx, D., & Frank, S. L. (2020). Comparing Transformers\nand RNNs on predicting human sentence processing data.\narXiv:2005.09471 [cs].\nMichaelov, J. A., & Bergen, B. K. (2020, November). How\nwell does surprisal explain N400 amplitude under differ-\nent experimental conditions? In Proceedings of the 24th\nConference on Computational Natural Language Learn-\ning (CoNLL 2020).Online: Association for Computational\nLinguistics.\nMisra, K., Ettinger, A., & Rayz, J. (2020). Exploring\nBERT’s Sensitivity to Lexical Cues using Tests from Se-\nmantic Priming. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020(pp. 4625–4635). On-\nline: Association for Computational Linguistics.\nNieuwland, M. S., & Kuperberg, G. R. (2008). When the\ntruth is not too hard to handle: An event-related potential\nstudy on the pragmatics of negation. Psychological Sci-\nence, 19(12), 1213–1218.\nParviz, M., Johnson, M., Johnson, B., & Brock, J. (2011,\nDecember). Using Language Models and Latent Seman-\ntic Analysis to Characterise the N400m Neural Response.\nIn Proceedings of the Australasian Language Technology\nAssociation Workshop 2011(pp. 38–46). Canberra, Aus-\ntralia.\nR Core Team. (2020). R: A language and environment for\nstatistical computing [Computer software manual]. Vienna,\nAustria. Retrieved from https://www.R-project.org/\nRabovsky, M., Hansen, S. S., & McClelland, J. L. (2018,\nSeptember). Modelling the N400 brain potential as change\nin a probabilistic representation of meaning. Nature Hu-\nman Behaviour, 2(9), 693–705. doi: 10.1038/s41562-018-\n0406-4\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., &\nSutskever, I. (2019). Language Models are Unsupervised\nMultitask Learners. , 24.\nTaylor, W. L. (1957). “Cloze” readability scores as indices\nof individual differences in comprehension and aptitude.\nJournal of Applied Psychology, 41(1), 19–26.\nThornhill, D. E., & Van Petten, C. (2012). Lexical versus con-\nceptual anticipation during sentence processing: Frontal\npositivity and N400 ERP components. International Jour-\nnal of Psychophysiology, 83(3), 382–392.\nVan Petten, C. (1993). A comparison of lexical and sentence-\nlevel context effects in event-related potentials. Language\nand Cognitive Processes, 8(4), 485–531.\nVan Petten, C., & Luka, B. J. (2012, February). Prediction\nduring language comprehension: Beneﬁts, costs, and ERP\ncomponents. International Journal of Psychophysiology,\n83(2), 176–190. doi: 10.1016/j.ijpsycho.2011.09.015\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., . . . Polosukhin, I. (2017). Attention is\nall you need. In I. Guyon et al. (Eds.), Advances in neural\ninformation processing systems(V ol. 30, pp. 5998–6008).\nCurran Associates, Inc.\nWilcox, E. G., Gauthier, J., Hu, J., Qian, P., & Levy, R. P.\n(2020). On the Predictive Power of Neural Language Mod-\nels for Human Real-Time Comprehension Behavior. In\nProceedings of the 42nd Annual Meeting of the Cognitive\nScience Society (CogSci 2020)(p. 7)."
}