{
  "title": "Flow-based network intrusion detection based on BERT masked language model",
  "url": "https://openalex.org/W4310511492",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5039327444",
      "name": "Loc Gia Nguyen",
      "affiliations": [
        "Nagaoka University",
        "Nagaoka University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5020753132",
      "name": "Kohei Watabe",
      "affiliations": [
        "Nagaoka University",
        "Nagaoka University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093410479",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034863102",
    "https://openalex.org/W3163315201"
  ],
  "abstract": "A Network Intrusion Detection System (NIDS) is an important tool that\\nidentifies potential threats to a network. Recently, different flow-based NIDS\\ndesigns utilizing Machine Learning (ML) algorithms have been proposed as\\npotential solutions to detect intrusions efficiently. However, conventional\\nML-based classifiers have not seen widespread adoption in the real-world due to\\ntheir poor domain adaptation capability. In this research, our goal is to\\nexplore the possibility of improve the domain adaptation capability of NIDS.\\nOur proposal employs Natural Language Processing (NLP) techniques and\\nBidirectional Encoder Representations from Transformers (BERT) framework. The\\nproposed method achieved positive results when tested on data from different\\ndomains.\\n",
  "full_text": "Flow-based Network Intrusion Detection Based on BERT Masked\nLanguage Model\nLoc Gia Nguyen\nNagaoka University of Technology\nNagaoka, Niigata, Japan\ns203145@stn.nagaokaut.ac.jp\nKohei Watabe\nNagaoka University of Technology\nNagaoka, Niigata, Japan\nk_watabe@vos.nagaokaut.ac.jp\nABSTRACT\nA Network Intrusion Detection System (NIDS) is an important\ntool that identifies potential threats to a network. Recently, dif-\nferent flow-based NIDS designs utilizing Machine Learning (ML)\nalgorithms have been proposed as potential solutions to detect in-\ntrusions efficiently. However, conventional ML-based classifiers\nhave not seen widespread adoption in the real-world due to their\npoor domain adaptation capability. In this research, our goal is to\nexplore the possibility of improve the domain adaptation capabil-\nity of NIDS. Our proposal employs Natural Language Processing\n(NLP) techniques and Bidirectional Encoder Representations from\nTransformers (BERT) framework. The proposed method achieved\npositive results when tested on data from different domains.\nCCS CONCEPTS\nâ€¢ Security and privacy â†’Intrusion detection systems .\nKEYWORDS\nIntrusion Detection System, Domain adaptation, BERT\nACM Reference Format:\nLoc Gia Nguyen and Kohei Watabe. 2022. Flow-based Network Intrusion\nDetection Based on BERT Masked Language Model. In CoNEXT Student\nWorkshop 2022 (CoNEXT-SW â€™22), December 9, 2022, Roma, Italy.ACM, New\nYork, NY, USA, 2 pages. https://doi.org/10.1145/3565477.3569152\n1 INTRODUCTION\nIt is common in practical application of NIDS for there to be a\nchange in the data distribution between its training data and the\ndata it encounters when deployed. Conventional ML algorithms\noften adapt poorly to such change, which limit their usefulness\nin real-world scenarios [ 1]. To address this, Energy-based Flow\nClassifier (EFC) [3] was proposed as a solution. Despite having good\nadaptability, EFC produces high false positives rate for domains\nwhere the distribution of features of malicious flows overlap with\nthat of benign flows. We theorize that the reason for the limitations\nof conventional ML algorithms and EFC is the use of singular flows\nas input data, as the classifier can only model the distribution of\nfeatures within a flow. This limitation can be overcome with the\nuse of sequences of flows, allowing the classifier to further models\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCoNEXT-SW â€™22, December 9, 2022, Roma, Italy\nÂ© 2022 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-9937-1/22/12.\nhttps://doi.org/10.1145/3565477.3569152\n... ...\nLinear\nlayer\n768 to 2 \nbenign 0.99\nmalicious 0.01 \n...\nbenign\n...\npredictions\nprobabilities\noutput\npredictions\nsequence\nof  flows\n[12,\n43,\n23,\n54,85,  \n23]\n[0.1, -0.2, ...]\n[-0.0, -0.2, ...]\n[0.3, 0.9, ...]\n[0.7, -0.2, ...][0.8, -0.3, ...]\n[0.2, 0.1, ...]\nSrc Pt: 445\nDst Pt: 48888 \nProto+Flags: TCP.AP... \n# Packets: 1 # Bytes: 108\nDuration: 0.000 \n6 features\n[0.3, 0.4, ...]\n[0.7, -0.3, ...]\n[-0.9, 0.7, ...]\n[0.5, -0.8, ...][-0.3, -0.3, ...]\n[0.4, 0.6, ...]\n[0.99,\n0.01] benign\n...\nBERT\n768-dim vector\n(128 x 6) \n768-dim vector\n(128 x 6) 2-dim vector\n6-dim vector\nFigure 1: Proposed system architecture\nthe distribution of a flow in relation to other flows. To utilize the\ncontext information from a sequence of flow, we use the BERT\nframework, which is able to process inputs in relation to all the\nother inputs in a sequence.\nBERT[2] is a transformer-based machine learning technique for\nNLP developed by Google. The BERT framework is comprised of\ntwo steps: pre-training and fine-tuning. In pre-training, the BERT\nmodel is trained on unlabeled data. For fine-tuning, the model is\nfirst initialized using the pre-trained parameters, and then trained\nusing labeled data from the downstream tasks. BERT is pre-trained\nwith two unsupervised tasks, which are Masked Language Model-\ning (MLM) and Next Sentence Prediction (NSP). In MLM, some of\nthe words in a sentence are replaced with a different token. The\nobjective is to predict the original value of the masked words based\non other unmasked words in the sentence. In NSP, BERT takes\nsentence pairs as input. The objective is to predict whether the\nsecond sentence in the pair is the next sentence in the document.\nFor fine-tuning, task-specific inputs and outputs are added to a\npre-trained BERT model.\nOur research employs CIDDS-001[ 4] and CIDDS-002[ 5] data\nsets that contains flow samples from a small business environment\nemulated using OpenStack. CIDDS-001 also contains real traffic\nflow samples captured from an external server directly deployed\non the internet.\n2 PROPOSAL\nWe first organize network traffic flows into structures similar to a\nlanguage, treating a flow as a word and a sequence of flows as a\nsentence. In this study, the BERT model is pre-trained with only\nthe MLM task. For fine-tuning, a linear layer with softmax output\nis used. It is important to preserve the distribution of flows within a\nsequence; therefore, during training, the data set is not shuffled. A\ntraining sample is generated by selecting a segment of flows from\nthe data set at random.\narXiv:2306.04920v1  [cs.CR]  8 Jun 2023\nCoNEXT-SW â€™22, December 9, 2022, Roma, Italy Loc Gia Nguyen and Kohei Watabe\nTable 1: Average composition of data sets\nCIDDS-001 CIDDS-001 CIDDS-002\nOpenStack External Server\n1 set 10 sets 10 sets 10 sets\nCIDDS-001 large CIDDS-001 internal CIDDS-001 external CIDDS-002\nlabel # label # label # label #\nğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ 25178585 ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ 10000 ğ‘¢ğ‘›ğ‘˜ğ‘›ğ‘œğ‘¤ğ‘›10000 ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™10000\nğ‘‘ğ‘œğ‘  2775655 ğ‘‘ğ‘œğ‘  9000 ğ‘ ğ‘¢ğ‘ ğ‘ğ‘–ğ‘ğ‘–ğ‘œğ‘¢ğ‘ 10000 ğ‘ ğ‘ğ‘ğ‘› 10000\nğ‘ğ‘œğ‘Ÿğ‘¡ğ‘†ğ‘ğ‘ğ‘› 194642 ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘†ğ‘ğ‘ğ‘› 935\nğ‘ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘ğ‘› 5266 ğ‘ğ‘–ğ‘›ğ‘”ğ‘†ğ‘ğ‘ğ‘› 45\nğ‘ğ‘Ÿğ‘¢ğ‘¡ğ‘’ğ¹ğ‘œğ‘Ÿğ‘ğ‘’ 4992 ğ‘ğ‘Ÿğ‘¢ğ‘¡ğ‘’ğ¹ğ‘œğ‘Ÿğ‘ğ‘’ 20\nTotal 28159140 Total 20000 Total 20000 Total 20000\nThe overall architecture of the system is illustrated in Figure 1.\nSix features from each flow (Src Pt, Dst Pt, Proto+Flags, Packets,\nBytes, Duration) are used as input data, these features are dis-\ncretized as described in EFCâ€™s original paper [3]. The discrete value\nof each features are encoded as numbers ( flowğ‘– ). BERT decodes\neach number into a 128-dimension vector, concatenates them to\nform a 768-dimension vector (ğ‘’flowğ‘– ), and processes it to produce a\ndifferent 768-dimension vector (â„flowğ‘– ). The output of BERT is then\npassed through a Multilayer Perceptron classifier (a linear layer\nwith softmax output), which reduces the dimension from 768 to\n2. This 2-dimension vector represents the predicted probability of\nthe flow being benign and malicious (e.g. benign 0.99, malicious\n0.01). The predicted class of the flow is the class with the higher\nprobability (e.g. benign).\nWe used data from three different domains: CIDDS-001 Open-\nStack, CIDDS-001 External Server, and CIDDS-002 to evaluate the\ndomain adaptation capability of the proposed method. Average\ncomposition of the data sets used in the experiment are shown in\nTable 1. Training was performed on one set of CIDDS-001 large.\nWhile testing was performed on CIDDS-001 internal, CIDDS-001 ex-\nternal, and CIDDS-002, each containing ten sets randomly selected\nfrom the full data sets. This testing scheme mimics the one used by\nCamila et al.[3] to make the results of the proposed method more\ncomparable to those of EFC. Flows labeled ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ are considered\nbenign, while those labeled otherwise are considered malicious.\nFor CIDDS-001 external, flows labeled ğ‘¢ğ‘›ğ‘˜ğ‘›ğ‘œğ‘¤ğ‘› and ğ‘ ğ‘¢ğ‘ ğ‘ğ‘–ğ‘ğ‘–ğ‘œğ‘¢ğ‘  are\nconsidered benign and malicious respectively.\nWe assess the performance of our method in comparison to\nEFC and ML classifiers including Decision Tree (DT), K-Nearest\nNeighbors (KNN), Multilayer Perceptron (MLP), Naive Bayes (NB),\nand Support Vector Machine (SVM). Each classifierâ€™s performance\nis measured using Accuracy and F1 score [1].\n3 RESULTS AND DISCUSSION\nTable 2 shows the average performance and standard error for each\nclassifier. All classifiers achieved higher Accuracy and F1 Score on\nCIDDS-001 internaltest sets (same domain as training data) com-\npared to the other test sets (different domains from training data).\nBoth the proposed method and EFC maintained performance across\nthe two different domains, with the proposed method outperform-\ning EFC.\nWe also experimented with training the classifiers on smaller but\nbalanced data sets (containing 80000 flows with the same proportion\nof labels as in CIDDS-001 internal). However, performance was\nTable 2: Average classification performance and standard\nerror\nTrain CIDDS-001 largeTest CIDDS-001 internal Test CIDDS-001 external Test CIDDS-002Classifier Accuracy F1 score Accuracy F1 score Accuracy F1 scoreProposal 0.994(0.002) 0.994(0.002)0.895(0.027) 0.904(0.022) 0.916(0.045) 0.877(0.072)EFC 0.941(0.005) 0.941(0.004) 0.747(0.039) 0.796(0.025) 0.846(0.046) 0.800(0.070)DT 0.996(0.001) 0.996(0.001)0.870(0.028) 0.874(0.024) 0.818(0.061) 0.707(0.106)KNN 0.989(0.002) 0.989(0.003) 0.839(0.008) 0.811(0.011) 0.818(0.061) 0.707(0.106)MLP 0.992(0.001) 0.992(0.001) 0.573(0.009) 0.285(0.023) 0.832(0.059) 0.729(0.108)NB 0.903(0.002) 0.892(0.002) 0.500(0.000) 0.000(0.000) 0.499(0.000) 0.001(0.000)SVM 0.570(0.015) 0.245(0.047) 0.738(0.016) 0.718(0.021) 0.513(0.013) 0.090(0.037)\nworse for all classifiers when compared to those trained on CIDDS-\n001 large. Notably, the performance of the proposed method was\nsignificantly affected for all domains. By creating balanced data\nsets, the distribution of flows within a sequence was also altered.\nThis suggests that the distribution of flows within a sequence is\nlearned by the model of the proposed method.\n4 CONCLUSION AND FUTURE WORK\nIn this study, we suggest the use of singular flows input to be a\npossible explanation for the poor domain adaptation capability of\nconventional ML-based classifiers. Then we proposed the used of\nsequences of flows to address this limitation. We utilized BERT\nmodel for the representation of flow sequences and an MLP clas-\nsifier to discriminate between benign and malicious flows. Early\nexperimental results showed that the proposed method is capable\nof achieving good and consistent results across different domains.\nHowever, more extensive testing on recent data sets is needed to\nfurther evaluate its domain adaptation capability.\nIn future work, we plan to investigate the impact of flow se-\nquence sampling method on result, such as using only benign flows\nor grouping flows into sequences that originate from the same hosts.\nMaking the system less reliant on labeled data is another research\ngoals of ours. We aim to achieve this by modeling sequences of\nbenign flows then look for anomalous representations produce by\nBERT, indicating malicious flows.\nACKNOWLEDGMENTS\nThis work was partly supported by JSPS KAKENHI Grant Number\nJP20H04172.\nREFERENCES\n[1] Zeeshan Ahmad, Adnan Shahid Khan, Cheah Wai Shiang, Johari Abdullah, and\nFarhan Ahmad. 2021. Network Intrusion Detection System: A Systematic Study\nof Machine Learning and Deep Learning Approaches. Trans. Emerg. Telecommun.\nTechnol. 32, 1 (jan 2021), 30 pages. https://doi.org/10.1002/ett.4150\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). Association for Computational Linguistics, Minneapolis, Minnesota,\n4171â€“4186. https://doi.org/10.18653/v1/N19-1423\n[3] Camila F. T. Pontes, Manuela M. C. de Souza, JoÃ£o J. C. Gondim, Matt Bishop, and\nMarcelo Antonio Marotta. 2021. A New Method for Flow-Based Network Intrusion\nDetection Using the Inverse Potts Model. IEEE Transactions on Network and Service\nManagement 18, 2 (2021), 1125â€“1136. https://doi.org/10.1109/TNSM.2021.3075503\n[4] Markus Ring, Sarah Wunderlich, Dominik GrÃ¼dl, Dieter Landes, and Andreas\nHotho. 2017. Flow-based benchmark data sets for intrusion detection. In Proceed-\nings of the 16th European Conference on Cyber Warfare and Security. 361â€“369.\n[5] Markus Ring, Sarah Wunderlich, Dominik GrÃ¼dl, Dieter Landes, and Andreas\nHotho. 2017. Creation of Flow-Based Data Sets for Intrusion Detection. Journal of\nInformation Warfare16 (12 2017), 41â€“54.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8280706405639648
    },
    {
      "name": "Domain adaptation",
      "score": 0.8001171946525574
    },
    {
      "name": "Intrusion detection system",
      "score": 0.7162303924560547
    },
    {
      "name": "Transformer",
      "score": 0.6153115630149841
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5560827255249023
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5483669638633728
    },
    {
      "name": "Encoder",
      "score": 0.529490053653717
    },
    {
      "name": "Machine learning",
      "score": 0.5119665861129761
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4646569788455963
    },
    {
      "name": "Inference",
      "score": 0.4327506124973297
    },
    {
      "name": "Data mining",
      "score": 0.36263805627822876
    },
    {
      "name": "Engineering",
      "score": 0.09465700387954712
    },
    {
      "name": "Classifier (UML)",
      "score": 0.07844522595405579
    },
    {
      "name": "Operating system",
      "score": 0.0704641342163086
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}