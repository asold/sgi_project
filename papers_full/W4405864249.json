{
  "title": "EEG-based emotion recognition using multi-scale dynamic CNN and gated transformer",
  "url": "https://openalex.org/W4405864249",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101325235",
      "name": "Zhuoling Cheng",
      "affiliations": [
        "Yangtze University"
      ]
    },
    {
      "id": "https://openalex.org/A5113155752",
      "name": "Xuekui Bu",
      "affiliations": [
        "Yangtze University"
      ]
    },
    {
      "id": "https://openalex.org/A2148438882",
      "name": "Qingnan Wang",
      "affiliations": [
        "Huaihua University"
      ]
    },
    {
      "id": "https://openalex.org/A1951789788",
      "name": "Tao Yang",
      "affiliations": [
        "First People's Hospital of Jingzhou"
      ]
    },
    {
      "id": "https://openalex.org/A2139411652",
      "name": "Jihui Tu",
      "affiliations": [
        "Yangtze University"
      ]
    },
    {
      "id": "https://openalex.org/A5101325235",
      "name": "Zhuoling Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5113155752",
      "name": "Xuekui Bu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2148438882",
      "name": "Qingnan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1951789788",
      "name": "Tao Yang",
      "affiliations": [
        "First People's Hospital of Jingzhou"
      ]
    },
    {
      "id": "https://openalex.org/A2139411652",
      "name": "Jihui Tu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4229024336",
    "https://openalex.org/W2980306787",
    "https://openalex.org/W2912483755",
    "https://openalex.org/W2937977583",
    "https://openalex.org/W2889717020",
    "https://openalex.org/W2963631961",
    "https://openalex.org/W4206239953",
    "https://openalex.org/W4205558134",
    "https://openalex.org/W4221056805",
    "https://openalex.org/W2962905870",
    "https://openalex.org/W2093440960",
    "https://openalex.org/W2999200800",
    "https://openalex.org/W3004161400",
    "https://openalex.org/W2524421030",
    "https://openalex.org/W3003908700",
    "https://openalex.org/W3217075389",
    "https://openalex.org/W4281399130",
    "https://openalex.org/W4292317441",
    "https://openalex.org/W3171411730",
    "https://openalex.org/W4283069747",
    "https://openalex.org/W4296617823",
    "https://openalex.org/W4384917402",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W4200042752",
    "https://openalex.org/W4289538860",
    "https://openalex.org/W4386086404",
    "https://openalex.org/W2625929003",
    "https://openalex.org/W2002055708",
    "https://openalex.org/W1947251450",
    "https://openalex.org/W2786768213",
    "https://openalex.org/W4394699177",
    "https://openalex.org/W3197261948",
    "https://openalex.org/W2580887161",
    "https://openalex.org/W3084140714",
    "https://openalex.org/W2995613213",
    "https://openalex.org/W4321021683",
    "https://openalex.org/W4225411558",
    "https://openalex.org/W2790404832",
    "https://openalex.org/W3024961463",
    "https://openalex.org/W4400951729",
    "https://openalex.org/W4324131915",
    "https://openalex.org/W4389427650",
    "https://openalex.org/W3194836659"
  ],
  "abstract": "Emotions play a crucial role in human thoughts, cognitive processes, and decision-making. EEG has become a widely utilized tool in emotion recognition due to its high temporal resolution, real-time monitoring capabilities, portability, and cost-effectiveness. In this paper, we propose a novel end-to-end emotion recognition method from EEG signals, called MSDCGTNet, which is based on the Multi-Scale Dynamic 1D CNN and the Gated Transformer. First, the Multi-Scale Dynamic CNN is used to extract complex spatial and spectral features from raw EEG signals, which not only avoids information loss but also reduces computational costs associated with the time-frequency conversion of signals. Then, the Gated Transformer Encoder is utilized to capture global dependencies of EEG signals. This encoder focuses on specific regions of the input sequence while reducing computational resources through parallel processing with the improved multi-head self-attention mechanisms. Third, the Temporal Convolution Network is used to extract temporal features from the EEG signals. Finally, the extracted abstract features are fed into a classification module for emotion recognition. The proposed method was evaluated on three publicly available datasets: DEAP, SEED, and SEED_IV. Experimental results demonstrate the high accuracy and efficiency of the proposed method for emotion recognition. This approach proves to be robust and suitable for various practical applications. By addressing challenges posed by existing methods, the proposed method provides a valuable and effective solution for the field of Brain-Computer Interface (BCI).",
  "full_text": "EEG-based emotion recognition \nusing multi-scale dynamic CNN and \ngated transformer\nZhuoling Cheng1, Xuekui Bu1, Qingnan Wang2, Tao Yang3 & Jihui Tu1\nEmotions play a crucial role in human thoughts, cognitive processes, and decision-making. EEG has \nbecome a widely utilized tool in emotion recognition due to its high temporal resolution, real-time \nmonitoring capabilities, portability, and cost-effectiveness. In this paper, we propose a novel end-to-\nend emotion recognition method from EEG signals, called MSDCGTNet, which is based on the Multi-\nScale Dynamic 1D CNN and the Gated Transformer. First, the Multi-Scale Dynamic CNN is used to \nextract complex spatial and spectral features from raw EEG signals, which not only avoids information \nloss but also reduces computational costs associated with the time-frequency conversion of signals. \nThen, the Gated Transformer Encoder is utilized to capture global dependencies of EEG signals. This \nencoder focuses on specific regions of the input sequence while reducing computational resources \nthrough parallel processing with the improved multi-head self-attention mechanisms. Third, the \nTemporal Convolution Network is used to extract temporal features from the EEG signals. Finally, the \nextracted abstract features are fed into a classification module for emotion recognition. The proposed \nmethod was evaluated on three publicly available datasets: DEAP , SEED, and SEED_IV. Experimental \nresults demonstrate the high accuracy and efficiency of the proposed method for emotion recognition. \nThis approach proves to be robust and suitable for various practical applications. By addressing \nchallenges posed by existing methods, the proposed method provides a valuable and effective solution \nfor the field of Brain-Computer Interface (BCI).\nKeywords EEG signals, Temporal-spatial features, Multi-scale dynamic 1D CNN, Gated transformer \nencoder, Temporal convolution network\nEmotions are physiological and cognitive reactions that humans experience in response to events or situations, \nwhich play a crucial role in human thoughts, cognitive processes, and decision making. Emotion analysis has \nfound wide applications in various fields 1, including disease diagnosis, psychological analysis and evaluation, \nand social behavior analysis 2, etc. Traditional emotion analysis methods used non-physiological signals to \ndetect the emotion state, including facial expression, speech, and behavioral characteristics 3–6. However, these \nmethods are often influenced by individual and cultural differences, which leads to subjective interpretations \nof emotions and potential misidentification. With the rapid development of brain-computer interface (BCI)7–9, \nElectroencephalography (EEG)10 is a technique that captures brain signals corresponding to various states from \nthe scalp surface area through the use of BCI. These signals are often categorized into different frequency bands, \neach associated with specific brain activities. These frequency bands are delta, theta, alpha, beta, and gamma, \nwhich covers a range from 0.1 Hz to more than 100 Hz. EEG has been extensively used in emotion research \nbecause of its high temporal resolution, real-time monitoring, portability, and cost-effectiveness. Therefore, the \napplication of EEG to emotion analysis has garnered widespread attention and has become a research hotspot.\nMany emotion recognition methods based on EEG signals have been proposed. The clustering method, as \ndescribed in11,12, organizes EEG signals into clusters with similar characteristics based on the intrinsic structure \nof the data. While this method is known for its simplicity and efficiency, it is susceptible to producing incorrect \ndetections. The primary challenges lie in the selection of initial centers and determining the appropriate number \nof clusters. Errors in these aspects can lead to suboptimal clustering results and compromise the accuracy of the \ndetection process. Machine learning methods, as outlined in13–16, employ manual feature selection and classifiers \nfor emotion detection, yielding improved performance compared to clustering methods. The key advantage lies \n1School of Electronic Information and Electrical Engineering, Yangtze University, Jingzhou 434100, Hubei, \nChina. 2School of Physics, Electronics and Intelligent Manufacturing, Huaihua University, Hunan 418000, \nChina. 3Department of Neurology, Jingzhou First People’s Hospital, Jingzhou 434000, Hubei, China. email:  \ntujh@yangtzeu.edu.cn\nOPEN\nScientific Reports |        (2024) 14:31319 1| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports\n\nin the machine learning model’s ability to learn the relationships between input features and corresponding labels \nduring the training process. This enables the model to generalize effectively when confronted with new data. \nHowever, it’s worth noting that the use of artificial feature extraction to construct EEG signal features in these \nmethods can potentially limit their generalization ability. With the development of deep learning, Convolutional \nNeural Network (CNN)17–19 has been utilized for EEG emotion detection. In this approach, EEG signals are first \nconverted into spectrum images by time-frequency conversion, and then these spectrum images are fed into \nthe CNN network for feature extraction and classification. Compared to traditional machine learning, CNN \ndemonstrates superior performance due to its enhanced generalization ability. Dynamical graph convolutional \nneural networks (DGCNN)20 to classify graphs encoded with spatiotemporal information of the emotion EEG \nsource, and it can effectively extract spatiotemporal features. However, one limitation of them is its inability to \ncapture temporal features since EEG is a time serial signal, which leads to potential shortcomings in detection \naccuracy. Given that LSTM or RNN can capture temporal characteristics, they are introduced in combination \nwith CNN for emotion detection21–23. CNN is employed to extract local feature of EEG signal, while LSTM or \nRNN extract temporal features. This approach tends to yield more satisfactory results compared to using CNN \nalone, but these methods require preprocessing of the data using time-frequency transformation, which could \npotentially lead to a loss of information and increase computational demands. The Transformer24 is an Encoder-\nDecoder architecture with a Self-Attention Mechanism introduced by Vaswani et al. in 2017. Initially proposed \nfor natural language25 processing tasks like machine translation, its versatility has led to widespread adoption in \nvarious domains, including computer vision, speech recognition, and more. The advantages of the transformer \nare attributed to its parallelization capabilities, scalability to handle long sequences, and its effectiveness in \ncapturing contextual information. Consequently, the transformer framework has been increasingly applied to \nEEG emotion detection 26,27, which is achieving better performance than the combined method of CNN and \nLSTM.\nThe above methods have achieved satisfactory results, but the following challenges still need to be addressed: \n(1) Reduction of complexity. The currently existing methods require a pre-processing process, that is, time-\nfrequency transformation, which can be effective in extracting features from signals in both time and frequency \ndomains. However, this approach poses the following problems: firstly, time-frequency conversion results in \ninformation loss and increased computational load; secondly, the preprocessing introduces complexity into the \nsystem. (2) Prediction accuracy and robustness. Given that EEG signals are the time sequence data, different \nsegments of the sequence have varying levels of importance and influence on the prediction result. Existing \nmodels often overlook this issue, which can result in failure to capture crucial patterns or information present \nin the EEG data. This can ultimately lead to a reduction in both performance and generalization capabilities. \nIn this paper, motivated by the practical application of EEG emotion detection, we propose a novel end-to-end \nemotion recognition method from EEG signals using a Multi-Scale Dynamic CNN and a Gated Transformer \nEncoder, named MSDCGTNet. Experiments were conducted on three public datasets: DEAP , SEED, and \nSEED_IV , achieving impressive accuracy results of 99.66%, 98.85%, and 99.67%, respectively. Our model not \nonly provides novel and robust tools for EEG-based emotion recognition but also holds significant potential for \ndiverse applications within brain-computer interface (BCI) systems.\nIn short, the novelty and significant contributions of the study are as follows:\n (1)  In this paper, we propose a novel end-to-end framework that utilizes the Multi-Scale Dynamic 1D CNN \nand the Gated Transformer Encoder for EEG-based emotion recognition. This unique framework addresses \nthe challenges of EEG feature extraction and emotion classification as a unified process. Additionally, it \nreduces the model’s complexity.\n (2)  The Multi-Scale Dynamic 1D CNN is employed to extract spatial-spectral features from raw EEG signals, \nwhich not only helps avoid information loss caused by time-frequency conversion but also significantly \nenhances recognition performance.\n (3)  The Gated Transformer Encoder is used to extract global features from EEG signals. The gated layers can \nlearn to dynamically select relevant features, which is particularly useful for EEG data where important \nsignals may vary across time and channels. By incorporating non-linear transformations, the model can \nbetter capture complex relationships in the data that are not easily addressed by linear operations alone. \nFurthermore, the improved multi-head self-attention mechanism efficiently handles long EEG sequences \ncompared to traditional Transformers, thereby reducing computational complexity and making it suitable \nfor real-time processing of large-scale EEG signals.\n (4)  We compared our method with other state-of-the-art approaches using three public datasets: DEAP , SEED, \nand SEED_IV . The results demonstrate that our method achieves superior recognition accuracy and effi -\nciency compared to other leading methods.\nThe remaining sections of this article are as follows: In “Methods” the detailed structure of our method is reviewed. \nIn “Experiment” , the experimental dataset, experimental evaluation metrics, performance comparison of the \nmethod with other methods are described and future direction is discussed. In “ Conclusion” , the conclusion is \npresented.\nMethods\nAs shown in the Fig.  1, the method based on Multi-Scale dynamic 1DCNN and the Gated transformer for \nemotion recognition from raw EEG data is introduced in this paper. The method is divided into four parts, \n(1) Multi- Scale dynamic 1DCNN, which extracts the local characteristic from the raw EEG data. (2) The \nGated Transformer Encoder, which extracts the global features. (3) Temporal Convolutional Network (TCN), \nwhich extract discriminative time series features while integrating historical information of the time series. (4) \nScientific Reports |        (2024) 14:31319 2| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nSoftMax, which selects the class with the highest probability as the final prediction. This section details the main \nidea of MSDCGTNet.\nEEG signal description\nEEG data can be regarded as two-dimensional sequence data with spatial dimensions (EEG electrode channels) \nand temporal dimensions (data sampling points). The spatial dimension reflects the activation state of the brain \nin different functional areas, while the temporal dimension reflects the temporal change of brain activity. Since \nEEG segments of varying lengths contain different emotion information, we use time windows to segment the \nEEG signals. Each EEG segment signal is captured by C channels over a time interval with T timestamps. This \ndata is then represented as a C × T matrix, as shown in Fig. 2.\nWhere Xi= [xi(t1), xi(t2), xi(t3),…, xi(tT)] is the i-th channel over a time interval.\nThe objective of the MSDCGTNet model is to map the input EEG data Xi to its corresponding category yi. \nGiven a set of labeled EEG data U = {(Xi,y i)}m\ni=1, where yi ∈ {1, … ,n} is the category label corresponding to \ntrial Xi, n is the total number of categories defined by set U, and m is the total number of samples.\nFig. 2. The electrode map of the data collection channel.\n \nFig. 1. Structural flowchart of the MSDCGTNet model.\n \nScientific Reports |        (2024) 14:31319 3| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nMulti-scale dynamic 1D CNN\nAt present, most existing methods for EEG-based emotion recognition rely on time-frequency conversion \nsince CNN network typically requires image-like inputs. This involves transforming EEG signal into various \nspectrograms, such as Fbank, MFCC, and speech spectrograms. From which multiple features are then extracted \nfor recognition. However, these methods have several limitations: (1) Information Loss. Time-frequency \nconversion leads to information loss, thereby reducing the accuracy of recognition. (2) Computational Time. \nThe time-frequency conversion process consumes significant computing resources, which can hinder real-time \nEEG processing. (3) Non-stationary and Dynamic. EEG signals are non-stationary and dynamic, and features \nextracted through serial convolution may be too simplistic, decreasing the model’s generalization ability. To \naddress these limitations, we propose the Multi-Scale Dynamic 1D CNN, inspired by the pioneering work \ndiscussed in26. This method allows for efficient feature extraction directly from raw EEG signals, circumventing \nthe drawbacks associated with time-frequency conversion.\nAs shown in Fig.  3, the multi-scale dynamic convolution module is designed to effectively capture the \ndynamic frequency characteristics of EEG signals by using convolution kernels of different sizes. The detailed \nsteps are as follows: ① Temporal Convolution. Each convolution kernel applies a frequency filter for temporal \nconvolution to capture variations in the time domain. ② Spatial Convolution. Convolution with a kernel size of \n(C, 1) is performed to process EEG signals in the spatial domain, focusing on the spatial relationships between \nEEG channels. ③ Dimensionality Reduction. An average pooling layer with a size of (1, 4) is used to reduce the \ndimensionality of the temporal features, and dropout technology is employed to prevent overfitting. ④ Feature \nFusion. The concatenation layer fuses the output features from the pooling layer. As a result, the tensor obtained \nafter the multi-scale dynamic convolutional module represents signal sources from different frequencies and \nelectrode channels.\nIn the first layer of multi-scale dynamic convolution, the temporal convolution kernels are sized according \nto a specific ratio of the EEG sampling frequency f. The αi determines the relationship between the convolution \nkernel size K and the EEG sampling frequency f, where i represents the scale level in the multi-scale network. \nThe network has multiple scales L, and for each scale i(ranging from 1 to L), α is set to 0.5, the kernel size is \nadjusted according to the radio coefficient. Let represents the size of the i-th level convolution kernel. The size is \ndetermined by the following equation:\n ki\nT = (1,αif),i ∈ [3, 4, 5] (1)\nFig. 3. The detailed structure of multi-scale dynamic CNN.\n \nScientific Reports |        (2024) 14:31319 4| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nConsidering that excessive convolution operations can increase the number of parameters, and the brain signals \nrelated to emotions are in the following frequency bands28: α: 8–13 Hz, β: 14–30 Hz, γ: 31–51 Hz, thereby affecting \noperational efficiency, the number of scale level L is set to 3. For i ranging from 3 to 5, the ratio coefficients \nbecome [0.125, 0.0625, 0.03125], meaning the sizes of correspond to [(1, 16), (1, 8), (1, 4)]. This adjustment \nensures that the model maintains its classification efficiency without compromising on performance. From the \nfrequency perspective, when the length of k is set to different ratios of f, the temporal convolution layers can \ncapture frequency information of 8 Hz, 16 Hz, and 32 Hz, respectively 29. From the temporal perspective, these \nconvolution kernel sizes correspond to time steps of 125 ms, 62.5 ms, and 31.25 ms in duration.\nThe gated transformer encoder\nEach convolutional layer under the CNN framework shares weights, which means that they focus equally on each \npart of the EEG signal. To learn the global dependencies in EEG signals and focus on the relevant parts crucial \nfor classification tasks, the Gated Transformer Encoder is proposed to capture global features in EEG signals, \nwhich are crucial for understanding the overall context and dependencies within the data. The multi-head self-\nattention mechanism in the Transformer encoder allows the model to focus on different parts of the input data \nsimultaneously, identifying the most relevant features for the classification task. The GLU layer combines linear \ntransformations with a gating mechanism, which helps in selectively passing important features while filtering \nout less relevant information. This improves model performance and efficiency by reducing computational \noverhead.\nThe output of the Multi-Scale Dynamic Convolution module is defined as O = {x1, x2, x3, …, xn}, which serves \nas the input to the Gated Transformer Encoder for further processing and feature extraction. Here, O consists of \nN vectors, each of which has a dimension d. The Gated Transformer Encoder proposed in this paper incorporates \na gated linear unit (GLU) layer to the traditional Transformer encoder. It is composed of l identical layers, each \nconsisting of three modules: the multi-head attention mechanism (MHA), the multi-layer perceptron (MLP), \nand the gated linear unit (GLU). The Gated Transformer Encoder can be expressed in formulas (2) to (4).\n O′′\nl = MHA (LayerNorm (Ol−1)) +Ol−1 (2)\n O′\nl = MLP (LayerNorm (O′′\nl )) +O′′ (3)\n Ol = GLU(O′\nl) (4)\nThe improved multi-head self-attention mechanism\nSince Artificial Neural Networks (ANN) tend to lose a significant amount of information during long-distance \npropagation and are not very sensitive to feature importance, they can struggle to capture complex dependencies \nin data, which can lead to suboptimal performance in tasks that require understanding intricate relationships \nor maintaining long-range contextual information. This limitation often necessitates the use of more advanced \narchitectures, such as Transformers, which utilize self-attention mechanisms to better preserve and emphasize \ncritical features across long sequences. However, the computational complexity of the Multi-Head Self-\nAttention mechanism is dominated by the dot-product calculation, which is O(n2·d). This quadratic dependence \non the sequence length n makes self-attention computationally expensive for long sequences. To address the \ncomputational and memory challenges associated with self-attention, an improved attention mechanism \nmethod30 is proposed to reduces complexity by projecting the keys and values to a lower-dimensional space, \nwhich is achieve linear complexity relative to sequence length. This approach aims to retain the benefits of self-\nattention while making it feasible to apply to longer sequences and larger datasets.\nAs shown in Fig.  4, the key idea behind the improved method is to replace the standard self-attention \nmechanism in transformers with a more efficient linear attention mechanism. The detailed steps are as follows:\n (1)  Linear projection of keys and values.\nIn the standard Transformer attention mechanism, Q, K, and V are the queries, keys, and values, respectively. \nInstead of directly computing K and V , the improved method introduces two learned projection matrices \nPK ∈ Rn×k and PV ∈ Rn×k, which project the original keys K and values V into a lower-dimensional space of \ndimension, where k < n (sequence length). The formula of projected Keys K and Values V is as follows.\n \nKp = K · PK\nVp = V · PV\n (5)\nThese matrices are used to reduce the dimensionality of the keys and value from Rn×d to Rk×d .\n (2)  Query, key, and value computation.\nCompute the query (Q), key (K), and value (V) matrices using learned weight matrices:\n Q = X · WQ,K = X · WK,V = X · WV  (6)\n (3)  Scaled dot-product attention.\nCompute attention scores using the scaled dot-product attention with projected keys:\nScientific Reports |        (2024) 14:31319 5| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\n \nAttention(Q, Kp,V p)= softmax(Q · KT\np√dk\n) · Vp (7)\nThe softmax function normalizes the attention scores, ensuring they sum to 1.\n (4)  Multi-head attention.\nSimilar to the standard Transformer, the improved can utilize multiple attention heads. Each head operates \nindependently on different parts of the input space:\n headi = Attention(QWQ,i,K pWK,i,V pWV,i) (8)\nConcatenate the outputs of all heads and apply a final linear transformation:\n MultiHead(Q, Kp,V p)= Concat(head1, head2, ..., headh) · WO (9)\nWQ, i, WK, i, WV,  i, WO are learned matrices for each head, and h is the number of attention heads.\n (5)  Feedforward network.\nPass the result through a position-wise feedforward neural network, which consists of two linear transformations \nwith a ReLU activation in between:\n FFN (x)= ReLU(x · W1 + b1) · W2 + b2 (10)\nThis network is applied independently to each position in the sequence.\n (6)  Residual connections and layer normalization.\nAdd residual connections around each sub-layer followed by layer normalization:\n LayerNorm(x + SubLayer(x)) (11)\nThis helps stabilize training and allows for better gradient flow through the network.\nThe gated linear unit (GLU)\nAs show in Fig.  5, to retain the temporal and spatial information when processing EEG data and to speed up \noperations during parallel data processing in the Transformer, the Gated Linear Unit (GLU) 31 is integrated \ninto the Transformer encoder. The GLU controls the information transmission of EEG signals during training, \nthereby improving the network’s learning performance. The GLU consists of a convolutional layer and a gating \nlayer, and its calculation is shown in Equation:\n H(x)=( x ∗ W + b) ⊗ σ(x ∗ U + c) (12)\n where x is the embedding vector; W and U are the convolution kernel in the convolution operation, b and c \nare the bias parameters, * is the convolution operation, ⊗ is the multiplication of the corresponding elements \nof the matrix, σ is the sigmoid activation function. The sigmoid activation function, denoted as σ, compresses \nFig. 4. The detailed structure of the improved multi-head self-attention mechanism.\n \nScientific Reports |        (2024) 14:31319 6| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nall information before the current moment in the time window. This facilitates the control of effective EEG \ninformation entering the next layer.\nTemporal convolutional network (TCN)\nAfter extracting EEG signal features of different frequencies and electrode channels, TCN module32 is applied to \nextract discriminative time series features while integrating historical information of the time series. The TCN \nis composed of stacked residual blocks, with each residual block consisting of two dilated causal convolution \n(DCC) layers with different dilation coefficients, as shown in Fig. 6. Each convolution layer is followed by batch \nnormalization (Batch Norm) and an Exponential Linear Unit (ELU) activation function.\nClassification\nEmotion recognition from EEG signal is actually a multi-classification problem. SoftMax is often used in the \nfinal layer of a neural network for multi-class classification tasks. The predicted class is the one with the highest \nprobability according to the SoftMax output. The SoftMax function is differentiable, which is crucial for training \nneural networks using gradient-based optimization algorithms. Therefore, SoftMax is employed to emotion \nclassification in this paper.\nGiven an input vector x of length M, the SoftMax function computes a vector S(x)i of the same length M. The \nSoftMax function is defined as follows:\nFig. 5. The detailed structure of GLU.\n \nScientific Reports |        (2024) 14:31319 7| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\n \nS(x)i = exi\nM∑\nj=1\nexj  (13)\n \nj =\nM\narg max\ni=1\nP (Si) (14)\n where e is the base of the natural logarithm (Euler’s number), xi is the i-th element of the input vector x, S(x)i is \nthe i-th element of the output vector S(x), M is the total number of classes. The argmax designates the selection \nof the category label that yields the maximum conditional probability. Therefore, the category with the highest \nprobability j is selected as the prediction result.\nFig. 6. The detailed structure of TCN.\n \nScientific Reports |        (2024) 14:31319 8| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nExperiment\nIn this section, experimental results are presented to verify the performance of our method. First, the experiment \ndata, environment, evaluation metrics and hyperparameter setting are introduced in detail. Then, both \nthe efficiency and the effectiveness of our method are evaluated and compared with several state-of-the-art \nmethodologies. Finally, the ablation experiments and parameter analysis were employed to further evaluate our \nproposed approach.\nDatasets\nThree open-source datasets (DEAP , SEED, SEED_IV) are utilized as the test datasets to access the performance of \nthe proposed method. These datasets used in our experimentation and the experimental protocol are described \nin detail as follows.\n (1)  The DEAP dataset33, created by the Queen Mary University of London, is designed for emotion analysis. It \nconsists of physiological signals recorded from 32 participants while they watch 40 music video clips. All \nparticipants were students, consisting of 16 males and 16 females, with ages ranging from 19 to 37 years \nand an average age of 26.9 years. They were all in good physical health, had no mental disorders or brain \ninjuries, and were right-handed. These stimuli are carefully selected to evoke various emotions, which pro-\nvides a rich dataset for researchers to study the relationship between physiological responses, particularly \nEEG signals. 63 s of EEG signal data were collected by 32-channel Biosemi ActiveTwo device according to \nthe international 10–20 system. The first 3 s of signals were the pre-trial baseline signals, and the remaining \n60 s of EEG recordings were mood signals. At the end of each experiment, the subjects performed a self-as-\nsessment of their arousal and valence to determine whether the corresponding emotions were accurately \nelicited. The EEG signals were downsampled to 128 Hz. The electromyography (EMG) and electroencepha-\nlogram (EOG) signals were removed before the dataset was released. They were passed through a bandpass \nfilter between 4 and 45 Hz to filter out noise. Emotions were classified into 4 categories based on the degree \nof arousal and level of valence (1–9) of each marker: happy, sad, fearful, and neutral.\n (2)  The SEED dataset34 is a large-scale open-source dataset developed by BCMI Labs for EEG-based emotion \nclassification tasks, which can be divided into 3 categories of emotions (positive, neutral, and negative). \nEmotion clips were meticulously selected, ensuring that the clips can reliably evoke the corresponding emo-\ntions in subjects. The dataset consists of 15 emotion clips extracted from different Chinese films, each clip \nis approximately 4 min long and exclusively depicts one type of emotion. Fifteen healthy native Chinese \nstudents, including seven males and eight females, with an average age of 23, participated in the EEG signal \nacquisition experiment. EEG signals were recorded using a 62-channel ESI NeuroScan system with a sam-\nple rate of 1000 Hz. An experiment consisted of fifteen trials with each trial corresponding to an emotion \nfilm clip. Before each clip, there was a 5-s hint, followed by a 45-s self-assessment period, and a 15-s rest \nafter each clip. After the experiment, based on the subjects’ responses, only the sessions in which the target \nemotion was successfully elicited were selected for further analysis. The experiment was repeated three \ntimes for each subject. After the three sessions, all sessions for each subject were combined. EEG signals \nheavily contaminated by EMG and EOG were manually removed before the dataset was released. For data \nprocessing, the original EEG signals were first down-sampled to 200 Hz and then filtered to 4–50 Hz.\n (3)  The SEED-IV dataset35 is an evolution of the original SEED dataset, which is a multimodal dataset that in-\nclude 62-channel EEG signals from 15 subjects, including eight females. Each subject watch 24 movie clips \nwith a duration of approximately 2 min, introducing a change in the number of emotion categories from \nthree to four: happy, sad, fear, and neutral. In addition to EEG signals, SEED-IV provides eye movement \nfeatures recorded by SMI eye-tracking glasses. This multimodal dataset enhances its utility for emotion rec-\nognition by incorporating both EEG and eye movement data. For data processing, the original EEG signals \nwere first down-sampled to 200 Hz and then filtered to 4–50 Hz.\nExperimental settings\nAll experiments were conducted on the Dell 5810 server with the following main parameters: Intel Xeon E5-2660 \nprocessor, 64G of RAM, NVIDIA 2080Ti graphics card, and Centos 7.0 operating system. The Adam algorithm \nis used for training, the cross-entropy is employed as the loss function, and the entire model and recognition \nprocess were implemented using PyTorch framework.\nFor training the model, Adam was used with a batch size of 20 for all three datasets. The configuration for \nthe learning rate, number of epochs, and weight decay varied for each dataset. DEAP dataset: Learning rate of \n1e−5, 300 epochs, and weight decay of 0.01. SEED and SEED_IV datasets: Learning rate of 1e −4, 200 epochs, \nand weight decay of 1e−4.\nEvaluation metrics\nIn this experiment, three metrics—Accuracy (ACC), Standard Deviation (STD), and Average Time (AT)—\nare employed to evaluate the effectiveness of the proposed method. These metrics serve different purposes in \nassessing the model’s performance. Accuracy (ACC): ACC is the ratio of correctly recognized EEG types, which \ncan provide an overall measure of the model’s correctness in recognizing emotion states. Standard Deviation \n(STD): STD reflects the dispersion of recognition accuracy across all epochs. It indicates the variability or \nconsistency of the model’s performance. Average Time (AT): AT represents the average time taken to identify \neach EEG emotion sample. This metric assesses the efficiency of the proposed method in processing and \nclassifying emotion states. The definitions of these metrics are given as follows.\nScientific Reports |        (2024) 14:31319 9| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\n ACC = TP + TN\nN  (15)\n STD =\n√\nN∑\ni=1\n(xi − x)\nN\n (16)\n AT = T\nN  (17)\n where TP represents the true positive, indicating the number of correctly predicted EEG signals classified as \npositive cases, and TN represents the true negative, indicating the number of correctly predicted EEG signals \nclassified as negative cases. N is the total number of EEG test samples. x is the achieved classification accuracy \nfor each test experiment, and \n−\nx is the average classification accuracy of all test experiments. T represents the \ntime to identify the emotion categories of all EEG test samples.\nAccuracy analysis\nAccuracy (ACC) and Standard Deviation (STD) were used to evaluate the accuracy of emotion recognition \nand compare the performance of our method against existing approaches. The experiment is divided into two \nsegments: (1) Assessment of Various Feature Selections with Transformer Encoder. This sub-section involves \nevaluating different feature selections when combined with the transformer encoder. The goal is to understand \nthe impact of different features on the performance of the method. (2) Comparison with Existing Methods. The \nsecond sub-section focuses on comparing the performance of the proposed method with existing approaches. \nThis step aims to provide insights into how well the proposed method performs relative to established methods \nin the field. In addition, a t-test is a statistical hypothesis test used to determine if there is a significant difference \nbetween the means of two groups. It is a simple yet powerful tool for comparing the means of two methods. By \nselecting the appropriate test type, calculating the t-statistic, and interpreting the p-value, we can objectively \nevaluate whether the observed differences between methods are significant. In our experiment, the p-value \nbetween the proposed MSDCGTNet and other methods was computed to assess whether the observed \ndifferences are due to chance or represent a true distinction between the two approaches.\nSince the EEG signal is a type of multi-channel long time series data, most methods for emotion recognition \nusing EEG signal rely on time-frequency conversion. This involves transforming the EEG signal into various \nspectrograms, such as wavelets 36–38, spectrograms and FFT (Fourier transform). Subsequently, multiple \nfeatures are extracted from these spectrograms and fed into transformer encoder for emotion recognition. \nTable  1 presents the classification accuracy achieved through the combination of various time-frequency \ntransformations with transformer encoders. It can be seen from Table  1 that all the methods demonstrated \nsatisfactory performance since these methods uses local and global features. However, our proposed approach \nyielded the best results, because it used 1DCNN to extract features directly from the raw EEG signal. This not \nonly reduces the computation cost but also avoids information loss due to the time-frequency conversion.\nWe also compared the proposed method to emotion recognition with other methods. All methods have been \nrepeated, tested using the same dataset and evaluated using the same assessment indicators. Table 2 presents the \nrecognition accuracy of various methods. References39,40 utilized the feature selection and SVM classifier for the \npurpose of emotion recognition from EEG signals, which can achieve accuracies of 92.3%, 83.99% and 70.58% \nacross the DEAP , SEED and SEED_IV datasets, respectively. However, the artificial feature is subjective, so it \ntends to impact the accuracy of emotion recognition. Reference 41 adopted 2DCNN to generate a virtual image \nfrom EEG signals for emotion recognition, and reference 42 adopted two-level 1DCNN ensemble to predict \nthe emotion states, which had the better detection accuracy, reaching 81.51% and 98.43% on DEAP datasets. \nHowever, they do not consider temporal features. GMSS 43 utilized graph-based multi-task self-supervised \nlearning model for EEG emotion recognition, which achieved accuracies of 86.52% and 86.37% on the SEED and \nSEED_IV datasets, and reference44 used DGCNN to classify graphs encoded with spatiotemporal information \nof the emotion EEG source, which achieve accuracy of 97.66% on the DEAP datasets, respectively, this was \nattributed to its ability to capture inherent spatial connections between distinct areas of the brain. However, \nthey did not take into account temporal features. Reference 45 utilized recurrent neural networks (RNN) to \nhandle emotion recognition, and references 46 used CNN and LSTM to detect the emotion, which reached \nDEAP SEED SEED_IV\nACC ± STD (%) Time (s) ACC ± STD (%) Time (s) ACC ± STD (%) Time (s)\nSpectrogram 94.25 ± 0.08 (p = 4e-3) 0.02 77.95 ± 0.24 (p = 4e-6) 0.05 95.17 ± 0.13 (p = 6e-3) 0.08\nWavelet transform 98.96 ± 0.01 (p = 3e-2) 0.09 98.12 ± 0.15 (p = 1e-2) 0.09 79.50 ± 0.18 (p = 7e-6) 0.15\nFFT 99.56 ± 0.12 (p = 4e-2) 0.04 79.60 ± 0.99 (p = 4e-7) 0.04 91.03 ± 0.42 (p = 1e-4) 0.06\nOur 99.66 ± 0.15 0.0043 98.85 ± 0.81 0.0052 99.67 ± 0.12 0.0077\nTable 1. Comparison of efficiency and accuracy of DEAP , SEED and SEED_IV on different signal processing \nmethods.\n \nScientific Reports |        (2024) 14:31319 10| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\naccuracies of 94.24% and 95.44% on the SEED dataset, it is attributed to the effectiveness of RNN and LSTM \nin capturing temporal information. Nevertheless, this type of method requires a large amount of computation. \nReference26 used time-frequency conversion and transformer encoder for EEG emotion recognition, which \nhad the higher detection accuracy, reaching 99.4% and 99.1% on the DEAP dataset. However, Time-frequency \nconversion results in information loss. Compared with the models in Table  2, our model presented the best \nperformance among all candidate models, as: ① 1D-CNN extracted the local feature information, which avoids \nthe information loss; ② Transformer Encoder extracted the global and temporal information, which ensures that \nthe extracted information is comprehensive.\nAs shown in Tables  1 and 2, all p-values are less than 0.05, indicating that the accuracy improvement of \nthe proposed method is statistically significant rather than a result of random chance. Therefore, compared to \nexisting methods, the proposed method demonstrates a meaningful and substantial improvement in accuracy.\nGiven the limited amount of data and the uneven distribution of categories, 5-fold cross-validation was \nemployed to ensure the model’s robustness and its ability to generalize to new, unseen data. This method provides \na more reliable estimate of the model’s performance. Each dataset was randomly divided into five equally sized \nsubsets, or ‘folds. ’ For each fold, the model was trained on a combination of four folds (80% of the data) and \nvalidated on the remaining fold (20% of the data). This process was repeated five times, with each fold used \nexactly once as the validation set. Figure  7 shows the cross-validation results, demonstrating the effectiveness \nand practicality of this approach across different data splits. The experimental results indicate that the model \nconsistently achieves high accuracy across all folds, highlighting its stability and strong generalization ability. \nThese findings emphasize the method’s robustness and effectiveness across diverse data partitions.\nEfficiency analysis\nDetection time is crucial for emotion recognition, as it determines whether the emotion recognition is real-time. \nTable 3 illustrates the run times of different methods on various datasets for EEG-based emotion recognition, \nand these details are utilized to demonstrate the efficiency of the proposed method.\nAs shown in Table 3, the recognition time of reference39 and reference40 are 1.34 and 0.09 s, respectively. The \nreason for the low recognition efficiency of the two methods is that the SVM cannot be accelerated via GPU. \nThe recognition time of reference41 and reference42 are 0.04 and 0.12 s, respectively. The CNN method used in \nreference41 and reference42 can make use of GPU acceleration, so the detection efficiency is improved. Reference43 \nemploys GNN for emotion detection, which achieved the recognition time of 0.17 s, and reference 44 adopted \nFig. 7. Results of the 5-fold cross-validation.\n \nDEAP SEED SEED_IV\nValence Arousal\nACC ± STD (%) Classes ACC ± STD (%) ClassesACC ± STD (%) Classes ACC ± STD (%) Classes\n39 92.3 ± 0.12 (p = 4e−5) 2 86.3 ± 0.3 (p = 1e−6) 2 83.99 ± 11.72 (p = 3e−2) 3 70.58 ± 17.01 (p = 6e−3) 4\n40 85.71/multimodal (p = 1e−5) 68.26 (p = 6e−5) 3 73.14 (p = 6e−5) 4\n41 81.51 (p = 3e−2) 2 79.42 (p = 3e−2) 2 69.37 ± 0.20 (p = 1e−4) 3 68.96 (p = 2e−4) 4\n42 98.43 (p = 1e−5) 2 97.65 (p = 3e−5) 2 87.33 ± 0.76 (p = 1e−2) 3 95.76 (p = 6e−3) 4\n43 88.91 ± 0.05 (p = 7e−5) 2 92.73 ± 0.04 (p = 5e−4) 2 86.52 ± 6.22 (p = 3e−3) 3 86.37 ± 11.45 (p = 3e−5) 4\n44 97.66 ± 0.02 (p = 3e−3) 2 97.78 ± 0.11 (p = 7e−3) 2 96.53 ± 0.54 (p = 8e−3) 3 86.24 ± 0.40 (p = 2e−2) 4\n45 92.26 ± 0.07 (p = 2e−5) 2 86.33 ± 0.30 (p = 6e−6) 2 94.24 ± 5.95 (p = 4e−2) 3 79.37 ± 17.01 (p = 4e−2) 4\n46 96.12 ± 1.65 (p = 4e−3) 2 96.66 ± 2.49 (p = 2e−2) 2 95.44 ± 3.32 (p = 5e−4) 3 91.67 ± 1.32 (p = 6e−4) 4\n26 99.4 (p = 4e−2) 2 99.1 (p = 1e−2) 2 85.77 ± 0.83 (p = 1e−5) 3 98.78 ± 1.1 (p = 4e−2) 4\nOurs 99.66 ± 0.02/4 98.85 ± 0.81 3 99.67 ± 0.12 4\nTable 2. Performance comparison of DEAP , SEED, and SEED_IV data sets on different method.\n \nScientific Reports |        (2024) 14:31319 11| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nDGCNN to classify emotions, with the detection time set to 0.14 s. However, the utilization of high-order distant \nneighbors in GNN introduces a challenge known as “neighborhood explosion” . This occurrence demands more \nmemory to store the exponentially increasing number of neighbor nodes, and significantly affects the model’s \nrunning speed. Reference 45,46 use RNN or LSTM to detect the emotion state, which reached the recognition \ntime of 0.01 and 0.03 s, respectively, but RNN and LSTM calculations depend on the outputs of previous time \nsteps, which makes parallelization challenging and results in relatively slow operations. Compared with other \nmethods, the proposed method in our paper and reference 26 can achieve the highest detection efficiency since \ntransformer encoder have strong parallel processing capabilities, which can accelerate computation. The reason \nis that the latter requires a time-frequency transformation, thus it consumes more computation. Taking detection \nperformance and flexibility into consideration, our method is very competitive for practical applications.\nAblation experiment\nAblation studies involve systematically analyzing the impact of removing or modifying specific components \nor features within a model. Table 4 shows the relationship between the number of convolutional scale and the \nperformance of the approach. Table 5 illustrates the impact of the GLU and TCN modules on the performance \nof the proposed algorithm.\nTable 4 shows that the classification accuracy increases as the number of parallel convolution branches \nincreases. However, the number of model parameters also rises with the addition of more convolution branches. \nThis indicates that a greater number of convolution branches allow the model to extract more enriched time-\nfrequency domain features, thereby enhancing classification accuracy. While convolution operations are \ndesigned to extract hidden features, an excessive number of convolution kernels can increase computational \nload. Therefore, to achieve desired classification accuracy while maintaining efficiency, the network structure \nshould remain as compact as possible. In summary, to improve network efficiency and reduce the number of \nparameters, this paper uses a multi-scale convolution with three branches to effectively extract and integrate \nEEG signal features.\nDataset Parallel convolution branches (L) ACC/% Kappa Model parameters/M\nDEAP\n1 93.59 0.8340 20.5\n2 98.30 0.9064 25.1\n3 99.59 0.9643 30.5\n4 99.66 0.9702 35.2\n5 99. 97 0.9910 41.3\nSEED\n1 93.59 0.8202 29.3\n2 97.04 0.8293 33.9\n3 98.85 0.9398 38.3\n4 98.90 0.9492 42.8\n5 99.02 0.9602 47.3\nSEED_IV\n1 97.04 0.8905 22.4\n2 98.85 0.9342 30.1\n3 99.67 0.9786 38.5\n4 99.72 0.9853 46.5\n5 99.89 0.9943 54.5\nTable 4. The model performance with different scale.\n \nDEAP SEED SEED_IV\nValence Arousal\nTotal time (s) Single time (s) Total time (s) Single time (s)Total time (s) Single time (s) Total time (s) Single time (s)\n39 80.40 1.34 74.40 1.24 512.33 2.13 432.21 1.80\n40 5.50 0.09 5.50 0.09 35.66 0.15 31.80 0.13\n41 2.64 0.04 2.79 0.05 100.80 0.04 10.78 0.04\n42 7.06 0.12 6.56 0.11 182.40 0.76 33.60 0.14\n43 10.38 0.17 10.50 0.18 138.31 0.58 136.03 0.57\n44 8.39 0.14 10.22 0.17 153.21 0.64 143.59 0.60\n45 0.53 0.01 0.33 0.01 5.21 0.02 13.22 0.06\n46 1.80 0.03 1.88 0.03 7.86 0.03 10.20 0.04\n26 0.12 0.0020 0.12 0.0020 0.96 0.0040 2.16 0.0090\nOurs 0.26 0.0043 0.26 0.0043 1.25 0.0052 1.85 0.0077\nTable 3. Time efficiency comparison of different methods on DEAP , SEED and SEED_IV datasets.\n \nScientific Reports |        (2024) 14:31319 12| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nTable 5 shows the performance of the models after each component was removed, in terms of accuracy and \nstatistical significance (P-value). It can be seen that our model achieves the highest accuracy when both the GLU \nand TCN architectures are retained. Specifically, removing either GLU or TCN leads to a decrease in accuracy, \nindicating that both modules are crucial for improving model performance. Moreover, we performed a statistical \nsignificance test and the P-values for the comparisons with other models were all below 0.05, indicating that the \nproposed model significantly outperformed the alternatives. Based on these results, we conclude that both the \nGLU and TCN architectures make important contributions to the model’s performance, and their combination \nenhances the model’s effectiveness on the task.\nParameter analysis\nParameter selection and analysis were employed to further evaluate our proposed method. This paper analyzes \nthe impact of two parameters on the experimental results. (1) EEG Segment Length: The length of different EEG \nsegments contains varying amounts of information, thereby influencing the accuracy of emotion detection (2) \nLearning Rate and Number of Iterations: These parameters affect the convergence of the model.\nAs shown in Fig.  8, we illustrate the relationship between the length of the EEG segments and detection \naccuracy. The yellow line shows the variation of detection accuracy with different EEG segment length on the \nDEAP dataset, which reveals a tendency for accuracy to decrease as the segment length increases. The best \nperformance is observed at a length of 2 s, which can achieve an accuracy of 99.66% and a standard deviation \nof 0.15. The blue line shows the variation of detection accuracy with different EEG segment length on the \nSEED dataset, which demonstrates that the detection accuracy fluctuates with the time slice size. The best \nperformance is observed when the segment length is T = 17 s, which can reach an accuracy (ACC) of 98.85% \nand a standard deviation (STD) of 0.81. The green line shows the variation of detection accuracy with different \nFig. 8. Performances (average ACC ± STD %) of segment length on DEAP , SEED and SEED_IV .\n \nDataset Model ACC/% P-value\nDEAP\nNo GLU 98.04 ± 0.22 3e−2\nNo TCN 94.34 ± 0.14 2e−3\nOurs 99.66 ± 0.15 –\nSEED\nNo GLU 87.34 ± 0.43 4e−5\nNo TCN 84.23 ± 1.73 3e−5\nOurs 98.85 ± 0.81 –\nSEED_IV\nNo GLU 97.22 ± 0.32 2e−2\nNo TCN 95.23 ± 1.21 1e−3\nOurs 99.67 ± 0.12 –\nTable 5. The performance with different model.\n \nScientific Reports |        (2024) 14:31319 13| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nEEG segment lengths on the SEED_IV dataset. The recognition accuracy reaches more than 90% at segment \nlength of8,11] and [15,17, and a segment length of T = 17 s delivers the highest accuracy, with an average accuracy \n(ACC) of 99.67% and a standard deviation (STD) of 0.12. The experimental results highlight the importance of \nselecting the appropriate segment length for emotion recognition. It is crucial to stay within the optimal range, \nas exceeding it can lead to a significant reduction in classification accuracy.\nThe Fig.  9 illustrates the loss curve in relation to the learning rate. Figure  9a indicates that training on the \nDEAP dataset converges at approximately 300 epochs, when loss curves close to the convergence point, lr-\n0.00001 had the most accurate performance. Figure 9b showed that the loss curves on the SEED dataset converge \nsuccessfully after about 200 epochs of training, lr-0.0001 was the best performance. Figure  9c showed that the \ncurves on the SEED_IV dataset converge successfully after 200 epochs of training, lr-0.0001 had the highest \naccuracy. All the loss curves of model tend to be gentle in the later training period and only fluctuate across a \nsmall range convergence at different learning rates.\nInterpretability of the model\nSHAP (Shapley Additive Explanations) is a method for interpreting complex deep learning models by assigning \nan importance value to each feature for a given prediction. These values help identify the contribution of \nindividual features to the model’s predictions. Figure 10 illustrates the significance of the top 20 most influential \nfeatures (channels) to the model across the three datasets. The figure highlights that the regions contributing to \nemotion detection include FP1, FP2, F3, F4, F7, and F8. These regions belong to the frontal area of the brain, \nwhich is responsible for cognitive and emotional regulation 47,48. This alignment indicates that the proposed \nemotion detection model corresponds well with the brain’s known working mechanisms, further supporting the \nmodel’s validity and effectiveness.\nIn summary, the experimental results demonstrate that our proposed method achieves high accuracy and \nefficiency in emotion recognition. Specifically, the method delivers satisfactory results across different datasets, \nindicating better adaptability to various EEG acquisition devices. However, our work has two limitations: (1) The \nmodel’s training and testing are conducted on a server, which limits its practicality for real-world applications. \n(2) The model relies on large training datasets and faces challenges in generalizing to data from different EEG \ndevices, potentially hindering its applicability in scenarios with limited data availability. To address these issues, \nfuture work will explore strategies such as transfer learning49, domain adaptation, and model lightweighting to \nenhance the method’s applicability and efficiency.\nConclusion\nIn this paper, a novel End-to-End Emotion Recognition Method from EEG Signals Using Multi-Scale Dynamic \nCNN and Gated Transformer Encoder is proposed. First, Multi-Scale Dynamic CNN is used to extract the \ntemporal-spatial features from raw EEG signals, which mitigates the information loss caused by time-frequency \nconversion but also greatly enhances the detection performance. Secondly, the gated transformer encoder is \nutilized to capture global dependencies of EEG signals, thereby improving prediction and generalization \ncapabilities and reduces computational demands through parallel processing of multiple self-attention \nmechanisms. Then, the temporal convolution network is used to extract temporal features available for EEG \nsignals. Finally, SoftMax is utilized for emotion state classification. The experimental results show satisfactory \nperformance on three public datasets (DEAP , SEED, SEED_IV). The proposed approach holds great potential \nfor diverse applications within brain-computer interface (BCI) systems. In the future work, we will focus on \nutilizing the proposed model for in-depth EEG signal analysis and its application in disease diagnosis and \nmental health detection.\nScientific Reports |        (2024) 14:31319 14| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nFig. 9. The convergence at different learning rates.\n \nScientific Reports |        (2024) 14:31319 15| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nFig. 10. SHAP value for different categories across three datasets.\n \nScientific Reports |        (2024) 14:31319 16| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nData availability\nThe dataset is shared open source. Availability Link:  h t t    p  s :  / /  w  w  w .  e e c s  . q  m  u l .  a c . u k / m m v / d a t a s e t s / d e a p / d o w n l o a \nd . h t m l     . https://bcmi.sjtu.edu.cn/seed/seed.html.\nReceived: 8 August 2024; Accepted: 9 December 2024\nReferences\n 1. Houssein, E. H., Hammad, A. S. & Ali, A. A. Human emotion recognition from EEG-based brain–computer interface using \nmachine learning: a comprehensive review. Neural Comput. Appl. 34, 12527–12557 (2022).\n 2. Cui, Y ., Xu, Y . & Wu, D. EEG-Based driver drowsiness estimation using feature weighted episodic training. IEEE Trans. Neural Syst. \nRehabil. Eng. 27, 2263–2273 (2019).\n 3. Barrett, L. F ., Adolphs, R., Marsella, S., Martinez, A. & Pollak, S. D. Emotional expressions reconsidered: challenges to inferring \nemotion from human facial movements. Psychol. Sci. Public. Interest. 20, 1–68 (2019).\n 4. Zhang, Z., Wu, B. & Schuller, B. Attention-augmented end-to-end multi-task learning for emotion prediction from speech. In \nICASSP 2019–2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 6705–6709 (2019).\n 5. Zhao, J., Mao, X. & Chen, L. Speech emotion recognition using deep 1D & 2D CNN LSTM networks. Biomed. Signal. Process. \nControl. 47, 312–323 (2019).\n 6. Noroozi, F . et al. Survey on emotional body gesture recognition. IEEE Trans. Affect. Comput. 12, 505–523 (2018).\n 7. Kim, D. et al. Deep learning application to clinical decision support system in sleep stage classification. J. Personal. Med.. 12 (2022).\n 8. Wang, Z., Wang, Y ., Hu, C., Yin, Z. & Song, Y . Transformers for EEG-based emotion recognition: a hierarchical spatial information \nlearning model. IEEE Sens. J. 22, 4359–4368 (2022).\n 9. Araújo, T., Teixeira, J. P . & Rodrigues, P . M. Smart-data-driven system for alzheimer disease detection through \nelectroencephalographic signals. Bioengineering. 9, (2022).\n 10. Zheng, W ., Zhu, J. & Lu, B. Identifying stable patterns over time for emotion recognition from EEG. IEEE Trans. Affect. Comput. \n10, 417–429 (2016).\n 11. Huang, D., Guan, C., Ang, K. K., Zhang, H. & Pan, Y . Asymmetric spatial pattern for EEG-based emotion detection. In The 2012 \nInternational Joint Conference on Neural Networks (IJCNN), 1–7 (2012).\n 12. Farooq, F . et al. Motor imagery based multivariate EEG signal classification for brain controlled interface applications. In 2019 7th \nInternational Conference on Mechatronics Engineering (ICOM), 1–6 (2019).\n 13. Hosseini, M., Hosseini, A. & Ahi, K. A review on machine learning for EEG signal processing in bioengineering. IEEE Rev. Biomed. \nEng. 14, 204–218 (2020).\n 14. Liu, S. et al. Study on an effective cross-stimulus emotion recognition model using EEGs based on feature selection and support \nvector machine. Int. J. Mach. Learn. Cybernet. 9, 721–726 (2016).\n 15. Zhang, J., Yin, Z., Chen, P . & Nichele, S. Emotion recognition using multi-modal data and machine learning techniques: a tutorial \nand review. Inf. Fusion. 59, 103–126 (2020).\n 16. Cai, J., Xiao, R., Cui, W ., Zhang, S. & Liu, G. Application of Electroencephalography-based machine learning in emotion \nrecognition: a review. Front. Syst. Neurosci., 15 (2021).\n 17. Du, Y . & Liu, J. IENet: a robust convolutional neural network for EEG based brain-computer interfaces. J. Neural Eng. 19 (2022).\n 18. Khan, M. S. et al. CNN-XGBoost fusion-based affective state recognition using EEG spectrogram image analysis. Sci. Rep. 12, \n14122 (2022).\n 19. Zahid, M. U. et al. Robust R-Peak detection in low-quality holter ECGs using 1D convolutional neural network. IEEE Trans. \nBiomed. Eng. 69, 119–128 (2020).\n 20. Asadzadeh, S., Rezaii, Y . & Beheshti, T. Accurate emotion recognition using bayesian model based EEG sources as dynamic graph \nconvolutional neural network nodes. Sci. Rep. 12, 10282 (2022).\nFigure 10. (continued)\n \nScientific Reports |        (2024) 14:31319 17| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\n 21. Wang, Y . et al. Arrhythmia classification algorithm based on multi-head self-attention mechanism. Biomed. Signal. Process. Control. \n79, 104206 (2023).\n 22. Pamungkas, Y ., Wibawa, A. D. & Rais, Y . Classification of emotions (positive-negative) based on EEG statistical features using \nRNN, LSTM, and Bi-LSTM algorithms. In 2022 2nd International Seminar on Machine Learning, Optimization, and Data Science \n(ISMODE), 275–280 (2022).\n 23. Chen, X., Teng, X., Chen, H. S., Pan, Y . & Geyer, P . Toward reliable signals decoding for electroencephalogram: a benchmark study \nto EEGNeX. ArXiv, 2022. abs/2207.12369.\n 24. Vaswani, A. et al. Attention is all you need. arXiv (2017).\n 25. Belinkov, Y . & Glass, J. R. Analysis methods in neural language processing: a survey. Trans. Assoc. Comput. Linguist.. 7, 49–72 \n(2018).\n 26. Arjun, A., Rajpoot, A. S. & Raveendranatha Panicker, M. Introducing attention mechanism for EEG signals: emotion recognition \nwith vision transformers. In 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), \n5723–5726 (2021).\n 27. Xie, J. et al. A transformer-based approach combining deep learning network and spatial-temporal information for raw EEG \nclassification. IEEE Trans. Neural Syst. Rehabil. Eng. 30, 2126–2136 (2022).\n 28. Akhand, M. A. H. et al. Improved EEG-based emotion recognition through information enhancement in connectivity feature map. \nSci. Rep. 13, 13804 (2023).\n 29. Alarcão, S. M. & Fonseca, M. J. Emotions recognition using EEG signals: a survey. IEEE Trans. Affect. Comput. 10, 374–393 (2019).\n 30. Wang, S., Li, B. Z., Khabsa, M., Fang, H. & Ma, H. Linformer: self-attention with linear complexity. ArXiv, abs/2006.04768 (2020).\n 31. Dauphin, Y ., Fan, A., Auli, M. & Grangier, D. Language modeling with gated convolutional networks. In International Conference \non Machine Learning (2016).\n 32. Bai, S., Kolter, J. Z., & Koltun, V . An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. \nArXiv abs/1803.01271 (2018).\n 33. Koelstra, S. et al. DEAP: a database for emotion analysis;using physiological signals. IEEE Trans. Affect. Comput. 3, 18–31 (2012).\n 34. Zheng, W . L. & Lu, B. L. Investigating critical frequency bands and channels for EEG-Based emotion recognition with deep neural \nnetworks. IEEE Trans. Auton. Ment. Dev. 7, 162–175 (2015).\n 35. Zheng, W ., Liu, W ., Lu, Y ., Lu, B. & Cichocki, A. EmotionMeter: a multimodal framework for recognizing human emotions. IEEE \nTrans. Cybern.. 49, 1110–1122 (2019).\n 36. Jin, M., Zhu, E., Du, C., He, H. & Li, J. P . G. C. N. Pyramidal graph convolutional network for EEG emotion recognition. IEEE \nTrans. Multimed.. 26, 9070–9082 (2023).\n 37. Xu, B., Shen, H., Cao, Q., Qiu, Y . & Cheng, X.  Graph wavelet neural network. ArXiv, abs/1904.07785 (2019).\n 38. Zhang, Z., Zhong, S. & Liu, Y . G. A. N. S. E. R. A self-supervised data augmentation framework for EEG-based emotion recognition. \nIEEE Trans. Affect. Comput. 14, 2048–2063 (2021).\n 39. Liu, Y . et al. Real-time movie-induced discrete emotion recognition from EEG signals. IEEE Trans. Affect. Comput. 9, 550–562 \n(2018).\n 40. Zhang, H. Expression-EEG based collaborative multimodal emotion recognition using deep autoencoder. IEEE Access. 8, 164130–\n164143 (2020).\n 41. Islam, M. R. & Ahmad, M. Virtual image from EEG to recognize appropriate emotion using convolutional neural network.  In 1st \nInternational Conference on Advances in Science, Engineering and Robotics Technology (ICASERT), 1–4 (2019).\n 42. Hussain, M., Qazi, E., Aboalsamh, H. & Ullah, I. Emotion recognition system based on two-level ensemble of deep-convolutional \nneural network models. IEEE Access. 11, 16875–16895 (2023).\n 43. Li, Y . et al. Graph-based multi-task self-supervised learning for EEG emotion recognition. IEEE Trans. Affect. Comput. 14, 2512–\n2525 (2022).\n 44. Song, T., Zheng, W ., Song, P . & Cui, Z. EEG emotion recognition using dynamical graph convolutional neural networks. IEEE \nTrans. Affect. Comput. 11, 532–541 (2020).\n 45. Zhong, P ., Wang, D. & Miao, C. EEG-based emotion recognition using regularized graph neural networks. IEEE Trans. Affect. \nComput. 13, 1290–1301 (2019).\n 46. Dar, M. N. et al. Insights from EEG analysis of evoked memory recalls using deep learning for emotion charting. Sci. Rep. 14, 17080 \n(2024).\n 47. Malezieux, M., Klein, A. S. & Gogolla, N. Neural Circuits Emot. Annu. Rev. Neurosci. 46, 211–231 (2023).\n 48. Nejati, V . et al. The role of prefrontal cortex and temporoparietal junction in interpersonal comfort and emotional approach. Sci. \nRep. 13, 21636 (2023).\n 49. Li, W . et al. Can emotion be transferred?—A review on transfer learning for EEG-based emotion recognition. IEEE Trans. Cogn. \nDev. Syst. 14, 833–846 (2021).\nAuthor contributions\nZ.C. and X.K., data processing, experiments and evaluation, manuscript writing, Q.W ., T.Y ., J.T., supervision, \nconceptualization of the methodology, evaluation of the results, original manuscript and review. The authors \nconfirm the absence of any participants in the article.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to J.T.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nScientific Reports |        (2024) 14:31319 18| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2024 \nScientific Reports |        (2024) 14:31319 19| https://doi.org/10.1038/s41598-024-82705-z\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6805869936943054
    },
    {
      "name": "Electroencephalography",
      "score": 0.6697494983673096
    },
    {
      "name": "Speech recognition",
      "score": 0.48622074723243713
    },
    {
      "name": "Transformer",
      "score": 0.4549798369407654
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45335710048675537
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40801945328712463
    },
    {
      "name": "Neuroscience",
      "score": 0.2707318663597107
    },
    {
      "name": "Psychology",
      "score": 0.12265235185623169
    },
    {
      "name": "Electrical engineering",
      "score": 0.08779671788215637
    },
    {
      "name": "Engineering",
      "score": 0.059218525886535645
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177739611",
      "name": "Yangtze University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I12393601",
      "name": "Huaihua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210166515",
      "name": "First People's Hospital of Jingzhou",
      "country": "CN"
    }
  ]
}