{
  "title": "Large Language Models are Built-in Autoregressive Search Engines",
  "url": "https://openalex.org/W4385570369",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3159583917",
      "name": "Noah Ziems",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A2124365211",
      "name": "Wenhao Yu",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A2104410935",
      "name": "Zhihan Zhang",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A2098004101",
      "name": "Meng Jiang",
      "affiliations": [
        "University of Notre Dame"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297899309",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W4386566654",
    "https://openalex.org/W3168875417",
    "https://openalex.org/W4224438163",
    "https://openalex.org/W4221166196",
    "https://openalex.org/W3199493219",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4297162632",
    "https://openalex.org/W3091432621",
    "https://openalex.org/W4320465836",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W3154690330",
    "https://openalex.org/W4385572921",
    "https://openalex.org/W3175515348",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W4385859558",
    "https://openalex.org/W3092288641",
    "https://openalex.org/W3197499505",
    "https://openalex.org/W4309417034",
    "https://openalex.org/W4385574243",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4389921502",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W3034439313",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2066636486"
  ],
  "abstract": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases.In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval.Surprisingly, when providing a few Query-URL pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90% of the corresponding documents contain correct answers to open-domain questions.In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers.Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings.The code for this work can be found at https://github.com/Ziems/llm-url.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 2666–2678\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models are Built-in Autoregressive Search Engines\nNoah Ziems, Wenhao Yu, Zhihan Zhang, Meng Jiang\nUniversity of Notre Dame\n{nziems2, wyu1, zzhang23, mjiang2}@nd.edu\nAbstract\nDocument retrieval is a key stage of standard\nWeb search engines. Existing dual-encoder\ndense retrievers obtain representations for ques-\ntions and documents independently, allowing\nfor only shallow interactions between them. To\novercome this limitation, recent autoregressive\nsearch engines replace the dual-encoder archi-\ntecture by directly generating identifiers for rel-\nevant documents in the candidate pool. How-\never, the training cost of such autoregressive\nsearch engines rises sharply as the number of\ncandidate documents increases. In this paper,\nwe find that large language models (LLMs) can\nfollow human instructions to directly generate\nURLs for document retrieval.\nSurprisingly, when providing a few Query-URL\npairs as in-context demonstrations, LLMs can\ngenerate Web URLs where nearly 90% of the\ncorresponding documents contain correct an-\nswers to open-domain questions. In this way,\nLLMs can be thought of as built-in search\nengines, since they have not been explicitly\ntrained to map questions to document identi-\nfiers. Experiments demonstrate that our method\ncan consistently achieve better retrieval perfor-\nmance than existing retrieval approaches by a\nsignificant margin on three open-domain ques-\ntion answering benchmarks, under both zero\nand few-shot settings. The code for this work\ncan be found athttps://github.com/Ziems/\nllm-url.\n1 Introduction\nAlong with the success of deep learning, dual-\nencoder based retrievers have become the domi-\nnant method for Web searching (Zhu et al., 2021;\nZhao et al., 2022). For example, DPR (Karpukhin\net al., 2020) employs two independent encoders to\nencode the question and the document respectively,\nthen estimates their relevance by computing a sin-\ngle similarity score between two representations.\nada-001babbage-001curie-001davinci-001davinci-002/003\n0%20%40%60%80%100%\n0.11 101001000\nURL Match\n# Parameters (Billions)\nFigure 1: Successful URL reconstructions by differ-\nent size of GPT-3 as URL generators. The models are\nprompted with the first 100 words of a Wikipedia page\nthen tasked with generating the URL of the page they\ncame from. Tested on 10k Wikipedia pages sampled\nfrom the top 100k most frequent Wikipedia entities.\nHowever, these methods suffer from two ma-\njor drawbacks. First, the representations of ques-\ntions and documents are typically obtained inde-\npendently in modern dual-encoder dense retrieval\nmodels (Karpukhin et al., 2020), allowing for only\nshallow interactions between them (Khattab et al.,\n2021). Second, the question or document repre-\nsentation is embedded into a single dense vector,\npotentially missing fine-grained information when\ncomputing the similarity between the two vector\nrepresentations (Khattab and Zaharia, 2020).\nInstead of computing similarity between ques-\ntion and document embeddings, autoregressive\nsearch engines aim to directly generate document\nidentifiers then map them to complete documents\nin the predetermined candidate pool. This approach\nhas attracted increasing interest in information re-\ntrieval (IR) and related fields (Tay et al., 2022;\nBevilacqua et al., 2022; Wang et al., 2022). Com-\npared to dual-encoder dense retrieval methods, au-\ntoregressive search engines enjoy a number of ad-\nvantages. First, autoregressive generation models\nproduce document identifiers by performing deep\ntoken-level cross-attention, resulting in a better esti-\n2666\nmation than shallow interactions in dense retrievers.\nSecond, autoregressive search engines have been\nshown to have strong generalization abilities, out-\nperforming BM25 in a zero-shot setting (Tay et al.,\n2022). While it is theoretically possible to scale an\nautoregressive search engine to the size of a large\nlanguage model (LLM), such as GPT-3 with 175B\nparameters, in practice it is not feasible due to the\ncomputational overhead of training such a large au-\ntoregressive search engine from scratch (Tay et al.,\n2022). To reduce the high training cost of autore-\ngressive search engine, a smaller model size is pre-\nferred. However, the results of our pilot study in\nFigure 1 show smaller language models are sig-\nnificantly worse at mapping passages to document\nidentifiers than larger ones. Moreover, different\nretrieval tasks can have unique retrieval require-\nments. One task may require a model to retrieve\nfactual evidence to support or refute a claim (i.e.,\nfact checking) (Onoe et al., 2021) while another\nmay require a model to retrieve specific trivia infor-\nmation about an entity (i.e., entity linking) (Petroni\net al., 2021; Zhang et al., 2022). It would be better\nif the retriever was capable of generalizing to new\nretrieval tasks with only a few examples.\nIn this work, we explore the use of in-context\ndemonstrations to prompt LLMs to directly gen-\nerate web URLs for document retrieval, namely\nLLM-URL . Surprisingly, we find that by provid-\ning a few (query, URL) pairs as contextual demon-\nstrations, large language models (e.g. GPT-3) gen-\nerate Web URLs where nearly 90% of the cor-\nresponding documents contain answers to open-\ndomain questions. In this way, LLMs can be\nthought of as built-in search engines, as they have\nnot been explicitly trained to map questions or\ndocuments to identifiers. Instead of using newly-\ncreated document identifiers, LLM-URL leverages\nexisting and widely used document identifiers di-\nrectly, i.e., URLs. We compare our approach to\nexisting document retrieval methods on three differ-\nent open-domain question answering (QA) datasets:\nWebQ (Berant et al., 2013), NQ (Kwiatkowski\net al., 2019), and TriviaQA (Joshi et al., 2017).\nFurther, to avoid exceeding the limit on the number\nof input tokens of LLMs, we employ an unsuper-\nvised passage filtering module to remove irrelevant\nportions of supporting documents. To summarize,\nour main contributions are as follows:\n1. We reveal that LLMs are built-in autoregres-\nsive search engines capable of document re-\ntrieval by directly generating Web page URLs\nunder both zero and few-shot settings.\n2. We show retrieving documents by generating\nURLs with LLMs significantly outperforms\nexisting methods for document retrieval, as\nmeasured by Recall@K. Further, we show\nthat breaking the retrieved documents into pas-\nsages then using a ranker to filter the passages\nsignificantly reduces the number of support-\ning passages while maintaining high recall.\n3. We show the retrieved documents improve\ndownstream QA performance as measured by\nEM when compared to baseline methods.\n2 Related Work\n2.1 Traditional Document Retrievers\nTraditional methods such as TF-IDF and BM25\nexplore sparse retrieval strategies by matching the\noverlapping contents between questions and pas-\nsages (Robertson and Zaragoza, 2009; Chen et al.,\n2017; Yang et al., 2019). DPR (Karpukhin et al.,\n2020) revolutionized the field by utilizing dense\ncontextualized vectors for passage indexing. It\nis first initialized as a pretrained BERT model,\nthen trained discriminatively using pairs of queries\nand relevant documents, with hard negatives from\nBM25. Recent research has improved DPR via\nbetter training strategies (Xiong et al., 2020; Qu\net al., 2021; Zhang et al., 2023a) and passage re-\nranking (Mao et al., 2021; Yu et al., 2021; Ju\net al., 2022). However, representations of ques-\ntions and documents are typically obtained inde-\npendently in modern dual-encoder dense retrieval\nmodels (Karpukhin et al., 2020; Xiong et al., 2020),\nallowing for only shallow interactions between\nthem (Khattab et al., 2021).\n2.2 Autoregressive Search Engines\nRecent works have investigated the use of auto-\nregressive language models to generate identifier\nstrings for documents as an intermediate target for\nretrieval (Yu et al., 2022), such as Wikipedia page\ntitles (De Cao et al., 2020), root-to-leaf paths in\na hierarchical cluster tree (Tay et al., 2022), or\ndistinctive n-grams that can be mapped to full pas-\nsages (Bevilacqua et al., 2022). Since the series\nof work was carried out almost simultaneously by\ndifferent research groups, they are often referred\nto multiple different names in the literature, such\nas autoregressive search engine, differential search\n2667\nDocument:The club was founded in Baltimore, Maryland in 1901, and moved to New York in1903.From1923to…\nAnswer: 1903Query: When did the Yankees move to New York? Which 3 URLs would have the answer?\nLarge Language Model\nPassageReader\nURL::https://en.wikipedia.org/wiki/New_York_Yankees\nPassageRanker\nFigure 2: The overall pipeline of our proposed LLM-URL . Given a question, LLM-URL first generates a set\nof URLs which are extracted from the generated text. The URLs are retrieved from the Internet then broken into\npassages which are ranked and filtered such that only the most relevant are kept. Finally, these passages are given as\ninput to a reader model along with the original question to generate a final answer.\nindex (DSI), and neural document indexers (NDI).\nCompared to traditional dense document retrievers,\nthese methods leverage a generation model to pro-\nduce the document indexes. By forcing the genera-\ntion model to explain every token in the question\nand document using cross-attention, the genera-\ntion abilities of the model significantly improve.\nOur work is closely related to these works, show-\ning experimentally that properly prompting pre-\ntrained large language models can achieve better\nperformance than traditional dense retrieval mod-\nels (Ouyang et al., 2022; Yu et al., 2023) .\n3 Proposed Method\nIn this section we describe a new method, which\nwe refer to as LLM-URL , that employs a large\nlanguage model (LLM) to perform effective and\nefficient web document retrieval for knowledge-\nintensive NLP tasks such as open-domain question\nanswering (ODQA).\nODQA is a two step process consisting of a re-\ntriever and a reader. Given a question q, the goal\nof the retriever is to find the top- n passages Pn\nrelevant to answering q. Given q and the top- n\nrelevant passages Pn, the goal of the reader is to\nuse internal knowledge along with Pn to generate\na correct answer a to question q. The passage re-\ntriever plays an essential role in this process. When\nPn contains more passages that have the correct\nanswer, the reader has a higher chance of finding\nit. Instead of heavily training a dedicated retriever,\nour LLM-URL solves the problem in a different\nway as shown in Figure 2.\nGiven a question q, our LLM-URL should find\na set of relevant passages to Pn and give it to\nthe reader. First, it prompts a LLM (e.g., GPT-\n3) to directly generate m URLs for q. By de-\nfault, it uses “Which m Wikipedia URLs would\nhave the answer?” as the instruction which is ap-\npended to each input question as the prompt. We\nalso append the beginning of the Wikipedia URL\n(https://en.wikipedia.org/wiki) to the end\nof the prompt to encourage the generation of URLs\nand restrict generation to the Wikipedia article\nURL format. As LLM has the ability of in-context\nlearning, we take this advantage to enable the few-\nshot setting in the prompt. The prompt described\nabove also includes a series of in-context demon-\nstrations. Each demonstration contains a question\nsampled from the training set following the prompt\ndescribed above. At the end of each demonstration,\nm URLs which point to gold-labeled documents\nare listed. In the zero-shot setting, the original\nprompt is used without any demonstrations. In the\nfew-shot setting, the original prompt appended to a\nseries of d demonstrations (d=10 in this work).\nGiven the prompt, the LLM returns a generated\nsequence of tokens. Ideally these tokens would con-\nstruct a sequence ofm separated URLs. In practice,\nthe generated sequence often has extra information\nsuch as a proposed answer that is unreliable and\nneeds to be filtered. We use a regular expression to\nextract all URLs from the sequence and discard all\n2668\n60657075808590\n12345678910\nRecall (%)\nNumber of Generated URLs: mWebQTQANQ\nFigure 3: We prompt LLM-URL to generate m docu-\nments and measure the recall asm increases. Significant\nrecall improvements are seen when m is small but as it\nincreases the marginal benefit decreases.\nextra information. This also filters out many URLs\nthat are improperly formatted. After extraction,\nGET requests are made using the extracted URLs\nand the contents of each retrieval is used to create\na set of fetched documents Df . Often, |Df | < m\nbecause some of the generated URLs do not follow\na correct format or do not point to real web pages\non the Internet.\nThe set of fetched documents Df can be passed\ndirectly to a reader if m is a small value or the\nreader being used can handle many large docu-\nments. However, this is usually not the case. Often,\nDf needs to be filtered such that only a small num-\nber of the most relevant passages are given to the\nreader. To do this, ourLLM-URL first breaks each\ndocument d ∈ Df into a set of small passages. The\npassages from each document are collected into a\nnew set, Pf . A scoring function is used to quantify\nthe relevance of each passage with respect to the\nquestion q, with high values indicating high rele-\nvance with respect to q and low scores indicating\nlow relevance. A simple scoring function such as\nBM25 can be used or a more complex one such as\nDPR (Karpukhin et al., 2020) can. The passages\nin Pf are then sorted from highest to lowest and\nthe top n are kept as Pn. Finally, Pn are given to a\nreader along with q to generate an answer.\nAdvantages of LLM-URL : Existing autore-\ngressive retrieval methods such as DSI and SEAL\nuse a pre-trained large language model then fine\ntune it to take questions as input and generate rel-\nevant document identifiers as output (Tay et al.,\n2022; Bevilacqua et al., 2022). Both DSI and\nSEAL do extensive experiments on a variety of doc-\nument identifiers which are generated by a heav-\nily trained language model. Examples of these\nidentifiers include unstructured atomic identifiers,\nnaively structured string identifiers, hierarchical\ndocument clustering, and others. LLM-URL in-\nstead uses pre-existing document identifiers that\nexist on the internet: URLs. Using URLs instead\nof the aforementioned identifiers has multiple ad-\nvantages. URLs often contain words related to\nthe information they link to, allowing for strong\nassociation of topics with their URLs. For exam-\nple, the title of each Wikipedia page is used in its\nURL, allowing the LLM is able to directly generate\nthe URL by leveraging semantic information from\nthe question. To validate the importance of URLs\nthemselves, we also experiment with prompting the\nLLM to generate Wikipedia titles instead of URLs\nand find Recall@1 significantly reduces compared\nto prompting for URL generation. We believe this\nis because the URL format itself helps prompt the\nmodel for specific information in a specific format.\nFurther, the use of URLs allows us to simply ob-\ntain the evidence document via a HTTP request\nwithout any need of training a model or building an\nindex to find the mapping between identifiers and\ndocuments.\n4 Experiments\nIn this section, we present and discuss results from\nour experiments to “directly” demonstrate that our\nLLM-URL is a strong retriever and “indirectly”\nshow that it achieves competitive performance on\nthe ODQA task against state-of-the-art solutions.\nLarge Language Model: Following Figure 1,\nthe large language model we use to generate URLs\nfor our experiments is GPT-3text-davinci-003 with\ngreedy decoding and a temperature of 0. A variety\nof different prompts are tested for generating URLs,\nbut little difference in performance is observed, so\nwe simply use the best performing prompt which\nis discussed in Section 3.\nDatasets: We use three ODQA datasets including\nWeb Questions, Natural Questions, and Trivia QA.\nWe use them to perform evaluation on both the task\nof document or passage retrieval and ODQA itself.\n4.1 Retrieval\nWe expect retrievers to find the most relevant doc-\numents and/or passages. We conduct experiments\non both document retrieval and passage retrieval.\nEvaluation metrics. Recall@k (k=1, 10, 100) is\ncalculated by measuring the percentage of docu-\n2669\nMethod Document Recall@1 Document Recall@10\nWebQ NQ TriviaQA WebQ NQ TriviaQA\nContriever (Izacard et al., 2021) 63.8 53.2 60.6 63.8 80.8 82.5\nBM25 (Robertson and Zaragoza, 2009)49.5 47.2 63.0 81.5 76.8 82.3\nGoogle API 61.1 55.5 51.4 - - -\nLLM-URL (Zero-Shot) 76.8 61.7 71.3 87.7 83.2 85.5\nLLM-URL (Few-Shot) 79.7 62.6 73.5 89.9 83.9 86.8\nTable 1: Document retrieval as measured by Recall@k. Google API Recall@10 results are left out due to high cost.\nMethod Passage Recall@1 Passage Recall@10 Passage Recall@100\nWebQ NQ TriviaQA WebQ NQ TriviaQA WebQ NQ TriviaQA\nContriever 18.2 18.8 34.0 55.7 54.8 67.9 79.8 79.6 83.3\nBM25 19.1 22.8 46.2 51.8 55.6 71.7 76.6 79.6 83.9\nLLM-URL (Zero-Shot) 22.2 24.0 46.7 63.1 60.6 76.6 83.8 78.3 83.6\nLLM-URL (Few-Shot) 22.3 25.5 49.1 64.8 60.8 77.8 85.9 79.0 84.8\nTable 2: Passage retrieval as measured by Recall@1, Recall@10 and Recall@100. Here LLM-URL is equipped\nwith BM25 to perform the ranking task.\nments or passages in the top-k which contain one of\nthe gold labeled answers while exact match is calcu-\nlated by the percentage of predicted answers which\nmatch one of the gold labeled answers. While\nLLM-URL is not constrained by which URLs can\nbe generated for document retrieval, we restrict\nall generations to Wikipedia URLs only for fair\ncomparison, as discussed in Section 3 All baseline\nmodels also use Wikipedia for retrieval, with some\nfetching documents in real time and others fetching\nfrom an offline corpus.\n4.1.1 Document Retrieval\nBaselines: Contriever (Izacard et al., 2021) and\nBM25 (Robertson and Zaragoza, 2009) are usu-\nally used for passage retrieval. Contriever is a\ndual encoder which uses a dot product between\ndense representations of a question and passage\nto calculate relevance. BM25 is a sparse retriever\nwhich uses the overlapping contents between ques-\ntion and passage to calculate relevance. Because\nwe use the same passage size to chunk Wikipedia\ndocuments, we were able to map their retrieved\npassages back to the original documents. We use\nGoogle API (Brin and Page, 1998) restricted to\nWikipedia as a third baseline to retrieve relevant\ndocuments given a question.\nExisting works such as DSI and SEAL have in-\nvestigated the use of autoregressive language mod-\nels to generate identifier strings for documents as\nan intermediate target for retrieval. DSI is a Trans-\n50%60%70%80%90%100%\n12345678910\nPercentage of Valid URLsNumber of Generated URLs: m\nWebQTQANQ> 68%\nFigure 4: The percentage of valid URLs generated from\nLLM-URL as the total number of generated URLs m\nincreases from 1 to 10. As m increases, invalid URL\ngenerations become more frequent.\nformer which has been trained to map directly\nfrom question to document identifiers by memo-\nrizing the contents of the entire corpus (Tay et al.,\n2022). SEAL is a variant of DSI which uses ngrams\nas document ids to improve retrieval performance\n(Bevilacqua et al., 2022). Neither DSI nor SEAL\nreport retrieval results on full documents and do\nnot have publicly available implementations, so\nthey are left out and discussed in Table 3 and Sec-\ntion 4.1.2 on passage retrieval.\nUnlike the baselines, our LLM-URL employs\nan LLM. It has two settings: zero-shot and few-\nshot. In the zero-shot setting, no in-context demon-\nstrations are given whereas in the few-shot setting\na few demonstrations are appended to the prompt.\n2670\nMethod Recall@1 Recall@10\nDSI1 25.1 56.6\nSEAL1 26.3 74.5\nLLM-URL (Zero-Shot) 24.0 60.6\nLLM-URL (Few-Shot) 25.5 60.8\n1explicitly trained for retrieval on NQ\nTable 3: Passage retrieval as measured by Recall@1 and\nRecall@10. LLM-URL is equipped with BM25 for\npassage ranking. Other datasets are left out due to not\nbeing reported in either paper and no public implemen-\ntations.\nResults: The results of our document retrieval\nexperiments are shown in Table 1. In this setting\nRecall@k is calculated directly after the documents\nare retrieved with no intermediary steps. LLM-\nURL significantly outperforms baseline methods\non all datasets for both Recall@1 and Recall@10.\nSpecifically, zero-shot LLM-URL improves doc-\nument Recall@1 relatively by 20.4%, 11.2%, and\n13.2% over the strongest baseline on WebQ, NQ,\nand TriviaQA, respectively. Few-shot LLM-URL\nfurther expands the improvement to 24.9%, 12.8%,\nand 16.7%, respectively. URLs can be extracted\nfrom the large-scale parameters of LLMs, and these\nURLs can lead to more accurate documents than\nwhat existing methods can retrieve. Both the LLM\nparameters and in-context demonstrations are sig-\nnificantly useful in document retrieval.\nFigure 3 shows Recall scores converge when\nthe number of generated URLs m increases. Due\nto the diminishing returns from increasing m, our\nexperiments do not explore values of m greater\nthan 10.\nAre the generated URLs valid?It is worth not-\ning that the generated URLs are not always valid.\nSome generated URLs do not have valid URL syn-\ntax and some point to Wikipedia pages that do not\nexist. Rarely, URLs will be generated for domains\naside from Wikipedia. For fair comparison, all of\nthese faulty URLs are discarded and only docu-\nments coming from valid Wikipedia articles are\nkept.\nFurther analysis is done to measure the ratio of\nvalid Wikipedia URLs while the total number of\ngenerated URLs m increases from 1 to 10, shown\nin Figure 4. The number of valid URL generations\nremains surprisingly high (i.e., higher than 68%)\nas m increases from 1 to 10. However, the rate of\nvalid generations appears to fall off as m increases,\nMethod Zero-Shot QA EM\nWebQ NQ TriviaQA\nContriever + InstructGPT16.8 19.1 52.4\nBM25 + InstructGPT 16.0 20.5 53.3\nGoogle + InstructGPT 19.9 27.8 58.7\nGenRead (InstructGPT) 24.8 28.2 59.3\nDSI1 + FiD - 31.4 2 -\nSEAL1 + FiD - 43.6 41.8\nInstructGPT (no docs.) 18.6 20.9 52.6\nLLM-URL (Zero-Shot) 28.1 26.4 60.1\nLLM-URL (Few-Shot) 29.0 27.3 60.7\n1explicitly trained for retrieval on NQ\n2result from Bevilacqua et al. (2022)\nTable 4: Zero-shot open-domain QA performance as\nmeasured by exact match (EM). All LLM-URL models\nuse InstructGPT as the reader unless otherwise stated.\nindicating there are diminishing returns from each\nmarginal increase of m.\n4.1.2 Passage Retrieval\nBaselines: Four methods, including Contriever,\nBM25, DSI (Tay et al., 2022), and SEAL (Bevilac-\nqua et al., 2022), were introduced in Section 4.1.1.\nGoogle API was used for document retrieval and\nnot applied to passages.\nResults: The results of our passage retrieval\nexperiments are shown in Table 2. In this set-\nting Recall@k is calculated on the top-k passages\nranked by the ranker instead of just on the raw doc-\numents shown in Table 1. LLM-URL performs\nslightly better than baseline methods for Recall@1\nand Recall@10 and as well as baseline methods\nfor Recall@100. In the zero-shot setting, LLM-\nURL improves relative Recall@1 by 16.2%, 5.3%,\nand 1.1% with respect to the strongest baseline on\nWebQ, NQ, and TriviaA respectively. The few-shot\nsetting of LLM-URL expands the improvement\nto 16.8%, 11.8%, and 6.3%, respectively. For Re-\ncall@10, similar improvements can be seen.\nFor Recall@100, performance is better relative\nto baseline models for all datasets except NQ. In\nthe zero-shot setting, LLM-URL improves the rel-\native Recall@100 by 5.0% for WebQ and performs\nslightly worse than the best baseline method on NQ\nand TriviaQA by 1.7% and 0.4% respectively. The\nfew-shot setting of LLM-URL for Recall@100\nshows a slight improvement on WebQ and Trivi-\naQA, but performs slightly worse than the strongest\nbaseline on NQ.\n2671\nDespite being limited to only the passages from\n10 documents, LLM-URL performs better than\nbaseline methods for smaller k and performs as\nwell as baseline methods for higher values of k.\nThe comparison between LLM-URL and exist-\ning document identifier-based methods such as DSI\nand SEAL are shown in Table 3. For Recall@1,\nzero-shot LLM-URL performs slightly worse than\nthe best baseline by 8.8%. This performance gap is\nslightly smaller in the few-shot setting with LLM-\nURL performing 3.1% worse than the best baseline.\nFor Recall@10, zero-shot LLM-URL performs\nworse than the best baseline by 18.7%. Few-shot\nLLM-URL performs only slightly better than the\nzero-shot setting, performing worse than the best\nbaseline by 18.4%.\n4.2 Open-Domain Question Answering\nEvaluation metric: We use exact match (EM),\nwhich is short for exact string match with the cor-\nrect answer, because the goal of ODQA is to find\nan exact answer to any question using Wikipedia\narticles.\nResults: Here we discuss the downstream QA\nperformance of LLM-URL . In this setting, an an-\nswer only has an exact match if the normalized gen-\nerated text is within the list of acceptable answers to\na question. When combined with InstructGPT as a\nreader, LLM-URL performs significantly better on\nWebQ and slightly better on TriviaQA when com-\npared with the best performing baseline methods.\nOn NQ, LLM-URL +InstructGPT performs worse\nthan baseline NDIs and only slightly worse than\nthe best remaining baseline. In the zero-shot set-\nting, LLM-URL +InstructGPT improves upon the\nbest baseline method by 13.3% and 1.3% on WebQ\nand TriviaQA respectively. LLM-URL +Instruct-\nGPT performs worse than the best baseline method\nby 39.5% on NQ. In the few-shot setting, LLM-\nURL +InstructGPT performs better than the best\nbaseline method by 16.9% and 2.3% on WebQ and\nTriviaQA respectively. LLM-URL +InstructGPT\nperforms worse than the best baseline method by\n37.4% on NQ.\nDespite not being explicitly trained for retrieval,\nLLM-URL +InstructGPT performs significantly\nbetter than baseline methods for WebQ, achieves\non-par performance with existing methods for Triv-\niaQA, and performs slightly worse than existing\nmethods for NQ.\nOur results indicate LLM-URL could be a\n67.865 45.6\n4.813.819020406080100\nTQAWebQNQ\nRecall@1(%) CommonUncommon\nFigure 5: Common vs Uncommon entity recall@1.\nCommon is defined as a question containing entities\nin the top-1 million most common entities on Wikipedia.\nLLM-URL performs much better when retrieving in-\nformation about common entities.\npromising solution to retrieval for a wide range of\nknowledge intensive tasks with little to no training\ndata required.\n4.3 Discussions\n4.3.1 Time Sensitive Queries\nThere are a number of additional qualitative ben-\nefits that LLM-URL has over existing methods.\nOne large advantage of LLM-URL is that the doc-\numents are retrieved in real time from the source.\nSo long as the source stays up to date without the\nURL itself changing, our proposed method is ca-\npable of answering time sensitive queries without\nany extra modifications.\nIn contrast, existing dual encoder approaches\nsuch as Contriever require a document to be re-\nencoded each time it changes. Existing methods\nsuch as SEAL (Bevilacqua et al., 2022) and DSI\nare also tricky to keep up to date for time sensitive\nqueries as the LLM would have to be retrained to\nlearn the new content of the updated documents.\n4.3.2 Frequent Entities Analysis\nFollowing (Mallen et al., 2022), we analyze the re-\ntrieval performance of LLM-URL when the gold-\nlabeled answer entity is common versus when it\nis not. For each question-answer pair in a given\ndataset we check to see if the labeled entity ex-\nists within the top one-million common entities\nfrom Wikipedia. Using this, we split our dataset\ninto two distinct subsets: question-answer pairs\nthat contain a common entity and those that do\nnot. In measuring the performance of our model\non these two distinct sets across Web Questions,\nNatural Questions, and TriviaQA, we find LLM-\nURL performs significantly better on common en-\ntity question-answer pairs. The results of our analy-\nsis are shown in Figure 5. Across all three datasets,\n2672\nLLM-URL Exists Answer Contriever Answer BM25 Answer\nwiki/Jellyfish ✓ ✓ Smack (ship) ✗ Collective noun ✗\nwiki/Collective_noun ✓ ✗ Collective noun ✗ Determiner ✗\nwiki/Smack_(group) ✗ ✗ Cetacean intelligence ✗ Glass sea creatures ✗\nwiki/Cnidaria ✓ ✓ Well smack ✗ Minotaur ✗\nwiki/Medusozoa ✓ ✓ Plankton ✗ Mass noun ✗\nwiki/Scyphozoa ✓ ✓ Sperm whale ✗ Well smack ✗\nwiki/Cubozoa ✓ ✓ Loaded question ✗ Nomenclature ✗\nwiki/Hydrozoa ✓ ✓ Jabberwocky ✗ Archomental ✗\nwiki/Staurozoa ✓ ✓ Merrow ✗ Honey Smacks ✗\nwiki/Rhizostomeae ✓ ✓ Loaded question ✗ Well smack ✗\nTable 5: Case study of retrieved documents from the question “A ’smack’ is a collective noun for a group of which\nsea creatures?”. “Exists” means whether the URL points to a valid Wiki page. “Answer” means whether the\ndocument contains the answer. We omit the prefix of generated URLs for brevity (https://en.wikipedia.org/).\nFor BM25 and Contriever, we show the document titles of the top-10 retrieved passages, respectively. The correct\nanswer is “jellyfish.”\nthe recall of common-entity question-answer pairs\nis many times greater than the recall from the rest\nof the dataset.\nPrevious work has shown LLMs in the closed-\nbook setting, where the model must rely solely\non the information contained within its weights,\nperform much better on common-entities versus\nuncommon ones (Mallen et al., 2022). Our results\nshow this problem extends beyond the closed-book\nsetting and also applies to retrieval when using\nLLM-URL . This also could explain the high word\ncount from documents we found when evaluating\nLLM-URL . The average Wikipedia article is 644\nwords, but the average word count from Wikipedia\ndocuments retrieved via LLM-URL was 10k. We\nbelieve this discrepancy is caused by common en-\ntities having much more detail in their Wikipedia\narticles and in turn having much higher word count.\n4.3.3 Case Study\nIn Table 5, we show a case study comparing LLM-\nURL with two baseline retrieval methods, BM25\nand Contriever, on the question “A ‘smack’ is a\ncollective noun for a group of which sea creatures?”\nwhich is in the TriviaQA test set. The gold-labeled\nanswer to this question is “jellyfish”.\nIn the closed-book setting, InstructGPT mistak-\nenly predicts “dolphins” as the answer. When using\nContriever to retrieve 10 passages from Wikipedia\ngiven the query, none of the passages contains the\ngold answer. For instance, Contriever retrieves pas-\nsages about “smack”, a kind of fishing vessel, along\nwith other passages about sperm whales, plank-\nton, and other unrelated topics. Similar results are\nfound while using BM25 as the retriever.\nIn contrast, LLM-URL performs much better in\nthis scenario, retrieving 7 documents which contain\nthe answer. The top retrieved document is exactly\nabout the gold answer “jellyfish”. The fourth to the\ntenth documents all talk about different types of\njellyfish. After being chunked into passages then\nsorted by the ranker, the top 10 passages are con-\ncatenated. Among them, it contains “A group of\njellyfish is called a smack,” which contains the an-\nswer to the question and comes directly from the\nfirst retrieved document, titled “jellyfish.” When In-\nstructGPT is then prompted with these 10 passages\nalong with the question, the gold answer “jellyfish”\nis correctly generated.\nThis case study highlights multiple advantages\nof LLM-URL . First, LLM-URL finds documents\nrelated to both the question and the answer. It di-\nrectly locates documents that talks about “jellyfish”\ninstead while BM25 and Contriever locate docu-\nments related to the question only–not the answer.\nSecond, LLM-URL is more precise than BM25\nor Contriever. In this case, 7 out of 10 generated\nURLs from LLM-URL point to a Wikipedia doc-\nument that contains the answer. However, both\nBM25 and Contriever fail to retrieve any docu-\nments containing the answer. Third, the set of docu-\nments retrieved by LLM-URL are complementary\nto each other, while in BM25 or contriever, each\ndocument in the top-10 is selected independently.\nThis is because the LLM is able to refer to previous\ngenerated URLs before it generates the next one,\nallowing each newly generated URL to be condi-\n2673\ntioned on all the previous URLs. This leads to a\nmore informative evidence context in open-domain\nquestion answering.\n5 Conclusion and Future Work\nIn this paper, we explored whether large language\nmodels can generate URLs prompted by human in-\nstructions for document retrieval. Surprisingly, we\nfound that by providing a few (query, URL) pairs\nas in-context demonstrations, large language mod-\nels (e.g. GPT-3) generated Web URLs where near\n90% of the corresponding documents contain cor-\nrect answers to open-domain questions in WebQ.\nFurthermore, by breaking the retrieved documents\ninto passages then ranking them with BM25, we\nshowed a significant number of unnecessary pas-\nsages could be filtered out while retaining high\nrecall, which outperformed baseline methods by a\nsignificant margin.\nThere are numerous exciting directions for future\nwork. While a number of broad spectrum retrieval\nbenchmarks such as BIER (Thakur et al., 2021)\nexist, it remains to be seen whether the few-shot\ndemonstrations shown in this work can be further\ntuned for specific retrieval tasks. Promptagator\n(Dai et al., 2022) shows significant performance\nimprovements can be achieved by tuning prompts\nin a similar way.\nFurther, it remains to be seen whether fine tuning\nthe prompt for each individual question can further\nimprove the retrieval performance. As with Promp-\ntagator, prior work has shown using clustering to\nselect diverse demonstrations for any given ques-\ntion further improves retrieval performance as well\nas downstream QA performance.\nLimitations\nDespite the strong performance on the presented\ndatasets, our approach is limited in its ability to\nupdate knowledge state and adapt to new domains.\nA major feature of retrieve-then-read is the ability\nto swap in new documents when new information\nis learned, such as temporally more recent docu-\nments, or adding in documents from a new domain\nto quickly adapt to a new downstream task. Our\napproach relies on a large language model to con-\ntain all this knowledge and adding new knowledge\nwould likely require some retraining. In addition,\nlarge generation models still suffer from hallucina-\ntion errors, resulting in incorrect predictions. When\ntasked with generating 10 URLs, LLM-URL may\nonly generate 6 or 7 which link to valid documents.\nFinally, our approach involves very large language\nmodels, slow web requests, and document process-\ning which may make it cumbersome to use in prac-\ntice.\nAcknowledgements\nThis work was supported by NSF IIS-2119531, IIS-\n2137396, IIS-2142827, CCF-1901059, and ONR\nN00014-22-1-2507. Wenhao Yu was partly sup-\nported by the Bloomberg Data Science Fellowship.\nReferences\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In EMNLP, pages 1533–\n1544.\nMichele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis,\nWen-tau Yih, Sebastian Riedel, and Fabio Petroni.\n2022. Autoregressive search engines: Generating\nsubstrings as document identifiers. arXiv preprint\narXiv:2204.10628.\nSergey Brin and Lawrence Page. 1998. The anatomy\nof a large-scale hypertextual web search engine. In\nProceedings of the Seventh International Conference\non World Wide Web 7, WWW7, page 107–117, NLD.\nElsevier Science Publishers B. V .\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Procs. of ACL.\nZhuyun Dai, Vincent Y . Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B.\nHall, and Ming-Wei Chang. 2022. Promptagator:\nFew-shot dense retrieval from 8 examples. arXiv\npreprint arXiv:2209.11755.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2020. Autoregressive entity retrieval.\nIn International Conference on Learning Representa-\ntions.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Unsupervised dense infor-\nmation retrieval with contrastive learning.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL, pages 1601–1611.\nMingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang,\nand Yanfang Ye. 2022. Grape: Knowledge graph\nenhanced passage reader for open-domain question\nanswering. arXiv preprint arXiv:2210.02933.\n2674\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Relevance-guided supervision for openqa with\ncolbert. Transactions of the Association for Compu-\ntational Linguistics, 9:929–944.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research\nand development in Information Retrieval, pages 39–\n48.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: A benchmark\nfor question answering research. TACL, pages 452–\n466.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Inves-\ntigating effectiveness and limitations of paramet-\nric and non-parametric memories. arXiv preprint\nArXiv:2212.10511.\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2021. Reader-guided passage reranking for open-\ndomain question answering. In Findings of ACL-\nIJCNLP.\nYasumasa Onoe, Michael J. Q. Zhang, Eunsol Choi, and\nGreg Durrett. 2021. CREAK: A dataset for common-\nsense reasoning over entity knowledge. In Proceed-\nings of the Neural Information Processing Systems\nTrack on Datasets and Benchmarks 1, NeurIPS.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\net al. 2021. Kilt: a benchmark for knowledge in-\ntensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and\nHaifeng Wang. 2021. Rocketqa: An optimized train-\ning approach to dense passage retrieval for open-\ndomain question answering. In Procs. of NAACL.\nStephen E. Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nYi Tay, Vinh Q Tran, Mostafa Dehghani, Jianmo Ni,\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Gupta, et al. 2022. Transformer mem-\nory as a differentiable search index. arXiv preprint\narXiv:2202.06991.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2).\nYujing Wang, Yingyan Hou, Haonan Wang, Ziming\nMiao, Shibin Wu, Hao Sun, Qi Chen, Yuqing Xia,\nChengmin Chi, Guoshuai Zhao, Zheng Liu, Xing\nXie, Hao Sun, Weiwei Deng, Qi Zhang, and Mao\nYang. 2022. A neural corpus indexer for document\nretrieval. In Advances in Neural Information Pro-\ncessing Systems.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N Bennett, Junaid Ahmed, and\nArnold Overwijk. 2020. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In International Conference on Learning\nRepresentations.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nbertserini. In NAACL 2019 (demo).\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2021. Kg-fid: In-\nfusing knowledge graph in fusion-in-decoder for\nopen-domain question answering. arXiv preprint\narXiv:2110.04330.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. International Conference\nfor Learning Representation (ICLR).\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2022. A\nsurvey of knowledge-enhanced text generation. ACM\nComputing Surveys (CSUR).\nZhihan Zhang, Xiubo Geng, Tao Qin, Yunfang Wu,\nand Daxin Jiang. 2021. Knowledge-aware procedu-\nral text understanding with multi-stage training. In\nWWW ’21: The Web Conference 2021.\n2675\nZhihan Zhang, Wenhao Yu, Zheng Ning, Mingxuan Ju,\nand Meng Jiang. 2023a. Exploring contrast consis-\ntency of open-domain question answering systems on\nminimally edited questions. Trans. Assoc. Comput.\nLinguistics.\nZhihan Zhang, Wenhao Yu, Mengxia Yu, Zhichun Guo,\nand Meng Jiang. 2023b. A survey of multi-task learn-\ning in natural language processing: Regarding task\nrelatedness and training methods. In Proceedings\nof the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics, EACL\n2023.\nZhihan Zhang, Wenhao Yu, Chenguang Zhu, and Meng\nJiang. 2022. A unified encoder-decoder framework\nwith entity memory. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022.\nWayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-\nRong Wen. 2022. Dense text retrieval based on pre-\ntrained language models: A survey. arXiv preprint\narXiv:2211.14876.\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\nRetrieving and reading: A comprehensive survey on\nopen-domain question answering. arXiv preprint\narXiv:2101.00774.\n2676\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nOur limitations section is not numbered, but it is the last section of our paper.\n□\u0017 A2. Did you discuss any potential risks of your work?\nWe do not believe any of the risks mentioned in the checklist apply to this paper.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nOur introduction is found in section one and the abstract comes before that.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nSection 4 describes our experiments\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nYes these details are found in a graph in Section 1 and hyperparameters are found in our experiments\nsection (4).\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n2677\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nYes we discuss our hyperparameters in section 4\n□\u0017 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nOur method is deterministic so there is no variation in our results.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n2678",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8304941654205322
    },
    {
      "name": "Identifier",
      "score": 0.7702745199203491
    },
    {
      "name": "Information retrieval",
      "score": 0.7626674175262451
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5972482562065125
    },
    {
      "name": "Language model",
      "score": 0.5534942150115967
    },
    {
      "name": "Search engine",
      "score": 0.5376213192939758
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5206179618835449
    },
    {
      "name": "Key (lock)",
      "score": 0.5091552138328552
    },
    {
      "name": "Code (set theory)",
      "score": 0.4790840744972229
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.47254228591918945
    },
    {
      "name": "Encoder",
      "score": 0.45801132917404175
    },
    {
      "name": "Question answering",
      "score": 0.4489724040031433
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.42303013801574707
    },
    {
      "name": "World Wide Web",
      "score": 0.3423546552658081
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2926274836063385
    },
    {
      "name": "Machine learning",
      "score": 0.2006499171257019
    },
    {
      "name": "Programming language",
      "score": 0.0917411744594574
    },
    {
      "name": "Computer security",
      "score": 0.08626171946525574
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}