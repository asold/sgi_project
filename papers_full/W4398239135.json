{
    "title": "Beyond Accuracy and Robustness Metrics for Large Language Models for Code",
    "url": "https://openalex.org/W4398239135",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2592435482",
            "name": "Daniel Rodríguez-Cárdenas",
            "affiliations": [
                "William & Mary",
                "Williams (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3177813494",
        "https://openalex.org/W2907705732",
        "https://openalex.org/W3211801722",
        "https://openalex.org/W4286531991",
        "https://openalex.org/W4402665833",
        "https://openalex.org/W3184831477",
        "https://openalex.org/W4286530324",
        "https://openalex.org/W4389544179",
        "https://openalex.org/W2954823997",
        "https://openalex.org/W2888328667",
        "https://openalex.org/W2993007949",
        "https://openalex.org/W2972082064",
        "https://openalex.org/W4284709233",
        "https://openalex.org/W3161903544",
        "https://openalex.org/W4324138978",
        "https://openalex.org/W2736762043",
        "https://openalex.org/W2511803001",
        "https://openalex.org/W4245415816",
        "https://openalex.org/W3103170042",
        "https://openalex.org/W4254188649"
    ],
    "abstract": "In recent years, Large Language Models for code (LLMc) have transformed the landscape of software engineering (SE), demonstrating significant efficacy in tasks such as code completion, summarization, review, tracing, translation, test case generation, clone detection, and bug fixing. Notably, GitHub Copilot [31] and Google's CodeBot [21] exemplify how LLMc contributes to substantial time and effort savings in software development. However, despite their widespread use, there is a growing need to thoroughly assess LLMc, as current evaluation processes heavily rely on accuracy and robustness metrics, lacking consensus on additional influential factors in code generation. This gap hinders a holistic understanding of LLMc performance, impacting interpretability, efficiency, bias, fairness, and robustness. The challenges in benchmarking and data maintenance compound this issue, underscoring the necessity for a comprehensive evaluation approach. To address these issues, this dissertation proposes the development of a benchmarking infrastructure, named HolBench, aimed at overcoming gaps in evaluating LLMc quality. The goal is to standardize testing scenarios, facilitate meaningful comparisons across LLMc, and provide multi-metric measurements beyond a sole focus on accuracy. This approach aims to decrease the costs associated with advancing LLMc research, enhancing their reliability for adoption in academia and industry.",
    "full_text": null
}