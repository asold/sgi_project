{
  "title": "Improving N-gram Language Models with Pre-trained Deep Transformer",
  "url": "https://openalex.org/W2990835715",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2233861485",
      "name": "Wang Yi-ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289493327",
      "name": "Huang, Hongzhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2000409007",
      "name": "Liu Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2943443080",
      "name": "Pang Yutong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1928402660",
      "name": "Wang Yongqiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748002541",
      "name": "Zhai, ChengXiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2390690688",
      "name": "Peng Fu-chun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2401714573",
    "https://openalex.org/W2295078202",
    "https://openalex.org/W3008525923",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2176797192",
    "https://openalex.org/W2939021222",
    "https://openalex.org/W2252186615",
    "https://openalex.org/W2037942319",
    "https://openalex.org/W2916979304"
  ],
  "abstract": "Although n-gram language models (LMs) have been outperformed by the state-of-the-art neural LMs, they are still widely used in speech recognition due to its high efficiency in inference. In this paper, we demonstrate that n-gram LM can be improved by neural LMs through a text generation based data augmentation method. In contrast to previous approaches, we employ a large-scale general domain pre-training followed by in-domain fine-tuning strategy to construct deep Transformer based neural LMs. Large amount of in-domain text data is generated with the well trained deep Transformer to construct new n-gram LMs, which are then interpolated with baseline n-gram systems. Empirical studies on different speech recognition tasks show that the proposed approach can effectively improve recognition accuracy. In particular, our proposed approach brings significant relative word error rate reduction up to 6.0% for domains with limited in-domain data.",
  "full_text": "IMPROVING N-GRAM LANGUAGE MODELS WITH PRE-TRAINED DEEP TRANSFORMER\nYiren Wang1,∗, Hongzhao Huang2, Zhe Liu2, Yutong Pang2, Yongqiang Wang2,\nChengXiang Zhai1, Fuchun Peng2\n1 University of Illinois at Urbana-Champaign, 2 Facebook AI\nABSTRACT\nAlthough n-gram language models (LMs) have been outperformed\nby the state-of-the-art neural LMs, they are still widely used in\nspeech recognition due to its high efﬁciency in inference. In this\npaper, we demonstrate that n-gram LM can be improved by neural\nLMs through a text generation based data augmentation method. In\ncontrast to previous approaches, we employ a large-scale general\ndomain pre-training followed by in-domain ﬁne-tuning strategy to\nconstruct deep Transformer based neural LMs. Large amount of\nin-domain text data is generated with the well trained deep Trans-\nformer to construct new n-gram LMs, which are then interpolated\nwith baseline n-gram systems. Empirical studies on different speech\nrecognition tasks show that the proposed approach can effectively\nimprove recognition accuracy. In particular, our proposed approach\nbrings signiﬁcant relative word error rate reduction up to 6.0% for\ndomains with limited in-domain data.\nIndex Terms— n-gram, language model, transformer, pre-\ntraining, interpolation\n1. INTRODUCTION\nN-gram language models (LMs) are widely used in the automatic\nspeech recognition (ASR) systems due to its simplicity and high ef-\nﬁciency in inference. However, n-gram LMs suffer from perfor-\nmance bottleneck caused by the poor generalization to unseen n-\ngrams and lack of ability to capture long range dependencies. Neural\nlanguage models [1, 2, 3] have overcome such deﬁciencies with dis-\ntributed representation learning in the continuous space and achieved\nthe state-of-the-art language modeling performances. However, the\nhigh computational cost hampers inference latency and makes it hard\nto directly integrate neural LMs into the ﬁrst-pass decoding of an\nASR system. Instead, neural LMs are commonly used in second-\npass rescoring and have shown effectiveness in improving recogni-\ntion accuracy [4, 5, 6]. Still, since the N-best list or lattice for rescor-\ning generally depends on n-gram LMs from the ﬁrst-pass decoding,\nimproving the performance of n-gram LMs is of great importance.\nDifferent approaches have been proposed to improve n-gram\nLMs with neural LMs, including probability based methods that di-\nrectly convert probabilities of neural LMs to the n-gram LMs [7, 8,\n9], and text generation based methods that leverage shallow recurrent\nneural networks to generate text for n-gram training [10]. Empirical\nstudies have shown that the latter generally leads to better perfor-\nmances [9]. However, previous work in this line has not fully lever-\naged the state-of-the-art deep neural networks such as deep Trans-\nformer [11, 12], and is less applicable to situations where the in-\ndomain data are too limited to train a good neural LM.\n∗This work is conducted during internship at Facebook.\nFrom another perspective, constructing good LMs depends on\nadequate high-quality training data. Unfortunately, in many cases,\nonly limited in-domain data are accessible, making the data sparsity\nproblem even more severe for n-gram LMs, and also introducing\noptimization difﬁculties for training neural LMs. Effectively lever-\naging the rich general domain corpora could help ease challenge and\nconstruct high-capacity neural networks with good generalization.\nIn this paper, we propose a new text generation based data aug-\nmentation approach that fully utilizes the large dataset and high-\ncapacity neural networks to improve the n-gram LMs. Inspired by\nthe recent advances in language model pre-training approaches [12,\n13, 14], where the key idea is to pre-train the model on a unlabeled\ncorpora and then ﬁne-tune on different supervised downstream tasks,\nwe introduce a general domain pre-training followed by in-domain\nﬁne-tuning strategy to construct high-capacity neural LMs for text\ngeneration. Speciﬁcally, a deep Transformer based neural LM is ﬁrst\npre-trained on a large general domain corpora, and then ﬁne-tuned\non the in-domain dataset. The well trained neural LM is then used to\ngenerate a large amount of high quality text to construct a synthetic\ncorpus. The new n-gram LMs trained on the synthetic datasets are\ninterpolated with baseline n-gram LMs and used for ASR decoding.\nThis paper has two main contributions: (1) We are the ﬁrst to\nleverage deep Transformer for text generation to improve ASR sys-\ntems. In contrast to previous methods using shallow feedforward\nor recurrent neural networks [10], our choice of deep Transformer\nnetwork, which has been shown to excel at capturing long-term de-\npendencies in text [11, 12, 13], results in stronger modeling ability\nand therefore guarantees better generation quality. (2) We propose a\ngeneral domain pre-training followed by in-domain ﬁne-tuning strat-\negy, which takes full advantage of the combination of large dataset\nand high-capacity models. Previous approaches with standard in-\ndomain training are likely to encounter difﬁculties when there is very\nlimited in-domain training data, either in optimization that leads to\nsub-optimal performances, or in generalization that results in text\ngenerated with poor diversity. The pre-training strategy contributes\nto overcome such limitations, making our approach more robust and\ngenerally applicable to different practical scenarios.\nExperiments on two datasets show that our approach can effec-\ntively improve recognition accuracy over the strong baseline ASR\nsystems and the existing LSTM based neural LM data augmentation\nmethods. In particular, our proposed approach brings signiﬁcant rel-\native word error rate reduction up to 6.0% for domains with limited\nin-domain data, which shows that our approach is very effective to\nimprove speech recognition systems on new domains without extra\nefforts to collect in-domain data manually.\n2. RELATED WORK\nNeural LMsNeural language models are proposed to overcome the\narXiv:1911.10235v1  [cs.CL]  22 Nov 2019\ncurse of dimensionality by learning distributed word representations\nand probability function of word sequences [1]. Different neural\nnetwork architectures have been proposed, including feedforward\nNN [1], RNN [2, 15], LSTM [3] and Transformer [11, 12], among\nwhich the self-attention based Transformer is the state-of-the-art ar-\nchitecture for many sequence modeling tasks due to its superiority in\ncapturing longer-range linguistic structure. Although still less used\nin the ﬁrst-pass ASR decoding due to its high inference latency, neu-\nral LMs have shown effectiveness in many other applications such\nas natural language generation [16, 17].\nLM pre-training Recent emerging language model pre-training\napproaches, such as OpenAI GPT [12], GPT2 [14] and BERT [13],\nhave shown effectiveness for improving many natural language\nprocessing tasks. The key idea is to pre-train a deep Transformer\nnetwork on unlabeled corpora then ﬁne-tune the parameters on the\ndownstream tasks. These approaches, however, mainly focus on the\nsemi-supervised learning paradigm that leverages unsupervised pre-\ntraining to improve the supervised downstream tasks. In contrast,\nwe focus mainly on the language modeling task and leverage the\ngeneral domain pre-training to promote in-domain modeling.\nImproving n-gram LMs Different methods have been proposed\nto improve n-gram LMs with neural LMs, including converting a\nfeedforward nerual LM into an n-gram LM by directly assigning\nthe probabilities [9], converting recurrent neural network (RNN)\nLM into backoff LMs and further improved quality with an iterative\napproach [7]. The closest line of work to ours leverages multiple\nRNNLMs from different domains to generate text data for improv-\ning n-gram LMs [10]. However, their use of shallow RNN models\nand in-domain training restricts the generation ability. In contrast,\nour choice of pre-trained deep Transformer is able to generate text\nin higher quality by capturing longer range dependency, and better\ndiversity through better model generalization.\n3. APPROACH\nWe introduce the details of the proposed data augmentation method\nfor improving n-gram LMs in this section.\n3.1. Overall Pipeline\nThe overall pipeline of the proposed approach is depicted in Fig 1\n(left), which consists of four steps including pre-training, ﬁne-\ntuning, generation and interpolation. Speciﬁcally, we ﬁrst pre-train\na deep Transformer on the large general-domain corpus and then\nﬁne-tune on the target domain dataset. We use the obtained neural\nLM to generate large amount of high quality in-domain text data,\nwhich is then used to construct a synthetic dataset for n-gram LM\ntraining. The new n-gram LMs are eventually interpolated with the\nprevious baseline n-gram LMs and evaluated in the ASR system.\n3.2. Pre-training and Fine-tuning\nWe propose a general-domain pre-training followed by in-domain\nﬁne-tuning strategy. Given a large and diverse collection C =\n{X1,...,X N}, where each Xi is a sequence of word or subword\nunits Xi = {x(i)\n1 ,...,x (i)\nk }, we use the standard left-to-right lan-\nguage modeling objective to maximize the likelihood:\nL(C) =\n∑\ni\n∑\nt\nlogP(x(i)\nt |x(i)\n<t; Θ) (1)\nwhere the conditional probability P is modeled by a neural network\nwith parameters Θ and x(i)\n<t is the history up to t.\nFig. 1. (left) Overall pipeline of the proposed data augmentation\napproach. (right) Transformer architecture for neural LM.\nFine-tuning is then performed for each domain of interest. Given\na in-domain collection D= {Y1,...,Y M}, Yi = {y(i)\n1 ,...,y (i)\nk },\nthe model is trained using the same objective in Eqn. 1. The model\nparameters Θ are initialized by the pre-trained model and further\noptimized on Dtill converge.\nIn this work, we use the deep Transformer decoder [12], a vari-\nant of Transformer [11], as the architecture for our neural LM Θ.\nAs is illustrated in Fig 1 (right), the model is composed of a stack\nof N transformer blocks. Each block has two types of basic lay-\ners: (1) Multi-head self attention layer, which generates an adap-\ntive weighted sum of the input hidden representations from previous\nlayer; (2) Feed forward layer, which applies non-linear transforma-\ntion to the hidden vector. Each basic layer is associated with layer\nnormalization, residual connection and dropout.\nThis is the ﬁrst work that leverages deep Transformer for text\ngeneration for ASR. Intuitively, deep Transformer is superior to pre-\nvious shallow feed forward or recurrent neural LMs in two ways: (1)\nthe self attention mechanism eases the challenge of long-range de-\npendency learning, which is particularly important for high-quality\ntext generation; (2) the high model capacity and depth leads to bet-\nter modeling and generalization ability. Our proposed pre-training\nstrategy helps overcome previous limitations in lack of in-domain\ntraining data by making use of the largely available general-domain\ndata, and makes it possible to construct such strong neural LM.\n3.3. Text Generation\nWe generate a large amount of text data with the well trained neu-\nral LM. The generation is performed by sampling from the model\ndistribution given the preﬁxed context. Specially, we ﬁrst construct\na preﬁx corpus DP = {S1,...,S n}, where Si = {s(i)\n1 ,...,s (i)\nk }\nwith kpreﬁx tokens. During text generation, Siis fed into the model\nas the preﬁxed context. The neural LM produces probabilities over\nthe vocabulary V:\npi = exp(hi\nτ )\n∑\njexp(\nhj\nτ )\n(2)\nwhere h = (h1,...,h |V|) is the logit vector, and τ is the temper-\nature for sampling. Then the output tokens are sampled from the\nprobability distribution.\nA rule-based data ﬁltration is performed on the generated syn-\nthetic corpus to ensure data quality for n-gram LM training. We\ndesigned multiple different rules including ﬁlter by maxmimum and\nminimum sequence length, ﬁlter by out-of-vocabulary words, ﬁlter\nby domain-speciﬁc keyword, and ﬁlter by number of duplicated gen-\neration. The thresholds for each ﬁltering rule are selected based on\ndata distribution of the in-domain training data.\n4. EXPERIMENT\nWe evaluate the effectiveness of the proposed approach on two\nspeech recognition datasets. We compare the performance of deep\nTransformer with shallow LSTMs, as well as the pre-training strat-\negy with the traditional in-domain training. As we will show, the\nproposed combination of pre-training strategy and deep Transformer\nlead to substantial improvements.\n4.1. Datasets\nWe conduct experiments on two in-house speech recognition datasets:\nthe speech assistant dataset (denoted as Assistant), and the conver-\nsational speech dataset (denoted as Conversation). Both datasets are\ncompletely anonymized and no user-identiﬁable information (UII)\nis access to both annotators and researchers.\nAssistant The Assistant dataset consists of English utterances that\nare commands users give to Facebook Portal 1 after the wakeword\n“Hey Portal” to carry out certain actions. The utterances can be cat-\negorized into various sub-domains by the type of actions, such as\nmaking phone calls to their friends (calling), and device control (de-\nvice), or getting weather information (weather). We use a mixed set\nof utterances that are randomly sampled from both internal dogfood-\ning and Facebook Portal live trafﬁc. Internal dogfooding is an activ-\nity from internal employees with signed agreements to have their\nactivity reviewed and tested. We choose to exclude some domains\nthat contain limited utterance patterns such as calling as enriching\nthe training data is not helpful for these domains. All these sampled\nutterances are voice morphed before sending to annotators for tran-\nscription. In total, we use a collection of 730k utterances as training\ndata, 15k as development data, and 40k as test data.\nConversation The Conversation dataset was collected through\ncrowd-sourcing. It consists of conversations between each pair\nof crowd-sourcers with more than 20 topics that are commonly\nmentioned in daily life, including family, travel, etc. We split the\ndata into training (240k), development (7k), and test (20k) sets.\nGeneral-domain Pre-trainingWe use a large in-house English text\ncorpus as general domain data for neural LM pre-training, which\ncontains a random sample of110M public posts and comments users\nshared on Facebook. We use byte pair encoding (BPE2) [18] to seg-\nment word tokens into subword units, forming a 25k-subword vo-\ncabulary used for both Assistant and Conversation dataset. We di-\nrectly converted the text data into machine reading format for model\ntraining and did not manually look into the actual content.\n4.2. Experiment Setups\nBaselines We compare our proposed approaches with two baselines,\nincluding (1) baselinen-gram without augmentation (Baseline) [19],\nand (2) data augmentation with text generated by LSTM trained on\nin-domain data [10] (LSTM (in-domain)).\n1Portal is a video enabled smart device.\n2https://github.com/glample/fastBPE\nTable 1. The overall relative word error rate reduction (WERR) for\neach data augmentation approach on Assistant.\nWERR\nLSTM (in-domain) 1.07%\nTransformer (in-domain) 1.30%\nTransformer (pre-trained) 2.25%\nTable 2. Relative word error rate reduction (WERR) for each data\naugmentation approach on different Assistant sub-domains, includ-\ning device, weather and music.\nDevice Weather Music\nLSTM (in-domain) 3.16% 1 .64% 2 .42%\nTransformer (in-domain) 3.56% 3 .10% 3 .52%\nTransformer (pre-trained) 3.75% 6.01% 4.91%\nASR SystemWe use a state-of-the-art hybrid ASR system that uti-\nlizes multi-layer Latency Controlled Bidirectional Long Short-Term\nMemory RNNs (LC-BLSTM) for acoustic modeling with grapheme\nrepresentations. And it uses pruned 4-gram LMs in the ﬁrst-pass\ndecoding with an in-house dynamic decoder, where the ﬁnal LMs\nare interpolated with LMs trained from both in-domain and general-\ndomain training data. For each approach on each dataset, we opti-\nmize all model hyper-parameters on the development sets.\nModel Settings We adopt the GPT conﬁguration following [12],\nwith the dimension of word embeddings, hidden states and non-\nlinear layers set as 768, 768 and 3072 respectively. The numbers\nof both decoder blocks and attention heads are set as 12, and the\ndropout rate is 0.1. For the LSTM baseline, we adopt a model with\nsimilar model size as Transformer for fair comparison. We use a\nstack of 2 LSTM layers, where the dimension of word embeddings,\nhidden states set as 1024 and 2048 respectively. The dropout rate\nis 0.2. We use the Adam optimization scheme following [12]. The\nmodels are trained on 16 V100 GPUs, and based on the PyTorch\nimplementation of Transformer3.\nText GenerationWe extract the preﬁx sequences withktokens from\nthe in-domain training data, where k ∈{1,2,3,4,5,6}. We keep\nthe top 5 sampling hypotheses with length penalty set as 1.0. The\ntemperature for sampling is set as τ ∈{1.0,1.1,..., 1.5}for the\nbest balance of generation quality and diversity.\nEvaluation For evaluation, we interpolate the new n-gram LM with\nthe baseline n-grams and evaluate the methods via word error rate\n(WER) of the ASR system, and report WER reductions (WERR)\nover the baseline approach.\n4.3. Assistant\nWe report the overall word error rate reduction over the baseline\napproach on Assistant and several sub-domains including device,\nweather and calling in Table 1 and Table 2, respectively. From these\ntables we have the following observations:\n1. The proposed data augmentation approaches effectively im-\nprove the overall quality of n-gram LMs in the ASR systems. In\nparticular, our pre-trained deep Transformer achieves over2.2% rel-\native reduction in WER, which signiﬁcantly outperforms the LSTM-\nbased approach.\n3https://github.com/pytorch/fairseq\nTable 3. Word-level perplexity of neural LMs on Assistant test set.\nPre-trained Fine-tuned\nLSTM (in-domain) − 7.56\nTransformer (in-domain) − 7.11\nTransformer (pre-trained) 55.96 6.25\nTable 4. Examples generated by Transformer pre-trained and ﬁne-\ntuned on Assistant in-domain data. The examples are excluded from\nthe in-domain training data.\nDomain Examples\nreplay the current track\nMusic what album is this track from\nplay french playlist on spotify\nwhat’s the hourly forecast for today\nWeather what’s the weather in youngstown ohio\nwhat’s the temperature in delray beach ﬂorida\n2. The proposed approach is particularly beneﬁcial for sub-\ndomains with less training data. Due to the unbalance of Assistant\ndataset, some sub-domains like weather and music are important yet\nhave only a small training collection. The proposed approach with\npre-trained Transformer achieves over 6% and 4.9% relative WER\nreduction in the weather and music sub-domains and outperforms\naugmentation with LSTM or Transformer constructed with tradi-\ntional training strategy without pre-training by large margin. For\nthe large domains such as device, we observe fewer gains from pre-\ntraining as the in-domain data is already sufﬁcient to train a neural\nLM with good performance in these large domains. However, we\ncan still see that Transformer outperforms LSTM, and pre-training\nslightly further improves the performance.\n3. The improvements have been brought by both use of deep\nTransformer architecture and pre-training strategy. WER reduction\nis observed by replacing the LSTM to deep Transformer network for\nneural LM, which indicates the superiority of the model architecture.\nWith the pre-training strategy, the model can be even better utilized\nand results in the best ASR decoding performance.\nWe further present detailed analysis on the different neural LMs.\nTable 3 shows the perplexity of neural LMs with different architec-\nture and training strategy, which veriﬁes that deep Transformer has\nbetter modeling performance than the previous LSTM/RNN, and\ndemonstrates that the general domain pre-training and in-domain\nﬁne-tuning strategy is an important component for high-quality deep\nmodel construction. Table 4 presents multiple generated cases in the\nmusic and weather sub-domains, which are not originally included\nin the in-domain Assistant training collection. The examples illus-\ntrate that the pre-trained neural LM can generate high-quality text\nfor data augmentation and enrich the sequence patterns to help ease\nthe problem of data sparsity for n-gram LMs.\n4.4. Conversation\nWe further evaluate the approach on the Conversation dataset, which\nhas a much smaller training collection (240k) than Assistant (730k),\nwith more complex and diverse patterns. As can been seen from\nTable 6, the pre-trained deep Transformer demonstrates signiﬁcant\nsuperiority over the neural LMs with traditional training scheme in\nsuch a scenario with the lack of in-domain training data.\nTable 5. Relative word error rate reduction (WERR) over the base-\nline approach on Conversation test set. “#Aug” denotes the number\nof augmented training data.\n#Aug WERR\nLSTM (in-domain) 4M 1.00%\nTransformer (in-domain) 4M 1.86%\nTransformer (pre-trained) 4M 2.62%\nLSTM (in-domain) 18M 1.66%\nTransformer (in-domain) 18M 2.12%\nTransformer (pre-trained) 18M 3.08%\nTable 6. Word-level perplexity of neural LMs on Conversation test\nset. “pre-trained” denotes model pre-trained on general background\ndata and then ﬁne-tuned on Conversation dataset. “in-domain” de-\nnotes model trained only on Conversation.\nPre-trained Fine-tuned\nLSTM (in-domain) − 112.45\nTransformer (in-domain) − 89.36\nTransformer (pre-trained) 80.43 46 .68\nThe performances are presented in Table 5. With 4 million\ninstances of synthetic in-domain training corpus, the proposed\napproach with pre-trained deep Transformer achieves over 2.6%\nrelative WER reduction, compared with 1.8% relative reduction of\nTransformer and 1.0% of LSTM with traditional in-domain training\nstrategy. The performance continues to grow when we enlarge the\nvolume of generated data to 18 millions, and achieves over 3.0%\nrelative WER reduction over the strong baseline system.\nThese results corroborate our motivation and demonstrate that:\n1. The proposed approach is simple yet effective in improving\nn-gram LMs in ASR. The number of augmented training data can\nbe easily scaled up for further decoding performance improvements\nwith minimal computational cost.\n2. The general domain pre-training then in-domain ﬁne-tuning\nstrategy is the key component of the proposed method. The superi-\nority of pre-training over the traditional in-domain training strategy\nis at two scales: (i) The large-scale general pre-training enables con-\nstruction of the state-of-the-art deep Transformer rather than shal-\nlow RNNs [10], which leads to strong neural LMs with large model\ncapacity and better generalization to generate text with both high\nquality and good diversity. (ii) In the cases with lack of in-domain\ntraining data, direct in-domain training results in sub-optimal perfor-\nmances of the neural LMs (Table 6). The pre-training stategy over-\ncomes the problem, making it more robust and generally applicable\nto different scenarios.\n5. CONCLUSION\nIn this paper, we introduce a text generation based data augmentation\napproach that effectively improves n-gram LMs and achieves better\nASR decoding accuracy. Our contributions are at two scales: (1) We\nare the ﬁrst to leverage deep Transformer for text generation for ASR\nsystems; (2) We proposed a general domain pre-training followed by\nin-domain ﬁne-tuning strategy that enables us to fully leverage the\nlarge corpora and the high-capacity neural networks. The approach\nis general and widely applicable to different data domains to help\nimprove the ﬁrst-pass decoding accuracy of the ASR systems.\n6. REFERENCES\n[1] Yoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Chris-\ntian Jauvin, “A neural probabilistic language model,” Journal\nof machine learning research, vol. 3, no. Feb, pp. 1137–1155,\n2003.\n[2] Tom ´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan ˇCernock`y,\nand Sanjeev Khudanpur, “Recurrent neural network based lan-\nguage model,” in Eleventh annual conference of the interna-\ntional speech communication association, 2010.\n[3] Martin Sundermeyer, Ralf Schl ¨uter, and Hermann Ney, “Lstm\nneural networks for language modeling,” in Thirteenth annual\nconference of the international speech communication associ-\nation, 2012.\n[4] Anoop Deoras, Tom ´aˇs Mikolov, and Kenneth Church, “A fast\nre-scoring strategy to capture long-distance dependencies,” in\nProceedings of the Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computational Lin-\nguistics, 2011, pp. 1116–1127.\n[5] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals,\n“Listen, attend and spell: A neural network for large vocabu-\nlary conversational speech recognition,” in2016 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2016, pp. 4960–4964.\n[6] Wayne Xiong, Lingfeng Wu, Fil Alleva, Jasha Droppo, Xue-\ndong Huang, and Andreas Stolcke, “The microsoft 2017 con-\nversational speech recognition system,” in 2018 IEEE interna-\ntional conference on acoustics, speech and signal processing\n(ICASSP). IEEE, 2018, pp. 5934–5938.\n[7] Ebru Arısoy, Stanley F Chen, Bhuvana Ramabhadran, and Ab-\nhinav Sethy, “Converting neural network language models\ninto back-off language models for efﬁcient decoding in auto-\nmatic speech recognition,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 22, no. 1, pp. 184–192,\n2013.\n[8] Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumita, Hai\nZhao, and Bao-Liang Lu, “Converting continuous-space lan-\nguage models into n-gram language models for statistical ma-\nchine translation,” in Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing, 2013, pp.\n845–850.\n[9] Heike Adel, Katrin Kirchhoff, Ngoc Thang Vu, Dominic\nTelaar, and Tanja Schultz, “Comparing approaches to con-\nvert recurrent neural networks into backoff language models\nfor efﬁcient decoding,” in Fifteenth Annual Conference of the\nInternational Speech Communication Association, 2014.\n[10] Masayuki Suzuki, Nobuyasu Itoh, Tohru Nagano, Gakuto Ku-\nrata, and Samuel Thomas, “Improvements to n-gram language\nmodel using text generated from neural language model,” in\nICASSP 2019-2019 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp.\n7245–7249.\n[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin, “Attention is all you need,” in Advances in neural\ninformation processing systems, 2017, pp. 5998–6008.\n[12] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever, “Improving language understanding by generative\npre-training,” Technical report, OpenAI, 2018.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova, “Bert: Pre-training of deep bidirectional transform-\ners for language understanding,” in Proceedings of the 2019\nConference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 2019, pp. 4171–4186.\n[14] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever, “Language models are unsuper-\nvised multitask learners,” OpenAI Blog, vol. 1, no. 8, 2019.\n[15] Tom ´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur, “Extensions of recur-\nrent neural network language model,” in 2011 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2011, pp. 5528–5531.\n[16] Ilya Sutskever, James Martens, and Geoffrey E Hinton, “Gen-\nerating text with recurrent neural networks,” in Proceedings\nof the 28th International Conference on Machine Learning\n(ICML-11), 2011, pp. 1017–1024.\n[17] Ryo Masumura, Taichi Asami, Takanobu Oba, Hirokazu Masa-\ntaki, Sumitaka Sakauchi, and Akinori Ito, “Combinations of\nvarious language model technologies including data expansion\nand adaptation in spontaneous speech recognition,” in Six-\nteenth Annual Conference of the International Speech Com-\nmunication Association, 2015.\n[18] Rico Sennrich, Barry Haddow, and Alexandra Birch, “Neu-\nral machine translation of rare words with subword units,” in\nProceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 2016, pp.\n1715–1725.\n[19] Duc Le, Xiaohui Zhang, Weiyi Zheng, Christian F ¨ugen, Geof-\nfrey Zweig, and Michael Seltzer, “From senones to chenones:\nTied context-dependent graphemes for hybrid speech recogni-\ntion,” in IEEE Automatic Speech Recognition and Understand-\ning Workshop, 2019.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7807857990264893
    },
    {
      "name": "Computer science",
      "score": 0.7606527209281921
    },
    {
      "name": "Language model",
      "score": 0.6266947984695435
    },
    {
      "name": "Inference",
      "score": 0.6251273155212402
    },
    {
      "name": "n-gram",
      "score": 0.5312162637710571
    },
    {
      "name": "Gram",
      "score": 0.5306251049041748
    },
    {
      "name": "Artificial intelligence",
      "score": 0.52159583568573
    },
    {
      "name": "Speech recognition",
      "score": 0.48739010095596313
    },
    {
      "name": "Word error rate",
      "score": 0.48721522092819214
    },
    {
      "name": "Construct (python library)",
      "score": 0.4508053660392761
    },
    {
      "name": "Deep neural networks",
      "score": 0.44028571248054504
    },
    {
      "name": "Deep learning",
      "score": 0.3928271532058716
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3448150157928467
    },
    {
      "name": "Machine learning",
      "score": 0.34007006883621216
    },
    {
      "name": "Engineering",
      "score": 0.0920867919921875
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Bacteria",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}