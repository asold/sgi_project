{
  "title": "Genetic Prompt Search via Exploiting Language Model Probabilities",
  "url": "https://openalex.org/W4385768063",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5006691314",
      "name": "Jiangjiang Zhao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "China Mobile (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100741930",
      "name": "Zhuoran Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5011978021",
      "name": "Fangchun Yang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297582752",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2160960847",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4320341763",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2162737890",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W1979356488",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3212893438",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4221142421",
    "https://openalex.org/W4225590069",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4385573069",
    "https://openalex.org/W2138537392",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W4385573003",
    "https://openalex.org/W4385573610",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4385574139",
    "https://openalex.org/W3216866458",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4386566526",
    "https://openalex.org/W3162385798",
    "https://openalex.org/W4285247752"
  ],
  "abstract": "Prompt tuning for large-scale pretrained language models (PLMs) has shown remarkable potential, especially in low-resource scenarios such as few-shot learning. Moreover, derivative-free optimisation (DFO) techniques make it possible to tune prompts for a black-box PLM to better fit downstream tasks. However, there are usually preconditions to apply existing DFO-based prompt tuning methods, e.g. the backbone PLM needs to provide extra APIs so that hidden states (and/or embedding vectors) can be injected into it as continuous prompts, or carefully designed (discrete) manual prompts need to be available beforehand, serving as the initial states of the tuning algorithm. To waive such preconditions and make DFO-based prompt tuning ready for general use, this paper introduces a novel genetic algorithm (GA) that evolves from empty prompts, and uses the predictive probabilities derived from the backbone PLM(s) on the basis of a (few-shot) training set to guide the token selection process during prompt mutations. Experimental results on diverse benchmark datasets show that the proposed precondition-free method significantly outperforms the existing DFO-style counterparts that require preconditions, including black-box tuning, genetic prompt search and gradient-free instructional prompt search.",
  "full_text": "Genetic Prompt Search via Exploiting Language Model Probabilities\nJiangjiang Zhao1,2 , Zhuoran Wang3 , Fangchun Yang1\n1Beijing University of Posts and Telecommunications, P.R. China\n2China Mobile Online Services Co., Ltd. Beijing, P.R. China\n3Clouchie Limited, London, United Kingdom\nzjjbupt@bupt.edu.cn, wangzhuoran@clouchie.ai, fcyang@bupt.edu.cn\nAbstract\nPrompt tuning for large-scale pretrained language\nmodels (PLMs) has shown remarkable potential,\nespecially in low-resource scenarios such as few-\nshot learning. Moreover, derivative-free optimi-\nsation (DFO) techniques make it possible to tune\nprompts for a black-box PLM to better fit down-\nstream tasks. However, there are usually precon-\nditions to apply existing DFO-based prompt tuning\nmethods, e.g. the backbone PLM needs to provide\nextra APIs so that hidden states (and/or embed-\nding vectors) can be injected into it as continuous\nprompts, or carefully designed (discrete) manual\nprompts need to be available beforehand, serving as\nthe initial states of the tuning algorithm. To waive\nsuch preconditions and make DFO-based prompt\ntuning ready for general use, this paper introduces\na novel genetic algorithm (GA) that evolves from\nempty prompts, and uses the predictive probabili-\nties derived from the backbone PLM(s) on the ba-\nsis of a (few-shot) training set to guide the token\nselection process during prompt mutations. Exper-\nimental results on diverse benchmark datasets show\nthat the proposed precondition-free method signif-\nicantly outperforms the existing DFO-style coun-\nterparts that require preconditions, including black-\nbox tuning, genetic prompt search and gradient-\nfree instructional prompt search.\n1 Introduction\nThe recent successes of pretrained language models (PLMs)\nare revolutionising the field of natural language processing\n(NLP) [Devlin et al., 2019; Radford et al., 2019; Liu et al.,\n2019; Raffel et al., 2020; Clark et al., 2020]. Meanwhile,\nextremely large PLMs have demonstrated great potential in\nfew-shot learning scenarios (e.g. [Brown et al., 2020]), which\nmakes them increasingly attractive as out-of-box tools for\ngeneral use. Fine-tuning such large-scale PLMs can still be\ncomputationally expensive, even on a few-shot training set.\nBut this is significantly relieved by a new paradigm called\nprompt tuning. Prompt tuning methods [Li and Liang, 2021;\nGao et al., 2021; Lester et al., 2021; Shin et al., 2020;\nLiu et al., 2022; Liu et al., 2021a; Liu et al., 2023] work\nby inserting a small number of tunable variables into a back-\nbone PLM’s input (and sometimes also the hidden states[Liu\net al., 2021a]) to bias its predictive probabilities towards de-\nsired output, while keeping the backbone model’s parame-\nters frozen during the learning process. The tunable variables\nhere can either be continuous vectors (namely soft prompts)\nor surface tokens (discrete prompts), for which obtaining\npromising values is the essential objective and can be solved\nby either gradient-based optimisers [Kingma and Ba, 2015;\nLoshchilov and Hutter, 2019] or derivative-free optimisation\n(DFO) techniques [Kolda et al., 2003; Rios and Sahinidis,\n2013; Yu and Gen, 2010].\nTo democratise the access to those very large PLMs, a com-\nmon practice is to deliver them as back-box services with\ncloud APIs only [Brown et al., 2020; Ouyang et al., 2022]\n(a.k.a LM-as-a-Service [Sun et al., 2022b]). This implies\nprompt tuning via DFO to be an senseful and important re-\nsearch direction, where the backbone’s parameters are not re-\nquired to be exposed to the tuner. Existing work has proven\nthe feasibility of applying DFO for prompt tuning [Xu et\nal., 2022; Prasad et al., 2022; Sun et al., 2022b; Sun et al.,\n2022a]. For example, Xuet al.[2022] and Prasad et al.[2022]\nintroduced search heuristics to refine human generated (dis-\ncrete) prompts via edit operations such as paraphrase, cloze,\ndeletion, swap, etc. On the other hand, Sun et al. [2022b;\n2022a] proposed the so-call ‘black-box tuning (BBT)’ meth-\nods for continuous prompt optimisation based on the co-\nvariance matrix adaptation evolution strategy [Hansen et al.,\n2003].\nHowever, the limitations of existing methods are obvi-\nous. The discrete prompt search heuristics in [Xu et al.,\n2022; Prasad et al., 2022] require carefully designed man-\nual prompts available beforehand, which yield additional\nhuman efforts. Furthermore, their performance may also\nhighly rely on the suitableness of those initial manual prompts\n(cf. §4.3). In addition, methods of this kind usually re-\nquire auxiliary language models (LMs) to paraphrase exist-\ning prompts (e.g. T5 11B [Raffel et al., 2020] used in [Xu\net al., 2022 ] and PEGASUS [Zhang et al., 2020 ] used in\n[Prasad et al., 2022]), which involves extra dependencies. For\nBBT-style continuous prompt optimisation[Sun et al., 2022b;\nSun et al., 2022a], it requires the backbone PLM to provide\nextra APIs so that the continuous prompts can be injected\nas word embeddings (or hidden states for BBTv2 [Sun et\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5296\nal., 2022a ]), which may not always be attainable in prac-\ntice. A na ¨ıve alternative is in-context learning (ICL) pro-\nposed along with GPT-3 [Brown et al., 2020], which simply\nprepends training examples to the input as the prompt. But\nprepending too many or too long examples could make the\nfinal input exceed the sequence length threshold of the back-\nbone model, which restricts ICL’s applicable tasks. In addi-\ntion, the performance of ICL is usually less competitive, as\nseen in [Sun et al., 2022b; Sunet al., 2022a; Xuet al., 2022;\nGao et al., 2021] and also in our experiments (cf. §4.3).\nThis paper aims to waive the above preconditions re-\nquired by the existing DFO-based prompt tuning methods,\nand presents a novel genetic algorithm (GA)[Mitchell, 1998]\nthat generates discrete prompts from the ground up. The\nproposed method, named Generic Algorithm for Predictive\nProbability guided Prompting (GAP3) 1, works as follows.\nFirstly, discontinuous prompt chunks are considered as chro-\nmosomes, with prompt tokens being genes. Then, starting\nfrom an empty one, GAP3 evolves the prompts via chro-\nmosome crossovers and gene mutations. At each mutation\nstep, either a new mask token is inserted into a random chro-\nmosome at a random position, or a random existing gene is\nmasked. After this, the masked slot will be filled by a to-\nken that approximately maximises the predictive probability\nof the ground-truth labels on a (few-shot) training set. The\nalgorithm iterates for a predefined number of steps, with indi-\nviduals consisting of diverse chromosomes/genes competing\nto survive and breed, according to their fitness scores com-\nputed on the training set.\nIn comparison to existing DFO-based prompting methods,\nthe major advantages of the proposed GAP3 are three-fold:\n1. No extra APIs for vector injections are required, since\nGAP3 searches for discrete prompts.\n2. No manual prompts are required, as GAP3 is initialised\nwith an empty prompt.\n3. For backbone PLMs that yield predictive probabili-\nties for masked tokens, e.g. masked language mod-\nels (MLMs) [Devlin et al., 2019; Liu et al., 2019] or\nT5-style encoder-decoder networks [Raffel et al., 2020;\nLewis et al., 2020], GAP3 generates prompt tokens di-\nrectly based on the backbone itself, without needing an\nauxiliary LM, which further reduces preconditions for\nits applications. (If a casual LM [Radford et al., 2018;\nRadford et al., 2019] is the backbone of interest, GAP3\nwill need an auxiliary MLM for token generation (cf.\n§4.3), where the MLM can also be a black-box model.)\nTaking RoBERTaLARGE [Liu et al., 2019] and GPT-2LARGE\n[Radford et al., 2019] as the backbone PLMs of interest, re-\nspectively, the performance of GAP3 is evaluated on 7 bench-\nmark datasets (the same as those used in [Sun et al., 2022b]),\nin comparison with that of the existing counterparts, includ-\ning BBT [Sun et al., 2022b], genetic prompt search (GPS)\n[Xu et al., 2022], gradient-free instructional prompt search\n(GRIPS) [Prasad et al., 2022 ], as well as ICL [Brown et\nal., 2020]. The experimental results prove the effectiveness\n1Code and supplementary material available at: https://github.\ncom/zjjhit/gap3\nof the proposed GAP3, where it outperforms all the DFO-\nstyle baselines, achieving at least 2.9% and 2.4% absolute\nimprovements in the average scores for RoBERTaLARGE and\nGPT-2LARGE, respectively.\n2 Related Work\nParameter-efficient tuning (PET). PET reduces the cost\nof adapting a large PLM to downstream tasks, by tuning only\na small proportion of the parameters instead of the full model\n[Houlsby et al., 2019; Pfeiffer et al., 2020]. Prompt tun-\ning [Lester et al., 2021; Liu et al., 2022; Liu et al., 2021a;\nLiu et al., 2021b; Qin and Eisner, 2021] form a sub-direction\nof PET, where the tunable parameters are the injected soft\nprompts. Despite the success of PET methods, they are not\nsuitable for the growing trend of LM-as-a-Service deploy-\nments, as gradient-based optimisations are required.\nDiscrete prompt search. Discrete prompts are more\npreferable in black-box scenarios, since no model-level mod-\nifications are involved. Manually created intuitive prompts\nwere found helpful in earlier research [Petroni et al., 2019;\nSchick and Sch ¨utze, 2021b; Schick and Sch ¨utze, 2021a ],\nbut they are suboptimal in a general sense. For methods\nthat searches for prompts automatically, paraphrasing is a\ncommonly used methodology to expand the existing prompt\nset (usually initialised with manual prompts) for succeeding\nsearch heuristics to winnow [Xu et al., 2022; Prasad et al.,\n2022]. In addition, Hou et al. [2022] proposed to ensemble\nprompts via boosting. But such ensemble multiplies the in-\nference cost at the same time. Reinforcement learning (RL)\nhas recently been employed by Deng et al.[2022] and Diao\net al.[2023] to optimise discrete prompts for black-box back-\nbones. It’s worth noting that the method proposed in Deng et\nal. [2022] involves action explorations in a considerably large\nprompt space, which results in significantly more API calls\nto train the model than its counterparts (such as BBT [Sun\net al., 2022b]). Other proposed methodologies include min-\ning prompt templates from the web [Jiang et al., 2021] and\ntraining specific prompt generators [Ben-David et al., 2022],\nwhich correspond to extra computational and human efforts.\nHybrid methods. It is also possible to tune a backbone\nmodel based on both discrete prompts and differentiable pa-\nrameters, of which typical examples include AutoPrompt\n[Shin et al., 2020 ] who search for discrete prompt tokens\nbased on gradients, and LM-BFF [Gao et al., 2021] that com-\nbines automatic prompt generation and model fine-tuning.\nMethods of this kind should be considered as more compa-\nrable to PET, as they violate the black-box assumption.\n3 Methodology\n3.1 Prompt Template\nAssume that the downstream task is to classify the input text\n[X] to a label [Y]. A prompt template here means a permu-\ntation to arrange [X], [Y] and the prompt [T], e.g. tem-\nplate ‘[X][T][Y]’ stands for placing the prompt between\nthe input text and the label. We further assume that a task’s\ninput may consist of multiple text pieces (e.g. when clas-\nsifying a pair of sentences) and prompt chunks are allowed\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5297\nto be placed at multiple positions. Therefore, we gener-\nalise the above template representation with subscripts, e.g.\n‘[X1][T1][X2][T2][Y]’ means two prompt chunks in-\nserted between two text pieces and between the second text\npiece and the label, respectively.\n3.2 Genetic Algorithm for Prompting\nFor a given downstream task and a training set, the proposed\nGAP3 works as follows. Firstly, we define a prompt template\n(cf. §3.1). The template here only decides a permutation of\nthe text pieces. The actually prompt chunks are initialised as\nempty sets at this stage. To interpret the process in GA-style,\nwe regard each prompt chunk [Ti] as a chromosome, where\nthe actual tokens to be filled into it are regarded as genes. In\naddition, we call entire prompts with diverse chromosomes\nindividuals. After this, we generate the first population of N\nindividuals by randomly mutate one gene of the initial empty\nindividual, where N is a predefined hyperparameter. Con-\ncretely, in this very first step, the mutation means randomly\nselecting a chromosome (that is blank), and inserting a token\nto it. Then the algorithm iterates as follows.\n1. Evaluate each individual on the training set, to obtain a\nfitness score;\n2. Keep top\n√\nN most fit individuals, called elites;\n3. Randomly draw pairs of individuals from the elites to\nperform (probabilistic) chromosome crossovers, until a\nnew population of size N is yielded.\n4. Mutate each new individual’s gene (probabilistically) by\nrandomly inserting a new token or replacing a random\nexisting token with a new one;\nThe above process will be repeated for M steps before the\nbest individual being chosen as the final output, where M is\nalso a predefined hyperparameter.\nElite selection. Besides ranking the current population of\nindividuals according to their fitness scores, we also maintain\nthe overall best individual achieved in the previous iterations.\nIf the ‘best so far’ individual surpasses the current elites, we\nwill add it into the current elite group (while dropping the\ntail elite). This is to avoid good genes being lost during the\ncrossovers and mutations.\nCrossover. Elite pairs are drawn in a weighted roulette\nwheel manner, with respect to their fitness scores. After\nthis, for each [Ti], with probability ρc, we exchange the\ncorresponding chromosomes between an elite pair, to yield\ntwo new individuals. The hyperparameter ρc is called the\ncrossover probability.\nMutation. With probability ρm (called the mutation prob-\nability), we mutate an individual. When this happens, we\nrandomly draw an action between ‘insert’ or ‘replace’ with\neven probabilities. If the ‘insert’ action is applied, a mask\ntoken will be inserted at a random position of a random chro-\nmosome. Otherwise, a random existing gene (of a random\nchromosome) will be masked (by replacing that token with a\nmask token). This implies, in either way, there will be one\nmask token in the current prompt. Then, LM probabilities\nwill be exploited to realise that mask to a real token.\nAlgorithm 1 GAP3\nInput:\nD – training set\nT0 – initial (empty) template\nLM – backbone model\nHyperparameter:\nM – number of iterations\nN – population size\nρc – crossover probability\nρm – mutation probability\nOutput: T* – the most fit prompt\n1: G ← { },n ← int(\n√\nN), T* ← NULL\n2: for 1 . . . Ndo\n3: T ← T0\n4: T ← mutate(T, D, LM, prob=1.0)\n5: G ← G ∪ {T}\n6: end for\n7: for 1 . . . Mdo\n8: G ← sort( G, key=fitness(G, D, LM) )\n9: if better than(G[0], T*) then\n10: T* ← G[0], E ← G[0 : n]\n11: else\n12: E ← {T*} ∪G[0 : n − 1]\n13: end if\n14: G′ ← { }\n15: while |G′| < Ndo\n16: T, T′ ← roulette( E, weights=fitness(E, D, LM) )\n17: T, T′ ← crossover(T, T′, prob=ρc)\n18: T ← mutate(T, D, LM, prob=ρm)\n19: T′ ← mutate(T′, D, LM, prob=ρm)\n20: G′ ← G′ ∪ {T, T′}\n21: end while\n22: G ← G′\n23: end for\n24: return T*\nAlgorithm 1 gives the pseudo-code of the proposed GAP3,\nwhere hyperparameters and constant objects are denoted in\nitalic type. We leave the detailed explanations of the token\nselection process and the consequential design of the fitness\nfunction to §3.3 and §3.4, respectively, to keep the discus-\nsions on the main algorithm coherent here.\n3.3 Mutation Guided by LM Probabilities\nNotations. Let (x, y) denote a data example, wherex is the\ninput token sequence, and y is the label (token). Without loss\nof generality, for tasks involvingm pieces of text as input, we\nwill let x := (x1, . . . ,xm), with eachxi consisting of a token\nsequence. Then a prompt T can be regarded as a function that\napplies the prompt tokens to the data point (x, y), according\nto its associated template, to obtain a final token sequence, as\nT (x, y). We use ti to denote the token indexed by i in T ,\nand let T\n#\ni stand for the prompt with ti replaced by a mask\ntoken. We also use t\n#\ni to refer the i-indexed masked token.\nSimilarly, (x, y#) stands for the data point its labely masked.\nFirstly, assume we have an ‘ideal’ LM that satisfies the\nBayes’ rule. Then, for an arbitrary data point and an arbi-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5298\ntrarily masked token t\n#\ni , we would have:\nP(t\n#\ni = t|T\n#\ni (x, y#), y# = y|\n{z }\nT #\ni (x,y)\n)P(y# = y|T\n#\ni (x, y#))\n= P(y# = y|T\n#\ni (x, y#), t\n#\ni = t| {z }\nT #\ni←t(x,y#)\n)P(t\n#\ni = t|T\n#\ni (x, y#))\n(1)\nwhich yields:\nP(y# = y|T\n#\ni←t(x, y#))\n= P(t\n#\ni = t|T\n#\ni (x, y))P(y# = y|T\n#\ni (x, y#))\nP(t\n#\ni = t|T\n#\ni (x, y#))\n(2)\nwhere P(·|·) stands for the conditional probability given by\nthe LM, and T\n#\ni←t stands for the prompt obtained by substi-\ntuting the masked token t\n#\ni with token t. We call Eq. 1 an\n‘ideal’ assumption, because it holds if and only if the prob-\nabilities here are ground truth probabilities, which will not\nbe achievable in practice (since general PLMs are not trained\nsubject to such a constraint to satisfy the Bayes’ rule).\nTherefore, to make the equation valid, we introduce a bias\nitem λ and reformulate Eq. 2 in logarithmic form, as:\nlogP(y|t)(y# = y|T\n#\ni←t(x, y#))\n= logP(t|y)(t\n#\ni = t|T\n#\ni (x, y))\n+ logP(y|∗)(y# = y|T\n#\ni (x, y#))\n−log P(t|∗)(t\n#\ni = t|T\n#\ni (x, y#)) − λ(x, y, t,T\n#\ni )\n(3)\nwhere we name the probabilities in the form of P(·|·), for the\nease of reference in future discussions. Now, recall the mu-\ntation process in §3.2. Given an arbitrary prompt T\n#\ni with a\nmasked token, and a training set D, one would want to un-\nmask T\n#\ni by seeking the token ˆt that maximising the label\nposterior on D, as:\nˆt = arg max\nt∈V\nX\n(x,y)∈D\nlog P(y|t)(y# = y|T\n#\ni←t(x, y#)) (4)\nwhere V is the vocabulary of the LM. If we omit the biases\n(λs), P(y|t) can be computed tractably based on right-hand\nside of Eq. 3, which implies invoking the LM twice for each\ntraining example (x, y), by feeding it with T\n#\ni (x, y#) and\nT\n#\ni (x, y), respectively. However, the biases (λs) here are\nindispensable, while computing them for all possible data-\nprompt-token-mask combinations is obviously intractable.\nHence, we design a heuristic to address this, as follows.\nLet ¯λ(t) := max (x,y)∈D,T #\ni\nλ(x, y, t,T\n#\ni ). If we replace\nλ(x, y, t,T\n#\ni ) in Eq. 3 with ¯λ(t), the left-hand side be-\ncomes a lower bound of the original log P(y|t). Firstly, we\ninitialise ¯λ(t) as 0 for all t ∈ V. We define log ¯P(y|t) :=\nlog P(t|y) + logP(y|∗) −log P(t|∗) − ¯λ(t) (represented in sim-\nplified notations). Then, at each time a prompt is mutated, we\nperform a two-step update, as:\nˆt ← arg max\nt∈V\nX\nD\nlog ¯P(y|t); (5)\n¯λ(ˆt) ← max\nh\n¯λ(ˆt), max\nD\n\u0010\nlog ¯P(y|ˆt) − log P(y|ˆt)\n\u0011i\n.\n(6)\nwhere log P(y|ˆt) is also computed based on the LM. Note\nhere, P(y|ˆt) essentially evaluates the performance of the ob-\ntained prompt on the training data, as is also used by the fit-\nness function (cf. §3.4). Therefore, computing it is an in-\nevitable effort, instead of an extra cost.\nRemark on ¯λ. A more intuitive explanation of ¯λ(t)’s func-\ntion is that, it penalises those tokens who tend to occur re-\npeatedly but will overestimate the predictive probabilities of\nthe labels.\nRemark on Eq. 5. In practice, always adopting the top-1\ntoken ˆt may yield duplicated prompts (especially when gener-\nating the initial population (cf. §3.2)). Therefore, we actually\ncollect top-n tokens based on P\nD log ¯P(y|t). Then, starting\nfrom the first one, we examine whether the prompt unmasked\nbased on the current token has already been seen previously.\nIf yes, we move to the next token, until an unseen consequen-\ntial prompt is obtained.\nMasked LM vs. casual LM. The most elegant part of the\nproposed GAP3 is that, if the backbone PLM is an MLM\n(or a T5-like encoder-decoder network [Raffel et al., 2020;\nLewis et al., 2020 ]), all the four predictive probabilities\nP(t|y), P(y|∗), P(t|∗) and P(y|t) can be obtained from the back-\nbone model itself. Nevertheless, if the backbone is a casual\nLM who can only predict P(y|t), we can use an auxiliary\nMLM to computeP(t|y), P(y|∗) and P(t|∗), in which case,¯λ(t)\nto a great extent prevents the algorithm repeatedly generating\ntokens highly biased to the auxiliary.\n3.4 Fitness Function\nFor a given task, the actual objective metric (such as accu-\nracy or F1-score) on the training set will be a straightforward\nmeasure of the fitness for those individuals yielded in the GA.\nHowever, in the few-shot learning case, it will be very easy\nto have many individuals achieving a same metric score. Too\nmany indistinguishable individuals occurring in a population\nmay result in less chance of breeding given to those poten-\ntially more competitive genes.\nTherefore, in GAP3, we actually make the fitness mea-\nsure two-dimensional. The task-specific objective metric is\nthe dominant fitness. If (and only if) two individuals have an\nequal score in the dominant fitness, we further compare them\naccording to a secondary fitness. The secondary fitness score\nis computed as:\nF2nd(T ) = 1\n|D|\nX\n(x,y)∈D\nδy,ˆy\nP(y# = y|T (x, y#))P\ny′∈Y P(y# = y′|T (x, y#))\nˆy = arg max\ny′∈Y\nP(y# = y′|T (x, y#)) (7)\nwhere Y denotes the task’s label set, δ·,· is the Kronecker\ndelta function. F2nd means that we renormalise the pre-\ndictive probabilities on the label set, and average over the\n‘hinge’ probabilities, where incorrectly predicted examples\ncontribute zero values. Note here, in the weighted roulette\nwheel selection process, we only use F2nd scores for the\nweights, as they are more distinguishable and partially re-\nflects the classification accuracy.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5299\n4 Experiments\nWe conduct a group of main comparative experiments and\nan ablation study. §4.1, §4.2 and §4.3 describe the settings,\nbaselines and results for the main experiments, respectively.\nThose for the ablation study is presented in §4.4, specifically.\n4.1 Settings\nDatasets. The datasets used in the main experiments con-\nsist of 7 benchmark NLP tasks, which are the same as in[Sun\net al., 2022b], including Yelp polarity, AG’s News and DBPe-\ndia from [Zhang et al., 2015], SST-2, MRPC and RTE from\nthe GLUE benchmarks [Wang et al., 2018], as well as SNLI\n[Bowman et al., 2015]. The experiments are in ak-shot learn-\ning setting, where for each task, we randomly sample k = 16\nexamples for each label from the original training set. For\nSST-2, MRPC and RTE, we use their development sets as the\ntest sets. For the other tasks, we use the original test sets. For\nMRPC, F1 is used as the evaluation metric, while accuracy is\nthe metric for all the other tasks.\nBackbone PLMs. We choose RoBERTaLARGE [Liu et al.,\n2019] and GPT-2 LARGE [Radford et al., 2019 ] as back-\nbones, conducting two groups of experiments, respectively.\nFor the GPT-2LARGE backbone, we use RoBERTa LARGE and\nBERTBASE (cased) [Devlin et al., 2019], respectively, as the\nauxiliaries for GAP3 (cf. §3.3).\nHyperparameters for GAP3. We set GAP3’s population\nsize N = 64 and iteration number M = 50, with crossover\nand mutation probabilities ρc = 0 .5 and ρm = 0 .75, re-\nspectively. For the RoBERTa LARGE backbone, the above\nsettings result in M × N × (1 + 2ρm) = 8000 expected\nnumber of API calls (one call for fitness plus two calls for\nmutation (with probability ρm) per individual per data ex-\nample). For the GPT-2 LARGE backbone, the same settings\ncorrespond to 3200 API calls to GPT-2 LARGE and 4800 to\nRoBERTaLARGE/BERTBASE. The prompt templates and label\nwords for GAP3 on each task can be found in appendix A.\n4.2 Baselines\nWe choose the following existing DFO-based prompting\nmethods as baselines. The implementations of the baselines\nare all based on the original source code provided by their\nauthors.\nBBT. BBT [Sun et al., 2022b] requires for an additional k-\nshot development set, which is also randomly sampled from\nthe original training sets of the tasks, without overlapping\nwith the k-shot training examples. BBT’s budget for API\ncalls is set to 8000 (the same as in [Sun et al., 2022b]), with\nthe prompt length 50.\nGPS. GPS [Xu et al., 2022] is another GA-based prompt-\ning method that evolves by using T5 11B [Raffel et al., 2020]\nto paraphrase the prompts. As GPS requires multiple manual\nprompts to initialise the first population, we only conduct ex-\nperiments for it on Yelp polarity, AG’s News, SNLI and RTE.\nWe use the manual prompts presented in[Schick and Sch¨utze,\n2022] to initialise the Yelp polarity and AG’s News experi-\nments. Experiments on SNLI are initialised with the ANLI\nmanual prompts presented in [Sanh et al., 2022]. Sanh et\nal. [2022] also provides a manual prompt set for RTE, which\nis directly adopted here. We set GPS’ population size to 25\nand number of iterations to 10.\nGRIPS. GRIPS [Prasad et al., 2022 ] is also a heuristic\nsearch based prompting method, which evolve prompts based\non PEGASUS [Zhang et al., 2020] and generic token-level\nedits. GRIPS requires manual instructions to initialise. For\nthe GPT-2LARGE experiments, we initialise it based on the\nNatural-Instructions dataset [Mishra et al., 2022], and set the\niteration number to 50 with 100 candidates generated per iter-\nation. However, based on the same settings, we failed to ob-\ntain any reasonable results for the RoBERTaLARGE backbone.\nGRIPS either fails to find a valid update or yields results no\nbetter than chance. (This may be because either GRIPS itself\nor the initial instructional manual prompt is unsuitable for an\nMLM backbone.) Therefore, we omit the comparison with\nGRIPS for the RoBERTaLARGE backbone.\nIn addition, we also compare our GAP3’s performance\nwith that of ICL [Brown et al., 2020], manual prompts,\ngradient-based prompt tuning (PT) [Li and Liang, 2021] as\nwell as full-model fine-tuning (FT) . For ICL, we concate-\nnate the k-shot training examples in a random order (but with\na balanced label distribution) to form a prefix prompt for the\ninput. In regard to manual prompts, for Yelp polarity, AG’s\nNews, SNLI and RTE, we test all the available prompts and\nchoose the best score. For the other tasks, we just use the\nsimple prompt templates given in [Sun et al., 2022b]. For PT\nand full-model FT, Adam optimisers[Kingma and Ba, 2015]\nare employed. For PT, with learning rate 5e-4 and batch size\n16, it runs for 1000 epochs. For full-model FT, with the same\nbatch size, but learning rate 1e-5, we run it for 200 epochs.\nWe did not choose BBTv2 [Sun et al., 2022a] as a baseline\nin this work, because we consider hidden-state injection as a\nmuch stronger violation to the black-box assumption, which\nwill be unfair to other methods.\nLabel words. We use label words slightly different from\nthose in [Sun et al., 2022b]. Because in our case, we intu-\nitively expect the label words more substitutable to each other\nfrom natural language point of view. In order to eliminate the\nbias in the experimental results caused by this difference, for\nBBT and ICL, we experiment them with both the label words\nin [Sun et al., 2022b] and ours, and choose the better results\nobtained. For GPS and GRIPS, the label words are embedded\nin their initial manual prompts, which we keep unchanged.\nFairness of resources used. For the RoBERTaLARGE back-\nbone, our GAP3 and BBT are compared under the same API\ncall budget. However, BBT uses an additional k-shot devel-\nopment set, which means doubling the number of labelled\nexamples required. If we assume the general cost of an API\ncall to be linear to the scale of the model behind, for the GPT-\n2LARGE backbone, GAP3 would be much more cost-efficient\nthan BBT, as 60% of its API calls are spent on the auxiliary\nRoBERTaLARGE/BERTBASE that is much smaller than GPT-\n2LARGE. Based on the same assumption, we can consider the\nresources used by GAP3, GPS and GRIPS as approximately\ncomparable.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5300\nMethod SST-2\nacc\nYelp P.\nacc\nAG’s News\nacc\nDBPedia\nacc\nMRPC\nf1\nSNLI\nacc\nRTE\nacc Avg.\nGradient-based methods\nPT [Li and Liang, 2021] 78.3(8.4) 91.7(1.2) 79.8(0.6) 88.5(0.7) 55.0(5.3) 39.3(2.2) 52.0(1.9) 69.2\nFull-model FT 89.6(0.7) 96.2 (0.4) 87.7 (0.7) 98.0 (0.7) 73.7(9.9) 74.6 (4.2) 69.1 (1.5) 84.1\nGradient-free methods\nManual 79.7 92.6 79.2 41.3 80.2 36.0 51.6 65.8\nICL [Brown et al., 2020] 70.1(9.6) 53.1(0.6) 62.7(14.0) 39.4(8.4) 49.1(5.1) 35.8(2.2) 48.9(4.5) 51.3\nGPS [Xu et al., 2022] – 87.5(1.2) 76.3(3.5) – – 37.7(2.1) 51.9(4.5) 63.3\nBBT [Sun et al., 2022b] 88.9(1.5) 91.4(1.3) 82.5(0.7) 79.8(2.0) 63.9(2.9) 44.7(1.0) 49.7(2.0) 71.5\nGAP3 89.7(2.8) 93.0(2.3) 83.2(3.2) 83.7(2.9) 70.2(4.5) 51.1(4.6) 49.7(1.5) 74.4\nTable 1: Experimental results for the RoBERTaLARGE backbone. All the numbers are percentage numbers with ’%’ omitted. The mean and\nstandard deviation computed based on 3 different splits. Bold results are the best ones in the gradient-free group. Underlined results are the\noverall best ones in both groups.\nMethod SST-2\nacc\nYelp P.\nacc\nAG’s News\nacc\nDBPedia\nacc\nMRPC\nf1\nSNLI\nacc\nRTE\nacc Avg.\nGradient-based methods\nPT [Li and Liang, 2021] 82.0(0.6) 82.0(7.0) 82.1(1.5) 95.9(0.2) 65.1(8.5) 46.3(3.7) 53.1(5.0) 72.4\nFull-model FT 86.4(5.5) 95.0 (0.2) 88.1 (1.1) 97.9 (0.1) 69.8(12.3) 56.6 (2.8) 54.3(3.3) 78.3\nGradient-free methods\nManual 61.4 60.2 78.7 48.5 38.9 38.7 57.0 54.8\nICL [Brown et al., 2020] 48.1(0.9) 63.0(9.8) 51.2(13.9) 37.9(9.7) 52.2(12.6) 37.6(2.6) 53.9(2.4) 49.2\nGRIPS [Prasad et al., 2022] 75.8(1.5) 79.0(1.0) 68.1(3.0) 75.7(3.3) 61.7(1.1) 37.1(1.7) 52.1(2.5) 64.2\nGPS [Xu et al., 2022] – 89.7(4.5) 73.7(1.2) – – 37.5(1.8) 53.7(4.1) 63.6\nBBT [Sun et al., 2022b] 76.8(2.8) 84.4(4.3) 77.3(2.2) 79.8(2.0) 69.3(2.7) 42.1(1.0) 51.5(2.7) 68.8\nGAP3 + RoBERTaLARGE 79.7(5.3) 90.2(3.6) 82.4(1.7) 80.1(7.1) 71.3(3.3) 41.3(1.8) 53.4(4.0) 71.2\nGAP3 + BERTBASE 82.6(4.9) 89.9(4.0) 80.7(1.7) 81.7(0.7) 72.3(5.1) 37.2(2.2) 49.3(1.8) 70.5\nTable 2: Experimental results for the GPT-2 LARGE backbone. All the numbers are percentage numbers with ’%’ omitted. The mean and\nstandard deviation computed based on 3 different splits. Bold results are the best ones in the gradient-free group. Underlined results are the\noverall best ones in both groups.\n4.3 Results\nExperimental results with RoBERTa LARGE and GPT-2LARGE\nas the backbones are shown in Table 1 and 2, respectively. It\ncan be found that in both scenarios, the proposed GAP3 out-\nperforms the other baselines in the gradient-free group with\na notable margin. Interestingly, for the GPT-2 LARGE back-\nbone, the BERTBASE (110M) auxiliary works almost as good\nas the RoBERTaLARGE (354M) auxiliary. Although showing a\nslightly lower average score, the former achieves even higher\nscores on SST-2, DBPedia and MRPC than the latter. This\nalso suggests that GAP3’s dependence on a particular auxil-\niary MLM is weak. In addition, GAP3 surpasses gradient-\nbased PT for RoBERTaLARGE, and achieves an average score\nclose to gradient-based PT for GPT-2LARGE.\nHowever, full-model FT still appears to be the most com-\npetitive paradigm. Despite the capability to tune black-box\nbackbones, none of the methods in the gradient-free group\nachieves an overall score comparable to full-model FT. Sim-\nilar findings were also indicated in a recent study [Chen et\nal., 2022] particularly designed to analyse this aspect. In ad-\ndition, on RTE, none of the gradient-free methods performs\nnotably better than chance, which indicates the existence of\nparticular problems that are more difficult for DFO-style al-\ngorithms to solve.\n4.4 Ablation Study\nAblation experiments are conducted based on the SST-2\nand AG’s News datasets and the RoBERTa LARGE backbone,\nwhere we vary one hyperparameter, while keeping the oth-\ners fixed. The default hyperparameter values are the same as\nthose in §4.1, except that we use 32-shot learning by default\nin this section, to reduce the variance over 3 different runs.\nk-shot. We increase the number of training examples per\nlabel (i.e. k), with k being 16, 32, 64 and 128, respectively,\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5301\nFigure 1: Ablation study on number of training examples (per label).\nand plot the corresponding performance of GAP3 in Figure 1.\nIt can be found that the obtained accuracy scores grow with\nk on both dataset sets. Furthermore, up to k = 128 , there\nis no clear trend of convergence appearing, which suggests\nthat GAP3 has the potential capability of learning from larger\ntraining sets.\nAPI call budget. The expected number of API calls for\nGAP3 is jointly determined by three hyperparameters, the\npopulation size (N), the number of iterations (M) and the\nmutation probability (ρm). We plot the ablation results of\nthe above hyperparameters together in Figure 2, against the\nnumber of API calls they yield. It is understandable that more\nAPI calls normally correspond to better results. However, due\nto the randomness in GAP3’s strategies, exceptions may oc-\ncur by chance, where a setting with fewer API calls happen\nto outperform those with more API calls.\nOther hyperparameters. As shown in Figure 3, GAP3 is\nto some extent sensitive to the crossover probability. It sug-\ngests that some further heuristics would need to be designed\nin the future, to seek an optimal value for ρc. In addition, we\nalso experiment with an alternative secondary fitness func-\ntion (cf. §3.4), which is obtained by omitting the Kronecker\ndelta in Eq. 7. The results indicate that doing so will re-\nduce the mean accuracy by 1.0% on SST-2 and 0.5% on AG’s\nNews.\nFigure 2: Ablation study on expected number of API calls, with re-\nspect to iteration number, population size and mutation probability.\n5 Further Discussions\nLabel word selection. The label words in this work are\nmanually assigned. Preliminary attempts were made to\nsearch for label words automatically, based on the method\nproposed in [Gao et al., 2021], which, however, resulted in\nserious overfitting. Better strategies to gain label words for\nGAP3 will be addressed in our feature research.\nPrompt length. In GAP3, prompt length is not predefined,\nbut the iteration number hyperparameter will determine an\nupper threshold of the maximum possible prompt length. In\naddition, the template defined in §3.1 does not necessarily\nmean that every prompt slot[Ti] will have tokens in the end.\nThere are possibilities that the final survival individual has\nsome of its chromosomes remain empty. The above features\nmake the prompts generated by GAP3 more flexible and less\nhyperparameter-dependent.\nInterpretability of prompts. Generally speaking, the\nprompts generated by GAP3 are not understandable by hu-\nman, since it is not designed to gain human-readable text.\nNevertheless, one can still find some ‘keywords’ within the\nprompts interpretable. Example prompts learned in §4.3 can\nbe found in the supplementary material2.\n2https://github.com/zjjhit/gap3/blob/main/learned-prompts.pdf\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5302\nFigure 3: Ablation study on crossover probability.\nLimitation. Due to theM×N×(1+2ρm) API calls yielded\nduring GAP3’s evolutions, it will be computationally expen-\nsive to apply it directly to a full-sized training set. This is\nalso a common limitation of the existing DFO-based prompt-\ning methods. In our GAP3 case, reshaping the evolution and\nevaluation processes in a k-fold manner will possibly relieve\nthe computational complexity problem, which we will further\ninvestigate in future studies.\n6 Conclusion\nThis paper introduces GAP3, an LM probability guided GA,\nto search prompts automatically for black-box PLM back-\nbones. Despite its outstanding performance on diverse bench-\nmarks, the most significant superiority of GAP3 is the waiver\nof the preconditions required by existing DFO-based prompt-\ning methods, such as injection APIs or manual prompts. The\nzero or minimal dependency of GAP3 on additional resources\nsuggests it to be an out-of-box complementary to those LM-\nas-a-Service instances. The computational cost of applying\nit to full-sized training problems would be the current major\nlimitation of GAP3. Addressing this limitation will be one of\nour future research directions.\nA Prompt Templates and Labels for GAP3\nPrompt templates and label words (verbalisers) used in our\nGAP3 experiments for the RoBERTaLARGE and GPT-2LARGE\nTask Template Labels\nSST-2 [T1][X][T2][Y] bad, good\nYelp P. [T1][X][T2][Y] bad, good\nAG’s\nNews\n[X][T][Y]\nworld,\nsports,\nbusiness,\ntechnology\nDBPedia [T1][Y][T2][X]\ncompany,\neducation,\nartist, ath-\nlete, office,\ntransporta-\ntion, build-\ning, nature,\nvillage, ani-\nmal, plant,\nalbum, film,\nliterature\nMRPC [T1][X1][Y][T2][X2] No, Yes\nSNLI [T1][X1][Y][T2][X2]\nYes,\nMaybe,\nNo\nRTE [T1][X1][Y][T2][X2] Yes, No\nTable 3: Templates and labels for the RoBERTaLARGE backbone.\nTask Template Labels\nSST-2 [T1][X][T2][Y] –\nYelp P. [T1][X][T2][Y] –\nAG’s\nNews\n[X][T][Y] –\nDBPedia [T1][X][T2][Y] –\nMRPC [T1][X1][T2][X2][T3][Y] no, yes\nSNLI [T1][X1][T2][X2][T3][Y]\nalways,\nsome-\ntimes,\nnever\nRTE [T1][X1][T2][X2][T3][Y] true,\nfalse\nTable 4: Templates and labels for the GPT-2 LARGE backbone. ‘–’\nstands for labels identical to those in Table 3.\nbackbones are listed in Table 3 and 4, respectively.\nAcknowledgements\nThe first author thanks the members of the Algorithm and\nDevelopment Team at the Data Intelligence Centre of China\nMobile Online Services Co., Ltd. for fruitful discussions on\nthis work. The second author thanks XREAL for supporting\nthis research by providing the computing power.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5303\nReferences\n[Ben-David et al., 2022] Eyal Ben-David, Nadav Oved, and\nRoi Reichart. PADA: Example-based Prompt Learning for\non-the-fly Adaptation to Unseen Domains.Transactions of\nACL, 10:414–433, 2022.\n[Bowman et al., 2015] Samuel R. Bowman, Gabor Angeli,\nChristopher Potts, and Christopher D. Manning. A large\nannotated corpus for learning natural language inference.\nIn Proceedings of EMNLP, 2015.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learn-\ners. In Advances in Neural Information Processing Sys-\ntems, volume 33, 2020.\n[Chen et al., 2022] Guanzheng Chen, Fangyu Liu, Zaiqiao\nMeng, and Shangsong Liang. Revisiting parameter-\nefficient tuning: Are we really there yet? CoRR,\nabs/2202.07962, 2022.\n[Clark et al., 2020] Kevin Clark, Minh-Thang Luong,\nQuoc V . Le, and Christopher D. Manning. ELECTRA:\nPre-training text encoders as discriminators rather than\ngenerators. In Proceedings of ICLR, 2020.\n[Deng et al., 2022] Mingkai Deng, Jianyu Wang, Cheng-\nPing Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng\nSong, Eric Xing, and Zhiting Hu. RLPrompt: Optimiz-\ning discrete text prompts with reinforcement learning. In\nProceedings of EMNLP, 2022.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understand-\ning. In Proceedings of NAACL-HLT, 2019.\n[Diao et al., 2023] Shizhe Diao, Zhichao Huang, Ruijia Xu,\nXuechun Li, Yong Lin, Xiao Zhou, and Tong Zhang.\nBlack-box prompt learning for pre-trained language mod-\nels. Transactions on Machine Learning Research, 2023.\n[Gao et al., 2021] Tianyu Gao, Adam Fisch, and Danqi\nChen. Making pre-trained language models better few-\nshot learners. In Proceedings of ACL-IJCNLP, 2021.\n[Hansen et al., 2003] Nikolaus Hansen, Sibylle D. M ¨uller,\nand Petros Koumoutsakos. Reducing the time complex-\nity of the derandomized evolution strategy with covariance\nmatrix adaptation (CMA-ES). Evolutionary Computation,\n11(1):1–18, 2003.\n[Hou et al., 2022] Bairu Hou, Joe O’Connor, Jacob Andreas,\nShiyu Chang, and Yang Zhang. PromptBoosting: Black-\nbox text classification with ten forward passes. CoRR,\nabs/2212.09257, 2022.\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanis-\nlaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\nParameter-efficient transfer learning for NLP. In Proceed-\nings of the ICML, 2019.\n[Jiang et al., 2021] Zhengbao Jiang, Jun Araki, Haibo Ding,\nand Graham Neubig. How can we know when language\nmodels know? on the calibration of language models for\nquestion answering. Transactions of ACL, 9, 2021.\n[Kingma and Ba, 2015] Diederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. In Proceed-\nings of ICLR, 2015.\n[Kolda et al., 2003] Tamara G. Kolda, Robert Michael\nLewis, and Virginia Torczon. Optimization by direct\nsearch: New perspectives on some classical and modern\nmethods. SIAM Review, 45(3):385–482, 2003.\n[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah\nConstant. The power of scale for parameter-efficient\nprompt tuning. In Proceedings of EMNLP, 2021.\n[Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal,\nMarjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer. BART:\nDenoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension. In\nProceedings of ACL, 2020.\n[Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Prefix-\ntuning: Optimizing continuous prompts for generation. In\nProceedings of ACL-IJCNLP, 2021.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoBERTa: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692, 2019.\n[Liu et al., 2021a] Xiao Liu, Kaixuan Ji, Yicheng Fu,\nZhengxiao Du, Zhilin Yang, and Jie Tang. P-Tuning v2:\nPrompt tuning can be comparable to fine-tuning univer-\nsally across scales and tasks. CoRR, abs/2110.07602,\n2021.\n[Liu et al., 2021b] Xiao Liu, Yanan Zheng, Zhengxiao Du,\nMing Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\nunderstands, too. CoRR, abs/2103.10385, 2021.\n[Liu et al., 2022] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng\nTam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-Tuning:\nPrompt tuning can be comparable to fine-tuning across\nscales and tasks. In Proceedings of ACL (Volume 2: Short\nPapers), 2022.\n[Liu et al., 2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu,\nZhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\nPre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing. ACM\nComputing Surveys, 55(9):no.195:1–35, 2023.\n[Loshchilov and Hutter, 2019] Ilya Loshchilov and Frank\nHutter. Decoupled weight decay regularization. In Pro-\nceedings of ICLR, 2019.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5304\n[Mishra et al., 2022] Swaroop Mishra, Daniel Khashabi,\nChitta Baral, and Hannaneh Hajishirzi. Cross-task gener-\nalization via natural language crowdsourcing instructions.\nIn Proceedings of ACL, 2022.\n[Mitchell, 1998] Melanie Mitchell. An Introduction to Ge-\nnetic Algorithms. The MIT Press, 1998.\n[Ouyang et al., 2022] Long Ouyang, Jeff Wu, Xu Jiang,\nDiogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. Training lan-\nguage models to follow instructions with human feedback.\nCoRR, abs/2203.02155, 2022.\n[Petroni et al., 2019] Fabio Petroni, Tim Rockt ¨aschel, Se-\nbastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang\nWu, and Alexander Miller. Language models as knowl-\nedge bases? In Proceedings of EMNLP-IJCNLP, 2019.\n[Pfeiffer et al., 2020] Jonas Pfeiffer, Andreas R ¨uckl´e,\nClifton Poth, Aishwarya Kamath, Ivan Vuli ´c, Sebas-\ntian Ruder, Kyunghyun Cho, and Iryna Gurevych.\nAdapterHub: A framework for adapting transformers. In\nProceedings of EMNLP: System Demonstrations, 2020.\n[Prasad et al., 2022] Archiki Prasad, Peter Hase, Xiang\nZhou, and Mohit Bansal. GrIPS: Gradient-free, edit-based\ninstruction search for prompting large language models.\nCoRR, abs/2203.07281, 2022.\n[Qin and Eisner, 2021] Guanghui Qin and Jason Eisner.\nLearning how to ask: Querying LMs with mixtures of soft\nprompts. In Proceedings of NAACL-HLT, June 2021.\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan,\nTim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training. Technical report,\nOpenAI, 2018.\n[Radford et al., 2019] Alec Radford, Jeff Wu, Rewon Child,\nDavid Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical re-\nport, OpenAI, 2019.\n[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam\nRoberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits\nof transfer learning with a unified text-to-text transformer.\nJournal of Machine Learning Research, 21(140):1–67,\n2020.\n[Rios and Sahinidis, 2013] Luis Miguel Rios and Niko-\nlaos V . Sahinidis. Derivative-free optimization: a review of\nalgorithms and comparison of software implementations.\nJournal of Global Optimization, 56(3):1247––1293, 2013.\n[Sanh et al., 2022] Victor Sanh, Albert Webson, Colin Raf-\nfel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan\nDey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim,\nGunjan Chhablani, Nihal V . Nayak, Debajyoti Datta,\nJonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo\nManica, Sheng Shen, Zheng Xin Yong, Harshit Pandey,\nRachel Bawden, Thomas Wang, Trishala Neeraj, Jos\nRozen, Abheesht Sharma, Andrea Santilli, Thibault F´evry,\nJason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Bi-\nderman, Leo Gao, Thomas Wolf, and Alexander M. Rush.\nMultitask prompted training enables zero-shot task gener-\nalization. In Proceedings of ICLR, 2022.\n[Schick and Sch¨utze, 2021a] Timo Schick and Hinrich\nSch¨utze. Exploiting cloze-questions for few-shot text\nclassification and natural language inference. In Proceed-\nings of EACL, 2021.\n[Schick and Sch¨utze, 2021b] Timo Schick and Hinrich\nSch¨utze. Few-shot text generation with natural language\ninstructions. In Proceedings of EMNLP, 2021.\n[Schick and Sch¨utze, 2022] Timo Schick and Hinrich\nSch¨utze. True few-shot learning with Prompts—A\nreal-world perspective. Transactions of ACL, 10:716–731,\n2022.\n[Shin et al., 2020] Taylor Shin, Yasaman Razeghi, Robert L.\nLogan IV , Eric Wallace, and Sameer Singh. AutoPrompt:\nEliciting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of EMNLP,\n2020.\n[Sun et al., 2022a] Tianxiang Sun, Zhengfu He, Hong Qian,\nYunhua Zhou, Xuanjing Huang, and Xipeng Qiu. BBTv2:\nTowards a gradient-free future with large language models.\nIn Proceedings of EMNLP, 2022.\n[Sun et al., 2022b] Tianxiang Sun, Yunfan Shao, Hong Qian,\nXuanjing Huang, and Xipeng Qiu. Black-box tuning for\nlanguage-model-as-a-service. In Proceedings of ICML,\n2022.\n[Wang et al., 2018] Alex Wang, Amanpreet Singh, Julian\nMichael, Felix Hill, Omer Levy, and Samuel Bowman.\nGLUE: A multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, 2018.\n[Xu et al., 2022] Hanwei Xu, Yujun Chen, Yulun Du, Nan\nShao, Yanggang Wang, Haiyu Li, and Zhilin Yang. GPS:\nGenetic prompt search for efficient few-shot learning.\nCoRR, abs/2210.17041, 2022.\n[Yu and Gen, 2010] Xinjie Yu and Mitsuo Gen. Introduction\nto Evolutionary Algorithms. Springer, 2010.\n[Zhang et al., 2015] Xiang Zhang, Junbo Zhao, and Yann\nLeCun. Character-level convolutional networks for text\nclassification. In Advances in Neural Information Process-\ning Systems, volume 28, 2015.\n[Zhang et al., 2020] Jingqing Zhang, Yao Zhao, Mohammad\nSaleh, and Peter Liu. PEGASUS: Pre-training with ex-\ntracted gap-sentences for abstractive summarization. In\nProceedings of ICML, 2020.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5305",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8272987604141235
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7195891737937927
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6017544269561768
    },
    {
      "name": "Language model",
      "score": 0.5706124305725098
    },
    {
      "name": "Security token",
      "score": 0.5670574307441711
    },
    {
      "name": "Process (computing)",
      "score": 0.5020155906677246
    },
    {
      "name": "Embedding",
      "score": 0.46536970138549805
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44965577125549316
    },
    {
      "name": "Machine learning",
      "score": 0.43735480308532715
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4317840039730072
    },
    {
      "name": "Black box",
      "score": 0.42511290311813354
    },
    {
      "name": "Programming language",
      "score": 0.07434341311454773
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}