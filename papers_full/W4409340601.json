{
  "title": "Enhancing the Readability of Online Patient Education Materials Using Large Language Models: Cross-Sectional Study",
  "url": "https://openalex.org/W4409340601",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2001016007",
      "name": "John Will",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Mahin Gupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131876550",
      "name": "Jonah Zaretsky",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5115406768",
      "name": "Aliesha Dowlath",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2887503932",
      "name": "Paul Testa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3112684172",
      "name": "Jonah Feldman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2595038737",
    "https://openalex.org/W2576167606",
    "https://openalex.org/W2063478977",
    "https://openalex.org/W2997474044",
    "https://openalex.org/W1963769263",
    "https://openalex.org/W2045241688",
    "https://openalex.org/W4295233302",
    "https://openalex.org/W2765803284",
    "https://openalex.org/W3042167699",
    "https://openalex.org/W1967390364",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W2783929218",
    "https://openalex.org/W2167596180",
    "https://openalex.org/W2064058875",
    "https://openalex.org/W2267595383",
    "https://openalex.org/W4391957412",
    "https://openalex.org/W1996409699",
    "https://openalex.org/W2792805190",
    "https://openalex.org/W4384010535",
    "https://openalex.org/W2621009456",
    "https://openalex.org/W4386845378",
    "https://openalex.org/W4390771646",
    "https://openalex.org/W4392643126",
    "https://openalex.org/W4404467874",
    "https://openalex.org/W4367307887",
    "https://openalex.org/W4386022860",
    "https://openalex.org/W4400385109",
    "https://openalex.org/W4401871516",
    "https://openalex.org/W4400974494",
    "https://openalex.org/W1966710786",
    "https://openalex.org/W4240903751",
    "https://openalex.org/W4253307708",
    "https://openalex.org/W4391540882",
    "https://openalex.org/W4250789630"
  ],
  "abstract": "Background Online accessible patient education materials (PEMs) are essential for patient empowerment. However, studies have shown that these materials often exceed the recommended sixth-grade reading level, making them difficult for many patients to understand. Large language models (LLMs) have the potential to simplify PEMs into more readable educational content. Objective We sought to evaluate whether 3 LLMs (ChatGPT [OpenAI], Gemini [Google], and Claude [Anthropic PBC]) can optimize the readability of PEMs to the recommended reading level without compromising accuracy. Methods This cross-sectional study used 60 randomly selected PEMs available online from 3 websites. We prompted LLMs to simplify the reading level of online PEMs. The primary outcome was the readability of the original online PEMs compared with the LLM-simplified versions. Readability scores were calculated using 4 validated indices Flesch Reading Ease, Flesch-Kincaid Grade Level, Gunning Fog Index, and Simple Measure of Gobbledygook Index. Accuracy and understandability were also assessed as balancing measures, with understandability measured using the Patient Education Materials Assessment Tool-Understandability (PEMAT-U). Results The original readability scores for the American Heart Association (AHA), American Cancer Society (ACS), and American Stroke Association (ASA) websites were above the recommended sixth-grade level, with mean grade level scores of 10.7,10.0, and 9.6, respectively. After optimization by the LLMs, readability scores significantly improved across all 3 websites when compared with the original text. Compared with the original website, Wilcoxon signed rank test showed ChatGPT improved the readability to 7.6 from 10.1 (P&lt;.001); Gemini, to 6.6 (P&lt;.001); and Claude, to 5.6 (P&lt;.001). Word counts were significantly reduced by all LLMs, with a decrease from a mean range of 410.9-953.9 words to a mean range of 201.9-248.1 words. None of the ChatGPT LLM-simplified PEMs were inaccurate, while 3.3% of Gemini and Claude LLM-simplified PEMs were inaccurate. Baseline understandability scores, as measured by PEMAT-U, were preserved across all LLM-simplified versions. Conclusions This cross-sectional study demonstrates that LLMs have the potential to significantly enhance the readability of online PEMs while maintaining accuracy and understandability, making them more accessible to a broader audience. However, variability in model performance and demonstrated inaccuracies underscore the need for human review of LLM output. Further study is needed to explore advanced LLM techniques and models trained for medical content.",
  "full_text": null,
  "topic": "Readability",
  "concepts": [
    {
      "name": "Readability",
      "score": 0.8989406824111938
    },
    {
      "name": "Preprint",
      "score": 0.8069301843643188
    },
    {
      "name": "Cross-sectional study",
      "score": 0.505474328994751
    },
    {
      "name": "Computer science",
      "score": 0.45056501030921936
    },
    {
      "name": "Psychology",
      "score": 0.40169745683670044
    },
    {
      "name": "Medical education",
      "score": 0.37647828459739685
    },
    {
      "name": "World Wide Web",
      "score": 0.3543006181716919
    },
    {
      "name": "Multimedia",
      "score": 0.33992499113082886
    },
    {
      "name": "Medicine",
      "score": 0.3289095163345337
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210086933",
      "name": "NYU Langone Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I138873065",
      "name": "Long Island University",
      "country": "US"
    }
  ]
}