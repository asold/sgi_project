{
  "title": "Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection",
  "url": "https://openalex.org/W4385574050",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5003845489",
      "name": "Luca Di Liello",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A5064222284",
      "name": "Siddhant Garg",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5060844217",
      "name": "Luca Soldaini",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5056376686",
      "name": "Alessandro Moschitti",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3173783447",
    "https://openalex.org/W2120735855",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2907644564",
    "https://openalex.org/W2988869004",
    "https://openalex.org/W3029927342",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4206334925",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W2803820154",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4210984894",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3005296017",
    "https://openalex.org/W3115540322",
    "https://openalex.org/W2951528484",
    "https://openalex.org/W2760753016",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3021534166",
    "https://openalex.org/W4287854682",
    "https://openalex.org/W3034212969",
    "https://openalex.org/W3142869857",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2767857566",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W1966443646"
  ],
  "abstract": "An important task for designing QA systems is answer sentence selection (AS2): selecting the sentence containing (or constituting) the answer to a question from a set of retrieved relevant documents. In this paper, we propose three novel sentence-level transformer pre-training objectives that incorporate paragraph-level semantics within and across documents, to improve the performance of transformers for AS2, and mitigate the requirement of large labeled datasets. Specifically, the model is tasked to predict whether: (i) two sentences are extracted from the same paragraph, (ii) a given sentence is extracted from a given paragraph, and (iii) two paragraphs are extracted from the same document. Our experiments on three public and one industrial AS2 datasets demonstrate the empirical superiority of our pre-trained transformers over baseline models such as RoBERTa and ELECTRA for AS2.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11806–11816\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nPre-training Transformer Models with Sentence-Level Objectives for\nAnswer Sentence Selection\nLuca Di Liello1∗, Siddhant Garg2, Luca Soldaini3†, Alessandro Moschitti2\n1University of Trento , 2Amazon Alexa AI, 3Allen Institute for AI\nluca.diliello@unitn.it\n{sidgarg,amosch}@amazon.com\nlucas@allenai.org\nAbstract\nAn important task for designing QA systems\nis answer sentence selection (AS2): selecting\nthe sentence containing (or constituting) the an-\nswer to a question from a set of retrieved rele-\nvant documents. In this paper, we propose three\nnovel sentence-level transformer pre-training\nobjectives that incorporate paragraph-level se-\nmantics within and across documents, to im-\nprove the performance of transformers for AS2,\nand mitigate the requirement of large labeled\ndatasets. Specifically, the model is tasked to\npredict whether: (i) two sentences are extracted\nfrom the same paragraph, (ii) a given sentence\nis extracted from a given paragraph, and (iii)\ntwo paragraphs are extracted from the same\ndocument. Our experiments on three public and\none industrial AS2 datasets demonstrate the em-\npirical superiority of our pre-trained transform-\ners over baseline models such as RoBERTa and\nELECTRA for AS2.\n1 Introduction\nQuestion Answering (QA) finds itself at the core\nof several commercial applications, for e.g., virtual\nassistants such as Google Home, Alexa and Siri.\nAnswer Sentence Selection (AS2) is an important\ntask for QA Systems operating on unstructured text\nsuch as web documents. When presented with a\nset of relevant documents for a question (retrieved\nfrom a web index), AS2 aims to find the best an-\nswer sentence for the question.\nThe recent popularity of pre-trained transform-\ners (Devlin et al., 2019; Liu et al., 2019; Clark et al.,\n2020), has made them the de-facto approach for\nmost QA tasks, including AS2. Several research\nworks (Garg et al., 2020; Laskar et al., 2020; Lauri-\nola and Moschitti, 2021) fine-tune transformers for\nAS2, by posing it as a sentence-pair task and per-\nforming inference over the encoded representations\nof the question and answer candidates.\n∗Work done as an intern at Amazon Alexa AI\n†Work completed at Amazon Alexa AI\nAS2 is a knowledge-intensive complex reason-\ning task, where the answer candidates for a ques-\ntion can stem from multiple documents, possibly\non different topics linked to concepts in the ques-\ntion. While there have been recent works (Ginzburg\net al., 2021; Caciularu et al., 2021) proposing pre-\ntraining strategies for obtaining multi-document\naware document representations over long input\nencoders such as the Longformer (Beltagy et al.,\n2020), there has been limited research (Giorgi et al.,\n2021) on enhancing sentence-pair representations\nwith paragraph and document level semantics.\nFurthermore, obtaining high quality human la-\nbeled examples for AS2 is expensive and time con-\nsuming, due to the large number of answer candi-\ndates to be annotated for each question. Domain-\nspecific AS2 datasets such as WikiQA (Yang et al.,\n2015) and TREC-QA (Wang et al., 2007) only con-\ntain a few thousand questions. Garg et al. (2020)\nshow that effectively fine-tuning pre-trained trans-\nformers on these domain specific AS2 datasets re-\nquires an intermediate fine-tuning transfer on a\nlarge scale AS2 dataset (ASNQ).\nTowards improving the downstream perfor-\nmance of pre-trained transformers for AS2 and mit-\nigating the requirement of large scale labeled data\nfor fine-tuning, we propose three novel sentence-\nlevel transformer pre-training objectives, which can\nincorporate paragraph-level semantics across mul-\ntiple documents. Analogous to the sentence-pair\nnature of AS2, we design our pre-training objec-\ntives to operate over a pair of input text sequences.\nThe model is tasked with predicting: (i) whether\nthe sequences are two sentences extracted from the\nsame paragraph, (ii) whether the first sequence is a\nsentence that is extracted from the second sequence\n(paragraph), and (iii) whether the sequences are two\nparagraphs belonging to the same document.\nWe evaluate our paragraph-aware pre-trained\ntransformers for AS2 on three popular public\ndatasets: ASNQ, WikiQA and TREC-QA; and one\n11806\nindustrial QA benchmark 1. Results show that our\npre-training can improve the performance of fine-\ntuning baseline transformers such as RoBERTa and\nELECTRA on AS2 by ∼3−4% points without re-\nquiring any additional data (labeled/unlabeled).\n2 Related Work\nAnswer Sentence Selection (AS2) Earlier ap-\nproaches for AS2 used CNNs (Severyn and Mos-\nchitti, 2015) or alignment networks (Shen et al.,\n2017a; Tran et al., 2018) to learn and score question\nand answer representations. Since then, compare-\nand-aggregate architectures have also been exten-\nsively studied (Wang and Jiang, 2017; Bian et al.,\n2017; Yoon et al., 2019). Garg et al. achieved\nstate-of-the-art results by fine-tuning transformers\non a large QA corpora first, and then adapting to a\nsmaller AS2 dataset.\nToken-Level Pre-training Objectives Masked\nLanguage Modeling (MLM) is one of the most\npopular token-level pre-training objectives used for\ntransformers (Devlin et al., 2019; Liu et al., 2019).\nSome other models trained using token-level pre-\ntraining objectives are Yang et al. (2020) and Clark\net al. (2020). Joshi et al. (2020) modify MLM\nto a span-prediction objective to make the model\ngeneralize well to machine reading tasks in QA.\nSentence-Level Pre-training Objectives In addi-\ntion to MLM, Devlin et al. (2019) uses the next\nsentence prediction (NSP) objective, which was\nlater shown to not provide empirically improve-\nments over MLM by Liu et al. (possibly due to\nthe task being very simple). Lan et al. (2020) pro-\npose a sentence order prediction (SOP) objective.\nIppolito et al. (2020) enhance NSP to a multiple-\nchoice prediction of the next sentence over a set of\ncandidates, however they embed each sentence in-\ndependently without cross-attention between them\nsimilar to (Reimers and Gurevych, 2019). Gao et al.\n(2021) propose a supervised contrastive learning\napproach for enhancing sentence representations\nfor textual similarity tasks.\nParagraph/Document-level Semantics (Chang\net al., 2019) pre-train Bi-HLSTMs for obtain-\ning hierarchical document representations. HIB-\nERT (Zhang et al., 2019) uses document-level to-\nken masking and sentence masking pre-training\nobjectives for generative tasks such as document\nsummarization. Transformer pre-training objec-\n1We will release code and pre-trained models at https:\n//github.com/amazon-research/wqa-pretraining\ntives at different granularities of document seman-\ntics are discussed in (Li et al., 2020) for fact verifi-\ncation, and in (Chang et al., 2020) for retrieval.\nGinzburg et al.; Caciularu et al. propose pre-\ntraining strategies for document embeddings for\nretrieval tasks such as document-matching. De-\nCLUTR (Giorgi et al., 2021) uses contrastive learn-\ning for cross-encoding two sentences coming from\nthe same/different documents in a transformer, and\nis evaluated on pairwise binary classification tasks\nlike natural language inference. Our work differs\nfrom this since we use a cross-encoder architecture\nto capture cross-attention between the question and\nanswer, and evaluate our approach on the relevance\nranking task of AS2 over hundreds of candidates.\nContemporary works (Di Liello et al., 2022) pre-\ntrain transformers using paragraph-aware objec-\ntives for multi-sentence inference tasks. Our work\ndiffers from this since we only encode a pair of\nsentences using the transformer, while the former\nencode multiple sentences and use sophisticated\nprediction heads to aggregate information across\nmultiple representations.\nTransformers for Long Inputs Longformer (Belt-\nagy et al., 2020), Big Bird (Zaheer et al., 2020), etc.\nmodel very long inputs (e.g, entire documents) by\nreducing the complexity of transformer attention.\nThis provides longer context, which is useful for\nmachine reading and summarization.\n3 Answer Sentence Selection (AS2)\nIn this section we formally define the task of AS2.\nGiven a question qand a set of answer candidates\nA={a1,...,a n}, the objective is to select the can-\ndidate ¯a∈Athat best answers q. AS2 can be mod-\neled as a ranking task overAto learn a scoring func-\ntion f : Q×A→R that predicts the probability\nf(q,a) of an answer candidateabeing correct. The\nbest answer ¯acorresponds to argmaxn\ni=1 f(q,ai).\nPre-trained transformers are used as QA pair en-\ncoders for AS2 to approximate the function f.\n4 Sentence-Level Pre-training Objectives\nDocuments are typically organized into paragraphs,\nby humans, to address the document’s general topic\nfrom different viewpoints. We propose three pre-\ntraining objectives to exploit the intrinsic informa-\ntion contained in the structure of documents. For\nall these objectives, we provide a pair of text se-\nquences as input to the transformer to jointly reason\nover them, analogous to the AS2 task.\n11807\nSpans in Same Paragraph (SSP) Given two se-\nquences (A,B) as input to the transformer, the ob-\njective is to predict if Aand Bbelong to the same\nparagraph in a document. To create positive pairs\n(A,B), given a document D, we extract two small,\ncontiguous and disjoint subsets of sentences to be\nused as Aand Bfrom a single paragraph Pi ∈D.\nTo create negative pairs, we sample spans of sen-\ntences B′from different paragraphsPj,j ̸= iin the\nsame document D(hard negatives) and also from\ndifferent documents (easy negatives). The nega-\ntive pairs correspond to (A,B′). Posing the above\npre-training objective in terms of spans (instead\nof sentences) allows for modifying the lengths of\nthe inputs A,B (by changing number of sentences\n∈A,B). When fine-tuning transformers for AS2,\ntypically the question is provided as the first input\nand a longer answer candidate/paragraph is pro-\nvided as the second input. For our experiments\n(Sec 5), we use a longer span for input B than A.\nSpan in Paragraph (SP) Given two sequences\n(A,B) as input to the transformer, the objective\nis to predict if Ais a span of text extracted from\na paragraph B in a document. To create positive\npairs (A,B), given a paragraph Pi in a document\nD, we extract a small contiguous span of sentences\nAfrom it and create the input pair as (A,Pi \\A).\nTo create negative pairs, we select other paragraphs\nPj,j ̸= iin the same document Dand remove a\nrandomly chosen span A′from each of them. The\nnegative pairs correspond to (A,Pj \\A′). This is\nnecessary to ensure that the model does not simply\nrecognize whether the second input is a complete\nparagraph or a clipped version. To create easy neg-\natives, we use the above approach for paragraphs\nPj sampled from documents other than D.\nParagraphs in Same Document (PSD) Given\ntwo sequences (A,B) as input to the transformer,\nthe objective is to predict ifAand Bare paragraphs\nbelonging to the same document. To create positive\npairs (A,B), given a document Dk, we randomly\nselect paragraphs Pi,Pj ∈Dk and obtain a pair\n(Pi,Pj). To create negative pairs, we randomly\nselect P′\nj /∈Dk, and obtain a pair (Pi,P′\nj).\n5 Experiments\n5.1 Datasets\nPre-training To eliminate any improvements\nstemming from the usage of more data, we per-\nform pre-training on the same corpora as RoBERTa:\nEnglish Wikipedia, the BookCorpus, OpenWeb-\nText and CC-News. We perform continuous pre-\ntraining starting from RoBERTa (Liu et al., 2019)\nand ELECTRA (Clark et al., 2020) checkpoints,\nusing a combination of our objectives with the orig-\ninal ones (MLM for RoBERTa and MLM + Token\nDetection for ELECTRA). Refer to Appendix A\nfor complete details.\nAS2 Fine-tuning We consider three public and\none industrial AS2 benchmark as fine-tuning\ndatasets for AS2 (statistics presented in Ap-\npendix A). We use standard evaluation metrics for\nAS2: Mean Average Precision (MAP), Mean Re-\nciprocal Recall (MRR) and Precision@1 (P@1).\n• ASNQ is a large-scale AS2 dataset (Garg et al.,\n2020) with questions from Google search engine\nqueries, and answer candidates extracted from a\nWikipedia page. ASNQ is a modified version of\nthe Natural Questions (NQ) (Kwiatkowski et al.,\n2019), obtained by labeling sentences from long\nanswers that contain the short answer as positives\nand all others as negatives. We use the dev and test\nsplits released by Soldaini and Moschitti 2.\n• WikiQA is a popular AS2 dataset (Yang et al.,\n2015) where questions are derived from query logs\nof the Bing search engine, and the answer candi-\ndates are extracted from a Wikipedia page. This\ndataset has a subset of questions having no correct\nanswers (all-) or having only correct answers (all+).\nWe remove both the (all-) and (all+) questions for\nour experiments (standard “clean” setting).\n• TREC-QA is a popular AS2 dataset (Wang et al.,\n2007) of factoid questions, extracted from the\nTREC-8 to TREC-13 QA tracks. The answer can-\ndidates are sentences that contain one or more non-\nstopwords in common with the question, extracted\nfrom multiple documents. For the dev and test sets,\nwe remove questions without answers, or having\nonly correct or only incorrect answer candidates\n(“clean” setting (Shen et al., 2017b)).\n• WQA A large scale industrial AS2 dataset con-\ntaining non-representative de-identified user ques-\ntions from Alexa virtual assistant. For every ques-\ntion, ∼15 answer candidates are collected from a\nlarge web index of more than 100M documents us-\ning Elasticsearch. Results on WQA are presented\nrelative to the RoBERTa-Base baseline due to the\ndata being internal.\n2https://github.com/alexa/\nwqa-cascade-transformers\n11808\nModel ASNQ WikiQA TREC-QA WQA\nP@1 MAP MRR P@1 MAP MRR P@1 MAP MRR P@1 MAP MRR\nRoBERTa-Base 61.8(0.2)66.9(0.1)73.1(0.1) 78.3(2.8)85.8(1.3)87.2(1.3) 90.0(1.9)89.7(0.7)94.4(1.1) Baseline\n(Ours)RoBERTa + SSP64.1(0.3)68.1(0.2)74.5(0.3) 82.9(0.7)88.7(0.3)89.9(0.4) 88.5(1.2)89.3(0.7)93.6(0.6) +0.2% +0.6%+0.3%\n(Ours)RoBERTa + SP64.1(0.2)68.3(0.1)74.5(0.2) 81.0(0.8)87.7(0.3)88.9(0.4) 90.9(2.6)90.1(0.8)94.7(1.3) +0.4%+0.7%+0.5%\n(Ours)RoBERTa + PSD 62.6(0.4)67.7(0.2)73.7(0.3) 80.5(1.6)86.4(1.1)88.0(1.0) 90.3(1.3)90.3(0.5)95.1(0.7) +0.4%+0.7%+0.5%\n(Ours)RoBERTa + All 63.9(0.4)68.0(0.1)74.1(0.2) 82.5(0.9)88.2(0.4)89.5(0.4) 87.9(1.2)89.3(0.7)93.4(0.6) +0.5%+0.8%+0.6%TANDA RoBERTa - - - 83.0 (1.3)88.5(0.8)89.9(0.8) 89.7(0.0)90.1(0.6)94.1(0.4) +0.5% +0.5% +0.5%\nELECTRA-Base 62.4(0.4)67.5(0.2)73.6(0.2) 77.1(4.0)85.0(2.6)86.5(2.7) 90.3(1.7)89.9(0.4)94.0(0.9) +1.0% +1.2% +0.9%\n(Ours)ELECTRA + SSP65.3(0.3)69.7(0.2)75.7(0.2) 82.5(2.0)88.6(1.4)90.0(1.4) 88.5(1.9)89.6(0.7)93.5(0.9) +1.4%+1.5%+1.3%\n(Ours)ELECTRA + SP 65.0(0.2)69.0(0.1)75.1(0.1) 81.8(2.3)88.1(1.5)89.5(1.5) 91.2(1.5)90.3(0.7)94.6(0.7) +1.4%+1.5%+1.3%\n(Ours)ELECTRA + PSD65.3(0.4)68.9(0.3)75.1(0.3) 78.6(0.7)85.6(0.7)87.3(0.6) 85.9(2.2)87.9(1.1)92.2(1.1) +1.6%+1.6%+1.3%\n(Ours)ELECTRA + All 65.0(0.3)69.3(0.2)75.2(0.2) 80.8(1.9)87.3(1.2)88.7(1.1) 92.6(1.8)90.4(0.4)95.5(1.0) +1.5%+1.6%+1.4%TANDA ELECTRA - - - 85.6 (1.1)90.2(0.8)91.4(0.7) 92.6(1.5)91.6(0.7)95.5(0.7) +1.9% +1.6% +1.5%\nTable 1: Results (with std. dev. across 5 runs in parentheses) of our pretrained transformers when fine-tuned on AS2\ndatasets. SSP, SP, PSD denote our pretraining objectives, and ‘All’ denotes using SSP+SP+PSD together. TANDA\nuses additional labeled data as an intermediate transfer step. We underline statistically significant improvements\nover the baseline (T-test at a 95% confidence level). Results on WQA are relative to the RoBERTa baseline.\n5.2 Experimental Setup and Details\nWe use our 3 pre-training objectives: SSP, SP and\nPSD, for both RoBERTa and ELECTRA, obtaining\n6 different continuously pre-trained models. We\nset the maximum pre-training steps to 400k for\nSSP and 200k for SP and PSD. This corresponds\nto each model processing ∼210B tokens during\npre-training, which is about 10% of the ∼2100B\ntokens used for pre-training RoBERTa. Notice also\nthat the compute FLOPs are even less than the\n10% of the original training because we used a\nshorted max sequence length. More details about\nthe continuous pre-training hyper-parameters are\ngiven in Appendix B.\nWe also combine all 3 objectives together\n(SSP+SP+PSD) for both RoBERTa and ELECTRA,\nwith the same setting as SSP. We fine-tune each of\nour pre-trained models on all four AS2 datasets\n(with early stopping on the dev set) and compute\nresults on their respective test splits.\nBaselines We use RoBERTa and ELECTRA mod-\nels as baselines. We also use TANDA (Garg et al.,\n2020), the state of the art for AS2, as an upper-\nbound baseline as it uses an additional intermediate\ntransfer step on ASNQ (∼20M labeled QA pairs).\nNote that we don’t consider Ginzburg et al.; Caci-\nularu et al.; Chang et al. as baselines as they are de-\nsigned for document-matching and retrieval tasks,\nand Beltagy et al.; Zaheer et al. as they are used\nfor long-context tasks like MR and summarization.\n5.3 Results\nWe present results of our pre-trained models on\nthe AS2 datasets in Table 1. We observe that\nthe models trained with our pre-training objec-\ntives significantly outperform the baseline mod-\nels when fine-tuned for the AS2 tasks. For ex-\nample, on ASNQ, using our SP objective with\nRoBERTa-Base gains2.3% in P@1 over the base-\nline RoBERTa-Base model. On WikiQA, the per-\nformance gap is even larger with the SSP objec-\ntive corresponding to 4.6% points for RoBERTa-\nBase and 5.4% for ELECTRA-Base over the cor-\nresponding baselines. Performance improvements\non TREC-QA and WQA are smaller but consis-\ntent, around 1% and 0.6% in P@1. Combining\nSSP+SP+PSD together consistently achieves either\nthe best results (TREC-QA and WQA), or close to\nthe best results (ASNQ and WikiQA).\nFor questions in ASNQ and WikiQA, all candi-\ndate answers are extracted from a single Wikipedia\ndocument, while for TREC-QA and WQA, candi-\ndate answers come from multiple documents ex-\ntracted from heterogeneous web sources. By design\nof our objectives SSP, SP and PSD, they perform\ndifferently when fine-tuning on different datasets.\nFor example, SSP aligns well with ASNQ and Wik-\niQA as they contain many negative candidates, per\nquestion, extracted from the same document as the\npositive (i.e, ‘hard’ negatives). As per our design\nof the SSP objective, for every positive sequence\npair, we sample 2 ‘hard’ negatives coming from\nthe same document as the positive pair. The pres-\nence of hard negatives is of particular importance\nfor WikiQA and ASNQ, as it forces the models to\nlearn and contrast more subtle differences between\nanswer candidates, which might likely be more\nrelated as they come from the same document.\nOn the other hand, PSD is designed so as to\nsee paragraphs from same or different documents\n(with no analogous concept of ‘hard’ negatives of\nSSP and SP). For this reason, PSD is better aligned\nfor fine-tuning on datasets where candidates are\nextracted from multiple documents, such as WQA\n11809\nModel+Data Sampling ASNQ WikiQA TREC-QA WQA\nRoBERTa-Base 61.8 78.3 90.0 Baseline\n+ SSP Data (MLM-only) 63.4 76.7 87.4 -0.6%\n+ SSP 64.1 82.9 88.5 +0.2 %\n+ SP Data (MLM-only) 62.8 76.8 88.8 -1.0%\n+ SP 64.1 81.0 90.9 +0.4 %\n+ PSD Data (MLM-only) 64.1 79.1 87.1 -1.3%\n+ PSD 62.6 80.5 90.3 +0.4%\nTable 2: P@1 of our pretrained models using SSP, SP\nand PSD objectives in addition to only MLM. We high-\nlight in bold and underline results like in Table 1.\nModel + Pre-training Objective Accuracy F1\nRoBERTa-Base +SSP 91.8 83.1\nELECTRA-Base +SSP 90.4 79.9\nRoBERTa-Base +SP 91.3 83.3\nELECTRA-Base +SP 89.9 80.1\nRoBERTa-Base +PSD 83.5 61.4\nELECTRA-Base +PSD 82.3 57.1\nBERT (Devlin et al., 2019) (NSP) 96.9 97.1\nALBERT (Lan et al., 2020) (SOP) 93.7 94.7\nTable 3: Comparison of accuracy and F1-score of pre-\ntraining objectives on the pre-training validation set.\nand TREC-QA.\nComparison with TANDA For RoBERTa,\nour pre-trained models can surprisingly im-\nprove/achieve comparable performance to TANDA.\nNote that our models achieve this performance\nwithout using the latter’s additional ∼20M labeled\nASNQ QA pairs. This lends support to our pre-\ntraining objectives mitigating the requirement of\nlarge scale labeled data for AS2 fine-tuning. For\nELECTRA, we only observe comparable perfor-\nmance to TANDA for WQA and TREC-QA.\nAblation: MLM-only Pre-training To mitigate\nany improvements stemming from the specific data\nsampling techniques used by our objectives, we pre-\ntrain 3 models (starting from RoBERTa-Base) with\nthe same data sampling as each of the SSP, SP and\nPSD models, but only using the MLM objective.\nWe report results in Table 2, and observe that,\nalmost always, models pre-trained only with MLM\nunder-perform models trained with SSP, SP and\nPSD objectives in addition to MLM. Thus, the\nempirical improvements of our methods are de-\nrived from the novel pre-training objectives, and\nnot data sampling. Surprisingly, for some models,\nthe MLM-only continuous pre-training performs\nworse than the baseline RoBERTa-Base. We be-\nlieve that restarting the training with a different\nlearning-rate3, a shorter sequence length, and with-\nout the original optimizer and scheduler internal\nstates (for a small amount of steps) is sub-optimal\nfor the model.\nAblation: Pre-training Task ‘Difficulty’ We\nevaluate the pre-trained models (after convergence)\non their specific tasks over the validation split of\nWikipedia (to enable evaluating baselines such as\nBERT and ALBERT). Table 3 summarizes the ac-\ncuracy and F1 of the models on the various tasks.\nThe results show that our objectives are gener-\nally harder than NSP (Next Sentence Prediction by\nDevlin et al., 2019) and SOP (Sentence Order Pre-\ndiction by Lan et al., 2020). In fact, NSP and SOP\nhave been shown to not add any significant per-\nformance improvements in addition to MLM (Liu\net al., 2019), and this corresponds to the model be-\ning able to perform this task extremely well (dev\naccuracy ∼94% with NSP and ∼97% with SOP)\nwithout learning any new semantics that may be\nuseful for downstream tasks.\nOn the other hand, our pre-training objectives are\n“more challenging” than these previously proposed\nobjectives due to the requirement of reasoning over\nmultiple paragraphs and multiple documents, ad-\ndressing same or different topics at the same time.\nIn fact, Table 3 shows that after convergence, our\npre-trained model still finds it difficult to achieve a\nhigher accuracy for our sentence level pre-training\ntasks. Empirically in Table 1, we observed that pre-\ntraining with our objectives is able to rank the more\nrelevant answers at the top, which we hypothesize\nis due to the model learning how to reason over\nmultiple paragraphs and documents already while\nperforming continuous pre-training.\n6 Conclusion\nIn this paper we have presented three sentence-\nlevel pre-training objectives for transformers to in-\ncorporate paragraph and document-level semantics.\nOur objectives predict whether (i) two sequences\nare sentences extracted from the same paragraph,\n(ii) first sequence is a sentence extracted from the\nsecond, and (iii) two sequences are paragraphs be-\nlonging to the same document. We evaluate our\npre-trained models for the task of AS2 on four\ndatasets. Our results show that our pre-trained mod-\nels outperform the baseline transformers such as\nRoBERTa and ELECTRA.\n3The original models use a triangular learning-rate\n11810\nLimitations\nWe only consider English language datasets for\nour experiments in this paper. However we hy-\npothesize that our pre-training objectives should\nprovide similar performance improvements when\nextended to other languages with limited morphol-\nogy, like English. The pre-training objectives pro-\nposed in our work are designed considering An-\nswer Sentence Selection (AS2) as the target task,\nand can be extended for other tasks like Natu-\nral Language Inference, Question-Question Sim-\nilarity, etc. in future work. The pre-training ex-\nperiments in our paper require large amounts of\nGPU and compute resources (multiple NVIDIA\nA100 GPUs running for several days) to finish\nthe model pre-training. This makes re-training\nmodels using our pre-training approaches compu-\ntationally expensive using newer data. To mitigate\nthis, we are releasing our code and pre-trained\nmodel checkpoints at https://github.com/\namazon-research/wqa-pretraining, which can\ndirectly be used by fine-tuning them on AS2\ndatasets.\nAcknowledgements\nWe thank the anonymous reviewers and the ARR\naction-editor for their valuable suggestions. We\nwould like to thank Thuy Vu for developing and\nsharing the WQA dataset.\nReferences\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nWeijie Bian, Si Li, Zhao Yang, Guang Chen, and\nZhiqing Lin. 2017. A compare-aggregate model with\ndynamic-clip attention for answer selection. Proceed-\nings of the 2017 ACM on Conference on Information\nand Knowledge Management.\nAvi Caciularu, Arman Cohan, Iz Beltagy, Matthew Pe-\nters, Arie Cattan, and Ido Dagan. 2021. CDLM:\nCross-document language modeling. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 2648–2662, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nMing-Wei Chang, Kristina Toutanova, Kenton Lee, and\nJacob Devlin. 2019. Language model pre-training\nfor hierarchical document representations. CoRR,\nabs/1901.09128.\nWei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yim-\ning Yang, and Sanjiv Kumar. 2020. Pre-training tasks\nfor embedding-based large-scale retrieval. In Inter-\nnational Conference on Learning Representations.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\nIn International Conference on Learning Representa-\ntions.\nNicki Skafte Detlefsen, Jiri Borovec, Justus Schock,\nAnanya Harsh Jha, Teddy Koker, Luca Di Liello,\nDaniel Stancl, Changsheng Quan, Maxim Grechkin,\nand William Falcon. 2022. Torchmetrics - measuring\nreproducibility in pytorch. Journal of Open Source\nSoftware, 7(70):4101.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLuca Di Liello, Siddhant Garg, Luca Soldaini, and\nAlessandro Moschitti. 2022. Paragraph-based trans-\nformer pre-training for multi-sentence inference. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2521–2531, Seattle, United States. Association\nfor Computational Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nSiddhant Garg and Alessandro Moschitti. 2021. Will\nthis question be answered? question filtering via\nanswer model distillation for efficient question an-\nswering. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7329–7346, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nSiddhant Garg, Thuy Vu, and Alessandro Moschitti.\n2020. Tanda: Transfer and adapt pre-trained trans-\nformer models for answer sentence selection. Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, 34(05):7780–7788.\nDvir Ginzburg, Itzik Malkiel, Oren Barkan, Avi Caciu-\nlaru, and Noam Koenigstein. 2021. Self-supervised\ndocument similarity ranking via contextualized lan-\nguage models and hierarchical inference. In Find-\nings of the Association for Computational Linguis-\n11811\ntics: ACL-IJCNLP 2021, pages 3088–3098, Online.\nAssociation for Computational Linguistics.\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.\n2021. DeCLUTR: Deep contrastive learning for un-\nsupervised textual representations. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers) , pages 879–895, Online.\nAssociation for Computational Linguistics.\nAaron Gokaslan and Vanya Cohen. 2019. Open-\nwebtext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nDaphne Ippolito, David Grangier, Douglas Eck, and\nChris Callison-Burch. 2020. Toward better storylines\nwith sentence-level language models. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7472–7478, Online.\nAssociation for Computational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations.\nMd Tahmid Rahman Laskar, Jimmy Xiangji Huang, and\nEnamul Hoque. 2020. Contextualized embeddings\nbased transformer encoder for sentence similarity\nmodeling in answer selection task. In Proceedings of\nthe 12th Language Resources and Evaluation Confer-\nence, pages 5505–5514, Marseille, France. European\nLanguage Resources Association.\nIvano Lauriola and Alessandro Moschitti. 2021. An-\nswer sentence selection using local and global context\nin transformer models. ECIR.\nXiangci Li, Gully A. Burns, and Nanyun Peng. 2020. A\nparagraph-level multi-task learning model for scien-\ntific fact-verification. CoRR, abs/2012.14500.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nCoRR, abs/1908.10084.\nAliaksei Severyn and Alessandro Moschitti. 2015.\nLearning to rank short text pairs with convolutional\ndeep neural networks. Proceedings of the 38th Inter-\nnational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval.\nGehui Shen, Yunlun Yang, and Zhi-Hong Deng. 2017a.\nInter-weighted alignment network for sentence pair\nmodeling. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1179–1189, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nGehui Shen, Yunlun Yang, and Zhi-Hong Deng. 2017b.\nInter-weighted alignment network for sentence pair\nmodeling. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1179–1189, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nLuca Soldaini and Alessandro Moschitti. 2020. The\ncascade transformer: an application for efficient an-\nswer sentence selection. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5697–5708, Online. Association\nfor Computational Linguistics.\nQuan Hung Tran, Tuan Lai, Gholamreza Haffari, Ingrid\nZukerman, Trung Bui, and Hung Bui. 2018. The\ncontext-dependent additive recurrent neural net. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1274–1283, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nMengqiu Wang, Noah A. Smith, and Teruko Mita-\nmura. 2007. What is the Jeopardy model? a quasi-\nsynchronous grammar for QA. In Proceedings of the\n2007 Joint Conference on Empirical Methods in Natu-\nral Language Processing and Computational Natural\nLanguage Learning (EMNLP-CoNLL), pages 22–32,\nPrague, Czech Republic. Association for Computa-\ntional Linguistics.\nShuohang Wang and Jing Jiang. 2017. A compare-\naggregate model for matching text sequences. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nYi Yang, Scott Wen-tau Yih, and Chris Meek. 2015.\nWikiqa: A challenge dataset for open-domain ques-\ntion answering. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing. ACL - Association for Computational\nLinguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2020.\n11812\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding.\nSeunghyun Yoon, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, and Kyomin Jung. 2019. A compare-\naggregate model with latent clustering for answer\nselection. Proceedings of the 28th ACM Interna-\ntional Conference on Information and Knowledge\nManagement.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI-\nBERT: Document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n5059–5069, Florence, Italy. Association for Compu-\ntational Linguistics.\n11813\nAppendix\nA Datasets\nA.1 Pre-training\nFor continued pre-training, we pre-process the En-\nglish Wikipedia4, the BookCorpus5, OpenWebText\n(Gokaslan and Cohen, 2019) and the CC-News 6\ndatasets. We do not use the STORIES dataset as\nit is no longer available for research use 7. We\nclean every dataset by removing headers, titles,\ntables and any HTML content. For every docu-\nment, we keep paragraphs containing at least 60\ncharacters and documents containing at least 200\ncharacters. After cleaning, we obtain 5GB, 10GB,\n34GB and 360GB of raw text from the BookCor-\npus, Wikipedia, OpenWebText and CC-News re-\nspectively. We split paragraph into lists of sen-\ntences using the blingfire tokenizer8. We present\nthe details of our pre-training objectives in Sec-\ntion 4. We present the details on sampling lengths\nand number of negatives for each of the objectives\nbelow:\n• Spans in Same Paragraph (SSP) We randomly\nsample the number of sentences in Ain the interval\n[1,3] and B in [1,5]. This is to keep the inputs\nto the model analogous to those in AS2 (shorter\nquestion text, followed by longer answer text). We\nsample up to 2 hard negatives from the same para-\ngraph as A(if possible), and sample easy negatives\nfrom other documents so as to make the total num-\nber of negatives to be 4.\n• Span in Paragraph (SP) We randomly sample\nthe number of sentences in A∈Pi in the interval\n[1,3]. The number of sentences in the right part\nis given by the length of Pi \\A(positive pair) or\nPj \\Xj (negative pair). Similar to SSP, we sample\nup to 2 hard negatives from the same document (if\npossible), and sample easy negatives from other\ndocuments so as to make the total number to be 4.\n• Paragraphs in Same Document (PSD) We\nchose a random pair of paragraphs Aand Bfrom\na single document and then we randomly sample\n4https://dumps.wikimedia.org/enwiki/20211101/\n5https://huggingface.co/datasets/\nbookcorpusopen\n6https://commoncrawl.org/2016/10/\nnews-dataset-available/\n7https://github.com/tensorflow/models/\ntree/archive/research/lm_commonsense#\n1-download-data-files\n8https://github.com/microsoft/BlingFire\n4 paragraphs from other documents to create the\nnegative pairs with A.\nA.2 Fine-tuning\nHere we present statistics and links for down-\nloading the AS2 datasets used: ASNQ 9, Wik-\niQA10, TREC-QA and WQA; to benchmark our\npre-trained models. Table 4 shows the number of\nunique questions and answer candidates for each\ndataset and for each split.\nDataset Split # Q # C Avg. # C/Q\nASNQ\nTrain 57,242 20,377,568 356.0\nDev 1,336 463,914 347.2\nTest 1,336 466,148 348.9\nWikiQA\nTrain 2,118 20,360 9.6\nDev 122 1,126 9.2\nTest 237 2,341 9.9\nTREC-QA\nTrain 1,226 53,417 43.6\nDev 69 1,343 19.5\nTest 68 1,442 21.2\nWQA\nTrain 9,984 149,513 15.0\nDev 5,000 74,805 15.0\nTest 5,000 74,712 14.9\nTable 4: Data Statistics for AS2 dataset. “Avg. # C/Q” is\nthe average number of answer candidates per question.\nB Experimental Setup\nWe experiment with the base architecture, which\nuses an hidden size of 768, 12 transformer layers,\n12 attention heads and feed-forward size of 3072.\nPre-training We perform continued pre-training\nstarting from the publicly released checkpoints of\nRoBERTa-Base (Liu et al., 2019) and ELECTRA-\nBase (Clark et al., 2020). We optimize using Adam,\nwhich we instantiate with β1 = 0.9, β2 = 0.999\nand ϵ = 10−8. We use a triangular learning rate\nwith 10k warmup steps. The peak learning rate is\nset to 1 ∗10−4. We apply a weight decay of 0.01,\ngradient clipping when values are larger than 1.0\nand dropout ratio is set to0.1. We set the batch size\nto 4096 examples for every combination of models\nand objectives. We truncate the input sequences\nto 128 tokens for SSP and to 256 tokens with SP\nand PSD. Finally, we perform 400k training steps\nwith models using SSP and 200k steps with the\nother objectives: SP and PSD. The total amount\nof tokens seen in the continued pre-training is the\nsame for all models and equal to ∼210B.\nWe combine the binary classification loss of SSP,\nSP and PSD with MLM for RoBERTa and with\nMLM (of the generator) and TD (token detection)\n9https://github.com/alexa/wqa_tanda\n10http://aka.ms/WikiQA\n11814\nfor ELECTRA. For RoBERTa, we perform binary\nclassification on the first [CLS] token in addition\nto MLM. For ELECTRA, using the generator +\ndiscriminator architecture, we perform MLM on\nthe generator; and token-detection along with bi-\nnary classification on the discriminator using our\npre-training objectives. Through experimentation,\nfor RoBERTa, we use equal weights for MLM and\nour pre-training objectives. For ELECTRA, we\ncombine MLM, TD and our pre-training objectives\nwith the weights 1.0, 50.0 and 1.0 respectively.\nFine-tuning The evaluation of the models is per-\nformed on four different datasets for Answer Sen-\ntence Selection. We maintain the same hyper-\nparameters used in pre-training apart from the learn-\ning rate, the number of warmup steps and the batch\nsize. We do early stopping on the development\nset if the number of non-improving validations (pa-\ntience) is higher than 5. For ASNQ, we found\nthat using a very large batch size is beneficial, pro-\nviding a higher accuracy. We use a batch size of\n2048 examples on ASNQ for RoBERTa models\nand 1024 for ELECTRA models. The peak learn-\ning rate is set to 1 ∗10−5 for all models, and the\nnumber of warmup steps to 1000. For WikiQA,\nTREC-QA and WQA, we select the best batch\nsize out of {16,32,64}and learning rate out of\n{2∗10−6,5∗10−5,1∗10−5,2∗10−5}using cross-\nvalidation. We train the model for 6 epochs on\nASNQ, and up to 40 epochs on WikiQA, TREC-\nQA, and WQA. The performance of practical AS2\nsystems is typically measured using Precision-at-1\nP@1 (Garg and Moschitti, 2021). In addition to\nP@1, we also use Mean Average Precision (MAP)\nand Mean Reciprocal Recall (MRR) to evaluate the\nranking of the set of candidates produced by the\nmodel.\nWe used metrics from Torchmetrics (Detlefsen\net al., 2022) to compute MAP, MRR, Precision@1\nand Accuracy.\nC Experiments and Results\nC.1 Ablation: MLM-only Pre-training\nTable 5 presents a more detailed comparison be-\ntween models continuously pre-trained only with\nMLM and models using also the sentence-level\nclassification loss functions we proposed in this\npaper.\nD Qualitative Examples from AS2\nWe present some qualitative examples from the\nthree public AS2 datasets. We highlight cases in\nwhich the baseline RoBERTa-Base model is unable\nto rank the correct answer in the top position, but\nwhere our model pretrained with SP is successful.\nThe examples are provided in Table 6.\n11815\nModel+ Data SamplingASNQ WikiQA TREC-QA WQA\nP@1 MAP MRR P@1 MAP MRR P@1 MAP MRR P@1 MAP MRR\nRoBERTa-Base 61.8(0.2)66.9(0.1)73.1(0.1) 78.3(2.8)85.8(1.3)87.2(1.3) 90.0(1.9)89.7(0.7)94.4(1.1) Baseline\n+ SSP Data (MLM-only) 63.4(0.4)67.1(0.2)73.8(0.2) 76.7(0.9)84.5(0.7)85.8(0.7) 87.4(1.3)88.8(0.6)93.1(1.0) -0.6% -0.2% -0.3%+ SSP 64.1(0.3)68.1(0.2)74.5(0.3) 82.9(0.7)88.7(0.3)89.9(0.4) 88.5(1.2)89.3(0.7)93.6(0.6) +0.2% +0.6%+0.3%\n+ SP Data (MLM-only) 62.8 (0.3)67.2(0.2)73.7(0.2) 76.8(1.6)84.7(0.8)86.2(0.7) 88.8(1.3)89.8(0.3)93.7(0.9) -1.0% -0.4% -0.6%+ SP 64.1(0.2)68.3(0.1)74.5(0.2) 81.0(0.8)87.7(0.3)88.9(0.4) 90.9(2.6)90.1(0.8)94.7(1.3) +0.4%+0.7%+0.5%\n+ PSD Data (MLM-only) 64.1(0.5)67.3(0.2)73.7(0.2) 79.1(1.6)85.6(1.4)87.1(1.2) 87.1(2.8)89.6(1.0)92.7(1.3) -1.3% -0.3% -0.6%+ PSD 62.6 (0.4)67.7(0.2)73.7(0.3) 80.5(1.6)86.4(1.1)88.0(1.0) 90.3(1.3)90.3(0.5)95.1(0.7) +0.4%+0.7%+0.5%\nTable 5: Results (with std. dev. across 5 runs in parentheses) of our pretrained transformer models when fine-tuned\non AS2 datasets with MLM-only pre-training. SSP, SP and PSD refer to our pretraining objectives. Results on\nWQA are relative to RoBERTa baseline. We highlight in bold and underline results like in Table 1.\nASNQ\nQ: how many players in football hall of fame\nA1:Two coaches ( Marv Levy , Bud Grant ) , one administrator ( Jim Finks ) , and five players ( Warren Moon , Fred Biletnikoff\n, John Henry Johnson , Don Maynard , Arnie Weinmeister ) who spent part of their careers in the Canadian Football League ( CFL )\nhave been inducted ; two of which have been inducted into the Canadian Football Hall of Fame : Warren Moon and Bud Grant.\nA2:As of 2018 , 318 individuals have been elected .\nA3:Six players or coaches who spent part of their careers in the short-lived United States Football League ( USFL ) have been inducted .\nA4:Current rules of the committee stipulate that between four and eight individuals are selected each year .\nA5:Fifteen inductees spent some of their playing career in the All - America Football Conference during the late 1940s .\nWikiQA\nQ: how are antibodies used in\nA1:Antibodies are secreted by a type of white blood cell called a plasma cell .\nA2:An antibody (Ab), also known as an immunoglobulin (Ig), is a large Y-shaped protein produced by B-cells that is used by the\nimmune system to identify and neutralize foreign objects such as bacteria and viruses .\nA3:Using this binding mechanism, an antibody can tag a microbe or an infected cell for attack by other parts of the immune system,\nor can neutralize its target directly (for example, by blocking a part of a microbe that is essential for its invasion and survival).\nA4:Antibodies can occur in two physical forms, a soluble form that is secreted from the cell, and a membrane -bound form that is\nattached to the surface of a B cell and is referred to as the B cell receptor (BCR).\nA5:The BCR is only found on the surface of B cells and facilitates the activation of these cells and their subsequent differentiation into\neither antibody factories called plasma cells , or memory B cells that will survive in the body and remember that same antigen so the B\ncells can respond faster upon future exposure.\nTREC-QA\nQ: Where is the group Wiggles from ?\nA1:Let ’s now give a welcome to the Wiggles , a goofy new import from Australia .\nA2:The Wiggles are four effervescent performers from the Sydney area : Anthony Field , Murray Cook , Jeff Fatt and Greg Page .\nA3:In Australia , the Wiggles is like really huge .\nA4:His group had kids howling with joy with routines involving Dorothy the Dinosaur , Henry the Octopus and Wags the Dog .\nA5:While relatively new to the American scene , the Wiggles seem to be on to something , judging by kids ’ reactions to the group ’s\nbelly-slapping shows .\nTable 6: Qualitative examples from AS2 datasets where the baseline RoBERTa-Base model is unable to rank a\ncorrect answer for the question at the top position, but our SP pre-trained model can (top ranked correct answer by\nSP). Here we present the top ranked answers {A1,...,A 5}in the order given by the RoBERTa-Base model. For all\nthese examples we highlight the top ranked answer by the baseline RoBERTa-Base model in red since it is incorrect,\nand any other correct answer in green.\n11816",
  "topic": "Paragraph",
  "concepts": [
    {
      "name": "Paragraph",
      "score": 0.9370021820068359
    },
    {
      "name": "Sentence",
      "score": 0.8675481677055359
    },
    {
      "name": "Transformer",
      "score": 0.8420684337615967
    },
    {
      "name": "Computer science",
      "score": 0.7649248838424683
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5902753472328186
    },
    {
      "name": "Natural language processing",
      "score": 0.586885929107666
    },
    {
      "name": "Voltage",
      "score": 0.0798833966255188
    },
    {
      "name": "Engineering",
      "score": 0.079151451587677
    },
    {
      "name": "World Wide Web",
      "score": 0.07314568758010864
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}