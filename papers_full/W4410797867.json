{
    "title": "Exploring the occupational biases and stereotypes of Chinese large language models",
    "url": "https://openalex.org/W4410797867",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2223198026",
            "name": "Leilei Jiang",
            "affiliations": [
                "Hefei University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2644983566",
            "name": "Zhu Guixiang",
            "affiliations": [
                "Nanjing University of Finance and Economics"
            ]
        },
        {
            "id": "https://openalex.org/A2133472488",
            "name": "Jianshan Sun",
            "affiliations": [
                "Hefei University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2032104534",
            "name": "Cao Jie",
            "affiliations": [
                "Hefei University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2099029284",
            "name": "Jia Wu",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A2223198026",
            "name": "Leilei Jiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2644983566",
            "name": "Zhu Guixiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2133472488",
            "name": "Jianshan Sun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2032104534",
            "name": "Cao Jie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099029284",
            "name": "Jia Wu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4367595583",
        "https://openalex.org/W4392748121",
        "https://openalex.org/W4375858605",
        "https://openalex.org/W4410204425",
        "https://openalex.org/W4401416853",
        "https://openalex.org/W4405205161",
        "https://openalex.org/W4395465733",
        "https://openalex.org/W4394994587",
        "https://openalex.org/W3199135928",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4377290676",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W2994826132",
        "https://openalex.org/W2111596861",
        "https://openalex.org/W2010006683",
        "https://openalex.org/W4319167005",
        "https://openalex.org/W4386302153",
        "https://openalex.org/W4386249236",
        "https://openalex.org/W4402775249",
        "https://openalex.org/W4206285331",
        "https://openalex.org/W4385574250",
        "https://openalex.org/W4403680681",
        "https://openalex.org/W4386496078",
        "https://openalex.org/W4386730022",
        "https://openalex.org/W3101767999",
        "https://openalex.org/W4287889445",
        "https://openalex.org/W4321488452",
        "https://openalex.org/W4385572500",
        "https://openalex.org/W4318046968",
        "https://openalex.org/W4287887760",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W4312091229",
        "https://openalex.org/W3186191775",
        "https://openalex.org/W4316660749",
        "https://openalex.org/W4281553050",
        "https://openalex.org/W3169113923",
        "https://openalex.org/W4366701024",
        "https://openalex.org/W3174220540",
        "https://openalex.org/W4402683756",
        "https://openalex.org/W4313483977",
        "https://openalex.org/W4376653782",
        "https://openalex.org/W4387294080",
        "https://openalex.org/W2970442950",
        "https://openalex.org/W4293873568",
        "https://openalex.org/W3184144760",
        "https://openalex.org/W2999765337",
        "https://openalex.org/W3104847483",
        "https://openalex.org/W3128232076",
        "https://openalex.org/W2057265946"
    ],
    "abstract": "Abstract Large Language Models (LLMs) are transforming various aspects of our daily lives and work through their generated content, known as Artificial Intelligence Generated Content (AIGC). To effectively harness this change, it is essential to understand the limitations within these models. While extensive prior research has addressed biases in OpenAI’s ChatGPT, limited attention has been given to biases present in Chinese Large Language Models (C-LLMs). This study systematically examines biases in five representative C-LLMs. We collected 90 Chinese surnames derived from authoritative demographic statistics and 12 occupations covering various professional sectors as input prompts. Each prompt was generated three times by the C-LLMs, resulting in a dataset comprising 16,200 generated personal profiles. We then evaluated these profiles for biases regarding gender, region, age, and educational background. Our findings reveal that the content produced by each examined C-LLMs exhibits significant gender and regional biases, as well as age and educational stereotypes. Notably, while most models can generate some unbiased content, ChatGLM stands out as the exception. In contrast, Tongyiqianwen is the only model that may refuse to generate certain content, due to its strong privacy protection mechanisms. We also further analyze the underlying mechanisms of bias formation by examining different stages of the model lifecycle and considering the unique characteristics of the Chinese linguistic and sociocultural context. This paper will contribute substantially to the literature on biases in C-LLMs and provide important insights for users aiming to utilize these models more effectively and ethically.",
    "full_text": "Exploring the occupational biases \nand stereotypes of Chinese large \nlanguage models\nLeilei Jiang1, Guixiang Zhu2, Jianshan Sun1, Jie Cao1 & Jia Wu3\nLarge Language Models (LLMs) are transforming various aspects of our daily lives and work through \ntheir generated content, known as Artificial Intelligence Generated Content (AIGC). To effectively \nharness this change, it is essential to understand the limitations within these models. While extensive \nprior research has addressed biases in OpenAI’s ChatGPT, limited attention has been given to biases \npresent in Chinese Large Language Models (C-LLMs). This study systematically examines biases in five \nrepresentative C-LLMs. We collected 90 Chinese surnames derived from authoritative demographic \nstatistics and 12 occupations covering various professional sectors as input prompts. Each prompt was \ngenerated three times by the C-LLMs, resulting in a dataset comprising 16,200 generated personal \nprofiles. We then evaluated these profiles for biases regarding gender, region, age, and educational \nbackground. Our findings reveal that the content produced by each examined C-LLMs exhibits \nsignificant gender and regional biases, as well as age and educational stereotypes. Notably, while \nmost models can generate some unbiased content, ChatGLM stands out as the exception. In contrast, \nTongyiqianwen is the only model that may refuse to generate certain content, due to its strong privacy \nprotection mechanisms. We also further analyze the underlying mechanisms of bias formation by \nexamining different stages of the model lifecycle and considering the unique characteristics of the \nChinese linguistic and sociocultural context. This paper will contribute substantially to the literature \non biases in C-LLMs and provide important insights for users aiming to utilize these models more \neffectively and ethically.\nKeywords AI-generated content (AIGC), Chinese Large Language Models (C-LLMs), Bias, Occupational \nstereotype\nWith the advent of ChatGPT, a phenomenal product by OpenAI, the large language model (LLM) industry \nand the field of Generative AI have witnessed rapid expansion 1. In recent years, international giants such \nas OpenAI, Microsoft, and Google have actively strengthened their positions in this domain. Currently, the \nChinese market has experienced significant innovations, with leading domestic companies including Baidu, \nAlibaba, and iFLYTEK launching their own chinese large language models (C-LLMs) tailored specifically to \ntheir business objectives and strategic requirements. Large language models(LLMs), pre-trained on extensive \ndatasets, can capture complex linguistic structures and patterns, exhibiting remarkable capabilities in language \ncomprehension and text generation. Consequently, these models have been widely employed for various natural \nlanguage processing tasks, including translation, text generation, and question answering, finding increasing \napplications in healthcare2, education3, and finance4. Compared to global counterparts such as ChatGPT, C-LLMs \nare specifically optimized for China’s linguistic environment, cultural nuances, and market demands, providing \ndistinctive advantages in addressing local issues, regulations, and application scenarios. As a result, a growing \nnumber of users increasingly depend on C-LLMs for tasks such as data collection, analytical reasoning, and \nintelligent customer service5,6. In the future, C-LLMs are expected to fundamentally transform organizational \nprocesses, ranging from sentiment analysis to personalized recommendations7,8.\nAs with any technological advancement, the societal impact of artificial intelligence depends on its \napplication9. While C-LLMs have significantly benefited businesses and society, the biases in their generated \ncontent are well-documented across fields such as healthcare 10, workplaces11, and education 12. For instance, \noccupations such as firefighters, security guards, and engineers are often stereotypically associated with male \nfigures, highlighting societal gender biases prevalent in professions involving physical strength and technical \n1College of Management, Hefei University of Technology, Hefei 230009, China. 2College of Information Engineering, \nNanjing University of Finance and Economic, Nanjing 210023, China. 3College of Computing, Macquarie University, \nSydney, NSW 2109, Australia. email: cao_jie@hfut.edu.cn\nOPEN\nScientific Reports |        (2025) 15:18777 1| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports\n\nexpertise13. This phenomenon, termed occupational gender segregation in management literature, arises from \nsystemic social factors rather than physiological differences 14, resulting in biased career choices based on \ngender. Additionally, society holds age-related stereotypes for certain occupations 15. For example, models are \ngenerally viewed as younger individuals, while professors are commonly perceived as older. When C-LLMs \nabsorb these societal stereotypes through training data, they may unintentionally exhibit age biases regarding \nspecific professions. Similarly, the educational distribution within various professions reflects societal tendencies \nto evaluate occupations based on perceived superiority or inferiority, often correlating with the proportion of \npractitioners holding advanced educational degrees. Studies indicate that biased LLMs can shape users’ views and \nbehaviors in harmful ways, marginalizing certain groups and hindering social progress16. Although considerable \nresearch has explored biases within international models like ChatGPT, GPT-3, and LLaMA-2, there remains \nlimited systematic research regarding biases specific to C-LLMs17,18.\nGiven their robust capabilities in text generation and reasoning, it is imperative to examine potential biases in \nC-LLMs. As is illustrated in Fig. 1, this study addresses this critical research gap by designing a novel experimental \napproach. Specifically, we selected representative combinations of Chinese surnames and occupations to create \ndiverse input prompts, requiring the C-LLMs to generate biographical content. Key information was extracted \nfrom these generated profiles, which was then statistically analyzed to identify biases across five dimensions: \ngender, age, occupation, educational level, and region. Our empirical findings provide compelling evidence of \nsignificant gender and regional biases within the generated content, as well as reinforced stereotypes related \nto age, educational background, and occupation. This study also examines the underlying mechanisms of bias \nformation by analyzing different stages of the model lifecycle. It further incorporates the unique characteristics \nof the Chinese linguistic and sociocultural environment to explore how bias gradually emerges through systemic \nprocesses during training, modeling, evaluation, and deployment.\nDue to the widespread use of LLMs in various application scenarios 19, our research makes the following \ncontributions: First, this paper explores the biases of C-LLMs from the perspective of generating biographical \ncontent, enriching the existing literature on the biases of C-LLMs and advancing theoretical developments in the \nfield of natural language processing. Second, recognizing that unique characteristics of different C-LLMs, our \nfindings offer practical guidance for users in selecting appropriate models and assist developers in enhancing \nmodel fairness. Ultimately, we aim to offer bias-related insights that will steer the development of C-LLMs \nand other generative language models towards more ethical, equitable, and advantageous outcomes, thereby \nmitigating potential adverse societal impacts.\nRelated works\nBias evaluation in LLM\nTo help individuals better leverage the transformative potential of LLMs, researchers have extensively studied \nbiases inherent in models like ChatGPT and GPT-3. hn and Oh 20 employed the “Category Bias Score” metric, \nwhich quantifies ethnic bias through normalized probability variance of category words and evaluates model \npredictions of attribute words. Smith et al. 21 introduced the HolisticBias dataset to measure biases in LLMs, \nuncovering disparities in semantic probability descriptions, text generation styles, and classifications of offensive \ncontent. Furthermore, in the demographic domain, Armstrong et al.22 have used LLM to evaluate and generate \nresumes, revealing potential biases related to gender, race, and education that may affect the recruitment process. \nSimultaneously, Newstead et al.23 discovered gender bias in leadership development with Generative AI, which \nrooted stereotypes of female leaders being timid and emotional, while male leaders were seen as decisive and \nauthoritative. In politics, researchers such as Venkit et al. 24 have found a significant bias in GPT-2 towards \ncountries with fewer internet users by generating stories about different national populations and analyzing \ntheir emotional tendencies. Regarding personalized recommendations, Zhang et al. 25 proposed the Fairness \nin Large Language Models (FaiRLLM) evaluation metric, showing that ChatGPT often displays preferences or \ndiscrimination in generating movie and music suggestions based on user sensitive attributes such as age, race, \nand gender. Finally, in emotion detection, Huang et al.26 capplied counterfactual evaluation methods to measure \nemotional biases related to attributes like nationality, profession, and names.\nFig. 1. Framework for evaluating bias in content generated by C-LLMs. The AIGC can be categorized into \nthree types: Certain Content, Fuzzy Content, and Unknown Content, which represent different levels of \nconfidence in the model’s responses. Then the bias exhibited by the model is evaluated through statistical \nanalysis.\n \nScientific Reports |        (2025) 15:18777 2| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\nBias mitigation in LLM\nTo ensure LLMs serve society equitably, safely, and responsibly, researchers have explored bias mitigation \ntechniques across three stages: Pre - processing, In-training, and Post - processing. Pre-processing strategies \nfocus on adjusting model inputs, such as data and prompts, to influence output. For example, Borchers et al. 27 \nfine-tuned a model using job advertisement data to reduce bias and enhance authenticity. Lu et al. 28 employed \nCounterfactual Data Augmentation (CDA) to combat occupational gender bias by adding pairs matched to \ngender, encouraging the model to ignore gender distinctions. Similarly, Venkit et al.29 reduced nationality bias \nby inserting positive adjectives before prompts, curbing negative portrayals of certain countries. In-training \nmethods adjust the training process to reduce bias. Park et al. 30 introduced Stereotype Neutralization (SN) \nregularization, which orthogonalizes stereotype-related words from gender direction vectors, diminishing \ngender bias in occupational terminology. Li et al. 31 proposed a two-stage debiasing model combining prompt \nadjustment and contrastive learning, amplifying bias in the first stage and decreasing similarities in the second. \nGaci et al.32 equalized attention scores in text encoders to reduce social bias while maintaining semantic integrity. \nPost-processing techniques address bias in model outputs. Tokpo and Calder 33 used LIME 34 to identify and \nneutralize biased words, preserving content while adjusting tone. Similarly, Wang et al.35 introduced a dataset for \npolite language rewriting, training a model to generate more polite outputs that retain semantic meaning while \naltering sentiment and emotion. Fig. 2 shows the pathway for mitigating bias in LLM.\nMethodology\nInvestigated C-LLMs\nUnder a rigorous investigation, we finally selected five representative C-LLMs for comprehensive evaluation. \nThe first model is ChatGLM( https://chatglm.cn/main/alltoolsdetail), developed based on the ChatGLM2 \narchitecture, which excels in multi-turn dialogue, content creation, and information summarization. The second \nmodel is Xinghuo( https://xinghuo.xfyun.cn/desk), a strong competitor to ChatGPT, with core strengths in text \ngeneration, logical reasoning, and multi-modal interaction. Wenxinyiyan( https://yiyan.baidu.com/), Baidu’s \nnext-generation knowledge-enhanced LLM, has a user base that exceeds 200 million, and its daily API call \nvolume exceeds 200 million, reflecting its excellent performance. Tongyiqianwen(  h t t p s : / / t o n g y i . a l i y u n . c o m / q \ni a n w e n / ? s t = n u l l     ) , launched by Alibaba Cloud, is one of the first four domestic models to pass the ’large model \nstandard compliance evaluation’ , meeting national standards for versatility and intelligence. Lastly, the Baichuan \nAI( https://platform.baichuan-ai.com/playground) integrates technologies such as intent understanding, \ninformation retrieval, and reinforcement learning, demonstrating outstanding performance in knowledge-based \nquestion answering and text creation. All of the C-LLMs examined in this study have employed Reinforcement \nLearning Human Feedback (RLHF), resulting in outstanding performance across a wide range of applications. \nSpecifically, RLHF is applied during the later stages of model training, utilizing Supervised Fine-Tuning (SFT) \nand reinforcement learning strategies informed by human feedback. The primary aim of this approach is to \nenhance the model’s performance and alignment on specific tasks. This training method enables the model to \ngain a deeper understanding of human intentions, follow instructions more effectively, and achieve seamless \nmulti-turn dialogue. Table  1 summarizes the relevant details of the C-LLMs investigated in this study.\nModel Version Creator Release year RLHF\nChatGLM Glm-4 Tsinghua University 2023.8.31 Ye s\nXinghuo Spark4.0 ultra iFlytek 2024.6.27 Ye s\nWenxinyiyan ERNIE-3.5-8K Baidu 2023.7.06 Ye s\nTongyiqianwen Qwen-plus Ali Cloud 2024.6.24 Ye s\nBaichuan AI Baichuan 4 Baichuan AI 2024.5.22 Ye s\nTable 1. Relevant information of the five examined C-LLMs.\n \nFig. 2. Pathways for applying pre-processing, in-training, and post-processing bias mitigations in LLM. (a) \nPre-Processing mitigations modify the model’s inputs, including training data and prompts, to reduce bias \nbefore training begins.(b) In-Training mitigations alter learning procedures during the training process, such \nas adjusting loss functions. (c) Post-Processing mitigations refine the model’s initial outputs after inference, \napplying bias correction mechanisms to produce less biased responses.\n \nScientific Reports |        (2025) 15:18777 3| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\nData collection and preprocessing\nIn our study, we utilized Python to implement the application programming interface (API) for the inspected \nC-LLMs. This API interface is designed to facilitate interaction between users and the model and to collect \nresponse data from the model based on specific prompt inputs. When evaluating the output quality of C-LLMs, \nwe observed that these models generate text based on the probability distributions and patterns found in their \ntraining data. The randomness of the model can be adjusted by modifying the temperature parameter. To achieve \nan appropriate level of variation in the responses, we maintained the default temperature setting of 1 during the \nAPI calls. This approach was chosen to ensure the research results are both reliable and diverse.\nTo optimize the model’s input prompts, we carefully selected 90 representative Chinese surnames and 12 \noccupational categories. The surname list was based on the official ranking published by the Ministry of Public \nSecurity, covering more than 80% of the national population and featuring widespread distribution across \nregions, thus ensuring high representativeness. Regarding occupational categorization, we considered both \ngender distribution and societal function. Specifically, we included three professions with clear hierarchical \nstructures (Teaching Assistant, Teacher, Professor), three male-dominated professions (Programmer, Chef, \nArchitect), three gender-balanced professions (Journalist, Lawyer, Doctor), and three female-dominated \nprofessions (Nurse, Model, Flight Attendant). Although the sample is limited in scope, these occupations span \nmajor societal sectors such as education, healthcare, law, media, and service industries, and exhibit distinct \ngender structures. Therefore, they serve as a reasonable entry point for initial exploration of potential gender \nand social biases in LLM outputs. In constructing the prompts, we combined surnames with occupations to \nelicit model-generated personal descriptions containing attributes such as gender, age, educational background, \nand place of origin. To eliminate contextual carryover, we restarted the dialogue session after each successful \ngeneration to ensure output independence. Given the importance of analyzing gender-related probabilities, each \nsurname-occupation pair was repeated three times, and all outputs were collected. As a result, each occupational \ncategory yielded a total of 270 data samples. Detailed information on the sample structure is presented in Fig. 3.\nIt is worth noting that in assessing gender bias, we relied on national labor statistics as a reference baseline. \nHowever, for variables such as educational background, age, and regional origin, due to the lack of publicly \navailable occupational distribution data, we focused on observing distributional trends in the generated content \nrather than quantifying bias with strict statistical comparisons. Our analysis therefore places greater emphasis \non identifying qualitative stereotypes manifested in the model outputs, rather than measuring precise levels of \ndeviation.\nThe following is an example of the specific research conducted:\nPrompt: Please generate an introduction for (Liu) (Assistant), including relevant information such as gender, \nage, educational background, and place of origin.\nIn our study, which focused on the analysis of content generated by C-LLMs, we used a string matching \napproach using Python to extract information on gender and educational background 36. Furthermore, we \nutilized regular expressions to search for and retrieve details on age and place of origin. The extracted information \nwas then outputted in Excel format using the “pandas” library 37. To ensure the accuracy and effectiveness of \nsubsequent data analysis, we implemented the following preprocessing steps on the collected data: Firstly, we \nobserved that among all C-LLMs, only Tongyiqianwen refused to generate specific content when generating \npersonal introductions for certain professions, explicitly acknowledging its nature as an AI and emphasizing \nits commitment to privacy protection. For this subset of data, we decided to remove it from our analysis. \nAdditionally, the study found that, except for ChatGLM, the other four C-LLMs exhibited a certain proportion \nof neutral responses in their output-failing to clearly express personal gender, age, and place of origin. Table  2 \nsummarizes the specific forms of these neutral and biased responses.\nFig. 3. The components of total sample date.\n \nScientific Reports |        (2025) 15:18777 4| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\nWe further analyzed neutral or unbiased responses separately. Fig.  4 illustrates the count (y-axis: Unbiased \nValue, indicating the number of unbiased descriptions as defined by Table 2) of neutral descriptions across \ngender, age, and region generated by examined C-LLMs. Results show higher regional neutrality across all \nmodels, whereas gender and age dimensions exhibit relatively lower unbiased occurrences. Specifically, Xinghuo \nexhibits the highest frequency of neutral descriptions regarding region, while Baichuan AI performs better in \nproducing unbiased outputs for gender and age. Conversely, Tongyiqianwen generally displays lower unbiased \noccurrences, reflecting clear disparities among the models. These differences potentially result from variations \nin training data distributions, socio-cultural contexts, and annotation methodologies. Selecting an appropriate \nmodel requires careful consideration of these biases according to specific application scenarios. Future research \nshould emphasize enhancing gender and age neutrality through targeted fine-tuning strategies to improve the \noverall fairness of C-LLMs.\nExperiments and analyses\nThis study systematically encoded and quantitatively analyzed key categorical variables, including occupation, \nsex, age, educational level, and region. We conduct a correlation analysis, and the coefficients reported are Pearson \ncorrelation coefficients, with a significance level of α =0 .05, to examine the degree of association between \noccupation and the variables of gender, age, educational level, and region. As shown in Table  3, the analysis \nreveals statistically significant correlations between occupation and gender, age, and educational background \n(p< 0.05, marked with * in the table), whereas no significant correlation is observed between occupation and \nregion (p> 0.05). These results indicate the presence of occupational stereotypes in C-LLMs related to gender, \nage, and educational attainment, but no evident occupational stereotyping with respect to regional distribution. \nBased on these findings, the subsequent analysis focuses on examining biases and representational tendencies \nin C-LLMs across the four dimensions of gender, age, education, and region, while further exploring the \noccupational stereotypes reflected in gender, age, and educational background.\nFig. 4. Unbiased statistical representation of age, gender, and region in C-LLMs. A higher value indicates \nstronger unbiased performance in the corresponding dimension.\n \nAspect Unbiased description Examples of unbiased outputs Examples of biased outputs\nGender he/she, fill in according to the actual situation, man/woman, \ngender unknown.\nDr. Liu, (man/woman), Age: (can be filled in as \nneeded), holds a Ph.D. degree and is from (a certain \nregion in China). (He/She) has a gentle and kind \npersonality, with extensive clinical experience, \nearning the trust and praise of numerous patients.\nDr. Liu, (male), (35) years old, \nwas born in the beautiful city \nof (Suzhou, Jiangsu Province), \nChina. He graduated from a \nprestigious Chinese medical \nuniversity with a Doctor of \nMedicine (M.D.) degree. (He) \nis committed to providing \nhigh-quality medical care to \nhis patients.\nAge fill in according to the actual situation, age unknown, xx \nyears old, unlimited.\nRegion\nChina, a certain province, a certain city, a certain region, \ncould be anywhere, fill in according to the specific situation, \narea unknown, unlimited.\nTable 2. Unbiased and biased examples of personal information generation by C-LLMs related to gender, age, \nand region. This table presents examples of unbiased and biased outputs generated by C-LLMs in the fields of \ngender, age, and region, focusing on their performance in unbiased descriptions.\n \nScientific Reports |        (2025) 15:18777 5| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\nBias evaluation\nGender perspective\nTo quantify the degree of gender distribution bias in various professions generated by different models, this \nstudy uses Eq.( 1) as a metric to evaluate the gender bias of each model between professions. In this context, \nB represents the bias rate, f indicates the observed gender proportion, and g denotes the expected gender \nproportion. However, due to the presence of unbiased items and unknown items (the portions of the data that \nTongyiqianwen refused to generate) in the collected model generated data, we have optimized and extended \nEq.(1) and proposed Eq.( 2) for a more accurate calculation of occupational gender bias. Eq.( 2) innovatively \nincorporates the impact of gender unbiased and unknown items on gender proportion within occupation j \n(uj ). uj  cannot accurately reflect the gender distribution. If these neutral or unknown proportions were not \nconsidered (directly using Hij  as the baseline without adjusting for unbiased items), it would distort the bias \nmeasurement due to incomplete and imprecise gender proportion calculation. Hence, by incorporating uj , \nEq.(2) effectively excludes the neutral or unknown proportions, ensuring higher accuracy and data integrity \nin bias measurement. Notably, a positive value indicates that the gender proportion of an occupation in AIGC \nexceeds its proportion in the labor market, reflecting a preference for that gender. A negative value signifies that \nthe gender representation of an occupation in AIGC falls below its proportion in the labor market, indicating \nan underrepresentation or bias against that gender. The closer the value is to zero, the smaller the gender bias in \nthat occupation. Furthermore, Eq.(3) is used to calculate the overall bias Btj  for each occupation, where B1j  \nrepresents the male bias and B2j  represents the female bias in profession j.\n \nB = f − g\ng  (1)\nwhere B represents the relative bias, which measures the deviation between the measured value and the true \nvalue. f denotes the measured value, g represents the expected value.\n \nBij = Fij − Hij ∗ (1 − uj )\nHij ∗ (1 − uj ) ,i =1 , 2; j =1 , 2, ...,12 (2)\nwhere Bij  represents the degree of bias in occupation j toward gender i ( i=1 for male, i=2 for female). Fij  \ndenotes the proportion of gender i in occupation j within the model-generated content, while Hij  represents the \nexpected proportion of gender i in occupation j based on Chinese Statistical Y earbook and employment reports \npublished by authoritative statistical institutions. uj  refers to the proportion of unbiased and unknown terms \nrelated to gender in occupation j, reflecting considerations of data completeness and accuracy.\n Btj = |B1j | + |B2j | , 1= male;2= female (3)\nwhere Btj  represents the overall gender bias in occupation j, B1j  denotes the absolute bias for males in \noccupation j, and B2j  denotes the absolute bias for females in occupation j.\nBased on the calculations from Eq.(3), we have plotted the average overall bias for each examined C-LLMs, \nas shown in Fig. 5a. The data indicate that ChatGLM performs the best among the models, with an overall bias \nof 0.474. Wenxinyiyan, Tongyiqianwen, and Baichuan AI follow, with overall biases of 0.524, 0.535, and 0.557, \nrespectively. Although the latter two exhibit relatively higher levels of bias, they still perform better than the \nworst model, the Xinghuo Large Model. The Xinghuo Large Model shows the highest overall bias, reaching a \nsignificant level of 0.776. This could have implications for the fairness of large language models in areas such as \nrecruitment, career planning, and education.\nFigures 5b and 5c respectively illustrate the male and female occupational gender biases exhibited by \ndifferent C-LLMs. Our findings reveal a clear presence of occupational stereotypes: The examined C-LLMs \ngenerally exhibit a preference for males when describing class-based occupations, male-dominated professions, \nand occupations with a relatively balanced gender distribution (as evidenced by positive values in Fig. 5b and \nnegative values in Fig. 5c). This gender preference only disappears when describing occupations predominantly \nheld by women, shifting instead to a preference for females (negative values in Fig. 5b and positive values in Fig. \n5c). For instance, in the nursing profession, all models consistently demonstrate a strong preference for women. \nNotably, in female-dominated occupations such as modeling and flight attendants, Xinghuo, Tongyi Qianwen, \nand Baichuan AI still exhibit varying degrees of male preference. This observation suggests that these three \nmodels require further optimization and refinement to improve stability and mitigate biases in their generated \ncontent.\nVariables\nChatGLM\n(Job)\nXinghuo\n(Job)\nTongyiqianwen\n(Job)\nWenxinyiyan\n(Job)\nBaichuan AI\n(Job)\nSex 0.371* 0.278* 0.447* 0.428* 0.325*\nAge -0.516* -0.465* -0.442* -0.510* -0.404*\nEducation -0.649* -0.646* -0.650* -0.583* -0.659*\nRegion -0.023 -0.002 -0.029 -0.018 -0.014\nTable 3. Pearson correlation coefficients between occupation and gender, age, education level, and region.\n \nScientific Reports |        (2025) 15:18777 6| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\nAge perspective\nIn the evolutionary process of careers, the close correlation between vocational choices and individual age forms \na significant dimension for research. Individuals in different age groups exhibit distinct characteristics in their \ncareer choices and development cycles. This paper elaborates on the age distribution in the content generation \nprocesses of C-LLMs, aiming to uncover the underlying patterns. Table 4 presents a descriptive statistical analysis \nof the age distribution in the outputs of each model. After eliminating the unbiased and unknown elements from \nthe models, we ensured that the effective data size for each C-LLMs was no less than 2,748. The study found \nthat the average age of the content generated by the models ranges from 35 to 40 years old, coinciding with the \nmature stage of a career. This finding reflects the model’s emphasis on the core strength of the vocational lifecycle \nduring content construction. It is noteworthy that Tongyiqianwen exhibits the highest standard deviation in \nage distribution, indicating extensive coverage and diversity in the age span of generated content. In contrast, \nXinghuo, with the lowest standard deviation of 7.183, highlights a high concentration of its output content at \nStatistics ChatGLM Xinghuo Tongyiqianwen Wenxinyiyan Baichuan AI\nN 3240 3075 2748 3000 2929\nMin 23 18 25 20 20\nMax 55 68 58 58 65\nMean 35.19 36.57 38.64 38.12 38.37\nStandard\nDeviation 7.913 7.183 8.586 7.764 8.003\nSkewness 1.039 0.531 0.362 0.490 0.303\nKurtosis 0.454 -0.078 -0.832 -0.419 -0.665\nTable 4. Descriptive statistics of age in content generated by examined models.\n \nFig. 5. Gender bias in occupational representation by C-LLMs. (a) Total gender bias: The comparison of \nthe average overall gender bias across all occupations for each examined C-LLMs. Error bar indicates 95% \nconfidence interval. (b) Bias of Man: A positive value indicates a bias toward men in the given profession, while \na negative value suggests an underrepresentation of men in that profession. (c)Bias of Woman: A positive value \nindicates a bias toward women in the given profession, while a negative value suggests an underrepresentation \nof women in that profession.\n \nScientific Reports |        (2025) 15:18777 7| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\nthe age level. This may be attributed to the model’s preference for data from a particular age group during the \ntraining process or the limitations of its generation strategies.\nFurther analysis reveals that the ChatGLM model shows significant positive skewness, favoring younger age \ngroups, while the Baichuan AI model, with skewness near zero, maintains a balanced age distribution. This \ndiscrepancy may be due to differences in the design of the model architecture, the diversity of training datasets, \nand the strategies to control randomness in content generation algorithms. The variance in skewness metrics \noffers a unique perspective on the preference of different C-LLMs algorithms for age characteristics. Moreover, \nmost models display negative kurtosis values, suggesting that these models tend to avoid outputs at extreme age \nvalues during content generation. This characteristic may be influenced by multiple factors, such as the design of \nthe loss function of the model, regularization strategies, and optimization objectives. In-depth analysis of the age \ndistribution differences among these models can facilitate further optimization, enabling the generated content \nto better align with the needs of specific application scenarios.\nAs illustrated in Fig.  6a, C-LLMs generally display a consistent trend in predicting the mean age levels for \nvarious occupations. Specifically, for occupations such as professors, teachers, and doctors, the predicted average \nage by the models tends to be relatively high (40-60 years old). This result can be attributed to the fact that these \noccupations typically require extensive professional experience and long-term accumulation of expertise. In \ncontrast, for occupations such as models and flight attendants, the predicted average age is lower (20-30 years \nold), which can be explained by the preference for younger professionals in these fields.\nFurther analysis of the data presented in Fig.  6b reveals significant fluctuations in the models’ prediction \nresults across different age groups. Notably, while the Xinghuo model peaks at the 26-30 and 36-40 age ranges, \nother models reach their predictive peaks within the 31-35 and 41-45 age brackets, this discrepancy may stem \nfrom differences in the training data and algorithmic processing methods used by various C-LLMs. Predictions \nfor those under 25, between 36-40, and over 46-50 are markedly lower. This result indicates that C-LLMs tend \nto favor middle-aged individuals ( between 30 and 45 years old) when outputting age-related characteristics for \noccupations, with notably less representation for extremely young or older groups.\nFrom a perspective of social impact, such bias can exacerbate stereotypes about young and old individuals, \nreduce their representation in application scenarios, and consequently affect the completeness and fairness of \nthe model. Therefore, to enhance the models’ generalization capabilities and promote fairness in applications, \nit is imperative to strive for greater diversity in training datasets. This includes encompassing a wider range of \noccupational categories, age levels, and cultural backgrounds. Ensuring this diversity will allow the C-LLMs to \nmore accurately reflect and serve the needs of all strata of society.\nEducation perspective\nFig. 7a illustrates the distribution of educational backgrounds in content generated by different models. The \nanalysis reveals a general tendency across all models to produce individuals with bachelor’s degrees or higher, \nsuggesting that these models are more likely to associate professions with higher levels of education. Specifically, \nChatGLM generates the highest proportion of doctoral-level profiles, while Tongyiqianwen and Wenxinyiyan \nproduce more individuals with master’s degrees. ChatGLM and Xinghuo lead in generating bachelor’s degree \nholders. In contrast, the Baichuan AI model shows a more balanced distribution across bachelor’s, master’s, and \ndoctoral levels. These differences in the distribution of educational background among the models reflect their \napplicability at various educational levels, providing researchers with a basis for selecting appropriate models \nthat better match the educational background and knowledge needs of the users.\nFurthermore, to deeply explore the distribution of educational backgrounds within professional fields, this \nstudy categorizes occupations and conducts a statistical analysis of the educational distribution in content \ngenerated by five C-LLMs, as shown in Fig. 7b. The data reveal a distinct stratification of educational levels across \nFig. 6. Age bias in occupational representation by C-LLMs. (a) Age Distribution Across Occupations: The \nvertical axis represents the average age predicted by the model for each occupation. (b) Age Group Count: The \nvertical axis represents the frequency of occurrences for each age group in the model’s predictions.\n \nScientific Reports |        (2025) 15:18777 8| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\ndifferent professions. Doctoral degrees are mainly focused on professors and doctors; professions like journalists \nand architects have a high percentage of master’s degrees; bachelor’s degrees are predominant among nurses and \nflight attendants, while occupations such as chefs and models have a higher proportion of degrees at the bachelor’s \nlevel or below. These characteristics of educational distribution across occupations suggest that C-LLMs, to \nsome extent, perpetuate societal stereotypes and biases regarding the educational levels of professions. In other \nwords, professions with a higher proportion of individuals holding advanced degrees are often perceived as \nmore advantageous or higher-tiered. This phenomenon is manifested in the output of large language models, \nrevealing a new dimension of professional bias: the potential preference and reverence for professions associated \nwith higher education. This analysis helps us better understand the multiple factors contributing to differences \nin educational distribution and provides a research pathway for evaluating the effectiveness of C-LLMs and \nthe dynamic impact on various professions. It holds significant importance in promoting model fairness and \nreducing bias.\nRegional perspective\nFig. 8 illustrates the distribution characteristics of various Chinese provinces of origin involved when different \nC-LLMs generate introductory content. This study aims to delve into the coverage and application status of \nvarious C-LLMs within different regions of China by examining this distribution.\nAs observed from Fig.  8, the five C-LLMs exhibit significant regional heterogeneity in terms of application \nand data distribution across Chinese regions. Firstly, concerning the breadth of regional coverage, the Xinghuo \nmodel has the narrowest scope, covering only 11 regions. In contrast, the Tongyiqianwen model has the most \nextensive regional coverage, reaching up to 17 regions. This comparison suggests that the Xinghuo model \nrequires further optimization in terms of its generalization capability. Secondly, regarding the depth of regional \ncoverage, the performance of each model varies. Specifically, all models exhibit the highest data density in \nJiangsu Province, with Sichuan, Guangdong, Shandong, and Hunan provinces also demonstrating relatively \nhigh values in some models. This indicates that these provinces are the primary areas for the application or data \nsourcing of the models. Furthermore, moderate values are observed in provinces such as Zhejiang, Liaoning, \nHubei, and Hebei, suggesting that although these provinces have a certain degree of representativeness in the \napplication of the models, their influence remains somewhat limited compared to regions with higher values. \nNotably, provinces in the west, south and northeast, such as Xinjiang, Qinghai, and Yunnan, exhibit low data \ndensity or even a complete absence of data in most models. This observation directly reflects the inadequacies \nin the data collection and application coverage in these regions, highlighting a significant bias in the regional \ncoverage of the models.\nIn total, Fig. 8 not only provides a visual representation of the uneven geographical coverage and application \nof the five C-LLMs, but also deeply reveals the potential regional bias issues that may exist in their development \nand application. If such biases are not addressed, they can lead to the neglect of specific regional cultures, \nlinguistic characteristics, and user needs, thereby affecting the overall performance and universality of the \nmodels. Therefore, future developers of C-LLMs should prioritize the diversity and balance of data sources. By \noptimizing data collection strategies and algorithm design, they should strive to reduce and eliminate regional \nFig. 7. Educational background bias in occupational representation by C-LLMs. (a) Educational Bias Across \nModels: This figure illustrates the educational preferences exhibited by different C-LLMs. (b) Educational \nDistribution Across Occupations: This figure presents the proportion of different educational levels associated \nwith various professions in AIGC.\n \nScientific Reports |        (2025) 15:18777 9| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\nbiases, with the aim of constructing a more comprehensive, fair, and efficient intelligent language processing \nsystem.\nBias mechanism analysis\nThe biases and stereotypes regarding gender, age, educational background, and regional characteristics exhibited \nby C-LLMs in generating personal profiles are not incidental errors. Instead, they are systemic results arising \nfrom interconnected influences throughout the model lifecycle. From training data collection and model training \nmechanisms to evaluation designs and deployment interactions, decisions made at every stage may either \nintroduce or amplify biases. Furthermore, the distinctive features of the Chinese language and its sociocultural \nenvironment further exacerbate the complexity and subtlety of these biases. As illustrated in Fig. 9, this section \nwill thoroughly examine the mechanisms by which biases are produced in C-LLMs, emphasizing four critical \nstages within the specific context of the Chinese language.\nBias introduction at the training data stage\nTraining data forms the foundational layer of knowledge representation in LLMs, and its quality and \nrepresentativeness play a decisive role in shaping the fairness and diversity of model outputs. In the case of \nC-LLMs, the pre-training corpora often exhibit pronounced structural biases 38. Existing studies indicate that \napproximately 65-80% of the pre-training data for C-LLMs come from high-traffic platforms such as social \nmedia, news outlets, and digital publications. These sources disproportionately reflect the linguistic styles and \nsociocultural ideologies of urban, young, and highly educated users, while significantly underrepresenting \nmarginalized voices, minority dialects, and cross-cultural expressions39. This imbalance leads C-LLMs to overfit \nmainstream narratives, reducing their ability to generalize across diverse demographic and cultural contexts.\nMoreover, widespread societal biases embedded in the source data, such as rigid gender norms, regional \nstereotypes, and occupational hierarchies, are easily internalized by the model if not explicitly mitigated. For \nexample, in training corpora, terms like “engineer” and “leader” frequently co-occur with male pronouns, whereas \nroles such as “nurse” or “secretary” often align with female references40. Furthermore, data collection practices \nalso amplify these structural biases. In pursuit of scale and efficiency, developers often prioritize high-engagement \nplatforms, which inherently filter for popular discourses and trending topics. As a result, marginalized narratives \nare not only under-collected but may also be systematically removed during preprocessing. Human annotators \nFig. 8. Regional distribution in the content generated by C-LLMs. Map lines delineate study areas and do not \nnecessarily depict accepted national boundaries.\n \nScientific Reports |        (2025) 15:18777 10| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\nor content moderation algorithms frequently exclude politically sensitive or non-mainstream content, forming \nthe so-called “information silence zones”41. This selective representation in training data becomes the root cause \nof bias propagation in downstream outputs.\nBias amplification in model training mechanisms\nC-LLMs are primarily trained to minimize prediction errors, a process that inherently biases the learning \ndynamics toward dominant linguistic patterns and high-frequency co-occurrences. This optimization objective \ncauses models to preferentially internalize mainstream expressions while neglecting rare or marginalized \nlanguage features42. The Transformer architecture, particularly its attention mechanism, further reinforces these \ntendencies by amplifying associations between frequently co-occurring tokens. For instance, the CORGI-PM \ncorpus reveals that occupational terms like “programmer” are predominantly associated with male pronouns, \nreflecting societal stereotypes embedded in training data43. Similarly, Zhao et al. demonstrate that large language \nmodels tend to associate technical professions with male identities, further perpetuating gendered assumptions44. \nThese associations are not neutral byproducts of statistical modeling, but rather reflections of deeply embedded \nsocietal norms present in the training data.\nMoreover, the dynamics of model optimization exacerbate these initial biases. An internal technical report \non ERNIE 3.0 reveals that the gradient variability of gender-related parameters dropped by 73% during the early \ntraining stages, indicating rapid convergence on socially dominant associations 45. This suggests that models \ntend to lock in biased correlations early in the training process due to their optimization trajectory. Without \nregularization mechanisms or counter-bias signals, these early-formed associations become effectively “frozen” \nwithin the model. In addition, instruction fine-tuning and RLHF processes introduce further subjectivity \nthrough human feedback46. When feedback data lack demographic diversity or are filtered through unbalanced \nannotator perspectives, it risks reinforcing the very stereotypes the model was meant to avoid. Therefore, both \nthe architecture design and optimization procedure of C-LLMs play pivotal roles in amplifying initial corpus \nbiases, transforming subtle data imbalances into persistent output patterns.\nBias concealment in the evaluation stage\nThe evaluation stage is intended to ensure quality control and fairness checks during the development of C-LLMs. \nHowever, current evaluation practices often fail to uncover embedded structural biases due to limitations in \nboth assessment metrics and dataset construction. Mainstream metrics such as accuracy, BLEU score, and \nperplexity focus on aggregate semantic performance but are insufficient to detect disparities across demographic \nsubgroups47. Consequently, they may mask performance disparities among different social groups. Recent \nempirical studies reinforce these concerns. For example, the C-EV AL benchmark shows that only 0.7% of its test \ncases involve minority languages, and 99.2% of samples exclusively use standardized Mandarin 48. This severe \nlack of linguistic and cultural diversity constrains the benchmark’s ability to evaluate fairness in non-mainstream \ncontexts. More importantly, traditional metrics struggle to detect discriminatory outputs from models. The \nrecently proposed BEATS bias evaluation framework reveals that conventional single-metric indicators, such as \naccuracy, exhibit significant blind spots in detecting discriminatory outputs from large language models, often \nobscuring biased behaviors toward specific demographic groups49.\nAdditionally, automated evaluation tools exhibit limited effectiveness. These tools struggle to capture implicit \nmeaning, metaphorical usage, and polysemous constructs, especially in Chinese, where contextual cues are \nsubtle and grammar is non-explicit. Koo et al. introduced the Cognitive Bias Benchmark for LLMs as Evaluators \n(CoBBLEr), and found that around 40% of model judgments showed explicit biases. Meanwhile, the Rank-\nFig. 9. Analytical framework for bias and stereotype generation mechanisms in C-LLMs. This figure illustrates \nthe interaction between four critical components-training data, model architecture, model evaluation, and \nmodel deployment-and the unique characteristics of the Chinese context, which collectively lead to bias and \nstereotype generation in C-LLMs.\n \nScientific Reports |        (2025) 15:18777 11| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\nBiased Overlap (RBO) between model outputs and human bias assessments was only 49.6%, revealing significant \ninconsistency50. This suggests that LLMs, even when used in evaluation, may reproduce the very biases they aim \nto detect, leading to a recursive bias loop within the evaluation stage. Furthermore, human evaluators may also \nunconsciously introduce subjective biases due to their homogeneous backgrounds or experiential tendencies. \nPrior studies show that crowd annotators tend to assign biased ratings based on personal experience, especially \nwhen dealing with socially sensitive tasks 51. In summary, existing evaluation systems contain substantial bias \nconcealment across metric design, corpus composition, evaluation tool efficacy, and evaluator subjectivity. \nConsequently, models that perform well under standardized evaluations may still pose considerable risks of \ngenerating unfair outcomes in real-world scenarios.\nBias reproduction in deployment and interaction\nThe deployment of C-LLMs in real-world environments establishes a feedback loop through which social biases \nmay be amplified and reinforced 52,53. Increasingly, C-LLMs are applied in socially sensitive domains such as \neducation, recruitment, healthcare, and legal services. In the absence of robust bias mitigation strategies, these \nmodels may influence users’ perceptions and decisions in subtle but consequential ways. Empirical studies \nhave shown that LLMs trained on biased datasets tend to reproduce occupational stereotypes. A recent audit \nof model-generated job descriptions found that technical roles were assigned male pronouns over 85% of the \ntime, whereas caregiving and nursing roles were disproportionately associated with female pronouns 54. When \nembedded into algorithmic decision-making systems, such biased outputs risk normalizing discriminatory \nassumptions in hiring practices and career guidance applications.\nMoreover, The interactive nature of C-LLMs compounds these risks. These models are highly responsive to \nuser prompts and adept at adapting to linguistic nuances, which enhances usability but also increases vulnerability \nto prompt-induced bias. When users submit biased or leading queries, models frequently affirm the underlying \nassumptions without critique, thereby fostering a “bias echo chamber” over repeated interactions. Prompt-based \nexperiments confirm this phenomenon, showing that gender-biased questions significantly elevate stereotypical \ncompletion rates across mainstream LLMs55. An additional concern is the lack of transparency and explainability \nin most deployed C-LLMs. Operating as black-box systems, these models offer limited interpretability for users \nand developers, making it difficult to trace, understand, or correct biased outputs. Research on AI explainability \nhas shown that low system transparency diminishes user trust and impedes timely detection of harmful outputs \nat the point of use 56. In sum, the combination of high adaptability, low explainability, and wide deployment in \nsocially impactful contexts makes C-LLMs vulnerable to reinforcing harmful social biases in practice. Without \nsystemic safeguards, deployment itself becomes a vector of bias reproduction.\nBias reinforcement in the Chinese linguistic context\nThe unique linguistic structure and sociocultural characteristics of the Chinese language create fertile ground for \nthe reinforcement of biases in C-LLMs. This reinforcement arises primarily from linguistic ambiguity, nuanced \nexpression styles, selective silencing in corpus construction, and distinctive cultural contexts. Firstly, unlike \nIndo-European languages, Chinese lacks explicit grammatical markers for gender, number, or tense, making \nsemantic disambiguation highly context-dependent. As a result, when identity attributes such as profession \nor social status are involved, models often resolve ambiguity using learned statistical associations-frequently \ndefaulting to dominant norms. For example, Chaturvedi et al. found that LLMs associate male-related terms (e.g., \n“ability to work under pressure”) in job postings with a higher probability of recommending male candidates, \nwhile linking female-related terms (e.g., “detail-oriented and patient”) to a higher likelihood of recommending \nfemale candidates54. Consequently, even in the absence of explicit gender indicators, these models internalize \nand perpetuate entrenched gender stereotypes in occupational contexts. Secondly, Chinese discourse is \ncharacterized by high-context communication, where indirectness, euphemism, and metaphor are common \nrhetorical strategies. These features complicate bias detection and mitigation. Expressions such as “leftover \nwoman” or “strong woman” may appear neutral on the surface but encode deeply entrenched gender hierarchies \nand social expectations. Most C-LLMs lack sufficient sociolinguistic grounding to distinguish between literal \nand connotative meaning, thus replicating these implicit biases in output generation57.\nMoreover, Bias reinforcement is further compounded by selective silence in corpus construction. Empirical \naudits of Chinese web corpora have revealed substantial underrepresentation of minority discourses, including \ncontent related to gender equality, LGBTQ+ rights, and regional identities48. Such omissions not only skew the \ntraining distribution but also reinforce dominant ideological narratives. Lastly, the corpus implicitly embodies \nsocietal structures and cultural norms, including traditional gender roles, educational elitism, and regional \nhierarchies, subtly reinforcing these biases within models. Repeated exposure to such structural biases leads \nmodels to adopt them as linguistic norms, translating societal inequalities into algorithmic outputs. Therefore, \nbias reinforcement in C-LLMs is not merely a technical artifact but a sociolinguistic consequence of modeling a \nhigh-context, ideologically stratified language without appropriate cultural calibration.\nDiscussion\nIn this study, we proposed a structured approach to evaluating bias in Chinese large language models (C-LLMs) \nthrough the generation of personal profile texts. Our findings indicate that C-LLMs consistently exhibit traces \nof bias and social stereotypes inherited from their training data. First, with regard to neutrality, all four C-LLMs \nexamined-Xinghuo, Tongyiqianwen, Wenxinyiyan, and Baichuan AI-demonstrate varying degrees of restraint in \ngenerating specific demographic attributes such as gender, age, and education. Among them, only Tongyiqianwen \nclearly adopts a privacy-preserving stance by refusing to produce such content, highlighting notable differences \nin model behaviors that users should consider when selecting a model for specific applications. Second, in the bias \nanalysis across four dimensions-gender, age, education, and regional background-the models reveal consistent \nScientific Reports |        (2025) 15:18777 12| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\npatterns of stereotypical representations. Male bias is prevalent across most occupations, with the default \ngender often assumed to be male; this bias diminishes only in female-dominated professions and occasionally \nreverses. Age preferences cluster around the ranges of 31-35 and 41-45, except in cases where professional norms \ndictate otherwise. Educationally, the models tend to favor profiles with at least a bachelor’s degree, though the \ndegree distribution varies by profession. Regional bias is evident in the overrepresentation of candidates from \nChina’s eastern and central provinces, with limited reference to western and northern regions.These biases \nreflect systemic outcomes shaped by multiple stages in the model lifecycle: data collection during pre-training, \nalgorithmic configurations during training, evaluation criteria during assessment, and user interaction during \ndeployment. Furthermore, the inherent cultural and linguistic characteristics of the Chinese language contribute \nto the subtlety and complexity of bias expression in C-LLMs. Understanding these mechanisms is essential for \ninterpreting model outputs and designing more equitable AI systems.\nThis study has several practical contributions. First, we have shown that C-LLMs exhibit a certain degree \nof bias with respect to gender, age, educational level, and other factors. This finding provides a reference for \nidentifying and correcting biases within the models, contributing to the fairness of generated content, and \nenhancing its acceptance and usage across various applications. Second, By addressing both the micro-level \nmodeling mechanisms and the macro-level linguistic structures, this study systematically elucidates the \nmultifactorial mechanisms underlying bias generation in C-LLMs. It offers structural insights for future efforts \nin bias detection and mitigation mechanism design, and holds promising application prospects. Third, with the \nadvancement of artificial intelligence technology, regulatory bodies are imposing stricter requirements on the \nfairness and transparency of LLMs. Investigating model bias aligns with relevant regulations and standards, and \nit contributes to the further development of natural language processing technologies, leading to more efficient \nand reliable model design and application.\nNotwithstanding the above, this study has several limitations that future research should address. First, the \noccupational scope covered here is somewhat limited, omitting emerging professions and highly specialized \ntechnical roles. Future research should broaden the occupational classification systems and refine the research \nscope to more accurately capture dynamic labor market changes. Second, this study analyzed only five C-LLMs \nand excluded models developed in other languages. Future studies could include additional models, such as \nOpenAI’s ChatGPT, and conduct comparative assessments to explore how biases differ among models across \nvarious domains. Moreover, while this research applied a strict definition for assessing gender bias the analysis of \nage, education, and regional biases relied on descriptive methods due to a lack of baseline data. This inconsistency \nlimits bias measurement precision. Future research should collect or integrate detailed demographic benchmark \ndata to facilitate more systematic and comparable bias evaluations. Finally, this study primarily employed \nsummary-generation tasks to analyze biases in C-LLM outputs, which may introduce subjectivity or ambiguity. \nFuture research should consider alternative task types, such as disambiguation tasks 17, to better define and \nquantify potential biases in these models. As research in this field progresses, C-LLMs are expected to evolve \ntoward greater equity and efficiency.\nData availability\nThe data and code necessary to replicate this project are available at  h t t p s : / / d a t a . m e n d e l e y . c o m / d a t a s e t s / n 5 4 m \nk s 9 d z x / 1     .  \nReceived: 22 February 2025; Accepted: 23 May 2025\nReferences\n 1. Wu, T. et al. A brief overview of chatgpt: The history, status quo and potential future development. IEEE/CAA J. Autom.Sinica 10, \n1122–1136 (2023).\n 2. Tan, Y . et al. Medchatzh: A tuning llm for traditional chinese medicine consultations. Comput. Biol. Med. 172, 108290 (2024).\n 3. Tsai, M.-L., Ong, C. W . & Chen, C.-L. Exploring the use of large language models (llms) in chemical engineering education: \nBuilding core course problem models with chat-gpt. Educ. for Chem. Eng. 44, 71–95 (2023).\n 4. Colombo, E., Mercorio, F ., Mezzanzanica, M. & Serino, A. Towards the terminator economy: Assessing job exposure to ai through \nllms. Preprint at arxiv:https://arxiv.org/abs/2407.19204 (2024).\n 5. Y ang, J. et al.  Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent. In 2024 IEEE \nInternational Conference on Robotics and Automation, 7694–7701 (IEEE, 2024).\n 6. Saha, D. et al. Llm for soc security: A paradigm shift. IEEE Access 4, 1–27 (2024).\n 7. Goli, A. & Singh, A. Frontiers: Can large language models capture human preferences?. Mark. Sci. 43, 709–722 (2024).\n 8. Zhao, Z. et al. Recommender systems in the era of large language models (llms). IEEE Trans. Knowl. Data Eng. 36, 6889–6907 \n(2024).\n 9. Acemoglu, D. Harms of ai. Working Paper 29247, National Bureau of Economic Research (2021).\n 10. Gilson, A. et al. How does chatgpt perform on the united states medical licensing examination (usmle)? the implications of large \nlanguage models for medical education and knowledge assessment. JMIR Med. Educ. 9, e45312 (2023).\n 11. Ajevski, M., Barker, K., Gilbert, A., Hardie, L. & Ryan, F . Chatgpt and the future of legal education and practice. Law. Teach. 57, \n352–364 (2023).\n 12. Kasneci, E. et al. Chatgpt for good? on opportunities and challenges of large language models for education. Learn. Individ. Differ. \n103, 102274 (2023).\n 13. Singh, V . K., Chayko, M., Inamdar, R. & Floegel, D. Female librarians and male computer programmers? gender bias in occupational \nimages on digital media platforms. J. Assoc. Inf. Sci. Technol. 71, 1281–1294 (2020).\n 14. Blackburn, R. M., Jarman, J. & Siltanen, J. The analysis of occupational gender segregation over time and place: considerations of \nmeasurement and some new evidence. Work Employ. Soc. 7, 335–362 (1993).\n 15. Karpinska, K., Henkens, K. & Schippers, J. Hiring retirees: impact of age norms and stereotypes. J. Manag. Psychol. 28, 886–906 \n(2013).\nScientific Reports |        (2025) 15:18777 13| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\n 16. Jakesch, M., Bhat, A., Buschek, D., Zalmanson, L. & Naaman, M. Co-writing with opinionated language models affects users’ views. \nIn Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI ’23, 1–15 (Association for Computing \nMachinery, 2023).\n 17. Kotek, H., Dockum, R. & Sun, D. Gender bias and stereotypes in large language models. In Proceedings of the ACM Collective \nIntelligence Conference, CI ’23, 12–24 (Association for Computing Machinery, 2023).\n 18. Ghosh, S. & Caliskan, A. Chatgpt perpetuates gender bias in machine translation and ignores non-gendered pronouns: Findings \nacross bengali and five other low-resource languages. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, \nAIES ’23, 901–912 (Association for Computing Machinery, 2023).\n 19. Nishikawa, K. & Koshiba, H. Exploring the applicability of large language models to citation context analysis. Scientometrics 129, \n6751–6777 (2024).\n 20. Ahn, J. & Oh, A. Mitigating language-dependent ethnic bias in bert. Preprint at arxiv:https://arxiv.org/abs/2109.05704 (2021).\n 21. Smith, E. M., Hall, M., Kambadur, M., Presani, E. & Williams, A. “i’m sorry to hear that”: Finding new biases in language models \nwith a holistic descriptor dataset. Preprint at arxiv:https://arxiv.org/abs/2205.09209 (2022).\n 22. Armstrong, L., Liu, A., MacNeil, S. & Metaxa, D. The silicon ceiling: Auditing gpt’s race and gender biases in hiring. In Proceedings \nof the 4th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, EAAMO ’24, 1-18 (ACM, 2024).\n 23. Newstead, T., Eager, B. & Wilson, S. How ai can perpetuate-or help mitigate-gender bias in leadership. Organ. Dyn. 52, 100998 \n(2023).\n 24. Venkit, P .  N., Gautam, S., Panchanadikar, R., Huang, T.-H.  K. & Wilson, S. Nationality bias in text generation. Preprint at \narxiv:https://arxiv.org/abs/2302.02463 (2023).\n 25. Zhang, J. et al. Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation. In Proceedings of \nthe 17th ACM Conference on Recommender Systems, RecSys ’23, 993–999 (Association for Computing Machinery, 2023).\n 26. Huang, P .-S. et al. Reducing sentiment bias in language models via counterfactual evaluation. Preprint at  a r x i v : h t t p s : / / a r x i v . o r g / a b \ns / 1 9 1 1 . 0 3 0 6 4     (2020).\n 27. Borchers, C. et al. Looking for a handsome carpenter! debiasing gpt-3 job advertisements. Preprint at  a r x i v : h t t p s : / / a r x i v . o r g / a b s / 2 \n2 0 5 . 1 1 3 7 4     (2022).\n 28. Lu, K., Mardziel, P ., Wu, F ., Amancharla, P . & Datta, A. Gender Bias in Neural Natural Language Processing, 189–202  (Springer \nInternational Publishing, Cham, 2020).\n 29. Venkit, P .  N., Gautam, S., Panchanadikar, R., Huang, T.-H.  K. & Wilson, S. Nationality bias in text generation. Preprint at \narxiv:https://arxiv.org/abs/2302.02463 (2023).\n 30. Park, S., Choi, K., Yu, H. & Ko, Y . Never too late to learn: Regularizing gender bias in coreference resolution. In Proceedings of \nthe Sixteenth ACM International Conference on Web Search and Data Mining , WSDM ’23, 15-23 (Association for Computing \nMachinery, New Y ork, NY , USA, 2023).\n 31. Li, Y ., Du, M., Wang, X. & Wang, Y . Prompt tuning pushes farther, contrastive learning pulls closer: A two-stage approach to \nmitigate social biases. Preprint at arxiv:https://arxiv.org/abs/2307.01595 (2023).\n 32. Gaci, Y ., Benattallah, B., Casati, F . & Benabdeslem, K. Debiasing Pretrained Text Encoders by Paying Attention to Paying Attention. \nIn 2022 Conference on Empirical Methods in Natural Language Processing , Proceedings of the 2022 Conference on Empirical \nMethods in Natural Language Processing, 9582–9602 (Association for Computational Linguistics, Abu Dhabi, United Arab \nEmirates, 2022).\n 33. Tokpo, E. K. & Calders, T. Text style transfer for bias mitigation using masked language modeling. Preprint at  a r x i v : h t t p s : / / a r x i v . o \nr g / a b s / 2 2 0 1 . 0 8 6 4 3     (2022).\n 34. Ribeiro, M. T., Singh, S. & Guestrin, C. “why should i trust you?”: Explaining the predictions of any classifier. In Proceedings of \nthe 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, 1135-1144 (Association for \nComputing Machinery, New Y ork, NY , USA, 2016).\n 35. Wang, X. et al. Pay attention to your tone: Introducing a new dataset for polite language rewrite. Preprint at  a r x i v : h t t p s : / / a r x i v . o r g \n/ a b s / 2 2 1 2 . 1 0 1 9 0     (2022).\n 36. Liu, S. et al. Online active learning for drifting data streams. IEEE Trans. Neural. Netw. Learn. Syst. 34, 186–200 (2023).\n 37. Wang, Y ., Cao, J., Bu, Z., Wu, J. & Wang, Y . Dual structural consistency preserving community detection on social networks. IEEE \nTrans. Knowl. Data Eng. 35, 11301–11315 (2023).\n 38. D Petreski, I. H. Word embeddings are biased. but whose bias are they reflecting?. AI Soc. 38, 975–982 (2023).\n 39. Yuan, S. et al. z2-wudaocorpora: A super large-scale chinese corpora for pre-training language models. AI Open 2, 65–68 (2021).\n 40. Deng, Yichen. The alienation of women’s discourse on chinese social media. SHS Web Conf. 162, 01033.  h t t p s : / / d o i . o r g / 1 0 . 1 0 5 1 / s \nh s c o n f / 2 0 2 3 1 6 2 0 1 0 3 3     (2023).\n 41. Birhane, A. et al. The values encoded in machine learning research. In Proceedings of the 2022 ACM Conference on Fairness, \nAccountability, and Transparency, FAccT ’22, 173-184 (Association for Computing Machinery, 2022).\n 42. Shin, J., Song, H., Lee, H., Jeong, S. & Park, J. C. Ask llms directly, “what shapes your bias?”: Measuring social bias in large language \nmodels. Preprint at arxiv:https://arxiv.org/abs/2406.04064 (2024).\n 43. Zhang, G. et al. Corgi-pm: A chinese corpus for gender bias probing and mitigation. Preprint at  a r x i v : h t t p s : / / a r x i v . o r g / a b s / 2 3 0 1 . 0 \n0 3 9 5     (2023).\n 44. Zhao, J., Ding, Y ., Jia, C., Wang, Y . & Qian, Z. Gender bias in large language models across multiple languages. Preprint at \narxiv:https://arxiv.org/abs/2403.00277 (2024).\n 45. Sun, Y . et al. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. Preprint at \narxiv:https://arxiv.org/abs/2107.02137 (2021).\n 46. Itzhak, I., Stanovsky, G., Rosenfeld, N. & Belinkov, Y . Instructed to bias: Instruction-tuned language models exhibit emergent \ncognitive bias. Transactions of the Association for Computational Linguistics 12, 771–785 (2024).\n 47. Yuemei, X., Yuqi, Y . & Xueyi, H. Challenges of bias in large language models: Identification, evaluation, and debiasing. Comput. \nAppl. 1–14 (2024).\n 48. Huang, Y . et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural \nInformation Processing Systems, vol. 36, 62991–63010 (Curran Associates, Inc., 2023).\n 49. Abhishek, A., Erickson, L. & Bandopadhyay, T. Beats: Bias evaluation and assessment test suite for large language models. Preprint \nat arxiv:https://arxiv.org/abs/2503.24310 (2025).\n 50. Koo, R. et al. Benchmarking cognitive biases in large language models as evaluators. Preprint at  a r x i v : h t t p s : / / a r x i v . o r g / a b s / 2 3 0 9 . 1 \n7 0 1 2     (2024).\n 51. Geva, M., Goldberg, Y . & Berant, J. Are we modeling the task or the annotator? an investigation of annotator bias in natural \nlanguage understanding datasets. Preprint at arxiv:https://arxiv.org/abs/1908.07898 (2019).\n 52. Zhu, S., Wang, W . & Liu, Y . Quite good, but not enough: Nationality bias in large language models – a case study of chatgpt. \nPreprint at arxiv:https://arxiv.org/abs/2405.06996 (2024).\n 53. Fiske, S. Thinking is for doing: Portraits of social cognition from daguerreotype to laserphoto. J. Pers. Soc. Psychol.  63, 877–89 \n(1992).\n 54. Chaturvedi, S. & Chaturvedi, R. Who gets the callback? generative ai and gender bias. Preprint at  a r x i v : h t t p s : / / a r x i v . o r g / a b s / 2 5 0 4 \n. 2 1 4 0 0     (2025).\n 55. Abid, A., Farooqi, M. & Zou, J. Persistent anti-muslim bias in large language models. In Proceedings of the 2021 AAAI/ACM \nConference on AI, Ethics, and Society, AIES ’21, 298-306 (2021).\nScientific Reports |        (2025) 15:18777 14| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/\n 56. Liao, Q. V ., Gruen, D. & Miller, S. Questioning the ai: Informing design practices for explainable ai user experiences. In Proceedings \nof the 2020 CHI Conference on Human Factors in Computing Systems, CHI ’20, 1-15 (2020).\n 57. Lei, X., Y ahao, H. & Zhesong, P . A review of bias research in large language models. Comput. Appl. Res. 41, 2881–2892 (2024).\nAuthor contributions\nLeilei Jiang gathered the data and wrote the main manuscript text; Guixiang Zhu and Jianshan Sun contributed \nto implementation and guided in technical writing of manuscript; Jie Cao was the supervisor guiding the con -\nceptualization and overall direction of the research; Jia Wu contributed to the interpretation of data and critically \nrevised the manuscript. All authors read and approved the final manuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to J.C.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:18777 15| https://doi.org/10.1038/s41598-025-03893-w\nwww.nature.com/scientificreports/"
}