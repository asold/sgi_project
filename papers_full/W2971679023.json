{
  "title": "An Economic Operation Analysis Method of Transformer Based on Clustering",
  "url": "https://openalex.org/W2971679023",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2162937802",
      "name": "Jun-de chen",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2126159105",
      "name": "Defu Zhang",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2182451034",
      "name": "Yaser Ahangari Nanehkaran",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2162937802",
      "name": "Jun-de chen",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2126159105",
      "name": "Defu Zhang",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2182451034",
      "name": "Yaser Ahangari Nanehkaran",
      "affiliations": [
        "Xiamen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2129066856",
    "https://openalex.org/W6631560520",
    "https://openalex.org/W2924549079",
    "https://openalex.org/W2072426588",
    "https://openalex.org/W2941197459",
    "https://openalex.org/W2801279680",
    "https://openalex.org/W6668990524",
    "https://openalex.org/W2884562637",
    "https://openalex.org/W1997527287",
    "https://openalex.org/W1916665293",
    "https://openalex.org/W2940320374",
    "https://openalex.org/W1980878435",
    "https://openalex.org/W2031757379",
    "https://openalex.org/W2141311859",
    "https://openalex.org/W2017247719",
    "https://openalex.org/W4240726130",
    "https://openalex.org/W2021870408",
    "https://openalex.org/W2891813652",
    "https://openalex.org/W2790072082",
    "https://openalex.org/W2963649946",
    "https://openalex.org/W2913950581",
    "https://openalex.org/W2101626336",
    "https://openalex.org/W2165651146",
    "https://openalex.org/W2193209126",
    "https://openalex.org/W2085439998",
    "https://openalex.org/W2143885411",
    "https://openalex.org/W1566474244",
    "https://openalex.org/W2168457746",
    "https://openalex.org/W2078669100",
    "https://openalex.org/W3009784374",
    "https://openalex.org/W1526620847",
    "https://openalex.org/W2073459066",
    "https://openalex.org/W2131850647",
    "https://openalex.org/W2140190241"
  ],
  "abstract": "The economic operation of power transformers is analyzed in the present paper, which is performed by the clustering analysis method. In order to overcome the disadvantages of the conventional k-means algorithm lacking the stability and accuracy, we propose a novel boost k-means algorithm by optimizing the choice of initial cluster centers, and no additional parameters are required. The proposed approach outperforms the conventional approach in most experiments, for the best one, the accuracy of the proposed approach is 20.37% higher than that of the traditional approach. More importantly, empirical research is conducted in the paper. The index system reflecting the load characteristics of power transformers is established, and using the boost k-means algorithm, the economic operation analysis of power transformers is conducted. The clustering results of different transformers are obtained and the relevant suggestions are given as well. The empirical analysis results prove the validity of the proposed approach, and it can be efficiently applied for the economic operation analysis of transformers.",
  "full_text": "Received August 9, 2019, accepted August 30, 2019, date of publication September 4, 2019, date of current version September 20, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2939481\nAn Economic Operation Analysis Method of\nTransformer Based on Clustering\nJUNDE CHEN, DEFU ZHANG\n , AND YASER AHANGARI NANEHKARAN\nSchool of Informatics, Xiamen University, Xiamen 361005, China\nCorresponding author: Defu Zhang (dfzhang@xmu.edu.cn)\nThis work was supported in part by the National Natural Science Foundation of China under Project 61672439, and in part by the\nFundamental Research Funds for the Central Universities under Grant 20720181004.\nABSTRACT The economic operation of power transformers is analyzed in the present paper, which is\nperformed by the clustering analysis method. In order to overcome the disadvantages of the conventional\nk-means algorithm lacking the stability and accuracy, we propose a novel boost k-means algorithm by\noptimizing the choice of initial cluster centers, and no additional parameters are required. The proposed\napproach outperforms the conventional approach in most experiments, for the best one, the accuracy of\nthe proposed approach is 20.37% higher than that of the traditional approach. More importantly, empirical\nresearch is conducted in the paper. The index system reﬂecting the load characteristics of power transformers\nis established, and using the boost k-means algorithm, the economic operation analysis of power transformers\nis conducted. The clustering results of different transformers are obtained and the relevant suggestions are\ngiven as well. The empirical analysis results prove the validity of the proposed approach, and it can be\nefﬁciently applied for the economic operation analysis of transformers.\nINDEX TERMS Index system, boost k-means, transformer, economic operation.\nI. INTRODUCTION\nThe power transformer is one of the most critical equipment\nin the power system, and the energy loss of the transformer\noccupies a large proportion in the distribution network.\nIn general, the energy loss of the transformer takes possession\nof about 50% in the entire energy loss of area power system\nand about 10% of the total installed capacity [1]. Obviously,\nthe economic operation of transformers plays an important\nrole in the energy conservation of power system [2], [3].\nBased on the actual load and combined with the parameter\ninformation of transformers, the traditional method mainly\nevaluates the economic operation of the transformer by the\nsize of the load rate. This kind of method may depict the\ncurrent operating state of the transformers, but it is difﬁcult\nto reﬂect the historical situation and the future trend of the\ntransformer load. Therefore, it is necessary to conduct a\ncomprehensive load analysis for the power transformers, seek\nmore scientiﬁc characteristic indicators which can reﬂect the\nload situation, and establish a data analysis model for the\neconomic operation of the power transformers.\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Md. Abdur Razzaque.\nAt present, many data mining techniques have presented\npromising performance to analyze the power transformer\noperation data. In the literature [4], [5], the fuzzy set methods\nare applied to monitor the transformer condition based on\nIEC/IEEE standards. The fuzzy method improves the identiﬁ-\ncation accuracy of transformers and gives remarkable effects,\nwhereas it needs to determine the input/output membership\nfunctions, diagnostic rules, and defuzziﬁcation. Additionally,\nthe classiﬁcation algorithms, particularly various artiﬁcial\nneural networks (ANNs), have been widely used in this\nstudy [6]–[9], because of their learning capability, parallel\ndistributed process, recognition performance, and nonlin-\near classiﬁcation ability. The limitations of ANNs are the\nrequired training process, determining a proper design, and\nnetwork parameters assignment [10]. Clustering analysis is\nan important branch of data mining, and clustering algorithms\nplay a critical role in data analysis, data mining and engi-\nneering signal processing, etc. Among them, k-means is one\nof the most popular in reality because of its simplicity and\neffectiveness [11], it is a partition-based and fast convergence\nspeed clustering algorithm that can effectively handle large\ndata [12], [13]. However, due to different settings of the\nparameters and random selection of initial cluster centers,\n127956 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/VOLUME 7, 2019\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\nthe conventional k-means algorithm is not stable, and it may\nproduce different clustering partitions for the same dataset.\nTherefore, it is necessary to improve the performance of the\nalgorithm before the economic operation analysis of power\ntransformers.\nThis paper makes two main contributions. First,\nwe improve the conventional algorithm and propose a boost\nk-means to optimize the choice of initial cluster centers.\nThrough calculating the distance between different points\nand sets, the nearest points are classiﬁed as a set and the\nunclassiﬁed point is moved to the sets using the fast-moving\napproach, then the average value can be got as the initial cen-\nter. The procedure is repeated until all the initial cluster cen-\nters are found. Moreover, the index system for the economic\noperation analysis of power transformers is established, and\nbased on the load intensity and time change information, the\ncharacteristic indicators including ALR, LRF, LRG, etc. are\nextracted, which reﬂect the situation of load intensity, load\ndispersion, and load change trend separately. Thus, according\nto the characteristic indicators, the various transformers are\nclustered using the proposed boost k-means method, and the\nﬁnal results are obtained to determine whether the transform-\ners are economically operated.\nII. PROBLEM DESCRIPTION\nGenerally, there are two major problems for the classical\nk-means algorithms, one is the selection of initial cluster cen-\nters, which is selected at random [14], the other is the determi-\nnation of cluster number. The basic idea of this algorithm is to\ngive a database D, the k value of clusters is input by the user,\nat the beginning, the D is divided into k parts randomly, then\nthe division is adjusted by updating the center of the clusters,\nwhen the global difference function converges, the process\nis ended. The sum function of squared errors is used as the\nobjective function of k-means algorithms, as expressed in\nEq. (1).\nJc =\nk∑\ni=1\n∑\np∈Ci\n∥p −Mi∥2 (1)\nwhere Mi is the mean value of the data in class Ci, p is the\npoint in class Ci.\nBy minimizing the objective function with multiple iter-\nations, the cluster centers of the algorithm are constantly\nupdated, that is, the algorithm is to ﬁnd the clustering center\nvector V = (v1, v2,..., vk )T of categories to minimize\nthe objective function Jc. The search direction of the objective\nfunction is always along the direction of decreased error\nsquare, and different initial values make the vector V proceed\nin different paths, in this case, the ﬁnal clustering result of the\nobject depends largely on the initial partition or the choice of\nseed points. Therefore, the conventional k-means algorithm\nis sensitive to the initial clustering center in theory based on\nthe above analysis.\nThere are also many research efforts focusing on the selec-\ntion of initial cluster centers. For example, the k-means ++\nalgorithm [15] is proposed to address the challenge, in this\nalgorithm, only the ﬁrst cluster center is randomly selected\nwhile the remainder initial cluster centers are selected as far\nas possible from the ﬁrst point. However, random selection is\nstill commonly used in practice [16]. Erisolgu et al. [17] pro-\nposed an incremental approach for computing initial cluster\ncenters. In this approach, the reduced dataset is partitioned\none by one until the number of clusters equals the prede-\nﬁned number of clusters. But the number of clusters must\nbe known in advance, and how to get it is not given, which\nfalls into the egg-chicken loop again. The others include\nusing an optimization algorithm to ﬁne-tune the initial cluster\ncenters, such as in [18], the genetic algorithm (GA) is used\nfor the selection. However, the parameters of optimization\nalgorithms are much, which even exceed that of the k-means\nalgorithm itself. Actually, it is considered that the algorithm\nshould be free of parameters [19].\nAdditionally, the determination of the cluster number is\nthe other problem for the analysis. The number of clus-\nters in the data to be analyzed must be known in advance\nbecause many clustering algorithms require the number of\nclusters as an input parameter to run the algorithms [19].\nHowever, the number of clusters that exist in real data is\nusually unknown. Therefore, a number is often guessed in\npractical cluster analysis, which often results in unsatisfac-\ntory results. Although several methods for estimating the\nnumber of clusters in data have been developed [20]–[24],\nthey either produce incorrect results or are difﬁcult to use\nin real applications. Thus, ﬁnding the correct number of\nclusters from real data remains a traditional problem in cluster\nanalysis. It is also an active research topic.\nIII. PROPOSED APPROACH\nThe basic purpose of clustering is to divide the data into a\nset of clusters in which the objects in the same clusters are\nclose to each other, whereas the objects in different clusters\nare far from each other. Just as mentioned in Section II,\nthe conventional k-means algorithm is sensitive to the initial\ncluster center and which makes the algorithm results unstable.\nSo, by optimizing the choice of initial cluster center, the paper\nproposes a boost k-means algorithm to improve the conven-\ntional algorithm. This approach ﬁrst ﬁnds the nearest points\nand separates them from the original dataset to form a new set,\nthe other points in original dataset are moved to the formed\nset until the number of which reaches the maximum. Then,\nthe processes are repeated until all the points are grouped to\nthe sets, and the average values of all the sets are computed\nas the initial cluster centers of cluster algorithm. The main\nprocesses are listed as follows.\n1. Set the total sample set as U. Calculate the distance\nbetween the sample pairs and sort them by distance, as com-\nputed in Eq. (2).\nd(x,y)=\n√\n(x1 −y1)2 +(x2 −y2)2 +...+(xn −yn)2 (2)\nwhere d(x, y) is the distance, x and y are any two points.\nVOLUME 7, 2019 127957\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\n2. The closest points are found, which forms the sample set\nAm(1 ≤m ≤k), then they are removed from the total sample\nset.\n3. Calculate the distance from the sample points in set U\nto set Am, and then the objective function is deﬁned\nMin.Lr =Min.\n∑\ndi∈Cr\ndi (3)\nUnder the condition of Lr reducing, the close points are\nmerged into the set Am and removed from the set U. Particu-\nlarly, the distance from a sample point i to a set C is deﬁned\nas the following Eq. (4).\nd(i,C) =min(d(i,j),j ∈C) (4)\n4. Repeat step 3 until the number of samples in Am reach a\ncertain threshold ε, which can be computed using Eq. (5).\nε=N/K (5)\nwhere N is the total number of samples, k is the class number\n5. If m is less than k, then m is equal to m +1. Compute\nthe closest points from the updated set U, which forms a new\nset and these points are deleted from the set U. Return to step\n3 and implement.\n6. The arithmetic mean values of the ﬁnal k sets is com-\nputed using Eq.(6), which can be used as the initial cluster\ncenters and input to the k-means for the clustering.\nci =\n∑\nAm/|Am| (6)\nGenerally, the details of boost k-means are presented in\nAlgorithm 1.\nMoreover, the convergence of the algorithm can be further\nanalyzed. As stated before, giving the loss Lr =∑\ndi∈Cr di,\nwe have the objective function\nMin.\n∑\nxi∈U ∥Cr −xi∥ (7)\naccordingly, Eq. (7) can be transformed as\nMin.L =1\n2\n∑K\nr=1\n∑N\ni=1 (Cr −xi)2 (8)\nThen, pick xi in random ( xi ∈ U) and check 1L when\nmoving xi from set Su to Sv. When the value of L is declined,\nthis algorithm process is continuous, otherwise, it ends,\nas expressed in Eq. (9).\nγ(xn) =\n{\n1, if arg minr ∥xn −Cr ∥2\n0, otherwise (9)\nIn reality, the value of 1L cannot decrease all along, so the\nalgorithm is convergent. Additionally, the calculation and\nupdating of the initial cluster center take the form :\nci =\n∑\nn γnr xn/\n∑\nn γnr , (10)\nwhich is also the optimized solution of the current objective\nfunction. Basically, this is a coordinate descent process, for\nevery iteration, the value of the objective function will be\ndecreased until it reaches a minimum. Thus, the algorithm\nprocess is convergent, as depicted in Figure 1.\nAlgorithm 1:Boost k-Means\nInput:\nData size: N .\nNo. threshold of each set: ε\nBegin\nCalculate the pairs and the closest pairs is formed\nthe set Am.\nAm is removed from total set U. Initialize t =0.\nRepeat {\nt ←t+1\nFor eachxj ∈U do\nj∗ ←argmini {||← xjCi ←2 ||} //Assign xj\nto the closest cluster.\nAm =Am∪{xj}, |Am|≤ ε//end for each.\nFor eachi =1 to k do\nci ← 1\n|Ci|\n∑\nxj∈Ci xj //Get the initial centers\nas the input of km\nUntil ∑k\ni=1 |Ci|≤ N //Until all the points are\nclustered.\n}\nFunction KM(k, ci) //conduct k-means algorithm, ci\nis the\nk-means ←ci do k-means // calculated centers,\nk =N/ ε.\nOutput:\nThe ﬁnal clustering result.\nEnd.\nFIGURE 1. The fast and best convergence approach.\nThere will be two approaches for the point moving,\nthe ‘‘fast’’ moving and ‘‘best’’ moving. If the value of the\nobjective function is decreased ( 1L < 0) when moving the\npoint xi from one set to another, it is deﬁned as the ‘‘fast’’\nmoving (Only the partial points outside of objective sets need\nto be compared.). On the other hand, for the ‘‘best’’ moving\napproach, it requires a negative maximum when moving the\npoint xi from one set to another ( 1L <0 & 1L =min(1L)).\nIn this case, all the points outside of the objective set need\nto be compared and the closest points are merged into the\nobjective set.\nFor the approach of ‘‘fast’’ moving, once the arithmetic\nmean values of each set is calculated, we can use it as the\ninitial cluster center of classical k-means and input to run the\n127958 VOLUME 7, 2019\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\nalgorithm further, the obtained results are the ﬁnal results. For\nthe approach of ‘‘best’’ moving, the partitioned categories can\nbe regarded as the approximate clustering result and directly\nused as well. At present, the approach of ‘‘fast’’ move is\nemployed in our practice. A brief example is illustrated as\nfollows. Table 1 lists a randomly generated two-dimensional\ndataset, and Figure 2 shows its distribution.\nTABLE 1. Randomly generated two-dimensional dataset.\nFIGURE 2. The data distribution of table 1.\nSuppose that the data is divided into k = 3 classes,\nthe possible number of isolated points m is taken as 1, and\nthe threshold of sample number is set to 4. Firstly, we can\nﬁnd that point 2 (61,69) and point 4 (58,76) are the closest\npoints to each other. So, the point 2 (61,69) and point 4\n(58,76) are selected to form the ﬁrst sample set A1, which\nis deleted from the total set U. Since the maximum number\nof samples in each sample set is set to 4, the search for the\nclosest point from U to A1 will be continued, it is easy to\nknow that the point 6(65,44) is the closest, so that the point\n6(65,44) is added to set A1 and removed from the set U. After\nremoving 2, 4, and 6 points in the set U, the nearest points\nare the point 1(89, 49) and point 9(94, 40), which formed the\nset A2, likewise, points 5(93, 87), 10(80, 96) are also added\nto the set A2. Then, points 3(27,8), 7(7,10) formed the A3,\nin this way, the divided 3 classes are separately obtained and\nthe classiﬁcation of this dataset is performed. What is more,\nthe arithmetic mean of these sample sets can be computed\nrespectively, and the initial cluster centers for all categories\nare obtained as (61, 63), (89, 68), (17, 9), thus, the new initial\ncluster center is generated, which is more consistent with the\nactual distribution of samples and achieves better clustering\nresults.\nIV. EXPERIMENT ANALYSIS\nUCI Machine Learning Repository [25] is an international\ngeneral database for the algorithm test of machine learning,\nand the real-world datasets are downloaded from the UCI\ndatabase. To verify the above points and evaluate the perfor-\nmance of the proposed approach, a lot of experiments were\nconducted on the downloaded UCI datasets. The algorithms\nwere mainly implemented using C language in codeblocks\ntools, whereas the ﬁgures and partial algorithms were con-\nducted in R, Matlab, and SPSS, etc.\nA. VERIFICATION TEST\nThe iris dataset is a feature set of plant ﬂowers, each sample\nconsists of 4 features: sepal length (SL), sepal width (SW),\npetal length (PL), and petal width (PW). In this dataset, there\nare 150 samples classiﬁed as 3 clusters and each cluster has\n50 samples. Thus, we can deﬁne the number of samples as\n150, the attribute dimension is m =4, and the number of\nclusters is k =3. Accordingly, the different initial cluster\ncenters are employed to cluster the samples, in the experi-\nments, the comparison of the ﬁnal results presents that 91 of\nthe 150 samples are with the different clustering category,\nso the difference rate is reached to 60.67%, which shows that\nthe different initial cluster centers have a big impact on the\nﬁnal results. Table 2 displays the partial results.\nTABLE 2. Result comparison of different initial cluster centers.\nThe initial cluster centers used in these two groups are\npresented as follows.\n1#: (5.1, 3.5, 1.4, 0.2), (4.7, 3.2, 1.3, 0.2), (4.9, 3.0, 1.4,\n0.2)\n2#: (7.7, 3.0, 6.1, 2.3), (4.4, 3.2, 1.3, 0.2), (5.7, 4.4, 1.5,\n0.4)\nThen, the good result of the two groups is selected to\ncompare with the actual categories, there are 16 cluster errors\nin the 150 sample data, and the error rate is 10.67%. In the\nsame way, the wine dataset is tested as well. This dataset is\na chemical composition analysis for the wine brewed from\nthree different cultivated plants in the same region of Italy.\nIt contains 178 sample records, 13 chemical species and\n3 different types of raw material cultivation, so the attribute\ndimension is set m =13 and the number of clusters is set\nk =3. For the results of two initial cluster centers, there are\n89 different samples, the difference rate is 50%. Likewise,\nthe good result of the two groups is selected to compare\nVOLUME 7, 2019 127959\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\nTABLE 3. The tested UCI dataset and the experimental results.\nwith the actual categories, there are 76 cluster errors in the\n178 samples, and the error rate is 42.69%.\nFrom the above veriﬁcation experiments, we can see that\nthe initial cluster centers have a great impact on the ﬁnal\noutputs of the algorithms, and the results obtained by various\ninitial cluster centers are quite different. The initial cluster\ncenters are selected at random, which makes the results of\nthe conventional k-means algorithm unstable and affects the\naccuracy of the algorithm.\nB. COMPARISON EXPERIMENT\nTo investigate the performance of the proposed algorithms by\nexperiments, seven datasets including Balance, Wine, Musk,\nPegdigits, Skin segmentation (seg.), etc. are downloaded\nfrom UCI database and used in the experiments. They are\nfrequently used as the benchmark datasets in probing the\nperformance of different algorithms.\nThe Balance dataset is generated to model psychological\nexperimental results. Each example is classiﬁed as having\n3 categories: the balance scale tip to the right, tip to the\nleft, or be balanced. There are 4 attributes and 625 samples\n(49 balanced, 288 left, 288 right) in this dataset.\nThe Diabetes dataset is obtained from two sources: an auto-\nmatic electronic recording device and paper records. There\nare 768 samples divided into 2 clusters in this dataset, and\neach sample is determined by 8 features.\nThe Musk dataset consists of 102 molecules (39 musks and\n63 non-musks). Musk is a molecule that binds to a target\nprotein. The 102 molecules consist of 6,598 data each of\nwhich is represented by a 166-dimensional feature vector\nderived from their surface properties.\nThere are 10,992 samples collected in the Pendigits\ndataset, which represents handwriting digits written by\n44 writers. The 10,992 handwriting digits are categorized into\n10 clusters with respect to digits between 0 and 9 and each\ninstance is described by 16 features.\nThe Skin seg. dataset is collected by randomly sampling\nB, G, R values from face images of various age, gender, and\nrace groups. The dataset is composed of 245,057 samples\nclassiﬁed into 2 clusters (50,859 skin samples and 194,198\nnon-skin samples), and each sample contains 3 features.\nTable 3 displays the information about the seven datasets.\nAccording to the statistic variables of correct detections\n(also known as true positives), misdetections (also known as\nfalse negatives) and false positives, the accuracy is the most\ncommonly used indicator to evaluate the performance of a\ndata clustering system. It is deﬁned as follows.\nAccuracy = TP +TN\nTP +TN +FP +FN (11)\nwhere TP(true positive) is the number of instances that actu-\nally belong to cluster C and are correctly classiﬁed by the\nclustering algorithm. FP (false positive) is the number of\ninstances that do not belong to cluster C but mistakenly\nclassiﬁed as this cluster. TN (true negative) is the number\nof instances which are not in cluster C in reality, and they\nare correctly classiﬁed. FN (false negative) is the number of\ndata which are in cluster C in reality, but they are incorrectly\nclassiﬁed as others.\nThus, for the proposed method, the comparative exper-\niments are performed on the above testing datasets. The\n10,992 handwriting digits of Pendigits dataset, which are\naccurately clustered by the k-means with 6418 instances,\nthe accuracy rate is 58.39%. In contrast, the correct clustering\nnumber of boost k-means on Pendigits dataset is 7972, which\nis more than that of the conventional k-means. The accuracy\nof boost k-means is 72.53%.\nSimilarly, considering the 6,598 samples of Musk dataset,\n3,562 samples are classiﬁed correctly by the conventional\nk-means algorithm, the correct rate is 53.99%, and the\n4,906 samples are classiﬁed correctly by the boost k-means\n(KM) algorithm, the correct rate is 74.36%. Figure 3 presents\nthe results of confusion matrices on different datasets.\nThe correct classifying number of the Wine dataset is\n102 for the conventional k-means algorithms, the accuracy\nrate is 57.30%. By contrast, for the boost k-means, the cor-\nrect number of the dataset is 125 and accuracy is 70.22%.,\nthe calculated initial cluster center are:\n(13.55, 2.05, 2.43, 17.89, 106.44, 2.64, 2.60, 0.32, 0.32,\n5.38, 1.02, 2.98, 1033.53)\n(12.67, 2.53, 2.37, 21.08, 95.37, 1.94, 1.50, 0.41, 0.41,\n5.02, 0.87, 2.29, 558.78)\n(12.88, 2.49, 2.41, 20.08, 103.15, 2.14, 1.66, 0.37, 0.37,\n5.28, 0.90, 2.44, 707.02)\nIn the same way, the experiments are conducted on the\nother UCI datasets and all the results are displayed in Table 3.\n127960 VOLUME 7, 2019\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\nFIGURE 3. The results of confusion matrices on different datasets.\nAs seen from the above Table 3, the accuracy results on the\nseven datasets for the traditional k-means algorithm and boost\nk-means algorithm are given separately. Thus, the method\nof statistic test can be used to determine whether there is a\nsigniﬁcant difference between the two algorithms. Wilcoxon\nsigned-ranks test [26], [27], which is one of the most widely\nused statistical tests in behavioral studies, is a nonparamet-\nric statistical test. It makes weaker assumptions about the\ndistribution of data than other groups of statistical tests (for\nexample, tests based on the normal distribution include t-test,\nANOV A and linear regression, etc.) [28]. Because of these\nfeatures, the Wilcoxon test has been widely used in many\nﬁelds, especially in algorithm comparison analysis. The brief\ndescription of the test process is illustrated as follows.\nLet di be the difference of clustering performance between\nthe two algorithms on the i-th dataset, and arrange the abso-\nlute values of their difference from small to large (Take the\naverage value if the rank is the same.), then the rank of each\ndataset is calculated. Let R+represent that the sum of rank\nfor the ﬁrst algorithm is better than that of the second one,\nand R- is on the contrary, as expressed in Eq. (12).\nR+=\n∑\ndi>0\nrank(di) +1\n2\n∑\ndi=0\nrank(di)\nR−=\n∑\ndi<0\nrank(di) +1\n2\n∑\ndi=0\nrank(di) (12)\nThe above seven datasets are calculated below.\nR+=2 +5 +3 +7 +6 +4 =27\nR−=1 (13)\nLet T =min(R+,R−), it is easy to know the T value is 1.\nThen, the critical value table of the Wilcoxon test is checked,\nand under the condition of a =0.05, the difference between\nVOLUME 7, 2019 127961\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\nTABLE 4. The calculation time of initial cluster centers (s).\nFIGURE 4. The clustering chart of algorithms.\nthe algorithms is signiﬁcant when T is less or equal to\n2 (T ≤ 2). Here, the T value satisﬁes the condition and\nit is less than the critical value. Therefore, the result of the\nstatistical test rejects the null hypothesis, indicating the sig-\nniﬁcant differences between the two algorithms. It means that\nthe boost k-means outperforms the conventional algorithm in\na statistical sense. Besides, there are six datasets for the boost\nk-means is superior to the conventional k-means, in terms of\nquantity, it is more than that of the conventional algorithm.\nOn the other hand, the calculation time of initial cluster\ncenters for the different datasets is displayed in Table 4.\nFurthermore, to compare with the other algorithms,\nthe Analog Complexing (AC) cluster algorithm [29] is\nselected in our experiments. This algorithm assumes each\nsample as a pattern, by computing the similarity between\npatterns, the more similar patterns are grouped into one\nclass, and the less similar patterns are classiﬁed into different\nclasses. The earliest application of AC algorithm in clustering\nanalysis was by Lemke et al. [30] and Ivakhnenko [31], and\nmany good effects were obtained for this algorithm after con-\ntinuous development and improvement. Therefore, with the\nconventional k-means and proposed boost k-means together,\nthe AC algorithm is implemented in the experiments using the\nKnowledgeMiner software [29]. Figure 4 depicts the cluster-\ning effect diagram of each algorithm on the wine dataset. For\nthe conventional k-means and boost k-means, the clustering\nnumber was set k =3 and the maximum iteration number\nwas 10. The AC algorithm was computed with 95% similar-\nity, and a total of 93 categories were clustered. Considering\nthe actual categories, there were total 3 categories for this\ndataset, the categories 1 and 81 with the largest number were\ncorresponded to the actual ﬁrst and third class respectively,\nthe other categories were processed as the second class. The\ndetailed results of this phase are described in subsequent\nsections.\nIt can be seen from the above ﬁgure that the performance\nof boost k-means is most consistent with the actual category\nstatus, and the number of clustered category that matches the\nreal category is 125, the matching rate is 70.22%. The AC\nalgorithm determines the clustering number automatically,\nbut the clustering results are too detailed, the matched number\nof this algorithm is 103, the matching rate is 57.87%. For\nthe conventional k-means algorithm, the matched number\nis 102 and the matching rate is 57.30%. In addition to the\ncomparison with the actual category, the compactness and\nseparation validity function [32] can be used to evaluate the\nclustering results, as deﬁned in Eq. (14).\ns(U,k) =\n1\nn\nn∑\ni=1\nk∑\nj=1\n(uij)2 ⏐⏐xi −cj\n⏐⏐2\nmin\np,q=1,2,...,k\n⏐⏐cp −cq\n⏐⏐2 (14)\nwhere the cp, cq, and cj refer to the cluster centers, xi is any\nsample in the dataset, k is the number of class and U is the\nsample set. Then, the Xie-Beni ( XB) index is used for the\nevaluation of cluster effects, as expressed in Eq. (15).\nXB =max\nk\n{max\n\nS(U,k)}, k =2,3,...,n −1 (15)\nThe S(U,k) is the ratio of the average distance between data\nobjects and their corresponding cluster centers to the mini-\nmum distance of cluster centers, in principle, when the S(U,k)\nis smaller, the clustering quality will be higher. Table 5 dis-\nplays the calculated XB values of the above algorithms.\nTABLE 5. The XB evaluation value.\nFrom the XB values calculated in Table 5, it can be seen\nthat the differences between the algorithms are remarkable.\nThe XB value of AC algorithm is the largest while that of\nboost k-means is the smallest, which indicates that the boost\nk-means outperforms the other algorithms in the experiment.\nTherefore, based on the cluster analysis diagram, the sta-\ntistical matching results with actual categories, and the XB\nevaluation indicator, our method shows a signiﬁcant per-\nformance comparing with the conventional algorithm and\nanother method. Experimental results verify the effectiveness\nand superiority of the proposed method. Thus, it can be ﬁnally\napplied in the empirical analysis.\nV. EMPIRICAL ANALYSIS AND DISCUSSION\nA. BUILDING INDEX SYSTEM\nIn order to evaluate the operation status of power transformers\neffectively, it is necessary to sort out the characteristic indi-\ncators reﬂecting the economic operation of transformers.\n127962 VOLUME 7, 2019\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\nInitially, the raw data are collected. Including the max-\nimum load of transformers, installation capacity, voltage\ngrade, and others, the relevant variables are selected for the\nanalysis, and the data preprocessing works such as check-\ning for abnormal data, imputation of missing data, revision\nof holiday data, etc. are performed. Then, using the index\ncalculation formula to calculate the characteristic indicators,\nthe index system is established. On the basis of this, the model\nis built and the results are analyzed in depth. Overall, accord-\ning to the information contained in the transformer records,\nthe following characteristic attributes are collected.\n1. The load intensity information. This can be obtained\nfrom the daily maximum load and the capacity parameters\nof the transformers.\n2. The time change information. It reﬂects the time-series\nchange of analysis variables, and which is obtained from the\ndata collecting time.\nThereby, considering the load intensity and time change\ninformation, the current load level of the transformer can be\ndivided into three levels: high, medium and low. Relatively,\nthe load trend is classiﬁed as fast, slow and stable separately.\nIn this way, a Nine-square grid map is formed, as depicted in\nFigure 5. Thus, there are 9 categories in total, and the number\nof clusters can be set to 9 accordingly.\nFIGURE 5. The nine-square grid map.\nIn this ﬁgure, the class A shows that the current load\nand load trend are both high, which reﬂects the load of this\ncategory is large and the load trend change is on the rise.\nTherefore, this category needs to be focused and expanded\ncapacity when necessary. Similarly, the situation of class I is\nexactly the opposite, both the current load and load trend are\nlow, so it is recommended to reduce capacity if necessary,\nclass E is the economic operation category currently.\nMoreover, the transformer load rate is the ratio of the actual\nmaximum load to the transformer load volume, and from\nwhich three variables reﬂecting the economic operation rule\nof the transformers are derived. These three variables are\nthe average load rate ( ALR), load rate ﬂuctuation ( LRF) and\nload rate gradient ( LRG) respectively, which are calculated as\nfollows.\n1) THE CALCULATION OF AVERAGE LOAD RATE\nThe average load rate is deﬁned using Eq. (16).\nALR =1\nm\n∑m\nj=1 [MLRi1]j, (16)\nwhere j =1,2,3,..., 365, and m represents the number of\ndays, MLRi1 is the load ratio of the i-th transformer. This\nmethod is simple and the result can be directly calculated\naccording to the transformer load record.\n2) THE CALCULATION OF LOAD RATE FLUCTUATION\nReferring to the formula\nLRS =\n√\n1\nm\n∑m\nj=1\n(\n[MLRi1]j −ALR\n)2, (17)\nthe standard deviation of load rate can be calculated, where\nthe LRS denotes the standard deviation, j =1,2,3..., 365,m\nis the number of days, MLRi1 is the load ratio of the i-th\ntransformer, ALR is average load rate. Thus, based on the\naverage load rate and standard deviation of load rate, the load\nrate ﬂuctuation can be calculated in Eq. (18).\nLRF =LRS/ALR ×100% (18)\nThe load rate ﬂuctuation is interpreted as the degree of load\ndispersion per unit average load, which reﬂects the relative\nsize of load rate dispersion.\n3) THE CALCULATION OF LOAD RATE GRADIENT\nThe load rate gradient is calculated as\nLRG =(ALR2 −ALR1)/ALR1 (19)\nwhere LRG represents the load rate gradient, ALR1 is the\naverage load rate in the ﬁrst half period of the transformers,\nALR2 is the average load rate in the second half period of\nthe transformers. The load rate gradient reﬂects the changing\ntrend of transformer load rate, if the value of load rate gradient\nis 1, it implies that the trend of transformer load rate is getting\nlarger, and this situation is probably due to the increase of\nuser’s power load. If the load rate gradient value is 0, it means\nthat the trend of the transformer load rate is getting smaller,\nand this situation may be due to the decreasing power load of\nusers.\nB. MODEL CALCULATION\nAfter the index system reﬂecting the load characteristics of\nthe transformers is established, it can be further analyzed. For\nexample, if the statistical period is one year, the daily average\nload rate can be obtained as MLRj\ni1 =∑365\nj=1 MLRj\ni1/365, and\nwhen the transformer load rate is MLRj\ni1 ∈[30%,70%], the\noperation state of the transformer is economical. However,\nwhen the transformer load rate is MLRj\ni1 <30%, it means that\nthe transformer is not running economically. Additionally,\nwhen the transformer load rate is MLRj\ni1 > 70%, it implies\nthat the load of the transformer is high. In this case, the trans-\nformer is easy to be damaged, and the factors should be\nVOLUME 7, 2019 127963\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\nfurther investigated to improve the operation economy of the\ntransformer.\nBased on the establishment method of index system men-\ntioned in the previous section, the data of special transform-\ners are extracted and the relevant characteristic indicators\nincluding average load rate, load rate ﬂuctuation and load\nrate gradient are calculated separately. The partial data are\ndisplayed in Table 6.\nTABLE 6. Calculation of the characteristic index and clustering result.\nAfter performing the data preprocessing, the feature sam-\nples are clustered by the boost k-means, and the parameter\nof this algorithm is set as: the distance function is applied as\nthe Euclidean distance, the maximum number of iterations is\n500, the number of clusters is set to 9, the number of seeds is\n10. Thus, the clustering results are obtained as the last column\nof Table 6.\nFurthermore, the category 5 is randomly selected for the\neconomic operation analysis, and the frequency density of the\nthree characteristic variables is depicted in Figure 6. It can\nbe observed from this ﬁgure that the average load rate of\ncategory 5 is mainly concentrated between 30% and 70%,\nwhich reﬂects that the transformer operation is economical\nand ideal. The load rate ﬂuctuation is concentrated at 20% to\n40%, which means the transformer operation is smooth and\nthe power supply is normal. The load rate gradient is basically\nbetween -10% and 30%, and most of which is concentrated at\n0 to 30%, it implies that the load trend is relatively stable, and\nsome of the power supply load is increased, but the increased\nmargin is not big.\nFor category 5, the distribution of the average load rate is\ndepicted in Figure 7.\nFIGURE 6. The chart of frequency density.\nFIGURE 7. The distribution chart of ALR for category 5.\nAs stated before, the ALR (average load rate) range of the\neconomic operation for power transformers is in [30%,70%],\nit is uneconomical when ALR is less than 30%, while over-\nloaded when ALR is higher than 70%. According to this rule,\nmost of the transformer samples in category 5 are within this\nrange, it indicates that most of them are economical to operate\nand in good working condition. However, the transformers\nwith less than 30% ALR still account for a certain proportion,\nwhich is not economical, and maybe the electricity consump-\ntion of users is decreasing. It is recommended to observe\nclosely for a period of time, and if necessary, the capacity\nreduction can be suggested. In addition, as for the ALR of\nindividuals is more than 70%, the overall load is higher, it is\nrecommended to pay attention to these sites, and the capacity\nexpansion is considered when necessary.\nSimilarly, the same analysis is performed on the other\ncategories. After establishing index system and applying the\nboost k-means clustering on the load analysis of transformers,\nthe overall operation characteristics and optimization sug-\ngestions are given, which provides an effective way for the\neconomic operation analysis of power transformers.\nVI. CONCLUSION\nAccurate characteristic parameters are the key factors to\nanalyze the operation situation of transformers, which often\naffect the ﬁnal analysis results. Considering the load intensity\nand time change information, the variables including the\naverage load rate ( ALR), load rate ﬂuctuation ( LRF) and\nload rate gradient ( LRG) are extracted as the characteristic\nindicators of transformers. A characteristic analysis method\nfor the economic operation of transformers is developed, and\nthe speciﬁc calculation processes are given, so the feature\nengineering is established. Meanwhile, to further monitor the\noperation status of transformers efﬁciently, the data mining\ntechniques are introduced into the analysis, in particular, the\nclustering analysis method is discussed in theory and the rele-\nvant literature is reviewed. Then, on the basis of the above, the\nboost k-means is proposed in the paper and the comparative\nexperiments are performed as well. After that, the empirical\nresearch is conducted in our work. The performance of exper-\nimental analysis was remarkably improved by the calculated\n127964 VOLUME 7, 2019\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\ninitial cluster center using our algorithm. The clustering\nresults are stable and the error is small, which overcomes the\nshortcomings of traditional algorithms. Additionally, through\nconsidering the load intensity and time change information,\nthe number of clusters is determined according to the feature\nanalysis results, which shows certain theoretical and practical\nsigniﬁcance. In the future, we will explore the possibility\nof developing a general method to automatically determine\nparameters and make it usable in real applications.\nACKNOWLEDGMENT\nThe authors would like to thank the project chance provided\nby Guangxi Electric Power Research Institute and thank\nMr. Zhang Liang-jun, the chairman of Guangzhou TipDM\nIntelligent Technology Company Ltd., for valuable discus-\nsion and contribution to the successful delivery of the project.\nThey would also like to thank all the editors and anonymous\nreviewers for their constructive advice.\nREFERENCES\n[1] Y . Weipeng and Z. Yao, ‘‘Economic operation of transformers in the area\npower network based on real-time analysis and control,’’ in Proc. China\nInt. Conf. Electr. Distrib. , Guangzhou, China, Dec. 2008, pp. 1–5.\n[2] M. Dong, H. Zheng, Y . Zhang, K. Shi, S. Yao, X. Kou, G. Ding, and\nL. Guo, ‘‘A novel maintenance decision making model of power transform-\ners based on reliability and economy assessment,’’ IEEE Access , vol. 7,\npp. 28778–28790, 2019.\n[3] W. Zhang, T. Li, and X. Yuan, ‘‘Study on a united evaluation algorithm and\nits applications for economic operation of transformer,’’ Energy Procedia,\nvol. 16, pp. 2073–2080, Jan. 2012.\n[4] A. D. Ashkezari, H. Ma, T. K. Saha, and C. Ekanayake, ‘‘Application\nof fuzzy support vector machine for determining the health index of the\ninsulation system of in-service power transformers,’’ IEEE Trans. Dielectr.\nElectr. Insul., vol. 20, no. 3, pp. 965–973, Jun. 2013.\n[5] Z. Xuewei and L. Hanshan, ‘‘Research on transformer fault diagnosis\nmethod and calculation model by using fuzzy data fusion in multi-sensor\ndetection system,’’ Optik, vol. 176, pp. 716–723, Jan. 2019.\n[6] Y .-C. Huang, ‘‘A new data mining approach to dissolved gas analysis of\noil-insulated power apparatus,’’ IEEE Trans. Power Del. , vol. 18, no. 4,\npp. 1257–1261, Oct. 2003.\n[7] A. J. X. Guo and F. Zhu, ‘‘Spectral-spatial feature extraction and classiﬁca-\ntion by ANN supervised with center loss in hyperspectral imagery,’’ IEEE\nTrans. Geosci. Remote Sens. , vol. 57, no. 3, pp. 1755–1767, Mar. 2019.\n[8] R. J. Haddad, B. Guha, Y . Kalaani, and A. El-Shahat, ‘‘Smart distributed\ngeneration systems using artiﬁcial neural network-based event classiﬁ-\ncation,’’ IEEE Power Energy Technol. Syst. J. , vol. 5, no. 2, pp. 18–26,\nJun. 2018.\n[9] W.-M. Lin, ‘‘Transformer-fault diagnosis by integrating ﬁeld data and\nstandard codes with training enhancible adaptive probabilistic network,’’\nIET Proc.-Gener. Transmiss. Distrib. , vol. 152, no. 3, pp. 335–341,\nMay 2005.\n[10] R. M. A. Velásquez, J. Vanessa, M. Lara, and A. Melgar, ‘‘Converting\ndata into knowledge for preventing failures in power transformers,’’ Eng.\nFailure Anal., vol. 101, pp. 215–229, Jul. 2019.\n[11] Y .-K. Lam, P. W. M. Tsang, and C.-S. Leung, ‘‘PSO-based K-Means clus-\ntering with enhanced cluster matching for gene expression data,’’ Neural\nComput. Appl., vol. 22, nos. 7–8, pp. 1349–1355, Jun. 2013.\n[12] S. Wang, M. Li, N. Hu, E. Zhu, J. Hu, X. Liu, and J. Yin, ‘‘ K-Means\nclustering with incomplete data,’’ IEEE Access , vol. 7, pp. 69162–69171,\n2019.\n[13] J. Feng, Y . Zhang, G. Yue, X. Liu, H. Su, and P.-F. Zhang, ‘‘Atherosclerotic\nplaque pathological analysis by unsupervised K-Means clustering,’’ IEEE\nAccess, vol. 6, pp. 21530–21535, 2018.\n[14] H. Jw, Data Mining: Concepts and Techniques , 2nd ed. San Francisco, CA,\nUSA: Morgan Kaufmann, 2006, pp. 383–386.\n[15] D. Arthur and S. Vassilvitskii, ‘‘ k-Means++: The advantages of careful\nseeding,’’ in Proc. ACM-SIAM Symp. Discrete Algorithms , New Orleans,\nLA, USA, Jan. 2007, pp. 1027–1035.\n[16] M. A. Masud, J. Z. Huang, C. Wei, J. Wang, I. Khan, and M. Zhong,\n‘‘I-nice: A new approach for identifying the number of clusters and initial\ncluster centres,’’ Inf. Sci., vol. 466, pp. 129–151, Oct. 2018.\n[17] M. Erisoglu, N. Calis, and S. Sakallioglu, ‘‘A new algorithm for initial\ncluster centers in k-Means algorithm,’’ Pattern Recognit. Lett. , vol. 32,\nno. 14, pp. 1701–1705, Oct. 2011.\n[18] W. Kwedlo and P. Iwanowicz, ‘‘Using genetic algorithm for selection of\ninitial cluster centers for the K-means method,’’ in Proc. Int. Conf. Artif.\nIntell. Soft Comput. (ICAISC) , 2010, pp. 165–172.\n[19] P. Fränti and P. SamiSieranoja, ‘‘How much can k-means be improved\nby using better initialization and repeats?’’ Pattern Recognit. , vol. 93,\npp. 95–112, Sep. 2019.\n[20] R. C. de Amorim and C. Hennig, ‘‘Recovering the number of clusters\nin data sets with noise features using feature rescaling factors,’’ Inf. Sci. ,\nvol. 324, pp. 126–145, Dec. 2015.\n[21] C. Hennig and T. F. Liao, ‘‘How to ﬁnd an appropriate clustering for mixed-\ntype variables with application to socio-economic stratiﬁcation,’’ J. Roy.\nStat. Soc. C, Appl. Stat. , vol. 62, no. 3, pp. 309–369, May 2013.\n[22] C.-W. Tsai, W.-L. Chen, M.-C. Chiang, ‘‘A modiﬁed multiobjective EA-\nbased clustering algorithm with automatic determination of the number of\nclusters,’’ in Proc. IEEE Int. Conf. Syst., Man, Cybern. (SMC) , Oct. 2012,\npp. 14–17.\n[23] L. Wang, C. Leckie, K. Ramamohanarao, and J. Bezdek, ‘‘Automatically\ndetermining the number of clusters in unlabeled data sets,’’ IEEE Trans.\nKnowl. Data Eng. , vol. 21, no. 3, pp. 335–350, Mar. 2009.\n[24] Y . Ye, J. Z. Huang, X. Chen, S. Zhou, G. Williams, and X. Xu, ‘‘Neighbor-\nhood density method for selecting initial cluster centers in K-means clus-\ntering,’’ in Proc. 10th Paciﬁc-Asia Conf. Knowl. Discovery Data Mining\n(PAKDD). Berlin, Germany: Springer-Verlag, 2006, pp. 189–198.\n[25] C. J. Merz. (1996). UCI Repository of Machine Learning Database .\n[Online]. Available: https://www.ics.uci.edu/mlearn/MLRepository.html\n[26] L. Deng, J. Pei, J. Ma, and D. L. Lee, ‘‘A rank sum test method for infor-\nmative gene discovery,’’ in Proc. 10th ACM SIGKDD Int. Conf. Knowl.\nDiscovery Data Mining , Aug. 2004, pp. 410–419.\n[27] S. Li, X. Wu, and X. Hu, ‘‘Gene selection using genetic algorithm and\nsupport vectors machines,’’ Soft Comput. , vol. 12, no. 7, pp. 693–698,\nMay 2008.\n[28] E. EiitiKasuya, ‘‘Wilcoxon signed-ranks test: Symmetry should be con-\nﬁrmed before the test,’’ Animal Behav., vol. 3, no. 79, pp. 765–767, 2010.\n[29] F. Lemke and J. A. Müeller, ‘‘Self-organising data mining,’’ Syst. Anal.\nModel. Simul., vol. 43, no. 2, pp. 231–240, 2010.\n[30] F. Lemke, J.-A. Müller, and F. List-Platz, ‘‘Self-organizing data mining\nbased on GMDH principle,’’ Mathematik, 2019. [Online]. Available:\nhttps://pdfs.semanticscholar.org/a86d/dc1aaa3a68c37f0ad32be1f41cc1d\n66481a3.pdf\n[31] A. G. Ivakhnenko, ‘‘Sorting methods in self-organization of models and\nclusterizations (review of new basic ideas)-iterative (multirow) polynomial\nGMDH algorithms,’’ Soviet J. Automat. Inf. Sci. , vol. 22, no. 4, pp. 1–12,\n2002.\n[32] M. Halkidi, Y . Batistakis, and M. Vazirgiannis, ‘‘On clustering validation\ntechniques,’’J. Intell. Inf. Syst. , vol. 17, nos. 2–3, pp. 107–145, Dec. 2001.\nJUNDE CHEN received the bachelor’s degree\nfrom Xiangtan University, in 2004, and the mas-\nter’s degree from Sichuan University, in 2010.\nHe is currently pursuing the Ph.D. degree with\nthe School of Informatics, Xiamen University. His\nresearch interests include the aspects of data min-\ning and image processing.\nVOLUME 7, 2019 127965\nJ. Chenet al.: Economic Operation Analysis Method of Transformer Based on Clustering\nDEFU ZHANG is currently with the School of\nInformatics, Xiamen University. He has published\narticles in the following journals: the INFORMS\nJournal on Computing , Computers & Operations\nResearch, the European Journal of Operational\nResearch, and Expert System with Applications .\nHis research interests include all aspects of com-\nputational intelligence, image analysis, and data\nmining.\nYASER AHANGARI NANEHKARAN received\nthe B.E. degree in power electrical engineering\nfrom the IAU of Ardabil Branch, Ardabil, Iran, and\nthe M.Sc. degree in IT from Cankaya University,\nAnkara, Turkey. He is currently pursuing the Ph.D.\ndegree with the Department of Computer Science,\nXiamen University, Xiamen, China. His research\ninterests mainly include data mining, big data, and\ndeep learning techniques.\n127966 VOLUME 7, 2019",
  "topic": "Cluster analysis",
  "concepts": [
    {
      "name": "Cluster analysis",
      "score": 0.7972855567932129
    },
    {
      "name": "Computer science",
      "score": 0.7431187033653259
    },
    {
      "name": "Transformer",
      "score": 0.6562782526016235
    },
    {
      "name": "Empirical research",
      "score": 0.4931316077709198
    },
    {
      "name": "Data mining",
      "score": 0.4503171443939209
    },
    {
      "name": "Electric power system",
      "score": 0.44010990858078003
    },
    {
      "name": "Reliability engineering",
      "score": 0.40360909700393677
    },
    {
      "name": "Power (physics)",
      "score": 0.1804196536540985
    },
    {
      "name": "Voltage",
      "score": 0.16618746519088745
    },
    {
      "name": "Machine learning",
      "score": 0.15755194425582886
    },
    {
      "name": "Mathematics",
      "score": 0.14964953064918518
    },
    {
      "name": "Engineering",
      "score": 0.13459810614585876
    },
    {
      "name": "Electrical engineering",
      "score": 0.09586384892463684
    },
    {
      "name": "Statistics",
      "score": 0.07775887846946716
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}