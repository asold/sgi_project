{
    "title": "Surgicberta: a pre-trained language model for procedural surgical language",
    "url": "https://openalex.org/W4385892080",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5037203349",
            "name": "Marco Bombieri",
            "affiliations": [
                "University of Verona"
            ]
        },
        {
            "id": "https://openalex.org/A5043305308",
            "name": "Marco Rospocher",
            "affiliations": [
                "University of Verona"
            ]
        },
        {
            "id": "https://openalex.org/A5017409996",
            "name": "Simone Paolo Ponzetto",
            "affiliations": [
                "University of Mannheim"
            ]
        },
        {
            "id": "https://openalex.org/A5065107543",
            "name": "Paolo Fiorini",
            "affiliations": [
                "University of Verona"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3174847532",
        "https://openalex.org/W3152757982",
        "https://openalex.org/W4310858512",
        "https://openalex.org/W4380631538",
        "https://openalex.org/W4285116174",
        "https://openalex.org/W4285139439",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2798935874",
        "https://openalex.org/W2740765036",
        "https://openalex.org/W1663984431",
        "https://openalex.org/W3108490334",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W3021766370",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3034323081",
        "https://openalex.org/W4285253024",
        "https://openalex.org/W4296194906",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3141847762",
        "https://openalex.org/W3199912944",
        "https://openalex.org/W2158847908",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2103076621",
        "https://openalex.org/W3012922460",
        "https://openalex.org/W3024308166",
        "https://openalex.org/W4283368635",
        "https://openalex.org/W4313197536",
        "https://openalex.org/W2770445088",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3088059392",
        "https://openalex.org/W4210898640",
        "https://openalex.org/W3202563726",
        "https://openalex.org/W3207576362",
        "https://openalex.org/W4283813460",
        "https://openalex.org/W3095092693",
        "https://openalex.org/W4312220150",
        "https://openalex.org/W2976476443",
        "https://openalex.org/W4220989182",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3034775979"
    ],
    "abstract": "Abstract Pre-trained language models are now ubiquitous in natural language processing, being successfully applied for many different tasks and in several real-world applications. However, even though there is a wealth of high-quality written materials on surgery, and the scientific community has shown a growing interest in the application of natural language processing techniques in surgery, a pre-trained language model specific to the surgical domain is still missing. The creation and public release of such a model would serve numerous useful clinical applications. For example, it could enhance existing surgical knowledge bases employed for task automation, or assist medical students in summarizing complex surgical descriptions. For this reason, in this paper, we introduce SurgicBERTa , a pre-trained language model specific for the English surgical language, i.e., the language used in the surgical domain. SurgicBERTa has been obtained from RoBERTa through continued pre-training with the Masked language modeling objective on 300 k sentences taken from English surgical books and papers, for a total of 7 million words. By publicly releasing SurgicBERTa , we make available a resource built from the content collected in many high-quality surgical books, online textual resources, and academic papers. We performed several assessments in order to evaluate SurgicBERTa , comparing it with the general domain RoBERTa . First, we intrinsically assessed the model in terms of perplexity, accuracy, and evaluation loss resulting from the continual training according to the masked language modeling task. Then, we extrinsically evaluated SurgicBERTa on several downstream tasks, namely (i) procedural sentence detection, (ii) procedural knowledge extraction, (iii) ontological information discovery, and (iv) surgical terminology acquisition. Finally, we conducted some qualitative analysis on SurgicBERTa , showing that it contains a lot of surgical knowledge that could be useful to enrich existing state-of-the-art surgical knowledge bases or to extract surgical knowledge. All the assessments show that SurgicBERTa better deals with surgical language than a general-purpose pre-trained language model such as RoBERTa , and therefore can be effectively exploited in many computer-assisted applications in the surgical domain.",
    "full_text": "International Journal of Data Science and Analytics (2024) 18:69–81\nhttps://doi.org/10.1007/s41060-023-00433-5\nREGULAR PAPER\nSurgicberta: a pre-trained language model for procedural surgical\nlanguage\nMarco Bombieri 1 · Marco Rospocher 2 · Simone Paolo Ponzetto 3 · Paolo Fiorini 1\nReceived: 3 March 2023 / Accepted: 12 July 2023 / Published online: 16 August 2023\n© The Author(s) 2023\nAbstract\nPre-trained language models are now ubiquitous in natural language processing, being successfully applied for many different\ntasks and in several real-world applications. However, even though there is a wealth of high-quality written materials on surgery,\nand the scientiﬁc community has shown a growing interest in the application of natural language processing techniques in\nsurgery, a pre-trained language model speciﬁc to the surgical domain is still missing. The creation and public release of such\na model would serve numerous useful clinical applications. For example, it could enhance existing surgical knowledge bases\nemployed for task automation, or assist medical students in summarizing complex surgical descriptions. For this reason, in\nthis paper, we introduce SurgicBERTa, a pre-trained language model speciﬁc for the English surgical language, i.e., the\nlanguage used in the surgical domain. SurgicBERTa has been obtained from RoBERTa through continued pre-training\nwith the Masked language modeling objective on 300 k sentences taken from English surgical books and papers, for a total of\n7 million words. By publicly releasing SurgicBERTa, we make available a resource built from the content collected in many\nhigh-quality surgical books, online textual resources, and academic papers. We performed several assessments in order to\nevaluate SurgicBERTa, comparing it with the general domain RoBERTa. First, we intrinsically assessed the model in terms\nof perplexity, accuracy, and evaluation loss resulting from the continual training according to the masked language modeling\ntask. Then, we extrinsically evaluated SurgicBERTa on several downstream tasks, namely (i) procedural sentence detection,\n(ii) procedural knowledge extraction, (iii) ontological information discovery, and (iv) surgical terminology acquisition. Finally,\nwe conducted some qualitative analysis on SurgicBERTa, showing that it contains a lot of surgical knowledge that could\nbe useful to enrich existing state-of-the-art surgical knowledge bases or to extract surgical knowledge. All the assessments\nshow that SurgicBERTa better deals with surgical language than a general-purpose pre-trained language model such as\nRoBERTa, and therefore can be effectively exploited in many computer-assisted applications in the surgical domain.\nKeywords Transformers · Language models · Natural language processing · Medicine\n1 Introduction\nThe ﬁeld of artiﬁcial intelligence known as natural language\nprocessing (NLP) allows for automated processing and anal-\nysis of everyday language. In the past two decades, NLP has\nrapidly expanded across all information technology domains\nand is now being utilized more frequently in medicine. Its\nB Marco Bombieri\nmarco.bombieri_01@univr.it\n1 Department of Computer Science, University of V erona,\nV erona, Italy\n2 Department of Foreign Languages and Literatures, University\nof V erona, V erona, Italy\n3 DWS Group, University of Mannheim, Mannheim, Germany\napplications include enhancing the use of unstructured elec-\ntronic health records, aiding communication with patients,\nconducting consultations, and ﬁnding pertinent information\nin papers [ 21]. Most cutting-edge NLP techniques rely on\nstatistical language modeling, which involves representing\nwords as numerical vectors that capture their probability dis-\ntribution in a sentence structure [ 12]. These vectors, also\nknown as word embeddings, are numerical representations of\nwords and are frequently generated through self-supervised\nmachine learning methods applied to large, unlabeled textual\ndatasets. More advanced language models create distinct rep-\nresentations for a word based on its context, allowing them\nto accurately capture polysemous terms that have multiple\nmeanings. Contextual language models based on Trans-\nformer architectures, such as BERT [ 8]o rR o B E R T a[20], are\n123\n70 International Journal of Data Science and Analytics (2024) 18:69–81\ntrained using a deep neural network with a masked language\nmodeling (MLM) objective [ 33]. These models use a bidirec-\ntional self-attention mechanism [ 34] to associate each word\nwith its context, or the words surrounding it in the sentence.\nThese features enable contextual language models to outper-\nform non-contextual ones in various NLP tasks [ 8]. Although\ntrained on enormous digital corpora consisting of billions of\nwords, language models trained on general text frequently\ndo not work effectively in very specialized domains such as\nscientiﬁc ones. As a result, several recent NLP studies have\nconcentrated on retraining or ﬁne-tuning language models\nfor very specialized domains using domain-speciﬁc text (as\nexplained in detail in Sect. 2).\nWhile a large number of domain-speciﬁc language mod-\nels have been developed to improve the understanding of the\nsemantic information in their ﬁeld of expertise, to the best\nof our knowledge a specialized model for surgical language\ndoes not exist yet, even if the scientiﬁc community has shown\ngrowing interest in the application of NLP in surgery [ 19, 28,\n38–40]. There is an abundance of high-quality resources in\nthe surgical literature, including books, online materials, and\nacademic papers that are adopted and utilized by universi-\nties around the globe. The vast quantity of this high-quality\navailable information can be a valuable resource for vari-\nous clinical applications, involving both humans and smart\nrobotics systems, if automatically processed via NLP tech-\nniques. For instance, one possible application of using the\ncontent extracted from textual resources is for building or\nextending the knowledge bases exploited by surgical robots,\nwhich they can use to make informed decisions in real-life\nintervention situations. Similarly, as reported in recent stud-\nies focusing on the clinical ﬁeld [ 30, 42], humans can also\nbeneﬁt from this information in question-answering appli-\ncations. These systems could be useful for medical students\nduring their early training phase, or to provide a summary or\nsimpliﬁed version of surgical descriptions.\nIn this paper, we follow this line of research and intro-\nduce a new pre-trained language model trained on procedural\nsurgical language, named SurgicBERTa.T h em a i n ,n o v e l\ncontributions\n1 presented in this paper are:\n1. The development of SurgicBERTa, a pre-trained lan-\nguage model speciﬁc for the understanding of procedural\nsurgical language;\n2. The intrinsic evaluation of SurgicBERTa with respect\nto the general-purpose model RoBERTa;\n3. The extrinsic evaluation of SurgicBERTa with respect to\nRoBERTa, that is, the comparison of their performances\nwhen employed on four different downstream tasks;\n1 All the materials and results presented in this paper are novel, except\nthe experiments described in Sect. 4.3, which were ﬁrst presented in [ 4].\n4. The public release of SurgicBERTa to the research com-\nmunity: https://gitlab.com/altairLab/surgicberta.\nThe quantitative assessments are complemented with qual-\nitative analysis on SurgicBERTa, showing that it contains\na lot of surgical domain knowledge that could be use-\nful to enrich existing state-of-the-art surgical knowledge\nbases. The evaluation indicates that SurgicBERTa better\ndeals with surgical language than a state-of-the-art yet open-\ndomain and general-purpose model such as RoBERTa, and\ntherefore can be effectively exploited in many computer-\nassisted applications, speciﬁcally in the surgical domain.\nThe paper is organized as follows: Sect. 2 revises rel-\nevant works in this area. Then, SurgicBERTa is pre-\nsented in Sect. 3. The required textual data is collected,\nextracted, pre-processed and used for the continuous train-\ning of RoBERTa on the MLM task with domain-speciﬁc\ntext. Section 4 presents the intrinsic metrics and tasks used\nto evaluate SurgicBERTa. In particular, metrics for the\nintrinsic evaluation of SurgicBERTa (i.e., perplexity, accu-\nracy, and evaluation loss of the MLM task) are presented in\nSect. 4.1, while Sects. 4.2–4.5 present the downstream tasks\nused to compare SurgicBERTa with RoBERTa, namely,\n(i) procedural sentences detection, (ii) procedural knowl-\nedge extraction, (iii) ontological information discovery, and\n(iv) surgical terminology acquisition. Section 4.6 reports and\nqualitatively discusses some examples of surgical domain\nknowledge contained in SurgicBERTa. Finally, Sect. 5\nsummarizes obtained results and proposes future works.\n2 Related works\n2.1 Transformers and pre-trained language models\nTransformers are deep-learning models widely used in NLP\n[34] and computer vision [ 9]. In particular, they have fun-\ndamentally changed the landscape of NLP by gradually\nreplacing recurrent neural networks across the board. The\ncore innovative part of these architectures is the self-attention\nmechanism [34]. Since one word can have different meanings\nin different contexts, self-attention allows the model to look\nat other positions in the input sequence for clues that can help\nlead to a better encoding for the current word. Moreover, the\ncreation of large-scale, Transformer-based pre-trained lan-\nguage models such as BERT or RoBERTa has revolutionized\nthe NLP domain. These models only use the encoder part of\nthe Transformer (in contrast, e.g., to denoising autoencoders\nsuch as BART [ 16]). Such pre-trained large models are pre-\ntrained once in an unsupervised way, e.g., on a language\nmodel objective, and can be ﬁne-tuned for a large number of\nNLP tasks with a modest amount of training data, achieving\nstate-of-the-art results on many of them, such as sentiment\n123\nInternational Journal of Data Science and Analytics (2024) 18:69–81 71\nanalysis, textual entailment, and natural language inference,\ncrucially also across languages [ 15].\n2.2 Pre-trained language models in biomedicine\nTransformer-based pre-trained language models have also\nbeen ﬁne-tuned for different tasks in the biomedical domain.\nHowever, they were originally built for general English,\nand thus they may miss some domain words or expres-\nsions. To overcome this limit, there is the possibility to train\nfrom scratch a model speciﬁc to a given domain of interest,\nsuch as in [ 42] where a large model speciﬁc to the clini-\ncal domain using > 90 billion words of text is proposed.\nDeveloping such a model from scratch is very expensive for\nthe computational resources and the training time required.\nFor this reason, domain adaptation techniques, such as the\nMLM described in Sect. 3, have been proposed and widely\nused in biomedicine with ﬁne-tuning for various downstream\ntasks. In [ 44], domain adaptation is used to obtain a cancer\ndomain-speciﬁc language model for effectively extracting\nbreast cancer phenotypes from electronic health records.\nIn [ 37], the authors utilize pre-trained neural models to\nclassify patients as either seizure-free or not, as well as to\nextract text from clinical notes that contains their seizure fre-\nquency and the date of their last seizure. The ﬁrst step of\nthis pipeline is the unsupervised domain adaptation, using\nprogress notes that were not selected for annotation. The\nobtained model has been ﬁne-tuned for the classiﬁcation\nand extraction tasks. Also, [ 41] adopted a domain adaptation\ntechnique on clinical notes from the Medical Information\nMart for Intensive Care III database [ 14] to extract clinically\nrelevant information. In [ 18], causal precedence relations\nare recognized among the chemical interactions in the\nbiomedical literature to understand the underlying biological\nmechanisms. However, detecting such causal relations can be\nchallenging because annotating such causal relation detec-\ntion datasets requires considerable expert knowledge and\neffort. To overcome this limitation, in-domain pre-training\nof neural models with knowledge distillation techniques has\nbeen adopted, showing that the neural models outperform\nprevious baselines even with a small number of annotated\ndata. In [ 7], a domain adaptation strategy is adopted to\nencourage the model to learn features from the context to\ncurate all validated antibiotic resistance genres, i.e., the abil-\nity of bacteria to survive and propagate in the presence of\nantibiotics, from scientiﬁc papers. In [ 30], a domain adapta-\ntion technique has been used to align large language models\nto new medical domains, showing that, after a proper adap-\ntation step, they encode some clinical knowledge usable in\nquestion-answering applications. Finally, a domain adap-\ntation technique has been adopted for biomedical domain\nadaptation in languages different than English, such as Span-\nish [ 6] and Chinese [ 43] showing the same improvement\ntrend when compared to the corresponding base models.\nHowever, due to the syntactic, semantic, and terminolog-\nical differences between domains, it is often difﬁcult to use\nthese models to gain beneﬁts outside the domain they were\ntrained on. It is generally accepted that model performance\nmay degrade when evaluated on data with a different dis-\ntribution [ 31]. Consequently, domain adaptation on relevant\ndomain data is essential to improve performance in very spe-\ncialized domains [ 1], and despite the availability of several\nbiomedical language models, to the best of our knowledge, a\npre-trained surgical language model is missing. Such a model\nis essential for mining surgical procedural knowledge from\ntext and developing intelligent surgical systems.\n3 A language model for the surgical domain:\nSURGIC BERTA\nThis section describes the development of Surgic-\nBERTa, the pre-trained language model for the surgical\ndomain that we contribute. SurgicBERTa has been devel-\noped on top of RoBERTa, an already available pre-trained\nlanguage model for English for the general domain. Specif-\nically, the roberta-base version of the HuggingFace\nTransformer library has been adopted. Therefore, the eval-\nuation (presented in Sect. 4) will compare these two models\nalong several dimensions.\nRoBERTa [20] is a Transformer model that adopts the\nsame encoder–decoder architecture made popular by BERT\n[8], while being trained on a larger quantity of data, consisting\nin a combination of datasets totaling around 160 GB of raw\ntext: namely, texts from BookCorpus and English Wikipedia,\ndata from the English portion of the CommonCrawl News,\nfrom OpenWebText, and some stories from CommonCrawl\ndata. RoBERTa has been trained via MLM with dynamic\nmasking: i.e., each time a sequence is input to the model, a\nnew masking pattern is created. Differently from BERT, Ro-\nBERTa was not trained also on next sentence prediction, as\nthis training task did not contribute a signiﬁcant improvement\nof the performance in downstream tasks [ 20].\nLeveraging RoBERTa as a starting point, we devel-\noped a new model that is tailored to the surgical domain.\nThis involved the continuous training of RoBERTa on\na large corpus of surgical text for the MLM unsuper-\nvised task. In the MLM task, a token w\nt is replaced with\n⟨mask ⟩ and predicted using all past and future tokens\nW \\t := (w1,..., wt−1,wt+1,..., w|W |). Figure 1 illus-\ntrates the MLM task used to derive SurgicBERTa.\nIn more detail, to obtain a surgical model as general as\npossible, we collected 300 K sentences (7 M million words)\nfrom surgery books covering several heterogeneous surgi-\ncal domains, including, for instance, orthopedics, abdominal\n123\n72 International Journal of Data Science and Analytics (2024) 18:69–81\nSurgicBERTa masked language model\nSentence:\nInput: [s] radical prostatectomy is the surgical removal\nradical prostatectomy is the surgical removal of the <mask>.\nof the <mask> [\\s]\n[s] radical prostatectomy is the surgical removal of the [\\s]\n1. prostate\n2. tumor\n3. specimen\n4. …\nOutput:\nThe token “pprostate” has the highest probability\nFig. 1 MLM task used for adapting SurgicBERTa to the surgical domain. s and \\s are special tokens denoting the sentence’s beginning and end,\nrespectively\nsurgery, and eye surgery. We searched for surgery books writ-\nten in English on the web pages of several publishing houses.\nAs keywords, we used the name of the surgical macro-\nareas (e.g., general surgery, abdominal surgery, gynecology\nsurgery, eye surgery, etc.). From the results, we downloaded\nthe digital version only of the texts to which our universities\nhave proper free legitimate access.\n2 A very minimal pre-\nprocessing of the sentences was performed, mainly to clean\nthe text from bibliographic references and URLs. In more\ndetail, 15% of tokens are selected for possible replacement.\nAmong those selected tokens, 80% are replaced with the\nspecial ⟨mask ⟩ token, 10% are left unchanged and 10% are\nreplaced by a random token. The model is then trained to\npredict the initial masked tokens using cross-entropy loss.\nFollowing the RoBERTa approach, tokens are dynamically\nmasked instead of ﬁxing them statically for the whole dataset\nduring pre-processing. This improves variability and makes\nthe model more robust when training for multiple epochs.\nSurgicBERTa is computed using one NVIDIA RTX A6000\n2 By choosing only the texts to which we have free access thanks to\nour institutions’ agreement, we have probably excluded some resources\nwhich, if used in the training phase, would have allowed us to increase\nthe training material and therefore probably also the performance. How-\never, given the high diversity of the resources made available by our\ninstitutions and used for the training material, we do not believe that\nthis choice has too much impact on performance.\nGPU, with 48 GB of GPU memory. We trained for 30 epochs\nwith a learning rate of 5 e−06 and a batch size of 32. The\nAdam optimizer has been used. The implementation is based\non PyTorch and Transformers libraries. The entire training\nrequired about 8 hours to be completed.\n4 Evaluation\nThis section presents the intrinsic evaluation (Sect. 4.1) and\nthe four downstream tasks that we use to evaluate Surgic-\nBERTa in Sect. 4.2 through 4.5, namely: procedural/non-\nprocedural surgical sentence classiﬁcation, surgical infor-\nmation extraction, ontological information discovery, and\nsurgical terminology acquisition.\n4.1 Intrinsically evaluating the quality of language\nmodeling\n4.1.1 Evaluation metrics\nPerplexity is one of the most common metrics for evaluating\nlanguage models and measures the degree of uncertainty of a\nlanguage model to generate a new token, averaged over very\nlong sequences [27]. This means that the lower the perplexity,\n123\nInternational Journal of Data Science and Analytics (2024) 18:69–81 73\ncalculated as the exponentiated average negative log likeli-\nhood of a sequence, the better the language model is able to\npredict a given text. While perplexity can be computed out of\nthe box for traditional language models trained on guessing\nthe next word given the previous context, i.e., autoregres-\nsive or causal language models, it is not well deﬁned for\nlanguage models like BERT or RoBERTa trained with the\nmasked language modeling technique. For these models, we\ncan compute instead the perplexity from their pseudo-log\nlikelihood scores (PPL) [36], which corresponds to the sum\nof conditional log probabilities of each sentence token [ 29].\nFormally, the pseudo-log likelihood scores ( PPL )o fas e n -\ntence W = (w\n1,...,w |W |) under a language model with\nparameters /Theta1is deﬁned as:\nPPL(W ) :=\n|W |∑\nt=1\nlog PMLM(wt |W \\t ;/Theta1)\nwhere PMLM(wt |W \\t ;/Theta1)is the conditional probability\nof token wt given all past and future tokens W \\t :=\n(w1,...,w t−1,wt+1,...,w |W |).\nThe (pseudo) perplexity PP of a masked language model\n[27] on a corpus of sentences W is then computed as:\nPP(W) := exp\n(\n− 1\nN\n∑\nw∈W\nPPL (W )\n)\nwhere N is the number of tokens in the corpus. By computing\nPP on a test corpus for both RoBERTa and SurgicBERTa,\nwe are evaluating the model’s ability to predict the unseen\ntext from the corpus and take this as an intrinsic evaluation\nmetric of the quality of the two models.\n3\nOther intrinsic metrics used to evaluate RoBERTa and\nSurgicBERTa on the surgical domain in this paper are the\naccuracy of MLM computed on the masked tokens during\nthe evaluation step and the evaluation loss. Accuracy mea-\nsures how well our model predicts the masked words by\ncomparing the model predictions with the proper values in\nterms of percentage. Instead, the loss is a value that repre-\nsents the summation of errors in a model. It measures how\nwell or badly the model is performing. If the errors are high,\nthe loss will be high, and then the model will not perform\nwell.\nGenerally, the higher the accuracy in the evaluation dataset\nand the lower the evaluation loss, the better the model will\nperform.\n3 Our comparison is fair in that RoBERTa and SurgicBERTa share\nthe same tokenizer and the same vocabulary.\nTable 1 Perplexity, accuracy, and evaluation loss\nPre-trained model Perplexity Accuracy Evaluation loss\nRoBERTa 15.410 0.546 2.735\nSurgicBERTa 4.300 0 .699 1 .458\nBold values mark the better scores for each metric\n4.1.2 Results and discussion\nTable 1 reports perplexity, accuracy, and loss values of Ro-\nBERTa and SurgicBERTa obtained during the evaluation\nof the MLM tasks as described in Sect. 4.1. SurgicBERTa\nhas lower perplexity ( −11.11), greater accuracy ( +15.30%),\nand lower evaluation loss ( −1.277) than RoBERTa.A l l\nobtained results intrinsically conﬁrm that SurgicBERTa\nbetter deals with surgical language than RoBERTa.\n4.2 Extrinsic evaluation: task A—procedural content\ndetection\n4.2.1 Task deﬁnition\nThe detection of procedural content consists of a binary\nclassiﬁcation task where the aim is to classify each sen-\ntence of a corpus into two different classes ( procedural and\nnon-procedural) This task is generally a preliminary and\nessential step for the business or robotic process automation\nstarting from procedural content stored in textual materials\nbecause it allows models to deal with only those sentences\nthat are important for the extraction of a workﬂow [ 26]. In\nthe case of the surgical domain, the two classes are deﬁned\nin [ 2]:\n• Procedural sentences describe a speciﬁc action per-\nformed by either the robot or the human surgeon (e.g.,\nan intervention on the body, the positioning of the robot).\nAn example of a procedural sentence is “The colon is\nreﬂected medially over the kidney along the white line of\nToldt.”;\n• Non-procedural sentences do not contain any indication\nof a speciﬁc surgeon action, but rather describe general,\ncomplementary information or anatomical features, not\nnecessarily speciﬁc to perform a particular step of the\nintervention. An example of a non-procedural sentence\nis “This permits greater range of camera movement infe-\nriorly within the retroperitoneum.”\nAs training and testing material, we exploit the latest avail-\nable version (v1.1) of the SPKS dataset,\n4 containing 2250\n4 https://gitlab.com/altairLab/spks-dataset.\n123\n74 International Journal of Data Science and Analytics (2024) 18:69–81\nsentences manually annotated as procedural (approx. 68%)\nand non-procedural (approx. 32%).\nIn order to ﬁne-tune RoBERTa and SurgicBERTa for\nprocedural sentence classiﬁcation, these pre-trained mod-\nels have been extended to produce a classiﬁcation output\n(procedural/non-procedural) by adding a softmax-activated\nclassiﬁcation layer on the pre-trained language models, and\nthen by ﬁne-tuning them on the SPKS dataset. A standard\ncross-entropy loss function has been adopted for classiﬁca-\ntion. Due to the reduced size of the dataset, we utilized the\nclassical 10-fold cross-validation protocol, which involves\ndividing the dataset into ten sets. In each iteration, one set is\nused for testing the classiﬁer, while the remaining nine sets\nare used for training and hyperparameter tuning. This process\nis repeated ten times, and the classiﬁcation performance is\nevaluated by computing the average of the evaluation metrics\nover the ten iterations.\nStandard metrics for classiﬁcation tasks, namely preci-\nsion (P), recall (R), and F1-score, are used to compute\nperformance. The metrics are calculated for each class\n(procedural/non-procedural) and we report for each of them\nthe macro average, i.e., the mean of the considered metric\non the two classes. In addition, we also compute Accuracy\n(Acc), i.e., the ratio between the correctly predicted classes,\ndivided over the test set size, that in the case of binary classiﬁ-\ncation, coincides with the micro average of P , R, and F1. For\ntesting the statistical signiﬁcance, we computed the p value\napplying the McNemar’s test with signiﬁcance threshold α\nof 0 .05, as implemented in [ 10].\n4.2.2 Results and discussion\nResults of the procedural sentence detection task described\nin Sect. 4.2 have been reported in Table 2. SurgicBERTa\nimproves all the performance metrics when compared to\nRoBERTa on both procedural and non-procedural classes.\nOverall, averaging the performances on both classes, Surgic-\nBERTa improves the accuracy of 0 .014, and Macro-F1\nof 0 .015, conﬁrming the beneﬁt of having a domain-\nspeciﬁc language for surgical-related text classiﬁcation. The\nobserved performance difference of the two systems is sta-\ntistically conﬁrmed by the considered signiﬁcance test.\n4.3 Extrinsic evaluation: task B—procedural\nknowledge extraction\n4.3.1 Task deﬁnition 5\nThe purpose of this task is the extraction of procedural\ninformation from texts using semantic role labeling (SRL)\n5 This section summarizes ﬁndings and content previous presented in\n[4].\nTable 2 Text classiﬁcation performance of the tested methods (Extrinsic Evaluation—Task A)\nModel Procedural Non-procedural Macro\nP R F1 P R F1 Acc P R F1\nRoBERTa 0.889 (0.019) 0.928 (0.063) 0.908 (0.039) 0.831 (0.053) 0.753 (0.017) 0.790 (0.029) 0.872 (0.021) 0.860 (0.029) 0.841 (0.021) 0.849 (0.022)\nSurgicBERTa 0.894 (0.018) 0.945 (0.032) 0.919 (0.016) 0.865 (0.047) 0.762 (0.010) 0.810 (0.028) 0.886 (0.015) 0.880 (0.018) 0.853 (0.022) 0.864 (0.019)\nThe best scores are highlighted in bold. The standard deviation between the various folds is reported in brackets\n123\nInternational Journal of Data Science and Analytics (2024) 18:69–81 75\ntechniques. Given a sentence, the SRL task aims at label-\ning the semantic arguments of the sentence predicates in\norder to extract Who does What to Whom, How, When, and\nWhere. In this paper, we adopt the PropBank [ 23] approach\nfor SRL, leveraging the catalog of semantic roles and predi-\ncate meanings codiﬁed in the Robotic-Surgery Propositional\nFramebank (RSPF) [ 3].\nSRL can be organized in two complementary subtasks:\n(i) predicate disambiguation , i.e., the understanding of the\ncorrect meaning of a word describing an action (a.k.a., a\npredicate), and (ii) semantic arguments identiﬁcation and\nclassiﬁcation, i.e., the detection of the argument spans of a\npredicate, and the assignment of them to the correct semantic\nrole labels from RSPF. For example, given the sentence:\nThe colon is reﬂected medially over the kidney along\nthe white line of Toldt.\nwith task (i) the method should recognize that reﬂect has in\nthis context the RSPF’s meaning of reﬂect.03, i.e., to bend\nor fold back , and not for example the RSPF’s meaning of\nreﬂect.02, i.e., think about or reﬂect.01, i.e., cast an image\nback, casting back an image . Then, given this meaning, the\nmethod has to solve the task (ii), i.e., to tokenize and classify\nthe arguments in the sentence as follows:\n[Arg.1: The colon] is [V: reﬂected] [Arg.2: over the\nkidney] [Arg.3: along the white line of Toldt].\nwhere Arg.1, Arg.2 and Arg.3 indicate (a) the thing reﬂected,\n(b) its location, and (c) other spatial useful indications ,\nrespectively.\nModern SRL methods rely on neural architectures that\nrequire annotated data to learn the language in a supervised\nway [11, 17]. To train, validate, and test the models, we used\ntwo different manually annotated textual datasets for seman-\ntic role labeling: CoNLL-2012 [ 25] and a smaller dataset\nspeciﬁc to robotic surgery [ 5]. CoNLL-2012 is a large-scale\ngeneral-English corpus with 318 k annotated predicates, cov-\nering multiple genres. We used this dataset to teach the\ncommon neural architecture the basic knowledge about the\nSRL task. The smaller dataset is instead domain-speciﬁc,\ncontaining 1559 SRL-annotated sentences regarding robotic\nsurgery procedures, thus including both traditional surgical\nactions and speciﬁc robot operations. We used this smaller\ndataset to specialize the models, helping them to better under-\nstand surgical language and perform the SRL task more\neffectively in the given domain. The train, test, and valida-\ntion splits already provided with the smaller dataset are used\nfor the training, tuning, and evaluation of the performances.\nSpeciﬁcally, 80% of the sentences are utilized for training\n(with 10% of them being set aside for validation), while the\nremaining 20% are dedicated to the test dataset. Moreover,\nfor comparing the two language models on this task, the same\nmetrics adopted for the procedural content detection task are\nTable 3 Performance (overall) on the SRL task (Extrinsic Evaluation—\nTask B). The best scores are highlighted in bold\nPredicates Arguments\nPre-trained model Accuracy Precision Recall F1\nRoBERTa 0.907 0.771 0.752 0.762\nSurgicBERTa 0.925 0 .778 0 .768 0 .773\nused (cf. Sect. 4.2). For testing the statistical signiﬁcance, we\napplied the Bootstrap test on the accuracy of the label (predi-\ncates and arguments) predictions with signiﬁcance threshold\nα of 0 .05 and using the implementation of [ 10].\n4.3.2 Results and discussion\nTable 3 reports the performance of the procedural knowl-\nedge extraction task described in Sect. 4.3. SurgicBERTa\nsubstantially improves the predicated disambiguation task\naccuracy of 0 .018 when compared to RoBERTa. Moreover,\nSurgicBERTa outperforms RoBERTa in all evaluation\nmetrics related to the arguments disambiguation task. In par-\nticular, it improves the precision of 0 .007, recall of 0 .016,\nand F1 of 0 .011. The improvement is conﬁrmed to be sta-\ntistically signiﬁcant by the performed Bootstrap test. These\nresults extrinsically demonstrate the beneﬁt of having spe-\ncialized RoBERTa in the surgical domain for the accurate\nextraction of actions and related information from surgical\ntext.6\n4.4 Extrinsic evaluation: task C—ontological\ninformation about the surgery and anatomical\ntarget\n4.4.1 Task deﬁnition\nThe purpose of this task is to associate the name of the sur-\ngical procedure with the corresponding anatomical target or\nrelevant feature to verify if the language models have learned\nthis type of knowledge during training. For example, the\nprostatectomy has to be associated with prostate, nephrec-\ntomy with kidney, and mastectomy with breast. To evaluate\nour models on this task, we built a dataset consisting of the\ndeﬁnition of 20 different surgical procedures. In particular,\nsurgical procedures that can be performed with the aid of\na robot have been chosen, together with other very frequent\nlaparoscopic ones. The deﬁnitions are retrieved from the web\nor surgical manuals not used during the training of the lan-\nguage models. From them, the name of the corresponding\n6 A more ﬁne-grained assessment of the application of SurgicBERTa\nfor SRL on the surgical domain is provided in [ 4], where different com-\nplementary analyses and comparisons (e.g., zero-shot learning, few-shot\nlearning) are performed.\n123\n76 International Journal of Data Science and Analytics (2024) 18:69–81\nanatomical target has been removed, and the models are asked\nto guess it. As evaluation metrics, we consider the ranking\nof the correct target word with respect to the others returned\nby the model, the reciprocal rank (RR), and the mean recip-\nrocal rank (MRR) [ 35]. We have chosen these metrics and\nnot others primarily with “accuracy\" because we have a ﬁnite\nlist of candidates in output that we want to be able to scroll\nthrough. MRR is a metric used to assess the performance\nof systems that provide a ranked list of answers in response\nto user queries. In the case of this task, answers are words\nreturned to ﬁll the ⟨mask ⟩ , i.e., the anatomical part corre-\nsponding to the procedure description, and queries are the\nsentences describing the procedure. In more detail, for a sin-\ngle query, the RR is deﬁned as\n1\nrank , where rank is the position\nof the correct answer among the ones (sorted by probability,\nfrom the highest to the lowest) predicted by the model. For\nmultiple queries |Q|, the MRR is the mean of the |Q| RRs,\ni.e.,\nMRR = 1\n|Q|\n|Q|∑\ni=1\n1\nranki\n= 1\n|Q|\n|Q|∑\ni=1\nRRi (1)\nThe vocabulary has not been restricted, i.e., a list of possible\ncandidates to choose from has not been used so that models\ncan return any word belonging to the vocabulary.\nTo better clarify with an example, consider the following\nsentence (i.e., query):\na sacrocolpopexy is a surgical procedure used to treat\n⟨mask ⟩ organ prolapse.\nModels are asked to ﬁll in the missing word with the correct\none which in the above example is pelvic. They will propose\na list of possible candidates sorted by probability. For exam-\nple, for the above sentence, RoBERTa and SurgicBERTa\nreturn the correct word pelvic in the third and ﬁrst position,\nthus obtaining an RR of 0 .33 and 1 .0 with a log likelihood\nprobability of 0 .043 and 1 .0 respectively. For testing the\nstatistical signiﬁcance, we applied the Bootstrap test on the\nRRs of the corrected predictions, using the same α threshold\nand implementation of the other tasks.\n4.4.2 Results and discussion\nThis section summarizes the results of the above-described\ntask, i.e., that of predicting the anatomical target given the\nname and a brief deﬁnition of the surgical intervention related\nto that anatomical target. On average, the correct target is\nreturned by RoBERTa in position 2 .35, while Surgic-\nBERTa outperforms RoBERTa proposing the correct target\nin position 1 .35. The MRR of RoBERTa is 0.731, while that\nof SurgicBERTa is 0.902. In more detail, 30% of the times\nSurgicBERTa performs better than RoBERTa in terms of\nFig. 2 Reciprocal rank of the predicted word in the task of predict-\ning the anatomical target given the information of a surgical procedure\n(Extrinsic Evaluation—Task C)\nRR. The base model performs better than SurgicBERTa\nonly in one case (query 19), where the model is asked to\npredict the anatomical part related to the “endarterectomy,”\nthat is the “artery.” Since RoBERTa performs only slightly\nbetter than SurgicBERTa (the ﬁrst returns “artery\" in 4th\nposition while the latter in 5th), this is perhaps due to the\nfact that the base model may have already seen a similar sen-\ntence (or documents describing the “endarterectomy\") during\nits training phase. The violin plots of Fig. 2 summarize the\nobtained RRs on each query sentence: the one for Surgic-\nBERTa is very wide at the top and skinny in the middle and\nat the bottom, while the one of RoBERTa, albeit having a\nsimilar distribution, is much less wide at the top and has a\nmedian weight lower than that of SurgicBERTa. The shape\nof the distribution indicates that the RRs of SurgicBERTa\nare highly concentrated around the ﬁrst quartile, meaning\nthat the model is predicting very well the proper anatomical\ntarget very well. In contrast, the RRs of RoBERTa are more\nevenly distributed across the entire range, highlighting lower\nscores. The computed p value (< 0.05) conﬁrms the sta-\ntistical signiﬁcance of the observed performance difference,\nand thus the beneﬁt of having specialized RoBERTa for the\nsurgical language.\n4.5 Extrinsic evaluation: task D—surgical\nterminology acquisition\n4.5.1 Task deﬁnition\nThis task is the same as the previous one but applied to a dif-\nferent dataset and therefore proposed for a different purpose:\nto verify whether SurgicBERTa masters the surgical lan-\nguage and can use it more appropriately than RoBERTa.\n123\nInternational Journal of Data Science and Analytics (2024) 18:69–81 77\nIn particular, a dataset of 50 surgical sentences was col-\nlected from different sources, i.e., surgical books, academic\npapers, and web pages not used during the MLM training.\nThe sentences were randomly chosen from those that met the\nfollowing requirements:\n• The sentence has not been used to train SurgicBERTa;\n• One of the following holds:\n– The sentence contains an expression commonly used\nin surgery. To deﬁne widely used expressions, we\nhave selected those typically abbreviated with an\nacronym in papers. In the sentences included in the\ndataset, the abbreviations have been substituted with\nthe original expression, and the language models are\nasked to complete them correctly in the correspond-\ning context;\n– The sentence contains a description of a surgical pro-\ncedure. In the sentences inserted in the dataset, the\nverb describing the action is masked, and the lan-\nguage model is asked to guess it based on the context.\nSince the task is the same as the previous one, we used\nthe same metrics adopted for it, i.e., the position in which the\ncorrect solution is proposed, the RR and the MRR. We also\napplied the same statistical signiﬁcance test.\n4.5.2 Results and discussion\nTable4 summarizes the obtained results for the task described\nin Sect. 4.5. SurgicBERTa substantially improves all pro-\nposed metrics: the mean position at which the word ﬁlling\ncorrectly the masks is proposed by SurgicBERTa among\nthe list of returned ones is 19 .19 times better than the\nRoBERTa one. This means SurgicBERTa is much more\nfamiliar with surgical terminology than RoBERTa. Conse-\nquently, the MRR is improved by 0 .396. 66% of the times\nSurgicBERTa improves the RRs when compared to Ro-\nBERTa. Only in two cases (out of 50) RoBERTa performs\nbetter than SurgicBERTa: similarly to task C, it is difﬁcult\nto understand why this happens, and the same considera-\ntions may apply. The violin plots of Fig. 3 illustrate the RRs\nof the two language models for each query: while the one\nfor SurgicBERTa is wide at the top, the one for RoBERTa\nis wide at the bottom. Furthermore, SurgicBERTa has a\nmedian weight much higher than that of RoBERTa.T h i s\nhighlights the best accuracy of SurgicBERTa in managing\nsurgical terminology, also conﬁrmed by the signiﬁcance test\nperformed ( p value < 0.05). Hence, also this task conﬁrms\nthat SurgicBERTa better captures the surgical language.\nTable 4 Mean position and MRR on the task of surgical terminology\nacquisition (Extrinsic Evaluation—Task D)\nPre-trained model Mean position MRR\nRoBERTa 152.720 0.262\nSurgicBERTa 7.960 0 .658\nFig. 3 Reciprocal rank of the predicted word in the task of surgical\nterminology acquisition (Extrinsic Evaluation—Task D)\n4.6 Qualitative examples of surgical knowledge\navailable in pre-trained language models\nThere is a lot of domain information implicit in pre-trained\nlanguage models [ 24]. Adapting the domain through con-\ntinual learning with MLM helps to capture this kind of\nknowledge. However, it is complicated to quantify this\ndomain knowledge objectively and exhaustively due to the\nlack of any gold standard for the surgical domain. For this\nreason, this section proposes a qualitative analysis, provid-\ning examples of domain information stored in pre-trained\nlanguage models.\nTo start with, RoBERTa and SurgicBERTa are asked\nto return the name of the most used surgical robot in the\noperating room. In particular, RoBERTa and SurgicBERTa\nare asked to substitute the ⟨mask ⟩ in the following sentence\nwith the most appropriate ﬁve words, ranking them in order\nof probability:\nThe most commonly used surgical robot is ⟨mask ⟩ .\nResults are reported in Table 5. While to the best of our\nknowledge, none of the top ﬁve words returned by RoBERTa\n123\n78 International Journal of Data Science and Analytics (2024) 18:69–81\nTable 5 RoBERTa and SurgicBERTa most probable words for the\nmost used surgical robots\nRoBERTa SurgicBERTa\nRank Word Probability Word Probability\n1 Braun 0.031 Zeus 0.261\n2 Juno 0.027 Xi 0.111\n3 Hawk 0.017 Si 0.055\n4 Orion 0.016 robotic 0.035\n5 MRI 0.016 S 0.030\nFig. 4 Illustration of the critical view of safety method during a chole-\ncystectomy\nis the name of a surgical robot, Zeus,7 Xi,8 and Si9 returned\nby SurgicBERTa are instead examples of surgical robots\nthat have been used in operating theaters. This means that\nthe continual MLM learning with domain text has captured\nthis kind of information that now is available in the model.\nNonetheless, it is interesting to note how some of the words\nreturned by RoBERTa are sometimes related to the robotics\nﬁeld: “Hawk,” “Orion,” “Juno” are also examples of (non-\nsurgical) robots. This observation may suggest that while the\ngeneral model tries to be correct, it lacks speciﬁc domain\nknowledge.\nAs reported in Table 1, SurgicBERTa has a perplexity\nsubstantially lower than RoBERTa in the MLM task when\napplied to surgical literature. This intrinsically means that\nSurgicBERTa has learned the surgical language and thus\nalso the composition of well-known surgical expressions.\nConsider the following example highlights how Surgic-\nBERTa has learned specialized domain terminology. In\nsurgery, the expression critical view of safety refers to a\n7 https://en.wikipedia.org/wiki/ZEUS_robotic_surgical_system.\n8 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6193435/.\n9 https://www.davincisurgerycommunity.com/Systems_I_A/\nda_Vinci_Si_Si_e.\nFig. 5 Pfannenstiel incision to access the abdomen. This ﬁgure is\nadapted from [ 13]\nmethod of secure identiﬁcation in open cholecystectomy in\nwhich the cystic duct and artery are putatively identiﬁed, after\nwhich the gallbladder is taken off the cystic plate so that the\ngallbladder is attached only by the two cystic structures [ 32]\na ss h o w nb yF i g .4.\nTo verify if RoBERTa and SurgicBERTa know this\ninformation, they are asked to complete the following sen-\ntence:\nDuring cholecystectomy, it is important to achieve the\ncritical view of ⟨mask ⟩ .\nSurgicBERTa returns the word safety as 1 st result with\na probability of 0 .3428, while RoBERTa returns it only at\n47th position with the probability of 0 .0032.\nThis section ends with another example of domain knowl-\nedge available in SurgicBERTa. In surgery, a Pfannenstiel\nincision is a type of surgical incision that allows access to the\nabdomen (Fig. 5). The following test wants to investigate if\npre-trained language models know this information:\nThe Pfannenstiel is a type of surgical incision that\nallows access to the ⟨mask ⟩ .\nThe correct word is abdomen and is retrieved by Surgic-\nBERTa at the 1 st position with probability 0 .1267 and by\nRoBERTa at the 5 th position with probability 0 .0478, after\nthe words brain (0.1969), heart (0.1488), skin (0.0713), and\nvagina (0.0542).\nThese qualitative examples show that in SurgicBERTa\nthere is a lot of surgical information that could be used,\nfor instance, to enrich and complement the one codiﬁed in\ndomain ontologies and knowledge bases.\nNevertheless, since the model was ﬁne-tuned on the MLM\ntask on surgical domain texts, SurgicBERTa could also\nsuffer from the problems that the models thus generated typ-\nically have. Among all, we underline the frequent risk of\n123\nInternational Journal of Data Science and Analytics (2024) 18:69–81 79\nintroducing bias into the models which in the case of a sur-\ngical model could be that of making predictions of words\nalways considering a standard human anatomy, ignoring all\npossible particular cases. Also, SurgicBERTa was obtained\nby specializing RoBERTa on the surgical case, so some of\nthe known biases of the latter are likely to be replicated on\nSurgicBERTa as well. All of these problems can be reduced\nby choosing better training materials or adapting de-biasing\ntechniques to the domain. Furthermore, the relevance of the\nreturned word could be low in domains not seen (enough)\nduring the training: using reinforcement learning with human\nfeedback techniques [ 22] could help to reduce these prob-\nlems.\n5 Conclusions\nThis paper proposed SurgicBERTa, a pre-trained language\nﬁne-tuned for capturing surgical language and knowledge,\ni.e., the vocabulary and expertise provided in surgical books\nand academic papers.\nThe building process has been described, and the model\nhas been evaluated both intrinsically, by considering per-\nplexity, accuracy, and evaluation loss during the MLM task,\nand extrinsically, by considering several downstream tasks,\nnamely (i) procedural sentences detection, (ii) procedural\nknowledge extraction, (iii) ontological information discov-\nery, and (iv) surgical terminology learning. All the results\nconﬁrm that SurgicBERTa deals with surgical language\nand knowledge more adequately than RoBERTa, a lan-\nguage model targeting general-domain English. Moreover,\nthe potential of SurgicBERTa has been investigated qual-\nitatively by showing several examples of surgical domain\nknowledge available in the model, which could be used to\ncomplement other knowledge sources, e.g., state-of-the-art\nsurgical knowledge bases. As future works, we will enrich\nSurgicBERTa by continuously training it on a larger surgi-\ncal dataset and extending it in a multilingual scenario.\nAcknowledgements This project has received funding from the Euro-\npean Research Council (ERC) under the European Union’s Horizon\n2020 research and innovation program (grant agreement No. 742671\n“ARS”).\nAuthor Contributions “MB, MR, SPP , and PF contributed in the fol-\nlowing way:—Conceptualization: MB, MR, SPP—Methodology: MB,\nMR, SPP—Supervision: MR, SPP , PF—Funding acquisition: PF—\nWriting—original draft: MB–Writing—review and editing: MB, MR,\nSPP , PF.\nFunding Open access funding provided by Universitá degli Studi di\nV erona within the CRUI-CARE Agreement.\nData availability The SurgicBERTa language model together with the\nresources used in our experiments is available under an open license at\nhttps://gitlab.com/altairLab/surgicberta.\nDeclarations\nConﬂict of interest Marco Bombieri, Marco Rospocher, Simone Paolo\nPonzetto, and Paolo Fiorini declare that they do not have conﬂicts of\ninterest.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\n1. Bear Don’t Walk, I.V .O.J., Sun, T., Perotte, A., et al.: Clinically\nrelevant pretraining is all you need. J. Am. Med. Inform. Assoc.\n28(9), 1970–1976 (2021)\n2. Bombieri, M., Rospocher, M., Dall’Alba, D., et al.: Automatic\ndetection of procedural knowledge in robotic-assisted surgical\ntexts. Int. J. Comput. Assist. Radiol. Surg. 16(8), 1287–1295\n(2021). https://doi.org/10.1007/s11548-021-02370-9\n3. Bombieri, M., Rospocher, M., Ponzetto, S.P ., et al.: The robotic\nsurgery procedural framebank. In: Proceedings of the Lan-\nguage Resources and Evaluation Conference. European Language\nResources Association, Marseille, France, pp. 3950–3959 (2022).\nhttps://aclanthology.org/2022.lrec-1.420\n4. Bombieri, M., Rospocher, M., Ponzetto, S.P ., et al.: Machine\nunderstanding surgical actions from intervention procedure text-\nbooks. Comput. Biol. Med. (2023). https://doi.org/10.1016/j.\ncompbiomed.2022.106415\n5. Bombieri, M., Rospocher, M., Ponzetto, S.P ., et al.: The robotic-\nsurgery propositional bank. Lang. Resour. Evaluation (2023).\nhttps://doi.org/10.1007/s10579-023-09668-x\n6. Carrino, C.P ., Llop, J., Pàmies, M., et al.: Pretrained biomedical\nlanguage models for clinical NLP in Spanish. In: Proceedings of\nthe 21st Workshop on Biomedical Language Processing. Associa-\ntion for Computational Linguistics, Dublin, Ireland, pp. 193–199\n(2022). https://doi.org/10.18653/v1/2022.bionlp-1.19\n7. Chandak, S., Zhang, L., Brown, C., et al.: Towards automatic cura-\ntion of antibiotic resistance genes via statement extraction from\nscientiﬁc papers: A benchmark dataset and models. In: Proceed-\nings of the 21st Workshop on Biomedical Language Processing.\nAssociation for Computational Linguistics, Dublin, Ireland (2022)\n8. Devlin, J., Chang, M., Lee, K., et al.: BERT: pre-training of\ndeep bidirectional transformers for language understanding. In:\nBurstein, J., Doran, C., Solorio, T. (eds.) Proceedings of the 2019\nConference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2–7, 2019, V ol-\nume 1 (Long and Short Papers). Association for Computational\n123\n80 International Journal of Data Science and Analytics (2024) 18:69–81\nLinguistics, pp. 4171–4186 (2019). https://doi.org/10.18653/v1/\nn19-1423,\n9. Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al.: An image is worth\n16 x 16 words: transformers for image recognition at scale. In:\n9th International Conference on Learning Representations, ICLR\n2021, Virtual Event, Austria, May 3–7, 2021. OpenReview.net\n(2021). https://openreview.net/forum?id=YicbFdNTTy\n10. Dror, R., Baumer, G., Shlomov, S., et al.: The hitchhiker’s guide\nto testing statistical signiﬁcance in natural language processing.\nIn: Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (V olume 1: Long Papers). Association\nfor Computational Linguistics, Melbourne, Australia, pp. 1383–\n1392 (2018) https://doi.org/10.18653/v1/P18-1128\n11. He, L., Lee, K., Lewis, M., et al.: Deep semantic role labeling: What\nworks and what’s next. In: Barzilay, R., Kan, M. (eds.) Proceedings\nof the 55th Annual Meeting of the Association for Computational\nLinguistics, ACL 2017, V ancouver, Canada, July 30 - August 4,\nV olume 1: Long Papers. Association for Computational Linguis-\ntics, pp. 473–483 (2017). https://doi.org/10.18653/v1/P17-1044\n12. Hirschberg, J., Manning, C.D.: Advances in natural language pro-\ncessing. Science 349(6245), 261–266 (2015). https://doi.org/10.\n1126/science.aaa8685\n13. Jeelani, K.: Surgical Anatomy of the Female Pelvis and Abdominal\nWall, pp. 8–14. Cambridge University Press, Cambridge (2020).\nhttps://doi.org/10.1017/9781108644396.002\n14. Johnson, A., Pollard, T., Shen, L., et al.: Mimic-iii, a freely acces-\nsible critical care database. Sci. Data 3(160), 035 (2016). https://\ndoi.org/10.1038/sdata.2016.35\n15. Lauscher, A., Ravishankar, V ., Vuli´ c, I., et al.: From zero to hero:\nOn the limitations of zero-shot cross-lingual transfer with multilin-\ngual transformers (2020). https://doi.org/10.48550/ARXIV .2005.\n00633\n16. Lewis, M., Liu, Y ., Goyal, N., et al.: Bart: denoising sequence-to-\nsequence pre-training for natural language generation, translation,\nand comprehension (2019). arXiv preprint arXiv:1910.13461\n17. Li, T., Jawale, P .A., Palmer, M., et al.: Structured tuning for seman-\ntic role labeling. In: Jurafsky, D., Chai, J., Schluter, N., et al. (eds.)\nProceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5–10, 2020.\nAssociation for Computational Linguistics, pp. 8402–8412 (2020)\nhttps://doi.org/10.18653/v1/2020.acl-main.744\n18. Liang, Z., Noriega-Atala, E., Morrison, C., et al.: Low resource\ncausal event detection from biomedical literature. In: Proceedings\nof the 21st Workshop on Biomedical Language Processing. Asso-\nciation for Computational Linguistics, Dublin, Ireland (2022)\n19. Lin, C., Zheng, S., Liu, Z., et al.: SGT: scene graph-guided trans-\nformer for surgical report generation. In: Wang, L., Dou, Q.,\nFletcher, P .T., et al. (eds.) Medical Image Computing and Com-\nputer Assisted Intervention - MICCAI 2022 - 25th International\nConference, Singapore, September 18–22, 2022, Proceedings, Part\nVII, Lecture Notes in Computer Science, vol. 13437, pp. 507–518.\nSpringer (2022) https://doi.org/10.1007/978-3-031-16449-1_48\n20. Liu, Y ., Ott, M., Goyal, N., et al.: Roberta: A robustly optimized\nBERT pretraining approach. CoRR. arXiv: org/abs/1907.11692 ,\n(2019)\n21. Locke, S., Bashall, A., Al-Adely, S., et al.: Natural language pro-\ncessing in medicine: a review. Trends Anaesth. Crit. Care 38, 4–9\n(2021)\n22. Osborne, P ., Nõmm, H., Freitas, A.: A survey of text games for rein-\nforcement learning informed by natural language. Trans. Assoc.\nComput. Linguistics 10, 873–887 (2022)\n23. Palmer, M., Kingsbury, P .R., Gildea, D.: The proposition bank:\nan annotated corpus of semantic roles. Comput. Linguistics 31(1),\n71–106 (2005). https://doi.org/10.1162/0891201053630264\n24. Petroni, F., Rocktäschel, T., Riedel, S., et al.: Language models\nas knowledge bases? In: Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP). Association for Computational Linguistics,\nHong Kong, China, pp. 2463–2473 (2019). https://doi.org/10.\n18653/v1/D19-1250\n25. Pradhan, S., Moschitti, A., Xue, N., et al.: Towards robust lin-\nguistic analysis using ontonotes. In: Hockenmaier, J., Riedel, S.\n(eds.) Proceedings of the Seventeenth Conference on Computa-\ntional Natural Language Learning, CoNLL 2013, Soﬁa, Bulgaria,\nAugust 8–9, 2013. ACL, pp. 143–152 (2013). https://aclanthology.\norg/W13-3516/\n26. Qian, C., Wen, L., Kumar, A., et al.: An approach for process\nmodel extraction by multi-grained text classiﬁcation. Lecture Notes\nin Computer Science (including subseries Lecture Notes in Arti-\nﬁcial Intelligence and Lecture Notes in Bioinformatics) 12127\nLNCS:268–282 (2020)\n27. Salazar, J., Liang, D., Nguyen, T.Q., et al.: Masked language\nmodel scoring. In: Jurafsky, D., Chai, J., Schluter, N., et al. (eds.)\nProceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5–10, 2020.\nAssociation for Computational Linguistics, pp. 2699–2712 (2020)\nhttps://doi.org/10.18653/v1/2020.acl-main.240\n28. Seenivasan, L., Islam, M., Krishna, A.K., et al.: Surgical-vqa: visual\nquestion answering in surgical scenes using transformer. In: Wang,\nL., Dou, Q., Fletcher, P .T., et al. (eds.) Medical Image Comput-\ning and Computer Assisted Intervention - MICCAI 2022 - 25th\nInternational Conference, Singapore, September 18-22, 2022, Pro-\nceedings, Part VII, Lecture Notes in Computer Science, vol. 13437,\npp. 33–43 . Springer (2022). https://doi.org/10.1007/978-3-031-\n16449-1_4\n29. Shin, J., Lee, Y ., Jung, K.: Effective sentence scoring method\nusing bert for speech recognition. In: Lee, W.S., Suzuki, T. (eds.)\nProceedings of The Eleventh Asian Conference on Machine Learn-\ning. Proceedings of Machine Learning Research, PMLR, vol.\n101, pp. 1081–1093 (2019). https://proceedings.mlr.press/v101/\nshin19a.html\n30. Singhal, K., Azizi, S., Tu, T., et al.: (2022) Large language mod-\nels encode clinical knowledge. https://doi.org/10.48550/ARXIV .\n2212.13138\n31. Sohn, S., Wang, Y ., Wi, C.I., et al.: Clinical documentation vari-\nations and NLP system portability: a case study in asthma birth\ncohorts across institutions. J. Am. Med. Inform. Assoc. 25(3), 353–\n359 (2017)\n32. Strasberg, S., Hertl, M., Soper, N.: An analysis of the problem of\nbiliary injury during laparoscopic cholecystectomy. Surg. Gynecol.\nObstet. 180(1), 101–125 (1995)\n33. Taylor, W.L.: Cloze procedure: a new tool for measuring read-\nability. J. Q.\n30(4), 415–433 (1953). https://doi.org/10.1177/\n107769905303000401\n34. V aswani,A., Shazeer, N., Parmar, N., et al.: Attention is all\nyou need. In: Guyon, I., von Luxburg, U., Bengio, S., et al.\n(eds.) Advances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Processing Sys-\ntems 2017, December 4–9, 2017, Long Beach, CA, USA,\npp. 5998–6008 (2017) https://proceedings.neurips.cc/paper/2017/\nhash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n35. V oorhees, E.M.: The TREC-8 question answering track report. In:\nV oorhees, E.M., Harman, D.K. (eds.) Proceedings of The Eighth\nText REtrieval Conference, TREC 1999, Gaithersburg, Maryland,\nUSA, November 17–19, 1999, NIST Special Publication, vol. 500–\n246. National Institute of Standards and Technology (NIST), http://\ntrec.nist.gov/pubs/trec8/papers/qa_report.pdf (1999)\n36. Wang, A., Cho, K.: BERT has a mouth, and it must speak: BERT\nas a Markov random ﬁeld language model. In: Proceedings of\nthe Workshop on Methods for Optimizing and Evaluating Neural\nLanguage Generation. Association for Computational Linguis-\n123\nInternational Journal of Data Science and Analytics (2024) 18:69–81 81\ntics, Minneapolis, Minnesota, pp. 30–36 (2019). https://doi.org/\n10.18653/v1/W19-2304\n37. Xie, K., Gallagher, R.S., Conrad, E.C., et al.: Extracting seizure\nfrequency from epilepsy clinic notes: a machine reading approach\nto natural language processing. J. Am. Med. Inform. Assoc. 29(5),\n873–881 (2022)\n38. Xu, M., Islam, M., Lim, C.M., et al.: Class-incremental domain\nadaptation with smoothing and calibration for surgical report gener-\nation. In: de Bruijne, M., Cattin, P .C., Cotin, S., et al. (eds.) Medical\nImage Computing and Computer Assisted Intervention - MICCAI\n2021 - 24th International Conference, Strasbourg, France, Septem-\nber 27 - October 1, 2021, Proceedings, Part IV , Lecture Notes in\nComputer Science, vol. 12904, pp. 269–278. Springer (2021a).\nhttps://doi.org/10.1007/978-3-030-87202-1_26\n39. Xu, M., Islam, M., Ming Lim, C., et al.: Learning domain adaptation\nwith model calibration for surgical report generation in robotic\nsurgery. In: 2021 IEEE International Conference on Robotics and\nAutomation (ICRA), pp. 12350–12356 (2021b). https://doi.org/10.\n1109/ICRA48506.2021.9561569\n40. Xu, M., Islam, M., Ren, H.: Rethinking surgical captioning: End-\nto-end window-based MLP transformer using patches. In: Wang,\nL., Dou, Q., Fletcher, P .T., et al. (eds.) Medical Image Comput-\ning and Computer Assisted Intervention - MICCAI 2022 - 25th\nInternational Conference, Singapore, September 18–22, 2022, Pro-\nceedings, Part VII, Lecture Notes in Computer Science, vol. 13437,\npp. 376–386. Springer (2022). https://doi.org/10.1007/978-3-031-\n16449-1_36\n41. Yang, X., Bian, J., Hogan, W.R., et al.: Clinical concept extraction\nusing transformers. J. Am. Med. Inform. Assoc. 27(12), 1935–1942\n(2020)\n42. Yang, X., Chen, A., PourNejatian, N., et al.: A large language model\nfor electronic health records. npj Digit. Med. 5(1), 194 (2022).\nhttps://doi.org/10.1038/s41746-022-00742-2\n43. Yao, L., Jin, Z., Mao, C., et al.: Traditional Chinese medicine clini-\ncal records classiﬁcation with BERT and domain speciﬁc corpora.\nJ. Am. Med. Inform. Assoc. 26(12), 1632–1636 (2019)\n44. Zhou, S., Wang, N., Wang, L., et al.: CancerBERT: a cancer\ndomain-speciﬁc language model for extracting breast cancer phe-\nnotypes from electronic health records. J. Am. Med. Inform. Assoc.\n(2022)\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123"
}