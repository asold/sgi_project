{
  "title": "Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain",
  "url": "https://openalex.org/W4399702760",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2979755547",
      "name": "Fabio Dennstädt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2510676551",
      "name": "Johannes Zink",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1947475651",
      "name": "Paul-Martin Putora",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2251524424",
      "name": "Janna Hastings",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979772484",
      "name": "Nikola Cihoric",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4200398992",
    "https://openalex.org/W3040258807",
    "https://openalex.org/W3003735349",
    "https://openalex.org/W3165135368",
    "https://openalex.org/W3111278950",
    "https://openalex.org/W4200362342",
    "https://openalex.org/W2999783216",
    "https://openalex.org/W3164863290",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4366823941",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W2529878079",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4389917881",
    "https://openalex.org/W4389898530",
    "https://openalex.org/W4361204756",
    "https://openalex.org/W2949422680",
    "https://openalex.org/W2804868354",
    "https://openalex.org/W3187789458",
    "https://openalex.org/W3081220506",
    "https://openalex.org/W3036575216",
    "https://openalex.org/W4200279042",
    "https://openalex.org/W3048529076",
    "https://openalex.org/W2885938281",
    "https://openalex.org/W2783658081",
    "https://openalex.org/W2790981410",
    "https://openalex.org/W3004612364",
    "https://openalex.org/W4313446661",
    "https://openalex.org/W2961191798",
    "https://openalex.org/W2043566294",
    "https://openalex.org/W2752764465",
    "https://openalex.org/W2885869279",
    "https://openalex.org/W4376619531",
    "https://openalex.org/W4323347493",
    "https://openalex.org/W4384641573",
    "https://openalex.org/W4313262066",
    "https://openalex.org/W4390814081",
    "https://openalex.org/W4389952972",
    "https://openalex.org/W4389520065",
    "https://openalex.org/W4383092930",
    "https://openalex.org/W4313592949",
    "https://openalex.org/W4385454937",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4387440823",
    "https://openalex.org/W3029811621",
    "https://openalex.org/W4389671813",
    "https://openalex.org/W4387144848",
    "https://openalex.org/W4385571451",
    "https://openalex.org/W4386120650"
  ],
  "abstract": "Abstract Background Systematically screening published literature to determine the relevant publications to synthesize in a review is a time-consuming and difficult task. Large language models (LLMs) are an emerging technology with promising capabilities for the automation of language-related tasks that may be useful for such a purpose. Methods LLMs were used as part of an automated system to evaluate the relevance of publications to a certain topic based on defined criteria and based on the title and abstract of each publication. A Python script was created to generate structured prompts consisting of text strings for instruction, title, abstract, and relevant criteria to be provided to an LLM. The relevance of a publication was evaluated by the LLM on a Likert scale (low relevance to high relevance). By specifying a threshold, different classifiers for inclusion/exclusion of publications could then be defined. The approach was used with four different openly available LLMs on ten published data sets of biomedical literature reviews and on a newly human-created data set for a hypothetical new systematic literature review. Results The performance of the classifiers varied depending on the LLM being used and on the data set analyzed. Regarding sensitivity/specificity, the classifiers yielded 94.48%/31.78% for the FlanT5 model, 97.58%/19.12% for the OpenHermes-NeuralChat model, 81.93%/75.19% for the Mixtral model and 97.58%/38.34% for the Platypus 2 model on the ten published data sets. The same classifiers yielded 100% sensitivity at a specificity of 12.58%, 4.54%, 62.47%, and 24.74% on the newly created data set. Changing the standard settings of the approach (minor adaption of instruction prompt and/or changing the range of the Likert scale from 1–5 to 1–10) had a considerable impact on the performance. Conclusions LLMs can be used to evaluate the relevance of scientific publications to a certain review topic and classifiers based on such an approach show some promising results. To date, little is known about how well such systems would perform if used prospectively when conducting systematic literature reviews and what further implications this might have. However, it is likely that in the future researchers will increasingly use LLMs for evaluating and classifying scientific publications.",
  "full_text": "Dennstädt et al. Systematic Reviews          (2024) 13:158  \nhttps://doi.org/10.1186/s13643-024-02575-4\nRESEARCH Open Access\n© The Author(s) 2024. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecom-\nmons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nSystematic Reviews\nTitle and abstract screening for literature \nreviews using large language models: \nan exploratory study in the biomedical domain\nFabio Dennstädt1,3*  , Johannes Zink2, Paul Martin Putora1,3, Janna Hastings4,5,6 and Nikola Cihoric3 \nAbstract \nBackground Systematically screening published literature to determine the relevant publications to synthesize \nin a review is a time-consuming and difficult task. Large language models (LLMs) are an emerging technology \nwith promising capabilities for the automation of language-related tasks that may be useful for such a purpose.\nMethods LLMs were used as part of an automated system to evaluate the relevance of publications to a certain \ntopic based on defined criteria and based on the title and abstract of each publication. A Python script was created \nto generate structured prompts consisting of text strings for instruction, title, abstract, and relevant criteria to be \nprovided to an LLM. The relevance of a publication was evaluated by the LLM on a Likert scale (low relevance to high \nrelevance). By specifying a threshold, different classifiers for inclusion/exclusion of publications could then be defined. \nThe approach was used with four different openly available LLMs on ten published data sets of biomedical literature \nreviews and on a newly human-created data set for a hypothetical new systematic literature review.\nResults The performance of the classifiers varied depending on the LLM being used and on the data set ana-\nlyzed. Regarding sensitivity/specificity, the classifiers yielded 94.48%/31.78% for the FlanT5 model, 97.58%/19.12% \nfor the OpenHermes-NeuralChat model, 81.93%/75.19% for the Mixtral model and 97.58%/38.34% for the Platypus 2 \nmodel on the ten published data sets. The same classifiers yielded 100% sensitivity at a specificity of 12.58%, 4.54%, \n62.47%, and 24.74% on the newly created data set. Changing the standard settings of the approach (minor adaption \nof instruction prompt and/or changing the range of the Likert scale from 1–5 to 1–10) had a considerable impact \non the performance.\nConclusions LLMs can be used to evaluate the relevance of scientific publications to a certain review topic and clas-\nsifiers based on such an approach show some promising results. To date, little is known about how well such systems \nwould perform if used prospectively when conducting systematic literature reviews and what further implications this \nmight have. However, it is likely that in the future researchers will increasingly use LLMs for evaluating and classifying \nscientific publications.\nKeywords Natural language processing, Systematic literature review, Biomedicine, Title and abstract screening, Large \nlanguage models\n*Correspondence:\nFabio Dennstädt\nfabiodennstaedt@gmx.de\nFull list of author information is available at the end of the article\nPage 2 of 14Dennstädt et al. Systematic Reviews          (2024) 13:158 \nBackground\nSystematic literature reviews (SLRs) summarize knowl -\nedge about a specific topic and are an essential ingredient \nfor evidence-based medicine. Performing an SLR involves \na lot of effort, as it requires researchers to identify, filter, \nand analyze substantial quantities of literature. Typically, \nthe most relevant out of thousands of publications need \nto be identified for the topic and key information needs \nto be extracted for the synthesis. Some estimates indicate \nthat systematic reviews typically take several months to \ncomplete [1, 2], which is why the latest evidence may not \nalways be taken into consideration.\nTitle and abstract screening forms a considerable part \nof the systematic reviewing workload. In this step, which \ntypically follows defining a search strategy and precedes \nthe full-text screening of a smaller number of search \nresults, researchers determine whether a certain publi -\ncation is relevant for inclusion in the systematic review \nbased on title and abstract. Automating title and abstract \nscreening has the potential to save time and thereby \naccelerate the translation of evidence into practice. It may \nalso make the reviewing methodology more consistent \nand reproducible. Thus, the automation or semi-automa -\ntion of this part of the reviewing workflow has been of \nlongstanding interest [3–5].\nSeveral approaches have been developed that use \nmachine learning (ML) to automate or semi-automate \nscreening [1, 6]. For example, systematic review software \napplications such as Covidence [7] and EPPI-Reviewer \n[8] (which use the same algorithm) offer ML-assisted \nranking algorithms that aim to show the most relevant \npublications for the search criteria higher in the review -\ning to speed up the manual review process. Elicit [9] is \na standalone literature discovery tool that also offers an \nML-assisted literature search facility. Furthermore, sev -\neral dedicated tools have been developed to specifically \nautomate title and abstract screening [1, 10]. Examples \ninclude Rayyan [11], DistillerSR [12], Abstrackr [13], \nRobotAnalyst [14], and ASReview [5]. These tools typi -\ncally work via different technical strategies drawn from \nML and topic modeling to enable the system to learn how \nsimilar new articles are to a core set of identified ‘good’ \nresults for the topic. These approaches have been found \nto lead to a considerable reduction in the time taken to \ncomplete systematic reviews [15].\nMost of these systems require some sort of pre-selec -\ntion or specific training for the larger corpus of publica -\ntions to be analyzed (e.g., identification of some “relevant” \npublications by a human so that the algorithm can select \nsimilar papers) and are thus not fully automated.\nFurthermore, dedicated models are required that are \nbuilt for the specific purpose together with appropriate \ntraining data. Fully automated systems that achieve \nhigh levels of performance and can be flexibly applied \nto various topics have not yet been realized.\nLarge language models (LLMs) are an approach to \nnatural language processing in which very large-scale \nneural networks are trained on vast amounts of tex -\ntual data to generate sequences of words in response to \ninput text. These capable models are then subject to dif -\nferent strategies for additional training to improve their \nperformance on a wide range of tasks. Recent techno -\nlogical advancements in model size, architecture, and \ntraining strategies have led to general-purpose dialog \nLLMs achieving and exceeding state-of-the-art perfor -\nmance on many benchmark tasks including medical \nquestion answering [16] and text summarization [17].\nRecent progress in the development of LLMs led to \nvery capable models. While models developed by pri -\nvate companies such as GPT-3/GPT-3.5/GPT-4 from \nOpenAI [18] or PaLM and Gemini from Google [19, 20] \nare among the most powerful LLMs currently available, \nopenly available models are actively being developed by \ndifferent stakeholders and in some cases achieve per -\nformances not far from the state of the art [21].\nLLMs have shown remarkable capabilities in a vari -\nety of subjects and tasks that would require a profound \nunderstanding of text and knowledge for a human to \nperform. Among others, LLMs can be used for classifi -\ncation [22], information extraction [23], and knowledge \naccess [24]. Furthermore, they can be flexibly adapted \nvia prompt engineering techniques [25] and parameter \nsettings, to behave in a desired way. At the same time, \nconsiderable problems with the usage of LLMs such as \n“hallucinations” of models [26], inherent biases [27, 28], \nand weak alignment with human evaluation [29] have \nbeen described. Therefore, even though the text output \ngenerated by LLMs is based on objective statistical cal -\nculations, the text output itself is not necessarily factual \nand correct and furthermore incorporates subjectivity \nbased on the training data. This implies, that an LLM-\nbased evaluation system has a priori some fundamental \nlimitations. However, using LLMs for evaluating sci -\nentific publications is a novel and interesting approach \nthat may be helpful in creating fully automated and still \nflexible systems for screening and evaluating scientific \nliterature.\nTo investigate whether and how well openly avail -\nable LLMs can be used for evaluating the relevance of \npublications as part of an automated title and abstract \nscreening system, we conducted a study to evaluate the \nperformance of such an approach in the biomedical \ndomain with modern openly available LLMs.\nPage 3 of 14\nDennstädt et al. Systematic Reviews          (2024) 13:158 \n \nMethods\nUsing LLMs for title and abstract screening\nWe designed an approach for evaluating the relevance of \npublications based on title and abstract using an LLM. \nThis approach is based on the following strategy:\n• An instruction prompt to evaluate the relevance of \na scientific publication for inclusion into an SLR is \ngiven to an LLM.\n• The prompt includes the title and abstract of the pub-\nlication and the criteria that are considered relevant.\n• The prompt furthermore includes the request to \nreturn just a number as an answer, which corre -\nsponds to the relevance of the publication on a Likert \nscale (“not relevant” to “highly relevant”).\n• The prompt for each publication is created in a struc-\ntured and automated way.\n• A numeric threshold may be defined which separates \nrelevant publications from irrelevant publications \n(corresponding to the definition of a classifier).\nThe prompts are created in the following way:\nPrompt = [Instruction] + [Title of publication] + [Abstract \nof publication] + [Relevant Criteria].\n(“ + ” is not part of the final prompt but indicates the \nmerge of the text strings).\n[Instruction] is the text string describing the general \ninstruction for the LLM to evaluate the publication. The \nLLM is asked to evaluate the relevance of a publication \nfor an SLR on a numeric scale (low relevance to high rel -\nevance) based on the title and abstract of the publication \nand based on defined relevant criteria.\n[Title of publication] is the text string “Title:” together \nwith the title of the publication.\n[Abstract of publication] is the text string “, Abstract:” \ntogether with the abstract of the publication.\n[Relevant Criteria] is the text that describes the cri -\nteria to evaluate the relevance of a publication. The rel -\nevant criteria are defined beforehand by the researchers \ndepending on the topic to determine which publications \nare relevant. The [Relevant Criteria] text string remains \nunchanged for all the publications that should be checked \nfor relevance.\nThe answer to the LLM usually consists just of a digit \non a numeric scale (e.g., 1–5). However, variations are \nacceptable if the answer can unambiguously be assigned \nto one of the possible scores on the Likert scale (e.g., the \nanswer “The relevance of the publication is 3. ” can unam-\nbiguously be assigned to the score 3). This assignment of \nanswers to a score can be automated with a string-search \ncommand, meaning a simple regular expression com -\nmand searching for a positive integer number, which will \nbe extracted from the text string.\nA request is sent to the LLM for each publication in the \ncorpus. In cases for which an LLM provided an invalid \n(unprocessable) response for a publication, that response \nwas excluded from the direct downstream analysis. It was \ndetermined for how many publications invalid responses \nwere given and how many of these publications would \nhave been relevant.\nA schematic illustration of the approach is shown in \nFig.  1. An example of a prompt is provided in Supple -\nmentary material 1: Appendix 1.\nA Python script was created to automate the process \nand to apply it to a data set with a collection of different \npublications.\nWith the publications being sorted into different rel -\nevance groups, a threshold can be defined, which is used \nby a classifier to separate relevant from irrelevant publi -\ncations. For example, a 3 + classifier would classify pub -\nlications with a score of ≥ 3 as relevant, and publications \nwith a score < 3 as irrelevant.\nEvaluation\nThe performance of the approach was tested with dif -\nferent LLMs, data sets and settings as described in the \nfollowing:\nLanguage models\nA variety of different models were tested. To investigate \nthe approach with different LLMs (that are also diverse \nregarding design and training data), the following four \nmodels were used in the experiments:\n• FlanT5-XXL (FlanT5) is an LLM developed by \nGoogle Research. It’s a variant of the T5 (text-to-text) \nmodel, that utilizes a unified text-to-text framework \nallowing it to perform a wide range of NLP tasks \nwith the same model architecture, loss function, \nand hyperparameters. FlanT5 is a variant that was \nenhanced through fine-tuning over a thousand addi -\ntional tasks and supporting more languages. It is pri -\nmarily used for research in various areas of natural \nlanguage processing, such as reasoning and question-\nanswering [30, 31].\n• OpenHermes-2.5-neural-chat-7b-v3-1-7B (OHNC) \n[32] is a powerful open-source LLM, which was \nmerged from the two models OpenHermes 2.5 Mis -\ntral 7B [33] and Neural-Chat (neural-chat-7b-v3-1) \n[34]. Despite having only 7 billion parameters it per -\nforms better than some larger models on various \nbenchmarks.\n• Mixtral-8 × 7B-Instruct v0.1 (Mixtral) is a pretrained \ngenerative Sparse Mixture of Experts LLM developed \nby Mistral AI [35, 36]. It was reported to outperform \nPage 4 of 14Dennstädt et al. Systematic Reviews          (2024) 13:158 \npowerful models like gpt-3.5-turbo, Claude-2.1, Gem-\nini Pro, and Llama 2 70B-chat on human benchmarks.\n• Platypus2-70B-Instruct (Platypus 2) is a powerful lan-\nguage model with 70 Billion parameters [37]. The \nmodel itself is a merge of the models Platypus2-70B \nand SOLAR-0-70b-16bit (previously published as \nLLaMa-2-70b-instruct-v2) [38].\nData sets\nPublished data sets\nA list of several data sets for SLRs is provided to  \nthe public by the research group of the ASReview \ntool [39]. The list contains data sets on a variety of \ndifferent biomedical subjects of previously published \nSLRs. For testing the LLM approach on an individual \ndata set, the [Relevant Criteria] string for each data set \nwas created based on the description in the publica -\ntion of the corresponding SLR. We tested the approach \non a total of ten published data sets covering different \nbiomedical topics (Table  1, Supplementary material 2: \nAppendix 2).\nNewly created data set on CDSS in radiation oncology\nTo test the approach also in a prospective setting on a \nnot previously published review, we created a data set \nFig. 1 Schematic illustration of the LLM-based approach for evaluating the relevance of a scientific publication. In this example, a 1–5 scale \nand a 3 + classifier are used\nPage 5 of 14\nDennstädt et al. Systematic Reviews          (2024) 13:158 \n \nfor a new, hypothetical SLR, for which title and abstract \nscreening should be performed.\nThe use case was an SLR on “Clinical Decision Sup -\nport System (CDSS) tools for physicians in radiation \noncology” . A CDSS is an information technology system \ndeveloped to support clinical decision-making. This \ngeneral definition may include diagnostic tools, knowl -\nedge bases, prognostic models, or patient decision aids \n[50]. We decided that the hypothetical SLR should be \nonly about software-based systems to be used by clini -\ncians for decision-making purposes in radiation oncol -\nogy. We defined the following criteria for the [Relevant \nCriteria] text of the provided prompt:\n• Only inclusion of original articles, exclusion of \nreview articles.\n• Publications examining one or several clinical deci -\nsion-support systems relevant to radiation therapy.\n• Decision-support systems are software-based.\n• Exclusion of systems intended for support of non-\nclinicians (e.g., patient decision aids).\n• Publications about models (e.g., prognostic models) \nshould only be included if the model is intended to \nsupport clinical decision-making as part of a soft -\nware application, which may resemble a clinical \ndecision support system.\nThe following query was used for searching relevant pub-\nlications on PubMed: “(clinical decision support system) \nAND (radiotherapy OR radiation therapy)” .\nTitles and abstracts of all publications found with the \nquery were collected. A human-based title and abstract \nscreening was performed to obtain the ground truth data \nset. Two researchers (FD and NC) independently labeled \nthe publications as relevant/not relevant based on the title \nand abstract and based on the [Relevant criteria] string. \nThe task was to label those publications relevant that may \nbe of interest and should be analyzed as full text, while all \nother publications should be labeled irrelevant. After labe-\nling all publications, some of the publications were deemed \nrelevant only by one of the two researchers. To obtain a \nfinal decision, a third researcher (PMP) independently did \nthe labeling for the undecided cases.\nThe aim was to create a human-based data set purely \nrepresenting the process of title and abstract screening \nwithout further information or analysis.\nA manual title and abstract screening was conducted on \n521 publications identified in the search with 36 publica -\ntions being identified as relevant and labeled accordingly in \nthe data set. This data set was named “CDSS_RO” . It should \nbe noted that this data set is qualitatively different from the \n10 published data sets, as not only the publications that \nmay be finally included in an SLR are labeled as relevant, \nbut all publications that should be analyzed in full text \nbased on title and abstract. The file is provided at https://  \ngithub. com/ med- data- tools/ title- abstr act- scree ning- ai).\nParameters and settings of LLM‑based title and abstract \nscreening\nStandard parameters\nThe LLM-based title and abstract screening as described \nabove requires the definition of some parameters. The \nstandard settings for the approach were the following:\n• [Instruction] string: We used the following standard \n[Instruction] string:\n“On a scale from 1 (very low probability) to X (very \nhigh probability), how would you rate the relevance \nof the following scientific publication to be included \nin a systematic literature review based on the rele -\nvant criteria and based on title and abstract?”\nTable 1 Published data sets used for testing the approach\nName Topic Number of \npublications\nRelevant \npublications (%)\nReference\nAppenzeller-Herzog_2020 Wilson’s Disease 3479 26 (0.75%) [40]\nBos_2018 Cerebral small vessel disease and dementia 5756 10 (0.18%) [41]\nDonners_2021 Emicizumab 660 15 (2.27%) [42]\nJeyaraman_2021 Osteoarthritis 1194 96 (2.26%) [43]\nLeenaars_2020 Rheumatoid arthritis 9543 792 (8.30%) [44]\nMejboom_2021 TNFα-inhibitors and biosimilars 2224 37 (1.66%) [45]\nMuthu_2021 Spine surgery 3254 354 (10.88%) [46]\nOud_2018 Borderline personality disorder 1053 20 (1.90%) [47]\nvan_de_Schoot_2018 PTSD 6225 38 (0.61%) [48]\nWolters_2018 Dementia and heart disease 5038 19 (0.38%) [49]\nPage 6 of 14Dennstädt et al. Systematic Reviews          (2024) 13:158 \n• Range of scale: defines the range of the Likert scale \nmentioned in the [Instruction] string (marked as X in \nthe standard string above). For the standard settings, \na value of 5 was used.\n• Model parameters of the LLMs were defined in the \nsource code. To obtain reproducible results, the \nmodel parameters were set accordingly for the model \nto become deterministic (e.g., the temperature value \nis a parameter that defines how much variation a \nresponse of a model should have. Values greater than \n0 add a random element to the output, which should \nbe avoided for the reproducibility of the LLM-based \ntitle and abstract screening).\nAdaptation of instruction prompt and range\nThe behavior of an LLM is highly dependent on the pro -\nvided prompt. Adequate adaptation of the prompt may \nbe used to improve the performance of an LLM for cer -\ntain tasks [25]. To investigate what impact a slightly \nadapted version of the Instruction prompt would have \non the results, we added the string “(Note: Give a low \nscore if not all criteria are fulfilled. Give only a high score \nif all or almost all criteria are fulfilled.)” in the instruc -\ntion prompt as additional instruction and examined the \nimpact on the performance. Furthermore, the range of \nthe scale was changed from 1–5 to 1–10 in some experi -\nments to investigate what impact this would have on the \nperformance.\nStatistical analyses\nThe performance of the approach, depending on mod -\nels and threshold, was determined by calculating the \nsensitivity (= recall), specificity, accuracy, precision, and \nF1-score of the system, based on the amount of correctly \nand incorrectly included/excluded publications for each \ndata set.\nComparison with the automated classifier of Natukunda \net al.\nThe LLM-based title and abstract screening was compared \nto another, recently published approach for fully auto -\nmated title and abstract screening. This approach, devel -\noped by Natukunda et  al., uses an unsupervised Latent \nDirichlet Allocation-based topic model for screening [51]. \nUnlike the LLM-based approach, it does not require an \nadditional [Relevant Criteria] string, but defined search \nkeywords to determine which publications are relevant. \nThe approach was used to do a screening on the ten pub -\nlished data sets as well as on the CDSS_RO data set. To \nobtain the required keywords we processed the text of the \nused search terms by splitting combined text into indi -\nvidual words and removing stop words, duplicates, and \npunctuation (as described in the original publication of \nNatukunda et al.).\nResults\nPerformance of LLM‑based title and abstract screening \nof different models on published data sets\nThe LLM-based screening with a Likert scale of 1–5 pro -\nvided clear results for evaluating the relevance of a publi -\ncation in the majority of cases. Out of the total of 44,055 \npublications among the 10 published data sets, valid and \nunambiguously assignable answers were given for 44,055 \npublications (100%) by the FlanT5 model, for 44,052 pub-\nlications (99.993%) by the OHNC model, for 44,026 pub -\nlications (99.93%) by the Mixtral model and for 44,054 \npublications (99.998%) by the Platypus 2 model. The few \npublications for which an invalid answer was given were \nexcluded from further analysis. None of the excluded \npublications was relevant. The distribution of scores \ngiven was different between the different models. For \nexample, the OHNC model ranked the majority of pub -\nlications with a score of 3 (47.2%) or 4 (34.2%), while the \nFlanT5 model ranked almost all publications with a score \nof either 4 (68.1%) or 2 (31.7%). For all models, the group \nof publications labeled as relevant in the data sets was \nranked with higher scores compared to the overall group \nof publications (mean score of 3.89 compared to 3.38 for \nFlanT5, 3.86 compared to 3.14 for OHNC, 4.16 compared \nto 2.12 for Mixtral and 3.80 compared to 2.92 for Platy -\npus 2). An overview is provided in Fig. 2.\nBased on the scores given, according classifiers that label \npublications with a score of greater than or equal to “X” as \nrelevant, have higher rates of sensitivity and lower rates \nof specificity with decreasing threshold (decreasing “X”).\nClassifiers with a threshold of ≥ 3 (3 + classifiers) \nwere further analyzed, as these classifiers were consid -\nered to correctly identify the vast majority of relevant \npublications (high sensitivity) without including too \nmany irrelevant publications (sufficient specificity). The \n3 + classifiers had a sensitivity/specificity of 94.8%/31.8% \nfor the FlanT5 model, of 97.6%/19.1% for the OHNC \nmodel, of 81.9%/75.2% for the Mixtral model, and of \n97.2%/38.3% for the Platypus 2 model on all ten pub -\nlished data sets. The performance of the classifiers was \nquite different depending on the data set used (Fig.  3). \nDetailed results on the individual data sets are presented \nin Supplementary material 3: Appendix 3.\nThe highest specificity at 100% sensitivity was seen for \nthe Mixtral model on the data set Wolters_2018 with all \n19 relevant publications being scored with 3–5, while \n4410 of 5019 irrelevant publications were scored with \n1 or 2 (specificity of 87.87%). The lowest sensitivity was \nobserved with the Mixtral model on the dataset Jeyara -\nman_2021 with 23.96% sensitivity at 94.63% specificity.\nPage 7 of 14\nDennstädt et al. Systematic Reviews          (2024) 13:158 \n \nUsing LLM‑based title and abstract screening for a new \nsystematic literature review\nOn the newly created manually labeled data set, the \n3 + classifiers had 100% sensitivity for all four models  \nwith specificity ranging from 4.54 to 62.47%. The \nresults of the LLM-based title and abstract screen -\ning, dependent on the threshold for the classifiers are \npresented as receiver operating characteristics (ROC) \ncurves in Fig.  4 as well as in Supplementary material 3: \nAppendix 3.\nFig. 2 Distribution of scores given by the different models\nFig. 3 Sensitivity and specificity of the 3 + classifiers on different data sets using different models. Each data point represents the results of one \nof the data sets\nPage 8 of 14Dennstädt et al. Systematic Reviews          (2024) 13:158 \nDependence of LLM‑based title and abstract screening \non Instruction prompt and on a range of scale\nSeveral runs of the Python script with different settings \n(adapted [Instruction] string and/or range of scale 1–10 \ninstead of 1–5) were performed, which led to different \nresults. Minor adaptation of the Instruction string with \nan additional demand to focus on the mentioned criteria \nhad a different impact on the performance of the classi -\nfiers depending on the LLM used. While the sensitivity \nof the 3 + classifiers remained at 100% for all four mod -\nels, the specificity was lower for the OHNC model (2.89% \nvs. 4.54%), the Mixtral model (56.29% vs. 62.47%) and the \nPlatypus 2 model (15.88% vs. 24.74%), while it was higher \nfor the FlanT5 model (25.15% vs. 12.58%).\nChanging the range of scale from 1–5 to 1–10 and \nusing a 6 + classifier instead of a 3 + classifier led to a \nlower sensitivity for the OHNC model (97.22% vs. 100%), \nwhile increasing the specificity (13.49% vs. 4.54%). For \nthe other models, the sensitivity remained at 100% with \nhigher specificity for the Platypus 2 model (51.34% vs. \n24.74%) and the FlanT5 model (50.52% vs. 12.58%). \nThe specificity was unchanged for the Mixtral model at \n62.47%, which was the highest value among all combina -\ntions at 100% sensitivity. No combination of the settings \nfor a range of scales and with/without prompt adapta -\ntion was superior among all models. An overview of the \nresults is provided in Fig. 5.\nComparison with unsupervised title and abstract screening \nof Natukunda et al.\nThe screening approach developed by Natukunda et  al. \nachieved an overall sensitivity of 52.75% at 56.39% speci -\nficity on the ten published data sets. As for the LLM-\nbased screening, the performance of this approach was \ndependent on the data set analyzed. The lowest sen -\nsitivity was observed for the Jeyaraman_2021 data set \n(1.04%), while the highest sensitivity was observed for the \nWolters_2018 dataset (100%). Compared to the 3 + clas-\nsifier with the Mixtral model, the LLM-based approach \nhad higher sensitivity on 9 data sets and equal sensitivity \non 1 data set, while it had higher specificity on 6 data sets \nand lower specificity on 4 data sets.\nOn the CDSS_RO data set, the approach of Natukunda \net  al. achieved 94.44% sensitivity (lower than all four \nLLMs) at 39.59% specificity (lower than the Mixtral \nmodel and higher than the FlanT5, OHNC, and Platypus \n2 models). Further data on the comparison is provided in \nSupplementary material 4: Appendix 4.\nDiscussion\nWe developed and elaborated a flexible approach to use \nLLMs for automated title and abstract screening that has \nshown some promising results on a variety of biomedi -\ncal topics. Such an approach could potentially be used to \nautomatically pre-screen the relevance of publications \nFig. 4 Receiver operating characteristics (ROC) curves of the LLM-based title and abstract screening for the different models on the CDSS_RO data \nset\nPage 9 of 14\nDennstädt et al. Systematic Reviews          (2024) 13:158 \n \nbased on title and abstract. While the results are far from \nperfect, using LLMs for evaluating the relevance of pub -\nlications could potentially be helpful (e.g., as a pre-pro -\ncessing step) when performing an SLR. Furthermore, the \napproach is widely applicable without the development of \ncustom tools or training custom models.\nAutomated and semi‑automated screening\nA variety of different ML and AI tools have been devel -\noped to assist researchers in performing SLRs [5, 10, \n52, 53]. Fully automated systems (like the LLM-based \napproach presented in our study) still fail to differenti -\nate relevant from irrelevant publications near the level of \nhuman evaluation [51, 54].\nA well-functioning fully automated title and abstract \nscreening system that could be used on different subjects \nin the biomedical domain and possibly also in other sci -\nentific areas would be very valuable. While human-based \nscreening is the current gold standard, it has consider -\nable drawbacks. From a methodological point of view, \none major problem of human-based literature evaluation, \nincluding title and abstract screening, is the subjectivity \nof the process [55]. Evaluating the publications (based \non title and abstract) is dependent on the experience and \nindividual judgments of the person doing the screen -\ning. To overcome this issue, SLRs of high quality require \nmultiple independent researchers to do the evaluation \nwith specific criteria upon inclusion/exclusion defined \nbeforehand [56]. Nevertheless, subjectivity remains an \nunresolved issue, which also limits the reproducibility \nof results. From a practical point of view, another major \nproblem is the considerable workload needed to be per -\nformed by humans, especially if thousands of publica -\ntions need to be assessed, which is multiplied by the \nneed to have multiple reviewers and to discuss disagree -\nments. The challenge of workload is not just a matter of \ninconvenience, as SLRs on subjects that require tens of \nthousands of publications to be searched, may just not be \nfeasible for small research teams to do, or may already be \noutdated after the time it would take to do the screening \nand analyze the results.\nWhile fully automated screening approaches may also \nbe affected by subjectivity (since the training data of \nmodels is itself generated by processes which are affected \nby subjectivity), the results would at least be more repro -\nducible, and automation can be applied at scale in order \nto overcome the problem of practicability.\nWhile current fully automated systems cannot replace \nhumans in title and abstract screening, they may never -\ntheless be helpful. Such systems are already being used in \nsystematic reviews and most likely their usage will con -\ntinue to grow [57].\nFig. 5 Performance of the classifiers depending on adaptation of the prompt and on the range of scale\nPage 10 of 14Dennstädt et al. Systematic Reviews          (2024) 13:158 \nIdeally, a fully automated system should not miss a \nsingle relevant publication (100% sensitivity) while mini -\nmizing as far as possible the number of irrelevant publi -\ncations included. This would allow confident exclusion of \nsome of the retrieved search results which is a big asset to \nreducing time taken in manual screening.\nLLMs for title and abstract screening\nBy creating structured prompts with clear instructions, \nan LLM can feasibly be used for evaluating the relevance \nof a scientific publication. In comparison to some other \nsolutions, the LLM-based screening may have some \nadvantages. On the one hand, the flexible nature of the \napproach allows adaptation to a specific subject. Depend-\ning on the question, different prompts for relevant crite -\nria and instructions can be used to address the individual \nresearch question. On the other hand, the approach can \ncreate reproducible results, given a fixed model, param -\neters, prompting strategy, and defined threshold. At the \nsame time, it is scalable to process large numbers of pub -\nlications. As we have seen, such an approach is feasible \nwith a performance similar to or even better in com -\nparison to other current solutions like the approach of \nNatukunda et  al. However, it should be noted that the \nperformance varied considerably depending on which of \nthe 10 + 1 data sets were used.\nFurther applications of LLMs in literature analysis\nWhile we investigated LLMs for evaluating the relevance \nof publications and in particular for title and abstract \nscreening, it is being discussed how these models may \nbe used for a variety of tasks in literature analysis [58, \n59]. For example, Wang et al. obtained promising results \nwhen investigating if ChatGPT may be used for writing \nBoolean Queries for SLRs [60]. Aydin et  al., also using \nChatGPT, employed the LLM to write an entire Litera -\nture Review about Digital Twins in Healthcare [61].\nGuo et  al. recently performed a study using the Ope -\nnAI API with gpt-3.5 and gpt-4 to create a classifier for \nclinical reviews [62]. They observed promising results \nwhen comparing the performance of the classifier against \nhuman-based screening with a sensitivity of 76% at 91% \nspecificity on six different review papers. In contrast to \nour approach, they used a Boolean classifier instead of a \nLikert scale. Another approach was developed by Akins -\neloyin et al., who used ChatGPT to create a method for \ncitation screening by ranking the relevance of publica -\ntions using a question-answering framework [63].\nThe question may arise what the purpose of using a \nLikert scale instead of a direct binary classifier  is (also \nsince some models only rarely use some of the score val -\nues; see e.g., FlanT5 in Fig.  2). The rationale for using the \nLikert scale arose out of some preliminary, unsystematic \nexplorations we conducted using different models and \nranges of scale (including binary). We realized that using \na Likert scale has some advantages as it sorts the publi -\ncations into several groups depending on the estimated \nrelevance. This also allows flexible adjustment of the \nthreshold (which may potentially also be useful if the \nuser wants to rather focus on sensitivity or rather on \nspecificity).\nHowever, there seem to be several feasible approaches \nand frameworks to use LLMs for the screening of \npublications.\nIt should be noted that an LLM-based approach for \nevaluating the relevance of publications might just as \nwell be used for a variety of different classification tasks \nin literature analysis. For example, one may adopt the \n[Instruction prompt] asking the LLM not to evaluate the \nrelevance of a publication on a Likert scale, but for clas -\nsification into several groups like “original article” , “trial” , \n“letter to the editor” , etc. From this point of view, the title \nand abstract screening is just a special use case of LLM-\nbased classification.\nFuture developments\nThe capabilities of LLMs and other AI models will con -\ntinue to evolve, which will increase the performance of \nfully automated systems. As we have seen, the results are \nhighly dependent on the LLM used for the approach. In \nany case, there may still be substantial room for improve-\nment and optimization and it currently is unclear what \nLLM-based approach with which prompts, models, and \nsettings yields the best results over a large variety of data \nsets.\nFurthermore, LLMs may not only be used for the \nscreening of titles and abstracts but for the analysis of \nfull-text documents. The newest generation of language \nand multimodal models may process whole articles or \npotentially also image data from publications [64, 65]. \nBeyond that, LLM-based evaluation of scientific data and \npublications may only be one of several options for AI \nassistance in literature analysis. Future systems may com-\nbine different ML and AI approaches for optimal auto -\nmated processing of literature and scientific data.\nLimitations of LLM‑based title and abstract screening\nEven though the LLM-based screening presented in our \nwork shows some promising results, it also has some \ndrawbacks and limitations. While the open framework \nwith adaptable prompts makes the approach flexible, \nthe performance of the approach is highly dependent \non the used model, the input parameters/settings, and \nthe data set analyzed. If a slightly different instruction or \nanother scale (1–10 instead of 1–5) is used, this can have \na considerable impact on the performance. The classifiers \nPage 11 of 14\nDennstädt et al. Systematic Reviews          (2024) 13:158 \n \nanalyzed in our study failed to consistently identify rel -\nevant publications at 100% sensitivity without consid -\nerably impairing the specificity. In academic research, \nthe bar for automated screening tools needs to be very \nhigh, as ideally not a single relevant publication should \nbe missed. The LLM-based title and abstract screen -\ning requires the definition of clear criteria for inclusion/\nexclusion. For research questions with less clear relevance \ncriteria, LLMs may not be that useful for the evaluation. \nThis may potentially be one reason, why the performance \nof the approach was quite different in our study depend -\ning on the data set analyzed. Overall, there are still many \nopen questions, and it is unclear if and how high levels \nof performance can be consistently guaranteed so that \nsuch a system can be relied on. It is interesting that the \nMixtral model, even though it seemed to have the highest \nlevel of performance on average, performed poorly with \nlow sensitivity on one data set (Fig. 3). Further research is \nneeded to investigate the requirements for good perfor -\nmance of the LLMs in evaluating scientific literature.\nAnother limitation of the approach in its current form \nis a considerable demand for resources regarding cal -\nculation power and hardware equipment. Answering \nthousands of long text prompts with modern, multi-\nbillion-parameter LLMs requires sufficient IT infra -\nstructure and calculation power to perform. The issue of \nresource demand is especially relevant if many thousand \npublications are evaluated and if very complex models \nare used.\nFundamental issues of using LLMs in literature analysis\nOn a more fundamental level, there are some general \nissues regarding the use of LLMs for literature studies. \nLLMs calculate the probability for a sequence of words \nbased on their training data which derives from past \nobservations and knowledge. They can thereby inherit \nunwanted features and biases (such as for example eth -\nnic or gender biases) [29, 66]. In a recent study by Koo \net al., it was shown that the cognitive biases and prefer -\nences of LLMs are not the same as the ones of humans \nas a low correlation between ratings given by LLMs \nand humans was observed [67]. The authors therefore \nstated that LLMs are currently not suitable as fair and \nreliable automatic evaluators. Considering that using \nLLMs for evaluating and processing scientific publica -\ntions may be seen as a problematic and questionable \nundertaking. However, the biases present in language \nmodels affect different tasks differently, and it remains \nto be seen how they might differentially affect different \nscreening tasks in the literature review [28].\nNevertheless, it is most likely that LLMs and other AI \nsolutions will be increasingly used in conducting and \nevaluating scientific research [68]. While this certainly \nwill provide a lot of chances and opportunities, it is \nalso potentially concerning. The amount and propor -\ntion of text being written by AI models is increasing. \nThis includes not only public text on the Internet but \nalso scientific literature and publications [69, 70]. The \nfact that ChatGPT has been chosen as one of the top \nresearchers of the year 2023 by Nature and has fre -\nquently been listed as co-author, shows how immedi -\nate the impact of the development has already been \n[71]. At the same time, most LLMs are trained on large \namounts of text provided on the Internet. The idea that \nin the future LLMs might be used to evaluate publica -\ntions written with the help of LLMs that may them -\nselves be trained on data created by LLMs may lead \nto disturbing negative feedback loops which decrease \nthe quality of the results over time [72]. Such a devel -\nopment could actually undermine academia and evi -\ndence-based science [73], also due to the known fact \nthat LLMs tend to “hallucinate” , meaning that a model \nmay generate text with illusory statements not based on \ncorrect data [26]. It is important to be aware that LLMs \nare not directly coupled to evidence and that there is no \nrestriction preventing a model from generating incor -\nrect statements. As part of a screening tool assigning \njust a score value to the relevance of a publication, this \nmay be a mere factor impairing the performance of the \nsystem – yet for LLM-based analysis in general this is a \nmajor problem.\nThe majority of studies that so far have been pub -\nlished on using LLMs for publication screening used \nthe currently most powerful models that are operated \nby private companies—most notably the ChatGPT \nmodels GPT-3.5 and GPT-4 developed by OpenAI \n[18, 74]. Using models that are owned and controlled \nby private companies and that may change over time is \nassociated with additional major problems when using \nthem for publication screening, such as a lack of repro -\nducibility. Therefore, after initial experiments with such \nmodels, we decided to use openly available models for \nour study.\nLimitations of the study\nOur study has some limitations. While we present a \nstrategy for using LLMs to evaluate the relevance of \npublications for an SLR, our work does not provide a \ncomprehensive analysis of all possible capabilities and \nlimitations. Even though we achieved promising results \non ten published data sets and a newly created one in \nour study, generalization of the results may be limited \nas it is not clear how the approach would perform on \nmany other subjects within the biomedical domain more \nbroadly and within other domains. To get a more com -\nprehensive understanding, thorough testing with many \nPage 12 of 14Dennstädt et al. Systematic Reviews          (2024) 13:158 \nmore data sets about different topics would be needed, \nwhich is beyond the scope of this work. Testing the \nscreening approach on retrospective data sets is also per \nse problematic. While a good performance on retrospec -\ntive data should hopefully indicate a good performance \nif used prospectively on a new topic, this does not have \nto be the case [75]. Indeed, naively assuming a classifier \nthat was tested on retrospective data will perform equally \non a new research question is clearly problematic, since a \nnew research question in science is by definition new and \nunfamiliar and therefore will not be represented in previ -\nously tested data sets.\nFurthermore, models that are trained on vast amounts \nof scientific literature may even have been trained on \nsome publications or the reviews that are used in the \nretrospective benchmarking of an LLM-based classifier, \nwhich obviously creates a considerable bias. To objec -\ntively assess how well an LLM-based solution can evalu -\nate scientific publications for new research questions, \nlarge cultivated and independent prospective data sets \non many different topics would be needed, which will be \nvery challenging to create. It is interesting that the LLM-\nbased title and abstract screening in our study would \nhave also performed well on our new hypothetical SLR \non CDSS in radiation therapy, but of course, this alone is \na too limited data basis from which to draw general con -\nclusions. Therefore, it currently cannot be reliably known \nin which situations such an LLM-based evaluation may \nsucceed or may fail.\nRegarding the ten published data sets, the results also \nneed to be interpreted with caution. These data sets may \nnot truly represent the singular task of title and abstract \nscreening. For example, in the Appenzeller-Herzog_2020 \ndata set, only the 26 publications that were finally \nincluded (not only after title and abstract screening but \nalso after further analysis) were labeled as relevant [40]. \nWhile these publications ideally should be correctly \nidentified by an AI-classifier, there may be other publica -\ntions in the data set, that per se cannot be excluded solely \nbased on title and abstract. Furthermore, we had to retro-\nspectively define the [Relevant Criteria] string based on \nthe text in the publication of the SLR. This obviously is a \nsuboptimal way to define inclusion and exclusion criteria, \nas the defined string may not completely align with the \ncriteria intended by the researchers of the SLR.\nWe also want to emphasize that the comparison with \nthe approach of Natukunda et al. needs to be interpreted \nwith caution since the two approaches are not based on \nexactly the same prerequisites: the LLM-based approach \nrequires a [Relevant Criteria] string, while the approach \nof Natukunda et al. requires defined keywords.\nWhile overall our work shows that LLM-based title and \nabstract screening is possible and shows some promising \nresults on the analyzed data sets, our study cannot fully \nanswer the question of how well LLMs would perform if \nthey were used for new research. Even more importantly, \nwe cannot answer the question of to what extent LLMs \nshould be used for conducting literature reviews and for \ndoing research.\nConclusions\nLarge language models can be used for evaluating the rel-\nevance of publications for SLRs. We were able to imple -\nment a flexible and cross-domain system with promising \nresults on different biomedical subjects. With the con -\ntinuing progress in the fields of LLMs and AI, fully auto -\nmated computer systems may assist researchers in \nperforming SLRs and other forms of scientific knowledge \nsynthesis. However, it remains unclear how well such \nsystems will perform when being used in a prospective \nmanner and what implications this will have on the con -\nduction of SLRs.\nAbbreviations\nAI  Artificial intelligence\nAPI  Application programming interface\nCDSS  Clinical Decision Support System\nFlanT5  FlanT5-XXL model\nGPT  Generative pre-trained transformer\nMixtral  Mixtral-8 × 7B-Instruct v0.1 model\nML  Machine learning\nLLM  Large language model\nOHNC  OpenHermes-2.5-neural-chat-7b-v3-1-7B model\nPlatypus 2  Platypus2-70B-Instruct model\nROC  Receiver operating characteristic\nSLR  Systematic literature review\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13643- 024- 02575-4.\nSupplementary Material 1: Appendix 1: Sample prompt.\nSupplementary Material 2: Appendix 2: Relevant criteria of published \ndatasets.\nSupplementary Material 3: Appendix 3: Performance of models on data \nsets.\nSupplementary Material 4: Appendix 4: Comparison with other approach.\nAcknowledgements\nNot applicable.\nAuthors’ contributions\nAll authors contributed to designing the concept and methodology of the \npresented approach of LLM-based evaluation of the relevance of a publica-\ntion to an SLR. The Python script was created by FD and JZ. The experiments \nwere conducted by FD and JH. All authors contributed in writing and revising \nthe manuscript. All authors have read and approved the final version of the \nmanuscript.\nFunding\nNot applicable.\nPage 13 of 14\nDennstädt et al. Systematic Reviews          (2024) 13:158 \n \nAvailability of data and materials\nAll data generated and analyzed during this study are either included in this \npublished article (and its supplementary information files) or publicly avail-\nable on the Internet. The Python script as well as the CDSS_RO data set are \navailable under https:// github. com/ med- data- tools/ title- abstr act- scree ning- ai. \nThe ten published data sets analyzed in our study are available on the GitHub \nRepository of the research group of the ASReview Tool [39].\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nNC is a technical lead for the SmartOncology© project and medical advisor \nfor Wemedoo AG, Steinhausen AG, Switzerland. The authors declare that they \nhave no other competing interests.\nAuthor details\n1 Department of Radiation Oncology, Cantonal Hospital of St. Gallen, St. \nGallen, Switzerland. 2 Institute for Computer Science, University of Würzburg, \nWürzburg, Germany. 3 Department of Radiation Oncology, Inselspital, Bern Uni-\nversity Hospital and University of Bern, Bern, Switzerland. 4 Institute for Imple-\nmentation Science in Health Care, University of Zurich, Zurich, Switzerland. \n5 School of Medicine, University of St. Gallen, St. Gallen, Switzerland. 6 Swiss \nInstitute of Bioinformatics, Lausanne, Switzerland. \nReceived: 17 June 2023   Accepted: 30 May 2024\nReferences\n 1. Khalil H, Ameen D, Zarnegar A. Tools to support the automation of \nsystematic reviews: a scoping review. J Clin Epidemiol. 2022;144:22–42.\n 2. Clark J, Scott AM, Glasziou P . Not all systematic reviews can be com-\npleted in 2 weeks—But many can be (and should be). J Clin Epidemiol. \n2020;126:163.\n 3. Clark J, Glasziou P , Del Mar C, Bannach-Brown A, Stehlik P , Scott AM. A full \nsystematic review was completed in 2 weeks using automation tools: a \ncase study. J Clin Epidemiol. 2020;121:81–90.\n 4. Pham B, Jovanovic J, Bagheri E, Antony J, Ashoor H, Nguyen TT, et al. Text \nmining to support abstract screening for knowledge syntheses: a semi-\nautomated workflow. Syst Rev. 2021;10(1):156.\n 5. van de Schoot R, de Bruin J, Schram R, Zahedi P , de Boer J, Weijdema \nF, et al. An open source machine learning framework for efficient and \ntransparent systematic reviews. Nat Mach Intell. 2021;3(2):125–33.\n 6. Hamel C, Hersi M, Kelly SE, Tricco AC, Straus S, Wells G, et al. Guidance for \nusing artificial intelligence for title and abstract screening while conduct-\ning knowledge syntheses. BMC Med Res Methodol. 2021;21(1):285.\n 7. Covidence [Internet]. [cited 2024 Jan 14]. Available from: www. covid ence. org.\n 8. Machine learning functionality in EPPI-Reviewer [Internet]. [cited 2024 \nJan 14]. Available from: https:// eppi. ioe. ac. uk/ CMS/ Porta ls/ 35/ machi \nne_ learn ing_ in_ eppi- revie wer_v_ 7_ web_ versi on. pdf.\n 9. Elicit [Internet]. [cited 2024 Jan 14]. Available from: https:// elicit. org/.\n 10. Harrison H, Griffin SJ, Kuhn I, Usher-Smith JA. Software tools to support \ntitle and abstract screening for systematic reviews in healthcare: an \nevaluation. BMC Med Res Methodol. 2020;20(1):7.\n 11. Rayyan [Internet]. [cited 2024 Jan 14]. Available from: https:// www.  \nrayyan. ai/.\n 12. DistillerSR [Internet]. [cited 2024 Jan 14]. Available from: https:// www.  \ndisti llersr. com/ produ cts/ disti llersr- syste matic- review- softw are.\n 13. Abstrackr [Internet]. [cited 2024 Jan 14]. Available from: http:// abstr  \nackr. cebm. brown. edu/ accou nt/ login.\n 14. RobotAnalyst [Internet]. [cited 2024 Jan 14]. Available from: http://  \nwww. nactem. ac. uk/ robot analy st/.\n 15. Clark J, McFarlane C, Cleo G, Ishikawa Ramos C, Marshall S. The impact \nof systematic review automation tools on methodological quality and \ntime taken to complete systematic review Tasks: Case Study. JMIR Med \nEduc. 2021;7(2): e24418.\n 16. Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB, et al. Comparing \nphysician and artificial intelligence chatbot responses to patient ques-\ntions posted to a public social media forum. JAMA Intern Med. 2023 \n[cited 2024 Jan 14]; Available from: https:// jaman etwork. com/ journ als/ \njamai ntern almed icine/ fulla rticle/ 28043 09.\n 17. Tang L, Sun Z, Idnay B, Nestor JG, Soroush A, Elias PA, et al. Evaluat -\ning Large Language Models on Medical Evidence Summarization \n[Internet]. Health Informatics; 2023 Apr [cited 2024 Jan 14]. Available \nfrom: http://medrxiv.org/lookup/doi/https:// doi. org/ 10. 1101/ 2023. 04. \n22. 23288 967.\n 18. OpenAI: GPT3-apps [Internet]. [cited 2024 Jan 14]. Available from: \nhttps:// openai. com/ blog/ gpt-3- apps.\n 19. Google: PaLM [Internet]. [cited 2024 Jan 14]. Available from: https:// ai. googl \neblog. com/ 2022/ 04/ pathw ays- langu age- model- palm- scali ng- to. html.\n 20. Google: Gemini [Internet]. [cited 2024 Jan 14]. Available from: https://  \ndeepm ind. google/ techn ologi es/ gemin i/# hands- on.\n 21. Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, et al. A Survey of Large \nLanguage Models. 2023 [cited 2024 Jan 14]; Available from: https://  \narxiv. org/ abs/ 2303. 18223.\n 22. McNichols H, Zhang M, Lan A. Algebra error classification with large \nlanguage models [Internet]. arXiv; 2023 [cited 2023 May 25]. Available \nfrom: http:// arxiv. org/ abs/ 2305. 06163.\n 23. Wadhwa S, Amir S, Wallace BC. Revisiting relation extraction in the era \nof large language models [Internet]. arXiv; 2023 [cited 2024 Jan 14]. \nAvailable from: http:// arxiv. org/ abs/ 2305. 05003.\n 24. Trajanoska M, Stojanov R, Trajanov D. Enhancing knowledge graph \nconstruction using large language models [Internet]. arXiv; 2023 [cited \n2024 Jan 14]. Available from: http:// arxiv. org/ abs/ 2305. 04676.\n 25. Reynolds L, McDonell K. Prompt programming for large language \nmodels: beyond the few-shot paradigm [Internet]. arXiv; 2021 [cited \n2024 Jan 14]. Available from: http:// arxiv. org/ abs/ 2102. 07350.\n 26. Guerreiro NM, Alves D, Waldendorf J, Haddow B, Birch A, Colombo P , \net al. Hallucinations in Large Multilingual Translation Models [Internet]. \narXiv; 2023 [cited 2024 Jan 14]. Available from: http:// arxiv. org/ abs/  \n2303. 16104.\n 27. Zack T, Lehman E, Suzgun M, Rodriguez JA, Celi LA, Gichoya J, et al. \nAssessing the potential of GPT-4 to perpetuate racial and gender \nbiases in health care: a model evaluation study. Lancet Digital Health. \n2024;6(1):e12-22.\n 28. Hastings J. Preventing harm from non-conscious bias in medical genera-\ntive AI. Lancet Digital Health. 2024;6(1):e2-3.\n 29. Digutsch J, Kosinski M. Overlap in meaning is a stronger predictor of \nsemantic activation in GPT-3 than in humans. Sci Rep. 2023;13(1):5035.\n 30. Huggingface: FlanT5-XXL [Internet]. [cited 2024 Jan 14]. Available from: \nhttps:// huggi ngface. co/ google/ flan- t5- xxl.\n 31. Chung HW, Hou L, Longpre S, Zoph B, Tay Y, Fedus W, et al. Scaling \nInstruction-Finetuned Language Models [Internet]. arXiv; 2022 [cited \n2024 Jan 14]. Available from: http:// arxiv. org/ abs/ 2210. 11416.\n 32. Huggingface: OpenHermes-2.5-neural-chat-7b-v3–1–7B [Internet]. [cited \n2024 Jan 14]. Available from: https:// huggi ngface. co/ Weyaxi/ OpenH \nermes-2. 5- neural- chat- 7b- v3-1- 7B.\n 33. Huggingface: OpenHermes-2.5-Mistral-7B [Internet]. [cited 2024 Jan 14]. \nAvailable from: https:// huggi ngface. co/ tekni um/ OpenH ermes-2. 5- Mistr \nal- 7B.\n 34. Huggingface: neural-chat-7b-v3–1 [Internet]. [cited 2024 Jan 14]. Avail-\nable from: https:// huggi ngface. co/ Intel/ neural- chat- 7b- v3-1.\n 35. Huggingface: Mixtral-8x7B-Instruct-v0.1 [Internet]. [cited 2024 Jan 14]. \nAvailable from: https:// huggi ngface. co/ mistr alai/ Mixtr al- 8x7B- Instr \nuct- v0.1.\n 36. Jiang AQ, Sablayrolles A, Roux A, Mensch A, Savary B, Bamford C, et al. \nMixtral of Experts [Internet]. [cited 2024 Jan 14]. Available from: http:// \narxiv. org/ abs/ 2401. 04088.\n 37. Huggingface: Platypus2–70B-Instruct [Internet]. [cited 2024 Jan 14]. Avail-\nable from: https:// huggi ngface. co/ garage- bAInd/ Platy pus2- 70B- instr uct.\n 38. Huggingface: SOLAR-0–70b-16bit [Internet]. [cited 2024 Jan 14]. Available \nfrom: https:// huggi ngface. co/ upsta ge/ SOLAR-0- 70b- 16bit# updat es.\nPage 14 of 14Dennstädt et al. Systematic Reviews          (2024) 13:158 \n 39. Systematic Review Datasets: ASReview [Internet]. [cited 2024 Jan 14]. \nAvailable from: https:// github. com/ asrev iew/ syste matic- review- datas ets.\n 40. Appenzeller-Herzog C, Mathes T, Heeres MLS, Weiss KH, Houwen RHJ, \nEwald H. Comparative effectiveness of common therapies for Wilson \ndisease: a systematic review and meta-analysis of controlled studies. Liver \nInt. 2019;39(11):2136–52.\n 41. Bos D, Wolters FJ, Darweesh SKL, Vernooij MW, De Wolf F, Ikram MA, et al. \nCerebral small vessel disease and the risk of dementia: a systematic \nreview and meta-analysis of population-based evidence. Alzheimer’s & \nDementia. 2018;14(11):1482–92.\n 42. Donners AAMT, Rademaker CMA, Bevers LAH, Huitema ADR, Schut-\ngens REG, Egberts TCG, et al. Pharmacokinetics and associated efficacy \nof emicizumab in humans: a systematic review. Clin Pharmacokinet. \n2021;60(11):1395–406.\n 43. Jeyaraman M, Muthu S, Ganie PA. Does the source of mesenchymal \nstem cell have an effect in the management of osteoarthritis of the \nknee? Meta-analysis of randomized controlled trials. CARTILAGE. 2021 \nDec;13(1_suppl):1532S-1547S.\n 44. Leenaars C, Stafleu F, De Jong D, Van Berlo M, Geurts T, Coenen-de Roo T, \net al. A systematic review comparing experimental design of animal and \nhuman methotrexate efficacy studies for rheumatoid arthritis: lessons for \nthe translational value of animal studies. Animals. 2020;10(6):1047.\n 45. Meijboom RW, Gardarsdottir H, Egberts TCG, Giezen TJ. Patients retran-\nsitioning from biosimilar TNFα inhibitor to the corresponding originator \nafter initial transitioning to the biosimilar: a systematic review. BioDrugs. \n2022;36(1):27–39.\n 46. Muthu S, Ramakrishnan E. Fragility analysis of statistically significant out-\ncomes of randomized control trials in spine surgery: a systematic review. \nSpine. 2021;46(3):198–208.\n 47. Oud M, Arntz A, Hermens ML, Verhoef R, Kendall T. Specialized psycho-\ntherapies for adults with borderline personality disorder: a systematic \nreview and meta-analysis. Aust N Z J Psychiatry. 2018;52(10):949–61.\n 48. Van De Schoot R, Sijbrandij M, Depaoli S, Winter SD, Olff M, Van Loey \nNE. Bayesian PTSD-trajectory analysis with informed priors based on a \nsystematic literature search and expert elicitation. Multivar Behav Res. \n2018;53(2):267–91.\n 49. Wolters FJ, Segufa RA, Darweesh SKL, Bos D, Ikram MA, Sabayan B, \net al. Coronary heart disease, heart failure, and the risk of demen-\ntia: A systematic review and meta-analysis. Alzheimer’s Dementia. \n2018;14(11):1493–504.\n 50. Sutton RT, Pincock D, Baumgart DC, Sadowski DC, Fedorak RN, Kroeker \nKI. An overview of clinical decision support systems: benefits, risks, and \nstrategies for success. npj Digit Med. 2020 Feb 6;3(1):17.\n 51. Natukunda A, Muchene LK. Unsupervised title and abstract screening \nfor systematic review: a retrospective case-study using topic modelling \nmethodology. Syst Rev. 2023;12(1):1.\n 52. Marshall IJ, Wallace BC. Toward systematic review automation: a practical \nguide to using machine learning tools in research synthesis. Syst Rev. \n2019 Dec;8(1):163, s13643–019–1074–9.\n 53. Wallace BC, Trikalinos TA, Lau J, Brodley C, Schmid CH. Semi-automated \nscreening of biomedical citations for systematic reviews. BMC Bioinfor-\nmatics. 2010;11(1):55.\n 54. Li D, Wang Z, Wang L, Sohn S, Shen F, Murad MH, et al. A text-mining \nframework for supporting systematic reviews. Am J Inf Manag. \n2016;1(1):1–9.\n 55. de Almeida CPB, de Goulart BNG. How to avoid bias in systematic reviews \nof observational studies. Rev CEFAC. 2017;19(4):551–5.\n 56. Siddaway AP , Wood AM, Hedges LV. How to do a systematic \nreview: a best practice guide for conducting and reporting narra-\ntive reviews, meta-analyses, and meta-syntheses. Annu Rev Psychol. \n2019;70(1):747–70.\n 57. Santos ÁOD, Da Silva ES, Couto LM, Reis GVL, Belo VS. The use of artificial \nintelligence for automating or semi-automating biomedical literature \nanalyses: a scoping review. J Biomed Inform. 2023;142: 104389.\n 58. Haman M, Školník M. Using ChatGPT to conduct a literature review. \nAccount Res. 2023;6:1–3.\n 59. Liu R, Shah NB. ReviewerGPT? An exploratory study on using large lan-\nguage models for paper reviewing [Internet]. arXiv; 2023 [cited 2024 Jan \n14]. Available from: http:// arxiv. org/ abs/ 2306. 00622\n 60. Wang S, Scells H, Koopman B, Zuccon G. Can ChatGPT write a good \nboolean query for systematic review literature search? [Internet]. arXiv; \n2023 [cited 2024 Jan 14]. Available from: http:// arxiv. org/ abs/ 2302. 03495.\n 61. Aydın Ö, Karaarslan E. OpenAI ChatGPT generated literature review: \ndigital twin in healthcare. SSRN Journal [Internet]. 2022 [cited 2024 Jan \n14]; Available from: https:// www. ssrn. com/ abstr act= 43086 87.\n 62. Guo E, Gupta M, Deng J, Park YJ, Paget M, Naugler C. Automated paper \nscreening for clinical reviews using large language models [Internet]. \narXiv; 2023 [cited 2024 Jan 14]. Available from: http:// arxiv. org/ abs/ 2305. \n00844.\n 63. Akinseloyin O, Jiang X, Palade V. A novel question-answering framework \nfor automated citation screening using large language models [Internet]. \nHealth Informatics; 2023 Dec [cited 2024 Jan 14]. Available from: http://\nmedrxiv.org/lookup/doi/https:// doi. org/ 10. 1101/ 2023. 12. 17. 23300 102.\n 64. Koh JY, Salakhutdinov R, Fried D. Grounding language models to images \nfor multimodal inputs and outputs. 2023 [cited 2024 Jan 14]; Available \nfrom: https:// arxiv. org/ abs/ 2301. 13823.\n 65. Wang L, Lyu C, Ji T, Zhang Z, Yu D, Shi S, et al. Document-level machine \ntranslation with large language models [Internet]. arXiv; 2023 [cited 2024 \nJan 14]. Available from: http:// arxiv. org/ abs/ 2304. 02210.\n 66. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P , et al. Lan-\nguage Models are Few-Shot Learners [Internet]. arXiv; 2020 [cited 2024 \nJan 14]. Available from: http:// arxiv. org/ abs/ 2005. 14165.\n 67. Koo R, Lee M, Raheja V, Park JI, Kim ZM, Kang D. Benchmarking cognitive \nbiases in large language models as evaluators [Internet]. arXiv; 2023 \n[cited 2024 Jan 14]. Available from: http:// arxiv. org/ abs/ 2309. 17012.\n 68. Editorial —Artificial Intelligence language models in scientific writing. \nEPL. 2023 Jul 1;143(2):20000.\n 69. Grimaldi G, Ehrler BAI, et al. Machines Are About to Change Scientific \nPublishing Forever. ACS Energy Lett. 2023;8(1):878–80.\n 70. Grillo R. The rising tide of artificial intelligence in scientific journals: a \nprofound shift in research landscape. Eur J Ther. 2023;29(3):686–8.\n 71. nature: ChatGPT and science: the AI system was a force in 2023 — for \ngood and bad [Internet]. [cited 2024 Jan 14]. Available from: https:// \nwww. nature. com/ artic les/ d41586- 023- 03930-6.\n 72. Chiang CH, Lee H yi. Can large language models be an alternative to \nhuman evaluations? 2023 [cited 2024 Jan 6]; Available from: https:// arxiv. \norg/ abs/ 2305. 01937.\n 73. Erler A. Publish with AUTOGEN or perish? Some pitfalls to avoid in the \npursuit of academic enhancement via personalized large language \nmodels. Am J Bioeth. 2023;23(10):94–6.\n 74. OpenAI: ChatGPT [Internet]. [cited 2024 Jan 14]. Available from: https:// \nopenai. com/ blog/ chatg pt.\n 75. Gates A, Gates M, Sebastianski M, Guitard S, Elliott SA, Hartling L. The \nsemi-automation of title and abstract screening: a retrospective explora-\ntion of ways to leverage Abstrackr’s relevance predictions in systematic \nand rapid reviews. BMC Med Res Methodol. 2020;20(1):139.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.8383111953735352
    },
    {
      "name": "Task (project management)",
      "score": 0.6228303909301758
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.47237327694892883
    },
    {
      "name": "Automation",
      "score": 0.43321067094802856
    },
    {
      "name": "Data science",
      "score": 0.35584399104118347
    },
    {
      "name": "Computer science",
      "score": 0.17260658740997314
    },
    {
      "name": "Systems engineering",
      "score": 0.07094329595565796
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2799884508",
      "name": "Kantonsspital St. Gallen",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I25974101",
      "name": "University of Würzburg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I12708293",
      "name": "SIB Swiss Institute of Bioinformatics",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I202963720",
      "name": "University of St.Gallen",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I202697423",
      "name": "University of Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I2801112126",
      "name": "University Hospital of Bern",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I118564535",
      "name": "University of Bern",
      "country": "CH"
    }
  ],
  "cited_by": 78
}