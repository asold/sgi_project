{
  "title": "Norm Tweaking: High-Performance Low-Bit Quantization of Large Language Models",
  "url": "https://openalex.org/W4393152626",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2097837833",
      "name": "Liang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163957136",
      "name": "Qingyuan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097431157",
      "name": "Bo Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2953550412",
      "name": "Xiangxiang Chu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097837833",
      "name": "Liang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163957136",
      "name": "Qingyuan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097431157",
      "name": "Bo Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2953550412",
      "name": "Xiangxiang Chu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6893640197",
    "https://openalex.org/W2156150815",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W4381573231",
    "https://openalex.org/W2963259903",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W4313484599",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4377297674",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4324297016",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W4379260375",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4378770729",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4385569937",
    "https://openalex.org/W4281758439",
    "https://openalex.org/W4362598949",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4327810129",
    "https://openalex.org/W4377371819",
    "https://openalex.org/W4321636575",
    "https://openalex.org/W4281651027",
    "https://openalex.org/W2981040094",
    "https://openalex.org/W2953356739"
  ],
  "abstract": "As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower-bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our method demonstrates significant improvements in both weight-only quantization and joint quantization of weights and activations, surpassing existing PTQ methods. On GLM-130B and OPT-66B, our method even achieves the same level of accuracy at 2-bit quantization as their float ones. Our simple and effective approach makes it more practical for real-world applications.",
  "full_text": "Norm Tweaking: High-Performance Low-Bit Quantization of Large Language\nModels\nLiang Li, Qingyuan Li, Bo Zhang, Xiangxiang Chu\nMeituan\nfliliang58,liqingyuan02,zhangbo97,chuxiangxiangg@meituan.com\nAbstract\nAs the size of large language models (LLMs) continues to\ngrow, model compression without sacriﬁcing accuracy has\nbecome a crucial challenge for deployment. While some\nquantization methods, such as GPTQ, have made progress in\nachieving acceptable 4-bit weight-only quantization, attempts\nat lower-bit quantization often result in severe performance\ndegradation. In this paper, we introduce a technique called\nnorm tweaking, which can be used as a plugin in current PTQ\nmethods to achieve high precision while being cost-efﬁcient.\nOur approach is inspired by the observation that rectifying the\nquantized activation distribution to match its ﬂoat counterpart\ncan readily restore accuracy for LLMs. To achieve this, we\ncarefully design a tweaking strategy that includes calibration\ndata generation and channel-wise distance constraint to up-\ndate the weights of normalization layers for better generaliza-\ntion. We conduct extensive experiments on various datasets\nusing several open-sourced LLMs. Our method demonstrates\nsigniﬁcant improvements in both weight-only quantization\nand joint quantization of weights and activations, surpass-\ning existing PTQ methods. On GLM-130B and OPT-66B,\nour method even achieves the same level of accuracy at 2-\nbit quantization as their ﬂoat ones. Our simple and effective\napproach makes it more practical for real-world applications.\nIntroduction\nRecently, OpenAI’s ChatGPT (OpenAI 2023b) has demon-\nstrated outstanding performance on text generation, spark-\ning a research frenzy in large language models (LLMs).\nSome of the most famous LLMs include GPT series like\nGPT-3 (Brown et al. 2020), GPT-4 (OpenAI 2023a), and\nPaLM (Chowdhery et al. 2022), Ernie (Zhang et al. 2019).\nOpen-sourced ones like GLM (Du et al. 2021), BLOOM\n(Laurenc ¸on et al. 2022), OPT (Zhang et al. 2022) and\nLLaMa series (Touvron et al. 2023) have remarkably accel-\nerated the development of the community. In essence, LLMs\nare generative models that are trained on excessively large\namounts of text data that mimics how humans use language,\nand they exhibit superior zero-shot performance in a large\nrange of natural language processing (NLP) tasks, includ-\ning language translation, sentiment analysis, text classiﬁca-\ntion, and question answering, etc. They are increasingly be-\nCopyright c\r 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n0 5 10 15 20 25 30\nLayer\n0.00\n0.01\n0.02\n0.03\nGPTQ\nNorm T weaking\nFigure 1: Activation distribution of norm tweaking is closer\nto its ﬂoat counterpart compared with GPTQ. A batch size\nof 128 is used to compute the mean difference \u0001\u0016.\ning used in applications such as chatbots, language under-\nstanding, and speech recognition systems.\nNevertheless, due to the large scale (normally tens of bil-\nlions or even trillions of parameters) of large language mod-\nels, it causes large resource consumption even for deploy-\nment. Taking GPT-3 as an example, it has 175 billion param-\neters and uses FP16 for inference, occupying approximately\n350 GB of GPU memory, which means at least 8 NVIDIA\nA100 GPUs are needed to support the deployment of a sin-\ngle model. Therefore, it is more than necessary to reduce the\ncost.\nModel quantization, as a classic method of model com-\npression, can effectively reduce the memory consumption of\nLLMs. For example, when using 4-bit quantization, GPT-3\ncan be deployed on 2 A100 GPUs due to one-fourth of mem-\nory reduction. GPTQ (Frantar et al. 2022) is currently the\nmost prominent low-bit weight-only quantization method,\nwhich can compress some LLMs to 4-bit while maintain-\ning acceptable precision degradation. Smoothquant (Xiao\net al. 2023) could achieve 8-bit quantization for both weights\nand activations, by equivalently transferring the multipli-\ncation factors in weights and activations. However, these\nmethods suffer from signiﬁcant accuracy loss when applied\nto lower-bit quantization, such as 2-bit weight-only quan-\ntization using GPTQ or W4A8(4-bit for weights and 8-bit\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18536\nfor activation) quantization using SmoothQuant. Accord-\ning to ZeroQuant-V2 (Yao et al. 2023), LLaMa-65B with\nGPTQ 2-bit quantization, the accuracy on the LAMBADA\ndataset (Paperno et al. 2016) decreased from 79% to 57%,\nfor which reason it proposes a quantization-aware training\nmethod based on low-rank compensation. However, it not\nonly requires additional training costs but also introduces\nadditional parameters, which is not a viable choice for efﬁ-\ncient deployment.\nTo improve the lower-bit performance of quantized mod-\nels, we ﬁrst draw an intuition that LLMs have sufﬁcient\nnoise resilience, such that it calls a tender solution for pre-\ncision recovery. It is demonstrated in Prompt Quantiza-\ntion (Xu et al. 2023) that for a compressed LLM, provid-\ning an appropriate prompt can yield high-precision genera-\ntion without updating parameters. ZeroQuantV2 (Yao et al.\n2023) indicates that the larger parameter a model has, the\nless degradation will the quantization have. Next, we ex-\nplore why LLMs behave poorly on lower-bit quantization\nfrom a numerical perspective. We observe that the distribu-\ntion of the quantized model’s output tensor deviates signiﬁ-\ncantly from that of the original ﬂoat model, and it accumu-\nlates layer by layer to become uncontrollable, see Figure 1.\nTherefore a question is raised: could we improve the perfor-\nmance of the quantized model by simply matching its activa-\ntion distribution to that of the ﬂoat model?\nTo achieve this goal, we propose a method called Norm-\nTweaking to enhance the quantized model by slightly ad-\njusting the parameters of the LayerNorm layer to tweak\nthe quantized distribution. This method can be widely ap-\nplied to a variety of quantization methods, achieving signif-\nicant accuracy improvement with only minimal additional\ncomputational cost. Our method is evaluated on various\nmodels and datasets, and the results indicate that Norm-\nTweaking consistently improves the performance of GPTQ\nand SmoothQuant on different large language models. For\nLLaMa models, Norm-Tweaking demonstrates a general\nperformance enhancement over GPTQ on diverse datasets,\nwith a notable accuracy improvement of approximately 10%\non the LAMBADA dataset. Moreover, during subjective\nevaluations of quantized models, we observe that Norm-\nTweaking excels in preserving the general semantic ability\nof extremely low-bit quantized models. In a nutshell, our\ncontribution is three-fold,\n1. Firstly, we discover that large language models in gen-\neral are robust against weight distortion, merely slight\npartial weight adjustment could recover its accuracy even\nin extreme low-bit regime. It is unnecessary to adopt\nheavy quantization-aware training or other sophisticated\ntechniques.\n2. Secondly, we carefully devise an LLM tweaking strategy\ncomposed of three parts (1) adjusting only the parame-\nters of LayerNorm layers while freezing other weights,\nwhich can be applied to nearly all LLMs since it is per-\nvasively used; (2) constrained data generation enlight-\nened by LLM-QAT (Liu et al. 2023) to obtain the re-\nquired calibration dataset, which effectively reduces the\ndependence on speciﬁc datasets during model quantiza-\ntion and ﬁne-tuning process; (3) a channel-wise tweaking\nloss to speciﬁcally minimize the difference of the activa-\ntion distribution of the quantized model to that of its ﬂoat\ncounterpart.\n3. Last but not least, our technique is simple and effec-\ntive with minimal resource consumption which can be\nused as a plugin in other PTQ methods. Extensive ex-\nperiments demonstrate that our proposed norm-tweaking\nmethod achieves high-performance quantization for gen-\neral LLMs, surpassing algorithms such as GPTQ.\nRelated Work\nLLM Optimization. As most LLMs are based on Trans-\nformer (Vaswani et al. 2017), which is a typical memory-\nintensive architecture. The inference bottleneck lies more\nin the GPU’s memory bandwidth, hence reducing its mem-\nory access can signiﬁcantly improve the inference speed.\nFlashAttention (Dao et al. 2022), DeepSpeed (Aminabadi\net al. 2022), and FlexGen (Sheng et al. 2023) propose\noptimized transformer implementations or efﬁcient mem-\nory management to improve the throughput of LLMs.\nOthers achieve this goal through model pruning, such as\nLoSparse (Li et al. 2023), SparseGPT (Frantar and Alistarh\n2023), and LLM-Pruner (Ma, Fang, and Wang 2023). Min-\niMoE (Zhang et al. 2023) obtains smaller models with high\nperformance through distillation.\nPost-training Quantization. Weight-only quantization\nschemes like GPTQ (Frantar et al. 2022) compresses and\nstores weight parameters, and decompresses them to FP16\nfor inference during calculation. This approach can effec-\ntively reduce the proportion of memory access time during\ninference while maintaining model accuracy. LLM.int8()\n(Dettmers et al. 2022) proposes to use ﬂoat calculation or to\nadjust the multiplication operations of LayerNorm to re-\nduce quantization loss. Smoothquant (Xiao et al. 2023) pro-\nposes a method to reduce the activation ranges by equiv-\nalently transferring the multiplication factors in weights\nand activations. GPTQ (Frantar et al. 2022) reconstruct\nweights based on the method in OBS (Hassibi, Stork, and\nWolff 1993) via Hessian matrix to reduce quantization er-\nror. GPTQ has been widely applied in many scenarios where\nsome LLMs could achieve high precision at 4-bit quantiza-\ntion. RPTQ (Yuan et al. 2023) and AWQ (Lin et al. 2023)\nfurther improve this method.\nQuantization-aware Training. Another method to im-\nprove the performance of the quantized models is\nquantization-aware training (QAT), which is to ﬁne-tune the\nquantized models to match the original ﬂoat models. QAT is\nwidely studied in convolutional networks, but it encounters\nsigniﬁcant setbacks in large language model quantization.\nAs the training process of LLMs consumes a huge amount\nof text data (usually in the order of trillions of tokens), how\nto efﬁciently ﬁne-tune the quantized LLMs while maintain-\ning their general knowledge and generalization ability re-\nmains an open question. To name a few attempts, LLM-QAT\n(Liu et al. 2023) requires the update the whole parameters\nof the LLMs on a set of at least 100k sampled data. Zero-\nQuantV2 (Yao et al. 2023) introduces a Low Rank Com-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18537\npensation to achieve parameter-efﬁcient ﬁne-tuning, but this\napproach neither eliminates the need for a large amount of\ntraining data nor avoids the introduction of additional pa-\nrameters.\nMethod\nMotivation\nBased on the observation shown in Figure 1, the difference\nbetween the output tensors of each layer in the quantized\nmodel and its ﬂoating counterpart accumulates, while the\noutput of the quantized model gradually deviates from the\nquantization-friendly zero-mean distribution. This is some-\nwhat expected since LayerNorm magniﬁes the outlier\n(Xiao et al. 2023) and no measure is taken to deal with\nthis effect. Hence when we iteratively update the quantized\nweights of each layer using GPTQ, it inevitably disrupts the\nzero-mean distribution of the current layer and increases the\ndeviation.\nTo this end, we aim to improve the quantized model’s per-\nformance by adjusting its output distribution to approach\nthat of its ﬂoat counterpart. Complete ﬁne-tuning of the\nquantized model through QAT is a direct approach, but due\nto the large number of parameters in the LLM model and the\nenormous amount of required training data, QAT becomes\nan impractical option. In order to achieve high performance\nthe quantized model within the time constraint, we are\ndriven to improve current PTQ methods. As LayerNorm\nis highly effective in managing the distribution, we choose\nto adjust this layer to achieve the goal. It is also econom-\nical to update its weight considering the small number of\nparameters. Furthermore, nearly all mainstream LLMs use\nLayerNorm or similar operators, so that the method can\nbe applied universally to a variety of large language models.\nTherefore, our core objective can be summarized as adjust-\ning the parameters of LayerNorm to make the output dis-\ntribution of the quantized model approach that of the ﬂoat\nmodel, which can be expressed formally as,\nargmin\nWln\nLdist(T(X); ^T(X)) (1)\nwhere T(XjWattn;Wmlp;Wln) denotes a Transformer\nblock, including the Attention module, MLP module, Layer-\nNorm layer, and activation functions, and ^T(X) represents\nits quantized version. Ldist(\u0001) denotes the distribution loss\nfunction between the quantized and ﬂoat models. Our goal\nis then to design a strategy to optimize ^Wln to minimize\nLdist(\u0001), while keeping ^Wattn and ^Wmlp frozen.\nNorm Tweaking\nMotivated by the above analysis, we propose a PTQ method\nfor LLMs, called Norm-Tweaking, to quickly restore mod-\nels’ performance by slightly tweaking LayerNorm layers of\nthe quantized model. Norm tweaking serves as a plugin to be\neasily embedded into other quantization methods. Here, we\ntake GPTQ as an example and present a weight-only post-\nquantization algorithm pipeline, as shown in Algorithm 1.\nFirstly, we use the LLM model to generate a set of text data\nAlgorithm 1: Norm-Tweaking\nInput: Pre-trained LLM model\nOutput: Quantized LLM model\n1: Generate calibration dataset (n samples = 128,\ntoken length= 2048) using pre-trained LLM model\n2: for eachlayer-lin the Transformer structure (L layers\ntotal) do\n3: if l= 0 then\n4: use calibration data as input\n5: else\n6: use last output qOutl\u00001 as input\n7: end if\n8: Calculate the ﬂoat output fOutl\n9: Quantize the weights of layer l\n10: Freeze all Linear’s weights in layerl\n11: for eachitfor total Iters do\n12: Calculate the ﬂoat output qOutl\n13: Calculate Ldist between fOutl and qOutl\n14: Backward and update LayerNorms’ parameters\n15: end for\n16: end for\n17: Get the high-performance quantized LLMs\nas for calibration (explained in detail in the section on Cal-\nibration Dataset Generation), instead of directly sampling\nfrom real datasets. Next, we iteratively process each trans-\nformer layer, quantizing and updating the weights of the Lin-\near layers, just like GPTQ. Finally, we compute a channel-\nwise loss based on the difference between the distribution\nof quantized output and ﬂoat output. We then use stochastic\ngradient descent to update the parameters ofLayerNorm in\nthis layer, forcing the activation distribution of the quantized\nmodel to mimic that of the ﬂoat model. During this process,\nthe rest parameters of the current layer such as Linear are\nfrozen and do not participate in the weight update.\nAlthough only the parameters of LayerNorm are up-\ndated, our process is distinct from parameter-efﬁcient ﬁne-\ntuning strategies. It should be noted that the parameters\nof the LayerNorm layer are very sensitive and excessive\ntuning can seriously damage the quantized models’ perfor-\nmance (see Table 6). We slightly update the LayerNorm with\na relaxed constraint, whose goal is to make the quantized\nmodels’ distribution approaching that of ﬂoat ones. This is\nprecisely why we deﬁnite our method as a tweaking, rather\nthan a ﬁne-tuning.\nAt a glimpse, we carefully design the entire tweaking pro-\ncedure to achieve our goal. For example, we use a very small\nnumber of iterations during tuning, typically only one itera-\ntion on the calibration text is required. We also adopt a small\nlearning rate and design a step scheduler to assign different\nlearning rates for the subsequent layers. In addition, our cali-\nbration data generation and the design of the distribution loss\nfunction harmoniously resonate with our tweaking principle.\nCalibration Data Generation\nA crucial problem that matters in the generalization ability\nof the quantized model is the appropriate choice of calibra-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18538\ntion data. We found that different calibration datasets sub-\nstantially affect the performance of the quantized model. It\nusually performs well on the calibration dataset, but it gen-\nerally suppresses the performance on other datasets. LLM-\nQAT (Liu et al. 2023) demonstrated that training the quan-\ntized model with a speciﬁc dataset further damages LLMs’\ngeneralization ability. Therefore, we adopt a data genera-\ntion scheme following LLM-QAT that utilizes the generated\ndata of the model itself for calibration instead of a speciﬁc\nreal dataset. The beneﬁt is that thus-generated data can ef-\nfectively activate the neurons of the LLM which facilitates\nmodel quantization. It also enjoys rich semantic information\nstored in the model and it is less biased towards a speciﬁc\ndataset which is more generalizable.\nOur generation process is a variant of that of LLM-QAT.\nFirstly, a random token is taken from a list of given lan-\nguages and then a two-stage pattern proposed by LLM-\nQAT is employed where the picked token is fed as the in-\nput prompt to let LLMs generate subsequent tokens. We en-\nhance this data generation process by enforcing a restriction\non the ﬁrst random token. We observe a signiﬁcant disparity\nin terms of proportions between the language categories in\nthe training corpus and tokenization vocabulary. As shown\nin Table 1, taking BLOOM as an example, it is trained on\na total of 1.61 TB of text, with the top ﬁve language types\naccounting for over 75% of the corpus. If we consider the\nrelated corpus (e.g. zht as a traditional version of zhs)\nand derivative ones (e.g. programming languages) of these\nﬁve language types, the proportion exceeds 90%. In con-\ntrast, there are 250680 tokens in the tokenization vocabu-\nlary, whose total number of tokens corresponding to these\nﬁve languages only accounts for 17%. Therefore, the ﬁrst\ntoken of input directly affects the language type of the gen-\nerated text. If we randomly select from the entire vocabulary,\nwe cannot get appropriate calibration data that matches the\ntraining corpus. To this very purpose, we restrict the ﬁrst ran-\ndom token to be selected only from the language categories\nin the list of top languages that have the highest proportion,\nwhich turns out to effectively improve the generalization of\nthe quantized model on different datasets (Table 8).\nLanguage en zhs fr es pt\nCorpus(MB) 485.0 261.0 208.2 175.1 79.3\nV ocab 7943 380 15483 6999 8669\nTable 1: Text size and token count for the top 5 languages.\nChannel-wise Distribution Loss\nTo guide the direction of parameter updates, it is crucial to\ndesign a corresponding loss function. In this context, we\naim to minimize the difference between the activation dis-\ntribution of the quantized model and its original ﬂoat model.\nFirstly, as the activation distribution of LLMs exhibits sig-\nniﬁcant differences along the channel dimension, with some\nchannels displaying extreme values (referred to as outliers)\n(Xiao et al. 2023), it poses great challenges for the quanti-\nzation process. In order to preserve the differences between\nchannels while tweaking the model parameters and to retain\nthe original model capacity as much as possible, we enforce\na channel-wise constraint. Secondly, a strict alignment of\nthe point-wise activation values between quantized and ﬂoat\nmodels may result in overﬁtting to calibration data, thereby\ncompromising the generalization performance across differ-\nent datasets. Therefore, we adopt a more relaxed alignment\nstrategy by directly aligning the mean and variance between\neach channel, instead of strictly aligning the targets at the\npoint-wise level. As a result, we introduce a channel-wise\ndistribution loss function, as shown below:\nLdist = 1\nC\nCX\nc=1\n(\n\r\r\u0016c\nf \u0000\u0016c\nq\n\r\r\n2 +\n\r\n\r(\u001bc\nf)2 \u0000(\u001bc\nq)2\r\n\r\n2) (2)\nwhere C is the number of channels, \u0016 and \u001b represent the\nmean and variance of each channel in tensorT, the subscript\nf and q indicates the ﬂoat and quantized model.\nFurthermore, current algorithms like GPTQ iteratively\nquantize LLMs layer by layer, whose deviation of intermedi-\nate activation distributions gradually accumulates, resulting\nin large errors in the ﬁnal layers. Thus, we apply a layer-level\nscheduler to adjust the learning rate of each layer during the\ntweaking process where we simply adopt a step increase to\nallocate different learning rates on different layers.\nlri = lr0 \u0003(1 + scale\u0003(i=L)) (3)\nExperiments\nSettings\nWe tested our method on LLMs of different sizes and types,\nincluding GLM (Du et al. 2021), BLOOM (Laurenc ¸on et al.\n2022), OPT (Zhang et al. 2022) and LLaMa series (Touvron\net al. 2023). Our Norm-Tweaking results presented in the\npaper, unless otherwise noted, are obtained using weight-\nonly quantization based on the GPTQ algorithm. Consider-\ning the kernel support for deployment frameworks, such as\nFasterTransformer (NVIDIA 2023), we use symmetric per-\nchannel quantization. In the tweaking process, we choose\nthe Adam optimizer (Kingma and Ba 2015) to update the\nLayerNorm parameters of LLMs or the RMSNorm (Zhang\nand Sennrich 2019) parameters of LLaMA. The learning rate\nneeds to be carefully set. A large learning rate would dam-\nage the ﬁnal results. In our experiments, we typically use a\ngrid search to obtain the optimal learning rate, with an initial\nvalue set at 1e-5.\nOur primary experimental evaluations are performed on\nthe LAMBADA dataset (Paperno et al. 2016), which is\nrenowned for its high demand for the understanding abil-\nity of natural language. This dataset necessitates a compre-\nhensive understanding of the entire text to provide precise\nanswers. To further substantiate the generalization of our\nmethod on different datasets, we employed Benchmark Har-\nness (Gao et al. 2021) to conduct tests on a broader spec-\ntrum of datasets, encompassing HellaSwag (Zellers et al.\n2019), PIQA (Bisk et al. 2020), WinoGrande (Sakaguchi\net al. 2021), OpenBookQA (Mihaylov et al. 2018), and some\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18539\nModel FP16 W4 W2\nGPTQ Norm-Tweaking GPTQ Norm-Tweaking\nBLOOM-7b1 (Laurenc ¸on et al. 2022) 57.6751 55.0615 57.4811 (2.4196\") 33.4714 37.4539 (3.9825\")\nBLOOM-176b 67.7081 67.1842 67.6887 (0.5045\") 63.0507 65.6317 (2.581\")\nLLaMa-7b (Touvron et al. 2023) 73.5106 71.8999 72.4820 (0.5387\") 11.8766 21.3856 (9.509\")\nLLaMa-65b 79.0996 78.0516 79.2354 (1.1838\") 57.1512 67.4753 (10.3241\")\nGLM-130b (Du et al. 2021) 69.4159 69.2218 69.1964 (0.0254#) 67.6499 69.4293 (1.7794\")\nOPT-66b (Zhang et al. 2022) 73.2971 73.0060 73.8405 (0.8345\") 71.3953 73.4912 (2.0959\")\nTable 2: The quantized accuracy results of LLMs on the LAMBADA dataset. W4/2: 4/2-bit weights-only quantization.\ndatasets from the General Language Understanding Evalu-\nation (GLUE) benchmark. We also use WikiText-2 (Mer-\nity et al. 2016), PTB (Marcus et al. 1994), C4 (Raffel\net al. 2020) in Table 5, to provide some demonstrations of\ntext generated by quantized LLMs, which helps to more\nintuitively visualize the performance recovery of Norm-\nTweaking. Following the settings in GPTQ, we used a cal-\nibration dataset size with n\nsamples=128, with the maxi-\nmum sequence length token length=2048.\nTweaking Cost\nWe demonstrate that Norm-Tweaking incurs extremely low\ncosts. Taking BLOOM (Laurenc ¸on et al. 2022) as an exam-\nple, given the hidden dimension ash, each transformer block\ngenerally has 4 Linear layers, with a total parameter count of\nabout 12h2 + 9h, while LayerNorm has two layers, with a\nparameter count of 4h. The hidden dimension his typically\nvery large (for example, 14336 for BLOOM-176B), so the\nparameter quantity of the Linear layer is much larger than\nthat of the LayerNorm layer (on the order of 107 \u0018109).\nIn addition, to avoid overﬁtting on speciﬁc calibration data,\nwe only perform one iteration on each sample of text. There-\nfore, the proposed Norm-Tweaking method has minimal re-\nsource consumption and extra time.\nModel BLOOM-7B LLaMA-7B OPT-13B\nGPTQ 19.6 15.5 27\nGPTQ+NT 22.8 27.3 46.6\nTable 3: Quantization runtime measured in minutes for\nGPTQ and Norm-Tweaking on various LLMs.\nTable 3 shows the time cost taken to quantize LLMs us-\ning GPTQ and Norm-Tweaking. All experiments were con-\nducted on a single NVIDIA A100 GPU. The additional time\ncost of Norm-Tweaking is less than the time cost of GPTQ\nitself, and our method still remains within the category of\npost-quantization. For BLOOM-7B, the time cost increase\naccounts for only 16%.\nResults on LAMBADA\nAs shown in Table 2, our proposed model quantization\nmethod is applied to LLMs at different scales, including\nBLOOM, LLaMa, GLM, and OPT, where the accuracy\nof each quantized model is evaluated on the LAMBADA\ndataset and is compared comprehensively with GPTQ. In\naddition, we also conduct experiments on 2-bit weight-only\nMethod Mode BLOOM-7B OPT-13B\nw/o PTQ FP16 57.6751 69.0860\nRTN W4A16 48.3602 62.7402\nRTN+NT W4A16 51.5622 64.7584\nSmoothQuant W4A8 53.9492 68.6590\nSmoothQuant+NT W4A8 54.5896 69.7264\nTable 4: Norm-Tweaking (NT) on various LLM PTQ meth-\nods. Note for OPT-13B, W5A8 is used for SmoothQuant and\nSmoothQuant+NT.\nquantization with a ﬁne-grained quantization with a group\nof 64. Our Norm-Tweaking post-quantization method gen-\nerally outperforms the GPTQ algorithm in terms of model\naccuracy. In 2-bit quantization, the GPTQ algorithm caused\nsigniﬁcant accuracy loss for most models, making a large\npart of results are not acceptable. However, our proposed\nquantization method is able to achieve accuracy perfor-\nmance close to the ﬂoating-point model even on the GLM-\n130B and OPT-66B models, and it outperforms GPTQ by\nnearly 10% on LLaMa.\nComparison with RTN and SmoothQuant\nWe integrate Norm-Tweaking into two commonly used post-\nquantization methods, round-to-nearest (RTN) (Yao et al.\n2022; Dettmers et al. 2022) and SmoothQuant (Xiao et al.\n2023), to verify its general effectiveness across different al-\ngorithms. Several LLMs are quantized in different modes\nand evaluated on the LAMBADA dataset, results are shown\nin Table 4. Speciﬁcally, we apply 4-bit weight-only quan-\ntization to RTN, and W4A8 (4-bit for weight and 8-bit for\nactivation) quantization to the SmoothQuant. Note OPT-\n13b is severely compromised when using SmoothQuant\nW4A8 quantization, resulting in an accuracy of 0. The\nresults demonstrate the universality of Norm-Tweaking,\nas it provides stable performance improvements for dif-\nferent quantization methods, including RTN, GPTQ, and\nSmoothQuant, as well as for different quantization modes,\nincluding weight-only and both weight and activation.\nBenchmark Harness\nWe benchmark the 2-bit quantized LLaMa-7b and LLaMa-\n65b on the few-shot evaluation framework LM Evaluation\nHarness (Gao et al. 2021) in Table 7. In short of space and\ntime, we report the result on a selected list of tasks. Our\nproposed method generally outperforms GPTQ 2-bit results,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18540\nModel BLOOM -176B\nFP16 Beijing is the capital of China. Beijing is also one of the world’s oldest cities. It is also one\nof the most modern and beautiful cities of China. As of 2016, it had a population of over\n20 million. Beijing is considered the most beautiful city in China. It is known for its many\nbeautiful and ancient buildings and sites. These include the Forbidden City, the Temple of\nHeaven, and the Great Wall of China.\nGPTQ (4-bit) Beijing is the capital of China. There are 13.7 million people in the city of Beijing. The city\nis divided into 16 districts, and the most important districts are Dongcheng,Xicheng,Haidian,\nChaoyang,Fengtai, Shijingshan and Daxing. The city is located in the north-east of China.\nThe city is located in the north-east of China, close to the Paciﬁc coast. The city is located on\nthe left bank of the Beijing River.\nNorm-Tweaking (4-bit) Beijing is the capital of China, and the city is one of the most popular tourist destinations in\nthe world. You can ﬁnd here a lot of interesting places. You can visit the Forbidden City. It\nis located in the center of the city and is considered the most signiﬁcant attraction. Also you\ncan visit the Temple of Heaven. This place is located a few kilometers from the center of the\ncity. You can admire the beauty of the Beijing city from the observation deck of the World\nTrade Center.\nModel LLaMa-65B\nFP16 Beijing is the capital of China, and also one of the largest cities in the world. It is a modern\ncity that has successfully managed to retain its ancient Chinese roots and the essence of its\nculture. A city of over 21 million people, Beijing is a major hub for international business,\nand as such attracts a large number of travelers.\nGPTQ (2-bit) Beijing is the capital of China, and has a rich history datin back to 5th in 1910s. Peking was\nthe old capital in 1910s and renamed as Beijing in 1913, and became capital in 1972. Beijing\nis an interesting city, withthe Forbidden City in the Forbidden City, which is a world heritage\nsite.\nNorm-Tweaking (2-bit) Beijing is the capital of China. The country has a population of around 1.3 billion Chinese\npeople. The country is one of the leading exporters in the world, and also one of the leading\nimporters of the world. China is one of the leading manufacturers of the world. China is a\nlarge country, and is one of the largest countries in the world.\nTable 5: Example of 4-bit quantized BLOOM-176B and 2-bit quantized LLaMa-65B text generation on the speciﬁed prompt\n“Beijing is the capital of China”. The text in italic is either grammatically wrong or counterfactual.\nIters 1 2 5\nAcc 57.4811 55.7539 52.1056\nIters 10 20 50\nAcc 46.8465 32.3307 11.3332\nTable 6: Effect of tweaking iterations.\nwith some even better than FP16 accuracy. This again proves\nthe robustness of our method and strong generalizability to\na wide range of datasets. We discuss the performance varia-\ntions among datasets in the appendix.\nSubjective Evaluation\nSubjective evaluation of the generated results is a common\nand effective method for evaluating the performance of lan-\nguage models such as LLM. In Table 5, the FP16 mode of\nLLaMa-65B and BLOOM-176B, as well as quantized model\nwith GPTQ and Norm-Tweaking are evaluated through the\nlens of human evaluation on generated results. With the\nsame input prompt, it can be seen that different models\ngive signiﬁcantly different results, especially the GPTQ low-\nbit quantization model, which is subject to obvious errors.\nThese errors mainly manifest either grammatical errors (e.g.\nmisspelled words or incorrect use of punctuation or spaces),\nlogical errors in the language (e.g. repeated statements), and\nfactual errors (e.g. birth date). Nevertheless, adopting the\nquantization method proposed in this paper, the quantized\nmodel obtained under the same settings does not have these\nobvious errors in the output results, suggesting the robust-\nness of our quantization method.\nAblation\nTweaking Iterations.We investigate the effect of the num-\nber of iterations for Norm-Tweaking and report the results\nof BLOOM-7B tested on LAMBADA dataset in Table 6.\nIt turns out that increasing the iteration numbers during the\ntweaking process signiﬁcantly damages the model’s accu-\nracy performance. This is as expected since the parameters\nof LayerNorm are highly sensitive, where excessive itera-\ntions can easily lead to the collapse of model performance.\nThis is also why we recommend tweaking instead of tuning,\nwhich also clearly distinguishes us from those QAT methods\nsuch as LLM-QAT.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18541\nModel (Precision) HellaSwag PIQA WinoGrande OpenBookQA RTE MRPC QNLI BOOLQ CB COPA WIC\nLLaMa-7b (FP16) 56.44 78.35 67.09 28.00 53.07 68.38 49.57 73.15 33.93 84.00 50.00\nw/ GPTQ (2-bit) 30.73 58.49 48.54 13.20 53.43 49.75 51.53 52.02 37.50 68.00 49.53\nw/ Norm-Tweak (2-bit) 34.03 61.81 52.17 15.80 51.26 54.66 50.61 56.91 48.21 68.00 51.41\nLLaMa-65b (FP16) 63.97 81.66 77.19 36.40 71.48 68.38 54.00 82.32 64.29 91.00 58.46\nw/ GPTQ (2-bit) 45.99 72.20 60.77 23.20 60.65 64.95 52.35 66.33 39.29 82.00 49.84\nw/ Norm-Tweak (2-bit) 52.15 74.04 67.24 26.80 61.37 68.38 49.60 76.15 30.36 93.0 50.00\nBLOOM-176b (FP16) 55.91 78.78 70.32 32.20 62.09 34.80 51.38 69.85 71.43 87.00 48.43\nw/ GPTQ (2-bit) 50.04 75.73 68.67 27.40 57.40 54.66 49.86 66.64 46.43 81.00 50.00\nw/ Norm-Tweak (2-bit) 54.64 78.51 71.51 32.00 58.84 35.29 51.42 71.74 48.21 87.00 48.90\nOPT-66b (FP16) 56.45 78.62 68.82 30.40 59.93 34.07 52.24 69.72 39.29 86.00 50.00\nw/ GPTQ (2-bit) 49.72 75.35 65.90 25.80 54.51 45.34 53.08 64.68 41.07 86.00 50.47\nw/ Norm-Tweak (2-bit) 49.81 75.41 64.25 26.80 54.51 68.38 49.53 69.88 41.07 85.00 50.47\nTable 7: The quantized accuracy results of LLMs on the LM Evaluation Harness benchmark.\nCalibration Data WikiText2 PTB C4\nWikiText2 12.16 21.17 18.28\nPTB 12.51 20.72 18.42\nC4 12.28 20.97 18.16\nRandom 13.25 22.82 19.60\nGenData V1 12.43 21.25 18.34\nGenData V2 12.32 20.95 18.28\nTable 8: Effects of different calibration datasets. V1 follows\nLLM-QAT, and V2 is our improved version.\nCalibration Data.Table 8 shows how the choice of cali-\nbration dataset signiﬁcantly affects the performance of quan-\ntized models on different datasets. We use three real datasets\nWikiText2 (Merity et al. 2016), PTB (Marcus et al. 1994),\nand C4 (Raffel et al. 2020), as well as random data and\ngenerated data, as calibration sets to quantize the BLOOM-\n7B model using GPTQ. And we give the perplexity (PPL)\non WikiText2, PTB, and C4 respectively, with lower PPL\nindicating better performance. The ﬁrst three rows show\nthe strong correlation between GPTQ and the calibration\ndataset, that is, a LLM calibrated on a certain dataset per-\nforms better on that dataset, but correspondingly worse on\nother datasets.\nTo avoid the dependence on real data, we randomly sam-\nple data from Gaussian distribution with the same mean and\nvariance of the real data for calibration. However, the per-\nformance of the quantized model was extremely poor. We\nguess that this is because random data is without actual se-\nmantic meaning, which cannot produce positive activations\nfor LLMs when being used as a calibration dataset. We ex-\nploit the LLM itself to generate calibration data. It can pro-\nduce meaningful text and effectively activate the model. The\nresults show that using generated data for calibration can im-\nprove the performance of the quantized model, and it does\nnot show dependence on speciﬁc data. Using the language\nscope restriction proposed in this paper can further improve\nthe quality of generated data.\nLoss Function.To showcase the importance of our pro-\nposed channel-wise distribution loss LDist, we compare it\nwith several different loss functions like mean square error\nModel LMSE LKL LDist\nBLOOM-7b 55.8704 56.2779 57.4811\nLLaMa-7b 72.3850 71.7446 72.4820\nOPT-13b 68.3291 68.2709 68.7173\nTable 9: Comparison of different loss functions.\nLMSE and Kullback-Leibler Divergence loss LKD (Hin-\nton, Vinyals, and Dean 2015), the result is shown in Table 9\nwhere the proposed LDist works best in all cases. This re-\nsult echos our analysis that channel-wise treatment is nec-\nessary (better than LKL) to deal with outliers while point-\nwise alignment (LMSE) harms the performance. As a col-\nlaborative result of multiple components in Norm-Tweaking,\nthe difference of quantized activation distribution to its ﬂoat\ncounterpart is largely narrowed, as shown in Figure 1. This\nobservation fairly answers our original question that mini-\nmizing the activation distribution of LLMs between two pre-\ncisions readily renders high performance, even for extremely\nlow-bit quantization.\nConclusion\nIn conclusion, we have proposed a novel quantization com-\npression method for large-scale language models (LLM) that\nsurpasses existing state-of-the-art methods such as GPTQ\nand SmoothQuant. Our method is characterized by generat-\ning generalizable calibration data and tweaking the normal-\nization layer with channel-wise distribution loss, enabling\nus to quickly achieve high-precision model quantization in\na low-cost manner. Notably, we have explored LLM model\ncompression at the 2-bit range, marking state-of-the-art per-\nformance. Our approach delivers a promising solution for re-\nducing the computational and storage costs associated with\nLLMs while maintaining their high performance.\nAcknowledgements\nThis work was supported by National Key R&D Program of\nChina (No. 2022ZD0118700).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18542\nReferences\nAminabadi, R. Y .; Rajbhandari, S.; Zhang, M.; Awan, A. A.;\nLi, C.; Li, D.; Zheng, E.; Rasley, J.; Smith, S.; Ruwase, O.;\nand He, Y . 2022. DeepSpeed Inference: Enabling Efﬁcient\nInference of Transformer Models at Unprecedented Scale.\narXiv:2207.00032.\nBisk, Y .; Zellers, R.; Gao, J.; Choi, Y .; et al. 2020. Piqa: Rea-\nsoning about physical commonsense in natural language.\nIn Proceedings of the AAAI conference on artiﬁcial intel-\nligence, volume 34, 7432–7439.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nIn Conference on Neural Information Processing Systems\n(NeurIPS).\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311.\nDao, T.; Fu, D. Y .; Ermon, S.; Rudra, A.; and R´e, C. 2022.\nFlashAttention: Fast and Memory-Efﬁcient Exact Attention\nwith IO-Awareness. arXiv preprint arXiv:2205.14135.\nDettmers, T.; Lewis, M.; Belkada, Y .; and Zettlemoyer, L.\n2022. LLM.int8(): 8-bit Matrix Multiplication for Trans-\nformers at Scale. arXiv preprint arXiv:2208.07339.\nDu, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.;\nand Tang, J. 2021. Glm: General language model pre-\ntraining with autoregressive blank inﬁlling. arXiv preprint\narXiv:2103.10360.\nFrantar, E.; and Alistarh, D. 2023. SparseGPT: Massive\nLanguage Models Can Be Accurately Pruned in One-Shot.\narXiv:2301.00774.\nFrantar, E.; Ashkboos, S.; Hoeﬂer, T.; and Alistarh, D. 2022.\nGptq: Accurate post-training quantization for generative\npre-trained transformers. arXiv preprint arXiv:2210.17323.\nGao, L.; Tow, J.; Biderman, S.; Black, S.; DiPoﬁ, A.; Fos-\nter, C.; Golding, L.; Hsu, J.; McDonell, K.; Muennighoff,\nN.; Phang, J.; Reynolds, L.; Tang, E.; Thite, A.; Wang, B.;\nWang, K.; and Zou, A. 2021. A framework for few-shot\nlanguage model evaluation.\nHassibi, B.; Stork, D. G.; and Wolff, G. J. 1993. Optimal\nbrain surgeon and general network pruning. In IEEE Inter-\nnational Conference on Neural Networks.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. In NIPS Deep Learning\nand Representation Learning Workshop.\nKingma, D. P.; and Ba, J. 2015. Adam: A method for\nstochastic optimization. International Conference on Learn-\ning Representations (ICLR).\nLaurenc ¸on, H.; Saulnier, L.; Wang, T.; Akiki, C.; del Moral,\nA. V .; Le Scao, T.; V on Werra, L.; Mou, C.; Ponferrada,\nE. G.; Nguyen, H.; et al. 2022. The BigScience Corpus:\nA 1.6 TB Composite Multilingual Dataset.\nLi, Y .; Yu, Y .; Zhang, Q.; Liang, C.; He, P.; Chen, W.;\nand Zhao, T. 2023. LoSparse: Structured Compression of\nLarge Language Models based on Low-Rank and Sparse\nApproximation. In Krause, A.; Brunskill, E.; Cho, K.; En-\ngelhardt, B.; Sabato, S.; and Scarlett, J., eds., Proceedings\nof the 40th International Conference on Machine Learning,\nvolume 202 of Proceedings of Machine Learning Research,\n20336–20350. PMLR.\nLin, J.; Tang, J.; Tang, H.; Yang, S.; Dang, X.; and Han,\nS. 2023. AWQ: Activation-aware Weight Quantization for\nLLM Compression and Acceleration. arXiv:2306.00978.\nLiu, Z.; Oguz, B.; Zhao, C.; Chang, E.; Stock, P.; Mehdad,\nY .; Shi, Y .; Krishnamoorthi, R.; and Chandra, V . 2023.\nLLM-QAT: Data-Free Quantization Aware Training for\nLarge Language Models. arXiv preprint arXiv:2305.17888.\nMa, X.; Fang, G.; and Wang, X. 2023. LLM-Pruner:\nOn the Structural Pruning of Large Language Models.\narXiv:2305.11627.\nMarcus, M.; Kim, G.; Marcinkiewicz, M. A.; MacIntyre,\nR.; Bies, A.; Ferguson, M.; Katz, K.; and Schasberger, B.\n1994. The Penn treebank: Annotating predicate argument\nstructure. In Human Language Technology: Proceedings of\na Workshop held at Plainsboro, New Jersey, March 8-11,\n1994.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R.\n2016. Pointer sentinel mixture models. arXiv preprint\narXiv:1609.07843.\nMihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A.\n2018. Can a suit of armor conduct electricity? a new\ndataset for open book question answering. arXiv preprint\narXiv:1809.02789.\nNVIDIA. 2023. FasterTransformer.\nOpenAI. 2023a. GPT-4 Technical Report.\narXiv:2303.08774.\nOpenAI. 2023b. Introducing ChatGPT.\nPaperno, D.; Kruszewski, G.; Lazaridou, A.; Pham, Q. N.;\nBernardi, R.; Pezzelle, S.; Baroni, M.; Boleda, G.; and\nFern´andez, R. 2016. The LAMBADA dataset: Word pre-\ndiction requiring a broad discourse context. arXiv preprint\narXiv:1606.06031.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. 2020. Explor-\ning the Limits of Transfer Learning with a Uniﬁed Text-to-\nText Transformer. Journal of Machine Learning Research,\n21(140): 1–67.\nSakaguchi, K.; Bras, R. L.; Bhagavatula, C.; and Choi, Y .\n2021. Winogrande: An adversarial winograd schema chal-\nlenge at scale. Communications of the ACM, 64(9): 99–106.\nSheng, Y .; Zheng, L.; Yuan, B.; Li, Z.; Ryabinin, M.; Fu,\nD. Y .; Xie, Z.; Chen, B.; Barrett, C.; Gonzalez, J. E.; Liang,\nP.; R´e, C.; Stoica, I.; and Zhang, C. 2023. FlexGen: High-\nThroughput Generative Inference of Large Language Mod-\nels with a Single GPU. arXiv:2303.06865.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efﬁcient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18543\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. InConference on Neural Information\nProcessing Systems (NeurIPS).\nXiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; and Han,\nS. 2023. Smoothquant: Accurate and efﬁcient post-training\nquantization for large language models. In International\nConference on Machine Learning, 38087–38099. PMLR.\nXu, Z.; Liu, Z.; Chen, B.; Tang, Y .; Wang, J.; Zhou,\nK.; Hu, X.; and Shrivastava, A. 2023. Compress, Then\nPrompt: Improving Accuracy-Efﬁciency Trade-off of LLM\nInference with Transferable Prompt. arXiv preprint\narXiv:2305.11186.\nYao, Z.; Aminabadi, R. Y .; Zhang, M.; Wu, X.; Li, C.; and\nHe, Y . 2022. ZeroQuant: Efﬁcient and Affordable Post-\nTraining Quantization for Large-Scale Transformers. arXiv\npreprint arXiv:2206.01861.\nYao, Z.; Wu, X.; Li, C.; Youn, S.; and He, Y . 2023.\nZeroQuant-V2: Exploring Post-training Quantization in\nLLMs from Comprehensive Study to Low Rank Compen-\nsation. arXiv:2303.08302.\nYuan, Z.; Niu, L.; Liu, J.; Liu, W.; Wang, X.; Shang, Y .;\nSun, G.; Wu, Q.; Wu, J.; and Wu, B. 2023. RPTQ: Reorder-\nbased Post-training Quantization for Large Language Mod-\nels. arXiv:2304.01089.\nZellers, R.; Holtzman, A.; Bisk, Y .; Farhadi, A.; and Choi,\nY . 2019. Hellaswag: Can a machine really ﬁnish your sen-\ntence? arXiv preprint arXiv:1905.07830.\nZhang, B.; and Sennrich, R. 2019. Root Mean Square Layer\nNormalization. arXiv:1910.07467.\nZhang, C.; Yang, Y .; Liu, J.; Wang, J.; Xian, Y .; Wang, B.;\nand Song, D. 2023. Lifting the Curse of Capacity Gap in\nDistilling Language Models. arXiv:2305.12129.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.\nOPT: Open pre-trained transformer language models. arXiv\npreprint arXiv:2205.01068.\nZhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu,\nQ. 2019. ERNIE: Enhanced language representation with\ninformative entities. arXiv preprint arXiv:1905.07129.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18544",
  "topic": "Tweaking",
  "concepts": [
    {
      "name": "Tweaking",
      "score": 0.9553380608558655
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.5637701153755188
    },
    {
      "name": "Computer science",
      "score": 0.497470885515213
    },
    {
      "name": "Norm (philosophy)",
      "score": 0.47970765829086304
    },
    {
      "name": "Bit (key)",
      "score": 0.41292357444763184
    },
    {
      "name": "Arithmetic",
      "score": 0.40520554780960083
    },
    {
      "name": "Mathematics",
      "score": 0.36079055070877075
    },
    {
      "name": "Computer engineering",
      "score": 0.3516901731491089
    },
    {
      "name": "Algorithm",
      "score": 0.28841036558151245
    },
    {
      "name": "Computer security",
      "score": 0.101703941822052
    },
    {
      "name": "Operating system",
      "score": 0.09135034680366516
    },
    {
      "name": "Political science",
      "score": 0.07271653413772583
    },
    {
      "name": "Law",
      "score": 0.07106152176856995
    }
  ],
  "institutions": [],
  "cited_by": 8
}