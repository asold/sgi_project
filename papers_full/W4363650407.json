{
  "title": "Radiology report generation using transformers conditioned with non-imaging data",
  "url": "https://openalex.org/W4363650407",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Aksoy, Nurbanu",
      "affiliations": [
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A3041434466",
      "name": "Ravikumar Nishant",
      "affiliations": [
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A2739497112",
      "name": "Frangi, Alejandro F.",
      "affiliations": [
        "University of Leeds"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2800782462",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W2890888035",
    "https://openalex.org/W3200579823",
    "https://openalex.org/W3024545783",
    "https://openalex.org/W2979956313",
    "https://openalex.org/W3151410070",
    "https://openalex.org/W3098325931",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W4394659038",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2979861699",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3037451698",
    "https://openalex.org/W3149790948",
    "https://openalex.org/W3090105394",
    "https://openalex.org/W3104609094",
    "https://openalex.org/W3130502265",
    "https://openalex.org/W3124149278",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W2946976711",
    "https://openalex.org/W3034999214"
  ],
  "abstract": "Medical image interpretation is central to most clinical applications such as disease diagnosis, treatment planning, and prognostication. In clinical practice, radiologists examine medical images and manually compile their findings into reports, which can be a time-consuming process. Automated approaches to radiology report generation, therefore, can reduce radiologist workload and improve efficiency in the clinical pathway. While recent deep-learning approaches for automated report generation from medical images have seen some success, most studies have relied on image-derived features alone, ignoring non-imaging patient data. Although a few studies have included the word-level contexts along with the image, the use of patient demographics is still unexplored. This paper proposes a novel multi-modal transformer network that integrates chest x-ray (CXR) images and associated patient demographic information, to synthesise patient-specific radiology reports. The proposed network uses a convolutional neural network to extract visual features from CXRs and a transformer-based encoder-decoder network that combines the visual features with semantic text embeddings of patient demographic information, to synthesise full-text radiology reports. Data from two public databases were used to train and evaluate the proposed approach. CXRs and reports were extracted from the MIMIC-CXR database and combined with corresponding patients' data MIMIC-IV. Based on the evaluation metrics used including patient demographic information was found to improve the quality of reports generated using the proposed approach, relative to a baseline network trained using CXRs alone. The proposed approach shows potential for enhancing radiology report generation by leveraging rich patient metadata and combining semantic text embeddings derived thereof, with medical image-derived visual features.",
  "full_text": "Radiology Report Generation Using Transformers\nConditioned with Non-imaging Data\nNurbanu Aksoya,c, Nishant Ravikumara,c*, and Alejandro F Frangia,b,c,*\naCISTIB Centre for Computational Imaging and Simulation Technologies in Biomedicine\nbLeeds Institute for Cardiovascular and Metabolic Medicine, School of Medicine\ncSchool of Computing, University of Leeds, Leeds, UK\n*Indicates joint last authors\nABSTRACT\nMedical image interpretation is central to most clinical applications such as disease diagnosis, treatment planning,\nand prognostication. In clinical practice, radiologists examine medical images (e.g. chest x-rays, computed\ntomography images, etc.) and manually compile their findings into reports, which can be a time-consuming\nprocess. Automated approaches to radiology report generation, therefore, can reduce radiologist workload and\nimprove efficiency in the clinical pathway. While recent deep-learning approaches for automated report generation\nfrom medical images have seen some success, most studies have relied on image-derived features alone, ignoring\nnon-imaging patient data. Although a few studies have included the word-level contexts along with the image,\nthe use of patient demographics is still unexplored. On the other hand, prior approaches to this task commonly\nuse encoder-decoder frameworks that consist of a convolution vision model followed by a recurrent language\nmodel. Although recurrent-based text generators have achieved noteworthy results, they had the drawback\nof having a limited reference window and identifying only one part of the image while generating the next\nword. This paper proposes a novel multi-modal transformer network that integrates chest x-ray (CXR) images\nand associated patient demographic information, to synthesise patient-specific radiology reports. The proposed\nnetwork uses a convolutional neural network (CNN) to extract visual features from CXRs and a transformer-\nbased encoder-decoder network that combines the visual features with semantic text embeddings of patient\ndemographic information, to synthesise full-text radiology reports. The designed network not only alleviates\nthe limitations of the recurrent models but also improves the encoding and generative processes by including\nmore context in the network. Data from two public databases were used to train and evaluate the proposed\napproach. CXRs and reports were extracted from the MIMIC-CXR database and combined with corresponding\npatients’ data (gender, age, and ethnicity) from MIMIC-IV. Based on the evaluation metrics used (BLEU 1-4 and\nBERTScore), including patient demographic information was found to improve the quality of reports generated\nusing the proposed approach, relative to a baseline network trained using CXRs alone. The proposed approach\nshows potential for enhancing radiology report generation by leveraging rich patient metadata and combining\nsemantic text embeddings derived thereof, with medical image-derived visual features.\nKeywords: Radiology Report Generation, Transformer, Self Attention\n1. INTRODUCTION\nIn almost all branches of health sciences, medical imaging modalities are used for disease diagnosis, accurate\ntreatment planning, patient care and prognosis. Interpretation of medical images is undertaken by radiologists,\nand their findings are compiled into a full-text report, taking into account other relevant patient information\n(such as clinical history, and patient demographics). It is important that these reports are comprehensive,\naccurate and generated in a short period of time, with a specific pre-defined structure. In clinical practice,\nchest X-ray (CXR) images are the most widely used imaging modality and it is generally the first step in\nscreening patients for a large variety of lung diseases. Reports generated following CXR examinations typically\ninclude radiologists’ interpretations categorised as ’findings’, and ’impressions’, and are indicative of normal\nand abnormal features/appearances in image regions. Writing such extensive reports requires comprehensive\ndomain knowledge and experience. Additionally, it is a time-consuming, laborious, and error-prone task, even\nFurther author information: (Send correspondence to N.A)\nN.A.: E-mail: N.Aksoy1@leeds.ac.uk\nN.R.: E-mail: N.Ravikumar@leeds.ac.uk\nA.F.: E-mail: A.Frangi@leeds.ac.uk, Telephone: +44(0)113 343 9640\narXiv:2311.11097v1  [cs.CV]  18 Nov 2023\nfor experienced staff. Given the large volume of CXR examinations performed regularly in most hospitals,\napproaches that facilitate the automatic generation of radiology reports could help reduce radiologists’ workload\nand improve efficiency in clinical pathways.\nIn this context, there have been several attempts to automatically generate radiology reports. Most existing\ndeep learning approaches proposed for radiology report generation for given CXR images, leverage networks\ncomprising a convolutional encoder and recurrent decoder, which was originally introduced for the task of image\ncaptioning1–3 The majority of existing literature on report generation that is based on deep learning methods,\nleverage the convolutional encoder-recurrent decoder type frameworks that were originally introduced in image\ncaptioning task. 1–3 Recently, due to the great success of transformers in neural machine translation tasks, a\nfew studies have been shifting from recurrent models to transformer-based models.4–7 Although previous studies\nhave shown promising results, they often treat the task of report generation as an image captioning problem.\nWhile radiology report generation shares some similarities with image captioning, it differs in other respects,\nsuch as: (1) the annotations for medical images (e.g. CXR images) are in the form of paragraphs rather than a\nsingle (short) sentence and correspondingly, the generated reports are required to be more comprehensive than\nexpected in typical image captioning tasks encountered in the computer vision domain; (2) distinctions between\nmedical images are subtle and visual semantic information is not as easy to extract as for natural images; and\n(3) the information needed to generate image captions is often embedded in natural images, however, additional\ninformation/context may be required to accurately analyse and interpret medical images.\nExisting approaches for CXR report generation utilise just the images as inputs and ignore additional non-\nimaging patient information that is typically available to radiologists when interpreting images. Learning from\nCXR images alone is challenging as each report is originally generated based on relevant medical records and\ninformation (such as gender, age, weight, pre-existing conditions/comorbidity information, etc.). As CXR images\nare two-dimensional projections of three-dimensional structures, some useful information is lost and semantic gaps\noccur in the data available to networks/algorithms to learn from. Some studies have tried to break the resulting\nsemantic gap by adding more context to the network. They have extracted some medical concepts from the\ntarget sequence,8 produced a high-level context for each report, 7 classified the images into different categories, 9\nor classified the reports into normal and abnormal classes. 10 While these approaches have demonstrated some\nsuccess, they primarily rely on augmenting information seen by the network and do not enrich the context\nprovided to the network during training with new information (such as patient demographic data for example).\nWith the aim of addressing the existing semantic gap, we introduce a novel multi-modal transformer-based\ndeep neural network that is capable of extracting information from imaging and non-imaging patient data\nsimultaneously, to generate radiology reports. To the best of our knowledge, this is the first study to explore\nautomatic CXR report generation by combining information from CXR images with non-imaging structured\npatient data (i.e. not available for linked radiology reports).\n2. RELATED WORKS\n2.1 Visual Captioning Methodologies\nVisual Captioning, also known as Image Captioning, is the task of automatically generating a natural language\ndescription of a given image. Firstly, it requires the recognition and detection of objects located in the image,\nthe identification of attributes, and the determination of their relationships and interactions. Subsequently,\nit must generate coherent sentences based on the features extracted. Since the task entails the incorporation\nof computer vision and natural language processing, it has attracted tremendous attention in the artificial\nintelligence community. Based on the recent literature, the most common deep learning architecture used by\nresearchers follows a standard encoder-decoder baseline that consists of two phases. Broadly, the encoder part\nreceives an image and sends it to Convolutional Neural Network (CNN) which is usually pre-trained on large\ndatasets for classification and recognition tasks. The layers of CNN extract region-based visual features and\nthese high-level image representations are used as input by Recurrent Neural Network (RNN)-based decoder to\ngenerate a relevant caption.\nHowever, the recurrent-based report generation models have a well-known vanishing and exploding gradients\nproblem which means that the recent input sequence causes a bias as there is limited access to the previous\ninputs and there is no direct access to all inputs. Leveraging recent advances in transformers, 11 which is a self-\nattention-based neural network, in the natural language processing (NLP) area, state-of-the-art image captioning\narchitectures have tended to substitute their model components with the transformers. 12 The main advantages\nof the transformers over other architectures are that it does not use recurrence, and it is entirely based on an\nattention mechanism. It takes and executes the input sequence as a whole, allows more parallelisation, and learns\nthe relationship between words in the sequences by the use of multi-head attention mechanisms and positional\nembeddings. Since more context is included in the network, transformer-based architectures can learn faster\nand more effectively. In the wake of this phenomenon, several variants of transformers were developed for image\ncaption generators. The original transformer architecture also inherits an image-text embedding type framework\nwhich consists of an encoder and decoder.\nThe most commonly adapted transformer-based encoder-decoder architecture for image captioning has three\nmain components; visual feature extraction model, Transformer-based encoder and transformer-based decoder.\nAs in previous studies, pre-trained CNN models are also employed for high-level feature extraction. However, in\nthis approach, the output of the visual model is used by a transformer-based encoder to map the visual features\nand generate the sequence of image representations. Then, the transformer-based decoder receives the results\nof the encoder to generate a corresponding caption of the given image. 13 presented Captioning Transformer\n(CT) model that uses ResNeXt 14 CNN model as an encoder and Transformer as a decoder. 15 also used the\nTransformer model as a decoder along with the ResNet CNN model, additionally, they improved the network\nwith a combination of spatial and adaptive attention. 16 enhanced the vanilla Transformer architecture with\nEntangled Attention (ETA) and Gated Bilateral Controller (GBC), their proposed model allows the process of\nsemantic and visual concepts concurrently.\nMoreover,5 introduced a fully-attentive model called a Meshed-Memory Transformer that consists of a\nMemory-Augmented Encoder, which has enriched with learnable the keys and values with a priori informa-\ntion and used a learnable gating mechanism to perform mesh connectivity and Meshed Decoder that performs\na meshed connection between all encoding layers. On the other hand, 12 proposed a full Transformers network\nwithout having any convolutional operation in the encoder. Different from previous studies, their model, CaP-\ntion TransformeR (CPTR), uses the raw image and adjusts it according to the accepted input form of the\ntransformer encoder by dividing the original image into N patches. After reshaping the patches, the obtained\npatch embedding is incorporated with positional embedding.\n2.2 Radiology Report Generation\nIn recent years, many studies have had great success in fine-tuning deep neural networks to generate medical\nreports. Most existing radiology report generation studies adopt image captioning approaches for medical report\ngeneration and leverage the CNN-LSTM framework; 17 employed a pre-trained VGG-19 model to learn visual\nfeatures and use the extracted features to predict relevant tags for any given chest X-ray. The predicted tags are\nused as semantic features in the network and both semantic and visual features are fed into the Co- Attention\nNetwork. Hierarchical LSTM has used the context vector provided by Co-attention Network to generate the topic\nand description of the given X-ray. Although the model obtained promising results and achieved great success in\nits field, the repetitive sentences in reports and the generation of different results for the same patient undermined\nits credibility from both medical and computational perspectives.18 improved the pre-trained Resnet- 152 encoder\nusing multi-view content (both lateral and frontal view) and incorporated them to ensure the consistency of the\nresults. They also generated a report with a sentence decoder and additionally used the first predicted sentence\nas a joint input along with image encoding.\nAnother major study was proposed by, 8 who pre-trained their multi-view encoder from scratch using the\nCheXpert dataset 19 instead of using ImageNet pre-trained models. In order to enhance the decoder, they\nextracted and applied medical concepts from the reports. Applying medical concepts conveyed the semantics in\nthe content of the report and they achieved noteworthy results. The idea of using medical concepts was also\nemployed by;20 whilst they applied a similar approach in principle, they proposed a reward term to extract more\nprecise concepts. Although the medical concepts obtained were more accurate compared with other studies, they\nwere still not very informative about the given X-Ray.\nOn the other hand, 10 argued that the format of the normal and abnormal reports differs, therefore, a single\nframework cannot handle both styles accurately. To overcome this limitation, they followed a slightly different\napproach and first, they classified the reports as normal and abnormal. Then, they generated the Findings\nsection and summarised it to acquire the Impression section for both normal and abnormal reports. They also\nadopted pre-trained CNN model, InceptionV3, for visual feature extraction and used attention-based LSTM for\ntext generation. More recently, studies have taken advantage of Transformer for medical report generation, after\nits success for text generation based on non-linguistic representation.4 designed a hierarchical Transformer model\nwhich contains a novel encoder that can extract the regions of interest from the original image by using a region\ndetector and uses these regions to obtain visual representations. Moreover,6 introduced a medical report generator\nvia a memory-driven Transformer. They have used a relational memory to keep the knowledge from the previous\ncase, in this manner, the generator model can remember similar reports when generating the current report. 7\nproposed a progressive Transformer-based report generation framework that produces high-level context from\nthe given X-ray and converts them into a radiology report by employing the Transformer architecture. Their\nproposed model consists of pre-trained CNN as a visual backbone, a mesh-memory Transformer 5 as a visual\nlanguage model and BART21 as a language model.\n3. DATA AND METHODS\n3.1 Data and Pre-processing\nTwo publicly available databases, namely, MIMIC-CXR22 and MIMIC-IV23 were used to create the dataset used\nthroughout this study for training and evaluating the proposed approach.\nMIMIC-CXR (version 2.0.0)consists of 377,110 CXR images (anteroposterior, posteroanterior, and lat-\neral views) and the corresponding 227,835 de-identified free-text radiology reports, acquired from 63,473 patients.\nEach report has several sections including - ’examination’, ’indication’, ’technique’, ’comparison’, ’findings’, and\n’impressions’. However, there is a lot of missing information in both the ’comparison’ and ’indication’ sections\ndue to the anonymisation of the data. Moreover, the ’impressions’ section has several identical entries across\ndifferent patients, resulting in data imbalance/biases. Therefore, only the ’findings’ section of the reports is used\nthroughout this study. All reports were subsequently pre-processed and standardised in the following way - the\nlength of each report was calculated and reports containing less than 9 words were removed from the dataset.\nAll characters in all reports were converted into lowercase. Punctuation marks, tokens with a number, stop\nwords, and sequences that do not provide meaningful information were removed from each report. Also, as\nwe do not have access to patients’ historical clinical records, we removed the reports referring to prior studies\non the same subject. Then, different texts that convey the same meaning (i.e. are semantically similar) were\nstandardised to minimise variability. This step is neglected by the other studies. However, it is important to\nreduce the diversity of the language and terminology used in the reports. Finally, <start> and <end> tokens\nwere added to each sequence. On the other hand, the images were filtered to only include anteroposterior and\nposteroanterior projections. Images were then re-sized to 299 px × 299 px and their intensities were normalised.\nMIMIC-IV (version 1.0)consists of anonymised patient information who were admitted to the Beth Israel\nDeaconess Medical Center (BIDMC). Firstly, gender, age, and ethnicity values for each patient were extracted\nfrom relevant tables. The gender values were converted into binary data by assigning a value of 0 for female and\n1 for male patients. We only used the five ethnicity values that occurred the most, and we created a new variable\nfor each of them using one-hot encoding. The range of age was restricted to 19 to 91 and then normalised to\nbe between 0 and 1. Both MIMIC-CXR and MIMIC-IV databases have a common unique identification number\nfor each patient, which is used as a key to retrieve relevant information and create a data point for each subject.\nThe final dataset has 43,628 data points, and each data point has a CXR image, the corresponding findings\nsection/report, and gender, age, and ethnicity data. One of the main challenges for the report generation task\nis the biased dataset. In addition to the fact that most of the cases are normal, the several instances of identical\nreports across different patients pose additional challenges. To address this problem, we sampled the dataset\nand created 4 subsets that were as balanced as possible, which were used for the experiments conducted in this\nstudy. We split each subset into training, validation and test sets with a split ratio of 70: 20: 10, respectively.\n3.2 Network Design\nAs shown in Fig. 1, the report generation network has three main components: Visual Unit, Semantic Unit,\nand Generation Unit. The representation of each image is derived in two steps in the Visual Unit. The\nVisual and Semantic Units constitute the encoder and the Generation Unit represents the decoder in\nthe proposed approach. First, the image is passed through a CNN (EfficientNet was used) for visual feature\nextraction. The pre-trained model receives resized (299x299) images and generates a 1280-length visual feature\nvector. The extracted features are then used in the Multi-Head Self-Attention block for deeper understanding\nof the relationships between the detected entities. In the Semantic Unit, meanwhile, the relevant patient data\n(gender, age and ethnicity) are encoded and semantic features are obtained. In order to enhance the image\nrepresentation, we add a Visual-Semantic Self-Attention block to the encoder which uses both types of features\nand derives a new hybrid image representation. This hybrid image representation is in turn provided as input\nto the decoder. In the decoder/ Generation Unit, the target report corresponding to the given CXR image\nis converted into a vector representation with positional encoding and fed to the transformer-decoder (during\ntraining only), which in turn generates the target report, conditioned on the derived hybrid image representation.\nFigure 1. The overall framework of the proposed CXR report generation model.\n3.3 Implementation Details\nIn the encoding phase, we employ EfficientNet24 as a base model to extract visual features and it is trained end-\nto-end by fine-tuning all parameters. The pre-trained model receives resized (299x299) images and generates a\n1280-length visual feature vector. Following normalisation, the image vector is passed through the feed-forward\nlayer with the ReLU activation function. The output of the feed-forward layer is sent to the multi-head self-\nattention (MHA) layer followed by the normalisation layer. The normalised image representation is used as a\nquery parameter in the Visual-Semantic Self-Attention block. Meanwhile, the relevant patient data (gender, age\nand ethnicity) are extracted from the MIMIC-IV database, After the semantic data is vectorised and encoded\nas described in Sec. 3.1, it is passed to a fully connected layer. During the experiments, we first presented each\npatient data type to the model alone, and then we performed further experiments to concatenate the types that\nshowed improvements.\nThe decoder is an auto-regressive model that generates words recursively, starting with the <start> token\nassociated with each tokenised target report and stops decoding/predicting words when the <end> token is\ngenerated. Due to its auto-regressive nature, the decoder takes the information from the previous iteration to\npredict the next word. During training, the target report corresponding to the given image is passed through an\nembedding layer and used as input to train the decoder. The first layer of the decoder is a masked multi-head\nself-attention layer. Different from MHA in the encoder, the masking method is used here to ensure that the\nattention mechanism does not share any information about future tokens. In this way, each token only has\naccess to information regarding itself and the previously generated values. The output of the masked multi-head\nattention layer is combined with the output of the decoder using a multi-modal attention layer, where the query\nmatrix is created from the former and semantically enhanced hybrid image representations are used for the keys\nand values matrix. The output of the decoder is passed through a SoftMax layer, which calculates probability\nscores between 0 and 1, and returns the word with the highest probability score as the predicted word. During\ninference, only CXR images and relevant patient data are used as inputs and the report generation process\nis triggered with <start> token. The decoder generates words in a recursive loop until the <end> token is\npredicted.\nWe built five different models; the network trained on CXRs alone is used as a baseline model. We named the\nother models based on the type of patient data used in the network. They adopt the same principles to combine\nthe non-imaging data with image-derived features. Each model was trained using the Adam optimiser (with a\nlearning rate of 3e-4) and a Sparse Categorical Cross Entropy loss function. A batch size of 64, a maximum\nreport length of 50, a vocabulary size of 2212, a dense dimension of 512, and an embedding dimension of 512 are\nused during training and validation.\nThe decoder begins with <start> token, generates words recursively and stops the decoding process when\nthe <end> token is generated. Due to its auto-regressive nature, it takes the information from the previous\niteration to predict the next word. During training, the target report corresponding to the given image is passed\nthrough the embedding layer. The transformer does not implement a loop and all inputs are processed in parallel.\nAlthough this is one of its main advantages over recurrence models, the notion of the sequence order is lost during\nthis operation. Therefore, positional information is injected into the output of the embedding layer with the\npositional encoding and it is sent to the first layer of the decoder which is a masked multi-head self-attention layer.\nDifferent from MHA in the encoder, the masking method is used here to ensure that the attention mechanism\ndoes not share any information about future tokens. In this way, each token only has access to information\nregarding itself and the previously generated values. The MHA layer is followed by the normalisation layer and\nMulti-modal attention layer, respectively. The multi-modal attention layer incorporates information from both\nthe encoder and the decoder. When calculating self-attention, the queries matrix is created from the previous\nlayer, and the semantically enhanced image representations are used for the keys and values matrix. The output\ngoes through the normalisation layer followed by the feed-forward layer with the ReLU activation function and\nthe last feed-forward layer takes the outputs and functions as a classifier. The final output is passed through\nthe SoftMax layer, which calculates probability scores between 0 and 1, and returns the word with the highest\nprobability score as the predicted word. The model was trained using the Adam optimiser (with a learning rate\nof 3e-4) and a Sparse Categorical Cross Entropy loss function. A batch size of 64, a maximum report length of\n50, a report vocabulary size of 2212, a dense dimension of 512, and an embedding dimension of 512 are used\nduring training and validation. During inference, only CXR and relevant patient data are used and the report\ngeneration process is triggered with <start> token. The decoder generates the words in a recursive loop until\nthe <end> token is predicted. We also define a text sampling function that selects candidate words from a\nprobability array and set them to a temperature parameter is 0.5.\n4. RESULTS AND DISCUSSION\nFollowing data pre-processing (described in Sec. 3.1), the total data size was reduced to 40,700. We, then,\nsampled four different sets of 4500 data with no overlap. Each model investigated was trained and evaluated on\nthese sets using the same train, validation and test split for a fair comparison. The average results of the models\nacross all four test sets are summarised in Table 1 using the Bilingual Evaluation Understudy (BLEU) score and\nBERTScore. Diversity of the language has an effect on the BLEU score since it measures the similarity between\ntarget text and ground truth based on n-gram precision. As it is insufficient to capture semantic similarity, we\nalso evaluated the models with BERT-Score that leverages pre-trained BERT contextual embeddings in order to\ncalculate the token similarity instead of relying on exact string matching.\nThe results show that the use of patient data in the report generation task is beneficial and improves the\nmodel performance. We also assessed the statistical significance of the obtained results using the Student’s t-test\nwith a significance level of α = 0.05 for each metric. The best results showing significant improvements over the\nrest are indicated in bold in Table 1. The ethnicity-enriched model achieved the highest scores followed by the\ngender- and age-enriched models.\nTable 1. Quantitative results using Natural Language Generation metrics\nModel BLEU-1 BLEU-2 BLEU-3 BLEU-4 RBERT F1ScoreBERT\nBaseline Model 0.314±0.004 0.193±0.003 0.140±0.001 0.089±0.002 0.27±0.003 0.20±0.002 0.23±0.03\nGender Model 0.322±0.003 0.209±0.004 0.142±0.002 0.091±0.002 0.27±0.001 0.24±0.003 0.25±0.002\nAge Model 0.322±0.002 0.205±0.005 0.141±0.003 0.088±0.003 0.28±0.001 0.21±0.002 0.25±0.004\nEthnicity Model0.333±0.004 0.210±0.003 0.142±0.002 0.088±0.002 0.30±0.013 0.23±0.008 0.27±0.010\nEthnicity+Gender0.321±0.003 0.206±0.005 0.141±0.002 0.087±0.002 0.26±0.009 0.22±0.005 0.24±0.006\nFigure 2. Illustrations of reports from ground-truth and ethnicity-enriched models for different cases.\nThe example predictions from the ethnicity-enriched model are shown in Fig. 2. For a better assessment of\nthe results, we classified the illustrated samples into four categories: accurate prediction, different expressions,\nmissing and false arguments, and false prediction. The text that conveys the same meaning is shown in the same\ncolour. The red text represents missing statements, whereas the underlined text represents incorrect predictions.\nIn all cases, the order of findings aligns with the reports written by the radiologists and the generated reports\nare structurally correct. On the other hand, the model was not able to identify rare cases in the test set. It also\nmissed some arguments and generated redundant and repetitive words. Even though we tried to balance the\ntrain set, the number of some reports was relatively more than others which caused a bias while generating the\nnext word.\n5. CONCLUSION\nThis paper introduced a multi-modal transformer network that is capable of using both imaging and non-\nimaging data (metadata) to generate radiology reports for CXR images. The ethnicity-enriched model achieved\nthe highest scores followed by the gender- and age-enriched ones. Using two types of metadata (gender and\nethnicity) simultaneously did not show any improvement over the previous one-type approaches. The proposed\napproach achieves 6.1%, 8.0%, 1.4%, 11% and 17.4% improvement over the baseline, in terms of BLEU-1,\nBLEU-2, BLEU-3, PBERT and F1ScoreBERT , respectively.\n6. ACKNOWLEDGEMENTS\nThis study is fully sponsored by Turkish Ministry of National Education.\nREFERENCES\n[1] Monshi, M. M. A., Poon, J., and Chung, V., “Deep learning in generating radiology reports: A survey,”\nArtificial Intelligence in Medicine 106, 101878 (2020).\n[2] Boag, W., Hsu, T.-M. H., McDermott, M., Berner, G., Alesentzer, E., and Szolovits, P., “Baselines for chest\nx-ray report generation,” in [ Machine Learning for Health Workshop ], 126–140, PMLR (2020).\n[3] Kaur, N., Mittal, A., and Singh, G., “Methods for automatic generation of radiological reports of chest\nradiographs: a comprehensive survey,” Multimedia Tools and Applications , 1–31 (2021).\n[4] Xiong, Y., Du, B., and Yan, P., “Reinforced transformer for medical image captioning,” in [ International\nWorkshop on Machine Learning in Medical Imaging ], 673–680, Springer (2019).\n[5] Cornia, M., Stefanini, M., Baraldi, L., and Cucchiara, R., “Meshed-memory transformer for image caption-\ning,” in [Proceedings of the IEEE/CVF conference on computer vision and pattern recognition], 10578–10587\n(2020).\n[6] Chen, Z., Song, Y., Chang, T.-H., and Wan, X., “Generating radiology reports via memory-driven trans-\nformer,” arXiv preprint arXiv:2010.16056 (2020).\n[7] Nooralahzadeh, F., Gonzalez, N. P., Frauenfelder, T., Fujimoto, K., and Krauthammer, M., “Progressive\ntransformer-based generation of radiology reports,” arXiv preprint arXiv:2102.09777 (2021).\n[8] Yuan, J., Liao, H., Luo, R., and Luo, J., “Automatic radiology report generation based on multi-view image\nfusion and medical concept enrichment,” in [ International Conference on Medical Image Computing and\nComputer-Assisted Intervention ], 721–729, Springer (2019).\n[9] Alfarghaly, O., Khaled, R., Elkorany, A., Helal, M., and Fahmy, A., “Automated radiology report generation\nusing conditioned transformers,” Informatics in Medicine Unlocked 24, 100557 (2021).\n[10] Singh, S., Karimi, S., Ho-Shon, K., and Hamey, L., “Show, tell and summarise: learning to generate and\nsummarise radiology findings from medical images,” Neural Computing and Applications 33(13), 7441–7465\n(2021).\n[11] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  L., and Polosukhin,\nI., “Attention is all you need,” Advances in neural information processing systems 30 (2017).\n[12] Liu, W., Chen, S., Guo, L., Zhu, X., and Liu, J., “Cptr: Full transformer network for image captioning,”\narXiv preprint arXiv:2101.10804 (2021).\n[13] Zhu, X., Li, L., Liu, J., Peng, H., and Niu, X., “Captioning transformer with stacked attention modules,”\nApplied Sciences 8(5), 739 (2018).\n[14] Xie, S., Girshick, R., Doll´ ar, P., Tu, Z., and He, K., “Aggregated residual transformations for deep neural\nnetworks,” in [Proceedings of the IEEE conference on computer vision and pattern recognition ], 1492–1500\n(2017).\n[15] Zhang, W., Nie, W., Li, X., and Yu, Y., “Image caption generation with adaptive transformer,” in [ 2019\n34rd Youth Academic Annual Conference of Chinese Association of Automation (YAC) ], 521–526, IEEE\n(2019).\n[16] Li, G., Zhu, L., Liu, P., and Yang, Y., “Entangled transformer for image captioning,” in [ Proceedings of the\nIEEE/CVF international conference on computer vision ], 8928–8937 (2019).\n[17] Jing, B., Xie, P., and Xing, E., “On the automatic generation of medical imaging reports,” arXiv preprint\narXiv:1711.08195 (2017).\n[18] Xue, Y., Xu, T., Rodney Long, L., Xue, Z., Antani, S., Thoma, G. R., and Huang, X., “Multimodal\nrecurrent model with attention for automated radiology report generation,” in [International Conference on\nMedical Image Computing and Computer-Assisted Intervention ], 457–466, Springer (2018).\n[19] Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H., Haghgoo, B., Ball, R.,\nShpanskaya, K., et al., “Chexpert: A large chest radiograph dataset with uncertainty labels and expert\ncomparison,” in [Proceedings of the AAAI conference on artificial intelligence ], 33(01), 590–597 (2019).\n[20] Yang, S., Niu, J., Wu, J., and Liu, X., “Automatic medical image report generation with multi-view\nand multi-modal attention mechanism,” in [ International Conference on Algorithms and Architectures for\nParallel Processing], 687–699, Springer (2020).\n[21] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer,\nL., “Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and\ncomprehension,” arXiv preprint arXiv:1910.13461 (2019).\n[22] Johnson, A. E., Pollard, T. J., Greenbaum, N. R., Lungren, M. P., Deng, C.-y., Peng, Y., Lu, Z., Mark,\nR. G., Berkowitz, S. J., and Horng, S., “Mimic-cxr-jpg, a large publicly available database of labeled chest\nradiographs,” arXiv preprint arXiv:1901.07042 (2019).\n[23] Johnson, A., Bulgarelli, L., Pollard, T., Celi, L. A., Mark, R., and Horng, S., “MIMIC-IV-ED (version 1.0).\nPhysioNet .” https://doi.org/10.13026/77z6-9w59 (2021).\n[24] Tan, M. and Le, Q., “Efficientnet: Rethinking model scaling for convolutional neural networks,” in [ Inter-\nnational conference on machine learning ], 6105–6114, PMLR (2019).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.690032422542572
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5454148054122925
    },
    {
      "name": "Workload",
      "score": 0.5434262752532959
    },
    {
      "name": "Metadata",
      "score": 0.5194615721702576
    },
    {
      "name": "Medical imaging",
      "score": 0.4991886615753174
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49706533551216125
    },
    {
      "name": "Deep learning",
      "score": 0.47933492064476013
    },
    {
      "name": "Medical physics",
      "score": 0.3746150732040405
    },
    {
      "name": "Radiology",
      "score": 0.36493247747421265
    },
    {
      "name": "Machine learning",
      "score": 0.35288292169570923
    },
    {
      "name": "Medicine",
      "score": 0.22894984483718872
    },
    {
      "name": "World Wide Web",
      "score": 0.1125364601612091
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}