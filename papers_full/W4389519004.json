{
    "title": "Probing LLMs for hate speech detection: strengths and vulnerabilities",
    "url": "https://openalex.org/W4389519004",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3154918784",
            "name": "Sarthak Roy",
            "affiliations": [
                "Indian Institute of Technology Kharagpur"
            ]
        },
        {
            "id": null,
            "name": "Ashish Harshvardhan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2134540012",
            "name": "Animesh Mukherjee",
            "affiliations": [
                "Indian Institute of Technology Kharagpur"
            ]
        },
        {
            "id": "https://openalex.org/A2904032887",
            "name": "Punyajoy Saha",
            "affiliations": [
                "Indian Institute of Technology Kharagpur"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3201622928",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2962686197",
        "https://openalex.org/W4378465262",
        "https://openalex.org/W4321524280",
        "https://openalex.org/W4327811957",
        "https://openalex.org/W4366733439",
        "https://openalex.org/W4285246417",
        "https://openalex.org/W2920807444",
        "https://openalex.org/W4389636360",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4366850809",
        "https://openalex.org/W4320858112",
        "https://openalex.org/W2923321784",
        "https://openalex.org/W3013149459",
        "https://openalex.org/W3173380736"
    ],
    "abstract": "Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select two large language models (GPT-3.5 and text-davinci) and three datasets - HateXplain, implicit hate and ToxicSpans. We find that on average including the target information in the pipeline improves the model performance substantially (∼20-30%) over the baseline across the datasets. There is also a considerable effect of adding the rationales/explanations into the pipeline (∼10-20%) over the baseline across the datasets. In addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute ‘jailbreak’ prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6116–6128\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nProbing LLMs for hate speech detection: strengths and vulnerabilities\nSarthak Roy, Ashish Harshavardhan, Animesh Mukherjee and Punyajoy Saha\nIndian Institute of Technology, Kharagpur\n{sarthak.cse22@kgpian,animeshm@cse}.iitkgp.ac.in,\n{ashishaug29,punyajoys}@iitkgp.ac.in\nAbstract\nRecently efforts have been made by social me-\ndia platforms as well as researchers to detect\nhateful or toxic language using large language\nmodels. However, none of these works aim\nto use explanation, additional context and vic-\ntim community information in the detection\nprocess. We utilise different prompt variation,\ninput information and evaluate large language\nmodels in zero shot setting (without adding any\nin-context examples). We select three large\nlanguage models (GPT-3.5, text-davinci and\nFlan-T5) and three datasets - HateXplain, im-\nplicit hate and ToxicSpans. We find that on\naverage including the target information in the\npipeline improves the model performance sub-\nstantially (∼20−30%) over the baseline across\nthe datasets. There is also a considerable ef-\nfect of adding the rationales/explanations into\nthe pipeline (∼10 −20%) over the baseline\nacross the datasets. In addition, we further pro-\nvide a typology of the error cases where these\nlarge language models fail to (i) classify and\n(ii) explain the reason for the decisions they\ntake. Such vulnerable points automatically con-\nstitute ‘jailbreak’ prompts for these models and\nindustry scale safeguard techniques need to be\ndeveloped to make the models robust against\nsuch prompts.\n1 Introduction\nAbusive language has become a perpetual problem\nin today’s online social media. An ever-increasing\nnumber of individuals are falling prey to online\nharassment, abuse and cyberbullying as established\nin a recent study by Pew Research (V ogels, 2021).\nIn the online setting, such abusive behaviours can\nlead to traumatization of the victims (Vedeler et al.,\n2019), affecting them psychologically. Further-\nmore, widespread usage of such content may lead\nto increased bias against the target community,\nmaking violence normative (Luft, 2019). Many\ngruesome incidents like the mass shooting at Pitts-\nburgh synagogue1, the Charlottesville car attack2,\netc. have all been caused by perpetrators consum-\ning/producing such abusive content.\nIn response to this, social media platforms have\nimplemented moderation policies to reduce the\nspread of hate/abusive content. One important step\nin content moderation is filtering of abusive content.\nA common way to do this is to train language mod-\nels on human annotated contents for classification.\nHowever there are challenges in this approach in\nthe forms of heavy resources required in terms of\nlabour and expertise to annotate these hateful con-\ntents. This exercise also exposes the annotators to a\nwide array of hateful contents that is almost always\npsychologically very taxing. Therefore, recently,\nmany works have tried to understand if Large lan-\nguage models (LLMs) can be used for detecting\nsuch abusive language3 (Huang et al., 2023; Ziems\net al., 2023), but none of these study the role of\nadditional context as input (output) to (from) such\nLLMs.\nWe, for the first time, introduce several prompt\nvariations and input instructions to probe two\nof the LLMs (GPT 3.5 and text-davinci) across\nthree datasets - HateXplain (Mathew et al., 2021),\nimplicit hate (ElSherief et al., 2021) and Toxic-\nSpans (Pavlopoulos et al., 2022). Note that all\nthese three datasets contain ground truth explana-\ntions in the form of either rationales (Mathew et al.,\n2021; Pavlopoulos et al., 2022) or implied state-\nments (ElSherief et al., 2021) that tells why an\nannotator took a particular labelling decision. In\naddition, two of the datasets also contain the infor-\nmation about the target/victim community against\nwhom the hate speech was hurled. In particular, we\ndesign prompts that contain (a) only the hate post\nas input and a query for the output label (vanilla)\n1https://en.wikipedia.org/wiki/Pittsburgh_\nsynagogue_shooting\n2https://en.wikipedia.org/wiki/\nCharlottesville_car_attack\n3https://tinyurl.com/detoxigen\n6116\n(b) post as well as the definition of hate speech as\ninput and a query for the output label (c) post (-/+\ndefinition) and the target community information\nas input and a query for the output label, (d) post\n(-/+ definition) and the explanation as input and a\nquery for the output label, (e) post (-/+ definition)\nas input and a query for the output label and the tar-\nget community, and (f) post (-/+ definition) as input\nand a query for the output label and the explanation.\nWe record the performance of all these approaches\nand also identify the most confusing cases. In order\nto facilitate future research, we further provide a\ntypology of the error cases where the LLMs fail to\nclassify and usually provide poor explanations for\nthe classification decisions taken. This typology\nwould naturally constitute the ‘jailbreak’ prompts4\nto which such LLMs are vulnerable thus pointing\nto the exact directions in which industry scale safe-\nguards need to be built.\nWe make the following observations.\n• In terms of vanilla prompts (that is case (a)),\nwe find that flan-T5-large performs the\nbest among the three models; we also ob-\nserve that text-davinci-003 is better than\ngpt-3.5-turbo-0301 although the latter is a\nmore recent version.\n• Our proposed strategies of prompts individ-\nually benefit the LLMs in most cases. The\nprompt with target community in the pipeline\ngives the best performance with ∼20 −30%\nimprovements over the vanilla setup. None of\nthese LLMs are able to benefit themselves if\nmultiple prompt strategies are combined.\n• While doing a detailed error analysis, we\nfind that the misclassification of non-hate/non-\ntoxic class is the most common error for im-\nplicit hate and ToxicSpans datasets while for\nthe Hatexplain dataset the majority of misclas-\nsifications are from the normal to the offensive\nclass. There are also a large number of cases\nwhere the model confuses between the hate\nspeech and the offensive class.\n• From the typology induced from the error\ncases, we find many interesting patterns.\nThese LLMs make errors due to the presence\nof sensitive or controversial terms in otherwise\nnon-hateful posts. Presence of negation words\nand words expressing support for a commu-\nnity are misclassified as hateful. Ideological\n4https://docs.kanaries.net/articles/\nchatgpt-jailbreak-prompt\nposts and posts containing opinions or fact\ncheck information about news articles are of-\nten misclassified as toxic/hateful. On the other\nhand, many offensive/hateful posts are marked\nnormal by the model either due to a vocabu-\nlary gap or presence of unknown or polyse-\nmous words. Similarly, these models miss to\nclassify implicitly toxic posts and mark them\nas non-toxic.\nWe make our codes and resources used for this\nresearch publicly available for reproducibility pur-\nposes5.\n2 Related works\nLarge language models : Based on the training\nsetup, the model architecture and the use cases\nLLMs can be broadly classified into encoder-only,\nencoder-decoder based and decoder-only types. In\nrecent years decoder-only LLMs6 have seen a huge\nsurge with industry scale releases like chatGPT,\ngpt-4, BARD, LLaMa, etc. Decoder-only LLMs\nhave been used for benchmarks like GLUE (Zhong\net al., 2023) where there is no downstream appli-\ncation involved. In the classification setting LLMs\nhave been extensively used for sentiment analy-\nsis (Zhang et al., 2023b). They have also been\nheavily used in NLI and QA tasks on multiple\ndatasets (Chowdhery et al., 2022). In the genera-\ntion setting, LLMs have found applications in sum-\nmarization (Zhang et al., 2023a), machine trans-\nlation (Chowdhery et al., 2022) and open-ended\ngeneration (Brown et al., 2020).\nLLMs for hate speech detection : In (Zhu et al.,\n2023), the authors use chatGPT to relabel vari-\nous datasets one of which is on hate speech de-\ntection and found that the agreement with human\nannotations is still quite poor. The authors in (Li\net al., 2023) use chatGPT to classify a comment as\nharmful (i.e., hateful, offensive, or toxic – HOT)\nand found that the model is better in identifying\nnon-HOT comments than HOT comments. Finally\nin (Huang et al., 2023) the authors attempted to\nclassify implicit hate speech using chatGPT. How-\never, their prompt was framed as a ‘yes/no’ ques-\ntion (rather than based on the exact classes i.e.,\nimplicit hate, explicit hate, non-hate as in the origi-\nnal study (ElSherief et al., 2021)) which makes the\nproblem lose its original fervour.\n5https://shorturl.at/nqTX2\n6https://github.com/Hannibal046/Awesome-LLM\n6117\n3 Datasets and metrics\nFor all these datasets, we utilise (create) a test\ndataset for our experiments.\nImplicit hate dataset : The implicit hate (ElSh-\nerief et al., 2021) corpus is a specialized collec-\ntion of data aimed at detecting hate speech. It pro-\nvides detailed labels (implicit_hate, explicit_hate\nor non_hate) for each message, including informa-\ntion about the implied meaning behind the content.\nThe dataset comprises 22,056 tweets sourced from\nmajor extremist groups in the United States.\nWe test on a subset of 2147 samples (108 en-\ntries labelled explicit_hate, 710 entries labelled\nimplicit_hate, and 1329 entries labelled not_hate)\nwhich are sampled from the entire dataset in a strati-\nfied fashion. Note that we do not have explanations\nand targets for all the posts. The implied statements\nand targets were available only for the samples with\nlabel implicit_hate. Hence, when we experiment\nwith explanation as input (see section 5), we pass\nthe input post as the implied statement. This is for\nthe explicit_hate and not_hate data points. Both\nthese categories do not required any additional im-\nplied statement as there is nothing implied in them.\nIn case of targets as inputs (see section 5), we re-\nmove the explicit_hate datapoints since they are\ntargetting some victim community but the targets\nare not present in the annotated dataset. The targets\nfor not_hate are set as ‘none’.\nHateXplain dataset: HateXplain (Mathew et al.,\n2021) is a benchmark dataset specifically designed\nto address bias and explainability in the domain of\nhate speech. It provides comprehensive annotations\nfor each post, encompassing three key perspectives:\nclassification (hate speech, offensive, or normal),\nthe targeted community, and the rationales - which\ndenote the specific sections of a post that influenced\nthe labelling decision (hate, offensive, or normal).\nWe test on the already released test dataset con-\ntaining 1924 samples (594 entries labelled as hate\nspeech, 782 entries labelled as normal, and 548\nentries labelled as offensive). Note that we do not\nhave rationales for the normal posts. In the expla-\nnations as input experiments (see section 5), the\ncomplete post (tokenized) is taken as their rationale\nfor the normal posts.\nToxicSpans dataset: The ToxicSpans (Pavlopou-\nlos et al., 2022) dataset is a subset (containing\n11,006 samples labelled toxic) of the Civil Com-\nments dataset (1.2M Posts). The dataset also con-\ntains the toxic spans, i.e., the region of the texts\nfound toxic. Not all the posts had toxic spans anno-\ntated. We create a test set of 2000 samples by pick-\ning 1000 samples labelled toxic from this dataset\n(where the toxic spans were available), and 1000\nsamples labelled non-toxic from the Civil Com-\nments dataset (Borkan et al., 2019). Note that we\nhave the spans/rationales marked only for the toxic\ndata points. For the non-toxic posts For expla-\nnations as input experiments (see section 5), the\ncomplete post (tokenized) is taken as the rationale.\nMetrics: For primary evaluation, we rely on clas-\nsification performance. We use precision, recall,\naccuracy and macro F1-score to measure classifica-\ntion performance, which are all standard metrics.\nThe data points which have rationales are addi-\ntionally evaluated using other generation metrics.\nFor the natural language explanations (implicit hate\ndataset) we use BERTScore averaged over all data\npoints. BERTScore (Zhang et al.) computes a simi-\nlarity score for each token in the candidate sentence\nwith each token in the reference sentence. However,\ninstead of exact matches, they compute token simi-\nlarity using contextual embeddings. For extractive\nexplanation (HateXplain/ToxicSpans dataset), we\nuse average sentence-BLEU (Papineni et al., 2002)\nscore which is the standard among the generation\nmetrics.\n4 Models\nFor our experiments we utilise three LLMs. Two\nof these are from the proprietary GPT-3.5 model\nseries7 while the third is an open source one from\nthe T5 series.\nGPT-3.5 models are better than their prede-\ncessors GPT 3 (Brown et al., 2020). Both of\nthese models are highly advanced language mod-\nels capable of generating human-like text based\non the provided prompts but they differ in some\nkey ways. As per their documentation 8, GPT-3\nwas optimized on code completion tasks to create\ncode-davinci-002. This was further improved\nusing instruction finetuning (Ouyang et al., 2022)\nto create text-davinci-002. This was later up-\ngraded to text-davinci-003 which was trained\non a larger dataset (ope) making it better at higher\nquality text generation, following instructions and\ngenerating longer context. gpt-3.5-turbo-0301\nis an improvement over text-davinci-003. We\n7This was made available by the Microsoft Accelerating\nAI Academic Research grant.\n8https://platform.openai.com/docs/model-index-for-\nresearchers\n6118\nchoose these two models in our study namely the\ngpt-3.5-turbo and the text-davinci-003.\nThe third model we use is the open source\nflan-T5-large which is an instruction finetuned\nvariant of the popular T5 model (Chung et al.,\n2022). As per their documentation the model was\ninstruction finetuned with an emphahsis on scaling\nthe number of tasks, scaling the model size and\nintroducing chain of thought data in the finetuning\npipeline. The authors have claimed that this partic-\nular sort of finetuning has benefited the T5 models\ngreatly by outperforming models of higher size like\nGPT-3.\n5 Prompts\nIn this section, we list the prompt variations used\nin this work. A concise summary of the different\nvariants is noted in the Appendix A (Table 7) and\nthe details for each are discussed in the subsections\nbelow.\nDataset Label list\nHateXplain normal, offensive or hate speech\nImplicit hate explicit_hate, implicit_hate, or not_hate\nToxicSpans toxic or non_toxic\nTable 1: The list of labels for each dataset.\nVanilla prompts : In this category, we use a\nprompt template where we ask the model to\nclassify the given post into a label out of a\nlist_of_labels. In addition, we also provide\na few example_outputs (one class per line) for\nhelping the models generate proper answer. The\nlist_of_labels for each datasets are noted in the\nTable 1.\n(+) definitions: In the vanilla prompts we assumed\nthat the LLMs are to an extent aware of the labels\nfor classification. Here, we provide the definitions\nas an additional context to the LLMs. This can\nhelp the LLMs understand the classification tasks\nbetter. These definitions are added as a list where\neach label’s definition is separated by a new line.\nWe note this prompt template as Vanilla + Defn in\nAppendix A, Table 7. Individual definitions of the\ntasks are added in Appendix B.\n(+) explanations Recently, there has been a huge\ninterest in developing explainable deep learning\nmodels where the prediction decision is supported\nby an explanation (Bhatt et al., 2020). We test\ntwo hypotheses – (a) whether providing explana-\ntions to LLMs as inputs (corresponding to the tem-\nplates Vanilla + Exp (input) and Vanilla + Defn +\nExp (input)) improve its labelling decisions and (b)\nwhether asking LLMs for an explanation about its\nlabelling decision forces it to predict better labels\nand as well generate relevant explanations (cor-\nresponding to the templates Vanilla + Exp (out-\nput) and Vanilla + Defn + Exp (output) ). For\nthe HateXplain and the ToxicSpans dataset the\nground truth explanations are in the form of ra-\ntionales (i.e., a part (parts) of the post that the an-\nnotator marked as the reason for his/her labelling\ndecision). For the implicit hate speech dataset\nthe ground truth explanations are in the form of\nimplied statements. In the templates Vanilla +\nExp (output) and Vanilla + Defn + Exp (out-\nput) we use two variables explanation_type and\nexplanation_format (see Appendix A, Table 7).\nFor each dataset, we note the values in these lists\nbelow.\n• HateXplain: Here, explanation_type is\n“extract the words from the post that you\nfound as hate speech or offensive” and\nexplanation_format is “the list of extracted\nwords, separated by ”. Enclose the list with <\n< < > > >”\n• ToxicSpans: Here, explanation_type is “ex-\ntract the words from the post that you found as\ntoxic” and explanation_format is “the list\nof extracted words, separated by ”. Enclose\nthe list with < < < > > >”\n• Implicit hate : Here, explanation_type\nis “with an explanation in 15 words” and\nexplanation_format is “the explanation en-\nclosed in < < < > > >”\nIn the templates Vanilla + Exp (input)and Vanilla\n+ Defn + Exp (input) we use a single variable\nexplanation (see Appendix A, Table 7). For each\ndataset, we note the value in this list below.\n• HateXplain: explanation is “the rationales\n{rationales} as an explanation”.\n• ToxicSpans: explanation is “the span {span}\nas an explanation”.\n• Implicit hate: explanation is “the implied\nstatement {implied statement} as an explana-\ntion”.\n(+) targets A very important information in any\nhate speech detection pipeline is the victim com-\nmunity the abusive/hate speech targets. Here again\nwe test two hypotheses – (a) whether providing\ntarget to LLMs as inputs (corresponding to the tem-\nplates Vanilla + Tar (input) and Vanilla + Defn\n+ Tar (input)) improve its labelling decisions and\n6119\nDatasets F1-Score Precision Recall\nHateXplain 0.698 0.687 NR\nImplicit hate (full) 0.54 0.64 0.47\nToxicSpans 0.31 0.79 0.20\nTable 2: BERT-Hatexplain inference on three datasets used\nto prompt the LLMs. NR: not reportedin the original pa-\nper (Mathew et al., 2020).\n(b) whether asking LLMs for the target information\nforces it to predict better labels and as well gener-\nate correct targets (corresponding to the templates\nVanilla + Tar (output)and Vanilla + Defn + Tar\n(output)). In the templates Vanilla + Tar (out-\nput) and Vanilla + Defn + Tar (output) we use\ntwo variables target_type and target_format\n(see Appendix A, Table 7). For all the datasets, we\nreplace the variable target_type with “also men-\ntion which group of people does it target” and the\nvariable target_format with “list targeted groups\nenclosed in < < < > > >”. In the templates Vanilla\n+ Tar (input) and Vanilla + Defn + Tar (input)\nwe replace the variable targets (see Appendix A,\nTable 7) with the ground truth target community\ninformation which is only applicable for the Hat-\neXplain and the implicit hate speech datasets.\n6 Results\nIn this section, we note the results of the dif-\nferent prompt variations on the models for the\nthree datasets. As a baseline, we consider BERT-\nHateXplain (Mathew et al., 2020) and run the\nmodel on the implicit hate and ToxicSpan datasets.\nWe have taken the results for the HateXplain\ndataset from the original paper. Table 2 shows\nthe results of this baseline.\nOur key results are noted in Tables 3, 4 and 5\nfor the HateXplain, implicit hate and ToxicSpans\ndataset respectively.\nVanilla prompts : In terms of vanilla\nprompts, flan-T5-large performs better than\ngpt-3.5-turbo-0301 and text-davinci-003\nfor the HateXplain and implicit hate (0.59 and 0.63\nF1-scores respectively) datasets. Nevertheless,\nit performs at par with the other two models\nfor the ToxicSpans dataset. Among the larger\nmodels, we find that text-davinci-003 is\nbetter than the gpt-3.5-turbo-0301 in all the\ndatasets in terms of macro F1-score, reporting\n15.38%, 12.50% and 4.41% higher values for\nHateXplain, implicit hate, and ToxicSpans\nrespectively. Overall the performance for the\nHateXplain (0.39 in gpt-3.5-turbo-0301 and\n0.45 in text-davinci-003) and the implicit\nhate (0.32 in gpt-3.5-turbo-0301 and 0.36 in\ntext-davinci-003) datasets are much less than\nthe ToxicSpans (0.68 in gpt-3.5-turbo-0301\nand 0.71 in text-davinci-003) dataset.\n(+) definitions: Adding definitions to the prompts\ndoes not always help in improving the per-\nformance. In terms of F1-score, for Hat-\neXplain, we see an improvement of 25.64%\nfor gpt-3.5-turbo-0301 while the performance\nworsens by 17.78% for text-davinci-003 and\n20.34% for flan-T5-large. The situation is re-\nverse for the ToxicSpans dataset where we see\nan improvement of 1.40% for text-davinci-003\nand 13.75% for flan-T5-large while it worsens\nby 7.35% for gpt-3.5-turbo-0301. For Toxi-\ncSpans, adding definitions to the input prompt\ngives the best performance across all the prompt-\ning strategies. For implicit hate, there is an im-\nprovement of 12.50%, 16.67% and 4.76% for\ngpt-3.5-turbo-0301, text-davinci-003 and\nflan-T5-large respectively.\n(+) explanations: As discussed earlier, we\nexploit the power of explanations/rationales in\ntwo ways. For the case where we ask the\nmodel to generate explanations along with the\nlabel in the output, we see a similar trend like\nadding definitions. In terms of F1-score, for\nHateXplain, we see an improvement of 23.07%\nfor gpt-3.5-turbo-0301 while the performance\nworsens by 8.89% for text-davinci-003 and\n10.17% for flan-T5-large. The situation is\nsimilar for the ToxicSpans dataset we see com-\nparable performance for gpt-3.5-turbo-0301\nwhile it worsens by 9.86% for text-davinci-003\nbut it improves by 18.84 % for flan-T5-large.\nFor implicit hate, there is improvement of\n6.25% and 22.22% for gpt-3.5-turbo-0301 and\ntext-davinci-003 respectively but it worsens by\n11.11% for flan-T5-large. We further compare\nthe generated explanations with the already avail-\nable ground truth explanations using BERTScore\nfor the implicit hate dataset and sentence-BLEU\nfor the other two datasets. We observe that the\ngpt-3.5-turbo-0301 model generates better ex-\nplanations (averaged over all data points) than the\ntext-davinci-003 model for the HateXplain and\nthe implicit hate datasets. For the ToxicSpans\ndataset the results are reversed.\nWhen we add the respective explana-\ntions/rationales in the input as prompts, in\n6120\nterms of F1-score, for HateXplain the performance\nis 12.82% better for gpt-3.5-turbo-0301 and\n15.55% better for text-davinci-003 and compa-\nrable for flan-T5-large. For implicit hate the\nresults are 3.03% better for gpt-3.5-turbo-0301\nand 8.33% better for text-davinci-003 but\nworsens by 14.29% for flan-T5-large. The\nF1-scores for ToxicSpans dataset are 8.11%\nbetter for gpt-3.5-turbo-0301 and 20.29% for\nflan-T5-large but significantly worse (∼9.86%)\nfor text-davinci-003.\n(+) targets: We exploit the power of targets\nagain in two ways. For the case, where we ask\nthe model to generate target along with the label\nin the output, we again see a similar trend like\nadding definitions. In terms of F1-score, for\nHateXplain, we see an improvement of 28.20%\nfor gpt-3.5-turbo-0301 while the performance\nworsens by 15.56% for text-davinci-003 and\n11.86% for flan-T5-large. The situation is\nsimilar for the ToxicSpans dataset we see compara-\nble performance for gpt-3.5-turbo-0301 while\nit worsens by 11.27% for text-davinci-003\nand 18.84% for flan-T5-large. For im-\nplicit hate, there is improvement of 9.38%\nand 30.56% for gpt-3.5-turbo-0301 and\ntext-davinci-003 respectively and comparable\nresults for flan-T5-large.\nFor the case, where we use the target as in-\nputs in the prompt we see improvement for both\nthe datasets where the target is present in the\nground truth except for flan-T5-large. In terms\nof F1-score, for HateXplain, we see an improve-\nment of 33.33% for gpt-3.5-turbo-0301 and\n26.67% for text-davinci-003 and a drop in per-\nformance by 6.78% for flan-T5-large. For im-\nplicit dataset, the ground truth targets are present\nonly for the implicit hate data points. Thus while\ncomparing with the Vanilla setup, we only con-\nsider the implicit hate data points to have a fair\ncomparison. The revised F1 scores for the Vanilla\nsetup become 0.52 for gpt-3.5-turbo-0301\nand 0.68 for text-davinci-003. Comparison\nwith this revised values show an improvement\nof 17.30% for gpt-3.5-turbo-0301 while for\ntext-davinci-003 the results remain roughly un-\nchanged. In case of flan-T5-large the perfor-\nmance drops by 9.52%. For ToxicSpans we do not\nhave the target annotated and thus we skip this ex-\nperiment for this dataset. Overall target as inputs\noutperforms most of the other prompt strategies\nacross both these models and the two datasets. This\nleads us to believe that future annotation exercises\nfor training hate speech detection models should\nalmost always benefit if the target information is\nalso annotated.\nCombinations: Next, we evaluate the cases when\nwe utilise definitions as well as another additional\nprompt strategy (target/explanation at input/output).\nThe performance of adding explanations either at\ninput/output along with definitions does not help\nthe models much with the performance remain-\ning comparable, e.g., for gpt-3.5-turbo-0301\non HateXplain (+) explanation (input) with defi-\nnition vs with (+) explanation (input) or worsens\ne.g., for gpt-3.5-turbo-0301 on HateXplain (+)\nexplanation (output) with definition vs with (+)\nexplanation (output). The quality of the explana-\ntions generated (in terms of average BERTScore\nor sentence-BLEU) are almost always worse in\npresence of the definitions. worse Similar situ-\nation arises when we add definitions with target\n(input/output) where the performance either re-\nmains comparable or worsens. Only in the case\nof the ToxicSpans dataset, we see that adding\nboth explanation (input) and definition for the\ngpt-3.5-turbo-0301 and flan-T5-large model\nand gives the best performance.\n7 Error analysis and attack points\nIn this section we shall first discuss the cases where\nthe models encounter a large number of misclassi-\nfications and then outline a technique to induce a\ntypology of the different attack points to which the\nmodels are vulnerable.\nSignificance test : We compare the results of\nthe vanilla model for each dataset with the best\nperforming prompt plus model combination us-\ning the Mann-Whitney U test. We find that\nthe results are significant. We next present the\np-values for each – (a) gpt-3.5-turbo-0301 +\nHateXplain: p = 0 .0042 (vanilla vs target-\nat-input), (b) gpt-3.5-turbo-0301 + implicit\nhate: p = 0 .0 (vanilla vs target-at-input),\n(c) gpt-3.5-turbo-0301 + ToxicSpans: p =\n0.00088 (vanilla vs explanation-at-input), (d)\ntext-davinci-003 + HateXplain: p = 9.946e−1\n(vanilla vs target-at-input), (e) text-davinci-003\n+ implicit hate: p = 4.441e −16 (vanilla vs target-\nat-input), (f) text-davinci-003 + ToxicSpans:\np = 0.01524 (vanilla vs definition-at-input), (g)\nflan-T5-large + HateXplain: p = 0.0 (vanilla vs\n6121\nModel Strategies Metrics\nEx (o) Ta (o) D Ex (i) Ta (i) Acc Pre Rec F1 BS/\nBL\nGPT-3.5\n0.45 0.54 0.46 0.39\n/Check-mark 0.50 0.56 0.52 0.48 0.15\n/Check-mark0.50 0.63 0.53 0.49\n/Check-mark /Check-mark0.49 0.60 0.52 0.49 0.13\n/Check-mark0.48 0.55 0.49 0.44\n/Check-mark0.53 0.60 0.56 0.52\n/Check-mark /Check-mark0.52 0.60 0.55 0.50\n/Check-mark /Check-mark0.52 0.63 0.55 0.51\n/Check-mark0.52 0.61 0.55 0.50\n/Check-mark /Check-mark0.49 0.63 0.53 0.47\nDavinci\n0.47 0.55 0.47 0.45\n/Check-mark 0.46 0.53 0.44 0.41 0.05\n/Check-mark0.44 0.58 0.44 0.37\n/Check-mark /Check-mark0.41 0.57 0.41 0.34 7e-4\n/Check-mark /Check-mark0.46 0.56 0.45 0.41\n/Check-mark0.53 0.57 0.51 0.52\n/Check-mark0.57 0.62 0.57 0.57\n/Check-mark /Check-mark0.54 0.64 0.53 0.53\n/Check-mark0.44 0.55 0.43 0.38\n/Check-mark /Check-mark0.41 0.49 0.42 0.33\nFlan-T5\n0.60 0.70 0.65 0.59\n/Check-mark 0.54 0.63 0.60 0.53 0.12\n/Check-mark0.51 0.67 0.58 0.47\n/Check-mark /Check-mark0.57 0.57 0.57 0.57 0.02\n/Check-mark /Check-mark0.54 0.69 0.60 0.51\n/Check-mark0.60 0.62 0.62 0.60\n/Check-mark0.56 0.64 0.61 0.55\n/Check-mark /Check-mark0.62 0.67 0.65 0.62\n/Check-mark0.64 0.70 0.57 0.52\n/Check-mark /Check-mark0.62 0.61 0.61 0.61\nTable 3: Results for the HateXplain dataset with different\nprompt variations. Ex: explanation, Ta: target, D: definition,\ni: input, o: output. Acc: accuracy, Pre: precision, Rec: recall,\nBS: BERTScore, BL: sentence-BLEU. The best results are\nhighlighted.\ndefinition+target-at-input), (h) flan-T5-large +\nimplicit hate: p = 0.000358 (vanilla vs definition-\nat-input), (i) flan-T5-large + ToxicSpans: p =\n0.0 (vanilla vs explanation -at-input).\nCases of misclassification : For the implicit\nhate dataset we observe that in case of the\ngpt-3.5-turbo-0301 model, for all the different\nprompt variants the largest number of misclassifica-\ntions is from the non-hate to the implicit hate class.\nFor the text-davinci-003 model the major obser-\nvations are as follows. In the vanilla setup (with or\nwithout definition), the implicit hate class is most\noften confused with the explicit hate class. How-\never, if the prompt has an explanation component\n(either at input or at output), once again, there is\na large number of misclassifications from the non-\nhate to the implicit hate class. For flan-T5-large\nwe observe that the model fails to classify the im-\nplicit hate speech class. The implicit hate is classi-\nfied as non-hate or explicit hate following the trend\nof the other two models.\nFor the HateXplain dataset we observe that in\ncase of the gpt-3.5-turbo-0301 model, for all\nModel Strategies Metrics\nEx (o) Ta (o) D Ex (i) Ta (i) Acc Pre Rec F1 BS/\nBL\nGPT-3.5\n0.35 0.45 0.51 0.32\n/Check-mark 0.36 0.46 0.52 0.34 0.85\n/Check-mark0.41 0.49 0.47 0.36\n/Check-mark /Check-mark0.41 0.49 0.49 0.36 0.85\n/Check-mark0.37 0.44 0.41 0.33\n/Check-mark /Check-mark0.45 0.45 0.40 0.34\n/Check-mark0.61 0.68 0.68 0.61\n/Check-mark /Check-mark0.56 0.69 0.65 0.55\n/Check-mark0.38 0.47 0.49 0.35\n/Check-mark /Check-mark0.38 0.48 0.45 0.33\nDavinci\n0.52 0.44 0.54 0.36\n/Check-mark 0.50 0.47 0.54 0.44 0.79\n/Check-mark0.51 0.44 0.52 0.42\n/Check-mark /Check-mark0.36 0.42 0.50 0.32 0.78\n/Check-mark0.45 0.40 0.45 0.39\n/Check-mark /Check-mark0.44 0.46 0.46 0.42\n/Check-mark0.73 0.70 0.68 0.68\n/Check-mark /Check-mark0.73 0.70 0.66 0.67\n/Check-mark0.61 0.48 0.56 0.47\n/Check-mark /Check-mark0.45 0.45 0.53 0.39\nFlan-T5\n0.67 0.64 0.62 0.63\n/Check-mark 0.66 0.66 0.58 0.56 0.86\n/Check-mark0.69 0.67 0.66 0.66\n/Check-mark /Check-mark0.67 0.66 0.66 0.66 0.83\n/Check-mark /Check-mark0.64 0.62 0.54 0.50\n/Check-mark0.64 0.61 0.56 0.54\n/Check-mark0.64 0.58 0.57 0.57\n/Check-mark /Check-mark0.62 0.56 0.55 0.54\n/Check-mark0.63 0.64 0.65 0.63\n/Check-mark /Check-mark0.67 0.64 0.61 0.61\nTable 4: Results for the implicit hate dataset with different\nprompt variations. Ex: explanation, Ta: target, D: definition,\ni: input, o: output. Acc: accuracy, Pre: precision, Rec: recall,\nBS: BERTScore, BL: sentence-BLEU. The best results are\nhighlighted.\nthe different prompt variants the largest number\nof misclassifications is from the normal to the of-\nfensive class. Another confusion that the model\noften faces is between the hate and the offensive\nclass; if the prompts do not contain the definition\n(of hate/offensive speech), then offensive speech\nis largely mislabelled as hate speech while the re-\nsults are exactly reversed if the prompts contain\nthe definition. For the text-davinci-003 model\nonce again we observe for all the different prompt\nvariants the largest number of misclassifications\nis from the normal to the offensive class. Further,\nirrespective of whether the prompts contain the def-\ninition or not, hate speech is heavily mislabelled\nas offensive speech. For flan-T5-large both of-\nfensive and hatespeech classes are missclassified\nas normal speech for all prompt variations.\nFor the ToxicSpans dataset in case of the\ngpt-3.5-turbo-0301 model, the non-toxic data\npoints are heavily misclassified as toxic for\nall prompt variants. Curiously, adding defini-\ntion to the prompts increases this misclassifica-\ntion. These observations remain similar for the\n6122\nModel Strategies Metrics\nEx (o) Ta (o) D Ex (i) Ta (i) Acc Pre Rec F1 BS/BL\nGPT-3.5\n0.69 0.73 0.69 0.68\n/Check-mark 0.70 0.75 0.70 0.68 0.26\n/Check-mark0.67 0.75 0.67 0.63\n/Check-mark /Check-mark0.66 0.76 0.66 0.72 0.20\n/Check-mark0.75 0.78 0.75 0.74\n/Check-mark /Check-mark0.71 0.79 0.71 0.69\n/Check-mark /Check-mark- - - -\n/Check-mark0.70 0.74 0.70 0.69\n/Check-mark /Check-mark0.70 0.75 0.70 0.68\nDavinci\n0.71 0.71 0.71 0.71\n/Check-mark 0.66 0.69 0.66 0.64 0.45\n/Check-mark0.72 0.72 0.72 0.72\n/Check-mark /Check-mark0.71 0.71 0.71 0.71 0.35\n/Check-mark0.65 0.66 0.65 0.64\n/Check-mark /Check-mark0.68 0.69 0.68 0.68\n/Check-mark /Check-mark- - - -\n/Check-mark0.66 0.69 0.66 0.64\n/Check-mark /Check-mark0.69 0.71 0.69 0.68\nFlan-T5\n0.70 0.76 0.70 0.69\n/Check-mark 0.78 0.84 0.78 0.82 0.35\n/Check-mark0.80 0.81 0.80 0.80\n/Check-mark /Check-mark0.82 0.84 0.82 0.82 0.43\n/Check-mark0.83 0.84 0.83 0.83\n/Check-mark /Check-mark0.83 0.83 0.83 0.83\n/Check-mark /Check-mark- - - -\n/Check-mark0.62 0.76 0.62 0.56\n/Check-mark /Check-mark0.79 0.80 0.79 0.78\nTable 5: Results for the ToxicSpans dataset with different\nprompt variations. Ex: explanation, Ta: target, D: definition,\ni: input, o: output. Acc: accuracy, Pre: precision, Rec: recall,\nBS: BERTScore, BL: sentence-BLEU. The best results are\nhighlighted.\ntext-davinci-003 model. For flan-T5-large\nnon-toxic data points are misclassified as toxic for\nall the prompt variations except for when target\nis asked to be generated in which case toxic data-\npoints are misclassified as non-toxic.\nIn order to better elicit the above observations,\nwe present the confusion matrices for the best\nprompt combinations for each model and each\ndataset in Appendix C.\nTypology induction: In order to analyse the data\npoints where the models are most vulnerable we\nemploy the following heuristics for each dataset.\nIn these heuristics we always consider the Vanilla\n+ Defn + Exp (output) setup.\nFor the implicit hate dataset we sort the data\npoints in non-decreasing order based on the\nBERTScore between the ground truth implied state-\nment and the generated explanation at the output.\nStarting from the top of this list, we consider the\ndata points that are either not_hate in the ground\ntruth and misclassified as implicit_hate or vice\nversa by all the three models. Note that these\ndata points constitute the cases where the models\nmisclassify and provide poor explanation for their\nclassification decision. These data points are then\npassed through LDA (Jelodar et al., 2019) (number\nof topics, K = 3) to induce the types.\nFor the HateXplain and ToxicSpans datasets we\nsort the data points in non-decreasing order based\non the sentence-BLEU scores between the ground\ntruth rationales and the generated rationales at the\noutput. We select 80 low-scoring data points ac-\ncording to the BLEU/BERTScore measure. For\nthe HateXplain (ToxicSpans) dataset, starting from\nthe top of this list, we consider the data points that\nare either normal (non_toxic) in the ground truth\nand misclassified as hate speech/offensive (toxic)\nor vice versa by all the three models. These points\nare then passed through LDA (number of topics,\nK = 3) to induce the types. For each topic, we\nchoose four words which have the highest proba-\nbility of association with the topic. These collected\nset of words are then manually coded with topic\nnames by two researchers with long experience in\nhate speech research. In order to obtain a semantic\nclarity upon the derived topic names, a meticulous\nmanual annotation process is undertaken. This an-\nnotation task is done by three domain experts, and\nnotably, unanimous consensus is reached among\nall annotators regarding the semantic interpretation\nand nomenclature of each delineated topic name.\nWe note the emerging typologies in the Table 6.\nObservations from the induced typology : For\neach dataset, we note some of the interesting ty-\npologies that emerged. (1) For the implicit hate\ndataset, one curious case of misclassification (non-\nhate →implicit hate) is the presence of the ‘racist’\nword, while in another category we find the model\nmarked something as implicit hate because it has\npro-white sentiments. Even when post representing\nnews articles or opinion pieces contain sensitive\nwords like ‘blacks’, ‘antifa’ etc., the models la-\nbel them as implicit hate. (2) For the HateXplain\ndataset, we find that hateful or offensive posts get\nmisclassified to normal due to multiple reasons –\n(a) presence of negations like ‘neither’, ‘nor’, ‘no’\netc. in the post, (b) vocabulary gap where the mod-\nels do not know the usage of words like ‘muzrat’\nused in the post, and (c) presence of polysemous\nwords in the post with one of the meanings usually\nbeing a slang (usually not found in standard English\ndictionaries), e.g., ‘dyke’, ‘furries’, ‘furfaggotry’\netc. (3) For ToxicSpans dataset, we note that ideo-\nlogical posts and posts with fact-checking related\nstatements about political news often get misclas-\nsified as toxic. On the other hand, posts that are\nimplicitly toxic often get misclassified as non-toxic\n6123\nDataset Topic cluster Examples GT PR Type\nImplicit hate\nantifa, self, make, right, time, rt,\nmemorial, requires, government, say bring back huac rt : you can be racist all\nyou want but socialism is another beast\nn_h imp_h racist\nthese are the same people who yell about\nhow they’re not racist . this thread is so\nsad\nn_h imp_h racist\nwhite, people, good, anti, nationalist,\nright, chief, staff, south, cruz not enough power to make a white lol n_h imp_h pro-white\nhow is appreciating your heritage white\nprivilege ? we’re the only ones not al-\nlowed to do it\nn_h imp_h pro-white\nracist, white, america, make, people,\namericans, know, really, trump, new yeah but who is defining intolerance . an-\ntifa are intolerant .\nn_h imp_h stmt_with_sensitive_words\nblack #unemployment rate : 9.2 % more\nthan double white unemployment of 4.4\n% via #hireamerican\nn_h imp_h stmt_with_sensitive_words\nHateXplain\ntoxic, talk, men, masculinity, freakin,\nattack, nigger, slut, people, women tbtc take back the country toxic masculin-\nity patriarchy harm men noxious female-\nness and public slut posturing empower\nwomen neither feminism nor now could\nhave pulled this off this is the doing of\ntaxpayer supported state university sys-\ntem maga\no n negation\nnote no nigger was harmed during the\nmaking of this film.\nh n negation\nwhite, like, line, did, large, muzzies,\nwomen, tactics, scarf, counter why he can not get a hoe that work minus\nthe crazy\no n vocab_gap\nmassive muzrat espionage h n vocab_gap\ncommon, kike, christian, better, stick,\nprefer, pissing, discussion, sure, provide additionally furries can also transmit the\ninfectious disease of furfaggotry to others\nh n polysemy\na dyke puts the mannish moves on a\nfemme at a lesbian bar in paris <num-\nber> photograph by george brassai\no n polysemy\nToxicSpans\ntrump, right, good, man, state,\nlike, stupid, don, money, did Right Charlie the media fabricated all\nthose live broadcast during the primaries.\nThen they altered everything Trump said\nduring the campaign. Do you have Twit-\nter running 24/7 to keep up with Donald’s\nlatest rants?\nn_t t ideology\nThe Liberals leasing the Trudeau name\nfor their leadership is turning out to be a\nbig, fat, failed experiment!!\nn_t t ideology\npeople, trump, just, know, tax,\nlike, think, don, vote, need The headline for this article has changed\nat least twice since it was originally\nposted yesterday. Here’s the latest update:\nUnhinged Trump re-emerges, defending\nfirst month in White House\nn_t t fact_check_pol_news\nThis article is entirely WRONG! An on-\ngoing deficit will disintegrate the finan-\ncial system AND THE COUNTRY in less\nthan 30 years. . . . Computer projections\nby more than one analyst suggest a \"ki-\nnetic\" outcome within 15 years. . .\nn_t t fact_check_pol_news\njust, like, make, stupid, sure, don,\nperson, people, trump, does Oh, gay and black, you just caused all\nour white christian friends here to start\nsalivating at the same time, for what I’m\nnot sure.\nt n_t implicit_semantics\nThis individual does need therapy, I hope\nhe gets the help he needs as well as one\nof those ridiculously light sex offender\nsentence. This system is broken when it\ncomes to crimes of this nature. The list\nwas designed to be a warning for parents,\nand yet\nt n_t implicit_semantics\nTable 6: The typologies induced from the error cases for each dataset. GT: ground truth, PR: prediction, n_h: not_hate, imp_h:\nimplicit_hate, o: offensive, n: normal, h: hate speech, n_t: non_toxic, t: toxic, stmt_with_sensitive_words: statement with\nsensitive words, vocab_gap: vocabulary gap, fact_check_pol_news: statement on fact checking information about political news.\nby the models.\n8 Conclusion\nIn this work, we extensively study three LLMs\nacross three datasets on a variety of prompt setups.\nOverall these models, though advanced, still face\nlot of challenges in hate speech detection in their\nvanilla setting. Our prompt strategies, when ap-\nplied individually, improve the performance of such\nmodels. However, merging of the prompt strategies\ndo not help much. Last, we do a detailed error anal-\nysis and typology coding to find ‘jailbreak’ points\nwhere these models are vulnerable.\n6124\n9 Limitations\nWe mostly focus on English datasets in this paper\nsince we wanted to present a detailed analysis of\nthe use of additional context (explanation, target\ncommunity) which is often not present in multilin-\ngual datasets. We use these LLMs as a black box\nhence we do not know the inner workings of the\nproprietary LLMs. Lastly, although we cover a lot\nof well thought out prompt variations in our paper,\nthese variations are not exhaustive.\n10 Ethical statement\nHere we discuss the ethical considerations that\nwere not explicitly noted in the body of the paper.\nWe use three of the LLMs gpt-3.5-turbo-0301,\ntext-davinci-003 and flan-T5-large to detect\nhate speech. These experiments were purely done\nfrom a research point of view; the actual applica-\ntion of such model in real world should be done\nwith caution. This is also evident from the chal-\nlenges these models face while classifying different\nforms of hate.\nReferences\nHow do text-davinci-002 and text-davinci-\n003 differ? | openai help center.\nhttps://help.openai.com/en/articles/6779149-how-\ndo-text-davinci-002-and-text-davinci-003-differ.\n(Accessed on 06/20/2023).\nUmang Bhatt, Alice Xiang, Shubham Sharma, Adrian\nWeller, Ankur Taly, Yunhan Jia, Joydeep Ghosh,\nRuchir Puri, José M. F. Moura, and Peter Eckersley.\n2020. Explainable machine learning in deployment.\nIn Proceedings of the 2020 Conference on Fairness,\nAccountability, and Transparency, FAT* ’20, page\n648–657.\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum\nThain, and Lucy Vasserman. 2019. Nuanced metrics\nfor measuring unintended bias with real data for text\nclassification. CoRR, abs/1903.04561.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nMai ElSherief, Caleb Ziems, David Muchlinski, Vaish-\nnavi Anupindi, Jordyn Seybolt, Munmun De Choud-\nhury, and Diyi Yang. 2021. Latent hatred: A bench-\nmark for understanding implicit hate speech. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 345–363.\nAssociation for Computational Linguistics.\nFan Huang, Haewoon Kwak, and Jisun An. 2023. Is\nchatgpt better than human annotators? potential and\nlimitations of chatgpt in explaining implicit hate\nspeech. In Companion Proceedings of the ACM\nWeb Conference 2023, WWW ’23 Companion, page\n294–297.\nHamed Jelodar, Yongli Wang, Chi Yuan, Xia Feng,\nXiahui Jiang, Yanchao Li, and Liang Zhao. 2019.\nLatent dirichlet allocation (lda) and topic modeling:\nmodels, applications, a survey. Multimedia Tools and\nApplications, 78:15169–15211.\nLingyao Li, Lizhou Fan, Shubham Atreja, and Libby\nHemphill. 2023. “hot” chatgpt: The promise of chat-\ngpt in detecting and discriminating hateful, offensive,\nand toxic comments on social media.\nAliza Luft. 2019. Dehumanization and the normaliza-\ntion of violence: It’s not what you think – items.\n6125\nhttps://tinyurl.com/s4mxz7j2. (Accessed on\n05/02/2021).\nBinny Mathew, Punyajoy Saha, Seid Muhie Yimam,\nChris Biemann, Pawan Goyal, and Animesh Mukher-\njee. 2020. Hatexplain: A benchmark dataset\nfor explainable hate speech detection. CoRR,\nabs/2012.10289.\nBinny Mathew, Punyajoy Saha, Seid Muhie Yimam,\nChris Biemann, Pawan Goyal, and Animesh Mukher-\njee. 2021. Hatexplain: A benchmark dataset for ex-\nplainable hate speech detection. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 35, pages 14867–14875.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nJohn Pavlopoulos, Léo Laugier, Alexandros Xenos, Jef-\nfrey Sorensen, and Ion Androutsopoulos. 2022. From\nthe detection of toxic spans in online discussions to\nthe analysis of toxic-to-civil transfer. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (ACL 2022)., Dublin,\nIreland. Association for Computational Linguistics.\nJanikke Solstad Vedeler, Terje Olsen, and John Eriksen.\n2019. Hate speech harms: A social justice discussion\nof disabled norwegians? experiences. Disability &\nSociety, 34(3):368–383.\nEmily A V ogels. 2021. The state of online\nharassment | pew research center. https:\n//www.pewresearch.org/internet/2021/\n01/13/the-state-of-online-harassment/ .\n(Accessed on 05/02/2021).\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. Bertscore: Evaluating text\ngeneration with bert. In International Conference on\nLearning Representations.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B. Hashimoto.\n2023a. Benchmarking large language models for\nnews summarization.\nWenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan,\nand Lidong Bing. 2023b. Sentiment analysis in the\nera of large language models: A reality check.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2023. Can chatgpt understand too? a\ncomparative study on chatgpt and fine-tuned bert.\nYiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui,\nand Gareth Tyson. 2023. Can chatgpt reproduce\nhuman-generated labels? a study of social computing\ntasks.\nCaleb Ziems, William Held, Omar Shaikh, Jiaao Chen,\nZhehao Zhang, and Diyi Yang. 2023. Can large lan-\nguage models transform computational social sci-\nence?\nA Prompt strategies\nThe Table 7 shows the different prompt variants we\nuse in this study.\nB Definitions\nB.1 Implicit hate dataset\n• implicit_hate: Implicit hate speech is defined\nby coded or indirect language that disparages\na person or group on the basis of protected\ncharacteristics like race, gender, and cultural\nidentity.\n• explicit_hate: Explicit hate refers to openly\nexpressed, direct forms of hatred and preju-\ndice toward individuals or groups based on\ntheir characteristics.\n• not_hate: This class refers to speech or ac-\ntions that do not involve any form of hatred,\nprejudice, or discrimination toward individu-\nals or groups based on their characteristics.\nB.2 HateXplain dataset\n• hate speech: Any speech or text that attacks\na person or group on the basis of attributes\nsuch as race, religion, ethnic origin, national\norigin, gender, disability, sexual orientation,\nor gender identity.\n• offensive: The text or speech which uses abu-\nsive slurs or derogatory terms but may not be\nhate speech.\n• normal: The text which is neither offensive\nor hate speech and adheres to social norms.\nB.3 ToxicSpans dataset\n• Toxic: In social media and online forum, toxic\ncontent can be defined as rude, disrespectful,\nor unreasonable posts that would make users\nwant to leave the conversation.\n• Non_toxic: The speech or text that is not toxic\nand is fit for use in conversation.\n6126\nCategory Prompt templates\nVanilla\nClassify the post delimited by three backticks as list_of_labels.\nProvide the answer as either list_of_labels only.\nexample_outputs\n“‘post“‘\nVanilla + Defn\nConsider the following definitions.\nlist_of_defintions\nBased on this classify the post delimited by three backticks as list_of_labels\nProvide answer in the format : either list_of_labels only.\nexample_outputs\n“‘post“‘\nVanilla + Exp (output)\nClassify the post delimited by three backticks as list_of_labels and explanation_type.\nProvide answer in the format : either list_of_labels only followed by explanation_format\nexample_outputs\n“‘post“‘\nVanilla + Exp (input)\nClassify the post delimited by three backticks as list_of_labels taking into account the explanation\nfor why a post should be considered list_of_labels.\nProvide answer in the format : either list_of_labels only.\nexample_outputs\n“‘post“‘\nVanilla + Tar (output)\nClassify the post delimited by three backticks as list_of_labels and target_type\nProvide answer in the format : either list_of_labels only followed by target_format\nexample_outputs\n“‘post“‘\nVanilla + Tar (input)\nClassify the post delimited by three backticks as list_of_labels with respect to the victim community targets .\nProvide answer in the format : either list_of_labels only.\nexample_outputs\n“‘post“‘\nVanilla + Defn + Exp (input)\nConsider the following definitions.\nlist_of_defintions\nBased on this classify the post delimited by three backticks as list_of_labels taking into account the explanation\nfor why a post should be considered list_of_labels\nProvide answer in the format : either list_of_labels only.\nexample_outputs\n“‘post“‘\nVanilla + Defn + Exp (output)\nConsider the following definitions.\nlist_of_defintions\nBased on this classify the post delimited by three backticks as list_of_labels and explanation_type.\nProvide answer in the format : either list_of_labels only followed by explanation_format\nexample_outputs\n“‘post“‘\nVanilla + Defn + Tar (output)\nConsider the following definitions.\nlist_of_defintions\nBased on this classify the post delimited by three backticks as list_of_labels and target_type\nProvide answer in the format : either list_of_labels only followed by target_format\nexample_outputs\n“‘post“‘\nVanilla + Defn + Tar (input)\nConsider the following definitions.\nlist_of_defintions\nBased on this classify the post delimited by three backticks as list_of_labels with respect to the victim community targets .\nProvide answer in the format : either list_of_labels only.\nexample_outputs\n“‘post“‘\nTable 7: The different prompts used in our experiments.\n6127\nFigure 1: Vanilla + target_input for implicit hate.\nFigure 2: Vanilla + target_input for HateXplain.\nC Confusion matrix of best performing\nprompt combinations\nC.1 GPT-3.5\nThe confusion matrices for the three best prompt\ncombinations for the three datasets in connection\nto the gpt-3.5-turbo-0301 are illustrated in Fig-\nures 1, 2, and 3.\nC.2 Davinci\nThe confusion matrices for the three best prompt\ncombinations for the three datasets in connection\nto the text-davinci-003 are illustrated in Fig-\nures 4, 5, and 6.\nC.3 Flan-T5\nThe confusion matrices for the three best prompt\ncombinations for the three datasets in connection to\nthe flan-T5-large are illustrated in Figures 7, 8,\nand 9.\nFigure 3: Vanilla + explanation_input for ToxicSpans.\nFigure 4: Vanilla + target_input for implicit hate.\nFigure 5: Vanilla + target_input for HateXplain.\nFigure 6: Vanilla + definition_input for ToxicSpans.\nFigure 7: Vanilla + definition_input for implicit hate.\nFigure 8: Vanilla + definition_input + target_input for\nHateXplain.\nFigure 9: Vanilla + explanation_input for ToxicSpans.\n6128"
}