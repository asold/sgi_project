{
  "title": "How Linguistically Fair Are Multilingual Pre-Trained Language Models?",
  "url": "https://openalex.org/W3126947648",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2162966668",
      "name": "Monojit Choudhury",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2115313977",
      "name": "Amit Deshpande",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3023437581",
    "https://openalex.org/W2919290281",
    "https://openalex.org/W2915444840",
    "https://openalex.org/W2982180741",
    "https://openalex.org/W2905749056",
    "https://openalex.org/W2279316390",
    "https://openalex.org/W2773523653",
    "https://openalex.org/W3032388710",
    "https://openalex.org/W2807830867",
    "https://openalex.org/W3011663485",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W6767737316",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2061963820",
    "https://openalex.org/W2890994198",
    "https://openalex.org/W3012945277",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W3017311573",
    "https://openalex.org/W3004604609",
    "https://openalex.org/W2980404057",
    "https://openalex.org/W6741990110",
    "https://openalex.org/W6724650596",
    "https://openalex.org/W6641347922",
    "https://openalex.org/W2899641520",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W6638861442",
    "https://openalex.org/W6646321290",
    "https://openalex.org/W2807105703",
    "https://openalex.org/W2796825045",
    "https://openalex.org/W2952929029",
    "https://openalex.org/W2250513429",
    "https://openalex.org/W1988662426",
    "https://openalex.org/W2970413168",
    "https://openalex.org/W7028093957",
    "https://openalex.org/W2943927551",
    "https://openalex.org/W4206856021",
    "https://openalex.org/W1963703166",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W1861914525",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2952768586",
    "https://openalex.org/W3105492289",
    "https://openalex.org/W4287827771",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W3021766370",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2950733326",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W4243419638",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3014635508",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W4289293239",
    "https://openalex.org/W1830426376",
    "https://openalex.org/W3032020872",
    "https://openalex.org/W2963808661",
    "https://openalex.org/W3035379020",
    "https://openalex.org/W2963934714",
    "https://openalex.org/W1974210025",
    "https://openalex.org/W4285790115",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W1985077192",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W2029467699",
    "https://openalex.org/W3035257795",
    "https://openalex.org/W3014590323",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034284720"
  ],
  "abstract": "Massively multilingual pre-trained language models, such as mBERT and XLM-RoBERTa, have received significant attention in the recent NLP literature for their excellent capability towards crosslingual zero-shot transfer of NLP tasks. This is especially promising because a large number of languages have no or very little labeled data for supervised learning. Moreover, a substantially improved performance on low resource languages without any significant degradation of accuracy for high resource languages lead us to believe that these models will help attain a fairer distribution of language technologies despite the prevalent unfair and extremely skewed distribution of resources across the world’s languages. Nevertheless, these models, and the experimental approaches adopted by the researchers to arrive at those, have been criticised by some for lacking a nuanced and thorough comparison of benefits across languages and tasks. A related and important question that has received little attention is how to choose from a set of models, when no single model significantly outperforms the others on all tasks and languages. As we discuss in this paper, this is often the case, and the choices are usually made without a clear articulation of reasons or underlying fairness assumptions. In this work, we scrutinize the choices made in previous work, and propose a few different strategies for fair and efficient model selection based on the principles of fairness in economics and social choice theory. In particular, we emphasize Rawlsian fairness, which provides an appropriate framework for making fair (with respect to languages, or tasks, or both) choices while selecting multilingual pre-trained language models for a practical or scientific set-up.",
  "full_text": "How Linguistically Fair Are Multilingual Pre-Trained Language Models?\nMonojit Choudhury, Amit Deshpande\nMicrosoft Research Lab India\nfmonojitc, amitdeshg@microsoft.com\nAbstract\nMassively multilingual pre-trained language models, such as\nmBERT and XLM-RoBERTa, have received signiﬁcant atten-\ntion in the recent NLP literature for their excellent capabil-\nity towards crosslingual zero-shot transfer of NLP tasks. This\nis especially promising because a large number of languages\nhave no or very little labeled data for supervised learning.\nMoreover, a substantially improved performance on low re-\nsource languages without any signiﬁcant degradation of accu-\nracy for high resource languages lead us to believe that these\nmodels will help attain a fairer distribution of language tech-\nnologies despite the prevalent unfair and extremely skewed\ndistribution of resources across the world’s languages.\nNevertheless, these models, and the experimental approaches\nadopted by the researchers to arrive at those, have been crit-\nicised by some for lacking a nuanced and thorough compari-\nson of beneﬁts across languages and tasks. A related and im-\nportant question that has received little attention is how to\nchoose from a set of models, when no single model signiﬁ-\ncantly outperforms the others on all tasks and languages. As\nwe discuss in this paper, this is often the case, and the choices\nare usually made without a clear articulation of reasons or\nunderlying fairness assumptions. In this work, we scrutinize\nthe choices made in previous work, and propose a few dif-\nferent strategies for fair and efﬁcient model selection based\non the principles of fairness in economics and social choice\ntheory. In particular, we emphasize Rawlsian fairness, which\nprovides an appropriate framework for making fair (with re-\nspect to languages, or tasks, or both) choices while selecting\nmultilingual pre-trained language models for a practical or\nscientiﬁc set-up.\n1 Introduction\nNLP technologies require large amount of labeled and/or un-\nlabeled data for training. The distribution of resources across\nlanguages, on the other hand, is extremely skewed (Joshi\net al. 2020; Bender 2011), because data annotation is an ex-\npensive and effort-intensive affair. This digital divide is fur-\nther widened by modern technologies, especially the deep\nlearning approaches to NLP, which require even more data\nthat all but a handful of the world’s languages possess.\nSince their introduction in 2019, massively Multilingual\npre-trained language models (MultiLMs), such as Multilin-\nCopyright c\r 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ngual BERT (mBERT) (Devlin et al. 2019), XLM-RoBERTa\n(XLM-R) (Conneau et al. 2020), Massively Multilingual\nTranslation Encoder (MMTE) (Arivazhagan et al. 2019) and\nUnicoder (Liang et al. 2020), have received signiﬁcant at-\ntention from the NLP researchers. Their surprisingly well\ncrosslingual zero-shot transfer capabilities (Pires, Schlinger,\nand Garrette 2019; Wu and Dredze 2020) have revolution-\nized our approach to multilingual NLP, and offer a promise\nof sophisticated and efﬁcient NLP systems for all languages\nregardless of the availability of labeled data. This is because\nonce a MultiLM is pre-trained on unlabeled corpora of a\nlarge number (typically 100+) of languages, task-speciﬁc\nlabeled data for ﬁne-tuning is required only for one lan-\nguage, called the pivot (usually a high-resource language\nsuch as English). The ﬁne-tuned model works across all the\nlanguages that the MultiLM was pre-trained on, albeit with\nvarying degrees of success.\nThis crosslingual zero-shot setting also creates the fol-\nlowing interesting and important dilemma which researchers\nand engineers working in this ﬁeld are often faced with.\nImagine that a researcher has come up with two MultiLMs,\nAand B. She decides to test her models on a standard bench-\nmark, say XNLI (Conneau et al. 2018), which has training\ndata for Natural Language Inferencing task in English and\ntest data for 15 languages including English. She observes\nthat Aperforms better than Bon 10 languages, Bperforms\nbetter than Aon 3 languages, and on the remaining two, the\nmodels perform equally well. Should she declare that model\nAis better than Bbecause it outperforms the latter on most\nof the languages in XNLI? Or should she declare the one\nwith higher average accuracy as the winner? Or should it be\nsome other statistic, say the median or geometric mean of\nthe performances, according to which the models should be\ncompared?\nWe will refer to this dilemma as the Multilingual Model\nSelection Problem or MMSP for short. The problem is not\nonly philosophically interesting, but also has great practical\nsigniﬁcance for two reasons. First, before our researcher can\nresolve the dilemma, she has to decide what does “better”\nmean in the case of MultiLMs. Intuitively, average perfor-\nmance over the set of tested languages appears to be a sensi-\nble choice, and has been adopted by several researchers (e.g.,\nLiang et al. (2020); Aharoni, Johnson, and Firat (2019)).\nHowever, the average performance on a limited set of tasks\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n12710\nand languages does not capture practically important fac-\ntors such as the set of languages and tasks the model is\nexpected to work well on, and the amount of data across\nlanguages that the MultiLM was pre-trained and ﬁne-tuned\non (Hu et al. 2020; Wu and Dredze 2020; Lauscher et al.\n2020). More importantly, as far as we have seen, authors do\nnot provide any formal justiﬁcation or articulate the guiding\nprinciples behind their choice of the statistic.\nSecond, one should keep in mind that one of the primary\nadvantages as well as the reason behind the popularity of\nMultiLMs is their excellent crosslingual zero-shot transfer\nability. As Wu and Dredze (2020) point out, “top high re-\nsource languages are slightly hurt by massively multilingual\njoint training”; yet these are preferred because they offer a\nsolution for low-resource languages which do not have suf-\nﬁcient labeled data. Indeed, equal or equitable accuracy of\nMultiLMs across languages and tasks has been one of the\ncritical points of scrutiny in the recent times (Hu et al. 2020).\nIf this is the central tenet behind the conception and con-\nstruction of MultiLMs, then MMSP must be resolved in a\nmanner that conforms to this normative principle of “fair-\nness” or “distributive justice” across languages.\nIn this paper, drawing inspiration and ideas from the dis-\ncourse on fairness in machine learning, ethics, social choice\ntheory, economics and decision theory, we provide several\npossible resolution of the MMSP, which are driven by dif-\nferent choice of the normative principles of efﬁciency and\nfairness. We do not give any new results but instead scru-\ntinize a few popular and important works on MultiLMs by\nexplicitly calling out the principles of distributive justice en-\ntailed by the choices made by the researchers while resolv-\ning the MMSP, but not stated as such. As we shall see, most\nof the work, whenever possible, follow the Pareto-efﬁciency\nprinciple, i.e., choose the model which does as good or better\nthan all others on all languages that were tested for; other-\nwise, a utilitarian approach is adopted, where a simple un-\nweighted average performance across languages is used as\nthe model selection criterion. We further argue that the util-\nitarian approach does not conform to the egalitarian prin-\nciples on which MultiLMs are founded. Instead, given the\nskewed resource distribution across languages and other lim-\nitations of the current technology, a practical trade-off be-\ntween the egalitarian and utilitarian ideologies could be the\nprioritarian or Rawlsian principle (Rawls 1999) of distribu-\ntive justice, where one should select the model which max-\nimizes the minimum performance over all languages. Rawl-\nsian fairness is based on theprinciple of least difference, and\nproposes to narrow the gap between unequal accuracies or\nutilities, instead of equalizing them. Interestingly, Rawlsian\nfairness based resolution of MMSP is also the robust utilitar-\nian choice under an adversarial assumption, and it also en-\nsures a progressively more egalitarian distribution under the\nassumption that language resources for all languages will\ngrow over time, without being severely unfair to high per-\nforming languages in the short run.\nRecently several researchers have critiqued the utilitarian\napproach to MMSP (Wu and Dredze 2020; Hu et al. 2020);\nthe objective of those studies have been to empirically show\nthat in most cases a single Pareto-efﬁcient model does not\nexist. These critiques, however, do not propose any mecha-\nnism to resolve the MMSP under such a situation. Instead,\nthey suggest that given the current state of limited under-\nstanding and testing of the models on a handful of languages\nand tasks, the resolution of MMSP should be deferred till\nwe have a clearer understanding of the model performances.\nWhile we agree with the criticisms, and the necessity of\nwide-scale experimentation, it is unlikely that in the near fu-\nture we will have a MultiLM that is Pareto-efﬁcient across\nlanguages and tasks. Therefore, it is useful and important to\nresolve the MMSP under partial knowledge in such a man-\nner that the solutions hold irrespective of the state of tech-\nnology and resource availability.\nIt is important to note that the ongoing discourse on\nfairness in ML (Mehrabi et al. 2019; Barocas, Hardt,\nand Narayanan 2017; Leben 2020), and more speciﬁcally\nNLP (Blodgett et al. 2020), argue for individual and group\nfairness (Binns 2018). We are not aware of any work that\ndiscuss the speciﬁc issues of distributive justice when lan-\nguages, instead of individuals, are viewed as entities. While\na language can be equated to a group of individuals, as we\ndiscuss in Sec 4, there are important philosophical and prac-\ntical distinctions between these notions, such that they merit\nindependent treatments.\nThe rest of the paper is organized as follows. In Sec 2,\nwe formally introduce the MMSP and discuss its potential\nresolutions under various normative principles of distribu-\ntive justice. This section will also introduce the reader to\nthe basic concepts and some well established results of dis-\ntributive justice in social choice theory. Sec 3 reinterprets\nthe model selection decisions taken in the recent papers. We\ndiscuss certain philosophical, technical and practical aspects\nof MMSP and its resolution in Sec 4, and conclude in Sec 5\nwith some practical recommendations.\n2 Principles of Optimal Model Selection\nIn this section, we formally deﬁne the Multilingual Model\nSelection Problem(MMSP) and lay down fundamental guid-\ning principles from economics and ethics for optimal model\nselection. To apply these principles, languages need to be\nconsidered as individual entities instead of representing a\ngroup of users (see details in Sec 4).\nMultilingual Model Selection Problem (MMSP): Given\ntwo MultiLMs A and B along with m different tasks\nt1;t2;:::;t m and ndifferent languages L1;L2;:::;L n, let\naij and bij respectively denote the accuracy of modelAand\nBfor task ti in language Lj. This subsumes the casem= 1,\nwhere we are given the accuracy of two MultiLMs on differ-\nent languages but for the same task. For simplicity, we keep\naside the pros and cons of various modeling and training\nchoices, and focus only on their performance as black-box\nmodels. Then the problem of picking the better model be-\ntween Aand Bboils down to comparing their performance\nvectors a = ( a11;:::;a mn) and b = ( b11;:::;b mn) ac-\ncording to an objective that quantiﬁes efﬁciency and fair-\nness. Such objectives are well-studied in economics, ethics,\nsocial choice theory, decision theory, and are guided by cer-\ntain normative principles of efﬁciency and fairness (Yaari\n1981; Sen 1985; Young 1994). There is a recent, renewed\n12711\ninterest to revisit these principles in the context of fairness\nin machine learning (Binns 2018; Heidari et al. 2019; Leben\n2020; Hossain, Mladenovic, and Shah 2020).\n2.1 Utilitarian Multilingual Model Selection\nWhen we focus only on the performance, the choice space\nof models can be identiﬁed with their performance vec-\ntors, i.e., we identify a model A with its performance vec-\ntor a = ( a11;:::;a mn), whose coordinates are its accura-\ncies on different languages for different tasks. When build-\ning state of the art (SOTA) MultiLMs, the ideal is to create a\nmodel whose performance surpasses others in all languages\nand tasks. This is formalized by the well-known Pareto-\nefﬁciency principle stated below.\nDeﬁnition 1. (Pareto-efﬁciency and Pareto-optimality)\n1. The Pareto-efﬁciency principle suggests that a model A\nis better than a model B, if a Pareto-dominates b, i.e.,\naij \u0015bij, for all i;j.\n2. A model Ais called Pareto-optimal, if there is no other\nmodel that performs better than A in every coordinate,\ni.e., no other model Pareto-dominates it.\nNote that Pareto-optimality does not necessarily mean\nPareto-dominance over all other models but rather a guaran-\ntee that no other model would Pareto-dominate it. To under-\nstand Pareto-optimality, we need to understand the choice\nspace of all available models. A common assumption about\nthe choice space is called non-polarization, as deﬁned be-\nlow.\nDeﬁnition 2. (Non-polarization) The choice space satisﬁes\nnon-polarization, if the set of all performance vectors a, as\nAvaries over all available model choices, is a convex setC.\nNon-polarization is a reasonable assumption because we\ncan always do a thought experiment that picks the model A\nwith probability pand the model B with probability 1 \u0000p\nto achieve performance vector given by the convex combi-\nnation pa + (1 \u0000p)b. The following is a folklore result,\ncommonly manifested as von Neumann-Morgenstern util-\nity theorem in decision theory (von Neumann, Morgenstern,\nand Rubinstein 1944) and Harsanyi’s utilitarian theorem in\neconomics (Harsanyi 1955).\nProposition 1. Assuming non-polarization, a model A is\nPareto-optimal if and only if Amaximizes the weighted ac-\ncuracy Pm\ni=1\nPn\nj=1 wijaij, for some non-negative weights\nwij, over all models in the choice space.\nBy rescaling, we can assume that the weights give a con-\nvex combination, i.e., the weight vectorw lies in \u0001 = fw :Pm\ni=1\nPn\nj=1 wij = 1 and 0 \u0014wij \u00141;8i;jg.\nIt is important to note that Proposition 1 does not pre-\nscribe any speciﬁc weights. The natural choice being equal\nweights leads to the average accuracy objective. The utili-\ntarian approach of maximizing the weighted accuracy is jus-\ntiﬁed only when the weights are chosen appropriately after\ncalibrating the accuracies across languages and tasks against\neach other, based on considerations, such as, the size of the\ntraining and test data across languages, and difﬁculty level\nand usefulness of the tasks. Thus, for MultiLMs, the choice\nof weights can be argued further based on domain expertise\nand the economics of model deployment.\nHowever, the utilitarian promise of Pareto-optimality falls\nshort for several reasons. Firstly, the choice of weights that\nmakes a model Pareto-optimal may be debatable, leading to\ntwo or more SOTA models such that no single model Pareto-\ndominates the others. Secondly, if we think of the choice\nof weights as reﬂecting the balance of test data across lan-\nguages and tasks, or usefulness of a task, then this balance\nmay change over time after deployment, and so would our\nchoice for the best MultiLM. It is a priori unclear if there is\na choice of weights that is both utilitarian and robust to the\nabove considerations. In case of unknown weights, a natu-\nrally robust and conservative choice of weights is given by\nwrobust = argmin\nw2\u0001\nmax\na2C\nmX\ni=1\nnX\nj=1\nwijaij: (1)\nIn practice, \u0001 can be a carefully chosen set based on do-\nmain knowledge and robustness requirement. We shall re-\nvisit Equation (1) in a different light in the next subsection.\n2.2 Fair Multilingual Model Selection\nFairness across different languages has been an implicit\ngoal in MultiLMs, as better models must have high ac-\ncuracy on both high-resource and low-resource languages.\nA commonly studied notion of fairness in machine learn-\ning is egalitarian, where an ideal fair classiﬁer must have\npredictive parity (e.g., equal accuracy, equal false positive\nrates) across certain demographics (Barocas, Hardt, and\nNarayanan 2019). Similarly, equal accuracy across different\nlanguages is a desirable long-term goal for MultiLMs. How-\never, equal or near-equal accuracy is a constraint and not\nan objective in itself. In group-fairness literature, there is a\nlong line of work in training a fair model to maximize av-\nerage accuracy subject to egalitarian constraints across dif-\nferent groups; see (Celis et al. 2019; Zafar et al. 2019) and\nthe references therein. However, the training guarantees do\nnot easily carry over to the test data. Moreover, insisting on\nequal or near-equal accuracy across different languages may\nlead us to pick a sub-optimal model of diminished overall\naccuracy, especially, when the availability of training data\nis inherently unequal across languages and the difﬁculty of\neach task is inherently different.\nThe least difference principle proposed by Rawls (1999)\nin distributive justice offers another perspective at the egal-\nitarian approach, and proposes to narrow the gap between\nunequal accuracies instead of insisting on equalizing them.\nDeﬁnition 3. (Least difference principle)\nConsider two models A and B with performance vectors\na = (a11;:::;a mn) and b = (b11;:::;b mn), respectively.\nSuppose there exist two indices (i;j) and (k;l) such that\naij < bij < bkl < akl, and apq = bpq on all other in-\ndices (p;q) =2f( i;j);(k;l)g. Then the least different prin-\nciple prefers model B over A because it reduces the gap\nbetween two unequal coordinates while keeping the rest un-\nchanged.\nDecision theory and social choice theory literature con-\ntains additional axioms that a well-behaved choice process\n12712\nmust satisfy, e.g., the von Neumann-Morgenstern (VNM) ra-\ntionality axioms (von Neumann, Morgenstern, and Rubin-\nstein 1944) and the Independence of Irrelevant Alternatives\n(IIA) (Ray 1973). In our context, the most important proper-\nties for a choice process to be well-behaved are as follows.\n1. For any permutation \u001bof the coordinates, if we prefer a\nover b then we must prefer \u001b(a) over \u001b(b).\n2. For any monotone function f : [0 ;1] ! [0;1],\nif we prefer a over b then we must prefer ~a =\n(f(a11);:::;f (amn)) over ~b = (f(b11);:::;f (bmn)).\nKeeping the mathematical description aside, a high-level\ntakeaway from the independence of irrelevant alternatives\nfor MultiLMs is that if an ideal ordering or preference\namong models is well-behaved then any modiﬁcation to\ntraining or testing methodology that affects all the accura-\ncies in similar fashion should not affect the ideal ordering or\npreference among models.\nPrevious work in economics has shown that the above\nprinciples and axioms guide us to a well-deﬁned max-min\nobjective for ﬁnding an optimal model from a given set of\nfeasible models. Although, the max-min objective looks pri-\noritarian, where the beneﬁts of the worst-off matter more\nthan the beneﬁts of the better-off, the following proposition\nshows that it can be derived from utilitarian considerations\nof Pareto-efﬁciency and the Rawlsian fairness through least\ndifferent principle. It is a direct consequence of a theorem\ndue to Hammond (Hammond 1976) and Strasnick (Strasnick\n1976), which we restate from (Yaari 1981).\nProposition 2. Assuming non-polarization, Pareto-\nefﬁciency and least difference principles, any well-behaved\noptimal choice A\u0003must be a solution to maxa2Cmini;j aij.\nProposition 2 implies that any optimal MultiLM model\nthat satisﬁes Pareto-efﬁciency and Rawlsian fairness must\nmaximize the minimum accuracy. This is in contrast with the\negalitarian solutions that must equalize accuracies. More-\nover, we can rewrite maxa2Cmini;j aij as the minimum\nover convex combinations and use Sion’s minimax theorem\n(Sion 1958) and Equation (1) to get\nmax\na2C\nmin\nw2\u0001\nmX\ni=1\nnX\nj=1\nwijaij = min\nw2\u0001\nmax\na2C\nmX\ni=1\nnX\nj=1\nwijaij\n=\nmX\ni=1\nnX\nj=1\n(wrobust)ijaij:\nThus, it is interesting to note that the optimal choice for\nRawlsian fairness coincides with the optimal robust choice\nmade from a purely utilitarian viewpoint.\n2.3 Deontological Multilingual Model Selection\nThe approaches discussed above try to ﬁnd the right ob-\njective so that the outcome (namely, the optimal choice of\nMultiLM) would meet certain efﬁciency and fairness princi-\nples. In ethics, the outcome-oriented approaches are known\nas consequentialist. On the other hand, deontological ap-\nproaches divide the beneﬁts proportional to their rights,\nrewarding intended beneﬁciaries or compensating the vic-\ntims. For example, an egalitarian view of dividing the rights\nequally leads to weighted accuracy with equal weights. De-\nontological approach has also seen renewed interest in fair\nmachine learning (Leben 2020; Saleiro et al. 2018).\nIn deontological approach to fairness, the philosophical\nconcepts of moral luck and desert play a key role. Con-\nsidering the resources from different languages that went\ninto training a MultiLM, the beneﬁts must be divided in\nsome way. What we consider as resources are often split\nfurther into luck and work parts. In the context of languages\nand MultiLMs, an example of luck is typological similari-\nties (Bender 2011; Joshi et al. 2020), e.g., a model trained\non English would do much better on Dutch than Japanese\nbecause of typological similarities (Lin et al. 2019). One in-\nterpretation of the work part can be the amount of collective\neffort or money spent by the NLP community to create la-\nbeled data for different languages. If we take an approach\nthat the inequality between high-resource and low-resource\nlanguages is pure luck, then a libertarian objective would be\na weighted accuracy, where the weights are proportional to\nthe resources that went into training a MultiLM. However, if\nwe follow the approach of moral desert, an equally weighted\naccuracy or considering only the gains above a common\nbaseline, would be well-justiﬁed. If we make a ﬁner dis-\ntinction between luck and work parts, we could say that a\nMultiLM is deontologically fair as long as it does not pe-\nnalize any language due to typological difference but it is\nokay to have unequal accuracy across languages based on\nthe availability of training data.\n3 Reinterpreting the Choices Made in\nMultiLM Literature\nIn this section, we will review the current trends in resolu-\ntion of MMSP, and reinterpret those decisions in light of the\nfairness and efﬁciency principles discussed in the previous\nsection. Our aim here is not to conduct a thorough technical\nsurvey of MultiLMs; neither it is to quantify the statistics of\nnormative principles that are followed. Rather, we shall dis-\ncuss a few representative work to illustrate two broad trends\nthat we observe in the literature - Pareto-efﬁciency based\nresolution, and the average performance across languages,\nboth of which are based on the utilitarian principle. Then we\ndiscuss how some of these resolutions would have changed\nif we were to follow the Rawlsian fairness principles. We\nalso present some of the recent critiques of MMSP, which,\nas we shall argue, are mostly deontological in nature.\nA Note on Tasks and Datasets: MultiLMs are typically\nevaluated on test-benches such as XNLI (Conneau et al.\n2018), XGLUE (Liang et al. 2020) [a cross-lingual evalu-\nation benchmark consisting of NER (Sang 2002), POS (Ze-\nman et al. 2019), News Classiﬁcation (NC), MLQA (Lewis\net al. 2019), XNLI, PAWS-X (Yang et al. 2019), Query-\nAd Matching (QADSM), Web Page Ranking (WPR), QA\nMatching (QAM), Question Generation (QG) and News Ti-\ntle Generation (NTG) tasks], XTREME (Hu et al. 2020)\n[also a collection of cross-lingual test benches consist-\ning of XNLI, PAWS-X, POS, NER (Pan et al. 2017),\n12713\nXQUAD (Artetxe, Ruder, and Yogatama 2020), MLQA, Ty-\nDiQA (Clark et al. 2020), BUCC (Zweigenbaum, Sharoff,\nand Rapp 2018) and Tatoeba (Artetxe and Schwenk 2019)],\nGLUECoS (Khanuja et al. 2020), and LinCE (Aguilar, Kar,\nand Solorio 2020). While comparing MultiLMs, researchers\nreport and compare the performance of the models on the\ndatasets, rather than the tasks. For instance, MLQA and\nXQuAD involve the same NLP task, namely Question An-\nswering through span extraction from Wikipedia, but the\naccuracies on these are reported independently. Therefore,\nhere we shall use the term task to refer to the dataset, rather\nthan the task itself.\n3.1 Pareto-Efﬁciency Principle\nArguably, the aim of most model building exercises is to\ncome up with MultiLM architectures, training processes or\nsimply larger models that are Pareto-efﬁcient with respect\nto tasks and languages that it has been tested on, in com-\nparison to the existing SOTA MultiLMs. Whenever possi-\nble, this indeed would be the ideal resolution of MMSP\nunder any of the consequentialist approaches - Utilitarian\nas well as Prioritarian (or Rawlsian). Some of the popu-\nlar MultiLMs had achieved Pareto-efﬁciency over the ex-\nisting SOTA, often by a considerable margin for all lan-\nguages. For example, XLM (Conneau and Lample 2019)\nPareto-dominates mBERT (Devlin et al. 2019) as well as\n(Artetxe and Schwenk 2019) for XNLI; Unicoder (Huang\net al. 2019) Pareto-dominates XLM, and XLM-R (Conneau\net al. 2020) Pareto-dominates Unicoder, again for XNLI.\nOn the GLUECoS dataset, Khanuja et al. (2020) show that\ntheir proposed method is Pareto-efﬁcient across all tasks for\nEnglish-Spanish code-mixing, though it is not the case for\nEnglish-Hindi, where mBERT and the proposed method are\nPareto-optimal over the tasks.\n3.2 Utilitarian Principle\nWhenever Pareto-efﬁciency for a model is not observed,\nusually the average performance over all languages is used\nas a metric to determine the “better” model. An illustrative\ncase-study is that of Liang et al. (2020), where along with a\nnew test-bench, XGLUE, authors also propose a new Mul-\ntiLM - the Unicoder. Unicoder is compared to mBERT and\nXLM-R on 11 tasks across 19 languages (Table 4 in the\npaper), where average across languages, and then average\nacross tasks are used as the indicators to declare that Uni-\ncoder is the best performing model. This is clearly an appli-\ncation of the utilitarian principle.\nIn Table 1, we summarize the performance of the models\nacross tasks (except for the two text generation tasks, QG\nand NTG, for which a different metric of performance is\nused) by indicating the average and minimum values, as well\nas the Pareto-optimality of the models. As we can see, Uni-\ncoder has a higher average for all but two tasks - POS and\nQADSM. Nevertheless, it is Pareto-efﬁcient only for three\ntasks - XNLI, PAWS-X and QAM.\nAlso interesting are the experiments on pivots for XNLI,\nwhere the authors show that the performance of the model\nacross languages are substantially inﬂuenced by the choice\nof the pivot language (Table 5 in the paper). Here too, since\nTasks mBERT XLM-R Unicoder\nNER 78.2, 69.2 79.0, 70.4 79.7, 71.8\nPOS 74.7, 43.3 79.8, 55.2 79.6, 56.3\nNC 82.7, 78.0 83.4, 78.2 83.5, 78.5,\nMLQA 60.7, 47.9 65.1, 60.5 66.0, 62.1,\nXNLI 66.3, 50.4 74.2, 64.7 75.3, 66.3,\nPAWS-X 87.2, 82.9 89.5, 86.9 90.1, 87.4,\nQADSM 64.2, 60.3 68.6, 65.8 68.4, 64.6\nWPR 73.5, 64.5 73.8, 63.9 73.9, 64.4,\nQAM 66.1, 64.7 68.4, 67.8 68.9, 68.4,\nTable 1: Performance of mBERT, XLM-R and Unicoder\ntaken from (Liang et al. 2020). The ﬁrst and second numbers\nin each cell denote the average and minimum performances\nrespectively. The numbers in bold indicate that the model is\nPareto-optimal on that task; if there is only one bold cell in a\nrow, it indicates that the model is Pareto-efﬁcient. Gray cells\ndenote the Rawlsian choice for the task.\nno pivot language provides a Pareto-efﬁcient model, the au-\nthors resolve the MMSP by looking at the average perfor-\nmance across languages, concluding that “the best pivot lan-\nguages are Spanish (es), Greek (el) and Turkish (tr), rather\nthan English (en).” Authors do not offer any principled ap-\nproach for breaking this tie.\nSome other studies that use average across languages for\nreporting and/or model selection purposes include (Aharoni,\nJohnson, and Firat 2019) for machine translation, (Ahmad\net al. 2019) for parsing, and (Rijhwani et al. 2019) for en-\ntity linking. We have not come across any work that applies\nweighted average, presumably because weights are difﬁcult\nto determine and justify. Also, it is uncommon to average\nover tasks; usually, the performance range across tasks, and\nthe nature and complexity of the tasks are too diverse for\naverage to be a meaningful quantity.\n3.3 Hypothetical Resolutions under Rawlsian\nFairness\nWhat if instead of Pareto-efﬁciency or the utilitarian notion\nof averaging, we were to resolve the MMSP through Rawl-\nsian fairness? Recall that Rawlsian fairness recommends the\nselection of the model that maximizes the minimum perfor-\nmance over the languages. While this looks like a prioritian\nobjective, it is actually also the robust utilitarian choice un-\nder the very realistic assumption that the language-resources\nand utility of tasks change over time.\nLet us focus again on Table 1, where the cells highlighted\nin gray show the Rawlsian choice of the model for a task; or\nin other words, the cells that have the maximum value for the\nminimum (the second number in the tuples) in a row. Uni-\ncoder emerges as the Rawlsian choice for 7 out of the 9 tasks\n(actually 9 of the 11 tasks), except for QADSM and WPR.\nFurther, if we were to look at the max-min across all tasks\nand languages to make our ﬁnal choice, it would still be the\nUnicoder. Thus, instead of averaging across languages and\ntasks, Unicoder’s superiority over the other models could\nas well be established through this Rawlsian resolution of\nMMSP.\n12714\nTasks mBERT XLM XLM-R MMTE\nXNLI 65.4, 49.7 69.1, 58.7 79.2, 71.2 67.5, 61.9\nPAWS-X 81.9, 69.6 80.9, 64.8 86.4, 79.0 81.3, 69.2\nPOS 70.3, 41.7 70.1, 20.5 72.6, 15.9 72.3, 43.1\nNER 62.2, 3.6 61.2, 0.3 65.4, 1.3 58.3, 3.9\nXQuAD 64.5, 42.7 59.8, 35.4 76.6, 59.3 64.4, 48.4\nMLQA 61.4, 50.2 48.5, 34.4 71.6, 62.1 60.3, 46.2\nTyDiQA 59.7, 49.3 43.6, 14.2 65.1, 31.9 58.1, 49.9\nBUCC 56.7, 50.0 56.8, 46.6 66.0, 56.7 59.8, 53.3\nTatoeba 38.7, 11.5 32.6, 12.4, 57.3, 14.1 37.9, -\nTable 2: A summary of results on the XTREME benchmark\nas reported in (Hu et al. 2020). We follow the same conven-\ntion as for Table 1.\nInterestingly, under the Rawlsian fairness assumption, the\nutilitarian tie among the three pivot languages - Greek, Turk-\nish and Spanish (Table 5, Liang et al. 2020) breaks in favor\nof Turkish. More importantly and surprisingly, Swahili, for\nwhich the accuracies are the lowest for most pivots, emerges\nas the overall Rawlsian choice for the pivot language.\nThe XTREME test-bench (Hu et al. 2020) is yet another\ninteresting case-study. Table 2 presents a summary of the\nresults from the paper on the XTREME tasks. XLM-R per-\nforms better than the other three models - XLM, mBERT and\nMMTE in terms of the utilitarian objective of average. It is\nalso Pareto-optimal for all tasks and Pareto-efﬁcient for 5\nout of 9 tasks. However, by the Rawlsian criterion (the cells\nhighlighted in gray in the Table) one would choose MMTE\nfor 3 tasks, namely POS, NER and TyDiQA. The authors,\non the other hand, summarize their observations as: “XLMR\nis the best-performing zero-shot transfer model and gener-\nally improves upon mBERT signiﬁcantly... MMTE achieves\nperformance competitive with mBERT on most tasks, with\nstronger results on XNLI, POS, and BUCC.”\n3.4 Deontological Critiques of MultiLMs\nCritiques of the current approaches to MultiLMs have fol-\nlowed three main lines of arguments. The ﬁrst line of ar-\ngument is concerned with their limited evaluation on a few\ntasks and languages, even though the explicit or implicit\nclaim has been that zero-shot transfer works for all lan-\nguages that the MultiLMs have been pre-trained on (Hu et al.\n2020; Wu and Dredze 2020). This, the critiques argue, is\nmisleading because for most languages the models’ perfor-\nmance is unknown. The second line of criticisms challenges\nthe fundamental assumption that MultiLMs provide a so-\nlution to low-resource languages by questioning the base-\nlines they are compared against. For instance, according to\nWu and Dredze (2020), “the 30% languages with least pre-\ntraining resources perform worse than using no pretrained\nlanguage model at all. Therefore, we caution against us-\ning mBERT alone for low resource languages ... On the\nother end of the spectrum, the highest resource languages\n(top 10%) are hurt by massively multilingual joint training.”\nSimilarly, Joshi et al. (2020) argue that MultiLMs do not\nhelp 88% of the world’s languages for which even unlabeled\ndata is unavailable; the only classes of languages it helps, if\nat all, are those which have a large amount of unlabeled data\nbut not sufﬁcient labeled data, namely the “Rising Stars” and\n“The Hopefuls” which together have 47 languages.\nThe third set of critiques challenge the zero-shot as-\nsumption for model building and evaluation of Multi-\nLMs (Lauscher et al. 2020; Artetxe et al. 2020). In prac-\ntice, they argue, one can always build some labeled re-\nsources for a language if one intends to improve the per-\nformance of the system in that language. In general, sev-\neral studies have shown that building shared MultiLMs with\na smaller set of related languages (Lin et al. 2019; Ahmad\net al. 2019), and ﬁne-tuning in a few-shot rather than zero-\nshot mode (Lauscher et al. 2020) is more effective than mas-\nsively multilingual universal models. Conneau et al. (2020)\nshow that for a given model size, the average zero-shot per-\nformance increases only up to addition of certain number\nof languages during the pre-training phase, after which it\nstarts declining; this would make truly universal MultiLMs\nimpractically large. They refer to this phenomenon as “the\ncurse of multilinguality”.\nWe observe that these critiques are based on certain de-\nontological principles of fairness, where all languages are\nassumed to have an a priori right to technology irrespective\nof the number of speakers or the availability of resources.\nThe ﬁrst line of arguments are more egalitarian in nature,\nwhere the ideal objective is to have equal accuracies across\nall languages. For instance, Hu et al. (2020) suggests a met-\nric called the “transfer gap” which measures the average\ndifference between the performance for the pivot and other\nlanguages. An ideal MultiLM should minimize the transfer\ngap, and thus, fulﬁll the egalitarian objective of equal accu-\nracy across all languages. The second line of argument is\nmore libertarian in nature, where the implicit objective is\nto do “better” for all languages with respect to the SOTA;\nhowever, the the gap in performances across languages is\nacceptable as long as all languages are beneﬁting in some\nway. These arguments are agnostic to the reasons that might\nhave caused the gap at the ﬁrst place, and appear to suggest\nthat one should use the most appropriate technology, given\nthe amount of resources available. This is precisely where\nthe third line of criticisms tend to disagree. These arguments\nsuggest that instead of depending on MultiLMs, or the cur-\nrent state of technology and resources, one should try to take\nthe necessary steps, that is to say, do the necessary work, to\nattain a more egalitarian state. Thus, these arguments con-\nform to the “desert principle” of fairness, where only those\nperformance gaps between languages are acceptable that are\nexplainable in terms of the difference in the amount of work\nput in for the language, say, in building resources. Perfor-\nmance differences due to inherent features of a language,\nsuch as typological factors, are akin to luck and cannot be\nallowed (for instance, by choosing a typologically dissimilar\nlanguage as a pivot). Hence, the proponents recommend cre-\nation of different MultiLMs for language groups (Lin et al.\n2019) and creation of labeled data for ﬁne-tuning (Lauscher\net al. 2020), whenever possible.\nIt is important to note that the recommendations or cri-\ntiques in a paper often span multiple ideas that implicitly\nconform to different normative principles of efﬁciency and\nfairness. Even when these normative principles are explicitly\n12715\nstated, their application in practice is not always straightfor-\nward (Binns 2018; Leben 2020; Young 1994).\n4 Discussion\nLanguage as an Entity: The notion of distributive justice\nin ethics and economics starts with the assumption that the\nrecipient of the resource or utility is an individual or a group\nof individuals. The discourse on ethics in machine learn-\ning has also followed this idea of group and individual fair-\nness (Binns 2018). In contrast, the present discussion on\nMMSP considers a language as the recipient of distribu-\ntive justice. Since a language can be equated to a group of\nindividuals, it might be argued that the principles of group\nfairness are applicable here as well. However, there are im-\nportant philosophical and practical distinctions between the\nnotions of “language as an entity” and “language as a group\nof individuals”, which we believe are crucial to appropri-\nately contextualize the present analysis. One might choose\nto resolve the MMSP by imagining language as a group of\nindividuals, let us say, the users of the technologies in that\nlanguage, which would be a subset of the speakers of the lan-\nguage. Under this assumption, the consequentialist should\nnot look at the accuracy of the MultiLMs on test-benches.\nRather, the true utility of a MultiLM, and more broadly, an\nNLP system can be measured only in terms of its usefulness\nfor the end-users (Soria, Quochi, and Russo 2018). For cer-\ntain languages, a predictive keyboard or speech recognition\nsystem might be of much greater value than, say, an NLI\nsystem or POS tagger. Therefore, even though “language as\na group of end-users” is a practically important and useful\nconstruct, it cannot be operationalized in this context be-\ncause of the way MultiLMs are evaluated currently.\nOn the other hand, “language as an entity” would mean\nthat each language has a distinct identity, deﬁned by its vo-\ncabulary, syntactic structure, its typological features, amount\nof available resources, and so on. Under this notion, tasks\nsuch as parsing, POS or NLI, and limited evaluation on test-\nbenches make perfect sense. A MultiLM or any similar mul-\ntilingual system should then be evaluated similarly across\nlanguages and its utility could be quantiﬁed by the perfor-\nmance on a ﬁxed set of linguistic tasks. It is this notion of\n“language as an entity” which drives the research and dis-\ncourse of MultiLMs and more broadly, multilingual NLP,\nincluding this study on MMSP.\nFairness in NLP: In NLP, the fairness discourse has been\naround representational biases, such as those in the word-\nembeddings, and performance bias against users and user\ngroups such as gender, age or regional varieties of a lan-\nguage; see (Blodgett et al. 2020) and references therein. Be-\nsides a few notable exceptions (Zhao et al. 2020; Sweeney\nand Najaﬁan 2019), in all the cases the object of study is\na representation or system for a single language. Most re-\nlated to the present work are the discussions around whether\nalgorithms or methods are really language agnostic, as is of-\nten claimed (Bender 2011), and the issue of determining the\nbest transfer language for a given language in a crosslingual\ntransfer setting (Lin et al. 2019; Ahmad et al. 2019); see\nRuder (2020) for an excellent compilation of reasons and\ncritiques of English-centric NLP. While these studies raise\nmany important ethical concerns, mostly deontological in\nnature, no proposals have been put forward for formal reso-\nlution of these dilemmas under different assumptions of nor-\nmative principles.\nOn one hand, these debates are similar to the MMSP, as\nthey treat language as an entity and as the recipient of the\ndistributive justice. On the other hand, there is an important\ntechnical difference that unlike the case of MultiLMs, here\nthe model parameters are not shared between languages, and\nare optimized for one or a pair of languages at a time. There-\nfore, instead of resolving MMSP, one could always choose\ndifferent algorithms or different pivot/transfer languages to\noptimize the performance of the system for a particular lan-\nguage. However, we would like to reemphasize that MMSP\napplies even when the language models do not share any pa-\nrameters, as long as we can frame it as a black-box model\nselection problem, where multiple agencies (could be ap-\nproaches, algorithms, or even companies) offer different sets\nof models serving multiple languages, and one needs to\nchoose between the agencies.\n5 Practical Recommendations\nStating the Fairness and Efﬁciency Assumptions: We recom-\nmend that all empirical studies involving MultiLM (and/or\ncases where common algorithms or approaches are applied\nto multiple languages) should clearly articulate the fairness\nand efﬁciency principles they are following, and the assump-\ntions they are making while resolving the MMSP, drawing\nconclusions and/or providing usage recommendations.\nUtilitarian Resolution to MMSP: In case the researcher\nhas a strong reason to follow the utilitarian principle, they\ncan resolve the MMSP in favor of the model that Pareto\ndominates all other models. When no model Pareto domi-\nnates, the model with the highest (weighted) average perfor-\nmance can be chosen. If there is a tie (deﬁned as exact or\nsufﬁciently close average performances), they could invoke\nthe prioritarian principle to resolve the tie in favor of the\nmodel that maximizes the minimum accuracy.\nPrioritarian Resolution to MMSP: In case the researcher\nwishes to resolve the MMSP following the prioritarian or\nRawlsian principle, they can use the max-min, or more gen-\nerally, thelex-min objective, i.e., maximize the minimum ac-\ncuracy, and if two models that maximize the minimum ac-\ncuracy have the same minimum accuracy, then maximize the\nsecond minimum accuracy, and so on.\nConsidering Outliers: Under all circumstances, the prin-\nciples should be applied only after critically analyzing the\noutlier languages that have remarkably high or low perfor-\nmances. These cases should be included or excluded be-\nfore applying the fairness principles – a decision that can be\nmade based on practical considerations such as how reliable\nis the test-set of a particular language.\nAlternative Leader-boards: Current leader-boards for\nMultiLMs, e.g. https://sites.research.google/xtreme, rank\nthe models based on the average performance. We recom-\nmend alternative leader-boards or rankings based on various\nfairness principles, including but not limited to the prioritar-\nian or Rawlsian principle.\n12716\nAcknowledgements\nWe would like to thank Mr. Anirudh Srinivasan for pointers\nto the recent literature on MultiLMs and their critiques.\nEthics Statement\nThis work raises some important ethical questions regard-\ning the selection strategies for massively multilingual pre-\ntrained models. We believe that this work will help create\nawareness on linguistic fairness, diversity and inclusion of\nall and especially low resource languages. We do not fore-\nsee any adverse ethical implications of the current study.\nReferences\nAguilar, G.; Kar, S.; and Solorio, T. 2020. LinCE: A Cen-\ntralized Benchmark for Linguistic Code-switching Evalua-\ntion. In Proceedings of The 12th Language Resources and\nEvaluation Conference, 1803–1813.\nAharoni, R.; Johnson, M.; and Firat, O. 2019. Massively\nMultilingual Neural Machine Translation. In Proceedings\nof the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Papers) ,\n3874–3884.\nAhmad, W. U.; Zhang, Z.; Ma, X.; Hovy, E. H.; Chang, K.-\nW.; and Peng, N. 2019. On Difﬁculties of Cross-Lingual\nTransfer with Order Differences: A Case Study on Depen-\ndency Parsing. In NAACL-HLT (1).\nArivazhagan, N.; Bapna, A.; Firat, O.; Lepikhin, D.; John-\nson, M.; Krikun, M.; Chen, M. X.; Cao, Y .; Foster, G.;\nCherry, C.; et al. 2019. Massively multilingual neural ma-\nchine translation in the wild: Findings and challenges.arXiv\npreprint arXiv:1907.05019 .\nArtetxe, M.; Ruder, S.; and Yogatama, D. 2020. On the\nCrosslingual Transferability of Monolingual Representa-\ntions. In Proceedings of ACL, 1946–1958.\nArtetxe, M.; Ruder, S.; Yogatama, D.; Labaka, G.; and\nAgirre, E. 2020. A Call for More Rigor in Unsupervised\nCross-lingual Learning. arXiv preprint arXiv:2004.14958 .\nArtetxe, M.; and Schwenk, H. 2019. Massively Multilingual\nSentence Embeddings for Zero-Shot Cross-Lingual Transfer\nand Beyond. Transactions of the Association for Computa-\ntional Linguistics 7(0).\nBarocas, S.; Hardt, M.; and Narayanan, A. 2017. Fairness in\nmachine learning. NIPS Tutorial 1.\nBarocas, S.; Hardt, M.; and Narayanan, A. 2019. Fair-\nness and Machine Learning. fairmlbook.org. http://www.\nfairmlbook.org.\nBender, E. M. 2011. On achieving and evaluating language-\nindependence in NLP. Linguistic Issues in Language Tech-\nnology 6(3): 1–26.\nBinns, R. 2018. Fairness in Machine Learning: Lessons\nfrom Political Philosophy. Proceedings of Machine Learn-\ning Research, 149–159. PMLR.\nBlodgett, S. L.; Barocas, S.; Daum´e III, H.; and Wallach, H.\n2020. Language (Technology) is Power: A Critical Survey\nof” Bias” in NLP. Proc of ACL .\nCelis, L. E.; Huang, L.; Keswani, V .; and Vishnoi, N. K.\n2019. Classiﬁcation with Fairness Constraints: A Meta-\nAlgorithm with Provable Guarantees. In Proceedings of\nFAT* 19, 319–328.\nClark, J. H.; Choi, E.; Collins, M.; Garrette, D.;\nKwiatkowski, T.; Nikolaev, V .; and Palomaki, J. 2020. TyDi\nQA: A Benchmark for Information-Seeking Question An-\nswering in Typologically Diverse Languages. Transactions\nof the Association of Computational Linguistic .\nConneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V .;\nWenzek, G.; Guzm ´an, F.; Grave, E.; Ott, M.; Zettlemoyer,\nL.; and Stoyanov, V . 2020. Unsupervised cross-lingual rep-\nresentation learning at scale. In Proceedings of ACL.\nConneau, A.; and Lample, G. 2019. Cross-lingual language\nmodel pretraining. In Advances in Neural Information Pro-\ncessing Systems, 7059–7069.\nConneau, A.; Rinott, R.; Lample, G.; Williams, A.; Bow-\nman, S.; Schwenk, H.; and Stoyanov, V . 2018. XNLI: Eval-\nuating Cross-lingual Sentence Representations. In Proceed-\nings of the 2018 Conference on Empirical Methods in Natu-\nral Language Processing, 2475–2485.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL-HLT (1).\nHammond, P. J. 1976. Equity, Arrow’s Conditions, and\nRawls’ Difference Principle.Econometrica 44(4): 793–804.\nHarsanyi, J. C. 1955. Cardinal Welfare, Individualistic\nEthics, and Interpersonal Comparisons of Utility. Journal\nof Political Economy 63(4): 309–321.\nHeidari, H.; Loi, M.; Gummadi, K. P.; and Krause, A. 2019.\nA Moral Framework for Understanding Fair ML through\nEconomic Models of Equality of Opportunity. In Proceed-\nings of FAT’19, 181–190.\nHossain, S.; Mladenovic, A.; and Shah, N. 2020. Design-\ning Fairly Fair Classiﬁers Via Economic Fairness Notions.\nIn Proceedings of The Web Conference 2020, WWW ’20,\n1559–1569.\nHu, J.; Ruder, S.; Siddhant, A.; Neubig, G.; Firat, O.; and\nJohnson, M. 2020. Xtreme: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual generalization.\narXiv preprint arXiv:2003.11080 .\nHuang, H.; Liang, Y .; Duan, N.; Gong, M.; Shou, L.; Jiang,\nD.; and Zhou, M. 2019. Unicoder: A Universal Language\nEncoder by Pre-training with Multiple Cross-lingual Tasks.\nIn Proceedings of EMNLP-IJCNLP.\nJoshi, P.; Santy, S.; Budhiraja, A.; Bali, K.; and Choudhury,\nM. 2020. The state and fate of linguistic diversity and inclu-\nsion in the NLP world. In Proceedings of the ACL.\nKhanuja, S.; Dandapat, S.; Srinivasan, A.; Sitaram, S.;\nand Choudhury, M. 2020. GLUECoS: An Evaluation\nBenchmark for Code-Switched NLP. arXiv preprint\narXiv:2004.12376 .\n12717\nLauscher, A.; Ravishankar, V .; Vuli ´c, I.; and Glava ˇs, G.\n2020. From Zero to Hero: On the Limitations of Zero-\nShot Cross-Lingual Transfer with Multilingual Transform-\ners. arXiv preprint arXiv:2005.00633 .\nLeben, D. 2020. Normative Principles for Evaluating Fair-\nness in Machine Learning. In Proceedings of the AAAI/ACM\nConference on AI, Ethics, and Society, AIES ’20, 86–92.\nLewis, P.; Oguz, B.; Rinott, R.; Riedel, S.; and Schwenk, H.\n2019. MLQA: Evaluating Cross-lingual Extractive Question\nAnswering. ArXiv abs/1910.07475.\nLiang, Y .; Duan, N.; Gong, Y .; Wu, N.; Guo, F.; Qi, W.; et al.\n2020. XGLUE: A new benchmark dataset for cross-lingual\npre-training, understanding and generation. arXiv preprint\narXiv:2004.01401 .\nLin, Y .-H.; Chen, C.-Y .; Lee, J.; Li, Z.; Zhang, Y .; Xia, M.;\nRijhwani, S.; He, J.; Zhang, Z.; Ma, X.; et al. 2019. Choos-\ning transfer languages for cross-lingual learning. arXiv\npreprint arXiv:1905.12688 .\nMehrabi, N.; Morstatter, F.; Saxena, N.; Lerman, K.; and\nGalstyan, A. 2019. A survey on bias and fairness in machine\nlearning. arXiv preprint arXiv:1908.09635 .\nPan, X.; Zhang, B.; May, J.; Nothman, J.; Knight, K.; and\nJi, H. 2017. Cross-lingual name tagging and linking for 282\nlanguages. In Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers), 1946–1958.\nPires, T.; Schlinger, E.; and Garrette, D. 2019. How Mul-\ntilingual is Multilingual BERT? In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Lin-\nguistics, 4996–5001.\nRawls, J. 1999. A Theory of Justice. Harvard University\nPress. ISBN 9780674000773. URL http://www.jstor.org/\nstable/j.ctvkjb25m.\nRay, P. 1973. Independence of Irrelevant Alternatives.\nEconometrica 41(5): 987–991. ISSN 00129682, 14680262.\nURL http://www.jstor.org/stable/1913820.\nRijhwani, S.; Xie, J.; Neubig, G.; and Carbonell, J. 2019.\nZero-shot neural transfer for cross-lingual entity linking. In\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 33, 6924–6931.\nRuder, S. 2020. Why should we do NLP beyond English?\nURL https://ruder.io/nlp-beyond-english/. Last accessed:\n02/16/2021.\nSaleiro, P.; Kuester, B.; Stevens, A.; Anisfeld, A.; Hinkson,\nL.; London, J.; and Ghani, R. 2018. Aequitas: A Bias and\nFairness Audit Toolkit. arXiv preprint arXiv:1811.05577 .\nSang, E. F. T. K. 2002. Introduction to the CoNLL-2002\nShared Task: Language-Independent Named Entity Recog-\nnition. ArXiv cs.CL/0209010.\nSen, A. 1985. Social Choice and Justice: A Review Article.\nJournal of Economic Literature 23(4): 1764–1776. ISSN\n00220515. URL http://www.jstor.org/stable/2725708.\nSion, M. 1958. On general minimax theorems. Paciﬁc Jour-\nnal of Mathematics 8(1): 171–176.\nSoria, C.; Quochi, V .; and Russo, I. 2018. The DLDP Survey\non Digital Use and Usability of EU Regional and Minority\nLanguages. In Proceedings of LREC 2018.\nStrasnick, S. 1976. Social Choice and the Derivation of\nRawls’s Difference Principle. The Journal of Philosophy\n73(4): 85–99. ISSN 0022362X. URL http://www.jstor.org/\nstable/2025509.\nSweeney, C.; and Najaﬁan, M. 2019. A transparent frame-\nwork for evaluating unintended demographic bias in word\nembeddings. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, 1662–1667.\nvon Neumann, J.; Morgenstern, O.; and Rubinstein, A. 1944.\nTheory of Games and Economic Behavior (60th Anniver-\nsary Commemorative Edition). Princeton University Press.\nISBN 9780691130613. URL http://www.jstor.org/stable/j.\nctt1r2gkx.\nWu, S.; and Dredze, M. 2020. Are All Languages\nCreated Equal in Multilingual BERT? arXiv preprint\narXiv:2005.09093 .\nYaari, M. E. 1981. Rawls, Edgeworth, Shapley, Nash: The-\nories of Distributive Justice Re-examined. Journal of Eco-\nnomic Theory 24: 1–39.\nYang, Y .; Zhang, Y .; Tar, C.; and Baldridge, J. 2019. PAWS-\nX: A Cross-lingual Adversarial Dataset for Paraphrase Iden-\ntiﬁcation. ArXiv abs/1908.11828.\nYoung, H. P. 1994. Equity: In Theory and Practice. Prince-\nton University Press. ISBN 9780691043197. URL http:\n//www.jstor.org/stable/j.ctv10crfx7.\nZafar, M. B.; Valera, I.; Gomez-Rodriguez, M.; and Gum-\nmadi, K. P. 2019. Fairness Constraints: A Flexible Ap-\nproach for Fair Classiﬁcation. Journal of Machine Learning\nResearch 20(75): 1–42. URL http://jmlr.org/papers/v20/18-\n262.html.\nZeman, D.; Nivre, J.; Abrams, M.; Aepli, N.; and al et.\n2019. Universal Dependencies 2.5. URL http://hdl.handle.\nnet/11234/1-3105. LINDAT/CLARIAH-CZ digital library\nat the Institute of Formal and Applied Linguistics ( ´UFAL),\nFaculty of Mathematics and Physics, Charles University.\nZhao, J.; Mukherjee, S.; Hosseini, S.; Chang, K.-W.; and\nAwadallah, A. H. 2020. Gender Bias in Multilingual\nEmbeddings and Cross-Lingual Transfer. arXiv preprint\narXiv:2005.00699 .\nZweigenbaum, P.; Sharoff, S.; and Rapp, R. 2018. Overview\nof the Third BUCC shared task: Spotting parallel sentences\nin comparable corpora. In Proceedings of 11th Workshop on\nBuilding and Using Comparable Corpora, 39–42.\n12718",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7769762873649597
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5286515355110168
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5086520314216614
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5032500624656677
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4715580642223358
    },
    {
      "name": "Language model",
      "score": 0.4412422180175781
    },
    {
      "name": "Natural language processing",
      "score": 0.4341316521167755
    },
    {
      "name": "Machine learning",
      "score": 0.3285641670227051
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210124949",
      "name": "Microsoft Research (India)",
      "country": "IN"
    }
  ]
}