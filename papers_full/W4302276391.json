{
  "title": "CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI",
  "url": "https://openalex.org/W4302276391",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222773856",
      "name": "Hung, Alex Ling Yu",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2351735574",
      "name": "Zheng Hao-xin",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2039513909",
      "name": "Miao Qi",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A4222773859",
      "name": "Raman, Steven S.",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A4222248916",
      "name": "Terzopoulos, Demetri",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A4222773861",
      "name": "Sung, Kyunghyun",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A4222773856",
      "name": "Hung, Alex Ling Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2351735574",
      "name": "Zheng Hao-xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2039513909",
      "name": "Miao Qi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222773859",
      "name": "Raman, Steven S.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222248916",
      "name": "Terzopoulos, Demetri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222773861",
      "name": "Sung, Kyunghyun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2937483840",
    "https://openalex.org/W2801556653",
    "https://openalex.org/W2922071185",
    "https://openalex.org/W2978144600",
    "https://openalex.org/W6715899890",
    "https://openalex.org/W3159396227",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3071727253",
    "https://openalex.org/W2939917862",
    "https://openalex.org/W3081623163",
    "https://openalex.org/W2988053426",
    "https://openalex.org/W3093860814",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W2741891296",
    "https://openalex.org/W2958620810",
    "https://openalex.org/W6754551840",
    "https://openalex.org/W2963395421",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W2888539709",
    "https://openalex.org/W3047502985",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2774320778",
    "https://openalex.org/W3007268491",
    "https://openalex.org/W2923997689",
    "https://openalex.org/W2964334073",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W6750469568",
    "https://openalex.org/W6748666111",
    "https://openalex.org/W6638667902",
    "https://openalex.org/W6724804524",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3127316871",
    "https://openalex.org/W2921406441",
    "https://openalex.org/W2799597343",
    "https://openalex.org/W2798665804",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W3131993672",
    "https://openalex.org/W6717372056",
    "https://openalex.org/W2762439315",
    "https://openalex.org/W2107030642",
    "https://openalex.org/W2750855567",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W6750558302",
    "https://openalex.org/W2997876081",
    "https://openalex.org/W3094774173",
    "https://openalex.org/W2913406457",
    "https://openalex.org/W2604790786",
    "https://openalex.org/W2956326158",
    "https://openalex.org/W2914806156",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3204614423",
    "https://openalex.org/W3204255739",
    "https://openalex.org/W3200379731",
    "https://openalex.org/W3161200432",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W6640174519",
    "https://openalex.org/W4229494842",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2386192529",
    "https://openalex.org/W2979340752",
    "https://openalex.org/W2017735759",
    "https://openalex.org/W3101874879",
    "https://openalex.org/W4210632743",
    "https://openalex.org/W3103010481"
  ],
  "abstract": "Prostate cancer is the second leading cause of cancer death among men in the United States. The diagnosis of prostate MRI often relies on accurate prostate zonal segmentation. However, state-of-the-art automatic segmentation methods often fail to produce well-contained volumetric segmentation of the prostate zones since certain slices of prostate MRI, such as base and apex slices, are harder to segment than other slices. This difficulty can be overcome by leveraging important multi-scale image-based information from adjacent slices, but current methods do not fully learn and exploit such cross-slice information. In this paper, we propose a novel cross-slice attention mechanism, which we use in a Transformer module to systematically learn cross-slice information at multiple scales. The module can be utilized in any existing deep-learning-based segmentation framework with skip connections. Experiments show that our cross-slice attention is able to capture cross-slice information significant for prostate zonal segmentation in order to improve the performance of current state-of-the-art methods. Cross-slice attention improves segmentation accuracy in the peripheral zones, such that segmentation results are consistent across all the prostate slices (apex, mid-gland, and base). The code for the proposed model is available at https://bit.ly/CAT-Net.",
  "full_text": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 42, NO. 1, JANUARY 2023 291\nCA T -Net: A Cross-Slice Attention T ransformer\nModel for Prostate Zonal Segmentation in MRI\nAlex Ling Yu Hung, Haoxin Zheng, Qi Miao, Steven S. Raman, Demetri T erzopoulos,Life Fellow, IEEE,\nand Kyunghyun Sung , Member, IEEE\nAbstract — Prostatecancer is the second leading cause of\ncancer death among men in the United States. The diagnosis\nof prostate MRI often relies on accurate prostate zonal seg-\nmentation. However, state-of-the-art automatic segmenta-\ntion methods often fail to produce well-contained volumetric\nsegmentation of the prostate zones since certain slices of\nprostate MRI, such as base and apex slices, are harder to\nsegment than other slices. This difﬁculty can be overcome\nby leveraging important multi-scale image-based informa-\ntion from adjacent slices, but current methods do not fully\nlearn and exploit such cross-slice information. In this paper,\nwe propose a novel cross-slice attention mechanism, which\nwe use in a Transformer module to systematically learn\ncross-slice information at multiple scales. The module can\nbe utilized in any existing deep-learning-based segmenta-\ntion framework with skip connections. Experiments show\nthat our cross-slice attention is able to capture cross-slice\ninformation signiﬁcant for prostate zonal segmentation in\norder to improve the performance of current state-of-the-\nart methods. Cross-slice attention improves segmentation\naccuracy in the peripheral zones, such that segmentation\nresults are consistent across all the prostate slices (apex,\nmid-gland, and base). The code for the proposed model is\navailable at https://bit.ly/CAT-Net.\nIndex Terms— Attention mechanism, deep learning, mag-\nnetic resonance imaging, prostate zonal segmentation,\ntransformer network.\nManuscript received 16 August 2022; accepted 23 September\n2022. Date of publication 4 October 2022; date of current version\n29 December 2022. This work was supported in part by the National Insti-\ntutes of Health under Grant R01-CA248506 and in part by the Integrated\nDiagnostics Program, Departments of Radiological Sciences and Pathol-\nogy, David Geffen School of Medicine, UCLA.\n(Corresponding author:\nAlex Ling Yu Hung.)\nThis work involved human subjectsor animals in its research. Approval\nof all ethical and experimental procedures and protocols was granted\nby the UCLA Institutional Review Board (IRB) with a waiver for written\ninformed consent and is compliant with the 1996 United States Health\nInsurance Portability and Accountability Act.\nAlex Ling Yu Hung and Haoxin Zheng are with the Computer Science\nDepartment and the Department of Radiological Sciences, University of\nCalifornia, Los Angeles (UCLA), Los Angeles, CA 90095 USA (e-mail:\nalexhung96@ucla.edu; haoxinzheng@ucla.edu).\nQi Miao is with the Department of Radiological Sciences, University\nof California, Los Angeles (UCLA), Los Angeles, CA 90095 USA, and\nalso with the Department of Radiology, The First Afﬁliated Hospital of\nChina Medical University, Shenyang, Liaoning 110001, China (e-mail:\nmeganmiaoqi@126.com).\nSteven S. Raman and Kyunghyun Sung are with the Department of\nRadiological Sciences, University of California, Los Angeles (UCLA),\nLos Angeles, CA 90095 USA (e-mail: sraman@mednet.ucla.edu;\nksung@mednet.ucla.edu).\nDemetri Terzopoulos is with the Computer Science Department, Uni-\nversity of California, Los Angeles (UCLA), Los Angeles, CA 90095 USA,\nand also with VoxelCloud, Inc., Los Angeles, CA 90024 USA (e-mail:\ndt@cs.ucla.edu).\nDigital Object Identiﬁer 10.1109/TMI.2022.3211764\nI. I NTRODUCTION\nP\nROSTATE cancer (PCa) is the most common cancer\nand the second leading cause of cancer-related death\namong men in the United States [1]. Multi-parametric MRI\n(mpMRI), including T2-weighted imaging (T2WI), diffusion-\nweighted imaging (DWI), and dynamic contrast-enhanced\n(DCE) MRI, is now the preferred non-invasive imaging tech-\nnique for prostate cancer (PCa) diagnosis prior to biopsy [2].\nAccording to the Prostate Imaging Reporting and Data System\n(PI-RADS) [3], the current clinical standard for interpreting\nmpMRI, a suspicious lesion should be analyzed differently\nin different prostate zones, among them the transition zone\n(TZ) and the peripheral zone (PZ), due to variations in image\nappearance and cancer prevalence [4]. The zonal information\nis essential and should be provided explicitly for the accurate\nidentiﬁcation and assessment of suspicious lesions. Moreover,\nthe size of the TZ is often used to evaluate and monitor benign\nprostate hyperplasia (BPH) in clinical practice [5]. However,\nthe manual annotation of prostate zones is typically time-\nconsuming and highly variable depending on experience level.\nTherefore, reliable and robust automatic zonal segmentation\nmethods are needed in order to improve PCa detection.\nWith the emergence of deep learning (DL), DL-based\nmedical image segmentation methods have been proposed\nto automatically segment the prostate zones [6], [7], [8],\n[9], [10]. DL-enabled segmentation methods tend to perform\nwell in general, but several studies report that the apex and\nbase locations of the prostate are more difﬁcult to segment\nthan the mid-gland slices [11], [12]. Leveraging information\nfrom nearby slices can improve the performance of these\nmethods in all parts of the prostate. However, most 2D-based\nsegmentation methods do not fully consider or systematically\nlearn the available cross-slice information, thus they may\ndisregard important structural information about the prostate,\nleading to inconsistent segmentation results. It is crucial to take\nthe through-plane information or cross-slice relationship into\nfull consideration when devising prostate zonal segmentation\nmodels.\nSeveral 3D DL-based medical image segmentation networks\nhave been proposed in previous studies [13], [14], but their\napplication to prostate MRI has been limited. Following the\nstandard guideline of PI-RADS [3], T2WI images are acquired\nusing the multi-slice 2D Turbo Spin Echo (TSE) sequence,\nresulting in high in-plane image resolution (e.g., 0.3–1.0 mm)\nbut low through-plane resolution (e.g., 3.0–6.0 mm). Due to\nthe anisotropic nature of the image resolution, existing 3D\nsegmentation networks may not bedirectly applicable as they\nare typically designed for nearly isotropic 3D images [13],\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n292 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 42, NO. 1, JANUARY 2023\n[14], and their performance suffers when they are confronted\nwith anistropic data [15], [16], [17].\nTransformer networks have become the dominant DL archi-\ntecture in the natural language processing (NLP) domain as\ntheir multi-head self-attention (MHSA) mechanisms learn the\nglobal context and the relationship among different word\nembeddings [18]. Following similar ideas, MHSA mechanisms\nhave recently been applied in the computer vision domain\nto address tasks such as object detection, segmentation, and\nclassiﬁcation.\nAs it can capture the long-range dependencies in\nimages [18], [19], [20], self-attention is a promising mech-\nanism for systematically learning the cross-slice information\nneeded to improve 3D medical image analysis; that is, as com-\npared to convolutional networks whose inherent inductive bias\nis to focus more on neighboring features. To exploit the fact\nthat prostate zonal segmentation can beneﬁt from information\nspanning all the slices through the entire prostate instead of\nonly neighboring slices, we devise a 2.5D cross-slice attention-\nbased module that can be incorporated within any network\narchitecture with skip connections in order to capture the\nglobal cross-slice relationship. Referring to Fig. 1, we note\nthat a 2.5D method employs only 2D convolutional layers\nyet learns 3D information, whereas 2D methods also use 2D\nconvolutional layers, but consider only a single image, while\n3D methods input 3D volumes and use 3D convolutional\nlayers.\nOur main contributions include the following:\n1) We formally propose the use of a cross-slice attention\nmechanism to capture the r elationship between MRI\nslices for the purposes of prostate zonal segmentation.\n2) We devise a novel 2.5D Cross-slice Attention Trans-\nformer (CAT) module that can be incorporated into\nexisting skip-connection-based network architecture to\nexploit long-range information from other slices.\n3) We perform an experimental study which demonstrates\nthat our proposed DL models, called CAT-Net, are able\nto improve zonal segmentation results both in general\nand on different prostate parts, resulting in a new state\nof the art performance.\nII. R\nELA TEDWORK\nA. Fully Convolutional Network Architectures\nThe U-Net model [7] has revolutionized medical image\nsegmentation using an encoder-decoder convolutional\nneural network (CNN) architecture with multi-scale skip\nconnections between the encoder and decoder that preserve\nhigh-resolution image information. Building upon U-Net,\na number of subsequent segmentation models have been\nproposed. ResU-Net [21] applied the residual connections\nfrom ResNet [22], and the combination of ResNet with U-Net\nhas also proven effective outside of medical image segmenta-\ntion [23], [24]. Based on ResU-Net, Alom et al. [25] proposed\na segmentation network with better feature representation by\nmeans of feature accumulation with recurrent convolutional\nlayers. Apart from residual blocks, other attention-based\nmodules have been incorporated into U-Net to improve\nsegmentation; e.g., Rundo et al. [26] incorporated a squeeze\nFig. 1. (a) A conventional 2D segmentation network inputs a slice\nof interest, performs 2D convolutions, and predicts its segmentation\nmask. (b) A conventional 2.5D network inputs a middle slicei along\nwith two nearby slicesi − /BD and i + 1 (deﬁned as 2.5D w/3), performs\n2D convolutions, and predicts the segmentation mask of slicei. (c) A\nconventional 3D network, inputs the entire volume ofl slices, performs\n3D convolutions, and predicts segmentation masks for alll slices. (d) Our\nproposed framework employs 2D convolutions, encodes all the slices in\nthe same volume, passes the encoded feature maps from the different\nslices through the Cross-slice Attention Transformer (CA T) module, and\ndecodes them to predict segmentation maps for all the slices.\nand excitation (SE) module [27], a form of channel-wise\nattention, and Oktay et al. [28] took the attention module [29]\na step further and applied it in U-Net. Other U-Net variants\nhave recently emerged. nnU-Net [16] modiﬁed the batch\nnormalization [30] in U-Net to instance normalization [31]\nand employed a leaky rectiﬁed linear activation unit (ReLU).\nCompared to U-Net, U-Net++ [32] used more nested and\ndense skip connections to better capture the ﬁne-grained\ndetails of foreground objects. MSU-Net [33] added multi-scale\nblocks, which consist of convolutions with different kernel\nsizes, into U-Net to improve the segmentation details. Gu et\nal. [34] proposed a novel dense atrous convolution (DAC)\nblock along with a residual multi-kernel pooling (RMP) block\nand put them in the bottleneck layer of U-Net to capture more\nhigh-level features and preserve more spatial information.\nApart from U-Net-based network architectures, DRINet [35]\nconsists of dense connection blocks, residual Inception blocks,\nHUNG et al.: CAT -Net: CROSS-SLICE ATTENTION TRANSFORMER MODEL FOR PROSTATE ZONAL SEGMENTATION 293\nand unpooling blocks, which can learn more distinctive\nfeatures. Crossbar-Net [36] samples vertical and horizonal\npatches and processes them separately in two sub-models.\nBardis et al. [6] applied the U-Net on T2WI images to\nperform prostate zonal segmentation. Building on top of\nDeepLabV3+ [37], Liu et al. [11] employed multi-scale\nfeature pyramid attention (MFPA) to further utilize encoder-\nside information at different scales to segment prostate zones,\nand subsequently expanded the network structure with a spatial\nattentive module (SAM) and Bayesian epistemic uncertainty\nbased on dropout [8]. Cuocolo et al. [38] have thoroughly\ninvestigated network structures for prostate zonal segmentation\nand concluded that ENet [39] is superior to U-Net and\nERFNet [40]. Zabihollahy et al. [9] employed two separate\nnetworks for the segmentation of different zones and combined\nthem with post-processing. The Dense-2 U-Net [10] was\nshown to be the best performing 2D deep model for prostate\nzonal segmentation on the public dataset ProstateX [41]. Not\nlimited to T2WI images, Rundo et al. [42] performed multi-\nspectral MRI prostate gland segmentation based on clustering.\nThree-dimensional CNNs are popular for 3D medical image\nsegmentation. 3D U-Net [13], VNet [43], and DenseV oxel-\nNet [14] are U-Net architectures that use 3D rather than\n2D convolution, and they have proven effective on image\ndata whose cross-pixel distance is similar in all three dimen-\nsions. Wang et al. [44] used a two-stage 3D U-Net for multi-\nmodality whole heart segmentation. Other researchers have\napplied modiﬁed 3D U-Nets to infant brain segmentation [45],\nlung nodule segmentation [46], and brain tumor segmenta-\ntion [47]. With regard to the application of 3D methods to\nprostate zonal segmentation, Nai et al. [12] concluded that\nmost methods have similar performance, but that the addition\nof ADC and DWI data would slightly improve performance.\nYu et al. [48] used mixed residual connections in a volu-\nmetric ConvNet for whole prostate segmentation. Z-Net [49],\ncapable of capturing more features in a multi-level manner,\nwas built on U-Net to perform 3D prostate segmentation.\nWang et al. [50] incorporated group dilated convolution into a\ndeeply supervised fully convolutional framework for prostate\nsegmentation.\nB. Transformer Architectures\nOriginally developed for NLP, the Transformer archi-\ntecture [18] has recently b een gaining momentum in\nvision. Dosvitskiy et al. [51] proposed the Vision Trans-\nformer (ViT) for image classiﬁcation, treating images as\n16 × 16 words. Unlike the ViT, the Swin Transformer [52]\nused shifted windows instead of 16 × 16 ﬁxed-size win-\ndows. Carion et al. [53] proposed the Detection Transformer\n(DETR) for object detection.\nSeveral Transformer-based methods have been proposed\nfor medical image segmentation. MedT [19] coupled a gated\nposition-sensitive axial attention mechanism with a Local-\nGlobal (LoGo) training methodology, improving segmentation\nquality over the U-Net and attention U-Net. UTNet [54]\nincorporated Transformer blocks into the skip connections\nin U-Net, allowing the skip connection feature maps to go\nthrough a Transformer block before they reach the decoder\nside. CoTr [55] instead concatenated all the skip connection\nfeature maps and applied the Transformer on the concatenated\nvector. Petit et al. [56] proposed a U-Net based Transformer\nframework with a self attention or cross attention module\nafter each skip connection of the normal U-Net. However,\nthese Transformer-based medical image segmentation methods\napply attention only between pixels or patches, not between\nslices.\nOur work is closest to that by Guo and Terzopoulos [57],\nwhich utilized attention between slices at the bottom layer of\na U-Net with a Transformer network. The work demonstrated\nits feasibility for 3D medical image segmentation but did\nnot consider cross-slice information at different scales and\ndifferent semantic information learned by multiple heads of\nthe attention mechanism.\nIII. M\nETHODS\nA. Overview\nWe propose a 2.5D Cross-slice Attention Transformer\n(CAT) module to systematically learn cross-slice information\nfor prostate zonal segmentation. The module is applicable\nwithin U-Net-like architectures with skip connections between\nthe encoder and decoder. Segmentation models using CAT\nmodules consist of three parts: a standard 2D encoder, a stan-\ndard 2D decoder, and CAT modules in different layers. In other\nwords, were we to remove the CAT modules, the network\nwould be a pure 2D network with no interaction between\nslices. The overall structure of the CAT module is illustrated\nin Fig. 2, and the incorporation of CAT modules into existing\ndeep models, such as nnU-Net and nnU-Net++, is illustrated\nin Fig. 3.\nThe remainder of this section is organized as follows:\nSection III-B introduces the cross-slice attention mechanism.\nCross-slice attention is used in the Transformer block, which\nis discussed in Section III-C. Positional encoding followed by\nN Transformer blocks comprise the CAT module, which is\ndescribed in Section III-D, where we also explain how the\nCAT module is used in existing networks. We mainly discuss\nhow the CAT module can be incorporated into U-Net and U-\nNet++ networks to yield our novel CAT-Net models, but any\nother skip-connection-based networks are also amenable.\nB. Cross-Slice Attention\nPrevious studies have used attention modules to learn inter-\nchannel or inter-pixel relationships, whereas our goal here\nis to use the attention mechanism to learn the cross-slice\nrelationship for the purposes of our segmentation task. This\nis important because single-slice prostate zonal segmentation\ncan suffer from ambiguities, especially near apex and base\nslices. This is also true for manual annotation as clinicians\ntypically refer to nearby slices while annotating the current\nslice of interest. We devise an algorithm that mimics the\nmanual segmentation process by attending to other slices when\nthe current slice is being annotated. To this end, we regard\neach slice as being analogous to a word in NLP problems\nwhile keeping the spatial information of images intact. After\nthe images are encoded by the encoder, we treat the image\nfeatures as a deep representation of the “word”.\n294 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 42, NO. 1, JANUARY 2023\nFig. 2. CA T module includes a positional encoding andN Transformer blocks.\nFig. 3. Implementation of CA T modules in the(a) nnU-Net and (b) nnU-Net++. Following the notation in [32], circles represent feature maps\ncalculated at the corresponding node; gray circles represent the 2D encoder and white circles represent the 2D decoder. Rectangles represent the\nCA T modules, which operate in 3D. The input stack of images from the patient ﬂow through the encoder, into CA T modules, and then through the\ndecoder to produce the output segmentation.\nLet an l-slice stack of input images be represented by the\n4D tensor x ∈ Rl×h×w×c , which is a stack of feature maps of\nheight h, widthw, and number of channelsc. To mitigate com-\nputational expense, we leverage the typically high correlation\nof nearby pixels in feature maps and downsamplex by average\npooling with a kernel size ofk, generating a condensed stack\nof feature mapsxpool ∈ Rl×h\nk ×w\nk ×c. We then calculate queries\nQ\u0003,k e y sK \u0003 as a linear projection ofxpool,a n dv a l u e sV as a\nlinear projection ofx in the channel dimension:\nQ\u0003 = xpoolWQ , (1)\nK \u0003 = xpoolWK , (2)\nV = xW V , (3)\nwhere WQ , WK , WV ∈ Rc×c are learnable weights. Note that\nQ\u0003, K \u0003,a n dV are the deep representation of thel slices and\nthat the above 4D tensor multiplication is deﬁned as follows:\nThe product C = BW of al ×h ×w ×c tensor B and a c ×c\nmatrix W is calculated as\nC[i, j,k,m]=\nc∑\nn=1\nB[i, j,k,n]W[n,m]. (4)\nThe attention matrixA ∈ Rl×l , which determines how much\nattention the algorithm pays to other slices while segmenting\na slice, is calculated as\nA = softmax\n(\nQK T\n√\nhwc/k2\n)\n, (5)\nwhere each row ofQ, K ∈ Rl×hwc\nk2 , the reshaped matrices of\nQ\u0003 and K \u0003, represents the query and key for each slice. The\nsoftmax is performed on the second dimension. An element\nA[i, j] in the attention matrix indicates how similar the query\nfor slice i is to the keys for slicej; i.e., it is computed as a\nfunction that performs a weighted average of the values for all\nthe slices to account for the interactions between queries and\nkeys. The output of the cross-slice attentiony ∈ Rl×h×w×c is\ncomputed as\ny = AV . (6)\nFig. 4illustrates our cross-slice attention mechanism, which,\nunlike the 2D self-attention mechanism [18], uses a condensed\ndeep image feature map to calculate queries Q and keys\nK and uses a normally encoded feature map for values V ,\nwhile working in a 4D space to calculate the attention matrix\nbetween slices instead of between pixels.\nC. Transformer Block\nWe incorporate our cross-slice attention mechanism into\na Transformer block, which is the structure widely used in\nTransformer-based approaches, where one typically sees a\nmulti-headed attention module followed by linear operations,\nnon-linear modules, and normalizations with skip connections\nin between. Our Transformer block is shown within the dashed\nbox inFig. 2. The input to the Transformer block goes through\na multi-headed cross-slice attention and is then subjected to\nlinear projections along with non-linear activations. Specif-\nically, we perform multiple cross-slice attention in parallel,\nHUNG et al.: CAT -Net: CROSS-SLICE ATTENTION TRANSFORMER MODEL FOR PROSTATE ZONAL SEGMENTATION 295\nFig. 4. Cross-slice attention with inputx and outputy.\nobtaining yi ,w h e r ei = 1,2,..., H (H is the number of\nheads), similar to previous work [18]. This is done so that\nthe network can learn multiple semantics during the attention\nprocedure; i.e., for different meanings, the network learns\ndifferent attention matrices. Subsequently, we concatenate all\nthe yi in the c dimension to obtain the outputy of the multi-\nheaded cross-slice attention module. The ﬁnal output of the\nTransformer block can be expressed as\nz = Layer_Norm(GELU(zintW2 + b2) + zint), (7)\nwith intermediate result\nzint = Layer_Norm(GELU(yW1 + b1) + x), (8)\nwhere W1 and W2 are the linear projection matrices,b1 and\nb2 are bias terms, GELU is the Gaussian error linear unit [58],\nand Layer_Norm performs layer normalization [59] across the\nh, w,a n dc dimensions.\nD. Network Architecture\nAs shown in Fig. 3 , we use nnU-Net [16] and\nnnU-Net++ [32] as the backbones of our CAT-Net\narchitectures, which we denote as CAT-nnU-Net and\nCAT-nnU-Net++, respectively. Other network architectures\nwith skip connections between the encoder and decoder\nwould also be suitable. We ﬁnd that our attention mechanism\nworks well on nnU-Net-like designs where Leaky ReLU and\ninstance normalization is used rather than normal ReLU and\nbatch normalization.\nThe 2D encoder E takes in a l × c\n0 × h0 × w0 tensor,\nwhere l, h0, w0,a n dc0 are the number of slices, height, width\nand the number of channels, respectively. It treats l as the\nbatch dimension, where the slices do not interfere with each\nother. A deep feature mapxi ∈ Rl×hi ×wi ×ci is input into the\nCAT module (Fig. 2), in which it is subjected to a positional\nencoding and N Transformer blocks. The positional encoding\nis important in this task, since it tells the network the location\nof the slice. As in previous work [18], we ﬁrst add to it a\nlearnable positional encoding PEi initialized as follows [60]:\nFor all elements on slicep and channel 2j,\nPEi [p,:,:,2 j]= sin\n( p\n100002 j/ci\n)\n, (9)\nand for all the elements on slicep and channel 2j + 1,\nPEi [p,:,:,2 j + 1]= cos\n( p\n100002 j/ci\n)\n. (10)\nThen, the feature is passed through N Transformer blocks,\nwhere the feature maps of different slices interact to yield\nthe outputs of the skip connectionzi ∈ Rl×hi ×wi ×ci .A g a i n ,\nwe treat the ﬁrst dimension as the batch dimension, interpret-\ning z\ni as l feature maps of 2D images. The 2D decoder takes\nin zi and outputs the ﬁnal segmentation masks.\nMore generally, encoderE is given l image slices denoted\nas x0 ∈ Rl×h0×w0 ×c0 and returns the encoded images\nxi ∈ Rl×hi ×wi ×ci at different scales 1 ≤ i ≤ L,\nwhere L is the total number of layers in the encoder:\n{xi }1≤i≤L = E(x0).\nWe denote the lowest layer in the decoder asDL and the\nlayers above it asDL−1, DL−2,... , and the output of decoder\nlayer i as di . In a conventional skip-connection-based network,\nthe decoder takes in the feature maps from different scales:\ndi =\n{\nDi (xi ) if i = L,\nDi (xi ,di+1) otherwise, (11)\nwhereas in our CAT-Net, CAT modules are inserted into the\nnetwork:\ndi =\n{\nDi (CATi (xi )) if i = L,\nDi (CATi (xi ),di+1) otherwise, (12)\nwhere CATi is the CAT module at scalei.\n296 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 42, NO. 1, JANUARY 2023\nOur approach works better on networks with skip connec-\ntions at different scales since multi-resolution feature maps\ncapture different semantic information [61], [62]; thus, the\nattention at different scales should not be the same. The cross-\nslice attention matrices should be different across the different\nlayers of the network, whereas in networks without skip con-\nnections across the different scales, the semantic information\nat different scales is not represented in the feature maps,\nand the same attention matrix would be applied, leading to\nunsatisfactory results.\nIV . E\nXPERIMENTS\nOur experimental study was performed in compliance with\nthe United States Health Insurance Portability and Account-\nability Act (HIPAA) of 1996 and was approved by the insti-\ntutional review board (IRB) with a waiver of the requirement\nfor informed consent.\nA. Study Population and MRI Data\n1) Our Dataset: 296 patients who underwent pre-operative\n3 Tesla (3T) mpMRI prior to robotic-assisted laparoscopic\nprostatectomy were included in the study. Patients with prior\nradiotherapy or hormonal therapy and with an endorectal\ncoil were excluded. All mpMRI scans were performed on\none of the four 3T MRI scanners (Siemens Healthineers\nMAGNETOM Trio, Skyra, Prisma, and Vida) from January\n2013 to December 2018 at a single academic institution. T2WI\nwas acquired using a T2-weighted Turbo Spin Echo (TSE)\nMR sequence following the standardized imaging protocol\nof the European Society of Urogenital Radiology (ESUR)\nPI-RADS guidelines [3]. The T2WI images were used for the\nzonal segmentation with an in-plane resolution of 0.625 mm\n2,\na through-plane resolution of 3 mm, and an image size of\n320 × 320 × 20 voxels. We cropped the central images to\n128 × 128 and used 238, 29, and 29 patients for training,\nvalidation, and testing, respectively. For experiments involving\ncross validation, we randomized the data and grouped them\ninto ﬁve folds.\n2) ProstateX Dataset: A total of 193 patients from the\nProstateX [41] dataset were included in the study. The imaging\nwas performed by Siemens MAGNETOM Trio and Skyra 3T\nMR scanners. A turbo spin echo sequence was used to acquire\nT2WI images, which had a resolution of 0.5 mm in plane and\na slice thickness of 3.6 mm. We excluded some data from the\noriginal dataset due to differences in image sizes and sequence\nlengths. The image size is 384× 384 × 18 voxels, where we\npick only the middle 18 slicesfor each patient. We cropped\nthe central images to 160 × 160 and resampled them to\n128 × 128 and used 157, 20, and 20 patients for training,\nvalidation, and testing, respectively.\nB. Implementation Details\nTo provide input to the models, we normalized the T2WI\nMRI images; i.e., the normalized intensity of pixel(i, j) on\nslice k is ˜Ii,j,k = (Ii,j,k − μ)/σ,w h e r eIi,j,k is the original\npixel intensity and μ and σ are the per-patient mean and\nstandard deviation of the pixel intensity, respectively.\nFor our nnU-Net implementation, we downsampled ﬁve\ntimes on the encoder side, and had 64, 128, 256, 512, 1024,\nand 2048 ﬁlters in each of the convolutional layers of the\nencoder. For our nnU-Net++ implementation, we downsam-\npled four times on the encoder side and had 64, 128, 256,\n512, and 1024 ﬁlters in each of the convolutional layers of the\nencoder. The decoders were the exact opposite of the encoders.\nFor the 3D networks, we performed cross-slice upsampling\nand provided the upsampled volumes as inputs to the 3D mod-\nels that require more than 2 times downsampling. For a fair\ncomparison against the other U-Net-based models, we ensured\nthat the upsampled volumes could be downsampled at least\nﬁve times. During testing, only the results from the non-\ninterpolated slices were considered in our comparisons against\nother models. For other parts of the 3D networks and all the\nother models in our experiments, we strictly adhered to the\narchitectures described in the original papers.\nFor data augmentation, we performed only center crop,\nhorizontal ﬂip, and Gamma transform. The same data aug-\nmentation scheme was applied across all of the experiments.\nWe applied cross entropy loss as the loss function for\n150 epochs in all the training procedures, and used Adam [63]\nwith a learning rate of 0.0001 and weight decay regulariza-\ntion [64] with the parameter set to 1 × 10\n−5. We did not\nperform any post-processing after the segmentation. We main-\ntained a simple training setting in order to elucidate the beneﬁt\nof our CAT module. In our model, we set the number of\nTransformer blocks N = 2, the number of headsH = 3, and\nthe average pooling size k = 4. The CAT-Nets were trained\non a single Nvidia Quadro RTX 8000 GPU, while the other\nnetworks were trained on an Nvidia RTX 3090 GPU.\nC. Evaluation\n1) Evaluation Metrics: We used Intersection over Union\n(IoU), Dice coefﬁcient (Dice), Relative absolute volume differ-\nence (RA VD), and average symmetric surface distance (ASSD)\nfor evaluation, and these metrics were calculated in a 3D\npatient-wise manner. For experiments involving statistical test-\ning, we used the Mann-Whitney U Test [65] to statistically test\nthe distribution of results from our model and the competing\nmodels.\n2) Quantitative Evaluation: We performed a comprehensive\ncomparison between our model and state-of-the-art 2D, 2.5D,\nand 3D medical image segmentation models.\nWith regard to 2D methods, we compared ours\nagainst DeepLabV3+ [37], the Liu et al. model [8], the\nZabihollaghy et al. model [9], CE-Net [34], MSU-Net [33],\nDense-2 U-Net [10], nnU-Net ++ [32], and nnU-Net [16].\nDense-2 U-Net as well as the models of Liu et al. and\nZabihollaghy et al. were designed for zonal segmentation,\nwhile the others are generic segmentation models. Speciﬁcally,\nwe adopted the 2D nnU-Net from the paper [16]. Additionally,\nwe adapted into U-Net++ the Leaky ReLU along with the\ninstance normalization design,which is the main architectural\ncontribution of nnU-Net, and denote it as nnU-Net ++.\nWe did not directly compare against U-Net [7] and U-Net++\nbecause previous work has demonstrated the superiority\nof nnU-Net-based designs, which is supported by our\nexperimental results in Section IV-C.3.\nHUNG et al.: CAT -Net: CROSS-SLICE ATTENTION TRANSFORMER MODEL FOR PROSTATE ZONAL SEGMENTATION 297\nTABLE I\nPERFORMANCE OF NN U-N ET BASED MODELS AND OTHER MODELS ON OUR DATASET\nTABLE II\nPERFORMANCE OF NN U-N ET BASED MODELS AND OTHER MODELS ON PROSTATE X\nWe continued using nnU-Net and nnU-Net++ as the back-\nbones of our 2.5D models. Like Zhang et al. [66], we stacked\nnearby slices together and inputted the stacks to 2D net-\nworks in order to segment the middle slices. In other words,\nduring each run, the networks take in a stack of images\ni −k,..., i,..., i +k and output the segmentation mask only\nfor the middle slice i. In our experiments, we stacked 3, 5,\nand 7 slices together and fed them into the networks, which\nwe denote as nnU-Net and nnU-Net++ w/3, w/5, and w/7.\nNote that nnU-Net w/1 and nnU-Net ++ w/1 are the 2D\nnetworks where we only input 1 image per segmentation.\nFor the 3D methods, we used the most popular segmentation\nmodels, DenseV oxNet [14], VNet [43], and 3D U-Net [13].\nTable I and Table II compare nnU-Net based architectures\non our dataset and ProstateX, respectively, whileTable IIIand\nTable IV compare nnU-Net++ based architectures on our\ndataset and ProstateX, respectively. Our model outperforms\nevery other model in almost every metric. Conventional 2.5D\nmethods are better than 2D methods for nnU-Net based\nmodels, but performance starts to drop as the method uses\na 7-image stack input. For nnU-Net++ based models, 2.5D\nmethods show little orno improvement over 2D methods and\nthe optimal number of slices to include is unclear. Naively\nstacking nearby slices fails to fully utilize the newly added\ninformation from nearby slices without an explicit information\nexchange mechanism. The best performing 3D methods are\nusually better than most 2D methods and they are sometimes\nbetter than conventional 2.5D methods, but there is no clear-\ncut winner among the competing methods. Speciﬁcally, 3D U-\nNet is the best performing 3D model on our dataset, but VNet\nshows better performance on ProstateX. Furthermore, the 3D\nmodels generally perform decently in segmenting the PZ in our\ndataset, but perform poorly on ProstateX. As is shown in the\ntables, using either nnU-Net or nnU-Net++ as the backbone\nalong with our CAT modules yields better performance than\nany of the current models. The cross-slice attention in our\nmodel enables the network to learn the relationship between\nslices, which results in better performance.\nThe preceding tables reveal that, aside from our model, 2.5D\nmethods usually performed the best. However, the optimal\nnumber of adjacent slices differed between datasets, prostate\nzones, and backbone networks. For example, for nnU-Net\nbased models, nnU-Net w/5 performed best on our dataset\nwhile nnU-Net w/3 performed best on ProstateX. By contrast,\n298 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 42, NO. 1, JANUARY 2023\nTABLE III\nPERFORMANCE OF NN U-N ET++ BASED MODELS AND OTHER MODELS ON OUR DATASET\nTABLE IV\nPERFORMANCE OF NN U-N ET++ BASED MODELS AND OTHER MODELS ON PROSTATE X\nour model consistently performed the best on the different\ndatasets, prostate zones, and backbone networks.\nFor a more rigorous evaluation, we selected the 2D, 2.5D,\nand 3D methods in the nnU-Net based comparison that\nperformed best on our dataset (i.e., Table I), which are\nnnU-Net w/1, nnU-Net w/5, and 3D U-Net, and carried out\na 5-fold cross validation to determine the robustness of our\nmethod. The results shown in Table V, where the p-values\nwere calculated based on the Mann-Whitney U Test between\nCAT-nnU-Net and competing models, further establish that\nour method signiﬁcantly improves the performance of existing\nmodels. Although relative to the 3D U-Net our method does\nnot signiﬁcantly improve the ASSD in both the TZ and PZ,\nour CAT module signiﬁcantly outperforms the 3D U-Net in\nother metrics. Even though nnU-Net w/5 has better perfor-\nmance than 3D U-Net numerically, the fact that CAT-nnU-Net\nexhibits more signiﬁcant improvement over nnU-Net w/5 than\n3D U-Net reveals that the performance of 3D U-Net is less\nconsistent.\n3) Ablation Study: We conducted ablation studies using\nour dataset in which we compared the performance of both\nnn-based and conventional U-Net and U-Net++ architectures\nwith and without CAT modules. Our ﬁndings are reported in\nTable VIand Table VII. CAT modules substantially improved\nthe segmentation of PZ on conventional network architectures.\nIn general, the nn-based networks outperform conventional\nnetworks, and incorporating CAT modules into nn-based net-\nworks yields the best performance.\nIn view of the larger improvement in performance when\nadding CAT modules to nnU-Net and the long training time\nof CAT-nnU-Net++, we performed an ablation study into the\neffect of positional encoding and Transformer blocks using\nonly the CAT-nnU-Net. The results on our dataset are reported\nin Table VIII, where we used 5-fold cross validation and the\np-values were calculated based on the Mann-Whitney U Test\nbetween the network incorporating all the components and\nnetworks missing positional encoding or transformer blocks.\nWe can conclude from the table that using both positional\nencoding and Transformer blocks can help with prostate zonal\nsegmentation. Using both or either one alone outperforms\nusing neither. There is no statistical signiﬁcance between using\nboth and using transformer blocks alone in PZ segmentation,\nHUNG et al.: CAT -Net: CROSS-SLICE ATTENTION TRANSFORMER MODEL FOR PROSTATE ZONAL SEGMENTATION 299\nTABLE V\nPERFORMANCE OF NN U-N ET BASED MODELS AND OTHER MODELS ON OUR DATASET\nTABLE VI\nABLATION STUDY OF THE EFFECT OF NN -BASED DESIGN AND CA T MODULES ON U-N ET\nTABLE VII\nABLATION STUDY OF THE EF F E C TO FN N-BASED DESIGN AND CA T MODULES ON U-N ET++\nTABLE VIII\nABLATION STUDY OF THE EFFECT OF POSITIONAL ENCODING AND TRANSFORMER BLOCKS\nbut using both signiﬁcantly outperforms using only Trans-\nformer blocks in TZ segmentation. Using both is also better\nthan using just positional encoding.\n4) Qualitative Evaluation: Some representative results on our\ndataset are shown in Fig. 5.I n Fig. 5a, we compare our\nCAT-nnU-Net with other nnU-Net-based 2D and 2.5D models.\nThe other nnU-Net-based models tend to overpredict PZ on\nthe side of the TZ.\nIn the example ofFig. 5a, our model performs better in the\nsegmentation of PZ, especially in the lower middle region and\nthe two upper head regions of the PZ. Without effectively\nconsidering the cross-slice relationship, e.g., where the PZ\nstarts to emerge, the models would poorly segment the upper\nhead region of the PZ. This phenomenon is clearly revealed\nby the results of the other models, which over-predict the\nPZ region with many false positive predictions. Additionally,\nwhen including 7 slices in the 2.5D segmentation, the segmen-\ntation of the lower part of the PZ produces spurious spur-like\nregions. As the arrows indicate inFig. 5a, there are obvious\nFig. 5. Comparison of the CA T -nnU-Net against(a) other nnU-Net\nbased models and(b) nnU-Net w/1, nnU-Net w/5, and 3D U-Net. The\nsegmentation masks of PZ are shown in white, those of TZ in gray, and\nthose of other tissues in black.\nerrors in the segmentation, but the segmentation produced by\nour method does not suffer from such errors.\n300 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 42, NO. 1, JANUARY 2023\nTABLE IX\nPERFORMANCE WHEN USING CA T MODULES ONL Y INLAYERS 0–3 AND IN ALL 6L AYERS\nFig. 6. Performance of(a) TZ and(b) PZ segmentation on different prostate parts by different algorithms.\nFor a clearer demonstration of CAT-nnU-Net’s superiority,\nin Fig. 5b we compare it against the best performing 2D and\n2.5D nnU-Net based methods and the best 3D method based\non the quantitative results, which are nnU-Net w/1, nnU-Net\nw/5, and 3D U-Net. As is indicated by the arrows, other\nmodels tend to overpredict the TZ segmentation. Moreover,\nthe 3D U-Net also has some problems in segmenting the upper\nhead region of the PZ. By contrast, our method can perform\naccurate segmentation of the TZwithout predicting unrealistic\nshape for it, and around the upper head region of the PZ.\n5) Evaluation on Different Parts of the Prostate : Prostate\nzonal segmentation approaches exhibit signiﬁcant performance\ndifferences in different parts ofthe prostate [8]; hence, we will\nnext investigate the performance of our method on different\nparts of the prostate on our dataset. Deﬁning the ﬁrst and\nlast three slices of the prostate as apex and base, respectively,\nand the remaining slices are mid-gland slices, we continue our\nevaluation against other nnU-Net based models and 3D U-Net.\nAs shown in Fig. 6, other models perform differently in\nthe different prostate zones. Among the other nnU-Net-based\nmodels, nnU-Net w/5 is the best for TZ segmentation in\nall parts based on the quantitative results, while it performs\nbadly in apex slices. Although nnU-Net w/7 performs well in\nsegmenting PZ in the apex and base slices, its performance in\nthe mid-gland slices and TZ segmentation is underwhelming.\nTherefore, it would be hard to determine how many slices to\ninclude when performing traditional 2.5D prostate zonal seg-\nmentation. However, our method inputs all the slices and learns\nthe relationship between slices. 3D U-Net performs well in PZ\nsegmentation on base slices and is on par with our method in\nTZ segmentation on mid-gland slices, but the performance of\n3D U-Net diminishes elsewhere. Although our method yields\na marginally worse result than the nnU-Net w/7 on apex\nin the PZ and the 3D U-Net on base in the PZ, it clearly\noutperforms the other methods in general. Our method is\nconsistently among the top two best performing methods in\nevery scenario, whereas the performances of the other methods\ndecrease in certain scenarios.Note that performance compar-\nisons across different parts of the prostate may have practical\nlimitations because of ground-truth annotation complexities\ndue to the inherent ambiguity of zonal appearance at the\napex and base slices, which manifests as high inter-reader\nvariability [8].\nFig. 7 shows qualitative results of prostate zonal segmenta-\ntion on a single patient. Among the models, our CAT-nnU-Net\nis the most consistent, whereas other 2D and 2.5D models\nsuffer from over-prediction and inconsistency in anatomical\nstructures.\n6) Understanding the Attention Matrices: Since CAT-nnU-\nNet yields a larger improvement over nnU-Net than\nCAT-nnU-Net++ yields over nnU-Net++,w eu s e dt h ef o r -\nmer in the following experiment. The most notable attention\nmatrices A from each layer are visualized in Fig. 8, rang-\ning from ﬁne-resolution, top Layer 0 to coarse-resolution,\nbottom Layer 5. The CAT modules in the ﬁner Layers 0–3\nwere able to learn something meaningful, while those in the\ncoarser Layers 4 and 5 learned nothing. This observation is\nsupported by the segmentation performance results reported\nin Table IX, which are similar with and without attention\nin Layers 4 and 5. The full network yields slightly better\nresults on TZ but it has worse results on PZ. The cross-slice\nattention in the coarsest layers may confuse the network when\nsegmenting the more challenging PZ. Apparently, the cross-\nslice attention mechanism is unable to learn anything useful in\nLayers 4 and 5 because the network need not rely on nearby\nslices to learn coarse information. This seems to be consistent\nwith how clinicians annotate prostate images, as they need to\nrefer to nearby slices only to segment the ﬁner details.\nHUNG et al.: CAT -Net: CROSS-SLICE ATTENTION TRANSFORMER MODEL FOR PROSTATE ZONAL SEGMENTATION 301\nFig. 7. Comparison of our CA T -nnU-Net against other U-Net based models on different prostate parts.\nFig. 8. Attention matrices A from Layers 0–5. Each element A[i,j]\nindicates the attention slicei pays to slicej, with darker pixels denoting\nhigher values, and∑\nj A[i,j]= 1.\nThough the CAT modules in Layers 4 and 5 learn nothing\nuseful, we include them nonetheless, since doing so does\nnot signiﬁcantly hurt performance. The attention matrices in\nLayers 0 and 1 indicate that for the details, mid-gland slices\nwill attend more to either the apex or the base slices, and\nslices in the apex and base will attend more to the slices in\ntheir own prostate part. At the same time, apex and base slices\nwill also attend slightly to each other, which shows that there\nare some long-range dependencies in the segmentation of ﬁner\ndetails. The attention in Layers 2 and 3, which contain more\nsigniﬁcant information than Layers 0 and 1, focuses more on\nnearby slices in the mid-gland slices. This is in line with the\nresults of the previous section, where more slices are needed to\naccurately segment the base and apex, hence the more blurry\nregions at the top left and bottom right of Layer 2 as well as\nthe bottom right of Layer 3.\nV. D\nISCUSSION\nOur study demonstrated that applying the CAT modules\non skip connections at different scales can improve the per-\nformance of prostate zonal segmentation by systematically\nexploiting cross-slice attention. In particular, we see a sig-\nniﬁcant improvement in PZ segmentation compared with TZ\nsegmentation. We believe that this may be because it is rela-\ntively easier to segment the TZ without accounting for nearby\nslices as the shape of the TZ is well-deﬁned and consistent\nacross subjects. By contrast, PZ segmentation is harder than\nTZ segmentation since the shape of the PZ is less clear in\ncertain slices, which may be why our cross-slice attention\nmodule improved the performance of PZ segmentation relative\nto existing networks.\nOur cross-slice attention module has shown its superiority to\nother methods for the following reasons: 2D networks cannot\nacquire useful information from nearby slices. 3D convolution\napproaches need to work with interpolated data due to the\nanisotropy of the MRI data (i.e., through-plane resolution\nsubstantially lower than in-plane resolution). This can be\nproblematic if the number of slices is large. The problem with\nconventional 2.5D methods may be that the network cannot\nsufﬁciently process which slice is the one to be segmented.\nAdditionally, including more slices in the stack introduces\nmore useless information, so the network could have a harder\ntime learning the useful information. Furthermore, inputting\nmore slices might make the network more susceptible to\noverﬁtting.\nWe ﬁnd that CAT modules yield the biggest improvement on\nskip-connection-based encoder-decoder networks, e.g., U-Net\nand U-Net++ using leaky ReLU as activation and instance\nnormalization, which is the scheme in nnU-Net. However, even\nwith different activation functions and normalization schemes,\nperformance drops when applying the cross-slice attention\non frameworks without skip connections, e.g., DeepLabV3+\nand the model of Liu et al. The networks can learn different\nattention at different resolutions since the information in the\nfeature maps at different scales differs. This enables the\n302 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 42, NO. 1, JANUARY 2023\nnetwork to learn more meaningful cross-slice information.\nOn the other hand, the network could be confused by the cross-\nslice information without skip connections at different scales,\nleading to worse performance. Further investigation into why\nthe CAT modules do not work on architectures lacking skip\nconnections and how to make similar ideas work on these\narchitectures would be a good direction for further study.\nWe only considered T2WI, but in practice performing\nprostate segmentation on different MRI or multispectral MRI\ncan potentially improve segmentation performance [12], [42].\nOther image contrasts, such as T1WI, are capable of providing\ninformation related to the segmentation that is not present\nin T2WI [67]. To perform multispectral MRI tasks within\nour framework, we can add the multispectral MRI images as\ndifferent input channels to the network, where each channel\nis one type of MRI image. However, further consideration\nmay be needed for zonal segmentation since T1WI gener-\nally does not contain good tissue contrast between prostate\nzones.\nWe evaluated our approach on two datasets: our dataset\nand the ProstateX dataset. In absolute terms, its performance\ndiffered between the two datasets, but our CAT module\nconsistently improved performance on both datasets in rel-\native terms. Although our method was evaluated only on\nprostate zonal segmentation due to the limited data available,\nit promises to be applicable in other segmentation tasks where\ninformation in nearby slices is needed. This is most useful\nin anisotropic imaging data where cross-slice information is\ncrucial to accurate segmentation since 3D CNNs either tend\nto underperform or must work with interpolated data having\nhigh memory requirements. For isotropic data, our method\nwould still produce decent results compared to other 2D and\n2.5D methods, albeit with high memory requirements since\nour method is less memory efﬁcient than those lacking CAT\nmodules. For instance, for inputs of size 128 × 128 × 20,\nCAT-nnU-Net has 6.1 × 10\n8 trainable parameters compared\nwith 1 .4 × 108 for nnU-Net, and CAT-nnU-Net ++ has\n3.9 × 108 trainable parameters as opposed to 3.7 × 107 for\nU-Net++. How to adapt our method to problems other than\nprostate zonal segmentation and how to make it more memory\nefﬁcient would be interesting directions for future research.\nVI. C ONCLUSION\nWe have demonstrated improved prostate zonal\nsegmentation by applying a self-attention mechanism to\nsystematically learn and leverage cross-slice information.\nMore speciﬁcally, we have proposed a Cross-slice Attention\nTransformer (CAT) module and incorporated it into state-\nof-the-art 2D skip-connection-based deep networks. Our\nresulting CAT-Net models perform better than the current\nstate-of-the-art 2D, 2.5D, and 3D competitors in the task\nof prostate zonal segmentation, especially in the peripheral\nzone (PZ) of the prostate. Compared with conventional\n2.5D prostate segmentation methods, our segmentation\nperformance was good across the apex, mid-gland, and base\nslices. Furthermore, our analysis of the attention matrices\nprovided insights into how the cross-slice attention mechanism\nhelps in prostate segmentation. Our approach has proven to\nbe useful in skip-connection-based networks like U-Net and\nU-Net++, and our ablation study has shown that each of its\ncomponents contributes such that their combination yields\nthe best results. Further research is needed to investigate the\noptimal incorporation of our CAT modules into other network\narchitectures.\nR\nEFERENCES\n[1] P. Rawla, “Epidemiology of prostate cancer,”World J. Oncol., vol. 10,\nno. 2, p. 63, 2019.\n[2] M. B. Appayya et al., “National implementation of multi-parametric\nmagnetic resonance imaging for prostate cancer detection—\nRecommendations from a U.K. Consensus meeting,”BJU Int., vol. 122,\nno. 1, p. 13, 2018.\n[3] B. Turkbey et al., “Prostate imaging reporting and data system version\n2.1: 2019 update of prostate imaging reporting and data system version\n2,” Eur. Urol., vol. 76, pp. 340–351, Sep. 2019.\n[4] B. Israel, M. V . D. Leest, M. Sedelaar, A. R. Padhani, P. Zámecnik, and\nJ. O. Barentsz, “Multiparametric magnetic resonance imaging for the\ndetection of clinically signiﬁcant prostate cancer: What urologists need\nto Know. Part 2: Interpretation,”Eur. Urol., vol. 77, no. 4, pp. 469–480,\nApr. 2020.\n[5] N. Lawrentschuk, G. Ptasznik, and S. Ong, “Benign prostate dis-\norders,” in Endotext [Internet], K. R. Feingold et al., Eds. South\nDartmouth, MA, USA: MDText.com, Oct. 2021. [Online]. Available:\nhttps://www.ncbi.nlm.nih.gov/books/NBK279008/\n[6] M. Bardis et al., “Segmentation of the prostate transition zone and\nperipheral zone on MR images with deep learning,” Radiol., Imag.\nCancer, vol. 3, no. 3, May 2021, Art. no. e200024.\n[7] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-\nworks for biomedical image segmentation,” in Proc. Int. Conf. Med.\nImage Comput. Comput.-Assist. Intervent., Munich, Germany, in Lecture\nNotes in Computer Science, vol. 9349. Cham, Switzerland: Springer,\n2015, pp. 234–241.\n[8] Y . Liu et al., “Exploring uncertainty measures in Bayesian deep attentive\nneural networks for prostate zonal segmentation,”IEEE Access,v o l .8 ,\npp. 151817–151828, 2020.\n[9] F. Zabihollahy, N. Schieda, S. K. Jeyaraj, and E. Ukwatta, “Automated\nsegmentation of prostate zonal anatomy on T2-weighted (T2W) and\napparent diffusion coefﬁcient (ADC) map MR images using U-Nets,”\nMed. Phys., vol. 46, pp. 3078–3090, Jul. 2019.\n[10] N. Aldoj, F. Biavati, F. Michallek, S. Stober, and M. Dewey, “Auto-\nmatic prostate and prostate zones segmentation of magnetic resonance\nimages using DenseNet-like U-Net,”Sci. Rep., vol. 10, no. 1, pp. 1–17,\nDec. 2020.\n[11] Y . Liu et al., “Automatic prostate zonal segmentation using fully\nconvolutional network with feature pyramid attention,” IEEE Access,\nvol. 7, pp. 163626–163632, 2019.\n[12] Y .-H. Nai et al., “Evaluation of multimodal algorithms for the segmenta-\ntion of multiparametric MRI prostate images,”Comput. Math. Methods\nMed., vol. 2020, pp. 1–12, Oct. 2020.\n[13] O. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,\n“3D U-Net: Learning dense volumetric segmentation from sparse anno-\ntation,” in Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Inter-\nvent., Athens, Greece, in Lecture Notes in Computer Science, vol. 9902.\nCham, Switzerland: Springer, 2016, pp. 424–432.\n[14] L. Yu et al., “Automatic 3D cardiovascular MR segmentation\nwith densely-connected volumetric convnets,” in Proc. Int. Conf.\nMed. Image Comput. Comput.-Assist. Intervent. , Quebec City, QC,\nCanada, in Lecture Notes in Computer Science, vol. 10435. Cham,\nSwitzerland: Springer, 2017, pp. 287–295.\n[15] H. Jia et al., “3D APA-Net: 3D adversarial pyramid anisotropic convo-\nlutional network for prostate segmentation in MR images,”IEEE Trans.\nMed. Imag., vol. 39, no. 2, pp. 447–457, Feb. 2020.\n[16] F. Isensee et al., “NnU-Net: Self-adapting framework for U-Net-based\nmedical image segmentation,” 2018,\narXiv:1809.10486.\n[17] F. Isensee, P. F. Jaeger, P. M. Full, I. Wolf, S. Engelhardt, and\nK. H. Maier-Hein, “Automatic cardiac disease assessment on cine-\nMRI via time-series segmentation and domain speciﬁc features,”\nin Proc. Int. Workshop Stat. Atlases Comput. Models Heart , 2017,\npp. 120–129.\n[18] A. Vaswani et al., “Attention is all you need,” inProc. Adv. Neural Inf.\nProcess. Syst., 2017, pp. 5998–6008.\n[19] J. Maria Jose Valanarasu, P. Oza, I. Hacihaliloglu, and V . M. Patel,\n“Medical transformer: Gated axial-attention for medical image segmen-\ntation,” 2021, arXiv:2102.10662.\nHUNG et al.: CAT -Net: CROSS-SLICE ATTENTION TRANSFORMER MODEL FOR PROSTATE ZONAL SEGMENTATION 303\n[20] G. Tang, M. Müller, A. Rios, and R. Sennrich, “Why self-attention? A\ntargeted evaluation of neural machine translation architectures,” 2018,\narXiv:1808.08946.\n[21] A. Khanna, N. D. Londhe, S. Gupta, and A. Semwal, “A deep residual\nU-Net convolutional neural networkfor automated lung segmentation in\ncomputed tomography images,”Biocybern. Biomed. Eng., vol. 40, no. 3,\npp. 1314–1327, Jul. 2020.\n[22] K. He, X. Zhang, S. Ren, and J. S un, “Deep residual learning for\nimage recognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[23] Z. Zhang, Q. Liu, and Y . Wang, “Road extraction by deep residual\nU-Net,” IEEE Geosci. Remote Sens. Lett., vol. 15, no. 5, pp. 749–753,\nMay 2018.\n[24] F. I. Diakogiannis, F. Waldner, P. Caccetta, and C. Wu, “ResUNet-\na: A deep learning framework for semantic segmentation of remotely\nsensed data,”ISPRS J. Photogramm. Remote Sens., vol. 162, pp. 94–114,\nApr. 2020.\n[25] M. Z. Alom, C. Yakopcic, M. Hasan, T. M. Taha, and V . K. Asari,\n“Recurrent residual U-Net for medical image segmentation,” J. Med.\nImag., vol. 6, no. 1, 2019, Art. no. 014006.\n[26] L. Rundo et al., “USE-Net: Incorporating squeeze-and-excitation blocks\ninto U-Net for prostate zonal segmentation of multi-institutional MRI\ndatasets,” Neurocomputing, vol. 365, pp. 31–43, Nov. 2019.\n[27] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018,\npp. 7132–7141.\n[28] O. Oktay et al., “Attention U-Net: Learning where to look for the\npancreas,” 2018,arXiv:1804.03999.\n[29] S. Jetley, N. A. Lord, N. Lee, and P. H. S. Torr, “Learn to pay attention,”\n2018, arXiv:1804.02391.\n[30] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” in Proc. 32nd\nInt. Conf. Mach. Learn., 2015, pp. 448–456.\n[31] D. Ulyanov, A. Vedaldi, and V . Lempitsky, “Instance normalization: The\nmissing ingredient for fast stylization,” 2016,arXiv:1607.08022.\n[32] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++:\nA nested U-Net architecture for medical image segmentation,” inDeep\nLearning in Medical Image Analysis and Multimodal Learning for Clin-\nical Decision Support(Lecture Notes in Computer Science), vol. 11045.\nCham, Switzerland: Springer, 2018, pp. 3–11.\n[33] R. Su, D. Zhang, J. Liu, and C. Cheng, “MSU-Net: Multi-scale U-net\nfor 2D medical image segmentation,”Frontiers Genet., vol. 12, p. 140,\nFeb. 2021.\n[34] Z. Gu et al., “Ce-Net: Context encoder network for 2D medical image\nsegmentation,” IEEE Trans. Med. Imag., vol. 38, no. 10, pp. 2281–2292,\nOct. 2019.\n[35] L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara, and D. Rueckert,\n“DRINet for medical image segmentation,” IEEE Trans. Med. Imag.,\nvol. 37, no. 11, pp. 2453–2462, Nov. 2018.\n[36] Q. Yu, Y . Shi, J. Sun, Y . Gao, J. Zhu, and Y . Dai, “Crossbar-Net: A\nnovel convolutional neural network for kidney tumor segmentation in\nCT images,”IEEE Trans. Image Process., vol. 28, no. 8, pp. 4060–4074,\nAug. 2019.\n[37] L.-C. Chen, Y . Zhu, G. Papandreou,F. Schroff, and H. Adam, “Encoder–\ndecoder with atrous separable convolution for semantic image segmen-\ntation,” inProc. Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 801–818.\n[38] R. Cuocolo et al., “Deep learning whole-gland and zonal prostate\nsegmentation on a public MRI dataset,”J. Magn. Reson. Imag., vol. 54,\nno. 2, pp. 452–459, 2021.\n[39] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “ENet: A deep\nneural network architecture for real-time semantic segmentation,” 2016,\narXiv:1606.02147.\n[40] E. Romera, J. M. Álvarez, L. M. Bergasa, and R. Arroyo, “ERFNet: Efﬁ-\ncient residual factorized ConvNet for real-time semantic segmentation,”\nIEEE Trans. Intell. Transp. Syst., vol. 19, no. 1, pp. 263–272, Jan. 2018.\n[41] G. Litjens, O. Debats, J. Barentsz, N. Karssemeijer, and H. Huisman,\n“Computer-aided detection of prostate cancer in MRI,” IEEE Trans.\nMed. Imag., vol. 33, no. 5, pp. 1083–1092, Jan. 2014.\n[42] L. Rundo et al., “Fully automatic multispectral MR image segmentation\nof prostate gland based on the fuzzy c-means clustering algorithm,” in\nMultidisciplinary Approaches to Neural Computing(Smart Innovation,\nSystems and Technologies), vol. 69. Cham, Switzerland: Springer, 2018,\npp. 23–37.\n[43] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-Net: Fully convolutional\nneural networks for volumetric medical image segmentation,” inProc.\n4th Int. Conf. 3D Vis. (3DV), Oct. 2016, pp. 565–571.\n[44] C. Wang, T. MacGillivray, G. Macnaught, G. Yang, and D. Newby,\n“A two-stage 3D UNet framework for multi-class segmentation on full\nresolution image,” 2018,arXiv:1804.04341.\n[45] S. Qamar, H. Jin, R. Zheng, P. Ahmad, and M. Usama, “A variant form\nof 3D-UNet for infant brain segmentation,”Future Gener. Comput. Syst.,\nvol. 108, pp. 613–623, Jul. 2020.\n[46] Z. Xiao, B. Liu, L. Geng, F. Zhang, and Y . Liu, “Segmentation of lung\nnodules using improved 3D-UNet neural network,”Symmetry, vol. 12,\nno. 11, p. 1787, Oct. 2020.\n[47] J. Chang et al., “Brain tumor segmentation based on 3D Unet\nwith multi-class focal loss,” in Proc. 11th Int. Congr. Image Sig-\nnal Process., Biomed. Eng. Informat. (CISP-BMEI) , Oct. 2018,\npp. 1–5.\n[48] L. Yu, X. Yang, H. Chen, J. Qin, and P. A. Heng, “V olumetric ConvNets\nwith mixed residual connections for automated prostate segmentation\nfrom 3D MR images,” in Proc. 31st AAAI Conf. Artif. Intell., 2017,\npp. 66–72.\n[49] Y . Zhang, J. Wu, W. Chen, Y . Chen, and X. Tang, “Prostate segmentation\nusing Z-Net,” in Proc. IEEE 16th Int. Symp. Biomed. Imag. (ISBI),\nApr. 2019, pp. 11–14.\n[50] B. Wang et al., “Deeply supervised 3D fully convolutional\nnetworks with group dilated convolution for automatic MRI\nprostate segmentation,” Med. Phys., vol. 46, no. 4, pp. 1707–1718,\nApr. 2019.\n[51] A. Dosovitskiy et al., “An image is worth 16×16 words: Transformers\nfor image recognition at scale,” 2020,arXiv:2010.11929.\n[52] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” 2021,arXiv:2103.14030\n.\n[53] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nProc. Eur. Conf. Comput. Vis., Glasgow, U.K., in Lecture Notes in\nComputer Science, vol. 12346. Cham, Switzerland: Springer, 2020,\npp. 213–229.\n[54] Y . Gao, M. Zhou, and D. N. Metaxas, “UTNet: A hybrid transformer\narchitecture for medical image segmentation,” in Proc. Int. Conf.\nMed. Image Comput. Comput.-Assist. Intervent., Strasbourg, France, in\nLecture Notes in Computer Science, vol. 12908. Cham, Switzerland:\nSpringer, 2021, pp. 61–71.\n[55] Y . Xie, J. Zhang, C. Shen, and Y . Xia, “CoTr: Efﬁciently bridging\nCNN and transformer for 3D medical image segmentation,” 2021,\narXiv:2103.03024.\n[56] O. Petit, N. Thome, C. Rambour, L. Themyr, T. Collins, and L. Soler,\n“U-Net transformer: Self and cross attention for medical image segmen-\ntation,” in Proc. Int. Workshop Mach. Learn. Med. Imag., in Lecture\nNotes in Computer Science, Strasbourg, France, vol. 12966. Cham,\nSwitzerland: Springer, 2021, pp. 267–276.\n[57] D. Guo and D. Terzopoulos, “A transformer-based network for\nanisotropic 3D medical image segmentation,” inProc. 25th Int. Conf.\nPattern Recognit. (ICPR), Jan. 2021, pp. 8857–8861.\n[58] D. Hendrycks and K. Gimpel, “Gaussian error linear units (GELUs),”\n2016, arXiv:1606.08415.\n[59] J. Lei Ba, J. Ryan Kiros, and G. E. Hinton, “Layer normalization,” 2016,\narXiv:1607.06450.\n[60] J. Gehring, M. Auli, D. Grangier , D. Yarats, and Y . N. Dauphin,\n“Convolutional sequence to sequence learning,” inProc. 34th Int. Conf.\nMach. Learn., 2017, pp. 1243–1252.\n[61] L. A. Gatys, A. S. Ecker, and M. Bethge, “A neural algorithm of artistic\nstyle,” 2015,arXiv:1508.06576.\n[62] C. Olah, A. Mordvintsev, and L. Schubert, “Feature visualiza-\ntion,” Distill, 2017. [Online]. Available: https://distill.pub/2017/feature-\nvisualization, doi: 10.23915/distill.00007.\n[63] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\n2014, arXiv:1412.6980.\n[64] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\n2017, arXiv:1711.05101.\n[65] N. Nachar, “The Mann-Whitney U: A test for assessing whether two\nindependent samples come from the same distribution,” Tuts. Quant.\nMethods Psychol., vol. 4, no. 1, pp. 13–20, Mar. 2008.\n[66] H. Zhang et al., “Multiple sclerosis lesion segmentation with Tiramisu\nand 2.5D stacked slices,” in Proc. Int. Conf. Med. Image Com-\nput. Comput.-Assist. Intervent., Shenzhen, China, in Lecture Notes in\nComputer Science, vol. 11764. Cham, Switzerland: Springer, 2019,\npp. 338–346.\n[67] S. Ozer et al., “Supervised and unsupervised methods for prostate cancer\nsegmentation with multispectral MRI,” Med. Phys., vol. 37, no. 4,\npp. 1873–1883, 2010.",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.8403946161270142
    },
    {
      "name": "Computer science",
      "score": 0.7570381164550781
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6026009321212769
    },
    {
      "name": "Image segmentation",
      "score": 0.4853709936141968
    },
    {
      "name": "Prostate",
      "score": 0.45357170701026917
    },
    {
      "name": "Transformer",
      "score": 0.44229936599731445
    },
    {
      "name": "Computer vision",
      "score": 0.39210838079452515
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3548523783683777
    },
    {
      "name": "Medicine",
      "score": 0.10755640268325806
    },
    {
      "name": "Cancer",
      "score": 0.07702124118804932
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I91656880",
      "name": "China Medical University",
      "country": "CN"
    }
  ]
}