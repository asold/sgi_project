{
    "title": "ConsistNER: Towards Instructive NER Demonstrations for LLMs with the Consistency of Ontology and Context",
    "url": "https://openalex.org/W4393156816",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5057833078",
            "name": "Chenxiao Wu",
            "affiliations": [
                "Southeast University"
            ]
        },
        {
            "id": "https://openalex.org/A5007700207",
            "name": "Wenjun Ke",
            "affiliations": [
                "Southeast University"
            ]
        },
        {
            "id": "https://openalex.org/A5100396173",
            "name": "Peng Wang",
            "affiliations": [
                "Southeast University"
            ]
        },
        {
            "id": "https://openalex.org/A5112991468",
            "name": "Zhizhao Luo",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5102013275",
            "name": "Guozheng Li",
            "affiliations": [
                "Southeast University"
            ]
        },
        {
            "id": "https://openalex.org/A5103214522",
            "name": "Wanyi Chen",
            "affiliations": [
                "Southeast University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6685810106",
        "https://openalex.org/W3200787399",
        "https://openalex.org/W3186405834",
        "https://openalex.org/W2169099542",
        "https://openalex.org/W6681084769",
        "https://openalex.org/W4221165572",
        "https://openalex.org/W2170872814",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W2952410804",
        "https://openalex.org/W6770509782",
        "https://openalex.org/W3091869361",
        "https://openalex.org/W1980867644",
        "https://openalex.org/W4293043115",
        "https://openalex.org/W4366735603",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4200634402",
        "https://openalex.org/W2601450892",
        "https://openalex.org/W2953014395",
        "https://openalex.org/W4383175808",
        "https://openalex.org/W4321524373",
        "https://openalex.org/W3175225269",
        "https://openalex.org/W2991358176",
        "https://openalex.org/W2997124358",
        "https://openalex.org/W3207577858",
        "https://openalex.org/W2976444281",
        "https://openalex.org/W4385572845",
        "https://openalex.org/W3139958517",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2952230511",
        "https://openalex.org/W4284714522",
        "https://openalex.org/W4385573954",
        "https://openalex.org/W3105470358",
        "https://openalex.org/W4221159609",
        "https://openalex.org/W2963625095",
        "https://openalex.org/W4366400290"
    ],
    "abstract": "Named entity recognition (NER) aims to identify and classify specific entities mentioned in textual sentences. Most existing superior NER models employ the standard fully supervised paradigm, which requires a large amount of annotated data during training. In order to maintain performance with insufficient annotation resources (i.e., low resources), in-context learning (ICL) has drawn a lot of attention, due to its plug-and-play nature compared to other methods (e.g., meta-learning and prompt learning). In this manner, how to retrieve high-correlated demonstrations for target sentences serves as the key to emerging ICL ability. For the NER task, the correlation implies the consistency of both ontology (i.e., generalized entity type) and context (i.e., sentence semantic), which is ignored by previous NER demonstration retrieval techniques. To address this issue, we propose ConsistNER, a novel three-stage framework that incorporates ontological and contextual information for low-resource NER. Firstly, ConsistNER employs large language models (LLMs) to pre-recognize potential entities in a zero-shot manner. Secondly, ConsistNER retrieves the sentence-specific demonstrations for each target sentence based on the two following considerations: (1) Regarding ontological consistency, demonstrations are filtered into a candidate set based on ontology distribution. (2) Regarding contextual consistency, an entity-aware self-attention mechanism is introduced to focus more on the potential entities and semantic-correlated tokens. Finally, ConsistNER feeds the retrieved demonstrations for all target sentences into LLMs for prediction. We conduct experiments on four widely-adopted NER datasets, including both general and specific domains. Experimental results show that ConsistNER achieves a 6.01%-26.37% and 3.07%-21.18% improvement over the state-of-the-art baselines on Micro-F1 scores under 1- and 5-shot settings, respectively.",
    "full_text": "ConsistNER: Towards Instructive NER Demonstrations for LLMs with the\nConsistency of Ontology and Context\nChenxiao Wu1*, Wenjun Ke1, 3*†, Peng Wang1, 2, 3† , Zhizhao Luo4, Guozheng Li1, Wanyi Chen2\n1School of Computer Science and Engineering, 2School of Cyber Science and Engineering, Southeast University\n3Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast\nUniversity), Ministry of Education, China\n4Beijing Institute of Computer Technology and Application\nyanzu0311@gmail.com, {kewenjun, pwang, liguozheng, wanyichen}@seu.edu.cn, qibai-aluminum@outlook.com\nAbstract\nNamed entity recognition (NER) aims to identify and clas-\nsify specific entities mentioned in textual sentences. Most ex-\nisting superior NER models employ the standard fully su-\npervised paradigm, which requires a large amount of anno-\ntated data during training. In order to maintain performance\nwith insufficient annotation resources (i.e. , low resources),\nin-context learning (ICL) has drawn a lot of attention, due\nto its plug-and-play nature compared to other methods (e.g. ,\nmeta-learning and prompt learning). In this manner, how to\nretrieve high-correlated demonstrations for target sentences\nserves as the key to emerging ICL ability. For the NER task,\nthe correlation implies the consistency of both ontology (i.e.,\ngeneralized entity type) and context (i.e. , sentence seman-\ntic), which is ignored by previous NER demonstration re-\ntrieval techniques. To address this issue, we propose Con-\nsistNER, a novel three-stage framework that incorporates on-\ntological and contextual information for low-resource NER.\nFirstly, ConsistNER employs large language models (LLMs)\nto pre-recognize potential entities in a zero-shot manner. Sec-\nondly, ConsistNER retrieves the sentence-specific demonstra-\ntions for each target sentence based on the two following con-\nsiderations: (1) Regarding ontological consistency, demon-\nstrations are filtered into a candidate set based on ontology\ndistribution. (2) Regarding contextual consistency, an entity-\naware self-attention mechanism is introduced to focus more\non the potential entities and semantic-correlated tokens. Fi-\nnally, ConsistNER feeds the retrieved demonstrations for all\ntarget sentences into LLMs for prediction. We conduct exper-\niments on four widely-adopted NER datasets, including both\ngeneral and specific domains. Experimental results show that\nConsistNER achieves a 6.01%-26.37% and 3.07%-21.18%\nimprovement over the state-of-the-art baselines on Micro-F1\nscores under 1- and 5-shot settings, respectively.\nIntroduction\nNER is a fundamental natural language processing (NLP)\ntask for various downstream tasks such as entity linking\n(Sevgili and Shelmanov 2020), event extraction (Xiang\nand Wang 2019), and Q&A (Kolomiyets and Moens 2011).\nExisting NER methods with pre-trained language models\n*These authors contributed equally.\n†Corresponding authors.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(PLMs) have achieved outstanding performance by employ-\ning the standard fully supervised paradigm (Peters et al.\n2017; Souza, Nogueira, and Lotufo 2019). However, such\na supervised paradigm heavily depends on large-scale anno-\ntated data. Hence, in real-world scenarios, existing methods\ntend to struggle when recognizing new entities with insuffi-\ncient annotation resources (i.e., low resources).\nSeveral types of methods have been proposed to allevi-\nate the challenge of low-resource NER, varying in meta-\nlearning (Wu et al. 2020; de Lichy, Glaude, and Campbell\n2021; Ma et al. 2022b), prompt learning (Ma et al. 2022a;\nLiu et al. 2022; Chen et al. 2022) and in-context learn-\ning (Brown et al. 2020; Smith et al. 2022; Du et al. 2022).\nAmong these, in-context learning (ICL), which concate-\nnates a query and few-shot demonstrations to prompt LLMs\nfor prediction, is plug-and-play and does not require addi-\ntional inductive bias learning or sophisticated template de-\nsign. Generally, the primary research of ICL can be grouped\ninto two directions: query forms (Wang et al. 2023b; Mishra\net al. 2022; Wang et al. 2022; Wei et al. 2023; Wang et al.\n2023a) and demonstration retrieval techniques (Ma et al.\n2023; Jimenez Gutierrez et al. 2022; Lee et al. 2022; Wang\net al. 2023a). The variances in query forms have the potential\nto cause significant differences in model predictions. Com-\npared to directly applying the straightforward instructions\nas queries (Wang et al. 2023b; Mishra et al. 2022; Wang\net al. 2022), multi-turn Q&A according to entity types could\nimprove NER performance (Wei et al. 2023; Wang et al.\n2023a). However, queries are often manually created based\non human introspection (Petroni et al. 2019; Brown et al.\n2020; Schick and Sch¨utze 2021b,a,c), which results in query\ndesign becoming an engineering problem that requires ex-\ntensive human experience and time (Shin et al. 2021).\nOn detailed analysis, the key to emerging ICL ability\nlies in how to retrieve high-correlated demonstrations. Cur-\nrent retrieval techniques have shown certain promising re-\nsults in this regard (Ma et al. 2023; Jimenez Gutierrez et al.\n2022; Wang et al. 2023a). However, these techniques fail\nto locate the delicate demonstration considering the consis-\ntency of ontology (i.e. , generalized entity type) and con-\ntext (i.e., sentence semantic), simultaneously. (1) Regard-\ning the lack of ontological consistency, Ma et al. (2023)\nand Guti ´errez et al. (2022) use CLS embeddings of PLMs\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19234\nTarget: New Year\n(1): Null\n(2): new year\n(3): Sally\n(4): Jakarta\n(4): new year\n#1: So I wish you all the best of luck.\n#2: Now it is looking more likely to spill into the new year.\n#3: All right Sally good luck to you and your son.\n#4: Officials hope the Jakarta side of the investigation will\nresume early in the new year.\nInput \nRetrieve #1, #3\nCLS-based Method\nI thank my friends and wish everyone a Happy New Year.\nembedding space\nTarget Sentence\nTraining Set\nEncoder\n(1) \n(2) \n(3) \n(4) \nCLS NER Model\nNew .. \nRetrieve #2, #4\nembedding space\nnew .. \nJak.. \nnew .. \nSal.. \ntag\nEncoder\nEntity-based Method\nFigure 1: An illustration of different demonstration retrieval\ntechniques, where different colors denote different gener-\nalized entity types. For the CLS-based Method, examples\n#1, #3 are retrieved based on the sentence embedding (CLS)\nsimilarity. For the Entity-based Method, after extracting en-\ntities with a NER tagging model, examples #2, #4 are re-\ntrieved based on the entity (New Year) embedding similarity.\nto select semantically similar training examples as demon-\nstrations. In this manner, the retrieved demonstrations might\nlack ontological consistency with target sentences, whose\npre-given entity types still remain inconsistent after being\ngeneralized. As Figure 1 shown, considering the CLS-based\nmethod, the retrieved examples #1, #3, which only contain\na PER entity Sally, could not provide sufficient assistance\nin recognizing the EVENT entity New Year in the target\nsentence. (2) Regarding the lack of contextual consistency,\nWang et al. (2023a) suggest retrieving demonstrations based\non the entity similarity, which could not guarantee the con-\ntextual consistency between the demonstrations and target\nsentences. Considering the entity-based method in Figure 1,\nthe DATE entity new yearin examples #2, #4 conveys dif-\nferent meanings compared to the EVENT entity New Yearin\nthe target sentence. This could mislead LLMs into recogniz-\ning the New Yearin the target sentence as a DATE entity.\nTo address the above two issues, i.e. the inconsistency\nof ontology and context, we propose ConsistNER, a novel\nthree-stage framework that incorporates ontological and\ncontextual information for low-resource NER. Firstly, LLMs\nare employed in a zero-shot manner to pre-recognize po-\ntential entities, enabling the utilization of local informa-\ntion (i.e., entities and entity types) for later. Secondly, to\nretrieve the sentence-specific demonstrations for each tar-\nget sentence, we consider both ontological and contextual\nconsistency. Motivated by the bag of words (BoW) (Zhang,\nJin, and Zhou 2010), we derive the bag of ontology (BoO)\nand the ontology distribution (OD) representation to filter\ndemonstrations and construct the candidate set to main-\ntain ontological consistency. Inspired by prototypical net-\nwork (Snell, Swersky, and Zemel 2017), we design a dual\nself-attention to derive the entity-aware contextual (EAC)\nrepresentation, which pays more attention to the potential\nentities and semantic-correlated tokens, to maintain contex-\ntual consistency. Finally, after all target sentences have re-\ntrieved their own sentence-specific demonstrations, Consist-\nNER feeds dataset-specific demonstrations, which are se-\nlected based on the occurrences, into LLMs to solve NER.\nExperimental results on four benchmark datasets from\nCoNLL2003, OntoNotes5.0, NCBI and BC5CDR demon-\nstrate the superior performance of ConsistNER over state-\nof-the-art low-resource NER models, with improvements on\nF1 scores of 6.01%, 26.37%, 19.56% and 21.44% on the\nfour datasets, respectively. The effectiveness of maintaining\nontological and contextual consistency between demonstra-\ntions and target sentences is also verified. Additionally, we\nanalyze the theoretical boundary of ConsistNER, which re-\nveals the importance of pre-recognition quality. To sum up,\nour major contributions are three-fold:\n• We propose ConsistNER, a framework devoted to re-\ntrieving high-correlated demonstrations, in order to\ntackle the low-resource NER task with LLMs.\n• We devise a novel sample correlation measure mecha-\nnism that considers both entity type ontology and con-\ntextual semantic, specifically designed for the NER task.\n• Extensive experiments demonstrate the superiority of\nConsistNER over state-of-the-art baselines. Further anal-\nysis verify effectiveness of ConsistNER in maintaining\nontological and contextual consistency.\nMethod\nWe formulate the low-resource NER in the typical N-way-\nK-shot form. Given the training set D and the target sen-\ntence set T (both unlabeled), for N entity types, Consist-\nNER first retrieves and annotatesK examples for each entity\ntype from D to form the support set S = {(xi, yi)}N×K\ni=1 ,\nwhere xi = {xi\n1, xi\n2, ..., xi\nm} is an m-token text and yi =\n{(ei\nj, yi\nj)}l\nj=1, y ∈ Y is a list of l tuples, where e and\ny denote contained entity and its entity type, and Y is a\npre-defined entity type set. ConsistNER then feeds S into\nLLMs for prediction and outputs a list of l′ recognized tu-\nples {(ej, yj)}l′\nj=1 for each target sentence in T .\nThe overall architecture of ConsistNER is shown in Fig-\nure 2. In the first stage, ConsistNER pre-recognizes all texts\nfrom D and T in a zero-shot manner to extract local in-\nformation (i.e., entities and entity types), obtaining the pre-\nrecognized training set ˆD and target sentence set ˆT . In the\nsecond stage, for the target #i from ˆT , ConsistNER first fil-\nters demonstrations from ˆD and constructs the candidate set\nCi for it based on the OD representation. Subsequently, we\nbuild a dual (target- and train-level) attention mechanism to\nderive the EAC representation and retrieve target #i-specific\ndemonstrations ˆCi from Ci. In the third stage, we select the\ndataset-specific demonstrations ˆS from { ˆCi}| ˆT |\ni=1 based on the\noccurrences, which are then annotated asS to prompt LLMs\nto recognize entities in T .\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19235\nStage #2: Sentence-specific Demonstration Retrieval\ntarget #1-specific\ndemonstrations \nStage #1: Pre-recognition\nInstruction: \nIdentify the entities expressed by each\nsentence, then locate each entity to words\nin sentence. The possible entity types are\n[GPE, EVENT, DATE, ...]\nPre-recognized Training Set \n:\n#1: So I wish you all the best of luck.\n#2: Now it is looking more likely to spill\ninto the new year.\n#3: May you meet with good fortune and\nprosperity in the New Year! \n......\n#2, #3 ...\n...... \nPre-recognized Target Sentence Set :\n#1: I thank my London friends and wish\neveryone a Happy New Year.\n......\n=[1, 0, 2, ...]\nStep #1: Ontology-based Demonstration Filtering\n=[0.077, 0.000, 0.154, ...]\n=[0, 0, 0, ...]\n=[0, 0, 2, ...]\n=[0, 0, 2, ...]\n......\n=[0.000, 0.000, 0.000, ...]\n=[0.000, 0.000, 0.154, ...]\n=[0.000, 0.000, 0.154, ...]\n......\nOverlap?\n Candidate Set \nStep #2: Context-based Demonstration Retrieval \nEncoderEncoder Prototype ... \n......      ......\nTarget-level Attention\n...\n...\n...\n...\n...GPE\nDATE ...\nI thank my London New Year .\n...\nInstruction: \nIdentify the entities expressed by each\nsentence, then locate each entity to\nwords in sentence. The possible entity\ntypes are [GPE, EVENT, DATE, ...]\nSupport Set \n:\nMay you meet with good fortune and\nprosperity in the New Year!\n[(New Year, EVENT)]\n... ...\nInput: I thank my London friends and\nwish everyone a Happy New Year.\n#3, ...Replace\n[(London, GPE), (New Year, EVENT)]\nSimilarity\nTrain-level Attention\nStage #3: Execution\ndataset-specific\ndemonstrations \n#3: 76 times\n......\nRank\nAnnotate\n, \ntarget #1\n, , , ... \nGeneralize\nFigure 2: Overview of ConsistNER w.r.t. the target sentence I thank my London friends and wish everyone a Happy New Year.\nPre-recognition\nPrevious work viewed NER as a token-level classification\ntask, which emphasized the significance of local informa-\ntion (Yan et al. 2021). Therefore, a high-correlated NER\ndemonstration should be consistent with the target sentence\nat the token level. Specifically, for NER, the most crucial lo-\ncal information is entities and entity types. To enable the\nutilization of such information in subsequent stages, we\nadopt LLMs in a zero-shot manner (i.e., without demonstra-\ntions) (Kojima et al. 2022) to identify potential entities in D\nand T to obtain the pre-recognized ˆD and ˆT . For example,\nas shown in Stage #1 in Figure 2, the pre-recognition results\nof target #1 in ˆT are London(GPE) and New Year(DATE).\nBut in fact, New Yearis an EVENT entity representing a tra-\nditional festival rather than a DATE entity.\nSentence-specific Demonstration Retrieval\nWith ˆD and ˆT from Stage #1, we can retrieve sentence-\nspecific demonstrations ˆCi from ˆD for target #i in ˆT con-\nsidering both ontological and contextual consistency.\nOntology-based Demonstration Filtering Since entity\ntypes grouped under the same ontology are often corre-\nlated (Roche 2003), we assume that not only demonstra-\ntions with the same entity types could assist in recognition,\nbut also demonstrations sharing the same ontology could\nachieve this. Based on this consideration, we first lever-\nage a pre-defined mapping schema to generalize pre-given\nentity types into the ontology level. As shown in Step #1\nof Stage #2 in Figure 2, generalized pre-given entity types\nGPE, EVENT and DATE belong to different ontologies rep-\nresented by different colors.\nSubsequently, motivated by the bag of words (BoW)\nwhere a text is represented as the bag of its words (Zhang,\nJin, and Zhou 2010), we propose the ontology-level BoW,\nthe bag of ontology (BoO), to describe the occurrences of\neach ontology in a text. As shown in Step #1, the BoO for\ntarget #1 is [1, 0, 2, ...]. Additionally, with the text length,\nthe ontology distribution (OD) representation φ could be\nderived by computing the BoO values per unit text length,\nwhich denotes the density of each ontology in a text. The\nOD representation for target #1 is [0.077, 0.000, 0.154, ...].\nWith the OD representation, for the target #i in ˆT , we\ninclude training examples from ˆD that have overlapping on-\ntology in the demonstration candidate set Ci. As shown in\nStep #1, examples #2 and #3 are included in the target #1\ncandidate set C1. In this way, each included demonstration\ncould provide assistance in predicting the entities belonging\nto overlapped ontology. Notably, when the target sentence\ndoes not contain any ontology, we also include training ex-\namples without ontology in its demonstration candidate set.\nContext-based Demonstration Retrieval The vanilla\nsentence representation (e.g. , the CLS embeddings or the\nmean of all token embeddings (Huang et al. 2021)) treats\neach token equally, which is not appropriate for NER that\npays more attention to the potential entities and semantic-\ncorrelated tokens. Hence, we build a dual (train-level and\ntarget-level) self-attention mechanism to derive the entity-\naware contextual (EAC) representation for training exam-\nples and target sentences, respectively.\nFor train-level attention, we first employ BERT (Devlin\net al. 2019) to encode training examples. Given the pth m-\ntoken training example xp = {xp\n1, xp\n2, ..., xp\nm} ∈ˆD, BERT\nwill map all tokens into the hidden embedding representa-\ntion Hp = {hp\n1, hp\n2, ...,hp\nm}, where h ∈ Rdh is the repre-\nsentation of x and dh is its dimension.\nBased on the pre-recognition, we could derive the raw en-\ntity semantic representation by averaging the embeddings of\nthe potential entity span. Given the pth training sample xp\nwith l potential entities {(ep\n1, yp\n1), ...,(ep\nl , yp\nl )}, where ep\ni and\nyp\ni denote the ith entity and its entity type, the raw entity se-\nmantic representation hp\nei ∈ Rdh of ep\ni is as follows:\nhp\nei = 1\n|Mp\ni |\nX\nj∈Mp\ni\nhp\nj (1)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19236\nwhere Mp\ni denotes the index set of the ith entity span.\nSince each token in the context contributes differently\nwhen recognizing entities, we assume that the tokens seman-\ntically correlated to the entity could provide more assistance.\nTherefore, when computing the sentence representation, we\nnot only pay more attention to the potential entities but also\nto semantic-correlated tokens. To achieve this, raw entity se-\nmantic representation is utilized to sift out the entity-specific\ninformation through the sentence attentively (Cong et al.\n2022). The entity-specific information ˆhp\ni ∈ Rdh of ep\ni is\nas follows:\nˆhp\ni = softmax(hp\nei HpT\n√dh\n)Hp (2)\nSince a sentence may contain multiple potential entities,\nwe consider the average of the entity-specific information\nof all potential entities as the EAC representation. The EAC\nrepresentation εp ∈ Rdh of xp is as follows:\nεp = 1\nl\nlX\ni=1\nˆhp\ni (3)\nwhere l denotes the number of potential entities in xp.\nInspired by prototypical network (Snell, Swersky, and\nZemel 2017), we compute the prototype for each entity\ntype by averaging the raw entity semantic representation of\nthat type in the training set ˆD. The prototype representation\nσk ∈ Rdh of the kth entity type is as follows:\nσk = 1\n|Sk|\nX\nep\ni ∈Sk\nhp\nei (4)\nwhere Sk is the entity set of the kth entity type in ˆD.\nFor target-level attention, we also employ BERT to en-\ncode target sentences. Given theqth m-token target sentence\nxq = {xq\n1, xq\n2, ..., xq\nm} ∈ˆT , BERT maps all tokens into the\nhidden embedding representation Hq = {hq\n1, hq\n2, ...,hq\nm}.\nGiven that recognizing entity types is simpler than recog-\nnizing entities, we tend to rely more on the entity types pre-\nrecognized by LLMs rather than the entities. Hence, when\ncomputing the EAC representation of target sentences, we\nonly utilize the pre-recognized entity types.\nGiven the qth target sentence xq with l potential enti-\nties {(eq\n1, yq\n1), ...,(eq\nl , yq\nl )}, if it contains kth entity type yk,\nwe use the corresponding prototype representation σk to\ngather prototype-specific semantics through the sentence at-\ntentively. The prototype-specific information ˆhq\nk ∈ Rdh of\nσk is as follows:\nˆhq\nk = softmax(σkHqT\n√dh\n)Hq (5)\nSimilarly, we consider the average of the prototype-\nspecific information of the contained entity types as the EAC\nrepresentation. The EAC representation εq ∈ Rdh of xq is\nas follows:\nεq = 1\n|Yq|\nX\nyk∈Yq\nˆhq\nk (6)\nwhere Yq is the entity type set of xq.\nAs shown in Step #2 of Stage #2 in Figure 2, when\ncomputing the EAC representation of target #1, the pre-\nrecognized entity types GPE and DATE will pay more at-\ntention (deeper color) toLondon and New Year, respectively.\nEventually, with the OD (φ) obtained in Step #1 and EAC\n(ε) representation for each training example and target sen-\ntence, we consider the weighted sum of their respective sim-\nilarities as the similarity between the pth training example\nand the qth target sentence:\nsimp,q = λS(φp, φq) + (1− λ)S(εp, εq) (7)\nwhere S denote pre-defined similarity measures (e.g. , co-\nsine similarity), and λ is the hyperparameter weight. Ac-\ncordingly, we can retrieve demonstrations from the candi-\ndate set Ci for the target #i,. As shown in Step #2, example\n#3 has been retrieved for target #1.\nSince all above computations depend on pre-recognition,\nerrors originating from Stage #1 would propagate through-\nout the entire pipeline. As shown in Stage #1, after LLMs\nincorrectly pre-recognize New Year as a DATE entity\nrather than an EVENT entity in target #1, most retrieved\ndemonstrations will contain DATE entities. Inevitably, these\ndemonstrations would lead LLMs to predict DATE, result-\ning in inaccurate predictions. Therefore, similar to (Ma et al.\n2023; Jimenez Gutierrez et al. 2022), we select top-kseman-\ntically similar training examples to replace part of the re-\ntrieved demonstrations, serving as the final sentence-specific\ndemonstrations ˆCi.\nExecution\nAfter all target sentences have retrieved their own sentence-\nspecific demonstrations { ˆCi}| ˆT |\ni=1, we can not directly anno-\ntate them to form the support set S due to the low-resource\nsettings. Therefore, we select the ones that occur the most\nfrequently per entity type as the dataset-specific demonstra-\ntions ˆS. Subsequently, we manually annotate these demon-\nstrations to form S, which is then fed into LLMs for predic-\ntion. For each target sentence in T , ConsistNER outputs a\nlist of l′ recognized tuples {(ej, yj)}l′\nj=1. As shown in Stage\n#3 in Figure 2, ConsistNER outputs [(London, GPE), (New\nYear, EVENT)], where the recognition of New Yearin target\n#1 has been corrected to EVENT.\nExperiments\nDatasets and Experimental Settings\nFor the general domain, our experiments consider two\ndatasets. (1) CoNLL2003 (Sang and Meulder 2003) consists\nof data taken from Reuters news stories. (2) OntoNotes5.0 1\nis a large corpus comprising various genres of text, such\nas news, weblogs, etc. For the specific domain, our exper-\niments consider two datasets. (1) NCBI (Do ˘gan, Leaman,\nand Lu 2014) consists of 793 PubMed abstracts annotated\nwith disease-related mentions. (2) BC5CDR (Li et al. 2016)\nconsists of 1,500 PubMed articles annotated with chemical\n1https://catalog.ldc.upenn.edu/LDC2013T19\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19237\nModel CoNLL2003 OntoNotes5.0 NCBI BC5CDR\n1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot\nProtoBERT 49.90±8.6† 61.30±9.1† 20.30±6.9† 36.70±4.1† 17.24‡ 34.18‡ 23.61‡ 40.58‡\nNNShot 61.20±10.4† 74.10±2.3† 27.80±9.4† 50.50±4.1† 11.82‡ 16.22‡ 32.96‡ 39.30‡\nStructShot 62.40±10.5† 74.80±2.4† 27.90±9.7† 52.90±4.8† 4.63‡ 13.89‡ 16.09‡ 30.97‡\nCONTaiNER 61.20±10.7† 75.80±2.7† 32.10±9.8† 56.20±5.0† 16.51‡ 26.83‡ 37.25‡ 41.21‡\nCOPNER 67.00±3.8† 74.90±2.9† - - 15.54‡ 24.23‡ 36.30‡ 42.78‡\nVQ + Random 68.57±2.7 73.51±2.3 55.29±1.5 - 25.54±2.3 34.34±4.0 51.59±3.4 59.50±0.9\nVQ + CLS 70.66±0.2 75.49±0.1 56.87±0.3 - 27.23±0.1 41.92±0.2 54.10±0.2 60.19±0.1\nVQ + ConsistNER 73.01±0.1 78.87±0.1 58.47±0.2 - 34.85±0.2 42.73±0.2 57.98±0.2 61.20±0.1\nG-N* + Random 57.26±2.2 63.87±2.0 49.52±1.3 - 28.65±3.9 37.95±3.6 55.88±2.1 60.20±2.0\nG-N* + CLS 60.94±0.2 67.94±0.2 50.83±0.5 - 35.15±0.3 41.44±0.4 56.73±0.3 62.65±0.2\nG-N* + ConsistNER 61.10±0.3 71.83±0.2 50.95±0.7 - 36.80±0.2 43.87±0.5 58.69±0.2 63.96±0.3\nTable 1: Performance (%) of different Models on the CoNLL2003, OntoNotes5.0, NCBI and BC5CDR datasets under the 1-\nand 5-shot settings, where VQ and G-N* denote the vanilla query and GPT-NER*, respectively. The best results are in bold.\nDue to the 4096 token limit, the 5-shot setting on OntoNotes5.0 is not feasible. Results with † and ‡ are retrieved from original\npapers and (Li and Zhang 2023), respectively.\nand disease mentions. Since the entity types in CoNLL2003\nand BC5CDR are sufficiently abstract, we only formulate\nmapping schemas for OntoNotes5.0 and NCBI, which gen-\neralize entity types from the concrete to the abstract. For ex-\nample, we generalize OntoNotes5.0 entity types FAC, GPE\nand LOC into the LOC ontology.\nWe employ ChatGPT2 (gpt-3.5-turbo) as the LLM back-\nbone in our experiments. We adopt two query forms. (1)\nVanilla query (VQ) (Wang et al. 2023b; Mishra et al.\n2022; Wang et al. 2022) uses straightforward instructions to\nrecognize entities in a sentence. (2) GPT-NER (Wang et al.\n2023a) transforms the sequence labeling task into a genera-\ntion task, which uses special symbols (i.e. , @@ and ##) to\nmark entities in a sentence. However, for each input, GPT-\nNER needs to enquire n times, where n denotes the number\nof entity types, resulting in significant overhead. Hence, sim-\nilar to (Li and Zhang 2023), we use</type> and <type> to\nmark entities and only need to enquire 1 time for each input,\nreferred to asGPT-NER* (G-N*). We adopt two demonstra-\ntion retrieval techniques to compare with ConsistNER.\n(1) Random retrieval strategy indicates randomly select-\ning k examples from the training set as demonstrations. (2)\nCLS retrieval strategy indicates selecting k nearest neighbor\n(kNN) (Ma et al. 2023; Jimenez Gutierrez et al. 2022) of the\ninput from the training set as demonstrations, where the dis-\ntance is measured by the cosine similarity between the CLS\nembeddings. We use the span-level Micro-F1 score for eval-\nuation and report the mean and associated standard deviation\nover 5 runs on the full testing set.\nBaselines\nWe choose five representative low-resource NER models\nas baselines for comparison. ProtoBERT (Snell, Swersky,\nand Zemel 2017) combines prototype-based learning with\nBERT (Devlin et al. 2019). NNShot (Wiseman and Stratos\n2019) is a simple method based on token-level nearest neigh-\nbor classification. StructShot (Yang and Katiyar 2020)\nadopts an additional Viterbi decoder (Forney 1973) based\n2https://openai.com/blog/chatgpt\non NNShot. CONTaiNER (Das et al. 2022) leverages con-\ntrastive learning to infer the distributional distance of Gaus-\nsian embeddings of entities. COPNER (Huang et al. 2022)\nproposes to leverage class-specific words from natural lan-\nguage to serve as the agents of corresponding entity types.\nMain Results\nThe main results are shown in Table 1. Firstly, regard-\nless of different query forms or demonstration retrieval\ntechniques, LLMs consistently outperform other fine-tuning\nbaselines, which indicates the effectiveness of in-context\nlearning in low-resource scenarios. Specifically, under the 1-\nshot setting, ConsistNER achieves performance gains up\nto 6.01%, 26.37%, 19.56% and 21.44% over the strongest\nbaselines on the four datasets, respectively. Under the 5-\nshot setting, ConsistNER achieves performance gains up\nto 3.07%, 9.69% and 21.18% over the strongest baselines\non CoNLL2003, NCBI and BC5CDR, respectively. This\ndemonstrates the growing LLMs superiority as annotation\nresources decrease. Moreover, LLM methods show lower\nstandard deviations than fine-tuning baselines, indicating\ntheir stability with limited annotations.\nSecondly, for both VQ and G-N∗, CLS consistently out-\nperforms Random by up to 4.10%, 1.58%, 7.58% and\n2.51% on the four datasets, which shows the importance\nof contextual consistency. And greater improvements can be\nobserved in specific domains, demonstrating that LLMs lack\nthe domain-specific knowledge (e.g., disease and chemical)\nwhich can be provided by appropriate demonstrations.\nThirdly, for both VQ and G-N∗, ConsistNER consis-\ntently outperforms CLS by up to 3.89%, 1.60%, 7.62% and\n3.88% on the four datasets, which demonstrates the signif-\nicance of maintaining ontological consistency and focusing\nmore on potential entities and semantic-correlated tokens.\nThe VQ outperforms G-N* on CoNLL2003 and\nOntoNotes5.0, but performs worse on NCBI and BC5CDR.\nDespite expectations, G-N* should consistently excel due\nto better query form design. However, VQ tends to over-\nconfidently label Null tokens as entities. This results in the\nVQ over-recognizing entities and performing poorly on the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19238\n1   2   4   5   6   8   10K-shot\nF1(%)\n(a) Performance gains with\nmore demonstrations for Set-\nS and Sen-S on CoNLL2003.\n1   2   4   5   6   8   10K-shot\nF1(%)\n(b) Performance comparison\n(ontology-based v.s. entity\ntype-based) on NCBI.\n  0   20   40  60  80  100 (%) \nF1(%)\nProportions of Replacement\n(c) The impact of different\nproportions of replacement on\nCoNLL2003.\n  0   20   40  60  80  100 (%) \nF1(%)\nProportions of Replacement\n(d) The impact of different\nproportions of replacement on\nOntoNotes5.0.\nFigure 3: Results of ablation experiments.\nModel CoNLL OntoNotes NCBI BC5CDR\nConsistNER 73.01 58.47 34.85 57.98\nw/o OC 71.12 57.75 32.97 57.27\nw/o OC & Attn. 70.66 56.87 27.23 54.10\nw/o OC & CC 68.57 55.29 25.54 51.59\nw/o Repl. 71.93 57.13 33.06 55.15\nTable 2: Results (%) of ablation experiments under the 1-\nshot setting, where OC, Attn., CC and Repl. denote onto-\nlogical consistency, attention mechanism, contextual consis-\ntency and demonstration replacement.\nNCBI and BC5CDR which contain fewer entities in a sen-\ntence. Unlike G-N*, which omits numerous entities, it per-\nforms poorly on CoNLL2003 and OntoNotes5.0 due to their\nhigh entity density.\nAblation Experiments\nEffect of Different Demonstration Numbers We con-\nduct experiments on CoNLL2003 to explore the effect of\ndifferent demonstration numbers. As shown in Figure 3a,\nSet-S and Sen-S represent dataset-specific and sentence-\nspecific demonstrations, respectively. We observe that as\nthe number of demonstrations increases, the performance of\nboth Set-S and Sen-S is on an upward trend, which is due\nto the additional information provided by more demonstra-\ntions. Notably, after the 5-shot, the Sen-S performance is\nconverged, while the Set-S performance continues to rise.\nThis phenomenon owes to the fact that the current constraint\non Set-S performance is no longer insufficient demonstra-\ntions, but rather errors within the pre-recognition. These er-\nrors result in continually selecting inconsistent demonstra-\ntions, thereby preventing further performance improvement.\nAbout Ontological Consistency To verify the effective-\nness of maintaining ontological consistency, we remove Step\n#1 of Stage #2 and set λ in Equation 7 to 0. As shown\nin Table 2, comparing ConsistNER and w/o OC, we\nobserve the performance degradation by up to 1.89% on\nCoNLL2003, which shows the significance of maintaining\nthe consistency of ontology. As mentioned before, we se-\nlect candidate demonstrations based on ontology overlap.\nIntuitively, using entity type overlap as the criterion seems\nmore reasonable. As shown in Figure 3b, although using\nentity type-based criterion has advantages when demon-\nstrations are limited, as the number of demonstrations in-\ncreases, the ontology-based criterion gradually surpasses it.\nThis is because the former criterion overly emphasizes low-\nlevel entity type consistency, thus losing the diversity of re-\ntrieved demonstrations or even omitting maintaining con-\ntextual consistency. Besides, LLMs often struggle to distin-\nguish entity types belonging to the same ontology. There-\nfore, demonstrations with overlapping ontology can enhance\nLLMs’ understanding of each entity type under the same on-\ntology and thus reducing incorrect recognition.\nAbout Contextual Consistency To verify the effective-\nness of maintaining contextual consistency, we remove the\nentire Stage #2. As shown in Table 2, comparing w/o OC\nand w/o CC & OC, we observe the performance degrada-\ntion by up to 7.43% on NCBI, which shows the significance\nof maintaining the consistency of context. Meanwhile, to\nverify the effectiveness of the self-attention mechanism, we\nremove Step #1 of Stage #2 and the self-attention mecha-\nnism. As shown in Table 2, comparing w/o OC and w/o\nOC & Attn., we observe that even without maintaining\nontological consistency, the entity-aware self-attention still\nimproves performance by up to 5.74% on NCBI, which\nshows importance of paying more attention to the potential\nentities and semantic-correlated tokens for NER.\nAbout Demonstration Replacement To verify the effec-\ntiveness of demonstration replacement, we remove the re-\nplacement operation in Stage #2. As shown in Table 2,\ncomparing ConsistNER and w/o Repl., we observe\nthe performance degradation by up to 2.83% on BC5CDR,\nwhich shows the significance of the replacement. We also\nstudy the impact of different proportions of replacement. As\nshown in Figure 3c and 3d, we observe that for CoNLL2003\nand OntoNotes5.0, the suitable replacement proportion is\naround 60%. A proportion that is too low would hinder the\nmitigation of error propagation, while too high would limit\nthe effectiveness of the self-attention mechanism.\nAnalysis Experiments\nSet-S and Sen-S To investigate the performance influ-\nenced by Set-S and Sen-S, we conduct experiments\nto compare their performance. As shown in Table 3,\nSet-S consistently underperforms Sen-S, with perfor-\nmance degradation as high as 14.54%, 10.67%, 12.26%, and\n9.22% on four datasets, respectively. This shows that each\ntarget sentence has its own high-correlated demonstrations\nthat maintain ontological and contextual consistency, while\nSet-S needs to balance all target sentences, unavoidably\nleading to a decrease in performance.\nTheoretical Boundary As mentioned earlier, the pre-\nrecognition quality could affect demonstration retrieval. In\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19239\nModel CoNLL2003 OntoNotes5.0 NCBI BC5CDR\nSet-S Sen-S Set-S Sen-S Set-S Sen-S Set-S Sen-S\nVQ + CLS 70.66±0.2 80.14±0.1 56.87±0.3 59.97±0.3 27.23±0.1 34.07±0.2 54.10±0.2 59.12±0.2\nVQ + ConsistNER 73.01±0.1 82.22±0.1 58.47±0.2 64.21±0.4 34.85±0.2 38.21±0.1 57.98±0.2 62.48±0.1\nG-N* + CLS 60.94±0.2 70.80±0.0 50.83±0.5 53.40±0.3 35.15±0.3 40.77±0.1 56.73±0.3 59.72±0.1\nG-N* + ConsistNER 61.10±0.3 75.64±0.1 50.95±0.7 61.62±0.6 36.80±0.2 49.06±0.1 58.69±0.2 67.91±0.2\nTable 3: Performance (%) comparison between Set-S and Sen-S under the 1-shot setting, where Set-S and Sen-S denote set-\nspecific and sentence-specific demonstrations, respectively. The rest abbreviations herein are the same as in Table 1.\nModel CoNLL2003 OntoNotes5.0 NCBI BC5CDR\n1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot\nVQ + ER 73.01 78.87 58.47 - 34.85 42.73 57.98 61.20\nVQ + TB 74.80(+1.79) 81.00(+2.13) 64.11(+5.64) - 38.80(+3.95) 46.09(+3.36) 61.41(+3.43) 64.70(+3.50)\nG-N* + ER 61.10 71.83 50.95 - 36.80 43.87 58.69 63.96\nG-N* + TB 63.11(+2.01) 73.86(+2.03) 55.81(+4.86) - 42.29(+5.49) 49.67(+5.80) 62.44(+3.75) 67.06(+3.10)\nTable 4: Performance (%) comparison between ER and TB under the 1- and 5-shot settings, where ER and TB denote empirical\nresults and theoretical boundaries of ConsistNER, respectively. The rest abbreviations herein are the same as in Table 1.\nlight of this, we replace the pre-recognition results with\nthe ground truth, enabling the retrieved demonstrations to\nmaximally emerge the ICL capabilities of LLMs, leading\nto the achievement of the theoretical performance bound-\naries. As shown in Table 4, we can observe that, regard-\nless of datasets, number of demonstrations or query forms,\nthe theoretical boundaries (TB) of ConsistNER are con-\nsistently 1.79%-5.64% above the empirical results (ER) of\nConsistNER. Specifically, we notice that the performance\nimprovement is more significant on OntoNotes5.0 compared\nto other datasets. This is because OntoNotes5.0 contains\nmore entity types, thereby posing more challenges for LLMs\nto recognize entities. Accordingly, the pre-recognition on\nOntoNotes5.0 will include more errors, leading to more\ninappropriate demonstrations being retrieved to misguide\nLLMs. Thus, with the improved pre-recognition quality,\nthere is a substantial performance boost on OntoNotes5.0.\nGiven the TB, we could approximate it by improving pre-\nrecognition quality. For example, we could use pre-built lex-\nicons (Chiu and Nichols 2016; Collobert et al. 2011) or\ngazetteers (Liu, Yao, and Lin 2019) in pre-recognition to re-\ntrieve more instructive demonstrations.\nAdvantages of Ontology The major innovative advan-\ntages of ontology are in two aspects: generalization in\nconcept hierarchy and generalization in diverse tasks. (1)\nGeneralization in concept hierarchy.During pre-training,\nLLMs primarily encounter general knowledge while lack-\ning domain-specific knowledge, which leads to their inabil-\nity to differentiate excessively fine-grained entity types. In\nthis case, pre-recognition which outputs entity types has a\nlow accuracy. Inspired by the concept hierarchy of ontology\nin knowledge graphs, we generalize entity types upwards\nto ontology (e.g., Biden: president→official→person) and\nrequire pre-recognition only to output ontology. Thus pre-\nrecognition is aligned with the pre-training tasks of LLMs\nand provides more solid prior knowledge for later stages.\n(2) Generalization in diverse tasks.When dealing with other\ninformation extraction tasks like relation extraction, proper-\nties and property hierarchy of ontology (e.g., relations and\ngeneralized relations) can also be useful. This open-ended\nquestion remains to be further explored in future research.\nA Chaos Phenomenon Furthermore, we have discovered\na chaos phenomenon, where erroneously pre-recognized\ntraining examples could still assist LLMs. For example, after\nboth Orlando from the target sentence LA LAKERS 92 Or-\nlando 81and MINNESOTA from the training example Texas\n13 MINNESOTA 2have been erroneously pre-recognized as\nLOC entities, this training example will be retrieved as a\ndemonstration for its both ontological and contextual con-\nsistency with the target sentence. However, the labels in\ndemonstrations are correct, i.e., MINNESOTA as an ORG,\nwhich could correct the misrecognition of Orlando. The su-\nperficial reason behind this phenomenon is that LLMs can\npre-recognize Orlando and MINNESOTA as the same type\nof entity, but they cannot distinguish whether they are LOC\nor ORG entities. Essentially, for pre-recognition, Consist-\nNER aims to identify the fine-grained text information used\nto retrieve high-correlated demonstrations, rather than sim-\nply type entities. Therefore, the accuracy of pre-recognition\nis not crucial, as long as it labels entities of the same type\nwith the same label. Thus this chaos phenomenon reveals\nthe robustness of ConsistNER.\nConclusion\nIn this paper, we argue that the key to emerging ICL abil-\nity lies in how to retrieve high-correlated demonstrations,\nwhere correlation implies the consistency of both ontology\nand context for NER. Based on this motivation, we propose\na three-stage framework, ConsistNER, to incorporate onto-\nlogical and contextual information for low-resource NER. In\nthis way, the retrieved demonstrations could maintain both\nontological and contextual consistency. We conduct experi-\nments on both general and specific domains, which demon-\nstrates that ConsistNER not only effectively instructs LLMs,\nbut also boosts NER with state-of-the-art performance.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19240\nAcknowledgments\nWe thank the anonymous reviewers for their insightful com-\nments. This work was supported by National Science Foun-\ndation of China (Grant Nos.62376057) and the Start-up Re-\nsearch Fund of Southeast University (RF1028623234). All\nopinions are of the authors and do not reflect the view of\nsponsors.\nReferences\nBrown, T.; Mann, B.; Ryder, N.; et al. 2020. Language Mod-\nels Are Few-shot Learners. In NeurIPS.\nChen, X.; Li, L.; Deng, S.; et al. 2022. LightNER: A\nLightweight Tuning Paradigm for Low-resource NER via\nPluggable Prompting. In COLING.\nChiu, J. P.; and Nichols, E. 2016. Named Entity Recognition\nwith Bidirectional LSTM-CNNs. In TACL.\nCollobert, R.; Weston, J.; Bottou, L.; et al. 2011. Nat-\nural Language Processing (almost) from Scratch. JMLR,\n12(2011): 2493–2537.\nCong, X.; Sheng, J.; Cui, S.; et al. 2022. Relation-guided\nFew-shot Relational Triple Extraction. In SIGIR.\nDas, S. S. S.; Katiyar, A.; Passonneau, R. J.; and Zhang, R.\n2022. CONTaiNER: Few-shot Named Entity Recognition\nvia Contrastive Learning. In ACL.\nde Lichy, C.; Glaude, H.; and Campbell, W. 2021. Meta-\nlearning for Few-shot Named Entity Recognition. In\nMetaNLP.\nDevlin, J.; Chang, M.-W.; Lee, K.; et al. 2019. BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language\nUnderstanding. In NAACL.\nDo˘gan, R. I.; Leaman, R.; and Lu, Z. 2014. NCBI disease\ncorpus: a resource for disease name recognition and concept\nnormalization. Journal of biomedical informatics, 47: 1–10.\nDu, N.; Huang, Y .; Dai, A. M.; et al. 2022. Glam: Efficient\nScaling of Language Models with Mixture-of-experts. In\nICML.\nForney, G. D. 1973. The Viterbi Algorithm. Proceedings of\nIEEE, 61(3): 268–278.\nHuang, J.; Tang, D.; Zhong, W.; et al. 2021. Whitening-\nBERT: An Easy Unsupervised Sentence Embedding Ap-\nproach. In Findings of EMNLP.\nHuang, Y .; He, K.; Wang, Y .; et al. 2022. Copner: Con-\ntrastive Learning with Prompt Guiding for Few-shot Named\nEntity Recognition. In COLING.\nJimenez Gutierrez, B.; McNeal, N.; Washington, C.; Chen,\nY .; Li, L.; Sun, H.; and Su, Y . 2022. Thinking about GPT-\n3 In-Context Learning for Biomedical IE? Think Again. In\nFindings of EMNLP.\nKojima, T.; Gu, S. S.; Reid, M.; et al. 2022. Large Language\nModels Are Zero-shot Reasoners. In NeurIPS.\nKolomiyets, O.; and Moens, M.-F. 2011. A Survey on Ques-\ntion Answering Technology from an Information Retrieval\nPerspective. Information Sciences, 181(24): 5412–5434.\nLee, D.-H.; Kadakia, A.; Tan, K.; et al. 2022. Good Exam-\nples Make A Faster Learner: Simple Demonstration-based\nLearning for Low-resource NER. In ACL.\nLi, J.; Sun, Y .; Johnson, R. J.; Sciaky, D.; Wei, C.-H.; Lea-\nman, R.; Davis, A. P.; Mattingly, C. J.; Wiegers, T. C.; and\nLu, Z. 2016. BioCreative V CDR task corpus: a resource for\nchemical disease relation extraction. Database, 2016.\nLi, M.; and Zhang, R. 2023. How far is Language Model\nfrom 100% Few-shot Named Entity Recognition in Medical\nDomain. arXiv preprint arXiv:2307.00186.\nLiu, A. T.; Xiao, W.; Zhu, H.; Zhang, D.; Li, S.-W.; and\nArnold, A. 2022. QaNER: Prompting question answer-\ning models for few-shot named entity recognition. arXiv\npreprint arXiv:2203.01543.\nLiu, T.; Yao, J.-G.; and Lin, C.-Y . 2019. Towards Improving\nNeural Named Entity Recognition with Gazetteers. In ACL.\nMa, R.; Zhou, X.; Gui, T.; et al. 2022a. Template-free\nPrompt Tuning for Few-shot NER. In NAACL.\nMa, T.; Jiang, H.; Wu, Q.; et al. 2022b. Decomposed Meta-\nlearning for Few-shot Named Entity Recognition. In Find-\nings of ACL.\nMa, Y .; Cao, Y .; Hong, Y .; and Sun, A. 2023. Large lan-\nguage model is not a good few-shot information extrac-\ntor, but a good reranker for hard samples! arXiv preprint\narXiv:2303.08559.\nMishra, S.; Khashabi, D.; Baral, C.; et al. 2022. Cross-\nTask Generalization via Natural Language Crowdsourcing\nInstructions. In ACL.\nPeters, M. E.; Ammar, W.; Bhagavatula, C.; and other. 2017.\nSemi-supervised Sequence Tagging with Bidirectional Lan-\nguage Models. In ACL.\nPetroni, F.; Rockt¨aschel, T.; Lewis, P.; et al. 2019. Language\nModels as Knowledge Bases? In EMNLP.\nRoche, C. 2003. Ontology: A Survey. IFAC Proceedings\nVolumes, 36(22): 187–192.\nSang, E. F. T. K.; and Meulder, F. D. 2003. Introduction\nto the CoNLL-2003 Shared Task: Language-independent\nNamed Entity Recognition. In NAACL.\nSchick, T.; and Sch¨utze, H. 2021a. Exploiting Cloze Ques-\ntions for Few Shot Text Classification and Natural Language\nInference. In EACL.\nSchick, T.; and Sch ¨utze, H. 2021b. Few-shot Text Genera-\ntion with Pattern-exploiting Training. In EMNLP.\nSchick, T.; and Sch ¨utze, H. 2021c. It’s Not Just Size That\nMatters: Small Language Models Are Also Few-Shot Learn-\ners. In NAACL.\nSevgili, O.; and Shelmanov, A. 2020. Neural Entity Linking:\nA Survey of Models Based on Deep Learning. Semantic\nWeb, 13(3).\nShin, R.; Lin, C. H.; Thomson, S.; et al. 2021. Constrained\nLanguage Models Yield Few-Shot Semantic Parsers. In\nEMNLP.\nSmith, S.; Patwary, M.; Norick, B.; LeGresley, P.; Rajbhan-\ndari, S.; Casper, J.; Liu, Z.; Prabhumoye, S.; Zerveas, G.;\nKorthikanti, V .; et al. 2022. Using deepspeed and megatron\nto train megatron-turing nlg 530b, a large-scale generative\nlanguage model. arXiv preprint arXiv:2201.11990.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19241\nSnell, J.; Swersky, K.; and Zemel, R. 2017. Prototypical\nNetworks for Few-shot Learning. In NeurIPS.\nSouza, F.; Nogueira, R.; and Lotufo, R. 2019. Portuguese\nnamed entity recognition using BERT-CRF. arXiv preprint\narXiv:1909.10649.\nWang, S.; Sun, X.; Li, X.; Ouyang, R.; Wu, F.; Zhang,\nT.; Li, J.; and Wang, G. 2023a. Gpt-ner: Named en-\ntity recognition via large language models. arXiv preprint\narXiv:2304.10428.\nWang, X.; Zhou, W.; Zu, C.; Xia, H.; Chen, T.; Zhang, Y .;\nZheng, R.; Ye, J.; Zhang, Q.; Gui, T.; et al. 2023b. Instruc-\ntUIE: Multi-task Instruction Tuning for Unified Information\nExtraction. arXiv preprint arXiv:2304.08085.\nWang, Y .; Mishra, S.; Alipoormolabashi, P.; et al. 2022.\nSuper-NaturalInstructions: Generalization via Declarative\nInstructions on 1600+ NLP Tasks. In EMNLP.\nWei, X.; Cui, X.; Cheng, N.; Wang, X.; Zhang, X.; Huang,\nS.; Xie, P.; Xu, J.; Chen, Y .; Zhang, M.; et al. 2023. Zero-\nshot information extraction via chatting with chatgpt. arXiv\npreprint arXiv:2302.10205.\nWiseman, S.; and Stratos, K. 2019. Label-agnostic Sequence\nLabeling by Copying Nearest Neighbors. In ACL.\nWu, Q.; Lin, Z.; Wang, G.; et al. 2020. Enhanced Meta-\nlearning for Cross-lingual Named Entity Recognition with\nMinimal Resources. In AAAI.\nXiang, W.; and Wang, B. 2019. A Survey of Event Extrac-\ntion from Text. IEEE Access, 7: 173111–173137.\nYan, H.; Gui, T.; Dai, J.; et al. 2021. A Unified Generative\nFramework for Various NER Subtasks. In ACL.\nYang, Y .; and Katiyar, A. 2020. Simple and Effective Few-\nshot Named Entity Recognition with Structured Nearest\nNeighbor Learning. In EMNLP.\nZhang, Y .; Jin, R.; and Zhou, Z.-H. 2010. Understanding\nBag-of-words Model: A Statistical Framework. JMLC, 1:\n43–52.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19242"
}