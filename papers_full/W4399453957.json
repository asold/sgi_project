{
  "title": "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models",
  "url": "https://openalex.org/W4399453957",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2167628657",
      "name": "Zejun Zhang",
      "affiliations": [
        "Data61",
        "Australian National University"
      ]
    },
    {
      "id": "https://openalex.org/A2779303333",
      "name": "Zhenchang Xing",
      "affiliations": [
        "Data61",
        "Australian National University"
      ]
    },
    {
      "id": "https://openalex.org/A2106238987",
      "name": "Xiao-Xue Ren",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A1971717091",
      "name": "Qinghua Lu",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    },
    {
      "id": "https://openalex.org/A2104907697",
      "name": "XiWei Xu",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2898514735",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4399557965",
    "https://openalex.org/W3217531701",
    "https://openalex.org/W4391558516",
    "https://openalex.org/W4308641648",
    "https://openalex.org/W4313563756",
    "https://openalex.org/W4284676027",
    "https://openalex.org/W4384304865",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4384345667",
    "https://openalex.org/W4388483649",
    "https://openalex.org/W2605547445",
    "https://openalex.org/W4386005320",
    "https://openalex.org/W2795027827",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W3203321135",
    "https://openalex.org/W4308641593",
    "https://openalex.org/W4384345655",
    "https://openalex.org/W4384009715",
    "https://openalex.org/W4394769416",
    "https://openalex.org/W4252386722",
    "https://openalex.org/W2994647549",
    "https://openalex.org/W4312927344"
  ],
  "abstract": "Pythonic idioms are highly valued and widely used in the Python programming community. However, many Python users find it challenging to use Pythonic idioms. Adopting rule-based approach or LLM-only approach is not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong detection and wrong refactoring. Motivated by the determinism of rules and adaptability of LLMs, we propose a hybrid approach consisting of three modules. We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks. The ARIs are Python code generated by prompting LLMs to generate code. We first construct a knowledge module with three elements including ASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation into an ARI library for subsequent use. After that, for any syntax-error-free Python code, we invoke ARIs from the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does not meet the condition. Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code. Next, we conduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic idioms in RIdiom. Our approach exhibits superior accuracy, F1 score, and recall, while maintaining precision levels comparable to RIdiom, all of which consistently exceed or come close to 90% for each metric of each idiom. Lastly, we extend our evaluation to encompass four new Pythonic idioms. Our approach consistently outperforms Prompt-LLM, achieving metrics with values consistently exceeding 90% for accuracy, F1-score, precision, and recall.",
  "full_text": "Refactoring to Pythonic Idioms: A Hybrid Knowledge-Driven\nApproach Leveraging Large Language Models\nZEJUN ZHANG, Australian National University, Australia and CSIRO’s Data61, Australia\nZHENCHANG XING, CSIRO’s Data61, Australia and Australian National University, Australia\nXIAOXUE REN∗, Zhejiang University, China\nQINGHUA LU, CSIRO’s Data61, Australia\nXIWEI XU, CSIRO’s Data61, Australia\nPythonic idioms are highly valued and widely used in the Python programming community. However, many\nPython users find it challenging to use Pythonic idioms. Adopting rule-based approach or LLM-only approach\nis not sufficient to overcome three persistent challenges of code idiomatization including code miss, wrong\ndetection and wrong refactoring. Motivated by the determinism of rules and adaptability of LLMs, we propose\na hybrid approach consisting of three modules. We not only write prompts to instruct LLMs to complete tasks,\nbut we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks. The ARIs are Python code generated\nby prompting LLMs to generate code. We first construct a knowledge module with three elements including\nASTscenario, ASTcomponent and Condition, and prompt LLMs to generate Python code for incorporation\ninto an ARI library for subsequent use. After that, for any syntax-error-free Python code, we invoke ARIs\nfrom the ARI library to extract ASTcomponent from the ASTscenario, and then filter out ASTcomponent that\ndoes not meet the condition. Finally, we design prompts to instruct LLMs to abstract and idiomatize code,\nand then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code. Next, we\nconduct a comprehensive evaluation of our approach, RIdiom, and Prompt-LLM on nine established Pythonic\nidioms in RIdiom. Our approach exhibits superior accuracy, F1-score, and recall, while maintaining precision\nlevels comparable to RIdiom, all of which consistently exceed or come close to 90% for each metric of each\nidiom. Lastly, we extend our evaluation to encompass four new Pythonic idioms. Our approach consistently\noutperforms Prompt-LLM, achieving metrics with values consistently exceeding 90% for accuracy, F1-score,\nprecision, and recall.\nCCS Concepts: • Software and its engineering →Software evolution.\nAdditional Key Words and Phrases: Pythonic Idioms, Large Language Model, Code Change\nACM Reference Format:\nZejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu. 2024. Refactoring to Pythonic Idioms:\nA Hybrid Knowledge-Driven Approach Leveraging Large Language Models. Proc. ACM Softw. Eng. 1, FSE,\nArticle 50 (July 2024), 22 pages. https://doi.org/10.1145/3643776\n∗Corresponding author.\nAuthors’ addresses: Zejun Zhang, Australian National University, Canberra, Australia and CSIRO’s Data61, Canberra,\nAustralia, zejun.zhang@anu.edu.au; Zhenchang Xing, CSIRO’s Data61, Canberra, Australia and Australian National\nUniversity, Canberra, Australia, zhenchang.xing@data61.csiro.au; Xiaoxue Ren, Zhejiang University, Hangzhou, China,\nxxren@zju.edu.cn; Qinghua Lu, CSIRO’s Data61, Sydney, Australia, qinghua.lu@data61.csiro.au; Xiwei Xu, CSIRO’s Data61,\nSydney, Australia, xiwei.xu@data61.csiro.au.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\ncontact the owner/author(s).\n© 2024 Copyright held by the owner/author(s).\nACM 2994-970X/2024/7-ART50\nhttps://doi.org/10.1145/3643776\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\narXiv:2406.03660v1  [cs.SE]  6 Jun 2024\n50:2 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\n1 INTRODUCTION\nPythonic idioms refer to programming practices and coding conventions that align with the core\nphilosophy and style of the Python programming language [6, 17, 40, 51]. RIdiom [51, 53] identified\nnine Pythonic idioms by comparing syntax differences between Python and Java. Farooq et al. [17]\nconducted a literature review to identify and explore the usage of twenty-seven Pythonic idioms.\nAn example of a Pythonic idiom is the chain-comparison idiom, which allows comparing multiple\nvariables in one comparison operation, such as “ -size <= x.indices(size)[0] <= size”. The Python\ncommunity continually strives to design and improve them to achieve code conciseness and\nimproved performance [6, 13]. For the above example of chain-comparison, in contrast to the non-\nidiomatic equivalent, “-size <= x.indices(size)[0] and x.indices(size)[0] <= size”, the chain-comparison\nsimplifies code and improves performance. Given these benefits, the community and renowned\nPython developers actively promote the widespread adoption of Pythonic idioms [7, 8, 11, 21, 26, 40].\nHowever, previous studies [6, 51] have indicated that Python users often be unaware of Pythonic\nidioms or unsure of how to correctly use Pythonic idioms, as Pythonic idioms are scattered across\nvarious materials, and are known for their versatile nature [ 7, 8, 11, 40]. For example, chain-\ncomparison idiom also supports “ in” operator (e.g., “ line <= r[1] in rlist ”), yet this usage often\ngoes unnoticed by many Python users. To help Python users use Pythonic idioms, refactoring\nnon-idiomatic code with Pythonic idioms emerges as a promising solution [36, 51]. Through pilot\nstudies, we find the task faces three challenges including code miss, wrong detection and wrong\nrefactoring because of the versatile nature of Pythonic idioms. Code miss refers to missing non-\nidiomatic code that can be refactored with Pythonic idioms . For example, RIdiom misses a\nFor statement code that can be refactored with set comprehension, as shown in code 1○of Figure 1.\nWrong detection refers tomisidentifying non-refactorable non-idiomatic code with Pythonic\nidioms as refactorable . For example, Prompt-LLM wrongly thinks a For statement without adding\nelements to a set as refactorable with set comprehension, as shown in code 5○ of Figure 1. Wrong\nrefactoring refers to giving wrong idiomatic code for refactorable non-idiomatic code with\nPythonic idioms. For example, Prompt-LLM does not correctly refactor code “0< y_int <h_i and\nw_i < 0” with chain comparison, the corresponding idiomatic code is “ w_i < 0 < y_int < h_i ” as\nshown in code 3○of Figure 1.\nA most related work, RIdiom [53], employs a rule-based approach to establish detection rules\nand refactoring procedures to refactor the non-idiomatic code into idiomatic code for nine Pythonic\nidioms. However, it is noteworthy that when confronted with intricate instances of non-idiomatic\ncode, the reliance on pre-defined, inflexible rules cannot overcome the above three challenges (see\nRIdiom examples of Figure 1 in Section 2). Even when identified, formulating rules to refactor it into\nidiomatic code is challenging. On the other hand, in light of the success of Large Language Models\n(LLMs), users can simply describe natural language prompts to instruct LLMs to perform specific\nsoftware engineering tasks, such as code generation [4, 16, 19, 33, 45] and program synthesis [22–24,\n35]. It inspires us to explore LLMs for code idiomatization, wherein we observe their powerful ability\nin certain scenarios, e.g., code 1○of Figure 1 can be correctly refactored with set comprehension by\nLLMs. However, without knowledge guiding, LLMs may make obvious mistakes that can be easily\navoided using rules because of the inherent randomness and black boxes of LLMs (see Prompt-LLM\nexamples of Figure 1 in Section 2).\nThis observation underscores the insufficiency of relying solely on rule-based approach or LLMs.\nIt motivates us to propose a hybrid approach that combines the determinism inherent in rule-based\napproach with the adaptability offered by LLMs. Specifically, our hybrid approach comprises three\ncore modules: a knowledge module , an extraction module , and an idiomatization module .\nFor each module, we write prompts to instruct LLMs to complete tasks or invokes Analytic Rule\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:3\nInterfaces (ARIs) to complete tasks. ARIs are Python code generated by prompting LLMs to generate\ncode. The knowledge module is to construct a knowledge base consisting of three elements of non-\nidiomatic code of thirteen Pythonic idioms and an ARI library. The three elements are ASTscenario\n(the usage scenario of non-idiomatic code), ASTcomponent (the composition of non-idiomatic code)\nand Condition (the condition that refactorable non-idiomatic code must meet). The ARI library\nconsists of ARIs to extract three elements and auxiliary ARIs to rewrite non-idiomatic code into\nidiomatic code. In the extraction module, for any syntax-error-free Python code, we invoke ARIs\nfrom the ARI library to extract ASTscenario and ASTcomponent that satisfies the condition, which\nwill be input into the idiomatization module. The idiomatization module consists of three steps:\nabstracting code, idiomatizing code and rewriting code. We first abstractly represent the code of\nASTcomponent by prompting LLMs. We then idiomatize the abstract code through LLM prompts,\nproducing an abstract idiomatic code. Finally, we utilize ARIs to rewrite the non-idiomatic code\ninto idiomatic code by using the abstract idiomatic code.\nWe conduct two experiments to evaluate the effectiveness and scalability of our approach. For\neffectiveness, we examine nine Pythonic idioms identified by RIdiom [53]. To determine a complete,\ncorrect and unbias benchmark, we randomly sample methods from the methods of each Pythonic\nidiom in RIdiom [ 53]. We independently run our approach, RIdiom and Prompt-LLM for the\nsampled methods to generate code pairs, and invite external workers to verify the correctness of\ncode pairs by each approach manually. Then two authors and external workers discuss and resolve\nthe inconsistencies. The metrics of accuracy, F1-score, precision, and recall were employed for\nevaluating results. The results demonstrate our approach achieves the best performance in accuracy,\nF1-score and recall compared to RIdiom and Prompt-LLM and achieves comparable precision with\nRIdiom. To evaluate the scalability of our approach, we choose four new Pythonic idioms not\ncovered by RIdiom [53]. To avoid the bias of the benchmark, we randomly sample 600 methods\nfrom all methods in RIdiom [53]. To ensure the correctness and completeness of our approach, we\nfollowing the same process in Section 4.1.2. Since RIdiom does not support idiomatization for the\nfour Pythonic idioms. We do not run RIdiom on the new four idioms. Our approach consistently\noutperformed in accuracy, F1-score, precision, and recall, all surpassing 90% for each Pythonic\nidiom, which shows that our approach can be effectively extend to new Pythonic idioms.\nIn summary, the contributions of this paper are as follows:\n•This is the first work to exploit LLMs into code idiomatization with Pythonic idioms, paving the\nway for new opportunities in code idiomatization.\n•We propose a hybrid knowlege-driven approach with ARIs and prompts based on LLMs to refactor\nnon-idiomatic code into idiomatic code with Pythonic idioms.\n•We conduct experiments on both established and new Pythonic idioms. The high accuracy,\nF1-score, precision and recall verify the effectiveness and scalability of our approach. We provide\na replication package [5] for future studies.\n2 MOTIVATING EXAMPLES\nAlthough using Pythonic idioms can improve the conciseness and performance [ 6, 27, 51, 52],\nrefactoring non-idiomatic Python code with Pythonic idioms for a given Python code is not easy.\nRIdiom [53] is the state-of-the-art rule-based approach that formulates detection and refactoring\nrules to automatically refactor non-idiomatic code into idiomatic code for nine Pythonic idioms.\nRecently, large language models (LLMs) have achieved great success in various software engineering\ntasks [9, 18, 24, 34, 35, 37]. LLMs can directly complete various tasks by receiving natural language\nprompts as input, a process we refer to as Prompt-LLM. To explore challenges encountered in this\nendeavor for the two approaches, we randomly collect ten Python methods crawled by RIdiom [53].\nThe methods may contain several non-idiomatic code that can be refactored with a Pythonic idiom.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:4 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\nFig. 1. Motivating examples\nNext, we apply RIdiom and Prompt-LLM to each Python method for nine Pythonic idioms from\nRIdiom. Two authors collaborate to check results of RIdiom and Prompt-LLM and then identify\nchallenges encountered by the two approaches. We summarize three challenges that are described\nas follows.\n(1) Code Miss: miss non-idiomatic code that can be refactored with Pythonic idioms. The\ncode written by Python users comes in various styles, the code may contain several refactorable non-\nidiomatic code with a Pythonic idiom and the form of non-idiomatic code may be diverse. Missing\nthe refactorable non-idiomatic code can lead to redundant code and performance degradation.\nUnfortunately, code missing is common in the two approaches. On the one hand, given the diverse\nand intricate nature of non-idiomatic code patterns, some instances may pose challenges that\nsurpass the capabilities of straightforward rule-based identification. For example, code 1○of the\nRIdiom column of Figure 1 is a “for” statement with two “continue” statements that can be refactored\nwith set-comprehension. Since set-comprehension does not support “continue” keyword, RIdiom\nwrongly assumes the code cannot be refactored. Actually, we can change “ z is x” and “z not in\ndf” into “z is not x ” and “z in df ”, and then we use the “ and” to connect the two conditions to\nremove continue statements. On the other hand, in a codebase, instances of non-idiomatic code are\ndistributed throughout, necessitating a comprehensive scan of the entire codebase to identify such\noccurrences. Unlike rule-based programs that deterministically scan Python code from start to end,\nLLMs operate as black boxes. This non-deterministic nature can inadvertently lead to the oversight\nof refactorable non-idiomatic code [6, 17, 52]. For example, code 4○ of the Prompt-LLM column in\nFigure 1 shows LLMs miss a “for” statement that can be refactored with set-comprehension.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:5\n(2) Wrong Detection: misidentify non-refactorable non-idiomatic code with Pythonic\nidioms as refactorable, which can lead to misunderstandings among Python users regarding\nPythonic idioms and potentially introducing bugs into the codebase. Although not common in RId-\niom, it should not be ignored. The detection rules of RIdiom are human-defined, and developers may\noverlook the nuances of code structures and Python syntax semantics, leading to false discoveries.\nFor example, for 2○of Figure 1, RIdiom determines “-1, 0” is an arithmetic sequence, so it wrongly\nthinks that “a[-1], a[0]” can be obtained by a sliced object “a[-1:1]”. However, since Python list grows\nlinearly and is not cyclic, as such slicing does not wrap (from end back to start going forward) as\nwe expect, the “a[-1:1]” actually is empty. On the other hand, although LLM has powerful abilities,\nits flexibility and adaptability usually cause inappropriate or off-topic response. For example, code\n5○ of the Prompt-LLM column in Figure 1 shows that the Prompt-LLM wrongly classifies a “for”\nstatement that can be refactored with dict-comprehension as refactorable non-idiomatic code with\nset-comprehension. Actually, we can add a condition to check whether the for statement has an\n“add” function call to filter out the wrong detection. For another example, non-idiomatic code of\nchain comparison should have two comparison operations. However, Prompt-LLM often mistakenly\nsuggests refactoring a single comparison operation with chain comparison, even though it cannot\nbe refactored in this way. For instance, when encountering a single comparison operation like\n“start is not None”, Prompt-LLM wrongly assumes it can be refactored using chain comparison.\n(3) Wrong Refactoring: give wrong idiomatic code for refactorable non-idiomatic code\nwith Pythonic idioms. It occurs in identifying refactorable non-idiomatic code but wrongly refac-\ntoring it, resulting in inconsistency in code behavior before and after refactoring. The diversity and\ncomplexity of such non-idiomatic code make both the rule-based approach and the Prompt-LLM\napproach prone to errors. For example, for the chain-comparison, to chain two comparison opera-\ntions into one comparison operation, we need to reverse compare operands for each comparison\noperation and consider if we need to change the comparison operation. When one comparison\noperation has more than one comparison operation, it is more likely to make mistakes. For example,\ncode 3○of the RIdiom column in Figure 1 shows that RIdiom wrongly transforms “a != c > d” into “d\n< c > a”. Directly using LLMs may make unexpected mistakes. For example, code 6○of Prompt-LLM\ncolumn of Figure 1 shows that Prompt-LLM assumes that “h_i” is the chained comparison operand\nand then wrongly refactors it into “0 < y_int < h_i < 0”.\nThe three challenges shown in Figure 1 indicate that the rule-based approach, while deterministic,\nmay still fall short in identifying all refactorable non-idiomatic code instances, especially those\nthat are inherently complex or difficult to address through formulating rules (e.g., 1○ of Figure 1).\nConversely, relying solely on the flexibility and adaptability of LLMs without knowledge guidance\ncan lead LLMs to make obvious mistakes. For example, refactorable non-idiomatic code with set\ncomprehension should contain an “add” function call. Regrettably, code 5○of Figure 1 lacks this\nfunction call. The absence of this contextual knowledge leads LLMs to misidentify it can be refac-\ntored with set comprehension. Therefore, a judicious approach emerges: initially employing code\nto handle deterministic and simple tasks, and then leveraging LLMs to tackle the more challenging\nrefactoring endeavors where the rule-based approach may struggle. This hybrid approach stands\npoised to offer a comprehensive and effective solution.\nMake Python code idiomatic encounters three challenges, including code miss, wrong detection and\nwrong refactoring. Adopting only one approach (rule-based approach or LLMs) alone is not sufficient.\n3 APPROACH\nInspired by the motivating examples in Section 2, we propose a hybrid approach based on LLMs\nto refactor non-idiomatic code with Pythonic idioms. Figure 2 shows the approach overview. We\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:6 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\nFig. 2. Approach overview\nfirst construct knowledge base of non-idiomatic code of Pythonic idioms, which consists of three\nelements: ASTscenario, ASTcomponent and Condition, and an ARI library consisting of ARIs to\nextract the three elements and ARIs to rewrite code. After that, for a given Python code, we first call\nARIs to extract its ASTcomponent from the ASTscenario, and then filter out ASTcomponent that does\nnot meet the condition. Then we input the ASTcomponent and ASTscenario from extraction module\ninto the idiomatization module. The idiomatization module consists of three steps: abstracting\ncode, idiomatizing code and rewriting code. To reduce the pressure on LLMs to idiomatize code, we\nfirst abstract code by abstracting expression of the code corresponding ASTcomponent. And then\nwe write prompts to make LLMs idiomatize the abstract code. After we get the abstract idiomatic\ncode, we need to replace the abstract expression with the original expressions, and then rewrite\nthe non-idiomatic code with the idiomatic code. Since the rewriting operations are simple, we call\nARIs from the Auxiliary ARIs to complete.\n3.1 Knowledge Module\nAs investigated in Section 2, lacking specific knowledge, directly using LLMs to find a non-idiomatic\ncode with a Pythonic idiom from a given Python code is like finding a needle in a haystack. For\nexample, the non-idiomatic code of set-comprehension should have a For node containing an\nadd function call. Without the knowledge, Prompt-LLM misses one For node as shown in 4○\nof Figure 1, and misidentifies a For node without the add function call as non-idiomatic code\nof set-comprehension as shown in 5○ of Figure 1. We observe the non-idiomatic code that can\nbe refactored with Pythonic idioms has deterministic knowledge. Therefore, we can construct a\nknowledge base to boost the ability of LLMs.\n3.1.1 Pythonic idioms library. Pythonic idioms are highly valued by developers [1, 21, 26], many\nstudies summarize Pythonic idioms and research their usage [6, 17, 32, 51]. RIdiom [53] identified\nnine Pythonic idioms by comparing the syntax difference between Python and Java. The nine\nPythonic idioms are list/set/dict-comprehension, chain-comparison, truth-test, loop-else, assign-\nmulti-targets, for-multi-targets and star-in-func-call. State-of-the-art of research [17] based on a\nliterature review identified a total of 27 detectable idioms, of which five (list/set/dict-comprehension,\nchain-comparison, truth-test) overlap with those defined by RIdiom. After excluding infrequently\nused idioms or those with rarely corresponding non-idiomatic Python code 1, four idioms remain:\n1For example, @staticmethod, assert and etc. are common syntax in programming languages, a few python developers use\nother syntax alone to achieve the same functionality without these idioms.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:7\nTable 1. Pythonic Idioms Library for Thirteen Pythonic Idioms\nSource Idiom Explanation Non-Idiomatic Code Idiomatic Code\nRIdiom [53]\n, Farooq\net al. [17]\nlist/set/dict\n-comprehension\nUse one line to append elements\nto an iterable\nnew_cols = []\nfor col in old_cols:\nnew_cols.append(col + postfix)\nnew_cols = [col + postfix\nfor col in old_cols]\nchain-\ncomparison\nChain mutliple comparison expressions\ninto one comparison expression [51] a > b and a < 1 b < a < 1\ntruth-test Directly check the “truthiness” of\nan object embedding_dim % 2 == 0 not embedding_dim % 2\nRIdiom [53]\nloop-else A loop statement has an else clause\nwhile attempt < 3:\n...\nif body is not None:\nbreak\nif body is None:\n...\nwhile attempt < 3:\n...\nif body is not None:\nbreak\nelse:\n...\nassign-multi-tar Assign multiple values to multiple\nvariables in an assign statement\nself._ad = device\nself._sl4a_client = None\nself._ad, self._sl4a_client =\ndevice, None\nfor-multi-tar Unpack the iterated target of a for\nstatement\nfor sample in family.samples:\nif sample[0] > 2:\n...\nfor e0, *e in family.samples:\nif e0 > 2:\n...\nstar-in-func-call Unpack an iterable to the positional\narguments in a function call\nnn.Linear(gate_channels[i]\n, gate_channels[i+1])\nnn.Linear(*gate_channels[i:\ni + 2] )\nFarooq\net al. [17]\nwith Automatically close a file after\nit has been opened\nbamfiles = [x.strip() for\nx in open(bamfile)]\nwith open(bamfile) as f:\nbamfiles = [x.strip() for x in f]\nenumerate\nReturn a tuple containing a count\n(from start which defaults to 0) and\nthe values obtained from iterating\nover iterable.\nfor i in range(len(text)):\nw = text[i]\nif w in token2id:\nR[i] = token2id[w]\nfor (i, w) in enumerate(text):\nif w in token2id:\nR[i] = token2id[w]\nchain-ass-\nsame-value\nAssign a value to multiple\nvariables.\nglobal_draw_name = None\n_test_name = None\nglobal_draw_name =\n_test_name =None\nfstring\nDynamically combine data from\nvariables and other data structures\ninto a readable string output.\nlog.info(‘sample_num_list is %s ’\n% repr(self.sample_num_list))\nlog.info(f‘sample_num_list is\nrepr(self.sample_num_list)’)\nwith, enumerate, fstring, and chain-assign-same-value. This culminates in a total of 13 Pythonic\nidioms. Table 1 gives the explanation and code examples of the 13 Pythonic idioms.\n3.1.2 Three elements of non-idiomatic code of Pythonic idioms.We construct the knowledge base of\nnon-idiomatic code of Pythonic idioms as triples of <element, relation, element>. It comprises three\nfundamental elements: ASTscenario, ASTcomponent, and Condition. Each element focuses on a\nunique aspect: ASTscenario represents usage scenarios for non-idiomatic code linked to a Pythonic\nidiom. ASTcomponent defines the composition of such code, and Condition outlines necessary\nconditions for the ASTcomponents. There exist two relationships between these elements: the\nASTcomponent relies on ASTscenario, and the ASTcomponent adheres to the specified conditions\nto be considered as non-idiomatic code of Pythonic idioms. Table 2 shows the three elements of\nnon-idiomatic code of thirteen Pythonic idioms. The details are as follows:\nASTscenario: A non-idiomatic code associated with a Pythonic idiom may have restrictions\non usage scenarios, corresponding to a distinct Abstract Syntax Tree (AST) node, referred to as\nASTscenario. For example, chain-comparison idiom can chain two comparison operations using\nthe “and” operator into one comparison. So the ASTscenario is a BoolOP node whose op is “and” as\nshown in Table 2. For another example, for the list comprehension idiom in Table 2, it allows the\naddition of elements to an object in just one line, as opposed to using a for statement. Since the for\nstatement has no restrictions on the usage scenario, it does not possess an associated ASTscenario.\nASTcomponent: A non-idiomatic code associated with a Pythonic idiom has a deterministic\ncomposition, corresponding to few AST nodes, referred to as ASTcomponent. It serves as a pivotal\nentity in discerning and addressing non-idiomatic code patterns. Taking the chain-comparison\nidiom in Table 2 as an example, its ASTcomponent comprises two Compare nodes within a BoolOP\nnode. These Compare nodes form essential elements of the non-idiomatic code pattern. For another\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:8 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\nTable 2. Three Elements of Non-Idiomatic Code of Thirteen Pythonic Idioms\nIdiom ASTscenario ASTcomponent Condition\nlist/set-comprehension — A For node\nAn Assign node\n1. The For node has “append/add” function call\n2. The function name of “append/add” function call\nis the assigned variable of the Assign node\ndict-comprehension — A For node\nAn Assign node\n1. The For node has an assign statement whose\nassigned variable is a Subscript node\n2. The value of the Subscript node\nis the assigned variable of the Assign node\nchain-comparison A BoolOP node\nwhose op is “and” Two Compare nodes 1. Compare operands of the two Compare nodes\nintersect\ntruth-test A test-type node A Compare node 1. The op of the Compare node is “== “or “!=”\n2. The one comparison operand is belong to EmptySet\nloop-else — A For/While node\nAn If node\n1. The For/While node has break statements\n2. The If node is the next statement of For node\nassign-multi-tar — Consecutive Assign nodes —\nfor-multi-tar — A For node\n1. The body of the For node has a Subscript node\n2. The value of the Subscript node is the iterated\nvariable of the For node\nstar-in-func-call A Call node Consecutive Subscript nodes 1. The values of Subscript nodes are the same\nwith — A Call node 1. The function name of the Call node is “open”\nenumerate — A For node 1. The iterated object is not a function call whose\nfunction name is “enumerate”\nchain-ass-same-value — Consecutive Assign nodes 1. The values of consecutive Assign nodes are the same\nfstring — A BinOP node 1. The op of the BinOp node is “%”\nexample, for the list comprehension idiom in Table 2, which involves appending elements to an\nobject in a for statement, its ASTcomponent is a For node and an Assign node.\nCondition: A non-idiomatic code associated with a Pythonic idiom may entail specific conditions,\nreferred to as Condition for its ASTcomponent. It serves as a guiding principle for identification of\nrefactorable ASTcomponent and avoid LLMs from idiomatizing non-refactorable ones that does\nnot meet the specified conditions. For example, for the chain-comparison idiom in Table 2, the\ncondition stipulates that the compare operands of the two Compare nodes must intersect. For\nanother example, for the list-comprehension in Table 2, its non-idiomatic code is to append elements\nto a list, so the For node of ASTcomponent should has a “append” function call whose function\nname is the assigned variable of the Assign node.\nRelation: There are two relationships between the three elements. The ASTcomponent is depend\non the ASTscenario, the ASTcomponent should satisfy the Condition. For example, for the chain-\ncomparison idiom, its AST component (two Compare nodes) is depend on the ASTscenario (BoolOp\nnode whose op is “and”), and its ASTcomponent satisfies the Condition (Compare operands of\nthe two Compare nodes intersect). These relationships establish a clear framework for identifying\nnon-idiomatic code. We elaborate it in Section 3.2.\n3.1.3 ARI library. The Analytic Rule Interface (ARI) library consists of ARIs to extract three\nelements in Table 2 and auxiliary ARIs. In contrast to directly relying on prompts to instruct LLMs\nin extracting the three essential elements from a given Python code, we employ LLMs to generate\ncode to implement the required functionality. This approach addresses two key considerations.\nFirstly, within a project, it may have thousands of lines of code, and the non-idiomatic code is a\nsmall part of it. It is expensive for LLMs to handle so many codes and is difficult to make sure\nthe non-idiomatic code is not missing and is correct from unrelated code in a given Python code.\nSecondly, by generating code through a single invocation of LLMs, we establish a reusable ARI\nlibrary that can be leveraged consistently across different given Python code. Figure 3 shows\nexamples to prompt LLMs to generate code.\nPrompt LLMs to generate ARIs to extract three elements: We first create three prompt\ntemplates for the three elements: ASTscenario, ASTcomponent and Condition. Then we instantiate\nthe prompt templates with three elements of each Pythonic idiom from Table 2. Finally, we instruct\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:9\nFig. 3. ARI library built by prompting LLMs to generate code\nthe LLM to generate code. Following the retrieval of the generated Python code, authors manually\nvalidate its correctness. Once verified, the code is cataloged as a reusable ARI, poised for application\nin subsequent any Python code 2. This systematic approach ensures the reliability and reusability\nof the generated code for element extraction.\nThe template of ASTscenario is “Write Python method code to extract [ASTscenario] from a\nPython code”, The [ASTscenario] is a placeholder which corresponds to the ASTscenario of a\nPythonic idiom in Table 2. For example, for the chain-comparison idiom in Figure 3, the template is\ninstantiated into “Write Python method code to extract BoolOP nodes whose op is “and” . ” When the\nprompt is input into the LLM, the LLM responds with an ARI called “extract_and_boolops(code)”.\nThe template of ASTcomponent is “Write Python method code to extract [ASTcomponent] from\n[ASTscenario] / a Python code”. The [ASTcomponent] and [ASTscenario] are placeholders which\ncorresponds to the ASTscenario and ASTcomponent of a Pythonic idiom in Table 2. For example, for\nthe chain-comparison in Figure 3 and Table 2, its ASTscenario is present, the template is instantiated\ninto “Write Python method code to extract combinations consisting of two different Compare nodes\nfrom a BoolOP node ”. When the prompt is input into the LLM, the LLM responds with an ARI\ncalled “extract_compare_combinations(node)”. For another example, for the list-comprehension,\nthe ASTscenario is absent as shown in Table 2, the the template is instantiated into “Write Python\nmethod code to extract For nodes from a Python code ”. When the prompt is input into the LLM, the\nLLM responds with an ARI called “extract_for_nodes(code)”.\nThe template of Condition is “Write Python method code to check [condition]”. The [condition]\nis a placeholder which corresponds to the Condition of a Pythonic idiom in Table 2. For example,\nfor the chain-comparison in Figure 3, the template is instantiated into “Write Python method code\nto check if compare operands of two Compare nodes intersect ”. When the prompt is input into the\nLLM, the LLM responds with an ARI called “compare_operands_intersect(node1, node2)”. Prompt\nLLMs to generate auxiliary ARIs: For the auxiliary ARIs, it is a replace operation utilized in\nthe idiomatization module. Since the replace operation is a simple task, we do not need to instruct\nLLMs to complete for each given Python code. And invocating the ARI can correctly and effectively\nreplace substring1 with substring2 in a string. Specifically, we input the prompt “ Write Python\nmethod code to replace substring1 with substring2 in a string ” into the LLM, the LLM responds with\n“replace_substring(string, substring1, substring2)” whose body is “return string.replace(substring1,\nsubstring2)”.\n3.2 Extraction Module\nAfter we construct the knowledge base of three elements of non-idiomatic code of Pythonic\nidioms and ARIs to extract three elements. We call ARIs from ARI library in order ASTscenario,\nASTcomponent and Condition. To elaborate, if the ASTscenario exists, we first extract ASTscenario\n2We manually verify that all ARIs are correct\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:10 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\nFig. 4. Examples of extraction module\nfrom a Python code, and then extract the ASTcomponent from the ASTscenario followed by\nfiltering out components that do not satisfy the condition if condition exists. If the ASTscenario\ndoes not exist, we directly extract the ASTcomponent from a Python code followed by filtering\nout ASTcomponent that does not satisfy the condition if the condition exists. For example, for the\nchain-comparison of Figure 4, we first call “ extract_and_boolops” ARI to get all BoolOP nodes\nfrom a Python code. Then, for each BoolOP node, we call “extract_compare_combinations” ARI\nto extract all two different Compare nodes from the BoolOP node. Finally, for each two Compare\nnodes, we call “compare_operands_intersect” ARI to filter out two Compare nodes without common\ncomparison operands. For another example, for the list-comprehension, its ASTscenario is empty,\nwe directly call “extract_for_nodes(code)” to extract all For nodes from a Python code, and then we\ncall “has_append(node)” filter out For nodes without “append” function call whose function name\nis the assigned variable of the Assign node.\n3.3 Idiomatization Module\nAfter extracting the code of ASTscenario and ASTcomponent, we can do the idiomatization task.\nSince the diversity and complexity of refactorable non-idiomatic code, it is not easy to complete the\ntask by formulating rules. For example, consider code 1○ Figure 1. To refactor the code containing\ntwo continue statement with set-comprehension, we need to change “z is x” and “z not in df” into “z\nis not x” and “z in df”, and then we use the “and” to connect the two conditions. Finally, we transform\nit with set-comprehension. Similarly, in the case of 6○of Figure 1, to refactor non-idiomatic code\nwith chain-comparison, we need to reverse compare operands and determine whether need to\nchange comparison operators based on the code semantic. Fortunately, LLMs trained on large\ncorpora have rich knowledge and huge potential to complete complex tasks with natural language\nprompts [9, 18, 24, 34, 35]. Therefore, we write prompts to instruct LLMs to transform non-idiomatic\ncode into idiomatic code for Pythonic idioms. To correctly complete the refactoring task, we design\nthree steps including abstracting code, idiomatizing code and rewriting code.\n3.3.1 Abstracting Code. The initial step involves the abstraction of the code which corresponds\nto ASTcomponent, wherein we keep related code snippets with Pythonic idioms while abstract\nunrelated code snippets with Pythonic idioms. By distilling the code to its core elements, we\nenhance the clarity and simplicity of the subsequent idiomatization process. To determine the\nabstract form for each Pythonic idiom, we invite three external workers with more than five years\nPython programming experience. We first introduce Pythonic idioms to make sure they are familiar\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:11\nFig. 5. Examples of idiomatization module\nwith the idioms. Each worker is then asked to independently review the non-idiomatic code dataset\nof Pythonic idioms from previous researches [17, 51], and then writes a description of abstracting\ncode and provides three examples of original non-idiomatic code and abstract non-idiomatic code.\nThen two authors discuss their results and give the final description of abstracting code (<prompt>)\nand three examples of original non-idiomatic code and the corresponding abstract non-idiomatic\ncode for each Pythonic idiom (<examples>).\nFor the abstracting code for each Pythonic idiom, if the prompt has a specified object to abstract,\nwe use ARIs from Auxiliary ARIs in Section 3.1.3 to replace a given string with another given\nstring. Otherwise, we use prompt to instruct LLMs to complete the task. For example, for the star-\nin-func-call, the extracted non-idiomatic code is “feat.shape[-2], feat.shape[-1]” and the abstracted\nexpression is a specified object (“ feat.shape”). Therefore, we invoke “ replace_substring” from\nauxiliary ARIs to replace “feat.shape”with “v”, so the abstract code is “v[-2], v[-1]”. For another\nexample in the Abstracting Code of Figure 5, it does not have a specified object to abstract, We\nuse prompt “Use a symbol v to simplify each comparison operand within the following Python code.\nThe same comparison operand is represented by the same symbol. ” to abstract represent the code\n“args.save_steps > 0 and global_step % args.save_steps == 0”. The LLM responds with a symbol\nmapping and the abstract Python code “v1 > v2 and v3 == v2”, facilitating subsequent idiomatizing\nprocess.\n3.3.2 Idiomatizing code. Building upon the abstracted Python code representation, this step focuses\non the actual idiomatization of the code. To determine the prompt of idomatizing code for each\nPythonic idiom, following the similar process 3.3.1, we invite the same three external works to\nfurther independently write a description to idiomatize the abstract code for each Pythonic idiom\nand provide examples with the abstract code and the corresponding idiomatic code for each Pythonic\nidiom. And then two authors discuss their results to give the final description of idiomatizing code\n(<prompt>), and the abstract code and the corresponding idiomatic code for each Pythonic idiom\n(<examples>).\nFor example, for the chain-comparison in the Idiomatizating Code of Figure 5, we use prompt\n“Reverse compare operands of the first comparison operation, the second comparison, or the first and\nthe second comparison operations so that “ v2 and v2” is in the new Python code, and then simplify it ”\nto idiomatize the abstract Python code: “v1 > v2 and v3 == v2”. The LLM responds with Yes and\nthe abstract idiomatic Python code “v1 > v2 == v3”. For another example, for the abstract code of\nchain-comparison “v1 in v2 and v3 in v2”, The LLM responds with No because reversing compare\noperands is invalid for the “in” operator that can change the code semantic.\n3.3.3 Rewriting code. Following the acquisition of abstract idiomatic code from the idiomatiza-\ntion process, the final step involves rewriting the non-idiomatic code. It is achieved through the\napplication of the “replace” ARI sourced from the Auxiliary ARIs. The “replace” operation serves\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:12 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\na dual purpose: it facilitates the restoration of the abstract idiomatic code and allows for direct\ncode rewriting by replacing the ASTcomponent with the idiomatic code in the ASTscenario, if\nASTscenario exists. Therefore, it may invoke “replace” several times. To determine the process\nof invoking “replace”, following the similiar process in Section 3.3.1, we invite the same three\nexternal workers to further independently summarize steps to use “replace” to complete rewriting\nnon-idiomatic code into idiomatic code. And then two authors discuss their results to give the final\nsteps of rewriting code for each Pythonic idiom.\nFor example, for the Rewriting Code process illustrated in Figure 5, we first replace abstract\nsymbols with their corresponding original expressions within the abstract idiomatic code: “v1 >\nv2 ==v3”, yielding the genuine idiomatic code. Subsequently, we replace the Compare1 and ‘and’\nwith an empty string in the ASTscenario code, yielding the new ASTscenario code. Finally, we\nreplace the Compare2 with the genuine idiomatic code in the new ASTscenario code, yielding a\nfinal idiomatic code: “args and args.save_steps > 0 == global_step % args.save_steps”. The rewriting\nstep ensures the precise transformation of non-idiomatic code into its idiomatic counterpart.\n4 EVALUATION\nTo evaluate our approach, we study two research questions:\nRQ1 (Effectiveness): What is the effectiveness of our approach in refactoring non-idiomatic\nPython code into idiomatic Python code with nine Pythonic idioms?\nRQ2 (Scalability): Can our approach be effectively extended to new Pythonic idioms?\n4.1 RQ1: Effectiveness of Refactoring Non-Idiomatic Python Code with Nine Pythonic\nIdioms\n4.1.1 Motivation. RIdiom [51] can automatically refactor non-idiomatic code into idiomatic code\nwith nine Pythonic idioms by formulating detection and refactoring rules, but it causes huge human\ninvestment in formulating rules. The current success of ChatGPT [ 3] demonstrates remarkable\nability of LLMs to comprehend human prompts and complete the corresponding tasks. Therefore,\nwe are interested in understanding the performance of our approach based on LLMs.\n4.1.2 Approach. To clarify the effectiveness of our approach, we perform effectiveness comparison\nby calculating metrics on a dataset with our approach and the state-of-the-art baselines.\nDataSet. To evaluate the effectiveness of our approach, it is fundamental to have a correct and\ncomplete benchmark of code refactorings consisting of code pairs <non-idiomatic Python code,\nidiomatic Python code>. Manually constructing a complete and correct benchmark is unrealistic\nbecause code refactoring involves a lot of time and manpower, and inevitably comes with personal\nbias. Recently, RIdiom [53] can automatically refactor non-idiomatic Python code with nine Pythonic\nidioms, which provides a good starting point. It provides code pairs within crawled methods for\neach Pythonic idiom. Considering the effort of manual verification, we randomly sample methods\nwith a confidence level of 95% and a confidence interval of 5 from RIdiom [53] for each Pythonic\nidiom. Then, to ensure the completeness of the benchmark, we run RIdiom [53], our approach and\nPrompt-LLM on the sample methods for nine Pythonic idioms to collect code pairs <non-idiomatic\nPython code, idiomatic Python code>. To validate the correctness, we invite 18 external workers\nwith more than five years Python programming experience. We divide them into 9 groups, and each\ngroup of two workers independently checks the correctness of the code pairs for an idiom. The\nCohen’s Kappa values [46] of nine groups for their annotation results all exceeded 0.75 (substantial\nagreement). Finally, the two authors and external workers discuss and resolve the inconsistencies\nand ensure the correctness of the benchmark for nine Pythonic idioms. Table 3 shows the benchmark\nof nine Pythonic idioms. The Method column represents the number of sampled methods for each\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:13\nTable 3. Benchmark of Nine Pythonic Idioms\nIdiom Method Code Pair\nlist-comprehension 389 512\nset-comprehension 305 361\ndict-comprehension 370 448\nchain-comparison 391 574\ntruth-test 394 610\nloop-else 319 360\nassign-multi-targets 395 729\nfor-multi-targets 370 463\nstar-in-func-call 381 621\nTotal 3311 4678\nPythonic idiom, and the Code Pair column represents the number of code pairs <non-idiomatic\ncode, idiomatic code> for each Pythonic idiom. Since sampled methods for each Pythonic idiom are\nfrom methods of RIdiom, it is reasonable to expect that the number of code pairs is greater than\nthe number of the corresponding methods for each Pythonic idiom.\nBaselines. We compare our approach with two baselines. The first baseline is RIdiom that is an\napproach based on rules proposed by Zhang et al. [ 51]. It detects and refactors non-idiomatic\nPython code into the corresponding idiomatic Python code with nine Pythonic idioms by manually\nformulating detection rules and refactoring steps. The second (Prompt-LLM) is to directly call the\nLLM to find code pairs for any given method code for each Pythonic idiom to illustrate the capability\nof LLM and the strengths of our proposed approach using LLM. For fairness of comparison, similar\nto process in Section 3.3.1, we invite three external works to independently write a prompt and\nprovide three examples of a Python code and the corresponding code pairs. Then two authors\ndiscuss their results and give the final prompt and three examples for each Pythonic idiom. Examples\nare shown in Prompt-LLM column of Figure 1. Since the success of ChatGPT, the LLM our approach\nand baselines use are state-of-the-art of GPT-3.5-turbo [2]. And we set temperature to 0 to make\nthe outputs mostly deterministic.\nMetrics. By referring metrics using by previous researches on code refactorings [15, 39, 44, 51], we\nuse four metrics accuracy, F1-score, precision and recall. To calculate the four metrics, we need to\ndefine true positives, false positives and false negatives which are represented as 𝑇𝑃, 𝐹𝑃 and 𝐹𝑁,\nrespectively. We define a𝑇𝑃 as a code pair detected by an approach is in the benchmark. We define\na 𝐹𝑃 as a code pair detected by an approach is not in the benchmark. We define a 𝐹𝑁 as a code\npair of the benchmark is not determined by an approach. The accuracy and F1-score represents the\noverall performance. We calculate accuracy, F1-score, precision and recall as follows:\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 = 𝑇𝑃\n𝑇𝑃 +𝐹𝑃, 𝑅𝑒𝑐𝑎𝑙𝑙 = 𝑇𝑃\n𝑇𝑃 +𝐹𝑁\n𝐹1 = 2 ∗𝑃∗𝑅\n𝑃+𝑅 , 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦 = 𝑇𝑃\n𝑇𝑃 +𝐹𝑃 +𝐹𝑁\n4.1.3 Result. Figure 6 presents the accuracy, F1-score, precision and recall of our approach, RIdiom,\nPrompt-LLM for nine Pythonic idioms.\nComparison with RIdiom. In comparison to RIdiom, our approach consistently outperforms\nin three metrics: accuracy, F1-score, and recall across all Pythonic idioms. Notably, our approach\nexhibits a distinct advantage in both recall and accuracy over RIdiom. For eight Pythonic id-\nioms, the accuracy and the recall exceed 90%. While our approach registers slightly below 90% in\nboth recall and accuracy for the assign-multi-targets idiom, it is obviously close to 90% (87.8%).\nIn contrast, RIdiom falls short of 90% in accuracy for five Pythonic idioms (list-comprehension,\ndict-comprehension, chain-comparison, loop-else, and assign-multi-targets), and four of these\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:14 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\nlist-compre set-compre dict-comprechain-compare truth-test loop-else ass-mul-tar for-mul-tar star-call\n0\n10\n20\n30\n40\n50\n60\n70\n80\n85\n90\n95\n100Accuracy (%)\n93.1\n95.6\n93.0 94.3\n100.0\n92.1\n87.8\n95.3 95.8\n82.5\n90.9\n84.8 86.1\n93.5\n89.6\n82.7\n90.9\n95.2\n36.9\n52.9 54.9\n2.1\n27.3\n20.4\n12.7\n3.7 2.9\na) accuracy comparison\nOurs\nRIdiom\nPrompt-LLM\nlist-compre set-compre dict-comprechain-compare truth-test loop-else ass-mul-tar for-mul-tar star-call\n0\n10\n20\n30\n40\n50\n60\n70\n80\n85\n90\n95\n100F1-Score (%)\n93.4\n95.8 94.0 95.4\n100.0\n95.2\n92.5\n96.0 97.8\n90.4\n95.2\n91.8 92.3\n96.7\n93.8\n90.5\n95.0\n97.2\n52.3\n63.4 67.2\n3.7\n41.2\n26.8\n22.5\n5.3 5.3\nb) F1 comparison\nOurs\nRIdiom\nPrompt-LLM\nlist-compre set-compre dict-comprechain-compare truth-test loop-else ass-mul-tar for-mul-tar star-call\n0\n10\n20\n30\n40\n50\n60\n70\n80\n85\n90\n95\n100Precision (%)\n93.8\n96.1 95.1 96.6\n100.0\n97.1 97.7\n95.5 97.4\n100.0 100.0 100.0 99.4\n98.5\n97.9 100.0 98.4 98.7\n72.2 74.5 75.9\n8.1\n80.1\n30.9 34.3\n7.5 7.3\nc) precision comparison\nOurs\nRIdiom\nPrompt-LLM\nlist-compre set-compre dict-comprechain-compare truth-test loop-else ass-mul-tar for-mul-tar star-call\n0\n10\n20\n30\n40\n50\n60\n70\n80\n85\n90\n95\n100Recall (%)\n93.1\n95.6\n93.0 94.3\n100.0\n93.4\n87.8\n96.6 98.2\n82.5\n90.9\n84.8 86.1\n94.9\n90.1\n82.7\n91.8\n95.8\n41.0\n55.1\n60.2\n2.4\n27.7 23.7\n16.7\n4.1 4.2\nd) recall comparison\nOurs\nRIdiom\nPrompt-LLM\nFig. 6. Scatter plot with straight lines of accuracy, F1-score, precision and recall of three approaches for nine\nPythonic idioms\nidioms (list-comprehension, dict-comprehension, chain-comparison, and assign-multi-targets) also\nhave recall results below 90%. For the list-comprehension, the disparity in recall and accuracy\nbetween our approach and RIdiom exceeds 10%. For the other four idioms (dict-comprehension,\nchain-comparison, truth-test, and assign-multi-targets), these differences surpass 5%. For the re-\nmaining four idioms (set-comprehension, loop-else, for-multi-targets, and star-in-func-call), while\nthe differences are relatively smaller, our approach maintains a consistent edge over RIdiom.\nFor the precision, although RIdiom exhibits a slight advantage in precision (with differences\nranging from 0.8% to 6.2%) for eight of the idioms, our approach consistently achieves over 93.8%\nprecision for each idiom. Moreover, our approach surpasses RIdiom for the truth-test idiom. It is\nimportant to note that F1-score strikes a balance between precision and recall, and in this regard,\nour approach consistently outperforms RIdiom across all Pythonic idioms. The comprehensive\nanalysis underscores the notable advantages of our approach over RIdiom for the task of refactoring\nnon-idiomatic code with Pythonic idioms, and can make up for the shortcomings of RIdiom in\nrecall.\nComparison with Prompt-LLM. Compared to our approach, Prompt-LLM consistently exhibits\nthe lowest performance across four metrics for each Pythonic idiom. The disparities between our\napproach and Prompt-LLM in terms of accuracy, F1-score, precision, and recall for the nine Pythonic\nidioms are substantial, ranging from 38.1% to 92.9%, 26.8% to 92.5%, 19.9% to 90.1%, and 32.8% to\n94%, respectively.\nCompared to Prompt-LLM, our approach maintains stable and commendable performance across\nall idioms, with each metric surpassing the 90% threshold for eight of them. Even in the case of\nassign-multi-targets, where both accuracy and recall fall slightly below 90%, they are still notably\nclose at 90% (87.8%). In contrast, Prompt-LLM fails to achieve a metric score of 90% for any of the\nidioms.\nPrompt-LLM demonstrates poor performance and exhibits significant variability across different\nPythonic idioms. For list/set/dict-comprehension, Prompt-LLM displays relatively better perfor-\nmance across all metrics, exceeding 35%. This underscores LLMs’ enhanced proficiency in handling\nthe three idioms. For truth-test, Prompt-LLM exhibits superior precision (80.1%) compared to\nrecall (27.7%), indicating a higher likelihood of missing refactorable non-idiomatic code instances\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:15\nFig. 7. The examples that our approach wrongly refactors or refrains from refactoring\nassociated with the truth-test idiom. For loop-else and assign-multi-targets, Prompt-LLM’s perfor-\nmance remains limited across all metrics, falling below 35%. Furthermore, for the remaining three\nidioms (chain-comparison, for-multi-targets, and star-in-func-call), Prompt-LLM’s performance is\nmarkedly poorer, registering below 10% on all metrics. It suggests that Prompt-LLM struggles to\neffectively detect and refactor non-idiomatic code associated with the three idioms. Therefore, our\napproach significantly enhances the capabilities of LLMs, consistently demonstrating stable and\ncommendable performance across all metrics.\nFailure analysis of our approach. For the code pairs in the benchmarks that our approach does\nnot find or wrongly refactor non-idiomatic Python code with nine Pythonic idioms, we summarize\ntwo reasons as follows:\n(1) LLMs may produce suboptimal results when refactoring is too complicated. While LLMs\nhave achieved success, it is reasonable that LLMs cannot handle all situations. For example, for\nthe first example of Figure 7, the non-idiomatic code appends two different elements to the list\n“possible_mistakes” in each iteration of the for statement. Our approach finally gives the idiomatic\ncode by concatenating two lists which independently appending two elements. The idiomatic code\nis wrong because the order of elements is different from the non-idiomatic code. Although the\nnon-idiomatic code can be refactored with list-comprehension, the idiomatic code needs other\nstatements to adjust the order of elements. For another example, for the non-idiomatic code\n“gnn_layer(..., n_points[idx1], n_points[idx2])”, it is non-refactorable code with star-in-func-call\nbecause the “idx1, idx2” is not an arithmetic sequence. However, our approach mistakenly assumes\nthe subscript sequence of “_points[idx1], n_points[idx2]” is an arithmetic sequence and refactor it\ninto “gnn_layer(..., *n_points[idx1:idx2 + 1])” with star-in-func-call.\n(2) LLMs may refrain from refactoring when benefits in idiomatic code appear limited: While\nLLM demonstrates proficiency in refactoring Python code using specific Pythonic idioms, there\nare instances where it abstains from doing so. For example, for the second example of Figure 7,\nthe non-idiomatic code “ for u in urls_results: urls.add(u) ” actually can be refactored with set-\ncomprehension. However, LLM responds with “The given code is already simple and concise. Using\nset comprehension here would not make the code more readable or efficient. ” . LLM refuses to refactor\nit with set-comprehension because it may determine that applying set-comprehension would not\nnotably enhance code conciseness. For another example, for the non-idiomatic code “slice2[axis] =\nslice(None, -1); slice1 = tuple(slice1)”, LLM responds with “The given code cannot be refactored with\none assign statement as the variables being assigned are not of the same type” . The LLMs refuses to\nrefactor because it thinks “slice2[axis]” and “slice1” are not the same type.\nOur approach can achieve best accuracy, F1-score and recall compared to other baselines. Although\nour approach performs slightly worse than RIdiom on precision, our approach can complement RIdiom\non recall.\n4.2 RQ2: Scalability of Our Approach\n4.2.1 Motivation. Although RIdiom can achieve good result on nine Pythonic idiom, it cannot\nhandle new Pythonic idioms and is difficult to extend to new Pythonic idioms because RIdiom\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:16 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\nTable 4. Benchmark of Four New Pythonic Idioms\nIdiom Methods Code Pairs\nwith 600 64\nenumerate 600 565\nchain-assign-same-value 600 156\nfstring 600 223\nTotal 600 1008\nwith\nenumerate chain-assign fstring\n0\n10\n20\n30\n40\n50\n90\n95\n100Accuracy (%)\n100.0 99.5\n97.4\n92.6\n14.8\n28.5\n9.0\n31.9\na) accuracy comparison\nOurs\nPrompt-LLM\nwith\nenumerate chain-assign fstring\n0\n10\n20\n30\n40\n50\n90\n95\n100F1-Score (%)\n100.0 99.7 98.7\n94.9\n25.8\n44.4\n16.6\n48.4\nb) F1 comparison\nOurs\nPrompt-LLM\nwith\nenumerate chain-assign fstring\n0\n10\n20\n30\n40\n50\n90\n95\n100Precision (%)\n100.0 99.5 98.7\n93.0\n17.4\n42.6\n10.9\n43.6\nc) precision comparison\nOurs\nPrompt-LLM\nwith\nenumerate chain-assign fstring\n0\n10\n20\n30\n40\n50\n90\n95\n100Recall (%)\n100.0 100.0 98.7 97.0\n50.0\n46.3\n34.5\n54.3\nd) recall comparison\nOurs\nPrompt-LLM\nFig. 8. Scatter plot with straight lines of accuracy, F1-score, precision and recall of two approaches for four\nnew Pythonic idioms\nneeds manually formulate extracting and refactoring rules. So we are interested in whether our\napproach can be effectively extended to new Pythonic idioms that RIdiom cannot handle.\n4.2.2 Approach. Following the approach in Section 4.1.2, we present baselines, dataset and metrics.\nBaselines and metrics are the same as metrics and baselines in Section 4.1.2. Particularly, since\nRIdiom cannot handle the new four Pythonic idioms, the baseline is excluded. The dataset is\ndetailed as follows: DataSet. As Section 3.1.1 illustrates, there are four new Pythonic idioms (with,\nenumerate, fstring and chain-assign-same-value) that RIdiom cannot handle. Considering that we\nneed a certain number of code pairs (<non-idiomatic Python code, idiomatic Python code>) to\nevaluate our approach, but manually verifying code pairs is time consuming and the current GPT-3.5\nis not free, we randomly sample 600 Python methods from crawled methods by RIdiom [51] that is\na tool to refactor non-idiomatic code into idiomatic code with nine Pythonic idioms. Following the\nsimilar method in Section 4.1, to ensure the completeness of dataset as much as possible, we run our\napproach and Prompt-LLM on the sampled Python methods to collect code pairs <non-idiomatic\nPython code, idiomatic Python code>. To validate the correctness, we invite 8 external workers\nwith more than five years Python programming experience. We divide them into four groups, and\neach group of two workers independently checks the correctness of the code pairs for an idiom. The\nCohen’s Kappa values [46] of four groups for their annotation results all exceeded 0.75 (substantical\nagreement). Finally, the two authors and external workers discuss and resolve the inconsistencies\nand ensure the correctness of the benchmark for the new four Pythonic idioms. Table 4 shows the\nbenchmark of new four Pythonic idioms. The Method column represents the number of sampled\nmethods for each Pythonic idiom, and the Code Pair column represents the number of code pairs\n<non-idiomatic code, idiomatic code> for each Pythonic idiom. The number of code pairs is less\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:17\nthan the number of methods is reasonable because not each method may contain non-idiomatic\ncode. For example, in the case of the with idiom, its non-idiomatic code should include file opening\noperations. However, not all methods necessarily involve file opening. Given that the with idiom is\nwidely adopted by Python users [38], the count is relatively low (64). Conversely, for the enumerate\nidiom, its non-idiomatic code typically corresponds to a for statement, which is more prevalent in\ncode. As a result, the count is relatively higher (565).\n4.2.3 Result. Figure 8 presents the accuracy, F1-score, precision and recall of our approach, and\nPrompt-LLM for new four Pythonic idioms. For each Pythonic idioms, the metrics all are above 90%.\nAnd the accuracy, F1-score, precision and recall are above 95% for three idioms (with, enumerate\nand chain-assign-same-value). Compared to our approach, Prompt-LLM consistently exhibits poor\nperformance across four metrics for each Pythonic idiom. The disparities between our approach\nand Prompt-LLM in terms of accuracy, F1-score, precision, and recall for the four Pythonic idioms\nare substantial, ranging from 60.7% to 88.4%, 46.5% to 82.1%, 49.4% to 87.8%, and 42.7% to 64.2%,\nrespectively.\nThe high accuracy, F1-score, precison and recall of code refactorings of the four new Pythonic idioms\nillustrate that our approach can be effectively extended to other Pythonic idioms.\n5 DISCUSSION\n5.1 Implications\nOur hybrid approach in Section 3 demonstrates excellent performance in Section 4, we now delve\ninto the scalability of our approach and future work for researchers. Currently, our approach only\nsupports syntactically correct Python code, as it requires parsing the code into an Abstract Syntax\nTree (AST). However, handling Python code with syntax errors on the hybrid framework is feasible.\nOne solution is to incorporate a syntax-fixing module to rectify syntax errors in the Python code\nbefore it input into extraction module in Section 3.2. Another approach is to introduce an alternative\nmethod within the extraction module in Section 3.2. Specifically, prompting LLMs to extract three\nelements or prompting LLMs to generate AST for Python code instead of invoking APIs, which can\nbe effective when the syntax error cannot be fixed.\nPrior studies [6, 17, 51, 52] have shown that employing Pythonic idioms may yield benefits,\nsuch as code conciseness and improved performance, but it can also have drawbacks, including\npotential impacts on code readability and performance degradation. Our current focus is solely\non refactoring non-idiomatic code with Pythonic idioms. The results in Section 4 and Figure 7\nhighlight that LLMs may abstain from refactoring when benefits from idiomatic code are limited.\nThis observation prompts researchers to consider using LLMs combined with knowledge base to\ngenerate comments explaining the positive and negative effects of refactoring non-idiomatic code\nwith Pythonic idioms. This can enhance Python users’ understanding and effective utilization of\nPythonic idioms.\nFurthermore, our approach combines ARIs and prompts based on LLMs to refactor non-idiomatic\ncode with Pythonic idioms. While current Language Models (e.g., ChatGPT [ 34]) are not freely\navailable, there has been a recent emergence of free alternatives (e.g., Llama 2 [43]). Over time, it\nis plausible that more LLMs may become more accessible, potentially even free of charge. This\naccessibility could assist developers in effectively refactoring code using Pythonic idioms to alleviate\nthe limitation of rule-based approach.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:18 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\n5.2 Threats to Validity\nInternal Validity: The one internal threat is inaccuracy when evaluating the correctness of\nbenchmark in Section 4. For each Pythonic idiom, we invite two external workers with more than\nfive Python programming experience. And two authors and external workes discuss to resolve the\ninconsistencies to ensure the correctness of the benchmark.\nExternal Validity: One external threat is that our approach is limited to thirteen Pythonic idioms.\nRIdiom [53] can only handle nine Pythonic idioms, our approach contains four new Pythonic idioms\nwhich validate the scalability of our approach. When there are more Pythonic idioms, developers\ncan determine the three elements of Pythonic idioms to extend to more Pythonic idioms. In the\nfuture, we will automatically mine undetected Pythonic idioms and automatically determine three\nelements of Pythonic idioms. The other external threat is the representative of the benchmark\nselected to evaluate our approach. To mitigate this threat, our experimented methods of benchmark\nare from previous research [53], representing an unbiased benchmark for our research.\n6 RELATED WORK\nStudies on Pythonic idioms. Pythonic idioms are highly valued by researchers, there are several\nstudies [6, 17, 21, 26, 31, 41, 42, 53, 54] to mine Pythonic idioms or help Python users use Pythonic\nidioms better. Alexandru et al. [ 6] and Farooq et al. [ 17] conducted an independent literature\nreview to create a catalogue of Pythonic idioms. There are 27 detectable Pythonic idioms involving\nbuilt-in methods, APIs and syntax. Phan-udom et al. [ 36] first collected 58 non-idiomatic code\ninstances and 55 idiomatic. Subsequently, they provided Pythonic code examples akin to code in\nprojects of developer. Sakulniwat et al. [38] proposed a technique to visualize and understand the\nusage of the with Pythonic idiom and found developers tend to adopt the idiomatic code over\ntime. Dilhara et al. [15] mined frequent code changes in Python ML systems and found some of\ninvolving Pythonic idioms. RIdiom [53] first identified nine Pythonic idioms by comparing the\nsyntax difference between Python and Java and then designed detection and refactoring rules\nto automatically refactor non-idiomatic code into idiomatic code with the nine Pythonic idioms.\nLeelaprute et al. [27] analyzed the performance of Python features (e.g., collections.defaultdict and\nlambda) and two Pythonic idioms (list/dict comprehension) with different input sizes. Zhang et\nal. [52] conducted a large-scale empirical study for nine Pythonic idioms by creating a synthetic\ndataset and real-project dataset in RIdiom [53]. They found Pythonic idioms do not always result\nin performance speedup and can cause degraded performance. Zhang et al. [ 54] explored the\nchallenges in comprehending Pythonic idioms, their conciseness, and the potential impact of\ncomprehension challenges on code. They developed the DeIdiom tool to interpret these idioms into\nequivalent non-idiomatic code, facilitating developers in comprehending and effectively leveraging\nPythonic idioms. In this work, we focus one refactoring non-idiomatic code with Pythonic idioms.\nUnlike RIdiom [53], we do not rely on a rule-based approach; instead, we employ a hybrid approach\nutilizing APIs and prompts based on LLMs. Besides, we extend our approach to four new Pythonic\nidioms, which further verify the scalability of our approach.\nStudies on LLMs. Building on the achievements of Language Models (LLMs) like GPT-3 [9] and\nGPT-4 [34] in the field of Natural Language Processing [10, 12, 28, 37, 47–50], researchers are now\ndelving into their potential applications in software engineering [4, 14, 18, 20, 22, 24, 25, 29, 30, 35].\nHuang et al. [24] introduced a chain-of-thought approach based on LLMs, comprising four steps:\nextracting structure hierarchy, isolating nested code blocks, generating CFGs for these nested\nblocks, and amalgamating all CFGs. This approach surpasses existing CFG tools in terms of both\nnode and edge coverage, particularly for incomplete or erroneous code. Feng et al. [18] proposed a\ntwo-phase approach utilizing a chain-of-thought prompt to guide LLMs in extracting S2R entities,\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:19\nfollowed by matching these entities with GUI states to replicate the bug reproduction steps. This\ndemonstrates that instructing LLMs through prompts can effectively achieve bug replay. Peng et\nal. [35] presented TYPEGEN, which generates prompts incorporating domain knowledge, then\nfeeds them into LLMs for type prediction. This approach uses few annotated examples to achieve\nsuperior performance compared to rule-based type inference approaches. In this work, we focus on\nmaking Python code idiomatic with Pythonic idioms. Unlike previous approaches that exclusively\ndesign prompts for instructing LLMs to generate code or complete tasks, we propose a hybrid\napproach using APIs generated by LLMs to extract three elements, along with prompts to guide\nLLMs in performing the idiomatization task. It demonstrates that LLMs and rule-based approaches\ncomplement each other which can assist researchers to solve software engineering tasks better.\n7 CONCLUSION AND FUTURE WORK\nRefactoring non-idiomatic code with Pythonic idioms is not easy because of three challenges\nincluding code miss, wrong detection and wrong refactoring. Depending solely on Large Language\nModels (LLMs) or a rule-based approach (RIdiom) has its limitations in addressing these challenges.\nTo alleviate the challenges, we propose a hybrid approach based on LLMs to refactor non-idiomatic\ncode with Pythonic idioms. In detail, we first extract three elements of a Pythonic idiom and\ncreate an API library by prompting LLMs to generate code. We then invoke the APIs to extract\nnon-idiomatic code in the extraction module. Finally, we prompt LLMs to abstract code, idiomatize\nthe abstract code and then invoke APIs to rewrite non-idiomatic code into idiomatic code. The\nresults of our experiments, conducted on nine Pythonic idioms from RIdiom [53], as well as four\nnew Pythonic idioms [17] not covered by RIdiom, demonstrate high levels of accuracy, F1-score,\nprecision, and recall. This substantiates the effectiveness and scalability of our proposed approach.\nIn the future, we will keep improving our approach and extend our approach to accommodate\nPython code with syntax errors. Besides, we plan to offer explanations regarding the impacts, such\nas enhanced readability and performance, that result from refactoring code into idiomatic Python.\n8 DATA AVAILABILITY\nOur replication package can be found here [5].\nREFERENCES\n[1] 2022. Programming Idioms . https://programming-idioms.org/\n[2] 2023. GPT. https://platform.openai.com/docs/guides/gpt\n[3] 2023. Introducing ChatGPT . https://chat.openai.com/\n[4] 2023. OpenAI Codex . https://openai.com/blog/openai-codex\n[5] 2023. Replication Package . https://github.com/idiomaticrefactoring/IdiomatizationLLM\n[6] Carol V Alexandru, José J Merchante, Sebastiano Panichella, Sebastian Proksch, Harald C Gall, and Gregorio Robles.\n2018. On the usage of pythonic idioms. In Proceedings of the 2018 ACM SIGPLAN International Symposium on New\nIdeas, New Paradigms, and Reflections on Programming and Software . 1–11.\n[7] D. Bader. 2017. Python Tricks: A Buffet of Awesome Python Features . BookBaby. https://books.google.co.in/books?id=\nC0VKDwAAQBAJ\n[8] D. Beazley and B.K. Jones. 2013. Python Cookbook: 3rd Edition . O’Reilly Media, Incorporated. https://books.google.\ncom.au/books?id=oBKwkgEACAAJ\n[9] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL]\n[10] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. CodeT: Code\nGeneration with Generated Tests. arXiv:2207.10397 [cs.CL]\n[11] Quantified Code. 2014. The Little Book of Python Anti-Patterns . https://github.com/quantifiedcode/python-anti-patterns\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:20 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\n[12] Antonia Creswell, Murray Shanahan, and Irina Higgins. 2022. Selection-Inference: Exploiting Large Language Models\nfor Interpretable Logical Reasoning. https://doi.org/10.48550/arXiv.2205.09712\n[13] Python developers. 2000. Python Enhancement Proposals . https://peps.python.org/pep-0000/\n[14] Malinda Dilhara, Abhiram Bellur, Timofey Bryksin, and Danny Dig. 2024. Unprecedented Code Change Automation:\nThe Fusion of LLMs and Transformation by Example. arXiv:2402.07138 [cs.SE]\n[15] Malinda Dilhara, Danny Dig, and Ameya Ketkar. 2023. PYEVOLVE: Automating Frequent Code Changes in Python\nML Systems. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE, 995–1007.\n[16] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023. Self-collaboration Code Generation via ChatGPT.\narXiv:2304.07590 [cs.SE]\n[17] Aamir Farooq and Vadim Zaytsev. 2021. There is More than One Way to Zen Your Python. InProceedings of the 14th\nACM SIGPLAN International Conference on Software Language Engineering . 68–82.\n[18] Sidong Feng and Chunyang Chen. 2023. Prompting Is All You Need: Automated Android Bug Replay with Large\nLanguage Models. arXiv:2306.01987 [cs.SE]\n[19] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettle-\nmoyer, and Mike Lewis. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. arXiv:2204.05999 [cs.SE]\n[20] Michael Fu, Chakkrit Tantithamthavorn, Trung Le, Van Nguyen, and Dinh Phung. 2022. VulRepair: A T5-Based\nAutomated Software Vulnerability Repair. InProceedings of the 30th ACM Joint European Software Engineering Conference\nand Symposium on the Foundations of Software Engineering (Singapore, Singapore) (ESEC/FSE 2022) . Association for\nComputing Machinery, New York, NY, USA, 935–947. https://doi.org/10.1145/3540250.3549098\n[21] Raymond Hettinger. 2013. Transforming code into beautiful, idiomatic Python . https://www.youtube.com/watch?v=\nOSGv2VnC0go\n[22] Qing Huang, Zhiqiang Yuan, Zhenchang Xing, Xiwei Xu, Liming Zhu, and Qinghua Lu. 2023. Prompt-Tuned Code\nLanguage Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code. In Proceedings of the\n37th IEEE/ACM International Conference on Automated Software Engineering (Rochester, MI, USA)(ASE ’22). Association\nfor Computing Machinery, New York, NY, USA, Article 79, 13 pages. https://doi.org/10.1145/3551349.3556912\n[23] Qing Huang, Jiahui Zhu, Zhenchang Xing, Huan Jin, Changjing Wang, and Xiwei Xu. 2023. A Chain of AI-based\nSolutions for Resolving FQNs and Fixing Syntax Errors in Partial Code. arXiv:2306.11981 [cs.SE]\n[24] Qing Huang, Zhou Zou, Zhenchang Xing, Zhenkang Zuo, Xiwei Xu, and Qinghua Lu. 2023. AI Chain on Large Language\nModel for Unsupervised Control Flow Graph Generation for Statically-Typed Partial Code. arXiv:2306.00757 [cs.SE]\n[25] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul\nSharma. 2022. Jigsaw: Large Language Models Meet Program Synthesis. In Proceedings of the 44th International\nConference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE ’22) . Association for Computing Machinery, New\nYork, NY, USA, 1219–1231. https://doi.org/10.1145/3510003.3510203\n[26] Jeff Knupp. 2013. Writing Idiomatic Python 3.3 . Jeff Knupp.\n[27] Pattara Leelaprute, Bodin Chinthanet, Supatsara Wattanakriengkrai, Raula Gaikovina Kula, Pongchai Jaisri, and\nTakashi Ishio. 2022. Does coding in pythonic zen peak performance? preliminary experiments of nine pythonic idioms\nat scale. In Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension . 575–579.\n[28] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K. Lahiri, and Siddhartha Sen. 2023. CodaMosa: Escaping Coverage\nPlateaus in Test Generation with Pre-trained Large Language Models. In 2023 IEEE/ACM 45th International Conference\non Software Engineering (ICSE) . 919–931. https://doi.org/10.1109/ICSE48619.2023.00085\n[29] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Ré mi Leblond, Tom Eccles, James\nKeeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,\nXinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz,\nEsme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-\nlevel code generation with AlphaCode.Science 378, 6624 (dec 2022), 1092–1097. https://doi.org/10.1126/science.abq1158\n[30] Zhe Liu, Chunyang Chen, Junjie Wang, Xing Che, Yuekai Huang, Jun Hu, and Qing Wang. 2023. Fill in the Blank:\nContext-Aware Automated Text Input Generation for Mobile GUI Testing. In Proceedings of the 45th International\nConference on Software Engineering (Melbourne, Victoria, Australia) (ICSE ’23) . IEEE Press, 1355–1367. https://doi.org/\n10.1109/ICSE48619.2023.00119\n[31] José J. Merchante. 2017. From Python to Pythonic: Searching for Python idioms in GitHub. https://api.semanticscholar.\norg/CorpusID:211530803\n[32] José Javier Merchante and Gregorio Robles. 2017. From Python to Pythonic: Searching for Python idioms in GitHub.\nIn Proceedings of the Seminar Series on Advanced Techniques and Tools for Software Evolution . 1–3.\n[33] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Haiquan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\n2022. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In International\nConference on Learning Representations . https://api.semanticscholar.org/CorpusID:252668917\n[34] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\nRefactoring to Pythonic Idioms: A Hybrid Knowledge-Driven Approach Leveraging Large Language Models 50:21\n[35] Yun Peng, Chaozheng Wang, Wenxuan Wang, Cuiyun Gao, and Michael R. Lyu. 2023. Generative Type Inference for\nPython. arXiv:2307.09163 [cs.SE]\n[36] Purit Phan-udom, Naruedon Wattanakul, Tattiya Sakulniwat, Chaiyong Ragkhitwetsagul, Thanwadee Sunetnanta,\nMorakot Choetkiertikul, and Raula Gaikovina Kula. 2020. Teddy: Automatic Recommendation of Pythonic Idiom\nUsage For Pull-Based Software Projects. In 2020 IEEE International Conference on Software Maintenance and Evolution\n(ICSME). IEEE, 806–809.\n[37] Xiaoxue Ren, Xinyuan Ye, Dehai Zhao, Zhenchang Xing, and Xiaohu Yang. 2023. From Misuse to Mastery: Enhancing\nCode Generation with Knowledge-Driven AI Chaining. In 2023 38th IEEE/ACM International Conference on Automated\nSoftware Engineering (ASE). IEEE Computer Society, Los Alamitos, CA, USA, 976–987. https://doi.org/10.1109/ASE56229.\n2023.00143\n[38] Tattiya Sakulniwat, Raula Gaikovina Kula, Chaiyong Ragkhitwetsagul, Morakot Choetkiertikul, Thanwadee Sunetnanta,\nDong Wang, Takashi Ishio, and Kenichi Matsumoto. 2019. Visualizing the usage of pythonic idioms over time: A\ncase study of the with open idiom. In 2019 10th International Workshop on Empirical Software Engineering in Practice\n(IWESEP). IEEE, 43–435.\n[39] Danilo Silva and Marco Tulio Valente. 2017. RefDiff: Detecting refactorings in version histories. In 2017 IEEE/ACM\n14th International Conference on Mining Software Repositories (MSR) . IEEE, 269–279.\n[40] Brett Slatkin. 2020. Effective Python : 90 specific ways to write better Python / Brett Slatkin. (second edition. ed.).\nAddison-Wesley, Place of publication not identified.\n[41] Mark Summerfield. 2009. Programming in Python 3: A Complete Introduction to the Python Language (2nd ed.).\nAddison-Wesley Professional.\n[42] Balázs Szalontai, Ákos Kukucska, András Vadász, Balázs Pintér, and Tibor Gregorics. 2023. Localizing and Idiomatizing\nNonidiomatic Python Code with Deep Learning. InIntelligent Computing, Kohei Arai (Ed.). Springer Nature Switzerland,\nCham, 683–702.\n[43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem\nCucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,\nYinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,\nJeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,\nXiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov,\nand Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]\n[44] Nikolaos Tsantalis, Matin Mansouri, Laleh M Eshkevari, Davood Mazinanian, and Danny Dig. 2018. Accurate and\nefficient refactoring detection in commit history. In Proceedings of the 40th international conference on software\nengineering. 483–494.\n[45] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation vs. Experience: Evaluating the Usability\nof Code Generation Tools Powered by Large Language Models. In CHI ’22: CHI Conference on Human Factors in\nComputing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, Extended Abstracts , Simone D. J. Barbosa, Cliff\nLampe, Caroline Appert, and David A. Shamma (Eds.). ACM, 332:1–332:7. https://doi.org/10.1145/3491101.3519665\n[46] Anthony J Viera, Joanne M Garrett, et al. 2005. Understanding interobserver agreement: the kappa statistic. Fam med\n37, 5 (2005), 360–363.\n[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.\n2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL]\n[48] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. AI Chains: Transparent and Controllable Human-AI\nInteraction by Chaining Large Language Model Prompts. In Proceedings of the 2022 CHI Conference on Human Factors\nin Computing Systems (New Orleans, LA, USA) (CHI ’22) . Association for Computing Machinery, New York, NY, USA,\nArticle 385, 22 pages. https://doi.org/10.1145/3491102.3517582\n[49] Tongshuang Wu, Michael Terry, and Carrie J. Cai. 2022. AI Chains: Transparent and Controllable Human-AI Interaction\nby Chaining Large Language Model Prompts. arXiv:2110.01691 [cs.HC]\n[50] Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022. Re3: Generating Longer Stories With Recur-\nsive Reprompting and Revision. In Conference on Empirical Methods in Natural Language Processing . https:\n//api.semanticscholar.org/CorpusID:252873593\n[51] Zejun Zhang, Zhenchang Xing, Xin Xia, Xiwei Xu, and Liming Zhu. 2022. Making Python code idiomatic by automatic\nrefactoring non-idiomatic Python code with pythonic idioms. In Proceedings of the 30th ACM Joint European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022) . Association for\nComputing Machinery, New York, NY, USA, 696–708. https://doi.org/10.1145/3540250.3549143\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.\n50:22 Zejun Zhang, Zhenchang Xing, Xiaoxue Ren, Qinghua Lu, and Xiwei Xu\n[52] Zejun Zhang, Zhenchang Xing, Xin Xia, Xiwei Xu, Liming Zhu, and Qinghua Lu. 2023. Faster or Slower? Performance\nMystery of Python Idioms Unveiled with Empirical Evidence. In Proceedings of the 45th International Conference on\nSoftware Engineering (Melbourne, Victoria, Australia) (ICSE ’23) . IEEE Press, 1495–1507. https://doi.org/10.1109/\nICSE48619.2023.00130\n[53] Zejun Zhang, Zhenchang Xing, Xiwei Xu, and Liming Zhu. 2023. RIdiom: Automatically Refactoring Non-Idiomatic\nPython Code with Pythonic Idioms. In2023 IEEE/ACM 45th International Conference on Software Engineering: Companion\nProceedings (ICSE-Companion) . IEEE, 102–106.\n[54] Zejun Zhang, Zhenchang Xing, Dehai Zhao, Qinghua Lu, Xiwei Xu, and Liming Zhu. 2024. Hard to Read and Understand\nPythonic Idioms? DeIdiom and Explain Them in Non-Idiomatic Equivalent Code. In 2024 IEEE/ACM 46th International\nConference on Software Engineering (ICSE ’24) (, Lisbon, Portugal,) (ICSE 2024) . Association for Computing Machinery,\nNew York, NY, USA. https://doi.org/10.1145/3597503.3639101\nReceived 2023-09-29; accepted 2024-01-23\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 50. Publication date: July 2024.",
  "topic": "Code refactoring",
  "concepts": [
    {
      "name": "Code refactoring",
      "score": 0.8354290127754211
    },
    {
      "name": "Computer science",
      "score": 0.7131122946739197
    },
    {
      "name": "Natural language processing",
      "score": 0.5176925659179688
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4210207760334015
    },
    {
      "name": "Programming language",
      "score": 0.38187938928604126
    },
    {
      "name": "Linguistics",
      "score": 0.32153820991516113
    },
    {
      "name": "Software",
      "score": 0.1309225857257843
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118347636",
      "name": "Australian National University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I42894916",
      "name": "Data61",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1292875679",
      "name": "Commonwealth Scientific and Industrial Research Organisation",
      "country": "AU"
    }
  ]
}