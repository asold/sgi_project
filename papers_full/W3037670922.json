{
  "title": "Video-Grounded Dialogues with Pretrained Generation Language Models",
  "url": "https://openalex.org/W3037670922",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2156084543",
      "name": "Le, Hung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224947216",
      "name": "Hoi, Steven C. H.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2810643877",
    "https://openalex.org/W2962934715",
    "https://openalex.org/W2963174698",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W2972777589",
    "https://openalex.org/W3015746101",
    "https://openalex.org/W2967674528",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2337252826",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W2970869018",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns. We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network. Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.",
  "full_text": "Video-Grounded Dialogues with Pretrained Generation Language Models\nHung Le†∗, Steven C.H. Hoi†‡\n†Singapore Management University\nhungle.2018@phdcs.smu.edu.sg\n‡Salesforce Research Asia\nshoi@salesforce.com\nAbstract\nPre-trained language models have shown re-\nmarkable success in improving various down-\nstream NLP tasks due to their ability to cap-\nture dependencies in textual data and gener-\nate natural responses. In this paper, we lever-\nage the power of pre-trained language mod-\nels for improving video-grounded dialogue,\nwhich is very challenging and involves com-\nplex features of different dynamics: (1) Video\nfeatures which can extend across both spatial\nand temporal dimensions; and (2) Dialogue\nfeatures which involve semantic dependencies\nover multiple dialogue turns. We propose\na framework by extending GPT-2 models to\ntackle these challenges by formulating video-\ngrounded dialogue tasks as a sequence-to-\nsequence task, combining both visual and tex-\ntual representation into a structured sequence,\nand ﬁne-tuning a large pre-trained GPT-2 net-\nwork. Our framework allows ﬁne-tuning lan-\nguage models to capture dependencies across\nmultiple modalities over different levels of in-\nformation: spatio-temporal level in video and\ntoken-sentence level in dialogue context. We\nachieve promising improvement on the Audio-\nVisual Scene-Aware Dialogues (A VSD) bench-\nmark from DSTC7, which supports a potential\ndirection in this line of research.\n1 Introduction\nRecent work in large-scale pre-training transformer-\nbased neural networks (Liu et al., 2019; Devlin\net al., 2019; Radford et al., 2019) has boosted\nthe performance in various NLP tasks. The\ntransformer-based architecture of these models al-\nlows them to capture various dependencies when\ntrained on very large datasets. The pre-trained mod-\nels are adapted into downstream tasks to generate\ntext that is more natural, ﬂuent, and richer than\n∗This work was mostly done when Hung Le was an intern\nat Salesforce Research Asia, Singapore.\nmodels not initialized with pre-trained weights.\nSimilar to pre-trained CNN-based neural networks\ndeveloped in computer vision research (He et al.,\n2016; Huang et al., 2017) which can learn high-\nresolution features in images, pre-trained language\nmodels (LMs) are capable of capturing ﬁne-grain\ntextual dependencies in text data of rich semantics.\nWhile the beneﬁts of pre-trained language mod-\nels present in many downstream NLP tasks such as\nmachine translation and question answering (QA)\n(Devlin et al., 2019; Lan et al., 2020), they are par-\nticularly suitable to adapt to dialogue response gen-\neration tasks for two major reasons: (1) Dialogue\nresponse generation usually involves more complex\ndynamics between input and output text sequences.\nThe input typically involves dialogue history, in-\ncluding conversational exchanges between users\nand dialogue agents. A dialogue agent needs to\ncapture relevant dependencies along each dialogue\nturns to generate a sensible response. (2) Compared\nto other NLP tasks, it is very challenging to collect\nand create large-scale dialogue datasets. Adopting\npre-training approaches could ameliorate the lim-\nited dialogue datasets by leveraging rich linguistic\ndependencies learned from other available text data.\nWe are motivated by these observations to adapt\npre-trained language models into a dialogue task\nand improve the quality of generated responses.\nAlong the line of research that combines both\nvision and language (Antol et al., 2015; Hori et al.,\n2019), transformer-based neural networks can also\nbe applied to capture various dependencies across\ndifferent types of input modalities (text and image)\nwith appropriate objective loss functions (Alberti\net al., 2019; Su et al., 2020; Chen et al., 2019). The\nmulti-head attention mechanism of these models\ncan detect long-range dependencies between each\ntoken in the input text and each image patch or\nspatial objects in the input image. We extend this\nframework to a video-dialogue task and fully lever-\narXiv:2006.15319v1  [cs.CL]  27 Jun 2020\nVideo\nhow man\ny\npeo\nple are in the vide\no SEPSEP ther\ne\npers\nononejustis\nPre-trained Transformer\nusr usr usr usr usr usr usr sysusr sys syssyssyssys\nDialogue History Response\n8 9 10 11 12 13 147 40\nWord \nLevel\nModality \nLevel\nPosition \nLevel\nther\ne is pers\nononejust\n5 5 5 5 5 5 5 55 5 5555 Turn \nLevel\nSEP\nsys\n2 3 4 5 61\n1\na \nman\n is stan\nding\nSEP\ncap cap cap capcap\n0 0 0 00\n42 43 44 4541\nCaption\n52\nvis visvis\n47 48 49 50 5146\n-1 -1-1\nvis vis vis visvis\n-2 -2 -2 -1-2\n53\nSpatial \nLevel\nModality \nLevel\nPosition \nLevel\nTemporal \nLevel\nMASK\n...\n...\n...\n...MASK\nEOS\n...\nMASK MASK MASK MASK MASK MASK MASKMASKMASK MASKMASKMASK MASK MASK MASKMASKMASK MASK MASKMASK\nt=0 t=T\nFigure 1: The proposed VGD-GPT2 architecture for video-grounded dialogues based on the pre-trained trans-\nformer model (GPT-2). The video and text input are combined together over multiple encoding layers to inject\ndifferent attributes to encoded features.\nage the power of pre-trained models to obtain lin-\nguistic and visual representations in dialogues and\nvideos. Speciﬁcally, we tackle the Audio-Visual\nScene-aware Dialogues (A VSD) task (Hori et al.,\n2019) which aims to generate dialogue responses\ngrounded on both visual and audio features of the\nvideo. The dialogue agent needs to create responses\nthat not only match the dialogue ﬂow but also ad-\ndress user questions about a given video over mul-\ntiple dialogue turns.\nFirst, we detail how to formulate input compo-\nnents of a video-grounded dialogue as a down-\nstream task of pre-trained language models. We fol-\nlow the general sequence-to-sequence framework,\nwhereby the input components are combined to a\nstructured sequence of multiple modalities and out-\nput is a system response. We then apply pre-trained\nmodels (Radford et al., 2019) to leverage the deep\nattention neural networks to capture text and video\ndependencies with ﬁne granularity. Speciﬁcally,\nwe propose to capture dependencies between each\ntoken in text data and each spatial feature along\nthe temporal dimension of the input video. Lastly,\nwe present a multi-task learning framework that\nincludes additional learning objectives in addition\nto dialogue response generation objective. Our\npromising results on the A VSD benchmark demon-\nstrate the efﬁcacy of our proposed framework.\n2 Related Work\nWe brieﬂy describe related work in two major lines\nof research: dialogues and vision-text modeling.\n2.1 Dialogue Modeling\nWhang et al. (2019) applies pre-trained language\nmodels for response selection tasks in open-domain\ndialogues. The output of the language model (e.g.\n[CLS] token in BERT) is used as a contextual repre-\nsentation of each pair of dialogue context and can-\ndidate response. Budzianowski and Vuli ´c (2019)\nassumes access to ground-truth dialogue states\nand generates responses in task-oriented dialogues\nby combining input components into a single se-\nquence. As dialogue states and database states are\nused as raw text input, the models can be ﬁne-tuned\nfrom a deep pre-trained language model such as\nGPT. Chao and Lane (2019) and Lai et al. (2020)\nuse pre-trained LMs to track dialogue states in task-\noriented dialogues by utilizing the output represen-\ntations to predict slot values. In this work, we aim\nto address video-grounded dialogue tasks and gen-\nerate natural responses in an end-to-end manner.\n2.2 Vision-Text Modeling\nThe transformer-based neural architecture of pre-\ntrained language models has been used to learn\ncross-modal representations for vision-text NLP\ntasks. Li et al. (2019) uses a BERT-based architec-\nture to improve linguistic and visual representations\nfor image captioning tasks. Lu et al. (2019) follows\na similar approach to tackle visual QA but segre-\ngates the visual and text input components rather\ncombining both into a single sequence. Alberti et al.\n(2019) leverages a pre-trained BERT model to im-\nprove cross-modal representations in either early\nfusion or late fusion approach. We are motivated\nto extend this line of research to a video-based\nsetting. Video is considered much more compli-\ncated than image due to the additional temporal\nvariation across video frames. A related work to\nours is VideoBERT (Sun et al., 2019) which uti-\nlizes BERT models for video captioning. Instead\nof using visual features to represent video frames,\nVideoBERT transforms frame-level features into\nvisual tokens and uses them as raw text input to a\nBERT-based architecture.\n3 Method\nOur model architecture can be seen in Figure 1. We\nare inspired by Transformer-based LM approaches\nthat leverage different levels of features in text,\nsuch as word, character, and position levels. We\napply this principle and technique to overcome the\nchallenge in A VSD which involves multi-turn dia-\nlogue input combined with video input with spatial-\ntemporal variations. We propose to decompose\nvideos into patches but maintain a structured tem-\nporal sequence. This sequence is then directly com-\nbined with text inputs of dialogue which are also\narranged in a temporally ordered manner. This kind\nof feature reformulation is simple yet powerful as\nit allows explicit dependency learning across all\npairs of text tokens and video patches. Therefore,\nit can facilitate stronger signals to answer human\nqueries in greater granularities.\n3.1 Model Architecture\nWe trained a GPT model based on the GPT-2\n(Radford et al., 2019) architecture. The GPT-2\nmodel is based on the transformer network\n(Vaswani et al., 2017) which includes 12 to\n24 layers of masked multi-head attention on\nvery large text data. Following the success of\nGPT-2 in generation-based tasks, we adapt the\npower of GPT-2 pre-trained models to generate\nvideo-grounded dialogue responses and call our\nframework “VGD-GPT2”. First, we modify the\ninput components as a long sequence of video\nframes or video segments and dialogue turns.\nVideo Representations. Each video frame or\nvideo segment is further structured as a sequence\nof spatial regions, which can be extracted using a\npre-trained video model. For an input video V , we\ndenote the output of a pre-trained 2D CNN or 3D\nCNN video model as Zpre\nV ∈RF×P×demb where\ndemb is the feature dimension of the pre-trained\nvideo model, F is the resulting number of sam-\npled video frames or video segments, and P is the\nnumber of spatial regions in each video frame. We\nreshape ZV as a sequence of image patches and\npass it through a linear transformation with ReLU\nactivation to match the feature dimension d of pre-\ntrained language model:\nZspatial\nV = ReLU(Zpre\nV WV ) ∈RFP ×d (1)\nwhere WV ∈Rdemb×d. We denote this as spatial-\nlevel features of input video. As can be seen\nfrom Figure 1, we inject different types of input\nattributes into XV by adding three additional en-\ncoding layers:\n(1) Modality-level encoding that informs the type\nof information. We use a modality token “vis” to\nuniformly represent visual information type.\n(2) Temporal-level encoding that informs model\nthe frame-level (or segment-level) position of input\nfeatures.\n(3) Position-level encoding that incorporates the\nspatial-level ordering. This is equivalent to the po-\nsitional encoding of tokens in sentences seen in\nBERT-based language models.\nAll the three layers are trainable parameters to en-\nable models to learn the dynamics of input features.\nAll encoding layers are modeled to have the same\nfeature dimension d of the pre-trained model. We\ncombine all encoding layers through element-wise\nsummation, resulting in a rich video representation:\nZV = Zspatial\nV + Zmod\nV + Ztemporal\nV + Zpos\nV (2)\nText Representations. Similarly, we break down\ndialogue history H as sequence of dialogue\nturns H = ( H1, H2, ..., Ht) where t is the\ncurrent dialogue turn. Each dialogue turn is\nrepresented as a pair of user utterance U and\nsystem response S concatenated sequentially\nH = (( U1, S1), (U2, S2), ..., Ut)) (St is the\ntarget response that need to be generated by the\nmodels). Each utterance is then represented as\na sequence of tokens x so the dialogue history\ncan be represented as XH = (x1, x2, ..., xLH )\nand Y = St = ( y1, y2, ..., yLY ) where LH\nand LY are the total number of tokens in the\ndialogue history and target response respectively.\nFollowing the A VSD setting (Hori et al., 2019),\nwe utilize the text input of video caption C.\nThe video caption typically provides a linguistic\nsummary of the video in one or two sentences.\nThe caption can be represented as a sequence of\ntokens XC = (x1, x2, ..., xLC ). We combine all\ntext input sequences to form a single sequence\nXT = (XC, XH, Y−1) as input to the models. Y−1\nis the target response sequence shifted left by one\nposition to enable auto-regressive prediction of\noutput tokens. We denote embedded features as\nZtoken\nT as the token-level encoding layer of the text\ninput. Similar to video features, we add additional\nlayers to inject different attributes of XT (See\nFigure 1):\n(1) Modality-level encoding that differentiates\nsegments in XT. We use 3 different modality\ntokens: “cap”, “sys”, and “usr” to specify whether\nthe token in the corresponding position is part of\ninput caption, system responses, or user utterances.\n(2) Turn-level encoding that encodes the turn\nnumber of the token in the corresponding position.\n(3) Position-level encoding that is used to inject\nsignals of the token ordering.\nSimilar to video representation, the encoded in-\nput is combined through element-wise summation:\nZT = Ztoken\nT + Zmod\nT + Zturn\nT + Zpos\nT (3)\nWe concatenated bothZV and ZT to create a single\ninput sequence ZV Tof length (F ×P +LC +LH +\nLY ) and embedding dimension d. ZV Tis used as\ninput to a pre-trained GPT-2 for ﬁne-tuning.\n3.2 Optimization\nFollowing a similar strategy adopted by Wolf et al.\n(2019), we ﬁne-tune the models in a multi-task\nsetting with the following objectives:\n(1) Response Generation: this is a typical objective\nfunction that maximizes the likelihood of output\ntarget response conditioned on the source sequence.\n(2) Masked Multi-modal Modeling: we explore two\nloss functions: masked language modeling (MLM)\nand masked visual modeling (MVM). We mask\nboth tokens and spatial regions in video frames\nin training instances and require the model to re-\ngenerate them with the remaining inputs. MLM is\nlearned similarly as response generation by pass-\ning through a linear layer with softmax. MVM is\nlearned by minimizing the L1 loss in feature space\nbetween the output representation of the masked\nvisual region and the original input representation.\nBoth are passed through a linear transformation\nto the same dimensional space. This is similar to\nthe perceptual loss proposed by (Johnson et al.,\n2016; Dosovitskiy and Brox, 2016) for image style\ntransfer and image resolution tasks. We follow\nBERT (Devlin et al., 2019) and replace about 15%\nof tokens and image region inputs in each training\ninstance at random with a [MASK] token. The cor-\nresponding output representations are then used to\nrecover the original tokens or image regions.\n(3) Matching Video-Text Pair(MVT): for about\n15% of training instances, we adapt the pretrained\nlanguage model to the dialogue domain by replac-\ning the original input with an incorrect dialogue\nor video input at random. We use a special token\n[CLS] concatenated to the input sequence to learn\nthe contextual representation. The vector integrates\ncontextual cues through Transformer attention lay-\ners and the corresponding output representation is\nused to predict if the input video-text pair is correct.\n4 Experiments\n4.1 Experimental Testbed and Setup\nWe use the open-source implementation of the GPT-\n2 architecture and obtain pre-trained model check-\npoints 1. We experiment with two pre-trained GPT-\n2 models: small (S) and medium (M) (Radford\net al., 2019). We use Adam optimizer with a learn-\ning rate of 5e-5 based on grid search. We adopt\na learning rate decay schedule as similarly used\nby Vaswani et al. (2017). we set the weight on the\nresponse generation loss to be 1.5 times higher than\nthe other losses.\nWe experiment with the the video-grounded di-\nalogue task in the large-scale A VSD benchmark\nin DSTC7 (Hori et al., 2019). The A VSD bench-\nmark contains dialogues grounded on the Charades\nvideos (Sigurdsson et al., 2016). Each dialogue\nconsists of up to 10 dialogue turns, each turn in-\ncluding a user utterance and system response (See\nTable 1 for more details of the dataset).\nTo extract visual features, we used the 3D CNN-\nbased ResNext-101 (Xie et al., 2017) pre-trained\non Kinetics (Hara et al., 2018) to obtain the spatio-\ntemporal video features. We ﬁxed the batch size\nto 16 and the maximum sequence length compat-\nible with the corresponding GPT2 models. We\nsampled video features every 16 frames without\noverlapping. We trained up to 50 epochs on 4\nGPUs. We report the objective scores, including\nBLEU, METEOR, ROUGE-L, and CIDEr. We\ncompare system-generated responses with 6 refer-\nence ground-truth responses.\n# Train Val. Test\nDialogs 7,659 1,787 1,710\nTurns 153,180 35,740 13,490\nWords 1,450,754 339,006 110,252\nTable 1: Summary of DSTC7 A VSD.\n4.2 Results\nWe compare the proposed VGD-GPT2 model with\nthe following baseline models:\n(1) Baseline (Hori et al., 2019) proposes a novel\n1https://github.com/huggingface/\ntransfer-learning-conv-ai\nModel Spatial Temporal MLM MVM MVT BLEU1 BLEU2 BLEU3 BLEU4 METEOR ROUGE-L CIDEr\nBaseline ✓ 0.626 0.485 0.383 0.309 0.215 0.487 0.746\nA VSD Winner ✓ 0.718 0.584 0.478 0.394 0.267 0.563 1.094\nMTN ✓ 0.731 0.597 0.490 0.406 0.271 0.564 1.127\nVGD-GPT2 (S)✓ ✓ ✓ ✓ ✓ 0.750 0.621 0.516 0.433 0.283 0.581 1.196\nVGD-GPT2 (S)✓ ✓ ✓ ✓ 0.753 0.619 0.512 0.424 0.280 0.571 1.185\nVGD-GPT2 (S) ✓ ✓ ✓ ✓ 0.750 0.616 0.511 0.427 0.280 0.579 1.188\nVGD-GPT2 (S)✓ ✓ ✓ ✓ 0.745 0.613 0.508 0.423 0.281 0.579 1.173\nVGD-GPT2 (S)✓ ✓ ✓ ✓ 0.749 0.613 0.505 0.419 0.274 0.571 1.153\nVGD-GPT2 (S)✓ ✓ ✓ ✓ 0.744 0.612 0.505 0.421 0.281 0.581 1.192\nVGD-GPT2 (M)✓ ✓ ✓ ✓ ✓ 0.749 0.620 0.520 0.436 0.282 0.582 1.194\nTable 2: Evaluation on the A VSD benchmark of baselines and different variants of VGD-GPT2 based on: (1)\nvideo features in spatial or temporal (or both) dimension and (2) ﬁne-tuning objective functions: MLM - masked\nlanguage modeling, MVM: mask visual modeling, and MVT - matching video-text pair.\nsequence-to-sequence approach with question-\nguided LSTM on both video visual and audio tem-\nporal features. Dialogue history is encoded by a\nhierarchical LSTM and the ﬁnal representation is\nconcatenated with question and video representa-\ntions as input to decode dialog responses.\n(2) A VSD Winner(Sanabria et al., 2019) extends\nthe previous work with more reﬁned visual features\nand transfer learning from a video summary task.\n(3) MTN (Le et al., 2019) adopts a transformer-\nbased approach with question-guided attention on\nvisual features formulated as an auto-encoding\nmodule. Table 2 shows the details of our results.\nOur VGD-GPT2 model outperforms the exist-\ning approaches across all the automated metrics.\nThe results show that ﬁne-tuning a language model\nwith video-grounded dialogues can help to generate\nquality responses and improve model performance.\nBy initializing our models with a language model\npre-trained on massive text data, we obtain richer\nfeature representations that capture more complex\ndependencies between inputs.\nCompared with the baseline with Transformer-\nbased neural networks (Le et al., 2019), our model\ntreats both visual and text features with equal im-\nportance at different levels of different dimensions.\nSpeciﬁcally, we aligned the token level with spatial\nlevel and turn level with temporal level between\nvisual and text features. By contrast, MTN only\nconsiders the temporal variation of the visual fea-\ntures and mainly focuses on text-based attention.\nOur early fusion strategy with a multi-level align-\nment approach of multi-modal inputs allows higher\nresolution relations between all feature representa-\ntions in later layers of neural networks.\n4.3 Ablation Analysis\nBesides, Table 2 also shows that ﬁne-tuning a pre-\ntrained model with both spatial-temporal informa-\ntion and multi-task objectives can beneﬁt the main\ntask of response generation. To obtain spatial-only\nand temporal-only features, we follow a similar\napproach from (Jang et al., 2017) by using average\npooling to pool the visual features along the tem-\nporal or spatial dimensions. Considering CIDEr\nas the evaluation measure, learning dependencies\nin both spatial and temporal dimensions can im-\nprove the performance by 0.01 absolute score from\nspatial-only feature and 0.008 absolute score from\ntemporal-only feature.\nOur proposed auxiliary objectives also help to\nimprove model performance by adapting the pre-\ntrained model to the current data domain, video-\nbased dialogues. MLM and MVM are used to\nimprove learning of local dependencies in token\nand spatial levels, while MVT is used to support\nlearning global dependencies between text and vi-\nsual modalities. We observe that adding MVM\nobjective function can increase the CIDEr score\nthe most, by 0.043 absolute score, as compared\nto adding MVT (0.023 absolute score) or MLM\n(0.004 absolute score) objective function.\nWe also found moderate performance improve-\nments in BLEU3, BLEU4, and ROUGE-L, when\nincreasing GPT-2 from small to medium size. We\nnote that the increasing model parameters in GPT-\n2 may require longer ﬁne-tuning procedure or a\nlarger dialogue training dataset to fully optimize\nthe models in the dialogue domain.\n5 Conclusions\nIn this work, we leverage pre-trained language mod-\nels for a video-grounded dialogue task. We propose\na sequence-to-sequence framework and a multi-\ntask ﬁne-tuning approach to adapt the pre-trained\nmodels to the video dialogue domain. Despite us-\ning GPT-2 models, our framework can be extended\nwith other language models and similarly adopted\nto improve other multi-modal dialogues. Our early\nfusion strategy effectively uniﬁes different levels\nof features in both dialogues and video without\ncomplicating the network architecture.\nReferences\nChris Alberti, Jeffrey Ling, Michael Collins, and David\nReitter. 2019. Fusion of detected objects in text\nfor visual question answering. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2131–2140, Hong Kong,\nChina. Association for Computational Linguistics.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE international\nconference on computer vision, pages 2425–2433.\nPaweł Budzianowski and Ivan Vuli´c. 2019. Hello, it’s\nGPT-2 - how can I help you? towards the use of pre-\ntrained language models for task-oriented dialogue\nsystems. In Proceedings of the 3rd Workshop on\nNeural Generation and Translation, pages 15–22,\nHong Kong. Association for Computational Linguis-\ntics.\nGuan-Lin Chao and Ian Lane. 2019. Bert-dst: Scal-\nable end-to-end dialogue state tracking with bidi-\nrectional encoder representations from transformer.\narXiv preprint arXiv:1907.03040.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2019. Uniter: Learning univer-\nsal image-text representations. arXiv preprint\narXiv:1909.11740.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAlexey Dosovitskiy and Thomas Brox. 2016. Generat-\ning images with perceptual similarity metrics based\non deep networks. In Advances in neural informa-\ntion processing systems, pages 658–666.\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.\n2018. Can spatiotemporal 3d cnns retrace the his-\ntory of 2d cnns and imagenet? In Proceedings of\nthe IEEE conference on Computer Vision and Pat-\ntern Recognition, pages 6546–6555.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nC. Hori, H. Alamri, J. Wang, G. Wichern, T. Hori,\nA. Cherian, T. K. Marks, V . Cartillier, R. G. Lopes,\nA. Das, I. Essa, D. Batra, and D. Parikh. 2019. End-\nto-end audio visual scene-aware dialog using mul-\ntimodal attention-based video features. In ICASSP\n2019 - 2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),\npages 2352–2356.\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and\nKilian Q Weinberger. 2017. Densely connected con-\nvolutional networks. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4700–4708.\nYunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim,\nand Gunhee Kim. 2017. Tgif-qa: Toward spatio-\ntemporal reasoning in visual question answering. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2758–2766.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016.\nPerceptual losses for real-time style transfer and\nsuper-resolution. In European conference on com-\nputer vision, pages 694–711. Springer.\nTuan Manh Lai, Quan Hung Tran, Trung Bui, and\nDaisuke Kihara. 2020. A simple but effective bert\nmodel for dialog state tracking on resource-limited\nsystems. In ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 8034–8038. IEEE.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nHung Le, Doyen Sahoo, Nancy Chen, and Steven Hoi.\n2019. Multimodal transformer networks for end-to-\nend video-grounded dialogue systems. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 5612–5623,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nGen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and\nMing Zhou. 2019. Unicoder-vl: A universal encoder\nfor vision and language by cross-modal pre-training.\narXiv preprint arXiv:1908.06066.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, pages 13–23.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nRamon Sanabria, Shruti Palaskar, and Florian Metze.\n2019. Cmu sinbads submission for the dstc7 avsd\nchallenge. In DSTC7 at AAAI2019 workshop, vol-\nume 6.\nGunnar A Sigurdsson, G ¨ul Varol, Xiaolong Wang, Ali\nFarhadi, Ivan Laptev, and Abhinav Gupta. 2016.\nHollywood in homes: Crowdsourcing data collec-\ntion for activity understanding. In European Confer-\nence on Computer Vision, pages 510–526. Springer.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\nIn International Conference on Learning Represen-\ntations.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019. Videobert: A joint\nmodel for video and language representation learn-\ning. In The IEEE International Conference on Com-\nputer Vision (ICCV).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nTaesun Whang, Dongyub Lee, Chanhee Lee, Kisu\nYang, Dongsuk Oh, and HeuiSeok Lim. 2019. Do-\nmain adaptive training bert for response selection.\narXiv preprint arXiv:1908.04812.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents. arXiv preprint\narXiv:1901.08149.\nSaining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu,\nand Kaiming He. 2017. Aggregated residual trans-\nformations for deep neural networks. In Proceed-\nings of the IEEE conference on computer vision and\npattern recognition, pages 1492–1500.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8390250205993652
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7360372543334961
    },
    {
      "name": "Natural language processing",
      "score": 0.5908960700035095
    },
    {
      "name": "Artificial intelligence",
      "score": 0.586293637752533
    },
    {
      "name": "Language model",
      "score": 0.5748656392097473
    },
    {
      "name": "Sentence",
      "score": 0.5209335684776306
    },
    {
      "name": "Security token",
      "score": 0.5074347853660583
    },
    {
      "name": "Context (archaeology)",
      "score": 0.47760888934135437
    },
    {
      "name": "Sequence labeling",
      "score": 0.4673970341682434
    },
    {
      "name": "Modalities",
      "score": 0.46010836958885193
    },
    {
      "name": "Representation (politics)",
      "score": 0.44572627544403076
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4410703480243683
    },
    {
      "name": "Automatic summarization",
      "score": 0.4247840940952301
    },
    {
      "name": "Task (project management)",
      "score": 0.362379789352417
    },
    {
      "name": "Machine learning",
      "score": 0.32632848620414734
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79891267",
      "name": "Singapore Management University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    }
  ],
  "cited_by": 2
}