{
  "title": "GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model",
  "url": "https://openalex.org/W4385572416",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5006843261",
      "name": "Shicheng Tan",
      "affiliations": [
        "Anhui University"
      ]
    },
    {
      "id": "https://openalex.org/A5090754832",
      "name": "Weng Lam Tam",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5107562180",
      "name": "Yuanchun Wang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5101212445",
      "name": "Wenwen Gong",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5110990988",
      "name": "Shu Zhao",
      "affiliations": [
        "Anhui University"
      ]
    },
    {
      "id": "https://openalex.org/A5100364094",
      "name": "Peng Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5044791875",
      "name": "Jie Tang",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W2996834012",
    "https://openalex.org/W4291127200",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3177005875",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W3174510164",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4287854875",
    "https://openalex.org/W3152607317",
    "https://openalex.org/W4285269381",
    "https://openalex.org/W3213180921",
    "https://openalex.org/W3008219293",
    "https://openalex.org/W3110846353",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W3199246732",
    "https://openalex.org/W3200786561",
    "https://openalex.org/W2997666887",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3177378457",
    "https://openalex.org/W3200808010",
    "https://openalex.org/W2969601108",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3203532272",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3174394143",
    "https://openalex.org/W4385573711",
    "https://openalex.org/W3173482217",
    "https://openalex.org/W4303443398"
  ],
  "abstract": "Shicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Shu Zhao, Peng Zhang, Jie Tang. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 5: Industry Track, pages 134–148\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nGKD: A General Knowledge Distillation Framework for Large-scale\nPre-trained Language Model\nShicheng Tan∗1, Weng Lam Tam2, Yuanchun Wang3, Wenwen Gong 4,\nShu Zhao†1, Peng Zhang2, Jie Tang†4\n1Anhui University, 2Zhipu.AI, 3Renmin University of China, 4Tsinghua University\ntsctan@foxmail.com,{rainatam9784,frederickwang99}@gmail.com\nwenweng@mail.tsinghua.edu.cn,zhaoshuzs2002@hotmail.com\npeng.zhang@zhipuai.cn,jietang@tsinghua.edu.cn\nAbstract\nCurrently, the reduction in the parameter scale\nof large-scale pre-trained language models\n(PLMs) through knowledge distillation has\ngreatly facilitated their widespread deployment\non various devices. However, the deployment\nof knowledge distillation systems faces great\nchallenges in real-world industrial-strength ap-\nplications, which require the use of complex\ndistillation methods on even larger-scale PLMs\n(over 10B), limited by memory on GPUs and\nthe switching of methods. To overcome these\nchallenges, we propose GKD, a general knowl-\nedge distillation framework that supports dis-\ntillation on larger-scale PLMs using various\ndistillation methods. With GKD, developers\ncan build larger distillation models on memory-\nlimited GPUs and easily switch and combine\ndifferent distillation methods within a single\nframework. Experimental results show that\nGKD can support the distillation of at least\n100B-scale PLMs and 25 mainstream methods\non 8 NVIDIA A100 (40GB) GPUs. 1\n1 Introduction\nPre-trained language models, such as BERT (De-\nvlin et al., 2019), RoBERTa (Liu et al., 2019), and\ntheir variants, have achieved excellent success in\nnatural language processing (NLP) tasks when they\nusually have hundreds of millions of parameters.\nConsidering computationally expensive resource\nconstraints, a wide range of real-world applica-\ntions are often impeded. Knowledge distillation\n(Hinton et al., 2015), as a method for compressing\nlarge-scale pre-trained language models, is attract-\ning more and more attention. As large-scale PLMs\ncontinue to grow in scale, and with advancements\n*This work was done when the author visited Zhipu.AI.\n†Corresponding authors.\nThe other authors also include Yang Yang, Hongyin Tang,\nKeqing He, Jiahao Liu, and Jingang Wang from Meituan.\n1The code is available at https://github.com/aitsc/\nGLMKD.\nin knowledge distillation methods, it becomes in-\ncreasingly pressing to apply knowledge distillation\nresearch in controlled laboratory settings to the real\nworld.\nThe field of knowledge distillation for language\nmodels has witnessed a phenomenal progress in\nrecent years, particularly with regards to the reduc-\ntion of model size, leading to the development of\na plethora of sophisticated distillation techniques\n(Liu et al., 2022; Wu et al., 2022) and a comprehen-\nsive toolkit (Yang et al., 2020b). However, despite\nthese rich research outcomes, there are still major\nchallenges in deploying knowledge distillation sys-\ntems for real-world industrial-strength applications,\nincluding:\n• Obstacles to Distilling Ultra-large-scale\nPLMs. Contrary to distillation in controlled\nlaboratory settings aimed at models with bil-\nlions of parameters, many industrial-strength\napplications (Yu et al., 2022) rely on ultra-\nlarge-scale PLMs (on the order of 10B or\neven larger). The training of ultra-large-scale\nPLMs is already challenging, and the distilla-\ntion process requires simultaneous training of\nboth large and small models, leading directly\nto difficulties in distillation of ultra-large-scale\nPLMs. Furthermore, there are also methods\n(Wu et al., 2021a; Yuan et al., 2021) for distill-\ning multiple large models into a single small\nmodel, which pose significant challenges in\nmemory-constrained GPU environments.\n• Obstacles to Switching Distillation Meth-\nods. Deploying a knowledge distillation sys-\ntem requires the implementation of numerous\ndistillation methods to meet different require-\nments, but due to the differences in implemen-\ntation of these methods, it is difficult to switch\nand combine them easily within a framework.\nIt is important to have an architecture that\naccommodates a range of distillation meth-\n134\nods while ensuring efficient training, such as\navoiding excessive extraction of intermediate\nfeatures that lead to memory waste. Thus, a\ncompatible and efficient architecture is cru-\ncial for successful deployment of knowledge\ndistillation systems.\nTo overcome these challenges, we present a gen-\neral knowledge distillation framework (GKD) for\ndeploying knowledge distillation systems that sup-\nport various scale PLMs and methods. To over-\ncome the obstacles to distilling ultra-large-scale\nPLMs, GKD leverages the techniques of training\nlarge transformer models to the distillation process\nthat requires training multiple large (teacher) and\nsmall (student) models simultaneously, incorporat-\ning the latest model and data parallel strategies.\nTo overcome the obstacles to switching distillation\nmethods, GKD employs a dynamic hook mech-\nanism and auxiliary model to extract and operate\nintermediate layer features and inference process of\nmodels in each iteration. While being compatible\nwith various methods, it avoids the waste of mem-\nory caused by extracting all intermediate layers.\nGKD presents the first exploration of knowledge\ndistillation for language models in industrial sce-\nnarios. Specifically, our main contribution lies in:\n• Larger-scale Model Distillation. We pro-\npose a teacher-student parallel strategy based\non advanced memory optimization methods,\naddressing the challenge of distilling ultra-\nlarge-scale PLMs (over 10B) due to memory\nconstraints. The proposed strategy supports\ndistillation of at least 100B-scale PLMs on 8\nNVIDIA A100 (40GB) GPUs.\n• More Compatible Method Architecture.\nWe propose an efficient adaptive architecture\ncompatible with various methods, addressing\nthe challenge of switching and using different\ndistillation methods within a single framework\nwith difficulty. The proposed architecture sup-\nports at least 25 model distillation methods.\n• Easy-to-use Open Source Toolkit. We have\nopen-sourced the required toolkit for GKD,\nwhich provides a command-line interface for\n25 distillation methods, facilitating developers\nto deploy knowledge distillation systems for\nultra-large-scale PLMs.\n2 Related work\nIn recent years, knowledge distillation for com-\npressing PLMs has gained increased attention.\nThese works studied ways of better utilizing lan-\nguage model features for transferring knowledge\nfrom large teacher models to a smaller student\nmodel, involving hidden layers (Jiao et al., 2020),\nattention layers (Wang et al., 2021), soft labels\n(Jafari et al., 2021), and hard labels (Jafari et al.,\n2022). These works validated their methods with\nPLMs of hundreds of millions of parameters, such\nas BERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019), XLNet (Yang et al., 2019), etc. However,\ndeployment of the distillation system on GPUs with\nlimited memory has been hindered by the reliance\non ultra-large-scale PLMs (10B or even larger). An\noffline distillation method (Liu et al., 2021) that\nsaved teacher features before training the student\nindividually reduced memory pressure, but was\nlimited to methods with smaller feature scales and\nwithout teacher-student interaction. In this work,\nGKD was compatible with ultra-large-scale PLMs\ndistillation via the introduction of Megatron-LM\n(Shoeybi et al., 2019) based on model parallelism\nand Zero Redundancy Optimizer (ZeRO) (Rajbhan-\ndari et al., 2020) based on data parallelism.\nWhile some code for knowledge distillation\nmethods focused on language models was made\npublic (Sanh et al., 2019; Jiao et al., 2020; Sun\net al., 2020), there was a lack of a general frame-\nwork for deploying knowledge distillation sys-\ntems. TextBrewer (Yang et al., 2020b) packaged\nsome abstract and simple distillation processes and\nloss functions, but lacked implementation of many\nmethods and was difficult to adapt to increasingly\ncomplex distillation methods. There were signif-\nicant differences in the implementation of these\nmethods, such as DIITO (Wu et al., 2022) requiring\ndynamic intervention of the intermediate layer com-\nputation in the model; SID (Aguilar et al., 2020)\nchanging the intermediate layer features during\ntraining; Continuation-KD (Jafari et al., 2022) al-\ntering the loss calculation method as the epoch\nincreased, and so on. These differences in imple-\nmentation made it difficult for them to be easily\nswitched and combined within a single framework,\nhindering the application of various advanced meth-\nods in knowledge distillation systems. In this work,\nGKD accommodated various advanced knowledge\ndistillation methods through a dynamic hook mech-\nanism and auxiliary models.\n135\nUser requirements\nModel building \n \nModel training \n \nSome methods require multiple training\nStudent\nLogs\nDeployment\nN\nYIs it suitable?\nAnalysis\n(Parallel strategy) (Adaptive architecture)\nTask\nStudent\nMethod\nTeacher\nFigure 1: The framework of the GKD. From the user requirements to the model deployment on the device, the GKD\nincludes the six main processes involved in the deployment of the knowledge distillation system.\n3 GKD\nIn this section, we first introduce the overview\nframework of the proposed GKD, then delve into\nthe details of how GKD implements larger-scale\nmodel distillation and a more compatible method\narchitecture, from the perspective of model build-\ning and training.\n3.1 Overview Framework\nFigure 1 shows the overview framework of GKD,\nwhich consists of six main processes:\n(1) User Requirements:This process begins with\nthe user specifying their requirements and forming\na configuration file, which includes the choice of\ntraining task, distillation method, teacher model,\nstudent model, etc.\n(2) Model Building:This process addresses the\nobstacles to distilling ultra-large-scale PLMs by\nimplementing a teacher-student parallel strategy\nthat combines Megatron-LM (Shoeybi et al., 2019)\nand ZeRO (Rajbhandari et al., 2020). The process\ninvolves selecting and executing parameter initial-\nization strategies for the student model, such as\ninitializing the student model with a pre-trained\nstudent, a truncated parameter teacher, random ini-\ntialization methods, or other distilled students. It\nalso includes initializing the training data with a\ntokenizer.\n(3) Model Training:This process addresses the\nobstacles to switching distillation methods by im-\nplementing an efficient adaptive architecture that\nis compatible with various methods. This process\nincludes the initialization of methods to extract\nand compute different model features based on the\nrequirements of different methods at different itera-\ntion numbers.\n(4) Multiple Training:This process is utilized\nfor methods that require multiple training, such\nas task-specific methods (Jiao et al., 2020) that\nnecessitate distillation in the task-specific stage\nafter distillation in the pre-training stage.\n(5) Analysis: This process confirms the compli-\nance of the distilled student model with deployment\nrequirements through analysis, such as examining\nthe performance on the test set and other phenom-\nena that can be utilized to enhance the model.\n(6) Deployment: This process deploys the stu-\ndent model on the corresponding device, such as\nlow-computing mobile devices or services with\nhigher load deployment under equal computing\npower.\nThese six processes are performed in sequence\nto form the workflow of the knowledge distilla-\ntion system. The greatest contribution of GKD lies\nin the design of the model building and training,\nas the other processes do not pose a challenge to\nthe deployment of the knowledge distillation sys-\ntem. In the following sections, we will provide a\ndetailed description of how GKD enables larger-\nscale model distillation in the model building and\nmore compatible method architectures in the model\ntraining.\n3.2 Model Building\nThe challenge in building models lies in allocating\nultra-large-scale PLMs, consisting of a student and\none or more teacher models, on a GPU with only\nseveral tens of GB of memory. To address this\nchallenge, we propose a teacher-student parallel\nstrategy that splits the model parameters to differ-\nent GPUs while preserving the feature distance\ncomputation between the teacher and student mod-\nels. This strategy is inspired by the optimization of\nsingle ultra-large-scale PLMs, including Megatron-\nLM (Shoeybi et al., 2019) which splits each pa-\nrameter matrix in the transformer across multiple\nGPUs, and ZeRO (Rajbhandari et al., 2020) which\npartitions each layer of transformers sequentially\nacross multiple GPUs.\nAs shown in Figure 2, we demonstrate the com-\nparison between the previous strategy and our pro-\nposed teacher-student parallel strategy using an\nexample. The example includes the allocation of\n136\nModel\nParallel\nModel\nParallel\nData\nParallel \n(ZeRO)\nMicro-batch Student ModelTeacher Model Optimizer States\n(a) Previous strategy\n(b) Teacher-student parallel strategy\nData\nParallel\nFigure 2: This comparison between the previous strat-\negy and the proposed teacher-student parallel strategy\nis demonstrated through an example, where it can be\nobserved that the teacher-student parallel strategy sig-\nnificantly reduces the memory utilization of each GPU.\ntwo 6-layer transformer teacher models and one 4-\nlayer transformer student model on the GPU. The\ncurrent methods allocate all the model parameters\non each GPU, severely limiting the training of ultra-\nlarge-scale PLMs and multiple models. To reduce\nthe memory usage on each GPU without compro-\nmising the interaction between the teacher and the\nstudent, our teacher-student parallel strategy evenly\ndistributes the parameters of the teacher and student\non different GPUs, with each GPU corresponding\nto the matching parameters of the teacher and stu-\ndent. With the model parallel and data parallel\ncount being 2, the memory usage can be reduced\nby at least half. If utilizing ZeRO-Offload (Ren\net al., 2021), the optimizer states can further be\nstored in CPU memory to reduce the utilization of\nGPU memory.\n3.3 Model Training\nThe challenge in training models lies in how to\neasily switch and use different distillation meth-\nods within a single framework. To address this\nchallenge, we propose an efficient adaptive archi-\ntecture that is compatible with various methods. It\nimplements the operation of different methods and\nthe calculation of features through a dynamic hook\nMicro-batchExtraction hooks \nOperation hooks\nT TS\nAuxiliary model\nOptimize\ndata and hooks\nfeatures\nhooks\nloss\nFigure 3: A workflow for efficient adaptive architecture\ncompatible with various methods in a single iteration.\nmechanism and an auxiliary model, respectively.\nAs shown in the workflow in Figure 3, the dynamic\nhook mechanism constructs extraction hooks for\nextracting model features and operation hooks for\nmodifying the model inference process during each\niteration. These hooks are described by a config-\nuration file similar to JSON, which only requires\nrecording the operations required by the method\nand playing a role during the model inference pro-\ncess. The auxiliary model calculates the loss func-\ntion based on these hooks and the returned model\nfeatures. Table 1 describes the features that this\narchitecture can adapt to existing methods.\nIt is worth noting that GKD can achieve method\ncombination by integrating hooks from different\nmethods. GKD can also record all model features\nthrough extraction hooks and save the distance of\nteacher and student features in the auxiliary model\nfor later analysis of the correlation between feature\ndistance and task performance in the distillation\nprocess.\n4 Experiments\nIn this section, we verified that GKD, which is used\nfor distillation of language models, can support at\nleast 100B-scale parameters and 25 mainstream\nmethods on 8 NVIDIA A100 (40GB) GPUs.\n4.1 Experimental Setup\nDatasets All methods that require distillation\nin the pre-training stage use BooksCorpus (Zhu\net al., 2015) and English Wikipedia as training data\n(19GB). For the task-specific stage (fine-tuning),\nwe evaluate different distillation methods using the\nmore challenging SuperGLUE benchmark (Wang\n137\nCompatible features Representative methods\nModify the inference process of the\nmodel\nDIITO (Wu et al., 2022), LRC-BERT (Fu et al., 2021), Theseus (Xu et al., 2020)\nDynamically modify the feature extrac-\ntion or inference process\nSID (Aguilar et al., 2020), Theseus (Xu et al., 2020)\nAdditional trainable parametersTinyBERT (Jiao et al., 2020), RAIL-KD (Haidar et al., 2022), Universal-KD (Wu et al.,\n2021b), LRC-BERT (Fu et al., 2021)\nDynamically change loss functionAnnealing-KD (Jafari et al., 2021), Continuation-KD (Jafari et al., 2022), MobileBERT (Sun\net al., 2020)\nComplex intermediate layer calculationCKD (Park et al., 2021), MGSKD (Liu et al., 2022), ALP-KD (Passban et al., 2021)\nTrain student by multiple teachersTMKD (Yang et al., 2020a), MT-BERT (Wu et al., 2021a), RL-KD (Yuan et al., 2021),\nUncertainty (Li et al., 2021)\nMultiple training reduces teacher until\nstudent scale\nTAKD (Mirzadeh et al., 2020), DGKD (Son et al., 2021)\nOther simple methods KD (Hinton et al., 2015), PD (Turc et al., 2019), PKD (Sun et al., 2019), DistilBERT (Sanh\net al., 2019), MiniLM (Wang et al., 2020), MiniLMv2 (Wang et al., 2021)\nTable 1: The compatible features and representative methods of our proposed adaptive architecture.\net al., 2019).\nMethods We tested 22 distillation methods\nspecifically designed for language models, as well\nas three classic methods (KD, TAKD, and DGKD)\nfrom computer vision, which are listed in Ta-\nbles 1 and 2. The implementation of the teacher-\nstudent parallel strategy was carried out using the\nMegatron-LM (Shoeybi et al., 2019) and Deep-\nSpeed (Rasley et al., 2020) framework.\nModels The commonly used BERT (Devlin et al.,\n2019) lacks open-source ultra-large-scale PLMs,\nso we employed a more advanced GLM (Du et al.,\n2022), which boasts open-source models of 10B-\nscale or even 130B-scale (Zeng et al., 2023), signif-\nicantly reducing the deployment cost of the knowl-\nedge distillation system. The scale of teachers and\nstudents are presented in Tables 2 and 3.\nRefer to Appendix C for more implementation\ndetails.\n4.2 Results\nMore Compatible Method Architecture To ver-\nify the proposed adaptive architecture can effec-\ntively be compatible with various methods, we\ntested 25 mainstream distillation methods and\npresent the results in Table 2. The results demon-\nstrate that these methods can be easily switched and\nutilized in GKD. It is worth noting that TinyBERT\n(without data augmentation) outperformed all the\nlatest methods in our setup. This suggests that the\nlatest methods may not necessarily be the most ef-\nfective, and different requirements may necessitate\ndifferent methods. Additionally, the reliability of\nGKD is further validated from the perspective of\nloss function values in Appendix B.1.\nLarger-scale Model Distillation To verify the\nproposed teacher-student parallel strategy can sup-\nport distillation of 100B-scale model on 8 NVIDIA\nA100 (40GB) GPUs, we present the memory and\ntime consumption of different strategies for distill-\ning models of varying scale in Table 3. The results\nindicate that previous strategies encountered GPU\nmemory overflow when distilling 6B-scale models,\nwhereas our strategy is capable of supporting the\ndistillation of 100B-scale models. The results in\nrows 9, 10, and 11 respectively demonstrate that\nGPU memory consumption can be reduced through\nsplitting the model parameters, optimizer states, or\nstoring the optimizer states in CPU memory. If not\nlimited to 8 GPUs, our strategy has the potential\nto distill even larger models. Appendix B.2 further\nexamines the trade-off between memory and time\nconsumption.\n4.3 Further Exploration\nIn addition to compatibility with various methods,\nGKD also allows for effortless combination of dif-\nferent methods. In Appendix A.1, we have dis-\ncovered a method that achieves SOTA results by\ncombining the advantages of different distillation\nmethods. Appendix A.2 presents a tool that ana-\nlyzes the correlation between feature distance and\ntask performance through GKD, enhancing the in-\nterpretability of the distillation process.\n5 Conclusions\nIn this paper, we propose a general knowledge dis-\ntillation framework, GKD, for deploying knowl-\nedge distillation systems targeting large-scale\nPLMs. GKD satisfies the demands of real-world\napplications by employing a parallel strategy and\n138\nMethods ReCoRD COPA WSC RTE BoolQ WiC CB MultiRC avgF1/Acc. Acc. Acc. Acc. Acc. Acc. F1/Acc. F1 a/EM\nGLMBase(teacher, 110M) 72.80/72.17 66.00 77.88 72.92 79.39 66.14 88.19/91.07 72.32/26.34 71.72\nGLMLarge(teacher, 340M) 80.08/79.54 78.00 81.73 79.78 82.63 70.06 86.33/89.29 76.39/37.67 77.11\nSingle-teacher: Teacher (GLMBase)⇒Student (66M)\nKD (Hinton et al., 2015) 22.66/21.99 61.67 63.46 54.63 66.07 57.05 61.75/72.02 51.98/2.41 52.41\nPD (Turc et al., 2019) 54.36/53.59 65.67 66.67 59.45 69.82 59.20 80.13/81.55 65.97/15.29 62.03\nPKD (Sun et al., 2019) 61.77/60.99 60.00 65.38 68.83 77.73 65.78 82.76/85.12 69.99/22.67 66.17\nDistilBERT (Sanh et al., 2019)59.79/59.05 65.00 68.59 60.89 73.39 60.34 77.48/83.33 66.98/17.38 63.78\nTheseus (Xu et al., 2020) 57.07/56.33 61.67 66.35 68.11 77.81 64.37 89.14/87.50 69.08/21.79 66.09\nTinyBERT (Jiao et al., 2020) 65.60/64.88 70.3375.00 71.96 77.97 67.87 89.58/89.88 71.37/25.74 70.83\nMobileBERT†(Sun et al., 2020) 59.29/58.61 65.33 68.59 58.97 74.61 63.85 86.65/88.69 66.87/19.41 65.14\nSID (Aguilar et al., 2020) 27.17/26.19 65.00 65.06 58.12 69.33 57.16 51.02/73.81 59.26/14.55 55.08\nMiniLM (Wang et al., 2020) 60.00/59.24 62.00 63.46 67.63 75.88 64.99 67.63/79.17 67.36/19.66 63.81\nMiniLMv2 (Wang et al., 2021)60.88/60.16 62.00 62.82 66.67 76.73 63.69 66.38/76.79 68.68/21.65 63.65\nALP-KD (Passban et al., 2021)57.72/56.90 60.67 64.74 68.11 77.20 64.79 74.82/79.76 68.21/19.90 64.27\nLRC-BERT (Fu et al., 2021) 55.10/54.44 65.67 66.67 56.56 74.86 57.63 80.27/81.55 65.75/16.16 62.25\nAnnealing-KD (Jafari et al., 2021)56.08/55.39 69.33 66.67 58.97 70.57 59.82 85.78/85.12 66.26/13.92 63.33\nCKD (Park et al., 2021) 56.35/55.65 65.00 66.67 61.25 71.63 58.83 88.61/84.52 66.11/15.22 63.33\nUniversal-KD (Wu et al., 2021b)58.67/57.83 58.67 66.67 70.16 77.56 65.52 87.52/85.71 69.96/22.63 66.22\nDIITO (Wu et al., 2022) 63.71/63.0072.00 69.23 65.46 75.46 60.76 86.75/85.12 66.28/17.63 66.77\nContinuation-KD (Jafari et al., 2022)55.61/54.91 68.67 64.74 58.72 71.42 58.25 85.61/83.93 66.64/13.33 62.73\nRAIL-KD (Haidar et al., 2022)59.85/59.19 66.67 70.19 60.53 69.00 60.34 78.98/83.33 66.55/15.60 63.56\nMGSKD (Liu et al., 2022) 50.29/49.49 65.00 65.06 65.94 73.31 63.17 83.89/84.52 67.32/15.56 63.50\nMulti-teacher: Teachers (GLMBaseand GLMLarge)⇒Student (66M)\nTMKD (Yang et al., 2020a) 65.77/65.0970.33 63.14 66.91 75.37 63.38 70.22/79.17 68.76/22.77 65.63\nMT-BERT (Wu et al., 2021a)46.81/46.08 59.00 63.46 65.46 66.90 62.33 78.76/80.36 57.53/2.06 59.12\nRL-KD (Yuan et al., 2021) 59.78/58.99 58.33 66.03 69.07 77.93 65.78 76.87/82.74 69.24/22.21 65.26\nUncertainty (Li et al., 2021) 58.52/57.67 59.33 64.10 70.16 77.55 65.78 80.85/83.33 69.47/22.49 65.39\nTeacher assistants: Teacher (GLMLarge)⇒Assistant (200M)⇒Assistant (110M)⇒Student (66M)\nTAKD (Mirzadeh et al., 2020)25.50/24.69 60.33 66.03 55.11 66.39 57.94 76.28/76.79 55.90/1.50 54.52\nDGKD (Son et al., 2021) 23.68/22.96 61.00 66.99 55.96 65.71 58.73 75.45/75.60 48.06/1.50 54.00\nTable 2: Results of 25 mainstream distillation methods implemented using GKD on the SuperGLUE validation set.\nDue to the alteration of the model structure by MobileBERT†, the parameters of the teacher and student models are\n293M and 25M, respectively. ⇒denotes distillation process. The results for all methods were averaged over three\nrandom seeds.\nStrategy Teacher⇒Student (scale) MA (GB) CA (GB) Time (ms) Mem (GB) MP DP ZeRO Offload\nPrevious\n110M⇒22M 0.99 1.27 10.40 56.96 1 8\n110M⇒66M 1.73 2.02 10.82 57.60 1 8\n340M⇒66M 3.11 3.58 16.41 63.46 1 8\n5B⇒1B 32.44 36.57 53.34 61.58 1 8\n6B⇒1.2B GPU memory overflow 1 8\nOurs\n6B⇒1.2B 18.91 21.40 85.61 57.28 2 4\n7.5B⇒1.5B 24.22 27.36 87.08 60.44 2 4\n10B⇒2B 30.91 34.54 105.40 62.33 2 4\n10B⇒2B 18.45 22.56 119.72 68.68 2 4 ✓\n10B⇒2B 15.83 22.55 387.19 106.35 2 4 ✓ ✓\n25B⇒5B 20.41 23.51 379.38 63.07 8 1\n50B⇒10B 17.93 20.94 4570.54 230.27 8 1 ✓ ✓\n65B⇒13B 22.48 26.10 6412.11 293.11 8 1 ✓ ✓\n90B⇒18B 30.56 35.27 7193.26 373.81 8 1 ✓ ✓\n100B⇒20B 33.62 36.88 9081.97 410.83 8 1 ✓ ✓\n110B⇒22B GPU memory overflow 8 1 ✓ ✓\nTable 3: The consumption of memory and time during the pre-training stage of TinyBERT when distilling\nteacher models of different scales on 8 NVIDIA A100 (40GB) GPUs is presented. The micro batch and gradient\naccumulation steps are set to 1. Where MA denotes the maximum memory allocated on the GPU, CA denotes\nthe maximum cached memory on the GPU, Time denotes the time required to train each sample, Mem denotes\nthe size of occupied CPU memory, MP denotes the number of model parallelism, DP denotes the number of data\nparallelism, ZeRO denotes whether the optimizer states are partitioned across different GPUs, and Offload denotes\nwhether the optimizer states are stored in CPU memory.\n139\nadaptive architecture, allowing for the distillation\nof ultra-large scale PLMs (over 10B) and the switch\nof various advanced distillation methods. In the fu-\nture, we plan to launch our knowledge distillation\nsystem for facilitating the mass production and de-\nployment of student models.\nAcknowledgements\nThis work is supported by Technology and In-\nnovation Major Project of the Ministry of Sci-\nence and Technology of China under Grant\n2020AAA0108400 and 2020AAA0108402, the\nNatural Science Foundation of China under Grant\nNo. 61836013, the Major Program of the Na-\ntional Social Science Foundation of China un-\nder Grant No. 18ZDA032, and funds from CCF-\nZhipu.AI and Beijing Academy of Artificial Intel-\nligence (BAAI). The GPUs used are sponsored by\nZhipu.AI.\nReferences\nGustavo Aguilar, Yuan Ling, Yu Zhang, Benjamin Yao,\nXing Fan, and Chenlei Guo. 2020. Knowledge distil-\nlation from internal representations. In AAAI, pages\n7350–7357.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL, pages 4171–4186.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In ACL, pages 320–335.\nHao Fu, Shaojun Zhou, Qihong Yang, Junjie Tang, Gui-\nquan Liu, Kaikui Liu, and Xiaolong Li. 2021. Lrc-\nbert: Latent-representation contrastive knowledge\ndistillation for natural language understanding. In\nAAAI, pages 12830–12838.\nMd. Akmal Haidar, Nithin Anchuri, Mehdi Reza-\ngholizadeh, Abbas Ghaddar, Philippe Langlais, and\nPascal Poupart. 2022. Rail-kd: Random intermediate\nlayer mapping for knowledge distillation. In NAACL,\npages 1389–1400.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nAref Jafari, Ivan Kobyzev, Mehdi Rezagholizadeh, Pas-\ncal Poupart, and Ali Ghodsi. 2022. Continuation\nkd: Improved knowledge distillation through the\nlens of continuation optimization. In EMNLP, page\n5260–5269.\nAref Jafari, Mehdi Rezagholizadeh, Pranav Sharma, and\nAli Ghodsi. 2021. Annealing knowledge distillation.\nIn EACL, pages 2493–2504.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinybert: Distilling bert for natural language under-\nstanding. In EMNLP, pages 4163–4174.\nLei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou,\nand Xu Sun. 2021. Dynamic knowledge distillation\nfor pre-trained language models. In EMNLP, pages\n379–389.\nChang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan\nZhao. 2022. Multi-granularity structural knowledge\ndistillation for language model compression. In ACL,\npages 1001–1011.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYuanxin Liu, Fandong Meng, Zheng Lin, Weiping\nWang, and Jie Zhou. 2021. Marginal utility dimin-\nishes: Exploring the minimum knowledge for bert\nknowledge distillation. In ACL, pages 2928–2941.\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang\nLi, Nir Levine, Akihiro Matsukawa, and Hassan\nGhasemzadeh. 2020. Improved knowledge distilla-\ntion via teacher assistant. In AAAI, pages 5191–5198.\nGeondo Park, Gyeongman Kim, and Eunho Yang. 2021.\nDistilling linguistic context for language model com-\npression. In EMNLP, pages 364–378.\nPeyman Passban, Yimeng Wu, Mehdi Rezagholizadeh,\nand Qun Liu. 2021. Alp-kd: Attention-based layer\nprojection for knowledge distillation. In AAAI, pages\n13657–13665.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: memory optimizations\ntoward training trillion parameter models. In SC,\npage 20.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In KDD, pages 3505–\n3506.\nJie Ren, Samyam Rajbhandari, Reza Yazdani Am-\ninabadi, Olatunji Ruwase, Shuangyan Yang, Min-\njia Zhang, Dong Li, and Yuxiong He. 2021. Zero-\noffload: Democratizing billion-scale model training.\nIn ATC, pages 551–564. USENIX Association.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. CoRR,\nabs/1910.01108.\n140\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nWonchul Son, Jaemin Na, Junyong Choi, and Wonjun\nHwang. 2021. Densely guided knowledge distillation\nusing multiple teacher assistants. In ICCV, pages\n9375–9384.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. In EMNLP, pages 4322–4331.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices. In ACL, pages 2158–2170.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nThe impact of student initialization on knowledge\ndistillation. CoRR, abs/1908.08962.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In NeurIPS, pages 3261–3275.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2021. Minilmv2: Multi-head self-\nattention relation distillation for compressing pre-\ntrained transformers. In ACL, pages 2140–2151.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In NeurIPS.\nChuhan Wu, Fangzhao Wu, and Yongfeng Huang.\n2021a. One teacher is enough? pre-trained language\nmodel distillation from multiple teachers. In ACL,\npages 4408–4413.\nYimeng Wu, Mehdi Rezagholizadeh, Abbas Ghad-\ndar, Md. Akmal Haidar, and Ali Ghodsi. 2021b.\nUniversal-kd: Attention-based output-grounded in-\ntermediate layer knowledge distillation. In EMNLP,\npages 7649–7661.\nZhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa\nKreiss, Hanson Lu, Thomas Icard, Christopher Potts,\nand Noah D. Goodman. 2022. Causal distillation for\nlanguage models. In NAACL, pages 4288–4295.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. Bert-of-theseus: Compressing\nbert by progressive module replacing. In EMNLP,\npages 7859–7869.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and\nDaxin Jiang. 2020a. Model compression with two-\nstage multi-teacher knowledge distillation for web\nquestion answering system. In WSDM, pages 690–\n698.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS, pages 5754–5764.\nZiqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang\nChe, Ting Liu, Shijin Wang, and Guoping Hu. 2020b.\nTextbrewer: An open-source knowledge distillation\ntoolkit for natural language processing. In ACL,\npages 9–16.\nJifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Xinyu\nGuan, Jing Zhang, Lei Hou, Juanzi Li, and Jie Tang.\n2022. Xdai: A tuning-free framework for exploiting\npre-trained language models in knowledge grounded\ndialogue generation. In KDD, pages 4422–4432.\nFei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong,\nYan Fu, and Daxin Jiang. 2021. Reinforced multi-\nteacher selection for knowledge distillation. In AAAI,\npages 14284–14291.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan\nMa, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng\nZhang, Yuxiao Dong, and Jie Tang. 2023. GLM-\n130B: an open bilingual pre-trained model. In ICLR.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In ICCV, pages 19–27.\nIEEE Computer Society.\nA Further Exploration\nIn this section, we further explore the capabilities\nof GKD in combining different distillation methods\nand enhancing the interpretability of the distillation\nprocess.\nA.1 Method Combination\nThanks to the dynamic hook mechanism, GKD\nis capable of combining methods by integrating\nhooks from different methods. As shown in Table\n4, we demonstrate results from several dozen com-\nbinations of different model features. To conserve\ncomputational power, we set the batch size to 32\nduring pre-training and set the sizes of the teacher\nand student models to 110M and 22M, respectively.\nIn the task-specific stage, the batch size and learn-\ning rate were fixed at 16 and 1e-5, respectively,\nwithout the use of grid search and seed averaging.\nBased on the results in Table 4, the following con-\nclusions can be drawn.\n(1) We discovered the method BestC which\nachieves the SOTA, outperforming TinyBERT by\n1.24% on average in SuperGLUE. BestC combines\n141\nMethods Pre-training stage Task-specific stage SGEmb Att Q/K V HS Soft Hard Emb Att Q/K V HS Soft Hard\nKD Random initialization parameters CE CE 49.48\nTruncate fine-tuned teacher parameters CE CE 51.62\nCE CE CE CE 59.68\nCE CE CE 60.24\nKL CE CE 60.62\nKL CE 63.16\nMSE CE 63.46\nRAIL-KD Truncate fine-tuned teacher parameters MSE−f CE CE 51.63\nMiniLM KLf KLf CE 60.65\nMiniLMv2 KLf KLf CE 60.47\nKLf KLf KL KLf KLf CE 65.41\nKLf KLf KL CE 64.64\nMGSKDMSE MSE MSE MSE/HL MSE/HL KL 59.65\nTinyBERTMSE MSE MSE MSE MSE MSE CE 65.81\nMSE MSE MSE KL MSE MSE MSE CE 66.19\nMSE MSE MSE KL CE 62.75\nMSE MSE MSE CE 63.52\nMSE MSE KL MSE MSE CE 66.51\nMix5 MSE MSE+KLf KLf KLf MSE+Cos KL CE MSE MSE+KLf KLf KLf MSE CE CE 65.63\nMSE MSE+KLf KLf KLf MSE+Cos KL CE CE 62.18\nMSE MSE KL f KLf MSE+Cos KL CE MSE MSE KL f KLf MSE CE CE 66.58\nMSE MSE KL f KLf MSE+Cos KL CE CE 63.06\nMSE MSE+KLf KLf MSE+Cos KL CE MSE MSE+KLf KLf MSE CE CE 66.25\nMSE MSE+KLf KLf MSE+Cos KL CE CE 62.86\nMSE MSE+KLf KLf KLf MSE CE MSE MSE+KLf KLf KLf MSE CE CE 66.54\nMSE MSE+KLf KLf KLf MSE CE CE 64.64\nKLf KLf KLf KL CE KLf KLf KLf CE CE 64.68\nKLf KLf KLf KL CE CE 60.49\nBestC MSE KL f KLf MSE KL MSE KL f KLf MSE CE 67.05\nMSE KL f KLf MSE KL CE 62.71\nMSE KL f KLf MSEf KL MSE KL f KLf MSEf CE 65.73\nMSE KL f KLf MSEf KL CE 62.53\nMSE MSE f KL MSE MSE f CE 66.17\nMSE MSE f KL CE 62.58\nMSEf KL MSEf CE 65.79\nMSEf KL CE 63.69\nKLf KLf MSEf KL KLf KLf MSEf CE 66.01\nKLf KLf MSEf KL CE 63.87\nMSE KL f KLf KL MSE KL f KLf CE 66.53\nMSE KL f KLf KL CE 63.26\nMSE KL f KLf MSEf2 KL MSE KL f KLf MSEf2 CE 65.55\nMSE KL f KLf MSEf2 KL CE 62.56\nMSE KL f KLf MSE−f KL MSE KL f KLf MSE−f CE 66.22\nMSE KL f KLf MSE−f KL CE 63.26\nKLf KLf MSE KL KLf KLf MSE CE 66.78\nKLf KLf MSE KL CE 63.12\nTable 4: Results of combining various features of models using GKD on the SuperGLUE validation set. Emb, Att,\nQ/K, V , HS, Soft, Hard, and SG denote the output of the embedding layer, attention scores, query/key matrix, value\nmatrix, hidden state, soft labels, hard labels, and the average score on the SuperGLUE benchmark, respectively.\nMSE, KL, CE, Cos, and HL respectively denote the distance functions between the teacher and student features as\nmean squared error, Kullback-Leibler divergence, cross-entropy, cosine distance, and Huber loss. MSEf , MSE−f ,\nand MSEf2 respectively indicate the calculation of MSE for the last layer, before the last layer, and the second-to-last\nlayer’s hidden state. MSE+KLf represents the sum of MSE and KL calculated for the last layer’s attention scores.\nThe Mix5 method can be understood as a combination of the KD (Hinton et al., 2015), TinyBERT (Jiao et al., 2020),\nMiniLM (Wang et al., 2020), MiniLMv2 (Wang et al., 2021), and DistilBERT (Sanh et al., 2019) methods.\nthe features of TinyBERT, MiniLMv2, and soft la-\nbels. (2) The method that performs distillation in\nthe pre-training stage (row 5) outperforms those\nusing randomly initialized parameters (row 3) or\ntruncated fine-tuned teacher parameters (row 4) in\nthe pre-training stage. (3) The methods using soft\nlabels in the pre-training stage (rows 14 and 17)\noutperform those not using soft labels (rows 12\nand 16). (4) Starting from row 21, we compare\nthe results of various combinations distilled in the\ntask-specific stage and not distilled (only trained\non hard labels). We find that distillation in the task-\nspecific stage greatly improves the performance of\nthe task.\nA.2 Enhanced Interpretability\nThanks to the adaptive architecture, GKD can\nrecord all model features through extraction hooks\nand save the distance of teacher and student fea-\n142\nKL1 KL5 KL10 KL15 KL20 MSE\nSpearman\nAtt1\nAtt2\nAtt3\nAtt4\nAtt5\nAtt6\nEmb\nHS1\nHS2\nHS3\nHS4\nHS5\nHS6\nK1\nK2\nK3\nK4\nK5\nK6\nSoft\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nV1\nV2\nV3\nV4\nV5\nV6\n0.24\n0.95\n0.96\n0.93\n0.92\n0.96\n0.82\n0.7\n0.98\n0.98\n0.97\n0.97\n0.98\n-0.02\n0.56\n-0.08\n-0.53\n-0.26\n-0.39\n-0.15\n-0.86\n-0.86\n-0.85\n-0.38\n-0.45\n-0.57\n0.7\n0.94\n0.89\n0.8\n0.76\n0.84\n0.74\n0.96\n0.98\n0.97\n0.98\n0.98\n0.97\n0.69\n0.98\n0.98\n0.98\n0.98\n0.98\n0.04\n0.59\n0.31\n-0.46\n-0.26\n-0.37\n0.3\n-0.83\n-0.85\n-0.77\n-0.33\n-0.41\n-0.4\n0.71\n0.94\n0.89\n0.8\n0.76\n0.84\n0.7\n0.97\n0.98\n0.97\n0.98\n0.98\n0.97\n0.68\n0.98\n0.98\n0.98\n0.98\n0.98\n0.04\n0.59\n0.33\n-0.45\n-0.26\n-0.39\n0.36\n-0.83\n-0.85\n-0.76\n-0.33\n-0.41\n-0.38\n0.71\n0.94\n0.89\n0.8\n0.76\n0.85\n0.68\n0.97\n0.98\n0.97\n0.98\n0.98\n0.97\n0.68\n0.98\n0.98\n0.98\n0.98\n0.98\n0.04\n0.59\n0.33\n-0.45\n-0.26\n-0.4\n0.37\n-0.83\n-0.85\n-0.75\n-0.33\n-0.42\n-0.37\n0.71\n0.94\n0.89\n0.8\n0.76\n0.85\n0.67\n0.97\n0.98\n0.97\n0.98\n0.98\n0.97\n0.68\n0.98\n0.98\n0.98\n0.98\n0.98\n0.04\n0.59\n0.33\n-0.44\n-0.26\n-0.4\n0.38\n-0.83\n-0.85\n-0.75\n-0.33\n-0.42\n-0.37\n0.71\n0.94\n0.89\n0.8\n0.76\n0.85\n0.64\n0.98\n0.98\n0.98\n0.98\n0.99\n0.97\n0.68\n0.98\n0.98\n0.98\n0.98\n0.98\n0.02\n0.59\n0.34\n-0.45\n-0.26\n-0.4\n0.1\n-0.84\n-0.85\n-0.74\n-0.34\n-0.43\n-0.37\n0.7\n0.94\n0.89\n0.8\n0.76\n0.85\nDistance\nKL1 KL5 KL10 KL15 KL20 MSE\nSpearman\nEmb\nHS1\nHS2\nHS3\nHS4\nHS5\nHS6\nK1\nK2\nK3\nK4\nK5\nK6\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nV1\nV2\nV3\nV4\nV5\nV6\n0.24\n0.36\n0.93\n0.8\n0.7\n0.76\n0.75\n0.87\n0.92\n0.95\n0.96\n0.97\n0.98\n0.81\n0.96\n0.98\n0.98\n0.98\n0.98\n-\n0.02\n0.39\n0.55\n0.53\n0.28\n-0.56\n0.65\n0.33\n0.81\n0.97\n0.93\n0.93\n0.96\n0.77\n0.79\n0.76\n0.96\n0.94\n0.91\n0.35\n0.97\n0.97\n0.97\n0.97\n0.94\n0.04\n0.4\n0.8\n0.85\n0.61\n0.05\n0.87\n0.35\n0.79\n0.95\n0.96\n0.95\n0.96\n0.53\n0.77\n0.76\n0.95\n0.91\n0.87\n0.19\n0.96\n0.97\n0.97\n0.96\n0.83\n0.02\n0.39\n0.84\n0.89\n0.69\n0.11\n0.93\n0.36\n0.79\n0.93\n0.95\n0.95\n0.96\n0.05\n0.77\n0.76\n0.95\n0.9\n0.87\n0.21\n0.96\n0.97\n0.97\n0.96\n0.75\n0.02\n0.42\n0.86\n0.9\n0.72\n0.13\n0.95\n0.36\n0.79\n0.92\n0.95\n0.95\n0.95\n-\n0.18\n0.78\n0.77\n0.95\n0.89\n0.87\n0.24\n0.96\n0.97\n0.97\n0.96\n0.72\n0.03\n0.44\n0.87\n0.9\n0.73\n0.15\n0.9\n0.38\n0.7\n0.84\n0.9\n0.92\n0.91\n0.07\n0.93\n0.74\n0.94\n0.64\n0.78\n0.82\n0.96\n0.89\n0.9\n0.88\n0.73\n0.05\n0.49\n0.88\n0.92\n0.64\n-\n0.29\nDistance after pair-wise scaled dot-product\nKL1 KL5 KL10 KL15 KL20 MSE\nPearson\nAtt1\nAtt2\nAtt3\nAtt4\nAtt5\nAtt6\nEmb\nHS1\nHS2\nHS3\nHS4\nHS5\nHS6\nK1\nK2\nK3\nK4\nK5\nK6\nSoft\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nV1\nV2\nV3\nV4\nV5\nV6\n0.93\n0.95\n0.97\n0.99\n0.99\n0.99\n0.89\n0.99\n0.97\n0.96\n0.97\n0.95\n0.93\n-0.95\n-0.81\n-0.9\n-0.82\n-0.81\n-0.83\n-0.49\n-0.86\n-0.78\n-0.85\n-0.81\n-0.82\n-0.79\n0.7\n0.36\n0.11\n-0.01\n-0.03\n0.07\n0.9\n0.95\n0.98\n0.98\n0.98\n0.99\n0.93\n0.99\n0.97\n0.95\n0.93\n0.93\n0.92\n-0.93\n-0.74\n-0.89\n-0.86\n-0.83\n-0.82\n-0.37\n-0.83\n-0.75\n-0.89\n-0.87\n-0.85\n-0.82\n0.73\n0.36\n0.12\n-\n0.01\n-0.03\n0.07\n0.89\n0.95\n0.98\n0.99\n0.98\n0.99\n0.9\n0.99\n0.97\n0.95\n0.93\n0.92\n0.91\n-0.92\n-0.74\n-0.89\n-0.87\n-0.84\n-0.82\n-0.39\n-0.83\n-0.75\n-0.88\n-0.88\n-0.85\n-0.82\n0.73\n0.36\n0.12\n-\n0.01\n-0.03\n0.08\n0.89\n0.95\n0.98\n0.99\n0.99\n0.99\n0.89\n0.99\n0.97\n0.95\n0.93\n0.92\n0.91\n-0.92\n-0.74\n-0.89\n-0.87\n-0.84\n-0.82\n-0.39\n-0.83\n-0.75\n-0.88\n-0.88\n-0.85\n-0.82\n0.73\n0.36\n0.12\n-\n0.01\n-0.03\n0.08\n0.89\n0.95\n0.98\n0.99\n0.99\n0.99\n0.89\n0.99\n0.97\n0.95\n0.93\n0.92\n0.91\n-0.92\n-0.74\n-0.89\n-0.87\n-0.84\n-0.82\n-0.4\n-0.83\n-0.75\n-0.88\n-0.88\n-0.85\n-0.82\n0.73\n0.36\n0.12\n-\n0.01\n-0.03\n0.08\n0.89\n0.95\n0.98\n0.99\n0.98\n0.98\n0.9\n0.99\n0.97\n0.95\n0.93\n0.92\n0.91\n-0.92\n-0.74\n-0.89\n-0.87\n-0.83\n-0.82\n-0.39\n-0.83\n-0.75\n-0.88\n-0.88\n-0.85\n-0.82\n0.73\n0.36\n0.12\n-\n0.01\n-0.03\n0.08\nDistance\nKL1 KL5 KL10 KL15 KL20 MSE\nPearson\nEmb\nHS1\nHS2\nHS3\nHS4\nHS5\nHS6\nK1\nK2\nK3\nK4\nK5\nK6\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nV1\nV2\nV3\nV4\nV5\nV6\n0.92\n-0.19\n0.96\n0.94\n0.94\n0.94\n0.95\n0.74\n0.96\n0.98\n0.98\n0.98\n0.98\n0.98\n0.95\n0.98\n0.98\n0.98\n0.98\n0.69\n0.52\n0.44\n0.54\n0.7\n0.48\n0.94\n0.73\n0.95\n0.96\n0.92\n0.92\n0.91\n0.73\n0.97\n0.99\n0.98\n0.97\n0.96\n0.88\n0.99\n0.99\n0.98\n0.98\n0.97\n0.34\n0.51\n0.44\n0.24\n0.32\n0.61\n0.93\n0.81\n0.95\n0.98\n0.96\n0.95\n0.93\n0.71\n0.96\n0.99\n0.99\n0.98\n0.96\n0.81\n0.99\n0.99\n0.98\n0.98\n0.92\n0.25\n0.47\n0.44\n0.24\n0.33\n0.65\n0.91\n0.82\n0.95\n0.98\n0.97\n0.96\n0.94\n0.63\n0.96\n0.99\n0.99\n0.98\n0.96\n0.83\n0.99\n0.99\n0.98\n0.98\n0.89\n0.24\n0.48\n0.44\n0.25\n0.33\n0.67\n0.89\n0.83\n0.95\n0.98\n0.97\n0.97\n0.95\n0.52\n0.96\n0.99\n0.99\n0.98\n0.96\n0.85\n0.99\n0.99\n0.98\n0.98\n0.87\n0.24\n0.49\n0.44\n0.25\n0.34\n0.67\n0.96\n0.65\n0.58\n0.87\n0.95\n0.95\n0.93\n0.61\n0.82\n0.99\n0.98\n0.85\n0.64\n0.85\n0.59\n0.89\n0.91\n0.84\n0.59\n0.24\n0.89\n0.9\n0.65\n0.58\n0.76\nDistance after pair-wise scaled dot-product\nFigure 4: Spearman and Pearson correlation coefficient of pre-training loss with the distance between teacher and\nstudent features of TinyBERT. This records the training process of TinyBERT in Table 2, where the sizes of the\nteacher and student models are 110M and 66M respectively. The distance after pair-wise scaled dot-product is\ncalculated by first computing features H ← HHT\n√dimensionality . Att1, HS1, Q1, K1, and V1 denote the attention\nscores, hidden state, query matrix, key matrix, and value matrix of the first layer transformer, respectively. Soft and\nEmb denote the soft labels and output of the embedding-layer respectively. KL1, KL5, KL10, KL15, and KL20\ndenote the KL divergence with temperatures of 1, 5, 10, 15, and 20, respectively.\ntures in the auxiliary model for later analysis of the\ncorrelation between feature distance and task per-\nformance in the distillation process. As an example\nof TinyBERT’s pre-training stage distillation, we\npresent the Spearman and Pearson correlation coef-\nficients between the feature distance and training\nloss, and between the feature distance and task per-\nformance, respectively, in Figures 4 and 5. The\nfollowing conclusions can be drawn.\n(1) The results shown in Figure 4 indicate that\nwhile TinyBERT trains its embedding layer, atten-\ntion scores, and hidden state, many other features\n(e.g., the value matrix and features obtained after\npair-wise scaled dot-product) also decrease in dis-\ntance between teacher and student as the training\nloss decreases. This suggests that we may be able\nto find a way to automatically have a large number\nof student features approach the teacher without\nhaving to distill all features, thus reducing the cost\nof distillation. (2) The results shown in Figure\n5 indicate that the distillation in the pre-training\nstage of TinyBERT actually leads to a decrease in\npre-training task performance. This suggests that\nthe performance of pre-training tasks is not neces-\nsarily positively correlated with the performance\nof downstream tasks. It is noteworthy that there\nare a small number of features (e.g., soft labels)\nwhose distance is related to task performance. Our\nhypothesis is that distilling features that are related\nto task performance may further improve task per-\nformance, and the third conclusion in Appendix\nA.1 supports this hypothesis.\n143\nKL1 KL5 KL10 KL15 KL20 MSE\nSpearman\nAtt1\nAtt2\nAtt3\nAtt4\nAtt5\nAtt6\nEmb\nHS1\nHS2\nHS3\nHS4\nHS5\nHS6\nK1\nK2\nK3\nK4\nK5\nK6\nSoft\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nV1\nV2\nV3\nV4\nV5\nV6\n0.16\n-0.09\n-0.08\n-0.09\n-0.09\n-0.08\n-0.19\n-0.43\n-0.08\n-0.06\n-0.06\n-0.07\n-0.07\n-0.19\n0.23\n0.78\n0.34\n0.39\n0.15\n0.95\n0.11\n0.12\n0.14\n0.41\n0.35\n0.34\n0.38\n-0.04\n0.05\n0.15\n0.18\n0.15\n-0.21\n-0.09\n-0.08\n-0.08\n-0.08\n-0.08\n-0.07\n-0.45\n-0.08\n-0.07\n-0.06\n-0.07\n-0.07\n-0.22\n0.18\n0.46\n0.24\n0.19\n0.0\n0.77\n0.16\n0.1\n0.02\n0.26\n0.29\n0.26\n0.37\n-0.04\n0.05\n0.15\n0.17\n0.14\n-0.18\n-0.08\n-0.08\n-0.09\n-0.08\n-0.08\n-0.08\n-0.45\n-0.08\n-0.07\n-0.06\n-0.07\n-0.08\n-0.22\n0.18\n0.43\n0.23\n0.17\n0.01\n0.71\n0.16\n0.1\n-0.01\n0.24\n0.29\n0.24\n0.37\n-0.04\n0.05\n0.15\n0.17\n0.14\n-0.16\n-0.08\n-0.08\n-0.09\n-0.08\n-0.08\n-0.08\n-0.45\n-0.08\n-0.07\n-0.06\n-0.07\n-0.08\n-0.22\n0.18\n0.43\n0.24\n0.17\n0.02\n0.69\n0.16\n0.1\n-0.02\n0.24\n0.29\n0.24\n0.37\n-0.04\n0.05\n0.15\n0.17\n0.14\n-0.15\n-0.08\n-0.08\n-0.09\n-0.08\n-0.08\n-0.08\n-0.45\n-0.08\n-0.07\n-0.06\n-0.07\n-0.08\n-0.22\n0.18\n0.43\n0.24\n0.17\n0.02\n0.69\n0.16\n0.1\n-0.02\n0.24\n0.29\n0.24\n0.36\n-0.04\n0.05\n0.15\n0.17\n0.14\n-0.11\n-0.08\n-0.08\n-0.08\n-0.08\n-0.07\n-0.08\n-0.45\n-0.08\n-0.07\n-0.06\n-0.07\n-0.08\n-0.23\n0.18\n0.42\n0.24\n0.16\n0.03\n0.59\n0.15\n0.1\n-0.03\n0.24\n0.29\n0.23\n0.37\n-0.04\n0.05\n0.15\n0.17\n0.14\nDistance\nKL1 KL5 KL10 KL15 KL20 MSE\nSpearman\nEmb\nHS1\nHS2\nHS3\nHS4\nHS5\nHS6\nK1\nK2\nK3\nK4\nK5\nK6\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nV1\nV2\nV3\nV4\nV5\nV6\n-0.29\n0.68\n-0.04\n-0.09\n-0.2\n-0.16\n-0.18\n-0.12\n-0.07\n-0.06\n-0.05\n-0.07\n-0.07\n-0.23\n-0.07\n-0.06\n-0.05\n-0.06\n-0.08\n-0.74\n-0.8\n-0.62\n-0.64\n-0.91\n-0.58\n-0.23\n0.52\n0.14\n-0.08\n-0.09\n-0.11\n-0.09\n-0.17\n-0.26\n-0.43\n-0.06\n-0.12\n-0.15\n0.66\n-0.08\n-0.09\n-0.07\n-0.07\n-0.07\n-0.44\n-0.54\n-0.26\n-0.15\n-0.56\n-0.96\n-0.16\n0.51\n0.04\n-\n0.09\n-0.09\n-0.1\n-0.1\n-0.32\n-0.28\n-0.45\n-0.07\n-0.17\n-0.18\n0.8\n-0.09\n-0.1\n-0.07\n-0.07\n-0.11\n-0.37\n-0.48\n-0.2\n-0.1\n-0.45\n-0.95\n-0.13\n0.5\n0.02\n-0.09\n-0.09\n-0.09\n-0.1\n-0.48\n-0.28\n-0.45\n-0.06\n-0.17\n-0.18\n0.79\n-0.09\n-0.1\n-0.07\n-0.07\n-0.15\n-0.37\n-0.48\n-0.18\n-0.09\n-0.41\n-0.95\n-0.11\n0.49\n0.01\n-0.1\n-0.08\n-0.09\n-0.1\n-0.5\n-0.28\n-0.44\n-0.06\n-0.17\n-0.18\n0.75\n-0.09\n-0.09\n-0.06\n-0.07\n-0.16\n-0.38\n-0.48\n-0.17\n-0.09\n-0.39\n-0.95\n-0.08\n0.46\n-0.02\n-0.1\n-0.09\n-0.1\n-0.12\n-0.39\n-0.09\n-0.47\n-0.06\n-0.09\n0.05\n0.04\n-0.04\n0.17\n0.0\n-0.05\n0.12\n-0.41\n-0.48\n-0.17\n-0.13\n-0.52\n-0.79\nDistance after pair-wise scaled dot-product\nKL1 KL5 KL10 KL15 KL20 MSE\nPearson\nAtt1\nAtt2\nAtt3\nAtt4\nAtt5\nAtt6\nEmb\nHS1\nHS2\nHS3\nHS4\nHS5\nHS6\nK1\nK2\nK3\nK4\nK5\nK6\nSoft\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nV1\nV2\nV3\nV4\nV5\nV6\n-0.1\n-0.21\n-0.22\n-0.3\n-0.31\n-0.34\n-0.38\n-0.36\n-0.38\n-0.38\n-0.38\n-0.38\n-0.38\n0.17\n0.18\n0.58\n0.49\n0.49\n0.33\n0.83\n0.52\n0.4\n0.39\n0.54\n0.47\n0.49\n0.02\n-\n0.33\n-0.08\n0.02\n0.01\n0.01\n-\n0.1\n-0.22\n-0.27\n-0.33\n-0.34\n-0.36\n-0.36\n-0.36\n-0.38\n-0.38\n-0.38\n-0.39\n-0.39\n0.11\n0.06\n0.41\n0.37\n0.34\n0.21\n0.78\n0.55\n0.35\n0.13\n0.37\n0.39\n0.37\n-\n0.03\n-0.33\n-0.08\n0.01\n0.0\n0.0\n-0.09\n-0.21\n-0.27\n-0.33\n-0.34\n-0.36\n-0.38\n-0.36\n-0.38\n-0.38\n-0.38\n-0.39\n-0.39\n0.1\n0.06\n0.4\n0.36\n0.33\n0.21\n0.72\n0.55\n0.35\n0.11\n0.35\n0.39\n0.36\n-\n0.04\n-0.33\n-0.08\n0.01\n0.0\n0.0\n-0.09\n-0.21\n-0.27\n-0.34\n-0.34\n-0.36\n-0.38\n-0.36\n-0.38\n-0.38\n-0.38\n-0.39\n-0.39\n0.1\n0.06\n0.4\n0.36\n0.33\n0.21\n0.7\n0.55\n0.35\n0.1\n0.35\n0.4\n0.35\n-\n0.04\n-0.33\n-0.08\n0.01\n0.0\n0.0\n-0.09\n-0.21\n-0.27\n-0.34\n-0.34\n-0.36\n-0.38\n-0.36\n-0.38\n-0.38\n-0.38\n-0.39\n-0.39\n0.1\n0.06\n0.4\n0.36\n0.33\n0.21\n0.7\n0.55\n0.35\n0.1\n0.35\n0.4\n0.35\n-\n0.04\n-0.33\n-0.08\n0.01\n0.0\n0.0\n-0.09\n-0.21\n-0.27\n-0.33\n-0.34\n-0.35\n-0.38\n-0.36\n-0.38\n-0.38\n-0.38\n-0.39\n-0.39\n0.09\n0.06\n0.4\n0.36\n0.32\n0.22\n0.64\n0.55\n0.35\n0.09\n0.34\n0.4\n0.35\n-\n0.03\n-0.33\n-0.08\n0.01\n0.0\n0.0\nDistance\nKL1 KL5 KL10 KL15 KL20 MSE\nPearson\nEmb\nHS1\nHS2\nHS3\nHS4\nHS5\nHS6\nK1\nK2\nK3\nK4\nK5\nK6\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nV1\nV2\nV3\nV4\nV5\nV6\n-0.26\n0.69\n-0.19\n-0.14\n-0.13\n-0.14\n-0.14\n-0.5\n-0.26\n-0.24\n-0.27\n-0.32\n-0.34\n-0.22\n-0.23\n-0.23\n-0.27\n-0.3\n-0.34\n-0.71\n-0.69\n-0.69\n-0.66\n-0.82\n-0.5\n-0.17\n0.32\n-0.12\n-0.34\n-0.4\n-0.41\n-0.41\n-0.48\n-0.33\n-0.29\n-0.31\n-0.36\n-0.35\n0.07\n-0.35\n-0.28\n-0.3\n-0.31\n-0.32\n-0.43\n-0.6\n-0.56\n-0.44\n-0.64\n-0.89\n-0.26\n0.21\n-\n0.12\n-0.31\n-0.38\n-0.39\n-0.41\n-0.52\n-0.35\n-0.29\n-0.31\n-0.36\n-0.35\n0.36\n-0.34\n-0.29\n-0.3\n-0.31\n-0.32\n-0.34\n-0.55\n-0.54\n-0.42\n-0.61\n-0.86\n-0.33\n0.18\n-0.13\n-0.3\n-0.36\n-0.37\n-0.4\n-0.55\n-0.35\n-0.29\n-0.31\n-0.35\n-0.35\n0.29\n-0.34\n-0.29\n-0.3\n-0.31\n-0.31\n-0.31\n-0.54\n-0.53\n-0.42\n-0.6\n-0.85\n-0.36\n0.17\n-0.13\n-0.3\n-0.35\n-0.36\n-0.4\n-0.54\n-0.36\n-0.29\n-0.31\n-0.35\n-0.35\n0.21\n-0.34\n-0.29\n-0.3\n-0.32\n-0.3\n-0.31\n-0.54\n-0.53\n-0.42\n-0.6\n-0.84\n-0.29\n0.13\n-0.07\n-0.24\n-0.31\n-0.32\n-0.34\n-0.34\n-0.51\n-0.3\n-0.31\n-0.23\n-0.03\n-0.15\n-0.52\n-0.05\n-0.35\n-0.16\n0.04\n-0.25\n-0.48\n-0.52\n-0.51\n-0.65\n-0.48\nDistance after pair-wise scaled dot-product\nFigure 5: Spearman and Pearson correlation coefficient of task performance (perplexity of the language model on\nthe validation set) of pre-training stage with the distance between teacher and student features of TinyBERT. This\nrecords the training process of TinyBERT in Table 2, where the sizes of the teacher and student models are 110M\nand 66M respectively. The distance after pair-wise scaled dot-product is calculated by first computing features\nH ← HHT\n√dimensionality . Att1, HS1, Q1, K1, and V1 denote the attention scores, hidden state, query matrix, key\nmatrix, and value matrix of the first layer transformer, respectively. Soft and Emb denote the soft labels and output of\nthe embedding-layer respectively. KL1, KL5, KL10, KL15, and KL20 denote the KL divergence with temperatures\nof 1, 5, 10, 15, and 20, respectively.\nB Additional Analysis\nIn this section, we further verify the reliability of\nGKD from the perspective of loss function value,\nand analyze the balance of memory and time con-\nsumption in the teacher-student parallel strategy.\nB.1 Are the Loss Values of GKD Normal?\nIn order to further verify the reliability of GKD, we\npresent the loss function values of each method at\nvarious distillation stages in Figure 6. The down-\nward trend of all the loss values is consistent with\nour expectations, with two noteworthy observa-\ntions: (1) MobileBERT and SID tend to gradually\nincrease the number of distilled layers during train-\ning, hence the loss values exhibit an up-and-down\ntrend. (2) The ReCoRD dataset, shown in task-\nspecific stages, was trained for 5 epochs, therefore\nsome methods may show loss changes in stair-step\nfashion, such as Annealing-KD and Universal-KD.\nB.2 Trade-off between Memory and Time\nConsumption\nIn order to speed up the training process while en-\nsuring that the distillation process is not limited by\nGPU memory, we conducted a full combination\nof all optimization options to find the best balance\nbetween memory and time. Table 5 showcases the\nresource usage of 5B-scale and 10B-scale teacher\nmodels under different MP, DP, ZeRO, and Offload\noptions during distillation. The results of the test-\ning lead us to the following recommendations: In\ncases of insufficient GPU memory, ZeRO should\n144\n0.1% 7.7% 15.5% 23.1% 30.9% 38.5% 46.2% 53.9% 61.6% 69.3% 77.0% 84.7% 92.4%\nTraining progress\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized training loss\nPre-training stage\nDIITO\nDistilBERT\nMiniLM\nMiniLMv2\nMobileBERT\nTMKD\nTinyBERT\n0.2% 7.9% 15.7% 23.3% 30.9% 38.8% 46.4% 54.0% 61.7% 69.5% 77.1% 84.8% 92.6%\nTraining progress\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized training loss\nT ask-specific stage\nALP-KD\nAnnealing-KD\nCKD\nContinuation-KD\nDGKD\nKD\nLRC-BERT\nMGSKD\nMT-BERT\nPD\nPKD\nRAIL-KD\nRL-KD\nSID\nTAKD\nTMKD\nTheseus\nTinyBERT\nUncertainty\nUniversal-KD\n0.2% 7.9% 15.7% 23.3% 30.9% 38.8% 46.4% 54.0% 61.7% 69.5% 77.1% 84.8% 92.6%\nTraining progress\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized training loss\nT ask-specific stage 2\nAnnealing-KD\nLRC-BERT\nMGSKD\nRL-KD\nTinyBERT\nUniversal-KD\nFigure 6: Loss function values of 25 methods across different distillation stages. The loss values are normalized due\nto the varying range of values across different methods. Some methods are distilled at most 3 times, including a\npre-training stage and two task-specific stages (ReCoRD dataset). TAKD and DGKD based on teacher-assistant\nstrategy showcase the final distillation process.\n145\nTeacher⇒Student (scale)MA (GB) CA (GB) Time (ms) Mem (GB) MP DP ZeRO Offload\n5B⇒1B\n17.65 33.11 169.01 95.15 1 8 ✓† ✓†\n9.07 14.97 262.49 86.56 2 4 ✓† ✓†\n4.87 6.09 430.61 86.20 4 2 ✓† ✓†\n2.72 3.71 884.05 82.61 8 1 ✓† ✓†\n17.65 30.77 175.08 95.13 1 8 ✓ ✓\n9.06 13.74 252.99 86.74 2 4 ✓ ✓\n4.87 5.79 437.50 86.01 4 2 ✓ ✓\n2.72 3.43 831.22 83.24 8 1 ✓ ✓\n18.92 31.04 61.43 70.50 1 8 ✓†\n10.44 14.24 78.97 62.18 2 4 ✓†\n6.31 7.42 129.91 61.59 4 2 ✓†\n4.23 5.10 260.96 58.72 8 1 ✓†\n18.92 28.81 60.25 70.52 1 8 ✓\n10.43 13.73 80.64 62.38 2 4 ✓\n6.31 7.32 129.40 62.27 4 2 ✓\n4.23 5.04 243.80 58.76 8 1 ✓\n32.44 36.57 53.34 61.58 1 8\n16.34 18.50 68.17 62.18 2 4\n8.31 9.41 121.26 62.16 4 2\n4.27 5.04 231.95 58.83 8 1\n10B⇒2B\n30.73 36.89 226.51 104.46 1 8 ✓† ✓†\n15.84 24.19 378.93 106.31 2 4 ✓† ✓†\n8.41 10.08 664.45 95.51 4 2 ✓† ✓†\n4.70 6.00 1210.07 98.12 8 1 ✓† ✓†\n30.73 36.89 222.50 104.45 1 8 ✓ ✓\n15.83 22.55 387.19 106.35 2 4 ✓ ✓\n8.41 9.72 693.03 95.50 4 2 ✓ ✓\n4.70 5.81 1224.11 98.09 8 1 ✓ ✓\n33.23 36.91 85.52 66.17 1 8 ✓†\n18.46 23.13 119.30 68.61 2 4 ✓†\n11.11 12.91 186.53 57.42 4 2 ✓†\n7.53 8.87 310.56 59.88 8 1 ✓†\n33.23 36.90 88.21 66.13 1 8 ✓\n18.45 22.56 119.72 68.68 2 4 ✓\n11.11 12.78 198.84 57.45 4 2 ✓\n7.53 9.01 329.12 59.83 8 1 ✓\nGPU memory overflow 1 8\n30.91 34.54 105.40 62.33 2 4\n15.64 17.62 174.30 57.79 4 2\n8.00 8.98 311.44 59.73 8 1\nTable 5: The consumption of memory and time during the pre-training stage of TinyBERT when distilling\nteacher models of different scales on 8 NVIDIA A100 (40GB) GPUs is presented. The micro batch and gradient\naccumulation steps are set to 1. Where MA denotes the maximum memory allocated on the GPU, CA denotes\nthe maximum cached memory on the GPU, Time denotes the time required to train each sample, Mem denotes\nthe size of occupied CPU memory, MP denotes the number of model parallelism, DP denotes the number of data\nparallelism, ZeRO denotes whether the optimizer states are partitioned across different GPUs, and Offload denotes\nwhether the optimizer states are stored in CPU memory. In addition to the optimizer states, the model gradients can\nalso be partitioned across different GPUs or stored in CPU memory. The dagger symbol (†) represents optimization\nof both the optimizer states and the model gradients simultaneously.\n146\nHyperparametersReCoRD COPA WSC RTE BoolQ WiC CB MultiRC\nSequence length 512 256 128 256 256 256 256 512\nEpochs 5 50 50 50 20 30 50 15\nDropout 0.1\nAttention Dropout 0.1\nWarmup Ration 0.1\nWeight Decay 0.1\nLearning Rate Decay Linear\nAdamϵ 1E-8\nAdamβ1 0.9\nAdamβ2 0.999\nGradient Clipping 0.1\nTable 6: Other hyperparameters for the task-specific stage on the 8 datasets of the SuperGLUE benchmark.\nbe considered first for partitioning the optimizer\nstates and model gradients, followed by increasing\nthe number of model parallelism, and lastly, us-\ning ZeRO-Offload to store the optimizer states and\nmodel gradients in CPU memory.\nC Implementation Details\nIn this section, we provide further details regard-\ning the hyperparameters and models to facilitate\nreplication by developers.\nC.1 Hyperparameters\nThe batch size, number of iterations, and peak\nlearning rate for the pre-training stage were set\nto 64, 150000, and 4e-4, respectively. The task-\nspecific hyperparameters for specific methods were\nset to the optimal values from their correspond-\ning papers, while other hyperparameters (see Ta-\nble 6) were kept consistent with the fine-tuning\nteacher. For single-teacher methods in the task-\nspecific stage, grid search was used to optimize\nhyperparameters, including learning rate {5e-6,1e-\n5,2e-5} and batch size {16,32}. Table 7 presents\nthe learning rate and batch size for each method on\neach dataset in the SuperGLUE benchmark. The\nresults for all methods were averaged over three\nrandom seeds.\nC.2 Models\nTable 8 shows the specific parameters of all the\nmodels utilized in this paper. The 110M, 340M,\nand 10B scale models are from GLM pre-trained\nmodels 2. The 293M-scale model with the Mobile-\nBERT structure (inverted-bottleneck structure) was\nobtained by us through a week of pre-training with\n16 NVIDIA A100 (40GB) GPUs, and the 25M-\nscale model is also with the MobileBERT structure.\n2https://github.com/THUDM/GLM\nWhen conducting pre-training tasks, the models\nwith the MobileBERT structure require the expan-\nsion of the token dimension, thus the actual number\nof parameters is greater than the scale. The other\nsized teacher models were tested with randomly\ninitialized parameters to assess resource consump-\ntion. All the distillation processes were conducted\nusing half-precision floating-point (fp16) models.\n147\nMethods ReCoRD COPA WSC RTE BoolQ WiC CB MultiRC\nbs/lr bs/lr bs/lr bs/lr bs/lr bs/lr bs/lr bs/lr\nGLMBase(teacher, 110M) bs (batch size) = 16, lr (learning rate) = 1E-5GLMLarge(teacher, 340M)\nSingle-teacher: Teacher (GLMBase)⇒Student (66M)\nKD (Hinton et al., 2015) 16/5E-06 16/2E-05 16/1E-05 16/2E-05 16/2E-05 16/5E-06 16/2E-05 16/5E-06\nPD (Turc et al., 2019) 16/1E-05 32/5E-06 16/2E-05 16/1E-05 32/1E-05 16/5E-06 16/2E-05 16/5E-06\nPKD (Sun et al., 2019) 32/2E-05 32/2E-05 16/2E-05 32/5E-06 16/1E-05 16/5E-06 16/2E-05 32/2E-05\nDistilBERT (Sanh et al., 2019)16/1E-05 16/2E-05 16/1E-05 16/5E-06 32/2E-05 32/2E-05 32/2E-05 16/1E-05\nTheseus (Xu et al., 2020) 32/2E-05 16/1E-05 16/1E-05 32/1E-05 16/1E-05 32/1E-05 16/2E-05 32/5E-06\nTinyBERT (Jiao et al., 2020) 32/1E-05 16/5E-06 32/5E-06 16/2E-05 16/1E-05 16/5E-06 16/1E-05 16/1E-05\nMobileBERT (Sun et al., 2020)16/1E-05 16/1E-05 32/2E-05 32/2E-05 32/2E-05 32/1E-05 32/2E-05 16/5E-06\nSID (Aguilar et al., 2020) 16/2E-05 32/5E-06 16/5E-06 16/2E-05 16/2E-05 16/2E-05 16/1E-05 16/2E-05\nMiniLM (Wang et al., 2020) 16/2E-05 32/1E-05 32/2E-05 32/1E-05 16/1E-05 16/1E-05 32/1E-05 32/2E-05\nMiniLMv2 (Wang et al., 2021)16/1E-05 16/1E-05 16/5E-06 32/2E-05 16/2E-05 32/2E-05 16/1E-05 16/1E-05\nALP-KD (Passban et al., 2021)16/2E-05 16/1E-05 16/2E-05 16/2E-05 16/2E-05 32/2E-05 16/2E-05 32/2E-05\nLRC-BERT (Fu et al., 2021) 16/2E-05 32/1E-05 16/2E-05 32/1E-05 16/2E-05 16/5E-06 16/2E-05 16/5E-06\nAnnealing-KD (Jafari et al., 2021)16/2E-05 16/5E-06 16/2E-05 16/2E-05 16/2E-05 32/5E-06 16/1E-05 32/5E-06\nCKD (Park et al., 2021) 32/2E-05 16/2E-05 16/5E-06 16/1E-05 16/2E-05 16/1E-05 16/1E-05 32/2E-05\nUniversal-KD (Wu et al., 2021b)32/2E-05 32/5E-06 32/5E-06 32/1E-05 32/5E-06 16/5E-06 16/1E-05 16/1E-05\nDIITO (Wu et al., 2022) 16/5E-06 32/1E-05 16/2E-05 16/1E-05 16/2E-05 16/1E-05 16/1E-05 16/5E-06\nContinuation-KD (Jafari et al., 2022)16/2E-05 32/1E-05 16/1E-05 16/1E-05 16/2E-05 32/1E-05 16/1E-05 16/5E-06\nRAIL-KD (Haidar et al., 2022)16/1E-05 16/1E-05 16/2E-05 16/5E-06 32/2E-05 16/1E-05 32/1E-05 32/2E-05\nMGSKD (Liu et al., 2022) 16/5E-06 16/2E-05 32/2E-05 16/5E-06 16/5E-06 16/1E-05 32/2E-05 32/5E-06\nMulti-teacher: Teachers (GLMBaseand GLMLarge)⇒Student (66M)\nTMKD (Yang et al., 2020a)\nsame as GLMBase\nMT-BERT (Wu et al., 2021a)\nRL-KD (Yuan et al., 2021)\nUncertainty (Li et al., 2021)\nTeacher assistants: Teacher (GLMLarge)⇒Assistant (200M)⇒Assistant (110M)⇒Student (66M)\nTAKD (Mirzadeh et al., 2020) same as KDDGKD (Son et al., 2021)\nTable 7: Hyperparameters for all methods in Table 2 on the 8 datasets of the SuperGLUE benchmark.\nScale #Parameters #Dimensions #Layers #Heads Max-seq Vocabulary\n22M 22788864 384 6 12\n512 30592\n25M 37371392 128 24 4\n66M 66811392 768 6 12\n110M 109338624 768 12 12\n293M 306174464 1024 24 4\n340M 334688256 1024 24 16\n1B 1022682240 1728 26\n64 1024 50304\n1.2B 1173458944 1792 28\n1.5B 1521700224 1984 30\n2B 1920122880 2048 36\n5B 5030587776 3264 38\n6B 5915828736 3456 40\n7.5B 7385878656 3776 42\n10B 9880682496 4096 48\n13B 13170418176 4736 48\n18B 18125342976 5248 54\n20B 20175676160 5440 56\n22B 22104152064 5504 60\n25B 24660072448 5632 64\n50B 49577504000 8000 64\n65B 64813768448 9152 64\n90B 89957891328 10624 66\n100B 99465734144 11008 68\n110B 109620044032 11392 70\nTable 8: The scale details of all the models utilized in this paper.\n148",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5688714981079102
    },
    {
      "name": "Zhàng",
      "score": 0.5482786297798157
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5413557887077332
    },
    {
      "name": "Natural language processing",
      "score": 0.49124783277511597
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46093785762786865
    },
    {
      "name": "Distillation",
      "score": 0.4214697480201721
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.41650480031967163
    },
    {
      "name": "History",
      "score": 0.11045268177986145
    },
    {
      "name": "Geography",
      "score": 0.10323354601860046
    },
    {
      "name": "Chemistry",
      "score": 0.09589806199073792
    },
    {
      "name": "Archaeology",
      "score": 0.09482592344284058
    },
    {
      "name": "Cartography",
      "score": 0.08011773228645325
    },
    {
      "name": "China",
      "score": 0.07790115475654602
    },
    {
      "name": "Thermodynamics",
      "score": 0.07274028658866882
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I143868143",
      "name": "Anhui University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ]
}