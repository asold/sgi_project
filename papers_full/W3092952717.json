{
  "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
  "url": "https://openalex.org/W3092952717",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2672090663",
      "name": "Lin, Jimmy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221795652",
      "name": "Nogueira, Rodrigo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3160855242",
      "name": "Yates, Andrew",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971209824",
    "https://openalex.org/W2069003154",
    "https://openalex.org/W2911430044",
    "https://openalex.org/W2952313649",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3014568172",
    "https://openalex.org/W1978394996",
    "https://openalex.org/W2106365165",
    "https://openalex.org/W3135934234",
    "https://openalex.org/W2942607211",
    "https://openalex.org/W2396476612",
    "https://openalex.org/W2965843218",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2991671759",
    "https://openalex.org/W1974339500",
    "https://openalex.org/W2794432940",
    "https://openalex.org/W3115195983",
    "https://openalex.org/W2015720094",
    "https://openalex.org/W3023786569",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W3105107530",
    "https://openalex.org/W2341132943",
    "https://openalex.org/W2901894078",
    "https://openalex.org/W2986300872",
    "https://openalex.org/W3211347078",
    "https://openalex.org/W2962765866",
    "https://openalex.org/W2995335514",
    "https://openalex.org/W2906581649",
    "https://openalex.org/W3021300761",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3034506277",
    "https://openalex.org/W3023238803",
    "https://openalex.org/W2102698119",
    "https://openalex.org/W2156186232",
    "https://openalex.org/W2006187024",
    "https://openalex.org/W2016078760",
    "https://openalex.org/W3044812140",
    "https://openalex.org/W2134557008",
    "https://openalex.org/W2997090102",
    "https://openalex.org/W658020064",
    "https://openalex.org/W3013072759",
    "https://openalex.org/W3091207065",
    "https://openalex.org/W2969624041",
    "https://openalex.org/W3172753173",
    "https://openalex.org/W2611029872",
    "https://openalex.org/W1971987737",
    "https://openalex.org/W2590822507",
    "https://openalex.org/W2109244020",
    "https://openalex.org/W3023151789",
    "https://openalex.org/W3015713034",
    "https://openalex.org/W2951937595",
    "https://openalex.org/W1969340322",
    "https://openalex.org/W2913443447",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W1489949474",
    "https://openalex.org/W1558382083",
    "https://openalex.org/W2143331230",
    "https://openalex.org/W2148781362",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2096937925",
    "https://openalex.org/W3152887675",
    "https://openalex.org/W2963813662",
    "https://openalex.org/W2995289474",
    "https://openalex.org/W2105157020",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2887005207",
    "https://openalex.org/W2890181840",
    "https://openalex.org/W3022143756",
    "https://openalex.org/W2048978851",
    "https://openalex.org/W2975938167",
    "https://openalex.org/W2910577570",
    "https://openalex.org/W2963469388",
    "https://openalex.org/W2087710618",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2109664771",
    "https://openalex.org/W2969574947",
    "https://openalex.org/W2537515450",
    "https://openalex.org/W3103686454",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2799232306",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2082359527",
    "https://openalex.org/W2007239001",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2339562433",
    "https://openalex.org/W2052569738",
    "https://openalex.org/W2157963512",
    "https://openalex.org/W2937036051",
    "https://openalex.org/W1237471683",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3118831779",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3118027562",
    "https://openalex.org/W2725049817",
    "https://openalex.org/W2173213060",
    "https://openalex.org/W2119565742",
    "https://openalex.org/W2053713852",
    "https://openalex.org/W2040337753",
    "https://openalex.org/W3131259441",
    "https://openalex.org/W3159838520",
    "https://openalex.org/W2047295649",
    "https://openalex.org/W153917764",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2164547069",
    "https://openalex.org/W3130740619",
    "https://openalex.org/W2949547296",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W224439989",
    "https://openalex.org/W1979346010",
    "https://openalex.org/W3030045039",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2064579132",
    "https://openalex.org/W3093815109",
    "https://openalex.org/W1801866228",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W3105817677",
    "https://openalex.org/W3188983256",
    "https://openalex.org/W3101622805",
    "https://openalex.org/W3037722615",
    "https://openalex.org/W2622338386",
    "https://openalex.org/W3135665608",
    "https://openalex.org/W1973435495",
    "https://openalex.org/W3098468692",
    "https://openalex.org/W2061397531",
    "https://openalex.org/W1759973002",
    "https://openalex.org/W836999996",
    "https://openalex.org/W3021805648",
    "https://openalex.org/W1594112393",
    "https://openalex.org/W3008088841",
    "https://openalex.org/W2462891382",
    "https://openalex.org/W2136189984",
    "https://openalex.org/W2108278040",
    "https://openalex.org/W3021397474",
    "https://openalex.org/W3111107049",
    "https://openalex.org/W2025356973",
    "https://openalex.org/W3135351079",
    "https://openalex.org/W3034837085",
    "https://openalex.org/W2593864460",
    "https://openalex.org/W2148972377",
    "https://openalex.org/W2071080574",
    "https://openalex.org/W3030698044",
    "https://openalex.org/W2035569891",
    "https://openalex.org/W3169937871",
    "https://openalex.org/W2916068640",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2103931177",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2752382061",
    "https://openalex.org/W2997051142",
    "https://openalex.org/W3094833381",
    "https://openalex.org/W3180230246",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W1984565341",
    "https://openalex.org/W2167510893",
    "https://openalex.org/W2061172803",
    "https://openalex.org/W3090721331",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3099384026",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2087663869",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3093955333",
    "https://openalex.org/W2043909051",
    "https://openalex.org/W2774514250",
    "https://openalex.org/W3104657626",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3123218467",
    "https://openalex.org/W1982858363",
    "https://openalex.org/W2798664956",
    "https://openalex.org/W2169213601",
    "https://openalex.org/W3212575067",
    "https://openalex.org/W2947497897",
    "https://openalex.org/W3039017601",
    "https://openalex.org/W3154670582",
    "https://openalex.org/W2741632195",
    "https://openalex.org/W2115584760",
    "https://openalex.org/W3102286003",
    "https://openalex.org/W2588670714",
    "https://openalex.org/W2996064239",
    "https://openalex.org/W2982096936",
    "https://openalex.org/W2885586573",
    "https://openalex.org/W3020908159",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3153794000",
    "https://openalex.org/W2798988488",
    "https://openalex.org/W3152624702",
    "https://openalex.org/W3034521898",
    "https://openalex.org/W3156899621",
    "https://openalex.org/W2741170534",
    "https://openalex.org/W2008285050",
    "https://openalex.org/W2094145178",
    "https://openalex.org/W1743573446",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2052989395",
    "https://openalex.org/W2948384082",
    "https://openalex.org/W2147717514",
    "https://openalex.org/W2107695330",
    "https://openalex.org/W1968927634",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W3015381124",
    "https://openalex.org/W2117473841",
    "https://openalex.org/W2108862644",
    "https://openalex.org/W3044284384",
    "https://openalex.org/W3174203100",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2941126434",
    "https://openalex.org/W2752172973",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W3037854022",
    "https://openalex.org/W3022373106",
    "https://openalex.org/W2190694985",
    "https://openalex.org/W2947881255",
    "https://openalex.org/W2143927888",
    "https://openalex.org/W3032541190",
    "https://openalex.org/W3106498098",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3122203402",
    "https://openalex.org/W2124634352",
    "https://openalex.org/W2027518030",
    "https://openalex.org/W2203835818",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2398442511",
    "https://openalex.org/W3156830884",
    "https://openalex.org/W2082729696",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2158751803",
    "https://openalex.org/W3016194403",
    "https://openalex.org/W2065108361",
    "https://openalex.org/W2782157559",
    "https://openalex.org/W3004654429",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W3035300716",
    "https://openalex.org/W2068098598",
    "https://openalex.org/W2612431505",
    "https://openalex.org/W2104944809",
    "https://openalex.org/W2006996936",
    "https://openalex.org/W2099435855",
    "https://openalex.org/W2899507349",
    "https://openalex.org/W3006057906",
    "https://openalex.org/W1973697738",
    "https://openalex.org/W2945127593",
    "https://openalex.org/W3184918446",
    "https://openalex.org/W2921990970",
    "https://openalex.org/W3025117556",
    "https://openalex.org/W3021052948",
    "https://openalex.org/W2911997761",
    "https://openalex.org/W1993692165",
    "https://openalex.org/W2155482025",
    "https://openalex.org/W2162346592",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2429667833",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W3105949871",
    "https://openalex.org/W2164259805",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2961394393",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1974360117",
    "https://openalex.org/W2915092747",
    "https://openalex.org/W3157758108",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W3015612646",
    "https://openalex.org/W1991360400",
    "https://openalex.org/W2007291639",
    "https://openalex.org/W2012318340",
    "https://openalex.org/W2539671052",
    "https://openalex.org/W2952354317",
    "https://openalex.org/W2047221353",
    "https://openalex.org/W1502916507",
    "https://openalex.org/W2507134384",
    "https://openalex.org/W2019509999",
    "https://openalex.org/W2959579033",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2902365885",
    "https://openalex.org/W2586017539",
    "https://openalex.org/W2994636820",
    "https://openalex.org/W2955375559",
    "https://openalex.org/W2783640434",
    "https://openalex.org/W2260194779",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2134797427",
    "https://openalex.org/W2963386594",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2146081744",
    "https://openalex.org/W19293941",
    "https://openalex.org/W2536015822",
    "https://openalex.org/W2989195139",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W3092510499",
    "https://openalex.org/W2982596739",
    "https://openalex.org/W2145907631",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2888302696",
    "https://openalex.org/W2769041395",
    "https://openalex.org/W2152784831",
    "https://openalex.org/W2048045485",
    "https://openalex.org/W3095156104",
    "https://openalex.org/W2134131174",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W2062270497",
    "https://openalex.org/W2095683564",
    "https://openalex.org/W2239566685",
    "https://openalex.org/W2020237802",
    "https://openalex.org/W3064953855",
    "https://openalex.org/W2091379987",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W3171636345",
    "https://openalex.org/W3178067142",
    "https://openalex.org/W3092302355",
    "https://openalex.org/W2170245882",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2055981215"
  ],
  "abstract": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP), information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage architectures and dense retrieval techniques that perform ranking directly. There are two themes that pervade our survey: techniques for handling long documents, beyond typical sentence-by-sentence processing in NLP, and techniques for addressing the tradeoff between effectiveness (i.e., result quality) and efficiency (e.g., query latency, model and index size). Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading.",
  "full_text": "Pretrained Transformers for Text Ranking:\nBERT and Beyond\nJimmy Lin,1 Rodrigo Nogueira,1 and Andrew Yates2,3\n1 David R. Cheriton School of Computer Science, University of Waterloo\n2 University of Amsterdam\n3 Max Planck Institute for Informatics\nVersion 0.99 — August 20, 2021\nAbstract\nThe goal of text ranking is to generate an ordered list of texts retrieved from a\ncorpus in response to a query for a particular task. Although the most common\nformulation of text ranking is search, instances of the task can also be found\nin many text processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language processing\n(NLP), information retrieval (IR), and beyond. For text ranking, transformer-based\nmodels produce high quality results across many domains, tasks, and settings.\nThis survey provides a synthesis of existing work as a single point of entry for\npractitioners who wish to deploy transformers for text ranking and researchers who\nwish to pursue work in this area. We cover a wide range of techniques, grouped\ninto two categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly. Examples\nin the ﬁrst category include approaches based on relevance classiﬁcation, evidence\naggregation from multiple segments of text, and document and query expansion.\nThe second category involves using transformers to learn dense representations of\ntexts, where ranking is formulated as comparisons between query and document\nrepresentations that take advantage of nearest neighbor search.\nAt a high level, there are two themes that pervade our survey: techniques for\nhandling long documents, beyond typical sentence-by-sentence processing in NLP,\nand techniques for addressing the tradeoff between effectiveness (i.e., result quality)\nand efﬁciency (e.g., query latency, model and index size). Much effort has been\ndevoted to developing ranking models that address the mismatch between document\nlengths and the length limitations of existing transformers. The computational\ncosts of inference with transformers has led to alternatives and variants that aim\nfor different tradeoffs, both within multi-stage architectures as well as with dense\nlearned representations.\nAlthough transformer architectures and pretraining techniques are recent innova-\ntions, many aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of pretrained\ntransformers for text ranking, this survey also attempts to prognosticate where the\nﬁeld is heading.\narXiv:2010.06467v3  [cs.IR]  19 Aug 2021\nContents\n1 Introduction 4\n1.1 Text Ranking Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.2 A Brief History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.2.1 The Beginnings of Text Ranking . . . . . . . . . . . . . . . . . . . . . . . 10\n1.2.2 The Challenges of Exact Match . . . . . . . . . . . . . . . . . . . . . . . 12\n1.2.3 The Rise of Learning to Rank . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.2.4 The Advent of Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . 15\n1.2.5 The Arrival of BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n1.3 Roadmap, Assumptions, and Omissions . . . . . . . . . . . . . . . . . . . . . . . 19\n2 Setting the Stage 20\n2.1 Texts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.2 Information Needs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.3 Relevance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.4 Relevance Judgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.5 Ranking Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.6 Community Evaluations and Reusable Test Collections . . . . . . . . . . . . . . . 30\n2.7 Descriptions of Common Test Collections . . . . . . . . . . . . . . . . . . . . . . 34\n2.8 Keyword Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.9 Notes on Parlance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3 Multi-Stage Architectures for Reranking 46\n3.1 A High-Level Overview of BERT . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.2 Simple Relevance Classiﬁcation: monoBERT . . . . . . . . . . . . . . . . . . . . 51\n3.2.1 Basic Design of monoBERT . . . . . . . . . . . . . . . . . . . . . . . . . 52\n3.2.2 Exploring monoBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n3.2.3 Investigating How BERT Works . . . . . . . . . . . . . . . . . . . . . . . 61\n3.2.4 Nuances of Training BERT . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n3.3 From Passage to Document Ranking . . . . . . . . . . . . . . . . . . . . . . . . . 67\n3.3.1 Document Ranking with Sentences: Birch . . . . . . . . . . . . . . . . . . 68\n3.3.2 Passage Score Aggregation: BERT–MaxP and Variants . . . . . . . . . . . 72\n3.3.3 Leveraging Contextual Embeddings: CEDR . . . . . . . . . . . . . . . . . 77\n3.3.4 Passage Representation Aggregation: PARADE . . . . . . . . . . . . . . . 82\n3.3.5 Alternatives for Tackling Long Texts . . . . . . . . . . . . . . . . . . . . . 86\n3.4 From Single-Stage to Multi-Stage Rerankers . . . . . . . . . . . . . . . . . . . . . 87\n3.4.1 Reranking Pairs of Texts . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n3.4.2 Reranking Lists of Texts . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n3.4.3 Efﬁcient Multi-Stage Rerankers: Cascade Transformers . . . . . . . . . . 94\n3.5 Beyond BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n2\n3.5.1 Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n3.5.2 Ranking with Transformers: TK, TKL, CK . . . . . . . . . . . . . . . . . 101\n3.5.3 Ranking with Sequence-to-Sequence Models: monoT5 . . . . . . . . . . . 104\n3.5.4 Ranking with Sequence-to-Sequence Models: Query Likelihood . . . . . . 109\n3.6 Concluding Thoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n4 Reﬁning Query and Document Representations 112\n4.1 Query and Document Expansion: General Remarks . . . . . . . . . . . . . . . . . 113\n4.2 Pseudo-Relevance Feedback with Contextualized Embeddings: CEQE . . . . . . . 115\n4.3 Document Expansion via Query Prediction: doc2query . . . . . . . . . . . . . . . 118\n4.4 Term Reweighting as Regression: DeepCT . . . . . . . . . . . . . . . . . . . . . . 122\n4.5 Term Reweighting with Weak Supervison: HDCT . . . . . . . . . . . . . . . . . . 125\n4.6 Combining Term Expansion with Term Weighting: DeepImpact . . . . . . . . . . 127\n4.7 Expansion of Query and Document Representations . . . . . . . . . . . . . . . . . 128\n4.8 Concluding Thoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n5 Learned Dense Representations for Ranking 132\n5.1 Task Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n5.2 Nearest Neighbor Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n5.3 Pre-BERT Text Representations for Ranking . . . . . . . . . . . . . . . . . . . . . 138\n5.4 Simple Transformer Bi-encoders for Ranking . . . . . . . . . . . . . . . . . . . . 139\n5.4.1 Basic Bi-encoder Design: Sentence-BERT . . . . . . . . . . . . . . . . . 141\n5.4.2 Bi-encoders for Dense Retrieval: DPR and ANCE . . . . . . . . . . . . . 143\n5.4.3 Bi-encoders for Dense Retrieval: Additional Variations . . . . . . . . . . . 148\n5.5 Enhanced Transformer Bi-encoders for Ranking . . . . . . . . . . . . . . . . . . . 150\n5.5.1 Multiple Text Representations: Poly-encoders and ME-BERT . . . . . . . 151\n5.5.2 Per-Token Representations and Late Interactions: ColBERT . . . . . . . . 153\n5.6 Knowledge Distillation for Transformer Bi-encoders . . . . . . . . . . . . . . . . 155\n5.7 Concluding Thoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n6 Future Directions and Conclusions 161\n6.1 Notable Content Omissions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n6.2 Open Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n6.3 Final Thoughts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\nAcknowledgements 171\nVersion History 172\nReferences 173\n3\n1 Introduction\nThe goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a\nquery for a particular task. The most common formulation of text ranking is search, where the search\nengine (also called the retrieval system) produces a ranked list of texts (web pages, scientiﬁc papers,\nnews articles, tweets, etc.) ordered by estimated relevance with respect to the user’s query. In this\ncontext, relevant texts are those that are “about” the topic of the user’s request and address the user’s\ninformation need. Information retrieval (IR) researchers call this the ad hoc retrieval problem.1\nWith keyword search, also called keyword querying (for example, on the web), the user typically\ntypes a few query terms into a search box (for example, in a browser) and gets back results containing\nrepresentations of the ranked texts. These results are called ranked lists, hit lists, hits, “ten blue\nlinks”,2 or search engine results pages (SERPs). The representations of the ranked texts typically\ncomprise the title, associated metadata, “snippets” extracted from the texts themselves (for example,\nan extractive keyword-in-context summary where the user’s query terms are highlighted), as well\nas links to the original sources. While there are plenty of examples of text ranking problems (see\nSection 1.1), this particular scenario is ubiquitous and undoubtedly familiar to all readers.\nThis survey provides an overview of text ranking with a family of neural network models known as\ntransformers, of which BERT (Bidirectional Encoder Representations from Transformers) [Devlin\net al., 2019], an invention of Google, is the best-known example. These models have been responsible\nfor a paradigm shift in the ﬁelds of natural language processing (NLP) and information retrieval (IR),\nand more broadly, human language technologies (HLT), a catch-all term that includes technologies\nto process, analyze, and otherwise manipulate (human) language data. There are few endeavors\ninvolving the automatic processing of natural language that remain untouched by BERT. 3 In the\ncontext of text ranking, BERT provides results that are undoubtedly superior in quality than what\ncame before. This is a robust and widely replicated empirical result, across many text ranking tasks,\ndomains, and problem formulations.\nA casual skim through paper titles in recent proceedings from NLP and IR conferences will leave the\nreader without a doubt as to the extent of the “BERT craze” and how much it has come to dominate the\ncurrent research landscape. However, the impact of BERT, and more generally, transformers, has not\nbeen limited to academic research. In October 2019, a Google blog post4 conﬁrmed that the company\nhad improved search “by applying BERT models to both ranking and featured snippets”. Ranking\nrefers to “ten blue links” and corresponds to most users’ understanding of web search; “feature\nsnippets” represent examples of question answering5 (see additional discussion in Section 1.1). Not to\nbe outdone, in November 2019, a Microsoft blog post6 reported that “starting from April of this year,\nwe used large transformer models to deliver the largest quality improvements to our Bing customers\nin the past year”.\nAs a speciﬁc instance of transformer architectures, BERT has no doubt improved how users ﬁnd\nrelevant information. Beyond search, other instances of the model have left their marks as well. For\nexample, transformers dominate approaches to machine translation, which is the automatic translation\nof natural language text7 from one human language to another, for example, from English to French.\n1There are many footnotes in this survey. Since nobody reads footnotes, we wanted to take one opportunity to\ninform the reader here that we’ve hidden lots of interesting details in the footnotes. But this message is likely to\nbe ignored anyway.\n2Here’s the ﬁrst interesting tidbit: The phrase “ten blue links” is sometimes used to refer to web search and has a\nfascinating history. Fernando Diaz helped us trace the origin of this phrase to a BBC article in 2004 [BBC,\n2004], where Tony Macklin, director of product at Ask UK, was quoted saying “searching is going to be about\nmore than just 10 blue links”. Google agreed: in 2010, Jon Wiley, Senior User Experience Designer for Google,\nsaid, “Google is no longer just ten blue links on a page, those days are long gone” [ReadWrite, 2010].\n3And indeed, programming languages as well [Alon et al., 2020, Feng et al., 2020]!\n4https://www.blog.google/products/search/search-language-understanding-bert/\n5https://support.google.com/websearch/answer/9351707\n6https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-\nsearch-experience-using-azure-gpus/\n7A machine translation system can be coupled with an automatic speech recognition system and a speech\nsynthesis system to perform speech-to-speech translation—like a primitive form of the universal translator from\nStar Trek or (a less annoying version of) C-3PO from Star Wars!\n4\nBlog posts by both Facebook8 and Google9 tout the effectiveness of transformer-based architectures.\nOf course, these are just the high-proﬁle announcements. No doubt many organizations—from\nstartups to Fortune 500 companies, from those in the technology sector to those in ﬁnancial services\nand beyond—have already or are planning to deploy BERT (or one of its siblings or intellectual\ndecedents) in production.\nTransformers were ﬁrst presented in June 2017 [Vaswani et al., 2017] and BERT was unveiled in\nOctober 2018.10 Although both are relatively recent inventions, we believe that there is a sufﬁcient\nbody of research such that the broad contours of how to apply transformers effectively for text ranking\nhave begun to emerge, from high-level design choices to low-level implementation details. The “core”\naspects of how BERT is used—for example, as a relevance classiﬁer—is relatively mature. Many of\nthe techniques we present in this survey have been applied in many domains, tasks, and settings, and\nthe improvements brought about by BERT (and related models) are usually substantial and robust. It\nis our goal to provide a synthesis of existing work as a single point of entry for practitioners who\nwish to gain a better understanding of how to apply BERT to text ranking problems and researchers\nwho wish to pursue further advances in this area.\nLike nearly all scientiﬁc advances, BERT was not developed in a vacuum, but built on several\nprevious innovations, most notably the transformer architecture itself [Vaswani et al., 2017] and the\nidea of self-supervised pretraining based on language modeling objectives, previously explored by\nULMFiT [Howard and Ruder, 2018] and ELMo (Embeddings from Language Models) [Peters et al.,\n2018]. Both ideas initially came together in GPT (Generative Pretrained Transformer) [Radford et al.,\n2018], and the additional innovation of bidirectional training culminated in BERT (see additional\ndiscussions about the history of these developments in Section 3.1). While it is important to\nrecognize previous work, BERT is distinguished in bringing together many crucial ingredients\nto yield tremendous leaps in effectiveness on a broad range of natural language processing tasks.\nTypically, “training” BERT (and in general, pretrained models) to perform a downstream task involves\nstarting with a publicly available pretrained model (often called a “model checkpoint”) and then\nfurther ﬁne-tuning the model using task-speciﬁc labeled data. In general, the computational and\nhuman effort involved in ﬁne-tuning is far less than pretraining. The commendable decision by\nGoogle to open-source BERT and to release pretrained models supported widespread replication\nof the impressive results reported by the authors and additional applications to other tasks, settings,\nand domains. The rapid proliferation of these BERT applications was in part due to the relatively\nlightweight ﬁne-tuning process. BERT supercharged subsequent innovations by providing a solid\nfoundation to build on.\nThe germinal model, in turn, spawned a stampede of other models differing to various extents in archi-\ntecture, but nevertheless can be viewed as variations on its main themes. These include ERNIE [Sun\net al., 2019b], RoBERTa [Liu et al., 2019c], Megatron-LM [Shoeybi et al., 2019], XLNet [Yang\net al., 2019f], DistilBERT [Sanh et al., 2019], ALBERT [Lan et al., 2020], ELECTRA [Clark et al.,\n2020b], Reformer [Kitaev et al., 2020], DeBERTa [He et al., 2020], Big Bird [Zaheer et al., 2020],\nand many more. Additional pretrained sequence-to-sequence transformer models inspired by BERT\n8https://engineering.fb.com/ai-research/scaling-neural-machine-translation\n9https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html\n10The nature of academic publishing today means that preprints are often available (e.g., on arXiv) several\nmonths before the formal publication of the work in a peer-reviewed venue (which is increasingly becoming a\nformality). For example, the BERT paper was ﬁrst posted on arXiv in October 2018, but did not appear in a\npeer-reviewed venue until June 2019, at NAACL 2019 (a top conference in NLP) . Throughout this survey,\nwe attribute innovations to their earliest known preprint publication dates, since that is the date when a work\nbecomes “public” and available for other researchers to examine, critique, and extend. For example, the earliest\nuse of BERT for text ranking was reported in January 2019 [Nogueira and Cho, 2019], a scant three months\nafter the appearance of the original BERT preprint and well before the peer-reviewed NAACL publication.\nThe rapid pace of progress in NLP, IR, and other areas of computer science today means that by the time an\ninnovation formally appears in a peer-reviewed venue, the work is often already “old news”, and in some\ncases, as with BERT, the innovation had already become widely adopted. In general, we make an effort to cite\nthe peer-reviewed version of a publication unless there is some speciﬁc reason otherwise, e.g., to establish\nprecedence. At the risk of bloating this already somewhat convoluted footnote even more, there’s the additional\ncomplication of a conference’s submission deadline. Clearly, if a paper got accepted at a conference, then the\nwork must have existed at the submission deadline, even if it did not appear on arXiv. So how do we take\nthis into account when establishing precedence? Here, we just throw up our hands and shrug; at this point,\n“contemporaneous” would be a fair characterization.\n5\ninclude T5 [Raffel et al., 2020], UniLM [Dong et al., 2019], PEGASUS [Zhang et al., 2020c], and\nBART [Lewis et al., 2020b].\nAlthough a major focus of this survey is BERT, many of the same techniques we describe can\n(and have been) applied to its descendants and relatives as well, and BERT is often incorporated as\npart of a larger neural ranking model (as Section 3 discusses in detail). While BERT is no doubt\nthe “star of the show”, there are many exciting developments beyond BERT being explored right\nnow: the application of sequence-to-sequence transformers, transformer variants that yield more\nefﬁcient inference, ground-up redesigns of transformer architectures, and representation learning with\ntransformers—just to name a few (all of which we will cover). The diversity of research directions\nbeing actively pursued by the research community explains our choice for the subtitle of this survey\n(“BERT and Beyond”). While many aspects of the application of BERT and transformers to text\nranking can be considered “mature”, there remain gaps in our knowledge and open research questions\nyet to be answered. Thus, in addition to synthesizing the current state of knowledge, we discuss\ninteresting unresolved issues and highlight where we think the ﬁeld is going.\nLet us begin!\n1.1 Text Ranking Problems\nWhile our survey opens with search (speciﬁcally, what information retrieval researchers call ad hoc\nretrieval) as the motivating scenario due to the ubiquity of search engines, text ranking appears in\nmany other guises. Beyond typing keywords into a search box and getting back “ten blue links”,\nexamples of text ranking abound in scenarios where users desire access to relevant textual information,\nin a broader sense.\nConsider the following examples:\nQuestion Answering (QA). Although there are many forms question answering, the capability that\nmost users have experience with today appears in search engines as so-called “infoboxes” or what\nGoogle calls “featured snippets”11 that appear before (or sometimes to the right of) the main search\nresults. In the context of a voice-capable intelligent agent such as Siri or Alexa, answers to user\nquestions are directly synthesized using text-to-speech technology. The goal is for the system to\nidentify (or extract) a span of text that directly answers the user’s question, instead of returning a\nlist of documents that the user must then manually peruse. In “factoid” question answering, systems\nprimarily focus on questions that can be answered with short phrases or named entities such as dates,\nlocations, organizations, etc.\nAlthough the history of question answering systems dates back to the 1960s [Simmons, 1965], modern\nextractive approaches (i.e., that is, techniques focused on extracting spans of text from documents)\ntrace their roots to work that began in the late 1990s [V oorhees, 2001]. Most architectures that adopt\nan extractive approach break the QA challenge into two steps: First, select passages of text from\na potentially large corpus that are likely to contain answers, and second, apply answer extraction\ntechniques to identify the answer spans. In the modern neural context, Chen et al. [2017a] called this\nthe retriever–reader framework. The ﬁrst stage (i.e., the “retriever”) is responsible for tackling the text\nranking problem. Although question answering encompasses more than just extractive approaches or\na focus on factoid questions, in many cases methods for approaching these challenges still rely on\nretrieving texts from a corpus as a component.\nCommunity Question Answering (CQA). Users sometimes search for answers not by attempting\nto ﬁnd relevant information directly, but by locating another user who has asked the same or similar\nquestion, for example, in a frequently-asked questions (FAQ) list or in an online forum such as Quora\nor Stack Overﬂow. Answers to those questions usually address the user’s information need. This\nmode of searching, which dates back to the late 1990s [Burke et al., 1997], is known as community\nquestion answering (CQA) [Srba and Bielikova, 2016]. Although it differs from traditional keyword-\nbased querying, CQA is nevertheless a text ranking problem. One standard approach formulates the\nproblem as estimating semantic similarity between two pieces of texts—more speciﬁcally, if two\nnatural language questions are paraphrases of each other. A candidate list of questions (for example,\nbased on keyword search) is sorted by the estimated degree of “paraphrase similarity” (for example,\nthe output of a machine-learned model) and the top-kresults are returned to the user.\n11https://blog.google/products/search/reintroduction-googles-featured-snippets/\n6\nInformation Filtering. In search, queries are posed against a (mostly) static collection of texts.\nFiltering considers the opposite scenario where a (mostly) static query is posed against a stream of\ntexts. Two examples of this mode of information seeking might be familiar to many readers: push\nnotiﬁcations that are sent to a user’s mobile device whenever some content of interest is published\n(could be a news story or a social media post); and, in a scholarly context, email digests that are sent\nto users whenever a paper that matches the user’s interest is published (a feature available in Google\nScholar today). Not surprisingly, information ﬁltering has a long history, dating back to the 1960s,\nwhen it was called “selective dissemination of information” (SDI); see Housman and Kaskela [1970]\nfor a survey of early systems. The most recent incarnation of this idea is “real-time summarization”\nin the context of social media posts on Twitter, with several community-wide evaluations focused on\nnotiﬁcation systems that inform users in real time about relevant content as it is being generated [Lin\net al., 2016]. Before that, document ﬁltering was explored in the context of the TREC Filtering\nTracks, which ran from 1995 [Lewis, 1995] to 2002 [Robertson and Soboroff, 2002], and the general\nresearch area of topic detection and tracking, also known as TDT [Allan, 2002]. The relationship\nbetween search and ﬁltering has been noted for decades: Belkin and Croft [1992] famously argued\nthat they represented “two sides of the same coin”. Models that attempt to capture relevance forad\nhoc retrieval can also be adapted for information ﬁltering.\nText Recommendation. When a search system is displaying a search result, it might suggest other\ntexts that may be of interest to the user, for example, to assist in browsing [Smucker and Allan, 2006].\nThis is frequently encountered on news sites, where related articles of interest might offer background\nknowledge or pointers to related news stories [Soboroff et al., 2018]. In the context of searching the\nscientiﬁc literature, the system might suggest papers that are similar in content: An example of this\nfeature is implemented in the PubMed search engine, which provides access to the scientiﬁc literature\nin the life sciences [Lin and Wilbur, 2007]. Citation recommendation [Ren et al., 2014, Bhagavatula\net al., 2018] is another good example of text recommendation in the scholarly context. All of these\nchallenges involve text ranking.\nText Ranking as Input to Downstream Modules. The output of text ranking may not be intended\nfor direct user consumption, but may rather be meant to feed downstream components: for example,\nan information extraction module to identify key entities and relations [Gaizauskas and Robertson,\n1997], a summarization module that attempts to synthesize information from multiple sources with\nrespect to an information need [Dang, 2005], a clustering module that organizes texts based on content\nsimilarity [Vadrevu et al., 2011], or a browsing interface for exploration and discovery [Sadler, 2009].\nEven in cases where a ranked list of results is not directly presented to the user, text ranking may still\nform an important component technology in a larger system.\nWe can broadly characterize ad hoc retrieval, question answering, and the different tasks described\nabove as “information access”—a term we use to refer to these technologies collectively. Text ranking\nis without a doubt an important component of information access.\nHowever, beyond information access, examples of text ranking abound in natural language processing.\nFor example:\nSemantic Similarity Comparisons. The question of whether two texts “mean the same thing” is a\nfundamental problem in natural language processing and closely related to the question of whether\na text is relevant to a query. While there are some obvious differences, researchers have explored\nsimilar approaches and have often even adopted the same models to tackle both problems. In the\ncontext of learned dense representations for ranking, the connections between these two problems\nhave become even more intertwined, bringing the NLP and IR communities closer and further erasing\nthe boundaries between text ranking, question answering, paraphrase detection, and many related\nproblems. Since Section 5 explores these connections in detail, we will not further elaborate here.\nDistant Supervision and Data Augmentation. Training data form a crucial ingredient in NLP\napproaches based on supervised machine learning. All things being equal, the more data the better,12\nand so there is a never-ending quest for practitioners and researchers to acquire more, more, and\nmore! Supervised learning requires training examples that have been annotated for the speciﬁc\n12A well-known observation dating back at least decades; see, for example, Banko and Brill [2001].\n7\ntask, typically by humans, which is a labor-intensive process. For example, to train a sentiment\nclassiﬁer, we must somehow acquire a corpus of texts in which each instance has been labeled with\nits sentiment (e.g., positive or negative). There are natural limits to the amount of data that can be\nacquired via human annotation: in the sentiment analysis example, we can automatically harvest\nvarious online sources that have “star ratings” associated with texts (e.g., reviews), but even these\nlabels are ultimately generated by humans. This is a form of crowdsourcing, and merely shifts the\nsource of the labeling effort, but does not change the fundamental need for human annotation.\nResearchers have extensively explored many techniques to overcome the data bottleneck in supervised\nmachine learning. At a high level, distant supervision and data augmentation represent two successful\napproaches, although in practice they are closely related. Distant supervision involves training models\nusing low-quality “weakly” labeled examples that are gathered using heuristics and other simple but\nnoisy techniques. One simple example is to assume that all emails mentioning Viagra are spam for\ntraining a spam classiﬁer; obviously, there are “legitimate” non-spam emails (called “ham”) that\nuse the term, but the heuristic may be a reasonable way to build an initial classiﬁer [Cormack et al.,\n2011]. We give this example because it is easy to convey, but the general idea of using heuristics to\nautomatically gather training examples to train a classiﬁer in NLP dates back to Yarowsky [1995], in\nthe context of word sense disambiguation.13\nData augmentation refers to techniques that exploit a set of training examples to gather or create\nadditional training examples. For example, given a corpus of English sentences, we could translate\nthem automatically using a machine translation (MT) system, say, into French, and then translate\nthose sentences back into English (this is called back-translation).14 With a good MT system, the\nresulting sentences are likely paraphrases of the original sentence, and using this technique we can\nautomatically increase the quantity and diversity of the training examples that a model is exposed to.\nText ranking lies at the heart of many distant supervision and data augmentation techniques for\nnatural language processing. We illustrate with relation extraction, which is the task of identifying\nand extracting relationships in natural language text. For example, from the sentence “Albert Einstein\nwas born in Ulm, in the Kingdom of Württemberg in the German Empire, on 14 March 1879”, a\nsystem could automatically extract the relation birthdate(Albert Einstein, 1879/03/14); these\nare referred to as “tuples” or extracted facts. Relations usually draw from a relatively constrained\nvocabulary (dozens at most), but can be domain speciﬁc, for example, indicating that a gene regulates\na protein (in the biomedical domain).\nOne simple technique for distant supervision is to search for speciﬁc patterns or “cue phrases” such as\n“was born in” and take the tokens occurring to the left and to the right of the phrase as participating in\nthe relation (i.e., they form the tuple). These tuples, together with the source documents, can serve as\nnoisy training data. One simple technique for data augmentation is to take already known tuples, e.g.,\nAlbert Einstein and his birthdate, and search a corpus for sentences that contain those tokens (e.g., by\nexact or approximate string matching). Furthermore, we can combine the two techniques iteratively:\nsearch with a pattern, identify tuples, ﬁnd texts with those tuples, and from those learn more patterns,\ngoing around and around.15 Proposals along these lines date back to the late 1990s [Riloff, 1996, Brin,\n1998, Agichtein and Gravano, 2000].16 Obviously, training data and extracted tuples gathered in this\nmanner are noisy, but studies have empirically shown that such approaches are cheap when used alone\nand effective in combination with supervised techniques. See Smirnova and Cudré-Mauroux [2018]\n13Note that the term “distant supervision” was coined in the early 2000s, so it would be easy to miss these early\npapers by keyword search alone; Yarowsky calls his approach “unsupervised”.\n14The “trick” of translating a sentence from one language into another and then back again is nearly old as\nmachine translation systems themselves. An apocryphal story from the 1960s goes that with an early English–\nRussian MT system, the phrase “The spirit is willing, but the ﬂesh is weak” translated into Russian and back\ninto English again became “The whisky is strong, but the meat is rotten” [Hutchins, 1995] (in some accounts,\nwhisky is replaced with vodka). The earliest example we could ﬁnd of using this trick to generate synthetic\ntraining data is Alshawi et al. [1997]. Bannard and Callison-Burch [2005] is often cited for using “pivot\nlanguages” (the other language we translate into and back) as anchors for automatically extracting paraphrases\nfrom word alignments.\n15The general idea of training a machine learning model on its own output, called self-training, dates back to at\nleast the 1960s [Scudder, 1965].\n16Although, once again, they did speciﬁcally use the modern terminology of distant supervision and data\naugmentation.\n8\nfor a survey of distant supervision techniques applied to relation extraction, and Snorkel [Ratner et al.,\n2017] for a modern implementation of these ideas.\nWrapped inside these distant supervision and data augmentation techniques are usually variants\nof text ranking problems, centered around the question of “is this a good training example?” For\nexample, given a collection of sentences that match a particular pattern, or when considering multiple\npatterns, which ones are “good”? Answering this question requires ranking texts with respect to\nthe quality of the evidence, and many scoring techniques proposed in the above-cited papers share\nsimilarities with the probabilistic framework for relevance [Robertson and Zaragoza, 2009].\nAn entirely different example comes from machine translation: In modern systems, such as those\nbuilt by Facebook and Google referenced in the introduction, translation models are learned from\na parallel corpus (also called bitext), comprised of pairs of sentences in two languages that are\ntranslations of each other [Tiedemann, 2011]. Some parallel corpora can be found “naturally” as the\nbyproduct of an organization’s deliberate effort to disseminate information in multiple languages, for\nexample, proceedings of the Canadian Parliament in French and English [Brown et al., 1990], and\ntexts produced by the United Nations in many different languages. In modern data-driven approaches\nto machine translation, these pairs serve as the input for training translation models.\nSince there are limits to the amount of parallel corpora available, researchers have long explored\ntechniques that can exploit comparable data, or texts in different languages that are topically similar\n(i.e., “talk about the same thing”) but are not necessarily translations of each other [Resnik and\nSmith, 2003, Munteanu and Marcu, 2005, Smith et al., 2010]. Techniques that can take advantage of\ncomparable corpora expand the scope and volume of data that can be thrown at the machine translation\nproblem, since the restriction for semantic equivalence is relaxed. Furthermore, researchers have\ndeveloped techniques for mining comparable corpora automatically at scale [Uszkoreit et al., 2010,\nTure and Lin, 2012]. These can be viewed as a cross-lingual text ranking problem [Ture et al., 2011]\nwhere the task is to estimate the semantic similarity between sentences in different languages, i.e., if\nthey are mutual translations.\nSelecting from Competing Hypotheses. Many natural language tasks that involve selecting from\ncompeting hypotheses can be formulated as text ranking problems, albeit on shorter segments of text,\npossibly integrated with additional features. The larger the hypothesis space, the more crucial text\nranking becomes as a method to ﬁrst reduce the number of candidates under consideration.\nThere are instances of text ranking problems in “core” NLP tasks that at ﬁrst glance have nothing\nto do with text ranking. Consider the semantic role labeling problem [Gildea and Jurafsky, 2001,\nPalmer et al., 2010], where the system’s task is to populate “slots” in a conceptual “frame” with\nentities that ﬁll the “semantic roles” deﬁned by the frame. For example, the sentence “John sold his\nviolin to Mary” depicts a COMMERCIAL TRANSACTION frame, where “John” is the SELLER , Mary\nis the BUYER , and the violin is the GOODS transacted. One strategy for semantic role labeling is\nto identify all entities in the sentence, and for each slot, rank the entities by the likelihood that each\nplays that role. For example, is “John”, “Mary”, or “the violin” most likely to be theSELLER ? This\nranking formulation can be augmented by attempts to perform joint inference to resolve cases where\nthe same entity is identiﬁed as the most likely ﬁller of more than one slot; for example, resolving\nthe case where a model (independently) identiﬁes “John” erroneously as both the most likely buyer\nand the most likely seller (which is semantically incoherent). Although the candidate entities are\nshort natural language phrases, they can be augmented with a number of features, in which case the\nproblem begins to share characteristics with ranking in a vector space model. While the number of\nentities to be ranked is not usually very big, what’s important is the amount of evidence (i.e., different\nfeatures) used to estimate the probability that an entity ﬁlls a role, which isn’t very different from\nrelevance classiﬁcation (see Section 3.2).\nAnother problem that lends itself naturally to a ranking formulation is entity linking, where the task\nis to resolve an entity with respect to an external knowledge source such as Wikidata [Vrande ˇci´c\nand Krötzsch, 2014]. For example, in a passage of text that mentions Adam Smith, which exact\nperson is being referenced? Is it the famous 18th century Scottish economist and moral philosopher,\nor one of the lessor-known individuals that share the same name? An entity linking system “links”\nthe instance of the entity mention (in a piece of text) to a unique id in the knowledge source: the\nScottish economist has the unique id Q9381,17 while the other individuals have different ids. Entity\n17https://www.wikidata.org/wiki/Q9381\n9\nlinking can be formulated as a ranking problem, where candidates from the knowledge source are\nranked in terms of their likelihood of being the actual referent of a particular mention [Shen et al.,\n2015]. This is an instance of text ranking because these candidates are usually associated with textual\ndescriptions—for example, a short biography of the individual—which forms crucial evidence. Here,\nthe “query” is the entity to be linked, represented not only by its surface form (i.e., the mention\nstring), but also the context in which the entity appears. For example, if the text discusses the Wealth\nof Nations, it’s likely referencing the famous Scot.\nYet another example of text ranking in a natural language task that involves selecting from competing\nhypotheses is the problem of fact veriﬁcation [Thorne et al., 2018], for example, to combat the\nspread of misinformation online. Verifying the veracity of a claim requires fetching supporting\nevidence from a possibly large corpus and assessing the credibility of those sources. The ﬁrst step\nof gathering possible supporting evidence is a text ranking problem. Here, the hypothesis space\nis quite large (passages from an arbitrarily large corpus), and thus text ranking plays a critical\nrole. In the same vein, for systems that engage in or assist in human dialogue, such as intelligent\nagents or “chatbots”, one common approach to generating responses (beyond question answering and\ninformation access discussed above) is to retrieve possible responses from a corpus (and then perhaps\nmodifying them) [Henderson et al., 2017, Dinan et al., 2019, Roller et al., 2020]. Here, the task is to\nrank possible responses with respect to their appropriateness.\nThe point of this discussion is that while search is perhaps the most visible instance of the text\nranking problem, there are manifestations everywhere—not only in information retrieval but also\nnatural language processing. This exposition also explains our rationale in intentionally using the\nterm “text ranking” throughout this survey, as opposed to the more popular term “document ranking”.\nIn many applications, the “atomic unit” of text to be ranked isnot a document, but rather a sentence,\na paragraph, or even a tweet; see Section 2.1 and Section 2.9 for more discussions.\nTo better appreciate how BERT and transformers have revolutionized text ranking, it is ﬁrst necessary\nto understand “how we got here”. We turn our attention to this next in a brief exposition of important\ndevelopments in information retrieval over the past three quarters of a century.\n1.2 A Brief History\nThe vision of exploiting computing machines for information access is nearly as old as the invention\nof computing machines themselves, long before computer science emerged as a coherent discipline.\nThe earliest motivation for developing information access technologies was to cope with the explosion\nof scientiﬁc publications in the years immediately following World War II.18 Vannevar Bush’s often-\ncited essay in The Atlantic in July 1945, titled “As We May Think” [Bush, 1945], described a\nhypothetical machine called the “memex” that performs associative indexing to connect arbitrary\nitems of content stored on microﬁlm, as a way to capture insights and to augment the memory of\nscientists. The article describes technologies that we might recognize today as capturing aspects of\npersonal computers, hypertext, the Semantic Web, and online encyclopedias.19 A clearer description\nof what we might more easily identify today as a search engine was provided by Holmstrom [1948],\nalthough discussed in terms of punch-card technology!\n1.2.1 The Beginnings of Text Ranking\nAlthough the need for machines to improve information access was identiﬁed as early as the mid-\n1940s, interestingly, the conception of text ranking was still a decade away. Libraries, of course, have\nexisted for millennia, and the earliest formulations of search were dominated by the automation of\nwhat human librarians had been doing for centuries: matching based on human-extracted descriptors\n18Scholars have been complaining about there being more information than can be consumed since shortly after\nthe invention of the printing press. “Is there anywhere on earth exempt from these swarms of new books?\nEven if, taken out one at a time, they offered something worth knowing, the very mass of them would be an\nimpediment to learning from satiety if nothing else”, the philosopher Erasmus complained in the 16th century.\n19Bush talks about naming “trails”, which are associations between content items. Today, we might call these\nsubject–verb–object triples. Viewed from this perspective, the memex is essentially a graph store! Furthermore,\nhe envisioned sharing these annotations, such that individuals can build on each others’ insights. Quite\nremarkably, the article mentions text-to-speech technology and speech recognition, and even speculates on\nbrain–computer interfaces!\n10\nof content stored on physical punch-card representations of the texts to be searched (books, scientiﬁc\narticles, etc.). These descriptors (also known as “index terms”) were usually assigned by human\nsubject matter experts (or at least trained human indexers) and typically drawn from thesauri, “subject\nheadings”, or “controlled vocabularies”—that is, a predeﬁned vocabulary. This process was known\nas “indexing”—the original sense of the activity involved humans, and is quite foreign to modern\nnotions that imply automated processing—or is sometimes referred to as “abstracting”. 20 Issuing\nqueries to search content required librarians (or at least trained individuals) to translate the searcher’s\ninformation need into these same descriptors; search occurs by matching these descriptors in a\nboolean fashion (hence, no ranking).\nAs a (radical at the time) departure from this human-indexing approach, Luhn [1958] proposed\nconsidering “statistical information derived from word frequency and distribution . . . to compute\na relative measure of signiﬁcance”, thus leading to “auto-abstracts”. He described a precursor of\nwhat we would recognize today as tf–idf weighting (that is, term weights based on term frequency\nand inverse document frequency). However, Luhn neither implemented nor evaluated any of the\ntechniques he proposed.\nA clearer articulation of text ranking was presented by Maron and Kuhns [1960], who characterized\nthe information retrieval problem (although they didn’t use these words) as receiving requests from\nthe user and “to provide as an output an ordered list of those documents which most probably satisfy\nthe information needs of the user”. They proposed that index terms (“tags”) be weighted according to\nthe probability that a user desiring information contained in a particular document would use that\nterm in a query. Today, we might call this query likelihood [Ponte and Croft, 1998]. The paper also\ndescribed the idea of a “relevance number” for each document, “which is a measure of the probability\nthat the document will satisfy the given request”. Today, we would call these retrieval scores. Beyond\nlaying out these foundational concepts, Maron and Kuhns described experiments to test their ideas.\nWe might take for granted today the idea that automatically extracted terms from a document can\nserve as descriptors or index terms for describing the contents of those documents, but this was an\nimportant conceptual leap in the development of information retrieval.\nThroughout the 1960s and 1970s, researchers and practitioners debated the merits of “automatic\ncontent analysis” (see, for example, Salton [1968]) vs. “traditional” human-based indexing. Salton\n[1972] described a notable evaluation comparing the SMART retrieval system based on the vector\nspace model with human-based indexing in the context of MEDLARS (Medical Literature Analysis\nand Retrieval System), which was a computerized version of the Index Medicus, a comprehensive print\nbibliographic index of medical articles that the U.S. National Library of Medicine (NLM) had been\npublishing since 1879. SMART was shown to produce higher-quality results, and Salton concluded\n“that no technical justiﬁcation exists for maintaining controlled, manual indexing in operational\nretrieval environments”. This thread of research has had signiﬁcant impact, as MEDLARS evolved\ninto MEDLINE (short for MEDLARS onLINE). In the internet era, MEDLINE became publicly\naccessible via the PubMed search engine, which today remains the authoritative bibliographic\ndatabase for the life sciences literature.\nThe mode of information access we take for granted today—based on ranking automatically con-\nstructed representations of documents and queries—gradually gained acceptance, although the history\nof information retrieval showed this to be an uphill battle. Writing about the early history of infor-\nmation retrieval, Harman [2019] goes as far as to call these “indexing wars”: the battle between\nhuman-derived and automatically-generated index terms. This is somewhat reminiscent of the rule-\nbased vs. statistical NLP “wars” that raged beginning in the late 1980s and into the 1990s, and goes\nto show how foundational shifts in thinking are often initially met with resistance. Thomas Kuhn\nwould surely ﬁnd both these two cases to be great examples supporting his views on the structure of\nscientiﬁc revolutions [Kuhn, 1962].\nBringing all the major ideas together, Salton et al. [1975] is frequently cited for the proposal of the\nvector space model, in which documents and queries are both represented as “bags of words” using\nsparse vectors according to some term weighting scheme (tf–idf in this case), where document–query\nsimilarity is computed in terms of cosine similarity (or, more generally, inner products). However, this\ndevelopment did not happen all at once, but represented innovations that gradually accumulated over\n20Thus, an indexer is a human who performs indexing, not unlike the earliest uses of computers to refer to\nhumans who performed computations by hand.\n11\nthe two preceding decades. For additional details about early historical developments in information\nretrieval, we refer the reader to Harman [2019].\n1.2.2 The Challenges of Exact Match\nFor the purposes of establishing a clear contrast with neural network models, the most salient feature\nof all approaches up to this point in history is their reliance exclusively on what we would call today\nexact term matching—that is, terms from documents and terms from queries had to match exactly to\ncontribute to a relevance score. Since systems typically perform stemming—that is, the elimination of\nsufﬁxes (in English)—matching occurs after terms have been normalized to some extent (for example,\nstemming would ensure that “dog” matches “dogs”).\nNevertheless, with techniques based on exact term matching, a scoring function between a query q\nand a document dcould be written as:\nS(q,d) =\n∑\nt∈q∩d\nf(t) (1)\nwhere f is some function of a term and its associated statistics, the three most important of which are\nterm frequency (how many times a term occurs in a document), document frequency (the number\nof documents that contain at least once instance of the term), and document length (the length of\nthe document that the term occurs in). It is from the ﬁrst two statistics that we derive the ubiquitous\nscoring function tf–idf, which stands for term frequency, inverse document frequency. In the vector\nspace model, cosine similarity has a length normalization component that implicitly handles issues\nrelated to document length.\nA major thread of research in the 1980s and into the 1990s was the exploration of different term\nweighting schemes in the vector space model [Salton and Buckley, 1988a], based on easily computed\nterm-based statistics such as those described above. One of the most successful of these methods,\nOkapi BM25 [Robertson et al., 1994, Crestani et al., 1999, Robertson and Zaragoza, 2009], still\nprovides the starting point of many text ranking approaches today, both in academic research as well\nas commercial systems.21\nGiven the importance of BM25, the exact scoring function is worth repeating to illustrate what a\nranking model based on exact term matching looks like. The relevance score of a document dwith\nrespect to a query qis deﬁned as:\nBM25(q,d) =\n∑\nt∈q∩d\nlog N −df(t) + 0.5\ndf(t) + 0.5 · tf(t,d) ·(k1 + 1)\ntf(t,d) + k1 ·\n(\n1 −b+ b·ld\nL\n) (2)\nAs BM25 is based on exact term matching, the score is derived from a sum of contributions from\neach query term that appears in the document. In more detail:\n• The ﬁrst component of the summation (the log term) is the idf (inverse document frequency)\ncomponent: N is the total number of documents in the corpus, and df(t) is the number of\ndocuments that contain term t(i.e., its document frequency).\n• In the second component of the summation, tf(t,d) represents the number of times term t\nappears in document d(i.e., its term frequency). The expression in the denominator involving b\nis responsible for performing length normalization, since collections usually have documents\nthat differ in length: ld is the length of document dwhile Lis the average document length\nacross all documents in the collection.\nFinally, k1 and bare free parameters. Note that the original formulation by Robertson et al. [1994]\nincludes additional scoring components with parameters k2 and k3, but they are rarely used and\nare often omitted from modern implementations. In addition to the original scoring function de-\nscribed above, there are several variants that have been discussed in the literature, including the one\nimplemented in the popular open-source Lucene search library; see Section 2.8 for more details.\n21Strictly speaking, BM25 derives from the probabilistic retrieval framework, but its ultimate realization is\na weighting scheme based on a probabilistic interpretation of how terms contribute to document relevance.\nRetrieval is formulated in terms of inner products on sparse bag-of-words vectors, which is operationally\nidentical to the vector space model; see, for example, Crestani et al. [1999].\n12\nWhile term weighting schemes can model term importance (sometimes called “salience”) based on\nstatistical properties of the texts, exact match techniques are fundamentally powerless in cases where\nterms in queries and documents don’t match at all. This happens quite frequently, when searchers\nuse different terms to describe their information needs than what authors of the relevant documents\nused. One way of thinking about search is that an information seeker is trying to guess the terms\n(i.e., posed as the query) that authors of relevant texts would have used when they wrote the text\n(see additional discussion in Section 2.2). We’re looking for a “tragic love story” but Shakespeare\nwrote about “star-crossed lovers”. To provide a less poetic, but more practical example, what we\ncall “information ﬁltering” today was known as “selective dissemination of information (SDI)” in the\n1960s (see Section 1.1). Imagine the difﬁculty we would face trying to conduct a thorough literature\nreview without knowing the relationship between these key terms. Yet another example, also from\nSection 1.1: early implementations of distant supervision did not use the term “distant supervision”.\nIn both these cases, it would be easy to (falsely) conclude that no prior work exists beyond recent\npapers that use contemporary terminology!\nThese are just two examples of the “vocabulary mismatch problem” [Furnas et al., 1987], which\nrepresents a fundamental challenge in information retrieval. There are three general approaches to\ntackling this challenge: enrich query representations to better match document representations, enrich\ndocument representations to better match query representations, and attempts to go beyond exact\nterm matching:\n• Enriching query representations. One obvious approach to bridge the gap between query and\ndocument terms is to enrich query representations with query expansion techniques [Carpineto\nand Romano, 2012]. In relevance feedback, the representation of the user’s query is augmented\nwith terms derived from documents that are known to be relevant (for example, documents\nthat have been presented to the user and that the user has indicated is relevant): two popular\nformulations are based on the vector space model [Rocchio, 1971] and the probabilistic retrieval\nframework [Robertson and Spark Jones, 1976]. In pseudo-relevance feedback [Croft and Harper,\n1979], also called “blind” relevance feedback, top-ranking documents are simplyassumed to\nbe relevant, thus providing a source for additional query terms. Query expansion techniques,\nhowever, do not need to involve relevance feedback: examples include Xu and Croft [2000], who\nintroduced global techniques that identify word relations from the entire collection as possible\nexpansion terms (this occurs in a corpus preprocessing step, independent of any queries),\nand V oorhees [1994], who experimented with query expansion using lexical-semantic relations\nfrom WordNet [Miller, 1995]. A useful distinction when discussing query expansion techniques\nis the dichotomy between pre-retrieval techniques, where expansion terms can be computed\nwithout examining any documents from the collection, and post-retrieval techniques, which are\nbased on analyses of documents from an initial retrieval. Section 4 discusses query expansion\ntechniques in the context of transformers.\n• Enriching document representations. Another obvious approach to bridge the gap between\nquery and document terms is to enrich document representations. This strategy works well for\nnoisy transcriptions of speech [Singhal and Pereira, 1999] and short texts such as tweets [Efron\net al., 2012]. Although not as popular as query expansion techniques, researchers nevertheless\nexplored this approach throughout the 1980s and 1990s [Salton and Buckley, 1988b, V oorhees\nand Hou, 1993]. The origins of document expansion trace even earlier to Kwok [1975], who\ntook advantage of bibliographic metadata for expansion, and ﬁnally, Brauen et al. [1968], who\nused previously issued user queries to modify the vector representation of a relevant document.\nHistorically, document expansion techniques have not been as popular as query expansion\ntechniques, but we have recently witnessed a resurgence of interest in document expansion in\nthe context of transformers, which we cover in Section 4.\n• Beyond exact term matching.Researchers have investigated models that attempt to address the\nvocabulary mismatch problem without explicitly enriching query or document representations.\nA notable attempt is the statistical translation approach of Berger and Lafferty [1999], who\nmodeled retrieval as the translation of a document into a query in a noisy channel model.\nTheir approach learns translation probabilities between query and document terms, but these\nnevertheless represent mappings between terms in the vocabulary space of the documents. Other\nexamples of attempts to go beyond exact match include techniques that attempt to perform\nmatching in some semantic space induced from data, for example, based on latent semantic\nanalysis [Deerwester et al., 1990] or latent Dirichlet allocation [Wei and Croft, 2006]. However,\n13\nneither approach has gained widespread adoption as serious competition to keyword-based\nquerying. Nevertheless, there are clear connections between this thread of work and learned\ndense representations for ranking, which we detail in Section 5.\nAt a high level, retrieval models up until this time contrast with “soft” or semantic matching enabled\nby continuous representations in neural networks, where query terms do not have to match document\nterms exactly in order to contribute to relevance. Semantic matching refers to techniques and attempts\nto address a variety of linguistic phenomena, including synonymy, paraphrase, term variation, and\ndifferent expressions of similar intents, speciﬁcally in the context of information access [Li and Xu,\n2014]. Following this usage, “relevance matching” is often used to describe the correspondences\nbetween queries and texts that account for a text being relevant to a query (see Section 2.2). Thus,\nrelevance matching is generally understood to comprise both exact match and semantic match\ncomponents. However, there is another major phase in the development of ranking techniques before\nwe get to semantic matching and how neural networks accomplish it.\n1.2.3 The Rise of Learning to Rank\nBM25 and other term weighting schemes are typically characterized as unsupervised, although\nthey contain free parameters (e.g., k1 and b) that can be tuned given training data. The next major\ndevelopment in text ranking, beginning in the late 1980s, is the application of supervised machine-\nlearning techniques to learn ranking models: early examples include Fuhr [1989], Wong et al. [1993],\nand Gey [1994]. This approach, known as “learning to rank”, makes extensive use of hand-crafted,\nmanually-engineered features, based primarily on statistical properties of terms contained in the texts\nas well as intrinsic properties of the texts:\n• Statistical properties of terms include functions of term frequencies, document frequencies,\ndocument lengths, etc., the same components that appear in a scoring function such as BM25.\nIn fact, BM25 scores between the query and various document ﬁelds (as well as scores based\non other exact match scoring functions) are typically included as features in a learning-to-rank\nsetup. Often, features incorporate proximity constraints, such as the frequency of a term pair\nco-occurring within ﬁve positions. Proximity constraints can be localized to a speciﬁc ﬁeld in\nthe text, for example, the co-occurrence of terms in the title of a web page or in anchor texts.\n• Intrinsic properties of texts, ranging from very simple statistics, such as the amount of JavaScript\ncode on a web page or the ratio between HTML tags and content, to more sophisticated measures,\nsuch as the editorial quality or spam score as determined by a classiﬁer. In the web context,\nfeatures of the hyperlink graph, such as the count of inbound and outgoing links and PageRank\nscores, are common as well.\nA real-world search engine can have hundreds of features (or even more). 22 For systems with a\nsufﬁciently larger user base, features based on user behavior—for example, how many times users\nissued a particular query or clicked on a particular link (in different contexts)—are very valuable\nrelevance signals and are thoroughly integrated into learning-to-rank methods.\nThis rise of learning to rank was driven largely by the growth in importance of search engines as\nindispensable tools for navigating the web, as earlier approaches based on human-curated directories\n(e.g., Yahoo!) became quickly untenable with the explosion of available content. Log data capturing\nbehavioral traces of users (e.g., queries and clicks) could be used to improve machine-learned ranking\nmodels. A better search experience led to user growth, which yielded even more log data and\nbehavior-based features to further improve ranking quality—thus closing a self-reinforcing virtuous\ncycle (what Jeff Bezos calls “the ﬂywheel”). Noteworthy innovations that played an important role\nin enabling this growth included the development and reﬁnement of techniques for interpreting\nnoisy user clicks and converting them into training examples that could be fed into machine-learning\nalgorithms [Joachims, 2002, Radlinski and Joachims, 2005].\nAs we lack the space for a detailed treatment of learning to rank, we refer interested readers to two\nsurveys [Liu, 2009, Li, 2011] and focus here on highlights that are most directly relevant for text\nranking with transformers. At a high-level, learning-to-rank methods can be divided into three basic\ntypes, based on the general form of their loss functions:\n22https://googleblog.blogspot.com/2008/03/why-data-matters.html\n14\n• A pointwise approach only considers losses on individual documents, transforming the ranking\nproblem into classiﬁcation or regression.\n• A pairwise approach considers losses on pairs of documents, and thus focuses on preferences,\nthat is, the property wherein Ais more relevant than (or preferred over) B.\n• A listwise approach considers losses on entire lists of documents, for example, directly opti-\nmizing a ranking metric such as normalized discounted cumulative gain (see Section 2.5 for a\ndiscussion of metrics).\nSince this basic classiﬁcation focuses on the form of the loss function, it can also be used to describe\nranking techniques with transformers.\nLearning to rank reached its zenith in the early 2010s, on the eve of the deep learning revolution,\nwith the development of models based on tree ensembles [Burges, 2010].23 At that time, there was an\nemerging consensus that tree-based models, and gradient-boosted decision trees [Ganjisaffar et al.,\n2011] in particular, represented the most effective solution to learning to rank. By that time, tree\nensembles had been deployed to solve a wide range of problems; one notable success story is their\nimportant role in winning the Netﬂix Prize, a high-proﬁle competition that aimed to improve the\nquality of movie recommendations.24\nNote that “learning to rank” should not be understood as being synonymous with “supervised\nmachine-learning approaches to ranking”. Rather, learning to rank refers to techniques that emerged\nduring a speciﬁc period in the history of information retrieval. Transformers for text ranking can be\ncharacterized as a supervised machine-learning approach, but would not generally be regarded as a\nlearning-to-rank method. In particular, there is one key characteristic that distinguishes learning to\nrank from the deep learning approaches that came after. What’s important isnot the speciﬁc supervised\nmachine-learning model: in fact, neural networks have been used since the early 1990s [Wong et al.,\n1993], and RankNet [Burges et al., 2005], one of the most inﬂuential and well-known learning-to-rank\nmodels, adopted a basic feedforward neural architecture. Instead, learning to rank is characterized\nby its use of numerous sparse, usually hand-crafted features. However, to muddle the waters a bit,\nthe phrase “deep learning to rank” has recently emerged in the discourse to describe deep learning\napproaches that also incorporate sparse features [Pasumarthi et al., 2019].\n1.2.4 The Advent of Deep Learning\nFor text ranking, after learning to rank came deep learning, following initial excitement in the com-\nputer vision and then the natural language processing communities. In the context of information\nretrieval, deep learning approaches were exciting for two reasons: First, continuous vector repre-\nsentations freed text retrieval from the bounds of exact term matching (as already mentioned above,\nwe’ll see exactly how below). Second, neural networks promised to obviate the need for laboriously\nhand-crafted features (addressing a major difﬁculty with building systems using learning to rank).\nIn the space of deep learning approaches to text ranking, it makes sense to further distinguish “pre-\nBERT” models from BERT-based models (and more generally, transformer models). After all, the\n“BERT revolution” is the motivation for this survey to begin with. In the Deep Learning Track at\nTREC 2019,25 the ﬁrst large-scale evaluation of retrieval techniques following the introduction of\nBERT, its impact, and more generally, the impact of pretrained neural language models, was clear\nfrom the effectiveness of the submissions [Craswell et al., 2020]. Analysis of the results showed\nthat, taken as a family of techniques, BERT-based models achieved substantially higher effectiveness\nthan pre-BERT models, across implementations by different teams. The organizers of the evaluation\nrecognized this as a meaningful distinction that separated two different “eras” in the development of\ndeep neural approaches to text ranking.\nThis section provides a high-level overview of pre-BERT models. Needless to say, we do not have\nsufﬁcient space to thoroughly detail roughly half a dozen years of model progression, and therefore\nrefer the reader to existing surveys devoted to the topic [Onal et al., 2018, Mitra and Craswell, 2019a,\nXu et al., 2020]. Note that here we focus speciﬁcally on models designed for document ranking and\n23Although a speciﬁc thread of work in the learning-to-rank tradition, called “counterfactual learning to\nrank” [Agarwal et al., 2019] remains active today.\n24https://www.netflixprize.com/\n25See Section 2.6 for an overview of what TREC is.\n15\nE1\nq1\nE2\nq2\nE3\nq3\nF1\nd1\nF2\nd2\nFm\ndm\n…\ns\n(a) a generic representation-based neural ranking model\nE1q1\nE2q2\nE3q3\nF1\nd1\nF2\nd2\nFm\ndm…\ns\n…\n…\n… (b) a generic interaction-based neural ranking model\nFigure 1: Two classes of pre-BERT neural ranking models. Representation-based models (left) learn\nvector representations of queries and documents that are compared using simple metrics such as\ncosine similarity to compute relevance scores. Interaction-based models (right) explicitly model term\ninteractions in a similarity matrix that is further processed to compute relevance scores.\nleave aside another vast body of literature, mostly from the NLP community, on the closely related\nproblem of computing the semantic similarity between two sentences (for example, to detect if two\nsentences are paraphrases of each other). Models for these tasks share many architectural similarities,\nand indeed there has been cross-fertilization between the NLP and IR communities in this regard.\nHowever, there is one major difference: inputs to a model for computing semantic similarity are\nsymmetric, i.e., Rel(s1,s2) = Rel(s2,s1), whereas queries and documents are obviously different\nand cannot be swapped as model inputs. The practical effect is that architectures for computing\nsemantic similarity are usually symmetric, but may not be for modeling query–document relevance.\nInterestingly, recent developments in learned dense representations for ranking are erasing the\ndistinction between these two threads of work, as we will see in Section 5.\nPre-BERT neural ranking models are generally classiﬁed into two classes: representation-based\nmodels and interaction-based models. Their high-level architectures are illustrated in Figure 1.\nRepresentation-based models (left) focus on independently learning dense vector representations of\nqueries and documents that can be compared to compute relevance via a simple metric such as cosine\nsimilarity or inner products. Interaction-based models (right) compare the representations of terms in\nthe query with terms in a document to produce a similarity matrix that captures term interactions.\nThis matrix then undergoes further analysis to arrive at a relevance score. In both cases, models can\nincorporate many different neural components (e.g., convolutional neural networks and recurrent\nneural networks) to extract relevance signals.\nBoth representation-based and interaction-based models are usually trained end-to-end with relevance\njudgments (see Section 2.4), using only the embeddings of query and document terms as input.\nNotably, additional features (hand-crafted or otherwise) are typically not used, which is a major\ndeparture from learning to rank. Below, we provide more details, with illustrative examples:\nRepresentation-based models. This class of models (Figure 1, left) learns vector representations of\nqueries and documents that can be compared at ranking time to compute query–document relevance\nscores. Since the query and document “arms” of the network are independent, this approach allows\ndocument representations to be computed ofﬂine. One of the earliest neural ranking models in the\ndeep learning era, the Deep Structure Semantic Model (DSSM) [Huang et al., 2013] constructs\ncharacter n-grams from an input (i.e., query or document) and passes the results to a series of\nfully-connected layers to produce a vector representation. At retrieval time, query and document\nrepresentations can then be compared with cosine similarity. Shen et al. [2014] improved upon DSSM\nby using CNNs to capture context. Rather than learning text representations as part of the model, the\nDual Embedding Space Model (DESM) [Mitra et al., 2016, Nalisnick et al., 2016] represents texts\n16\nusing pre-trained word2vec embeddings [Le and Mikolov, 2014] and computes relevance scores by\naggregating cosine similarities across all query–document term pairs. Language models based on\nword embeddings [Ganguly et al., 2015] can also be categorized as representation-based models.\nInterestingly, we are witnessing a resurgence of interest in representation-based approaches, albeit\nusing transformer architectures. The entirety of Section 5 is devoted to this topic.\nInteraction-based models. This class of models (Figure 1, right) explicitly captures “interactions”\nbetween terms from the query and terms from the document. These interactions are typically\noperationalized using a similarity matrix with rows corresponding to query terms and columns\ncorresponding to document terms. Each entry mi,j in the matrix is usually populated with the cosine\nsimilarity between the embedding of the i-th query term and the embedding of the j-th document\nterm.26 At a high level, these models operate in two steps: feature extraction and relevance scoring.\n• In the feature extraction step, the model extracts relevance signals from the similarity matrix. By\nexploiting continuous vector representations of terms, these models can potentially overcome\nthe vocabulary mismatch problem. Unigram models like DRMM [Guo et al., 2016] and\nKNRM [Xiong et al., 2017] aggregate the similarities between each query term and each\ndocument term, which can be viewed as histograms. DRMM creates explicit histograms,\nwhile KNRM uses Gaussian kernels to create differentiable “soft histograms” that allow the\nembeddings to be learned during training. Position-aware models like MatchPyramid [Pang\net al., 2016], PACRR [Hui et al., 2017], Co-PACRR [Hui et al., 2018], and ConvKNRM [Dai\net al., 2018] use additional architectural components to identify matches between sequences of\nquery and document terms.27\n• In the relevance scoring step, features extracted from above are combined and processed to\nproduce a query–document relevance score. This step often consists of applying pooling opera-\ntions, concatenating extracted features together, and then passing the resulting representation to\na feedforward network that computes the relevance score.\nWhile interaction-based models generally follow this high-level approach, many variants have been\nproposed that incorporate additional components. For example, POSIT-DRMM [McDonald et al.,\n2018] uses an LSTM to contextualize static embeddings before comparing them. EDRM [Liu et al.,\n2018b] extends ConvKNRM by incorporating entity embeddings. HiNT [Fan et al., 2018b] splits\nthe document into passages, creates a similarity matrix for each, and then combines passage-level\nsignals to predict a single document-level relevance score. The NPRF [Li et al., 2018] framework\nincorporates feedback documents by using a neural ranking method like KNRM to predict their\nsimilarity to a target document being ranked.\nIn general, studies have shown pre-BERT interaction-based models to be more effective but slower\nthan pre-BERT representation-based models. The latter reduces text ranking to simple similarity\ncomparisons between query vectors and precomputed document vectors, which can be performed\nquickly on large corpora using nearest neighbor search techniques (see Section 5.2). In contrast,\ninteraction-based models are typically deployed as rerankers over a candidate set of results retrieved\nby keyword search. Interaction-based models also preserve the ability to explicitly capture exact\nmatch signals, which remain important in relevance matching (see discussion in Section 3.2.3).\nHybrid models. Finally, representation-based and interaction-based approaches are not mutually\nexclusive. A well-known hybrid is the DUET model [Mitra et al., 2017, Mitra and Craswell,\n2019b], which augments a representation-learning component with an interaction-based component\nresponsible for identifying exact term matches.\n26Although other distance metrics can be used as well, for example, see He and Lin [2016], Pang et al. [2016].\n27One might argue that, with this class of models, we have simply replaced feature engineering (from learning\nto rank) with network engineering, since in some cases there are pretty clear analogies between features in\nlearning to rank and the relevance signals that different neural architectural components are designed to identify.\nWhile this is not an unfair criticism, it can be argued that different network components more compactly\ncapture the intuitions of what makes a document relevant to a query. For example, bigram relations can be\ncompactly expressed as convolutions, whereas in learning to rank distinct bigram features would need to be\nenumerated explicitly.\n17\nMS MARCO Passage\nDevelopment Test\nMethod MRR@10 MRR@10\nBM25 (Microsoft Baseline) 0.167 0.165\nIRNet (Deep CNN/IR Hybrid Network) January 2nd, 2019 0.278 0.281\nBERT [Nogueira and Cho, 2019] January 7th, 2019 0.365 0.359\nTable 1: The state of the leaderboard for the MS MARCO passage ranking task in January 2019,\nshowing the introduction of BERT and the best model (IRNet) just prior to it. This large gain in\neffectiveness kicked off the “BERT revolution” in text ranking.\nThere has undeniably been signiﬁcant research activity throughout the 2010s exploring a wide\nrange of neural architectures for document ranking, but how far has the ﬁeld concretely advanced,\nparticularly since approaches based on deep learning require large amounts of training data? Lin\n[2018] posed the provocative question, asking if neural ranking models were actually better than\n“traditional” keyword-matching techniques in the absence of vast quantities of training data available\nfrom behavior logs (i.e., queries and clickthroughs). This is an important question because academic\nresearchers have faced a perennial challenge in obtaining access to such data, which are available to\nonly researchers in industry (with rare exceptions). To what extent do neural ranking models “work”\non the limited amounts of training data that are publicly available?\nYang et al. [2019b] answered this question by comparing several prominent interaction-based and\nrepresentation-based neural ranking models to a well-engineered implementation of bag-of-words\nsearch with well-tuned query expansion on the dataset from the TREC 2004 Robust Track [V oorhees,\n2004]. Under this limited data condition, most of the neural ranking methods were unable to beat\nthe keyword search baseline. Yates et al. [2020] replicated the same ﬁnding for an expanded set of\nneural ranking methods with completely different implementations, thus increasing the veracity of the\noriginal ﬁndings. While many of the papers cited above report signiﬁcant improvements when trained\non large, proprietary datasets (many of which include behavioral signals), the results are difﬁcult to\nvalidate and the beneﬁts of the proposed methods are not broadly accessible to the community.\nWith BERT, though, everything changed, nearly overnight.\n1.2.5 The Arrival of BERT\nBERT [Devlin et al., 2019] arrived on the scene in October 2018. The ﬁrst application of BERT to\ntext ranking was reported by Nogueira and Cho [2019] in January 2019 on the MS MARCO passage\nranking test collection [Bajaj et al., 2018], where the task is to rank passages (paragraph-length\nextracts) from web pages with respect to users’ natural language queries, taken from Bing query logs\n(see more details in Section 2.7). The relevant portion of the leaderboard at the time is presented in\nTable 1, showing Microsoft’s BM25 baseline and the effectiveness of IRNet, the best system right\nbefore the introduction of BERT (see Section 2.5 for the exact deﬁnition of the metric). Within less\nthan a week, effectiveness shot up by around eight points28 absolute, which corresponds to a ∼30%\nrelative gain.\nSuch a big jump in effectiveness that can be directly attributed to an individual model is rarely\nseen in either academia or industry, which led to immediate excitement in the community. The\nsimplicity of the model led to rapid widespread replication of the results. Within a few weeks, at least\ntwo other teams had conﬁrmed the effectiveness of BERT for passage ranking, and exploration of\nmodel variants built on the original insights of Nogueira and Cho [2019] had already begun.29 The\nskepticism expressed by Lin [2018] was retracted in short order [Lin, 2019], as many researchers\nquickly demonstrated that with pretrained transformer models, large amounts of relevance judgments\nwere not necessary to build effective models for text ranking. The availability of the MS MARCO\npassage ranking test collection further mitigated data availability issues. The combination of these\nfactors meant that, nearly overnight, exploration at the forefront of neural models for text ranking\nwas within reach of academic research groups, and was no longer limited to researchers in industry\nwho had the luxury of access to query logs.\n28A change of 0.01 is often referred to as a “point”; see Section 2.5.\n29https://twitter.com/MSMarcoAI/status/1095035433375821824\n18\nNogueira and Cho [2019] kicked off the “BERT revolution” for text ranking, and the research\ncommunity quickly set forth to build on their results—addressing limitations and expanding the work\nin various ways. Looking at the leaderboard today, the dominance of BERT remains evident, just by\nlooking at the names of the submissions.\nThe rest, as they say, is history. The remainder of this survey is about that history.\n1.3 Roadmap, Assumptions, and Omissions\nThe target audience for this survey is a ﬁrst-year graduate student or perhaps an advanced under-\ngraduate. As this is not intended to be a general introduction to natural language processing or\ninformation retrieval, we assume that the reader has basic background in both. For example, we\ndiscuss sequence-to-sequence formulations of text processing problems (to take an example from\nNLP) and query evaluation with inverted indexes (to take an example from IR) assuming that the\nreader has already encountered these concepts before.\nFurthermore, we expect that the reader is already familiar with neural networks and deep learning,\nparticularly pre-BERT models (for example, CNNs and RNNs). Although we do provide an overview\nof BERT and transformer architectures, that material is not designed to be tutorial in nature, but\nmerely intended to provide the setup of how to apply transformers to text ranking problems.\nThis survey is organized as follows:\n• Setting the Stage (Section 2). We begin with a more precise characterization of the problem\nwe are tackling in the speciﬁc context of information retrieval. This requires an overview of\nmodern evaluation methodology, involving discussions about information needs, notions of\nrelevance, ranking metrics, and the construction of test collections.\n• Multi-Stage Architectures for Reranking (Section 3). The most straightforward application\nof transformers to text ranking is as reranking models to improve the output quality of candidates\ngenerated by keyword search. This section details various ways this basic idea can be realized\nin the context of multi-stage ranking architectures.\n• Reﬁning Query and Document Representations (Section 4). One fundamental challenge in\nranking is overcoming the vocabulary mismatch problem, where users’ queries and documents\nuse different words to describe the same concepts. This section describes expansion techniques\nfor query and document representations that bring them into closer “alignment”.\n• Learned Dense Representations for Ranking (Section 5). Text ranking can be cast as a\nrepresentation learning problem in terms of efﬁcient comparisons between dense vectors that\ncapture the “meaning” of documents and queries. This section covers different architectures as\nwell as training methods for accomplishing this.\n• Future Directions and Conclusions (Section 6). We have only begun to scratch the surface\nin applications of transformers to text ranking. This survey concludes with discussions of\ninteresting open problems and our attempts to prognosticate where the ﬁeld is heading.\nGiven limits in both time and space, it is impossible to achieve comprehensive coverage, even in a\nnarrowly circumscribed topic, both due to the speed at which research is progressing and the wealth\nof connections to related topics.\nThis survey focuses on what might be characterized as “core” text ranking. Noteworthy intentional\nomissions include other aspects of information access such as question answering, summarization,\nand recommendation, despite their close relationship to the material we cover. Adequate treatments\nof each of these topics would occupy an equally lengthy survey! Our focus on “core” text ranking\nmeans that we do not elaborate on how ranked results might be used to directly supply answers (as\nin typical formulations of question answering), how multiple results might be synthesized (as in\nsummarization), and how systems might suggest related texts based on more than just content (as in\nrecommendations).\n19\n2 Setting the Stage\nThis section begins by more formally characterizing the text ranking problem, explicitly enumerating\nour assumptions about characteristics of the input and output, and more precisely circumscribing\nthe scope of this survey. In this exposition, we will adopt the perspective of information access,\nfocusing speciﬁcally on the problem of ranking texts with respect to their relevance to a particular\nquery—what we have characterized as the “core” text ranking problem (and what information retrieval\nresearchers would refer to as ad hoc retrieval). However, most of our deﬁnitions and discussions carry\nstraightforwardly to other ranking tasks, such as the diverse applications discussed in Section 1.1.\nFrom the evaluation perspective, this survey focuses on what is commonly known as the Cranﬁeld\nparadigm, an approach to systems-oriented evaluation of information retrieval (IR) systems based\non a series of experiments by Cyril Cleverdon and his colleagues in the 1960s. For the interested\nreader, Harman [2011] provides an overview of the early history of IR evaluation. Also known as\n“batch evaluations”, the Cranﬁeld paradigm has come to dominate the IR research landscape over\nthe last half a century. Nevertheless, there are other evaluation paradigms worth noting: interactive\nevaluations place humans “in the loop” and are necessary to understand the important role of user\nbehavior in information seeking [Kelly, 2009]. Online services with substantial numbers of users can\nengage in experimentation using an approach known as A/B testing [Kohavi et al., 2007]. Despite\nour focus on the Cranﬁeld paradigm, primarily due to its accessibility to the intended audience\nof our survey, evaluations from multiple perspectives are necessary to accurately characterize the\neffectiveness of a particular technique.\n2.1 Texts\nThe formulation of text ranking assumes the existence of a collection of texts or a corpus C= {di}\ncomprised of mostly unstructured natural language text. We say “mostly unstructured” because texts\nare, of course, typically broken into paragraphs, with section headings and other discourse markers—\nthese can be considered a form of “structure”. This stands in contrast to, for example, tabular data or\nsemi-structured logs (e.g., in JSON), which are comprised of text as well. We speciﬁcally consider\nsuch types of textual data out of scope in this survey.\nOur collection Ccan be arbitrarily large (but ﬁnite)—in the case of the web, countless billions\nof pages. This means that issues related to computational efﬁciency, for example the latency and\nthroughput of text ranking, are important considerations, especially in production systems. We mostly\nset aside issues related to multilinguality and focus on English, although there are straightforward\nextensions to some of the material discussed in this survey to other languages that serve as reasonable\nbaselines and starting points for multilingual IR.30\nIt is further assumed that the corpus is provided “ahead of time” to the system, prior to the arrival of\nqueries, and that a “reasonable” amount of ofﬂine processing may be conducted on the corpus. This\nconstraint implies that the corpus is mostly static, in the sense that additions, deletions, or modiﬁ-\ncations to texts happen in batch or at a pace that is slow compared to the amount of preprocessing\nrequired by the system for proper operation.31 This assumption becomes important in the context of\ndocument expansion techniques we discuss in Section 4.\nTexts can vary in length, ranging from sentences (e.g., searching for related questions in a community\nquestion answering application) to entire books, although the organization of the source texts, how\nthey are processed, and the ﬁnal granularity of ranking can be independent. To illustrate: in a\ncollection of full-text scientiﬁc articles, we might choose to only search the article titles and abstracts.\nThat is, the ranking model only considers selected portions of the articles; experiments along these\nlines date back to at least the 1960s [Salton and Lesk, 1968]. An alternative might be to segment\nfull-text articles into paragraphs and consider each paragraph as the unit of retrieval, i.e., the system\n30With respect to multilinguality, IR researchers have explored two distinct problem formulations: mono-lingual\nretrieval in languages other than English (where one major challenge is mitigating the paucity of training data),\nand cross-lingual retrieval, where queries are in a different language than the corpus (for example, searching\nTelugu documents with English queries). A worthy treatment of multilinguality in IR would occupy a separate\nsurvey, and thus we consider these issues mostly out of scope. See additional discussions in Section 6.2.\n31For example, daily updates to the corpus would likely meet this characterization, but not streams of tweets that\nrequire real-time processing. See, for example, Busch et al. [2012] for an overview techniques for real-time\nindexing and search.\n20\nreturns a list of paragraphs as results. Yet another alternative might be to rank articles by aggregating\nevidence across paragraphs—that is, the system treats paragraphs as the atomic unit of analysis,\nbut for the goal of producing a ranking of the articles those paragraphs are drawn from. Zhang\net al. [2020a] provided a recent example of these different schemes in the context of the biomedical\nliterature. Approaches to segmenting documents into passages for ranking purposes and integrating\nevidence from multiple document granularities—commonly referred to as passage retrieval—was\nan active area of research in the 1990s [Salton et al., 1993, Hearst and Plaunt, 1993, Callan, 1994,\nWilkinson, 1994, Kaszkiel and Zobel, 1997, Clarke et al., 2000]. Note that for certain types of text,\nthe “right level” of granularity may not be immediately obvious: For example, when searching email,\nshould the system results be comprised of individual emails or email threads? What about when\nsearching (potentially long) podcasts based on their textual transcripts? What about chat logs or\ntranscriptions of phone calls?\nIn this survey, we have little to say about the internal structure of texts other than applying the most\ngeneric treatments (e.g., segmenting by paragraphs or overlapping windows). Speciﬁc techniques are\noften domain-speciﬁc (e.g., reconstructing and segmenting email threads) and thus orthogonal to our\nfocus. However, the issue of text length is an important consideration in applications of transformer\narchitectures to text ranking (see Section 3.3). There are two related issues: transformers are typically\npretrained with input sequences up to a certain maximum length, making it difﬁcult to meaningfully\nencode longer sequences, and feeding long texts into transformers results in excessive memory usage\nand inference latency. These limitations have necessitated the development of techniques to handle\nranking long texts. In fact, many of these techniques draw from work in passage retrieval referenced\nabove, dating back nearly three decades (see Section 3.3.2).\n2.2 Information Needs\nHaving sufﬁciently characterized the corpus, we now turn our attention to queries. In the web context,\nshort keyword queries that a user types into a search box are merely the external manifestations of\nan information need, which is the motivation that compelled the user to seek information in the ﬁrst\nplace. Belkin [1980] calls this an “anomalous state of knowledge” (ASK), where searchers perceive\ngaps in their cognitive states with respect to some task or problem; see also Belkin et al. [1982a,b].\nStrictly speaking, queries are not synonymous with information needs [Taylor, 1962]. The same\ninformation need might give rise to different manifestations with different systems: for example, a\nfew keywords are typed into the search box of a web search engine, but a ﬂuent, well-formed natural\nlanguage question is spoken to a voice assistant.32\nIn this survey, we are not concerned with the cognitive processes underlying information seeking, and\nfocus on the workings of text ranking models only after they have received a tangible signal to process.\nThus, we somewhat abuse the terminology and refer to the query as “the thing” that the ranking\nis computed with respect to (i.e., the input to the ranking model), and use it as a metonym for the\nunderlying information need. In other words, although the query is not the same as the information\nneed, we only care about what is fed to the ranking model (for the purposes of this survey), in which\ncase this distinction is not particularly important.33 We only consider queries that are expressed in\ntext, although in principle queries can be presented in different modalities, for example, speech34 or\nimages, or even “query by humming” [Ghias et al., 1995].\nNevertheless, to enable automated processing, information needs must be encoded in some represen-\ntation. In the Text Retrieval Conferences (TRECs), an inﬂuential series of community evaluations in\ninformation retrieval (see Section 2.6), information needs are operationalized as “topics”.35 Figure 2\nprovides an example from the TREC 2004 Robust Track.\nA TREC topic for ad hoc retrieval is comprised of three ﬁelds:\n32In the latter case, researchers might refer to these as voice queries, but it is clear that spoken utterances are\nvery different from typed queries, even if the underlying information needs are the same.\n33Note, however, that this distinction may be important from the perspective of relevance judgments; see more\ndiscussion in Section 2.3.\n34Spoken queries can be transcribed into text with the aid of automatic speech recognition (ASR) systems.\n35Even within TREC, topic formats have evolved over time, but the structure we describe here has been stable\nsince TREC-7 in 1998 [V oorhees and Harman, 1998].\n21\n<top>\n<num> Number: 336\n<title> Black Bear Attacks\n<desc> Description:\nA relevant document would discuss the frequency of vicious black bear\nattacks worldwide and the possible causes for this savage behavior.\n<narr> Narrative:\nIt has been reported that food or cosmetics sometimes attract hungry black\nbears, causing them to viciously attack humans. Relevant documents would\ninclude the aforementioned causes as well as speculation preferably from the\nscientific community as to other possible causes of vicious attacks by black\nbears. A relevant document would also detail steps taken or new methods\ndevised by wildlife officials to control and/or modify the savageness of the\nblack bear.\n</top>\nFigure 2: An example ad hoc retrieval “topic” (i.e., representation of an information need) from the\nTREC 2004 Robust Track, comprised of “title”, “description”, and “narrative” ﬁelds.\n• the “title”, which consists of a few keywords that describe the information need, close to a query\nthat a user would type into a search engine;\n• the “description”, typically a well-formed natural language sentence that describes the desired\ninformation; and,\n• the “narrative”, a paragraph of prose that details the characteristics of the desired information,\nparticularly nuances that are not articulated in the title or description.\nIn most information retrieval evaluations, the title serves as the query that is fed to the system to\ngenerate a ranked list of results (that are then evaluated). Some papers explicitly state “title queries”\nor something to that effect, but many papers omit this detail, in which case it is usually safe to assume\nthat the topic titles were used as queries.\nAlthough in actuality the narrative is a more faithful description of the information need, i.e., what\nthe user really wants, in most cases feeding the narrative into a ranking model leads to poor results\nbecause the narrative often contains terms that are not important to the topic. These extraneous\nterms serve as distractors to a ranking model based on exact term matches, since such a model will\ntry to match all query terms.36 Although results vary by domain and the speciﬁc set of topics used\nfor evaluation, one common ﬁnding is that either the title or the title and description concatenated\ntogether yields the best results with bag-of-words queries; see, for example, Walker et al. [1997].\nHowever, the differences in effectiveness between the two conditions are usually small. Nevertheless,\nthe key takeaway here is that the expression of the information need that is fed to a ranking model\noften has a substantive effect on retrieval effectiveness. We will see that this is particularly the case\nfor BERT (see Section 3.3.2).\nHaving more precisely described the inputs, we can now formally deﬁne the text ranking problem:\nGiven an information need expressed as a query q, the text ranking task is to return\na ranked list of ktexts {d1,d2 ...d k}from an arbitrarily large but ﬁnite collection\nof texts C= {di}that maximizes a metric of interest, for example, nDCG, AP, etc.\nDescriptions of a few common metrics are presented in Section 2.5, but at a high level they all aim to\nquantify the “goodness” of the results with respect to the information need. The ranking task is also\ncalled top-kretrieval (or ranking), where kis the length of the ranked list (also known as the ranking\nor retrieval depth).\nThe “thing” that performs the ranking is referred to using different terms in the literature: {rank-\ning, retrieval, scoring} ×{function, model, method, technique ... }, or even just “the system”\n36Prior to the advent of neural networks, researchers have attempted to extract “key terms” or “key phrases” from\nso-called “verbose” queries, e.g., Bendersky and Croft [2008], though these usually refer to sentence-length\ndescriptions of information needs as opposed to paragraph-length narratives.\n22\nwhen discussed in an end-to-end context. In this survey, we tend to use the term “ranking model”,\nbut consider all these variations roughly interchangeable. Typically, the ranked texts are associ-\nated with scores, and thus the output of a ranking model can be more explicitly characterized as\n{(d1,s1),(d2,s2) ... (dk,sk)}with the constraint that s1 >s2 >...s k.37\nA distinction worth introducing here: ranking usually refers to the task of constructing a ranked\nlist of texts selected from the corpus C. As we will see in Section 3.2, it is impractical to apply\ntransformer-based models to directly rank all texts in a (potentially large) corpus to produce the top k.\nInstead, models are often used torerank a candidate list of documents, typically produced by keyword\nsearch. More formally, in reranking, the model takes as input a list of texts R = {d1,d2 ...d k}\nand produces another list of texts R′= {d′\n1,d′\n2 ...d ′\nk}, where R′is a permutation of R. Ranking\nbecomes conceptually equivalent to reranking if we feed a reranker the entire corpus, but in practice\nthey involve very different techniques: Section 3 and Section 4 primarily focus on reranking with\ntransformer-based models, while Section 5 covers nearest neighbor search techniques for directly\nranking dense representations generated by transformer-based models. Nevertheless, in this survey\nwe adopt the expository convention of referring to both as ranking unless the distinction is important.\nSimilarly, we refer to ranking models even though a particular model may, in fact, be performing\nreranking. We believe this way of writing improves clarity by eliminating a distinction that is usually\nclear from context.\nFinally, as information retrieval has a rich history dating back well over half a century, the parlance\ncan be confusing and inconsistent, especially in cases where concepts overlap with neighboring\nsub-disciplines of computer science such as natural language processing or data mining. An example\nhere is the usage of “retrieval” and “ranking” in an interchangeable fashion. These issues are for the\nmost part not critical to the material presented in this survey, but we devote Section 2.9 to untangling\nterminological nuances.\n2.3 Relevance\nThere is one ﬁnal concept necessary to connect the query, as an expression of the information need,\nto the “goodness” of the ranked texts according to some metric: Ultimately, the foundation of all\nranking metrics rests on the notion of relevance,38 which is a relation between a text and a particular\ninformation need. A text is said to be relevant if it addresses the information need, otherwise it is not\nrelevant. However, this binary treatment of relevance is a simpliﬁcation, as it is more accurate, for\nexample, to characterize relevance using ordinal scales in multiple dimensions [Spink and Greisdorf,\n2001]. Discussions and debates about the nature of relevance are almost as old as the quest for\nbuilding automated search systems itself (see Section 1.2), since relevance ﬁgures into discussions of\nwhat such systems should return and how to evaluate the quality of their outputs. Countless pages have\nbeen written about relevance, from different perspectives ranging from operational considerations\n(i.e., for designing search systems) to purely cognitive and psychological studies (i.e., how humans\nassimilate and use information acquired from search systems). We refer the reader to Saracevic\n[2017] for a survey that compiles accumulated wisdom on the topic of relevance spanning many\ndecades [Saracevic, 1975].\nWhile seemingly intuitive, relevance is surprisingly difﬁcult to precisely deﬁne. Furthermore, the\ninformation science literature discusses many types of relevance; for the purposes of measuring\nsearch quality, information retrieval researchers are generally concerned withtopical relevance, or\nthe “aboutness” of the document—does the topic or subject of the text match the information need?\nThere are other possible considerations as well: for example, cognitive relevance, e.g., whether the\ntext is understandable by the user, or situational relevance, e.g., whether the text is useful for solving\nthe problem at hand.\nTo illustrate these nuances: A text might be topically relevant, but is written for experts whereas the\nsearcher desires an accessible introduction; thus, it may not be relevant from the cognitive perspective.\nA text might be topically relevant, but the user is searching for information to aid in making a speciﬁc\ndecision—for example, whether to send a child to public or private school—and while the text\nprovides helpful background information, it offers no actionable advice. In this case, we might say\n37A minor complication is that ranking models might produce score ties, which need to be resolved at evaluation\ntime since many metrics assume monotonically increasing ranks; see Section 2.5 for more details.\n38“Relevancy” is sometimes used, often by industry practitioners. However, information retrieval researchers\nnearly always use the term “relevance” in the academic literature.\n23\nthat the document is topically relevant but notuseful, i.e., from the perspective of situational relevance.\nAlthough it has been well understood for decades that relevance is a complex phenomenon, there\nremains a wide gap between studies that examine these nuances and the design of search systems and\nranking models, as it is not clear how such insights can be operationalized.\nMore to the task at hand: in terms of developing ranking models, the most important lesson from\nmany decades of information retrieval research is that relevance is in the eye of the beholder, that it is\na user-speciﬁc judgment about a text that involves complex cognitive processes. To put more simply:\nfor my information need, I am the ultimate arbiter of what’s relevant or not; nobody else’s opinion\ncounts or matters. Thus, relevance judgments represent a speciﬁc person’s assessment of what’s\nrelevant or not—this person is called the assessor (or sometimes the annotator). In short, all relevance\njudgments are opinions, and thus are subjective. Relevance is not a “truth” (in a platonic sense) or an\n“inherent property” of a piece of text (with respect to an information need) that the assessor attempts\nto “unlock”. Put differently, unlike facts and reality, everyone can have different notions of relevance,\nand they are all “correct”.\nIn this way, relevance differs quite a bit from human annotations in NLP applications, where\n(arguably), there is, for example, the true part-of-speech tag of a word or dependency relation\nbetween two words. Trained annotators can agree on a word’s part of speech nearly all the time, and\ndisagreements are interpreted as the result of a failure to properly deﬁne the subject of annotation\n(i.e., what a part of speech is). It would be odd to speak of an annotator’sopinion of a word’s part of\nspeech, but that is exactly what relevance is: an assessor’s opinion concerning the relation between a\ntext and an information need.\nWith this understanding, it shouldn’t be a surprise then that assessor agreement on relevance judgments\nis quite low: 60% overlap is a commonly cited ﬁgure [V oorhees, 2000], but the range of values\nreported in the literature vary quite a bit (from around 30% to greater than 70%), depending on\nthe study design, the information needs, and the exact agreement metric; see [Harman, 2011] for a\ndiscussion of this issue across studies spanning many decades. The important takeaway message is\nthat assessor agreement is far lower than values an NLP researcher would be comfortable with for a\nhuman annotation task (κ> 0.9 is sometimes used as a reference point for what “good” agreement\nmeans). The reaction from an NLP researcher would be, “we need better annotation guidelines”.\nThis, however, is fundamentally not possible, as we explain below.\nWhy is agreement so low among relevance judgments provided by different assessors? First, it is\nimportant to understand the setup of such experiments. Ultimately, all information needs arise from\na single individual. In TREC, a human assessor develops the topic, which represents a best effort\narticulation of the information need relatively early in the information seeking process. Topics are\nformulated after some initial exploratory searches, but before in-depth perusal of texts from the\ncorpus. The topics are then released to teams participating in the evaluation, and the same individual\nwho created the topic then assesses system outputs (see Section 2.6 for more details).\nThus, if we ask another assessor to produce an independent set of relevance judgments (for example,\nin the same way we might ask multiple annotators to assign part-of-speech tags to a corpus in an NLP\nsetting in order to compute inter-annotator agreement), such a task is based on a particular external\nrepresentation of that information need (e.g., a TREC topic, as in Figure 2). 39 Thus, the second\nindividual is judging relevance with respect to an interpretation of that representation. Remember,\nthe actual characteristics of the desired information is a cognitive state that lies in the user’s head,\ni.e., Belkin’s anomalous state of knowledge. Furthermore, in some cases, the topic statements aren’t\neven faithful representations of the true information need to begin with: details may be missing and\ninconsistencies may be present in the representations themselves. The paradox of relevance is that if\na user were able to fully and exhaustively articulate the parameters of relevance, there may likely be\nno need to search in the ﬁrst place—for the user would already know the information desired.\nWe can illustrate with a concrete example based on the TREC topic shown in Figure 2 about “black\nbears attacks”: consider, would documents about brown (grizzly) bears be relevant?40 It could be\nthe case that the user is actually interested in attacks by bears (in general), and just happens to have\nreferenced black bears as a starting point. It could also be the case that the user speciﬁcally wants\n39As far as we know, assessors cannot Vulcan mind meld with each other.\n40In TREC “lore”, this was a serious debate that was had “back in the day”. The other memorable debate along\nsimilar lines involved Trump and the Taj Mahal in the context of question answering.\n24\nonly attacks by black bears, perhaps to contrast with the behavior of brown bears. Or, it could be\nthe case that the user isn’t familiar with the distinction, started off by referencing black bears, and\nonly during the process of reading initial results is a decision made about different types of bears.\nAll three scenarios are plausible based on the topic statement, and it can be seen now how different\ninterpretations might give rise to very different judgments.\nBeyond these fundamental issues, which center around representational deﬁciencies of cognitive\nstates, there are issues related to human performance. Humans forget how they interpreted a previously\nencountered text and may judge two similar texts inconsistently. There may be learning effects that\ncarry across multiple texts: for example, one text uses terminology that the assessor does not recognize\nas being relevant until a second text is encountered (later) that explains the terminology. In this case,\nthe presentation order of the texts matters, and the assessor may or may not reexamine previous texts\nto adjust the judgments. There are also more mundane factors: Assessors may get tired and misread\nthe material presented. Sometimes, they just make mistakes (e.g., clicked on the wrong button in an\nassessment interface). All of these factors further contribute to low agreement.\nOne obvious question that arises from this discussion is: With such low inter-annotator agreement,\nhow are information retrieval researchers able to reliably evaluate systems at all? Given the critical\nrole that evaluation methodology plays in any empirical discipline, it should come as no surprise that\nresearchers have examined this issue in detail. In studies where we have multiple sets of relevance\njudgments (i.e., from different assessors), it is easy to verify that the score of a system does indeed\nvary (often, quite a bit) depending on which set of relevance judgments the system is evaluated with\n(i.e., whose opinion of relevance). However, the ranking of a group of systems is usually stable with\nrespect to assessor variations [V oorhees, 2000].41 How stable? Exact values depend on the setting,\nbut measured in terms of Kendall’sτ, a standard rank correlation metric, values consistently above\n0.9 are observed. That is, if system Ais better than system B, then the score of system Awill likely\nbe higher than the score of system B, regardless of the relevance judgments used for evaluation.42\nThis is a widely replicated and robust ﬁnding, and these conclusions have been shown to hold across\nmany different retrieval settings [Sormunen, 2002, Trotman and Jenkinson, 2007, Bailey et al., 2008,\nWang et al., 2015].\nThis means that while the absolute value of an evaluation metric must be interpreted cautiously,\ncomparisons between systems are generally reliable given a well-constructed test collection; see\nmore discussions in Section 2.6. The inability to quantify system effectiveness in absolute terms is\nnot a limitation outside of the ability to make marketing claims.43 As most research is focused on\nthe effectiveness of a particular proposed innovation, the desired comparison is typically between a\nranking model with and without that innovation, for which a reusable test collection can serve as an\nevaluation instrument.\n2.4 Relevance Judgments\nFormally, relevance judgments, also called qrels, comprise a set of(q,d,r ) triples, where the relevance\njudgment ris a (human-provided) annotation on (q,d) pairs. Relevance judgments are also called\nrelevance labels or human judgments. Practically speaking, they are contained in text ﬁles that can be\ndownloaded as part of a test collection and can be treated like “ground truth”.44 In Section 2.6, we\ndescribe a common way in which test collections are created via community evaluations, but for now\nit sufﬁces to view them as the product of (potentially large-scale) human annotation efforts.\nIn the simplest case, r is a binary variable—either document dis relevant to query q, or it is not\nrelevant. A three-way scale of not relevant, relevant, and highly-relevant is one common alternative,\n41Note that while studies of assessor agreement predated this paper by several decades at least, for example, Lesk\nand Salton [1968], the work of V oorhees is generally acknowledged as establishing these ﬁndings in the context\nof modern test collections.\n42Conﬂated with this high-level summary is the effect size, i.e., the “true” difference between the effectiveness\nof systems, or an inferred estimate thereof. With small effect sizes, system Avs. system Bcomparisons are\nless likely to be consistent across different assessors. Not surprisingly, V oorhees [2000] studied this as well;\nsee Wang et al. [2015] for a more recent examination in a different context.\n43Occasionally on the web, one stumbles upon a statement like “our search engine achieves 90% accuracy”\nwithout references to the corpus, information needs, or users. Such marketing slogans are utterly meaningless.\n44However, IR researchers tend to avoid the term “ground truth” because relevance judgments are opinions, as\nwe discussed in Section 2.2.\n25\nand in web search, a ﬁve-point scale is often used—perfect, excellent, good, fair, and bad—which even\nhas an acronym: PEGFB.45 Non-binary relevance judgments are called graded relevance judgments:\n“graded” is used in the sense of “grade”, deﬁned as “a position in a scale of ranks or qualities” (from\nthe Merriam–Webster Dictionary).\nRelevance judgments serve two purposes: they can be used to train ranking models in a supervised\nsetting and they can also be used to evaluate ranking models. To a modern researcher or practitioner\nof applied machine learning, this distinction might seem odd, since these are just the roles of the\ntraining, development, and test split of a dataset, but historically, information retrieval test collections\nhave not been large enough to meaningfully train ranking models (with the exception of simple\nparameter tuning). However, with the release of the MS MARCO datasets, which we introduced in\nSection 1.2.5 and will further discuss in Section 2.7, the community has gained public access to a\nsufﬁciently large collection of relevance judgments for training models in a supervised setting. Thus,\nthroughout this survey, we use the terms relevance judgments, test collections, and training data\nroughly interchangeably.\nResearchers describe datasets for supervised learning of ranking models in different ways, but they are\nequivalent. It makes sense to explicitly discuss some of these variations to reduce possible confusion:\nOur view of relevance judgments as (q,d,r ) triples, where ris a relevance label on query–document\npairs, is perhaps the most general formulation. However, documents may in fact refer to paragraphs,\npassages, or some other unit of retrieval (see discussion in Section 2.9). Most often, drefers to\nthe unique id of a text from the corpus, but in some cases (for example, some question answering\ndatasets), the “document” may be just a span of text, without any direct association to the contents of\na corpus.\nWhen the relevance judgments are binary, i.e., ris either relevant or non-relevant, researchers often\nrefer to the training data as comprising (query, relevant document) pairs. In some papers, the training\ndata are described as (query, relevant document, non-relevant document) triples, but this is merely\na different organization of (q,d,r ) triples. It is important to note that non-relevant documents are\noften qualitatively different from relevant documents. Relevant documents are nearly always judged\nby a human assessor as being so. Non-relevant documents, however, may either come from explicit\nhuman judgments or they may be heuristically constructed. For example, in the MS MARCO passage\nranking test collection, non-relevant documents are sampled from BM25 results not otherwise marked\nas relevant (see Section 2.7 for details). Here, we have a divergence in data preparation for training\nversus evaluation: heuristically sampling non-relevant documents is a common technique when\ntraining a model. However, such sampling is almost never used during evaluation. Thus, there arises\nthe distinction between documents that have been explicitly judged as non-relevant and “unjudged”\ndocuments, which we discuss in the context of ranking metrics below.\n2.5 Ranking Metrics\nRanking metrics quantify the quality of a ranking of texts and are computed from relevance judgments\n(qrels), described in the previous section. The ranked lists produced by a system (using a particular\napproach) for a set of queries (in TREC, topics) is called a “run”, or sometimes a “submission”,\nin that ﬁles containing these results represent the artifacts submitted for evaluation, for example,\nin TREC evaluations (more below). The qrels and the run ﬁle are fed into an evaluation program\nsuch as trec_eval, the most commonly used program by information retrieval researchers, which\nautomatically computes a litany of metrics. These metrics deﬁne the hill to climb in the quest for\neffectiveness improvements.\nBelow, we describe a number of common metrics that are used throughout this survey. To be\nconsistent with the literature, we largely follow the notation and convention of Mitra and Craswell\n[2019a]. We rewrite a ranked list R = {(di,si)}l\ni=1 of length las {(i,di)}l\ni=1, retaining only the\nrank iinduced by the score si’s. Many metrics are computed at a particular cutoff (or have variants\nthat do so), which means that the ranked list Ris truncated to a particular length k, {(di,si)}k\ni=1,\nwhere k≤l: this is notated as Metric@k. The primary difference between land kis that the system\ndecides l(i.e., how many results to return), whereas kis a property of the evaluation metric, typically\nset by the organizers of an evaluation or the authors of a paper. Sometimes,land kare left unspeciﬁed,\nin which case it is usually the case that l= k= 1000. In most TREC evaluations, runs contain up\n45Yes, there are those who actually try to pronounce this jumble of letters.\n26\nto 1000 results per topic, and the metrics evaluate the entirety of the ranked lists (unless an explicit\ncutoff is speciﬁed).\nFrom a ranked list R, we can compute the following metrics:\nPrecision is deﬁned as the fraction of documents in ranked list Rthat are relevant, or:\nPrecision(R,q) =\n∑\n(i,d)∈Rrel(q,d)\n|R| , (3)\nwhere rel(q,d) indicates whether document d is relevant to query q, assuming binary relevance.\nGraded relevance judgments are binarized with some relevance threshold, e.g., in a three-grade\nscale, we might set rel(q,d) = 1 for “relevant” and “highly relevant” judgments. Often, precision is\nevaluated at a cutoff k, notated as Precision@kor abbreviated as P@k. If the cutoff is deﬁned in\nterms of the number of relevant documents for a particular topic (i.e., a topic-speciﬁc cutoff), the\nmetric is known as R-precision.\nPrecision has the advantage that it is easy to interpret: of the topkresults, what fraction are relevant?46\nThere are two main downsides: First, precision does not take into account graded relevance judgments,\nand for example, cannot separate “relevant” from “highly relevant” results since the distinction is\nerased in rel(q,d). Second, precision does not take into account rank positions (beyond the cutoff\nk). For example, consider P@10: relevant documents appearing at ranks one and two (with no other\nrelevant documents) would receive a precision of 0.2; P@10 would be exactly the same if those\ntwo relevant documents appeared at ranks nine and ten. Yet, clearly, the ﬁrst ranked list would be\npreferred by a user.\nRecall is deﬁned as the fraction of relevant documents (in the entire collection C) for q that are\nretrieved in ranked list R, or:\nRecall(R,q) =\n∑\n(i,d)∈Rrel(q,d)\n∑\nd∈Crel(q,d) , (4)\nwhere rel(q,d) indicates whether document d is relevant to query q, assuming binary relevance.\nGraded relevance judgments are binarized in the same manner as precision.\nMirroring precision, recall is often evaluated at a cutoff k, notated as Recall@kor abbreviated R@k.\nThis metric has the same advantages and disadvantages as precision: it is easy to interpret, but does\nnot take into account relevance grades or the rank positions in which relevant documents appear.47\nReciprocal rank (RR) is deﬁned as:\nRR(R,q) = 1\nranki\n, (5)\nwhere ranki is the smallest rank number of a relevant document. That is, if a relevant document\nappears in the ﬁrst position, reciprocal rank = 1, 1/2 if it appears in the second position, 1/3 if it\nappears in the third position, etc. If a relevant document does not appear in the top k, then that query\nreceives a score of zero. Like precision and recall, RR is computed with respect to binary judgments.\nAlthough RR has an intuitive interpretation, it only captures the appearance of the ﬁrst relevant result.\nFor question answering or tasks in which the user may be satisﬁed with a single answer, this may be\nan appropriate metric, but reciprocal rank is usually a poor choice for ad hoc retrieval because users\n46There is a corner case here if l<k : for example, what is P@10 for a ranked list that only has ﬁve results? One\npossibility is to always use kin the denominator, in which case the maximum possible score is 0.5; this has\nthe downside of averaging per-topic scores that have different ranges when summarizing effectiveness across\na set of topics. The alternative is to use las the denominator. Unfortunately, treatment is inconsistent in the\nliterature.\n47Note that since the denominator in the recall equation is the total number of relevant documents, the symmetric\nsituation of what happens when l<k does not exist as it does with precision. However, a different issue arises\nwhen kis smaller than the total number of relevant documents, in which case perfect recall is not possible.\nTherefore, it is inadvisable to set kto a value smaller than the smallest total number of relevant documents\nfor a topic across all topics in a test collection. While in most formulations, kis ﬁxed for all topics in a test\ncollection, there exist variant metrics (though less commonly used) where kvaries per topic, for example, as a\nfunction of the number of (known) relevant documents for that topic.\n27\nusually desire more than one relevant document. As with precision and recall, reciprocal rank can be\ncomputed at a particular rank cutoff, denoted with the same @kconvention.\nAverage Precision (AP)is deﬁned as:\nAP(R,q) =\n∑\n(i,d)∈RPrecision@i(R,q) ·rel(q,d)\n∑\nd∈Crel(q,d) , (6)\nwhere all notation used have already been deﬁned. The intuitive way to understand average precision\nis that it is the average of precision scores at cutoffs corresponding to the appearance of every\nrelevant document; rel(q,d) can be understood as a binary indicator variable, where non-relevant\ndocuments contribute nothing. Since the denominator is the total number of relevant documents,\nrelevant documents that don’t appear in the ranked list at all contribute zero to the average. Once\nagain, relevance is assumed to be binary.\nTypically, average precision is measured without an explicit cutoff, over the entirety of the ranked\nlist; since the default length of lused in most evaluations is 1000, the practical effect is that AP is\ncomputed at a cutoff of rank 1000, although it is almost never written as AP@1000. Since the metric\nfactors in retrieval of all relevant documents, a cutoff would artiﬁcially reduce the score (i.e., it has\nthe effect of including a bunch of zeros in the average for relevant documents that do not appear in the\nranked list). Evaluations use average precision when the task requires taking into account recall, so\nimposing a cutoff usually doesn’t make sense. The implied cutoff of 1000 is a compromise between\naccurate measurement and practicality: in practice, relevant documents appearing below rank 1000\ncontribute negligibly to the ﬁnal score (which is usually reported to four digits after the decimal\npoint), and run submissions with 1000 hits per topic are still manageable in size.\nAverage precision is more difﬁcult to interpret, but it is a single summary statistic that captures\naspects of both precision and recall, while favoring appearance of relevant documents towards the top\nof the ranked list. The downside of average precision is that it does not distinguish between relevance\ngrades; that is, “marginally” relevant and “highly” relevant documents make equal contributions to\nthe score.\nNormalized Discounted Cumulative Gain (nDCG) is a metric that is most frequently used to\nmeasure the quality of web search results. Unlike the other metrics above, nDCG was speciﬁcally\ndesigned for graded relevance judgments. For example, if relevance were measured on a ﬁve-point\nscale, rel(q,d) would return r∈{0,1,2,3,4}. First, we deﬁne Discounted Cumulative Gain (DCG):\nDCG(R,q) =\n∑\n(i,d)∈R\n2rel(q,d) −1\nlog2(i+ 1). (7)\nGain is used here in the sense of utility, i.e., how much value does a user derive from a particular\nresult. There are two factors that go into this calculation: (1) the relevance grade (i.e., highly relevant\nresults are “worth” more than relevant results) and (2) the rank at which the result appears (relevant\nresults near the top of the ranked list are “worth” more). The discounting refers to the decay in the\ngain (utility) as the user consumes results lower and lower in the ranked list, i.e., factor (2). Finally,\nwe introduce normalization:\nnDCG(R,q) = DCG(R,q)\nIDCG(R,q), (8)\nwhere IDCG represents the DCG of an “ideal” ranked list: this would be a ranked list that begins\nwith all of the documents of the highest relevance grade, then the documents with the next highest\nrelevance grade, etc. Thus, nDCG represents DCG normalized to a range of [0,1] with respect to\nthe best possible ranked list. Typically, nDCG is associated with a rank cutoff; a value of 10 or 20 is\ncommon. Since most commercial web search engines present ten results on a page (on the desktop,\nat least), these two settings represent nDCG with respect to the ﬁrst or ﬁrst two pages of results. For\nsimilar reasons, nDCG@3 or nDCG@5 are often used in the context of mobile search, given the\nmuch smaller screen sizes of phones.\nThis metric is popular for evaluating the results of web search for a number of reasons: First, nDCG\ncan take advantage of graded relevance judgments, which provide ﬁner distinctions on output quality.\nSecond, the discounting and cutoff represent a reasonably accurate (albeit simpliﬁed) model of\nreal-world user behavior, as revealed through eye-tracking studies; see, for example, Joachims et al.\n28\n[2007]. Users do tend to scan results linearly, with increasing probability of “giving up” and “losing\ninterest” as they consume more and more results (i.e., proceed further down the ranked list). This is\nmodeled in the discounting, and there are variants of nDCG that apply different discounting schemes\nto model this aspect of user behavior. The cutoff value models a hard stop when users stop reading\n(i.e., give up). For example, nDCG@10 quantiﬁes the result quality of the ﬁrst page of search results\nin a browser, assuming the user never clicks “next page” (which is frequently the case).\nAll of the metrics we have discussed above quantify the quality of a single ranked list with respect\nto a speciﬁc topic (query). Typically, the arithmetic mean across all topics in a test collection is\nused as a single summary statistic to denote the quality of a run for those topics.48 We emphasize\nthat it is entirely meaningless to compare effectiveness scores from different test collections (since\nscores do not control for differences due to corpora, topic difﬁculty, and many other issues), and even\ncomparing a run that participated in a particular evaluation with a run that did not can be fraught with\nchallenges (see next section).\nA few additional words of caution: aggregation can hide potentially big differences in per-topic scores.\nSome topics are “easy” and some topics are “difﬁcult”, and it is certainly possible that a particular\nranking model has an afﬁnity towards certain types of information needs. These nuances are all lost\nin a simple arithmetic mean across per-topic scores.\nThere is one frequently unwritten detail that is critical to the interpretation of metrics worth discussing.\nWhat happens if the ranked list Rcontains a document for which no relevance judgment exists, i.e.,\nthe document does not appear in the qrels ﬁle for that topic? This is called an “unjudged document”,\nand the standard treatment (by most evaluation programs) is to consider unjudged documents not\nrelevant. Unjudged documents are quite common because it is impractical to exhaustively assess the\nrelevance of every document in a collection with respect to every information need; the question of\nhow to select documents for assessment is discussed in the next section, but for now let’s just take\nthis observation as a given.\nThe issue of unjudged documents is important because of the assumption that unjudged documents\nare not relevant. Thus, a run may score poorly not because the ranking model is poor, but because\nthe ranking model produces many results that are unjudged (again, assume this as a given for now;\nwe discuss why this may be the case in the next section). The simplest way to diagnose potential\nissues is to compute the fraction of judged documents at cutoff k(Judged@kor J@k). For example,\nif we ﬁnd that 80% of the results in the top 10 hits are unjudged, Precision@10 is capped at 0.2.\nThere is no easy ﬁx to this issue beyond diagnosing and noting it: assuming that unjudged documents\nare not relevant is perhaps too pessimistic, but the alternative of assuming that unjudged documents\nare relevant is also suspect. While information retrieval researchers have developed metrics that\nexplicitly account for unjudged documents, e.g., bpref [Buckley and V oorhees, 2004], the condensed\nlist approach [Sakai, 2007], and rank-based precision (RBP) [Moffat and Zobel, 2008], in our opinion\nthese metrics have yet to reach widespread adoption by the community.\nThere is a ﬁnal detail worth explicitly mentioning. All of the above metrics assume that document\nscores are strictly decreasing, and that there are no score ties. Otherwise, the evaluation program\nmust arbitrarily make some decision to map identical scores to different ranks (necessary because\nmetrics are deﬁned in terms of rank order). For example, trec_eval breaks ties based on the reverse\nlexicographical order of the document ids. These arbitrary decisions introduce potential differences\nacross alternative implementations of the same metric. Most recently, Lin and Yang [2019] quantiﬁed\nthe effects of scoring ties from the perspective of experimental repeatability and found that score\nties can be responsible for metric differences up to the third place after the decimal point. While the\noverall effects are small and not statistically signiﬁcant, to eliminate this experimental confound, they\nadvocated that systems should explicitly ensure that there are no score ties in the ranked lists they\nproduce, rather than let the evaluation program make arbitrary decisions.49 Of course, Lin and Yang\nwere not the ﬁrst to examine this issue, see for example, Cabanac et al. [2010], Ferro and Silvello\n[2015] for additional discussions.\n48Although other approaches for aggregation have been explored, such as the geometric and harmonic means [Ra-\nvana and Moffat, 2009].\n49This can be accomplished by ﬁrst deﬁning a consistent tie-breaking procedure and then subtracting a small ϵto\nthe tied scores to induce the updated rank ordering.\n29\nWe conclude this section with a number of remarks, some of which represent conventions and tacit\nknowledge by the community that are rarely explicitly communicated:\n• Naming metrics. Mean average precision, abbreviated MAP, represents the mean of average\nprecision scores across many topics. Similarly, mean reciprocal rank, abbreviated MRR,\nrepresents the mean of reciprocal rank scores across topics. 50 In some papers, the phrase\n“early-precision” is used to refer to the quality of top ranked results—as measured by a metric\nsuch as Precision@kor nDCG@kwith a relatively small cutoff (e.g., k = 10). It is entirely\npossible for a system to excel at early precision (i.e., identify a few relevant documents and\nplace them near the top of the ranked list) but not necessarily be effective when measured using\nrecall-oriented metrics (which requires identifying all relevant documents).\n• Reporting metrics. Most test collections or evaluations adopt an ofﬁcial metric, or sometimes,\na few ofﬁcial metrics. It is customary when reporting results to at least include those ofﬁcial\nmetrics; including additional metrics is usually ﬁne, but the ofﬁcial metrics should not be\nneglected. The choice of metric is usually justiﬁed by the creators of the test collection or the\norganizers of the evaluation (e.g., we aim to solve this problem, and the quality of the solution\nis best captured by this particular metric). Unless there is a compelling reason otherwise, follow\nestablished conventions; otherwise, results will not be comparable.\nIt has been a convention, for example, at TREC, that metrics are usually reported to four places\nafter the decimal, e.g., 0.2932. In prose, a unit of 0.01 in score is often referred to as a point, as\nin, an improvement from 0.19 to 0.29 is a ten-point gain. In some cases, particularly in NLP\npapers, metrics are reported in these terms, e.g., multiplied by 100, so 0.2932 becomes 29.32.51\nWe ﬁnd this convention acceptable, as there is little chance for confusion. Finally, recognizing\nthat a difference of 0.001 is just noise, some researchers opt to only report values to three digits\nafter the decimal point, so 0.2932 becomes 0.293.\n• Comparing metrics. Entire tomes have been written about proper evaluation practices when\ncomparing results, for example, what statistical tests of signiﬁcance to use and when. As we\nlack the space for a detailed exposition, we refer readers to Sakai [2014] and Fuhr [2017] as\nstarting points into the literature.\nHaving deﬁned metrics for measuring the quality of a ranked list, we have now described all\ncomponents of the text ranking problem: Given an information need expressed as a query q, the text\nranking task is to return a ranked list of ktexts {d1,d2 ...d k}from an arbitrarily large but ﬁnite\ncollection of texts C= {di}that maximizes a metric of interest. Where are the resources we need to\nconcretely tackle this challenge? We turn our attention to this next.\n2.6 Community Evaluations and Reusable Test Collections\nBased on the discussions above, we can enumerate the ingredients necessary to evaluate a text ranking\nmodel: a corpus or collection of texts to search, a set of information needs (i.e., topics), and relevance\njudgments (qrels) for those needs. Together, these comprise the components of what is known as a\ntest collection for information retrieval research. With a test collection, it become straightforward to\ngenerate rankings with a particular ranking model and then compute metrics to quantify the quality\nof those rankings, for example, using any of those discussed in the previous section. And having\nquantiﬁed the effectiveness of results, it then becomes possible to make measurable progress in\nimproving ranking models. We have our hill and we know how high up we are. And if we have\nenough relevance judgments (see Section 2.4), we can directly train ranking models. In other words,\nwe have a means to climb the hill.\n50Some texts use MAP to refer to the score for a speciﬁc topic, which is technically incorrect. This is related to a\nsomewhat frivolous argument on metric names that has raged on in the information retrieval community for\ndecades now: there are those who argue that even the summary statistic across multiple topics for AP should\nbe referred to as AP. They point as evidence the fact that no researcher would ever write “MP@5” (i.e., mean\nprecision at rank cutoff 5), and thus to be consistent, every metric should be preﬁxed by “mean”, or none at\nall. Given the awkwardness of “mean precision”, the most reasonable choice is to omit “mean” from average\nprecision as well. We do not wish to take part in this argument, and use “MAP” and “MRR” simply because\nmost researchers do.\n51This likely started with BLEU scores in machine translation.\n30\nAlthough conceptually simple, the creation of resources to support reliable, large-scale evaluation of\ntext retrieval methods is a costly endeavor involving many subtle nuances that are not readily apparent,\nand is typically beyond the resources of individual research groups. Fortunately, events such as the\nText Retrieval Conferences (TRECs), organized by the U.S. National Institute for Standards and\nTechnology (NIST), provide the organizational structure as well as the resources necessary to bring\ntogether multiple teams in community-wide evaluations. These exercises serve a number of purposes:\nFirst, they provide an opportunity for the research community to collectively set its agenda through\nthe types of tasks that are proposed and evaluated; participation serves as a barometer to gauge interest\nin emerging information access tasks. Second, they provide a neutral forum to evaluate systems in a\nfair and rigorous manner. Third, typical byproducts of evaluations include reusable test collections\nthat are capable of evaluating systems that did not participate in the evaluation (more below). Some\nof these test collections are used for many years, some even decades, after the original evaluations\nthat created them. Finally, the evaluations may serve as testbeds for advancing novel evaluation\nmethodologies themselves; that is, the goal is not only to evaluate systems, but the processes for\nevaluating systems.\nTREC, which has been running for three decades, kicks off each spring with a call for participation.\nThe evaluation today is divided into (roughly half a dozen) “tracks” that examine different information\naccess problems. Proposals for tracks are submitted the previous year in the fall, where groups of\nvolunteers (typically, researchers from academia and industry) propose to organize tracks. These\nproposals are then considered by a committee, and selected proposals deﬁne the evaluation tasks\nthat are run. Over its history, TREC has explored a wide range of tasks beyond ad hoc retrieval,\nincluding search in a variety of different languages and over speech; in specialized domains such\nas biomedicine and chemistry; different types of documents such as blogs and tweets; different\nmodalities of querying such as ﬁltering and real-time summarization; as well as interactive retrieval,\nconversational search, and other user-focused issues. For a general overview of different aspects\nof TREC (at least up until the middle of the ﬁrst decade of the 2000s), the “TREC book” edited\nby V oorhees and Harman [2005] provides a useful starting point.\nTracks at TREC often reﬂect emerging interests in the information retrieval community; explorations\nthere often set the agenda for the ﬁeld and achieve signiﬁcant impact beyond the academic ivory tower.\nWriting in 2008, Hal Varian, chief economist at Google, acknowledged that in the early days of the\nweb, “researchers used industry-standard algorithms based on the TREC research to ﬁnd documents\non the web”.52 Another prominent success story of TREC is IBM’s Watson question answering\nsystem that resoundingly beat two human champions on the quiz show Jeopardy! in 2011. There is a\ndirect lineage from Watson, including both the techniques it used and the development team behind\nthe scenes, to the TREC question answering tracks held in the late 1990s and early 2000s.\nParticipation in TREC is completely voluntary with no external incentives (e.g., prize money),53 and\nthus researchers “vote with their feet” in selecting tracks that are of interest to them. While track\norganizers begin with a high-level vision, the development of individual tracks is often a collaboration\nbetween the organizers and participants, aided by guidance from NIST. System submissions for the\ntasks are typically due in the summer, with evaluation results becoming available in the fall time\nframe. Each TREC cycle concludes with a workshop held on the grounds of the National Institute\nof Standards and Technology in Gaithersburg, Maryland, where participants convene to discuss the\nevaluation results and present their solutions to the challenges deﬁned in the different tracks.54 The\ncycle then begins anew with planning for the next year.\nBeyond providing the overarching organizational framework for exploring different tracks at TREC,\nNIST also contributes evaluation resources and expertise, handling the bulk of the “mechanics”\nof the evaluation. Some of this was already discussed in Section 2.2: Unless specialized domain\nexpertise is needed, for example, in biomedicine, NIST assessors perform topic development, or the\ncreation of the information needs, and provide the relevance assessments as well. Historically, most\nof the NIST assessors are retired intelligence analysts, which means that assessing, synthesizing,\nand otherwise drawing conclusions from information was, literally, their job. Topic development is\nusually performed in the spring, based on initial exploration of the corpus used in the evaluation. To\n52https://googleblog.blogspot.com/2008/03/why-data-matters.html\n53An exception is that sometimes a research sponsor (funding agency) uses TREC as an evaluation vehicle, in\nwhich case teams that receive funding are compelled to participate.\n54In the days before the COVID-19 pandemic, that is.\n31\nthe extent possible, the assessor who created the topic (and wrote the topic statement) is the person\nwho provides the relevance judgments (later that year, generally in the late summer to early fall time\nframe). This ensures that the judgments are as consistent as possible. To emphasize a point we have\nalready made in Section 2.2: the relevance judgments are the opinion of this particular person.55\nWhat do NIST assessors actually evaluate? In short, they evaluate the submissions (i.e., “runs”) of\nteams who participated in the evaluation. For each topic, using a process known as pooling [Sparck\nJones and van Rijsbergen, 1975, Buckley et al., 2007], runs from the participants are gathered, with\nduplicates removed, and presented to the assessor. To be clear, a separate pool is created for each\ntopic. The most common (and fair) way to construct the pools is to select the top k results from\neach participating run, where kis determined by the amount of assessment resources available. This\nis referred to as top-kpooling or pooling to depth k. Although NIST has also experimented with\ndifferent approaches to constructing the pools, most recently, using bandit techniques [V oorhees,\n2018], top-kpooling remains the most popular approach due to its predictability and well-known\nproperties (both advantages and disadvantages).\nSystem results for each query (i.e., from the pools) are then presented to an assessor in an evaluation\ninterface, who supplies the relevance judgments along the previously agreed scale (e.g., a three-way\nrelevance grade). To mitigate systematic biases, pooled results are not associated with the runs they\nare drawn from, so the assessor only sees (query, result) pairs and has no explicit knowledge of the\nsource. After the assessment process completes, all judgments are then gathered to assemble the qrels\nfor those topics, and these relevance judgments are used to evaluate the submitted runs (e.g., using\none or a combination of the metrics discussed in the previous section).\nRelevance judgments created from TREC evaluations are used primarily in one of two ways:\n1. They are used to quantify the effectiveness of systems that participated in the track. The\nevaluation of the submitted runs using the relevance judgments created from the pooling process\naccomplishes this goal, but the results need to be interpreted in a more nuanced way than\njust comparing the value of the metrics. Whether system differences can be characterized as\nsigniﬁcant or meaningful is more than just a matter of running standard signiﬁcance tests, but\nmust consider a multitude of other factors, including all the issues discussed in Section 2.2\nand more [Sanderson and Zobel, 2005]. Details of how this is accomplished depend on the\ntask and vary from track to track; for an interested reader, V oorhees and Harman [2005] offer a\ngood starting point. For more details, in each year’s TREC proceedings, each track comes with\nan overview paper written by the organizers that explains the task setup and summarizes the\nevaluation results.\n2. Relevance judgments contribute to a test collection that can be used as a standalone evaluation\ninstrument by researchers beyond the original TREC evaluation that created them. These test\ncollections can be used for years and even decades; for example, as we will describe in more\ndetail in the next section, the test collection from the TREC 2004 Robust Track is still widely\nused today!\nIn the context of using relevance judgments from a particular test collection, there is an important\ndistinction between runs that participated in the evaluation vs. those that did not. These “after-the-fact”\nruns are sometimes called “post hoc” runs.\nFirst, the results of ofﬁcial submissions are considered by most researchers to be more “credible”\nthan post-hoc runs, due to better methodological safeguards (e.g., less risk of overﬁtting). We return\nto discuss this issue in more detail in Section 2.7.\nSecond, relevance judgments may treat participating systems and post-hoc submissions differently,\nas we explain. There are two common use cases for test collections: A team that participated in\nthe TREC evaluation might use the relevance judgments to further investigate model variants or\nperhaps conduct ablation studies. A team that did not participate in the TREC evaluation might use\nthe relevance judgments to evaluate a newly proposed technique, comparing it against runs submitted\nto the evaluation. In the former case, a variant technique is likely to retrieve similar documents as\na submitted run, and therefore less likely to encounter unjudged documents—which, as we have\npreviously mentioned, are treated as not relevant by standard evaluation tools (see Section 2.5). In\n55The NIST assessors are invited to the TREC workshop, and every year, some subset of them do attend. And\nthey’ll sometimes even tell you what topic was theirs. Sometimes they even comment on your system.\n32\nthe latter case, a newly proposed technique may encounter more unjudged documents, and thus\nscore poorly—not necessarily because it was “worse” (i.e., lower quality), but simply because it was\ndifferent. That is, the new technique surfaced documents that had not been previously retrieved (and\nthus never entered the pool to be assessed).\nIn other words, there is a danger that test collections encourage researchers to search only “under the\nlamplight”, since the universe of judgments is deﬁned by the participants of a particular evaluation\n(and thus represents a snapshot of the types of techniques that were popular at the time). Since many\ninnovations work differently than techniques that came before, old evaluation instruments may not be\ncapable of accurately quantifying effectiveness improvements associated with later techniques. As a\nsimple (but contrived) example, if the pools were constructed exclusively from techniques based on\nexact term matches, the resulting relevance judgments would be biased against systems that exploited\nsemantic match techniques that did not rely exclusively on exact match signals. In general, old\ntest collections may be biased negatively against new techniques, which is particularly undesirable\nbecause they may cause researchers to prematurely abandon promising innovations simply because\nthe available evaluation instruments are not able to demonstrate their improvements.\nFortunately, IR researchers have long been cognizant of these dangers and evaluations usually take\na variety of steps to guard against them. The most effective strategy is to ensure a rich and diverse\npool, where runs adopt a variety of different techniques, and to actively encourage “manual” runs\nthat involve humans in the loop (i.e., users interactively searching the collection to compile results).\nSince humans obviously do more than match keywords, manual runs increase the diversity of the\npool. Furthermore, researchers have developed various techniques to assess the reusability of test\ncollections, characterizing their ability to fairly evaluate runs from systems that did not participate\nin the original evaluation [Zobel, 1998, Buckley et al., 2007]. The literature describes a number of\ndiagnostics, and test collections that pass this vetting are said to be reusable.\nFrom a practical perspective, there are several steps that researchers can take to sanity check their\nevaluation scores to determine if a run is actually worse, or simply different. One common technique\nis to compute and report the fraction of unjudged documents, as discussed in the previous section.\nIf two runs have very different proportions of unjudged documents, this serves as a strong signal\nthat one of those runs may not have been evaluated fairly. Another approach is to use a metric that\nexplicitly attempts to account for unjudged documents, such as bpref or RBP (also discussed in the\nprevious section).\nObviously, different proportions of unjudged documents can be a sign that effectiveness differences\nmight be attributable to missing relevance judgments. However, an important note is that the absolute\nproportion of unjudged documents is not necessarily a sign of unreliable evaluation results in itself.\nThe critical issue is bias, in the sense of Buckley et al. [2007]: whether the relevance judgments\nrepresent a random (i.e., non-biased) sample of all relevant documents. Consider the case where two\nruns have roughly the same proportion of unjudged documents (say, half are unjudged). There are few\nﬁrm conclusions that can be drawn in this situation without more context. Unjudged documents are\ninevitable, and even a relatively high proportion of unjudged isn’t “bad” per se. This could happen,\nfor example, when two runs that participated in an evaluation are assessed with a metric at a cutoff\nlarger than the number of documents each run contributed to the pool. For example, the pool was\nconstructed with top-100 pooling, but MAP is measured to rank 1000. In such cases, there is no\nreason to believe that the unjudged documents are systematically biased against one run or the other.\nHowever, in other cases (for example, the bias introduced by systems based on exact term matching),\nthere may be good reason to suspect the presence of systematic biases.\nTREC, as a speciﬁc realization of the Cranﬁeld paradigm, has been incredibly inﬂuential, both on IR\nresearch and more broadly in the commercial sphere; for example, see an assessment of the economic\nimpact of TREC conducted in 2010 [Rowe et al., 2010]. TREC’s longevity—2021 marks the thirtieth\niteration—is just one testament to its success. Another indicator of success is that the “TREC model”\nhas been widely emulated around the world. Examples include CLEF in Europe and NTCIR and\nFIRE in Asia, which are organized in much the same way.\nWith this exposition, we have provided a high-level overview of modern evaluation methodology for\ninformation retrieval and text ranking under the Cranﬁeld paradigm—covering inputs to and outputs\nof the ranking model, how the results are evaluated, and how test collections are typically created.\n33\nWe conclude with a few words of caution already mentioned in the introductory remarks: The beauty\nof the Cranﬁeld paradigm lies in a precise formulation of the ranking problem with a battery of\nquantitative metrics. This means that, with sufﬁcient training data, search can be tackled as an\noptimization problem using standard supervised machine-learning techniques. Beyond the usual\nconcerns with overﬁtting, and whether test collections are realistic instances of information needs\n“in the wild”, there is a fundamental question regarding the extent to which system improvements\ntranslates into user beneﬁts. Let us not forget that the latter is the ultimate goal, because users\nseek information to “do something”, e.g., decide what to buy, write a report, ﬁnd a job, etc. A\nwell-known ﬁnding in information retrieval is that better search systems (as evaluated by the Cranﬁeld\nmethodology) might not lead to better user task performance as measured in terms of these ultimate\ngoals; see, for example, Hersh et al. [2000], Allan et al. [2005]. Thus, while evaluations using the\nCranﬁeld paradigm undoubtedly provide useful signals in characterizing the effectiveness of ranking\nmodels, they do not capture “the complete picture”.\n2.7 Descriptions of Common Test Collections\nSupervised machine-learning techniques require data, and the community is fortunate to have access\nto many test collections, built over decades, for training and evaluating text ranking models. In this\nsection, we describe test collections that are commonly used by researchers today. Our intention\nis not to exhaustively cover all test collections used by every model in this survey, but to focus on\nrepresentative resources that have played an important role in the development of transformer-based\nranking models.\nWhen characterizing and comparing test collections, there are a few key statistics to keep in mind:\n• Size of the corpus or collection, in terms of the number of texts |C|, the mean length of each\ntext L(C), the median length of each text ˜L(C), and more generally, the distribution of the\nlengths. The size of the corpus is one factor in determining the amount of effort required to\ngather sufﬁcient relevance judgments to achieve “good” coverage. The average length of a text\nprovides an indication of the amount of effort required to assess each result, and the distribution\nof lengths may point to ranking challenges.56\n• Size of the set of evaluation topics, both in terms of the number of queries |q|and the average\nlength of each query L(q). Obviously, the more queries, the better, from the perspective of\naccurately quantifying the effectiveness of a particular approach. Average query length offers\nclues about the expression of the information needs (e.g., amount of detail).\n• The number of relevance judgments available, both in terms of positive and negative labels.\nWe can quantify this in terms of the average number of judgments per query|J|/qas well as\nthe number of relevant labels per query |Rel|/q.57 Since the amount of resources (assessor\ntime, money for paying assessors, etc.) that can be devoted to performing relevance judgments\nis usually ﬁxed, there are different strategies for allocating assessor effort. One choice is to\njudge many queries (say, hundreds), but examine relatively few results per query, for example,\nby using a shallow pool depth. An alternative is to judge fewer queries (say, dozens), but\nexamine more texts per query, for example, by using a deeper pool depth. Colloquially, these\nare sometimes referred to as “shallow but wide” (or “sparse”) judgments vs. “narrow but deep”\n(or “dense”) judgments. We discuss the implications of these different approaches in the context\nof speciﬁc test collections below.\nIn addition, the number of relevant texts (i.e., positive judgments) per topic is an indicator of\ndifﬁculty. Generally, evaluation organizers prefer topics that are neither too difﬁcult nor too\neasy. If the topics are too difﬁcult (i.e., too few relevant documents), systems might all perform\npoorly, making it difﬁcult to discriminate system effectiveness, or systems might perform well\nfor idiosyncratic reasons that are difﬁcult to generalize. On the other hand, if the topics are too\n56Retrieval scoring functions that account for differences in document lengths, e.g., Singhal et al. [1996],\nconstituted a major innovation in the 1990s. As we shall see in Section 3, long texts pose challenges for\nranking with transformer-based models. In general, collections with texts that differ widely in length are more\nchallenging, since estimates of relevance must be normalized with respect to length.\n57In the case of graded relevance judgments, there is typically a binarization scheme to separate relevance grades\ninto “relevant” and “not relevant” categories for metrics that require binary judgments.\n34\nCorpus |C| L(C) ˜L(C)\nMS MARCO passage corpus 8,841,823 56.3 50\nMS MARCO document corpus 3,213,835 1131.3 584\nRobust04 corpus (TREC disks 4&5) 528,155 548.6 348\nTable 2: Summary statistics for three corpora used by many text ranking models presented in this\nsurvey: number of documents |C|, mean document length L(C), and median document length ˜L(C).\nThe MS MARCO passage corpus was also used for the TREC 2019/2020 Deep Learning Track\npassage ranking task and the MS MARCO document corpus was also used for the TREC 2019/2020\nDeep Learning Track document ranking task.\nDataset |q| L(q) |J| |J|/q |Rel|/q\nMS MARCO passage ranking (train) 502,939 6.06 532,761 1.06 1.06\nMS MARCO passage ranking (development) 6,980 5.92 7,437 1.07 1.07\nMS MARCO passage ranking (test) 6,837 5.85 - - -\nMS MARCO document ranking (train) 367,013 5.95 367,013 1.0 1.0\nMS MARCO document ranking (development 5,193 5.89 5,193 1.0 1.0\nMS MARCO document ranking (test) 5,793 5.85 - - -\nTREC 2019 DL passage 43 5.40 9,260 215.4 58.2\nTREC 2019 DL document 43 5.51 16,258 378.1 153.4\nTREC 2020 DL passage 54 6.04 11,386 210.9 30.9\nTREC 2020 DL document 45 6.31 9,098 202.2 39.3\nRobust04 249 (title) 2.67 311,410 1250.6 69.9\n(narr.) 15.32\n(desc.) 40.22\nTable 3: Summary statistics for select queries and relevance judgments used by many text ranking\nmodels presented in this survey. For Robust04, we separately provide average lengths of the title,\nnarrative, and description ﬁelds of the topics. Note that for the TREC 2019/2020 DL data, relevance\nbinarization is different for passage vs. documents; here we simply count all judgments that have a\nnon-zero grade.\neasy (i.e., too many relevant documents), then all systems might obtain high scores, also making\nit difﬁcult to separate “good” from “bad” systems.\nA few key statistics of the MS MARCO passage ranking test collection, MS MARCO document\nranking test collection, and the Robust04 test collection are summarized in Table 2 and Table 3. The\ndistributions of the lengths of texts from these three corpora are shown in Figure 3. In these analyses,\ntokens counts are computed by splitting texts on whitespace,58 which usually yields values that differ\nfrom lengths computed from the perspective of keyword search (e.g., due to stopwords removal and\nde-compounding) and lengths from the perspective of input sequences to transformers (e.g., due to\nsubword tokenization).\nWe describe a few test collections in more detail below:\nMS MARCO passage ranking test collection. This dataset, originally released in 2016 [Nguyen\net al., 2016], deserves tremendous credit for jump-starting the BERT revolution for text ranking.\nWe’ve already recounted the story in Section 1.2.5: Nogueira and Cho [2019] combined the two\ncritical ingredients (BERT and training data for ranking) to make a “big splash” on the MS MARCO\npassage ranking leaderboard.\nThe MS MARCO dataset was originally released in 2016 to allow academic researchers to explore\ninformation access in the large-data regime—in particular, to train neural network models [Craswell\net al., 2021a]. Initially, the dataset was designed to study question answering on web passages,\nbut it was later adapted into traditional ad hoc ranking tasks. Here, we focus only on the passage\nranking task [Bajaj et al., 2018]. The corpus comprises 8.8 million passage-length extracts from web\npages; these passages are typical of “answers” that many search engines today show at the top of\n58Speciﬁcally, Python’ssplit() method for strings.\n35\n0 30 60 90 120 150\n0\n500,000\n1,000,000\n1,500,000\nNumber of texts\nMS MARCO Passage\n0 1,000 2,000 3,000 4,000 5,000\n0\n50,000\n100,000\n150,000\nNumber of texts\nMS MARCO Document\n0 500 1,000 1,500 2,000\n0\n10,000\n20,000\n30,000\n40,000\n50,000\nNumber of texts\nRobust04\nFigure 3: Histograms capturing the distribution of the lengths of texts (based on whitespace tokeniza-\ntion) in three commonly used corpora.\n36\ntheir result pages (these are what Google calls “featured snippets”, and Bing has a similar feature).\nThe information needs are anonymized natural language questions drawn from Bing’s query logs,\nwhere users were speciﬁcally looking for an answer; queries with navigational and other intents\nwere discarded. Since these questions were drawn from user queries “in the wild”, they are often\nambiguous, poorly formulated, and may even contain typographical and other errors. Nevertheless,\nthese queries reﬂect a more “natural” distribution of information needs, compared to, for example,\nexisting question answering datasets such as SQuAD [Rajpurkar et al., 2016].\nFor each query, the test collection contains, on average, one relevant passage (as assessed by human\nannotators). In the training set, there are a total of 532.8K (query, relevant passage) pairs over 502.9K\nunique queries. The development (validation) set contains 7437 pairs over 6980 unique queries. The\ntest (evaluation) set contains 6837 queries, but relevance judgments are not publicly available; scores\non the test queries can only be obtained via a submission to the ofﬁcial MS MARCO leaderboard.59\nThe ofﬁcial evaluation metric is MRR@10.\nOne notable feature of this resource worth pointing out is the sparsity of judgments—there are many\nqueries, but on average, only one relevant judgment per query. This stands in contrast to most test\ncollections constructed by pooling, such as those from TREC evaluations. As we discussed above,\nthese judgments are often referred to as “shallow” or “sparse”, and this design has two important\nconsequences:\n1. Model training requires both positive as well as negative examples. For this, the task organizers\nhave prepared “triples” ﬁles comprising (query, relevant passage, non-relevant passage) triples.\nHowever, these negative examples are heuristically-induced pseudo-labels: they are drawn from\nBM25 results that have not been marked as non-relevant by human annotators. In other words,\nthe negative examples have not been explicitly vetted by human annotators as deﬁnitely being\nnot relevant. The absence of a positive label does not necessarily mean that the passage is\nnon-relevant.\n2. As we will see in Section 3.2, the sparsity of judgments holds important implications for the\nability to properly assess the contribution of query expansion techniques. This is a known\ndeﬁciency, but there may be other yet-unknown issues as well. The lack of “deep” judgments\nper query in part motivated the need for complementary evaluation data, which are supplied by\nthe TREC Deep Learning Tracks (discussed below).\nThese ﬂaws notwithstanding, it is difﬁcult to exaggerate the important role that the MS MARCO\ndataset has played in advancing research in information retrieval and information access more broadly.\nNever before had such a large and realistic dataset been made available to the academic research\ncommunity.60 Previously, such treasures were only available to researchers inside commercial\nsearch engine companies and other large organizations with substantial numbers of users engaged in\ninformation seeking.\nToday, this dataset is used by many researchers for diverse information access tasks, and it has become\na common starting point for building transformer-based ranking models. Even for ranking in domains\nthat are quite distant, for example, biomedicine (see Section 6.2), many transformer-based models\nare ﬁrst ﬁne-tuned with MS MARCO data before further ﬁne-tuning on domain- and task-speciﬁc\ndata (see Section 3.2.4). Some experiments have even shown that ranking models ﬁne-tuned on this\ndataset exhibit zero-shot relevance transfer capabilities, i.e., the models are effective in domains\nand on tasks without having been previously exposed to in-domain or task-speciﬁc labeled data (see\nSection 3.5.3 and Section 6.2).\nIn summary, the impact of the MS MARCO passage ranking test collection has been no less than\ntransformational. The creators of the dataset (and Microsoft lawyers) deserve tremendous credit for\ntheir contributions to broadening the ﬁeld.\nMS MARCO document ranking test collection. Although in reality the MS MARCO document\ntest collection was developed in close association with the TREC 2019 Deep Learning Track [Craswell\net al., 2020] (see below), and a separate MS MARCO document ranking leaderboard was established\n59http://www.msmarco.org/\n60Prior to MS MARCO, a number of learning-to-rank datasets comprising features values were available to\nacademic researchers, but they did not include actual texts.\n37\nonly in August 2020, it makes more sense conceptually to structure the narrative in the order we\npresent here.\nThe MS MARCO document ranking test collection was created as a document ranking counterpart\nto the passage ranking test collection. The corpus, which comprises 3.2M web pages with URL,\ntitle, and body text, contains the source pages of the 8.8M passages from the passage corpus [Bajaj\net al., 2018]. However, the alignment between the passages and the documents is imperfect, as the\nextraction was performed on web pages that were crawled at different times.\nFor the document corpus, relevance judgments were “transferred” from the passage judgments; that\nis, for a query, if the source web page contained a relevant passage, then the corresponding document\nwas considered relevant. This data preparation possibly created a systematic bias in that relevant\ninformation was artiﬁcially centered on a speciﬁc passage within the document, more so than they\nmight occur naturally. For example, we are less likely to see a relevant document that contains\nshort relevant segments scattered throughout the text; this has implications for evidence aggregation\ntechniques that we discuss in Section 3.3.\nIn total, the MS MARCO document dataset contains 367K training queries and 5193 development\nqueries; each query has exactly one relevance judgment. There are 5793 test queries, but relevance\njudgments are withheld from the public. As with the MS MARCO passage ranking task, scores for\nthe test queries can only be obtained by a submission to the leaderboard. The ofﬁcial evaluation\nmetric is MRR@100. Similar comments about the sparsity of relevance judgments, made in the\ncontext of the passage dataset above, apply here as well.\nTREC 2019/2020 Deep Learning Tracks. Due to the nature of TREC planning cycles, the organi-\nzation of the Deep Learning Track at TREC 2019 [Craswell et al., 2020] predated the advent of BERT\nfor text ranking. Coincidentally, though, it represented the ﬁrst large-scale community evaluation that\nprovided a comparison of pre-BERT and BERT-based ranking models, attracting much attention and\nparticipation from researchers. The Deep Learning Track continued in TREC 2020 [Craswell et al.,\n2021b] with the same basic setup.\nFrom the methodological perspective, the track was organized to explore the impact of large amounts\nof training data, both on neural ranking models as well as learning-to-rank techniques, compared to\n“traditional” exact match techniques. Furthermore, the organizers wished to investigate the impact of\ndifferent types of training labels, in particular, sparse judgments (many queries but very few relevance\njudgments per query) typical of data gathered in an industry setting vs. dense judgments created by\npooling (few queries but many more relevance judgments per query) that represent common practice\nin TREC and other academic evaluations. For example, what is the effectiveness of models trained\non sparse judgments when evaluated with dense judgments?\nThe evaluation had both a document ranking and a passage ranking task; additionally, the organizers\nshared a list of results for reranking if participants did not wish to implement initial candidate\ngeneration themselves. The document corpus and the passage corpus used in the track were exactly\nthe same as the MS MARCO document corpus and the MS MARCO passage corpus, respectively,\ndiscussed above. Despite the obvious connections, the document and passage ranking tasks were\nevaluated independently with separate judgment pools.\nBased on pooling, NIST assessors evaluated 43 queries for both the document ranking and passage\nranking tasks in TREC 2019; in TREC 2020, there were 54 queries evaluated for the passage ranking\ntask and 45 queries evaluated for the document ranking task. In all cases relevance judgments were\nprovided on a four-point scale, although the binarization of the grades (e.g., for the purposes of\ncomputing MAP) differed between the document and passage ranking tasks; we refer readers to the\ntrack overview papers for details [Craswell et al., 2020, 2021b]. Statistics of the relevance judgments\nare presented in Table 3. It is likely the case that these relevance judgments alone are insufﬁcient to\neffectively train neural ranking models (too few labeled examples), but they serve as a much richer\ntest set compared to the MS MARCO datasets. Since there are many more relevant documents per\nquery, metrics such as MAP are (more) meaningful, and since the relevance judgments are graded,\nmetrics such as nDCG make sense. In contrast, given the sparse judgments in the original MS\nMARCO datasets, options for evaluation metrics are limited. In particular, evaluation of document\nranking with MRR@100 is odd and rarely seen.\nMackie et al. [2021] built upon the test collections from the TREC 2019 and 2020 Deep Learning\nTracks to create a collection of challenging queries called “DL-HARD”. The goal of this resource was\n38\nto increase the difﬁculty of the Deep Learning Track collections using queries that are challenging for\nthe “right reasons”. That is, queries that express complex information needs rather than queries that\nare, for example, factoid questions (“how old is vanessa redgrave”) or queries that would typically be\nanswered by a different vertical (“how is the weather in jamaica”). DL-HARD combined difﬁcult\nqueries judged in the TREC 2019 and 2020 Deep Learning Track document and passage collections\n(25 from the document collection and 23 from the passage collection) with additional queries with\nnew sparse judgments (25 for the document collection and 27 for the passage collection). The authors\nassessed query difﬁculty using a combination of automatic criteria derived from a web search engine\n(e.g., whether the query could be answered with a dictionary deﬁnition infobox) and manual criteria\nlike the query’s answer type (e.g., deﬁnition, factoid, or long answer). The resource also includes\nentity links for the queries and annotations of search engine result type, query intent, answer type,\nand topic domain.\nTREC 2004 Robust Track (Robust04). Although nearly two decades old, the test collection from\nthe Robust Track at TREC 2004 [V oorhees, 2004] is widely considered one of the best “general\npurpose” ad hoc retrieval test collections available to academic researchers, with relevance judgments\ndrawn from diverse pools with contributions from different techniques, including manual runs. It\nis able to fairly evaluate systems that did not participate in the original evaluation (see Section 2.6).\nRobust04 is large as academic test collections go in terms of the number of topics and the richness of\nrelevance judgments, and created in a single TREC evaluation cycle. Thus, this test collection differs\nfrom the common evaluation practice where test collections from multiple years are concatenated\ntogether to create a larger resource. Merging multiple test collections in this way is possible when the\nunderlying corpus is the same, but this approach may be ignoring subtle year-to-year differences. For\nexample, there may be changes in track guidelines that reﬂect an evolving understanding of the task,\nwhich might, for example, lead to differences in how the topics are created and how documents are\njudged. The composition of the judgment pools (e.g., in terms of techniques that are represented)\nalso varies from year to year, since they are constructed from participants’ systems.\nThe TREC 2004 Robust Track used the corpus from TREC Disks 4 & 5 (minus Congressional\nRecords),61 which includes material from the Financial Times Limited, the Foreign Broadcast\nInformation Service, and the Los Angeles Times totaling approximately 528K documents. Due to its\ncomposition, this corpus is typically referred to as containing text from the newswire domain. The\ntest collection contains a total of 249 topics with around 311K relevance judgments, with topics ids\n301–450 and 601–700.62\nDue to its age, this collection is particularly well-studied by researchers; for example, a meta-analysis\nby Yang et al. [2019b] identiﬁed over 100 papers that have used the collection up until early 2019.63\nThis resource provides the context for interpreting effectiveness results across entire families of\napproaches and over time. However, the downside is that the Robust04 test collection is particularly\nvulnerable to overﬁtting.\nUnlike most TREC test collections with only around 50 topics, researchers have had some success\ntraining ranking models using Robust04. However, for this use, there is no standard agreed-upon\nsplit, but ﬁve-fold cross validation is the most common conﬁguration. It is often omitted in papers,\nbut researchers typically construct the splits by taking consecutive topic ids, e.g., the ﬁrst ﬁfty topics,\nthe next ﬁfty topics, etc.\nAdditional TREC newswire test collections.Beyond Robust04, there are two more recent newswire\ntest collections that have been developed at TREC:\n• Topics and relevance judgments from the TREC 2017 Common Core Track [Allan et al.,\n2017], which used 1.8M articles from the New York Times Annotated Corpus.64 Note that this\nevaluation experimented with a pooling methodology based on bandit techniques, which was\nfound after-the-fact to have a number of ﬂaws [V oorhees, 2018], making it less reusable than\ndesired. Evaluations conducted on this test collection should bear in mind this caveat.\n61https://trec.nist.gov/data/cd45/index.html\n62In the original evaluation, 250 topics were released, but for one topic no relevant documents were found in the\ncollection.\n63https://github.com/lintool/robust04-analysis\n64https://catalog.ldc.upenn.edu/LDC2008T19\n39\n• Topics and relevance judgments from the TREC 2018 Common Core Track [Allan et al., 2018],\nwhich used a corpus of 600K articles from the TREC Washington Post Corpus.65\nNote that corpora for these two test collections are small by modern standards, so they may not\naccurately reﬂect search scenarios today over large amounts of texts. In addition, both test collections\nare not as well-studied as Robust04. As a positive, this means there is less risk of overﬁtting, but this\nalso means that there are fewer effective models to compare against.\nTREC web test collections. There have been many evaluations at TREC focused on searching\ncollections of web pages. In particular, the following three are commonly used:\n• Topics and relevance judgments from the Terabyte Tracks at TREC 2004–2006, which used\nthe GOV2 corpus, a web crawl of the .gov domain comprising approximately 25.2M pages by\nCSIRO (Commonwealth Scientiﬁc and Industrial Research Organisation), distributed by the\nUniversity of Glasgow.66\n• Topics and relevance judgments from the Web Tracks at TREC 2010–2012. The evaluation used\nthe ClueWeb09 web crawl,67 which was gathered by Carnegie Mellon University in 2009. The\ncomplete corpus contains approximately one billion web pages in 10 different languages, totaling\n5 TB compressed (25 TB uncompressed). Due to the computational requirements of working\nwith such large datasets, the organizers offered participants two conditions: retrieval over the\nentire English portion of the corpus (503.9M web pages), or just over a subset comprising\n50.2M web pages, referred to as ClueWeb09b. For expediency, most researchers, even today,\nreport experimental results only over the ClueWeb09b subset.\n• Topics and relevance judgments from the Web Tracks at TREC 2013 and TREC 2014. Typically,\nresearchers use the ClueWeb12-B13 web crawl, which is a subset comprising 52.3M web\npages taken from the full ClueWeb12 web crawl, which contains 733M web pages (5.54 TB\ncompressed, 27.3 TB uncompressed).68 This corpus was also gathered by Carnegie Mellon\nUniversity, in 2012, as an update of ClueWeb09. Unlike ClueWeb09, ClueWeb12 only contains\nweb pages in English.\nUnfortunately, there is no standard agreed-upon evaluation methodology (for example, training/test\nsplits) for working with these test collections, and thus results reported in research papers are\nfrequently not comparable (this issue applies to many other TREC collections as well). Additionally,\nunjudged documents are a concern, particularly with the ClueWeb collections, because the collection\nis large relative to the amount of assessment effort that was devoted to evaluating the judgment\npools. Furthermore, due to the barrier of entry in working with large collections, there were fewer\nparticipating teams and less diversity in the retrieval techniques deployed in the run submissions.\nWe end this discussion with a caution, that as with any data for supervised machine learning, test\ncollections can be abused and there is the ever-present danger of overﬁtting. When interpreting\nevaluation results, it is important to examine the evaluation methodology closely—particularly issues\nrelated to training/test splits and how effectiveness metrics are aggregated (e.g., if averaging is\nperformed over topics from multiple years).\nFor these reasons, results from the actual evaluation (i.e., participation in that year’s TREC) tend\nto be more “credible” in the eyes of many researchers than “post hoc” (after-the-fact) evaluations\nusing the test collections, since there are more safeguards to prevent overﬁtting and (inadvertently)\nexploiting knowledge from the test set. Section 2.6 mentioned this issue in passing, but here we\nelaborate in more detail:\nParticipants in a TREC evaluation only get “one shot” at the test topics, and thus the test set\ncan be considered blind and unseen. Furthermore, TREC evaluations limit the total number of\nsubmissions that are allowed from each research group (typically three), which prevents researchers\nfrom evaluating many small model variations (e.g., differing only in tuning parameters), reporting\n65https://trec.nist.gov/data/wapost/\n66http://ir.dcs.gla.ac.uk/test_collections/\n67https://lemurproject.org/clueweb09/\n68https://lemurproject.org/clueweb12/\n40\nonly the best result, and neglecting to mention how many variants were examined. This is an example\nof so-called “p-hacking”; here, in essence, tuning on the test topics. More generally, it is almost\nnever reported in papers how many different techniques the researchers had tried before obtaining a\npositive result. Rosenthal [1979] called this the “ﬁle drawer problem”—techniques that “don’t work”\nare never reported and simply stuffed away metaphorically in a ﬁle drawer.\nWith repeated trials, of course, comes the dangers associated with overﬁtting, inadvertently exploiting\nknowledge about the test set, or simply “getting lucky”. Somewhat exaggerating, of course: if you\ntry a thousand things, something is likely to work on a particular set of topics. 69 Thus, post-hoc\nexperimental results that show a technique beating the top submission in a TREC evaluation should be\ntaken with a grain of salt, unless the researchers answer the question: How many attempts did it take\nto beat that top run? To be clear, we are not suggesting that researchers are intentionally “cheating” or\nengaging in any nefarious activity; quite the contrary, we believe that researchers overwhelmingly act\nin good faith all the time. Nevertheless, inadvertent biases inevitably creep into our methodological\npractices as test collections are repeatedly used.\nNote that leaderboards with private held-out test data70 mitigate, but do not fundamentally solve this\nissue. In truth, there is “leakage” any time researchers evaluate on test data—at the very least, the\nresearchers obtain a single bit of information: Is this technique effective or not? When “hill climbing”\non a metric, this single bit of information is crucial to knowing if the research is “heading in the\nright direction”. However, accumulated over successive trials, this is, in effect, training on the test\ndata. One saving grace with most leaderboards, however, is that they keep track of the number of\nsubmissions by each team. For more discussion of these issues, speciﬁcally in the context of the MS\nMARCO leaderboards, we refer the reader to Craswell et al. [2021a].\nThere isn’t a perfect solution to these issues, because using a test collection once and then throwing\nit away is impractical. However, one common way to demonstrate the generality of a proposed\ninnovation is to illustrate its effectiveness on multiple test collections. If a model is applied in a\nmethodologically consistent manner across multiple test collections (e.g., the same parameters, or at\nleast the same way of tuning parameters without introducing any collection-speciﬁc “tricks”), the\nresults might be considered more credible.\n2.8 Keyword Search\nAlthough there are active explorations of alternatives (the entirety of Section 5 is devoted to this topic),\nmost current applications of transformers for text ranking rely on keyword search in a multi-stage\nranking architecture, which is the focus of Section 3 and Section 4. In this context, keyword search\nprovides candidate generation, also called initial retrieval or ﬁrst-stage retrieval. The results are then\nreranked by transformer-based models. Given the importance of keyword search in this context, we\noffer some general remarks to help the reader understand the role it plays in text ranking.\nBy keyword search or keyword querying, we mean a large class of techniques that rely on exact term\nmatching to compute relevance scores between queries and texts from a corpus, nearly always with\nan inverted index (sometimes called inverted ﬁles or inverted lists); see Zobel and Moffat [2006]\nfor an overview. This is frequently accomplished with bag-of-words queries, which refers to the\nfact that evidence (i.e., the relevance score) from each query term is considered independently. A\nbag-of-words scoring function can be cast into the form of Equation (1) in Section 1.2, or alternatively,\nas the inner product between two sparse vectors (where the vocabulary forms the dimension of the\nvector). However, keyword search does not necessarily imply bag-of-words queries, as there is a rich\nbody of literature in information retrieval on so-called “structured queries” that attempt to capture\nrelationships between query terms—for example, query terms that co-occur in a window or are\ncontiguous (i.e., n-grams) [Metzler and Croft, 2004, 2005].\nNevertheless, one popular choice for keyword search today is bag-of-words queries with BM25\nscoring (see Section 1.2),71 but not all BM25 rankings are equivalent. In fact, there are many examples\nof putative BM25 rankings that differ quite a bit in effectiveness. One prominent example appears on\nthe leaderboard of the MS MARCO passage ranking task: a BM25 ranking produced by the Anserini\n69https://xkcd.com/882/\n70And even those based on submitting code, for example, in a Docker image.\n71However, just to add to the confusion, BM25 doesn’t necessarily imply bag-of-words queries, as there are\nextensions of BM25 to phrase queries, for example, Wang et al. [2011]\n41\nsystem [Yang et al., 2017, 2018] scores 0.186 in terms of MRR@10, but the Microsoft BM25 baseline\nscores two points lower at 0.165.\nNon-trivial differences in “BM25 rankings” have been observed by different researchers in multiple\nstudies [Trotman et al., 2014, Mühleisen et al., 2014, Kamphuis et al., 2020]. There are a number\nof reasons why different implementations of BM25 yield different rankings and achieve different\nlevels of effectiveness. First, BM25 should be characterized as a family of related scoring functions:\nBeyond the original formulation by Robertson et al. [1994], many researchers have introduced\nvariants, as studied by Trotman et al. [2014], Mühleisen et al. [2014], Kamphuis et al. [2020]. Thus,\nwhen researchers refer to BM25, it is often not clear which variant they mean. Second, document\npreprocessing—which includes document cleaning techniques, stopwords lists, tokenizers, and\nstemmers—all have measurable impact on effectiveness. This is particularly the case with web search,\nwhere techniques for removing HTML tags, JavaScript, and boilerplate make a big difference [Roy\net al., 2018]. The additional challenge is that document cleaning includes many details that are\ndifﬁcult to document in a traditional publication, making replicability difﬁcult without access to\nsource code. See Lin et al. [2020a] for an effort to tackle this challenge via a common interchange\nformat for index structures. Finally, BM25 (like most ranking functions) has free parameters that\naffect scoring behavior, and researchers often neglect to properly document these settings.\nAll of these issues contribute to differences in “BM25”, but previous studies have generally found\nthat the differences are not statistically signiﬁcant. Nevertheless, in the context of text ranking with\ntransformers, since the BM25 rankings are used as input for further reranking, prudent evaluation\nmethodology dictates that researchers carefully control for these differences, for example with careful\nablation studies.\nIn addition to bag-of-words keyword search, it is also widely accepted practice in research papers\nto present ranking results with query expansion using pseudo-relevance feedback as an additional\nbaseline. As discussed in Section 1.2.2, query expansion represents one main strategy for tackling\nthe vocabulary mismatch problem, to bring representations of queries and texts from the corpus\ninto closer alignment. Speciﬁcally, pseudo-relevance feedback is a widely studied technique that\nhas been shown to improve retrieval effectiveness on average; this is a robust ﬁnding supported\nby decades of empirical evidence. Query expansion using the RM3 pseudo-relevance feedback\ntechnique [Abdul-Jaleel et al., 2004], on top of an initial ranked list of documents scored by BM25,\nis a popular choice (usually denoted as BM25 + RM3) [Lin, 2018, Yang et al., 2019b].\nTo summarize, it is common practice to compare neural ranking models against both a bag-of-words\nbaseline and a query expansion technique. Since most neural ranking models today (all of those\ndiscussed in Section 3) act as rerankers over a list of candidates, these two baselines also serve as the\nstandard candidate generation approaches. In this way, we are able to isolate the contributions of the\nneural ranking models.\nA related issue worth discussing is the methodologically poor practice of comparisons to low baselines.\nIn a typical research paper, researchers might claim innovations based on beating some baseline\nwith a novel ranking model or approach. Such claims, however, need to be carefully veriﬁed by\nconsidering the quality of the baseline, in that it is quite easy to demonstrate improvements over low\nor poor quality baselines. This observation was made by Armstrong et al. [2009], who conducted\na meta-analysis of research papers between 1998 and 2008 from major IR research venues that\nreported results on a diverse range of TREC test collections. Writing over a decade ago in 2009, they\nconcluded: “There is, in short, no evidence that ad-hoc retrieval technology has improved during the\npast decade or more”. The authors attributed much of the blame to the “selection of weak baselines\nthat can create an illusion of incremental improvement” and “insufﬁcient comparison with previous\nresults”. On the eve of the BERT revolution, Yang et al. [2019b] conducted a similar meta-analysis\nand showed that pre-BERT neural ranking models were not any more effective than non-neural\nranking techniques, at least with limited amounts of training data; but see a follow-up by Lin [2019]\ndiscussing BERT-based models. Nevertheless, the important takeaway message remains: when\nassessing the effectiveness of a proposed ranking model, it is necessary to also assess the quality of\nthe comparison conditions, as it is always easy to beat a poor model.\nThere are, of course, numerous algorithmic and engineering details to building high-performance and\nscalable keyword search engines. However, for the most part, readers of this survey—researchers and\npractitioners interested in text ranking with transformers—can treat keyword search as a “black box”\nusing a number of open-source systems. From this perspective, keyword search is a mature technology\n42\nthat can be treated as reliable infrastructure, or in modern “cloud terms”, as a service.72 It is safe to\nassume that this infrastructure can robustly deliver high query throughput at low query latency on\narbitrarily large text collections; tens of milliseconds is typical, even for web-scale collections. As\nwe’ll see in Section 3.5, the inference latency of BERT and transformer models form the performance\nbottleneck in current reranking architectures; candidate generation is very fast in comparison.\nThere are many choices for keyword search. Academic IR researchers have a long history of building\nand sharing search systems, dating back to Cornell’s SMART system [Buckley, 1985] from the\nmid 1980s. Over the years, many open-source search engines have been built to aid in research, for\nexample, to showcase new ranking models, query evaluation algorithms, or index organizations. An\nincomplete list, past and present, includes (in an arbitrary order) Lemur/Indri [Metzler and Croft,\n2004, Metzler et al., 2004], Galago [Cartright et al., 2012], Terrier [Ounis et al., 2006, Macdonald\net al., 2012], ATIRE [Trotman et al., 2012], Ivory [Lin et al., 2009], JASS [Lin and Trotman, 2015],\nJASSv2 [Trotman and Crane, 2019], MG4J [Boldi and Vigna, 2005], Wumpus, and Zettair.73\nToday, only a few organizations—mostly commercial web search engines such as Google and\nBing—deploy their own custom infrastructure for search. For most other organizations building\nand deploying search applications—in other words, practitioners of information retrieval—the open-\nsource Apache Lucene search library74 has emerged as thede facto standard solution, usually via either\nOpenSearch,75 Elasticsearch,76 or Apache Solr,77 which are popular search platforms that use Lucene\nat their cores. Lucene powers search in production deployments at numerous companies, including\nTwitter, Bloomberg, Netﬂix, Comcast, Disney, Reddit, Wikipedia, and many more. Over the past\nfew years, there has been a resurgence of interest in using Lucene for academic research [Azzopardi\net al., 2017b,a], to take advantage of its broad deployment base and “production-grade” features; one\nexample is the Anserini toolkit [Yang et al., 2017, 2018].\n2.9 Notes on Parlance\nWe conclude this section with some discussion of terminology used throughout this survey, where\nwe have made efforts to be consistent in usage. As search is the most prominent instance of text\nranking, our parlance is unsurprisingly dominated by information retrieval. However, since IR has\na long and rich history stretching back well over half a century, parlance has evolved over time,\ncreating inconsistencies and confusion, even among IR researchers. These issues are compounded by\nconceptual overlap with neighboring sub-disciplines of computer science such as natural language\nprocessing or data mining, which sometimes use different terms to refer to the same concept or use a\nterm in a different technical sense.\nTo start, IR researchers tend to favor the term “document collection” or simply “collection” over\n“corpus” (plural: corpora), which is more commonly used by NLP researchers. We use these terms\ninterchangeably to refer to the “thing” containing the texts to be ranked.\nIn the academic literature (both in IR and across other sub-disciplines of computer science), the\nmeaning of the term “document” is overloaded: In one sense, it refers to the units of texts in the\nraw corpus. For example, a news article from the Washington Post, a web page, a journal article,\na PowerPoint presentation, an email, etc.—these would all be considered documents. However,\n“documents” can also refer generically to the “atomic” unit of ranking (or equivalently, the unit of\nretrieval). For example, if Wikipedia articles are segmented into paragraphs for the purposes of\nranking, each paragraph might be referred to as a document. This may appear odd and may be a\nsource of confusion as a researcher might continue to discuss document ranking, even though the\ndocuments to be ranked are actually paragraphs.\nIn other cases, document ranking is explicitly distinguished from passage ranking—for example,\nthere are techniques that retrieve documents from an inverted index (documents form the unit of\nretrieval), segment those documents into passages, score the passages, and then accumulate the scores\nto produce a document ranking, e.g., Callan [1994]. To add to the confusion, there are also examples\n72Indeed, many of the major cloud vendors do offer search as a service.\n73http://www.seg.rmit.edu.au/zettair/\n74https://lucene.apache.org/\n75https://opensearch.org/\n76https://github.com/elastic/elasticsearch\n77https://solr.apache.org/\n43\nwhere passages form the unit of retrieval, but passage scores are aggregated to rank documents,\ne.g., Hearst and Plaunt [1993] and Lin [2009]. We attempt to avoid this confusion by using the\nterm “text ranking”, leaving the form of the text underspeciﬁed and these nuances to be recovered\nfrom context. The compromise is that text ranking may sound foreign to a reader familiar with the\nIR literature. However, text ranking more accurately describes applications in NLP, e.g., ranking\ncandidates in entity linking, as document ranking would sound especially odd in that context.\nThe information retrieval community often uses “retrieval” and “ranking” interchangeably, although\nthe latter is much more precise. They are not, technically, the same: it would be odd refer to boolean\nretrieval as ranking, since such operations are manipulations of unordered sets. In a sense, retrieval is\nmore generic, as it can be applied to situations where no ranking is involved, for example, fetching\nvalues from a key–value store. However, English lacks a verb that is more precise than to retrieve, in\nthe sense of “to produce a ranking of texts” from, say, an inverted index,78 and thus in cases where\nthere is little chance for confusion, we continue to use the verbs “retrieve” and “rank” as synonyms.\nNext, discussions about the positions of results in a ranked list can be a source of confusion, since\nrank monotonically increases but lower (numbered) ranks (hopefully) represent better results. Thus, a\nphrase like “high ranks” is ambiguous between rank numbers that are large (e.g., a document at rank\n1000) or documents that are “highly ranked” (i.e., high scores = low rank numbers = good results).\nThe opposite ambiguity occurs with the phrase “low ranks”. To avoid confusion, we refer to texts\nthat are at the “top” of the ranked list (i.e., high scores = low rank numbers = good results) and texts\nthat are near the “bottom” of the ranked list or “deep” in ranked list.\nA note about the term “performance”: Although the meaning of performance varies across different\nsub-disciplines of computer science, it is generally used to refer to measures related to speed such\nas latency, throughput, etc. However, NLP researchers tend to use performance to refer to output\nquality (e.g., prediction accuracy, perplexity, BLEU score, etc.). This can be especially confusing\nin a paper (for example, about model compression) that also discusses performance in the speed\nsense, because “better performance” is ambiguous between “faster” (e.g., lower inference latency)\nand “better” (e.g., higher prediction accuracy). In the information retrieval literature, “effectiveness”\nis used to refer to output quality,79 while “efﬁciency” is used to refer to properties such a latency,\nthroughput, etc.80 Thus, it is common to discuss effectiveness/efﬁciency tradeoffs. In this survey, our\nuse of terminology is more closely aligned with the parlance in information retrieval—that is, we\nuse effectiveness (as opposed to “performance”) as a catch-all term for output quality and we use\nefﬁciency in the speed sense.\nFinally, “reproducibility”, “replicability”, and related terms are often used in imprecise and confusing\nways. In the context of this survey, we are careful to use the relevant terms in the sense deﬁned by\nACM’s Artifact Review and Badging Policy.81 Be aware that a previous version of the policy had the\nmeaning of “reproducibility” and “replicability” swapped, which is a source of great confusion.\nWe have found the following short descriptions to be a helpful summary of the differences:\n• Repeatability: same team, same experimental setup\n• Reproducibility: different team, same experimental setup\n• Replicability: different team, different experimental setup\nFor example, if the authors of a paper have open-sourced the code to their experiments, and another\nindividual (or team) is able to obtain the results reported in their paper, we can say that the results\nhave be successfully reproduced. The deﬁnition of “same results” can be sometimes fuzzy, as it is\nfrequently difﬁcult to arrive at exactly the same evaluation ﬁgures (say, nDCG@10) as the original\npaper, especially in the context of experiments based on neural networks, due to issues such as\nrandom seed selection, the stochastic nature of the optimizer, different versions of the underlying\nsoftware toolkit, and a host of other complexities. Generally, most researchers would consider a\n78“To rank text from an inverted index” sounds very odd.\n79Although even usage by IR researchers is inconsistent; there are still plenty of IR papers that use “performance”\nto refer to output quality.\n80Note that, however, efﬁciency means something very different in the systems community or the high-\nperformance computing community.\n81https://www.acm.org/publications/policies/artifact-review-and-badging-current\n44\nresult to be reproducible as long as others were able to conﬁrm the veracity of the claims at a high\nlevel, even if the experimental results do not perfectly align.\nIf the individual (or team) was able to obtain the same results reported in a paper, but with an indepen-\ndent implementation, then we say that the ﬁndings are replicable. Here though, the deﬁnition of an\n“independent implementation” can be somewhat fuzzy. For example, if the original implementation\nwas built using TensorFlow and the reimplementation used PyTorch, most researchers would consider\nit a successful replication effort. But what about two different TensorFlow implementations where\nthere is far less potential variation? Would this be partway between reproduction and replication?\nThe answer isn’t clear.\nThe main point of this discussion is that while notions of reproducibility and replicability may seem\nstraightforward, there are plenty of nuance and complexities that are often swept under the rug. For\nthe interested reader, see Lin and Zhang [2020] for additional discussions of these issues.\nOkay, with the stage set and all these terminological nuances out of the way, we’re ready to dive into\ntransformers for text ranking!\n45\n3 Multi-Stage Architectures for Reranking\nThe simplest and most straightforward formulation of text ranking is to convert the task into a text\nclassiﬁcation problem, and then sort the texts to be ranked based on the probability that each item\nbelongs to the desired class. For information access problems, the desired class comprises texts that\nare relevant to the user’s information need (see Section 2.2), and so we can refer to this approach as\nrelevance classiﬁcation.\nMore precisely, the approach involves training a classiﬁer to estimate the probability that each text\nbelongs to the “relevant” class, and then at ranking (i.e., inference) time sort the texts by those\nestimates.82 This approach represents a direct realization of the Probability Ranking Principle ,\nwhich states that documents should be ranked in decreasing order of the estimated probability of\nrelevance with respect to the information need, ﬁrst formulated by Robertson [1977]. Attempts\nto build computational models that directly perform ranking using supervised machine-learning\ntechniques date back to the late 1980s [Fuhr, 1989]; see also Gey [1994]. Both these papers describe\nformulations and adopt terminological conventions that would be familiar to readers today.\nThe ﬁrst application of BERT to text ranking, by Nogueira and Cho [2019], used BERT in exactly\nthis manner. However, before describing this relevance classiﬁcation approach in detail, we begin\nthe section with a high-level overview of BERT (Section 3.1). Our exposition is not meant to be a\ntutorial: rather, our aim is to highlight the aspects of the model that are important for explaining its\napplications to text ranking. Devlin et al. [2019] had already shown BERT to be effective for text\nclassiﬁcation tasks, and the adaptation by Nogueira and Cho—known as monoBERT—has proven to\nbe a simple, robust, effective, and widely replicated model for text ranking. It serves as the starting\npoint for text ranking with transformers and provides a good baseline for subsequent ranking models.\nThe progression of our presentation takes the following course:\n• We present a detailed study of monoBERT, starting with the basic relevance classiﬁcation design\nproposed by Nogueira and Cho [2019] (Section 3.2.1). Then:\n– A series of contrastive and ablation experiments demonstrate monoBERT’s effectiveness\nunder different conditions, including the replacement of BERT with simple model variants\n(Section 3.2.2). This is followed by a discussion of a large body of research that investigates\nhow BERT works (Section 3.2.3).\n– The basic “recipe” of applying BERT (and other pretrained transformers) to perform a\ndownstream task is to start with a pretrained model and then ﬁne-tune it further using la-\nbeled data from the target task. This process, however, is much more nuanced: Section 3.2.4\ndiscusses many of these techniques, which are broadly applicable to transformer-based\nmodels for a wide variety of tasks.\n• The description of monoBERT introduces a key limitation of BERT for text ranking: its inability\nto handle long input sequences, and hence difﬁculty in ranking texts whose lengths exceed the\ndesigned model input (e.g., “full-length” documents such as news articles, scientiﬁc papers, and\nweb pages). Researchers have devised multiple solutions to overcome this challenge, which are\npresented in Section 3.3. Three of these approaches—Birch [Akkalyoncu Yilmaz et al., 2019b],\nBERT–MaxP [Dai and Callan, 2019b], and CEDR [MacAvaney et al., 2019a]—are roughly\ncontemporaneous and represent the “ﬁrst wave” of transformer-based neural ranking models\ndesigned to handle longer texts.\n• After presenting a number of BERT-based ranking models, we turn our attention to discuss\nthe architectural context in which these models are deployed. A simple retrieve-and-rerank\napproach can be elaborated into a multi-stage ranking architecture with reranker pipelines,\nwhich Section 3.4 covers in detail.\n• Finally, we describe a number of efforts that attempt to go beyond BERT, to build ranking\nmodels that are faster (i.e., achieve lower inference latency), are better (i.e., obtain higher\nranking effectiveness), or realize an interesting tradeoff between effectiveness and efﬁciency\n(Section 3.5). We cover ranking models that exploit knowledge distillation to train more compact\n82Note that treating relevance as a binary property is already an over-simpliﬁcation. Modeling relevance on\nan ordinal scale (e.g., as nDCG does) represents an improvement, but whether a piece of text satisﬁes an\ninformation need requires considerations from many facets; see discussion in Section 2.2.\n46\n… … … … … … … ……\n[SEP]EA\nP8\nE[CLS]\nT[CLS]\n[CLS]\nE1\nT1\nA1\nE2\nT2\nA2\nE3\nT3\nA3\nE4\nT4\n[SEP]\nE5\nT5\nB1\nE6\nT6\nB2\nE7\nT7\nE[SEP]\nT[SEP]\n[CLS]t1 t2 t3 t4 t5 t6\nEA EA EA EA EA EA EA\nP0 P1 P2 P3 P4 P5 P6\n+ + + + + + + ++ + + + + + + +TokenEmbeddingsSegmentEmbeddingsPositionEmbeddings\nt7\nEA\nP7\n++\nIllustration of BERT, showing composition of input embeddings. Redrawn from Devlin et al. (NAACL 2019)\nBy Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International(CC BY 4.0): https://creativecommons.org/licenses/by/4.0/\nFigure 4: The architecture of BERT. Input vectors comprise the element-wise summation of token\nembeddings, segment embeddings, and position embeddings. The output of BERT is a contextual\nembedding for each input token. The contextual embedding of the [CLS] token is typically taken as\nan aggregate representation of the entire sequence for classiﬁcation-based downstream tasks.\nstudent models and other transformer architectures, including ground-up redesign efforts and\nadaptations of pretrained sequence-to-sequence models.\nBy concluding this section with efforts that attempt to go “beyond BERT”, we set up a natural\ntransition to ranking based on learned dense representations, which is the focus of Section 5.\n3.1 A High-Level Overview of BERT\nAt its core, BERT (Bidirectional Encoder Representations from Transformers) [Devlin et al., 2019] is\na neural network model for generating contextual embeddings for input sequences in English, with a\nmultilingual variant (often called “mBERT”) that can process input in over 100 different languages.\nHere we focus only on the monolingual English model, but mBERT has been extensively studied as\nwell [Wu and Dredze, 2019, Pires et al., 2019, Artetxe et al., 2020].\nBERT takes as input a sequence of tokens (more speciﬁcally, input vector representations derived\nfrom those tokens, more details below) and outputs a sequence of contextual embeddings, which\nprovide context-dependent representations of the input tokens.83 This stands in contrast to context-\nindependent (i.e., static) representations, which include many of the widely adopted techniques that\ncame before such as word2vec [Mikolov et al., 2013a] or GloVe [Pennington et al., 2014].\nThe input–output behavior of BERT is illustrated in Figure 4, where the input vector representations\nare denoted as:\n[E[CLS],E1,E2,...,E [SEP]], (9)\nand the output contextual embeddings are denoted as:\n[T[CLS],T1,T2,...,T [SEP]], (10)\nafter passing through a number of transformer encoder layers. In addition to the text to be processed,\ninput to BERT typically includes two special tokens, [CLS] and [SEP], which we explain below.\nBERT can be seen as a more sophisticated model with the same aims as ELMo [Peters et al., 2018],\nfrom which BERT draws many important ideas: the goal of contextual embeddings is to capture\ncomplex characteristics of language (e.g., syntax and semantics) as well as how meanings vary\nacross linguistic contexts (e.g., polysemy). The major difference is that BERT takes advantage of\ntransformers, as opposed to ELMo’s use of LSTMs. BERT can be viewed as the “encoder half”\n83The literature alternately refers to “contextual embeddings” or “contextualized embeddings”. We adopt the\nformer in this survey.\n47\nof the full transformer architecture proposed by Vaswani et al. [2017], which was designed for\nsequence-to-sequence tasks (i.e., where both the input and output are sequences of tokens) such as\nmachine translation.\nBERT is also distinguished from GPT [Radford et al., 2018], another model from which it traces\nintellectual ancestry. If BERT can be viewed as an encoder-only transformer, GPT is the opposite: it\nrepresents a decoder-only transformer [Liu et al., 2018a], or the “decoder half” of a full sequence-\nto-sequence transformer model. GPT is pretrained to predict the next word in a sequence based on\nits past history; in contrast, BERT uses a different objective, which leads to an important distinction\ndiscussed below. BERT and GPT are often grouped together (along with a host of other models) and\nreferred to collectively as pretrained language models, although this characterization is somewhat\nmisleading because, strictly speaking, a language model in NLP provides a probability distribution\nover arbitrary sequences of text tokens; see, for example Chen and Goodman [1996]. In truth, coaxing\nsuch probabilities out of BERT require a bit of effort [Salazar et al., 2020], and transformers in\ngeneral can do much more than “traditional” language models!\nThe signiﬁcant advance that GPT and BERT represent over the original transformer formula-\ntion [Vaswani et al., 2017] is the use of self supervision in pretraining, whereas in contrast, Vaswani\net al. began with random initialization of model weights and proceeded to directly train on labeled\ndata, i.e., (input sequence, output sequence) pairs, in a supervised manner. This is an important\ndistinction, as the insight of pretraining based on self supervision is arguably the biggest game\nchanger in improving model output quality on a multitude of language processing tasks. The beauty\nof self supervision is two-fold:\n• Model optimization is no longer bound by the chains of labeled data. Self supervision means\nthat the texts provide their own “labels” (in GPT, the “label” for a sequence of tokens is the\nnext token that appears in the sequence), and that loss can be computed from the sequence\nitself (without needing any other external annotations). Since labeled data derive ultimately\nfrom human effort, removing the need for labels greatly expands the amount of data that can be\nfed to models for pretraining. Often, computing power and available data instead become the\nbottleneck [Kaplan et al., 2020].\n• Models optimized based on one or more self-supervised objectives, without reference to any\nspeciﬁc task, provide good starting points for further ﬁne-tuning with task-speciﬁc labeled\ndata. This led to the “ﬁrst pretrain, then ﬁne-tune” recipe of working with BERT and related\nmodels, as introduced in Section 1. The details of this ﬁne-tuning process are task speciﬁc\nbut experiments have shown that a modest amount of labeled data is sufﬁcient to achieve a\nhigh level of effectiveness. Thus, the same pretrained model can serve as the starting point for\nperforming multiple downstream tasks after appropriate ﬁne-tuning.84\nIn terms of combining the two crucial ingredients of transformers and self supervision, GPT predated\nBERT. However, they operationalize the insight in different ways. GPT uses a traditional language\nmodeling objective: given a corpus of tokens U= {u1,u2,...,u n}, the objective is to maximize the\nfollowing likelihood:\nL(U) =\n∑\ni\nlog P(ui|ui−k,...,u i−1; Θ) (11)\nwhere kis the context window size and the conditional probability is modeled by a transformer with\nparameters Θ.\nIn contrast, BERT introduced the so-called “masked language model” (MLM) pretraining objective,\nwhich is inspired by the Cloze task [Taylor, 1953], dating from over half a century ago. MLM is a\nfancy name for a fairly simple idea, not much different from peek-a-boo games that adults play with\ninfants and toddlers: during pretraining, we randomly “cover up” (more formally, “mask”) a token\nfrom the input sequence and ask the model to “guess” (i.e., predict) it, training with cross entropy\nloss.85 The MLM objective explains the “B” in BERT, which stands for bidirectional: the model\nis able to use both a masked token’s left and right contexts (preceding and succeeding contexts) to\nmake predictions. In contrast, since GPT uses a language modeling objective, it is only able to\n84With adaptors [Houlsby et al., 2019], it is possible to greatly reduce the number of parameters required to\nﬁne-tune the same “base” transformer for many different tasks.\n85The actual procedure is a bit more complicated, but we refer the reader to the original paper for details.\n48\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nA1\nE2\nU2\nA2\nE3\nU3\nA3\nEn-2\nUn-2\nAn-2\nEn-1\nUn-1\nAn-1\nEn\nUn\nAn\nE[SEP]\nT[SEP]\n[SEP]\nClass Label\n…\n…\n… … … … … … … ……\nIllustration of BERT for single-sentence classification tasks. Redrawn from Devlin et al. (NAACL 2019)\nBy Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International(CC BY 4.0): https://creativecommons.org/licenses/by/4.0/\nSentence\n…\n(a) Single-Input Classiﬁcation\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nA1\nEn\nUn\nAn\nE[SEP1]\nT[SEP1]\n[SEP]\nF1\nV1\nB1\nFm\nVm\nBm\nE[SEP2]\nT[SEP2]\n[SEP]\nClass Label\n…\n…\n…\n…\n… … … … … … … ……\nIllustration of BERT for two-sentence classification tasks. Redrawn from Devlin et al. (NAACL 2019)\nBy Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International(CC BY 4.0): https://creativecommons.org/licenses/by/4.0/\nSentence 1Sentence 2\n… … (b) Two-Input Classiﬁcation\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nA1\nE2\nU2\nA2\nE3\nU3\nA3\nEn-2\nUn-2\nAn-2\nEn-1\nUn-1\nAn-1\nEn\nUn\nAn\nE[SEP]\nT[SEP]\n[SEP]\nB-PERO O O O O\n…\n…\n… … … … … … … ……\nIllustration of BERT for single-sentence sequence labeling tasks. Redrawn from Devlin et al. (NAACL 2019)\nBy Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International(CC BY 4.0): https://creativecommons.org/licenses/by/4.0/\nSentence\n…\n(c) Single-Input Token Labeling\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nA1\nEn\nUn\nAn\nE[SEP1]\nT[SEP1]\n[SEP]\nF1\nV1\nB1\nFm\nVm\nBm\nE[SEP2]\nT[SEP2]\n[SEP]\nStart/End Span\n…\n…\n…\n…\n… … … … … … … ……\nIllustration of BERT for two-sentence sequence labeling tasks. Redrawn from Devlin et al. (NAACL 2019)\nBy Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International(CC BY 4.0): https://creativecommons.org/licenses/by/4.0/\nQuestionCandidate\n… … (d) Two-Input Token Labeling\nFigure 5: Illustration of how BERT is used for different NLP tasks. The inputs are typically, but not\nalways, sentences.\nuse preceding tokens (i.e., the left context in a language written from left to right; formally, this is\ncalled “autoregressive”). Empirically, bidirectional modeling turns out to make a big difference—as\ndemonstrated, for example, by higher effectiveness on the popular GLUE benchmark.\nWhile the MLM objective was an invention of BERT, the idea of pretraining has a long history.\nULMFiT (Universal Language Model Fine-tuning) [Howard and Ruder, 2018] likely deserves the\ncredit for popularizing the idea of pretraining using language modeling objectives and then ﬁne-tuning\non task-speciﬁc data—the same procedure that has become universal today—but the application of\npretraining in NLP can be attributed to Dai and Le [2015]. Tracing the intellectual origins of this idea\neven back further, the original inspiration comes from the computer vision community, dating back at\nleast a decade [Erhan et al., 2009].\nInput sequences to BERT are usually tokenized with the WordPiece tokenizer [Wu et al., 2016],\nalthough BPE [Sennrich et al., 2016] is a common alternative, used in GPT as well as RoBERTa [Liu\net al., 2019c]. These tokenizers have the aim of reducing the vocabulary space by splitting words\ninto “subwords”, usually in an unsupervised manner. For example, with the WordPiece vocabulary\nused by BERT,86 “scrolling” becomes “scroll” + “##ing”. The convention of prepending two hashes\n(##) to a subword indicates that it is “connected” to the previous subword (i.e., in a language usually\nwritten with spaces, there is no space between the current subword and the previous one).\nFor the most part, any correspondence between “wordpieces” and linguistically meaningful units\nshould be considered accidental. For example, “walking” and “talking” arenot split into subwords,\nand “biking” is split into “bi” + “##king”, which obviously do not correspond to morphemes. Even\nmore extreme examples are “biostatistics” (“bio” + “##sta” + “##tist” + “##ics”) and “adversarial”\n(“ad”, “##vers”, “##aria”, “##l”). Nevertheless, the main advantage of WordPiece tokenization (and\nrelated methods) is that a relatively small vocabulary (e.g., 30,000 wordpieces) is sufﬁcient to model\nlarge, naturally-occurring corpora that may have millions of unique tokens (based on a simple method\nlike tokenization by whitespace).\n86Speciﬁcally, bert-base-cased.\n49\nWhile BERT at its core converts a sequence of input embeddings into a sequence of corresponding\ncontextual embeddings, in practice it is primarily applied to four types of tasks (see Figure 5):\n• Single-input classiﬁcation tasks, for example, sentiment analysis on a single segment of text.\nBERT can also be used for regression, but we have decided to focus on classiﬁcation to be\nconsistent with the terminology used in the original paper.\n• Two-input classiﬁcation tasks, for example, detecting if two sentences are paraphrases. In\nprinciple, regression is possible here also.\n• Single-input token labeling tasks, for example, named-entity recognition. For these tasks, each\ntoken in the input is assigned a label, as opposed to single-input classiﬁcation, where the label\nis assigned to the entire sequence.\n• Two-input token labeling tasks, e.g., question answering (or more precisely, machine reading\ncomprehension), formulated as the task of labeling the begin and end positions of the answer\nspan in a candidate text (typically, the second input) given a question (typically, the ﬁrst input).\nThe ﬁrst token of every input sequence to BERT is a special token called [CLS]; the ﬁnal representa-\ntion of this special token is typically used for classiﬁcation tasks. The [CLS] token is followed by the\ninput or inputs: these are typically, but not always, sentences—indeed, as we shall see later, the inputs\ncomprise candidate texts to be ranked, which are usually longer than individual sentences. For tasks\ninvolving a single input, another special delimiter token [SEP] is appended to the end of the input\nsequence. For tasks involving two inputs, both are packed together into a single contiguous sequence\nof tokens separated by the [SEP] token, with another [SEP] token appended to the end. For token\nlabeling tasks over single inputs (e.g., named-entity recognition), the contextual embedding of the\nﬁrst subword is typically used to predict the correct label that should be assigned to the token (e.g.,\nin a standard BIO tagging scheme). Question answering or machine reading comprehension (more\ngenerically, token labeling tasks involving two inputs) is treated in a conceptually similar manner,\nwhere the model attempts to label the beginning and end positions of the answer span.\nTo help the model understand the relationship between different segments of text (in the two-input\ncase), BERT is also pretrained with a “next sentence prediction” (NSP) task, where the model learns\nsegment embeddings, a kind of indicator used to differentiate the two inputs. During pretraining,\nafter choosing a sentence from the corpus (segment A), half of the time the actual next sentence from\nthe corpus is selected for inclusion in the training instance (as segment B), while the other half of the\ntime a random sentence from the corpus is chosen instead. The NSP task is to predict whether the\nsecond sentence indeed follows the ﬁrst. Devlin et al. [2019] hypothesized that NSP pretraining is\nimportant for downstream tasks, especially those that take two inputs. However, subsequent work\nby Liu et al. [2019c] questioned the necessity of NSP; in fact, on a wide range of NLP tasks, they\nobserved no effectiveness degradation in models that lacked such pretraining.\nPulling everything together, the input representation to BERT for each token comprises three compo-\nnents, shown at the bottom of Figure 4:\n• the learned token embedding of the token from the WordPiece tokenizer [Wu et al., 2016] (i.e.,\nlookup from a dictionary);\n• the segment embedding, which is a learned embedding indicating whether the token belongs\nto the ﬁrst input (A) or the second input (B) in tasks involve two inputs (denoted EA and EB)\nin Figure 4;\n• the position embedding, which is a learned embedding capturing the position of the token in a\nsequence, allowing BERT to reason about the linear sequence of tokens (see Section 3.2 for\nmore details).\nThe ﬁnal input representation to BERT for each token comprises the element-wise summation of its\ntoken embedding, segment embedding, and position embedding. It is worth emphasizing that the\nthree embedding components are summed, not assembled via vector concatenation (this is a frequent\npoint of confusion).\nThe representations comprising the input sequence to BERT are passed through a stack of transformer\nencoder layers to produce the output contextual embeddings. The number of layers, the hidden\ndimension size, and the number of attention heads are hyperparameters in the model architecture.\n50\nSize Layers Hidden Size Attention Heads Parameters\nTiny 2 128 2 4M\nMini 4 256 4 11M\nSmall 4 512 4 29M\nMedium 8 512 8 42M\nBase 12 768 12 110M\nLarge 24 1024 16 340M\nTable 4: The hyperparameter settings of various pretrained BERT conﬁgurations. Devlin et al. [2019]\npresented BERTBase and BERTLarge, the two most commonly used conﬁgurations today; other model\nsizes by Turc et al. [2019] support explorations in effectiveness/efﬁciency tradeoffs.\nHowever, there are a number of “standard conﬁgurations”. While the original paper [Devlin et al.,\n2019] presented only the BERTBase and BERTLarge conﬁgurations, with 12 and 24 transformer\nencoder layers, respectively, in later work Turc et al. [2019] pretrained a greater variety of model\nsizes with the help of knowledge distillation; these are all shown in Table 4. In general, size correlates\nwith effectiveness in downstream tasks, and thus these conﬁgurations are useful for exploring\neffectiveness/efﬁciency tradeoffs (more in Section 3.5.1).\nWe conclude our high-level discussion of BERT by noting that its popularity is in no small part\ndue to wise decisions by the authors (and approval by Google) to not only open source the model\nimplementation, but also publicly release pretrained models (which are quite computationally ex-\npensive to pretrain from scratch). This led to rapid reproduction and replication of the impressive\nresults reported in the original paper and provided the community with a reference implementation\nto build on. Today, the Transformers library 87 by Hugging Face [Wolf et al., 2020] has emerged\nas the de facto standard implementation of BERT as well as many transformer models, supporting\nboth PyTorch [Paszke et al., 2019] and TensorFlow [Abadi et al., 2016], the two most popular deep\nlearning libraries today.\nWhile open source (sharing code) and open science (sharing data and models) have become the norms\nin recent years, as noted by Lin [2019], the decision to share BERT wasn’t necessarily a given. For\nexample, Google could have elected not to share the source code or the pretrained models. There are\nmany examples of previous Google innovation that were shared in academic papers only, without a\ncorresponding open-source code release; MapReduce [Dean and Ghemawat, 2004] and the Google\nFile System [Ghemawat et al., 2003] are two examples that immediately come to mind, although\nadmittedly there are a number of complex considerations that factor into the binary decision to release\ncode or not. In cases where descriptions of innovations in papers were not accompanied by source\ncode, the broader community has needed to build its own open-source implementations from scratch\n(Hadoop in the case of MapReduce and the Google File System). This has generally impeded overall\nprogress in the ﬁeld because it required the community to rediscover many “tricks” and details from\nscratch that may not have been clear or included in the original paper. The community is fortunate that\nthings turned out the way they did, and Google should be given credit for its openness. Ultimately,\nthis led to an explosion of innovation in nearly all aspect of natural language processing, including\napplications to text ranking.\n3.2 Simple Relevance Classiﬁcation: monoBERT\nThe task of relevance classiﬁcation is to estimate a score si quantifying how relevant a candidate text\ndi is to a query q, which we denote as:\nP(Relevant = 1|di,q). (12)\nBefore describing the details of how BERT is adapted for this task, let us ﬁrst address the obvious\nquestion of where the candidate texts come from: Applying inference to every text in a corpus for\nevery user query is (obviously) impractical from the computational perspective, not only due to costly\nneural network inference but also the linear growth of query latency with respect to corpus size.\nWhile such a brute-force approach can be viable for small corpora, it quickly runs into scalability\nchallenges. It is clearly impractical to apply BERT inference to, say, a million texts for every query.88\n87https://github.com/huggingface/transformers\n88Even if you’re Google!\n51\nInverted Index\nInitial RetrievalTexts Ranked ListCandidate Texts\nQueries\nReranker\nInverted Index\nInitial RetrievalTexts Candidate Texts\nQueries\nRerankerRerankedCandidates\nReranker\nReranker… Ranked List\nFigure 6: A retrieve-and-rerank architecture, which is the simplest instantiation of a multi-stage\nranking architecture. In the candidate generation stage (also called initial retrieval or ﬁrst-stage\nretrieval), candidate texts are retrieved from the corpus, typically with bag-of-words queries against\ninverted indexes. These candidates are then reranked with a transformer-based model such as\nmonoBERT.\nAlthough architectural alternatives are being actively explored by many researchers (the topic of\nSection 5), most applications of BERT for text ranking today adopt a retrieve-and-rerank approach,\nwhich is shown in Figure 6. This represents the simplest instance of a multi-stage ranking architecture,\nwhich we detail in Section 3.4. In most designs today, candidate texts are identiﬁed from the corpus\nusing keyword search, usually with bag-of-words queries against inverted indexes (see Section 2.8).\nThis retrieval stage is called candidate generation, initial retrieval, or ﬁrst-stage retrieval, the output\nof which is a ranked list of texts, typically ordered by a scoring function based on exact term\nmatches such as BM25 (see Section 1.2). This retrieve-and-rerank approach dates back to at least the\n1960s [Simmons, 1965] and this architecture is mature and widely adopted (see Section 3.4).\nBERT inference is then applied to rerank these candidates to generate a score si for each text di in\nthe candidates list. The BERT-derived scores may or may not be further combined or aggregated\nwith other relevance signals to arrive at the ﬁnal scores used for reranking. Nogueira and Cho [2019]\nused the BERT scores directly to rerank the candidates, thus treating the candidate texts as sets, but\nother approaches take advantage of, for example, the BM25 scores from the initial retrieval (more\ndetails later). Naturally, we expect that the ranking induced by these ﬁnal scores have higher quality\nthan the scores from the initial retrieval stage (for example, as measured by the metrics discussed in\nSection 2.5). Thus, many applications of BERT to text ranking today (including everything we present\nin this section) are actually performing reranking. However, for expository clarity, we continue to\nrefer to text ranking unless the distinction between ranking and reranking is important (see additional\ndiscussion in Section 2.2).\nThis two-stage retrieve-and-rerank design also explains the major difference between Nogueira and\nCho [2019] and the classiﬁcation tasks described in the original BERT paper. Devlin et al. [2019]\nonly tackled text classiﬁcation tasks that involve comparisons of two input texts (e.g., paraphrase\ndetection), as opposed to text ranking, which requires multiple inferences. Nogueira and Cho’s\noriginal paper never gave their model a name, but Nogueira et al. [2019a] later called the model\n“monoBERT” to establish a contrast with another model they proposed called “duoBERT” (described\nin Section 3.4.1). Thus, throughout this survey we refer to this basic model as monoBERT.\n3.2.1 Basic Design of monoBERT\nThe complete monoBERT ranking model is shown in Figure 7. For the relevance classiﬁcation task,\nthe model takes as input a sequence comprised of the following:\n[[CLS],q, [SEP],di,[SEP]], (13)\nwhere qcomprises the query tokens and di comprises tokens from the candidate text to be scored.\nThis is the same input sequence conﬁguration as in Figure 5(b) for classiﬁcation tasks involving two\ninputs. Note that the query tokens are taken verbatim from the user (or from a test collection); this\ndetail will become important when we discuss the effects of feeding BERT different representations of\nthe information need (e.g., “title” vs. “description” ﬁelds in TREC topics) in Section 3.3. Additionally,\nthe segment Aembedding is added to query tokens and the segment Bembedding is added to the\ncandidate text (see Section 3.1). The special tokens [CLS] and [SEP] are exactly those deﬁned by\n52\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nq1\nE2\nU2\nq2\nE3\nU3\nq3\nE[SEP1]\nT[SEP1]\n[SEP]\nF1\nV1\nd1\nF2\nV2\nd2\nFm\nVm\ndm\nE[SEP2]\nT[SEP2]\n[SEP]\n…\n…\nquery text\ns\nEA EA EA EA EA EB EB EB EB…P0 P1 P2 P3 P4 P5 P6 Pm+4Pm+5…+ + + + + + + + ++ + + + + + + + +TokenEmbeddingsSegmentEmbeddingsPositionEmbeddings\n… … … … … … … …… …\n…\nFigure 7: The monoBERT ranking model adapts BERT for relevance classiﬁcation by taking as\ninput the query and a candidate text to be scored (surrounded by appropriate special tokens). The\ninput vector representations comprise the element-wise summation of token embeddings, segment\nembeddings, and position embeddings. The output of the BERT model is a contextual embedding for\neach input token. The ﬁnal representation of the [CLS] token is fed to a fully-connected layer that\nproduces the relevance score sof the text with respect to the query.\nBERT. The ﬁnal contextual representation of the[CLS] token is then used as input to a fully-connected\nlayer that generates the document score s(more details below).\nCollectively, this conﬁguration of the input sequence is sometimes called the “input template” and\neach component has (a greater or lesser) impact on effectiveness; we empirically examine variations\nin Section 3.2.2. This general style of organizing task inputs (query and candidate texts) into an input\ntemplate to feed to a transformer for inference is called a “cross-encoder”. This terminology becomes\nparticularly relevant in Section 5, when it is contrasted with a “bi-encoder” design where inference is\nperformed on queries and texts from the corpus independently.\nSince BERT was pretrained with sequences of tokens that have a maximum length of 512, tokens in\nan input sequence that is longer will not have a corresponding position embedding, and thus cannot\nbe meaningfully fed to the model. Without position embeddings, BERT has no way to model the\nlinear order and relative positions between tokens, and thus the model will essentially treat input\ntokens as a bag of words. In the datasets that Nogueira and Cho explored, this limitation was not an\nissue because the queries and candidate texts were shorter than the maximum length (see Figure 3 in\nSection 2.7).\nHowever, in the general case, the maximum sequence length of 512 tokens presents a challenge to\nusing BERT for ranking longer texts. We set aside this issue for now and return to discuss solutions\nin Section 3.3, noting, however, that the simplest solution is to truncate the input. Since transformers\nexhibit quadratic complexity in both time and space with respect to the input length, it is common\npractice in production deployments to truncate the input sequence to a length that is shorter than the\nmaximum length to manage latency. This might be a practical choice independent of BERT’s input\nlength limitations.\nAn important detail to note here is that the length limitation of BERT is measured in terms of the\nWordPiece tokenizer [Wu et al., 2016]. Because many words are split into subwords, the number of\nactual WordPiece tokens is always larger than the output of a simple tokenization method such as\nsplitting on whitespace. The practical consequence of this is that analyses of document lengths based\non whitespace tokenization such as Figure 3 in Section 2.7, or tokenization used by standard search\n53\nengines that include stopword removal, can only serve as a rough guide of whether a piece of text\nwill “ﬁt into” BERT.\nThe sequence of input tokens constructed from the query and a candidate text is then passed to\nBERT, which produces a contextual vector representation for each token (exactly as the model was\ndesigned to do). In monoBERT, the contextual representation of the [CLS] token (TCLS) as input to a\nsingle-layer, fully-connected neural network to obtain a probability si that the candidate di is relevant\nto q. The contextual representations of the other tokens are not used by monoBERT, but later we will\ndiscuss models that do take advantage of those representations. More formally:\nP(Relevant = 1|di,q) = si\n∆\n= softmax(T[CLS]W + b)1, (14)\nwhere T[CLS] ∈RD, Dis the model embedding dimension, W ∈RD×2 is a weight matrix, b∈R2 is\na bias term, and softmax(·)i denotes the i-th element of the softmax output. Since the last dimension\nof the matrix W is two, the softmax output has two dimensions (that is, the single-layer neural\nnetwork has two output neurons), one for each class, i.e., “relevant” and “non-relevant”.\nBERT and the classiﬁcation layer together comprise the monoBERT model. Following standard\npractices, the entire model is trained end-to-end for the relevance classiﬁcation task using cross-\nentropy loss:\nL= −\n∑\nj∈Jpos\nlog(sj) −\n∑\nj∈Jneg\nlog(1 −sj), (15)\nwhere Jpos is the set of indexes of the relevant candidates and Jneg is the set of indexes of the\nnon-relevant candidates, which is typically part of the training data. Since the loss function takes\ninto account only one candidate text at a time, this can be characterized as belonging to the family\nof pointwise learning-to-rank methods [Liu, 2009, Li, 2011]. We refer the interested reader to the\noriginal paper by Nogueira and Cho for additional details, including hyperparameter settings.\nTo be clear, “training” monoBERT starts with a pretrained BERT model, which can be downloaded\nfrom a number of sources such as the Hugging Face Transformers library [Wolf et al., 2020]. This\nis often referred to as a “model checkpoint”, which encodes a speciﬁc set of model parameters\nthat capture the results of pretraining. From this initialization, the model is then ﬁne-tuned with\ntask-speciﬁc labeled data, in our case, queries and relevance judgments. This “recipe” has emerged as\nthe standard approach of applying BERT to perform a wide range of tasks, and ranking is no exception.\nIn the reminder of this survey, we take care to be as precise as possible, distinguishing pretraining\nfrom ﬁne-tuning;89 Section 3.2.4 introduces additional wrinkles such as “further pretraining” and\n“pre–ﬁne-tuning”. However, we continue to use “training” (in a generic sense) when none of these\nterms seem particularly apt.90\nBefore presenting results, it is worthwhile to explicitly point out two deﬁciencies of this approach to\nmonoBERT training:\n• The training loss makes no reference to the metric that is used to evaluate the ﬁnal ranking\n(e.g., MAP), since each training example is considered in isolation; this is the case with all\npointwise approaches. Thus, optimizing cross-entropy for classiﬁcation may not necessarily\nimprove an end-to-end metric such as mean average precision; in the context of ranking, this\nwas ﬁrst observed by Morgan et al. [2004], who called this phenomenon “metric divergence”.\nIn practice, though, more accurate relevance classiﬁcation generally leads to improvements as\nmeasured by ranking metrics, and ranking metrics are often correlated with each other, e.g.,\nimproving MRR tends to improve MAP and vice versa.\n• Texts that BERT sees at inference (reranking) time are different from examples fed to it during\ntraining. During training, examples are taken directly from labeled examples, usually as part\nof an information retrieval test collection. In contrast, at inference time, monoBERT sees\ncandidates ranked by BM25 (for example), which may or may not correspond to how the\ntraining examples were selected to begin with, and in some cases, we have no way of knowing\nsince this detail may not have been disclosed by the creators of the test collection. Typically,\n89And indeed, according to a totally scientiﬁc poll, this is what the interwebs suggest: https://twitter.com/\nlintool/status/1375064796912087044.\n90For example, it seems odd to use “ﬁne-tuning” when referring to a model that uses a pretrained BERT as a\ncomponent, e.g., “to ﬁne-tune a CEDR model” (see Section 3.3.3).\n54\nMS MARCO Passage\nDevelopment Test\nMethod MRR@10 Recall@1k MRR@10\n(1) IRNet (best pre-BERT) 0.278 - 0.281\n(2a) BM25 (Microsoft Baseline,k= 1000) 0.167 - 0.165\n(2b) + monoBERT Large[Nogueira and Cho, 2019] 0.365 - 0.359\n(2c) + monoBERT Base[Nogueira and Cho, 2019] 0.347 - -\n(3a) BM25 (Anserini,k= 1000) 0.187 0.857 0.190\n(3b) + monoBERT Large[Nogueira et al., 2019a] 0.372 0.857 0.365\n(4a) BM25 + RM3 (Anserini,k= 1000) 0.156 0.861 -\n(4b) + monoBERT Large 0.374 0.861 -\nTable 5: The effectiveness of monoBERT on the MS MARCO passage ranking test collection.\nduring training, monoBERT is exposed to fewer candidates per query than at inference time, and\nthus the model may not accurately learn an accurate distribution of ﬁrst-stage retrieval scores\nacross a pool of candidates varying in quality. Furthermore, the model usually does not see a\nrealistic distribution of positive and negative examples. In some datasets, for example, positive\nand negative examples are balanced (i.e., equal numbers), so monoBERT is unable to accurately\nestimate the prevalence of relevant texts (i.e., build a prior) in BM25-scored texts; typically, far\nless than half of the texts from ﬁrst-stage retrieval are relevant.\nInterestingly, even without explicitly addressing these two issues, the simple training process described\nabove yields a relevance classiﬁer that works well as a ranking model in practice.91\nResults. The original paper by Nogueira and Cho [2019] evaluated monoBERT on two datasets: the\nMS MARCO passage ranking test collection and the dataset from the Complex Answer Retrieval\n(CAR) Track at TREC 2017. We focus here on results from MS MARCO, the more popular of the\ntwo datasets, shown in Table 5. In addition to MRR@10, which is the ofﬁcial metric, we also report\nrecall at cutoff 1000, which helps to quantify the upper bound effectiveness of the retrieve-and-rerank\nstrategy. That is, if ﬁrst-stage retrieval fails to return relevant passages, the reranker cannot conjure\nrelevant results out of thin air. Since we do not have access to relevance judgments for the test set, it\nis only possible to compute recall for the development set.\nThe original monoBERT results, copied from Nogueira and Cho [2019] as row (2b) in Table 5, was\nbased on reranking baseline BM25 results provided by Microsoft, row (2a), with BERTLarge. This\nis the result that in January 2019 kicked off the “BERT craze” for text ranking, as we’ve already\ndiscussed in Section 1.2. The effectiveness of IRNet in row (1), the best system right before the\nintroduction of monoBERT, is also copied from Table 1. The effectiveness of ranking withBERTBase\nis shown in row (2c), also copied from the original paper. We see that, as expected, a larger model\nyields higher effectiveness. Nogueira and Cho [2019] did not compute recall, and so the ﬁgures are\nnot available for the conditions in rows (2a)–(2c).\nNot all BM25 implementations are the same, as discussed in Section 2.8. The baseline BM25\nresults from Anserini (at k = 1000), row (3a), is nearly two points higher in terms of MRR@10\nthan the results provided by Microsoft’s BM25 baseline, row (2a). Reranking Anserini results\nusing monoBERT is shown in row (3b), taken from Nogueira et al. [2019a], a follow-up paper;\nnote that reranking does not change recall. We see that improvements to ﬁrst-stage retrieval do\ntranslate into more effective reranked results, but the magnitude of the improvement is not as large\nas the difference between Microsoft’s BM25 and Anserini’s BM25. The combination of Anserini\nBM25 + monoBERTLarge, row (3b), provides a solid baseline for comparing BERT-based reranking\nmodels. These results can be reproduced with PyGaggle, 92 which provides the current reference\nimplementation of monoBERT recommended by the model’s authors.\n91Many feature-based learning-to-rank techniques [Liu, 2009, Li, 2011] are also quite effective without explicitly\naddressing these issues, and so this behavior of BERT is perhaps not surprising.\n92http://pygaggle.ai/\n55\n1 2.5 10 530\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n# relevant query–passage training instances (thousands)\nMRR@10\nBERT-BASE\nBM25\nmonoBERTBase Effectiveness vs. Training Data Size on MS MARCO Passage\nFigure 8: The effectiveness of monoBERTBase on the development set of the MS MARCO passage\nranking test collection varying the amount of training data used to ﬁne-tune the model and reranking\nk = 1000 candidate texts provided by ﬁrst-stage retrieval using BM25. Results report means and\n95% conﬁdence intervals over ﬁve trials.\n3.2.2 Exploring monoBERT\nTo gain a better understanding of how monoBERT works, we present a series of additional experi-\nments that examine the effectiveness of the model under different contrastive and ablation settings.\nSpeciﬁcally, we investigate the following questions:\n1. How much data is needed to train an effective model?\n2. What is the effect of different candidate generation approaches?\n3. How does retrieval depth kimpact effectiveness?\n4. Do exact match scores from ﬁrst-stage retrieval contribute to overall effectiveness?\n5. How important are different components of the input template?\n6. What is the effect of swapping out BERT for another model that is a simple variant of BERT?\nWe answer each of these questions in turn, and then move on to discuss efforts that attempt to\nunderstand why the model “works” so well.\nEffects of Training Data Size. How much data do we need to train an effective monoBERT model?\nThe answer to this ﬁrst question is shown in Figure 8, with results taken from Nogueira et al. [2020].\nIn these experiments, BERTBase was ﬁne-tuned with 1K, 2.5K, and 10K positive query–passage\ninstances and an equal number of negative instances sampled from the training set of the MS\nMARCO passage ranking test collection. Effectiveness on the development set is reported in terms of\nMRR@10 with the standard setting of reranking k= 1000 candidate texts provided by Anserini’s\nBM25; note that the x-axis is in log scale. For the sampled conditions, the experiment was repeated\nﬁve times, and the plot shows the 95% conﬁdence intervals. The setting that uses all training instances\nwas only run once due to computational costs. Note that these ﬁgures come from a different set\nof experimental trials than the results reported in the previous section, and thus MRR@10 from\nﬁne-tuning with all data is slightly different from the comparable condition in Table 5. The dotted\nhorizontal black line shows the effectiveness of BM25 without any reranking.\nAs we expect, effectiveness improves as monoBERT is ﬁne-tuned with more data. Interestingly, in a\n“data poor” setting, that is, without many training examples, monoBERT actually performsworse\nthan BM25; this behavior has been noted by other researchers as well [Zhang et al., 2020g, Mokrii\n56\nTREC 2019 DL Passage\nMethod nDCG@10 MAP Recall@1k\n(3a) BM25 (Anserini,k= 1000) 0.5058 0.3013 0.7501\n(3b) + monoBERT Large 0.7383 0.5058 0.7501\n(4a) BM25 + RM3 (Anserini,k= 1000) 0.5180 0.3390 0.7998\n(4b) + monoBERT Large 0.7421 0.5291 0.7998\nTable 6: The effectiveness of monoBERT on the TREC 2019 Deep Learning Track passage ranking\ntest collection, where the row numbers are consistent with Table 5.\net al., 2021]. As a rough point of comparison, the TREC 2019 Deep Learning Track passage ranking\ntest collection comprises approximately 9K relevance judgments (both positive and negative); see\nTable 3. This suggests that monoBERT is quite “data hungry”: with 20K total training instances,\nmonoBERT barely improves upon the BM25 baseline. The log–linear increase in effectiveness as a\nfunction of data size is perhaps not surprising, and consistent with previous studies that examined the\neffects of training data size [Banko and Brill, 2001, Brants et al., 2007, Kaplan et al., 2020].\nEffects of Candidate Generation. Since monoBERT operates by reranking candidates from ﬁrst-\nstage retrieval, it makes sense to investigate its impact on end-to-end effectiveness. Here, we examine\nthe effects of query expansion using pseudo-relevance feedback, which is a widely studied technique\nfor improving retrieval effectiveness on average (see Section 2.8). The effectiveness of keyword\nretrieval using BM25 + RM3, a standard pseudo-relevance feedback baseline, is presented in row\n(4a) of Table 5, with the implementation in Anserini. We see that MRR@10 decreases with pseudo-\nrelevance feedback, although there isn’t much difference in terms of recall. Further reranking with\nBERT, shown in row (4b), yields MRR@10 that is almost the same as reranking BM25 results, shown\nin row (3b). Thus, it appears that starting with worse quality candidates in terms of MRR@10 (BM25\n+ RM3 vs. BM25), monoBERT is nevertheless able to identify relevant texts and bring them up into\ntop-ranked positions.\nWhat’s going on here? These unexpected results can be attributed directly to artifacts of the relevance\njudgments in the MS MARCO passage ranking test collection. It is well known that pseudo-relevance\nfeedback has a recall enhancing effect, since the expanded query is able to capture additional terms\nthat may appear in relevant texts. However, on average, there is only one relevant passage per query\nin the MS MARCO passage relevance judgments; we have previously referred to these as sparse\njudgments (see Section 2.7). Recall that unjudged texts are usually treated as not relevant (see\nSection 2.5), as is the case here, so a ranking technique is unlikely to receive credit for improving\nrecall. Thus, due to the sparsity of judgments, the MS MARCO passage ranking test collection\nappears to be limited in its ability to detect effectiveness improvements from pseudo-relevance\nfeedback.\nWe can better understand these effects by instead evaluating the same experimental conditions, but\nwith the TREC 2019 Deep Learning Track passage ranking test collection, which has far fewer\ntopics, but many more judged passages per topic (“dense judgments”, as described in Section 2.7).\nThese results are shown in Table 6, where the rows have been numbered in the same manner as\nTable 5. We can see that these results support our explanation above: in the absence of BERT-based\nreranking, pseudo-relevance feedback does indeed increase effectiveness, as shown by row (3a)\nvs. row (4a). In particular, recall increases by around ﬁve points. The gain in nDCG@10 is more\nmodest than the gain in MAP because, by deﬁnition, nDCG@10 is only concerned with the top 10\nhits, and the recall-enhancing effects of RM3 have less impact in improving the top of the ranked\nlist. Furthermore, an increase in the quality of the candidates does improve end-to-end effectiveness\nafter reranking, row (3b) vs. row(4b), although the magnitude of the gain is smaller than the impact\nof pseudo-relevance feedback over simple bag-of-word queries. An important takeaway here is the\nimportance of recognizing the limitations of a particular evaluation instrument (i.e., the test collection)\nand when an experiment exceeds its assessment capabilities.\nEffects of Reranking Depth. Within a reranking setup, how does monoBERT effectiveness change\nas the model is provided with more candidates? This question is answered in Figure 9, where we\nshow end-to-end effectiveness (MRR@10) of monoBERT with BM25 supplying different numbers\nof candidates to rerank. It is no surprise that end-to-end effectiveness increases as retrieval depth k\n57\n10 100 1,000 10,000 50,0000.26\n0.28\n0.3\n0.32\n0.34\n0.36\n0.38\n0.4\nNumber of Candidate Documents\nMRR@10\nmonoBERTLarge Effectiveness vs. Reranking Depth on MS MARCO Passage\nFigure 9: The effectiveness of monoBERTLarge on the development set of the MS MARCO passage\nranking test collection varying the number of candidate documents kprovided by ﬁrst-stage retrieval\nusing BM25. End-to-end effectiveness grows with reranking depth.\nincreases, although there is clearly diminishing returns: going from 1000 hits to 10000 hits increases\nMRR@10 from 0.372 to 0.377. Further increasing kto 50000 does not measurably change MRR@10\nat all (same value). Due to computation costs, experiments beyond 50000 hits were not performed.\nQuite interestingly, the effectiveness curve doesnot appear to be concave. In other words, it is not the\ncase (at least out to 50000 hits) that effectiveness decreases with more candidates beyond a certain\npoint. This behavior might be plausible because we are feeding BERT increasingly worse results,\nat least from the perspective of BM25 scores. However, it appears that BERT isnot “confused” by\nsuch texts. Furthermore, these results conﬁrm that ﬁrst-stage retrieval serves primarily to increase\ncomputational efﬁciency (i.e., discarding obviously non-relevant texts), and that there are few relevant\ntexts that have very low BM25 exact match scores.\nSince latency increases linearly with the number of candidates processed (in the absence of intra-\nquery parallelism), this ﬁnding also has important implications for real-world deployments: system\ndesigners should simply select the largest k practical given their available hardware budget and\nlatency targets. There does not appear to be any danger in considering kvalues that are “too large”\n(which would be the case if the effectiveness curve were concave, thus necessitating more nuanced\ntuning to operate at the optimal setting). In other words, the tradeoff between effectiveness and\nlatency appears to be straightforward to manage.\nEffects of Combining Exact Match Signals. Given the above results, a natural complementary\nquestion is the importance of exact match signals (e.g., BM25 scores) to end-to-end effectiveness. One\nobvious approach to combining evidence from initial BM25 retrieval scores and monoBERT scores is\nlinear interpolation, whose usage in document ranking dates back to at least the 1990s [Bartell et al.,\n1994]:\nsi\n∆\n= α·ˆsBM25 + (1 −α) ·sBERT, (16)\nwhere si is the ﬁnal document score, ˆsBM25 is the normalized BM25 score, sBERT is the monoBERT\nscore, and α∈[0..1] is a weight the indicates their relative importance. Since monoBERT scores are\nsBERT ∈[0,1], we also normalize BM25 scores to be in the same range via linear scaling:\nˆsBM25 = sBM25 −smin\nsmax −smin\n, (17)\nwhere sBM25 is the original score, ˆsBM25 is the normalized score, and smax and smin are the maximum\nand minimum scores, respectively, in the ranked list.\n58\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0.2\n0.25\n0.3\n0.35\n0.4\nα\nMRR@10\nmonoBERTLarge Effectiveness with BM25 Interpolation on MS MARCO Passage\nFigure 10: The effectiveness of monoBERTLarge on the development set of the MS MARCO passage\nranking test collection varying the interpolation weight of BM25 scores: α= 0.0 means that only the\nmonoBERT scores are used and α= 1.0 means that only the BM25 scores are used. BM25 scores do\nnot appear to improve end-to-end effectiveness using this score fusion technique.\nExperimental results are presented in Figure 10, which shows that MRR@10 monotonically decreases\nas we increase the weight placed on BM25 scores. This ﬁnding seems consistent with the reranking\ndepth analysis in Figure 9. It stands to reason that if increasing kfrom 10000 to 50000 still improves\nMRR@10 (albeit slightly), then the BM25 score has limited value, i.e., it is unlikely that the BM25\nscore has much discriminative power between those ranks. Put differently, monoBERT doesn’t appear\nto need “help” from BM25 to identify relevant texts.\nSo, do exact match scores contribute relevance signals that are not already captured by transformers?\nWe are careful to emphasize that this experiment alone does not deﬁnitively answer the question: it\nonly shows that with a simple interpolation approach, BM25 scores do not appear to provide additional\nvalue to monoBERT on the MS MARCO passage ranking task. In contrast, Birch [Akkalyoncu Yilmaz\net al., 2019b] (see Section 3.3.1) as well as experiments with CEDR [MacAvaney et al., 2019a] (see\nSection 3.3.3) both incorporate BM25 scores, and evidence on question answering tasks is fairly\nconclusive that retrieval scores are helpful in boosting end-to-end effectiveness [Yang et al., 2019c,\nYang and Seo, 2020, Karpukhin et al., 2020b, Ma et al., 2021c].\nEffects of Input Template Variations. As explained in the previous sections, the input to\nmonoBERT is comprised of three different sequences of dense vectors summed together at the\ntoken level (token, segment, and position embeddings). The sequence contains the inputs as well\nas the special tokens [CLS] and [SEP] that need to be positioned at speciﬁc locations. Together,\nthese elements deﬁne the “input template” of how queries and candidate texts are fed to BERT. How\nimportant are each of these components? Here, we investigate which parts of the input are essential\nto monoBERT’s effectiveness. Table 7 summarizes the results of these experiments.\nWe began by conﬁrming that monoBERT is actually making use of relevance signals from token\npositions to aid in ranking. If we remove the position embeddings but keep everything else in the\ninput template the same, which essentially ablates the model to relying only on a bag of words,\nMRR@10 drops nearly six points, see rows (1) vs. (2). This suggests that token positions are clearly\nan important relevance signal in monoBERT. Yet, interestingly, even without position information,\nmonoBERT remains much more effective than the BM25 baseline, which suggests that the model\nis able to extract token-level signals in a bag-of-words setting (e.g., synonym, polysemy, semantic\nrelatedness, etc.). This can be interpreted as evidence that monoBERT is performing “soft” semantic\nmatching between query terms and terms in the candidate text.\n59\nMS MARCO Passage\nDevelopment\nMethod Input Template MRR@10\n(1) BERTLarge, no modiﬁcation [CLS]q[SEP]d[SEP] 0.365\n(2) w/o positional embeddings [CLS]q[SEP]d[SEP] 0.307\n(3) w/o segment type embeddings[CLS]q[SEP]d[SEP] 0.359\n(4) swapping query and document[CLS]d[SEP]q[SEP] 0.366\n(5) No [SEP] [CLS] Query:qDocument:d 0.358\nTable 7: The effectiveness of different monoBERTLarge input template variations on the development\nset of the MS MARCO passage ranking test collection.\nFor tasks involving two inputs, we face the issue of how to “pack” the disparate inputs into a single\nsequence (i.e., the input template) to feed to BERT. The standard solution devised by Devlin et al.\n[2019] uses a combination of the [SEP] tokens and segment embeddings. The monoBERT model\ninherits this basic design, but here we investigate different techniques to accomplish the goal of\n“marking” disparate inputs so that the model can distinguish different parts of the task input.\nAs a simple ablation, we see that removing the segment embeddings has little impact, with only a\nsmall loss in MRR@10. This shows that monoBERT can distinguish query and document tokens\nusing only the separator tokens and perhaps the absolute positions of the tokens. Since most queries\nin MS MARCO have less than 20 tokens, could it be the case that monoBERT simply memorizes the\nfact that query tokens always occur near the beginning of the input sequence, effectively ignoring the\nseparator tokens? To test this hypothesis, we swapped the order in which the query and the candidate\ntext are fed to monoBERT. Since the candidate texts have a much larger variation in terms of length\nthan the queries, the queries will occur in a larger range of token positions in the input sequence, thus\nmaking it harder for monoBERT to identify query tokens based solely on their absolute positions.\nRows (1) vs. (4) show minimal difference in MRR@10 under this swapped treatment, which adds\nfurther evidence that monoBERT is indeed using separator tokens and segment type embeddings to\ndistinguish between the query and the candidate text (in the default input template).\nGiven that the [SEP] token does seem to be playing an important role in segmenting the input\nsequence to monoBERT, a natural follow-up question is whether different “delimiters” might also\nwork. As an alternative, we tried replacing [SEP] with the (literal) token “Query:” prepended to the\nquery and the token “Document:” prepended to the candidate text. This design is inspired by “text\nonly” input templates that are used in T5, described later in Section 3.5.3. The results are shown in\nrow (5) in Table 7, where we observe a drop in MRR@10. This suggests that [SEP] indeed does\nhave a special status in BERT, likely due to its extensive use in pretraining.\nClearly, the organization of the input template is important, which is an observation that has been\nnoted by other researchers as well across a range of NLP tasks [Haviv et al., 2021, Le Scao and\nRush, 2021]. Speciﬁcally for ranking, Boualili et al. [2020] suggested that BERT might beneﬁt from\nexplicit exact match cues conveyed using marker tokens. However, the authors reported absolute\nscores that do not appear to be competitive with the results reported in this section, and thus it is\nunclear if such explicit cues continue to be effective with stronger baselines. Nevertheless, it is clear\nthat the organization of the input sequence can make a big difference in terms of effectiveness (in\nranking and beyond), and there is no doubt a need for more thorough further investigations.\nEffects of Simple monoBERT variants. As discussed in the introduction of this survey, the public\nrelease of BERT set off a stampede of follow-up models, ranging from relatively minor tweaks to\nsimple architectural variants to entirely new models inspired by BERT. Of course, the distinction\nbetween a “variant” and a new model is somewhat fuzzy, but many researchers have proposed models\nthat are compatible with BERT in the sense that they can easily be “swapped in” with minimal\nchanges.93 In many cases, a BERT variant takes the same input template as monoBERT and operates\nas a relevance classiﬁer in the same way.\nOne notable BERT variant is RoBERTa [Liu et al., 2019c], which can be described as Facebook’s\nreplication study of BERT’s pretraining procedures “from scratch”, with additional explorations of\n93In some cases, when using the Hugging Face Transformer library, swapping in one of these alternative models\nis, literally, a one-line change.\n60\nMS MARCO Passage(Dev)\nMethod MRR@10\n(1) monoBERTLarge 0.372\n(2) monoRoBERTaLarge 0.365\nTable 8: The effectiveness of monoRoBERTaLarge on the development set of the MS MARCO passage\nranking test collection. The monoBERTLarge results are copied from Table 5.\nmany design choices made in Devlin et al. [2019]. The authors of RoBERTa argued that Google’s\noriginal BERT model was signiﬁcantly under-trained. By modifying several hyperparameters and\nby removing the next sentence prediction (NSP) task (see Section 3.1), RoBERTa is able to match\nor exceed the effectiveness of BERT on a variety of natural language processing tasks. Table 8\nshows the results of replacing BERTLarge with RoBERTaLarge in monoBERT, evaluated on the MS\nMARCO passage ranking test collection. These results have not been previously published, but the\nexperimental setup is the same as in Section 3.2 and the monoBERTLarge results are copied from row\n(3b) in Table 5. We see that although RoBERTa achieves higher effectiveness across a range of NLP\ntasks, these improvements do not appear to carry over to text ranking, as monoRoBERTaLarge reports\na slightly lower MRR@10. This ﬁnding suggests that information access tasks need to be examined\nindependently from the typical suite of tasks employed by NLP researchers to evaluate their models.\nBeyond RoBERTa, there is a menagerie of BERT-like models that can serve as drop-in replacements\nof BERT for text ranking, just like monoRoBERTa. As we discuss models that tackle ranking longer\ntexts in the next section (Section 3.3), in which BERT serves as a component in a larger model, these\nBERT alternatives can likewise be “swapped in” seamlessly. Because these BERT-like models were\ndeveloped at different times, the investigation of their impact on effectiveness has been mostlyad\nhoc. For example, we are not aware of a systematic study of monoX, where X spans the gamut of\nBERT replacements. Nevertheless, researchers have begun to experimentally study BERT variants\nin place of BERT “classic” for ranking tasks. We will interleave the discussion of ranking models\nand adaptations of BERT alternatives in the following sections. At a high level, these explorations\nallow researchers to potentially “ride the wave” of model advancements at a relatively small cost.\nHowever, since improvements on traditional natural language processing tasks may not translate\ninto improvements in information access tasks, the effectiveness of each BERT variant must be\nempirically validated.\nDiscussion and Analysis. Reﬂecting on the results presented above, it is quite remarkable how\nmonoBERT offers a simple yet effective solution to the text ranking problem (at least for texts\nthat ﬁt within its sequence length restrictions). The simplicity of the model has contributed greatly\nto its widespread adoption. These results have been widely replicated and can be considered\nrobust ﬁndings—for example, different authors have achieved comparable results across different\nimplementations and hyperparameter settings. Indeed, monoBERT has emerged as the baseline for\ntransformer-based approaches to text ranking, and some variant of monoBERT serves as the baseline\nfor many of the papers cited throughout this survey.\n3.2.3 Investigating How BERT Works\nWhile much work has empirically demonstrated that BERT can be an effective ranking model, it is not\nclear exactly why this is the case. As Lin [2019] remarked, it wasn’t obvious that BERT, speciﬁcally\ndesigned for NLP tasks, would “work” for text ranking; in fact, the history of IR is littered with ideas\nfrom NLP that intuitively “should work”, but never panned out, at least with the implementations of\nthe time. In this section, we present several lines of work investigating why BERT performs well for\nboth NLP tasks in general and for information access tasks in particular.\nWhat is the relationship between BERT and “pre-BERT” neural ranking models? Figure 11\ntries to highlight important architectural differences between BERT and pre-BERT neural ranking\nmodels: for convenience, we repeat the high-level designs of the pre-BERT representation-based and\ninteraction-based neural ranking models, taken from Figure 1 in Section 1.2.4. As a high-level recap,\nthere is experimental evidence suggesting that interaction-based approaches (middle) are generally\nmore effective than representation-based approaches (left) because the similarity matrix explicitly\n61\nE1\nq1\nE2\nq2\nE3\nq3\nF1\nd1\nF2\nd2\nFm\ndm\n…\ns\n(a) Representation-Based\nE1q1\nE2q2\nE3q3\nF1\nd1\nF2\nd2\nFm\ndm…\ns\n…\n…\n… (b) Interaction-Based\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nq1\nE2\nU2\nq2\nE3\nU3\nq3\nE[SEP1]\nT[SEP1]\n[SEP]\nF1\nV1\nd1\nF2\nV2\nd2\nFm\nVm\ndm\nE[SEP2]\nT[SEP2]\n[SEP]\n…\n…\ns\n… … … … … … … …… …\n… (c) monoBERT\nFigure 11: Side-by-side comparison between high-level architectures of the two main classes of\npre-BERT neural ranking models with monoBERT, where all-to-all attention at each transformer\nlayer captures interactions between and within terms from the query and the candidate text.\ncaptures exact as well as “soft” semantic matches between individual terms and sequences of terms\nin the query and the candidate text.\nIn BERT, all-to-all interactions between and within query terms and terms from the candidate text are\ncaptured by multi-headed attention at each layer in the transformer. Attention appears to serve as a\none-size-ﬁts-all approach to extracting signal from term interactions, replacing the various techniques\nused by pre-BERT interaction-based models, e.g., different pooling techniques, convolutional ﬁlters,\netc. Furthermore, it appears that monoBERT does not require any specialized neural architectural\ncomponents to model different aspects of relevance between queries and a candidate text, since each\nlayer of the transformer is homogeneous and the same model architecture is used for a variety of\nnatural language processing tasks. However, it also seems clear that ranking is further improved by\nincorporating BERT as a component to extract relevance signals that are further processed by other\nneural components, for example, PARADE (see Section 3.3.4). In other words, BERT can be used\ndirectly for ranking or as a building block in a larger model.\nWhat does BERT learn from pretraining? There has been no shortage of research that attempts\nto reveal insights about how BERT “works” in general. Typically, this is accomplished through\nvisualization techniques (for example, of attention and activation patterns), probing classiﬁers, and\nmasked word prediction. We discuss a small subset of ﬁndings in the context of NLP here and refer the\nreader to a survey by Rogers et al. [2020] for more details. Probing classiﬁers have been used in many\nstudies to determine whether something can be predicted from BERT’s internal representations. For\nexample, Tenney et al. [2019] used probes to support the claim that “BERT rediscovers the classical\nNLP pipeline” by showing that the model represents part-of-speech tagging, parsing, named-entity\nrecognition, semantic role labeling, and coreference (in that order) in an interpretable and localizable\nway. That is, internal representations encode information useful for these tasks, and some layers are\nbetter than others at producing representations that are useful for a given task. However, Elazar et al.\n[2021] used “amnesic probing” to demonstrate that such linguistic information is not necessarily used\nwhen performing a downstream task.\nOther researchers have examined BERT’s attention heads and characterized their behavior. For\nexample, Clark et al. [2019] categorized a few frequently observed patterns such as attending to\ndelimiter tokens and speciﬁc position offsets, and they were able to identify attention heads that\ncorrespond to linguistic notions (e.g., verbs attending to direct objects). Kovaleva et al. [2019]\nspeciﬁcally focused on self-attention patterns and found that a limited set of attention patterns are\nrepeated across different heads, suggesting that the model is over-parameterized. Indeed, manually\ndisabling attention in certain heads leads to effectiveness improvements in some NLP tasks [V oita\net al., 2019]. Rather than attempting to train probing classiﬁers or to look “inside” the model, others\nhave investigated BERT’s behavior via a technique called masked term prediction. Since BERT was\npretrained with the masked language model (MLM) objective, it is possible to feed the masked token\n[MASK] to the model and ask it to predict the masked term, as a way to probe what the model has\nlearned. Ettinger [2020] found that BERT performs well on some tasks like associating a term with\nits hypernym (broader category) but performs much worse on others like handling negations. For\nexample, BERT’s top three predictions remained the same when presented with both “A hammer is\nan [MASK]” and “A hammer is not an[MASK]”.\n62\nWhile these studies begin to shed light on the inner workings of BERT, they do not speciﬁcally\nexamine information access tasks, so they offer limited insight on how notions of relevance are\ncaptured by BERT.\nHow does BERT perform relevance matching? Information retrieval researchers have attempted\nto speciﬁcally investigate relevance matching by BERT in ranking tasks [Padigela et al., 2019, Qiao\net al., 2019, Câmara and Hauff, 2020, Zhan et al., 2020b, Formal et al., 2021b, MacAvaney et al.,\n2020b]. For example, Qiao et al. [2019] argued that BERT should be understood as an “interaction-\nbased sequence-to-sequence matching model” that prefers semantic matches between paraphrase\ntokens. Furthermore, the authors also found that BERT’s relevance matching behavior differs from\nneural rankers that are trained from user clicks in query logs. Zhan et al. [2020b] attributed the\nprocesses of building semantic representations and capturing interaction signals to different layers,\narguing that the lower layers of BERT focus primarily on extracting representations, while the higher\nlayers capture interaction signals to ultimately predict relevance.\nCâmara and Hauff [2020] created diagnostic datasets to test whether BERT satisﬁes a range of IR\naxioms [Fang et al., 2004, 2011] describing how retrieval scores should change based on occurrences\nof query terms, the discriminativeness (idf) of matched terms, the number of non-query terms in a\ndocument, semantic matches against query terms, the proximity of query terms, etc. Using these\ndiagnostic datasets, they found that a distilled BERT model [Sanh et al., 2019] satisﬁes the axioms\nmuch less frequently than Indri’s query likelihood model despite being much more effective, leading\nto the conclusion that the axioms alone cannot explain BERT’s effectiveness. Similarly, in the context\nof the ColBERT ranking model (described later in Section 5.5.2), Formal et al. [2021b] investigated\nwhether BERT has a notion of term importance related to idf. They found that masking low idf\nterms inﬂuences the ranking less than masking high idf terms, but the importance of a term does not\nnecessarily correlate with its idf.\nFurthering this thread of research on creating “diagnostics” to investigate ranking behavior, Mac-\nAvaney et al. [2020b] proposed using “textual manipulation tests” and “dataset transfer tests” in\naddition to the diagnostic tests used in earlier work. They applied these tests to monoBERT as well as\nto other models like T5 (described later in Section 3.5.3). The authors found that monoBERT is better\nthan BM25 at estimating relevance when term frequency is held constant, which supports the ﬁnding\nfrom Câmara and Hauff [2020] that monoBERT does not satisfy term frequency axioms. Using\ntextual manipulation tests in which existing documents are modiﬁed, MacAvaney et al. [2020b] found\nthat shufﬂing the order of words within a sentence or across sentences has a large negative effect,\nwhile shufﬂing the order of sentences within a document has a modest negative effect. However,\nshufﬂing only prepositions had little effect. Surprisingly, in their experiments, monoBERT increases\nthe score of texts when non-relevant sentences are added to the end but decreases the score when\nrelevant terms from doc2query–T5 (described later in Section 4.3) are added to the end. Using dataset\ntransfer tests, which pair together two versions of the same document, MacAvaney et al. [2020b]\nfound that monoBERT scores informal text slightly higher than formal text and ﬂuent text slightly\nhigher than text written by non-native speakers.\nWhile progress has been made in understanding exactly how BERT “works” for text ranking, the\nexplanations remain incomplete, to some extent inconsistent, and largely unsatisfying. BERT shows\nevidence of combining elements from both representation-based models as well as interaction-based\nmodels. Furthermore, experimental results from input template variations above show that monoBERT\nleverages exact match, “soft” semantic match, as well as term position information. How exactly\nthese different components combine—for different types of queries, across different corpora, and\nunder different settings, etc.—remains an open question.\n3.2.4 Nuances of Training BERT\nWith transformers, the “pretrain then ﬁne-tune” recipe has emerged as the standard approach of\napplying BERT to speciﬁc downstream tasks such as classiﬁcation, sequence labeling, and ranking.\nTypically, we start with a “base” pretrained transformer model such as theBERTBase and BERTLarge\ncheckpoints directly downloadable from Google or the Hugging Face Transformers library. This\nmodel is then ﬁne-tuned on task-speciﬁc labeled data drawn from the same distribution as the target\ntask. For ranking, the model might be ﬁne-tuned using a test collection comprised of queries and\nrelevance judgments under a standard training, development (validation), and test split.\n63\nHowever, there are many variations of this generic “recipe”, for example:\n• Additional unsupervised pretraining.\n• Fine-tuning on one or more out-of-domain (or more generally, out-of-distribution) labeled data\nwith respect to the target task.\n• Fine-tuning on synthetically generated labeled data or data gathered via distant supervision\ntechniques (also called weak supervision).\n• Speciﬁc ﬁne-tuning strategies such as curriculum learning.\nAn important distinction among these techniques is the dichotomy between those that take advantage\nof self supervision and those that require task-speciﬁc labeled data. We describe these two approaches\nseparately below, but for the most part our discussions occur at a high level because the speciﬁc\ntechniques can be applied in different contexts and on different models. Thus, it makes more sense\nto introduce the general ideas here, and then interweave experimental results with the contexts or\nmodels they are applied to (throughout this section). This is the narrative strategy we have adopted,\nbut to introduce yet another layer of complexity, these techniques can be further interwoven with\nknowledge distillation, which is presented later in Section 3.5.1.\nAdditional Unsupervised Pretraining. The checkpoints of publicly downloadable models such as\nBERTBase and BERTLarge are pretrained on “general domain” corpora: for example, BERT uses the\nBooksCorpus [Zhu et al., 2015] as well as Wikipedia. While there may be some overlap between\nthese corpora and the target corpus over which ranking is performed, they may nevertheless differ in\nterms of vocabulary distribution, genre, register, and numerous other factors. Similarly, while the\nmasked language model (MLM) and next sentence prediction (NSP) pretraining objectives lead to a\nBERT model that performs well for ranking, neither objective is closely related to the ranking task.\nThus, it may be helpful to perform additional pretraining on the target corpus or with a new objective\nthat is tailored for ranking. It is important here to emphasize that pretraining requires only access to\nthe corpus we are searching and does not require any queries or relevance judgments.\nIn order to beneﬁt from additional pretraining on a target corpus, the model should be given the\nchance to learn more about the distribution of the vocabulary terms and their co-occurrences prior to\nlearning how to rank them. Put differently, the ranking model should be given an opportunity to “see”\nwhat texts in a corpus “look like” before learning relevance signals. To our knowledge, Nogueira\net al. [2019a] was the ﬁrst to demonstrate this idea, which they called target corpus pretraining (TCP),\nspeciﬁcally for ranking in the context of their multi-stage architecture (discussed in Section 3.4.1).\nHere, we only present their results with monoBERT. Instead of using Google’s BERT checkpoints as\nthe starting point of ﬁne tuning, they began by additional pretraining on the MS MARCO passage\ncorpus using the same objectives from the original BERT paper, i.e., masked language modeling and\nnext sentence prediction. Only after this additional pretraining stage was the model then ﬁne-tuned\nwith the MS MARCO passage data. This technique has also been called “further pretraining”, and its\nimpact can be shown by comparing row (2a) with row (2b). Although the improvement is modest,\nthe gain is “free” in the sense of not requiring any labeled data, and so adopting this technique might\nbe worthwhile in certain scenarios.\nThese results are in line with ﬁndings from similar approaches for a variety of natural language\nprocessing tasks [Beltagy et al., 2019, Raffel et al., 2020, Gururangan et al., 2020]. However, as\na counterpoint, Gu et al. [2020] argued that for domains with abundant unlabeled text (such as\nbiomedicine), pretraining language models from scratch is preferable to further pretraining general-\ndomain language models. This debate is far from settled and domain adaptation continues to be an\nactive area of research, both for text ranking and NLP tasks in general.\nOther researchers have proposed performing pretraining using a modiﬁed objective, with the goal of\nimproving BERT’s effectiveness on downstream tasks. For example, ELECTRA (described later in\nSection 3.3.1) replaces the masked language model task with a binary classiﬁcation task that involves\npredicting whether each term is the original term or a replacement.\nSpeciﬁcally for information retrieval, Ma et al. [2021b] proposed a new “representative words\nprediction” (ROP) task that involves presenting the model with two different sets of terms and asking\nthe model to predict which set is more related to a given document. A pretraining instance comprises\ntwo segments: segment A consists of one set of terms (analogous to the query in monoBERT)\n64\nMS MARCO Passage\nDevelopment Test\nMethod MRR@10 MRR@10\n(1) Anserini (BM25) = Table 5, row (3a) 0.187 0.190\n(2a) + monoBERT = Table 5, row (3b) 0.372 0.365\n(2b) + monoBERT + TCP 0.379 -\nTable 9: The effectiveness of target corpus pretraining (TCP) for monoBERT on the MS MARCO\npassage ranking test collection.\nand segment B contains a document. Given this input, the [CLS] token is provided as input to a\nfeedforward network to predict a score. This is performed for a “relevant” and a “non-relevant” set\nof terms, and their scores are fed into a pairwise hinge loss. To choose the two sets of terms, a set\nsize is ﬁrst sampled from a Poisson distribution, and then two sets of terms of the sampled size are\nrandomly sampled from a single document with stopwords removed. A multinomial query likelihood\nmodel with Dirichlet smoothing [Zhai, 2008] is then used to calculate a score for each set of terms;\nthe set with the higher score is treated as the “relevant” set.\nMa et al. [2021b] evaluated the impact of performing additional pretraining with BERTBase on\nWikipedia and the MS MARCO document collection with the MLM objective, their proposed ROP\nobjective, and a combination of the two. They found that pretraining with ROP improves effectiveness\nover pretraining with MLM on the Robust04, ClueWeb09B, and GOV2 test collections when reranking\nBM25. Each document in these datasets was truncated to ﬁt into monoBERT. Combining the MLM\nand ROP objectives yielded little further improvement. However, the models reported in this work\ndo not appear to yield results that are competitive with many of the simple models we describe later\nin this section, and thus it is unclear if this pretraining technique can yield similar gains on better\nranking models.\n“Multi-Step” Supervised Fine-Tuning Strategies. In the context of pretrained transformers, ﬁne-\ntuning involves labeled data drawn from the same distribution as the target downstream task. However,\nit is often the case that researchers have access to labeled that is not “quite right” with respect to\nthe target task. In NLP, for example, we might be interested in named-entity recognition (NER) in\nscientiﬁc articles in the biomedical domain, but we have limited annotated data. Can NER data on\nnews articles, for example, nevertheless be helpful? The same train of thought can be applied to text\nranking. Often, we are interested in a slightly different task or a different domain than the ones we\nhave relevance judgments for. Can we somehow exploit these data?\nNot surprisingly, the answer is yes and researchers have experimented with different “multi-step”\nﬁne-tuning strategies for a range of NLP applications. The idea is to leverage out-of-task or out-of-\ndomain labeled data (or out-of-distribution labeled data that’s just not “right” for whatever reason) to\nﬁne-tune a model before ﬁne-tuning on labeled data drawn from the same distribution as the target\ntask. Since there may be multiple such datasets, the ﬁne-tuning process may span multiple “stages”\nor “phases”. In the same way that target corpus pretraining gives the model a sense of what the texts\n“look like” before attempting to learn relevance signals, these technique attempts to provide the model\nwith “general” knowledge of the task before learning from task-speciﬁc data. To our knowledge,\nthe ﬁrst reported instance of sequential ﬁne-tuning with multiple labeled datasets is by Phang et al.\n[2018] on a range of natural language inference tasks.\nThis technique of sequentially ﬁne-tuning on multiple datasets, as speciﬁcally applied to text ranking,\nhas also been explored by many researchers: Akkalyoncu Yilmaz et al. [2019b] called this cross-\ndomain relevance transfer. Garg et al. [2020] called this the “transfer and adapt” (TAND A) approach.\nDai and Callan [2019a] ﬁrst ﬁne-tuned on data from search engine logs before further ﬁne-tuning on\nTREC collections. Zhang et al. [2021] called this “pre–ﬁne-tuning”, and speciﬁcally investigated the\neffectiveness of pre–ﬁne-tuning a ranking model on the MS MARCO passage ranking test collection\nbefore further ﬁne-tuning on collection-speciﬁc relevance judgments. Mokrii et al. [2021] presented\nanother study along similar lines. Applied to question answering, Yang et al. [2019d] called this\n“stage-wise” ﬁne-tuning, which is further detailed in Xie et al. [2020]. For consistency in presentation,\nin this survey we refer to such sequential or multi-step ﬁne-tuning strategies as pre–ﬁne-tuning, with\nthe convenient abbreviation of pFT (vs. FT for ﬁne tuning). This technique is widely adopted—\n65\nobviously applicable to monoBERT, but can also be used in the context of other models. We do\nnot present any experimental results here, and instead examine the impact of pre–ﬁne-tuning in the\ncontext of speciﬁc ranking models presented in this section.\nOne possible pitfall when ﬁne-tuning with multiple labeled datasets is the phenomenon known as\n“catastrophic forgetting”, where ﬁne-tuning a model on a second dataset interferes with its ability\nto perform the task captured by the ﬁrst dataset. This is undesirable in many instances because we\nmight wish for the model to adapt “gradually”. For example, if the ﬁrst dataset captured text ranking\nin the general web domain and the second dataset focuses on biomedical topics, we would want the\nmodel to gracefully “back off’ to general web knowledge if the query was not speciﬁcally related to\nbiomedicine. Lovón-Melgarejo et al. [2021] studied catastrophic forgetting in neural ranking models:\nCompared to pre-BERT neural ranking models, they found that BERT-based models seem to be able\nto retain effectiveness on the pre–ﬁne-tuning dataset after further ﬁne-tuning.\nPre–ﬁne-tuning need not exploit human labeled data. For example, relevance judgments might\ncome from distant (also called weak) supervision techniques. Zhang et al. [2020d] proposed a\nmethod for training monoBERT with weak supervision by using reinforcement learning to select\n(anchor text, candidate text) pairs during training. In this approach, relevance judgments are used to\ncompute the reward guiding the selection process, but the selection model does not use the judgments\ndirectly. To apply their trained monoBERT model to rerank a target collection, the authors trained a\nlearning-to-rank method using coordinate ascent with features consisting of the ﬁrst-stage retrieval\nscore and monoBERT’s[CLS] vector. The authors found that these extensions improved over prior\nweak supervision approaches used with neural rankers [Dehghani et al., 2017, MacAvaney et al.,\n2019b]. Beyond weak supervision, it might even been possible to leverage synthetic data, similar to\nthe work of Ma et al. [2021a] (who applied the idea to dense retrieval), but this thread has yet to be\nfully explored.\nThe multi-step ﬁne-tuning strategies discussed here are related to the well-studied notion of curriculum\nlearning. MacAvaney et al. [2020e] investigated whether monoBERT can beneﬁt from a training\ncurriculum [Bengio et al., 2009] in which the model is presented with progressively more difﬁcult\ntraining examples as training progresses. Rather than excluding training data entirely, they calculate\na weight for each training example using proposed difﬁculty heuristics based on BM25 ranking. As\ntraining progresses, these weights become closer to uniform. MacAvaney et al. [2020e] found that\nthis weighted curriculum learning approach can signiﬁcantly improve the effectiveness of monoBERT.\nWhile both pre–ﬁne-tuning and curriculum learning aim to sequence the presentation of examples\nto a model during training, the main difference between these two methods is that pre–ﬁne-tuning\ngenerally involves multiple distinct datasets. In contrast, curriculum learning strategies can be applied\neven on a single (homogeneous) dataset.\nOne main goal of multi-step ﬁne-tuning strategies is to reduce the amount of labeled data needed in\nthe target domain or task by exploiting existing “out-of-distribution” datasets. This connects to the\nbroad theme of “few-shot” learning, popular in natural language processing, computer vision, and\nother ﬁelds as well. Taking this idea to its logical conclusion, researchers have explored zero-shot\napproaches to text ranking. That is, the model is trained on (for example) out-of-domain data and\ndirectly applied to the target task. Examples include Birch (see Section 3.3.1) and monoT5 (see\nSection 3.5.3), as well as zero-shot domain adaptation techniques (see Section 6.2). We leave details\nto these speciﬁc sections.\nTo wrap up the present discussion, researchers have explored many different techniques to “train”\nBERT and other transformers beyond the “pretrain then ﬁne-tune” recipe. There is a whole litany\nof tricks to exploit “related” data, both in an unsupervised as well as a supervised fashion (and to\neven “get away” with not using target data at all in a zero-shot setting). While these tricks can indeed\nbe beneﬁcial, details of how to properly apply them (e.g., how many epochs to run, how many and\nwhat order to apply out-of-domain datasets, how to heuristically label and select data, when zero-shot\napproaches might work, etc.) remain somewhat of an art, and their successful application typically\ninvolves lots of trial and error. Some of these issues are discussed by Zou et al. [2021] in the context\nof applying transformer-based models in Baidu search, where they cautioned that blindly ﬁne-tuning\nrisks unstable predictions, poor generalizations, and deviations from task metrics, especially when\nthe training data are noisy. While we understand at a high level why various ﬁne-tuning techniques\nwork, more research is required to sharpen our understanding so that expected gains can be accurately\npredicted and modeled without the need to conduct extensive experiments repeatedly.\n66\nThese are important issues that remain unresolved, and in particular, pretraining and pre–ﬁne-tuning\nbecome important when transformers are applied to domain-speciﬁc applications, such as legal texts\nand scientiﬁc articles; see additional discussions in Section 6.2.\n3.3 From Passage to Document Ranking\nOne notable limitation of monoBERT is that it does not offer an obvious solution to the input length\nrestrictions of BERT (and of simple BERT variants). Nogueira and Cho [2019] did not have this\nproblem because the test collections they examined did not contain texts that overﬂowed this limit.\nThus, monoBERT is limited to ranking paragraph-length passages, not longer documents (e.g., news\narticles) as is typically found in most ad hoc retrieval test collections. This can be clearly seen\nin the histogram of text lengths from the MS MARCO passage corpus, shown in Figure 3 from\nSection 2.7. The combination of BERT’s architecture and the pretraining procedure means that the\nmodel has difﬁculty handling input sequences longer than 512 tokens, both from the perspective\nof model effectiveness and computational requirements on present-day hardware. Let us begin by\nunderstanding in more detail what the issues are.\nSince BERT was pretrained with only input sequences up to 512 tokens, learned position embeddings\nfor token positions past 512 are not available. Because position embeddings inform the model about\nthe linear order of tokens, if the input sequence lacks this signal, then everything the model has\nlearned about the linear structure of language is lost (i.e., the input will essentially be treated as\na bag of words). We can see from the experimental results in Table 7 that position embeddings\nprovide important relevance signals for monoBERT. Henderson [2020] explained this by pointing\nout that BERT can be thought of as a “bag of vectors”, where structural cues come only from the\nposition embeddings. This means that the vectors in the bag are exchangeable, in that renumbering\nthe indices used to refer to the different input representations will not change the interpretation of the\nrepresentation (provided that the model is adjusted accordingly as well). While it may be possible to\nlearn additional position embeddings during ﬁne-tuning with sufﬁcient training data, this does not\nseem like a practical general-purpose solution. Without accurate position embeddings, it is unclear\nhow we would prepare input sequences longer than 512 tokens for inference (more details below).\nFrom the computational perspective, the all-to-all nature of BERT’s attention patterns at each\ntransformer encoder layer means that it exhibits quadratic complexity in both time and space with\nrespect to input length. Thus, simply throwing more hardware at the problem (e.g., GPUs with more\nRAM) is not a practical solution; see Beltagy et al. [2020] for experimental results characterizing\nresource consumption on present-day hardware with increasing sequence lengths. Instead, researchers\nhave tackled this issue by applying some notion of sparsity to the dense attention mechanism. See\nTay et al. [2020] for a survey of these attempts, which date back to at least 2019 [Child et al., 2019].\nWe discuss modiﬁcations to the transformer architecture that replace all-to-all attention with more\nefﬁcient alternatives later in Section 3.3.5.\nThe length limitation of BERT (and transformers in general) breaks down into two distinct but related\nchallenges for text ranking:\nTraining. For training, it is unclear what to feed to the model. The key issue is that relevance\njudgments for document ranking (e.g., from TREC test collections) are provided at the document\nlevel, i.e., they are annotations on the document as a whole. Obviously, a judgment of “relevant”\ncomes from a document containing “relevant material”, but it is unknown how that material is\ndistributed throughout the document. For example, there could be a relevant passage in the middle of\nthe document, a few relevant passages scattered throughout the document, or the document may be\nrelevant “holistically” when considered in its entirety, but without any speciﬁcally relevant passages.\nIf we wish to explicitly model different relevance grades (e.g., relevant vs. highly relevant), then this\n“credit assignment” problem becomes even more challenging.\nDuring training, if the input sequence (i.e., document plus the query and the special tokens) exceeds\nBERT’s length limitations, it must be truncated somehow, lest we run into exactly the issues discussed\nabove. Since queries are usually shorter than documents, and it make little sense to truncate the query,\nwe must sacriﬁce terms from the document text. While we could apply heuristics, for example, to feed\nBERT only spans in the document that contain query terms or even disregard this issue completely\n(see Section 3.3.2), there is no guarantee that training passages from the document fed to BERT are\nactually relevant. Thus, training will be noisy at best.\n67\nInference. At inference time, if a document is too long to feed into BERT in its entirety, we must\ndecide how to preprocess it. We could segment the document into chunks, but there are many design\nchoices: For example, ﬁxed-width spans or natural units such as sentences? How wide should these\nsegments be? Should they be overlapping? Furthermore, applying inference over different chunks\nfrom a document still requires some method for aggregating evidence.\nIt is possible to address the inference challenge by aggregating either passage scores or passage\nrepresentations. Methods that use score aggregation predict a relevance score for each chunk, and\nthese scores are then aggregated to produce a document relevance score (e.g., by taking the maximum\nscore across the chunks). Methods that perform representation aggregation ﬁrst combine passage\nrepresentations before predicting a relevance score. With a properly designed aggregation technique,\neven if each passage is independently processed, the complete ranking model can be differentiable\nand thus amenable to end-to-end training via back propagation. This solves the training challenge as\nwell, primarily by letting the model ﬁgure out how to allocate “credit” by itself.\nBreaking this “length barrier” in transitioning from passage ranking to full document ranking was the\nnext major advance in applying BERT to text ranking. This occurred with three proposed models\nthat were roughly contemporaneous, dating to Spring 2019, merely a few months after monoBERT:\nBirch [Akkalyoncu Yilmaz et al., 2019b], which was ﬁrst described by Yang et al. [2019e], BERT–\nMaxP [Dai and Callan, 2019b], and CEDR [MacAvaney et al., 2019a]. Interestingly, these three\nmodels took different approaches to tackle the training and inference challenges discussed above,\nwhich we detail in turn. We then present subsequent developments: PARADE [Li et al., 2020a], which\nincorporates and improves on many of the lessons learned in CEDR, and a number of alternative\napproaches to ranking long texts. All of these ranking models are still based on BERT or a simple\nBERT variant at their cores; we discuss efforts to move beyond BERT in Section 3.5.\n3.3.1 Document Ranking with Sentences: Birch\nThe solution presented by Birch [Akkalyoncu Yilmaz et al., 2019b] can be summarized as follows:\n• Avoid the training problem entirely by exploiting labeled data where length issues don’t exist,\nand then transferring the learned relevance matching model on those data to the domain or task\nof interest.\n• For the inference problem, convert the task of estimating document relevance into the task of\nestimating the relevance of individual sentences and then aggregating the resulting scores.\nIn short, Birch solved the training problem above by simply avoiding it. Earlier work by the same\nresearch group [Yang et al., 2019e] that eventually gave rise to Birch ﬁrst examined the task of\nranking tweets, using test collections from the TREC Microblog Tracks [Ounis et al., 2011, Soboroff\net al., 2012, Lin and Efron, 2013, Lin et al., 2014]. These evaluations focused on information seeking\nin a microblog context, where users desire relevant tweets with respect to an information need at\na particular point in time. As tweets are short (initially 140 characters, now 280 characters), they\ncompletely avoid the length issues we discussed above.\nNot surprisingly, ﬁne-tuning monoBERT on tweet data led to large and statistically signiﬁcant gains\non ranking tweets. However, Yang et al. [2019e] discovered that a monoBERT model ﬁne-tuned with\ntweet data was also effective for ranking documents from a newswire corpus. This was a surprising\nﬁnding: despite similarities in the task (both aread hoc retrieval problems), the domains are completely\ndifferent. Newswire articles comprise well-formed and high-quality prose written by professional\njournalists, whereas tweets are composed by social media users, often containing misspellings,\nungrammatical phrases, and incoherent meanings, not to mention genre-speciﬁc idiosyncrasies such\nas hashtags and @-mentions.\nIn other words, Yang et al. [2019e] discovered that, for text ranking, monoBERT appears to have\nvery strong domain transfer effects for relevance matching. Training on tweet data and performing\ninference on articles from a newswire corpus is an instance of zero-shot cross-domain learning, since\nthe model had never been exposed to annotated data from the speciﬁc task.94 This ﬁnding predated\nmany of the papers discussed in Section 3.2.4, but in truth Birch had begun to explore some of the\nideas presented there (e.g., pre–ﬁne-tuning as well as zero-shot approaches).\n94There is no doubt, of course, that BERT had been exposed to newswire text during pretraining.\n68\nRobust04 Core17 Core18\nMethod MAP nDCG@20 MAP nDCG@20 MAP nDCG@20\n(1) BM25 + RM3 0.2903 0.4407 0.2823 0.4467 0.3135 0.4604\n(2a) 1S: BERT(MB) 0.3408† 0.4900† 0.3091† 0.4628 0.3393 † 0.4848†\n(2b) 2S: BERT(MB) 0.3435† 0.4964† 0.3137† 0.4781 0.3421 † 0.4857†\n(2c) 3S: BERT(MB) 0.3434† 0.4998† 0.3154† 0.4852† 0.3419† 0.4878†\n(3a) 1S: BERT(MSM) 0.3028† 0.4512 0.2817 † 0.4468 0.3121 0.4594\n(3b) 2S: BERT(MSM) 0.3028† 0.4512 0.2817 † 0.4468 0.3121 0.4594\n(3c) 3S: BERT(MSM) 0.3028† 0.4512 0.2817 † 0.4468 0.3121 0.4594\n(4a) 1S: BERT(MSM→MB) 0.3676† 0.5239† 0.3292† 0.5061† 0.3486† 0.4953†\n(4b) 2S: BERT(MSM→MB) 0.3697† 0.5324† 0.3323† 0.5092† 0.3496† 0.4899†\n(4c) 3S: BERT(MSM→MB) 0.3691† 0.5325† 0.3314† 0.5070† 0.3522† 0.4899†\nTable 10: The effectiveness of Birch on the Robust04, Core17, and Core18 test collections. The\nsymbol † denotes signiﬁcant improvements over BM25 + RM3 (paired t-tests, p <0.01, with\nBonferroni correction).\nThis domain-transfer discovery was later reﬁned by Akkalyoncu Yilmaz et al. [2019b] in Birch.\nTo compute a document relevance score sf, inference is applied to each individual sentence in the\ndocument, and then the top nscores are combined with the original document score sd (i.e., from\nﬁrst-stage retrieval) as follows:\nsf\n∆\n= α·sd + (1 −α) ·\nn∑\ni=1\nwi ·si (18)\nwhere si is the score of the i-th top scoring sentence according to BERT. Inference on individual\nsentences proceeds in the same manner as in monoBERT, where the input to BERT is comprised of\nthe concatenation of the query qand a sentence pi ∈Dinto the sequence:\n[[CLS],q, [SEP],pi,[SEP]] (19)\nIn other words, the ﬁnal relevance score of a document comes from the combination of the original\ncandidate document score sd and evidence contributions from the top sentences in the document as\ndetermined by the BERT model. The parameters αand wi’s can be tuned via cross-validation.\nResults and Analysis. Birch results are reported in Table 10 withBERTLarge on the Robust04, Core17,\nand Core18 test collections (see Section 2.7), with metrics directly copied from Akkalyoncu Yilmaz\net al. [2019b]. To be explicit, the query tokens qfed into BERT come from the “title” portion of the\nTREC topics (see Section 2.2), i.e., short keyword phrases. This distinction will become important\nwhen we discuss Dai and Callan [2019b] next. The results in the table are based on reranking the\ntop k= 1000 candidates using BM25 from Anserini for ﬁrst-stage retrieval using the topic titles as\nbag-of-words queries. See the authors’ paper for detailed experimental settings. Note that none of\nthese collections were used to ﬁne-tune the BERT relevance models; the only learned parameters are\nthe weights in Eq. (18).\nThe top row shows the BM25 + RM3 query expansion baseline. The column groups present\nmodel effectiveness on the Robust04, Core17, and Core18 test collections. Each row describes an\nexperimental condition: nS indicates that inference was performed on the top nscoring sentences\nfrom each document. Up to three sentences were considered; the authors reported that more sentences\ndid not yield any improvements in effectiveness. The notation in parentheses describes the ﬁne-tuning\nprocedure: MB indicates that BERT was ﬁne-tuned on data from the TREC Microblog Tracks; MSM\nindicates that BERT was ﬁne-tuned on data from the MS MARCO passage retrieval test collection;\nMSM →MB refers to a model that was ﬁrst pre–ﬁne-tuned on the MS MARCO passage data and\nthen further ﬁne-tuned on MB.95 Table 10 also includes results of signiﬁcance testing using paired\nt-tests, comparing each condition to the BM25 + RM3 baseline. Statistically signiﬁcant differences\n(p< 0.01), with appropriate Bonferroni correction, are denoted by the symbol †next to the result.\nBirch ﬁne-tuned on microblog data (MB) alone signiﬁcantly outperforms the BM25 + RM3 baseline\nfor all three metrics on Robust04. On Core17 and Core18, signiﬁcant increases in MAP are observed\n95Akkalyoncu Yilmaz et al. [2019b] did not call this pre–ﬁne-tuning since the term was introduced later.\n69\nas well (and other metrics in some cases). In other words, the relevance classiﬁcation model learned\nfrom labeled tweet data successfully transferred over to news articles despite the large aforementioned\ndifferences in domain.\nInterestingly, Akkalyoncu Yilmaz et al. [2019b] reported that ﬁne-tuning on MS MARCO alone yields\nsmaller gains over the baselines compared to ﬁne-tuning on tweets. The gains in MAP are statistically\nsigniﬁcant for Robust04 and Core17, but not Core18. In her thesis, Akkalyoncu Yilmaz [2019]\nconducted experiments that offered an explanation: this behavior is attributable to mismatches in\ninput text length between the training and test data. The average length of the tweet training examples\nis closer to the average length of sentences in Robust04 than the passages in the MS MARCO passage\ncorpus (which are longer). By simply truncating the MS MARCO training passages to the average\nlength of sentences in Robust04 and ﬁne-tuning the model with these new examples, Akkalyoncu\nYilmaz reported a large boost in effectiveness: 0.3300 MAP on Robust04. While this result is still\nbelow ﬁne-tuning only with tweets, simply truncating MS MARCO passages also degrades the quality\nof the dataset, in that it could have discarded the relevant portions of the passages, thus leaving behind\nan inaccurate relevance label.\nThe best condition in Birch is to pre–ﬁne-tune with MS MARCO passages, and then further ﬁne-tune\nwith tweet data, which yields effectiveness that is higher than ﬁne-tuning with either dataset alone.\nLooking across all ﬁne-tuning conﬁgurations of Birch, it appears that the top-scoring sentence of\neach candidate document alone is a good indicator of document relevance. Additionally considering\nthe second ranking sentence yields at most a minor gain, and in some cases, adding a third sentence\nactually causes effectiveness to drop. In all cases, however, contributions from BM25 scores remain\nimportant—the model places non-negligible weight on αin Eq. (18). This result does not appear\nto be consistent with the monoBERT experiments described in Figure 10, which shows that beyond\ndeﬁning the top kcandidates fed to monoBERT, BM25 scores do not provide any additional relevance\nsignal, and in fact interpolating BM25 scores hurts effectiveness. The two models, of course, are\nevaluated on different test collections, but the question of whether exact term match scores are still\nnecessary for relevance classiﬁcation with BERT remains not completely resolved.\nThe thesis of Akkalyoncu Yilmaz [2019] described additional ablation experiments that reveal\ninteresting insights about the behavior of BERT for document ranking. It has long been known (see\ndiscussion in Section 1.2.2) that modeling the relevance between queries and documents requires a\ncombination of exact term matching (i.e., matching the appearance of query terms in the text) as well\nas “semantic matching”, which encompasses attempts to capture a variety of linguistic phenomena\nincluding synonymy, paraphrases, etc. What is the exact role that each plays in BERT? To answer\nthis question, Akkalyoncu Yilmaz [2019] performed an ablation experiment where all sentences that\ncontain at least one query term were discarded; this had the effect of eliminating all exact match\nsignals and forced BERT to rely only on semantic match signals. As expected, effectiveness was much\nlower, reaching only 0.3101 MAP on Robust04 in the best model conﬁguration, but the improvement\nover the BM25 + RM3 baseline (0.2903 MAP) remained statistically signiﬁcant. This result suggests\nthat with BERT, semantic match signals make important contributions to relevance matching.\nAs an anecdotal example, for the query “international art crime”, in one relevant document, the\nfollowing sentence was identiﬁed as the most relevant: “Three armed robbers take 21 Renaissance\npaintings worth more than $5 million from a gallery in Zurich, Switzerland.” Clearly, this sentence\ncontains no terms from the query, yet provides information relevant to the information need. An\nanalysis of the attention patterns shows strong associations between “art” and “paintings” and between\n“crime” and “robbers” in the different transformer encoder layers. Here, we see that BERT accurately\ncaptures semantically important matches for the purposes of modeling query–document relevance,\nproviding qualitative evidence supporting the conclusion above.\nTo provide some broader context for the level of effectiveness achieved by Birch: Akkalyoncu Yilmaz\net al. [2019b] claimed to have reported the highest known MAP at the time of publication on the\nRobust04 test collection. This assertion appears to be supported by the meta-analysis of Yang et al.\n[2019b], who analyzed over 100 papers up until early 2019 and placed the best neural model at\n0.3124 [Dehghani et al., 2018]. These results also exceeded the previous best known score of 0.3686,\na non-neural method based on ensembles [Cormack et al., 2009] reported in 2009. On the same\ndataset, CEDR [MacAvaney et al., 2019a] (which we discuss in Section 3.3.3) achieved a slightly\nhigher nDCG@20 of 0.5381, but the authors did not report MAP. BERT–MaxP (which we discuss\nnext in Section 3.3.2) reported 0.529 nDCG@20. It seems clear that the “ﬁrst wave” of text ranking\n70\nmodels based on BERT was able to outperform pre-BERT models and at least match the best non-\nneural techniques known at the time.96 These scores, in turn, have been bested by even newer ranking\nmodels such as PARADE [Li et al., 2020a] (Section 3.3.4) and monoT5 (Section 3.5.3). The best\nBirch model also achieved a higher MAP than the best TREC submissions that did not use past\nlabels or involve human intervention for both Core17 and Core18, although both test collections were\nrelatively new at the time and thus had yet to receive much attention from researchers.\nAdditional Studies. Li et al. [2020a] introduced a Birch variant called Birch–Passage, which differs\nin four ways: (1) the model is trained end-to-end, (2) it is ﬁne-tuned with relevance judgments on\nthe target corpus (with pre–ﬁne-tuning on the MS MARCO passage ranking test collection) rather\nthan being used in a zero-shot setting, (3) it takes passages rather than sentences as input, and (4) it\ndoes not combine retrieval scores from the ﬁrst-stage ranker. In more detail: Passages are formed\nby taking sequences of 225 tokens with a stride of 200 tokens. As with the original Birch design,\nBirch–Passage combines relevance scores from the top three passages. To train the model end-to-end,\na fully-connected layer with all weights initially set to one is used to combine the three scores; this is\nequivalent to a weighted summation. Instead of BERTLarge as in the original work, Li et al. [2020a]\nexperimented with BERTBase as well as the ELECTRABase variant.\nELECTRA [Clark et al., 2020b] can be described as a BERT variant that attempts to improve\npretraining by substituting its masked language model pretraining task with a replaced token detection\ntask, in which the model predicts whether a given token has been replaced with a token produced by\na separate generator model. The contextual representations learned by ELECTRA were empirically\nshown to outperform those from BERT on various natural language processing tasks given the same\nmodel size, data, and compute.\nResults copied from Li et al. [2020a] are shown in row (4) in Table 11. The “Title” and “Description”\ncolumns denote the effectiveness of using different parts of a TREC topic in the input template\nfed to the model for reranking; the original Birch model only experimented with topic titles. The\neffectiveness differences between these two conditions were ﬁrst observed by Dai and Callan [2019b]\nin the context of MaxP, and thus we defer our discussions until there. Comparing these results to the\noriginal Birch experiments, repeated in row (3) from row (4c) in Table 10, it seems that one or more\nof the changes in Birch–Passage increased effectiveness. However, due to differences in experimental\ndesign, it is difﬁcult to isolate the source of the improvement.\nTo better understand the impact of various design decisions made in Li et al. [2020a] and Akka-\nlyoncu Yilmaz et al. [2019b], we conducted additional experiments with Birch–Passage using the\nCapreolus toolkit [Yates et al., 2020]; to date, these results have not be reported elsewhere. In addition\nto the various conditions examined by Li et al., we also considered the impact of linear interpolation\nwith ﬁrst-stage retrieval scores and the impact of pre–ﬁne-tuning. These experiments used the same\nﬁrst-stage ranking, folds, hyperparameters, and codebase as Li et al. [2020a], thus enabling a fair and\nmeaningful comparison.\nResults are shown in Table 11, grouped into “no interpolation” and interpolation “with BM25 + RM3”\ncolumns. These model conﬁgurations provide a bridge that allows us to compare the results of Li\net al. [2020a] and Akkalyoncu Yilmaz et al. [2019b] in a way that lets us better attribute the impact\nof different design choices. Rows (5a) and (5b) represent Birch–Passage using either BERTBase\nor ELECTRABase, without pre–ﬁne-tuning in both cases. It seems clear that a straightforward\nsubstitution of BERTBase for ELECTRABase yields a gain in effectiveness. Here, model improvements\non general NLP tasks reported by Clark et al. [2020b] do appear to translate into effective gains in\ndocument ranking.\nComparing the interpolated results on title (keyword) queries, we see that Birch–Passage performs\nslightly worse than the original Birch model, row (3), using BERTBase, row (5a), and slightly better\nthan the original Birch model using ELECTRABase, row (5b). While ELECTRABase is about one-\nthird the size ofBERTLarge, it is worth noting that Birch–Passage has the advantage of being ﬁne-tuned\non Robust04. These results can be viewed as a replication (i.e., independent implementation) of the\nmain ideas behind Birch, as well as their generalizability, since we see that a number of different\ndesign choices leads to comparable levels of effectiveness.\n96The comparison to Cormack et al. [2009], however, is not completely fair due to its use of ensembles, whereas\nBirch, BERT–MaxP, and CEDR are all individual ranking models.\n71\nRobust04\nNo interpolation with BM25 + RM3\nnDCG@20 nDCG@20\nMethod Title Desc Title Desc\n(1) BM25 0.4240 0.4058 - -\n(2) BM25 + RM3 0.4514 0.4307 - -\n(3) Birch (MS→MB, BERTLarge) = Table 10, row (4c) - - 0.5325 -\n(4) Birch–Passage (ELECTRABasew/ MSM pFT) [Li et al., 2020a] 0.5454 0.5931 - -\n(5a) Birch–Passage (BERTBase, no pFT) 0.4959 † 0.5502 0.5260† 0.5723\n(5b) Birch–Passage (ELECTRABase, no pFT) 0.5259 0.5611 0.5479 0.5872\nTable 11: The effectiveness of Birch variants on the Robust04 test collection using title and de-\nscription queries with and without BM25 + RM3 interpolation. Statistically signiﬁcant decreases\nin effectiveness from Birch–Passage (ELECTRABase) are indicated with the symbol †(two-tailed\npaired t-test, p< 0.05, with Bonferroni correction).\nAlso from rows (5a) and (5b), we can see that both Birch–Passage variants beneﬁt from linear\ninterpolation with BM25 + RM3 as the ﬁrst-stage ranker. Comparing title and description queries,\nBirch–Passage performs better with description queries regardless of the interpolation setting and\nwhich BERT variant is used (more discussion next, in the context of MaxP). Row (5b) vs. row (4)\nillustrates the effects of pre–ﬁne-tuning, which is the only difference between those two conditions.\nIt should be no surprise that ﬁrst ﬁne-tuning with a very large, albeit out-of-domain, dataset has a\nbeneﬁcial impact on effectiveness. In Section 3.3.2, we present additional experimental evidence\nsupporting the effectiveness of this technique.\nTakeaway Lessons. Summarizing, there are two important takeaways from Birch:\n1. BERT exhibits strong zero-shot cross-domain relevance classiﬁcation capabilities when used in\na similar way as monoBERT. That is, we can train a BERT model using relevance judgments\nfrom one domain (e.g., tweets) and directly apply the model to relevance classiﬁcation in a\ndifferent domain (e.g., newswire articles) and achieve a high-level of effectiveness.\n2. The relevance score of the highest-scoring sentence in a document is a good proxy for the\nrelevance of the entire document. In other words, it appears that document-level relevance can\nbe accurately estimated by considering only a few top sentences.\nThe ﬁrst point illustrates the power of BERT, likely attributable to the wonders of pretraining. The\nﬁnding with Birch is consistent with other demonstrations of BERT’s zero-shot capabilities, for\nexample, in question answering [Petroni et al., 2019]. We return to elaborate on this observation in\nSection 3.5.3 in the context of ranking with sequence-to-sequence models and also in Section 6.2 in\nthe context of domain-speciﬁc applications.\nThe second point is consistent with previous ﬁndings in the information retrieval literature as well as\nthe BERT–MaxP model that we describe next. We defer a more detailed discussion of this takeaway\nafter presenting that model.\n3.3.2 Passage Score Aggregation: BERT–MaxP and Variants\nAnother solution to the length limitations of BERT is offered by Dai and Callan [2019b], which can\nbe summarized as follows:\n• For training, don’t worry about it! Segment documents into overlapping passages: treat all\nsegments from a relevant document as relevant and all segments from a non-relevant document\nas not relevant.\n• For the inference problem, segment documents in the same way, estimate the relevance of\neach passage, and then perform simple aggregation of the passage relevance scores (taking the\nmaximum, for example; see more details below) to arrive at the document relevance score.\nIn more detail, documents are segmented into passages using a 150-word sliding window with a stride\nof 75 words. Window width and stride length are hyperparameters, but Dai and Callan [2019b] did\n72\nRobust04 ClueWeb09b\nnDCG@20 nDCG@20\nModel Title Desc Title Desc\n(1) BoW 0.417 0.409 0.268 0.234\n(2) SDM 0.427 0.427 0.279 0.235\n(3) LTR 0.427 0.441 0.295 0.251\n(4a) BERT–FirstP 0.444 † 0.491† 0.286 0.272 †\n(4b) BERT–MaxP 0.469 † 0.529† 0.293 0.262 †\n(4c) BERT–SumP 0.467 † 0.524† 0.289 0.261\n(5) BERT–FirstP (Bing pFT) - - 0.333 † 0.300†\nTable 12: The effectiveness of different passage score aggregation approaches on the Robust04 and\nClueWeb09b test collections. The symbol †denotes signiﬁcant improvements over LTR (p< 0.05).\nnot report experimental results exploring the effects of different settings. Inference on the passages is\nthe same as in Birch and in monoBERT, where for each passagepi ∈D, the following sequence is\nconstructed and fed to BERT as the input template:\n[[CLS],q, [SEP],pi,[SEP]] (20)\nwhere q is the query. The [CLS] token is then fed into a fully-connected layer (exactly as in\nmonoBERT) to produce a score si for passage pi.97 The passage relevance scores {si}are then\naggregated to produce the document relevance score sd according to one of three approaches:\n• BERT–MaxP: take the maximum passage score as the document score, i.e., sd = max si\n• BERT–FirstP: take the score of the ﬁrst passage as the document score, i.e., sd = s1.\n• BERT–SumP: take the sum of all passage scores as the document score, i.e., sd = ∑\nisi.\nAnother interesting aspect of this work is an exploration of different query representations that are fed\ninto BERT, which is the ﬁrst study of its type that we are aware of. Recall that in Birch, BERT input is\ncomposed from the “title” portion of TREC topics, which typically comprises a few keywords, akin to\nqueries posed to web search engines today (see Section 2.2). In addition to using these as queries, Dai\nand Callan [2019b] also investigated using the sentence-long natural language “description” ﬁelds as\nquery representations fed to BERT. As the experimental results show, this choice has a large impact\non effectiveness.\nResults and Analysis. Main results, in terms of nDCG@20, copied from Dai and Callan [2019b]\non Robust04 and test collections on ClueWeb09b (see Section 2.7) are presented in Table 12. Just\nlike in Birch and monoBERT, the retrieve-and-rerank strategy was used—in this case, the candidate\ndocuments were supplied by bag-of-words default ranking with the Indri search engine. 98 These\nresults are shown in row (1) as “BoW”. The top k = 100 results, with either title or description\nqueries were reranked with BERTBase; for comparison, note that Birch reranked with k= 1000.\nDifferent aggregation techniques were compared against two baselines: SDM, shown in row (2),\nrefers to the sequential dependence model [Metzler and Croft, 2005]. On top of bag-of-words queries\n(i.e., treating all terms as independent unigrams), SDM contributes evidence from query bigrams that\noccur in the documents (both ordered and unordered). Previous studies have validated the empirical\neffectiveness of this technique, and in this context SDM illustrates how keyword queries can take\nadvantage of simple “structure” present in the query (based purely on linear word order). As another\npoint of comparison, the effectiveness of a simple learning-to-rank approach was also examined,\nshown in row (3) as “LTR”. The symbol †denotes improvements over LTR that are statistically\nsigniﬁcant (p< 0.05).\nWithout pre–ﬁne-tuning, the overall gains coming from BERT on ranking web pages (ClueWeb09b)\nare modest at best, and for title queries none of the aggregation techniques even beat the LTR baseline.\n97According to the original paper, this was accomplished with a multi-layer perceptron; however, our description\nis more accurate, based on personal communications with the ﬁrst author.\n98The ranking model used was query-likelihood with Dirichlet smoothing (µ= 2500); this detail was omitted\nfrom the original paper, ﬁlled in here based on personal communications with the authors.\n73\nRobust04\nnDCG@20\nMethod Avg. Length SDM MaxP\n(1) Title 3 0.427 0.469\n(2a) Description 14 0.404 0.529\n(2b) Description, keywords 7 0.427 0.503\n(3a) Narrative 40 0.278 0.487\n(3b) Narrative, keywords 18 0.332 0.471\n(3c) Narrative, negative logic removed 31 0.272 0.489\nTable 13: The effectiveness of SDM and BERT–MaxP using different query types on the Robust04\ntest collection.\nPre–ﬁne-tuning BERT–FirstP on a Bing query log signiﬁcantly improves effectiveness, row (5),\ndemonstrating that BERT can be effective in this setting with sufﬁcient training data.99 Since it is\nunclear what conclusions can be drawn from the web test collections, we focus the remainder of our\nanalysis on Robust04. Comparing the different aggregation techniques, the MaxP approach appears\nto yield the highest effectiveness. The low effectiveness of FirstP on Robust04 is not very surprising,\nsince it is not always the case that relevant material appears at the beginning of a news article. Results\nshow that SumP is almost as effective as MaxP, despite having the weakness that it performs no\nlength normalization; longer documents will tend to have higher scores, thus creating a systematic\nbias against shorter documents.\nLooking at the bag-of-words baseline, row (1), the results are generally consistent with the literature:\nWe see that short title queries are more effective than sentence-length description queries; the drop is\nbigger for ClueWeb09b (web pages) than Robust04 (newswire articles). However, reranking with the\ndescriptions as input to BERT is signiﬁcantly more effective that reranking with titles, at least for\nRobust04. This means that BERT is able to take advantage of richer natural language descriptions of\nthe information need. This ﬁnding appears to be robust, as the Birch–Passage experimental results\nshown in Table 11 conﬁrm the higher effectiveness of description queries over title queries as well.\nDai and Callan [2019b] further investigated the intriguing ﬁnding that reranking documents using\ndescription queries is more effective than title queries, as shown in Table 12. In addition to considering\nthe description and narrative ﬁelds from the Robust04 topics, they also explored a “keyword” version\nof those ﬁelds, stripped of punctuation as well as stopwords. For the narrative, they also discarded\n“negative logic” that may be present in the prose. For example, consider topic 697:\nTitle: air trafﬁc controller\nDescription: What are working conditions and pay for U.S. air trafﬁc controllers?\nNarrative: Relevant documents tell something about working conditions or pay\nfor American controllers. Documents about foreign controllers or individuals are\nnot relevant.\nIn this topic, the second sentence in the narrative states relevance in a negative way, i.e., what makes\na document not relevant. These are removed in the “negative logic removed” condition.\nResults of these experiments are shown in Table 13, where the rows show the different query\nconditions described above.100 For each of the conditions, the average length of the query is provided:\nas expected, descriptions are longer than titles, and narratives are even longer. It is also not surprising\nthat removing stopwords reduces the average length substantially. In these experiments, SDM (see\nabove) is taken as a point of comparison, since it represents a simple attempt to exploit “structure”\nthat is present in the query representations. Comparing the “title” query under SDM and the BoW\nresults in Table 12, we can conﬁrm that SDM does indeed improve effectiveness.\nThe MaxP ﬁgures in the ﬁrst two rows of Table 13 are identical to the numbers presented in Table 12\n(same experimental conditions, just arranged differently). For SDM, we see that using description\n99As a historical note, although Dai and Callan [2019b] did not use the terminology of pre–ﬁne-tuning, this work\nrepresents one of the earliest example of the technique, as articulated in Section 3.2.4.\n100For these experiments, stopwords ﬁltering in Indri (used for ﬁrst-stage retrieval) was disabled (personal\ncommunication with the authors).\n74\nqueries decreases effectiveness compared to the title queries, row (2a). In contrast, BERT is able to\ntake advantage of the linguistically richer description ﬁeld to improve ranking effectiveness, also\nrow (2a). If we use only the keywords that are present in the description (only about half of the\nterms), SDM is able to “gain back” its lost effectiveness, row (2b). We also see from row (2b) that\nremoving stopwords and punctuation from the description decreases effectiveness with BERT–MaxP.\nThis is worth restating in another way: stopwords (that is, non-content words) contribute to ranking\neffectiveness in the input sequence fed to BERT for inference. These terms, by deﬁnition, do not\ncontribute content; instead, they provide the linguistic structure to help the model estimate relevance.\nThis behavior makes sense because BERT was pretrained on well-formed natural language text, and\nthus removing non-content words during ﬁne-tuning and inference creates distributional mismatches\nthat degrade model effectiveness.\nLooking at the narratives, which on average are over ten times longer than the title queries, we\nsee the same general pattern. 101 SDM is not effective with long narrative queries, as it becomes\n“confused” by extraneous words present that are not central to the information need, row (3a). By\nfocusing only on the keywords, SDM performs much better, but still worse than title queries, row (3b).\nRemoving negative logic has minimal impact on effectiveness compared to the full narrative queries,\nas the queries are still quite long, row (3c). For BERT–MaxP, reranking with full topic narratives\nbeats reranking with only topic titles, but this is still worse than reranking with topic descriptions,\nrow (3a). As is consistent with the descriptions case, retaining only keywords hurts effectiveness,\ndemonstrating the important role that non-content words play. For BERT, removing the negative\nlogic has negligible effect overall, just as with SDM; there doesn’t seem to be sufﬁcient evidence to\ndraw conclusions about each model’s ability to handle negations.\nTo further explore these ﬁndings, Dai and Callan [2019b] conducted some analyses of attention\npatterns in their model, similar to some of the studies discussed in Section 3.2.2, although not in a\nsystematic manner. Nevertheless, they reported a few intriguing observations: for the description\nquery “Where are wind power installations located?”, a high-scoring passage contains the sentence\n“There were 1,200 wind power installations in Germany.” Here, the preposition in the document\n“in” received the strongest attention from the term “where” in the topic description. The preposition\nappears in the phrase “in Germany”, which precisely answers a “where” question. This represents a\nconcrete example where non-content words play an important role in relevance matching: these are\nexactly the types of terms that would be discarded with exact match techniques!\nAre we able to make meaningful comparisons between Birch and BERT–MaxP based on available\nexperimental evidence? Given that they both present evaluation results on Robust04, there is\na common point for comparison. However, there are several crucial differences that make this\ncomparison difﬁcult: Birch uses BERTLarge whereas BERT–MaxP usesBERTBase. All things being\nequal, a larger (deeper) transformer model will be more effective. There are more differences:\nBERT–MaxP only reranks the topk= 100 results from ﬁrst-stage retrieval, whereas Birch reranks\nthe top k = 1000 hits. For this reason, computing MAP (at the standard cutoff of rank 1000) for\nBERT–MaxP would not yield a fair comparison to Birch; however, as nDCG is an early-precision\nmetric, it is less affected by reranking depth. Additionally, Birch combines evidence from the original\nBM25 document scores, whereas BERT–MaxP does not consider scores from ﬁrst-stage retrieval\n(cf. results of interpolation experiments in Section 3.2.2).\nFinally, there is the issue of training. Birch operates in a zero-shot transfer setting, since it was\nﬁne-tuned on the MS MARCO passage ranking test collection and TREC Microblog Track data;\nRobust04 data was used only to learn the sentence weight parameters. In contrast, the BERT–MaxP\nresults come from ﬁne-tuning directly on Robust04 data in a cross-validation setting. Obviously,\nin-domain training data should yield higher effectiveness, but the heuristic of constructing overlapping\npassages and simply assuming that they are relevant leads inevitably to noisy training examples. In\ncontrast, Birch beneﬁts from far more training examples from MS MARCO (albeit out of domain). It\nis unclear how to weigh the effects of these different training approaches.\nIn short, there are too many differences between Birch and BERT–MaxP to properly isolate and\nattribute effectiveness differences to speciﬁc design choices, although as a side effect of evaluating\nPARADE, a model we discuss in Section 3.3.4, Li et al. [2020a] presented experiment results that try\nto factor away these differences. Nevertheless, on the whole, the effectiveness of the two approaches\nis quite comparable: in terms of nDCG@20, 0.529 for BERT–MaxP with description, 0.533 for Birch\n101Here, BERT is reranking results from title queries in ﬁrst-stage retrieval.\n75\nwith three sentences (MS MARCO →MB ﬁne-tuning) reported in row (4c) of Table 10. Nevertheless,\nat a high level, the success of these two models demonstrates the robustness and simplicity of\nBERT-based approaches to text ranking. This also explains the rapid rise in the popularity of such\nmodels—they are simple, effective, and easy to replicate.\nAdditional Studies. Padaki et al. [2020] followed up the work of Dai and Callan [2019b] to explore\nthe potential of using query expansion techniques (which we cover in Section 4.2) to generate better\nqueries for BERT-based rankers. In one experiment, they scraped Google’s query reformulation\nsuggestions based on the topic titles, which were then manually ﬁltered to retain only those that were\nwell-formulated natural language questions semantically similar to the original topic descriptions.\nWhile reranking using these suggestions was not as effective as reranking using the original topic\ndescriptions, they still improved over reranking with titles (keywords) only. This offers additional\nsupporting evidence that BERT not only exploits relevance signals in well-formed natural language\nquestions, but critically depends on them to achieve maximal effectiveness.\nThe work of Dai and Callan [2019b] was successfully replicated by Zhang et al. [2021] on Robust04\nstarting from an independent codebase. They performed additional experiments evaluating BERT–\nMaxP on another dataset (Gov2) and investigated the effects of using a simple BERT variant in\nplace of BERTBase (see Section 3.2.2). The authors largely followed the experimental setup used\nin the original work, with two different design choices intended to examine the generalizability\nof the original results: a different set of folds was used and the ﬁrst-stage retrieval results were\nobtained using BM25 with RM3 expansion rather than using query likelihood. Results on Gov2 are\nin agreement with those on Robust04 using both title and description queries: MaxP aggregation\noutperformed FirstP and SumP, as well as a newly introduced AvgP variant that takes the mean of\ndocument scores.\nIn terms of BERT variants, Zhang et al. [2021] experimented with RoBERTa (introduced in Sec-\ntion 3.2.2), ELECTRA (introduced in Section 3.3.1), and another model called ALBERT [Lan et al.,\n2020]. ALBERT reduces the memory footprint of BERT by tying the weights in its transformer\nlayers together (i.e., it uses the same weights in every layer).\nResults of Zhang et al. [2021] combining MaxP aggregation with different BERT variants are shown in\nTable 14, copied directly from their paper. For convenience, we repeat the reference MaxP condition\nof Dai and Callan [2019b] from row (4b) in Table 12 as row (1). Row group (2) shows the effect\nof replacing BERT with one of its variants; none of these conditions used pre–ﬁne-tuning. While\nthese model variants sometimes outperform BERTBase, Zhang et al. [2021] found that none of the\nimprovements were statistically signiﬁcant according to a two-tailedt-test (p< 0.01) with Bonferroni\ncorrection. It is worth noting that this includes the comparison between BERTBase and BERTLarge;\nBERTBase appears to be more effective on Gov2 (although the difference is not statistically signiﬁcant\neither). Rows (3a) and (3b) focus on the comparison between BERTBase and ELECTRABase with\npre–ﬁne-tuning on the MS MARCO passage ranking task (denoted “MSM pFT”). Zhang et al. [2021]\nreported that the improvement in this case of ELECTRABase over BERTBase is statistically signiﬁcant\nin three of the four settings based on two-tailed t-test (p< 0.01) with Bonferroni correction. If we\ncombine this ﬁnding with the Birch–Passage results presented in Table 11, row (5b), there appears to\nbe multiple sources of evidence suggesting that ELECTRABase is more effective than BERTBase for\ntext ranking tasks.\nTakeaway Lessons. There are two important takeaways from the work from Dai and Callan [2019b]:\n• Simple maximum passage score aggregation—taking the maximum of all the passage relevance\nscores as the document relevance score—works well. This is a robust ﬁnding that has been\nreplicated and independently veriﬁed.\n• BERT can exploit linguistically rich descriptions of information needs that include non-content\nwords to estimate relevance, which appears to be a departure from previous keyword search\ntechniques.\nThe ﬁrst takeaway is consistent with Birch results. Conceptually, MaxP is quite similar to the “1S”\ncondition of Birch, where the score of the top sentence is taken as the score of the document. Birch\nreported at most small improvements, if any, when multiple sentences are taken into account, and no\nimprovements beyond the top three sentences. The effectiveness of both techniques is also consistent\nwith previous results reported in the information retrieval literature. There is a long thread of work,\n76\nRobust04 Gov2\nnDCG@20 nDCG@20\nModel Title Desc Title Desc\n(1) BERT–MaxP = Table 12, row (4b) 0.469 0.529 - -\n(2a) BERTBase 0.4767 0.5303 0.5175 0.5480\n(2b) BERTLarge 0.4875 0.5448 0.5161 0.5420\n(2c) ELECTRABase 0.4959 0.5480 0.4841 0.5152\n(2d) RoBERTaBase 0.4938 0.5489 0.4679 0.5370\n(2e) ALBERTBase 0.4632 0.5400 0.5354 0.5459\n(3a) BERTBase(MSM pFT) 0.4857 0.5476 0.5473 0.5788\n(3b) ELECTRABase(MSM pFT) 0.5225 † 0.5741† 0.5624 0.6062†\nTable 14: The effectiveness of different BERT variants using MaxP passage score aggregation on\nthe Robust04 and Gov2 test collections. Statistically signiﬁcant increases in effectiveness over the\ncorresponding BERTBase model are indicated with the symbol †(two-tailed t-test, p <0.01, with\nBonferroni correction).\ndating back to the 1990s, that leverages passage retrieval techniques for document ranking [Salton\net al., 1993, Hearst and Plaunt, 1993, Callan, 1994, Wilkinson, 1994, Kaszkiel and Zobel, 1997, Clarke\net al., 2000]—that is, aggregating passage-level evidence to estimate the relevance of a document.\nIn fact, both the “Max” and “Sum” aggregation techniques were already explored over a quarter\nof a century ago in Hearst and Plaunt [1993] and Callan [1994], albeit the source of passage-level\nevidence was far less sophisticated than the transformer models of today.\nAdditional evidence from user studies suggest why BERT–MaxP and Birch work well: it has been\nshown that providing users concise summaries of documents can shorten the amount of time required\nto make relevance judgments, without adversely affecting quality (compared to providing users with\nthe full text) [Mani et al., 2002]. This ﬁnding was recently replicated and expanded upon by Zhang\net al. [2018], who found that showing users only document extracts reduced both assessment time\nand effort in the context of a high-recall retrieval task. In a relevance feedback setting, presenting\nusers with sentence extracts in isolation led to comparable accuracy but reduced effort compared\nto showing full documents [Zhang et al., 2020b]. Not only from the perspective of ranking models,\nbut also from the perspective of users, well-selected short extracts serve as good proxies for entire\ndocuments for the purpose of assessing relevance. There are caveats, however: results presented\nlater in Section 3.3.5 suggest that larger portions of documents need to be considered to differentiate\nbetween different grades of relevance (e.g., relevant vs. highly relevant).\n3.3.3 Leveraging Contextual Embeddings: CEDR\nJust as in applications of BERT to classiﬁcation tasks in NLP (see Section 3.1), monoBERT, Birch,\nand BERT–MaxP use only the ﬁnal representation of the [CLS] token to compute query–document\nrelevance scores. Speciﬁcally, all of these models discard the contextual embeddings that BERT\nproduces for both the query and the candidate text. Surely, representations of these terms can also be\nuseful for ranking? Starting from this question, MacAvaney et al. [2019a] were the ﬁrst to explore\nthe use of contextual embeddings from BERT for text ranking by incorporating them into pre-BERT\ninteraction-based neural ranking models. Their approach, Contextualized Embeddings for Document\nRanking (CEDR), addressed BERT’s input length limitation by performing chunk-by-chunk inference\nover the document and then assembling relevance signals from each chunk.\nFrom the scientiﬁc perspective, MacAvaney et al. [2019a] investigated whether BERT’s contextual\nembeddings outperform static embeddings when used in a pre-BERT neural ranking model and\nwhether they are complementary to the more commonly used [CLS] representation. They hypothe-\nsized that since interaction-based models rely on the ability of the underlying embeddings to capture\nsemantic term matches, using richer contextual embeddings to construct the similarity matrix should\nimprove the effectiveness of interaction-based neural ranking models.\nSpeciﬁcally, CEDR uses one of three neural ranking models as a “base”: DRMM [Guo et al., 2016],\nKNRM [Xiong et al., 2017], and PACRR [Hui et al., 2017]. Instead of static embeddings (e.g., from\nGloVe), the embeddings that feed these models now come from BERT. In addition, the aggregate\n[CLS] representation from BERT is concatenated to the other signals consumed by the feedforward\n77\nE1E2E3\nF1 F2 Fm\n…\n…\n…\nE1E2E3\nF1 F2 Fm\n…\n…\n…\nE[CLS]\nT[CLS]\n[CLS]\nE1\nq1\nE2\nq2\nE3\nq3\nE[SEP1]\n[SEP]\nF1\nd1\nF2\nd2\nFm\ndm\nE[SEP2]\nT[SEP2]\n[SEP]…\n… … … … … … … …… …\nsE1E2E3\nF1 F2 Fm\n…\n…\n…\nU1 U2 U3 T[SEP1]V1 V2 Vm…\n…\nFigure 12: The architecture of CEDR, which comprises two main sources of relevance signals: the\n[CLS] representation and the similarity matrix computed from the contextual embeddings of the\nquery and the candidate text. This illustration contains a number of intentional simpliﬁcations in\norder to clearly convey the model’s high-level design.\nnetwork of each base model. Thus, query–document relevance scores are derived from two main\nsources: the [CLS] token (as in monoBERT, Birch, and BERT–MaxP) and from signals derived from\nquery–document term similarities (as in pre-BERT interaction-based models). This overall design is\nillustrated in Figure 12. The model is more complex than can be accurately captured in a diagram,\nand thus we only attempt to highlight high-level aspects of the design.\nTo handle inputs longer than 512 tokens, CEDR splits documents into smaller chunks, as evenly as\npossible, such that the length of each input sequence (complete with the query and special delimiter\ntokens) is not longer than the 512 token maximum. BERT processes each chunk independently and\nthe output from each chunk is retained. Once all of a document’s chunks have been processed, CEDR\ncreates a document-level [CLS] representation by averaging the [CLS] representations from each\nchunk (i.e., average pooling). The document-level [CLS] representation is then concatenated to\nthe relevance signals that are fed to the underlying interaction-based neural ranking model. Unlike\nin monoBERT, Birch, and BERT–MaxP, which discard the contextual embeddings of the query\nand candidate texts, CEDR concatenates the contextual embeddings of the document terms from\neach chunk to form the complete sequence of contextual term embeddings for the entire document.\nSimilarity matrices are then constructed by computing the cosine similarity between each document\nterm embedding and each query term embedding from the ﬁrst document chunk. Note that in this\ndesign, BERT is incorporated into interaction-based neural ranking models in a way that retains the\ndifferentiability of the overall model. This allows end-to-end training with relevance judgments and\nprovides the solution to the length limitations of BERT.\nGiven that the input size in a transformer encoder is equal to its output size, each layer in BERT can be\nviewed as producing some (intermediate) contextual representation. Rather than using only the term\nembeddings generated by BERT’s ﬁnal transformer encoder layer, CEDR constructs one similarity\nmatrix for each layer. Analogously to how the [CLS] representation is handled, the relevance signals\nfrom each matrix are concatenated together. Unlike the contextual embeddings, though, only the ﬁnal\n[CLS] representation is used. With the [CLS] representation and similarity matrix signals, CEDR\nproduces a ﬁnal document relevance score by using the same series of fully-connected layers that is\nused by the underlying base neural ranking model. In more detail:\n78\nRobust04 Web\nMethod Input Representation nDCG@20 nDCG@20\n(1) BM25 n/a 0.4140 0.1970\n(2) Vanilla BERT BERT (ﬁne-tuned) [B] 0.4541 [B] 0.2895\n(3a) PACRR GloVe 0.4043 0.2101\n(3b) PACRR BERT 0.4200 0.2225\n(3c) PACRR BERT (ﬁne-tuned) [BVG] 0.5135 [BG] 0.3080\n(3d) CEDR–PACRR BERT (ﬁne-tuned) [BVG] 0.5150 [BVGN] 0.3373\n(4a) KNRM GloVe 0.3871 [B] 0.2448\n(4b) KNRM BERT [G] 0.4318 [B] 0.2525\n(4c) KNRM BERT (ﬁne-tuned) [BVG] 0.4858 [BVG] 0.3287\n(4d) CEDR–KNRM BERT (ﬁne-tuned) [BVGN] 0.5381 [BVG] 0.3469\n(5a) DRMM GloVe 0.3040 0.2215\n(5b) DRMM BERT 0.3194 [BG] 0.2459\n(5c) DRMM BERT (ﬁne-tuned) [G] 0.4135 [BG] 0.2598\n(5d) CEDR–DRMM BERT (ﬁne-tuned) [BVGN] 0.5259 [BVGN] 0.3497\nTable 15: The effectiveness of CEDR variants on Robust04 and the test collections from the TREC\n2012–2014 Web Tracks. Signiﬁcant improvements (pairedt-tests, p< 0.05) are indicated in brackets,\nover BM25, Vanilla BERT, the corresponding model trained with GloVe embeddings, and the\ncorresponding Non-CEDR model (i.e., excluding [CLS] signals).\n• CEDR–DRMM uses a fully-connected layer with ﬁve output nodes and a ReLU non-linearity\nfollowed by a fully-connected layer with a single output node.\n• CEDR–KNRM uses one fully-connected layer with a single output node.\n• CEDR–PACRR uses two fully-connected layers with 32 output nodes and ReLU non-linearities\nfollowed by a fully-connected layer with a single output node.\nAll variants are trained using a pairwise hinge loss and initialized with BERTBase. The ﬁnal query–\ndocument relevance scores are then used to rerank a list of candidate documents.\nAs a baseline model for comparison, MacAvaney et al. [2019a] proposed what they called “Vanilla\nBERT”, which is an ablated version of CEDR that uses only the signals from the[CLS] representa-\ntions. Speciﬁcally, documents are split into chunks in exactly the same way as the full CEDR model\nand the [CLS] representations from each chunk are averaged before feeding a standard relevance\nclassiﬁer (as in monoBERT, Birch, and BERT–MaxP). This ablated model quantiﬁes the effectiveness\nimpact of the query–document term interactions.\nResults and Analysis. CEDR was evaluated using Robust04 and a non-standard combination of\ndatasets from the TREC 2012–2014 Web Tracks that we simply denote as “Web” (see Section 2.7\nand the original paper for details). Results in terms of nDCG@20 are shown in Table 15, with ﬁgures\ncopied directly from MacAvaney et al. [2019a]. CEDR was deployed as a reranker over BM25 results\nfrom Anserini, the same as Birch. However, since CEDR only reranks the top k = 100 hits (as\nopposed to k= 1000 hits in Birch), the authors did not report MAP. Nevertheless, since nDCG@20\nis an early-precision metric, the scores can be meaningfully compared. Copying the conventions used\nby the authors, the preﬁx before each result in brackets denotes signiﬁcant improvements over BM25,\nVanilla BERT, the corresponding model trained with GloVe embeddings, and the corresponding\nNon-CEDR model (i.e., excluding [CLS] signals), based on paired t-tests (p< 0.05).\nIn Table 15, each row group represents a particular “base” interaction-based neural ranking model,\nwhere the rows with the “CEDR–” preﬁx denote the incorporation of the [CLS] representations. The\n“Input Representation” column indicates whether static GloVe embeddings [Pennington et al., 2014]\nor BERT’s contextual embeddings are used. When using contextual embeddings, the original versions\nfrom BERT may be used or the embeddings may be ﬁne-tuned on the ranking task along with the\nunderlying neural ranking model. When BERT is ﬁne-tuned on the ranking task, a Vanilla BERT\nmodel is ﬁrst ﬁne-tuned before training the underlying neural ranking model. That is, BERT is ﬁrst\nﬁne-tuned in the Vanilla BERT conﬁguration for relevance classiﬁcation, and then it is ﬁne-tuned\nfurther in conjunction with a particular interaction-based neural ranking model. This is another\nexample of the multi-step ﬁne-tuning strategy discussed in Section 3.2.4.\n79\nRobust04\nMethod Conﬁguration Reference nDCG@20\nBirch 3S: BERT (MS MARCO→MB) Table 10, row (4c) 0.533\nBERT–MaxP Description Table 12, row (4b) 0.529\nCEDR–KNRM BERT (ﬁne-tuned) Table 15, row (4d) 0.538\nTable 16: The effectiveness of the best Birch, BERT–MaxP, and CEDR conﬁgurations on the\nRobust04 test collection.\nLet us examine these results. First, consider whether contextual embeddings improve over static\nGloVe embeddings: the answer is clearly yes.102 Even without ﬁne-tuning on the ranking task, BERT\nembeddings are more effective than GloVe embeddings across all models and datasets, which is likely\nattributable to their ability to better capture term context. This contrast is shown in the (b) rows vs.\nthe (a) rows. Fine-tuning BERT yields additional large improvements for most conﬁgurations, with\nthe exception of DRMM on the Web data. These results are shown in the (c) rows vs. the (b) rows.\nNext, consider the effectiveness of using only contextual embeddings in an interaction-based neural\nranking model compared to the effectiveness of using only the [CLS] representation, represented\nby Vanilla BERT in row (2). When using contextual embeddings, the PACRR and KNRM models\nperform substantially better than Vanilla BERT; see the (c) rows vs. row (2). DRMM does not appear\nto be effective in this conﬁguration, however, as shown in row (5c). This may be caused by the fact\nthat DRMM’s histograms are not differentiable, which means that BERT is ﬁne-tuned using only the\nrelevance classiﬁcation task (i.e., BERT weights are updated when Vanilla BERT is ﬁrst ﬁne-tuned,\nbut the weights are not updated when DRMM is further ﬁne-tuned). Nevertheless, there is some\nreason to suspect that the effectiveness of Vanilla BERT is under-reported, perhaps due to some\ntraining issue, because an equivalent approach by Li et al. [2020a] is much more effective (more\ndetails below).\nFinally, consider whether the [CLS] representation from BERT is complementary to the contextual\nembeddings from the remaining tokens in the input sequence. The comparison is shown in the (d)\nrows vs. the (c) rows, where CEDR–PACRR, CEDR–KNRM, and CEDR–DRMM represent the full\nCEDR model that incorporates the [CLS] representations on top of the models that use ﬁne-tuned\ncontextual embeddings. In all cases, incorporating the [CLS] representations improve effectiveness\nand the gains are signiﬁcant in the majority of cases.\nA natural question that arises is how CEDR compares to Birch (Section 3.3.1) and BERT–MaxP\n(Section 3.3.2), the two other contemporaneous models in the development of BERT for ranking\nfull documents. Fortunately, all three models were evaluated on Robust04 and nDCG@20 was\nreported for those experiments, which offers a common reference point. Table 16 summarizes the\nbest conﬁguration of each model. While the experimental setups are different, which prevents a fair\ndirect comparison, we can see that the effectiveness scores all appear to be in the same ballpark. This\npoint has already been mentioned in Section 3.3.2 but is worth repeating: it is quite remarkable that\nthree ranking models with different designs, by three different research groups with experiments\nconducted on independent implementations, all produce similar results. This provides robust evidence\nthat BERT does really “work” for text ranking.\nThe connection between Birch and BERT–MaxP has already been discussed in the previous section,\nbut both models are quite different from CEDR, which has its design more ﬁrmly rooted in pre-BERT\ninteraction-based neural ranking models. Speciﬁcally, Birch and BERT–MaxP are both entirely\nmissing the explicit similarity matrix between query–document terms that forms a central component\nin CEDR, and instead depend entirely on the [CLS] representations. The CEDR experiments\nunequivocally show that contextual embeddings from BERT improve the quality of the relevance\nsignals extracted from interaction-based neural ranking models and increase ranking effectiveness,\nbut the experiments are not quite so clear on whether the explicit interactions are necessary to begin\nwith. In fact, there is evidence to suggest that with BERT, explicit interactions are not necessary:\nfrom the discussion in Section 3.2, it might be the case that BERT’s all-to-all attention patterns at\neach transformer layer, in effect, already capture all possible term interactions.\n102Apart from contextualization, GloVe embeddings also differ in that some terms may be out-of-vocabulary.\nMacAvaney et al. [2019a] attempted to mitigate this issue by ensuring that terms always have a similarity of\none with themselves.\n80\nRobust04\nNo interpolation with BM25 + RM3\nnDCG@20 nDCG@20\nMethod Title Desc Title Desc\n(1) BM25 0.4240 0.4058 - -\n(2) BM25 + RM3 0.4514 0.4307 - -\n(3a) KNRM w/ FT BERTBase(no pFT) = Table 15, row (3c) 0.4858 - - -\n(3b) CEDR–KNRM w/ FT BERTBase(no pFT) = Table 15, row (3d) 0.5381 - - -\n(4a) KNRM w/ FT ELECTRABase(MSM pFT) 0.5470 0.6113 - -\n(4b) CEDR–KNRM w/ FT ELECTRABase(MSM pFT) 0.5475 0.5983 - -\n(5a) KNRM w/ FT BERTBase(no pFT) 0.5027 †‡ 0.5409†‡ 0.5183†‡ 0.5532†‡\n(5b) KNRM w/ FT ELECTRABase(no pFT) 0.5505 0.5954 0.5454 0.6016\n(6a) CEDR–KNRM w/ FT BERTBase(no pFT) 0.5060 †‡ 0.5661†‡ 0.5235†‡ 0.5798†‡\n(6b) CEDR–KNRM w/ FT ELECTRABase(no pFT) 0.5326 0.5905 0.5536 0.6010\nTable 17: The effectiveness of CEDR variants on the Robust04 test collection using title and\ndescription queries with and without BM25 + RM3 interpolation. In rows (5a) and (6a), statistically\nsigniﬁcant decreases in effectiveness from row (5b) and row (6b) are indicated with the symbol †and\nthe symbol ‡, respectively (two-tailed paired t-test, p< 0.05, with Bonferroni correction).\nAdditional Studies. As already noted above, the Vanilla BERT experimental results by MacAvaney\net al. [2019a] are not consistent with follow-up work reported by Li et al. [2020a] (more details in\nthe next section). Researchers have also reported difﬁculties reproducing results for CEDR–KNRM\nand ablated variants using the authors’ open-source code withBERTBase.103 In response, the CEDR\nauthors have recommended resolving these issues by replacing BERTBase with ELECTRABase and\nalso adopting the Capreolus toolkit [Yates et al., 2020] as the reference implementation of CEDR.\nFurther experiments by Li et al. [2020a] with Capreolus have conﬁrmed that CEDR is effective when\ncombined with ELECTRABase, but they have not afﬁrmed the ﬁnding by MacAvaney et al. [2019a]\nthat the [CLS] token is complementary to the contextual embeddings.\nExperimental results copied from Li et al. [2020a] are shown in rows (4a) and (4b) of Table 17.\nComparing these rows, there does not appear to be any beneﬁt to using the [CLS] token with title\nqueries, and using the [CLS] token actually reduces effectiveness with description queries. Note that\nthe results in row groups (3) and (4) are not comparable because the latter conﬁgurations have the\nadditional beneﬁt of pre–ﬁne-tuning on the MS MARCO passage dataset, indicated by “MSM pFT”.\nTo better understand the reproduction difﬁculties with the CEDR codebase, we replicated some of\nthe important model conﬁgurations using the Capreolus toolkit [Yates et al., 2020] to obtain new\nresults with the different CEDR–KNRM conditions; these results have to date not been reported\nelsewhere. In particular, we consider the impact of linear interpolation with the ﬁrst-stage retrieval\nscores, the impact of using different BERT variants, and the impact of using title vs. description\nqueries. These experiments used the same ﬁrst-stage ranking, folds, hyperparameters, and codebase\nas Li et al. [2020a], allowing meaningful comparisons. Results are shown in row groups (5) and (6)\nin Table 17 and are directly comparable to the results in row group (4), but note that these results do\nnot beneﬁt from pre–ﬁne-tuning.\nWe can view row (5a) as a replication attempt of the CEDR results in row (3a), and row (6a) as a\nreplication attempt of the CEDR results in row (3b), since the latter in each pair of comparisons is\nbased on an independent implementation. The results do appear to conﬁrm the reported issues with\nreproducing CEDR using the original codebase by MacAvaney et al. However, this concern also\nappears to be assuaged by the authors’ recommendation of replacing BERT with ELECTRA. While\nthe original CEDR paper found that including the [CLS] token improved over using only contextual\nembeddings, row (3a) vs. (3b), the improvement is inconsistent in our replication, as seen in row (5a)\nvs. (6a) and row (5b) vs. (6b).\nThus, to be clear, our results here support the ﬁnding by MacAvaney et al. that incorporating contextual\nembeddings in a pre-BERT interaction-based model can be effective (i.e., outperforms non-contextual\nembeddings), but our experiments do not appear to support the ﬁnding that incorporating the [CLS]\ntoken further improves effectiveness. Comparing row (5a) with (5b) and row (6a) with (6b) in\n103See https://github.com/Georgetown-IR-Lab/cedr/issues/22.\n81\nTable 17, we see that variants usingELECTRABase consistently outperform those using BERTBase.\nMoreover, considering the results reported by Li et al. [2020a], in row group (4), we see that the\nimprovements from pre–ﬁne-tuning are less consistent than those reported by Zhang et al. [2021]\n(see Section 3.3.2). Pre–ﬁne-tuning ELECTRA–KNRM slightly reduces effectiveness on descripton\nqueries but improves effectiveness on title queries, row (4a) vs. row (5b). CEDR–KNRM beneﬁts\nfrom pre–ﬁne-tuning with both query types, but the improvement is larger for title queries, rows (4b)\nand (6b). With the exception of row (5b), interpolating the reranker’s retrieval scores with scores\nfrom ﬁrst-stage retrieval improves effectiveness.\nTakeaway Lessons. Despite some lack of clarity in the experimental results presented by MacAvaney\net al. [2019a] in being able to unequivocally attribute effectiveness gains to different architectural\ncomponents of the overall ranking model, CEDR to our knowledge is the ﬁrst end-to-enddifferentiable\nBERT-based ranking model for full-length documents. While Birch and BERT–MaxP could have\nbeen modiﬁed to be end-to-end differentiable—for example, as Li et al. [2020a] have done with\nBirch–Passage, presented in Section 3.3.1—neither Akkalyoncu Yilmaz et al. [2019b] nor Dai and\nCallan [2019b] made this important leap. This strategy of handling long documents by aggregating\ncontextual term embeddings was later adopted by Boytsov and Kolter [2021]. The CEDR design has\ntwo important advantages: the model presents a principled solution to the length limitations of BERT\nand allows uniform treatment of both training and inference (reranking). Our replication experiments\nconﬁrm the effectiveness of using contextual embeddings to handle ranking long texts, but the role of\nthe [CLS] token in the complete CEDR architecture is not quite clear.\n3.3.4 Passage Representation Aggregation: PARADE\nPARADE [Li et al., 2020a], which stands for Passage Representation Aggregation for Document\nReranking, is a direct descendant of CEDR that also incorporates lessons learned from Birch and\nBERT–MaxP. The key insight of PARADE, building on CEDR, is to aggregate therepresentations of\npassages from a long text rather than aggregating the scores of individual passages, as in Birch and\nBERT–MaxP. As in CEDR, this design yields an end-to-end differentiable model that can consider\nmultiple passages in unison, which also uniﬁes training and inference. However, PARADE abandons\nCEDR’s connection to pre-BERT neural ranking models by discarding explicit term-interaction\nsimilarity matrices. The result is a ranking model that is simpler than CEDR and generally more\neffective.\nMore precisely, PARADE is a family of models that splits a long text into passages and performs\nrepresentation aggregation on the [CLS] representation from each passage. Speciﬁcally, PARADE\nsplits a long text into a ﬁxed number of ﬁxed-length passages. When texts contain fewer passages,\nthe passages are padded and masked out during representation aggregation. When texts contain more\npassages, the ﬁrst and last passages are always retained, but the remaining passages are randomly\nsampled. Consecutive passages partially overlap to minimize the chance of separating relevant\ninformation from its context.\nA passage representation pcls\ni is computed for each passage Pi given a query qusing a pretrained\ntransformer encoder:\npcls\ni = ELECTRABase(q,Pi) (21)\nNote that instead of BERT, the authors opted to use ELECTRA [Clark et al., 2020b] with pre–\nﬁne-tuning on the MS MARCO passage ranking test collection (which Zhang et al. [2021] also\nexperimented with in their investigation of MaxP, described in Section 3.3.2). The six PARADE\nvariants proposed by Li et al. [2020a] each take a sequence of passage representations pcls\n1 ,...,p cls\nn\nas input and aggregate them to produce a document representation dcls. In more detail, they are as\nfollows, where dcls[i] refers to the i-th component of the dcls vector:\n• PARADEAvg performs average pooling across passage representations. That is,\ndcls[i] = avg(pcls\n1 [i],...,p cls\nn [i]). (22)\n• PARADESum performs additive pooling across passage representations. That is,\ndcls[i] =\nn∑\nj=0\npcls\nj [i]. (23)\n82\nE’[CLS]\nT’[CLS]\ns\n… … … … …\nP1 P2 Pm\npnCLSp1CLSp2CLS\n…\n…\n…\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nq1\nE2\nU2\nq2\nE3\nU3\nq3\nE[SEP1]\nT[SEP1]\n[SEP]\nF1\nV1\nd1\nF2\nV2\nd2\nFm\nVm\ndm\nE[SEP2]\nT[SEP2]\n[SEP]\n…\n…\n… … … … … … … …… …\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nq1\nE2\nU2\nq2\nE3\nU3\nq3\nE[SEP1]\nT[SEP1]\n[SEP]\nF1\nV1\nd1\nF2\nV2\nd2\nFm\nVm\ndm\nE[SEP2]\nT[SEP2]\n[SEP]\n…\n…\n… … … … … … … …… …\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nq1\nE2\nU2\nq2\nE3\nU3\nq3\nE[SEP1]\nT[SEP1]\n[SEP]\nF1\nV1\nd1\nF2\nV2\nd2\nFm\nVm\ndm\nE[SEP2]\nT[SEP2]\n[SEP]\n…\n…\n… … … … … … … …… …\n…\nFigure 13: The architecture of the full PARADE model, showing the [CLS] representations from\neach passage, which are aggregated by another transformer to produce the ﬁnal relevance score. Note\nthat the [CLS] token of the upper transformer is not the same as the [CLS] token of BERT.\n• PARADEMax performs max pooling across passage representations. That is,\ndcls[i] = max(pcls\n1 [i],...,p cls\nn [i]). (24)\n• PARADEAttn computes a weighted average of the passage representations by using a feedfor-\nward network to produce an attention weight for each passage. That is,\nw1,...,w n = softmax(W ·pcls\n1 ,...,W ·pcls\nn ), (25)\ndcls =\nn∑\ni=1\nwi ·pcls\ni . (26)\n• PARADECNN uses a stack of convolutional neural networks (CNNs) to repeatedly aggregate\npairs of passage representations until only one representation remains; weights are shared\nacross all CNNs. Each CNN takes two passage representations as input and produces a single,\ncombined representation as output. That is, the CNNs have a window size of two, a stride of two,\nand a number of ﬁlters equal to the number of dimensions in a single passage representation.\nThe number of passages used as input, which is a hyperparameter, must be a power of two in\norder for the model to generate a single representation after processing. PARADECNN produces\none relevance score sj for each CNN layer. Let mj be the number of representations after the\nj-th CNN, m0 = n, and r0\ni = pcls\ni . Then,\nrj\n1,...,r j\nmj = CNN(rj−1\n1 ,...,r j−1\nmj−1 ), (27)\nsj = max(FFN(rj\n1),..., FFN(rj\nmj )). (28)\n• PARADE (i.e., the full model, or PARADETransformer) aggregates the passage representations\nusing a small stack of two randomly-initialized transformer encoders that take the passage\nrepresentations as input. Similar to BERT, a [CLS] token (although with its own token em-\nbedding different from BERT’s) is prepended to the passage representations that are fed to\nthe transformer encoder stack; there is, however, no comparable[SEP] token for terminating\nthe sequence. The [CLS] output representation of the ﬁnal transformer encoder is used as the\ndocument representation dcls:\ndcls,d1,...,d n = TransformerEncoder2(TransformerEncoder1([CLS],pcls\n1 ,...,p cls\nn )). (29)\nThe architecture of the full PARADE model is shown in Figure 13.\n83\nNote that the ﬁrst four approaches treat each dimension of the passage representation as an indepen-\ndent feature. That is, pooling is performed across passage representations. With all variants except for\nPARADECNN, the ﬁnal document representation dcls is fed to a fully-connected layer with two output\nnodes that then feeds a softmax to produce the ﬁnal relevance score. In the case of PARADECNN, the\nﬁnal relevance score is the sum of the CNN scoressj and the maximum passage score s0. This makes\nPARADECNN more interpretable than the full PARADE model because each document passage is\nassociated with a relevance score (similar to MaxP, Birch, and BERT-KNRM). All PARADE variants\nare trained end-to-end.\nPARADE’s approach follows a line of prior work on hierarchical modeling of natural language\ntext, which, to our knowledge, began in the context of deep learning with Hierarchical Attention\nNetworks (HANs) for document classiﬁcation [Yang et al., 2016]. Their architecture uses two layers\nof RNNs to model text at the word level and at the sentence level. Jiang et al. [2019] extended this\nbasic strategy to three levels (paragraphs, sentences, and words) and applied the resulting model to\nsemantic text matching of long texts. PARADE’s approach is most similar to that by Liu and Lapata\n[2019] and Zhang et al. [2019], who proposed a hierarchical transformer for document classiﬁcation.\nNevertheless, to our knowledge, PARADE represents the ﬁrst application of hierarchical models to\nad hoc retrieval.\nResults and Analysis. Li et al. [2020a] evaluated the PARADE models on the Robust04 and Gov2\ntest collections using both title (keyword) and description (sentence) queries. Each PARADE model\nwas built on top of ELECTRABase, which was pre–ﬁne-tuned on the MS MARCO passage ranking\ntask. The entire model was then trained on the target test collection using cross-validation. Both the\nunderlying ELECTRA model and the full PARADE models used pairwise hinge loss during training.\nDocuments were split into passages of 225 terms with a stride of 200 terms. The maximum number\nof passages per document was set to 16. Candidate documents for each query were obtained with\nBM25 + RM3 using Anserini, and the top k= 1000 documents were reranked. However, note that\nthe ﬁnal results do not include interpolation with scores from ﬁrst-stage retrieval.\nResults copied from Li et al. [2020a] are shown in Table 18 for Robust04 and Table 19 for Gov2.\nWe refer the reader to the original paper for additional experiments, which include investigations of\nthe impact of the underlying BERT model used and the number of candidate documents reranked.\nIn order to evaluate the impact of passage representation aggregation, the PARADE models were\ncompared with ELECTRA–MaxP (i.e., BERT–MaxP built on top of ELECTRABase) and Birch,\nwhich both aggregate passage scores, and CEDR, which aggregates term representations. Li et al.\n[2020a] reported results on the improved Birch–Passage variant (described in Section 3.3.1) in row (3)\nthat takes passages rather than sentences as input and is ﬁne-tuned end-to-end on the target dataset.104\nLike the PARADE variants, the ELECTRA–MaxP, Birch–Passage, and CEDR models shown in rows\n(3), (4), and (5) are built on top of an ELECTRABase model that has already been ﬁne-tuned on MS\nMARCO. The CEDR–KNRM model uses “max” rather than “average” aggregation to combine the\n[CLS] representations, which the authors found to perform slightly better. Statistically signiﬁcant\ndifferences between the full PARADE model (i.e., PARADETransformer) and other methods based on\npaired t-tests (p< 0.05) are indicated by the symbol †next to the scores.\nWe see that, in general, ranking effectiveness increases with more sophisticated representation\naggregation approaches. The experimental results suggest the following conclusions:\n• PARADE (6f), which performs aggregation using transformer encoders, and PARADECNN (6e)\nare consistently the most effective across different metrics, query types, and test collections.\nPARADECNN usually performs slightly worse than the full PARADE model, but the differences\nare not statistically signiﬁcant.\n• PARADEAvg (6a) is usually the least effective.\n• PARADESum (6b) and PARADEAttn (6d) perform similarly; PARADESum is slightly more\neffective on Robust04 and PARADEAttn is slightly more effective on Gov2. PARADESum can\nbe viewed as PARADEAttn with uniform attention weights, so this result suggests that the\nattention scores produced by PARADEAttn may not be necessary.\n• PARADEMax outperforms both PARADESum and PARADEAttn on Robust04, but its effective-\nness varies on Gov2; MAP is higher than both but nDCG@20 is lower than both.\n104This approach could also be considered an end-to-end “ELECTRA–KMaxP”.\n84\nRobust04\nTitle Description\nMethod MAP nDCG@20 MAP nDCG@20\n(1) BM25 0.2531 † 0.4240† 0.2249† 0.4058†\n(2) BM25 + RM3 0.3033 † 0.4514† 0.2875† 0.4307†\n(3) Birch–Passage = Table 11, row (4) 0.3763 0.5454 † 0.4009† 0.5931†\n(4) ELECTRA–MaxP 0.3183 † 0.4959† 0.3464† 0.5540†\n(5a) ELECTRA–KNRM = Table 17, row (4a) 0.3673† 0.5470† 0.4066 0.6113\n(5b) CEDR–KNRM = Table 17, row (4b) 0.3701† 0.5475† 0.4000† 0.5983†\n(6a) PARADEAvg 0.3352† 0.5124† 0.3640† 0.5642†\n(6b) PARADESum 0.3526† 0.5385† 0.3789† 0.5878†\n(6c) PARADEMax 0.3711† 0.5442† 0.3992† 0.6022\n(6d) PARADEAttn 0.3462† 0.5266† 0.3797† 0.5871†\n(6e) PARADECNN 0.3807 0.5625 0.4005 † 0.6102\n(6f) PARADE 0.3803 0.5659 0.4084 0.6127\nTable 18: The effectiveness of PARADE variants on the Robust04 test collection using title and\ndescription queries. Statistically signiﬁcant differences in effectiveness between a given method and\nthe full PARADE model are indicated with the symbol †(two-tailed paired t-test, p< 0.05).\nGov2\nTitle Description\nMethod MAP nDCG@20 MAP nDCG@20\n(1) BM25 0.3056 † 0.4774† 0.2407† 0.4264†\n(2) BM25 + RM3 0.3350 † 0.4851† 0.2702† 0.4219†\n(3) Birch–Passage = Table 11, row (4) 0.3406 † 0.5520† 0.3270 0.5763 †\n(4) ELECTRA–MaxP = Table 14, row (6) 0.3193† 0.5265† 0.2857† 0.5319†\n(5a) ELECTRA–KNRM = Table 17, row (4a) 0.3469† 0.5750† 0.3269 0.5864 †\n(5b) CEDR–KNRM = Table 17, row (4b) 0.3481† 0.5773† 0.3354† 0.6086\n(6a) PARADEAvg 0.3174† 0.5741† 0.2924† 0.5710†\n(6b) PARADESum 0.3268† 0.5747† 0.3075† 0.5879†\n(6c) PARADEMax 0.3352† 0.5636† 0.3160† 0.5732†\n(6d) PARADEAttn 0.3306† 0.5864† 0.3116† 0.5990\n(6e) PARADECNN 0.3555† 0.6045 0.3308 0.6169\n(6f) PARADE 0.3628 0.6093 0.3269 0.6069\nTable 19: The effectiveness of PARADE models on the Gov2 test collection using title and description\nqueries. Statistically signiﬁcant differences in effectiveness between a given method and the full\nPARADE model are indicated with the symbol †(two-tailed paired t-test, p< 0.05).\nCompared to the baselines, the full PARADE model and PARADECNN consistently outperforms\nELECTRA–MaxP, row (4), and almost always outperforms Birch, row (3), and CEDR, row (5).\nIn addition to providing a point of comparison for PARADE, these experiments also shed additional\ninsight about differences between Birch, ELECTRA–MaxP, and CEDR in the same experimental\nsetting. Here, it is worth spending some time discussing these results, independent of PARADE.\nConﬁrming the ﬁndings reported by Dai and Callan [2019b], the effectiveness of all models increases\nwhen moving from Robust04 title queries to description queries. However, the results are more mixed\non Gov2, and description queries do not consistently improve across metrics. ELECTRA–KNRM\n(5a) and CEDR–KNRM (5b) are comparable in terms of effectiveness to Birch–Passage on Robust04\nbut generally better on Gov2. All Birch and CEDR variants are substantially more effective than\nELECTRA–MaxP, providing further support for the claim that considering multiple passages from a\nsingle document passage can improve relevance predictions.\nTakeaway Lessons. We see two main takeaways from PARADE, both building on insights initially\ndemonstrated by CEDR: First, aggregating passage representations appears to be more effective than\naggregating passages scores. By the time a passage score is computed, a lot of the relevance signal\nhas already been “lost”. In contrast, passage representations are richer and thus allow higher-level\ncomponents to make better decisions about document relevance. Second, chunking a long text and\nperforming chunk-level inference can be an effective strategy to addressing the length restrictions of\n85\nBERT. In our opinion, this approach is preferable to alternative solutions that try to directly increase\nthe maximum length of input sequences to BERT [Tay et al., 2020] (see next section). The key to\nchunk-wise inference lies in properly aggregating representations that emerge from inference over the\nindividual chunks. Pooling, particularly max pooling, is a simple and effective technique, but using\nanother transformer to aggregate the individual representations appears to be even more effective,\nsuggesting that there are rich signals present in the sequence of chunk-level representations. This\nhierarchical approach to relevance modeling retains the important model property of differentiability,\nenabling the uniﬁcation of training and inference.\n3.3.5 Alternatives for Tackling Long Texts\nIn addition to aggregating passage scores or representations, two alternative strategies have been\nproposed for ranking long texts: making use of passage-level relevance labels and modifying the\ntransformer architecture to consume long texts more efﬁciently. We discuss both approaches below.\nPassage-level relevance labels. As an example of the ﬁrst strategy, Wu et al. [2020b] considered\nwhether having graded passage-level relevance judgments at training time can lead to a more ef-\nfective ranking model. This approach avoids the label mismatch at training time (for example,\nwith MaxP) since passage-level judgments are used. To evaluate whether this approach improves\neffectiveness, the authors annotated a corpus of Chinese news articles with passage-level cumulative\ngain, deﬁned as the amount of relevant information a reader would encounter after having read a\ndocument up to a given passage. Here, the authors operationalized passages as paragraphs. The\ndocument-level cumulative gain is then, by deﬁnition, the highest passage-level cumulative gain,\nwhich is the cumulative gain reached after processing the entire document. Based on these human\nannotations, Wu et al. [2020b] made the following two observations:\n• On average, highly-relevant documents are longer than other types of documents, measured\nboth in terms of the number of passages and the number of words.\n• The higher the document-level cumulative gain, the more passages that need to be read by a\nuser before the passage-level cumulative gain reaches the document-level cumulative gain.\nThese ﬁndings suggest that whether a document is relevant can be accurately predicted from its\nmost relevant passage—which is consistent with BERT–MaxP and Birch, as well as the user studies\ndiscussed in Section 3.3.2. However, to accurately distinguish between different relevance grades (e.g.,\nrelevant vs. highly-relevant), a model might need to accumulate evidence from multiple passages,\nwhich suggests that BERT–MaxP might not be sufﬁcient. Intuitively, the importance of observing\nmultiple passages is related to how much relevance information accumulates across the full document.\nTo make use of their passage-level relevance labels, Wu et al. [2020b] proposed the Passage-level\nCumulative Gain model (PCGM), which begins by applying BERT to obtain individual query–passage\nrepresentations (i.e., the ﬁnal representation of the [CLS] token). The sequence of query–passage\nrepresentations is then aggregated with an LSTM, and the model is trained to predict the cumulative\ngain after each passage. An embedding of the previous passage’s predicted gain is concatenated to\nthe query–passage representation to complete the model. At inference time, the gain of a document’s\nﬁnal passage is used as the document-level gain. One can think of PCGM as a principled approach to\naggregating evidence from multiple passages, much like PARADE, but adds the requirement that\npassage-level gain labels are available. PCGM has two main advantages: the LSTM is able to model\nand extract signal from the sequence of passages, and the model is differentiable and thus amenable\nto end-to-end training.\nThe PCGM model was evaluated on two Chinese test collections. While experimental results\ndemonstrate some increase in effectiveness over BERT–MaxP, the increase was not statistically\nsigniﬁcant. Unfortunately, the authors did not evaluate on Robust04, and thus a comparison to other\nscore and passage aggregation approaches is difﬁcult. However, it is unclear whether the lack of\nsigniﬁcant improvements is due to the design of the model, the relatively small dataset, or some\nissue with the underlying observations about passage-level gains. Nevertheless, the intuitions of Wu\net al. [2020b] in recognizing the need to aggregate passage representations do appear to be valid, as\nsupported by the experiments with PARADE in Section 3.3.4.\nTransformer architectures for long texts. Researchers have proposed a variety of techniques to\ndirectly apply the transformer architecture to long documents by reducing the computational cost of\n86\nMS MARCO Doc(Dev) TREC 2019 DL Doc\nMethod MRR@10 nDCG@10 MAP\n(1) Birch (BM25 + RM3) - 0.640 0.328\n(2) Sparse-Transformer 0.328 0.634 0.257\n(3) Longformer-QA 0.326 0.627 0.255\n(4) QDS-Transformer 0.360 0.667 0.278\nTable 20: The effectiveness of efﬁcient transformer variants on the development set of the MS\nMARCO document ranking task and the TREC 2019 Deep Learning Track document ranking test\ncollection.\nits attention mechanism, which is quadratic with respect to the sequence length (see discussion in\nSection 3.3).\nKitaev et al. [2020] proposed the Reformer, which replaces standard dot-product attention by a design\nbased on locality-sensitive hashing to efﬁciently compute attention only against the most similar\ntokens, thus reducing model complexity from O( L2) to O(Llog L), where Lis the length of the\nsequence. Another solution, dubbed Longformer by Beltagy et al. [2020], addressed the blow-up in\ncomputational costs by sparsifying the all-to-all attention patterns in the basic transformer design\nthrough the use of a sliding window to capture local context and global attention tokens that can be\nspeciﬁed for a given task. Researchers have begun to apply Longformer-based models to ranking\nlong texts [Sekuli´c et al., 2020, Jiang et al., 2020].\nJiang et al. [2020] proposed the QDS-Transformer, which is a Longformer model where the query\ntokens are global attention tokens (i.e., each query term attends to all query and document terms). The\nauthors evaluated the QDS-Transformer on the MS MARCO document ranking test collection and on\nthe TREC 2019 Deep Learning Track document ranking test collection where they reranked the BM25\nresults provided by the track organizers. QDS-Transformer was compared against Longformer-QA,\nwhich adds a special token to the query and document for global attention, as proposed by Beltagy\net al. [2020], and Sparse-Transformer [Child et al., 2019], which uses local attention windows with\nno global attention.\nExperimental results are shown in Table 20. The Sparse-Transformer and Longformer-QA models\nperform similarly, rows (2) and (3), suggesting that the global token approach used by Longformer-\nQA does not represent an improvement over the local windows used by Sparse-Transformer. QDS-\nTransformer, row (4), outperforms both approaches, which suggests that treating the query tokens as\nglobal attention tokens is important. For context, we present the closest comparable Birch condition\nwe could ﬁnd in row (1); this corresponds to run bm25_marcomb submitted to the TREC 2019 Deep\nLearning Track [Craswell et al., 2020], which reranked the top 1000 hits from BM25 + RM3 as\nﬁrst-stage retrieval. The higher MAP of Birch is likely due to a deeper reranking depth, but the\neffectiveness of QDS-Transformer is only a little bit higher. For Robust04, Jiang et al. [2020] reported\nan nDCG@20 of 0.457, which is far lower than many of the ﬁgures reported in this section. Although\nthere aren’t sufﬁcient common reference points, taken as a whole, it is unclear if QDS-Transformer is\ntruly competitive compared to many of the models discussed earlier.\nTakeaway Lessons. While replacing all-to-all attention lowers the computational complexity in the\nalternative transformer architectures discussed in this section, it is not clear whether they can match\nthe effectiveness of reranking methods based either on score or representation aggregation. Note that\nthe strategy of sparsifying attention patterns leads down the road to an architecture that looks quite\nlike PARADE. In PARADE’s hierarchical model, a second lightweight transformer is applied to the\n[CLS] representations from the individual passages, but this design is operationally identical to a\ndeeper transformer architecture where the top few layers adopt a special attention pattern (e.g., via\nmasking). In fact, we might go as far to say that hierarchical transformers and selective sparsiﬁcation\nof attention are two ways of describing the same idea.\n3.4 From Single-Stage to Multi-Stage Rerankers\nThe applications of BERT to text ranking that we have covered so far operate as rerankers in a\nretrieve-and-rerank setup, which as we have noted dates back to at least the 1960s [Simmons, 1965].\n87\nInverted Index\nInitial RetrievalTexts Ranked ListCandidate Texts\nQueries\nReranker\nInverted Index\nInitial RetrievalTexts Candidate Texts\nQueries\nRerankerRerankedCandidates\nReranker\nReranker… Ranked List\nFigure 14: A retrieve-and-rerank design (top) is the simplest instantiation of a multi-stage ranking\narchitecture (bottom). In multi-stage ranking, the candidate generation stage (also called initial\nretrieval or ﬁrst-stage retrieval) is followed by more than one reranking stages.\nAn obvious extension of this design is to incorporate multiple reranking stages as part of a multi-stage\nranking architecture, as shown in Figure 14. That is, following candidate generation or ﬁrst-stage\nretrieval, instead of having just a single reranker, a system could have an arbitrary number of reranking\nstages, where the output of each reranker feeds the input to the next. This basic design goes by a few\nother names as well: reranking pipelines, ranking cascades, or “telescoping”.\nWe formalize the design as follows: a multi-stage ranking architecture comprises N reranking stages,\ndenoted H1 to HN. We refer to the candidate generation stage (also called initial retrieval or ﬁrst-stage\nretrieval) as H0, which retrieves k0 texts from the corpus to feed the rerankers. Candidate generation\nis typically accomplished using an inverted index, but may exploit dense retrieval techniques or\ndense–sparse hybrids as well (see Section 5). Each stage Hn,n ∈{1,...N }receives a ranked list\nRn−1 comprising kn−1 candidates from the previous stage. Each stage, in turn, provides a ranked\nlist Rn comprising kn candidates to the subsequent stage, with the requirement that kn ≤kn−1.105\nThe ranked list generated by the ﬁnal stage HN is the output of the multi-stage ranking architecture.\nThis description intentionally leaves unspeciﬁed the implementation of each reranking stage, which\ncould be anything ranging from decisions made based on the value of a single hand-crafted feature\n(known as a “decision stump”) to a sophisticated machine-learned model (for example, based on\nBERT). Furthermore, each stage could decide how to take advantage of scores from the previous\nstage: one common design is that scores from each stage are additive, or a reranker can decide to\ncompletely ignore previous scores, treating the previous candidate texts as an unordered set.\nOne practical motivation for the development of multi-stage ranking is to better balance tradeoffs\nbetween effectiveness (most of the time, referring to the quality of the ranked lists) and efﬁciency\n(for example, retrieval latency or query throughput). Users, of course, demand systems that are both\n“good” and “fast”, but in general, there is a natural tradeoff between these two desirable characteristics.\nMulti-stage ranking evolved in the context of learning to rank (see Section 1.2.3): For example,\ncompared to unigram features (i.e., of individual terms) such as BM25 scores, many n-gram features\nare better signals of relevance, but also more computationally expensive to compute, in both time and\nspace. To illustrate: one helpful feature is the count of query n-grams that occur in a text (that is, the\nranking model checks whether matching query terms are contiguous). This is typically accomplished\nby storing the positions of terms in the text (which consumes space) and intersecting lists of term\npositions (within individual documents) to determine whether the terms appear contiguously (which\ntakes time). Thus, we see a common tradeoff between feature cost and output quality, and more\ngenerally, between effectiveness and efﬁciency.\n105We leave aside a minor detail here in that a stage can return a ranked list of a particular length, and the next\nstage may choose to truncate that list prior to processing. The net effect is the same; a single parameter kn is\nsufﬁcient to characterize such a design.\n88\nThus, a ranking model (e.g., learning to rank) that takes advantage of “expensive” features will often\nbe slow, since inference must be performed on every candidate. Latency increases linearly with the\nnumber of candidates considered and can be managed by varying the depth of ﬁrst-stage retrieval,\nmuch like the experiments presented in Section 3.2.2 in the context of monoBERT. However, it is\ndesirable that the candidate pool contains as many relevant texts as possible (i.e., have high recall), to\nmaximize the opportunities for a reranker to identify relevant texts; obviously, rerankers are useless\nif there are no relevant texts in the output of ﬁrst-stage retrieval to process. Thus, designers of\nproduction real-world systems are faced with an effectiveness/efﬁciency tradeoff.\nThe intuition behind the multi-stage design is to exploit expensive features only when necessary:\nearlier stages in the reranking pipeline can use “cheap” features to discard candidates that are easy to\ndistinguish as not relevant; “expensive” features can then be brought to bear after the “easy” non-\nrelevant candidates have been discarded. Latency can be managed because increasingly expensive\nfeatures are computed on fewer and fewer candidates. Furthermore, reranking pipelines can exploit\n“early exits” that bypass later stages if the results are “good enough” [Cambazoglu et al., 2010]. In\ngeneral, the multi-stage design provides system designers with tools to balance effectiveness and\nefﬁciency, often leading to systems that are both “good” and “fast”.106\nThe development of this idea in modern times has an interesting history. It had been informally known\nby many in the information retrieval community since at least the mid-2000s that Microsoft’s Bing\nsearch engine adopted a multi-stage design; for one, it was the most plausible approach for deploying\nthe learning-to-rank models they were developing at the time [Burges et al., 2005]. However, the\nearliest “ofﬁcial” public acknowledgment we are aware of appears to be in a SIGIR 2010 Industry\nTrack keynote by Jan Pedersen, whose presentation included a slide that explicitly showed this\nmulti-stage architecture. Bing named these stages “L0” through “L4”, with “L0” being “Boolean\nlogic” (understood to be conjunctive query processing, i.e., the “ANDing” of query terms), “L1”\nbeing “IR score” (understood to be BM25), and “L2/L3/L4” being machine-learned models. Earlier\nthat year, a team of authors from Yahoo! [Cambazoglu et al., 2010] described a multi-stage ranking\narchitecture in the form of with additive ensembles (the score of each stage is added to the score of\nthe previous stages). However, the paper did not establish a clear connection to production systems.\nIn the academic literature, Matveeva et al. [2006] described the ﬁrst known instance of multi-stage\nranking (“nested” rankers, as the authors called it). The term “telescoping” was used to describe\nthe pruning process where candidates were discarded between stages. Interestingly, the paper was\nmotivated by high-accuracy retrieval and did not discuss the implications of their techniques on\nsystem latency. Furthermore, while four of the ﬁve co-authors were afﬁliated with Bing, the paper\nprovided no indications of or connections to the design of the production search engine. One of\nthe earliest academic papers to include efﬁciency objectives in learning to rank was by Wang et al.\n[2010], who explicitly modeled feature costs in a framework to jointly optimize effectiveness and\nefﬁciency; cf. [Xu et al., 2012]. In a follow-up, Wang et al. [2011] proposed a boosting algorithm\nfor learning ranking cascades to directly optimize this quality/speed tradeoff. Within the academic\nliterature, this is the ﬁrst instance we are aware of that describes learning the stages in a multi-stage\nranking architecture. Wang et al. coined the term “learning to efﬁciently rank” to describe this thread\nof research. Nevertheless, it is clear that industry led the way in explorations of this design, but since\nthere is paucity of published material about production systems, we have no public record of when\nvarious important innovations occurred and when they were deployed.\nSince the early 2010s, multi-stage ranking architectures have received substantial interest in the\nacademic literature [Tonellotto et al., 2013, Asadi and Lin, 2013, Capannini et al., 2016, Clarke\net al., 2016, Chen et al., 2017c, Mackenzie et al., 2018] as well as industry. Beyond Bing, publicly\ndocumented production deployments of such an architecture at scale include Alibaba’s e-commerce\nsearch engine [Liu et al., 2017] and elsewhere within Alibaba as well [Yan et al., 2021], Baidu’s web\nsearch engine [Zou et al., 2021], and Facebook search [Huang et al., 2020]. In fact, Facebook writes:\nFacebook search ranking is a complex multi-stage ranking system where each stage\nprogressively reﬁnes the results from the preceding stage. At the very bottom of\n106Note an important caveat here is the assumption that users only desire a few relevant documents, as is typical\nin web search and operationalized in terms of early-precision metrics. Multi-stage architectures might not\nbe as useful if users desire high recall, which is important for many scenarios in the medical domain (for\nexample, systematic reviews) or the legal domain (for example, patent search).\n89\nthis stack is the retrieval layer, where embedding based retrieval is applied. Results\nfrom the retrieval layer are then sorted and ﬁltered by a stack of ranking layers.\nWe see that multi-stage ranking remains very much relevant in the neural age. While keyword-based\nretrieval has been replaced with retrieval using learned dense representations (see Section 5) as the\nﬁrst stage in this case, and subsequent reranking stages are now primarily driven by neural models,\nthe general multi-stage design has not changed.\nHaving provided sufﬁcient background, the remainder of this section presents a few multi-stage\nranking architectures speciﬁcally designed around transformer models. Section 3.4.1 describes a\nreranking approach that explicitly compares the relevance of pairs of texts in a single inference step,\nwhich can be logically extended to assessing the relevance of lists of texts, which we describe in\nSection 3.4.2. We then present cascade transformers in Section 3.4.3, which treat transformer layers\nas reranking stages.\n3.4.1 Reranking Pairs of Texts\nThe ﬁrst application of transformers in a multi-stage ranking architecture was described by Nogueira\net al. [2019a] as a solution for mitigating the quadratic computational costs associated with a ranking\nmodel that applies inference on an input template that incorporates pairs of texts, as we explain below.\nRecall that monoBERT turns ranking into a relevance classiﬁcation problem, where we sort texts by\nP(Relevant = 1|di,q) given a query qand candidates {di}. In the terminology of learning to rank,\nthis model is best described as a “pointwise” approach since each text is considered in isolation during\ntraining [Liu, 2009, Li, 2011]. An alternative is a “pairwise” approach, which focuses oncomparisons\nbetween pairs of documents. Intuitively, pairwise ranking has the advantage of harnessing signals\npresent in other candidate texts to decide if a text is relevant to a given query; these comparisons are\nalso consonant with the notion of graded relevance judgments (see Section 2.5).\nThe “duoBERT” model proposed by Nogueira et al. [2019a] operationalizes this intuition by explicitly\nconsidering pairs of text. In this ranking model, BERT is trained to estimate the following:\nP(di ≻dj|di,dj,q), (30)\nwhere di ≻dj is a commonly adopted notation for stating that di is more relevant than dj (with\nrespect to the query q).\nBefore going into details, there are two conceptual challenges to realizing this ranking strategy:\n1. The result of model inferences comprises a set of pairwise comparisons between candidate texts.\nEvidence from these pairs still need to be aggregated to produce a ﬁnal ranked list.\n2. One simple implementation is to compare each candidate to every other candidate (e.g., from\nﬁrst-stage retrieval), and thus the computational costs increase quadratically with the size of the\ncandidate set. Since monoBERT’s effectiveness increases with the size of the candidates set\n(see Section 3.2), there emerges an effectiveness/efﬁciency tradeoff that needs to be controlled.\nNogueira et al. [2019a] proposed a number of evidence aggregation strategies (described below) to\ntackle the ﬁrst challenge and adopts a multi-stage ranking architecture to address the second challenge.\nIn summary, in a multi-stage design, a relevance classiﬁer can be used to select a smaller set of\ncandidates from ﬁrst-stage retrieval to be fed to the pairwise reranker.\nThe duoBERT model is trained to estimate pi,j, the probability that di ≻dj, i.e., candidate di is\nmore relevant than dj. It takes as input a sequence comprised of a query and two texts, comprising\nthe input template:\n[[CLS],q, [SEP],di,[SEP],dj,[SEP]], (31)\nSimilar to the implementation of monoBERT, each input token inq, di, and dj is represented by the\nelement-wise sum of the token, segment type, and position embeddings. In the duoBERT model,\nthere are three segment types: type Afor q tokens, and types B and C for the di and dj tokens,\nrespectively. Type embeddings Aand Bare learned during pretraining, but the new type segment\nC embedding is learned from scratch during ﬁne-tuning. Due to the length limitations of BERT,\nthe query, candidates di and dj are truncated to 62, 223, and 223 tokens, respectively, so that the\nentire sequence has at most 512 tokens when concatenated with the [CLS] token and the three [SEP]\n90\nMS MARCO Passage\nDevelopment Test\nMethod MRR@10 MRR@10\n(1) Anserini BM25 = Table 5, row (3a) 0.187 0.190\n(2) + monoBERT ( k0 = 1000) = Table 5, row (3b) 0.372 0.365\n+ monoBERT (k0 = 1000)\n(3a) + duoBERT MAX (k1 = 50) 0.326 -\n(3b) + duoBERT MIN (k1 = 50) 0.379 -\n(3c) + duoBERT SUM (k1 = 50) 0.382 0.370\n(3d) + duoBERT BINARY(k1 = 50) 0.383 -\n(4a) + monoBERT + TCP 0.379 -\n(4b) + monoBERT + duoBERTSUM + TCP 0.390 0.379\nTable 21: The effectiveness of the monoBERT/duoBERT pipeline on the MS MARCO passage\nranking test collection. TCP refers to target corpus pretraining.\ntokens. Using the above length limits, for the MS MARCO passage ranking test collection, Nogueira\net al. [2019a] did not have to truncate any of the queries and less than 1% of the candidate texts were\ntruncated. Similar to monoBERT, the ﬁnal representation of the [CLS] token is used as input to a\nfully-connected layer to obtain the probability pi,j. For kcandidates, |k|×(|k|−1) probabilities are\ncomputed.\nThe model is trained end-to-end with the following loss:\nLduo = −\n∑\ni∈Jpos,j∈Jneg\nlog(pi,j) −\n∑\ni∈Jneg,j∈Jpos\nlog(1 −pi,j), (32)\nNote that in the equation above, candidates di and dj are never both relevant or not relevant. Since\nthis loss function considers pairs of candidate texts, it can be characterized as belonging to the family\nof pairwise learning-to-rank methods [Liu, 2009, Li, 2011] (but see additional discussions below).\nFor details about the training procedure, including hyperparameter settings, we refer the reader to the\noriginal paper.\nAt inference time, the pairwise scores pi,j are aggregated so that each document receives a single\nscore si. Nogueira et al. [2019a] investigated a number of different aggregation methods:\nMAX : si = max\nj∈Ji\npi,j, (33)\nMIN : si = min\nj∈Ji\npi,j, (34)\nSUM : si =\n∑\nj∈Ji\npi,j, (35)\nBINARY : si =\n∑\nj∈Ji\n1 pi,j>0.5. (36)\nwhere Ji = {0 ≤j <|D|,j ̸= i}and mis the number of samples drawn without replacement\nfrom the set Ji. The SUM method measures the pairwise agreement that candidate di is more\nrelevant than the rest of the candidates {dj}j̸=i. The BINARY method is inspired by the Condorcet\nmethod [Montague and Aslam, 2002], which serves as a strong aggregation baseline [Cormack et al.,\n2009]. The MIN (MAX) method measures the relevance of di only against its strongest (weakest)\n“competitor”. The ﬁnal ranked list (for evaluation) is obtained by reranking the candidates according\nto their scores si.\nBefore presenting experimental results, it is worthwhile to clarify a possible point of confusion. In\n“traditional” (i.e., pre-neural) learning to rank, “pairwise” and “pointwise” refer to the form of the\nloss, not the form of the inference mechanism. For example, RankNet [Burges et al., 2005] is trained\nin a pairwise manner (i.e., loss is computed with respect to pairs of texts), but inference (i.e., at query\ntime) is still performed on individual texts. In duoBERT, both training and inference are performed\non pairs of texts in a cross-encoder design where all three inputs (the query and the two texts to be\ncompared) are “packed” into the input template fed to BERT.\n91\nResults on the MS MARCO passage ranking test collection are shown in Table 21, organized in\nthe same manner as Table 5; the experimental conditions are directly comparable. Row (1) reports\nthe effectiveness of Anserini’s initial candidates using BM25 scoring. In row (2), BM25 results\nreranked with monoBERT using BERTLarge (k0 = 1000) are shown, which is exactly the same as row\n(3b) in Table 5. Rows (3a)–(3d) report results from reranking the top 50 results from the output of\nmonoBERT (i.e., k1 = 50) using the various aggregation techniques presented above. Effectiveness\nin terms of the ofﬁcial metric MRR@10 is reported on the development set for all aggregation\nmethods (i.e., duoBERT using BERTLarge), but Nogueira et al. [2019a] only submitted results from\nthe SUM condition for evaluation on the test set. We see that MAX aggregation is not as effective as\nthe other three techniques, but the difference between MIN, SUM, and BINARY are all quite small.\nIn the same paper, Nogueira et al. [2019a] also introduced the target corpus pretraining (TCP)\ntechnique presented in Section 3.2.4. Rows (4a) and (4b) in Table 5 report results of applying TCP\nwith monoBERT and monoBERT + duoBERT. Here, we see that the gains are relatively modest, but\nas discussed earlier, unsupervised pretraining can be viewed as a source of “free” improvements in\nthat these gains do not require any additional labeled data.\nIn all the experimental conditions above, duoBERT considers the top 50 candidates from monoBERT\n(i.e., k1 = 50), and thus requires an additional 50 ×49 BERT inferences to compute the ﬁnal ranking\n(the time required for aggregation is negligible). For simplicity, Nogueira et al. [2019a] used the total\nnumber of BERT inferences as a proxy to capture overall query latency. Based on this metric, since\nmonoBERT with k0 = 1000 requires 1000 BERT inferences, a monoBERT + duoBERT pipeline\nrepresents a 3.5×increase in latency. While it is true that each pair of texts in duoBERT takes longer\nto process than a single text in monoBERT due to the longer input length, this detail does not change\nthe argument qualitatively (although the actual tradeoff point in our analysis below might change if\nwe were to measure wall-clock latency; there are GPU batching effects to consider as well).\nFrom this perspective, duoBERT does not seem compelling because the gain from monoBERT +\nduoBERT vs. monoBERT alone is far more modest than the gain from monoBERT vs. BM25 (at the\nk0 and k1 settings shown in Table 21). However, the more pertinent question is as follows: Given\na ﬁxed budget for neural inference, how should we allocate resources between monoBERT and\nduoBERT? In this scenario, the pairwise reranking approach becomes much more compelling. We\ndemonstrate this below:\nIn general, a two-stage conﬁguration provides a richer design space for selecting a desirable operating\npoint to balance effectiveness and efﬁciency under a certain computational budget. With a single\nreranking stage (monoBERT), the only choice is to vary the k0 parameter, but with two rerankers,\nit is possible to simultaneously tune k0 and k1. These tradeoff curves are shown in Figure 15, with\nduoBERTSUM for aggregation. This experiment was not reported in Nogueira et al. [2019a] and here\nwe present results that have not yet been published anywhere else. In the plot, the gray line shows\neffectiveness with different values ofk0 for monoBERT in a single-stage setup (this is the same as the\ncurve in Figure 9, just across a narrower range). The other lines show settings of k1 ∈{10,30,50},\nand with each k1 setting, points in each tradeoff curve represent k0 = {50,100,200,500,1000}. In\nthe two-stage conﬁguration, the number of inferences per query is calculated as k0 + k1(k1 −1).\nThus, the xaxis is a reasonable proxy of the total computational budget.\nHypothetical vertical lines intersecting with each curve denote the best effectiveness that can be\nachieved with a particular computational budget: these results suggest that if a system designer were\nwilling to expend more than couple of hundred BERT inferences, then a two-stage conﬁguration\nis more effective overall. That is, rather than simply increasing the reranking depth of single-stage\nmonoBERT, it is better to reallocate some of the computational budget to a pairwise approach that\nexamines pairs of candidate texts. The Pareto frontier in the effectiveness/efﬁciency tradeoff space\nis shown in Figure 15 as the dotted black line. For each point on the frontier, there exists no other\nsetting that achieves both higher MRR@10 while requiring fewer inferences. This frontier serves as\na guide for system designers in choosing desirable operating points in the effectiveness/efﬁciency\ndesign space.\nTakeaway Lessons. Multi-stage ranking architectures represent a straightforward generalization of\nthe retrieve-and-rerank approach adopted in monoBERT. Introducing multiple rerankers in a pipeline\ngreatly expands the possible operating points of an end-to-end system in the effectiveness/efﬁciency\ntradeoff space, potentially leading to settings that are both better and faster than what can be achieved\nwith a single-stage reranker. On potential downside, however, is that multi-stage pipelines introduce\n92\n50 100 200 500 1,000 2,000 3,450 10,0000.320\n0.340\n0.360\n0.380\n0.400\nBERT Inferences/Query\nMRR@10\nduoBERT,k1 = 10\nduoBERT,k1 = 30\nduoBERT,k1 = 50\nmonoBERT\nPareto frontier\nEffectiveness/Efﬁciency Tradeoffs on MS MARCO Passage\nFigure 15: Effectiveness/efﬁciency tradeoff curves for different monoBERT and monoBERT +\nduoBERTSUM settings on the development set of the MS MARCO passage ranking test collection.\nEfﬁciency is measured in the number of BERT inferences per query. For monoBERT, the tradeoff\ncurve plots different values of k0 (the same as in Figure 9). For monoBERT + duoBERTSUM , each\ncurve plots a different k1, and points on each curve correspond to k0 = {50,100,200,500,1000};\nthe number of inferences per query is calculated as k0 + k1(k1 −1). The Pareto frontier is shown as\nthe dotted black line.\nadditional “tuning knobs” that need to be properly adjusted to achieve a desired tradeoff. In the\nmonoBERT/duoBERT design, these parameter settings (k0,k1) are difﬁcult to learn as the pipeline is\nnot differentiable end-to-end. Thus, the impact of different parameter settings must be empirically\ndetermined from a test collection.\n3.4.2 Reranking Lists of Texts\nGiven a query, the duoBERT model described in the previous section estimates the relevance of a\ntext relative to another text, where both texts are directly fed into BERT for consideration in a single\ninference pass. This pairwise approach can be more effective than pointwise rerankers based on\nrelevance classiﬁcation such as monoBERT because the pairwise approach allows the reranker to “see”\nwhat else is in the set of candidates. One natural extension of the pairwise approach is the “listwise”\napproach, in which the relevance of a text is estimated jointly with multiple other candidates. Here\nwe describe two proposed listwise reranking methods.\nBefore proceeding, two important caveats: First, the labels “pairwise” and “listwise” here explicitly\nrefer to the form of the input template for inference (which necessitates, naturally, modiﬁcations to the\nloss function during model training). Thus, our usage of these terms diverges from “traditional” (i.e.,\npre-neural) learning to rank, which describes only the form of the loss; see, for example, ListNet [Cao\net al., 2007]. We do not cover these listwise learning-to-rank methods here and instead refer the\nreader to existing surveys [Liu, 2009, Li, 2011]. Second, while listwise approaches may not have\nbeen proposed explicitly in the context of multi-stage ranking architectures, they are a natural ﬁt for\nthe same reasons as duoBERT. Given the length limitations of many neural models and the blow-up\nin terms of input permutations that need to be considered, a stage-wise reranking approach makes a\nlot of sense.\nWe begin with Ai et al. [2019], who proposed a listwise reranking approach based on learning what\nthey called a groupwise multivariate scoring function. In their approach, each textdi is represented by\na hand-crafted feature vectorxi, which can include signals designed to capture query–text interactions.\nThe concatenation of nsuch feature vectors is fed to a fully-connected neural network that outputs n\nrelevance scores, one for each text. Depending on the query, the number of candidate texts kcan be\n93\nquite large (e.g., k= 1000). Consequently, it is not practical to feed all candidates to the model at\nonce since the input sequence would become prohibitively long, thus making the model difﬁcult to\neffectively train. Instead, the authors proposed to compute size-npermutations of kcandidate texts\nand independently feed each group of nfeature vectors to the model. At inference time, the ﬁnal\nscore of each text is the sum of the scores in each group it was part of.\nThe model is trained with the following cross-entropy loss:\nL= −\nk∑\ni=1\nwiyilog pi, (37)\nwhere wi is the Inverse Propensity Weight [Joachims et al., 2017, Liu, 2009] of thei-th results and\nyi = 1 if the text is relevant and zero otherwise. The probability pi is obtained by applying a softmax\nto all logits tof the candidate texts:\npi = eti\n∑k\nj=1 etj\n(38)\nResults on publicly available datasets are encouraging, but the effectiveness of this approach is not\nclearly superior to pointwise or pairwise approaches. The authors identiﬁed possible improvements,\nincluding the design of the feedforward network and a better way to organize model input than a\nsimple concatenation of features from the candidate texts.\nInstead of feeding hand-crafted features to a fully-connected neural network as in Ai et al. [2019],\nZhang et al. [2020f] proposed to directly feed raw candidate texts into pretrained transformers. Due\nto model length limitations, however, candidate texts are truncated until they ﬁt into a 512 token\nsequence. The resulting listwise reranker showed small improvements over its pairwise counterpart\non two ranking datasets: the ﬁrst is a non-public dataset in Chinese, while the second is a modiﬁed\nversion of the MS MARCO passage ranking test collection. Unfortunately, modiﬁcations to the latter\nrender the results not comparable to other papers, so we lack meaningful points of comparison.\nTakeaway Lessons. Listwise rerankers represent a natural extension of pairwise rerankers and are\nintuitively appealing because relevance scores can be estimated jointly. However, the necessity of\nfeeding multiple candidate texts into a neural model in each inference pass leads to potentially long\ninput sequences and thus presents a major technical challenge, for all the reasons already discussed\nthroughout this section. For the problem of label prediction in a fact veriﬁcation setting, Pradeep et al.\n[2021a] demonstrated the effectiveness of a listwise approach in which multiple claims are presented\nto a pretrained transformer model in a single input template. In this case, the candidate sentences\nare shorter than typical texts to be ranked, and thus the work highlights the potential of the listwise\napproach, as long as we can overcome the model length limitations. This remains an open problem\nin the general case, and despite encouraging results, in our opinion, ranking models that consider\nlists of candidates have not been conclusively demonstrated to be more effective than models that\nconsider pairs of candidates.\n3.4.3 Efﬁcient Multi-Stage Rerankers: Cascade Transformers\nMulti-stage ranking pipelines exploit faster (and possibly less effective) models in earlier stages\nto discard likely non-relevant documents so there are fewer candidates under consideration by\nmore expensive models in later stages. In the case of the mono/duoBERT architecture described\nabove, the primary goal was to make a more inference-heavy model (i.e., duoBERT) more practical.\nIndeed, experimental results in the previous section offer a guide for how to optimally allocate\nresources to monoBERT and duoBERT inference given a computational budget. In other words, the\ngoal is to improve the quality of a single-stage monoBERT design while maintaining acceptable\neffectiveness/efﬁciency tradeoffs.\nHowever, the mono/duoBERT architecture isn’t particularly useful if we desire a system that is even\nfaster (but perhaps less effective) than the baseline (single-stage) monoBERT design. In this case,\none possibility is to use a standard telescoping pipeline that potentially include pre-BERT neural\nranking methods, as suggested by Matsubara et al. [2020]. Given monoBERT as a starting point,\nanother obvious solution is to leverage the large body of research on model pruning and compression,\nwhich is not speciﬁc to text ranking or even natural language processing. In Section 3.5, we cover\nknowledge distillation and other threads of research in this broad space. Here, we discuss a solution\nthat shares similar motivations, but is clearly inspired by multi-stage ranking architectures.\n94\nSoldaini and Moschitti [2020] began with the observation that a model like monoBERTis already\nlike a multi-stage ranking architecture if we consider each layer of the transformer encoder as a\nseparate ranking stage. In the monoBERT design, inference is applied to all input texts (for example,\nk0 = 1000 ). This seems like a “waste”, and we could accelerate inference if the model could\nsomehow predict that a particular text was not likely to be relevant partway through the layers.\nTherefore, a sketch of the solution might look like the following: start with a pool of candidate texts,\napply inference on the entire batch using the ﬁrst few layers, discard the least promising candidates,\ncontinue inference with the next few layers, discard the least promising candidates, and so on, until\nthe end, when only the most promising candidates have made it all the way through the layers. With\ncascade transformers, Soldaini and Moschitti [2020] did exactly this.\nMore formally, with cascade transformers, intermediate classiﬁcation decision points (which we’ll call\n“early exits” for reasons that will become clear in a bit) are built in at layersj = λ0 +λ1 ·(i−1),∀i∈\n{1,2,... }, where λ0,λ1 ∈N are hyperparameters. Speciﬁcally, Soldaini and Moschitti [2020] build\non the base version of RoBERTa [Liu et al., 2019c], which has 12 layers; they used a setting ofλ0 = 4\nand λ1 = 2, which yields ﬁve rerankers, with decision points at layers 4, 6, 8, 10, and 12. 107 The\nrationale for skipping the ﬁrst λ0 layers is that relevance classiﬁcation effectiveness is too poor for the\nmodel to be useful; this observation is consistent with ﬁndings across many NLP tasks [Houlsby et al.,\n2019, Lee et al., 2019a, Xin et al., 2020]. The [CLS] vector representation at each of the jlayers\n(i.e., each of the cascade rerankers) is then fed to a fully-connected classiﬁcation layer that computes\nthe probability of relevance for the candidate text; this remains a pointwise relevance classiﬁcation\ndesign. At inference time, at each of the jlayers, the model will score P candidate documents and\nretain only the top (1 −α) ·P scoring candidates, where α∈[0 ... 1] is a hyperparameter, typically\nbetween 0.3 and 0.5. That is, α·P candidates are discarded at each stage.\nIn practice, neural network inference is typically conducted on GPUs in batches. Soldaini and\nMoschitti [2020] worked through a concrete example of how these settings play out in practice:\nConsider a setting of α = 0.3 with a batch size b = 128. With the ﬁve cascade reranker design\ndescribed above, after layer 4, the size of the batch is reduced to 90, i.e., ⌊0.3 ·128⌋= 38 candidates\nare discarded after the ﬁrst classiﬁer. At layer 6, after the second classiﬁcation, 27 additional\ncandidates are discarded, with only 63 remaining. At the end, only 31 candidates are left. Thus,\ncascade transformers have the effect of reducing the average batch size, which increases throughput on\nGPUs compared to a monolithic design, where inference must be applied to all input instances. In the\nexample above, suppose that based on a particular hardware conﬁguration we can process a maximum\nbatch size of 84 using a monolithic model. With cascade transformers, we can instead process batches\nof 128 instances within the same memory constraints, since (4·128+2·90+2·63+2·44+2·28)/12 =\n80.2 <84. This represents a throughput increase of 52%.\nThe cascade transformer architecture requires training all the classiﬁers at each of the individual\nrerankers (i.e., early exit points). The authors described a procedure wherein for each training batch,\none of the rerankers is sampled (including the ﬁnal output reranker): its loss against the target labels\nis computed and back-propagated through the entire model, down to the embedding layers. This\nsimple uniform sampling strategy was found to be more effective than alternative techniques such as\nround-robin selection and biasing the early rerankers.\nSoldaini and Moschitti [2020] evaluated their cascade transformers on the answer selection task in\nquestion answering, where the goal is to select from a pool of candidate sentences the ones that\ncontain the answer to a given natural language question. This is essentially a text ranking task on\nsentences, where the ranked output provides the input to downstream modules that identify answer\nspans. The authors reported results on multiple answer selection datasets, but here we focus on two:\nAnswer Sentence Natural Questions (ASNQ) [Garg et al., 2020], which is a large dataset constructed\nby extracting sentence candidates from the Google Natural Question (NQ) dataset [Kwiatkowski\net al., 2019], and General Purpose Dataset (GPD), which is a proprietary dataset comprising questions\nsubmitted to Amazon Alexa with answers annotated by humans. In both cases, the datasets include\nthe candidates to be reranked (i.e., ﬁrst-stage retrieval is ﬁxed and part of the test collection itself).\n107In truth, Soldaini and Moschitti [2020] describe their architecture in terms of reranking with multiple\ntransformer stacks, e.g., ﬁrst with a 4-layer transformer, then a 6-layer transformers, then a 8-layer transformer,\netc. However, since in their design, all common transformer layers have shared weights, it is entirely equivalent\nto a monolithic 12-layer transformer with ﬁve intermediate classiﬁcation decision points (or early exits).\nWe ﬁnd this explanation more intuitive and better aligned with the terminology used by other researchers.\nNevertheless, we retain the authors’ original description of calling this design a ﬁve-reranker cascade.\n95\nASNQ GDP\nMethod MAP nDCG@10 MRR MAP nDCG@10 MRR Cost Reduction\n(1) T ANDABASE 0.655 0.651 0.647 0.580 0.722 0.768\n(2a) CT (α= 0.0) 0.663 0.661 0.654 0.578 0.719 0.769\n(2b) CT (α= 0.3) 0.653 0.653 0.653 0.557 0.698 0.751 −37%\n(2c) CT (α= 0.4) 0.648 0.650 0.648 0.528 0.686 0.743 −45%\n(2d) CT (α= 0.5) 0.641 0.650 0.645 0.502 0.661 0.729 −51%\nTable 22: The effectiveness and cost reduction of cascade transformers on the ASNQ and GPD\ndatasets. The parameter αcontrols the proportion of candidates discarded at each pipeline stage.\nResults copied from the authors’ paper are shown in Table 22. The baseline is TAND ABASE [Garg\net al., 2020], which is monoBERT with a multi-stage ﬁne-tuning procedure that uses multiple\ndatasets—what we introduced as pre–ﬁne-tuning in Section 3.2.4. For each dataset, effectiveness\nresults in terms of standard metrics are shown; the ﬁnal column denotes an analytically computed cost\nreduction per batch. The cascade transformer architecture is denoted CT, in row group (2). In row (2a),\nwith α= 0.0, all candidate sentences are scored using all layers of the model (i.e., no candidates are\ndiscarded). This model performs slightly better than the baseline, and these gains can be attributed to\nthe training of the intermediate classiﬁcation layers, since the rest of the CT architecture is exactly the\nsame as the TAND A baseline. Rows (2b), (2c), and (2d) report effectiveness with different αsettings.\nOn the ASNQ dataset, CT with α= 0.5 is able to decrease inference cost per batch by around half\nwith a small decrease in effectiveness. On the GPD dataset, inference cost can be reduced by 37%\n(α= 0.3) with a similarly modest decrease in effectiveness. These experiments clearly demonstrated\nthat cascade transformers provide a way for system designers to control effectiveness/efﬁciency\ntradeoffs in multi-stage ranking architectures. As with the mono/duoBERT design, the actual\noperating point depends on many considerations, but the main takeaway is that these designs provide\nthe knobs for system designers to express their desired tradeoffs.\nAt the intersection of model design and the practical realities of GPU-based inference, Soldaini and\nMoschitti [2020] discussed a point that is worth repeating here. In their design, a ﬁxed αis crucial to\nobtaining the performance gains observed, although in theory one could devise other approaches to\npruning. For example, candidates could be discarded based on a score threshold (that is, discard all\ncandidates with scores below a given threshold). Alternatively, it might even be possible to separately\nlearn a lightweight classiﬁer that dynamically decides the candidates to discard. The challenge with\nthese alternatives, however, is that it becomes difﬁcult to determine batch sizesa priori, and therefore\nto efﬁciently exploit GPU resources (which depend critically on regular computations).\nIt is worth noting that cascade transformers were designed to rank candidate sentences in a question\nanswering task, and cannot be directly applied to document ranking, even with relatively simple\narchitectures like Birch and BERT–MaxP. There is the practical problem of packing sentences (from\nBirch) or passages (from BERT–MaxP) into batches for GPU processing. As we can see from the\ndiscussion above, cascade transformers derive their throughput gains from the ability to more densely\npack instances into the same batch for efﬁcient inference. However, for document ranking, it is\nimportant to distinguish between scores of segments within documents as well as across documents.\nThe simple ﬁltering decision in terms of αcannot preserve both relationships at the same time if\nsegments from multiple documents are mixed together, but since documents have variable numbers\nof sentences or passages, strictly segregating batches by document will reduce the regularity of the\ncomputations and hence the overall efﬁciency. To our knowledge, these issues have not been tackled,\nand cascade transformers have not been extended for ranking texts that are longer than BERT’s 512\ntoken length limit. Such extensions would be interesting future work.\nTo gain a better understanding of cascade transformers, it is helpful to situate this work within the\nbroader context of other research in NLP. The insight that not all layers of BERT are necessary for\neffectively performing a task (e.g., classiﬁcation) was shared independently and contemporaneously\nby a number of different research teams. While Soldaini and Moschitti [2020] operationalized this\nidea for text ranking in cascade transformers, other researchers applied the same intuition for other\nnatural language processing tasks. For example, DeeBERT [Xin et al., 2020] proposed building early\nexit “off ramps” in BERT to accelerate inference for test instances based on an entropy threshold;\ntwo additional papers, Schwartz et al. [2020] and Liu et al. [2020] implemented the same idea with\n96\nonly minor difference in details. Quite amazingly, these three papers, along with the work of Soldaini\nand Moschitti, were all published at the same conference, ACL 2020!\nAlthough this remarkable coincidence suggests early exit was an idea “whose time had come”, it is\nimportant to recognize that, in truth, the idea had been around for a while—just not in the modern\ncontext of neural networks. Over a decade ago, Cambazoglu et al. [2010] proposed early exits in\nadditive ensembles for ranking, but in the context gradient-boosted decision trees, which exhibit the\nsame regular, repeating structure (at the “block” level) as transformer layers. Of course, BERT and\npretrained transformers offer a “fresh take” that opens up new design choices, but many of the lessons\nand ideas from (much older) previous work remain applicable.\nA ﬁnal concluding thought before moving on: the above discussion suggests that the distinction\nbetween monolithic ranking models and multi-stage ranking is not clear cut. For example, is the\ncascade transformer a multi-stage ranking pipeline or a monolithic ranker with early exits? Both\nseem apt descriptions, depending on one’s perspective. However, the mono/duoBERT combination\ncan only be accurately described as multi-stage ranking, since the two rerankers are quite different.\nPerhaps the distinction lies in the “end-to-end” differentiability of the model (and hence how it is\ntrained)? But differentiability stops at the initial candidate generation stage since all the architectures\ndiscussed in this section still rely on keyword search. Learned dense representations, which we cover\nin Section 5, can be used for single-stage direct ranking, but can also replace keyword search for\ncandidate generation, further muddling these distinctions. Indeed, the relationship between these\nvarious architectures remains an open question and the focus of much ongoing research activity,\nwhich we discuss in Section 6.\nTakeaway Lessons. Cascade transformers represent another example of a multi-stage ranking\npipeline. Compared to the mono/duoBERT design, the approach is very different, which illustrates\nthe versatility of the overall architecture. Researchers have only begun to explore this vast and\ninteresting design space, and we expect more interesting future work to emerge.\n3.5 Beyond BERT\nAll of the ranking models discussed so far in this section are still primarily built around BERT or a\nsimple BERT variant, even if they incorporate other architectural components, such as interaction\nmatrices in CEDR (see Section 3.3.3) or another stack of transformers in PARADE (see Section 3.3.4).\nThere are, however, many attempts to move beyond BERT to explore other transformer models,\nwhich is the focus of this section.\nAt a high level, efforts to improve ranking models can be characterized as attempts to make ranking\nbetter, attempts to make ranking faster, attempts to accomplish both, or attempts to ﬁnd other operating\npoints in the effectiveness/efﬁciency tradeoff space. Improved ranking effectiveness is, of course,\na perpetual quest and needs no elaboration. Attempts to make text ranking models faster can be\nmotivated by many sources. Here, we present results by Hofstätter and Hanbury [2019], shown in\nFigure 16. The plot captures the effectiveness vs. query latency (millisecond per query) of different\nneural ranking models on the development set of the MS MARCO passage ranking test collection.\nNote that the x axis is in log scale! Pre-BERT models can be deployed for real-world applications\nwith minimal modiﬁcations, but it is clear that naïve production deployments of BERT are impractical\nor hugely expensive in terms of required hardware resources. In other words, BERT is good but slow:\nCan we trade off a bit of quality for better performance?\nThis section is organized roughly in increasing “distance from BERT”. Admittedly, what’s BERT and\nwhat’s “beyond BERT” is somewhat an arbitrary distinction. These classiﬁcations represent primarily\nour judgment for expository purposes and shouldn’t be taken as any sort of deﬁnitive categorization.\nBuilding on our previous discussion of simple BERT variants in Section 3.2.2, we begin by discussing\nefforts to distill BERT into smaller models in Section 3.5.1. Distilled models are similar to the simple\nBERT variants in that they can easily be “swapped in” as a replacement for BERT “classic”. Attempts\nto design transformer-based architectures speciﬁcally for text ranking from the ground up—the\nTransformer Kernel (TK) and Conformer Kernel (CK) models—are discussed next in Section 3.5.2.\nFinally, we turn our attention to ranking with pretrained sequence-to-sequence transformers in\nSection 3.5.3 and Section 3.5.4, which are very different from the transformer encoder design of\nBERT and BERT variants.\n97\n1 10 100 1,000 2,0000.2\n0.25\n0.3\n0.35\n0.4\nQuery Latency (milliseconds)\nMRR@10\nBERT\nKNRM\nMatchP.\nPACRR\nDUET\nC-KNRM\nEffectiveness/Efﬁciency Tradeoffs on MS MARCO Passage\nFigure 16: Effectiveness/efﬁciency tradeoffs comparing BERT with pre-BERT models (using FastText\nembeddings) on the development set of the MS MARCO passage ranking test collection, taken\nfrom Hofstätter and Hanbury [2019]. Note that the x-axis is in log scale.\n3.5.1 Knowledge Distillation\nKnowledge distillation refers to a general set of techniques where a smaller student model learns to\nmimic the behavior of a larger teacher model [Ba and Caruana, 2014, Hinton et al., 2015]. The goal\nis for the student model to achieve comparable effectiveness on a particular task but more efﬁciently\n(e.g., lower inference latencies, fewer model parameters, etc.). While knowledge distillation is model\nagnostic and researchers have explored this approach for many years, to our knowledge Tang et al.\n[2019] were the ﬁrst to apply the idea to BERT, demonstrating knowledge transfer between BERT\nand much simpler models such as single-layer BiLSTMs. A much simpler RNN-based student model,\nof course, cannot hope to achieve the same level of effectiveness as BERT, but if the degradation is\nacceptable, inference can be accelerated by an order of magnitude or more. These ideas have been\nextended by many others [Sun et al., 2019a, Liu et al., 2019b, Sanh et al., 2019, Hofstätter et al.,\n2020], with a range of different student models, including smaller versions of BERT.\nUnsurprisingly, knowledge distillation has been applied to text ranking. Researchers have investigated\nwhether the efﬁciency of BERT can be improved by distilling a larger trained (BERT) model into a\nsmaller (but still BERT-based) one [Gao et al., 2020c, Li et al., 2020a, Chen et al., 2021, Zhang et al.,\n2020f]. To encourage the student model to mimic the behavior of the teacher model, one common\ndistillation objective is the mean squared error between the student’s and teacher’s logits [Tang et al.,\n2019, Tahami et al., 2020]. The student model can be ﬁne-tuned with the linear combination of the\nstudent model’s cross-entropy loss and the distillation objective as the overall loss:\nL= α·LCE + (1 −α) ·||rt −rs||2 (39)\nwhere LCE is the cross-entropy loss, rt and rs are the logits from the teacher and student models,\nrespectively, and αis a hyperparameter. As another approach, TinyBERT proposed a distillation\nobjective that additionally considers the mean squared error between the two models’ embedding\nlayers, transformer hidden states, and transformer attention matrices [Jiao et al., 2019]. In the\ncontext of text ranking, Chen et al. [2021] reported that this more complicated objective can improve\neffectiveness.\nGao et al. [2020c] observed that distillation can be applied to both a BERT model that has already been\nﬁne-tuned for relevance classiﬁcation (“ranker distillation”) and to pretrained but not yet ﬁne-tuned\nBERT itself (“LM distillation”). Concretely, this yields three possibilities:\n1. apply distillation so that a (randomly initialized) student model learns to directly mimic an\nalready ﬁne-tuned teacher model using the distillation objective above (“ranker distillation”),\n2. apply LM distillation into a student model followed by ﬁne-tuning the student model for the\nrelevance classiﬁcation task (“LM distillation + ﬁne-tuning”), or\n98\nMS MARCO Passage TREC 2019 DL Passage Latency\nMethod Layers MRR@10 MRR nDCG@10 (ms / doc)\n(1) monoBERTBase 12 0.353 0.935 0.703 2.97\n(2a) Ranker distillation 6 0.338 0.927 0.686 1.50\n(2b) LM Distillation + Fine-Tuning 6 0.356 0.965 0.719 1.50\n(2c) LM + Ranker Distillation 6 0.360 0.952 0.692 1.50\n(3a) Ranker distillation 4 0.329 0.935 0.669 0.33\n(3b) LM Distillation + Fine-Tuning 4 0.332 0.950 0.681 0.33\n(3c) LM + Ranker Distillation 4 0.350 0.929 0.683 0.33\nTable 23: The effectiveness of distilled monoBERT variants on the development set of the MS\nMARCO passage ranking test collection and the TREC 2019 Deep Learning Track passage ranking\ntest collection. Inference times were measured on an NVIDIA RTX 2080 Ti GPU.\n3. apply LM distillation followed by ranker distillation (“LM + ranker distillation”).\nOperationally, the third approach is equivalent to the ﬁrst approach, except with a better initialization\nof the student model. The relative effectiveness of these three approaches is an empirical question.\nTo answer this question, Gao et al. [2020c] used the TinyBERT distillation objective to distill a\nBERTBase model into smaller transformers: a six-layer model with a hidden dimension of 768 or a\nfour-layer model with a hidden dimension of 312. Both the student and teacher models are designed\nas relevance classiﬁers (i.e., monoBERT).\nEvaluation on the development set of the MS MARCO passage ranking test collection and TREC\n2019 Deep Learning Track passage ranking test collection are shown in Table 23, with results copied\nGao et al. [2020c]. The six-layer and four-layer student models are shown in row groups (2) and (3),\nrespectively, and the monoBERTBase teacher model is shown in row (1). The (a), (b), (c) rows of\nrow groups (2) and (3) correspond to the three approaches presented above. The ﬁnal column shows\ninference latency measured on an NVIDIA RTX 2080Ti GPU.\nWe see that ranker distillation alone performs the worst; the authors reported a statistically signiﬁcant\ndecrease in effectiveness from the teacher model across all metrics and both test collections. Both\nLM distillation followed by ﬁne-tuning and LM distillation followed by ranker distillation led to\nstudent models comparable to the teacher in effectiveness. We see that in terms of MRR, “LM +\nranker distillation” outperforms “LM distillation + ﬁne-tuning” on the MS MARCO passage ranking\ntest collection, but the other way around for the TREC 2019 Deep Learning Track document ranking\ntest collection; note though, that the ﬁrst has far more queries than the second and thus might provide\na more stable characterization of effectiveness. Overall, the six-layer distilled model can perform\nslightly better than the teacher model while being twice as fast,108 whereas the four-layer distilled\nmodel gains a 9×speedup in exchange for a small decrease in effectiveness.\nAs another example of explorations in knowledge distillation, Li et al. [2020a] investigated how well\ntheir PARADE model performs when distilled into student models that range in size. Speciﬁcally,\nthey examined two approaches:\n1. train the full PARADE model using a smaller BERT variant distilled fromBERTLarge by Turc\net al. [2019] in place of BERTBase, and\n2. apply ranker distillation with the MSE distillation objective, where PARADE trained with\nBERTBase is used as the teacher model, and the student model is PARADE with a smaller BERT\nvariant (i.e., one of the pre-distilled models provided by Turc et al. [2019]).\nExperimental results for Robust04 title queries are shown in Table 24, with ﬁgures copied from Li\net al. [2020a]. Row (1) presents the effectiveness of the teacher model, which is the same model\nshown in row (6f) in Table 18. However, in order to reduce the computational requirements, the\nexperimental setup here differs from that used in Table 18 in two ways: fewer terms per document\nare considered (1650 rather than 3250) and fewer documents are being reranked (100 rather than\n1000); thus, the starting effectiveness is lower. Rows (2–8) present the distillation results: The\n108We suspect that the slightly higher effectiveness is due to a regularization effect, but this ﬁnding needs more\ndetailed investigation.\n99\nRobust04\nTrain Distill Parameters Latency\nMethod L / H nDCG@20 nDCG@20 (Count) (ms / doc)\n(1) Base 12 / 768 0.5252 - 123M 4.93\n(2) (unnamed) 10 / 768 0.5168 0.5296 † 109M 4.19\n(3) (unnamed) 8 / 768 0.5168 0.5231 95M 3.45\n(4) Medium 8 / 512 0.5049 0.5110 48M 1.94\n(5) Small 4 / 512 0.4983 0.5098 † 35M 1.14\n(6) Mini 4 / 256 0.4500 0.4666 † 13M 0.53\n(7) (unnamed) 2 / 512 0.4673 0.4729 28M 0.74\n(8) Tiny 2 / 128 0.4216 0.4410 † 5M 0.18\nTable 24: The effectiveness of training PARADE using a smaller BERT vs. distilling a BERTBase\nPARADE teacher into smaller BERT models on Robust04 title queries. Inference times were\nmeasured on a Google TPU v3-8. The symbol †indicates a signiﬁcant improvement of a “Distill”\nmodel over the corresponding “Train” model (pairedt-test, p< 0.05).\n“Train” column shows the results of training PARADE with BERT models of different sizes. This\ncorresponds to the LM distillation plus ﬁne-tuning setting from Gao et al. [2020c] (except that the\nfull PARADE model involves more than just ﬁne tuning). The “Distill” column shows the results\nof distilling PARADE from a teacher using BERTBase, into smaller, already distilled students. This\ncorresponds to the LM distillation plus ranker distillation setting from Gao et al. [2020c]. Inference\ntimes were measured using a Google TPU v3-8 with a batch size of 32. The symbol †indicates a\nsigniﬁcant improvement of a “Distill” model over the corresponding “Train” model, as determined\nby a paired t-test (p< 0.05).\nComparing the “Train” and “Distill” columns, it is clear that distilling into a smaller model is\npreferable to training a smaller model directly. The models under the ranker distillation condition\nare always more effective than the models that are trained directly, and this increase is statistically\nsigniﬁcant in most cases. These results are consistent with the ﬁnding of Gao et al. [2020c], at least\non the MS MARCO passage ranking test collection.\nIn rows (2) and (3) of Table 24, we see that reducing the number of transformer encoder layers in\nBERTBase under the “Train” condition sacriﬁces only a tiny bit of nDCG@20 for noticeably faster\ninference. However, the “Distill” versions of these models perform comparably to the original\nBERTBase version, indicating that distillation into a “slightly smaller” model can improve efﬁciency\nwithout harming effectiveness. The same trends continue with smaller BERT variants, with effec-\ntiveness decreasing as the model size decreases. We also see that ranker distillation is consistently\nmore effective than directly training smaller models. The difference between the teacher and ranker-\ndistilled models becomes statistically signiﬁcant from row (4) onwards. This indicates that ranker\ndistillation can be used to eliminate about a quarter of PARADE’s parameters and reduce inference\nlatency by about a third without signiﬁcantly harming the model’s effectiveness.\nThe papers of Gao et al. [2020c] and Li et al. [2020a], unfortunately, explored different datasets\nand different metrics with no overlap—thus preventing a direct comparison. Furthermore, there are\ntechnical differences in their approaches: Gao et al. [2020c] began with TinyBERT’s distillation\nobjective [Jiao et al., 2019] to produce their smaller BERT models. On the other hand, Li et al. [2020a]\nused as starting points the pre-distilled models provided by Turc et al. [2019]. Since the starting\npoints differ, it is not possible to separate the impact of the inherent quality of the smaller BERT\nmodels from the impact of the PARADE aggregation mechanisms in potentially compensating for a\nsmaller but less effective BERT model. Nevertheless, both papers seem to suggest that ﬁne-tuning a\nsmaller model directly is less effective than distilling into a smaller model from a ﬁne-tuned (larger)\nteacher, although the evidence is equivocal from Gao et al. [2020c] because only one of the two test\ncollections support this observation.\nHowever, beyond text ranking, we ﬁnd broader complementary support for this conclusion: results\non NLP tasks show that training a larger model and then compressing it is more computationally\nefﬁcient than spending the comparable resources directly training a smaller model [Li et al., 2020b].\nWe also note the connection here with the so-called “Lottery Ticket Hypothesis” [Frankle and Carbin,\n100\n2019, Yu et al., 2020a], although more research is needed here to fully reconcile all these related\nthreads of work.\nTakeaway Lessons. Knowledge distillation is a general-purpose approach to controlling effective-\nness/efﬁciency tradeoffs with neural networks. It has previously been demonstrated for a range of\nnatural language processing tasks, and recent studies have applied the approach to text ranking as\nwell. While knowledge distillation inevitably degrades effectiveness, the potentially large increases\nin efﬁciency make the tradeoffs worthwhile under certain operating scenarios. Emerging evidence\nsuggests that the best practice is to distill a large teacher model that has already been ﬁne-tuned for\nranking into a smaller pretrained student model.\n3.5.2 Ranking with Transformers: TK, TKL, CK\nEmpirically, BERT has proven to be very effective for many NLP and information access tasks.\nCombining this robust ﬁnding with the observation that BERT appears to be over-parameterized (for\nexample, Kovaleva et al. [2019]) leads to the interesting question of whether smaller models might be\njust as effective, particularly if limited to a speciﬁc task such as text ranking. Knowledge distillation\nfrom larger BERT models into smaller BERT models represents one approach to answering this\nquestion (discussed above), but could we arrive at better effectiveness/efﬁciency tradeoffs if we\nredesigned neural architectures from scratch?\nHofstätter et al. [2019] and Hofstätter et al. [2020] tried to answer this question by proposing a\ntext ranking model called the Transformer Kernel (TK) model, which might be characterized as a\n“clean-slate” redesign of transformer architectures speciﬁcally for text ranking. The only common\nfeature between monoBERT and the TK model is that both use transformers to compute contextual\nrepresentations of input tokens. Speciﬁcally, TK uses separate transformer stacks to compute\ncontextual representations of query terms and terms from the candidate text, which are then used to\nconstruct a similarity matrix that is consumed by a KNRM variant [Xiong et al., 2017] (discussed in\nSection 1.2.4). Since the contextual representations of texts from the corpus can be precomputed and\nstored, this approach is similar to KNRM in terms of the computational costs incurred at inference\ntime (plus the small amount of computation needed to compute a query representation).\nThe idea of comparing precomputed term embeddings within an interaction-based model has been well\nexplored in the pre-BERT era, with models like POSIT-DRMM [McDonald et al., 2018], which used\nRNNs to produce contextual embeddings but consumed those embeddings with a more complicated\narchitecture involving attention and pooling layers. The main innovation in the Transformer Kernel\nmodel is the use of transformers as encoders to produce contextual embeddings, which we know are\ngenerally more effective than CNNs and RNNs on a wide range of NLP tasks.\nIn more detail, given a sequence of term embeddings t1,...,t n, TK uses a stack of transformer\nencoder layers to produce a sequence of contextual embeddings T1,...,T n:\nT1,...,T n = Encoder3(Encoder2(Encoder1(t1,...,t n)). (40)\nThis is performed independently for terms from the query and terms in the texts from the corpus (the\nlatter, as we note, can be precomputed). The contextual embeddings from the query and candidate\ntext are then used to construct a similarity matrix that is passed to the KNRM component, which\nproduces a relevance score that is used for reranking. Hofstätter et al. [2019] pointed out that the\nsimilarity matrix constitutes an information bottleneck, which provides a straightforward way to\nanalyze the term relationships learned by the transformer stack. An intentionally simpliﬁed diagram\nof the TK architecture is shown in Figure 17, where we focus on the high-level design and elide a\nnumber of details.\nIn a follow-up, Hofstätter et al. [2020] proposed the TK with local attention model (TKL), which\nreplaced the transformer encoder layers’ self-attention with local self-attention, meaning that the\nattention for a distant term (deﬁned as more than 50 tokens away) is always zero and thus does not\nneed to be computed. TKL additionally uses a modiﬁed KNRM component that performs pooling\nover windows of document terms rather than over the entire document.\nExtending these idea further, Mitra et al. [2020] extended TK with the Conformer Kernel (CK) model,\nwhich adds an explicit term-matching component based on BM25 and two efﬁciency improvements:\nassuming query term independence [Mitra et al., 2019] and replacing the transformer encoder layers\nwith new “conformer” layers. The query term independence assumption is made by applying the\n101\nE1\nq1\nE2\nq2\nE3\nq3\nF1\nd1\nF2\nd2\nFm\ndm…\n… … … … … ……\ns\nE1E2E3\nF1 F2 Fm\n…\n…\n…\nU1 U2 U3 V1 V2 Vm…\n…\nFigure 17: The architecture of the Transformer Kernel (TK) model. The main idea is to use separate\ntransformer stacks to independently compute contextual representations for the query and candidate\ntext, and then construct a similarity matrix that is consumed by a pre-BERT interaction-based model.\nThis illustration contains intentional simpliﬁcations to clearly convey the model’s high-level design.\nencoder layers to only the document (i.e., using non-contextual query term embeddings) and applying\nKNRM’s aggregation to score each query term independently, which are then summed. Similar\nto TKL’s local attention, the proposed conformer layer is a transformer encoder layer in which\nself-attention is replaced with separable self-attention and a grouped convolution is applied before\nthe attention layer.\nThe TK, TKL, and CK models are trained from scratch (yes, from scratch) with an embedding\nlayer initialized using context-independent embeddings. The TK and TKL models use GloVe\nembeddings [Pennington et al., 2014], and the CK model uses a concatenation of the “IN” and “OUT”\nvariants of word2vec embeddings [Mitra et al., 2016]. This design choice is very much in line with\nthe motivation of rethinking transformers for text ranking from the ground up. However, these designs\nalso mean that the models do not beneﬁt from self-supervised pretraining that is immensely beneﬁcial\nfor BERT (see discussion in Section 3.1). While the models do make use of trained embeddings, the\ntransformer layers used to contextualize the embeddings are randomly initialized.\nExperiments demonstrating the effectiveness of TK, TKL, and CK are shown in Table 25, pieced\ntogether from a number of sources. Results on the development set of the MS MARCO passage\nranking test collection for TK, rows (4b)–(4d), are taken from Hofstätter et al. [2020], as well as their\nreplication of monoBERT baselines, row group (3). For reference, row (1) repeats the effectiveness\nof monoBERTLarge from Table 5. Although Hofstätter et al. [2020] additionally reported results\non the MS MARCO document ranking test collection, we do not include those results here since\nthe TKL and CK papers did not evaluate on that test collection. For TKL, results are copied from\nHofstätter et al. [2020] on the TREC 2019 Deep Learning Track document ranking test collection,\nand for CK, results are copied from Mitra et al. [2020] for the same test collection. Fortunately, TK\nmodel submissions to the TREC 2019 Deep Learning Track [Craswell et al., 2020] provide a bridge\nto help us understand the relationship between these models. From what we can tell, the TK (3 layer,\n102\nMS MARCO Passage TREC 2019 DL Doc\nMethod MRR@10 MRR nDCG@10\n(1) monoBERTLarge= Table 5, row (3b) 0.372 - -\n(2a) Co-PACRR [Hofstätter et al., 2020] 0.273\n(2b) ConvKNRM [Hofstätter et al., 2020] 0.277\n(2c) FastText + ConvKNRM Hofstätter et al. [2019] 0.278\n(3a) monoBERTBase[Hofstätter et al., 2020] 0.376 - -\n(3b) monoBERTLarge[Hofstätter et al., 2020] 0.366 - -\n(4a) TK (3 layer, FastText, window pooling) - 0.946 0.644\n(4b) TK (3 layer) 0.314 0.942 0.605\n(4c) TK (2 layer) 0.311 - -\n(4d) TK (1 layer) 0.303 - -\n(5) TKL (2 layer) - 0.957 0.644\n(6a) CK (2 layer) - 0.845 0.554\n(6b) CK (2 layer) + Exact Matching - 0.906 0.603\nTable 25: The effectiveness of the TK, TKL, and CK models on the development set of the MS\nMARCO passage ranking test collection and the TREC 2019 Deep Learning Track document ranking\ntest collection.\nFastText, window pooling) results, row (4a), corresponds to runTUW19-d3-re and the TK (3 layer)\nresults, row (4b), corresponds to runTUW19-d2-re.\nTo aid in the interpretation of these results, row group (2) shows results from a few pre-BERT\ninteraction-based neural ranking models. Rows (2a) and (2b) are taken directly from Hofstätter et al.\n[2020] for Co-PACRR [Hui et al., 2018] and ConvKNRM [Dai et al., 2018]. Row (2c) is taken\nfrom Hofstätter et al. [2019], which provides more details on the ConvKNRM design. These three\nresults might be characterized as the state of the art in pre-BERT interaction-based neural ranking\nmodels, just prior to the community’s shift over to transformer-based approaches. We see that the TK\nmodel is more effective than these pre-BERT models, but still much less effective than monoBERT.\nThus, TK can be characterized as a less effective but more efﬁcient transformer-based ranking model,\ncompared to monoBERT. This can be seen in Figure 18, taken from Hofstätter et al. [2020], which\nplots the effectiveness/efﬁciency tradeoffs of different neural ranking models. With a latency budget\nof less than around 200ms, TK is more effective than monoBERTBase and TK represents strictly an\nimprovement over pre-BERT models across all latency budgets.\nInterestingly, there is little difference between the two-layer and three-layer TK models, which is\nconsistent with the results presented in the context of distillation above. On the TREC 2019 Deep\nLearning Track document ranking test collection, the TK and TKL models perform substantially\nbetter than the CK model,109 though the conformer layers used by CK are more memory-efﬁcient.\nThat is, the design of CK appears to further trade effectiveness for efﬁciency. Incorporating an\nexact matching component consisting of BM25 with learned weights improves the effectiveness of\nthe CK model, but it does not reach the effectiveness of TK or TKL. There does not appear to be\nmuch difference in the effectiveness of TK vs. TKL. Unfortunately, it is difﬁcult to quantify the\neffectiveness/efﬁciency tradeoffs of TKL and CK compared to TK, as we are not aware of a similar\nanalysis along the lines of Figure 18.\nTakeaway Lessons. What have we learned from these proposed transformer architectures for text\nranking? Thus far, the results are a bit mixed.\nOn the one hand, we believe it is very important for the community to explore a diversity of\napproaches, and to rethink how we might redesign transformers for text ranking given a blank slate.\nThe TK/TKL/CK models have tackled this challenge head on, but it is too early to draw any deﬁnitive\nconclusions from these efforts. Furthermore, CK represents an exploration of the space between\npre-BERT interaction-based neural ranking models and TK, i.e., even more computationally efﬁcient,\nbut also even less effective. There is, in our opinion, an even more interesting tradeoff space between\n109Note that TK and TKL in these experiments performed reranking on a ﬁxed candidate set (what the TREC\n2019 Deep Learning Track organizers called the “reranking” condition), whereas CK reranked the output of\nits own ﬁrst-stage retrieval (the “full ranking” condition).\n103\n1 50 100 150 200 250 3000.180\n0.200\n0.220\n0.240\n0.260\n0.280\n0.300\n0.320\n0.340\nQuery Latency (milliseconds)\nMRR@10\nTK\nCONV-KNRM\nCO-PACRR\nBERT-Base\nBERT-Large\nBM25\nEffectiveness/Efﬁciency Tradeoffs on MS MARCO Passage\nFigure 18: Effectiveness/efﬁciency tradeoffs of the TK model compared to BERT and other pre-BERT\ninteraction-based neural ranking models on the MS MARCO passage ranking test collection, taken\nfrom Hofstätter et al. [2020].\nTK and monoBERT. That is, can we give up a bit more of TK’s efﬁciency to close its effectiveness\ngap with monoBERT?\nOn the other hand, it is unclear whether the current design of the TK/TKL/CK models can beneﬁt\nfrom the massive amounts of self-supervised pretraining that is the hallmark of BERT, and based on\nthe discussion in Section 3.1, is the main source of the big leaps in effectiveness we’ve witnessed on\na variety of NLP tasks. In other words, what is more important, pretraining (to produce high-quality\ncontextual representations) or the model architecture (to capture relevance based on the query and\ndocument representations)? Boytsov and Kolter [2021] explored architectures that use a pre-neural\nlexical translation model to aggregate evidence from BERT-based contextualized embeddings; this\ndeviates from the standard cross-encoder design to eliminate attention-based interactions between\nterms from the queries and the documents. Their results were able to isolate the contributions of con-\ntextual representations and thus highlights the importance of pretraining. One possible interpretation\nis that given sufﬁciently good representations of the queries and texts from the corpus, the “relevance\nmatching machinery” is perhaps not very important. Currently, we still lack deﬁnitive answers, but\nthis represents an interesting future direction worth exploring.\n3.5.3 Ranking with Sequence-to-Sequence Models: monoT5\nAll of the transformer models for text ranking that we have discussed so far in this section can be\ncharacterized as encoder-only architectures. At a high level, these models take vector representations\nderived from a sequence of input tokens and emit relevance scores. However, the original transformer\ndesign [Vaswani et al., 2017] is an encoder–decoder architecture, where an input sequence of tokens\nis converted into vector representations, passed through transformer encoder layers to compute\nan internal representation (the encoding phase), which is then used in transformer decoder layers\nto generate a sequence of tokens (the decoding phase). While the alignment is imperfect, it is\nhelpful to characterize previous models in terms of this full encoder–decoder transformer architecture.\nGPT [Radford et al., 2018] described itself as a transformer decoder, and to ﬁt with this analogy, Raffel\net al. [2020] characterized BERT as being an “encoder-only” design.\nIn NLP, encoder–decoder models are also referred to as sequence-to-sequence models because\na sequence of tokens comes in and sequence of tokens comes out. This input–output behavior\nintuitively captures tasks such as machine translation—where the input sequence is in one language\nand the model output is the input sequence translated into a different language—and abstractive\n104\nsummarization—where the input sequence is a long(er) segment of text and the output sequence\ncomprises a concise summary of the input sequence capturing key content elements.\nUntil recently, tasks whose outputs were not comprised of a sequence of tokens, such as the tasks\ndiscussed in Section 3.1, were mostly addressed by encoder-only models. These tasks had a natural\nmapping to the architecture of a model like BERT: Classiﬁcation tasks over inputs took advantage of\nthe [CLS] representation and [SEP] tokens as delimiters in a straightforward manner. Even though\nsequence labeling tasks such as named-entity recognition can be conceived as outputting a sequence\nof tags, a formulation as token-level classiﬁcation (over the tag space) was more natural since there is\na strict one-to-one correspondence between a token and its label (whereas most sequence-to-sequence\nmodels do not rigidly enforce this one-to-one correspondence). In this case, the contextual embedding\nof each token can be used for classiﬁcation in a straightforward manner. However, with the advent\nof pretrained sequence-to-sequence models such as T5 (Text-to-Text Transfer Transformer) [Raffel\net al., 2020], UniLM [Dong et al., 2019], BART [Lewis et al., 2020b], and PEGASUS Zhang et al.\n[2020c], researchers began to explore the use of sequence-to-sequence models for a variety of natural\nlanguage processing tasks.\nThe main idea introduced by Raffel et al. [2020] is to cast every natural language processing task as\nfeeding a sequence-to-sequence model some input text and training it to generate some output text.\nThese tasks include those that are more naturally suited for sequence-to-sequence models such as\nmachine translation, as well as tasks for which a sequence-to-sequence formulation might seem a\nbit “odd’, such as detecting if two sentences are paraphrases, detecting if a sentence is grammatical,\nword sense disambiguation, and sentiment analysis, which are more accurately characterized as either\nclassiﬁcation or regression tasks. The authors even recast the co-reference resolution task into this\nsequence-to-sequence framework.\nLike BERT, T5 is ﬁrst pretrained on a large corpus of diverse texts using a self-supervised objective\nsimilar to masked language modeling in BERT, but adapted for the sequence-to-sequence context.\nJust like in BERT, these pretrained models (which have also been made publicly available) are then\nﬁne-tuned for various downstream tasks using task-speciﬁc labeled data, where each task is associated\nwith a speciﬁc input template.\nThese templates tell the model “what to do”. For example, to translate a text from English to German,\nthe model is fed the following template:\ntranslate English to German: [input] (41)\nwhere the sentence to be translated replaces [input] and “translate English to German:” is a literal\nstring, which the model learns to associate with a speciﬁc task during the ﬁne-tuning process. In\nother words, a part of the input sequence consists of a string that informs the model what task it is to\nperform. To give another example, a classiﬁcation task such as sentiment analysis (with the SST2\ndataset) has the following template:\nsst2 sentence: [input] (42)\nwhere, once again, [input] is replaced with the actual input sentence and “sst2 sentence:” is a\nliteral string indicating the task. For this task, the “ground truth” (i.e., output sequence) for the\nsequence-to-sequence model is a single token, either “positive” or “negative” (i.e., the literal string).\nIn other words, given training examples processed into the above template, the model is trained to\ngenerate either the token “positive” or “negative”, corresponding to the prediction.\nThis idea is pushed even further with regression tasks such as the Semantic Textual Similarity\nBenchmark [Cer et al., 2017], where the target outputs are human-annotated similarity scores\nbetween one and ﬁve. In this case, the target output is quantized to the nearest tenth, and the model is\ntrained to emit that literal token. Raffel et al. [2020] showed that this “everything as sequence-to-\nsequence” formulation is not only tenable, but achieves state-of-the-art effectiveness (at the time the\nmodel was introduced) on a broad range natural language processing tasks. Although it can seem\nunnatural for certain tasks, this formulation has proven to be quite powerful; later work extended this\napproach to even more tasks, including commonsense reasoning [Khashabi et al., 2020, Yang et al.,\n2020a] and fact veriﬁcation [Pradeep et al., 2021a].\nInspired by the success of the sequence-to-sequence formulation, Nogueira et al. [2020] investigated\nif the T5 model could also be applied to text ranking. It is, however, not entirely straightforward how\nthis could be accomplished. There are a number of possible formulations: As text ranking requires\n105\nMS MARCO Passage\nDevelopment Test\nMethod # Params MRR@10 MRR@10\n(1) BM25 - 0.184 0.186\n(2) + BERT-large 340 M 0.372 0.365\n(3a) + T5-base 220 M 0.381 -\n(3b) + T5-large 770 M 0.393 -\n(3c) + T5-3B 3 B 0.398 0.388\nTable 26: The effectiveness of monoT5 on the MS MARCO passage ranking task.\na score for each document to produce a ranked list, T5 could be trained to directly produce scores\nas strings like in the STS-B task, if we found the right test collection. Graded relevance judgments\nmight work, but unfortunately most test collections of this type are quite small; the MS MARCO\npassage ranking test collection provides only binary relevance. An alternative would be to encode all\nthe candidate texts (from initial retrieval) into a single input template and train the model to select the\nmost relevant ones. This would be similar to the listwise approach presented in Section 3.4.2, but\nas we have discussed, documents can be long, so this is not feasible given the length limitations of\ncurrent transformer models. Thus, ranking necessitates multiple inference passes with the model and\nsomehow aggregating the outputs.\nNogueira et al. [2020] ultimately solved these challenges by exploiting internal model representations\njust prior to the generation of an output token for relevance classiﬁcation. Their model, dubbed\n“monoT5” (mirroring “monoBERT”), uses the following input template:\nQuery: [q] Document: [d] Relevant: (43)\nwhere [q] and [d] are replaced with the query and document texts, respectively, and the other parts\nof the template are verbatim string literals. The model is ﬁne-tuned to produce the tokens “true” or\n“false” depending on whether the document is relevant or not to the query. That is, “true” and “false”\nare the “target tokens” (i.e., ground truth predictions in the sequence-to-sequence transformation).\nAt inference time, to compute probabilities for each query–text pair, a softmax is applied only to the\nlogits of the “true” and “false” tokens in the ﬁrst decoding step.110 Speciﬁcally, the ﬁnal estimate we\nare after in relevance classiﬁcation, P(Relevant = 1|q,d), is computed as the probability assigned to\nthe “true” token normalized in this manner. Similar to monoBERT, monoT5 is deployed as a reranker.\nHow does monoT5 compare against monoBERT? Results on the MS MARCO passage ranking test\ncollection are presented in Table 26, copied from Nogueira et al. [2020]. Interestingly, monoT5-base\nhas a higher effectiveness than monoBERT-large, row (2) vs. row (3a), but it has fewer parameters\n(220M vs. 340M) and it is approximately two times faster at inference. Using larger models increases\neffectiveness but at increased costs for memory and computation: monoT5-3B is 1.6 points better\nthan monoT5-base but its approximately 14 times larger and 10 times slower, row (3a) vs. row (3c).\nNot only does monoT5 appear to be more effective overall, but more data efﬁcient to train as well.\nThe effectiveness of monoT5-base vs. monoBERTBase on the development set of the MS MARCO\npassage ranking task as a function of the amount of training data provided during ﬁne-tuning is shown\nin Figure 19. The monoBERTBase values are exactly the same as in Figure 8, since both are copied\nfrom Nogueira et al. [2020]. In these experiments, both models were ﬁne-tuned with 1K, 2.5K, and\n10K positive query–passage instances and an equal number of negative instances sampled from the\nfull training set of the MS MARCO passage ranking test collection. Effectiveness on the development\nset is reported in terms of MRR@10 with the standard setting of reranking k= 1000 candidate texts\nfrom BM25; note that the x-axis is in log scale. For the sampled conditions, the experiment was\nrepeated ﬁve times, and the plot shows the 95% conﬁdence intervals. The setting that used all training\ninstances was only run once due to computational costs. The dotted horizontal black line shows the\neffectiveness of BM25 without any reranking.\nGiven the same amount of training, monoT5 appears to consistently outperform monoBERT. With just\n1K positive query–passage instances, monoT5 is able to exceed the effectiveness of BM25; in contrast,\n110The T5 model tokenizes sequences using the SentencePiece model [Kudo and Richardson, 2018], which\nmight split a word into subwords. The selected targets (“true” and “false”) are represented as single tokens;\nthus, each class is represented by a single logit.\n106\n1 2.5 10 530\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n# relevant query-doc training instances (thousands)\nMRR@10\nT5-BASE\nBERT-BASE\nBM25\nEffectiveness vs. Training Data Size on MS MARCO Passage\nFigure 19: The effectiveness of monoT5-base and monoBERTBase on the development set of the MS\nMARCO passage ranking test collection varying the amount of training data used to ﬁne-tune the\nmodels. Results report means and 95% conﬁdence intervals over ﬁve trials. Note that the x-axis is in\nlog scale.\nRobust04 Core17 Core18\nMethod MAP nDCG@20 MAP nDCG@20 MAP nDCG@20\n(1a) Birch = Table 10, row (4b) 0.3697 0.5325 0.3323 0.5092 0.3522 0.4953\n(1b) PARADE = Table 18, row (6f) 0.4084 0.6127 - - - -\n(2a) BM25 0.2531 0.4240 0.2087 0.3877 0.2495 0.4100\n(2b) + T5-base 0.3279 0.5298 0.2758 0.5180 0.3125 0.4741\n(2c) + T5-large 0.3288 0.5345 0.2799 0.5356 0.3330 0.5057\n(2d) + T5-3B 0.3876 0.6091 0.3193 0.5629 0.3749 0.5493\n(3a) BM25 + RM3 0.2903 0.4407 0.2823 0.4467 0.3135 0.4604\n(3b) + T5-base 0.3340 0.5532 0.3067 0.5203 0.3364 0.4698\n(3c) + T5-large 0.3382 0.5287 0.3109 0.5299 0.3557 0.5007\n(3d) + T5-3B 0.4062 0.6122 0.3564 0.5612 0.3998 0.5492\nTable 27: The effectiveness of monoT5 on the Robust04, Core17, and Core18 test collections. Note\nthat the T5 models are trained only on the MS MARCO passage ranking test collection and thus these\nrepresent zero-shot results.\nmonoBERT exhibits the “a little bit is worse than none” behavior [Zhang et al., 2020g], and doesn’t\nbeat BM25 until it has been provided around 10K positive training instances. The effectiveness gap\nbetween the two models narrows as the amount of training data grows, which suggests that monoT5\nis more data efﬁcient and able to “get more” out of limited training data.\nAnother way to articulate the ﬁndings in Figure 19 is that monoT5 appears to excel at few-shot\nlearning. Taking this idea to its logical end, how might the model perform in a zero-shot setting? We\nhave already seen that monoBERT exhibits strong cross domain transfer capabilities, for example, in\nthe context of pre–ﬁne-tuning techniques (see Section 3.2.4) and Birch (see Section 3.3.1), so might\nwe expect monoT5 to also perform well?\nIndeed, Nogueira et al. [2020] explored exactly the zero-shot approach with monoT5, ﬁne-tuning\nthe model on the MS MARCO passage ranking test collection and directly evaluating on other test\ncollections: Robust04, Core17, and Core18. These results are shown in Table 27, with the best\nconﬁguration of Birch, copied from Table 10 and shown here in row (1a). The authors used Birch as\ntheir baseline for comparison, which we have retained here. Row group (2) presents results reranking\nﬁrst-stage candidates from BM25 using Anserini, and row group (3) present results reranking ﬁrst-\n107\nstage candidates from BM25 + RM3. Each row indicates the size of the T5 model (base, large, and\n3B). As expected, effectiveness improves with increased model size, although the differences between\nthe base and large variants are relatively small. The T5-3B model, where “3B” denotes three billion\nparameters, achieves MAP and nDCG scores on Robust04 that are close to the best PARADE results\nreported in Section 3.3.4, repeated here as row (1b). Some caveats for this comparison, though: While\nPARADE is based on ELECTRA-base and is around 30×smaller than monoT5-3B, it was trained on\nRobust04 (via cross validation). In contrast, all monoT5 results are zero-shot.\nPerhaps it is no surprise that larger models are more effective, but how exactly does monoT5 “work”?\nOne salient difference is the encoder-only vs. encoder–decoder design, and Nogueira et al. [2020]\nargued that the decoder part of the model makes important contributions to relevance modeling. They\ninvestigated how the choice of target tokens impacts the effectiveness of the model, i.e., the prediction\ntarget or the ground truth “output sequence”. Instead of training the model to generate “true” and\n“false”, they reported a number of different conditions, e.g., swapping “true” and “false” so they mean\nthe opposite, mapping relevance to arbitrary words such as “hot” vs. “cold”, “apple” vs. “orange”, and\neven meaningless subwords. Perhaps not surprisingly, when the model is ﬁne-tuned with sufﬁcient\ntraining data, this choice doesn’t really matter (i.e., little impact on effectiveness). However, in a\nlow-resource setting (fewer training examples), the authors noticed that the choice of the target token\nmatters quite a bit. That is, attempting to coax the model to associate arbitrary tokens with relevance\nlabels becomes more difﬁcult with fewer training examples than the “true”/“false” default, which\nsuggests that the model is leveraging the decoder part of the network to assist in building a relevance\nmatching model. Exactly how, Nogueira et al. [2020] offered no further explanation.\nThese results support the ﬁnding that with transformer-based ranking models, the design of the input\ntemplate (i.e., how the various components of the input are “packed” together and fed to the model)\ncan have a large impact on effectiveness [Puri and Catanzaro, 2019, Schick and Schütze, 2021, Haviv\net al., 2021, Le Scao and Rush, 2021]. Some of these explorations in the context of monoBERT were\npresented in Section 3.2.2. Those experiments showed that the[SEP] token plays an important role in\nseparating the query from the candidate text, and using this special token is (slightly) more effective\nthan using natural language tokens such as “Query:” and “Document:” as delimiters. However, this\nstrategy cannot be directly applied to T5 because the model was not pretrained with a [SEP] token.\nIn the original formulation by Raffel et al. [2020], all tasks were phrased in terms of natural language\ntemplates (without any special tokens), and different from BERT, segment embeddings were not\nused in the pretraining of T5. Hence, monoT5 relies solely on the literal token “Document:” as a\nseparator between query and document segments. This raises the interesting question of whether there\nare “optimal” input and output templates for sequence-to-sequence models. And if so, how might\nwe automatically ﬁnd these templates to help the model learn more quickly, using fewer training\nexamples? These remain open research questions awaiting further exploration.\nExtensions to duoT5. The parallel between monoBERT and monoT5, both trained as relevance\nclassiﬁers, immediately suggests the possibility of a pairwise approach built on T5. Indeed, T5 can\nalso be used as a pairwise reranker, similar to duoBERT (see Section 3.4.1). This approach, proposed\nin a model called duoT5, was introduced by Pradeep et al. [2021b]. The model takes as input a query\nqand two documents (texts), di and dj in the following input template:\nQuery: q Document0: di Document1: dj Relevant: (44)\nThe model is ﬁne-tuned to produce the token “true” if di is more relevant than dj to query q, and\n“false” otherwise, just like in monoT5.\nAt inference time, the model estimates pi,j = P(di ≻dj|di,dj,q), i.e., the probability of text di\nbeing more relevant than text dj. Exactly as with monoT5, this probability is computed by applying a\nsoftmax to the logits of the “true” and “false” tokens. Similar to duoBERT, duoT5 is deployed as a\nsecond-stage reranker in a multi-stage reranking pipeline, in this case, over the results of monoT5.\nThe model generates all unique pairs (dj,dj) (at a particular cutoff), feeds them into the model, and\nthe resulting pairwise probabilities pi,j are aggregated to form a single relevance score si for each\ntext di; the candidate texts are then reranked by this score. We refer the reader to Pradeep et al.\n[2021b] for more details on how duoT5 is ﬁne-tuned and how the aggregation scores are computed.\nThe mono/duoT5 pipeline was evaluated at the TREC 2020 Deep Learning Track. Combined with the\ndoc2query document expansion technique (presented later in Section 4.3), the complete architecture\nobtained the top result on document ranking and the second best result on passage ranking [Craswell\n108\nTREC 2020 DL Passage TREC 2020 DL Doc\nMethod MAP nDCG@10 MAP nDCG@10\nBM25 + RM3 0.3019 0.4821 0.4006 0.5248\nBM25 + RM3 + monoT5-3B + duoT5-3B 0.5355 0.7583 0.5270 0.6794\nTable 28: The effectiveness of mono/duoT5 on the TREC 2020 Deep Learning Track passage and\ndocument ranking test collections.\net al., 2021b]. The effectiveness of conﬁgurations without document expansion are show in Table 28,\ncopied from Pradeep et al. [2021b]. Here, we clearly see a large gain from mono/duoT5 reranking,\nbut unfortunately, the ofﬁcial submissions did not include additional ablation conditions that untangle\nthe contributions of monoT5 and duoT5.\nTakeaway Lessons. Full encoder–decoder transformers are quite a bit different from encoder-\nonly architectures such as BERT, and designed for very different tasks (i.e., sequence-to-sequence\ntransformations, as opposed to classiﬁcation and sequence labeling). It is not immediately obvious\nhow such models can be adapted for ranking tasks, and the “trick” for coaxing relevance scores out of\nsequence-to-sequence models is the biggest contribution of monoT5. Experiments demonstrated that\nmonoT5 is indeed more effective than monoBERT: While larger model sizes play a role, empirical\nevidence suggests that size alone isn’t the complete story. The generation (decoder) part of the\ntransformer model clearly impacts ranking effectiveness, and while Nogueira et al. [2020] presented\nsome intriguing ﬁndings, there remain many open questions.\n3.5.4 Ranking with Sequence-to-Sequence Models: Query Likelihood\nLanguage modeling approaches have a long history in information retrieval dating back to the\ntechnique proposed by Ponte and Croft [1998] known as query likelihood. Query likelihood is\nsimple and intuitive: it says that we should rank documents based on ˆp= (Q|Md), the probability\nthat the query is “generated” by a model M of document d(hence, this is also called a generative\napproach to text ranking). The original formulation was based on unigram language models (multiple\nBernoulli, to be precise), and over the years, many researchers have explored richer language models\nas well as more sophisticated model estimation techniques. However, the query likelihood variant\nbased on a multinomial distribution with Dirichlet smoothing has proven to be the most popular;\nsee discussion and comparisons in Zhai [2008]. This ranking model, for example, is the default in\nthe Indri search engine [Metzler et al., 2004], which was highly inﬂuential around that time. In the\ncontext of (feature-based) learning to rank, features based on language modeling became one of many\nsignals that were considered in ranking.\nWith the advent of neural language models and pretrained transformers, we have witnessed the\nresurgence of generative approaches to retrieval, monoT5 in the previous section being a good example.\nAn alternative was proposed by dos Santos et al. [2020], which can be characterized as the ﬁrst attempt\nto implement query likelihood with pretrained transformers. The authors investigated both encoder–\ndecoder designs (BART) [Lewis et al., 2020b] as well as decoder–only designs (GPT) [Radford et al.,\n2018] to model the process of generating a query qgiven a relevant text das input.\nIn the context of GPT, the approach uses the following template for each (query, relevant text) pair:\n<bos> text <boq> query <eoq> (45)\nwhere everything before the special token<boq> (“beginning of query”) is considered the prompt, and\nthe model is ﬁne-tuned (via teacher forcing) to generate the query, ending with <eoq>. At inference\ntime, the relevance score si of text di is the probability estimated by the model for generating q:\nsi = P(q|di) =\n|q|∏\nj=1\nP(qj|q<j,di), (46)\nwhere qj is the j-th query term and q<j are the query terms before qj. Once trained, the model is\ndeployed in a standard reranking setting, where kcandidate texts {di}k\ni=1 are fed to the model to\ncompute {P(q|di)}k\ni=1, and these scores are used to rank the candidate texts.\n109\nYahooQA WikiQA WikipassageQA InsuranceQA\nMethod MAP MRR MAP MRR MAP MRR MAP MRR\n(1) Discriminative (BERT) 0.965 0.965 0.844 0.856 0.775 0.838 0.410 0.492\n(2) Discriminative (BART) 0.967 0.967 0.845 0.861 0.803 0.866 0.435 0.518\n(3) Generative (GPT2) 0.954 0.954 0.819 0.834 0.755 0.831 0.408 0.489\n(4) Generative (BART) 0.970 0.970 0.849 0.861 0.808 0.867 0.444 0.529\nTable 29: The effectiveness of reranking using query likelihood based on BART and GPT2 on various\ntest collections.\nSince BART is a sequence-to-sequence model (like T5), each (query, relevant text) pair becomes a\ntraining instance directly. That is, the relevant text is the input sequence and the query is the target\noutput sequence.\nTo ﬁne-tune their models, dos Santos et al. [2020] experimented with three different losses, but found\nthat hinge loss produced the best results on average:\nL=\n∑\n(q,d+,d−)∈D\nmax{0,−log P(q|d+) + logP(q|d−)}, (47)\nwhere d+ and d−are relevant and non-relevant texts for the query q, respectively, and D is the\ntraining dataset.\nThe authors also compared their proposed generative model with a discriminative approach. Using\nBART, the vector representation generated by the decoder for the ﬁnal query token is fed to a relevance\nclassiﬁcation layer that is trained using the same pairwise ranking loss used to train its generative\ncounterpart.\nResults for both methods on four publicly available answer selection datasets are presented in\nTable 29, directly copied from dos Santos et al. [2020]. The table shows results from the generative\nand discriminative methods ﬁne-tuned on a BART-large model, rows (2) and (4), as well as the\ngenerative method using GPT2-large, row (3). The authors additionally compared their proposed\nmethods against a discriminative BERT baseline, row (1), that uses the [CLS] vector as input to a\nbinary classiﬁcation layer, similar to monoBERT but using a different loss function.\nThe generative BART model gives slightly better results than the discriminative one, row (2) vs. row\n(4), in almost all metrics of the four datasets the authors evaluated on. Comparing the two query\nlikelihood implementations, we see that GPT2 is less effectiveness than BART, row (3) vs. row (4),\nthus providing additional evidence that MLM pretraining results in better models than LM pretraining\n(see Section 3.1). Since the authors did not use any of the datasets monoT5 was evaluated with, it is\ndifﬁcult to directly compare the two approaches.\nTakeaway Lessons. The query likelihood approach of dos Santos et al. [2020] complements Nogueira\net al. [2020] in demonstrating the effectiveness of sequence-to-sequence transformers for ranking.\nAdditionally, this work draws nice connections to the language modeling approach to IR that dates\nback to the late 1990s, providing a fresh new “twist” to well-studied ideas. Unfortunately, we are not\nable to directly compare the effectiveness of these two methods since they have not been evaluated\non common test collections. Nevertheless, ranking with generative models appears to be a promising\nfuture direction.\n3.6 Concluding Thoughts\nWe have arrived at the research frontier of text ranking using transformers in the context of reranking\napproaches. At a very high level, we can summarize the current developments as follows: First\ncame the basic relevance classiﬁcation approach of monoBERT, followed by model enhancements\nto address the model’s input length limitations (Birch, MaxP, CEDR, PARADE, etc.) as well as\nexploration of BERT variants. In parallel with better modeling, researchers have investigated more\nsophisticated training techniques (e.g., pre–ﬁne-tuning) to improve effectiveness.\nFollowing these initial developments, the design space of transformer architectures for ranking opened\nup into a diversity of approaches, with researchers branching off in many different directions. The\n110\nTK, TKL, and CK models represent a reductionist approach, rethinking the design of transformer\narchitectures from the ground up. Nogueira et al. [2019b] opted for the “more pretraining, bigger\nmodels” approach, taking advantage of broader trends in NLP. GPT-3 [Brown et al., 2020] is\nperhaps the most extreme expression of this philosophy to date. The insight of exploiting generative\napproaches for ranking was shared by dos Santos et al. [2020] as well, and together they highlight the\npotential of sequence-to-sequence models for text ranking.\nWhere do we go from here? Direct ranking with learned dense representations is an emerging\narea that we cover in Section 5, but beyond that lies unexplored ground. There are a number of\npromising future paths, which we return to discuss in Section 6. However, we ﬁrst turn our attention\nto techniques for enriching query and document representations.\n111\n4 Reﬁning Query and Document Representations\nThe vocabulary mismatch problem [Furnas et al., 1987]—where searchers and the authors of the texts\nto be searched use different words to describe the same concepts—was introduced in Section 1.2.2\nas a core problem in information retrieval. Any ranking technique that depends on exact matches\nbetween queries and texts suffers from this problem, and researchers have been exploring approaches\nto overcome the limitations of exact matching for decades. Text ranking models based on neural\nnetworks, by virtue of using continuous vector representations, offer a potential solution to the\nvocabulary mismatch problem because they are able to learn “soft” or semantic matches—this was\nalready demonstrated by pre-BERT neural ranking models (see Section 1.2.4).\nHowever, in the architectures discussed so far—either the simple retrieve-and-rerank approach or\nmulti-stage ranking—the initial candidate generation stage forms a critical bottleneck since it still\ndepends on exact matching (for example, using BM25). A relevant text that has no overlap with query\nterms will not be retrieved, and hence will never be encountered by any of the downstream rerankers.\nIn the best case, rerankers can only surface candidate texts that are deep in the ranked list (and as\nwe’ve seen, transformers are quite good at that). They, of course, cannot conjure relevant results out\nof thin air if none exist in the pool of candidates to begin with!\nIn practice, it is not likely that a relevant text has no overlap with the query,111 but it is common for\nrelevant documents to be missing a key term from the query (for example, the document might use\na synonym). Thus, the vocabulary mismatch problem can be alleviated in a brute force manner by\nsimply increasing the depth of the candidates that are generated in ﬁrst-stage retrieval. Relevant texts\nwill show up, just deeper in the ranked list. We see this clearly in Figure 9 (from Section 3.2.2), where\nmonoBERT is applied to increasing candidate sizes from bag-of-words queries scored with BM25:\neffectiveness increases as more candidates are examined. Nevertheless, this is a rather poor solution.\nThe most obvious issue is that reranking latency increases linearly with the size of the candidates list\nunder consideration, since inference needs to be applied to every candidate—although this can be\nmitigated by multi-stage rerankers that prune the candidates successively, as discussed in Section 3.4.\nThe solution, naturally, is to reﬁne (or augment) query and document representations to bring them\ninto closer “alignment” with respect to the user’s information need. In this section, we present a\nnumber of such techniques based on pretrained transformers that operate on thetextual representations\nof queries and documents. These can be characterized as query and document expansion techniques,\nwhich have a rich history in information retrieval, dating back many decades [Carpineto and Romano,\n2012].112 We begin with a brief overview in Section 4.1, but our treatment is not intended to be\ncomprehensive. Instead, we focus only on the preliminaries necessary to understand query and\ndocument expansion in the context of pretrained transformer models.\nOur discussion of query and document expansion in this section proceeds as follows: Following high-\nlevel general remarks, in Section 4.2 we dive into query expansion techniques using pseudo-relevance\nfeedback that take advantage of transformer-based models. We then present four document expansion\ntechniques: doc2query [Nogueira et al., 2019b], DeepCT [Dai and Callan, 2019a], HDCT [Dai and\nCallan, 2020], and DeepImpact [Mallia et al., 2021]. All of these techniques focus on manipulating\nterm-based (i.e., textual) representations of queries and texts from the corpus. In Section 4.7 we turn\nour attention to techniques that manipulate query and text representations that are not based directly\non textual content.\nThe discussions in this section, particularly ones involving non-textual representations in Section 4.7,\nset up a nice segue to learned dense representations, the topic of Section 5. Here, query and document\nexpansions can be viewed as attempts to tackle the vocabulary mismatch problem primarily in terms\nof textual representations—by augmenting either queries or documents with “useful” terms to aid in\n111Although it is possible in principle for texts that contain zero query terms to be relevant to an information\nneed, there is a closely-related methodological issue of whether test collections contain such judgments. With\nthe pooling methodology that underlies the construction of most modern test collections (see Section 2.6),\nonly the results of participating teams are assessed. Thus, if participating systems used techniques that rely on\nexact term matching, it is unlikely that a relevant document with no query term overlap will ever be assessed\nto begin with. For this reason, high-quality test collections require diverse run submissions.\n112In this section, we intentionally switch from our preferred terminology of referring to “texts” (see Section 2.9)\nback to “documents”, as “document expansion” is well known and the alternative “text expansion” is not a\ncommonly used term.\n112\nrelevance matching. A potentially “smarter” approach is to, of course, use transformers to learn dense\nrepresentations that attempt to directly overcome these challenges. This, however, we’ll get to later.\n4.1 Query and Document Expansion: General Remarks\nQuery expansion and document expansion techniques provide two potential solutions to the vocab-\nulary mismatch problem. The basic idea behind document expansion is to augment (i.e., expand)\ntexts from the corpus with additional terms that are representative of their contents or with query\nterms for which those texts might be relevant. As an example, a text discussing automobile sales\nmight be expanded with the term “car” to better match the query “car sales per year in the US”. In\nthe simplest approach, these expansion terms can be appended to the end of the document, prior to\nindexing, and retrieval can proceed exactly as before, but on the augmented index. A similar effect\ncan be accomplished with query expansion, e.g., augmenting the query “car sales per year in the US”\nwith the term “automobile”. An augmented query increases the likelihood of matching a relevant text\nfrom the corpus that uses terms not present in the original query. Note that some of the techniques\nwe present in this section are, strictly speaking, reweighting techniques, in that they do not add new\nterms, but rather adjust the weights of existing terms to better reﬂect their importance. However, for\nexpository convenience we will use “expansion” to encompass reweighting as well.\nBoth query and document expansion ﬁt seamlessly into multi-stage ranking architectures. Query\nexpansion is quite straightforward—conceptually, various techniques can be organized as modules\nthat take an input query and output a (richer) expanded query. These are also known as “query\nrewriters”; see, for example, public discussions in the context of the Bing search engine.113 Strictly\nspeaking, query rewriting is more general than query expansion, since, for example, a rewriter might\nremove terms deemed extraneous in the user’s query.114 As another example, a query rewriter might\nannotate named entities in a query and link them to entities in a knowledge graph for special handling\nby downstream modules. Nevertheless, both “expansion” and “rewriting” techniques share the aim of\nbetter aligning query and document representations, and in an operational context, this distinction\nisn’t particularly important. Query expansion modules can be placed at any stage in a multi-stage\nranking architecture: one obvious place is to provide a richer query for ﬁrst-stage retrieval, but in\nprinciple, query expansion (or rewriting) can be applied to any reranking stage.\nSimilarly, document expansion ﬁts neatly into multi-stage ranking. An index built on the expanded\ncorpus can serve as a drop-in replacement for ﬁrst-stage retrieval to provide a richer set of candidate\ndocuments for downstream reranking. This might lead to an end-to-end system that achieves higher\neffectiveness, or alternatively, the same level of effectiveness might be achieved at lower latency costs\n(for example, using less computationally intensive rerankers). In other words, document expansion\npresents system developers with more options in the effectiveness/efﬁciency tradeoff space. Selecting\nthe desired operating point, of course, depends on many organization-, domain-, and task-speciﬁc\nfactors that are beyond the scope of this present discussion.\nIn some ways, query expansion and document expansion are like “yin” and “yang”. The advantages\nof document expansion precisely complement the shortcomings of query expansion, and vice versa.\nIn more detail, there are two main advantages to document expansion:\n• Documents are typically much longer than queries, and thus offer more context for a model to\nchoose appropriate expansion terms. As we have seen from the work of Dai and Callan [2019b]\n(see Section 3.3.2), BERT beneﬁts from richer contexts and, in general, transformers are able to\nbetter exploit semantic and other linguistic relations present in a ﬂuent piece of natural language\ntext (compared to a bag of keywords).\n• Most document expansion techniques are embarrassingly parallel, i.e., they are applied indepen-\ndently to each document. Thus, the associated computations can be distributed over arbitrarily\nlarge clusters to achieve a desired throughput for corpus processing.\nIn contrast, there are three main advantages of query expansion:\n113https://blogs.bing.com/search-quality-insights/May-2018/Towards-More-Intelligent-\nSearch-Deep-Learning-for-Query-Semantics\n114For example, given the query “I would like to ﬁnd information about black bear attacks”, removing the phrase\n“I would like to ﬁnd information about” would likely improve keyword-based retrieval.\n113\n• Query expansion techniques lend themselves to much shorter experimental cycles and provide\nmuch more rapid feedback, since trying out a new technique does not usually require any\nchanges to the underlying index. In contrast, exploring document expansion techniques takes\nmuch longer, since each new model (or even model variant) must be applied to the entire\ncollection, the results of which must be reindexed before evaluations can be conducted. This\nmeans that even simple investigations such as parameter tuning can take a long time.\n• Query expansion techniques are generally more ﬂexible. For example, it is easy to switch\non or off different features at query time (for example, selectively apply expansion only to\ncertain intents or certain query types). Similarly, it is quite easy to combine evidence from\nmultiple models without building and managing multiple (possibly large) indexes for document\nexpansion techniques.\n• Query expansion techniques can potentially examine multiple documents to aggregate evidence.\nAt a high level, they can be categorized into “pre-retrieval” and “post-retrieval” approaches.\nInstances of the latter class of techniques perform expansion based on the results of an initial\nretrieval, and thus they can aggregate evidence from multiple documents potentially relevant to\nthe query.115 Obviously, such techniques are more computationally expensive than pre-retrieval\napproaches that do not exploit potentially relevant documents from the corpus, but previous\nwork has shown the potential advantages of post-retrieval approaches [Xu and Croft, 2000].\nQuery expansion and document expansion have histories that date back many decades, arguably to\nthe 1960s. Neither is by any means new, but the use of neural networks, particularly transformers,\nhas been game-changing.\nOperational Considerations. For query expansion (or more generally, query rewriting), there’s not\nmuch to be said. If we view different techniques as “query-in, query-out” black boxes, operational\ndeployment is straightforward. Of course, one needs to consider the latency of the technique itself\n(e.g., computationally intensive neural models might not be practically deployable since inference\nneeds to be applied to every incoming query). Furthermore, expanded queries tend to be longer, and\nthus lead to higher latencies in ﬁrst-stage retrieval (even if the query expansion technique itself is\ncomputationally lightweight). Nevertheless, these effects are usually modest in comparison to the\ncomputational demands of neural inference (e.g., in a reranking pipeline). Of course, all these factors\nneed to be balanced, but such decisions are dependent on the organization, task, domain, and a host\nof other factors—and thus we are unable to offer more speciﬁc advice.\nHow might document expansion be implemented in practice? In the text ranking scenarios we\nconsider in this survey, the assumption is that the corpus is “mostly” static and provided to the\nsystem “ahead of time” (see Section 2.1). Thus, it is feasible to consider document expansion as\njust another step in a system’s document preprocessing pipeline, conceptually no different from\nstructure (e.g., HTML) parsing, boilerplate and junk removal, etc. As mentioned above, document\nexpansion in most cases is embarrassingly parallel—that is, model inference is applied to each\ndocument independently—which means that inference can be distributed over large clusters. This\nmeans that computationally expensive models with long inference latencies may still be practical\ngiven sufﬁcient resources. Resource allocation, of course, depends on a cost/beneﬁt analysis that is\norganization speciﬁc.\nFrom the technical perspective, a common design for production systems is nightly updates to the\ncorpus (e.g., addition, modiﬁcation, or removal of texts), where the system would process only the\nportion of the corpus that has changed, e.g., apply document expansion to only the new and modiﬁed\ncontent. The underlying indexes would then need to be updated and redeployed to production.\nSee Leibert et al. [2011] for an example of production infrastructure designed along these lines.\nAt search time, it is worth noting that ﬁrst-stage retrieval latencies might increase with document\nexpansion due to the expanded texts being longer, but usually the differences are modest, especially\ncompared to the demands of neural inference for rerankers.\n115Note that while it is possible, for example, to perform cluster analysis on the corpus in a preprocessing step\n(possibly even informed by a query log), it is much more difﬁcult to devise document expansion methods that\naggregate evidence from multiple documents in a query-speciﬁc manner.\n114\n4.2 Pseudo-Relevance Feedback with Contextualized Embeddings: CEQE\nPseudo-relevance feedback (sometimes called blind relevance feedback) is one of the oldest post-\nretrieval query expansion techniques in information retrieval, dating back to the 1970s [Croft and\nHarper, 1979]. This technique derives from the even older idea of relevance feedback, where the goal\nis to leverage user input to reﬁne queries so that they better capture the user’s idea of relevant content.\nIn a typical setup, the system performs an initial retrieval and presents the user with a (usually, short)\nlist of texts, which the user assesses for relevance. The system then uses these judgments to reﬁne the\nuser’s query. One of the earliest and simplest approaches, the Rocchio algorithm [Rocchio, 1971],\nperforms these manipulations in the vector space model: starting with the representation of the query,\nthe system adds the aggregate representation of relevant documents and subtracts the aggregate\nrepresentation of the non-relevant documents. Thus, the expanded query becomes “more like” the\nrelevant documents and “less like” the non-relevant documents.\nThe obvious downside of relevance feedback, of course, is the need for a user in the loop to make the\nrelevance judgments. For pseudo-relevance feedback, in contrast, the system takes the top documents\nfrom initial retrieval, simply assumes that they are relevant, and then proceeds to expand the query\naccordingly. Empirically, pseudo-relevance feedback has been shown to be a robust method for\nincreasing retrieval effectiveness on average.116 The intuition is that if the initial retrieved results\nare “reasonable” in terms of quality, an analysis of their contents will allow a system to reﬁne its\nquery representation, for example, by identifying terms not present in the original query that are\ndiscriminative of relevant texts. In other words, the expanded query more accurately captures the\nuser’s information need based on a “peek” at the corpus.\nThus, “traditional” (pre-BERT, even pre-neural) query expansion with pseudo-relevance feedback\nis performed by issuing an initial query to gather potentially relevant documents, identifying new\nkeywords (or phrases) from these documents, adding them to form an expanded query (typically, with\nassociated weights), and then reissuing this expanded query to obtain a new ranked list. Example\nof popular methods include RM3 (Relevance Model 3) [Lavrenko and Croft, 2001, Abdul-Jaleel\net al., 2004, Yang et al., 2019b], axiomatic semantic matching [Fang and Zhai, 2006, Yang and Lin,\n2019], and Bo1 [Amati and van Rijsbergen, 2002, Amati, 2003, Plachouras et al., 2004]. As this is\nnot intended to be a general tutorial on pseudo-relevance feedback, we recommend that interested\nreaders use the above cited references as entry points into this vast literature.\nNevertheless, there are two signiﬁcant issues when trying to implement the standard pseudo-relevance\nfeedback “recipe” (presented above) using transformers:\n• One obvious approach would be to apply BERT (and in general, transformers) to produce a\nbetter representation of the information need for downstream rerankers—that is, to feed into the\ninput template of a cross-encoder. As we’ve seen in Section 3, BERT often beneﬁts from using\nwell-formed natural language queries rather than bags of words or short phrases. This makes\ntraditional query expansion methods like RM3 a poor ﬁt, because they add new terms to a query\nwithout reformulating the output into natural language. The empirical impact is demonstrated\nby the experiments of Padaki et al. [2020] (already mentioned in Section 3.3.2), who found that\nexpansion with RM3 actually reduced the effectiveness of BERT–MaxP. In contrast, replacing\nthe original keyword query with a natural language query reformulation improved effectiveness.\n• Another obvious approach would be to apply transformers to produce a better query for the\n“second round” of keyword-based retrieval. Traditional query expansion methods such as RM3\nproduce weights to be used with the new expansion terms, and due to these weights, expansion\nterms tend to have less inﬂuence on the ranking than the original query terms. This reduces the\nimpact of bad expansion terms from imperfect algorithms. With existing BERT-based methods,\nquery terms are not associated with explicit weights, so it is not possible to “hedge our bets”.\nTo the ﬁrst point, Padaki et al. [2020] and Wang et al. [2020b] devised query reformulation (as\nopposed to query expansion) methods, which ensure that queries are always in natural language.\nThe gains from both approaches are small, however, and neither modiﬁes the keyword query for the\n“second round” retrieval, so they are not the focus of this section.\n116The on average qualiﬁcation here is important, as pseudo-relevance feedback can be highly effective for some\nqueries, yet spectacularly fail on other queries.\n115\nGiven the difﬁculty of providing a BERT-based reranker (i.e., cross-encoder) with an expanded\nquery, Naseri et al. [2021] instead explored how BERT could be used to improve the selection of\nexpansion terms for the “second round” keyword retrieval. That is, rather than improving downstream\ncross-encoder input for reranking, the authors used BERT’s contextual embeddings to improve the\nquery expansion terms selected by a ﬁrst-stage retriever with pseudo-relevance feedback. Their CEQE\nmodel (“Contextualized Embeddings for Query Expansion”) is intended to be a replacement for a\npurely keyword-based ﬁrst-stage retriever, to generate better candidate texts to feed a downstream\ntransformer-based reranker. This approach avoids the above issues with term expansion because the\ndownstream reranker continues to use the original query in its input template.\nCEQE can be viewed as an extension of the RM3 pseudo-relevance feedback technique that uses\ncontextual embeddings to compute the probability of a candidate expansion term wgiven both a\nquery Qand a document D, drawn from initial retrieval:\n∑\nD\np(w,Q,D ) =\n∑\nD\np(w|Q,D)p(Q|D)p(D) (48)\nAs with RM3, p(Q|D) is calculated using a query likelihood model and p(D) is assumed to be a\nuniform distribution. The remaining quantity, p(w|Q,D), is calculated using contextual embeddings\nproduced by monoBERT. To produce these contextual embeddings, documents are ﬁrst split into\npassages of 128 tokens: the query and each passage are then fed to monoBERT, and the contextual\nembeddings produced by the eleventh transformer layer in BERTBase117 are retained. From these\nembeddings, p(w|Q,D) is calculated using either a centroid representation of the query or a term-\nbased representation with pooling. Let MD\nw be the set of all mentions (occurrences) of term win\ndocument D and MD\n∗ be the set of all mentions of any term in the document. Both approaches\ncalculate a score for each unique term wby taking the cosine similarity between each mention of the\nterm in the document mD\nw and normalizing by the sum of all term mentions in the document:\np(w|q,D) =\n∑\nmDw ∈MDw\ncos(q,mD\nw)\n∑\nmD\nt ∈MD∗\ncos(q,mD\nt ) (49)\nUsing the centroid approach, the contextual embeddings for each query term are averaged to form a\nquery centroid that serves as the qin the above equation. With the term-based approach, p(w|q,D) is\ncalculated for each query term separately, and max pooling or multiplicative pooling is applied to\naggregate the per-term scores.\nNaseri et al. [2021] evaluated the three variants of their approach (referred to as CEQE-Centroid,\nCEQE-MulPool, and CEQE-MaxPool) using BERTBase on the Robust04 collection and the TREC\n2019 Deep Learning Track document ranking task. They performed retrieval using the Galago 118\nquery likelihood implementation with stopword removal and Krovetz stemming, which leads to\nﬁrst-stage results that differ slightly from those obtained with Anserini (for example, in Section 3.2.2).\nThe authors considered up to the top-ranked 100 documents when calculating p(w|Q,D); this has\nsimilar computational costs as reranking the top 100 documents using monoBERT. Thus, CEQE can\nbe characterized as a computationally expensive extension of RM3 that trades off efﬁciency for an\nimproved estimate of p(w,Q,D ).\nResults from the authors’ original paper are shown in Table 30. In addition to BM25 with RM3\nexpansion, CEQE was compared against Static-Embed [Kuzi et al., 2016], which is an RM3 extension\nthat uses GloVe embeddings [Pennington et al., 2014] rather than contextual embeddings. Naseri\net al. [2021] also considered a “Static-Embed-PRF” variant of Static-Embed that is restricted to\nexpansion terms found in the feedback documents; here, we report the better variant on each dataset,\nwhich is Static-Embed-PRF on Robust04 and Static-Embed on the TREC 2019 Deep Learning Track\ndocument ranking task. The CEQE variants, row group (3), signiﬁcantly outperform Static-Embed,\nrow (2), across datasets and metrics, suggesting that the advantages of using contextual embeddings\nwhen reranking also improve effectiveness when choosing expansion terms.\nHowever, results appear to be more mixed when compared against BM25 + RM3, row (1), with\nCEQE showing small but consistent improvements across datasets and metrics. These gains are only\n117In the original BERT paper [Devlin et al., 2019], embeddings from this layer were more effective for named\nentity recognition than embeddings from the twelfth (last) layer (see Table 7).\n118http://www.lemurproject.org/galago.php\n116\nRobust04 TREC 2019 DL Doc\nMethod MAP Recall@100 Recall@1k MAP Recall@100 Recall@1k\n(1) BM25 + RM3 0.3069 0.4610 ‡ 0.7588‡ 0.3975‡ 0.4434‡ 0.7750‡\n(2) Static-Embed 0.2703 0.4324 0.7231 0.3373 0.3973 0.7179\n(3a) CEQE-Centroid 0.3019 ‡ 0.4593‡ 0.7653†‡ 0.4144‡ 0.4464‡ 0.7804‡\n(3b) CEQE-MulPool 0.2845 ‡ 0.4517‡ 0.7435‡ 0.3724‡ 0.4295‡ 0.7560‡\n(3c) CEQE-MaxPool 0.3086 ‡ 0.4651‡ 0.7689†‡ 0.4161†‡ 0.4506‡ 0.7832‡\n(4) CEQE-MaxPool (ﬁne-tuned) 0.3071‡ 0.4647‡ 0.7626‡ - - -\nTable 30: The effectiveness of CEQE on the Robust04 test collection (using title queries) and the\nTREC 2019 Deep Learning Track document ranking test collection. Statistically signiﬁcant increases\nin effectiveness over BM25 + RM3 and Static-Embed are indicated with the symbol †and the\nsymbol ‡, respectively (p< 0.05, two-tailed paired t-test).\nstatistically signiﬁcant for Recall@1k and MAP on Robust04 and TREC 2019 DL Doc, respectively.\nAmong the CEQE variants, max pooling is consistently the most effective. In row group (3), the\nBERTBase model was not ﬁne-tuned; row (4) shows the effectiveness of CEQE-MaxPool when using\na monoBERT ranking model that was ﬁrst ﬁne-tuned on Robust04. Somewhat surprisingly, this\napproach performs slightly worse than CEQE-MaxPool without ﬁne-tuning, row (3c), suggesting that\nimprovements in reranking do not necessarily translate to improvements in selecting expansion terms.\nIn addition to considering the question of whether contextual embeddings can be used to improve\nRM3, Naseri et al. [2021] performed experiments measuring the impact of combining CEQE’s\nﬁrst-stage retrieval with CEDR (see Section 3.3.3). Here, CEDR can be applied in two ways: ﬁrst,\nintegrated into query expansion, and second, as a downstream reranker. In the ﬁrst approach, BM25\nresults are ﬁrst reranked with CEDR before either CEQE or RM3 is applied to extract the query\nexpansion terms; the new expanded query is then used for the “second round” keyword retrieval.\nExperiments conﬁrm that CEDR improves both CEQE and RM3 when used in this manner. In the\nsecond approach, CEDR-augmented query expansion (CEQE or RM3) results can then be reranked by\nCEDR again. That is, when reranking BM25 with CEDR, performing query expansion based on these\nresults to obtain a new keyword-based ranking, and then reranking the top 1000 documents again\nwith CEDR, CEQE-MaxPool reaches 0.5621 nDCG@20 on Robust04 whereas RM3 reaches only\n0.5565 nDCG@20. In both approaches (i.e., with or without a second round of CEDR reranking),\nCEQE consistently outperforms RM3, but the improvements are small and signiﬁcant only for recall.\nHowever, these increases in effectiveness come at the cost of requiring multiple rounds of reranking\nwith a computationally-expensive model, and thus it is unclear if such a tradeoff is worthwhile in a\nreal-world setting. We refer the reader to the original work for additional details on these experiments.\nTakeaway Lessons. At a high level, there are two ways to integrate transformer-based models to\npseudo-relevance feedback techniques:\n• We can use existing query expansion methods to produce an augmented query that is fed to\na transformer-based reranker. As demonstrated by Padaki et al. [2020], this approach is not\neffective since models like monoBERT work better when given natural language input, and\nmost existing query expansion methods do not produce ﬂuent queries.\n• Transformer-based models can aid in the selection of better query expansion terms, as demon-\nstrated by CEQE [Naseri et al., 2021]. While CEQE’s use of contextual embeddings substantially\nimproves over expansion with static embeddings, improvements over RM3 are smaller and\ncome at a high computational cost, since it requires BERT inference over top-kcandidates.\nWhile we believe it is clear that contextual embeddings are superior to static embeddings for pseudo-\nrelevance feedback, it remains unclear whether the straightforward application of transformers\ndiscussed in this section are compelling when considering effectiveness/efﬁciency tradeoffs. Since\npseudo-relevance feedback is a post-retrieval query expansion technique, it necessitates a round of\nretrieval and analyses of the retrieved texts. Thus, in order to be practical, these analyses need to be\nlightweight yet effective. However, it does not appear that researchers have devised a method that\nmeets these requirements yet.\n117\n4.3 Document Expansion via Query Prediction: doc2query\nSwitching gears, let’s discuss document expansion techniques that contrast with the query expansion\ntechniques presented in the previous section. While document expansion dates back many decades,\nthe ﬁrst successful application of neural networks to our knowledge was introduced by Nogueira et al.\n[2019b], who called their technique doc2query. The basic idea is to train a sequence-to-sequence\nmodel that, given a text from a corpus, produces queries for which that document might be relevant.\nThis can be thought of as “predictively” annotating a piece of text with relevant queries. Given a\ndataset of (query, relevant text) pairs, which are just standard relevance judgments, a sequence-to-\nsequence model can be trained to generate a query given a text from the corpus as input.\nWhile in principle one can use these predicted queries in a variety of ways, doc2query takes perhaps\nthe most straightforward approach: the predictions are appended to the original texts from the corpus\nwithout any special markup to distinguish the original text from the expanded text, forming the\n“expanded document”. This expansion procedure is performed on every text from the corpus, and the\nresults are indexed as usual. The resulting index can then provide a drop-in replacement for use in\nﬁrst-stage retrieval in a multi-stage ranking pipeline, compatible with any of the reranking models\ndescribed in this survey.\nIt should be no surprise that the MS MARCO passage ranking test collection can be used as training\ndata: thus, doc2query was designed to make query predictions on passage-length texts. In terms of\nmodeling choices, it should also be no surprise that Nogueira et al. [2019b] exploited transformers\nfor this task. Speciﬁcally, they examined two different models:\n• doc2query–base: the original proposal of Nogueira et al. [2019b] used a “vanilla” transformer\nmodel trained from scratch (i.e., not pretrained).\n• doc2query–T5: in a follow up, Nogueira and Lin [2019] replaced the “vanilla” non-pretrained\ntransformer with T5 [Raffel et al., 2020], a pretrained transformer model.\nTo train both models (more accurately, to ﬁne tune, in the case of T5), the following loss is used:\nL= −\nM∑\ni=1\nlog P(qi|q<i,d), (50)\nwhere a query qconsists of tokens q0,...,q M, and P(yi|x) is the probability assigned by the model\nat the i-th decoding step to token ygiven the input x. Note that at training time the correct tokens q<i\nare always provided as input in the i-th decoding step. That is, even though the model might have\npredicted another token at the (i−1)-th step, the correct token qi−1 will be fed as input to the current\ndecoding step. This training scheme is called teacher forcing or maximum likelihood learning and is\ncommonly used in text generation tasks such as machine translation and summarization.\nAt inference time, given a piece of text as input, multiple queries can be sampled from the model\nusing top-krandom sampling [Fan et al., 2018a]. In this sampling-based decoding method, at each\ndecoding step a token is sampled from the top-ktokens with the highest probability from the model.\nThe decoding stops when a special “end-of-sequence” token is sampled. In contrast to other decoding\nmethods such as greedy or beam search, top-ksampling tends to generate more diverse texts, with\ndiversity increasing with greater values of k[Holtzman et al., 2019]. Note that the kparameter is\nindependent of the number of sampled queries; for example, we can setk= 10 and sample 40 queries\nfrom the model. In other words, each inference pass with the model generates one predicted query,\nand typically, each text from the corpus is expanded with many predicted queries.\nResults on the MS MARCO passage ranking test collection are shown in Table 31, with ﬁgures\ncopied from Nogueira et al. [2019b] for doc2query–base and from Nogueira and Lin [2019] for\ndoc2query–T5 (which used the T5-base model). In the case of doc2query–base, each text was\nexpanded with 10 queries, and in the case of doc2query–T5, each text was expanded with 40 queries.\nThe expanded texts were then indexed with Anserini, and retrieval was performed either with BM25,\nin row group (1), or BM25 + RM3, in row group (2). For additional details such as hyperparameter\nsettings and the effects of expanding the texts with different numbers of predicted queries, we refer\nthe reader to the original papers. In addition to the usual metrics for the MS MARCO passage\nranking test collection, the results table also presents query latencies for some of the conditions where\ncomparable ﬁgures are available.\n118\nMS MARCO Passage\nDevelopment Test Latency\nMethod MRR@10 Recall@1k MRR@10 (ms/query)\n(1a) BM25 0.184 0.853 0.186 55\n(1b) w/ doc2query–base [Nogueira et al., 2019b] 0.218 0.891 0.215 61\n(1c) w/ doc2query–T5 [Nogueira and Lin, 2019] 0.277 0.947 0.272 64\n(2a) BM25 + RM3 0.156 0.861 - -\n(2b) w/ doc2query–base 0.194 0.892 - -\n(2c) w/ doc2query–T5 0.214 0.946 - -\n(3) Best non-BERT [Hofstätter et al., 2019] 0.290 - 0.277 -\n(4) BM25 + monoBERTLarge[Nogueira et al., 2019a] 0.372 0.853 0.365 3,500\nTable 31: The effectiveness of doc2query on the MS MARCO passage ranking test collection.\nTREC 2019 DL Passage\nMethod nDCG@10 MAP Recall@1k\n(1a) BM25 0.506 0.301 0.750\n(1b) w/ doc2query–base 0.514 0.324 0.749\n(1c) w/ doc2query–T5 0.642 0.403 0.831\n(2a) BM25 + RM3 0.518 0.339 0.800\n(2b) w/ doc2query–base 0.564 0.368 0.801\n(2c) w/ doc2query–T5 0.655 0.449 0.886\n(3) BM25 + RM3 + monoBERTLarge 0.742 0.505 -\n(4) TREC Best [Yan et al., 2019] 0.765 0.503 -\nTable 32: The effectiveness of doc2query on the TREC 2019 Deep Learning Track passage ranking\ntest collection.\nThe effectiveness differences between doc2query with the “vanilla” (non-pretrained) transformer and\nthe (pretrained) T5 model with BM25 retrieval are clearly seen in row (1c) vs. row (1b). Note that\nboth models are trained using the same dataset. It should come as no surprise that T5 is able to make\nbetter query predictions. While the T5 condition used a larger model that has more parameters than\nthe base transformer, over-parameterization of the base transformer can lead to poor predictions, and\nit appears clear that pretraining makes the crucial difference, not model size per se. With BM25 +\nRM3, row (2c) vs. row (2b), the gap between doc2query–T5 and doc2query–base is reduced, but these\nexperiments exhibit the same issues as with the monoBERT experiments (see Section 3.2) where\nsparse judgments are not able to properly evaluate the beneﬁts of query expansion (more below).\nTable 31 shows two additional points of reference: monoBERT, shown in row (4) as well as the best\ncontemporaneous non-BERT model, shown in row (3). The effectiveness of doc2query is substantially\nbelow monoBERT reranking, but it is about 50×faster, since the technique is still based on keyword\nsearch with inverted indexes and does not require inference with neural networks at query time. The\nmodest increase in query latency is due to the fact that the expanded texts are longer. The comparison\nto row (3) shows that doc2query is able to approach the effectiveness of non-BERT neural models (at\nthe time the work was published) solely with document expansion. Results also show that doc2query\nimproves Recall@1k, which means that more relevant texts are available to downstream rerankers\nwhen used in a multi-stage ranking architecture, thus potentially improving end-to-end effectiveness.\nEvaluation results of doc2query on the TREC 2019 Deep Learning Track passage ranking test\ncollection are shown in Table 32; these results have not been reported elsewhere. The primary goal of\nthis experiment is to quantify the effectiveness of doc2query using non-sparse judgments, similar to\nthe experiments reported in Section 3.2. As we discussed previously, sparse judgments from the MS\nMARCO passage ranking test collection are not sufﬁcient to capture improvements attributable to\nRM3, whereas with the TREC 2019 Deep Learning Track passage ranking test collection, it becomes\nevident that pseudo-relevance feedback with RM3 is more effective than simple bag-of-words queries\nwith BM25, row (2a) vs. (1a); this is repeated from Table 6 in Section 3.2.\nSimilarly, results show that on an index that has been augmented with doc2query predictions (based\non either the “vanilla” transformer or T5), BM25 + RM3 is more effective than just BM25 alone;\n119\nInput:July is the hottest month in Washington DC with an average temperature of 27◦C\n(80◦F) and the coldest is January at 4◦C (38◦F) with the most daily sunshine hours at 9 in\nJuly. The wettest month is May with an average of 100mm of rain.\nTarget query:what is the temperature in washington\ndoc2query–base:weather in washington dc\ndoc2query–T5:what is the weather in washington dc\nInput:The Delaware River ﬂows through Philadelphia into the Delaware Bay. It ﬂows\nthrough and(sic)aqueduct in the Roundout Reservoir and then ﬂows through Philadelphia\nand New Jersey before emptying into the Delaware Bay.\nTarget query:where does the delaware river start and end\ndoc2query–base:what river ﬂows through delaware\ndoc2query–T5:where does the delaware river go\nInput:sex chromosome - (genetics) a chromosome that determines the sex of an individual;\nmammals normally have two sex chromosomes chromosome - a threadlike strand of DNA\nin the cell nucleus that carries the genes in a linear order; humans have 22 chromosome\npairs plus two sex chromosomes.\nTarget Query:which chromosome controls sex characteristics\ndoc2query–base: deﬁnition sex chromosomes\ndoc2query–T5: what determines sex of someone\nFigure 20: Examples of predicted queries on passages from the MS MARCO passage corpus\ncompared to user queries from the relevance judgments.\ncompare row (2b) vs. row (1b) and row (2c) vs. row (1c). In other words, the improvements from\ndocument expansion and query expansion with pseudo-relevance feedback are additive. Overall,\ndoc2query–T5 with BM25 + RM3 achieves the highest effectiveness.\nTable 32 shows two additional comparison conditions: row (3), which applies monoBERT to rerank\nBM25 + RM3 results, and row (4), the top-scoring submission to TREC 2019 Deep Learning Track\npassage ranking task [Yan et al., 2019]. While the effectiveness of doc2query falls well short of\nmonoBERT reranking, row (2c) vs. row (3), this is entirely expected, and the much faster query\nlatency of doc2query has already been pointed out. The two techniques target different parts of the\nmulti-stage pipeline, so we see them as complementary. We further note that the work of Yan et al.\n[2019] adopted a variant of doc2query (and further exploits ensembles), which provides independent\nevidence supporting the effectiveness of document expansion via query prediction.\nWhere exactly are the gains of doc2query coming from? Figure 20 presents three examples from\nthe MS MARCO passage corpus, showing query predictions by both the vanilla transformer as well\nas T5. The predicted queries seem quite reasonable based on manual inspection. Interestingly, both\nmodels tend to copy some words from the input text (e.g., “washington dc” and “river”), meaning\nthat the models are effectively performing term reweighting (i.e., increasing the importance of key\nterms). Nevertheless, the models also produce words not present in the input text (e.g., weather),\nwhich can be characterized as expansion by adding synonyms and semantically related terms.\nTo quantify these effects more accurately, it is possible to measure the proportion of terms predicted\nby doc2query–T5 that already exist in the original text (i.e., are copied) vs. terms that do not exist\nin the original text (i.e., are new terms). Here, we describe such an analysis, which has not been\npreviously published. Excluding stopwords, which corresponds to 51% of the predicted query terms,\nwe ﬁnd that 31% are new while the rest (69%) are copied. The sequence-to-sequence model learned\nto generate these new terms based on the training data, to “connect the dots” between queries and\nrelevant passages that might not contain query terms. In other words, doc2query is learning exactly\nhow to bridge the vocabulary mismatch.\nTable 33 presents the results of an ablation analysis: starting with the original text, we add only\nthe new terms, row (2a); only the copied terms, row (2b); and both, row (2c). Each variant of the\nexpanded corpus was then indexed as before, and results of bag-of-words keyword search with BM25\nare reported. The ﬁnal condition is the same as row (1c) in Table 31, repeated for convenience.\n120\nMS MARCO Passage(Dev)\nMethod MRR@10 Recall@1k\n(1) Original text 0.184 0.853\n(2a) + Expansion w/ new terms 0.195 0.907\n(2b) + Expansion w/ copied terms 0.221 0.893\n(2c) + Expansion w/ copied terms + new terms 0.277 0.944\n(3) Expansion terms only (without original text) 0.263 0.927\nTable 33: The effectiveness of ablated variants of doc2query–T5 on the development set of the MS\nMARCO passage ranking test collection.\nWe see that expansion with only new terms yields a small improvement over just the original\ntexts. Expanding with copied terms alone provides a bigger gain, indicating that the effects of\nterm reweighting appear to be more impactful than attempts to enrich the vocabulary. However,\ncombining both types of terms yields a big jump in effectiveness, showing that both sources of signal\nare complementary. Interestingly, the gain from both types of terms together is greater than the sum\nof the gains from each individual contribution in isolation. This can be characterized with the popular\nadage, “the whole is greater than the sum of its parts”, and suggests complex interactions between\nthe two types of terms that we do not fully understand yet. In most IR experiments, gains from\nthe combination of two innovations are usually smaller than the sum of the gain from each applied\nindependently; see Armstrong et al. [2009] for discussion of this observation. Finally, row (3) in\nTable 33 answers this interesting question: What if we discarded the original texts and indexed only\nthe expansion terms (i.e., the predicted queries)? We see that effectiveness is surprisingly high, only\nslightly worse than the full expansion condition. In other words, it seems like the original texts can,\nto a large extent, be replaced by the predicted queries from the perspective of bag-of-words search.\nTakeaway Lessons. To sum up, document expansion with doc2query augments texts with potential\nqueries, thereby mitigating vocabulary mismatch and reweighting existing terms based on predicted\nimportance. The expanded collection can be indexed and used exactly as before—either by itself\nor as part of a multi-stage ranking architecture. Perhaps due to its simplicity and effectiveness,\ndoc2query has been successfully replicated for text ranking independently on the MS MARCO test\ncollections [Yan et al., 2019], and according to Yan et al. [2021], has been deployed in production at\nAlibaba. Furthermore, the technique has been adapted and also successfully applied to other tasks,\nincluding scientiﬁc document retrieval [Boudin et al., 2020], creating artiﬁcial in-domain retrieval\ndata [Ma et al., 2021a], and helping users in ﬁnding answers in product reviews [Yu et al., 2020b].\nDocument expansion with doc2query shifts computationally expensive inference with neural networks\nfrom query time to indexing time. As a drop-in replacement for the original corpus, keyword search\nlatency increases only modestly due to the increased length of the texts. The tradeoff is much more\ncomputationally intensive data preparation prior to indexing: for each text in a corpus, multiple\ninference passes are needed to generate the expanded queries. If the corpus is large (e.g., billions\nof documents), this method can be prohibitively expensive.119 For researchers working on the MS\nMARCO corpora, however, this is usually not an issue because Nogueira and Lin [2019] have made\ntheir query predictions on standard corpora publicly available for download, making doc2query\npretty close to a “free boost” that can be integrated with other techniques (for example, DeepImpact,\ndiscussed in Section 4.6). However, the MS MARCO corpora are relatively small compared to other\ncommonly used academic test collections such as the ClueWeb web crawls. Applying doc2query\non these larger collections would require signiﬁcantly more compute resources, and thus presents\nbarriers to academic research.\nFinally, the astute reader might have noticed that this section only presents doc2query results on\npassages and not longer spans of text. This leads to the obvious question: How do we apply doc2query\nto longer texts? We defer this discussion to Section 4.5 in the context of HDCT.\n119Unless you’re Google. Or even if you’re Google?\n121\nTerm weight:0.0 0.1 0.2 0.3 0.4 >0.5\nQuery who is susan boyle\nRelevant AmateurvocalistSusanBoylebecameanovernightsensationafterappearing\nontheﬁrstroundof2009’spopularU.K.realityshowBritain’sGotTalent.\nNon-Relevant\nBestAnswer: a trollisgenerallysomeonewhotriestogetattentionbyposting\nthingseveryonewilldisagree, likegoingtoasusanboylefanpageandwriting\nsusanboyleis uglyonthewall. theyareusually14-16yearoldswhocrave\nattention.\nQuery what values do zoos serve\nRelevant\nZoosserveseveralpurposesdependingonwhoyouask. 1) Park/Garden: Some\nzoosaresimilarto a botanicalgardenor citypark. Theygivepeopleliving\nin crowded, noisycitiesa placeto walkthrougha beautiful, wellmaintained\noutdoorarea. Theanimalexhibitscreateinterestingsceneryandmakeforafun\nexcursion.\nNon-Relevant\nThereareNOpurebredBengaltigersintheU.S. Theonlypurebredtigersinthe\nU.S.areinAZAzoosandinclude133Amur(AKASiberian), 73Sumatranand\n50MalayantigersintheSpeciesSurvivalPlan. AllotherU.S.captivetigersare\ninbredandcrossbredanddonotserveanyconservationvalue.\nQuery do atoms make up dna\nRelevant\nDNAonlyhas5 differentatoms- carbon, hydrogen, oxygen, nitrogenand\nphosphorous. Accordingtooneestimation, thereareabout204billionatoms\nineachDNA.\nNon-Relevant\nGenomicsinTheoryandPractice. WhatisGenomics. Genomicsisa studyof\nthegenomesoforganisms. It maintaskistodeterminetheentiresequenceof\nDNAorthecompositionoftheatomsthatmakeuptheDNAandthechemical\nbondsbetweentheDNAatoms.\nFigure 21: Motivating examples for DeepCT, which show passages containing query terms that\nappear in both relevant and non-relevant contexts, taken from Dai and Callan [2019a].\n4.4 Term Reweighting as Regression: DeepCT\nResults from doc2query show that document expansion has two distinct but complementary effects:\nthe addition of novel expansion terms that are not present in the original text and copies of terms that\nare already present in the text. The duplicates have the effect of reweighting terms in the original text,\nbut using a sequence-to-sequence model to generate terms seems like an inefﬁcient and roundabout\nway of achieving this effect.\nWhat if we were able to directly estimate the importance of a termin the context that the term appears\nin? This is the premise of the Deep Contextualized Term Weighting (DeepCT) framework [Dai\nand Callan, 2019a]. Consider a BM25 score (see Section 1.2.2), which at a high level comprises a\nterm frequency and a document frequency component. Setting aside length normalization, the term\nfrequency (i.e., the number of times the term appears in a particular text) is the primary feature that\nattempts to capture the term’s importance in the text, since the document frequency component of\nBM25 is the same for that term across different texts (with the same length). Quite obviously, terms\ncan have the same term frequency but differ in the “importance” they play.\nA few motivating examples taken from Dai and Callan [2019a] are presented in Figure 21. In the\nﬁrst example, the non-relevant passage actually has more occurrences of the query terms “susan”\nand “boyle”, yet it is clear that the ﬁrst passage provides a better answer. The second and third\nexamples similarly reinforce the observation that term frequencies alone are often insufﬁcient to\nseparate relevant from non-relevant passages. Speciﬁcally, in the third example, “atoms” appear twice\nin both passages, but it seems clear that the ﬁrst passage is relevant while the second is not.\nTo operationalize these intuitions, the ﬁrst and most obvious question that must be addressed is: How\nshould term importance weights or scores (we use these two terms interchangeably) be deﬁned? Dai\nand Callan [2019a] proposed a simple measured called query term recall, or QTR:\nQTR(t,d) = |Qd,t|\n|Qd|, (51)\n122\nwhere |Qd|is the set of queries that are relevant to document d, and |Qd,t|is the subset of |Qd|that\ncontain term t. The importance score yt,d for each term tin dcan then be deﬁned as follows:\nyt,d\n∆\n= QTR(t,d). (52)\nThe score yt,d is in the range [0 ... 1]. At the extremes, yt,d = 1 if t occurs in all queries for\nwhich dis relevant, and yt,d = 0 if tdoes not occur in any query relevant to d. Going back to the\nexamples in Figure 21, “susan” and “boyle” would receive lower importance weights in the second\npassage because it doesn’t come up in queries about “susan boyle” as much as the ﬁrst passage. With\nappropriate scaling, these weights can be converted into drop-in replacements of term frequencies,\nreplacing the term frequency values that are stored in a standard inverted index. In turn, a DeepCT\nindex can be used in the same way as any other standard bag-of-words inverted index, for example,\nto generate candidate texts in a multi-stage ranking architecture.\nHaving thus deﬁned term importance weights using query term recall, it then becomes relatively\nstraightforward to formulate the prediction of these weights as a regression problem. Not surprisingly,\nBERT can be exploited for this task. More formally, DeepCT uses a BERT-based model that receives\nas input a text dand outputs an importance score yt,d for each term tin d. The goal is to assign high\nscores to terms that are central to the text, and low scores to less important terms. These scores are\ncomputed by a regression layer as:\nˆyt,d = w·Tt,d + b, (53)\nwhere wis a weight vector, bis a bias term, and Tt,d is the contextual embedding of term tin the text.\nLike doc2query, DeepCT is trained using (query, relevant text) pairs from the MS MARCO passage\nranking test collection. The BERT model and the regression layer are trained end-to-end to minimize\nthe following mean squared error (MSE) loss:\nL=\n∑\nt\n(ˆyt,d −yt,d)2 (54)\nwhere ˆyt,d and yt,d have already been deﬁned. Note that the BERT tokenizer often splits terms from\nthe text into subwords (e.g., “adversarial” is tokenized into “ad”, “##vers”, “##aria”, “##l”). DeepCT\nuses the weight for the ﬁrst subword as the weight of the entire term; other subwords are ignored\nwhen computing the MSE loss.\nOnce the regression model has been trained, inference is applied to compute ˆyt,d for each text d\nfrom the corpus. These weights are then rescaled from [0..1] to integers between 0 and 100 so they\nresemble term frequencies in standard bag-of-words retrieval methods. Finally, the texts are indexed\nusing these rescaled term weights using a simple trick that does not require changing the underlying\nindexing algorithm to support custom term weights. New “pseudo-documents” are created in which\nterms are repeated the same number of times as their importance weights. For example, if the term\n“boyle” is assigned a weight of four, it is repeated four times, becoming “boyle boyle boyle boyle” in\nthis new pseudo-document. A new corpus comprising these pseudo-documents, in which the repeated\nterms are concatenated together, is then indexed like any other corpus. Retrieval is performed on this\nindex as with any other bag-of-words query,120 although it is important to retune parameters in the\nscoring function.\nExperiment results for DeepCT using BERTBase for regression on the MS MARCO passage ranking\ntest collection are presented in Table 34, copied from Dai and Callan [2019a]. The obvious point\nof comparison is doc2query, and thus we have copied appropriate comparisons from Table 31 and\nTable 33. Note that doc2query–base, row (1b), predated DeepCT, and is included in the authors’\ncomparison, but doc2query–T5 was developed after DeepCT.\nHow do the two approaches compare? It appears that DeepCT is more effective than the “vanilla” (i.e.,\nnon-pretrained) version of doc2query but is not as effective as doc2query based on T5, which beneﬁts\nfrom pretraining. Evaluation in terms of Recall@1k tells a consistent story: all three techniques\nincrease the number of relevant documents that are available to downstream rerankers, and the\neffectiveness of DeepCT lies between doc2query–base and doc2query–T5. In row (1d), we repeat the\nresults of the doc2query–T5 ablation analysis in Table 33, where only repeated expansion terms are\n120Note that phrase queries are no longer meaningful since the pseudo-documents corrupt any positional\nrelationship between the original terms.\n123\nMS MARCO Passage\nDevelopment Test\nMethod MRR@10 Recall@1k MRR@10\n(1a) BM25 0.184 0.853 0.186\n(1b) w/ doc2query–base 0.218 0.891 0.215\n(1c) w/ doc2query–T5 0.277 0.947 0.272\n(1d) w/ doc2query–T5 (copied terms only) 0.221 0.893 -\n(2) DeepCT 0.243 0.913 0.239\nTable 34: The effectiveness of DeepCT on the MS MARCO passage ranking test collection.\nincluded. This discards the effects of new terms, bringing the comparison into closer alignment with\nDeepCT. Comparing row (2) with row (1d), we see that DeepCT’s principled approach to reweighting\nterms is more effective than relying on a sequence-to-sequence model to reweight terms indirectly by\ngenerating multiple copies of the terms in independent query predictions.\nIt is worth noting that a comparison between the two methods is not entirely fair since doc2query’s\nT5-base model is twice the size of DeepCT’s BERTBase model, and it was pretrained on a larger\ncorpus. Thus, we cannot easily separate the impact on effectiveness of simply having a bigger model,\nas opposed to fundamental characteristics of the underlying techniques.\nWhile not as effective as the best variant of doc2query, DeepCT does have a number of advantages:\nits model is more lightweight in terms of neural network inference and thus preprocessing an entire\ncorpus with DeepCT (which is necessary prior to indexing) is much faster. DeepCT uses an encoder–\nonly model (e.g., BERT), which tends to be faster than encoder–decoder (i.e., sequence-to-sequence)\nmodels used by doc2query since there is an additional output sequence generation phase. Furthermore,\nDeepCT requires only one inference pass per text to compute term importance weights for all terms\nin the text, whereas doc2query requires an inference pass to generate each query prediction, which\nmust be repeated multiple times (typically tens of times).121\nThe other major difference between DeepCT and doc2query is that DeepCT is restricted to reweighting\nterms already present in a text, whereas doc2query can augment the existing text with new terms,\nthus potentially helping to bridge the vocabulary mismatch gap. The higher recall observed with\ndoc2query–T5 in Table 34 is perhaps attributable to these expansion terms. The addition of new terms\nnot present in the original texts, however, increases keyword search latency by a modest amount due\nto the increased length of the texts. In contrast, the performance impact of DeepCT is negligible, as\nexperimentally validated by Mackenzie et al. [2020].122\nTakeaway Lessons. At a high level, doc2query and DeepCT represent two different realizations\nof the insight that transformers can be applied to preprocess a corpus in a manner that improves\nretrieval effectiveness. Both techniques share two key features: they eliminate the need for expensive\nneural network inference at query time (as inference is pushed into the preprocessing stage), and\nthey provide drop-in replacements for keyword search. For certain applications, we might imagine\nthat bag-of-word keyword retrieval over doc2query or DeepCT indexes might be “good enough”,\nand results can be directly returned to users (without additional reranking). In this case, we have\ncompletely eliminated query-time dependencies on inference using neural networks (and their\nassociated hardware requirements). Alternatively, either doc2query or DeepCT can be used for\ncandidate generation in a multi-stage reranking pipeline to improve recall, thus providing downstream\nrankers with more relevant documents to process and potentially improving end-to-end effectiveness.\n121Although this is easily parallelizable on a cluster.\n122Note that it is not a forgone conclusion that term reweighting will retain the same performance proﬁle in\nbag-of-word querying (i.e., query latencies and their distributions) compared to “normal” term frequencies.\nWhile the terms have not changed, the term weights have, which could affect early-exit and other optimizations\nin modern query evaluation algorithms (which critically depend on the relative weights between terms in the\nsame text). Thus, the performance impact of term weighting requires empirical examination and cannot be\nderived from ﬁrst principles; see Mackenzie et al. [2020] for an in-depth and nuanced look at these effects.\nInterestingly, in the case of DeepImpact, a document expansion and reweighting technique we discuss in\nSection 4.6, the distribution of weights does substantially increase query latency.\n124\n4.5 Term Reweighting with Weak Supervison: HDCT\nIn follow-up work building on DeepCT, Dai and Callan [2020] proposed HDCT, a context-aware\nhierarchical document term weighting framework. Similar to DeepCT, the goal is to estimate a term’s\ncontext-speciﬁc term importance based on contextual embeddings from BERT, which is able to\ncapture complex syntactic and semantic relations within local contexts. Like DeepCT, these term\nimportance weights (or scores) are mapped into integers so that they can be directly interpreted as\nterm frequencies, replacing term frequencies in a standard bag-of-words inverted index.\nLike much of the discussion in Section 3.3, HDCT was designed to address the length limitations of\nBERT. DeepCT did not encounter this issue because it was only applied to paragraph-length texts\nsuch as those in the MS MARCO passage corpus. As we’ve already discussed extensively, BERT\nhas challenges with input sequences longer than 512 tokens for a number of reasons. The obvious\nsolution, of course, is to split texts into passages and process each passage individually. Later in this\nsection, we discuss similarly straightforward extensions of doc2query to longer texts as a point of\ncomparison.\nTo process long texts, HDCT splits them into passages comprising consecutive sentences that are up\nto about 300 words. After processing each passage with BERT, the contextual embedding of each\nterm is fed into a linear layer to map the vector representation into a scalar weight:\nˆyt,p = w·TBERT(t,p) + b, (55)\nwhere TBERT(t,p) is the contextual embedding produced by BERT for term tin passage p, wis the\nweight vector, and bis the bias. Like DeepCT, predicting the importance weight of term tin passage\np, denoted ˆyt,p, is formulated as a regression problem.123\nBy construction, ground truth labels are in the range [0,1] (see below), and thus so are the predictions,\nˆyt,p ∈[0,1]. They are then scaled into an integer as follows:\ntfBERT(t,p) = round\n(\nN ·\n√\nˆyt,p\n)\n, (56)\nwhere N = 100 retains two-digit precision and taking the square root has a smoothing effect.124 The\nweight tfBERT(t,p) captures the importance of term tin passage paccording to the BERT regression\nmodel, rescaled to a term frequency–like value.\nThere are still a few more steps before we arrive at document-level tfBERT weights. So far, we have a\nbag-of-words vector representation for each passage p:\nP-BoWHDCT(p) = [tfBERT(t1,p),tfBERT(t2,p),..., tfBERT(tm,p)]. (57)\nGathering the results from each passage yields a sequence of bag-of-words passage vectors:\n{P-BoWHDCT(p1),P-BoWHDCT(p2),..., P-BoWHDCT(pm)}. (58)\nFinally, the importance weight for each term tin document dis computed as:\nD-BoWHDCT(d) =\nn∑\ni=1\npwi ×P-BoWHDCT(pi), (59)\nwhere pwi is the weight for passage pi. Dai and Callan [2020] experimented with two ways\nfor computing the passage weights: in the “sum” approach,pwi = 1, and in the “decay” approach,\npwi = 1/i. The ﬁrst approach considers all passages equal, while the second discounts passages based\non their position, i.e., passages near the beginning of the text are assigned a higher weight. Although\n“decay” is slightly more effective on newswire documents than “sum”, the authors concluded that\n“sum” appears to be more robust, and also works well with web pages. At the end of these processing\nsteps, each (potentially long) text is converted into a bag of terms, where each term is associated with\nan integer importance weight.\n123Note that although DeepCT and HDCT are by the same authors, the two papers use slightly different notation,\nin some cases, for the same ideas; for example Eq. (55) and Eq. (53) both express term importance prediction\nas regression. Nevertheless, we preserve the notation used in each of the original papers for clarity.\n124Note that DeepCT is missing this square root.\n125\nMS MARCO Doc(Dev)\nMethod MRR@100\n(1) BM25FE 0.283\n(2a) w/ HDCT title 0.300 1,2b\n(2b) w/ HDCT PRF (AOL queries) 0.291 1\n(2c) w/ HDCT PRF (MS MARCO queries) 0.307 1,2ab\n(3) w/ HDCT supervision (MS MARCO doc) 0.320 1,2abc\n(4a) BM25 (tuned) [Lin et al., 2021a] 0.277\n(4b) BM25 + doc2query–T5 (tuned) [Lin et al., 2021a] 0.327\nTable 35: The effectiveness of HDCT on the development set of MS MARCO document ranking test\ncollection. Statistically signiﬁcant differences are denoted by the superscripts.\nGiven this setup, the only remaining issue is the “ground truth”yt,p labels for the term importance\nweights. Recall that in DeepCT, these scores are derived from query term recall based on (query,\nrelevant text) pairs from the MS MARCO passage ranking test collection. There are two issues for\nthis approach:\n1. Labeled datasets at this scale are costly to build.\n2. Relevance judgments are made at the document level, but the HDCT regression problem is\nformulated at the passage level; see Eq. (55).\nThus, Dai and Callan [2020] explored weak supervision techniques to automatically generate training\nlabels. Note that the second motivation is exactly the same issue Akkalyoncu Yilmaz et al. [2019b]\ndealt with in Birch, and the ﬁndings here are consistent (see Section 3.3.1). In the end, experiments\nwith HDCT found that automatically deriving global (document-level) labels appears to be sufﬁcient\nfor training local (passage-level) term importance predictors; BERT’s contextual embeddings appear\nto generate high-quality local weights at the passage level. This is similar to the “don’t worry about\nit” approach adopted by BERT–MaxP (see Section 3.3.2).\nDai and Callan [2020] proposed two techniques for generating term importance weights for training:\n• If (query, relevant text) pairs are not available, simply use an existing retrieval system (e.g.,\nBM25 ranking) to collect pseudo-relevant documents (by assuming that the top retrieved results\nare relevant). This, though, still requires access to a collection of queries. From this synthetic\ndataset, QTR in Eq. (51) can be computed and used as yt,p.\n• Analogously, document ﬁelds that are commonly used in search—for example, titles and anchor\ntexts—can provide an indication of what terms are important in the document’s text. This idea\nof using document metadata as distant supervision signals to create synthetic datasets dates\nbacks to the early 2000s [Jin et al., 2002].\nHaving deﬁned the target labels yt,p, the BERT regression model can be trained. As with DeepCT,\nHDCT is trained end-to-end to minimize mean squared error (MSE) loss.\nAn evaluation of HDCT using BERTBase on the development set of the MS MARCO document\nranking test collection is shown in Table 35, copied from Dai and Callan [2020]. Their paper\npresented evaluation on web collections as well as a number of detailed analyses and ablation studies,\nbut for brevity here we only convey the highlights. Statistically signiﬁcant differences are denoted by\nthe superscripts, e.g., row (2a) is signiﬁcantly better than row (1) and row (2b).\nAs the baseline, Dai and Callan [2020] built an ensemble of BM25 rankers on different document\nﬁelds: title, body, and URL in the case of MS MARCO documents. This is shown in row (1). The\neffectiveness of the HDCT passage regression model for predicting term importance, trained on\nthe MS MARCO document ranking test collection, which contains approximately 370K (query,\nrelevant document) pairs, is shown in row (3). This condition captures the upper bound of the weak\nsupervision techniques, since the labels are provided by humans. Row (2a) shows the effectiveness\nof using document titles for weak supervision. Rows (2b) and (2c) show the effectiveness of using\npseudo-relevant documents, with different queries. In row (2b), the AOL query log [Pass et al.,\n2006] is used, which might be characterized as “out of domain” queries. In row (2c), queries from\n126\nthe training set of the MS MARCO document ranking test collection were used (but without the\ncorresponding relevant documents); this can be characterized as weak supervision using “in domain”\nqueries. We see that weak supervision with MS MARCO queries (i.e., “in domain” queries) is more\neffective than using document metadata, but using the AOL query log (i.e., “out of domain” queries)\nis worse than simply using document metadata.\nDrawing results from Lin et al. [2021a], we are able to provide a comparison between HDCT and\ndoc2query–T5. In Table 35, row (4a) shows their reported BM25 results on the MS MARCO\ndocument ranking test collection, which is on par with the results in row (1). Row (4b) shows\ndocument expansion using doc2query–T5 using a model trained on the passage dataset. In these\nexperiments, the expansion was performed as follows: ﬁrst, each document was segmented into\npassages; expansion was performed on each passage independently to generate the predicted queries,\nand ﬁnally, all the predictions were concatentated together and appended to the original document.\nFor additional details, see Pradeep et al. [2021b] and documentation in the reference implementation\nat doc2query.ai. The appropriate comparison condition is row (3), since doc2query–T5 was trained\non MS MARCO data in a supervised way.125\nInterestingly, whereas Table 34 shows that doc2query–T5 is more effective than DeepCT for passage\nretrieval, results in Table 35 suggest that the effectiveness of HDCT is on par with doc2query–T5\nfor document retrieval, even though it only performs term weighting. We suspect that the simple\ndocument expansion adaptation of doc2query–T5 is not an entirely adequate solution, because not\nall parts of a long text are a priori likely to be relevant. In other words, there are some parts of a\nlong text that are more important than others. With the simple expansion approach described above,\ndoc2query is indiscriminately generating expansions for all passages, even lower quality ones; this\nmight dilute the impact of high-quality predictions from “important” passages. HDCT attempts to\ncapture similar intuitions using passage weights, as in Eq. (59), but the model is hampered by the\nlack of passage-level judgments.\nTakeaway Lessons. Building on DeepCT, HDCT provides three additional important lessons. First,\nit offers relatively simple solutions to the length limitations of BERT, thus allowing the same ideas\nbehind DeepCT to be applied to longer texts. Second, while an accurate term weighting model can\nbe learned with manual relevance judgments, weak supervision with labels from pseudo-relevant\ndocument gets us around 65% of the gains from a fully-supervised approach. Finally, term reweighting\nonly with HDCT yields increased effectiveness that is on par with a simple extension of doc2query to\nlonger texts, suggesting that there remains more work to be done on reﬁning document expansion\ntechniques for full-length documents.\n4.6 Combining Term Expansion with Term Weighting: DeepImpact\nOne of the advantages of doc2query compared to DeepCT (and HDCT) is that it can generate terms\nthat are not present in the original text, which increases the likelihood that the text will be retrieved in\nresponse to queries formulated in different ways. This tackles the core of the vocabulary mismatch\nchallenge. However, to produce these diverse terms, we need to sample multiple query predictions\nfrom the sequence-to-sequence model, which is not only computationally expensive, but may result\nin spurious terms that are unrelated to the original text. One advantage of DeepCT (and HDCT) over\ndoc2query is its ability to precisely control the importance weights on individual terms. In contrast,\nterm weighting in doc2query is primarily a side effect of repeat occurrences of duplicate and novel\nterms in the predicted queries. Since multiple queries are sampled from the sequence-to-sequence\nmodel independently, doc2query is not able to explicitly control term weights.\nWhat if we could obtain the best of both worlds by combining DeepCT and doc2query? Mallia\net al. [2021] did exactly this, in what they called DeepImpact, which combines the two techniques\nin a straightforward yet effective manner. DeepImpact ﬁrst performs document expansion using\ndoc2query and then uses a scoring model to estimate the importance of terms in the expanded\ndocument (i.e., their term weights). This two-step process allows the model to ﬁlter out (or at least\ndown-weight) non-relevant terms produced by doc2query while appropriately reweighting relevant\nexisting and new terms.\n125A minor detail here: doc2query–T5 was trained with MS MARCO passage data, while HDCT was trained\nwith MS MARCO document data.\n127\nTo compute term weights, DeepImpact begins by feeding the original text and expansion terms\nfrom doc2query–T5 into BERTBase to generate contextual embeddings. The ﬁrst occurrence of each\nunique term is then used as input to a two-layer MLP with ReLU activations to predict the term’s\nweight. Differently from DeepCT, which is trained with a regression loss (based on query term recall,\nsee Section 4.4), the DeepImpact scoring model is trained with pairwise cross-entropy loss, based\non (query, positive passage, negative passage) triples from the MS MARCO passage ranking test\ncollection. The objective is to maximize the difference between query–document scores of a positive\nexample and a negative example, where query–document scores are computed as the sum of the\nscores from document and expansion terms that occur in the queries.\nThe trained model is then used to compute the term weights of the document and expansion terms\nfor each text in a corpus. These real-valued weights are then quantized into the range of [1,2b −1],\nwhere b = 8. Recall that in DeepCT, integer weights are indexed using a standard search engine\nby creating pseudo-documents where a term is repeated a number of times equal to its weight\n(see Section 4.4). Instead of adopting this approach, Mallia et al. [2021] indexed the expansion\nresults by directly storing the quantized weight in the term frequency position of a standard inverted\nindex in the open-source PISA search engine [Mallia et al., 2019] via a custom data ingestor. This\nyields what the literature calls an impact index [Anh et al., 2001]; these quantized scores are called\n“impacts”. At query time, query–document scores are computed as the sum of the integer weights\n(computed from the DeepImpact scoring model) of document and expansion terms that match query\nterms. This approach to ranking builds on a long line of research dating back decades that exploits\nquery evaluation optimizations based on integer arithmetic [Anh et al., 2001, Anh and Moffat, 2002,\nTrotman et al., 2012, Crane et al., 2013, Lin and Trotman, 2015, Crane et al., 2017], as opposed to\nﬂoating point operations, which are required for BM25.\nExperiments on the MS MARCO passage ranking test collection demonstrate that DeepImpact is\nmore effective than both DeepCT and doc2query–T5, as shown in Table 36, with the effectiveness\nﬁgures copied from the authors’ original paper. The latency ﬁgures for the (a) rows are based on the\nPISA system [Mallia et al., 2019], which implements highly optimized query evaluation algorithms\nthat can be quite a bit faster than Lucene. The latency ﬁgures for reranking, i.e., the (b) rows, are taken\nfrom Figure 16 in Section 3.5; these numbers are representative of the typical latencies associated\nwith BERT-based reranking. Results show that while DeepImpact is certainly more effective, it is\nalso slower than doc2query–T5 at query time (although we are still squarely in the realm of latencies\nadequate to support interactive retrieval). This is a curious ﬁnding, as the two techniques differ only\nin the weights assigned to the terms; both are still based on bag-of-words keyword retrieval. The\nauthors trace this to the query processing strategy: the distribution of scores induced by DeepImpact\ncannot be efﬁciently exploited by the underlying MaxScore query evaluation algorithm used by PISA\nin these experiments.\nThese results also show that the effectiveness of DeepImpact alone is only around three points less\nthan BM25 + monoBERT on the MS MARCO passage ranking test collection, as seen in row (1b)\nvs. row (4a). This is quite impressive and worth emphasizing: DeepImpact is more than an order\nof magnitude faster than BM25 + monoBERT reranking and furthermore does not require neural\ninference (e.g., with GPUs) at query time. However, since DeepImpact’s Recall@1k is similar to\nthat of doc2query–T5, both methods yield similar effectiveness when combined with a monoBERT\nreranker, see row (3b) vs. (4b). That is, although DeepImpact used alone is much more effective than\ndoc2query–T5, in terms of end-to-end effectiveness as part of a reranking pipeline, there doesn’t\nseem to be any noticeable difference in output quality as a ﬁrst-stage ranker.\nTakeaway Lessons. DeepImpact is an effective document expansion and term weighting method\nthat combines the strengths of doc2query and DeepCT. On the MS MARCO passage ranking task, it\nachieves a level of effectiveness that approaches a simple monoBERT reranker with only keyword-\nbased retrieval, requiring no neural inference at query time.\n4.7 Expansion of Query and Document Representations\nAll the techniques presented thus far have involved manipulations ofterm-based representations of\nqueries and documents. That is, the query expansion techniques involve augmenting the original\nquery with additional terms (with associated weights), and similarly, document expansion techniques\ninvolve adding terms to the documents (or reweighting existing terms).\n128\nMS MARCO Passage(Dev) Latency\nMethod MRR@10 Recall@1k (ms/query)\n(1a) BM25 0.184 0.853 13\n(1b) + monoBERT 0.355 0.853 10,700\n(2a) DeepCT 0.244 0.910 10\n(2b) + monoBERT 0.360 0.910 10,700\n(3a) doc2query–T5 0.278 0.947 12\n(3b) + monoBERT 0.362 0.947 10,700\n(4a) DeepImpact 0.326 0.948 58\n(4b) + monoBERT 0.362 0.948 10,700\nTable 36: The effectiveness of DeepImpact on the development set of the MS MARCO passage\nranking test collection.\nIn contrast to these term expansion approaches, researchers have considered the problem of expanding\nquery and document representations that are non-textual in nature (as one might expect, leveraging\nthe output of transformers). In this section, we discuss two techniques that create additional query\nrepresentations using pseudo-relevance feedback [Zheng et al., 2020, Yu et al., 2021] and a technique\nbased on augmenting document representations [MacAvaney et al., 2020d].\nExpansion of query representations. The BERT-QE approach proposed by Zheng et al. [2020]\nextends the pre-BERT NPRF (Neural Pseudo Relevance Feedback) approach [Li et al., 2018] to take\nadvantage of BERT-based relevance classiﬁcation. Given a monoBERT model ﬁne-tuned for ranking\non a target dataset, BERT-QE consists of three steps:\n1. The top-1000 documents from a ﬁrst-stage retrieval method are reranked with monoBERT to\nproduce a set of kd = 10 top-ranked feedback documents.\n2. The feedback documents are divided into separate passages using a sliding window of size\nm= 10, and monoBERT is used to produce a relevance score for each passage ci with respect\nto the query q, rel(q,ci). The top kc = 10 passages are retained to produce a set of feedback\npassages.\n3. A monoBERT model is used to compare each feedback passage to a candidate document d\nthat is being ranked, i.e., rel(ci,d). This is performed for each document dfrom the top-1000\ndocuments in the initial reranking (step 1). Given these scores, an overall score rel(P,D) is\nproduced that represents how similar the candidate document is to the complete set of feedback\npassages P:\nrel(P,d) =\n∑\npi∈P\nrel(pi,d) ·softmax(rel(q,pi)) (60)\nEach document’s ﬁnal relevance score is computed as the interpolation of the query–document\nrelevance score after reranking rel(q,d) and the overall feedback passage–document relevance\nscore rel(P,d).\nZheng et al. [2020] evaluated their approach using BERT on the Robust04 and Gov2 test collections\n(using title queries). To rerank long documents, the authors used a variation of BERT–MaxP where\neach document was represented by its highest-scoring passage according to a monoBERT model that\nwas pre–ﬁne-tuned on the MS MARCO passage ranking test collection. After applying this procedure\nas a preprocessing step, the monoBERT model was ﬁne-tuned on the target collection to rerank results\nfrom the DPH + KL query expansion method [Amati et al., 2007]. According to Zheng et al., this\npreprocessing technique reduced training time without harming effectiveness. The authors trained the\nmonoBERT model used in step (1) using a cross-entropy loss; the model was not ﬁne-tuned end-to-\nend with steps (2) and (3). Here, we present results using two BERT-QE variants: BERT-QE-Large\nuses a BERTLarge model with 340M parameters for all three steps, whereas BERT-QE-Medium uses a\nBERTLarge model for step (1) and a smaller BERTMedium model with only 42M parameters for steps\n(2) and (3). See the original paper for detailed analyses of effectiveness/efﬁciency tradeoff when\ndifferent BERT models are used in the various steps.\nExperimental results are shown in Table 37, directly copied from Zheng et al. [2020]. DPH +\nKL, row (1a), is the ﬁrst-stage retrieval method for BERT-QE, but BM25 + RM3 results are also\n129\nRobust04 Gov2\nMethod P@20 nDCG@20 MAP P@20 nDCG@20 MAP\n(1a) DPH + KL 0.3924 0.4397 0.3046 0.5896 0.5122 0.3605\n(1b) BM25 + RM3 0.3821 0.4407 0.2903 0.5634 0.4851 0.3350\n(2a) BERTBaseMaxP 0.4653 0.5278 0.3652 0.6591 0.5851 0.3971\n(2b) BERTLargeMaxP 0.4769 0.5397 0.3743 0.6638 0.5932 0.4082\n(3a) BERT-QE-Large 0.4888† 0.5533† 0.3865† 0.6748† 0.6037† 0.4143†\n(3b) BERT-QE-Medium 0.4888† 0.5569† 0.3829† 0.6732† 0.6002 0.4131 †\nTable 37: The effectiveness of BERT-QE on the Robust04 and Gov2 test collections using title queries.\nStatistically signiﬁcant increases in effectiveness over BERTLarge are indicated with the symbol †\n(p< 0.01, two-tailed paired t-test).\npresented for context in row (1b). Rows (2a) and (2b) present the MaxP baselines fromBERTBase and\nBERTLarge, respectively. BERT-QE-Large, row (3a) consistently achieves signiﬁcant improvements\nin effectiveness compared to the BERT model it is built upon, row (2b). This comes at the cost of\nrequiring about 11×more computations than the underlying BERTLarge model. BERT-QE-Medium,\nrow (3b) performs almost as well, with signiﬁcant improvements over BERTLarge in all cases except\nfor nDCG@20 on Gov2. This conﬁguration requires only 2 ×more computations compared to\nBERTLarge, and thus may represent a better tradeoff between efﬁciency and effectiveness. Comparing\nrows (2a) and (2b), BERTLarge obtains improvements over BERTBase, which differs from the results\npreviously observed in Section 3.3.2. The source of this difference is unclear: at a minimum,\nthe ﬁrst-stage ranking method, folds, and implementation differ from those used in the previous\nexperiments.\nAnother work that takes an approach similar to BERT-QE is the PRF Graph-based Transformer (PGT)\nof Yu et al. [2021], where feedback documents are also compared to each candidate document. In\ntheir most effective variant, PGT applies Transformer-XH [Zhao et al., 2019] to feedback documents\nfrom a ﬁrst-stage ranking method, where each feedback document is placed into the following input\ntemplate:\n[[CLS],query,[SEP],candidate document,[SEP],feedback document,[SEP]]. (61)\nThis step produces a vector composed of the weighted sum of the [CLS] tokens from the feedback\ndocuments, which is then used to predict a relevance score. The model is trained with cross-entropy\nloss and evaluated on the TREC 2019 Deep Learning Track passage ranking task. When combined\nwith BM25 for ﬁrst-stage retrieval, it signiﬁcantly improved over monoBERT in terms of MAP@10,\nbut yielded only a small improvement in terms of nDCG@10 and performed worse in terms of\nMAP@100. Yu et al. [2021] also evaluated other less effective PGT variants that make changes to\nthe feedback document representations (e.g., by not prepending the query and candidate document)\nor to the graph structure (e.g., by including a node for the query and candidate document). We do not\ndiscuss these variants here, and instead refer readers to the authors’ original paper.\nExpansion of document representations. Rather than creating additional query representations like\nthe papers discussed above, the EPIC model (short for “Expansion via Prediction of Importance\nwith Contextualization”) proposed by MacAvaney et al. [2020d] creates expanded densedocument\nrepresentations. At its core, EPIC is a bi-encoder model that expands dense document representations\ndirectly without considering the query or feedback documents (bi-encoders and dense representations\nwill be detailed in Section 5). EPIC represents both query and texts from the corpus as vectors\nwith |V|dimensions, where V is the WordPiece vocabulary. Queries are represented as sparse\nvectors in which only tokens appearing in the query have non-zero values, while documents are\nrepresented as dense vectors. Query vectors contain term importance weights that are computed from\nthe corresponding contextual term embeddings using a feedforward layer. Document vectors are\nproduced by ﬁrst projecting each contextual term embedding to |V|dimensions, which the authors\ndescribed as an expansion step. The expanded document term vectors are then weighted with a\ndocument quality score (using a feedforward layer that takes the [CLS] token of the document as\ninput) and a term importance weight, which is computed analogously to query term importance\nweights, and then combined into a single document representation with max pooling. Finally, EPIC\ncomputes relevance scores by taking the inner product between query and document representations.\nThe model is trained using a cross-entropy loss.\n130\nIn their experiments, MacAvaney et al. [2020d] applied EPIC as a reranker on top of documents\nretrieved by BM25 or doc2query–T5. While EPIC was able to signiﬁcantly outperform these ﬁrst-\nstage retrieval approaches, when reranking BM25 it was less effective than variants of the efﬁcient TK\nreranking model (described in Section 3.5.2), which is the appropriate point of comparison because\nlow query latency was one of the authors’ selling points.\nTakeaway Lessons. To sum up, expanding query representations rather than expanding the query\ndirectly can be effective. While these are interesting ideas, it is not clear if they are compelling when\ncompared to dense retrieval techniques in terms of effectiveness/efﬁciency tradeoffs (as we’ll shortly\nsee). We wrap up this section with a few concluding thoughts and then proceed to focus on ranking\nwith learned dense representations.\n4.8 Concluding Thoughts\nQuery and document expansion techniques have a long history in information retrieval dating back\nmany decades. Prior to the advent of BERT, and even neural networks, expansion techniques have\nfocused on bringing queries and texts from the corpus into “better alignment” by manipulating\nsparse (i.e., keyword-based) representations. That is, query and document expansion techniques\nliterally added terms to the query and documents, respectively (possibly with weights). Indeed, many\ninitial attempts at transformer-based query and document expansion techniques largely mimicked\nthis behavior, focusing on term-based manipulations. On the document end, techniques such as\ndoc2query, DeepCT, and HDCT have been shown to be simple and effective. On the query end,\nthe results are mixed (i.e., modest gain in effectiveness, but at great computational cost) and do not\nappear to be unequivocally compelling.\nMore recently, researchers have begun to explore expansion methods that move beyond manipulations\nof term-based representations, like the work discussed in Section 4.7. Conceptually, these techniques\nbegin to blur the lines between transformer-based reranking models and expansion methods, and serve\nas a nice segue to ranking with dense representations, the topic of the next section. Operationally,\npost-retrieval query expansion methods (which include techniques based on pseudo-relevance feed-\nback) behave no differently from rerankers in a multi-stage reranking pipeline, except that the module\ninvolves another round of keyword-based retrieval. But internally, if the model is manipulating\ntransformer-based representations, isn’t it just another kind of transformer-based reranking? Docu-\nment expansion approaches that directly manipulate non-keyword representations begin to take on\nsome of the characteristics of transformer-based dense representations.\nThe blurring of these distinctions allows us to draw connections between methods that have very\ndifferent motivations and offers a lens through which to evaluate effectiveness/efﬁciency tradeoffs.\nFor example, if the goal of query expansion is to provide better candidate texts for a downstream\nreranker, then the end-to-end tradeoffs must be considered. For example, it could be the case that an\nimproved query expansion method only modestly improves the output quality of the downstream\nrerankers, but requires an increase in computational costs that make adoption impractical. We\nsee hints of this in CEQE from Section 4.2, where a BM25 →CEQE →CEDR pipeline is only\nslightly more effective than a similar pipeline using RM3 in place of CEQE. In some other cases,\nimprovements in ﬁrst-stage retrieval don’t have much effect on downstream rerankers. Consider\nDeepImpact from Section 4.6: monoBERT reranking of ﬁrst-stage retrieval with DeepImpact is only\nslightly better than monoBERT reranking of BM25 results, even though, in isolation, DeepImpact\nis far more effective. In fact, with monoBERT reranking, end-to-end effectiveness appears to be\nsimilar with either doc2query–T5 or DeepImpact as ﬁrst-stage retrieval. We suspect that this happens\nbecause of a mismatch between texts that the rerankers see during training and inference. Typically,\nmonoBERT is trained on candidates from BM25 initial retrieval (and indeed, as are most ranking\nmodels discussed in Section 3), but at query time the rerankers may be presented with candidates\nproduced by a different approach. Thus, independent stage-wise optimizations may not translate into\nincreased end-to-end effectiveness.\nRegardless, document and query expansion techniques that focus on manipulating representations\ninstead of terms appear to be, at a high level, a very promising direction for tackling the vocabulary\nmismatch problem. Such an approach brings us quite close to directly ranking with learned dense\nrepresentations. That, we turn to next.\n131\n5 Learned Dense Representations for Ranking\nArguably, the single biggest beneﬁt brought about by modern deep learning techniques to text\nranking is the move away from sparse signals, mostly limited to exact matches, to continuous\ndense representations that are able to capture semantic matches to better model relevance (see\nSection 1.2). With so-called dense retrieval techniques, the topic of this section, we can perform\nranking directly on vector representations (naturally, generated by transformers). This approach\nhas the potential to address the vocabulary mismatch problem by directly performing relevance\nmatching in a representation space that “captures meaning”—as opposed to reranking the output of\nkeyword-based ﬁrst-stage retrieval, which still relies on sparse exact match signals (document and\nquery expansion techniques discussed in Section 4 notwithstanding).\nThe potential of dense representations for analyzing natural language was ﬁrst demonstrated with\nword embeddings on word analogy tasks, which is generally viewed as the beginning of the “neural\nrevolution” in natural language processing. However, as soon as we try to build continuous represen-\ntations for any larger spans of text (phrases, sentences, paragraphs, and documents), many of the same\nissues that arise in text ranking come into focus. Here, as we will see, there is a close relationship\nbetween notions of relevance from information retrieval and notions of textual similarity from natural\nlanguage processing.\nThe focus of this section is the application of transformers to generate representations of texts that are\nsuitable for ranking in a supervised setting; this is a special case of what machine learning researchers\nwould call representation learning. We begin with a more precise formulation of what we mean by\ntext ranking using learned dense representations (also called dense retrieval), and identify connections\nbetween relevance and textual similarity problems. In particular, while we adopt a ranking perspective,\nthe core challenge remains the problem of estimating the relation between two pieces of text.\nIn the same way that keyword search requires inverted indexes and associated infrastructure to\nsupport top-k ranking using exact matches on a large corpus, top- k ranking in terms of simple\nvector comparison operations such as inner products on dense representations requires dedicated\ninfrastructure as well. We present an overview of this problem, known as nearest neighbor search, in\nSection 5.2. Efﬁcient, scalable solutions are available today in open-source libraries.\nAs with neural reranking techniques, it is helpful to discuss historical developments in terms of\n“pre-BERT” and “post-BERT” models: Section 5.3 overviews ranking based on dense representations\nprior to BERT. We can clearly see connections from recent work to similar ideas that have been\nexplored for many years, the main difference being the type of neural model applied.\nAfter this setup, our survey of dense retrieval techniques is divided into three parts:\n• Section 5.4 introduces the so-called bi-encoder design, which is contrasted with rerankers based\non a cross-encoder design (all of the models presented in Section 3). This section focuses on\n“simple” bi-encoders, where each text from the corpus is represented by a single vector, and\nranking is based on simple comparison operations such as inner products.\n• Section 5.5 presents techniques that enhance the basic bi-encoder design in two ways: each text\nfrom the corpus can be represented by multiple vectors and ranking can be performed using\nmore complex comparisons between the representations. These techniques aim for different\neffectiveness/efﬁciency tradeoffs compared to “simple” bi-encoders.\n• Section 5.6 discusses dense retrieval techniques that take advantage of knowledge distillation.\nInstead of directly training dense retrieval models, we ﬁrst train larger or more effective models\n(e.g., cross-encoders), and then transfer their knowledge into bi-encoder models.\nFinally, we conclude our treatment of learned dense representations in Section 5.7 with a discussion\nof open challenges and some speculation on what’s to come.\n5.1 Task Formulation\nWe begin by more precisely deﬁning the family of techniques covered in this section. Because text\nranking with dense representations, or dense retrieval, is an emerging area of research, the literature\nhas not yet converged on consistent terminology. In this survey, we try to synthesize existing work\nand harmonize different deﬁnitions without unnecessarily introducing new terms.\n132\nThe core problem of text ranking remains the same as the setup introduced in Section 2: We assume\nthe existence of a corpus C= {di}comprised of an arbitrary number of texts. Given a query q, the\ntask is to generate a top- kranking of texts from Cthat maximizes some metric of quality. In the\nmulti-stage ranking architectures covered in Section 3, this is accomplished by ﬁrst-stage retrieval\nusing keyword search (i.e., based on sparse bag-of-words representations), followed by one or more\nrerankers (based on BERT or some other transformer architecture operating on dense representations).\nDense retrieval, in contrast, has a different setup. In the basic problem formulation, we would like\nto learn some transformation η : [t1...tn] →Rn on queries and texts from the corpus, 126 denoted\nηq(·) and ηd(·), respectively, that converts sequences of tokens into ﬁxed-width vectors,127 such that\nthe similarity between ηq(·) and ηd(·) is maximized for texts relevant to a query and the similarity\nbetween ηq(·) and ηd(·) is minimized for non-relevant texts to a query, given a particular similarity\ncomparison function φ.\nAt query (search) time, for a given query q, we wish to retrieve the top ktexts from the corpus C\nwith the highest similarity given the same encoders ηq and ηd and the comparison function φ. In the\ncase where φis deﬁned in terms of a small number of simple vector comparison operations such as\nthe inner product, efﬁcient and scalable off-the-shelf solutions exist in libraries for nearest neighbor\nsearch (see Section 5.2). More complex comparison functions are also possible, representing tradeoffs\nbetween effectiveness and efﬁciency.\nSpeciﬁcally, in dense retrieval, we wish to estimate the following:\nP(Relevant = 1|di,q)\n∆\n= φ(ηq(q),ηd(di)), (62)\nthat is, the relevance of a text with respect to a query.\nSince there is no currently agreed upon symbol for the transformation that maps token sequences to\nvectors (also called a representation function) in the literature, we introduce the symbol η(eta) as a\nmnemonic for “encoder”. We use this notation throughout this section since it appropriately evokes\nthe notion of feeding the input sequence into a deep neural network. Encoders for queries and texts\nfrom the corpus could either be the same or they could use separate models; we discuss this design\nchoice in more detail below.\nThe output of the encoder is a dense representation (typically, a ﬁxed-width vector). One intuitive\nway to think about these representations is “like word embeddings, but for sequences of tokens”.\nThese representations are dense in the commonly understood sense, typically having hundreds of\ndimensions, with each dimension taking on non-zero values, as opposed to sparse representations\nwhere the number of dimensions is equal to the vocabulary size, with most of the elements being zero.\nThus, dense representations establish a poignant contrast to sparse representations, which has entered\nthe lexicon to describe bag-of-words representations such as BM25-weighted document vectors.\nSimilarly, sparse retrieval is often used today to characterize keyword search based on exact match,\neven though the term itself is a recent invention.\nWhat about the similarity function? Generally, φis assumed to be symmetric, i.e., φ(u,v) = φ(v,u).\nFurthermore, φshould be “fast to compute”. There is, unfortunately, no precise, widely agreed upon\ndeﬁnition of what this means, except by illustration. Most commonly, φis deﬁned to be the inner\nproduct between the representation vectors (or cosine similarity, where the only difference is length\nnormalization), although other metrics such as (one minus) L1 or L2 distance are sometimes used.\nWhile in principle φcould be a deep neural network, it is understood that the comparison function\nmust be lightweight—otherwise, we could just deﬁne φto be inference by BERT, and we’re back to\nsomething like the monoBERT model again. Nevertheless, as we will discuss, there are interesting\noptions for φthat occupy the middle ground between these extremes (see Section 5.5).\nThus, dense retrieval techniques need to address two challenges:\n• the representation problem, or the design of the encoders η·, to accurately capture the “meaning”\nof queries and texts from the corpus for the purposes of ranking; and,\n126In the context of dense retrieval, we refer generically to “texts from the corpus” as the retrieval units fed to\nηd. Although this terminology can be slightly unwieldy at times, it avoids the confusion as to whether these\nretrieval units are passages, spans, paragraphs, documents, etc.\n127Note that this is a simpliﬁcation, as we present later in this section dense retrieval models where the encoders\ngenerate multiple vectors and matrices.\n133\nE1\nq1\nE2\nq2\nE3\nq3\nF1\nd1\nF2\nd2\nFm\ndm\n…\ns\n(a) Representation-Based\nE1q1\nE2q2\nE3q3\nF1\nd1\nF2\nd2\nFm\ndm…\ns\n…\n…\n… (b) Interaction-Based\nE[CLS]\nT[CLS]\n[CLS]\nE1\nU1\nq1\nE2\nU2\nq2\nE3\nU3\nq3\nE[SEP1]\nT[SEP1]\n[SEP]\nF1\nV1\nd1\nF2\nV2\nd2\nFm\nVm\ndm\nE[SEP2]\nT[SEP2]\n[SEP]\n…\n…\ns\n… … … … … … … …… …\n… (c) monoBERT\nFigure 22: The evolution of neural models for text ranking, copied from Figure 11 in Section 3.2.3:\nrepresentation-based approaches (left), interaction-based approaches (middle), and BERT (right).\nDense representations for ranking are most similar to representation-based approaches, except that\nmore powerful transformer-based encoders are used to model queries and texts from the corpus.\n• the comparison problem, or the design of φ, which involves a balance between what can be\nefﬁciently computed at scale and what is necessary to capture relevance in terms of the dense\nrepresentations.\nAs we’ll discuss in Section 5.3, both challenges predate BERT, although transformers broaden the\ndesign space of η·and φ.\nThe complete model comprised of ηq, ηd, and φis usually developed in a supervised manner. In\nthe transformer context, the encoders (and φin some cases as well) are trained (or ﬁne-tuned, to be\nmore accurate) with labeled data capturing the target task. The outputs of ηq and ηd in the supervised\nscenario are called learned representations, and thus the problem formulation is an instance of\nrepresentation learning. In principle, the encoders may have never been exposed to labeled training\ndata for the target task. When using pretrained transformers, however, the models may have been\nexposed to the target corpus during pretraining, but it seems odd to call the encoders “unsupervised”\nin this context. More common is the case where the models are ﬁne-tuned on out-of-distribution data\n(e.g., different queries, different corpora, or both) and directly applied to previously unseen texts in a\n“zero-shot” manner.\nAnother way to think about dense representations for ranking is in terms of the evolution of broad\nclasses of neural ranking models, dating back to pre-BERT approaches discussed in Section 1.2.4.\nA side-by-side comparison between pre-BERT representation-based models, pre-BERT interaction-\nbased models, and BERT is shown in Figure 11 in Section 3.2.3 and repeated here as Figure 22. The\ndense retrieval approaches we focus on in this section are architecturally similar to representation-\nbased approaches, Figure 22(a), except that more powerful transformer-based encoders are used\nto model queries and texts from the corpus. In previous models, the “arms” of the network that\ngenerate the vector representations (i.e., the encoders) are based on CNNs or RNNs. Today, these\nhave been replaced with BERT and other transformers. For the choice of the comparison function,\npre-BERT representation-based neural ranking models adopt a simple φsuch as inner product. With\ntransformer-based representations, such simple comparison functions remain common. However,\nresearchers have also explored more complex formulations of φ, as we will see in Section 5.5. Some\nof these approaches incorporate interactions between terms in the queries and texts from the corpus,\nreminiscent of pre-BERT interaction-based models.\nWhat are the motivations for exploring this formulation of the text ranking problem? We can point to\ntwo main reasons:\n• BERT inference is slow. This fact, as well as potential solutions, was detailed in Section 3.5.\nThe formulation of text ranking in terms of φ(ηq(q),ηd(d)) has two key properties:\nFirst, note that ηd(d) is not dependent on queries. This means that text representations can be\nprecomputed and stored, thus pushing potentially expensive neural network inference into a\npreprocessing stage—similar to doc2query and DeepCT (see Section 4). Although ηq(q) still\nneeds to be computed at query time, only a single inference is required, and over a relatively\nshort sequence of tokens (since queries are usually much shorter than texts from the corpus).\n134\nSecond, the similarity function φ is fast by design and ranking in terms of φ over a large\n(precomputed) collection of dense vectors is typically amenable to solutions based on nearest\nneighbor search (see Section 5.2).\n• Multi-stage ranking architectures are inelegant. Initial candidate retrieval is based on keyword\nsearch operating on sparse bag-of-words representations, while all subsequent neural reranking\nmodels operate on dense representations.\nThis has a number of consequences, the most important of which is the inability to perform\nend-to-end training. In practice, the different stages in the pipeline are optimized separately.\nTypically, ﬁrst-stage retrieval is optimized for recall, to provide the richest set of candidates\nto feed downstream rerankers. However, increased recall in candidate generation may not\ntranslate into higher end-to-end effectiveness. One reason is that there is often a mismatch\nbetween the data used to train the reranker (a static dataset, such as the MS MARCO passage\nranking test collection) and the candidate texts that are seen at inference time (e.g., the output of\nBM25 ranking or another upstream reranker). Although this mismatch can be mitigated by data\naugmentation and sampling tricks, they are heuristic at best.\nAlternatively, if the text ranking problem can be boiled down to the comparison function φ,\nwe would no longer need multi-stage ranking architectures. This is exactly the promise of\nrepresentation learning: that is it possible to learn encoders whose output representations are\ndirectly optimized in terms of similarity according to φ.128\nBefore describing ranking techniques for learned dense representations, it makes sense to discuss\nsome high-level modeling choices. The ranking problem we have deﬁned in Eq. (62) shares many\nsimilarities with, but is nevertheless distinct from, a number of natural language processing tasks that\nare functions of two input sequences:\n• Semantic equivalence. Research papers are often imprecise in claiming to work on computing\n“semantic similarity” between two texts, as semantic similarity is a vague notion. 129 Most\nresearch, in fact, use semantic similarity as a shorthand to refer to a series of tasks known as the\nSemantic Textual Similarity (STS) tasks [Agirre et al., 2012, Cer et al., 2017]. Thus, semantic\nsimilarity is operationally deﬁned by the annotation guidelines of those tasks, which fall around\nthe notion of semantic equivalence, i.e., “Do these two sentences mean the same thing?” While\nthese concepts are notoriously hard to pin down, the task organizers have carefully thought\nthrough and struggled with the associated challenges; see for example, Agirre et al. [2012].\nUltimately, these researchers have built a series of datasets that reasonably capture operational\ndeﬁnitions amenable to computational modeling.130\n• Paraphrase. Intuitively, paraphrase can be understood as synonymy, but at the level of token\nsequences. For example, “John sold the violin to Mary” and “Mary bought the violin from John”\nare paraphrases, but “Mary sold the violin to John” is not a paraphrase of either. We might\nformalize these intuitions in terms of substitutability, i.e., two texts (phrases, sentence, etc.)\nare paraphrases if one can be substituted for another without signiﬁcantly altering the meaning.\nFrom this, it is possible to build computational models that classify text pairs as either being\nparaphrases or not.131\n128Note that as a counterpoint, dense retrieval results can still be reranked, which puts us back in exactly this\nsame position again.\n129As a simple example, are apples and oranges similar? Clearly not, because otherwise we wouldn’t use the\nphrase “apples and oranges” colloquially to refer to different things. However, from a different perspective,\napples and oranges are similar in that they’re both fruits. The only point we’re trying to make here is that\n“semantic similarity” is an ill-deﬁned notion that is highly context dependent.\n130Formally, semantic equivalence is better conceptualized on an interval scale, so the problem is properly that\nof regression. However, most models convert the problem into classiﬁcation (i.e., equivalent or not) and then\nreinterpret (e.g., renormalize) the estimated probability into the ﬁnal scale.\n131In practice, paraphrase tasks are much more nuanced. Substitutability needs to be deﬁned in some context,\nand whether two texts are acceptable paraphrases can be strongly context dependent. Consider a community\nquestion answering application: “What are some cheap hotels in New York?” is clearly not a paraphrase of\n“What are cheap lodging options in London?” A user asking one question would not ﬁnd the answer to the\nother acceptable. However, in a slightly different context, “What is there to do in Hawaii?” and “I’m looking\nfor fun activities in Fiji.” might be good “paraphrases”, especially for a user who is in the beginning stages of\nplanning for a vacation and has not yet decided on a destination (and hence open to suggestions). As an even\n135\n• Entailment. The notion of entailment is formalized in terms of truth values: a text tentails\nanother text hif, typically, a human readingtwould infer that his most likely true [Giampiccolo\net al., 2007]. Thus, “John sold the violin to Mary” entails “Mary now owns the violin”. Typically,\nentailment tasks involve a three-way classiﬁcation of “entailment”, “contradiction”, or “neutral”\n(i.e., neither). Building on the above example, “John then took the violin home” would contradict\n“John sold the violin to Mary”, and “Jack plays the violin” would be considered “neutral” since\nthe original sentence tells us nothing about Jack.\nThus, relevance, semantic equivalence, paraphrase, entailment are all similar tasks (pun intended)\nbut yet are very different in certain respects. One main difference is that semantic equivalence and\nparaphrase are both symmetric relations, i.e., R(u,v) = R(v,u), but relevance and entailment are\nclearly not. Relevance is distinguished from the others in a few more respects: Queries are usually\nmuch shorter than the units of retrieval (for example, short keyword queries vs. long documents),\nwhereas the two inputs for semantic equivalence, paraphrase, entailment are usually comparable in\nlength (or at the very least, both are sentences). Furthermore, queries can either be short keywords\nphrases that are rather impoverished in terms of linguistic structure or well-formed natural language\nsentences (e.g., in the case of question answering); but for the other three tasks, it is assumed that all\ninputs are well-formed natural language sentences.\nWhen faced with these myriad tasks, a natural question would be: Do these distinctions matter? With\nBERT, the answer is, likely not. Abstractly, these are all classiﬁcation on two input texts 132 (see\nSection 3.1) and can be fed to BERT using the standard input template:\n[[CLS],s1,[SEP],s2,[SEP]] (63)\nwhere s1 and s2 are the two inputs. Provided that BERT is ﬁne-tuned with annotated data that capture\nthe nuances of the target task, the model should be able to “ﬁgure out” how to model the relevant\nrelationship, be it entailment, paraphrase, or query–document relevance. In fact, there is strong\nempirical evidence that this is the case, since BERT has been shown to excel at all these tasks.\nHowever, for ranking with learned dense representations, these task differences may very well be\nimportant and have concrete implications for model design choices. For text ranking, recall that we\nare trying to estimate:\nP(Relevant = 1|d,q)\n∆\n= φ(ηq(q),ηd(d)) (64)\nDoes it make sense to use a single η(·) for both qand d, given the clear differences between queries\nand texts from the corpus (in terms of length, linguistic well-formedness, etc.)? It seems that we\nshould learn separate ηq(·) and ηd(·) encoders? Speciﬁcally, in Figure 22(a), the two “arms” of the\nnetwork should not share model parameters, or perhaps not even share the same architecture? As we\nwill see, different models make different choices in this respect.\nNow, consider reusing much of the same machinery to tackle paraphrase detection, which can be\nformulated also as an estimation problem:\nP(Paraphrase = 1|s1,s2)\n∆\n= φ(η(s1),η(s2)) (65)\nHere, it would make sense that the same encoder is used for both input sentences, suggesting that\nmodels for relevance and paraphrase need to be different? Completely different architectures, or the\nsame design, but different model parameters? What about for entailment, where the relationship is\nnot symmetric? Researchers have grappled with these issues and offer different solutions. However, it\nremains an open question whether model-speciﬁc adaptations are necessary and which design choices\nare actually consequential.\nEstimating the relevance of a piece of text to a query is clearly an integral part of the text ranking\nproblem. However, in the context of dense representations, we have found it useful to conceptualize\nsemantic equivalence, paraphrase, and entailment (and broadly, sentence similarity tasks) as ranking\nmore extreme example, “Do I need a visa to travel to India?” and “What immunizations are recommended for\ntravel to India?” would appear to have little to do with each other. However, for a user whose underlying intent\nwas “I’m traveling to India, what preparations are recommended?”, answers to both questions are certainly\nrelevant, making them great “paraphrases” in a community question answering application. In summary, there\nare subtleties that defy simple characterization and are very difﬁcult to model.\n132In the case of Semantic Textual Similarity (STS) tasks, can be converted into classiﬁcation.\n136\nproblems also. In certain contexts, this formulation is natural: in a community question answering\napplication, for example, we might wish to ﬁnd the entry from a corpus of question–answer pairs\nwhere the question is the closest paraphrase to the user’s query. Thus, we would need to compute a\nranking of questions with respect to the degree of “paraphrase closeness”. However, other applications\ndo not appear to ﬁt a ranking formulation: for example, we might simply wish to determine if two\nsentences are paraphrases of each other, which certainly doesn’t involve ranking.\nOperationally, though, these two tasks are addressed in the same manner: we wish to estimate the\nprobability deﬁned in Eq. (65); the only difference is how many pairs we perform the estimation over.\nIn other words, in our problem formulation, ranking is simply probability estimation over a set of\ncandidates and then sorting by those estimated probabilities. We adopt a ranking conceptualization\nin this section because it allows us to provide a uniform treatment of these different phenomena.\nHowever, note that historically, these ideas developed mostly as separate, independent threads—for\nexample, most research on sentence similarity tasks did not speciﬁcally tackle retrieval problems; we\npresent more details about the development of these ideas in Section 5.4.\n5.2 Nearest Neighbor Search\nThere is one important implementation detail necessary for ranking with dense representations:\nsolving the nearest neighbor search problem. Recall that in the setup of the dense retrieval problem\nwe assume the existence of a corpus of texts C= {di}. Since a system is provided C“in advance”, it\nis possible to precompute the output of ηd(·) for all di; slightly abusing notation, we refer to these\nas ηi’s. Although this may be computationally expensive, the task is embarrassingly parallel and\ncan be distributed across an arbitrarily large cluster of machines. The counterpart, ηq(q), must be\ncomputed at query time; also, slightly abusing notation, we refer to this as ηq. Thus, the ranking\nproblem is to ﬁnd the top kmost similar ηi vectors measured in terms of φ. Similar to search using\ninverted indexes, this is also a top-kretrieval problem. When φis deﬁned in terms of inner products\nor a handful of other simple metrics, this is known as the nearest neighbor search problem.\nThe simplest solution to the nearest neighbor search problem is to scan all the ηi vectors and brute\nforce compute φ(ηq,ηi). The top k ηi’s can be stored in a heap and returned to the user after the\nscan completes. For small collections, this approach is actually quite reasonable, especially with\nmodern hardware that can exploit vectorized processing with SIMD instructions on the CPU [Wang\nand Lin, 2015] or exploit the parallelism of GPUs for this task. However, this brute force approach\nbecomes impractical for collections beyond a certain point. Multi-dimensional indexes (e.g., KD-\ntrees) offer solutions to the nearest neighbor search problem, but their standard use case is for\ngeospatial applications, and they typically do not scale to the size (in the number of dimensions) of\nthe representations that our encoders generate.\nModern efﬁcient and scalable solutions to the nearest neighbor search problem are based on ap-\nproximations, hence approximate nearest neighbor (ANN) search. There are a number of ways this\ncan be formalized: for example, Indyk and Motwani [1998] deﬁne the kϵ–nearest neighbor search\nproblem as the ﬁndings the kclosest vectors {η1,η2,...η k}such that the distance of ηi to ηq is at\nmost (1 + ϵ) times the distance from the actual ith nearest point to ηq. This is typically referred to as\nthe approximate nearest neighbor search problem.133 The approximation in this context is acceptable\nin practical applications because φdoes not model the task perfectly to begin with. In search, we are\nultimately interested in capturing relevance, and φis merely a proxy.\nThe earliest solutions to approximate nearest neighbor search were based on locality-sensitive\nhashing [Indyk and Motwani, 1998, Gionis et al., 1999, Bawa et al., 2005], but proximity graph\nmethods are generally acknowledged as representing the best approach today. Methods based\non hierarchical navigable small world (HNSW) graphs [Malkov and Yashunin, 2020] represent the\ncurrent state of the art in ANN search based on a popular benchmark.134 A popular open-source library\nfor ANN search is Faiss135 by Facebook [Johnson et al., 2017], which provides implementations of\nboth brute-force scans and HNSW. Many of the techniques discussed in this section use Faiss.\n133Historically, these developments were based on minimizing distance, as opposed to maximizing similarity. We\nretain the terminology of the original formulation here, but since both similarity and distance are in the range\n[0,1], similarity can be deﬁned as one minus distance. This makes maximizing similarity and minimizing\ndistance equivalent.\n134http://ann-benchmarks.com/\n135https://github.com/facebookresearch/faiss\n137\nThroughout this section, we assume the use of some library that efﬁciently solves the (approximate)\nnearest neighbor search problem for an arbitrarily large collection of dense vectors, in the same\nway that we assume the existence of efﬁcient, scalable keyword search using inverted indexes (see\nSection 2.8). There are, of course numerous algorithmic and engineering details to making such\ncapabilities a reality, but they are beyond the scope of this survey.\n5.3 Pre-BERT Text Representations for Ranking\nWhile the ideas behind word embeddings and continuous representations of words go back decades,\nword2vec [Mikolov et al., 2013b,a] is often regarded as ﬁrst successful implementation that heralded\nthe beginning of neural revolution in natural language processing. Although the paper was primarily\nabout word representations and similarities between words, the authors also attempted to tackle\ncompositionality and phrase representations. As we have discussed, a ranking problem emerges as\nsoon as we try to build and compare dense representations of text beyond individual words.\nWith word embeddings, word representations are static vectors and similarity comparisons are\ntypically performed via cosine similarity. However, for any unit of text beyond individual words, there\nare many options for tackling the representation problem and the comparison problem. Researchers\nhave grappled with these two challenges long before transformers were invented, and in fact, many\nrecent advances can be characterized as adaptations of old ideas, but with transformers. Thus, it\nmakes sense to survey these pre-BERT techniques.\nAfter the initial successes of word embeddings, the next burst of research activity focused on building\nsentence representations (and in general, representations of longer segments of text). To be clear, here\nwe are concerned with deriving representations from novel, previously unseen sentences; thus, for\nexample, the paragraph vector representation of Le and Mikolov [2014] is beyond the scope of this\ndiscussion since the technique requires training on a corpus to derive representations of paragraphs\ncontained in it. Since natural language has a hierarchical structure, many researchers adopted a\nhierarchical approach to composing word representations into sentence representations, for example,\nrecursive neural networks [Socher et al., 2013], and later, Tree-LSTMs [Tai et al., 2015]. Even later\n(but pre-BERT) models incorporated attention and interaction modeling in complex architectures with\nmany distinct architectural components; examples include He and Lin [2016], Chen et al. [2017b],\nLan and Xu [2018].\nAs an alternative, Iyyer et al. [2015] proposed Deep Averaging Networks, which disregarded hierar-\nchical structure to compute both sentence- as well as document-level representations by averaging\nthe embeddings of individual words and then passing the results through feedforward layers. The\nauthors demonstrated that, for classiﬁcation tasks, these simple networks were competitive with, and\nin some cases, outperformed more sophisticated models while taking far less time to train.\nTo our knowledge, the ﬁrst comprehensive evaluation of different aggregation techniques for sentence\nsimilarity tasks was the work of Wieting et al. [2016], who examined six different architectures for\ngenerating sentence embeddings, ranging from simple averaging of individual word representations\n(i.e., mean pooling) to an LSTM-based architecture. The authors examined both an in-domain\nsupervised setting, where models were trained with annotated semantic similarity data drawn from\nthe same distribution as the test data, as well as general purpose, domain independent embeddings for\nword sequences, using data from a wide range of other domains. While LSTMs worked well with\nin-domain data, simple averaging vastly outperformed LSTMs in out-of-domain settings.\nLater work examined other simple approaches for aggregating individual word representations into\nrepresentations of larger segments of text: weighted average of word embeddings with learned\nweights [De Boom et al., 2016], weighted average of word embeddings followed by modiﬁcation with\nSVD [Arora et al., 2017], random walks [Ethayarajh, 2018], and different pooling techniques [Shen\net al., 2018]. In our framework, these can be viewed as explorations of η. The high-level conclusion\nseems to be that simple aggregation and comparison methods are robust, fast to compute, and effective,\neither competitive with or outperforming more complex models.\nThe references cited above draw mostly from the NLP literature, where researchers are mostly\nconcerned with textual similarity and related tasks. Contemporaneously, IR researchers had been\nexploring similar ideas for document ranking with various representation-based models (see Sec-\ntion 1.2.4). For example, the Deep Structure Semantic Model (DSSM) [Huang et al., 2013] constructs\nvector representations of queries and documents using feedforward networks. For ranking, query\n138\nand document representations are directly compared using cosine similarity. In fact, the models\nwe presented in Section 5.4 all adopt this basic design, except that the feedforward networks are\nreplaced with transformers. As another example, the Dual Embedding Space Model (DESM) [Mitra\net al., 2016, Nalisnick et al., 2016] computes query–document relevance scores by aggregating cosine\nsimilarities across all query–document term pairs.\nThere are many other instances of learned representations for ranking similar to DSSM in the literature.\nHenderson et al. [2017] examined the problem of suggesting email responses in Gmail. Given a\ntraining corpus of (message, response) pairs, encoders using feedforward networks were trained\nto maximize the inner product between the representations of the training pairs. Similar ideas for\nend-to-end retrieval with learned representations were later explored by Gillick et al. [2018]. With an\nexpansive scope, Wu et al. [2018a] proposed StarSpace, with the tagline of “embed all the things”,\nthat tried to unify a wide range of tasks (classiﬁcation, ranking, recommendation, and more) as simple\nsimilarity comparisons of learned representations. Zamani et al. [2018] proposed the Standalone\nNeural Ranking Model (SNRM), which learned sparse query and document representations that could\nbe stored in a standard inverted index for efﬁcient retrieval.\nFinally, in addition to explorations of different encoder models, there has also been work on different\ncomparison functions, i.e., φ, beyond simple operations such as inner products. For example, Wang\nand Jiang [2017] explored the use of different comparison functions in text matching tasks and\nconcluded that some simple formulations based on element-wise operations can work better than\nneural networks. Another noteworthy innovation is word mover’s distance (WMD), which deﬁnes the\ndistance between two texts as the minimum amount of distance that the word representations of one\ntext need to “travel” to reach the corresponding word representations of the other text [Kusner et al.,\n2015]. This computation implicitly involves “aligning” semantically similar words from the two texts,\nwhich differs from the designs discussed above that compare aggregate representations. However,\nWMD is expensive to compute, and despite follow-up work speciﬁcally tackling this issue (e.g., Wu\net al. [2018b]), this approach does not appear to have gained widespread adoption for dense retrieval.\n5.4 Simple Transformer Bi-encoders for Ranking\nIn presenting the ﬁrst class of methods to ranking with learned dense representations—dense retrieval\nwith simple transformer bi-encoders—let us begin with a recap of the problem formulation presented\nin Section 5.1. Given an encoder ηq for queries, an encoder ηd for texts from the corpus, and a\ncomparison function φ, dense retrieval involves estimating the following over a corpusC= {di}:\nP(Relevant = 1|di,q)\n∆\n= φ(ηq(q),ηd(di)), (66)\nBased on these estimates of relevance, the ranker returns the top ktexts from the corpus. No surprise,\ntransformers form the basis of the encoders ηd and ηd.\nWe refer to this as a “bi-encoder” design, a term introduced by Humeau et al. [2019], and schematically\nillustrated in Figure 22(a).136 This contrasts with a “cross-encoder”, which is the standard BERT\ndesign that beneﬁts from all-to-all attention across tokens in the input sequence, corresponding to\nFigure 22(c). All the models we discussed in Section 3 can be considered cross-encoders. That is, a\nbi-encoder takes two inputs and generates two representations via ηq and ηd (which may, in fact, be\nthe same) that can be compared with φ, whereas a cross-encoder takes two inputs concatenated into\na single sequence that comprises an input template and generates an estimate of relevance directly.\nNote that, critically, computing ηd(di) does not depend on queries, i.e., the output of ηq(q), which\nmeans that representations of texts from the corpus can be computed “ahead of time” and indexed to\nfacilitate low latency querying.\nIn this section, we focus on “simple” bi-encoders, where (1) each query or text from the corpus is\nrepresented by a single ﬁxed-width vector, and (2) the similarity comparison functionφis deﬁned as a\nsimple operation such as inner product. Given these two constraints, retrieval can be cast as a nearest\nneighbor search problem with computationally efﬁcient off-the-shelf solutions (see Section 5.2). In\nthe next section (Section 5.5), we cover bi-encoders that relax both of these constraints.\n136The bi-encoder design is sometimes referred to as a Siamese architecture or “twin towers”; both terms are\npotentially problematic in that the former is considered by some to be derogatory and the later evokes negative\nimages of 9/11. The term bi-encoders seem both technically accurate and not associated with negative\nconnotations (that we are aware of).\n139\nWe begin by illustrating the basic design of bi-encoders with Sentence-BERT [Reimers and Gurevych,\n2019] in Section 5.4.1. Sentence-BERT, however, focused on sentence similarity tasks and did not\nspeciﬁcally tackle retrieval problems. In Section 5.4.2, we present DPR [Karpukhin et al., 2020b]\nand ANCE [Xiong et al., 2021] as exemplary instances of dense retrieval implementations built on\nthe basic bi-encoder design. Additional bi-encoder variants that help us better understand the design\nspace and key research issues are discussed in Section 5.4.3\nBefore getting started, however, we present some historical background on the development of\ndense retrieval techniques in order to recognize precedence and the important contributions of many\nresearchers. Since our overall presentation does not necessarily focus on the earliest known work, we\nfeel it is important to explicitly acknowledge how these ideas evolved.\nTransformer-based dense representations for semantic equivalence, paraphrase, entailment, and\nother sentence similarity tasks can be traced back to the Universal Sentence Encoder (USE) [Cer\net al., 2018a,b], which dates to March 2018, even before BERT was introduced! The Universal\nSentence Encoder aspired to be just that: to encode “sentences into embedding vectors that speciﬁcally\ntarget transfer learning to other NLP tasks”. USE was trained in an unsupervised manner using\ndata from a variety of web sources, including Wikipedia, web news, web question-answer pages\nand discussion forums, and augmented with supervised data from the Stanford Natural Language\nInference (SNLI) corpus [Bowman et al., 2015]. The goal of USE and much follow-up work was to\ncompute embeddings of segments of texts (sentences, paragraphs, etc.) for similarity comparisons.\nWork on BERT-based dense representations for similarity comparisons emerged in 2019 from a\nfew sources. To our knowledge, the earliest paper is by Humeau et al. [2019], dating from April\n2019. We use the bi-encoder vs. cross-encoder terminology that they introduced. Although the\nwork examined retrieval tasks, the setup was limited in scope (see Section 5.5.1 for more details).\nSeveral roughly contemporaneous papers appeared shortly thereafter. Sentence-BERT [Reimers and\nGurevych, 2019] applied the bi-encoder design to a number of sentence similarity tasks. At the same\ntime, Barkan et al. [2020] investigated how well a BERT-based cross-encoder could be distilled into a\nBERT-based bi-encoder, also in the context of sentence similarity tasks.137 However, neither Reimers\nand Gurevych [2019] nor Barkan et al. [2020] explicitly examined retrieval tasks.\nIn terms of explicitly applying transformer-based bi-encoders to retrieval tasks, we believe precedence\ngoes to Lee et al. [2019b].138 However, instead of direct retrieval supervision using labeled data, they\nelected to focus on pretraining using weak supervision techniques derived from the Inverse Cloze\nTask (ICT) [Taylor, 1953]. Related work by Guu et al. [2020] folded dense retrieval directly into\nthe pretraining regime. As later demonstrated by Karpukhin et al. [2020b] on some of the same\nquestion answering benchmarks, these approaches did not appear to be as effective as direct retrieval\nsupervision: Lee et al. [2019b] reported uneven gains over previous approaches based on BM25 +\nBERT such as BERTserini [Yang et al., 2019c] and the techniques proposed by Guu et al. [2020]\nappeared to be more complex, more computationally expensive, and less effective. However, as\nexplained by Kenton Lee (based on personal communications), these two papers aimed to tackle\na different problem, a setup where annotated data for direct supervision was unavailable, and thus\nrequired different solutions. For this reason, it might not be fair to only compare these techniques in\nterms of effectiveness.\nShortly thereafter, Yang et al. [2019a] proposed PairwiseBERT, which applied bi-encoders to align\ncross-lingual entities in knowledge graphs by comparing textual descriptions of those entities; this\nwas formulated as a cross-lingual ranking problem. Also contemporaneous was the “two-tower\n137The ﬁrst arXiv submission of Humeau et al. [2019] unambiguously pre-dated Sentence-BERT, as the latter\ncites the former. However, Humeau et al.’s original arXiv paper did not appear in a peer-reviewed venue until\nApril 2020, at ICLR [Humeau et al., 2020]. The arXiv versions of Reimers and Gurevych [2019] and Barkan\net al. [2020] appeared within two weeks of each other in August 2019.\n138As an interesting historical side note, similar ideas (but not using transformers) date back at least a decade [Yih\net al., 2011], and arguably even further back in the context of supervised dimensionality reduction tech-\nniques [Yu et al., 2006]. What’s even more remarkable is that some of the co-authors of Yih et al. [2011]\nare also co-authors on recent dense retrieval papers, which suggests that these ideas had been “brewing” for\nmany years, and ﬁnally, with pretrained transformers, the “technical machinery” ﬁnally “caught up” to enable\nthe successful execution of much older ideas and insights. See additional discussion in Section 6 where we\nwonder if everything’s a remix.\n140\n( u, v, |u –v| )SoftmaxClassifier\nE[CLS]\nT[CLS]\nE1\nU1\nE2\nU2\nE3\nU3\nEn-2\nUn-2\nEn-1\nUn-1\nEn\nUn\nE[SEP]\nT[SEP]…\n…\nSentence A\nPoolingu\nE[CLS]\nT[CLS]\nE1\nU1\nE2\nU2\nE3\nU3\nEn-2\nUn-2\nEn-1\nUn-1\nEn\nUn\nE[SEP]\nT[SEP]…\n…\nSentence B\nPoolingv\ncosine similarity\nE[CLS]\nT[CLS]\nE1\nU1\nE2\nU2\nE3\nU3\nEn-2\nUn-2\nEn-1\nUn-1\nEn\nUn\nE[SEP]\nT[SEP]…\n…\nSentence A\nPoolingu\nE[CLS]\nT[CLS]\nE1\nU1\nE2\nU2\nE3\nU3\nEn-2\nUn-2\nEn-1\nUn-1\nEn\nUn\nE[SEP]\nT[SEP]…\n…\nSentence B\nPoolingv\nFigure 23: The architecture of Sentence-BERT, redrawn from Reimers and Gurevych [2019]. The\ntraining architecture for the classiﬁcation objective is shown on the left. The architecture for inference,\nto compute similarity scores, is shown on the right.\nretrieval model” of Chang et al. [2020], which focused on different weakly supervised pretraining\ntasks, like Lee et al. [2019b].139\nThe next major development was a parade of dense retrieval papers in rapid succession in 2020:\nTwinBERT [Lu et al., 2020] in February, CLEAR [Gao et al., 2020d], DPR [Karpukhin et al., 2020a],\nand MatchBERT [Yang et al., 2020c] in April, RepBERT [Zhan et al., 2020c] in June, and ANCE in\nJuly [Xiong et al., 2020b]. By around mid-2020, the promise and potential of dense retrieval had\nbeen ﬁrmly established in the literature.\n5.4.1 Basic Bi-encoder Design: Sentence-BERT\nWe present a more detailed description of Sentence-BERT [Reimers and Gurevych, 2019] as the\ncanonical example of a bi-encoder design for generating semantically meaningful sentence em-\nbeddings to be used in large-scale textual similarity comparisons (see Section 5.1). The overall\narchitecture is shown in Figure 23, redrawn from the authors’ paper. The diagram on the left shows\nhow Sentence-BERT is trained: each “arm” of the network corresponds toη(·) in our terminology,\nwhich is responsible for producing a ﬁxed-sized vector for the inputs (sentences in this case). Reimers\nand Gurevych [2019] experimented with both BERT and RoBERTa as the basis of the encoder and\nproposed three options to generate the representation vectors:\n• Take the representation of the [CLS] token.\n• Mean pooling across all contextual output representations.\n• Max pooling across all contextual output representations.\nThe ﬁrst option is obvious, while the other two draw from previous techniques discussed in Section 5.3.\nThe result is η(Sentence A) = uand η(Sentence B) = v, providing the solution to the representation\nproblem discussed in Section 5.1. Each “arm” of the bi-encoder uses the same model since the target\ntask is textual similarity, which is a symmetric relationship.\nDepending on the speciﬁc task formulation, the entire architecture is trained end-to-end as follows:\n• For classiﬁcation tasks, the representation vectors u, v, and their element-wise difference |u−v|\nare concatenated and fed to a softmax classiﬁer:\no= softmax(Wt ·[u⊕v⊕|u−v|]) (67)\nwhere ⊕denotes vector concatenation and Wt represents the trainable weights; standard cross-\nentropy loss is used.\n139Reimers and Gurevych [2019] and Yang et al. [2019a] both appeared at the same conference (EMNLP 2019,\nin November). Lee et al. [2019b] appeared a few months earlier at ACL in July 2020. Chang et al. [2020] was\nsubmitted for review at ICLR 2020 in September 2019.\n141\n• For regression tasks, mean squared loss between the ground truth and the cosine similarity of\nthe two sentence embeddings uand vis used.\nReimers and Gurevych [2019] additionally proposed a triplet loss structure, which we do not cover\nhere because it was only applied to one of the evaluation datasets.\nAt inference time, the trained encoder ηis applied to both sentences, producing sentence vectors u\nand v. The cosine similarity between these two vectors is directly interpreted as a similarity score;\nthis is shown in Figure 23, right. That is, in our terminology, φ(u,v) = cos(u,v). This provides the\nanswer to the comparison problem discussed in Section 5.1.\nSentence-BERT was evaluated in three different ways for textual similarity tasks:\n• Untrained. BERT (or RoBERTa) can be directly applied “out of the box” for semantic similarity\ncomputation.\n• Fine-tuned on out-of-domain datasets. Sentence-BERT was ﬁne-tuned on a combination of\nthe SNLI and Multi-Genre NLI datasets [Bowman et al., 2015, Williams et al., 2018]. The\ntrained model was then evaluated on the Semantic Textual Similarity (STS) benchmark [Cer\net al., 2017].\n• Fine-tuned on in-domain datasets. Sentence-BERT was ﬁrst ﬁne-tuned on the SNLI and Multi-\nGenre NLI datasets (per above), then further ﬁne-tuned on the training set of the STS benchmark\nbefore evaluation on its test set. This is similar to the multi-step ﬁne-tuning approaches discussed\nin Section 3.2.4.\nBelow, we present a few highlights summarizing experimental results, but refer readers to the authors’\noriginal paper for details. Sentence-BERT was primarily evaluated on sentence similarity tasks, not\nactual retrieval tasks, and since we do not present results on these tasks elsewhere in this survey,\nreporting evaluation ﬁgures here would be of limited use without points of comparison. Nevertheless,\nthere are a number of interesting ﬁndings worth discussing:\n• Without any ﬁne-tuning, average pooling of BERT’s contextual representations appears to be\nworse than average pooling of static GloVe embeddings, based on standard metrics for semantic\nsimilarity datasets. Using the [CLS] token was even worse than average pooling, suggesting\nthat it is unable to serve as a good representation “out of the box” (that is, without ﬁne-tuning\non task-speciﬁc data).\n• Not surprisingly, out-of-domain ﬁne-tuning leads to large gains on the STS benchmark over the\nuntrained condition. Also as expected, further in-domain ﬁne-tuning provides an additional boost\nin effectiveness, consistent with the multi-step ﬁne-tuning approaches discussed in Section 3.2.4.\nIn this setting, although the bi-encoder remained consistently worse than the cross-encoder, in\nsome cases the differences were relatively modest.\n• Ablation studies showed that with ﬁne-tuning, average pooling was the most effective design\nfor η, slightly better than max pooling or using the [CLS] token. Although the effectiveness\nof the [CLS] token was quite low “out of the box” (see above), after ﬁne-tuning, it was only\nslightly worse than average pooling.\n• For classiﬁcation tasks, an interesting ﬁnding is the necessity of including |u−v|in the input to\nthe softmax classiﬁer (see above). If the input to the softmax omits |u−v|, effectiveness drops\nsubstantially.\nClosely related to Sentence-BERT, the contemporaneous work of Barkan et al. [2020] investigated\nhow well a BERT-based cross-encoder can be distilled into a BERT-based bi-encoder for sentence\nsimilarity tasks. To do so, the authors trained a BERTLarge cross-encoder to perform a speciﬁc task\nand then distilled the model into a BERTLarge bi-encoder that produces a dense representation of its\ninput by average pooling the outputs of its ﬁnal four transformer layers. The experimental results\nwere consistent with the same general ﬁndings in Sentence-BERT: After distillation for a speciﬁc\ntask, the bi-encoder student performs competitively but remains consistently less effective than the\ncross-encoder teacher. However, as expected, the bi-encoder is signiﬁcantly more efﬁcient.\nTakeaway Lessons. Sentence-BERT provides a good overview of the basic design of bi-encoders,\nbut its focus was on textual similarity and not ranking. For a range of sentence similarity tasks, the\n142\nempirical results are clear: a bi-encoder design is less effective than a comparable cross-encoder\ndesign, but far more efﬁcient since similarity comparisons can be captured in simple vector operations.\nHowever, we need to look elsewhere for empirical validation of dense retrieval techniques.\n5.4.2 Bi-encoders for Dense Retrieval: DPR and ANCE\nWith the stage set by Sentence-BERT [Reimers and Gurevych, 2019], we can proceed to discuss\ntransformer-based bi-encoders speciﬁcally designed for dense retrieval. In this section, we present\nthe dense passage retriever (DPR) of Karpukhin et al. [2020b] and the approximate nearest neighbor\nnegative contrastive estimation (ANCE) technique of Xiong et al. [2021]. Interestingly, while these\ntwo techniques emerged separately from the NLP community (DPR) and the IR community (ANCE),\nwe are seeing the “coming together” of both communities to tackle dense retrieval.\nWhile neither DPR nor ANCE represents the earliest example of dense retrieval, considering a\ncombination of clarity, simplicity, and technical innovation, they capture in our opinion exemplary\ninstances of dense retrieval techniques based on simple bi-encoders and thus suitable for pedagogical\npresentation. In terms of technical contributions, both techniques grappled successfully with a key\nquestion in bi-encoder design: How do we select negative examples during training? Recall that our\ngoal is to maximize the similarity between queries and relevant texts and minimize the similarity\nbetween queries and non-relevant texts: Relevant texts, of course, come from human relevance\njudgments, usually as part of a test collection. But where do the non-relevant texts come from?\nDPR’s in-batch negative sampling provides a simple yet effective baseline, and ANCE demonstrates\nthe beneﬁts of selecting “hard” negative examples, where “hard” is operationalized in terms of the\nencoder itself (i.e., non-relevant texts that are similar to the query representation).\nThe dense passage retriever (DPR) of Karpukhin et al. [2020b], originally presented in April\n2020 [Karpukhin et al., 2020a], describes a standard “retriever–reader” architecture for question an-\nswering [Chen et al., 2017a]. In this design, a passage retriever selects candidate texts from a corpus,\nwhich are then passed to a reader to identify the exact answer spans. This architecture, of course,\nrepresents an instance of multi-stage ranking, which as we discussed extensively in Section 3.4, has a\nlong history dating back decades. Here, we focus only on the retriever, which adopts a bi-encoder\ndesign for dense retrieval.\nDPR uses separate encoders for the query and texts from the corpus, which in our notation corresponds\nto ηq and ηd, respectively; both encoders take the [CLS] representation from BERTBase as its output\nrepresentation. DPR was speciﬁcally designed for passage retrieval, so ηd takes relatively small spans\nof texts as input (the authors used 100-word segments of text in their experiments).\nIn DPR, relevance between the query representation and the representations of texts from the corpus,\ni.e., the comparison function φ, is deﬁned in terms of inner products:\nφ(ηq(q),ηd(di)) = ηq(q)⊺ηd(di) (68)\nThe model is trained as follows: let D= {⟨qi,d+\ni ,d−\ni,1,d−\ni,2,...d −\ni,n⟩}m\ni=1 be the training set compris-\ning minstances. Each instance contains a question q, a positive passage d+ that contains the answer\nto q, and nnegative passages d−\n1 ,d−\n2 ,...d−\nn. DPR is trained with the following loss function:\nL(q,d+,d−\n1 ,d−\n2 ,...d−\nn) = −log exp [φ(ηq(q),ηd(d+))]\nexp [φ(ηq(q),ηd(d+))] + ∑n\nj=1 exp\n[\nφ(ηq(q),ηd(d−\nj ))\n]. (69)\nThe ﬁnal important design decision in training DPR—and in general, a critical component of any\ndense retrieval technique—lies in the selection of negative examples. If our goal is to train a model\nthat maximizes the similarity between queries and relevant texts while at the same time minimizing\nthe similarity between queries and non-relevant texts (with respect to the comparison function φ),\nthen we need to deﬁne the composition of the non-relevant texts more precisely.\nKarpukhin et al. [2020b] experimented with three different approaches: (1) random, selecting random\npassages from the corpus, (2) BM25, selecting passages returned by BM25 that don’t contain the\nanswer, and (3) in-batch negative sampling, or selecting passages from other examples in the same\ntraining batch together with a mix of passages retrieved by BM25. Approach (2) can be viewed\nas selecting “difﬁcult” negatives using BM25, since the negative samples are passages that score\nhighly according to BM25 (i.e., contain terms from the question), but nevertheless do not contain\n143\nthe answer. With approach (3), the idea of training with in-batch negatives can be traced back to at\nleast Henderson et al. [2017], who also applied the technique to train a bi-encoder for retrieval, albeit\nwith simple feedforward networks over n-grams instead of transformers.\nEmpirically, approach (3) proved to be the most effective, and it is efﬁcient as well since the negative\nexamples are already present in the batch during training. Furthermore, effectiveness increases as\nthe batch size grows, and thus the quality of the encoders improves as we are able to devote more\ncomputational resources during training. We refer interested readers to the original paper for details\nregarding the exact experimental settings and results of contrastive experiments that examine the\nimpact of different negative sampling approaches.\nDPR was evaluated on a number of standard question answering datasets in the so-called “open-\ndomain” (i.e., retrieval-based) setting, where the task is to extract answers from a large corpus of\ndocuments—in this case, a snapshot of English Wikipedia. Following standard experimental settings,\npassages were constructed from Wikipedia articles by taking 100-word segments of text; these\nformed the units of retrieval and served as inputs to ηd. The ﬁve QA datasets used were Natural\nQuestions [Kwiatkowski et al., 2019], TriviaQA [Joshi et al., 2017], WebQuestions [Berant et al.,\n2013], CuratedTREC [Baudiš and Šedivý, 2015], and SQuAD [Rajpurkar et al., 2016].\nHere, we are only concerned with retrieval effectiveness, as opposed to end-to-end QA effectiveness.\nThe commonly accepted metric for this task is top- k accuracy, k ∈{20,100}, which measures\nthe fraction of questions for which the retriever returns at least one correct answer. This is akin to\nmeasuring recall in a multi-stage ranking architecture (see Section 3.4): in a pipeline design, these\nmetrics quantify the upper bound effectiveness of downstream components. In the case of question\nanswering, if the retriever doesn’t return candidate texts containing answers, there’s no way for a\ndownstream reader to recover. Note that in the NLP community, metrics are often reported in “points”,\ni.e., values are multiplied by 100, so 0.629 is shown as 62.9.\nInstead of directly reporting results from Karpukhin et al. [2020b], we share results from Ma et al.\n[2021c], which is a replication study of the original paper. Ma et al. were able to successfully replicate\nthe dense retrieval results and obtain scores that were very close to those in the original paper (in\nmost cases, within a tenth of a point). However, their experiments led to a substantive contrary\nﬁnding: according to the original paper, there is little to be gained from a hybrid technique combining\nDPR (dense) with BM25 (sparse) results via linear combination. In some cases, DPR alone was\nmore effective than combining DPR with BM25, and even if the hybrid achieved a higher score, the\nimprovements were marginal at best. The experiments of Ma et al., however, reported higher BM25\nscores than the original paper.140 This, in turn, led to higher effectiveness for the hybrid technique,\nand thus Ma et al. concluded that DPR + BM25 was more effective than DPR alone. In other words,\ndense–sparse hybrids appear to offer beneﬁts over dense retrieval alone.\nTable 38 shows the DPR replication results, copied from Ma et al. [2021c]. The authors applied paired\nt-tests to determine the statistical signiﬁcance of the differences ( p <0.01) with the Bonferroni\ncorrection as appropriate. The symbol †on a BM25 result indicates that the effectiveness difference vs.\nDPR is signiﬁcant; the symbol ‡indicates that the hybrid technique is signiﬁcantly better than BM25\n(for SQuAD) or DPR (for all remaining collections). We see that in four of the ﬁve datasets, dense\nretrieval alone (DPR) is more effective than sparse retrieval (BM25); in these cases, the differences\nare statistically signiﬁcant for both top-20 and top-100 accuracy.141 Ma et al. [2021c] experimented\nwith two different approaches for combining DPR with BM25 scores; as there were no signiﬁcant\ndifferences between the two, we report the technique they called Hybridnorm (see paper for details).\nAccording to their results, in most cases, the dense–sparse hybrid was more effective than BM25 (for\nSQuAD) or DPR (for all remaining collections). The improvements were statistically signiﬁcant in\nnearly all cases.\nBuilding on the basic bi-encoder design, Xiong et al. [2021] made the observation that non-relevant\ntexts ranked highly by an exact match method such as BM25 are likely to be different from non-\nrelevant texts ranked highly by a BERT-based bi-encoder. Thus, selecting negative examples from\n140This ﬁnding has been conﬁrmed by the original authors (personal communication).\n141The exception appears to be SQuAD, where BM25 effectiveness is higher, likely due to two reasons: First,\nthe dataset was created from only a few hundred Wikipedia articles, and thus the distribution of the training\nexamples is highly biased. Second, questions were created by human annotators based on the articles, thus\nleading to question formulations with high lexical overlap, giving an unnatural and unfair advantage to an\nexact match technique like BM25.\n144\nCollection / MethodTop-20 Top-100\nNaturalQuestions\n(1a) DPR 79.5 86.1\n(1b) BM25 62.9 † 78.3†\n(1c) Hybridnorm 82.6‡ 88.6‡\nTriviaQA\n(2a) DPR 78.9 84.8\n(2b) BM25 76.4 † 83.2†\n(2c) Hybridnorm 82.6‡ 86.5‡\nWebQuestions\n(3a) DPR 75.0 83.0\n(3b) BM25 62.4 † 75.5†\n(3c) Hybridnorm 77.1‡ 84.4‡\nCuratedTREC\n(4a) DPR 88.8 93.4\n(4b) BM25 80.7 † 89.9†\n(4c) Hybridnorm 90.1 95.0 ‡\nSQuAD\n(5a) DPR 52.0 67.7\n(5b) BM25 71.1 † 81.8†\n(5c) Hybridnorm 75.1‡ 84.4‡\nTable 38: The effectiveness of DPR (dense retrieval), BM25 (sparse retrieval), and dense–sparse\nhybrid retrieval on ﬁve common QA datasets. The symbol †on a BM25 result indicates effective-\nness that is signiﬁcantly different from DPR. The symbol ‡indicates that the hybrid technique is\nsigniﬁcantly better than BM25 (for SQuAD) or DPR (for all remaining collections).\nBM25 results may not be the best strategy. Instead, to train more effective bi-encoder models, the\nauthors proposed using approximate nearest neighbor (ANN) techniques to identify negative examples\nthat are ranked highly by the bi-encoder model being trained. Xiong et al. [2021] argued that their\napproach, called ANCE for “Approximate nearest neighbor Negative Contrastive Estimation”, is\ntheoretically more effective than both sampling BM25 results, which biases the model to mimic\nsparse retrieval, and in-batch negative sampling, which yields uninformative negative examples.\nANCE adopts a basic bi-encoder design just like DPR. It takes the [CLS] representation from\nRoBERTabase as the encoder η, and (unlike DPR) uses a single encoder for both the query and the\ndocument (i.e., ηq = ηd). During training, hard negative examples are selected via ANN search on an\nindex over the representations generated by the encoder being trained. Instead of maintaining a fully\nup-to-date index, which is computationally impractical, the ANN index is updated asynchronously.\nThat is, every mbatches, the entire corpus is re-encoded with ηand the ANN index is rebuilt. This is\nstill computationally expensive, but workable in practice. The training process begins with a “BM25\nwarm up” where the model is ﬁrst trained with BM25 negatives. The index refresh rate (together\nwith the learning rate) can be viewed as hyperparameters to trade off effectiveness and training\nefﬁciency, but the authors noted that a poor setting makes the training unstable. Given positive\ntraining examples, i.e., (query, relevant passage) pairs from the MS MARCO passage ranking test\ncollection, and negative training examples (from ANN search), the ANCE bi-encoder is trained with\na negative log likelihood loss.\nResults on the development set of the MS MARCO passage ranking task and the TREC 2019\nDeep Learning Track passage ranking task are presented in Table 39, copied from Xiong et al.\n[2021]. To provide a basis for comparison for the MS MARCO passage ranking task, we include\neffectiveness results from a standard cross-encoder design, i.e., BM25 ( k = 1000) + monoBERT,\ntaken from Nogueira and Cho [2019], shown in rows (1b) and (1c) for different BERT model sizes.\nThe effectiveness of the corresponding ﬁrst-stage retrieval using Microsoft’s BM25 implementation\n(prior to monoBERT reranking) is shown in row (1a). These are exactly the same ﬁgures reported in\nTable 5 from Section 3.2.1. Since ANCE uses RoBERTaBase, BM25 + monoBERTBase, row (1c), is the\nmore appropriate reference condition.142 For the TREC 2019 Deep Learning Track passage ranking\ntask, in row (2b) we report results from run p_bert submitted by the team h2oloo, which also\n142Note that while Nogueira et al. [2019a] reported a slightly higher monoBERT effectiveness due to better\nﬁrst-stage retrieval, they only presented results for BERTLarge and not BERTBase.\n145\nMS MARCO Passage(Dev) TREC 2019 DL Passage\nMethod MRR@10 Recall@1k nDCG@10\n(1a) BM25 (Microsoft Baseline) 0.167 - -\n(1b) BM25 + monoBERTLarge 0.365 - -\n(1c) BM25 + monoBERTBase 0.347 - -\n(2a) TREC 2019 run:baseline/bm25base_p - - 0.506\n(2b) TREC 2019 run:h2oloo/p_bert - - 0.738\n(3a) ANCE 0.330 0.959 0.648\n(3b) DR w/ in-batch 0.261 0.949 0.552\n(3c) DR w/ BM25 0.299 0.928 0.591\n(3d) DR w/ in-batch + BM25 (≈DPR) 0.311 0.952 0.600\nTable 39: The effectiveness of ANCE and cross-encoder baselines on the development set of the MS\nMARCO passage ranking test collection and the TREC 2019 Deep Learning Track passage ranking\ntest collection.\nMS MARCO Doc(Dev) TREC 2019 DL Doc\nMethod MRR@100 Recall@1k nDCG@10\n(1a) ANCE (MaxP) + BERT Base MaxP 0.432 - -\n(2a) TREC 2019 run:baseline/bm25base - - 0.519\n(2b) TREC 2019 run:h2oloo/bm25_marcomb - - 0.640\n(3a) ANCE (FirstP) 0.334 - 0.615\n(3b) ANCE (MaxP) 0.384 - 0.628\n(3c) DR (FirstP) w/ in-batch - - 0.543\n(3d) DR (FirstP) w/ BM25 - - 0.529\n(3e) DR (FirstP) w/ in-batch + BM25 (≈DPR) - - 0.557\nTable 40: The effectiveness of ANCE and cross-encoder baselines on the development set of the\nMS MARCO document ranking test collection and the TREC 2019 Deep Learning Track document\nranking test collection.\nrepresents BM25 (k= 1000) + monoBERT [Akkalyoncu Yilmaz et al., 2019a]; the corresponding\nﬁrst-stage retrieval with BM25 is reported in row (2a). Row (3a) presents the effectiveness of the full\nANCE model.\nIt is clear from Table 39 that a bi-encoder design is not as effective as a cross-encoder design\n(i.e., reranking ﬁrst-stage BM25 results with monoBERT). The differences between the comparable\nconditions in row groups (1) and (2) vs. row (3a) quantify the importance of attention between query\nand passage terms, as these interactions are eliminated in the bi-encoder design, reduced to an inner\nproduct (note, though, that bi-encoders preserve self-attention between terms in the query and terms\nin the passages). This, alas, is the cost of direct ranking with learned dense representations. Closing\nthe effectiveness gap between cross-encoders and bi-encoders is the goal of much subsequent work\nand research activity to this day.\nRows (3b) to (3d) in Table 39 represent ablations of the complete ANCE model. Dense retrieval (DR)\n“w/ in batch”, row (3b), uses in-batch negative sampling, but otherwise adopts the ANCE bi-encoder\ndesign. Dense retrieval (DR) “w/ BM25”, row (3c), uses BM25 results as negative examples, and\ncombining both “in batch” and “BM25” yields the DPR design, row (3d). Not surprisingly, the\ntechniques presented in rows (3b) and (3c) are less effective than ANCE, and ANCE appears to\nbe more effective than the DPR training scheme, row (3d). For detailed hyperparameter and other\nconﬁguration settings, we advise the reader to directly consult Xiong et al. [2021].\nIn addition to passage retrieval, ANCE was also evaluated on document retrieval. Results on the MS\nMARCO document ranking task and the TREC 2019 Deep Learning Track document ranking task\nare presented in Table 40. Extending ANCE from passage to document retrieval necessitated one\nimportant change to cope with the inability of transformers to process long input sequences (which\nwe discussed at length in Section 3.3). Here, Xiong et al. [2021] adopted the approaches of Dai and\nCallan [2019b] (see Section 3.3.2): FirstP, where the encoder only takes the ﬁrst 512 tokens of the\ndocument, and MaxP, where each document is split into 512-token passages (maximum 4) and the\n146\nNaturalQuestions TriviaQA\nMethod Top-20 Top-100 Top-20 Top-100\nfrom Karpukhin et al. [2020b]\n(1a) DPR 79.4 86.0 78.8 84.7\n(1b) BM25 59.1 73.7 66.9 76.7\n(1c) Hybrid 78.0 83.9 79.9 84.4\nfrom Ma et al. [2021c]\n(2a) DPR 79.5 86.1 78.9 84.8\n(2b) BM25 62.9 78.3 76.4 83.2\n(2c) Hybridnorm 82.6 88.6 82.6 86.5\n(3) ANCE 82.1 87.9 80.3 85.2\nTable 41: The effectiveness of ANCE and DPR on two QA datasets.\nhighest passage similarity is used for ranking (these settings differ from Dai and Callan [2019b]).\nThese two conﬁgurations are shown in row (3a) and row (3b), respectively. In the table, the results on\nthe TREC 2019 Deep Learning Track document ranking task are copied from Xiong et al. [2021], but\nthe paper did not report results on the MS MARCO document ranking task; instead, those ﬁgures are\ncopied from the ofﬁcial leaderboard.\nIn Table 40, rows (3c)–(3e) denote the same ablation conditions as in Table 39, with FirstP.143 In this\ncase, unfortunately, the comparable cross-encoder conditions are a bit harder to come by. For the\nMS MARCO document ranking task, note that the original MaxP work of Dai and Callan [2019b]\npredated the task itself. The closest condition we could ﬁnd is reported in row (1a), which uses ANCE\n(MaxP) itself for ﬁrst-stage retrieval, followed by reranking with a BERT cross-encoder.144 For the\nTREC 2019 Deep Learning Track document ranking task, the closet comparable condition we could\nﬁnd is run bm25_marcomb by team h2oloo, shown in row (2b), which represents BM25 (k= 1000)\nreranked by Birch, reported in Akkalyoncu Yilmaz et al. [2019a]. This run combines evidence from\nthe top three sentences, but is trained on MS MARCO passage data, thus muddling the comparisons.\nThe corresponding BM25 ﬁrst-stage retrieval results are shown in row (2a).\nWhile the contrastive comparisons are not perfect, these document ranking results are consistent\nwith the passage ranking results. Dense retrieval with bi-encoders do not appear to be as effective as\nreranking sparse retrieval results with cross-encoders, and the full ANCE model is more effective\nthan the ablation conditions, i.e., rows (3c)–(3e). Also consistent with Dai and Callan [2019b], MaxP\nis more effective than FirstP.\nOne key feature to making ANCE “work” is the synchronous ANN index update to supply informative\nnegative samples. Xiong et al. [2021] reported that for the MS MARCO document collection,\nindex refresh takes approximately 10 hours on a multi-GPU server. This quantiﬁes the additional\ncomputational costs of ANCE, compared to a simpler technique such as in-batch negative sampling.\nIndeed, there doesn’t appear to be a “free lunch”, and the reported effectiveness gains of ANCE come\nat the cost of slower training due to the expensive index refreshes.\nIn addition to evaluation on the MS MARCO datasets, Xiong et al. [2021] also evaluated ANCE\non some of the same datasets used in the DPR experiments, NaturalQuestions and TriviaQA. As\nthe authors directly compared ANCE with ﬁgures reported in Karpukhin et al. [2020b], we copy\nthose evaluation results directly into Table 41, in rows (1) and (3). For reference, we also share the\ncomparable conditions from the replication study of Ma et al. [2021c]. These experiments provide a\nfair “heads-up” comparison between ANCE and DPR.\nFocusing only on DPR, rows (1a) and (2a), and comparing against ANCE, row (3), the results conﬁrm\nthat ANCE is indeed more effective than DPR, although the differences are smaller for top-100 than\nfor top-20. Nevertheless, the gaps between ANCE and DPR appear to be smaller than the “DPR\nsetting” suggests in Tables 39 and 40. However, a hybrid combination of DPR and BM25 results,\nas reported by Ma et al. [2021c], appears to beat ANCE alone. Although Xiong et al. [2021] did\n143The FirstP setting was an experimental detail omitted in Xiong et al. [2021]; here we have clariﬁed based on\npersonal communications with the authors.\n144https://github.com/thunlp/OpenMatch/blob/master/docs/experiments-msmarco-doc.md\n147\nnot report any dense–sparse hybrid results, we would expect BM25 to improve ANCE as well if the\nresults were combined.\nFinally, Xiong et al. [2021] studied the effectiveness of ANCE as ﬁrst-stage retrieval in a production\ncommercial search engine. 145 Changing the training scheme of the dense retrieval model over\nto ANCE yielded ofﬂine gains of 16% on a corpus of 8 billion documents using 64-dimensional\nrepresentations with approximate nearest neighbor search. The authors were rather vague about the\nexact experimental settings, but it does appear that ANCE yields demonstrable gains in “real world”\nretrieval scenarios.\nTakeaway Lessons. Building on Sentence-BERT, we presented DPR and ANCE as two canonical\nexamples of a bi-encoder design speciﬁcally applied to dense retrieval. DPR presents a simple\nyet effective approach to training encoders with in-batch negative sampling, and ANCE further\ndemonstrates the beneﬁts of picking “difﬁcult” negative examples. Together, they provide a good\nexploration of one key issue in the design of dense retrieval techniques—how do we select negative\nexamples, with respect to the comparison function φ, that maximizes the similarity between queries\nand relevant documents and minimizes the similarity between queries and non-relevant documents?\nIn terms of the “bottom line”, empirical results from DPR and ANCE suggest that while bi-encoders\nfor dense retrieval based on simple inner-product comparisons are not as effective as cross-encoders,\nthey are generally more effective than sparse retrieval (e.g., BM25). Since in a bi-encoder we lose\nattention-based interactions between queries and texts from the corpus, this effectiveness degradation\nis to be expected. However, the beneﬁt of bi-encoders is the ability to perform ranking directly on\nprecomputed representations of texts from the corpus, in contrast to a retrieve-and-rerank architecture\nwith cross-encoders. Finally, there appear to be synergies between dense and sparse retrieval, as\ncombining evidence in dense–sparse hybrids usually leads to higher effectiveness than dense retrieval\n(or sparse retrieval) alone.\n5.4.3 Bi-encoders for Dense Retrieval: Additional Variations\nRoughly contemporaneously with DPR and ANCE, there was a ﬂurry of activity exploring bi-encoders\nfor dense retrieval during the Spring and Summer of 2020. In this section, we discuss some of these\nmodel variants. We emphasize that it is not our intention to exhaustively survey every proposed\nmodel, but rather to focus on variations that help us better understand the impact of different design\nchoices. The MS MARCO passage ranking task provides a common point of comparison: results are\nsummarized in Table 42, with ﬁgures copied from the original papers. For convenience, we repeat the\nBM25, monoBERT, and ANCE conditions from Table 39.\nCLEAR, short for “Complementing Lexical Retrieval with Semantic Residual Embedding” [Gao\net al., 2021c], was ﬁrst proposed in March 2020 [Gao et al., 2020d] and can be described as a\njointly-trained sparse–dense hybrid. Unlike DPR, where the dense retrieval component was trained\nin isolation and then combined with sparse retrieval results (BM25) using linear combination, the\nintuition behind CLEAR is to exploit a bi-encoder to capture semantic matching absent in the lexical\nmodel (BM25), instead of having the dense retrieval model “relearn” aspects of lexical matching.\nThus, “residual” in CLEAR refers to the goal of using the bi-encoder to “ﬁx” what BM25 gets wrong.\nLike ANCE but unlike DPR, CLEAR uses the same encoder (i.e., η) for both queries and texts from\nthe corpus. However, before the usual [CLS] token, another special token, either <QRY> or <DOC>,\nis prepended to indicate the query or document, respectively. The ﬁnal vector representation is\nproduced by average pooling the output contextual representations. The dense retrieval score (i.e., the\nφfunction) is computed as the inner product between encoder outputs. As CLEAR is a sparse–dense\nhybrid, the ﬁnal relevance score is computed by a linear combination of the lexical retrieval score\n(produced by BM25) and the dense retrieval score.\nCLEAR is trained using a pairwise hinge loss to maximize the similarity between a given query q\nand a relevant document d+ while minimizing the similarity between the query and a non-relevant\ndocument d−subject to a minimum margin:\nL(q,d+,d−) = max(0,m −s(q,d+) + s(q,d−)) (70)\nHowever, instead of using a ﬁxed margin (e.g., settingm= 1 for all training triples),mis dynamically\ncomputed based on the BM25 scores of the relevant and non-relevant documents, along with two\n145Since the authors reported Microsoft afﬁliations, presumably this refers to Bing.\n148\nMS MARCO Passage(Dev)\nMethod MRR@10 Recall@1k\n(1a) BM25 (Microsoft Baseline) 0.167 -\n(1b) BM25 + monoBERTLarge 0.365 -\n(1c) BM25 + monoBERTBase 0.347 -\n(2a) ANCE 0.330 0.959\n(2b) DR w/ in-batch 0.261 0.949\n(2c) DR w/ BM25 0.299 0.928\n(2d) DR w/ in-batch + BM25 (≈DPR) 0.311 0.952\n(3a) CLEAR (full model) 0.338 0.969\n(3b) CLEAR, dense only 0.308 0.928\n(3c) CLEAR, random negatives 0.241 0.926\n(3d) CLEAR, constant margin 0.314 0.955\n(4a) RocketQA (batch size = 4096) + DNS + DA 0.370 -\n(4b) RocketQA (batch size = 4096) 0.364 -\n(4c) RocketQA (batch size = 128) 0.310 -\n(5a) STAR (≈ANCE) 0.340 -\n(5b) STAR + ADORE 0.347 -\nTable 42: The effectiveness of various bi-encoder models on the development set of the MS MARCO\npassage ranking test collection.\nparameters, cand λ:\nm(q,d+,d−) = c−λ·\n(\nBM25(q,d+) −BM25(q,d−)\n)\n(71)\nThis is where the notion of “Semantic Residual Embedding” in CLEAR is operationalized. Because\nlittle loss is incurred when BM25 is able to accurately identify the relevant document, the dense\nretrieval model is steered to focus on cases where lexical matching fails. During training, negative\nexamples are selected from the non-relevant texts retrieved by BM25.\nResults from CLEAR are shown in row group (3) of Table 42, copied from Gao et al. [2021c]. The\neffectiveness of the full CLEAR model is reported in row (3a). Although it appears to be more\neffective than ANCE, row (2a), this is not a fair comparison because CLEAR is a sparse–dense hybrid\nwhile ANCE relies on dense retrieval only. Xiong et al. [2021] did not evaluate hybrid combinations\nof dense and sparse retrieval, but the DPR experiments of Ma et al. [2021c] suggest that dense–sparse\nhybrids are more effective than dense retrieval alone. Fortunately, Gao et al. [2021c] reported results\nfrom an ablation condition of CLEAR with only dense retrieval, shown in row (3b). This result\nsuggests that when considering only the quality of the learned dense representation, ACNE appears\nto be more effective. However, it is not clear exactly what characteristics of the approaches are\nresponsible for this effectiveness gap, since there are many differences between the two.\nAdditionally, rows (3c) and (3d) in Table 42 present ablation analyses on the full CLEAR model\n(which includes both dense and sparse components). In row (3c), the error-based negative samples\nwere replaced with random negative samples, and in row (3d), the residual margin in the loss function\nwas replaced with a constant margin, which is equivalent to the fusion of BM25 results and the\nresults in row (3b). These ablation conditions illustrate the contributions of the two main ideas behind\nCLEAR: training on “mistakenly-retrieved” texts from lexical retrieval improves effectiveness in\na sparse–dense fusion setting, as does coaxing the bi-encoder to compensate for lexical retrieval\nfailures via residual margins.\nRocketQA [Qu et al., 2021] is a dense retrieval technique that further investigates DPR’s in-batch\nnegative sampling method by pushing its technical limits to answer the question: What would happen\nif we just continued to increase the batch size? The answer is shown in row (4b) of Table 42, with a\nbatch size of 4096. For reference, row (4c) shows the effectiveness of a more “typical” batch size\nof 128, which is consistent with other dense retrieval models. Qu et al. [2021] also proposed two\nother innovations: using a cross-encoder to remove top-retrieved passages that are likely to be false\nnegatives during sampling (what they called “denoised negative sampling”) and data augmentation\nusing high-conﬁdence automatically labeled examples from a cross-encoder. Experimental results\nsuggest, however, that increasing the batch size has the largest beneﬁt to effectiveness. The full model,\nwith denoised negative sampling (= DNS) and data augmentation (= DA) achieves an MRR@10 of\n149\n0.370, shown in row (4a). To our knowledge, this is the best single (i.e., non-fusion, non-ensemble)\ndense retrieval result reported on the development set of MS MARCO passage ranking task.\nAnother proposed dense retrieval model is the work of Zhan et al. [2020a] (later published as Zhan\net al. [2021]), which extends ANCE to additionally ﬁne-tune the query encoder ηq. Recall that\nin ANCE, the same encoder is used for both the query and texts from the corpus (i.e., ηd = ηq).\nWith their technique called ADORE (Algorithm for Directly Optimizing Ranking pErformance),\nthe authors demonstrated that additional ﬁne-tuning of the query encoder ηq (but ﬁxing the passage\nencoder ηq after a training regime similar to ANCE where the same encoder is used for both in the\ninitial stages) can further increase retrieval effectiveness. For details, we refer the reader to Zhan\net al. [2021], but summarize key results here. Their baseline technique, called STAR (Stable Training\nAlgorithm for dense Retrieval), is shown in row (5a) of Table 42. It can be characterized as a variant\nof ANCE and achieves a slightly higher level of effectiveness. Further ﬁne-tuning of the query\nencoder with ADORE, shown in row (5b), leads to another modest increase in effectiveness.\nSo far, all of the bi-encoder designs we’ve discussed adopt BERT (or a closely related variant such\nas RoBERTa) as the base model of their encoders (i.e.,η). This, however, need not be the case. For\nexample, the BISON [Shan et al., 2020] (“BM25-weighted Self-Attention Framework”) bi-encoder\nmodel follows a similar approach to ANCE. However, rather than building the encoder using BERT,\nBISON uses a stack of modiﬁed “BISON encoder layers” that are trained directly on Bing query log\ndata. This is best described as a transformer encoder variant in which self-attention computations\nare weighted by term importance, calculated using a variant of tf–idf. The model is trained with a\nstandard cross-entropy loss. Unfortunately, BISON was not evaluated on the MS MARCO passage\nranking task, and thus a comparison to the techniques in Table 42 is not possible.\nThe ﬁnal bi-encoder variant we cover in this section is the work of Yang et al. [2020b], who considered\nthe problem of matching long texts (e.g., using entire documents both as the query and the texts to be\nsearched). They introduced MatchBERT, which can be characterized as a Sentence-BERT variant,\nas a building block in their hierarchical SMITH model. SMITH, short for “Siamese Multi-depth\nTransformer-based Hierarchical Encoder”, creates sentence-level representations with a stack of\ntwo transformer encoder layers; a stack of three transformer encoder layers converts these sentence\nrepresentations into a document representation, which is the output of η. Document representations\nare then compared with cosine similarity. As there are no common points of comparison between this\nwork and the others discussed above, we do not present results here.\nTakeaway Lessons. Beyond DPR and ANCE, which in our opinion are the two most representative\ndense retrieval techniques, there are many possible variations in bi-encoder designs. For the most\npart, these different design choices have only a modest impact on effectiveness, which taken together,\ncan be considered a series of independent replication studies on dense retrieval methods.\n5.5 Enhanced Transformer Bi-encoders for Ranking\nIn the “simple” bi-encoder designs discussed above, the representation vectors derived from the\nencoders ηq and ηdare compared using a simple operation such as inner product. Top-kranking in this\ncontext can be recast as nearest neighbor search, with efﬁcient off-the-shelf solutions (see Section 5.2).\nWhile usually much faster (can be orders of magnitude compared to reranking), bi-encoders are less\neffective than cross-encoder rerankers because the latter can exploit relevance signals derived from\nattention between the query and candidate texts at each transformer encoder layer. Thus, the tradeoff\nwith bi-encoders is invariably sacriﬁcing effectiveness for efﬁciency gains.\nAre different tradeoffs possible? For example, could we enhance φto better capture the complexities\nof relevance (perhaps in conjunction with the design of the encoders) to increase effectiveness at some\nacceptable loss in efﬁciency? The design of φ, however, is constrained by current nearest neighbor\nsearch techniques if we wish to take advantage of off-the-shelf libraries to perform ranking directly.\nPut differently, the transformation of dense retrieval into a nearest neighbor search problem that can\nbe tackled at scale critically depends on the choice of φ—using commonly available techniques today,\ndense retrieval is only possible for a small family of comparison functions such as inner product.\nAlternatively, researchers would need to build custom nearest neighbor search capabilities from\nscratch to support a speciﬁc comparison function. Therein lies the challenge.\nThe PreTTR (Precomputing Transformer Term Representations) model [MacAvaney et al., 2020c]\nillustrates a hybrid design between a bi-encoder and a cross-encoder. Starting with monoBERT, the\n150\nauthors modiﬁed the all-to-all attention patterns of BERT to eliminate attention between the query\nand the candidate text. That is, terms in the candidate text cannot attend to terms in the query, and\nvice versa; this is accomplished by a mask. If this mask is applied to all the layers in BERT, we\nhave essentially “cleaved” monoBERT into disconnected networks for the query and the candidate\ntext. In this case, the representations of the candidate texts (i.e., all texts from the corpus) can be\nprecomputed, and the overall design is essentially a bi-encoder. However, the attention mask can be\napplied to only some of the transformer encoder layers. Suppose we apply it to all but the ﬁnal layer:\nthis means that the representation of the candidate text just before the ﬁnal transformer encoder layer\ncan be precomputed. At inference time, the model can look up the precomputed representation and\nonly needs to apply inference with the ﬁnal layer; inference on the query, however, needs to proceed\nthrough all the layers. Since the candidate texts are usually much longer than the queries, this yields\nlarge savings in inference latency. By controlling the number of layers the attention mask is applied\nto, it is possible to trade effectiveness for efﬁciency.\nExplained in terms of our framework, in PreTTR, the choice ofφis the “upper layers” of a monoBERT\nmodel, while ηd for texts from the corpus comes from the “lower layers” of the same monoBERT\nmodel (via attention masking). Contemporaneously, Gao et al. [2020a] had similar intuitions as well,\nand later, Gao et al. [2020b] as well as Chen et al. [2020] elaborated on these ideas, where encoders\ngenerate multiple embeddings that are then fed to a second transformer “head” to compute relevance\nscores. While these papers illustrate hybrid models that lie between bi-encoders and cross-encoders,\ntheir designs remain mostly tied to a reranking setup, with candidate texts coming from a ﬁrst-stage\nretrieval technique (presumably based on keyword search).\nThere is, however, a path forward. In the previous section, we deﬁned “simple” bi-encoders as a class\nof techniques, where (1)ηq and ηdproduce ﬁxed-width vectors, and (2)φis a simple operation such as\ninner product. As it turns out, both constraints can be relaxed. Researchers have explored approaches\nthat represent each text from the corpus with multiple representation vectors: In Section 5.5.1, we\ndiscuss poly-encoders and ME-BERT, which operationalized this intuition in different ways. In\nSection 5.5.2, we describe ColBERT, which took this idea to what might be considered the logical\nextreme—by generating, storing, and comparing per token representations with a richer comparison\nfunction φthat is amenable to existing nearest neighbor search libraries.\n5.5.1 Multiple Text Representations: Poly-encoders and ME-BERT\nAs discussed in Section 5.4, Humeau et al. [2020] were, to our knowledge, the ﬁrst to have proposed\nsuccessful neural architectures for ranking using transformer-based dense representations. In fact,\nthey introduced the bi-encoder and cross-encoder terminology that we have adopted in this survey as\nbaselines for their proposed innovation, called the poly-encoder model.\nThe poly-encoder model aimed to improve the effectiveness of bi-encoders at the cost of a (modest)\ndecrease in efﬁciency, using a comparison function φthat takes advantage of multiple representations\nof texts from the corpus.146 In contrast to bi-encoders, where ηd converts a text from the corpus into\na single ﬁxed-width vector, poly-encoders generate mvector representations by learning m“context\ncodes” that “view” a text from the corpus in different ways.\nAt search (query) time, these mrepresentations are aggregated into a single vector via an attention\nmechanism with the query vector. The ﬁnal ranking score is computed via an inner product between\nthe query vector and this aggregated vector. In other words, φremains deﬁned in terms of inner\nproducts, but the mrepresentations of texts from the corpus are given an opportunity to interact with\nthe query vector before the ﬁnal score computation.\nHumeau et al. [2020] compared poly-encoders with bi-encoders and cross-encoders in the context\nof response selection, which is the task of retrieving appropriate responses to an utterance in a\nconversation [Lowe et al., 2015, Yoshino et al., 2019, Dinan et al., 2019]. That is, conversational\nutterances serve as queries and the model’s task is to identify the most appropriate piece of text to\n“say next”. While this task differs fromad hoc retrieval, it is nevertheless a retrieval task. We omit\nresults from their paper here since few of the other techniques presented in this survey use those\ndatasets, and thus there is little context for meaningful comparisons.\n146Confusingly, Humeau et al. [2020] called their query the “candidate” and a text from the corpus a “context”;\nhere, we have translated their terminology into the terminology used in this survey.\n151\nMS MARCO Passage(Dev) MS MARCO Doc(Dev)\nMethod MRR@10 MRR@100\n(1) BM25 (Anserini, top 1000) 0.187 0.209\n(2) DR w/ in-batch + BM25 = Table 39, row (3d) 0.311 -\n(3a) DE-BERT 0.302 0.288\n(3b) ME-BERT 0.334 0.333\n(3c) BM25 + DE-BERT 0.309 0.315\n(3d) BM25 + ME-BERT 0.343 0.339\nTable 43: The effectiveness of ME-BERT on the development set of the MS MARCO passage ranking\ntest collection.\nUnfortunately, Humeau et al. [2020] did not integrate poly-encoders with nearest neighbor search\ntechniques to perform end-to-end retrieval experiments. Their evaluation of efﬁciency only included\nreports of inference latency over ﬁxed sets of candidates from their datasets.147 In this limited setting,\nthe experimental results showed that poly-encoders were more effective than bi-encoders and more\nefﬁcient than cross-encoders.\nOther researchers have explored the idea of using multiple representations for dense retrieval. Luan\net al. [2021] proposed the ME-BERT (Multi-Vector Encoding from BERT) model, where instead\nof generating a single representation for each text from the corpus, mrepresentations are produced\nby the encoder (m = 8 is a typical value). The proposed technique for generating these different\nrepresentations is quite simple: take the contextual representations of the ﬁrst mtokens from BERT\noutput as the mrepresentations. That is, if m= 1, the text would be represented by the contextual\nrepresentation of the [CLS] token (much like DPR); if m= 2, additionally include the contextual\nrepresentation of the ﬁrst token in the text; if m= 3, the contextual representation of the ﬁrst and\nsecond tokens, and so on.\nAt search (query) time, the score between the query and a text from the corpus is simply the largest\ninner product between the query and any of these mrepresentations. Since the comparison function\nφremains the inner product, this operation can be efﬁciently implemented with standard nearest\nneighbor search techniques by simply adding mentries for each text from the corpus to the index.\nAdditionally, Luan et al. [2021] combined the results of dense retrieval with sparse retrieval (i.e.,\nBM25) using a linear combination of scores to arrive at dense–sparse hybrids; this is similar to\nDPR [Karpukhin et al., 2020b].\nThe ME-BERT model was trained with a combination of sampled negatives from precomputed BM25\nresults as well as in-batch negatives, similar to DPR, but using cross-entropy loss instead of DPR’s\ncontrastive loss. In addition, one round of hard negative mining was applied in some settings. We\nrefer interested readers to the original paper for details.\nExperimental results on the development set of the MS MARCO passage and document ranking\ntasks, copied from Luan et al. [2021], are shown Table 43. These models were trained on the training\nsplits of the respective MS MARCO datasets. To provide some historical context for interpreting\nthese results, the original arXiv paper that proposed ME-BERT [Luan et al., 2020] was roughly\ncontemporaneous with DPR and predated ANCE. The peer-reviewed version of the paper was not\npublished until nearly a year later, and during this gap, innovations in dense retrieval continued.\nThe effectiveness of the bi-encoder baseline (called DE-BERT) from Luan et al. [2021], where each\ntext from the corpus is represented by a single vector, is shown in row (3a). The closest comparison\nwe have to another paper is in the context of ANCE ablation experiments, corresponding to row (3d)\nin Table 39, repeated in Table 43 as row (2); recall that DPR was not evaluated on MS MARCO data.\nWhile training details differ (e.g., loss function, hyperparameters, etc.), the MRR@10 scores on the\ndevelopment set of the MS MARCO passage ranking test collection are comparable, which offers\nindependent veriﬁcation of the effectiveness of single-vector dense retrieval approaches in general.\nAs expected, the multi-representation ME-BERT approach outperforms the single-representation\nDE-BERT baseline, row (3b) vs. (3a). There is, however, an associated efﬁciency cost (query latency\nand larger indexes); Luan et al. [2021] reported these tradeoffs in graph, and thus it is not easy to\n147This aspect of experimental design was not clear from the paper, but our interpretation was conﬁrmed via\npersonal communications with the authors.\n152\nprovide a concise summary of their results, so we refer interested readers directly to the paper for\ndetails. Furthermore, it is not surprising that dense–sparse hybrids are more effective than dense\nretrieval alone, with both ME-BERT and DE-BERT. This is shown in (3c) vs. (3a) and (3d) vs. (3b),\nand the ﬁnding is consistent with results from DPR and elsewhere. While the results of Luan et\nal. demonstrated the effectiveness of multi-vector representational approaches, the effectiveness of\nME-BERT appears to lag behind other dense retrieval techniques in absolute terms. For example,\nthe full ANCE model is comparable in effectiveness to ME-BERT while only requiring a single\nrepresentation vector per text from the corpus; RocketQA [Qu et al., 2021] also achieves higher\neffectiveness with a single vector representation.\nTakeaway Lessons. If individual vectors are not sufﬁcient to represent texts from the corpus for\ndense retrieval, then why not use multiple vectors? This appears to be a simple method to improve\nthe effectiveness of dense retrieval while retaining compatibility with off-the-shelf nearest neighbor\nsearch techniques. Researchers have only begun to investigate this general approach, and there\nappears to be a lot of room for further innovations.\n5.5.2 Per-Token Representations and Late Interactions: ColBERT\nIf generating multiple representations from each text from the corpus is a promising approach, then\nwhy not take it to the logical extreme and generate a dense vector representationfor each token? This,\nin fact, is what Khattab and Zaharia [2020] accomplished with their ColBERT model! The authors’\ncore contribution is a clever formulation of the comparison function φthat supports rich interactions\nbetween terms in the query and terms in the texts from the corpus in a manner that is compatible\nwith existing nearest neighbor search techniques. This approach, called “late interactions”, explicitly\ncontrasts with the all-to-all interactions at each transformer layer in the standard cross-encoder design.\nWith ColBERT, Khattab and Zaharia [2020], demonstrated that ranking methods based on dense\nrepresentations can achieve levels of effectiveness that are competitive with a cross-encoder design,\nbut at a fraction of the query latency. While still slower than pre-BERT neural models, ColBERT\nsubstantially narrows the gap in term of query-time performance.\nMore formally, given a text tconsisting of a sequence of tokens [t1,...,t n], ColBERT computes a\nmatrix η([t1,...,t n]) ∈Rn×D, where nis the number of tokens in the text and Dis the dimension of\neach token representation. In other words, the output of the ηencoder is a matrix, not just a vector.\nColBERT uses the same BERT model to encode queries and texts from the corpus; to distinguish\nthem, however, a special token [Q] is prepended to queries and another special token [D] to texts\nfrom the corpus. As with other dense retrieval techniques, the corpus representations can be computed\nofﬂine since they do not depend on the query.\nTo control the vector dimension D, a linear layer without activation is added on top of the last\nlayer of the BERT encoder. This reduces the storage and hence memory requirements of the token\nrepresentations, which is an issue for low-latency similarity comparisons (more discussion of this\nlater). Additionally, the vector representation of each token is normalized to a unitary L2 norm; this\nmakes computing inner products equivalent to computing cosine similarity.\nAt search (query) time, a query qwith terms [q1,...,q m] is converted to η([q1,...,q m]) ∈Rm×D. A\nsimilarity (relevance) score sq,d is computed for each text dfrom the corpus as follows:\nsq,d =\n∑\ni∈η(q)\nmax\nj∈η(d)\nη(q)i ·η(d)j, (72)\nwhere η(t)i is the vector representing the i-th token of the text t(either the query or a text from the\ncorpus). Since each of these vectors has unit length, the similarity is the sum of maximum cosine\nsimilarities between each query term and the “best” matching term contained in the text from the\ncorpus; the authors called this the “MaxSim” operator. 148 The scoring function described above\nassumes that relevance scores are computed over all texts from the corpus; retrieving the topkcan\naccomplished by sorting the results in decreasing order according to sq,d.\n148An alternative way of explaining MaxSim is that the operator constructs a similarity matrix, performs\nmax pooling along the query dimension, followed by a summation to arrive at the relevance score. Such\na description establishes obvious connections to pre-BERT interaction-based neural ranking models (see\nSection 1.2.4).\n153\nTo directly perform top-kranking against all texts in a large corpus, ColBERT adopts an efﬁcient\ntwo-stage retrieval method, since a brute-force computation of the similarity values sq,d,∀d∈C is\nnot practical. As a preprocessing step, the representation of each token from the corpus is indexed\nusing Facebook’s Faiss library for nearest neighbor search [Johnson et al., 2017], where each vector\nretains a pointer back to its source (i.e., the text from the corpus that contains it). At query time,\nranking proceeds as follows:\n1. In the ﬁrst stage, each query term embedding η(q)i is issued concurrently as a query and the top\nk′texts from the corpus are retrieved (e.g.,k′= k/2), by following the pointer of each retrieved\nterm vector back to its source. The total number of candidate texts is thus m×k′(where mis\nthe number of query terms), with K ≤m×k′of those being unique. The intuition is that these\nKdocuments are likely to be relevant to the query because representations of their constituent\ntokens are highly similar to at least one of the query tokens.\n2. In the second stage, these Kcandidate texts gathered in the manner described above are scored\nusing all query token representations according to the MaxSim operator in Eq. (72).\nAs an additional optimization, ColBERT takes advantage of a cluster-based feature inside Faiss to\nincrease the efﬁciency of the vector searches.\nSomewhat ironic here is that in order for ColBERT to scale to real-world corpora, a multi-stage\narchitecture is required, which breaks the elegance of single-stage ranking with nearest neighbor\nsearch based on bi-encoders. In effect, the authors have replaced ﬁrst-stage retrieval using an inverted\nindex with ﬁrst-stage retrieval using a nearest neighbor search library followed by MaxSim reranking\n(which is much more lightweight than a transformer-based reranker).\nThe ColBERT model is trained end-to-end using the following loss:\nL(q,d+,d−) = −log esq,d+\nesq,d+ + esq,d− , (73)\nwhere d+ and d−are relevant and non-relevant documents to the query q, respectively. The non-\nrelevant documents are directly taken from the training data in triples format.\nAn additional trick used by ColBERT is to append [MASK] tokens to queries that are shorter than a\npredeﬁned length. According to the authors, this provides a form of query augmentation, since these\nextra tokens allow the model to learn to expand queries with new terms or to reweight existing terms\nbased on their importance to matching texts from the corpus.\nKhattab and Zaharia [2020] evaluated ColBERT on the development set of the MS MARCO passage\nranking test collection, which enables a fair comparison to the other techniques presented in this sur-\nvey. Results from their paper are presented in Table 44. Latency measurements were performed on an\nNVIDIA V100 GPU. Row (1a) and (1b) report the standard BM25 baseline and with monoBERTLarge\nreranking, respectively. Row (2) copies the author’s report of FastText + ConvKNRM, which can\nbe characterized as a competitive pre-BERT neural ranking model. Row (3) reports the result of\ndoc2query–T5. Row (4) reports effectiveness and query latency ﬁgures for ColBERT. We see that\nColBERT approaches the effectiveness of monoBERTLarge, row (1b), in terms of MRR@10 but is\napproximately 70×faster on a modern GPU. While ColBERT is more effective than doc2query–T5\nand ConvKNRM, it is still 5 ×slower. Note that ConvKNRM is evaluated on a GPU, whereas\ndoc2query–T5 runs on a CPU.\nTo summarize, results show that in terms of query latency, ColBERT has indeed closed much of the\ngap between monoBERT and pre-BERT neural ranking models. It is able to accomplish this with only\nmodest degradation in effectiveness compared to monoBERT reranking. However, although more\neffective, ColBERT is still many times slower than pre-BERT neural models and doc2query. Never-\ntheless, these results show that ColBERT represents a compelling point in the effectiveness/efﬁciency\ntradeoff space. However, in terms of multi-stage architectures, monoBERTLarge is only a baseline.\nThere exist even more effective reranking models, for example, duoBERT (see Section 3.4.1), and\nthe top leaderboard entries for the MS MARCO passage ranking task now report MRR@10 above\n0.400; for example, [Qu et al., 2021]. Thus, dense retrieval techniques by themselves still have a\nways to catch up to the effectiveness of the best multi-stage reranking pipelines. However, they can\nand are being used as replacements of ﬁrst-stage retrieval based on sparse (keyword) search to feed\ndownstream rerankers, for example, see Qu et al. [2021] and Hofstätter et al. [2021].\n154\nMS MARCO Passage(Dev)\nDevelopment Latency\nMethod MRR@10 Recall@1k (ms)\n(1a) BM25 (Anserini, top 1000) 0.187 0.861 62\n(1b) + monoBERT Large 0.374 0.861 32,900\n(2) FastText + ConvKNRM 0.290 - 90\n(3) doc2query–T5 0.277 0.947 87\n(4) ColBERT (with BERTBase) 0.360 0.968 458\nTable 44: The effectiveness of ColBERT on the development set of the MS MARCO passage ranking\ntest collection. Query latencies for ColBERT and monoBERTLarge are measured on a V100 GPU.\nFinally, there is one major drawback of ColBERT: the space needed to store the per-token representa-\ntions of texts from the corpus. For example, the MS MARCO passage corpus contains 8.8M passages.\nTo illustrate using round numbers, suppose that each passage has on average 50 tokens, each token\nis represented by a 128-dimensional vector, and we use 4 bytes to encode each dimension. We\nwould need 8.8M passages ×50 tokens ×128 dim ×4 bytes ∼225 GB of space! This accounting\nrepresents only the space required to store the raw representation vectors and does not include the\noverhead of index structures to facilitate efﬁcient querying. In practice, however, space usage can be\nreduced by using fewer bits to represent each dimension and by compressing the vectors. Khattab and\nZaharia reported that “only” 156 GB is required to store their index due to some of these optimizations.\nNevertheless, this is still orders of magnitude larger than the 661 MB required by the bag-of-words\nindex of the same collection with Lucene (see more discussions in Section 5.7). Since Faiss loads all\nindex data into RAM to support efﬁcient querying, we are trading off the cost of neural inference\nfor reranking (e.g., using GPUs) against the cost of large amounts of memory to support efﬁcient\nnearest neighbor search. We can imagine that these large memory requirements make ColBERT less\nattractive, and perhaps even impractical, for certain applications, particularly on large corpora.\nTakeaway Lessons. The design of bi-encoders and cross-encoders lie at opposite ends of the\nspectrum in terms of the richness of interaction between queries and texts from the corpus. Multi-\nvector approaches can preserve some level of interaction while remaining amenable to efﬁcient\nretrieval. Speciﬁcally, ColBERT’s MaxSim operator supports rich token-level “late interactions”\nin a manner that remains compatible with efﬁcient nearest neighbor search capabilities provided\nby existing libraries. The result is a “single-stage” dense retrieval technique whose effectiveness\napproaches monoBERT reranking, but at a fraction of the query latency.\n5.6 Knowledge Distillation for Transformer Bi-encoders\nDistillation methods are commonly used to decrease model size, thus reducing overall inference costs,\nincluding memory requirements as well as inference latency. As we’ve seen in Section 3.5.1, this is\ndesirable for reranking models, where inference needs to be applied over all candidate texts from\nﬁrst-stage retrieval.\nOne might wonder, why would knowledge distillation be desirable for training dense retrieval models?\nAfter all, the advantages of smaller and faster models are less compelling in the dense retrieval\nsetting, as applying inference over the entire corpus with a particular encoder can be considered a\npreprocessing step that is easy to parallelize.149 Nevertheless, there is a thread of research focused\non distilling “more powerful” cross-encoders into “less powerful” bi-encoders. Empirically, this\ntwo-step procedure seems to be more effective than directly training a bi-encoder; this ﬁnding appears\nto be consistent with reranker distillation results presented in Section 3.5.1.\nTo our knowledge, Lu et al. [2020] was the ﬁrst to apply distillation in the dense retrieval context.\nHowever, their work can be characterized as ﬁrst training a bi-encoder with BERT, and then distilling\ninto smaller encoder models—which is fundamentally different from the techniques that followed.\nFurthermore, the authors’ proposed TwinBERT model was not evaluated on public datasets, and thus\nthere is no way to compare its effectiveness to other techniques.\n149Although, admittedly, at “web scale” (i.e., for commercial web search engines), applying inference over the\nentire collection would still be quite costly.\n155\nMS MARCO Passage(Dev)\nMethod MRR@10 Recall@1k\n(1a) DistilBERTdotMargin-MSE w/ ensemble teacher 0.323 0.957\n(1b) DistilBERTdotwo/ distillation 0.299 0.930\n(2a) TCT-ColBERT (v1) 0.335 0.964\n(2b) TCT-ColBERT (v1) + BM25 0.352 0.970\n(2c) TCT-ColBERT (v1) + doc2query–T5 0.364 0.973\n(3a) TCT-ColBERT w/ HN+ (v2) 0.359 0.970\n(3b) TCT-ColBERT w/ HN+ (v2) + BM25 0.369 -\n(3c) TCT-ColBERT w/ HN+ (v2) + doc2query–T5 0.375 -\n(4a) DistilBERTdotTAS-Balanced 0.347 0.978\n(4b) DistilBERTdotTAS-Balanced + doc2query–T5 0.360 0.979\nTable 45: The effectiveness of various bi-encoder models trained with knowledge distillation on the\ndevelopment set of the MS MARCO passage ranking test collection.\nThe ﬁrst instance of distilling cross-encoders into bi-encoders that we are aware of is by Hofstätter\net al. [2020]. Their work established a three-step procedure that provides a reference point for this\nthread of research:\n1. Standard (query, relevant text, non-relevant text) training triples, for example, from the MS\nMARCO passage ranking test collection, are used to ﬁne-tune a teacher model (in this case, a\ncross-encoder).\n2. The teacher model is then used to score all the training triples, in essence generating a new\ntraining set.\n3. The training triples with the teacher scores are used to train a student model (in this case, a\nbi-encoder based on DistilBERT) via standard knowledge distillation techniques.\nNote that the inference required in step (2) only needs to be performed once and can be cached as\nstatic data for use in step (3). A noteworthy aspect of this procedure is that relevance labels are\nnot explicitly used in the training of the student model. Knowledge distillation is performed by\noptimizing the margin between the scores of relevant and non-relevant texts with respect to a query.\nConcretely, this is accomplished by what Hofstätter et al. [2020] calls Margin Mean Squared Error\n(Margin-MSE). Given a training triple comprised of the query q, relevant text d+, and non-relevant\ntext d−, the output margin of the teacher model is used to optimize the student model as follows:\nL(q,d+,d−) = MSE(Ms(q,d+) −Ms(q,d−),Mt(q,d+) −Mt(q,d−)), (74)\nwhere Ms(q,d) and Mt(q,d) are the scores from the student model and teacher model for d, respec-\ntively. MSE is the standard Mean Squared Error loss function between scores Sand targets T across\neach training batch:\nMSE(S,T) = 1\n|S|\n∑\ns∈S,t∈T\n(s−t)2 (75)\nAnother nice property of this setup is support for distilling knowledge from multiple teacher models\nvia ensembles.\nPutting all these elements together, effectiveness on the development set of the MS MARCO passage\nranking test collection is shown in row (1a) of Table 45, copied from Hofstätter et al. [2020].\nThis condition used Margin-MSE loss, DistilBERT as the student model, and a teacher ensemble\ncomprising three cross-encoders; the subscript “dot” is used by the authors to indicate a bi-encoder\nmodel. The same DistilBERTdot model trained without knowledge distillation is shown in row (1b),\nwhich exhibits lower effectiveness. This ﬁnding supports the idea that distilling from more powerful\nmodels (cross-encoders) into less powerful models (bi-encoders) is more effective than training\nless powerful models (bi-encoders) directly. Hofstätter et al. [2020] performed additional ablation\nanalyses and contrastive experiments examining the impact of different loss functions and teacher\nmodels; we direct readers to their paper for details.\nAs a point of contrast, Lin et al. [2020b] approached distillation in a different manner. Note that in\nstep (2) from Hofstätter et al. [2020], teacher scores are precomputed and stored; herein lies the key\n156\ndifference. The main idea of Lin et al. is to use in-batch negatives whose soft-labels are computed by\na fast teacher model on the ﬂy during knowledge distillation. Due to high inference costs, a teacher\nmodel based on a BERT cross-encoder would be impractical for this role, but ColBERT is both\nsufﬁciently efﬁcient and effective to serve as the teacher model in this design. The authors called\nthis model TCT-ColBERT, where TCT stands for “Tightly Coupled Teacher”. The student model is\ntrained with a loss function comprised of two terms: the ﬁrst term corresponds to the softmax cross\nentropy over relevance labels (thus, differing from Hofstätter et al. [2020], this approach does make\ndirect use of the original training data) and the second term captures the KL-divergence between the\nscore distributions of the teacher and student models with respect to all instances in the batch. We\nrefer readers to Lin et al. [2020b] for additional details.\nThe effectiveness of TCT-ColBERT (v1) on the development set of the MS MARCO passage ranking\ntest collection is shown in row (2a) of Table 45. The student model in this case was BERTBase, which\nwas the same as the teacher model, so we are distilling into a student model that is the same size as\nthe teacher model. However, the key here is that the cross-encoder is more effective, so we are still\ndistilling from a more powerful model into a less powerful model.\nWhile it appears that TCT-ColBERT (v1) achieves higher effectiveness than Hofstätter et al. [2020],\nthe comparison is not fair because TCT-ColBERT used a larger student model with more layers and\nmore parameters (BERTBase vs. DistilBERT). Nevertheless, the technique yields a bi-encoder on\npar with ANCE in terms of effectiveness (see Table 42). The dense retrieval model can be further\ncombined with sparse retrieval results, either bag-of-words BM25 or doc2query–T5; these conditions\nare shown in rows (2b) and (2c), respectively. As expected, dense–sparse hybrids are more effective\nthan dense retrieval alone.\nIn follow-up work, Lin et al. [2021b] further improved TCT-ColBERT in their “v2” model. The\nadditional trick, denoted as “HN+”, incorporates the hard-negative mining idea from ANCE, with the\nmain difference that ANCE’s negatives are dynamic (i.e., they change during training) while negatives\nfrom HN+ are static. An initially trained TCT-ColBERT model is used to encode the entire corpus,\nand new training triples are created by using hard negatives retrieved from these representations\n(replacing the BM25-based negatives). The ColBERT teacher is then ﬁne-tuned with this augmented\ntraining dataset (containing the hard negatives), and ﬁnally, the improved ColBERT teacher is distilled\ninto a bi-encoder student BERTBase model.\nThe effectiveness of this technique is shown in row (3a) of Table 45. We see that improvements from\nhard-negative mining are additive with the basic TCT-ColBERT design. Comparing with results in\nTable 42, the effectiveness of TCT-ColBERT w/ HN+ (v2) is second only to RocketQA; for reference,\nLin et al. [2021b] reported training with a modest batch size of 96, compared to 4096 for RocketQA.\nRows (3b) and (3c) report hybrid combinations of dense retrieval with BM25 and doc2query–T5,\nrespectively. We see that the model further beneﬁts from integration with sparse retrieval signals,\nparticularly with document expansion.\nAs a follow up to Hofstätter et al. [2020] and incorporating ideas from Lin et al. [2020b], Hofstätter\net al. [2021] focused on increasing the training efﬁciency of bi-encoder dense retrieval models via\ndistillation. Their main insight is that training batches assembled via random sampling (as is the\ntypical procedure) are likely to contain many low information training samples—for example, (query,\nnon-relevant text) pairs that are “too easy” and thus unhelpful in teaching the model to separate\nrelevant from non-relevant texts. As pointed out by Xiong et al. [2021], most in-batch negatives\nare uninformative because the sampled queries are very different, thus also making the constructed\ncontrastive pairs “too easy”. RocketQA gets around this with large batch sizes, thus increasing the\nlikelihood of obtaining informative training examples.\nRecognizing these issues, Hofstätter et al. [2021] proposed a more principled solution. The authors\nﬁrst clustered the training queries using k-means clustering (based on an initial bi-encoder). Instead\nof randomly selecting queries to form a batch, queries are sampled from the topic clusters so that\nthe contrastive examples are more informative: the authors called this topic-aware sampling (TAS).\nAs an additional reﬁnement, this sampling can be performed in a “balanced” manner to identify\nquery–passage pairs that range from “easy” to “difﬁcult’ (deﬁned in terms of the margin from the\nteacher model). Without balanced sampling, non-relevant passages would be over-represented since\nthey are more prevalent, once again, likely leading to uninformative training examples. Putting both\nthese ideas together, the authors arrived at the TAS-B (“B” for “Balanced”) technique. Beyond this\nhigh-level description, we refer readers to Hofstätter et al. [2021] for additional details.\n157\nResults of TAS-B on the development set of the MS MARCO passage ranking test collection are\nshown in row (4a) of Table 45, copied from Hofstätter et al. [2021]. In these experiments, DistilBERT\nserved as the student model and the teacher model was an ensemble comprised of a cross-encoder\nand ColBERT. Since the student models are the same, this result can be compared to row (1) from\nHofstätter et al. [2020]; however, comparisons to results in row groups (2) and (3) are not fair since\nTCT-ColBERT used BERTBase as the student (which has more layers and more parameters), and\nTAS-B uses an ensemble of cross-encoder and bi-encoder models as teachers. Nevertheless, we can\nsee that TAS-B improves upon the earlier distillation work of Hofstätter et al. [2020]. Furthermore,\nthe model is trainable on a single consumer-grade GPU in under 48 hours, compared to, for example,\nANCE and DPR, both of which were trained on 8×V100 GPUs. Beyond these speciﬁc experimental\nsettings, we note that TAS-B can be viewed as a general approach to constructing training batches,\nwhich is to some extent orthogonal to the dense retrieval model being trained. Although we are not\naware of any other applications of TAS-B, this would be interesting future work.\nTakeaway Lessons. All of the techniques surveyed in this section adopt a basic bi-encoder design\nfor the student models, similar to the models discussed in Section 5.4. However, instead of directly\ntraining the bi-encoder, distillation techniques are applied to transfer knowledge from more effective\nbut slower models (e.g., cross-encoders and ColBERT) into the bi-encoder. Empirically, this approach\nappears to be more effective: Setting aside RocketQA, which achieves its effectiveness through “brute\nforce” via large batch sizes and a cross-encoder to eliminate false negatives, the most effective dense\nretrieval models to date appear to be based on knowledge distillation. Nevertheless, it seems fair to\nsay that our understanding of the underlying mechanisms are incomplete.\nAs a starting point for future work, we end with this observation: The ﬁndings here appear to be\nconsistent with investigations of knowledge distillation in the context of reranking (see Section 3.5.1).\nIn both cases, distilling from a more powerful model into a less powerful model appears to be more\neffective than directly ﬁne-tuning a less powerful model. In the reranking context, since all the\ndesigns are based on cross-encoders, the “power” of the model is mostly a function of its size (number\nof layers, parameters, etc.). In the dense retrieval context, cross-encoders are clearly more “powerful”\nthan bi-encoders, even though the models themselves may be the same size. We believe that this is\nthe key insight, but more research is needed.\n5.7 Concluding Thoughts\nThere has been much excitement and progress in ranking with learned dense representations, which\nwe have covered in this section. Despite the potential of dense retrieval, there remain many challenges,\nwhich we discuss below:\nFirst, all dense retrieval models discussed in this section are trained in a supervised setting using\nhuman relevance judgments such as labels from the MS MARCO passage ranking test collection\n(either directly or indirectly via knowledge distillation). As with all supervised approaches, there’s\nthe important question of what happens when the model is presented with an out-of-distribution\nsample at inference time. In our case, this can mean that the encoder ηd for representing texts from\nthe corpus is presented with texts from a different domain, genre, etc. than what the model was\ntrained with, the query encoder ηq is fed queries that are different from the training queries, or both.\nFor example, what would happen if an encoder ηd trained with the MS MARCO passage ranking test\ncollection were applied to texts from the biomedical domain?\nIn fact, there is existing experimental evidence demonstrating that dense retrieval techniques are\noften ineffective in a zero-shot transfer setting to texts in different domains, different types of queries,\netc. Thakur et al. [2021] constructed a benchmark called BEIR by organizing over a dozen existing\ndatasets spanning diverse retrieval tasks in different domains into a single, uniﬁed framework. The\nauthors evaluated a number of dense retrieval techniques in a zero-shot setting and found that they\nwere overall less effective than BM25. In contrast to BM25, which generally “just works” regardless\nof the corpus and queries, dense retrieval models trained on MS MARCO data can lead to terrible\nresults when directly applied to other datasets. Addressing the generalizability of dense retrieval\ntechniques for “out of distribution” texts and queries is an important future area of research.\nSecond, dense retrieval techniques highlight another aspect of effectiveness/efﬁciency tradeoffs\nthat we have not paid much attention to. For the most part, our metrics of effectiveness are fairly\nstraightforward, such as those discussed in Section 2.5; there is literally decades of research in\n158\ninformation retrieval on evaluation metrics. In term of efﬁciency, we have mostly focused on query\nlatency. However, there is another aspect of efﬁciency that we have not seriously considered until\nnow—the size of the index structures necessary to support efﬁcient retrieval at scale. For inverted\nindexes to support, say, BM25 retrieval, the requirements are modest compared to the capabilities of\nservers today and not sufﬁciently noteworthy to merit explicit discussion.\nHowever, space becomes an important consideration with dense retrieval techniques. We present\nsome ﬁgures for comparison: A minimal Lucene index in Anserini, sufﬁcient to support bag-of-\nwords querying on the MS MARCO passage corpus (8.8M passages), only takes up 661 MB. 150\nA comparable HNSW index with 768-dimensional vectors in Faiss occupies 42 GB (with typical\nparameter settings), which is substantially larger. As reported in Section 5.5.2, Khattab and Zaharia\n[2020] reported that the comparable ColBERT index occupies 156 GB (since they need to store per\ntoken representations). These index sizes often translate into memory (RAM) requirements since\nmany existing nearest neighbor search libraries require memory-resident indexes to support efﬁcient\nquerying. Clearly, space is an aspect of performance (efﬁciency) that we need to consider when\nevaluating dense retrieval techniques. While researchers have begun to explore different techniques\nfor compressing dense representations, for example Izacard et al. [2020] and Yamada et al. [2021],\nthere is much more work to be done. Moving forward, we believe that an accurate characterization of\nthe tradeoff space of retrieval techniques must include quality (effectiveness of the results), time (i.e.,\nquery latency), as well as space (i.e., index size).\nThird, dense retrieval techniques today have largely sidestepped, but have not meaningfully addressed,\nthe length limitations of transformers. For the most part, the various techniques presented in this\nsection rely on encoders that are designed for processing relatively short segments of text—sentences,\nmaybe paragraphs, but deﬁnitely not full-length documents all at once. Luan et al. [2021] provided a\ntheoretical analysis on the relationship between document length and the representation vector size\nwith respect to ﬁdelity, which is their ability to preserve distinctions made by sparse bag-of-words\nretrieval models. Tu et al. [2020] empirically demonstrated that with USE [Cer et al., 2018a,b], the\nquality of the output representations for retrieval degrades as the length of the text increases. These\ntheoretical and empirical results match our intuitions—it becomes increasingly difﬁcult to “squeeze”\nthe meaning of texts into ﬁxed-width vectors as the length increases.\nMany of the dense retrieval techniques discussed in this section have not been applied to full-length\ndocuments. In many cases, researchers presented results on the MS MARCO passage ranking\ntask, but not the document ranking counterpart. For those that do, they primarily adopt the (simple\nand obvious) strategy of breaking long texts into shorter segments and encoding each segment\nindependently. In the case of question answering (for example, in DPR), this is an acceptable solution\nbecause retriever output is sent to the reader model for answer extraction. Furthermore, many natural\nlanguage questions can be answered by only considering relatively small text spans. In the case\nof document retrieval (for example, in the MaxP variant of ANCE), a document is represented by\nmultiple dense vectors, each corresponding to a segment of text in the document and independently\nencoded, and the representation most similar to the query representation is taken as the proxy of the\nentire document for ranking.\nWe are not aware of any dense retrieval techniques on full-length documents that integrate evidence\nfrom multiple parts of a document, for example, in the same way that PARADE (see Section 3.3.4)\ndoes in a reranking setting. SMITH might be an exception [Yang et al., 2020b], although it was not\ndesigned for ad hoc retrieval. In fact, it is unclear how exactly this could be accomplished while\nretaining compatibility with the technical infrastructure that exists today for nearest neighbor search.\nUnlike question answering, where answer extraction can often be accomplished with only limited\ncontext, document-level relevance judgments may require the assessment of a document “holistically”\nto determine its relevance, which is a fundamental limitation of techniques that independently consider\ndocument segments.\nFinally, there are large areas in the design space of dense retrieval techniques that remain unexplored.\nThis is not a research challenge per se, just an observation that much more work still needs to be done.\nThere are many obvious extensions and examples of techniques that can be “mixed-and-matched”\n150This index conﬁguration is minimal in that it only stores term frequencies and does not include positions\n(to support phrase queries), document vectors (to enable relevance feedback), and a copy of the corpus text\n(for convenient access). Even with all these additional features, the complete index is only 2.6 GB (and this\nincludes a compressed copy of the corpus).\n159\nto create combinations that have yet to be examined. For example, Luan et al. [2021] demonstrated\nthe effectiveness of multi-vector representations, but they evaluated only one speciﬁc approach to\ncreating such representations. There are many alternatives that have not be tried. As another example,\ntopic-aware sampling in the construction of training batches [Hofstätter et al., 2021] was developed\nin the context of knowledge distillation, but can broadly applied to other models as well. Another\nresearch direction now receiving attention can be characterized as the dense retrieval “counterparts”\nto the techniques discussed in Section 3.2.4. In the context of cross-encoders, researchers have\nexamined additional pretraining and multi-step ﬁne-tuning strategies, and there is work along similar\nlines, but speciﬁcally for dense retrieval [Lu et al., 2021, Gao and Callan, 2021a,b].\nThere is no doubt that dense retrieval—speciﬁcally, using learned dense representations from trans-\nformers for ranking—is an exciting area of research. For over half a century, exact match techniques\nusing inverted indexes have remained a central and indispensable component in end-to-end infor-\nmation access systems. Advances in the last couple of decades such as feature-driven learning to\nrank, and, more recently, neural networks, still mostly rely on exact match techniques for candidate\ngeneration since they primarily serve as rerankers. Dense retrieval techniques, however, seem poised\nto at least supplement decades-old exact match “sparse” techniques for generating top-krankings\nfrom a large corpus efﬁciently: learned representations have been shown to consistently outperform\nunsupervised bag-of-words ranking models such as BM25.151\nFurthermore, dense–sparse hybrids appear to be more effective than either alone, demonstrating that\nthey provide complementary relevance signals. Large-scale retrieval using dense vector representa-\ntions can often be recast as a nearest neighbor search problem, for which inverted indexes designed\nfor sparse retrieval do not offer the best solution. This necessitates a new class of techniques such as\nHNSW [Malkov and Yashunin, 2020], which have been implemented in open-source libraries such\nas Faiss [Johnson et al., 2017]. Thus, dense retrieval techniques require a different “software stack”\nalongside sparse retrieval with inverted indexes.\nComing to the end of our coverage of ranking with learned dense representations, we cautiously\nventure that describing dense retrieval techniques as a paradigm shift in retrieval might not be an\nexaggeration. We know of at least two instances of dense retrieval techniques deployed in production,\nby Bing (from a blog post152 and according to Xiong et al. [2021]) and Facebook [Huang et al., 2020]\nHowever, we don’t foresee sparse retrieval and inverted indexes being completely supplanted, at least\nin the near future, as there remains substantial value in dense–sparse hybrids. While challenges still\nlie ahead, some of which we’ve sketched above, dense retrieval technique represent a major advance\nin information access.\n151Although there is recent work on learned sparse representations that seems exciting as well [Bai et al., 2020,\nGao et al., 2021b, Zhao et al., 2021, Lin and Ma, 2021, Mallia et al., 2021, Formal et al., 2021a, Lassance\net al., 2021]; see additional discussions in Section 6.2.\n152https://blogs.bing.com/search-quality-insights/May-2018/Towards-More-Intelligent-\nSearch-Deep-Learning-for-Query-Semantics\n160\n6 Future Directions and Conclusions\nIt is quite remarkable that BERT debuted in October 2018, only around three years ago. Taking a\nstep back and reﬂecting, the ﬁeld has seen an incredible amount of progress in a short amount of\ntime. As we have noted in the introduction and demonstrated throughout this survey, the foundations\nof how to apply BERT and other transformer architectures to ranking are already quite sturdy—\nthe improvements in effectiveness attributable to, for example, the simple monoBERT design, are\nsubstantial, robust, and have been widely replicated in many tasks. We can conﬁdently assert that the\nstate of the art has signiﬁcantly advanced over this time span [Lin, 2019], which has been notable in\nthe amount of interest, attention, and activity that transformer architectures have generated. These are\nexciting times!\nWe are nearing the end of this survey, but we are still far from the end of the road in this line of\nresearch—there are still many open question, unexplored directions, and much more work to be done.\nThe remaining pages below represent our attempt to prognosticate on what we see in the distance, but\nwe begin with some remarks on material we didn’t get a chance to cover.\n6.1 Notable Content Omissions\nDespite the wealth of obvious connections between transformer-based text ranking models and\nother NLP tasks and beyond, there are a number of notable content omissions in this survey. As\nalready mentioned at the outset in Section 1.3, we intentionally neglected coverage of other aspects\nof information access such as question answering, summarization, and recommendation.\nThe omission of question answering, in particular, might seem particularly glaring, since at a high\nlevel the differences between document retrieval, passage retrieval, and question answering can\nbe viewed as granularity differences in the desired information. Here we draw the line between\nspan extraction and ranking explicitly deﬁned segments of text. Standard formulations of question\nanswering (more precisely, factoid question answering) require systems to identify the precise span\nof the answer (for example, a named entity or a short phrase) within a larger segment of text. These\nanswer spans are not predeﬁned, thus rendering the problem closer to that of sequence labeling rather\nthan ranking.\nGiven this perspective, we have intentionally omitted coverage of work in question answering focused\non span extraction. This decision is consistent with the breakdown of the problem in the literature. For\nexample, Chen et al. [2017a] outlined a “retriever–reader” framework: The “retriever” is responsible\nfor retrieving candidates from a corpus that are likely to contain the answer and the “reader” is\nresponsible for identifying the answer span. This is just an instance of the multi-stage ranking\narchitectures we have discussed in depth; one can simply imagine adding a reader to any existing\nmulti-stage design to convert a search system into a question answering system. The design of\nretrievers squarely lies within the scope of this survey, and indeed we have interwoven instances\nof such work in our narrative, e.g., DPR [Karpukhin et al., 2020b] in Section 5.4.2 and Cascade\nTransformers [Soldaini and Moschitti, 2020] in Section 3.4.3.\nNevertheless, the impact of BERT and other transformer architectures on span extraction in question\nanswering (i.e., the “reader”) has been at least as signiﬁcant as the impact of transformers in text\nranking. Paralleling Nogueira and Cho [2019], BERTserini [Yang et al., 2019c] was the ﬁrst instance\nof applying a BERT-based reader to the output of a BM25-based retriever to perform question\nanswering directly on Wikipedia. Prior to this work, BERT had been applied only in a reading\ncomprehension setup where the task is to identify the answer in a given document (i.e., there was no\nretriever component), e.g., Alberti et al. [2019]. A proper treatment of the literature here would take\nup another volume,153 but see Chen and Yih [2020] for a tutorial on recent developments.\nAnother closely related emerging thread of work that we have not covered lies at the intersection\nof question answering and document summarization. Like search and question answering, summa-\nrization research has been heavily driven by transformers in recent years, particularly sequence-to-\nsequence models given their natural ﬁt (i.e., full-length document goes in, summary comes out).\nRecent work includes Liu and Lapata [2019], Zhang et al. [2019], Subramanian et al. [2019], Zhang\net al. [2020c]. In the query-focused summarization variant of the task [Dang, 2005], target summaries\nare designed speciﬁcally to address a user’s information need. Techniques based on passage retrieval\n153Perhaps the topic for our next survey?\n161\ncan be viewed as a (strong) baseline for this task, e.g., selecting the most relevant sentence(s) from\nthe input text(s). Along similar lines, although most recent work on question answering is extractive\nin nature (i.e., identifying a speciﬁc answer span in a particular piece of text), researchers have\nbegun to explore abstractive question answering, where systems may synthesize an answer that is not\ndirectly contained in any source document [Izacard and Grave, 2020, Hsu et al., 2021]. Abstractive\napproaches have the potential advantage in providing opportunities for the underlying model to\nsynthesize evidence from multiple sources. At this point, the distinction between query-focused\nsummarization, passage retrieval, and abstractive question answering becomes quite muddled—but\nin a good way, because they present an exciting melting pot of closely related ideas, from which\ninteresting future work is bound to emerge.\nA ﬁnal glaring omission in this survey is coverage of interactive information access techniques.\nNearly all of the techniques we have discussed can be characterized as “one shot”, i.e., an information\nseeker poses a query to a system... and that’s it. Throughout this survey, we have been focused on\nmeasuring and optimizing the quality of system output in this setting and have for the most part\nneglected to discuss “what comes next”. Indeed, what happens after this initial query? Typically, if\nthe desired relevant information is not obtained, the user will try again, for example, with a different\nformulation of the query. Even if the information need is satisﬁed, the user may continue to engage\nin subsequent interactions as part of an information seeking session, for example, to ask related or\nfollow-up questions. Studies of interactive information retrieval systems date to the 1980s, but there\nhas been a resurgence of interest in the context of intelligent personal assistants such as Siri and\n“smart” consumer devices such as Alexa. No surprise, neural models (particularly transformers) have\nbeen applied to tackle many aspects of the overall challenge. While researchers use many terms\ntoday to refer to this burgeoning research area, the term “conversational search” or “conversational\ninformation seeking” has been gaining currency.\nAs we lack the space for a thorough treatment of the literature in this survey, we refer readers\nto a few entry points: two good places to start include a theoretical framework for conversational\nsearch by Radlinski and Craswell [2017] and a recent survey about conversational AI more broadly,\nencompassing dialogue systems, conversational agents, and chatbots by McTear [2020]. In the\ninformation retrieval community, one recent locus of activity has been the Conversational Assistance\nTracks (CAsT) at TREC, which have been running since 2019 [Dalton et al., 2019] with the goal\nof advancing research on conversational search systems by building reusable evaluation resources.\nIn the natural language processing community, there is substantial parallel interest in information\nseeking dialogues, particularly in the context of question answering [Choi et al., 2018, Elgohary et al.,\n2019]. There exist many datasets that capture typical linguistic phenomena observed in naturally\noccurring dialogues such as anaphora, ellipsis, and topic shifts.\n6.2 Open Research Questions\nLooking into the future, we are able to identify a number of open research questions, which we\ndiscuss below. These correspond to threads of research that are being actively pursued right now, and\ngiven the rapid pace of progress in the ﬁeld, we would not be surprised if there are breakthroughs in\nanswering these question by the time a reader consumes this survey.\nTransformers for Ranking: Apply, Adapt, or Redesign?At a high level, reranking models based\non transformers can be divided into three approaches:\n1. apply existing transformer models with minimal modiﬁcations—exempliﬁed by monoBERT\nand ranking with T5;\n2. adapt existing transformer models, perhaps adding additional architectural elements—\nexempliﬁed by CEDR and PARADE; or,\n3. redesign transformer-based architectures from scratch—exempliﬁed by the TK/CK models.\nWhich is the “best” approach? And to what end? Are we seeking the most effective model, without\nany considerations regarding efﬁciency? Or alternatively, are we searching for some operating point\nthat balances effectiveness and efﬁciency?\nThere are interesting and promising paths forward with all three approaches: The ﬁrst approach\n(“apply”) allows researchers to take advantage of innovations in natural language processing (that\n162\nmay not have anything to do with information access) “for free” and ﬁts nicely with the “more data,\nlarger models” strategy. The last approach (“redesign”), on the other hand, requires researchers to\nreconsider each future innovation speciﬁcally in the context of text ranking and assess its applicability.\nHowever, this approach has the advantage in potentially stripping away all elements unnecessary for\nthe problem at hand, thereby possibly achieving better effectiveness/efﬁciency tradeoffs (for example,\nthe TK/CK models). The second approach (“adapt”) tries to navigate the middle ground, retaining a\n“core” that can be swapped for a better model that comes along later (for example, PARADE swapping\nout BERT for ELECTRA).\nIn the design of transformer models for ranking, it is interesting to observe that the evolution of\ntechniques follows a trajectory resembling the back-and-forth swing of a pendulum. Pre-BERT\nneural ranking models were characterized by a diversity of designs, utilizing a wide range of\nconvolutional and recurrent components. In the move from pre-BERT interaction-based ranking\nmodels to monoBERT, all these architectural components became subsumed in the all-to-all attention\nmechanisms in BERT. For example, convolutional ﬁlters with different widths and strides didn’t\nappear to be necessary anymore, replaced in monoBERT by architecturally homogeneous transformer\nlayers. However, we are now witnessing the reintroduction of specialized components to explicitly\ncapture intuitions important for ranking—for example, the hierarchical design of PARADE (see\nSection 3.3.4) and the reintroduction of similarity matrices in TK/CK (see Section 3.5.2).\nThese points apply equally to ranking with learned dense representations. Current models either\n“apply” off-the-shelf transformers with minimal manipulations of their output (e.g., mean pool-\ning in Sentence-BERT) or “adapt” the output of off-the-self transformers with other architectural\ncomponents (e.g., poly-encoders). In principle, it would be possible to completely “redesign” trans-\nformer architectures for ranking using dense representations, similar to the motivation of TK/CK for\nreranking. This would be an interesting path to pursue.\nSo, does the future lie with apply, adapt, or redesign? All three approaches are promising, and\nwe see the community continuing to pursue all three paths moving forward. Finally, there is the\npossibility that the answer is actually “none of the above”! The very premise of this survey (i.e.,\ntransformer models) has been called into question: echoing the “pendulum” theme discussed above,\nsome researchers are re-examining CNNs [Tay et al., 2021] and even MLPs [Liu et al., 2021] for\nNLP tasks. Speciﬁcally for text ranking, Boytsov and Kolter [2021] explored the use of a pre-neural\nlexical translation model for evidence aggregation, arguing for improved interpretability as well as\na better effectiveness/efﬁciency tradeoff. We don’t see transformers becoming obsolete in the near\nfuture, but it is likely that one day we will move beyond such architectures.\nMulti-Stage Ranking and Representation Learning: What’s the Connection?While the organi-\nzation of this survey might suggest that multi-stage ranking and dense retrieval are distinct threads of\nwork, we believe that moving forward these two threads will become increasingly intertwined.\nRecall that one motivation for ranking with learned dense representations is to replace an entire\nmulti-stage ranking pipeline with a single retrieval stage that can be trained end to end. To some\nextent, this is convenient ﬁction: For a comparison function φmore complex than inner products or\na handful of other similarity functions, ranking is already multi-stage. ColBERT in the end-to-end\nsetting, for example, uses an ANN library to ﬁrst gather candidates that are then reranked, albeit with\nthe authors’ proposed lightweight MaxSim operator (see Section 5.5.2). Furthermore, withany design\nbased on inner products or a simple φ, we can further improve effectiveness by reranking its output\nwith a cross-encoder, since by deﬁnition cross-encoders support more extensive query–document\ninteractions than bi-encoders and thus can exploit richer relevance signals. In this case, we’re back to\nmulti-stage ranking architectures!\nEmpirically, the best dense retrieval techniques to date are less effective than the best reranking\narchitectures, for the simple reason discussed above—the output from dense retrieval techniques\ncan be further reranked to improve effectiveness. RocketQA [Qu et al., 2021] provides a great\nexample near the top of the leaderboard for the MS MARCO passage ranking task: starting with a\nstate-of-the-art dense retrieval model (discussed in Section 5.4.3) and then further applying reranking.\nPut differently, in a multi-stage ranking architecture, we can replace ﬁrst-stage retrieval based on\nsparse representations (e.g., bag-of-words BM25) with a dense retrieval model, or better yet, a hybrid\napproach that combines both dense and sparse relevance signals, such as many of the techniques\ndiscussed in Section 5.\n163\nIn fact, replacing candidate generation using inverted indexes with candidate generation using\napproximate nearest neighbor search is an idea that can be applied independent of BERT. For\nexample, Nakamura et al. [2019] began with a standard multi-stage design where BM25-based ﬁrst-\nstage retrieval feeds DRMM for reranking and investigated replacing the ﬁrst stage with approximate\nnearest-neighbor search based on representations from a deep averaging network [Iyyer et al., 2015].\nUnfortunately, the end-to-end effectiveness was worse, but this was “pre-BERT”, prior to the advent\nof the latest transformer models. More recently, Tu et al. [2020] had more success replacing candidate\ngeneration using BM25 with candidate generation using dense vectors derived from the transformer-\nbased Universal Sentence Encoder (USE) [Cer et al., 2018b]. They demonstrated that a multi-stage\narchitecture with an ANN ﬁrst stage can offer better tradeoffs between effectiveness and efﬁciency\nfor certain tasks, particularly those involving shorter segments of text.\nWe believe that there will always be multi-stage ranking architectures, since they can incorporate any\ninnovation that adopts a single-stage approach and then try to improve upon its results with further\nreranking. In real-world applications, when the elegance of single-stage models and the advantages\nof end-to-end training bump up against the realities of requirements to deliver the best output quality\nunder resource constraints, we suspect that the latter will generally win, “beauty” be damned.\nThere has been much research on learned dense representations for ranking, as we have covered in\nSection 5, and dense retrieval techniques have been demonstrated to be more effective than sparse\nretrieval techniques such as BM25 on standard benchmark datasets. However, this comparison is\nunfair, because we are comparing learned representations against representations that did not exploit\ntraining data; BM25 can be characterized as unsupervised. To better understand and categorize\nemerging retrieval techniques, Lin and Ma [2021] proposed a conceptual framework that identiﬁes\ntwo dimensions of interest: The contrast between sparse and dense vector representations and the\ncontrast between unsupervised and learned (supervised) representations. DPR, ANCE, and the\ntechniques discussed in Section 5 can be classiﬁed as learned dense representations. BM25 can\nbe classiﬁed as unsupervised sparse representations. But of course, it is possible to learn sparse\nrepresentations as well!154\nOne way to think about this idea is to understand learned dense representations as letting transformers\n“pick” the basis for its vector space to capture the “meaning” of texts. The dimensions of the resulting\nvectors can be thought of as capturing some latent semantic space. What if, as an alternative, we\nforced the encoder (still using transformers) to use the vocabulary of the corpus it is being trained on\nas the basis of its output representation? This is equivalent to learningweights on sparse bag-of-words\nrepresentations. DeepCT (see Section 4.4) is one possible implementation, but its weakness is that\nterms that do not occur in the text receive a weight of zero, and thus the model cannot overcome\nvocabulary mismatch issues. This limitation was later addressed by DeepImpact (see Section 4.6),\nbut there are other recent papers that build on the same intuitions— learning weights for sparse bag-\nof-words representations [Bai et al., 2020, Gao et al., 2021b, Zhao et al., 2021, Formal et al., 2021a,\nLassance et al., 2021]. In the future, we suspect that learned representations (using transformers)\nwill become the emphasis, while sparse vs. dense representations can be thought of as design choices\nmanifesting different tradeoffs (and not the most important distinction). Once again, hybrids that\ncombine sparse and dense signals might offer the best of both worlds.\nIn multi-stage ranking architectures, ﬁrst-stage retrieval based on learned dense representations are\nalready common. There is, however, nothing to prevent dense representations from being used\nin reranking models. In fact, there are already many such examples: Khattab and Zaharia [2020],\nHofstätter et al. [2020], and others have already reported such reranking experimental conditions\nin their papers. EPIC [MacAvaney et al., 2020d] is a reranking model explicitly designed around\ndense representations. Such approaches often manifest different tradeoffs from rerankers based\non cross-encoders: representations of texts from the corpus can be precomputed, and they support\ncomparison functions (i.e., φin our framework) that are more complex than a simple inner product.\nSuch formulations of φenable richer query–document interactions, but are usually more lightweight\nthan transformer-based multi-layer all-to-all attention. Thus, rerankers based on dense representations\npresent another option in a practitioner’s toolbox to balance effectiveness/efﬁciency tradeoffs.\n154Of course, this idea isn’t exactly new either! Zamani et al. [2018] explored learning sparse representations in\nthe context of pre-BERT neural models. Going much further back, Wilbur [2001] attempted to learn global\nterm weights using TREC data.\n164\nThis brings us to a ﬁnal direction for future work. In multi-stage approaches that mix sparse and\ndense representations—both in ﬁrst-stage retrieval and downstream rerankers—mismatches between\nthe distribution of the representations from different stages remain an issue (see discussion in\nSection 5.1). That is, the types of texts that a model is trained on (in isolation) may be very different\nfrom the types of texts it sees when inserted into a multi-stage architecture. We raised this issue in\nSection 3.2, although the mismatch between BM25-based ﬁrst-stage retrieval and BERT’s contextual\nrepresentation does not seem to have negatively impacted effectiveness. In truth, however, the design\nof most experiments today does not allow us to effectively quantify the potential gains that can come\nfrom better aligning the stages, since we haven’t observed them in the ﬁrst place. While there is\nprevious work that examines how multi-stage ranking pipelines can be learned [Wang et al., 2010,\nXu et al., 2012], there is little work in the context of transformer architectures speciﬁcally. A notable\nexception is the study by Gao et al. [2021a], who proposed simple techniques that allow downstream\nrerankers to more effectively exploit better ﬁrst-stage results, but more studies are needed.\nHow to Rank Out-of-Distribution Data? Nearly all of the techniques presented in this survey are\nbased on supervised learning, with the supervision signals ultimately coming from human relevance\njudgments (see Section 2.4). Although we have discussed many enhancements based on distant\nsupervision, data augmentation, and related techniques, newly generated or gathered data still serve\nprimarily as input to supervised learning methods for training reranking or dense retrieval models.\nThus, a natural question to ponder: What happens if, at inference (query) time, the models are fed\ninput that doesn’t “look like” the training data? These inputs can be “out-of-distribution” in at least\nthree different ways:\n• Different queries. The queries fed into the model differ from those the model encountered\nduring training. For example, the training data could comprise well-formed natural language\nquestions, but the model is applied to short keyword queries.\n• Different texts from the corpus. The texts that comprise the units of retrieval are very different\nfrom those fed to the model during training. For example, a bi-encoder trained on web documents\nis fed scientiﬁc articles or case law.\n• Different tasks. For example, a model trained with (query, relevant text) pairs might be applied\nin a community question answering context to retrieve relevant questions from a FAQ repository.\nThis task is closer to paraphrase detection between two sentences (questions) than query–\ndocument relevance. Task mismatch often occurs when there is no training data available for\nthe target task of interest (for example, in a specialized domain).\nIn many cases, the answer is: The model doesn’t perform very well on out-of-distribution data! Thus,\nthere is a large body of work in NLP focused on addressing these challenges, falling under the banner\nof domain adaptation or transfer learning. Recently, “zero-shot learning” and “few-shot learning”\nhave come into vogue. In the ﬁrst case, trained models are directly applied to out-of-distribution data,\nand in the few-shot learning case, the model gets a “few examples” to learn from.\nGiven that the standard “BERT recipe” consists of pretraining followed by ﬁne-tuning, methods for\naddressing out-of-distribution challenges immediately present themselves. In fact, we have already\ndiscussed many of these approaches in Section 3.2.4 in the context of reranking models—for example,\nadditional pretraining on domain-speciﬁc corpora to improve the base transformer and strategies\nfor multi-step ﬁne-tuning, perhaps enhanced with data augmentation. These techniques have been\nexplored, both for NLP tasks such as part-of-speech tagging and named-entity recognition as well as\ninformation access tasks.\nSpeciﬁcally for information retrieval, the TREC-COVID challenge has provided a forum where\nmany proposed solutions for domain adaptation have been deployed and evaluated. In 2020, the\nmost signiﬁcant event that has disrupted all aspects of life worldwide is, of course, the COVID-19\npandemic. Improved information access capabilities have an important role to play in the ﬁght against\nthis disease by providing stakeholders with high-quality information from the scientiﬁc literature\nto inform evidence-based decision making and to support insight generation. In the early stages\nof the pandemic, examples include public health ofﬁcials assessing the efﬁcacy of population-level\ninterventions such as mask ordinances, physicians conducting meta-analyses to update care guidelines\nbased on emerging clinical studies, and virologists probing the genetic structure of the virus to develop\nvaccines. As our knowledge of COVID-19 evolved and as the results of various studies became\n165\navailable, stakeholders needed to constantly re-assess current practices against the latest evidence,\nnecessitating high-quality information access tools to sort through the literature.\nOne prerequisite to developing and rigorously evaluating these capabilities is a publicly accessible\ncorpus that researchers can work with. As a response to this need, in March 2020 the Allen Institute\nfor AI (AI2) released the COVID-19 Open Research Dataset (CORD-19) [Wang et al., 2020a], which\nis a curated corpus of scientiﬁc articles about COVID-19 and related coronaviruses (e.g., SARS and\nMERS) gathered from a variety of sources such as PubMed as well as preprint servers. The corpus is\nregularly updated as the literature grows.\nThe NIST-organized TREC-COVID challenge [V oorhees et al., 2020, Roberts et al., 2020],155 which\nbegan in April 2020 and lasted until August 2020, brought TREC-style evaluations to the CORD-19\ncorpus. The stated goal of the effort was to provide “an opportunity for researchers to study methods\nfor quickly standing up information access systems, both in response to the current pandemic and to\nprepare for similar future events”. The challenge was organized into a series of “rounds”, each of\nwhich used a particular snapshot of the CORD-19 corpus.\nThe evaluation topics comprised a broad range of information needs, from those that were primarily\nclinical in nature (e.g., “Are patients taking Angiotensin-converting enzyme inhibitors (ACE) at\nincreased risk for COVID-19?”) to those focused on public health (e.g., “What are the best masks\nfor preventing infection by Covid-19?”). From a methodological perspective, TREC-COVID im-\nplemented a few distinguishing features that set it apart from other TREC evaluations. Each round\ncontained both topics that were persistent (i.e., carried over from previous rounds) as well as new\ntopics—the idea was to consider existing information needs in light of new evidence as well as to\naddress emerging information needs.\nThe TREC-COVID organizers adopted a standard pooling strategy for evaluating runs, but once an\narticle was assessed, its judgment was never revised (even if contrary evidence later emerged). To\navoid duplicate effort, the evaluation adopted a residual collection methodology, where previously\njudged articles were automatically removed from consideration. Thus, each round only considered\narticles that had not been examined before by a human assessor (on a per-topic basis); these were\neither newly published articles or existing articles that had not been previously submitted as part of a\nrun. Round 1 began with 30 topics, and each subsequent round introduced ﬁve additional topics, for a\ntotal of 50 topics in round 5.\nThis evaluation methodology had some interesting implications. On the one hand, each round\nessentially stood as a “mini-evaluation”, in the sense that scores across rounds are not comparable:\nboth the corpora and the topics were different. On the other hand, partial overlaps in both topics and\ncorpora across rounds connected them. In particular, for the persistent information needs, relevance\njudgments from previous rounds could be exploited to improve the effectiveness of systems in future\nrounds on the same topic. Runs that took advantage of these relevance judgments were known as\n“feedback” runs, in contrast to “automatic” runs that did not.\nOverall, the TREC-COVID challenge was a success in terms of participation. The ﬁrst round had\nover 50 participating teams from around the world, and although the participants dwindled somewhat\nas the rounds progressed, round 5 still had close to 30 participating teams. For reference, a typical\n“successful track” at TREC might draw around 20 participating teams.\nThe TREC-COVID challenge is of interest because it represented the ﬁrst large-scale evaluation\nof information access capabilities in a specialized domain following the introduction of BERT. As\nexpected, the evaluation showcased a variety of transformer-based models. Since all participants\nbegan with no in-domain relevance judgments, the evaluation provided an interesting case study in\nrapid domain adaption. The multi-round setup allowed teams to improve system output based on\nprevious results, to train their models using newly available relevance judgments, and to reﬁne their\nmethods based on accumulated experience. The biggest challenge was the paucity of labeled training\nexamples: on a per-topic basis, there were only a few hundred total judgments (both positive and\nnegative) per round.\nOverall, the evaluation realistically captured information access challenges in a rapidly evolving\nspecialized domain. The nature of the pandemic and the task design meant that research, system\ndevelopment, and evaluation efforts were intense and compressed into a short time span, thus leading\n155https://ir.nist.gov/covidSubmit/index.html\n166\nto rapid advances. As a result, innovations diffused from group to group much faster than under\nnormal circumstances. We summarize some of the important lessons learned below:\nEnsembles and fusion techniques work well. Many teams submitted runs that incorporated the output\nof different retrieval methods. Some of these were relatively simple, for example, exact match scoring\nagainst different representations of the articles (e.g., abstracts, full texts, and paragraphs from the\nfull text). Other sources of fusion involved variants of BERT-based models or transformer-based\nrerankers applied to different ﬁrst-stage retrieval approaches, e.g., Bendersky et al. [2020].\nSimple fusion techniques such as reciprocal rank fusion [Cormack et al., 2009] or linear combi-\nnations [V ogt and Cottrell, 1999] were effective and robust, with few or no “knobs” to tune and\ntherefore less reliant on training data. In the earlier rounds, this was a distinct advantage as all the\nteams were equally inexperienced in working with the corpus. In the ﬁrst round, for example, the\nbest automatic run was submitted by the sabir team, who combined evidence from bag-of-words\nvector-space retrieval against abstracts and full text using a linear combination. Even in the later\nrounds, ensembles and fusions techniques still provided a boost over individual transformer-based\nranking models. Some sort of fusion technique was adopted by nearly all of the top-scoring runs\nacross all rounds. While the effectiveness of ensemble and fusion techniques is well known, e.g.,\n[Bartell et al., 1994, Montague and Aslam, 2002], replicated ﬁndings in new contexts still contribute\nto our overall understanding of the underlying techniques.\nSimple domain adaptation techniques work well with transformers. Even prior to the COVID-19\npandemic, NLP researchers had already built and shared variants of BERT that were pretrained\non scientiﬁc literature. SciBERT [Beltagy et al., 2019] and BioBERT [Lee et al., 2020b] are two\nwell-known examples, and many TREC-COVID participants built on these models. Xiong et al.\n[2020a] demonstrated that the target corpus pretraining (TCP) technique described in Section 3.2.4\nalso worked for TREC-COVID.\nIn terms of ﬁne-tuning BERT-based reranking models for TREC-COVID, MacAvaney et al. [2020a]\nproposed an approach to automatically create (pseudo) in-domain training data from a larger general\ndataset. The idea was to ﬁlter the MS MARCO passage ranking test collection and retain only queries\nthat contain at least one term from the MedSyn lexicon [Yates and Goharian, 2013]. That is, the\nauthors used simple dictionary ﬁltering to create a “medical subset” of the MS MARCO passage\nranking test collection, dubbed Med-MARCO, which was then used to ﬁne-tune a monoBERT model\nbased on SciBERT [Beltagy et al., 2019]. In the ﬁrst round, this run was the second highest scoring\nautomatic run, but alas, it was still not as effective as the simple bag-of-words fusion run from the\nsabir team mentioned above. Data selection tricks for domain adaptation are not new [Axelrod\net al., 2011], but MacAvaney et al. demonstrated a simple and effective technique that was quickly\nadopted by many other participants in subsequent rounds. Reinforcement learning has also been\nproposed to select better examples to train rerankers [Zhang et al., 2020d]. The technique, dubbed\nReInfoSelect, was successfully applied by Xiong et al. [2020a], helping the team achieve the best\nfeedback submission in round 2.\nAnother interesting method to createsynthetic in-domain labeled data was used by teamunique_ptr,\nwho generated (query, relevant text) pairs from CORD-19 articles using a model similar to doc2query\nand then trained a dense retrieval model using these generated pairs [Ma et al., 2021a]. The team\nsubmitted the best feedback runs (and top-scoring runs overall) in rounds 4 and 5, which incorporated\nthis data generation approach in hybrid ensembles [Bendersky et al., 2020].\nOr just train a bigger model? As an alternative to domain adaptation techniques discussed above, we\ncould just build bigger models. For a wide range of NLP tasks, the GPT family [Brown et al., 2020]\ncontinues to push the frontiers of larger models, more compute, and more data. While this approach\nhas a number of obvious problems that are beyond the scope of this discussion, it nevertheless\ndemonstrates impressive effectiveness on a variety of natural language tasks, both in a zero-shot\nsetting and prompted with only a few examples.\nFor TREC-COVID, the covidex team [Zhang et al., 2020a] deployed an architecture compris-\ning doc2query–T5 for document expansion (see Section 4.3) and a reranking pipeline comprising\nmonoT5/duoT5 [Pradeep et al., 2021b] (see Section 3.5.3). Their approach with T5-3B (where 3B\nrefers to 3 billion parameters) yielded the best automatic runs for rounds 4 and 5, accomplished\nin a zero-shot setting since the models were trained only on MS MARCO passage data. In other\n167\nwords, they just trained a larger model with out-of-distribution data. Could this be another successful\napproach to domain adaptation?\nLearning with limited data remains a weakness with transformers. In the later rounds, we see that\nautomatic runs based on transformers outperformed non-transformer runs by large margins, whereas\nin many cases feedback runs based on transformers barely beat their non-transformer competition. In\nfact, simple relevance feedback techniques were quite competitive with transformer-based approaches.\nFor example, in round 2, a feedback run by the UIowaS team, which can be characterized as off-\nthe-shelf relevance feedback, reported the third highest score in that run category. Although two\nBERT-based feedback runs from thempiid5 team outperformed this relevance feedback approach,\nthe margins were quite slim. One possible explanation for these small differences is that we are\nreaching the inter-annotator agreement “limit” of this corpus with this set of topics, i.e., that results\nfrom top-performing systems are already good enough to the point that relevance judgments from\nhuman annotators cannot conﬁdently distinguish which is better.\nAs another example, the covidex team [Zhang et al., 2020a, Han et al., 2021] implemented an\napproach that treated relevance feedback as a document classiﬁcation problem using simple lin-\near classiﬁers [Cormack and Mojdeh, 2009, Grossman and Cormack, 2017, Yu et al., 2019]. In\nboth rounds 4 and 5, it was only narrowly beaten by the large-scale hybrid ensembles of team\nunique_ptr [Bendersky et al., 2020]. It seems that researchers have yet to ﬁgure out how to exploit\nsmall numbers of labeled examples to improve effectiveness. How to ﬁne-tune BERT and other\ntransformer models with limited data remains an open question, not only for text ranking, but across\nother NLP tasks as well [Zhang et al., 2020e, Lee et al., 2020a].\nWith a few notable exceptions, participants in the TREC-COVID challenge focused mostly on\nreranking architectures. However, as we have already discussed in Section 5.7, the same out-of-\ndistribution issues are present with learned dense representations as well. The recent BEIR benchmark\n[Thakur et al., 2021], already discussed, has shown that applied in a zero-shot manner to diverse\ndomains, dense retrieval techniques trained on MS MARCO data are less effective than BM25 overall.\nAddressing the generalizability and robustness of both reranking and dense retrieval techniques for\nout-of-distribution texts and queries is an important future area of research.\nHow to Move Beyond Ranking in English? It goes without saying that the web is multilingual\nand that speakers of all languages have information needs that would beneﬁt from information\naccess technologies. Yet, the techniques discussed in this survey have focused on English. We\nshould as a research community broaden the scope of exploration; not only would studies focused on\nmultilinguality be technically interesting, but potentially impactful in improving the lives of users\naround the world.\nAttempts to break the language barrier in information access can be divided into two related efforts:\nmono-lingual retrieval in non-English languages and cross-lingual retrieval.\nIn the ﬁrst scenario, we would like to support non-English speakers searching in their own languages—\nfor example, Urdu queries retrieving from Urdu documents. Of course, Urdu ranking models can be\nbuilt if there are sufﬁcient resources (test collections) in Urdu, as many supervised machine-learning\ntechniques for information retrieval are language agnostic. However, as we have already discussed\n(see Section 2.1), building test collections is an expensive endeavor and thus constructing such\nresources language by language is not a cost-effective solution if we wish to support the six thousand\nlanguages that are spoken in the world today. Can we leverage relevance judgments and data that are\navailable in high-resource languages (English, for example) to beneﬁt languages for which we lack\nsufﬁcient resources?\nThe second information access scenario is cross-lingual retrieval, where the language of the query\nand the language of the documents differ. Such technology, especially coupled with robust machine\ntranslation, can unlock stores of knowledge for users that they don’t otherwise have access to. For\nexample, Bengali speakers in India can search for information in English web pages, and a machine\ntranslation system can then translate the pages into Bengali for the users to consume. Even with\nimperfect translations, it is still possible to convey the gist of the English content, which is obviously\nbetter than nothing if the desired information doesn’t exist in Bengali. Note that cross-lingual retrieval\ntechniques can also beneﬁt speakers of English and other high-resource languages: for example, in\nWikipedia, it is sometimes the case that “localized versions” of articles contain more information than\nthe English versions. The Hungarian language article about a not-very-well-known Hungarian poet\n168\nor a location in Hungary might contain more information than the English versions of the articles. In\nthis case, English speakers can beneﬁt from cross-lingual retrieval techniques searching in Hungarian.\nExplorations of multilingual applications of BERT for information access are well underway. Google’s\nOctober 2019 blog post156 announcing the deployment of BERT (which we referenced in the intro-\nduction) offered some tantalizing clues:\nWe’re also applying BERT to make Search better for people across the world.\nA powerful characteristic of these systems is that they can take learnings from\none language and apply them to others. So we can take models that learn from\nimprovements in English (a language where the vast majority of web content exists)\nand apply them to other languages. This helps us better return relevant results in\nthe many languages that Search is offered in.\nFor featured snippets, we’re using a BERT model to improve featured snippets\nin the two dozen countries where this feature is available, and seeing signiﬁcant\nimprovements in languages like Korean, Hindi and Portuguese.\nRegarding the ﬁrst point, what Google was referring to may be something along the lines of what Shi\nand Lin [2019] (later appearing as Shi et al. [2020]) and MacAvaney et al. [2020f] demonstrated\naround November 2019. For example, the ﬁrst paper presented experimental results using an\nextension of Birch (see Section 3.3.1) showing that multilingual BERT is able to transfer models of\nrelevance across languages. Speciﬁcally, it is possible to train BERT ranking models with English\ndata to improve ranking quality in (non-English) mono-lingual retrieval as well as cross-lingual\nretrieval, without any special processing. These ﬁndings were independently veriﬁed by the work\nof MacAvaney et al. The second point in Google’s blog post likely refers to multi-lingual question\nanswering, where the recent introduction of new datasets has helped spur renewed interest in this\nchallenge [Cui et al., 2019, Liu et al., 2019a, Clark et al., 2020a, Asai et al., 2021].\nAlthough some of the early neural ranking approaches did explore cross-lingual retrieval [Vuli´c and\nMoens, 2015] and new research on this topic continues to emerge in the neural context [Yu and Allan,\n2020, Phang et al., 2020], we have not found enough references in the context of transformers to\nwarrant a detailed treatment in a dedicated section. However, moving forward, this is fertile ground\nfor exploration.\nFrom Transformers for Ranking to Ranking for Transformers? This survey is mostly about\napplications of transformers to text ranking. That is, how can pretrained models be adapted in\nservice of information access tasks. However, there is an emerging thread of work, exempliﬁed\nby REALM [Guu et al., 2020], that seeks to integrate text retrieval and text ranking directly into\nmodel pretraining. The idea is based on the observation that BERT and other pretrained models\ncapture a surprisingly large number of facts, simply as a side effect of the masked language model\nobjective [Petroni et al., 2019]. Is it possible to better control this process so that facts are captured in a\nmore modular and interpretable way? The insight of REALM is that prior to making a prediction about\na masked token, the model can retrieve and attend over related documents from a large corpus such\nas Wikipedia. Retrieval is performed using dense representations like those discussed in Section 5.\nSimilar intuitions have also been explored by others. For example, Wang and McAllester [2020]\nviewed information retrieval techniques as a form of episodic memory for augmenting GPT-2. In the\nproposal of Wu et al. [2020a], a “note dictionary” saves the context of a rare word during pretraining,\nsuch that when the rare word is encountered again, the saved information can be leveraged. Other\nexamples building on similar intuitions include the work of Lewis et al. [2020a] and Du et al. [2021].\nThus, the question is not only “What can transformers do for text ranking?” but also “What can\ntext ranking do for transformers?” We have some initial answers already, and no doubt, future\ndevelopments will be exciting.\nIs Everything a Remix? We have seen again and again throughout this survey that much recent\nwork seems to be primarily adaptation of old ideas, many of which are decades old. For example,\nmonoBERT, which heralded the BERT revolution for text ranking, is just pointwise relevance\nclassiﬁcation—dating back to the late 1980s [Fuhr, 1989]—but with more powerful models.\n156https://www.blog.google/products/search/search-language-understanding-bert/\n169\nTo be clear, we don’t think there is anything “wrong” (or immoral, or unethical, etc.) with recycling\nold ideas: in fact, the ﬁlmmaker Kirby Ferguson famously claimed that “everything is a remix”. He\nprimarily referred to creative endeavors such as music, but the observation applies to science and\ntechnology as well. Rifﬁng off Picasso’s quote “Good artists copy, great artists steal”, Steve Jobs once\nsaid, “We have always been shameless about stealing great ideas”.157 The concern arises, however,\nwhen we lose touch with the rich body of literature that deﬁnes our past, for the simple reason that\nprevious work didn’t use deep learning.\nIn “water cooler conversations” around the world and discussions on social media, (more senior)\nresearchers who were trained before the advent of deep learning often complain, and only partly\ntongue-in-cheek, that most students today don’t believe that natural language processing existed\nbefore neural networks. It is not uncommon to ﬁnd deep learning papers today that cite nothing but\nother deep learning papers, and nothing before the early 2010s. Isaac Newton is famous for saying\n“If I have seen further than others, it is by standing upon the shoulders of giants.” We shouldn’t forget\nwhose shoulders we’re standing on, but unfortunately, often we do.158\nOn a practical note, this means that there are likely still plenty of gems in the literature hidden in plain\nsight; that is, old ideas that everyone has forgotten, but has acquired new relevance in the modern\ncontext. It is likely that many future innovations will be remixes!\n6.3 Final Thoughts\nAt last, we have come to the end of our survey. Information access problems have challenged\ncivilizations since shortly after the invention of writing, when humankind’s collective knowledge\noutgrew the memory of its elders. Although the technologies have evolved over the millennia, from\nclay tablets to scrolls to books, and now electronic information that are “born” and stored digitally,\nthe underlying goals have changed little: we desire to develop tools, techniques, and processes to\naddress users’ information needs. The academic locus of this quest with computers, which resides\nin the information retrieval and natural language processing communities, has only been around for\nroughly three quarters of a century—a baby in comparison to other academic disciplines (say, physics\nor chemistry).\nWe can trace the evolution of information retrieval through major phases of development (exact match,\nlearning to rank, pre-BERT neural networks), as described in the introduction. No doubt we are\ncurrently in the “age” of BERT and transformers.159 Surely, there will emerge new technologies that\ncompletely supplant these models, bringing in the dawn of a new age. Nevertheless, while we wait for\nthe next revolution to happen, there is still much exploration left to be done with transformers; these\nexplorations may plant the seeds of or inspire what comes next. We hope that this survey provides a\nroadmap for these explorers.\n157That is, until his innovations get stolen. Steve Jobs is also reported to have said, “I’m going to destroy Android,\nbecause it’s a stolen product. I’m willing to go thermonuclear war on this.”\n158For this reason, we have taken care throughout this survey to not just cite the most recent (and conveniently\nlocatable) reference for a particular idea, but to trace back its intellectual history. In some cases, this has\ninvolved quite extensive and interesting “side quests” involving consultations with senior researchers who\nhave ﬁrsthand knowledge of the work (e.g., worked in the same lab that the idea was developed)—in essence,\noral histories. We are conﬁdent to differing degrees whether we have properly attributed various ideas, and\nwelcome feedback by readers to the contrary. We believe it is important to “get this right”.\n159Final footnote: or the “age of muppets”, as some have joked.\n170\nAcknowledgements\nThis research was supported in part by the Canada First Research Excellence Fund and the Natural\nSciences and Engineering Research Council (NSERC) of Canada. In addition, we would like to thank\nthe TPU Research Cloud for resources used to obtain new results in this work.\nWe’d like to thank the following people for comments on earlier drafts of this work: Chris Buckley,\nDanqi Chen, Maura Grossman, Sebastian Hofstätter, Kenton Lee, Sheng-Chieh Lin, Xueguang Ma,\nBhaskar Mitra, Jheng-Hong Yang, Scott Yih, and Ellen V oorhees. Special thanks goes out to two\nanonymous reviewers for their insightful comments and helpful feedback.\n171\nVersion History\nVersion 0.90 — October 14, 2020\nInitial release.\nVersion 0.95 — July 28, 2021\nMajor revisions include:\n• Broke out preprocessing and document expansion techniques from Section 3 into a new Section 4\ntitled “Reﬁning Query and Document Representations”, which includes (new) discussion of\nquery expansion techniques.\n• Rewrote Section 5 “Learned Dense Representations for Ranking” to incorporate new develop-\nments in dense retrieval.\n• Reduced emphasis on “Domain-Speciﬁc Applications” as a standalone subsection in Section 3,\nwith most of the content now interwoven throughout the rest of the survey.\n• Redrew all diagrams and ﬁgures throughout the book for a consistent look.\nSubstantive differences from version 0.90 are marked with themajorchange custom LATEXcommand.\nReaders speciﬁcally interested in these edits can recompile this survey with an alternate deﬁnition\nof the command that renders the changes in blue. Note that it is not the case that parts not marked\nwith majorchange remain unchanged from the previous version. The entire survey went through\nseveral rounds of copy editing, but we have not marked changes unless the text was in our opinion\nsubstantially altered.\nVersion 0.99 — August 19, 2021\nThis is the ﬁnal preproduction version shipped to the publisher. There are no major changes in content\ncompared to the previous version; we added references to a few more recently published papers\nand further copy edited the manuscript to reﬁne the prose. The majorchange “annotations” remain\nlargely unchanged from version 0.95.\n172\nReferences\nM. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,\nM. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker,\nV . Vasudevan, P. Warden, M. Wicke, Y . Yu, and X. Zheng. TensorFlow: A system for large-scale\nmachine learning. In Proceedings of the 12th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI ’16), pages 265–283, 2016.\nN. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, D. Metzler, M. D. Smucker,\nT. Strohman, H. Turtle, and C. Wade. UMass at TREC 2004: Novelty and HARD. In Proceedings\nof the Thirteenth Text REtrieval Conference (TREC 2004), Gaithersburg, Maryland, 2004.\nA. Agarwal, K. Takatsu, I. Zaitsev, and T. Joachims. A general framework for counterfactual learning-\nto-rank. In Proceedings of the 42nd Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 2019), pages 5–14, Paris, France, 2019.\nE. Agichtein and L. Gravano. Snowball: Extracting relations from large plain-text collections. In\nProceedings of the 5th ACM International Conference on Digital Libraries (DL 2000) , pages\n85–94, San Antonio, Texas, 2000.\nE. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre. SemEval-2012 task 6: A pilot on semantic\ntextual similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation\n(SemEval 2012), pages 385–393, Montréal, Canada, 2012.\nQ. Ai, X. Wang, S. Bruch, N. Golbandi, M. Bendersky, and M. Najork. Learning groupwise\nmultivariate scoring functions using deep neural networks. In Proceedings of the 2019 ACM SIGIR\nInternational Conference on Theory of Information Retrieval, pages 85–92, Santa Clara, California,\n2019.\nZ. Akkalyoncu Yilmaz. Cross-domain sentence modeling for relevance transfer with BERT. Master’s\nthesis, University of Waterloo, 2019.\nZ. Akkalyoncu Yilmaz, S. Wang, and J. Lin. H 2oloo at TREC 2019: Combining sentence and\ndocument evidence in the deep learning track. In Proceedings of the Twenty-Eighth Text REtrieval\nConference (TREC 2019), Gaithersburg, Maryland, 2019a.\nZ. Akkalyoncu Yilmaz, W. Yang, H. Zhang, and J. Lin. Cross-domain modeling of sentence-level\nevidence for document retrieval. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3490–3496, Hong Kong, China, 2019b.\nC. Alberti, K. Lee, and M. Collins. A BERT baseline for the natural questions. arXiv:1901.08634,\n2019.\nJ. Allan. Topic Detection and Tracking: Event-Based Information Organization. Kluwer Academic\nPublishers, Dordrecht, The Netherlands, 2002.\nJ. Allan, B. Carterette, and J. Lewis. When will information retrieval be “good enough”? User\neffectiveness as a function of retrieval accuracy. In Proceedings of the 28th Annual International\nACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005),\npages 433–440, Salvador, Brazil, 2005.\nJ. Allan, D. Harman, E. Kanoulas, D. Li, C. V . Gysel, and E. V oorhees. TREC 2017 common core\ntrack overview. In Proceedings of the Twenty-Sixth Text REtrieval Conference (TREC 2017) ,\nGaithersburg, Maryland, 2017.\nJ. Allan, D. Harman, E. Kanoulas, and E. V oorhees. TREC 2018 common core track overview.\nIn Proceedings of the Twenty-Seventh Text REtrieval Conference (TREC 2018) , Gaithersburg,\nMaryland, 2018.\nU. Alon, R. Sadaka, O. Levy, and E. Yahav. Structural language models of code. arXiv:1910.00577,\n2020.\n173\nH. Alshawi, A. L. Buchsbaum, and F. Xia. A comparison of head transducers and transfer for a limited\ndomain translation application. In Proceedings of the 35th Annual Meeting of the Association for\nComputational Linguistics and 8th Conference of the European Chapter of the Association for\nComputational Linguistics, pages 360–365, Madrid, Spain, 1997.\nG. Amati. Probabilistic Models of Information Retrieval Based on Divergence from Randomness.\nPhD thesis, University of Glasgow, 2003.\nG. Amati and C. J. van Rijsbergen. Probabilistic models of information retrieval based on measuring\nthe divergence from randomness. ACM Transactions on Information Systems, 20(4):357–389,\n2002.\nG. Amati, E. Ambrosi, M. Bianchi, C. Gaibisso, and G. Gambosi. FUB, IASI-CNR and University of\nTor Vergata at TREC 2007 blog track. In Proceedings of the Sixteenth Text REtrieval Conference\n(TREC 2007), Gaithersburg, Maryland, 2007.\nV . N. Anh and A. Moffat. Impact transformation: Effective and efﬁcient web retrieval. InProceedings\nof the 25th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2002), pages 3–10, Tampere, Finland, 2002.\nV . N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. InPro-\nceedings of the 24th Annual International ACM SIGIR Conference on Research and Development\nin Information Retrieval (SIGIR 2001), pages 35–42, New Orleans, Louisiana, 2001.\nT. G. Armstrong, A. Moffat, W. Webber, and J. Zobel. Improvements that don’t add up: Ad-hoc\nretrieval results since 1998. In Proceedings of the 18th International Conference on Information\nand Knowledge Management (CIKM 2009), pages 601–610, Hong Kong, China, 2009.\nS. Arora, Y . Liang, and T. Ma. A simple but tough-to-beat baseline for sentence embeddings. In\nProceedings of the 5th International Conference on Learning Representations (ICLR 2017), Toulon,\nFrance, 2017.\nM. Artetxe, S. Ruder, and D. Yogatama. On the cross-lingual transferability of monolingual rep-\nresentations. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, 2020.\nN. Asadi and J. Lin. Effectiveness/efﬁciency tradeoffs for candidate generation in multi-stage retrieval\narchitectures. In Proceedings of the 36th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 2013), pages 997–1000, Dublin, Ireland, 2013.\nA. Asai, J. Kasai, J. Clark, K. Lee, E. Choi, and H. Hajishirzi. XOR QA: Cross-lingual open-retrieval\nquestion answering. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 547–564, June\n2021.\nA. Axelrod, X. He, and J. Gao. Domain adaptation via pseudo in-domain data selection. In\nProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages\n355–362, Edinburgh, Scotland, 2011.\nL. Azzopardi, M. Crane, H. Fang, G. Ingersoll, J. Lin, Y . Moshfeghi, H. Scells, P. Yang, and\nG. Zuccon. The Lucene for Information Access and Retrieval Research (LIARR) Workshop at\nSIGIR 2017. In Proceedings of the 40th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 2017), pages 1429–1430, Tokyo, Japan, 2017a.\nL. Azzopardi, Y . Moshfeghi, M. Halvey, R. S. Alkhawaldeh, K. Balog, E. Di Buccio, D. Cecca-\nrelli, J. M. Fernández-Luna, C. Hull, J. Mannix, and S. Palchowdhury. Lucene4IR: Developing\ninformation retrieval evaluation resources using Lucene. SIGIR Forum, 50(2):58–75, 2017b.\nJ. Ba and R. Caruana. Do deep nets really need to be deep? In Advances in Neural Information\nProcessing Systems 27 (NIPS 2014), Montréal, Canada, 2014.\nY . Bai, X. Li, G. Wang, C. Zhang, L. Shang, J. Xu, Z. Wang, F. Wang, and Q. Liu. SparTerm:\nLearning term-based sparse representation for fast text retrieval. arXiv:2010.00768, 2020.\n174\nP. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P. de Vries, and E. Yilmaz. Relevance assessment:\nAre judges exchangeable and does it matter? In Proceedings of the 31st Annual International ACM\nSIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008), pages\n667–674, Singapore, 2008.\nP. Bajaj, D. Campos, N. Craswell, L. Deng, J. Gao, X. Liu, R. Majumder, A. McNamara, B. Mitra,\nT. Nguyen, M. Rosenberg, X. Song, A. Stoica, S. Tiwary, and T. Wang. MS MARCO: A Human\nGenerated MAchine Reading COmprehension Dataset. arXiv:1611.09268v3, 2018.\nM. Banko and E. Brill. Scaling to very very large corpora for natural language disambiguation. In\nProceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL\n2001), pages 26–33, Toulouse, France, 2001.\nC. Bannard and C. Callison-Burch. Paraphrasing with bilingual parallel corpora. In Proceedings\nof the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05) , pages\n597–604, Ann Arbor, Michigan, 2005.\nO. Barkan, N. Razin, I. Malkiel, O. Katz, A. Caciularu, and N. Koenigstein. Scalable attentive\nsentence-pair modeling via distilled sentence embedding. In Proceedings of the Thirty-Fourth\nAAAI Conference on Artiﬁcial Intelligence (AAAI-20), pages 3235–3242, New York, New York,\n2020.\nB. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval\nsystems. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 1994), pages 173–181, Dublin, Ireland, 1994.\nP. Baudiš and J. Šedivý. Modeling of the question answering task in the YodaQA system. In Pro-\nceedings of the International Conference of the Cross-Language Evaluation Forum for European\nLanguages, pages 222–228, Toulouse, France, 2015.\nM. Bawa, T. Condie, and P. Ganesan. LSH forest: Self tuning indexes for similarity search. In\nProceedings of the 14th International World Wide Web Conference (WWW 2005), pages 651–660,\nChiba, Japan, 2005.\nBBC. Ask Jeeves bets on smart search, 2004. URLhttp://news.bbc.co.uk/2/hi/technology/\n3686978.stm.\nN. J. Belkin. Anomalous states of knowledge as a basis for information retrieval. Canadian Journal\nof Information Science, 5:133–143, 1980.\nN. J. Belkin and W. B. Croft. Information ﬁltering and information retrieval: Two sides of the same\ncoin? Communications of the ACM, 35(12):29–38, 1992.\nN. J. Belkin, R. N. Oddy, and H. M. Brooks. ASK for information retrieval: Part I. Background and\ntheory. Journal of Documentation, 38(2):61–71, 1982a.\nN. J. Belkin, R. N. Oddy, and H. M. Brooks. ASK for information retrieval: Part II. Results of a\ndesign study. Journal of Documentation, 38(3):145–164, 1982b.\nI. Beltagy, K. Lo, and A. Cohan. SciBERT: A pretrained language model for scientiﬁc text. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n3606–3611, 2019.\nI. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer.\narXiv:2004.05150, 2020.\nM. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In Proceedings of the\n31st Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 2008), pages 491–498, Singapore, 2008.\nM. Bendersky, H. Zhuang, J. Ma, S. Han, K. Hall, and R. McDonald. RRF102: Meeting the\nTREC-COVID challenge with a 100+ runs ensemble. arXiv:2010.00200, 2020.\n175\nY . Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. InProceedings of the\n26th Annual International Conference on Machine Learning (ICML 2009), pages 41–48, Montréal,\nCanada, 2009.\nJ. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on Freebase from question–answer pairs.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing\n(EMNLP 2013), pages 1533–1544, Seattle, Washington, 2013.\nA. Berger and J. Lafferty. Information retrieval as statistical translation. In Proceedings of the\n22nd Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 1999), pages 222–229, Berkeley, California, 1999.\nC. Bhagavatula, S. Feldman, R. Power, and W. Ammar. Content-based citation recommendation.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages\n238–251, New Orleans, Louisiana, 2018.\nP. Boldi and S. Vigna. MG4J at TREC 2005. In Proceedings of the Fourteenth Text REtrieval\nConference (TREC 2005), Gaithersburg, Maryland, 2005.\nL. Boualili, J. G. Moreno, and M. Boughanem. MarkedBERT: Integrating traditional IR cues in\npre-trained language models for passage retrieval. In Proceedings of the 43rd International ACM\nSIGIR Conference on Research and Development in Information Retrieval (SIGIR 2020), pages\n1977–1980, 2020.\nF. Boudin, Y . Gallina, and A. Aizawa. Keyphrase generation for scientiﬁc document retrieval. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages\n1118–1126, 2020.\nS. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning natural\nlanguage inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 632–642, Lisbon, Portugal, 2015.\nL. Boytsov and Z. Kolter. Exploring classic and neural lexical translation models for information\nretrieval: Interpretability, effectiveness, and efﬁciency beneﬁts. In Proceedings of the 43rd\nEuropean Conference on Information Retrieval (ECIR 2021), Part I, pages 63–78, 2021.\nT. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine translation. In\nProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing\nand Computational Natural Language Learning (EMNLP-CoNLL), pages 858–867, Prague, Czech\nRepublic, 2007.\nT. L. Brauen, R. C. Holt, and T. R. Wilcox. Document indexing based on relevance feedback.\nIn G. Salton, editor, Scientiﬁc Report No. ISR-14: Information Storage and Retrieval . Cornell\nUniversity, Ithaca, New York, 1968.\nS. Brin. Extracting patterns and relations from the World Wide Web. In Proceedings of the WebDB\nWorkshop—International Workshop on the Web and Databases, at EDBT ’98, 1998.\nP. F. Brown, J. Cocke, S. D. Pietra, V . J. D. Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S.\nRoossin. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85,\n1990.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-\nshot learners. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pages\n1877–1901, 2020.\nC. Buckley. Implementation of the SMART information retrieval system. Department of Computer\nScience TR 85-686, Cornell University, 1985.\n176\nC. Buckley and E. M. V oorhees. Retrieval evaluation with incomplete information. InProceedings\nof the 27th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2004), pages 25–32, Shefﬁeld, United Kingdom, 2004.\nC. Buckley, D. Dimmick, I. Soboroff, and E. V oorhees. Bias and the limits of pooling for large\ncollections. Information Retrieval, 10(6):491–508, 2007.\nC. J. C. Burges. From RankNet to LambdaRank to LambdaMART: An overview. Technical Report\nMSR-TR-2010-82, Microsoft Research, 2010.\nC. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.\nLearning to rank using gradient descent. In Proceedings of the 22nd International Conference on\nMachine Learning (ICML 2005), pages 89–96, Bonn, Germany, 2005.\nR. D. Burke, K. J. Hammond, V . A. Kulyukin, S. L. Lytinen, N. Tomuro, and S. Schoenberg.\nQuestion answering from frequently-asked question ﬁles: Experiences with the FAQ Finder system.\nTechnical Report TR-97-05, University of Chicago, 1997.\nM. Busch, K. Gade, B. Larson, P. Lok, S. Luckenbill, and J. Lin. Earlybird: Real-time search at\nTwitter. In Proceedings of the 28th International Conference on Data Engineering (ICDE 2012),\npages 1360–1369, Washington, D.C., 2012.\nV . Bush. As we may think.Atlantic Monthly, 176(1):101–108, 1945.\nG. Cabanac, G. Hubert, M. Boughanem, and C. Chrisment. Tie-breaking bias: Effect of an uncon-\ntrolled parameter on information retrieval evaluation. In CLEF 2010: Multilingual and Multimodal\nInformation Access Evaluation, LNCS 6360, pages 112–123, Padua, Italy, 2010.\nJ. P. Callan. Passage-level evidence in document retrieval. In Proceedings of the 17th Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR 1994), pages 302–310, Dublin, Ireland, 1994.\nA. Câmara and C. Hauff. Diagnosing BERT with retrieval heuristics. In Proceedings of the 42nd\nEuropean Conference on Information Retrieval, Part I (ECIR 2020), pages 605–618, 2020.\nB. B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt. Early\nexit optimizations for additive machine learned ranking systems. In Proceedings of the Third ACM\nInternational Conference on Web Search and Data Mining (WSDM 2010), pages 411–420, New\nYork, New York, 2010.\nZ. Cao, T. Qin, T.-Y . Liu, M.-F. Tsai, and H. Li. Learning to rank: From pairwise approach to listwise\napproach. In Proceedings of the 24th International Conference on Machine Learning (ICML 2007),\npages 129–136, Corvalis, Oregon, 2007.\nG. Capannini, C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, and N. Tonellotto. Quality\nversus efﬁciency in document scoring with learning-to-rank models. Information Processing and\nManagement, 52(6):1161–1177, 2016.\nC. Carpineto and G. Romano. A survey of automatic query expansion in information retrieval. ACM\nComputing Surveys, 44(1):Article No. 1, 2012.\nM.-A. Cartright, S. Huston, and H. Feild. Galago: A modular distributed processing and retrieval\nsystem. In Proceedings of the SIGIR 2012 Workshop on Open Source Information Retrieval, pages\n25–31, Portland, Oregon, 2012.\nD. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. SemEval-2017 task 1: Semantic\ntextual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th\nInternational Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancouver, Canada,\n2017.\nD. Cer, Y . Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. St. John, N. Constant, M. Guajardo-Cespedes,\nS. Yuan, C. Tar, B. Strope, and R. Kurzweil. Universal sentence encoder. arXiv:1803.11175,\n2018a.\n177\nD. Cer, Y . Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. St. John, N. Constant, M. Guajardo-Cespedes,\nS. Yuan, C. Tar, B. Strope, and R. Kurzweil. Universal sentence encoder for English. In Pro-\nceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations, pages 169–174, Brussels, Belgium, 2018b.\nW.-C. Chang, F. X. Yu, Y .-W. Chang, Y . Yang, and S. Kumar. Pre-training tasks for embedding-\nbased large-scale retrieval. In Proceedings of the 8th International Conference on Learning\nRepresentations (ICLR 2020), 2020.\nD. Chen and W.-t. Yih. Open-domain question answering. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics: Tutorial Abstracts, pages 34–37, July 2020.\nD. Chen, A. Fisch, J. Weston, and A. Bordes. Reading Wikipedia to answer open-domain questions.\nIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, 2017a.\nJ. Chen, L. Yang, K. Raman, M. Bendersky, J.-J. Yeh, Y . Zhou, M. Najork, D. Cai, and E. Emadzadeh.\nDiPair: Fast and accurate distillation for trillion-scale text matching and pair modeling. InFindings\nof the Association for Computational Linguistics: EMNLP 2020, pages 2925–2937, 2020.\nQ. Chen, X. Zhu, Z.-H. Ling, S. Wei, H. Jiang, and D. Inkpen. Enhanced LSTM for natural language\ninference. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1657–1668, Vancouver, Canada, 2017b.\nR.-C. Chen, L. Gallagher, R. Blanco, and J. S. Culpepper. Efﬁcient cost-aware cascade ranking in\nmulti-stage retrieval. In Proceedings of the 40th Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR 2017), pages 445–454, Tokyo, Japan,\n2017c.\nS. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. In\nProceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL\n1996), pages 310–318, Santa Cruz, California, 1996.\nX. Chen, B. He, K. Hui, L. Sun, and Y . Sun. Simpliﬁed TinyBERT: Knowledge distillation for\ndocument retrieval. In Proceedings of the 43rd European Conference on Information Retrieval\n(ECIR 2021), Part II, pages 241–248, 2021.\nR. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers.\narXiv:1904.10509, 2019.\nE. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y . Choi, P. Liang, and L. Zettlemoyer. QuAC:\nQuestion answering in context. In Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 2174–2184, Brussels, Belgium, 2018.\nJ. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V . Nikolaev, and J. Palomaki. TyDi\nQA: A benchmark for information-seeking question answering in typologically diverse languages.\nTransactions of the Association for Computational Linguistics, 8:454–470, 2020a.\nK. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does BERT look at? An analysis\nof BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 276–286, Florence, Italy, 2019.\nK. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning. ELECTRA: Pre-training text encoders as\ndiscriminators rather than generators. In Proceedings of the 8th International Conference on\nLearning Representations (ICLR 2020), 2020b.\nC. L. A. Clarke, G. Cormack, and E. Tudhope. Relevance ranking for one to three term queries.\nInformation Processing and Management, 36(2):291–311, 2000.\nC. L. A. Clarke, J. S. Culpepper, and A. Moffat. Assessing efﬁciency–effectiveness tradeoffs in\nmulti-stage retrieval systems without using relevance judgments. Information Retrieval, 19(4):\n351–377, 2016.\n178\nG. V . Cormack and M. Mojdeh. Machine learning for information retrieval: TREC 2009 web,\nrelevance feedback and legal tracks. In Proceedings of the Eighteenth Text REtrieval Conference\n(TREC 2009), Gaithersburg, Maryland, 2009.\nG. V . Cormack, C. L. A. Clarke, and S. Büttcher. Reciprocal rank fusion outperforms Condorcet and\nindividual rank learning methods. In Proceedings of the 32nd Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR 2009), pages 758–759,\nBoston, Massachusetts, 2009.\nG. V . Cormack, M. D. Smucker, and C. L. A. Clarke. Efﬁcient and effective spam ﬁltering and\nre-ranking for large web datasets. Information Retrieval, 14(5):441–465, 2011.\nM. Crane, A. Trotman, and R. O’Keefe. Maintaining discriminatory power in quantized indexes.\nIn Proceedings of 22nd International Conference on Information and Knowledge Management\n(CIKM 2013), pages 1221–1224, San Francisco, California, 2013.\nM. Crane, J. S. Culpepper, J. Lin, J. Mackenzie, and A. Trotman. A comparison of document-at-a-time\nand score-at-a-time query evaluation. In Proceedings of the Tenth ACM International Conference\non Web Search and Data Mining (WSDM 2017), pages 201–210, Cambridge, United Kingdom,\n2017.\nN. Craswell, B. Mitra, E. Yilmaz, D. Campos, and E. M. V oorhees. Overview of the TREC 2019\ndeep learning track. arXiv:2003.07820, 2020.\nN. Craswell, B. Mitra, D. Campos, E. Yilmaz, and J. Lin. MS MARCO: Benchmarking ranking\nmodels in the large-data regime. In Proceedings of the 44th Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR 2021), pages 1566–\n1576, 2021a.\nN. Craswell, B. Mitra, E. Yilmaz, and D. Campos. Overview of the TREC 2020 deep learning track.\narXiv:2102.07662, 2021b.\nF. Crestani, M. Lalmas, C. J. van Rijsbergen, and I. Campbell. “Is this document relevant?... probably”:\nA survey of probabilistic models in information retrieval.ACM Computing Surveys, 30(4):528–552,\n1999.\nW. B. Croft and D. J. Harper. Probabilistic models of document retrieval with relevance information.\nJournal of Documentation, 35(4):285–295, 1979.\nY . Cui, W. Che, T. Liu, B. Qin, S. Wang, and G. Hu. Cross-lingual machine reading comprehension.\narXiv:1909.00361, 2019.\nA. M. Dai and Q. V . Le. Semi-supervised sequence learning. In Advances in Neural Information\nProcessing Systems 28 (NIPS 2015), pages 3079–3087, Montréal, Canada, 2015.\nZ. Dai and J. Callan. Context-aware sentence/passage term importance estimation for ﬁrst stage\nretrieval. arXiv:1910.10687, 2019a.\nZ. Dai and J. Callan. Deeper text understanding for IR with contextual neural language model-\ning. In Proceedings of the 42nd Annual International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR 2019), pages 985–988, Paris, France, 2019b.\nZ. Dai and J. Callan. Context-aware document term weighting for ad-hoc search. In Proceedings of\nThe Web Conference 2020 (WWW 2020), pages 1897–1907, 2020.\nZ. Dai, C. Xiong, J. Callan, and Z. Liu. Convolutional neural networks for soft-matching n-grams in\nad-hoc search. In Proceedings of the Eleventh ACM International Conference on Web Search and\nData Mining (WSDM 2018), pages 126–134, Marina Del Rey, California, 2018.\nJ. Dalton, C. Xiong, and J. Callan. CAsT 2019: The conversational assistance track overview.\nIn Proceedings of the Twenty-Eighth Text REtrieval Conference (TREC 2019) , Gaithersburg,\nMaryland, 2019.\nH. T. Dang. Overview of DUC 2005. In Proceedings of the 2005 Document Understanding\nConference (DUC 2005), 2005.\n179\nC. De Boom, S. V . Canneyt, T. Demeester, and B. Dhoedt. Representation learning for very short\ntexts using weighted word embedding aggregation. Pattern Recognition Letters, 80(C):150–156,\n2016.\nJ. Dean and S. Ghemawat. MapReduce: Simpliﬁed data processing on large clusters. In Proceedings\nof the 6th USENIX Symposium on Operating System Design and Implementation (OSDI 2004) ,\npages 137–150, San Francisco, California, 2004.\nS. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent\nsemantic analysis. Journal of the Association for Information Science, 41(6):391–407, 1990.\nM. Dehghani, H. Zamani, A. Severyn, J. Kamps, and W. B. Croft. Neural ranking models with weak\nsupervision. In Proceedings of the 40th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR 2017), pages 65–74, Tokyo, Japan, 2017.\nM. Dehghani, A. Mehrjou, S. Gouws, J. Kamps, and B. Schölkopf. Fidelity-weighted learning.\nIn Proceedings of the 6th International Conference on Learning Representations (ICLR 2018) ,\nVancouver, Canada, 2018.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, 2019.\nE. Dinan, V . Logacheva, V . Malykh, A. Miller, K. Shuster, J. Urbanek, D. Kiela, A. Szlam, I. Serban,\nR. Lowe, S. Prabhumoye, A. W. Black, A. Rudnicky, J. Williams, J. Pineau, M. Burtsev, and\nJ. Weston. The Second Conversational Intelligence Challenge (ConvAI2). arXiv:1902.00098,\n2019.\nL. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M. Zhou, and H.-W. Hon. Uniﬁed\nlanguage model pre-training for natural language understanding and generation. In Advances\nin Neural Information Processing Systems 32 (NeurIPS 2019), pages 13042–13054, Vancouver,\nCanada, 2019.\nC. dos Santos, X. Ma, R. Nallapati, Z. Huang, and B. Xiang. Beyond [CLS] through ranking by\ngeneration. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1722–1727, 2020.\nJ. Du, E. Grave, B. Gunel, V . Chaudhary, O. Celebi, M. Auli, V . Stoyanov, and A. Conneau. Self-\ntraining improves pre-training for natural language understanding. In Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5408–5418, 2021.\nM. Efron, P. Organisciak, and K. Fenlon. Improving retrieval of short texts through document\nexpansion. In Proceedings of the 35th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 2012), pages 911–920, Portland, Oregon, 2012.\nY . Elazar, S. Ravfogel, A. Jacovi, and Y . Goldberg. Amnesic probing: Behavioral explanation\nwith amnesic counterfactuals. Transactions of the Association for Computational Linguistics, 9:\n160–175, 2021.\nA. Elgohary, D. Peskov, and J. Boyd-Graber. Can you unpack that? Learning to rewrite questions-\nin-context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 5918–5924, Hong Kong, China, Nov. 2019.\nD. Erhan, P.-A. Manzagol, Y . Bengio, S. Bengio, and P. Vincent. The difﬁculty of training deep\narchitectures and the effect of unsupervised pre-training. In Proceedings of the 12th International\nConference on Artiﬁcial Intelligence and Statistics (AISTATS 2009), pages 153–160, Clearwater\nBeach, Florida, 2009.\nK. Ethayarajh. Unsupervised random walk sentence embeddings: A strong but simple baseline. In\nProceedings of the Third Workshop on Representation Learning for NLP, pages 91–100, Melbourne,\nAustralia, 2018.\n180\nA. Ettinger. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language\nmodels. Transactions of the Association for Computational Linguistics, 8:34–48, 2020.\nA. Fan, M. Lewis, and Y . Dauphin. Hierarchical neural story generation.arXiv:1805.04833, 2018a.\nY . Fan, J. Guo, Y . Lan, J. Xu, C. Zhai, and X. Cheng. Modeling diverse relevance patterns in ad-hoc\nretrieval. In Proceedings of the 41st Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 2018), pages 375–384, Ann Arbor, Michigan,\n2018b.\nH. Fang and C. Zhai. Semantic term matching in axiomatic approaches to information retrieval. InPro-\nceedings of the 29th Annual International ACM SIGIR Conference on Research and Development\nin Information Retrieval (SIGIR 2006), pages 115–122, Seattle, Washington, 2006.\nH. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. InProceedings of the\n27th Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 2004), pages 49–56, Shefﬁeld, United Kingdom, 2004.\nH. Fang, T. Tao, and C. Zhai. Diagnostic evaluation of information retrieval models. ACM Transac-\ntions on Information Systems, 29(2):1–42, 2011.\nZ. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou.\nCodeBERT: A pre-trained model for programming and natural languages. arXiv:2002.08155,\n2020.\nN. Ferro and G. Silvello. Rank-biased precision reloaded: Reproducibility and generalization. In\nProceedings of the 37th European Conference on Information Retrieval (ECIR 2015) , pages\n768–780, Vienna, Austria, 2015.\nT. Formal, B. Piwowarski, and S. Clinchant. SPLADE: Sparse lexical and expansion model for\nﬁrst stage ranking. In Proceedings of the 44th Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR 2021), pages 2288–2292, 2021a.\nT. Formal, B. Piwowarski, and S. Clinchant. A white box analysis of ColBERT. InProceedings of the\n43rd European Conference on Information Retrieval (ECIR 2021), Part II, pages 257–263, 2021b.\nJ. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In\nProceedings of the 7th International Conference on Learning Representations (ICLR 2019), New\nOrleans, Louisiana, 2019.\nN. Fuhr. Optimum polynomial retrieval functions based on the probability ranking principle. ACM\nTransactions on Information Systems, 7(3):183–204, 1989.\nN. Fuhr. Some common mistakes in IR evaluation, and how they can be avoided. SIGIR Forum, 51\n(3):32–41, 2017.\nG. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. The vocabulary problem in\nhuman-system communication. Communications of the ACM, 30(11):964–971, 1987.\nR. Gaizauskas and A. M. Robertson. Coupling information retrieval and information extraction:\nA new text technology for gathering information from the web. In Proceedings of RIAO 97:\nComputer-Assisted Information Searching on the Internet , pages 356–370, Montréal, Canada,\n1997.\nD. Ganguly, D. Roy, M. Mitra, and G. J. Jones. Word embedding based generalized language model\nfor information retrieval. In Proceedings of the 38th Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval (SIGIR 2015), pages 795–798, Santiago,\nChile, 2015.\nY . Ganjisaffar, R. Caruana, and C. V . Lopes. Bagging gradient-boosted trees for high precision, low\nvariance ranking models. In Proceedings of the 34rd Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval (SIGIR 2011), pages 85–94, Beijing, China,\n2011.\n181\nL. Gao and J. Callan. Is your language model ready for dense representation ﬁne-tuning?\narXiv:2104.08253, 2021a.\nL. Gao and J. Callan. Unsupervised corpus aware language model pre-training for dense passage\nretrieval. arXiv:2108.05540, 2021b.\nL. Gao, Z. Dai, and J. Callan. EARL: Speedup transformer-based rankers with pre-computed\nrepresentation. arXiv:2004.13313, 2020a.\nL. Gao, Z. Dai, and J. Callan. Modularized transfomer-based ranking framework. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n4180–4190, Nov. 2020b.\nL. Gao, Z. Dai, and J. Callan. Understanding BERT rankers under distillation. In Proceedings of the\n2020 ACM SIGIR on International Conference on Theory of Information Retrieval (ICTIR 2020),\npages 149–152, 2020c.\nL. Gao, Z. Dai, Z. Fan, and J. Callan. Complementing lexical retrieval with semantic residual\nembedding. arXiv:2004.13969, 2020d.\nL. Gao, Z. Dai, and J. Callan. Rethink training of BERT rerankers in multi-stage retrieval pipeline.\narXiv:2101.08751, 2021a.\nL. Gao, Z. Dai, and J. Callan. COIL: Revisit exact lexical match in information retrieval with\ncontextualized inverted list. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages\n3030–3042, 2021b.\nL. Gao, Z. Dai, T. Chen, Z. Fan, B. V . Durme, and J. Callan. Complementing lexical retrieval with\nsemantic residual embedding. In Proceedings of the 43rd European Conference on Information\nRetrieval (ECIR 2021), Part I, pages 146–160, 2021c.\nS. Garg, T. Vu, and A. Moschitti. TANDA: Transfer and adapt pre-trained transformer models for\nanswer sentence selection. In Proceedings of the Thirty-Fourth AAAI Conference on Artiﬁcial\nIntelligence (AAAI-20), pages 7780–7788, New York, New York, 2020.\nF. C. Gey. Inferring probability of relevance using the method of logistic regression. In Proceedings\nof the 17th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 1994), pages 222–231, Dublin, Ireland, 1994.\nS. Ghemawat, H. Gobioff, and S.-T. Leung. The Google File System. In Proceedings of the 19th\nACM Symposium on Operating Systems Principles (SOSP 2003), pages 29–43, Bolton Landing,\nNew York, 2003.\nA. Ghias, J. Logan, D. Chamberlin, and B. C. Smith. Query by humming: Musical information\nretrieval in an audio database. In Proceedings of the Third ACM International Conference on\nMultimedia, pages 231–236, San Francisco, California, 1995.\nD. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan. The third PASCAL recognizing textual\nentailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and\nParaphrasing, pages 1–9, Prague, Czech Republic, 2007.\nD. Gildea and D. Jurafsky. Automatic labeling of semantic roles. Computational Linguistics, 28(3):\n245–288, 2001.\nD. Gillick, A. Presta, and G. S. Tomar. End-to-end retrieval in continuous space. arXiv:1811.08008,\n2018.\nA. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimensions via hashing. InProceedings\nof the 25th International Conference on Very Large Data Bases (VLDB 1999), pages 518–529,\nEdinburgh, Scotland, 1999.\nM. R. Grossman and G. V . Cormack. MRG_UWaterloo and WaterlooCormack participation in the\nTREC 2017 common core track. In Proceedings of the Twenty-Sixth Text REtrieval Conference\n(TREC 2017), Gaithersburg, Maryland, 2017.\n182\nY . Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon. Domain-\nspeciﬁc language model pretraining for biomedical natural language processing.arXiv:2007.15779,\n2020.\nJ. Guo, Y . Fan, Q. Ai, and W. B. Croft. A deep relevance matching model for ad-hoc retrieval.\nIn Proceedings of the 25th ACM International on Conference on Information and Knowledge\nManagement (CIKM 2016), pages 55–64, Indianapolis, Indiana, 2016.\nS. Gururangan, A. Marasovi ´c, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. Smith.\nDon’t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pages 8342–8360, 2020.\nK. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language\nmodel pre-training. arXiv:2002.08909, 2020.\nX. Han, Y . Liu, and J. Lin. The simplest thing that can possibly work: (pseudo-)relevance feedback\nvia text classiﬁcation. In Proceedings of the 2021 ACM SIGIR International Conference on the\nTheory of Information Retrieval (ICTIR 2021), 2021.\nD. Harman. Information Retrieval Evaluation. Morgan & Claypool Publishers, 2011.\nD. Harman. Information retrieval: The early years. Foundations and Trends in Information Retrieval,\n13(5):425–577, 2019.\nA. Haviv, J. Berant, and A. Globerson. BERTese: Learning to speak to BERT. In Proceedings of the\n16th Conference of the European Chapter of the Association for Computational Linguistics: Main\nVolume, pages 3618–3623, 2021.\nH. He and J. Lin. Pairwise word interaction modeling with neural networks for semantic similarity\nmeasurement. In Proceedings of the 2016 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 937–948, San\nDiego, California, 2016.\nP. He, X. Liu, J. Gao, and W. Chen. DeBERTa: Decoding-enhanced BERT with disentangled attention.\narXiv:2006.03654, 2020.\nM. A. Hearst and C. Plaunt. Subtopic structuring for full-length document access. In Proceedings\nof the 16th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 1993), pages 56–68, Pittsburgh, Pennsylvania, 1993.\nJ. Henderson. The unstoppable rise of computational linguistics in deep learning. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, pages 6294–6306,\n2020.\nM. Henderson, R. Al-Rfou, B. Strope, Y .-h.Sung, L. Lukacs, R. Guo, S. Kumar, B. Miklos, and\nR. Kurzweil. Efﬁcient natural language response suggestion for Smart Reply. arXiv:1705.00652,\n2017.\nW. R. Hersh, A. Turpin, S. Price, B. Chan, D. Kramer, L. Sacherek, and D. Olson. Do batch and user\nevaluations give the same results? In Proceedings of the 23rd Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR 2000), pages 17–24,\nAthens, Greece, 2000.\nG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv:1503.02531,\n2015.\nS. Hofstätter and A. Hanbury. Let’s measure run time! Extending the IR replicability infrastructure\nto include performance aspects. In Proceedings of the Open-Source IR Replicability Challenge\n(OSIRRC 2019): CEUR Workshop Proceedings Vol-2409, pages 12–16, Paris, France, 2019.\nS. Hofstätter, N. Rekabsaz, C. Eickhoff, and A. Hanbury. On the effect of low-frequency terms\non Neural-IR models. In Proceedings of the 42nd Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval (SIGIR 2019), pages 1137–1140, Paris,\nFrance, 2019.\n183\nS. Hofstätter, M. Zlabinger, and A. Hanbury. TU Wien @ TREC deep learning ’19 – simple\ncontextualization for re-ranking. In Proceedings of the Twenty-Eight Text REtrieval Conference\n(TREC 2019), Gaithersburg, Maryland, 2019.\nS. Hofstätter, S. Althammer, M. Schröder, M. Sertkan, and A. Hanbury. Improving efﬁcient neural\nranking models with cross-architecture knowledge distillation. arXiv:2010.02666, 2020.\nS. Hofstätter, H. Zamani, B. Mitra, N. Craswell, and A. Hanbury. Local self-attention over long\ntext for efﬁcient document retrieval. In Proceedings of the 43rd Annual International ACM\nSIGIR Conference on Research and Development in Information Retrieval (SIGIR 2020), pages\n2021–2024, 2020.\nS. Hofstätter, M. Zlabinger, and A. Hanbury. Interpretable & time-budget-constrained contextual-\nization for re-ranking. In Proceedings of the 24th European Conference on Artiﬁcial Intelligence\n(ECAI 2020), pages 513–520, Santiago de Compostela, Spain, 2020.\nS. Hofstätter, S.-C. Lin, J.-H. Yang, J. Lin, and A. Hanbury. Efﬁciently teaching an effective dense\nretriever with balanced topic aware sampling. In Proceedings of the 44th Annual International\nACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021),\npages 113–122, 2021.\nJ. E. Holmstrom. Section III. Opening plenary session. In The Royal Society Scientiﬁc Information\nConference, 1948.\nA. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text degeneration.\nIn Proceedings of the 7th International Conference on Learning Representations (ICLR 2019) ,\nNew Orleans, Louisiana, 2019.\nN. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe, A. Gesmundo, M. Attariyan,\nand S. Gelly. Parameter-efﬁcient transfer learning for NLP. InProceedings of the 36th International\nConference on Machine Learning (ICML 2019), pages 2790–2799, Long Beach, California, 2019.\nE. M. Housman and E. D. Kaskela. State of the art in selective dissemination of information. IEEE\nTransactions on Engineering Writing and Speech, 13(2):78–83, 1970.\nJ. Howard and S. Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 328–339, Melbourne, Australia, 2018.\nC.-C. Hsu, E. Lind, L. Soldaini, and A. Moschitti. Answer generation for retrieval-based question\nanswering systems. arXiv:2106.00955, 2021.\nJ.-T. Huang, A. Sharma, S. Sun, L. Xia, D. Zhang, P. Pronin, J. Padmanabhan, G. Ottaviano, and\nL. Yang. Embedding-based retrieval in Facebook search. InProceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining (SIGKDD 2020) , pages\n2553–2561, 2020.\nP.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic\nmodels for web search using clickthrough data. In Proceedings of 22nd International Conference\non Information and Knowledge Management (CIKM 2013), pages 2333–2338, 2013.\nK. Hui, A. Yates, K. Berberich, and G. de Melo. PACRR: A position-aware neural IR model for\nrelevance matching. In Proceedings of the 2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1049–1058, Copenhagen, Denmark, 2017.\nK. Hui, A. Yates, K. Berberich, and G. de Melo. Co-PACRR: A context-aware neural IR model for\nad-hoc retrieval. In Proceedings of the Eleventh ACM International Conference on Web Search\nand Data Mining (WSDM 2018), pages 279–287, Marina Del Rey, California, 2018.\nS. Humeau, K. Shuster, M.-A. Lachaux, and J. Weston. Poly-encoders: Transformer architectures and\npre-training strategies for fast and accurate multi-sentence scoring. arXiv:1905.01969v1, 2019.\nS. Humeau, K. Shuster, M.-A. Lachaux, and J. Weston. Poly-encoders: Architectures and pre-training\nstrategies for fast and accurate multi-sentence scoring. In Proceedings of the 8th International\nConference on Learning Representations (ICLR 2020), 2020.\n184\nJ. Hutchins. “The whisky was invisible”, or persistent myths of MT. MT News International, 11:\n17–18, 1995.\nP. Indyk and R. Motwani. Approximate nearest neighbors: Towards removing the curse of dimen-\nsionality. In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, pages\n604–613, Dallas, Texas, 1998.\nM. Iyyer, V . Manjunatha, J. Boyd-Graber, and H. Daumé III. Deep unordered composition rivals\nsyntactic methods for text classiﬁcation. In Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pages 1681–1691, Beijing, China, July 2015.\nG. Izacard and E. Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering. arXiv:2007.01282, 2020.\nG. Izacard, F. Petroni, L. Hosseini, N. D. Cao, S. Riedel, and E. Grave. A memory efﬁcient baseline\nfor open domain question answering. arXiv:2012.15156, 2020.\nJ.-Y . Jiang, M. Zhang, C. Li, M. Bendersky, N. Golbandi, and M. Najork. Semantic text matching\nfor long-form documents. In Proceedings of the 2019 World Wide Web Conference (WWW 2019),\npages 795–806, San Francisco, California, 2019.\nJ.-Y . Jiang, C. Xiong, C.-J. Lee, and W. Wang. Long document ranking with query-directed sparse\ntransformer. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages\n4594–4605, 2020.\nX. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling BERT\nfor natural language understanding. arXiv:1909.10351, 2019.\nR. Jin, A. G. Hauptmann, and C. X. Zhai. Title language model for information retrieval. In Proceed-\nings of the 25th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2002), pages 42–48, Tampere, Finland, 2002.\nT. Joachims. Optimizing search engines using clickthrough data. In Proceedings of the 8th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD 2002),\npages 133–142, Edmonton, Canada, 2002.\nT. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and G. Gay. Evaluating the accuracy\nof implicit feedback from clicks and query reformulations in Web search. ACM Transactions on\nInformation Systems, 25(2):1–27, 2007.\nT. Joachims, A. Swaminathan, and T. Schnabel. Unbiased learning-to-rank with biased feedback. In\nProceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM\n2017), pages 781–789, Cambridge, United Kingdom, 2017.\nJ. Johnson, M. Douze, and H. Jégou. Billion-scale similarity search with GPUs. arXiv:1702.08734,\n2017.\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada, July 2017.\nC. Kamphuis, A. de Vries, L. Boytsov, and J. Lin. Which BM25 do you mean? A large-scale\nreproducibility study of scoring variants. In Proceedings of the 42nd European Conference on\nInformation Retrieval, Part II (ECIR 2020), pages 28–34, 2020.\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,\nand D. Amodei. Scaling laws for neural language models. arXiv:2001.08361, 2020.\nV . Karpukhin, B. O˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage\nretrieval for open-domain question answering. arXiv:2004.04906, 2020a.\n185\nV . Karpukhin, B. O˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage\nretrieval for open-domain question answering. InProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 6769–6781, 2020b.\nM. Kaszkiel and J. Zobel. Passage retrieval revisited. In Proceedings of the 20th Annual International\nACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1997),\npages 178–185, Philadelphia, Pennsylvania, 1997.\nD. Kelly. Methods for evaluating interactive information retrieval systems with users. Foundations\nand Trends in Information Retrieval, 3(1–2):1–224, 2009.\nD. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi.UNIFIED QA:\nCrossing format boundaries with a single QA system. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1896–1907, 2020.\nO. Khattab and M. Zaharia. ColBERT: Efﬁcient and effective passage search via contextualized late\ninteraction over BERT. In Proceedings of the 43rd Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval (SIGIR 2020), pages 39–48, 2020.\nN. Kitaev, Ł. Kaiser, and A. Levskaya. Reformer: The efﬁcient transformer. In Proceedings of the\n8th International Conference on Learning Representations (ICLR 2020), 2020.\nR. Kohavi, R. M. Henne, and D. Sommerﬁeld. Practical guide to controlled experiments on the web:\nListen to your customers not to the HiPPO. InProceedings of the 13th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (SIGKDD 2007), pages 959–967, San Jose,\nCalifornia, 2007.\nO. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky. Revealing the dark secrets of BERT. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China, 2019.\nT. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations, pages 66–71, 2018.\nT. S. Kuhn. The Structure of Scientiﬁc Revolutions. University of Chicago Press, 1962.\nM. J. Kusner, Y . Sun, N. I. Kolkin, and K. Q. Weinberger. From word embeddings to document\ndistances. In Proceedings of the 32nd International Conference on Machine Learning (ICML\n2015), pages 957–966, Lille, France, 2015.\nS. Kuzi, A. Shtok, and O. Kurland. Query expansion using word embeddings. In Proceedings of\n25th International Conference on Information and Knowledge Management (CIKM 2016), pages\n1929–1932, Indianapolis, Indiana, 2016.\nT. Kwiatkowski, J. Palomaki, O. Redﬁeld, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin,\nJ. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le,\nand S. Petrov. Natural Questions: A benchmark for question answering research. Transactions of\nthe Association for Computational Linguistics, 7:452–466, 2019.\nK. L. Kwok. The use of title and cited titles as document representation for automatic classiﬁcation.\nInformation Processing and Management, 11(8-12):201–206, 1975.\nW. Lan and W. Xu. Neural network models for paraphrase identiﬁcation, semantic textual similarity,\nnatural language inference, and question answering. In Proceedings of the 27th International\nConference on Computational Linguistics, pages 3890–3902, Santa Fe, New Mexico, 2018.\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. ALBERT: A lite BERT\nfor self-supervised learning of language representations. In Proceedings of the 8th International\nConference on Learning Representations (ICLR 2020), 2020.\nC. Lassance, T. Formal, and S. Clinchant. Composite code sparse autoencoders for ﬁrst stage\nretrieval. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 2021), pages 2136–2140, 2021.\n186\nV . Lavrenko and W. B. Croft. Relevance-based language models. InProceedings of the 24th Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR 2001), pages 120–127, New Orleans, Louisiana, 2001.\nQ. Le and T. Mikolov. Distributed representations of sentences and documents. In Proceedings of\nthe 31st International Conference on Machine Learning (ICML 2014), pages 1188–1196, Beijing,\nChina, 2014.\nT. Le Scao and A. Rush. How many data points is a prompt worth? In Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2627–2636, 2021.\nC. Lee, K. Cho, and W. Kang. Mixout: Effective regularization to ﬁnetune large-scale pretrained\nlanguage models. In Proceedings of the 8th International Conference on Learning Representations\n(ICLR 2020), 2020a.\nJ. Lee, R. Tang, and J. Lin. What would Elsa do? Freezing layers during transformer ﬁne-tuning.\narXiv:1911.03090, 2019a.\nJ. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. BioBERT: A pre-trained biomedical\nlanguage representation model for biomedical text mining. Bioinformatics, 36(4):1234–1240,\n2020b.\nK. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain question\nanswering. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, pages 6086–6096, Florence, Italy, 2019b.\nF. Leibert, J. Mannix, J. Lin, and B. Hamadani. Automatic management of partitioned, replicated\nsearch services. In Proceedings of the 2nd ACM Symposium on Cloud Computing (SoCC ’11),\nCascais, Portugal, 2011.\nM. E. Lesk and G. Salton. Relevance assessments and retrieval system evaluation. Information\nStorage and Retrieval, 4(4):343–359, 1968.\nD. D. Lewis. The TREC-4 ﬁltering track. In Proceedings of the Fourth Text REtrieval Conference\n(TREC-4), pages 165–180, Gaithersburg, Maryland, 1995.\nM. Lewis, M. Ghazvininejad, G. Ghosh, A. Aghajanyan, S. Wang, and L. Zettlemoyer. Pre-training\nvia paraphrasing. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020) ,\npages 18470–18481, 2020a.\nM. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V . Stoyanov, and L. Zettle-\nmoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7871–7880, 2020b.\nC. Li, Y . Sun, B. He, L. Wang, K. Hui, A. Yates, L. Sun, and J. Xu. NPRF: A neural pseudo relevance\nfeedback framework for ad-hoc information retrieval. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, pages 4482–4491, Brussels, Belgium, 2018.\nC. Li, A. Yates, S. MacAvaney, B. He, and Y . Sun. PARADE: Passage representation aggregation for\ndocument reranking. arXiv:2008.09093, 2020a.\nH. Li. Learning to Rank for Information Retrieval and Natural Language Processing. Morgan &\nClaypool Publishers, 2011.\nH. Li and J. Xu. Semantic matching in search. Foundations and Trends in Information Retrieval, 7\n(5):343–469, 2014.\nZ. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D. Klein, and J. E. Gonzalez. Train large, then compress:\nRethinking model size for efﬁcient training and inference of transformers. In Proceedings of the\n37th International Conference on Machine Learning (ICML 2020), pages 5958–5968, 2020b.\nJ. Lin. Is searching full text more effective than searching abstracts? BMC Bioinformatics, 10:46,\n2009.\n187\nJ. Lin. The neural hype and comparisons against weak baselines. SIGIR Forum, 52(2):40–51, 2018.\nJ. Lin. The neural hype, justiﬁed! A recantation. SIGIR Forum, 53(2):88–93, 2019.\nJ. Lin and M. Efron. Overview of the TREC-2013 microblog track. In Proceedings of the Twenty-\nSecond Text REtrieval Conference (TREC 2013), Gaithersburg, Maryland, 2013.\nJ. Lin and X. Ma. A few brief notes on DeepImpact, COIL, and a conceptual framework for\ninformation retrieval techniques. arXiv:2106.14807, 2021.\nJ. Lin and A. Trotman. Anytime ranking for impact-ordered indexes. In Proceedings of the ACM\nInternational Conference on the Theory of Information Retrieval (ICTIR 2015), pages 301–304,\nNorthampton, Massachusetts, 2015.\nJ. Lin and W. J. Wilbur. PubMed related articles: A probabilistic topic-based model for content\nsimilarity. BMC Bioinformatics, 8:423, 2007.\nJ. Lin and P. Yang. The impact of score ties on repeatability in document ranking. In Proceedings\nof the 42nd Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2019), pages 1125–1128, Paris, France, 2019.\nJ. Lin and Q. Zhang. Reproducibility is a process, not an achievement: The replicability of IR\nreproducibility experiments. In Proceedings of the 42nd European Conference on Information\nRetrieval, Part II (ECIR 2020), pages 43–49, 2020.\nJ. Lin, D. Metzler, T. Elsayed, and L. Wang. Of Ivory and Smurfs: Loxodontan MapReduce\nexperiments for web search. In Proceedings of the Eighteenth Text REtrieval Conference (TREC\n2009), Gaithersburg, Maryland, 2009.\nJ. Lin, M. Efron, Y . Wang, and G. Sherman. Overview of the TREC-2014 microblog track. In\nProceedings of the Twenty-Third Text REtrieval Conference (TREC 2014), Gaithersburg, Maryland,\n2014.\nJ. Lin, A. Roegiest, L. Tan, R. McCreadie, E. V oorhees, and F. Diaz. Overview of the TREC 2016\nreal-time summarization track. In Proceedings of the Twenty-Fifth Text REtrieval Conference\n(TREC 2016), Gaithersburg, Maryland, 2016.\nJ. Lin, J. Mackenzie, C. Kamphuis, C. Macdonald, A. Mallia, M. Siedlaczek, A. Trotman, and\nA. de Vries. Supporting interoperability between open-source search engines with the Common\nIndex File Format. In Proceedings of the 43rd Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR 2020), pages 2149–2152, 2020a.\nJ. Lin, X. Ma, S.-C. Lin, J.-H. Yang, R. Pradeep, and R. Nogueira. Pyserini: A Python toolkit for\nreproducible information retrieval research with sparse and dense representations. In Proceedings\nof the 44th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2021), pages 2356–2362, 2021a.\nS.-C. Lin, J.-H. Yang, and J. Lin. Distilling dense representations for ranking using tightly-coupled\nteachers. arXiv:2010.11386, 2020b.\nS.-C. Lin, J.-H. Yang, and J. Lin. In-batch negatives for knowledge distillation with tightly-coupled\nteachers for dense retrieval. In Proceedings of the 6th Workshop on Representation Learning for\nNLP (RepL4NLP-2021), pages 163–173, 2021b.\nH. Liu, Z. Dai, D. R. So, and Q. V . Le. Pay attention to MLPs.arXiv:2105.08050, 2021.\nJ. Liu, Y . Lin, Z. Liu, and M. Sun. XQA: A cross-lingual open-domain question answering dataset. In\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages\n2358–2368, Florence, Italy, 2019a.\nL. Liu, H. Wang, J. Lin, R. Socher, and C. Xiong. Attentive student meets multi-task teacher:\nImproved knowledge distillation for pretrained models. arXiv:1911.03588, 2019b.\n188\nP. J. Liu, M. Saleh, E. Pot, B. Goodrich, R. Sepassi, Ł. Kaiser, and N. Shazeer. Generating Wikipedia\nby summarizing long sequences. In Proceedings of the 6th International Conference on Learning\nRepresentations (ICLR 2018), Vancouver, Canada, 2018a.\nS. Liu, F. Xiao, W. Ou, and L. Si. Cascade ranking for operational e-commerce search. InProceedings\nof the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n(SIGKDD 2017), pages 1557–1565, Halifax, Canada, 2017.\nT.-Y . Liu. Learning to rank for information retrieval.Foundations and Trends in Information Retrieval,\n3(3):225–331, 2009.\nW. Liu, P. Zhou, Z. Wang, Z. Zhao, H. Deng, and Q. Ju. FastBERT: A self-distilling BERT with\nadaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 6035–6044, 2020.\nY . Liu and M. Lapata. Hierarchical transformers for multi-document summarization. InProceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070–5081,\nFlorence, Italy, 2019.\nY . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and\nV . Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach.arXiv:1907.11692,\n2019c.\nZ. Liu, C. Xiong, M. Sun, and Z. Liu. Entity-Duet neural ranking: Understanding the role of\nknowledge graph semantics in neural information retrieval. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2395–\n2405, Melbourne, Australia, 2018b.\nJ. Lovón-Melgarejo, L. Soulier, K. Pinel-Sauvagnat, and L. Tamine. Studying catastrophic forgetting\nin neural ranking models. In 43rd European Conference on Information Retrieval (ECIR 2021),\n2021.\nR. Lowe, N. Pow, I. Serban, and J. Pineau. The Ubuntu Dialogue Corpus: A large dataset for research\nin unstructured multi-turn dialogue systems. arXiv:1506.08909, 2015.\nS. Lu, C. Xiong, D. He, G. Ke, W. Malik, Z. Dou, P. Bennett, T. Liu, and A. Overwijk. Less is more:\nPre-training a strong siamese encoder using a weak decoder. arXiv:2102.09206, 2021.\nW. Lu, J. Jiao, and R. Zhang. TwinBERT: Distilling knowledge to twin-structured BERT models for\nefﬁcient retrieval. arXiv:2002.06275, 2020.\nY . Luan, J. Eisenstein, K. Toutanova, and M. Collins. Sparse, dense, and attentional representations\nfor text retrieval. arXiv:2005.00181, 2020.\nY . Luan, J. Eisenstein, K. Toutanova, and M. Collins. Sparse, dense, and attentional representations\nfor text retrieval. Transactions of the Association for Computational Linguistics, 9:329–345, 2021.\nH. P. Luhn. The automatic creation of literature abstracts. IBM Journal of Research Development, 2\n(2):159–165, 1958.\nJ. Ma, I. Korotkov, Y . Yang, K. Hall, and R. McDonald. Zero-shot neural passage retrieval via domain-\ntargeted synthetic question generation. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume, pages 1075–1088, 2021a.\nX. Ma, J. Guo, R. Zhang, Y . Fan, X. Ji, and X. Cheng. PROP: Pre-training with representative words\nprediction for ad-hoc retrieval. In Proceedings of the 14th ACM International Conference on Web\nSearch and Data Mining (WSDM 2021), pages 283–291, 2021b.\nX. Ma, K. Sun, R. Pradeep, and J. Lin. A replication study of dense passage retriever.\narXiv:2104.05740, 2021c.\nS. MacAvaney, A. Yates, A. Cohan, and N. Goharian. CEDR: Contextualized embeddings for\ndocument ranking. In Proceedings of the 42nd Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval (SIGIR 2019), pages 1101–1104, Paris,\nFrance, 2019a.\n189\nS. MacAvaney, A. Yates, K. Hui, and O. Frieder. Content-based weak supervision for ad-hoc re-\nranking. In Proceedings of the 42nd International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR 2019), pages 993–996, 2019b.\nS. MacAvaney, A. Cohan, and N. Goharian. SLEDGE: A simple yet effective baseline for coronavirus\nscientiﬁc knowledge search. arXiv:2005.02365, 2020a.\nS. MacAvaney, S. Feldman, N. Goharian, D. Downey, and A. Cohan. ABNIRML: Analyzing the\nbehavior of neural ir models. arXiv:2011.00696, 2020b.\nS. MacAvaney, F. M. Nardini, R. Perego, N. Tonellotto, N. Goharian, and O. Frieder. Efﬁcient\ndocument re-ranking for transformers by precomputing term representations. In Proceedings of the\n43rd Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 2020), pages 49–58, 2020c.\nS. MacAvaney, F. M. Nardini, R. Perego, N. Tonellotto, N. Goharian, and O. Frieder. Expansion via\nprediction of importance with contextualization. In Proceedings of the 43rd International ACM\nSIGIR Conference on Research and Development in Information Retrieval (SIGIR 2020), pages\n1573–1576, 2020d.\nS. MacAvaney, F. M. Nardini, R. Perego, N. Tonellotto, N. Goharian, and O. Frieder. Training\ncurricula for open domain answer re-ranking. In Proceedings of the 43rd International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR 2020), pages 529–538,\n2020e.\nS. MacAvaney, L. Soldaini, and N. Goharian. Teaching a new dog old tricks: Resurrecting multilingual\nretrieval using zero-shot learning. In Proceedings of the 42nd European Conference on Information\nRetrieval, Part II (ECIR 2020), pages 246–254, 2020f.\nC. Macdonald, R. McCreadie, R. L. Santos, and I. Ounis. From puppy to maturity: Experiences\nin developing Terrier. In Proceedings of the SIGIR 2012 Workshop on Open Source Information\nRetrieval, pages 60–63, Portland, Oregon, 2012.\nJ. Mackenzie, S. Culpepper, R. Blanco, M. Crane, C. L. A. Clarke, and J. Lin. Query driven algorithm\nselection in early stage retrieval. In Proceedings of the 11th ACM International Conference on\nWeb Search and Data Mining (WSDM 2018), pages 396–404, Marina Del Rey, California, 2018.\nJ. Mackenzie, Z. Dai, L. Gallagher, and J. Callan. Efﬁciency implications of term weighting for\npassage retrieval. In Proceedings of the 43rd Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR 2020), pages 1821–1824, 2020.\nI. Mackie, J. Dalton, and A. Yates. How deep is your learning: The DL-HARD annotated deep\nlearning dataset. In Proceedings of the 44th Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR 2021), pages 2335–2341, 2021.\nY . A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search using\nhierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 42(4):824–836, 2020.\nA. Mallia, M. Siedlaczek, J. Mackenzie, and T. Suel. PISA: Performant indexes and search for\nacademia. In Proceedings of the Open-Source IR Replicability Challenge (OSIRRC 2019): CEUR\nWorkshop Proceedings Vol-2409, pages 50–56, Paris, France, 2019.\nA. Mallia, O. Khattab, T. Suel, and N. Tonellotto. Learning passage impacts for inverted indexes. In\nProceedings of the 44th Annual International ACM SIGIR Conference on Research and Develop-\nment in Information Retrieval (SIGIR 2021), pages 1723–1727, 2021.\nI. Mani, G. Klein, D. House, and L. Hirschman. SUMMAC: A text summarization evaluation.\nNatural Language Engineering, 8(1):43–68, 2002.\nM. E. Maron and J. L. Kuhns. On relevance, probabilistic indexing and information retrieval. Journal\nof the ACM, 7(3):216–244, 1960.\n190\nY . Matsubara, T. Vu, and A. Moschitti. Reranking for efﬁcient transformer-based answer selection.\nIn Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2020), pages 1577–1580, 2020.\nI. Matveeva, C. Burges, T. Burkard, A. Laucius, and L. Wong. High accuracy retrieval with multiple\nnested ranker. InProceedings of the 29th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 2006), pages 437–444, Seattle, Washington,\n2006.\nR. McDonald, G. Brokos, and I. Androutsopoulos. Deep relevance ranking using enhanced document-\nquery interactions. In Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1849–1860, Brussels, Belgium, 2018.\nM. McTear. Conversational AI: Dialogue Systems, Conversational Agents, and Chatbots. Morgan &\nClaypool Publishers, 2020.\nD. Metzler and W. B. Croft. Combining the language model and inference network approaches to\nretrieval. Information Processing and Management, 40(5):735–750, 2004.\nD. Metzler and W. B. Croft. A Markov random ﬁeld model for term dependencies. In Proceedings\nof the 28th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2005), pages 472–479, Salvador, Brazil, 2005.\nD. Metzler, T. Strohman, H. Turtle, and W. B. Croft. Indri at TREC 2004: Terabyte track. In\nProceedings of the Thirteenth Text REtrieval Conference (TREC 2004), Gaithersburg, Maryland,\n2004.\nT. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words\nand phrases and their compositionality. In Advances in Neural Information Processing Systems 26\n(NIPS 2013), pages 3111–3119, Lake Tahoe, California, 2013a.\nT. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities in continuous space word representations.\nIn Proceedings of the 2013 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia,\n2013b.\nG. Miller. WordNet: A lexical database for English. Communications of the ACM, 38(11):49–51,\n1995.\nB. Mitra and N. Craswell. An introduction to neural information retrieval. Foundations and Trends in\nInformation Retrieval, 13(1):1–126, 2019a.\nB. Mitra and N. Craswell. An updated Duet model for passage re-ranking. arXiv:1903.07666, 2019b.\nB. Mitra, E. Nalisnick, N. Craswell, and R. Caruana. A dual embedding space model for document\nranking. arXiv:1602.01137, 2016.\nB. Mitra, F. Diaz, and N. Craswell. Learning to match using local and distributed representations\nof text for web search. In Proceedings of the 26th International Conference on World Wide Web\n(WWW 2017), pages 1291–1299, Perth, Australia, 2017.\nB. Mitra, C. Rosset, D. Hawking, N. Craswell, F. Diaz, and E. Yilmaz. Incorporating query\nterm independence assumption for efﬁcient retrieval and ranking using deep neural networks.\narXiv:1907.03693, 2019.\nB. Mitra, S. Hofstätter, H. Zamani, and N. Craswell. Conformer-kernel with query term independence\nfor document retrieval. arXiv:2007.10434, 2020.\nA. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM\nTransactions on Information Systems, 27(1):Article 2, 2008.\nI. Mokrii, L. Boytsov, and P. Braslavski. A systematic evaluation of transfer learning and pseudo-\nlabeling with BERT-based ranking models. arXiv:2103.03335, 2021.\n191\nM. Montague and J. A. Aslam. Condorcet fusion for improved retrieval. In Proceedings of the\nEleventh International Conference on Information and Knowledge Management (CIKM 2002),\npages 538–548, McLean, Virginia, 2002.\nW. Morgan, W. Greiff, and J. Henderson. Direct maximization of average precision by hill-climbing,\nwith a comparison to a maximum entropy approach. In Proceedings of HLT-NAACL 2004: Short\nPapers, pages 93–96, Boston, Massachusetts, 2004.\nH. Mühleisen, T. Samar, J. Lin, and A. de Vries. Old dogs are great at new tricks: Column stores\nfor IR prototyping. In Proceedings of the 37th Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR 2014), pages 863–866, Gold Coast,\nAustralia, 2014.\nD. S. Munteanu and D. Marcu. Improving machine translation performance by exploiting non-parallel\ncorpora. Computational Linguistics, 31(4):477–504, 2005.\nT. A. Nakamura, P. H. Calais, D. de Castro Reis, and A. P. Lemos. An anatomy for neural search\nengines. Information Sciences, 480:339–353, 2019.\nE. Nalisnick, B. Mitra, N. Craswell, and R. Caruana. Improving document ranking with dual word\nembeddings. In Proceedings of the 25th International Conference Companion on World Wide Web\n(WWW 2016), pages 83–84, Montréal, Canada, 2016.\nS. Naseri, J. Dalton, A. Yates, and J. Allan. CEQE: Contextualized embeddings for query expansion.\nIn Proceedings of the 43rd European Conference on Information Retrieval (ECIR 2021), Part I,\npages 467–482, 2021.\nT. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng. MS MARCO: A\nHuman Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268v1, 2016.\nR. Nogueira and K. Cho. Passage re-ranking with BERT. arXiv:1901.04085, 2019.\nR. Nogueira and J. Lin. From doc2query to docTTTTTquery, 2019.\nR. Nogueira, W. Yang, K. Cho, and J. Lin. Multi-stage document ranking with BERT.\narXiv:1910.14424, 2019a.\nR. Nogueira, W. Yang, J. Lin, and K. Cho. Document expansion by query prediction.\narXiv:1904.08375, 2019b.\nR. Nogueira, Z. Jiang, R. Pradeep, and J. Lin. Document ranking with a pretrained sequence-to-\nsequence model. In Findings of the Association for Computational Linguistics: EMNLP 2020 ,\npages 708–718, 2020.\nK. D. Onal, Y . Zhang, I. S. Altingovde, M. M. Rahman, P. Karagoz, A. Braylan, B. Dang, H.-L.\nChang, H. Kim, Q. McNamara, A. Angert, E. Banner, V . Khetan, T. McDonnell, A. T. Nguyen,\nD. Xu, B. C. Wallace, M. de Rijke, and M. Lease. Neural information retrieval: At the end of the\nearly years. Information Retrieval, 21(2–3):111–182, 2018.\nI. Ounis, G. Amati, V . Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A high performance\nand scalable information retrieval platform. In Proceedings of the OSIR Workshop, pages 18–25,\n2006.\nI. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview of the TREC-2011 microblog track. In\nProceedings of the Twentieth Text REtrieval Conference (TREC 2011), Gaithersburg, Maryland,\n2011.\nR. Padaki, Z. Dai, and J. Callan. Rethinking query expansion for BERT reranking. In Proceedings of\nthe 42nd European Conference on Information Retrieval, Part II (ECIR 2020), pages 297–304,\n2020.\nH. Padigela, H. Zamani, and W. B. Croft. Investigating the successes and failures of BERT for\npassage re-ranking. arXiv:1905.01758, 2019.\nM. Palmer, D. Gildea, and N. Xue. Semantic Role Labeling. Morgan & Claypool Publishers, 2010.\n192\nL. Pang, Y . Lan, J. Guo, J. Xu, and X. Cheng. A study of MatchPyramid models on ad-hoc retrieval.\narXiv:1606.04648, 2016.\nG. Pass, A. Chowdhury, and C. Torgeson. A picture of search. In Proceedings of the 1st International\nConference on Scalable Information Systems, Hong Kong, China, 2006.\nR. K. Pasumarthi, S. Bruch, X. Wang, C. Li, M. Bendersky, M. Najork, J. Pfeifer, N. Golbandi, R. Anil,\nand S. Wolf. TF-Ranking: Scalable TensorFlow library for learning-to-rank. In Proceedings of\nthe 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n(SIGKDD 2019), pages 2970–2978, Anchorage, Alaska, 2019.\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\nB. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep\nlearning library. In Advances in Neural Information Processing Systems 32 (NeurIPS 2019), pages\n8024–8035, Vancouver, Canada, 2019.\nJ. Pennington, R. Socher, and C. Manning. GloVe: Global vectors for word representation. In\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1532–1543, Doha, Qatar, 2014.\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep\ncontextualized word representations. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 2227–2237, New Orleans, Louisiana, 2018.\nF. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y . Wu, and A. Miller. Language models\nas knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2463–2473, Hong Kong, China, 2019.\nJ. Phang, T. Févry, and S. R. Bowman. Sentence encoders on STILTs: Supplementary Training on\nIntermediate Labeled-data Tasks. arXiv:1811.01088, 2018.\nJ. Phang, I. Calixto, P. M. Htut, Y . Pruksachatkun, H. Liu, C. Vania, K. Kann, and S. R. Bowman.\nEnglish intermediate-task training improves zero-shot cross-lingual transfer too.arXiv:2005.13013,\n2020.\nT. Pires, E. Schlinger, and D. Garrette. How multilingual is multilingual BERT? In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996–5001,\nFlorence, Italy, 2019.\nV . Plachouras, B. He, and I. Ounis. University of Glasgow at TREC2004: Experiments in web,\nrobust and terabyte tracks with Terrier. In Proceedings of the Thirteenth Text REtrieval Conference\n(TREC 2004), Gaithersburg, Maryland, 2004.\nJ. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. InProceedings of\nthe 21st Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 1998), pages 275–281, Melbourne, Australia, 1998.\nR. Pradeep, X. Ma, R. Nogueira, and J. Lin. Scientiﬁc claim veriﬁcation with VerT5erini. In\nProceedings of the 12th International Workshop on Health Text Mining and Information Analysis,\npages 94–103, 2021a.\nR. Pradeep, R. Nogueira, and J. Lin. The expando-mono-duo design pattern for text ranking with\npretrained sequence-to-sequence models. arXiv:2101.05667, 2021b.\nR. Puri and B. Catanzaro. Zero-shot text classiﬁcation with generative language models.\narXiv:1912.10165, 2019.\nY . Qiao, C. Xiong, Z. Liu, and Z. Liu. Understanding the behaviors of BERT in ranking.\narXiv:1904.07531, 2019.\n193\nY . Qu, Y . Ding, J. Liu, K. Liu, R. Ren, W. X. Zhao, D. Dong, H. Wu, and H. Wang. RocketQA:\nAn optimized training approach to dense passage retrieval for open-domain question answering.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 5835–5847, 2021.\nA. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by\ngenerative pre-training. Technical report, OpenAI, 2018.\nF. Radlinski and N. Craswell. A theoretical framework for conversational search. In Proceedings\nof the 2017 Conference on Human Information Interaction and Retrieval (CHIIR 2017) , pages\n117–126, Oslo, Norway, 2017.\nF. Radlinski and T. Joachims. Query chains: Learning to rank from implicit feedback. In Proceedings\nof the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n(KDD 2005), pages 239–248, Chicago, Illinois, 2005.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine\nLearning Research, 21:1–67, 2020.\nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine com-\nprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2383–2392, Austin, Texas, 2016.\nA. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. Ré. Snorkel: Rapid training data creation\nwith weak supervision. Proceedings of the VLDB Endowment, 11(3):269–282, 2017.\nS. D. Ravana and A. Moffat. Score aggregation techniques in retrieval experimentation. In Pro-\nceedings of the 20th Australasian Database Conference (ADC 2009), Wellington, New Zealand,\n2009.\nReadWrite. Google gets smarter & says there’s more to come, 2010. URL https://readwrite.\ncom/2010/05/05/google_gets_smarter_says_theres_more_to_come/.\nN. Reimers and I. Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\npages 3982–3992, Hong Kong, China, Nov. 2019.\nX. Ren, J. Liu, X. Yu, U. Khandelwal, Q. Gu, L. Wang, and J. Han. ClusCite: Effective citation\nrecommendation by information network-based clustering. In Proceedings of the 20th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, pages 821–830,\nNew York, New York, 2014.\nP. Resnik and N. A. Smith. The web as a parallel corpus. Computational Linguistics, 29(3):349–380,\n2003.\nE. Riloff. Automatically generating extraction patterns from untagged text. In Proceedings of the\nThirteenth National Conference on Artiﬁcial Intelligence and Eighth Innovative Applications of\nArtiﬁcial Intelligence Conference (AAAI/IAAI 1996), pages 1044–1049, Portland, Oregon, 1996.\nK. Roberts, T. Alam, S. Bedrick, D. Demner-Fushman, K. Lo, I. Soboroff, E. V oorhees, L. L. Wang,\nand W. R. Hersh. TREC-COVID: Rationale and structure of an information retrieval shared task\nfor COVID-19. Journal of the American Medical Informatics Association, 29(7):1431–1436, 2020.\nS. Robertson. The probability ranking principle in IR. Journal of Documentation, 33(4):294–304,\n1977.\nS. Robertson and I. Soboroff. The TREC 2002 ﬁltering track report. In Proceedings of the Eleventh\nText REtrieval Conference (TREC 2002), Gaithersburg, Maryland, 2002.\nS. Robertson and K. Spark Jones. Relevance weighting of search terms. Journal of the American\nSociety for Information Science, 27(3):129–146, 1976.\n194\nS. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Founda-\ntions and Trends in Information Retrieval, 3(4):333–389, 2009.\nS. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. Okapi at TREC-3.\nIn Proceedings of the 3rd Text REtrieval Conference (TREC-3), pages 109–126, Gaithersburg,\nMaryland, 1994.\nJ. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART\nRetrieval System—Experiments in Automatic Document Processing , pages 313–323. Prentice-Hall,\nEnglewood Cliffs, New Jersey, 1971.\nA. Rogers, O. Kovaleva, and A. Rumshisky. A primer in BERTology: What we know about how\nBERT works. Transactions of the Association for Computational Linguistics, 8:842–866, 2020.\nS. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson, Y . Liu, J. Xu, M. Ott, K. Shuster, E. M. Smith,\nY .-L. Boureau, and J. Weston. Recipes for building an open-domain chatbot.arXiv:2004.13637,\n2020.\nR. Rosenthal. The “ﬁle drawer problem” and tolerance for null results. Psychological Bulletin, 86(3):\n638–641, 1979.\nB. R. Rowe, D. W. Wood, A. N. Link, and D. A. Simoni. Economic impact assessment of NIST’s\nText REtrieval Conference (TREC) program: Final report. RTI Project Number 0211875, RTI\nInternational, 2010.\nD. Roy, M. Mitra, and D. Ganguly. To clean or not to clean: Document preprocessing and repro-\nducibility. Journal of Data and Information Quality, 10(4):Article 18, 2018.\nE. B. Sadler. Project Blacklight: A next generation library catalog at a ﬁrst generation university.\nLibrary Hi Tech, 27(1):57–67, 2009.\nT. Sakai. Alternatives to bpref. In Proceedings of the 30th Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR 2007), pages 71–78,\nAmsterdam, The Netherlands, 2007.\nT. Sakai. Statistical reform in information retrieval? SIGIR Forum, 48(1):3–12, 2014.\nJ. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff. Masked language model scoring. InProceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712,\n2020.\nG. Salton. Automatic content analysis in information retrieval. Technical Report TR68-5, Cornell\nUniversity, Department of Computer Science, January 1968.\nG. Salton. A new comparison between conventional indexing (MEDLARS) and automatic text\nprocessing (SMART). Journal of the American Society for Information Science , 23(2):75–84,\n1972.\nG. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information\nProcessing and Management, 24(5):513–523, 1988a.\nG. Salton and C. Buckley. On the use of spreading activation methods in automatic information. In\nProceedings of the 11th Annual International ACM SIGIR Conference on Research and Develop-\nment in Information Retrieval (SIGIR 1988), pages 147–160, Grenoble, France, 1988b.\nG. Salton and M. E. Lesk. Computer evaluation of indexing and text processing. Journal of the ACM,\n15(1):8–36, 1968.\nG. Salton, Y . Wong, and C.-S. Yang. A vector space model for automatic indexing.Communications\nof the ACM, 18(11):613–620, November 1975.\nG. Salton, J. Allan, and C. Buckley. Approaches to passage retrieval in full text information\nsystems. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 1993), pages 49–58, Pittsburgh, Pennsylvania,\n1993.\n195\nM. Sanderson and J. Zobel. Information retrieval system evaluation: Effort, sensitivity, and relia-\nbility. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR 2005), pages 162–169, Salvador, Brazil, 2005.\nV . Sanh, L. Debut, J. Chaumond, and T. Wolf. DistilBERT, a distilled version of BERT: Smaller,\nfaster, cheaper and lighter. In Proceedings of the 5th Workshop on Energy Efﬁcient Machine\nLearning and Cognitive Computing at NeurIPS 2019, Vancouver, Canada, 2019.\nT. Saracevic. Relevance: A review of and a framework for thinking on the notion in information\nscience. Journal of the American Society for Information Science, 26(6):321–343, 1975.\nT. Saracevic. The Notion of Relevance in Information Science. Morgan & Claypool Publishers, 2017.\nT. Schick and H. Schütze. Exploiting cloze-questions for few-shot text classiﬁcation and natural\nlanguage inference. In Proceedings of the 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main Volume, pages 255–269, 2021.\nR. Schwartz, G. Stanovsky, S. Swayamdipta, J. Dodge, and N. A. Smith. The right tool for the job:\nMatching model and instance complexities. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 6640–6651, 2020.\nH. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions\non Information Theory, 11(3):363–371, 1965.\nI. Sekuli´c, A. Soleimani, M. Aliannejadi, and F. Crestani. Longformer for MS MARCO document\nre-ranking task. arXiv:2009.09392, 2020.\nR. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword\nunits. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1715–1725, Berlin, Germany, 2016.\nX. Shan, C. Liu, Y . Xia, Q. Chen, Y . Zhang, A. Luo, and Y . Luo. BISON: BM25-weighted self-\nattention framework for multi-ﬁelds document search. arXiv:2007.05186, 2020.\nD. Shen, G. Wang, W. Wang, M. R. Min, Q. Su, Y . Zhang, C. Li, R. Henao, and L. Carin. Baseline\nneeds more love: On simple word-embedding-based models and associated pooling mechanisms.\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 440–450, Melbourne, Australia, 2018.\nW. Shen, J. Wang, and J. Han. Entity linking with a knowledge base: Issues, techniques, and solutions.\nIEEE Transactions on Knowledge and Data Engineering, 27(2):443–460, 2015.\nY . Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent semantic model with convolutional-pooling\nstructure for information retrieval. In Proceedings of 23rd International Conference on Information\nand Knowledge Management (CIKM 2014), pages 101–110, Shanghai, China, 2014.\nP. Shi and J. Lin. Cross-lingual relevance transfer for document retrieval. arXiv:1911.02989, 2019.\nP. Shi, H. Bai, and J. Lin. Cross-lingual training of neural models for document ranking. In Findings\nof the Association for Computational Linguistics: EMNLP 2020, pages 2768–2773, 2020.\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-LM: Training\nmulti-billion parameter language models using model parallelism. arXiv:1909.08053, 2019.\nR. F. Simmons. Answering English questions by computer: A survey. Communications of the ACM,\n8(1):53–70, 1965.\nA. Singhal and F. Pereira. Document expansion for speech retrieval. In Proceedings of the 22nd\nAnnual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 1999), pages 34–41, Berkeley, California, 1999.\nA. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proceedings of the\n19th Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 1996), pages 21–29, Zürich, Switzerland, 1996.\n196\nA. Smirnova and P. Cudré-Mauroux. Relation extraction using distant supervision: A survey. ACM\nComputing Survey, 51(5):106:1–106:35, 2018.\nJ. R. Smith, C. Quirk, and K. Toutanova. Extracting parallel sentences from comparable corpora\nusing document level alignment. In Human Language Technologies: The 2010 Annual Conference\nof the North American Chapter of the Association for Computational Linguistics, pages 403–411,\nLos Angeles, California, 2010.\nM. D. Smucker and J. Allan. Find-Similar: Similarity browsing as a search tool. InProceedings of the\n29th Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 2006), pages 461–468, Seattle, Washington, 2006.\nI. Soboroff, I. Ounis, C. Macdonald, and J. Lin. Overview of the TREC-2012 microblog track. In\nProceedings of the Twenty-First Text REtrieval Conference (TREC 2012), Gaithersburg, Maryland,\n2012.\nI. Soboroff, S. Huang, and D. Harman. TREC 2018 news track overview. In Proceedings of the\nTwenty-Seventh Text REtrieval Conference Proceedings (TREC 2018), Gaithersburg, Maryland,\n2018.\nR. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep\nmodels for semantic compositionality over a sentiment treebank. In Proceedings of the 2013\nConference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle,\nWashington, 2013.\nL. Soldaini and A. Moschitti. The Cascade Transformer: An application for efﬁcient answer sentence\nselection. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 5697–5708, 2020.\nE. Sormunen. Liberal relevance criteria of TREC—counting on negligible documents? In Proceedings\nof the 25th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2002), pages 324–330, Tampere, Finland, 2002.\nK. Sparck Jones and C. J. van Rijsbergen. Report on the need for and provision of an “ideal”\ninformation retrieval test collection. British Library Research and Development Report 5266,\nComputer Laboratory, University of Cambridge, 1975.\nA. Spink and H. Greisdorf. Regions and levels: Mapping and measuring users’ relevance judgments.\nJournal of the American Society for Information Science and Technology, 52(2):161–173, 2001.\nI. Srba and M. Bielikova. A comprehensive survey and classiﬁcation of approaches for community\nquestion answering. ACM Transactions on the Web, 10(3):Article No. 18, 2016.\nS. Subramanian, R. Li, J. Pilault, and C. Pal. On extractive and abstractive neural document\nsummarization with transformer language models. arXiv:1909.03186, 2019.\nS. Sun, Y . Cheng, Z. Gan, and J. Liu. Patient knowledge distillation for BERT model compression.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),\npages 4323–4332, Hong Kong, China, 2019a.\nY . Sun, S. Wang, Y . Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu, H. Tian, and H. Wu. ERNIE:\nEnhanced representation through knowledge integration. arXiv:1904.09223, 2019b.\nA. V . Tahami, K. Ghajar, and A. Shakery. Distilling knowledge for fast retrieval-based chat-bots.\narXiv:2004.11045, 2020.\nK. S. Tai, R. Socher, and C. D. Manning. Improved semantic representations from tree-structured\nlong short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1556–1566, Beijing, China, 2015.\nR. Tang, Y . Lu, L. Liu, L. Mou, O. Vechtomova, and J. Lin. Distilling task-speciﬁc knowledge from\nBERT into simple neural networks. arXiv:1903.12136, 2019.\n197\nY . Tay, M. Dehghani, D. Bahri, and D. Metzler. Efﬁcient transformers: A survey.arXiv:2009.06732,\n2020.\nY . Tay, M. Dehghani, J. P. Gupta, V . Aribandi, D. Bahri, Z. Qin, and D. Metzler. Are pretrained\nconvolutions better than pretrained transformers? In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 4349–4359, 2021.\nR. S. Taylor. The process of asking questions. American Documentation, 13(4):391–396, 1962.\nW. L. Taylor. Cloze procedure: A new tool for measuring readability. Journalism Bulletin, 30(4):\n415–433, 1953.\nI. Tenney, D. Das, and E. Pavlick. BERT rediscovers the classical NLP pipeline. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593–4601,\nFlorence, Italy, 2019.\nN. Thakur, N. Reimers, A. Rücklé, A. Srivastava, and I. Gurevych. BEIR: A heterogenous benchmark\nfor zero-shot evaluation of information retrieval models. arXiv:2104.08663, 2021.\nJ. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal. FEVER: A large-scale dataset for\nFact Extraction and VERiﬁcation. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 809–819, New Orleans, Louisiana, 2018.\nJ. Tiedemann. Bitext Alignment. Morgan & Claypool Publishers, 2011.\nN. Tonellotto, C. Macdonald, and I. Ounis. Efﬁcient and effective retrieval using selective pruning. In\nProceedings of the Sixth ACM International Conference on Web Search and Data Mining (WSDM\n2013), pages 63–72, Rome, Italy, 2013.\nA. Trotman and M. Crane. Micro- and macro-optimizations of SAAT search. Software: Practice and\nExperience, 49(5):942–950, 2019.\nA. Trotman and D. Jenkinson. IR evaluation using multiple assessors per topic. In Proceedings of\nthe Twelfth Australasian Document Computing Symposium (ADCS ’07), pages 9–16, Melbourne,\nAustralia, 2007.\nA. Trotman, X.-F. Jia, and M. Crane. Towards an efﬁcient and effective search engine. InProceedings\nof the SIGIR 2012 Workshop on Open Source Information Retrieval, pages 40–47, Portland, Oregon,\n2012.\nA. Trotman, A. Puurula, and B. Burgess. Improvements to BM25 and language models examined. In\nProceedings of the 2014 Australasian Document Computing Symposium (ADCS ’14), pages 58–66,\nMelbourne, Australia, 2014.\nZ. Tu, W. Yang, Z. Fu, Y . Xie, L. Tan, K. Xiong, M. Li, and J. Lin. Approximate nearest neighbor\nsearch and lightweight dense vector reranking in multi-stage retrieval architectures. InProceedings\nof the 2020 ACM SIGIR International Conference on the Theory of Information Retrieval (ICTIR\n2020), pages 97–100, 2020.\nI. Turc, M.-W. Chang, K. Lee, and K. Toutanova. Well-read students learn better: On the importance\nof pre-training compact models. arXiv:1908.08962, 2019.\nF. Ture and J. Lin. Why not grab a free lunch? Mining large corpora for parallel sentences to improve\ntranslation modeling. In Proceedings of the 2012 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pages 626–630,\nMontréal, Canada, 2012.\nF. Ture, T. Elsayed, and J. Lin. No free lunch: Brute force vs. locality-sensitive hashing for cross-\nlingual pairwise similarity. InProceedings of the 34th Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval (SIGIR 2011), pages 943–952, Beijing,\nChina, 2011.\n198\nJ. Uszkoreit, J. Ponte, A. Popat, and M. Dubiner. Large scale parallel document mining for machine\ntranslation. In Proceedings of the 23rd International Conference on Computational Linguistics\n(COLING 2010), pages 1101–1109, Beijing, China, 2010.\nS. Vadrevu, C. H. Teo, S. Rajan, K. Punera, B. Dom, A. Smola, Y . Chang, and Z. Zheng. Scalable\nclustering of news search results. In Proceedings of the Fourth ACM International Conference on\nWeb Search and Data Mining (WSDM 2011), pages 675–683, Hong Kong, China, 2011.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in Neural Information Processing Systems 30 (NIPS 2017),\npages 5998–6008, Long Beach, California, 2017.\nC. C. V ogt and G. W. Cottrell. Fusion via a linear combination of scores.Information Retrieval, 1(3):\n151–173, 1999.\nE. V oita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing multi-head self-attention:\nSpecialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, pages 5797–5808, Florence, Italy, 2019.\nE. M. V oorhees. Query expansion using lexical-semantic relations. InProceedings of the 17th Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR 1994), pages 61–69, Dublin, Ireland, 1994.\nE. M. V oorhees. Variations in relevance judgments and the measurement of retrieval effectiveness.\nInformation Processing and Management, 36(5):697–716, 2000.\nE. M. V oorhees. Overview of the TREC 2001 question answering track. InProceedings of the Tenth\nText REtrieval Conference (TREC 2001), pages 42–51, Gaithersburg, Maryland, 2001.\nE. M. V oorhees. Overview of the TREC 2004 robust track. In Proceedings of the Thirteenth Text\nREtrieval Conference (TREC 2004), pages 52–69, Gaithersburg, Maryland, 2004.\nE. M. V oorhees. On building fair and reusable test collections using bandit techniques. InProceedings\nof the 27th ACM International Conference on Information and Knowledge Management (CIKM\n2018), pages 407–416, Torino, Italy, 2018.\nE. M. V oorhees and D. Harman. Overview of the Seventh Text REtrieval Conference (TREC-7). In\nProceedings of the 7th Text REtrieval Conference (TREC-7), pages 1–24, Gaithersburg, Maryland,\n1998.\nE. M. V oorhees and D. K. Harman. TREC: Experiment and Evaluation in Information Retrieval .\nMIT Press, 2005.\nE. M. V oorhees and Y .-W. Hou. Vector expansion in a large collection. InProceedings of the First\nText REtrieval Conference (TREC-1), pages 343–351, 1993.\nE. M. V oorhees, T. Alam, S. Bedrick, D. Demner-Fushman, W. R. Hersh, K. Lo, K. Roberts,\nI. Soboroff, and L. L. Wang. TREC-COVID: Constructing a pandemic information retrieval test\ncollection. SIGIR Forum, 54(1):1–12, 2020.\nD. Vrandeˇci´c and M. Krötzsch. Wikidata: A free collaborative knowledgebase. Commuications of\nthe ACM, 57(10):78–85, 2014.\nI. Vuli´c and M.-F. Moens. Monolingual and cross-lingual information retrieval models based on\n(bilingual) word embeddings. In Proceedings of the 38th Annual International ACM SIGIR\nConference on Research and Development in Information Retrieval (SIGIR 2015), pages 363–372,\nSantiago, Chile, 2015.\nS. Walker, S. E. Robertson, M. Boughanem, G. J. Jones, and K. S. Jones. Okapi at TREC-6 automatic\nad hoc, VLC, routing, ﬁltering and QSDR. In Proceedings of the Ninth Text REtrieval Conference\n(TREC-6), pages 125–136, Gaithersburg, Maryland, 1997.\nH. Wang and D. McAllester. On-the-ﬂy information retrieval augmentation for language models.\nIn Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events,\npages 114–119, 2020.\n199\nL. Wang, J. Lin, and D. Metzler. Learning to efﬁciently rank. In Proceedings of the 33rd Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR 2010), pages 138–145, Geneva, Switzerland, 2010.\nL. Wang, J. Lin, and D. Metzler. A cascade ranking model for efﬁcient ranked retrieval. InProceedings\nof the 34th Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 2011), pages 105–114, Beijing, China, 2011.\nL. L. Wang, K. Lo, Y . Chandrasekhar, R. Reas, J. Yang, D. Burdick, D. Eide, K. Funk, Y . Katsis,\nR. Kinney, Y . Li, Z. Liu, W. Merrill, P. Mooney, D. Murdick, D. Rishi, J. Sheehan, Z. Shen, B. Stil-\nson, A. Wade, K. Wang, N. X. R. Wang, C. Wilhelm, B. Xie, D. Raymond, D. S. Weld, O. Etzioni,\nand S. Kohlmeier. CORD-19: The COVID-19 Open Research Dataset. arXiv:2004.10706, 2020a.\nS. Wang and J. Jiang. A compare-aggregate model for matching text sequences. In Proceedings of\nthe 5th International Conference on Learning Representations (ICLR 2017), 2017.\nX. Wang, C. Macdonald, and I. Ounis. Deep reinforced query reformulation for information retrieval.\narXiv:2007.07987, 2020b.\nY . Wang and J. Lin. The feasibility of brute force scans for real-time tweet search. InProceedings of\nthe ACM International Conference on the Theory of Information Retrieval (ICTIR 2015), pages\n321–324, Northampton, Massachusetts, 2015.\nY . Wang, G. Sherman, J. Lin, and M. Efron. Assessor differences and user preferences in tweet\ntimeline generation. In Proceedings of the 38th Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval (SIGIR 2015), pages 615–624, Santiago,\nChile, 2015.\nX. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In Proceedings of the\n29th Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 2006), pages 178–185, Seattle, Washington, 2006.\nJ. Wieting, M. Bansal, K. Gimpel, and K. Livescu. Towards universal paraphrastic sentence embed-\ndings. In Proceedings of the 4th International Conference on Learning Representations (ICLR\n2016), San Juan, Puerto Rico, 2016.\nW. J. Wilbur. Global term weights for document retrieval learned from TREC data. Journal of\nInformation Science, 27(5):303–310, 2001.\nR. Wilkinson. Effective retrieval of structured documents. In Proceedings of the 17th Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR 1994), pages 311–317, Dublin, Ireland, 1994.\nA. Williams, N. Nangia, and S. Bowman. A broad-coverage challenge corpus for sentence under-\nstanding through inference. In Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers), pages 1112–1122, New Orleans, Louisiana, 2018.\nT. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T. Le Scao,\nS. Gugger, M. Drame, Q. Lhoest, and A. Rush. Transformers: State-of-the-art natural language\nprocessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 38–45, 2020.\nS. K. M. Wong, Y . J. Cai, and Y . Y . Yao. Computation of term association by a neural network. InPro-\nceedings of the 16th Annual International ACM SIGIR Conference on Research and Development\nin Information Retrieval (SIGIR 1993), pages 107–115, Pittsburgh, Pennsylvania, 1993.\nL. Wu, A. Fisch, S. Chopra, K. Adams, A. Bordes, and J. Weston. StarSpace: Embed all the things!\nIn Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence (AAAI 2018), pages\n5569–5577, New Orleans, Louisiana, 2018a.\n200\nL. Wu, I. E.-H. Yen, K. Xu, F. Xu, A. Balakrishnan, P.-Y . Chen, P. Ravikumar, and M. J. Witbrock.\nWord mover’s embedding: From Word2Vec to document embedding. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pages 4524–4534, Brussels,\nBelgium, 2018b.\nQ. Wu, C. Xing, Y . Li, G. Ke, D. He, and T.-Y . Liu. Taking notes on the ﬂy helps BERT pre-training.\narXiv:2008.01466, 2020a.\nS. Wu and M. Dredze. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China, 2019.\nY . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey, M. Krikun, Y . Cao, Q. Gao,\nK. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, Ł. Kaiser, S. Gouws, Y . Kato, T. Kudo,\nH. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick,\nO. Vinyals, G. Corrado, M. Hughes, and J. Dean. Google’s neural machine translation system:\nBridging the gap between human and machine translation. arXiv:1609.08144, 2016.\nZ. Wu, J. Mao, Y . Liu, J. Zhan, Y . Zheng, M. Zhang, and S. Ma. Leveraging passage-level cumulative\ngain for document ranking. In Proceedings of The Web Conference 2020 (WWW 2020), pages\n2421–2431, 2020b.\nY . Xie, W. Yang, L. Tan, K. Xiong, N. J. Yuan, B. Huai, M. Li, and J. Lin. Distant supervision\nfor multi-stage ﬁne-tuning in retrieval-based question answering. In Proceedings of The Web\nConference 2020 (WWW 2020), pages 2934–2940, 2020.\nJ. Xin, R. Tang, J. Lee, Y . Yu, and J. Lin. DeeBERT: Dynamic early exiting for accelerating BERT\ninference. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics (ACL 2020), pages 2246–2251, 2020.\nC. Xiong, Z. Dai, J. Callan, Z. Liu, and R. Power. End-to-end neural ad-hoc ranking with kernel\npooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR 2017), pages 55–64, Tokyo, Japan, 2017.\nC. Xiong, Z. Liu, S. Sun, Z. Dai, K. Zhang, S. Yu, Z. Liu, H. Poon, J. Gao, and P. Bennett. CMT in\nTREC-COVID round 2: Mitigating the generalization gaps from web to special domain search.\narXiv:2011.01580, 2020a.\nL. Xiong, C. Xiong, Y . Li, K.-F. Tang, J. Liu, P. Bennett, J. Ahmed, and A. Overwijk. Approximate\nnearest neighbor negative contrastive learning for dense text retrieval. arXiv:2007.00808, 2020b.\nL. Xiong, C. Xiong, Y . Li, K.-F. Tang, J. Liu, P. Bennett, J. Ahmed, and A. Overwijk. Approximate\nnearest neighbor negative contrastive learning for dense text retrieval. In Proceedings of the 9th\nInternational Conference on Learning Representations (ICLR 2021), 2021.\nJ. Xu and W. B. Croft. Improving the effectiveness of information retrieval with local context analysis.\nACM Transactions on Information Systems, 18(1):79–112, 2000.\nJ. Xu, X. He, and H. Li. Deep learning for matching in search and recommendation. Foundations\nand Trends in Information Retrieval, 14(2–3):102–288, 2020.\nZ. E. Xu, K. Q. Weinberger, and O. Chapelle. The greedy miser: Learning under test-time budgets. In\nProceedings of the 29th International Conference on Machine Learning (ICML 2012), Edinburgh,\nScotland, 2012.\nI. Yamada, A. Asai, and H. Hajishirzi. Efﬁcient passage retrieval with hashing for open-domain ques-\ntion answering. In Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume\n2: Short Papers), pages 979–986, 2021.\nM. Yan, C. Li, C. Wu, B. Bi, W. Wang, J. Xia, and L. Si. IDST at TREC 2019 deep learning track:\nDeep cascade ranking with generation-based document expansion and pre-trained language model-\ning. In Proceedings of the Twenty-Eighth Text REtrieval Conference (TREC 2019), Gaithersburg,\nMaryland, 2019.\n201\nM. Yan, C. Li, B. Bi, W. Wang, and S. Huang. A uniﬁed pretraining framework for passage ranking\nand expansion. In Proceedings of the Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence (AAAI\n2021), pages 4555–4563, 2021.\nH.-W. Yang, Y . Zou, P. Shi, W. Lu, J. Lin, and X. Sun. Aligning cross-lingual entities with multi-\naspect information. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4422–4432, Hong Kong, China, 2019a.\nJ.-H. Yang, S.-C. Lin, R. Nogueira, M.-F. Tsai, C.-J. Wang, and J. Lin. Designing templates for\neliciting commonsense knowledge from pretrained sequence-to-sequence models. In Proceedings\nof the 28th International Conference on Computational Linguistics, pages 3449–3453, 2020a.\nL. Yang, M. Zhang, C. Li, M. Bendersky, and M. Najork. Beyond 512 tokens: Siamese multi-\ndepth transformer-based hierarchical encoder for document matching. In Proceedings of the 29th\nACM International Conference on Information and Knowledge Management (CIKM 2020), pages\n1725–1734, 2020b.\nL. Yang, M. Zhang, C. Li, M. Bendersky, and M. Najork. Beyond 512 tokens: Siamese multi-depth\ntransformer-based hierarchical encoder for document matching. arXiv:2004.12297, 2020c.\nP. Yang and J. Lin. Reproducing and generalizing semantic term matching in axiomatic information\nretrieval. In Proceedings of the 41th European Conference on Information Retrieval, Part I (ECIR\n2019), pages 369–381, Cologne, Germany, 2019.\nP. Yang, H. Fang, and J. Lin. Anserini: Enabling the use of Lucene for information retrieval\nresearch. In Proceedings of the 40th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR 2017), pages 1253–1256, Tokyo, Japan, 2017.\nP. Yang, H. Fang, and J. Lin. Anserini: Reproducible ranking baselines using Lucene. Journal of\nData and Information Quality, 10(4):Article 16, 2018.\nS. Yang and M. Seo. Is retriever merely an approximator of reader? arXiv:2010.10999, 2020.\nW. Yang, K. Lu, P. Yang, and J. Lin. Critically examining the “neural hype”: Weak baselines and the\nadditivity of effectiveness gains from neural ranking models. In Proceedings of the 42nd Annual\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR 2019), pages 1129–1132, Paris, France, 2019b.\nW. Yang, Y . Xie, A. Lin, X. Li, L. Tan, K. Xiong, M. Li, and J. Lin. End-to-end open-domain question\nanswering with BERTserini. InProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics (Demonstrations), pages 72–77, Minneapolis,\nMinnesota, 2019c.\nW. Yang, Y . Xie, L. Tan, K. Xiong, M. Li, and J. Lin. Data augmentation for BERT ﬁne-tuning in\nopen-domain question answering. arXiv:1904.06652, 2019d.\nW. Yang, H. Zhang, and J. Lin. Simple applications of BERT for ad hoc document retrieval.\narXiv:1903.10972, 2019e.\nZ. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy. Hierarchical attention networks for\ndocument classiﬁcation. In Proceedings of the 2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pages 1480–1489,\n2016.\nZ. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le. XLNet: Generalized autore-\ngressive pretraining for language understanding. In Advances in Neural Information Processing\nSystems 32 (NeurIPS 2019), pages 5754–5764, Vancouver, Canada, 2019f.\nD. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings\nof the 33rd Annual Meeting of the Association for Computational Linguistics , pages 189–196,\nCambridge, Massachusetts, June 1995.\n202\nA. Yates and N. Goharian. ADRTrace: Detecting expected and unexpected adverse drug reactions\nfrom user reviews on social media sites. In Proceedings of the 35th European Conference on\nInformation Retrieval (ECIR 2013), pages 816–819, Moscow, Russia, 2013.\nA. Yates, K. M. Jose, X. Zhang, and J. Lin. Flexible IR pipelines with Capreolus. In Proceedings\nof the 29th International Conference on Information and Knowledge Management (CIKM 2020),\npages 3181–3188, 2020.\nW.-t. Yih, K. Toutanova, J. C. Platt, and C. Meek. Learning discriminative projections for text\nsimilarity measures. In Proceedings of the Fifteenth Conference on Computational Natural\nLanguage Learning, pages 247–256, Portland, Oregon, 2011.\nK. Yoshino, C. Hori, J. Perez, L. F. D’Haro, L. Polymenakos, C. Gunasekara, W. S. Lasecki, J. K.\nKummerfeld, M. Galley, C. Brockett, J. Gao, B. Dolan, X. Gao, H. Alamari, T. K. Marks, D. Parikh,\nand D. Batra. Dialog System Technology Challenge 7. arXiv:1901.03461, 2019.\nH. Yu, S. Edunov, Y . Tian, and A. S. Morcos. Playing the lottery with rewards and multiple languages:\nLottery tickets in RL and NLP. In Proceedings of the 8th International Conference on Learning\nRepresentations (ICLR 2020), 2020a.\nH. Yu, Z. Dai, and J. Callan. PGT: Pseudo relevance feedback using a graph-based transformer. In\nProceedings of the 43rd European Conference on Information Retrieval (ECIR 2021), Part I, pages\n440–447, 2021.\nP. Yu and J. Allan. A study of neural matching models for cross-lingual IR. In Proceedings of the\n43rd Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR 2020), pages 1637–1640, 2020.\nQ. Yu, L. Bing, Q. Zhang, W. Lam, and L. Si. Review-based question generation with adaptive\ninstance transfer and augmentation. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 280–290, 2020b.\nR. Yu, Y . Xie, and J. Lin. Simple techniques for cross-collection relevance feedback. InProceedings\nof the 41th European Conference on Information Retrieval, Part I (ECIR 2019), pages 397–409,\nCologne, Germany, 2019.\nS. Yu, K. Yu, V . Tresp, H.-P. Kriegel, and M. Wu. Supervised probabilistic principal component\nanalysis. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining (SIGKDD 2006), pages 464–473, Philadelphia, Pennsylvania, 2006.\nM. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang,\nL. Yang, and A. Ahmed. Big Bird: Transformers for longer sequences. arXiv:2007.14062, 2020.\nH. Zamani, M. Dehghani, W. B. Croft, E. Learned-Miller, and J. Kamps. From neural re-ranking to\nneural ranking: Learning a sparse representation for inverted indexing. In Proceedings of the 27th\nACM International Conference on Information and Knowledge Management (CIKM 2018), pages\n497–506, Torino, Italy, 2018.\nC. Zhai. Statistical Language Models for Information Retrieval. Morgan & Claypool Publishers,\n2008.\nJ. Zhan, J. Mao, Y . Liu, M. Zhang, and S. Ma. Learning to retrieve: How to train a dense retrieval\nmodel effectively and efﬁciently. arXiv:2010.10469, 2020a.\nJ. Zhan, J. Mao, Y . Liu, M. Zhang, and S. Ma. An analysis of BERT in document ranking. InPro-\nceedings of the 43rd Annual International ACM SIGIR Conference on Research and Development\nin Information Retrieval (SIGIR 2020), pages 1941–1944, 2020b.\nJ. Zhan, J. Mao, Y . Liu, M. Zhang, and S. Ma. RepBERT: Contextualized text embeddings for\nﬁrst-stage retrieval. arXiv:2006.15498, 2020c.\nJ. Zhan, J. Mao, Y . Liu, J. Guo, M. Zhang, and S. Ma. Optimizing dense retrieval model training\nwith hard negatives. In Proceedings of the 44th Annual International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR 2021), pages 1503–1512, 2021.\n203\nE. Zhang, N. Gupta, R. Tang, X. Han, R. Pradeep, K. Lu, Y . Zhang, R. Nogueira, K. Cho, H. Fang,\nand J. Lin. Covidex: Neural ranking models and keyword search infrastructure for the COVID-19\nOpen Research Dataset. In Proceedings of the First Workshop on Scholarly Document Processing,\npages 31–41, 2020a.\nH. Zhang, M. Abualsaud, N. Ghelani, M. D. Smucker, G. V . Cormack, and M. R. Grossman.\nEffective user interaction for high-recall retrieval: Less is more. In Proceedings of the 27th\nACM International Conference on Information and Knowledge Management (CIKM 2018), pages\n187–196, Torino, Italy, 2018.\nH. Zhang, G. V . Cormack, M. R. Grossman, and M. D. Smucker. Evaluating sentence-level relevance\nfeedback for high-recall information retrieval. Information Retrieval, 23(1):1–26, 2020b.\nJ. Zhang, Y . Zhao, M. Saleh, and P. J. Liu. PEGASUS: Pre-training with extracted gap-sentences\nfor abstractive summarization. In Proceedings of the 37th International Conference on Machine\nLearning (ICML 2020), pages 2021–2032, 2020c.\nK. Zhang, C. Xiong, Z. Liu, and Z. Liu. Selective weak supervision for neural information retrieval.\nIn Proceedings of The Web Conference 2020 (WWW 2020), pages 474–485, 2020d.\nT. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, and Y . Artzi. Revisiting few-sample BERT ﬁne-tuning.\narXiv:2006.05987, 2020e.\nW. Zhang, J. Liu, Z. Wen, Y . Wang, and G. de Melo. Query distillation: BERT-based distillation\nfor ensemble ranking. In Proceedings of the 28th International Conference on Computational\nLinguistics: Industry Track, pages 33–43, 2020f.\nX. Zhang, F. Wei, and M. Zhou. HIBERT: Document level pre-training of hierarchical bidirectional\ntransformers for document summarization. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages 5059–5069, Florence, Italy, 2019.\nX. Zhang, A. Yates, and J. Lin. A little bit is worse than none: Ranking with limited training data. In\nProceedings of SustaiNLP: Workshop on Simple and Efﬁcient Natural Language Processing, pages\n107–112, 2020g.\nX. Zhang, A. Yates, and J. Lin. Comparing score aggregation approaches for document retrieval\nwith pretrained transformers. In Proceedings of the 43rd European Conference on Information\nRetrieval (ECIR 2021), Part II, pages 150–163, 2021.\nC. Zhao, C. Xiong, C. Rosset, X. Song, P. Bennett, and S. Tiwary. Transformer-XH: Multi-evidence\nreasoning with extra hop attention. In Proceedings of the 7th International Conference on Learning\nRepresentations (ICLR 2019), New Orleans, Louisiana, 2019.\nT. Zhao, X. Lu, and K. Lee. SPARTA: Efﬁcient open-domain question answering via sparse\ntransformer matching retrieval. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages\n565–575, 2021.\nZ. Zheng, K. Hui, B. He, X. Han, L. Sun, and A. Yates. BERT-QE: Contextualized Query Expansion\nfor Document Re-ranking. In Findings of the Association for Computational Linguistics: EMNLP\n2020, pages 4718–4728, Nov. 2020.\nY . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books\nand movies: Towards story-like visual explanations by watching movies and reading books. In\nProceedings of the 2015 IEEE International Conference on Computer Vision (ICCV 2015), pages\n19–27, Santiago, Chile, 2015.\nJ. Zobel. How reliable are the results of large-scale information retrieval experiments? In Proceed-\nings of the 21st Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (SIGIR 1998), pages 307–314, Melbourne, Australia, 1998.\nJ. Zobel and A. Moffat. Inverted ﬁles for text search engines. ACM Computing Surveys, 38(6):1–56,\n2006.\nL. Zou, S. Zhang, H. Cai, D. Ma, S. Cheng, D. Shi, Z. Zhu, W. Su, S. Wang, Z. Cheng, and D. Yin.\nPre-trained language model based ranking in Baidu search. arXiv:2105.11108, 2021.\n204",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7867048978805542
    },
    {
      "name": "Transformer",
      "score": 0.7634804844856262
    },
    {
      "name": "Sentence",
      "score": 0.6825239062309265
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5711395144462585
    },
    {
      "name": "Question answering",
      "score": 0.5408364534378052
    },
    {
      "name": "Natural language processing",
      "score": 0.5252016186714172
    },
    {
      "name": "Information retrieval",
      "score": 0.5226478576660156
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.4923025369644165
    },
    {
      "name": "Language model",
      "score": 0.4152308702468872
    },
    {
      "name": "Machine learning",
      "score": 0.40714186429977417
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210109712",
      "name": "Max Planck Institute for Informatics",
      "country": "DE"
    }
  ]
}