{
  "title": "Vision Transformer and Language Model Based Radiology Report Generation",
  "url": "https://openalex.org/W4312549535",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5061725368",
      "name": "Mashood Mohammad Mohsan",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2129681043",
      "name": "Muhammad Usman Akram",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1372903011",
      "name": "Ghulam Rasool",
      "affiliations": [
        "Moffitt Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2077232814",
      "name": "Norah Saleh Alghamdi",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    },
    {
      "id": "https://openalex.org/A5098620615",
      "name": "Muhammad Abdullah Aamer Baqai",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2100204024",
      "name": "Muhammad Abbas",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2028798134",
    "https://openalex.org/W4210315652",
    "https://openalex.org/W14244905",
    "https://openalex.org/W6784955093",
    "https://openalex.org/W2904711804",
    "https://openalex.org/W2979861699",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W6773815586",
    "https://openalex.org/W2979956313",
    "https://openalex.org/W2903721568",
    "https://openalex.org/W6785519123",
    "https://openalex.org/W3110077080",
    "https://openalex.org/W6803086743",
    "https://openalex.org/W4231122779",
    "https://openalex.org/W3181252431",
    "https://openalex.org/W6790992267",
    "https://openalex.org/W3151410070",
    "https://openalex.org/W4285531589",
    "https://openalex.org/W6839832337",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W4206368078",
    "https://openalex.org/W4382202677",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W6766193724",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Recent advancements in transformers exploited computer vision problems which results in state-of-the-art models. Transformer-based models in various sequence prediction tasks such as language translation, sentiment classification, and caption generation have shown remarkable performance. Auto report generation scenarios in medical imaging through caption generation models is one of the applied scenarios for language models and have strong social impact. In these models, convolution neural networks have been used as encoder to gain spatial information and recurrent neural networks are used as decoder to generate caption or medical report. However, using transformer architecture as encoder and decoder in caption or report writing task is still unexplored. In this research, we explored the effect of losing spatial biasness information in encoder by using pre-trained vanilla image transformer architecture and combine it with different pre-trained language transformers as decoder. In order to evaluate the proposed methodology, the Indiana University Chest X-Rays dataset is used where ablation study is also conducted with respect to different evaluations. The comparative analysis shows that the proposed methodology has represented remarkable performance when compared with existing techniques in terms of different performance parameters.",
  "full_text": "Received 6 December 2022, accepted 14 December 2022, date of publication 27 December 2022, date of current version 6 January 2023.\nDigital Object Identifier 10.1 109/ACCESS.2022.3232719\nVision Transformer and Language Model Based\nRadiology Report Generation\nMASHOOD MOHAMMAD MOHSAN\n 1, MUHAMMAD USMAN AKRAM\n1, (Senior Member, IEEE),\nGHULAM RASOOL\n 2, (Senior Member, IEEE), NORAH SALEH ALGHAMDI\n3,\nMUHAMMAD ABDULLAH AAMER BAQAI4, AND MUHAMMAD ABBAS1\n1Department of Computer and Software Engineering, National University of Sciences and Technology, Islamabad 44000, Pakistan\n2Machine Learning Department, Mofﬁtt Cancer Center, Tampa, FL 33612, USA\n3Department of Computer Sciences, College of Computer and Information Sciences, Princess Nourah Bint Abdulrahman University, Riyadh 11671, Saudi Arabia\n4College of Engineering, Michigan State University, East Lansing, MI 48824, USA\nCorresponding authors: Mashood M. Mohsan (mashood3624@gmail.com) and Norah Saleh Alghamdi (nosalghamdi@pnu.edu.sa)\nThis work was supported by the Princess Nourah bint Abdulrahman University Researchers Supporting under Project PNURSP2023R04,\nPrincess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia.\nABSTRACT Recent advancements in transformers exploited computer vision problems which results in\nstate-of-the-art models. Transformer-based models in various sequence prediction tasks such as language\ntranslation, sentiment classiﬁcation, and caption generation have shown remarkable performance. Auto\nreport generation scenarios in medical imaging through caption generation models is one of the applied\nscenarios for language models and have strong social impact. In these models, convolution neural networks\nhave been used as encoder to gain spatial information and recurrent neural networks are used as decoder\nto generate caption or medical report. However, using transformer architecture as encoder and decoder\nin caption or report writing task is still unexplored. In this research, we explored the effect of losing\nspatial biasness information in encoder by using pre-trained vanilla image transformer architecture and\ncombine it with different pre-trained language transformers as decoder. In order to evaluate the proposed\nmethodology, the Indiana University Chest X-Rays dataset is used where ablation study is also conducted\nwith respect to different evaluations. The comparative analysis shows that the proposed methodology\nhas represented remarkable performance when compared with existing techniques in terms of different\nperformance parameters.\nINDEX TERMS Vision transformers, language models, radiology report, decoder.\nI. INTRODUCTION\nDiseases that target the chest are particularly dangerous\nbecause of their impact on the lungs. Every year, millions\nof people face being diagnosed with a chest disease [1].\nAs the lungs are important organs in the human body, any\ndamage caused to them could have life threatening implica-\ntions. On average, 58000 deaths occur only due to pneumonia\n[2]. Chest x-rays are the primary practice to diagnose these\ndiseases. The radiologist conducts a thorough visual of the\nx-ray image and writes a report of the patient’s condition. This\nimplies the same deﬁnition of image based report generation\nwhich is the task of describing the visual content of image in\nnatural language by understanding the visual semantics [3].\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Carmelo Militello\n.\nA manually created report describes the general chest con-\ndition, ﬁndings, and diseases if found. The radiologist must\npossess the following skills to correctly read a chest x-ray [3]:\nthorough knowledge of the basic anatomy of thorax as well\nas physiology various chest diseases, ability to analyze the\nradiograph through identifying different pattern, ability to\nanalyze and evaluate the evolution over time of chest x-rays\nand recognize any changes that might occur, knowledge of\nclinical presentation and history, Knowledge of correlation to\ndiagnostic results. This laborious task can result in being error\nprone if written by an inexperienced physician while simulta-\nneously being tedious and time consuming for an experienced\nphysician. The problem of report generation being a lengthy\nprocess is highlighted when large amounts of chest x-rays\nneed to be analyzed. In highly populated areas, there could be\nhundreds of chest x-rays to analyze daily. Even after gaining\n1814 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 11, 2023\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\nextensive experience, it takes a radiologist, on average,\n5 to 10 minutes to correctly read a chest x-ray [4].\nGenerating a diagnostic report is actually a image-\nto-sequence problem whose inputs are pixels. A complete\ndiagnostic report consists of ﬁndings, impressions, and tags.\nPrevious solutions make use of a multi- tier system. The\ntags are considered as labels and a multi class classiﬁca-\ntions produces predicted labels for each chest x-ray image\n[4]. After performing a semantic analysis on the image, the\ncorrect description is attached. Descriptions in the reports are\nmulti sentence long and their generations is crucial to the\naccuracy and quality of the report.Many solutions employing\nLSTM network has been proposed to solve this problem\nsuch as [4]. However, report descriptions consist of long\nsentences and CNN fails to encode complete features in latent\nspace therefore effecting the accuracy’s of report generated\nby LSTM.\nThe literature reports earlier attempts to create a radi-\nologist report for a chest X-ray image by incorporating\nmultiple CNN-RNN frameworks. In 2017, researchers pro-\nposed Transformer, a new simple network architecture that\ncompletely eschews recurrence and convolutions in favour\nof attention mechanisms. Convolutional or recurrent neural\nnetworks in an encoder-decoder arrangement constitute the\nfoundation of the earlier dominating sequence transduction\nmodels. The Transformer model also uses the same con-\nﬁguration but relies only on attention mechanism. In order\nto increase overall efﬁciency, this article provides a novel\nTransformer Medical Report Generator (TrMRG) method for\nproducing a chest X-ray report.\nThe contributions of this paper are summarized as\nfollows:\n1) We propose TrMRG (Transformer Medical report\ngenerator), an end-to-end Transformer-based model\nfor report generation with pre-trained computer\nvision (CV) and language models. Although TrOCR\nwas the ﬁrst to adopt this architecture but it was used\nfor only classiﬁcation purposes. To the best of our\nknowledge, this is the ﬁrst effort to use pre-trained\nimage and text Transformers in tandem to generate\nmedical reports.\n2) A detailed ablation study has been conducted to iden-\ntify best pre-trained models for encoder and decoder to\ngenerate more reliable reports\n3) TrMRG achieves remarkable score with a standard\nTransformer-based encoder-decoder model, which is\nconvolution and recurrence free and does not rely on\ntraining from scratch. It can be easily ﬁne-tuned to pre-\ndict accurate reports. The model will be made publicly\navailable.\nThe remaining of the paper is structured as follows.\nSection 2 examines related literature. The methodology is\nexplained in Section 3. The ablation studies and experimental\nresults are presented in Section 4, and Section 5 contains the\nconclusion.\nII. LITERATURE REVIEW\nAutomated medical report generation is the application of\ncomputer vision and language models which has strong soci-\netal impact. Medical report generation process started from\n[5] who proposed a CNN-RNN architecture to generate cap-\ntions for images. These results were however too simple and\nlacked details. As more work was done in the ﬁeld, attention\nwas introduced and models like [6] used attention with RNN\nand CNN. This model produced signiﬁcantly better results.\nTransformers were introduced in 2019 [7]. It is free from\nconvolution and recurrence and solely focuses on attention.\nIt used Multi attention self-attention (MSA) with multiple\nencoder and decoder layers which made it outperform previ-\nous language models. Since 2019, transformers are not only\nused with text but also with images where they have out-\nperformed many existing techniques at different tasks. Here,\nwe have divided the literature review into different subsection\nfor better understanding.\nA. TRANSFORMER IN LANGUAGE\nTransformer models have performed admirably on a variety\nof linguistic tasks. BERT (Bidirectional Encoder Representa-\ntions from Transformers) [8], GPT2 (Generative Pre-trained\nTransformer) [9], and T5 (Text-to-Text Transfer Transformer)\n[10] are a few of the well-liked models. BERT pre-trained the\nTransformer in a self-supervised manner using the Masked\nLanguage Model (MLM) and Next Sentence Prediction\n(NSP). In the Masked Language Model, 15% of the words\nin a sentence are randomly masked, after which the model\nis trained to predict these words using cross-entropy loss.\nThe model gains the ability to take into account the bidirec-\ntional context while making predictions. The model predicts\na binary label for a pair of sentences in NSP. The model\nmay construct associations between two sentences as a result.\nThey are essential in issues involving natural language, like\nquestion-answering and natural language inference. A differ-\nent training approach was used in GPT-2, known as Causal\nLanguage Modelling, in which the model is trained to pre-\ndict the next word given all the previous words. GPT-2\nwas stacked with only decoder layers of Transformer and\ntoken embedding along with positional embedings were cal-\nculated and added to sequences of tokens as input. Multi-head\nself-attention, feedforward network, layer normalization, and\nresidual connections are applied by each layer. DistilGPT-2\n[11] is a popular compressed version of GPT-2. It has fewer\nparameters compared to GPT2. DistilGPT-2 is trained on\nOpenWebTextCorpus by using Knowledge distillation meth-\nods. MiniLM [12] uses the same tokenizer as XLM-R while\nhaving its architecture based on BERT. To make the teacher’s\nself-attention module more moldable and effective for the\nlearner, ﬁnal layer of the transformer is distilled.\nB. TRANSFORMER IN IMAGES\nTransformers were ﬁrst used in 2018 [24] as Image gener-\native model. In 2020, [25] Vision Transformer (ViT), also\nVOLUME 11, 2023 1815\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\nTABLE 1. Literature review summary table.\nknown as vanilla image transformer, was proposed to demon-\nstrate Transformer in image classiﬁcation which outper-\nformed existing image recognition benchmarks (ImageNet,\nCIFAR-100, VTAB, etc.). Vision Transformer (ViT) is the\nﬁrst implementation of a transformer in a deep neural network\non large-scale image datasets. They took sequences of image\npatches which were ﬂattened as vectors and then applied\nthe original transformer model. The model was trained on\na large dataset and then reﬁned to downstream recognition\nbenchmarks such as ImageNet classiﬁcation. Spatial biasness\nis one of the inductive biasness present in CNN hence a\nCNN assumes a certain structure is present in images so\nit updates it ﬁlter parameters accordingly in order to clas-\nsify images. Transformers does not inherit spatial biasness\nproperty opposed to CNN as they are solely based on atten-\ntions. DeiT [26] showed that it was possible to learn Trans-\nformers on mid-sized datasets in relatively shorter training\nepisodes. It used procedures found in CNNs such as augmen-\ntation and regularization and adopted a unique knowledge\ndistillation approach to train Transformers. A CNN model\nwas employed as teacher model to distill a student Trans-\nformer. New properties to ViT that are different compared\nto convolutional networks were added by the DINO work\n[27]. This used self-supervised learning techniques. The term\nDINO is a method interpreted as a form of self-distillation\nwith no labels. It provided solid foundation for the idea\nthat self-supervised learning could be key to developing a\nViT based BERT like model. The next model is ViT-MAE\n[28] which uses a simple MLM-like architecture. Patches of\ninput images forming a series are given as input and major-\nity are masked out. The remaining visible patches are then\ngiven the encoder. The encoded images along with MASK\ntokens added to them are given input to a decoder hence to\nreconstruct the original image. After pre-training is complete,\nthe encoder is utilised for recognition tasks on un-corrupted\nimages instead of the decoder, which is then destroyed.\nTable 2 shows are summary of different transformers.\nC. MEDICAL REPORT GENERATION\nAutomated medical report generation has a journey where\ndifferent techniques have been utilized before transformed for\nthis purpose. Early methods for writing reports included tem-\nplate ﬁlling, description retrieval, and hand crafted natural\nTABLE 2. Comparison of different Image Transformers.\nlanguage generating techniques. In its basic arrangement, the\ntask can be named as an image-to-sequence problem with\npixel-values in form of series as inputs. These input patches of\ninput are represented as feature vectors in the visual encoding\nstage, which encodes the input, also known as latent space\nvector, for the next generative step known as the language\ngeneration. A string of words or subwords that have been\ndecoded using a particular vocabulary are the result of this.\nXiong et al. [13] proposed a 2 part model. The ﬁrst part was\nan Image Encoder and the second part was a non-recurrent\nCaptioning Decoder. Li et al. [14] proposed the KERP\n(Knowledge-driven Encode, Retrieve, Paraphrase) approach\nthat integrated contemporary learning-based methodologies\nfor report generation with knowledge and retrieval-based\nmethods. The work of producing reports was divided by\nKERP into learning medical abnormality graphs and sub-\nsequent natural language modelling. First, visual features\nwere converted into a structured abnormality graph using\nan encoder. The templates were then obtained by a retrieve\nmodule based on the abnormalities found. A paraphrase\nunit then revised the templates to ﬁt the given situation.\nA memory-driven transformer to produce a report was sug-\ngested by [15]. Their strategy involved equipping the trans-\nformer with a relational memory to store important data.\nAdditionally, a memory-driven conditional layer normali-\nsation was used to include memory into the transformer’s\ndecoder. Srinivasan et al. [16] proposed a deep neural net-\nwork which predicted tags and created a report for a given\nchest x-ray. To get the tag embeddings, they used a convo-\nlutional neural network followed by transformers for learn-\ning self and cross attention. Image tags and features are\nencoded with self-attention to get a more detailed repre-\nsentation. Both of the above features were made use of in\ncross-attention, also known as Encoder-Decoder attention,\nalong the sequence of input in order to generate the report\n1816 VOLUME 11, 2023\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\nFIGURE 1. Illustration of proposed method. The left part shows the image based encoder which consists of N layers each containing attention heads.\nThe right part shows the language based decoder consisting of N layers.\nﬁndings section. Impressions were generated by applying\ncross-attention on ﬁndings and input sequence. Reference\n[29] presented a method where they combined CNN based\nfeatures with attention layer and LSTM to generate more\nreliable reports. They utilized IU and MIMIC datasets for\nevaluation purposes and outperformed simple attention based\nmethods. A Knowledge Graph Auto-Encoder (KGAE) model\nthat allowed unrelated sets of images and reports for training\nwas proposed by [17]. An encoder and decoder driven by\nknowledge along with knowledge graph was also included\nin it. The visual and written realms were linked by the\nknowledge graph (like a latent space). The images were pro-\njected at the proper coordinates in the latent space where the\nencoder reported. The decoder used the provided coordinates\nto generate the report. This model was unsupervised because\nit may be trained using diverse sets of photos and reports.\nNguyen et al. [18] proposed Classiﬁcation of Clinical history\nand Chest X-ray to generate embedding of diseases along\nwith a Transformer decoder sub-modules in an a fully dif-\nferentiable paradigm to generate complete diagnostic reports.\nTo ensure consistency with disease related topics, a weighted\nembedding representation was fed to the interpreter. Liu et al.\n[19] proposed a model which mimics the process of radi-\nologists. First they identiﬁed the disease from the image\nusing a CNN and then used prior knowledge for report\ngeneration. Nooralahzadeh et al. [20] proposed a two-step\nmodel which derived global concepts from the image then\nreformed them into ﬁner and coherent texts using a trans-\nformer architecture. You et al. [22] proposed a AlignTrans-\nformer framework, Align Hierarchical Attention (AHA) and\nMulti-Grained Transformer (MGT) were the components of\nthe AlignTransformer framework. Wang et al. [23] proposed\na method which explicitly quantiﬁed visual and textual uncer-\ntainties for radiology report generation. Alfarghaly et al. [21]\nproposed a deep learning model consisting of CNN model\nas encoder and a Transformer model as decoder. They used\nChexnet, as encoder, to predict the tags for images and also\nto generate latent space vector. Chexnet is one of the largest\nmodels used in this area. They then calculated weighted\nsemantic features from the predicted tags pretrained embed-\ndings. Finally to generate a report, they used a GPT2 pre\ntrained model on the latent space vetor and semantic features.\nVOLUME 11, 2023 1817\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\nGPT2 was one of the largest language generation model and\ndespite this, their unique deep learning farmework under-\nperformed when compared to others as their BLEU-1 score\nwas only 0.387 and BLEU-2 score was 0.245. An end-\nto-end Transformer model for Optical Character Recognition\nwas presented in [30]. It was the ﬁrst research that utilized\npre-trained weights for both image and language models.\nTrOCR was limited for being a classiﬁcation model only.\nTable 1 shows an overview of literature review with respect\nto different methodologies their respective approaches.\nIII. METHODOLOGY\nFig 1 shows an overview of our proposed methodology.\nAn encoder and a decoder are both parts of the TrMRG\nmodel. The encoder receives an image as input, breaks it up\ninto patches, and then adds positional encoding to it. Each\nlayer of encoder contains multiple self-attention heads which\nencode its feature representation. The encoded features are\npassed to multiple self attention heads in decoder layers in\nform of Queries Q and Keys K to decode it. After passing\nthe output of the decoder to the linear layer and softmax,\nwords probabilities are predicted. The output of encoder and\ndecoder is stated as Hidden states in Fig 1. In literature, the\noutput of encoder is also named latent space.\nA. POSITIONAL ENCODING\nTransformers by nature does not contain any recurrence or\nany convolution so in order to maintain order of sequence,\nposition information must be stored. Tokens converted into\nembedding vectors are summed up with Positional Encoding\nvectors. For text models such as MiniLM, a 1-dimensional\npositional encoding is added where sinusoidal waves of vari-\nous frequencies are used:\nPE(p, 2i) =sin(p/100002i/d ) (1)\nPE(p, 2i +1) =sin(p/100002i/d ) (2)\nwhere p represents the current position of word in a sequence\nand L is the total length of sentence and i has a range of\n0 <= i < L/2 and the input embedding dimension is\nlabelled as d. The dimensions of input embedding as well as\nof positional encoding are kept same so they can be added.\nImage Transformer models such as ViT also used the same\npositional encoding as using 2-dimensional positional encod-\ning, no signiﬁcant performance was observed.\nB. ENCODER\nA stack of N identical layers stacked together. Each layer\nis composed of two sub-layers. Multi-headed Self Attention\nis ﬁrst sub-layer followed by a simple positionally linked\nfeed-forward network. In order to rtain previous informa-\ntion and avoid vanishing gradient problem a residual con-\nnection is employed along with layer normalization, after\neach sub-layers. The LayerNorm of each sub-output layer\nis equal to LayerNorm(x +Sublayer(x)), where Sublayer(x)\nis the function that each sub-layer implements on its own.\nThe dimension of all sub-layers, input, and output of the\nmodel is kept the same for assisting residual connections.\n1) IMAGE INPUT REPRESENTATION\nThe encoder accepts an image as input and downsample\nor upsample it to a predetermined size (H , W ). While the\nadjusted picture’s size are guaranteed to be divisible by the\npatch size P, the encoder separates the input image into a\nbatch of patches with deﬁned sizes.The Transformer encoder,\nby nature, cannot process a whole entity until there are a\nseries of input tokens or in case of images there must be\nseries of patches. The patches are then linearly projected\nonto D-dimension vectors, where D is the hidden size of the\nTransformer across all of its layers, after being ﬂattened into\nvectors.\n‘‘[CLS]’’, a unique token, typically used for the image\nclassiﬁcation task is retained, much like ViT and DeiT. The\nentire image is represented by the ‘‘[CLS]’’ token, which\ncombines the data from every patch embedding. The special\ndistillation token employed in DeiT is also kept when utilising\nthe DeiT pre-trained models for encoder initialization so that\nthe student model can distill knowledge from the teacher\nmodel. Given learn-able 1D position embeddings based on\ntheir absolute positions are the patch embeddings and two\nspecial tokens. Afterward, the series of input is fed to a stack\nof identical encoder layers. MSA and feed-forward fully con-\nnected sublayer are present in each Transformer layer. Layer\nnormalisation and residual connection come after these.\n2) SELF ATTENTION\nThe attention mechanism involves dividing up the attention\namong the values and producing the weighted sum of them,\nwhere the weights of the values are determined by the relevant\nkeys and queries. All of the queries, keys, and values for the\nself-attention modules originate from the same sequence. The\nattention output matrix is calculated as follows:\nSelf Attention (Q, K, V ) =softmax((Q ·KT)/\n√\ndk ) ·V (3)\nThe softmax function’s extremely small gradients are avoided\nby applying the scaling factor √dk , where dimension of query\nand key vector is represented by dk . The model may jointly\ngather data from several representation subspaces thanks to\nthe MSA sub-layer, which projects the queries, keys, and\nvalues h times with various learnable projection weights.\nMultihead(Q, K, V ) =concat(h1 . . .hn) ·W o (4)\nwhere\nheadi =Self Attention(W q\ni qj, W k\ni K, W v\ni V )\nFigure 2 illustrates the calculation of a multi-headed self\nattention layer using heat maps. A vanilla image transformer\nmodel such as ViT have the same architecture of an encoder\nof [7]. The only difference is that Image transformer takes\nimage as series of input and [7] takes text in a sequence. The\nself attention calculation mechanism is also same.\n1818 VOLUME 11, 2023\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\nFIGURE 2. Graphical representation of Multi-headed self-attention calculation in a single layer of Transformers.\nC. DECODER\nDecoder also contains N number of identical layers stacked\nsuch as in Encoder but with additional sub-layer. The decoder\nperforms multi-head attention over the hidden states from\nEncoder, which is then transformed into Queries and Keys,\nhence inserting a third sub-layer along with two sub-layers\nalready present in each Decoder. Each of the sub-layers\napplies residual connections in a manner similar to the\nencoder before layer normalisation. Decoder uses ‘‘Masked\nMulti-head attention’’ layer in order to stop paying attention\nto succeeding locations. The prediction of location i can only\nbe based on the known outputs at positions lower than i\nwhich is due to the masking technique along with the shifting\nof output embedding offset by one place because of Casual\nLanguage Modelling (CLM).\n1) TEXT INPUT REPRESENTATION\nA sequence of tokens is given as input to decoder. Using the\nEmbedding look-up table, of V vocabulary length, the feature\nvectors of dmodel dimension of each token is fetched and\npositional encoding of sequence is added. At time of training\nwhole report is given as decoder input but at inference only\ninitial start token is given to produce next words using CLM.\n2) SELF ATTENTION\nFor TrMRG, we employ the original Transformer decoder.\nWith the exception of inserting ‘‘encoder-decoder atten-\ntion’’ between the multi-head self-attention and feedfor-\nward network to distribute various levels of attention on the\nencoder output. Similar to Encoder layer, the Decoder also\nhas a stack of identical layers. The keys, values and the\nqueries in the encoder-decoder attention module are derived\nfrom the encoder output and the decoder input, respectively.\nIn order to avoid receiving more information during train-\ning than necessary for prediction, the decoder also makes\nuse of attention masking in self-attention. In literature, the\n‘‘Encoder-Decoder attention’’ is also stated as cross attention.\nTABLE 3. Comparison of different Image Transformers.\nIV. EXPERIMENTS & RESULTS\nThe proposed model has been tested using chest X-rays\nimages and associated reports. The details are given in fol-\nlowing sections\nA. DATASET\nThe Indiana University Chest x-Ray dataset (IU X-Ray)\ncontains a collection of pairs of Chest x-ray images and\ndiagnostic of reports [31]. The dataset includes 3955 reports\nproduced by radiologists and 7,470 pairs of frontal and lateral\nchest x-ray pictures. Each report consists of different sections\ne.g. impression, ﬁndings, tags, comparison, and indication.\nIn this research, we solely use frontal CXR as input and treat\nthe contents of ﬁndings as the target captions to be generated.\nFigure 4 gives an illustration of a randomly selected frontal\nview along with associated ﬁndings.\nThe dataset, specially images, are pre-processed to support\nthe implementation of proposed model. Reports (Text data) is\ncleaned by removing unnecessary spaces, special characters\nVOLUME 11, 2023 1819\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\nFIGURE 3. Data sample containing CXR, Impressions and Findings from IU dataset.\nincluding comma and full stop and hidden words such as\n‘‘xxxx’’, also all characters are converted into lower case.\nA total of 3203 reports are selected for our research and\n2404 random samples are selected for Training set, 500 for\nValidation set and 300 for Testing set.\nB. IMPLEMENTATION DETAILS\nThe pre-trained weights for the encoders and decoders are\nutilised to ﬁne-tune them on the IU X-Ray dataset during the\ntraining phase. The proposed model is implemented using\nPyTorch and trained on a GeForce RTX 2070 8GB GPU.\nThe AdamW optimizer is used to ﬁne-tune the network for\n25 epochs with a batch size of 1. For testing, the parameter\nvalues that produce the best results on the validation dataset\nare employed.\nC. EVALUATION METRICS\nTo evaluate our results, we adopt BLEU [32], METEOR [33],\nROUGE-L [34] and CIDEr [35] metrics that are widely-used\nto evaluate the natural language generation model. The eval-\nuation of machine translation is originally intended to focus\non BLEU and METEOR in particular. ROUGE-L is a tool for\nevaluating summary quality. CIDEr is made to assess image\ncaptioning applications.\nD. ABLATION STUDY\nTo evaluate the proposed report generation model, we have\nconducted detailed ablation studies. All of our experiments\nare on IU dataset and we reported both qualitative and quan-\ntitative results.\n1) QUANTITATIVE RESULTS\nOur TrMRG model consists of pre-trained image and lan-\nguage model on natural images or text datasets so in ﬁrst abla-\ntion study, we experimented with 4 image models as encoder\nand 3 Language models as decoder. The qualitative outcomes\nof ablation study are shown in Table 4. The n-gram similarity\nbetween the generated sentences and the ground-truth phrases\nis the foundation for the automatic evaluation measures (like\nBLEU). The table shows that MiniLM based decoder in\ngeneral performed well while generating radiology reports.\nFor encoder, Dino, Deit and ViT produced better results\nrespectively.\nTABLE 4. Comparison of different Image Transformers.\n2) QUALITATIVE ANALYSIS\nThe IU dataset is quite imbalanced and shows biasness due\nto more normal reports. So instead of just relying on quan-\ntitative values as presented in table 4, we also performed\nqualitative ablation study. In this section, we evaluate the\noverall quality of generated reports through several examples.\nFigure 4 presents reports generated by all 12 models from\nablation study for two different scenario of radiology reports.\nIt can be observed that in ﬁrst scenario when example is from\nthe most occurring cases, reports generated by all models are\nfollowing the ground truth. The reports generated by all mod-\nels are clear and provide more detail which is closer to refer-\nence report. However, when we look at second scenario which\nis one of the rare occurring scenario, the models behaviour is\ndifferent. Most of the models generate ﬁrst scenario reports\ndue to biased dataset. The ablation study shows that ViT\nalongwith MiniLM produces the best qualitative results that\nis why we have selected this combination in proposed model.\nIn general, our model can produce descriptions that follow the\nlogical progression of radiologists’ reports, which begin with\nbroad information such views, positive ﬁndings, followed by\n1820 VOLUME 11, 2023\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\nFIGURE 4. Illustration of reports generated by all 12 models for 2 randomly selected cases from IU dataset. From left to right results of ViT,\nDEIT, DINO and Vit_MAE models are shown and from top to bottom all decoder models MiniLM, GPT2 and DistilGPT2 results are shown\nrespectively.\nnegative discoveries, in the sequence of lung, heart, pleura,\nand others.\n3) COMPARISON WITH LITERATURE\nAfter ﬁnalizing the model through ablation study, we also\nperform comparative analysis with existing state of the art\ntechniques. Table 5 shows results on the automatic metrics\nfor the Findings module compared to literature. CDGPT2\nused Transformer with pre-trained weights at Decoder\nonly whereas for Encoder a Traditional CNN was utilized\npre-trained on CXR images and yet still managed to score\n0.38 BLEU-1 score. In our TrMRG model, we have employed\nTransformer based both Encoder and Decoder pre-trained on\nnatural images and language and scored the highest BLEU-1\nscore. Other researches for instance Align Transformer and\nAuto Encoder Transformer have made use of Transformer\nNetwork at Encoder and Decoder but the selection of ecnoder\nand decoder caused lower BLEU-1 for these. In comparison\nwith all these techniques on IU dataset, TrMRG model has\nshown highest BLEU-1 score which indicates its overall efﬁ-\nciency for generating reports that resemble those written by\nradiologist.\nVOLUME 11, 2023 1821\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\nTABLE 5. Comparison with review literature.\nFIGURE 5. Illustration of reports generated by TrMRG. The left most columns shows the CXR of patient given as input and the middle\ncolumn represents the ground truth whereas the last column shows the generated report from TrMRG.\nE. DISCUSSION\nTransformer models are still in evolutionary phase for learn-\ning how to generate complete paragraphs of medical reports\nfrom sparse dataset. Particularly, it is noted that the majority\nof reports are composed of sentences that are repetitive and\nstrikingly identical, which are of a descriptive nature and do\nnot explain anomalies and disorders. The doctor’s reports\nfrequency plot of distinct sentences reveals a long tail of\ndistribution, with anomalous sentences frequently appearing\nwith a frequency of f =1 across the whole dataset. In fact,\nf < 3 occurs in 6,290 of the 8,022 different sentences\n[36]. Figure 5 exemplify the results of our selected model\nTrMRG on 3 different cases. It can be noticed that diagnoses\nof our model relatively similar to doctors report. However at\nexample 3, we can see that our model missed to highlight the\nabnormality in CXR and this is all due to lack of abnormal\nsamples.\nThe results show that proposed TrMRG model has per-\nformed well when compared to existing models especially\nin terms of BLEU-1 score. The experimental analysis shows\nthat BLEU score is bit biased towards unbalanced dataset.\nIn case of medical reports where most of the data consists of\nnormal scenarios and reports, a model can quickly achieve\ngood ratings on these automatic evaluation metrics [4] by\nproducing just normal ﬁndings. However in proposed model,\nthe learning is bit generic and it is able to generate different\nreports based on varying inputs. In order to overcome the\nlimitation of BLEU scores, we have also evaluated the pro-\nposed model in form of METEOR, ROUGE-L and CIDEr.\nTable-5 showed that the proposed model has performed\n1822 VOLUME 11, 2023\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\nbetter in majority of these matrices. However, from ﬁgure 5,\nit is clear that for all normal cases the reports generated by\nproposed TrMRG is similar to reference report but in cases\nof diseases the model tends to miss some important medical\nterms. The main reason is quite small corpus size and unique\nmedical terms in IU dataset. This is one of the main limitation\nof proposed model right now which we intend to overcome\nwith help of large medical corpus along with modiﬁcation in\nthe model to further reﬁne it for less occurring words.\nV. CONCLUSION\nIn this paper, we proposed a novel approach (TrMRG) to\ngenerate radiology reports without using convolutional neural\nnetworks which inherits spatial biasness. TrMRG used Image\nbased Transformer pre-trained on Imagenet dataset and Lan-\nguage models pre-trained on natural language datasets. Both\nencoder and decoder were ﬁne-tuned on IU dataset. The\nexperiments proved effectiveness of our method, which not\nonly generated meaningful reports, but also achieved com-\npetitive BLEU score as compared to other models. It is\nobserved that even transformer being data hungry in nature\nand having no inductive biasness in it can be utilized if\nproperly ﬁne-tuned on smaller datasets such as IU dataset.\nThe ablation study in form of qualitative and quantitative\nresults justiﬁed the usefulness of TrMRG in assisting the\nradiologist.\nREFERENCES\n[1] O. Er, N. Yumusak, and F. Temurtas, ‘‘Chest diseases diagnosis using arti-\nﬁcial neural networks,’’ Expert Syst. Appl., vol. 37, no. 12, pp. 7648–7655,\nDec. 2010.\n[2] T. Gupte, A. Knack, and J. D. Cramer, ‘‘Mortality from aspiration pneu-\nmonia: Incidence, trends, and risk factors,’’ Dysphagia, vol. 37, no. 6,\npp. 1493–1500, Dec. 2022.\n[3] L. Delrue, R. Gosselin, B. Ilsen, A. Van Landeghem, J. de Mey, and\nP. Duyck, ‘‘Difﬁculties in the interpretation of chest radiography,’’ in\nComparative Interpretation of CT and Standard Radiography of the Chest.\nBerlin, Germany: Springer, 2011, pp. 27–49.\n[4] B. Jing, P. Xie, and E. Xing, ‘‘On the automatic generation of\nmedical imaging reports,’’ in Proc. 56th Annu. Meeting Assoc. Com-\nput. Linguistics. Melbourne, VIC, Australia: Association for Com-\nputational Linguistics, Jul. 2018, pp. 2577–2586. [Online]. Available:\nhttps://aclanthology.org/P18-1240\n[5] I. Allaouzi, M. B. Ahmed, B. Benamrou, and M. Ouardouz, ‘‘Automatic\ncaption generation for medical images,’’ in Proc. 3rd Int. Conf. Smart\nCity Appl. New York, NY, USA: Association for Computing Machinery,\nOct. 2018, pp. 1–6.\n[6] J. Yuan, H. Liao, R. Luo, and J. Luo, ‘‘Automatic radiology report genera-\ntion based on multi-view image fusion and medical concept enrichment,’’\nin Medical Image Computing and Computer Assisted Intervention—\nMICCAI 2019 (Lecture Notes in Computer Science), vol. 11769. Midtown\nManhattan, NY USA: Springer, 2019, pp. 721–729.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. U. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30. Red Hook, NY, USA: Curran Associates,\n2017, pp. 1–11.\n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,’’\nin Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Hum.\nLang. Technol., vol. 1. Minneapolis, MN, USA: Association for Com-\nputational Linguistics, Jun. 2019, pp. 4171–4186. [Online]. Available:\nhttps://aclanthology.org/N19-1423\n[9] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n‘‘Language models are unsupervised multitask learners,’’ OpenAI,\nSan Francisco, CA, USA, Tech. Rep., 2019.\n[10] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,\nW. Li, and P. J. Liu, ‘‘Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer,’’J. Mach. Learn. Res., vol. 21, no. 140, pp. 1–67,\n2020.\n[11] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, ‘‘DistilBERT, a distilled\nversion of BERT: Smaller, faster, cheaper and lighter,’’ in Proc. NeurIPS\nEMC Workshop, 2019, pp. 673–680.\n[12] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou, ‘‘MiniLM: Deep\nself-attention distillation for task-agnostic compression of pre-trained\ntransformers,’’ in Proc. Adv. Neural Inf. Process. Syst., Annu. Conf. Neural\nInf. Process. Syst. (NeurIPS), vol. 33, Dec. 2020.\n[13] Y. Xiong, B. Du, and P. Yan, ‘‘Reinforced transformer for medical image\ncaptioning,’’ in Proc. Int. Workshop Mach. Learn. Med. Imag. Midtown\nManhattan, NY, USA: Springer, 2019, pp. 673–680.\n[14] C. Y. Li, X. Liang, Z. Hu, and E. P. Xing, ‘‘Knowledge-driven encode,\nretrieve, paraphrase for medical image report generation,’’ in Proc. 33rd\nAAAI Conf. Artif. Intell. 31st Innov. Appl. Artif. Intell. Conf. 9th AAAI\nSymp. Educ. Adv. Artif. Intell. (AAAI/IAAI/EAAI). Palo Alto, CA, USA:\nAAAI Press, 2019, pp. 6666–6673, doi: 10.1609/aaai.v33i01.33016666.\n[15] Z. Chen, Y. Song, T.-H. Chang, and X. Wan, ‘‘Generating radiology\nreports via memory-driven transformer,’’ inProc. Conf. Empirical Methods\nNatural Lang. Process. (EMNLP) . Stroudsburg, PA, USA: Association for\nComputational Linguistics, Nov. 2020, pp. 1439–1449. [Online]. Avail-\nable: https://aclanthology.org/2020.emnlp-main.112\n[16] P. Srinivasan, D. Thapar, A. Bhavsar, and A. Nigam, ‘‘Hierarchical X-ray\nreport generation via pathology tags and multi head attention,’’ in Proc.\nAsian Conf. Comput. Vis. (ACCV), Nov. 2020, pp. 1–17.\n[17] F. Liu, C. You, X. Wu, S. Ge, S. Wang, and X. Sun, ‘‘Auto-encoding\nknowledge graph for unsupervised medical report generation,’’ in Proc.\nAdv. Neural Inf. Process. Syst., A. Beygelzimer, Y. Dauphin, P. Liang,\nand J. W. Vaughan, Eds., 2021, pp. 16266–16279. [Online]. Available:\nhttps://openreview.net/forum?id=nIL7Q-p7-Sh\n[18] H. Nguyen, D. Nie, T. Badamdorj, Y. Liu, Y. Zhu, J. Truong, and\nL. Cheng, ‘‘Automated generation of accurate & ﬂuent medical X-ray\nreports,’’ in Proc. Conf. Empirical Methods Natural Lang. Process.\nPunta Cana, Dominican Republic: Association for Computational Linguis-\ntics, Nov. 2021, pp. 3552–3569.\n[19] F. Liu, X. Wu, S. Ge, W. Fan, and Y. Zou, ‘‘Exploring and distilling\nposterior and prior knowledge for radiology report generation,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,\npp. 13748–13757.\n[20] F. Nooralahzadeh, N. P. Gonzalez, T. Frauenfelder, K. Fujimoto,\nand M. Krauthammer, ‘‘Progressive transformer-based generation\nof radiology reports,’’ in Proc. Findings Assoc. Comput. Linguistics\n(EMNLP). Punta Cana, Dominican Republic: Association for\nComputational Linguistics, 2021, pp. 2824–2832. [Online]. Available:\nhttps://aclanthology.org/2021.ﬁndings-emnlp.241\n[21] O. Alfarghaly, R. Khaled, A. Elkorany, M. Helal, and A. Fahmy, ‘‘Auto-\nmated radiology report generation using conditioned transformers,’’ Infor-\nmat. Med. Unlocked, vol. 24, 2021, Art. no. 100557.\n[22] D. You, F. Liu, S. Ge, X. Xie, J. Zhang, and X. Wu, ‘‘AlignTransformer:\nHierarchical alignment of visual regions and disease tags for medical report\ngeneration,’’ inProc. 24th Int. Conf., Med. Image Comput. Comput. Assist.\nIntervent. (MICCAI), Strasbourg, France, Sep./Oct. 2021.\n[23] Y. Wang, Z. Lin, Z. Xu, H. Dong, J. Tian, J. Luo, Z. Shi, Y. Zhang, J. Fan,\nand Z. He, ‘‘Trust it or not: Conﬁdence-guided automatic radiology report\ngeneration,’’ 2021, arXiv:2106.10887.\n[24] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and\nD. Tran, ‘‘Image transformer,’’ in Proc. 35th Int. Conf. Mach. Learn.,\nvol. 80, J. Dy and A. Krause, Eds., Jul. 2018, pp. 4055–4064. [Online].\nAvailable: https://proceedings.mlr.press/v80/parmar18a.html\n[25] A. Kolesnikov, A. Dosovitskiy, D. Weissenborn, G. Heigold, J. Uszkoreit,\nL. Beyer, M. Minderer, M. Dehghani, N. Houlsby, S. Gelly, T. Unterthiner,\nand X. Zhai, ‘‘An image is worth 16 ×16 words: Transformers for image\nrecognition at scale,’’ in Proc. 9th Int. Conf. Learn. Represent. (ICLR),\n2021.\n[26] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. Jegou, ‘‘Training data-efﬁcient image transformers & distillation\nthrough attention,’’ in Proc. Int. Conf. Mach. Learn., vol. 139, Jul. 2021,\npp. 10347–10357.\n[27] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski,\nand A. Joulin, ‘‘Emerging properties in self-supervised vision transform-\ners,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 9650–9660.\nVOLUME 11, 2023 1823\nM. M. Mohsan et al.: Vision Transformer and Language Model Based Radiology Report Generation\n[28] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, ‘‘Masked autoen-\ncoders are scalable vision learners,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR), Jun. 2022, pp. 16000–16009.\n[29] M. Sirshar, M. F. K. Paracha, M. U. Akram, N. S. Alghamdi, S. Z. Y. Zaidi,\nand T. Fatima, ‘‘Attention based automated radiology report genera-\ntion using CNN and LSTM,’’ PLoS ONE, vol. 17, no. 1, Jan. 2022,\nArt. no. e0262209.\n[30] M. Li, T. Lv, L. Cui, Y. Lu, D. Florencio, C. Zhang, Z. Li, and\nF. Wei, ‘‘TrOCR: Transformer-based optical character recognition with\npre-trained models,’’ in Proc. 37th AAAI Conf. Artif. Intell., Assoc.\nAdvancement Artif. Intell., Washington, DC, USA, 2021.\n[31] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan,\nL. Rodriguez, S. Antani, G. R. Thoma, and C. J. McDonald, ‘‘Preparing\na collection of radiology examinations for distribution and retrieval,’’\nJ. Amer. Med. Inform. Assoc., vol. 23, no. 2, pp. 304–310, Mar. 2016.\n[32] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu, ‘‘BLEU: A method for\nautomatic evaluation of machine translation,’’ in Proc. 40th Annu. Meet-\ning Assoc. Comput. Linguistics. Philadelphia, PA, USA: Association for\nComputational Linguistics, Jul. 2002, pp. 311–318. [Online]. Available:\nhttps://aclanthology.org/P02-1040\n[33] A. Lavie and A. Agarwal, ‘‘Meteor: An automatic metric for MT evaluation\nwith high levels of correlation with human judgments,’’ in Proc. 2nd\nWorkshop Stat. Mach. Transl., Jul. 2007, pp. 228–231.\n[34] C.-Y. Lin, ‘‘Rouge: A package for automatic evaluation of summaries,’’\nin Proc. Conf. Question Answering Restricted Domains. Assoc. Comput.\nLinguistics, Barcelona, Spain, Jan. 2004, p. 10.\n[35] R. Vedantam, C. L. Zitnick, and D. Parikh, ‘‘CIDEr: Consensus-based\nimage description evaluation,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2015, pp. 4566–4575.\n[36] P. Harzig, Y.-Y. Chen, F. Chen, and R. Lienhart, ‘‘Addressing data\nbias problems for chest X-ray image report generation,’’ 2019,\narXiv:1908.02123.\nMASHOOD MOHAMMAD MOHSANreceived\nthe B.S. degree in computer science from the Uni-\nversity of Agriculture (UAF), Faisalabad, in 2020.\nHe is currently pursuing the M.S. degree in com-\nputer software engineering with the College of\nElectrical and Mechanical Engineering, National\nUniversity of Sciences and Technology (NUST),\nRawalpindi and doing collaborative research in\nmedical imaging.\nMUHAMMAD USMAN AKRAM(Senior Mem-\nber, IEEE) received the B.S. degree (Hons.) in\ncomputer system engineering and the master’s and\nPh.D. degrees in computer engineering from the\nCollege of Electrical and Mechanical Engineer-\ning, National University of Sciences and Tech-\nnology (NUST), Rawalpindi, Pakistan, in 2008,\n2010, and 2012, respectively. He is currently an\nAssociate Professor with the College of Electrical\nand Mechanical Engineering, NUST. He has over\n200 international publications in well reputed journals and conferences.\nHis main areas of research interests include biomedical imaging and image\nprocessing.\nGHULAM RASOOL (Senior Member, IEEE)\nreceived the Ph.D. degree in systems engineer-\ning from the University of Arkansas, Little Rock,\nin 2014. He was a Postdoctoral Fellow at the Reha-\nbilitation Institute of Chicago and Northwestern\nUniversity, from 2014 to 2016. He is currently\nan Assistant Member with the Machine Learn-\ning Department, Mofﬁtt Cancer Center, Tampa,\nFL, USA. His current research interests include\nimproving cancer care using machine learning,\nartiﬁcial intelligence, and statistical signal and image processing.\nNORAH SALEH ALGHAMDI received the Bachelor of Computer Sci-\nence degree from Taif University, Taif, Saudi Arabia, and the Master of\nComputer Science and Ph.D. degrees from the Department of Computer\nScience, La Trobe University, Melbourne, Australia. She is currently an\nAssociate Professor with the Department of Computer Science, College of\nComputer and Information Sciences, Princess Nourah Bint Abdulrahman\nUniversity (PNU), Riyadh, Saudi Arabia. She has been the Vice-Dean of\nquality assurance, since 2019. She is currently the Director of businesses\nand projects management in her college. She has authored or coauthored\nmany articles published in a well-known journals in the research ﬁeld. Her\nresearch interests include data mining, machine learning, text analytics,\nimage classiﬁcation, bioengineering, and deep learning. She has participated\nin organizing the international conference on computing (ICC 2019). She is a\nmember of the reviewer committee of several journals, such as IEEE A CCESS,\nJournal of Computer Science, and International Journal of Web Information\nSystems.\nMUHAMMAD ABDULLAH AAMER BAQAIis\ncurrently pursuing the B.S. degree in computer sci-\nence engineering with Michigan State University\n(MSU), East Lansing, MI, USA. He is involved\nin collaborative research in the ﬁeld of machine\nlearning and medical imaging.\nMUHAMMAD ABBASreceived the Ph.D. degree\nfrom the University of Manchester, U.K. His\nresearch interests include software engineering,\nERP systems, and machine learning.\n1824 VOLUME 11, 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.818084716796875
    },
    {
      "name": "Transformer",
      "score": 0.7504692077636719
    },
    {
      "name": "Encoder",
      "score": 0.5405115485191345
    },
    {
      "name": "Language model",
      "score": 0.5341697335243225
    },
    {
      "name": "Architecture",
      "score": 0.518265962600708
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49965929985046387
    },
    {
      "name": "Deep learning",
      "score": 0.443400114774704
    },
    {
      "name": "Natural language processing",
      "score": 0.33763784170150757
    },
    {
      "name": "Speech recognition",
      "score": 0.3342990577220917
    },
    {
      "name": "Engineering",
      "score": 0.1122683584690094
    },
    {
      "name": "Voltage",
      "score": 0.08692967891693115
    },
    {
      "name": "Electrical engineering",
      "score": 0.08007052540779114
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I929597975",
      "name": "National University of Sciences and Technology",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I3019308854",
      "name": "Moffitt Cancer Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I106778892",
      "name": "Princess Nourah bint Abdulrahman University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I87216513",
      "name": "Michigan State University",
      "country": "US"
    }
  ],
  "cited_by": 38
}