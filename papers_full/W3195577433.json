{
  "title": "On the Opportunities and Risks of Foundation Models",
  "url": "https://openalex.org/W3195577433",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5069576651",
      "name": "Rishi Bommasani",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027693232",
      "name": "Drew A. Hudson",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015355317",
      "name": "Ehsan Adeli",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084043782",
      "name": "Russ B. Altman",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5055978130",
      "name": "Simran Arora",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5009828979",
      "name": "Sydney von Arx",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076189854",
      "name": "Michael S. Bernstein",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5021676288",
      "name": "Jeannette Bohg",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5088410008",
      "name": "Antoine Bosselut",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084989076",
      "name": "Emma Brunskill",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5038255653",
      "name": "Erik Brynjolfsson",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019482456",
      "name": "Shyamal Buch",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5070767948",
      "name": "Dallas Card",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5005608407",
      "name": "Rodrigo Castellon",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5033738047",
      "name": "Niladri S. Chatterji",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074961334",
      "name": "Annie Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5050985956",
      "name": "Kathleen Creel",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5047265330",
      "name": "Jared Quincy Davis",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5052171928",
      "name": "Dorottya Demszky",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019674079",
      "name": "Chris Donahue",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5061135580",
      "name": "Moussa Doumbouya",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001994692",
      "name": "Esin Durmus",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5091179481",
      "name": "Stefano Ermon",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5071371645",
      "name": "John Etchemendy",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5070411028",
      "name": "Kawin Ethayarajh",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100450462",
      "name": "Li Fei-Fei",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5005431772",
      "name": "Chelsea Finn",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5032764769",
      "name": "Trevor Gale",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5042379226",
      "name": "Lauren Gillespie",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5063897942",
      "name": "Karan Goel",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001961716",
      "name": "Noah D. Goodman",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5030406488",
      "name": "Shelby Grossman",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5068178240",
      "name": "Neel Guha",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015518638",
      "name": "Tatsunori Hashimoto",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5049073875",
      "name": "Peter Henderson",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5075075495",
      "name": "John Hewitt",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5058408154",
      "name": "Daniel E. Ho",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5054737143",
      "name": "Jenny Hong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5055442462",
      "name": "Kyle Hsu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5032074795",
      "name": "Jing Huang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5079244248",
      "name": "Thomas Icard",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5112853549",
      "name": "Saahil Jain",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5087088138",
      "name": "Dan Jurafsky",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5011684148",
      "name": "Pratyusha Kalluri",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066892638",
      "name": "Siddharth Karamcheti",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020476329",
      "name": "Geoff Keeling",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5039473897",
      "name": "Fereshte Khani",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015201177",
      "name": "Omar Khattab",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5079904764",
      "name": "Pang Wei Koh",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5063737268",
      "name": "Mark Krass",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5032451496",
      "name": "Ranjay Krishna",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5080647450",
      "name": "Rohith Kuditipudi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014379995",
      "name": "Ananya Kumar",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074277807",
      "name": "Faisal Ladhak",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100756491",
      "name": "Mina Lee",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5070994105",
      "name": "Tong Lee",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5091272738",
      "name": "Jure Leskovec",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5040012627",
      "name": "Isabelle Levent",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5075181115",
      "name": "Xiang Lisa Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100752870",
      "name": "Xuechen Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101821970",
      "name": "Tengyu Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5041464552",
      "name": "Ali Ahmad Malik",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5046006076",
      "name": "Christopher D. Manning",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5008545107",
      "name": "Suvir Mirchandani",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074009646",
      "name": "Eric Mitchell",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5039717840",
      "name": "Zanele Munyikwa",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101603882",
      "name": "Suraj Nair",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5039749428",
      "name": "Avanika Narayan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5079514101",
      "name": "Deepak Narayanan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5035267335",
      "name": "Benjamin T. Newman",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5018535992",
      "name": "Allen Nie",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5018518655",
      "name": "Juan Carlos Niebles",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5091221987",
      "name": "Hamed Nilforoshan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5017688180",
      "name": "Julian Nyarko",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019464639",
      "name": "Giray Ogut",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5054285881",
      "name": "Laurel Orr",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5023621106",
      "name": "Isabel Papadimitriou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5016998074",
      "name": "Joon-Sung Park",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074969309",
      "name": "Chris Piech",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001944487",
      "name": "Eva Portelance",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5042601761",
      "name": "Christopher Potts",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5031731960",
      "name": "Aditi Raghunathan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5017349449",
      "name": "Rob Reich",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103198578",
      "name": "Hongyu Ren",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5050798856",
      "name": "Frieda Rong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5088352526",
      "name": "Yusuf Roohani",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5051264071",
      "name": "Camilo Ruiz",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5063387701",
      "name": "Jack Ryan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103852640",
      "name": "Christopher Ré",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5080725225",
      "name": "Dorsa Sadigh",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028645925",
      "name": "Shiori Sagawa",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5026521456",
      "name": "Keshav Santhanam",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5065371323",
      "name": "Andy Shih",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101973028",
      "name": "Krishnan Srinivasan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081558429",
      "name": "Alex Tamkin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5017940830",
      "name": "Rohan Taori",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5070255304",
      "name": "Armin W. Thomas",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5006851333",
      "name": "Florian Tramèr",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5030480633",
      "name": "Rose E. Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100702488",
      "name": "William Yang Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2573660794",
    "https://openalex.org/W2964325845",
    "https://openalex.org/W3034733718",
    "https://openalex.org/W2060427373",
    "https://openalex.org/W2752026105",
    "https://openalex.org/W3126816608",
    "https://openalex.org/W2194321275",
    "https://openalex.org/W3097185012",
    "https://openalex.org/W3123407021",
    "https://openalex.org/W1980510330",
    "https://openalex.org/W2998862821",
    "https://openalex.org/W3169890186",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W2138664833",
    "https://openalex.org/W3011721937",
    "https://openalex.org/W2962059918",
    "https://openalex.org/W3164718925",
    "https://openalex.org/W3109763399",
    "https://openalex.org/W1517663366",
    "https://openalex.org/W2911681509",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2982915530",
    "https://openalex.org/W2965357874",
    "https://openalex.org/W2114766824",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3175339114",
    "https://openalex.org/W1677409904",
    "https://openalex.org/W2962900737",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W1534477342",
    "https://openalex.org/W3137080992",
    "https://openalex.org/W3009625718",
    "https://openalex.org/W2167541073",
    "https://openalex.org/W3034972497",
    "https://openalex.org/W2164739783",
    "https://openalex.org/W9657784",
    "https://openalex.org/W3148330722",
    "https://openalex.org/W3190965961",
    "https://openalex.org/W3212494601",
    "https://openalex.org/W2962957005",
    "https://openalex.org/W3174288500",
    "https://openalex.org/W3122548859",
    "https://openalex.org/W3121596465",
    "https://openalex.org/W2802642435",
    "https://openalex.org/W2962730405",
    "https://openalex.org/W2911087563",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2997710335",
    "https://openalex.org/W3128974224",
    "https://openalex.org/W3003802619",
    "https://openalex.org/W2962528222",
    "https://openalex.org/W2504978847",
    "https://openalex.org/W3007684729",
    "https://openalex.org/W3161867324",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W3108655859",
    "https://openalex.org/W3009571263",
    "https://openalex.org/W2963634205",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W2896439460",
    "https://openalex.org/W3023363161",
    "https://openalex.org/W2986128786",
    "https://openalex.org/W2963523627",
    "https://openalex.org/W3100711616",
    "https://openalex.org/W3035362403",
    "https://openalex.org/W2995154514",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3156216837",
    "https://openalex.org/W3122847979",
    "https://openalex.org/W3038443040",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3176198948",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2063303346",
    "https://openalex.org/W3019437576",
    "https://openalex.org/W3029724270",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W3126362025",
    "https://openalex.org/W3098372854",
    "https://openalex.org/W3168745292",
    "https://openalex.org/W1583912456",
    "https://openalex.org/W3105871743",
    "https://openalex.org/W3037749908",
    "https://openalex.org/W1899309388",
    "https://openalex.org/W3176502545",
    "https://openalex.org/W2996641835",
    "https://openalex.org/W2411042438",
    "https://openalex.org/W3155267227",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3009354621",
    "https://openalex.org/W2964340170",
    "https://openalex.org/W3157218387",
    "https://openalex.org/W2964318098",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3025552214",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2143522680",
    "https://openalex.org/W1974424100",
    "https://openalex.org/W2279316390",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W3184784418",
    "https://openalex.org/W3128232076",
    "https://openalex.org/W3169976301",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2026520905",
    "https://openalex.org/W2742126782",
    "https://openalex.org/W2776202271",
    "https://openalex.org/W1998898494",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W2036963181",
    "https://openalex.org/W3013484318",
    "https://openalex.org/W2990397898",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W3038245394",
    "https://openalex.org/W2117868896",
    "https://openalex.org/W1969608617",
    "https://openalex.org/W2971034910",
    "https://openalex.org/W3154575616",
    "https://openalex.org/W3083104667",
    "https://openalex.org/W2972668795",
    "https://openalex.org/W3112116031",
    "https://openalex.org/W2091845343",
    "https://openalex.org/W3108961219",
    "https://openalex.org/W2513506629",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3035446294",
    "https://openalex.org/W3211962263",
    "https://openalex.org/W2124386111",
    "https://openalex.org/W2519091744",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2970283086",
    "https://openalex.org/W3036369012",
    "https://openalex.org/W2996086858",
    "https://openalex.org/W3015982254",
    "https://openalex.org/W3120167236",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W3033733989",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W1581742186",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2897056441",
    "https://openalex.org/W2563734883",
    "https://openalex.org/W2963826371",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W2895777173",
    "https://openalex.org/W3126354626",
    "https://openalex.org/W2962736495",
    "https://openalex.org/W2962790223",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2990751682",
    "https://openalex.org/W3120044914",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2164418233",
    "https://openalex.org/W2215686446",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W1517749756",
    "https://openalex.org/W2095739681",
    "https://openalex.org/W3036438747",
    "https://openalex.org/W2911742574",
    "https://openalex.org/W3169064633",
    "https://openalex.org/W2775677082",
    "https://openalex.org/W3047863327",
    "https://openalex.org/W2951046202",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2973525135",
    "https://openalex.org/W3204412088",
    "https://openalex.org/W2095932468",
    "https://openalex.org/W3156891177",
    "https://openalex.org/W1529593020",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W2964118342",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W2963636167",
    "https://openalex.org/W2398582829",
    "https://openalex.org/W2896868419",
    "https://openalex.org/W2962854673",
    "https://openalex.org/W1594201624",
    "https://openalex.org/W3214068000",
    "https://openalex.org/W3171483929",
    "https://openalex.org/W3025733956",
    "https://openalex.org/W3101367838",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2591056115",
    "https://openalex.org/W2982486166",
    "https://openalex.org/W3187047591",
    "https://openalex.org/W3171633069",
    "https://openalex.org/W3037163353",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2992319600",
    "https://openalex.org/W2963139417",
    "https://openalex.org/W3133441681",
    "https://openalex.org/W2968547875",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W1988702697",
    "https://openalex.org/W1677253155",
    "https://openalex.org/W2970571188",
    "https://openalex.org/W417212831",
    "https://openalex.org/W2162041888",
    "https://openalex.org/W2135260086",
    "https://openalex.org/W3164935232",
    "https://openalex.org/W1980862600",
    "https://openalex.org/W2134816385",
    "https://openalex.org/W2119002393",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W2981030070",
    "https://openalex.org/W1738019091",
    "https://openalex.org/W2274565976",
    "https://openalex.org/W2098774185",
    "https://openalex.org/W3010102662",
    "https://openalex.org/W3032086959",
    "https://openalex.org/W2899027170",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W2775461895",
    "https://openalex.org/W2104094955",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W3144696998",
    "https://openalex.org/W3157788795",
    "https://openalex.org/W3119719070",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2131629857",
    "https://openalex.org/W2986445670",
    "https://openalex.org/W2927351257",
    "https://openalex.org/W3036318353",
    "https://openalex.org/W2944050646",
    "https://openalex.org/W2774423163",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2954996726",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W3035302202",
    "https://openalex.org/W1985683154",
    "https://openalex.org/W3204086435",
    "https://openalex.org/W3145444543",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3043057662",
    "https://openalex.org/W3167831519",
    "https://openalex.org/W2237880282",
    "https://openalex.org/W3119334521",
    "https://openalex.org/W1965976819",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W2964335542",
    "https://openalex.org/W3101155908",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W2975758481",
    "https://openalex.org/W2963919731",
    "https://openalex.org/W3042040243",
    "https://openalex.org/W2805389643",
    "https://openalex.org/W2051267297",
    "https://openalex.org/W3173182783",
    "https://openalex.org/W3136363192",
    "https://openalex.org/W2462906003",
    "https://openalex.org/W2775714759",
    "https://openalex.org/W3120827806",
    "https://openalex.org/W3161922060",
    "https://openalex.org/W2963262589",
    "https://openalex.org/W2541261117",
    "https://openalex.org/W1552513054",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2962957031",
    "https://openalex.org/W2991870143",
    "https://openalex.org/W3124633769",
    "https://openalex.org/W2962411443",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W3171737424",
    "https://openalex.org/W3170111948",
    "https://openalex.org/W1567324058",
    "https://openalex.org/W2971316968",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W3094446431",
    "https://openalex.org/W2605717780",
    "https://openalex.org/W3013547323",
    "https://openalex.org/W3021397474",
    "https://openalex.org/W2736137960",
    "https://openalex.org/W3089824566",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2604036740",
    "https://openalex.org/W2962922665",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W1821977771",
    "https://openalex.org/W2963802910",
    "https://openalex.org/W2884943453",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3009999522",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3152791938",
    "https://openalex.org/W3152801999",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W2946011656",
    "https://openalex.org/W2896721250",
    "https://openalex.org/W2139484679",
    "https://openalex.org/W3167002466",
    "https://openalex.org/W2963172539",
    "https://openalex.org/W3009928773",
    "https://openalex.org/W1786044565",
    "https://openalex.org/W3035267217",
    "https://openalex.org/W2747592475",
    "https://openalex.org/W3006130287",
    "https://openalex.org/W116673708",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W3113447514",
    "https://openalex.org/W3193102826",
    "https://openalex.org/W3112788634",
    "https://openalex.org/W3094749184",
    "https://openalex.org/W3149148583",
    "https://openalex.org/W2161444669",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W1984449009",
    "https://openalex.org/W1559010669",
    "https://openalex.org/W1591713425",
    "https://openalex.org/W1597739853",
    "https://openalex.org/W2951585248",
    "https://openalex.org/W2911940735",
    "https://openalex.org/W2971278627",
    "https://openalex.org/W2889624842",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2075812737",
    "https://openalex.org/W2103066851",
    "https://openalex.org/W2102167286",
    "https://openalex.org/W3035172746",
    "https://openalex.org/W2979186816",
    "https://openalex.org/W3093122194",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W179875071",
    "https://openalex.org/W3182696977",
    "https://openalex.org/W2146388339",
    "https://openalex.org/W2896385413",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2963982496",
    "https://openalex.org/W3167355190",
    "https://openalex.org/W2091966899",
    "https://openalex.org/W2518186251",
    "https://openalex.org/W2961073278",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W3136736511",
    "https://openalex.org/W3105424285",
    "https://openalex.org/W2112507308",
    "https://openalex.org/W2122617678",
    "https://openalex.org/W2009225674",
    "https://openalex.org/W3155287831",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W3006320872",
    "https://openalex.org/W3121000782",
    "https://openalex.org/W3184011599",
    "https://openalex.org/W2888230425",
    "https://openalex.org/W3162938759",
    "https://openalex.org/W2508771403",
    "https://openalex.org/W2736809457",
    "https://openalex.org/W3034656957",
    "https://openalex.org/W3103536442",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2120354757",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W2995406462",
    "https://openalex.org/W3035668167",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2962822926",
    "https://openalex.org/W2963382180",
    "https://openalex.org/W2996603747",
    "https://openalex.org/W2607719644",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W2964235839",
    "https://openalex.org/W1923658367",
    "https://openalex.org/W3047498477",
    "https://openalex.org/W3117576675",
    "https://openalex.org/W2963596026",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2970789589",
    "https://openalex.org/W3136987292",
    "https://openalex.org/W2940107683",
    "https://openalex.org/W3173154111",
    "https://openalex.org/W2897154134",
    "https://openalex.org/W2061562262",
    "https://openalex.org/W1982250324",
    "https://openalex.org/W26351987",
    "https://openalex.org/W3098037474",
    "https://openalex.org/W2962990575",
    "https://openalex.org/W2963367210",
    "https://openalex.org/W3100659335",
    "https://openalex.org/W3094024085",
    "https://openalex.org/W2964342357",
    "https://openalex.org/W3174902251",
    "https://openalex.org/W2807910285",
    "https://openalex.org/W2962911098",
    "https://openalex.org/W3083832252",
    "https://openalex.org/W2063280109",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W3039737909",
    "https://openalex.org/W3166707765",
    "https://openalex.org/W2988620880",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2069239883",
    "https://openalex.org/W2805572053",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W3164182456",
    "https://openalex.org/W2769520592",
    "https://openalex.org/W3175311566",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3181186005",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2963943967",
    "https://openalex.org/W2186574084",
    "https://openalex.org/W2964339842",
    "https://openalex.org/W3040416015",
    "https://openalex.org/W2788388592",
    "https://openalex.org/W2969096242",
    "https://openalex.org/W2901900691",
    "https://openalex.org/W1991564165",
    "https://openalex.org/W2973832122",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3082042211",
    "https://openalex.org/W3133874049",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W3214095652",
    "https://openalex.org/W2056621158",
    "https://openalex.org/W2963590100",
    "https://openalex.org/W3104739822",
    "https://openalex.org/W3174519801",
    "https://openalex.org/W2097295005",
    "https://openalex.org/W2144881961",
    "https://openalex.org/W2929088353",
    "https://openalex.org/W2417717420",
    "https://openalex.org/W3158990693",
    "https://openalex.org/W3164045210",
    "https://openalex.org/W1640628807",
    "https://openalex.org/W2971067248",
    "https://openalex.org/W2911526543",
    "https://openalex.org/W2994615180",
    "https://openalex.org/W2963305465",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W1594873046",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3156643189",
    "https://openalex.org/W3082115681",
    "https://openalex.org/W3046882683",
    "https://openalex.org/W2995993311",
    "https://openalex.org/W2962787423",
    "https://openalex.org/W3099579437",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W1986209830",
    "https://openalex.org/W3172314079",
    "https://openalex.org/W2102085635",
    "https://openalex.org/W2963712109",
    "https://openalex.org/W3143784018",
    "https://openalex.org/W3009065172",
    "https://openalex.org/W2963591054",
    "https://openalex.org/W3171418006",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W3091818438",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W3130198422",
    "https://openalex.org/W2907886210",
    "https://openalex.org/W2590082389",
    "https://openalex.org/W2963780574",
    "https://openalex.org/W2964105864",
    "https://openalex.org/W3169250519",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W2158108973",
    "https://openalex.org/W3195013866",
    "https://openalex.org/W2991588540",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3133802387",
    "https://openalex.org/W3137147200",
    "https://openalex.org/W3134095442",
    "https://openalex.org/W3163244177",
    "https://openalex.org/W2964060387",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W3036261496",
    "https://openalex.org/W3124481204",
    "https://openalex.org/W3118216348",
    "https://openalex.org/W3177804148",
    "https://openalex.org/W2965838084",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W3103764297",
    "https://openalex.org/W3164854573",
    "https://openalex.org/W2953532875",
    "https://openalex.org/W1985125099",
    "https://openalex.org/W651564569",
    "https://openalex.org/W2131975293",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2995525544",
    "https://openalex.org/W2790166049",
    "https://openalex.org/W2899476926",
    "https://openalex.org/W3185922622",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W2412247133",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W2623107000",
    "https://openalex.org/W3088993044",
    "https://openalex.org/W3125792578",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W2961301154",
    "https://openalex.org/W3171613114",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W2068976153",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2992155277",
    "https://openalex.org/W2807730028",
    "https://openalex.org/W3125912651",
    "https://openalex.org/W3083835029",
    "https://openalex.org/W2472819217",
    "https://openalex.org/W2731230621",
    "https://openalex.org/W3122288112",
    "https://openalex.org/W2959065917",
    "https://openalex.org/W3104911444",
    "https://openalex.org/W2976890614",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3166052165",
    "https://openalex.org/W3166845084",
    "https://openalex.org/W3105023749",
    "https://openalex.org/W1514587017",
    "https://openalex.org/W3174714208",
    "https://openalex.org/W3155981360",
    "https://openalex.org/W3160314846",
    "https://openalex.org/W3048253689",
    "https://openalex.org/W2124354030",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3172673629",
    "https://openalex.org/W3165994454",
    "https://openalex.org/W3013451997",
    "https://openalex.org/W2611748211",
    "https://openalex.org/W2574741565",
    "https://openalex.org/W2952349219",
    "https://openalex.org/W3158303960",
    "https://openalex.org/W2591700809",
    "https://openalex.org/W2017369298",
    "https://openalex.org/W3168584517",
    "https://openalex.org/W3028766998",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W3033311119",
    "https://openalex.org/W3097981861",
    "https://openalex.org/W2129286663",
    "https://openalex.org/W2934582344",
    "https://openalex.org/W3155742828",
    "https://openalex.org/W2769112066",
    "https://openalex.org/W2963611459",
    "https://openalex.org/W3157919170",
    "https://openalex.org/W3177189402",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3173617765",
    "https://openalex.org/W2969881216",
    "https://openalex.org/W2963507484",
    "https://openalex.org/W266133316",
    "https://openalex.org/W2981720610",
    "https://openalex.org/W3133180075",
    "https://openalex.org/W3127572416",
    "https://openalex.org/W3109650690",
    "https://openalex.org/W2626792426",
    "https://openalex.org/W3175765954",
    "https://openalex.org/W192920577",
    "https://openalex.org/W2557283755",
    "https://openalex.org/W2109555487",
    "https://openalex.org/W3176484337",
    "https://openalex.org/W3175995235",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2910132948",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2885659818",
    "https://openalex.org/W3185376810",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W3104570641",
    "https://openalex.org/W3098288266",
    "https://openalex.org/W3174718878",
    "https://openalex.org/W3125419471",
    "https://openalex.org/W3117461888",
    "https://openalex.org/W2893749619",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W3023884994",
    "https://openalex.org/W3125460529",
    "https://openalex.org/W2971466976",
    "https://openalex.org/W3192405822",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W2945976633",
    "https://openalex.org/W3042720530",
    "https://openalex.org/W3091546937",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2980946630",
    "https://openalex.org/W3165345393",
    "https://openalex.org/W2991497298",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W3168154341",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3177174258",
    "https://openalex.org/W189742998",
    "https://openalex.org/W1532362218",
    "https://openalex.org/W2970259623",
    "https://openalex.org/W3098998028",
    "https://openalex.org/W3128644037",
    "https://openalex.org/W3046189516",
    "https://openalex.org/W64581525",
    "https://openalex.org/W2950398529",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W2201912979",
    "https://openalex.org/W3134111219",
    "https://openalex.org/W2888339491",
    "https://openalex.org/W3041990891",
    "https://openalex.org/W2151554678",
    "https://openalex.org/W1969767174",
    "https://openalex.org/W2337688041",
    "https://openalex.org/W3192479589",
    "https://openalex.org/W3106428938",
    "https://openalex.org/W1753385428",
    "https://openalex.org/W3166514800",
    "https://openalex.org/W2012312630",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W3133629262",
    "https://openalex.org/W2963818033",
    "https://openalex.org/W3103412473",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3216465419",
    "https://openalex.org/W2528489519",
    "https://openalex.org/W2606502823",
    "https://openalex.org/W3126553126",
    "https://openalex.org/W3028958483",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3087217556",
    "https://openalex.org/W3007309629",
    "https://openalex.org/W1488996941",
    "https://openalex.org/W3180342177",
    "https://openalex.org/W3098949126",
    "https://openalex.org/W3123388499",
    "https://openalex.org/W3137864305",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2991187744",
    "https://openalex.org/W2752747624",
    "https://openalex.org/W2965128575",
    "https://openalex.org/W2606433045",
    "https://openalex.org/W3017567979",
    "https://openalex.org/W3171159410",
    "https://openalex.org/W3171460770",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W3168236463",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W3108032709",
    "https://openalex.org/W2969388332",
    "https://openalex.org/W2579563793",
    "https://openalex.org/W254293499",
    "https://openalex.org/W3195782808",
    "https://openalex.org/W2971173235",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W2158392329",
    "https://openalex.org/W2902634493",
    "https://openalex.org/W2750404541",
    "https://openalex.org/W3012711473",
    "https://openalex.org/W3108602244",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W3132004818",
    "https://openalex.org/W2099017849",
    "https://openalex.org/W3130005562",
    "https://openalex.org/W1996672843",
    "https://openalex.org/W2006447892",
    "https://openalex.org/W2964071299",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W3154938165",
    "https://openalex.org/W2996094799",
    "https://openalex.org/W3037847693",
    "https://openalex.org/W3086007799",
    "https://openalex.org/W3035564946",
    "https://openalex.org/W2108554506",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2791655542",
    "https://openalex.org/W2957814540",
    "https://openalex.org/W3016154458",
    "https://openalex.org/W1907762490",
    "https://openalex.org/W2038252445",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3125934621",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W2948740140",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W2963459876",
    "https://openalex.org/W3133689212",
    "https://openalex.org/W2937108468",
    "https://openalex.org/W3156509887",
    "https://openalex.org/W2599025709",
    "https://openalex.org/W1965189280",
    "https://openalex.org/W3109947925",
    "https://openalex.org/W2914514892",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1500036797",
    "https://openalex.org/W2999044305",
    "https://openalex.org/W2980951591",
    "https://openalex.org/W3160574899",
    "https://openalex.org/W2971048680",
    "https://openalex.org/W2058583438",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2884554049",
    "https://openalex.org/W2963026102",
    "https://openalex.org/W2597603852",
    "https://openalex.org/W2483390977",
    "https://openalex.org/W2963855730",
    "https://openalex.org/W2932047266",
    "https://openalex.org/W2946787352",
    "https://openalex.org/W1995089537",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3041409527",
    "https://openalex.org/W2795038878",
    "https://openalex.org/W2969281871",
    "https://openalex.org/W2149252982",
    "https://openalex.org/W2945744671",
    "https://openalex.org/W2988421999",
    "https://openalex.org/W2994695355",
    "https://openalex.org/W2911835721",
    "https://openalex.org/W2886380958",
    "https://openalex.org/W2895560838",
    "https://openalex.org/W3120706522",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3004937305",
    "https://openalex.org/W2034328075",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W3153553004",
    "https://openalex.org/W3096973715",
    "https://openalex.org/W2973400721",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W3035180000",
    "https://openalex.org/W3130660608",
    "https://openalex.org/W2963656855",
    "https://openalex.org/W1742722778",
    "https://openalex.org/W2963214037",
    "https://openalex.org/W3021538051",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3155088253",
    "https://openalex.org/W1579838312",
    "https://openalex.org/W2129069237",
    "https://openalex.org/W3124729825",
    "https://openalex.org/W3169068609",
    "https://openalex.org/W2982157693",
    "https://openalex.org/W3174249157",
    "https://openalex.org/W1560257815",
    "https://openalex.org/W2145482038",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W3131755153",
    "https://openalex.org/W2956434358",
    "https://openalex.org/W2963819570",
    "https://openalex.org/W3013692475",
    "https://openalex.org/W3212014628",
    "https://openalex.org/W3125833936",
    "https://openalex.org/W2751765471",
    "https://openalex.org/W1933657216",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W2108703536",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2970442950",
    "https://openalex.org/W2970806862",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W1585486752",
    "https://openalex.org/W2432911982",
    "https://openalex.org/W1546928450",
    "https://openalex.org/W2525360812",
    "https://openalex.org/W3114796327",
    "https://openalex.org/W2913640436",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3125892916",
    "https://openalex.org/W2895763047",
    "https://openalex.org/W2989929945",
    "https://openalex.org/W3104223418",
    "https://openalex.org/W3048399222",
    "https://openalex.org/W2994896922",
    "https://openalex.org/W2910707576",
    "https://openalex.org/W3013629321",
    "https://openalex.org/W2893303656",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3149173402",
    "https://openalex.org/W3093768608",
    "https://openalex.org/W3135560598",
    "https://openalex.org/W2757528734",
    "https://openalex.org/W3112501082",
    "https://openalex.org/W3168796319",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W2487175659",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W2963052389",
    "https://openalex.org/W3039578880",
    "https://openalex.org/W3118813946",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W2967444033",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W3156194904",
    "https://openalex.org/W3038003025",
    "https://openalex.org/W3167118264",
    "https://openalex.org/W2472097883",
    "https://openalex.org/W2948625193",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3167252906",
    "https://openalex.org/W3104613320",
    "https://openalex.org/W3194210660",
    "https://openalex.org/W3168799536",
    "https://openalex.org/W2962972504",
    "https://openalex.org/W2810363185",
    "https://openalex.org/W3109762849",
    "https://openalex.org/W3169022154",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W3169291081",
    "https://openalex.org/W3174418826",
    "https://openalex.org/W3038831159",
    "https://openalex.org/W567721252",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2952826391",
    "https://openalex.org/W1952186860",
    "https://openalex.org/W3034528629",
    "https://openalex.org/W1535992660",
    "https://openalex.org/W3159134453",
    "https://openalex.org/W2995592319",
    "https://openalex.org/W2952866402",
    "https://openalex.org/W2971088228",
    "https://openalex.org/W3157225713",
    "https://openalex.org/W1487554816",
    "https://openalex.org/W3017701505",
    "https://openalex.org/W3140760072",
    "https://openalex.org/W3139729976",
    "https://openalex.org/W2604799231",
    "https://openalex.org/W2810095012",
    "https://openalex.org/W2962919088",
    "https://openalex.org/W2139612737",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W1967097499",
    "https://openalex.org/W3209796554",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W3010694149",
    "https://openalex.org/W3192256856",
    "https://openalex.org/W2914112028",
    "https://openalex.org/W3117865842",
    "https://openalex.org/W2011605810",
    "https://openalex.org/W3037697022",
    "https://openalex.org/W2963578416",
    "https://openalex.org/W2939984132",
    "https://openalex.org/W3034917890",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3028676366",
    "https://openalex.org/W3004636994",
    "https://openalex.org/W3096964654",
    "https://openalex.org/W2964001908",
    "https://openalex.org/W3101243562",
    "https://openalex.org/W2606722458",
    "https://openalex.org/W3016013581",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2979715301",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W2404544029",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3093859587",
    "https://openalex.org/W3030416041",
    "https://openalex.org/W3159875059",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2038200352",
    "https://openalex.org/W3131999495",
    "https://openalex.org/W2963464195",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3094136328",
    "https://openalex.org/W3156636320",
    "https://openalex.org/W3027353834",
    "https://openalex.org/W2769041395",
    "https://openalex.org/W2966564143",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W3137474564",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3104150569",
    "https://openalex.org/W2884061367",
    "https://openalex.org/W2157539922",
    "https://openalex.org/W2113072832",
    "https://openalex.org/W2963827721",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2140534852",
    "https://openalex.org/W1990100773",
    "https://openalex.org/W2900833639",
    "https://openalex.org/W2899136066",
    "https://openalex.org/W2990051111",
    "https://openalex.org/W3093882308",
    "https://openalex.org/W1932116995",
    "https://openalex.org/W2565695915",
    "https://openalex.org/W3174266714",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W3172068781",
    "https://openalex.org/W2915268314",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3012624518",
    "https://openalex.org/W2125124735",
    "https://openalex.org/W3195547103",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W3183009143",
    "https://openalex.org/W2799058376",
    "https://openalex.org/W3208631485",
    "https://openalex.org/W3157186409",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W3047375952",
    "https://openalex.org/W3128677983",
    "https://openalex.org/W2953494151",
    "https://openalex.org/W72496981",
    "https://openalex.org/W2126204609",
    "https://openalex.org/W2211629196",
    "https://openalex.org/W633707111",
    "https://openalex.org/W3188969238",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W3170051695",
    "https://openalex.org/W2529194139",
    "https://openalex.org/W3035729345",
    "https://openalex.org/W3083918922",
    "https://openalex.org/W3122334902",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2929928372",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2007336324",
    "https://openalex.org/W2073555950",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W3133204645",
    "https://openalex.org/W2983841094",
    "https://openalex.org/W3029010582",
    "https://openalex.org/W3133759846",
    "https://openalex.org/W2618318883",
    "https://openalex.org/W2901707424",
    "https://openalex.org/W3177095392",
    "https://openalex.org/W3150689959",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2964185501",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3133114212",
    "https://openalex.org/W3090656107",
    "https://openalex.org/W3133206434",
    "https://openalex.org/W3125488228",
    "https://openalex.org/W3098680936",
    "https://openalex.org/W2143891888",
    "https://openalex.org/W3098425262",
    "https://openalex.org/W2770056630",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W2735625891",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W2034392969",
    "https://openalex.org/W2319844980",
    "https://openalex.org/W2963326042",
    "https://openalex.org/W3087547017",
    "https://openalex.org/W3173765893",
    "https://openalex.org/W3107906842",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W3016970897",
    "https://openalex.org/W3027735641",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W3173247149",
    "https://openalex.org/W3035000591",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2897142515",
    "https://openalex.org/W2044319790",
    "https://openalex.org/W3094459920",
    "https://openalex.org/W3213382547",
    "https://openalex.org/W3175727500",
    "https://openalex.org/W2963273111",
    "https://openalex.org/W2170479554",
    "https://openalex.org/W3035102548",
    "https://openalex.org/W2106229755",
    "https://openalex.org/W3036684238",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W2903903405",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2619251116",
    "https://openalex.org/W2513408092",
    "https://openalex.org/W3128560087",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W1663973292",
    "https://openalex.org/W3168194750",
    "https://openalex.org/W3162689995",
    "https://openalex.org/W3183115158",
    "https://openalex.org/W2996325578",
    "https://openalex.org/W3167022671",
    "https://openalex.org/W2810192346",
    "https://openalex.org/W3166049810",
    "https://openalex.org/W3011762034",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W2025021739",
    "https://openalex.org/W2802594920",
    "https://openalex.org/W3104617516",
    "https://openalex.org/W2912237282",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W1963835707",
    "https://openalex.org/W3126379365",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W3140373187",
    "https://openalex.org/W3100909505",
    "https://openalex.org/W3126258442",
    "https://openalex.org/W3126568801",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W3082815030",
    "https://openalex.org/W1677767588",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W3166143997",
    "https://openalex.org/W3033324992",
    "https://openalex.org/W3163497472",
    "https://openalex.org/W2136922672",
    "https://openalex.org/W2970395295",
    "https://openalex.org/W2900152462",
    "https://openalex.org/W2963579811",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W3163970098",
    "https://openalex.org/W3151216969",
    "https://openalex.org/W3037480398",
    "https://openalex.org/W3097977265",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2970854840",
    "https://openalex.org/W3162090017",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3212166010",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2114090089",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W3036677371",
    "https://openalex.org/W3095992020",
    "https://openalex.org/W3124701710",
    "https://openalex.org/W3113471301",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W3190479309",
    "https://openalex.org/W3185212449",
    "https://openalex.org/W1589481967",
    "https://openalex.org/W2945393245",
    "https://openalex.org/W2059216172",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3203534993",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2963448850",
    "https://openalex.org/W3038103902",
    "https://openalex.org/W3180801265",
    "https://openalex.org/W3124317057",
    "https://openalex.org/W2973042192",
    "https://openalex.org/W2981540061",
    "https://openalex.org/W2572789406",
    "https://openalex.org/W3164703200",
    "https://openalex.org/W2935928764",
    "https://openalex.org/W3208212324",
    "https://openalex.org/W2756889350",
    "https://openalex.org/W3098466758",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2963516265",
    "https://openalex.org/W3100177202",
    "https://openalex.org/W3135190223",
    "https://openalex.org/W2463120400",
    "https://openalex.org/W2953151445",
    "https://openalex.org/W3176945030",
    "https://openalex.org/W3177141404",
    "https://openalex.org/W2999490220",
    "https://openalex.org/W2963120843",
    "https://openalex.org/W2963826681",
    "https://openalex.org/W3000127051",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2970377754",
    "https://openalex.org/W2945057689",
    "https://openalex.org/W1483819513",
    "https://openalex.org/W2787752464",
    "https://openalex.org/W3034220715",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W3035990676",
    "https://openalex.org/W2754979670",
    "https://openalex.org/W3034515982",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W3116286104",
    "https://openalex.org/W3214375535",
    "https://openalex.org/W3156661774"
  ],
  "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
  "full_text": "On the Opportunities and Risks of\nFoundation Models\nRishi Bommasani* Drew A. Hudson Ehsan Adeli Russ Altman Simran Arora\nSydney von Arx Michael S. Bernstein Jeannette Bohg Antoine Bosselut Emma Brunskill\nErik Brynjolfsson Shyamal Buch Dallas Card Rodrigo Castellon Niladri Chatterji\nAnnie Chen Kathleen Creel Jared Quincy Davis Dorottya Demszky Chris Donahue\nMoussa Doumbouya Esin Durmus Stefano Ermon John Etchemendy Kawin Ethayarajh\nLi Fei-Fei Chelsea Finn Trevor Gale Lauren Gillespie Karan Goel Noah Goodman\nShelby Grossman Neel Guha Tatsunori Hashimoto Peter Henderson John Hewitt\nDaniel E. Ho Jenny Hong Kyle Hsu Jing Huang Thomas Icard Saahil Jain\nDan Jurafsky Pratyusha Kalluri Siddharth Karamcheti Geoff Keeling Fereshte Khani\nOmar Khattab Pang Wei Koh Mark Krass Ranjay Krishna Rohith Kuditipudi\nAnanya Kumar Faisal Ladhak Mina Lee Tony Lee Jure Leskovec Isabelle Levent\nXiang Lisa Li Xuechen Li Tengyu Ma Ali Malik Christopher D. Manning\nSuvir Mirchandani Eric Mitchell Zanele Munyikwa Suraj Nair Avanika Narayan\nDeepak Narayanan Ben Newman Allen Nie Juan Carlos Niebles Hamed Nilforoshan\nJulian Nyarko Giray Ogut Laurel Orr Isabel Papadimitriou Joon Sung Park Chris Piech\nEva Portelance Christopher Potts Aditi Raghunathan Rob Reich Hongyu Ren\nFrieda Rong Yusuf Roohani Camilo Ruiz Jack Ryan Christopher Ré Dorsa Sadigh\nShiori Sagawa Keshav Santhanam Andy Shih Krishnan Srinivasan Alex Tamkin\nRohan Taori Armin W. Thomas Florian Tramèr Rose E. Wang William Wang Bohan Wu\nJiajun Wu Yuhuai Wu Sang Michael Xie Michihiro Yasunaga Jiaxuan You Matei Zaharia\nMichael Zhang Tianyi Zhang Xikun Zhang Yuhui Zhang Lucia Zheng Kaitlyn Zhou\nPercy Liang*1\nCenter for Research on Foundation Models (CRFM)\nStanford Institute for Human-Centered Artificial Intelligence (HAI)\nStanford University\nAI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) trained on broad\ndata (generally using self-supervision at scale) that can be adapted to a wide range of downstream tasks.\nWe call these models foundation models to underscore their critically central yet incomplete character.\nThis report provides a thorough account of the opportunities and risks of foundation models, ranging\nfrom their capabilities (e.g., language, vision, robotic manipulation, reasoning, human interaction) and\ntechnical principles (e.g., model architectures, training procedures, data, systems, security, evaluation,\ntheory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse,\neconomic and environmental impact, legal and ethical considerations). Though foundation models are\nbased on standard deep learning and transfer learning, their scale results in new emergent capabilities,\nand their effectiveness across so many tasks incentivizes homogenization. Homogenization provides\npowerful leverage but demands caution, as the defects of the foundation model are inherited by all the\nadapted models downstream. Despite the impending widespread deployment of foundation models,\nwe currently lack a clear understanding of how they work, when they fail, and what they are even\ncapable of due to their emergent properties. To tackle these questions, we believe much of the critical\nresearch on foundation models will require deep interdisciplinary collaboration commensurate with\ntheir fundamentally sociotechnical nature.\n1Corresponding author: pliang@cs.stanford.edu *Equal contribution.\n1\narXiv:2108.07258v3  [cs.LG]  12 Jul 2022\n2 Center for Research on Foundation Models (CRFM)\nContents\nContents 2\n1 Introduction 3\n1.1 Emergence and homogenization 3\n1.2 Social impact and the foundation models ecosystem 7\n1.3 The future of foundation models 9\n1.4 Overview of this report 12\n2 Capabilities 21\n2.1 Language 22\n2.2 Vision 28\n2.3 Robotics 34\n2.4 Reasoning and search 40\n2.5 Interaction 44\n2.6 Philosophy of understanding 48\n3 Applications 53\n3.1 Healthcare and biomedicine 54\n3.2 Law 59\n3.3 Education 67\n4 Technology 73\n4.1 Modeling 74\n4.2 Training 81\n4.3 Adaptation 85\n4.4 Evaluation 91\n4.5 Systems 97\n4.6 Data 101\n4.7 Security and privacy 105\n4.8 Robustness to distribution shifts 109\n4.9 AI safety and alignment 114\n4.10 Theory 118\n4.11 Interpretability 123\n5 Society 129\n5.1 Inequity and fairness 130\n5.2 Misuse 136\n5.3 Environment 140\n5.4 Legality 146\n5.5 Economics 149\n5.6 Ethics of scale 152\n6 Conclusion 161\nAcknowledgments 161\nReferences 161\nOn the Opportunities and Risks of Foundation Models 3\n1 INTRODUCTION\nThis report investigates an emerging paradigm for building artificial intelligence (AI) systems\nbased on a general class of models which we term foundation models .2 A foundation model is any\nmodel that is trained on broad data (generally using self-supervision at scale) that can be adapted\n(e.g., fine-tuned) to a wide range of downstream tasks; current examples include BERT [Devlin et al.\n2019], GPT-3 [Brown et al. 2020], and CLIP [Radford et al. 2021]. From a technological point of view,\nfoundation models are not new — they are based on deep neural networks and self-supervised\nlearning, both of which have existed for decades. However, the sheer scale and scope of foundation\nmodels from the last few years have stretched our imagination of what is possible; for example,\nGPT-3 has 175 billion parameters and can be adapted via natural language prompts to do a passable\njob on a wide range of tasks despite not being trained explicitly to do many of those tasks [Brown\net al. 2020]. At the same time, existing foundation models have the potential to accentuate harms,\nand their characteristics are in general poorly understood. Given their impending widespread\ndeployment, they have become a topic of intense scrutiny [Bender et al. 2021].\n1.1 Emergence and homogenization\nThe significance of foundation models can be summarized by two words: emergence and homoge-\nnization. Emergence means that the behavior of a system is implicitly induced rather than explicitly\nconstructed; it is both the source of scientific excitement and anxiety about unanticipated con-\nsequences. Homogenization indicates the consolidation of methodologies for building machine\nlearning systems across a wide range of applications; it provides strong leverage towards many\ntasks but also creates single points of failure. To better appreciate emergence and homogenization,\nlet us reflect on their rise in AI research over the last 30 years.\nFig. 1. The story of AI has been one of increasing emergence and homogenization. With the introduction of\nmachine learning, how a task is performed emerges (is inferred automatically) from examples; with deep\nlearning, the high-level features used for prediction emerge; and with foundation models, even advanced\nfunctionalities such as in-context learning emerge. At the same time, machine learning homogenizes learning\nalgorithms (e.g., logistic regression), deep learning homogenizes model architectures (e.g., Convolutional\nNeural Networks), and foundation models homogenizes the model itself (e.g., GPT-3).\nMachine learning. Most AI systems today are powered by machine learning, where predictive\nmodels are trained on historical data and used to make future predictions. The rise of machine\nlearning within AI started in the 1990s, representing a marked shift from the way AI systems were\nbuilt previously: rather than specifying how to solve a task, a learning algorithm would induce\nit based on data — i.e., the how emerges from the dynamics of learning. Machine learning also\n2We chose the termfoundation models to capture the unfinished yet important status of these models — see §1.1.1:naming\nfor further discussion of the name.\n4 Center for Research on Foundation Models (CRFM)\nrepresented a step towards homogenization: a wide range of applications could now be powered\nby a single generic learning algorithm such as logistic regression.\nDespite the ubiquity of machine learning within AI, semantically complex tasks in natural lan-\nguage processing (NLP) and computer vision such as question answering or object recognition,\nwhere the inputs are sentences or images, still required domain experts to perform “feature en-\ngineering” — that is, writing domain-specific logic to convert raw data into higher-level features\n(e.g., SIFT [Lowe 1999] in computer vision) that were more suitable for popular machine learning\nmethods.\nDeep learning. Around 2010, a revival of deep neural networks under the moniker of deep learn-\ning [LeCun et al. 2015] started gaining traction in the field of machine learning. Deep learning\nwas fueled by larger datasets, more computation (notably, the availability of GPUs), and greater\naudacity. Deep neural networks would be trained on the raw inputs (e.g., pixels), and higher-level\nfeatures would emerge through training (a process dubbed “representation learning”). This led to\nmassive performance gains on standard benchmarks, for example, in the seminal work of AlexNet\n[Krizhevsky et al. 2012] on the ImageNet dataset [Deng et al. 2009]. Deep learning also reflected a\nfurther shift towards homogenization: rather than having bespoke feature engineering pipelines for\neach application, the same deep neural network architecture could be used for many applications.\nFoundation models. Foundation models have taken shape most strongly in NLP, so we focus our\nstory there for the moment. That said, much as deep learning was popularized in computer vision\nbut exists beyond it, we understand foundation models as a general paradigm of AI, rather than\nspecific to NLP in any way. By the end of 2018, the field of NLP was about to undergo another\nseismic change, marking the beginning of the era of foundation models. On a technical level,\nfoundation models are enabled by transfer learning [Thrun 1998] and scale. The idea of transfer\nlearning is to take the “knowledge” learned from one task (e.g., object recognition in images) and\napply it to another task (e.g., activity recognition in videos). Within deep learning, pretraining is\nthe dominant approach to transfer learning: a model is trained on a surrogate task (often just as a\nmeans to an end) and then adapted to the downstream task of interest via fine-tuning.\nTransfer learning is what makes foundation models possible, but scale is what makes them\npowerful. Scale required three ingredients: (i) improvements in computer hardware — e.g., GPU\nthroughput and memory have increased 10 ×over the last four years (§4.5: systems); (ii) the\ndevelopment of the Transformer model architecture [Vaswani et al . 2017] that leverages the\nparallelism of the hardware to train much more expressive models than before (§4.1: modeling);\nand (iii) the availability of much more training data.\nThe importance of the availability of data and the ability to harness it cannot be underestimated.\nTransfer learning with annotated datasets has been common practice for at least a decade, for\nexample, pretraining on the ImageNet dataset [Deng et al . 2009] for image classification in the\ncomputer vision community. However, the non-trivial cost of annotation imposes a practical limit\non the benefits of pretraining.\nIn self-supervised learning on the other hand, the pretraining task is derived automatically from\nunannotated data.3 For example, the masked language modeling task used to train BERT [Devlin\net al. 2019] is to predict a missing word in a sentence given its surrounding context (e.g., I like\nsprouts). Self-supervised tasks are not only more scalable, only depending on unlabeled\ndata, but they are designed to force the model to predict parts of the inputs, making them richer\nand potentially more useful than models trained on a more limited label space.\n3Interestingly, self-supervised learning was dominant in the early days of deep learning [Hinton et al. 2006], but was for\na decade largely overtaken by pure supervised learning as labeled datasets became larger.\nOn the Opportunities and Risks of Foundation Models 5\nThere had been considerable progress in self-supervised learning dating back to word embeddings\n[Turian et al. 2010; Mikolov et al. 2013; Pennington et al. 2014], which associated each word with a\ncontext-independent vector, provided the basis for a wide range of NLP models. Shortly thereafter,\nself-supervised learning based on autoregressive language modeling (predict the next word given\nthe previous words) [Dai and Le 2015] became popular. This produced models that represented\nwords in context, such as GPT [Radford et al. 2018], ELMo [Peters et al. 2018], and ULMFiT [Howard\nand Ruder 2018].4\nThe next wave of developments in self-supervised learning — BERT [Devlin et al. 2019] GPT-2\n[Radford et al. 2019], RoBERTa [Liu et al. 2019], T5 [Raffel et al. 2019], BART [Lewis et al. 2020a] —\nquickly followed, embracing the Transformer architecture, incorporating more powerful deep\nbidirectional encoders of sentences, and scaling up to larger models and datasets.\nWhile one can view this last wave of technical developments purely through the lens of self-\nsupervised learning, there was a sociological inflection point around the introduction of BERT.\nBefore 2019, self-supervised learning with language models was essentially asubarea in NLP, which\nprogressed in parallel to other developments in NLP. After 2019, self-supervised learning with\nlanguage models became more of a substrate of NLP, as using BERT has become the norm. The\nacceptance that a single model could be useful for such a wide range of tasks marks the beginning\nof the era of foundation models.\nFoundation models have led to an unprecedented level of homogenization: Almost all state-of-\nthe-art NLP models are now adapted from one of a few foundation models, such as BERT, RoBERTa,\nBART, T5, etc. While this homogenization produces extremely high leverage (any improvements in\nthe foundation models can lead to immediate benefits across all of NLP), it is also a liability; all AI\nsystems might inherit the same problematic biases of a few foundation models [Bolukbasi et al .\n2016; Caliskan et al. 2017; Abid et al. 2021, inter alia ]) — see §5.1: fairness, §5.6: ethics for further\ndiscussion.\nWe are also beginning to see a homogenization across research communities. For example, similar\nTransformer-based sequence modeling approaches are now applied to text [Devlin et al . 2019;\nRadford et al. 2019; Raffel et al. 2019], images [Dosovitskiy et al. 2020; Chen et al. 2020d], speech [Liu\net al. 2020d], tabular data [Yin et al. 2020], protein sequences [Rives et al. 2021], organic molecules\n[Rothchild et al. 2021], and reinforcement learning [Chen et al. 2021b; Janner et al. 2021]. These\nexamples point to a possible future where we have a unified set of tools for developing foundation\nmodels across a wide range of modalities [Tamkin et al. 2021b].\nBesides the homogenization of approaches, we also see the homogenization of actual models\nacross research communities in the form of multimodal models — e.g., foundation models trained\non language and vision data [Luo et al. 2020; Kim et al. 2021a; Cho et al. 2021; Ramesh et al. 2021;\nRadford et al. 2021]. Data is naturally multimodal in some domains—e.g., medical images, structured\ndata, clinical text in healthcare (§3.1: healthcare). Thus, multimodal foundation models are a\nnatural way of fusing all the relevant information about a domain, and adapting to tasks that also\nspan multiple modes (Figure 2).\nFoundation models have also led to surprising emergence which results from scale. For example,\nGPT-3 [Brown et al. 2020], with 175 billion parameters compared to GPT-2’s 1.5 billion, permits\nin-context learning, in which the language model can be adapted to a downstream task simply by\nproviding it with a prompt (a natural language description of the task), an emergent property that\nwas neither specifically trained for nor anticipated to arise.\n4The prescient work of Collobert and Weston [2008] is related: they trained on a scalable task akin to masked language\nmodeling jointly with downstream tasks, rather than producing a single foundation model that can be adapted after the fact\nto downstream tasks.\n6 Center for Research on Foundation Models (CRFM)\nFig. 2. A foundation model can centralize the information from all the data from various modalities. This\none model can then be adapted to a wide range of downstream tasks.\nHomogenization and emergence interact in a potentially unsettling way. Homogenization could\npotentially provide enormous gains for many domains where task-specific data is quite limited — see\nthe opportunities presented in several such domains (e.g., §3.1: healthcare, §3.2: law, §3.3: edu-\ncation); on the other hand, any flaws in the model are blindly inherited by all adapted models\n(§5.1: fairness, §5.6: ethics). Since the power of foundation models comes from their emergent\nqualities rather than their explicit construction, existing foundation models are hard to understand\n(§4.4: evaluation, §4.10: theory, §4.11: interpretability) and they have unexpected failure\nmodes (§4.7: security, §4.8: robustness). Since emergence generates substantial uncertainty over\nthe capabilities and flaws of foundation models, aggressive homogenization through these models is\nrisky business. Derisking is the central challenge in the further development of foundation models\nfrom an ethical (§5.6: ethics) and AI safety (§4.9: ai-safety) perspective.\n1.1.1 Naming.\nWe introduce the term foundation models to fill a void in describing the paradigm shift we are\nwitnessing; we briefly recount some of our reasoning for this decision. Existing terms (e.g.,pretrained\nmodel, self-supervised model ) partially capture the technical dimension of these models, but fail to\ncapture the significance of the paradigm shift in an accessible manner for those beyond machine\nlearning. In particular, foundation model designates a model class that are distinctive in their\nsociological impact and how they have conferred a broad shift in AI research and deployment.\nIn contrast, forms of pretraining and self-supervision that technically foreshadowed foundation\nmodels fail to clarify the shift in practices we hope to highlight.\nOn the Opportunities and Risks of Foundation Models 7\nFig. 3. Before reasoning about the social impact of foundation models, it is important to understand that\nthey are part of a broader ecosystem that stretches from data creation to deployment. At both ends, we\nhighlight the role of people as the ultimate source of data into training of a foundation model, but also as the\ndownstream recipients of any benefits and harms. Thoughtful data curation and adaptation should be part\nof the responsible development of any AI system. Finally, note that the deployment of adapted foundation\nmodels is a decision separate from their construction, which could be for research.\nAdditionally, while many of the iconic foundation models at the time of writing are language\nmodels, the term language model is simply too narrow for our purpose: as we describe, the scope of\nfoundation models goes well beyond language. We also considered terms such as general-purpose\nmodel and multi-purpose model that capture the important aspect that these models can serve\nmultiple downstream tasks, but both fail to capture their unfinished character and the need for\nadaptation. Terms such as task-agnostic model would capture the manner of training, but fail to\ncapture the significant implication to downstream applications.\nWe chose the new termfoundation models to identify the models and the emerging paradigm that\nare the subject of this report. In particular, the word “foundation” specifies the role these models\nplay: a foundation model is itself incomplete but serves as the common basis from which many\ntask-specific models are built via adaptation. We also chose the term “foundation\" to connote the\nsignificance of architectural stability, safety, and security: poorly-constructed foundations are a\nrecipe for disaster and well-executed foundations are a reliable bedrock for future applications. At\npresent, we emphasize that we do not fully understand the nature or quality of the foundation that\nfoundation models provide; we cannot characterize whether the foundation is trustworthy or not.\nThus, this is a critical problem for researchers, foundation model providers, application developers\nwho rely on foundation models, policymakers, and society at large to address.\n1.2 Social impact and the foundation models ecosystem\nFoundation models are scientifically interesting due to their impressive performance and capabilities,\nbut what makes them critical to study is the fact that they are quickly being integrated into real-\nworld deployments of AI systems with far-reaching consequences on people. For example, Google\nsearch, which boasts 4 billion users, now depends on foundation models like BERT [Devlin et al.\n2019] as one of its signals.5\n5https://blog.google/products/search/search-language-understanding-bert/\n8 Center for Research on Foundation Models (CRFM)\nWe must thus pause and ask: What is the nature of this social impact? In this report, we address\nmany aspects of this question: the potential exacerbation of social inequities (§5.1: fairness), the\neconomic impact due to increased capabilities (§5.5: economics), the environmental impact due to\nincreased computation demands (§5.3: environment), potential concerns of amplifying disinfor-\nmation (§5.2: misuse), legal ramifications due to powerful generative capabilities (§5.4: legality),\nethical issues resulting from homogenization, and the broader political economy in which founda-\ntion models are developed and deployed (§5.6: ethics). Given the protean nature of foundation\nmodels and their unmapped capabilities, how can we responsibly anticipate and address the ethical\nand societal considerations they raise? A recurring theme is that it is easier to reason about the\nsocial impact of specific systems deployed to specific users than it is to reason about the social\nimpact of foundation models, which could be adapted to any number of unforeseen downstream\nsystems.\nBefore attempting to answer these questions, we need to lay some groundwork. First, let us\ndistinguish between research on foundation models and deployment of foundation models. Most of\nwhat is publicly known is foundation models research — through academic papers, demonstrations,\nand progress on leaderboards. While the production of knowledge can play a vital role in shaping\nthe future, the direct social impact is through the actual deployment of these models, which is\ngoverned by proprietary practices on often private data. Sometimes the deployment is through\nnew products — e.g., GitHub’s Copilot6 based on OpenAI’s Codex model [Chen et al. 2021f], but\noften, it is through upgrades to existing products (e.g., Google search using BERT). Research models\nare often not extensively tested and might have unknown failure modes; warning labels should\nbe placed on research models that are not fit to deploy. On the other hand, deployed foundation\nmodels that actually affect people’s lives should be subject to much more rigorous testing and\nauditing.\nTo further understand the research and deployment of foundation models, we must zoom out\nand consider the full ecosystem that these foundation models inhabit, from data creation to actual\ndeployment. It is important to note that the foundation model is only one component (though an\nincreasingly important component) of an AI system. Simplifying, we can think about the ecosystem\nof a foundation model in terms of sequence of stages, extending the training and adaptation stages\nfrom before.7 Appropriately, as we’re interested in social impact, people occupy both ends of the\npipeline. This ecosystem view allows us to see that different questions about foundation models\n(e.g., whether a foundation model is ethical) should actually be answered with respect to different\nstages.\n(1) Data creation: Data creation is fundamentally a human-centric process: all data is created\nby people and most data is at least implicitly about people. Sometimes data is created by\npeople for other people in the form of emails, articles, photos, etc., and sometimes it is a\nmeasurement of people (e.g., genomic data) or a measurement of the environment people\nlive in (e.g., satellite images). It is important to note that all data has an owner and is created\nwith a purpose (where that purpose may or may not include training a foundation model).\n(2) Data curation: Data is then curated into datasets. There is no single natural distribution\nof data; even the most permissive Internet crawl requires some selection and post-filtering.\nEnsuring data relevance and quality while respecting legal and ethical constraints is critical\nbut challenging. While this is recognized in industry, it is underappreciated in AI research\n(§4.6: data).\n6https://copilot.github.com/\n7In practice, the end of the pipeline is followed by monitoring, and feedback is used to readjust the previous stages.\nOn the Opportunities and Risks of Foundation Models 9\n(3) Training: Training foundation models on these curated datasets8 is the celebrated centerpiece\nin AI research, though it is only one of many stages.\n(4) Adaptation: In the context of machine learning research, adaptation is about creating a new\nmodel based on the foundation model that performs some task (e.g., document summarization).\nFor deployment, adaptation is about creating a system, which requires potentially many\ndifferent modules, custom rules (e.g., restrictions on the output space) or classifiers (e.g., for\ntoxicity classification), and combination with other complementary signals (e.g., a question\nanswering model’s generated answers would be validated against relevant documents). For\nexample, a problematic model capable of generating toxic content might be tolerable if\nappropriate precautions are taken downstream. The extra application-specific logic is crucial\nfor mitigating harms.\n(5) Deployment: The direct social impact of an AI system occurs when it is deployed to people.\nThough we would not want to deploy potentially harmful foundation models trained on\nquestionable data, there might still be value in permitting them in research to advance\nscientific understanding, though one must still exercise caution. More generally, it is standard\npractice in large-scale deployments to conduct gradual releases, where deployment happens\nto an increasing fraction of users; this can partially mitigate any potential harms.\nWhile this report is about foundation models, it is important to note that many of the impacts come\nfrom decisions made in other stages in the pipeline, and thoughtful monitoring and intervention is\nneeded at every stage. While large organizations might own the entire pipeline, each stage could\nbe performed by a different organization, e.g., a company which specializes in creating custom\nfoundation models for various domains that application-developers can use.\nThink ecosystem, act model. While the social impact depends on the whole ecosystem, it is still\nimportant to be able to reason about the social implications of a foundation model, given that many\nresearchers’ and practitioners’ purview is restricted to the training stage. This is difficult because\nfoundation models are unfinished intermediate objects that can be adapted to many downstream\napplications, sometimes by an entirely different entity for unforeseen purposes. What we need\nare two things: (i) surrogate metrics for a representative set of potential downstream evaluation\n(§4.4: evaluation), and (ii) a commitment to documenting these metrics [Mitchell et al . 2019]\nsimilar to data sheets for materials such as metals and plastics, which can be adapted to many\ndownstream use cases.\nCharacterizing the potential downstream social impact of foundation models is challenging and\ndemands a deep understanding of both the technological ecosystem and of society. One cannot\nfully assess the harms (§5.1: fairness) of a foundation model without recognizing how it will be\ndeployed, and one cannot just define automatic metrics without considering the rich social and\nhistorical context.\n1.3 The future of foundation models\nFoundation models have demonstrated raw potential, but we are still in the early days. Despite their\ndeployment into the real world, these models are very much research prototypes that are poorly\nunderstood. Even the professional norms — what Robert Merton calls the ethos of science [Merton\n1979] — around foundation models are underdeveloped. For example, there is lack of agreement on\nbasic questions such as when models are “safe” to release or how the community should react in\nresponse to methodological misconduct. Given that the future of foundation models is thus filled\nwith uncertainty, a big question is: who will determine this future?\n8A foundation model (e.g., Codex) can also be trained with another model (e.g., GPT-3) as a starting point.\n10 Center for Research on Foundation Models (CRFM)\nDisciplinary diversity. The technology behind foundation models is based on decades of research\nin machine learning, optimization, NLP, computer vision, and other fields. These technical contri-\nbutions have come from both academia and industrial research labs. However, research on building\nfoundation models themselves has occurred almost exclusively in industry — big tech companies\nsuch as Google, Facebook, Microsoft, or Huawei, or startups such as OpenAI or AI21 Labs, though\nAI2 is a notable exception [Peters et al. 2018; Zellers et al. 2019b].\nThe furious pace of technological progress and the entrenchment due to centralization raise\npowerful concerns that demand the attention of humanists and social scientists in addition to\ntechnologists. We should not rely on post-hoc audits of ethical and social consequences, conducted\nonly after the technical architecture and deployment decisions have been made. We instead need\nto infuse social considerations and ethical design deeply into the technological development of\nfoundation models and their surrounding ecosystem from the start. Academic institutions are\nunique in that they host the widest set of disciplines under one roof, thus bringing together\ncomputer scientists, social scientists, economists, ethicists, legal scholars, etc. Given the importance\nof disciplinary diversity in understanding and solving problems that combine technical, ethical,\nlegal, social, and political dimensions [Hong and Page 2004; Solomon 2006; Steel et al. 2018], we\ntherefore see academia as playing a crucial role in developing foundation models in such a way to\npromote their social benefit and mitigate their social harms, as well as determining the contexts\nunder which actions in each of the stages of the ecosystem (§1.2: ecosystem) ranging from data\ncuration to deployment should be strictly prohibited.\nIncentives. The political economy in which foundations models are designed, developed, and\ndeployed provides an inevitable incentive structure for decision-making at every stage. How\npeople and institutions respond to incentives is an elementary lesson of economics. Market-driven\ncommercial incentives can align well with social benefit: making foundation models more accurate,\nreliable, safe, and efficient while searching for a wide variety of potential use cases can produce a\ngreat deal of social utility. However, commercial incentives can also lead to market failures and\nunderinvestment in domains where shareholders are unable to capture the value of innovation.\nJust as the pharmaceutical industry has little incentive to devote significant resources to the\nresearch and development of malaria treatments, because poor people cannot afford medications,9\nthe tech industry has little incentive to devote significant resources to technologies designed\nfor improving the condition of poor and marginalized people [Reich et al . 2021]. What’s more,\ncommercial incentives can lead companies to ignore social externalities [Acemoglu 2021; Reich et al.\n2021] such as the technological displacement of labor, the health of an informational ecosystem\nrequired for democracy, the environmental cost of computing resources, and the profit-driven sale\nof technologies to non-democratic regimes. Finally, there is little incentive for any given company\nto create an open, decentralized ecosystem for developing foundation models that encourages\nbroad participation.\nIn contrast, the long-standing and deeply-seated research mission of universities is the production\nand dissemination of knowledge and creation of global public goods [Kerr 2001; Rhoten and Calhoun\n2011; Nussbaum 2010]. We believe that academia is distinctively positioned to shape the development\nof foundation models to ensure that we capture directions with potentially large social benefit that\nmight not otherwise be prioritized by industry.\nLoss in accessibility. Unfortunately, academia has not been able to participate in the fullest way\npossible due to the loss in accessibility. One of the often overlooked effects of the deep learning\nrevolution was the increase in reproducibility and open science: it increasingly became the norm\n9See https://www.gatesfoundation.org/about/our-role.\nOn the Opportunities and Risks of Foundation Models 11\nto publicly release code and datasets, and packages such as TensorFlow [Abadi et al . 2016] and\nPyTorch [Paszke et al. 2019] made it much easier for people to collaborate and build off of each\nother’s work. Initiatives like the ML Reproducibility Challenge10 as well as reproducibility checklists\nadopted by major conferences [Pineau et al. 2020], alongside platforms like CodaLab Worksheets11\nhelped advance community standards for reproducibility. This resulted in a surge in technological\ninnovation and progress.\nFoundation models start to roll back this positive trend. Some models (e.g., GPT-3) are not\nreleased at all (only API access to a limited pool of people). Even datasets (e.g., for GPT-2) are not\nreleased. While trained models may be available (e.g., BERT), the actual training of foundation\nmodels is unavailable to the vast majority of AI researchers, due to the much higher computational\ncost and the complex engineering requirements.\nSome meaningful research can still be done by training smaller models within reach of an\nacademic budget, and indeed the surprisingly regularity predicted by scaling laws [Kaplan et al.\n2020] make this a viable strategy for cases where the differences due to scale are quantitative\n(e.g., accuracy goes up). However, due to the emergent nature of these foundation models, some\nfunctionalities like in-context learning have only been demonstrated in models of sufficient size, so\nscale is needed to even ask the right questions.\nIt is also possible to productively study pre-existing models that have been released; indeed, this\nhas led to a large subcommunity within NLP for probing these models [Rogers et al. 2020; Manning\net al. 2020]. Having access to existing models can be useful for powering downstream applications\nor identifying defects (e.g., bias), but this might not be enough for us to design better architectures\nor training objectives for foundation models that can fix these defects (e.g., mitigate the bias). It is\nworth reflecting on how much of NLP research today is based on BERT, a particular (and somewhat\narbitrary) foundation model. Given the need to infuse social awareness and ethical design into the\nconstruction of these models, it is possible that we need to build foundation models that look quite\ndifferent from what exists today. This will demand intense experimentation at scale.\nCommunity efforts such as EleutherAI12 and Hugging Face’s BigScience project13 are attempting\nto train large foundation models, but the gap between the private models that industry can train\nand the ones that are open to the community will likely remain large if not grow. Further, today\nstartups (OpenAI, Anthropic, AI21 Labs, etc.) are much more well-resourced than academia and can\ntherefore still afford to train the largest foundation models (e.g., OpenAI’s GPT-3). However, big\ntech companies are on a completely different level in terms of resources, especially in terms of the\ninfrastructure, users, and data that come from their market position. The fundamental centralizing\nnature of foundation models means that the barrier to entry for developing them will continue\nto rise, so that even startups, despite their agility, will find it difficult to compete, a trend that is\nreflected in the development of search engines [Radinsky 2015].\nOne way to close the resource gap is for the government to invest in public infrastructure. We can\nlook to Big Science projects such as the Hubble Space Telescope and the Large Hadron Collider as\ninspiration, where substantial investment made possible fundamental scientific discoveries which\nwouldn’t have been possible. One can imagine a similar infrastructure for computing, from which\nacademic research on foundation models would greatly benefit. In the US, the nascent National\nResearch Cloud initiative14 is a step in this direction.\n10https://paperswithcode.com/rc2020\n11https://worksheets.codalab.org/\n12https://www.eleuther.ai/\n13https://bigscience.huggingface.co/\n14https://hai.stanford.edu/policy/national-research-cloud\n12 Center for Research on Foundation Models (CRFM)\nAnother complementary approach is to rely on volunteer computing, in which any of the\nbillions of computing devices (nodes) can connect to a central server and contribute computation.\nThe Folding@home project has successfully implemented this approach for simulating protein\ndynamics [Beberg et al . 2009]. Recently, the Learning@home project is attempting to harness\nvolunteer computing for training foundation models [Ryabinin and Gusev 2020]. The high latency\nconnections between nodes and the high bandwidth requirements for training foundation models\nmake this an open technical challenge.\nSummary. There are tremendous economic incentives to push the capabilities and scale of foun-\ndation models, so we anticipate steady technological progress over the coming years. But the\nsuitability of a technology relying largely on emergent behavior for widespread deployment to\npeople is unclear. What is clear that we need to be cautious, and that now is the time to establish\nthe professional norms that will enable the responsible research and deployment of foundation\nmodels. Academia and industry need to collaborate on this: industry ultimately makes concrete\ndecisions about how foundation models will be deployed, but we should also lean on academia,\nwith its disciplinary diversity and non-commercial incentives around knowledge production and\nsocial benefit, to provide distinctive guidance on the development and deployment of foundation\nmodels that is both technically and ethically grounded.\n1.4 Overview of this report\nIn March 2021, we created an informal community at Stanford University of students, faculty,\nand researchers interested in some aspect of foundation models.15 From the very beginning, the\ncommunity included not just AI researchers, but those eager to apply foundation models to their\ndomain (e.g., healthcare and law), as well as those who were interested in societal concerns\n(e.g., ethics and economics). As discussions progressed, we noticed that there were many gaps in\nmutual understanding — how the technology worked, how industry develops foundation models,\nhow to think about the ethical concerns, etc., and existing literature only covered bits and pieces.\nWe wanted to therefore provide a fuller picture of foundation models, identify opportunities and\nrisks, and establish a constructive vision for the future responsible development of foundation\nmodels.\nThe writing of this report was an experiment: we had over 100 people from different backgrounds\ncome together to write a single report covering a wide range of aspects of foundation models. A\nlarge part of this report is a survey of existing work, but through many discussions, we have unified\nit in one report to highlight all the interdisciplinary connections.\nStructure. The report is divided into 26 sections, each discussing one aspect of foundation models.\nThe sections are grouped into four parts: capabilities (§2: capabilities), applications (§3: ap-\nplications), technology (§4: technology), and society (§5: society), although there are many\nconnections across sections. These connections highlight an integrated approach in which the\ntechnologies and capabilities are developed in a way that is sensitive to real societal concerns, while\nbeing inspired by and grounded out in applications.\nWhile we have sought to capture most of the important topics surrounding foundation models,\nthis report will inevitably be incomplete, especially as the field evolves quickly. For example, many\napplications (e.g., natural sciences, music, finance, agriculture) are not included, though they are as\nlikely to be affected as the applications we have chosen to discuss. It would also be interesting to\n15This community led to the founding of the Center for Research on Foundation Models (CRFM) , a new interdisciplinary\ninitiative at the Stanford Institute for Human-Centered AI (HAI).\nOn the Opportunities and Risks of Foundation Models 13\n2. Capabilities\n5. Society\n4. Technology\nLanguage Vision Robotics Reasoning Interaction Philosophy\nInequity Misuse Environment Legality Economics Ethics\nModeling Training Adaptation Evaluation Systems Data\nSecurity Robustness\nAI Safety \n& Alignment Theory Interpretability\n2.1 2.2 2.3 2.4 2.5 2.6\n3. Applications\nHealthcare Law Education\n3.1 3.2 3.3\n4.1 4.2 4.3 4.4 4.5 4.6\n4.114.104.94.84.7\n5.1 5.2 5.3 5.4 5.5 5.6\nPaper Roadmap\nFig. 4. This report is divided into four parts: capabilities, applications, technology, and society, where each\npart contains a set of sections, and each section covers one aspect of foundation models.\nstudy how foundation models relate to research in neuroscience, cognitive science, and psychology\nto explain intelligence and aid efforts in computational social science to understand society.\n14 Center for Research on Foundation Models (CRFM)\nAuthor Contributions Percy Liang initiated and conceptualized the framing and structure of\nthe overall report. He and Rishi Bommasani worked together to lead the decentralized writing\neffort and provided guidance on individual sections. Drew A. Hudson created all the figures in\nthe report, discussing their structure and content with the authors of each section. Each of the 26\nsections of this report was written by a subset of authors, whose names are listed at the beginning\nof each section. There were, however, many discussions that spanned multiple sections, so the\nactual contributions to each section generally came from a broader set. Finally, we note that not all\nthe views expressed in this report are held by all the authors.\n1.4.1 Overview of capabilities.\nFoundation models acquire various capabilities that can power applications. We have chosen to\ndiscuss five potential capabilities: the ability to process different modalities (e.g., language, vision), to\naffect the physical world (robotics), to perform reasoning, and to interact with humans (interaction).\nFinally, we conclude with a philosophical discussion of potential limits on their capabilities.\n§2.1: Language. NLP as a field has blazed the trail for foundation models. While these models\ndominate standard benchmarks, there is a clear gap between the capabilities these models acquire\ncurrently and those that characterize language as a complex system for human communication and\nthought. In response to this, we emphasize the full range oflinguistic variation (e.g., different styles,\ndialects, languages), which poses an opportunity and challenge given some variants are data-limited.\nFurther, child language acquisition is more sample efficient than the training of foundation models;\nwe examine how signals beyond text and grounding may help to bridge this gap. Both of these\ncharacteristics of language provide clear directions for future foundation models research.\n§2.2: Vision. Computer vision led the adoption of deep learning in AI [Russakovsky et al. 2015],\ndemonstrating that models pretrained on large annotated datasets can transfer to numerous down-\nstream settings. Now, pretraining on web-scale raw data instead of curated datasets, foundation\nmodels are on the rise in computer vision [e.g., Radford et al . 2021]. These models have shown\npromising results for standard tasks in the field, like image classification and object detection,\nand training on multimodal and embodied data beyond images may enable progress on significant\nchallenges (e.g., 3D geometric and physical understanding, commonsense reasoning). We also\ndiscuss some of the key challenges in modeling (e.g., the ability to scale effectively to videos)\nand evaluation (e.g., the measurement of higher-order capabilities) along with the applications\n(e.g., ambient intelligence for healthcare) and societal considerations (e.g., surveillance) that will\ndetermine the impact of foundation models for computer vision going forward.\n§2.3: Robotics. A longstanding goal of robotics research is to develop “generalist” robots capable\nof performing myriad tasks across physically diverse environments. Unlike language and vision,\nwhich have led the way with foundation models both due to the abundance of raw data to train\nthese models on and the availability of virtual applications to apply these models to, robotics faces\nfundamental challenges due to being anchored to the physical world. The principal challenge in\ndeveloping new types of foundation models for robotics — different in nature than their language\nand vision counterparts — is acquiring sufficient data of the right form that is conducive to learning:\nwe explore how plentiful data (e.g., generic videos of humans, amongst others) that is not specific\nto particular environments and across modalities (e.g., language, vision) may help to bridge this\ngap. These new robotic foundation models could allow for easier task specification and learning ,\nushering in new applications (e.g., better robotic assistance for household tasks) and heightening\nthe importance of robustness and safety (e.g., formal safety evaluation).\nOn the Opportunities and Risks of Foundation Models 15\n§2.4: Reasoning and search. Reasoning and search problems such as theorem proving and pro-\ngram synthesis have been long-standing challenges in AI. The combinatorial search space renders\ntraditional search-based methods intractable. However, humans are known to operate intuitively\neven in the most mathematical of domains [Lakoff and Núñez 2000], and indeed existing work\nsuch as AlphaGo have already shown that deep neural networks can be effective in guiding the\nsearch space. But humans also transfer knowledge across tasks, facilitating much more efficient\nadaptation and the ability to reason more abstractly. Foundation models offer the possibility of\nclosing this gap: their multi-purpose nature along with their strong generative and multimodal\ncapabilities offer new leverage for controlling the combinatorial explosion inherent to search.\n§2.5: Interaction. Foundation models show clear potential to transform the developer and user\nexperience for AI systems: foundation models lower the difficulty threshold for prototyping and\nbuilding AI applications due to their sample efficiency in adaptation, and raise the ceiling for novel\nuser interaction due to their multimodal and generative capabilities. This provides a synergy we\nencourage going forward: developers can provide applications that better fit the user’s needs and\nvalues, while introducing far more dynamic forms of interaction and opportunities for feedback.\n§2.6: Philosophy of understanding. What could a foundation model come to understand about\nthe data it is trained on? Focusing on the case of natural language, we identify different positions\non the nature of understanding and explore their relevance for our central question. Our tentative\nconclusion is that skepticism about the capacity of future foundation models to understand natural\nlanguage may be premature, especially where the models are trained on multi-modal data.\n1.4.2 Overview of applications.\nAt present, foundation model research is largely confined to computer science and AI, with the\nimpact of foundation models and the applications they support largely being centered in the tech\nindustry. Moving forward, foundation models present clear potential to transform and extend the\nreach of AI across many sectors beyond the tech industry, suggesting a more pervasive effect on\npeople’s lives. While there is a multitude of applications and domains to consider, we we have chosen\nthree applications — healthcare, law, and education — because they represent foundational pillars of\nour society. For foundation models to significantly contribute to these application domains, models\nwill require specific capabilities (§2:capabilities) as well as technical innovation (§4:technology)\nto account for the unique considerations in each domain. Further, since these domains are critical\nto societal function (§5: society), applying foundation models in these domains requires engaging\nwith deeply sociotechnical matters such as those those pertaining to data (§4.6: data), privacy\n(§4.7: security), interpretability (§4.11: interpretability), fairness (§5.1: fairness) and ethics\n(§5.6: ethics).\n§3.1: Healthcare and biomedicine. Healthcare tasks (e.g., patient care via disease treatment)\nand biomedical research (e.g., scientific discovery of new therapies) require expert knowledge that\nis limited and expensive. Foundation models present clear opportunities in these domains due to\nthe abundance of data across many modalities (e.g., images, text, molecules) to train foundation\nmodels, as well as the value of improved sample efficiency in adaptation due to the cost of ex-\npert time and knowledge. Further, foundation models may allow for improved interface design\n(§2.5: interaction) for both healthcare providers and patients to interact with AI systems, and\ntheir generative capabilities suggest potential for open-ended research problems like drug discovery.\nSimultaneously, they come with clear risks (e.g., exacerbating historical biases in medical datasets\nand trials). To responsibly unlock this potential requires engaging deeply with the sociotechnical\n16 Center for Research on Foundation Models (CRFM)\nmatters of data sources and privacy as well as model interpretability and explainability, alongside\neffective regulation of the use of foundation models for both healthcare and biomedicine.\n§3.2: Law. Legal applications require that attorneys read and produce long coherent narratives\nthat incorporate shifting contexts and decipher ambiguous legal standards. Foundation models may\nprovide benefits in this domain:ample data exists in the form of legal documents and their generative\ncapabilities are well-suited to themany generative tasks required in law , but significant improvements\nare required for foundation models to be able to reliablyreason over various sources of information to\ngenerate truthful long-form documents. As is the care in healthcare (§3.1:healthcare), the sample\nefficiency of adaptation for foundation models is of heightened value given the costs of expert time\nand knowledge in the legal domain, which may allow for the re-allocation of expertise towards\npressing problems of justice and government service. The responsible development of foundation\nmodels for law will require specific consideration of privacy, and highlights core limitations of\nexisting foundation models that will require fundamental advances with respect to provenance for\ntheir behavior and guarantees for the factuality of their generation.\n§3.3: Education. Education is a complex and subtle domain; effective teaching involves reasoning\nabout student cognition and should reflect the learning goals of students. The nature of foundation\nmodels presents promise here that has yet to be realized in the sphere of AI for education: while\ncertain many streams of data in education are individually too limited to train foundation models,\nthe ability to leverage relevant data from outside the domain (e.g., the Internet) and make use\nof data across multiple modalities (e.g., textbooks, mathematical formula, diagrams, video-based\ntutorials) jointly offers hope for foundation models that are broadly applicable to educational tasks.\nIf foundation models lead to a significant improvement in education-relevant capabilities, there\nis clear potential for new applications that align with the open-ended generative (e.g., problem\ngeneration) and interactive (e.g., feedback to teachers) aspects of foundation models; the sample\nefficient adaptation of foundation models suggests greater ability for adaptive and personalized\nlearning. In this event, renewed consideration is required of hallmarks of applying technology to\neducation (e.g., student privacy), along with certain concerns becoming more critical (e.g., inequity\nin access to technology in education, technology-aided plagiarism).\n1.4.3 Overview of technology.\nNow we discuss the technology behind building better model architectures, training and adaptation\nprocedures, and of course scaling up the systems. One crucial but often overlooked topic is data —\nwhere does it come from and what is its composition? In addition, we want foundation models to\nbe robust to distribution shifts and secure against attackers. Finally, we wish to understand why\nfoundation models work from both a mathematical perspective as well as an empirical perspective.\n§4.1: Modeling. What structural properties give rise to a foundation model? In the modeling\nsection, we explore the underlying architectures behind foundation models and identify 5 key\nattributes. First, we start by discussing expressivity of the computational model — to capture and\nassimilate real-world information, and scalability — to adeptly handle large quantities of high-\ndimensional data. These properties are successfully realized by existing architectures such as the\ntransformer network [Vaswani et al. 2017] that underpins most foundation models to date. We then\nproceed to attributes may be essential for the next generation of models, including:multimodallity —\nto consume, process and potentially produce content from different sources and domains, memory\ncapacity — to effectively store and retrieve the acquired knowledge, and finally, compositionality,\nto foster successful generalization to novel settings and environments. We believe that realizing the\nOn the Opportunities and Risks of Foundation Models 17\nfull potential envisioned for foundation models will hinge on modelling advances to fulfill these\ndesiderata.\n§4.2: Training. Training objectives mathematically specify how models should learn and acquire\ncapabilities from their training data. The current status quo for training foundation models involves\nmodality-specific objectives (e.g., masked language modeling [Devlin et al . 2019] for text and\nSimCLR [Chen et al . 2020c] for images) that are often chosen heuristically. We envision that\nfuture training objectives for foundation models will reflect two changes: principled selection\nderived from systematic evidence and evaluation (§4.4: evaluation), and domain-generality to\nprovide rich, scalable, and unified training signal across data sources and modalities. We also\ndiscuss important design trade-offs, including generative vs discriminative training, the choice\nof input data representation, and the potential of future training objectives that involve explicit\nrepresentations of goals.\n§4.3: Adaptation. Foundation models are intermediary assets; they are unfinished and generally\nshould not be used directly, instead requiring adaptation for specific downstream tasks. The de\nfacto approach for adaptation has been fine-tuning, with recent work suggesting that lightweight\nfine-tuning alternatives and prompting-based methods may achieve favorable accuracy-efficiency\ntradeoffs. Moving forward, we envision a more expansive view of adaptation that goes beyond\njust specializing foundation models to perform the task of interest: adaptation will alleviate defi-\nciencies of stand-alone foundation models (e.g., temporal adaptation to reflect changes over time\nin the world) or introduce constraints (e.g., GDPR compliance relating to the right to be forgotten ;\n§4.7: security); this broader perspective on adaptation coincides with a need for new evaluation\nprotocols (§4.4: evaluation) that systematically evaluate adaptation methods while controlling for\nresources (e.g., runtime, memory) and access requirements involved in adaptation.\n§4.4: Evaluation. Evaluation offers context to foundation models by providing a means to track\nprogress, understand models, and document their capabilities and biases. Foundation models\nchallenge the ability of standard evaluation paradigms in machine learning to achieve these goals\nsince they are one step removed from specific tasks. To envision new paradigms in evaluation\nthat suit foundation models, we discuss (a) evaluating foundation models directly to measure their\ninherent capabilities and inform how foundation models are trained, (b) evaluating task-specific\nmodels by controlling for adaptation resources and access , and (c) broader evaluation design to\nprovide richer context beyond measures of accuracy (e.g., robustness (§4.8: robustness), fairness\n(§5.1: fairness), efficiency (§4.5: systems), environmental impact (§5.3: environment)). Reform of\nevaluation practices will allow for evaluation that adequately serves both the diverse goals and\nstakeholders involved in the foundation model paradigm.\n§4.5: Systems. While the training data (§4.6: data) determines the theoretical information avail-\nable for foundation models, and model architectures (§4.1: modeling) and training objectives\n(§4.2: training) determine how much of this information can be extracted, computer systems\ndetermine what is practically achievable. Systems are a key bottleneck for scaling in terms of data\nand model size, both of which appear to reliably track with improvements in capabilities. To ensure\nthat we can train the next generation of foundation models efficiently (with respect to time and\ncost), we will require the co-design of algorithms, models, software, and hardware. This co-design\nis already starting to happen to in various forms, from carefully tuned parallelism strategies to new\narchitectures such as retrieval-based and mixture-of-expert models. Beyond training, we consider\nwhat will be required to deploy applications on top of foundation models (e.g., efficient inference).\n18 Center for Research on Foundation Models (CRFM)\n§4.6: Data. Data is the lifeblood of foundation models; the training data of these models largely\ndetermines what these capabilities these models can acquire. The centrality of data is not unique\nto foundation models; recent calls for data-centric AI [Press 2021; Ré 2021] indicate the pervasive\nimportance of managing, understanding, and documenting data used to train machine learning\nmodels. For foundation models specifically, the current modus operandi is for training data to be\nselected using unspecified or unclear principles with a general lack of transparency regarding the\nnature of training data. We believe an alternative approach is needed to re-imagine the data ecosys-\ntem surrounding foundation models: we draw upon work on data visualization and management\nto propose a data hub for foundation models. We articulate how this proposal relates to many of\nthe relevant data-centric considerations for foundation models: selection, curation, documentation,\naccess, visualization and inspection, quality assessment, and legal regulation.\n§4.7: Security and privacy. Security and privacy for foundation models is largely uncharted at\npresent. Fundamentally, foundation models are a high-leverage single point of failure , making\nthem a prime target for attack: existing work demonstrates a variety of security vulnerabilities\n(e.g., adversarial triggers to generate undesirable outputs) or privacy risks (e.g., memorization of\ntraining data) for these models. Further, the generality of foundation models compounds these\nconcerns, intensifying the risk for function creep or dual use (i.e., use for unintended purposes). For\nsecurity, we view foundation models as akin to operating systems in traditional software systems;\nwe discuss steps towards secure foundation models which, if achieved, would provide a strong\nabstraction layer to build upon for reliable ML applications. For privacy, by leveraging knowledge\ntransfer from public data, foundation models may enable more sample efficient adaptation to\nsensitive data distributions, i.e., privacy-preserving applications may incur less degradation in\naccuracy when built using foundation models.\n§4.8: Robustness to distribution shifts. A major limitation of standard machine learning is that\nit produces models that are not robust to distribution shifts, where the training distribution does\nnot match the test distribution (for the downstream task). Existing work shows that adapting a\nfoundation model trained on a broad range of unlabeled data improves the robustness of adapted\nmodels across a wide variety of shifts. This opens a new set of promising directions for improving\ntraining and adaptation of foundation models for robustness. However, we do not believe that\nfoundation models are a panacea for robustness — challenges such as extrapolation across time\nand spurious correlations are not likely to be fully addressed.\n§4.9: AI safety and alignment. Ensuring foundation models are reliable (§4.5: systems), robust\n(§4.8: robustness), and interpretable (§4.11: interpretability) is increasingly important when\nconsidering the potential real-world applications of these models. In addition to critical and imme-\ndiate considerations, we also consider the relationship between foundation models and larger-scale\nrisks, hazards, and harms that have the potential for increased relevance as model capabilities\ncontinue to advance. For example, we consider the importance of aligning foundation models\nsuch that they are not deployed with misspecified goals or values . We also discuss the relevance\nof forecasting the emergent behaviors of foundation models (e.g., the ability to deceive or plan\nstrategically), which may complicate attempts to adapt them to particular tasks, and may require\nnew approaches for interpretability (§4.11: interpretability) or evaluation (§4.4: evaluation).\n§4.10: Theory. Learning theory provides a broad foundation for the variety of contexts encountered\nin applied machine learning; theory offers both understanding, principles, and guarantees to\ncomplement empirical findings. At present, the study of foundation models is largely empirical:\nthe theory of standard supervised learning, while relatively mature, is inadequate to fully explain\nfoundation models. Specifically, the discrepancy between the training phase and the adaptation\nOn the Opportunities and Risks of Foundation Models 19\nphase within the foundation model regime pinpoints the insufficiency of existing theory, since these\nphases correspond to (potentially) completely different tasks and data distributions. Nevertheless,\nwe endeavor that advances in theory to address this discrepancy, even in simple, limited settings,\nwill provide useful insights.\n§4.11: Interpretability. Interpretability provides clarity to foundation models: the opacity of\nthe deep neural networks that underpin foundation models, alongside the expected ubiquity of\nfoundation models, heightens the need to understand these models and their capabilities. Inter-\npretability methods at present generally are designed for interpreting and explaining the behavior\nof task-specific models; the nature of foundation models (i.e., the wide array of tasks these models\nare beneficial for and the unexpected emergent properties they acquire) introduces new challenges\nfor interpretability research. To frame the discussion of interpretability for foundation models, we\npropose the one model-many models paradigm, which aims to determine the extent to which theone\nmodel (the foundation model) and its many models (its adapted derivatives) share decision-making\nbuilding blocks. In addition to interpreting the decision-making components involved, we further\ndiscuss explainability in the context of foundation models (e.g., the validity ofpost hoc explanations\ngenerated by models) as well as the mechanisms that drive model behavior (which may clarify\nthe extent to which understanding foundation models can extend to understanding their adapted\nderivatives). Given the critical role we ascribe interpretability in the study of foundation models,\nwe conclude with an assessment of the societal impact of interpretability and non-interpretability.\n1.4.4 Overview of society.\nWe believe the rapid development of foundation models, adapted and deployed to various applica-\ntions, will have wide-ranging consequences on the health of societies. What makes these models so\nexciting and also so troubling is their task agnosticity. Societal impact is easier (but still non-trivial)\nto understand and reason about when we talk about specific systems deployed to users, but how\ncan we take into account the societal impact of all possible systems and use cases when developing\nfoundation models?\n§5.1: Inequity and fairness. In many contexts, machine learning has been shown to contribute\nto, and potentially amplify, societal inequity. Foundation models may extend this trend, i.e., fur-\nthering the unjust treatment of people who have been historically discriminated against. However,\nunderstanding the relationship between inequity and foundation models requires reckoning with\nthe abstraction of foundation models; foundation models are intermediary assets that are adapted\nfor applications that impact users. Therefore, we delineate intrinsic biases, i.e., properties in founda-\ntion models that portend harm, and extrinsic harms, i.e., harms arising in the context of specific\napplications built using foundation models. We taxonomize various sources (e.g., training data, lack\nof diversity among foundation model developers, the broader sociotechnical context) that give rise\nto these biases and harms, emphasizing the importance, and technical difficulty, of source tracing to\nunderstand ethical and legal responsibility. We do not view unfairness as inevitable in the founda-\ntion model paradigm: to address unfair outcomes that arise from foundation models, we dually\nconsider proactive interventions (e.g., technical methods like counterfactual data augmentation)\nand reactive recourse (e.g., mechanisms for feedback propagation and attribution of moral/legal\nresponsibility).\n§5.2: Misuse. We define foundation model misuse as the use of foundation models as they are\ntechnically intended (e.g., to generate language or video), but with the goal of causing societal harm\n(e.g., to generate disinformation, to develop deepfakes for harassment). We argue that advances in\nfoundation models will result in higher-quality machine-generated content that will be easier to\n20 Center for Research on Foundation Models (CRFM)\ncreate and personalize for misuse purposes. For example, disinformation actors may use them to\nquickly generate collections of articles targeted across different demographic groups (e.g., national-\nity, political party, religion, etc.). While these new capabilities may limit existing human detection\nmethods for harmful content (e.g., tracking similar text across different sources), foundation models\nmay themselves provide promising potential as automated misuse detectors.\n§5.3: Environment. Foundation models are the byproducts of computationally expensive training\nregimes, with the existing trajectory favoring even more intensive models; the energy required for\nthis training coincides with the release of more carbon into the atmosphere and the degradation of\nthe environment. At present, current discussion centers these enormous single-time training costs\nand the potential to amortize these costs across repeated use. We seek to clarify these discussions\nby identifying assumptions that shape the calculus of environmental impact for foundation models.\nFurther, we envision that the ecosystem surrounding foundation models requires a multi-faceted\napproach: (a) more compute-efficient models, hardware, and energy grids all may mitigate the\ncarbon burden of these models, (b) environmental cost should be a clear factor that informs how\nfoundation models are evaluated (§4.4: evaluation), such that foundation models can be more\ncomprehensively juxtaposed with more environment-friendly baselines, and (c) the cost-benefit\nanalysis surrounding environmental impact necessitates greater documentation and measurement\nacross the community.\n§5.4: Legality. Foundation models rest on tenuous legal footings at present; how the law bears on\nboth the development and use of these models is largely unclear. Legal and regulatory frameworks\nfor foundation models specifically, alongside those for AI technology more generally, will be\nneeded to influence, constrain, and even foster practices in research, development, and deployment.\nCentering on the legal landscape of the United States, where existing consideration of algorithmic\ntools remains broadly uncertain, we highlight the pertinent issues of liability for model predictions\nand protections from model behavior. With respect to both issues, we describe how legal standards\nwill need to be advanced to address these given the intermediary status of foundation models (as\nopposed to that of user-facing task-specific models).\n§5.5: Economics. Foundation models are likely to have substantial economic impact due to their\nnovel capabilities and potential applications in a wide variety of industries and occupations. We\nconsider the implications of the development and use of foundation models for the future of the US\nand global economy with a focus on productivity, wage inequality, and concentration of ownership.\n§5.6: Ethics of scale. In addition to running the risk of increasing inequity, as discussed in\n§5.1: fairness, the widespread adoption of foundation models poses other ethical, political and\nsocial concerns. We discuss ethical issues related to the scale of application of foundation models,\nsuch as homogenization and the concentration of power, as well as the norms and release strategies\nappropriate to address them.\nOn the Opportunities and Risks of Foundation Models 21\n2 CAPABILITIES\nFoundation models acquire capabilities, some that surprisingly emerge from their learning pro-\ncess, that power downstream applications (§3: applications). Specifically, we discuss linguistic\n(§2.1: language) and visual (§2.2: vision) capabilities alongside the ability to affect the physical\nworld (§2.3: robotics), perform reasoning and search (§2.4: reasoning), and interact with hu-\nmans (§2.5: interaction). In addition, we discuss how self-supervision (the technical approach\nused to learn most current foundation models) philosophically relates to the ability to understand\n(§2.6: philosophy).\n22 Center for Research on Foundation Models (CRFM)\n2.1 Language\nAuthors: Isabel Papadimitriou, Christopher D. Manning\n2.1.1 The nature of human language.\nLanguage is the basis of most human communication and interaction. However, it is not just a\nmeans for humans to achieve shared goals: language is central to human thought, to how social\nand emotional relations are formed, to how we identify ourselves socially and personally, and\nto how humans record knowledge and develop societal intelligence. Spoken or signed languages\narise in every human society, and the languages of the world are both incredibly diverse in the\nways that they express and structure the information they convey, while also exhibiting surprising\nconcordance in the richness of what makes a language [Comrie 1989]. Languages are remarkably\ncomplex yet efficient systems, acquired consistently by children in a short amount of time, and\nwhich evolve and encompass the changing needs and conditions of linguistic communities. Due to\nthis centrality of language in human activities, language understanding and generation is a critical\nelement of research in artificial intelligence. Natural language processing (NLP) is the subfield of\nartificial intelligence concerned with language and, together with the related fields of automatic\nspeech recognition (ASR) and text-to-speech (TTS), has the goal of giving computers the ability to\nunderstand and generate human language in much the same way human beings can.\nTo date in 2021, NLP has been the field most profoundly affected by foundation models. The first\ngeneration of foundation models showcased an impressive variety of linguistic abilities, as well as\na surprising amount of adaptability to a large range of linguistic situations. Since the introduction\nof the early foundation models ELMo [Peters et al. 2018] and BERT [Devlin et al. 2019] in 2018, the\nfield of NLP has become largely centered around using and understanding foundation models. The\nfield has shifted to using foundation models as the primary tool, moving towards more generalized\nlanguage learning as a central approach and goal. In this section, we go over the recent successes\nof foundation models in NLP, detail how foundation models have changed the overall process and\nmentality for training machine learning models for language, and discuss some of the theoretical\nand practical challenges facing foundation models as they are applied to a broader set of languages\nand more realistic and complex linguistic situations.\n2.1.2 Impact of foundation models on NLP.\nFoundation models have had a huge impact on the field of NLP, and are now central to most NLP\nsystems and research. On a first level, many foundation models are skilled language generators: for\nexample, Clark et al. [2021] demonstrate that non-experts have difficulty distinguishing short-form\nEnglish text that was written by GPT-3 from that written by humans. However, the feature of\nfoundation models that has been most impactful in NLP is not their raw generation abilities but\ntheir surprising generality and adaptability : a single foundation model can be adapted in different\nways in order to achieve many linguistic tasks.\nThe field of NLP has historically focused on defining and engineering systems for challenging\nlinguistic tasks, with the vision that models that are good at these tasks will lead to competent\nlanguage systems for downstream applications. NLP tasks include classification tasks for a whole\nsentence or document (e.g., sentiment classification, like predicting whether a movie review is\npositive or negative), sequence labeling tasks, in which we classify each word or phrase in a\nsentence or document (e.g., predicting if each word is a verb or a noun, or which spans of words\nrefer to a person or an organization), span relation classification , (e.g., relation extraction or parsing,\nlike whether a person and location are linked by a “current residence” relation, or a verb and a\nnoun by a “subject-verb” relation) and generation tasks , producing new text that is conditioned\nOn the Opportunities and Risks of Foundation Models 23\nFig. 5. Only a tiny percentage of the world’s languages are currently represented in foundation models.\nThere are over 6,000 languages in the world, with estimates varying due to the inherent uncertainty of what\nconstitutes a separate language [Nordhoff and Hammarström 2011]. This map shows the languages of the\nworld, with each dot representing one language and its color indicating the top-level language family. Data is\nfrom Glottolog [Hammarström et al. 2021]. We label a few of the languages on the map as examples.\nstrongly on an input (e.g., producing a translation or summary of a text, recognizing or producing\nspeech, or responding in a conversation) [Jurafsky and Martin 2009]. In the past, NLP tasks had\ndistinct research communities that developed task-specific architectures, often based on pipelines\nof different models, each performing a linguistic sub-task such as token segmentation, syntactic\nparsing, or coreference resolution.\nBy contrast, the dominant modern approach for performing each task is to use a single foundation\nmodel and adapt it slightly using relatively small amounts of annotated data specific to each task\n(sentiment classification, named entity tagging, translation, summarization) to create an adapted\nmodel. This has proved to be an extremely successful approach: for the vast majority of the tasks\ndescribed above, a foundation model that is slightly adapted for a task greatly outperforms previous\nmodels or pipelines of models that were built specifically to perform that one task. To take just one\nexample, the best system for answering open-ended science questions in 2018, before foundation\nmodels, could get 73.1% on the NY Regents 8th grade science exam. A year later in 2019, an adapted\nfoundation model scored 91.6% [Clark et al. 2019].\nThe emergence of foundation models that are largely trained togenerate language has constituted\nan important shift in the role of language generation in NLP. Until around 2018, the problem of\ngenerating general-purpose language was considered very difficult and essentially unapproachable\nexcept through other linguistic sub-tasks [Paris et al . 2013]. Instead, NLP research was mostly\nfocused on linguistically analyzing and understanding text. Now, it is possible to train highly\ncoherent foundation models with a simple language generation objective, like “predict the next\nword in this sentence”. These generative models now constitute the primary vehicle through which\nmachine learning for language is done — including the analysis and understanding tasks that were\nonce considered prerequisites for generation. The successful generation exhibited by foundation\n24 Center for Research on Foundation Models (CRFM)\nmodels has also led to a flowering of research for language generation tasks like summarization\nand dialogue generation. The rise of the foundation model paradigm has begun to play a similar\nrole in spoken language as well as written. Modern automatic speech recognition (ASR) models\nlike wav2vec 2.0 are trained on large datasets of speech audio alone, and then adapted on audio\nwith associated transcriptions for the task of ASR [Baevski et al. 2020].\nDue to the changes brought about by the foundation model paradigm, the focus of research and\npractice in NLP has shifted from making bespoke architectures for different tasks to exploring\nhow to best leverage foundation models. Research into adaptation methods has blossomed (see\n§4.3: adaptation for a detailed look at adaptation), and the surprising successes of foundation\nmodels have also caused a shift in research interest towards analyzing and understanding foundation\nmodels (see §4.11: interpretability for interpretability and analysis of foundation models).\n2.1.3 Language variation and multilinguality.\nThough foundation models are surprisingly versatile with the linguistic knowledge they obtain from\npretraining, there are limits to this adaptability: it is not clear how successful current foundation\nmodels are at handling language variation. Language varies greatly. Apart from the fact that there\nare thousands of different languages in the world, language varies even within one language or\nwithin one speaker. To point out a few examples, informal conversation manifests differently from\nwritten language, the grammatical constructions that people reach for when speaking to friends\nare very different from those used when speaking to someone with authority, and communities of\nspeakers within a language use different dialects. Social and political factors are embedded in how\nlanguage variation is viewed and valued, and in how much different varieties are represented in NLP\nresearch (see for example Blodgett and O’Connor [2017] on the failures of NLP for African American\nEnglish, and §5.1: fairness for a deeper discussion on inequities in foundation models). Due to their\nlarge capacity for learning linguistic information and flexibly adapting that knowledge, foundation\nmodels hold promise for expanding NLP to encompass more linguistic diversity. It remains an open\nresearch question to understand whether it is possible to make foundation models that robustly and\nequitably represent language with both its major and subtle variations, giving equal weight and\nacuity to what makes each linguistic variety distinct [research posing and addressing this question\nincludes Ponti et al. 2019; Bender 2011; Joshi et al. 2020].\nFollowing the success of foundation models for English, multilingual foundation models have\nbeen released to extend that success to non-English languages. For most of the over 6,000 languages\nin the world, the text data available is not enough to train a large-scale foundation model. To give\none example, there are over 65 million speakers of Fula, a West African language, but few if any\nresources available for NLP in Fula [Nguer et al. 2020]. Multilingual foundation models address this\nby jointly training on multiple languages simultaneously. The multilingual foundation models to\ndate (mBERT, mT5, XLM-R) are each trained on around 100 languages [Devlin et al. 2019; Goyal et al.\n2021; Xue et al. 2020]. Joint multilingual training relies on the reasonable assumption that the shared\nstructures and patterns between languages can lead to sharing and transfer from the high-resource\nlanguages to the low-resource ones, making foundation models possible for languages where we\ncould not train a stand-alone model. Experiments using and analyzing multilingual foundation\nmodels have shown that there is indeed a surprising amount of transfer between and parallel\nencoding of the different languages in multilingual foundation models [Wu and Dredze 2019;\nChoenni and Shutova 2020; Pires et al. 2019; Libovick`y et al. 2019; Chi et al. 2020; Papadimitriou\net al. 2021; Cao et al. 2019].\nHowever, the extent to which these models are robustly multilingual is still an open question. It\nremains unclear how much models trained on this data can represent aspects of languages that are\ndrastically different from English or for which few language resources are available [Wu and Dredze\nOn the Opportunities and Risks of Foundation Models 25\n2020], and whether their apparent multilingual performance relies more on assimilation [Lauscher\net al. 2020; Virtanen et al. 2019; Artetxe et al. 2020]. Multilingual models show better performance in\nlanguages that are similar to the highest-resource languages in their training data, and it has been\nshown that languages in multilingual models compete for model parameters, making it unclear\nhow much variation can fit in a single model [Wang et al. 2020d]. A salient issue stems from the\ndata that we use to train multilingual foundation models: in many multilingual corpora, English\ndata is not only orders of magnitude more abundant than that of lower-resource languages, but it\nis often cleaner, broader, and contains examples showcasing more linguistic depth and complexity\n[Caswell et al. 2021] (see Nekoto et al. [2020] on building participatory and robust multilingual\ndatasets). However, the answer does not simply lie in creating more balanced corpora: there are so\nmany axes of language variation that it would be infeasible to create a corpus that is balanced and\nrepresentative in all regards. The future, versatility, and equity of foundation models all depend on\nrobustly handling language variation despite unbalanced data [e.g., Oren et al. 2019].\nCurrent multilingual foundation models in their raw form, and naive unsupervised multilingual\ntraining as a method, may not model the subtleties of languages and language varieties to their full\nextent. Nevertheless, they remain useful for some multilingual applications, for example through\nadapting multilingual models for low-resource languages not in their original training set [Wang\net al. 2020b]. Moreover, the results for the (non-public) GShard neural machine translation model\nshow the largest gains over monolingual baselines for the lowest resource languages, with the\ngains increasing with model size [Lepikhin et al. 2021]. The research community should critically\nexamine how foundation models deal with language variation, understand the limits of foundation\nmodels in bringing equity and representation to NLP, and not settle on promoting foundation\nmodels that erase language variation and mostly conform to the linguistic majority in their training\ndata.\n2.1.4 Inspiration from human language acquisition.\nThough foundation models have constituted a huge source of progress in creating NLP systems\nthat act more like humans, there are still significant ways in which the linguistic system that they\nacquire, as well as the learning process, differ from human language. Understanding the implications\nof this gap between machine and human language learning is a necessary part of developing a\nresearch community informed about the linguistic limits and possibilities of foundation models.\nHuman language acquisition is very efficient: foundation models like GPT-3 are trained on around\nthree to four orders of magnitude more language data than most humans will ever hear or read, and\ncertainly much more than children have been exposed to by the time they are mostly linguistically\ncompetent. One salient difference between foundation models and human language acquisition is\nthat human language is grounded to the real world [Saxton 2017]. For example babies and caretakers\npoint to objects during language development [Colonnesi et al. 2010], and babies learn the grounded\nmeanings of words that refer to common objects before they learn a lot of the other aspects of\nthe linguistic system [Bergelson and Swingley 2012]. Most foundation models used in NLP, on the\nother hand, learn from the distributional information of raw, ungrounded text, and (in contrast\nto human learners) Zhang et al . [2021] show that RoBERTa models express abstract syntactic\nfeatures before usable meaning. Powerful ungrounded statistical learning is indeed also present\nin babies [Saffran et al. 1996], so it is no doubt an important factor in acquisition. Nevertheless,\nadvancing grounded language learning for foundation models remains an important direction for\napproaching human acquisition efficiency [Dupoux 2018; Tan and Bansal 2020; Zellers et al. 2021a,\ninter alia ] (see §2.2: vision and §2.3: robotics for the multimodal potential of foundation models,\nand §2.6: philosophy for a discussion of whether foundation models can understand language\nwithout grounding). Another important direction is examining the inductive biases in foundation\n26 Center for Research on Foundation Models (CRFM)\nFig. 6. Language Acquisition for humans and foundation models. While there are certainly different inductive\nbiases between the human brain and foundation models, the ways that they learn language are also very\ndifferent. Most saliently, humans interact with a physical and social world in which they have varied needs\nand desires, while foundation models mostly observe and model data produced by others.\nmodels and how they relate to the inductive biases in the human mind, both those specific to\nlanguage learning and those general to human cognition [Linzen and Baroni 2021]. Though the\nhuman brain may be more architecturally specialized for efficient language acquisition, foundation\nmodels are not blank-slate learners [Baroni 2021], and understanding and aligning these linguistic\ninductive biases is an important future direction for research in foundation models.\nA significant factor in the efficiency of language acquisition is the fact that humans acquire a\nsystematic and generalizable language system. Though there are many differing theories about what\ntypes of theoretical abstractions the human language system makes [e.g., Comrie 1989; Chomsky\n2014; Croft 2001; Jackendoff 2011], it is generally agreed that humans learn language in a way that\nallows them to easily slot new knowledge into existing abstractions and productively create new\ngrammatical sentences. For example, a ten-year-old child has acquired a lot of the abstractions\nabout how their language works, though the actual words and constructions that they produce\nwill change drastically over the next ten years. Foundation models, on the other hand, often do not\nacquire the systematic abstractions that we expect from humans. For example, when a foundation\nmodel produces a linguistic construction accurately one time there is no guarantee that future\nuses of that construction will be mostly consistent, especially after a significant domain shift in\nthe subject matter [examples of work examining limitations of foundation models in systematicity\ninclude Lake and Baroni 2018; Kim and Linzen 2020; Bahdanau et al. 2018; Chaabouni et al. 2021].\nNLP faces the challenge of developing some sort of systematicity in acquisition for foundation\nmodels, without regressing to systems that rely too heavily on rigid linguistic rules.\nLanguage learning continues for a speaker’s whole lifetime: the grammar of human languages\nevolves, and humans flexibly adapt to novel linguistic situations [Sankoff 2018]. For example, as\nnew terms and concepts arise in an adult’s life, they can use them relatively easily in grammatical\nOn the Opportunities and Risks of Foundation Models 27\nsentences, and humans often adapt their grammatical patterns to fit in with different social groups\n[Rickford et al. 1994]. On the other hand, the linguistic system of foundation models is mostly set\nby the training data, and is relatively static [Lazaridou et al. 2021; Khandelwal et al. 2020]. Though\nadaptation methods can prime foundation models for different tasks (see §4.3: adaptation), it still\nremains unclear how to change the more basic linguistic foundation of a foundation model without\na large amount of training. Making adaptable models that naturally mirror human-like linguistic\naccommodation and language evolution is an important research area for the future of foundation\nmodels.\nFoundation models have drastically changed the research and practice of NLP. Foundation models\nhave given rise to many new research directions for the community: understanding generation as\na fundamental aspect of language, studying how to best use and understand foundation models,\nunderstanding the ways in which foundation models may increase inequities in NLP, examining\nwhether foundation models can satisfactorily encompass linguistic variation and diversity, and\nfinding ways to draw on human language learning dynamics. Most of the complex NLP tasks\nthat the research community focused on before foundation models are now best handled, to an\nalmost-human level, using one of a few publicly released foundation models. Nevertheless, there\nremain significant gaps between this performance and the needs for useful and safe deployment of\nfoundation models in complex downstream settings.\n28 Center for Research on Foundation Models (CRFM)\n2.2 Vision\nAuthors: Shyamal Buch, Drew A. Hudson, Frieda Rong, Alex Tamkin, Xikun Zhang, Bohan Wu, Ehsan\nAdeli, Stefano Ermon, Ranjay Krishna, Juan Carlos Niebles, Jiajun Wu, Li Fei-Fei\nFig. 7. By harnessing self-supervision at scale, foundation models for vision have the potential to distill raw,\nmultimodal sensory information into visual knowledge, which may effectively support traditional perception\ntasks and possibly enable new progress on challenging higher-order skills like temporal and commonsense\nreasoning (§2.2.1: vision-capabilities). These inputs can come from a diverse range of data sources and\napplication domains, suggesting promise for applications in healthcare and embodied, interactive perception\nsettings (§2.2.2: vision-challenges). Image credits [Zamir et al. 2018; Haque et al. 2020].\nVision underlies one of the primary modes through which a living organism understands its\nenvironment. The ability to see enables the near-constant, long-range gathering of dense signals, a\ncritical capability developed over an evolutionary time-scale in a diverse range of life forms [Parker\n2003; Zhang and Shu 2021]. For a skill executed effortlessly by even simple living creatures,\ntransferring the same abilities to machines has proved remarkably challenging, leading computer\nvision and robotics researcher Hans Moravec in 1988 to observe a paradox: in AI, (what were\nconsidered) hard problems are easy and likewise easy problems are hard, and among the “easiest”\nproblems of them all is the visual acuity which we use each day to continually interpret complex\nscenes in a matter of milliseconds [Moravec 1988; Thorpe et al. 1996; Fei-Fei et al. 2007].\nOn the other end of this formidable challenge is the substantial scope of transformative appli-\ncations which computer vision holds the key to: self-driving cars that can free commuters from\ngridlock (§2.3: robotics), life-saving AI tools that can assist overworked specialists by detecting\nrare medical events (§3.1: healthcare), next-generation tools for multimedia creation and editing\n(§2.5: interaction), among others. Reflecting on the applications and settings where human\nperception is instrumental offers a sense of the potential areas where computer vision can assist\nand transform.\nOn the Opportunities and Risks of Foundation Models 29\nThe field of computer vision and the challenges we define draw inspiration in many ways from\nhuman perception capabilities. Several classical theories [e.g., Biederman 1972; McClelland and\nRumelhart 1981; Marr 1982] suggested that humans may perceive real world scenes by contextual-\nizing parts as a larger whole, and pointed the way for computer vision techniques to progressively\nmodel the physical world with growing levels of abstractions [Lowe 1992; Girshick et al . 2014].\nGibson [1979] suggested that human vision is inherently embodied and interactive ecological\nenvironments may play a key role in its development. These ideas continue to motivate the ongoing\ndevelopment of computer vision systems, iterating towards a contextual, interactive, and embodied\nperception of the world.\nIn the context of computer vision, foundation models translate raw perceptual information\nfrom diverse sources and sensors into visual knowledge that may be adapted to a multitude of\ndownstream settings (Figure 7). To a large extent, this effort is a natural evolution of the key ideas\nthat have emerged from the field over the last decade. The introduction of ImageNet [Deng et al.\n2009] and the advent of supervised pretraining led to a deep learning paradigm shift in computer\nvision. This transition marked a new era, where we moved beyond the classic approaches and\ntask-specific feature engineering of earlier days [Lowe 2004; Bay et al. 2006; Rosten and Drummond\n2006] towards models that could be trained once over large amounts of data, and then adapted\nfor a broad variety of tasks, such as image recognition, object detection, and image segmentation\n[Krizhevsky et al. 2012; Szegedy et al. 2015; He et al. 2016a; Simonyan and Zisserman 2015]. This\nidea remains at the core of foundation models.\nThe bridge to foundation models comes from the limitations of the previous paradigm. Traditional\nsupervised techniques rely on expensive and carefully-collected labels and annotations, limiting\ntheir robustness, generalization and applicability; in contrast, recent advances in self-supervised\nlearning [Chen et al . 2020c; He et al . 2020] suggest an alternative route for the development\nof foundation models that could make use of large quantities of raw data to attain a contextual\nunderstanding of the visual world. Relative to the broader aims of the field, the current capabilities of\nvision foundation models are currently early-stage (§2.2.1: vision-capabilities): we have observed\nimprovements in traditional computer vision tasks (particularly with respect to generalization\ncapability) [Radford et al. 2021; Ramesh et al. 2021] and anticipate that the near-term progress\nwill continue this trend. However, in the longer-term, the potential for foundation models to\nreduce dependence on explicit annotations may lead to progress on essential cognitive skills\n(e.g., commonsense reasoning) which have proven difficult in the current, fully-supervised paradigm\n[Zellers et al. 2019a; Martin-Martin et al. 2021]. In turn, we discuss the potential implications of\nfoundation models for downstream applications, and the central challenges and frontiers that must\nbe addressed moving forward (§2.2.2: vision-challenges).\n2.2.1 Key capabilities and approaches.\nAt a high-level, computer vision is the core sub-field of artificial intelligence that explores ways to\nendow machines with the capacity to interpret and understand the visual world. It encompasses a\nmultitude of tasks, sub-domains and downstream applications, where the community has made\ncontinual progress over the last several decades [Zamir et al. 2018]. A selection of example tasks16:\n(1) semantic understanding tasks, which aim to discover the properties and relations among entities\nwithin visual scenes; these include image classification, object detection, semantic segmentation,\naction recognition, and scene graph generation, among others [e.g., Krizhevsky et al. 2012; He et al.\n2016a; Krishna et al. 2017; Russakovsky et al. 2015; Krizhevsky et al. 2009; Kay et al. 2017; Lin et al.\n2014]. (2) geometric, motion and 3D tasks, seeking to represent the geometry, pose and structure\n16This, of course, is a coarse selection: please see the categories at the annual conference on Computer Vision and Pattern\nRecognition (CVPR) for a more complete (but evolving) picture of the tasks in the field.\n30 Center for Research on Foundation Models (CRFM)\nof still or moving objects, and include tasks of depth estimation, structure-from-motion, surface\nnormal detection, curvature line and keypoint estimation, to name a few [e.g., Laina et al . 2016;\nAgarwal et al. 2011; Wang et al. 2015a; Zamir et al. 2018; Ullman 1979]. (3) multimodal integration\ntasks, combining semantic and geometric understanding with other modalities such as natural\nlanguage; these include, for instance, visual question answering, image captioning, and instruction\nfollowing [e.g., Antol et al. 2015; Chen et al. 2015b; Anderson et al. 2018; Goyal et al. 2017b; Hudson\nand Manning 2019b; Johnson et al . 2017; Luo et al. 2020; Akbari et al. 2021; Huang et al. 2021c;\nTsimpoukelli et al. 2021]. We highlight a subset of traditional core tasks in Figure 7.\nThe predominant paradigm for addressing these tasks, driven by the emergence of ImageNet\n[Deng et al. 2009] during the early 2010s, tends to center around a familiar core idea: First, pretrain\na model on a large collection of carefully annotated data [Russakovsky et al . 2015] with a fully\nsupervised training task, like image classification. Then, adapt the model downstream on task-\nspecific datasets and domains [Lin et al. 2014; Chen et al. 2015b; Antol et al. 2015] by fine-tuning\nto reach state-of-the-art performance [Krizhevsky et al. 2012; Simonyan and Zisserman 2015; He\net al. 2016a; Xu and Saenko 2016]. This notion of pretraining followed by adaptation persists\nin the definitions we consider now for foundation models (§1: introduction). The limitations\nof this fully supervised paradigm motivate the transition to foundation models: the reliance on\nexternal supervised annotations constrains the upper bound capability of previous approaches to\ncapture the diverse spectrum of visual inputs in a scalable, robust and generalizable manner. Recent\ndevelopments in the domain of visual synthesis and unsupervised learning offer a compelling\nalternative. GANs, for instance, learn to generate visual content of high fidelity, realism and diversity,\nby featuring two competing networks of a generator and a discriminator that can supervise one\nanother from image collections alone [e.g., Goodfellow et al . 2014; Hudson and Zitnick 2021].\nOther neural models infer the visual properties of objects and scenes without explicitly annotated\nsupervision, by employing variational auto-encoding, contrastive learning or other self-supervised\ntechniques [e.g., Kingma and Welling 2014; Chen et al. 2020c; He et al. 2020]. For instance, He et al.\n[2021] build upon prior work on representation learning with masked image encoding [e.g., Pathak\net al. 2016; Vincent et al. 2008] by, in part, combining recent advancements in flexible architectures\n(e.g., vision transformers [Dosovitskiy et al. 2021; Zhai et al. 2021]) with increased scaling.\nWith foundation models, the development of such self-supervision techniques has enabled train-\ning at greater scales of visual data [Changpinyo et al . 2021], both in terms of its scope as well\nas its potential diversity. Accordingly, we have seen early indicators of progress on traditional\nvision tasks in terms of both standard accuracy metrics and few-shot generalization. For image\nclassification and object detection, self-supervised techniques have reported competitive perfor-\nmance to prior fully-supervised approaches [He et al. 2019; Chen et al. 2020c; Radford et al. 2021;\nHénaff et al. 2021], without explicit annotations during training and greater sample efficiency\nduring adaptation. For visual synthesis, notable examples include DALL-E [Ramesh et al. 2021]\nand CLIP-guided generation [Radford et al. 2021; Galatolo et al. 2021], where researchers leverage\nmultimodal language and vision input to render compelling visual scenes. In the short-term, we\nanticipate that the capabilities of these foundation models will continue to improve along these\ndirections, as training objectives are refined [Chen et al. 2020a; Hénaff et al. 2021; Selvaraju et al.\n2021] and architectures are designed to incorporate additional modalities [Jaegle et al. 2021b].\nNotably, current foundation models for computer vision are nascent relative to their NLP coun-\nterparts (§2.1: language): promising early efforts are still largely centered on RGB image inputs\nand a subset of core traditional vision tasks. However, the field continues to progress on broader\nchallenges centered on embodied and interactive perception settings (critical for foundation models\nfor robotics [Bohg et al. 2017, §2.3: robotics]). We note a subset of these higher-order goals in\nFigure 7, including physical scene understanding, reasoning over visual commonsense and temporal\nOn the Opportunities and Risks of Foundation Models 31\nevents, and perception for social affordances. Each of these have been goals for fully-supervised\nsystems, but have proven challenging in part due to the difficulty of annotating these tasks at\nscale. For instance, standard systems for visual-question answering struggle to answer questions\nthat require commonsense understanding, since these questions often require external knowledge\nbeyond what is present in the pixels alone [Zellers et al. 2019a]. Perceiving human gaze and so-\ncial affordances in a robust manner remain ongoing challenges for embodied vision systems in\ninteractive agents [Martin-Martin et al. 2021]. By reducing the dependence on explicit annotations,\nfoundation models may enable further progress towards these goals than was previously feasible.\nRelated progress in language foundation models (§2.1: language), which have been able to capture\na degree of commonsense over language events [Brown et al. 2020], also suggests a potential avenue\ntowards achieving similar capability over multimodal visual inputs. While the exact roadmap for\nhow to achieve these capabilities in foundation models remains an open problem, a combination\nof new efficient and flexible architectures (§4.1: modeling), large-scale training (§4.5: systems),\nself-supervision techniques (§4.2: training) and few-shot adaptation schemes (§4.3: adaptation)\nmay open the door towards capabilities that have been difficult to reach so far.\n2.2.2 Central research challenges.\nOur discussion of research challenges is motivated by the downstream application domains where\nfoundation models may further the integration and impact of vision models. We highlight a few\nsuch areas: (1) ambient intelligence for healthcare and home environments: building upon existing\napproaches for ambient intelligence in these settings [Haque et al. 2017; Lyytinen and Yoo 2002;\nHong and Landay 2004], foundation models may offer the potential for better detection of fine-\ngrained human activities and medical events, as well as improved assistive interaction for clinicians,\npatients, and everyday consumers (see also §3.1:healthcare). (2) mobile and consumer applications :\nfoundation models with stronger multimodal grounding may enable more capable interactivity of\nservices in mobile settings, and fundamental improvements in generation capability from vision and\nlanguage inputs can benefit computational photography and content editing applications [Delbracio\net al. 2021; Ramesh et al. 2021; Park et al. 2019] (see also §2.5:interaction). (3) embodied, interactive\nagents: perception models have already proven effective as both inputs [Sermanet et al. 2018] and\nreward functions [Chen et al. 2021c; Shao et al. 2020] in robotics settings; foundation models trained\non large collections of egocentric (real/simulated, human/robotic) visual data [Damen et al. 2018;\nChen et al. 2021e] may potentially further this progress by capturing a wider distribution of visual\nscenes, objects, and actions (see also §2.3: robotics).\nThe extent to which foundation models may further impact these application settings hinges on\nthe degree to which the capabilities outlined in §2.2.1: vision-capabilities are realized. To bridge\nthe significant gaps between present, short-term, and long-term anticipated capabilities, we must\naddress current limitations of foundation models for vision, including their training and evaluation.\nBelow, a subset of corresponding key challenges:\nSemantic systematicity and perceptual robustness. Humans have a remarkable capacity for\ngeneralizing visual understanding to unseen compositions, and reasoning about the physical and\ngeometric properties of novel objects and scenes [Lake et al . 2015]. While current foundation\nmodels have shown promising capability for image synthesis and early results for generalization to\nfine-grained language inputs, these models still struggle to generalize to compositions of simple\nshapes and colors [Ramesh et al. 2021; Radford et al. 2021; Rong 2021]. Generalizability goes beyond\nsemantics as well; visual scenes and objects have a natural regularity to their physical dynamics\nand geometric properties. Foundation models have shown early indications of understanding\nscene and object geometry [Ramesh et al. 2021]. Further, early efforts towards physical scene and\n32 Center for Research on Foundation Models (CRFM)\ngeometric understanding in perception models may provide guidance for ongoing foundation model\ndevelopment [Yi et al. 2019; Bakhtin et al. 2019; Li et al. 2020b]. Indeed, the continued incorporation\nof multiple modalities (e.g., audio) in foundation models may prove beneficial towards these aims\n[Zhang et al. 2017; Gao et al. 2020b; Jaegle et al. 2021a]. However, the specific techniques to enable\ngeneralizing the initial observed capabilities robustly to a wide range of natural scenes and objects\nat the level of humans remains an open research challenge for foundation models.\nComputational efficiency and dynamics modeling. Humans are surprisingly efficient at pro-\ncessing the continuous visual stream of objects, scenes, and events necessary to support an un-\nderstanding of event dynamics [Zacks et al. 2001; Tversky and Zacks 2013]. Foundation models in\nlanguage (§2.1: language) have shown initial steps towards modeling longer-term coherence of\nevents; the analogous ability to capture long-range temporal correlations and causal coherence in\nvisual input would stand to benefit downstream settings like robotics [Dai et al. 2019; Alyamkin\net al. 2019; Goel et al. 2020b; Feng et al. 2019, §2.3: robotics]. However, relative to word token-level\ninputs in language, low-level computer vision inputs are extremely high-dimensional: a single\n1080p frame contains over 2 million pixels. In this context, modeling the richer event dynamics in\nlong-range video sequences seems like a daunting endeavor, especially with additional modalities\n(e.g., speech, optical flow, etc.) and increasing resolutions. Understandably, a naïve approach to\nfully processing every individual pixel is likely prohibitive. Current vision models [e.g., Radford\net al. 2021; Sun et al. 2019a; Tan and Bansal 2019; Kim et al. 2021a] often address this by processing\nembeddings that summarize image patches or even groups of frames altogether, but this has the\npotential drawback of losing fine-grained details [Ramesh et al. 2021]. In addition to considerations\nof the raw input space, foundation models for vision may need to revisit the design of fundamental\narchitecture primitives (§4.1: modeling) for efficient and effective modeling: alternatives to 3D\nconvolutions may better address its cubic complexity [Fan et al. 2020; Sitzmann et al. 2019], while\nparticle-based representations may prove more effective for modeling physical dynamics [Bear\net al. 2021]. Further, deployment of these vision models to downstream application settings will\nalso necessitate advancements in systems design (§4.5: systems). Taken together, the bottleneck of\nefficient and effective modeling for larger-scale, dynamic vision inputs remains a multi-faceted\nresearch direction that must be addressed going forward.\nTraining, environments, and evaluation. Equally critical to realizing the potential of founda-\ntion models are the supporting elements for training and evaluating them. Current foundation\nmodels for vision have largely focused on a small subset of modalities shown in Figure 7 (e.g., datasets\nof RGB images and text), since these are perhaps the most readily accessible [Changpinyo et al.\n2021; Radford et al. 2021]. This motivates the development and use of additional large-scale training\ndatasets which contain a diverse collection of inputs across a broad spectrum of modalities. While\nadditional annotations may not strictly be necessary, the input quality impacts the learning effi-\nciency of the models; techniques that leverage foundation models of other types (e.g., language) to\nhelp improve quality are a promising route forward [Zellers et al. 2021b]. We also want to consider\nsettings beyond static datasets: classic studies have suggested that perceptual understanding in\nhumans is linked to its embodiment and interactive, ecological settings [Gibson 1979]. As stepping\nstones towards longer-term capabilities of embodiment and interaction (§2.3: robotics), ongoing\ndevelopment of simulation environments that capture physical, visual, and ecological realism\nwith multiple modalities and viewpoints may play an important role in providing scalable and\nhigh-fidelity visual inputs for this goal [Kolve et al. 2017a; Savva et al. 2019b; Gan et al. 2020; Shen\net al. 2021a; Srivastava et al. 2021]. Finally, there is the question of metrics: how do we evaluate the\nfaithfulness of generative foundation model outputs with respect to semantics? Standard metrics\nlike Fréchet Inception Distance, suffer from known flaws [Bińkowski et al . 2018]; such issues\nOn the Opportunities and Risks of Foundation Models 33\nparallel ones in natural language processing (e.g., metrics like BLEU do not correlate with causal\njudgements from humans). Having human judgements as part of evaluation may be one route,\nbut incurs significant cost and may not be as scalable [Zhou et al. 2019; Khashabi et al. 2021]. The\noutstanding and open challenges surrounding the training (§4.2: training), data (§4.6: data), and\nevaluation (§4.4: evaluation) settings for vision foundation models are indeed quite nuanced, and\nwill be a central area of research going forward.\nConcluding remarks. In this section, we explored foundation models in the context of computer\nvision, from identifying roots in previous computer vision paradigms, to contextualizing its current\nand anticipated capabilities, to proposing research directions moving forward. We conclude with a\nbrief discussion of some broader societal implications of foundation models for computer vision\nand their continued development (see also §5: society). The ubiquity of cameras in our society\nmeans that advances in computer vision techniques have great potential for disruptive impact; this\ncarries a corresponding burden of responsibility for careful consideration of its risks. There is a\nwell-documented history of learned bias in computer vision models, resulting in lower accuracies\nand correlated errors for underrepresented groups, with consequently inappropriate and premature\ndeployment to some real-world settings [e.g., Buolamwini and Gebru 2018, §5.1: fairness]. Many\nof the same underlying issues continue to persist in current foundation models [Agarwal et al. 2021].\nAs data from additional sensor modalities (e.g., wearable or ambient sensors, Figure 7) become\nincorporated in these foundation models, concerns surrounding privacy and surveillance become\nparamount (see §5.6: ethics). Furthermore, generated deepfake images and misinformation pose\ngreater risks as the semantic and generative capability of vision foundation models continues to\ngrow [Dolhansky et al. 2020; Ramesh et al. 2021, §5.2: misuse]. While the intriguing open challenges\nand opportunities ahead for computer vision and foundation models are significant, addressing\nthese and related risks concurrently remains essential.\n34 Center for Research on Foundation Models (CRFM)\n2.3 Robotics\nAuthors: Siddharth Karamcheti, Annie Chen, Suvir Mirchandani, Suraj Nair, Krishnan Srinivasan,\nKyle Hsu, Jeannette Bohg, Dorsa Sadigh, Chelsea Finn\nFig. 8. Building new types of foundation models for robotics will require massive datasets spanning diverse\nenvironments and behaviors. Simulation, robotic interaction, videos of humans, and natural language descrip-\ntions could all be useful data sources for these models. Despite the challenges of acquiring data, developing\nnew foundation models for robotics has tremendous potential for a variety of problem formulations in task\nspecification and robot learning. Image credits: [Finn et al. 2016b; Szot et al. 2021].\nA longstanding challenge of robotics research is to endow robots with the ability to handle the\nmyriad conditions they will encounter in real-world settings. In this section, we discuss how the\nideas underlying foundation models can potentially help bring about “generalist” robots that can,\nfor example, cook a new meal in a new house, with a new kitchen. To make progress towards\nthis goal, existing foundation models will not suffice. We need new types of models trained on\na multitude of data sources, spanning grounded robotic interaction data to videos of humans\nperforming tasks, amongst others. We focus on how such foundation models can apply to the\nproblem of a robot controlling its own physical embodiment to successfully perform different tasks.\nThis is a high-dimensional and closed-loop decision-making problem: the actions that a robot\ntakes directly influence what it perceives next, which in turn influences the next robot action.\nThis closed-loop aspect is not traditionally studied in language and computer vision, where large\noffline datasets are dominant and foundation models have already seen success. We focus on how\nthe demonstrated benefits of foundation models — large-scale, self-supervised learning — can be\nleveraged in this new closed-loop data regime. The promise of a new type of robotic foundation\nmodel is in its ability to amplify the potential of robots to improve key facets of daily life ranging\nfrom manufacturing [Nof 1999; Sanneman et al. 2020], construction [Khoshnevis 2004; Bock 2007],\nautonomous driving [Thorpe et al. 1988; Badue et al. 2020], to household aid [Thrun and Mitchell\n1995; Brooks 2002; Dillmann 2004; Goodrich and Schultz 2007; Gupta et al . 2018; Shridhar et al.\n2020] and personal assistance [Dragan and Srinivasa 2013; Javdani et al. 2018], amongst others. Our\ndiscussion in this section primarily focuses on mobile manipulation robots for household tasks, but\nwe expect its essence to be broadly applicable to the other use-cases of robotics listed above.\nOn the critical path towards building new types of foundation models for robotics is embracing\nopportunities in task specification and task learning , coupled with tackling challenges in data\nacquisition and safety and robustness . Consider the following robot learning paradigm: starting with\nOn the Opportunities and Risks of Foundation Models 35\na description of a task capturing what a user might like the robot to do (e.g., “make breakfast”) — learn\na corresponding policy to generate the desired robot actions. While policies can be parameterized\nin different ways, a common choice is that of a function that maps the task representation and\nenvironment observation (e.g., a scene image from a fixed or egocentric camera, or inputs from\nalternative sensors like LIDAR) to robot actions [Andrychowicz et al. 2017; Nair et al. 2018]. As the\nrobot acts in a task-conditioned manner, the subsequent states are fed back to the policy, generating\nmore actions until the task has been satisfied.\nYet, implementing such a paradigm in practice is difficult. To begin, what is the right interface\nfor describing one’s goals? For a given user in one context, “make breakfast” carries an implication\nof a full breakfast that consists of fried eggs, toast, and a glass of orange juice; for another user,\n“make breakfast” may imply idlis with sambar and a tumbler of filter coffee. In general, high-level\ncontext-dependent goals like these do not stand alone and can introduce a multitude of ambiguities.\nHow does one specify a goal (and corresponding subgoals) with enough clarity to both resolve\nthese ambiguities, and in so doing, allow a robot to make progress on the given task? Additionally,\nhow might we craft general task representations that might aid generalization to similar objectives\n(e.g., fetching a glass of milk instead of orange juice). Going a step further, how do we build methods\nthat aid robots in learning policies for new tasks and new environments (in this case, a brand new\nkitchen with new utensils, appliances, layouts, etc.)?\nRecent breakthroughs in applying foundation models for language and vision (§2.1: language\nand §2.2: vision) suggest several potential benefits of large-scale, self-supervised pretraining for\nimproving generalization. The ability to tap into diverse streams of data to learn meaningful\nrepresentational priors (akin to those learned by models such as BERT and GPT-3) holds promise\nfor learning powerful robotic foundation models for task specification. Diverse robotic interaction\ndata can be used for learning action-conditional dynamics models or policies indexing general\nand semantically meaningful skills thereby holding promise for task learning. Yet while these\nopportunities exist, the key stumbling block is collecting the right data . Unlike language and\nvision data, robotics data is neither plentiful nor representative of a sufficiently diverse array of\nembodiments, tasks, and environments — we (as a field) still have not converged on thekinds of data\nthat would be maximally useful for enabling generalist robotics (e.g., offline demonstrations, third-\nperson recordings of humans, egocentric videos, autonomous experience, etc.) Coupled with issues\nin obtaining the right scale and diversity of data are questions of ensuring safety and robustness:\nhow do we behave in a new environment without causing damage?\nBuilding new types of foundation models for robotics thus consists of a dichotomy of opportuni-\nties and challenges: opportunities for task specification and learning balanced against challenges of\ndata collection and safe deployment. This section explores both by presenting a picture of how\nrobotic foundation models might help us develop generalist robots, in a way that not only mean-\ningfully addresses the challenges associated with building such systems, but that also embraces\nthe potential of multi-modality — incorporating perception, actuation, and language — as well as\nhuman-robot interaction for specification and learning.\n2.3.1 Opportunities.\nRobotic foundation models could take a variety of forms: problems in robotics do not easily conform\nto a one-size-fits-all model, since different problems have different input-output signatures — a\ncontrast to domains like NLP where many problems can be cast into a general “text-in, text-out”\nsignature. We focus on opportunities in generalizable task specification and learning across tasks,\nenvironments, and robot embodiments.\n36 Center for Research on Foundation Models (CRFM)\nFoundation models for task specification. Before robots can learn how to solve tasks in a\ngeneral purpose way, they must understand what the desired task is: for example, to be useful in a\nnew kitchen, a robot needs to know what we would like it to cook, as well as behaviors we would\nlike it to avoid. Therefore, a necessary first step towards developing generalist robots is building\na new type of foundation models for reliable task specification, i.e., the intuitive and effective\ncommunication of task objectives, preferences, and constraints. We formalize task specification\nas a process that transforms a human-provided task description into a quantitative metric that\nmeasures a robot’s task completion and progress — e.g., a reward function. This signal is crucial for\noptimizing robot behavior, diagnosing failures, and prompting human feedback. As the most natural\nway to describe a task can vary depending on the user, environment, or task, robotic foundation\nmodels for task specification should accept a variety of description modalities, such as goal states\n[Fu et al. 2018; Singh et al. 2019], natural language [MacGlashan et al. 2015; Karamcheti et al. 2017;\nMisra et al. 2017b; Co-Reyes et al. 2019; Shao et al. 2020], videos of humans [Shao et al. 2020; Chen\net al. 2021c; Liu et al. 2018], pairwise or ranking comparisons [Biyik and Sadigh 2018], interactive\ncorrections [Co-Reyes et al. 2019; Karamcheti et al. 2020] and physical feedback [Ross et al. 2011;\nBajcsy et al. 2017].\nAn important requirement of general purpose models for task specification is the ability to\ntransfer to new environments and tasks. Reliably transforming task descriptions into generalizable\nreward signals for robot learning remains an open problem [Taylor et al. 2016] — one that robotic\nfoundation models will arguably be well suited for. When applied to task specification, such\nmodels should provide more robust (§4.8: robustness) reward signals by learning from large and\nbroad datasets — even leveraging multiple of the description modalities listed above. One possible\ninstantiation of a new foundation model for task specification might be to learn a mapping from\narbitrary (language, current observation) pairs to reward signals by training on diverse language\nand vision datasets [Bahdanau et al. 2019; Fu et al. 2019; Chen et al. 2021c]. By learning informative\npriors from these broad, diverse datasets, such a model may be able to generalize to unseen language\ninstructions and observations in unseen environments. In general, the potential for new foundation\nmodels to be able to deftly bridge modalities and generalize broadly make them appealing for\ngeneral purpose task specification.\nFoundation models for task learning. In addition to enabling more general task specification,\nrobotic foundation models could make learning to solve new tasks more efficient and reliable. In\nthis context, these new types of foundation models might take the form of a joint distribution over\nactions, sensor observations, rewards, and other properties of interest. Conditioning on different\ndimensions of this joint distribution recovers different inference problems, each corresponding to a\ndifferent signature:\n•Dynamics modeling : 𝑝(future observations |actions, past observations) [Finn and Levine\n2017; Hafner et al. 2019; Wu et al. 2021d].\n•Policy learning : 𝑝(actions |observations, goal) [Kaelbling 1993; Schaul et al. 2015; Ding et al.\n2019].\n•Inverse reinforcement learning : 𝑝(reward function |observations, actions) [Ng and Russell\n2000; Ziebart et al. 2008; Finn et al. 2016a].\nA plausible training objective for a robotic foundation model is to predict the different elements of\nthe joint distribution described above in an autoregressive fashion [Janner et al. 2021; Chen et al.\n2021b, §4.1: modeling]. However, these are not the only options. In particular, robot datasets contain\nlarge amounts of unlabeled data consisting of synchronized observations from many different\nsensor modalities (e.g., RGB and depth cameras, haptic sensors, microphones, etc.) and a sequence\nof actions that the robot has performed to generate these observations. Beyond the objectives above,\nOn the Opportunities and Risks of Foundation Models 37\na robotic foundation model could be trained to predict observations of one sensor modality from\nanother or to predict whether two streams of sensory observations are from the same segment\nof time. These kinds of self-supervised objectives can leverage multi-modal correspondences to\nproduce low-dimensional representations of high-dimensional data, and can even be combined with\nthe above objectives to yield models, policies, and rewards on top of those representations.These\nobjectives may facilitate the training of powerful robotic foundation models from unlabeled data —\nas long as the data exhibits diverse, meaningful behavior. §2.3.2: robotics-challenges discusses\nthe challenges of collecting such data further.\nIn language and vision, foundation models have demonstrated the capability to learn broadly\napplicable priors from large, diverse datasets, that can be subsequently adapted to downstream\ntasks (§2.1: language, §2.2: vision). Robotic foundation models have the potential to similarly\nenable few-shot adaptation of perception and control to new environments, tasks, and embodiments\nthough leveraging different data, self-supervised objectives, and modalities than have been studied\nwith existing language and vision models. Consider our running kitchen example. To cook in a\nnew kitchen, a robot needs to adapt to the specific environment — its spatial layout, the available\nequipment, etc. Priors learned from offline videos of humans, robotic interaction, text, and/or\nsimulation might encode general aspects of kitchens, such as the fact that stoves are usually\nagainst walls and must be turned on in order to produce heat. Such commonsense knowledge,\nphysical priors, and visual priors could make adaptation to new environments more sample efficient.\nSimilarly, developing a new foundation model for robot task learning might enable the use of a\nlarge number of cooking videos in its training dataset to adapt a policy for a common skill, such as\n“fry an egg, ” to a specific user’s preferences from a low number of demonstrations — allowing for\nsample efficient adaptation. Finally, with their potential to learn the cross-modal representations\ndescribed earlier, robotic foundation models could help enable adaptation to new embodiments.\nThis aspect of adaptation is crucial to make these models widely useful.\n2.3.2 Challenges and risks.\nDespite this exciting vision, multiple challenges need to be overcome. To enable the generalization\ndiscussed above, we must collect robotic datasets of sufficient size and diversity. Additionally, we\nneed mechanisms to ensure that we can deploy learned behaviors safely in the real world.\nData needs & challenges. Learning a policy for a robot that perceives the state of its environment\nvia sensors and takes actions to accomplish tasks traditionally requires large datasets of the robot\ninteracting in the real world . On the other hand, many learning tasks in computer vision and natural\nlanguage processing rely on large and diverse offline datasets that can easily be scraped from\nthe web. Motivated by the advances of existing foundation models in language and vision, we\nare excited by the possibility of leveraging large offline data sources for training new foundation\nmodels for robotics.\nOne path towards this goal is collecting large datasets for offline learning, for example using\nteleoperation [Mandlekar et al. 2019], kinesthetic teaching [Sharma et al. 2018], or autonomous\nmethods [Pinto and Gupta 2016; Gupta et al. 2018; Levine et al. 2018; Dasari et al. 2019; Kalashnikov\net al. 2021; Chen et al. 2021d], which have shown some promising indications on generalization.\nWhile scaling up robot data collection to the size of vision and language datasets [Deng et al. 2009;\nKrishna et al. 2017; Raffel et al. 2019; Gao et al. 2020a] remains an open challenge, the increasing\nscale and quality of robotic datasets suggests they can play an important role in learning foundation\nmodels for robotics. Moreover, because robots have the ability to actively and autonomously shape\ntheir environment, they should be able to generate targeted unlabeled data at scale.\n38 Center for Research on Foundation Models (CRFM)\nGiven the challenging closed-loop nature of learning control, it is possible that collecting datasets\nof size comparable to those used in vision and language is insufficient for robotics. One exciting\noption is to additionally leverage external, non-robotic sources of data such as videos of humans or\nexisting vision and natural language datasets. Such data is diverse and exists in large quantities\non the web [Deng et al. 2009; Lee et al. 2012; Heilbron et al. 2015; Goyal et al. 2017a; Damen et al.\n2018; Gao et al. 2020a; Grauman et al. 2021], affording the possibility of broad generalization if\nproperly leveraged. Elegantly addressing the gap between the robot’s domain and those found in\nvideos or language on the web remains an open challenge; however, recent progress in domain\nadaptation [Smith et al. 2019; Schmeckpeper et al. 2020] and using pretrained video and language\nmodels in robotics [Lynch and Sermanet 2020; Shao et al. 2020; Chen et al. 2021c] present promising\ndirections towards closing this gap.\nFinally, simulation presents a boundless source of rich interactive data that robots can learn from,\nwith a range of sensor modalities like rendered visuals, point-clouds, and simulated touch/audio.\nHowever, a major challenge lies in bridging the gap between simulation and the real world, both in\nthe underlying physics and in the semantic distribution of environments and tasks. Recent work\nhas shown that by using extensive domain randomization, tasks ranging from flight [Sadeghi and\nLevine 2017] to contact-rich manipulation [Mahler et al. 2017; OpenAI et al. 2019] and locomotion\n[Peng et al. 2020; Hwangbo et al. 2019] skills learned in simulation can be transferred to real robots\nwith some success, and that the semantic and visual distribution of the real world can be simulated\nby scanning the real world into a simulation [Chang et al. 2017; Kolve et al. 2017b; Savva et al. 2019a;\nSzot et al. 2021; Shen et al. 2021a]. While these are promising steps towards closing the sim-to-real\ngap, effective and general sim-to-real learning of manipulation and locomotion skills remains an\nopen challenge. Simulation data, real robot data, videos of humans, and natural language data could\nall be essential to learning robotic foundation models.\nSafety & robustness. Further complicating the development of new foundation models for robotics\nis ensuring their safety and robustness when training or deploying them in the real world. We\ncan expect the safety risks from these models for robotics to be different from their language\ncounterparts given that embodied agents are empowered to manipulate and interact with their\nsurroundings directly in the physical world. One core safety challenge for learning-based systems is\nthe chicken-and-egg problem of needing to specify system constraints for safety prior to collecting\ndata, after which unforeseen unsafe behaviors requiring additional constraints may emerge. For\ninstance, an agent adapting to a new kitchen outside of the training distribution requires sufficient\nsafety guarantees to ensure safe data collection, which may either adversely affect task performance\nor cause the agent to fail in novel ways. One way to resolve this is restricting the complexity of\nthe environment or increasing the complexity of the robot such that irrecoverable states or unsafe\nactions are avoided by construction. The robot can also be tasked with autonomously resetting the\nenvironment to facilitate uninterrupted learning (or adaptation) from large-scale data collection\n[Eysenbach et al. 2017; Gupta et al. 2021b]. This would either mean ensuring that nothing in the\nkitchen is breakable, or ensuring and replacing the items the agent may break while it attempts to\ncollect data.\nTo address risks posed by robotic foundation models that fail to generalize or produce unexpected\nbehaviors to new stimuli, potential future directions include developing a causal analysis of agents\n[Déletang et al. 2021], new formal safety evaluation tools, and realistic simulation environments\n[Corso et al. 2020; Dreossi et al. 2017; Julian and Kochenderfer 2019]. Finally, deriving formal safety\nguarantees for robotic foundation models, e.g., Hamilton-Jacobi reachability of safe-sets [Chow\net al. 2018; Fisac et al. 2019; Herbert et al. 2021] or developing safety boundaries for learning that\nare interpretable (§4.11: interpretability) to human operators, could help reduce risks posed\nOn the Opportunities and Risks of Foundation Models 39\nby such models [Berkenkamp et al. 2017]. As the development and study of these new types of\nfoundation models progresses, solutions to these challenges will be crucial.\nConclusion. While the promise of robotic foundation models are many — spanning multiple levels\nof the robotics pipeline from task specification to task learning — the challenges are significant.\nCollecting data in the physical world that covers diverse environments and embodiments at scale is\na sizable hurdle, and ensuring the safety and robustness of such systems is equally exigent. Despite\nthis, our optimism prevails; tackling these challenges now, before developing models offers us the\nchance to identify ways to collect the right data, from the right sources, at the right scale to build\nsafe and reliable robotic foundation models with the capabilities we desire.\nUnderpinning this section has been a theme of multimodality. Robotic foundation models — in\nall possible instantiations — have and will continue to benefit from work in other subfields of\nAI such as language and vision (§2.1: language, §2.2: vision). Yet as we consider incorporating\nthese extensions from other fields, there are interdisciplinary challenges on the horizon that touch\nother aspects of foundation models: systems innovation for training and deploying such models\nfor real-time robotics (§4.5: systems), innovation in interfaces for robust human-robot interaction\n(§2.5: interaction), and lessons to incorporate as we better grasp the safety and robustness of such\nmodels (§4.9: ai-safety, §4.8: robustness). Building a reliable ecosystem and thoughtful research\npractices around foundation models in general, and robotic foundation models in particular, is key\nto realizing these goals.\n40 Center for Research on Foundation Models (CRFM)\n2.4 Reasoning and search\nAuthors: Yuhuai Wu, Frieda Rong, Hongyu Ren, Sang Michael Xie, Xuechen Li, Andy Shih, Drew A.\nHudson, Omar Khattab\nFig. 9. Multimodality can allow foundation models to not only reason with formal symbolic language, but\nalso exploit visual aspects of the problem, such as equivalence, symmetry, and Euclidean geometry, to prune\nthe infinite search space and find promising constructions for a solution (§2.4.1: reasoning-tasks), mimicking\nthe way humans reason about geometry problems.\nReasoning and search have been a central theme throughout the history of AI. Classic tests of\nintellect, from strategy games to abstract mathematical discovery, served as inspirational goal posts\nthat pushed the limits of “machine intelligence” through a need to devise ever smarter ways of\nsearching for winning solutions. In the early days, symbolic methods were the dominant approach\nfor reasoning [Russell and Norvig 2020], but the involved engineering effort and the need to\nformalize heuristics to tackle intractable search spaces quickly proved cumbersome. More recently,\ndata-driven methods using neural networks have shown encouraging results — e.g., defeating the\nbest humans in Go [Silver et al. 2016], a board game with a much larger space of actions than the\nclassic challenge of chess — by exploiting statistical structures and learning useful heuristics. This\nsection outlines existing reasoning tasks, ones that require scaling to ever-larger search spaces and\nunderstanding the world broadly (§2.4.1: reasoning-tasks). We then argue in §2.4.2: reasoning-\nrole that foundation models should play a central role towards general reasoning as vehicles for\ncapturing the statistical regularities of unbounded search spaces (generativity), allowing positive\ntransfer across tasks and scenarios (universality), and exploiting the grounding of knowledge in\nmulti-modal environments (grounding).\n2.4.1 What are the current tasks?\nMany reasoning problems pose unbounded search spaces, where systems must deal with numerous\nkinds of open-ended alternatives. Consider trying to prove that the angles ∠𝐵and ∠𝐶 are equal for\nan isosceles triangle △𝐴𝐵𝐶with 𝐴𝐵 = 𝐴𝐶(Figure 9). A system can perform any number of actions\nat each step of reasoning . For instance, the system could add a new auxiliary point with an arbitrary\nconstruction, say a perpendicular line, a parallel line, or a tangent circle, and the search space only\ngrows larger as the diagram grows more complicated. One way to prove this theorem is to draw\na line 𝐴𝐷 that is the angle bisector of 𝐴, and use the congruence of the two triangles △𝐴𝐵𝐷 and\n△𝐴𝐶𝐷 to show ∠𝐵 = ∠𝐶, but how can systems find this without extensive search?\nMore generally, a mathematician is not confined with searching in diagram constructions and\nEuclidean theorems: mathematicians can apply a vast number of theorems from various branches\nOn the Opportunities and Risks of Foundation Models 41\nFig. 10. Left: A reaction route for 1,6-Heptadiene-3,5-dione predicted by machine learning-based drug\nretrosynthesis planner AiZynthFinder [Genheden et al. 2020; Yoshikawa et al. 2021]. Right: A sample proof\ntree in propositional logic where the formulas outlined in green represent axioms. Although they arise from\ndifferent domains, both trees are structurally the same.\nof mathematics, make high-level conjectures, formalize new mathematical concepts, or find coun-\nterexamples. This contrasts with more structured AI challenges such as the game of Go, whose\nsearch space is considered much smaller.17\nBesides theorem proving, many real-world problems deal with unbounded search spaces, such as\nprogram synthesis [Gulwani et al. 2017], drug discovery [Drews 2000], chemical synthesis [Segler\net al. 2018], computer-aided design [Haigh 1985], combinatorial optimization [Bengio et al. 2021],\nand more. These reasoning problems tend to exhibit similar structure, like the bijection between\nretrosynthesis in drug discovery and theorem proving in propositional logic, illustrated in Figure 10:\nin both problems, one is building a tree of synthesis, whose nodes are chemical products on the\none side and propositions on the other, and the leaf nodes are the products on the one side, and\nend axioms on the other. In these problems, a simulated environment is often provided, which\nallows a solver to run several search threads towards building the solution tree. The simulator\noften provides intermediate feedback, say, informing the solver with the remaining propositions\nto establish before the proof is considered complete. The solver in turn needs to select the most\npromising search thread and proceed based on the intermediate feedback.\nRecently, there has been a surge of interest in applying learning-based approaches to tackle\nreasoning problems. To overcome the unbounded search space challenge, researchers first started\nwith a constrained search space to make the problem tractable [Huang et al. 2018; Bansal et al. 2019].\nBut such approaches suffered from the limited kinds of actions the solver could issue. For example,\nthe solver could only apply theorems from a known database to prove the target theorem, instead\nof synthesizing novel theorems and lemmas. Because large language models offered a generic way\nof modeling the output space as a sequence, they quickly became a more favorable choice, allowing\nthe generation of arbitrary kinds of actions. Researchers have applied these language model-based\napproaches to various applications, such as predicting protein structures [Senior et al. 2020], proving\nformal theorems [Polu and Sutskever 2020; Han et al. 2021], conjecturing theorems [Urban and\n17Less than the number of grid points on the Go board (i.e., 361 actions for a 19×19 board).\n42 Center for Research on Foundation Models (CRFM)\nJakubuv 2020; Rabe et al. 2021; Li et al. 2021b], synthesizing programs from natural language [Chen\net al. 2021f; Ling et al. 2016], repairing, generating and understanding code [Yasunaga and Liang\n2021; Lu et al. 2021b; Guo et al. 2020; Svyatkovskiy et al. 2020; Kim et al. 2021b; Zügner et al. 2021]. It\nhas also been shown that scaling model size significantly improves reasoning capabilities [Polu and\nSutskever 2020], and furthermore standard techniques from language modelling, such as pretraining,\ncan also greatly improve performance on these tasks [Rabe et al. 2021; Polu and Sutskever 2020].\n2.4.2 What’s the role of foundation models?\nGenerativity. We believe that the generative capabilities of foundation models are essential for\neffective reasoning. Due to the unbounded search space, it becomes intractable to enumerate\nall kinds of possibilities. Instead, with foundation models, one can model the distribution of the\noptimal decisions, and generate suitable candidates to proceed to the next step. In particular, as\nfoundation models offer a generic way of modeling the output space as a sequence, the next\ndecision generation is entirely unconstrained and hence universal. Such flexibility is essential for\nmany of the reasoning challenges we discussed, to allow creative generation in domains such as\nmathematical conjecturing [Li et al. 2021b] and synthesizing novel programs [Chen et al. 2021f].\nAs one scales up foundation models, the capabilities of capturing such statistical structures also\ngrow immensely [Polu and Sutskever 2020].\nUniversality. As we mentioned in the last section, many reasoning problems exhibit similar latent\nstructures. We believe that the unifying framework imposed by a foundation model can transfer and\nshare significant heuristics across tasks, ranging from generalizing low-level techniques that work\nwell for one task to new scenarios all the way to directly finding meta-techniques that work well\nacross numerous kinds of problems. In addition, since a foundation model is trained across many\ndomains, it can positively transfer meta-knowledge encoded in the foundation models’ weights\nacross tasks and domains [Papadimitriou and Jurafsky 2020; Wu et al. 2021f; Lu et al. 2021a]. The\nfoundation model training and adaptation framework encourage a separation of concerns, where\nfoundation model training learns meta-knowledge such as the shared search tree structure between\ndrug retrosynthesis and propositional logic proofs, and the adaptation phase can focus on learning\nthe task specific vocabulary. Thus, foundation models can reduce the complexity of the learning\nproblem in the adaptation phase, improving sample complexity and generalization.\nGrounding. Reasoning problems are often easily expressed in symbolic languages (e.g., mathe-\nmatics, code, SMILE representation of molecules). However, these symbols have deep underlying\nsemantic meanings — saying “isosceles triangle” paints a vivid image in the human mind. Founda-\ntion models can enable deep groundings and semantic meanings. First, grounding representations\nin other modalities, such as visual or physical, are essential to grasp abstract concepts in reasoning\ntasks and endow them with concrete meaning [Larkin and Simon 1987; Jamnik 2001]. Since the\nmodels may be trained on multiple modalities, foundation models can assist in understanding a\nrange of data sources (e.g., images, texts). Hence, in the geometry example case, with its understand-\ning of geometrical shapes learned in natural images, a foundation model could effectively utilize\nthe diagrammatic representation of the problem. However, aligned multi-modal data in reasoning\nis scarce, and it remains an open question whether foundation models can discover connections\nbetween different modalities in an unsupervised manner (e.g., discovering of commutative diagram\nwith the corresponding algebraic equations). Furthermore, even within the symbolic domain, sym-\nbols can have various levels of interpretation. For example, high-level programming languages can\nbe translated to low-level assembly codes. Foundation models can learn a shared representation\nthat encompasses these various views. Past works have shown that self-supervised tasks [Han et al.\nOn the Opportunities and Risks of Foundation Models 43\n2021; Peng et al. 2021; Li et al. 2021a] allow the model to understand the inner workings behind the\nhigh-level code scripts, and further assist downstream tasks.\n2.4.3 Future challenges in reasoning.\nDue to the intrinsic difficulty of these problems, high-quality annotated data is scarce and harder\nto collect compared to raw images and text. There have been several attempts towards alleviating\nthis issue. In mathematics, researchers proposed to generate synthetic theorems in the hope of\ngeneralizing to realistic theorems [Wang and Deng 2020; Wu et al. 2021a; Firoiu et al. 2021; Zhou\net al. 2021c]. Another approach is to design self-supervised tasks to augment datasets [Yasunaga\nand Liang 2020; Ren et al. 2020; Han et al. 2021; Rozière et al. 2021; Yasunaga and Liang 2021], or\nbetter pretraining objectives [Wu et al. 2021f]. However, we still lack general principled approaches\nin designing self-supervised tasks, as most of the existing works are tailored to specific problem\nsetups [Yasunaga and Liang 2020; Ren and Leskovec 2020; Han et al. 2021]. Building a foundation\nmodel will encourage a unifying framework of constructing a suite of self-supervised tasks that\ncan be applied to all reasoning problems. In addition, interactivity (§2.5: interaction) could,\nwith enough scalability, alleviate the data scarcity problem by bringing humans into the loop to\nminimally guide the learning curriculum or data augmentation process, for example, in selecting\naxioms to add or conjectures to explore, while interactive tools themselves are a motivating use of\nfoundation models for reasoning [Han et al. 2021; Chen et al. 2021f] in assisting people with the\nmost cognitively demanding or laborious aspects. Interpretation-friendly interactive tools could\nfind further applications in education by assisting humans in learning with the help of highly\ncapable foundation models (§3.3: education).\nImproving the high-level reasoning capabilities is a core challenge for existing foundation models.\nHumans perform abstract reasoning and high-level planning in tackling difficult problem-solving\ntasks [Miller et al. 1960]. For example, when building a software tool or proving a theorem, we often\nstart with a high-level sketch before delving into the low-level details [Koedinger and Anderson\n1990]. Existing foundation models are not trained to generate such high-level plans. Instead, they\noften focus solely on predicting the next low-level steps [Polu and Sutskever 2020; Han et al. 2021;\nChen et al. 2021f]. Unfortunately, to train foundation models to emulate human-like reasoning,\nwe again face a data collection challenge. Although such data does exist in limited settings [Li\net al. 2021b], in general, data for high-level reasoning is scarce and difficult to collect. One line of\nresearch is to let abstract and modular hierarchy to emerge by itself during learning [Ellis et al .\n2021; Hong et al. 2021], but it still remains an open question how to scale these approaches to more\ngeneral and realistic settings.\nAside from these challenges, there exist many open questions that are also essential to topics\ndiscussed in other sections. What constitutes a good architecture for reasoning reliably (§4.1: mod-\neling)? How can we understand and interpret these models theoretically (§4.10: theory and\npractically §4.11: interpretability)? Can we train robust reasoning models that could generalize\nto out-of-domain problems (§4.8: robustness and §4.3: adaptation)? We believe research about\nfoundation models on each of these fronts can greatly broaden their impact for the field of reasoning.\n44 Center for Research on Foundation Models (CRFM)\n2.5 Interaction\nAuthors: Joon Sung Park, Chris Donahue, Mina Lee, Siddharth Karamcheti, Dorsa Sadigh, Michael S.\nBernstein\nFig. 11. Foundation models will bring significant opportunities to developers by lowering the difficulty\nthreshold for building AI-infused applications, and to the application users by raising the ceiling for what\ntypes of interactions are achievable. In some cases, the line between developers and users will start to blur,\nand users may be able to easily develop their own AI applications, for instance with natural language.\nThe early forms of foundation models such as GPT-3 [Brown et al. 2020] and DALL·E [Ramesh\net al. 2021] have demonstrated a high level of versatility both in terms of their ability to let even non-\nML experts to prototype powerful AI-infused applications, and their ability to seamlessly integrate\nmodalities ranging from texts to images. As the development of foundation models matures, the\nmodels’ capacity will continue to expand and their versatility may ultimately lead to fundamental\nchanges in how we interact with AI by allowing us to rapidly prototype and build highly dynamic\nand generative AI-infused applications. In this section, we discuss the opportunities that these\nchanges present from the perspectives of two important stakeholders: (1) applications developers\nwho will interact with foundation models to design user experience, and (2) end-users who will use\nor be affected by the AI-infused applications powered by foundation models. Finally, we consider\nscenarios in which the line that rigidly separates developers and end-users today may start to blur,\naffording new opportunities for creating AI-infused applications that more closely satisfy users’\nneeds and values.\n2.5.1 Impact on AI-infused application developers’ development process.\nHow will foundation models transform the way developers create AI-infused applications? Despite\nthe monumental progress in machine learning algorithms and systems infrastructure, some point\nout that designing novel and positive forms of human-AI interaction remains difficult [Dove et al.\n2017; Cooper et al . 2014]. The vast amount of data, computing resources, and skills needed to\ncreate a powerful task-specific model is frequently in conflict with the iterative prototyping process\nnecessary to elicit and satisfy users’ needs and values [Yang et al. 2016]. This challenge is further\nOn the Opportunities and Risks of Foundation Models 45\ncompounded by the fact that AI responses can be unpredictable, and models can produce a vast\ngenerative output space, making it difficult for people to build effective mental models of their\nperformance. There has already been some progress on tackling these challenges in the form of\nwork on interactive machine learning (e.g., Crayon [Fails and Olsen 2003], Regroup [Amershi et al.\n2012]) and design frameworks for conveying uncertainty in AI to end-users (e.g., principles of mixed-\ninitiative [Horvitz 1999]). However, more work is still needed to overcome these obstacles [Yang\net al. 2020].\nFoundation models pose important opportunities to address many of the challenges mentioned\nabove. For instance, language-based foundation models’ ability to take natural language as input, and\nto generalize to many downstream tasks, could significantly lower the difficulty “threshold” [Myers\net al. 2000] for application development, i.e., by enabling the development of sophisticated models\nwithout having to collect significant amounts of data and train large models from scratch. This\ncould enable even non-ML experts to quickly prototype AI-infused applications. At the same time,\nthe powerful generative and potentially multi-modal capabilities of foundation models could offer\na far higher “ceiling” [Myers et al. 2000] of what types of interactions are achievable both in terms\nof their quality and diversity as we will discuss below. However, how successfully we can leverage\nthese capacities will depend on how effectively we can wrangle foundation models into forms that\nwill be more manageable by application developers.\nUnfortunately, the same generalizability and high ceiling that give foundation models their edge\ncan also make these models difficult to work with, as they may be even more unpredictable and\ncomplex than single-purpose AI models. Indeed, recent work has shown that it can be difficult to\nmake models like GPT-3 consistently perform the intended task [Reynolds and McDonell 2021],\nwhile understanding what it is capable of is still an active area of research [Hendrycks et al .\n2021a]. In an effort to improve the reliability and trustworthiness of AI-infused applications, we\nrecommend that future work should continue to investigate how to achieve more predictable\nand robust behaviors from foundation models (e.g., through fine-tuning, or in cases where the\nmain mode of interaction is natural language prompt, through prompt-engineering [Reynolds and\nMcDonell 2021; Liu et al. 2021d], calibrating [Zhao et al. 2021], or pre-formatting a task-specific\nendpoint.18 Please see §4.8: robustness for more details).\n2.5.2 Impact on end-user interaction with AI-infused applications.\nBeyond the new ways developers might create AI-infused applications, what changes will foun-\ndation models bring to the experience for end-users interacting with these applications? Existing\ndesign frameworks for developing user-facing AI applications focus on augmenting (rather than\nreplacing) users’ abilities as described by Douglas Engelbart [Engelbart 1963] — we expect that\nthese frameworks should and will remain relevant for the development of future AI-infused appli-\ncations. For instance, maintaining users’ agency and reflecting their values will continue to be a\ncentral theme for foundation model-powered applications. Additionally, the benefits of allowing\nAI agents to take initiatives and automate users’ routines versus the benefits of waiting for users’\ndirect manipulation [Shneiderman and Maes 1997] will need to be carefully weighed [Horvitz\n1999]. Moreover, users’ values should be directly gathered and reflected through processes such\nas participatory [Lee et al. 2019] and value-sensitive design [Smith et al. 2020] that advocate for\nactively involving all stakeholders during the designing of the AI-infused applications.\nThese issues may become especially salient with foundation models because the model may\nbehave in ways that surprise and disappoint users and communities. Generative capabilities might\nexpose biases or points of view that are counter to the communities’ goals, or more insidiously,\n18https://beta.openai.com/docs/guides/classifications\n46 Center for Research on Foundation Models (CRFM)\ndraw on such associations in their behavior without the community being aware. This will place a\nlarge burden on the groups utilizing foundation models to monitor their models’ behavior, and to\nthe extent possible, adapt them to act in appropriate ways.\nWhile the design frameworks for thinking about AI-infused applications to augment users’\nabilities should remain the same, the actual forms of interactions that are attainable may dramatically\ndiversify due to foundation models’ powerful generative and multi-modal capacities. Already, early\ngenerations of what can be considered foundation model-powered software tools for multimedia\ncreation and editing have started to drive a new frontier that empowers even novice content\ncreators to generate high-quality multimedia from coarse, intuitive specifications (e.g., collaborative\nauthoring for writers [Lee et al. 2022], text-to-image generation for digital artists,19 mastering for\nmusicians,20 and code completion for programmers).21 Improved foundation models might enable\neven more ambitious tools (e.g., a fan might provide thematic material for a song which will then be\ngenerated in the style of their favorite band, or a business owner might provide simple descriptions\nof their product which will be used to create a full website). Moreover, foundation models will\nbe used to enrich static multimedia (e.g., automatically remastering legacy multimedia content\ninto new formats, or generating unique experiences for each player in new video games) and may\neven lead to new forms of multi-modal interactions using interfaces that themselves mix different\nmodalities, such as visual and gesture-based interaction.\nWe are starting to see glimpses of how foundation models might materialize into concrete\ninteractions in applications ranging from AI Dungeon22 to Microsoft PowerApps23 and CoPilot.24\nAs we start to envision new forms of interactions, it is of increasing importance for us to think\ncritically about the potential implications these interactions will have on individual users and\nsociety to maximize their positive impact. For example, how will foundation model-powered\napplications change the way we communicate with one another? Will a powerful model write\nemails in our stead and if so, how will this reshape people’s trust, credibility, and identity knowing\nthat the writers may not have written the emails themselves, and how will this alter our writing\nstyles [Hancock et al. 2020]? Who will own the authorship of the model-generated content and\nhow could the shifting responsibilities and ownership of the consent be misused [Weiner 2018]\n(see §5.5: economics for a more in-depth discussion)? What are the long-term implications that\nfoundation models will have on our work, language and culture [Hancock et al. 2020; Buschek et al.\n2021]? Of particular relevance to this last question is the fact that foundation models are trained on\nobserved data and do not necessarily inform us about causality. Hence, how can we ensure that\nthe use of foundation models leads us to a desired future and not a repetition of the past? Though\nthese issues are not necessarily unique to foundation models, they will be amplified and become\nmore prevalent as foundation models accelerate the creation of effective AI-infused applications.\n2.5.3 Blurring the line between developers and end-users.\nToday, the line that separates the developers of AI models and end-users is rigid — it is rarely the\ncase that an end-user has the data, computing resources, and expertise to be able to develop a new\nmodel that suits one’s values and needs well. While a generic model (i.e., one that is not specific to a\nspecific user or community) could be sufficient in some cases, recent years have seen an increasing\nnumber of scenarios in which such models fail to serve users. For instance, a text classification\n19https://github.com/nerdyrodent/VQGAN-CLIP\n20https://www.landr.com/\n21https://copilot.github.com/\n22https://play.aidungeon.io/main/home\n23https://powerapps.microsoft.com/en-us/\n24https://copilot.github.com/\nOn the Opportunities and Risks of Foundation Models 47\nmodel designed to identify problematic comments for one online community might work well for\nthat community but will fail in others whose norms and cultures may differ significantly (e.g., NSFW\ncommunities on Reddit might be more tolerant of certain content, while science communities might\nreject seemingly mundane anecdotes that are not based on scientific research) [Chandrasekharan\net al. 2018]. In another example, AI-powered sensors and robotics tools designed for one target\npopulation may fail without the ability to quickly adapt in-context for users with different abilities\nand needs [Karamcheti et al. 2021]. While recent work has presented promising avenues for future\nresearch on how end-users may be able to co-create AI models by manually providing models’\nparameters or datasets (e.g., WeBuildAI [Lee et al. 2019]), the results are still preliminary and often\nfocus on rudimentary models.\nIf foundation models can sufficiently lower the difficulty threshold for building AI-infused\napplications, they could present an important opportunity to more tightly couple users’ needs and\nvalues with the models’ behaviors by allowing users to actively partake in the development process\nof the models. Recent work has shown that GPT-3, for example, can robustly perform classification\ntasks in a few-shot or even in zero-shot fashion when given an adequate task description in its\nnatural language prompt [Brown et al. 2020]. An online community trying to moderate its own\ncontent might be able to leverage such a capability to create bespoke AI classifiers that filter content\nbased on classification task descriptions that the community has agreed on (of course, this power\ncould also be instead misused to silence the voices of certain members within the community — we\npoint to §5.2: misuse for further discussion on this topic). In addition, the powerful in-context\nlearning capabilities that foundation models will exhibit may allow foundation model-powered\napplications to more effectively optimize their interfaces on a per-user basis. This could open doors\nto tackling many salient problems in human-computer and robot interaction such as balancing the\npower of users’ direct manipulation and automation in mixed-autonomy settings.\nOf course, there will still be important challenges that we would need to overcome to truly\nrealize this potential for blurring the line between users and developers. These challenges include\nmitigating existing biases in foundation models, as well as making the models’ behavior more\nrobust and manageable even for non-ML experts (compared to ML experts, it could be even more\ndifficult for non-ML experts to understand the full capacities and mechanisms of foundation models,\nwhich can lead to unexpected pitfalls in the development cycle [Yang et al. 2018]). Future work\nshould explore how foundation models could be situated in the context of interactive machine\nlearning and study how we can support even those with limited experience with machine learning\nto leverage these models in a robust manner. Nonetheless, the ability for end-users to be involved in\ndeveloping AI-infused applications is an exciting opportunity that could introduce a new paradigm\nfor how we will interact with these applications in the future.\n48 Center for Research on Foundation Models (CRFM)\n2.6 Philosophy of understanding\nAuthors: Christopher Potts, Thomas Icard, Eva Portelance, Dallas Card, Kaitlyn Zhou, John Etchemendy\nWhat could a foundation model come to understand about the data it is trained on? An answer\nto this question would be extremely informative about the overall capacity of foundation models to\ncontribute to intelligent systems. In this section, we focus on the case of natural language, since\nlanguage use is a hallmark of human intelligence and central to the human experience.\nThe best foundation models at present can consume and produce language with striking fluency,\nbut they invariably lapse into the sort of incoherence that suggests they are merely “stochastic\nparrots” [Bender et al. 2021]. Are these lapses evidence of inherent limitations, or might future\nfoundation models truly come to understand the symbols they process?\nOur aim in this section is to clarify these questions, and to help structure debates around them.\nWe begin by explaining what we mean by foundation model , paying special attention to how\nfoundation models are trained, since the training regime delimits what information the model gets\nabout the world. We then address why it is important to clarify these questions for the further\ndevelopment of such models. Finally, we seek to clarify what we mean byunderstanding, addressing\nboth what understanding is (metaphysics) and how we might come to reliably determine whether\na model has achieved understanding (epistemology).\nUltimately, we conclude that skepticism about the capacity of future models to understand\nnatural language may be premature. It is by no means obvious that foundation models alone could\never achieve understanding, but neither do we know of definitive reasons to think they could not.\n2.6.1 What is a foundation model?\nThere is not a precise technical definition of foundation model . Rather, this is an informal label\nfor a large family of models, and this family of models is likely to grow and change over time in\nresponse to new research. This poses challenges to reasoning about their fundamental properties.\nHowever, there is arguably one defining characteristic shared by all foundation models: they are\nself-supervised. Our focus is on the case where self-supervision is the model’s only formal objective.\nIn self-supervision, the model’s sole objective is to learn abstract co-occurrence patterns in the\nsequences of symbols it was trained on. This task enables many of these models to generate plausible\nstrings of symbols as well. For example, many foundation models are structured so that one can\nprompt them with a sequence like “The sandwich contains peanut” and ask them to generate a\ncontinuation – say, “butter and jelly”. Other models are structured so that they are better at filling\nin gaps; you might prompt a model with “The sandwich contains __ and jelly” and expect it to\nfill in “peanut butter”. Both capabilities derive from these models’ ability to extract co-occurrence\npatterns from their training data.\nThere is no obvious sense in which this kind of self-supervision tells the model anything about\nwhat the symbols mean. The only information it is given directly is information about which words\ntend to co-occur with which other words. On the face of it, knowing that “The sandwich contains\npeanut” is likely to be continued with “butter and jelly” says nothing about what sandwiches are,\nwhat jelly is, how these objects will be combined, etc. This might seem to suggest an inherent\nlimitation on what a foundation model could achieve. However, we need not restrict the model to\nseeing only textual input. A foundation model might be trained on a wide range of different symbols:\nnot just language but also computer code, database files, images, audio, and sensor readings. As\nlong as it is just learning co-occurrence patterns of the sequences it is exposed to, then it counts as\na foundation model by our definition. As part of this learning, the model might come to represent\nstrong associations between a given piece of text and a particular sensor reading, or between a\nOn the Opportunities and Risks of Foundation Models 49\nsequence of pixel values and a database entry. These associations might reflect important aspects\nof the world we inhabit and the language we use to talk about it.\n2.6.2 What is at stake?\nBefore considering analyses of what understanding is, it is worth reflecting on why we might care\nabout the question of whether a foundation model could achieve it. These models are poised to be\ndeployed for numerous purposes with various functionalities. Some of our goals in deployment\nmay only be met to the extent that the model is capable of understanding. Here we list a few such\ngoals:\n•Trust: One might argue that we cannot trust a system’s linguistic behavior unless it un-\nderstands the language it is using. Of course, we currently trust engineered systems to do\nthings (e.g., manufacturing auto parts) without the question of understanding even arising,\nbut language might be special in this regard, since it is uniquely human. In addition, language\ncan be used to deceive and misrepresent, so understanding alone clearly does not imply trust.\nOn the whole, then, understanding might be taken as a necessary condition for trust in the\ncontext of language use.\n•Interpretability: If genuine natural language understanding in some way involves main-\ntaining and updating an internal model of the world (including, e.g., the speech context),\nand if we (as engineers) are able to analyze how linguistic input and output interface with\nthis internal model, that could afford substantial gains in interpretability, predictability, and\ncontrol of these systems.\n•Accountability: Not unrelated to the previous points, in the future we may find it desirable\nto hold artificial agents in some way accountable for the language they produce [The HAI\nAdaptive Agents Group 2021]. Depending on how we think about concepts like accountability,\nresponsibility, agency, and the like, language understanding may emerge as a prerequisite.\nThe mere possibility that understanding will play an indispensable role in any of these matters\nprovides strong motivation to develop a framework for theorizing about it.\n2.6.3 What is understanding?\nOur central question is whether a foundation model could come to understand a natural language.\nWith the above, we can now sharpen it: is self-supervision sufficient for understanding, keeping in\nmind that there are no constraints on the data used for this supervision? In order to address this\nquestion, we first need to define what we mean by understanding.\nAs a start, we find it helpful to make explicit a distinction that is sometimes conflated in discus-\nsions of the topic. The distinction is between themetaphysics and the epistemology of understanding.\nMetaphysics concerns what it would mean (“in principle”) for an agent to achieve understanding.\nEpistemology, by contrast, concerns how (“in practice”) we could ever come to know that an agent\nhas achieved the relevant type of understanding. In short, metaphysics is more about our ultimate\ntarget, whereas epistemology is more about how (if at all) we could know when we have reached it.\nOur epistemology thus depends to some extent on our metaphysics.\n50 Center for Research on Foundation Models (CRFM)\nMetaphysics of understanding. Philosophy of language offers a number of alternatives for\nwhat it is to understand natural language.25 Simplifying the landscape for the sake of brevity, the\nfollowing three broad classes of views all have connections with research lines in AI and NLP:26\n•Internalism: Language understanding amounts to retrieval of the right internal representa-\ntional structures in response to linguistic input. Thus, language understanding is not even a\npossibility without a rich internal conceptual repertoire of the right kind.\n•Referentialism: Roughly, an agent understands language when they are in a position to\nknow what it would take for different sentences in that language to be true (relative to a\ncontext). That is, words have referents and (declarative) utterances are truth-evaluable, and\nunderstanding involves a capacity to evaluate them relative to presentation of a situation or\nscenario.\n•Pragmatism: Understanding requires nothing in the way of internal representations or\ncomputations, and truth and reference are not fundamental. Rather, what matters is that the\nagent be disposed to use language in the right way. This might include dispositions toward\ninference or reasoning patterns, appropriate conversational moves, and so on. Crucially, the\nrelevant verbal abilities constitute understanding.27\nWhile this is a simplified picture of the space of possibilities, we already see how they relate in\nquite different ways to the goals mentioned above. On the pragmatist view, for instance, achiev-\ning language understanding does not imply anything about our ability to trust or interpret the\nsystem, insofar as it guarantees nothing about the agent’s internal structure or its relation to the\n(non-linguistic) world. On the internalist view, by contrast, a fairly robust kind of internal/causal\ninterpretability is at least strongly suggested. The question of whether or not a foundation model\ncould understand language in principle takes on a very different character depending on which of\nthese metaphysical characterizations we adopt.\nInternalism and referentialism can both be cast as defining a mapping problem: to associate a\nlinguistic sign with a “meaning” or a “semantic value”. For internalism this will be a representation or\nconcept, a program for computing a value, or some other type of internal object. For referentialism,\nit might be a mapping from a word to an external referent, or a mapping from a situation to a truth\nvalue (all relative to a context). Could self-supervision suffice for achieving the desired mapping\nin a foundation model? Here, the nature of the training examples might be relevant. If the model\nreceives only linguistic inputs, then its capacity to learn this mapping might be fundamentally\nlimited in ways that prevent it from learning to refer in the relevant sense. (Indeed, Merrill et al.\n[2021] identify some theoretical limits, albeit under very strong assumptions about what it means to\nlearn the meaning of a symbol.) However, if the input symbol streams include diverse digital traces\nof things in the world – images, audio, sensors, etc. – then the co-occurrence patterns might contain\nenough information for the model to induce high-fidelity proxies for the required mapping.28 For\n25Relatedly, there is a sizable literature in philosophy of science focused on the concept of understanding, mainly as it\nrelates to scientific explanation. See Grimm [2021].\n26We are leaving aside other questions that may be relevant to the metaphysics of understanding, such as whether or not\nconsciousness or some form of subjective experience may be necessary. These are pressing philosophical issues, but they\nare not easily connected to research in AI and NLP.\n27For an accessible introduction to internalist as well as referential views, we recommend Elbourne [2011]. This version\nof pragmatism arguably finds its roots in Wittgenstein [1953], but it is expressed most succinctly by Turing [1950], in which\nTuring suggests replacing the question of whether a machine can think with questions about a specific behavioral test\n(which came to be known as the Turing Test).\n28To the extent that the mapping embodies causal information, we must also contend with theoretical limitations\nconcerning the possibility of drawing causal inferences from correlational (or even experimental) data (see Spirtes et al.\n2001; Bareinboim et al. 2020).\nOn the Opportunities and Risks of Foundation Models 51\nreferentialism, there is still a further question of how these proxies relate to the actual world, but\nthe same question arises for human language users as well.\nBender and Koller [2020] give an interesting argument that combines referentialism with prag-\nmatism. They imagine an agent O that intercepts communications between two humans speaking\na natural language L. O inhabits a very different world from the humans and so does not have\nthe sort of experiences needed to ground the humans’ utterances in the ways that referentialism\ndemands. Nonetheless, O learns from the patterns in the humans’ utterances, to the point where O\ncan even successfully pretend to be one of the humans. Bender and Koller then seek to motivate the\nintuition that we can easily imagine situations in which O’s inability to ground L in the humans’\nworld will reveal itself, and that this will in turn reveal that O does not understand L. The guiding\nassumption seems to be that the complexity of the world is so great that no amount of textual\nexchange can fully cover it, and the gaps will eventually reveal themselves. In the terms we have\ndefined, the inability to refer is taken to entail that the agent is not in the right dispositional state\nfor understanding.\nFundamentally, the scenario Bender and Koller describe is one in which some crucial information\nfor understanding is taken to be missing, and a simple behavioral test reveals this. We can agree\nwith this assessment without concluding that foundation models are in general incapable of\nunderstanding. This again brings us back to the details of the training data involved. If we modify\nBender and Koller’s scenario so that the transmissions include digitally encoded images, audio, and\nsensor readings from the humans’ world, and O is capable of learning associations between these\ndigital traces and linguistic units, then we might be more optimistic – there might be a practical\nissue concerning O’s ability to get enough data to generalize, but perhaps not an in principle\nlimitation on what O can achieve.29\nWe tentatively conclude that there is no easy a priori reason to think that varieties of under-\nstanding falling under any of our three positions could not be learned in the relevant way. With\nthis possibility thus still open, we face the difficult epistemological challenge of clarifying how we\ncould hope to evaluate potential success.\nEpistemology of understanding. A positive feature of pragmatism is that, by identifying success\nwith the manifestation of concrete behaviors, there is no great conceptual puzzle about how to test\nfor it. We simply have to convince ourselves that our limited observations of the system’s behavior\nso far indicate a reliable disposition toward the more general class of behaviors that we took as our\ntarget. Of course, agreeing on appropriate targets is very difficult. When concrete proposals are\nmade, they are invariably met with objections, often after putative success is demonstrated.\nThe history of the Turing Test is instructive here: although numerous artificial agents have passed\nactual Turing Tests, none of them has been widely accepted as intelligent as a result. Similarly, in\nrecent years, a number of benchmark tasks within NLP have been proposed to evaluate specific\naspects of understanding (e.g., answering simple questions, performing commonsense reasoning).\nWhen systems surpass our estimates of human performance, the community’s response is generally\nthat the test was flawed, not that the target was reached. There may be some suite of behaviors\nthat is our real target, but it is just hard to circumscribe or turn into a practical test.30 Then again,\nthis might reveal that internalism or referentialsm are what we had in mind all along.\n29On our reading, Bender and Koller [2020] allow that multimodal data might change the scenario, especially if O is\nallowed to have cooperative interactions with the humans about shared scenarios and topics.\n30Part of the difficulty may also relate to the fact that typical humans make frequent errors in many of these domains,\nbut not necessarily the same types of errors that are made by current systems. Characterizing the target behaviours may\nthus involve more than just identifying the “correct” behaviour.\n52 Center for Research on Foundation Models (CRFM)\nIf we take internalism or referentialism as the ultimate target – our gold standard for what\nunderstanding is – then behavioral tests will always be at best imperfect as a means of assessing\nwhether understanding has been achieved. The imperfections are two-fold. First, behavioral tests\nwill always have gaps that could allow unsophisticated models to slip through. Second, a system\nmight have achieved the mapping that these views require, but we may be unable to show this with\nbehavioral testing. Recent experiences with the model GPT-3 show how challenging this might\nbecome: depending on the prompt one uses, one can see surprisingly coherent outputs or utter\nnonsense, and so prompt engineering requires deep expertise [Rong 2021].\nThus, both internalism and referentialism call for structural evaluation methods that allow us to\nstudy their internal representations, probing them for information [Tenney et al. 2019; Manning\net al. 2020], studying their internal dynamics [Sundararajan et al . 2017], and perhaps actively\nmanipulating them according to specific experimental protocols supporting causal inference [Vig\net al. 2020; Geiger et al. 2020]. There may be fundamental limitations on what we can learn from\npractical experiments about the inner workings of a complex foundation model, but it is clear that\nthese methods will be useful whenever our target aligns with internalism or referentialism.\n2.6.4 Moving the discussion forward.\nIt seems clear that there are no easy answers to the question of whether foundation models will\never understand language. To even begin to address the question, one must resolve a difficult\nmetaphysical question about which there are a number of substantively distinct views. The meta-\nphysical question then feeds into an epistemological question that poses many practical challenges.\nNonetheless, the above discussion does invite one practical conclusion: if foundation models are\npursued as a path to language understanding in artificial agents, then multimodal training regimes\nmay well be the most viable strategy, as they would seem the most likely to provide the model with\nthe requisite information. Whether self-supervision then suffices is a completely open question.\nOn the Opportunities and Risks of Foundation Models 53\n3 APPLICATIONS\nThe capabilities (§2: capabilities) of foundation models indicate that they have the potential\nto transform various sectors and industries, extending the role AI plays in society (§5: society).\nAmong the myriad applications where foundation models may be applied, we will focus on three\ndisciplines — healthcare (§3.1: healthcare), law (§3.2: law), and education (§3.2: law) — that are\nall foundational to societal function. Within each, we discuss the opportunities that foundation\nmodels pose for this domain alongside challenges (e.g., interpretability; §4.11: interpretability)\nand concerns (e.g., privacy; §4.7: security).\n54 Center for Research on Foundation Models (CRFM)\n3.1 Healthcare and biomedicine\nAuthors: Michihiro Yasunaga, Jing Huang, Camilo Ruiz, Yuhui Zhang, Giray Ogut, Saahil Jain, William\nWang, Yusuf Roohani, Hongyu Ren, Antoine Bosselut, Ehsan Adeli, Jure Leskovec, Russ Altman\nFig. 12. Foundation models in healthcare and biomedicine. We visualize an interactive framework where\nfoundation models enable various tasks across healthcare and biomedicine when trained on multimodal\ndata generated by various sources in the healthcare ecosystem. The first column lists several sources of data,\nincluding care providers, payers, institutions (universities, non-profits, and governments), pharma, wearables,\nand medical publications/forums. The second column shows several data modalities generated by the data\nsources. They include images (e.g., chest X-rays), videos (such as ultrasounds), graphs of chemical compounds,\ntables for electronic health records (EHRs), text such as clinical notes, time series such as ECGs, and genetic\ndata. The third column visualizes a foundation model trained on such data and then applied to healthcare\nand biomedicine downstream tasks listed in the fourth column. This process can generate new data that will\nfurther improve the foundation model, hence the bidirectional relation between the foundation models and\nthe tasks.\nHealthcare and biomedicine are an enormous application area in society, for instance, with\nexpenditures accounting for 17% of gross domestic product (GDP) in the US [Swensen et al. 2011;\nvan Hartskamp et al. 2019; Keehan et al. 2020]. Both healthcare (which focuses on the delivery of\ncare to patients via diagnosis, treatment, and health administration) and biomedical research (which\nfocuses on the scientific understanding of disease and the discovery of new therapies) demand\nsignificant expenses, time, and comprehensive medical knowledge [Yu et al. 2018; Korngiebel and\nMooney 2021]. We envision that foundation models can be a central storage of medical knowledge\nthat is trained on diverse sources/modalities of data in medicine [Krumholz et al. 2016; Soltanian-\nZadeh 2019; Suresh et al. 2020] (Figure 12 left), and can be queried/updated interactively by medical\nprofessionals (e.g., healthcare providers and biomedical researchers access published findings and\nupload new publications) [Ionescu et al. 2020] and queried by the public. As foundation models\nhave strong adaptation capabilities (e.g., fine-tuning, prompting [Brown et al . 2020]), they can\nbe efficiently adapted to various individual tasks in healthcare and biomedicine (e.g., question\nOn the Opportunities and Risks of Foundation Models 55\nanswering app used by patients [Klasnja and Pratt 2012; Zhu et al. 2019; Daniel et al. 2019; Liu et al.\n2020a], clinical trial matching system [Ni et al. 2015; Harrer et al. 2019; Beck et al. 2020] accessed by\nresearchers and patients; Figure 12 right). This way, foundation models can be a central interface\nthat supports various interactions between data, tasks, and people in healthcare and biomedicine,\nthereby advancing the efficiency and accuracy of healthcare/biomedical applications [Elbattah et al.\n2021]. We elaborate these opportunities in §3.1.1: healthcare-tasks and §3.1.2: biomed-tasks.\nAt the same time, healthcare/biomedical applications pose unique challenges that motivate fur-\nther research in foundation models, such as integrating multimodal data in healthcare/biomedicine\n[Miura et al. 2021; Liu et al. 2021a] and observing ethical and legal regulations in medicine (pri-\nvacy, safety and explainability) [Guan 2019; Xu et al . 2019]. We elaborate these challenges in\n§3.1.3: healthcare-biomed-challenge.\n3.1.1 Opportunities in healthcare.\nFoundation models may improve the delivery of care to patients through healthcare providers and\nhospitals. Currently, healthcare cost increases every year [Keehan et al. 2020], and studies estimate\nthat 30% of healthcare spending may be wasteful due to administrative inefficiency and preventable\nmedical errors [Kocher 2021]. Moreover, as the demand for healthcare increases, the society faces a\nserious shortage in healthcare providers [Kirch and Petelle 2017]. This inefficiency and shortage in\nhealthcare necessitate developing fast and accurate interfaces for healthcare providers and patients,\nsuch as automated aid systems for diagnosis/treatment, summarization of patient records, and\nanswering of patient questions [Davenport and Kalakota 2019; Nie et al. 2018; Wang et al. 2021b]. In\nparticular, in an urgent pandemic crisis such as COVID-19, fast diagnosis/screening (e.g., automatic\nanalysis of chest X-ray images) as well as automated question answering for patients (e.g., symptom\nchecking and care) and the public (e.g., disease prevention) are vital to reduce the spread of diseases\nand allocate healthcare resources for critical patients, saving more lives [Lalmuanawma et al. 2020].\nAs foundation models have a strong capability to serve as an integrated knowledge reservoir,\nthey can be queried and adapted to various individual tasks in healthcare. Below are examples of\nimportant tasks in healthcare that would benefit from foundation models.\nInterface for healthcare providers. Foundation models can improve the efficiency and accuracy\nof care by providers. Healthcare providers spend unnecessary time editing electronic heath records\n(EHRs) [Kocher 2021], and preventable medical errors (e.g., hospital readmissions, surgical errors)\ncause wastes in healthcare [Shrank et al. 2019; Shah et al. 2020]. Foundation models can be adapted\nas an efficient and accurate interface into EHRs (clinical notes, lab value histories and imaging files)\n[Li et al. 2020c; Steinberg et al. 2021; Percha 2021], helping healthcare providers create summaries\nof patient visitation [Krishna et al. 2020], retrieving relevant cases and literature, and suggesting\nlab tests, diagnosis, treatments and discharges [Zhang et al. 2019b; Rasmy et al. 2021]. Foundation\nmodels can also be adapted to help a surgical robot monitor and achieve accurate surgeries [Diana\nand Marescaux 2015; Agrigoroaie and Tapus 2016; Yu et al . 2019]. See §2.3: robotics for more\ndiscussions on foundation models for robotics.\nInterface for patients. Foundation models can be adapted to serve as an interface to patients, pro-\nviding relevant information about clinical appointments [Bates 2019], answering patient questions\nrelated to preventive care [Demner-Fushman et al. 2020], along with relevant medical explana-\ntory information (e.g., text and graphics that explain conditions) [Chaix et al. 2019], and helping\nassistive-care robots for patients [Jeong et al. 2015; Abdi et al. 2018]. See §2.5: interaction for\nmore discussion on foundation models for user interaction. Foundation models can also serve as\nan interface with the general public to answer questions related to public health and pandemic\nprevention (such as the COVID-19 case) [Bharti et al. 2020; Herriman et al. 2020]. At the same time,\n56 Center for Research on Foundation Models (CRFM)\nwe note that the interface must guarantee factual accuracy to ensure public trust in medical advice\n[Kreps and Kriner 2020] (see §3.1.3: healthcare-biomed-challenge).\n3.1.2 Opportunities in biomedicine.\nFoundation models may facilitate biomedical research such as discovery of drugs and understanding\nof diseases, which ultimately translates to improved healthcare solutions [Hanney et al . 2015].\nCurrently, biomedical discovery requires significant human resources, experimental time and\nfinancial costs. For instance, drug development involves a complex process, from basic drug research\nof protein target identification and potent molecule discovery to clinical development (e.g., clinical\ntrials) to the final drug approval, which typically takes over 10 years and costs more than one billion\ndollars [Wouters et al. 2020]. Facilitating and accelerating biomedical discovery using existing data\nand published findings is an imperative problem in biomedicine [Yu et al. 2018]. In particular, a novel\ndisease outbreak such as COVID-19 costs millions of lives and trillions of dollars [Lalmuanawma\net al. 2020; McKibbin et al. 2020]; if we can speed up drug development for new diseases, that would\nbe very helpful. Foundation models can be particularly helpful for biomedical discovery in two\naspects. First, foundation models have a strong generative capability (e.g., coherent text generation\nin GPT-3), which can help generative tasks in biomedical research such as generating experimental\nprotocols (clinical trials) and designing molecules that work (drug discovery) given existing data\n[Kadurin et al. 2017; Harrer et al. 2019]. Second, foundation models have a potential to integrate\ndiverse data modalities in medicine, which enables investigating biomedical concepts (e.g., disease)\nfrom multiple scales (using molecule-, patient- and population-level data) and multiple knowledge\nsources (using imaging, textual and chemical descriptions). This facilitates biomedical discoveries\nthat are difficult to obtain if using single-modality data [Lanckriet et al . 2004; Aerts et al. 2006;\nKong et al. 2011; Ribeiro et al. 2012; Wang et al. 2014, 2015c; Ruiz et al . 2020; Wu et al. 2021h].\nFoundation models also enable transfer knowledge across modalities. Lu et al. [2021a] showed how\na transformer model trained on natural language (a data-rich modality) could be adapted for other\nsequence-based tasks such as protein fold prediction, which is a long-studied predictive task in\nbiomedicine [Jumper et al. 2020]. Below are examples of important tasks in biomedicine that will\nbenefit from foundation models.\nDrug discovery. To discover a drug or a therapeutic that treats a disease, researchers must first\nidentify a target (e.g., proteins, genes, RNA causally implicated in the disease) and must then search\nfor molecules (e.g., chemical compounds, antibodies) that bind to the target and treat the disease.\nTypically, identifying the appropriate target and generating a corresponding molecule requires\nyears of expensive wet lab experiments [Hughes et al. 2011; Schenone et al. 2013; Schneider 2018].\nFoundation models’ generativity can improve the search space and efficiency (see §2.4:reasoning),\nwhich not only reduces the amount of experiments but also helps to discover new and better\ndrugs [Jin et al . 2018; You et al. 2018; Walters and Barzilay 2020; Stokes et al . 2020]. Moreover,\nthe simultaneous solution of related drug discovery problems (i.e., target identification, efficacy\nprediction, side effect prediction, and others) by a single foundation model may improve the\nsolutions to each of them [Ramsundar et al. 2015; Camacho et al. 2018; Duran-Frigola et al. 2020;\nHuang et al. 2021a]. As an example, one area where foundation models have shown significant\npotential for impacting therapeutic design is the modeling of proteins using language models.\nSuccessful applications range from predicting viral mutations that can escape a vaccine-induced\nimmune response to predicting protein docking potential for better design of therapeutic antibodies\n[Bepler and Berger 2021; Hie et al. 2021; Tsaban et al. 2021; Wu et al. 2021b; Rives et al. 2021].\nPersonalized medicine. Personalized medicine aims to select the optimal treatment for individual\npatients based on their health history, genetics, imaging, and other personal measurements [Collins\nOn the Opportunities and Risks of Foundation Models 57\nand Varmus 2015; Ashley 2016]. For instance, given a set of drugs and a patient genome, foundation\nmodels may help predict which drug is likeliest to treat the patient with minimal side effects\n[Whirl-Carrillo et al. 2012; Tatonetti et al. 2012; Gerstung et al. 2017; Grinfeld et al. 2018; Adam et al.\n2020]. Foundation models are uniquely powerful in their ability to integrate multimodal patient\ndata ranging from the EHR [Rajkomar et al. 2018] to medical imaging [Bera et al. 2019; Ouyang\net al. 2020] to drug and molecular measurements [Gottlieb et al. 2011; Ruiz et al. 2020] to make an\noptimal prediction.\nClinical trials. Clinical trials study efficacy and safety of treatment or drug candidates. Conven-\ntional clinical trials are inefficient and costly:80% of trials fail due to inability to show efficacy/safety\nor problems with patient matching [Ali et al. 2020; Liu et al. 2021c]. Foundation models can help in\nthe following: predicting potential failures and design promising clinical trial protocols (e.g., patient\neligibility criteria) based on existing studies; and automating matching of eligible patients based on\npatient individual profiles, which are multimodal data including EHRs, gene sequence, etc. [Harrer\net al. 2019].\n3.1.3 Challenges and future research in foundation models.\nWhile there are potential opportunities for foundation models to help, healthcare/biomedical\napplications also pose unique challenges that motivate further research in foundation models.\nMultimodality. Medical data are highly multimodal, with various data types (text, image, video,\ndatabase, molecule), scales (molecule, gene, cell, tissue, patient, population) [Kong et al . 2011;\nRuiz et al. 2020], and styles (professional and lay language) [Lavertu and Altman 2019; Li et al .\n2019]. Current self-supervised models are developed for each modality (e.g., text [Lee et al. 2020b],\nimage [Chaitanya et al. 2020], gene [Ji et al. 2021], protein [Jumper et al. 2020]), and do not jointly\nlearn from diverse modalities. To learn the inter-modality and cross-modality information from\nthese diverse multimodal medical data, we need to investigate both feature-level and semantic-level\nfusion strategies in the training of foundation models. If done effectively, this has a potential to\nunify biomedical knowledge and facilitate discoveries as discussed in §3.1.2: biomed-tasks.\nExplainability. Explainability — providing evidence and logical steps for decision making — is\ncrucial in healthcare and biomedicine [Holzinger et al. 2019], and is made obligatory under the\nGeneral Data Protection Regulation (GDPR). For instance, in diagnosis and clinical trials, patient\nsymptoms and temporal relevance must be explained as evidence. This helps the resolution of\npotential disagreement between the system and human experts. Explainability is also needed for\ninformed consent in healthcare [Amann et al. 2020]. However, current foundation models’ training\nobjectives do not include explainability, requiring future research in this direction [Linardatos et al.\n2021]. Incorporation of knowledge graphs may be a step to further improve model explainability\n[Roberts et al. 2020; Xu et al. 2020; Jin et al. 2021]. Readers are refered to §4.11: interpretability\nfor more discussion on explainability.\nLegal and ethical regulations. Healthcare applications must observe legal and ethical regulations\nwith guarantees, such as patient safety, privacy and fairness. For instance, regarding safety, predic-\ntions made by foundation models must be factually accurate with established medical knowledge,\nand must quantify uncertainty or choose to defer to an expert when uncertain [Challen et al. 2019;\nMozannar and Sontag 2020]. For privacy, the use of patient health records must observe the privacy\nlaws, such as HIPAA [Act 1996] in the case of the US. Federated learning is one potential solution to\nkeeping the raw, sensitive data private in the training of foundation models [Chamikara et al. 2021].\nFor fairness, researchers will need to be mindful of common pitfalls or otherwise risk exacerbating\nexisting social inequalities [Chen et al . 2019; Wiens et al . 2019; Chen et al . 2020b]. They must\n58 Center for Research on Foundation Models (CRFM)\nensure that the training and evaluation data for foundation models is sufficiently representative of\ndifferent sexes, races, ethnicities and socioeconomic backgrounds; an area where medical datasets\nand clinical trials have had a long history of bias [Martinez-Martin et al. 2020; Kaushal et al. 2020].\nResearch is also needed to debias and regularize models to ensure fairness when representative\ndata is scarce [Zhao et al. 2020a]. Foundation model developers also need to consult with ethics\nand law researchers, and observe regulations in the specific circumstances (e.g., country, region)\nwhere they are deployed. We also refer readers to §4.7: security, §4.8: robustness, §5.1: fairness,\n§5.4: legality for details on privacy, robustness, fairness and legality.\nExtrapolation. The process of biomedical discovery involves extrapolation. For instance, founda-\ntion models must be able to quickly adapt to new experimental technologies (e.g., new assays, new\nimaging techniques such as high resolution microscopy) or new settings (e.g., new target diseases\nsuch as COVID-19) [Jaroch et al. 2018; Benam et al. 2019]. The ability to leverage existing datasets\nand extrapolate to new settings is a key machine learning challenge in biomedicine [Snell et al .\n2017; Ma et al. 2021b]. While GPT-3 exhibits some extrapolation behaviors (e.g., generating new\ntext not seen before), its mechanism is unclear and still in its infancy. Further research is needed\nfor improving the extrapolation capability of foundation models, especially when considering the\ndiverse range of data modalities and tasks that is inherent to healthcare and biomedicine but is not\ncommonly studied in current GPT-3 and related models. Also see §4.8: robustness.\nOn the Opportunities and Risks of Foundation Models 59\n3.2 Law\nAuthors: Peter Henderson, Lucia Zheng, Jenny Hong, Neel Guha, Mark Krass, Julian Nyarko, Daniel E.\nHo\nFig. 13. An example of various steps of a civil case in the United States and where foundation models might\nhelp. At each stage different modalities might need to be processed and adaptation is needed to a new court\nor legal perspective.\nFrom family court to criminal justice and from environmental policy to corporate transactions,\nthe reach of the law is vast. In the United States, 31 there are over 1.3M lawyers [American Bar\nAssociation 2021] and annual revenues for legal services exceed $300B [MarketLine 2021]. Yet\n“access to justice” remains far out of reach for most. Legal services can be prohibitively expensive.\nRoughly 86% of low-income individuals with civil legal problems in the United States, for instance,\nreport receiving inadequate or no legal help [Legal Services Corporation 2017]. Even when counsel is\nappointed, lawyers might be strained by increasingly large caseloads. Studies have shown that public\ndefenders, for example, are often overworked and underfunded [Lefstein and Spagenberg 2009;\nSchumm 2012; American Bar Association 2004]. The U.S. Department of Justice reported that in 2007,\n73% of county-based public defender offices exceeded the maximum recommended limit of cases\nreceived per attorney and 15 of 19 reporting state public defender programs exceeded the maximum\nrecommended limit of felony or misdemeanor cases per attorney [Farole and Langston 2010;\nLangston and Farole 2010]. Even in a country with one of the highest per capita rates of attorneys,\njustice can appear out of reach. U.S. President Jimmy Carter once opined, “Ninety percent of our\nlawyers serve ten percent of our people. We are overlawyered and underrepresented” [Carter 1978].\nAccording to a leading voice in access to justice, technology may provide a path forward [Rhode\n2014], a view echoed by many others [Cabral et al. 2012].\nWhat role might foundation models play in the law?32 A major promise is that foundation models\ncan improve access to justice and government services by leveling procedural and financial barriers\nto legal services. The challenges posed by legal applications can, in turn, motivate basic research\nquestions for foundation models. Many legal applications pose unique challenges to computational\nsolutions. Legal language is specialized and legal outcomes often rely on the application of ambigu-\nous and unclear standards to varied and previously unseen fact patterns. At the same time, due to\nits high costs, labeled training data is scarce. Depending on the specific task, these idiosyncrasies\n31We restrict our discussion to legal applications in the United States because of the expertise of the authors. Some\ndiscussion here may apply to legal venues globally, however.\n32We note that for the purposes of this section we consider foundation models to be any self-supervised pretrained model\nthat is used to quickly adapt to new contexts with little supervised learning. See also the discussion in §1: introduction\nand §2.6: philosophy for an expanded definition.\n60 Center for Research on Foundation Models (CRFM)\ncan pose insurmountable obstacles to the successful deployment of traditional models. In contrast,\ntheir flexibility and capability to learn from few examples suggest that foundation models could be\nuniquely positioned to address the aforementioned challenges.\nThroughout this section, foundation models may take as context many modalities as evidence:\naudio during trial proceedings, video and images during discovery, and text in conducting legal\nresearch. Yet, the majority of legal tasks in which reliance on foundation models will be beneficial\ninvolve text-based inputs and outputs. As such, we mainly focus on text-based domains while only\nbriefly discussing others. To ground the discussion, Figure 13 describes the stages of a civil lawsuit\nin the United States and where foundation models might come into play in this process. Figure 14\nshows the logic flow required to generate just part of one paragraph of a legal brief, which might\nserve as a concrete example of a task that foundation models might one day be used for.\nAn Important Consideration. Before proceeding, we note that the ethical, legal, and fairness\nconsiderations expanded on in §5.6: ethics, §5.4: legality, and §5.1: fairness are particularly\nimportant to examine before using foundation models in an applied legal or government context,\nas these applications often have important, real-world consequences to those affected [Surden\n2020]. Foundation models must also be thoroughly scrutinized before deployment, as discussed in\n§4.4: evaluation. For example, the legal system places particular emphasis on — and may even\nmandate — transparency, accountability, and explainability. Consequently, it is questionable whether\ncurrent models are positioned to solve many of the most pressing, legal problems. Nonetheless, the\nneed to expand and improve access to legal and government services provides a worthy goal for\nfoundation models.\n3.2.1 Opportunities in law.\nLegal applications can range from the use of machine learning in government contexts [Engstrom\net al. 2020; Coglianese and Ben Dor 2020; Re and Solow-Niederman 2019] to aiding lawyers in their\nprovision of legal services [Zheng et al. 2021; Huang et al. 2021b; Ostendorff et al. 2021; Vold and\nConrad 2021]. We note that prior work has also surveyed machine learning-assisted legal tasks in\ntext-based domains [Zhong et al. 2020; Chalkidis et al. 2020], although it has been noted that recent\nlegal AI research has focused on geographic regions outside of the U.S. [Zheng et al. 2021]. While\nmany of the topics we discuss here may be applicable to different legal systems, due to the expertise\nof our team we focus primarily on the U.S. In particular, we concentrate on three broad categories\nof legal applications that may benefit from foundation models in the U.S. legal system: private law\nor civil justice (claims between private individuals, arising out of, for instance, contracts, property\nor torts), criminal law (i.e., the prosecution of individuals for criminal behavior), and (non-criminal)\npublic law (e.g., the regulation of private behavior by government agencies).\nCivil law. In U.S. civil proceedings, parties must typically find and pay attorneys to be represented.\nAs a result, many individuals, especially those with low income, struggle to secure adequate legal\nrepresentation [Rhode 2004]. Foundation models have the potential to improve access to justice by\nreducing the cost, improving the quality, and extending the reach of legal services. In Figure 13, we\ndescribe the process by which a civil lawsuit is filed in a U.S. court and where foundation models\nmay play a role in aiding both attorneys and judges.\nEven before an attorney is involved in the legal process, clients may benefit from the deployment\nof foundation models. Recent work has used machine learning models to identify the relevant legal\nissues contained in a plain-language description of facts presented by a client.33 Tools like these\ncan help provide a recommendation for the type of legal action needed to address the issue at hand\nor to recommend a specialized attorney. A number of other similar efforts have sought to increase\n33https://spot.suffolklitlab.org/\nOn the Opportunities and Risks of Foundation Models 61\naccess to justice by providing information tailored to a client’s particular needs [Cabral et al. 2012;\nBrescia et al. 2014; Queudot et al. 2020; Westermann et al. 2019].\nOnce a client speaks with an attorney, prior to civil litigation, the attorney may seek to avoid a\ncostly trial. At this stage, they can rely on foundation models to evaluate contracts, review terms\nof service, find relevant patents, and conduct other pre-litigation processes in order to ensure\nthat their clients are at an advantage [Betts and Jaep 2017; Elwany et al . 2019; Lippi et al. 2019;\nLee and Hsiang 2019; Hendrycks et al . 2021c; Hegel et al. 2021]. Notably, recent work has both\ndescribed the challenges and benefits of using foundation models for contract review [Leivaditi\net al. 2020; Hegel et al. 2021; Hendrycks et al. 2021c]. In addition to reviewing and drafting legal\ndocuments, client interactions and documents can be translated to reduce costs and barriers to the\nprovision of legal services [Cuéllar 2019]. But translation of legal documents requires precision\nand an understanding of highly technical language, which makes collecting training data costly.\nAdditionally, translating client statements or trial proceedings often requires an understanding of\nlocal dialects and language. This, too, makes it difficult to collect enough ground truth translation\ndata to train on. As a result, traditional supervised methods rarely achieve the level of accuracy\nrequired in the legal domain [Vieira et al. 2020]. Foundation models may improve performance in\nthis area over fully supervised mechanisms by adapting quickly in these low-resource contexts.\nDuring litigation, foundation models can help lawyers to conduct legal research, draft legal lan-\nguage, or assess how judges evaluate their claims [Zheng et al. 2021; Huang et al. 2021b; Ostendorff\net al. 2021; Vold and Conrad 2021; Chalkidis et al. 2020, 2019]. This could potentially reduce the\ncosts of and improve legal services. For example, recent work has utilized pretrained models for the\nrecommendation of relevant citations and holding statements when writing legal texts [Zheng et al.\n2021; Huang et al. 2021b; Ostendorff et al. 2021]. Other work uses pretrained models for improved\nlegal question answering to power commonly used legal search engines and help lawyers conduct\nlegal research [Vold and Conrad 2021]. A wide variety of work has also examined automated\ncontract drafting and review, a task that could similarly benefit from foundation models [Hendrycks\net al. 2021c; Betts and Jaep 2017]. Perhaps most compelling, foundation models may help assist\nlawyers generate legal briefs (written arguments). The models might find novel arguments or\nidentify problems in attorney-written portions of the brief. For example, Tippett et al. [2021] predict\nthe outcome of a legal proceeding based on features extracted from the filed briefs. Foundation\nmodels can be leveraged to use raw language as inputs rather than extracted features. This might\nprovide attorneys with more informative recommendations as to how their brief could be improved\nto ensure a favorable outcome.\nAfter opening and reply briefs are filed, parties then begin the discovery process, which has\nalready used simple machine learning models for the better part of a decade [Grossman and\nCormack 2010]. Attorneys use these systems to label whether a document should be produced to\nthe opposing party. The documents are multi-modal in nature, often containing video, images, audio,\nand text. Current systems are costly because they used supervised learning and active learning\nto label the documents as responsive [Grossman and Cormack 2010; Oard et al. 2018; Yang et al.\n2021]. Instead, few-shot or zero-shot document retrieval capabilities that might be possible with\nfoundation models would help ease concerns about the large costs of the current process.34 To avoid\nthe possibilities of gamesmanship in the discovery process, Cui [2018] has proposed a zero-shot\n(or few-shot) adaptation process that can only be operationalized through the use of foundation\nmodels.\nAfter discovery, once the trial begins, foundation models could help parties prepare for trial\nby predicting what the judge might focus on during questioning [Dickinson 2018], adapting to\n34https://www.kirkland.com/publications/article/2020/04/technology-assisted-review-framework\n62 Center for Research on Foundation Models (CRFM)\nthe current context from judges’ prior published opinions. In the courtroom, foundation models\nmight be used to examine audio and video of courtroom proceedings to determine if outcomes\nwere biased against the defendant because of their race or dialect.35\nOnce the trial concludes, foundation models could help judges and law clerks to properly evaluate\nlegal claims from both parties using similar technologies, or the use of contextual embeddings from\nfoundation models might assist in statutory interpretation [Nyarko and Sanga 2020; Choi 2020].\nRecent work (without reliance on foundation models or NLP) has examined whether an appeals\ndecision can be predicted from a set of extracted features, like citation counts and the appearance\nof key words [Katz et al. 2017; Boniol et al. 2020]. It is possible that such models could be improved\nusing foundation models and applied to help judges draft decisions by flagging obvious mistakes in\ntheir opinion, as has been discussed in the context of adjudicative agencies [Engstrom et al. 2020;\nRay and Lubbers 2014]. They can also be used to identify racial biases in legal opinions and help\njudges revise their opinions accordingly [Rice et al. 2019].\nCriminal law. One particularly contentious area has been the use of risk scores in government\nsettings, particularly in criminal law. Some may want to use language-based foundation models\nto aid in making charging decisions or parole decisions based on a given text-based narrative of\nthe events. Careful consideration must be taken before using foundation models for risk scoring\ndue to the potential for biases, especially when language data is included [Bender et al. 2021; Berk\net al. 2021; Laufer 2020]. But foundation models may play a role in many other dimensions of\ncriminal justice. The same tools as in civil litigation, above, can also be used by prosecutors and\ndefense attorneys. This can help appointed attorneys perform their job more efficiently and reduce\nunnecessary overhead. As a result, they may be able to balance already heavy caseloads more\neffectively. For example, public defenders are often viewed as being overworked and underfunded,\nwhich would lead to avoidable procedural errors.36 Foundation models can help reduce some of\nthese resource constraints by identifying errors and automating simple tasks. However, they are\nnot a solution on their own.\nIn other areas, foundation models can act as an oversight mechanism to reduce structural\ninequities. Pretrained models have been used for processing parole hearing transcripts to find\ninstances of anomalous outcomes [Bell et al. 2021]. Recent work has also removed linguistic cues\nfor a suspect’s race in police reports to promote race-blind charging decisions and avoid racially\nbiased prosecutions [Chohlas-Wood et al. 2020]. Other work has helped identify disrespectful police\ncommunications [Voigt et al. 2017]. In these contexts, it is very costly to label data since annotators\nmust be given access to sensitive data and appropriate background checks are often required. To\nreduce these costs, foundation models can be used to pretrain and adapt quickly to downstream\ntasks where labels are scarce.\nPublic law. Government agencies regulate vast parts of society, and foundation models have wide\npotential applicability across public law. This includes: analyzing public comments in the notice-\nand-comment process, assisting patent examination, retrieving relevant documents in response to\nFreedom of Information Act requests, aiding in mass adjudication, among many others. Recent\nwork has surveyed these government applications in a variety of contexts and we refer the reader\nto the relevant sources for in-depth discussion [Engstrom et al. 2020; Coglianese and Ben Dor 2020].\nIn many of these applications, foundation models can improve the quality, efficiency, utility, and\n35For example, speaking African-American Vernacular English dialects in the courtroom has been shown as a potential\nsource of bias during trial. https://www.nytimes.com/2019/01/25/us/black-dialect-courtrooms.html\n36See, for example, in People v. Superior Court (Vasquez) , 27 Cal.App.5th 36 (2018) a defendant did not receive a trial for\n17 years because the public defender’s office had severe budget cuts and understaffing. The court ruled that the systemic\nbreakdown in the public defender’s office constituted a due process violation and the defendant’s case was dismissed.\nOn the Opportunities and Risks of Foundation Models 63\naccessibility of government services: labels are scarce, resources are constrained, and contexts are\nconstantly shifting. As such, the adaptability and flexibility of foundation models are often required\nto improve efficiency and performance. To give an illustrative example of just one such application,\nexisting work has leveraged NLP for facilitative moderation in public comment forums. In this\nuse case, predictive models help lay-users improve arguments and identify misstatements in their\ncomments. Such a system has already been deployed in the U.S. Department of Transportation\nrulemaking process [Park et al. 2012], although it can likely be improved through the linguistic\nreasoning capabilities of foundation models. But government agencies must comply with constitu-\ntional, statutory, and administrative obligations (see §5.4: legality), so additional care is needed in\nthese settings.\n3.2.2 How can foundation models uniquely help?\nThe above examples of legal applications are unique in several ways. First, the cost of annotating\ndata is very high. Often, the expertise to create high-quality labels can only be found in attorneys,\nwho may charge hundreds of dollars per hour. Even after labels are obtained, certain data may be\nsensitive and cannot be pooled together to training a large language model. Given recent progress\nin few-shot learning [Brown et al. 2020], foundation models are among the most promising paths\nfor learning models with limited annotations.\nSecond, legal decision-making requires context at various scales: knowledge of all historical\ndecisions and standards, knowledge of the case law that remains relevant in the present, and\nknowledge of the nuances of the individual case at hand. Foundation models are uniquely poised to\nhave the potential to learn shared representations of historical and legal contexts, as well as have\nthe linguistic power and precision for modeling an individual case.\n3.2.3 What are foundation models lacking that requires more research?\nTo illustrate the deficiencies current foundation models need to overcome in order to be realistically\ndeployed, we consider as an example the automatic creation of a legal brief to submit to a court.\nA brief lays out the arguments to a judge before a hearing. Once a party has filed an opening\nbrief, the opposing party files a response. The judge then evaluates the briefs and asks questions of\nboth parties at a hearing before making a decision. Figure 14 visualizes the structure of such a legal\nbrief with some of its characteristic features.\nAn automated brief generation mechanism might take as context relevant documents and facts\nof a case (as specified by an attorney) as well as a rough sketch of the desired outcome. It would\nthen generate a legal brief with complex legal arguments to submit to the court.\nLong Documents and Narratives. To achieve this goal, the model must be able to read long contexts\nand produce long narratives. Legal documents tend to be far longer than documents in any other\ncontext. The average U.S. Supreme Court opinion contains around 4,700 words,37 a brief on the\nmerits to the Supreme Court can have as many as 15,000 words,38 a law review article often contains\n20,000 to 30,000 words,39 parole transcripts can be hundreds of pages long [Bell et al. 2021], and\ntrial records can be even longer. Current foundation models have struggled with such long contexts\nand outputs (see §4.1: modeling for more discussion).\nRetrieval, Concept Drift, Argument Formation, and Logical Reasoning. In addition to reading case-\nspecific documents, the foundation model must retrieve the relevant case law and understand which\n37https://www.americanbar.org/groups/public_education/publications/teaching-legal-docs/how-to-read-a-u-s–\nsupreme-court-opinion/\n38https://www.supremecourt.gov/casehand/courtspecchart02162010.aspx\n39https://www.stanfordlawreview.org/submissions/article-submissions/\n64 Center for Research on Foundation Models (CRFM)\nFig. 14. An extract from a fictional brief written by one of the authors of this work. The prototypical form\nthat law students are instructed to write a brief involves: (1) introducing the argument; (2) stating the legal\nrule in a persuasive manner; (3) applying the legal rule to the facts of the case; (4) persuasively concluding\nthe argument. This often involves information retrieval and paraphrasing from both prior cases and the facts\nof the current case.\ncase law is still valid and which has been overruled, taking into account potential concept drift since\nit was trained. More work in editing grounded information in foundation models will be required\nas case law evolves [De Cao et al. 2021]. Using retrieved legal standards, the foundation model must\nthen understand how to weave them into a persuasive argument. Emerging research has studied\nways of using foundation models to measure, detect, and generate persuasive texts [Duerr and\nGloor 2021; Li et al. 2020a; Longpre et al. 2019], which may provide stepping stones towards this\ngoal.\nBut legal briefing also requires the ability to identify a relevant legal rule and determine how it\napplies to a new situation. For example, Holzenberger et al. [2020] provide a dataset and methodology\nfor evaluating this ability in current models based on the tax code. Given a set of tax code rules\nand a description of someone’s situation, the model must determine how the rules apply to the\nindividual in order to determine what their tax liability is.40 Holzenberger et al. [2020] found that\ndomain experts can express the relevant tax law as a prolog solver to determine an individual’s tax\nliability with perfect accuracy. However, they found that simply using a BERT model, even when\npretrained on in-domain legal text, was not sufficient to allow the model to deduce and apply the\nrelevant rules, a process also referred to as “legal reasoning. ”\n40For example, a rule might look like, “In the case of an individual who is not married and is not a surviving spouse,\nparagraphs (1) and (2) shall be applied by substituting ‘$750’ for ‘$600’. ” US Internal Revenue Code (IRC) §63(f )(3).\nOn the Opportunities and Risks of Foundation Models 65\nThese challenges can further be illustrated through a real exchange with GPT-3, demonstrat-\ning that current models are unable to perform even comparatively simple tasks involving legal\nreasoning.\nLegal Reasoning with GPT-3.\n“Liquidated damages” are a form of monetary compensation for a breach of contract that the\nparties specify in their agreement. In general, courts in the U.S. will not enforce liquidated\ndamages provisions if the amount stipulated in the contract is so high that it acts as a form\nof punishment, rather than to be merely compensatory.\nFirst, we see if the model encoded the correct rule about the enforcement of liquidated\ndamages provisions.\nq: Are liquidated damages clauses enforceable?\na: Liquidated damages clauses are generally enforceable unless the sum stipulated\nis exorbitant or unconscionable.\nThe rule appears correct. But then we apply the rule to a set of facts. Here, the model\nneeds to infer that one million in damages is exorbitant for a Toyota Corolla and thus, the\nliquidated damages provision should not be enforced.\nq: X and Y contract over the sale of a Toyota Corolla. Their contract specifies that\nX has to pay Y one million dollars in damages if X does not deliver the car on\ntime. X does not deliver the car on time. Is Y entitled to damages? If so, in what\namount?\na: Yes, Y is entitled to $1,000,000 in damages. Damages are meant to compensate\nthe injured party for the loss suffered due to the breach. Y has suffered a loss of\n$1,000,000 because X has not delivered the car on time.\nThe model is unable to infer that $1M for a Toyota Corolla is exorbitant. Even if the model\nis provided with both the abstract rule and its precise translation to the context in the\nprompt — and the application should thus be trivial — GPT-3 is still unable to conduct the\nlogical reasoning required.\nq: X and Y contract over the sale of a Toyota Corolla. Their contract specifies that\nX has to pay Y the exorbitant amount of one million dollars in damages if X\ndoes not deliver the car on time. X does not deliver the car on time. The law\nrequires that liquidated damages clauses are enforced unless the sum stipulated\nis exorbitant or unconscionable. Is Y entitled to damages? If so, in what amount?\na: Yes, Y is entitled to the liquidated damages. The amount in this case is not\nexorbitant since the price was set by the parties themselves.\nPrecision. The foundation model also cannot create fake facts in the process, a problem that has\ncome up in current models [Gretz et al. 2020; Zellers et al. 2019b]. Specificity and truthfulness are of\nheightened important in legal contexts, where imprecise statements can have drastic, unanticipated\nconsequences, and false statements can lead to sanctions against attorneys.\nFew-shot Learning. Some challenges facing foundation models beyond those described in the\nabove example include few-shot learning, which is still in its infancy [Perez et al. 2021]. Research on\nfew-shot learning techniques and increasing access to legal corpora can work in tandem. Because\nfoundation models need to be precise, not only in factual truth, as discussed above, but also in\ntechnical legal language, it remains unclear to what extent information obtained from one corpus\ncan be utilized in another corpus of a different domain. Few-shot learning thus remains important.\n66 Center for Research on Foundation Models (CRFM)\nAdaptation. Some gains have been observed from domain-adaptive pretraining on unlabeled\nlegal corpora. These gains appear to be most pronounced when the pretraining corpus is highly\nrelevant to the downstream task and labeled training data is limited (a setting which is common in\nthe law) [Zheng et al. 2021]. It has not yet been comprehensively studied whether this extends to\na diverse set of legal tasks, but leveraging unlabeled domain-specific corpora for self-supervised\ntraining of foundation models may provide complementary improvements to few-shot methods.\nAccess to Clean In-Domain Data. Some recent efforts have sought to create large labeled datasets\nfor more challenging legal benchmark tasks through automation [Zheng et al. 2021] or manual\nannotation by volunteer legal experts [Hendrycks et al. 2021c]. These efforts have demonstrated\nthat larger language models that are pretrained on more data achieve performance gains on certain\nchallenging tasks, compared to more limited gains observed in other settings [Chalkidis et al. 2020;\nElwany et al. 2019; Zhong et al. 2020]. This work suggests that larger legal benchmark datasets may\nbe necessary to observe further gains from applying transfer learning techniques to foundation\nmodels. However, creating benchmark datasets for tasks that are legally meaningful and difficult\nfrom an NLP perspective can itself be challenging, as human expert annotation can be costly and\nautomated methods that utilize conventional tokenization and sentence segmentation techniques\ncan fail to account for unique aspects of legal text, such as the structure of legal citations [Bommarito\net al. 2018; Savelka et al. 2017]. As a consequence of these challenges, many existing legal domain-\nspecific labeled datasets are small, not publicly available, or reflect simpler tasks that have been\nsolved by methods often pre-dating the development of foundation models.41\nMuch available legal data may also be unrepresentative. Since only a fraction of cases end up in\nlegal opinions, it is unclear whether the disputes in publicly available data are representative of the\ntypical disputes presented to a model in practice [Priest and Klein 1984]. Costly training data for\nmore representative scenarios may be concentrated in the biggest law firms. These law firms may\nhave the ability to retain and accumulate data across many cases and clients. One concern then is\nthat foundation models could concentrate power even more among the few actors that have the\nresources to train models on in-domain data — unless the models can generalize sufficiently well.\nReliability. Finally, we again note that even if foundation models could successfully perform all\ntasks in the legal domain, deployment remains a major challenge: a failure of a foundation model\nin the law will have real, damaging consequences to both clients and attorneys (see also discussion\non fairness, legality, and ethics in §5.1: fairness, §5.4: legality, and §5.6: ethics). For this reason\nmachine translation software has already been deemed unreliable for use as evidence in some\ncourts,42 although it continues to be relied upon in others.43\nGiven all of these complexities, legal briefing and reasoning is likely beyond the capabilities of\ncurrent models, but appears to be within the future realm of possibilities. As such, these serve as a\npotential lode star for the ongoing development of foundation models.\n41For law firms and legal technology companies, tasks for which high performance can already be achieved, and can\ntherefore be more immediately productized, may be considered more worthwhile to invest costly manual labeling efforts\ntowards.\n42See discussion by Vieira et al. [2020].\n43For example, in Vasquez v. United States , No. 3: 16-cv-2623-D-BN (Dist. Court, ND Texas 2019), counsel relied on Google\nTranslate to prove that the previous (native speaker) attorney has mistranslated a plea deal.\nOn the Opportunities and Risks of Foundation Models 67\n3.3 Education\nAuthors: Ali Malik, Dorottya Demszky, Pang Wei Koh, Moussa Doumbouya, Drew A. Hudson, Allen\nNie, Hamed Nilforoshan, Alex Tamkin, Emma Brunskill, Noah Goodman, Chris Piech\nFig. 15. Foundation models in education could be trained on multiple data sources to learn the capabilities\nnecessary for education: an understanding of various subject matter and different pedagogical techniques.\nThese foundation models can be applied in a general-purpose way across a range of tasks and goals such as\nunderstanding students, assisting teachers, and generating educational content.\nIn the year 2000, the largest gathering of world leaders convened at the United Nations Millennial\nSummit to reflect on an ideal vision for the future. Delegates concluded that a primary focus should\nbe education, declaring it “a foundation for human fulfillment, peace, sustainable development,\neconomic growth, decent work, gender equality and responsible global citizenship. \" This discussion\nwas ultimately recodified into the United Nations Sustainable Development goal to “ensure inclusive\nand quality education for all and promote lifelong learning\" [United Nations General Assembly\n2015]. However, providing high quality, inclusive education at a large scale poses difficult societal\nand economic challenges. The price of education per student is growing faster than economy-wide\ncosts [Bowen 2012], limiting the resources available to support student learning. In the United\nStates, one symptom is that private education debt held by students has reached $1.6 trillion,\nsurpassing total credit card debt [Friedman 2020]. Considering the rising need to provide adult\nretraining, the gap between the demand for education and our ability to provide it is alarmingly\nlarge and has concerning achievement disparities across protected demographics.\nWith the advent of the digital age and the rapid growth in digital learning, computational\napproaches to education have shown promise in increasing the effectiveness of learners and\nteachers. Several core directions have emerged as potentially impactful applications of AI for\neducation [Woolf et al. 2013], such as systems that can provide meaningful feedback to students\n[Malik et al. 2021], help teachers improve [Jensen et al . 2020; Demszky et al. 2021; Suresh et al.\n68 Center for Research on Foundation Models (CRFM)\n2021], or even create personalised and adaptive learning experiences that tailor the learning process\nto individual students’ needs and dispositions [Connor 2019].\nDespite this potential, building technical solutions to effectively scale inclusively and quality of\neducation has proven to be exceptionally difficult. One particular challenge is that existing work\nhas focused on custom solutions to highly specific tasks for which large amounts of training data\nhas to be collected from scratch. Due to the difficulty and cost of creating large datasets, using\nthis approach to solve every educational task independently is fundamentally limited. Instead, is it\npossible to create general-purpose approaches that are reusable across various tasks and subjects?\nFoundation models have already started to boost the performance of some specific flagship tasks\nin education. Recent examples include using MathBERT [Shen et al. 2021b] to power “knowledge\ntracing\" — the challenge of tracking a student’s understanding over time given their past responses —\nand the “feedback challenge\", where an algorithm has to interpret a student’s answer to a structured\nopen-ended task, such as a coding question [Wu et al. 2021e]. Can foundation models lead to even\nmore transformative changes in this domain? And what are the known and imagined risks of\nfoundation models applied to education? In this section, we first frame the conversation around\nthe ethical considerations. We then ground our discussion in two concrete tasks: (1) understanding\nstudent misconceptions, and (2) improving student understanding through instruction.\n3.3.1 Important concerns for centering foundation models in education research.\nThe future of AI for education is exciting, especially in the context of foundation models. However,\nwe caution the reader to be especially thoughtful about the impact of any AI research applied to\neducation.44 The goal of education are deeply interwoven with complex, long term social impact.\nWhile we actively work to improve digital education, it is imperative that we put in substantial\nthought to try and imagine the complexities of any disruption in this space [Piech and Einstein\n2020]. Ethical challenges range from issues such as data bias, legal constraints, and the impact of\ndigital socialization. These issues are not unique to foundation models, but they are worth reflecting\non regularly as research makes substantial progress in AI for education. Reflection on impact is\nespecially important when research starts by asking “what can new AI technology afford?\"\nMany of the issues in §5.6: ethics apply to education. For example, as in many other domains,\nsmall biases in foundation model training data could be hard to track down [Dixon et al . 2018;\nBolukbasi et al. 2016], but have important implications for equity of educational access. Moreover,\nthese systems may experience a high degree of “feedback\", where the collected data continually\nreinforces the model’s decisions. This issue of bias goes beyond what data is collected and in-\ncludes concerns over the applications that researchers choose to work on. Below, we discuss other\neducation-specific issues. Many of the issues revolve around the question: “who benefits?\" and for\nwhom is new technology created?\nRemoving teachers from the loop One of the goals of digital education, especially based on\nAI, is to increase the productivity of the learning experience so that more learning happens per\nunit time or unit cost. One can imagine that decision makers could use this increased productivity\nto remove human teachers from the loop. The long term implications of such decisions are hard\nto know a priori . Could interacting with an education system optimized to maximize “learning”\nhave adverse effects on socioemotional skill development? Could it create fewer opportunities for\n44In 2013, Facebook initiated Free Basics, a project to provide free internet to the world and thus spread opportunity and\ninterconnection. Now, the United Nations Human Rights Council reports that, in Myanmar, Facebook’s efforts to follow\nthrough on such aspirations without proper human moderation accelerated hate speech, instigated division, and incited\noffline violence in the Rohingya genocide. Free Basics now serves as a warning of the complexities of technological impact\non society.\nOn the Opportunities and Risks of Foundation Models 69\nFig. 16. The figure illustrates a system that embeds signals from various modalities (image, speech, sign,\ntext) and languages into a universal feature space. Such a feature space allows ideas to be linked across\nmodalities and languages. Pedagogically relevant link types include analogies (similarities across languages)\nand contrasts (distinct concepts across languages), both of which can occur in the same modality or across\ndifferent modalities.\ninteracting with others? Loneliness is on the rise in younger generations [Cigna 2018], and teachers\nare a modulating force for pressures that AI researchers might not envision.\nWas this work done by the learner or a foundation model? Another challenge is how to\neffectively teach students who have access to foundation-model-based tools. For example, it will\nbe much more complex for teachers to understand the extent of a student’s contribution if the\nstudent worked together with a powerful generative model, or to regulate ineffective collaborations\nand detect plagiarism. Visual Studio has recently released GitHub CoPilot, an AI pair-programmer\nbuilt upon GPT-3 [Chen et al. 2021f]. How will this change computer science education? Many\nchallenges for beginner programmers might be trivial to CoPilot or its technical successors, which\ncould undermine the learning experience for novices. It would be instructive to study other examples\nof technological advances that disrupted education for certain subjects, such as calculators in math\nclassrooms and Google Translate in language courses, both of which now coexist with traditional\ninstruction.\nPrivacy and security. One important ethical issue in the use of AI in education is highlighted by\nthe strict legal guidelines concerning privacy in student work. For example, in the United States,\nstudent information is protected by the Family Education Rights and Privacy Act (FERPA). These\nlaws and regulations are especially important for children under 13, who have their data privacy and\nsecurity additionally protected by the Children’s Online Privacy Protection Act. Among other things,\nFERPA limits teachers from sharing personally identifiable student work. This could directly impact\ninitiatives to share data used both for training and for evaluating foundation models. Moreover,\nthere is an open question as to whether the weights of a foundation model could somehow leak the\n(possibly private) data it was trained upon [Nasr et al. 2018; Song et al. 2017]. These issues, and\ntheir corresponding approaches, are similar to the challenges described in §3.1: healthcare.\nThis list is not exhaustive and the ethical considerations will vary from project to project.\n70 Center for Research on Foundation Models (CRFM)\n3.3.2 Foundation models of student thought.\nWhen building AI tools for inclusive, and joyful education, there are many tasks where foundation\nmodels could be useful. Many of those tasks require us to first understand the learners whom we\nare trying to help, especially in the context of open ended work.\nWhat would it take for a foundation model to be able to reason about student understanding? It\nis easy to imagine a foundation model which has been adapted to answer a math question correctly,\nbut it is less clear how to build a model that can diagnose mistakes in student understanding based\non the student’s answers. To explore this theme, we consider the case study of providing feedback\nto students who are working on open-ended tasks such as writing a short paragraph, drawing a\nphysics diagram, or writing code. This “feedback challenge” exemplifies how foundation models\ncan be helpful off-the-shelf for learners, and also demonstrates open areas for foundation model\nresearch.\nTo effectively provide feedback to students, two central capabilities are required: (1) understand-\ning the subject matter of the task (e.g., physics or coding), and (2) the diagnostic ability to “notice\":\na technical term in education for inferring why a student made a mistake. For typical student in-\nteractions in a typical classroom, there is not enough data for an AI model to learn, from scratch,\nboth of these central capabilities. Even for massive courses with millions of students, supervised\nalgorithms barely understand the complex student reasoning behind even short, four-line programs\n[Malik et al. 2021]. As such, the feedback task inherently requires a transfer of understanding from\nexternal data and experience.\nFoundation models, as they currently exist, are directly helpful for the first of these capabilities:\nunderstanding a specific subject matter . For example, when learning to provide feedback on short\nprogramming questions, a foundation model such as GPT-3 can efficiently understand what fluent\ncode looks like with a few examples. Some research in this direction has already started exploring\nfoundation models that can quickly adapt to questions in new subject matter domains [Wu et al.\n2021e; Condor et al. 2021]. Similarly, foundation models could also integrate multiple modes of\ninformation such as the text of a task’s prompt, diagrams in the question, or even the content of a\ngrading rubric provided to teaching assistants. This unified representational ability can help foun-\ndation models comprehend a subject matter through richer sources of information. As a concrete\ncase study, many of these insights were leveraged as core components of an algorithm which was\nable to grade an introductory Computer Science midterm at Stanford University, with the same\neffectiveness as human teaching assistants [Wu et al. 2021e]. In this case, subject matter encoding\nwas built on a foundation model that had been adapted on GitHub code and a corresponding small\ndataset for each question’s subject matter. In general, we can imagine leveraging various sources of\ndata to adapt foundation models to different subject matter. For example, math adaptation could use\nmathematical websites or textbooks [Shen et al. 2021b] or historical student answers on platforms\nsuch as Gradescope; spoken language understanding could leverage radio archives or podcasts;\nand domains like creative writing could look to large digital archives like Project Gutenberg.\nIn contrast to subject matter, adapting a foundation model to the task of mapping observed\nmistakes to flaws in a student’s thought processes is much less well-explored. The ability for\nan instructor to “notice” the reasons behind why a student makes a specific mistake is a critical\ncomponent of the feedback challenge. Imagine, for example, a student learning two digit addition\nwho answers the question “what is 26 + 19?\" with the response “315. \" Take a moment and try to\nguess why they gave that answer and what misconceptions they have.45. This ability to notice could\nbe posed as an adaptation task for foundation models (§4.3: adaptation) or perhaps even as a\nreasoning task (§2.4: reasoning).\n45This student has made the common mistake of concatenating the results of adding the one’s digit and ten’s digit\nOn the Opportunities and Risks of Foundation Models 71\nWhile difficult, training an AI system to notice is an achievable goal. Across classrooms, and\nacross learning tasks in a given domain, there are generalizable patterns in how students arrive at\ntheir answers. The labeled data that can directly be used for this adaptation task, such as instructor-\nwritten feedback to student work in [Wu et al. 2021e], are often held privately by instructors in\ndisparate datasets. However, publicly accessible data, such as StackOverflow interactions, might also\nbe creatively used to adapt a foundation model to notice. Some research has also explored effective\nways of extracting, from instructors, generative descriptions of how students make mistakes [Malik\net al. 2021; Gulwani and Singh 2013] — these hand-written generative models could also be used to\ngenerate adaptation data to help foundation models diagnose student mistakes.\n3.3.3 Foundation models for instruction.\nReasoning about student understanding is an essential step towards a second objective: provide\ninclusive, high quality instruction. Computational approaches to instruction focus on different tasks\nlike content personalization [Connor 2019], question generation [Guo et al. 2016; Willis et al. 2019;\nSrivastava and Goodman 2021], adaptive curriculum design [Mandel et al . 2014; Doroudi et al .\n2017], and predicting instructor intervention [Chandrasekaran and Kan 2019; Alrajhi et al. 2021]. In\nthis subsection, we discuss how foundation models could be useful in the act of teaching students.\nSince effective teaching requires reasoning about student understanding, the previous discussions\non understanding subject matter and “noticing” are extremely relevant. However, providing effective\ninstruction requires an additional capability: that of understanding pedagogy [McKenzie 2003].\nThis encapsulates an effective understanding of techniques to guide a student, such as asking\nSocratic questions or providing analogies/contrasting cases; using encouraging or supportive\nlanguage; tailoring the difficulty of questions to the student; and generating examples that are\nrelevant to a student’s interests and background.\nHow can foundation models be adapted to understand good pedagogy for instruction? One idea\nis to consider adaptation using data source where instruction is the primary role. For example, data\nfrom question answering forums like StackOverflow could potentially be used to build a tutor which\ncan parrot common Socratic questions. Similarly, a foundation model adapted on encyclopedias\nsuch as Wikipedia might be able to give answers to student questions which are (often) factually\ncorrect. There are also public data sources like textbooks, lecture videos, lesson plans, and graded\nfeedback that collectively contain important pedagogical behaviours which could be adapted by\nfoundation models (Figure 15).\nAnother adaptation challenge for instruction based on foundation model is to learn how to\nspeak to students like teachers. The language used by teachers is often different from the language\nused by the general population. Teachers are ideally trained to speak to students with respect and\nin a way that intentionally helps them form a positive identity with the subject being learned\n[Truax 2018]. Cautionary examples like Microsoft’s 2016 Twitter bot “Tay, \" a chatbot that started\ngenerating hate speech within 24 hours of being deployed live, show us the importance of explicitly\naccounting for this factor in education. To train a language model which is more heavily influenced\nby professional teachers in classrooms, we could perhaps adapt foundation models to data sources\nlike lecture videos or recorded office hour videos.\nThe adaptation problem above is compounded by the fact that different education contexts vary\nsignificantly in the kind of language that would be appropriate: for example, effective instruction\nin a 5th-grade science class would look quite different from that in a college physics class, much\nless a college literature class. This presents technical challenges beyond what would be faced in\ntypical NLP domain shift settings (e.g., question answering based on news articles vs. Reddit posts),\nas the foundation model would need to be fluidly adaptable in terms of its tone and language, and\nnot just the factual content that it generates.\n72 Center for Research on Foundation Models (CRFM)\nBeyond sound pedagogical techniques and instructional language, how might foundation models\nprovide even more insightful forms of instruction? §2.1: language of this paper highlights the fact\nthat remarkably complex language can be acquired by babies in a short amount of time. As the\nauthors point out, a salient difference between foundation model training and human language\nacquisition is that “human language is grounded to the real world: for example, a baby’s caretakers\npoint to objects while they talk about them. \" This same insight can also inspire ideas as to how\nfoundation models can be used for generative education. Humans seem to learn well when presented\nwith real-world analogies and contrasts which may be cross-cutting between their current context\nand past experiences. For example, when teaching sign language, an instructor might use an analogy\nsuch as \"the hand shapes for the word ‘morning’ looks like the sun rising\" or note that “the hand\nshape you just made look very similar to another word, so let us focus on the differences. \" As\nanother example, when teaching Swahili to a learner who already knows Arabic and English, an\ninstructor could point out that the Swahili word for 8 (pronounced nane) is a “false friend” that is\nphonetically similar to English word for 9 (pronounced nine). Foundation models that can integrate\nmulti-modal data have the potential to make these kinds of rich analogies and comparisons that\nare typical in childhood language learning (Figure 16).\nOn the Opportunities and Risks of Foundation Models 73\n4 TECHNOLOGY\nThe technological foundations of foundation models give rise to the capabilities (§2: capabilities)\nthat determine their potential. To understand the technology used in development, we consider\nthe data (§4.6: data), model architectures (§4.1: modeling) and systems (§4.5: systems) used to\ntrain (§4.2: training), and further adapt, (§4.3: adaptation) these models alongside the theory\n(§4.10: theory) that should be developed to understand this paradigm. To then understand the\nresulting models, we discuss how to evaluate (§4.4: evaluation) and interpret (§4.11: inter-\npretability) alongside the importance of robustness (§4.8: robustness), security and privacy\n(§4.7: security), and long-term AI safety (§4.9: ai-safety) for ensuring the reliability of these\nmodels when deployed in society (§5: society).\n74 Center for Research on Foundation Models (CRFM)\n4.1 Modeling\nAuthors: Drew A. Hudson, Antoine Bosselut, Alex Tamkin, Omar Khattab, Jared Quincy Davis, Jiaxuan\nYou, Trevor Gale\nFig. 17. The five key properties of a foundation model: expressivity — to flexibly capture and represent rich\ninformation; scalability — to efficiently consume large quantities of data; multimodality — to connect together\nvarious modalities and domains; memory capacity — to store the vast amount of accumulated knowledge;\nand compositionality — to generalize to new contexts, tasks and environments.\nThe emerging paradigm of foundation models has attained impressive achievements in AI over\nthe last few years, as models such as BERT [Devlin et al. 2019] shine at a wide spectrum of language\nunderstanding tasks: from textual classification and entailment to question answering and reading\ncomprehension, while GPT-3 composes rich and fluent tales about unicorns [Brown et al. 2020]\nand DALL-E shows signs of visual creativity, generating from scratch strikingly-realistic pictures\nof avocado chairs [Ramesh et al. 2021].\nThese and other instances of recent foundation models not only achieve remarkable performance\nacross a multitude of diverse downstream tasks and applications [Rajpurkar et al. 2018; Wang et al.\n2019a], but also manifest noteworthy behaviors of interpretability [Karras et al. 2020], robustness\n[Devlin et al. 2019], controllability [Patashnik et al. 2021] and generalization [Brown et al. 2020].\nWhat does it take for a model to demonstrate such qualities? What architectures are capable of\nconsuming large quantities of potentially multimodal information and translate them into rich\nknowledge of the world? And overall, what desirable properties should a network possess to give\nrise to a foundation model?\nHere, we identify and discuss five such properties, spanningexpressivity, scalability, multimodality,\nmemory capacity , and compositionality, that we believe are essential for a foundation model in\norder to: (1) distill and accumulate knowledge from various sources and domains, (2) organize it in\nan effective and scalable representation, and (3) flexibly generalize it towards novel contexts. For\neach of these properties, we motivate their necessity, provide examples of contemporary models\nthat incorporate them, and explore key challenges and promising avenues for future research and\ndevelopment. See Figure 17 for an overview diagram.\nOn the Opportunities and Risks of Foundation Models 75\n4.1.1 Expressivity.\nExpressivity concerns with the theoretical and practical capacity of a network to model the data\ndistribution it is trained over and represent it in a flexible manner. Prior works have proposed\nformal expressivity measures to characterize the complexity of functions a network can compute,\nor more precisely, approximate, which is essentially affected by its depth, width, connectivity, and\nstructural patterns [Raghu et al. 2017].\nAs the No Free Lunch theorem suggests, there is no single model or algorithm that suits best for\nall cases [Wolpert and Macready 1997], and so, for our purposes, we are particularly interested in\nidentifying which models could effectively capture the facets ofnatural information, such as human\nlanguage or real-world images [Goodfellow et al. 2016]. These modalities are either continuous (as\nin vision) or discrete (as in language), are distinctly hierarchical and high-dimensional, and present\na complex set of relations and interactions among their constituent elements, whether these are\npixels, words or physical objects.\nIndeed, recent breakthroughs in generative modeling provide strong evidence for the high\nexpressivity of neural networks, as they successfully express distributions of textual [Brown et al.\n2020; Devlin et al. 2019; Lieber et al. 2021; Wang and Komatsuzaki 2021], auditory [van den Oord\net al. 2016], and visual [Karras et al. 2020; Brock et al. 2018] domains, and generate samples of high\nfidelity, diversity and realism.\nInductive Biases. Much of the success of neural networks over the last decade in modeling natural\ndata is owed to the networks’ high depths , as could be roughly measured by the number of stacked\nnon-linear layers they are composed of, or the number of computational steps they take during\ntheir chain-of-reasoning. Great depths play a crucial role in enhancing networks’ expressivity,\nallowing them to form powerful hierarchical and distributed representations that could generalize\nfrom the training data to new unseen examples [He et al. 2016b; Levine et al. 2020].\nThe universal approximation theorem [Lu et al. 2019b] indeed states that even simple multilayer\nperceptrons (MLPs) can represent a broad set of functions, while different inductive biases, as those\nimplemented in Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs)\n[Goodfellow et al. 2016], can improve the learning efficiency and enhance the capacity of a given\nnetwork to model different forms of information: sequential data, common to language, speech and\ntime-series, for the former, or spatially-invariant information, as in images or videos, for the latter.\nTransformer Networks & Attention. Meanwhile, transformer networks [Vaswani et al. 2017],\nintroduced more recently, demonstrate the importance of capturing long-range dependencies and\npairwise or higher-order interactions between elements. They build on theself-attention mechanism\n[Vaswani et al. 2017; Bahdanau et al. 2014] that enables shorter computation paths and provides\ndirect means to compare elements far-across the input data (such as a pronoun and its antecedent\nin a sentence, or two sentences that refer to the same topic).\nFrom another perspective, the multiplicative interaction embodied in both attention as well as\ngating structures (as in LSTMs [Hochreiter and Schmidhuber 1997] or Mixture-of-Experts [Shazeer\net al. 2017]) offers a more flexible alternative to the rigid fixed-weight computation of MLPs and\nCNNs, dynamically adapting the computation to the input at hand. This proves especially useful\nfor language modeling, where, for instance, given a sentence like “She ate the ice-cream with the\nX\", while a feed-forward network would always process it in the very same manner, an attention-\nbased model could adapt its computation to the input — updating the contextual representation of\nthe word “ate” if the prepositional phrase (PP) attachment X is “spoon”, or instead link it to the\n“ice-cream\" if X refers e.g., to “strawberries\" [Zavrel et al. 1997].\n76 Center for Research on Foundation Models (CRFM)\nGeneral-Purpose Computation. A final notable advantage of attention over prior architectures\nstems from its strongergenerality, where it is not strongly tied to a particular task or domain, as is the\ncase for the local receptive field of convolution or the sequential assumption of recurrent networks,\nboth reflecting inherent properties specific to the vision and language modalities respectively.\nWe hypothesize that the general-purpose nature of attention and transformers contributes to\ntheir broad applicability for a wide range of research problems and applications [Liu et al. 2019;\nDosovitskiy et al. 2020; Hudson and Zitnick 2021].\nThis contrast captures a more general trade-off between task-specialization and expressivity :\nmodels with stronger structural priors can leverage them to improve sample efficiency on the\nparticular tasks that benefit from these assumptions; while conversely, models that integrate weaker\ninductive biases learn more slowly, but can in turn scale to higher volumes of data and adapt to a\ndiverse set of domains, since they do not rely on restrictive or task-specific suppositions. As both\ndata and compute turn more accessible, we observe that the exploration of models with a minimal\nset of inductive biases that can “let the data speak for itself\" seems to serve as a more promising\napproach for future research in the field.\nChallenges & Future Directions. Notwithstanding the stellar progress and accomplishments of\nneural networks in general, and foundation models in particular, in terms of expressivity, notable\nchallenges still remain. Leading approaches [Choromanski et al . 2020; Dosovitskiy et al . 2020]\nkeep struggling with modeling of extremely long-range dependencies, such as those occurring in\nbooks, movies, or even DNA sequences, which may be attributed to the quadratic computation of\ncontemporary transformer-based approaches [Wang et al. 2020c; Lin et al. 2021].\nThis challenge essentially reflects the trade-off between efficiency and expressivity : where explicit\nmodeling of long-distance interactions through short and direct computation paths improves\nexpressivity on the one hand, but comes at the expense of scalability due to computation entailed\nby the increased connectivity on the other [Child et al . 2019; Kitaev et al . 2020; Choromanski\net al. 2020]. Models such as the GANformer [Hudson and Zitnick 2021] and the Perceiver [Jaegle\net al. 2021b,a] explore ways to balance these two properties and propose transformers with linear\ncomplexity that rely on bipartite or bottleneck attention, so to improve computational efficiency\nwhile maintaining high-expressivity. We believe that identifying an effective equilibrium between\nthese two objectives offers an interesting avenue for future research.\nAnother important research direction relates to the expansion of foundation models, which, so\nfar, have mainly focused on the language domain [Peters et al . 2018; Devlin et al. 2019; Brown\net al. 2020], to different modalities, such as the structural [Scarselli et al . 2008; Veličković et al.\n2017] and perceptual [Tolstikhin et al. 2021; Jaegle et al. 2021b; Tan and Le 2021], each involving a\nunique set of associated challenges. Likewise, we believe that exploring architectures for reasoning\n(§2.4: reasoning), which demands iterative computation chains and interaction with symbolic\ninformation, constitutes a valuable goal for future foundation models research.\n4.1.2 Scalability.\nClosely connected to model’s expressivity is the notion of scalability. As rich data from varied\nsources becomes more readily available, and computational resources get stronger and more\nefficient (§4.5: systems), we should look for ways to match this rate of progress and harness it\nto improve AI competency and versatility. For foundation models to effectively fit the complex\nand high-dimensional distribution of images or text, they should thereby be scalable across all\ndimensions: including both models’ depth and width as well as their training time, number of\nparameters, and amount of data they could process.\nOn the Opportunities and Risks of Foundation Models 77\nOptimization. Specifically, foundation models should both be: (1) easy-to-train (§4.2: training),\nby being resilient to noise or imperfections in the data, and robust againstinstabilities like vanishing\n[Helfrich et al. 2018; Glorot and Bengio 2010] or exploding gradients [Hochreiter and Schmidhuber\n1997; Nair and Hinton 2010], but also (2) easy-to-adapt (§4.3: adaptation), by overcoming phe-\nnomena of catastrophic forgetting [Kirkpatrick et al. 2017] and supporting few-shot learning [Sung\net al. 2018]. We are still in the early days of understanding the principles that drive the scalability\nof learning algorithms, and while recent works have started to shed some light on these themes\n[Liu et al. 2020c; Kuditipudi et al. 2019; Nakkiran et al. 2019], much work remains to be done.\nHardware Compatibility. Moving beyond aspects of robustness and optimization, foundation\nmodels should also be practically efficient (§4.5: systems), and take advantage of contemporary\nand future hardware [Hooker 2020]. One example of that is parallelizablity, an important property\nthat characterizes the computation supported by GPUs. Indeed, much of the transformers’ great\nsuccess over the previously dominating recurrent approach was driven by their higher degree of\nparallelism.\nLooking forward, given the fast-pace progress of systems development, we should further ensure\nthat models are designed to co-adapt to future hardware advances. Consequently, foundation models\nshould ideally be amenable to schemes such as distributed training, which is gaining popularity,\nas is the case for e.g., Mixture-of-Experts, and possibly leverage properties such as sparsity of the\ncomputation or representation, as is the case for the Longformer [Beltagy et al . 2020], BigBird\n[Zaheer et al. 2020], and Sparse Transformer [Child et al. 2019] approaches, and which likely will\nbecome more central in future hardware and processors.\n4.1.3 Multimodality.\nTraditionally, the fields of computer vision, robotics, and NLP have made progress in an independent\nmanner, with separate communities developing specific approaches suitable for each modality.\nA conducive consequence the rise of deep learning has brought about was the bridges it helped\nforming among the various communities and research areas within AI, as seemingly different\nproblems could now be tackled by closely-related approaches, and studies of originally remote\ntopics began converging to a common ground. This breakthrough opened up a new range of\npossibilities, fostering pioneering exploration into the theme of multimodality, encompassing areas\nas varied as language grounding [Lynch and Sermanet 2020], visual semantics [Conser et al. 2019],\nembodied environments [Savva et al. 2019a] and interactive agents [Gray et al. 2019].\nEssentially, multimodality serves as a key component of intelligence, and is a crucial factor for\nthe development of both thorough and broad comprehension of the world. Concretely, language\nlearning is more effective when occurring in a grounded environment rather than in a vacuum.\nAnd inversely, from the vision perspective, language encourages the emergence of abstractions that\nlink between low-level perceptual signals and statistics to semantic concepts of objects, properties,\nagents and motivations, thereby enriching and elevating visual representations.\nIn light of these observations, we argue that foundation models should ideally connect together\nthe different modalities, distill their embodied information into a shared multifaceted representation,\nand capture the full range of inter-connections and relations among them so as to furnish a wide\nrange of capabilities (see §2.1: language, §2.2: vision,§2.3: robotics, §2.4: reasoning).\nGenerality and Specialization. An important design choice for multimodal foundation models\nis the degree of specialization, or the structural sharing between the modules responsible for each\nmodality. Naturally, data of different domains exhibits diverse kinds of structures and properties —\nwhere, for instance, language is discrete while vision is continuous. At first sight, this variation hints\nthat specialized inductive biases tailored for each modality could be of aid. Yet, as training scales\n78 Center for Research on Foundation Models (CRFM)\nupwards and models are provided with the opportunity to base their learning less on structural\npriors and more on the data itself, general approaches that maintain only a handful of broad\ngeneral assumptions prove in fact a lot more successful than task-specific alternatives. And so,\nas corroborated by recent success of general-purpose models like transformers across different\nmodalities — both linguistic [Liu et al. 2019; Lan et al. 2019] and visual [Dosovitskiy et al. 2020;\nHudson and Zitnick 2021], we see that generality is critical for improving AI capabilities.\nMultimodal Interactions. Another key consideration for multimodal models relates to weight\nsharing: do the various modalities benefit from using the same or different parameters for their\nrespective components? Prior works have shown that fruitful transfer could certainly occur across\nmodalities, but the ideal degree of sharing remains unclear, so is the existence of principled ways\nfor discovering it.\nFinally, a major design question concerns with the forms of the multimodal interactions supported\nby the model, which vary widely between concrete cases and examples: Cross-modal or late-fusion\nmodels such as ConVIRT [Zhang et al. 2020a] and CLIP [Radford et al. 2021] maintain fully separate\nencoders for each data source, and compare their spaces only at the ultimate computation stage,\nusing e.g., a simple dot product. Meanwhile, early-fusion models, such as ViLBERT [Lu et al. 2019a;\nCho et al. 2021], jointly reason over multiple modalities necessary for tasks of visual reasoning\nand question answering. Identifying the optimal stage and form for merging the respective vector\nspaces [Nagrani et al. 2021] remains an open research question.\nOverall, while there seems to be a consensus within the community about the importance of\nmultimodality, models that go beyond shallow alignment of vision and language are yet to exist,\nand the theme of grounded language learning in embodied environments still has much room for\nexploration.\n4.1.4 Memory.\nSo far, we have discussed the foundation models’ goal to gather and accumulate information from\nvaried modalities at large scales. This knowledge encompasses both broad understanding of the\nworld as well as specific mastery of niche subjects or particular facts. Representing such a large\nbody of learned information is by no means trivial, and is leading to interesting questions about\neffective mechanisms for access, storage, retrieval and manipulation of particular items or memories.\nExplicit Storage. An important design principle that could achieve these desiderata is to separate\nout computation from memory [Weston et al. 2014; Graves et al. 2016; Hudson and Manning 2018,\n2019a] in order to enhance models’ ability to transfer knowledge by applying previously acquired\nabstract skills to new concrete settings.\nIn this context, it is important to distinguish between explicit facts — that can be stored in an\nexternal memory storage, andimplicit knowledge — that is reflected through the networks’ trainable\nweights. Such decoupling of explicit and implicit knowledge enjoys multiple advantages compared\nto the alternative of implicitly encoding all information together through the network weights.\nThe separation mitigates the inflation in models’ size and number of parameters needed to store\nthe growing quantities of knowledge [Guu et al. 2020], improves models’ trust and reliability by\nincreasing their knowledge provenance [Cheney et al. 2009], and most notably, is key for memory\nupdate, manipulation or adaptation [Lewis et al. 2020b] (§4.3: adaptation), which could in turn\nenable generalization to novel contexts and downstream tasks.\nIndeed, disentanglement between memory and computation has been a recurring goal in deep\nlearning and NLP research over the last years, including models such as Memory Networks [Weston\net al. 2014; Sukhbaatar et al. 2015], the Neural Turing Machine [Graves et al. 2014, 2016], the Neural\nState Machine [Hudson and Manning 2019a], and MAC [Hudson and Manning 2018]. Furthermore,\nOn the Opportunities and Risks of Foundation Models 79\nusing key-value structures [Miller et al. 2016] for accessing external memories has been shown to be\nvery effective for modeling long-term dependencies [Henaff et al. 2016; Bosselut et al. 2018; Lample\net al. 2019]. Transformers, the celebrated architecture underlying most foundation models to date,\nlikewise exhibits operations that involve key-value memory-access and computation among the\ncontextual word representations they gradually build [Geva et al. 2020].\nInformation Retrieval. Once a model completes gathering the information after training, there\nare multiple ways to retrieve particular facts or memories necessary for downstream applications\nand tasks. Some employ explicit prompting techniques that query the model’s knowledge through\ninput sequences [Petroni et al. 2019; Kassner et al. 2021; Jiang et al. 2020] while other approaches\ninvolve implicit recollection and reshaping of the prior knowledge through an adaption phase\n[Bosselut et al. 2019; Hwang et al . 2021]. A third category of methods goes a step further and\ncombines neural-based computation with symbolic aggregation and retrieval of information from\neither unstructured textual repositories [Karpukhin et al. 2020; Lewis et al. 2020b; Khattab et al.\n2020] or even structured resources such as knowledge graphs [Zhang et al. 2019a; Peters et al. 2019;\nLiu et al. 2020e; Verga et al. 2020; Yasunaga et al. 2021].\nHowever, there istrade-off between the strong memorization skills offered by retrieval mechanisms\non the one hand and the richer representations learned when there is an information bottleneck on\nthe other. Indeed, over-reliance on retrieval reduces the opportunities to learn how to represent\ninformation in compact and abstract manners, distill key insights and concepts out of the vast\namounts of input information the model is exposed too, and, basically, separate the wheat from\nthe chaff. For instance, the in-context learning abilities of GPT-3 possibly emerge as a by-product\nof enforcing the network to represent the input sequential data through its bounded memory\narchitecture [Brown et al. 2020]. Overall, While they certainly have some merits [Guu et al. 2020],\nmodels that rely on external retrieval mechanisms may not learn to generalize as effectively as\nbounded, compact and abstract representations.\nKnowledge Manipulation. Finally, when considering large-scale learning over long durations,\nit is crucial to note the dynamic nature of knowledge, where facts’ correctness and validity can\nchange over time as the world keeps evolving — and what was true or relevant yesterday may not\nbe so tomorrow. It is therefore crucial for a model to represent its knowledge in a manner that\nsupports efficient update or manipulation of facts as part of its lifelong learning.\n4.1.5 Compositionality.\nCompositionality can be defined as the principle according to which the meaning of the whole is\nderived from the meaning of its constituent parts , and the rules applied to combine them [Janssen\nand Partee 1997; Bottou 2014]. It is a crucial ingredient of human intelligence [Lake et al. 2017],\nunderlying our capabilities to plan, reason and learn readily and efficiently from a handful of\nexamples. Compositionality may hold the key to achieve out-of-distribution — or specifically —\ncombinatorial generalization. Drawing on classic ideas from symbolic AI, it encourages and enhances\ndesirable properties within neural networks, such as interpretability, controllability and data-\nefficiency [Lake et al. 2017], and can take different forms, characterizing variety of elements:\nModel. Compositionality can be reflected at the model level, in terms of its architectural properties,\nstructure, and degree of modularity — which can increase training and inference efficiency of large\nneural models [Shazeer et al. 2017]. It also links to themes of interpretability and multimodality, as\nit relates to the interfaces between the different modules the model is composed of, what modes of\ninteractions they employ, and how transparent they are.\n80 Center for Research on Foundation Models (CRFM)\nComputation. Models such as Module Networks [Andreas et al. 2016] and Mixture-of-Experts\n[Shazeer et al. 2017] go further along this direction, exhibiting not only structural modularity,\nbut also compositional computation , supported by the specialization of sub-networks to different\noperations, in a manner that adapts and tailors the model behavior to the input at hand. While\nsome methods rely on concatenation of hand-engineered modules [Andreas et al. 2016], alternative\napproaches enable the network specialization to naturally emerge through learning [Shazeer et al.\n2017]. Other models, such as MAC [Hudson and Manning 2018] and Dynamic Memory Networks\n[Xiong et al. 2016] perform an explicit iterative computation , where a given task is decomposed\ninto multiple reasoning steps, performed one by one, manifesting sequential progression from a set\nof initial facts to novel inferences and conclusions.\nTraining & Data. Not only can the model or its computation be compositional, but so can be the\ndata or training processes too [Andreas 2020]. Instead of training one model over a complete dataset,\none could split, or decompose it into subsets, train different models on each one independently,\nand ultimately recombine them at test time through various ensemble techniques [Dietterich 2000].\nSuch approaches could have far-reaching implications on the training and deployment procedures\nof foundation models, in both practical and even societal regards.\nRepresentation. We have discussed compositionality of different elements, such as the model, the\ncomputation, the training schemes or the data. But most notably, the learned representation itself,\nwhich emerges over the course of the model training and adaptation, can also be compositional\n[Andreas 2019]. Indeed, a promising manner to represent knowledge is through structured, poten-\ntially graph-based, object-oriented representations [Zhang et al. 2019a; Wang et al. 2021a], that\ncenter around identifying entities and event nodes and forming connections, analogies and relation\nedges among them. It reflects a natural way to organize information about the world, where inputs\nfrom different modalities can be channeled and aggregated around semantic multi-faceted concepts.\nSuch representations could support multi-hop reasoning and inference [Washington et al. 1995; Sun\net al. 2020b; Yu et al. 2020c], and potentially also enable stronger out-of-distribution generalization\nthrough recombination.\nHowever, compositionality can also hinder the expressivity of the representation, and impede its\ncapacity to account for idiosyncrasies, exceptions, and contextual correlations [Misra et al. 2017a].\nIn other words, the whole can sometimes be greater than the sum of its parts, where for instance,\nred wine is not the same as red onion. But while many approaches that have dominated over the\nlast decade tend to focus mostly on one end of the spectrum, and learn monolithic distributed\nrepresentations, we believe that exploring manners to reach a better balance between contextuality\nand compositionality is a promising avenue for future research.\n4.1.6 Summary.\nWe have introduced five properties that we believe are essential for the next generation of foundation\nmodels, in order to effectively distill the large amounts of information around us so to successfully\naddress downstream tasks: expressivity — to flexibly capture and assimilate real-world information,\nscalability — to adeptly handle high volumes of high-dimensional data,multimodality — to consume,\nprocess and potentially produce content from different sources and domains, memory capacity —\nto effectively store and retrieve the acquired knowledge, and finally, compositionality, to foster\nsuccessful generalization to novel tasks, settings and environments. We believe that the realization\nof the full potential of foundation models, as is envisioned and discussed in detail throughout this\nreport, will rely on research of new architectural and modeling advances to fulfill these desiderata.\nOn the Opportunities and Risks of Foundation Models 81\n4.2 Training\nAuthors: Alex Tamkin\nTraining objectives are mathematical functions describing how to transform a model architecture\nand large amount of broad data into a foundation model. For example, GPT-3 was trained with\na language modeling objective, which rewards the model for predicting the next word correctly\n[Shannon 1948]. We begin by laying out some goals of these training approaches, describe important\ndesign trade-offs in current approaches, and outline important goals for the path ahead.\n4.2.1 Goals of training objectives.\nHere we outline some key goals for training algorithms in light of the recent rapid progress in\nthese methods and models.46\nLeveraging broad data. The rise of self-supervised learning algorithms has unlocked the power\nof internet-scale datasets which would be intractable to annotate by hand. This kind of broad data\ncomes in many forms, including images, audio recordings, and video (§2.2: vision); robotic and\nsensor data (§2.3: robotics); and text, either in isolation or paired with other modalities like images\n(§2.1: language). Because this data lacks external annotations, a major focus for researchers is\ndesigning bespoke self-supervised algorithms that leverage the unique structure within each kind\nof data to produce a training signal for a foundation model.\nDomain completeness. An important goal for foundation model training algorithms is to be\ndomain complete , in the sense that solving the training task requires capabilities that are broadly\nuseful for downstream tasks in the domain (see §2.1: language, §2.2: vision, §2.3: robotics). This\nproperty is crucial for the generality of a foundation model. For example, language modeling may\nrequire models to acquire capabilities as wide-ranging as coreference, sentiment and translation as\nthe model learns to predict the next word in a document. In contrast, a supervised learning task\nlike sentiment classification may lead to a more narrow set of capabilities (see §2.1: language). As\nimportant as this quality is, it is not obvious a priori what tasks will result in a domain complete\ncapabilities, or even how to evaluate the full breadth of a model’s capabilities (see §4.4:evaluation\nand §4.10: theory).\nScaling and compute efficiency. Procedures for training foundation models must reliably convert\ndata, a model architecture, and compute into a broadly capable model. To maximize the capability\nof a foundation model, we can identify the bottlenecks to this process and propose new training\nalgorithms which remove them. The rise of self-supervised algorithms has made model size and\ncompute resources increasingly salient bottlenecks [Kaplan et al. 2020; Henighan et al. 2020], leading\nto a shift where models are evaluated not solely on their capabilities but rather on the amount and\nkind of compute needed to reach those capabilities (§4.4: evaluation). The efficiency of training\nobjectives can vary tremendously,47 laying in sharp relief how important the design of a training\napproach is to the emergence of powerful capabilities given a fixed compute budget. Thus, a major\ngoal for training researchers is to design training objectives with a richer training signal, resulting\nin models which learn faster and attain stronger capabilities.48 One force aiding this development\nis the surprising predictability of how capabilities scale with different kinds of architectures, data\n46We use “training\" instead of pretraining to emphasize the primacy of the foundation model itself, and because some\nmethods for adapting foundation models to downstream tasks do not involve any later stage of training.\n47e.g., 4x for ELECTRA [Clark et al. 2020] vs BERT [Devlin et al. 2019], 12x for contrastive vs generative approaches to\nCLIP training [Radford et al. 2021]\n48Of course, a key goal for computer systems designers is to alleviate compute as a bottleneck for training (see §4.5: sys-\ntems) And the choice of a training method is ultimately also constrained by the availability of diverse, high-quality data\n82 Center for Research on Foundation Models (CRFM)\nsizes, and compute [Hestness et al. 2017; Kaplan et al. 2020], a striking phenomenon which enables\nmodel developers to make choices based on clearer trends instead of more costly random searches.\n4.2.2 Design trade-offs in current SSL methods.\nCurrent self-supervised learning (SSL) methods for training foundation models are diverse, but\nwhat unites them is that they produce prediction problems from unlabeled data without the need\nfor human annotators. SSL objectives manufacture a rich training signal from this data through\ncarefully-designed constraints, either on the data itself (e.g., redacting or noising) or on the way\nthe model is able to represent or process the data (e.g., latent bottlenecks). At some level, these\nconstraints “bake in\" the kinds of capabilities desired when adapting models to downstream tasks\n(§4.3: adaptation).49\nHere, we describe three important design choices that current models explore, along with their\nrespective tradeoffs in terms of their resulting capabilities.\nAt what level of abstraction should we model? A fundamental question is what the input\nrepresentation of a foundation model should be. One option is to model the input at the level of raw\nbytes. However, this high dimensionality may cause the model to focus on predicting less semantic\naspects of the input, 50 slowing the rate at which it acquires more generally-useful capabilities.\nThese approaches also become intractable when training models like transformers [Vaswani et al.\n2017] whose compute costs grow quadratically with the input size. 51 Another option is to use\ndomain knowledge to reduce the input space of a model — such strategies include patch embeddings\n[Dosovitskiy et al. 2020] as well as fixed or learned tokenization [Schuster and Nakajima 2012;\nSennrich et al. 2016; Kudo and Richardson 2018; van den Oord et al. 2017; Ramesh et al. 2021]. These\nmethods may alleviate some challenges facing generative approaches, but have the trade-off that\nthey may jettison possibly-useful information in the input.52 The choice of a continuous vs discrete\ninput also has trade-offs for adaptation (§4.3: adaptation); more work is needed to capture the\nbenefits of both approaches.\nGenerative vs discriminative models Generative training approaches are conceptually elegant\nyet powerful — they train models to learn joint or conditional distributions over training inputs.\nTwo major families of generative foundation models include autoregressive foundation models\n[van den Oord et al . 2016; Radford and Narasimhan 2018; Chen et al . 2020d; Yang et al . 2019;\nRamesh et al. 2021], which generate inputs piece by piece, and denoising foundation models [Devlin\net al. 2019; Raffel et al . 2019] which corrupt and then recover the inputs. The specific kind of\ngeneration performed in the training process determines what kind of interactivity is available\n(§4.6: data), which continues to be a major challenge for many domains, including robotics (§2.3:robotics) and low-resource\nlanguages (§2.1: language)\n49For example, the causal language modeling objective used to train GPT-3 [Brown et al. 2020] enabled conditioning it\nvia prefixes. And the color jitter augmentations used during contrastive learning [Chen et al. 2020c] encourage invariance\nto properties not thought to be useful for downstream tasks. Better understanding how the particular choice and structure\nof these constraints influences the capabilities acquired by the model is an important area for future work (§4.10: theory).\n50e.g., blades of grass, audio compression artifacts, or spellings of words\n51See §2.2: vision and §4.1: modeling for discussions of training costs for high-dimensional sequences, such as images\nand video\n52For example, tokenizing text may make it harder to learn rhymes, puns, or other tasks that benefit from character-level\ninformation [Branwen 2020]\nOn the Opportunities and Risks of Foundation Models 83\nduring adaptation53 (see §2.5: interaction and §4.3: adaptation), and future models may enable\nan even richer set of interactions.54\nWhile generative training approaches have their benefits, several discriminative approaches have\nalso recently gained traction. These methods do not enable generation-based interaction, yet they\nmay enable more efficient learning for classification- or regression-based tasks in high-dimensional\ncontinuous settings like images, audio, and video. Most of these methods output vectors for (parts\nof) inputs, which are trained to be similar for different “views” of an input [Wu et al. 2018; van den\nOord et al. 2018; Chen et al. 2020c; He et al. 2020; Grill et al. 2020; Caron et al. 2021; Zhang et al.\n2020a; Radford et al. 2021] or used to predict whether parts of inputs are real or fake [Clark et al.\n2020; Iida et al. 2021]. Better understanding the trade-offs between generative and discriminative\ntraining, as well as capturing the best of both approaches, remain interesting avenues for future\nstudy.\nCapturing multimodal relationships. Another increasingly important research area is captur-\ning the relationships between multiple kinds of data. What this means may differ based on the\ncontext and the goals of a modeler. For example, CLIP [Radford et al. 2021] and ViLBERT [Lu et al.\n2019a] are both multimodal vision-language, but differ in the precise way they are multimodal.55\nThe former encodes images and text separately into vectors, enabling users who have examples\nfrom a single modality to retrieve, score, or classify examples from the other modality. The second\nprocesses images and text jointly at an early stage of the model, enabling downstream applications\nlike visual question answering where reasoning over pairs of related images and text (e.g., images\nand questions about them) are provided. Multimodal foundation models remain a nascent research\narea; much is still unexplored about the different ways a model can be multimodal as well as better\nunderstanding the capabilities these additional modalities bring.\n4.2.3 Paths forward.\nWe close with some important goals for the future of foundation model training.\nOut-of-the-box SSL Right now, SSL objectives are highly domain-specific: different methods\ncurrently prevail in natural language processing, computer vision, and speech processing. This\nhas two major disadvantages: First, these different techniques make it challenging to grasp the\ncommon threads and scientific principles underlying why each of these methods work. Second,\nthis domain-specificity requires developing new foundation model training methods from scratch\nfor each new field, including medical, scientific, and new multimodal settings. A more general\nobjective for efficiently training foundation models on any kind of data would represent a significant\nmilestone for the foundation model training community [Tamkin et al. 2021b].\nObtaining a rich training signal It is clear that not all training objectives are made equal — some\nare radically more efficient than others, translating into far more capable foundation models for a\ngiven compute budget. Are there training methods orders of magnitude more efficient than those\ncurrently known? If so, how can we find them? These investigations will be shaped by many forces,\nincluding what future software and hardware advances (§4.5: systems) make possible. We also\nneed not view data (§4.6: data) and training algorithms as independent factors: not only does the\n53For example, autoregressive models like GPT-3 enable prefix-based conditioning, while denoising models like T5 or\nBERT facilitate the use of bidirectional context to replace arbitrary-length spans or fix typos.\n54Other kinds of generative approaches less studied in a foundation modeling context include diffusion and score-based\nmodels [Sohl-Dickstein et al. 2015; Song and Ermon 2019; Ho et al. 2020], VAEs [Kingma and Welling 2014], flow models\n[Dinh et al. 2015; Kingma and Dhariwal 2018], and GANs [Goodfellow et al. 2014] — it remains to be seen whether these or\nother future approaches can also enable learning as diverse variety of capabilities as autoregressive or denoising approaches.\n55See §2.2: vision and §2.1: language for more discussion of multimodality in vision and language specifically\n84 Center for Research on Foundation Models (CRFM)\nquality and availability of the data influence the training signal,56 but the training algorithm itself\ncould adaptively seek out or construct richer training examples as the model improves to accelerate\nlearning [Tamkin et al. 2021c].\nGoal-directed training of foundation models. Adaptation methods such as prompting (§4.3:adap-\ntation) draw on emergent properties that result almost as an afterthought of training. Can we train\nfoundation models where the ability to understand and reliably carry out goals in a complex world is\npart of the model’s training objective? A focus on developing general capabilities distinguishes this\ndirection from the goal of adapting an existing foundation model to a specific task via reinforcement\nlearning (e.g., Stiennon et al. [2020]). Instead, one might imagine more sophisticated versions of\ncurrent methods which acquire a diverse range of real-world capabilities from raw online [Klyubin\net al. 2005; Singh et al. 2005; Salge et al. 2013; Shakir Mohamed 2015; Florensa et al. 2017; Pathak\net al. 2017; Haber et al. 2018] or offline [Precup et al. 2000; Lange et al. 2012; Ajay et al. 2021; Yang\nand Nachum 2021; Schwarzer et al. 2021] interactions, without the need for human annotations or\ntask construction. Such methods might use techniques quite similar to existing SSL algorithms:\ne.g., training sequence models in goal-directed contexts where they can be directly asked to carry\nout certain tasks via conditioning (e.g., UDRL [Schmidhuber 2019; Srivastava et al. 2019] or Decision\nTransformer [Chen et al. 2021b]; also see §2.3: robotics). The complex behaviors that have already\nemerged in simple interactive environments [Baker et al. 2020] suggest multitask, multiagent, and\nmultimodal goal-directed training of foundation models as an interesting avenue for future study.\n56Including any undesirable or biased capabilities (§5.1: fairness)\nOn the Opportunities and Risks of Foundation Models 85\n4.3 Adaptation\nAuthors: Xiang Lisa Li*, Eric Mitchell*, Sang Michael Xie, Xuechen Li, Tatsunori Hashimoto\nFig. 18. During adaptation, a foundation model is converted into an adapted model (bottom row) in order to\nreflect updated information, desired behaviors, or deployment constraints.\nWhile foundation models provide a powerful general-purpose engine for processing multi-modal\ninformation, adapting a foundation model before use is necessary for some applications. Broadly, an\nadaptation procedure produces an adapted model by conditioning a foundation model on additional\ninformation, either by priming the foundation model through the inclusion of new data or a\nprompt in its input or by updating some or all of the foundation model’s parameters to reflect\nthe new information. For example, in text summarization, appending a prompt such as TL;DR\nto the input article can improve foundation model performance [Radford et al . 2019] by acting\nas a task specification for the foundation model. Alternatively, fine-tuning the parameters of a\nfoundation model with an organization’s internal, domain-specific data could improve the model’s\naccuracy by adding information relevant to the organization’s use case. In this section, we describe\nexisting approaches to adaptation and several factors that determine whether a particular adaptation\nprocedure is appropriate for a particular setting. We additionally describe various use cases for\nfoundation model adaptation, including relatively well-studied settings such as specialization of a\nfoundation model to a particular task or domain as well as more speculative settings like test-time\ndata removal [Bourtoule et al. 2019] and editing model behavior on particular inputs [Sinitsin et al.\n2020]. We conclude by presenting a long-horizon goal for future research in foundation model\nadaptation.\n4.3.1 Methods for foundation model adaptation.\nMany methods for adapting foundation models have been proposed, making the decision of\nwhich adaptation procedure to use for a particular problem or compute environment difficult. We\nemphasize three factors of particular importance for practitioners to consider when selecting an\nadaptation procedure: (1) the compute budget (specifically storage and memory); (2) the amount of\ntask-specific data available; and (3) extent of access to foundation model gradients.\n86 Center for Research on Foundation Models (CRFM)\nFactor 1: Compute budget. For foundation models with billions or trillions of parameters, fine-\ntuning all model parameters may demand prohibitively large memory. Also, separately fine-tuning\nfor many tasks can incur unacceptable storage costs. There are many works that propose methods\nto reduce the storage for adapting foundation models, and we refer to this class of lightweight\nadaptation methods as low-storage adaptation. Typically, methods in this class freeze most of the\npretrained foundation model parameters and only learn a relatively small number of task-specific\nparameters (either by fine-tuning some pretrained parameters or by adding altogether new modules),\nreducing the storage overhead for each task.\nThe key design decision for such algorithms is the choice of parameters to be adapted. Perhaps\nthe simplest approach is to tune only the final layer of the pretrained model, while other works tune\nonly the model’s bias vectors [Zaken et al. 2021], low-rank residuals to model weight tensors [Hu\net al. 2021], or masks over model parameters [Zhao et al. 2020b]. Another line of research tunes “soft”\nprompts [Li and Liang 2021; Qin and Eisner 2021; Liu et al. 2021e; Lester et al. 2021; Hambardzumyan\net al. 2021], corresponding to sequences of arbitrary parameter vectors rather than embeddings of\nthe model’s vocabulary, and conditions the foundation model on these prompts by concatenation\nwith input activations, either at the input layer or at all layers. Another approach freezes all model\nparameters and interleaves new MLP modules with trainable parameters between existing model\nlayers [Houlsby et al . 2019]. While these lightweight adaptation techniques seem to trade off\nparameter efficiency and performance on downstream tasks, they sometimes achieve comparable\nperformance to full fine-tuning, despite updating 1000×fewer parameters [Zaken et al. 2021; Li\nand Liang 2021; Hu et al. 2021]. Lester et al. [2021] shows an instance when the performance gap\nbetween full fine-tuning and lightweight adaptation vanishes as the model size increases. We remain\nuncertain how lightweight adaptation techniques scale as model size increases [Aghajanyan et al.\n2020]. Because GPU memory is typically a more limiting resource than disk storage, low-memory\nadaptation procedures are perhaps even more critical to democratizing foundation models than\nlow-storage adaptation procedures. Various techniques for low-memory model training have been\nproposed, which can be directly applied to fine-tuning-based adaptation procedures (§4.5:systems).\nHowever, some low-memory procedures such as gradient checkpointing [Chen et al. 2016] trade\noff computation and memory, potentially exacerbating the significant energy consumption of\nfoundation models [Bender et al. 2021].\nFactor 2: Data availability. Task specialization mostly demands task-specific labeled data as\ntraining signals.57 However, the cost of annotation varies greatly across tasks and languages; for\nexample, annotating MRI data requires expert medical knowledge, whereas labeling sentiment\nfor English texts requires only common sense judgement. When adaptation data is plentiful, we\nmight apply traditional fine-tuning approaches or their lightweight counterparts. In low-resource\nlanguage-based settings, combining prompting and fine-tuning has been shown to be a promising\ndirection [Schick and Schütze 2021a,b; Gao et al. 2020c; Perez et al. 2021; IV et al. 2021; Min et al.\n2021]. Le Scao and Rush [2021] shows that a well-tuned prompt can be worth around 100 training\nexamples, and fine-tuning a carefully prompted foundation model is significantly more data-efficient\nthan fine-tuning an unconditioned foundation model.\nFactor 3: Access to foundation model gradients. Despite the significant impact of foundation\nmodels on some research communities, little standardization of distribution practices exists for large\nscale foundation models (with more than 50 billion parameters). As we gradually become aware of\nthe potential hazards from the misuse of foundation models (see §5.2:misuse), providing access to all\n57Prompts are an exception, although we might consider prompts to implicitly represent the information contained in a\nbatch of labeled data [Le Scao and Rush 2021].\nOn the Opportunities and Risks of Foundation Models 87\nof a foundation model’s parameters for fine-tuning might result in ethical concerns. Moreover, most\nusers do not have enough compute resources to exploit their full access. For example, the memory\nrequirements of foundation models might preclude their direct fine-tuning for many organizations\nand institutions. Consequently, future foundation model providers would likely restrict access to\nthe full parameters of the model and instead provide surrogate API access, as exemplified by an\nearly foundation model, GPT-3. On one extreme, if a foundation model provider enables access\nonly to the model output (e.g., the textual continuation of a prompt, the generated image, or a score\nevaluating the alignment between an image and a textual description), foundation models can be\nadapted using in-context learning [Brown et al. 2020]. In-context learning freezes the foundation\nmodel parameters, and steers the output of the foundation models by conditioning on a (typically\nnatural language) prompt, which might be composed of task instructions or demonstrations. To\nimprove the performance of in-context learning, the prompt needs to be carefully designed, either\nby manual search or an automated procedure [Jiang et al. 2020; Shin et al. 2020], and validated on the\nadaptation data. At the other extreme, if the foundation model provider grants access to gradients\nwith respect to model parameters, full fine-tuning can be applied, where all model parameters are\nupdated to improve performance on a downstream task. As a middle ground, we might obtain\ngradient access only to foundation model inputs,58 which are considerably lower dimensional than\nfoundation model parameters. In this case, we could deploy lightweight adaptation techniques [Liu\net al. 2021e; Li and Liang 2021; Lester et al. 2021], which freeze the model parameters and optimize\na continuous prefix or prompt for each task.\n4.3.2 Use cases for adaptation.\nAdaptation is useful whenever the desired use case of a model differs from the relatively general\ntraining objective used for foundation model training (§4.2: training). Most commonly considered\nis the case in which a foundation model is adapted to perform a specific task (e.g., text summarization\nor animal classification from images), narrowing the scope of the model. Indeed, the vast majority\nof existing approaches described earlier in this section have targeted this setting. However, other\nforms of adaptation are useful, such as making local model edits to correct undesirable predictions\nfor particular inputs or adding privacy constraints to the trained foundation model, which are\ntask-agnostic. In this subsection, we describe a variety of use cases for adaptation, the methods\nthat are most applicable to them, and remaining challenges in addressing these settings.\nTask specialization. The most widely-studied case of foundation model adaptation is that of task\nspecialization, in which a foundation model is adapted to optimize performance for a specific task\nor set of tasks. For example, specializing for summarization tasks would induce foundation model\nbehavior to extract key ideas from the input document(s) and re-organize them in short summary\nsentences. Various adaptation procedures have proven effective for task specialization, showing\nsignificant improvement over performance of the unadapted model [Howard and Ruder 2018;\nBrown et al. 2020]. In addition to the relatively widely-studied setting of specializing foundation\nmodels to specific tasks, other task-agnostic adaptation problems become increasingly challenging\n(but no less important) for foundation models owing to their size and computational demands.\nFor example, the cost of training foundation models makes continual training over time to keep\na model’s predictions up to date with current events particularly expensive. Additionally, the\nchallenge of collecting massive anonymized datasets used to train foundation models (§4.6: data)\nmake the likelihood of personal information leakage into training sets non-trivial; mechanisms to\nefficiently remove training data from a foundation model post-facto are therefore desirable.\n58Assuming the foundation model provider enables the input space to be continuous.\n88 Center for Research on Foundation Models (CRFM)\nTemporal adaptation. Ideally, foundation models store knowledge that closely represents the\nstate of the world, independent of modality. However, the world is constantly changing; new heads\nof state are elected, clothing styles change, social norms and beliefs shift (§5.6: ethics), and the use\nof language evolves, causing a shift in the input distribution, target predictive distribution, or both.\nThis temporal shift presents a challenging statistical problem due to the induced distribution shift,\nas discussed in §4.8: robustness. For foundation models, temporal shift also presents a particu-\nlarly difficult computational problem; due to the computationally demanding nature of training\nfoundation models [Shoeybi et al. 2019; Brown et al. 2020], frequent re-training from scratch might\ncarry unacceptable financial or environmental impacts [Bender et al. 2021] (§5.3: environment),\nor simply take too long to be a viable method for keeping models up to date. In visual domains,\ngradual self-training on unlabeled data across intermediate time points can bridge the temporal\nshift across a long time period, but remains an expensive retraining procedure [Kumar et al. 2020a].\nIn the context of language models, temporally-partitioned diagnostic datasets help quantify the\nrate at which large language models become outdated [Lazaridou et al. 2021; Hombaiah et al. 2021;\nDhingra et al. 2021], showing that classic techniques like re-weighting training data and dynamic\nevaluation (updating model parameters with new data at production time [Mikolov et al. 2010])\ncan partially alleviate, but not fully solve, this problem. Explicitly conditioning a language model\non the time period it is intended to model is another technique that has shown some promise\n[Dhingra et al. 2021]. An alternative strategy to addressing temporal shift is to design retrieval-\nbased (semi-parametric) models, which augment the model input with additional context retrieved\nfrom a large, human-interpretable database (e.g., Wikipedia articles) [Karpukhin et al. 2020; Lewis\net al. 2020b; Guu et al. 2020; Khandelwal et al. 2020; Khattab et al. 2020]. For retrieval-based models,\nadaptation corresponds to updating individual units of information in the database (e.g., single\nparagraphs of encyclopedia articles) without re-training the model. While promising, challenges\nfor retrieval-based approaches remain both in training an accurate retrieval mechanism and in\naccurately conditioning on the retrieved information [Lewis et al. 2020b]. We revisit the problem\nof temporal adaptation in the more general context of continual learning later in the section.\nDomain specialization. In addition to task specialization, it is often necessary to specialize a\nfoundation model to a particular domain (such as legal documents), without limiting the breadth of\ntasks the foundation model can accomplish. This specialization induces a mismatch between the\nfoundation model training and adaptation data distributions (§4.8: robustness) which may require\nnew adaptation methods to handle. Prior works have found that diverse and general pretraining\ndata can cause negative transfer for current adaptation methods. For example, Cole et al. [2021]\nshows that fine-tuning a model pretrained only on the iNaturalist animal classification dataset\nprovides better downstream performance than fine-tuning a model pretrained on iNaturalist along\nwith 750K other images; similarly LegalBERT [Chalkidis et al . 2020], which is pretrained only\non legal documents, improves over BERT [Devlin et al. 2019], which is trained on a much more\ndiverse training set on the downstream task of text classification and sequence tagging in legal\ndocuments. One approach to domain specialization is to include an intermediate adaptation step,\nwhere the foundation model continues training on unlabeled data from the specialized domain. For\ninstance, this approach improves the downstream performance significantly for satellite images and\nspecialized text topics [Reed et al. 2021; Gururangan et al. 2020]. However, continual foundation\nmodel training may perform worse than re-training from scratch in certain domains such as legal\ndocuments [Chalkidis et al. 2020]. Elucidating the scenarios in which continual training does or\ndoes not benefit performance is an important direction for future work.\nLocal model editing. In some settings, it is useful to adapt a foundation model locally, mean-\ning that the model’s predictive distribution should be adapted only for a single input or a local\nOn the Opportunities and Risks of Foundation Models 89\nneighborhood around a single input, without changing the model’s behavior for unrelated inputs.\nFor example, when a foundation model produces an especially problematic mistranslation for a\nparticular input phrase and target language, it is desirable to correct this mistranslation without\naffecting the model’s behavior for unrelated phrases. Past work has studied the problem of applying\napproximately localized updates to large neural networks through new pretraining objectives that\nenable easy editing with standard gradient descent [Sinitsin et al. 2020], higher-order networks\nthat predict parameter edits for an underlying model [Cao et al. 2021; Mitchell et al. 2021; Hase\net al. 2021], and constrained fine-tuning procedures [Zhu et al. 2020]. However, existing methods\nvary in the reliability with which they can perform model edits without damaging global model\nperformance. Furthermore, scaling these methods to massive foundation models is not straightfor-\nward due to their size and the computational cost of training objectives that require computing\nhigher-order gradients [Sinitsin et al. 2020; Cao et al. 2021; Mitchell et al. 2021].\nApplying constraints. There are settings in which foundation models need to be adapted to satisfy\nprivacy constraints. For instance, Carlini et al. [2021] demonstrated that existing foundation models\nare able to memorize sensitive information in the training data and can regurgitate such data when\nqueried via standard APIs. While this phenomenon calls for improved data curation, developing\nadaptation procedures which eliminate or reduce the influence of specific data examples on the\ntrained model would be a complementary solution. Improved adaptation strategies (along with\nbetter pretraining methods) in this direction will also benefit institutions working with foundation\nmodels under the General Data Protection Regulation (GDPR), as the mandate gives users the right\nto be forgotten . While research on the topic of machine unlearning [Bourtoule et al. 2019; Cao and\nYang 2015] has started to gain traction, the problem has not yet been studied in depth for foundation\nmodels. In addition, foundation models trained on less curated internet data have been shown to\nexhibit harmful biases targeting specific groups (e.g., gender and racial bias) [Bender et al. 2021;\nBasta et al. 2019; Kurita et al. 2019; Sheng et al. 2019] and can produce toxic outputs [Gehman et al.\n2020] (§5.2: misuse). While strategies such as further fine-tuning the foundation model on carefully\ncurated datasets (for potentially multiple generations) [Solaiman and Dennison 2021] or applying\ncontrollable generation techniques [Keskar et al. 2019] have shown some success in mitigating\nharmful behavior, a framework for training equitable and safe foundation models (§5.1: fairness)\nwill likely require further research with a collective effort across the data collection, training, and\nadaptation phases as well as consultation with domain experts.\n4.3.3 A long-term goal for foundation model adaptation research.\nTo the extent that adaptation is concerned with efficiently integrating a model’s existing knowledge\nwith new data or objectives, a natural extension of adaptation is continual learning [McCloskey\nand Cohen 1989; Parisi et al. 2019] or continual adaptation. The ability to adapt a foundation model\ncontinually is desirable, whether to keep a model’s knowledge continually up-to-date with world\nevents or cultural developments, continually add data from completely new domains or modalities\nas they become available, or continually edit a model’s memories to comply with privacy or legal\nconstraints as a society’s values or laws evolve. However, continual learning problems typically\ninduce catastrophic forgetting [McCloskey and Cohen 1989; Ratcliff 1990; Kirkpatrick et al. 2017] in\nneural networks, where old tasks or data are rapidly forgotten as the training distribution changes.\nWe consider continual adaptation of a foundation model as a grand challenge for future foundation\nmodel adaptation research. Tackling this challenge requires closing the performance gap between\na foundation model trained continuously on a non-stationary stream of data from different tasks,\ndomains, or time periods and the same foundation model trained from i.i.d. data from the aggregate\nmixture [Lopez-Paz and Ranzato 2017]. Innovations in both model architectures and training\n90 Center for Research on Foundation Models (CRFM)\nobjectives are likely to be necessary in order to do so. For example, while memory mechanisms have\nlong been speculated as key to successful continual learning [French 1999] and have shown some\npromise for foundation models [Lewis et al. 2020b; Guu et al. 2020; Borgeaud et al. 2021], effectively\nleveraging updated memories remains a challenging problem [Zhang and Choi 2021]. In addition,\ntechniques for localizing knowledge in a foundation model in order to make targeted parameter\nupdates [Dai et al. 2021a] or learning such update rules [Cao et al. 2021; Mitchell et al. 2021] may\nhelp prevent forgetting, but repeated application of such updates still induces significant forgetting\n[Hase et al. 2021]. Continual foundation model adaptation may also require new understanding of\nhow the problem of catastrophic forgetting manifests at the scale of foundation models as well as\ndeveloping new meta-learning techniques [Schmidhuber 1987; Santoro et al. 2016; Finn et al. 2017]\nto improve forward transfer of already-learned information to new settings.\nContinually training on experiences gathered by deployed foundation models, or even training\non aggregated experiences gathered by many different models, may speed the rate of progress of\nfoundation model development, but incur risks of feedback loops and eroded alignment of model\nbehavior and stakeholder values. Despite the aforementioned challenges, continual foundation\nmodel adaptation holds the promise of more rapidly responding to shifts in socio-cultural values,\nbetter leveraging existing knowledge to learn new concepts, lessening the environmental impact\nand increasing the accessibility of foundation models by eliminating the computational burden of\ntraining from scratch, and reducing the extent that previously-learned concepts must be re-learned\ndue to forgetting.\nOn the Opportunities and Risks of Foundation Models 91\n4.4 Evaluation\nAuthors: Rishi Bommasani, Kawin Ethayarajh, Omar Khattab\n4.4.1 Introduction.\nEvaluation gives context to machine learning models: it serves as a means for (1) trackingprogress —\nhow do we we measure the performance of models and how do we design improved models\n(§4.1: modeling); (2) understanding — what behaviors do models exhibit (§4.11: interpretability)\nand how do they perform on different slices of data (§4.8:robustness); and (3)documentation — how\ndo we efficiently summarize model behavior and communicate this to diverse stakeholders. For\nfoundation models, each of these purposes for evaluation are critical but the nature of foundation\nmodels introduces new challenges that are not generally encountered in other AI or ML settings:\n(1) Tracking progress requires relative comparison, but comparing foundation models is compli-\ncated by the fact that foundation models must be adapted (potentially in different ways) to\nperform tasks.\n(2) Understanding requires specified in-advance knowledge (e.g., taxonomies) of what is being\nevaluated for, but foundation models acquire emergent skills (e.g., in-context learning) that\nwill be difficult to anticipate in designing evaluations.\n(3) Documentation requires clear desiderata to meaningfully inform decision-making, but foun-\ndation models can be adapted for myriad applications, which makes comprehensive docu-\nmentation challenging.\nTo orient the discussion of evaluating foundation models, we distinguish two classes of evaluation\nthat arise from the abstraction of foundation models: intrinsic evaluation of the foundation model,\nwhich is inherently divorced from a specific task due to the task-agnosticity of these models, and\nextrinsic evaluation of task-specific models, which is necessarily dependent on both the foundation\nmodel and the adaptation mechanism. Further, we recognize that due to the anticipated impact\nand scope of foundation models, a variety of stakeholders (e.g., foundation model providers and\napplication developers, auditors and policymakers, practitioners and researchers) will require\nevaluation of both foundation models and task-specific derivatives, with these evaluations serving\ndifferent purposes and involving different desiderata based on the stakeholder. With this in mind,\nstandard paradigms for the evaluation of machine learning models are not designed explicitly for\nthe setting of foundation models. Therefore, we emphasize intrinsic evaluation (§4.4.2:evaluation-\nintrinsic), the importance of adaptation in extrinsic evaluation (§4.4.3: evaluation-adaptation),\nand evaluation design (§4.4.4: evaluation-design) as clear steps towards an evaluation frame-\nwork that is better suited to foundation models. This discussion contributes to broader dialogue\nsurrounding the role of evaluation of machine learning systems [Galliers and Spärck Jones 1993;\nLipton and Steinhardt 2019; Ribeiro et al. 2020; Linzen 2020; Kiela et al. 2021; Milli et al. 2021; Jacobs\nand Wallach 2021; Bowman and Dahl 2021; Dehghani et al. 2021; Ma et al. 2021a, inter alia ] and,\ngiven the complexities of evaluation, may benefit from drawing upon theories of measurement\nand evaluation that exist beyond machine learning [Messick 1987; Jackman 2008; Loevinger 1957;\nMessick 1988; Hand 2010; Brewer and Crano 2014].\n4.4.2 Intrinsic evaluation.\nEvaluation of machine learning systems has traditionally been grounded in tasks, often ones that\nare envisioned as functions specifically useful for applications (e.g., translation, object recognition).\nIn contrast, since foundation models are intermediary assets that must be further adapted or\nspecialized to perform useful tasks, the standard evaluation paradigm must be altered to facilitate\nthe direct understanding and comparison of foundation models.\n92 Center for Research on Foundation Models (CRFM)\nOne approach is to evaluate foundation models in terms of the task associated with the training\nobjective. For example, a language model like GPT-3, which was trained by predicting the next word\ngiven the preceding context, may be evaluated based on the probabilities it assigns words given\ntheir preceding context in held-out test data (i.e., perplexity on language modelling benchmarks\nlike LAMBADA [Paperno et al . 2016]). This approach has shown promise in NLP thus far, but\nwe identify two fundamental limitations it exhibits. First, relying on the training objective for\nevaluation lacks generality: foundation models trained using different incompatible objectives\ncannot be readily compared or understood in a consistent frame. Second, evaluation in this way\nrelies upon a proxy relationship to be meaningful, i.e., measurements in terms of the training\nobjective should correlate with other more meaningful and intelligible quantities (e.g., the quality\nof content generated via a foundation model). While this proxy relationship has proven to be robust\nin the past in some contexts, it likely will break down when assessing more diverse capabilities of\nfoundation models, their behavior in more diverse environments or domains, and considerations\nbeyond in-domain accuracy (we discuss this more extensively in §4.4.4: evaluation-design). In\nlight of these limitations, we anticipate that two approaches will need to be considered, offering\ncomplementary benefits.\nImputing intrinsic evaluation from broad extrinsic evaluation. One route towards evaluat-\ning foundation models is to adapt them to a wide range of tasks and measure the performance\nof the resulting task-specific models. As the foundation model is the shared basis across all of\nthese models, the performance in aggregate reflects on the nature, and quality, of this shared\nbasis. At present, many subareas of AI have begun to construct meta-benchmarks, i.e., a single\nevaluation that consolidates individual evaluations across a number of different tasks or domains\n[Wang et al. 2019b,a; Hu et al. 2020; Santurkar et al. 2020; Gehrmann et al. 2021; Hendrycks et al.\n2021b; Koh et al. 2021; Tamkin et al. 2021b]. Given the growing adoption of this paradigm and\nits established strengths, here we note why it is likely insufficient to fully satisfy the goals of\nevaluations with respect to foundation models. Meta-benchmark evaluation requires adaptation\n(minimally to specialize the foundation model to each of the tasks in the meta-benchmark), which\nmakes reasoning about the foundation model itself challenging given the addition process (i.e., adap-\ntation) involved. Specifically, this complicates matters of progress, both in terms of tracking (e.g., is\nperformance attributable to potent foundation models or well-designed adaption practices) and\nin terms of identifying improvements in the process used to learn foundation models (e.g., fun-\ndamental improvements in data selection (§4.6: data), training objectives (§4.2: training), and\nmodel architectures (§4.1: modeling) may be difficult to identify by comparing the performance on\na meta-benchmark between two foundation models). In addition, this evaluation paradigm makes\nit difficult to understand or document properties and capabilities specific to the foundation model,\nwhich may make it unwieldy to convey to certain stakeholders (e.g., SuperGLUE performance\nmay not be sufficiently informative, or may be misleading, for policymakers) or use as grounds for\nanticipating their behavior for new tasks or domains.\nDirect evaluation of intrinsic properties. To complement the use of meta-benchmarks, we also\nargue for why measuring the properties (e.g., specific capabilities or biases) of foundations models\ndirectly is valuable, divorced from specific tasks. 59 For example, we may endeavor to directly\nmeasure the linguistic capabilities of foundation models to identify syntactically valid and invalid\nsentences. To motivate the value of this approach, we return to the purposes for evaluation. Notably,\n59Strictly speaking, these direct evaluations may still involve formulation as a task and foundation model specialization\nto perform the task, but the objective is more akin to probing (see §4.11: interpretability) of trying to measure the\nfoundation model as directly as possible.\nOn the Opportunities and Risks of Foundation Models 93\narticulating the presence and intensity of capabilities, skills, and biases identifies concrete areas for\nimprovement (progress), elucidates the current potential (understanding), and expresses relevant\naspects efficiently (documentation). Such an approach also is in service of broadly comprehensible\nevaluation, i.e., evaluation that can be understood by both technical experts, non-technical experts\n(e.g., policymakers or social scientists) and the general purpose. For example, characterizing the\npersuasive or rhetorical capabilities of these models may especially intuitive for internalizing their\npotential for disinformation and misuse (§5.2: misuse) [Buchanan et al. 2021].\nDirect evaluation of properties also serves as an important pathway towards better handling of the\nemergent properties of foundation models; to demonstrate this, we take in-context learning as a case\nstudy. In particular, Brown et al. [2020] not only demonstrated GPT-3’s signature capability of robust\nin-context learning, but also were the first to specifically identify in-context learning as a specific\nway to adapt and interact with models (through their exploration of GPT-3). Traditional task-based\nextrinsic evaluation does not provide a clear means by which in-context learning could have been\nidentified; directly interacting with the foundation model appears to be necessary in this case. More\ngenerally, while it appears inevitable that many unanticipated phenomena like in-context learning\nwill be recognized through the unstructured or loosely structured exploration of these models and\ntheir capabilities, we believe new approaches to evaluation should be sought out that structure\nthis exploration or, more ambitiously, suggest new properties that can then be more rigorously\ntested for. Intrinsic evaluation may also lower the threshold for demonstrating the potential of\nfoundation models; new approaches for foundation models may be sufficiently promising if they\ndemonstrate improvements in intrinsic evaluation, even if they are not immediately accompanied by\ncorresponding well-suited adaptation methods for eliciting these capabilities in extrinsic evaluation.\nThere is a significant open question of how intrinsic evaluation should be implemented; the\nmechanics of such evaluation are unclear. We enumerate a few general principles and considerations\nthat may help inform the design and execution of intrinsic evaluation.\n(1) Inspiration from evaluation of humans. Many of the relevant properties, capabilities, and\nbiases we are interested in for foundation models are also of interest for humans, which\nsuggests that methods for measuring these properties in humans may prove to be instructive,\nor even directly translatable, for evaluating foundation models. For example, psycholinguistic\nmeasures of human linguistic competencies can be modified to evaluate foundation model\nlinguistic competencies [Levy 2008; Frank et al. 2013; Linzen et al. 2016; Ettinger and Linzen\n2016; Marvin and Linzen 2018; van Schijndel and Linzen 2018; Futrell et al. 2019; Prasad et al.\n2019; Ettinger 2020] or psychological measures of human social biases can be modified to\nevaluate foundation model social biases [Greenwald et al . 1998; Caliskan et al. 2017; May\net al. 2019; Guo and Caliskan 2021].\n(2) Human-in-the-loop evaluation. Human-in-the-loop evaluation may prove to be critical to\nprovide a more exploratory means for understanding foundation models, including assessing\ntheir generative or interactive capabilities. In particular, human interaction with foundation\nmodels directly may better identify their emergent capabilities and limitations and direct\nauditing of foundation models [e.g., Raji and Buolamwini 2019, §5.6: ethics] may advances\ngoals for documentation and transparency.\n(3) Validity of intrinsic measures. While intrinsic measures allow for direct measurement at the\nsource, i.e., measurement and evaluation of the properties of a foundation model independent\nof adaptation and specific tasks, they pose challenges for building trust in the validity\n[Messick 1987, 1988] of the evaluation. In particular, extrinsic evaluation outcomes may also\nbe important in validating intrinsic measure design, e.g., the predictive validity of intrinsic\n94 Center for Research on Foundation Models (CRFM)\nmeasures (i.e., their ability to (statistically) predicted related downstream outcomes) may\nprove to be a central criterion.\n4.4.3 Extrinsic evaluation and adaptation.\nEvaluating task-specific models has historically involved reporting the performance (generally\nmeaning the accuracy) of the model on a specific held-out test set. While this paradigm may partially\nsuffice to understand or document a model, it often amounts to unfair comparisons between task-\nspecific models produced with different (and, potentially, unequal) resources, making it difficult\nto gauge how much progress has been made. The concern of unfair comparisons is exacerbated\nin the foundation model regime: different foundation models (e.g., BERT and GPT-3) may form\nthe foundation for different task-specific models, and these foundation models may involve vastly\ndifferent amounts of training data and computation.\nTo account for the resources required to achieve specific levels of performance, Linzen [2020]\nargues that (pre)training resources should be acknowledged and tracked in evaluation. We believe\nthis is a scientifically principled proposal; comparing different approaches for training foundation\nmodels without accounting for training resources is likely to be misleading. However, given that\nthe process for creating foundation models is especially expensive (e.g., requiring significant human\nand financial capital), and often governed by societal factors (e.g., commercial incentives) in addition\nto scientific factors, it may be the case that the foundation models in practice will vary greatly\nin the training resources afforded, making controlled comparison difficult. Here, we consider an\nalternative, which may be more pervasively viable, to partially account for the resources involved\nto complement the proposal of Linzen [2020]. In particular, we consider why extrinsic evaluation\nshould acknowledge adaptation resources, which is critical for ensuring that extrinsic evaluation is\nable to identify the most performant adaptation methods (which intrinsic evaluation, fundamentally,\ncannot do). We draw attention to the fact that adaptation resources often are construed as the data\nused to adapt models, but additional resources [e.g., data used to choose adaptation methods; Perez\net al. 2021] and constraints (e.g., the level of access required to adapt the foundation model; see\n§4.3: adaptationand §5.6: ethics for further discussion) should also be accounted for.\nAccounting for adaptation resources. Accounting for the resources expended to adapt founda-\ntion models for specific tasks requires a complete understanding of what resources or constraints are\nused for different adaptation methods, i.e., evaluations that endeavor to account for these resources\nmust evolve alongside developments in what resources are used in adaptation (§4.3: adaptation).\nIn existing task-specific evaluations, most evaluations specify the amount of data that can be used\nto adapt a (foundation) model to the task. However, Perez et al. [2021] identify a key nuance here\nthat has been discounted in past work, in that this should encapsulate all data used to inform\nadaptation, i.e., both the data used to adapt the foundation model and the data used to choose the\nadaptation method. Further, in the foundation model regime, the notion of access requirements for\ndifferent adaptation methods is also a new consideration that should be factored into evaluation.\nConcretely, some adaptation methods may generally outperform others but may require greater\nability to access or modify the foundation model compared to others (e.g., fine-tuning requires\nfoundation model gradients to modify a foundation model, whereas prompting may only require\nblackbox access in specifying inputs).\nAccounting for the resources involved in adaptation enriches what conclusions can be reasonably\ndrawn from evaluation of task-specific models. At present, task-specific evaluation may provide\nsufficient clarity for certain types of understanding or documentation of particular task-specific\nartifacts (i.e., the exact models being evaluated) but do not provide clear signal for how different\nadaptation methods perform and how to select a specific adaptation method in a given context. In\nOn the Opportunities and Risks of Foundation Models 95\ncontrast, by accounting for the resources and access requirements involved in adaptation, evaluation\nbetter enables research to identify which adaptation methods or processes make best use of the\nresources provided, i.e., signal is offered not just for the specific artifacts being evaluated but the\nmore general processes by which they were derived. The proposed evaluation protocol, therefore,\nclearly works towards identifying which adaptation methods should be used; we note that all of\nthese conclusions should always be taken as specific to a given foundation model, as evaluation in\nthis form does not provide sufficient evidence to conclude an adaptation method is uniformly the\nbest across foundation models.60\n4.4.4 Evaluation design.\nIn theory, the goal of evaluation is to measure and characterize various theoretical constructs\n(e.g., accuracy, robustness (§4.8: robustness), fairness (§5.1: fairness), efficiency (§4.5: systems),\nenvironmental impact (§5.3: environment)) in service of various purposes (i.e., progress, un-\nderstanding, documentation). However, in practice, the utility of evaluation will be determined\nby how evaluations are designed and executed. For example, automated measurements of the\ngenerative capabilities of foundation models (e.g., their factual correctness) may poorly capture the\nnature of these qualities and, instead, human-in-the-loop evaluation may better contextualize these\ncapabilities.\nIn considering the evaluation design we envision for foundation models and their adapted\nderivatives, we begin with the mechanics of evaluation. Traditionally, the evaluation of machine\nlearning models has involved a large training set that is used to learn the model, an optional\nvalidation set that is used to set hyperparameters, and a test set to evaluate the generalization of\nthe learned model to held-out data [Bishop 2006]. As a result, creating benchmarks to evaluate\nmodels has historically required large amounts of data, most of which is allocated towards training,\nwhich complicates the design of certain diagnostic or nuanced evaluations when data is scarce or\nexpensive to attain [Rogers 2020, 2021]. In contrast, because the benefits of foundation models will\noften coincide with the sample efficiency of adaptation (i.e., few-shot or zero-shot capabilities) and\nthe diversity of possible applications, we instead envision a regime where benchmarks for individual\ntasks are much smaller (since far less data needs to be provided as “training\", i.e., adaptation, data)\nand are far more diverse (both to capture various capabilities in intrinsic evaluation and more\nstrongly ground evaluation in ecologically valid ways [Bronfenbrenner 1977; de Vries et al. 2020]\nduring extrinsic evaluation). This suggests that the nature of foundation models may cause a shift\nin nature of benchmarks (and the mentality of those constructing benchmarks), de-emphasizing\nquantity as a key priority in benchmarks as opposed to quality and diversity. The NLP community\nhas begun to see the beginnings of such a regime with expansive and diverse benchmarks like\nBIG-Bench61 and FLEX [Bragg et al. 2021]; this paradigm lowers the barrier for benchmark design,\nthereby enabling the broader community to partake in evaluation design.62\nAlongside the mechanics of evaluation, the presentation of and interface to the evaluation results\ninforms how these results will be used inform decision-making (e.g., new modelling approaches,\nmodel selection, auditing). Leaderboards have become the de facto paradigm in machine learning,\nwhereby models are ranked by a specific and singular criterion (generally a form of accuracy). This\napproach has generally led to significant and rapid progress in system quality over time [e.g., Wang\n60Current results, instead, suggest that different adaptation methods are better-suited to different types of foundation\nmodels and training objectives [Liu et al. 2021e; Lester et al. 2021].\n61https://github.com/google/BIG-bench\n62Traditionally, the design of benchmarks like ImageNet [Deng et al. 2009] and SQuAD [Rajpurkar et al. 2016] has been\nconducted by high-resourced research labs that can afford to pay for the creation of these datasets through crowdsourcing\n[Rogers 2020].\n96 Center for Research on Foundation Models (CRFM)\net al. 2019a], but significant concerns have been raised of whether this yields more general im-\nprovements [e.g., Linzen 2020; Bowman and Dahl 2021]. 63 As is true for all machine learning\nmodels, it is rarely the case that the desiderata for foundation models and their derivatives will be\nsingular; instead, we anticipate the breadth of their application and societal impact necessitates\nheightened consideration of criteria beyond accuracy (e.g., robustness, fairness, efficiency and\nenvironmental impact). To this end, we note that evaluation of foundation models should report\nmeasurements across these diverse fronts; existing benchmarks are increasingly designed to reflect\nmore than just accuracy (e.g., robustness [Koh et al. 2021; Goel et al. 2021], fairness [Nadeem et al.\n2021; Nangia et al . 2020], efficiency and environmental impact [Coleman et al . 2017]). Further,\nwe note that if the reporting of performance across this different categories is done in the form\nof a leaderboard, mechanisms to disambiguate potential trade-offs (to induce a ranking) will be\nespecially necessary [Ethayarajh and Jurafsky 2020]. In particular, since different stakeholders\nwill have different preferences (e.g., the weight they ascribe to different properties) and values\n[Birhane et al. 2020], leaderboard design should allow stakeholders to interact and manipulate how\nthe ranking is done to align with their values; Ma et al. [2021a] presents an early attempt to enable\nthis by comparing the utility of models using an economic framing based on a user’s specified\nutility function.\n4.4.5 Takeaways.\nEvaluation performs several roles (i.e., progress, understanding, documentation) that are vital for\nall machine learning paradigms, including the foundation model paradigm. Foundation models\nintroduce new challenges for existing evaluation frameworks; designing evaluations that directly\ntarget the foundation model regime will better serve not only the multiple purposes of evaluation,\nbut also the myriad of stakeholders involved.\n(1) While machine learning evaluation traditionally has considered task-specific models, evalu-\nating foundation models involves engaging with the fact that these models are not specific\nto a task. Evaluation of these models likely will involve integrating two complementary\napproaches: (a) imputing the properties of foundation models from broad evaluation of\ntask-specific derivatives and (b) direct measurement of these properties in foundation models.\n(2) Existing evaluation frameworks often do not account for the resources required to create the\nmodels being evaluated, leading to unfair comparisons . For foundation models, we discuss an\nevaluation paradigm that emphasizes accounting for adaptation resources (e.g., all data used\nin adaptation, access requirements for the foundation model), which appears to lead to more\ninformative evaluations that better shape how adaptation is conducted.\n(3) Existing evaluation design often is limited in the diversity of metrics considered and requires\nlarge adaptation datasets. For foundation models, we echo growing calls for evaluation to\nconsider a broader range of desiderata (e.g., robustness, fairness, efficiency, environmental\nimpact) to capture the wide range of stakeholder values/preferences, as well highlight how\nthe sample efficiency of adapting adaption models may allow for more diverse evaluations\nby re-allocating resources involved in designing evaluations.\n63We note the connection to Strathern’s Law [Strathern 1997] (sometimes referred to as Goodhart’s Law [Goodhart\n1984]): “When a measure becomes a target, it ceases to be a good measure. \"\nOn the Opportunities and Risks of Foundation Models 97\n4.5 Systems\nAuthors: Deepak Narayanan, Trevor Gale, Keshav Santhanam, Omar Khattab, Tianyi Zhang, Matei\nZaharia\nFig. 19. Plot showing the growth of number of parameters and number of training operations (FLOPs) of\ntransformer-based language models (shown in blue), and memory capacity and peak device throughput\nof NVIDIA P100, V100, and A100 GPUs (shown in red) with time. The rate of growth (slope of each line)\nof state-of-the-art language models (roughly 10×a year) far exceeds the rate of increase in computational\ncapacity of hardware (roughly 10×in four years), motivating the need for parallelism across a large number of\naccelerators and co-design of algorithms, models, software, and hardware to drive further progress. Number\nof parameters and number of training operations are obtained from relevant papers [Brown et al. 2020], and\nmemory capacities and peak throughputs are obtained from GPU specification sheets.\nComputer systems are one of the largest bottlenecks to developing foundation models. Foundation\nmodels are frequently too large to fit in the main memory of a single accelerator (e.g., GPU) and\nrequire an immense amount of computation to train (e.g.,> 1000 petaFLOP/s-days for GPT-3 [Brown\net al. 2020]). Additionally, these models will likely get larger over time: for instance, the compute and\nmemory requirements of state-of-the-art language models have grown by three orders of magnitude\nin the last three years, and are projected to continue growing far faster than hardware capabilities\n(Figure 19). Once trained, these large models are expensive to perform inference with and difficult\nto debug, monitor, and maintain in production applications. We believe that further advances in the\nperformance and usability of foundation models will require careful co-design across algorithms,\nmodels, software, and hardware systems, as well as new interfaces for programming and deploying\nML applications. In this section, we discuss the key computer systems challenges in developing\nand productionizing large-scale foundation models.\n4.5.1 Improving performance through co-design.\nToday, training large-scale foundation models [Brown et al . 2020; Rae et al. 2021; NVIDIA and\nMicrosoft 2021; Wang and Komatsuzaki 2021] can often require custom software systems such as\nMegatron, DeepSpeed, or Mesh Transformer JAX [Shoeybi et al. 2019; Rasley et al. 2020; Wang\n2021], built on top of standard frameworks like PyTorch, TensorFlow, and JAX [Paszke et al. 2019;\nAbadi et al. 2016; Bradbury et al. 2018]. These software systems rely on a number of innovations\nacross the stack to train models efficiently at scale: new parallelization dimensions such as pipeline\n98 Center for Research on Foundation Models (CRFM)\nparallelism [Huang et al. 2019; Narayanan et al. 2019] that limit communication while keeping\ndevices busy, state-sharding optimizers to reduce memory usage [Rajbhandari et al. 2020], just-in-\ntime (JIT) compilers to optimize the computation graph [PyTorch 2021], and optimized libraries\nlike cuDNN and NCCL [NVIDIA 2021]. Megatron and DeepSpeed are efficient to a particular\nscale; for example, Megatron can extract up to 52% of the theoretical peak throughput of modern\nhardware with approximately 3000 GPUs on a model with a trillion parameters [Narayanan et al.\n2021b]. However, scaling to larger models with more GPUs still is challenging, since existing\nparallelization strategies break down at larger GPU counts. Data parallelism is limited by the\nbatch size [Li et al. 2020e], pipeline parallelism by the number of layers in the model [Huang et al.\n2019; Narayanan et al . 2019], and tensor model parallelism by the number of GPUs in a single\nserver [Shoeybi et al. 2019].\nWhile we will continue to realize performance gains from new hardware, growth in the resource\nrequirements of large models far outstrips generational hardware improvements [Brown et al. 2020].\nTo facilitate the next major leap in model capacity and to democratize the advances in model quality,\nit will be increasingly critical to co-design training algorithms, models, software, and hardware,\nbecause many of the avenues to dramatically increase performance alter the semantics of the\ntraining computation. For example, executing operations in lower precision (such asfp16) can help\nincrease throughput on modern hardware (e.g., the V100 and A100 GPUs have dedicated tensor core\nunits for lower-precision matrix multiplication), but also affect the numerics of the optimization\nprocedure [Micikevicius et al. 2017]. Similarly, exploiting weight sparsity can significantly improve\ntraining and inference times [Elsen et al. 2020; Gale et al. 2020] by only performing mathematical\noperations on the non-zeros in the model, but requires different training algorithms [Jayakumar et al.\n2021; Evci et al. 2020; Dettmers and Zettlemoyer 2019]. Other examples of co-design include model\narchitectures that map more efficiently to hardware [So et al. 2019; Child et al. 2019; Wang et al.\n2020c; Lee-Thorp et al. 2021; Kitaev et al. 2020; Beltagy et al. 2020; Tay et al. 2020; Ren et al. 2021],\nefficient optimizers [Anil et al. 2020; Shazeer and Stern 2018], novel tokenization alternatives [Xue\net al. 2021; Tay et al. 2021], specially architected hardware training platforms [Jouppi et al. 2017;\nMudigere et al. 2021; Selene 2021], and distributed parallelization strategies with relaxed weight\nupdate semantics [Narayanan et al. 2019, 2021a].\nCase study: efficient knowledge representation. As a concrete case study of successful co-\ndesign, retrieval-based models such as REALM, RAG, ColBERT-QA, and RETRO [Guu et al. 2020;\nLewis et al. 2020b; Khattab et al. 2020; Borgeaud et al. 2021] take a different approach to model design\nthan simply increasing the number of model parameters. Instead of trying to accumulate implicit\nknowledge from ever-larger datasets directly into a DNN model with billions of parameters (like GPT-\n3), retrieval-based models store knowledgeoutside the model parameters in the form of text passages,\ncapturing knowledge within the passages with dense vector representations. These models then use\nscalable top-𝑘 search mechanisms to extract knowledge pertinent to each input, while keeping the\nDNN model itself small (§4.1.4: modeling-memory). This design improves computational efficiency\nas well as maintainability of the model in production: for example, developers can update the\nknowledge of the model just by replacing a text passage, without needing to retrain a large DNN.\nRetrieval-based models have achieved promising initial results by leveraging several new cross-\nfunctional ideas, including backpropagating the loss through the retriever during training [Guu et al.\n2020] (which requires approximating the gradient through a knowledge store consisting of millions\nof passages) and modeling fine-grained interactions between queries and passages [Khattab and\nZaharia 2020; Khattab et al. 2020] (which requires decomposing the computation into vector-level\nnearest-neighbor search operations). These techniques allow retrieval-based models to be accurate\nOn the Opportunities and Risks of Foundation Models 99\nand efficient, but demand functionality not readily supported by popular ML frameworks and\nnearest-neighbor indexes such as FAISS [Johnson et al. 2019].\n4.5.2 Automated optimization.\nAnother important challenge in systems is to automate the application of optimizations that\nstraddle algorithms, models, software, and hardware. While many optimizations and parallelization\nstrategies are complementary, identifying the most effective combination of optimizations is\nchallenging since the joint search space grows combinatorially and optimizations interact in\nnon-trivial ways [Narayanan et al. 2021b]. Foundation models heighten the need for automated\noptimization as manual experimentation is extremely expensive and time-consuming at the scale\nof thousands of GPUs.\nRecent work in this area has focused on systems targeting semantics-preserving optimizations. In\nparticular, systems have been proposed to automatically discover mathematically-equivalent graph\nsubstitutions [Jia et al. 2019a; Wang et al. 2021c], facilitate the distributed execution of operator\ngraphs through both high-level APIs and low-level compilers [Rasley et al. 2020; Mandeep Baines\n2021; Bradbury et al. 2018; Shazeer et al. 2018; Lepikhin et al. 2020], and automate the selection of\nhybrid distribution strategies [Jia et al. 2019b; Santhanam et al. 2021]. These systems have helped\ndeploy many foundation models in industry [Fedus et al. 2021; M2M-100 2020; Turing-NLG 2020].\nUnfortunately, automated optimization becomes much harder when composing semantics-\naltering optimizations (§4.5.1: systems-co-design), as it is often unclear how to jointly model\nthe statistical impacts of these techniques (e.g., how many training iterations are needed to reach\na specific accuracy?). We will therefore need new software tools, libraries, and compilers to au-\ntomatically identify compositions of optimizations that target comprehensive metrics like time-\nto-accuracy [Coleman et al . 2017; Mattson et al . 2020]. Building such tools will require tight\ncollaboration between systems and machine learning experts.\n4.5.3 Execution and programming models.\nThe unique multi-task nature of foundation models provides an opportunity to amortize training\nand inference costs over many applications. In particular, paradigms such as adaptation mean\nmore sharing across model instances [Raffel 2021]. For example, two models prefix-tuned [Li\nand Liang 2021] from the same pretrained model can share the same model “stem, ” reducing the\nstorage footprint (the shared stem only needs to be stored once), while also making it possible for\nexecution to be shared and batched across the prefix-tuned models [Shen et al. 2019; Narayanan\net al. 2018]. Consequently, the specific adaptation mechanism used informs system optimization\n(§4.3: adaptation).\nIt is an open question as to what programming interface should be used to specify that various\nadapted models are derived from the same pretrained model (e.g., models 𝑌 and 𝑍 are derived\nfrom the same pretrained model 𝑋), or that various components of two models share parameters\n(e.g., two models 𝐴 and 𝐵 share the same stem up to layer 𝑖). Ludwig [Molino et al . 2019] and\nPyTorch’s Module offer easy ways to compose functionality within a model, but no system today\nsupports cross-model dependencies. Giving users the opportunity to provide annotations will allow\ntraining and inference systems to optimize and orchestrate computation more efficiently; without\nsuch annotations, systems will not have visibility into what computation and parameters can be\nshared across model instances. A model’s “adaptation history” (what models is this particular\nmodel adapted from) can also be used for debugging: an adapted model’s errors on particular\ntypes of inputs could originate from the pretrained model, pointing to issues in the pretraining\nprocess versus adaptation process. Frameworks like PyTorch, as well as software libraries for\n100 Center for Research on Foundation Models (CRFM)\ntraining foundation models such as HuggingFace Transformers [Wolf et al. 2020], do not allow for\nfine-grained lineage information across entire model instances to be specified.\nBuilding and maintaining a cluster of thousands of accelerators also requires tremendous effort.\nNew training paradigms like Learning@Home [Ryabinin and Gusev 2020; Diskin et al. 2021] explore\nleveraging volunteer compute over the internet to train foundation models collaboratively. Such\nfundamentally new execution models can decrease the cost of training for any one entity, but\nrequire collaboration across a number of different areas like security (to ensure that a malicious\nvolunteer cannot significantly alter the training process), distributed systems (to deal with fault\ntolerance issues as volunteers drop), and crowdsourcing.\n4.5.4 Productionization of foundation models.\nAs the community continues to push the capabilities of foundation models, realizing their potential\nwill require addressing the challenges associated with deploying these resource-intensive models\nin production. These challenges include performing model inference with tight latency targets, and\nensuring that models and data are monitored in an automated way.\nFor applications with strict cost and latency constraints, model compression techniques like\ndistillation [Hinton et al. 2015; Li et al. 2020d; Sanh et al. 2019], quantization [Polino et al. 2018;\nGholami et al. 2021; Zhou et al. 2018], pruning [LeCun et al. 1990; Gordon et al. 2020; McCarley et al.\n2019; Wang et al. 2019c; Sajjad et al. 2020], and sparsity [Gale et al. 2020; Elsen et al. 2020] could\naid deployment by transforming larger models to obtain desired inference-time properties. These\ntechniques were originally intended for smaller models (e.g., BERT-L) in low-memory environments\n(e.g., mobile phones), but are now necessary to handle the extreme scale of modern foundation\nmodels in datacenter deployments. Parallelization techniques like tensor model parallelism [Shoeybi\net al. 2019], traditionally used for training, might also be useful to reduce inference latency, and\nalso provide additional memory capacity across GPUs to fit the model’s parameters.\nIn addition to these practical constraints, increases in the size and complexity of foundation\nmodels and the datasets used to train them pose new challenges to model and dataset lifecycle\nmanagement. Since models with a large number of parameters are hard to manually inspect by\nhumans, we need better systems for automated dataset curation (§4.6: data) and model quality\nassurance. Techniques like behavioral testing [Ribeiro et al. 2020] and model assertions [Kang et al.\n2020] facilitate easier model maintenance in production by providing analogs to unit tests, runtime\nmonitoring (in the form of test-time assertions), and continuous model improvement (as new inputs\ncome in) for models deployed in end applications. These tools can help address issues of fairness\nand bias (§5.1: fairness), and reduce model mispredictions.\nOn the Opportunities and Risks of Foundation Models 101\n4.6 Data\nAuthors: Laurel Orr, Simran Arora, Karan Goel, Avanika Narayan, Michael Zhang, Christopher Ré\nFoundation models signal a paradigm shift where increasingly massive quantities of data are\nbeing “fed” to these models for improved adaptation performance [Devlin et al . 2019; Radford\net al. 2021; Tolstikhin et al. 2021] with the overarching rule-of-thumb being \"the more data the\nbetter\" [Kaplan et al. 2020]. As previous sections have mentioned, this focus on data curation has\nraised concerns around the foundation model data lifecycle including (1) managing the data at\nsuch a large scale (§1: introduction), (2) integrating data across new modalities (§2.3: robotics,\n§3.1: healthcare), (3) reasoning over licensing and governance regulations — especially when\nconsidering the massing web-crawls used in foundation models training — (§3.1: healthcare,\n§5.4: legality), and (4) understanding the data quality (§4.4: evaluation).\nWhile foundation models add new and difficult facets to these challenges, we see parallels\nbetween these issues and core challenges in communities such as data management and data\nanalytics as well as industrial ML pipelines. For example, data management has long studied\nscalable declarative systems for data analysis, versioning, provenance, and integration—addressing\nchallenges (1) and (2) [Zaharia et al. 2012; Cudré-Mauroux et al. 2009; Stonebraker and Weisberg\n2013; Stonebraker and Ilyas 2018; Hellerstein and Stonebraker 2005]. Industry has pipelines dealing\nwith challenge (3) to manage diverse data licenses and help mitigate data violations. There is an\nentire ecosystem of research and systems tackling challenge (4) to support interactive data analytics\nand visualization [Hohman et al. 2020].64 While these solutions are not necessarily \"foundation\nmodel-ready\", we believe a path to better management of the foundation model data lifecycle should\ntake inspiration from these existing systems.\nIn this section, we address managing the foundation model data lifecycle. We first outline\nfour desiderata including data management at scale, support for heterogenous data sources, data\ngovernance, and data quality monitoring. We then envision how all of these requirements can be\nintegrated into a holistic data management solution called a data hub . The data hub is simply a\ndata management toolkit that can be used by the private or public sectors to better support the\ninteractive management of the foundation model data lifecycle.\n4.6.1 Data Management Desiderata.\nCurrent practices in foundation model development are generally ad-hoc across the entire lifecycle\nfrom data curation and data documentation to model monitoring and patching [Gebru et al. 2018;\nBandy and Vincent 2021; Bender and Friedman 2018]. Research in the data management community\nhas shown that well-defined data management platforms facilitate ML model development at\nscale through data ingestion, data versioning, data provenance, efficient analysis, and model\nmonitoring [Hellerstein and Stonebraker 2005; Agrawal et al. 2019; Vartak et al. 2016; Ikeda and\nWidom 2010].65 Taking inspiration from the data management community, we consider core\ndesiderata when building a holistic data management platform for foundation models.\n(1) Scalability. Foundation models are being trained on increasingly massive quantities of\ndata [Kaplan et al. 2020] with the WuDao 2.0 model being trained on 4.9 TB of multi-modal\ndata.66 This scale is expected to increase as most recent models are trained largely on public\n64VIS, CHI, SIGGRAPH are a few communities that research interactive data analytics methods and systems. Software\nsystems and libraries such as Pandas, Matplotlib, and Seaborn also aid users in interactive exploration.\n65Feature stores like Michelangelo also support end-to-end ML model building https://eng.uber.com/michelangelo-\nmachine-learning-platform/.\n66https://www.scmp.com/tech/tech-war/article/3135764/us-china-tech-war-beijing-funded-ai-researchers-surpass-\ngoogle-and\n102 Center for Research on Foundation Models (CRFM)\nfacing datasets. Public data represents an extremely small fraction of data compared to the\npetabytes of business and personal data collected every day and used in industrial foundation\nmodel pipelines [Marr 2017]. There is therefore a growing need for highly scalable techniques\nthat can handle multi-modal foundation model datasets.\n(2) Data integration. Recent work using foundation models demonstrates that leveraging\nintegrated structured and unstructured data can help models better generalize to rare con-\ncepts [Orr et al. 2020] and improve factual knowledge recall [Orr et al. 2020; Logeswaran\net al. 2019; Zhang et al. 2019a; Peters et al. 2019; Poerner et al. 2020]. Despite these recent\nsuccesses, integrating datasets for foundation models remains a challenge. Many works use\nunstructured text data with structured entity knowledge or image data [Antol et al. 2015].\nThere is a growing need to integrate datasets across diverse modalities such as text, video,\neye-tracking [Hollenstein et al. 2020], and robotic simulations [Lynch and Sermanet 2021]\n(see §2.3: robotics). We need data-integration solutions that can be applied at an industrial\nscale to multiple modalities and to multiple domains, such as government, business, and\nscience.\n(3) Privacy and governance controls. The training data used for foundation models may risk\nthe violation of the privacy of data subjects; their data may be disclosed, collected, or used\nwithout their consent [Jo and Gebru 2020] or outside the context for which consent was\noriginally given. The issue of consent and use is especially relevant for foundation models\nwhere downstream applications cannot always be anticipated. As explained in §5.4:legality,\nthese issues are compounded with the prevalence of web scraped datasets for foundation\nmodel training. As there are still open legal questions about how web-crawled data will be\ngoverned and copyrighted,67 the consequences of using web data remain unclear to foundation\nmodel providers in the public and private sector. We need tooling to help foundation model\nproviders adapt to emerging regulations and guidelines to ensure safe and responsible data\nmanagement.\n(4) Understanding data quality. Data quality impacts model performance [Lee et al. 2021b];\nhowever, toolkits or methods to systematically and scalably understand the training data and\nrelevant data subsets are still in their infancy. The data creation process can be messy, and\nthe data can contain different types of biases [Blodgett et al. 2020; Bender et al. 2021] (see\n§5.1: fairness) and consist of poisoned, false, or duplicated information [Chang et al. 2020;\nCarlini and Terzis 2021; Buchanan et al. 2021; Lee et al. 2021b]. Data is also continuously\nupdated and refined [Kiela et al. 2021] and may have emergent entities [Fetahu et al. 2015],\ndistribution shift [Chen et al. 2021a], and concept meaning shift [Kenter et al. 2015; Lazaridou\net al. 2021]. Further, once deployed, foundation models may present undesirable behavior on\ncritical, fine-grained sub-populations of data that foundation model providers need to detect\nand mitigate [Goel et al. 2021; Hohman et al. 2018; Ré et al. 2019; Oakden-Rayner et al. 2019].\nWe need toolkits that can detect and potentially mitigate different types of undesirable data\nto improve model performance in an interactive and iterative fashion. Such toolkits also need\nto adapt to the dynamical nature of training data.\n4.6.2 Data Hub Solution.\nPulling on years of work from data management, data science, and data analytics, we envision a\nfoundation model lifecycle data management solution, which we term a data hub . While examples\n67These issues have recently come to bear by the debate surrounding the use of GitHub data in Copilot’s Codex tool to\nhelp developers code https://www.pwvconsultants.com/blog/questions-around-bias-legalities-in-githubs-copilot/\nOn the Opportunities and Risks of Foundation Models 103\nof ML-focused data hubs 68 as well as more traditional data management systems exist, 69 they\neither (1) do not treat data integration as a first class primitive, (2) do not natively support the\nend-to-end lifecycle with model predictions, or (3) do not allow for interaction-driven data curation\nand refinement, where foundation model providers can dynamically explore and update possible\ndatasets subject to access control guidelines. We now discuss how the data hub addresses the four\ndesiderata.\nData scale. To address the management at scale challenge, the data hub will need standard data\nmanagement solutions [Armbrust et al. 2009] such as infrastructure to store and maintain large-\nscale datasets as they change over time and scalable interfaces to query, select, and filter datasets.\nThe hub should support heterogenous compute as well as cloud infrastructure to support scalable\nsolutions in different environments.\nData integration. The hub should incorporate data integration as a first class citizen. It will need\nadvanced data integration solutions [Stonebraker and Ilyas 2018; Abiteboul 1997; Dong et al. 2020;\nRekatsinas et al. 2017a]70 to allow for the merging of structured and unstructured knowledge across\nmodalities and domains. Further, this implies the hub will need to support storing and querying\nover heterogeneous datasets and sources.\nAccess control. Considering the access controls of the hub, the hub will need to support diverse\ndocumentation, e.g., dataset sheets [Gebru et al. 2018] or data statements [Bender and Friedman\n2018], to allow data curators to reflect on their processes and be transparent about the intended\nuse cases, potential biases, and limitations of their dataset. The data hub will need to decide which\ndocumentation is required for data to be uploaded (e.g., the data source and data description) and\nwhich information is recommended (e.g., what tasks the data could be used for). Furthermore,\ndocumentation may need to be updated as datasets evolve [Goel et al. 2021].\nData sources are often associated with licenses, and the hub will need to integrate different\nsources with different legal concerns and conditions [Masur 2018]. 71 Further, certain datasets\nhave legal guidelines to protect the privacy of the data subjects. The hub will need methods to\nensure a dataset does not release personally identifiable information (PII),72 that the aggregation\nof anonymized or de-identified data does not release PII,73 and that the data subjects have given\ninformed consent for their data to be disseminated.74\nPulling on ideas from data integration [Rekatsinas et al. 2017b], the hub should support mech-\nanisms to enable efficient and safe maintenance and sharing of data resources. Especially as the\nlegality of certain public datasets (e.g., web dumps) are still being decided (§5.4: legality), the\nhub critically needs tooling to help identify licensing violations and mitigate the impact of any\ngovernance violation. As certain violations will likely relate to model behavior, we need systems to\nsupport better understanding of model behavior, as we describe next.\nData quality tooling. Drawing on the field of data analysis and exploration, as users interactively\nselect, filter, and refine the data to use for training or adaptation, the hub will need tools to quickly\n68Some public data hubs include: https://data.world/, https://dataverse.harvard.edu/dataverse/harvard, https://\ndatacommons.org/, https://www.data.gov/, https://www.kaggle.com/, https://huggingface.co/datasets, https://www.ldc.\nupenn.edu/\n69Some traditional data management systems for foundation models include: https://aws.amazon.com/big-data/datalakes-\nand-analytics/, https://eng.uber.com/michelangelo-machine-learning-platform/, https://kafka.apache.org/\n70https://www.tamr.com/\n71https://content.next.westlaw.com/4-532-4243\n72https://www.justice.gov/opcl/privacy-act-1974\n73http://www2.ed.gov/policy/gen/guid/fpco/ferpa/library/georgialtr.html\n74https://www.dhs.gov/sites/default/files/publications/privacy-policy-guidance-memorandum-2008-01.pdf\n104 Center for Research on Foundation Models (CRFM)\nunderstand a user’s current dataset and its impact on model behavior [Hohman et al . 2020].75\nFurthermore, these systems can allow end-to-end foundation model monitoring by incorporating\nmodel performance through recent work on slice (sub-population) finding [Chung et al. 2019], model\nvalidation on relevant subsets [Goel et al. 2021; Ribeiro et al. 2020], and data valuation [Ghorbani and\nZou 2019]. Recent works also present methods that use the model to detect which subpopulations\nof data contribute the most to a given output to further aid model debugging [Keskar et al. 2019].\nOnce users can monitor model behavior—especially on rare, yet critical sub-populations—, the\nhub should provide methods and guidance for users to maintain models by correcting model errors.\nAlthough “model patching” [Goel et al. 2020a] is still an open problem, the work of [Orr et al. 2020]\nprovides a first description of using data engineering to maintain a production self-supervised\nsystem that corrected for undesirable behavior through changes to the data, not model. We believe\nthe data hub will need to support interfaces for users to inject targeted data modifications for model\nmaintenance.\nWe also acknowledge that data curation and exploration are not performed in isolation, and\nbelieve the data hub should support a community around sharing useful metrics and analysis\npipelines. Inspired by similar community sharing platforms like Hugging Face’s ModelHub76 or\nTableau Public’s visualization sharing platform,77 we want users to share insights about foundation\nmodel training data.\nOpen questions. Although our described data hub is inspired by existing toolkits and solutions,\nwe do not believe they are all ready for the challenges of foundation models. In particular, some\nopen questions revolving around designing a data hub are:\n•How should we support data versioning so datasets can be updated while maintaining old\nversions for reproducibility [Agrawal et al . 2019]? Once models are deployed and error\nbuckets are identified, datasets may need to be updated to include more examples from these\nerror buckets. How should these new, targeted examples be collected?\n•As described in §4.2: training, we imagine fewer models will be trained from scratch and\nmore will be fine-tuned. How do we support provenance or lineage information to understand\nwhere the original data came from, while maintaining subject privacy [Chen et al. 2015a]?\n•In the public sector, a data hub may be organized and run by an open-source community\nof individuals consisting of data curators and foundation model providers. In this setting,\nanswers to questions such as who stores the data? who pays for any compute? who is liable if\nlicensing is violated? are particularly murky. How can the data hub provide that right tooling\nso that once answers to such questions are resolved, they can be operationalized with ease?\n•What is the right set of statistics over the data to provide adequate documentation, without\nbeing too costly or difficult to obtain?\n•How can a data hub support targeted data modifications such as augmentation [Ma 2019;\nShorten and Khoshgoftaar 2019] or data programming [Ratner et al. 2017]?\n•How can monitoring toolkits better detect when a foundation model needs to be updated\ndue to poor performance on dynamically changing evaluation data?\nOur vision for a data hub is not complete or fully detailed. However, we present initial thoughts\non data challenges, and one solution to prompt thinking for how to improve data management for\nthe foundation model lifecycle.\n75Examples of data-focused interactive toolkits include https://www.tableau.com/ and https://www.paxata.com/.\n76https://huggingface.co/models\n77https://public.tableau.com/en-us/s/about\nOn the Opportunities and Risks of Foundation Models 105\n4.7 Security and privacy\nAuthors: Florian Tramèr*, Rohith Kuditipudi*, Xuechen Li*\nFig. 20. Risks and opportunities raised by foundation models for security and privacy of ML systems.\nAs central components in critical data-driven decision-making systems, machine learning models\nmust address a variety of security and privacy threats.78 These threats can be characterized using\nthe traditional “CIA triad” of computer security. ML systems should protect the Confidentiality\nof user data against inference and reconstruction attacks [Fredrikson et al. 2015; Shokri et al. 2017;\nCarlini et al. 2019, 2021]. Moreover, the secrecy of trained models themselves can be at risk of\nmodel stealing attacks [Tramèr et al. 2016; Papernot et al. 2017]. The Integrity of ML systems can\nbe compromised by adversarial examples [Biggio et al. 2013; Szegedy et al. 2014] and data poisoning\nattacks [Biggio et al. 2012; Chen et al. 2017]. Finally, resource-depletion attacks [Shumailov et al.\n2020; Hong et al. 2020a] can threaten the Availability of ML systems.\nIn regard to these threats, we posit that the security role of foundation models in future machine\nlearning systems will be akin to the role played by the operating system in traditional software\nsystems. Due to its generality and ubiquity, a foundation model may become asingle point of failure\nand thus a prime target for attacks against applications derived from this model. In turn however, a\nfoundation model imbued with strong security and privacy properties could form the backbone for\nthe design of a variety of secure and reliable ML applications. Of course, these applications may\nstill have to be designed to enforce specific security and privacy guarantees (in the same way that\nsoftware designers cannot rely on a secure operating system to protect against all security risks).\n4.7.1 Risks.\nSingle points of failure. A foundation model that is adapted to a variety of applications represents\na single point of failure for these applications. For example, data poisoning attacks on a foundation\nmodel, where an adversary inserts malicious examples into the training data, might impact all\nadapted applications as well. Similarly, adversarial examples against a foundation model (i.e., small\ninput perturbations that cause the model to output very different features) could more easily\ntransfer to adapted applications. Wallace et al. [2019] even find that a single adversarial trigger\n78In this section, we focus on security for foundation models . Some applications of foundation models for security (e.g., de-\ntection of toxic content) are discussed in §5.2: misuse.\n106 Center for Research on Foundation Models (CRFM)\nadded to any input can cause language models such as GPT-2 to output a predefined piece of text.\nA foundation model can also become a single point of failure for data privacy. If a foundation\nmodel is pretrained on a company’s private data and the model memorizes part of this data, all\ndownstream applications could run the risk of exposing this data [Carlini et al. 2021]. The provider\nof a foundation model may also be a single point of trust for the privacy of application data. For\nexample, the current API for GPT-3 requires that all (potentially sensitive) data used for fine-tuning\nor inference be uploaded to OpenAI’s servers. Designing a foundation model service that avoids\nthis centralization of trust is an interesting problem.\nIf the parameters of a foundation model are public, model stealing attacks on adapted applications\ncould be facilitated, as the attacker only needs to reverse-engineer the “delta” with respect to the\npublic foundation model [Krishna et al. 2019] (e.g., a linear model trained on features extracted\nfrom a public frozen model).\nFinally, denial-of-service attacks on the foundation model provider could also be a concern and\nmight be exacerbated by querying the model with special high-cost inputs [Shumailov et al. 2020].\nData poisoning. Successful foundation models have so far been trained on large and often uncu-\nrated datasets scraped from the Web [Radford et al. 2021, 2019]. This permissive data collection —\ncoupled with a lack of direct training supervision — facilitates poisoning attacks on a foundation\nmodel’s training data (e.g., injecting hateful speech targeted at a specific individual or company\ninto a few outbound pages from Reddit). Worse, the power of poisoning attacks may be exacerbated\nby the growing size and accuracy of today’s models [Carlini 2021].\nTo illustrate, Schuster et al. [2021] show that a code auto-completion system trained with GPT-\n2 on Github data can be poisoned into suggesting insecure code snippets with the injection of\nonly a few malicious files. Carlini and Terzis [2021] further show that targeted attacks against\nCLIP-style [Radford et al. 2021] models require modifying as little as two out of 3 million training\nexamples.\nFunction creep & dual use. Foundation models learn general features that enable them to be\neasily adapted to a variety of tasks. This flexibility, however, raises concerns that foundation models\ncould be used beyond their originally foreseen purposes — a risk commonly referred to as function\ncreep or dual use . Examples of function creep in machine learning include overlearning [Song and\nShmatikov 2019] and adversarial reprogramming [Elsayed et al. 2018].\nTo illustrate, CLIP was originally trained to solve the generic task of predicting image-text pairs,\nbut in doing so also learned to capture rich facial features [Goh et al. 2021]. While CLIP’s “model\ncard”79 explicitly places facial recognition and other surveillance technologies as out-of-scope,\nCLIP can certainly be re-purposed for such tasks [Radiya-Dixit and Tramèr 2021]. This example\nillustrates that it may be challenging to constrain (or even foresee) the possible nefarious uses of a\nfoundation model when it is designed. §5.2: misuse provides further discussions on dual (mis)use\nof foundation models.\nMultimodal inconsistencies. Multimodality may increase the attack surface of foundation mod-\nels, by enabling adversaries to exploit inconsistencies across modalities. The possibility of such\nattacks was demonstrated in an (in)famous example of CLIP classifying an apple with the word\n“iPod” stuck to it as an iPod [Goh et al. 2021]. More generally, whenever a concept can be expressed\nusing different modalities, inconsistencies across these modalities may be exploitable.\nSuch inconsistencies are particularly concerning when a foundation model is adapted to a task\nthat primarily relies on only one of the learned modalities. For example, consider using features\nextracted from CLIP for facial recognition. This is a purely visual task, yet the adapted model’s\n79https://github.com/openai/CLIP/blob/main/model-card.md. Accessed 06.30.2021\nOn the Opportunities and Risks of Foundation Models 107\nfeatures will still be sensitive to textual signals (thus, an attacker might be able to evade facial\nrecognition by wearing clothes with imprinted text). Alternatively, consider an autonomous driving\nsystem (an application that also relies primarily on vision) that sees a billboard with the word\n“green” on it, and mistakenly interprets this as a green light.\n4.7.2 Opportunities.\nSecurity choke points. If adapted applications can inherit vulnerabilities from a foundation\nmodel, they can also inherit desirable security characteristics — such as robustness to adversarial\nexamples or poisoning attacks. Foundation models could thus serve as security choke points . For\nexample, a model robust to adversarial examples can retain its robustness when it is adapted to\nother tasks [Shafahi et al. 2019]. Similarly, a foundation model provider that can (somehow) defend\nagainst poisoning, model-stealing or resource-depletion attacks could then provide such security\nguarantees for its customers’ applications.\nThe tradeoff between a foundation model’s role as a single point of failure or as a security choke\npoint is reminiscent of similar security tradeoffs in other abstraction layers in the software stack\n(e.g., an operating system, database system, or a Web browser). By virtue of serving many different\napplications, an abstraction layer is a prime target for attack, but can typically also leverage far\ngreater resources to enhance its security compared to any single application.\nCheaper private learning. Current foundation models are often trained by amassing vast amounts\nof data from publicly available sources (e.g., from the open Web). This practice may raise concerns\nabout privacy — in the broad sense of taking user data out of its intended context [Nissenbaum\n2004; Carlini et al . 2021]. While some existing works aim to mitigate a model’s propensity to\nmemorize training data (e.g., by de-duplicating training data [Lee et al. 2021b], or by pretraining\nunder differential privacy [Anil et al. 2021]), such solutions are unlikely to meet the broad privacy\nexpectations that users could associate with text data [Brown et al. 2022]. On the other hand, public\npretraining could also end up being a win for user privacy in applications that handle scarce and\nsensitive data (e.g., in healthcare).\nAs an example, consider the problem of training a differentially private model [Dwork et al .\n2006] for a healthcare task. Training such a model “end-to-end” (i.e., without leveraging any\npretraining) to a decent privacy-utility tradeoff currently requires vast amounts of privacy-sensitive\ndata [McMahan et al. 2018; Basu et al. 2021]. In contrast, a foundation model pretrained on public\ndata in many cases could be adapted to perform specific tasks with significantly less confidential\ndata [Bommasani et al. 2019; Tramèr and Boneh 2021; Li et al. 2022; Yu et al. 2022].\nRobustness to adversarial examples at scale. There is evidence suggesting that training a\nmodel that is robust to adversarial examples requires vastly more data compared to standard\ntraining [Schmidt et al. 2018], but that unlabeled data may suffice to bridge this gap [Carmon et al.\n2019; Uesato et al. 2019]. Moreover, increasing model size and capacity (i.e., over-parameterization)\nhas also been shown to be necessary for achieving adversarial robustness in some settings [Madry\net al. 2018; Bubeck and Sellke 2021]. Understanding how best to leverage over-parameterization\nand unlabeled data to achieve adversarial robustness is an important direction for future research.\nGiven their unprecedented scale (both in terms of model size and training set size), foundation\nmodels are uniquely positioned to benefit from this line of inquiry.\nDespite their unprecedented scale, current foundation models unfortunately see little gains\nin robustness to worst-case adversarial perturbations [Fort 2021; Wallace et al. 2019]. However,\nmultimodal models such as CLIP are surprisingly robust to (non-adversarial) distributional shifts\n(see §4.8: robustness). Whether these gains in distributional robustness can translate to increased\nresilience against real-world attacks is another exciting open question. Particularly in settings\n108 Center for Research on Foundation Models (CRFM)\nwhere adversaries are subject to various constraints (e.g., limited query access or computational\nbudget), there is reason to be optimistic that enhanced distributional robustness could lead to\nconcomitant gains in overall security — even if the foundation model remains vulnerable to worst-\ncase “white-box” attacks.\nOn the Opportunities and Risks of Foundation Models 109\n4.8 Robustness to distribution shifts\nAuthors: Sang Michael Xie, Ananya Kumar, Rohan Taori, Tony Lee, Shiori Sagawa, Pang Wei Koh,\nTatsunori Hashimoto\nReal-world ML systems need to be robust to distribution shifts — they should work well on test\ndistributions which differ from the train distribution. High-stakes applications such as poverty\nmapping in under-resourced countries [Xie et al . 2016; Jean et al . 2016], self-driving cars [Yu\net al. 2020a; Sun et al. 2020a], and medical diagnosis [AlBadawy et al. 2018; Dai and Gool 2018]\nall require models that generalize well to circumstances not seen in the training data, e.g., test\nexamples from different countries, under different driving conditions, or from different hospitals.\nPrior work has shown that these types of distribution shifts can cause large drops in performance\neven in state-of-the-art models [Blitzer et al. 2006; Daumé III 2007; Sugiyama et al. 2007; Ganin\nand Lempitsky 2015; Peng et al. 2019; Kumar et al. 2020a; Arjovsky et al. 2019; Szegedy et al. 2014;\nHendrycks and Dietterich 2019; Sagawa et al. 2020a; Recht et al. 2019; Abney 2007; Ruder and Plank\n2018; Geirhos et al. 2018; Kumar et al. 2020b; Yu et al. 2020b; Geirhos et al. 2020; Xie et al. 2021a;\nKoh et al. 2021].\nIn this section, we consider the role of foundation models on robustness to distribution shifts. A\nfoundation model is trained on a large and diverse unlabeled dataset sampled from a distribution\n𝑝pre and can be adapted to many downstream tasks. For each downstream task T, the foundation\nmodel is adapted to labeled training data sampled from an in-distribution (ID) training distribution\n𝑝T\nID, and then evaluated on an out-of-distribution (OOD) test distribution 𝑝T\nOOD. For example, a\npoverty prediction model [Xie et al. 2016; Jean et al. 2016] may be pretrained on unlabeled satellite\ndata from across the world to learn useful features for all countries, then fine-tuned on labeled\nexamples from Nigeria, and finally evaluated in Malawi where labeled examples are scarce.\nWe argue that 1) foundation models are a particularly promising approach to robustness. Existing\nwork shows that pretraining on unlabeled data is an effective, general-purpose way to improve\naccuracy on OOD test distributions, in contrast to many robustness interventions which are\nconstrained to narrow types of distribution shifts. However, we also discuss why 2) foundation\nmodels may not always mitigate distribution shifts, such as shifts due to spurious correlations\nor changes over time. Finally, 3) we outline several research directions to leverage and improve\nfoundation models for robustness.\nWe note that one of the ways in which foundation models lead to improved extrapolation is by\nproviding inductive biases (via model initialization) for the adapted model, which are learned on a\ndiverse dataset that extends beyond the downstream training data. However, this same inductive\nbias can also encode harmful associations from the pretrained data and lead to representational\nand allocational harms in the presence of distribution shift. See §4.6: dataand §5.1: fairness for\nfurther discussion of such harms and methods for mitigation.\n4.8.1 Advantages.\nBy learning representations on a large and diverse foundation model training distribution𝑝pre, foun-\ndation models can improve accuracy of the adapted derivative on the downstream test distribution\n𝑝T\nOOD. OpenAI’s CLIP model, which is a foundation model trained on a diverse set of images and\nnatural language documents, has been shown to be robust to some benchmark distribution shifts\non ImageNet [Radford et al. 2021]: for example, both CLIP and a standard ResNet50 obtain 76%\naccuracy on ImageNet, but CLIP achieves 6% higher accuracy on ImageNetV2 [Recht et al. 2019] and\n35% higher accuracy on ImageNet Sketch [Radford et al. 2021], which are both related but different\nfrom the original ImageNet training distribution. In contrast, many other robustness interventions,\nsuch as adversarial training [Madry et al. 2018], invariant risk minimization [Arjovsky et al. 2019],\n110 Center for Research on Foundation Models (CRFM)\nFig. 21. In-distribution (ID) and out-of-distribution (OOD) inputs for a variety of distribution shifts. The\nprediction tasks depicted here are image classification for images and fact verification for text. Although\nrepresentations learned by foundation models improve downstream robustness for many shifts (e.g., common\ncorruptions) [Hendrycks and Dietterich 2019; Xie et al. 2021a; Radford et al. 2021], some shifts such as spurious\ncorrelations (where grass is predictive of cow) [Beery et al. 2020] and extrapolation across time (with facts\nthat change over time) [Lazaridou et al. 2021] are still likely unaddressed by foundation models.\nor using larger models have had little impact on effective robustness (defined as the gap between\nin-distribution and out-of-distribution performance) on these ImageNet tasks, especially without\nexplicit knowledge of the distribution shift [Taori et al. 2020; Santurkar et al. 2020; Radford et al.\n2021; Miller et al. 2021].\nMany other works demonstrate that pretraining on large datasets can improve robustness to\ncommon image corruptions, label shift, and label corruptions [Hendrycks et al. 2019a,b]; to real-\nworld spatial shifts in satellite imagery tasks [Xie et al. 2021a; Kumar et al. 2022]; and to shifts across\ntopics in natural language understanding tasks [Hendrycks et al. 2020; Fisch et al. 2019; Yogatama\net al. 2019]. As another example, diversifying the foundation model training data to include multiple\nlanguages (as in multilingual BERT [Liu et al. 2020b]) significantly improves performance in unseen\nlanguage pairs.\n4.8.2 Persistent challenges.\nDespite promising signs that foundation models will result in substantial improvements to robust-\nness, we anticipate that foundation models are not a panacea for distribution shifts. We discuss this\nin the context of two broad categories of distribution shifts below.\nSpurious correlations. Spurious correlations are statistical correlations between features and\nlabels with predictive power on the training distribution but not on the test distribution [Heinze-\nDeml and Meinshausen 2017; Arjovsky et al . 2019; Sagawa et al. 2020a]. Well-known examples\ninclude reliance on background color for object recognition [Xiao et al. 2020], surgical markers for\nmedical diagnostics [Winkler et al. 2019], annotator biases in crowdsourced data [Tsuchiya 2018;\nGururangan et al. 2018; Poliak et al. 2018; Geva et al. 2019], and demographic biases [Abid et al.\n2021; Nadeem et al. 2021; Gehman et al. 2020]. Models learn these spurious correlations largely\nbecause the foundation model training and adaptation data exhibit these biases [Nagarajan et al.\nOn the Opportunities and Risks of Foundation Models 111\n2020; Gehman et al. 2020], and this issue cannot simply be addressed with larger models [Sagawa\net al. 2020b].\nFoundation models may exacerbate or mitigate the effects of spurious correlations, but this\ndepends on the nature of the particular downstream task and its relation to the foundation model\ntraining data and algorithm. By training with a diverse dataset, foundation models may improve\nrobustness to spurious correlations that are found only in a subset of the training data: e.g., existing\nstudies find that pretrained language models can avoid spurious correlations by quickly learning\nfrom counterexamples to the spurious correlations [Tu et al. 2020]. However, foundation models\ncan also exacerbate the issue by introducing biases present in the foundation model training data,\nas observed for demographic biases in GPT-3 and other NLP models [Abid et al. 2021; Nadeem et al.\n2021; Gehman et al. 2020]. Moreover, training at scale alone need not fully address the root issue of\nidentifying and not relying on the features that are predictive on the downstream training set but\nnot on the downstream test set [Heinze-Deml and Meinshausen 2017]. Addressing these challenges\nwill require us to understand and manage the inductive bias from foundation model training and\ndevelop adaptation algorithms that are resistant to learning spurious correlations.\nExtrapolation and temporal drift. Finally, the few- and zero-shot capabilities of foundation\nmodels will mean that these models will increasingly be used far beyond the training distribution.\nWhile large-scale foundation model training can help with certain forms of extrapolation to\nnew distributions [Papadimitriou and Jurafsky 2020], there may be limits to their extrapolation\ncapabilities. For example, existing language models cannot handle changes to world knowledge or\nlanguage change without re-training [Lazaridou et al. 2021; Dhingra et al. 2021], zero-shot transfer\nin CLIP suffers greatly in satellite image domains [Radford et al. 2021], and ImageNet pretraining\ndoes not substantially improve the performance of large models on medical images [Raghu et al.\n2019; Ke et al . 2021]. We believe that foundation models cannot be assumed to automatically\nextrapolate within a given modality (e.g., all images), and it will become increasingly important\nto define and separate the forms of extrapolation that are newly enabled by foundation models\nfrom those that remain out of reach. Though existing taxonomies for distribution shifts have been\nproposed in generality [Quiñonero-Candela et al. 2009; Ye et al. 2021], fully understanding and\ndefining the types of distribution shifts for which foundation models are effective is a major open\nproblem for robustness research.\n4.8.3 Opportunities.\nFoundation models hold substantial promise as a general-purpose robustness intervention for\ndistribution shifts and open new avenues for robustness research. We outline some opportunities\nand open questions below.\nUnderstanding foundation model representations. Existing studies of the robustness of foun-\ndation models have been largely empirical, and there is little understanding of the mechanism\nbehind gains in robustness. Sun et al. [2019b] hypothesize that pretrained representations bring\ndisparate domains (such as ID and OOD distributions) closer together, which can in turn improve\ngeneralization from labeled ID data to OOD data [Ben-David et al. 2010]. Controlled experimenta-\ntion on measuring the distance between domain representations with and without pretraining can\nelucidate this effect. There are initial promising directions in characterizing foundation model train-\ning (e.g., contrastive learning as a spectral graph decomposition [HaoChen et al. 2021a]) and their\ninductive biases [Saunshi et al. 2020a; Lee et al. 2020a; Zhang and Hashimoto 2020; Xie et al. 2020].\nHowever these theories are limited and fail to address other empirically effective foundation models\nsuch as fully generative language models (e.g., GPT-3 [Brown et al. 2020] and image-GPT [Chen\n112 Center for Research on Foundation Models (CRFM)\net al. 2020d]). Further understanding how these inductive biases are useful under distribution shift\nmay lead to a more complete theory (§4.10:theory) of how foundation models improve robustness.\nData augmentation in foundation model training. While foundation models trained without\nknowledge of the downstream tasks can avoid some task-specific biases and often improve robust-\nness, certain statistical biases stemming from how the foundation model was trained may persist.\nAs a concrete example, many contemporary self-supervision algorithms are heavily dependent\non choosing an appropriate set of data augmentations [Chen et al. 2020c], which in turn confers\ndifferent types of robustness in the adaptation phase. For instance, Xiao et al . [2021] show that\na foundation model for vision trained with contrastive learning on rotation augmentations may\nimprove OOD performance on adaptation tasks with rotation invariance, but may not improve\nrobustness for tasks where OOD generalization requires other invariances. Further research into\nwhat types of data augmentations improve robustness for a wide range of downstream tasks — in-\ncluding data augmentations that are learned from data [Wong and Kolter 2020; Tamkin et al. 2021c]\nor designed to be generally applicable across data modalities [Verma et al . 2021] — will inform\nbetter foundation model training algorithms (§4.2: training).\nEncoding structure in foundation model training. In general, exploring new ways of encod-\ning known structure and invariances in the data is an important path forward for foundation\nmodel training. Many real-world tasks have additional metadata (e.g., spatial location coordinates,\nclimate information from auxiliary satellites in our poverty prediction example), which may pro-\nvide additional structure for OOD generalization (e.g., across geographic areas) [Xie et al. 2021a;\nKoh et al. 2021]. For example, Xie et al . [2021a] show that metadata can be used as targets for\npretraining to improve downstream OOD accuracy. In language, modeling the tags in HTML data\nprovides additional downstream-task-adjacent supervision, allows for new forms of prompting\n(e.g., filling in <title> tags for title suggestion), and improves data efficiency [Aghajanyan et al.\n2021]. While current data augmentation methods encode hand-crafted knowledge, other avenues\nsuch as exploiting metadata could provide more automated ways of determining which structures\nand invariances to incorporate for foundation model training.\nSpecialization vs. diversity in foundation model training data. The choice of foundation\nmodel training data has downstream effects — training on a more diverse dataset is not always\nbetter for downstream performance than a more specialized foundation model [Cole et al. 2021;\nChalkidis et al. 2020] (see §4.3: adaptationfor a more detailed discussion). In some domains such\nas satellite images and specialized text topics, continued pretraining on the specialized domain\nimproves the downstream performance significantly [Reed et al. 2021; Gururangan et al. 2020]. This\nis a potential source of tension: on one hand, we might want to train the foundation model on a\nlarge, diverse dataset in order to have more robust performance under distribution shifts, while on\nthe other hand, we might need to specialize the foundation model to improve its in-distribution and\nout-of-distribution performance on downstream tasks. A better understanding of how specialization\naffects the in-distribution and out-of-distribution performance of foundation models will allow us\nto design and collect more effective foundation model training sets.\nAdaptation methods. Although foundation models provide a strong starting point, how the\nadaptation method uses the pretrained information can affect robustness. For instance, lightweight\ntuning methods for language models (e.g., adapter/prefix/prompt tuning [Houlsby et al . 2019;\nLi and Liang 2021; Lester et al . 2021]), which adapt the model for a new task by optimizing a\nsmall set of parameters (such as a continuous prompt) while keeping the other foundation model\nparameters frozen, seem to give OOD performance benefits (§4.3: adaptation). Xie et al. [2021b]\nexplain this in a special case, where composing a learned model with a frozen foundation model\nOn the Opportunities and Risks of Foundation Models 113\ncan reduce the complexity of the learned model, improving generalization both ID and OOD.\nOn vision datasets, Wortsman et al. [2021]; Kumar et al. [2022] find that freezing a foundation\nmodel and training only the head can lead to better OOD performance than fine-tuning the entire\nmodel. Kumar et al. [2022] explain this theoretically by showing that full fine-tuning can distort\npretrained features even in a simple setting (two layer linear networks). However, it is still poorly\nunderstood in general why freezing parameters seems to improve OOD performance. Finally, while\ncurrent adaptation methods may suffice for good ID generalization, the methods do not explicitly\naccount for distribution shift in their design. As a first step, we can investigate how methods for\ndistribution shifts such as domain adaptation, domain generalization, and semi-supervised learning\nmethods interact with foundation models when used for adaptation. Progress in these directions\ncan lead to adaptation methods that can better leverage foundation models for robustness.\n114 Center for Research on Foundation Models (CRFM)\n4.9 AI safety and alignment\nAuthors: Alex Tamkin, Geoff Keeling, Jack Ryan, Sydney von Arx\nThe field of Artificial Intelligence (AI) Safety concerns itself with potential accidents, hazards,\nand risks of advanced AI models, especially larger-scale risks to communities or societies. Current\nfoundation models may be far from posing such risks; however, the breadth of their capabilities\nand potential applications is striking, and a clear shift from previous ML paradigms. While AI\nsafety has historically occupied a more marginal position within AI research, the current transition\ntowards foundation models and their corresponding generality offers an opportunity for AI safety\nresearchers to revisit the core questions of the field in a new light and reassess their immediate or\nnear-future relevance.80\n4.9.1 Traditional problems in AI safety.\nA major branch of AI safety research concerns the implications of advanced AI systems, including\nthose that might match or exceed human performance across a broad class of cognitive tasks\n[Everitt et al. 2018].81 A central goal of safety research in this context is to mitigate large-scale risks\nposed by the development of advanced AI.82 These risks may be significantly more speculative\nthan those considered in §5.2: misuse, §4.8: robustness, and §4.7: security; however, they are of\nfar greater magnitude, and could at least in principle result from future, highly-capable systems. Of\nparticular concern are global catastrophic risks: roughly, risks that are global or trans-generational\nin scope—causing death or otherwise significantly reducing the welfare of those affected (e.g., a\nnuclear war or rapid ecological collapse) [Bostrom and Cirkovic 2011]. What AI safety research\namounts to, then, is a family of projects which aim to characterize what (if any) catastrophic risks are\nposed by the development of advanced AI, and develop plausible technical solutions for mitigating\nthe probability or the severity of these risks. The best-case scenario from the point of view of AI\nsafety is a solution to the control problem: how to develop an advanced AI system that enables us\nto reap the computational benefits of that system while at the same time leaving us with sufficient\ncontrol such that the deployment of the system does not result in a global catastrophe [Bostrom\nand Cirkovic 2011]. However technical solutions are not sufficient to ensure safety: ensuring that\nsafe algorithms are actually those implemented into real-world systems and that unsafe systems\nare not deployed may require additional sociotechnical measures and institutions.\nReinforcement Learning (RL), which studies decision-making agents optimized towards rewards,\nhas been a dominant focus in AI safety for the past decade. What is at issue here is the difficulty of\nspecifying and instantiating a reward function for the AI that aligns with human values, in the\nminimal sense of not posing a global catastrophic threat.83 While this problem, known as value\nalignment [Gabriel 2020; Yudkowsky 2016], may seem trivial at first glance, human values are\ndiverse,84 amorphous, and challenging to capture quantitatively. Due to this, a salient concern is\nreward hacking, where the AI finds an unforeseen policy that maximizes a proxy reward for human\nwellbeing, but whose misspecification results in a significant harm.85 Many efforts to combat the\n80See Amodei et al. [2016] and Hendrycks et al. [2021d] for broader perspectives on open problems in AI Safety.\n81This is referred to by some as AGI or artificial general intelligence, although terminology use varies [e.g., see Karnofsky\n2016].\n82Note that this does not require a belief that building certain kinds of advanced AI is a desirable goal, nor even certainty\nthat it is an achievable one.\n83See Hubinger et al. [2019] for a discussion of some challenges that arise at the threshold between reward specification\nand reward instantiation.\n84See Gabriel [2020] for an extended discussion of human diversity, ethics, and the value alignment problem\n85See this spreadsheet for a list of real-world examples of reward hacking, including an aircraft landing algorithm which\nachieved a perfect score by outputting large forces that exploited a flaw in the simulator.\nOn the Opportunities and Risks of Foundation Models 115\nvalue alignment problem have focused on maximizing corrigibility, which is when errors in the\ndesign of a system can be corrected once the system is running [Soares et al. 2015]. This can be far\nfrom straightforward—in the RL context, an agent with a specified goal would be incentivized to\nprohibit attempts to alter that goal, as any attempt to alter that goal would likely be suboptimal for\nthe goal’s realization [Omohundro 2008].\nHowever, pure RL is not the only theorized route to advanced AI. Foundation models can also be\ntrained with simple (self-)supervised objectives like next-token prediction, yet can still be used in\ninteractive and goal-directed ways, with or without additional RL training. Moreover, it appears\nthat many of these methods may result in increased capabilities through straightforward scaling of\ncompute, number of parameters, and dataset size [Hestness et al. 2017; Kaplan et al. 2020]. What\nconcepts like value alignment and corrigibility amount to in the broader context of foundation\nmodels differ in several respects to the pure RL case, and must accordingly be carefully theorized.\n4.9.2 Current foundation models and AI safety.\nMany of these risks in the RL setting result from models optimized to carry out goals. However, a\nkey challenge for AI safety research on recent foundation models is that goal-directed behavior\nmay emerge despite not being explicitly optimized for (see also §4.2: training). As an example,\nlarge language models may be trained on corpora where agents use language in goal-directed ways,\nsuch as in persuasive text. To predict the next token well, a model may acquire a general capability\nto reason and produce arguments, which could emerge with suitable contexts. Foundation models\ntrained on other kinds of human data may capture other kinds of goal-directed behavior present in\nthe data; e.g., robotic agents trained to mimic humans in videos may attempt to punch or knock-out\ntheir human operators if their training data includes videos of boxing matches. Recent work has\nalso attempted to directly train agents to produce goal-directed behavior; for example, the Decision\nTransformer trains a sequence model on trajectories prepended with their returns [Srivastava et al.\n2019; Schmidhuber 2019; Chen et al . 2021b]. One can then generate high-return trajectories by\n“prompting” this model with a high return, which raises similar questions of reward hacking from\nthe RL context.\nHowever, a major aim of safety research on goal-directed models is to gain more principled\ncontrol and explainability over the actions being pursued by the agent, as opposed to relying on\ninscrutable decisions from a blackbox neural network.86 This makes current foundation models\nan exciting avenue of study for AI safety research, as aligning them may be a useful precursor for\naligning more advanced models [Christiano 2016; Cotra 2021; Kenton et al. 2021]. One challenge is\nthe misalignment between the foundation model’s training objective and the desired behavior; for\nexample, a language model may be trained to predict the next word of all documents in the training\ncorpus regardless of veracity, but users may want the model to only output true or helpful text\n[Tamkin et al. 2021a]. One potential way to steer goal-directed agents towards desired behavior\nmay be to train them with natural language descriptions of actions — this may enable steering them\nwith language as well as enabling them to output interpretable language describing the task they\n\"believe\" they are performing, similar to methods for controllable generation and source attribution\n[e.g., Keskar et al . 2019, see also §2.3: robotics, §2.5: interaction, and §4.11: interpretability].\nHowever, further advances would be necessary to ensure the reliability and self-consistency of\nsuch models in the wild (§4.8: robustness), as well as gaining a more mechanistic understanding\nof how these models operate [Cammarata et al. 2020, also see §4.11: interpretability]. And even\nif natural language-based control of future foundation models enables better task specification\n86For more on the relationship between understanding and semantics see §2.6: philosophy\n116 Center for Research on Foundation Models (CRFM)\nand monitoring, models may acquire deceptive or otherwise undesirable behavior from human\ndata — identifying and neutralizing this behavior is another important direction for future study.\nWhile the self-supervised objectives described in the previous paragraph train models to capture\nhuman behavior in the data, new training paradigms may produce goal-directed foundation models\ncapable of carrying out a wide range of tasks in complex environments, and which exhibit capa-\nbilities superior to humans in different domains (see §4.2: training). For example, goal-directed\nfoundation models may be trained in an open-ended self-play setting, similar to AlphaGo, or in vast\nmultitask single-agent RL setups. This might lead to emergent capabilities that complicate efforts to\nget agents to carry out goals, especially if many agents are trained together in a rich world-simulator\nthat encourages the development of skills like deception, misdirection, dissimulation, persuasion,\nand strategic planning. Aside from countering deceptive behavior, it also remains unclear how to\neffectively evaluate and control the behavior of very capable models, known as scalable oversight\nor alignment [Amodei et al. 2016; Leike et al. 2018]; e.g., scoring novel reactions proposed by a\nchemical foundation model (see §4.4:evaluation). New human-in-the-loop approaches for training,\nsteering, monitoring, and understanding these models are thus exciting future directions.\nFinally, even before any of these more advanced capabilities emerge, an important research\narea for AI safety in the near term is characterizing and forecasting the capabilities of current\nself-supervised foundation models. There are three aspects which make this challenging. First, the\ngenerality of foundation models means that they can be applied to countless different kinds of\napplications in unexpected ways. Enumerating current and planned applications of foundation\nmodels is not sufficient to capture the full range of ways they could be used. Second, even within\na particular application, model capabilities are emergent: they grow and change in unexpected\nways as models scale. For example, the ability to control GPT-3 via “prompting\" was an emergent\nphenomenon of which only the barest glimpses were evident in the smaller GPT-2 model [Radford\net al. 2019; Brown et al. 2020]. What the emergent properties of future foundation models will look\nlike is unknown. Third, even within a particular application and scale, a model’s capabilities are not\neasy to characterize. For example, the ability of GPT-3 to perform addition improves dramatically\nonce commas are added to the inputs [Branwen 2020; Brockman 2020]. Similarly, small rewordings\nof prompts can have large impacts on task performance. Since the space of prompts is intractable to\nenumerate, it is challenging to definitely assert that any task is outside the reach of current prompt-\nbased foundation models — this is a major challenge for reasoning about possible catastrophic risks\nfrom foundation models.\n4.9.3 Potential catastrophic risks from future foundation models.\nThe broad and quickly-growing capabilities of current models suggest the benefit of attempting to\ncharacterize possible catastrophic risks from more advanced systems. We see at least two ways in\nwhich advanced foundation models might contribute to such outcomes.\nCatastrophic robustness failures. §4.8: robustness discusses how models may behave in unex-\npected or harmful ways when confronted with new kinds of data [Amodei et al. 2016; Yudkowsky\net al. 2008]. These failures may be especially consequential if foundation models are integrated\ninto important systems that leverage foundation models’ ability to quickly adapt to many different\ntasks and situations. Failures could be catastrophic if they occur in warfare systems (resulting in\nunwanted discharge of weapons, possibly igniting a conflict), critical infrastructure (accidental\ndestruction of critical energy or agricultural capabilities), or if they become essential to a large\nfraction of economic activity (whose unexpected failure could result in a sudden collapse in living\nstandards and political instability; see also §5.5: economics). Indeed, the threat of catastrophic\nrobustness failures is particularly pertinent for foundation models in contrast to other kinds of\nOn the Opportunities and Risks of Foundation Models 117\nAI. This is because a foundation model consists of a single model that may be adapted for many\ndifferent use cases, such that robustness failures derived from the statistical associations learned by\nthe model could in principle manifest in a correlated way across several different domains. If the\nsame foundation model is integrated into multiple critical functions, then lack of robustness in the\nmodel could lead to correlated failures that span multiple critical functions or failsafes.\nMisspecified goals. The use of foundation models might increase the risks of optimizing mis-\naligned yet easy-to-specify goals, often referred to as Goodhart’s Law [Kenton et al. 2021; Goodhart\n1984]. A current-day example of these risks is the negative effects of some recommender systems\n(e.g., polarization, media addiction) which may optimize simple engagement metrics rather than\na harder-to-measure combination of societal and consumer well-being [Burr et al. 2018; Milano\net al. 2020]. Future institutions may leverage uninterpretable foundation models to maximize\nsimple measures such as profit or GDP, due to these models’ ability to adapt to the many different\nsubproblems each of these metrics is dependent on. However, at larger scales optimizing for these\nproxy metrics instead of a more holistic goal designed for human welfare could inadvertently lead\nto environmental or geopolitical harms [Gabriel 2020; Creel and Hellman 2021].\n4.9.4 Conclusion.\nIn sum, we argue that current and potential future emergent properties of foundation models make\nthem ripe objects of study for the field of AI safety. We encourage future work on characterizing\nand forecasting the exact capabilities and risks of foundation models; developing new methods\nto align foundation models to human values and desired goals; and for states, research labs, and\nbusinesses to coordinate on proactive measures to mitigate salient risks.\n118 Center for Research on Foundation Models (CRFM)\n4.10 Theory\nAuthors: Aditi Raghunathan, Sang Michael Xie, Ananya Kumar, Niladri Chatterji, Rohan Taori,\nTatsunori Hashimoto, Tengyu Ma\nRigorous mathematical theory plays a foundational role in many engineering and science disci-\nplines (e.g., information theory in electrical engineering). We believe that theory of foundation\nmodels can be particularly beneficial in guiding technical decisions and innovations because of\nthe huge computational costs associated with experimenting on foundation models. In addition,\ntheoretical insights help elucidate fundamental limitations and explain surprising empirical phe-\nnomena. However, the community currently has a limited theoretical understanding of foundation\nmodels, despite much recent progress [Arora et al. 2019b; HaoChen et al. 2021a; Wei et al. 2021,\n2020b; Zhang and Hashimoto 2021; Saunshi et al. 2020b; Dao et al. 2019; Tosh et al. 2020, 2021; Cai\net al. 2021; Lee et al. 2020a; Zimmermann et al. 2021; Bansal et al. 2020; Wang and Isola 2020; Tsai\net al. 2020; Tian et al. 2020a,b; Tripuraneni et al. 2020; Du et al. 2020].\nDeep neural networks form the backbone of foundation models. Even in the well-studied su-\npervised learning setting, where the train and test scenarios have the same distribution, there are\nnumerous open questions around deep nets such as understanding non-convex optimization, the\nimplicit regularization effect of optimizers, and expressivity. Foundation models raise questions\nthat significantly go beyond the supervised deep learning setting. The core problem in theoretically\nanalyzing foundation models is understanding why training on one distribution with a possibly\nunsupervised/self-supervised loss leads to good adaptation performance on different downstream\ndistributions and tasks.87\nWe will discuss an intuitive modularization to analyze foundation models that lays bare the\nconnections between supervised learning and foundation models, concrete and core technical\nquestions, and some promising theoretical tools to address these questions. These new core questions\ncan provide useful insight into foundation models and can be studied in parallel to supervised\ndeep learning theory. While we focus on analyzing the downstream performance, the proposed\nmodularization and tools could prove useful to analyze other metrics of interest such as robustness\nto distribution shifts (§4.8: robustness) and security (§4.7: security).\nFig. 22. The analysis of foundation models from pretraining on diverse data to downstream performance on\nadapted tasks involves capturing the relation between different loss terms as shown above. The main challenge\nis to analyze the highlighted pretraining-adaptation interface which requires reasoning carefully about the\npopulation losses in addition to the model architecture, losses and data distributions of the pretraining and\nadaptation stages (§4.10.2: theory-interface). Analysis of generalization and optimization largely reduces to\ntheir analysis in standard supervised learning.\n87The theory for foundation models closely relates to, but also goes beyond the theory for transfer learning (which is\nitself an underexplored area): foundation models are possibly trained with unlabeled data and will be adapted to many or all\nnatural tasks, whereas transfer learning typically studies labeled source tasks and a fixed number of target tasks.\nOn the Opportunities and Risks of Foundation Models 119\n4.10.1 Theoretical formulations and modularizations.\nRecall that foundation models are trained on a large amount of raw data (§4.2: training) then\nadapted to specific tasks (§4.3: adaptation) and therefore can be decomposed naturally into\ntraining and adaptation phases. We identify interfaces between them and disentangle parts specific\nto foundation models from parts that require standard deep learning theory, so that they can be\nindependently worked on. We introduce a modularized analysis framework, which has also been\nimplicitly or explicitly employed in recent works, e.g., Arora et al. [2019b]; HaoChen et al. [2021a];\nWei et al. [2020b]; Tripuraneni et al. [2020]. The crucial component in this modularized analysis\nturns out to be the pretrain-adaptation interface. We first describe the modularization, and discuss\nwhy we find this modularization promising and finally some limitations.\nWe will refer to the training phase explicitly as “pretraining” to distinguish it from the adaptation\nphase that could also involve training on a few samples from a particular task.\nPretraining phase. The pretraining of foundation models often involves a data distribution 𝑝pre\n(e.g., the distribution of natural text) and a pretraining loss function ℓpre (𝑥;𝜃)that measures the\nloss (e.g., language modeling loss in GPT-3) on an input 𝑥 for a model with parameters 𝜃 ∈Θ. Let\nˆ𝑝pre denote the empirical distribution over a large number of independent samples from 𝑝pre.\nPretraining minimizes the loss ℓpre on ˆ𝑝pre, which we call the empirical pretraining loss , and\nproduces a model ˆ𝜃FM:\nb𝐿pre (𝜃)\ndef\n= E𝑥∼ˆ𝑝pre [ℓpre (𝑥;𝜃)], and ˆ𝜃FM\ndef\n= arg min\n𝜃∈Θ\nb𝐿pre (𝜃). (1)\nWe consider the corresponding loss on the population distribution 𝑝pre, called the population\npretraining loss, as a central concept:\n𝐿pre (𝜃)\ndef\n= E𝑥∼𝑝pre [ℓpre (𝑥;𝜃)]. (2)\nOptimization-based adaptation phase. We frame adaptation as a general constrained optimiza-\ntion problem that depends on ˆ𝜃FM, abstracting away those adaptation methods that are based on\noptimizing certain loss functions such as fine-tuning and prompt-tuning (see, e.g., [Houlsby et al.\n2019; Li and Liang 2021; Lester et al. 2021], and §4.3: adaptation).\nSince different adaptation methods could modify different subsets of the model parameters, we\ndenote the space of adapted model parameters by some Γ. Given a downstream task distribution\n𝑝task (e.g., question answering in a particular domain) and a few empirical samples ˆ𝑝task sampled\nfrom 𝑝task, we model the adaptation phase as minimizing some adaptation loss ℓadapt on ˆ𝑝task w.r.t\nadapted parameters 𝛾 ∈Γ:\n𝛾task (ˆ𝜃FM)\ndef\n= arg min\n𝛾∈Γ,𝐶(𝛾; ˆ𝜃FM)≤𝑐0\nb𝐿adapt (𝛾, ˆ𝜃FM), (3)\nwhere b𝐿adapt (𝛾, ˆ𝜃FM)\ndef\n= E𝑥∼ˆ𝑝task [ℓadapt (𝑥;𝛾, ˆ𝜃FM)]is the empirical adaptation loss, and𝐶(𝛾, ˆ𝜃FM)≤ 𝑐0\nis an optional constraint that controls the complexity of the adapted parameters, encompassing\nboth explicit regularization (e.g., model dimensionality and norm) and the implicit regularization\nof the adaptation process.\nWe list some common adaptation methods and discuss the corresponding adapted parameter𝛾\nand constraints 𝐶(𝛾, ˆ𝜃FM)≤ 𝑐0.\n(1) Linear probing: training a linear classifier on top of the representations from a foundation\nmodel. Here Γ = R𝑘 is the set of linear classifiers on the representations of dimensionality 𝑘,\nand 𝐶(𝛾, ˆ𝜃FM)could be the ℓ2 or ℓ1 norm of 𝛾.\n120 Center for Research on Foundation Models (CRFM)\n(2) Fine-tuning: optimizing a randomly initialized linear head for a few steps, and all other\nparameters 𝜃 from the initialization of ˆ𝜃FM. Here 𝛾 is the concatenation of 𝜃 and the linear\nhead. Such a process could correspond to some implicit regularization of 𝛾 towards the\ninitialization ˆ𝜃FM captured by 𝐶(𝛾, ˆ𝜃FM)≤ 𝑐0. The exact term 𝐶(𝛾, ˆ𝜃FM)would depend on the\noptimization algorithm used, and such a characterization of the implicit regularization of\noptimization is an area of active research study [e.g., Gunasekar et al . 2017; Soudry et al.\n2018; Gunasekar et al. 2018; Arora et al. 2019a; Blanc et al. 2019; Woodworth et al. 2020; Wei\net al. 2020a; HaoChen et al . 2021b; Damian et al . 2021; Kumar et al . 2022, and references\ntherein].88\n(3) Prompt-tuning: optimizing a small set of continuous task-specific vectors that prepend the\ntask inputs. Here 𝛾 is the continuous prompt vectors which often has small dimensionality,\nand we may optionally have a constraint on the norms of 𝛾.\nOne obvious limitation to note is that this formulation excludes adaptation methods such as\nin-context learning [Brown et al. 2020] where there is no “training” (i.e., the minimization of some\nempirical adaptation loss) during the adaptation phase. We discuss this and other limitations in\n§4.10.3: theory-incontext.\nTwo central quantities for the adaptation phase are the population adaptation loss\n𝐿adapt (𝛾, ˆ𝜃FM)= E𝑥∼𝑝task [ℓadapt (𝑥;𝛾, ˆ𝜃FM)] (4)\nand the minimal adaptation loss\n𝐿★\nadapt (ˆ𝜃FM)= min\n𝛾∈Γ,𝐶(𝛾; ˆ𝜃FM)≤𝑐0\n𝐿adapt (𝛾, ˆ𝜃FM) (5)\nSeparate analysis for modularized phases. Existing generalization theory for standard super-\nvised learning aims to show that b𝐿pre ≈𝐿pre and b𝐿adapt ≈𝐿adapt. Addressing these questions\nspecifically for deep nets is an active research area. We can also leverage the standard learning\ntheory decomposition to bound the final downstream task loss by the excess generalization error\nand the minimal adaptation loss as follows.\n𝐿adapt (𝛾task, ˆ𝜃FM)≤ 𝐿★\nadapt (ˆ𝜃FM)\n|       {z       }\nminimal adaptation loss\n+generalization error (6)\nwhere the generalization error captures the closeness between𝐿adapt and b𝐿adapt.89 The decomposition\nand relationship between these key quantities are shown in Figure 22. The generalization and\noptimization arrows, as argued above, largely reduce to deep learning theory in the supervised\nsetting. What we are left with is the main challenge with foundation models, which is to understand\nwhy the minimal adaptation loss𝐿∗\nadapt (ˆ𝜃FM)can be small as a result of a small pretraining population\nloss, which study in §4.10.2: theory-interface.\nThe work of Arora et al. [2019b] pioneered the pursuit of this question by bounding from above\n𝐿★\nadapt (ˆ𝜃FM)by 𝐿pre (ˆ𝜃FM)in the context of contrastive learning, and HaoChen et al. [2021a]; Tosh\net al. [2020, 2021] relax the data assumptions. Other pretraining methods successfully analyzed\n88It may not always be feasible to characterize the inductive bias of adaptation via an explicit constraint 𝐶(𝛾, ˆ𝜃FM)≤ 𝑐0.\nThe modularization we propose is also applicable in these cases, but for notational simplicity, we focus on the case where\nimplicit regularization can be approximated via an explicit constraint.\n89More precisely, the generalization error term is the sum of𝐿adapt (𝛾task, ˆ𝜃FM)−b𝐿adapt (𝛾task, ˆ𝜃FM)and b𝐿adapt (𝛾★\ntask, ˆ𝜃FM)−\n𝐿adapt (𝛾★, ˆ𝜃FM) = b𝐿adapt (𝛾★\ntask, ˆ𝜃FM)− 𝐿★\nadapt (ˆ𝜃FM), where 𝛾★\ntask is the minimizer of (5). (6) follows easily be using\nb𝐿adapt (𝛾task, ˆ𝜃FM)≤ b𝐿adapt (𝛾★\ntask, ˆ𝜃FM).\nOn the Opportunities and Risks of Foundation Models 121\nunder this framework (implicitly or explicitly) include pretraining with language models [Wei et al.\n2021] or self-supervision [Lee et al. 2020a], with self-training algorithms [Wei et al. 2020b; Cai et al.\n2021], and with multiple supervised tasks [Tripuraneni et al. 2020; Du et al. 2020].\n4.10.2 Why is the pretraining-adaptation interface interesting?.\nAs shown in Figure 22, the main missing link beyond standard supervised theory is:\nUnder what conditions does a small population pretraining loss 𝐿pre (ˆ𝜃FM)imply a small minimal\nadaptation loss 𝐿★\nadapt (ˆ𝜃FM)and why?\nThe conditions that lead to a successful interface could depend on several quantities such as the\npretraining and adaptation distributions, objectives and training methods, as well as the model\narchitecture. This question is beyond the scope of standard generalization theory, but it does narrow\nus down to a few important factors specific to foundation models, and captures the essence of\nvarious important open questions on foundation models as we argue below.\nFirst, we note that this interface deals with population quantities that concern two different\ndistributions. Hence, the conditions for a successful interface are likely to involve special properties\nof the distributions, for example, the diversity of the pretraining distribution and structural shifts\nbetween the pretraining and adaptation data. This makes the analysis of the interface challenging\n(as discussed below in §4.10.4: theory-tools) as we need to make careful modeling assumptions\nabout how the two distributions relate to one another. However, this presents the possibility that\ntools and techniques developed to analyze such interfaces could be useful to understand the effect\nof distribution shifts and to predict when foundation models can improve robustness.\nSecond, the population losses and possibly the conditions of a successful interface depend on\nthe model architecture . This raises the challenge of opening up the black-box of the neural nets.\nWhat does a small pretraining loss on a particular distribution tell us about the properties of the\nintermediate layers? Such analyses would also guide us in designing new adaptation methods that\nmore carefully exploit different intermediate representations.\nThird, few-shot learning or the sample efficiency of adaptation can be captured through the\nconstraint on the complexity measure 𝐶(𝛾, ˆ𝜃FM)< 𝑐0 in the minimal adaptation loss. We need to\nformally characterize these complexity measures (e.g., by understanding the implicit regularization\neffect of the adaptation process) and further understand why a small population pretraining loss\nwould imply a low-complexity adaptation parameters 𝛾task. A satisfactory answer to this question\nwould likely allow us to improve the sample-efficiency of downstream adaptation.\nFinally, and importantly, critical components of the interface are the choice of thepretraining\nand adaptation losses . We want to understand how to best combine the pretraining and adaptation\nobjectives for successful adaptation. It is possible that the pretraining objective that best guarantees\nsuccessful adaptation differs from what is explicitly minimized during the pretraining process — the\ninterface above allows one to use any surrogate population objective on the pretraining distribution.\nIn addition, new surrogate objectives that provably lead to good adaptation across a broad set of\ntasks could shed light on the fundamental aspects that make foundation models successful.\nTo summarize, the interface precludes the issue of generalization and allows us to formally\nreason about the interaction between several important quantities of the pretraining and adaptation\nphases that can guide practice in important ways.\n4.10.3 Challenge: analysis of in-context learning and other emergent behavior.\nGPT-3 [Brown et al. 2020] demonstrates the power of in-context learning, an adaptation method\nthat does not need any parameter optimization. In the adaptation phase, the pretrained language\nfoundation model takes in a prompt — a sequence of tokens that concatenates input-output examples\n122 Center for Research on Foundation Models (CRFM)\nfrom the task — followed by a test example and simply generates the label of the test example\nby conditioning on the sequence seen thus far (prompt plus test example). In other words, there\nis no explicit training or change to the model parameters. What is the mechanism by which the\nmodel “learns” from the different examples by simply executing with the examples as inputs? The\nprevious modularization does not directly apply because we do not obtain new model parameters\nduring adaptation, but rather we only use the generative capabilities of the foundation model by\nexecuting on structurally-designed inputs. However, the idea of separating pretraining with infinite\ndata and pretraining with finite data can still be useful. For example, a recent work starts with\nthe assumption of infinite pretraining data and sufficient model expressivity to study in-context\nlearning [Xie et al. 2021c]. These assumptions reduce the characterization of in-context learning to a\nmatter of analyzing the pretraining distribution conditioned on in-context learning prompts, which\nare drawn from a different distribution than the pretraining data. In particular, Xie et al. [2021c]\nproposes that in-context learning emerges from long-term coherence structure in the pretraining\ndistribution, which is described by a latent variable generative model with coherence structure.\nMore broadly, while the modularization proposed in this section provides a nice framework to gain\nuseful theoretical insights into foundation models, it is possible that some emergent behavior like\nin-context learning and other capabilities yet to be discovered would require going beyond the\nmodularization, e.g., by opening the black box of the architecture.\n4.10.4 Challenge: appropriate data assumptions and mathematical tools.\nUnderstanding the interface between pretraining and adaptation phases requires a more careful\nstudy of data distributions than in traditional supervised learning. This is because the pretraining\nand task adaptation distributions are inherently different. By definition, foundation models are\ntrained on raw data that is typically extremely diverse and task-agnostic, while the adaptation\ndata depends heavily on the task. Similarly, in-context learning emerges as a result of learning to\ngenerate data that looks like the pretraining distribution, and thereby understanding in-context\nlearning requires careful modeling of the pretraining data. Hence answering the central questions\naround foundation models requires realistic and interpretable assumptions that are also amenable to\nanalysis. Recent works either assume certain properties of the population data, e.g., the expansion\nproperty in HaoChen et al. [2021a]; Wei et al. [2020b], or that the population data is generated\nfrom latent variable models with some structure [Saunshi et al. 2020a; Wei et al. 2021; Arora et al.\n2016; Lee et al. 2020a; Zhang and Hashimoto 2020; Tosh et al. 2021].\nWe generally lack mathematical tools for relating properties of foundation models to the structure\nin the population data distribution. HaoChen et al. [2021a] applies spectral graph theory to leverage\nthe inner-class connectivity in the population distribution. More precise characterization of ˆ𝜃FM via\nprobabilistic and analytical derivations is possible for latent variable models, but so far restricted to\nrelatively simple ones. The community will significantly benefit from more systematic and general\nmathematical tools to address this question.\nIt is also highly desirable to define simple toy cases so that theoreticians can precisely compare\nthe strengths of various tools and analyses. For example, HaoChen et al . [2021a] and Wei et al.\n[2020b] consider the mixture of manifolds problem which might potentially be a good simplified\ntest bed for vision applications. We need more interesting test beds for discrete domains such as\nNLP. We believe that tractable theoretical models which capture relevant properties of real datasets\nare a crucial step towards placing foundation models on solid theoretical footing.\nOn the Opportunities and Risks of Foundation Models 123\n4.11 Interpretability\nAuthors: John Hewitt*, Armin W. Thomas*, Pratyusha Kalluri, Rodrigo Castellon, Christopher D.\nManning\nCompared to most other machine learning models, foundation models are characterized by a vast\nincrease in training data and complexity and the emergence of unforeseen capabilities: foundation\nmodels are able to do unforeseen tasks and do these tasks in unforeseen ways. The increasing\nadoption of foundation models thereby creates growing desires, demands, and unprecedented\nchallenges for understanding their behavior.\nIn contrast to task-specific models, foundation models are trained across vast and usually highly\ndisparate datasets, potentially spanning many domains and modalities (see §4.2:training). Through\nthis training, foundation models learn an exceptionally wide range of behaviors, which can vary\nprofoundly between tasks and domains, as demonstrated by their ability to be adapted to different\ntypes of downstream tasks and to exhibit behaviors that are specific for each of these tasks (see\n§4.3: adaptation). Take GPT-3 as an example, which was trained as one huge model to simply\npredict the next word in a text. While this is a very specific and simple-to-define learning task, it\nhas enabled GPT-3 to gain capabilities that far exceed those that one would associate with next\nword prediction, by combining it with a vast training dataset that comprises all kinds of internet\ntext. As a result, GPT-3 can now adapt behaviors that are clearly outside of the scope of its original\ntraining task, such as simple arithmetic and computer programming, when provided with a few\ntraining samples. This demonstrates that it is challenging to answer even the seemingly simplest\nquestion about a foundation model: what capabilities does it have?\nMoreover, it is an open question to what extent these diverse capabilities rely on distinct or\nshared model mechanisms , akin to algorithmic building blocks within the model. On the one hand,\nfoundation models can be interpreted as single models, which utilize some set of generalizable\nmodel mechanisms to perform well across tasks and domains. In this case, a full understanding of\ntheir behavior can be gained by identifying and characterising these mechanisms. On the other\nhand, the ability of foundation models to adapt profoundly distinct behaviors for different tasks\nsuggests that they can also be understood as a large collection of independent expert models, each\ntailored to a specific task. For example, it seems unlikely that the model parameters that GPT-3\nuses to do arithmetic could have much to do with the parameters used to translate from English\nto French. In this case, explanations of model behavior in one task are therefore not necessarily\ninformative about behavior in other tasks. We refer to this as the one model–many model nature of\nfoundation models (see Figure 23) and argue that understanding where foundation models lie on\nthis spectrum between one and many models will be central to understanding their behavior.\nToward systematizing this area of study, we present and discuss three levels of understanding\nfoundation models [inspired by Marr 1982]: we first discuss the challenges and opportunities in\nunderstanding what a model is capable of doing, then why it outputs certain behaviors, and lastly\nhow it does it. Specifically, questions ofwhat aim to characterize the kinds of behaviors that a model\ncan perform without peeking inside the model, while questions of why aim to provide explanations\nof the model’s behaviors in terms of potential causes in the data, and questions of how aim to\nunderstand the internal model representations and mechanisms that produce these behaviors. After\npresenting all three levels, we conclude by discussing potential consequences resulting from the\nnon-interpretability and interpretability of foundation models.\n124 Center for Research on Foundation Models (CRFM)\nFig. 23. The one model–many model nature of foundation models: A central interpretability question is to\nunderstand where a foundation model lies on the spectrum between one model and many models. As one\nmodel, behavior can be made interpretable by identifying and characterising the finite number of generalizable\nmodel mechanisms used to produce behaviors across tasks (e.g., mechanisms that assign meaning to words,\ncompare quantities, and perform arithmetic). As many models, explanations of model behavior in one task are\nnot necessarily informative about behavior in other tasks, thus requiring the independent study of behavior\nin each task.\n4.11.1 Characterizing behavior.\nThe simplest understanding of a technology is widely taken to be knowing what the technology\ndoes. This seemingly straightforward question is significantly challenging for foundation models,\ndue to the myriad unforeseen behaviors and tasks that these models are capable of performing.\nTask-specific neural network models are trained to perform a single task in a single domain,\ne.g., image classification. Their task and the input and output domains are therefore clear; yet even\nfor these models it can be challenging to know exactly what the model will do, given a particular\ninput. For instance, model behaviors can unexpectedly differ greatly for two perceptually similar\ninputs [Garg and Ramakrishnan 2020; Jin et al . 2020] or two subpopulations of the same data\n(stratified, for example, by race or gender [Hovy and Søgaard 2015; Blodgett et al. 2016; Tatman\n2017; Buolamwini and Gebru 2018]).\nThis challenge of characterizing a model’s behavior is amplified manyfold for foundation models.\nThe space of tasks that the model is able to perform is generally large and unknown, the input and\noutput domains are often high-dimensional and vast (e.g., language or vision), and the models are\nless restricted to domain-specific behaviors or failure modes. Consider, for example, the surprising\nability of GPT-3 to be trained on large language corpora and to subsequently develop the ability\nto generate mostly-functional snippets of computer programs. A key challenge for characterizing\nthe behavior of foundation models is therefore to identify the capabilities that it has. Even further,\nfor each task that a foundation model can perform, and there may be many or infinitely many, all\nOn the Opportunities and Risks of Foundation Models 125\nthe challenges remain that one faces when trying to understand the behavior of much simpler,\ntask-specific models.\nCharacterizing each ‘task’ that a foundation model can perform is further complicated by their\none model–many models nature (see Figure 23). Again taking GPT-3 as an example, it was shown\nthat it can be tailored to many tasks through simple prompting (see §4.3: adaptation). Yet, each\ntask can be specified through many possible prompts and slight variations in prompts can result in\nmeaningful changes of model behavior. For instance, the task of sentiment classification of a movie\nreview can be specified by presenting the movie review followed by ‘Her sentiment towards the\nfilm was... ’ or ‘My overall feeling was that the movie was... ’; despite these prompts appearing to\npose closely related tasks, GPT-3 will exhibit different response accuracies for each prompt [Zhao\net al. 2021]. Observations like these raise important questions regarding the relationship between\nthe characteristics of prompts and the resulting model behaviors. Specifically, can meaningfully\ndifferent responses to seemingly similar prompts actually be considered as resulting from the\nsame model or do they result from highly distinct model mechanisms, and does characterizing the\nbehaviors of the foundation model (or its adapted derivatives) in one task truly aid in characterizing\nthe behaviors of other possible adaptations of the model?\nTo identify the capabilities that a foundation model has and those it is missing, researchers can\nutilize controlled evaluations. Here, domain experts design prompts that are known to require\na particular competence and then study the ability of a model to respond correctly to these\nprompts [Papadimitriou and Jurafsky 2020; Lu et al. 2021a; Kataoka et al. 2020; Wu et al. 2021c;\nXie et al. 2021a; Koh et al. 2021]. For example, psycholinguists have designed prompts that require\na language model to choose between a grammatically correct sentence and the same sentence\nwith a specific grammatical inaccuracy; knowing whether the model consistently prefers the\ngrammatically correct sentence over its grammatically incorrect counterpart tells us whether the\nmodel has the particular grammatical competence required to identify this inaccuracy [Linzen et al.\n2016].\nGiven the huge range of possible capabilities of foundation models, and our current lack of any\ngeneral method for determining a priori whether a foundation model will have a given capability,\nbespoke evaluations like these are crucial. They allow exploring the range of behaviors that\nfoundation models are capable of, while requiring minimal model access: we only need to present\ninputs and receive model outputs, and we need not depend on access to the implementation\nor parameters of a model. Given the infinitely many desirable and undesirable tasks, subtasks,\nand behaviors that foundation models may be capable of (or incapable of), characterizing model\nbehaviors and capabilities will be increasingly challenging and important. We believe that instead of\nrelying on a few experts to formulate and test for possible behaviors, it will be critical to extend these\ntypes of analyses to test for many more behaviors, in part by opening up this line of exploration to\ndiverse communities and experts in many disciplines, as well as by increasing access to and scale\nof these evaluations.\n4.11.2 Explaining behavior.\nIn addition to characterizing what a foundation model is doing, one can try to characterize why\nit performs certain behaviors by providing explanations of these behaviors in terms of potential\ncauses in the data. While current explanation approaches, which provide such explanations of\nbehavior, can reveal qualities of inputs that affect a model’s responses, they often require full access\nto the model to do so and are generally limited in their ability to elucidate any general model\nmechanisms, which foundation models use to respond to many inputs, tasks, and domains.\nCurrent explanatory approaches can generally be understood as distinct models, which are\ndesigned to provide an explanation of particular behaviors of anotherblack box model. Importantly,\n126 Center for Research on Foundation Models (CRFM)\nthese approaches are separate from the model whose behavior is analyzed, which by itself is not\ninterpretable. This separation can be problematic, as the provided explanations can lack faithfulness\n[Jacovi and Goldberg 2020], by being unreliable and misleading about the causes of a behavior\n[cf. Rudin 2019]. Even further, unsound explanations can entice humans into trusting unsound\nmodels more than they otherwise would (for a detailed discussion of trust in artificial intelligence,\nsee Jacovi et al. [2021]). These types of concerns grow as we transition from task-specific models\ntowards the wide adoption of foundation models, as their behavior is vastly more complex.\nCurrent explanatory approaches can largely be divided into either providing local or global\nexplanations of model behavior [Doshi-Velez and Kim 2017]. Local explanations seek to explain a\nmodel’s response to a specific input, e.g., by attributing a relevance to each input feature for the\nbehavior or by identifying the training samples most relevant for the behavior [Simonyan et al.\n2013; Bach et al. 2015; Sundararajan et al. 2017; Shrikumar et al. 2017; Springenberg et al. 2014;\nZeiler and Fergus 2014; Lundberg and Lee 2017; Zintgraf et al. 2017; Fong and Vedaldi 2017; Koh\nand Liang 2017]. Global explanations, in contrast, are not tied to a specific input and instead aim to\nuncover qualities of the data at large that affect model behaviors, e.g., by synthesizing the input\nthat the model associates most strongly with a behavior [Simonyan et al. 2013; Nguyen et al. 2016].\nLocal and global explanations have provided useful insights into the behavior of task-specific\nmodels [e.g., Li et al. 2015; Wang et al. 2015b; Lapuschkin et al. 2019; Thomas et al. 2019; Poplin et al.\n2018]. Here, the resulting explanations are often taken to be a heuristic of the model mechanisms\nthat gave rise to a behavior; for example, seeing that an explanation attributes high importance to\nhorizontal lines when the model reads a handwritten digit ‘7’ easily creates the impression that\nhorizontal lines are a generally important feature that the model uses to identify all sevens or\nperhaps to distinguish all digits.\nGiven the one model–many models nature of foundation models, however, we should be careful\nnot to jump from specific explanations of a behavior to general assumptions about the model’s\nbehavior. While current explanatory approaches may shed light on specific behaviors, for example,\nby identifying aspects of the data that strongly effected these behaviors, the resulting explanations\ndo not necessarily provide insights into the model’s behaviors for other (even seemingly similar)\ninputs, let alone other tasks and domains.\nAnother approach could be to sidestep these types of post-hoc explanations altogether by\nleveraging the generative abilities of foundation models in the form of self-explanations [cf. Elton\n2020; Chen et al . 2018], that is, by training these models to generate not only the response to\nan input, but to jointly generate a human-understandable explanation of that response. While it\nis unclear whether this approach will be fruitful in the future, there are reasons to be skeptical:\nlanguage models, and now foundation models, are exceptional at producing fluent, seemingly\nplausible content without any grounding in truth. Simple self-generated “explanations” could\nfollow suit. It is thus important to be discerning of the difference between the ability of a model to\ncreate plausible-sounding explanations and providing true insights into its behavior.\n4.11.3 Characterizing model mechanisms.\nDeep understanding of systems is generally taken to mean understanding how a system performs:\nwhich knowledge and mechanisms does it contain, and how are these assembled to form the whole?\nIf this is indeed possible, characterizing the representations within foundation models and the\nmechanisms that operate on them will be central to satisfying the desire to thoroughly understand\nthese proliferating models; and whether these mechanisms are many and specific or few and\ngeneralizable, they are at the core of the ability of foundation models to adopt a wide range of\nbehaviors in varied tasks and domains.\nOn the Opportunities and Risks of Foundation Models 127\nTo make the notions of model representations and mechanisms concrete, consider a simple\nbehavior exhibited by GPT-3: It was quickly observedwhat GPT-3 did when provided with examples\nof the addition of small numbers and then queried to perform addition of two new numbers: with\nhigh probability, it predicted the correct result of the addition [Branwen 2020; Brockman 2020].\nWhen asking why GPT-3 performed as it did, one could find evidence in the input, like aspects of\nits prompt that highly affected its response (these might be the two numbers to be added, though\nnot necessarily), or aspects of GPT-3’s training data that affected its response (these might be\nexamples of addition, though not necessarily). Delving into the model, we may envision a deeper\nunderstanding of the mechanisms that GPT-3 uses to add a specific pair of numbers and the\nmechanism that it uses to add other arbitrary pairs of numbers. We may also envision a deeper\nunderstanding of whether these mechanisms are similar to the mathematical notion of ‘addition’\nor merely correlated with this notion.\nBy understanding individual model mechanisms, we can build up a compositional understanding\nof complex behaviors of a foundation model. A task slightly more complex than the addition of\nnumbers is solving mathematical word problems, in which numbers come with units and the\nproblem is presented in natural language. Once we understand the mechanism (or mechanisms)\nby which a model performs addition, we can investigate whether this mechanism is used as an\nintermediate step in solving word problems. If the addition mechanism is used, we have built up\nour understanding of how the model solves word problems, we have increased confidence that the\nfoundation model generalizes the notions of quantities and addition (not another correlation or\nheuristic), and, furthermore, we have increased confidence in our ability to predict the model’swhy\n(which parts of the inputs it is attending to) and the output’s what (addition of two numbers). If the\naddition mechanism is not used, we may retain a healthy skepticism that this is truly addition, and\nwe can investigate which representations and mechanisms are used instead.\nIt is important to be aware that there are many potential cases of more complex and concerning\nmodel mechanisms, for instance, the estimation of race from the characters in a name, or the\npixels in an image. Establishing evidence of such a mechanism in a foundation model and its use\ncan support a moral or legal responsibility to ban the model from tasks like predictive policing,\nmarketing, loan applications, and surveillance at large.\nA plethora of methods have emerged to investigate these internal aspects of neural network\nmodels. Typically, these approaches separate the model into nodes (e.g., neurons, layers, or parts\nof layers), then interrogate either the representations captured in nodes or the mechanisms by\nwhich nodes are assembled. Some approaches are hypothesis driven: by hypothesizing that nodes\nmay capture certain information (e.g., a grammatical feature of a word, or the race of a person),\none can probe all nodes to quantify how much of that information they make available [Alain\nand Bengio 2016; Veldhoen et al. 2016; Belinkov et al. 2017; Adi et al. 2017; Conneau et al. 2018;\nHewitt and Liang 2019; Hewitt and Manning 2019; Voita and Titov 2020; Pimentel et al . 2020].\nOther approaches build on explanatory methods, and, instead of identifying which data cause\na certain behavior, they seek to identify which data cause a certain node to activate, or which\nnodes cause another node later in the model to activate, thereby uncovering collections of model\nrepresentations and mechanisms [Olah et al. 2020; Mu and Andreas 2020; Carter et al. 2019; Goh\net al. 2021]. Taken together, these approaches inspect the interior of models and provide a basis\nfor the ongoing explorations of the behavior of foundation models. Yet, the number of potential\nrepresentations and mechanisms within foundation models is vast, particularly given their one\nmodel–many models nature, and these types of approaches often only capture a small slice of a\nmodel’s interiority. It is thus an open challenge to expand the discovery of representations and\nmechanisms and to elucidate those that are most relevant or general for model behavior. As with\nmany approaches to interpreting foundation models, these types of explorations will benefit from\n128 Center for Research on Foundation Models (CRFM)\nincluding and supporting more diverse and interdisciplinary investigators and from more accessible,\nflexible, and scalable methods of discovery.\nIn summary, we believe that the one model–many models nature of foundation models (recall\nFigure 23) provides novel opportunities and challenges for current interpretability research: there\nare many adaptations of a single foundation model, and we simply do not know the extent to\nwhich they share common mechanisms. To the extent that mechanisms are shared, understanding\nfoundation models may be a tractable problem of characterizing these mechanisms and their\nrelations. To the extent that mechanisms are independent, each adaptation of a foundation model\nmust be analyzed independently, leading to profound uncertainty about the nature of any new\nadaptation of the foundation model.\n4.11.4 Impacts of non-interpretability and interpretability.\nLastly, we would like to highlight that the wide adoption of foundation models is at odds with a\nrecent plea of many interdisciplinary researchers not to use complex black box models for high\nstakes decisions [e.g., Rudin 2019], but instead to focus on the long-standing development and\napplication of more intrinsically interpretable models.\nIn the midst of these pleas, work aimed at interpreting foundation models is a double-edged sword.\nLarge machine learning models, and now foundation models, are most often deployed by powerful\ncorporations and institutions, and incremental advances in interpretability can be exaggerated to\n‘ethics-wash’ and continue use of models as though they have achieved interpretability, belying the\nreality that they remain far below traditional standards of algorithmic interpretability. Moreover,\nwhen approaches to interpretability regularly presume easy access to models and their implementa-\ntion and parameters, interpretability can serve not only as cover for powerful institutions but also\ncentralize model knowledge in the same hands. For those working toward the interpretability of\nfoundation models, it is a responsibility to consistently ask whether one is working toward making\nfoundation models interpretable to researchers and model owners or interpretable to everyone .\nSimultaneously, to the extent that foundation models are already being deployed, work on\ninterpretability presents unique opportunities to shift knowledge of foundation models, and thus\npower, back to datafied and evaluated peoples. Interpretation can facilitate the discovery of societally\nsalient aspects of models. More radically, work creating accessible methods that allow anyone to\ninterpret the behavior of foundation models shifts power to diverse peoples, creating opportunities\nto investigate models, opportunities to discover aspects of models important to individuals or their\ncommunities, and opportunities to meaningfully consent to, improve, or altogether contest the\nuse of foundation models. Finally, it is important for researchers to view the interpretability of\nfoundation models as not only a goal, but a question: research can explore and assess whether\nthe lack of foundation model interpretability is intrinsic and should be deeply studied and widely\nknown as a serious issue discouraging use (or increasing regulation) of these systems, or whether\nit is possible for future foundation models to uphold a high standard of interpretability for all.\nOn the Opportunities and Risks of Foundation Models 129\n5 SOCIETY\nThe societal impact of foundation models, referring both to the construction of the models them-\nselves and their role in developing applications, requires careful examination. Specifically, we\nanticipate that foundation models will have wide-ranging societal consequences that are challeng-\ning to understand: foundation models are intermediary assets that are not directly deployed, but\nrather serve as a foundation that is further adapted. As a result, traditional approaches to reasoning\nabout the societal impact of technology are likely complicated; societal impact is easier (but still\ndifficult) to grasp for systems with well-specified purposes. In this chapter, we discuss how we\nmay grapple with and beginning to understand the complexity of the societal impact of models\nfoundation models. Specifically, we discuss (i) the harms with respect to inequity (§5.1: fairness)\nand misuse (§5.2: misuse), (ii) the impact with respect to the economy (§5.5: economics) and\nenvironment (§5.3: environment), and (iii) the broader considerations with respect to the law\n(§5.4: legality) and ethics (§5.6: ethics).\n130 Center for Research on Foundation Models (CRFM)\n5.1 Inequity and fairness\nAuthors: Rishi Bommasani, Fereshte Khani, Esin Durmus, Faisal Ladhak, Dan Jurafsky\nFig. 24. The intrinsic bias present within foundation models is the byproduct of various training bias sources\n(left) which, alongside biases introduced during adaptation, determines theextrinsic harms (right) experienced\nby users in the context of specific downstream applications. We emphasize that the same foundation model\nis the shared foundation for many different applications; its biases propagate to these many applications as a\nresult. Further, since the harms experienced by users are the result of specific adapted models, attributing\nthese harms to the various processes and sources depicted in this diagram is both crucial and challenging.\n5.1.1 Introduction.\nFoundation models have the potential to yield inequitable outcomes: the treatment of people that is\nunjust, especially due to unequal distribution along lines that compound historical discrimination\n[Hellman 2021]. Like any AI system, foundation models can compound existing inequities by\nproducing unfair outcomes, entrenching systems of power, and disproportionately distributing\nnegative consequences of technology to those already marginalized [Sweeney 2013; Kay et al. 2015;\nBuolamwini and Gebru 2018; Benjamin 2019; Ajunwa 2019; D’Ignazio and Klein 2020; Crawford\n2021]. Here we ask what fairness-related harms relate to foundation models, what sources are\nresponsible for these harms, and how we can intervene to address them. The issues we discuss\nhere are related to broader questions of algorithmic fairness and AI ethics [Corbett-Davies and\nGoel 2018; Chouldechova and Roth 2020; Hellman 2020; Johnson 2020; Fazelpour and Danks 2021],\nrace and technology [Benjamin 2019; Hanna et al . 2020; Gebru 2021; Field et al . 2021], and the\ncoexistence of society and technology [Abebe et al. 2020].\n5.1.2 Harms.\nFoundation models are intermediary assets with no specified purpose before they are adapted;\nunderstanding their harms requires reasoning about both their properties and the role they play in\nbuilding task-specific models. We delineateintrinsic biases,90 i.e., properties of the foundation model\nthat indirectly but pervasively affect downstream applications, and extrinsic harms, i.e., harms that\narise in the context of specific downstream applications [Galliers and Spärck Jones 1993].\n90We use the word bias to denote the properties of a foundation model that contribute to inequity; we follow Blodgett\net al. [2020] in attempting, when possible, to delineate who is harmed and how they are harmed.\nOn the Opportunities and Risks of Foundation Models 131\nIntrinsic biases. Properties of the foundation model can lead to harm in downstream systems.\nAs a result, these intrinsic biases can be measured directly within the foundation model, though\nthe harm itself is only realized when the foundation model is adapted, and thereafter applied,\ni.e., these are latent biases or harms [DeCamp and Lindvall 2020]. We focus on the most widely\nstudied form of intrinsic bias, representational bias, specifically considering misrepresentation,\nunderrepresentation and overrepresentation. People can be misrepresented by pernicious stereo-\ntypes [Bolukbasi et al. 2016; Caliskan et al. 2017; Abid et al. 2021; Nadeem et al. 2021; Gehman et al.\n2020] or negative attitudes [Hutchinson et al. 2020], which can propagate through downstream\nmodels to reinforce this misrepresentation in society [Noble 2018; Benjamin 2019]. People can be\nunderrepresented or entirely erased, e.g., when LGBTQ+ identity terms [Strengers et al. 2020;\nOliva et al. 2021; Tomasev et al. 2021] or data describing African Americans [Buolamwini and Gebru\n2018; Koenecke et al. 2020; Blodgett and O’Connor 2017] is excluded in training data, downstream\nmodels will struggle with similar data at test-time. People can be overrepresented, e.g., BERT\nappears to encode an Anglocentric perspective [Zhou et al. 2021a] by default, which can amplify\nmajority voices and contribute to homogenization of perspectives [Creel and Hellman 2021] or\nmonoculture [Kleinberg and Raghavan 2021] (§5.6:ethics). These representational biases pertain to\nall AI systems, but their significance is greatly heightened in the foundation model paradigm. Since\nthe same foundation model serves as the basis for myriad applications, biases in the representation\nof people propagate to many applications and settings. Further, since the foundation model does\nmuch of the heavy-lifting (compared to adaptation, which is generally intended to be lightweight),\nwe anticipate that many of the experienced harms will be significantly determined by the internal\nproperties of the foundation model.\nExtrinsic harms. Users can experience specific harms from the downstream applications that are\ncreated by adapting a foundation model. These harms can berepresentational [Barocas et al. 2017;\nCrawford 2017; Blodgett et al. 2020], such as the sexualized depictions of black women produced by\ninformation retrieval systems [Noble 2018], the misgendering of persons by machine translation\nsystems that default to male pronouns [Schiebinger 2013, 2014], or the generation of pernicious\nstereotypes [Nozza et al. 2021; Sheng et al. 2019; Abid et al. 2021]. They can consist of abuse, such\nas when dialogue agents based on foundation models attack users with toxic content [Dinan et al.\n2021; Gehman et al. 2020] or microaggressions [Breitfeller et al. 2019; Jurgens et al. 2019]. All of\nthese user-facing behaviors can lead to psychological harms or the reinforcement of pernicious\nstereotypes [Spencer et al. 2016; Williams 2020].\nIn addition to harms experienced by individuals, groups or sub-populations may also be subject\nto harms such as group-levelperformance disparities. For example, systems may perform poorly\non text or speech in African American English [Blodgett and O’Connor 2017; Koenecke et al. 2020],\nincorrectly detect medical conditions from clinical notes for racial, gender, and insurance-status\nminority groups [Zhang et al. 2020b], or fail to detect the faces of people with darker skin tones\n[Wilson et al. 2019; Buolamwini and Gebru 2018]. As foundation models are more pervasively\napplied, including in high-stakes domains, these disparities can spiral into further, and more severe,\nharms. Koenecke et al. [2020] discuss how if African American English speakers cannot reliably\nuse speech recognition technologies (e.g., due to inequities in underlying foundation models), this\nmay mean they cannot benefit from certain derivative products (e.g., voice assistants, assistive\ntechnologies) and will be disadvantaged if these technologies are used to conduct interviews for\nemployment or transcribe courtroom proceedings. More generally, characterizing these group-level\nharms (and working towards justice for those harmed) also requires the AI community to improve\nits understanding of group-based prejudice [Allport 1954] and social groups: we point to relevant\nwork in the social sciences and other communities on moving beyond binary treatments of gender\n132 Center for Research on Foundation Models (CRFM)\n[Lindsey 2015; Westbrook and Saperstein 2015; Richards et al . 2017; Darwin 2017; Keyes 2018;\nHyde et al. 2019; Cao and Daumé III 2020; Dinan et al . 2020], more nuanced treatments of race\n[e.g., Penner and Saperstein 2008; Freeman et al . 2011; Saperstein and Penner 2012; Saperstein\net al. 2013; Penner and Saperstein 2015; Field et al. 2021], better addressing intersectional identities\n[e.g., Crenshaw 1989; Nash 2008; Gines 2011; Penner and Saperstein 2013; Ghavami and Peplau\n2013; Bright et al. 2016; Buolamwini and Gebru 2018; May et al. 2019; O’Connor et al. 2019; Guo\nand Caliskan 2021], and more modern treatments of disability [e.g., Batterbury 2012; Spiel et al .\n2019; Hutchinson et al. 2020].\nAdditional considerations. To more completely understand the harms of foundation models,\nfurther documentation is required of both the intrinsic biases and extrinsic harms; future work\nshould articulate the relationship between intrinsic biases and extrinsic harms [Blodgett et al .\n2020, 2021; Goldfarb-Tarrant et al . 2021]. This documentation requires centering stakeholders\nbeyond academics and industry practitioners: the inequitable impact of foundation models will\nbe experienced largely by minority populations, which are underrepresented in both academia\nand industry. For foundation models specifically, their creation and study likely will be conducted\nby those with the access and resources required, further emphasizing the importance of venues\nthat center marginalized voices [D’Ignazio and Klein 2020, §5.6: ethics]. In particular, user studies\nof specific adapted models, when aggregated across applications, can provide compelling and\nindividualized documentation of the harms that derive from the intrinsic biases of foundation\nmodels, all while centering individual users. In this way, we imagine the methodologies in human-\ncomputer interaction (HCI), with some adjustment to accommodate the abstraction involved in\nfoundation models, will help center the voices of marginalized communities (further discussion in\n§2.5: interaction).\n5.1.3 Sources.\nIn order to fully characterize and properly intervene on the harms of foundation models, we must\nbe able to trace their source to the properties of the foundation model and the adaptation process,\nand further decompose to the roles of individual sources of biases [Friedman and Nissenbaum\n1996]. Source tracing is vital for attributing ethical and legal responsibility for experienced harm,\nthough attribution will require novel technical research that foregrounds matters such as causality\n[Pearl 2000] and influence [Koh and Liang 2017].\nData. Data of several types shapes the behavior of applications, and the associated extrinsic harms,\nbased on foundation models: the training data used to train the foundation model, the adaptation\ndata used to adapt the foundation model, and test-time user data/interaction. For all of these data\nsources, the properties of the data (e.g., toxicity and hate speech [Henderson et al. 2017], abusive\nlanguage [Waseem et al. 2017], microaggressions [Breitfeller et al. 2019], stereotypes [Voigt et al.\n2018]) will manifest in the biases of the foundation model (and its adapted derivatives). 91 Since\nthe training data is the key data source that determines the foundation model and the associated\nintrinsic biases, we focus on the training data here. At present, the relationship between the training\ndata, along with associated data practices (e.g., data curation, data selection, and data weighting\n[Paullada et al. 2020; Bender et al . 2021; Rogers 2021]) and the intrinsic biases acquired by the\nfoundation model remains unclear; future work is critically needed to clarify this relationship. Since\nfoundation models generally require training data of immense scale, which poses clear challenges\nnot only to its documentation [Bender et al. 2021] but also comprehensive scientific exploration to\n91In adaptation, which involves labelled task-specific data, biases in the choices of the label space [Crawford 2021]\nand biases in the annotators who label that data [Geva et al. 2019; Sap et al. 2019] can also contribute to extrinsic harms\nexperienced by users.\nOn the Opportunities and Risks of Foundation Models 133\narticulate the relationship of data biases and model biases, we anticipate new protocols are required\nto address this scale. Establishing scaling laws for bias, akin to those for accuracy metrics [Kaplan\net al. 2020; Henighan et al. 2020], may enable systematic study at smaller scales to inform data\npractices at larger scales.\nModeling. Modeling decisions (e.g., training objective (§4.2:training), model architecture (§4.1:mod-\neling), adaptation method (§4.3: adaptation)) influence the biases in foundation models and their\nderivatives, thereby affecting the experienced extrinsic harms. Existing work demonstrates that\nfoundation models amplify training data biases, extending trends seen for machine learning and\ndeep learning models [Zhao et al. 2017; Wang et al. 2019d; Jia et al. 2020; Hashimoto et al. 2018],\nthough much still remains unclear about what and how model properties are responsible for this\nbias amplification. Further, given that applying foundation models directly may be infeasible (due\nto their scale), efforts to compress these models or make them more efficient also appear to amplify\nbias [Hooker et al . 2020; Renduchintala et al . 2021]. Amplification may also be exacerbated by\nfeedback loops, in which foundation models modify societal behavior and induce sociological\nchanges, which modifies subsequent training data; feedback effects of this form tend to exacerbate\ninequity in other ML applications [Lum and Isaac 2016; Ensign et al. 2018; Hashimoto et al. 2018].\nBeyond the explicit decisions made in training and applying foundation models, community values\n[Birhane et al. 2020] and norms (§5.6: ethics) both indirectly and implicitly [Liu et al . 2021b]\nshape decision-making in building models. As a result, measuring biases in conjunction with work\nintroducing foundation models [e.g., Brown et al . 2020] and in standard benchmarks [Friedman\nand Nissenbaum 1996, §4.4: evaluation], as well as conducting user studies with diverse user\ngroups to document experienced harm, are steps towards ensuring that best practices actively\nemphasize the consideration of bias and inequity.\nModelers. As with all algorithmic systems, poor representation and diversity of stakeholders and\nmarginalized communities in decision-making bodies that develop or apply foundation models is\ninherently problematic, and may contribute to greater experienced harm for these communities.92\nWhile difficult to document, existing efforts to develop foundation models suggest this as a possibil-\nity: Caswell et al. [2021] demonstrate the flawed data handling of less-represented languages in the\nmultilingual datasets used to train multilingual models and Hutchinson et al. [2020] show that mod-\nels often contain undesirable biases towards disabled persons. In both instances, these biases and\nharms may have been noticed earlier by better representation of these parties in developer teams.\nFurther, since end-users are likely more diverse than developers and may notice these concerns\nearlier, allowing for user feedback to contribute to foundation model design (§2.5: interaction) is\nan important direction forward.\n5.1.4 Interventions and recourse.\nAddressing, mitigating, and rectifying the inequities associated with technology requires integrat-\ning social and technical methodologies [Abebe et al . 2020]. For foundation models specifically,\nwe consider both proactive methods, which change how models are developed and deployed to\nprophylactically reduce harm, as well as reactive methods, which respond to harm and make\nchanges for the future. At its core, the abstraction of foundation models complicates both aspects:\nknowing if interventions at the level of the foundation level are successful in reducing harm requires\ndownstream observations at the level of specific deployed applications and recourse in the event\n92We note that diversity, both with respect to disciplinary backgrounds and demographic identities, is of fundamental\nimportance in these high-impact decision-making settings for reasons well beyond the potential improved recognition of\nfairness-related harms.\n134 Center for Research on Foundation Models (CRFM)\nof harm requires upstream propagation of both feedback and accountability to foundation model\nproviders.\nIntervention. General principles that govern intervention on technological systems apply to\nthe foundation model setting: identifying which sources are most responsible for bias or harm\nprovides the evidence required for targeted action. For example, the urgency of calls for improved\ndiversity in the teams that design, produce, and control technology (e.g., foundation models) and\ntheir applications [Longino 1990; Harding 2015; Nielsen et al. 2017; O’Connor et al. 2019; Hofstra\net al. 2020; Katell et al. 2020] is further intensified if the lack of diversity is shown to relate to harm\n[Caswell et al. 2021]. In addition, transparent documentation [e.g., Gebru et al . 2018; Bender and\nFriedman 2018; Mitchell et al. 2019] and auditing [e.g., Raji and Buolamwini 2019] are similarly\ncritical in providing the impetus for intervention and change [Burrell 2016; Lipton 2018; Creel 2020;\nRaji et al. 2020; Wilson et al. 2021]. The scale of foundation models, as well as the specifics of their\naccessibility, introduce new challenges for existing protocols for documentation and auditing that\nwe discuss further in §5.6: ethics.\nTo date, many of the interventions considered for reducing the inequitable impact of technology,\nincluding in the foundation model regime, are methods for technical mitigation that center the\ndata (to obviate reflecting inequities or biases) and modelling decisions (to avoid amplifying data\nbiases) involved. Of specific importance in the foundation model regime is recognizing that these\nmitigation approaches may target different steps in the pipeline such as the training data [e.g., Lu\net al. 2020], modelling objectives [e.g., Zhao et al . 2018]), and adaptation methods and test-time\nuse [e.g., Park et al . 2018; Zhao et al. 2019]. As a result, different approaches may not only be\nmore or less effective, but require action from different entities (e.g., foundation model providers vs.\napplication developers) and more or less intensively affect the expensive training process for these\nmodels (e.g., changing the process of creating a foundation model vs. altering itpost hoc ). Technical\nintervention of this form may also target different goals: some interventions, such as changing\nthe training data, aims to reduce intrinsic bias . On the other hand, most work on mitigation in\nalgorithmic/ML fairness instead considers reducing outcome disparities in terms of model behavior,\ni.e., the outputs of downstream systems that more directly relate to extrinsic harm . Technical\nmitigation of all forms at present is severely limited: methods that measure or combat intrinsic\nbias are brittle or ineffectual [Gonen and Goldberg 2019; Ethayarajh et al. 2019; Bommasani et al.\n2020; Zhou et al. 2021b; Antoniak and Mimno 2021], methods that measure or combat extrinsic\noutcome disparities may not align with stakeholder goals [Saha et al . 2020], and there is some\nevidence to suggest certain types of technical intervention may be simultaneously unsatisfiable\n[Corbett-Davies and Goel 2018; Kleinberg et al. 2017], impossible [Lechner et al. 2021], or may even\nexacerbate inequity [Xu et al. 2021]. In spite of this state of affairs, we continue to believe technical\nmethods will still play an instrumental role in addressing the harms that arise in the foundation\nmodel regime; in general, we advocate for transparency, especially given that technical mitigation\nmethods may not be able to achieve the intended goals. More broadly, claims of bias and bias\nmitigation must be made carefully to clearly communicate the status quo to various stakeholders\nwith differing expertise (e.g., application developers building on top of foundation models and\npolicymakers regulating the technology; [Nissim et al. 2020]).\nRecourse. Unfortunately, proactive intervention is unlikely to fully resolve all potential harm\nor inequity that may arise due to foundation models. When harm arises, there is currently no\nwidely-adopted (or legally required) framework for resolving the appropriate recourse for the\nharmed parties. While certain protocols may exist for specific applications, the abstraction of\nfoundation models again introduces a disconnect: harms likely are partially attributable to both\nthe foundation model providers and the downstream application developers, but allocating this\nOn the Opportunities and Risks of Foundation Models 135\nresponsibility to either party remains challenging. More simply, mechanisms are not in place to\neven communicate these harms to foundation model providers (even if feedback or complaints\nare raised to application developers). As a result, new norms and standards are needed on how\nfeedback from application developers and end-users should reach upstream to the foundation model\nproviders, how to determine the entities (e.g., foundation model providers, application developers)\nresponsible for these harms, and the relationship to legal responsibility (§5.4: legality). To make\nprogress on this matter, we encourage future work to consult the practices used in other domains\n(especially those with similar abstractions and multi-entity structures), and we anticipate any\nstandards introduced will likely need to be reasonably dynamic, so that they can be synchronized\nwith the rapidly changing status quo for these models and their applications.\n5.1.5 Takeaways.\nMachine learning has an established trackrecord of inequitable impact, with much of the burden of\nits harms borne by marginalized communities. Foundation models introduce new challenges to this\ncalculus but, ultimately, for their societal impact to be equitable, significant research and change is\nrequired to understand the harms they cause and to meaningfully address and rectify these harms:\n(1) The one-to-many nature of foundation models, i.e., the same few foundation models being\nused across many applications, means the intrinsic properties of foundation models pervade\nto many downstream applications. Pernicious biases in these models therefore have out-sized\neffect on the experienced harms.\n(2) Biases and harms in the foundation model regime originate from many sources (e.g., training\nand adaptation data, modelling and adaptation decisions, modeler diversity and community\nvalues). Attributing the sources for bias and harm is fundamental for questions of intervention\nand responsibility; attribution requires new technical research to be done reliably.\n(3) The inequities of foundation models are not inevitable, but addressing them requires a multi-\npronged approach comprised of both proactive intervention (e.g., data-centric and model-\ncentric changes) and reactive recourse (e.g., mechanisms for feedback and accountability).\n136 Center for Research on Foundation Models (CRFM)\n5.2 Misuse\nAuthors: Antoine Bosselut*, Shelby Grossman*, Ben Newman\nFig. 25. This figure shows the effect foundation models will have on manipulative and harmful content\ngeneration, and the implications for detection.\nIn this section, we consider misuse of foundation models — situations where people use foundation\nmodels as they are intended to be used (e.g., to generate language), but where their capabilities\nare intentionally leveraged to cause harm to populations or individuals. This definition positions\nmisuse concerns between those of inequity (where models can cause harm without bad intentions;\n§5.1: fairness) and security (where bad actors exploit unintentional abilities or vulnerabilities in\nmodels to cause harm; §4.7: security). Below, we outline how foundation models both enable new\nforms of misuse and support new tools for misuse detection and mitigation.\n5.2.1 Foundation models will be misused for harmful purposes.\nAdvances in the scale (§4.2: training), multimodality (§4.1: modeling), and adaptivity (§4.3: adap-\ntation) of generative foundation models will allow them to be misused to generate high-quality,\ncheap, and personalized content for harmful purposes. In this section, we discuss these three\ndimensions within the context of two examples of malicious activity: manipulative content creation\nand harassment.\nContent quality. Foundation models are capable of automatically generating much higher-quality,\nhuman-looking content than prior AI methods. They may empower disinformation actors, where\nstates, for example, create content to deceive foreign populations without being transparent that\nthe content is linked to a state. Currently, creating this content often requires hiring people\nwho speak the language of the population being targeted. Governments may outsource content\nOn the Opportunities and Risks of Foundation Models 137\nproduction to native speakers in the country they are targeting,93,94 but this decision causes real\nrisks for operational security. Foundation models will allow for the creation of content that is\noften indistinguishable from content created by humans [Kreps et al. 2020; Clark et al. 2021] — and\nindeed it will be able to do this for a wide variety of languages — enabling both goals of creating\ncontent that resonates and maintaining operational security.\nIn addition to deceiving foreign populations, foundation models’ ability to generate high quality\nsynthetic images (deepfakes) or text may be abused to harass individuals. Deepfakes have already\nbeen used for the purpose of harassment. For example, Rana Ayyub, an Indian investigative\njournalist, was targeted by a high-quality deepfake that superimposed her face onto a pornographic\nvideo, leading her to leave public life for months.95 Because foundation models are often multimodal\n(§4.1: modeling), they could similarly impersonate speech, motions, or writing, and potentially be\nmisused to embarrass, intimidate, and extort victims.96\nCost of content creation. Foundation models will substantially decrease the costs of content\ncreation, further lowering the barrier to entry for malicious actors to carry out harmful attacks\n[Brundage et al . 2018]. The budget for one 2017 influence operation that originated in Russia\nand targeted Americans was $12.2 million [DiResta et al . 2018]. More recently, individuals in\nRussia paid $75-$200 per article to American freelancers as part of a disinformation campaign.97\nFoundation models will lower these marginal costs. While foundation models, such as GPT-3, may\nmake mistakes when generating content [Buchanan et al. 2021], it will be more feasible to hire\na small number of editors to fix them than to hire content creators directly. Initial costs to train\nfoundation models are more significant (§4.5: systems), but these expenses should be manageable\nfor most state actors [Buchanan et al. 2021].\nIn addition to monetary cost, foundation models require fewer technical skills to achieve high-\nquality results. Current tools, such as video editing software, can enable credible photo or video\ndeepfakes, but require several hours of a skilled user’s time to yield quality content. Foundation\nmodels lower this barrier to use: their few-shot adaptation capabilities (§4.3: adaptation) enable\nnew modes of interaction for application users (§2.5: interaction) that will allow users to rapidly\niterate for content creation.\nPersonalization. Foundation models will reduce obstacles to creating personalized content. For\nexample, disinformation from Russian individuals that targeted the US in 2016 included highly\ncustomized content. Social media posts were crafted to push narratives about Syria (e.g., the U.S.\nshould get out of Syria) that resonated with Black Lives Matter activists [DiResta et al. 2018] (e.g., by\nsuggesting that the U.S. should focus on issues facing the Black community in America, and not on\nissues in Syria). The same narratives were repackaged to resonate with Texas secessionists [DiResta\net al. 2021]. Such a content creation endeavor is costly and time consuming. Foundation models\nwill allow for similar activity, but at scale due to the low cost of adaptation (§4.3: adaptation).\nIn addition to foundation models allowing an actor to personalize content for niche audiences,\nthey also allow an actor to personalize content to target a single individual — a capability that can\nbe abused by harassers. Foundation models that condition their generations on personal attributes\nor information can create realistic personalized content, which could be more embarrassing, place\nvictims in more danger,98 and lead to more successful extortion attempts.\n93https://www.lawfareblog.com/outsourcing-disinformation\n94https://fsi.stanford.edu/content/ira-takedown-20201215\n95https://www.huffingtonpost.co.uk/entry/deepfake-porn_uk_5bf2c126e4b0f32bd58ba316\n96 https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402\n97 https://www.nytimes.com/2020/09/02/technology/peacedata-writer-russian-misinformation.html\n98https://www.dw.com/en/social-media-uptick-in-honor-crime-in-middle-east/a-56370773\n138 Center for Research on Foundation Models (CRFM)\n5.2.2 Foundation models will be powerful detectors of harmful content.\nWhile the generative capabilities of foundation models will provide ample misuse opportunities,\nthese same abilities may make them strong detectors of harmful content. While these capabilities\nare equally relevant for detecting human- and model-generated content, we focus on the detection\nof model-generated content in this section. First, we outline the challenges that current manual\ndetection approaches will face in discovering harmful misuses of foundation model. Then, we\npropose how the interactive and multimodal representation capabilities of foundation models may\nmake them powerful tools for automatic detection of harmful content. Finally, we discuss the\nrisks associated with deploying automatic detection models in online settings to combat potential\nfoundation model misuse.\nRethinking human interventions. Currently, malicious practices are frequently uncovered\n(and on social media, sometimes removed) by humans searching the internet to uncover content\norigination.99 For example, fake social media profiles commonly steal profile photos from dating\nsites, which are discoverable through reverse image searches. Similarly, disinformation websites\nfrequently use plagiarized content to mask deceptive content [DiResta and Grossman 2019], which\nis easily identified by conducting internet phrase searches. Foundation models will limit the efficacy\nof these detection strategies. Already, relatively unsophisticated disinformation campaigns have\nleveraged AI-generated photos100 to remove the possibility of discovery through reverse image\nsearch. Tools for assessing whether these photos are AI-generated are available, but foundation\nmodels will complicate this work — for text and video as well — challenging manual human\ndiscovery techniques [Ippolito et al. 2020; Clark et al. 2021].\nFoundation models as detectors. The same abilities of foundation models that make them\nstrong generators of creative content may make them strong detectors of model-generated content.\nExisting works demonstrate that foundation models can be adapted to detect disinformation from\ntext generators [Zellers et al. 2019b] — which generate statistical textual artifacts [Holtzman et al.\n2020] — and that they can be used to evaluate the toxicity levels of their own generations using\nprompt questions [Schick et al. 2021]. Below, we describe how future foundation models will enable\nmore powerful detection systems of machine-generated, harmful content.\nImprovements in the interactive and multimodal interfaces of foundation models will provide\nnew opportunities to improve detection of foundation model misuse for harmful content generation.\nCurrent statistical detectors must be retrained and re-deployed to integrate new knowledge about\nthe textual content of misuse strategies [Dinan et al . 2019]. The rapid learning capabilities of\nfoundation models (§4.3: adaptation) may allow them to adapt from human feedback to new\nmisuse strategies that the foundation model was not initially trained to recognize [Lee et al. 2021a].\nSimultaneously, the multimodal abilities of foundation models will enable more expressive repre-\nsentation of misuse ecosystems. Prior work has explored how misinformation spreads more rapidly\nacross social networks than authentic content [Starbird et al. 2018; Vosoughi et al. 2018], yielding\nrecognizable signatures when analyzed retrospectively. The multimodal capabilities of foundation\nmodels could allow them to jointly learn representations of harmful content and its typical dis-\nsemination signature on social networks. These joint representations could provide powerful tools\nfor predicting whether certain types of automatically-generated content are indicative of misuse\nbehavior.\n99 https://www.theatlantic.com/ideas/archive/2020/09/future-propaganda-will-be-computer-generated/616400/\n100For a Middle East campaign example, see https://www.thedailybeast.com/right-wing-media-outlets-duped-by-a-\nmiddle-east-propaganda-campaign.\nFor an example from Cuba, see https://raw.githubusercontent.com/stanfordio/publications/main/twitter-CU-202009.pdf\nOn the Opportunities and Risks of Foundation Models 139\nRisks of foundation models as automatic detectors. Improvements in automatic detection\nsystems for both model-generated and human-generated harmful content will make these systems\nmore prevalent online, yielding potential negative consequences. Any detection system will have\nfalse positive cases where human-generated fair content will be flagged as harmful [Sap et al. 2019;\nXu et al. 2021]. The rate at which algorithmic false positives affect users (or groups of users) may\ncause downstream harm (§5.1: fairness). The adaptive capabilities of foundation models should\nmake systemic false positives easier to address as the model can be locally edited to re-classify those\nexamples (§4.3: adaptation). However, corner cases will likely not be prioritized and recourse will\nbe challenging in these situations.\nMore broadly, wide-scale deployment of misuse detection systems may engender an “arms race”\nbetween harmful content generators and detectors. Most content generators that use foundation\nmodels will lack the resources to develop them individually, and will use systems deployed by larger\nentities. While terms of use policies should outline acceptable uses of these systems (§5.6: ethics),\ndeployers of foundation models will also need internal detection systems to identify misuse of\ntheir products101 and mitigate them (§5.4: legality). However, there will be fewer controls for\nmisuse actors with the resources to develop their own foundation model-based content generators,\nputting pressure on platforms to curate the content shared through their distribution channels.\nOptimistically, content platforms encompass some of the most well-capitalized firms in the world.\nTheir resources may enable the development of detectors beyond the capabilities of most individual\nmisuse agents. This resource advantage could disincentivize individual foundation model develop-\nment due to the high costs of repeatedly training these systems at scale. However, many instances\nof foundation model misuse could still be successful even without the largest foundation models\nto power them, particularly as attackers may leverage the interactive capabilities of foundation\nmodels to rapidly generate content that can evade detection.\n101https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/\n140 Center for Research on Foundation Models (CRFM)\n5.3 Environment\nAuthors: Peter Henderson, Lauren Gillespie, Dan Jurafsky\nFig. 26. A visualization of a cost-benefit analysis for deploying a foundation model. The total value of a\nmodel can be approximated by first considering the net positive social benefits of the model, as well as any\nenvironmental benefits. Then, we subtract the negative energy costs to train and deploy the model, the social\ncost of the carbon emitted to train the model, and the secondary environmental effects. If the net costs\noutweigh the benefits, then foundation model developers and large-scale deployers should consider harm\nreduction strategies. This could include deploying a more efficient model or not deploying the model at all.\nFoundation models can potentially lead to many social and environmental benefits, for example in\nlegal domains (§3.2: law), healthcare (§3.1: healthcare), or even tackling climate change [Rolnick\net al. 2019]. But because of their scale, they themselves can negatively impact the environment\nthrough increased carbon emissions if model creators are not careful [Strubell et al. 2019; Lottick\net al. 2019; Schwartz et al. 2019; Lacoste et al. 2019; Cao et al. 2020; Henderson et al. 2020; Bender\net al. 2021; Patterson et al. 2021; Lannelongue et al. 2021; Parcollet and Ravanelli 2021]. Addressing\nsuch emissions is an imperative: current forecasts show that climate change is occurring more\nrapidly than previously thought [Masson-Delmotte et al. 2021].\nTo understand where such emissions can occur in foundation models, we consider their lifecycle.\nFirst, they are trained on vast amounts of data, possibly for up to months of time and often distributed\nacross hundreds to thousands of GPUs. Afterwards, they may be adapted to new domains or perhaps\ndistilled into smaller models. All of this can be considered part of the training regime. Models used\npurely for research may not move beyond these steps. After models have been adapted and/or\ndistilled, they might move on to be deployed into production. At this point many rounds of inference\nwill run through the model until a new model is trained and the cycle repeats.\nEach one of these steps has the potential to utilize large amounts of energy and can contribute\nto carbon emissions. Foundation models can generate large, one-time energy costs and carbon\nemissions during the initial training phase. For example, the amount of emissions from training one\nBERT-base model, under some conditions, would only be offset by 40 trees grown for 10 years.102\nAnd if deployed at scale, foundation models can require substantial energy to service millions of\nrequests103 — translating to large carbon emissions if nonrenewable resources are used.\n102Strubell et al. [2019] calculate carbon emissions for training BERT on an average energy grid in the U.S. and we use\nhttps://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator to convert that to equivalent emissions in other\ndomains. We note that this number can vary depending on the energy grid and other considerations [Henderson et al. 2020;\nPatterson et al. 2021].\n103For example, transformers are already used at scale for search both at Microsoft and Google. See https://www.blog.google/\nproducts/search/search-language-understanding-bert/ and https://azure.microsoft.com/en-us/blog/microsoft-makes-it-\neasier-to-build-popular-language-representation-model-bert-at-large-scale/.\nOn the Opportunities and Risks of Foundation Models 141\nTherefore, the environmental impacts of certain design decisions for both training and deploying\nfoundation models can be substantial. Even seemingly minuscule decisions, like reducing the\nnumber of layers a model has, may lead to significant environmental cost reductions at scale.\nFor example, based on calculations from Henderson et al. [2020], a slightly more energy efficient\ntranslation model deployed at the scale of a commercial translation service could save between\n78 kgCO2eq and 12,768 kgCO2eq of carbon emissions per day depending on the energy grid used.\nThis is roughly equivalent to the carbon sequestered by 1 to 211 trees grown for 10 years, or the\ncarbon sequestered by .35 to 57.4 acres of forest in one year.104 Thus the design, deployment, and\npost-deployment monitoring of foundation models should adequately reflect these risks.\nThere are of course uncertainties in calculating the amount of energy used or carbon emitted by\nany given model [Henderson et al. 2020; Cao et al. 2020; Patterson et al. 2021], and other sources of\nemissions may currently be much greater than those generated by foundation models [Mora et al.\n2018]. But if foundation models continue to scale and gain in popularity, they may very well become\na significant contributor to carbon emissions. Our goal is to provide a framework for foundation\nmodel developers and large-scale deployers105 to consider how they can mitigate any unnecessary\ncarbon emissions and keep the net social impact of these models positive. We recommend that:\n(1) Carbon impacts can and should be mitigated in many cases. This can be accomplished by\ntraining models in low-carbon intensity regions, or by using more efficient models and\nhardware (§5.3.1: environment-mitigation).\n(2) When all mechanisms for mitigation have been exhausted and mitigation is no longer\npossible, the costs and benefits to society should be assessed to determine if and when a\nlarger foundation model should be deployed over a smaller, more efficient, model — with the\nunderstanding that the up-front costs of a large foundation model may be amortized over\nthe lifetime of the model (§5.3.2: environment-costs).\n(3) Energy, computational, and carbon costs — as well as any efforts taken to mitigate negative im-\npacts — should be clearly reported to inform policymaking and research (§5.3.3:environment-\nreporting).\n5.3.1 Carbon impacts can and should be mitigated in many cases.\nThe carbon impacts of training foundation models differ from the impacts of deploying them for\ninference. Model training has no latency requirements, so training can be moved across energy\ngrids with relative ease in cloud environments. Every energy grid has its own carbon intensity —\nthe amount of carbon emitted per kilowatt-hour of energy used. For example, Québec has an\nextremely low carbon intensity due to its reliance on hydroelectricity, while Estonia’s energy grid\nhas an extremely high carbon intensity due to its reliance on shale oil (though that is changing\nquickly) [Henderson et al. 2020]. Recent research has even suggested that the top 5% of polluting\npower plants contributed 73% of all electricity-based emissions [Grant et al . 2021]. Thus, while\ntraining foundation models can be quite energy intensive, researchers have demonstrated that\nthe carbon impacts of these models can be partly mitigated by selecting energy grids with minimal\ncarbon emissions [Henderson et al. 2020; Lacoste et al. 2019; Patterson et al. 2021].\n104Sequestration estimated via https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator, but may be larger\ndepending on other estimation methods. More efficient energy grids will emit less carbon, resulting in wide estimated\nranges of impacts.\n105We focus on model developers and large-scale deployers, like those who build production systems on top of foundation\nmodels, because they are most able to make meaningful changes to reduce energy use and carbon emissions. A single\nchange by these actors — like using a more efficient model — can scale to massive carbon savings, which would otherwise\nrequire a massive campaign to reach all downstream model users.\n142 Center for Research on Foundation Models (CRFM)\nCarbon offsets have also been proposed as a stopgap until carbon-free renewable electricity is\navailable at all data centers. This strategy involves reducing carbon emissions in one activity to\noffset the emissions from another. But most — if not all — carbon offsets are a strictly worse solution\nthan not emitting CO2 in the first place [Holl and Brancalion 2020]. Some carbon offset programs\ncan even have a negative impact. For example, studies of forest planting campaigns (often a source\nof carbon offsetting) show that they can do more harm than good. They can yield monocultures (the\nuse of one particular species of tree) that diminish the biodiversity of the region and reduce carbon\nstorage in the forest soil [Heilmayr et al. 2020; Hong et al. 2020b]. This could result in more carbon\nemissions when using carbon offsets than if the original carbon had never been emitted in the first\nplace. Therefore, when training or deploying a foundation model, we recommend designing for as\nlittle carbon emission as possible up-front, rather than simply relying on carbon offsets to cancel\nemissions.\nWhen it is not possible to run in low-carbon regions, other mitigation strategies should be\nleveraged, reducing unnecessary energy usage. This includes:\n•using more efficient hardware,106\n•using mixed-precision training [Micikevicius et al . 2017] or quantization [Gholami et al .\n2021],\n•using more efficient architectures (e.g., using an evolved transformer over a vanilla trans-\nformer architecture; or using sparse models) [So et al. 2019; Patterson et al. 2021; Mostafa\nand Wang 2019],\n•distilling models and using distilled models (e.g., [Sanh et al. 2019]),\n•and utilizing other optimization strategies that will reduce energy costs (see more discussion\nin §4.5: systems).\nMaintainers of open source projects and cloud compute should strive to set their default settings\nto the most efficient possible, since “green defaults” are known to be the most effective mitigation\nstrategies (see discussion in [Henderson et al . 2020]). Other mitigation strategies can be found\nin recent literature [Strubell et al. 2019; Lacoste et al. 2019; Schwartz et al. 2019; Henderson et al.\n2020]. We also note that reducing and mitigating energy usage also has the added benefit of making\nmodels more accessible to those with limited compute access (see §5.6: ethics for more discussion).\nHowever, when a model is mainly used for inference, e.g., deployed in a production application,\nit often cannot be moved to a less carbon-intensive energy grid for low-latency applications. In\naddition to using the mitigation strategies specified above, in this case it is important to weigh the\nbenefits of the proposed foundation model versus a more energy efficient alternative. We discuss\nthis further in the subsequent section.\n5.3.2 Costs and benefits should be assessed before using foundation models.\nAfter taking as many steps as possible towards mitigation (or where mitigation is not possible), it\nis vital to assess the required size of a foundation model — or whether a foundation model should\nbe used at all. This cost-benefit analysis should consider:\n(1) Is the social cost and environmental cost from deploying the foundation model greater than\nthe social benefit of the model?\n(2) Would another, computationally simpler and cheaper approach achieve comparable social\nbenefit (e.g., a much more efficient foundation model, or perhaps simple baseline)?\nA simplified scheme for assessing this trade-off considers the overall impact of a model 𝑀 as:\n106Notably, California now regulates computers with inefficient GPUs for this reason, requiring that they stay below 30-100\nkWhs/year, depending on the manufacturing date and computer type. See Sections 1601-1608 of California’s Appliance\nEfficiency Regulations (Title 20).\nOn the Opportunities and Risks of Foundation Models 143\n𝑉(𝑀)= 𝑆(𝑀)−𝐶(𝑀)−𝐸(𝑀)−𝑂(𝑀). (7)\nFigure 26 represents this equation and the costs and benefits that may enter each variable. Here,\n𝑀 is the model and 𝑆 is the net social benefit, as well as environmental benefit, in dollars. 𝑆 can be\nincreased by improving healthcare, access to justice, decreasing poverty, improving environmental\nmonitoring, aiding ecosystem conservation efforts, and so on.\n𝐶is the social cost of carbon from energy use. This represents the future harm to society from the\ncarbon released as a present-day monetary value. The upper bound U.S. Environmental Protection\nAgency (EPA) estimate from 2017 for the social cost of carbon was $105 (in 2007 U.S. dollars) per\nmetric ton of CO2 emitted.107\n𝐸is the energy cost of the model. For example, in April 2021, the average U.S. residential energy\ncost was about $0.1376 per kWh.108 Added to this variable could be the costs from increased strain\non the energy grid. For example, a recent study suggested that the cost per energy grid interruption\nevent, normalized by average demand, could be as high as $15.9 per average kW [Sullivan et al.\n2015].109\n𝑂 is the social cost of other second order environmental effects. This could include:\n•The compounding carbon impacts from increased chip demand and chip production [Gupta\net al. 2021a].\n•Other environmental impacts of chip manufacturing, like the creation of toxic waste sites in\nSilicon Valley, whose health effects are unequally distributed to socially vulnerable popula-\ntions [Stewart et al. 2014], or pollution from manufacturing in Taiwan that has been linked\nto chronic health problems [Tu and Lee 2009; Lin et al. 2016].\n•The compounding effects of climate change that are not already included in the SCC model.\nFor example, these effects could include accelerated desertification [Huang et al. 2016], rapid\necosystem changes that put many species at risk of extinction [Urban 2015], and increased\ncarbon emissions due to melting permafrost [Schuur et al. 2015].\n•Unnecessary strain on chip production capacities. Recent chip shortages have led to work\nstoppages in automobile manufacturing.110 There is no evidence to suggest that increasing\ndemand for ML-optimized chips led to this shortage. 111 But such considerations fall into\nsecond order effects, where researchers might weigh whether the risks, however slight, of\ncontributing to such negative impacts are worth using or deploying a large model.112\nIt is important to consider in this analysis that the economic benefits and social costs of carbon\ncould be distributed unequally across communities, with poorer communities being impacted more\nheavily by climate change and wealthier communities being benefited by a model [Bender et al.\n2021].113 As such, when conducting the Equation 7 analysis, one should consider the benefits and\n107See https://19january2017snapshot.epa.gov/climatechange/social-cost-carbon_.html. But note that the social cost of\ncarbon can be a contentious metric [Stern and Stiglitz 2021]. By using a favorable discount factor, one can reduce carbon\ncosts. As such, it can the calculation of this metric can vary across methodologies.\n108https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=epmt_5_6_a\n109Like the social cost of carbon, calculation of these costs can fluctuate across modeling methodologies.\n110https://www.reuters.com/business/autos-transportation/ford-shut-some-n-american-plants-few-weeks-chip-\nshortage-2021-06-30/\n111Though recent reports have suggested that demand for datacenter chips have surpassed the gaming sector. See\nhttps://www.nextplatform.com/2020/08/21/the-local-maxima-ascension-of-datacenter-at-nvidia/.\n112Like for other metrics described previously, there is uncertainty as to how these impacts might be calculated and\nattributed to models.\n113See also, https://www.un.org/sustainabledevelopment/blog/2016/10/report-inequalities-exacerbate-climate-impacts-\non-poor/ and https://blogs.imf.org/2020/12/02/how-artificial-intelligence-could-widen-the-gap-between-rich-and-poor-\nnations/.\n144 Center for Research on Foundation Models (CRFM)\nFig. 27. A hypothetical example of amortized fine-tuning showing the point at which a foundation model\n(in this case BERT Base) will have lower energy costs than a transformer model trained from scratch. We\nestimate the up-front energy cost for training BERT from Strubell et al. [2019], and cost for fine-tuning a\ndownstream task from Chaudhary et al. [2020]. We compare against the linearly increasing cost of training a\ntransformer from scratch, from Strubell et al. [2019]. If BERT is used for less than ∼80 tasks, the up-front\nenergy costs are not recovered. After that point, BERT is more energy efficient than the model trained from\nscratch.\nharms to society more broadly rather than for a given organization or country. In this case𝑉(𝑀)\ncan be viewed as a distribution and should ideally be evenly distributed across the population.\nIn cases where the distribution is highly uneven — for example where all the benefits fall to the\nmodel designer while all the harms fall to populations that will never benefit from the model — the\ndesigner should spend substantially more effort on mitigation before deploying the model.\nThere is, of course, some uncertainty in which methodology to use when valuing each component\nof Equation 7. Empirical estimates for many of these terms can range in multiple magnitudes\ndepending on the data source and modeling choice for the phenomena, such as the different\nmechanisms for evaluating the social cost of carbon. And of course additional externalities, that\nmay be difficult to quantify monetarily, will continue to need to be considered. The key takeaway\nof this cost-benefit analysis, however, is not the dollar valuation of each term in the equation, but\nrather the existence of and relative importance of each of these effects. Our goal is to provide a\nhigh-level framework for beginning to consider these trade-offs. Future research may give more\nguidance on how to quantify each of these values.\nFinally, we note that these factors should also be evaluated over the lifetime of the model, not on\na per-run basis. Consider an alternative baseline model that must be trained from scratch for every\nnew task. The baseline may well require an expensive hyperparameter search to achieve equivalent\nperformance on downstream tasks. In contrast, the foundation model places the brunt of the costs\non the initial pretraining procedure, with fine-tuning perhaps being much simpler and more energy\nefficient. Over the lifetime of the foundation model, it could be more carbon efficient than the\nbaseline (Figure 27). Even more efficient adaptation mechanisms could improve this amortization\nfurther (see §4.3: adaptation).\nOn the Opportunities and Risks of Foundation Models 145\nThe efficiency of adaptation, however, is not guaranteed. It may be true that some foundation\nmodels will never be more efficient than a particular baseline, even when amortized over many\ntasks. For example, it cannot be assumed that a smaller model with fewer parameters will trans-\nlate to energy efficiency improvements. Due to increased hyperparameter tuning costs or other\noptimizations, the number of parameters has been shown not to correlate with energy efficiency\nin some cases [Zhou et al. 2020; Henderson et al. 2020]. Therefore, foundation model developers\nshould rigorously assess the efficiency of their models and adaptation mechanisms before beginning\nlarge-scale training efforts.\nThe framework in this section is meant to guide the reader in thinking about the environmental\nand societal trade-offs in training and deploying their model, but there are other substantial\nsocial justice considerations involved in deploying a foundation model, discussed in §5.6: ethics.\n§5.5: economics also discusses in more detail the dynamics of social welfare from algorithm\ndeployment.\n5.3.3 Carbon/energy impacts should be systematically reported.\nA cost-benefit analysis cannot be conducted unless researchers and engineers working on foundation\nmodels report the computational, energy, and carbon costs of their models. We encourage foundation\nmodel developers, providers, and curators to report these metrics, as well as what carbon reduction\nstrategies were used in the making of the foundation model. See [Henderson et al. 2020; Lottick\net al. 2019; Lacoste et al. 2019; Schmidt et al. 2021; Anthony et al. 2020] for examples of a Carbon\nImpact Statement and for tools that can facilitate this reporting. For researchers, such reporting can\noccur at publication time, but we also encourage industry actors to adopt transparency mechanisms\nto report these metrics for their deployed models.114 This will help set policy recommendations\nwithin industry and academia, as well as help downstream users identify carbon-friendly usage\npatterns. Standardized reporting will also aid in determining which models are accessible to those\nwith limited compute access (see §5.6: ethics for more discussion on accessibility).\nTo encourage more reporting of energy and carbon impacts, we suggest, among other strategies:\ngiving green badges at conferences, requiring reporting of relevant metrics for submission to con-\nference venues, lobbying large-scale deployers of foundation models to provide more transparency,\nand generally shifting professional norms in academia and industry towards standard reporting of\nthese metrics (see more discussion on professional norms in §5.6: ethics and more discussion on\nreporting mechanisms by Henderson et al. [2020]).\n114A small step toward this has been taken by some cloud compute providers that identify the most carbon friendly\ncloud regions. See, for example, https://cloud.google.com/blog/topics/sustainability/pick-the-google-cloud-region-with-the-\nlowest-co2.\n146 Center for Research on Foundation Models (CRFM)\n5.4 Legality\nAuthors: Neel Guha, Peter Henderson, Lucia Zheng, Mark Krass, Daniel E. Ho\nIn this section, we describe how US law may influence, constrain, or foster the creation and use\nof foundation models.115 We note that the legal landscape surrounding algorithmic tools remains\nuncertain. We highlight issues pertaining to (1) model training, (2) liability for model predictions,\nand (3) protections for model outputs.\nThough understanding how the law affects foundation models is crucial, it is important to\nrecognize that the law cannot be the only lens through which we evaluate the construction,\nmaintenance, and use of foundation models. Ethical frameworks are necessary to understand where\nlegally permissible applications of foundation models may still be ill-advised for the harms they\ninflict and are discussed in more depth in §5.6:ethics and §5.1: fairness. Studying the potential for\nmisuse and possible security concerns (see §5.2:misuse and §4.7: security) is critical for preventing\nharmful outcomes ex ante, as opposed to the ex post treatment that legal mechanisms often provide.\n5.4.1 Training.\nTraining foundation models will require accumulating vast amounts of multi-modal data, raising\nquestions around data collection and data use.\nFirst, the ability for model creators to grow datasets via web scraping will be governed by the\nmanner in which courts will interpret terms of service provisions and, notably, the U.S. Computer\nFraud and Abuse Act (CFAA), which criminalizes accessing a server “without authorization” [Wajert\nand Rottman 2019]. Courts are in conflict on these questions, and recent cases have sought to\nclarify the circumstances under which web scraping may be barred.116 The restrictiveness of data\naccess would fundamentally affect the diversity of data practitioners can use to train foundation\nmodels [Levendowski 2018].\nSecond, much of the data contained in training sets will be copyrighted and potentially protected\nby intellectual property law. However, copyright law recognizes exceptions when individuals may\nbe permitted to use copyrighted material. 117 Some scholars believe that the legal permissibility\nof training datasets will largely rest on whether courts interpret the process of model training as\n“transformative” under fair use doctrine [Lemley and Casey 2020]. Though the question of what\nqualifies as transformative is highly context dependent, the general rule is that transformative\nuses are those “that add something new, with a further purpose or different character, and do not\nsubstitute for the original use of the work\" [Office 2021]. Already, the recently released Github\nCopilot tool is bringing these arguments to the fore [Gershgorn 2021].\nFinally, some training datasets may run afoul of privacy laws. Illinois, for instance, enables indi-\nviduals to sue for improper collection or use of biometric data (e.g., retina or iris scans, fingerprints,\nvoiceprints, or scans of hand or face geometry).118 Foreign privacy laws like the E.U. ’s General Data\nProtection Regulation (GDPR) — which will affect American model creators if datasets contain\ninformation from E.U. citizens — would require data subjects to be informed about the purpose\nof data collection. Further issues could arise for laws like the California Consumer Protection\nPrivacy Act (CCPA), which provide individuals with a “right to be forgotten, ” raising questions as\n115Our perspective here centers on US law and legal frameworks. Discussions of the implications of foundation models\nwith respect to other countries may consequently take different perspectives.\n116Van Buren v. United States , 141 S.Ct. 1648 (2021).\n117See, e.g., , 17 U.S.C §107 to 112.\n118IBM is the defendant in a current class action alleging that IBM’s collection and use of this data (including for machine\nvision purposes) violates this statute. See Class Action Complaint at 2, Vance v. Int’l Bus. Machines Corp., No. 20 C 577 (N.D.\nIll. filed Jan. 24, 2020).\nOn the Opportunities and Risks of Foundation Models 147\nto whether model creators will need to “remove” training data from models [Villaronga et al. 2018;\nGinart et al. 2019].\n5.4.2 Output liability.\nThough foundation models themselves are task agnostic, fine-tuned models — or the representations\nlearned by foundation models themselves — may be used for traditional prediction tasks. Where\nthese tasks form components of larger decision-making systems, foundation models will thus\ninfluence actions, decisions, or policies. When these result in harm, model creators — and the\nindividuals operating them — may be legally responsible.\nEmbedding foundation models in physical systems (e.g., self-driving cars, electric grid man-\nagement, medical diagnostics, etc.) may result in physical harm to individuals. Here, courts will\nlikely resolve questions of liability under tort doctrine [Lemley and Casey 2019; Selbst 2020]. Key\nopen questions include the interplay between the liability of users, foundation model providers,\nand application developers, as well as the standards courts will use to assess the risk profile of\nfoundation models. Deployments in particularly sensitive domains (e.g., medicine) will require\nregulatory approval, and the development of standardized processes to assess safety [Wu et al .\n2021g].\nFine-tuned foundation models that classify individuals in ways that correlate with protected\nattributes (e.g., race, gender) may face challenges under civil rights laws. Scholars have noted that\nclaims for disparate treatment resulting from foundation models may be brought in the context of\nhiring, housing, or credit lending [Gillis and Spiess 2019; Scherer et al. 2019]. Exactly how courts\nwill adjudicate these issues is far from clear. Scholars have noted for instance, that the courts’\ntraditional views on “discrimination” would actually prevent machine learning practitioners from\nimplementing many algorithmic fairness techniques [Xiang 2021; Ho and Xiang 2020].119\nU.S. law recognizes special privileges and limits on governmental entities. Thus, the use of\nfoundation models by governmental entities — at a local, state or federal level — will implicate\nspecial considerations, in addition to equal protection claims. The use of models for risk assessment —\nor in other settings which result in a deprivation of life, liberty, or property — will invite procedural\ndue process claims.120 When models are used by administrative agencies (e.g., the Environmental\nProtection Agency) for instance, plaintiffs may allege that such use violates basic standards of due\nprocess, reasonableness / non-arbitrariness, and transparency.\n5.4.3 Legal protections for outputs.\nModel outputs — and by extension the model creators responsible for the models — may also be\nafforded certain legal protections. First, content produced by generative models may implicate\nfree speech issues. The extent to which courts will find First Amendment protections for machine\ngenerated content is unclear. Scholars have discussed a number of open questions, including\nwhether “AI speech” is protected [Massaro et al. 2016] or if model outputs are in effect the human\nprogrammer’s speech [Kajbaf 2019]. Others have noted the possibility of disclosure requirements\n(akin to safety disclosures for pharmaceutical drugs or other substances), also implicating speech\ndoctrine, under which models would be forced to share with listeners that their content is machine\ngenerated [Lamo and Calo 2019]. These issues could have wide ranging consequences, affecting\nwhether individuals can use foundation models to mass produce speech, or whether model creators\ncould be held liable for content generated by foundation models.\n119For more information on how models may embed certain biases, see §5.1: fairness.\n120Procedural due process recognizes that plaintiffs usually have certain basic rights during any deliberation that will\ndeprive them of life, liberty, or property (e.g., the right to cross-examine adverse witnesses).\n148 Center for Research on Foundation Models (CRFM)\nSecond, there is uncertainty regarding who may assert ownership over model outputs. Existing\ncopyright law does not recognize computer programs as authors, and hence, does not afford\ncopyright protection to “work” created by computer programs [Grimmelmann 2015]. As a result,\nscholars have advocated for a variety of approaches. Some have argued that, depending on the\ncircumstances, both the human creator of a program and its human user may have viable claims to\nbeing the “author” of the program’s output [Ginsburg and Budiardjo 2019].\nAs models are increasingly used in the process of “creation” — from artistic endeavors to more\nmundane settings like news filings — disputes over the ownership of machine generated content\nwill become more commonplace.\nWhile our analysis above only skims the surface of the legal issues implicated by foundation\nmodels, the resolution of these questions will be critical to the construction, use, and deployment\nof foundation models, or, to borrow Larry Lessig’s phrase, how “code is law” [Lessig 2000].\nOn the Opportunities and Risks of Foundation Models 149\n5.5 Economics\nAuthors: Zanele Munyikwa, Mina Lee, Erik Brynjolfsson\nFoundation models have the potential to substantially improve overall living standards by\nincreasing productivity and innovation. These models can be deployed to substitute for human\nlabor, augment humans, or help in the discovery of new tasks and opportunities, which can lead to\nincreased concentration of ownership and power, or more decentralization. On a broader level, the\nresult can be either increased inequality due to potential centralization (§5.1:fairness, §5.6: ethics),\nor more broadly shared prosperity due to the easier adaptation of foundation models for a wide\nrange of applications (§1: introduction). The ultimate outcomes on all these dimensions are\nnot dictated solely by technology or economics, but by the choices and actions of technologists,\npolicymakers, managers, workers, and other members of society.\nFoundation models can be thought of as what economists refer to as ageneral-purpose technology\n[Bresnahan and Trajtenberg 1995]. General-purpose technologies refer to technologies like the\nsteam engine and electricity, which drive waves of transformation and productivity growth due\nto their pervasiveness, improvement over time, and ability to spawn complementary innovations\n(a host of products and services that revolve around one core product). While foundation models\nmay not be pervasive at the moment, they seem poised to be the basis of widespread technological\ninnovations, and have the key hallmarks of a general-purpose technology. As a result, these models\nare likely to be economically important. In considering the impact of foundation models on the\neconomy, we will focus on three broad areas of impact: productivity, wage inequality, and ownership.\n5.5.1 Productivity and Innovation.\nFoundation models are likely to substantially increase both productivity and innovation. Productiv-\nity growth is one of the main contributing factors to boosting living standards, as it increases the\nwealth of nations and addresses a host of challenges from poverty and healthcare to the environment\nand education.\nProductivity is defined as output per unit input.121 One way to boost productivity is to reduce\nthe denominator; for instance, enabling a company’s advertisements to be written with fewer\ncopywriters or fewer labor hours per copywriter lowers the number of units of input. Productivity\ncan also be boosted by increasing the numerator, for instance by enabling a software developer\nto write more code in a given time. If the growth in the numerator is great enough, this can lead\nto more people developing software, not fewer [Autor 2015]. In many tasks, we have already\nobserved machine learning systems increasing productivity. For instance, an autocomplete system\nfor clinical documentation reduces keystroke burden of clinical concepts by 67% [Gopinath et al.\n2020]. Likewise, the potential for foundation models to affect productivity spans almost every\nindustry and many occupations. Considering language alone, an analysis of U.S. occupations\nusing the US Department of Labor’s O*NET database shows that many occupations involve the\ntypes of language-related work that could be affected by foundation models. Approximately 13%\nof occupations have a primary task that is related to writing, and the total wage bill of these\noccupations (annual salary multiplied by the number of individuals employed in the occupation)\nis over 675 billion dollars. However, the potential impact of foundation models extends beyond\nlanguage. They will also have effects on diagnostic imaging in medicine, graphic design, music122,\n121Note that when properly measured, productivity is not just a matter of counting units produced or hours work, but\nalso accounts for quality changes. Therefore, an increase in quality for a given amount of labor, such as more interesting\nfiction, also counts as an increase in productivity.\n122https://www.landr.com/\n150 Center for Research on Foundation Models (CRFM)\nand many other tasks where people are creating something that is similar to something else that\nalready exists [Winkler et al. 2019; Ramesh et al. 2021].\nPerhaps the most profound, if still speculative, effect of foundation models is their potential\nto enhance creativity and boost the rate of innovation itself. For instance, DALL·E [Ramesh et al.\n2021] could transform the market for illustrations much as inexpensive cameras revolutionized\nphotography. If these models enable humans to develop new ways to write new songs and novels\n(§2.5: interaction), discover variants of drug molecules (§3.1: healthcare), extend patents\n(§3.2: law), build innovative software applications, or develop new business processes, then not\nonly the level of productivity, but the rate of growth of productivity would be increased. In this\nway, foundation models have some of the characteristics of the ideas or blueprints in Paul Romer’s\ngrowth models [Romer 1990], or even meta-ideas (ideas about ideas) which, unlike most other\ngoods, are non-rival, thus speeding growth.\nIt is worth noting that changes in productivity are not always visible in the official statistics,\nbecause many aspects of input and output are difficult to measure [Brynjolfsson and Collis 2019].\nAs a result, the benefits and costs of foundation models will not be fully captured by traditional\nproductivity metrics, nor by related metrics like gross domestic product (GDP) or price levels (the\naverage of current prices across the entire spectrum of goods and services). This is especially true\nfor general purpose technologies historically, since they are catalysts for a cascade of secondary\ninnovations that often transform the set of goods and services in the economy, and even the nature\nof production and innovation over a period of years or even decades.\n5.5.2 Wage inequality.\nEven if foundation models increase average productivity or income, there is no economic law\nthat guarantees everyone will benefit. In part, this is because not all tasks will be affected to\nthe same extent. More importantly, the effects of foundation models on the demand for labor\n(and thus employment and wages) can be either positive or negative, regardless of productivity\ngrowth [Brynjolfsson and McAfee 2011; Brynjolfsson and Mitchell 2017]. When a technology\nsubstitutes for human labor in completing tasks, it tends to reduce demand for the workers doing\nthose tasks. This depresses employment and wages. However, when a technology complements labor,\nor facilitates the creation of new opportunities or tasks, it tends to increase labor demand [Acemoglu\nand Restrepo 2019]. Employment can (and often does) go up, even as productivity increases. For\ninstance, the invention of the airplane created the demand for an entirely new occupation, the\nairline pilot. In turn, the development of jet engines was complementary to human pilots, further\nincreasing demand for them. Similarly, the effects of foundation models on employment, wages,\nand income inequality will differ depending on how they are used.\nWhile the industrial revolution mainly transformed physical work, foundation models are likely\nto transform tasks involving cognitive work, like content creation and communication. In general,\nsince foundation models are intermediary assets that often possess strong generative capabilities,\nwe envision that they will be able to augment humans in many creative settings, rather than replace\nhumans as there are still significant limitations in using these models stand-alone for open-ended\ngenerative tasks [See et al. 2019]. As we describe in §2.5: interaction, foundation models may\nalso power systems that users can leverage to co-construct novel forms of art or more efficiently\nprototype new applications. Fluid human-machine and human-in-the-loop interaction will require\nadvances in interface design (§2.5: interaction) as well as fundamental improvements in the\ninterpretability (§4.11: interpretability) and robustness (§4.8: robustness) of these models, so\nthat humans can understand model behavior and expect models to perform well in diverse contexts.\nOn the Opportunities and Risks of Foundation Models 151\n5.5.3 Centralization.\nAnother key determinant of foundation models’ economic impact is who owns data and models. In\nparticular, pushing the frontier of foundation models has thus far primarily been the purview of\nlarge corporate entities. As a result, the ownership of data and models are often highly centralized,\nleading to market concentration (§5.6: ethics). In turn, this can lead to significant centralization of\ndecision rights and power, reducing income and opportunities for those who don’t have ownership.\nThis centralization of power can lead to an equilibrium where fewer people have social and\neconomic mobility and opportunity, a situation that Brynjolfsson [2022] calls \"The Turing Trap\". To\ncounterbalance this centralization, there have been grassroots efforts to open source AI research\nsuch as Masakhane, EleutherAI, and HuggingFace, or build foundation models through distributed\ntraining. However, it likely that the gap between the private models that industry can train and the\nones that are open to the community will remain large due to foundation models’ dependence on\nmassive amount of data and computational resources (§5.3: environment).123\n5.5.4 Other considerations.\nThis short chapter is not meant to be comprehensive of all the economic effects of foundation\nmodels. In addition to affecting productivity, wage inequality, and ownership, foundation models\nmay also have significant effects on job quality and job satisfaction. For instance, they may increase\njob satisfaction by automating repetitive, uninteresting parts of work, or decrease satisfaction by\nincreasing the pace of work, thereby inducing more frequent burnout. As discussed in §5.1:fairness\nand §5.6: ethics, they can also amplify and perpetuate bias, often in unexpected ways, or be used\nas a tool for reducing it. Foundation models can facilitate global trade and remote work, just as\nearlier uses of machine translation systems had significant effects in these areas [e.g., Brynjolfsson\net al. 2019]. There may also be significant environmental effects (§5.3: environment), as well as\nunexpected and unanticipated effects on the rate and direction of occupational change and business\ntransformation in an economy. More broadly, given the emergent capabilities of foundation models,\nwe should expect new unknown unknowns to arise that are difficult to predict, and which may\nhave substantial follow-on effects.124\nIn summary, foundation models are poised to be an important general-purpose technology of our\nera. They have potential to increase living standards substantially, but also pose risks of increasing\ninequality and concentrating power. The economic implications of these technologies are not\npredetermined, but rather depend on how technologists, policymakers, managers, workers, and\nother stakeholders answer challenges such as:\n•How can we harness the potential of foundation models to boost productivity?\n•Can we develop models that enhance creativity and boost the rate of innovation?\n•Will the benefits and control rights be limited to a few or widely shared?\nUnderstanding the economic potential of these systems is the first step to guiding them in directions\nthat match our values.\n123Lambda Lab estimates that GPT-3 training costs over $4.6M, research and development costs between $11.4M and $27.6M,\nhardware required to run GPT-3 costs between $100K and $150K without factoring in other costs (electricity, cooling, backup,\netc.), and running costs a minimum of $87K per year. (https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model)\n124As an example of a secondary effect, consider that the invention of the automobile influenced the development and\nexpansion of the suburbs.\n152 Center for Research on Foundation Models (CRFM)\n5.6 Ethics of scale\nAuthors: Kathleen Creel, Dallas Card, Rose E. Wang, Isabelle Levent, Alex Tamkin, Armin W. Thomas,\nLauren Gillespie, Rishi Bommasani, Rob Reich\nThe widespread adoption of foundation models poses ethical, social, and political challenges in\naddition to concerns about increased inequity, the subject of §5.1: fairness. In this section, we\ndiscuss social, political, and ethical risks related to the scale of application of foundation models, such\nas homogenization and the concentration of power, the norms and release strategies appropriate to\naddress them, and concerns about the broader political economy in which foundation models are\ndeveloped and deployed.\n5.6.1 Homogenization and scale.\nIf the same model is used across a variety of domains with minimal adaptation, the strengths,\nweaknesses, biases, and idiosyncrasies of the original model will be amplified (§5.1: fairness). This\nis true of the widespread adoption and reliance on any standardized technology. Similar to how a\nfailure in the manufacturing of a part used in many cars or airplanes could have widespread and\nsevere consequences across sectors, a bias or failure of serviceintrinsic to a foundation model could\nripple outwards. However, the current uninterpretability (§4.11: interpretability) of foundation\nmodels and their task-agnostic training makes predicting, understanding, and addressing these\nweaknesses challenging. If, as seems likely, foundation models become widely adopted, foundation\nmodel developers bear greater responsibilities of care than standard model developers, as their\nchoices in design and deployment have widespread implications [Arendt 1987].\nThe defining feature of foundation models — their capacity to be usefully adapted for a multiplicity\nof tasks — is what makes them likely to be widely adopted for a vast range of socially consequential\ntasks. In contrast to the current distributed and varied model of decision making, employing many\nadaptations of the same foundation model for multiple automated decision-making tasks means\nthat decision subjects may face a more homogeneous set of judgments rooted in the underlying\nfoundation model.\nThis algorithmic monoculture [Kleinberg and Raghavan 2021] could lead to consistent and\narbitrary rejection, mis-classification, or ill-treatment of individual decision subjects [Gandy 2021].\nWe will call this homogenization [Creel and Hellman 2021]. For example, §4.6.2: data-solutions\ndiscusses data quality issues that lead to undesirable behavior on subpopulations of data, where\nsubpopulations can be produced by any filter that stratifies the data, including by social group (see\nrelated discussions in §4.11.1: interpretability-behaviorand §4.8.1: robustness-advantages).\nUntil improvements are made in data quality tooling (§4.6.2: data-solutions) and the ability to\nidentify slices of data on which the model under-performs [Chung et al. 2019; Goel et al. 2021], a\nfoundation model might consistently fail to provide accurate information or services to a subgroup\nof people (see also §4.8: robustness.\nHomogenization has the potential to amplify bias; to standardize bias, compounding injustices\nrather than distributing them; and to amplify arbitrary exclusion [Creel and Hellman 2021; Gandy\n2021]. For example, Zhou et al. [2021a] have argued that BERT encodes an Anglocentric similarity\nmetric by default , one that could be harmful if applied across contexts where foundation models\nare applied. The application of foundation models across domains has the potential to act as an\nepistemically and culturally homogenizing force, spreading one implicit perspective, often a socially\ndominant one, across multiple domains of application.\nExisting trends in standardization of training corpora are likely to be exacerbated in foundation\nmodels due to the massive scale of both unlabeled and labeled data needed. To the extent that models\nOn the Opportunities and Risks of Foundation Models 153\ntrain on similar data, they are likely to acquire similar patterns of behavior, biases (§5.1.3:fairness-\nsources), and errors. Previous high-effort data curation and labeling efforts such as ImageNet have\nstandardized training corpora. In doing so, they have also standardized errors: models trained on\nImageNet often rely on the same “spurious cues” and “shortcuts”, for example using background\ntextures like green grass to predict foreground object classes such as cows [Geirhos et al . 2020;\nHendrycks et al. 2021e]. Despite their increased robustness to many types of distribution shifts\n(§4.8.1: robustness-advantages), foundation models and other large models have been no less\nlikely to learn spurious correlations (§4.8.2: robustness-challenges), and are therefore likely to\nlearn similar errors if trained on the same datasets. Similar effects may arise due to the choice of\npublicly available unlabeled data. Many foundation models are trained on unlabeled corpora that\nare chosen for their convenience and accessibility, for example public internet data [Caswell et al.\n2021], rather than their quality. However, publicly accessible data, whether labeled or unlabeled,\nis often outweighed by proprietary data in the training corpora of many proprietary foundation\nmodels, as discussed in [Marr 2017] and §4.6.1: data-desiderata. Therefore more research is\nneeded on the extent to which training on similar data homogenizes correlations within foundation\nmodels and the extent to which this homogenization might cause uniform failures in adapted\nderivatives of the model (unless constraints are applied to eliminate the behavior during each\nadaptation, as discussed in §4.3.2: adaptation-usecases).\nHomogenization is not inevitable. As model developers intentionally broaden the range of\nperspectives represented in their datasets (§5.1.3: fairness-sources), more research is needed on\nthe capacity of foundation models to deliver a diversity of perspectives when used for generative\ntasks. For example, Sheng et al. [2021] have demonstrated that dialogue systems that adopt “personas”\nof specific demographic groups behave differently on measures of social bias. In addition to choosing\nbetween “personas” with the goal of avoiding bias, “personas” that are diverse along a variety of\ncognitive and demographic axes could also be used to generate a broader range of coherent outputs\nfor generative tasks. There remain many open questions about how to balance diversity of outputs\nwith relevance and utility to an individual user.125\n5.6.2 Surveillance, exclusion, and power.\nA key premise of foundation models is that massive unlabeled datasets can be combined with vast\ncomputational resources to create a basis from which numerous products can be derived for a\nvariety of applications. This paradigm shift has the potential to alter social structures and shift\npower, establishing or entrenching the influence of model creators [Zimmerman 2020]. We discuss\nthree potential implications below.\nMass data collection and surveillance. Whereas collecting a labeled dataset typically requires\nworking with domain experts and understanding the problems with and limitations of such data,\nthe need for exceptionally large amounts of data in training foundation models has encouraged\nsome researchers to emphasize quantity rather than quality.126 Though preprocessing can help\nimprove the quality of this data [e.g., Brown et al . 2020], the scale involved necessitates automated\napproaches, which may be blunt or poorly documented [Dodge et al. 2021].\nAlthough there is an evolving landscape of data protection legislation (e.g., GDPR in Europe), a\nvariety of questionable practices continue to be used in acquiring data, from opaque policies [Obar\nand Oeldorf-Hirsch 2020] and the use of “dark patterns” (i.e., manipulative interfaces [Narayanan\n125For possible approaches to implementation, see the discussions of controllable generation in [Keskar et al. 2019] and\n§4.3.2: adaptation-usecasesand general discussions in [Dinan et al. 2021].\n126For example, Ding et al. [2021] collected 30 million text-image pairs, chose not to address artefacts such as watermarks\nand white edges, despite their impact on model quality.\n154 Center for Research on Foundation Models (CRFM)\net al. 2020]) to outright violation of terms of service. Indeed, this was essentially the strategy taken\nby Clearview AI — a company which scraped photos from social media, without user consent,\nand in violation of platforms’ terms of service, for the purpose of developing facial classification\nsoftware. The company was nevertheless able to sell this technology to police departments and\nother organizations, in many cases without the knowledge of state lawmakers or department heads\n[Mac et al. 2021]. To the extent that the paradigm of foundation models increases the value of being\nfirst to have the largest possible dataset for any particular domain, this may further encourage actors\nto pursue aggressive data collection, even when that pursuit is legally questionable or contrary to\nuser expectations [Nissenbaum 2009; Zuboff 2018].\nThe importance of data to foundation models also means that organizations already engaged in\nwidespread data collection will be in a strong position to develop such models, and will likely have\nincentive to maintain this advantage. To the extent that derivative products could themselves be\nused to collect additional data (e.g., in surveillance or health diagnostic applications), developers of\nfoundation models may seek to ensure that they obtain ownership of such data. Thus, even though\na key advantage of the foundation model paradigm is the ability to generate adapted derivatives,\nthe developers of foundation models might seek to license their work in a way that ensures that\ndata flows back to them from all adapted derivatives.127\nConcentration of power. Although the absolute cost of computation has become dramatically\ncheaper over time, the training of the largest foundation models currently requires computational\nresources that put their development beyond the reach of all but a few institutions and organizations\n(§5.3: environment). Thus, the question of who has access to the relevant computational resources\nand data will likely determine who is able to produce cutting-edge foundation models in the coming\nyears (see also §5.5.3: economics-centralization).\nGPT-3 was at least partly an experiment in scale, showing that major gains could be achieved by\nscaling up the model size, amount of data, and training time, without major modeling innovations.\nAlthough there is extensive ongoing research into reducing the amount of resources required\nin training such models (see §4.2: training), OpenAI’s work suggests that there are still gains\nto be had from even larger scale efforts [Kaplan et al . 2020], and it seems plausible that other\norganizations may seek to follow this path in other domains (for example, see [Lieber et al. 2021]).\nIf scale does turn out to be critical to success, the organizations most capable of producing competi-\ntive foundation models will be the most well-resourced: venture-funded start-ups, already-dominant\ntech giants, and state governments. This raises potential concerns about market concentration,\nand might indicate the kind of incumbent monopoly or oligopoly that currently exists in extreme\ncapital-intensive industries such as defense and semi-conductor manufacturing [Carril and Duggan\n2020].\nMoreover, this centralization of power raises concerns about the ability of currently-marginalized\nindividuals and communities to participate in the foundation model development process [Kalluri\n2020]. Especially within the realm of government services, the adoption of foundation models could\nfurther transfer decision making power from governments to corporate service providers, and\nintroduce additional barriers to due process and accountability [Citron 2008]. Nevertheless, more\ngrassroots efforts (e.g., Masakhane, EleutherAI, HuggingFace) provide encouraging alternatives, and\nthere is extensive work on ways to incorporate participatory or value-sensitive design [Friedman\nand Hendry 2019; Prabhakaran and Donald Martin 2020].\n127As a less sophisticated example, consider the credit scoring industry, which has been able to position itself such that\ninformation flows back to central data brokers as people use its products (as in vetting loan applications), and individuals\nhave little choice but to participate [Lauer 2017].\nOn the Opportunities and Risks of Foundation Models 155\nFueling widespread automated decision-making. Recent years have seen a dramatic expan-\nsion in the use of automated decision-making systems in industry and government [O’Neil 2016;\nEngstrom et al. 2020]. Although many of the concerns over such automation are not specific to\nfoundation models, the generative abilities of models such as GPT-3, as well as the impressive\nperformance on benchmark tasks (e.g., Devlin et al. [2019]), have the potential to prompt a less-than-\ncareful adoption of this technology by, for example, administrative agencies, many of which lack\nthe expertise necessary to understand sophisticated ML systems [Calo and Citron 2021]. As such, it\nis especially important to communicate clearly about the realistic capabilities and limitations of\nfoundation models.\nMost automated decision-making systems will exist as parts of broader sociotechnical systems\nin which humans play key roles [Selbst et al. 2018].128 As such, there is no guarantee that even\nlarge improvements in performance on standardized evaluations will translate into the desired\noutcomes in the real world (especially if systems are deployed without careful consideration or\nongoing evaluation). For example, research has shown that judges may re-impose racial prejudice\nin interpreting the outputs of a risk assessment system [Albright 2019], or otherwise impose their\nown biases [Stevenson and Doleac 2021]. Ongoing evaluation with proper ecological validity [de\nVries et al. 2020] will be critical in this regard, but may not stop potentially dangerous or costly\nsystems from being adopted without adequate evidence [Ferguson 2017]. Research is ongoing on\nmethods of refusal: ways for individuals to opt out of participation in foundation models and their\nadapted derivatives, either as data or decision subjects, without repercussions [Benjamin 2016].\nIn short, the existing problems with algorithmic decision making will be seen in the functioning\nof foundation models once they are deployed. And to the extent that adopting foundation models\naccelerates a shift from human to machine decision making, foundation models accentuate the con-\ncerns with automation. Although there are not obvious solutions to these challenges, it is important\nto make questions about how foundation models will impact power part of the conversation about\ntheir creation; to communicate with civil society organizations, policy makers, and citizens about\nthe capabilities and limitations of such systems; and to strive for broader dialogue among diverse\nsegments of society about the adoption of such models.\n5.6.3 Norms.\nPublic policy and formal regulation by law (§5.4: legality) play an essential role in creating the\ninfrastructure for technological innovation as well as mitigating the potentially harmful effects of\nwidely disseminated technologies. As illustrated by the decades-long gap between the Tuskegee\nSyphilis experiments and the development of research protocols and institutions like the IRB,\npublic policy to protect human subjects and stakeholders tends to lag behind public awareness and\nevidence of harms to them [Grady 2015; Stark 2012; Department of Health and Welfare 1979]. As a\nresult, society relies upon professional norms for responsible development and deployment and the\nestablishment of best practices.\nNorms exist on a continuum between recommendation and requirement. As a nascent technology,\nthe norms for responsible foundation model development and deployment are not yet well estab-\nlished at either strength of recommendation [Crootof 2019]. In what follows we will discuss norms\nfor deployed models, as models for research have a wider latitude.\nThose who wish developers of foundation models to adopt certain norms might lead by example,\nallowing their own conduct and statements torecommend the norm. As discussed in §1.2:ecosystem,\nwe believe that universities and other nonprofit institutions have an important role in modeling\nnorms for foundation models. As educational institutions, universities are in the unique position to\n128For an extended study of how humans interact with automated judgements, including discussion of both positive and\nnegative automation biases, see Hidalgo [2021].\n156 Center for Research on Foundation Models (CRFM)\nencourage the next generation of theorists and practitioners to consider the issues raised in this\nreport and also to foster interdisciplinary conversation between researchers and students [Rogers\n2021]. Universities and colleges may also contribute to the establishment of norms by auditing\nexisting foundation models and publishing their findings, instituting ethics review boards [Bernstein\net al. 2021], and developing their own foundation models.\nTo create and adopt norms will require institutionalization in funding structures, model reposi-\ntory, release practices, conference submission, and grant proposal requirements.129 For example,\nHuggingFace’s interface currently encourages the posting of data and model cards, including\ndiscussions of bias and social impact.130 Since it is not required, and perhaps since data quality\nwork is undervalued relative to its importance [Sambasivan et al . 2021], few are filled out. Bias\nand social impact are included in ethics statements for conferences and some forms of standard\nevaluation (as discussed in §4.4: evaluation), but otherwise treated as optional considerations by\nsome researchers. This must change.\nFor some socially consequential use cases, we recommend legal standards be established that\nrequire adapted derivatives to provably exhibit certain properties (§5.4: legality). Domains of\nspecial concern should be democratically decided but are likely to include allocating and distributing\ngovernment services, medical diagnosis and monitoring, hiring, and lending: all contexts in which\nopportunities or even lives of people rest on the proper functioning of an adapted derivative.\nWhat norms should we promote, institutionalize, or require? We recommend a few here, but\naim primarily to encourage dialogue about appropriate norms for the development and use of\nfoundation models. Prior work has often focused on norms that advocate documentation [Gebru\net al. 2018; Bender and Friedman 2018; Mitchell et al. 2019; Dodge et al. 2019]. Because many of\nthe negative social consequences that appear in a downstream context may initially appear to be\nextrinsic or particular to a use case (§5.1: fairness), documentation and transparency are especially\nimportant for foundation models. Currently, those who adapt foundation models that document\nthe biases or other negative features of their adapted derivatives have no automatic mechanism\nto report their findings to the developers of the foundation model. Compiling multiple reports of\nrelated problems in adapted derivatives may allow the model development team to discover an\nintrinsic property of the model that spans multiple use cases. Because creators of adapted derivatives\noften represent different entities than from foundation model developers or providers, additional\nreporting structures and norms or regulation would be needed for this type of feedback to reach\nfoundation model developers. Such feedback could also be made available to the general audience\nof model auditors, thereby making auditing and pursuing recourse more accessible.\nPublic commitment to norms, standards, and creation of reporting mechanisms could also allow\ndownstream users to submit feedback to foundation model providers. In order to enable this, adapted\nderivatives should be consistently labeled in a way that allows impacted parties to trace problems\nto their source. Significant technical and social barriers may impede this tracing in practice, such as\nprivacy considerations and the proprietary nature of many foundation models, but without labeling\nit would be impossible.\nIt is important that model developers and providers create mechanisms for such reporting.\nReporting mechanisms could be informed by similar structures on current platforms, such as issue\ntracking on open source projects on GitHub. In particular, the submitted issues should be public so\nthat other users can identify trends even if changes have not yet been made and so that developers\nand providers can be held accountable for unaddressed issues. Additional mechanisms are needed\n129For helpful discussion of partial compliance with “non-compulsory fairness-conscious policy” such as the norms under\ndiscussion here, see Dai et al. [2021b].\n130https://huggingface.co/docs/datasets/master/\nOn the Opportunities and Risks of Foundation Models 157\nto escalate trends upwards to foundation model providers. Similar suggestions regarding tracking\nissues in training data are discussed in Dinan et al. [2021] and §4.6: data.\nHolland et al . [2018] suggest the nutrition label as a helpful model, drawing from labeling\ndiscussions in consumer privacy [Kelley et al. 2009]. A nutrition label includes both a list of the\n“raw” ingredients and the full nutritional information of the processed food. So too a model card\n[Mitchell et al. 2019] or nutrition label for an adapted derivative could include both a list of the\n“raw materials” such as training data and foundation models used, and the full “nutritional content”\nof the adapted derivative such as its known capacities, weaknesses, and biases.\nReporting of the full pipeline is necessary in order for data subjects and impacted parties to trace\nharms to their sources. However, without the ability toattribute responsibility for the harm to either\nthe adapted derivative, the foundation model, or both, and without a framework for recourse once\nharm has been attributed, even a successful tracing of a harm will be unlikely to lead to changes in\nthe model (see also §5.1.4: fairness-recourse). Thus, significant technical, policy, and legal work\nis needed in order to develop frameworks for communicating data, model, and derivative contents\nto other experts and eventually to the public; to attribute responsibility for harms; and to create\navenues for recourse.\n5.6.4 Release and Auditing.\nIn February 2019, OpenAI embarked on an experiment. By releasing a reduced 124M parameter\nGPT-2, sans datasets, they hoped to buy time: time to test for bias, time to prepare for misuse, and\ntime for society to adapt to the presence of large language models [Solaiman et al . 2019]. Eight\nmonths later, when OpenAI released the full ∼1.5 billion parameter version, testing had exposed\nsome but by no means all of the model’s capabilities and limitations. When considering similar\nquestions today, the possible harms of release, centering primarily on misuse (§5.2:misuse),131 must\nbe weighed against the benefit of transparency that no closed-door testing can replicate, namely\nbroader and independent auditing and access.\nAuditing Auditors probe the limitations of current models and suggest paths to fixing them, as\nwell as testing the model’s adapted derivatives in a wide variety of natural settings. A policy\nof open access for auditing allows more numerous and diverse researchers to investigate any\nmodel’s biases, limitations, and security vulnerabilities, better informing acceptable uses of the\nmodels and calibrating appropriate trust in them [Danks 2019; Baier 1986].132 In order to support\nindependent audits of foundation models, model developers or third-party intermediaries could\nhost open API access for auditors, including gradient access, and allow access to training data [Raji\nand Buolamwini 2019; Raji et al. 2020].\nFoundation models trained on proprietary data in industry are unlikely to be released, and those\ntrained on private data (as in a medical context) should not be. In order for proprietary models to\nbenefit from independent audits, and for model subjects to benefit from improvements prompted\nby an auditing process, we recommend that audits occur during a staged release. While staged\nrelease may not illuminate all possible model use cases, one way to broaden the range of uncovered\nuse cases is to enlist a neutral third party to decide which individuals or organizations should\nreceive early access in the staged-release program. When model developers decide who should\nreceive staged access, they open themselves up to charges of favoritism, selective distribution,\nand manipulating public perception of their product. A neutral “staged release board”, or federal\nauditors, could provide a backstop against these failure modes and ensure that a wide range of\n131For analysis of harms related to misuse, see [Rini 2017] on fake news and [Rini 2020] on deepfakes.\n132Calibrating trust may require an explanation capable of illuminating features of the model relevant to trust, such as\n“discriminatory use of a sensitive feature” [Dimanov et al. 2020].\n158 Center for Research on Foundation Models (CRFM)\nauditors and users are provided access in order to capture a range of disciplinary expertise and\nsectors of society. A staged release board could also mitigate any perception that auditors would be\nat risk of losing their early access to the model if they share unflattering outputs, as they might be\nin a standard staged release process.\nAccess and adaptation. To the extent that there are social benefits to foundation models, release\nof models holds the potential to further distribute them. Large language models such as BERT and\nM-BERT are capable of cross-lingual transfer, which — when the models are open-sourced — may\nallow for adaptation to languages which otherwise would have too few texts available [Wu and\nDredze 2019; Wang et al . 2020a]. Given the number of languages not currently well served by\ncommercial providers, such a benefit alone could be substantial.\nRelease is not sufficient to democratize access to foundation models, as the barrier of compute\npower still precludes many from modifying or even loading foundation models, let alone developing\ntheir own. However, on each of these points we have seen significant recent technical improvement.\nMemory techniques such as the zero redundant optimizer (ZeRO) allow researchers to run and train\nvery large models on a simple setup [Rasley et al. 2020; Rajbhandari et al. 2021]. Techniques such\nas distillation could allow the release of smaller, more tractable models that recoup much of the\nperformance of their parent model while being much easier to train [Li et al. 2020d]. Development\nof less energy-intensive training methods, as discussed in §5.3: environment, could further spread\nthe ability to work with released models. Increases in efficiency such as the co-design of hardware\nand software are needed to train yet larger models, as discussed in §4.5: systems, but could also be\nused to lower the price of access to current models.\nThe most powerful of the harms, by contrast, are not obviously fueled by release. Sophisticated\nor institutional actors with the capacity to embark on large-scale disinformation, cyberwarfare,\nor targeted phishing also are likely to have the capacity to create a similar model if none were\nreleased. Although potentially significant, these harms should not therefore weight heavily on a\nrelease calculus [Solaiman et al. 2019; Shevlane and Dafoe 2020]. The harms to be weighed against\nthe benefits are those from less well-resourced actors who would not be able to create their own\nfoundation model but may be motivated to generate spam or abuse, fake reviews, or cheat on tests.\nDoes the benefit of release outweigh the potential for harm from actors sophisticated enough to\nuse a released model or API but not sophisticated enough to create their own? We believe that the\nanswer is yes. Research teams with the resources and connections necessary to develop foundation\nmodels are few in number. Even collectively, we are unlikely to be numerous or diverse enough to\nimagine all possible beneficial use cases or all possible probes that could illuminate the capability\nsurface of a foundation model.\n5.6.5 When not to build.\nThe development and deployment of powerful technologies is not like gravity, an external force\nthat acts upon us. Technologies reflect a set of choices made by humans; human agency shapes the\ntechnological frontier. It follows that technologists can choose when not to build, design, or deploy\nfoundation models [Zimmermann 2021]. This decision need not be binary; instead, one can refuse\nto engage in the default way by subverting embedded values, challenging assumptions, and shaping\nresearch agendas [Simpson 2007]. Technical artifacts, foundation models included, are inherently\npolitical, so the research about them has a socio-political context, not solely a technical one.\nDevelopers and researchers should be cognizant of which problems they seek to address, e.g., how\nto scale up a foundation model versus how to make it more computationally accessible; how those\nproblems are formulated; and who their solutions ultimately empower [Rogaway 2016; Winner\n1980; Passi and Barocas 2019]. We should value research that seeks to make foundation models more\nOn the Opportunities and Risks of Foundation Models 159\ninterpretable, accessible, sustainable, and fair (see §4.11: interpretability, §5.3: environment,\n§5.1: fairness).\nBy asking when not to build a foundation model or adapted derivative, we are implicitly asking\nnot only “What should we build or not build?” but also, “Under what conditions should a model\nbe built?” and “What criteria and principles govern building?” The first question stems from the\nmodel view; the following questions from the ecosystem view (§1: introduction).\nAn invitation to consider refusing to build is not tantamount to saying, “Do nothing. ” It is an\ninvitation to make deliberate and judicious choices about what is worth the time, financial resources,\nexpertise, and energy use to build, design, and deploy. Ultimately, this is a difficult, moral question\nrooted in context and values. There are cases in which the application of adaptive derivatives\n(and algorithms and machine learning more generally) is inappropriate, because the community\nimpacted protests or because the adaptive derivative naively exacerbates systemic issues that are\nbetter addressed with public policy, additional funding, or interdisciplinary collaborations [Angwin\net al. 2016].\nThe Belmont Report, as applied to machine learning in Floridi et al. [2018], provides one possible\nframework for this question. Drawing from the principle of \"beneficence\" [Department of Health\nand Welfare 1979], we can identify cases to reconsider building when an adaptive derivative or a\nresearch avenue might cause more harm than good or even provide no benefit at all. Alternatively,\nthere may be cases in which an adaptive derivative is better at a task on a metric of efficiency,\nperformance, and generalization, values prioritized in the machine learning community [Birhane\net al. 2020], but an individual, community, or organization might choose to prioritize an existing\nsolution that highlights other values such as human connection and interpretability [Benjamin\n2016].133 In doing so, they exercise their autonomy — as explained in the Belmont Report’s \"respect\nfor persons\" — in deciding that this is not an appropriate context in which to build [Department of\nHealth and Welfare 1979].\nAnswering the question of when not to build is a matter of individual responsibility as well as\na broader professional responsibility. The decision not to build something by one person, or one\nteam, or one company, invites the reply, “But if we don’t build this, someone else will, and they may\nlikely do it worse. ” A simple utilitarian weighing of comparative harms of the outcomes of the two\nmodels misses the importance of integrity. It matters very much whether we are the ones building\nthe bad model or whether someone else is [Williams 1973]. Individuals have reasons not to build\nsomething that goes against their values or that they cannot endorse as right to build [Korsgaard\n2009]. However, the structural environment so created is different. If even one company decides\nto build the most effective version of an ethically-dubious model, they open the door for other\ncompanies to consider similar avenues of research; they make it competitively disadvantageous\nnot to pursue the research [Askell et al. 2019]. When not to build is then a collective question as\nmuch as it is an individual one, requiring the community to adhere to codes of professional ethics\nand responsibility.\nIn the AI/ML community this infrastructure is underdeveloped compared to other fields such\nas the medical field. Although professional bodies like the Association for Computing Machinery\n(ACM) have ethics statements, both industry and academia lack widely used and accepted profes-\nsional oaths (e.g., the Hippocratic Oath or the the Obligation of the Engineer), regulatory bodies\ninvolved in deployment and research (e.g., the FDA for drugs), and official protocols for ethics\nreview (e.g., the IRB for research involving human subjects; [Bernstein et al. 2021]). The ability to\nopt-out can be incorporated into the foundation model ecosystem at many stages, including during\ndata production, adaptation, and deployment. As the norm veers towards collecting larger and\n133See also §4.11.4: interpretability-impactsfor relevant discussion of impacts of uninterpretability.\n160 Center for Research on Foundation Models (CRFM)\nlarger swaths of training data (§4.6: data), we should endeavor to maintain a \"respect for persons, \"\n[Department of Health and Welfare 1979] emphasizing privacy and consent as part of the data\nlife cycle. This would require innovation in data management and a more concrete understand-\ning — technically and philosophically — of informed consent online, ways of documenting and\nensuring that consent is respected, and privacy (see §4.6: data for a specific data management\nproposal; [Ohm 2014]). Although data and foundation models are diverse in their applications,\ndata participants should be able to indicate how they do not want to have their data used. An\nopt-out consent model favors developers, as it does not require them to to get consent for each new,\nunexpected use case. Important then is the right to revoke consent given vacuously for applications\nthat are now being pursued, but were not when consent was originally given.\n5.6.6 Conclusion.\nIn this section, we have surveyed some of the risks to society that accompany the widespread\nadoption of foundation models, such as the homogenization of outcomes and centralization of\npower. Developers of foundation models should adopt norms regarding development, auditing, and\nrelease of foundation models in order to address these risks, aided by legislative requirements, and\nindividuals should be able refuse to be data or decision subjects of foundations models without\nrepercussion.\nMany implications of foundation models’ generative and interactive capacities remain unsurveyed\nhere. For example, §5.5: economics discusses the potential gains to economic productivity from the\nautomation of creative and design work. However, in virtue of their generative nature, foundation\nmodels may replace work that many people find meaningful and fulfilling, such as graphic design\nand writing. We hope that the scope of this report will aid others in their pursuit of the questions\nof ethics and society unaddressed here.\nOn the Opportunities and Risks of Foundation Models 161\n6 CONCLUSION\nIn this report, we have endeavored to comprehensively discuss many of the most critical aspects of\nfoundation models, ranging from their technical underpinnings to their societal consequences. In\nthis way, we acknowledge the unusual approach taken: we have attempted to clarify the nature of\na paradigm that may only have just begun, rather than waiting for more to unfold or the dust to\nsettle. Therefore, much still remains unclear in spite of our efforts and we reiterate that this is just\nthe beginning of a paradigm shift: foundation models have only just begun to transform the way\nAI systems are built and deployed in the world. Moving forward, we view this document as serving\nan important role in orienting and framing dialogue on these models and this new paradigm in\nAI. That said, to ensure the responsible development and deployment of these models on durable\nfoundations, we envision collaboration between different sectors, institutions, and disciplines from\nthe onset to be especially critical.\nACKNOWLEDGMENTS\nWe would like to thank the following people for their valuable feedback: Mohit Bansal, Boaz Barak,\nYoshua Bengio, Stella Biderman, Su Lin Blodgett, Sam Bowman, Collin Burns, Nicholas Carlini,\nDavid Chalmers, Jack Clark, Jeff Dean, Jesse Dodge, Jarred Dunnmon, Gabe Dupre, Jason Eisner,\nIason Gabriel, Dan Hendrycks, Avery Hill, Yacine Jernite, Gabbrielle Johnson, Sarah Kreps, Jay\nMcClelland, Preetum Nakkiran, Julian Nyarko, Fernando Pereira, Vinodkumar Prabhakaran, Colin\nRaffel, Marten van Schijndel, Ludwig Schmidt, Yoav Shoham, Madalsa Singh, Megha Srivastava,\nJacob Steinhardt, Emma Strubell, Qian Yang, Luke Zettlemoyer, and Ruiqi Zhong. In addition, we\nwould like to especially thank Vanessa Parli for helping to organize this effort.\nCONFLICT OF INTEREST\nThis report was authored by the Center for Research on Foundation Models (CRFM), a center at\nStanford University borne out of the Stanford Institute for Human-Centered Artificial Intelligence\n(HAI). CRFM receives funding from Google, Microsoft, and the McGovern Foundation as of July\n2022, though this funding was not directly related to this report. Authors of this report may also be\naffiliated with other institutions beyond Stanford: their contributions reflect only their views and\nnot those of these institutions.\nREFERENCES\nMartin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,\nGeoffrey Irving, Michael Isard, et al. 2016. TensorFlow: A system for large-scale machine learning. In Proceedings of the\n12th USENIX Symposium on Operating Systems Design and Implementation (OSDI). Savannah, Georgia, USA .\nJordan Abdi, Ahmed Al-Hindawi, Tiffany Ng, and Marcela P Vizcaychipi. 2018. Scoping review on the use of socially\nassistive robot technology in elderly care. BMJ open 8, 2 (2018), e018815.\nRediet Abebe, Solon Barocas, Jon Kleinberg, Karen Levy, Manish Raghavan, and David G Robinson. 2020. Roles for computing\nin social change. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . 252–260.\nAbubakar Abid, M. Farooqi, and J. Zou. 2021. Persistent Anti-Muslim Bias in Large Language Models. ArXiv abs/2101.05783\n(2021). https://arxiv.org/abs/2101.05783\nSerge Abiteboul. 1997. Querying Semi-Structured Data. International Conference on Database Theory , 1–18.\nSteven Abney. 2007. Semisupervised Learning for Computational Linguistics (1st ed.). Chapman & Hall/CRC.\nDaron Acemoglu. 2021. Redesigning AI . MIT Press. https://books.google.com/books?id=HBb6DwAAQBAJ\nDaron Acemoglu and Pascual Restrepo. 2019. Automation and new tasks: How technology displaces and reinstates labor.\nJournal of Economic Perspectives 33, 2 (2019), 3–30.\nAccountability Act. 1996. Health insurance portability and accountability act of 1996. Public law 104 (1996), 191.\nGeorge Adam, Ladislav Rampášek, Zhaleh Safikhani, Petr Smirnov, Benjamin Haibe-Kains, and Anna Goldenberg. 2020.\nMachine learning approaches to drug response prediction: challenges and recent progress. NPJ Precision Oncology 4, 1\n(2020), 1–10.\n162 Center for Research on Foundation Models (CRFM)\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2017. Fine-grained Analysis of Sentence\nEmbeddings Using Auxiliary Prediction Tasks. In International Conference on Learning Representations .\nStein Aerts, Diether Lambrechts, Sunit Maity, Peter Van Loo, Bert Coessens, Frederik De Smet, Leon-Charles Tranchevent,\nBart De Moor, Peter Marynen, Bassem Hassan, et al. 2006. Gene prioritization through genomic data fusion. Nature\nBiotechnology 24, 5 (2006), 537–544.\nSameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. 2011.\nBuilding rome in a day. Commun. ACM 54, 10 (2011), 105–112.\nSandhini Agarwal, Gretchen Krueger, Jack Clark, Alec Radford, Jong Wook Kim, and Miles Brundage. 2021. Evaluating CLIP:\nTowards Characterization of Broader Capabilities and Downstream Implications. arXiv preprint arXiv:2108.02818 (2021).\nArmen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettlemoyer. 2021.\nHTLM: Hyper-Text Pre-Training and Prompting of Language Models. CoRR abs/2107.06955 (2021). arXiv:2107.06955\nhttps://arxiv.org/abs/2107.06955\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. 2020. Intrinsic Dimensionality Explains the Effectiveness of\nLanguage Model Fine-Tuning. CoRR abs/2012.13255 (2020). arXiv:2012.13255 https://arxiv.org/abs/2012.13255\nAshvin Agrawal, Rony Chatterjee, Carlo Curino, Avrilia Floratou, Neha Gowdal, Matteo Interlandi, Alekh Jindal, Kostantinos\nKaranasos, Subru Krishnan, Brian Kroth, et al . 2019. Cloudy with high chance of DBMS: A 10-year prediction for\nEnterprise-Grade ML. arXiv preprint arXiv:1909.00084 (2019).\nRoxana M Agrigoroaie and Adriana Tapus. 2016. Developing a healthcare robot with personalized behaviors and social\nskills for the elderly. In 2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI) . IEEE, 589–590.\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. 2021. OPAL: Offline Primitive Discovery for\nAccelerating Offline Reinforcement Learning. ArXiv abs/2010.13611 (2021).\nIfeoma Ajunwa. 2019. The paradox of automation as anti-bias intervention. Cardozo L. Rev. 41 (2019), 1671.\nHassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. 2021. Vatt:\nTransformers for multimodal self-supervised learning from raw video, audio and text. arXiv preprint arXiv:2104.11178\n(2021).\nGuillaume Alain and Yoshua Bengio. 2016. Understanding intermediate layers using linear classifier probes. In International\nConference on Learning Representations .\nEA AlBadawy, A Saha, and MA Mazurowski. 2018. Deep learning for segmentation of brain tumors: Impact of cross-\ninstitutional training and testing. Med Phys. 45 (2018).\nAlex Albright. 2019. If You Give a Judge a Risk Score: Evidence from Kentucky Bail Decisions. (2019). https://thelittledataset.\ncom/about_files/albright_judge_score.pdf Accessed 2021-07-18.\nZarqa Ali, John Robert Zibert, and Simon Francis Thomsen. 2020. Virtual clinical trials: Perspectives in dermatology.\nDermatology 236, 4 (2020), 375–382.\nGordon W. Allport. 1954. The Nature of Prejudice . Addison-Wesley Publishing Company. https://books.google.com/books?\nid=u94XUyRuDl4C\nLaila Alrajhi, Ahmed Alamri, Filipe Dwan Pereira, and Alexandra I Cristea. 2021. Urgency Analysis of Learners’ Comments:\nAn Automated Intervention Priority Model for MOOC. InInternational Conference on Intelligent Tutoring Systems . Springer,\n148–160.\nSergei Alyamkin, Matthew Ardi, Alexander C Berg, Achille Brighton, Bo Chen, Yiran Chen, Hsin-Pai Cheng, Zichen Fan,\nChen Feng, Bo Fu, et al . 2019. Low-power computer vision: Status, challenges, and opportunities. IEEE Journal on\nEmerging and Selected Topics in Circuits and Systems 9, 2 (2019), 411–421.\nJulia Amann, Alessandro Blasimme, Effy Vayena, Dietmar Frey, and Vince I Madai. 2020. Explainability for artificial\nintelligence in healthcare: a multidisciplinary perspective. BMC Medical Informatics and Decision Making 20, 1 (2020),\n1–9.\nAmerican Bar Association. 2004. Gideon’s Broken Promise: America’s Continuing Quest for Equal Justice . Technical Report.\nAmerican Bar Association.\nAmerican Bar Association. 2021. National Lawyer Population Survey 2021. https://www.americanbar.org/content/dam/\naba/administrative/market_research/2021-national-lawyer-population-survey.pdf\nSaleema Amershi, James Fogarty, and Daniel Weld. 2012. Regroup: Interactive Machine Learning for On-demand Group\nCreation in Social Networks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM.\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. Concrete problems in\nAI safety. arXiv preprint arXiv:1606.06565 (2016).\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton\nvan den Hengel. 2018. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real\nenvironments. In Computer Vision and Pattern Recognition (CVPR) .\nJacob Andreas. 2019. Measuring Compositionality in Representation Learning. International Conference on Learning\nRepresentations (2019).\nOn the Opportunities and Risks of Foundation Models 163\nJacob Andreas. 2020. Good-Enough Compositional Data Augmentation. Association for Computational Linguistics (July\n2020), 7556–7566. https://doi.org/10.18653/v1/2020.acl-main.676\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition . 39–48.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin,\nPieter Abbeel, and Wojciech Zaremba. 2017. Hindsight Experience Replay. arXiv preprint arXiv:1707.01495 (2017).\nJulia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine Bias: There’s software used across the country\nto predict future criminals. And it’s biased against blacks. ProPublica.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. 2021. Large-scale differentially private BERT.\narXiv preprint arXiv:2108.01624 (2021).\nRohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. 2020. Scalable Second Order Optimization for\nDeep Learning. arXiv preprint arXiv:2002.09018 (2020).\nLasse F. Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan. 2020. Carbontracker: Tracking and Predicting the\nCarbon Footprint of Training Deep Learning Models. ICML Workshop on Challenges in Deploying and monitoring\nMachine Learning Systems. arXiv:2007.03051.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\n2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision . 2425–2433.\nMaria Antoniak and David Mimno. 2021. Bad Seeds: Evaluating Lexical Methods for Bias Measurement. In Proceedings of\nACL 2021.\nHannah Arendt. 1987. Collective Responsibility . Springer Netherlands, Dordrecht, 43–50.\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. 2019. Invariant risk minimization. arXiv preprint\narXiv:1907.02893 (2019).\nMichael Armbrust, Armando Fox, Rean Griffith, Anthony D Joseph, Randy H Katz, Andrew Konwinski, Gunho Lee, David A\nPatterson, Ariel Rabkin, Ion Stoica, et al. 2009. Above the clouds: A berkeley view of cloud computing . Technical Report.\nTechnical Report UCB/EECS-2009-28, EECS Department, University of California . . . .\nSanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. 2019a. Implicit regularization in deep matrix factorization. In\nAdvances in Neural Information Processing Systems . 7411–7422.\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. 2019b. A theoretical\nanalysis of contrastive unsupervised representation learning. arXiv preprint arXiv:1902.09229 (2019).\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2016. A latent variable model approach to\nPMI-based word embeddings. Transactions of the Association for Computational Linguistics (2016).\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the Cross-lingual Transferability of Monolingual Representa-\ntions. arXiv:1910.11856 [cs] (May 2020). http://arxiv.org/abs/1910.11856\nEuan A Ashley. 2016. Towards precision medicine. Nature Reviews Genetics 17, 9 (2016), 507–522.\nAmanda Askell, Miles Brundage, and Gillian Hadfield. 2019. The Role of Cooperation in Responsible AI Development.\nhttp://arxiv.org/abs/1907.04534\nDavid H. Autor. 2015. Why Are There Still So Many Jobs? The History and Future of Workplace Automation. Journal of\nEconomic Perspectives 29, 3 (2015), 3–30.\nSebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek.\n2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one 10, 7\n(2015), e0130140.\nClaudine Badue, Rânik Guidolini, Raphael Vivacqua Carneiro, Pedro Azevedo, Vinicius Brito Cardoso, Avelino Forechi,\nLuan Jesus, Rodrigo Berriel, Thiago Meireles Paixão, Filipe Mutz, et al. 2020. Self-driving cars: A survey. Expert Systems\nwith Applications (2020), 113816.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A Framework for Self-Supervised\nLearning of Speech Representations. In Advances in Neural Information Processing Systems , H. Larochelle, M. Ranzato,\nR. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 12449–12460. https://proceedings.neurips.cc/\npaper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473 (2014).\nDzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, S. A. Hosseini, Pushmeet Kohli, and Edward Grefenstette. 2019.\nLearning to Understand Goal Specifications by Modelling Reward. InInternational Conference on Learning Representations\n(ICLR).\nDzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville.\n2018. Systematic Generalization: What Is Required and Can It Be Learned?. In International Conference on Learning\nRepresentations.\nAnnette Baier. 1986. Trust and Antitrust. Ethics 96, 2 (1986), 231–260. http://www.jstor.org/stable/2381376\n164 Center for Research on Foundation Models (CRFM)\nAndrea Bajcsy, Dylan P. Losey, M. O’Malley, and A. Dragan. 2017. Learning Robot Objectives from Physical Human\nInteraction. In Conference on Robot Learning (CORL) .\nBowen Baker, I. Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. 2020. Emergent Tool\nUse From Multi-Agent Autocurricula. ArXiv abs/1909.07528 (2020).\nAnton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. 2019. Phyre: A new benchmark\nfor physical reasoning. Advances in Neural Information Processing Systems 32 (2019), 5082–5093.\nJack Bandy and Nicholas Vincent. 2021. Addressing\" Documentation Debt\" in Machine Learning Research: A Retrospective\nDatasheet for BookCorpus. arXiv preprint arXiv:2105.05241 (2021).\nKshitij Bansal, Sarah M. Loos, Markus N. Rabe, Christian Szegedy, and Stewart Wilcox. 2019. HOList: An Environment for\nMachine Learning of Higher Order Logic Theorem Proving. InProceedings of the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach, California, USA (Proceedings of Machine Learning Research, Vol. 97) ,\nKamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 454–463. http://proceedings.mlr.press/v97/bansal19a.html\nYamini Bansal, Gal Kaplun, and Boaz Barak. 2020. For self-supervised learning, Rationality implies generalization, provably.\narXiv preprint arXiv:2010.08508 (2020).\nElias Bareinboim, Juan D. Correa, Duligur Ibeling, and Thomas Icard. 2020. On Pearl’s Hierarchy and the Foundations of\nCausal Inference. Technical Report R-60. Causal AI Lab, Columbia University. Forthcoming in Probabilistic and Causal\nInference: The Works of Judea Pearl (ACM Books).\nSolon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. 2017. The Problem With Bias: Allocative Versus\nRepresentational Harms in Machine Learning. (2017). Talk at SIGCIS Conference.\nMarco Baroni. 2021. On the proper role of linguistically-oriented deep net analysis in linguistic theorizing. arXiv preprint\narXiv:2106.08694 (2021).\nChristine Basta, Marta R Costa-Jussà, and Noe Casas. 2019. Evaluating the underlying gender bias in contextualized word\nembeddings. arXiv preprint arXiv:1904.08783 (2019).\nPriyam Basu, Tiasa Singha Roy, Rakshit Naidu, Zumrut Muftuoglu, Sahib Singh, and Fatemehsadat Mireshghallah. 2021.\nBenchmarking Differential Privacy and Federated Learning for BERT Models. arXiv preprint arXiv:2106.13973 (2021).\nMary Bates. 2019. Health care chatbots are here to help. IEEE pulse 10, 3 (2019), 12–14.\nSarah Batterbury. 2012. Language justice for Sign Language Peoples: The UN Convention on the Rights of Persons with\nDisabilities. Language Policy 11 (08 2012). https://doi.org/10.1007/s10993-012-9245-8\nHerbert Bay, Tinne Tuytelaars, and Luc Van Gool. 2006. Surf: Speeded up robust features. InEuropean conference on computer\nvision. Springer, 404–417.\nDaniel M Bear, Elias Wang, Damian Mrowca, Felix J Binder, Hsiau-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao,\nKevin Smith, Li Fei-Fei, et al. 2021. Physion: Evaluating Physical Prediction from Vision in Humans and Machines. arXiv\npreprint arXiv:2106.08261 (2021).\nAdam L Beberg, Daniel L Ensign, Guha Jayachandran, Siraj Khaliq, and Vijay S Pande. 2009. Folding@home: Lessons from\neight years of volunteer distributed computing. In 2009 IEEE International Symposium on Parallel & Distributed Processing .\n1–8.\nJ Thaddeus Beck, Melissa Rammage, Gretchen P Jackson, Anita M Preininger, Irene Dankwa-Mullan, M Christopher Roebuck,\nAdam Torres, Helen Holtzen, Sadie E Coverdill, M Paul Williamson, et al. 2020. Artificial intelligence tool for optimizing\neligibility screening for clinical trials in a large community cancer center. JCO clinical cancer informatics 4 (2020), 50–59.\nSara Beery, Elijah Cole, and Arvi Gjoka. 2020. The iWildCam 2020 Competition Dataset. arXiv preprint arXiv:2004.10340\n(2020).\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. 2017. What do Neural Machine Translation\nModels Learn about Morphology?. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) (Vancouver, Canada). Association for Computational Linguistics, 861–872. https:\n//doi.org/10.18653/v1/P17-1080\nKristen Bell, Jenny Hong, Nick McKeown, and Catalin Voss. 2021. The Recon Approach: A New Direction for Machine\nLearning in Criminal Law. Berkeley Technology Law Journal 37 (2021).\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint\narXiv:2004.05150 (2020).\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A\ntheory of learning from different domains. Machine Learning 79, 1 (2010), 151–175.\nKambez H Benam, Siobhan Gilchrist, Andre Kleensang, Ani B Satz, Catherine Willett, and Qiang Zhang. 2019. Exploring\nnew technologies in biomedical research. Drug discovery today 24, 6 (2019), 1242–1247.\nEmily M Bender. 2011. On achieving and evaluating language-independence in NLP.Linguistic Issues in Language Technology\n6, 3 (2011), 1–26.\nEmily M Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system\nbias and enabling better science. Transactions of the Association for Computational Linguistics (TACL) 6 (2018), 587–604.\nOn the Opportunities and Risks of Foundation Models 165\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic\nParrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and\nTransparency (Virtual Event, Canada) (FAccT ’21). Association for Computing Machinery, New York, NY, USA, 610–623.\nhttps://doi.org/10.1145/3442188.3445922\nEmily M Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of\ndata. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . 5185–5198.\nYoshua Bengio, Andrea Lodi, and Antoine Prouvost. 2021. Machine learning for combinatorial optimization: A methodological\ntour d’horizon. Eur. J. Oper. Res. 290, 2 (2021), 405–421. https://doi.org/10.1016/j.ejor.2020.07.063\nRuha Benjamin. 2016. Informed Refusal: Toward a Justicebased Bioethics. Science, Technology, & Human Values (June 2016),\n967–990.\nRuha Benjamin. 2019. Race after Technology . Polity Press.\nTristan Bepler and Bonnie Berger. 2021. Learning the protein language: Evolution, structure, and function. Cell Systems 12,\n6 (2021), 654–669.\nKaustav Bera, Kurt A Schalper, David L Rimm, Vamsidhar Velcheti, and Anant Madabhushi. 2019. Artificial intelligence\nin digital pathology—new tools for diagnosis and precision oncology. Nature Reviews Clinical Oncology 16, 11 (2019),\n703–715.\nElika Bergelson and Daniel Swingley. 2012. At 6–9 months, human infants know the meanings of many common nouns.\nProceedings of the National Academy of Sciences 109, 9 (2012), 3253–3258.\nRichard Berk, Hoda Heidari, Shahin Jabbari, Michael Kearns, and Aaron Roth. 2021. Fairness in Criminal Justice Risk Assess-\nments: The State of the Art. Sociological Methods & Research 50, 1 (2021), 3–44. https://doi.org/10.1177/0049124118782533\nFelix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. 2017. Safe Model-based Reinforcement\nLearning with Stability Guarantees. In Advances in Neural Information Processing Systems (NeurIPS) .\nMichael S. Bernstein, Margaret Levi, David Magnus, Betsy Rajala, Debra Satz, and Charla Waeiss. 2021. ESR: Ethics and\nSociety Review of Artificial Intelligence Research. http://arxiv.org/abs/2106.11521\nKathryn D Betts and Kyle R Jaep. 2017. The dawn of fully automated contract drafting: Machine learning breathes new life\ninto a decades-old promise. Duke L. & Tech. Rev. 15 (2017), 216.\nUrmil Bharti, Deepali Bajaj, Hunar Batra, Shreya Lalit, Shweta Lalit, and Aayushi Gangwani. 2020. Medbot: Conversational\nartificial intelligence powered chatbot for delivering tele-health after covid-19. In 2020 5th International Conference on\nCommunication and Electronics Systems (ICCES) . IEEE, 870–875.\nI. Biederman. 1972. Perceiving real-world scenes. Science (New York, N.Y.) 177, 4043 (July 1972), 77–80. https://doi.org/10.\n1126/science.177.4043.77\nBattista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndić, Pavel Laskov, Giorgio Giacinto, and Fabio\nRoli. 2013. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and\nknowledge discovery in databases . 387–402.\nBattista Biggio, Blaine Nelson, and Pavel Laskov. 2012. Poisoning Attacks Against Support Vector Machines. InInternational\nConference on Machine Learning (ICML) . 1467–1474.\nAbeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2020. The Underlying Values\nof Machine Learning Research. (2020). https://drive.google.com/file/d/1tjrm3Bf1hxV8iuPSiCcM1IazITGp-GZj/view\nChristopher M. Bishop. 2006. Pattern Recognition and Machine Learning . Springer.\nErdem Biyik and Dorsa Sadigh. 2018. Batch Active Preference-Based Learning of Reward Functions. In Conference on Robot\nLearning (CORL).\nMikołaj Bińkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. 2018. Demystifying MMD GANs.\narXiv:1801.01401 [stat.ML]\nGuy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. 2019. Implicit regularization for deep neural networks driven by\nan Ornstein-Uhlenbeck like process. arXiv preprint arXiv:1904.09080 (2019).\nJohn Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain Adaptation with Structural Correspondence Learning.\nIn Empirical Methods in Natural Language Processing (EMNLP) .\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (Technology) is Power: A Critical Survey\nof “Bias” in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, 5454–5476. https://doi.org/10.18653/v1/2020.acl-main.485\nSu Lin Blodgett, Lisa Green, and Brendan O’Connor. 2016. Demographic Dialectal Variation in Social Media: A Case Study\nof African-American English. In Empirical Methods in Natural Language Processing (EMNLP) . 1119–1130.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. 2021. Stereotyping Norwegian Salmon:\nAn Inventory of Pitfalls in Fairness Benchmark Datasets. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers). Association for Computational Linguistics, Online, 1004–1015. https://doi.org/10.18653/v1/2021.acl-long.81\n166 Center for Research on Foundation Models (CRFM)\nSu Lin Blodgett and Brendan O’Connor. 2017. Racial Disparity in Natural Language Processing: A Case Study of Social\nMedia African-American English. In Fairness, Accountability, and Transparency in Machine Learning (FAT/ML) Workshop,\nKDD.\nThomas Bock. 2007. Construction robotics. Autonomous Robots 22, 3 (2007), 201–209.\nJeannette Bohg, Karol Hausman, Bharath Sankaran, Oliver Brock, Danica Kragic, Stefan Schaal, and Gaurav S Sukhatme.\n2017. Interactive perception: Leveraging action in perception and perception in action. IEEE Transactions on Robotics 33,\n6 (2017), 1273–1291.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to Computer\nProgrammer as Woman is to Homemaker? Debiasing Word Embeddings. InAdvances in Neural Information Processing\nSystems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (Eds.), Vol. 29. Curran Associates, Inc. https:\n//proceedings.neurips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf\nMichael J Bommarito, Daniel Martin Katz, and Eric M Detterman. 2018. LexNLP: Natural language processing and information\nextraction for legal and regulatory texts. arXiv:1806.03688 [cs.CL]\nRishi Bommasani, Kelly Davis, and Claire Cardie. 2020. Interpreting Pretrained Contextualized Representations via\nReductions to Static Embeddings. InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Online, 4758–4781. https://doi.org/10.18653/v1/2020.acl-main.431\nRishi Bommasani, Steven Wu, and Xanda Schofield. 2019. Towards Private Synthetic Text Generation. In NeurIPS 2019\nMachine Learning with Guarantees Workshop .\nPaul Boniol, George Panagopoulos, Christos Xypolopoulos, Rajaa El Hamdani, David Restrepo Amariles, and Michalis\nVazirgiannis. 2020. Performance in the courtroom: Automated processing and visualization of appeal court decisions in\nfrance. arXiv preprint arXiv:2006.06251 (2020).\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den\nDriessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2021. Improving Language Models by Retrieving\nfrom Trillions of Tokens. arXiv preprint arXiv:2112.04426 (2021).\nAntoine Bosselut, Omer Levy, Ari Holtzman, C. Ennis, D. Fox, and Yejin Choi. 2018. Simulating Action Dynamics with\nNeural Process Networks. (2018).\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, A. Çelikyilmaz, and Yejin Choi. 2019. COMET:\nCommonsense Transformers for Automatic Knowledge Graph Construction. In ACL.\nNick Bostrom and Milan M Cirkovic. 2011. Global catastrophic risks . Oxford University Press.\nLéon Bottou. 2014. From machine learning to machine reasoning. Machine learning 94, 2 (2014), 133–149.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David\nLie, and Nicolas Papernot. 2019. Machine unlearning. arXiv preprint arXiv:1912.03817 (2019).\nWilliam G Bowen. 2012. The “cost disease” in higher education: is technology the answer? The Tanner Lectures Stanford\nUniversity (2012).\nSamuel R. Bowman and George Dahl. 2021. What Will it Take to Fix Benchmarking in Natural Language Understanding?. In\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies. Association for Computational Linguistics, Online, 4843–4855. https://doi.org/10.18653/v1/2021.\nnaacl-main.385\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula,\nAdam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: Composable Transformations of\nPython+NumPy Programs. http://github.com/google/jax\nJonathan Bragg, Arman Cohan, Kyle Lo, and Iz Beltagy. 2021. FLEX: Unifying Evaluation for Few-Shot NLP. ArXiv\nabs/2107.07170 (2021).\nGwern Branwen. 2020. GPT-3 creative fiction. (2020).\nLuke Breitfeller, Emily Ahn, David Jurgens, and Yulia Tsvetkov. 2019. Finding Microaggressions in the Wild: A Case\nfor Locating Elusive Phenomena in Social Media Posts. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) .\nAssociation for Computational Linguistics, Hong Kong, China, 1664–1674. https://doi.org/10.18653/v1/D19-1176\nRaymond H Brescia, Walter McCarthy, Ashley McDonald, Kellan Potts, and Cassandra Rivais. 2014. Embracing disruption:\nHow technological change in the delivery of legal services can improve access to justice. Alb. L. Rev. 78 (2014), 553.\nTimothy F. Bresnahan and M. Trajtenberg. 1995. General purpose technologies ’Engines of growth’? Journal of Econometrics\n65, 1 (1995), 83–108. https://doi.org/10.1016/0304-4076(94)01598-T\nMarilynn B. Brewer and William D. Crano. 2014. Research Design and Issues of Validity (2 ed.). Cambridge University Press,\n11–26. https://doi.org/10.1017/CBO9780511996481.005\nLiam Kofi Bright, Daniel Malinsky, and Morgan Thompson. 2016. Causally Interpreting Intersectionality Theory. Philosophy\nof Science 83, 1 (Jan. 2016), 60–81. https://doi.org/10.1086/684173\nOn the Opportunities and Risks of Foundation Models 167\nAndrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large Scale GAN Training for High Fidelity Natural Image\nSynthesis. In International Conference on Learning Representations .\nMatt Brockman. 2020. Math - GPT_Prompts. http://gptprompts.wikidot.com/logic:math#toc5\nUrie Bronfenbrenner. 1977. Toward an Experimental Ecology of Human Development. American Psychologist 32 (1977),\n513–531.\nR. Brooks. 2002. Flesh and Machines: How Robots Will Change Us.\nHannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tramèr. 2022. What Does it Mean for\na Language Model to Preserve Privacy? arXiv preprint arXiv:2202.05520 (2022).\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. 2020. Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165 (2020).\nMiles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas\nZeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó\nhÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page,\nJoanna Bryson, Roman Yampolskiy, and Dario Amodei. 2018. The Malicious Use of Artificial Intelligence: Forecasting,\nPrevention, and Mitigation. arXiv:1802.07228 [cs.AI]\nErik Brynjolfsson. 2022. The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence. Daedalus, Forthcoming\n(2022).\nErik Brynjolfsson and Avinash Collis. 2019. How should we measure the digital economy? Focus on the value created, not\njust the prices paid. Harvard business review 97, 6 (2019), 140–.\nErik Brynjolfsson, Xiang Hui, and Meng Liu. 2019. Does Machine Translation Affect International Trade? Evidence from a\nLarge Digital Platform. Management Science 65, 12 (dec 2019), 5449–5460. https://doi.org/10.1287/mnsc.2019.3388\nErik Brynjolfsson and Andrew McAfee. 2011. Race against the Machine.\nErik Brynjolfsson and Tom Mitchell. 2017. What can machine learning do? Workforce implications. Science 358, 6370 (2017),\n1530–1534.\nSébastien Bubeck and Mark Sellke. 2021. A Universal Law of Robustness via Isoperimetry. arXiv preprint arXiv:2105.12806\n(2021).\nBen Buchanan, Andrew Lohn, Micah Musser, and Katerina Sedova. 2021. Truth, Lies, and Automation: How Language Models\nCould Change Disinformation . Center for Security and Emerging Technology. https://doi.org/10.51593/2021CA003\nJoy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classifica-\ntion. In Conference on Fairness, Accountability and Transparency . 77–91.\nChristopher Burr, Nello Cristianini, and James Ladyman. 2018. An analysis of the interaction between intelligent software\nagents and human users. Minds and machines 28, 4 (2018), 735–774.\nJenna Burrell. 2016. How the machine ‘thinks’: Understanding opacity in machine learning algorithms. Big Data & Society\n3, 1 (Jan. 2016), 205395171562251. https://doi.org/10.1177/2053951715622512\nDaniel Buschek, Martin Zurn, and Malin Eiband. 2021. The Impact of Multiple Parallel Phrase Suggestions on Email Input\nand Composition Behaviour of Native and Non-Native English Writers. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems .\nJames E Cabral, Abhijeet Chavan, Thomas M Clarke, and John Greacen. 2012. Using technology to enhance access to justice.\nHarv. JL & Tech. 26 (2012), 241.\nTianle Cai, Ruiqi Gao, Jason D Lee, and Qi Lei. 2021. A Theory of Label Propagation for Subpopulation Shift. arXiv preprint\narXiv:2102.11203 (2021).\nAylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language\ncorpora contain human-like biases. Science 356, 6334 (2017), 183–186. https://doi.org/10.1126/science.aal4230\narXiv:https://science.sciencemag.org/content/356/6334/183.full.pdf\nRyan Calo and Danielle K. Citron. 2021. The Automated Administrative State: A Crisis of Legitimacy. Emory Law Journal\n40 (2021). Issue 4. https://scholarlycommons.law.emory.edu/elj/vol70/iss4/1\nDiogo M Camacho, Katherine M Collins, Rani K Powers, James C Costello, and James J Collins. 2018. Next-generation\nmachine learning for biological networks. Cell 173, 7 (2018), 1581–1592.\nNick Cammarata, Shan Carter, Gabriel Goh, Chris Olah, Michael Petrov, Ludwig Schubert, Chelsea Voss, Ben Egan, and\nSwee Kiat Lim. 2020. Thread: Circuits. Distill (2020). https://doi.org/10.23915/distill.00024 https://distill.pub/2020/circuits.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing Factual Knowledge in Language Models. arXiv:2104.08164 [cs.CL]\nQingqing Cao, Aruna Balasubramanian, and Niranjan Balasubramanian. 2020. Towards accurate and reliable energy\nmeasurement of NLP models. arXiv preprint arXiv:2010.05248 (2020).\n168 Center for Research on Foundation Models (CRFM)\nSteven Cao, Nikita Kitaev, and Dan Klein. 2019. Multilingual Alignment of Contextual Word Representations. InInternational\nConference on Learning Representations .\nYinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with machine unlearning. In 2015 IEEE Symposium on\nSecurity and Privacy . IEEE, 463–480.\nYang Trista Cao and Hal Daumé III. 2020. Toward Gender-Inclusive Coreference Resolution. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online,\n4568–4595. https://doi.org/10.18653/v1/2020.acl-main.418\nNicholas Carlini. 2021. Poisoning the Unlabeled Dataset of Semi-Supervised Learning. In USENIX Security Symposium .\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. 2019. The secret sharer: Evaluating and testing\nunintended memorization in neural networks. In USENIX Security Symposium . 267–284.\nNicholas Carlini and Andreas Terzis. 2021. Poisoning and Backdooring Contrastive Learning.arXiv preprint arXiv:2106.09667\n(2021).\nNicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom\nBrown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting Training Data from Large Language\nModels. In USENIX Security Symposium .\nYair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C. Duchi. 2019. Unlabeled Data Improves\nAdversarial Robustness. In Advances in Neural Information Processing Systems (NeurIPS) .\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv’e J’egou, J. Mairal, Piotr Bojanowski, and Armand Joulin. 2021. Emerging\nProperties in Self-Supervised Vision Transformers. ArXiv abs/2104.14294 (2021).\nRodrigo Carril and Mark Duggan. 2020. The impact of industry consolidation on government procurement: Evidence from\nDepartment of Defense contracting. Journal of Public Economics 184 (2020), 104141. https://doi.org/10.1016/j.jpubeco.\n2020.104141\nJimmy Carter. 1978. Excerpts From Carter’s Speech to the Bar Association. The New York Times (May 1978).\nShan Carter, Zan Armstrong, Ludwig Schubert, Ian Johnson, and Chris Olah. 2019. Activation atlas. Distill 4, 3 (2019), e15.\nIsaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant\nSubramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoît Sagot,\nClara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Javier Ortiz Suárez, Iroro Orife, Kelechi Ogueji,\nRubungo Andre Niyongabo, Toan Q. Nguyen, Mathias Müller, André Müller, Shamsuddeen Hassan Muhammad, Nanda\nMuhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha\nKudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\nSakine Çabuk Ballı, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe\nAzime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa\nAdeyemi. 2021. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. ArXiv abs/2103.12028 (2021).\nhttps://arxiv.org/abs/2103.12028\nRahma Chaabouni, Roberto Dessì, and Eugene Kharitonov. 2021. Can Transformers Jump Around Right in Natural Language?\nAssessing Performance Transfer from SCAN. arXiv preprint arXiv:2107.01366 (2021).\nKrishna Chaitanya, Ertunc Erdil, Neerav Karani, and Ender Konukoglu. 2020. Contrastive learning of global and local\nfeatures for medical image segmentation with limited annotations. Advances in Neural Information Processing Systems 33\n(2020).\nBenjamin Chaix, Jean-Emmanuel Bibault, Arthur Pienkowski, Guillaume Delamon, Arthur Guillemassé, Pierre Nectoux,\nand Benoît Brouard. 2019. When chatbots meet patients: one-year prospective study of conversations between patients\nwith breast cancer and a chatbot. JMIR cancer 5, 1 (2019), e12856.\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019. Neural legal judgment prediction in English.arXiv preprint\narXiv:1906.02059 (2019).\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. LEGAL-BERT:\nThe muppets straight out of law school. arXiv preprint arXiv:2010.02559 (2020).\nRobert Challen, Joshua Denny, Martin Pitt, Luke Gompels, Tom Edwards, and Krasimira Tsaneva-Atanasova. 2019. Artificial\nintelligence, bias and clinical safety. BMJ Quality & Safety 28, 3 (2019), 231–237.\nMahawaga Arachchige Pathum Chamikara, Peter Bertok, Ibrahim Khalil, Dongxi Liu, and Seyit Camtepe. 2021. Privacy\npreserving distributed machine learning with federated learning. Computer Communications 171 (2021), 112–125.\nMuthu Kumar Chandrasekaran and Min-Yen Kan. 2019. When to reply? context sensitive models to predict instructor\ninterventions in mooc forums. arXiv preprint arXiv:1905.10851 (2019).\nEshwar Chandrasekharan, Mattia Samory, Shagun Jhaver, Hunter Charvat, Amy Bruckman, Cliff Lampe, Jacob Eisenstein, and\nEric Gilbert. 2018. The Internet’s Hidden Rules: An Empirical Study of Reddit Norm Violations at Micro, Meso, and Macro\nScales. Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 32 (Nov. 2018), 25 pages. https://doi.org/10.1145/3274301\nAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng,\nand Yinda Zhang. 2017. Matterport3D: Learning from RGB-D Data in Indoor Environments. International Conference on\nOn the Opportunities and Risks of Foundation Models 169\n3D Vision (3DV) (2017).\nHongyan Chang, Ta Duy Nguyen, Sasi Kumar Murakonda, Ehsan Kazemi, and Reza Shokri. 2020. On adversarial bias and\nthe robustness of fair machine learning. arXiv preprint arXiv:2006.08669 (2020).\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Conceptual 12M: Pushing Web-Scale Image-Text\nPre-Training To Recognize Long-Tail Visual Concepts. InCVPR.\nYatin Chaudhary, Pankaj Gupta, Khushbu Saxena, Vivek Kulkarni, Thomas Runkler, and Hinrich Schütze. 2020. TopicBERT\nfor energy efficient document classification. arXiv preprint arXiv:2010.16407 (2020).\nAnnie S. Chen, Suraj Nair, and Chelsea Finn. 2021c. Learning Generalizable Robotic Reward Functions from \"In-The-Wild\"\nHuman Videos. In Robotics: Science and Systems (RSS) .\nAnnie S Chen, HyunJi Nam, Suraj Nair, and Chelsea Finn. 2021d. Batch exploration with examples for scalable robotic\nreinforcement learning. IEEE Robotics and Automation Letters 6, 3 (2021), 4401–4408.\nChaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin. 2018. This looks like that: deep\nlearning for interpretable image recognition. arXiv preprint arXiv:1806.10574 (2018).\nIrene Y Chen, Shalmali Joshi, and Marzyeh Ghassemi. 2020b. Treating health disparities with artificial intelligence. Nature\nmedicine 26, 1 (2020), 16–17.\nIrene Y Chen, Peter Szolovits, and Marzyeh Ghassemi. 2019. Can AI help reduce disparities in general medical and mental\nhealth care? AMA journal of ethics 21, 2 (2019), 167–179.\nLiang Chen, Peter Edwards, John D Nelson, and Timothy J Norman. 2015a. An access control model for protecting\nprovenance graphs. In 2015 13th Annual Conference on Privacy, Security and Trust (PST) . IEEE, 125–132.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, M. Laskin, P. Abbeel, A. Srinivas, and Igor Mordatch.\n2021b. Decision Transformer: Reinforcement Learning via Sequence Modeling. ArXiv abs/2106.01345 (2021).\nMayee Chen, Karan Goel, Nimit S Sohoni, Fait Poms, Kayvon Fatahalian, and Christopher Ré. 2021a. Mandoline: Model\nEvaluation under Distribution Shift. In International Conference on Machine Learning . PMLR, 1617–1629.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. 2020d. Generative\nPretraining From Pixels. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine\nLearning Research, Vol. 119) , Hal Daumé III and Aarti Singh (Eds.). PMLR, 1691–1703. http://proceedings.mlr.press/v119/\nchen20s.html\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,\nIgor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter\nWelinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021f. Evaluating Large\nLanguage Models Trained on Code. arXiv:2107.03374 [cs.LG]\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020c. A simple framework for contrastive learning\nof visual representations. In International Conference on Machine Learning (ICML) . 1597–1607.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training Deep Nets with Sublinear Memory Cost. CoRR\nabs/1604.06174 (2016). arXiv:1604.06174 http://arxiv.org/abs/1604.06174\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. 2020a. Improved Baselines with Momentum Contrastive Learning.\narXiv preprint arXiv:2003.04297 (2020).\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. 2015b.\nMicrosoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015).\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted backdoor attacks on deep learning systems\nusing data poisoning. arXiv preprint arXiv:1712.05526 (2017).\nYun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer,\nand Raquel Urtasun. 2021e. GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 7230–7240.\nJames Cheney, Laura Chiticariu, and Wang-Chiew Tan. 2009.Provenance in databases: Why, how, and where . Now Publishers\nInc.\nEthan A Chi, John Hewitt, and Christopher D Manning. 2020. Finding Universal Grammatical Relations in Multilingual\nBERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . 5564–5577.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating Long Sequences with Sparse Transformers.\narXiv preprint arXiv:1904.10509 (2019).\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying vision-and-language tasks via text generation. InInternational\nConference on Machine Learning (ICML) .\n170 Center for Research on Foundation Models (CRFM)\nRochelle Choenni and Ekaterina Shutova. 2020. Cross-neutralising: Probing for joint encoding of linguistic information in\nmultilingual models. arXiv preprint arXiv:2010.12825 (2020).\nAlex Chohlas-Wood, Joe Nudell, Zhiyuan Jerry Lin, Julian Nyarko, and Sharad Goel. 2020. Blind justice: Algorithmically\nmasking race in charging decisions . Technical Report. Technical report.\nJonathan H Choi. 2020. An Empirical Study of Statutory Interpretation in Tax Law. NYUL Rev. 95 (2020), 363.\nNoam Chomsky. 2014. The minimalist program . MIT press.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins,\nJared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al . 2020. Rethinking attention with performers. arXiv preprint\narXiv:2009.14794 (2020).\nAlexandra Chouldechova and Aaron Roth. 2020. A snapshot of the frontiers of fairness in machine learning. Commun. ACM\n63, 5 (April 2020), 82–89. https://doi.org/10.1145/3376898\nYinlam Chow, Ofir Nachum, Edgar A. Duéñez-Guzmán, and M. Ghavamzadeh. 2018. A Lyapunov-based Approach to Safe\nReinforcement Learning. In Advances in Neural Information Processing Systems (NeurIPS) .\nPaul Christiano. 2016. Prosaic AI Alignment. https://ai-alignment.com/prosaic-ai-control-b959644d79c2\nYeounoh Chung, Tim Kraska, Neoklis Polyzotis, Ki Hyun Tae, and Steven Euijong Whang. 2019. Slice finder: Automated\ndata slicing for model validation. In2019 IEEE 35th International Conference on Data Engineering (ICDE) . IEEE, 1550–1553.\nCigna. 2018. Cigna U.S. Loneliness Index . https://www.cigna.com/assets/docs/newsroom/loneliness-survey-2018-full-\nreport.pdf\nDanielle Keats Citron. 2008. Technological Due Process. Wash. U. L. Rev. 1249 (2008). https://openscholarship.wustl.edu/\nlaw_lawreview/vol85/iss6/2/\nElizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A Smith. 2021. All That’s\n’Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text. arXiv preprint arXiv:2107.00061 (2021).\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: Pre-training Text Encoders as\nDiscriminators Rather Than Generators. In International Conference on Learning Representations .\nPeter Clark, Oren Etzioni, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal,\nCarissa Schoenick, Oyvind Tafjord, Niket Tandon, Sumithra Bhakthavatsalam, Dirk Groeneveld, Michal Guerquin, and\nMichael Schmitz. 2019. From ’F’ to ’A’ on the N.Y. Regents Science Exams: An Overview of the Aristo Project. CoRR\nabs/1909.01958 (2019). http://arxiv.org/abs/1909.01958\nJohn D. Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, John DeNero, Pieter Abbeel, and Sergey Levine. 2019.\nGuiding Policies with Language via Meta-Learning. In International Conference on Learning Representations (ICLR) .\nCary Coglianese and Lavi Ben Dor. 2020. AI in Adjudication and Administration. Brooklyn Law Review, Forthcoming,\nUniversity of Pennsylvania Law School, Public Law Research Paper 19-41 (2020).\nElijah Cole, Xuan Yang, Kimberly Wilber, Oisin Mac Aodha, and Serge Belongie. 2021. When Does Contrastive Visual\nRepresentation Learning Work? arXiv:2105.05837 [cs.CV]\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis, Kunle Olukotun, Chris\nRé, and Matei Zaharia. 2017. DAWNBench: An End-to-End Deep Learning Benchmark and Competition. In NeurIPS\nWorkshop on Systems for Machine Learning .\nFrancis S Collins and Harold Varmus. 2015. A new initiative on precision medicine. New England Journal of Medicine 372, 9\n(2015), 793–795.\nRonan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks\nwith multitask learning. In International Conference on Machine Learning (ICML) . 160–167.\nCristina Colonnesi, Geert Jan JM Stams, Irene Koster, and Marc J Noom. 2010. The relation between pointing and language\ndevelopment: A meta-analysis. Developmental Review 30, 4 (2010), 352–366.\nBernard Comrie. 1989. Language universals and linguistic typology: Syntax and morphology . University of Chicago press.\nAubrey Condor, Max Litster, and Zachary Pardos. 2021. Automatic short answer grading with SBERT on out-of-sample\nquestions. In Proceedings of the 14th International Conference on Educational Data Mining .\nAlexis Conneau, Germán Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. 2018. What you can cram\ninto a single \\$&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Melbourne, Australia). Association for\nComputational Linguistics, 2126–2136. http://aclweb.org/anthology/P18-1198\nCarol McDonald Connor. 2019. Using technology and assessment to personalize instruction: Preventing reading problems.\nPrevention Science 20, 1 (2019), 89–99.\nErik Conser, Kennedy Hahn, Chandler M Watson, and Melanie Mitchell. 2019. Revisiting Visual Grounding. arXiv preprint\narXiv:1904.02225 (2019).\nAlan Cooper, Robert Reimann, David Cronin, and Christopher Noessel. 2014. About face: the essentials of interaction design.\nIn John Wiley & Sons .\nOn the Opportunities and Risks of Foundation Models 171\nSam Corbett-Davies and Sharad Goel. 2018. The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine\nLearning. arXiv:1808.00023 [cs.CY]\nAnthony Corso, Robert J. Moss, Mark Koren, R. Lee, and Mykel J. Kochenderfer. 2020. A Survey of Algorithms for Black-Box\nSafety Validation. arXiv preprint arXiv:2005.02979 (2020).\nAjeya Cotra. 2021. The case for aligning narrowly superhuman models. https://www.alignmentforum.org/posts/\nPZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models\nKate Crawford. 2017. The Problem With Bias. (2017). Keynote at NeurIPS.\nKate Crawford. 2021. Atlas of AI . Yale University Press.\nKathleen Creel and Deborah Hellman. 2021. The Algorithmic Leviathan: Arbitrariness, Fairness, and Opportunity in\nAlgorithmic Decision Making Systems. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and\nTransparency (Virtual Event, Canada) (FAccT ’21). Association for Computing Machinery, New York, NY, USA, 816.\nhttps://doi.org/10.1145/3442188.3445942\nKathleen A. Creel. 2020. Transparency in Complex Computational Systems. Philosophy of Science 87, 4 (Oct. 2020), 568–589.\nhttps://doi.org/10.1086/709729\nKimberlé Crenshaw. 1989. Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination\ndoctrine, feminist theory and antiracist politics. University of Chicago Legal Forum Vol.1989, Article 8 (1989). https:\n//chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1052&context=uclf\nWilliam Croft. 2001. Radical construction grammar: Syntactic theory in typological perspective . Oxford University Press on\nDemand.\nRebecca Crootof. 2019. Artificial Intelligence Research Needs Responsible Publication Norms. Lawfare.\nPhilippe Cudré-Mauroux, Hideaki Kimura, K-T Lim, Jennie Rogers, Roman Simakov, Emad Soroush, Pavel Velikhov, Daniel L\nWang, Magdalena Balazinska, Jacek Becla, et al. 2009. A demonstration of SciDB: a science-oriented DBMS. Proceedings\nof the VLDB Endowment 2, 2 (2009), 1534–1537.\nYuqing Cui. 2018. Application Of Zero-Knowledge Proof In Resolving Disputes Of Privileged Documents In E-Discovery.\nHarv. JL & Tech. 32 (2018), 633.\nMariano-Florentino Cuéllar. 2019. How to ensure equal access to the law when we speak 200 different languages.CalMatters\n(Feb 2019). https://law.stanford.edu/2019/02/05/how-to-ensure-equal-access-to-the-law-when-we-speak-200-different-\nlanguages/\nAndrew M. Dai and Quoc V. Le. 2015. Semi-supervised sequence learning. In Advances in Neural Information Processing\nSystems (NeurIPS) .\nDamai Dai, Li Dong, Y. Hao, Zhifang Sui, and Furu Wei. 2021a. Knowledge Neurons in Pretrained Transformers. ArXiv\nabs/2104.08696 (2021).\nDengxin Dai and Luc Van Gool. 2018. Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime.\nIn 2018 21st International Conference on Intelligent Transportation Systems (ITSC) . 3819–3824. https://doi.org/10.1109/\nITSC.2018.8569387\nJessica Dai, Sina Fazelpour, and Zachary Lipton. 2021b. Fair Machine Learning Under Partial Compliance. In Proceedings of\nthe 2021 AAAI/ACM Conference on AI, Ethics, and Society . ACM. https://doi.org/10.1145/3461702.3462521\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL:\nAttentive Language Models beyond a Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics . Association for Computational Linguistics, Florence, Italy, 2978–2988. https://doi.org/10.\n18653/v1/P19-1285\nDima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. 2018. Scaling Egocentric Vision: The EPIC-\nKITCHENS Dataset. In European Conference on Computer Vision (ECCV) .\nAlex Damian, Tengyu Ma, and Jason Lee. 2021. Label Noise SGD Provably Prefers Flat Global Minimizers.\narXiv:2106.06530 [cs.LG]\nJeanne E Daniel, Willie Brink, Ryan Eloff, and Charles Copley. 2019. Towards automating healthcare question answering in\na noisy multilingual low-resource setting. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics. 948–953.\nDavid Danks. 2019. The Value of Trustworthy AI. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society .\nACM. https://doi.org/10.1145/3306618.3314228\nTri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Ré. 2019. A kernel theory of modern\ndata augmentation. In International Conference on Machine Learning . PMLR, 1528–1537.\nHelana Darwin. 2017. Doing Gender Beyond the Binary: A Virtual Ethnography. Symbolic Interaction 40, 3 (2017), 317–334.\nhttps://doi.org/10.1002/symb.316 arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/symb.316\nSudeep Dasari, F. Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, K. Schmeckpeper, Siddharth Singh, Sergey Levine, and\nChelsea Finn. 2019. RoboNet: Large-Scale Multi-Robot Learning. In Conference on Robot Learning (CORL) .\n172 Center for Research on Foundation Models (CRFM)\nHal Daumé III. 2007. Frustratingly Easy Domain Adaptation. In Association for Computational Linguistics (ACL) .\nThomas Davenport and Ravi Kalakota. 2019. The potential for artificial intelligence in healthcare. Future healthcare journal\n6, 2 (2019), 94.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing Factual Knowledge in Language Models. arXiv preprint\narXiv:2104.08164 (2021).\nHarm de Vries, Dzmitry Bahdanau, and Christopher D. Manning. 2020. Towards Ecologically Valid Research on Language\nUser Interfaces. arXiv preprint arXiv:2007.14435 (2020).\nMatthew DeCamp and Charlotta Lindvall. 2020. Latent bias and the implementation of artificial intelligence in medicine.\nJournal of the American Medical Informatics Association 27, 12 (June 2020), 2020–2023. https://doi.org/10.1093/jamia/\nocaa094\nMostafa Dehghani, Yi Tay, Alexey Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler, and Oriol Vinyals.\n2021. The Benchmark Lottery. ArXiv abs/2107.07002 (2021).\nMauricio Delbracio, Damien Kelly, Michael S Brown, and Peyman Milanfar. 2021. Mobile Computational Photography: A\nTour. arXiv preprint arXiv:2102.09000 (2021).\nDina Demner-Fushman, Yassine Mrabet, and Asma Ben Abacha. 2020. Consumer health information and question answering:\nhelping consumers find answers to their health-related information needs. Journal of the American Medical Informatics\nAssociation 27, 2 (2020), 194–201.\nDorottya Demszky, Jing Liu, Zid Mancenido, Julie Cohen, Heather Hill, Dan Jurafsky, and Tatsunori Hashimoto. 2021.\nMeasuring Conversational Uptake: A Case Study on Student-Teacher Interactions.Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics (ACL) (2021).\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image\ndatabase. In Computer Vision and Pattern Recognition (CVPR) . 248–255.\nEducation Department of Health and Welfare. 1979. The Belmont Report.\nTim Dettmers and Luke Zettlemoyer. 2019. Sparse Networks from Scratch: Faster Training Without Losing Performance.\narXiv preprint arXiv:1907.04840 (2019).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Association for Computational Linguistics (ACL) . 4171–4186.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2021.\nTime-Aware Language Models as Temporal Knowledge Bases. arXiv:2106.15110 [cs.CL]\nM Diana and JJBJoS Marescaux. 2015. Robotic surgery. Journal of British Surgery 102, 2 (2015), e15–e28.\nGregory M Dickinson. 2018. A Computational Analysis of Oral Argument in the Supreme Court. Cornell JL & Pub. Pol’y 28\n(2018), 449.\nThomas G Dietterich. 2000. Ensemble methods in machine learning. In International workshop on multiple classifier systems .\nSpringer, 1–15.\nCatherine D’Ignazio and Lauren F. Klein. 2020. Data Feminism . MIT Press.\nR. Dillmann. 2004. Teaching and learning of robot tasks via observation of human performance. Robotics Auton. Syst. 47\n(2004), 109–116.\nBotty Dimanov, Umang Bhatt, Mateja Jamnik, and Adrian Weller. 2020. You Shouldn’t Trust Me: Learning Models Which\nConceal Unfairness From Multiple Explanation Methods.. In SafeAI@ AAAI.\nEmily Dinan, Gavin Abercrombie, A. Stevie Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena Rieser.\n2021. Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling. ArXiv abs/2107.03451 (2021).\nhttps://arxiv.org/abs/2107.03451\nEmily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, and Adina Williams. 2020. Multi-Dimensional Gender\nBias Classification. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\nAssociation for Computational Linguistics, Online, 314–331. https://doi.org/10.18653/v1/2020.emnlp-main.23\nEmily Dinan, Samuel Humeau, Bharath Chintagunta, and J. Weston. 2019. Build it Break it Fix it for Dialogue Safety:\nRobustness from Adversarial Human Attack. In EMNLP/IJCNLP.\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia\nYang, and Jie Tang. 2021. CogView: Mastering Text-to-Image Generation via Transformers. CoRR abs/2105.13290 (2021).\narXiv:2105.13290 https://arxiv.org/abs/2105.13290\nYiming Ding, Carlos Florensa, Mariano Phielipp, and P. Abbeel. 2019. Goal-conditioned Imitation Learning. In Advances in\nNeural Information Processing Systems (NeurIPS) .\nLaurent Dinh, David Krueger, and Yoshua Bengio. 2015. NICE: Non-linear Independent Components Estimation.\narXiv:1410.8516 [cs.LG]\nRenée DiResta and Shelby Grossman. 2019. Potemkin Pages & Personas: Assessing GRU Online Operations, 2014-2019.\nRenée DiResta, Shelby Grossman, and Alexandra Siegel. 2021. In-House Vs. Outsourced Trolls: How Digital Mercenaries\nShape State Influence Strategies. Political Communication (2021), 1–31.\nOn the Opportunities and Risks of Foundation Models 173\nRenée DiResta, K. Shaffer, Becky Ruppel, David Sullivan, Robert C. Matney, Ryan Fox, Jonathan Albright, and Ben Johnson.\n2018. The tactics & tropes of the Internet Research Agency. https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=\n1003&context=senatedocs\nMichael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Quentin Lhoest, Anton Sinitsin, Dmitry Popov, Dmitry\nPyrkin, Maxim Kashirin, Alexander Borzunov, et al. 2021. Distributed Deep Learning in Open Collaborations. arXiv\npreprint arXiv:2106.10207 (2021).\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2018. Measuring and Mitigating Unintended Bias\nin Text Classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (New Orleans, LA, USA)\n(AIES ’18) . Association for Computing Machinery, New York, NY, USA, 67–73. https://doi.org/10.1145/3278721.3278729\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. 2019. Show Your Work: Improved\nReporting of Experimental Results. arXiv:1909.03004 [cs.LG]\nJesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, and Matt Gardner. 2021.\nDocumenting the English Colossal Clean Crawled Corpus. CoRR abs/2104.08758 (2021). arXiv:2104.08758 https:\n//arxiv.org/abs/2104.08758\nBrian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The\ndeepfake detection challenge dataset. arXiv e-prints (2020), arXiv–2006.\nXin Luna Dong, Hannaneh Hajishirzi, Colin Lockard, and Prashant Shiralkar. 2020. Multi-modal Information Extraction from\nText, Semi-structured, and Tabular Data on the Web. InProceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining . 3543–3544.\nShayan Doroudi, Vincent Aleven, and Emma Brunskill. 2017. Robust Evaluation Matrix: Towards a More Principled\nOffline Exploration of Instructional Policies. In Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale\n(Cambridge, Massachusetts, USA) (L@S ’17). Association for Computing Machinery, New York, NY, USA, 3–12. https:\n//doi.org/10.1145/3051457.3051463\nFinale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint\narXiv:1702.08608 (2017).\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa\nDehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An Image is Worth 16x16 Words: Transformers\nfor Image Recognition at Scale. In International Conference on Learning Representations .\nA. Dosovitskiy, L. Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, M. Dehghani,\nMatthias Minderer, G. Heigold, S. Gelly, Jakob Uszkoreit, and N. Houlsby. 2021. An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale. ArXiv abs/2010.11929 (2021).\nGraham Dove, Kim Halskov, Jodi Forlizzi, and John Zimmerman. 2017. UX Design Innovation: Challenges for Working\nwith Machine Learning as a Design Material. In Proceedings of the 2017 CHI Conference on Human Factors in Computing\nSystems. ACM.\nAnca D Dragan and Siddhartha S Srinivasa. 2013. Formalizing Assistive Teleoperation. Robotics: Science and Systems VIII\n(2013), 73.\nT. Dreossi, Alexandre Donzé, and S. Seshia. 2017. Compositional Falsification of Cyber-Physical Systems with Machine\nLearning Components. In NFM.\nJ. Drews. 2000. Drug discovery: a historical perspective. Science 287 5460 (2000), 1960–4.\nSimon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. 2020. Few-Shot Learning via Learning the Representation,\nProvably. arXiv (2020).\nSebastian Duerr and Peter A. Gloor. 2021. Persuasive Natural Language Generation – A Literature Review.\narXiv:2101.05786 [cs.CL]\nEmmanuel Dupoux. 2018. Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the\ninfant language-learner. Cognition 173 (2018), 43–59.\nMiquel Duran-Frigola, Eduardo Pauls, Oriol Guitart-Pla, Martino Bertoni, Víctor Alcalde, David Amat, Teresa Juan-Blanco,\nand Patrick Aloy. 2020. Extending the small-molecule similarity principle to all levels of biology with the Chemical\nChecker. Nature Biotechnology 38, 9 (2020), 1087–1096.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Calibrating noise to sensitivity in private data\nanalysis. In Proceedings of the 3rd Theory of Cryptography Conference . 265–284.\nGrégoire Déletang, J. Grau-Moya, Miljan Martic, Tim Genewein, Tom McGrath, Vladimir Mikulik, M. Kunesch, S. Legg, and\nPedro A. Ortega. 2021. Causal Analysis of Agent Behavior for AI Safety. arXiv preprint arXiv:2103.03938 (2021).\nMahmoud Elbattah, Émilien Arnaud, Maxime Gignon, and Gilles Dequen. 2021. The Role of Text Analytics in Healthcare: A\nReview of Recent Developments and Applications.. In HEALTHINF. 825–832.\nPaul Elbourne. 2011. Meaning: a slim guide to semantics . Oxford University Press.\n174 Center for Research on Foundation Models (CRFM)\nKevin Ellis, Catherine Wong, Maxwell I. Nye, Mathias Sablé-Meyer, Lucas Morales, Luke B. Hewitt, Luc Cary, Armando\nSolar-Lezama, and Joshua B. Tenenbaum. 2021. DreamCoder: bootstrapping inductive program synthesis with wake-\nsleep library learning. In PLDI ’21: 42nd ACM SIGPLAN International Conference on Programming Language Design and\nImplementation, Virtual Event, Canada, June 20-25, 20211 , Stephen N. Freund and Eran Yahav (Eds.). ACM, 835–850.\nhttps://doi.org/10.1145/3453483.3454080\nGamaleldin F Elsayed, Ian Goodfellow, and Jascha Sohl-Dickstein. 2018. Adversarial reprogramming of neural networks.\narXiv preprint arXiv:1806.11146 (2018).\nErich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan. 2020. Fast Sparse ConvNets. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) .\nDaniel C Elton. 2020. Self-explaining AI as an alternative to interpretable AI. InInternational Conference on Artificial General\nIntelligence. Springer, 95–106.\nEmad Elwany, Dave Moore, and Gaurav Oberoi. 2019. Bert goes to law school: Quantifying the competitive advantage of\naccess to large legal corpora in contract understanding. arXiv preprint arXiv:1911.00473 (2019).\nDouglas C. Engelbart. 1963. A Conceptual Framework for the Augmentation of Man’s Intellect. In Computer-supported\ncooperative work: a book of readings .\nDavid Freeman Engstrom, Daniel E Ho, Catherine M Sharkey, and Mariano-Florentino Cuéllar. 2020. Government by\nalgorithm: Artificial intelligence in federal administrative agencies. NYU School of Law, Public Law Research Paper 20-54\n(2020).\nDanielle Ensign, Sorelle A Friedler, Scott Neville, Carlos Scheidegger, and Suresh Venkatasubramanian. 2018. Runaway\nfeedback loops in predictive policing. In Conference on Fairness, Accountability and Transparency . PMLR, 160–171.\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst. 2019. Understanding Undesirable Word Embedding Associations.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for Computational\nLinguistics, Florence, Italy, 1696–1705. https://doi.org/10.18653/v1/P19-1166\nKawin Ethayarajh and Dan Jurafsky. 2020. Utility is in the Eye of the User: A Critique of NLP Leaderboards. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational\nLinguistics, Online, 4846–4853. https://doi.org/10.18653/v1/2020.emnlp-main.393\nAllyson Ettinger. 2020. What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.\nTransactions of the Association for Computational Linguistics 8 (2020), 34–48. https://doi.org/10.1162/tacl_a_00298\nAllyson Ettinger and Tal Linzen. 2016. Evaluating vector space models using human semantic priming results. InProceedings\nof the 1st Workshop on Evaluating Vector-Space Representations for NLP . Association for Computational Linguistics, Berlin,\nGermany, 72–77. https://doi.org/10.18653/v1/W16-2513\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. 2020. Rigging the Lottery: Making All Tickets\nWinners. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning\nResearch, Vol. 119) , Hal Daumé III and Aarti Singh (Eds.). PMLR, 2943–2952.\nTom Everitt, Gary Lea, and Marcus Hutter. 2018. AGI safety literature review. arXiv preprint arXiv:1805.01109 (2018).\nBenjamin Eysenbach, Shixiang Gu, J. Ibarz, and Sergey Levine. 2017. Leave no Trace: Learning to Reset for Safe and\nAutonomous Reinforcement Learning. arXiv preprint arXiv:1711.06782 (2017).\nJerry Alan Fails and Dan R. Olsen. 2003. A design tool for camera-based interaction. In Proceedings of the conference on\nHuman factors in computing systems . ACM.\nLinxi Fan, Shyamal Buch, Guanzhi Wang, Ryan Cao, Yuke Zhu, Juan Carlos Niebles, and Li Fei-Fei. 2020. RubiksNet:\nLearnable 3D-Shift for Efficient Video Action Recognition. In Proceedings of the European Conference on Computer Vision\n(ECCV).\nDonald J Farole, Jr. and Lynn Langston. 2010. County-based and Local Public Defender Offices, 2007 . Technical Report. U.S.\nDepartment of Justice Bureau of Justice Statistics.\nSina Fazelpour and David Danks. 2021. Algorithmic bias: Senses, sources, solutions. Philosophy Compass 16, 8 (June 2021).\nhttps://doi.org/10.1111/phc3.12760\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch transformers: Scaling to trillion parameter models with simple\nand efficient sparsity. arXiv preprint arXiv:2101.03961 (2021).\nLi Fei-Fei, Asha Iyer, Christof Koch, and Pietro Perona. 2007. What do We Perceive in a Glance of a Real-World Scene?\nJournal of Vision 7, 1 (2007), 10–10.\nXin Feng, Youni Jiang, Xuejiao Yang, Ming Du, and Xin Li. 2019. Computer vision algorithms and hardware implementations:\nA survey. Integration 69 (2019), 309–320.\nAndrew Guthrie Ferguson. 2017. The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement . NYU\nPress. http://www.jstor.org/stable/j.ctt1pwtb27\nBesnik Fetahu, Abhijit Anand, and Avishek Anand. 2015. How much is Wikipedia lagging behind news?. In Proceedings of\nthe ACM Web Science Conference . 1–9.\nOn the Opportunities and Risks of Foundation Models 175\nAnjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021. A Survey of Race, Racism, and Anti-Racism in\nNLP. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers) . Association for Computational Linguistics,\nOnline, 1905–1925. https://doi.org/10.18653/v1/2021.acl-long.149\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\nIn International Conference on Machine Learning (ICML) .\nChelsea Finn and Sergey Levine. 2017. Deep visual foresight for planning robot motion. In International Conference on\nRobotics and Automation (ICRA) .\nC. Finn, S. Levine, and P. Abbeel. 2016a. Guided cost learning: Deep inverse optimal control via policy optimization. In\nInternational Conference on Machine Learning (ICML) . 49–58.\nChelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine, and Pieter Abbeel. 2016b. Deep spatial autoencoders for\nvisuomotor learning. In 2016 IEEE International Conference on Robotics and Automation (ICRA) . IEEE, 512–519.\nVlad Firoiu, Eser Aygün, Ankit Anand, Zafarali Ahmed, Xavier Glorot, Laurent Orseau, Doina Precup, and Shibl Mourad.\n2021. Training a First-Order Theorem Prover from Synthetic Data. The First Mathematical Reasoning in General Artificial\nIntelligence Workshop, ICLR 2021 (2021). https://mathai-iclr.github.io/papers/papers/MATHAI_18_paper.pdf\nJaime F. Fisac, Neil F. Lugovoy, Vicenç Rúbies Royo, S. Ghosh, and C. Tomlin. 2019. Bridging Hamilton-Jacobi Safety Analysis\nand Reinforcement Learning. In International Conference on Robotics and Automation (ICRA) .\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 Shared Task: Evaluating\nGeneralization in Reading Comprehension. In Workshop on Machine Reading for Question Answering (MRQA) .\nCarlos Florensa, Yan Duan, and P. Abbeel. 2017. Stochastic Neural Networks for Hierarchical Reinforcement Learning.\nArXiv abs/1704.03012 (2017).\nLuciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Luetge,\nRobert Madelin, Ugo Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy Vayena. 2018. AI4People—An\nEthical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. Minds and Machines\n28, 4 (Nov. 2018), 689–707. https://doi.org/10.1007/s11023-018-9482-5\nRuth C. Fong and Andrea Vedaldi. 2017. Interpretable Explanations of Black Boxes by Meaningful Perturbation. InProceedings\nof the IEEE International Conference on Computer Vision (ICCV) .\nStanislav Fort. 2021. Adversarial examples for the OpenAI CLIP in its zero-shot classification regime and their semantic\ngeneralization. https://stanislavfort.github.io/2021/01/12/OpenAI_CLIP_adversarial_examples.html\nS. Frank, Irene Fernandez Monsalve, Robin L. Thompson, and G. Vigliocco. 2013. Reading time data for evaluating broad-\ncoverage models of English sentence processing. Behavior Research Methods 45 (2013), 1182–1190.\nMatt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion attacks that exploit confidence information\nand basic countermeasures. In ACM SIGSAC Conference on Computer and Communications Security .\nJonathan B. Freeman, Andrew M. Penner, Aliya Saperstein, Matthias Scheutz, and Nalini Ambady. 2011. Looking the Part:\nSocial Status Cues Shape Race Perception. PLOS ONE 6, 9 (09 2011), 1–10. https://doi.org/10.1371/journal.pone.0025107\nRobert M French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences 3 (1999).\nBatya Friedman and David G. Hendry. 2019. Value Sensitive Design: Shaping Technology with Moral Imagination . The MIT\nPress.\nBatya Friedman and Helen Nissenbaum. 1996. Bias in Computer Systems. ACM Transactions on Information Systems 14, 3\n(July 1996), 330–347. https://doi.org/10.1145/230538.230561\nZack Friedman. 2020. Student loan debt statistics in 2020: A record $1.6 trillion. https://www.forbes.com/sites/zackfriedman/\n2020/02/03/student-loan-debt-statistics/?sh=34191d3281fe\nJustin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. 2019. From Language to Goals: Inverse Reinforcement\nLearning for Vision-Based Instruction Following. In International Conference on Learning Representations (ICLR) .\nJustin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. 2018. Variational Inverse Control with Events: A General\nFramework for Data-Driven Reward Definition. In Advances in Neural Information Processing Systems (NeurIPS) .\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019. Neural language models\nas psycholinguistic subjects: Representations of syntactic state. InProceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) .\nAssociation for Computational Linguistics, Minneapolis, Minnesota, 32–42. https://doi.org/10.18653/v1/N19-1004\nIason Gabriel. 2020. Artificial intelligence, values, and alignment. Minds and machines 30, 3 (2020), 411–437.\nFederico A Galatolo, Mario GCA Cimino, and Gigliola Vaglini. 2021. Generating images from caption and vice versa via\nCLIP-Guided Generative Latent Space Search. arXiv preprint arXiv:2102.01645 (2021).\nTrevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. 2020. Sparse GPU Kernels for Deep Learning. InSC20: International\nConference for High Performance Computing, Networking, Storage and Analysis . IEEE, 1–14.\nJulia Galliers and Karen Spärck Jones. 1993. Evaluating Natural Language Processing Systems . University of Cambridge,\nComputer Laboratory. https://books.google.com/books?id=ZxklAQAAIAAJ\n176 Center for Research on Foundation Models (CRFM)\nChuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhand-\nwaldar, Nick Haber, Megumi Sano, Kuno Kim, Elias Wang, Damian Mrowca, Michael Lingelbach, Aidan Curtis, Kevin\nFeigelis, Daniel M. Bear, Dan Gutfreund, David Cox, James J. DiCarlo, Josh McDermott, Joshua B. Tenenbaum, and Daniel\nL. K. Yamins. 2020. ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation. arXiv:2007.04954 [cs.CV]\nOscar H. Gandy, Jr. 2021. The Panoptic Sort: A Political Economy of Personal Information (2 ed.). Oxford University Press.\nYaroslav Ganin and Victor Lempitsky. 2015. Unsupervised Domain Adaptation by Backpropagation. In International\nConference on Machine Learning (ICML) . 1180–1189.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite,\nNoa Nabeshima, Shawn Presser, and Connor Leahy. 2020a. The Pile: An 800GB Dataset of Diverse Text for Language\nModeling. arXiv:2101.00027 [cs.CL]\nRuohan Gao, Changan Chen, Ziad Al-Halab, Carl Schissler, and Kristen Grauman. 2020b. VisualEchoes: Spatial Image\nRepresentation Learning through Echolocation. In ECCV.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020c. Making Pre-trained Language Models Better Few-shot Learners. CoRR\nabs/2012.15723 (2020). arXiv:2012.15723 https://arxiv.org/abs/2012.15723\nSiddhant Garg and Goutham Ramakrishnan. 2020. Bae: Bert-based adversarial examples for text classification.arXiv preprint\narXiv:2004.01970 (2020).\nTimnit Gebru. 2021. Race and Gender. In The Oxford Handbook of Ethics of AI , Markus Dirk Dubber, Frank Pasquale, and\nSunit Das (Eds.). Oxford.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Ill, and Kate\nCrawford. 2018. Datasheets for Datasets. arXiv preprint arXiv:1803.09010 (2018).\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating\nNeural Toxic Degeneration in Language Models. InFindings of the Association for Computational Linguistics: EMNLP 2020 .\nAssociation for Computational Linguistics, Online, 3356–3369. https://doi.org/10.18653/v1/2020.findings-emnlp.301\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, An-\ntoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin\nDurmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou,\nYacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan,\nMounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina\nMcMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niy-\nongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego\nRodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio\nSobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021.\nThe GEM Benchmark: Natural Language Generation, its Evaluation and Metrics. In Proceedings of the 1st Workshop on\nNatural Language Generation, Evaluation, and Metrics (GEM 2021) . Association for Computational Linguistics, Online,\n96–120. https://doi.org/10.18653/v1/2021.gem-1.10\nAtticus Geiger, Kyle Richardson, and Christopher Potts. 2020. Neural Natural Language Inference Models Partially Embed\nTheories of Lexical Entailment and Negation. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and\nInterpreting Neural Networks for NLP . Association for Computational Linguistics, Online, 163–173. https://doi.org/10.\n18653/v1/2020.blackboxnlp-1.16\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A\nWichmann. 2020. Shortcut Learning in Deep Neural Networks. arXiv preprint arXiv:2004.07780 (2020).\nRobert Geirhos, Carlos RM Temme, Jonas Rauber, Heiko H Schütt, Matthias Bethge, and Felix A Wichmann. 2018. Generali-\nsation in humans and deep neural networks. Advances in neural information processing systems 31 (2018), 7538–7550.\nSamuel Genheden, Amol Thakkar, Veronika Chadimová, Jean-Louis Reymond, Ola Engkvist, and Esben Bjerrum. 2020.\nAiZynthFinder: a fast, robust and flexible open-source software for retrosynthetic planning. Journal of Cheminformatics\n12, 1 (Nov. 2020). https://doi.org/10.1186/s13321-020-00472-1\nDavid Gershgorn. 2021. GitHub’s Automatic Coding Tool Rests on Untested Legal Ground. https://www.theverge.com/\n2021/7/7/22561180/github-copilot-legal-copyright-fair-use-public-code.\nMoritz Gerstung, Elli Papaemmanuil, Inigo Martincorena, Lars Bullinger, Verena I Gaidzik, Peter Paschka, Michael Heuser,\nFelicitas Thol, Niccolo Bolli, Peter Ganly, et al. 2017. Precision oncology for acute myeloid leukemia using a knowledge\nbank approach. Nature Genetics 49, 3 (2017), 332–340.\nMor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are We Modeling the Task or the Annotator? An Investigation of\nAnnotator Bias in Natural Language Understanding Datasets. In Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) .\nAssociation for Computational Linguistics, Hong Kong, China, 1161–1166. https://doi.org/10.18653/v1/D19-1107\nMor Geva, R. Schuster, Jonathan Berant, and Omer Levy. 2020. Transformer Feed-Forward Layers Are Key-Value Memories.\nArXiv abs/2012.14913 (2020).\nOn the Opportunities and Risks of Foundation Models 177\nNegin Ghavami and Letitia Anne Peplau. 2013. An Intersectional Analysis of Gender and Ethnic Stereotypes: Testing\nThree Hypotheses. Psychology of Women Quarterly 37, 1 (2013), 113–127. https://doi.org/10.1177/0361684312464203\narXiv:https://doi.org/10.1177/0361684312464203\nAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 2021. A Survey of Quantization\nMethods for Efficient Neural Network Inference. arXiv preprint arXiv:2103.13630 (2021).\nAmirata Ghorbani and James Zou. 2019. Data shapley: Equitable valuation of data for machine learning. In International\nConference on Machine Learning . PMLR, 2242–2251.\nJames J Gibson. 1979. The ecological approach to visual perception . Psychology Press.\nTalia B Gillis and Jann L Spiess. 2019. Big data and discrimination. The University of Chicago Law Review 86, 2 (2019),\n459–488.\nAntonio Ginart, Melody Y. Guan, Gregory Valiant, and James Zou. 2019. Making AI Forget You: Data Deletion in Machine\nLearning. arXiv:1907.05012 [cs.LG]\nKathryn T. Gines. 2011. Black Feminism and Intersectional Analyses. Philosophy Today 55, 9999 (2011), 275–284. https:\n//doi.org/10.5840/philtoday201155supplement68\nJane C Ginsburg and Luke Ali Budiardjo. 2019. Authors and machines. Berkeley Tech. LJ 34 (2019), 343.\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition . 580–587.\nXavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In\nInternational Conference on Artificial Intelligence and Statistics .\nAbhinav Goel, Caleb Tung, Yung-Hsiang Lu, and George K Thiruvathukal. 2020b. A survey of methods for low-power deep\nlearning and computer vision. In 2020 IEEE 6th World Forum on Internet of Things (WF-IoT) . IEEE, 1–6.\nKaran Goel, Albert Gu, Yixuan Li, and Christopher Ré. 2020a. Model Patching: Closing the Subgroup Performance Gap with\nData Augmentation. arXiv preprint arXiv:2008.06775 (2020).\nKaran Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong, Mohit Bansal, and Christopher\nRé. 2021. Robustness Gym: Unifying the NLP Evaluation Landscape. arXiv preprint arXiv:2101.04840 (2021).\nGabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah.\n2021. Multimodal neurons in artificial neural networks. Distill 6, 3 (2021), e30.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ricardo Muñoz Sánchez, Mugdha Pandya, and Adam Lopez. 2021. Intrinsic\nBias Metrics Do Not Correlate with Application Bias. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers). Association for Computational Linguistics, Online, 1926–1940. https://doi.org/10.18653/v1/2021.acl-long.150\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word\nembeddings but do not remove them. In Proceedings of NAACL 2019 .\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep learning . MIT press.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. 2014. Generative Adversarial Nets. In Advances in Neural Information Processing Systems (NeurIPS) .\nCharles A.E. Goodhart. 1984. Problems of monetary management: the UK experience. In Monetary Theory and Practice .\nSpringer, 91–121. https://link.springer.com/chapter/10.1007/978-1-349-17295-5_4\nM. A. Goodrich and A. Schultz. 2007. Human-Robot Interaction: A Survey. Found. Trends Hum. Comput. Interact. 1 (2007),\n203–275.\nDivya Gopinath, Monica Agrawal, Luke Murray, Steven Horng, David Karger, and David Sontag. 2020. Fast, Structured\nClinical Documentation via Contextual Autocomplete. InProceedings of the 5th Machine Learning for Healthcare Conference\n(Proceedings of Machine Learning Research, Vol. 126) , Finale Doshi-Velez, Jim Fackler, Ken Jung, David Kale, Rajesh\nRanganath, Byron Wallace, and Jenna Wiens (Eds.). PMLR, 842–870. http://proceedings.mlr.press/v126/gopinath20a.html\nMitchell A Gordon, Kevin Duh, and Nicholas Andrews. 2020. Compressing BERT: Studying the Effects of Weight Pruning\non Transfer Learning. arXiv preprint arXiv:2002.08307 (2020).\nAssaf Gottlieb, Gideon Y Stein, Eytan Ruppin, and Roded Sharan. 2011. PREDICT: a method for inferring novel drug\nindications with application to personalized medicine. Molecular Systems Biology 7, 1 (2011), 496.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. Larger-Scale Transformers for\nMultilingual Masked Language Modeling. arXiv preprint arXiv:2105.00572 (2021).\nR. Goyal, S. Kahou, Vincent Michalski, Joanna Materzynska, S. Westphal, Heuna Kim, Valentin Haenel, Ingo Fründ, P.\nYianilos, Moritz Mueller-Freitag, F. Hoppe, Christian Thurau, I. Bax, and R. Memisevic. 2017a. The “Something Something”\nVideo Database for Learning and Evaluating Visual Common Sense. 2017 IEEE International Conference on Computer\nVision (ICCV) (2017), 5843–5851.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017b. Making the v in vqa matter: Elevating\nthe role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition . 6904–6913.\n178 Center for Research on Foundation Models (CRFM)\nChristine Grady. 2015. Institutional Review Boards.Chest 148, 5 (Nov. 2015), 1148–1155. https://doi.org/10.1378/chest.15-0706\nDon Grant, David Zelinka, and Stefania Mitova. 2021. Reducing CO2 emissions by targeting the world’s hyper-polluting\npower plants. Environmental Research Letters (2021).\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger,\nHao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan,\nFiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv\nBatra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano\nFragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei\nHuang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao\nLi, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will\nPrice, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano,\nRuijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen,\nGiovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li,\nRichard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio\nTorralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. 2021. Ego4D: Around the World in 3,000 Hours of Egocentric\nVideo. arXiv:2110.07058 [cs.CV]\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural Turing machines. arXiv preprint arXiv:1410.5401 (2014).\nAlex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez\nColmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. 2016. Hybrid computing using a neural network\nwith dynamic external memory. Nature 538, 7626 (2016), 471–476.\nJonathan Gray, Kavya Srinet, Yacine Jernite, Haonan Yu, Zhuoyuan Chen, Demi Guo, Siddharth Goyal, C Lawrence Zitnick,\nand Arthur Szlam. 2019. Craftassist: A framework for dialogue-enabled interactive agents.arXiv preprint arXiv:1907.08584\n(2019).\nAnthony G. Greenwald, Debbie E. McGhee, and Jordan L.K. Schwartz. 1998. Measuring individual differences in implicit\ncognition: The implicit association test. Journal of Personality and Social Psychology 74, 6 (1998), 1464. https://psycnet.\napa.org/record/1998-02892-004\nShai Gretz, Yonatan Bilu, Edo Cohen-Karlik, and Noam Slonim. 2020. The workweek is the best time to start a family–A\nStudy of GPT-2 Based Claim Generation. arXiv preprint arXiv:2010.06185 (2020).\nJean-Bastien Grill, Florian Strub, Florent Altch’e, C. Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, B. A.\nPires, Z. Guo, M. G. Azar, Bilal Piot, K. Kavukcuoglu, R. Munos, and Michal Valko. 2020. Bootstrap Your Own Latent: A\nNew Approach to Self-Supervised Learning. ArXiv abs/2006.07733 (2020).\nStephen Grimm. 2021. Understanding. In The Stanford Encyclopedia of Philosophy (Summer 2021 ed.), Edward N. Zalta (Ed.).\nMetaphysics Research Lab, Stanford University.\nJames Grimmelmann. 2015. There’s No Such Thing as a Computer-Authored Work-And It’s a Good Thing, Too. Colum. JL\n& Arts 39 (2015), 403.\nJacob Grinfeld, Jyoti Nangalia, E Joanna Baxter, David C Wedge, Nicos Angelopoulos, Robert Cantrill, Anna L Godfrey, Elli Pa-\npaemmanuil, Gunes Gundem, Cathy MacLean, et al. 2018. Classification and personalized prognosis in myeloproliferative\nneoplasms. New England Journal of Medicine 379, 15 (2018), 1416–1430.\nMaura R Grossman and Gordon V Cormack. 2010. Technology-assisted review in e-discovery can be more effective and\nmore efficient than exhaustive manual review. Rich. JL & Tech. 17 (2010), 1.\nJian Guan. 2019. Artificial intelligence in healthcare and medicine: promises, ethical challenges and governance. Chinese\nMedical Sciences Journal 34, 2 (2019), 76–83.\nSumit Gulwani, Oleksandr Polozov, and Rishabh Singh. 2017. Program Synthesis. Found. Trends Program. Lang. 4, 1-2 (2017),\n1–119. https://doi.org/10.1561/2500000010\nSumit Gulwani and Rishabh Singh. 2013. Automated Feedback Generation for Introductory Programming Assignments. In\nACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2013) (acm sigplan conference on\nprogramming language design and implementation (pldi 2013) ed.). 15–26. https://www.microsoft.com/en-us/research/\npublication/automated-feedback-generation-for-introductory-programming-assignments/\nSuriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. 2018. Implicit bias of gradient descent on linear convolutional\nnetworks. In Advances in Neural Information Processing Systems . 9461–9471.\nSuriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. 2017. Implicit regular-\nization in matrix factorization. In Advances in Neural Information Processing Systems (NeurIPS) . 6151–6159.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy,\nShengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366\n(2020).\nQi Guo, Chinmay Kulkarni, Aniket Kittur, Jeffrey P. Bigham, and Emma Brunskill. 2016. Questimator: Generating Knowledge\nAssessments for Arbitrary Topics. InProceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence\nOn the Opportunities and Risks of Foundation Models 179\n(New York, New York, USA)(IJCAI’16). AAAI Press, 3726–3732.\nWei Guo and Aylin Caliskan. 2021. Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a\nDistribution of Human-like Biases. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society (Virtual\nEvent, USA) (AIES ’21) . Association for Computing Machinery, New York, NY, USA, 122–133. https://doi.org/10.1145/\n3461702.3462536\nAbhinav Gupta, Adithyavairavan Murali, Dhiraj Gandhi, and Lerrel Pinto. 2018. Robot Learning in Homes: Improving\nGeneralization and Reducing Dataset Bias. In NeurIPS.\nAbhishek Gupta, Justin Yu, Tony Zhao, Vikash Kumar, Aaron Rovinsky, Kelvin Xu, Thomas Devlin, and Sergey Levine.\n2021b. Reset-Free Reinforcement Learning via Multi-Task Learning: Learning Dexterous Manipulation Behaviors without\nHuman Intervention. arXiv preprint arXiv:2104.11203 (2021).\nUdit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and Carole-Jean Wu.\n2021a. Chasing Carbon: The Elusive Environmental Footprint of Computing. In 2021 IEEE International Symposium on\nHigh-Performance Computer Architecture (HPCA) . IEEE, 854–867.\nSuchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020.\nDon’t Stop Pretraining: Adapt Language Models to Domains and Tasks. arXiv:2004.10964 [cs.CL]\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation\nArtifacts in Natural Language Inference Data. In Association for Computational Linguistics (ACL) . 107–112.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-Augmented Language\nModel Pre-Training. arXiv preprint arXiv:2002.08909 (2020).\nN. Haber, Damian Mrowca, Li Fei-Fei, and Daniel Yamins. 2018. Learning to Play with Intrinsically-Motivated Self-Aware\nAgents. In NeurIPS.\nDanijar Hafner, T. Lillicrap, Ian S. Fischer, Ruben Villegas, David R Ha, Honglak Lee, and James Davidson. 2019. Learning\nLatent Dynamics for Planning from Pixels. In International Conference on Machine Learning (ICML) .\nMartin J. Haigh. 1985. An Introduction to Computer-Aided Design and Manufacture . Blackwell Scientific Publications, Ltd.,\nGBR.\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversarial ReProgramming. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers) . Association for Computational Linguistics, Online,\n4921–4933. https://doi.org/10.18653/v1/2021.acl-long.381\nHarald Hammarström, Robert Forkel, Martin Haspelmath, and Sebastian Bank. 2021. Glottolog 4.4 . Leipzig. https:\n//doi.org/10.5281/zenodo.4761960\nJesse Michael Han, Jason Rute, Yuhuai Wu, Edward W. Ayers, and Stanislas Polu. 2021. Proof Artifact Co-training for\nTheorem Proving with Language Models. The First Mathematical Reasoning in General Artificial Intelligence Workshop,\nICLR 2021 (2021). https://mathai-iclr.github.io/papers/papers/MATHAI_23_paper.pdf\nJeffrey T Hancock, Mor Naaman, and Karen Levy. 2020. AI-Mediated Communication: Definition, Research Agenda, and\nEthical Considerations. Journal of Computer-Mediated Communication (2020).\nD.J. Hand. 2010. Measurement Theory and Practice: The World Through Quantification . Wiley. https://books.google.com/\nbooks?id=rap0PwAACAAJ\nAlex Hanna, Emily Denton, Andrew Smart, and Jamila Smith-Loud. 2020. Towards a critical race methodology in algorithmic\nfairness. In Proceedings of the 2020 conference on fairness, accountability, and transparency . 501–512.\nStephen R Hanney, Sophie Castle-Clarke, Jonathan Grant, Susan Guthrie, Chris Henshall, Jorge Mestre-Ferrandiz, Michele\nPistollato, Alexandra Pollitt, Jon Sussex, and Steven Wooding. 2015. How long does biomedical research take? Studying\nthe time taken between biomedical and health research and its translation into products, policy, and practice. Health\nresearch policy and systems 13, 1 (2015), 1–18.\nJeff Z. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. 2021a. Provable Guarantees for Self-Supervised Deep Learning\nwith Spectral Contrastive Loss. CoRR abs/2106.04156 (2021). arXiv:2106.04156 https://arxiv.org/abs/2106.04156\nJeff Z HaoChen, Colin Wei, Jason Lee, and Tengyu Ma. 2021b. Shape matters: Understanding the implicit bias of the noise\ncovariance. In Conference on Learning Theory . PMLR, 2315–2357.\nAlbert Haque, Michelle Guo, Alexandre Alahi, Serena Yeung, Zelun Luo, Alisha Rege, Jeffrey Jopling, Lance Downing,\nWilliam Beninati, Amit Singh, et al. 2017. Towards vision-based smart hospitals: a system for tracking and monitoring\nhand hygiene compliance. In Machine Learning for Healthcare Conference . PMLR, 75–87.\nAlbert Haque, Arnold Milstein, and Li Fei-Fei. 2020. Illuminating the dark spaces of healthcare with ambient intelligence.\nNature 585, 7824 (2020), 193–202.\nSandra Harding. 2015. Objectivity and Diversity . University of Chicago Press. https://doi.org/doi:10.7208/9780226241531\nStefan Harrer, Pratik Shah, Bhavna Antony, and Jianying Hu. 2019. Artificial intelligence for clinical trial design. Trends in\npharmacological sciences 40, 8 (2019), 577–591.\n180 Center for Research on Foundation Models (CRFM)\nPeter Hase, Mona T. Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan\nIyer. 2021. Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs. CoRR\nabs/2111.13654 (2021). https://arxiv.org/abs/2111.13654\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. 2018. Fairness without demographics in\nrepeated loss minimization. In International Conference on Machine Learning . PMLR, 1929–1938.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2021. Masked autoencoders are scalable\nvision learners. arXiv preprint arXiv:2111.06377 (2021).\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2019. Momentum Contrast for Unsupervised Visual\nRepresentation Learning. arXiv preprint arXiv:1911.05722 (2019).\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum Contrast for Unsupervised Visual\nRepresentation Learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 9726–9735.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016a. Deep residual learning for image recognition. InProceedings\nof the IEEE conference on computer vision and pattern recognition . 770–778.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016b. Deep Residual Learning for Image Recognition. In\nComputer Vision and Pattern Recognition (CVPR) .\nAllison Hegel, Marina Shah, Genevieve Peaslee, Brendan Roof, and Emad Elwany. 2021. The Law of Large Documents:\nUnderstanding the Structure of Legal Contracts Using Visual Cues. arXiv preprint arXiv:2107.08128 (2021).\nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. 2015. ActivityNet: A Large-Scale Video\nBenchmark for Human Activity Understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition. 961–970.\nRobert Heilmayr, Cristian Echeverría, and Eric F Lambin. 2020. Impacts of Chilean forest subsidies on forest cover, carbon\nand biodiversity. Nature Sustainability 3, 9 (2020), 701–709.\nChristina Heinze-Deml and Nicolai Meinshausen. 2017. Conditional variance penalties and domain shift robustness. arXiv\npreprint arXiv:1710.11469 (2017).\nKyle Helfrich, Devin Willmott, and Qiang Ye. 2018. Orthogonal recurrent neural networks with scaled Cayley transform. In\nInternational Conference on Machine Learning . PMLR, 1969–1978.\nJoseph M Hellerstein and Michael Stonebraker. 2005. Readings in database systems . MIT press.\nDeborah Hellman. 2020. Measuring algorithmic fairness. Va. L. Rev. 106 (2020), 811.\nDeborah Hellman. 2021. Big Data and Compounding Injustice. Journal of Moral Philosophy, forthcoming, Virginia Public\nLaw and Legal Theory Research Paper 2021-27 (2021).\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. 2016. Tracking the world state with recurrent\nentity networks. arXiv preprint arXiv:1612.03969 (2016).\nOlivier J Hénaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, and João Carreira. 2021.\nEfficient visual pretraining with contrastive detection. ICCV (2021).\nPeter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. Towards the systematic\nreporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research 21, 248 (2020),\n1–43.\nPeter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle\nPineau. 2017. Ethical Challenges in Data-Driven Dialogue Systems. In AAAI/ACM AI Ethics and Society Conference .\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring\nmassive multitask language understanding. In International Conference on Learning Representations (ICLR) .\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b.\nMeasuring Massive Multitask Language Understanding. In International Conference on Learning Representations . https:\n//openreview.net/forum?id=d7KBjmI3GmQ\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021c. Cuad: An expert-annotated nlp dataset for legal contract\nreview. arXiv preprint arXiv:2103.06268 (2021).\nDan Hendrycks, Nicholas Carlini, John Schulman, Tom Dietterich, and Jacob Steinhardt. 2021d. Unsolved Problems in ML\nSafety. arXiv preprint (2021).\nDan Hendrycks and Thomas Dietterich. 2019. Benchmarking neural network robustness to common corruptions and\nperturbations. In International Conference on Learning Representations (ICLR) .\nDan Hendrycks, Kimin Lee, and Mantas Mazeika. 2019a. Using Pre-Training Can Improve Model Robustness and Uncertainty.\nIn International Conference on Machine Learning (ICML) .\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. Pretrained trans-\nformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100 (2020).\nDan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. 2019b. Using Self-Supervised Learning Can Improve\nModel Robustness and Uncertainty. In Advances in Neural Information Processing Systems (NeurIPS) .\nOn the Opportunities and Risks of Foundation Models 181\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021e. Natural Adversarial Examples.\narXiv:1907.07174 [cs.LG]\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, T. Brown, Prafulla\nDhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John\nSchulman, Dario Amodei, and Sam McCandlish. 2020. Scaling Laws for Autoregressive Generative Modeling. ArXiv\nabs/2010.14701 (2020). https://arxiv.org/abs/2010.14701\nSylvia L. Herbert, Jason J. Choi, Suvansh Qazi, Marsalis Gibson, K. Sreenath, and C. Tomlin. 2021. Scalable Learning of\nSafety Guarantees for Autonomous Systems using Hamilton-Jacobi Reachability. arXiv preprint arXiv:2101.05916 (2021).\nMaguire Herriman, Elana Meer, Roy Rosin, Vivian Lee, Vindell Washington, and Kevin G Volpp. 2020. Asked and answered:\nbuilding a chatbot to address Covid-19-related concerns. Nejm Catalyst Innovations in Care Delivery (2020).\nJ. Hestness, Sharan Narang, Newsha Ardalani, G. Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Y.\nYang, and Yanqi Zhou. 2017. Deep Learning Scaling is Predictable, Empirically. ArXiv abs/1712.00409 (2017).\nJohn Hewitt and Percy Liang. 2019. Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China. https://www.\naclweb.org/anthology/D19-1275\nJohn Hewitt and Christopher D. Manning. 2019. A Structural Probe for Finding Syntax in Word Representations. In North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL) (Minneapolis,\nUSA). Association for Computational Linguistics.\nHidalgo. 2021. How Humans Judge Machines . The MIT Press, Cambridge, Massachusetts.\nBrian Hie, Ellen D Zhong, Bonnie Berger, and Bryan Bryson. 2021. Learning the language of viral evolution and escape.\nScience 371, 6526 (2021), 284–288.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. arXiv preprint\narXiv:1503.02531 (2015).\nGeoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural\ncomputation 18, 7 (2006), 1527–1554.\nDaniel E Ho and Alice Xiang. 2020. Affirmative Algorithms: The Legal Grounds for Fairness as Awareness. U. Chi. L. Rev.\nOnline (2020), 134.\nJonathan Ho, Ajay Jain, and P. Abbeel. 2020. Denoising Diffusion Probabilistic Models. ArXiv abs/2006.11239 (2020).\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.\nBas Hofstra, Vivek V. Kulkarni, Sebastian Munoz-Najar Galvez, Bryan He, Dan Jurafsky, and Daniel A. McFarland. 2020. The\nDiversity–Innovation Paradox in Science. Proceedings of the National Academy of Sciences 117, 17 (April 2020), 9284–9291.\nhttps://doi.org/10.1073/pnas.1915378117\nFred Hohman, Minsuk Kahng, Robert Pienta, and Duen Horng Chau. 2018. Visual analytics in deep learning: An interrogative\nsurvey for the next frontiers. IEEE transactions on visualization and computer graphics 25, 8 (2018), 2674–2693.\nFred Hohman, Kanit Wongsuphasawat, Mary Beth Kery, and Kayur Patel. 2020. Understanding and visualizing data iteration\nin machine learning. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1–13.\nKaren D. Holl and Pedro H. S. Brancalion. 2020. Tree planting is not a simple solution.Science 368, 6491 (May 2020), 580–581.\nhttps://doi.org/10.1126/science.aba8232 ZSCC: 0000092 Publisher: American Association for the Advancement of Science\nSection: Perspective.\nSarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2018. The Dataset Nutrition Label: A\nFramework To Drive Higher Data Quality Standards. arXiv:1805.03677 [cs.DB]\nNora Hollenstein, Marius Troendle, Ce Zhang, and Nicolas Langer. 2020. ZuCo 2.0: A Dataset of Physiological Recordings\nDuring Natural Reading and Annotation. In Proceedings of the 12th Language Resources and Evaluation Conference .\nEuropean Language Resources Association, Marseille, France, 138–146.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2020. The Curious Case of Neural Text Degeneration. In\nInternational Conference on Learning Representations (ICLR) .\nNils Holzenberger, Andrew Blair-Stanek, and Benjamin Van Durme. 2020. A dataset for statutory reasoning in tax law\nentailment and question answering. arXiv preprint arXiv:2005.05257 (2020).\nAndreas Holzinger, Georg Langs, Helmut Denk, Kurt Zatloukal, and Heimo Müller. 2019. Causability and explainability\nof artificial intelligence in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 9, 4 (2019),\ne1312.\nSpurthi Amba Hombaiah, Tao Chen, Mingyang Zhang, Michael Bendersky, and Marc Najork. 2021. Dynamic Language\nModels for Continuously Evolving Content. CoRR abs/2106.06297 (2021). arXiv:2106.06297 https://arxiv.org/abs/2106.\n06297\nJoey Hong, David Dohan, Rishabh Singh, Charles Sutton, and Manzil Zaheer. 2021. Latent Programmer: Discrete Latent\nCodes for Program Synthesis. In International Conference in Machine Learning (ICML) .\n182 Center for Research on Foundation Models (CRFM)\nJason I Hong and James A Landay. 2004. An architecture for privacy-sensitive ubiquitous computing. In Proceedings of the\n2nd international conference on Mobile systems, applications, and services . 177–189.\nLu Hong and Scott E Page. 2004. Groups of diverse problem solvers can outperform groups of high-ability problem solvers.\nScience 101, 46 (2004).\nSanghyun Hong, Yiğitcan Kaya, Ionuţ-Vlad Modoranu, and Tudor Dumitraş. 2020a. A Panda? No, It’s a Sloth: Slowdown\nAttacks on Adaptive Multi-Exit Neural Network Inference. arXiv preprint arXiv:2010.02432 (2020).\nSongbai Hong, Guodong Yin, Shilong Piao, Ray Dybzinski, Nan Cong, Xiangyi Li, Kai Wang, Josep Peñuelas, Hui Zeng,\nand Anping Chen. 2020b. Divergent responses of soil organic carbon to afforestation. Nature Sustainability 3, 9 (2020),\n694–700.\nSara Hooker. 2020. The hardware lottery. arXiv preprint arXiv:2009.06489 (2020).\nSara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily L. Denton. 2020. Characterising Bias in Compressed\nModels. ArXiv abs/2010.03058 (2020). https://arxiv.org/abs/2010.03058\nEric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI Conference on Human Factors\nin Computing Systems . ACM.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. InProceedings of the 36th International\nConference on Machine Learning (Proceedings of Machine Learning Research, Vol. 97) , Kamalika Chaudhuri and Ruslan\nSalakhutdinov (Eds.). PMLR, 2790–2799. http://proceedings.mlr.press/v97/houlsby19a.html\nDirk Hovy and Anders Søgaard. 2015. Tagging Performance Correlates with Age. InAssociation for Computational Linguistics\n(ACL). 483–488.\nJeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Association for\nComputational Linguistics (ACL) .\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. LoRA:\nLow-Rank Adaptation of Large Language Models. CoRR abs/2106.09685 (2021). arXiv:2106.09685 https://arxiv.org/abs/\n2106.09685\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: A massively\nmultilingual multi-task benchmark for evaluating cross-lingual generalization. arXiv preprint arXiv:2003.11080 (2020).\nDaniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever. 2018. GamePad: A Learning Environment for Theorem\nProving. CoRR abs/1806.00608 (2018). arXiv:1806.00608 http://arxiv.org/abs/1806.00608\nJianping Huang, Haipeng Yu, Xiaodan Guan, Guoyin Wang, and Ruixia Guo. 2016. Accelerated dryland expansion under\nclimate change. Nature Climate Change 6, 2 (Feb. 2016), 166–171. https://doi.org/10.1038/nclimate2837 ZSCC: 0001034\nBandiera_abtest: a Cg_type: Nature Research Journals Number: 2 Primary_atype: Research Publisher: Nature Publishing\nGroup Subject_term: Developing world;Projection and prediction Subject_term_id: developing-world;projection-and-\nprediction.\nKexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun,\nand Marinka Zitnik. 2021a. Therapeutics data commons: machine learning datasets and tasks for therapeutics. arXiv\npreprint arXiv:2102.09548 (2021).\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam,\nQuoc V Le, Yonghui Wu, et al. 2019. GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism.\nAdvances in neural information processing systems 32 (2019), 103–112.\nZihan Huang, Charles Low, Mengqiu Teng, Hongyi Zhang, Daniel E Ho, Mark S Krass, and Matthias Grabmair. 2021b.\nContext-Aware Legal Citation Recommendation using Deep Learning. arXiv preprint arXiv:2106.10776 (2021).\nZhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. 2021c. Seeing Out of tHe bOx:\nEnd-to-End Pre-training for Vision-Language Representation Learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 12976–12985.\nEvan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. 2019. Risks from Learned\nOptimization in Advanced Machine Learning Systems. ArXiv abs/1906.01820 (2019).\nDrew Hudson and Christopher D Manning. 2019a. Learning by Abstraction: The neural state machine. In Advances in\nNeural Information Processing Systems . 5903–5916.\nDrew A Hudson and Christopher D Manning. 2018. Compositional Attention Networks for Machine Reasoning. In\nInternational Conference on Learning Representations (ICLR) .\nDrew A Hudson and Christopher D Manning. 2019b. GQA: A new dataset for real-world visual reasoning and compositional\nquestion answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 6700–6709.\nDrew A Hudson and C. Lawrence Zitnick. 2021. Generative Adversarial Transformers. Proceedings of the 38th International\nConference on Machine Learning, ICML 2021 (2021).\nJames P Hughes, Stephen Rees, S Barrett Kalindjian, and Karen L Philpott. 2011. Principles of early drug discovery. British\njournal of pharmacology 162, 6 (2011), 1239–1249.\nOn the Opportunities and Risks of Foundation Models 183\nBen Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl. 2020. Social\nBiases in NLP Models as Barriers for Persons with Disabilities. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics . Association for Computational Linguistics, Online, 5491–5501. https://doi.org/10.18653/\nv1/2020.acl-main.487\nJena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2021.\nCOMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs. InAAAI.\nJemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter.\n2019. Learning agile and dynamic motor skills for legged robots. Science Robotics 4, 26 (2019).\nJanet Shibley Hyde, Rebecca S. Bigler, Daphna Joel, Charlotte Chucky Tate, and Sari M. van Anders. 2019. The Future of Sex\nand Gender in Psychology: Five Challenges to the Gender Binary. American Psychologist 74 (2019), 171–193.\nH. Iida, Dung Thai, Varun Manjunatha, and Mohit Iyyer. 2021. TABBIE: Pretrained Representations of Tabular Data. In\nNAACL.\nRobert Ikeda and Jennifer Widom. 2010. Panda: A system for provenance and data. (2010).\nDaniela Ionescu et al. 2020. Deep learning algorithms and big health care data in clinical natural language processing.\nLinguistic and Philosophical Investigations 19 (2020), 86–92.\nDaphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and D. Eck. 2020. Automatic Detection of Generated Text is\nEasiest when Humans are Fooled. In ACL.\nRobert L. Logan IV, Ivana Balazevic, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. 2021. Cutting\nDown on Prompts and Parameters: Simple Few-Shot Learning with Language Models. CoRR abs/2106.13353 (2021).\narXiv:2106.13353 https://arxiv.org/abs/2106.13353\nRay Jackendoff. 2011. What is the human language faculty? Two views. Language 87, 3 (2011), 586–624. http://www.jstor.\norg/stable/23011656\nSimon Jackman. 2008. Measurement. Oxford Handbooks. https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/\n9780199286546.001.0001/oxfordhb-9780199286546-e-6\nAbigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT ’21). Association for Computing Machinery,\nNew York, NY, USA, 375–385. https://doi.org/10.1145/3442188.3445901\nAlon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate\nfaithfulness? arXiv preprint arXiv:2004.03685 (2020).\nAlon Jacovi, Ana Marasović, Tim Miller, and Yoav Goldberg. 2021. Formalizing trust in artificial intelligence: Prerequisites,\ncauses and goals of human trust in ai. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and\nTransparency. 624–635.\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula,\nDaniel Zoran, Andrew Brock, Evan Shelhamer, et al. 2021a. Perceiver IO: A General Architecture for Structured Inputs &\nOutputs. arXiv preprint arXiv:2107.14795 (2021).\nAndrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and João Carreira. 2021b. Perceiver: General\nPerception with Iterative Attention. In International Conference on Machine Learning (ICML) .\nM. Jamnik. 2001. Mathematical Reasoning with Diagrams.\nMichael Janner, Qiyang Li, and Sergey Levine. 2021. Reinforcement Learning as One Big Sequence Modeling Problem.ArXiv\nabs/2106.02039 (2021).\nTheo MV Janssen and Barbara H Partee. 1997. Compositionality. In Handbook of logic and language . Elsevier, 417–473.\nKarol Jaroch, Alina Jaroch, and Barbara Bojko. 2018. Cell cultures in drug discovery and development: The need of reliable\nin vitro-in vivo extrapolation for pharmacodynamics and pharmacokinetics assessment. Journal of Pharmaceutical and\nBiomedical Analysis 147 (2018), 297–312.\nShervin Javdani, Henny Admoni, Stefania Pellegrinelli, Siddhartha S Srinivasa, and J Andrew Bagnell. 2018. Shared autonomy\nvia hindsight optimization for teleoperation and teaming. International Journal of Robotics Research (IJRR) 37 (2018),\n717–742.\nSiddhant M Jayakumar, Razvan Pascanu, Jack W Rae, Simon Osindero, and Erich Elsen. 2021. Top-KAST: Top-K Always\nSparse Training. arXiv preprint arXiv:2106.03517 (2021).\nNeal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon. 2016. Combining satellite\nimagery and machine learning to predict poverty. Science 353, 6301 (2016), 790–794.\nEmily Jensen, Meghan Dale, Patrick J Donnelly, Cathlyn Stone, Sean Kelly, Amanda Godley, and Sidney K D’Mello. 2020.\nToward automated feedback on teacher discourse to enhance teacher learning. In Proceedings of the 2020 CHI Conference\non Human Factors in Computing Systems . 1–13.\nSooyeon Jeong, Kristopher Dos Santos, Suzanne Graca, Brianna O’Connell, Laurel Anderson, Nicole Stenquist, Katie\nFitzpatrick, Honey Goodenough, Deirdre Logan, Peter Weinstock, et al. 2015. Designing a socially assistive robot for\npediatric care. In Proceedings of the 14th international conference on interaction design and children . 387–390.\n184 Center for Research on Foundation Models (CRFM)\nY Ji, Z Zhou, H Liu, and RV Davuluri. 2021. DNABERT: pre-trained Bidirectional Encoder Representations from Transformers\nmodel for DNA-language in genome. Bioinformatics (2021).\nShengyu Jia, Tao Meng, Jieyu Zhao, and Kai-Wei Chang. 2020. Mitigating Gender Bias Amplification in Distribution by\nPosterior Regularization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Online, 2936–2942. https://doi.org/10.18653/v1/2020.acl-main.264\nZhihao Jia, James Thomas, Tod Warszawski, Mingyu Gao, Matei Zaharia, and Alex Aiken. 2019a. Optimizing DNN\nComputation with Relaxed Graph Substitutions. SysML 2019 (2019).\nZhihao Jia, Matei Zaharia, and Alex Aiken. 2019b. Beyond Data and Model Parallelism for Deep Neural Networks. SysML\n2019 (2019).\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How Can We Know What Language Models Know?\nTransactions of the Association for Computational Linguistics 8 (2020), 423–438. https://doi.org/10.1162/tacl_a_00324\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust? a strong baseline for natural language\nattack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34.\n8018–8025.\nQiao Jin, Zheng Yuan, Guangzhi Xiong, Qianlan Yu, Chuanqi Tan, Mosha Chen, Songfang Huang, Xiaozhong Liu, and Sheng\nYu. 2021. Biomedical question answering: A comprehensive review. arXiv preprint arXiv:2102.05281 (2021).\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. 2018. Junction tree variational autoencoder for molecular graph\ngeneration. In International Conference on Machine Learning . PMLR, 2323–2332.\nEun Seo Jo and Timnit Gebru. 2020. Lessons from archives: Strategies for collecting sociocultural data in machine learning.\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency . 306–316.\nGabbrielle M. Johnson. 2020. Algorithmic bias: on the implicit biases of social technology. Synthese (June 2020). https:\n//doi.org/10.1007/s11229-020-02696-y\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big\nData (2019).\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. Clevr:\nA diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference\non computer vision and pattern recognition . 2901–2910.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The State and Fate of Linguistic\nDiversity and Inclusion in the NLP World. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. 6282–6293.\nNorman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia,\nNan Boden, Al Borchers, et al. 2017. In-Datacenter Performance Analysis of a Tensor Processing Unit. In Proceedings of\nthe 44th Annual International Symposium on Computer Architecture . 1–12.\nKyle D. Julian and Mykel J. Kochenderfer. 2019. Guaranteeing Safety for Neural Network-Based Aircraft Collision Avoidance\nSystems. 2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC) (Sep 2019). https://doi.org/10.1109/dasc43569.\n2019.9081748\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger,\nRuss Bates, Augustin Žídek, Alex Bridgland, et al. 2020. High accuracy protein structure prediction using deep learning.\nFourteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstract Book) 22 (2020), 24.\nD. Jurafsky and J.H. Martin. 2009. Speech and Language Processing: An Introduction to Natural Language Processing, Compu-\ntational Linguistics, and Speech Recognition . Pearson Prentice Hall. https://books.google.gr/books?id=fZmj5UNK8AQC\nDavid Jurgens, Libby Hemphill, and Eshwar Chandrasekharan. 2019. A Just and Comprehensive Strategy for Using NLP\nto Address Online Abuse. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\nAssociation for Computational Linguistics, Florence, Italy, 3658–3666. https://doi.org/10.18653/v1/P19-1357\nArtur Kadurin, Sergey Nikolenko, Kuzma Khrabrov, Alex Aliper, and Alex Zhavoronkov. 2017. druGAN: an advanced\ngenerative adversarial autoencoder model for de novo generation of new molecules with desired molecular properties in\nsilico. Molecular pharmaceutics 14, 9 (2017), 3098–3104.\nLeslie Kaelbling. 1993. Learning to Achieve Goals. In International Joint Conference on Artificial Intelligence (IJCAI) .\nHilda Kajbaf. 2019. The First Amendment and Modern Technology: The Free Speech Clause and Chatbot Speech. Hastings\nConst. LQ 47 (2019), 337.\nDmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine,\nand Karol Hausman. 2021. MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale. arXiv preprint\narXiv:2104.08212 (2021).\nPratyusha Kalluri. 2020. Don’t ask if artificial intelligence is good or fair, ask how it shifts power. Nature 583, 169 (2020).\nhttps://doi.org/10.1038/d41586-020-02003-2\nDaniel Kang, Deepti Raghavan, Peter Bailis, and Matei Zaharia. 2020. Model Assertions for Monitoring and Improving ML\nModels. arXiv preprint arXiv:2003.01668 (2020).\nOn the Opportunities and Risks of Foundation Models 185\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford,\nJeffrey Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. ArXiv abs/2001.08361 (2020). https:\n//arxiv.org/abs/2001.08361\nSiddharth Karamcheti, Dorsa Sadigh, and Percy Liang. 2020. Learning Adaptive Language Interfaces through Decomposition.\nIn EMNLP Workshop for Interactive and Executable Semantic Parsing (IntEx-SemPar) .\nSiddharth Karamcheti, Edward C. Williams, Dilip Arumugam, Mina Rhee, Nakul Gopalan, Lawson L. S. Wong, and Stefanie\nTellex. 2017. A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting Action-Oriented and Goal-Oriented\nInstructions. In First Workshop on Language Grounding for Robotics @ ACL .\nSiddharth Karamcheti, Albert J. Zhai, Dylan P. Losey, and Dorsa Sadigh. 2021. Learning Visually Guided Latent Actions for\nAssistive Teleoperation. In Learning for Dynamics and Control .\nHolden Karnofsky. 2016. Potential risks from advanced artificial intelligence: the philanthropic opportunity. Open\nPhilanthropy Project 6 (2016).\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau\nYih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Online, 6769–6781.\nhttps://doi.org/10.18653/v1/2020.emnlp-main.550\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving\nthe image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\n8110–8119.\nNora Kassner, Philipp Dufter, and Hinrich Schütze. 2021. Multilingual LAMA: Investigating Knowledge in Multilingual\nPretrained Language Models. In EACL.\nHirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto, Eisuke Yamagata, Ryosuke Yamada, Nakamasa Inoue, Akio\nNakamura, and Yutaka Satoh. 2020. Pre-training without natural images. In Proceedings of the Asian Conference on\nComputer Vision .\nMichael Katell, Meg Young, Dharma Dailey, Bernease Herman, Vivian Guetler, Aaron Tam, Corinne Bintz, Daniella Raz, and\nP. M. Krafft. 2020. Toward situated interventions for algorithmic equity. In Proceedings of the 2020 Conference on Fairness,\nAccountability, and Transparency . ACM. https://doi.org/10.1145/3351095.3372874\nDaniel Martin Katz, Michael J Bommarito, and Josh Blackman. 2017. A general approach for predicting the behavior of the\nSupreme Court of the United States. PloS one 12, 4 (2017), e0174698.\nAmit Kaushal, Russ Altman, and Curt Langlotz. 2020. Geographic distribution of US cohorts used to train deep learning\nalgorithms. Jama 324, 12 (2020), 1212–1213.\nMatthew Kay, Cynthia Matuszek, and Sean A. Munson. 2015. Unequal Representation and Gender Stereotypes in Image\nSearch Results for Occupations. InProceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems\n(Seoul, Republic of Korea) (CHI ’15) . ACM, New York, NY, USA, 3819–3828. https://doi.org/10.1145/2702123.2702520\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim\nGreen, Trevor Back, Paul Natsev, et al. 2017. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950\n(2017).\nAlexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y Ng, and Pranav Rajpurkar. 2021. CheXtransfer: performance\nand parameter efficiency of ImageNet models for chest X-Ray interpretation. In Proceedings of the Conference on Health,\nInference, and Learning . 116–124.\nSean P Keehan, Gigi A Cuckler, John A Poisal, Andrea M Sisko, Sheila D Smith, Andrew J Madison, Kathryn E Rennie,\nJacqueline A Fiore, and James C Hardesty. 2020. National Health Expenditure Projections, 2019–28: Expected Rebound\nIn Prices Drives Rising Spending Growth: National health expenditure projections for the period 2019–2028. Health\nAffairs 39, 4 (2020), 704–714.\nPatrick Gage Kelley, Joanna Bresee, Lorrie Faith Cranor, and Robert W Reeder. 2009. A\" nutrition label\" for privacy. In\nProceedings of the 5th Symposium on Usable Privacy and Security . 1–12.\nTom Kenter, Melvin Wevers, Pim Huijnen, and Maarten De Rijke. 2015. Ad hoc monitoring of vocabulary shifts over time.\nIn Proceedings of the 24th ACM international on conference on information and knowledge management . 1191–1200.\nZachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving. 2021. Alignment of\nLanguage Agents. ArXiv abs/2103.14659 (2021).\nClark Kerr. 2001. The Uses of the University . Harvard University Press.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional\ntransformer language model for controllable generation. arXiv preprint arXiv:1909.05858 (2019).\nOs Keyes. 2018. The Misgendering Machines. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (Nov. 2018),\n1–22. https://doi.org/10.1145/3274357\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through\nMemorization: Nearest Neighbor Language Models. In International Conference on Learning Representations . https:\n186 Center for Research on Foundation Models (CRFM)\n//openreview.net/forum?id=HklBjCEKvH\nDaniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A Smith, and Daniel S\nWeld. 2021. Genie: A leaderboard for human-in-the-loop evaluation of text generation. arXiv preprint arXiv:2101.06561\n(2021).\nOmar Khattab, Christopher Potts, and Matei Zaharia. 2020. Relevance-Guided Supervision for OpenQA with ColBERT.\n(2020). arXiv:https://arxiv.org/abs/2007.00814\nO. Khattab and M. Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over\nBERT. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval\n(2020).\nBehrokh Khoshnevis. 2004. Automated construction by contour crafting—related robotics and information technologies.\nAutomation in construction 13, 1 (2004), 5–19.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad,\nAmanpreet Singh, Pratik Ringshia, et al . 2021. Dynabench: Rethinking benchmarking in NLP. arXiv preprint\narXiv:2104.14337 (2021).\nNajoung Kim and Tal Linzen. 2020. COGS: A Compositional Generalization Challenge Based on Semantic Interpretation. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 9087–9105.\nSeohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021b. Code prediction by feeding trees to transformers. In\n2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) . IEEE, 150–162.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021a. ViLT: Vision-and-language transformer without convolution or region\nsupervision. In International Conference on Machine Learning (ICML) .\nDiederik P. Kingma and Prafulla Dhariwal. 2018. Glow: Generative Flow with Invertible 1x1 Convolutions. In NeurIPS.\nDiederik P. Kingma and M. Welling. 2014. Auto-Encoding Variational Bayes. CoRR abs/1312.6114 (2014).\nDarrell G Kirch and Kate Petelle. 2017. Addressing the physician shortage: the peril of ignoring demography. Jama 317, 19\n(2017), 1947–1948.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan,\nJohn Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the national academy of sciences 114, 13 (2017), 3521–3526.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The Efficient Transformer.arXiv preprint arXiv:2001.04451\n(2020).\nPredrag Klasnja and Wanda Pratt. 2012. Healthcare in the pocket: mapping the space of mobile-phone health interventions.\nJournal of biomedical informatics 45, 1 (2012), 184–198.\nJon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2017. Inherent Trade-offs in the Fair Determination of Risk\nScores. In Innovations in Theoretical Computer Science (ITCS) .\nJon Kleinberg and Manish Raghavan. 2021. Algorithmic monoculture and social welfare. Proceed-\nings of the National Academy of Sciences 118, 22 (2021). https://doi.org/10.1073/pnas.2018340118\narXiv:https://www.pnas.org/content/118/22/e2018340118.full.pdf\nA. S. Klyubin, D. Polani, and Chrystopher L. Nehaniv. 2005. Empowerment: a universal agent-centric measure of control.\n2005 IEEE Congress on Evolutionary Computation 1 (2005), 128–135 Vol.1.\nRobert P Kocher. 2021. Reducing administrative waste in the US health care system. JAMA 325, 5 (2021), 427–428.\nKenneth R. Koedinger and John R. Anderson. 1990. Abstract planning and perceptual chunks: Elements of expertise in\ngeometry. Cognitive Science 14, 4 (1990), 511–550. https://doi.org/10.1016/0364-0213(90)90008-K\nAllison Koenecke, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor Toups, John R Rickford,\nDan Jurafsky, and Sharad Goel. 2020. Racial disparities in automated speech recognition. Proceedings of the National\nAcademy of Sciences 117, 14 (2020), 7684–7689.\nPang Wei Koh and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In International\nConference on Machine Learning (ICML) .\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu,\nMichihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A.\nEarnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn,\nand Percy Liang. 2021. WILDS: A Benchmark of in-the-Wild Distribution Shifts. In International Conference on Machine\nLearning (ICML) .\nEric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017a. AI2-THOR: An Interactive\n3D Environment for Visual AI. arXiv preprint arXiv:1712.05474 (2017).\nEric Kolve, R. Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav\nGupta, and Ali Farhadi. 2017b. AI2-THOR: An Interactive 3D Environment for Visual AI. ArXiv abs/1712.05474 (2017).\nJun Kong, Lee AD Cooper, Fusheng Wang, David A Gutman, Jingjing Gao, Candace Chisolm, Ashish Sharma, Tony Pan,\nErwin G Van Meir, Tahsin M Kurc, et al. 2011. Integrative, multimodal analysis of glioblastoma using TCGA molecular\nOn the Opportunities and Risks of Foundation Models 187\ndata, pathology images, and clinical outcomes. IEEE Transactions on Biomedical Engineering 58, 12 (2011), 3469–3474.\nDiane M Korngiebel and Sean D Mooney. 2021. Considering the possibilities and pitfalls of Generative Pre-trained\nTransformer 3 (GPT-3) in healthcare delivery. NPJ Digital Medicine 4, 1 (2021), 1–3.\nChristine Korsgaard. 2009. Self-constitution : agency, identity, and integrity . Oxford University Press, Oxford New York.\nSE Kreps and DL Kriner. 2020. Model uncertainty, political contestation, and public trust in science: Evidence from the\nCOVID-19 pandemic. Science advances 6, 43 (2020), eabd4563.\nSarah Kreps, R. Miles McCain, and Miles Brundage. 2020. All the News That’s Fit to Fabricate: AI-Generated Text as a Tool\nof Media Misinformation. Journal of Experimental Political Science (2020), 1–14. https://doi.org/10.1017/XPS.2020.37\nKundan Krishna, Sopan Khosla, Jeffrey P Bigham, and Zachary C Lipton. 2020. Generating soap notes from doctor-patient\nconversations. arXiv preprint arXiv:2005.01795 (2020).\nKalpesh Krishna, Gaurav Singh Tomar, Ankur P Parikh, Nicolas Papernot, and Mohit Iyyer. 2019. Thieves on sesame street!\nmodel extraction of bert-based apis. arXiv preprint arXiv:1910.12366 (2019).\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidi,\nLi-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. 2017. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International Journal of Computer Vision 123 (2017), 32–73.\nAlex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. (2009).\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural\nnetworks. Advances in neural information processing systems 25 (2012), 1097–1105.\nHarlan M Krumholz, Sharon F Terry, and Joanne Waldstreicher. 2016. Data acquisition, curation, and use for a continuously\nlearning health system. Jama 316, 16 (2016), 1669–1670.\nRohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Sanjeev Arora, and Rong Ge. 2019. Explaining\nlandscape connectivity of low-cost solutions for multilayer nets. arXiv preprint arXiv:1906.06247 (2019).\nTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for Neural Text Processing. In EMNLP.\nAnanya Kumar, Tengyu Ma, and Percy Liang. 2020a. Understanding Self-Training for Gradual Domain Adaptation. In\nInternational Conference on Machine Learning (ICML) .\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. 2022. Fine-Tuning Distorts\nPretrained Features and Underperforms Out-of-Distribution. In International Conference on Learning Representations\n(ICLR).\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. 2020b. Conservative Q-Learning for Offline Reinforcement\nLearning. (2020). https://arxiv.org/abs/2006.04779\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. 2019. Measuring bias in contextualized word\nrepresentations. arXiv preprint arXiv:1906.07337 (2019).\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of\nmachine learning. arXiv preprint arXiv:1910.09700 (2019).\nIro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. 2016. Deeper depth prediction\nwith fully convolutional residual networks. In 2016 Fourth international conference on 3D vision (3DV) . IEEE, 239–248.\nBrenden Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-\nsequence recurrent networks. In International conference on machine learning . PMLR, 2873–2882.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. 2015. Human-level concept learning through probabilistic\nprogram induction. Science 350, 6266 (2015), 1332–1338.\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. 2017. Building machines that learn and\nthink like people. Behavioral and Brain Sciences 40 (2017).\nGeorge Lakoff and Rafael Núñez. 2000. Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into\nBeing. Basic Books, New York. http://perso.unifr.ch/rafael.nunez/welcome.html\nSamuel Lalmuanawma, Jamal Hussain, and Lalrinfela Chhakchhuak. 2020. Applications of machine learning and artificial\nintelligence for Covid-19 (SARS-CoV-2) pandemic: A review. Chaos, Solitons & Fractals (2020), 110059.\nMadeline Lamo and Ryan Calo. 2019. Regulating bot speech. UCLA L. Rev. 66 (2019), 988.\nGuillaume Lample, Alexandre Sablayrolles, Marc’Aurelio Ranzato, Ludovic Denoyer, and H. Jégou. 2019. Large Memory\nLayers with Product Keys. In NeurIPS.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A Lite\nBERT for Self-supervised Learning of Language Representations. arXiv e-prints (2019), arXiv–1909.\nGert RG Lanckriet, Tijl De Bie, Nello Cristianini, Michael I Jordan, and William Stafford Noble. 2004. A statistical framework\nfor genomic data fusion. Bioinformatics 20, 16 (2004), 2626–2635.\nSascha Lange, Thomas Gabel, and Martin Riedmiller. 2012. Batch reinforcement learning. InReinforcement learning. Springer,\n45–73.\n188 Center for Research on Foundation Models (CRFM)\nLynn Langston and Donald J Farole, Jr. 2010. State Public Defender Programs, 2007 . Technical Report. U.S. Department of\nJustice Bureau of Justice Statistics.\nLoïc Lannelongue, Jason Grealey, and Michael Inouye. 2021. Green algorithms: Quantifying the carbon footprint of\ncomputation. Advanced Science (2021), 2100707.\nSebastian Lapuschkin, Stephan Wäldchen, Alexander Binder, Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller.\n2019. Unmasking Clever Hans predictors and assessing what machines really learn. Nature communications 10, 1 (2019),\n1–8.\nJill H. Larkin and Herbert A. Simon. 1987. Why a Diagram is (Sometimes) Worth Ten Thousand\nWords. Cognitive Science 11, 1 (1987), 65–100. https://doi.org/10.1111/j.1551-6708.1987.tb00863.x\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1551-6708.1987.tb00863.x\nJosh Lauer. 2017. Creditworthy: A History of Consumer Surveillance and Financial Identity in America . Columbia University\nPress. http://www.jstor.org/stable/10.7312/laue16808\nBenjamin Laufer. 2020. Feedback Effects in Repeat-Use Criminal Risk Assessments. arXiv:2011.14075 [cs.CY]\nAnne Lauscher, Vinit Ravishankar, Ivan Vulić, and Goran Glavaš. 2020. From Zero to Hero: On the Limitations of Zero-Shot\nLanguage Transfer with Multilingual Transformers. InProceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) . 4483–4499.\nAdam Lavertu and Russ B Altman. 2019. RedMed: Extending drug lexicons for social media applications. Journal of\nbiomedical informatics 99 (2019), 103307.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez,\nCyprien de Masson d’Autume, Sebastian Ruder, Dani Yogatama, et al. 2021. Pitfalls of Static Language Modelling. arXiv\npreprint arXiv:2102.01951 (2021).\nTeven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth?. InProceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association\nfor Computational Linguistics, Online, 2627–2636. https://doi.org/10.18653/v1/2021.naacl-main.208\nTosca Lechner, Shai Ben-David, Sushant Agarwal, and Nivasini Ananthakrishnan. 2021. Impossibility results for fair\nrepresentations. ArXiv abs/2107.03483 (2021).\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature 521, 7553 (2015).\nYann LeCun, John S Denker, and Sara A Solla. 1990. Optimal Brain Damage. In Advances in Neural Information Processing\nSystems. 598–605.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020b. BioBERT: a\npre-trained biomedical language representation model for biomedical text mining.Bioinformatics 36, 4 (2020), 1234–1240.\nJason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. 2020a. Predicting what you already know helps: Provable\nself-supervised learning. arXiv preprint arXiv:2008.01064 (2020).\nJieh-Sheng Lee and Jieh Hsiang. 2019. Patentbert: Patent classification with fine-tuning a pre-trained bert model. arXiv\npreprint arXiv:1906.02124 (2019).\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.\n2021b. Deduplicating Training Data Makes Language Models Better. arXiv:2107.06499 [cs.CL]\nMina Lee, Percy Liang, and Qian Yang. 2022. CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring\nLanguage Model Capabilities. Conference on Human Factors in Computing Systems (CHI) (May 2022). https://doi.org/10.\n1145/3491102.3502030\nMin Kyung Lee, Daniel Kusbit, Anson Kahng, Ji Tae Kim, Xinran Yuan, Allissa Chan, Daniel See, Ritesh Noothigattu, Siheon\nLee, Alexandros Psomas, and Ariel D. Procaccia. 2019. WeBuildAI: Participatory framework for algorithmic governance.\nCSCW (2019).\nNayeon Lee, Yejin Bang, Andrea Madotto, Madian Khabsa, and Pascale Fung. 2021a. Towards Few-shot Fact-Checking via\nPerplexity. In NAACL.\nYong Jae Lee, Joydeep Ghosh, and K. Grauman. 2012. Discovering important people and objects for egocentric video\nsummarization. 2012 IEEE Conference on Computer Vision and Pattern Recognition (2012), 1346–1353.\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. 2021. FNet: Mixing Tokens with Fourier Transforms.\narXiv preprint arXiv:2105.03824 (2021).\nNorman Lefstein and Robert L Spagenberg. 2009. Justice Denied: America’s Continuing Neglect of Our Constitutional Right\nto Counsel . Technical Report. National Right to Counsel Committee, The Constitution Project, National Legal Aid &\nDefender Association.\nLegal Services Corporation. 2017.The Justice Gap: Measuring the Unmet Civil Legal Needs of Low-income Americans . Technical\nReport. Prepared by NORC at the University of Chicago for Legal Services Corporation, Washington, DC.\nJ. Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and S. Legg. 2018. Scalable agent alignment via reward\nmodeling: a research direction. ArXiv abs/1811.07871 (2018).\nOn the Opportunities and Risks of Foundation Models 189\nSpyretta Leivaditi, Julien Rossi, and Evangelos Kanoulas. 2020. A Benchmark for Lease Contract Review. arXiv preprint\narXiv:2010.10386 (2020).\nMark A Lemley and Bryan Casey. 2019. Remedies for robots. The University of Chicago Law Review 86, 5 (2019), 1311–1396.\nMark A Lemley and Bryan Casey. 2020. Fair Learning. Tex. L. Rev. 99 (2020), 743.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer,\nand Zhifeng Chen. 2020. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. arXiv\npreprint arXiv:2006.16668 (2020).\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer,\nand Zhifeng Chen. 2021. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. In\nInternational Conference on Learning Representations .\nLawrence Lessig. 2000. Code Is Law. https://www.harvardmagazine.com/2000/01/code-is-law-html.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning.\narXiv:2104.08691 [cs.CL]\nAmanda Levendowski. 2018. How copyright law can fix artificial intelligence’s implicit bias problem. Wash. L. Rev. 93\n(2018), 579.\nSergey Levine, P. Pastor, A. Krizhevsky, and Deirdre Quillen. 2018. Learning hand-eye coordination for robotic grasping\nwith deep learning and large-scale data collection. The International Journal of Robotics Research 37 (2018), 421 – 436.\nYoav Levine, Noam Wies, Or Sharir, Hofit Bata, and Amnon Shashua. 2020. Limits to depth efficiencies of self-attention.\narXiv e-prints (2020), arXiv–2006.\nRoger Levy. 2008. Expectation-based syntactic comprehension. Cognition 106, 3 (2008), 1126–1177. https://doi.org/10.1016/\nj.cognition.2007.05.006\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke\nZettlemoyer. 2020a. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation,\nand Comprehension. In Association for Computational Linguistics (ACL) .\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike\nLewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-Augmented Generation\nfor Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems , H. Larochelle, M. Ranzato,\nR. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 9459–9474. https://proceedings.neurips.cc/\npaper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf\nIrene Li, Michihiro Yasunaga, Muhammed Yavuz Nuzumlalı, Cesar Caraballo, Shiwani Mahajan, Harlan Krumholz, and\nDragomir Radev. 2019. A neural topic-attention model for medical term abbreviation disambiguation. Machine Learning\nfor Health (ML4H) (2019).\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2015. Visualizing and understanding neural models in nlp. arXiv\npreprint arXiv:1506.01066 (2015).\nJialu Li, Esin Durmus, and Claire Cardie. 2020a. Exploring the Role of Argument Structure in Online Debate Persuasion.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for\nComputational Linguistics, Online, 8905–8912. https://doi.org/10.18653/v1/2020.emnlp-main.716\nShen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan,\nPritam Damania, et al. 2020e. PyTorch Distributed: Experiences on Accelerating Data Parallel Training. arXiv preprint\narXiv:2006.15704 (2020).\nWenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. 2021b. IsarStep: a Benchmark for High-level Mathematical\nReasoning. In International Conference on Learning Representations . https://openreview.net/forum?id=Pzj6fzU6wkj\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. 2022. Large Language Models Can Be Strong Differ-\nentially Private Learners. In International Conference on Learning Representations . https://openreview.net/forum?id=\nbVuP3ltATMz\nXuezixiang Li, Qu Yu, and Heng Yin. 2021a. PalmTree: Learning an Assembly Language Model for Instruction Embedding.\nCoRR abs/2103.03809 (2021). arXiv:2103.03809 https://arxiv.org/abs/2103.03809\nXiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. arXiv preprint\narXiv:2101.00190 (2021).\nYunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel L.K. Yamins, Jiajun Wu, Joshua B. Tenenbaum, and Antonio Torralba.\n2020b. Visual Grounding of Learned Physical Models. In ICML.\nYikuan Li, Shishir Rao, Jose Roberto Ayala Solares, Abdelaali Hassaine, Rema Ramakrishnan, Dexter Canoy, Yajie Zhu,\nKazem Rahimi, and Gholamreza Salimi-Khorshidi. 2020c. BEHRT: transformer for electronic health records. Scientific\nreports 10, 1 (2020), 1–12.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E Gonzalez. 2020d. Train Large, then\nCompress: Rethinking Model Size for Efficient Training and Inference of Transformers. arXiv preprint arXiv:2002.11794\n(2020).\n190 Center for Research on Foundation Models (CRFM)\nJindřich Libovick`y, Rudolf Rosa, and Alexander Fraser. 2019. How language-neutral is multilingual BERT? arXiv preprint\narXiv:1911.03310 (2019).\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical Details and Evaluation . White Paper.\nAI21 Labs.\nChu-Cheng Lin, Aaron Jaech, Xin Li, Matt Gormley, and Jason Eisner. 2021. Limitations of Autoregressive Models and Their\nAlternatives. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (NAACL-HLT) . Online, 5147–5173. http://cs.jhu.edu/~jason/papers/#lin-et-al-\n2021-naacl\nRo-Ting Lin, David C Christiani, Ichiro Kawachi, Ta-Chien Chan, Po-Huang Chiang, and Chang-Chuan Chan. 2016. Increased\nrisk of respiratory mortality associated with the high-tech manufacturing industry: A 26-Year study.International journal\nof environmental research and public health 13, 6 (2016), 557.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\nZitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision . Springer, 740–755.\nPantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. 2021. Explainable AI: A Review of Machine Learning\nInterpretability Methods. Entropy 23, 1 (2021), 18.\nLinda L. Lindsey. 2015. The sociology of Gender Theoretical Perspectives and Feminist Frameworks . Routledge. https:\n//www.routledge.com/Gender-Sociological-Perspectives/Lindsey/p/book/9781138103696\nWang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočisk`y, Andrew Senior, Fumin Wang, and Phil Blunsom.\n2016. Latent predictor networks for code generation. arXiv preprint arXiv:1603.06744 (2016).\nTal Linzen. 2020. How Can We Accelerate Progress Towards Human-like Linguistic Generalization?. InProceedings of the\n58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online,\n5210–5217. https://doi.org/10.18653/v1/2020.acl-main.465\nTal Linzen and Marco Baroni. 2021. Syntactic structure from deep learning. Annual Review of Linguistics 7 (2021), 195–212.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive\ndependencies. Transactions of the Association for Computational Linguistics (TACL) 4 (2016).\nMarco Lippi, Przemysław Pałka, Giuseppe Contissa, Francesca Lagioia, Hans-Wolfgang Micklitz, Giovanni Sartor, and Paolo\nTorroni. 2019. CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service. Artificial\nIntelligence and Law 27, 2 (2019), 117–139.\nZachary C. Lipton. 2018. The mythos of model interpretability. Commun. ACM 61, 10 (Sept. 2018), 36–43. https:\n//doi.org/10.1145/3233231\nZachary C. Lipton and Jacob Steinhardt. 2019. Troubling Trends in Machine Learning Scholarship: Some ML Papers\nSuffer from Flaws That Could Mislead the Public and Stymie Future Research. Queue 17, 1 (Feb. 2019), 45–77. https:\n//doi.org/10.1145/3317287.3328534\nAndy T. Liu, Shuwen Yang, Po-Han Chi, Po-Chun Hsu, and Hung yi Lee. 2020d. Mockingjay: Unsupervised Speech\nRepresentation Learning with Deep Bidirectional Transformer Encoders.ICASSP 2020 - 2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) (2020), 6419–6423.\nFenglin Liu, Shen Ge, and Xian Wu. 2021a. Competence-based Multimodal Curriculum Learning for Medical Report\nGeneration. In ACL.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021d. What Makes Good\nIn-Context Examples for GPT-3?. In ArXiv.\nNelson F. Liu, Tony Lee, Robin Jia, and Percy Liang. 2021b. Can Small and Synthetic Benchmarks Drive Modeling\nInnovation? A Retrospective Study of Question Answering Modeling Approaches. ArXiv abs/2102.01065 (2021). https:\n//arxiv.org/abs/2102.01065\nRuishan Liu, Shemra Rizzo, Samuel Whipple, Navdeep Pal, Arturo Lopez Pineda, Michael Lu, Brandon Arnieri, Ying Lu,\nWilliam Capra, Ryan Copping, et al. 2021c. Evaluating eligibility criteria of oncology trials using real-world data and AI.\nNature 592, 7855 (2021), 629–633.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and P. Wang. 2020e. K-BERT: Enabling Language\nRepresentation with Knowledge Graph. ArXiv abs/1909.07606 (2020).\nXuebo Liu, Longyue Wang, Derek F Wong, Liang Ding, Lidia S Chao, and Zhaopeng Tu. 2020c. Understanding and Improving\nEncoder Layer Fusion in Sequence-to-Sequence Learning. arXiv preprint arXiv:2012.14768 (2020).\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021e. GPT Understands, Too.\nCoRR abs/2103.10385 (2021). arXiv:2103.10385 https://arxiv.org/abs/2103.10385\nYe Liu, Shaika Chowdhury, Chenwei Zhang, Cornelia Caragea, and Philip S Yu. 2020a. Interpretable Multi-Step Reasoning\nwith Knowledge Extraction on Complex Healthcare Question Answering. arXiv preprint arXiv:2008.02434 (2020).\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.\n2020b. Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the Association for Computa-\ntional Linguistics 8 (2020), 726–742. https://doi.org/10.1162/tacl_a_00343\nOn the Opportunities and Risks of Foundation Models 191\nYuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. 2018. Imitation from observation: Learning to imitate\nbehaviors from raw video via context translation. In 2018 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 1118–1125.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and\nVeselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692\n(2019).\nJane Loevinger. 1957. Objective Tests as Instruments of Psychological Theory. Psychological Reports 3, 3 (1957), 635–694.\nhttps://doi.org/10.2466/pr0.1957.3.3.635 arXiv:https://doi.org/10.2466/pr0.1957.3.3.635\nLajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019. Zero-Shot\nEntity Linking by Reading Entity Descriptions. In arXiv:1906.07348v1.\nHelen Longino. 1990. Science as Social Knowledge: Values and Objectivity in Scientific Inquiry . Princeton University Press,\nPrinceton.\nLiane Longpre, Esin Durmus, and Claire Cardie. 2019. Persuasion of the Undecided: Language vs. the Listener. InProceedings\nof the 6th Workshop on Argument Mining . Association for Computational Linguistics, Florence, Italy, 167–176. https:\n//doi.org/10.18653/v1/W19-4519\nDavid Lopez-Paz and Marc’Aurelio Ranzato. 2017. Gradient Episodic Memory for Continual Learning. In NIPS.\nKadan Lottick, Silvia Susai, Sorelle A. Friedler, and Jonathan P. Wilson. 2019. Energy Usage Reports: Environmental\nawareness as part of algorithmic accountability. Workshop on Tackling Climate Change with Machine Learning at NeurIPS\n2019 (2019).\nDavid G Lowe. 1992. Robust model-based motion tracking through the integration of search and estimation. International\nJournal of Computer Vision 8, 2 (1992), 113–122.\nDavid G Lowe. 1999 1999. Object recognition from local scale-invariant features. In International Conference on Computer\nVision (ICCV) Proceedings of the seventh IEEE international conference on computer vision , Vol. 2. 1150–1157.\nDavid G Lowe. 2004. Distinctive image features from scale-invariant keypoints. International journal of computer vision 60,\n2 (2004), 91–110.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019a. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representa-\ntions for Vision-and-Language Tasks. In NeurIPS.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. 2021a. Pretrained Transformers as Universal Computation\nEngines. CoRR abs/2103.05247 (2021). arXiv:2103.05247 https://arxiv.org/abs/2103.05247\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta. 2020. Gender Bias in Neural Natural\nLanguage Processing. In Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th\nBirthday, Vivek Nigam, Tajana Ban Kirigin, Carolyn Talcott, Joshua Guttman, Stepan Kuznetsov, Boon Thau Loo, and\nMitsuhiro Okada (Eds.). Springer International Publishing, Cham, 189–202. https://doi.org/10.1007/978-3-030-62077-6_14\nLu Lu, Pengzhan Jin, and George Em Karniadakis. 2019b. Deeponet: Learning nonlinear operators for identifying differential\nequations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193 (2019).\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin\nJiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel\nSundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021b. CodeXGLUE: A Machine Learning Benchmark Dataset\nfor Code Understanding and Generation. CoRR abs/2102.04664 (2021). arXiv:2102.04664 https://arxiv.org/abs/2102.04664\nKristian Lum and William Isaac. 2016. To predict and serve? Significance 13, 5 (2016), 14–19.\nScott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Proceedings of the 31st\ninternational conference on neural information processing systems . 4768–4777.\nHuaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. 2020.\nUnivl: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint\narXiv:2002.06353 (2020).\nCorey Lynch and Pierre Sermanet. 2020. Grounding Language in Play. arXiv preprint arXiv:2005.07648 (2020).\nCorey Lynch and Pierre Sermanet. 2021. Language Conditioned Imitation Learning over Unstructured Data. (2021).\nKalle Lyytinen and Youngjin Yoo. 2002. Ubiquitous computing. Commun. ACM 45, 12 (2002), 63–96.\nM2M-100. 2020. M2M-100: AI Model That Translates 100 Languages Without Relying on English. https://about.fb.com/\nnews/2020/10/first-multilingual-machine-translation-model/.\nEdward Ma. 2019. NLP Augmentation. https://github.com/makcedward/nlpaug.\nJianzhu Ma, Samson H Fong, Yunan Luo, Christopher J Bakkenist, John Paul Shen, Soufiane Mourragui, Lodewyk FA Wessels,\nMarc Hafner, Roded Sharan, Jian Peng, et al. 2021b. Few-shot learning creates predictive models of drug response that\ntranslate from high-throughput screens to individual patients. Nature Cancer 2, 2 (2021), 233–244.\nZhiyi Ma, Kawin Ethayarajh, Tristan Thrush, Somya Jain, Ledell Wu, Robin Jia, Christopher Potts, Adina Williams, and\nDouwe Kiela. 2021a. Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking.\nArXiv abs/2106.06052 (2021). https://arxiv.org/abs/2106.06052\n192 Center for Research on Foundation Models (CRFM)\nRyan Mac, Caroline Haskins, Brianna Sacks, and Logan McDonald. 2021. Surveillance Nation. Buzzfeed News (9 April 2021).\nhttps://www.buzzfeednews.com/article/ryanmac/clearview-ai-local-police-facial-recognition Accessed 2021-07-18.\nJ. MacGlashan, Monica Babes-Vroman, M. desJardins, M. Littman, S. Muresan, S. Squire, Stefanie Tellex, Dilip Arumugam,\nand Lei Yang. 2015. Grounding English Commands to Reward Functions. In Robotics: Science and Systems (RSS) .\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards deep learning\nmodels resistant to adversarial attacks. In International Conference on Learning Representations (ICLR) .\nJeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, R. Doan, Xinyu Liu, J. A. Ojea, and Ken Goldberg. 2017. Dex-Net\n2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics. ArXiv abs/1703.09312\n(2017).\nAli Malik, Mike Wu, Vrinda Vasavada, Jinpeng Song, Madison Coots, John Mitchell, Noah Goodman, and Chris Piech. 2021.\nGenerative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems. InProceedings\nof the 14th International Conference on Educational Data Mining .\nVittorio Caggiano Naman Goyal Siddharth Goyal Myle Ott Benjamin Lefaudeux Vitaliy Liptchinsky Mike Rabbat Sam\nSheiffer Anjali Sridhar Min Xu Mandeep Baines, Shruti Bhosale. 2021. FairScale: A General Purpose Modular PyTorch\nLibrary for High Performance and Large Scale Training. https://github.com/facebookresearch/fairscale.\nTravis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. 2014. Offline Policy Evaluation across Rep-\nresentations with Applications to Educational Games. In Proceedings of the 2014 International Conference on Autonomous\nAgents and Multi-Agent Systems (Paris, France) (AAMAS ’14). International Foundation for Autonomous Agents and\nMultiagent Systems, Richland, SC, 1077–1084.\nAjay Mandlekar, Jonathan Booher, Max Spero, Albert Tung, Anchit Gupta, Yuke Zhu, Animesh Garg, Silvio Savarese, and Li\nFei-Fei. 2019. Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic Manipulation Dataset through\nHuman Reasoning and Dexterity. In International Conference on Intelligent Robots and Systems (IROS) .\nChristopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. 2020. Emergent linguistic structure\nin artificial neural networks trained by self-supervision. Proceedings of the National Academy of Sciences 117 (2020),\n30046–30054.\nMarketLine. 2021. Legal Services in the United States. https://www.marketresearch.com/MarketLine-v3883/Legal-Services-\nUnited-States-14193556/\nBernard Marr. 2017. Really Big Data At Walmart: Real-Time Insights From Their 40+ Petabyte Data Cloud. https://www.forbes.\ncom/sites/bernardmarr/2017/01/23/really-big-data-at-walmart-real-time-insights-from-their-40-petabyte-data-cloud.\nDavid Marr. 1982. Vision: A computational investigation into the human representation and processing of visual information .\nW.H. Freeman, San Francisco.\nRoberto Martin-Martin, Mihir Patel, Hamid Rezatofighi, Abhijeet Shenoi, JunYoung Gwak, Eric Frankel, Amir Sadeghian,\nand Silvio Savarese. 2021. JRDB: A Dataset and Benchmark of Egocentric Robot Visual Perception of Humans in Built\nEnvironments. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) (2021).\nNicole Martinez-Martin, Zelun Luo, Amit Kaushal, Ehsan Adeli, Albert Haque, Sara S Kelly, Sarah Wieten, Mildred K Cho,\nDavid Magnus, Li Fei-Fei, et al. 2020. Ethical issues in using ambient intelligence in health-care settings. The Lancet\nDigital Health (2020).\nRebecca Marvin and Tal Linzen. 2018. Targeted Syntactic Evaluation of Language Models. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Brussels,\nBelgium, 1192–1202. https://doi.org/10.18653/v1/D18-1151\nToni M Massaro, Helen Norton, and Margot E Kaminski. 2016. SIRI-OUSLY 2.0: what artificial intelligence reveals about the\nfirst amendment. Minn. L. Rev. 101 (2016), 2481.\nV. Masson-Delmotte, P. Zhai, A. Pirani, S. L. Connors, C. P/‘ean, S. Berger, N. Caud, Y. Chen, L. Goldfarb, M. I. Gomis, M.\nHuang, K. Leitzell, E. Lonnoy, J. B. R. Matthews, T. K. Maycock, T. Waterfield, O. Yelekci, R. Yu, and B. Zhou (Eds.). 2021.\nIPCC, 2021: Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment\nReport of the Intergovernmental Panel on Climate Change. (2021).\nDaniel Masur. 2018. Data Licensing—Tips and Tactics. Corporate Compliance Insights (2018).\nPeter Mattson, Christine Cheng, Cody Coleman, Greg Diamos, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon\nWei, Peter Bailis, Victor Bittorf, et al. 2020. MLPerf Training Benchmark. In Third Conference on Machine Learning and\nSystems.\nChandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. 2019. On Measuring Social Biases in\nSentence Encoders. InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for Computational Linguistics,\nMinneapolis, Minnesota, 622–628. https://doi.org/10.18653/v1/N19-1063\nJS McCarley, Rishav Chakravarti, and Avirup Sil. 2019. Structured Pruning of a BERT-Based Question Answering Model.\narXiv preprint arXiv:1910.06360 (2019).\nOn the Opportunities and Risks of Foundation Models 193\nJames L McClelland and David E Rumelhart. 1981. An interactive activation model of context effects in letter perception: I.\nAn account of basic findings. Psychological review 88, 5 (1981), 375.\nMichael McCloskey and Neal J. Cohen. 1989. Catastrophic Interference in Connectionist Networks: The Sequential Learning\nProblem. Psychology of Learning and Motivation, Vol. 24. Academic Press, 109–165. https://doi.org/10.1016/S0079-\n7421(08)60536-8\nJamie McKenzie. 2003. Pedagogy Does Matter! The Educational Technology Journal 13, 1 (2003).\nWarwick McKibbin, Roshen Fernando, et al. 2020. The economic impact of COVID-19. Economics in the Time of COVID-19\n45, 10.1162 (2020).\nH Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2018. Learning differentially private recurrent language\nmodels. In International Conference on Learning Representations .\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and Noah A Smith. 2021. Provable Limitations of Acquiring Meaning from\nUngrounded Form: What will Future Language Models Understand? arXiv preprint arXiv:2104.10809 (2021).\nRobert K Merton. 1979. The normative structure of science. The sociology of science: Theoretical and empirical investigations\n(1979), 267–278.\nSamuel Messick. 1987. Validity. ETS Research Report Series 1987, 2 (1987), i–208. https://onlinelibrary.wiley.com/doi/abs/10.\n1002/j.2330-8516.1987.tb00244.x\nSamuel Messick. 1988. The once and future issues of validity: Assessing the meaning and consequences of measurement.\nETS Research Report Series (1988). https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2330-8516.1986.tb00185.x\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael\nHouston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. 2017. Mixed precision training. arXiv preprint arXiv:1710.03740\n(2017).\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector\nSpace. arXiv preprint arXiv:1301.3781 (2013).\nTomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocký, and Sanjeev Khudanpur. 2010. Recurrent neural network\nbased language model. Proceedings of the 11th Annual Conference of the International Speech Communication Association,\nINTERSPEECH 2010 2, 1045–1048.\nSilvia Milano, Mariarosaria Taddeo, and Luciano Floridi. 2020. Recommender systems and their ethical challenges. AI &\nSOCIETY 35, 4 (2020), 957–967.\nAlexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-Value\nMemory Networks for Directly Reading Documents. InProceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing . 1400–1409.\nG. A. Miller, Galanter E., and K. H. Pribram. 1960. Plans and the structure of behavior . Holt, New York.\nJohn Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and\nLudwig Schmidt. 2021. Accuracy on the Line: on the Strong Correlation Between Out-of-Distribution and In-Distribution\nGeneralization. In International Conference on Machine Learning (ICML) .\nSmitha Milli, Luca Belli, and Moritz Hardt. 2021. From Optimizing Engagement to Measuring Value. In Proceedings of the\n2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT ’21). Association for\nComputing Machinery, New York, NY, USA, 714–722. https://doi.org/10.1145/3442188.3445933\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2021. Noisy Channel Language Model Prompting for\nFew-Shot Text Classification. arXiv:2108.04106 [cs.CL]\nDipendra Misra, John Langford, and Yoav Artzi. 2017b. Mapping instructions and visual observations to actions with\nreinforcement learning. arXiv preprint arXiv:1704.08795 (2017).\nIshan Misra, Abhinav Gupta, and Martial Hebert. 2017a. From red wine to red tomato: Composition with context. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 1792–1801.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. 2021. Fast Model Editing at Scale.\nIn International Conference on Learning Represenations . arXiv:2110.11309 https://arxiv.org/abs/2110.11309\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, In-\nioluwa Deborah Raji, and Timnit Gebru. 2019. Model Cards for Model Reporting. Proceedings of the Conference on\nFairness, Accountability, and Transparency (Jan 2019). https://doi.org/10.1145/3287560.3287596\nYasuhide Miura, Yuhao Zhang, Emily Bao Tsai, Curtis P Langlotz, and Dan Jurafsky. 2021. Improving factual completeness\nand consistency of image-to-text radiology report generation. NAACL (2021).\nPiero Molino, Yaroslav Dudin, and Sai Sumanth Miryala. 2019. Ludwig: A Type-Based Declarative Deep Learning Toolbox.\narXiv preprint arXiv:1909.07930 (2019).\nCamilo Mora, Randi L Rollins, Katie Taladay, Michael B Kantar, Mason K Chock, Mio Shimada, and Erik C Franklin. 2018.\nBitcoin emissions alone could push global warming above 2 C. Nature Climate Change 8, 11 (2018), 931–933.\nHans Moravec. 1988. Mind children: The future of robot and human intelligence . Harvard University Press.\n194 Center for Research on Foundation Models (CRFM)\nHesham Mostafa and Xin Wang. 2019. Parameter efficient training of deep convolutional neural networks by dynamic\nsparse reparameterization. In International Conference on Machine Learning . PMLR, 4646–4655. http://proceedings.mlr.\npress/v97/mostafa19a.html ZSCC: 0000081 ISSN: 2640-3498.\nHussein Mozannar and David Sontag. 2020. Consistent estimators for learning to defer to an expert. In International\nConference on Machine Learning . PMLR, 7076–7087.\nJesse Mu and Jacob Andreas. 2020. Compositional explanations of neurons. arXiv preprint arXiv:2006.14032 (2020).\nDheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade\nNie, Jongsoo Park, Liang Luo, et al . 2021. High-Performance, Distributed Training of Large-Scale Deep Learning\nRecommendation Models. arXiv preprint arXiv:2104.05158 (2021).\nBrad A. Myers, Scott E. Hudson, and Randy Pausch. 2000. Past, Present and Future of User Interface Software Tools. InACM\nTransactions on Computer Human Interaction . ACM.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models.\nIn Proceedings of ACL 2021 .\nVaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. 2020. Understanding the failure modes of out-of-\ndistribution generalization. arXiv preprint arXiv:2010.15775 (2020).\nArsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. 2021. Attention Bottlenecks for\nMultimodal Fusion. arXiv preprint arXiv:2107.00135 (2021).\nAshvin Nair, Vitchyr H. Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. 2018. Visual Reinforcement\nLearning with Imagined Goals. In NeurIPS.\nV. Nair and G. E. Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In International Conference on\nMachine Learning (ICML) . 807–814.\nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. 2019. Deep double descent:\nWhere bigger models and more data hurt. arXiv preprint arXiv:1912.02292 (2019).\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-Pairs: A Challenge Dataset for Measuring\nSocial Biases in Masked Language Models. InProceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP). Association for Computational Linguistics, Online, 1953–1967. https://doi.org/10.18653/v1/2020.\nemnlp-main.154\nArvind Narayanan, Arunesh Mathur, Marshini Chetty, and Mihir Kshirsagar. 2020. Dark Patterns: Past, Present, and Future.\nCommun. ACM 63, 9 (Aug. 2020), 42–47. https://doi.org/10.1145/3397884\nDeepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B\nGibbons, and Matei Zaharia. 2019. PipeDream: Generalized Pipeline Parallelism for DNN Training. In Proceedings of the\n27th ACM Symposium on Operating Systems Principles . 1–15.\nDeepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. 2021a. Memory-Efficient Pipeline-Parallel\nDNN Training. In International Conference on Machine Learning . PMLR, 7937–7947.\nDeepak Narayanan, Keshav Santhanam, Amar Phanishayee, and Matei Zaharia. 2018. Accelerating Deep Learning Workloads\nthrough Efficient Multi-Model Execution. In NeurIPS Workshop on Systems for Machine Learning . 20.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Anand Korthikanti,\nDmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021b. Efficient Large-Scale Language\nModel Training on GPU Clusters. arXiv preprint arXiv:2104.04473 (2021).\nJennifer C. Nash. 2008. Re-Thinking Intersectionality.Feminist Review 89, 1 (June 2008), 1–15. https://doi.org/10.1057/fr.2008.4\nMilad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Machine Learning with Membership Privacy Using Adversarial\nRegularization. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (Toronto,\nCanada) (CCS ’18). Association for Computing Machinery, New York, NY, USA, 634–646. https://doi.org/10.1145/3243734.\n3243855\nWilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi E Fasubaa, Taiwo Fagbohungbe, Solomon Oluwole\nAkinola, Shamsuddeen Hassan Muhammad, Salomon Kabongo Kabenamualu, Salomey Osei, Freshia Sackey, et al. 2020.\nParticipatory Research for Low-resourced Machine Translation: A Case Study in African Languages. InEMNLP (Findings).\nAndrew Y. Ng and Stuart Russell. 2000. Algorithms for Inverse Reinforcement Learning. In International Conference on\nMachine Learning .\nElhadji Mamadou Nguer, Alla Lo, Cheikh M Bamba Dione, Sileye O Ba, and Moussa Lo. 2020. SENCORPUS: A French-Wolof\nParallel Corpus. In Proceedings of the 12th Language Resources and Evaluation Conference . 2803–2811.\nAnh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. 2016. Synthesizing the preferred inputs for\nneurons in neural networks via deep generator networks. Advances in neural information processing systems 29 (2016),\n3387–3395.\nYizhao Ni, Stephanie Kennebeck, Judith W Dexheimer, Constance M McAneney, Huaxiu Tang, Todd Lingren, Qi Li,\nHaijun Zhai, and Imre Solti. 2015. Automated clinical trial eligibility prescreening: increasing the efficiency of patient\nidentification for clinical trials in the emergency department. Journal of the American Medical Informatics Association 22,\nOn the Opportunities and Risks of Foundation Models 195\n1 (2015), 166–178.\nAllen Nie, Ashley Zehnder, Rodney L Page, Yuhui Zhang, Arturo Lopez Pineda, Manuel A Rivas, Carlos D Bustamante, and\nJames Zou. 2018. DeepTag: inferring diagnoses from veterinary clinical notes. NPJ digital medicine 1, 1 (2018), 1–8.\nMathias Wullum Nielsen, Sharla Alegria, Love Börjeson, Henry Etzkowitz, Holly J. Falk-Krzesinski, Aparna Joshi, Erin\nLeahey, Laurel Smith-Doerr, Anita Williams Woolley, and Londa Schiebinger. 2017. Opinion: Gender diversity leads to\nbetter science. Proceedings of the National Academy of Sciences 114, 8 (Feb. 2017), 1740–1742. https://doi.org/10.1073/\npnas.1700616114\nHelen Nissenbaum. 2004. Privacy as contextual integrity. Wash. L. Rev. 79 (2004), 119.\nHelen Nissenbaum. 2009. Privacy in Context: Technology, Policy, and the Integrity of Social Life . Stanford University Press.\nMalvina Nissim, Rik van Noord, and Rob van der Goot. 2020. Fair Is Better than Sensational: Man Is to Doctor as Woman Is\nto Doctor. Computational Linguistics 46, 2 (June 2020), 487–497. https://doi.org/10.1162/coli_a_00379\nSafiya Umoja Noble. 2018. Algorithms of Oppression . New York University Press.\nShimon Y Nof. 1999. Handbook of industrial robotics . John Wiley & Sons.\nSebastian Nordhoff and Harald Hammarström. 2011. Glottolog/Langdoc: Defining dialects, languages, and language families\nas collections of resources. In First International Workshop on Linked Science 2011-In conjunction with the International\nSemantic Web Conference (ISWC 2011) .\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2021. HONEST: Measuring Hurtful Sentence Completion in Language\nModels. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies . Association for Computational Linguistics, Online, 2398–2406. https:\n//doi.org/10.18653/v1/2021.naacl-main.191\nMartha Craven Nussbaum. 2010. Not for profit: Why democracy needs the humanities . Princeton University Press.\nNVIDIA. 2021. NVIDIA Collective Communication Library (NCCL). https://developer.nvidia.com/nccl.\nNVIDIA and Microsoft. 2021. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest\nand Most Powerful Generative Language Model. https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-\ntrain-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/.\nJulian Nyarko and Sarath Sanga. 2020. A Statistical Test for Legal Interpretation: Theory and Applications. Available at\nSSRN 3737292 (2020).\nLuke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. 2019. Hidden Stratification Causes Clinically\nMeaningful Failures in Machine Learning for Medical Imaging. arXiv e-prints, art.arXiv preprint arXiv:1909.12475 (2019).\nDouglas W Oard, Fabrizio Sebastiani, and Jyothi K Vinjumur. 2018. Jointly minimizing the expected costs of review for\nresponsiveness and privilege in E-discovery. ACM Transactions on Information Systems (TOIS) 37, 1 (2018), 1–35.\nJonathan A. Obar and Anne Oeldorf-Hirsch. 2020. The biggest lie on the Internet: Ignoring the privacy policies and\nterms of service policies of social networking services. Information, Communication & Society 23, 1 (2020), 128–147.\nhttps://doi.org/10.1080/1369118X.2018.1486870\nCailin O’Connor, Liam Kofi Bright, and Justin P. Bruner. 2019. The Emergence of Intersectional Disadvantage. Social\nEpistemology 33, 1 (Jan. 2019), 23–41. https://doi.org/10.1080/02691728.2018.1555870\nU.S. Copyright Office. 2021. More Information on Fair Use. https://www.copyright.gov/fair-use/more-info.html.\nPaul Ohm. 2014. Changing the Rules: General Principles for Data Use and Analysis . Cambridge University Press, 96 – 111.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020. Zoom in: An introduction\nto circuits. Distill 5, 3 (2020), e00024–001.\nThiago Dias Oliva, Dennys Marcelo Antonialli, and Alessandra Gomes. 2021. Fighting hate speech, silencing drag queens?\nArtificial intelligence in content moderation and risks to LGBTQ voices online. Sexuality & Culture 25, 2 (2021), 700–732.\nS. Omohundro. 2008. The Basic AI Drives. In AGI.\nCathy O’Neil. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy . Crown\nPublishing Group, USA.\nOpenAI, I. Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino,\nMatthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, N. Tezak, Jerry Tworek, P. Welinder, Lilian Weng,\nQiming Yuan, Wojciech Zaremba, and Lei Zhang. 2019. Solving Rubik’s Cube with a Robot Hand. ArXiv abs/1910.07113\n(2019).\nYonatan Oren, Shiori Sagawa, Tatsunori Hashimoto, and Percy Liang. 2019. Distributionally Robust Language Modeling. In\nEmpirical Methods in Natural Language Processing (EMNLP) .\nLaurel Orr, Megan Leszczynski, Simran Arora, Sen Wu, Neel Guha, Xiao Ling, and Chris Ré. 2020. Bootleg: Chasing the Tail\nwith Self-Supervised Named Entity Disambiguation. In Arxiv.\nMalte Ostendorff, Elliott Ash, Terry Ruas, Bela Gipp, Julian Moreno-Schneider, and Georg Rehm. 2021. Evaluating Document\nRepresentations for Content-based Legal Literature Recommendations. arXiv preprint arXiv:2104.13841 (2021).\nDavid Ouyang, Bryan He, Amirata Ghorbani, Neal Yuan, Joseph Ebinger, Curtis P Langlotz, Paul A Heidenreich, Robert A\nHarrington, David H Liang, Euan A Ashley, et al. 2020. Video-based AI for beat-to-beat assessment of cardiac function.\n196 Center for Research on Foundation Models (CRFM)\nNature 580, 7802 (2020), 252–256.\nIsabel Papadimitriou, Ethan A Chi, Richard Futrell, and Kyle Mahowald. 2021. Deep Subjecthood: Higher-Order Grammatical\nFeatures in Multilingual BERT. In Proceedings of the 16th Conference of the European Chapter of the Association for\nComputational Linguistics: Main Volume . 2522–2532.\nIsabel Papadimitriou and Dan Jurafsky. 2020. Learning music helps you read: Using transfer to study linguistic structure in\nlanguage models. arXiv preprint arXiv:2004.14601 (2020).\nDenis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco\nBaroni, Gemma Boleda, and Raquel Fernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse\ncontext. In Association for Computational Linguistics (ACL) .\nNicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z.Berkay Celik, and Ananthram Swami. 2017. Practical\nBlack-Box Attacks against Deep Learning Systems using Adversarial Examples. InProceedings of the ACM Asia Conference\non Computer and Communications Security .\nTitouan Parcollet and Mirco Ravanelli. 2021. The Energy and Carbon Footprint of Training End-to-End Speech Recognizers.\n(2021).\nC.L. Paris, W.R. Swartout, and W.C. Mann. 2013. Natural Language Generation in Artificial Intelligence and Computational\nLinguistics. Springer US. https://books.google.gr/books?id=4vbiBwAAQBAJ\nGerman I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. 2019. Continual lifelong learning\nwith neural networks: A review. Neural Networks 113 (2019), 54–71. https://doi.org/10.1016/j.neunet.2019.01.012\nJoonsuk Park, Sally Klingel, Claire Cardie, Mary Newhart, Cynthia Farina, and Joan-Josep Vallbé. 2012. Facilitative\nmoderation for online participation in eRulemaking. In Proceedings of the 13th Annual International Conference on Digital\nGovernment Research. 173–182.\nJi Ho Park, Jamin Shin, and Pascale Fung. 2018. Reducing Gender Bias in Abusive Language Detection. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics,\nBrussels, Belgium, 2799–2804. https://doi.org/10.18653/v1/D18-1302\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. GauGAN: semantic image synthesis with spatially\nadaptive normalization. In ACM SIGGRAPH 2019 Real-Time Live! 1–1.\nAndrew Parker. 2003. In the blink of an eye: how vision sparked the big bang of evolution. (2003).\nSamir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness,\nAccountability, and Transparency . ACM. https://doi.org/10.1145/3287560.3287567\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,\nNatalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative\nStyle, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems (NeurIPS) .\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. StyleCLIP: Text-Driven Manipulation\nof StyleGAN Imagery. arXiv e-prints (2021), arXiv–2103.\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. 2017. Curiosity-Driven Exploration by Self-Supervised\nPrediction. 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2017), 488–489.\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. 2016. Context encoders: Feature\nlearning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition . 2536–2544.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier,\nand Jeff Dean. 2021. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 (2021).\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily L. Denton, and Alex Hanna. 2020. Data and its\n(dis)contents: A survey of dataset development and use in machine learning research. ArXiv abs/2012.05345 (2020).\nhttps://arxiv.org/abs/2012.05345\nJudea Pearl. 2000. Causality: Models, Reasoning and Inference . Vol. 29. Springer.\nDinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. 2021. How could Neural Networks understand\nPrograms? CoRR abs/2105.04297 (2021). arXiv:2105.04297 https://arxiv.org/abs/2105.04297\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. 2019. Moment Matching for Multi-Source\nDomain Adaptation. In International Conference on Computer Vision (ICCV) .\nXue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Edward Lee, Jie Tan, and Sergey Levine. 2020. Learning Agile\nRobotic Locomotion Skills by Imitating Animals. In Robotics: Science and Systems . https://doi.org/10.15607/RSS.2020.XVI.\n064\nAndrew M. Penner and Aliya Saperstein. 2008. How social status shapes race. Proceedings of the\nNational Academy of Sciences 105, 50 (2008), 19628–19630. https://doi.org/10.1073/pnas.0805762105\narXiv:https://www.pnas.org/content/105/50/19628.full.pdf\nAndrew M. Penner and Aliya Saperstein. 2013. Engendering Racial Perceptions: An Intersectional Analysis of How\nSocial Status Shapes Race. Gender & Society 27, 3 (2013), 319–344. https://doi.org/10.1177/0891243213480262\nOn the Opportunities and Risks of Foundation Models 197\narXiv:https://doi.org/10.1177/0891243213480262\nAndrew M. Penner and Aliya Saperstein. 2015. Disentangling the effects of racial self-identification and classification by\nothers: The case of arrest. Demography 52, 3 (2015), 1017–1024. https://link.springer.com/article/10.1007/s13524-015-\n0394-1\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global Vectors for word representation. In\nEmpirical Methods in Natural Language Processing (EMNLP) . 1532–1543.\nBethany Percha. 2021. Modern Clinical Text Mining: A Guide and Review. Annual Review of Biomedical Data Science 4\n(2021).\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True Few-Shot Learning with Language Models. arXiv preprint\narXiv:2105.11447 (2021).\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018.\nDeep contextualized word representations. In North American Association for Computational Linguistics (NAACL) .\nMatthew E. Peters, Mark Neumann, IV RobertLLogan, Roy Schwartz, V. Joshi, Sameer Singh, and Noah A. Smith. 2019.\nKnowledge Enhanced Contextual Word Representations. In EMNLP/IJCNLP.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, A. Bakhtin, Yuxiang Wu, Alexander H. Miller, and S. Riedel. 2019. Language\nModels as Knowledge Bases?. In EMNLP.\nChris Piech and Lisa Einstein. 2020. A Vision of AI for Joyful Education. Scientific American (Feb 2020). https://blogs.\nscientificamerican.com/observations/a-vision-of-ai-for-joyful-education/\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell. 2020. Information-\nTheoretic Probing for Linguistic Structure. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. Association for Computational Linguistics, Online, 4609–4622. https://www.aclweb.org/anthology/2020.acl-\nmain.420\nJoelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché Buc, Emily\nFox, and Hugo Larochelle. 2020. Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS\n2019 Reproducibility Program). arXiv:2003.12206 [cs.LG]\nLerrel Pinto and Abhinav Gupta. 2016. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours.\nIn IEEE International Conference on Robotics and Automation (ICRA) . IEEE, 3406–3413.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How Multilingual is Multilingual BERT?. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics . 4996–5001.\nNina Poerner, Ulli Waltinger, and Hinrich Schutze. 2020. E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT.\narXiv:1911.03681v2 (2020).\nAdam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis Only\nBaselines in Natural Language Inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational\nSemantics. Association for Computational Linguistics, New Orleans, Louisiana, 180–191. https://doi.org/10.18653/v1/S18-\n2023\nAntonio Polino, Razvan Pascanu, and Dan Alistarh. 2018. Model Compression via Distillation and Quantization. arXiv\npreprint arXiv:1802.05668 (2018).\nStanislas Polu and Ilya Sutskever. 2020. Generative Language Modeling for Automated Theorem Proving. CoRR\nabs/2009.03393 (2020). arXiv:2009.03393 https://arxiv.org/abs/2009.03393\nEdoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak, Ivan Vulić, Roi Reichart, Thierry Poibeau, Ekaterina Shutova,\nand Anna Korhonen. 2019. Modeling Language Variation and Universals: A Survey on Typological Linguistics for\nNatural Language Processing. Computational Linguistics 45, 3 (09 2019), 559–601. https://doi.org/10.1162/coli_a_00357\narXiv:https://direct.mit.edu/coli/article-pdf/45/3/559/1847397/coli_a_00357.pdf\nRyan Poplin, Avinash V Varadarajan, Katy Blumer, Yun Liu, Michael V McConnell, Greg S Corrado, Lily Peng, and Dale R\nWebster. 2018. Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning. Nature\nBiomedical Engineering 2, 3 (2018), 158–164.\nVinodkumar Prabhakaran and Jr. Donald Martin. 2020. Participatory Machine Learning Using Community-Based System\nDynamics. Health Hum Rights (2020).\nGrusha Prasad, Marten van Schijndel, and Tal Linzen. 2019. Using Priming to Uncover the Organization of Syntactic\nRepresentations in Neural Language Models. In Proceedings of the 23rd Conference on Computational Natural Language\nLearning (CoNLL). Association for Computational Linguistics, Hong Kong, China, 66–76. https://doi.org/10.18653/v1/K19-\n1007\nDoina Precup, R. Sutton, and Satinder Singh. 2000. Eligibility Traces for Off-Policy Policy Evaluation. In ICML.\nGil Press. 2021. Andrew Ng Launches A Campaign For Data-Centric AI. https://www.forbes.com/sites/gilpress/2021/06/16/\nandrew-ng-launches-a-campaign-for-data-centric-ai/?sh=44865f6a74f5.\nGeorge L Priest and Benjamin Klein. 1984. The selection of disputes for litigation. The Journal of Legal Studies 13, 1 (1984),\n1–55.\n198 Center for Research on Foundation Models (CRFM)\nPyTorch. 2021. PyTorch JIT. https://pytorch.org/docs/stable/jit.html.\nGuanghui Qin and Jason Eisner. 2021. Learning How To Ask: Querying LMs with Mixtures of Soft Prompts. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies (NAACL-HLT). Online, 5203–5212. http://cs.jhu.edu/~jason/papers/#qin-eisner-2021\nMarc Queudot, Éric Charton, and Marie-Jean Meurs. 2020. Improving Access to Justice with Legal Chatbots. Stats 3, 3\n(2020), 356–375.\nJoaquin Quiñonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. 2009. When Training and Test\nSets Are Different: Characterizing Learning Transfer. In Dataset Shift in Machine Learning . 3–28.\nMarkus N. Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy. 2021. Mathematical reasoning via self-supervised\nskip-tree training. ICLR (2021). https://openreview.net/forum?id=YmqAnY0CMEy\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. arXiv\npreprint arXiv:2103.00020 (2021).\nAlec Radford and Karthik Narasimhan. 2018. Improving Language Understanding by Generative Pre-Training.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative\npre-training. Technical Report. OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are\nunsupervised multitask learners. OpenAI Blog 1, 8 (2019).\nKira Radinsky. 2015. Data monopolists like Google are threatening the economy. Harvard Business Review 2 (2015).\nEvani Radiya-Dixit and Florian Tramèr. 2021. Data Poisoning Won’t Save You From Facial Recognition. arXiv preprint\narXiv:2106.14851 (2021).\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson,\nRoman Ring, Susannah Young, et al. 2021. Scaling Language Models: Methods, Analysis & Insights from Training Gopher.\narXiv preprint arXiv:2112.11446 (2021).\nColin Raffel. 2021. A Call to Build Models Like We Build Open-Source Software. https://colinraffel.com/blog/a-call-to-build-\nmodels-like-we-build-open-source-software.html.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.\nLiu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer.arXiv preprint arXiv:1910.10683\n(2019).\nMaithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. 2017. On the expressive power of deep\nneural networks. In international conference on machine learning . PMLR, 2847–2854.\nMaithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. 2019. Transfusion: Understanding Transfer Learning\nfor Medical Imaging. In Advances in Neural Information Processing Systems , H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d’Alché Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2019/\nfile/eb1e78328c46506b46a4ac4a1e378b91-Paper.pdf\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: Memory Optimizations toward Training\nTrillion Parameter Models. In SC20: International Conference for High Performance Computing, Networking, Storage and\nAnalysis. IEEE, 1–16.\nSamyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. ZeRO-Infinity: Breaking the GPU\nMemory Wall for Extreme Scale Deep Learning. arXiv:2104.07857 [cs.DC]\nInioluwa Deborah Raji and Joy Buolamwini. 2019. Actionable Auditing: Investigating the Impact of Publicly Naming\nBiased Performance Results of Commercial AI Products. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics,\nand Society (Honolulu, HI, USA) (AIES ’19) . Association for Computing Machinery, New York, NY, USA, 429–435.\nhttps://doi.org/10.1145/3306618.3314244\nInioluwa Deborah Raji, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila\nSmith-Loud, Daniel Theron, and Parker Barnes. 2020. Closing the AI accountability gap. In Proceedings of the 2020\nConference on Fairness, Accountability, and Transparency . ACM. https://doi.org/10.1145/3351095.3372873\nAlvin Rajkomar, Eyal Oren, Kai Chen, Andrew M Dai, Nissan Hajaj, Michaela Hardt, Peter J Liu, Xiaobing Liu, Jake Marcus,\nMimi Sun, et al. 2018. Scalable and accurate deep learning with electronic health records. NPJ Digital Medicine 1, 1\n(2018), 1–10.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know What You Don’t Know: Unanswerable Questions for SQuAD. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) . 784–789.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine\nComprehension of Text. In Empirical Methods in Natural Language Processing (EMNLP) .\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021.\nZero-Shot Text-to-Image Generation. arXiv:2102.12092 [cs.CV]\nOn the Opportunities and Risks of Foundation Models 199\nBharath Ramsundar, Steven M. Kearnes, Patrick Riley, Dale Webster, David E. Konerding, and Vijay S. Pande. 2015. Massively\nMultitask Networks for Drug Discovery. CoRR abs/1502.02072 (2015). arXiv:1502.02072 http://arxiv.org/abs/1502.02072\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. DeepSpeed: System Optimizations Enable\nTraining Deep Learning Models with over 100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining . 3505–3506.\nLaila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. 2021. Med-BERT: pretrained contextualized embeddings on\nlarge-scale structured electronic health records for disease prediction. NPJ digital medicine 4, 1 (2021), 1–13.\nR. Ratcliff. 1990. Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.\nPsychological review 97 2 (1990), 285–308.\nAlexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré. 2017. Snorkel: Rapid\nTraining Data Creation with Weak Supervision. Proceedings of the VLDB Endowment (PVLDB) (2017).\nGerald K Ray and Jeffrey S Lubbers. 2014. A government success story: How data analysis by the Social Security Appeals\nCouncil (with a push from the Administrative Conference of the United States) is transforming social security disability\nadjudication. Geo. Wash. L. Rev. 83 (2014), 1575.\nChristopher Ré, Feng Niu, Pallavi Gudipati, and Charles Srisuwananukorn. 2019. Overton: A data system for monitoring\nand improving machine-learned products. arXiv preprint arXiv:1909.05372 (2019).\nRichard M Re and Alicia Solow-Niederman. 2019. Developing artificially intelligent justice. Stan. Tech. L. Rev. 22 (2019), 242.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. Do ImageNet Classifiers Generalize to\nImageNet?. In International Conference on Machine Learning (ICML) .\nColorado J. Reed, Xiangyu Yue, Ani Nrusimha, Sayna Ebrahimi, Vivek Vijaykumar, Richard Mao, Bo Li, Shanghang\nZhang, Devin Guillory, Sean Metzger, Kurt Keutzer, and Trevor Darrell. 2021. Self-Supervised Pretraining Improves\nSelf-Supervised Pretraining. arXiv:2103.12718 [cs.CV]\nRob Reich, Mehran Sahami, and Jeremy M. Weinstein. 2021. System Error: Where Big Tech Went Wrong and How We Can\nReboot. Harper. https://books.google.com/books?id=mU0QEAAAQBAJ\nTheodoros Rekatsinas, Xu Chu, Ihab F. Ilyas, and Christopher Ré. 2017a. Holoclean: Holistic data repairs with probabilistic\ninference. Proceedings of the VLDB Endowment (PVLDB) (2017).\nTheodoros Rekatsinas, Manas Joglekar, Hector Garcia-Molina, Aditya Parameswaran, and Christopher Ré. 2017b. Slimfast:\nGuaranteed results for data fusion and source reliability. In Proceedings of the 2017 ACM International Conference on\nManagement of Data . 1399–1414.\nHongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. 2021. Combiner: Full\nAttention Transformer with Sparse Computation Cost. arXiv preprint arXiv:2107.05768 (2021).\nHongyu Ren, Weihua Hu, and Jure Leskovec. 2020. Query2box: Reasoning over knowledge graphs in vector space using\nbox embeddings. In International Conference on Learning Representations (ICLR) .\nHongyu Ren and Jure Leskovec. 2020. Beta embeddings for multi-hop logical reasoning in knowledge graphs. In NeurIPS.\nAdithya Renduchintala, Denise Diaz, Kenneth Heafield, Xian Li, and Mona Diab. 2021. Gender bias amplification during\nSpeed-Quality optimization in Neural Machine Translation. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short\nPapers). Association for Computational Linguistics, Online, 99–109. https://doi.org/10.18653/v1/2021.acl-short.15\nLaria Reynolds and Kyle McDonell. 2021. Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm.\nIn Extended Abstract, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM.\nDeborah L Rhode. 2004. Access to justice . Oxford University Press.\nDeborah L Rhode. 2014. Access to justice: A roadmap for reform. Fordham Urb. LJ 41 (2014), 1227.\nDiana Rhoten and Craig Calhoun. 2011.Knowledge matters: The public mission of the research university . Columbia University\nPress.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of\nNLP models with CheckList. arXiv preprint arXiv:2005.04118 (2020).\nRicardo T Ribeiro, Rui Tato Marinho, and J Miguel Sanches. 2012. Classification and staging of chronic liver disease from\nmultimodal data. IEEE Transactions on Biomedical Engineering 60, 5 (2012), 1336–1344.\nDouglas Rice, Jesse H Rhodes, and Tatishe Nteta. 2019. Racial bias in legal language. Research & Politics 6, 2 (2019),\n2053168019848930.\nC. Richards, W.P. Bouman, and M.J. Barker. 2017. Genderqueer and Non-Binary Genders . Palgrave Macmillan UK. https:\n//books.google.com/books?id=qFJDDwAAQBAJ\nJohn R Rickford, Faye McNair-Knox, et al. 1994. Addressee-and topic-influenced style shift: A quantitative sociolinguistic\nstudy. Sociolinguistic perspectives on register (1994), 235–276.\nRegina Rini. 2017. Fake News and Partisan Epistemology. Kennedy Institute of Ethics Journal 27, S2 (2017), 43–64. https:\n//doi.org/10.1353/ken.2017.0025\nRegina Rini. 2020. Deepfakes and the Epistemic Backstop. Philosopher’s Imprint 20, 24 (2020), 1–16.\n200 Center for Research on Foundation Models (CRFM)\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick,\nJerry Ma, and Rob Fergus. 2021. Biological structure and function emerge from scaling unsupervised learning to 250 million\nprotein sequences. Proceedings of the National Academy of Sciences 118, 15 (2021). https://doi.org/10.1073/pnas.2016239118\narXiv:https://www.pnas.org/content/118/15/e2016239118.full.pdf\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020. How Much Knowledge Can You Pack into the Parameters of a\nLanguage Model?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\n5418–5426.\nPhillip Rogaway. 2016. The Moral Character of Cryptographic Work. , 48 pages.\nAnna Rogers. 2020. Peer review in NLP: resource papers. https://hackingsemantics.xyz/2020/reviewing-data/\nAnna Rogers. 2021. Changing the World by Changing the Data.ArXiv abs/2105.13947 (2021). https://arxiv.org/abs/2105.13947\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in bertology: What we know about how bert works.\nTransactions of the Association for Computational Linguistics (TACL) 8 (2020), 842–866.\nDavid Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross,\nNikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. 2019. Tackling climate change with machine\nlearning. arXiv preprint arXiv:1906.05433 (2019).\nPaul M Romer. 1990. Endogenous technological change. Journal of political Economy 98, 5, Part 2 (1990), S71–S102.\nFrieda Rong. 2021. Extrapolating to Unnatural Language Processing with GPT-3’s In-context Learning: The Good, the Bad,\nand the Mysterious. http://ai.stanford.edu/blog/in-context-learning/\nStéphane Ross, Geoffrey Gordon, and Andrew Bagnell. 2011. A reduction of imitation learning and structured prediction to\nno-regret online learning. In Artificial Intelligence and Statistics (AISTATS) .\nEdward Rosten and Tom Drummond. 2006. Machine learning for high-speed corner detection. In European conference on\ncomputer vision . Springer, 430–443.\nDaniel Rothchild, Alex Tamkin, Julie Yu, Ujval Misra, and Joseph Gonzalez. 2021. C5T5: Controllable Generation of Organic\nMolecules with Transformers. ArXiv abs/2108.10307 (2021).\nBaptiste Rozière, Marie-Anne Lachaux, Marc Szafraniec, and Guillaume Lample. 2021. DOBF: A Deobfuscation Pre-Training\nObjective for Programming Languages. CoRR abs/2102.07492 (2021). arXiv:2102.07492 https://arxiv.org/abs/2102.07492\nSebastian Ruder and Barbara Plank. 2018. Strong Baselines for Neural Semi-Supervised Learning under Domain Shift. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Melbourne,\nAustralia). Association for Computational Linguistics, 1044–1054. http://aclweb.org/anthology/P18-1096\nCynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable\nmodels instead. Nature Machine Intelligence 1, 5 (2019), 206–215.\nCamilo Ruiz, Marinka Zitnik, and Jure Leskovec. 2020. Identification of disease treatment mechanisms through the multiscale\ninteractome. Nature Communications (2020).\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya\nKhosla, Michael Bernstein, et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer\nvision 115, 3 (2015), 211–252.\nStuart J. Russell and Peter Norvig. 2020. Artificial Intelligence: A Modern Approach (4th Edition) . Pearson. http://aima.cs.\nberkeley.edu/\nMax Ryabinin and Anton Gusev. 2020. Towards Crowdsourced Training of Large Neural Networks using Decentralized\nMixture-of-Experts. arXiv preprint arXiv:2002.04013 (2020).\nChristopher Ré. 2021. The Road to Software 2.0 or Data-Centric AI. https://hazyresearch.stanford.edu/data-centric-ai.\nFereshteh Sadeghi and Sergey Levine. 2017. CAD 2RL: Real Single-Image Flight without a Single Real Image. ArXiv\nabs/1611.04201 (2017).\nJenny R Saffran, Richard N Aslin, and Elissa L Newport. 1996. Statistical learning by 8-month-old infants. Science 274, 5294\n(1996), 1926–1928.\nShiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. 2020a. Distributionally Robust Neural Networks\nfor Group Shifts: On the Importance of Regularization for Worst-Case Generalization. In International Conference on\nLearning Representations (ICLR) .\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. 2020b. An investigation of why overparameterization\nexacerbates spurious correlations. In International Conference on Machine Learning (ICML) .\nDebjani Saha, Candice Schumann, Duncan C. McElfresh, John P. Dickerson, Michelle L. Mazurek, and Michael Carl Tschantz.\n2020. Human Comprehension of Fairness in Machine Learning. In Proceedings of the AAAI/ACM Conference on AI,\nEthics, and Society (New York, NY, USA) (AIES ’20) . Association for Computing Machinery, New York, NY, USA, 152.\nhttps://doi.org/10.1145/3375627.3375819\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2020. On the Effect of Dropping Layers of Pre-trained\nTransformer Models. arXiv preprint arXiv:2004.03844 (2020).\nChristoph Salge, C. Glackin, and D. Polani. 2013. Empowerment - an Introduction. ArXiv abs/1310.1863 (2013).\nOn the Opportunities and Risks of Foundation Models 201\nNithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. “Everyone\nwants to do the model work, not the data work”: Data Cascades in High-Stakes AI. In proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems . 1–15.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, A Distilled Version of BERT: Smaller,\nFaster, Cheaper and Lighter. arXiv preprint arXiv:1910.01108 (2019).\nGillian Sankoff. 2018. Language Change Across the Lifespan. Annual Review of Linguistics 4, 1 (2018), 297–316. https:\n//doi.org/10.1146/annurev-linguistics-011817-045438 arXiv:https://doi.org/10.1146/annurev-linguistics-011817-045438\nLindsay Sanneman, Christopher Fourie, and Julie Shah. 2020. The State of Industrial Robotics: Emerging Technologies,\nChallenges, and Key Research Directions. https://www.therobotreport.com/wp-content/uploads/2021/01/2020-Research-\nBrief-Sanneman-Fourie-Shah.pdf\nKeshav Santhanam, Siddharth Krishna, Ryota Tomioka, Andrew Fitzgibbon, and Tim Harris. 2021. DistIR: An Intermediate\nRepresentation for Optimizing Distributed Neural Networks. In Proceedings of the 1st Workshop on Machine Learning and\nSystems. 15–23.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy P. Lillicrap. 2016. Meta-Learning with\nMemory-Augmented Neural Networks. In ICML. 1842–1850. http://proceedings.mlr.press/v48/santoro16.html\nShibani Santurkar, Dimitris Tsipras, and Aleksander Madry. 2020. BREEDS: Benchmarks for Subpopulation Shift. arXiv\n(2020).\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The Risk of Racial Bias in Hate Speech\nDetection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . Association for\nComputational Linguistics, Florence, Italy, 1668–1678. https://doi.org/10.18653/v1/P19-1163\nAliya Saperstein and Andrew M. Penner. 2012. Racial Fluidity and Inequality in the United States. Amer. J. Sociology 118, 3\n(2012), 676–727. https://doi.org/10.1086/667722 arXiv:https://doi.org/10.1086/667722\nAliya Saperstein, Andrew M. Penner, and Ryan Light. 2013. Racial Formation in Perspective: Connecting Individuals,\nInstitutions, and Power Relations. Annual Review of Sociology 39, 1 (2013), 359–378. https://doi.org/10.1146/annurev-\nsoc-071312-145639 arXiv:https://doi.org/10.1146/annurev-soc-071312-145639\nN. Saunshi, S. Malladi, and S. Arora. 2020a. A Mathematical Exploration of Why Language Models Help Solve Downstream\nTasks. arXiv preprint arXiv:2010.03648 (2020).\nNikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. 2020b. A Mathematical Exploration of Why Language Models Help\nSolve Downstream Tasks. arXiv preprint arXiv:2010.03648 (2020).\nJaromir Savelka, Vern R Walker, Matthias Grabmair, and Kevin D Ashley. 2017. Sentence boundary detection in adjudicatory\ndecisions in the united states. Traitement automatique des langues 58 (2017), 21.\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu,\nVladlen Koltun, Jitendra Malik, et al. 2019a. Habitat: A Platform for Embodied AI Research. In2019 IEEE/CVF International\nConference on Computer Vision (ICCV) . IEEE Computer Society, 9338–9346.\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu,\nVladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. 2019b. Habitat: A Platform for Embodied AI Research. In\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) .\nMatthew Saxton. 2017. Child Language: Acquisition and Development . Sage Publications, London.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural\nnetwork model. IEEE transactions on neural networks 20, 1 (2008), 61–80.\nTom Schaul, Dan Horgan, K. Gregor, and D. Silver. 2015. Universal Value Function Approximators. InInternational Conference\non Machine Learning (ICML) .\nMonica Schenone, Vlado Dančík, Bridget K Wagner, and Paul A Clemons. 2013. Target identification and mechanism of\naction in chemical biology and drug discovery. Nature Chemical Biology 9, 4 (2013), 232–240.\nMatthew U Scherer, Allan G King, and Marko J Mrkonich. 2019. Applying Old Rules to New Tools: Employment Discrimina-\ntion Law in the Age of Algorithms. SCL Rev. 71 (2019), 449.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language\nInference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:\nMain Volume. Association for Computational Linguistics, Online, 255–269. https://aclanthology.org/2021.eacl-main.20\nTimo Schick and Hinrich Schütze. 2021b. It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot\nLearners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies . Association for Computational Linguistics, Online, 2339–2352. https:\n//doi.org/10.18653/v1/2021.naacl-main.185\nTimo Schick, Sahana Udupa, and H. Schutze. 2021. Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based\nBias in NLP. ArXiv abs/2103.00453 (2021).\nLonda Schiebinger. 2013. Machine Translation: Analyzing Gender. (2013). http://genderedinnovations.stanford.edu/case-\nstudies/nlp.html#tabs-2\n202 Center for Research on Foundation Models (CRFM)\nLonda Schiebinger. 2014. Scientific research must take gender into account. Nature 507, 7490 (2014), 9.\nK. Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, and Chelsea Finn. 2020. Reinforcement Learning with\nVideos: Combining Offline Observations with Interaction. ArXiv abs/2011.06507 (2020).\nJürgen Schmidhuber. 1987. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-...\nhook. Ph.D. Dissertation. Technische Universität München.\nJ. Schmidhuber. 2019. Reinforcement Learning Upside Down: Don’t Predict Rewards - Just Map Them to Actions. ArXiv\nabs/1912.02875 (2019).\nLudwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. 2018. Adversarially robust\ngeneralization requires more data. In Advances in Neural Information Processing Systems (NeurIPS) . 5014–5026.\nVictor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson,\nSorelle Friedler, and Sasha Luccioni. 2021. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning\nComputing. https://github.com/mlco2/codecarbon. (2021). https://doi.org/10.5281/zenodo.4658424\nGisbert Schneider. 2018. Automating drug discovery. Nature Reviews Drug Discovery 17, 2 (2018), 97–113.\nJoel M. Schumm. 2012. National Indigent Defense Reform: The Solution is Multifaceted . Technical Report. National Association\nof Criminal Defense Lawyers, American Bar Association.\nM. Schuster and Kaisuke Nakajima. 2012. Japanese and Korean voice search. 2012 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) (2012), 5149–5152.\nRoei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You autocomplete me: Poisoning vulnerabilities\nin neural code completion. In 30th {USENIX}Security Symposium ( {USENIX}Security 21) .\nE. a. G. Schuur, A. D. McGuire, C. Schädel, G. Grosse, J. W. Harden, D. J. Hayes, G. Hugelius, C. D. Koven, P. Kuhry, D. M.\nLawrence, S. M. Natali, D. Olefeldt, V. E. Romanovsky, K. Schaefer, M. R. Turetsky, C. C. Treat, and J. E. Vonk. 2015. Climate\nchange and the permafrost carbon feedback. Nature 520, 7546 (April 2015), 171–179. https://doi.org/10.1038/nature14338\nZSCC: NoCitationData[s0] Bandiera_abtest: a Cg_type: Nature Research Journals Number: 7546 Primary_atype: Reviews\nPublisher: Nature Publishing Group Subject_term: Biogeochemistry;Climate sciences;Earth and environmental sciences\nSubject_term_id: biogeochemistry;climate-sciences;earth-and-environmental-sciences.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. 2019. Green ai. arXiv preprint arXiv:1907.10597 (2019).\nMax Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, Devon Hjelm, Philip Bach-\nman, and Aaron C. Courville. 2021. Pretraining Representations for Data-Efficient Reinforcement Learning. ArXiv\nabs/2106.04799 (2021).\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, and Christopher D. Manning. 2019. Do Massively Pretrained\nLanguage Models Make Better Storytellers?. In Proceedings of the 23rd Conference on Computational Natural Language\nLearning (CoNLL). Association for Computational Linguistics, Hong Kong, China, 843–861. https://doi.org/10.18653/v1/\nK19-1079\nMarwin H. S. Segler, Mike Preuss, and Mark P. Waller. 2018. Planning chemical syntheses with deep neural networks and\nsymbolic AI. Nat. 555, 7698 (2018), 604–610. https://doi.org/10.1038/nature25978\nAndrew D Selbst. 2020. Negligence and AI’s human users. BUL Rev. 100 (2020), 1315.\nAndrew D. Selbst, Danah Boyd, Sorelle Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2018. Fairness and\nAbstraction in Sociotechnical Systems. In Proceeedings of the Conference on Fairness, Accountability, and Transparency .\nSelene. 2021. Selene Supercomputer. https://www.top500.org/system/179842/.\nRamprasaath R Selvaraju, Karan Desai, Justin Johnson, and Nikhil Naik. 2021. Casting your model: Learning to localize\nimproves self-supervised representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 11058–11067.\nAndrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Zídek,\nAlexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet\nKohli, David T. Jones, David Silver, Koray Kavukcuoglu, and Demis Hassabis. 2020. Improved protein structure prediction\nusing potentials from deep learning. Nat. 577, 7792 (2020), 706–710. https://doi.org/10.1038/s41586-019-1923-7\nRico Sennrich, B. Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units.\nArXiv abs/1508.07909 (2016).\nPierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain.\n2018. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE international conference on robotics\nand automation (ICRA) . IEEE, 1134–1141.\nAli Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, and Tom Goldstein. 2019. Adver-\nsarially robust transfer learning. arXiv preprint arXiv:1905.08232 (2019).\nNeal A Shah, Jessica Jue, and Tim K Mackey. 2020. Surgical data recording technology: a solution to address medical errors?\nAnnals of surgery 271, 3 (2020), 431–433.\nDanilo Jimenez Rezend Shakir Mohamed. 2015. Variational Information Maximisation for Intrinsically Motivated Reinforce-\nment Learning. In NIPS.\nOn the Opportunities and Risks of Foundation Models 203\nC. Shannon. 1948. A mathematical theory of communication. Bell Syst. Tech. J. 27 (1948), 379–423.\nLin Shao, Toki Migimatsu, Q. Zhang, Karen Yang, and Jeannette Bohg. 2020. Concept2Robot: Learning Manipulation\nConcepts from Instructions and Human Demonstrations. In Robotics: Science and Systems (RSS) .\nPratyusha Sharma, L. Mohan, Lerrel Pinto, and A. Gupta. 2018. Multiple Interactions Made Easy (MIME): Large Scale\nDemonstrations Data for Imitation. In Conference on Robot Learning (CORL) .\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hy-\noukJoong Lee, Mingsheng Hong, Cliff Young, et al. 2018. Mesh-TensorFlow: Deep Learning for Supercomputers.Advances\nin Neural Information Processing Systems 31 (2018), 10414–10423.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outra-\ngeously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017).\nNoam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In International\nConference on Machine Learning . PMLR, 4596–4604.\nBokui Shen, Fei Xia, Chengshu Li, Roberto Martın-Martın, Linxi Fan, Guanzhi Wang, Claudia D’Arpino, Shyamal Buch,\nSanjana Srivastava, Lyne P Tchapmi, Kent Vainio, Li Fei-Fei, and Silvio Savarese. 2021a. iGibson, a Simulation Environment\nfor Interactive Tasks in Large Realistic Scenes. International Conference on Intelligent Robots and Systems (IROS) (2021).\nHaichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi\nSundaram. 2019. Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis. In Proceedings of the 27th\nACM Symposium on Operating Systems Principles . 322–337.\nJia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil Heffernan, Xintao Wu, and Dongwon Lee. 2021b. MathBERT: A\nPre-trained Language Model for General NLP Tasks in Mathematics Education. arXiv:2106.07340 [cs.CL]\nEmily Sheng, Josh Arnold, Zhou Yu, Kai-Wei Chang, and Nanyun Peng. 2021. Revealing Persona Biases in Dialogue Systems.\narXiv:2104.08728 [cs.CL]\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The Woman Worked as a Babysitter: On Biases\nin Language Generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational\nLinguistics, Hong Kong, China, 3407–3412. https://doi.org/10.18653/v1/D19-1339\nToby Shevlane and Allan Dafoe. 2020. The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research\nReduce Misuse?. In Proceedings of the 2020 AAAI/ACM Conference on AI, Ethics, and Society (AIES ’20) . http://arxiv.org/\nabs/2001.00463\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge\nfrom Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, Online, 4222–4235. https:\n//doi.org/10.18653/v1/2020.emnlp-main.346\nBen Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface agents. In Interactions. ACM.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-LM:\nTraining Multi-Billion Parameter Language Models Using Model Parallelism. http://arxiv.org/abs/1909.08053 cite\narxiv:1909.08053.\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine\nlearning models. In IEEE Symposium on Security and Privacy . 3–18.\nConnor Shorten and Taghi M Khoshgoftaar. 2019. A survey on image data augmentation for deep learning. Journal of Big\nData 6, 1 (2019), 1–48.\nWilliam H Shrank, Teresa L Rogstad, and Natasha Parekh. 2019. Waste in the US health care system: estimated costs and\npotential for savings. Jama 322, 15 (2019), 1501–1509.\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and\nDieter Fox. 2020. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In Computer Vision\nand Pattern Recognition (CVPR) .\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating\nActivation Differences. In International Conference on Machine Learning (ICML) .\nIlia Shumailov, Yiren Zhao, Daniel Bates, Nicolas Papernot, Robert Mullins, and Ross Anderson. 2020. Sponge examples:\nEnergy-latency attacks on neural networks. arXiv preprint arXiv:2006.03463 (2020).\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser,\nIoannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal\nKalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis\nHassabis. 2016. Mastering the game of Go with deep neural networks and tree search. Nat. 529, 7587 (2016), 484–489.\nhttps://doi.org/10.1038/nature16961\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image\nclassification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013).\n204 Center for Research on Foundation Models (CRFM)\nK Simonyan and A. Zisserman. 2015. Very deep convolutional networks for large-scale image recognition. In International\nConference on Learning Representations (ICLR) .\nAudra Simpson. 2007. On Ethnographic Refusal: Indigeneity, ’Voice’ Colonial Citizenship. Junctures (Dec. 2007).\nAvi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, and Sergey Levine. 2019. End-to-End Robotic Reinforcement\nLearning without Reward Engineering. In Robotics: Science and Systems (RSS) .\nSatinder Singh, Andrew G Barto, and Nuttapong Chentanez. 2005. Intrinsically motivated reinforcement learning . Technical\nReport. MASSACHUSETTS UNIV AMHERST DEPT OF COMPUTER SCIENCE.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin, Sergei Popov, and Artem Babenko. 2020. Editable Neural Networks.\nIn International Conference on Learning Representations . https://openreview.net/forum?id=HJedXaEtvS\nVincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. 2019. Scene representation networks: Continuous 3d-structure-\naware neural scene representations. arXiv preprint arXiv:1906.01618 (2019).\nC Estelle Smith, Bowen Yu, Anjali Srivastava, Aaron Halfaker, Loren Terveen, and Haiyi Zhu. 2020. Keeping Community in\nthe Loop: Understanding Wikipedia Stakeholder Values for Machine Learning-Based Systems. In Proceedings of the 2020\nCHI Conference on Human Factors in Computing Systems . 1–14.\nLaura Smith, Nikita Dhawan, Marvin Zhang, P. Abbeel, and Sergey Levine. 2019. AVID: Learning Multi-Stage Tasks via\nPixel-Level Translation of Human Videos. ArXiv abs/1912.04443 (2019).\nJake Snell, Kevin Swersky, and Richard S Zemel. 2017. Prototypical networks for few-shot learning. arXiv preprint\narXiv:1703.05175 (2017).\nDavid So, Quoc Le, and Chen Liang. 2019. The Evolved Transformer. In Proceedings of the 36th International Conference on\nMachine Learning (Proceedings of Machine Learning Research, Vol. 97) , Kamalika Chaudhuri and Ruslan Salakhutdinov\n(Eds.). PMLR, 5877–5886. http://proceedings.mlr.press/v97/so19a.html\nNate Soares, Benja Fallenstein, Stuart Armstrong, and Eliezer Yudkowsky. 2015. Corrigibility. In Workshops at the Twenty-\nNinth AAAI Conference on Artificial Intelligence .\nJ. Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and S. Ganguli. 2015. Deep Unsupervised Learning using\nNonequilibrium Thermodynamics. ArXiv abs/1503.03585 (2015).\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger,\nJong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang. 2019.\nRelease Strategies and the Social Impacts of Language Models . Technical Report. OpenAI. http://arxiv.org/abs/1908.09203\nIrene Solaiman and Christy Dennison. 2021. Process for Adapting Language Models to Society (PALMS) with Values-Targeted\nDatasets. arXiv preprint arXiv:2106.10328 (2021).\nMiriam Solomon. 2006. Norms of epistemic diversity. Episteme 3, 1 (2006), 23–36.\nHamid Soltanian-Zadeh. 2019. Multimodal Analysis in Biomedicine. In Big Data in Multimodal Medical Imaging . Chapman\nand Hall/CRC, 193–203.\nCongzheng Song, Thomas Ristenpart, and Vitaly Shmatikov. 2017. Machine Learning Models That Remember Too Much. In\nProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (Dallas, Texas, USA) (CCS\n’17). Association for Computing Machinery, New York, NY, USA, 587–601. https://doi.org/10.1145/3133956.3134077\nCongzheng Song and Vitaly Shmatikov. 2019. Overlearning reveals sensitive attributes. arXiv preprint arXiv:1905.11742\n(2019).\nYang Song and S. Ermon. 2019. Generative Modeling by Estimating Gradients of the Data Distribution.ArXiv abs/1907.05600\n(2019).\nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. 2018. The implicit bias of gradient\ndescent on separable data. Journal of Machine Learning Research (JMLR) 19, 1 (2018), 2822–2878.\nSteven J. Spencer, Christine Logel, and Paul G. Davies. 2016. Stereotype threat. Annual Review of Psychology 67 (2016),\n415–437.\nKatta Spiel, Christopher Frauenberger, Os Keyes, and Geraldine Fitzpatrick. 2019. Agency of Autistic Children in Technology\nResearch—A Critical Literature Review. ACM Transactions on Computer-Human Interaction 26, 6 (Dec. 2019), 1–40.\nhttps://doi.org/10.1145/3344919\nPeter Spirtes, Clark N Glymour, and Richard Scheines. 2001. Causation, Prediction, and Search (2nd ed.). MIT Press.\nJost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for simplicity: The all\nconvolutional net. arXiv preprint arXiv:1412.6806 (2014).\nMegha Srivastava and Noah Goodman. 2021. Question Generation for Adaptive Education. In Association for Computational\nLinguistics (ACL).\nR. Srivastava, Pranav Shyam, Filipe Wall Mutz, Wojciech Jaśkowski, and J. Schmidhuber. 2019. Training Agents using\nUpside-Down Reinforcement Learning. ArXiv abs/1912.02877 (2019).\nSanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Vainio, Zheng Lian, Cem\nGokmen, Shyamal Buch, C Karen Liu, et al. 2021. BEHAVIOR: Benchmark for Everyday Household Activities in Virtual,\nInteractive, and Ecological Environments. arXiv preprint arXiv:2108.03332 (2021).\nOn the Opportunities and Risks of Foundation Models 205\nKate Starbird, Ahmer Arif, Tom Wilson, Katherine Van Koevering, Katya Yefimova, and Daniel Scarnecchia. 2018. Ecosystem\nor Echo-System? Exploring Content Sharing across Alternative Media Domains. Proceedings of the International AAAI\nConference on Web and Social Media 12, 1 (Jun. 2018). https://ojs.aaai.org/index.php/ICWSM/article/view/15009\nLaura Stark. 2012. Behind Closed Doors: IRBs and the Making of Medical Research . University of Chicago Press.\nDaniel Steel, Sina Fazelpour, Kinley Gillette, Bianca Crewe, and Michael Burgess. 2018. Multiple diversity concepts and\ntheir ethical-epistemic implications. European journal for philosophy of science 8, 3 (2018), 761–780.\nEthan Steinberg, Ken Jung, Jason A Fries, Conor K Corbin, Stephen R Pfohl, and Nigam H Shah. 2021. Language models are\nan effective representation learning technique for electronic health record data. Journal of Biomedical Informatics 113\n(2021), 103637.\nNicholas Stern and Joseph E Stiglitz. 2021. The social cost of carbon, risk, distribution, market failures: An alternative approach .\nTechnical Report. National Bureau of Economic Research.\nMegan T. Stevenson and Jennifer L. Doleac. 2021. Algorithmic Risk Assessment in the Hands of Humans. SSRN (2021).\nhttps://doi.org/10.2139/ssrn.3489440\nIris T Stewart, Christopher M Bacon, and William D Burke. 2014. The uneven distribution of environmental burdens and\nbenefits in Silicon Valley’s backyard. Applied Geography 55 (2014), 266–277.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and\nPaul Christiano. 2020. Learning to summarize from human feedback. ArXiv abs/2009.01325 (2020).\nJonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair,\nShawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al. 2020. A deep learning approach to antibiotic discovery.\nCell 180, 4 (2020), 688–702.\nMichael Stonebraker and Ihab F. Ilyas. 2018. Data Integration: The Current Status and the Way Forward. IEEE Computer\nSociety Technical Committee on Data Engineering (2018).\nMichael Stonebraker and Ariel Weisberg. 2013. The VoltDB Main Memory DBMS. IEEE Data Eng. Bull. 36, 2 (2013), 21–27.\nMarilyn Strathern. 1997. ‘Improving ratings’: audit in the British University system. European Review 5, 3 (1997), 305–\n321. https://www.cambridge.org/core/journals/european-review/article/abs/improving-ratings-audit-in-the-british-\nuniversity-system/FC2EE640C0C44E3DB87C29FB666E9AAB\nYolande Strengers, Lizhen Qu, Qiongkai Xu, and Jarrod Knibbe. 2020. Adhering, Steering, and Queering: Treatment of\nGender in Natural Language Generation. In Proceedings of the 2020 CHI Conference on Human Factors in Computing\nSystems. ACM. https://doi.org/10.1145/3313831.3376315\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP.\narXiv preprint arXiv:1906.02243 (2019).\nMasashi Sugiyama, Matthias Krauledat, and Klaus-Robert Muller. 2007. Covariate Shift Adaptation by Importance Weighted\nCross Validation. Journal of Machine Learning Research (JMLR) 8 (2007), 985–1005.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. InAdvances in neural information\nprocessing systems . 2440–2448.\nMichael Sullivan, Josh Schellenberg, and Marshall Blundell. 2015. Updated Value of Service Reliability Estimates for Electric\nUtility Customers in the United States . Technical Report LBNL–6941E, 1172643. LBNL–6941E, 1172643 pages. https:\n//doi.org/10.2172/1172643 ZSCC: 0000086.\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019a. Videobert: A joint model for video and\nlanguage representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 7464–7473.\nPei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning\nChai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim\nKrivokon, Amy Gao, Aditya Joshi, Sheng Zhao, Shuyang Cheng, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir\nAnguelov. 2020a. Scalability in Perception for Autonomous Driving: Waymo Open Dataset. arXiv:1912.04838 [cs.CV]\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng Zhang. 2020b. Colake:\nContextualized language and knowledge embedding. arXiv preprint arXiv:2010.00309 (2020).\nYu Sun, Eric Tzeng, Trevor Darrell, and Alexei A. Efros. 2019b. Unsupervised Domain Adaptation through Self-Supervision.\narXiv:1909.11825 [cs.LG]\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. InInternational Conference\non Machine Learning (ICML) . 3319–3328.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales. 2018. Learning to compare:\nRelation network for few-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition .\n1199–1208.\nHarry Surden. 2020. The ethics of artificial intelligence in law: Basic questions. Forthcoming chapter in Oxford Handbook of\nEthics of AI (2020), 19–29.\nAbhijit Suresh, Jennifer Jacobs, Vivian Lai, Chenhao Tan, Wayne Ward, James H Martin, and Tamara Sumner. 2021. Using\nTransformers to Provide Teachers with Personalized Feedback on their Classroom Discourse: The TalkMoves Application.\n206 Center for Research on Foundation Models (CRFM)\narXiv preprint arXiv:2105.07949 (2021).\nAnnamalai Suresh, R Udendhran, and S Vimal. 2020.Deep neural networks for multimodal imaging and biomedical applications .\nIGI Global.\nAlexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation using\ntransformer. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium\non the Foundations of Software Engineering . 1433–1443.\nLatanya Sweeney. 2013. Discrimination in Online Ad Delivery. Queue 11, 3, Article 10 (March 2013), 20 pages. https:\n//doi.org/10.1145/2460276.2460278\nStephen J Swensen, Gary S Kaplan, Gregg S Meyer, Eugene C Nelson, Gordon C Hunt, David B Pryor, Jed I Weissberg,\nJennifer Daley, Gary R Yates, and Mark R Chassin. 2011. Controlling healthcare costs by removing waste: what American\ndoctors can do now. BMJ quality & safety 20, 6 (2011), 534–537.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition . 1–9.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014.\nIntriguing properties of neural networks. In International Conference on Learning Representations (ICLR) .\nAndrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam,\nDevendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech\nGaluba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. 2021. Habitat 2.0:\nTraining Home Assistants to Rearrange their Habitat. arXiv:2106.14405 [cs.LG]\nAlex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021a. Understanding the Capabilities, Limitations, and\nSocietal Impact of Large Language Models. arXiv:2102.02503 [cs.CL]\nAlex Tamkin, Vincent Liu, Rongfei Lu, Daniel Fein, Colin Schultz, and Noah Goodman. 2021b. DABS: A Domain-Agnostic\nBenchmark for Self-Supervised Learning. arXiv:2111.12062 [cs.LG]\nA. Tamkin, Mike Wu, and Noah D. Goodman. 2021c. Viewmaker Networks: Learning Views for Unsupervised Representation\nLearning. ArXiv abs/2010.07432 (2021).\nHao Tan and Mohit Bansal. 2020. Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded\nSupervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\n2066–2080.\nHao Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers. In\nEmpirical Methods in Natural Language Processing (EMNLP) .\nMingxing Tan and Quoc V Le. 2021. Efficientnetv2: Smaller models and faster training. arXiv preprint arXiv:2104.00298\n(2021).\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. 2020. Measuring\nRobustness to Natural Distribution Shifts in Image Classification. arXiv preprint arXiv:2007.00644 (2020).\nRachael Tatman. 2017. Gender and Dialect Bias in YouTube’s Automatic Captions. InWorkshop on Ethics in Natural Langauge\nProcessing, Vol. 1. 53–59.\nNicholas P Tatonetti, P Ye Patrick, Roxana Daneshjou, and Russ B Altman. 2012. Data-driven prediction of drug effects and\ninteractions. Science translational medicine 4, 125 (2012), 125ra31–125ra31.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient Transformers: A Survey. arXiv preprint\narXiv:2009.06732 (2020).\nYi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu,\nand Donald Metzler. 2021. Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. arXiv\npreprint arXiv:2106.12672 (2021).\nJessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch. 2016. Alignment for Advanced Machine Learning\nSystems. In Ethics of Artificial Intelligence .\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. arXiv (2019).\nThe HAI Adaptive Agents Group. 2021. When Artificial Agents Lie, Defame, and Defraud, Who Is to Blame? https:\n//hai.stanford.edu/news/when-artificial-agents-lie-defame-and-defraud-who-blame\nArmin W Thomas, Hauke R Heekeren, Klaus-Robert Müller, and Wojciech Samek. 2019. Analyzing neuroimaging data\nthrough recurrent deep learning models.Frontiers in neuroscience 13 (2019), 1321. https://doi.org/10.3389/fnins.2019.01321\nCharles Thorpe, Martial H Hebert, Takeo Kanade, and Steven A Shafer. 1988. Vision and navigation for the Carnegie-Mellon\nNavlab. IEEE Transactions on Pattern Analysis and Machine Intelligence 10, 3 (1988), 362–373.\nSimon Thorpe, Denis Fize, and Catherine Marlot. 1996. Speed of processing in the human visual system. Nature 381, 6582\n(June 1996), 520–522. https://doi.org/10.1038/381520a0\nSebastian Thrun. 1998. Lifelong learning algorithms. Learning to learn (1998), 181–209.\nS. Thrun and Tom Michael Mitchell. 1995. Lifelong robot learning. Robotics Auton. Syst. 15 (1995), 25–46.\nOn the Opportunities and Risks of Foundation Models 207\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. 2020a. What makes for good views\nfor contrastive learning. arXiv preprint arXiv:2005.10243 (2020).\nYuandong Tian, Lantao Yu, Xinlei Chen, and Surya Ganguli. 2020b. Understanding self-supervised learning with dual deep\nnetworks. arXiv preprint arXiv:2010.00578 (2020).\nElizabeth Chika Tippett, Charlotte Alexander, and L Karl Branting. 2021. Does Lawyering Matter? Predicting Judicial\nDecisions from Legal Briefs, and What That Means for Access to Justice. Texas Law Review, Forthcoming (2021).\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Daniel\nKeysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. 2021. MLP-Mixer: An all-MLP Architecture for Vision.\narXiv:2105.01601 [cs.CV]\nNenad Tomasev, Kevin R. McKee, Jackie Kay, and Shakir Mohamed. 2021. Fairness for Unobserved Characteristics: Insights\nfrom Technological Impacts on Queer Communities. arXiv:2102.04257 (2021). https://doi.org/10.1145/3461702.3462540\nChristopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. 2020. Contrastive estimation reveals topic posterior information\nto linear models. arXiv:2003.02234 (2020).\nChristopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. 2021. Contrastive learning, multi-view redundancy, and linear\nmodels. In Algorithmic Learning Theory . PMLR, 1179–1206.\nFlorian Tramèr and Dan Boneh. 2021. Differentially Private Learning Needs Better Features (or Much More Data). In\nInternational Conference on Learning Representations .\nFlorian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. 2016. Stealing machine learning models via\nprediction APIs. In USENIX Security .\nNilesh Tripuraneni, Michael I Jordan, and Chi Jin. 2020. On the theory of transfer learning: The importance of task diversity.\narXiv preprint arXiv:2006.11650 (2020).\nMegan L. Truax. 2018. The Impact of Teacher Language and Growth Mindset Feedback on Writing Motiva-\ntion. Literacy Research and Instruction 57, 2 (2018), 135–157. https://doi.org/10.1080/19388071.2017.1340529\narXiv:https://doi.org/10.1080/19388071.2017.1340529\nTomer Tsaban, Julia K Varga, Orly Avraham, Ziv Ben Aharon, Alisa Khramushin, and Ora Schueler-Furman. 2021. Harnessing\nprotein folding neural networks for peptide-protein docking. bioRxiv (2021).\nYao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2020. Self-supervised learning from a\nmulti-view perspective. arXiv preprint arXiv:2006.05576 (2020).\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal Few-Shot\nLearning with Frozen Language Models. arXiv preprint arXiv:2106.13884 (2021).\nMasatoshi Tsuchiya. 2018. Performance Impact Caused by Hidden Bias of Training Data for Recognizing Textual Entailment.\nIn Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) . European\nLanguage Resources Association (ELRA), Miyazaki, Japan. https://aclanthology.org/L18-1239\nLifu Tu, Garima Lalwani, Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations\nusing pre-trained language models. Transactions of the Association for Computational Linguistics 8 (2020), 621–633.\nWenling Tu and Yujung Lee. 2009. Ineffective environmental laws in regulating electronic manufacturing pollution:\nExamining water pollution disputes in Taiwan. In 2009 IEEE International Symposium on Sustainable Systems and\nTechnology. IEEE, 1–6.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised\nlearning. In Association for Computational Linguistics (ACL) . 384–394.\nAlan M Turing. 1950. Computing machinery and intelligence. Mind 49 (1950), 433–460.\nTuring-NLG. 2020. Turing-NLG: A 17-Billion-Parameter Language Model by Microsoft. https://www.microsoft.com/en-\nus/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft//.\nBarbara Tversky and Jeffrey M Zacks. 2013. Event perception. Oxford handbook of cognitive psychology (2013), 83–94.\nJonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, and Pushmeet Kohli. 2019. Are\nLabels Required for Improving Adversarial Robustness?. In Advances in Neural Information Processing Systems (NeurIPS) .\nShimon Ullman. 1979. The interpretation of structure from motion. Proceedings of the Royal Society of London. Series B.\nBiological Sciences 203, 1153 (1979), 405–426.\nUnited Nations General Assembly. 2015. Transforming Our World: The 2030 Agenda for Sustainable Development.\nhttps://www.refworld.org/docid/57b6e3e44.html\nJosef Urban and Jan Jakubuv. 2020. First Neural Conjecturing Datasets and Experiments. InIntelligent Computer Mathematics\n- 13th International Conference, CICM 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings (Lecture Notes in Computer Science,\nVol. 12236), Christoph Benzmüller and Bruce R. Miller (Eds.). Springer, 315–323. https://doi.org/10.1007/978-3-030-53518-\n6_24\nMark C. Urban. 2015. Accelerating extinction risk from climate change. Science 348, 6234 (May 2015), 571–573. https:\n//doi.org/10.1126/science.aaa4984 ZSCC: 0000959 Publisher: American Association for the Advancement of Science\nSection: Report.\n208 Center for Research on Foundation Models (CRFM)\nAäron van den Oord, S. Dieleman, H. Zen, K. Simonyan, Oriol Vinyals, A. Graves, Nal Kalchbrenner, A. Senior, and K.\nKavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio. In SSW.\nAäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. ArXiv\nabs/1807.03748 (2018).\nAäron van den Oord, Oriol Vinyals, and K. Kavukcuoglu. 2017. Neural Discrete Representation Learning. In NIPS.\nMichael van Hartskamp, Sergio Consoli, Wim Verhaegh, Milan Petkovic, and Anja van de Stolpe. 2019. Artificial Intelligence\nin Clinical Health Care Applications: Viewpoint. Interactive Journal of Medical Research 8, 2 (Apr 2019), e12100. https:\n//doi.org/10.2196/12100\nMarten van Schijndel and Tal Linzen. 2018. A Neural Model of Adaptation in Reading. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Brussels, Belgium,\n4704–4710. https://doi.org/10.18653/v1/D18-1499\nManasi Vartak, Harihar Subramanyam, Wei-En Lee, Srinidhi Viswanathan, Saadiyah Husnoo, Samuel Madden, and Matei\nZaharia. 2016. ModelDB: a system for machine learning model management. In Proceedings of the Workshop on Human-\nIn-the-Loop Data Analytics . 1–3.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. 2017. Attention Is All You Need. arXiv preprint arXiv:1706.03762 (2017).\nSara Veldhoen, Dieuwke Hupkes, and Willem Zuidema. 2016. Diagnostic Classifiers: Revealing how Neural Networks\nProcess Hierarchical Structure. In Pre-Proceedings of the Workshop on Cognitive Computation: Integrating Neural and\nSymbolic Approaches (CoCo @ NIPS 2016) .\nPetar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2017. Graph\nAttention Networks. arXiv e-prints (2017), arXiv–1710.\nPat Verga, Haitian Sun, Livio Baldini Soares, and William W Cohen. 2020. Facts as experts: Adaptable and interpretable\nneural memory over symbolic knowledge. arXiv preprint arXiv:2007.00849 (2020).\nVikas Verma, Thang Luong, Kenji Kawaguchi, Hieu Pham, and Quoc Le. 2021. Towards domain-agnostic contrastive learning.\nIn International Conference on Machine Learning . PMLR, 10530–10541.\nLucas Nunes Vieira, Minako O’Hagan, and Carol O’Sullivan. 2020. Understanding the societal impacts of machine translation:\na critical review of the literature on medical and legal use cases. Information, Communication & Society (2020), 1–18.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason Huang, Yaron Singer,\nand Stuart Shieber. 2020. Causal mediation analysis for interpreting neural NLP: The case of gender bias. arXiv preprint\narXiv:2004.12265 (2020).\nEduard Fosch Villaronga, Peter Kieseberg, and Tiffany Li. 2018. Humans forget, machines remember: Artificial intelligence\nand the right to be forgotten. Computer Law & Security Review 34, 2 (2018), 304–313.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, , and Pierre-Antoine Manzagol. 2008. Extracting and Composing Robust\nFeatures with Denoising Autoencoders. In International Conference on Machine Learning (ICML) .\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani Luotolahti, Tapio Salakoski, Filip Ginter, and Sampo Pyysalo.\n2019. Multilingual is not enough: BERT for Finnish. arXiv preprint arXiv:1912.07076 (2019).\nRob Voigt, Nicholas P Camp, Vinodkumar Prabhakaran, William L Hamilton, Rebecca C Hetey, Camilla M Griffiths, David\nJurgens, Dan Jurafsky, and Jennifer L Eberhardt. 2017. Language from police body camera footage shows racial disparities\nin officer respect. Proceedings of the National Academy of Sciences 114, 25 (2017), 6521–6526.\nRob Voigt, David Jurgens, Vinodkumar Prabhakaran, Dan Jurafsky, and Yulia Tsvetkov. 2018. RtGender: A Corpus for\nStudying Differential Responses to Gender. In Proceedings of the Eleventh International Conference on Language Resources\nand Evaluation (LREC 2018) . European Language Resources Association (ELRA), Miyazaki, Japan. https://aclanthology.\norg/L18-1445\nElena Voita and Ivan Titov. 2020. Information-Theoretic Probing with Minimum Description Length. arXiv:2003.12298 [cs.CL]\nAndrew Vold and Jack G Conrad. 2021. Using Transformers to Improve Answer Retrieval for Legal Questions. (2021).\nSoroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false news online. Science 359, 6380 (2018),\n1146–1151. https://doi.org/10.1126/science.aap9559 arXiv:https://science.sciencemag.org/content/359/6380/1146.full.pdf\nLyndsey Wajert and Gabe Rottman. 2019. Scraping public websites likely doesn’t violate the Computer Fraud and Abuse\nAct, court holds. https://www.rcfp.org/scraping-not-violation-cfaa/.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers for Attacking\nand Analyzing NLP. In Empirical Methods in Natural Language Processing .\nW Patrick Walters and Regina Barzilay. 2020. Applications of deep learning in molecule generation and molecular property\nprediction. Accounts of Chemical Research 54, 2 (2020), 263–270.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. 2019a. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Advances\nin Neural Information Processing Systems (NeurIPS) .\nOn the Opportunities and Risks of Foundation Models 209\nAlex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019b. GLUE: A Multi-\nTask Benchmark and Analysis Platform for Natural Language Understanding. In International Conference on Learning\nRepresentations (ICLR).\nBen Wang. 2021. Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX.\nhttps://github.com/kingoflolz/mesh-transformer-jax.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.\ncom/kingoflolz/mesh-transformer-jax.\nHaojie Wang, Jidong Zhai, Mingyu Gao, Zixuan Ma, Shizhi Tang, Liyan Zheng, Yuanzhi Li, Kaiyuan Rong, Yuanyong Chen,\nand Zhihao Jia. 2021c. PET: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated\nCorrections. In 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21) . 37–54.\nLijun Wang, Wanli Ouyang, Xiaogang Wang, and Huchuan Lu. 2015b. Visual tracking with fully convolutional networks. In\nProceedings of the IEEE international conference on computer vision . 3119–3127.\nMingzhe Wang and Jia Deng. 2020. Learning to Prove Theorems by Learning to Generate Theorems. In Advances in\nNeural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual , Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien\nLin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/d2a27e83d429f0dcae6b937cf440aeb1-Abstract.html\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020c. Linformer: Self-Attention with Linear Complexity.\narXiv preprint arXiv:2006.04768 (2020).\nTongzhou Wang and Phillip Isola. 2020. Understanding contrastive representation learning through alignment and uniformity\non the hypersphere. In International Conference on Machine Learning . PMLR, 9929–9939.\nTianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. 2019d. Balanced datasets are not enough:\nEstimating and mitigating gender bias in deep image representations. In Proceedings of the IEEE/CVF International\nConference on Computer Vision . 5310–5319.\nWenhui Wang, Sen Yang, Xiang Zhang, and Jing Li. 2014. Drug repositioning by integrating target information through a\nheterogeneous network model. Bioinformatics 30, 20 (2014), 2923–2930.\nXiaolong Wang, David Fouhey, and Abhinav Gupta. 2015a. Designing deep networks for surface normal estimation. In\nProceedings of the IEEE conference on computer vision and pattern recognition . 539–547.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021a. KEPLER: A\nunified model for knowledge embedding and pre-trained language representation. Transactions of the Association for\nComputational Linguistics 9 (2021), 176–194.\nXuefeng Wang, Eric P Xing, and Daniel J Schaid. 2015c. Kernel methods for large-scale genomic data analysis. Briefings in\nbioinformatics 16, 2 (2015), 183–192.\nYu Wang, Jinchao Li, Tristan Naumann, Chenyan Xiong, Hao Cheng, Robert Tinn, Cliff Wong, Naoto Usuyama, Richard\nRogahn, Zhihong Shen, et al. 2021b. Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature.\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) .\nZihan Wang, Karthikeyan K, Stephen Mayhew, and Dan Roth. 2020a. Extending Multilingual BERT to Low-Resource\nLanguages. arXiv:2004.13640 [cs] (Apr 2020). http://arxiv.org/abs/2004.13640\nZihan Wang, K Karthikeyan, Stephen Mayhew, and Dan Roth. 2020b. Extending Multilingual BERT to Low-Resource\nLanguages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings . 2649–\n2656.\nZirui Wang, Zachary C Lipton, and Yulia Tsvetkov. 2020d. On Negative Interference in Multilingual Language Models. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) . 4438–4450.\nZiheng Wang, Jeremy Wohlwend, and Tao Lei. 2019c. Structured Pruning of Large Language Models. arXiv preprint\narXiv:1910.04732 (2019).\nZeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. 2017. Understanding Abuse: A Typology of\nAbusive Language Detection Subtasks. In Proceedings of the First Workshop on Abusive Language Online . Association for\nComputational Linguistics, Vancouver, BC, Canada, 78–84. https://doi.org/10.18653/v1/W17-3012\nK Washington, DS Browitt, K Murata, D Monroe, and T Heames. 1995. KBERT. Knowledge Based Estimation of Material\nRelease Transients. Technical Report. Sandia National Labs., Albuquerque, NM (United States).\nColin Wei, Sham Kakade, and Tengyu Ma. 2020a. The Implicit and Explicit Regularization Effects of Dropout. InInternational\nConference on Machine Learning .\nColin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. 2020b. Theoretical analysis of self-training with deep networks on\nunlabeled data. arXiv preprint arXiv:2010.03622 (2020).\nColin Wei, Sang Michael Xie, and Tengyu Ma. 2021. Why Do Pretrained Language Models Help in Downstream Tasks? An\nAnalysis of Head and Prompt Tuning. arXiv:2106.09226 [cs.LG]\nKen Weiner. 2018. Can AI Create True Art? Scientific American (2018).\n210 Center for Research on Foundation Models (CRFM)\nLaurel Westbrook and Aliya Saperstein. 2015. New Categories Are Not Enough: Rethinking the Measurement of Sex\nand Gender in Social Surveys. Gender & Society 29, 4 (2015), 534–560. https://doi.org/10.1177/0891243215584758\narXiv:https://doi.org/10.1177/0891243215584758\nHannes Westermann, Vern R Walker, Kevin D Ashley, and Karim Benyekhlef. 2019. Using factors to predict and analyze\nlandlord-tenant decisions to increase access to justice. In Proceedings of the Seventeenth International Conference on\nArtificial Intelligence and Law . 133–142.\nJason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint arXiv:1410.3916 (2014).\nMichelle Whirl-Carrillo, Ellen M McDonagh, JM Hebert, Li Gong, K Sangkuhl, CF Thorn, Russ B Altman, and Teri E Klein.\n2012. Pharmacogenomics knowledge for personalized medicine. Clinical Pharmacology & Therapeutics 92, 4 (2012),\n414–417.\nJenna Wiens, Suchi Saria, Mark Sendak, Marzyeh Ghassemi, Vincent X Liu, Finale Doshi-Velez, Kenneth Jung, Katherine\nHeller, David Kale, Mohammed Saeed, et al. 2019. Do no harm: a roadmap for responsible machine learning for health\ncare. Nature medicine 25, 9 (2019), 1337–1340.\nBernard Williams. 1973. In Utilitarianism: For and Against , J.C. Smart and Bernard Williams (Eds.). Cambridge University\nPress, 82–118.\nMonnica T. Williams. 2020. Psychology cannot afford to ignore the many harms caused by microaggressions. Perspectives\non Psychological Science 15, 1 (2020), 38–43.\nAngelica Willis, Glenn Davis, Sherry Ruan, Lakshmi Manoharan, James Landay, and Emma Brunskill. 2019. Key Phrase\nExtraction for Generating Educational Question-Answer Pairs. In Proceedings of the Sixth (2019) ACM Conference on\nLearning @ Scale (Chicago, IL, USA) (L@S ’19). Association for Computing Machinery, New York, NY, USA, Article 20,\n10 pages. https://doi.org/10.1145/3330430.3333636\nBenjamin Wilson, Judy Hoffman, and Jamie Morgenstern. 2019. Predictive Inequity in Object Detection. (2019).\nhttps://arxiv.org/pdf/1902.11097.pdf.\nChristo Wilson, Avijit Ghosh, Shan Jiang, Alan Mislove, Lewis Baker, Janelle Szary, Kelly Trindel, and Frida Polli. 2021.\nBuilding and Auditing Fair Algorithms: A Case Study in Candidate Screening. InProceedings of the Conference on Fairness,\nAccountability, and Transparency (FAccT 2021) . Virtual Event, Canada.\nJulia K. Winkler, Christine Fink, Ferdinand Toberer, Alexander Enk, Teresa Deinlein, Rainer Hofmann-Wellenhof, Luc\nThomas, Aimilios Lallas, Andreas Blum, Wilhelm Stolz, and Holger A. Haenssle. 2019. Association Between Surgical Skin\nMarkings in Dermoscopic Images and Diagnostic Performance of a Deep Learning Convolutional Neural Network for\nMelanoma Recognition. JAMA Dermatology 155, 10 (10 2019), 1135–1141. https://doi.org/10.1001/jamadermatol.2019.1735\narXiv:https://jamanetwork.com/journals/jamadermatology/articlepdf/2740808/jamadermatology_winkler_2019_oi_190038.pdf\nLangdon Winner. 1980. Do Artifacts Have Politics? Daedalus 109, 1 (1980), 121–136. http://www.jstor.org/stable/20024652\nPublisher: The MIT Press.\nL. Wittgenstein. 1953. Philosophical Investigations . Blackwell, Oxford.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan\nFuntowicz, Joe Davison, Sam Shleifer, et al . 2020. Transformers: State-of-the-Art Natural Language Processing. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations . 38–45.\nDavid H Wolpert and William G Macready. 1997. No free lunch theorems for optimization.IEEE transactions on evolutionary\ncomputation 1, 1 (1997), 67–82.\nEric Wong and J. Zico Kolter. 2020. Learning perturbation sets for robust machine learning. arXiv (2020).\nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and\nNathan Srebro. 2020. Kernel and rich regimes in overparametrized models. arXiv preprint arXiv:2002.09277 (2020).\nBeverly Park Woolf, H. Chad Lane, Vinay K. Chaudhri, and Janet L. Kolodner. 2013. AI Grand Challenges for Education. AI\nMagazine 34, 4 (Dec. 2013), 66–84. https://doi.org/10.1609/aimag.v34i4.2490\nMitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and\nLudwig Schmidt. 2021. Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903 (2021).\nOlivier J Wouters, Martin McKee, and Jeroen Luyten. 2020. Estimated research and development investment needed to\nbring a new medicine to market, 2009-2018. Jama 323, 9 (2020), 844–853.\nBohan Wu, Suraj Nair, Roberto Martín-Martín, Li Fei-Fei, and Chelsea Finn. 2021d. Greedy Hierarchical Variational\nAutoencoders for Large-Scale Video Prediction. arXiv preprint arXiv:2103.04174 (2021).\nEric Wu, Kevin Wu, Roxana Daneshjou, David Ouyang, Daniel E Ho, and James Zou. 2021g. How medical AI devices are\nevaluated: limitations and recommendations from an analysis of FDA approvals. Nature Medicine 27, 4 (2021), 582–584.\nKevin E Wu, Kathryn E Yost, Howard Y Chang, and James Zou. 2021h. BABEL enables cross-modality translation between\nmultiomic profiles at single-cell resolution. Proceedings of the National Academy of Sciences 118, 15 (2021).\nMike Wu, Chris Piech, Noah Goodman, and Chelsea Finn. 2021e. ProtoTransformer: A Meta-Learning Approach to Providing\nStudent Feedback. arxiv (2021).\nOn the Opportunities and Risks of Foundation Models 211\nShijie Wu and Mark Dredze. 2019. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong, China, 833–844.\nhttps://doi.org/10.18653/v1/D19-1077\nShijie Wu and Mark Dredze. 2020. Are All Languages Created Equal in Multilingual BERT?. In Proceedings of the 5th\nWorkshop on Representation Learning for NLP . 120–130. https://aclanthology.org/2020.repl4nlp-1.16\nYuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Grosse. 2021a. INT: An Inequality Benchmark for Evaluating Generalization\nin Theorem Proving. (2021). https://openreview.net/forum?id=O6LPudowNQm\nYuhuai Wu, Markus N. Rabe, Wenda Li, Jimmy Ba, Roger B. Grosse, and Christian Szegedy. 2021f. LIME: Learning Inductive\nBias for Primitives of Mathematical Reasoning. (2021).\nZachary Wu, Kadina E Johnston, Frances H Arnold, and Kevin K Yang. 2021b. Protein sequence design with deep generative\nmodels. Current Opinion in Chemical Biology 65 (2021), 18–27.\nZhengxuan Wu, Nelson F Liu, and Christopher Potts. 2021c. Identifying the Limits of Cross-Domain Knowledge Transfer\nfor Pretrained Models. arXiv preprint arXiv:2104.08410 (2021).\nZhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. 2018. Unsupervised Feature Learning via Non-parametric Instance\nDiscrimination. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018), 3733–3742.\nAlice Xiang. 2021. Reconciling legal and technical approaches to algorithmic bias. Tennessee Law Review 88, 3 (2021).\nKai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. 2020. Noise or Signal: The Role of Image Backgrounds in\nObject Recognition. arXiv preprint arXiv:2006.09994 (2020).\nTete Xiao, Xiaolong Wang, Alexei A. Efros, and Trevor Darrell. 2021. What Should Not Be Contrastive in Contrastive\nLearning. arXiv:2008.05659 [cs.CV]\nMichael Xie, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon. 2016. Transfer Learning from Deep Features for\nRemote Sensing and Poverty Mapping. In Association for the Advancement of Artificial Intelligence (AAAI) .\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. 2020. Self-training with Noisy Student improves ImageNet\nclassification. arXiv (2020).\nSang Michael Xie, Ananya Kumar, Robert Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. 2021a. In-N-Out: Pre-Training\nand Self-Training using Auxiliary Information for Out-of-Distribution Robustness. InInternational Conference on Learning\nRepresentations (ICLR).\nSang Michael Xie, Tengyu Ma, and Percy Liang. 2021b. Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders\nfor Improved Generalization. International Conference on Machine Learning (ICML) (2021).\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021c. An Explanation of In-context Learning as\nImplicit Bayesian Inference. arXiv preprint arXiv:2111.02080 (2021).\nCaiming Xiong, Stephen Merity, and Richard Socher. 2016. Dynamic memory networks for visual and textual question\nanswering. In International conference on machine learning . 2397–2406.\nAlbert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein. 2021. Detoxifying Language\nModels Risks Marginalizing Minority Voices. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics,\nOnline, 2390–2397. https://doi.org/10.18653/v1/2021.naacl-main.190\nHuijuan Xu and Kate Saenko. 2016. Ask, attend and answer: Exploring question-guided spatial attention for visual question\nanswering. In European Conference on Computer Vision . Springer, 451–466.\nJian Xu, Sunkyu Kim, Min Song, Minbyul Jeong, Donghyeon Kim, Jaewoo Kang, Justin F Rousseau, Xin Li, Weijia Xu, Vetle I\nTorvik, et al. 2020. Building a PubMed knowledge graph. Scientific data 7, 1 (2020), 1–15.\nJia Xu, Pengwei Yang, Shang Xue, Bhuvan Sharma, Marta Sanchez-Martin, Fang Wang, Kirk A Beaty, Elinor Dehan, and Baiju\nParikh. 2019. Translating cancer genomics into precision medicine with artificial intelligence: applications, challenges\nand future perspectives. Human genetics 138, 2 (2019), 109–124.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2021.\nByT5: Towards a Token-Free Future with Pre-Trained Byte-to-Byte Models.arXiv preprint arXiv:2105.13626 (2021).\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel.\n2020. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934 (2020).\nEugene Yang, Sean MacAvaney, David D Lewis, and Ophir Frieder. 2021. Goldilocks: Just-Right Tuning of BERT for\nTechnology-Assisted Review. arXiv preprint arXiv:2105.01044 (2021).\nMengjiao Yang and Ofir Nachum. 2021. Representation Matters: Offline Pretraining for Sequential Decision Making. In\nICML.\nQian Yang, Aaron Steinfeld, Carolyn P Rose, , and John Zimmerman. 2020. Re-examining Whether, Why, and How Human-AI\nInteraction Is Uniquely Difficult to Design. In Proceedings of the 2020 CHI Conference on Human Factors in Computing\nSystems.\n212 Center for Research on Foundation Models (CRFM)\nQian Yang, Jina Suh, Nan-Chen Chen, and Gonzalo Ramos. 2018. Grounding Interactive Machine Learning Tool Design in\nHow Non-Experts Actually Build Models. In DIS ’18: Proceedings of the 2018 Designing Interactive Systems Conference .\nACM.\nQian Yang, John Zimmerman, Aaron Steinfeld, and Anthony Tomasic. 2016. Planning Adaptive Mobile Experiences When\nWireframing. In Proceedings of the 2016 ACM Conference on Designing Interactive Systems . ACM.\nZhilin Yang, Zihang Dai, Yiming Yang, J. Carbonell, R. Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized Autoregres-\nsive Pretraining for Language Understanding. In NeurIPS.\nMichihiro Yasunaga and Percy Liang. 2020. Graph-based, Self-Supervised Program Repair from Diagnostic Feedback. In\nInternational Conference on Machine Learning (ICML) .\nMichihiro Yasunaga and Percy Liang. 2021. Break-It-Fix-It: Unsupervised Learning for Program Repair. In International\nConference on Machine Learning (ICML) .\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021. QA-GNN: Reasoning with\nLanguage Models and Knowledge Graphs for Question Answering. arXiv preprint arXiv:2104.06378 (2021).\nNanyang Ye, Kaican Li, Lanqing Hong, Haoyue Bai, Yiting Chen, Fengwei Zhou, and Zhenguo Li. 2021. OoD-Bench:\nBenchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms.\nKexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. 2019. Clevrer:\nCollision events for video representation and reasoning. arXiv preprint arXiv:1910.01442 (2019).\nPengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. 2020. TaBERT: Pretraining for Joint Understanding of\nTextual and Tabular Data. In ACL.\nDani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki\nLazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. 2019. Learning and Evaluating General Linguistic Intelligence. arXiv\npreprint arXiv:1901.11373 (2019).\nNaruki Yoshikawa, Ryuichi Kubo, and Kazuki Z. Yamamoto. 2021. Twitter integration of chemistry software tools. Journal\nof Cheminformatics 13, 1 (July 2021). https://doi.org/10.1186/s13321-021-00527-x\nJiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. 2018. Graph Convolutional Policy Network for Goal-\nDirected Molecular Graph Generation. InProceedings of the 32nd International Conference on Neural Information Processing\nSystems. Curran Associates Inc., Red Hook, NY, USA, 6412–6422.\nChao Yu, Jiming Liu, and Shamim Nemati. 2019. Reinforcement learning in healthcare: A survey. arXiv preprint\narXiv:1908.08796 (2019).\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, An-\ndre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. 2022. Differentially Private Fine-tuning of Language\nModels. In International Conference on Learning Representations . https://openreview.net/forum?id=Q42f0dfjECO\nDonghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. 2020c. Jaket: Joint pre-training of knowledge graph and\nlanguage understanding. arXiv preprint arXiv:2010.00796 (2020).\nFisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell.\n2020a. BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) .\nKun-Hsing Yu, Andrew L Beam, and Isaac S Kohane. 2018. Artificial intelligence in healthcare.Nature biomedical engineering\n2, 10 (2018), 719–731.\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. 2020b.\nMOPO: Model-based Offline Policy Optimization. arXiv preprint arXiv:2005.13239 (2020).\nEliezer Yudkowsky. 2016. The AI alignment problem: why it is hard, and where to start. Symbolic Systems Distinguished\nSpeaker (2016).\nEliezer Yudkowsky et al. 2008. Artificial intelligence as a positive and negative factor in global risk. Global catastrophic risks\n1, 303 (2008), 184.\nJeffrey M Zacks, Barbara Tversky, and Gowri Iyer. 2001. Perceiving, remembering, and communicating structure in events.\nJournal of experimental psychology: General 130, 1 (2001), 29.\nMatei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauly, Michael J Franklin, Scott\nShenker, and Ion Stoica. 2012. Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing.\nIn 9th {USENIX}Symposium on Networked Systems Design and Implementation ( {NSDI}12). 15–28.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham,\nAnirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big Bird: Transformers for Longer Sequences.. In NeurIPS.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. 2021. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-\nbased Masked Language-models. CoRR abs/2106.10199 (2021). arXiv:2106.10199 https://arxiv.org/abs/2106.10199\nAmir R. Zamir, Alexander Sax, William B. Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. 2018. Taskonomy:\nDisentangling Task Transfer Learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE.\nOn the Opportunities and Risks of Foundation Models 213\nJakub Zavrel, Walter Daelemans, and Jorn Veenstra. 1997. Resolving PP attachment ambiguities with memory-based\nlearning. In CoNLL97: Computational Natural Language Learning .\nMatthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In European conference on\ncomputer vision . Springer, 818–833.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019a. From Recognition to Cognition: Visual Commonsense\nReasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .\nRowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, and Yejin Choi. 2021a.\nPIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World. arXiv preprint arXiv:2106.00188\n(2021).\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019b.\nDefending Against Neural Fake News. In Advances in Neural Information Processing Systems (NeurIPS) . 9054–9065.\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. 2021b. MERLOT:\nMultimodal Neural Script Knowledge Models. arXiv preprint arXiv:2106.02636 (2021).\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2021. Scaling vision transformers. arXiv preprint\narXiv:2106.04560 (2021).\nHaoran Zhang, Amy X Lu, Mohamed Abdalla, Matthew McDermott, and Marzyeh Ghassemi. 2020b. Hurtful words:\nquantifying biases in clinical contextual word embeddings. In proceedings of the ACM Conference on Health, Inference,\nand Learning . 110–120.\nMichael Zhang and Eunsol Choi. 2021. SituatedQA: Incorporating Extra-Linguistic Contexts into QA. In Proceedings of the\n2021 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Online\nand Punta Cana, Dominican Republic, 7371–7387. https://doi.org/10.18653/v1/2021.emnlp-main.586\nT. Zhang and T. Hashimoto. 2020. On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic\nDependencies. In Association for Computational Linguistics (ACL) .\nTianyi Zhang and Tatsunori Hashimoto. 2021. On the Inductive Bias of Masked Language Modeling: From Statistical to\nSyntactic Dependencies. arXiv preprint arXiv:2104.05694 (2021).\nXingliang Zhang and Degan Shu. 2021. Current understanding on the Cambrian Explosion: questions and answers.\nPaläontologische Zeitschrift 95 (2021), 641–660.\nYuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. 2020a. Contrastive learning of\nmedical visual representations from paired images and text. arXiv preprint arXiv:2010.00747 (2020).\nYuhui Zhang, Allen Nie, Ashley Zehnder, Rodney L Page, and James Zou. 2019b. VetTag: improving automated veterinary\ndiagnosis coding via large-scale language modeling. NPJ digital medicine 2, 1 (2019), 1–8.\nYian Zhang, Alex Warstadt, Haau-Sing Li, and Samuel R Bowman. 2021. When Do You Need Billions of Words of Pretraining\nData?. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics .\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019a. ERNIE: Enhanced Language\nRepresentation with Informative Entities. In ACL.\nZhoutong Zhang, Qiujia Li, Zhengjia Huang, Jiajun Wu, Joshua B Tenenbaum, and William T Freeman. 2017. Shape and\nmaterial from sound. (2017).\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender Bias in\nContextualized Word Embeddings. InProceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) . Association for Computational\nLinguistics, Minneapolis, Minnesota, 629–634. https://doi.org/10.18653/v1/N19-1064\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men Also Like Shopping: Reducing\nGender Bias Amplification using Corpus-level Constraints. In Proceedings of the 2017 Conference on Empirical Methods\nin Natural Language Processing . Association for Computational Linguistics, Copenhagen, Denmark, 2979–2989. https:\n//doi.org/10.18653/v1/D17-1323\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-Wei Chang. 2018. Learning Gender-Neutral Word Embeddings. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . Association for Computational\nLinguistics, Brussels, Belgium, 4847–4853. https://doi.org/10.18653/v1/D18-1521\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Schütze. 2020b. Masking as an Efficient Alternative to Finetuning for\nPretrained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP). Association for Computational Linguistics, Online, 2226–2241. https://doi.org/10.18653/v1/2020.emnlp-\nmain.174\nQingyu Zhao, Ehsan Adeli, and Kilian M Pohl. 2020a. Training confounder-free deep learning models for medical applications.\nNature communications 11, 1 (2020), 1–9.\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot\nperformance of language models. arXiv preprint arXiv:2102.09690 (2021).\n214 Center for Research on Foundation Models (CRFM)\nLucia Zheng, Neel Guha, Brandon R Anderson, Peter Henderson, and Daniel E Ho. 2021. When Does Pretraining Help?\nAssessing Self-Supervised Learning for Law and the CaseHOLD Dataset. arXiv preprint arXiv:2104.08671 .\nHaoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2020. How does NLP benefit\nlegal system: A summary of legal artificial intelligence. arXiv preprint arXiv:2004.12158 (2020).\nJin Peng Zhou, Yuhuai Wu, Colin Li, and Roger Grosse. 2021c. REFACTOR: Learning to Extract Theorems from Proofs. The\nFirst Mathematical Reasoning in General Artificial Intelligence Workshop, ICLR 2021 (2021). https://mathai-iclr.github.io/\npapers/papers/MATHAI_22_paper.pdf\nKaitlyn Zhou, Kawin Ethayarajh, and Dan Jurafsky. 2021a. Frequency-based Distortions in Contextualized Word Embeddings.\nArXiv abs/2104.08465 (2021). https://arxiv.org/abs/2104.08465\nSharon Zhou, Mitchell L Gordon, Ranjay Krishna, Austin Narcomey, Li Fei-Fei, and Michael S Bernstein. 2019. Hype: A\nbenchmark for human eye perceptual evaluation of generative models. NeurIPS (2019).\nXiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and William Yang Wang. 2020. Hulk: An energy efficiency benchmark platform for\nresponsible natural language processing. arXiv preprint arXiv:2002.05829 (2020).\nXuhui Zhou, Maarten Sap, Swabha Swayamdipta, Yejin Choi, and Noah Smith. 2021b. Challenges in Automated De-\nbiasing for Toxic Language Detection. In Proceedings of the 16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume . Association for Computational Linguistics, Online, 3143–3155.\nhttps://aclanthology.org/2021.eacl-main.274\nYiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal Frossard. 2018. Adaptive Quantization for\nDeep Neural Networks. In Thirty-Second AAAI Conference on Artificial Intelligence .\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. 2020. Modifying\nMemories in Transformer Models. arXiv:2012.00363 [cs.CL]\nMing Zhu, Aman Ahuja, Wei Wei, and Chandan K Reddy. 2019. A hierarchical attention retrieval model for healthcare\nquestion answering. In The World Wide Web Conference . 2472–2482.\nBrian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. 2008. Maximum Entropy Inverse Reinforcement\nLearning. In Association for the Advancement of Artificial Intelligence (AAAI) .\nAnnette Zimmerman. 2020. If You Can Do Things with Words, You Can Do Things with Algorithms. https://dailynous.\ncom/2020/07/30/philosophers-gpt-3/#zimmermann\nAnnette Zimmermann. 2021. Stop Building Bad AI. Boston Review (July 2021). https://bostonreview.net/science-nature/\nannette-zimmermann-stop-building-bad-ai\nRoland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. 2021. Contrastive Learning\nInverts the Data Generating Process. arXiv preprint arXiv:2102.08850 (2021).\nLuisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. 2017. Visualizing deep neural network decisions: Prediction\ndifference analysis. arXiv preprint arXiv:1702.04595 (2017).\nShoshana Zuboff. 2018. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power .\nDaniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann. 2021. Language-agnostic\nrepresentation learning of source code from structure and context. arXiv preprint arXiv:2103.11318 (2021).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5773220062255859
    },
    {
      "name": "Software deployment",
      "score": 0.575282096862793
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5456886291503906
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5277671813964844
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.5253685712814331
    },
    {
      "name": "Sociotechnical system",
      "score": 0.48487743735313416
    },
    {
      "name": "Data science",
      "score": 0.4267963171005249
    },
    {
      "name": "Deep learning",
      "score": 0.4116297662258148
    },
    {
      "name": "Management science",
      "score": 0.3683730959892273
    },
    {
      "name": "Engineering ethics",
      "score": 0.3633073568344116
    },
    {
      "name": "Political science",
      "score": 0.182239830493927
    },
    {
      "name": "Engineering",
      "score": 0.17286711931228638
    },
    {
      "name": "Law",
      "score": 0.14342504739761353
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}