{
    "title": "Evaluating the Effectiveness of Large Language Models in Abstract Screening: A Comparative Analysis",
    "url": "https://openalex.org/W4393228461",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2120387538",
            "name": "Michael Li",
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ]
        },
        {
            "id": "https://openalex.org/A2095804757",
            "name": "Jianping Sun",
            "affiliations": [
                "University of North Carolina at Greensboro"
            ]
        },
        {
            "id": "https://openalex.org/A2159169709",
            "name": "Xianming Tan",
            "affiliations": [
                "University of North Carolina at Chapel Hill"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2567362164",
        "https://openalex.org/W4294215472",
        "https://openalex.org/W2106952837",
        "https://openalex.org/W6680839639",
        "https://openalex.org/W2146668368",
        "https://openalex.org/W2944878516",
        "https://openalex.org/W2116153150",
        "https://openalex.org/W2116489236",
        "https://openalex.org/W4313371821",
        "https://openalex.org/W1598602811",
        "https://openalex.org/W2963499153",
        "https://openalex.org/W4315641584",
        "https://openalex.org/W3010604170",
        "https://openalex.org/W2058168837",
        "https://openalex.org/W6927811720",
        "https://openalex.org/W2604515130",
        "https://openalex.org/W3209727249",
        "https://openalex.org/W4283818160",
        "https://openalex.org/W2159774315",
        "https://openalex.org/W2093268197",
        "https://openalex.org/W2068225985",
        "https://openalex.org/W1497939657",
        "https://openalex.org/W4241926107",
        "https://openalex.org/W2115881827",
        "https://openalex.org/W6748328607",
        "https://openalex.org/W4367368990",
        "https://openalex.org/W4383722515",
        "https://openalex.org/W2139739986",
        "https://openalex.org/W2789758093"
    ],
    "abstract": "<title>Abstract</title> <bold>Objective:</bold>This study aimed to evaluate the performance of Large Language Models (LLMs) in the task of abstract screening in systematic review and meta-analysis studies, exploring their effectiveness, efficiency, and potential integration into existing human expert-based workflows. <bold>Methods:</bold>We developed automation scripts in Python to interact with the APIs of several LLM tools, including ChatGPT v4.0, ChatGPT v3.5, Google PaLM, and Meta Llama 2. This study focused on three databases of abstracts and used them as benchmarks to evaluate the performance of these LLM tools in terms of sensitivity, specificity, and overall accuracy. The results of the LLM tools were compared to human-curated inclusion decisions, gold standard for systematic review and meta-analysis studies. <bold>Results:</bold>Different LLM tools had varying abilities in abstract screening. Chat GPT v4.0 demonstrated remarkable performance, with balanced sensitivity and specificity, and overall accuracy consistently reaching or exceeding 90%, indicating a high potential for LLMs in abstract screening tasks. The study found that LLMs could provide reliable results with minimal human effort and thus serve as a cost-effective and efficient alternative to traditional abstract screening methods. <bold>Conclusion:</bold>While LLM tools are not yet ready to completely replace human experts in abstract screening, they show great promise in revolutionizing the process. They can serve as autonomous AI reviewers, contribute to collaborative workflows with human experts, and integrate with hybrid approaches to develop custom tools for increased efficiency. As technology continues to advance, LLMs are poised to play an increasingly important role in abstract screening, reshaping the workflow of systemic review and meta-analysis studies.",
    "full_text": "Page 1/26\nEvaluating the Effectiveness of Large Language\nModels in Abstract Screening: A Comparative\nAnalysis\nMichael Li \nUNC-Chapel Hill: The University of North Carolina at Chapel Hill https://orcid.org/0000-0003-0153-\n8518\nJianping Sun \nUNC Greensboro: University of North Carolina at Greensboro\nXianming Tan \nUNC-Chapel Hill: The University of North Carolina at Chapel Hill\nResearch Article\nKeywords: Large Language Models, Abstract Screening, Automated Text Analysis, ChatGPT, Google\nPaLM, Meta Llama\nPosted Date: March 27th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3788921/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nVersion of Record: A version of this preprint was published at Systematic Reviews on August 21st, 2024.\nSee the published version at https://doi.org/10.1186/s13643-024-02609-x.\nPage 2/26\nAbstract\nObjective:This study aimed to evaluate the performance of Large Language Models (LLMs) in the task of\nabstract screening in systematic review and meta-analysis studies, exploring their effectiveness,\ne\u0000ciency, and potential integration into existing human expert-based work\u0000ows.\nMethods:We developed automation scripts in Python to interact with the APIs of several LLM tools,\nincluding ChatGPT v4.0, ChatGPT v3.5, Google PaLM, and Meta Llama 2. This study focused on three\ndatabases of abstracts and used them as benchmarks to evaluate the performance of these LLM tools\nin terms of sensitivity, speci\u0000city, and overall accuracy. The results of the LLM tools were compared to\nhuman-curated inclusion decisions, gold standard for systematic review and meta-analysis studies.\nResults:Different LLM tools had varying abilities in abstract screening. Chat GPT v4.0 demonstrated\nremarkable performance, with balanced sensitivity and speci\u0000city, and overall accuracy consistently\nreaching or exceeding 90%, indicating a high potential for LLMs in abstract screening tasks. The study\nfound that LLMs could provide reliable results with minimal human effort and thus serve as a cost-\neffective and e\u0000cient alternative to traditional abstract screening methods.\nConclusion:While LLM tools are not yet ready to completely replace human experts in abstract screening,\nthey show great promise in revolutionizing the process. They can serve as autonomous AI reviewers,\ncontribute to collaborative work\u0000ows with human experts, and integrate with hybrid approaches to\ndevelop custom tools for increased e\u0000ciency. As technology continues to advance, LLMs are poised to\nplay an increasingly important role in abstract screening, reshaping the work\u0000ow of systemic review and\nmeta-analysis studies.\n1. Introduction\nA systematic review is a rigorous form of research that collates and synthesizes all existing evidence on\na speci\u0000c research question [1]. It stands as a cornerstone not just in medical research but across\ndiverse academic disciplines. Unlike traditional literature reviews, systematic reviews follow a\ncomprehensive and standardized process, such as the PRISMA (Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses) guideline [2], designed to minimize bias and ensure\nreproducibility. Therefore, these reviews are recognized as one of the zenith levels of evidence in\nevidence-based research [3], and play a pivotal role in shaping clinical guidelines, healthcare policies, and\ninforming medical decisions [4].\nCommencing with a well-articulated research question, a typical systematic review launches an\nexhaustive search strategy that subsequently sweeps through databases, such as PubMed and Embase,\nsupplemented by additional sources such as clinical trial registries and pertinent article reference lists,\nwith aims to capture all relevant studies and mitigate bias. Predetermined inclusion and exclusion\ncriteria guide the subsequent screening and selection of studies, encompassing facts like study design,\npatient demographics, and intervention types. Reviewers, working independently, appraise each study's\nPage 3/26\neligibility, reconciling disagreements through discussions or third-party reviews. Data extraction and\nsynthesis follow, either through meta-analysis or narrative synthesis, depending on the heterogeneity of\nthe selected studies.\nThe practice of conducting systematic reviews has gained substantial popularity with considerable\ndemand within the academic community. A notable reference [5] to this trend found that approximately\n75 trials and potentially 11 systematic reviews are disseminated daily, based on data from around 2010.\nMoreover, a query for \"systematic review\" in the Google Scholar database yields approximately 17,000\nentries that have been published within the year 2023 alone, which translates into about 51 systematic\nreviews per day, as observed on November 26, 2023. This expanding volume of literature underscores\nthe critical role that systematic reviews play in the consolidation of research \u0000ndings across various\n\u0000elds of study.\nDespite their pivotal role, executing systematic reviews remains a formidable task due to the abstract\nscreening, a key phase that can be overwhelmingly time-consuming due to its volume. For example, in\nPolanin et al. [6], the authors reported that the research staff screened 29,846 abstracts independently\n(14,923 unique citations were double screened) over the course of 189 days. In addition, the variability in\nreporting standards, use of jargon, and varied study designs can further complicate the abstract\nscreening process [7]. Nevertheless, the repetitive nature of the task, combined with cognitive fatigue,\ncan lead to human errors [8]. Recent advancements in machine learning (ML) and deep learning propose\npossible solutions to these challenges. However, traditional ML models, while promising, require domain-\nspeci\u0000c training, a time-consuming process that often demands manual labeling of datasets [9].\nWith the advancements in technology, such as Natural Language Processing (NLP) and arti\u0000cial\nintelligence (AI), new avenues are being explored to address these concerns. Among the emerging\nsolutions, AI models like ChatGPT [10], PALM [11], and Llama 2 [12], are capturing the research\ncommunity's attention. Their collective potential, especially their capability to operate without exhaustive\ndomain-speci\u0000c training, makes them prime candidates for revolutionizing the systematic review\nprocess.\nWhile each of the aforementioned AI tools brings its unique capabilities to the table, the fundamental\nquestion remains: How do they stack up, both individually and collectively, in the realm of abstract\nscreening, against the human expert-based process? In pursuit of answers, this research seeks to\nthoroughly investigate the potential of ChatGPT, Google PALM, and Llama in automating the crucial\nabstract screening phase integral to systematic reviews. Our goal is to rigorously compare the\nperformance of these advanced AI-driven methods with existing machine learning (ML)-based\napproaches. In doing so, we aim to develop AI strategies that masterfully balance e\u0000ciency and\naccuracy with minimal human intervention, ultimately transforming systematic review practice across\ndisciplines.\nThe remainder of this paper is structured to provide a comprehensive exploration of our topic. We begin\nwith an in-depth examination of existing methods for abstract screening, including both manual and\nPage 4/26\nNLP-based approaches, laying the groundwork for understanding the current state of the \u0000eld. We then\nintroduce the use of Large Language Model (LLM) tools for abstract screening, detailing our\nexperimental design to meticulously evaluate their performance in this context. Subsequent sections\npresent our empirical \u0000ndings and results, shedding light on the capabilities and limitations of the AI\ntools in question. Finally, we engage in a thoughtful discussion, re\u0000ecting on the implications of our\n\u0000ndings and considering the future trajectory of abstract screening in systematic reviews.\n2. Existing Approaches to Abstract Screening in Systematic Reviews\nIn the vast realm of systematic reviews, the critical task of abstract screening serves as a foundational\nstep in curating the highest quality of evidence [2]. However, this process often presents signi\u0000cant\nchallenges due to the involvement of sifting through large volumes of literature to identify those that\nalign with prede\u0000ned criteria. Over time, various methodologies, ranging from manual evaluations to\nsophisticated AI-driven techniques, have been proposed to address the complexities of this task. In this\nsection, we \u0000rst describe the existing approaches on their operational mechanisms and associated\nadvantages and disadvantages.\n2.1 Manual Approach\nHistorically, the process of abstract screening was \u0000rmly rooted in manual evaluations. In this\nconventional approach, individual reviewers would scrutinize each abstract against prede\u0000ned criteria\n[13]. The meticulous nature of this method required that multiple experts independently evaluate the\nliterature to ensure both reliability and reduced biases [14]. While the depth of human expertise brought\nabout nuanced understanding, the manual nature of this method made it both time-consuming and, at\ntimes, prone to human error [6].\n2.2 NLP Based Approach\nAs technology evolved, the \u0000eld witnessed the incorporation of Natural Language Processing (NLP) to\nautomate abstract screening [9]. In this framework, abstract text undergoes preprocessing and\nvectorization. Supervised machine learning models, notably the Support Vector Machine (SVM) and the\nRandom Forest (RF), are then trained on this vectorized data to classify literature based on speci\u0000c\ncriteria [15]. The strength of this approach lies in its potential for e\u0000ciency. However, its e\u0000cacy and\naccuracy hinges heavily on the availability of a well-curated, labeled training set.\n2.3 Zero-Shot Classi\u0000cation\nA more recent and innovative approach is zero-shot classi\u0000cation, which was notably highlighted by Yin\net al [16]. Eschewing the need for an extensive labeled training dataset, zero-shot classi\u0000cation offers\nthe allure of categorizing abstracts without prior domain-speci\u0000c training. By calculating a probability\nscore for each abstract, researchers obtain a dynamic measure of its alignment with prede\u0000ned\ncategories. While it does not require training for the machine learning model, the tradeoff is a loss of\nsensitivity and potential omissions of pertinent studies [17]. In this study, for a given set of abstracts, we\nPage 5/26\n\u0000rst obtain embeddings (using Open AI's text embedding API, the babbage model) for abstracts and for a\npre-speci\u0000ed description of an ideal study to include. We then compute the cosine similarity score\nbetween each abstract (its embedding) and the pre-speci\u0000ed description (its embedding). We claim the\ntop 10% (in terms of similarity) of abstracts as positive (i.e., a study should be included).\n2.4 Hybrid Approach\nTo reconcile the strengths and limitations of these models, a hybrid methodology has emerged [18]. It\ncleverly marries the rapid categorization capabilities of zero-shot classi\u0000cation with the precision of\ntraditional machine learning models such as SVM and RF. Here, zero-shot classi\u0000cation provides an\ninitial curation of all abstracts in the training set. Then, based on zero-shot classi\u0000cation’s curation, the\nabstracts with classi\u0000cation score higher than a pre-speci\u0000ed threshold are selected to undergo manual\nreview to rectify zero-shot classi\u0000cation. The recti\u0000ed classi\u0000cation then serves as a foundation for\ntraining traditional models, which are then employed for screening the broader dataset (e.g., in the\ntesting set). The hybrid approach balances the speed of zero-shot and the precision of traditional ML,\nand potentially offers enhanced accuracy at reduced human efforts. However, this approach involves the\nuse of multiple methodologies and still relies on well-curated, labeled training data (in this case, a subset\nof the whole training set). For a given set of abstracts, we curate the \"positive\" abstracts (i.e., the top 10%\nof abstracts) identi\u0000ed by zero-shot, and then \u0000t a classi\u0000cation model for curated labels to the text\nembeddings using SVM. This learned classi\u0000cation model is then used to predict the class of other\nabstracts.\n3. Large Language Models\nAmidst the evolving methodologies described in Section 2, modern AI tools based on Large Language\nModels (LLMs), such as ChatGPT, PaLM, and Llama, are emerging as potential game-changers.\nGrounded in advanced language processing capabilities, these tools can be tailored to evaluate\nabstracts against nuanced criteria and offer detailed assessments and classi\u0000cations. Their prowess\nsignals transformative potential for abstract screening. In this section, we \u0000rst present two examples to\nillustrate the potential of LLMs in the context of abstract screening, and then precede with our in-depth\ndiscussion on the study's work\u0000ow and design that aims to critically investigate the performance of\nLLMs in this domain.\n3.1 Examples of Abstract Screening by Using LLMs\nIn this subsection, we demonstrate the process of LLM-based abstract screening using ChatGPT with\ntwo speci\u0000ed abstracts: (1) Millard et. al. (2020) [19] and (2) Zawia et. al. (1991) [20]. Using other LLM\ntools such as Llama, Google PALM for abstract screening is similar and will be skipped for brevity. \nAbstract (1)\nChatGPT Prompt: \"I would like you to help me with conducting a systematic review on animal\ndepression. I will provide the title and abstract for one journal article and would like you to screen the\nPage 6/26\npaper for inclusion. Here are the inclusion criteria: Does the study provide primary data of an animal\nmodel of depression or depressive-like phenotype with an appropriate control group? Here is the\nabstract: \nWistar-Kyoto (WKY) rats show endogenous depressive behavior that can be reversed by antidepressants.\nGiven that WKYs exhibit decreased sensitivity to some antidepressants and treatment-resistant\ndepressed patients often show hypothalamic-pituitary-thyroid (HPT) dysregulation, we examined the\nbehavioral and HPT hormonal responses of WKYs to altered thyroid status. \"Euthyroid\" WKYs had\nelevated basal plasma TSH and T(3) levels as compared to Wistars. Hypothyroidism increased TSH\nlevels more in WKYs than in Wistars and increased response latency in the open \u0000eld test (OFT) of WKYs\nonly. Administration of T(4) and T(3) suppressed plasma TSH equally in both strains. Wistars responded\nto increased T(3) levels with decreased response latency and increased activity in the OFT, but increased\nimmobility in the forced swim test. In contrast, WKYs responded only to the high T(3) levels with\ndecreased response latency in the OFT. These results suggest the existence of a decreased central\nnervous system sensitivity to thyroid hormones in WKYs that could be related to their depressive\nbehavior.\" \nChatGPT v4 response (accessed on 10/15/2023):\nPage 7/26\nGiven the abstract and the inclusion criteria, this study appears to meet the inclusion criteria for yoursystematic review on animal models of depression.\n \nConsideration for Inclusion: \n1. Animal Model and Depressive-like Phenotype: \nThe study utilizes Wistar-Kyoto (WKY) rats which are mentioned to show endogenous depressivebehavior. \nThis study investigates behaviors and hormonal responses associated with depression in thisparticular animal model, with a focus on the role of thyroid hormones. \n2. Primary Data and Appropriate Control Group: \nThe study provides primary data on behavioral and hormonal responses of WKY rats, comparingthem to Wistars (likely serving as the control group), under altered thyroid status, allowing forcomparisons between the depressive-like phenotype and controls. \nThe utilization of both \"Euthyroid\" WKYs and Wistars as controls helps in effectively investigatingthe behavioral changes and responses in WKY rats due to alterations in thyroid status. \n3. Behavioral Test Indicating Depression: \nDepressive behavior and responses are assessed using standardized tests like the open \u0000eldtest (OFT) and the forced swim test, which are conventional methods to investigate depressive-like behaviors in animal models. \nThe mention of antidepressant’s impact on WKY rats indicates a focus on depressive-likestates. \n4. Relevance to Human Condition: \nInsights gained from this study on decreased central nervous system sensitivity to thyroidhormones and its possible relation to depressive behavior in WKYs can have implications inunderstanding treatment-resistant depression in humans, making it valuable for the review. \nConclusion: \nGiven the provided abstract, this study does seem to \u0000t the inclusion criteria for a systematic reviewon animal depression, as it provides primary data on an animal model demonstrating depressive-likebehavior, includes an appropriate control group, and employs standardized tests to study thedepressive-like states and their relation to thyroid hormones. The detailed exploration of depressivebehaviors and hormonal imbalances in relation to altered thyroid status can offer valuable insightsfor understanding and treating depression.\n \nAbstract (2)\nChatGPT Prompt: “I would like you to help me with conducting a systematic review on animal\ndepression. I will provide the title and abstract for one journal article and would like you to screen the\npaper for inclusion. Here are the inclusion criteria: Does the study provide primary data of an animal\nmodel of depression or depressive-like phenotype with an appropriate control group? Here is the\nabstract: \nPage 8/26\nThe induction of the activity of cerebral ornithine decarboxylase (EC 4.1.1.17) and mRNA by electrical\nstimulation exhibits regional differences. The effects of the enzyme inhibitor di\u0000uoromethylornithine on\nthese regional variations was examined. Administration of this inhibitor resulted in pronounced\ndepression of both basal and induced activity of ornithine decarboxylase in the hippocampus. Basal\nactivity of the enzyme in the neocortex and the cerebellum appeared to be resistant to\ndi\u0000uoromethylornithine but the induced enzyme activity was sensitive to the effects of this inhibitor.\nSusceptibility to di\u0000uoromethylornithine may be directly correlated with a slower turnover rate for\nornithine decarboxylase. These results suggest that ornithine decarboxylase in the hippocampus may\npossess a longer half-life than its counterparts in other regions of the brain. Pretreatment with\ndi\u0000uoromethylornithine had no effect on the induced ornithine decarboxylase mRNA in the neocortex.\nThus, elevated activity of ornithine decarboxylase enzyme, due to electrical stimulation, appears to not\nhave any effect on either the transcription or the decay rate of the induced ornithine decarboxylase\nmRNA. These \u0000ndings support the concept of region-speci\u0000c regulation of cerebral ornithine\ndecarboxylase.”\nChatGPT v4 response (accessed on 10/15/2023):\nPage 9/26\nGiven the provided abstract and the inclusion criteria, this study does not appear to meet the criteriafor inclusion in a systematic review on animal models of depression or depressive-like phenotypeswith an appropriate control group. \nConsideration for Inclusion: \n1. Depression or Depressive-like Phenotype: \nThe abstract does not provide any information or data related to depression or depressive-likebehavior or phenotypes in animals. It is mainly focused on the activity of cerebral ornithinedecarboxylase and its regional differences in response to electrical stimulation and inhibitoradministration. \n2. Animal Model: \nThere is no mention of the use of any speci\u0000c animal model or any behavioral assays to assessdepressive-like symptoms or states in the provided abstract. \n3. Behavioral Assessment and Primary Data: \nThe abstract does not contain any details about behavioral assessments related to depressionor depressive-like states. It primarily provides data on the effects of enzyme inhibitordi\u0000uoromethylornithine on cerebral ornithine decarboxylase activity and mRNA, but does notrelate these \u0000ndings to depression or depressive-like behaviors. \n4. Relevance to Depression: \nWhile the study explores biochemical aspects in brain regions like the hippocampus, which isimplicated in mood disorders like depression, the connection between the \u0000ndings anddepressive symptoms or states is not established in the provided abstract.\nConclusion: \nThis article does not seem to \u0000t the inclusion criteria for a systematic review on animal models ofdepression or depressive-like phenotypes as it lacks primary data on depressive-like behaviors orphenotypes in animals, and there is no mention of the use of animal models representative ofdepression or an appropriate control group for such a comparison.\nFrom the above two examples presented, it is evident that ChatGPT adeptly comprehends the posed\nqueries and e\u0000ciently extracted the inclusion and exclusion criteria. It not only makes clear decisions\nregarding study inclusion but also articulates its rationale by cross-referencing each criterion.\nIt is worth noting that the same ChatGPT prompt might result in slightly different responses between\ndifferent users due to the variability in its response generation, but the conclusion for the eligibility of an\nabstract is typically consistent. In addition, enhancing the speci\u0000city of the criteria within the prompts\ncan further re\u0000ne the screening process. To illustrate, consider the following enhanced prompts:\nEnhanced ChatGPT Prompt for Abstract (1): “I seek assistance with a systematic review focused on\nanimal depression. I'll provide the abstract of a particular journal article and would appreciate an\nassessment for its inclusion based on the following criteria:   \n1. The study employs an animal model re\u0000ecting depression or a depressive-like phenotype. \n2. It incorporates an appropriate control group. \nPage 10/26\n3. Primary data is presented within the study.        \nand here are the exclusion criteria: \n1. The study bypasses the use of an animal model. \n2. The utilized animal model doesn't characterize depression or a depressive-like phenotype. \n3. Absence of a control group within the study. \n4. The study is primarily a review, devoid of primary data. \nHere is the abstract: …”\nChatGPT v4 response (accessed on 10/15/2023):\nLet's assess the provided abstract based on the inclusion and exclusion criteria:\nInclusion Criteria:\n1. Animal Model Re\u0000ecting Depression: The study uses Wistar-Kyoto (WKY) rats, which aredescribed as showing \"endogenous depressive behavior.\" Thus, this criterion is met.\n2. Appropriate Control Group: The study compares WKY rats to Wistars (another strain), whichserves as a comparison or control group. This criterion is met.\n3. Primary Data: The abstract mentions speci\u0000c results such as the elevated basal plasma TSH andT(3) levels in \"Euthyroid\" WKYs compared to Wistars and the different responses in the open\u0000eld test (OFT). This suggests the presence of primary data. Thus, this criterion is met.\nExclusion Criteria:\n1. Bypasses the Use of an Animal Model: The study uses WKY rats as an animal model. Thus, thiscriterion is not met, which is good for inclusion.\n2. Animal Model Doesn't Characterize Depression: The WKY rats show \"endogenous depressivebehavior.\" Thus, this criterion is not met, which again supports inclusion.\n3. Absence of a Control Group: The study does use a control group (Wistars). Thus, this criterion isnot met, supporting inclusion.\n4. Study is Primarily a Review: The abstract presents experimental data and does not indicate thatthe study is a review. Thus, this criterion is not met, which is favorable for inclusion.\nConclusion:\nGiven that the abstract meets all the inclusion criteria and none of the exclusion criteria, it should beconsidered in your systematic review on animal depression.\nWith these revised prompts applied to the previously discussed abstracts, the conclusions remain\nconsistent. Nonetheless, we contend that this enhanced structure not only provides a more rigorous\nframework but also aligns more closely with widely accepted practices in systematic review criteria\nde\u0000nition.\n3.2 Automated Work\u0000ow for Streamlining Abstract Screening via ChatGPT and other Tools\nFor systematic reviews, particularly when dealing with voluminous data, e\u0000cient work\u0000ows are\nparamount. The ChatGPT API (Application Programming Interface) offers a dynamic solution, enabling\nPage 11/26\nthe automation of abstract screening on a large scale, which circumvents the labor-intensive process of\nmanually inputting abstracts into a chatbot interface. In this subsection, we present an automated\nwork\u0000ow for streamlining abstract screening via ChatGPT. Note, though this automated work\u0000ow uses\nChatGPT as the platform, analogous work\u0000ows work for other AI platforms like PALM and Llama.\nAutomated Work\u0000ow:\n1. Data Collection: The preliminary step entails accumulating a list of titles and abstracts. By utilizing\ncarefully crafted keywords, we retrieve these from PubMed and other pertinent databases. This\ncomprehensive approach ensures the potential inclusion of all relevant studies for a subsequent\ndetailed screening. It's worth noting that while this list is expansive, most of these studies may not\n\u0000nd their way into the \u0000nal meta-analysis post-screening.  \n2. Automation through Python: We've devised a python script aimed at harnessing the capabilities of\nChatGPT for evaluating the amassed abstracts.\na. This script interacts with the ChatGPT API (speci\u0000cally, the GPT-4 version) and, when furnished\nwith tailored prompts, extracts structured responses from ChatGPT.  \nb. Typically, the AI's response commences with a succinct summary, delves into explanations\naligned with each criterion, and concludes with a decisive judgment, as exempli\u0000ed by the\nexamples in Section 3.1.  \nc. This automated process e\u0000ciently saves ChatGPT's verdicts on each abstract for ensuing\nanalyses. For instance, it extracts the \u0000nal decisions regarding the inclusion or exclusion of\nstudies and determines the stance on each pre-speci\u0000ed criterion for every abstract, as\nexempli\u0000ed by the last example in Section 3.1.   \nd. Additionally, to ascertain the e\u0000ciency and cost-effectiveness of this methodology, the script\nalso monitors the time, token usage, and the \u0000nancial implications of querying the OpenAI API.\nIn essence, we envision this procedure as delegating the meticulous task of poring over scienti\u0000c\nsummaries to an AI assistant. This virtual entity meticulously sifts through each summary, determining\nits alignment with stipulated criteria. \n3. Tuning Parameters in the ChatGPT API: The e\u0000cacy of the ChatGPT API isn't just dependent on the\ninput data, but also on speci\u0000c parameters that can be adjusted to re\u0000ne the responses. These\nparameters can signi\u0000cantly impact the conclusions drawn by the AI. Tweaking these parameters\ncan be analogized to modifying the 'personality' or 'inclination' of the AI, guiding it to provide varied\nperspectives or depths of analysis. \nBecause \u0000ne-tuning these parameters to achieve optimal performance for abstract screening can be\nvery challenging and is beyond the scope of this study, we primarily used the recommended settings for\nthese parameters by platform (see Table 1) in this study. \nTable 1: Parameters used in our study.\nPage 12/26\nParameters/LLMChatGPT (v4.0/v3.5)Google PaLM Meta Llama 2\nTemperature 0 0.25 0.75\nMaximum length1,000 2,048 500\nStop sequences None None None\nTop P 1 0.95 50\nTop K -- 40 0.9\nFrequency penalty0 -- --\nPresence penalty0 -- --\nA brief explanation of these parameters is as follows: \nTemperature: The temperature controls the randomness of the outputs, with a range from 0 to 2. A\ntemperature value greater than 1 is random and 0 is deterministic. The maximum temperature of 2\ngives the most creative and variable outputs. \nMax length: The max length is the maximum number of tokens for the model to generate as a\nresponse. A single word is generally 2-3 tokens. \nStop sequences: This parameter controls which tokens or phrases will stop the LLM from generating\nmore text.\nTop P: When generating text, the model samples from the top p percentage of most likely tokens to\ngenerate. The top p is the cumulative probability cutoff for the model’s selection of tokens to\ngenerate. Lower top p values mean sampling from a smaller, more top-weighted nucleus. \nTop K: When generating text, the model samples from the top k most likely tokens. When deciding\nthe next word to output, the model will select the most likely word when top k is lower. \nFrequency penalty: This frequency penalty parameter controls how the LLM penalizes tokens that\nhave already appeared in the input and output text. A frequency penalty of 0 implies that the\nfrequency of the tokens does not impact the LLM and will generate them based on their probability.\nPresence penalty: This parameter controls how the LLM penalizes tokens that have not been used\nrecently. A presence penalty of 0 means that the LLM does not care about the recency of the tokens\nand will generate them based on their probability. \n3.3 Study Design: \nIn our pursuit to assess ChatGPT's pro\u0000ciency in abstract screening, we selected certain benchmark\ndatabases that have existing performance data from other methodologies. This selection aids in a\ncomparative analysis of performance.  \n3.3.1 Databases: \nPage 13/26\nWe picked the following 3 databases from the publicly available Synergy dataset [21]: \n1. Bannach-Brown 2016 [22] - Topic: Use of Animal Models to Study Depressive Behavior. \nHuman-Curated (Gold Standard) Results: 1258 excluded abstracts and 230 included abstracts. \nWe randomly selected 100 excluded abstracts and 100 included abstracts for screening by LLM\ntools. \n2. Meijboom 2021 [23] - Topic: Retransitioning of Etanercept in Patients with a Rheumatic Disease \nHuman-Curated (Gold Standard) Results: 599 excluded abstracts and all 32 included abstracts. \nWe randomly selected 100 excluded abstracts and 32 included abstracts for screening by LLM\ntools.\n3. Menon 2022 [24] - Topic: Methodological Rigor of Systematic Reviews in Environmental Health\nHuman-Curated (Gold Standard) Results: 896 excluded abstracts and 73 included abstracts. \nWe randomly selected 100 excluded abstracts and all 73 included abstracts for screening by LLM\ntools. \nFor each chosen database, abstracts were categorized as either 'cases' (those included based on a gold\nstandard) or 'controls' (those excluded per the gold standard). From each category, we randomly\nselected 100 abstracts (we use all abstracts if there are less than 100 abstracts in that category in a\ndatabase). These abstracts underwent evaluation by ChatGPT (v4.0) as per our established work\u0000ow.\nSubsequently, ChatGPT's decisions were juxtaposed against the gold standard to determine sensitivity,\nspeci\u0000city, and overall accuracy. The same abstracts were also processed using ChatGPT (v3.5), Google\nPaLM, and Meta Llama to record their respective verdicts.  \n3.3.2 Statistical Analysis: \nTo quantify the e\u0000cacy of ChatGPT and other AI tools for each database, we calculated the following\nmetrics: (1) Sensitivity, (2) Speci\u0000city, (3) Overall Accuracy, where sensitivity is de\u0000ned as the number of\ntrue positives divided by the sum of true positives and false negatives, speci\u0000city as the number of true\nnegatives divided by the sum of true negatives and false positives, and accuracy as sum of true positives\nand true negatives divided by the total number of abstracts. For each metric, associated 95% con\u0000dence\nintervals were also determined. Although it is very common in the \u0000eld to report F1 score, recall rate, and\nprecision, we believe it is more appropriate to report sensitivity and speci\u0000city given this study design. In\naddition, F1 score, recall rate and precision can be derived from sensitivity and speci\u0000city.  \nFurthermore, to explore the potential of a uni\u0000ed decision-making process, we combined the decisions\nfrom all AI tools using a voting mechanism. The majority decision across the tools was taken as the \u0000nal\nverdict for each abstract. For this consolidated approach, we again computed sensitivity, speci\u0000city,\noverall accuracy, and the associated 95% CIs for each database. We also explore the use of latent class\nPage 14/26\nanalysis (LCA), a model-based clustering approach, to derive consolidated decisions. More details on\nthis LCA approach are provided in Section 5.  \nFor a given database, 100 cases and 100 controls yield a two-sided 95% con\u0000dence interval with a half-\nwidth equal to 0.048 when the underline sensitivity (speci\u0000city) is approximately 95%.  \nAll statistical analyses were conducted using the R statistical software (version 4.3.1). All tests were\ntwo-sided with an alpha level set at 0.05 unless otherwise mentioned. \n4. Results\nWe present the results for each of the 3 databases. For each, we \u0000rst present the prompts we used when\nwe called LLM tools to screen an abstract, then present the performance data (accuracy, sensitivity and\nspeci\u0000city for each method or LLM tool), followed by a summary of the performance, and a comparison\nof the performance of the different methods to that based on ChatGPT v4.0. \n4.1 Results on the Bannach-Brown 2016 database (See Table 2):  \nThe prompts we used for screening abstracts in this database is as follows: \nConduct a systematic review on animal depression. I provide the title and abstract for one journal article.\nProvide an overall assessment based on eligibility criteria with only one word answer yes or no with no\nexplanation. Then, for each inclusion or exclusion criterion, answer with only one word, yes if it is\nincluded by the inclusion criterion or excluded by the exclusion criterion, and answer no if it does not\nmeet the inclusion criterion or not excluded by the exclusion criterion. After answering all the criteria\nwith yes or no, then provide an overall explanation.  \nHere is the eligibility criteria: Inclusion Criteria: 1. Any article providing primary data of an animal model\nof depression or depressive-like phenotype with an appropriate control group (speci\u0000ed above). 2.\nAnimals of all ages, sexes and species, where depression-like phenotype intended to mimic the human\ncondition have been induced. Including animal models where depressive-like phenotypes are induced in\nthe presence of a comorbidity (e.g. obesity or cancer). 3. All studies that claim to model depression or\ndepressive-like phenotypes in animals. Studies that induce depressive behavior or model depression and\nthat also test a treatment or intervention (prior or subsequent to model induction), with no exclusion\ncriteria based on dosage, timing or frequency. 4. Studies measuring behavioral, anatomical and\nstructural, electrophysiological, histological and/or neurochemical outcomes and where genomic,\nproteomic or metabolomic outcomes are measured in addition to behavioral, anatomical,\nelectrophysiological, histological or neurochemical outcomes. Exclusion Criteria: 1. Review article,\neditorials, case reports, letters or comments, conference or seminar abstracts, studies providing primary\ndata but not appropriate control group. 2. Human studies and ex vivo, in vitro or in silico studies. Studies\nwill be excluded if authors state an intention to induce or investigate only anxiety or anxious behavior.\nStudies will be excluded if there is no experimental intervention on the animals (e.g. purely observational\nPage 15/26\nstudies). 3. Studies that investigate treatments or interventions, but no depressive behavior or model of\ndepression is induced (e.g. toxicity and side-effect studies). 4. Where metabolic outcome measures are\nthe primary outcome measure of a study. Where genomic, proteomic, metabolic or metabolomic\noutcomes are the sole outcome measures in a study, they will be excluded. \nHere is the abstract:  \nAbstract X  \nTable 2: Results on the Bannach-Brown 2016 database\n  Accuracy (95% CI) Sensitivity (95% CI) Speci\u0000city (95% CI)\nZero-Shot Method*  0.839 [0.820, 0.858]0.304 [0.246, 0.368]0.937 [0.922, 0.950]\nHybrid Method*  0.961 [0.950, 0.970]0.843 [0.790, 0.888]0.982 [0.974, 0.989]\nChatGPT (v4.0) 0.945 [0.904, 0.972]0.930 [0.861, 0.971]0.960 [0.901, 0.989]\nChatGPT (v3.5) 0.905 [0.856, 0.942]0.940 [0.874, 0.978]0.870 [0.788, 0.929]\nGoogle PaLM 0.900 [0.850, 0.938]0.850 [0.765, 0.914]0.950 [0.887, 0.984]\nMeta Llama 0.780 [0.716, 0.835]0.950 [0.887, 0.984]0.610 [0.507, 0.706]\nMajority voting 0.915 [0.867, 0.950]0.960 [0.901, 0.989]0.870 [0.788, 0.929]\nLCA-model 0.945 [0.904, 0.972]0.930 [0.861, 0.971]0.960 [0.901, 0.989]\nNote: 1. Google PaLM might generate empty responses to some abstracts, and we assumed all null\noutputs as missing.  “Majority voting” refers to majority voting without using decisions from Google\nPaLM. 2. Zero-shot is based on Open AI’s babbage embedding, and hybrid then trained the model based\non curated label for the top 10% cases identi\u0000ed by zero-shot. 3. Performance summaries for zero-shot\nand hybrid method are based on all 230 positive abstracts and all 1258 negative abstracts. 4. The LCA-\nmodel part, which can be seen as a more sophisticated version of “majority voting”, is explained in\nSection 5. These notes also apply to Tables 3 and 4. \nAmong all the LLM tools we tested, ChatGPT v4.0 stood out with the highest accuracy (0.945) and\nspeci\u0000city (0.960), and satisfactory sensitivity (0.930). Combined decision using major voting seemed to\nimprove sensitivity (0.970) considerably but did not improve speci\u0000city (0.870) much.  Comparatively, the\nzero-shot method was less effective across these metrics, while the hybrid method attained superior\naccuracy (0.961) and speci\u0000city (0.982), albeit with reduced sensitivity (0.843). We acknowledge that our\n\u0000ndings regarding the zero-shot and hybrid methods differ from those reported in [18] for the same set\nof abstracts. This discrepancy could be attributed to the use of distinct sets of embeddings in our\nanalysis compared to the previous study. \nComparison between LLM-tools. We compared the performance (sensitivity and speci\u0000city) between\nChatGPTv4.0 and other LLM tools using the McNemar test and found that ChatGPTv4.0 performed\nPage 16/26\nsigni\u0000cantly better (p-value = 0.002) than Google PALM in terms of sensitivity; ChatGPTv4.0 performed\nsigni\u0000cantly better than ChatGPTv3.5 (p-value = 0.008) and better than Llama (p-value < 0.001) in terms\nof speci\u0000city. Combining the decisions of different LLM tools using majority voting did not improve the\noverall accuracy compared to ChatGPTv4.0. Speci\u0000cally, there was no statistically signi\u0000cant difference\n(p-value = 0.134) in sensitivity between the combined decision (majority voting) and ChatGPTv4.0, and\nthe combined decision was signi\u0000cantly worse (p-value = 0.008) than ChatGPTv4.0 in terms of\nspeci\u0000city. \nComparison between ChatGPT v4.0 and Zero-shot and Hybrid Method: We assessed the performance\n(sensitivity and speci\u0000city) of ChatGPT v4.0 against both the zero-shot and hybrid approaches using the\nMcNemar test. Speci\u0000cally, we aligned the screening results from 100 cases and 100 controls as per the\nChatGPT v4.0 method and similarly for the zero-shot and hybrid methods, testing for inconsistencies\nbetween these approaches as previously done. Our analysis revealed that ChatGPT v4.0 signi\u0000cantly\noutperformed the zero-shot method in sensitivity (p-value < 0.001) but showed comparable effectiveness\nin speci\u0000city (p-value = 0.37). Additionally, ChatGPT v4.0 demonstrated marginally superior sensitivity\ncompared to the hybrid method (p-value = 0.07), while its performance in speci\u0000city was similar (p-value\n= 1.00). \n4.2 Results on the Meijboom 2021 database (see Table 3)\nThe prompts we used for screening abstracts in this database is as follows: \nConduct a systematic review on transitioning patients from an originator to a corresponding biosimilar.  \nI provide the title and abstract for one journal article.  Provide an overall assessment based on eligibility\ncriteria with only one word answer yes or no, with no explanation.  Then, for each inclusion or exclusion\ncriterion, answer with only one word, yes if it is included by the inclusion criterion or excluded by the\nexclusion criterion, and answer no if it does not meet the inclusion criterion or not excluded by the\nexclusion criterion.  \nAfter answering all the criteria with yes or no, then provide an overall explanation.  \nHere is the eligibility criteria: Here is the eligibility criteria: \nArticles were included if they met the following criteria:  \n1. Study involved transitioning from a TNF α  inhibitor (including etanercept, in\u0000iximab, and adalimumab)\noriginator to a biosimilar \n2. The number of patients who retransitioned was reported or could be calculated \n3. The article was an original research article published in a peer-reviewed journal \n4. The article included baseline characteristics of the patients who transitioned \nPage 17/26\n5. The article was written in English \n6. The full-text version of the article could be obtained.  \nTransitioning was de\u0000ned as patients in whom the biosimilar was introduced after the originator, without\ntreatment with other drugs in between. Retransitioning was de\u0000ned as restarting the originator directly\nafter discontinuing a biosimilar, without treatment with other drugs in between. In summary, transitioning\nwas de\u0000ned as switching from the originator to a biosimilar; retransitioning was de\u0000ned as switching\nfrom the originator to a biosimilar and back to the originator. Both transitioning and retransitioning\ninvolved changes with the same active biological substance. \nHere is the abstract: \nAbstract X \nTable 3: Results on the Meijboom 2021 database\n  Accuracy (95% CI) Sensitivity (recall) (95% CI) Speci\u0000city (95% CI)\nZero-Shot Method  0.862 [0.833, 0.888]0.125 [0.035, 0.290] 0.902 [0.875, 0.924]\nHybrid Method  0.954 [0.935, 0.969]0.065 [0.008, 0.214] 1.000 [0.994, 1.000]\nChatGPT (v4.0) 0.848 [0.776, 0.905]0.812 [0.636, 0.928] 0.860 [0.776, 0.921]\nChatGPT (v3.5) 0.591 [0.502, 0.676]0.969 [0.838, 0.999] 0.470 [0.369, 0.572]\nGoogle PALM 0.890 [0.802, 0.949]0.647 [0.383, 0.858] 0.954 [0.871, 0.990]\nMeta Llama 0.636 [0.548, 0.718]1.000 [0.891, 1.000] 0.520 [0.418, 0.621]\nMajority voting  0.720 [0.635, 0.794]1.000 [0.891, 1.000] 0.630 [0.528, 0.724]\nLCA-model 0.841 [0.778, 0.904]1.000 [0.891, 1.000] 0.790 [0.710, 0.870]\nAmong all the LLM tools we tested, ChatGPT v4.0 stood out with the highest accuracy (0.840), but not\nwith speci\u0000city (0.860) or satisfactory sensitivity (0.812). Compared to ChatGPTv4.0, combined decision\nusing major voting did not improve overall accuracy (0.720), but improved sensitivity (1.000) at the\nsacri\u0000ce of speci\u0000city (0.630). \nComparison between LLM-tools. We compared the performance (sensitivity and speci\u0000city) between\nChatGPTv4.0 and other LLM tools using the McNemar test and found that ChatGPTv4.0 performed\nsigni\u0000cantly better (p-value < 0.001) than Google PALM, but signi\u0000cantly worse than ChatGPT3.5 (p-value\n= 0.001) and Llama in terms of sensitivity; ChatGPTv4.0 performed signi\u0000cantly better than  \nChatGPTv3.5 (p-value < 0.001), and better than Llama (p-value < 0.001), but worse than Google PALM (p-\nvalue = 0.002), in terms of speci\u0000city. Combining the decisions of different LLM tools using majority\nvoting did not improve the overall accuracy compared to ChatGPTv4.0. Speci\u0000cally, there was\nstatistically signi\u0000cant difference (p-value = 0.008) in sensitivity between the combined decision\nPage 18/26\n(majority voting) and ChatGPTv4.0, and the combined decision was not signi\u0000cantly worse (p-value >\n0.50) than ChatGPTv4.0 in terms of speci\u0000city.\nComparison between ChatGPT v4.0 and Zero-shot and Hybrid Method: We evaluated the performance of\nChatGPT v4.0, focusing on sensitivity and speci\u0000city, in comparison with the zero-shot and hybrid\napproaches, employing the McNemar test as described above. In this analysis, we aligned the screening\nresults from 32 cases and 100 controls for the tests. Our \u0000ndings indicated that ChatGPT v4.0\nsigni\u0000cantly surpassed the zero-shot method in sensitivity (p-value = 0.0002) and exhibited marginally\nimproved speci\u0000city (p-value = 0.099). Furthermore, ChatGPT v4.0 showed notably higher sensitivity than\nthe hybrid method (p-value < 0.001), although its speci\u0000city was comparatively lower. \n4.3 Results on the Menon 2022 database (see Table 4)\nThe prompts we used for screening abstracts in this database is as follows: \n“Conduct a systematic review on the methodological rigour of systematic reviews in environmental\nhealth.  \nI provide the title and abstract for one journal article.  \nProvide an overall assessment based on eligibility criteria with only one word answer yes or no, with no\nexplanation.  \nThen, for each inclusion or exclusion criterion, answer with only one word, yes if it is included by the\ninclusion criterion or excluded by the exclusion criterion and answer no if it does not meet the inclusion\ncriterion or not excluded by the exclusion criterion.  \nAfter answering all the criteria with yes or no, then provide an overall explanation.   \nHere are the eligibility criteria: \nTo be eligible for inclusion in the SR sample, documents had to ful\u0000ll the following criteria:   \n1. Identify explicitly as a “systematic review” in their title \n2. Assess the effect of a non-acute, non-communicable, environmental exposure on a health outcome.\nEnvironmental exposures can include air and water pollutants, radiation, noise, occupational hazards,\nlifestyle factors (like diet or physical activity) and lifestyle choices in\u0000uenced by family and peers (like\nsubstance use), social and economic factors (like stress from work or living conditions)   \n3. Include studies in people or mammalian models \n4. Be available in HTML format \nHere is the abstract: \nPage 19/26\nAbstract X”\nTable 4: Results on the Menon 2022 database\n  Accuracy (95% CI) Sensitivity (95% CI) Speci\u0000city (95% CI)\nZero-Shot Method  0.882 [0.860, 0.902]0.384 [0.272, 0.505]0.923 [0.904, 0.940]\nHybrid Method  0.951 [0.936, 0.964]0.708 [0.589, 0.810]0.971 [0.958, 0.981]\nChatGPT (v4.0) 0.913 [0.861, 0.951]0.932 [0.847, 0.977]0.900 [0.824, 0.951]\nChatGPT (v3.5) 0.711 [0.637, 0.777]0.315 [0.211, 0.434]1.000 [0.964, 1.000]\nGoogle PALM 0.569 [0.486, 0.648]0.000 [0.000, 0.054]1.000 [0.958, 1.000]\nMeta Llama 0.827 [0.762, 0.880]0.808 [0.699, 0.891]0.840 [0.753, 0.906]\nMajority voting 0.884 [0.827, 0.928]0.808 [0.699, 0.891]0.940 [0.874, 0.978]\nLCA-Model 0.879 [0.830, 0.928]0.945 [0.893, 0.997]0.830 [0.756, 0.904]\nAmong all the LLM tools we tested, ChatGPT v4.0 stood out with the highest accuracy (0.913) and\nspeci\u0000city (0.932), but not with speci\u0000city (0.900). Compared to ChatGPTv4.0, combined decision using\nmajor voting did not improve overall accuracy (0.884) or sensitivity (0.808), but improved speci\u0000city\n(0.940). \nComparison between LLM-tools. We compared the performance (sensitivity and speci\u0000city) between\nChatGPTv4.0 and other LLM tools using the McNemar test and found that ChatGPTv4.0 performed\nsigni\u0000cantly better than ChatGPT3.5 (p-value < 0.001), Google PALM, and Llama (p-value = 0.02) in terms\nof sensitivity; ChatGPTv4.0 performed worse than ChatGPTv3.5, and Google PALM, in terms of\nspeci\u0000city. Combining the decisions of different LLM tools using majority voting did not improve the\noverall accuracy compared to ChatGPTv4.0. Speci\u0000cally, there was statistically signi\u0000cant difference (p-\nvalue = 0.008) in sensitivity between the combined decision (majority voting) and ChatGPTv4.0, and the\ncombined decision was not signi\u0000cantly different (p-value =0.134) than ChatGPTv4.0 in terms of\nspeci\u0000city.\nComparison between ChatGPT v4.0 and Zero-shot and Hybrid Method: We aligned the screening results\nfrom 73 cases and 100 controls based on the ChatGPT v4.0 method, and similarly for the zero-shot and\nhybrid methods, to test for inconsistencies between these approaches, using the McNemar test as done\nin previous assessments. Our analysis showed that ChatGPT v4.0 signi\u0000cantly outperformed the zero-\nshot method in both sensitivity (p-value < 0.001) and speci\u0000city (p-value = 0.016). In comparison with the\nhybrid method, ChatGPT v4.0 also demonstrated superior sensitivity (p-value < 0.001) and better\nspeci\u0000city (p-value = 0.04). \n4.4 Monetary Cost and Time Cost \nPage 20/26\nTo use the ChatGPT API or other LLM tools, the owners of these platforms charge a predetermined rate\nfor access to the corresponding APIs. These fees are calculated in USD per thousand tokens, where\ntokens are the basic units used by these LLM platforms to quantify text length. In this context, a token\ncan represent a word, a punctuation mark, or a character. The \u0000nancial cost of screening 200 abstracts\nwas approximately $6 for ChatGPT v4.0, $0.2 for ChatGPT v3.5, $10 for Llama (using Replicate), while\nGoogle PaLM offered its services for free to invited developers. Thus, the cumulative cost of evaluating\n200 abstracts across all platforms was approximately $16.2. In terms of time e\u0000ciency, processing 200\nabstracts with each of these LLM tools took approximately 10-20 minutes using a single thread.\nHowever, it is imperative to recognize that abstract screening lends itself well to parallelization.\nConsequently, one could signi\u0000cantly speed up the process by setting up multiple threads to\nsimultaneously screen different subsets of abstracts, thereby reducing the overall time required for\ncompletion. This parallel approach not only increases e\u0000ciency, but also ensures that large amounts of\ndata can be processed in a timely manner, making LLM tools even more attractive for large-scale\nabstract screening tasks. In summary, the monetary and time costs of using LLM tools for abstract\nscreening are negligible compared to manual labeling. \n5. Beyond Majority Voting\nWe have expanded our analysis to include a variety of approaches for synthesizing decisions across\ndifferent LLM tools. Our methodology is inspired by the concept of combining multiple diagnostic tests\nin the absence of a gold standard, akin to situations where human expert consensus is not available.\nThere are several publications discussing such scenarios [24–26], among which we proposed to use the\nlatent class analysis (LCA) models.\nLatent class analysis (LCA) is a statistical method used to identify subgroups within a population, which\nare not directly observed (hence 'latent') [27–29]. It is particularly useful when the research interest lies\nin categorizing individuals into mutually exclusive groups based on their responses to multiple observed\nvariables. In the context of abstract screening, LCA can offer a sophisticated means of integrating\ndecisions from different LLM tools without relying on a gold standard, typically provided by human\nexpert consensus. This approach assumes that the unobserved subgroups (or 'latent classes') explain\nthe dependence between the observed decisions made by each of the LLM tools.\nUtilizing the LCA model, we treat the decisions from all LLM tools as dichotomous variables,\ncorresponding to the adherence to each inclusion or exclusion criterion, as well as the overall decision.\nFor instance, within the Bannach-Brown 2016 database (BB2016), there are eight criteria in total with\nfour criteria each for inclusion and exclusion and one overall decision for inclusion, resulting in a total of\nnine binary items per LLM tool. In our analysis, we incorporated decisions from GPT v4.0, v3.5, and\nLlama 2. Decisions from Google PaLM were excluded due to a high frequency (10% or more) of\nincomplete responses. Consequently, for the Bannach-Brown 2016 database, we worked with 27 binary\nitems. For other databases such as Meijboom 2021(Mj2021) and Menon 2022 (Me2022), the binary\nPage 21/26\nitems totaled 21 and 15, respectively. It is important to note that LCA models were \u0000tted to the binary\ndata of each database independently.\nThe LCA model \u0000tting process enables us to calculate the posterior probabilities of each abstract\nbelonging to speci\u0000c latent classes or sub-groups. Abstracts are then categorized based on these\nprobabilities, with assignment to the class for which an abstract has the highest posterior membership\nprobability. The determination of the number of latent classes is a critical step in the LCA model \u0000tting,\nwhich requires a priori speci\u0000cation. In our evaluation, we explored models with class numbers ranging\nfrom 2 to 6 and utilized the Bayesian Information Criterion (BIC) to identify the most 'optimal' LCA model\nfor our datasets.\nTable 5\nNegative BIC values for LCA models withdifferent number of classes\n#Class BB2016 Mj2021 Me2022\n2 3287.7 1441.0 2017.7\n3 3036.2 1433.6 2012.6\n4 3091.9 1396.5 2060.2\n5 3128.7 1440.0 2112.5\n6 3194.9 1531.6 2129.0\nTable 5 shows that after applying the Bayesian Information Criterion (BIC) to determine the most\nappropriate model for our data, we identi\u0000ed a 3-class model as the best \u0000t for the binary decisions\nderived from the BB2016 database. Similarly, a 4-class model was optimal for the Mj2021 database,\nwhile a 3-class model was again best for the Me2022 database. The confusion matrices generated by\nthe selected LCA models for each database provided a clear juxtaposition between the LLM-assigned\nclasses and the actual labels of the abstracts (see Table 6).\nTable 6\nA crosstab of class assignments based on selected LCA model.\nClass LabelBB2016 Mj2021 Me2022\n  Excluded Included Excluded Included Excluded Included\n1 53 3 21 32 83 4\n2 4 93 42 0 4 54\n3 43 4 8 0 13 15\n4 - - 29 0 - -\nPage 22/26\nThe performance metrics derived from these models are noteworthy. For the BB2016 database, should\nwe categorize abstracts within class with label 2 as “included” and assign the others in classes with\nlabels 1 or 3 to the “excluded” category, the LCA model achieved a sensitivity rate of 93% and a\nspeci\u0000city rate of 96%, indicating a high degree of accuracy in classifying relevant and irrelevant\nabstracts. In the case of the Mj2021 database, if we view class 1 as “included” group and classes 2 and\n3 as “excluded” group, the model achieved a perfect sensitivity rate of 100%, meaning that it correctly\nidenti\u0000ed all relevant abstracts, although the speci\u0000city was lower at 79%, indicating a higher rate of\nfalse positives. Similarly, for the Me2022 database, considering class 1 to be the “excluded” category\nand classes 2 and 3 to be the “included” group, the model showed a sensitivity of 94.5% and a speci\u0000city\nof 83%, a good balance between identifying relevant abstracts and avoiding false inclusions.\nThese results highlight the robustness of the latent class analysis approach to the abstract screening\ntask, providing an effective method for classifying abstracts when a gold standard is not available. The\nvarying levels of sensitivity and speci\u0000city across databases underscore the need to tailor the LCA model\nto the speci\u0000c characteristics of each dataset as well as further future research.\n6. Discussion\nThis study began with a rigorous exploration of the capabilities of Large Language Models (LLMs) in\nabstract screening. We used automation scripts developed in Python to interact with the APIs of several\nLLM tools, including Chat GPT v4.0, Google PaLM 2, and Meta Llama 2. Our central goal was to evaluate\nthe e\u0000ciency and accuracy of these tools across three different databases of abstracts, leading us to a\ncomplex understanding of their potential in this context.\nLarge language models (LLMs), particularly ChatGPT, have garnered global attention since their\ninception. Employing LLMs for abstract screening in systematic reviews is an innovative concept [32, 33]\nand remains underexplored. This study presents the \u0000rst comprehensive evaluation of LLMs applied to\nsystematic review processes. The \u0000ndings are encouraging, suggesting that LLMs could revolutionize\nabstract screening. Speci\u0000cally, ChatGPT v4.0 exhibited stellar performance across three test scenarios,\nachieving an accuracy of at least 85%. Furthermore, it attained sensitivity and speci\u0000city rates ranging\nfrom 80% to an impressive 95%. These exceptional outcomes highlight the substantial promise of LLMs\nin abstract screening, offering an e\u0000cient and capable alternative to the conventional, laborious\napproaches that typically necessitate extensive human annotation.\nHowever, it is important to acknowledge that we are still in the early stages of integrating LLM tools into\nthe abstract screening \u0000eld, and they are not without their imperfections - for example, occasionally\nexcluding many (20%) relevant studies, even with the best-performed LLM tools, ChatGPT v4.0. These\ntools are not a universal solution to all the challenges associated with abstract screening, and they are\nnot ready to completely replace human expertise in this area. Instead, they should be embraced as\ninvaluable assistants in the abstract screening process.\nPage 23/26\nOur research suggests that LLM tools are ready to take on a role in abstract screening and are poised to\nhave an immediate and positive impact on the process. Their integration into abstract screening can\nmanifest itself in a few ways. They can serve as autonomous AI reviewers, adding an extra layer of\nscrutiny and ensuring thoroughness. Our \u0000ndings suggest that a collective decision, such as one derived\nfrom majority voting, can sometimes improve sensitivity, underscoring the potential of LLM tools as a\nreliable safeguard against oversight, ensuring both comprehensiveness and accuracy.\nIn addition, LLM tools can facilitate a synergistic partnership with human experts. They are adept at\nidentifying \"high-risk\" abstracts where different LLM tools have reached different judgments, \u0000agging\nthem for further human evaluation, and promoting a careful and error-free screening process while\nminimizing human effort.\nAnother exciting prospect is the integration of LLM tools with hybrid approaches. In this scenario, LLM\ntools could autonomously annotate abstracts in the training set, minimizing the need for human labeling.\nSubsequently, these labeled abstracts could be used to train custom NLP models, paving the way for a\nstreamlined and e\u0000cient abstract screening process, resulting in signi\u0000cant time and resource savings.\nFurther research is needed to understand how the \"uncertainty\" in LLM-based decisions, when human-\ncurated labels are not used, affects the performance of the hybrid approach. We also note from the test\nexamples that the performance of hybrid approaches varies widely from example to example and\ndepending on the text embedding tools used. Extensive research is needed to perfect the hybrid\napproaches.\nAnother future research direction is to explore how to \u0000ne-tune different versions of LLM tools and how\nto derive collective decisions. One idea is that by using different tuning parameters, one could propose\ndifferent versions of LLM tools, and if they perform similarly in terms of accuracy but give different\ndecisions (e.g., not highly correlated), we expect that better collective decisions would be obtained, as\nobserved in many ensemble learning experiences [31]. However, this could be costly and require a lot of\nexploration.\n7. Conclusion\nIn summary, while LLM tools may not be able to fully replace human experts in abstract screening, their\nability to transform the screening process is both undeniable and imminent. With continued advances in\ntechnology and ongoing re\u0000nement, these tools will play a critical role in the future of abstract screening,\nushering in a new era of e\u0000ciency and effectiveness.\nDeclarations\nEthics Approval and Consent to Participate\nNot applicable.  \nPage 24/26\nAvailability of Data and Materials\nThe datasets analyzed during the current study are available in the SYNERGY repository,\nhttps://github.com/asreview/synergy-dataset.  \nCompeting Interests\nThe authors declare that they have no competing interests. \nFunding\nNot applicable. \nAuthors’ Contributions\nML and XT analyzed the datasets. ML, JS, and XT wrote the manuscript. ML and XT devised the study. All\nauthors edited and approved the \u0000nal manuscript. \nAcknowledgements\nNot applicable.\nReferences\n1. Higgins JPT, Thomas J, Chandler J, et al. Cochrane Handbook for Systematic Reviews of\nInterventions. John Wiley & Sons; 2019.\n2. Moher D, Liberati A, Tetzlaff J, Altman DG, PRISMA Group. Preferred reporting items for systematic\nreviews and meta-analyses: the PRISMA statement. PLoS Med. 2009;6(7):e1000097.\n3. Sackett DL, Rosenberg WM, Gray JA, Haynes RB, Richardson WS. Evidence-based medicine: what it\nis and what it isn't. BMJ. 1996;312(7023):71–2.\n4. Lavis JN, Posada FB, Haines A, Osei E. Use of research to inform public policymaking. Lancet 2004\nNov 6–12;364(9445):1615–21.\n5. Bastian H, Glasziou P, Chalmers I. Seventy-\u0000ve trials and eleven systematic reviews a day: how will\nwe ever keep up? PLoS Med. 2010;7(9):e1000326.\n\u0000. Polanin JR, Pigott TD, Espelage DL, Grotperter JK. Best practice guidelines for abstract screening\nlarge-evidence systematic reviews and meta-analyses. Res Synth Methods. 2019;10(3):330–42.\n7. Turner EH, Matthews AM, Linardatos E, Tell RA, Rosenthal R. Selective publication of antidepressant\ntrials and its in\u0000uence on apparent e\u0000cacy. N Engl J Med. 2008;358(3):252–60.\n\u0000. Edwards P, Clarke M, DiGuiseppi C, Pratap S, Roberts I, Wentz R. Identi\u0000cation of randomized\ncontrolled trials in systematic reviews: accuracy and reliability of screening records. Stat Med.\n2002;21(11):1635–40.\nPage 25/26\n9. O’Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S. Using text mining for study\nidenti\u0000cation in systematic reviews: a systematic review of current approaches. Syst Rev. 2015;4:5.\n10. OpenAI. Introducing ChatGPT. OpenAI Blog. November 2023. Available from:\nhttps://openai.com/blog/chatgpt. Accessed on November 8, 2023.\n11. Google DeepMind. Introducing PaLM 2. Google Blog. May 2023. Available from:\nhttps://blog.google/technology/ai/google-palm-2-ai-large-language-model/. Accessed on November\n8, 2023.\n12. Meta AIR. Meta and Microsoft Introduce the Next Generation of Llama. Facebook AI Blog. July 2023.\nAvailable from: https://ai.meta.com/blog/llama-2/. Accessed on November 8, 2023.\n13. Higgins JPT, Green S, editors. Cochrane Handbook for Systematic Reviews of Interventions. The\nCochrane Collaboration; 2011.\n14. Smith V, et al. Inter-rater agreement in the selection of studies for systematic reviews was improved\nby blinding and training. J Clin Epidemiol. 2015;68(10):1189–97.\n15. Wallace BC et al. Deploying an interactive machine learning system in an evidence-based practice\ncenter: Abstrackr. Proc of the ACM IHI. 2012:819–24.\n1\u0000. Yin W, Hay J, Roth D. Benchmarking Zero-shot Text Classi\u0000cation: Datasets, Evaluation and\nEntailment Approach. Proc of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP). 2019:3905–3914.\n17. Xian Y, Lampert CH, Schiele B, Akata Z. Zero-Shot Learning-A Comprehensive Evaluation of the\nGood, the Bad and the Ugly. IEEE Trans Pattern Anal Mach Intell. 2019;41(9):2251–2265.\n10.1109/TPAMI.2018.2857768. PMID: 30028691.\n1\u0000. Moreno-Garcia CF, Jayne C, Elyan E, Aceves-Martins M. A novel application of machine learning and\nzero-shot classi\u0000cation methods for automated abstract screening in systematic reviews. Decis\nAnalytics J. 2023;6:100162. 10.1016/j.dajour.2023.100162.\n19. Millard SJ, Weston-Green K, Newell KA. The Wistar-Kyoto rat model of endogenous depression: A\ntool for exploring treatment resistance with an urgent need to focus on sex differences. Prog\nNeuropsychopharmacol Biol Psychiatry. 2020;101:109908. 10.1016/j.pnpbp.2020.109908.\n20. Zawia NH, Mattia CJ, Bondy SC. Differential effects of di\u0000uoromethylornithine on basal and induced\nactivity of cerebral ornithine decarboxylase and mRNA. Neuropharmacology. 1991;30(4):337–43.\n10.1016/0028-3908(91)90058-j.\n21. De Bruin J, Ma Y, Ferdinands G, Teijema J, Van de Schoot R. SYNERGY - Open machine learning\ndataset on study selection in systematic reviews. DataverseNL. 2023. 10.34894/HE6NAQ.\n22. Bannach-Brown A, Liao J, Wegener G, Macleod MR. Understanding in vivo modelling of depression\nin non-human animals: a systematic review protocol. Evid Based Preclinical Med. 2016;3(2):20–7.\n23. Meijboom RW, Gardarsdottir H, Becker ML, Ten Wolde S, Egberts TCG, Giezen TJ. Incidence of and\nReasons and Determinants Associated with Retransitioning from Biosimilar Etanercept to Originator\nPage 26/26\nEtanercept. BioDrugs. 2021;35(6):765–72. 10.1007/s40259-021-00501-x. Epub 2021 Oct 26. PMID:\n34704199; PMCID: PMC8613120.\n24. Menon JML, Struijs F, Whaley P. The methodological rigour of systematic reviews in environmental\nhealth. Crit Rev Toxicol. 2022;52(3):167–87. 10.1080/10408444.2022.2082917.\n25. Joseph L, Gyorkos TW, Coupal L. Bayesian estimation of disease prevalence and the parameters of\ndiagnostic tests in the absence of a gold standard. Am J Epidemiol. 1995;141(3):263–72.\n2\u0000. Hui SL, Walter SD. Estimating the error rates of diagnostic tests. Biometrics. 1980;36(1):167–71.\n27. Dendukuri N, Joseph L. Bayesian approaches to modeling the conditional dependence between\nmultiple diagnostic tests. Biometrics. 2001;57(1):158–67.\n2\u0000. Collins LM, Lanza ST. Latent class and latent transition analysis: With applications in the social,\nbehavioral, and health sciences. John Wiley & Sons; 2010.\n29. McCutcheon AL. Latent class analysis. Sage; 1987.\n30. Goodman LA. Exploratory latent structure analysis using both identi\u0000able and unidenti\u0000able models.\nBiometrika. 1974;61(2):215–31.\n31. Sagi O, Rokach L. (2018). Ensemble learning: A survey. Wiley Interdisciplinary Reviews: Data Mining\nand Knowledge Discovery, 8 (4), e1249.\n32. Qureshi R, Shaughnessy D, Gill KA, Robinson KA, Li T, Agai E. Are ChatGPT and large language\nmodels the answer to bringing us closer to systematic review automation? Syst Reviews.\n2023;12(1):72.\n33. Alshami A, Elsayed M, Ali E, Eltoukhy AE, Zayed T. Harnessing the Power of ChatGPT for Automating\nSystematic Review Process: Methodology, Case Study, Limitations, and Future Directions. Systems.\n2023;11(7):351."
}