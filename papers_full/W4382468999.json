{
  "title": "Unifying Vision-Language Representation Space with Single-Tower Transformer",
  "url": "https://openalex.org/W4382468999",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5064869475",
      "name": "Jiho Jang",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5078656022",
      "name": "Chaerin Kong",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5049270104",
      "name": "DongHyeon Jeon",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A5038612161",
      "name": "Seonhoon Kim",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084897975",
      "name": "Nojun Kwak",
      "affiliations": [
        "Seoul National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3107668149",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W2987283559",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W4225750637",
    "https://openalex.org/W4221167445",
    "https://openalex.org/W6600281463",
    "https://openalex.org/W3216109288",
    "https://openalex.org/W4307106676",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W4283208789",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W4319300833",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4287592659",
    "https://openalex.org/W3217799312",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W4287547182",
    "https://openalex.org/W4312746376",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4283075937",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W4282028729",
    "https://openalex.org/W4312629998",
    "https://openalex.org/W4312877428",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Contrastive learning is a form of distance learning that aims to learn invariant features from two related representations. In this work, we explore the hypothesis that an image and caption can be regarded as two different views of the underlying mutual information, and train a model to learn a unified vision-language representation space that encodes both modalities at once in a modality-agnostic manner. We first identify difficulties in learning a one-tower model for vision-language pretraining (VLP), and propose One Representation (OneR) as a simple yet effective framework for our goal. We discover intriguing properties that distinguish OneR from the previous works that have modality-specific representation spaces such as zero-shot localization, text-guided visual reasoning and multi-modal retrieval, and present analyses to provide insights into this new form of multi-modal representation learning. Thorough evaluations demonstrate the potential of a unified modality-agnostic VLP framework.",
  "full_text": "Unifying Vision-Language Representation Space\nwith Single-Tower Transformer\nJiho Jang1∗†, Chaerin Kong1∗†, Donghyeon Jeon2, Seonhoon Kim3†, Nojun Kwak1\n1Seoul National University\n2NA VER\n3Coupang\n{geographic,veztylord,nojunk}@snu.ac.kr, donghyeon.jeon@navercorp.com, sekim625@coupang.com\nAbstract\nContrastive learning is a form of distance learning that aims\nto learn invariant features from two related representations.\nIn this work, we explore the hypothesis that an image and\ncaption can be regarded as two different views of the under-\nlying mutual information, and train a model to learn a uni-\nfied vision-language representation space that encodes both\nmodalities at once in a modality-agnostic manner. We first\nidentify difficulties in learning a one-tower model for vision-\nlanguage pretraining (VLP), and propose One Representa-\ntion (OneR) as a simple yet effective framework for our goal.\nWe discover intriguing properties that distinguish OneR from\nthe previous works that have modality-specific representation\nspaces such as zero-shot localization, text-guided visual rea-\nsoning and multi-modal retrieval, and present analyses to pro-\nvide insights into this new form of multi-modal representa-\ntion learning. Thorough evaluations demonstrate the potential\nof a unified modality-agnostic VLP framework.\nIntroduction\nSelf-supervised learning (SSL) is the core driving force be-\nhind recent boom in large scale training (Devlin et al. 2018;\nRadford et al. 2018) as it provides means to leverage a huge\nstack of unlabeled data handily accessible from the web.\nIn the computer vision community, contrastive learning is\none of the most popular SSL frameworks that essentially\naims to maximize the mutual information between two re-\nlated representations, i.e., views. When training with im-\nages, this is realized by first generating different views from\nrandom augmentations and encouraging the model to learn\nthe augmentation-invariant features.\nMeanwhile, the seminal work of CLIP (Radford et al.\n2021) has declared the opening of the Vision-Language Pre-\ntraining (VLP) era, where many works (Li et al. 2022; Mu\net al. 2021; Li et al. 2021; Yang et al. 2022; Yu et al. 2022;\nYuan et al. 2021; Zhu et al. 2022) have leveraged the con-\ntrastive objective for connecting images and their descrip-\ntions. However, they fundamentally differ from the afore-\nmentioned SSL contrastive framework in that they learn\ntwo separate representation spaces each for vision and lan-\nguage, respectively. The features from each modality are\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Typical architectures of vision-language models. (a)\nis the basic form, with one transformer encoder and a projec-\ntor for each modality. (b) adds fusion encoder blocks on top.\n(c) uses a single transformer encoder, but has separate pro-\njections. (d) unifies the two modalities with a generic one-\ntower model (OneR).\ncompared only after sufficient abstraction operations, typ-\nically done with self-attention layers in transformers and\nseparate learnable projections. This renders them short for\nmodality-agnostic representation learning, a promising re-\nsearch direction towards a generic perceptual agent.\nA modality-agnostic representation learner should be ca-\npable of both 1) mapping visual and linguistic information\ninto a unified representation space at the global sequence\nlevel and 2) mixing information within an input sequence\nin a modality-blind manner with generic token level atten-\ntions. First we hypothesize that an image (e.g. a photo of\npanda) and its linguistic description (e.g. the phrase “a\nphoto of panda”) contains common information, which can\nbe viewed as two different representations of implicit under-\nlying information, analogous to the augmented views of an\nimage. Hence, we apply contrastive SSL approach, MoCo-\nv3 (Chen, Xie, and He 2021), in VLP setting to congre-\ngate relevant semantics, either from visual signals, linguistic\nsymbols, or their mixture, in a single unified representation\nspace. This way, our model learns to associate visual signals\nwith structured symbols from the lowest level, breaking the\nboundaries between the two.\nAs shown in Fig. 1, our approach is distinguished from\nthe conventional counterparts that acknowledge the innate\ndifferences between the two modalities and encode rele-\n∗ These authors contributed equally.\n† Work done at NA VER.\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n980\nFigure 2: A truly unified vision-language representation space displays intriguing properties. (top) Visualization of embedding\nsimilarities between image patches and the text prompt. (bottom left ) Steering image classification with additional text input\nprovided as simple token sequence concatenation. Here, we plot the attention map of[CLS]. (bottom right) This mixture input\ncan also control image retrieval by combining the information from two modalities.\nvant inductive bias into the model architecture. We adopt\na generic single-tower model, thus a single representation\nspace, to handle two different modalities at once. We em-\npirically demonstrate that the failure of naive single-tower\nimage-text contrastive learning is due to the modality gap,\nand propose cross-modal mixup as a simple yet effective\nremedy. Furthermore, we train our model to learn to ag-\ngregate information within each sequence in a modality-\nagnostic manner by forwarding concatenation of image and\ntext for contrastive loss computation. This allows our model\nto form integrated representations even from mixed inputs\nof image and text, achieving both of our previous desiderata.\nWe name our framework OneR, short for One Representa-\ntion that suits both modalities.\nAside from the academic pursuit of general intelligence,\nunifying multi-modal representation space with a single\ngeneric model has been shown to have benefits in scalabil-\nity and cross-modal/cross-task transferability (Wang et al.\n2021b; Mustafa et al. 2022). We further observe that our\nOneR’s capacity to associate low-level visual signals to lan-\nguage symbols makes it an excellent zero-shot object lo-\ncalizer, and we can steer its visual reasoning with auxil-\niary language guidance thanks to its natural ability to pro-\ncess image+text mixture inputs. The fact that mixture in-\nputs are mapped to the same One Representation space fur-\nther renders operations like multi-modal retrieval straight-\nforward unlike two-leg baselines (e.g., ALBEF (Li et al.\n2021)). We note that these properties do not rely on any\nmodality-specific heads, segment tokens, nor special cross-\nattention modules, but are natural outcomes of embedding\nFigure 3: Patch embedding similarity map w.r.t. the text\nquery. This clearly shows that two towers (e.g., CLIP), two\nlegs (e.g., ALBEF) and two heads all learn modality-specific\nfeatures spaces, forbidding similarity operations between\nembeddings. Projections are not applicable since they are\nonly suited for the [CLS] token.\nsimilarity and input concatenation.\nOur key contributions can be summarized as:\n• We analyze the collapse of naive single-tower vision-\nlanguage contrastive learning, and propose cross-modal\nmixup to mitigate the modality gap.\n• We present OneR, a simple modality-agnostic represen-\ntation learning framework that combines cross-modal\nmixup with contextual modality invariance to form a uni-\nfied embedding space.\n• We conduct extensive qualitative and quantitative eval-\nuations to demonstrate the advantage of our approach,\nwhich includes distinguished capabilities in zero-shot ob-\n981\n(a) Naive single-tower ITC.\n (b) OneR at the beginning and the end of the training.\nFigure 4: T-SNE (Van der Maaten and Hinton 2008) representation visualization. Single-tower model trained with naive image-\ntext contrastive objective fails to blend two distant modalities (left). Note that image features (blue dots) almost perfectly overlap\nwith concatenation features (green dots), possibly due to sequence length bias(best viewed zoom-in). Cross-modal mixup maps\nembeddings from two disjoint modalities to a common middle ground, and the corresponding image, text and image+text\nembeddings are well clustered after 40 epochs of training (right).\nject localization, text-guided visual reasoning and multi-\nmodal retrieval (See Fig. 2).\nOvercoming Modality Gap with\nCross-Modal Mixup\nA typical vision-language pretraining framework with con-\ntrastive objective employs batch-dependent InfoNCE (Oord,\nLi, and Vinyals 2018) that pulls positive {image, text} pairs\ntogether and pushes others apart. We state this image text\ncontrastive (ITC) loss as\nLITC = ctr(F(I), F(T)), (1)\nwhere ctr(A, B) = (NCE (A, B) +NCE (B, A))/2 em-\nploys the generic InfoNCE formulation, NCE (l, r), with\nthe right term (r ) being the EMA (exponential moving av-\nerage) model output in our setting. F(X) refers to the final\ntransformer hidden state, and I, Tstands for image and text\nrespectively.\nThis formulation works well in two-tower settings\n(Fig. 1a, 1b) with separate modality-specific encoders (Rad-\nford et al. 2021; Li et al. 2021), but we have observed train-\ning failure for a generic single-tower model (Fig. 1d, Tab.\n1). Visualization of the representation space in Fig. 4a indi-\ncates the presence of a severe modality gap, as visual signals\nand linguistic symbols are significantly dissimilar. Hence,\nthe model fails to blend these two distant modalities in a\nunified representation space, being unable to encode posi-\ntive {image, text} pair close together.\nCross-Modal Mixup\nMixup (Zhang et al. 2017) was originally introduced in\nthe vision community as a data-augmentation routine that\nimproves classification performance, model robustness and\ngeneralization by extending the training data distribution\nwith linear interpolation. Recently, a concurrent work (Hao\net al. 2022) has incorporated mixup into VLP in a simi-\nlar spirit, applying mixup augmentation within each modal-\nity separately. Different from this, we boldly apply mixup\nImagenet 0-shot Top-1 Acc. Top-5 Acc.\nITC 1.65 5.25\nITC (two heads) 17.46 35.32\nITC + XMC 22.12 42.12\nITC + XMC + CIC 22.86 42.88\nITC + CMC 23.70 43.15\nTable 1: Zero-shot Imagenet (Deng et al. 2009) evaluations.\nNote that all models are one tower except for the second row.\nAdding XMC enables one tower contrastive learning, and\nenforcing modality-blind token attentions further improves\nthe performance. Masked modeling is included in all abla-\ntion models.\nacross modality, not as a means to augment the training data\nbut as a projection to map image and text embeddings to\na common middle ground. We find it to be an extremely\nsimple yet effective starting point to evade the image-text\nmodality gap, from which the traditional contrastive learn-\ning successfully guides the model for instance discrimina-\ntion. The formal definition of our cross-modal mixup con-\nstrastive (XMC) loss can be stated as\nLXMC = ctr( F(I) +F(T)\n2 , F(I) +F(T)\n2 ), (2)\nwhere we use an online model and its momentum (EMA)\ncounterpart for feature extraction in practice1. This straight-\nforward approach to mitigate modality gap works surpris-\ningly well, blending representations from the two distant\nmodalities into a single embedding space successfully and\nthereby stabilizing training. Full quantitative evaluations are\npresented in Tab. 6.\n1Note that ctr by definition in Eq. (2) uses two separate feature\nextractors (online and EMA) symmetrically.\n982\nTowards Modality-Agnostic Representations\nIn the previous section, we have identified modality gap as\nthe primary obstacle for learning a unified vision-language\nrepresentation space, and proposed XMC loss to reconcile\nthe distant modalities. Stepping further, under the hypothe-\nsis that paired image and text contain similar information, a\nmodality-agnostic representation should depend only on the\ncontent of the underlying information, not the modality (for-\nmat; text or image) it is expressed in. In other words, the final\nembedding should be similar whether it uses image or text as\nthe context (i.e., key and value in self-attention). To enforce\nsuch behavior, we devise Contextual Invariance Contrastive\n(CIC) loss and incorporate it into our framework.\nContextual Modality Invariance\nThe high level idea is to encourage the model representa-\ntion from an image context to be close to that from the text\ncontext. Specifically, from a pair, we choose either the im-\nage or the text to be the query. Then, at one side, we use\nimage tokens for key and value, while on the other side, we\nuse the text tokens. CIC penalizes the distance between the\nfinal representations from each side, guiding the model to\nextract similar information regardless of the modality of the\ncontext. Combining it with XMC in Eq. (2), the formal def-\ninition becomes\nLCIC = ctr( F(I|T) +F(T|I)\n2 , F(I|I) +F(T|T)\n2 ), (3)\nwhere F(X|Y ) refers to the final embedding of X (query)\ngiven Y as the context (key and value). We note that F(X)\nin Eq. (1) and Eq. (2) is an abbreviated expression equivalent\nto F(X|X).\nContextual Mixup Contrast (CMC)\nAs apparent from Tab. 1, CIC improves overall performance\nby encouraging the model to not only embed paired im-\nage and text close together but also utilize information from\nimage and text tokens in a similar fashion from the lowest\nlevel. To maximally leverage CIC’s generic information ag-\ngregation capacity, we adapt our model for mixed modal-\nity input scenario. Formally, we replace the left contrastive\nterm in Eq. (3) with simple concatenation of {image, text}\n(F(I, T)) and train the model to optimize Contextual Mixup\nContrastive (CMC) objective instead.\nLCMC = ctr(F(I, T|I, T), F(I|I) +F(T|T)\n2 ) (4)\nThis is a generalized form that further integrates XMC and\nCIC, which explicitly guides the model to embed mixed\nmodality inputs to the unified V-L representation space af-\nter adequate integration of information from two different\nmodalities. We utilize this property for text-aided visual rea-\nsoning (Table 3) and multi-modal retrieval (Fig. 2). The\nhigh-level idea is that the self-attention feature of concate-\nnated input can be roughly decomposed to self-attention fea-\nture of each plus the cross-attention features, and the theo-\nretical verification is provided in the supplementary.\nFigure 5: Overview of OneR. Image-text contrastive and\ncontextual mixup contrastive objective provide guidance in\nparallel with masked modeling for three input types: image,\nlanguage and multi-modal (image+text).\nMethod\nFormulation\nITC F(I) F(T)\nXMC (F(I) +F(T))/2 ( F(I) +F(T))/2\nCIC (F(I|T) +F(T|I))/2 ( F(I) +F(T))/2\nCMC F(I, T|I, T) ( F(I) +F(T))/2\nTable 2: Summary of the contrastive objectives.\nOne Representation (OneR)\nFig. 5 illustrates the overall pipeline of OneR. Model input\ncan be one of image, text or image+text, and CMC objective\nin Eq. (4) is combined with the traditional image-text con-\ntrastive (ITC) loss. Masked modeling is also carried out for\nall three input types (i.e., image, text and multi-modal). Our\nframework employs no modality-specific architectural com-\nponent except for the initial token embedding layer, making\nour model generic and modality-agnostic with minimal in-\nductive bias. Tab. 2 summarizes the overall formulations.\nExperiments\nTraining Setup\nDatasets Following prior works (Li et al. 2021; Yang et al.\n2022; Gan et al. 2020), we train OneR on the combination\nof CC3M (Sharma et al. 2018), SBU Captions (Ordonez,\nKulkarni, and Berg 2011), Visual Genome (Krishna et al.\n2017) and COCO (Lin et al. 2014), which sums up to 4M im-\nages and 5.1M image-text pairs. Ablation models are trained\non CC3M.\nImplementation Details We adopt the model architecture\nof Masked AutoEncoder (He et al. 2022) with BERT (De-\nvlin et al. 2018) word embeddings and language modeling\nhead. Unlike most prior works on VLP, we initialize our en-\ntire model from scratch, as neither ViT nor BERT suits our\ngoal towards a unified VL representation space2. 1D and 2D\n2Two-legged models typically initialize their encoders with a\n983\nFigure 6: Qualitative evaluation for object-level scene understanding. We simply compute token similarities for OneR, and\nGrad-CAM is used for CLIP and ALBEF. It is visually apparent that OneR correctly associates low-level visual signals to its\ncorresponding language symbol, resulting in segmentation-map-like patch similarity maps.\nBootstrapped Guidance INet 0-shot CIFAR100 0-shot\ntop-1 top-5 top-1 top-5\nOneR (4M) 27.33 50.17 31.45 57.52\nOneR-B (4M) 28.00 50.69 32.23 58.24\nTable 3: Evaluation with bootstrapped language guidance.\nWe can feed predicted class labels in simple concatenation\nto the input image to further improve accuracy. Note that\nthis is not possible with two-tower or two-leg models, as the\nformer does not accept mixture inputs and the latter forms a\nseparate feature space after fusion, forbidding the similarity\noperation.\nsinusoidal positional embeddings are added to text and im-\nage respectively, and a single [CLS] token is prepended to\nall three input types. Special modality indicator tokens (e.g.,\n[SEP] or [SEG]) are further removed from typical one\ntower baselines in order to train a fully modality-agnostic\nrepresentation learner. We train our model with 32 A100\nGPUs for 40 epochs under PyTorch framework. Details on\nhyperparameters are listed in the supplementary.\nProperties of One Representation\nZero-shot Localization Conventional vision-language\ntransformers typically rely on [CLS] cross-attention map\nor Grad-CAM (Selvaraju et al. 2017) for visualization.\nHowever, the former attributes the global semantics to each\npretrained ViT and a pretrained language model such as BERT,\nwhich makes the training much simpler.\nCross-modal Transfer Arch. INet MS COCO\nAcc. TR@1 IR@1\nSBU two heads 7.28 8.88 5.73\none tower 6.49 8.60 5.77\nSBU + CC3M caption two heads 8.59 10.41 6.87\none tower 8.54 11.31 7.20\nGain two heads 1.31 1.53 1.14\none tower 2.07 2.71 1.43\nTable 4: Cross-modal knowledge transfer. Under a unified\nrepresentation space, additional training in one modality\nbenefits performance in the other modality with bigger mar-\ngins. TR and IR is for text and image retrieval, respectively.\nlocal region, rendering it unsuitable for complex scene\nunderstanding such as multi-class localization (Fig. 2),\nwhile the latter requires a separately devised procedure that\ninvolves back propagation. One of the most distinguished\nqualities of OneR is its natural proficiency for object\nlocalization. Throughout the paper, we simply compute the\ncosine similarities between image patch embeddings and\nthe average-pooled text embedding for visualization. This is\npossible only because OneR maps both visual and textual\ninformation to a unified embedding space where their\nfeature similarity correctly indicates the semantic relevance.\nOtherwise, the token level similarity map conveys no\nmeaningful information, as illustrated in Fig. 3.\nWe present qualitative comparison on zero-shot local-\nization with two competitive baselines, CLIP and ALBEF,\nwhere Grad-CAM is used for their visualizations as it yields\n984\nMethod Architecture Pre. #Images\nZero-shot MS-COCO (5K) Fine-tuned MS-COCO (5K)\nText Retrieval Image Retrieval Text Retrieval Image Retrieval\nR@1 R@5 R@1 R@5 R@1 R@5 R@1 R@5\nImageBert† One Tower O 6M 44.0 71.2 32.3 59.0 66.4 89.8 50.5 78.7\nViLT One Tower O 4M 56.5 82.6 40.4 70.0 61.5 86.3 42.7 72.9\nUni-Perceiver One Tower X 44.3M 57.7 85.6 46.3 75.0 64.7 87.8 48.3 75.9\nOneR One Tower X 4M 62.9 86.3 47.0 74.7 66.1 87.8 48.3 76.0\nCLIP Two Towers X 400M 58.4 81.5 37.8 62.4 - - - -\nFLA V A Two Legs O 70M 42.7 76.8 38.4 67.5 - - - -\nALBEF Two Legs O 4M 68.7 89.5 50.1 76.4 73.1 91.4 56.8 81.5\nTCL Two Legs O 4M 71.4 90.8 53.5 79.0 75.6 92.8 59.0 83.2\nTable 5: Quantitative evaluations on COCO image and text retrieval. Two-legs models generally perform better as they have\nmodality-specific encoders and more parameters. Note that previous vision-language models typically initialize their weights\nfrom a pretrained model such as Imagenet ViT or Bert to help training (Pre.). OneR, on the other hand, achieves the best zero-\nshot performance among one-tower models without any initialization prior, and compares on par after fine-tuning. † indicates\nthe use of an additional object detection module.\nthe best output. Looking at Fig. 6, we can see that Grad-\nCAM of ALBEF better captures the spatial details compared\nto CLIP, but OneR has the most fine-grained visual reason-\ning, resulting in almost segmentation-map-like patch simi-\nlarity maps. This clearly shows that OneR has the capacity\nto relate low-level visual signals to their corresponding lin-\nguistic concepts in a unified vision-language representation\nspace.\nText-guided Visual Reasoning As illustrated in Fig. 2,\nOneR’s ability to understand image+text mixture input\nopens up possibilities for diverse forms of multi-modal rea-\nsoning. For example, we can simply concatenate additional\ntext to the image input sequence to guide its visual represen-\ntation, which can be particularly useful in a complex scene\nunderstanding setting where an image contains more than\none dominant semantic. In such cases, we cantell the model\nwhere to focus to suit our goals. We provide quantitative re-\nsults to further demonstrate this property in Table 3, where\nwe bootstrap with language guidance to improve zero-shot\nclassification accuracy. Specifically, for each image, we re-\ntrieve top-10 class labels upon embedding similarity. Then\nwe concatenate each to the image sequence and compute\nsimilarity once more, similar to sample re-ranking. The intu-\nition is that when image+text input is given, image patches\nthat attend strongly to the text label are strengthened by the\nattention mechanism, resulting in clearer representations.\nWe note that we do not provide any external guidance during\nthis procedure, which makes these gains essentially free.\nCross-modal Knowledge Transfer We hypothesize that\nunder a unified vision-language representation space, addi-\ntional training on one modality should benefit performance\nin the other modality. Table 4 validates our conjectures,\nas additional training with language data results in greater\ngains for the unified one-tower model. This could indicate\nbetter scalability of one-tower models, as there is much more\nsingle-modality data available than image-text pairs in the\nweb, which we leave for future works.\nQuantitative Evaluations\nTable 5 shows the quantitative comparison with state-of-the-\nart methods on widely used image-text retrieval benchmark.\nMethod INet MS-COCO\nAcc. TR@1 TR@5 IR@1 IR@5\nCLIP 17.1 15.0 34.8 10.9 26.7\nSLIP 23.0 21.7 45.1 15.6 35.2\nITC (two heads) 17.5 10.4 26.8 10.7 26.4\nITC 1.6 0.8 2.5 0.7 2.2\n+ XMC 22.1 25.2 48.1 15.2 33.6\n+ XMC + CIC 22.9 25.4 48.1 16.3 35.5\n+ CMC (OneR) 23.7 25.5 48.2 16.9 36.9\nTable 6: Method ablation. Our proposed components con-\nsistently improve the performance, with the final CMC out-\nperforming the two-tower baseline that uses more parame-\nters and intra-modal contrastive loss. Additional ablations\nare presented in the supplementary.\nModels with modality-specific encoders typically show bet-\nter performance as they have more parameters and archi-\ntectural inductive bias. Among one-tower baselines, OneR\nshows the best zero-shot performance, sometimes with sig-\nnificant margins. We note that OneR achieves such compe-\ntent outcome without any initialization prior commonly used\nin the literature. This shows that vision and language modal-\nities can be effectively encoded in a single representation\nspace with minimal inductive bias, once the aforementioned\nobstacle (i.e., innate modality gap) is overcome.\nIn Table 6, we present full ablations for our framework.\nNaive ITC with one tower fails due to the modality gap, and\nadding modality-specific projectors can be the minimal ar-\nchitectural modification that works, but still lags behind our\nmethod. CMC combines XMC and CIC into a concise for-\nmulation, which is explained further in the supplementary,\nresulting in the best performance that surpasses competent\ntwo-tower baselines.\nVisual Reasoning Analysis\nWe further analyze the visual reasoning mechanism of OneR\nto provide insights into the properties of unified vision-\nlanguage representation space.\nRobustness Fig. 7 shows an example of how OneR recog-\n985\nFigure 7: As OneR learns to associate low-level visual sig-\nnals to the language, it shows robust visual reasoning even\nwith a relatively small pretraining dataset. Above, OneR ro-\nbustly recognizes bicycle from different visual clues (e.g.,\nhandles, wheels or the body).\nnizes an object (bicycle, in this case) with different visual\nclues. OneR recognizes a bicycle even from partial images\nof handles or wheels, which we believe is key to its robust-\nness in visual understanding. We present additional results\nin the supplementary materials, including inference in unfa-\nmiliar domains.\nMulti-level vision-language connection Looking at Fig. 8,\nOneR recognizes the moon as being visually similar to ba-\nnana in terms of embedding similarity, while ALBEF con-\ndenses the global semantic in [CLS], resulting in a ran-\ndomly scattered Grad-CAM. Although this can be viewed\nas a failure case of OneR, it reveals how OneR perceives the\nvisual signals. On the right, we can see that zebra and gi-\nraffe are visually similar, and their definitions contain simi-\nlar phrases such as ‘an African mammal’, resulting in some\noverlaps in the two similarity maps. However, after abstract-\ning the linguistic semantics, the model correctly identifies\neach, which shows its ability to process high-level seman-\ntics as well. Overall, OneR learns both low-level and high-\nlevel vision-language connections, making it a competent\nmodality-agnostic representation learner.\nRelated Works\nVision-Language Pretraining CLIP (Radford et al. 2021)\nfirst demonstrated the effectiveness of large-scale vision-\nlanguage contrastive learning. ALIGN (Jia et al. 2021)\nscaled up the training with noisy alt-text pair data. Another\nline of works (Li et al. 2020b,a; Chen et al. 2020b; Gan et al.\n2020; Kim, Son, and Kim 2021) leveraged an off-the-shelf\nobject detector to extract visual concepts first, which were\nthen used to train the multi-modal transformer. In an attempt\nto learn cross-modal interactions, ALBEF (Li et al. 2021),\nTCL (Yang et al. 2022), FLA V A (Singh et al. 2022), and Flo-\nrence (Yuan et al. 2021) adopted multi-modal fusion layers\non top of modality-specific transformer encoders. Another\ngroup of works (Li et al. 2022; Yu et al. 2022; Wang et al.\n2021b; Mokady, Hertz, and Bermano 2021) explored gener-\native modeling, typically in the form of image captioning, to\nFigure 8: (left ) Patch embedding similarity (OneR) and\nGrad-Cam (ALBEF). (right ) Patch embedding similarity\nmap w.r.t. definitions of zebra and giraffe.\nfurther improve performances on challenging tasks such as\nvisual question answering.\nUnified VL Framework Uni-Perceiver (Zhu et al. 2022)\nadopted a single-tower transformer architecture to tackle dif-\nferent V-L tasks in a unified manner. Unified-IO (Lu et al.\n2022) further used a pretrained VQ-V AE to model a wide\nrange of tasks with a generic sequence-to-sequence frame-\nwork. These works have demonstrated promising direction\ntowards a unified perception system, but the fact that they\nemploy multi-task pretraining strategy renders them less\nscalable compared to CLIP or ALIGN. UFO (Wang et al.\n2021a) has shown that a single transformer model suffices\nfor typical VLP when combined with two modality-specific\nprojectors. LIMoE (Mustafa et al. 2022), a concurrent work\nof ours, also explores single-tower (two heads) VLP but with\na new set of inductive biases, i.e., mixture of experts. OneR,\nin contrast, learns a common embedding space without any\nmodality-specific components, which empowers the model\nwith unique capabilities previously demonstrated.\nSelf-supervised Learning Self-supervised learning first\nbloomed in the NLP domain as masked language model-\ning (MLM) and language modeling (LM) enabled pretrain-\ning large language models with huge stock of unlabeled text\ncorpus (Devlin et al. 2018; Radford et al. 2018; Lewis et al.\n2019; Liu et al. 2019). In the vision community, contrastive\nlearning has led the rise of SSL. MoCo (He et al. 2020) and\nSimCLR (Chen et al. 2020a) are the pioneers to demonstrate\nthe potential of contrastive representation learning, which\nwe adapt for VLP setting. BYOL (Grill et al. 2020) and Sim-\nSiam (Chen and He 2021) explored new settings with no\nnegative samples that mitigate the batch size dependency.\nRecent works (Caron et al. 2021; Chen, Xie, and He 2021;\nJang et al. 2021) actively employ ViT (Dosovitskiy et al.\n2020) to improve the performance and discover new proper-\nties. This architecture is also widely used in VLP as it can\nmodel data from different modalities in an integrated man-\nner.\nConclusion\nModality-agnostic representation learning is a meaningful\nstep towards a generic perceptual agent that understands\nthe environment in a similar way as humans do. In this\nwork, we explore the difficulties of unifying modalities\ninto a single representation space, and introduce OneR\nas a generic framework that shows unique qualities as a\nmodality-agnostic representation learner.\n986\nAcknowledgements\nThis work was supported by NRF grant\n(2021R1A2C3006659) and IITP grants (No.2022-0-\n00953, No.2021-0-01343), all of which are funded by\nKorean Government. The authors would like to express our\nsincere appreciation to Jungwon Choi for her exceptional\nefforts in creating the figures for this paper.\nReferences\nCaron, M.; Touvron, H.; Misra, I.; J´egou, H.; Mairal, J.; Bo-\njanowski, P.; and Joulin, A. 2021. Emerging properties in\nself-supervised vision transformers. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n9650–9660.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020a.\nA simple framework for contrastive learning of visual repre-\nsentations. In International conference on machine learning,\n1597–1607. PMLR.\nChen, X.; and He, K. 2021. Exploring simple siamese repre-\nsentation learning. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 15750–\n15758.\nChen, X.; Xie, S.; and He, K. 2021. An empirical study of\ntraining self-supervised vision transformers. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 9640–9649.\nChen, Y .-C.; Li, L.; Yu, L.; El Kholy, A.; Ahmed, F.; Gan,\nZ.; Cheng, Y .; and Liu, J. 2020b. Uniter: Learning universal\nimage-text representations. 104–120.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nGan, Z.; Chen, Y .-C.; Li, L.; Zhu, C.; Cheng, Y .; and Liu,\nJ. 2020. Large-scale adversarial training for vision-and-\nlanguage representation learning. Advances in Neural In-\nformation Processing Systems, 33: 6616–6628.\nGrill, J.-B.; Strub, F.; Altch ´e, F.; Tallec, C.; Richemond,\nP.; Buchatskaya, E.; Doersch, C.; Avila Pires, B.; Guo, Z.;\nGheshlaghi Azar, M.; et al. 2020. Bootstrap your own latent-\na new approach to self-supervised learning. Advances in\nneural information processing systems, 33: 21271–21284.\nHao, X.; Zhu, Y .; Appalaraju, S.; Zhang, A.; Zhang, W.; Li,\nB.; and Li, M. 2022. MixGen: A New Multi-Modal Data\nAugmentation. arXiv preprint arXiv:2206.08358.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll ´ar, P.; and Girshick,\nR. 2022. Masked autoencoders are scalable vision learners.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 16000–16009.\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2020.\nMomentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 9729–9738.\nJang, J.; Kim, S.; Yoo, K.; Kim, J.; and Kwak, N. 2021. Self-\nDistilled Self-Supervised Representation Learning. arXiv\npreprint arXiv:2111.12958.\nJia, C.; Yang, Y .; Xia, Y .; Chen, Y .-T.; Parekh, Z.; Pham, H.;\nLe, Q.; Sung, Y .-H.; Li, Z.; and Duerig, T. 2021. Scaling\nup visual and vision-language representation learning with\nnoisy text supervision. In International Conference on Ma-\nchine Learning, 4904–4916. PMLR.\nKim, W.; Son, B.; and Kim, I. 2021. Vilt: Vision-and-\nlanguage transformer without convolution or region super-\nvision. In International Conference on Machine Learning,\n5583–5594. PMLR.\nKrishna, R.; Zhu, Y .; Groth, O.; Johnson, J.; Hata, K.;\nKravitz, J.; Chen, S.; Kalantidis, Y .; Li, L.-J.; Shamma,\nD. A.; et al. 2017. Visual genome: Connecting language and\nvision using crowdsourced dense image annotations. Inter-\nnational journal of computer vision, 123(1): 32–73.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2019. Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehen-\nsion. arXiv preprint arXiv:1910.13461.\nLi, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. Blip: Boot-\nstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. arXiv preprint\narXiv:2201.12086.\nLi, J.; Selvaraju, R.; Gotmare, A.; Joty, S.; Xiong, C.; and\nHoi, S. C. H. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation. Ad-\nvances in neural information processing systems, 34: 9694–\n9705.\nLi, W.; Gao, C.; Niu, G.; Xiao, X.; Liu, H.; Liu, J.; Wu, H.;\nand Wang, H. 2020a. Unimo: Towards unified-modal under-\nstanding and generation via cross-modal contrastive learn-\ning. arXiv preprint arXiv:2012.15409.\nLi, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.;\nWang, L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020b. Oscar:\nObject-semantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision , 121–\n137. Springer.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European conference\non computer vision, 740–755. Springer.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLu, J.; Clark, C.; Zellers, R.; Mottaghi, R.; and Kembhavi,\nA. 2022. Unified-io: A unified model for vision, language,\nand multi-modal tasks. arXiv preprint arXiv:2206.08916.\n987\nMokady, R.; Hertz, A.; and Bermano, A. H. 2021. Clip-\ncap: Clip prefix for image captioning. arXiv preprint\narXiv:2111.09734.\nMu, N.; Kirillov, A.; Wagner, D.; and Xie, S. 2021. Slip:\nSelf-supervision meets language-image pre-training. arXiv\npreprint arXiv:2112.12750.\nMustafa, B.; Riquelme, C.; Puigcerver, J.; Jenatton, R.; and\nHoulsby, N. 2022. Multimodal Contrastive Learning with\nLIMoE: the Language-Image Mixture of Experts. arXiv\npreprint arXiv:2206.02770.\nOord, A. v. d.; Li, Y .; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nOrdonez, V .; Kulkarni, G.; and Berg, T. 2011. Im2text: De-\nscribing images using 1 million captioned photographs. Ad-\nvances in neural information processing systems, 24.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International Conference on\nMachine Learning, 8748–8763. PMLR.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by gener-\native pre-training. OpenAI research covers:1806.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual explana-\ntions from deep networks via gradient-based localization. In\nProceedings of the IEEE international conference on com-\nputer vision, 618–626.\nSharma, P.; Ding, N.; Goodman, S.; and Soricut, R. 2018.\nConceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In Proceedings of\nthe 56th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), 2556–2565.\nSingh, A.; Hu, R.; Goswami, V .; Couairon, G.; Galuba, W.;\nRohrbach, M.; and Kiela, D. 2022. Flava: A foundational\nlanguage and vision alignment model. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 15638–15650.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research, 9(11).\nWang, J.; Hu, X.; Gan, Z.; Yang, Z.; Dai, X.; Liu, Z.;\nLu, Y .; and Wang, L. 2021a. UFO: A unified transformer\nfor vision-language representation learning. arXiv preprint\narXiv:2111.10023.\nWang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y .;\nand Cao, Y . 2021b. Simvlm: Simple visual language\nmodel pretraining with weak supervision. arXiv preprint\narXiv:2108.10904.\nYang, J.; Duan, J.; Tran, S.; Xu, Y .; Chanda, S.; Chen,\nL.; Zeng, B.; Chilimbi, T.; and Huang, J. 2022. Vision-\nLanguage Pre-Training with Triple Contrastive Learning. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 15671–15680.\nYu, J.; Wang, Z.; Vasudevan, V .; Yeung, L.; Seyedhos-\nseini, M.; and Wu, Y . 2022. Coca: Contrastive caption-\ners are image-text foundation models. arXiv preprint\narXiv:2205.01917.\nYuan, L.; Chen, D.; Chen, Y .-L.; Codella, N.; Dai, X.; Gao,\nJ.; Hu, H.; Huang, X.; Li, B.; Li, C.; et al. 2021. Florence: A\nnew foundation model for computer vision. arXiv preprint\narXiv:2111.11432.\nZhang, H.; Cisse, M.; Dauphin, Y . N.; and Lopez-Paz, D.\n2017. mixup: Beyond empirical risk minimization. arXiv\npreprint arXiv:1710.09412.\nZhu, X.; Zhu, J.; Li, H.; Wu, X.; Li, H.; Wang, X.; and Dai,\nJ. 2022. Uni-perceiver: Pre-training unified architecture for\ngeneric perception for zero-shot and few-shot tasks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 16804–16815.\n988",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7070502638816833
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.6246976852416992
    },
    {
      "name": "Representation (politics)",
      "score": 0.6017149090766907
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5690398812294006
    },
    {
      "name": "Modal",
      "score": 0.5109063982963562
    },
    {
      "name": "Feature learning",
      "score": 0.48113858699798584
    },
    {
      "name": "Modalities",
      "score": 0.4802434742450714
    },
    {
      "name": "Natural language processing",
      "score": 0.46541881561279297
    },
    {
      "name": "Transformer",
      "score": 0.41092365980148315
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    }
  ],
  "cited_by": 7
}