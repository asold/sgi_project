{
    "title": "Face Transformer for Recognition",
    "url": "https://openalex.org/W3151034650",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3210162993",
            "name": "Zhong Yaoyao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2349297223",
            "name": "Deng Wei-hong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2663800299",
        "https://openalex.org/W2145287260",
        "https://openalex.org/W2515770085",
        "https://openalex.org/W2404498690",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W3110144845",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W3099206234",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2557864411",
        "https://openalex.org/W1782590233",
        "https://openalex.org/W3022265721",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2871667416",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W3187418919",
        "https://openalex.org/W2963466847",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W1509966554",
        "https://openalex.org/W3101227480",
        "https://openalex.org/W2963351113",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W2969985801",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W2752828042",
        "https://openalex.org/W2962898354"
    ],
    "abstract": "Recently there has been a growing interest in Transformer not only in NLP but also in computer vision. We wonder if transformer can be used in face recognition and whether it is better than CNNs. Therefore, we investigate the performance of Transformer models in face recognition. Considering the original Transformer may neglect the inter-patch information, we modify the patch generation process and make the tokens with sliding patches which overlaps with each others. The models are trained on CASIA-WebFace and MS-Celeb-1M databases, and evaluated on several mainstream benchmarks, including LFW, SLLFW, CALFW, CPLFW, TALFW, CFP-FP, AGEDB and IJB-C databases. We demonstrate that Face Transformer models trained on a large-scale database, MS-Celeb-1M, achieve comparable performance as CNN with similar number of parameters and MACs. To facilitate further researches, Face Transformer models and codes are available at https://github.com/zhongyy/Face-Transformer.",
    "full_text": "1\nFace Transformer for Recognition\nYaoyao Zhong, Weihong Deng\nAbstractâ€”Recently there has been a growing interest in\nTransformer not only in NLP but also in computer vision.\nWe wonder if transformer can be used in face recognition\nand whether it is better than CNNs. Therefore, we investigate\nthe performance of Transformer models in face recognition.\nConsidering the original Transformer may neglect the inter-\npatch information, we modify the patch generation process and\nmake the tokens with sliding patches which overlaps with each\nothers. The models are trained on CASIA-WebFace and MS-\nCeleb-1M databases, and evaluated on several mainstream bench-\nmarks, including LFW, SLLFW, CALFW, CPLFW, TALFW,\nCFP-FP, AGEDB and IJB-C databases. We demonstrate that\nFace Transformer models trained on a large-scale database,\nMS-Celeb-1M, achieve comparable performance as CNN with\nsimilar number of parameters and MACs. To facilitate further\nresearches, Face Transformer models and codes are available at\nhttps://github.com/zhongyy/Face-Transformer.\nIndex Termsâ€”Face Recognition, Neural networks, Trans-\nformer.\nI. I NTRODUCTION\nR\nECENTLY it seems a popular trend to apply Transformer\nin different computer vision tasks including image clas-\nsiï¬cation [1], object detection [2], video processing [3] and\nso on. Although the inner workings of Transformer is not\nthat clear, researchers come up with idea after idea to apply\nTransformer in different kinds of ways [4], [5], [6] because of\nits strong representation ability.\nBased on large-scale training databases [7] and effec-\ntive loss functions [8], [9], [10], convolution neural net-\nworks (CNNs), from VGGNet [11] to ResNet [12], have\nachieved great success in face recognition over the past few\nyears [10]. DeepFace [13] ï¬rst uses a 9-layer CNN in face\nrecognition, and obtains a 97.35% accuracy on the LFW\ndatabase. FaceNet [14] adopts GoogleNet [15], assisted by\na private large scale dataset, achieving state-of-art perfor-\nmance (99.63% on LFW) at that time. SphereNet [8] adopts\na 64-layer ResNet [12] network, with a large-margin loss\nfunction, achieving 99.42% accuracy on the LFW database.\nArcFace [10] develops ResNet [12] with an IR block and\nachieves new state-of-art performance on several benchmarks.\nDespite the success of CNNs, we still wonder can Trans-\nformer be used in face recognition and whether it is better than\nResNet-like CNNs. Since Transformer has shown its excellent\nperformance combined with large scale databases [1], while\nthere have been lots of large scale training database in face\nrecognition. It is interesting to observe the performance of\ncombination of Transformer and large scale face training\ndatabases. Perhaps Transformer is just the best to challenge\nThe authors are with the Pattern Recognition and Intelligent Sys-\ntem Laboratory, School of Artiï¬cial Intelligence, Beijing University\nof Posts and Telecommunications, Beijing 100876, China (e-mail:\nzhongyaoyao@bupt.edu.cn; whdeng@bupt.edu.cn).\nthe CNNs hegemony over the face recognition task. It is\nknown that, the efï¬ciency bottleneck of Transformer models,\nis just the key of them, i.e., the self-attention mechanism,\nwhich incurs a complexity of ğ‘‚Â¹ğ‘›2Âºwith respect to sequence\nlength [16]. Of course efï¬ciency is important for face recog-\nnition models, but in this paper, let us mainly determine the\nfeasibility of applying Transformer models in face recognition\nand leave out the potential efï¬ciency problem of them.\nWe ï¬rst experiment with a standard Transformer [17] as\nViT [1] did. However, the original ViT directly ï¬‚atten the\nimages into patches, which may neglect inter-patch informa-\ntion. Since some of important facial features will be par-\ntitioned into different tokens. To better describe the inter-\npatch information, we slightly modify the tokens generation\nmethod of ViT, to make the image patch overlaps, which can\nimprove the performance compared with original ViT and will\nnot increase the computing cost. Face Transformer models\nare trained on a large scale training database, MS-Celeb-\n1M [7] database, supervised with CosFace [9], and evaluated\non several face recognition benchmarks including LFW [18],\nSLLFW [19], CALFW [20], CPLFW [21], TALFW [22] CFP-\nFP [23], AgeDB-30 [24], and IJB-C [25] databases. Finally,\nwe demonstrate that Transformer models trained on a large-\nscale database obtain comparable performance as CNN with\na similar number of parameters and MACs. In addition, it is\nreasonable to ï¬nd the Transformer models attend to the face\narea as we expected.\nThe contribution of our work is that we show the feasi-\nbility of Transformer models in face recognition and report\npromising experiment results. How to further improve the\nperformance and efï¬ciency of Transformer models in face\nrecognition is a promising task for future research.\nII. F ACE TRANSFORMER\nIn this paper, following the open-set face recognition\npipeline [8], Face Transformer is trained on face databases\n(with image ğ‘¿ with label ğ‘¦) in a supervised manner, where\nface images are encoded using a well-designed network, and\nthe output face image embeddings are supervised by an\nelaborate loss function [8], [9], [10] for better discriminative\nability, as shown in Figure 1.\nA. Network Architecture\nFace Transformer model follows the architecture of ViT [1],\nwhich applies the original Transformer [17].\nThe only difference is that, we modify the tokens generation\nmethod of ViT, to generate tokens with sliding patches, i.e.,\nto make the image patch overlaps, for the better description of\nthe inter-patch information, as shown in Figure 1. Speciï¬cally,\nwe extract sliding patches from the image ğ‘¿ 2Rğ‘Š\u0002ğ‘Š\u0002ğ¶ with\narXiv:2103.14803v2  [cs.CV]  13 Apr 2021\n2\nFig. 1. The overall of Face Transformer. The face images are split into multiple patches and input as tokens to the transformer encoder. To better describe the\ninter-patch information, we modify the tokens generation method of ViT [1], to make the image patch overlaps slightly, which can improve the performance\ncompared with original ViT. The Transformer encoder is basically a standard Transformer model [17]. Eventually, the face image embeddings can be used\nfor loss functions [9], [10]. The illustration is inspired by ViT [1].\npatch size ğ‘ƒ and stride ğ‘† for them (with implicit zero on\nboth sides of input), and ï¬nally obtain a sequence of ï¬‚attened\n2D patches ğ‘¿ğ’‘ 2Rğ‘\u0002Â¹ğ‘ƒ2\u0002ğ¶Âº. Â¹ğ‘ŠÂ–ğ‘ŠÂºis the resolution of the\noriginal image while Â¹ğ‘ƒÂ–ğ‘ƒÂºis the resolution of each image\npatch. The effective sequence length is the number of patches\nğ‘ = bğ‘ŠÂ¸2\u0002ğ‘\u0000Â¹ğ‘ƒ\u00001Âº\nğ‘† Â¸1c, where ğ‘ is the amount of zero-\npaddings.\nAs ViT did, a trainable linear projection maps the ï¬‚attened\npatches ğ‘¿ğ’‘ to the model dimension D, and outputs the patch\nembeddings ğ‘¿ğ’‘ ğ‘¬. The class token, i.e., a learnable embedding\n(ğ‘¿ğ‘ğ‘™ğ‘ğ‘ ğ‘  = ğ’›0\n0) is concatenated to the patch embeddings, and\nits state at the output of the Transformer encoder ( ğ’›0\nğ¿) is the\nï¬nal face image embedding, as Equation 2. Then, position\nembeddings are added to the patch embeddings to retain\npositional information. The ï¬nal embedding\nğ’›0 =\n\u0002\nğ‘¿ğ‘ğ‘™ğ‘ğ‘ ğ‘ ; ğ‘¿1\nğ‘ğ‘¬; ğ‘¿2\nğ‘ğ‘¬; Â•Â•Â• ; ğ‘¿ğ‘\nğ‘ ğ‘¬\n\u0003\nÂ¸ğ‘¬ğ‘ğ‘œğ‘ Â– (1)\nserves as input to the Transformer,\nğ’›0\nğ‘™ = ğ‘€ğ‘†ğ´Â¹ğ¿ğ‘Â¹ğ’›ğ‘™\u00001ÂºÂºÂ¸ ğ’›ğ‘™\u00001Â–ğ‘™ = 1Â–Â•Â•Â•Â–ğ¿Â–\nğ’›ğ‘™ = ğ‘€ğ¿ğ‘ƒÂ¹ğ¿ğ‘Â¹ğ’›0\nğ‘™ÂºÂºÂ¸ ğ’›0\nğ‘™Â–ğ‘™ = 1Â–Â•Â•Â•Â–ğ¿Â–\nğ’™ = ğ¿ğ‘Â¹ğ’›0\nğ¿ÂºÂ–\n(2)\nwhich consists of multiheaded self-attention (MSA) and MLP\nblocks, with LayerNorm (LN) before each block and residual\nconnections after each block, as shown in Figure 1. In Equa-\ntion 2, the output ğ’™ is the ï¬nal output of Transformer model.\nOne of the key block of Transformer, MSA, is composed\nof ğ‘˜ parallel self-attention (SA),\nÂ»ğ’’Â–ğ’ŒÂ–ğ’—Â¼= ğ’›ğ‘¼ğ’’ğ’Œğ’— Â–\nğ‘†ğ´Â¹ğ’›Âº= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ Â¹ğ’’ğ’Œğ‘‡Â\nâˆšï¸\nğ·â„Âºğ’—Â–\n(3)\nwhere ğ’› 2RÂ¹ğ‘Â¸1Âº\u0002ğ· is an input sequence, ğ‘¼ğ’’ğ’Œğ’— 2Rğ·\u00023ğ·â„\nis the weight matrix for linear transformation, and ğ‘¨ =\nğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ Â¹ğ’’ğ’Œğ‘‡Âpğ·â„Âºis the attention map. The output of MSA\nis the concatenation of ğ‘˜ attention head outputs\nğ‘€ğ‘†ğ´Â¹ğ’›Âº= Â»ğ‘†ğ´1 Â¹ğ’›Âº; ğ‘†ğ´2 Â¹ğ’›Âº; Â•Â•Â• ; ğ‘†ğ´ğ‘˜Â¹ğ’›ÂºÂ¼ğ‘¼ğ‘šğ‘ ğ‘Â– (4)\nwhere ğ‘¼ğ‘šğ‘ ğ‘ 2Rğ‘˜ğ·â„\u0002Â¹ğ·Â¸1Âº.\nB. Loss Function\nThe output ğ‘¥ of Equation 2, i.e., the ï¬nal output of Trans-\nformer model, is supervised by an elaborate loss function [8],\n[9], [10] for better discriminative ability,\nğ¿= \u0000log ğ‘ƒğ‘¦ = \u0000log ğ‘’ğ‘¾ğ’šğ‘‡ğ’™Â¸ğ’ƒğ’š\nÃğ¶\nğ‘—=1 ğ‘’ğ‘¾ğ’‹ğ‘‡ğ’™Â¸ğ’ƒğ’‹\nÂ• (5)\nwhere ğ‘¦ is the label, ğ‘ƒğ‘¦ is the predicted probability of\nassigning ğ’™ to class ğ‘¦, ğ¶ is the number of identities, ğ‘¾ğ‘— is the\nğ‘—-th column of the weight of the last fully connected layer, and\nğ’ƒğ’‹ 2RC is the bias. Softmax based loss functions [26], [8],\n[9], [10] remove the bias term and transform ğ‘¾ğ’‹ğ‘‡ğ’™ = ğ‘ cos ğœƒğ‘—,\nand incorporate large margin in the cos ğœƒğ‘¦ğ‘– term [8], [9], [10].\nTherefore, softmax based loss functions can be formulated as\nğ¿= \u00001\nğ‘\nğ‘âˆ‘ï¸\nğ‘–=1\nlog ğ‘ƒğ‘¦ğ‘–\n= \u00001\nğ‘\nğ‘âˆ‘ï¸\nğ‘–=1\nlog ğ‘’ğ‘ ğ‘“ Â¹ğœƒğ‘¦ğ‘–Âº\nğ‘’ğ‘ ğ‘“ Â¹ğœƒğ‘¦ğ‘–ÂºÂ¸Ãğ¶\nğ‘—=1Â–ğ‘—â‰ ğ‘¦ğ‘– ğ‘’ğ‘ cos ğœƒğ‘—\nÂ–\n(6)\nwhere ğ‘“Â¹ğœƒğ‘¦ğ‘–Âº= cos ğœƒğ‘¦ğ‘– \u0000ğ‘š in CosFace [9].\nIII. E XPERIMENT\nA. Implementation Details\nWe apply two training databases, CASIA-WebFace and MS-\nCeleb-1M [7]. CASIA-WebFace is a sweet training database\nand contains 0.49M images from 10,575 celebrities, which\ncan be seen as a relatively small-scale database compared\nwith million-scale ones [7]. MS-Celeb-1M is a popular large\nscale training database in face recognition and we use the\nclean version reï¬ned by insightface [10], which contains 5.3M\nimages of 93,431 celebrities. We choose CosFace [9] ( ğ‘  = 64\nand ğ‘š = 0.35) as the loss function for better convergence and\n3\nrecognition performance. The face images are aligned to 112\n\u0002112. The Horizontally ï¬‚ip with a probability of 50% is used\nfor training data augmentation.\nFor comparison, the CNN architecture used in our work\nis a modiï¬ed ResNet-100 [12] proposed in the ï¬rst version\nof ArcFace paper [10], which uses IR blocks (BN-Conv-BN-\nPReLU-Conv-BN) and applies the â€œBN [27]-Dropout [28]-\nFC-BNâ€ structure to get the ï¬nal 512- ğ· embedding feature.\nWe also experiment with the recent proposed T2T-ViT [5].\nThe number of parameters, MACs and inference speed (Tesla\nV100, Intel Xeon E5-2698 v4) of these face recognition\nmodels are listed in Table I. Details are as follows. For ViT\nmodels, the number of layers is 20, the number of heads is 8,\nhidden size is 512, MLP size is 2048. For the Token-to-Token\npart of T2T-ViT model, the depth is 2, hidden dim is 64, and\nMLP size is 512; while for the backbone, the number of layers\nis 24, the number of heads is 8, hidden size is 512, MLP size\nis 2048. Note that, the â€œViT-P10S8â€ represents the ViT model\nwith 10 \u000210 patch size, with stride ğ‘† = 8, and â€œViT-P8S8â€\nrepresents no overlapping between tokens.\nTABLE I\nNUMBER OF PARAMETERS , MAC S AND INFERENCE SPEED OF FACE\nRECOGNITION MODELS .\nModels Params (M) MACs (G) Img/Sec\nResNet-100 [12] 65.1 12.1 41.73\nViT-P8S8 [1] 63.2 12.4 41.72\nT2T-ViT [5] 63.5 12.7 38.08\nViT-P10S8 63.3 12.4 44.59\nViT-P12S8 63.3 12.4 42.45\nWe use AdamW [29] and cosine learning rate decay fol-\nlowing DeiT [4]. The models are trained from scratch without\npre-training. With 1 warmup epoch, the initial learning rate is\nset as 3e-4, and we lower it to 1e-4 when the training accuracy\nreaches a stable stage (about 20 epochs).\nB. Results on Mainstream Benchmarks\nWe mainly report recognition performance of models\non several mainstream benchmarks including LFW [18],\nSLLFW [19], CALFW [20], CPLFW [21], TALFW [22] CFP-\nFP [23], AgeDB-30 [24], and IJB-C [25] databases. LFW\ndatabase contains 13,233 face images from 5,749 different\nidentities, which is a classic benchmark for unconstrained\nface veriï¬cation. Similar-looking LFW (SLLFW), Cross-Age\nLFW (CALFW), Cross-Pose LFW (CPLFW) and Transferable\nAdversarial LFW (TALFW) databases are constructed based\non LFW database, to emphasize similar-looking challenges,\ncross-age challenge and cross-pose challenge, and adversar-\nial robustness of face recognition. CFP-FP database is built\nfor facilitating large pose variation and AgeDB-30 database\nis a manually collected cross-age database. IJB-C database\ncontains both still images and video frames to address the\nunconstrained face recognition.\nThe experimental results are shown in Table II and Table III.\nWe ï¬rst ï¬nd that in Table II, Face Transformer models\ntrained on CASIA-WebFace database performs much worse\nthan ResNet-100. Actually, we ï¬nd that the accuracy of Face\nTransformer models trained on CASIA-WebFace can reach a\nhigh level as ResNet-100, while models cannot generalize well\non test databases, which indicates that the scale of CASIA-\nWebFace may be not enough for Transformer models.\nWhile things change when we use a much larger training\ndatabase, MS-Celeb-1M. The performance of Face Trans-\nformer models demonstrate promising results on large-scale\nface training databases. The performance of Face Transformer\nis competitive compared to the ResNet-100 with similar\nnumber of parameters and MACs. Compared with â€œViT-\nP8S8â€, â€œViT-P10S8â€ and â€œViT-P12S8â€ have better perfor-\nmance, which demonstrates the overlapping patches can help\nin some degree. T2T-ViT also obtain good performance,\nwhile limited computer source, more hyper-parameters for\nT2T block remains to try. Another interest point is that,\nTransformer models obtain a little higher accuracy on TALFW\ndatabase, which is a database with transferable adversarial\nnoise. Since TALFW database is generated using CNNs as\nsurrogate models, it seems that there is no signiï¬cant speciï¬-\ncality with Transformer in terms of adversarial robustness. It\nis interesting to explore the performance of combination of\nFace Transformer models and adversarial training.\nC. Discussion\n1) Attention Area Analysis: Since the key of Transformer\nmodels is the self-attention mechanism, we analyze how the\nTransformer models concentrate on face images by analyzing\nthe ViT-P12S8 model trained on MS-Celeb-1M. Speciï¬cally,\nwe use the Attention Rollout [30] method, which recursively\nmultiplies the modiï¬ed attention matrices 0Â•5ğ‘¨ Â¸0Â•5ğ‘° of all\nlayers, where ğ‘¨ = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ Â¹ğ’’ğ’Œğ‘‡Âpğ·â„Âºis the attention map\nof Equation 3. We demonstrate that Transformer models attend\nto the face area as we expected, as shown in Figure 2.\nFig. 2. With the help of Attention Rollout [30] techniques, we analyze how the\nTransformer models (MS-Celeb-1M, ViT-P12S8) concentrate on face images,\nand ï¬nd that face Transformer models attend to the face area as we expected.\n2) Attention Matrices Visualization: To further understand\nthe Transformer models (MS-Celeb-1M, ViT-P12S8), we visu-\nalize the attention matrices of different layers, and calculate the\nmean attention distance in the image space, which is seemed\nas the receptive ï¬eld as CNNs [1], shown in Figure 3. While\nwe ï¬nd that although the deepest layers attend to long distance\nrelationship, it seems that the attention distance of the lowest\nlayer in Face Transformer models is relatively longer than the\noriginal ViT [1].\n3) Occlusion Robustness: The key of Face Transformer\nmodels is the self-attention mechanism and it seems that they\n4\nTABLE II\nPERFORMANCE ON LFW [18], SLLFW [19], CALFW [20], CPLFW [21], TALFW [22] CFP-FP [23] AND AGEDB-30 [24] DATABASES .\nTraining Data Models LFW SLLFW CALFW CPLFW TALFW CFP-FP AgeDB-30\nCASIA-WebFace\nResNet-100 [12] 99.55 98.65 94.13 90.93 53.17 96.30 95.50\nViT-P8S8 [1] 97.32 90.78 86.78 80.78 83.05 86.60 81.48\nViT-P12S8 97.42 90.07 87.35 81.60 84.00 85.56 81.48\nMS-Celeb-1M\nResNet-100 [12] 99.82 99.67 96.27 93.43 64.88 96.93 98.27\nViT-P8S8 [1] 99.83 99.53 95.92 92.55 74.87 96.19 97.82\nT2T-ViT [5] 99.82 99.63 95.85 93.00 71.93 96.59 98.07\nViT-P10S8 99.77 99.63 95.95 92.93 72.95 96.43 97.83\nViT-P12S8 99.80 99.55 96.18 93.08 70.13 96.77 98.05\nTABLE III\nCOMPARISON OF DIFFERENT MODELS TRAINED ON MS-C ELEB -1M ON\nTHE IJB-C DATABASE [25].\nModels Veriï¬cation 1:1 TAR@FAR\n1e-4 1e-3 1e-2 1e-1\nResNet-100 [12] 96.36 97.36 98.41 99.13\nViT-P8S8 [1] 95.96 97.28 98.22 98.99\nT2T-ViT [5] 95.67 97.10 98.14 98.90\nViT-P10S8 96.06 97.45 98.23 98.96\nViT-P12S8 96.31 97.49 98.38 99.04\nFig. 3. (1) Visualization of the attention matrices of different layers. (2) Mean\nattention distance of attended area by head and network depth.\nconcentrate more on the whole face, therefore, we wonder\nwhether them are more robust at classifying partial occluded\nface images. To explore the occlusion robustness of Face\nTransformer models, we apply random occlusion (zero value)\non face images of several test datasets, and test the recognition\nperformance of models as the occlusion area increases. The\nexperimental results are in Figure 4. We ï¬nd the performance\nof Face Transformer models decreases more compared with\nResNet-100, which indicates Face Transformer models behave\nno better than CNNs in occlusion robustness.\nFig. 4. The recognition performance of Face Transformer model and ResNet-\n100 as the occlusion area increases.\n4) Abortive attempts and Observations:In addition to the\nreported models, we would like to share some of our abortive\nattempts and observations. Note that, these observations may\nnot be rigorous enough to come to conclusions, but maybe\nthey are helpful for readers.\n(1) We ï¬rst tried SGD as previous works [9], [10] to train\nFace Transformer models, while models cannot converge. So\nï¬nally we apply AdamW, which has been proved as a effective\noptimizer for Transformer models.\n(2) We tried removing ğ‘¿ğ‘ğ‘™ğ‘ğ‘ ğ‘  Â¹ğ’›0\n0Âº, and used the mean\npooling of other tokens outputs. Compared with using ğ’›0\nğ¿ as\noutput, the recognition performance decreases slightly, while\naccuracy on TALFW database increases to more than 85%.\n(3) We tried removing the MLP to improve the efï¬ciency\nbut ï¬nd the training accuracy cannot increase to a normal\nlevel, which indicates that the MLP block is essential for Face\nTransformer models.\nIV. C ONCLUSION\nIn this paper, we aim to investigate the feasibility of\napplying Transformer models in face recognition. Finally,\nwe have demonstrated that Face Transformer models cannot\nwork with a relatively small database, CASIA-WebFace, while\nthey can obtain promising performance on the large-scale\nface training database, MS-Celeb-1M. In addition, we have\nprovided some analyses for better understanding the Face\nTransformer models.\n5\nREFERENCES\n[1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\nâ€œAn image is worth 16x16 words: Transformers for image recognition\nat scale,â€ arXiv preprint arXiv:2010.11929, 2020.\n[2] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, â€œEnd-to-end object detection with transformers,â€ in\nEuropean Conference on Computer Vision. Springer, 2020, pp. 213â€“\n229.\n[3] L. Zhou, Y . Zhou, J. J. Corso, R. Socher, and C. Xiong, â€œEnd-to-end\ndense video captioning with masked transformer,â€ in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\npp. 8739â€“8748.\n[4] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J Â´egou, â€œTraining data-efï¬cient image transformers & distillation\nthrough attention,â€ arXiv preprint arXiv:2012.12877, 2020.\n[5] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. Tay, J. Feng, and\nS. Yan, â€œTokens-to-token vit: Training vision transformers from scratch\non imagenet,â€ arXiv preprint arXiv:2101.11986, 2021.\n[6] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, â€œTransformer in\ntransformer,â€ arXiv preprint arXiv:2103.00112, 2021.\n[7] Y . Guo, L. Zhang, Y . Hu, X. He, and J. Gao, â€œMs-celeb-1m: A dataset\nand benchmark for large-scale face recognition,â€ inEuropean conference\non computer vision. Springer, 2016, pp. 87â€“102.\n[8] W. Liu, Y . Wen, Z. Yu, M. Li, B. Raj, and L. Song, â€œSphereface: Deep\nhypersphere embedding for face recognition,â€ in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2017, pp.\n212â€“220.\n[9] H. Wang, Y . Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and\nW. Liu, â€œCosface: Large margin cosine loss for deep face recognition,â€\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 5265â€“5274.\n[10] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, â€œArcface: Additive angular\nmargin loss for deep face recognition,â€ in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp.\n4690â€“4699.\n[11] K. Simonyan and A. Zisserman, â€œVery deep convolutional networks for\nlarge-scale image recognition,â€ arXiv preprint arXiv:1409.1556, 2014.\n[12] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image\nrecognition,â€ in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770â€“778.\n[13] Y . Taigman, M. Yang, M. Ranzato, and L. Wolf, â€œDeepface: Closing the\ngap to human-level performance in face veriï¬cation,â€ in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2014,\npp. 1701â€“1708.\n[14] F. Schroff, D. Kalenichenko, and J. Philbin, â€œFacenet: A uniï¬ed embed-\nding for face recognition and clustering,â€ in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2015, pp. 815â€“\n823.\n[15] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV . Vanhoucke, and A. Rabinovich, â€œGoing deeper with convolutions,â€\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2015, pp. 1â€“9.\n[16] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu et al., â€œA survey on visual transformer,â€ arXiv preprint\narXiv:2012.12556, 2020.\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ in Advances\nin Neural Information Processing Systems, 2017, pp. 5998â€“6008.\n[18] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, â€œLabeled\nfaces in the wild: A database for studying face recognition in uncon-\nstrained environments,â€ University of Massachusetts, Amherst, Tech.\nRep. 07-49, October 2007.\n[19] W. Deng, J. Hu, N. Zhang, B. Chen, and J. Guo, â€œFine-grained face\nveriï¬cation: Fglfw database, baselines, and human-dcmn partnership,â€\nPattern Recognition, vol. 66, pp. 63â€“73, 2017.\n[20] T. Zheng, W. Deng, and J. Hu, â€œCross-age LFW: A database for\nstudying cross-age face recognition in unconstrained environments,â€\narXiv:1708.08197, 2017.\n[21] T. Zheng and W. Deng, â€œCross-pose lfw: A database for studying cross-\npose face recognition in unconstrained environments,â€ Beijing University\nof Posts and Telecommunications, Tech. Rep. 18-01, February 2018.\n[22] Y . Zhong and W. Deng, â€œTowards transferable adversarial attack against\ndeep face recognition,â€ IEEE Transactions on Information Forensics and\nSecurity, vol. 16, pp. 1452â€“1466, 2020.\n[23] S. Sengupta, J.-C. Chen, C. Castillo, V . M. Patel, R. Chellappa, and\nD. W. Jacobs, â€œFrontal to proï¬le face veriï¬cation in the wild,â€ in 2016\nIEEE Winter Conference on Applications of Computer Vision (WACV).\nIEEE, 2016, pp. 1â€“9.\n[24] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, and\nS. Zafeiriou, â€œAgedb: the ï¬rst manually collected, in-the-wild age\ndatabase,â€ in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, 2017, pp. 51â€“59.\n[25] B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller, C. Otto,\nA. K. Jain, W. T. Niggel, J. Anderson, J. Cheney et al., â€œIarpa\njanus benchmark-c: Face dataset and protocol,â€ in 2018 International\nConference on Biometrics (ICB). IEEE, 2018, pp. 158â€“165.\n[26] F. Wang, X. Xiang, J. Cheng, and A. L. Yuille, â€œNormface: L2\nhypersphere embedding for face veriï¬cation,â€ in Proceedings of the 25th\nACM international conference on Multimedia. ACM, 2017, pp. 1041â€“\n1049.\n[27] S. Ioffe and C. Szegedy, â€œBatch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,â€ arXiv preprint\narXiv:1502.03167, 2015.\n[28] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, â€œDropout: a simple way to prevent neural networks from overï¬t-\nting,â€ Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929â€“\n1958, 2014.\n[29] I. Loshchilov and F. Hutter, â€œDecoupled weight decay regularization,â€\narXiv preprint arXiv:1711.05101, 2017.\n[30] S. Abnar and W. Zuidema, â€œQuantifying attention ï¬‚ow in transformers,â€\narXiv preprint arXiv:2005.00928, 2020."
}