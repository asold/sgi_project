{
  "title": "A Survey on Neural Network Language Models",
  "url": "https://openalex.org/W2948559998",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2393202745",
      "name": "Jing Kun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231908632",
      "name": "Xu Jungang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2951450659",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2094655846",
    "https://openalex.org/W2091981305",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2476140796",
    "https://openalex.org/W2399981156",
    "https://openalex.org/W2076094076",
    "https://openalex.org/W2251150025",
    "https://openalex.org/W2207587218",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2963077125",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2058373514",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2417736714",
    "https://openalex.org/W2040711288",
    "https://openalex.org/W2026383756",
    "https://openalex.org/W2551884415",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W2005708641",
    "https://openalex.org/W2474824677",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2518756966",
    "https://openalex.org/W2345764277",
    "https://openalex.org/W2157189245",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2152808281",
    "https://openalex.org/W1614298861"
  ],
  "abstract": "As the core component of Natural Language Processing (NLP) system, Language Model (LM) can provide word representation and probability indication of word sequences. Neural Network Language Models (NNLMs) overcome the curse of dimensionality and improve the performance of traditional LMs. A survey on NNLMs is performed in this paper. The structure of classic NNLMs is described firstly, and then some major improvements are introduced and analyzed. We summarize and compare corpora and toolkits of NNLMs. Further, some research directions of NNLMs are discussed.",
  "full_text": "A Survey on Neural Network Language Models\nKun Jing∗ and Jungang Xu∗\nSchool of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing\njingkun18@mails.ucas.ac.cn, xujg@ucas.ac.cn\nAbstract\nAs the core component of Natural Language Pro-\ncessing (NLP) system, Language Model (LM) can\nprovide word representation and probability indi-\ncation of word sequences. Neural Network Lan-\nguage Models (NNLMs) overcome the curse of di-\nmensionality and improve the performance of tra-\nditional LMs. A survey on NNLMs is performed in\nthis paper. The structure of classic NNLMs is de-\nscribed ﬁrstly, and then some major improvements\nare introduced and analyzed. We summarize and\ncompare corpora and toolkits of NNLMs. Further-\nmore, some research directions of NNLMs are dis-\ncussed.\n1 Introduction\nLM is the basis of various NLP tasks. For example, in Ma-\nchine Translation tasks, LM is used to evaluate probabilities\nof outputs of the system to improve ﬂuency of the translation\nin the target language. In Speech Recognition tasks, LM and\nacoustic model are combined to predict the next word.\nEarly NLP systems were based primarily on manually writ-\nten rules, which is time-consuming and laborious and cannot\ncover a variety of linguistic phenomena. In the 1980s, sta-\ntistical LMs were proposed, which assigns probabilities to a\nsequence sof N words, i.e.,\nP(s) =P(w1w2 ···wN)\n=P(w1)P(w2|w1) ···P(wN|w1w2 ···wN−1), (1)\nwhere wi denotes i-th word in the sequence s. The probabil-\nity of a word sequence can be broken into the product of the\nconditional probability of the next word given its predeces-\nsors that are generally called a history of context or context.\nConsidering that it is difﬁcult to learn the extremely many\nparameters of the above model, an approximate method is\nnecessary. N-gram model is an approximation method, which\nwas the most widely used and the state-of-the-art model be-\nfore NNLMs. A (k+1)-gram model is derived from the k-\norder Markov assumption. This assumption illustrates that\nthe current state depends only on the previous kstates, i.e.,\nP(wt|w1 ···wt−1) ≈P(wt|wt−k···wt−1), (2)\n∗K. Jing and J. Xu are corresponding authors.\nwhich are estimated by maximum likelihood estimation.\nPerplexity (PPL) [Jelinek et al., 1977 ], an information-\ntheoretic metric that measures the quality of a probabilistic\nmodel, is a way to evaluate LMs. Lower PPL indicates a bet-\nter model. Given a corpus containingNwords and a language\nmodel LM, the PPL of LM is\n2−1\nN\n∑N\nt=1 log2LM(wt|w1···wt−1). (3)\nIt is noted that PPL is related to corpus. Two or more LMs\ncan be compared on the same corpus in terms of PPL.\nHowever, n-gram LMs have a signiﬁcant drawback. The\nmodel would assign probabilities of 0 to the n-grams that do\nnot appear in the training corpus, which does not match the\nactual situation. Smoothing techniques can solve this prob-\nlem. Its main idea is robbing the rich for the poor, i.e., reduc-\ning the probability of events that appear in the training corpus\nand assigning the probability to events that do not appear.\nAlthough n-gram LMs with smoothing techniques work\nout, there still are other problems. A fundamental problem is\nthe curse of dimensionality, which limits modeling on larger\ncorpora for a universal language model. It is particularly ob-\nvious in the case when one wants to model the joint distri-\nbution in discrete space. For example, if one wants to model\nan n-gram LM with a vocabulary of size 10,000, there are\npotentially 10000n −1 free parameters.\nIn order to solve this problem, Neural Network (NN) is in-\ntroduced for language modeling in continuous space. NNs\nincluding Feedforward Neural Network (FFNN), Recurrent\nNeural Network (RNN), can automatically learn features and\ncontinuous representation. Therefore, NNs are expected to be\napplied to LMs, even other NLP tasks, to cover the discrete-\nness, combination, and sparsity of natural language.\nThe ﬁrst FFNN Language Model (FFNNLM) presented by\n[Bengio et al., 2003 ] ﬁghts the curse of dimensionality by\nlearning a distributed representation for words, which rep-\nresents a word as a low dimensional vector, called embed-\nding. FFNNLM performs better than n-gram LM. Then, RNN\nLanguage Model (RNNLM) [Mikolov et al., 2010] also was\nproposed. Since then, the NNLM has gradually become the\nmainstream LM and has rapidly developed. Long Short-term\nMemory RNN Language Model (LSTM-RNNLM) [Sunder-\nmeyer et al., 2012] was proposed for the difﬁculty of learning\nlong-term dependence. Various improvements were proposed\nfor reducing the cost of training and evaluation and PPL such\narXiv:1906.03591v2  [cs.CL]  13 Jun 2019\nwt−n+1 \nwt−2 \nwt−1 \n…\nH\nU\nW\ntanh\nsoftmax\nP(wt|context) \nC(wt−n+1) \nC(wt−2) \nC(wt−1) \nFigure 1: The FFNNLM proposed by [Bengio et al., 2003]. In this\nmodel, in order to predict the conditional probability of the wordwt,\nits previous n −1 words are projected by the shared projection ma-\ntrix C ∈R|V |×m into a continuous feature vector space according\nto their index in the vocabulary, where|V |is the size of the vocabu-\nlary and m is the dimension of the feature vectors, i.e., the word wi\nis projected as the distributed feature vectorC(wi) ∈Rm. Each row\nof the projection matrix C is a feature vector of a word in the vocab-\nulary. The input x of the FFNN is a concatenation of feature vectors\nof n −1 words. This model is followed by Softmax output layer\nto guarantee all the conditional probabilities of words positive and\nsumming to one. The learning algorithm is the Stochastic Gradient\nDescent (SGD) method using the backpropagation (BP) algorithm.\nas hierarchical Softmax, caching, and so on. Recently, atten-\ntion mechanisms have been introduced to improve NNLMs,\nwhich achieved signiﬁcant performance improvements.\nIn this paper, we concentrate on reviewing the methods and\ntrends of NNLM. Classic NNLMs are described in Section 2.\nDifferent types of improvements are introduced and analyzed\nseparately in Section 3. Corpora and toolkits are described in\nSection 4 and Section 5. Finally, conclusions are given, and\nnew research directions of NNLMs are discussed.\n2 Classic Neural Network Language Models\n2.1 FFNN Language Models\n[Xu and Rudnicky, 2000 ] tried to introduce NNs into LMs.\nAlthough their model performs better than the baseline n-\ngram LM, their model with poor generalization ability cannot\ncapture context-dependent features due to no hidden layer.\nAccording to Formula 1, the goal of LMs is equiv-\nalent to an evaluation of the conditional probability\nP(wk|w1 ···wk−1). But the FFNNs cannot directly process\nvariable-length data and effectively represent the historical\ncontext. Therefore, for sequence modeling tasks like LMs,\nFFNNs have to use ﬁxed-length inputs. Inspired by the n-\ngram LMs (see Formula 2), FFNNLMs consider the previous\nn−1 words as the context for predicting the next word.\n[Bengio et al., 2003] proposed the architecture of the orig-\ninal FFNNLM, as shown in Figure 1. This FFNNLM can be\nexpressed as:\ny= b+ Wx + Utanh(d+ Hx), (4)\nw(t)\ns(t) y(t)\ns(t-1)\nx(t)\nU V\ndelayed\nFigure 2: The RNNLM proposed by [Mikolov et al., 2010; Mikolov\net al., 2011]. The RNN has an internal state that changes with the\ninput on each time step, taking into account all previous contexts.\nThe state st can be derived from the input word vector wt and the\nstate st−1.\nwhere H, U, and W are the weight matrixes that is for the\nconnections between the layers; dand bare the biases of the\nhidden layer and the output layer.\nFFNNLM implements modeling on continuous space by\nlearning a distributed representation for each word. The word\nrepresentation is a by-product of LMs, which is used to im-\nprove other NLP tasks. Based on FFNNLM, two word repre-\nsentation models, CBOW and Skip-gram , were proposed by\n[Mikolov et al., 2013]. FFNNLM overcomes the curse of di-\nmensions by converting words into low-dimensional vectors.\nFFNNLM leads the trend of NNLM research.\nHowever, it still has a few drawbacks. The context size\nspeciﬁed before training is limited, which is quite different\nfrom the fact that people can use lots of context informa-\ntion to make predictions. Words in a sequence are time-\nrelated. FFNNLM does not use timing information for mod-\neling. Moreover, fully connected NN needs to learn many\ntrainable parameters, even though these parameters are less\nthan n-gram LM, which still is expensive and inefﬁcient.\n2.2 RNN Language Models\n[Bengio et al., 2003 ] proposed the idea of using RNN for\nLMs. They claimed that introducing more structure and pa-\nrameter sharing into NNs could capture longer contextual in-\nformation.\nThe ﬁrst RNNLM was proposed by [Mikolov et al., 2010;\nMikolov et al., 2011a]. As shown in Figure 2, at time step t,\nthe RNNLM can be described as:\nxt = [wT\nt ; sT\nt−1]T,\nst = f(Uxt + b),\nyt = g(Vst + d), (5)\nwhere U, W, V are weight matrixes; b, dare the biases of the\nstate layer and the output layer respectively; in [Mikolov et\nal., 2010; Mikolov et al., 2011a], f is the sigmoid function,\nand g is the Softmax function. RNNLMs could be trained\nby the backpropagation through time (BPTT) or the trun-\ncated BPTT algorithm. According to their experiments, the\nRNNLM is signiﬁcantly superior to FFNNLMs and n-gram\nLMs in terms of PPL.\nCompared with FFNNLM, RNNLM has made a break-\nthrough. RNNs have an inherent advantage in the processing\nof sequence data as variable-length inputs could be received.\nWhen the network input window is shifted, duplicate calcu-\nlations are avoided as the internal state. And the changes of\nthe internal state by the input at tstep reveal timing informa-\ntion. Parameter sharing signiﬁcantly reduces the number of\nparameters.\nAlthough RNNLMs could take advantage of all contexts\nfor prediction, it is challenging for training models to learn\nlong-term dependencies. The reason is that the gradients of\nparameters can disappear or explode during RNN training,\nwhich leads to slow training or inﬁnite parameter value.\n2.3 LSTM-RNN Language Models\nLong Short-Term Memory (LSTM) RNN solved this prob-\nlem. [Sundermeyer et al., 2012] introduced LSTM into LM\nand proposed LSTM-RNNLM. Except for the memory unit\nand the part of NN, the architecture of LSTM-RNNLM is\nalmost the same as RNNLM. Three gate structures (includ-\ning input, output, and forget gate) are added to the LSTM\nmemory unit to control the ﬂow of information. The general\narchitecture of LSTM-RNNLM can be formalized as:\nit = σ(Uixt + Wist−1 + Vict−1 + bi),\nft = σ(Ufxt + Wfst−1 + Vfct−1 + bf),\ngt = f(Uxt + Wst−1 + Vct−1 + b),\nct = ft ⊙ct−1 + it ⊙gt,\not = σ(Uoxt + Wost−1 + Voct + bo),\nst = ot ·f(ct),\nyt = g(Vst + Mxt + d), (6)\nwhere it, ft, ot are input gate, forget gate and output gate, re-\nspectively. ct is the internal memory of unit. st is the hidden\nstate unit. Ui, Uf, Uo, U, Wi, Wf, Wo, W, Vi, Vf, Vo, and V\nare all weight matrixes. bi, bf, bo, b, and dare bias. f is the\nactivation function and σis the activation function for gates,\ngenerally a sigmoid function.\nComparing the above three classic LMs, RNNLMs (in-\ncluding LSTM-RNNLM) perform better than FFNNLM, and\nLSTM-RNNLM maintains the state-of-the-art LM. The cur-\nrent NNLMs are based mostly on RNN or LSTM.\nAlthough LSTM-RNNLM performs well, training the\nmodel on a large corpus is very time-consuming because the\ndistributions of predicted words are explicitly normalized by\nthe Softmax layer, which leads to considering all words in\nthe vocabulary when computing the log-likelihood gradients.\nAlso, better performance of LM is expected. To improve\nNNLM, researchers are still exploring different techniques\nthat are similar to how humans process natural language.\n3 Improved Techniques\n3.1 Techniques for Reducing Perplexity\nNew structures and more effective information are introduced\ninto classic NNLMs, especially LSTM-RNNLM, for reduc-\ning PPL. Inspired by linguistics and how humans process\nnatural language, some novel, effective methods, including\ncharacter-aware models, factored models, bidirectional mod-\nels, caching, attention, etc., are proposed.\nCharacter-Aware Models\nIn natural language, some words in the similar form often\nhave the same or similar meaning. For example, man in\nsuperman has the same meaning as the one in policeman.\n[Mikolov et al., 2012] explored RNNLM and FFNNLM at the\ncharacter level. Character-level NNLM can be used for solv-\ning out-of-vocabulary (OOV) word problem, improving the\nmodeling of uncommon and unknown words because char-\nacter features reveal structural similarities between words.\nCharacter-level NNLM also reduces training parameters due\nto the small Softmax layers with character-level outputs.\nHowever, the experimental results showed that it was chal-\nlenging to train highly-accurate character-level NNLMs, and\nits performance is usually worse than the word-level NNLMs.\nThis is because character-level NNLMs have to consider a\nlonger history to predict the next word correctly.\nA variety of solutions that combine character- and word-\nlevel information, generally called character-aware LM, have\nbeen proposed. One approach is to organize character-level\nfeatures word by word and then use them for word-level\nLMs. [Kim et al., 2015] proposed Convolutional Neural Net-\nwork (CNN) for extracting character-level feature and LSTM\nfor receiving these character-level features of the word in a\ntime step. [Hwang and Sung, 2016 ] solved the problem of\ncharacter-level NNLMs using a hierarchical RNN architec-\nture consisting of multiple modules with different time scales.\nAnother solution is to input character- and word-level features\ninto NNLM simultaneously. [Miyamoto and Cho, 2016] sug-\ngested interpolating word feature vectors with character fea-\nture vectors extracted from words by BiLSTM and inputting\ninterpolation vectors into LSTM. [Verwimp et al., 2017] pro-\nposed a character-word LSTM-RNNLM that directly con-\ncatenated character- and word-level feature vectors and in-\nput concatenations into the network. Character-aware LM\ndirectly uses character-level LM as character feature extrac-\ntor for word-level LM. Therefore, the LM has rich character-\nword information for prediction.\nFactored Models\nNNLMs deﬁne the similarity of words based on tokens. How-\never, similarity can also be derived from word shape features\n(afﬁxes, uppercase, hyphens, etc.) or other annotations (such\nas POS). Inspired by factored LMs,[Alexandrescu and Kirch-\nhoff, 2006] proposed a factored NNLMs, a novel neural prob-\nabilistic LM that can learn the mapping from words and spe-\nciﬁc features of the words to continuous spaces.\nMany studies have explored the selection of factors. Dif-\nferent linguistic features are considered ﬁrst. [Wu et al.,\n2012] introduced morphological, grammatical, and seman-\ntic features to extend RNNLMs. [Adel et al., 2015] also ex-\nplored the effects of other factors, such as part-of-speech tags,\nBrown word clusters, open class words, and clusters of open\nclass word embeddings. Experimental results showed that\nBrown word clusters, part-of-speech tags, and open words\nare most effective for a Mandarin-English Code-Switching\ntask. Contextual information also was explored. For ex-\nample, [Mikolov and Zweig, 2012 ] used the distribution of\ntopics calculated from ﬁxed-length blocks of previous words.\n[Wang and Cho, 2015 ] proposed a new approach to incor-\nporating corpus bag-of-words (BoW) context into language\nmodeling. Besides, some methods based on text-independent\nfactors were proposed. [Ahn et al., 2016] proposed a Neural\nKnowledge Language Model that applied the notation knowl-\nedge provided by knowledge graph for RNNLMs.\nThe factored model allows the model to summarize word\nclasses with same characteristics. Applying factors other than\nword tokens to the neural network training can better learn the\ncontinuous representation of words, represent OOV words,\nand reduce the PPL of LMs. However, the selection of dif-\nferent factors is related to different upstream NLP tasks, ap-\nplications, etc. of LM. And there is no way to select factors\nin addition to experimenting with various factors separately.\nTherefore, for a speciﬁc task, an efﬁcient selection method of\nfactors is necessary. Meanwhile, corpora with factor labels\nhave to be established.\nBidirectional Models\nTraditional unidirectional NNs only predict the outputs from\npast inputs. A bidirectional NN can be established, which is\nconditional on future data. [Graves et al., 2013; Bahdanau\net al., 2014] introduced bidirectional RNN and LSTM neural\nnetworks (BiRNN and BiLSTM) into speech recognition or\nother NLP tasks. The BiRNNs utilize past and future con-\ntexts by processing the input data in both directions. One\nof the most popular works of the bidirectional model is the\nELMo model [Peters et al., 2018], a new deep contextualized\nword representation based on BiLSTM-RNNLMs. The vec-\ntors of the embedding layer of a pre-trained ELMo model is\nthe learned representation vector of words in the vocabulary.\nThese representations are added as the embedding layer of\nthe existing model and signiﬁcantly improve state of the art\nacross six challenging NLP tasks.\nAlthough BiLM using past and future contexts has\nachieved improvements, it is noted that BiLM cannot be used\ndirectly for LM because LM is deﬁned in the previous con-\ntext. BiLMs can be used for other NLP tasks, such as machine\ntranslation, speech recognition because the word sequence is\nregarded as a simultaneous input sequence.\nCaching\nWords that have appeared recently may appear again. Due\nto this assumption, the cache mechanism initially is used to\noptimize n-gram LM, overcoming the length limit of depen-\ndencies. It matches a new input and histories in the cache.\nCache mechanism was originally proposed for reducing PPL\nof NNLMs. [Soutner et al., 2012 ] attempted to combine\nFFNNLM with the cache mechanism and proposed a struc-\nture of the cache-based NNLMs, which leads to discrete prob-\nability change. To solve this problem, [Grave et al., 2016 ]\nproposed a continuous cache model, where the change de-\npends on the inner product of the hidden representations.\nAnother type of cache mechanism is that cache is used as\na speed-up technique for NNLMs. The main idea of this\nmethod is to store the outputs and states of LMs in a hash\ntable for future prediction given the same contextual history.\nFor example, [Huang et al., 2014] proposed the use of four\ncaches to accelerate model reasoning. The caches are respec-\ntively Query to Language Model Probability Cache, History\nto Hidden State Vector Cache, History to Class Normalization\nFactor Cache, and History and Class Id to Sub-vocabulary\nNormalization Factor Cache.\nThe caching technique is proved that it can reduce compu-\ntational complexity and improve the learning ability of long-\nterm dependence due to its caches. It can be applied ﬂexibly\nto existing models. However, if the size of the cache is lim-\nited, cache-based NNLM will not perform well.\nAttention\nRNNLMs predict the next word with its context. Not every\nword in the context is related to the next word and effective\nfor prediction. Similar to human beings, LM with the atten-\ntion mechanism uses the long history efﬁciently by select-\ning useful word representations from them. [Bahdanau et al.,\n2014] ﬁrst proposed the application of the attention mecha-\nnism to NLP tasks (machine translation in this paper). [Tran\net al., 2016; Mei et al., 2016] proved that the attention mech-\nanism could improve the performance of RNNLMs.\nThe attention mechanism obtains the target areas that need\nto be focused on by a set of attention coefﬁcients for each in-\nput. The attention vector ztis calculated by the representation\n{r0,r1,··· ,rt−1}of tokens:\nzt =\nt−1∑\ni=0\nαtiri. (7)\nThis attention coefﬁcient αti is normalized by the score eti\nby the Softmax function, where\neti = a(ht−1,ri), (8)\nis an alignment model that evaluates how well the representa-\ntion riof a token and the hidden stateht−1 match. This atten-\ntion vector is a good representation of the history of context\nfor prediction.\nThe attention mechanism has been widely used in Com-\nputer Vision (CV) and NLP. Many improved attention mech-\nanisms have been proposed, including soft/hard attention,\nglobal/local attention, key-value/key-value-predict attention,\nmulti-dimensional attention, directed self-attention, self-\nattention, multi-headed attention, and so on. These improved\nmethods are used in LMs and have improved the quality of\nLMs.\nOn the basis of the above improved attention mechanisms,\nsome competitive LMs or methods of word representation are\nproposed. Transformer proposed by [Vaswani et al., 2017 ]\nis the basis for the development of the subsequent models.\nTransformer is a novel structure based entirely on the atten-\ntion mechanism, which consists of an encoder and a decoder.\nSince then, GPT [Radford et al., 2018 ] and BERT [Devlin\net al., 2018 ] have been proposed. The main difference is\nthat GPT uses Transformer’s decoder, and BERT uses Trans-\nformer’s encoder. BERT is an attention-based bidirectional\nmodel. Unlike CBOW, Skip-gram, and ELMo, GPT and\nBERT represent words through the parameters of the entire\nmodel. They are state-of-the-art methods of word representa-\ntion in NLP.\nAttention mechanism with its applications for various tasks\nis one of the most popular research directions. Although vari-\nous structures of attention mechanism have been proposed, as\nthe core of attention mechanism, the methods for calculating\nthe similarity between words have still not been improved.\nProposing some novel methods for vector similarity play an\nimportant role in improving attention mechanism.\n3.2 Speed-up Techniques on Large Corpora\nTraining the model on a corpus with a large vocabulary is\nvery time-consuming. The main reason is the Softmax layer\nfor large vocabulary. Many approaches have been proposed\nto address the difﬁculty of training deep NNs with large out-\nput spaces. In general, they can be divided into four cat-\negories, i.e., hierarchical Softmax, sampling-based approx-\nimations, self-normalization, and exact gradient on limited\nloss functions. Among them, the former two are used widely\nin NNLMs.\nHierarchical Softmax\nSome methods based on hierarchical Softmax that decompose\ntarget conditional probability into multiples of some condi-\ntional probabilities are proposed. [Morin and Bengio, 2005 ]\nused a hierarchical binary tree (by the similarity from Word-\nnet) of an output layer, in which the V words in vocabulary\nare regarded as its leaves. This technique allows exponen-\ntially fast calculations of word probabilities and their gradi-\nents. However, it performs much worse than non-hierarchical\none despite using expert knowledge. [Mnih and Hinton,\n2008] improved it by a simple feature-based algorithm for au-\ntomatically building word trees from data. The performance\nof the above two models is mostly dependent on the tree,\nwhich is usually heuristically constructed.\nBy relaxing the constraints of the binary tree structure, [Le\net al., 2013 ] introduced a new, general, better class-based\nNNLM with a structured output layer, called Structured Out-\nput Layer NNLM. Given a history hof a word wi, the condi-\ntional probability can be formed as:\nP(wt|h) =P(c1(wt)|h)\nD∏\nd=2\nP(cd(wt)|h,c1,··· ,cd−1).\n(9)\nSince then, many scholars have improved this model. Hi-\nerarchical models based on word frequency classiﬁcation\n[Mikolov et al., 2011a] and Brown clustering [Si et al., 2012]\nwere proposed. It was proved that the model with Brown\nclustering performed better. [Zweig and Makarychev, 2013]\nproposed a speed optimal classiﬁcation, i.e., a dynamic pro-\ngramming algorithm that determines the classes by minimiz-\ning the running time of the model.\nHierarchical Softmax signiﬁcantly reduces model parame-\nters without increasing PPL. The reason is that the technique\nuses d+1 Softmax layers with d+1\n√\n|V|classes or log2|V|two\nclassiﬁcation instead of one Softmax layers with |V|classes.\nNevertheless, hierarchical Softmax leads NNLMs to perform\nworse. Hierarchical Softmax based on Brown clustering is a\nspecial case. At the same time, the existing class-based meth-\nods do not consider the context. The classiﬁcation is hard\nclassiﬁcation, which is a key factor of the increase of PPL.\nTherefore, it is necessary to study a method based on soft\nclassiﬁcation. It is our hope that while reducing the cost of\nNNLM training, PPL will remain unchanged, even decrease.\nSampling-based Approximations\nWhen the NNLMs calculate the conditional probability of the\nnext word, the output layer uses the Softmax function, where\nthe cost of calculation of the normalized denominator is ex-\ntremely expensive. Therefore, one approach is to randomly or\nheuristically select a small portion of the output and estimate\nthe probability and the gradient from the samples.\nInspired by Minimizing Contrastive Divergence, [Bengio\nand Senecal, 2003] proposed an importance sampling method\nand an adaptive importance sampling algorithm to accelerate\nthe training of NNLMs. The gradient of the log-likelihood\ncan be expressed as:\n∂logP(wt|wt−1\n1 )\n∂θ = −∂y(wt,wt−1\n1 )\n∂θ\n+\nk∑\ni=1\nP(vi|wt−1\n1 )∂y(vi,wt−1\n1 )\n∂θ . (10)\nThe weighted sum of the negative terms is obtained by im-\nportance sample estimates, i.e., sampling with Q instead of\nP. Therefore, the estimates of the normalized denominator\nand the log-likelihood gradient are respectively:\nˆZ(ht) = 1\nN\n∑\nw′Q(·|ht)\ne−y(w′,ht)\nQ(w′|ht) , (11)\nE[∂logP(wt|wt−1\n1 )\n∂θ ] =−∂y(wt,wt−1\n1 )\n∂θ\n+\n∑\nw′∈Γ\n∂y(w′,wt−1\n1 )\n∂θ e−y(w′,wt−1\n1 )/Q(w′|wt−1\n1 )\ne−y(w′,wt−1\n1 )/Q(w′|wt−1\n1 )\n. (12)\nExperimental results showed that adopting importance sam-\npling leads to ten times faster the training of NNLMs with-\nout signiﬁcantly increasing PPL. [Bengio and Senecal, 2008]\nproposed an adaptive importance sampling method using an\nadaptive n-gram model instead of the simple unigram model\nin [Bengio and Senecal, 2003 ]. Other improvements have\nbeen proposed, such as parallel training of small models to\nestimate loss for importance sampling, multiple importance\nsampling and likelihood weighting scheme, two-stage sam-\npling, and so on. In addition, there are other different sam-\npling methods for the training of NNLMs, including noise\ncomparison estimation, Locality Sensitive Hashing (LSH)\ntechniques, BlackOut.\nThese methods have signiﬁcantly accelerated the training\nof NNLMs by sampling, while the model evaluation remains\ncomputationally challenging. At present, the computational\nCorpus Train Valid Test V ocab\nBrown 800,000 200,000 181,041 47,578\nPenn Treebank 930,000 74,000 82,000 10,000\nWikiText-2 2000,000 220,000 250,000 33,000\nTable 1: Size (words) of small corpora\ncomplexity of the model with sampling-based approxima-\ntions still is high. Sampling strategy is relatively simple. Ex-\ncept for LSH techniques, other strategies are to select at ran-\ndom or heuristically.\n4 Corpora\nAs mentioned above, it is necessary to evaluate all LMs on the\nsame corpus, but which is impractical. This section describes\nsome of the common corpora.\nIn general, in order to reduce the cost of training and test,\nthe feasibility of models needs to be veriﬁed on the small cor-\npus ﬁrst. Common small corpora include Brown, Penn Tree-\nbank, and WikiText-2 (see Table 1).\nAfter the model structure is determined, it needs to be\ntrained and evaluated in a large corpus to prove that the model\nhas a reliable generalization. Common large corpora that are\nupdated over time from website, newspaper, etc. include Wall\nStreet Journal, Wikipedia, News Commentary, News Crawl,\nCommon Crawl, Associated Press (AP) News, and more.\nHowever, LMs are often trained on different large corpora.\nEven on the same corpus, various preprocessing methods and\ndifferent divisions of training/test set affect the results. At the\nsame time, training time is reported in different ways or is not\ngiven in some papers. The experimental results in different\npapers do not be compared fully.\n5 Toolkits\nTraditional LM toolkits mainly includes CMU-Cambridge\nSLM, SRILM, IRSTLM, MITLM, BerkeleyLM, which only\nsupport the training and evaluation of n-gram LMs with a va-\nriety of smoothing. With the development of Deep Learn-\ning, many toolkits based on NNLMs are proposed. [Mikolov\net al., 2011b] built the RNNLM toolkit, which supports the\ntraining of RNNLMs to optimize speech recognition and ma-\nchine translation, but it does not support parallel training al-\ngorithms and GPU. [Schwenk, 2013] constructed the neural\nnetwork open source tool CSLM (Continuous Space Lan-\nguage Modeling) to support the training and evaluation of\nFFNNs. [Enarvi and Kurimo, 2016 ] proposed the scalable\nneural network model toolkit TheanoLM, which trains LMs\nto score sentences and generate text.\nAccording to our survey, we found that there is no toolkit\nsupporting both the traditional N-gram LM and NNLM. And\nthey generally do not contain commonly used LM loads.\n6 Future Directions\nMost NNLMs are based on three classic NNLMs. LSTM-\nRNNLMs is the state-of-the-art LM. There are two directions\nof improving LMs, i.e., reducing PPL using a novel structure\nor an additional knowledge and accelerating the training and\nevaluation by estimate conditional probability.\nDuring the survey, some existing problems in NNLMs are\nfound and summarized. Therefore, we propose the future di-\nrection of LMs. Firstly, the methods that reduce the cost and\nthe number of parameters would continue to be explored to\nspeed up the training and evaluation without PPL increasing.\nThen, a novel structure to simulate the way humans work is\nexpected for improving the performance of LM. For exam-\nple, building a generative model, such as GAN, for LMs may\nbe a new direction. Last but not least, the current evaluation\nsystem of LMs is not standardized. It is necessary to build\nan evaluation benchmark for unifying the preprocessing and\nwhat results should be reported in papers.\n7 Conclusion\nThe study of NNLMs has been going on for nearly two\ndecades. NNLMs have made timely and signiﬁcant contribu-\ntions to NLP tasks. The different architectures of the classic\nNNLMs and their improvement are surveyed. Their related\ncorpora and toolkits that are essential for the study of NNLMs\nare also introduced.\nReferences\n[Adel et al., 2015] H. Adel, N. T. Vu, K. Kirchhoff,\nD. Telaar, and T. Schultz. Syntactic and semantic features\nfor code-switching factored language models. IEEE/ACM\nTrans. Audio, Speech & Language Processing, 23(3):431–\n440, 2015.\n[Ahn et al., 2016] S. Ahn, H. Choi, T. P¨arnamaa, and Y . Ben-\ngio. A neural knowledge language model. CoRR,\nabs/1608.00318, 2016.\n[Alexandrescu and Kirchhoff, 2006] A. Alexandrescu and\nK. Kirchhoff. Factored neural language models. In HLT-\nNAACL, 2006.\n[Bahdanau et al., 2014] D. Bahdanau, K. Cho, and Y . Ben-\ngio. Neural machine translation by jointly learning to align\nand translate. CoRR, abs/1409.0473, 2014.\n[Bengio and Senecal, 2003] Y . Bengio and J. Senecal. Quick\ntraining of probabilistic neural nets by importance sam-\npling. In AISTATS, 2003.\n[Bengio and Senecal, 2008] Y . Bengio and J. Senecal. Adap-\ntive importance sampling to accelerate training of a neural\nprobabilistic language model. IEEE Trans. Neural Net-\nworks, 19(4):713–722, 2008.\n[Bengio et al., 2003] Y . Bengio, R. Ducharme, P. Vincent,\nand C. Janvin. A neural probabilistic language model.\nJMLR, 3:1137–1155, 2003.\n[Devlin et al., 2018] J. Devlin, M. Chang, K. Lee, and\nK. Toutanova. BERT: pre-training of deep bidirec-\ntional transformers for language understanding. CoRR,\nabs/1810.04805, 2018.\n[Enarvi and Kurimo, 2016] S. Enarvi and M. Kurimo.\nTheanolm - an extensible toolkit for neural network\nlanguage modeling. In Interspeech, pages 3052–3056,\n2016.\n[Grave et al., 2016] E. Grave, A. Joulin, and N. Usunier. Im-\nproving neural language models with a continuous cache.\nCoRR, abs/1612.04426, 2016.\n[Graves et al., 2013] A. Graves, N. Jaitly, and A. Mohamed.\nHybrid speech recognition with deep bidirectional LSTM.\nIn IEEE Workshop on ASRU, pages 273–278, 2013.\n[Huang et al., 2014] Z. Huang, G. Zweig, and B. Dumoulin.\nCache based recurrent neural network language model in-\nference for ﬁrst pass speech recognition. In IEEE ICASSP,\npages 6354–6358, 2014.\n[Hwang and Sung, 2016] K. Hwang and W. Sung.\nCharacter-level language modeling with hierarchical\nrecurrent neural networks. CoRR, abs/1609.03777, 2016.\n[Jelinek et al., 1977] F. Jelinek, R. L. Mercer, L. R. Bahl, and\nJ. K. Baker. Perplexity—a measure of the difﬁculty of\nspeech recognition tasks. JASA, 62(S1):S63–S63, Decem-\nber 1977.\n[Kim et al., 2015] Y . Kim, Y . Jernite, D. Sontag, and A. M.\nRush. Character-aware neural language models. CoRR,\nabs/1508.06615, 2015.\n[Le et al., 2013] H. S. Le, I. Oparin, A. Allauzen, J. Gauvain,\nand F. Yvon. Structured output layer neural network lan-\nguage models for speech recognition. IEEE Trans. Audio,\nSpeech & Language Processing, 21(1):195–204, 2013.\n[Mei et al., 2016] H. Mei, M. Bansal, and M. R. Walter.\nCoherent dialogue with attention-based language models.\nCoRR, abs/1611.06997, 2016.\n[Mikolov and Zweig, 2012] T. Mikolov and G. Zweig. Con-\ntext dependent recurrent neural network language model.\nIn IEEE SLT, pages 234–239, 2012.\n[Mikolov et al., 2010] T. Mikolov, M. Karaﬁ ´at, L. Burget,\nJ. Cernock´y, and S. Khudanpur. Recurrent neural network\nbased language model. In INTERSPEECH, pages 1045–\n1048, 2010.\n[Mikolov et al., 2011a] T. Mikolov, S. Kombrink, L. Burget,\nJ. Cernock ´y, and S. Khudanpur. Extensions of recurrent\nneural network language model. InProc. of IEEE ICASSP,\npages 5528–5531, 2011.\n[Mikolov et al., 2011b] T. Mikolov, S. Kombrink, A. Deo-\nras, and L. Burget. RNNLM - Recurrent Neural Network\nLanguage Modeling Toolkit. InIEEE ASRU, page 4, 2011.\n[Mikolov et al., 2012] T. Mikolov, I. Sutskever, A. Deoras,\nH. Le, and S. Kombrink. Subword Language Modeling\nWith Neural Networks. 2012.\n[Mikolov et al., 2013] T. Mikolov, K. Chen, G. Corrado, and\nJ. Dean. Efﬁcient estimation of word representations in\nvector space. CoRR, abs/1301.3781, 2013.\n[Miyamoto and Cho, 2016] Y . Miyamoto and Ky. Cho.\nGated word-character recurrent language model. In Proc.\nof EMNLP, pages 1992–1997, 2016.\n[Mnih and Hinton, 2008] A. Mnih and G. E. Hinton. A scal-\nable hierarchical distributed language model. In Proc. of\nNIPS, pages 1081–1088, 2008.\n[Morin and Bengio, 2005] F. Morin and Y . Bengio. Hierar-\nchical probabilistic neural network language model. In\nProc. of AISTATS, 2005.\n[Peters et al., 2018] M. E. Peters, M. Neumann, M. Iyyer,\nM. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep\ncontextualized word representations. In Proc. of NAACL-\nHLT Volume 1 (Long Papers), pages 2227–2237, 2018.\n[Radford et al., 2018] A. Radford, K. Narasimhan, T. Sali-\nmans, and I. Sutskever. Improving Language Understand-\ning by Generative Pre-Training. 2018.\n[Schwenk, 2013] H. Schwenk. CSLM - a modular open-\nsource continuous space language modeling toolkit. In\nINTERSPEECH, ISCA, pages 1198–1202, 2013.\n[Si et al., 2012] Y . Si, Y . Guo, Y . Liu, J. Pan, and Y . Yan. Im-\npact of Word Classing on Recurrent Neural Network Lan-\nguage Model. In GCIS, pages 100–103, November 2012.\n[Soutner et al., 2012] D. Soutner, Z. Loose, L. M ¨uller, and\nA. Praz´ak. Neural network language model with cache. In\nTSD, pages 528–534. 2012.\n[Sundermeyer et al., 2012] M. Sundermeyer, R. Schl ¨uter,\nand H. Ney. LSTM neural networks for language mod-\neling. In INTERSPEECH, pages 194–197, 2012.\n[Tran et al., 2016] K. M. Tran, A. Bisazza, and C. Monz.\nRecurrent memory networks for language modeling. In\nNAACL HLT, pages 321–331, 2016.\n[Vaswani et al., 2017] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In NIPS, pages 6000–\n6010, 2017.\n[Verwimp et al., 2017] L. Verwimp, J. Pelemans, H. Van\nhamme, and P. Wambacq. Character-word LSTM lan-\nguage models. In Proc. of EACL Volume 1: Long Papers,\npages 417–427, 2017.\n[Wang and Cho, 2015] T. Wang and K. Cho. Larger-context\nlanguage modelling. CoRR, abs/1511.03729, 2015.\n[Wu et al., 2012] Y . Wu, X. Lu, H. Yamamoto, S. Matsuda,\nC. Hori, and H. Kashioka. Factored language model based\non recurrent neural network. In COLING, Proc. of the\nConference: Technical Papers, pages 2835–2850, 2012.\n[Xu and Rudnicky, 2000] W. Xu and A. Rudnicky. Can arti-\nﬁcial neural networks learn language models? In ICSLP /\nINTERSPEECH, pages 202–205, 2000.\n[Zweig and Makarychev, 2013] G. Zweig and\nK. Makarychev. Speed regularization and optimality\nin word classing. In IEEE ICASSP, pages 8237–8241,\n2013.",
  "topic": "Curse of dimensionality",
  "concepts": [
    {
      "name": "Curse of dimensionality",
      "score": 0.7594439387321472
    },
    {
      "name": "Computer science",
      "score": 0.7490649819374084
    },
    {
      "name": "Artificial neural network",
      "score": 0.6765105724334717
    },
    {
      "name": "Word (group theory)",
      "score": 0.6434798240661621
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6069216728210449
    },
    {
      "name": "Curse",
      "score": 0.599905252456665
    },
    {
      "name": "Representation (politics)",
      "score": 0.5654245615005493
    },
    {
      "name": "Natural language processing",
      "score": 0.5451393127441406
    },
    {
      "name": "Language model",
      "score": 0.5403630137443542
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.5081677436828613
    },
    {
      "name": "Cache language model",
      "score": 0.4294125437736511
    },
    {
      "name": "Natural language",
      "score": 0.3979487121105194
    },
    {
      "name": "Universal Networking Language",
      "score": 0.24231979250907898
    },
    {
      "name": "Linguistics",
      "score": 0.13648533821105957
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Comprehension approach",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 44
}