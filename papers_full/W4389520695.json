{
  "title": "Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation",
  "url": "https://openalex.org/W4389520695",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100741607",
      "name": "Hailin Chen",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5070206012",
      "name": "Amrita Saha",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5074834854",
      "name": "Steven C. H. Hoi",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5005443526",
      "name": "Shafiq Joty",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389524372",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4385572345",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4379137333",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W4365205411",
    "https://openalex.org/W2234664045",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4308014717",
    "https://openalex.org/W4378771755",
    "https://openalex.org/W4319453300",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3093893125"
  ],
  "abstract": "With the rise of powerful closed-sourced LLMs (ChatGPT, GPT-4), there are increasing interests in distilling the capabilies of close-sourced LLMs to smaller open-sourced LLMs. Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn. However, such standard distillation approach neglects the merits and conditions of the student model. Inspired by modern teaching principles, we design a personalised distillation process, in which the student attempts to solve a task first, then the teacher provides an adaptive refinement for the student to improve. Instead of feeding the student with teacher’s prior, personalised distillation enables personalised learning for the student model, as it only learns on examples it makes mistakes upon and learns to improve its own solution. On code generation, personalised distillation consistently outperforms standard distillation with only one third of the data. With only 2.5-3K personalised examples that incur a data-collection cost of 4-6$, we boost CodeGen-mono-16B by 7% to achieve 36.4% pass@1 and StarCoder by 12.2% to achieve 45.8% pass@1 on HumanEval.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6737–6749\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nPersonalised Distillation: Empowering Open-Sourced LLMs with Adaptive\nLearning for Code Generation\nHailin Chen∗♣♠, Amrita Saha∗♠, Steven HOI♠, Shafiq Joty♣♠\n♣Nanyang Technological University, Singapore\n♠Salesforce Research\n{hailin001, srjoty}@ntu.edu.sg\n{amrita.saha, shoi}@salesforce.com\nAbstract\nWith the rise of powerful closed-sourced LLMs\n(ChatGPT, GPT-4), there are increasing inter-\nests in distilling the capabilies of close-sourced\nLLMs to smaller open-sourced LLMs. Previ-\nous distillation methods usually prompt Chat-\nGPT to generate a set of instructions and an-\nswers, for the student model to learn. How-\never, such standard distillation approach ne-\nglects the merits and conditions of the student\nmodel. Inspired by modern teaching principles,\nwe design a personalised distillation process,\nin which the student attempts to solve a task\nfirst, then the teacher provides an adaptive re-\nfinement for the student to improve. Instead of\nfeeding the student with teacher’s prior, person-\nalised distillation enables personalised learning\nfor the student model, as it only learns on exam-\nples it makes mistakes upon and learns to im-\nprove its own solution. On code generation, per-\nsonalised distillation consistently outperforms\nstandard distillation with only one third of the\ndata. With only 2.5-3K personalised examples\nthat incur a data-collection cost of 4-6$, we\nboost CodeGen-mono-16B by 7% to achieve\n36.4% pass@1 and StarCoder by 12.2% to\nachieve 45.8% pass@1 on HumanEval.1\n1 Introduction\nRecently, powerful close-sourced large langauge\nmodels (LLMs) including ChatGPT, GPT-4 have\nbecome predominant, accumulating over 170 mil-\nlion users within 5 month of its launch. Such close-\nsourced LLMs demonstrate strong performance in\na wide range of tasks, from improving writing pro-\nficiency to code generation. However, due to their\nclosed-source nature, concerns have been raised\nregarding factors such as the availability of these\nservices, high associated costs, concerns on ethics\nand safety, and potential data privacy implications,\n*These authors contributed equally to this work\n1Our codes will be available at https://github.\ncom/salesforce/PersDistill\nall of which limit their seamless integration into\nreal-world applications. In light of these concerns,\na natural question arises: Can we distill the remark-\nable abilities exhibited by closed-source LLMs into\nsmaller open-source LLMs?\nResearchers have explored such distillation idea\n(Taori et al., 2023; Wang et al., 2022; Xu et al.,\n2023b), by querying ChatGPT to generate task\ninstruction and solution pairs, and using the col-\nlected data to finetune a student model. However,\nthis standard distillation approach fits different stu-\ndent models to the same data distribution (teacher’s\nprior), disregarding their unique abilities and ca-\npacity. In education domain, personalised learn-\ning which provides customized learning experience\nthat adapts to student’s learning progress and capac-\nity, has proven highly effective and widely adopted\n(Roberts-Mahoney et al., 2016; Shemshack and\nSpector, 2020). Inspired by such finding, we hy-\npothesize that personalised learning is also benefi-\ncial for model distillation.\nIn this work, we propose personalised distilla-\ntion and empirically evaluate its effectiveness in\nthe domain of code generation. Similar to standard\ndistillation, we first employ ChatGPT to generate\ntask instructions accompanied by unit test cases.\nThen we follow three steps for personalized distil-\nlation as shown in Figure 1. First, we let the student\nmodel attempt to solve the task. Then, we evaluate\nthe student’s attempt with unit test cases and get\nexecution feedback. If the execution feedback con-\ntains errors, in the final step we prompt the teacher\nmodel (ChatGPT) to refine the student’s attempt.\nSuch data collection process makes the learning\nexperience both interactive — as the student partic-\nipates to make attempts, and personalised — both\nthe input (tasks) and output (refinement data) are\ncustomised to the student. Essentially, personalised\nlabeled data help the student to refine its own policy,\nrather than adopting a new prior of the teacher.\nWith the personalized code data as target out-\n6737\nFigure 1: Overview of our framework. Left: standard distillation . 1 Teacher generates standard answer to a\ngiven problem for the student to learnRight: personalised distillation. 1 Student first generates its own attempt to\nsolve the task. 2 Executor evaluates generated code with unit test cases. 3 Teacher provides adaptive refinement\ngiven student’s attempt and its execution feedback.\nput, we construct three variants of finetuning data\n(i) PERsD data which formats it as a typical text-\nto-code generation task, (ii) PERsD-refine which\ntreats it as a code-refinement task, given a task in-\nstruction, incorrect code and execution error feed-\nback (ii) PERsD-combine which simply combines\nPERsD and PERsD-refine finetuning data, i.e. code\ngeneration and refinement tasks.\nWe collect 10K standard distillation examples\nand around 2.5-3K personalised examples for pre-\ntraining. Through zero-shot evaluation on Hu-\nmanEval (Chen et al., 2021) and MBPP (Austin\net al., 2021), we observe that all PERsD variants\nconsistently outperform their counterparts which\nuse standard distillation. This compelling result\nstrongly validates our hypothesis regarding the\nadvantages of personalized distillation. Ablation\nstudies further reinforce our hypothesis, uncover-\ning intriguing properties such as the benefits of\nmulti-round personalized distillation and the ability\nof our models to leverage execution feedback for\nself-correction. Notably, personalised distillation\nboosts the state-of-the-art open-sourced pretrain\nmodel StarCoder (Li et al., 2023a) significantly —\nby 12.2% to achieve 45.8 in pass@1 and 82.3 in\npass@100 on HumanEval.\n2 Related Work\n2.1 Distillation from ChatGPT\nPrevious works have explored distillation from\nChatGPT including Alpaca(Taori et al., 2023), Vi-\ncuna(Chiang et al., 2023) and Baize(Xu et al.,\n2023b). However, these works can all be consid-\nered as standard distillation as they do not consider\nthe conditions and capacity of student model. Wiz-\nMethods Personalised Interactive Code-related\nAlpaca ✗ ✗ ✗\nVicuna ✗ ✗ ✗\nBaize ✗ ✗ ✗\nWizardLM✗ ✗ ✗\nWizardCoder✗ ✗ ✓\nLion Input ✓ ✗\nPERsD Input + Output ✓ ✓\nTable 1: Related work on distillation from ChatGPT\nardLM(Xu et al., 2023a) and WizardCoder(Luo\net al., 2023) iteratively prompts teacher model to\ngenerate more complex instructions. Their ap-\nproach can be seen as an orthogonal advancement\nthat can potentially be combined with personalised\ndistillation.\nLion (Jiang et al., 2023) proposes to incorpo-\nrate student model’s answer and sample more hard\ntasks for which the student failed to solve. Thus,\nLion can be considered as input personalised dis-\ntillation as only the input tasks are customised for\ndifferent student. Our approach differs as we pro-\nvide customization both on input and output, and\nwe empirically show that personalising labels is\ncritically beneficial.\n2.2 Code Generation with Feedback\nRecently, there has been an increasing amount\nof research on exploring on how to use feed-\nback for an iterative and improved code genera-\ntion through code-refinement. Self-refine(Madaan\net al., 2023), Self-debug(Chen et al., 2023b) and\nReflexion (Shinn et al., 2023) are inference-time\nmethods which use powerful close-sourced LLMs\nto generate better code from internal or external\nfeedback. Although they show high performance,\n6738\nMethods Training Inference\nSingle Data Source Personalised w/ execution w/o\nModel feedback ChatGPT\nSelf-refine✓ No Training ✗ ✗ ✗\nSelf-debug✓ No Training ✗ ✓ ✗\nReflexion ✓ No Training ✗ ✓ ✗\nSelf-edit ✗ Standard GT ✗ ✓ ✓\nSelf-correct✗ Self-exploration✓ ✓ ✓\nILF ✗ Human labeled✓ ✓ ✓\nPERsD-refine✓ ChatGPT ✓ ✓ ✓\nTable 2: Related work on Code Generation w/ feedback\nthese methods are limited as they require access to\nclose-sourced LLMs.\nSelf-edit (Zhang et al., 2023) trains a separate code\neditor to rectify generated code from a base LLM.\nThe training label is from original gold answer,\nthus not label-personalised. Similarly, Self-correct\n(Welleck et al., 2022) trains a separate corrector\nmodel to rectify the output from a fixed genera-\ntor model. However, the training label is from\nself-exploration of the corrector model: sampling\nmultiple refinements and choosing the one leading\nto higher reward. Finally, ILF (Chen et al., 2023a)\ncollects human-annotated code refinement data to\ntrain a separate refinement model on it. Fhe refine-\nment model is used to generate text-to-code data\nfor finetuning the code-generation LLM. Unlike\nILF, our approach is more scalable as we do not re-\nquire human annotation and our personalized data\nproves significantly more effective than ILF as we\nempirically investigate in §5.\n2.3 Reinforcement Learning from (Human)\nFeedback\nAfter the launch of ChatGPT, aligning LLMs to\nhuman preference has drawn tremendous attention\nto research communities. As one of the most influ-\nential approaches in this direction, reinforcement\nlearning from human feedback (RLHF) (Ouyang\net al., 2022; Li et al., 2023b), adopts an actor-critic\nframework, where the student model is optimized\nto generate responses to receive higher reward from\nthe critic model. In InstructGPT (Ouyang et al.,\n2022), the critic (reward model) is trained from\nhuman annotation. Direct Preference Optimiza-\ntion (DPO) (Rafailov et al., 2023) drops the need\nof training a reward model, by using a reference\nLLM and offline trajectories to estimate the reward.\nChain-of-Hindsight (Liu et al., 2023) converts hu-\nman preference annotations into simple natural lan-\nguage feedback, and thus turns RL optimization\nto conditional generation. In above methods, the\nassumption is that there are no ground truth targets\nand thus they try to improve the LLM based on\nthe assessment (critic) of multiple generated out-\nputs. However, such RL-style training will be less\neffective and efficient to supervised finetuning, es-\npecially for challenging tasks with sparse rewards –\ne.g. sovling math puzzles or coding tasks. Unlike\nthese methods, our approach can acquire \"ground\ntruth\" outputs from a personalised teacher, thus su-\npervised finetuning can be applied which makes\nthe learning effective and efficient, even for chal-\nlenging tasks like solving coding problems.\n3 Method\n3.1 Standard Distillation\nAssume a dataset of code generation tasks\nD = {(t,u)} where each problem (or task)\nconsists of a task instruction t and a unit test\ncollection u. During training, we have access\nto a teacher model πϕ and a student model πθ.\nThe objective is to distill how the teacher solves\ncode generation tasks to the student model, in the\ncontext of D. For each task (t,u), we first query\nthe teacher πϕ(t) with the task instruction, to get a\ndirect generated code snippet cϕ. Then, we execute\nthe generated code cϕ against unit test cases u\nand get its execution feedback f ←EXEC (cϕ,u),\nwhere the EXEC function returns passed if the\ncode passes all the unit tests, otherwise it returns\nan error message from the executor. By filtering\nout the tasks where cϕ do not pass all the unit tests\n(i.e., f ̸= passed), we get a new clean dataset\nDSTAND = {(t,u,c )}, where each task consists\na task instruction t, a suite of unit tests u and a\ncorrect solution code c.\nWe then finetune the student model πθ on\n{(u,c)} ∼ DSTAND, where the input is the task\ninstruction uand the output is the corresponding\ncode solution c. We name this approach STAND.\n3.2 Personalised Distillation\nThe STAND approach simply samples training\nexamples (instructions and labels) from the\nprior distribution of the teacher model and\nfeeds it to the student without considering the\nconditions of the student model. Inspired by\nmodern education principles which advocates\ninteractive and personalised learning experience,\nwe propose personalised distillation: adapting\nteaching materials to student’s current knowledge\n6739\nAlgorithm 1 personalised distillation for code gen-\neration (PERsD-combined).\n1: Input: Dataset DSTAND, student LLM πθ, unit test ex-\necutor EXEC , refinement template Trefine, teacher LLM\nπϕ\n2: Drefine ←{} ▷refinement data for finetuning\n3: Dcode ←{} ▷direct generation data\n4: for (t,u,c ) ∈DSTAND do\n5: cθ ←πθ(t) ▷student generates cθ\n6: f ←EXEC (cθ,u) ▷exec. feedback for cθ\n7: if f ̸= passed then\n8: // personalised refinement from teacher\n9: crefine ←πϕ(t,cθ,f)\n10: // create refinement task instruction\n11: trefine ←Trefine(t,cθ,f)\n12: if EXEC (crefine,u) =passed then\n13: Drefine.insert({trefine,crefine})\n14: Dcode.insert({t,c})\n15: end if\n16: end if\n17: end for\n18: πθ∗ ←FINETUNE (πθ,Drefine + Dcode)\nand capacity. We propose three variants:\nPER SD-combined Algorithm 1 shows detailed\nsteps for PER SD-combined. This method takes the\nstandard distillation dataset DSTAND from §3.1 and\nfirst lets the student generate solutions for each task.\nThen it filters out the tasks where the student model\ncan already solve correctly. For the remaining tasks,\nit obtains the teacher’s personalised refinement con-\nditioned on the student’s attempt and its execution\nerror feedback, and only keeps the tasks where the\nteacher’s refinement is valid (i.e., passes all the unit\ntest cases). Figure 1 visualizes these three steps.\nFor this final task-set, we create two datasets:\ni) Dcode containing task instruction as input and\nteacher’s direct answer as output, and ii) Drefine\ncontaining task refinement instruction as input and\npersonalised refinement answer as output. The\ntask refinement instruction (line 9 in Algorithm\n1) is created by concatenating task instruction t,\nstudent’s attempt cθ and its execution feedback f\nwith a refinement template Trefine (More details in\nAppendix C). Such refinement instruction turns\nstandard code generation into a code refinement\ntask, teaching the student how to refine its own\nsolution. PER SD-combined then finetunes the\nstudent model on Drefine combined with Dcode.\nPER SD-refine Similar to PER SD-combined, this\nvariant follows line 1-15 of Algorithm 1 to collect\nrefinement data Drefine. However, it differs from\nthe above model as it only uses Drefine to finetune\nthe student model.\nPER SD This variant takes the training data Drefine\nfrom PER SD-refine and replace the input of each\ndata point from code refinement prompt to original\ntask instruction. It thus trains the student model\nwith personalised labels on code generation.\nTo illustrate the difference between personalised\nrefinement and teacher’s direct solution, we show\na real example in Figure 2. The top shows the per-\nsonalised refinement for the given task, while the\nbottom section shows the direct teacher’s gener-\nation for the same task. Note how the teacher’s\ndirect generation is significantly different from the\nstudent model’s attempt, while the teacher’s refine-\nment follows the student’s attempt and improves\nupon it. We hypothesize that such adaptive refine-\nment where the teacher aligns to student’s genera-\ntion, helps the student to learn more efficiently and\neffectively, similar to how humans benefit from\npersonalised learning.\nFigure 2: Example: (Top) Personalised refinement from\nstudent’s attempt and execution feedback; (Bottom) Di-\nrect solution generated by teacher conditioned on task.\n3.3 Iterative Inference\nLet Dtest = {(t,u)} denote our test set for\ninference, where each data point (t,u) consists\nof a task instruction t and a suite of hidden\nunit test cases u. We also assume that the task\ninstruction contains some simple unit test cases\nin its doc-string (as often seen in code generation\ninstructions), which we can extract and format\nusing rule-based heuristics to obtain a suite of seen\nunit test cases useen (More details in Appendix A).\n6740\nFor single-step inference, we use the standard\napproach to evaluate pass@k. Specifically, for\neach task t, we query the model ntimes with the\ntask instruction: ci\nθ ←πθ(t) for i= 1...n . Then,\nfollowing (Chen et al., 2021), we estimate pass@k\nfrom the number of attempts that passed the hidden\nunit test cases: E XEC (ci\nθ,u) =passed.\nMulti-step inference If the model πθ has been\ntrained to rectify, following our approach in\nPERsD-refine or PERsD-combine, and if unit\ntests are available during inference, we can per-\nform 2-step inference: for each generated at-\ntempt ci\nθ in 1-step, we first get execution feedback\nfi\nseen ← EXEC (ci\nθ,useen). If fi\nseen = passed,\nwe reuse the original attempt as the 2-step at-\ntempt. Otherwise, we create a refinement in-\nstruction ti ←Trefine(t,ci\nθ,fi\nseen) following the ap-\nproach in PERsD-refine or PERsD-combined, and\nquery the same model with the refinement instruc-\ntion for 2-step attempt: ci\nθ,2-step ← πθ(ti). We\nthen compute pass@k over the 2-step generations\nsimilar to 1-step inference.\n4 Experimental Setup\n4.1 Baselines\nThe first baseline is STAND, the standard distilla-\ntion approach mentioned in §3.1.\nTo measure the effectiveness of personalised\nlabels quantitatively, we also compare with\nInput-personalised distillation baselines as well,\nwhere only the input tasks are selected in a manner\ncustomized to the student’s abilities. However, the\noutput labels are not personalised, as they are taken\nfrom teacher’s direction generation c instead of\npersonalised refinement crefine. We start with Dcode\nfrom PER SD-combined and have three variants:\nINPD We finetune the student model πθ on\n{(t,c)} ∼ Dcode, where the input is a task\ninstruction and the output is a code solution. This\nvariant is more customized than STAND as it filters\nout the tasks which the student can already solve\ncorrectly.\nINPD-refine Similar to PERsD-refine, InpD-refine\ntrains the student model to rectify its wrong\nattempt. The difference is in InpD-refine, the\nrefined code is from teacher’s direct solution c,\ninstead of personalised refinement crefine.\nINPD-combined Similar to PER SD-combined,\nInpD-combined trains the student on rectifying its\nanswers as well as directly solving the task. The\ndifference is that in InpD-combined, the labels\nfor both code refinement and code generation are\ntaken from teacher’s direct solution c.\n4.2 Pretraining Data Construction\nTo construct our pretraining data, we adopted the\ndata collection process in code-alpaca(Chaudhary,\n2023) and used a set of 374 seed tasks from MBPP\n(task-ids 601-974) as in-context prompt to query\nChatGPT for novel code generation tasks. This\nseed-set increases the likelihood of ChatGPT gen-\nerating python codes.\nThrough this process, we obtained a corpus of\n20K code generation tasks from ChatGPT each\ncomprising a task instruction and the corresponding\ngenerated code, which is typically a single python\nfunction. Next we show each generated instance to\nChatGPT again and prompt it to generate 5 unique\ntest-case inputs (i.e. input argument values) for\nthe python function. We then parse and format\nthe generated test-case input and execute the gen-\nerated code on it obtain an output. Thus, out of\n20K, for 14880 instances we could successfully\ngenerate and parse 5 unit test case inputs and for\n10172 instances we were able to successfully exe-\ncute the generated code and obtain outputs on all\n5 inputs. This final corpus of 10K code generation\ntasks, each comprising a task instruction and the\ncorresponding generated code along with 5 unit test\ninput and outputs forms our standard distillation\ndataset DSTAND.\nTo collect personalised distillation data, we fol-\nlow §3.2 to first ask the student model to generate 1\noutput code per task, setting sampling temperature\nto 0.3. We then evaluate the student’s attempt and\nonly keep the tasks with the wrong generations (i.e.\nthe ones which failed any of the unit test-case). We\nuse this to query ChatGPT for personalised refine-\nments and only retain the valid refinements which\npassed all unit tests. Our prompt to ChatGPT con-\ntains the original task instruction and code from\nDSTAND along with the student model’s generated\ncode and execution feedback (compiler errors or\nunit test failures). Our instruction to ChatGPT is to\ngenerate a correct solution that rectifies the errors\nand is closest in semantics to the student’s code\n(More details in Appendix B). Table 3 shows the\nstatistics of personalised data construction process.\n6741\nStudent Model # Wrong Attempt # Validated Per- Data\nby Student sonalised Tasks Cost\nCodeGen-mono-6B (Nijkamp et al., 2023) 6.5K 3.25K 5.5$\nCodeGen-mono-6B (round2) 4K 1.4K 4.4$\nCodeGen-mono-16B 6.2K 2.8K 6.5$\nStarCoder (Li et al., 2023a) 4.3K 2.5K 4.3$\nTable 3: Statistics of Personalised Data Construction\n4.3 Model Evaluation\nWe evaluate our models on two datasets: Hu-\nmanEval(Chen et al., 2021), which contains 164\nPython problems, and the subset MBPP(Austin\net al., 2021) sanitized set that has no overlap with\nour MBPP seed tasks for pretraining data collec-\ntion. This corresponds to test+validation+prompt\nsplits of MBPP-sanitized and consists of 306\nPython problems. We use nucleus sampling with\ntemperature 0.2 to generate 20 candidates per\ntask for estimating pass@1, and with tempera-\nture 0.8, 100 candidates per task for estimating\npass@5/10/20/50/100.\nFor multi-step inference, we first extract the\n“seen” unit test-cases from the doc-string of the\ntask instruction (More details in Appendix A).\nNext, we generate output samples in the usual\ncode-generation style forming the set of 1-step gen-\nerations for each instance. Each of these candi-\ndate generations are then executed on the extracted\n“seen” unit test cases to obtain a refined code, thus\nforming the set of 2-step generations.\n4.4 Pretraining Setup\nFor all experiments with CodeGen-mono-6B back-\nbone, we use effective batch size of 1024 and pre-\ntrain for 20 epochs. For backbone as CodeGen-\nmono-16B, we use effective batch size of 1024\nand pretrain for 3 epochs, as the training converges\nmuch faster than CodeGen-mono-6B. For PERsD-\ncombine with StarCoder model, we use effective\nbatch size of 1024 and pretrain for 8 epochs, which\nresults in similar training loss as CodeGen-mono-\n16B. We implement using HuggingFace transform-\ners(Wolf et al., 2020) and DeepSpeed Zero (Ra-\njbhandari et al., 2020). All experiments are con-\nducted on a cluster of 8 A100-40GB GPUs.\n5 Experimental Results\n5.1 Main Results\nWe empirically test the hypothesis that personalised\ndistillation helps student model learn more effec-\ntively, by comparing PERsD models with baseline\ndistillation methods (InpD, StanD) in Table 4.\nPersonalised labeled-data is generally bet-\nter than standard data Comparing PERsD-\ncombine to InpD-combine, we find PERsD-\ncombine outperforms InpD-combine in all settings,\noften with a significant margin (two backbones,\ntwo datasets, two inference steps, 4 pass@k met-\nric). Similar observation holds true when compar-\ning PERsD-refine to InpD-refine (except for 2/32\nsettings), and PERsD to InpD. Thus, we conclude\nthat PERsD-variants are generally significantly bet-\nter than their InpD counterparts, providing strong\nevidence that personalised labels are more effective\nfor the student model to learn than standard labels.\nPERsD outperforms StanD with less than one-\nthird of its data We observe that PERsD out-\nperforms StanD for every pass@k on both 16B\nand 6B CodeGen-mono backbone across both Hu-\nmanEval and MBPP, even though StanD has 10K\ndata and PERsD has only 3.3K and 2.8K exam-\nples for CodeGen-mono-6B and 16B. The only\nexception is in the setting CodeGen-mono-16B,\nMBPP, pass@1, where StanD edges out PERsD\nby 1.2 points. Given that our pretraining data is\nconstructed from seed tasks taken from MBPP, we\nhypothesize that StanD might enjoy an unfair ad-\nvantage due to its having three times more data,\nmaking it more susceptible to data leakage. We\nverify such hypothesis further in §5.2. In summary,\nwith PERsD outperforming StanD in 15 out of 16\nsettings while having less than a third of the data,\nit’s evident that personalized labeled data makes\nthe learning more efficient.\nMulti-step inference consistently improves an-\nswer quality For PERsD-refine and PERsD-\ncombine models, we find that 2 step inference con-\nsistently improves performance on HumanEval and\nMBPP. This shows the models successfully learn\nhow to rectify its solution based on execution er-\nror feedback. Note that InpD-refine yields worse\naccuracy with 2 step inference on HumanEval\npass@10/20, strengthening the advantage of per-\nsonalised labeled data over standard labeled data.\n5.2 Train-Test overlap analysis\nAs observed in Table 4, PersD-variants enjoy\nhigher average improvements over their InpD coun-\nterparts, on HumanEvan than on MBPP. To delve\ndeeper, we conduct a data overlap analysis. For\neach test task, we extract the most similar training\ntask and use GPT-3.5-turbo to score their seman-\ntic similarity, with 0 indicating no relation and 1\n6742\n(a) Backbone as CodeGen-mono-6B\nMethods #DataPass@1 Pass@5 Pass@10 Pass@20step=1 step=2 step=1 step=2 step=1 step=2 step=1 step=2HumanEval\nStanD 10K 32.41 - 41.79 - 45.67 - 49.26 -InpD 3.3K 31.65 - 44.55 - 50.72 - 56.76 --refine 3.3K 29.70 29.70 43.82 41.99 51.28 47.89 58.29 53.51-combined 6.5K 30.15 32.30 42.94 45.27 47.91 50.50 52.54 55.46PERsD 3.3K34.63- 49.34- 55.34 - 60.41 --refine 3.3K 32.35 33.35 48.69 49.3556.07 56.87 63.60 64.76-combined 6.5K 33.8135.5344.6449.6749.96 55.67 55.23 61.21\nMBPP\nStanD 10K 43.11 - 55.24 - 59.07 - 62.51 -InpD 3.3K 43.59 - 55.83 - 63.13 - 67.34 --refine 3.3K 44.44 47.81 62.25 66.43 67.61 71.44 71.68 75.22-combined 6.5K 42.69 47.25 56.70 62.17 61.39 66.49 65.46 70.22PERsD 3.3K 45.47 - 59.90 - 64.85 - 69.73 --refine 3.3K48.24 52.65 63.65 68.49 69.00 73.34 73.16 77.62-combined 6.5K 42.77 48.92 56.91 62.29 61.43 66.89 65.22 70.96\n(b) Backbone as CodeGen-mono-16B\nMethods #DataPass@1 Pass@5 Pass@10 Pass@20step=1 step=2 step=1 step=2 step=1 step=2 step=1 step=2HumanEval\nStanD 10K 33.96 - 50.56 - 57.69 - 63.82 -InpD 2.8K 36.68 - 49.51 - 53.85 - 57.47 --refine 2.8K 30.55 31.28 48.40 48.13 55.00 54.52 61.31 60.62-combined 5.6K 34.66 36.49 50.65 53.89 56.75 60.07 62.78 65.85PERsD 2.8K37.74- 56.57- 63.92- 69.97--refine 2.8K 36.7737.9951.86 54.23 58.07 60.92 63.17 67.13-combined 5.6K 36.40 37.74 53.5755.8060.8163.3767.3 70.50\nMBPP\nStanD 10K 48.90 - 62.21 - 66.91 - 71.33 -InpD 2.8K 46.27 - 58.45 - 62.61 - 66.43 --refine 2.8K 48.79 54.8766.8971.3272.2475.71 75.82 78.84-combined 5.6K 47.39 53.59 59.14 66.38 63.48 70.76 67.10 74.35PERsD 2.8K 47.68 - 65.80 - 71.56 - 76.02 --refine 2.8K51.5056.2166.82 71.8672.0676.7876.0380.42-combined 5.6K 51.4456.4466.45 71.31 71.64 76.4376.0480.20\nTable 4: Comparing PERsD models to StanD & InpD\nindicating complete semantic overlap (further de-\ntails in Appendix D). Table 5 reveals more overlap\nin MBPP than HumanEval, and more overlap for\nStanD compared to PERsD. This overlap could be\nwhy StanD surpasses PERsD in the 1/16 setting\n(CodeGen-mono-16B, MBPP, pass@1), as StanD\nhas an unfair advantage of having significantly\nmore data leakage. In addition, if we test our meth-\nods on clean-MBPP where the leaked data points\nare removed, then PERsD becomes almost on-par\nwith StanD in this specific setting while having\nlarger margin over StanD on the rest 15/16 settings\n(from 4.8 points average margin to 5.9 points, more\ndetails at Appendix E). Altogether, this overlap\nanalysis, coupled with results from cleaned MBPP,\nfurther underscores the advantages of personalized\ndistillation.\n5.3 Effect of mixing StanD and InpD data\nTable 6 shows the ablation study on mixing stan-\ndard distillation data to PERsD-refine and InpD-\nrefine: while mixing standard data to InpD-refine\nimproves its 1-step performance on MBPP and\nMethod Backbone %(\"leak\") Similarity\nHumanEval\nStanD 6B,16B 6.1% 0.22\nPERsD 6B 3.6% 0.18\nPERsD 16B 3.05% 0.22\nMBPP\nStanD 6B,16B 18.24% 0.40\nPERsD 6B 8.47% 0.30\nPERsD 16B 7.49% 0.30\nTable 5: Train-Test Overlap Analysis. 6B/16B denotes\nCodeGen-mono-{6/16}B backbones. %(\"leak\") denotes\nthe percentage of test data that are semantically leaked\nin training data. ’Similarity’ represents the average\nsimilarity score (range: 0 to 1; higher values indicate\ngreater similarity)\nroughly maintains its performance on other set-\ntings, mixing StanD data to PERsD-refine signifi-\ncantly deteriorate its performance (except pass@1\ninf-step=2 on HumanEval). We conjecture that as\nStanD has much larger data volume than PERsD-\nrefine, it overwhelms the student training on stan-\ndard distillation. However, combining with a bal-\nanced input-personalised data can be beneficial, as\nwe observe from the good performance of PERsD-\ncombined in Table 4 on CodeGen-mono-16B.\nMethods Inf Pass@1 Pass@5 Pass@10 Pass@50 Pass@100\nStep HumanEval\nStanD + InpD-refine\n1\n30.59 40.04 44.20 54.23 58.54\nStanD + InpD-refine* 29.45 39.83 44.07 54.55 59.76\nStanD + PERsD-refine 32.13 43.82 48.66 59.55 64.02\nPERsD-refine 32.35 48.69 56.07 72.10 77.44\nStanD + InpD-refine\n2\n30.87 42.88 47.90 58.21 60.98\nStanD + InpD-refine* 30.12 42.71 47.42 58.69 64.02\nStanD + PERsD-refine35.0047.89 52.96 64.36 69.51\nPERsD-refine 33.35 49.35 56.87 74.13 79.88\nMBPP\nStanD + InpD-refine\n1\n42.60 53.18 56.49 62.11 63.07\nStanD + InpD-refine* 44.08 54.12 57.82 64.96 66.34\nStanD + PERsD-refine 45.63 53.20 56.38 63.02 65.36\nPERsD-refine 48.24 63.65 69.00 78.16 81.70\nStanD + InpD-refine\n2\n46.32 58.84 62.80 69.80 71.23\nStanD + InpD-refine* 46.92 58.18 62.03 68.82 68.95\nStanD + PERsD-refine 48.44 58.37 62.47 70.64 73.20\nPERsD-refine 52.65 68.49 73.34 82.72 85.62\nTable 6: Ablation on mixing StanD, with Backbone as\nCodeGen-mono 6B. InpD-refine* denotes using all 6.5K\ntasks where the student model made mistakes, which\ncovers around 3K more tasks than InpD-refine.\nSimilarly, in Table 7 we show another ablation:\nthat mixing InpD data with PERsD roughly main-\ntains the performance on HumanEval but degrades\non MBPP. This shows personalised labels are of\nhigher quality and mixing non personalised labels\nfor the same task generally hurts performance.\n6743\nMethods Pass@1 Pass@5 Pass@10 Pass@50 Pass@100\nHumanEval\nPERsD 34.63 49.34 55.34 65.56 67.93\nPERsD + InpD 34.88 48.35 54.06 64.88 68.90\nMBPP\nPERsD 45.47 59.90 64.85 76.05 80.07\nPERsD + InpD 43.84 59.02 63.77 71.69 74.84\nTable 7: Ablation on PERsD mixing InpD with\nCodeGen-mono 6B as backbone\nRound Inf Pass@1 Pass@5 Pass@10 Pass@50 Pass@100\nStep HumanEval\n1 1 33.81 44.64 49.96 61.75 70.73\n2 32.74 45.50 51.52 66.14 71.95\n1 2 35.53 49.67 55.67 68.16 77.44\n2 36.75 49.71 56.13 70.24 75.00\nMBPP\n1 1 42.77 56.91 61.43 68.84 70.67\n2 45.07 57.75 62.27 70.49 72.55\n1 2 48.92 62.29 66.89 75.09 77.25\n2 49.59 63.43 68.30 76.00 78.10\nTable 8: Ablation on multi-round distillation on PERsD-\ncombined with CodeGen-mono 6B as backbone\n5.4 Multi-round Distillation\nAfter finetuning the student model with the per-\nsonalised distillation data, can we perform an-\nother round of personalised distillation, on the new\nmodel? We show such an ablation study in Ta-\nble 8. Encouragingly, we find PERsD-combined\nround-2 generally outperforms PERsD-combined\nround-1 by a modest margin. This improvement\nprovides further evidence of the benefits of per-\nsonalized learning, even when applied to models\ntrained with personalized distillation. These find-\nings suggest the intriguing possibility of an online\nor active version of personalized distillation, where\ndata collection and model training occur simulta-\nneously to ensure each batch is fully personalized\nand has higher sample efficiency. However, we will\nleave such intriguing exploration for future work.\n5.5 Utilizing feedback for multi-step Inference\nTo better understand the role of execution feed-\nback during training and multi-step inference, we\nshow an ablation study in Table 9, where we\ncompare PERsD-combine with a specific variant\n(PERsD-combine*) that excludes feedback dur-\ning both training and inference. we observed that\nPERsD-combine* performs comparably to PERsD-\ncombine on HumanEval and slightly better on\nMBPP for 1-step inference. However, for 2-step\ninference, PERsD-combine* consistently underper-\nforms PERsD-combine. This result aligns well\nwith our expectations that code-rectification needs\nthe execution feedback to guide the refinement.\nMethods Inf Pass@1 Pass@5 Pass@10 Pass@50 Pass@100\nStep HumanEval\nPERsD-combine1 33.81 44.64 49.96 61.75 70.73\nPERsD-combine* 33.2945.47 50.90 62.87 68.29\nPERsD-combine2 35.53 49.67 55.67 68.16 77.44\nPERsD-combine* 34.59 49.54 55.59 67.27 71.95\nMBPP\nPERsD-combine1 42.77 56.9161.43 68.84 70.67\nPERsD-combine*44.76 56.9560.85 68.67 71.57\nPERsD-combine2 48.92 62.29 66.89 75.09 77.25\nPERsD-combine* 47.83 61.28 65.54 73.03 75.49\nTable 9: Ablation on removing execution feedback with\nCodeGen-mono 6B as backbone. PERsD-combine*\ndenotes combined personalised distillation without exe-\ncution feedback in input prompt.\n5.6 Cross-Model Personalised Distillation\nTo investigate whether personalised distillation\ndata of one model can be benefical to another,\nwe conduct an ablation in Table 10 by using\nPERsD-combined data of CodeGen-mono-6B to\ntrain CodeGen-mono-16B. The results show that\nsuch cross-model persionalised data do not per-\nform as well as real personalised data: leading to\na consistent performance drop by a large margin.\nThis finding reinforces our notion that learning data\nshould be tailored to the specific student model, as\npersonalized data suitable for one model may not\nnecessarily benefit others.\nModel Inf Pass@1 Pass@5 Pass@10 Pass@50 Pass@100\nStep HumanEval\nCodeGen-mono-6B1 33.81 44.64 49.96 61.75 70.73\nCodeGen-mono-16B* 32.99 47.81 54.58 69.31 73.98\nCodeGen-mono-16B36.40 53.57 60.81 74.64 79.88\nCodeGen-mono-6B2 35.53 49.67 55.67 68.16 77.44\nCodeGen-mono-16B* 35.85 51.31 58.23 74.02 76.60\nCodeGen-mono-16B37.74 55.80 63.37 77.14 81.10\nMBPP\nCodeGen-mono-6B1 42.77 56.91 61.43 68.84 70.67\nCodeGen-mono-16B* 43.24 60.14 65.19 72.31 74.19\nCodeGen-mono-16B51.44 66.45 71.64 80.62 82.93\nCodeGen-mono-6B2 48.92 62.29 66.89 75.09 77.25\nCodeGen-mono-16B* 48.12 65.31 70.02 76.60 78.70\nCodeGen-mono-16B56.44 71.31 76.43 84.39 86.76\nTable 10: Ablation on cross-model personalised distil-\nlation with PERsD-combined. CodeGen-mono-16B*\nmeans distillation data is from CodeGen-mono-6B.\n5.7 Comparison with other Feedback-based\nCode Generation Models\nComparison with ILF (Chen et al., 2023a): In\norder to compare with ILF, one of our closest re-\nlated work, we experiment on a separate setting:\n6744\nstarting with full MBPP dataset (974 tasks) and\nuse Task-Ids 11-111 as test split and remaining\n863 as training data. On the training set, our stu-\ndent model CodeGen-6B (same as ILF) generated\nwrong attempts on 562 tasks, which were shown to\nChatGPT along with the task instruction and execu-\ntion error feedback to eventually collect 288 valid\npersonalized code rectification labels.\nThe original MBPP text-to-code\ndata and this collected personalized\ncode-refinement data for the 288 tasks\nMBPP Test Set\nMethod Cost Pass@1 Pass@10\nILF >4K$ 36 68\nPERSD 0.65$ 46.8 67.4\n-refine 0.65$ 41.8 66.8\n-combined 0.65$47.8 64.8\nTable 11: Comparison with\nILF\nrespectively form\nthe finetuning data\nDcode and Drefine\non which we train\nmodels PER SD and\nPER SD-refine. We\nfurther combine\nDcode and Drefine\nto train PER SD-\ncombined. Our experimental results in Table\n11 show that all PER SD-variants significantly\noutperform ILF by 11.8% at pass@1 at a cost 1e-4\ntimes lower than ILF, thus showcasing the lack of\nscalability of ILF-style models.\nComparison with Self-Edit : Since Self-Edit\n(Zhang et al., 2023) uses a trainable CodeGen-\n350M code editor model and a frozen code-\ngeneration model, our experimental setup is not\ndirectly comparable with theirs. However, our\nINPD-refine and INPD-combined models can actu-\nally be considered as very close counterparts to a\nversion of Self-Edit with shared a code-generation\nand code-refinement model and CodeGen-6B back-\nbone. The consistent performance improvement of\nthe personalized distillation models over the input-\ndistilled ones across the board, alludes towards the\nprospect that PER SD-models are indeed more ef-\nfective than Self-Edit style models.\n5.8 Comparison with SOTA Models\nFianlly, we compare PERsD-combine models\nwith open-source and close-sourced state-of-the-\nart models on HumanEval in Table 12.We find\nthat PERsD-combine methods can significantly im-\nprove the backbone model, with a performance\ngain of 6.2 points for CodeGen-mono 6B (8.4% er-\nror reduction), 5.9 points for CodeGen-mono 16B\n(8.3% error reduction) and 12.2 points for Star-\nCoder (18.4% error reduction). Moreover, Star-\nCoder with PERsD-combined, outperforms other\nopen-sourced models except WizardCoder. Note\nModel Model size Pass@1 Pass@10 Pass@100\nClosed-source models\nLaMDA 137B 14.0 - 47.3\nPaLM 540B 26.2 - 76.2\nCodex 12B 28.8 46.8 72.3\ncode-cushman-001 - 33.5 54.3 77.4\ncode-davinci-002 - 47.0 74.9 92.1\nGPT-3.5 - 48.1 - -\nphi-1 1.3B 50.6 - -\nGPT-4 - 67.0 - -\nOpen-source models\nCodeGeeX 13B 22.9 39.6 60.9\nLLaMA 65B 23.7 - 79.3\nStarCoder 15B 33.6 - -\nCodeGen-mono 6B 26.1 42.3 65.8\nCodeGen-mono 16B 29.3 49.9 75.0\nInstructCodeT5+ 16B 35.0 54.5 77.9\nWizardCoder 15B 57.3 - -\nCodeGen-mono (PERsD-combined) 6B 33.8 50.0 70.7\nCodeGen-mono (PERsD-combined) 16B 36.4 60.8 79.9\nStarCoder (PERsD-combined) 15B 45.868.3 82.3\nTable 12: Results of pass@k(%) on HumanEval\nthat our model ues 5K data examples while Wiz-\nardCoder uses 78K. As mentioned in §2.1, Wiz-\nardCoder is an orthogonal approach that can be\nintegrated into personalised distillation.\n6 Conclusion\nIn this paper, we introduced personalized distilla-\ntion as a method for collecting customized labeled\ndata that adapts to the capacity of student models,\nresulting in more effective learning. We demon-\nstrated the advantages of personalized distillation\nover standard distillation in the field of code gen-\neration, achieving superior performance on both\nthe HumanEval and MBPP datasets. Through com-\nprehensive ablation studies, we confirmed that per-\nsonalized distillation leads to higher data quality,\nbenefits from multi-round distillation, and enables\nmodels to leverage execution feedback for self-\nrectification. We believe personalized distillation\nrepresents an exciting step towards better distilla-\ntion of closed-source LLMs to open-source models.\nLimitations\nIn this section, we discuss some limitations of this\npaper and future directions to make it more valu-\nable:\nOn Data Scale For a fair comparison, we have\nconducted all experiments based on the same 10K\nDSTAND data (introduced §4.2) and the correspond-\ning personalised data processed from DSTAND are\nof size 2-3K as shown in Table 3. However, as\nwe have proven personalised distillation supports\n6745\nmore effective and efficient learning, it is intriguing\nto investigate how well does personalised distilla-\ntion scale with the data size. For example, if we\nscale personalised distillation data to 50K, how\nmuch more performance gain will PERsD meth-\nods receive compared to InpD and StanD with the\nscaling of data size.\nOnline Personalised Distillation As discussed\nin §5.4, conducting a second round personalised\ndistillation continues to improve a student model\nthat is already trained with PERsD-combine. Such\nobservation suggests the potential of an online ver-\nsion of personalised distillation, which collects\na batch of personalised data on-the-fly with the\nteacher model, after each optimization step during\nfinetuning. As we have proven that true person-\nalised data is more beneficial than standard data\nor cross-model personalised data (§5.6), such on-\nline personalised distillation will in-principle maxi-\nmally benefit from personalised distillation, as each\nbatch of training data is fully tailored to the student\nmodel.\nReferences\nJacob Austin, Augustus Odena, Maxwell I. Nye,\nMaarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V . Le,\nand Charles Sutton. 2021. Program synthesis with\nlarge language models. CoRR, abs/2108.07732.\nSahil Chaudhary. 2023. Code alpaca: An\ninstruction-following llama model for code genera-\ntion. https://github.com/sahil280114/\ncodealpaca.\nAngelica Chen, Jérémy Scheurer, Tomasz Korbak,\nJon Ander Campos, Jun Shern Chan, Samuel R. Bow-\nman, Kyunghyun Cho, and Ethan Perez. 2023a. Im-\nproving code generation by training with natural lan-\nguage feedback.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pondé de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023b. Teaching large language mod-\nels to self-debug. CoRR, abs/2304.05128.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nYuxin Jiang, Chunkit Chan, Mingyang Chen, and\nWei Wang. 2023. Lion: Adversarial distillation\nof closed-source large language model. CoRR,\nabs/2305.12870.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny\nChim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue\nZhuo, Thomas Wang, Olivier Dehaene, Mishig\nDavaadorj, Joel Lamy-Poirier, João Monteiro, Oleh\nShliazhko, Nicolas Gontier, Nicholas Meade, Armel\nZebaze, Ming-Ho Yee, Logesh Kumar Umapathi,\nJian Zhu, Benjamin Lipkin, Muhtasham Oblokulov,\nZhiruo Wang, Rudra Murthy V , Jason Stillerman,\nSiva Sankalp Patel, Dmitry Abulkhanov, Marco\nZocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Ur-\nvashi Bhattacharyya, Wenhao Yu, Swayam Singh,\nSasha Luccioni, Paulo Villegas, Maxim Kunakov,\nFedor Zhdanov, Manuel Romero, Tony Lee, Na-\ndav Timor, Jennifer Ding, Claire Schlesinger, Hai-\nley Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra,\nAlex Gu, Jennifer Robinson, Carolyn Jane Ander-\nson, Brendan Dolan-Gavitt, Danish Contractor, Siva\nReddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jer-\nnite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas\nWolf, Arjun Guha, Leandro von Werra, and Harm\nde Vries. 2023a. Starcoder: may the source be with\nyou! CoRR, abs/2305.06161.\nZihao Li, Zhuoran Yang, and Mengdi Wang. 2023b. Re-\ninforcement learning with human feedback: Learn-\ning dynamic choices via pessimism. CoRR,\nabs/2305.18438.\nHao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023.\nChain of hindsight aligns language models with feed-\nback. CoRR, abs/2302.02676.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-\nubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,\nQingwei Lin, and Daxin Jiang. 2023. Wizardcoder:\nEmpowering code large language models with evol-\ninstruct. CoRR, abs/2306.08568.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\n6746\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. CoRR, abs/2303.17651.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2023. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\nICLR.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Ste-\nfano Ermon, Christopher D. Manning, and Chelsea\nFinn. 2023. Direct preference optimization: Your\nlanguage model is secretly a reward model. CoRR,\nabs/2305.18290.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: memory optimizations\ntoward training trillion parameter models. In SC,\npage 20. IEEE/ACM.\nHeather Roberts-Mahoney, Alexander J. Means, and\nMark J. Garrison. 2016. Netflixing human capital\ndevelopment: personalized learning technology and\nthe corporatization of k-12 education. Journal of\nEducation Policy, 31(4):405–420.\nAtikah Shemshack and Jonathan Michael Spector. 2020.\nA systematic literature review of personalized learn-\ning terms. Smart Learning Environments, 7(1):1–20.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: Language agents with verbal rein-\nforcement learning.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022. Self-instruct: Aligning language\nmodel with self generated instructions.\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2022. Generating sequences by learning to\nself-correct. CoRR, abs/2211.00053.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023a. Wizardlm: Empowering large lan-\nguage models to follow complex instructions. CoRR,\nabs/2304.12244.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023b. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv\npreprint arXiv:2304.01196.\nKechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023.\nSelf-edit: Fault-aware code editor for code genera-\ntion. CoRR, abs/2305.04087.\nA Details in Multi-step Model Evaluation\nAs the docstrings are ill-formated in HumanEval,\nwe write a simple rule-based parsing code snip-\npet to extract its seen unit test cases. On average\nper task, there is 2 seen unit test cases and 4.2\nunseen unit test cases. The overlap between seen\nand unseen tests is 11.33%. For MBPP, since con-\nventionally the instruction prompt is constructed\nby taking the task description and example usages\n(from the unit test cases) as part of the doc-string,\nwe consider all the unit test cases to be \"seen\" and\nuse all of them for multi-step inference.\nB ChatGPT Prompt Template for\nPersonalised Distillation\nIn Figure 3, we show the prompt template we use\nto query ChatGPT for personalised refinement. For\neach task example, with task instruction t, unit test\ncases uand correct code c, we query ChatGPT API\nwith two turn conversation history.\nFor the first turn, we use the template in Fig-\nure 3a and replace «TASK», «HEADER» with the\nactual task instruction t and function header ex-\ntracted. This is added to first turn’s user input and\nthe correct code cis included as first turn’s assistant\noutput. For the second turn, we use the template in\nFigure 3b and replace «CODE», «ERROR» with\nthe student model’s attempt and its execution feed-\nback. This is added to second turn’s user input and\nwe query ChatGPT with the constructed conver-\nstaion history to get second turn’s assistant output\nas personalised code refinement.\n6747\n(a) Turn-1 Prompt Template\n(b) Turn-2 Prompt Template\nFigure 3: Prompt templates to query personalised re-\nfinement. Top(a): prompt template for first turn con-\nversation, Botton(b): prompt template for second turn\nconversation.\nC Prompt Template for Code Refinement\nFinetuning\nFigure 4 shows the refinement template Trefine in-\ntroduced in §3.2), which is used to construct input\nprompt for code refinement finetuning. we replace\n«TASK» with task instruction, «CODE» with the\ninitial wrong attempt from student, «ERROR» with\nthe execution feedback, and «HEADER» with func-\ntion header extracted from task instruciton.\nD Details in Data Overlap Analysis\nThis section describes the detailed procedures to\nconduct train-test data overlap analysis. The objec-\ntive is to assess the extent of data leakage in the\ntest datasets originating from our self-constructed\npretraining corpus.\nFigure 4: Prompt template for code refinement finetun-\ning.\nFirstly, we have performed exact string match\nand found no data leakage in any test data (Hu-\nmanEval/MBPP).\nTo measure the semantic similarity between\ntraining/test tasks, we did the following:\n1. For each task in the test (MBPP/HumanEval)\nwe retrieve two closest training tasks (based\non cosine similarity of starcoder embedding\n& tf-idf vectors of task description).\n2. We use gpt-3.5-turbo-16k to identify whether\nthere is a data leak between a train and test\ninstance by classifying the pair into (“leak”,\n“somewhat similar”, “somewhat not similar”,\n“not related”). We use a prompt with instruc-\ntions and manually created few-shot exam-\nples and ask gpt-3.5 to generate the reasoning\nand categorization. We manually examined\nseveral examples per category to ensure the\nreasoning and judgment is done correctly and\nconsistently.\n3. Map the similarity categories to 0-1 similarity-\nscore (“leak” -> 1, “somewhat similar” ->\n0.75, “somewhat not similar” -> 0.25, “not\nrelated” -> 0) and show the mean score and %\nof cases classified as “leak”. Note that StanD\n& PERsD have 10K & 3K training data re-\nspectively so their scores are different.\nE Results in MBPP-Cleaned\nIn Appendix D, we find 55 data instances that are\npotentially leaked (with similarity score = 1) in\nMBPP test data. In this section, we construct a\nnew MBPP-Cleaned dataset, where the leaked data\npoints are removed (originally 306 problems→251\nproblems after filtering). The results on this new\nMBPP-Cleaned dataset is shown in Table 13. From\n6748\nthe results, we can see for setting CodeGen-mono-\n16B, pass@1, PERsD becomes almost on-par with\nStanD (from a gap of -1.21 to -0.17). For the rest of\n15/16 settings on PERsD comparing with StanD, its\naverage margin is increased from 4.8 points to 5.9\npoints. Besides, PERsD-refine on MBPP-Cleaned\nshows more consistent and sizable improvements\nover InpD-refine, with an average edge of +0.86 for\n1 step inference, and +1.91 for two step inference.\nOverall, with overlapped test data removed, PERsD\nmethods show even larger advantages compared to\nStanD or InpD methods.\n(a) Backbone as CodeGen-mono-6B\nMethods #DataPass@1 Pass@5 Pass@10 Pass@20step=1 step=2 step=1 step=2 step=1 step=2 step=1 step=2MBPP-Cleaned\nStanD 10K 37.51 - 50.89 - 55.15 - 58.87 -InpD 3.3K 38.80 - 53.91 - 58.47 - 62.73 --refine 3.3K 37.58 42.95 57.65 62.29 63.52 67.79 67.92 71.96-combined 6.5K 38.11 43.01 52.69 58.32 57.36 62.75 61.19 66.18PERsD 3.3K 41.30 - 56.20 - 61.86 - 67.53 --refine 3.3K43.86 47.73 59.33 64.41 65.19 69.95 69.62 74.33-combined 6.5K 38.86 43.75 52.78 57.04 57.35 61.78 61.52 66.19\n(b) Backbone as CodeGen-mono-16B\nMethods #DataPass@1 Pass@5 Pass@10 Pass@20step=1 step=2 step=1 step=2 step=1 step=2 step=1 step=2MBPP-Cleaned\nStanD 10K 43.10 - 57.53 - 62.92 - 68.12 -InpD 2.8K 40.64 - 53.88 - 58.82 - 62.88 --refine 2.8K 43.67 49.60 63.14 68.21 69.27 73.28 73.36 76.85-combined 5.6K 41.63 47.77 54.74 62.24 59.67 67.33 63.75 71.57PERsD 2.8K 42.93 - 62.40 - 68.90 - 74.10 --refine 2.8K47.73 52.63 63.62 69.21 69.84 75.17 74.90 79.69-combined 5.6K 46.33 51.67 63.46 68.65 69.49 74.26 74.53 78.83\nTable 13: Comparing performance of PERsD models to\nStanD & InpD on MBPP-Cleaned\n6749",
  "topic": "Distillation",
  "concepts": [
    {
      "name": "Distillation",
      "score": 0.8017128705978394
    },
    {
      "name": "Computer science",
      "score": 0.7071825265884399
    },
    {
      "name": "Code (set theory)",
      "score": 0.6553090810775757
    },
    {
      "name": "Process (computing)",
      "score": 0.6290289163589478
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.598481297492981
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3552192449569702
    },
    {
      "name": "Machine learning",
      "score": 0.34146177768707275
    },
    {
      "name": "Programming language",
      "score": 0.199928879737854
    },
    {
      "name": "Chemistry",
      "score": 0.07097119092941284
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}