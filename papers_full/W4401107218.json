{
    "title": "Sequence-to-sequence translation from mass spectra to peptides with a transformer model",
    "url": "https://openalex.org/W4401107218",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2558650980",
            "name": "Melih YILMAZ",
            "affiliations": [
                "Seattle University",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2464831776",
            "name": "William E. Fondrie",
            "affiliations": [
                "TRIA Bioscience (United States)",
                "Seattle University"
            ]
        },
        {
            "id": "https://openalex.org/A183753122",
            "name": "Wout Bittremieux",
            "affiliations": [
                "University of Antwerp"
            ]
        },
        {
            "id": null,
            "name": "Carlo F. Melendez",
            "affiliations": [
                "University of Washington",
                "Seattle University"
            ]
        },
        {
            "id": "https://openalex.org/A4316160257",
            "name": "Rowan Nelson",
            "affiliations": [
                "Seattle University",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A4316160258",
            "name": "Varun Ananth",
            "affiliations": [
                "Seattle University",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2166897319",
            "name": "Se-woong Oh",
            "affiliations": [
                "Seattle University",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2128170596",
            "name": "William Stafford Noble",
            "affiliations": [
                "Seattle University",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2558650980",
            "name": "Melih YILMAZ",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2464831776",
            "name": "William E. Fondrie",
            "affiliations": [
                "TRIA Bioscience (United States)",
                "Seattle University"
            ]
        },
        {
            "id": "https://openalex.org/A183753122",
            "name": "Wout Bittremieux",
            "affiliations": [
                "University of Antwerp"
            ]
        },
        {
            "id": null,
            "name": "Carlo F. Melendez",
            "affiliations": [
                "University of Washington",
                "Seattle University"
            ]
        },
        {
            "id": "https://openalex.org/A4316160257",
            "name": "Rowan Nelson",
            "affiliations": [
                "Seattle University",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A4316160258",
            "name": "Varun Ananth",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2166897319",
            "name": "Se-woong Oh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2128170596",
            "name": "William Stafford Noble",
            "affiliations": [
                "University of Washington",
                "Seattle University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2519691041",
        "https://openalex.org/W2026465178",
        "https://openalex.org/W2008117982",
        "https://openalex.org/W2749103096",
        "https://openalex.org/W3031716951",
        "https://openalex.org/W3164766217",
        "https://openalex.org/W2132164705",
        "https://openalex.org/W2736859409",
        "https://openalex.org/W2126571169",
        "https://openalex.org/W2001848433",
        "https://openalex.org/W1971887998",
        "https://openalex.org/W1995693931",
        "https://openalex.org/W2038615001",
        "https://openalex.org/W6675824230",
        "https://openalex.org/W752259472",
        "https://openalex.org/W2979770225",
        "https://openalex.org/W3136924813",
        "https://openalex.org/W4234552385",
        "https://openalex.org/W2956118338",
        "https://openalex.org/W2768087917",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4210902118",
        "https://openalex.org/W2888985586",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3203588026",
        "https://openalex.org/W4281911083",
        "https://openalex.org/W1964194772",
        "https://openalex.org/W2053943711",
        "https://openalex.org/W3024570138",
        "https://openalex.org/W2474302342",
        "https://openalex.org/W2947763854",
        "https://openalex.org/W2143210482",
        "https://openalex.org/W2985622460",
        "https://openalex.org/W2289616942",
        "https://openalex.org/W4312095560",
        "https://openalex.org/W4361279726",
        "https://openalex.org/W4389269199",
        "https://openalex.org/W4362670833",
        "https://openalex.org/W4390498794",
        "https://openalex.org/W4391724170",
        "https://openalex.org/W4392242910",
        "https://openalex.org/W2016589492",
        "https://openalex.org/W3011589104",
        "https://openalex.org/W2024668145",
        "https://openalex.org/W2096057003",
        "https://openalex.org/W6892597802",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3035965352",
        "https://openalex.org/W2342249984",
        "https://openalex.org/W6675354045",
        "https://openalex.org/W2993327327",
        "https://openalex.org/W3191740987",
        "https://openalex.org/W2011301426",
        "https://openalex.org/W3150635270",
        "https://openalex.org/W3099878876"
    ],
    "abstract": null,
    "full_text": "Article https://doi.org/10.1038/s41467-024-49731-x\nSequence-to-sequence translation from\nmass spectra to peptides with a\ntransformer model\nMelih Yilmaz 1,5, William E. Fondrie2,5, Wout Bittremieux3,5,\nCarlo F. Melendez4, Rowan Nelson 4, Varun Ananth1, Sewoong Oh1 &\nWilliam Stafford Noble1,4\nA fundamental challenge in mass spectrometry-based proteomics is the\nidentiﬁcation of the peptide that generated each acquired tandem mass\nspectrum. Approaches that leverageknown peptide sequence databases\ncannot detect unexpected peptides and can be impracticalor impossible to\napply in some settings. Thus, the ability to assign peptide sequences to tandem\nmass spectra without prior information— de novo peptide sequencing— is\nvaluable for tasks including antibody sequencing, immunopeptidomics, and\nmetaproteomics. Although many methods have been developed to address\nthis problem, it remains an outstanding challenge in part due to the difﬁculty\nof modeling the irregular data structure of tandem mass spectra. Here, we\ndescribe Casanovo, a machine learning model that uses a transformer neural\nnetwork architecture to translate the sequence of peaks in a tandem mass\nspectrum into the sequence of aminoacids that comprise the generating\npeptide. We train a Casanovo model from 30 million labeled spectra and\ndemonstrate that the model outperformsseveral state-of-the-art methods on a\ncross-species benchmark dataset. We also develop a version of Casanovo that\nis ﬁne-tuned for non-enzymatic peptides. Finally, we demonstrate that Casa-\nnovo’s superior performance improves the analysis of immunopeptidomics\nand metaproteomics experiments and allows us to delve deeper into the dark\nproteome.\nMass spectrometry is currently the most popular analytical technique\nto characterize the proteome, by identifying and quantifying proteins\npresent in complex biological systems1. During a bottom-up mass\nspectrometry proteomics experiment, proteins from a biological\nsample are enzymatically digested into peptides, their intact mass and\ncharge are measured, and they are fragmented using tandem mass\nspectrometry. The fundamental challenge of mass spectrometry pro-\nteomics is then to determine the amino acid sequences of the resulting\ntandem mass (MS/MS) spectra. The standard approach to solve this\nspectrum identiﬁcation problem is sequence database searching,\nduring which peptides are simulated in silico using known digestion\nrules from a database of protein sequences potentially present in the\nbiological samples, typically from a reference proteome. Next, each\nobserved MS/MS spectrum is scored against a list of candidate pep-\ntides based on simpliﬁed peptide fragmentation rules, and the best-\nscoring peptide-spectrum match (PSM) is reported. Pioneered by the\nSEQUEST algorithm\n2, dozens of database search engines have been\nsubsequently developed and are very widely deployed3.\nReceived: 16 January 2023\nAccepted: 18 June 2024\nCheck for updates\n1Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, USA.2Talus Bioscience, Seattle, USA.3Department of\nComputer Science, University of Antwerp, Antwerp, Belgium.4Department of Genome Sciences, University of Washington, Seattle, USA.5These authors\ncontributed equally: Melih Yilmaz, William E. Fondrie, Wout Bittremieux.e-mail: William-noble@uw.edu\nNature Communications|         (2024) 15:6427 1\n1234567890():,;\n1234567890():,;\nHowever, a fundamental requirement for sequence database\nsearching is that the set of proteins that may be present in the sample is\nknown in advance. While this is often the case for samples generated\nfrom species with well-characterized genomes, relying on a database\nprevents the detection of unexpected peptides. Such unexpected\npeptides include not just peptides derived from contaminant proteins\nor that arise due to variability in sample processing\n4, but also biolo-\ngically and clinically relevant peptides, such as peptides that deviate\nfrom the reference proteome due to genetic variation, peptides with\nunexpected post-translational modiﬁcations (PTMs), and peptides\noriginating from foreign sources, such as microbes or consumed\nfoods. Furthermore, there are tasks where generating a peptide data-\nbase can prove impractical or even impossible. For example, the\nantigenic peptides presented by major histocompatibility complex\n(MHC) proteins— the “immunopeptidome”— are often generated from\ntheir parent proteins in an unpredictable manner, requiring at mini-\nmum every possible protein subsequence to be considered\n5–7.\nSimilarly, constructing a peptide database for antibody sequen-\ncing is nearly impossible, due to the sequence variants created by V(D)J\nrecombination\n8. Finally, creating an accurate database for mixtures of\nmany organisms— metaproteomics— such as from microbiome or\nenvironmental samples, is often not feasible9.\nSuch applications require the ability to sequence peptides directly\nfrom the acquired MS/MS spectra de novo. Early de novo peptide\nsequencing algorithms used heuristic search\n10 or dynamic\nprogramming11,12 to propose peptides for the observed MS/MS spectra.\nIn addition to dynamic programming, the PepNovo algorithm 13\nattempted to account for rules governing peptide fragmentation in its\nprobabilistic score function and is closely related to the hidden Mar-\nkov model employed by Fischer et al.\n14 In 2015, the Novor algorithm15\nimproved the state of the art by using a decision tree as the score\nfunction for its dynamic programming algorithm.\nMore recently, as in many otherﬁelds, deep learning has become\nthe preferred solution for de novo peptide sequencing. DeepNovo\n8\ncombines a convolutional neural network and a recurrent neural net-\nwork to predict the subsequent amino acid when provided an MS/MS\nspectrum and a peptide preﬁx. SMSNet\n16 uses a similar network\narchitecture but reconciles the predicted sequences against a user-\nsupplied peptide database. PointNovo17, the successor to DeepNovo,\nleverages an order-invariant network architecture to model high-\nresolution MS/MS spectra\n18. Finally, pNovo 319 ﬁrst generates candidate\npeptides for each MS/MS spectrum using dynamic programming, after\nwhich aﬁnal score is determined by matching the spectrum against a\ntheoretical spectrum for each candidate peptide, simulated using the\npDeep\n20 learning-to-rank framework.\nDespite the advances of these deep learning-based methods for\nde novo peptide sequencing, they still suffer from several limitations.\nDe novo tools typically can only annotate a minority of MS/MS spectra\ncompared to sequence database searching, they struggle with natively\nencoding high-resolution MS/MS data, and they employ complex\nneural network architectures and post-processing steps.\nTo address these issues, here we describe Casanovo, which\nreframes the de novo peptide sequencing task as a machine translation\nproblem: like translating a sequence of words in a sentence from one\nlanguage to another, Casanovo translates a sequence of peaks in an\nMS/MS spectrum into a sequence of amino acids of the generating\npeptide. To do so, we leverage the state-of-the-art architecture for\nmodeling natural language— the transformer\n21. The transformer archi-\ntecture allows Casanovo to directly use them/z and intensity value\npairs that comprise an MS/MS spectrum without discretization of the\nm/z axis and to directly output a predicted peptide sequence without a\ncomplicated dynamic programming step. We have previously22 trained\nCasanovo on a limited collection of mass spectra from the multi-\nspecies benchmark used by Tran et al.\n8 In this work, we present sig-\nniﬁcant improvements to Casanovo and demonstrate its effectiveness\nat tackling common challenges with de novo peptide sequencing. We\nexpand our training set to use 30 million conﬁdent PSMs from the\nMassIVE-KB spectral library23, and we add a beam search decoding\np r o c e d u r et op r e d i c tt h eb e s tp e p t i d ef o re a c hM S / M Ss p e c t r u m .W e\ndemonstrate that, together, these updates signiﬁcantly improve\nCasanovo’s already state-of-the-art performance. Additionally, weﬁne-\ntune a non-enzymatic version of Casanovo for tasks such as immu-\nnopeptidomics. We demonstrate how high-performance de novo\npeptide sequencing using Casanovo enables fast and effective immu-\nnopeptidome analysis, bolsters the characterization of metapro-\nteomes, and sheds light on the dark proteome.\nResults\nA transformer architecture enables processing of raw mass\nspectra\nCasanovo uses a transformer architecture to perform a sequence-to-\nsequence modeling task, from MS/MS spectrum to the generating\npeptide (Fig.1). Transformers are built upon the attention function\n21,\nwhich allows transformer models to contextualize the elements of a\nsequence; transformer models thus learn the relationships of\nsequence elements to one another and how their interactions should\nbe interpreted. As such, the transformer architecture has found suc-\ncess in not only natural language processing, but also applications to\nbiological sequences\n24,25.\nIn Casanovo, each peak in an observed MS/MS spectrum is con-\nsidered as an element in a variable length sequence. Them/z and\nintensity values of each peak are encoded using, respectively, a col-\nlection of sinusoidal functions and a learned linear layer, and these\nencodings are summed. The encoded peaks are then input into the\ntransformer encoder, where context is learned between pairs of peaks\nin the MS/MS spectrum. The contextualized peak encodings are then\nused as input to the transformer decoder for predicting the peptide\nsequence.\nThe process of decoding proceeds in an iterative, autoregressive\nmanner. We begin by providing the mass and charge of the observed\nprecursor. The transformer decoder uses the contextualized peak\nencodings and the precursor information to begin predicting amino\nacids of the peptide. With theﬁrst predicted amino acid, we retain the\ntop k residues, wherek is a user-selected value for the number of\nFig. 1 | Casanovo performs de novo peptide sequencing using a transformer\narchitecture.The peaks from each MS/MS spectrum are contextualized by the\ntransformer encoder. The resulting peak encodings are then fed into the trans-\nformer decoder along with the observed precursor mass and charge to iteratively\ndecode the peptide sequence. Casanovo uses a beam search decoding strategy,\nfollowing the most promising sequence predictions until they terminate or exceed\nthe precursor mass. The highest scoring peptide sequence is returned as the\nputative peptide that generated the MS/MS spectrum.\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 2\nbeams in our beam search. In each subsequent iteration, amino acids\nare added to the decoded peptide sequence, retaining the topk\nsequences until the decoded sequences for all of the beams have ter-\nminated or exceeded the precursor mass. Finally, the sequence with\nthe highest score is retained as the putative peptide that generated the\nprovided MS/MS spectrum.\nIn generating its predictions, Casanovo will inevitably fail to\ngenerate plausible peptides for some MS/MS spectra. For example,\nsome MS/MS spectra contain too few fragment ions to be sequenced\nreliably, or the true generating peptide may bear a modiﬁcation that is\nunknown to Casanovo. We therefore reﬁne the PSMs proposed by\nCasanovo using a simple precursor massﬁlter: any PSMs for which the\nm/z of the peptide falls outside the speciﬁed tolerance of the observed\nprecursor, including potential isotopes, is discarded. Thisﬁlter elim-\ninates many poorly scored peptides from consideration. In our eva-\nluations, PSMs that would normally be removed by thisﬁlter were\nretained and ranked last among all PSMs assigned by Casanovo.\nCasanovo outperforms state-of-the-art methods\nTo evaluate Casanovo, we ﬁrst used the nine-species benchmark\ndataset originally created by Tran et al.8 to compare the performance\nof four de novo peptide sequencing algorithms: Novor, DeepNovo,\nPointNovo, and Casanovo. For these comparisons, we used the pub-\nlicly available, pretrained version of Novor to sequence the MS/MS\nspectra in the benchmark dataset. DeepNovo, PointNovo and Casa-\nnovo were trained in a cross-validated fashion, systematically training\non eight species and testing on the remaining species. For DeepNovo,\nwe used the models trained and provided by Tran et al.\n8 for each of the\ncross-validation splits. For PointNovo, we cross-validated nine models\nfrom scratch using the code and settings provided by Qiao et al.17.T h i s\nbenchmark version of Casanovo, Casanovobm, employs a simple\ngreedy decoding algorithm, rather than beam-search decoding. The\nresults (Fig. 2a) revealed that Casanovo\nbm substantially improved\npeptide-level sequencing performance over Novor, DeepNovo and\nPointNovo, with an average precision of 0.81 for Casaonovo\nbm com-\npared to 0.58, 0.70 and 0.74 for Novor, DeepNovo and PointNovo,\nrespectively. These results are consistent across all nine species in the\nbenchmark dataset (Supplementary Fig. S1).\nWe hypothesized that Casanovo could achieve even better per-\nformance if provided with a larger training set of higher quality PSMs;\nhence, we turned to the MassIVE-KB spectral library of human MS/MS\nproteomics data\n23. MassIVE-KB provided us with a set of 30 million\nhigh conﬁdence PSMs, which we previously collected for training our\nGLEAMS embedding model26. This dataset contains not only a greater\ndiversity of peptides and MS/MS spectra generated from multiple\ninstruments, but also additional types of post-translational modiﬁca-\ntions. We therefore created a new version of the nine-species bench-\nmark dataset using the same nine PRIDE datasets but including seven\ndifferent types of variable modi ﬁcations (methionine oxidation,\nasparagine deamidation, glutamine deamidation, N-terminal acetyla-\ntion, N-terminal carbamylation, N-terminal NH\n3 loss, and the combi-\nnation of N-terminal carbamylation and NH3 loss). In the process, we\nalso ﬁxed several problems that we uncovered in the previous\nbenchmark, including adding consideration of isotope errors and\neliminating peptides that occur in multiple species (see Methods for\ndetails). Theﬁnal, revised benchmark dataset consists of 2.8 million\nPSMs drawn from 343 RAWﬁles.\nThe results from evaluating with respect to this revised bench-\nmark demonstrate the value of training from a much larger collection\nof higher quality PSMs (Fig.2b). When trained on the MassIVE-KB\ndataset, the average precision of Casanovo increases from 0.83 to 0.95.\nFurthermore, Casanovo succeeds in making a larger proportion of\npredictions withm/z values that fall within 30 ppm of the observed\nprecursor (signiﬁed by the location of the diamonds in Fig.2b),\nincreasing from 70% to 97%. Additionally, an analysis of spectrum\nidentiﬁcations for all de novo sequencing tools on the nine-species\nbenchmark dataset shows that correct Casanovo PSMs include almost\nall correct identiﬁcations of the competing de novo sequencing\nmethods, as well as approximately 50% more correct PSMs that are\nunique to Casanovo (Supplementary Fig. S8). This version of Casanovo\nincorporates beam-search decoding, which improves both average\nprecision and coverage compared to greedy decoding for the same\nmodel (Supplementary Fig. S3).\nIn one sense, this comparison is unfair because some of the\nspectra in the new version of the benchmark contain PTMs that cannot\nbe identiﬁed by some of the competing methods. We therefore\neliminated these spectra from each test set and then re-computed the\nprecision-coverage curve. The results (Supplementary Fig. S4) are\nlargely unchanged, suggesting that the PTMs contribute little to the\nobserved overall differences in performance.\nTo better understand why the model trained on MassIVE-KB\noutperforms the one trained on the nine-species benchmark, we\nFig. 2 | Casanovo outperforms PointNovo, DeepNovo, and Novor on a nine-\nspecies benchmark. aCasanovo maintains high peptide-level precision (the pro-\nportion of correctly predicted peptides) across all values of coverage (the pro-\nportion of spectra for which a prediction is made). Each curve is computed by\nsorting predicted peptides for all nine species according to their peptide-level\nconﬁdence scores. For Casanovo, all peptides that pass the precursorm/z ﬁlter are\nranked above peptides that do not pass theﬁlter, and the boundary is indicated by a\ndiamond on each curve. Average precision (AP) corresponds to the area under the\nprecision-coverage curve.b Same asa, but using the revised benchmark and\nincluding a version of Casanovo trained on MassIVE-KB.c Casanovo’s amino acid-\nlevel precision is greatly improved by the expanded training data provided by\nMassIVE-KB. The test set is the revised nine-species benchmark, with PSMs only\ncontaining modiﬁcations considered by both DeepNovo and Casanovo.\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 3\nperformed two follow-up experiments. First, we trained a series of\nCasanovo models on randomly sampled nested subsets of MassIVE-KB,\nranging from 250,000 spectra to the full dataset of 28 million spectra.\nEach model was then evaluated with respect to the revised nine-\nspecies benchmark. The resulting learning curve (Supplementary\nFig. S5) shows that the test set performance depends strongly on the\nsize of the training set, though with diminishing returns after a million\nor so PSMs. Second, we directly compared a Casanovo model trained\nfrom a downsampled MassIVE-KB dataset to Casanovo\nbm which\naverages 9 models cross-validated on the nine-species benchmark,\nwhere the training sets contain approximately the same number of\npeptides (239,697 for Massive-KB and 246,713 for Casanovo\nbm). We\nthen evaluated both models using the revised nine-species benchmark.\nThe results (Supplementary Fig. S6) show that the model trained from\nMassIVE-KB substantially outperforms Casanovo\nbm, with the average\nprecision increasing from 0.83 to 0.90. Thus, these results suggest that\nthe improved performance of the MassIVE-KB model stems primarily\nfrom the improved quality of the data rather than the size of the\ndata set.\nIn addition to evaluating Casanovo’s ability to correctly predict\nwhole peptides, we also evaluated Casanovo’s ability to predict the\nindividual amino acids of each peptide. We did so by ranking amino\nacids by their associated conﬁdence score and then plotting a\nprecision-coverage curve. We compared two versions of Casanovo\n(trained from theﬁrst benchmark and from MassIVE-KB) with Deep-\nNovo and PointNovo on the revised nine-species benchmark with new\nmodiﬁcations eliminated (Fig.2c). The amino acid-level performance\nwas consistent with the trends we observed in peptide-level perfor-\nmance, with Casanovo outperforming Novor, DeepNovo and Point-\nNovo: Casanovo trained on MassIVE-KB achieves a remarkable average\nprecision of 0.98.\nFinally, to characterize the improved de novo sequencing per-\nformance of Casanovo across generating peptides of different lengths\nand precursor charge states, we compare all methods on subsets of the\nrevised nine-species benchmark dataset. First, we divide spectra into\nthree groups by charge state where groups contain precursors with 2+,\n3+ and 4+ or higher charge states each, and plot peptide precision-\ncoverage curves for each group (Supplementary Fig. S9). As expected,\naverage precision is lower across all methods for groups with higher\nprecursor charge states since those spectra tend to have more com-\nplex fragmentation patterns. However, the drop in performance is only\n12% for Casanovo in precursors with 4+ or higher charge states versus\nprecursors with 2+ charge states, thanks to the diversity of precursor\ncharge states in its training data where 11% of precursors have 4+ or\nhigher, whereas average precision for all competing methods decrea-\nses by more than 60%. Second, we bin spectra according to the length\nof their generating peptides into groups of short (fewer than 13 amino\nacids), medium (between 13 and 18 amino acids), and long (greater\nthan 18 amino acids) peptides, and compare de novo sequencing\nperformance in each group (Supplementary Fig. S10). Performance\ndegrades for longer peptides because incorrect amino acid predictions\ntend to accumulate during decoding, but the observed decrease in\naverage precision for Casanovo is much smaller relative to other\nmethods, highlighting Casanovo’s ability to accurately sequence long\npeptides as a key contributor to its improved performance.\nCasanovo unravels the immunopeptidome\nOne important application of de novo peptide sequencing is the\ncharacterization of the peptides presented by major histocompabil-\nitility complexes (MHCs), which is commonly referred to as the\n“immunopeptidome.” These antigen peptides are presented on the\nextracellular surface and serve as targets for immune cell recognition.\nHowever, because these antigen peptides are generated through\nlysosomal or proteasomal degradation, they do not exhibit the char-\nacteristic tryptic termini from most proteomics experiments.\nConsequently, the peptide search space is exponentially larger than\nconsidering only tryptic peptides— every peptide subsequence in a\nprotein within a deﬁned peptide length must be considered. Further-\nmore, mutations in these peptides are of particular interest, because\nthese mutation-containing neoantigens may serve as tumor-speciﬁc\nmarkers to activate T cells and initiate antitumor immune responses.\nUnfortunately, expanding the search space to consider all possible\nmutated peptides is prohibitive both in terms of search speed and\nstatistical power for traditional proteomics search engines.\nAlthough immunopeptidomics is a prime application for de novo\nsequencing, naively applying Casanovo directly to immunopepti-\ndomics data is problematic: the standard Casanovo model is heavily\nbiased to predict tryptic peptides due to their overrepresentation in\nMassIVE-KB. To demonstrate this effect, we analyzedﬁve mass spec-\ntrometry runs generated from MHC class I peptides isolated from\nMDA-MB-231 breast cancer cells\n5 in two different ways:ﬁrst using\nCasanovo and second by searching against a non-enzymatic digestion\nof the human proteome (see Methods). Among the peptides accepted\nat 1% FDR by the database search procedure, we observed a low pro-\nportion of“tryptic”peptides, i.e., peptides with C-terminal amino acids\nof K (1.12%) or R (0.80%). In contrast, among the top-scoring 10% of the\nCasanovo predictions, we observed a greater than six-fold increase in\nthe rate of tryptic peptide predictions (5.87% K and 6.76% R).\nWe hypothesized that we could reduce this tryptic bias and pro-\nduce a version of Casanovo that is better suited to immunopepti-\ndomics data byﬁne tuning our existing model using data that lacks a\ntryptic bias. To create such a dataset, we combined data from two\nsources. First, we segregated PSMs from MassIVE-KB according to their\nC-terminal amino acid and then uniformly sampled up to 50,000\npeptides within each group. For most amino acids, MassIVE-KB con-\ntained fewer than 50,000 PSMs, so for these we supplemented by\nrandomly extracting additional PSMs from the PROSPECT collection\n27\n(Supplementary Table S1). We then split this new collection of 1 million\nPSMs into training, validation, and testing sets. We thenﬁne-tuned our\nexisting Casanovo model by training it until convergence on this non-\nenzymatic training set.\nThe resulting model, Casanovo\nne,p e r f o r m sm a r k e d l yb e t t e rt h a n\nthe original Casanovo model at predicting peptides in our held-out,\nnon-enzymatic test set. On the held-out test set of 100,000 non-\nenzymatic PSMs, Casanovo\nne achieves an average precision of 0.83,\ncompared with 0.60 for the original Casanovo model on the same data\n(Fig. 3a). The predicted C-terminal amino acid frequencies are also\nmuch closer to the true frequencies, with K and R dropping to 1.81%\nand 1.79%, respectively (Fig.3b).\nWe next used Casanovo\nne to sequence the immunopepti-\ndome of MDA-MB-231 breast cancer cells5. For each peptide pre-\ndicted by Casanovo, we investigated whether it (1) occurs\nanywhere within the human proteome, and (2) occurs within the\nset of peptides detected using a database search procedure. We\nﬁrst searched the data against a non-enzymatic digestion of the\nhuman proteome using the Tide search engine\n28 followed by\nPercolator post-processing29, using settings similar to those in the\noriginal study 5 (see Methods). Out of 26,377 unique peptides\npredicted by Casanovo, 2459 match to the human proteome, and\na majority of these overlap with the 1544 unique peptides iden-\ntiﬁed by Tide at 1% FDR (Supplementary Fig. S11). Notably, these\noverlapping peptides are predicted with high con ﬁdence by\nCasanovo, almost all within theﬁrst 10,000 Casanovo predictions\n(Fig. 3c). Casanovo predicts an additional 1148 peptides that\nmatch to the human proteome but are not identiﬁed by Tide at 1%\nFDR, and further analysis shows that 751 (65.4%) of these peptides\ncorrespond to Tide hits that were not accepted at the 1% FDR\nthreshold. To further investiga te the plausibility of Casanovo\npredicted peptides as MHC antigens, we used NetMHCpan-4.1\n30 to\npredict MHC binding af ﬁnity for these peptides. First, we\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 4\ncompared peptides that were identiﬁed by both Casanovo and\ndatabase search with peptides that were predicted only by\nCasanovo and match to the human proteome. These two groups\nexhibit similar distributions of predicted binding afﬁnity proﬁles,\nwith 87% of peptides identiﬁed by Casanovo alone and 86% of\nthose identiﬁed by both methods predicted to be MHC binders at\n500 nM (Fig.3d). In contrast, when we evaluate peptides that are\nidentiﬁed by database search but not by Casanovo, the propor-\ntion of predicted MHC binders drops substantially to 50%. Over-\nall, these results suggest that Casanovo not only identiﬁes more\npeptides matching to the human proteome than the standard\ndatabase search procedure, but the peptides Casanovo predicts\nare also more likely to bind MHC antigens.\nWe also explored an alternative method for comparing Casanovo\nand Tide results, which does not rely on mapping Casanovo predic-\ntions to the reference proteome. For this analysis, we consider the 1497\npeptides identiﬁed by Tide at 1% FDR which yielded valid binding\nafﬁnity predictions from NetMHCpan alongside the top 1497 highest\nconﬁdence Casanovo predictions. The results (Fig.3e) agree with\nthose in Fig.3d: the 960 peptides in common between the two sets of\npeptides achieve the highest proportion of MHC binders (86%), the\nCasanovo-only predictions achieve a slightly lower percentage (80%),\nand the Tide-only predictions have the lowest percentage of MHC\nbinders (70%).\nCasanovo accurately sequences peptides from complex\nmetaproteomes\nProteomics applications extend far beyond the analysis of single\nmodel organisms or well-characterized biological systems. Indeed,\nthere is growing interest in using mass spectrometry proteomics\nmethods to investigate the dynamics of complex biological ecosys-\ntems— whether microbiomes or environmental specimens— for which\nthe identities of its members cannot be known a priori. Due to the\nunknown complexity of the sample and even the lack of reliable\nreference proteomes for the likely species in the sample, these meta-\nproteomics experiments are difﬁcult to analyze. One solution to these\nproblems is to search the spectra against a large database, such as one\ncontaining all the microbial sequences in public databases for a sample\nthat is likely dominated by unknown microbes. This“big database”\napproach is widely used but suffers from a signiﬁcant loss in statistical\npower due to the implicit multiple hypothesis testing correction that\nmust be made to account for the size of the database. An alternative\nsolution involvesﬁrst subjecting the sample to genome sequencing,\nand then using the inferred peptide sequences as the basis for a\n“metapeptide” database. This approach yields better power to detect\npeptides\n31 but requires the availability of a matched DNA sample and\nthe additional cost associated with DNA sequencing.\nWe hypothesized that Casanovo’s improved de novo sequencing\ncapabilities would be useful in both scenarios— either in the presence\nFig. 3 | Fine-tuning reduces Casanovo’s bias for tryptic peptides. aFine-tuning\nCasanovo (Casanovone) improves peptide-level precision on sequencing MS/MS\nspectra generated by non-tryptic peptides.b Casanovone predicts non-tryptic C-\nterminal peptides more readily than the standard Casanovo model, improving\nperformance on the non-enzymatic validation set.c Casanovo detects many pep-\ntides that are present in the human proteome but are not detected via database\nsearch. The dashed dark pink line only includes peptides detected by database\nsearch within the 1% FDR threshold, whereas the solid dark pink line includes all\npeptides from the database search, irrespective of FDR threshold.d The peptides\nproposed by Casanovo generally have higher predicted binding afﬁnities for the\nMHC class I receptor, matching the performance of a Tide database search. The\nvertical bar corresponds to the 500 nM binding afﬁnity below which peptides are\npredicted to be MHC binders.e Similar tod, but considering only the 1497 peptides\nthat are accepted at 1% FDR by Tide which yielded valid binding afﬁnity predictions\nfrom NetMHCpan and a corresponding set of 1497 highest-conﬁdence Casanovo\npeptides.\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 5\nor absence of a metapeptide database. To test this hypothesis, we\napplied Casanovo to data from six previously published ocean meta-\nproteomics samples, three from the Bering Strait and three from the\nChukchi Sea\n31. Critically, these samples were also subjected to DNA\nsequencing; hence, in addition to the non-redundant environmental\ndatabase, we also have a metapeptide database for each sampling\nlocation.\nWe began by measuring the extent to which peptides detected by\nCasanovo occur within the corresponding metapeptide database or\nwithin the larger, non-redundant protein database. Because these\nsamples were digested using trypsin, we used the standard Casanovo\nmodel, trained from the tryptic MassIVE-KB dataset. To control the\nerror rate for the matching of Casanovo predictions to these data-\nbases, we employed a procedure similar to target-decoy competition\nused in the false discovery rate estimation for database search (see\nMethods for details) and only considered as correct Casanovo pep-\ntides found in the corresponding database that fall within the 1% ran-\ndom matching threshold. Using this logic, we obtain much better\npower to detect peptides using Casanovo than using a standard\ndatabase search procedure against the metapeptide database\n(Fig. 4a–b). In particular, when we search the data against the meta-\npeptide database using Tide followed by Percolator\n29, we detect 5623\npeptides at 1% FDR in the Bering Strait data and 2460 peptides in the\nChukchi Sea data. In contrast, if we run Casanovo and accept as correct\nonly peptides that appear in the metapeptide database (subject to our\n1% random matching criterion), then we detect 8277 and 3532 pep-\ntides, respectively, in the two datasets, representing increases of 47%\nand 44%. Casanovo also outperforms database search when we con-\nsider the non-redundant protein database rather than the sample-\nspeciﬁc metapeptide database. We detect 1364 peptides in the Bering\nStrait data and 682 peptides in the Chukchi Sea data at 1% FDR by\nsearching the non-redundant environmental database using Tide and\nPercolator. In comparison, Casanovo predictions,ﬁltered at 1% error\nrate using the environmental database, detect 3425 and 1612 peptides,\nrespectively, representing increases of 151% and 136%, respectively.\nWhen using both metapeptide databases or the non-redundant\nenvironment database, Casanovo detects most of the peptides iden-\ntiﬁed by Tide database search and Percolator, where it respectively\ndetects 71% and 75% of Tide identiﬁcations on metapeptide and non-\nredundant databases, while also detecting a substantial number of\nadditional unique peptides (Supplementary Fig. S12).\nTo validate the peptides that were detected by Casanovo but not\nthe database search, we used the Prosit machine learning tool\n32 to\npredict spectrum peak intensities and retention times for peptide\nidentiﬁcations. First, we compared the cosine similarities between the\nobserved and predicted MS/MS spectrum peak intensities across three\ngroups of peptides: peptides only predicted by Casanovo that mat-\nched to the database with 1% error, peptides detected both by Casa-\nnovo and by Tide and Percolator at 1% FDR, and a control group of\npeptides detected by Tide and Percolator with >10% FDR. The control\ngroup was randomly sampled to be the same size as the Casanovo-only\ngroup. The results (Fig.4c) indicate that the Casanovo-only identiﬁ-\ncations have a high concentration of high cosine similarity peptides,\nsimilar to the overlapping identiﬁcations between Casanovo and\nFig. 4 | Casanovo improves power to detect peptides from metaproteomics\nsamples.Casanovo assigns more peptides matching the metapeptide database and\nthe non-redundant environment database than Tide and Percolator at 1% FDR in\nseawater samples froma the Bering Sea andb the Chukchi Sea. Peptides are ranked\naccording to the Casanovo conﬁdence score, assigning each peptide the maximum\nscore across all three runs from each sampling location. Horizontal lines indicate\nthe total number of distinct peptides detected by Tide+Percolator, searching\nagainst two different databases.c The PSMs assigned by Casanovo at a 1% error rate\nand Tide and Percolator at 1% FDR have high cosine similarities to the predicted MS/\nMS spectra for the respective peptides from Prosit when compared to control PSMs\nsampled from the Tide search results with > 10% FDR. Each group represents the\naggregated results for Bering and Chukchi Sea data, as well as non-redundant\nenvironmental and metapeptide databases.d The PSMs assigned by Casanovo at a\n1% error rate and Tide and Percolator at 1% FDR closely align with the predicted\nretention times from Prosit.\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 6\ndatabase search. This stands in contrast with the control group, which\nexhibits a much broader distribution of cosine similarities.\nSecond, we compared the observed retention times with the\npredicted retention times from Prosit for the same three groups of\npeptides. For each group, we calibrated the predicted retentions times\nto the observed retention times using linear regression (Supplemen-\ntary Fig. S13). We observed that the peptides detected only by Casa-\nnovo and those detected by Casanovo and Tide had a similar slope and\nresulted in similar residual distributions (Fig.4d). When compared\nagainst the control group, the residual distributions for peptides only\ndetected by Casanovo and those detected by Casanovo and Tide are\nclose to zero.\nUltimately, Casanovo does not yet allow us to achieve as much\npower with the non-redundant database as with the metapeptide\ndatabase. For example, for the Bering Strait data, the union of the 3425\npeptides detected using Casanovo and the 1364 peptides detected\nusing database search is 3750, which is fewer than the 5623 peptides\ndetected using the metapeptide database. (The corresponding num-\nbers for the Chukchi Sea data are 1798 and 2460.) This difference is\nperhaps not surprising, because the environmental non-redundant\ndatabase is incomplete: 3715 of the 5623 peptides found by the data-\nbase search procedure in the Bering Strait metapeptide database are\nnot even present in the environmental database. Thus, a rigorous FDR\ncontrol procedure for de novo peptide sequencing is needed in order\nto rescue the many peptides that are correctly detected by Casanovo\nbut cannot be validated by matching to a database.\nCasanovo shines a light on the dark proteome\nThe “dark matter” of mass spectrometry-based proteomics consists of\nMS/MS spectra that are observed repeatedly across experiments but\nconsistently fail to be identiﬁed. In many cases, these MS/MS spectra\nmay have been generated by peptides that are not in the canonical\nhuman proteome, because they represent contaminant peptides,\nresult from non-standard enzymatic cleavage, or contain sequence\nvariants. We hypothesized that Casanovo could shed light on some of\nthis dark matter.\nAccordingly, we applied Casanovo to a collection of MS/MS\nspectra drawn from a previous analysis\n26, in which 511 million human\nspectra from MassIVE were grouped into 60 million clusters, and the\nclusters were systematically analyzed using targeted open modiﬁca-\ntion searching of representative spectra. The analysis yielded a col-\nlection of 39 million unidentiﬁed clusters, containing a total of 207\nmillion MS/MS spectra. For our analysis, we selected 3.4 million of\nthese unidentiﬁed, clustered MS/MS spectra from eight randomly\nselected MassIVE datasets. These MS/MS spectra belong to 573,597\ndistinct clusters. Because we were investigating spectra that had\nalready failed to be identiﬁed using a standard, tryptic pipeline, we\nopted to use the non-enzymatic Casanovo model (Casanovo\nne)t o\nassign a peptide to each selected MS/MS spectrum, eliminating pep-\ntides for which the predictedm/z falls outside the associated mass\nrange. This analysis yielded a total of 1.3 million predicted peptides.\nWe sought to ascertain how well Casanovo had assigned peptide\nsequences to these dark matter clusters by addressing this question in\ntwo complementary ways. First, we identiﬁed all clusters in which a\nplurality (and at least two) of the spectra were assigned to the same\npeptide sequence, and then we mapped those peptides to the human\nreference proteome, allowing at most one amino acid mismatch. The\nﬁrst step of this procedure assigns peptides to 89,250 (16%) of the\nclusters, of which 65% could be matched to the human proteome. The\nclusters identiﬁed in this fashion vary in size, ranging from 2 to\n542 spectra per cluster, but when we limited the above analysis only to\nclusters larger than a certain size, we observed that the shares of\nidentiﬁed clusters more than doubled (Supplementary Fig. S14). Sec-\nond, we performed a complementary analysis,ﬁrst eliminating all\npredicted peptides that do not occur within the human proteome\n(again, allowing one mismatch) and thenﬁnding clusters with two or\nmore spectra assigned the same sequence and no other spectrum\nassigned to a different sequence. This procedure assigns peptides to\n52,523 clusters, corresponding to 9% of all previously unidentiﬁed\nclusters. The overlap between the two approaches— plurality vote fol-\nlowed by proteome matching or vice versa— is high: 98% of the 52,523\nclusters overlapped with the clusters from the previous analysis.\nOverall, Casanovo is able to assign peptides to 196,724 of the 3.4 mil-\nlion unidentiﬁed MS/MS spectra using the combination of these two\nstrategies.\nOne potential reason for an MS/MS spectrum to remain uni-\ndentiﬁed is the presence in the generating peptide sequence of a\ngenetic variant that does not appear in the reference proteome. To\ninvestigate whether Casanovo is identifying such sequences, we\nlooked more closely at the subset of Casanovo cluster assignments\nthat match to the human proteome with a single amino acid mismatch,\nfocusing on the 51,555 assignments that agree between the two\nm e t h o d sd e s c r i b e da b o v e .T w op i e c e so fe v i d e n c es u g g e s t st h a tt h e s e\npeptides are indeed enriched for genetic variants. First, we observe an\nenrichment for amino acid substitutions that can be explained by a\ncorresponding single-nucleotide substitution. Among the Casanovo\npredictions, 83.4% correspond to a potential single-nucleotide sub-\nstitution, compared with only 38.6% of all possible amino acid sub-\nstitutions thatﬁt this criterion. Second, we see a strong enrichment for\nsubstitutions with positive BLOSUM62 scores\n33.T h eB L O S U Ms c o r ei s\nan integerized log-odds score indicating the empirical substitutability\nof one amino acid for another. In the BLOSUM62 matrix, only 11% of the\n380 non-diagonal entries are positive. However, if we rank the\nCasanovo-predicted substitutions by frequency, weﬁnd that the top\nﬁv es u b s t i t u t i o n sh a v eB L O S U Ms c o r e so f1o r2( S u p p l e m e n t a r y\nTable S3). This observation strongly suggests that Casanovo is pre-\ndicting substitutions that are biochemically plausible.\nDiscussion\nCasanovo’s excellent performance derives from two sources: the\navailability of a large, high-quality set of training data, and the use of a\nmachine learning architecture that can make use of that data. Our\nexperiments suggest that the carefully curated MassIVE-KB collection\nprovides particularly good training data. This is likely because the\ndataset was derived from a massive collection of 669 million spectra, in\ncombination with extremely stringent FDR control. In particular, the\ndata were searched at 1 % FDR, after which only the top 100 PSMs for\neach unique precursor were retained, corresponding to 30 million\nhigh-quality PSMs (uniformly 0 % FDR from the original searches).\nThe transformer architecture is uniquely suited to contextualize\nthe elements of variable length sequences and has therefore proven\nimmensely successful in modeling natural language. In comparison to\nrecurrent neural networks, the transformer architecture is able to learn\nlong-range dependencies between the elements of a sequence and can\nbe parallelized for efﬁcient training. By encoding the peaks of a mass\nspectrum as a sequence, similar to tokenizing the words of a sentence,\nCasanovo leverages the strengths of the transformer architecture and\nthe rapid advances pioneered in large language models to improve de\nnovo peptide sequencing from MS/MS spectra\n34.O n ei m p o r t a n to p e n\nquestion, which we leave for future work, is how the number of model\nparameters impacts de novo sequencing performance.\nCasanovo’s utility extends beyond the applications we have\ndemonstrated here. Most obviously, any application in which a peptide\ndatabase is unavailable, incomplete, or extremely large may beneﬁt\nfrom de novo sequencing, such as paleoproteomics, forensics, or\nastrobiology\n35. However, even in the analysis of human or model\norganism data, Casanovo can assist in the detection of“foreign”\nspectra, i.e., spectra generated by peptides that are not present in the\ndatabase. Such foreign spectra might correspond to contaminants\nintroduced during the experiment itself, but they can also represent\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 7\nmicrobial species, genetic variation, or trans-spliced peptides. In gen-\neral, we envision applying Casanovo as a post-processor for spectra\nthat fail to be assigned a peptide during a standard database search\nprocedure, akin to the last stage of a cascade search\n36.\nOne important application of de novo sequencing that we have\nnot yet explored is antibody sequencing. However, a recent publica-\ntion carried out a systematic comparison of six de novo sequencing\ntools, including Casanovo, on the problem of antibody sequencing\n37.\nThe results show that Casanovo strongly outperforms the competing\nmethods by all of the measures that the authors considered. Notably,\nthis comparison employed a version of Casanovo that used greedy\ndecoding and was trained on only 2 million spectra. Hence, our results\n(Fig. 2b) suggest that the version of Casanovo trained from 30 million\nspectra will yield even better antibody sequencing performance.\nWe anticipate many opportunities forﬁne-tuning the Casanovo\nmodel for particular applications. Our analysis with the non-enzymatic\nmodel suggests that Casanovo’s enzymatic bias can be adjusted by\nusing a relatively small amount of training data. Thus, in the short term,\nwe plan to train variants of Casanovo that are appropriate for a variety\nof different cleavage enzymes. The Casanovo software makes such\nﬁne-tuning straightforward, so any user interested in adapting the\nmodel to a particular experimental setting should be able to do so.\nLonger term, an ideal model would take as input a spectrum along with\nrelevant metadata, such as the digestion enzyme, collision energy, and\ninstrument type, and predict accurately for many different types of\nexperimental settings.\nThe potential for deep learning methods to improve our ability to\nperform de novo sequencing has now been widely recognized. While\nthis paper was under review, at least six additional deep learning de\nnovo sequencing methods have been published, including\nGraphNovo\n38,P e p N e t39, Denovo-GCN40, Spectralis41, π-HelixNovo42,\nand NovoB43. Clearly, theﬁeld would beneﬁt from an exhaustive and\nrigorous benchmark comparison of this growingﬁeld of tools.\nOn a related note, at this stage one of the key bottlenecks in the\nﬁeld is the absence of a rigorous method for conﬁdence estimation for\nde novo sequencing. In our metaproteomics analysis, we have mat-\nched Casanovo predictions to a target and corresponding decoy\npeptide database, but such an approach misses out on the power of de\nnovo sequencing to assign peptides to foreign spectra. Thus, an open\nquestion is whether, for a given data-dependent acquisition dataset,\nCasanovo outperforms a standard database search procedure in terms\nof statistical power to detect peptides. Trained from sufﬁciently large\ntraining sets, we may be approaching the end of database searching as\nthe go-to method for analysis of DDA tandem mass spectrometry data.\nMethods\nCasanovo\nCasanovo consists of a transformer encoder and decoder stack as\ndescribed by Vaswani et al.21, which are respectively responsible for\nlearning latent representations of the input spectrum peaks and\ndecoding the amino acid sequence of the spectrum’s generating\npeptide. The encoder takesd-dimensional spectrum peak embed-\ndings as input and outputs d-dimensional latent representation\nvectors for each peak. Subsequently, the decoder takes as input\nthese representations of preﬁx amino acids, coupled with a d-\ndimensional precursor embedding encapsulating precursorm/z and\ncharge information, to predict the next amino acid in the peptide\nsequence. We discuss different aspects of our modeling strategy in\ndetail below.\nSpectrum preprocessing.W ep r e p r o c e s se a c hm a s ss p e c t r u mb y\nremoving noise peaks and scaling the peak intensities before they are\ntransformed into input embeddings for Casanovo. First, we discard\nany peaks outside the range 50–2500m/z,a sw e l la sa n yp e a k sw i t h i n2\nDa of the observed precursor mass. We then remove any peaks with an\nintensity value lower than 1% of the most intense peak’si n t e n s i t y ,a n d\nwe retain up to 150 of the most intense peaks in the spectrum. Finally,\npeaks intensities are square-root transformed and then normalized by\ndividing by the sum of the square-root intensities.\nInput embeddings. Each spectrumS = fðm\nj, IjÞgN\nj =1 is a bag of peaks,\nwhere each peak (mj, Ij) is a 2-tuple representing them/z value and\nintensity of the peak. For the task of de novo peptide sequencing, the\nmost important relationships for our model to learn are how the\nspacing ofm/z values between each pair of peaks corresponds to the\npeptide ions that may have generated them. Secondarily to the spacing\nof m/z values, the intensity of each peak also contains information\nabout the generating ion; for example, y-ions are generally more\nintense than b-ions for some fragmentation methods. Given this prior\nknowledge, we chose embedding methods that would enable Casa-\nnovo to learn from the spacing ofm/z values and that would emphasize\nthe relative importance of these peak attributes for the de novo\nsequencing task.\nWe use aﬁxed, sinusoidal embedding\n21 to project eachm/z value\nto ad-dimensional vector, them/z embeddingf.S p e c iﬁcally, we create\nthe m/z embedding from an equal number of sine and cosine wave-\nforms spanning the wavelengths from 0.001 to 10,000m/z,w h e r e\neach feature in the embeddingfi is a value from one waveform (Sup-\nplementary Fig. S15A). Letλmax be the maximum wavelength,λmin be\nthe minimum wavelength,i be the index of the feature (zero-based),\nand d be the number of features. We begin by deﬁning the number of\nfeatures that are to be represented by sine and cosine waveforms as\ndsin and dcos,r e s p e c t i v e l y :\ndsin = d\n2\n/C24/C25\nð1Þ\ndcos = d /C0 dsin ð2Þ\nThe encoded features are then calculated as:\nf iðmjÞ =\nsin mj= λmin\n2π\nλmax\nλmin\n/C16/C17 i= dsin/C0 1ðÞ/C18/C19/C18/C19\n,f o r i ≤ d=2\ncos mj= λmin\n2π\nλmax\nλmin\n/C16/C17 i/C0 dsinðÞ = dcos/C0 1ðÞ/C18/C19/C18/C19\n,f o r i > d=2\n8\n>>>>\n<\n>>>>\n:\nð3Þ\nwhere λ\nmax = 10,000 andλmin =0 :001 in Casanovo.\nThese input embeddings provide a granular representation of\nhigh-precisionm/z information and, similar to relative positions in the\noriginal transformer model21, may help the model attend tom/z dif-\nferences between peaks, which are critical for identiﬁcation of amino\nacids in the peptide sequence. In them/z embeddings, we chose high-\nfrequency waveforms to capture theﬁne structure present in a mass\nspectrum, such as that introduced by isotopes and near isobaric spe-\ncies. The waveforms then capture more distant relationships as they\nprogress to lower frequencies; thus, if one were to subtract them/z of\none peak from another, the features that are activated depend on the\nscale of their relationship. While consecutive b- and y-ions may activate\none set of features inf, complementary b- and y-ions would likely\nactivate a later set due to their largerm/z difference. Furthermore, the\ncosine similarity between pairs ofm/z embeddings are negatively\ncorrelated with theirm/z (Supplementary Fig. S15B). We postulate that\nthis property preserves information aboutm/z distances— which are\ncritical for de novo peptide sequencing— in a manner that is readily\naccessible to the subsequent transformer layers.\nThe intensity, which is measured with lower precision than them/\nz value, is embedded by projection tod dimensions through a linear\nlayer, after which them/z and intensity embeddings are summed to\nproduce the input peak embedding. However, in developing\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 8\nCasanovo, we found that using a sinusoidal encoding for intensity as\nwell asm/z performs similarly.\nPrecursor information, used as input to the decoder, consists of\nthe total massmprec ∈ R and charge statecprec ∈ { 1 ,...,1 0 }a s s o c i a t e d\nwith the spectrum. We use the same sinusoidal position embedding as\npeak m/z’sf o rmprec; cprec is embedded using an embedding layer, and\nthe embeddings are summed to obtain the input precursor\nembedding.\nModeling de novo peptide sequencing as a sequence-to-\nsequence task. The transformer architecture in Casanovo follows\nthe standard encoder-decoder design of Vaswani et al.\n21.T h ep r o c e s s\nbegins by embedding the peaks of a mass spectrum to obtain input\nembeddings f, which are then contextualized using the transformer\nencoder stack. Thus, a full mass spectrum consisting ofk peaks is\nrepresented as an unordered sequence of peak embeddingsg 2 R\nk × d.\nThe self-attention mechanism of the transformer encoder learns rela-\ntionships between these peaks and outputs a contextualized embed-\nding of each peak in the mass spectrum^g 2 R\nk × d.\nThe decoding process begins byfeeding both the precursor\nembedding and the contextualized spectrum embedding^g into the\ntransformer decoder stack. Decoding proceeds in an autoregressive\nmanner; up to a maximum number of iterations, the decoder stack will\nattempt to predict the next amino acid in the generating peptide from\nC-terminus to N-terminus. During the decoding phase, a learned\nrepresentation of the previously predicted amino acid is concatenated\nto the input into the decoder for each iteration and summed with a\nsinusoidal positional embedding of its position in the sequence. The\noutput of the decoder are scoress 2 R\np × v representing how conﬁdent\nCasanovo is about each amino acid it has predicted for a peptide\nsequence of lengthp and an amino acid vocabulary of sizev.\nTraining and inference strategy. Taking the previously described\nembeddings as input, the transforme ro u t p u t ss c o r e sw h i c ha r et r e a -\nted as a probability distribution over the amino acid vocabulary for the\nnext position in the sequence at each decoding step. The amino acid\nvocabulary includes 20 canonical amino acids (with cysteine con-\nsidered to be carbamidomethylated), post-translationally modiﬁed\nversions of three of them (oxidation of methionine and deamidation of\nasparagine or glutamine), N-terminal modiﬁcations (acetylation, car-\nbamylation, loss of ammonia, and the combination of loss of ammonia\nand carbamylation), plus a specialstop token to signal the end of\ndecoding, yielding a total of 28 tokens. During training, the decoder is\nf e dt h ea m i n oa c i dp r eﬁx for the ground truth peptide following the\nteacher forcing paradigm\n44. Cross-entropy between the model output\nprobabilities and a binary matrix representing the amino acid\nsequence of the ground truth peptide is minimized as the objective\nfunction. During inference, beam search is used toﬁnd the highest-\nscoring predicted peptide sequence, withk a user-speciﬁed value for\nthe number of beams. At each prediction step, for every peptide preﬁx\nconsidered, thek top-scoring amino acids are selected, after which the\nk top-ranked amino acid sequences are used for the subsequent\ndecoding step. Beams are terminated when thestop token is pre-\ndicted, the predicted peptide mass is similar to (given the precursor\nmass tolerance) or exceeds the precursor mass, or the pre-deﬁned\nmaximum peptide length ofℓ = 100 amino acids is reached. Asﬁnal\nprediction, the top-scoring peptide sequence thatﬁts the precursor\nmass tolerance (optionally accounting for isotope offsets) is selected.\nIf no peptide predictionﬁts the precursor mass tolerance, the top-\nscoring peptide sequence with a non-matching peptide mass is\nselected.\nModel and training hyperparameters. We train models with nine\nlayers, embedding sized = 512, and eight attention heads, yielding a\ntotal of ~47M model parameters. A batch size of 32 spectra and 10\n−5\nweight decay is used during training, with a peak learning rate of\n5×1 0−4. The learning rate is linearly increased from zero to its peak\nvalue in 100,000 warm-up steps, followed by a cosine shaped decay.\nThe MassIVE-KB model was trained for a single epoch on 30 million\nPSMs from the MassIVE-KB dataset, which took approximately 8 days\non 4 RTX 2080 Ti GPUs, while evaluating the performance on the\nvalidation set after every 50,000 iterations. Theﬁnal model weights\nwere taken from the snapshot with the lowest validation loss. This\nmodel wasﬁne-tuned on the non-enzymatic training dataset using a\npeak learning rate of 5 × 10\n−5.W es e l e c t e dt h eﬁne-tuned model with\nthe minimal validation loss, which occurred after 5 epochs. These\nmodel hyperparameters— number of layers, embedding size, number\nof attention heads, and learning rate schedule— are used for all\ndownstream experiments unless otherwise speciﬁed.\nPrecursor m/z ﬁltering. A critical constraint in de novo peptide\nsequencing requires the difference between the total mass of the\npredicted peptidem\npred and the observed precursor massmprec of the\nspectrum to be smaller than a threshold valueϵ (speciﬁed in ppm) for\nthe predicted sequence to be plausible:Δmppm =\njmprec/C0 mpredj ×1 06\nmprec\n<ϵ.\nTherefore, in addition to providing precursor information as an input\nfor the model to learn from, weﬁlter out peptide predictions that do\nnot satisfy the above constraint. The threshold valueϵ is a property of\nthe mass spectrometer that the data is collected with, and hence is\nknown at inference time. Accordingly, we chooseϵ based on the pre-\ncursor mass error tolerance used in the database search to obtain\nground truth peptide sequences for the test data.\nDatasets\nMassIVE-KB dataset. A large-scale, heterogeneous dataset derived\nfrom the MassIVE knowledge base (MassIVE-KB; v.2018-06-15) was\nused to develop Casanovo\n23. The MassIVE-KB data set consists of 31 TB\nof human data from 227 public proteomics datasets, containing over\n669 million MS/MS spectra. MassIVE-KB contains a designated subset\nof 30,506,973“high quality” PSMs, identiﬁed by applying a strict ( ~0%)\nPSM-level FDRﬁlter and then selecting at most 100 PSMs for each\ncombination of peptide sequence and charge. These 30 million PSMs\nwere randomly split so that the training, validation and test sets are\ndisjoint at the peptide-level and consist of approximately 28 million\ntraining PSMs, 1 million validation PSMs, and 1 million test PSMs.\nNine-species benchmark dataset. We created a new version of the\nnine-species benchmark originally described by Tran et al.\n8 To do so, we\ndownloaded the RAWﬁles from the same nine PRIDE projects (Sup-\nplementary Table S2) and converted them to MGF format using the\nThermoRawFileParser v1.3.4. We also downloaded the corresponding\nnine Uniprot reference proteomes and constructed a Tide index for\neach one, using Crux version 4.1. For one species (Vigna mungo), no\nreference proteome is available, so we used the proteome of the closely\nrelated speciesVigna radiata.W es p e c iﬁed Cys carbamidomethylation\na sas t a t i cm o d iﬁcation and allowed for the following variable mod-\niﬁcations: Met oxidation, Asn deamidation, Gln deamidation, N-term\nacetylation, N-term carbamylation, N-term NH\n3 loss, and the combina-\ntion of N-term carbamylation and NH3 loss by using the tide-index\noptions -mods-spec 1M+15.994915, 1N+0.984016, 1Q+0.984016\n–nterm-peptide-mods-spec 1X+ 42.010565, 1X+43.005814, 1X-\n17.026549, 1X+25.980265–max-mods 3. Note that one of the nine\nexperiments (Mus musculus) was performed using SILAC labeling, but\nwe searched without SILAC modiﬁcations and hence include in the\nbenchmark only PSMs from unlabeled peptides. Each index also con-\ntains a shufﬂed decoy peptide corresponding to each target peptide.\nEach MGFﬁle was searched against the corresponding index using the\nprecursor window size and fragment bin tolerance speciﬁed in the ori-\nginal study (Supplementary Table S2). We used XCorr scoring with\nTailor calibration\n45, and we allowed for 1 isotope error in the selection of\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 9\ncandidate peptides. All search results were then analyzed jointly per\nspecies using the Crux implementation of Percolator, with default\nparameters. For the benchmark, we retained all PSMs with Percolator q\nvalue < 0.01. We identiﬁed 13 MGFﬁles with fewer than 100 accepted\nPSMs, and we eliminated all of these PSMs from the benchmark. We then\npost-processed the PSMs to eliminate peptides that are shared between\nspecies. Among the 229,984 unique peptides, we identiﬁed 3797 (1.7%)\nthat occur in more than one species. For each such peptide, we selected\none of the associated species at random and then eliminated all PSMs\ncontaining that peptide in other species. Note that when identifying\nshared peptides between species, we considered all modiﬁed forms of a\ngiven peptide sequence to be the same. Hence, if a given peptide\nappears in more than one species, then that peptide, including all its\nmodiﬁed forms, is randomly assigned to a single species and eliminated\nfrom the others. Theﬁnal benchmark dataset consists of 2.8 million\nPSMs drawn from 343 RAWﬁles. The revised nine-species benchmark is\navailable on MassIVE athttps://doi.org/10.25345/C52V2CK8J.\nEvaluation metrics\nWe use precision calculated at the amino acid and peptide levels8,11,13 as\na function of coverage over the test set as performance measures to\nevaluate the quality of a given model’s predictions. In each case, for\neach spectrum we compare the predicted sequence to the ground\ntruth peptide from the database search. Following Tran et al.\n8, for the\namino acid-level measures weﬁrst calculate the numberNa\nmatch of\nmatched amino acid predictions, deﬁned as all predicted amino acids\nwhich (1) differ by < 0.1 Da in mass from the corresponding ground\ntruth amino acid, and (2) have either a preﬁxo rs u fﬁx that differs by no\nmore than 0.5 Da in mass from the corresponding amino acid\nsequence in the ground truth peptide. We then deﬁne amino acid-level\nprecision as N\na\nmatch=Na\npred,w h e r eNa\npred is the number of predicted\namino acids. For peptide predictions, a predicted peptide is con-\nsidered a correct match if all of its amino acids are matched. Among a\ncollection ofN\np\norig spectra, if our model makes predictions on a subset\nof Np\npred and correctly predictsNp\nmatch peptides, we deﬁne coverage as\nNp\npred=Np\norig and peptide-level precision as Np\nmatch=Np\npred.T op l o ta\nprecision-coverage curve, we sort predictions by the conﬁdence score\nprovided by the model. Amino acid-level conﬁdence scores are\nobtained by applying a softmax to the output of the transformer\ndecoder, which is a proxy for the probability of each predicted amino\nacid to occur in the given position along the peptide sequence. Casa-\nnovo reports the mean probability score over all amino acids as a\npeptide-level conﬁdence score, and reports a modiﬁed score at the\namino acid level, computed as the mean of the peptide score and the\nindividual amino acid probability score.\nCompeting methods\nWe downloaded DeepNovo weights fromhttps://github.com/nh2tran/\nDeepNovo/tree/PNASon Sep 6, 2022. Similar to Casanovobm,D e e p -\nNovo and PointNovo were trained in a cross-validated fashion using\nthe original nine-species benchmark, systematically training on eight\nspecies and testing on the remaining species. Accordingly, nine dif-\nferent sets of pre-trained DeepNovo weights were available, and the\ncorresponding set of weights were used for testing on each species\ndata set. In the absence of pre-trained PointNovo weights, we cross-\nvalidated nine models from scratch by training on eight species and\ntesting on the remaining species. We downloaded the PointNovo code\nprovided by Qiao et al.\n17 from https://zenodo.org/records/3960823on\nMar 27, 2023.\nWe downloaded Novor v1.05.0573 from https://github.com/\ncompomics/searchgui/tree/master/resources/Novoron Dec 3, 2022.\nCreating a non-enzymatic dataset\nTo create a dataset of PSMs that does not exhibit tryptic bias, we\nselected PSMs with a uniform distribution of amino acids at the\nC-terminal peptide positions from two datasets: MassIVE-KB23 and\nPROSPECT27. The MassIVE-KB dataset contains 30 million PSMs\nand consists entirely of data generated using trypsin; hence, only\na small proportion of the MassIVE-KB peptides do not end in K or\nR, corresponding to those that appear at the C-terminus of the\ncorresponding protein. The PROSPECT dataset is a collection of\n61 million PSMs generated from synthetic peptides. We per-\nformed three ﬁltering steps on this dataset: (1) removed duplicate\npeaks with identical m/z values from each spectrum, (2) elimi-\nnated spectra with fewer than 20 peaks, and (3) eliminated\nspectra with Andromeda score less than 70, selecting the highest-\nscoring peptide for each spectrum. To avoid over-representation\nof some peptides in this dataset, we randomly selected at most\n100 PSMs per unique peptide, similar to the processing that was\ndone during the creation of the MassIVE-KB dataset. This pre-\nselection step reduced the size of the PROSPECT dataset to 12.6\nmillion PSMs. Finally, to create a non-enzymatic dataset contain-\ning 1 million peptides, we selected 50,000 PSMs for each cano-\nnical amino acid. These PSMs were selected at random from\nMassIVE-KB, then supplemented as necessary from PROSPECT to\nobtain the desired count (Supplementary Table S1). This dataset\ncontained PSMs from 247,859 unique peptides, which were ran-\ndomly split into training, validation and test sets in the ratio 80/\n10/10. The non-enzymatic dataset with the training, validation\na n dt e s ts p l i t si sa v a i l a b l eo nM a s s I V Ea thttps://doi.org/10.25345/\nC5KS6JG0W.\nImmunopeptidome analysis\nFor the analysis of the MHC class I peptides, we used the Tide\nsearch engine and adopted search settings from the original\npublication\n5.T oc r e a t et h ep e p t i d ei n d e x ,w er a nt i d e - i n d e x\nallowing M oxidation or phosphorylation of S/T/Y, with a max-\nimum of one modiﬁcation per peptide. We set the digestion to be\n“non-speciﬁc-digest,” allowed zero missed cleavages, and speci-\nﬁed a peptide length range of 8 – 15 amino acids. Using the\ncanonical human reference proteome downloaded from Uniprot\non July 17, 2022, the resulting index contains 286,319,284 pep-\ntides and an equal number of shufﬂed decoy peptides. We sear-\nched the data using the tide-index command, specifying a\nprecursor window size of 30 ppm and using Tailor calibration.\nThe resulting sets of PSMs from allﬁve runs were analyzed jointly\nusing Percolator with default settings. All of the above commands\nwere implemented within Crux\n46 version 4.1-2fab3c91-2022-11-09.\nFor comparative analysis between Casanovo predictions and\ndatabase search results, only sequences within a peptide length\nrange of 9– 15 amino acids were considered. NetMHCpan-4.1 was\nused to predict MHC binding afﬁnities for peptide sequences\n30.\nBinding afﬁnity was predicted for 9-mer amino acid motifs in\nreference to the HLA-A02:01, HLA-A02:17, HLA-B41:01, HLA-\nB40:02, HLA-C02:02, and HLA-C17:01 alleles of the MHC\nmolecule.\nMetaproteomics analysis\nWe analyzed data from six mass spectrometry runs, three repli-\ncates each from the Bering Strait (BSt) and the Chukchi Sea (CS)\n31,\ndownloaded in mzXML format fromhttps://noble.gs.washington.\nedu/proj/metapeptide . From the same URL, we also downloaded\nthe two corresponding metapeptide databases, and we down-\nloaded the environmental non-redundant database (env_nr) from\nNCBI on Nov 12, 2022. We used Tide to build three peptide\nindices from the two metapeptide databases and from env_nr\nwith default parameters, except we allowed three methionine\noxidations per peptide and up to 1 missed cleavage. The resulting\nindices contained 41,665,963 peptides (CS), 34,116,884 peptides\n(BSt), and 310,021,565 peptides (env_nr), respectively, as well as\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 10\nan equal number of shufﬂed decoy peptides. Each mzXMLﬁle was\nsearched against the relevant metapeptide database and against\ne n v _ n ru s i n gt h et i d e - s e a r c hc o m m a n d ,s p e c i f y i n gap r e c u r s o r\nwindow of 10 ppm, allowing one isotope error, and using Tailor\ncalibration. Finally, we used Percolator\n29 with default parameters\nto jointly analyze the search results from each set of three runs\nagainst the same peptide index. All of the above commands were\nimplemented within Crux\n46 version 4.1-2fab3c91-2022-11-09.\nMetaproteomics spectrum ﬁles and peptides detected via data-\nbase search are available on MassIVE with the dataset identiﬁer\nMSV000094709 at https://doi.org/10.25345/C5ST7F78Z .\nTo estimate the error rate for the matching of Casanovo\npredicted peptides against a protein database, we developed a\np r o c e d u r ea k i nt ot h et a r g e t - d e c o yc o m p e t i t i o nu s e di nf a l s e\ndiscovery rate estimation for mass spectrometry database\nsearch\n47. To do so, we created a decoy protein database by ran-\ndomly shufﬂing each protein sequence. We then asked whether\neach Casanovo prediction appears in the target (i.e., unshufﬂed)\nor decoy protein list, marking each predicted peptide as match-\ning to the target, decoy, or neither. Peptides assigned to both\nwere randomly assigned to either the target or decoy. We then\nsegregated the peptides by length, and for each length we sorted\nthe Casanovo predictions by conﬁdence score. At each positionk\nin the ranked list, we estimate d the rate of random matching\namong the target matches asD\nk/Tk,w h e r eDk (respectively, Tk)i s\nthe number of decoys (respectively, targets) with rank smaller\nthan k. We then selected the largest value ofk such thatDk/Tk < α,\nfor some speci ﬁed random matching rate α.I nt h i sw o r k ,w e\nused α =0 . 0 1 .\nW eu s e dt h eo n l i n ev e r s i o no fP r o s i t32 at https://www.\nproteomicsdb.org/prosit/to predict peak intensities and retention\ntimes using the Prosit_2020_intensity_hcd and Prosit_2019_irt models,\nrespectively. Aﬁxed collision energy of 27 was used for all peptides\nbased on metadata from the spectrumﬁles. Spectrum peaks predicted\nby Prosit were matched to observed peaks using a 0.05 Da fragmentm/\nz tolerance to calculate the cosine similarity between the predicted and\nexperimental spectra.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nThe revised nine-species benchmark is available on MassIVE with the\ndataset identi ﬁer MSV000090982 at https://doi.org/10.25345/\nC52V2CK8J. The non-enzymatic dataset with training, validation and\ntest splits is available on MassIVE with the dataset identi ﬁer\nMSV000094014 at https://doi.org/10.25345/C5KS6JG0W. Metapro-\nteomics spectrumﬁles and peptides detected via database search are\navailable on MassIVE with the dataset identiﬁer MSV000094709 at\nhttps://doi.org/10.25345/C5ST7F78Z. The spectrum identiﬁcations for\nCasanovo and other evaluated tools on the revised nine-species\nbenchmark are available on MassIVE with the dataset identiﬁer\nMSV000094434 at https://doi.org/10.25345/C5B56DG2T. Similarly,\nCasanovo-only spectrum identiﬁcations for the immunopeptidomics,\nmetaproteomics, and dark matter analyses are available on MassIVE\nwith the dataset identiﬁer MSV000093980 at https://doi.org/10.\n25345/C5VD6PG45. Source data are provided with this paper.\nCode availability\nCasanovo’s source code and trained model weights from the MassIVE-\nKB training and non-enzymaticﬁne tuning are available under the\nApache 2.0 license at https://github.com/Noble-Lab/casanovo48.\nModel weights from the nine-species benchmark training are available\nat https://doi.org/10.5281/zenodo.10694984. The transformer model\nwas implemented using PyTorch\n49 and PyTorch Lightning50. Addi-\ntionally, NumPy51, Pandas52, and Scikit-Learn53 were used for scientiﬁc\ndata processing; and spectrum_utils54,P y t e o m i c s49,a n dp p x55 were\nused to process MS/MS data. Matplotlib56 and Seaborn57 were used for\nvisualization purposes.\nCasanovo’s source code and trained model weights are available\nunder the Apache 2.0 license at https://github.com/Noble-Lab/\ncasanovo. Casanovo, Casanovobm and Casanovone are all trained\nusing version 4.0.1.\nReferences\n1. Aebersold, R. & Mann, M. Mass-spectrometric exploration of pro-\nteome structure and function.Nature 537,3 4 7–355 (2016).\n2. Eng, J. K., McCormack, A. L. & Yates, J. R. An approach to correlate\ntandem mass spectral data of peptides with amino acid sequences\nin a protein database.J .A m .S o c .M a s sS p e c t r o m .5,\n976–989 (1994).\n3 . E n g ,J .K . ,S e a r l e ,B .C . ,C l a u s e r ,K .R .&T a b b ,D .L .Af a c ei nt h e\ncrowd: recognizing peptides through database search.Molecular\nand Cellular Proteomics10 (2011).\n4. Bittremieux, W. et al. Quality control in mass spectrometry-based\nproteomics.Mass Spectrom. Rev.37,6 9 7–711 (2018).\n5. Stopfer, L. E., Mesﬁn, J. M., Joughin, B. A., Lauffenburger, D. A. &\nWhite, F. M. Multiplexed relative and absolute quantitative immu-\nnopeptidomics reveals MHC I repertoire alterations induced by\nCDK4/6 inhibition.Nat. Commun.11,1 –14 (2020).\n6. Mayer, R. L. & Impens, F. Immunopeptidomics for next-generation\nbacterial vaccine development.Trends Microbiol.29,\n1034–1045 (2021).\n7. Hunt, D. F. et al. Characterization of peptides bound to the class I\nMHC molecule HLA-A2.1 by mass spectrometry.Sci. (N. Y., N. Y.)\n255,1 2 6 1–1263 (1992).\n8. Tran, N. H., Zhang, X., Xin, L., Shan, B. & Li, M. De novo peptide\nsequencing by deep learning.P r o c .N a t l .A c a d .S c i .U S A31,\n8247–8252 (2017).\n9. Muth, T., Benndorf, D., Reichl, U., Rapp, E. & Martens, L. Searching\nfor a needle in a stack of needles:challenges in metaproteomics\ndata analysis.Mol. Biosyst.9,5 7 8–585 (2013).\n10. Taylor, J. A. & Johnson, R. S. Sequence database searches viade\nnovo peptide sequencing by tandem mass spectrometry.Rapid\nCommun. Mass Spectrom.11,1 0 6 7–1075 (1997).\n1 1 . M a ,B .e ta l .P E A K S :p o w e r f u ls o f t w a r ef o rp e p t i d ede novo\nsequencing by tandem mass spectrometry.Rapid Commun. Mass\nSpectrom.17, 2337–2342 (2003).\n1 2 . D a n c i k ,V . ,A d d o n a ,T . ,C l a u s e r ,K . ,V a t h ,J .&P e v z n e r ,P .De novo\npeptide sequencing via tandem mass spectrometry.J. Comput.\nBiol. 6,3 2 7–\n342 (1999).\n13. Frank, A. & Pevzner, P. PepNovo: de novo peptide sequencing via\nprobabilistic network modeling.Anal. Chem.77, 964–973 (2005).\n14. Fischer, B. et al. A hidden Markov model for de novo peptide\nsequencing.Adv. Neural Inf. Process. Syst.17,4 5 7–464 (2005).\n15. Ma, B. Novor: Real-time peptide de novo sequencing software.J.\nA m .S o c .M a s sS p e c t r o m .26, 1885–1894 (2015).\n16. Karunratanakul, K., Tang, H.-Y., Speicher, D. W., Chuangsuwanich, E.\n& Sriswasdi, S. Uncovering thousands of new peptides with\nsequence-mask-search hybrid de novo peptide sequencing frame-\nwork. Mol. Cell. Proteom.18,2 4 7 8–2491 (2019).\n17. Qiao, R. et al. Computationally instrument-resolution-independent\nde novo peptide sequencing for high-resolution devices.Nat. Mach.\nIntell. 3,4 2 0–425 (2021).\n18. Qi, C. R., Su, H., Mo, K. & Guibas, L. J. PointNet: deep learning on\npoint sets for 3D classiﬁcation and segmentation. InProceedings of\nthe IEEE Conference On Computer Vision and Pattern Recognition,\n652–660 (2016).\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 11\n19. Yang, H., Chi, H., Zeng, W., Zhou, W. & He, S. pNovo 3: precise de\nnovo peptide sequencing using a learning-to-rank framework.\nBioinformatics35,i 8 3–i90 (2019).\n20. Zhou, X. et al. pDeep: predicting MS/MS spectra of peptides with\ndeep learning.Anal. Chem.89, 12690–12697 (2017).\n21. Vaswani, A. et al. Attention is all you need.Advances in Neural\nInformation Processing Systems30 (2017).\n2 2 . Y i l m a z ,M . ,F o n d r i e ,W .E . ,B i t t r e m i e u x ,W . ,O h ,S .&N o b l e ,W .S .De\nnovo mass spectrometry peptide sequencing with a transformer\nmodel. InProceedings of the International Conference on Machine\nLearning,2 5 5 1 4–25522 (2022).\n23. Wang, M. et al. Assembling the community-scale discoverable\nhuman proteome.Cell Syst.7,4 1 2–421.e5 (2018).\n24. Rives, A. et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences.\nProc. Natl. Acad. Sci. USA118, e2016239118 (2021).\n25. Avsec, Z. et al. Effective gene expression prediction from sequence\nby integrating long-range interactions.Nat. Methods18,\n1196–1203 (2021).\n2 6 . B i t t r e m i e u x ,W . ,M a y ,D .H . ,B i l m e s ,J .&N o b l e ,W .S .Al e a r n e d\nembedding for efﬁcient joint analysis of millions of mass spectra.\nNat. Methods19,6 7 5–678 (2022).\n27. Shouman, O., Gabriel, W., Giurcoiu, V.-G., Sternlicht, V. & Wilhelm,\nM. Prospect: Labeled tandem mass spectrometry dataset for\nmachine learning in proteomics. InThirty-sixth Conference on\nNeural Information Processing Systems Datasets and Benchmarks\nTrack (2022).\n28. Diament, B. & Noble, W. S. Faster SEQUEST searching for peptide\nidentiﬁcation from tandem mass spectra.J. Proteome Res.10,\n3871–3879 (2011).\n2 9 . K ä l l ,L . ,C a n t e r b u r y ,J .D . ,W e s t o n ,J . ,N o b l e ,W .S .&M a c C o s s ,M .J .\nSemi-supervised learning for peptide identiﬁcation from shotgun\nproteomics datasets.Nat. Methods4,9 2 3–925 (2007).\n30. Reynisson, B., Alvarez, B., Paul, S., Peters, B. & Nielsen, M.\nNetMHCpan-4.1 and NetMHCIIpan-4.0: improved predictions of\nMHC antigen presentation by concurrent motif deconvolution and\nintegration of MS MHC eluted ligand data.Nucleic Acids Res.48,\nW449–W454 (2020).\n3 1 . M a y ,D .H .e ta l .A na l i g n m e n t - f r e e“metapeptide” strategy for\nmetaproteomic characterization of microbiome samples using\nshotgun metagenomic sequencing.J. Proteome Res.\n15,\n2697–2705 (2016).\n32. Gessulat, S. et al. Prosit: proteome-wide prediction of peptide\ntandem mass spectra by deep learning.Nat. Methods16,\n509 (2019).\n33. Henikoff, S. & Henikoff, J. G. Amino acid substitution matrices from\nprotein blocks.P r o c .N a t l .A c a d .S c i .U S A89,1 0 9 1 5–10919 (1992).\n34. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1,4 1 7 1–4186 (2019).\n35. Johnson, R. S. et al. Assessing protein sequence database suitability\nusing de novo sequencing.Mol. Cell. Proteom.19,1 9 8–208 (2020).\n36. Kertesz-Farkas, A., Keich, U. & Noble, W. S. Tandem mass spectrum\nidentiﬁcation via cascaded search.J. Proteome Res.14,\n3027–3038 (2015).\n37. Beslic, D., Tscheuschner, G., Renard, B. Y., Weller, M. G. & Muth, T.\nComprehensive evaluation of peptide de novo sequencing tools for\nmonoclonal antibody assembly.Brieﬁngs in Bioinoformatics(2022).\nAdvance online access.\n38. Mao, Z., Zhang, R., Xin, L. & Li, M. Mitigating the missing fragmen-\ntation problem in de novo peptide sequencing with a two stage\ngraph-based deep learning model.Nature Machine Intelligence\n5 (2023).\n3 9 . L i u ,K . ,Y e ,Y . ,L i ,S .&T a n g ,H .A c c u r a t ed en o v op e p t i d es e q u e n c i n g\nusing fully convolutional neural networks.Nat. Commun.14,7 9 7 4\n(2023).\n40. Wu, R., Zhang, X., Wang, R. & Wang, H. Denovo-GCN: De novo\npeptide sequencing by graph convolutional neural networks.\nApplied Sciences13 (2023).\n41. Klaproth-Andrade, D. et al. Deep learning-driven fragment ion ser-\nies classiﬁcation enables highly precise and sensitive de novo\npeptide sequencing.Nat. Commun.15,1 5 1( 2 0 2 4 ) .\n42. Yang, T. et al. Introducingπ-HelixNovo for practical large-scale\nde novo peptide sequencing.Brief. Bioinforma.25,b b a e 0 2 1\n(2024).\n43. Lee, S. & Kim, H. Bidirectional de novo peptide sequencing using a\ntransformer model.PLOS Computational Biol.20, e1011892 (2024).\n44. Williams, R. J. & Zipser, D. A learning algorithm for continually\nrunning fully recurrent neural networks.Neural Comput.1,\n270–280 (1989).\n45. Sulimov, P. & Kertész-Farkas, A. Tailor: A nonparametric and rapid\nscore calibration method for database search-based peptide iden-\ntiﬁ\ncation in shotgun proteomics.J. Proteome Res.19,\n1481–1490 (2020).\n46. Park, C. Y., Klammer, A. A., Käll, L., MacCoss, M. P. & Noble, W. S.\nRapid and accurate peptide identiﬁcation from tandem mass\nspectra.J. Proteome Res.7,3 0 2 2–3027 (2008).\n4 7 . E l i a s ,J .E .&G y g i ,S .P .T a r g e t - d e c o ys e a r c hs t r a t e g yf o ri n c r e a s e d\nconﬁdence in large-scale protein identiﬁcations by mass spectro-\nmetry. Nat. Methods4,2 0 7–214 (2007).\n48. Yilmaz, M. Noble-lab/casanovo (2023).https://doi.org/10.5281/\nzenodo.11205039.\n49. Paszke, A. et al. Pytorch: An imperative style, high-performance\ndeep learning library. InAdvances in Neural Information Processing\nSystems 32,8 0 2 4–8035 (Curran Associates, Inc., Vancouver,\nCanada, 2019).\n50. Falcon, W. & Team, T. PyTorch Lightning the lightweight PyTorch\nwrapper for high-performance AI research. scale your models, not\nthe boilerplate (2019).\n51. Harris, C. R. et al. Array programming with NumPy.Nature 585,\n357–362 (2020).\n52. McKinney, W. Data structures for statistical computing in Python. in\nProceedings of the 9th Python in Science Conference (eds. van der\nWalt, S. & Millman, J.) 51–56 (Austin, Texas, USA, 2020).\n53. Pedregosa, F. et al. Scikit-learn: Machine Learning in Python.J.\nMach. Learn. Res.12, 2825–2830 (2011).\n54. Bittremieux, W. spectrum_utils: A python package for mass spec-\ntrometry data processing and visualization.Anal. Chem.92,\n659–661 (2020).\n55. Fondrie, W., Bittremieux, W. & Noble, W. S. ppx: Programmatic\naccess to proteomics data repositories.J. Proteome Res.20,\n4621–4624 (2021).\n56. Hunter, J. D. Matplotlib: A 2D graphics environment.Comput. Sci.\nEng. 9,9 0–95 (2007).\n57. Waskom, M. L. seaborn: Statistical data visualization.J. Open\nSource Softw.6, 3021 (2021).\nAcknowledgements\nThis work is in part supported by CCF-2019844 (S.O.) as a part of the NSF\nInstitute for Foundations of Machine Learning (IFML) as well as by\nNational Science Foundation award 2245300 (W.S.N.).\nAuthor contributions\nW.S.N., M.Y., W.B., and W.E.F. conceptualized the work. W.S.N. and S.O.\nsupervised the work. M.Y., W.B., and W.E.F. developed the Casanovo\nsoftware. M.Y., C.F.M., V.A., W.B., and R.N. carried out the analyses.\nW.S.N., M.Y., W.B., and W.E.F. wrote the manuscript. All authors\nreviewed and edited the manuscript.\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 12\nCompeting interests\nThe authors declare no conﬂicts of interest.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-49731-x.\nCorrespondenceand requests for materials should be addressed to\nWilliam Stafford Noble.\nPeer review informationNature Communicationsthanks Ekapol\nChuangsuwanich, and the other, anonymous, reviewer(s) for their con-\ntribution to the peer review of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024\nArticle https://doi.org/10.1038/s41467-024-49731-x\nNature Communications|         (2024) 15:6427 13"
}