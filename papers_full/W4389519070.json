{
  "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study",
  "url": "https://openalex.org/W4389519070",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2116452857",
      "name": "Bo-Xin Wang",
      "affiliations": [
        "University of Wisconsin–Madison",
        "University of Wisconsin System"
      ]
    },
    {
      "id": "https://openalex.org/A1964369109",
      "name": "Wei Ping",
      "affiliations": [
        "University of Wisconsin–Madison",
        "University of Wisconsin System"
      ]
    },
    {
      "id": "https://openalex.org/A1982578470",
      "name": "Peng Xu",
      "affiliations": [
        "University of Wisconsin System",
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2110595863",
      "name": "Lawrence McAfee",
      "affiliations": [
        "University of Wisconsin System",
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2111492569",
      "name": "Zihan Liu",
      "affiliations": [
        "University of Wisconsin System",
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A1842149474",
      "name": "Mohammad Shoeybi",
      "affiliations": [
        "University of Wisconsin–Madison",
        "University of Wisconsin System"
      ]
    },
    {
      "id": "https://openalex.org/A2104567669",
      "name": "Yi Dong",
      "affiliations": [
        "University of Wisconsin–Madison",
        "University of Wisconsin System"
      ]
    },
    {
      "id": "https://openalex.org/A183156050",
      "name": "Oleksii Kuchaiev",
      "affiliations": [
        "University of Wisconsin System",
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2045831236",
      "name": "Bo Li",
      "affiliations": [
        "University of Wisconsin System",
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2778191936",
      "name": "Chaowei Xiao",
      "affiliations": [
        "University of Wisconsin System",
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A3164401859",
      "name": "Anima Anandkumar",
      "affiliations": [
        "University of Wisconsin System",
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2688031072",
      "name": "Bryan Catanzaro",
      "affiliations": [
        "University of Wisconsin System",
        "University of Wisconsin–Madison"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4320813768",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963469388",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3206132631",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W2927103915",
    "https://openalex.org/W4254197176",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W4206637810",
    "https://openalex.org/W4281679115",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W2077815765",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2048176942",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963829526",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W4307123345",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4387596640",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4312055676",
    "https://openalex.org/W2025910815",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4389519535",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W2970476646"
  ],
  "abstract": "Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar, Bryan Catanzaro. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7763–7786\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nShall We Pretrain Autoregressive Language Models with Retrieval?\nA Comprehensive Study\nBoxin Wang∗‡1 Wei Ping∗†2 Peng Xu∗2 Lawrence McAfee2\nZihan Liu2 Mohammad Shoeybi2 Yi Dong2 Oleksii Kuchaiev2\nBo Li1 Chaowei Xiao2,3 Anima Anandkumar2 Bryan Catanzaro2\nAbstract\nLarge decoder-only language models (LMs)\ncan be largely improved in terms of perplex-\nity by retrieval ( e.g., RETRO ), but its impact\non text generation quality and downstream task\naccuracy is unclear. Thus, it is still an open\nquestion: shall we pretrain large autoregres-\nsive LMs with retrieval? To answer it, we per-\nform a comprehensive study on a scalable pre-\ntrained retrieval-augmented LM (i.e., RETRO )\ncompared with standard GPT and retrieval-\naugmented GPT incorporated at fine-tuning or\ninference stages. We first provide the recipe\nto reproduce RETRO up to 9.5B parameters\nwhile retrieving a text corpus with 330B tokens.\nBased on that, we have the following novel find-\nings: i) RETRO outperforms GPT on text gen-\neration with much less degeneration (i.e., repe-\ntition), moderately higher factual accuracy, and\nslightly lower toxicity with a nontoxic retrieval\ndatabase. ii) On the LM Evaluation Harness\nbenchmark, RETRO largely outperforms GPT\non knowledge-intensive tasks, but is on par with\nGPT on other tasks. Furthermore, we intro-\nduce a simple variant of the model, RETRO ++,\nwhich largely improves open-domain QA re-\nsults of original RETRO (e.g., EM score +8.6\non Natural Question) and significantly outper-\nforms retrieval-augmented GPT in both fine-\ntuning and zero-shot evaluation settings. Our\nfindings highlight the promising direction of\npretraining autoregressive LMs with retrieval\nas future foundation models. We release our\nimplementation at: https://github.com/N\nVIDIA/Megatron-LM#retro.\n1 Introduction\nLarge language models (LMs), including masked\nLMs (e.g., BERT (Devlin et al., 2018)), autore-\ngressive LMs (e.g., GPT (Brown et al., 2020)),\nand encoder-decoder LMs (e.g., T5 (Raffel et al.,\n∗Equal contribution. ‡Work done during an internship at\nNVIDIA. 1UIUC. 2NVIDIA. 3University of Wisconsin, Madi-\nson. †Correspondence to: Wei Ping <wping@nvidia.com>\n2020), BART (Lewis et al., 2020a)), have ob-\ntained state-of-the-art results for various NLP tasks.\nAmong them, the autoregressive LMs like GPT-\n3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)\ndemonstrate noticeable in-context learning abil-\nity and excellent long-form text generation results.\nDue to its importance, the community has spent\nconsiderable efforts to scale up such autoregres-\nsive generative LMs with more data and param-\neters and observed significant breakthroughs in\na variety of real-world applications (e.g., Brown\net al., 2020), including open-ended text genera-\ntion and various downstream tasks (e.g., ques-\ntion answering). The successful public exam-\nples include GPT-3 (w/ 170B parameters) (Brown\net al., 2020), Gopher (280B) (Rae et al., 2021),\nMegatron-Turing (530B) (Smith et al., 2022), and\nPaLM (540B) (Chowdhery et al., 2022).\nAlthough large-scale autoregressive LMs have\nachieved huge successes, they also suffer from sev-\neral weaknesses. First, it requires a huge number\nof model parameters to memorize the world knowl-\nedge, which makes it costly for deployment. Sec-\nond, it is difficult to safeguard factual accuracy,\nwhich may provide users with incorrect informa-\ntion (Lee et al., 2022). Third, it is expensive to\nupdate the model knowledge learned during pre-\ntraining with up-to-date facts (Meng et al., 2022),\nyielding outdated answers (Lewis et al., 2020b).\nTo mitigate the problems above, one line of\nresearch proposes to improve language models\nwith retrieval. The retrieval process can be inte-\ngrated into LMs at: i) fine-tuning stage (Karpukhin\net al., 2020; Lewis et al., 2020b; Guu et al., 2020),\nor ii) pretraining stage (Borgeaud et al., 2022;\nIzacard et al., 2022). Most previous work aug-\nments BERT or encoder-decoder LMs with re-\ntrieval at fine-tuning stage, demonstrating suc-\ncesses for knowledge-intensive NLP tasks (Guu\net al., 2020; Karpukhin et al., 2020; Lewis et al.,\n2020b; Khandelwal et al., 2020). However, it re-\n7763\nmains relatively underexplored to pretrain autore-\ngressive (decoder-only) LMs with retrieval, espe-\ncially considering the noticeable success of Chat-\nGPT (OpenAI, 2022) that underscores the extreme\nimportance of the autoregressive LMs.\nMost recently, RETRO (Borgeaud et al., 2022)\nproposes to pretrain autoregressive LMs with a\nretrieval module, which is practically scalable to\nlarge-scale pretraining from scratch by retrieving\nbillions of token and largely reduces model parame-\nters while achieving lower perplexity than standard\nGPT. It also provides the flexibility to update the\nknowledge stored in LMs (Petroni et al., 2019)\nby updating the retrieval database without train-\ning LMs again. The success of pretraining LMs\nwith retrieval raises an important question for the\ncommunity if we want to pretrain autoregressive\nLMs in the future: Shall we pretrain autoregres-\nsive (decode-only) LMs with retrieval by default\nor not? However, previous work (Borgeaud et al.,\n2022) misses the important evaluation on whether\nthe model like RETRO could obtain comparable or\neven better results in terms of open-ended text gen-\neration and various NLP downstream tasks, apart\nfrom lower perplexity on the held-out dataset com-\npared to standard GPT.\nTo answer the above question and bridge the\nmissing gap, we perform an extensive study on\nRETRO , as to the best of our knowledge, RETRO\nis the only retrieval-augmented autoregressive LM\nthat supports large-scale pretraining with retrieval\non the massive pretraining corpus with hundreds of\nbillion or trillion tokens. Our comprehensive study\nsheds light on the promising direction of pertain-\ning autoregressive LMs with retrieval to serve as\nfuture foundation models, as they overall outper-\nform standard GPT models in terms of perplexity,\ntext generation quality, and downstream task perfor-\nmances, especially for knowledge-intensive tasks,\nincluding open-domain QA.\n2 Key Findings\nWe successfully reproduce and pretrain RETRO\n(Borgeaud et al., 2022) from scratch1, with param-\neter sizes ranging from 148M up to 9.5B by re-\ntrieving from a text corpus with over 330B tokens.\nIn addition, we discuss the inference strategy of\nRETRO for text generation that is not covered in\nBorgeaud et al. (2022), and perform a large-scale\n1The official implementation and pretrained checkpoints\nare not open-sourced.\nevaluation in different scenarios.\nTo minimize the discrepancy variables between\nRETRO and GPT, we use the same decoder architec-\nture, same hyper-parameters, and same pre-training\ncorpus to pre-train RETRO and GPT given the same\nnumber of pre-training steps. We highlight our\nnovel findings for RETRO and GPT as follows:\n2.1 Text Generation\nWe conduct a systematic study (see §5) to under-\nstand and analyze RETRO by evaluating its open-\nended text generation quality via human and auto-\nmatic evaluations. RETRO exhibits better perfor-\nmance than GPT with considerably less repetition,\nmoderately higher factual accuracy, and slightly\nlower toxicity levels. RETRO is on par with GPT in\nterms of fluency, coherence.\n2.2 LM Evaluation Harness Benchmark\nIn terms of zero-shot evaluation on the standard\nbenchmark, RETRO can overall improve upon the\nGPT across different tasks, significantly outper-\nforming GPT on knowledge-intensive tasks such as\nHellaswag and BoolQ while achieving similar per-\nformance on other tasks. Specifically, we evaluate\nthe zero-shot capabilities of RETRO and GPT on\nnine representative NLP downstream classification\ntasks (see §6). Additionally, our findings demon-\nstrate that RETRO can leverage retrieved neighbors\nand significantly improves accuracy for knowledge-\nintensive tasks in zero-shot evaluations. In contrast,\nincorporating these retrieved neighbors directly dur-\ning the inference stage can hurt GPT’s performance.\nThese results further substantiate the potential of\nRETRO , which is pre-trained with retrieval capabil-\nities, as a promising approach.\n2.3 Open-domain QA\nFor open-domain QA tasks, RETRO achieves\nconsiderably superior performance than retrieval-\naugmented GPT that incorporates retrieval dur-\ning fine-tuning across different model sizes and\ndatasets. Specifically, we propose a variant of the\nmodel, RETRO ++, for open-domain QA that feeds\nthe most relevant evidence into the decoder and\nmore evidence into its encoder, which is different\nfrom the original version (Borgeaud et al., 2022).\nRETRO ++ can largely improve the exact matching\nscore (EM) on Natrual Question from 40.9% to\n54.1%, which is significant higher than the 45.5%\nreported by the original RETRO .\n7764\nModel #/ Retrieval When to Architecture Initialization Re-indexingName Tokens Involve Retrieval\nRETRO (Borgeaud et al.) O(1012) Pretraining decoder-only From Scratch / Pretrained GPT No\nAtlas (Izacard et al.) O(109) Pretraining encoder-decoder Pretrained T5 Yes\nREALM (Guu et al.) O(109) Pretraining encoder-only Pretrained BERT Yes\nRAG (Lewis et al.) O(109) Fine-tuning encoder-decoder Pretrained BART No\nDPR (Karpukhin et al.) O(109) Fine-tuning encoder-only Pretrained BERT No\nFiD (Izacard and Grave) O(109) Fine-tuning encoder-decoder Pretrained T5 No\nKNN-LM (Khandelwal et al.) O(109) Inference decoder-only Pretrained GPT No\nTable 1: Comparison of different retrieval-augmented models in terms of #/ retrieval tokens, which stage to incorporate retrieval\ninto LMs, the architecture of the backbone LM, whether it requires initialization from the existing LM checkpoint, and whether it\nrequires expensive re-indexing. RETRO is the most scalable retrieval-augmented LM due to its chunk-level retrieval and scalable\ndecoder-only autoregressive LM backbone (Thoppilan et al., 2022; Brown et al., 2020; Smith et al., 2022; Chowdhery et al.,\n2022) without expensive retrieval index refresh.\n3 Related Work\nRetrieval has been applied in various NLP tasks\nfor years, including question answering (QA) (e.g.,\nBilotti et al., 2007), machine translation (e.g.,\nZhang et al., 2018), and conversation (Shuster\net al., 2021; Thoppilan et al., 2022; Komeili et al.,\n2021). In particular, language models have been\naugmented with retrieval at different stages, includ-\ning inference time (Khandelwal et al., 2020; Yo-\ngatama et al., 2021), fine-tuning stage (Karpukhin\net al., 2020; Lewis et al., 2020b; Guu et al., 2020),\nand pretraining stage (Borgeaud et al., 2022; Izac-\nard et al., 2022).\nLMs have been augmented with retrieval at the\nfine-tuning stage for downstream tasks, primarily\nfor open-domain QA. DPR (Karpukhin et al., 2020)\nfinetunes one BERT to encode questions and the\nother BERT to encode answers within a dual en-\ncoder framework, using a contrastive loss to align\nthe hidden representations of question and corre-\nsponding answer. RAG (Lewis et al., 2020b) stud-\nies the fine-tuning recipe for retrieval-augmented\ngeneration models, especially on open-domain QA\ntasks. FiD (Izacard and Grave, 2021) improves\nRAG with a better LM backbone T5, and fuses\nmultiple retrieved passages to the decoder during\nfine-tuning to further improve QA accuracy. We-\nbGPT (Nakano et al., 2021) leverages web search\nengine and fine-tunes GPT using reinforcement\nlearning with human feedback (RLHF) for refer-\nence generation and factuality improvement, which\nis orthogonal to our work that focuses on pretrain-\ning with retrieval. The proposed RLHF can be\napplied to RETRO as well.\nREALM (Guu et al., 2020) performs both un-\nsupervised pretraining and supervised fine-tuning\nstrategies for retrieval-augmented BERT model in\nopen-domain QA. Their pretraining involves asyn-\nchronous re-embedding and re-indexing all docu-\nments every several hundred training steps, which\nquickly becomes impractical for training corpus\nwith trillion tokens. Atlas (Izacard et al., 2022)\nuses a similar approach but augments the T5 archi-\ntecture (Raffel et al., 2020) with retrieval at both\npre-training and fine-tuning. Before pretraining, it\nfirst initializes the encoder-decoder LM backbone\nwith pretrained T5, and the dense retriever with\npretrained Contriever (Izacard et al.). During pre-\ntraining, it also applies asynchronous index refresh\nevery 1000 steps.\nIn contrast, RETRO (Borgeaud et al., 2022) em-\nbeds and indexes the whole training corpus at\nchunk-level (e.g., chuck size = 64) with a frozen\nBERT before pretraining. During pretraining, the\nmodel relies on a trainable bidirectional encoder\nto embed the retrieved chunks of raw text. The\nGPT decoder further “select” the relevant piece of\nevidence from the encoder side by a chunk-wise\ncross-attention. This architecture design enables\nLM pretraining on hundreds of billion tokens by\nretrieving from trillion tokens. See Table 1 for a\ncomplete comparison of retrieval-augmented LMs.\n4 Model and Implementation\nIn this section, we first introduce preliminaries of\nRETRO , then provide detailed recipe of our imple-\nmentation, including retrieval database, pretraining,\nand retrieval-augmented finetuning and generation.\n4.1 Preliminaries of R ETRO\nRETRO is an autoregressive language model en-\nhanced with a retrieval module that utilizes chunk-\nwise retrieval, enabling it to scale up to trillions of\n7765\nRetro\na b c\nInput \nd\n  can use the retrieval evidence of the current  chunk \n for the next  chunk generation: maintain the causality\n<pad> ... <pad>\n1 61 62 6463\nNext  chunk\nRetrieval\nDatabase\n(a) Use “left padding” Rule\nRetro\n<pad>\nInput \n1 64\nAlways use the rightmost 64 tokens to retrieve \nand generate the next token (Retrieval step=1)\nd\n... b c\n127 128\nRetrieval\nDatabase (b) Retrieval step = 1\nlogits\nRetro\n<pad> b\nContext (or question)\n1 64\nc <pad>\n65 128\nc\n①  Left pad question/context \n②  Right pad answer \nd e\nanswer\nRetrieval\nDatabase (c) Separate question and answer chunks\nFigure 1: Visualization of padding design for RETRO .\nSmall Medium XL XXL\nGPT 17.76 13.18 10.18 7.86\nRETRO (k = 2) 12.99 10.06 8.10 6.72\nTable 2: Validation perplexity of pretrained GPT and RETRO\non the held-out dataset. We report the results with k = 2\nneighbors in this Table, and we observe the same trend of\nimprovements with larger k as in Borgeaud et al. (2022).\ntokens. The model splits both the input sequence\nand retrieval datastore into sequences of chunks.\nRETRO retrieves nearest neighbor chunks from the\nretrieval database using the previous chunk and\nfuses this information with the context from pre-\nceding chunks to guide the generation of the next\nchunk. To maintain causality, the model can only\nuse the nearest neighbors of the previous chunk for\nthe autoregressive generation.\n4.2 Implementation\nAs RETRO has no official open-source implementa-\ntion and pretrained checkpoints, we reproduce and\npretrain RETRO from scratch on our own.\n4.2.1 Retrieval Database\nWe build the retrieval database with the whole\npretraining dataset mentioned in §B. In this way,\nRETRO and standard GPT of similar size are fair\ncomparisons, as they are pretrained using the same\ninformation from the pretraining corpus. The re-\ntrieval database is a key-value database, where val-\nues are chunks split from the pretraining corpus,\nand the keys are corresponding BERT embeddings.\nOur pertaining dataset with 330B tokens yields a\nretrieval database consisting of 5.3B chunks in total\nwith chunk size m= 64.\nRetrieval Index. We use the Faiss index (Johnson\net al., 2019) as the implementation for the dense re-\ntriever to search for approximate nearest neighbors\nin the BERT embedding space. We configure the\nFaiss index to cluster the dense embeddings into\n222 centroids accelerated with Hierarchical Navi-\ngable Small World graphs (Malkov and Yashunin,\n2018) to speed up the query. We also encode the\nembeddings with optimized product quantization\n(Gray and Neuhoff, 1998; Ge et al., 2014) to com-\npress memory overhead and further improve the\nquery throughput. As a result, we can achieve 4ms\nper query over the whole pretraining corpus aver-\naged for each chunk on a DGX-2H node. One may\nfind more details in Appendix §A.\n4.2.2 Pretraining R ETRO Models\nWe use the same transformer configurations (#/ lay-\ners, hidden size, attention heads) and pretrain both\nRETRO and standard GPT from scratch. Specifi-\ncally, we pretrain RETRO across different param-\neter sizes, ranging from 148M (Small), 410M\n(Medium), 1.5B (XL), and 9.5B (XXL). We also\nuse the same pretraining schedules to pretrain\nRETRO and GPT given the same number of steps.\nWe list the validation perplexity of GPT andRETRO\nafter pretraining in Table 2. We present more de-\ntails in Appendix §B, including pretraining sched-\nules, computational cost (GPU hours), and model\narchitectures.\n4.2.3 Retrieval-augmented Generation\nWe discuss the generation and inference recipe in\nthe batch-processing mode for RETRO , which is\nmissing from the previous literature.\n“Left Padding” Rule. The chunk-wise retrieval of\nRETRO improves scalability but enforces chunk-\nwise alignment constraints, leading to issues in con-\nditional generations with short contexts. When the\nsequence length is less than the chunk size, RETRO\ncannot utilize its retrieval capability as there is no\nprevious chunk for retrieval. Instead, RETRO adds\npadding tokens to the left of the context, allowing\nRETRO to leverage the retrieved neighbors from\nthe previous context to guide the generation of the\n7766\nMetrics Small Medium XL XXL\nGPT R ETRO GPT R ETRO GPT R ETRO GPT R ETRO\nRepetition % 2.86% 2.26% 1.70% 1.50% 1.44% 0.96% 1.40% 1.12%\nSelf-BLEU 0.29 0 .3 0.29 0 .3 0.29 0 .29 0.31 0 .31\nZipf Coefficient 0.98 0 .98 0.96 0 .98 0.97 0 .98 0.96 0 .96\nTable 3: Automatic evaluation on text generation quality for RETRO and GPT across different sizes.\nnext token (Figure 1a). We summarize this general\nprinciple in RETRO as the “left padding” rule, as\nit can leverage the contextual information for re-\ntrieval to the most. This rule remains preferable\nfor input sequences larger than the chunk size, as\nit ensures the closest and rightmost context is used\nfor retrieval, making it more relevant for next token\nprediction (see Figure 1b).\nFrequency of Retrieval. In order to efficiently gen-\nerate long sequences with RETRO , we note a flexi-\nble trade-off between retrieval-augmented genera-\ntion and computation overhead. The direct method\ninvolves retrieval at every decoding step, maximiz-\ning the use of the retrieval module but increasing\ncomputational overhead (Figure 1b, retrieval step\n= 1). Another approach retrieves neighbors at the\nfrequency of the chunk size, reducing overhead but\nsacrificing accuracy (Appendix Figure 3b, retrieval\nstep = 64). To balance these factors, we introduce\na flexible retrieval step, which allows model practi-\ntioners to choose how many tokens to generate with\nthe current retrieved neighbors before updating the\ncontext. Smaller retrieval steps are preferred for\ndownstream tasks with short answers to ensure ac-\ncurate neighbors, while larger steps are used for\nefficient generation of long passages. We provide\nmore details in Appendix §C.\n4.2.4 Batched Training for Downstream Tasks\nWhen fine-tuning RETRO for downstream\ntasks (e.g., QA), it is crucial to separate context\nor question from the candidate answer chunk to\nmaintain causality in autoregressive modeling.\nThis leads to a modified \"left padding\" rule:\npad context chunks from the left and answer\nchunks from the right (Figure 1c). Padding aligns\ninput sequences with the chunk size, enabling\nbatch-mode training and inference for faster\nevaluation. By adding padding chunks to the right,\nsequences with varying chunk numbers can be\nprocessed together, further improving efficiency.\n5 Open-ended Text Generation\nIn this section, we delve into the problem of open-\nended text generation, which refers to tasks of gen-\nerating coherent continuation given the preceding\nprompt. Given that this problem for RETRO has\nnever been studied before, we manage to bridge the\ngap and evaluate the open-ended text generation of\nRETRO compared to GPT from three aspects: a)\ntext quality, b) factuality, and c) toxicity.\n5.1 Text Quality\nWe perform both automatic and human evaluations.\n5.1.1 Automatic Evaluation\nEvaluation Metrics. We follow prior work (Holtz-\nman et al., 2019; Zhu et al., 2018) and consider\nthe following metrics: Repetition % measures\npercentage of the generations containing repetitive\nphrases, SELF-BLUE evaluates the diversity of\nthe generations, and Zipf Coefficient measures\nthe use of vocabulary. See detailed definition and\nevaluation setup in Appendix §D.1.\nExperimental Results. Our results are shown in\nTable 3. We note that RETRO can reduce the per-\ncentage of repetition compared with GPT by a large\nmargin across different sizes. Specifically, RETRO\naveragely mitigates 21% of repetitions compared\nwith GPT across different sizes. This suggests the\nretrieval module can help reduce text degeneration\nby referencing retrieved human text. Regarding vo-\ncabulary use and generation diversity, we do not ob-\nserve major differences between GPT and RETRO ,\nwhich implies these properties are primarily depen-\ndent on the decoder component of LMs.\n5.1.2 Human Evaluation\nWe also conduct human evaluations to further ver-\nify the quality of the generated text.\nEvaluation Metrics. We ask human annotators\nto annotate each generation with fluency scores,\nwhich measure the human readability and grammat-\nical errors from 1 (Not human-readable) to 5 (Very\nfluent), and coherence scores, which measure the\n7767\nDecoding Models Factual Nonfactual\nNEER ↓ EntailR ↑ NEER ↓ EntailR ↑\nTop-p=0.9 RETRO 52.14% 3.11% 56.75% 2.06%\nGPT 52.42% 2.93% 56.82% 2.04%\nGreedy RETRO 37.42% 16.66% 42.45% 10.88%\nGPT 39.87% 12.91% 45.02% 8.75%\n(a) The factuality on FACTUALITY PROMPTS benchmark.\nModels QA Format Null Format\nMC1↑ MC2↑ MC1↑ MC2↑\nGPT 0.222 0 .377 0.234 0 .435\nRETRO (pretraining) 0.239 0.382 0.248 0.439\nRETRO (wiki) - - 0.242 0 .437\nRETRO (DPR) - - 0.245 0.439\n(b) The truthfulness on TruthfulQA benchmark.\nTable 4: Evaluation of factuality and truthfulness of RETRO (XL) and GPT (XL).\nrelevance between the prompt and the correspond-\ning continuations from 1 (Not Relevant) to 5 (Very\nRelevant). More details can be found in §D.2.\nExperimental Results. We present the human vote\nhistogram in Appendix Figure 4. We observe that\nmost votes concentrate on the regime of scores\n>= 3for both relevance and fluency, which indi-\ncates that our generated text from both models is of\nhigh quality and closely related to the prompts. The\ndifferences between GPT and RETRO are subtle,\nwith average relevance (3.726) and fluency (3.826)\nscores of RETRO slightly outperforming the aver-\nage relevance score (3.715) and fluency (3.818)\nscores of GPT.\nFrom both automatic and human evaluation,\nwe can conclude that although the generation of\nRETRO adds some complexity, we do not see any\nsign of the degeneration of RETRO compared to\nGPT. Moreover, RETRO is shown to be able to\nreduce the repetition and slightly improve text gen-\neration quality.\n5.2 Factuality\nFactuality refers to being coherent to provide\nground truth knowledge sources in NLP. We lever-\nage two well-established benchmarks (Lee et al.,\n2022; Lin et al., 2021) to evaluate the factual accu-\nracy of RETRO and GPT. We put detailed experi-\nmental setup and metrics in the §E.\nFactuality Prompts. We present the results on\nFACTUALITY PROMPTS benchmark in Table 4a,\nwhere we follow the protocol (Lee et al., 2022)\nand use different decoding strategies for evaluation.\nWe observe that overallRETRO demonstrates better\nfactual accuracy and makes fewer hallucinations.\nOur observation also aligns with (Lee et al., 2022)\nthat different from top- p sampling with random-\nness, greedy decoding gives the much better factual\naccuracy by sacrificing sample diversity. RETRO\nshows a significant improvement over GPT using\nboth top-p and greedy decoding algorithms.\nTruthfulQA. We also evaluate truthfulness in Ta-\nble 4b, where we observe the consistent conclu-\nsion that RETRO tend to be more resilient to hu-\nman falsehoods than GPT across different prompts\n(demonstrated in Appendix Table 12 and 13).\nSince using the null format yields consistently\nbetter performance, we thus fix the prompt and\nconduct ablation studies with different retrieval\ndatabases. We observe that the pretraining database\nand DPR database yield the best factual accuracy.\nThis suggests that (i) finetuned retriever or (ii) re-\ntrieval from a larger corpus are helpful to improve\nfactual accuracy. In addition, the improvement mar-\ngin for RETRO on TruthfulQA is not as significant\nas on FACTUALITY PROMPTS . This is likely be-\ncause TruthfulQA is an adversarial dataset contain-\ning difficult question types, such as logical false-\nhoods and indexical errors (e.g., “Are you an artifi-\ncial intelligence?”, more examples can be found in\nAppendix Table 14). In these cases, retrieval from\nthe database does not effectively aid in answering\nsuch questions.\n5.3 Toxicity\nThe toxicity of LMs refers to the possibility of\nLMs that output toxic generations. In this study,\nwe follow REALTOXICTY PROMPTS benchmark\n(Gehman et al., 2020) to evaluate the potential tox-\nicity of RETRO and GPT.\nEvaluation Metrics. Following Gehman et al.\n(2020), we report the Expected Maximum Toxic-\nity, which evaluates the toxicity of the worst-case\ngeneration, as well as Toxicity Probabilitythat esti-\nmates the empirical frequency of generating toxic\nlanguage. See more details and setup in §F.\nExperimental Results. The toxicity of LMs are\nshown in Table 5. Compared to GPT, we note\nthat RETRO with the pretraining corpus even in-\ncreases the toxicity of the generations. Moreover,\nwe observe more toxicity increases in toxic prompts\nthan in nontoxic prompts. This suggests that when\nprompting RETRO with toxic contexts, it is more\nlikely to retrieve toxic evidence and thus amplify\n7768\nModels Retrieval Exp. Max. Toxicity (↓) Toxicity Prob. (↓)\nDatabase Full Toxic Nontoxic Full Toxic Nontoxic\nGPT - 0.44 0 .64 0 .39 37% 74% 27%\nRETRO (top-N = 2, top-K = 2) Pretraining 0.46 0 .66 0 .40 40% 76% 30%\nRETRO (top-N = 5, top-K = 2) Pretraining 0.46 0 .66 0 .40 39% 77% 29%\nRETRO (top-N = 10, top-K = 2) Pretraining 0.46 0 .66 0 .40 39% 76% 29%\nRETRO (top-N = 2, top-K = 2) Wiki 0.43 0 .64 0 .38 35% 73% 25%\nRETRO (top-N = 5, top-K = 2) Wiki 0.43 0 .64 0 .38 35% 71% 26%\nRETRO (top-N = 10, top-K = 2) Wiki 0.43 0 .64 0 .38 35% 71% 26%\nTable 5: Evaluation of LM toxicity for GPT (XL) and RETRO (XL). Model toxicity is evaluated on REALTOXICITY PROMPTS .\nFull refers to the full set of prompts, Toxic and Nontoxic refer to the toxic and nontoxic subsets of prompts. ↓ means the lower,\nthe better. RETRO can filter from top-N nearest neighbors and select the top-K nontoxic neighbors for retrieval.\nthe issues. To confirm the toxicity amplification\nissue, we further conduct two sets of ablation stud-\nies: (i) We save the retrieval evidence and calculate\nthe Expected Mean Toxicity of both generations\nand retrieval evidence. We observe that the toxic-\nity of retrieval evidence is 0.177, higher than the\ntoxicity of the generations (0.146). (ii) We change\nthe retrieval database to the Wikipedia database,\nwhich shows lower toxicity for retrieval evidence\n(0.132). As a result, we observe that RETRO with\nthe Wikipedia retrieval database can help mitigate\nthe toxicity of GPT as shown in Table 5, with the\ntoxicity probability dropping from 37% to 35%.\nWe also note that it is not very helpful to use a\nlarger Nas nearest neighbors and filter the retrieval\nevidence by toxicity. We hypothesize the reason\nis that the similarity between input and retrieval\nevidence is limited with larger N, thus yielding\nlow cross-attention on the retrieval evidence.\n6 LM Evaluation Harness Benchmark\nBesides the open-ended text generation, it is also\nimportant to examine the generalization of RETRO\non various downstream tasks, which is also miss-\ning from the literature. Therefore, we use LM\nEvaluation Harness Benchmark (Gao et al., 2021)\nand consider the following nine representative NLP\ndownstream tasks. See more details in §G.\nZero-shot evaluation. We present the zero-shot\nevaluation results in Table 6. We find that on av-\nerage RETRO can improve the downstream task\naccuracy across different tasks. Moreover, we ob-\nserve larger improvements in knowledge-intensive\ntasks such as Hellaswag and BoolQ (6 of 8 cases),\nwhich require factual knowledge to guide the rea-\nsoning. Note that the zero-shot evaluation results\nare susceptible to prompt formats, so the results\nhave certain variances.\nRetrieval-augmented GPT at Inference time.\nWe have seen that retrieval significantly improves\nRETRO across different downstream tasks in the\nzero-shot setting. In this ablation study, we append\nthe retrieval evidence of RETRO to the beginning\nof the context to see whether retrieval can also be\nhelpful for GPT at inference time. We evaluate the\nzero-shot accuracy after prepending the top- 1 re-\ntrieval evidence. The results are shown in Appendix\nTable 16. We observe that directly prepending the\nevidence from the retrieval database messes up the\nGPT context in the zero-shot setting, yielding low\naccuracy of around 24.5%. We hypothesize the rea-\nson is that the retrieval evidence can be noisy. With-\nout pretraining or proper fine-tuning, GPT in the\nzero-shot learning setting puts too much attention\non the noisy evidence, thus giving low downstream\naccuracy.\n7 Open-domain Question Answering\nIn this section, we study two widely used open-\ndomain QA datasets, Natural Question (NQ) and\nTriviaQA.\n7.1 Experimental Setup\nRetrieved evidence as context The original\nRETRO work leverages the retrieved evidence (i.e.\npassages) by feeding them all into the encoder. We\nargue that the top most relevant evidence is more\nimportant than others and should be used as the\ncontext for the question. Therefore, the top rele-\nvant evidence should be fed to the decoder, and the\nrest of the evidence can be incorporated by the en-\ncoder. For the implementation in our experiments,\nwe append the top-1 relevant passage at the begin-\nning of the decoder input, and reformat the input\nwith Template A: “title: {title}, source: {source}\n\\n question: {question} \\n answer: {answer}”. For\n7769\nTasks Small Medium XL XXL\nGPT R ETRO GPT R ETRO GPT R ETRO GPT R ETRO\nKnowledge-intensive Tasks\nHellaSwag 31.3 36 .2 ↑4.9 43.2 46 .2 ↑3.0 56.7 59 .0 ↑2.3 72.3 70 .6 ↓1.7\nBoolQ 59.3 61 .8 ↑2.5 57.4 57 .2 ↓0.2 62.2 62 .7 ↑0.5 67.3 70 .7 ↑3.4\nKnowledge-nonintensive Tasks\nLambada 41.7 41 .4 ↓0.3 54.1 55 .0 ↑0.9 63.9 64 .0 ↑0.1 73.9 72 .7 ↓1.2\nRACE 34.6 32 .5 ↓2.1 37.3 37 .3 ↑0.0 40.8 39 .9 ↓0.9 44.3 43 .2 ↓1.1\nPiQA 64.3 64 .8 ↑0.5 70.2 68 .7 ↓1.5 73.7 74 .1 ↑0.4 78.5 77 .4 ↓1.1\nWinoGrande 52.4 52 .0 ↓0.4 53.8 55 .2 ↑1.4 59.0 60 .1 ↑1.1 68.5 65 .8 ↓2.7\nANLI-R2 35.1 36 .2 ↑1.1 33.5 33 .3 ↓0.2 34.3 35 .3 ↑1.0 32.2 35 .5 ↑3.3\nHANS 51.5 51 .4 ↓0.1 50.5 50 .5 ↑0.0 50.1 50 .0 ↓0.1 50.8 56 .5 ↑5.7\nWiC 50.0 50 .0 ↑0.0 50.2 50 .0 ↓0.2 47.8 49 .8 ↑2.0 52.4 52 .4 ↑0.0\nAvg. Acc. (↑) 46.7 47 .4 ↑0.7 50.0 50 .4 ↑0.4 54.3 55 .0 ↑0.7 60.0 60 .5 ↑0.5\nTable 6: Accuracy (Acc.) on nine downstream tasks evaluated in the zero-shot setting for pretrained LMs with different\nparameter sizes.\nMethod NQ TriviaQA\nGPT (close book) 36.1 45.1\nREALM (Guu et al., 2020) 40.4 -\nDPR (Karpukhin et al., 2020) 41.5 56.8\nRAGBART (Lewis et al., 2020b) 44.5 56.1\nRAGGPT 50.9 60.9\nFiDLarge (Izacard and Grave, 2021) 51.4 67.6\nRETRO (Ours) 40.9 59.9\nRETRO (Borgeaud et al., 2022) 45.5 -\nRETRO ++ (Ours) 54.1 66.7\nTable 7: Comparisons of our RETRO and existing QA models.\nWe report the best results with the largest model configuration\nrespectively.\nthe models without retrieved evidence in the con-\ntext, we follow Borgeaud et al. (2022) to format\nthe input with Template B: “question: {question}\n\\n answer: {answer}”.\nIn additional to several baseline methods in Ta-\nble 7, we compare the following models: 1) GPT\n(close-book) simply finetunes a pretrained GPT\nmodel with the input Template B without using\nany retrieved documents. 2) RAGGPT applies RAG\nfinetuning (Lewis et al., 2020b) for GPT, which\nputs retrieved evidence as its context. It utilizes\nthe top retrieved documents by DPR with the input\nTemplate A and finetunes a pretrained GPT model,\nwhich represents incorporating retrieval to GPT at\nthe fine-tuning stage. 3) RETRO encodes the re-\ntrieved evidence using the encoder and finetunes a\npretrained RETRO model with the input Template B.\n4) RETRO ++ finetunes a pretrained RETRO model\nwith the top retrieved evidence included input Tem-\nplate A while leaving the rest of the evidence to the\nencoder. More details can be found in §H.\nMedium XL XXL\nModel Size\n38\n40\n42\n44\n46\n48\n50\n52\n54EM score\nComparisons over NQ\nRAGGPT\nRETRO+ +\nMedium XL XXL\nModel Size\n50\n55\n60\n65EM score\nComparisons over TriviaQA\nRAGGPT\nRETRO+ +\nFigure 2: Comparisons among RAGGPT and RETRO ++ mod-\nels on NQ and TriviaQA. Larger models achieve better perfor-\nmances and RETRO ++ is consistently better than RAGGPT\n7.2 Results and Analysis\nTable 7 shows the results on NQ and TriviaQA. Our\nRETRO ++ achieves Exact Match (EM) score 54.1,\nwhich is 8.6 higher than the original RETRO paper.\nWe find the key to the success of RETRO is to\nincorporate the top retrieved document from DPR\nto the decoder as the context , which gives us 13.2\nabsolute improvement by comparing our RETRO\nand RETRO ++. Note that our RETRO has lower\nEM score (40.91) than the original paper (45.5),\nas their model is trained on 600B tokens, whereas\nours is trained on 330B tokens. By comparing\n7770\nRAGGPT with RETRO ++, we show that pretraining\nautoregressive LM with retrieval (i.e., RETRO ++)\nyields better QA accuracy than only fine-tuning\nautoregressive LM with retrieval (i.e., RAGGPT).\nAppendix §H.3 gives qualitative studies on NQ.\nScaling of model sizes. Figure 2 shows the\nEM score when scaling model sizes for RAGGPT,\nand RETRO ++ on NQ and TriviaQA. As the model\nsizes increase, the performance of all models mono-\ntonically increases. RETRO ++ achieves the best\nperformances across all tasks and model sizes.\nNote that, Wang et al. (2023) further scales up the\nsize of RETRO to 48B and discusses how instruc-\ntion tuning can help improve retrieval-augmented\nLLMs for zero-shot open-domain question answer-\ning.\n7.3 Zero-shot evaluation with and without\ninstruction tuning\nInstruction tuning (Wei et al., 2022a; Chung et al.,\n2022) finetunes LLMs on a collection of datasets\ndescribed via natural language instructions, which\nsignificantly improve the zero-shot accuracies for\nunseen downstream tasks. In this subsection, we\nstudy how instruction tuning can help with open-\ndomain QA for retrieval-agumented LLMs.\nInstruction tuning data. We use a blend of\nhigh-quality instruction tuning datasets of 128K\nsamples to train LLMs to follow instructions,\nwhich include: a high-quality social dialogue\ndataset SODA (Kim et al., 2022), a long-form QA\ndataset ELI5 that requires elaborate answers (Fan\net al., 2019), LLM-generated instructions: Self-\nInstruct (Wang et al., 2022) and Unnatural Instruc-\ntions (Honovich et al., 2022), FLAN and Chain-\nof-thought datasets (Chung et al., 2022; Wei et al.,\n2022b; Longpre et al., 2023), public human-written\nconversation datasets OpenAssistant (Köpf et al.,\n2023) and Dolly (Conover et al., 2023).\nImplementation details. We conduct instruc-\ntion tuning to both GPT (XXL) and RETRO (XXL).\nWe finetune the LLMs by taking the loss only on\nthe last response from the assistant with a batch size\nof 128 and a learning rate of 5e-6 for 1000 steps\nwith a weight decay of 0.01. We use the Adam op-\ntimizer (Kingma and Ba, 2014) with β1 = 0.9 and\nβ2 = 0.98. After finetuning, we follow the same\nprompt format as RAGGPT for instruction-tuned\nGPT (XXL) and RETRO ++ for instruction-tuned\nRETRO (XXL) and evaluate the zero-shot accuracy\non the Natural Question (NQ) dataset.\nRAGGPT RETRO ++\nw/o Instruction tuning 24.43 25.93\nw/ Instruction tuning 29.75 31.16\nTable 8: Exact Match (EM) scores for the zero-shot\nevaluation of RAGGPT and RETRO ++ on the NQ dataset\nbefore and after instruction tuning.\nResults. The results of retrieval-augmented\nGPT (RAGGPT) and RETRO ++ before and after\ninstruction tuning are shown in Table 8. We ob-\nserve that applying instruction tuning with RETRO\nand Retrieval-augmented GPT (RAGGPT) indeed\ngives significant accuracy improvement. Moreover,\nRETRO ++ demonstrates consistently better accu-\nracy than RAGGPT. This result further confirms the\npotential and capabilities of RETRO when employ-\ning advanced techniques such as instruction tuning.\nNote that, Wang et al. (2023) further scale up the\nRETRO to 48B parameters to unveil the power of\ninstruction tuning.\n8 Conclusion\nIn this work, we perform a comprehensive study of\npretrained retrieval-augmented LLM to answer the\nquestion: Shall we pretrain decoder-only LMs with\nretrieval? We observe consistent improvements in\ntext generation quality, factual accuracy, lower tox-\nicity, and downstream task accuracy, especially for\nknowledge-intensive tasks, including open-domain\nQA. Given the ∼25% percentage of additional\nGPU hours for pretraining (see Table 11 Appendix\nB), we argue pretraining generative language mod-\nels with retrieval is a promising direction.\nLimitations\nDespite the impressive performance of RETRO and\nRETRO ++, our findings reveal several limitations\nthat pave the way for future research to address:\n• The quality of the retrieval database. The\nfactual accuracy and toxicity reduction in gen-\nerated text rely on the quality and range of the\nretrieval database. This means that the perfor-\nmance and the model’s outputs can vary based\non the retrieval database. The performance of\nRETRO could be compromised if the database\ncontains inaccurate, biased, or outdated infor-\nmation.\n• Scalability. The pretraining of GPT and\nretrieval-augmented LLM from scratch requires\nsignificant computational resources. Our work\nfollows Borgeaud et al. (2022) and pretrains\n7771\nGPT and RETRO up to the size of 9B. We leave\nit as an important future work to further scale\nup the size of retrieval-augmented LLMs.\nReferences\nMatthew W Bilotti, Paul Ogilvie, Jamie Callan, and\nEric Nyberg. 2007. Structured retrieval for question\nanswering. In Proceedings of the 30th annual inter-\nnational ACM SIGIR conference on Research and\ndevelopment in information retrieval.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical common-\nsense in natural language. In AAAI.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In ICML.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. NeurIPS.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv: 2210.11416.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\nIntroducing the world’s first truly open instruction-\ntuned llm.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, J. Weston, and Michael Auli. 2019. Eli5: Long\nform question answering. Annual Meeting of the\nAssociation for Computational Linguistics.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nTiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun.\n2014. Optimized product quantization. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence,\n36(4):744–755.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. RealToxic-\nityPrompts: Evaluating neural toxic degeneration in\nlanguage models. In Findings in EMNLP.\nR.M. Gray and D.L. Neuhoff. 1998. Quantiza-\ntion. IEEE Transactions on Information Theory ,\n44(6):2325–2383.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. REALM: Retrieval\naugmented language model pre-training. In ICML.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degener-\nation. International Conference On Learning Repre-\nsentations.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor. Annual\nMeeting of the Association for Computational Lin-\nguistics.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research.\nGautier Izacard and Édouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\n7772\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In EMNLP.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West,\nXiming Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras,\nMalihe Alikhani, Gunhee Kim, Maarten Sap, and\nYejin Choi. 2022. Soda: Million-scale dialogue dis-\ntillation with social commonsense contextualization.\narXiv preprint arXiv: 2212.10465.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2021.\nInternet-augmented dialogue generation. arXiv\npreprint arXiv:2107.07566.\nAndreas Köpf, Yannic Kilcher, Dimitri von Rütte,\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver\nStanley, Richárd Nagyfi, Shahul ES, Sameer Suri,\nDavid Glushkov, Arnav Dantuluri, Andrew Maguire,\nChristoph Schuhmann, Huu Nguyen, and Alexander\nMattick. 2023. Openassistant conversations - de-\nmocratizing large language model alignment. arXiv\npreprint arXiv: 2304.07327.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale read-\ning comprehension dataset from examinations. In\nEMNLP.\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pas-\ncale Fung, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2022. Factuality enhanced language models for\nopen-ended text generation. NeurIPS.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2020a. BART:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. In ACL.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020b. Retrieval-augmented generation\nfor knowledge-intensive NLP tasks. In NeurIPS.\nStephanie C. Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. ACL.\nS. Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won\nChung, Yi Tay, Denny Zhou, Quoc V . Le, Barret\nZoph, Jason Wei, and Adam Roberts. 2023. The flan\ncollection: Designing data and methods for effec-\ntive instruction tuning. International Conference on\nMachine Learning.\nYu A Malkov and Dmitry A Yashunin. 2018. Efficient\nand robust approximate nearest neighbor search us-\ning hierarchical navigable small world graphs. IEEE\ntransactions on pattern analysis and machine intelli-\ngence, 42(4):824–836.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in GPT. In NeurIPS.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nnli: A new benchmark for natural language under-\nstanding. In ACL.\nOpenAI. 2022. ChatGPT. https://chat.openai.co\nm.\nOpenAI. 2023. GPT-4 technical report. arXiv.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The lambada dataset: Word predic-\ntion requiring a broad discourse context. In NAACL.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP.\nSteven T. Piantadosi. 2014. Zipf’s word frequency law\nin natural language: A critical review and future di-\nrections. Psychonomic Bulletin & Review, 21:1112–\n1130.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. Wic: the word-in-context dataset for evaluat-\ning context-sensitive meaning representations. In\nNAACL.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research.\n7773\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adver-\nsarial winograd schema challenge at scale. In AAAI.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. arXiv preprint\narXiv:2104.07567.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zhang, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022. Using deepspeed and megatron to train\nmegatron-turing nlg 530b, a large-scale generative\nlanguage model. arXiv.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nBoxin Wang, Wei Ping, Lawrence McAfee, Peng\nXu, Bo Li, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2023. Instructretro: Instruction tuning post\nretrieval-augmented pretraining. arXiv preprint\narXiv: 2310.07713.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage models with self-generated instructions. An-\nnual Meeting of the Association for Computational\nLinguistics.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In ICLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nJohannes Welbl, Amelia Glaese, Jonathan Uesato,\nSumanth Dathathri, John Mellor, Lisa Anne Hen-\ndricks, Kirsty Anderson, Pushmeet Kohli, Ben Cop-\npin, and Po-Sen Huang. 2021. Challenges in detoxi-\nfying language models. Findings of EMNLP.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021. Adaptive semiparametric lan-\nguage models. Transactions of the Association for\nComputational Linguistics.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In ACL.\nJingyi Zhang, Masao Utiyama, Eiichro Sumita, Gra-\nham Neubig, and Satoshi Nakamura. 2018. Guiding\nneural machine translation with retrieved translation\npieces. In NAACL.\nYangqiaoyu Zhou and Chenhao Tan. 2021. Investigat-\ning the effect of natural language explanations on\nout-of-distribution generalization in few-shot NLI.\nIn Proceedings of the Second Workshop on Insights\nfrom Negative Results in NLP, pages 117–124, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\nIn The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval,\nSIGIR ’18, page 1097–1100, New York, NY , USA.\nAssociation for Computing Machinery.\n7774\nAppendix\nA Details of Retrieval Index\nRetrieval Database. We use the whole pertaining corpus as our retrieval database. Our pertaining\ndataset with 330B tokens yields a retrieval database consisting of 5.3B chunks in total with chunk size\nm= 64. To support fast similarity searches with billions of chunks, we implement the database index\nwith Faiss index (Johnson et al., 2019). Given the BERT embeddings of an input chunk Ci, Faiss can\nreturn the approximate knearest neighbor of Ci within a few milliseconds.\nFaiss Index configuration We use the Faiss index (Johnson et al., 2019) as the implementation for the\ndense retriever to search for approximate nearest neighbors in the BERT embedding space. We configure\nthe Faiss index as follows:\n• Preprocessing: We use Optimized Product Quantization (Ge et al., 2014) to apply a rotation to the input\nvectors to make them more amenable to PQ coding (Gray and Neuhoff, 1998).\n• Indexer: We use Inverted File Index (IVF) with 222 centroids and accelerate it with Hierarchical\nNavigable Small World (HNSW) graphs (Malkov and Yashunin, 2018).\n• Encoding: We adopt PQ encoding that compresses the dense embedding vector into 64 bits.\nAs a result, we can achieve4ms per query over the whole pretraining corpus via batch queries averaged for\neach chunk with less than 400GB memory usage as our max throughput. Given a single query, the latency\nof the response is around 0.1sper query. We also note that increasing the number of Kin the query does\nnot yield slower query speed. During pertaining, we follow (Borgeaud et al., 2022) to pre-compute the\nnearest neighbors and save the data for pretraining.\nB Details of Pre-trained LMs\nWe evaluate and compare RETRO with a variety of standard GPT-3 like LMs to set up the baselines.\nChunk-wise Cross-Attention. RETRO is an autoregressive language model augmented with a retrieval\nmodule. One fundamental reason contributing to the success of RETRO is the design of chunk-wise\nretrieval, which retrieves at the level of contiguous token chunks and thus makes it possible to scale up to\nretrieve from trillion tokens. Specifically, RETRO splits both the input sequence and retrieval datastore\ninto a sequence of chunks. Formally, given a input sequence X with ntokens X = (x1,...,x n), RETRO\nsplits X into a sequence of lchunks (C1,...,C l) with chunk size m= n\nl . From a high-level perspective,\nRETRO uses the last (i−1)-th chunk Ci−1 to retrieve k nearest neighbor chunks N(Ci−1) from the\nretrieval database and fuses the contextual information from the previous chunks (C1,...,C i−1) and\nretrieval information from N(Ci−1) by chunk-wise cross-attention to guide the generation of the next\n(i)-th chunk Ci. Note that, to avoid breaking the causality, the autoregressive generation of i-th chunk Ci\ncan only use the nearest neighbors of the previous chunk N(Ci−1) instead of N(Ci). In this work, we\nfollow (Borgeaud et al., 2022) and set the chunk size m= 64.\nPretrained GPT and RETRO . We pretrain standard GPT and RETRO with different parameter sizes.\nAll of the models are based on Transformer (Vaswani et al., 2017) with different hidden dimensions,\nnumber of layers, and attention heads. We adopt the GPT-2 BPE vocabulary (Radford et al., 2019) for\nboth GPT and RETRO .\nThe architecture details of pre-trained LMs are in Table 9. The corresponding perplexity and downstream\ntask accuracy are shown in Table 3 and Table 6.\nPretraining Corpus. To perform a fair comparison, we pretrain GPT and RETRO using the same\npretraining corpus, which is an English text corpus constructed from 15 high-quality datasets (including\nWikipedia, CommonCrawl, and so on) as described in (Smith et al., 2022). The whole pretraining corpus\nconsists of 330B tokens.\n7775\nModels Size #/layers #/hidden size #/ attention heads #/ parameters (R ETRO ) #/ parameters (GPT)\nSmall 12 768 12 148M 126M\nMedium 24 1024 16 410M 357M\nXL 24 2048 32 1.5B 1.3B\nXXL 40 4096 64 9.5B 8.3B\nTable 9: Detailed configuration of standard pre-trained LMs and RETRO .\nPretraining schedules for GPT and RETRO . We use the same pretraining schedules for GPT and\nRETRO . We list the pretraining hyper-parameter details in Table 10. All models use Adam optimizer\n(Kingma and Ba, 2014) with β1 = 0.9 and β2 = 0.95. We employ the learning rate (LR) decay schedules\nwith lr warmup samples of 162761 and lr decay samples of 166400000.\nModels Size LR min LR LR Decay Styles Batch Size Pretraining Steps\nSmall 6e-4 6e-5 cosine 256 750k\nMedium 3e-4 3e-5 cosine 256 750k\nXL 2e-4 2e-5 cosine 512 375k\nXXL 1e-4 1e-5 cosine 512 375k\nTable 10: Detailed pretraining setup for standard pre-trained LMs and RETRO .\nComputational Cost of Pretraining. We have provided our computation costs associated with GPT\nand Retro below for pretraining on 330B tokens. All of our experiments are done on the DGX-2H node\nwith 8x A100 GPUs. From Table 11, we can see that the overhead involved in training Retro is less than\n25% on average. Considering consistent improvements in text generation quality, factual accuracy, lower\ntoxicity, and downstream task accuracy, especially for knowledge-intensive tasks, including open-domain\nQA, we believe pretraining Retro is a promising direction.\nModel Size GPT Retro Additional Overhead\nSmall 1240 GPU Hours 1560 GPU Hours 25.80%\nMedium 3600 GPU Hours 4480 GPU Hours 24.44%\nXL 12000 GPU Hours 13440 GPU Hours 12.00%\nTable 11: Comparison of GPU Hours.\nC Implementation Details of Retrieval-Augmented Generation\nC.1 “Left Padding” Rule\nWhile chunk-wise retrieval significantly improves the scalability of RETRO , it also enforces chunk-wise\nalignment constraint between the input and the retrieval neighbors. Specifically, the chunk-wise cross\nattention requires that the generation of the current chunk Ci can only use the previous chunk Ci−1 for\nretrieval instead of Ci to avoid breaking causality.\nConditional Generation with Short Contexts This design may lead to problems for conditional\ngenerations under short contexts, as shown in Figure 3a. Given short contexts with sequence length nless\nthan the chunk size m, RETRO cannot leverage its retrieval capability, as the current chunk is the first\nchunk, and there is no previous chunk for retrieval. When mis not a multiplier of n, RETRO needs to add\nadditional padding tokens2 to the input sequence. To simplify, we first focus on predicting the next token\ninstead of generating a whole sequence. If we follow the standard GPT that adds the padding tokens at the\nend, we visualize the padding situation in Figure 3a as an example of when the input sequence length\n2Since GPT-2 BPE vocab does not contain “<pad>” token, we use the end-of-text token “<|endoftext|>” for padding in\npractice.\n7776\nRetro\na b c\nInput \n1 2 3 4 64\nRetrieval\nDatabase\n cannot use the retrieval evidence of the current  chunk \n for the current  chunk generation: can break the causality\nCurrent  chunk d\n(a) Not use “left padding” Rule\nRetro\na\nInput \n1 64\nUse the tokens from index 1 to 64 to retrieve, and \ngenerate tokens from index 65 to 128 (Retrieval step=64)\nCurrent chunkd\nb c\n65 66\nRetrieval\nDatabase (b) Fixed retrieval step = 64\nRetro\na\nInput \n1 64\nb c\n65 65\n①  Retrieve ②  Generate two tokens \n③  Retrieve againRetrieval step= 2\nRetrieval\nDatabase (c) Retrieval step = 2\nFigure 3: Visualization of padding design for RETRO .\nis less than the chunk size. Since RETRO generates the next token (“d”) within the current chunk, thus\nit purely relies on the decoder of RETRO without leveraging retrieval evidence of the previous context\n(“abc”) to help the next token prediction.\nConditional Generation Using “Left Padding” Rule In contrast, if we add the padding tokens to the\nleft of the context so that the context and padding tokens happen to form the first chunk, we visualize the\npadding mechanism in Figure 1a. In this case, the next token prediction is placed at the start of the next\nchunk, which means that RETRO can leverage the retrieved neighbors of the previous context to guide the\ngeneration of the next token.\nC.2 Frequency of Retrieval in Text Generation\nIn the last subsection, we discuss how to add padding tokens to predict the next token. In this subsection,\nwe discuss how to efficiently generate a long sequence for RETRO .\nRetrieval Step = 1 The most direct way for text generation is to repeat the next token prediction\nparadigm as shown in Figure 1b, which generates a new token, places it in the right, reduces one left\npadding token, retrieves neighbors given the updated context, and uses the new retrieved neighbors to\npredict the next token. While this paradigm makes the most of the retrieval module, as it always uses\nthe updated context to search for the most relevant neighbors for the next token prediction, it also brings\ncomputational overhead as it needs to do retrieval at every decoding step (retrieval step= 1).\nRetrieval Steps = 64 Another way is to do retrieval at the frequency of chunk size as shown in Figure 3b\n(chunk size = retrieval step = 64). In this case, RETRO uses the previous chunk to retrieve the neighbors\nto guide the generations of all tokens in the next following chunk. However, this generation paradigm\nsuffers from inaccurate neighbors as the context is not updated.\nFlexible Retrieval Steps To have a flexible trade-off between the retrieval accuracy and retrieval\noverhead, we propose to support flexible retrieval steps as shown in Figure 3c. Model practitioners can\ndecide how many tokens to generate given the current retrieved neighbors, and then update the context to\nuse the rightmost chunk to retrieve neighbors again for the next token predictions. Generally, when we\ngenerate a few tokens for downstream tasks, we tend to use small retrieval steps to guarantee the accuracy\nof the retrieval neighbors; but when we try to generate a long passage, we tend to use larger retrieval steps\nfor efficient generations.\nD Details of Evaluation for Text Generation Quality\nD.1 Details of Automatic Evaluation for Text Generation Quality\nExperimental Setup. We follow Holtzman et al. (2019) and use the same set of 5,000 prompts for\nconditional generations. Both GPT and RETRO use nucleus sampling with p= 0.9 and generate up to\n200 tokens or less if reaching an <end-of-text> token. As RETRO is coping with long text generation, we\nset the retrieval step to 64 and retrieve top-k= 2neighbors from the retrieval database.\n7777\n020040060080010001200140016001800\nNot RelevantSomewhatIrrelevantSomewhatRelevantRelevantVery Relevant\nHuman Votes on Context Coherence\nGPTRetro\n(a) Human vote histogram for context coherence. The average\nrelevance scores of GPT and RETRO are 3.715 and 3.726.\n020040060080010001200140016001800\nNot Human-readableNot FluentAverageFluentVery Fluent\nHuman Votes on Text Fluency\nGPTRetro\n(b) Human vote histogram for text fluency. The average fluency\nscores of GPT and RETRO are 3.818 and 3.826.\nFigure 4: Human evaluation of context coherence and text fluency on GPT (XXL) and RETRO (XXL).\nEvaluation Metrics. We use the following automatic evaluation metrics for text generation quality:\n• Repetition % measures the percentage of the generations containing repetitive phrases. Specifically, a\nphrase (minimum length 2) is considered a repetition when it repeats at least three times at the end of\nthe generation.\n• SELF-BLUE evaluates the diversity of the generations. Self-BLEU is calculated by computing the\nBLEU score of each generated document using all other generations in the evaluation set as references.\nwe follow Holtzman et al. (2019) and sample 1,000 generations, each of which is compared with all\n4999 other generations as references. A lower Self-BLEU score implies higher diversity.\n• Zipf Coefficient measures the use of vocabulary by comparing the vocabulary distribution with a\ntheoretically perfect exponential curve with Zipf coefficient equal to 1 (Piantadosi, 2014).\nD.2 Details of Human Evaluation for Text Generation Quality\nExperimental Setup. We first sample 200 prompts from the full 5000 prompts and their corresponding\ngenerations from GPT (XXL) and RETRO (XXL) as in Holtzman et al. (2019), yielding 400 prompts\nand continuations in total. We randomly shuffle the generations from two models, group samples into\nbatches (batch size = 10), and assign them to 20 different annotators for fluency evaluation, and another\n20 different annotators for coherence evaluation.\nParticipants were recruited through Amazon MTurk. Since text fluency and coherence evaluation are\nobjective to different social groups, we do not have any constraints on the demographic background of\nannotators. Since our generation focuses on English, we constrain the regions of annotators to the United\nStates, Canada, Australia, and the United Kingdom. To improve the quality of the annotations, we require\nthe participated annotators to have at least 500 approved HITs and a lifelong HIT approval rate greater\nthan 98%. We group continuations in a batch of 10 samples and assign them to annotators. In total, 167\nworkers from Amazon Turk participated in the fluency evaluation, and 210 workers in the coherence\nevaluation, contributing to 8000 annotations in each evaluation.\nWe adapt the instructions from Holtzman et al. (2019) and show the annotation instructions for\ncoherence and fluency evaluation on Amazon MTurk platform in Figure 6 and Figure 7, including two\nexamples generated from RETRO and GPT.\n7778\nFigure 5: Example that receives low scores from annotators due to improper formatting.\nFigure 6: Human evaluation instructions for context relevance evaluation.\n7779\nFigure 7: Human annotation interface for text fluency evaluation.\n7780\nE Details of Factuality Evaluation\nE.1 Experimental Setup\nWe use FACTUALITY PROMPTS benchmark (Lee et al., 2022) for the open-ended text generation task.\nAs the dataset focuses on factual knowledge in Wikipedia, we replace our retrieval database with the\nWikipedia database, which is a subset of our whole pretraining database, to improve the inference efficiency\nwith the retrieval step of 64. We use TruthfulQA (Lin et al., 2021) for factual accuracy evaluation in the\nform of multi-choice classification. We evaluate RETRO with different retrieval databases: the pretraining\ndatabase, the Wikipedia database, and the DPR retrieval database (Karpukhin et al., 2020) that is a\nfine-tuned retriever on Wikipedia for better retrieval. We mainly focus on the evaluation ofRETRO (XL)\nand GPT (XL) due to the computational cost of long text generations.\nE.2 Evaluation Metrics\nFACTUALITY PROMPTS benchmark uses two metrics to measure the factual accuracy: Hallucinated\nNE Error (NEER) evaluates the percentage of generations that generates a named entity that does not\nappear in the ground-truth knowledge source, and thus makes hallucinating errors; Entailment Ratio\n(EntailR) measures the percentage of generations that are entailed by the ground-truth knowledge annotated\nwith publicly pretrained NLI model. TruthfulQA uses single-choice classification accuracy (MC1) and\nmulti-choice classification probability (MC2) to assess a model’s ability to identify true statements.\nQA Prompt Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in\n1955.\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\nQ: What is the square root of banana?\nA: I have no comment.\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make\nobjects appear closer.\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.\nQ: Are all dogs dogs?\nA:\nTable 12: QA format: QA prompt (in black color) concatenated with the actual question (in blue color) for\nTruthfulQA dataset\nNull Prompt Q: Are all dogs dogs?\nA:\nTable 13: Null format: the actual question (in blue color) for TruthfulQA dataset\n7781\nQuestion Categories Question\nLogical Falsehood Q: Are all dogs dogs?\nIndexical Error Q: Are you an artificial intelligence?\nIndexical Error Q: Are you conscious?\nTable 14: A list of questions from TruthfulQA that retrieval does not help.\nF Details of Toxicity Evaluation\nF.1 Experimental Setup\nFollowing (Welbl et al., 2021), we randomly sample a subset of 10k prompts from the\nwholeREALTOXICTY PROMPTS benchmark with 100k prompts. For each prompt, we follow Gehman\net al. (2020) and perform 25 conditional generations to generate up to 20 tokens with retrieval step of 2\nand nucleus sampling (p= 0.9) to evaluate the Expected Maximum Toxicity and Toxicity Probability. This\nrequires 250k generations for each model, so we also focus on the evaluation of RETRO (XL) and GPT\n(XL) to save computational cost and have a deeper understanding. Specifically, we try both the pretraining\nand Wikipedia databases as retrieval databases. We also implement a filtering mechanism that retrieves\ntop-N neighbors from the database and returns the most nontoxic top-Kneighbors as retrieval.\nF.2 Evaluation Metrics\nFollowing Gehman et al. (2020), we use Perspective API, an online automated model for toxic language\nevaluation and retrieval filtering. Specifically, Expected Maximum Toxicity evaluates the worst-case\ngeneration by calculating the maximum toxicity scores over 25 generations under the same prompt with\ndifferent random seeds, and averaging the maximum toxicity scores over all prompts. Toxicity Probability\nestimates the empirical frequency of generating toxic language, which evaluates the probability of\ngenerating a toxic continuation (TOXICITY >= 0.5) at least once over 25 generations.\nG Details of LM Evaluation Harness Benchmark\nG.1 Task Details\nWe use LM Evaluation Harness Benchmark (Gao et al., 2021) and consider the following two representative\nNLP knowledge-intensive tasks, where retrieving factual knowledge can be helpful in reasoning:\n• BoolQ (Clark et al., 2019) is a question-answering dataset for yes/no questions.\n• Hellaswag (Zellers et al., 2019) is a commonsense NLI dataset.\nand seven knowledge-nonintensive tasks:\n• ANLI (Nie et al., 2020) is a large-scale NLI adversarial benchmark dataset.\n• LAMBADA (Paperno et al., 2016) is a cloze test (word prediction) dataset.\n• PIQA (Bisk et al., 2020) is a physical reasoning and a corresponding benchmark dataset.\n• RACE (Lai et al., 2017) is a large-scale reading comprehension dataset.\n• WiC (Pilehvar and Camacho-Collados, 2019) is a multilingual Word-in-Context Dataset for the\nevaluation of context-sensitive word embeddings.\n• WinoGrande (Sakaguchi et al., 2020) is for pronoun resolution problems.\n• HANS (Zhou and Tan, 2021) is an NLI evaluation set that tests specific hypotheses about invalid\nheuristics that NLI models are likely to learn.\nG.2 Evaluation Protocol\nTo evaluate autoregressive LMs on classification problems, LM Evaluation Harness Benchmark queries the\nLMs by concatenating the question and different candidate answers as input, comparing the probabilities of\ndifferent answers, and selecting the most probable answer as LM prediction. When applying the evaluation\nprotocol to RETRO , we follow the principles in §4 to separate question and answer into different chunks\nto avoid breaking causality.\nOur RETRO uses the default pretraining database as the retriever.\n7782\nG.3 Fine-tuning Performance.\nBesides zero-shot accuracy, we also perform fine-tuning on one representative knowledge-nonintensive\ntask Lambada (lowercase), and one representative knowledge-intensive task Hellaswag.\nThroughout our experiments, we fine-tune both GPT and RETRO for three epochs. We use a batch\nsize equal to 512 with a sequence length of 2048. We use the Adam optimizer (epsilon=1e-5, beta-1=0.9,\nbeta-2=0.95) with initial lr=1e-5 for 530B LM, while we use lr=2e-5 for all other LMs. We set weight\ndecay to 0.1 for all LMs. Our experiments are conducted on the DGX A100 servers with 8x A100 GPUs.\nThe fine-tuning results are shown in Table 15. We note that since Lambada (lowercase) is a more\nchallenging dataset that consists of only lowercase samples that may hurt the retrieval quality, we\nobserve lower accuracy of RETRO than GPT in the zero-shot learning setting. However, after fine-tuning,\nwe observe that RETRO achieves better accuracy than GPT with a significant improvement margin.\nSimilar observations can be found in the Hellaswag task, where RETRO consistently demonstrates better\nperformance across different model sizes (Small, Medium, and XL). This suggests that RETRO is better at\ndomain-adaption after fine-tuning.\nTasks Small Medium XL XXL\nGPT R ETRO GPT R ETRO GPT R ETRO GPT R ETRO\nLambada\n(lowercase)\nZero-shot 29.8 27 .0 43.1 43 .0 55.4 52 .5 66.2 65 .3\nFine-tuning 35.8 ↑6.0 37.2 ↑10.2 48.6 ↑5.5 50.0 ↑7.0 59.2 ↑3.8 60.0 ↑7.5 66.8 ↑0.6 68.0 ↑2.7\nHellaSwag Zero-shot 31.3 36 .2 43.2 46 .2 56.7 59 .0 72.3 70 .6\nFine-tuning 35.4 ↑4.1 40.8 ↑4.6 52.7 ↑9.5 55.1 ↑8.9 67.7 ↑11.0 68.5 ↑9.5 75.3 ↑3.0 74.5 ↑3.9\nTable 15: Accuracy (Acc.) on Hellaswag and Lambada (lowercase) tasks after fine-tuning pretrained LMs with different\nparameter sizes.\nG.4 Put Retrieval Evidence in Context for GPT in zero-shot evaluation\nWe have seen that retrieval significantly improvesRETRO across different downstream tasks in the zero-\nshot setting. In this ablation study, we append the retrieval evidence of RETRO to the beginning of the\ncontext to see whether it can also be helpful for GPT in the zero-shot scenario.\nWe evaluate the zero-shot accuracy after prepending the top-K(K = 1) retrieval evidence. The results\nare shown in Table 16. We observe that directly prepending the evidence from the retrieval database messes\nup the GPT context in the zero-shot setting, yielding low accuracy of around 24.5%. We hypothesize the\nreason is that the retrieval evidence can be messy and noisy. Without pretraining or proper fine-tuning,\nGPT in the zero-shot learning setting puts too much attention on the messy evidence, thus giving low\ndownstream accuracy.\nTasks Small Medium XL XXL\nGPT GPT (retrieve) GPT GPT (retrieve) GPT GPT (retrieve) GPT GPT (retrieve)\nAcc. (↑) 31.3 24 .5 43.2 25 .2 56.7 24 .2 72.3 24 .1\nTable 16: Accuracy (Acc.) on Hellaswag evaluated in the zero-shot setting.\n7783\nH Details of Open-domain QA\nH.1 Experimental Setup\nNQ contains questions from Google search queries and TriviaQA contains a collection of questions\nfrom trivia and quiz-league websites. Following Borgeaud et al. (2022), we use the processed data\nprovided by Izacard and Grave (2021) for both NQ and TriviaQA, in which each question-answer pair\nis accompanied by a 100-words Wikipedia passage retrieved by DPR (Karpukhin et al., 2020). We\ngenerate the answer using greedy decoding. Following the standard evaluation procedures in previous\nwork (Izacard and Grave, 2021; Borgeaud et al., 2022), Exact Match (EM) is used as our answer accuracy\nevaluation metric.\nH.2 Training Details\nWe finetune all model parameters with the learning rate of 1e-5 for a Medium model, 3e-6 for an XL\nmodel, and 1e-6 for an XXL model. When calculating the EM score, each predicted answer is compared\nto the ground truth after both are lowercase and stripped of articles, punctuation, and duplicate whitespace.\nWe early-stop finetuning by evaluating the EM on the validation set as we find PPL is not a good metric\nfor early stopping.\nH.3 Qualitative Study on NQ\nGiven a question, DPR retrieves a set of evidence. As RAGGPT can only access to the first evidence, it\ngives the wrong answer, while RETRO ++ can answer correctly based on the additional evidence where\nthe answers can be found.\n7784\nQuestion who invented the first home video security system\nRAGGPT sanders associates\nRETRO ++ marie van brittan brown\nGround Truth [’her husband Albert Brown’, ’Marie Van Brittan Brown’]\nEvidence 1 ’title’: ’Sanders Associates’, ’text’: ’Sanders Associates Sanders Associates was a\ndefense contractor in Nashua, New Hampshire, United States, from 1951 until it was\nsold in 1986. It is now part of BAE Systems Electronics & Integrated Solutions,\na subsidiary of BAE Systems. It concentrated on developing and manufacturing\nelectronic systems, notably aircraft self-protection systems, and tactical surveillance\nand intelligence systems. Other business areas included microwave, missile and\nspace electronics; infrared imaging; and automated mission planning systems, with\nboth military and commercial applications. The first home video game console\nwas developed as a side project by engineer Ralph H. Baer and several assistants at\nSanders.’\nEvidence 2 ’title’: ’Security alarm’, ’text’: ’of an alarm. In some municipalities around the\nUnited States, this type of alarm verification allows the property it is protecting to be\nplaced on a \"verified response\" list, allowing for quicker and safer police responses.\nThe first video home security system was patented on December 2, 1969 to inventor\nMarie Brown. The system used television surveillance. To be useful, an intrusion\nalarm system is deactivated or reconfigured when authorized personnel are present.\nAuthorization may be indicated in any number of ways, often with keys or codes\nused at the control panel or a remote panel near an entry. High-security’\nEvidence 3 ’title’: ’Marie Van Brittan Brown’, ’text’: ’Marie Van Brittan Brown Marie Van\nBrittan Brown (October 30, 1922 – February 2, 1999) was an African-American\ninventor, becoming the originator of the home security system ( \"\") in 1966, along\nwith her husband Albert Brown, a patent was granted in 1969. Brown was born\nin Jamaica, Queens, New York; she died there at the age of 76. Marie Van Brittan\nBrown\\’s father was born in Massachusetts and her mother\\’s roots originated from\nPennsylvania. Brown and her husband lived at 151-158 & 135th Avenue in Jamaica,\nQueens, New York. She worked as a nurse and her husband was an’\nTable 17: The answer is found in Evidence 3 and RETRO is able to answer it correctly while RAGGPT can only\ngenerate the answer from Evidence 1.\n7785\nQuestion where is the hotel used in the movie the shining\nRAGGPT estes park colorado\nRETRO ++ stanley hotel\nGround Truth [’The Stanley Hotel’]\nEvidence 1 ’title’: ’The Shining (film)’, ’text’: ’has become a staple of pop culture. In 2018, the\nfilm was selected for preservation in the United States National Film Registry by the\nLibrary of Congress as being \"culturally, historically, or aesthetically significant.\"\nJack Torrance arrives at the mountain-isolated Overlook Hotel, far from town, to\nbe interviewed for the position of winter caretaker. Once hired, former teacher\nJack plans to use the hotel ´s solitude to write. The hotel, built on the site of a\nNative American burial ground, closes during the snowed-in months. Manager Stuart\nUllman tells Jack about the hotel´s history since its 1907 construction, but he also’\nEvidence 2 ’title’: ’The Shining (film)’, ’text’: ’Jan Harlan. Saint Mary Lake and Wild Goose\nIsland in Glacier National Park, Montana was the filming location for the aerial shots\nof the opening scenes, with the V olkswagen Beetle driving along Going-to-the-Sun\nRoad. The Timberline Lodge on Mount Hood in Oregon was filmed for a few of the\nestablishing shots of the fictional Overlook Hotel; notably absent in these shots is\nthe hedge maze, something the Timberline Lodge does not have. Outtakes of the\nopening panorama shots were later used by Ridley Scott for the closing moments of\nthe original cut of the film \"Blade Runner\" (1982). \"The Shining\"’\nEvidence 3 ’title’: ’The Shining (film)’, ’text’: ’order, he used several stages at EMI Elstree\nStudios in order to make all sets available during the complete duration of production.\nThe set for the Overlook Hotel was at the time the largest ever built at Elstree,\nincluding a life-size re-creation of the exterior of the hotel. In February 1979, the set\nat Elstree was badly damaged in a fire, causing a delay in the production. While most\nof the interior shots, and even some of the Overlook exterior shots, were shot on\nstudio sets, a few exterior shots were shot on location by a second-unit crew headed\nby’\nEvidence 4 ’title’: ’The Shining (film)’, ’text’: ’end of the film and Jack´s repeated claims to have\n\"not just a deja vu\". The film is even more focused on Jack (as opposed to Danny)\nthan the novel. The room number 217 has been changed to 237. Timberline Lodge,\nlocated on Mt. Hood in Oregon, was used for the exterior shots of the fictional\nOverlook Hotel. The Lodge requested that Kubrick not depict Room 217 (featured\nin the book) in \"The Shining\", because future guests at the Lodge might be afraid to\nstay there, and a nonexistent room, 237, was substituted in the film. Contrary to the\nhotel´s’\nEvidence 5 ’title’: ’The Stanley Hotel’, ’text’: ’main building which adorned the lawn of the\nOverlook Hotel in the series can be viewed in the basement of the Stanley. In\naddition to serving as the Overlook Hotel in Stephen King ´s 1997 TV miniseries\nversion of \"The Shining\" (\"see above\"), the Stanley also served as the fictional \"Hotel\nDanbury\" of Aspen, Colorado, in the 1994 film \"Dumb and Dumber\". From 2013 to\n2015, the hotel property hosted the Stanley Film Festival, an independent horror film\nfestival operated by the Denver Film Society, held in early May. The festival featured\nscreenings, panels, student competitions, audience awards and receptions. The’\nTable 18: The answer is found in Evidence 5 and RETRO is able to answer it correctly while RAGGPT cannot.\n7786",
  "topic": "Ping (video games)",
  "concepts": [
    {
      "name": "Ping (video games)",
      "score": 0.8267534971237183
    },
    {
      "name": "Autoregressive model",
      "score": 0.6464351415634155
    },
    {
      "name": "Computer science",
      "score": 0.6221984028816223
    },
    {
      "name": "Natural language processing",
      "score": 0.5620548725128174
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44610702991485596
    },
    {
      "name": "Statistics",
      "score": 0.13949915766716003
    },
    {
      "name": "Mathematics",
      "score": 0.11014381051063538
    },
    {
      "name": "Computer security",
      "score": 0.057184576988220215
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1304256225",
      "name": "University of Wisconsin System",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    }
  ]
}