{
    "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning",
    "url": "https://openalex.org/W3207699717",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1982127900",
            "name": "Wu Shaohua",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2054134520",
            "name": "Zhao XuDong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2104399034",
            "name": "Yu Tong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2359001800",
            "name": "Zhang Rongguo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2111765781",
            "name": "Shen Chong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1900138996",
            "name": "Liu Hongli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1967201970",
            "name": "Li Feng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1960338410",
            "name": "Zhu Hong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2373001106",
            "name": "Luo JianGang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2039357331",
            "name": "Xu Liang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2303041400",
            "name": "Zhang, Xuanwei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3106031450",
        "https://openalex.org/W3182414949",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W3107315802",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3132736064",
        "https://openalex.org/W2945785363",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W3114651185",
        "https://openalex.org/W3134582802",
        "https://openalex.org/W3040573126",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3158631574",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3157005959",
        "https://openalex.org/W2949884065",
        "https://openalex.org/W3182414670",
        "https://openalex.org/W2338908902",
        "https://openalex.org/W3093956460"
    ],
    "abstract": "Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",
    "full_text": "YUAN 1.0: L ARGE -SCALE PRE-TRAINED LANGUAGE MODEL IN\nZERO -SHOT AND FEW-SHOT LEARNING\nShaohua Wu∗ Xudong Zhao Tong Yu\nRongguo Zhang Chong Shen Hongli Liu Feng Li\nHong Zhu Jiangang Luo Liang Xu Xuanwei Zhang\nABSTRACT\nRecent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning\non many natural language processing (NLP) tasks by scaling up model size, dataset size and the\namount of computation. However, training a model like GPT-3 requires huge amount of computational\nresources which makes it challengeable to researchers. In this work, we propose a method that\nincorporates large-scale distributed training performance into model architecture design. With this\nmethod, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves\nexcellent performance on thousands GPUs during training, and the state-of-the-art results on natural\nlanguage processing tasks. A data processing method is designed to efﬁciently ﬁlter massive amount\nof raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built\nbased on this method. In addition, a calibration and label expansion method is proposed to improve\nthe Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of\nvarious tasks. Yuan 1.0 presents strong capacity of NLP tasks, and the generated articles are difﬁcult\nto distinguish from the human-written ones.\n1 Introduction\nThe Transformer architecture has been widely used in natural language processing[ 1, 2, 3]. In order to improve\nthe performance, a varieties of Transformer-based modiﬁcations have been proposed since 2017[ 1], but many of\nthem exhibit a lack of generalization across different implementations and tasks[ 4]. Kaplan, et al.[ 5] conﬁrms that\nperformance of the Transformer steadily improves with the scaling up of model size, dataset size, and the amount of\ncomputation for training. Roberta[2] shows that the accuracy of BERT can be substantially improved by training the\nmodel for a longer time with a larger corpus. The T5 model, built with vanilla Transformer structure and increased\nmodel size with 11 billion parameters, achieves the state-of-the-art (SOTA) performance in various NLP tasks[3]. It is\nproved that larger language models performs better than smaller ones.\nGPT-3 with 175 billion parameters, as a milestone, was proposed in 2020[6]. Before GPT-3, it was common to pre-train\na model with unsupervised learning on a large unlabeled dataset, then ﬁne-tune on a speciﬁc task. Because GPT-3\nmakes great progress on Zero-Shot and Few-Shot learning, it can be applied directly on a wide range of NLP tasks, and\ndisplays good performance without being ﬁne-tuned on those tasks. After GPT-3, several studies further increase the\nmodel size in two ways:\n• Singleton: Increase the number of layers and the size of a layer, such as GPT-3 and PanGu-α[7].\n• Mixture of Experts (MoE): Scaling the model size with Sparsely Gated Mixture-of-Experts (MoE), such as GShard[8],\nSwitch Transformer[9], Wudao[10, 11] and M6[12] . Each expert is a singleton model in a size up to 10B. With\nMoE, the model size can be successfully scaled up to more than 1000B [9, 11].\nBoth Singleton and MoE are effective to increase the model size, however, they behave differently in Zero-Shot and\nFew-Shot scenarios. Currently, the MoE method still follows the common way, in which pre-train the model on a large\n∗wushaohua@inspur.com, Inspur Artiﬁcial Intelligence Research Institute\narXiv:2110.04725v2  [cs.CL]  12 Oct 2021\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\ndataset and ﬁne-tune it on speciﬁc task. To our best knowledge, no MoE model is applied on Zero-Shot or Few-Shot\nlearning. However, both GPT-3 and PanGu-αwith singleton architecture, exhibits good performance on Zero-Shot and\nFew-Shot learning[6, 7].\nTraining a model with parameters greater than 100B requires huge amount of computational resources. Take GPT-3\n175B for example, it was trained on a cluster of 10,000 GPUs [ 6, 13]. Such a huge requirement on computational\nresources makes it difﬁcult for most researchers to train a model in a similar way. In this work, we propose Yuan 1.0\nsingleton model with 245B parameters. To accelerate the training process of Yuan 1.0, and thus reduce energy costs and\ncarbon emissions, we make a collaborative design of model architecture and large-scale distributed training. The main\ncontributions of our work are summarized as below,\n• A method that incorporates large-scale distributed training performance into model architecture design is proposed.\nWith this method, we trained our Yuan 1.0, the current largest singleton language model with 245B parameters, and\nachieved excellent performance on thousands GPUs\n• A data processing system is created to efﬁciently ﬁlter a massive amount of data from Internet. The current largest\nChinese corpus with 5TB high-quality text is built based on this system.\n• The model architecture with better performance in Pre-train and Fine-tune pattern is likely to behave opposite in\nZero-Shot and Few-Shot learning.\n• A method that can steadily improve the Zero-Shot and Few-Shot performance is proposed.\n2 Yuan 1.0\nThe basic architecture of Yuan 1.0 is a language model. For a given input(x1,x2,...,x n), the language model predicts\nthe probability of the output (y1,y2,...,y n):\np(y) =\nn∏\ni=1\np(yi|x1,x2,...,x i−1) (1)\nTo deal with different downstream tasks (translation, question answering, classiﬁcation etc.), each task is casted in\na diagram of text-to-text framework[3]. In this way, a pre-trained language model can be directly applied to handle\ndifferent tasks[3, 14].\n2.1 Model architecture\nIn this work, we consider two model architectures, Language Model (LM) and Preﬁx Language Model (PLM). In\nLM, which is one of the most commonly used architecture[6, 14, 15], the decoder in a Transformer is taken to auto-\nregressively generate an output sequence. The structure of a decoder LM is presented in Fig. 1(a).\nAt time t, a token at the rightmost of the output, x6, is generated based on the probability predicted by the model. Then\nthis token is concatenated to the input sequence and fed back into the model to generate the next token at time t+1. In\nthe pre-train and ﬁne-tune pattern, a LM performs better in natural language generation (NLG) tasks, but comparatively\nworse in natural language understanding (NLU) tasks. The reason of this drawback is that a LM, with casual masking\nof attention structure, forces the model’s prediction of theith token only depends on the tokens before i, which can be\nseen in Fig. 1(a). In contrast, PLM performs well in both NLU tasks and NLG tasks[3]. Instead of taking a casual mask,\nPLM uses a fully-visible mask in the range of the preﬁx portion of input sequence, which can be seen in Fig. 1(b).\n2.2 Cooperative design of model structure\nThe huge cost of computational resources is the bottlenecks that limits researchers to develop NLP models with hundreds\nof billions parameters. Take GPT-3 for example, it was trained on a large cluster with 10,000 GPUs[6, 13]. In order to\naccelerate the training process, we incorporate the key factors that affect the performance of large-scale distributed\ntraining into the design of Yuan 1.0 model structure.\nThe parameters of LM that affects both the accuracy and the performance of large-scale distributed training include\nthe number of layers, hidden size, global batch size, micro batch size, etc. In the large-scale distributed training of\nYuan models, we use three-dimensional parallel strategies, including tensor parallelism, pipeline parallelism and data\nparallelism[16]. In this section, we make theoretically analysis of these parameters, and demonstrate how to choose the\nmodel parameters under different parallel strategies. The notations used are presented in Table 1.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\n(a) LM\n (b) PLM\nFigure 1: Schematics of (a) LM and (b) PLM. Solid orange lines stand for fully-visible mask in preﬁx portion of input,\nand blue dot lines stand for casual mask.\nSymbol Notation\np The size of pipeline parallelism.\nt The size of tensor parallelism.\nd The size of data parallelism.\nn The number of GPUs. p·t·d= n\nB Global batchsize\nb Micro-batchsize\nm The number of micro batches in a model pipeline group,m= 1\nb ·B\nd\nL The number of Transformer layers\nh Hidden size\nS Sequence length\nl The number of layers in each pipeline stage, l= L/p\nTable 1: Notations in theoretical analysis of model parameters.\n2.2.1 Tensor Parallelism\nFigure 2: Schematics of tensor parallelism.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nIn tensor parallelism, the layers of a model are partitioned among devices within a node. The schematic of tensor\nparallelism is presented in Figure 2. In Transformer, tensors of Attention and MultiLayer perceptron (MLP) are split\nby row or column during forward and backward computing. Input tensor is broadcasted to each accelerator, in which\nmakes forward propagation. When the forward pass of Attention or MLP is ﬁnished, an all-reduce is performed. Then\nthe results are updated on all devices and sent to the next Layer. There are four all-reduce operations in the forward and\nbackward propagation per layer.\nThe ratio of the computation time to data communication time per layer ftp is,\nftp = 96t\n8 (t−1)\n(\nh+ S\n6\n)\n(2)\nAccording to Eq. 2, ftp of the tensor parallelism increases with h and S. The value of S is usually chosen as 512, or\n1024 [3, 7]. Because memory requirements of Attention is quadric to S, sparse Attention structure is necessary if S is\nincreased to 2048[6]. In order to save memory of accelerators and enable larger S, the activations are recomputed in\nbackward propagation[17]. With this method, the model can be trained with S = 2048smoothly with normal Attention\nstructure.\n2.2.2 Pipeline Parallelism\nFigure 3: Schematics of pipeline parallelism. F0, ... , F3 means forward pass in pipeline stage 0, ... , stage 3, while B0,\n... , B3 means backward pass in pipeline stage 0, ... , stage 3.\nFor language models with hundreds of billions parameters, the parameters can hardly be stored in a single node. Pipeline\nparallelism spliting the layers of LM among multiple nodes, is applied to solve the above mentioned problem (Fig. 3).\nEach node is one stage in the pipeline, which receives outputs from the previous stage and sends results to the next one.\nA node will be idle if the inputs received from its previous neighbor is not ready. The idle time for a pipeline is called\npipeline bubble[16, 18]. To increase the performance of pipeline parallelism, we have to decrease the time spent on\npipeline bubble. The fraction of ideal time spent in the pipeline bubble fpb is,\nfpb = (L/l−1)\nm (3)\nAccording to Eq. 3, the time spent on pipeline bubble increases with the number of layers L, and decreases with the\nnumber of micro-batch size m. There will be a better performance if m≫L/l. In pipeline parallelism, the ratio of the\ncomputation time to data communication time per node fpp is,\nfpp = 24L\np\n(\nh+ S\n6\n)\n(4)\nAccording to Eq. 4, the computational efﬁciency of a pipeline node improves with the increase of the values of h and S,\nwhich is similar to the situation of tensor parallelism.\n2.2.3 Data Parallelism\nThe global batch size is split among pipeline groups by data parallelism (Fig. 4). Each pipeline group with a copy of\nthe model is fed by local batches. In data parallelism, the ratio of computing time to communication time fdp is,\nfdp ≈4BSd\nd−1 (5)\nBecause d is often far greater than 1, Eq. 5 can be simpliﬁed to,\nfdp ≈4BS (6)\nThe computing efﬁciency improves with the increase of the global batch size B and sequence length S. Because the\nmemory requirements is quadric to sequence length S, increasing the global batch size seems to be a more effective\nway. However, there will be numerical instabilities during training when global batch size is too large [5, 19]. To avoid\nnumerical divergence, the global batch size is kept smaller than 107 tokens.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nFigure 4: Schematics of data parallelism.\n2.2.4 The principles of model parameters selections\nIn summary, we follow the rules below to select model parameters,\n• Increase the sequence length as much as possible, as it beneﬁts the tensor parallelism, pipeline parallelism, and\ndata parallelism. Because the memory requirement is quadric to the sequence length, it is worthy to re-compute\nactivations in the backward propagation to save memories.\n• Too many layers in language model have negative effect in performance, because it increases the time spent on\npipeline bubble.\n• Increasing the hidden size improves the performance of both tensor parallelism and pipeline parallelism.\n• Increasing the number of micro batches in a node improves the performance of pipeline parallelism. Increasing the\nglobal batch size improves the performance of data parallelism.\nThree models (Yuan LM-13B, Yuan 13B-PLM and Yuan 245B) are trained with parameters presented in Table 2.\nModel Layers Hidden\nsize\nGlobal\nBS\nMicro\nBS\nSequence\nLength\nt p d GPUs Parameters\n(billion)\nTraining\ntokens\n(billion)\nYuan LM-13B 40 5120 2688 4 2048 8 2 112 1792 12.87 300\nYuan PLM-13B 40 5120 2688 4 2048 8 2 112 1792 12.87 300\nYuan 245B 76 16384 3360 1 2048 8 38 7 2128 245.73 180\nTable 2: Parameters of Yuan models.\n3 Dataset\nA Chinese corpus with 5TB high-quality text is built, which is sufﬁcient to train Yuan 245B model without sampling\nthe dataset twice. To our best knowledge, this is the largest Chinese text dataset compared with CLUECorpus2020\n(100GB) [20], PanGu Corpus (1.1TB) [7], WuDaoCorpus2.0 (2.3TB Chinese text data and 300GB English text data)\n[11], and ERNIE 3.0 (4TB) [ 21]. In order to obtain the high-quality dataset, we develop a Massive Data Filtering\nSystem (MDFS) built on Spark to clean and ﬁlter the raw data, and train a Bert-based model to select high quality\nsamples. MDFS is consisted of three parts, data collection, coarse ﬁltering and ﬁne ﬁltering (Fig. 5). The raw data is\ncollected from Common Crawl, Sogou News, SogouT, Encyclopedia, and Books (Table 3). To process these raw data,\nwe run MDFS system on a high performance cluster with 36 nodes.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nFigure 5: Procedure of dataset processing.\nCategory Size (GB) Data source\nCommon Crawl 866,304 Web pages from 2017 to 2021\nPublic datasets 16,797 SogouT, Sogou News\nEncyclopedia 37 Baidu Baike, Wikipedia\nBooks 4,020 Books in philosophy, history, humanities, novel, etc.\nTable 3: Composition of raw data.\n3.1 Coarse Filtering\nThe coarse ﬁltering includes the following modules:\n• Article Extraction: Extracts contents of articles from crawled web pages, and removes ﬁeld like WARC headers,\nhyperlinks, etc. The following rules are applied,\na) A paragraph started with W ARC keyword is discarded.\nb) A paragraph without a valid punctuation at the end is discarded.\nc) A paragraph that contains neither English character nor Chinese character is discarded.\n• Empty Article Filtering: Removes empty articles.\n• Chinese Text Filtering: Selects Chinese texts with the following rules,\na) An article with less than 30 Chinese characters is removed.\nb) An article is removed if the percentage of Chinese characters is less than 60.\nc) After this step, the data size of Common Crawl is decreased from 866,304GB to 12,200GB.\n• Traditional to Simpliﬁed Chinese: Converts traditional Chinese characters to simpliﬁed Chinese characters.\n• Sensitive Words Filtering: Removes articles or paragraphs that include sensitive words. 9,759 sensitive words are\ncollected and classiﬁed into black and blue categories.\na) An article with words in black category is removed.\nb) A paragraph that contains words in blue category is removed, and other paragraphs in the same article is kept.\n• Symbol ﬁltering: Removes junk symbols, such as invisible Unicode characters, invisible ASCII characters and\nspeciﬁc punctuations.\n3.2 Fine Filtering\nTo extract high quality articles based on course ﬁltering text, we train a Bert-based model to classify high quality, low\nquality and advertisements. A datasets labeled with high quality articles, low quality articles, and advertisements is\nbuilt to train this model.\nAbout 2TB data is removed by the model, and 50% of the removed data are identiﬁed as advertisements. Considering\nthat the advertisements may also contain complete semantic information, we evaluate them manually to determine\nwhether it is necessary to recall. The processed dataset scatters on 36 nodes. To avoid additional bias from human,\nwe sample two sets on each node, and each set is evaluated by different reviewers. Parts of the statistical results are\nshown in Table 4. The similar percentage of high quality data for Sample1 and Sample2 indicates a high consistency in\ndata evaluation. As the percentage of high-quality data in advertisements is fairly low, it is reasonable to discard all\nadvertisements. In the manual review, we ﬁnd a 2.4% duplication rate in high-quality contents, while the duplication\nrate is 12.6% in advertisements. De-duplication is further applied to the high quality data.\nThe data size after ﬁne ﬁltering is shown in Table 5. The total size of high-quality dataset is 5.02TB. During training,\nonly articles with more than 150 characters are sampled.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nserver ID Sample1 Sample2\ncc01 1.25 0.60\ncc02 2.17 1.85\ncc03 2.88 3.43\ncc04 6.59 6.48\ncc05 1.02 1.49\ncc06 1.08 1.13\ncc07 0.64 0.72\ncc08 0.70 0.86\ncc09 0.65 0.60\ncc10 0.94 1.04\ncc11 0.70 0.81\ncc12 0.40 0.39\nmean 1.59 1.62\nTable 4: Percentage of high-quality data in advertisements on different servers.\nCategory Size before (GB) Size after (GB)\nCommon Crawl 866,304 4,200\nPublic datasets 16,797 268\nEncyclopedia 37 10.5\nBooks 4,020 655\nTable 5: Size of high-quality data after ﬁne ﬁltering.\n4 Experiments and Results\nThe Yuan models are trained on a cluster with 2128 GPUs. A stable real performance of 45% of the theoretical peak\nperformance is achieved on this cluster. Adam optimizer is used for training Yuan models. Please refer to Table 6 for\nmore details. To stabilize the training process, a linear warm up of learning rate is taken over the ﬁrst 1% tokens, then\nthe learning rate follows a cosine curve that slowly decays to 10% of its original value. The global batch size also\nlinearly increases to the full value over the ﬁrst 2% tokens, then it is kept till the end of training. During the training we\npack multiple documents into a single sequence of size 2048, and the documents are separated with a special token\n\"<eod>\". The tokens in the sequence are not masked in any way.\nModels Learning Rate Weight Decay β1 in Adam β2 in Adam\nYuan LM-13B 1.0E-4 0.01 0.9 0.95\nYuan PLM-13B 1.0E-4 0.01 0.9 0.95\nYuan 245B 1.6E-4 0.1 0.9 0.95\nTable 6: Hyper-parameters for Yuan models.\n4.1 Tasks description\nThe models are mainly evaluated on FewCLUE and ZeroCLUE[20], which can be classiﬁed into 4 categories, including\ntext classiﬁcation, Winograd Schema, natural language inference, and reading comprehension.\nText Classiﬁcation is consisted of sentimental classiﬁcation (Eprstmt: E-commerce Product Review Dataset for\nSentiment Analysis), news title classiﬁcation (Tnews: Toutiao Short Text Classiﬁcation for News), app description\nclassiﬁcation (Iﬂytek: Long Text classiﬁcation), and subject classiﬁcation (Csldcp: Chinese scientiﬁc literature subjects\nclassiﬁcation).\nEprstmt is a binary classiﬁcation with positive and negative product reviews. Tnews, Iﬂytek and Csldcp are multi-class\nclassiﬁcation with 15, 118 and 67 categories respectively. On tasks with labels as 0 or 1 or in English, we assign each\nlabel with a semantically Chinese meaningful name. For labels longer than one token, we convert those into one-token\nlabels with the same meaning. For all text classiﬁcation tasks, label is appended to the end of a sentence, connected\nwith prompt words. Our generative model predicts the label based on a given sentence, and calculate the probability\nP(label|sentence) of each candidate. The candidate with the largest probability is selected as the prediction.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nWinograd Schema task (Wsc) is a disambiguation task determining which noun a pronoun refers to. It is treated as a\nbinary classiﬁcation task in our evaluation.\nNatural Language Inference (NLI) includes Ocnli and Bustm, which concerns the ability to understand the relation\nbetween two sentences. Both of the tasks provide two sentences and a label of 0 or 1. We calculate the cross entropy\nloss of the second sentence with a candidate label, and treat the label with the lowest loss as the prediction.\nReading Comprehension includes Chid[22] and Csl. Chid is a Chinese Idiom cloze test dataset. Each sentence has\na blank inside, and for each blank, there are 7 candidate idioms with 1 true choice. For this task, each candidate is\nﬁlled in the blank, and we calculate the cross entropy loss of each combination. The one with the lowest loss is the\npredicted true idiom. Csl can be treated as either a reading comprehension or a binary classiﬁcation task. An abstract is\nprovided along with 4 keywords in the dataset. If all keywords are consistent with the abstract, the label should be true\nor 1. Otherwise, the label should be false or 0. All keywords are appended to the end of abstract. We calculate the\ncross entropy loss of the part after ABSTRACT in condition of ABSTRACT. The one with smaller loss is the predictive\nresult.\nGereration tasks: In addition to FewCLUE and ZeroCLUE, there are two generation tasks, CMRC2018 and WebQA.\nCMRC2018 is a span text extraction task, consisted of articles followed by several questions, and the answer to a\nquestion is a segment of the corresponding article. WebQA is a closed book question answering task. Model is evaluated\nby directly answering questions of CMRC2018 and WebQA without any auxiliary information. The generated answer\nis evaluated with EM and F1 scores.\n4.2 Comparison of LM and Preﬁx LM\nYuan LM-13B and Yuan PLM-13B are evaluated on FewCLUE[23] and ZeroCLUE[20] (Table 7). The SOTA results of\nZeroCLUE is benchmarked with zero-shot, while that of FewCLUE are benchmarked with ﬁne-tune. The Zero-Shot\nresults are in-context learning without tuning parameters. Table 7(a) indicates both LM and PLM have convincing\nScores Bustm Chid Csl Csldcp Eprstmt Iﬂytek Ocnli Tnews Wsc\nHuman 82.48 88 87.1 84 68 90 66 90.3 71 98\nSOTA 49.881 69.25 60.85 50.63 26.41 78.09 25.69 37.04 52.93 51.03\nLM-13B 56.88 59.375 86.14 50 47.533 88.125 37.87 46.875 57.01 38.99\nPreﬁx LM-13B 55.83 56.875 85.63 48.13 46.57 88.125 38.82 48.125 57.468 32.7\nScores Bustm Chid Csl Csldcp Eprstmt Iﬂytek Ocnli Tnews Wsc\nHuman 82.48 88 87.1 84 68 90 66 90.3 71 98\nSOTA 70.16 76.7 69.45 76.57 59.55 88.05 45.77 72.02 73.87 73.1\nLM-13B 69.14 81.25 72.27 81.25 60.9 90.625 54.87 46.25 52.45 82.39\nPreﬁx LM-13B 72.66 83.75 76.24 83.75 66.44 90.625 57.78 58.125 56.74 80.5\nTable 7: Performance of Yuan models on ZeroCLUE and FewCLUE tasks. The results are measured on evaluation\ndataset. (a)Performance of Yuan models on ZeroCLUE tasks with Zero-Shot learning (top); (b) Performance of Yuan\nmodels on FewCLUE with Fine-tune (bottom).\nin-context learning capability. The zero-shot average scores of both LM and PLM are superior to the SOTA one.\nOn Csldcp, Tnews and Iﬂytek tasks, we surpass the zero-shot SOTA by a large margin. Our models also achieve\nstrong performance on Ocnli, which is 6-8 points larger than the zero-shot SOTA. Table 7(a) displays the results after\ncalibration and label expansion, and the methods in details will be discussed in the next section.\nOur supervised ﬁne-tuning method is aligned with the design of GPT[ 14]. The average scores for LM and PLM\nare comparable to the SOTA ones (Table 7(b)). Compared to the few-shot learning results, ﬁne-tune makes great\nimprovement to Bustm, Csl and Wsc. However, for Chid, Eprstmt, Tnews and Ocnli, which are strong in zero-shot,\nﬁne-tune contributes little or even have negative effect. The ﬁne-tuned accuracy is superior to the SOTA ﬁne-tune\nresults on 7 tasks including Bustm, Chid, Csl, Csldcp, Eprstmt, Iﬂytek and Wsc. We submitted the PLM on FewCLUE,\nand LM on ZeroCLUE. Both of them currently topped on the list (Table 8). Comparing the results of LM and PLM,\nwe note that LM performs better on Zero-Shot learning, while PLM outperforms with ﬁne-tune. Fine-tune in general\nbrings better accuracy in most tasks. However, ﬁne-tune costs tremendous computational resources for Yuan 245B\nmodel, which makes ﬁne-tune uneconomic. Accordingly, we choose LM as basic architecture of Yuan 245B model.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nScores Bustm Chid Csl Csldcp Eprstmt Iﬂytek Ocnli Tnews Wsc\nHuman 82.48 88 87.1 84 68 90 66 90.3 71 98\nSOTA 49.881 69.25 60.85 50.63 26.41 78.09 25.69 37.04 52.93 51.03\nLM-13B 59.024 57.5 87.5 51.66 47.92 84.99 34.58 45.22 64.47 62.76\nScores Bustm Chid Csl Csldcp Eprstmt Iﬂytek Ocnli Tnews Wsc\nHuman 82.48 88 87.1 84 68 90 66 90.3 71 98\nSOTA 70.16 76.7 69.45 76.57 59.55 88.05 45.77 72.02 73.87 73.1\nPLM-13B 71.451 72.05 87.4 80.53 62.75 88.45 47.31 45.12 68.33 87.93\nTable 8: Performance of Yuan models on (a) ZeroCLUE (top) and (b) FewCLUE (bottom) tasks. The results were\nevaluated by the CLUE server.\nFigure 6: Training loss of Yuan 245B model.\n4.3 Results of Yuan 245B model\nModels Number of Parameters PetaFlops-days Training loss\nGPT-3 175B 3640 1.73\nPanGu-α 200B 796.3 2.49\nYuan 245B 245B 4095 1.64\nTable 9: Comparison of GPT-3, PanGu-αand Yuan.\nFig. 6 presents the training loss curves of Yuan 245B model. The loss decreases rapidly at the ﬁrst 10B tokens, and gets\nﬂatten over a long tail.\nTable 9 shows the comparison of GPT-3, PanGu-αand Yuan 245B in training. The PetaFlops-day of PanGu-αand\nYuan is computed as,\n8 ×Number_of_tokens×Number_of_parameters\n8.64E19 (7)\nActivations are recomputed during backward propagation. The computing amount of Yuan 245B is much greater than\nthat of PanGu-α. The training loss of Yuan 245B is the smallest among these three models.\nTable 10 compares the generation results between Yuan and recently published Chinese pretrained language models,\nPangu-α[7] and Ernie 3.0[21]. The average scores of Yuan outperform Pangu-αand Ernie 3.0 by a large margin on\nClose-book QA[24] and Span Extraction reading comprehension[25], which proves the excellent zero-shot generation\ncapacity of Yuan 245B. Regarding WebQA, Yuan signiﬁcantly improves the performance, no matter evaluated with EM\nor F1 Score. For CMRC2018, Yuan also achieved a better averaged score and F1 score compared to the SOTA, and it is\nlittle worse on EM compared to the SOTA.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nWebQA Score EM F1\nPanGu-α 9.8 5.13 14.7\nErnie 3.0 (SOTA) 30.74 22.53 38.95\nYuan 245B 40.467 30.57 50.36\nCMRC2018 Score EM F1\nPanGu-α 10.37 1.46 19.28\nErnie 3.0 (SOTA) 16.61 7.61 25.61\nYuan 245B 27.37 5.58 49.17\nTable 10: Performance of Pangu-α, Ernie 3.0, and Yuan 245B model on WebQA (left) and CMRC2018 (right) tasks.\n4.3.1 Accuracy improvements on X-Shot learning\nA noticeable shortcoming of in-context learning lies in its bias towards template sentences and labels. The bias mainly\ncomes from a dataset with imbalance distribution between classes, few-shot examples with a certain order, and labels\nwith different frequencies in the training corpus[ 26]. The extraneous bias limits model’s performance on natural\nlanguage tasks. Considering the probable sources of bias, we take calibration for in-context learning in two aspects: a\ncalibration on the calculation of probability, and an expansion of labels.\nBased on previous work on calibration[26, 27], the model’s bias can be ﬁxed with an empty text. We use a similar\nmethod to calibrate our in-text prediction.\narg maxP(prediction|givensentence)\nP(prediction|void) (8)\nTake Tnews and Ocnli as examples. In the case of Tnews, we calculate the probability of the last token in the sentence-\nlabel combination, which is actually a prediction of label.\nOrig：新闻：sentence。这条新闻是关于label。\nV oid：新闻：N/A。这条新闻是关于label。\nWith calibration, we calculate,\narg maxP(label|新闻：sentence。这条新闻是关于)\nP(label|新闻：N/A。这条新闻是关于) (9)\nIn the case of OCNLI, which is a two-sentence task, we calculate the cross entropy loss of the second sentence.\nOrig: sentence1? 对/错/可能，sentence2.\nV oid：N/A？对/错/可能，sentence2.\nWith calibration, we calculate:\narg maxP(label|sentence1?对/错/可能)\nP(label|N/A?对/错/可能) (10)\nIn the experiment, we ﬁnd that the difference in label frequency are inﬂuential to the prediction. The ideal condition is\nthat all labels have approximately the same frequency, but it is too tough for manual selection. We choose an Embedding\nCorpus[28] that covers over 8 million Chinese words and phrases, to assist us reveal the relevance between words. Each\nlabel is expanded into 5 synonyms, in order to reduce the bias led by a single label word or phrase.\nThree tasks are selected to display the effect of calibration, and the results are presented in Table 11. The combination\nof model calibration and label expansion leads a dramatic improvement for Eprstmt, which is a binary sentimental\nanalysis. For multi classiﬁcation on news (Tnews) and scientiﬁc literature (Csldcp), calibration also brings better results\nto a large extent.\nCsldcp Eprstmt Tnews\nOriginal 37.96 54.3 53.28\nCalibration and label expansion 48.017 86.88 57.19\nTable 11: Accuracy improvement of Yuan 245B model by calibration and label expansion. The results were evaluated\non ZeroCLUE evaluation datasets.\nFigure 7 presents the Zero-Shot and Few-Shot results of Yuan 245B on ZeroCLUE tasks. In Few-Shot, the number of\nsamples is determined by the number of classes. We take 4 samples for binary classiﬁcation, and 3 samples otherwise.\nCompared with zero-shot, we note that few-shot learning brings steady improvements on accuracy for most tasks,\nexcept Csldcp and Iﬂytek. Few-shot leads to a disastrous decrease for Iﬂytek with a near-random result. With the same\nmethod, few-shot has positive effect for Eprstmt (2 classes) and Tnews (15 classes). We observe similar situations in\nYuan LM-13B, and Yuan PLM-13B. Because there are large number of classes in Csldcp (67 classes) and Iﬂytek (118\nclasses), and the samples cannot cover all the classes, the bias caused by samples concatenated in the input make the\nmodel predication getting worse.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nFigure 7: Performance of Yuan 245B in Zero-Shot and Few-shot learning.\n4.3.2 Text generation of Yuan 245B model\nIn order to see how well Yuan can generated text, we arbitrarily select 24 articles that Yuan 1.0 generates, including 4\ncouplets, 5 traditional and modern Chinese poetries, 5 news articles, 5 stories and 5 dialogues. Couplet, poetry and\ndialogue can be seen as short-text task ( 10-20 tokens), while news and story generation can be seen as long-text task\n( 300 tokens). In comparison, the human-written articles are masterpieces from Chinese poems, pieces of classic novels,\nnews articles from Sohu News, and dialogues from LCCC-large dataset[29]. Participants are asked to select whether\nthe article was \"written by a human\" or \"written by a model\". We collect 83 valid questionnaire. According to our\ninterview, most interviewee will choose \"the better one\" as the article created by human.\nFigure 8: Human accuracy at detecting articles written by Yuan 245B model.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nInput Human Model Accuracy\nDia-logue 我好心问你\n吃了饭没，\n你又骂我\n宝贝儿我错了，再也不敢了 我又没骂你，你自己对号入座 38.28%\nPoetry 作一首七言\n诗\n塞上长城万里长，\n漫天风雪映重峦。\n一边江水人心去，\n满目青山绝照还。\n满目青山夕照明，\n愿随江舟泛轻风。\n今生常忆多情事，\n香闺如梦领神会。\n65.44%\nCouplet 上联：春来\n人入画，下\n联：\n福至梦开花 夜半月当灯 38.28%\nStory 起床穿衣，\n洗漱完毕，\n我 走 下 楼\n去。心情甚\n至有些愉快\n地向科芬道\n了早安。\n起床穿衣，洗漱完毕，我走下楼去。心情甚\n至有些愉快地向科芬道了早安。\n尽管这家伙昨天跟我开了个不 大不小的玩\n笑。\n酒吧里已经聚了很多人了，住店的客人都来\n了。昨天没来得及细看，今天一注意才知\n道，这些人绝大多数都是捕鲸者：大副、\n二副、三副；铁匠、木匠、铜匠；还有标枪\n手、守船人，不一而足。\n他们的肤色一律棕黑，衣着随便，蓄胡子的\n人占了很大的比例。\n你可以通过他们的外表来判断他们在岸上已\n经呆了多长时间了。\n瞧，这个小伙子，两颊赤红，像烤过的梨，\n他从印度洋回来不超过三天。\n他旁边那一位，颜色没他那么重，身上似乎\n有点锻木的味道，他上岸有一个星期了。\n有的人脸上只剩下一丝隐隐约约的热带的黄\n色了，他们在岸上肯定已经呆了好几个星期\n了。\n不过，谁也不能从魁魁格的面色来判断他上\n岸的时间。\n却没想到，下一刻，我就受到了一个不应该\n出现在这里的人的冷待。\n那人一身笔挺的深色西装，靠在车身上，一\n双灰蓝色的眼眸，充满了不满与敌意，打量\n着我。\n科芬几乎是立刻挡在了我面前，警戒地看着\n眼前的人，道：“特里。”\n我眨了眨眼，大脑一时有些空白。\n我努力地思考，到底发生了什么事。\n这个人，应该是叫特里没错吧。\n他应该是莫里的哥哥，但是他为什么会出现\n在这里？他不是应该在中央星吗？\n他为什么要用这种眼神看着我？\n我小心地打量着特里，不明白他的敌意是从\n哪里来的。\n“科芬，你先去上班吧。”我说道。\n“我知道了，林恩先生。”科芬看了特里一眼，\n才一步三回头地走向悬浮车。\n我在心底叹息了一声，走上前去，伸出手，\n道：“你好，特里，我是林恩。”\n特里面无表情地看了我一眼，伸出手与我交\n握。\n40.75%\nNews 摘 要 ： 辽\n宁主汛期首\n日遭暴雨侵\n袭当地多举\n措防洪涝 灾\n害。正文：\n中 新 网 沈\n阳7月22日\n电(司晓帅)\n辽宁省22日进入主汛期,一场自西向东的区域\n性暴雨过程也随之而来。记者当日下午从\n辽宁省防汛抗旱指挥部获悉,该省已采取多\n项举措应对此次降雨过程,严防洪涝灾害的\n发生。据辽宁省气象部门介绍,从7月21日8时\n至22日14时,该省西部降中到大雨,局部大暴\n雨,其它地区降小到中雨。西部地区平均\n降雨量达65毫米,最大点雨量为葫芦岛地区\n要路沟站达150毫米,100毫米以上 点雨量站\n达17处,50毫米以上点雨量站160处。\n记者22日从辽宁省防汛抗旱指挥部办公室\n获悉，今年主汛期首日，辽宁遭到强降雨\n袭击，全省平均降雨 量达39毫米，为历年\n同期最高。其中，沈阳、大连、鞍山、抚\n顺、本溪、丹东、锦州等8个市平均降雨量\n在40毫米以上，最大降雨量出现在本溪市，\n达70.5毫米。据辽宁省气象部门预测，22日夜\n间到23日，辽宁全省还将有一次大到暴雨过\n程。\n34.15%\nTable 12: Samples generated by human and Yuan 245B model.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nNo Input Zero-Shot One-Shot Three-Shot\n1 好事流传千古 红眼盯住一人 高风亮节万年 佳音永驻万家\n2 清风明月自来往 白鹤白鹿随去来 好鸟好花留我住 白鹤黄莺长往还\n3 三江顾客盈门至 四海宾朋 四海财源进宝来 四海宾朋满座来\n4 和风吹绿柳 小鸟唱好诗 细雨点红桃 细雨点红梅\n5 三阳日照平安宅 三阳居财丁兴旺 五福星临吉庆门 五福星临福寿门\nTable 13: Couplets generated by Yuan 245B with Zero-Shot, One-Shot and Three-Shot.\ninput Output\n“悬命”是拼命，接近全力，做出最大努力的\n意思。他凡事必悬命，绝不打马虎眼。用“悬\n命”造句：\n他这个人做事太悬命了，为了取得第一手材\n料，连饭也顾不上吃\n“腕前”与本领的意思相同。例：“他只使出了\n了一小部分腕前，就把事情完成得如此出\n色”。用“腕前”造句：\n这个人的腕前很大，他很能完成这项任务\n气持”是心情、精神状态的意思。例：“那种消\n极的气持又控制了他”。用“气持”写一句话：\n他的气持，使我无法接近\n“油断”意思是漫不经心，马虎，轻率。举\n例：“她虽然在戏中只扮演一个次要的角色，\n但她并没有半点油断，精彩的表演得到大家\n的称赞”。请用“油断”写一个句子：\n他的学习成绩很好，因为他并没有半点油断\n“胜手”意思是最得意拿手 的，擅长的。比\n如，“别的饭食我不会做，只有做面条还胜\n手”。请用“胜手”一词造句：\n我的作文水平在班里名列前茅，是同学们公\n认的胜手\nTable 14: The ability of imitation of Yuan.\nThe human accuracy at detecting articles created by Yuan 1.0 is 49.16%, which implies the difﬁculty for participants to\ndistinguish human-written and model-written articles, especially in regard to modern Chinese poetries and articles (Fig.\n8). The generation of news (42.12,%) and stories (49.15%) convinces us with excellent long-text generation capacity.\nSome of model-written articles are even better than parts of masterpieces in the view of our participants. The generation\nof couplets and poetries indicate that Yuan 1.0 is able to create text with rules and forms of ancient Chinese (Table 12)\nalthough ancient Chinese is not strengthened in our pretraining corpus. Yuan can also make a dialogue aligned with\nhuman’s expectation (45.68%). Yuan is able to generate articles, such as news, and stories, which is hard to tell whether\nthe article is human-written or model-written. However, you will see repetition to some extent, if the model is required\nto create an article with more than 1000 Chinese characters.\nFew-shot is also effective for text-generation, especially text with a certain format. Table 13 shows the couplets Yuan\ncreated with zero-shot, one-shot and three-shot, under the same set of hyper-parameters. Few-shot mainly contributes\nto: (a) increasing the stability and completeness of generation (No. 3); (b) more reasonable semantic meaning and\naccordant written style (No. 1); (c) avoidance of the word repetition that is a taboo for couplet creation (No. 2, 5); and\n(d) antithetically better (No. 4). The ability of imitation of Yuan is evaluated via learning and utilizing a brand new\nword. A deﬁnition and an example sentence are given in the input, and the model will write a new sentence with the\ngiven information. The nonexistent words include nouns and adjectives. Table 14 displays the one-shot examples. In\nall cases, the model makes approximately correct applications with the nonexistent Chinese words, which implies the\nlearning and imitation ability of our model. This ability is especially effective when model aids scientiﬁc article writing,\nas tremendous deﬁnition in academic articles could be alien to Yuan.\nIn few-shot and text generation experiments, we note the pre-trained language model’s sensitivity to steering, which\ncould be a potential risk on its application. Given an input without bias, the opinion created by the model could be either\npositive or negative. However, if given an input with a strong standpoint, the model tends to continue the article with\nthe same opinion. In the case displayed in Table 15, we would like to talk about the women status in the society, and\ntwo contrasting options are given as the input. Given \"traditional patriarchy still holds sway\", Yuan express the opinion\nas \"women status is determined by their fertility\". On the opposite, given \"40% of the labor force are women. There are\nmore readable girls and women than ever\", the model also follows the opinion as \"women have the competence to any\nwork\". It is proved that a model with no certain bias can be easily steered by human. As the capability of generating\n\"human-written-style\" article strengthen the risk in model abuse, the application of model needs to be regulated.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\nNegative Positive\nInput 由于种种历史原因，社会中父\n权制文化根深蒂固。父权制文\n化将现实存在的性别歧视看作\n是自然形成的、理所当然的和不\n可改变的，是由妇女的生理状况\n决定的。这种观念已经渗透到了\n社会文化之中。社会公众和一些\n政治家的言论中充斥着对妇女的\n陈规定型的偏见。20世纪50年代\n至60年代，“让妇女返回家庭”成\n为一股强大的社会思潮。人们普\n遍认为，妇女的角色就是生儿育\n女，成为贤妻良母。关于男性和\n女性在社会地位上是否存在的差\n异，我认为\n过去四分之一世纪里女孩和妇女\n的生活发生了巨大的变化。现在\n识字的女孩和妇女比历史上任何\n时候都多，而且在三分之一的发\n展中国家，学校里的女生人数多\n于男生。妇女现在占全球劳动力\n的40%。而且，在全世界所有地\n区，女性的平均寿命都超过了男\n性。这些变化发生的速度是惊人\n的，关于男性和女性在社会地位\n上是否存在的差异，我认为\nOutput 现在已经不需要再做任何的争论\n了。我相信，女性的社会地位和\n她们的生育能力之间有着直接的\n联系。女性的生育能力是社会地\n位的基础。女性在家庭中的地位\n是由她们的生育能力决定的。如\n果女性不能生育，那么她们在家\n庭中的地位也将下降。\n这个问题应该由社会来决定，而\n不是由妇女来决定。我认为，女\n性在工作中的表现与男性一样，\n她们有能力胜任任何工作。在\n现实生活中，她们中的很多人工\n作得比男性更出色。我希望我的\n女儿将来能够成为一名出色的律\n师、总统候选人、医生、总统国\n家安全事务顾问，或者宇航员。\n但是，我希望她不要因为别人告\n诉她不可以做什么而轻易放弃自\n己的理想。\nTable 15: The generation of Yuan 245B by positive and negative inputs.\n5 Conclusion\nWe proposed the current largest singleton language model Yuan 1.0 with 245B parameters that achieved good per-\nformance on different NLP tasks in Zero-Shot and Few-Shot learning. The architecture of Yuan 1.0 was designed\nby incorporating model structure with key factors that affects performance of large-scale distributed training. The\ntraining process achieved excellent performance on 2128 GPUs. Yuan 1.0 was trained on a new Chinese dataset of 5TB\nhigh-quality text that was built on 850TB raw data from Internet. Zero-Shot and Few-Shot performance was steadily\nimproved by calibration and label expansion. We found that the preﬁx language model that performs better in Pre-train\nand Fine-tune pattern behaves differently in Zero-Shot learning, while language model behaved the opposite. Yuan 1.0\nmodels achieved the state-of-the-art results on ZeroCLUE, FewCLUE, and generation tasks. The articles generated by\nYuan 1.0 are difﬁcult to distinguish from those written by humans.\nReferences\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention\nis all you need,” Advances in Neural Information Processing Systems, vol. 2017-Decem, pp. 5999–6009, jun 2017.\n[2] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov,\n“RoBERTa: A Robustly Optimized BERT Pretraining Approach,” 2019.\n[3] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu, “Exploring the\nlimits of transfer learning with a uniﬁed text-to-text transformer,” Journal of Machine Learning Research, vol. 21,\n2020.\n[4] S. Narang, H. W. Chung, T. Yi, W. Fedus, and C. Raffel, “Do transformer modiﬁcations transfer across implemen-\ntations and applications?,” 2021.\n[5] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei, “Scaling Laws for Neural Language Models,”arXiv, 2020.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\n[6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, “Language Models are Few-Shot Learners,” 2020.\n[7] W. Zeng, X. Ren, T. Su, H. Wang, Y . Liao, Z. Wang, X. Jiang, Z. Yang, K. Wang, X. Zhang, C. Li, Z. Gong,\nY . Yao, X. Huang, J. Wang, J. Yu, Q. Guo, Y . Yu, Y . Zhang, J. Wang, H. Tao, D. Yan, Z. Yi, F. Peng, F. Jiang,\nH. Zhang, L. Deng, Y . Zhang, Z. Lin, C. Zhang, S. Zhang, M. Guo, S. Gu, G. Fan, Y . Wang, X. Jin, Q. Liu,\nand Y . Tian, “PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel\nComputation,” 2021.\n[8] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen, “GShard: Scaling\nGiant Models with Conditional Computation and Automatic Sharding,”arXiv, 2020.\n[9] W. Fedus, B. Zoph, and N. Shazeer, “Switch Transformers: Scaling to Trillion Parameter Models with Simple and\nEfﬁcient Sparsity,” pp. 1–31, 2021.\n[10] Z. Zhang, X. Han, H. Zhou, P. Ke, Y . Gu, D. Ye, Y . Qin, Y . Su, H. Ji, J. Guan, F. Qi, X. Wang, Y . Zheng, G. Zeng,\nH. Cao, S. Chen, D. Li, Z. Sun, Z. Liu, M. Huang, W. Han, J. Tang, J. Li, X. Zhu, and M. Sun, “CPM: A\nLarge-scale Generative Chinese Pre-trained Language Model,” 2020.\n[11] Z. Zhang, Y . Gu, X. Han, S. Chen, C. Xiao, and Z. Sun, “CPM-2 : Large-scale Cost-efﬁcient Pre-trained Language\nModels,” no. 2, 2021.\n[12] J. Lin, R. Men, A. Yang, C. Zhou, M. Ding, Y . Zhang, P. Wang, A. Wang, L. Jiang, X. Jia, J. Zhang, J. Zhang,\nX. Zou, Z. Li, X. Deng, J. Liu, J. Xue, H. Zhou, J. Ma, J. Yu, Y . Li, W. Lin, J. Zhou, J. Tang, and H. Yang, “M6: A\nChinese Multimodal Pretrainer,” mar 2021.\n[13] N. Alarcon, “Openai presents gpt-3, a 175 billion parameters language model.”https://developer.nvidia.\ncom/blog/openai-presents-gpt-3-a-175-billion-parameters-language-model/ .\n[14] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving Language Understanding by Generative\nPre-Training,”Encyclopedia of Autism Spectrum Disorders, pp. 2640–2640, 2021.\n[15] I. S. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, “Language Models are Unsupervised\nMultitask Learners,”OpenAI Blog, vol. 1, no. May, pp. 1–7, 2020.\n[16] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, “Megatron-LM: Training Multi-Billion\nParameter Language Models Using Model Parallelism,” 2019.\n[17] T. Chen, B. Xu, C. Zhang, and C. Guestrin, “Training Deep Nets with Sublinear Memory Cost,” 2016.\n[18] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V . A. Korthikanti, D. Vainbrand, P. Kashinkunti,\nJ. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia, Efﬁcient Large-Scale Language Model Training on\nGPU Clusters, vol. 1. Association for Computing Machinery, 2021.\n[19] Y . You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer, and C.-J. Hsieh,\n“Large Batch Optimization for Deep Learning: Training BERT in 76 minutes,” 2019.\n[20] L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y . Li, Y . Xu, K. Sun, D. Yu, C. Yu, Y . Tian, Q. Dong, W. Liu, B. Shi,\nY . Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y . Li, Y . Patterson, Z. Tian, Y . Zhang, H. Zhou, S. Liu, Z. Zhao, Q. Zhao,\nC. Yue, X. Zhang, Z. Yang, K. Richardson, and Z. Lan, “CLUE: A Chinese Language Understanding Evaluation\nBenchmark,” apr 2020.\n[21] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y . Zhao, Y . Lu, W. Liu, Z. Wu, W. Gong,\nJ. Liang, Z. Shang, P. Sun, W. Liu, X. Ouyang, D. Yu, H. Tian, H. Wu, and H. Wang, “ERNIE 3.0: Large-scale\nKnowledge Enhanced Pre-training for Language Understanding and Generation,” 2021.\n[22] C. Zheng, M. Huang, and A. Sun, “ChID: A Large-scale Chinese IDiom Dataset for Cloze Test,” jun 2019.\n[23] L. Xu, X. Lu, C. Yuan, X. Zhang, H. Xu, H. Yuan, G. Wei, X. Pan, X. Tian, L. Qin, and H. Hai, “FewCLUE: A\nChinese Few-shot Learning Evaluation Benchmark,” jul 2021.\n[24] Y . Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, and Y . Bisk, “WebQA: Multihop and Multimodal QA,” sep\n2021.\n[25] Y . Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, and G. Hu, “A Span-Extraction Dataset for Chinese\nMachine Reading Comprehension,” oct 2018.\n[26] T. Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh, “Calibrate Before Use: Improving Few-Shot Performance\nof Language Models,” feb 2021.\nYuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning\n[27] A. Holtzman, P. West, V . Shwartz, Y . Choi, and L. Zettlemoyer, “Surface Form Competition: Why the Highest\nProbability Answer Isn’t Always Right,” apr 2021.\n[28] Y . Song, S. Shi, J. Li, and H. Zhang, “Directional skip-gram: Explicitly distinguishing left and right context\nforword embeddings,”NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies - Proceedings of the Conference, vol. 2, pp. 175–180,\n2018.\n[29] Y . Wang, P. Ke, Y . Zheng, K. Huang, Y . Jiang, X. Zhu, and M. Huang, “A Large-Scale Chinese Short-Text\nConversation Dataset,” aug 2020."
}