{
    "title": "TFE: A Transformer Architecture for Occlusion Aware Facial Expression Recognition",
    "url": "https://openalex.org/W3209577450",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2342841966",
            "name": "Jixun Gao",
            "affiliations": [
                "Henan University of Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A2118385872",
            "name": "Yuan-Yuan Zhao",
            "affiliations": [
                "Zhengzhou University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2342841966",
            "name": "Jixun Gao",
            "affiliations": [
                "Henan University of Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A2118385872",
            "name": "Yuan-Yuan Zhao",
            "affiliations": [
                "Zhengzhou University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6778485988",
        "https://openalex.org/W1990586396",
        "https://openalex.org/W3120399736",
        "https://openalex.org/W2963112684",
        "https://openalex.org/W3002638829",
        "https://openalex.org/W6757010476",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W3093370878",
        "https://openalex.org/W3157525179",
        "https://openalex.org/W1970942140",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2251198138",
        "https://openalex.org/W6741276872",
        "https://openalex.org/W6800016538",
        "https://openalex.org/W3044955494",
        "https://openalex.org/W2904483377",
        "https://openalex.org/W6756547384",
        "https://openalex.org/W6763438500",
        "https://openalex.org/W2083816032",
        "https://openalex.org/W2730601341",
        "https://openalex.org/W2745497104",
        "https://openalex.org/W2982070372",
        "https://openalex.org/W2138206939",
        "https://openalex.org/W6785269020",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W3035336958",
        "https://openalex.org/W3003720578",
        "https://openalex.org/W3001722345",
        "https://openalex.org/W6755364507",
        "https://openalex.org/W1967022116",
        "https://openalex.org/W2613375858",
        "https://openalex.org/W2186222003",
        "https://openalex.org/W4289710724",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3122081138",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4234552385",
        "https://openalex.org/W3139434170",
        "https://openalex.org/W3191359587"
    ],
    "abstract": "Facial expression recognition (FER) in uncontrolled environment is challenging due to various un-constrained conditions. Although existing deep learning-based FER approaches have been quite promising in recognizing frontal faces, they still struggle to accurately identify the facial expressions on the faces that are partly occluded in unconstrained scenarios. To mitigate this issue, we propose a transformer-based FER method (TFE) that is capable of adaptatively focusing on the most important and unoccluded facial regions. TFE is based on the multi-head self-attention mechanism that can flexibly attend to a sequence of image patches to encode the critical cues for FER. Compared with traditional transformer, the novelty of TFE is two-fold: (i) To effectively select the discriminative facial regions, we integrate all the attention weights in various transformer layers into an attention map to guide the network to perceive the important facial regions. (ii) Given an input occluded facial image, we use a decoder to reconstruct the corresponding non-occluded face. Thus, TFE is capable of inferring the occluded regions to better recognize the facial expressions. We evaluate the proposed TFE on the two prevalent in-the-wild facial expression datasets (AffectNet and RAF-DB) and the their modifications with artificial occlusions. Experimental results show that TFE improves the recognition accuracy on both the non-occluded faces and occluded faces. Compared with other state-of-the-art FE methods, TFE obtains consistent improvements. Visualization results show TFE is capable of automatically focusing on the discriminative and non-occluded facial regions for robust FER.",
    "full_text": "METHODS\npublished: 25 October 2021\ndoi: 10.3389/fnbot.2021.763100\nFrontiers in Neurorobotics | www.frontiersin.org 1 October 2021 | Volume 15 | Article 763100\nEdited by:\nYong Li,\nNanjing University of Science and\nTechnology, China\nReviewed by:\nTong Zhang,\nNanjing University of Science and\nTechnology, China\nYuan Zong,\nSoutheast University, China\n*Correspondence:\nJixun Gao\ngaojixun@haue.edu.cn\nReceived: 23 August 2021\nAccepted: 13 September 2021\nPublished: 25 October 2021\nCitation:\nGao J and Zhao Y (2021) TFE: A\nTransformer Architecture for Occlusion\nAware Facial Expression Recognition.\nFront. Neurorobot. 15:763100.\ndoi: 10.3389/fnbot.2021.763100\nTFE: A Transformer Architecture for\nOcclusion Aware Facial Expression\nRecognition\nJixun Gao1* and Yuanyuan Zhao2\n1 Department of Computer Science, Henan University of Engineering, Zhengzhou, China,2 Department of Computer Science,\nZhengzhou University of Technology, Zhengzhou, China\nFacial expression recognition (FER) in uncontrolled envir onment is challenging due\nto various un-constrained conditions. Although existing d eep learning-based FER\napproaches have been quite promising in recognizing fronta l faces, they still struggle\nto accurately identify the facial expressions on the faces t hat are partly occluded in\nunconstrained scenarios. To mitigate this issue, we propos e a transformer-based FER\nmethod (TFE) that is capable of adaptatively focusing on the most important and\nunoccluded facial regions. TFE is based on the multi-head se lf-attention mechanism\nthat can ﬂexibly attend to a sequence of image patches to enco de the critical cues\nfor FER. Compared with traditional transformer, the novelt y of TFE is two-fold: (i) To\neffectively select the discriminative facial regions, we i ntegrate all the attention weights\nin various transformer layers into an attention map to guide the network to perceive the\nimportant facial regions. (ii) Given an input occluded faci al image, we use a decoder\nto reconstruct the corresponding non-occluded face. Thus, TFE is capable of inferring\nthe occluded regions to better recognize the facial express ions. We evaluate the\nproposed TFE on the two prevalent in-the-wild facial expres sion datasets (AffectNet\nand RAF-DB) and the their modiﬁcations with artiﬁcial occlu sions. Experimental results\nshow that TFE improves the recognition accuracy on both the n on-occluded faces\nand occluded faces. Compared with other state-of-the-art F E methods, TFE obtains\nconsistent improvements. Visualization results show TFE i s capable of automatically\nfocusing on the discriminative and non-occluded facial reg ions for robust FER.\nKeywords: affective computing, facial expression recognition, occlusion, transformer, deep learning\n1. INTRODUCTION\nFacial expressions are the most natural way for humans to expre ss emotions. Facial expression\nrecognition (FER) has received signiﬁcant interest from psyc hologists and computer scientists\nas it facilitates a number of practical applications, such as hum an-computer interaction, pain\nestimation, and aﬀect analysis. Although current FER systems have obtained promising accuracy\nwhen recognizing facial images captured in controlled scena rios, these FER systems usually suﬀer\nfrom considerable performance degradation when recognizin g expressions in the wild conditions.\nTo ﬁll the gap between the FER accuracy on the controlled face s and in-the-wild faces, researchers\nstart to collect large-scale facial expression databases in uncontrolled environment (\nLi et al., 2017;\nMollahosseini et al., 2017 ). Despite the usage of face images in the uncontrolled scenar io, FER is still\nchallenging due to the existence of facial occlusions. It is non-trivial to solve the occlusion problem\nGao and Zhao Transformer for Facial Expression Recognition\nbecause facial occlusions are various and abundant. These f acial\nocclusions may appear in many forms, such as breathing masks,\nhands, drinks, fruits, and other objects that might appear in f ront\nof the human faces in our daily life. The facial occlusions ma y\nblock any other part of the face, and the variability of occlus ions\nwould inevitably induce the decreased FER performance.\nPrevious studies usually handled FER under occlusion with\nsub-region-based features (\nKotsia et al., 2008; Li et al., 2018a,b;\nWang et al., 2020b ), e.g., Kotsia et al. (2008) presented a\ndetailed analysis on occluded FER and conclude that FER will\nsuﬀer from more decreased performance with occluded mouth\nthan the occluded eyes. With the popularity of the data-driven\nconvolutional neural network (CNN) techniques, a number\nof recent eﬀorts on FER have been made on the collection\nof large-scale facial expression databases and exploit CNN to\nenhance the performance of FER.\nLi et al. (2018a) proposed to\ndecompose facial regions in the convolutional feature maps wi th\nthe manually deﬁned facial landmarks and fused the local and\nglobal facial representations via attention mechanism. However,\nthe recent CNN-based FER methods lack the ability to learn\nglobal interactions and relations between distant facial pa rts.\nThese methods are not capable of ﬂexibly attending to distinc tive\nfacial regions for precise FER under occlusions.\nInspired by the observation (\nNaseer et al., 2021 ) that\ntransformers are robust to occlusions, perturbations, and d omain\nshifts, we propose a Transformer Architecture for Facial\nExpression Recognition (TFE) under occlusions. Currently,\nvision transformers ( Dosovitskiy et al., 2020; Li et al., 2021 )\nhave demonstrated impressive performance across numerous\nmachine vision tasks. These models are based on multi-head\nself-attention mechanisms that can ﬂexibly attend to a sequ ence\nof image patches to encode contextual cues. The self-attenti on\nin the transformers has been shown to eﬀectively learn global\ninteractions and relations between distant object parts. A n umber\nof following studies on downstream tasks such as object dete ction\n(\nCarion et al., 2020 ), segmentation ( Jin et al., 2021 ), and video\nprocessing ( Girdhar et al., 2019; Fang et al., 2020 ) have veriﬁed\nthe feasibility of the transformers. Given the content-depe ndent\nlong-range interaction modeling capabilities, transforme rs can\nﬂexibly adjust their receptive ﬁeld to cope with occlusions in data\nand enhance the discriminability of the representations.\nIntuitively, human perceives the facial expressions via several\ncritical facial regions, e.g., eyes, eyebrows, and corners of the\nmouth. If some facial patches are occluded, human may judge\nthe expression according to the other highly informative reg ions.\nTo mimic the way that human recognizes the facial expression,\nwe propose a region selection unit (RS-Unit) that is capable\nof focusing on the important facial regions. To be speciﬁc,\nRS-Unit selects the discriminative facial regions and remove s\nthe redundant or occluded facial parts. We then combine the\nglobal classiﬁcation token with the selected part tokens as\nthe facial expression representation. With the proposed RS-\nUnit, TFE is able to adaptively perceive the distinctive and\nunobstructed regions in facial images. To further enhance t he\ndiscriminability of the representation, we exploit an auxili ary\ndecoder to reconstruct the corresponding non-occluded face .\nThus, TFE is capable of inferring the occluded facial regions via\nthe unoccluded parts to better recognize the facial expressio ns.\nFigure 1 illustrates the attention map of TFE on some facial\nimages. It is clear that TFE is capable of focusing on the criti cal\nand unoccluded facial parts for robust FER. More visual examples\nand explanations can be seen in section 4.2.1.\nThe contributions of this study can be summarized\nas follows:\n1. We propose a transformer architecture to recognize facial\nexpressions (TFE) from partially occluded faces. TFE consists\nof a region selection unit (RS-Unit) that automatically\nperceives and selects the critical facial regions for robust\nFER. TFE is deployed to focus on the most important and\nunoccluded facial regions.\n2. To further enhance the discriminability of the facial\nexpression representation, TFE contains an auxiliary image\ndecoder to reconstruct the corresponding non-occluded face.\nThe image decoder is merely exploited during the training\nprocess and incorporates no extra computation burden at\ninference time.\n3. Qualitative experimental results show the beneﬁts and the\nadvantages of the proposed TFE over other state-of-the-art\napproaches on two prevalent in-the-wild facial expression\ndatabases. Visualization results additionally show that TF E is\nsuperior in perceiving the informative facial regions.\n2. RELATED WORK\nWe discuss the previous literatures that are related to\nour proposed TFE, i.e., FER with occlusions and the\nvision transformer.\n2.1. Methods for FEE Under Occlusion\nFor FER tasks, occlusion is one of the inevitable challenges in\nreal-world scenarios. We just classify previous FER methods\ninto two classes: handcrafted features-based methods and d eep\nlearning-based approaches.\nEarly FER under occlusion methods typically encode\nhandcrafted features from face samples, and then learn classi ﬁers\nbased on the encoded features (\nRudovic et al., 2012; Zhang et al.,\n2014). Liu et al. (2013) proposed a novel FER method to mitigate\nthe partial occlusion issue via fusing Gabor multi-orientation\nrepresentations and local Gabor binary pattern histogram\nsequence.\nCotter (2010) introduced to use sparse representation\nfor FER. Especially, Kotsia et al. (2008) analyzed how partial\nocclusions aﬀect FER performance and found that FER suﬀers\nmore from mouth occlusion than the equivalent eyes occlusion .\nOver the recent years, Convolution Neural Network (CNN)\nhas shown exemplary performance on many computer vision\ntasks (\nSchroﬀ et al., 2015; Krizhevsky et al., 2017; Li et al.,\n2020). The promising learning ability of deep CNN can be\nattributed to the use of hierarchical feature extraction st ages\nthat can adaptively learn the features from the data in an end-\nto-end fashion. There are many CNN-based FER works (\nLevi\nand Hassner, 2015; Ding et al., 2017; Meng et al., 2017; Zeng\net al., 2018; Zhang et al., 2018; Li et al., 2019; Jiang et al., 2 020\n).\nFor FER under occlusion, Li et al. (2018a) proposed a CNN\nFrontiers in Neurorobotics | www.frontiersin.org 2 October 2021 | Volume 15 | Article 763100\nGao and Zhao Transformer for Facial Expression Recognition\nFIGURE 1 |Attention maps of several facial images with real ( A–D in top row) or synthesized ( E–H in bottom row) occlusions. Our proposed TFE is capable of\nperceiving the important facial regions for robust FER. A de ep red means high attention. Better viewed in color and zoom i n.\nwith attention mechanism (ACNN) to perceive facial expressio ns\nfrom unoccluded or partially occluded faces. ACNN crops facia l\npatches from the area of important facial features, e.g., mout h,\neyes, nose, and so on. The selected multiple facial patches are\nencoded as a weighed representation via a PG-Unit. The PG-\nUnit calculates the weight of each facial patch according to it s\nobstructed-ness via an attention net. Based on this work,\nWang\net al. (2020b) proposed to randomly crop relative large facial\npatches instead of small ﬁxed facial parts and reﬁne the attent ion\nweights by a region bias loss function and relation-attenti on\nmodule. Ding et al. (2020) proposed an occlusion-adaptive deep\nnetwork with a landmark-assisted attention branch network to\nperceive and drop the corrupted local features. Pan et al. (2019)\nintroduced to train two CNNs from non-occluded facial image s\nand occluded faces, respectively. Subsequently, they constr ain\nthe distribution of the encoded facial representations from two\nCNNs to be close via adversarial learning.\nOur proposed TFE diﬀers from previous CNN-based methods\nin two ways. One, TFE does not rely on facial landmarks for\nregional feature extraction. It is because the facial landm arks\nmay show considerable misalignments under severe occlusio ns.\nUnder this condition, the encoded facial parts are not part-\naligned or semantic meaningful. Two, TFE is a transformer-b ased\nand the self-attention mechanism in the transformer that ca n\nﬂexibly attend to a sequence of image patches to encode the\ncontextual cues. TFE consists of a region selection unit (RS- Unit)\nthat automatically perceives and selects the critical facia l regions\nfor robust FER. TFE is potentially to obtain higher FER accurac y\non both non-occluded and occluded faces. We will verify this i n\nsection 4.\n2.2. Vision Transformer\nTransformer models have largely facilitated research in ma chine\ntranslation and natural language processing (NLP) (\nWaswani\net al., 2017 ). Transformer models have become the outstanding\nstandard for NLP tasks. The main idea of the original transfo rmer\nis to calculate the self-attention by comparing a representat ion to\nall other representations in the input sequence. In detail, fe atures\nare ﬁrst encoded to obtain memory [including value ( V) and key\n(K)] and query ( Q) embedding by linear projections. The product\nof the query Q with keys K is used as the attention weights for\nvalue V. A position embedding is also exploited and added to\nthese representations to introduce the positional informatio n in\nsuch a non-convolutional paradigm. Transformers are especia lly\ngood at modeling long-range dependencies between elements\nof a sequence.\nInspired by the success of the transformer models, many\nrecent studies try to use transformers in computer vision\napplications (\nDosovitskiy et al., 2020; Li et al., 2021 ). Among\nthem, Dosovitskiy et al. (2020) applied a pure transformer\nencoder for image classiﬁcation. To obtain the input token\nrepresentations, they crop the input image into 16 × 16 small\npatches and linearly map the patches to the input dimension\nof the encoder. Since then, ViTs are gaining rapid interest\nin various computer vision tasks because they oﬀer a self-\nattention-based noval mechanism that can eﬀectively capture\nlong-range dependencies.\nTouvron et al. (2021) showed that\nViT models can achieve competitive accuracy on ImageNet\nwith stronger data augmentation and more regularization.\nSubsequently, transformer models are applied to other popular\ntasks such as object detection (\nCarion et al., 2020 ), segmentation\n(Jin et al., 2021 ), and video processing ( Girdhar et al., 2019; Fang\net al., 2020 ). In this study, we extend ViT to FER under occlusion\nand show its eﬀectiveness.\n3. METHOD\nFigure 2 illustrates the main idea of the proposed TFE. Given\nan input face image, TFE encodes its convolutional feature ma ps\nvia a commonly used backbone network such as ResNet-18 (\nHe\net al., 2016 ). Then, TFE encodes the robust facial expression\nrepresentation via the vision transformer and the proposed\nRS-Unit. During the training stage, the encoded convolutiona l\nfeature maps are decoded to reconstruct the unoccluded facial\nimage. Below, we present the details of each of them.\n3.1. Network Architecture\nFollowing ViT (\nDosovitskiy et al., 2020 ), we ﬁrst preprocess the\ninput image into a sequence of ﬂattened image patches. However ,\nthe conventional split approach merely cuts the images into\nFrontiers in Neurorobotics | www.frontiersin.org 3 October 2021 | Volume 15 | Article 763100\nGao and Zhao Transformer for Facial Expression Recognition\nFIGURE 2 |Main idea of the proposed Transformer Architecture for facial expression recognition (TFE). TFE perceives the informativ e facial expression representation\nvia the vision transformer and the proposed RS-Unit. In the right part, TFE uses an auxiliary decoder to reconstruct the unocc luded faces.\noverlapping or non-overlapping patches, which harms the local\nneighboring structures and shows substandard optimizabili ty\n(\nXiao et al., 2021 ). Inspired by Xiao et al. (2021) that exploits a few\nnumber of stacked 3 ×3 convolutions for image sequentialization,\nwe adopt the popular ResNet-based backbone ( He et al., 2016 ) to\nencode the input facial image I. A typical ResNet usually has four\nstages ( Li et al., 2021 ), and we use the output of the S-th stage as\nthe encoded feature maps X ∈ RH×W×C feature maps; thus, we\nget a total of N = H × W image tokens, each token Xi with a\nfeature dimension of C. As H equals W, here we use P = H = W\nfor brevity. In our proposed TFE, the image tokens have the\nspatial size 1 × 1, the input sequence is obtained by: (i) ﬂattening\nthe spatial dimensions of the feature map and (ii) projecting t he\nﬂattend tokens to the target transformer dimension.\nWe map the ﬂattend image token Xi into a latent D-\ndimensional feature space via a learnable fully connected neural\nlayer. With the sliced image token Xi ∈ RP2×D, 1 ≤ i ≤ N, a\ntrainable position embedding is plused to the token embedding s\nto retain positional information as follows:\nZ0 = [Xclass; X1E; X2E; XN E] + Epos, (1)\nZl′ = MSA(LN(Zl−1)) + Zl−1, l ∈ 1, 2, · · ·, L (2)\nZl = MLP(LN(Zl′)) + Z\n′\nl, l ∈ 1, 2, · · ·, L, (3)\nwhere N means the number of the image tokens, E is the\ntoken embedding projection, and Epos means the position\nembedding. L means the number of layers of the multi-head self-\nattention (MSA) and the multi-layer perceptron (MLP) blocks.\nThe transformer encoder includes alternating layers of mul ti-\nhead self-attention (MSA) and multilayer perceptron (MLP)\nblocks. We also add a layernorm (LN) layer before every block\nand residual connections after every block. Besides, the MLP\nconsists of two fully connected neural layers with a GELU\nnon-linearity. Xclass is a classiﬁcation token that consists of an\nembedding attached to the sequence of embedded patches. Afte r\nL transformer layers, a classiﬁcation head is attached to Z0\nL. We\nimplemented the classiﬁcation with a MLP that consists of one\nhidden layer at the training and testing phase.\n3.2. Vision Transformer With RS-Unit\nOne of the most important problems in FER under occlusion\nis to precisely perceive the discriminative facial regions tha t\nrepresent subtle facial deformations caused by facial express ions.\nTo this end, we proposed a RS-Unit to automatically select the\ncritical facial parts for robust FER under occlusions. Diﬀerent\nwith previous methods that use facial landmarks for facial reg ion\ndecomposition (\nLi et al., 2018a; Ding et al., 2020; Wang et al.,\n2020b), RS-Unit does not need auxiliary annotation and merely\nadopts the pre-computed multi-head attention information.\nSuppose the model consists of M self-attention heads and the\nhidden features, outputs of the last transformer layer are de noted\nas ZL = [Z0\nL, Z1\nL, Z2\nL, · · ·, ZN\nL ]. To better utilize the attention\nFrontiers in Neurorobotics | www.frontiersin.org 4 October 2021 | Volume 15 | Article 763100\nGao and Zhao Transformer for Facial Expression Recognition\nFIGURE 3 |Examples of the synthesized occluded images. The occluders are various in shape, color, and facial positions. (A) Anger, (B) neutral, (C) happy, and (D)\nsad.\ninformation, the input to the ﬁnal classiﬁcation layer is cha nged.\nIn detail, the raw attention weights are obtained via recursive\nmatrix multiplication in all the layers:\natotal =\nL∑\nl=0\nal. (4)\nAs atotal spots how information propagates from the preceding\ntransformer layer to the features in the later transformer l ayers,\natotal should be a promising choice to capture the important local\nfacial regions for FER ( He et al., 2021 ). Thus, we can choose the\npositions of the maximum values with regard to the M diﬀerent\nattention heads in atotal. We then choose the indexes of the\nmaximum values A1, A2, · · ·, AM w.r.t the M diﬀerent attention\nheads in atotal. These indexes are exploited as positions for RS-\nUnit to select the corresponding tokens in ZL. At last, we combine\nthe classiﬁcation token with the selected tokens along as th e\nﬁnal representation:\nZselect = Concat[Z0\nL, ZA1\nL , ZA2\nL , · · ·, ZAM\nL ]. (5)\nBy utilizing the entire input sequence with tokens tightly re lated\nto discriminative facial regions and combine the classiﬁca tion\ntoken as input to the classiﬁcation layer, our proposed TFE is\ncapable of utilizing the global facial information but also t he\nlocal facial regions that contain critical subtle facial def ormations\ninduced by facial expressions. Thus, our proposed TFE is\nexpected to perceive the discriminative facial regions for ro bust\nFER under occlusions.\n3.3. Image Reconstruction\nSince the facial expression is a subtle deformation of faces t hat\ncan be inferred from multiple facial regions, it is beneﬁcial to\nexplicitly infer the occluded facial parts from the unocclude d\nregions. In the image inpainting process, the model is tasked t o\nprecisely perceive the ﬁne-grained facial action units to inf er their\nco-occurrence (\nLi et al., 2018a ).\nInspired by this, we propose to reconstruct the facial image\nwith an auxiliary decoder. To this end, we synthesize the\noccluded face images by manually collecting abundant masks\nfor generating the occluders. We show some randomly selecte d\noccluded images in Figure 3. With the occluded faces Iocc\nand the corresponding original images Iori, we are capable of\nreconstructing the images as follows,\nLrec = ∥ Iori − Dec(Enc(Iocc))∥1, (6)\nwhere Enc means the convolutional feature extraction operation\nshown in Figure 2, Dec denotes the image decoding process.\n3.4. Overall Objective\nTransformer-based FER method is trained in an end-to-end\nfashion by minimizing the integration of the FER loss and the\nimage reconstruction loss in Equation (6). We integrate the two\ngoals and obtain the full objective function:\nLtotal = Lcls + λ Lrec, (7)\nwhere hyper-parameter λ controls the importance of the image\nreconstruction term.\n4. EXPERIMENT\n4.1. Implementation Details\nWe adopted ResNet-18 (\nHe et al., 2016 ) as the backbone network\nfor TFE due to its elegant structure and excellent performance in\nimage classiﬁcation. We used the output of the third stage as t he\nconvolutional feature maps: X ∈ R14×14×1024. Thus, the token\nsize is N = 14 × 14. We set L = 4, D = 768, and M = 12. We\ninitialized the backbone of TFE with the pre-trained model ba sed\non ImageNet dataset. We mixed all the facial expression datas ets\nwith their modiﬁcations with artiﬁcial facial occlusions wi th the\nratio of 1:1. TFE was optimized via a batch-based stochastic\ngradient descent manner. We actually set the batch size as 12 8\nand the base learning rate as 0.001. The weight decay was set a s\n0.0005 and the momentum was set as 0.9. The optimal setting for\nthe loss weight between the FER and image reconstruction term\nwas set as 1 : 1 by grid search.\n4.1.1. Datasets\nWe evaluated the methods on two facial expression datasets\n[RAF-DB (\nLi et al., 2017 ) and AﬀectNet ( Mollahosseini et al.,\n2017)]. We additionally evaluate our proposed TFE on FED-\nRO dataset ( Li et al., 2018a ). RAF-DB consists of about 30,000\nfacial images annotated with compound or basic expressions by\n40 trained human. We merely used the images with seven basic\nexpressions. We obtained totally 12,271 images for training data\nand 3,068 images for evaluation. AﬀectNet is currently the largest\ndataset with annotated facial expressions. AﬀectNet consists o f\napproximately 400,000 images manually annotated. We merely\nutilized the images with six basic and neutral expressions, W e\nobtained about 280,000 images for training and 3,500 images\nfor evaluation. FED-RO (\nLi et al., 2018a ) is a facial expression\nFrontiers in Neurorobotics | www.frontiersin.org 5 October 2021 | Volume 15 | Article 763100\nGao and Zhao Transformer for Facial Expression Recognition\nTABLE 1 |Test set accuracy on RAF-DB dataset.\nMethod Neutral Anger Disgust Fear Happy Sad Surprise ACC (Ov erall/Ave)\nAlexNet ( Li et al., 2017 ) 60.15 58.64 21.87 39.19 86.16 60.88 62.31 −/55.60\nVGG16 ( Li et al., 2017 ) 59.88 68.52 27.50 35.13 85.32 64.85 66.32 80.96/58.22\nDLP-CNN ( Li et al., 2017 ) 80.29 71.60 52.15 62.16 92.83 80.13 81.16 80.89/74.20\ngACNN ( Li et al., 2018a ) 84.30 78.42 53.11 55.39 93.17 82.88 86.27 85.07/76.22\nTAE (Li et al., 2020 ) 62.80 58.01 45.03 58.12 76.03 45.85 64.44 81.68/58.61\nTFE (Ours) 86.76 79.01 64.38 66.22 95.61 87.03 90.27 88.49/81 .33\nTABLE 2 |Validation set accuracy on AffectNet dataset.\nMethod Neutral Anger Disgust Fear Happy Sad Surprise ACC (Overall/ Ave)\nAlexNet ( Mollahosseini et al., 2017 )* − − − − − − − 47.00/47.00\nRAN-ResNet18 ( Wang et al., 2020b )* − − − − − − − 52.90/52.90\nVGG16 ( Simonyan and Zisserman, 2014 ) 89.61 53.42 20.61 32.03 90.03 35.01 37.22 51.13/51.13\nFAB-Net (Wiles et al., 2018 ) 38.64 30.62 48.42 32.14 82.25 35.61 51.42 45.59/45.59\nTAE (Li et al., 2020 ) 44.42 38.63 46.84 40.39 78.01 40.81 54.41 49.07/49.07\ngACNN ( Li et al., 2018a ) 73.42 66.18 32.59 46.22 93.81 55.82 43.43 58.78/58.78\nOADN ( Ding et al., 2020 ) − − − − − − − 61.90/61.90\nSCN ( Wang et al., 2020a ) − − − − − − − 60.23/60.23\nTFE (Ours) 76.03 68.09 46.83 47.03 94.12 57.32 53.90 63.33/63.33\nThe bold values denotes the best results.*Means the values are reported in the original papers.\ndatabase with real-world occlusions. Each face has real occ lusions\nin uncontrolled environment. There are totally 400 images i n\nFED-RO dataset annotated with seven expressions. We train the\nproposed TFE on the joint training data of AﬀectNet and RAF\ndataset, following the protocol suggested in\nLi et al. (2018a) .\nFollowing ( Li et al., 2018a ), we manually collected\napproximately 4 k images as masks for generating the occluders .\nThese occluders were discovered and saved from search engin e\nvia more than 50 keywords, such as hair, hat, book, beer, apple,\ncabinet, computer, orange, etc. The height H and width W of the\noccluders S satisfy H ∈ [96, 128] and W ∈ [96, 128]. Figure 3\nshows some occluded faces. It is evident that the artiﬁcial\noccluded facial images are diverse in occlusion patterns.\n4.1.2. Evaluation Metric\nWe report FER performance on both the occluded and non-\noccluded images of all the datasets. We used the overall and t he\noverall and average accuracy on seven facial expression cate gories\n(i.e., six prototypical plus neutral categories) as a performan ce\nmetric. Besides, we also report some confusion matrixes on RA F-\nDB dataset to show the discrepancies between the expressions.\n4.2. FER Experimental Results\nWe compare the proposed TFE with the state-of-the-art FER\nmethods, including DLP-CNN (\nLi et al., 2017 ), gACNN ( Li et al.,\n2018a), FAB-Net ( Wiles et al., 2018 ), TAE ( Li et al., 2020 ),\nOADN ( Ding et al., 2020 ), and SCN ( Wang et al., 2020a ). The\ncomparison results are shown in Tables 1–3.\nTable 1 shows the FER results of our method and previous\nstudies on RAF-DB dataset. Our TFE achieves 81.33% in\nTABLE 3 |Test set accuracy on FED-RO dataset.\nMethod ResNet18 RAN DLP-CNN gACNN OADN TFE\nACC (AVE) 64.25 67.98 60.31 66.50 68.11 70.60\nThe bold values denotes the best results.\nthe average accuracy on seven facial expression categories.\nCompared with DLP-CNN (\nLi et al., 2017 ), TFE obtains 7.13%\nimprovements in the average accuracy. Compared with the\nstrongest competing method in the same setting gACNN (\nLi\net al., 2018a ), TFE surpasses it by 5.61%. The beneﬁts of TFE over\nother methods can be explained in two-fold. First, TFE explici tly\nutilizes transformer layers in the network structure. The s elf-\nattention in the transformers has been shown to eﬀectively le arn\nlocal to global interactions and relations between distant f acial\nparts. Besides, the RS-Unit on top of the transformer layers in\nour proposed TFE helps perceive the critical facial regions. Thu s,\nTFE is capable of spotting the local subtle facial deformation s\ninduced by facial expressions. Second, TFE explicitly reconst ructs\nthe unoccluded facial images with an auxiliary decoder, whi ch\nfacilitates the backbone CNN in TFE to learn to infer the\noccluded facial parts via the important facial regions.\nTable 2 shows the comparisons of our TFE and other state-of-\nthe-art FER methods on AﬀectNet dataset. TFE achieves 63.33%\nin the average accuracy on seven facial expression categorie s.\nCompared with RAN-ResNet-18 (\nWang et al., 2020b ) that use\nmultiple crops of facial images as input and learns adaptive\nweights for each input image, TFE obtains 10.43% improvements\nFrontiers in Neurorobotics | www.frontiersin.org 6 October 2021 | Volume 15 | Article 763100\nGao and Zhao Transformer for Facial Expression Recognition\nTABLE 4 |Ablation study on RAF-DB dataset.\nMethod Neutral Anger Disgust Fear Happy Sad Surprise ACC (Ov erall/Ave)\nOriginal test set of RAF-DB dataset\nTFE ( w/o D, w/o T) 83.97 79.01 60.63 60.81 94.51 85.56 86.32 85.91 /79.69\nTFE ( w/ D, w/o T) 85.15 83.33 65.63 64.86 95.78 87.03 84.80 86.20/80.94\nTFE 86.76 79.01 64.38 66.22 95.61 87.03 90.27 88.64/81.33\nSynthesized occluded test set of RAF-DB dataset\nTFE ( w/o D, w/o T) 79.41 76.54 53.12 54.05 91.90 81.80 80.85 83.68/73.95\nTFE ( w/ D, w/o T) 81.47 75.93 55.62 59.46 93.42 84.73 80.55 84.00/75.88\nTFE 83.53 72.84 60.00 67.57 93.50 82.85 85.41 85.12/77.96\nThe bold values denotes the best results.\nFIGURE 4 |Confusion matrixes of TFE. (A) Denotes the confusion matrix for the original test set of RAF -DB. (B) Is the confusion matrix for the synthesized occluded\ntest set of RAF-DB. It is clear that TFE shows decreased perfor mance on most of the facial expression categories with the ma nually occluders in the facial images.\nin the average accuracy. Compared with the self-supervised\nmethods FAB-Net (\nWiles et al., 2018 ) and TAE ( Li et al., 2020 ),\nTFE shows its success in almost each facial expression catego ry.\nAmong the state-of-the-art FER methods, gACNN ( Li et al.,\n2018a) and OADN ( Ding et al., 2020 ) both exploit the 24 facial\nlandmarks for facial region decomposition and learn the path-\nspeciﬁc representation to better capture the local details of t he\ninput facial image. However, their FER performance still lags\nbehind our proposed TFE, as illustrated in Table 2. This is\nbecause the transformer layers in TFE naturally encode the pa tch-\nspeciﬁc face representation by tokenizing the input convoluti onal\nfeature maps. TFE does not rely on facial landmarks to extract\nthe local representations and avoids the negative inﬂuence\ninduced by the misalignments of the facial landmarks. We\nadditionally show the FER performance comparison on FED-RO\ndataset in Table 3. FED-RO dataset is the ﬁrst facial expression\ndataset with real occlusions. TFE achieves 70.60% in the ave rage\naccuracy and outperforms other compared methods with no\nexception. In summary, the experimental results in Tables 1–\n3 verify the superiority of the proposed TFE for robust facial\nexpression recognition.\n4.2.1. Ablation Study\nBoth the transformer layers and auxiliary decoder help TFE\nobtain improvements on FER. We performed a quantitative\nstudy of these two parts in order to better understand the bene ﬁts\nof TFE.\nWe show the FER performance of TFE without auxiliary\nimage reconstruction decoder and without the transformer l ayers\n(as well as RS-Unit) [TFE ( w/o D, w/o T )], and TFE with the\nauxiliary image reconstruction decoder but without transf ormer\nlayers and RS-Unit [TFE ( w/ D, w/o T )] in Table 4. It is clear\nthat TFE ( w/o D, w/o T ) shows decreased FER performance\non both the original and synthesized occluded face images.\nWith the auxiliary image reconstruction decoder, TFE ( w/ D,\nw/o T ) illustrates improved FER performance in many facial\nexpression categories. The comparisons between TFE ( w/o T, w/o\nD) and TFE ( w/ T, w/o D ) demonstrate the eﬀectiveness of the\nauxiliary image reconstruction decoder. With the transfor mer\nlayers and the auxiliary image decoder, TFE obtains the best FER\nperformance. As illustrated in Table 4, TFE shows its beneﬁts\nin Neutral, Fear, Surprise and obtains comparable accuracy in\nDisgust, Happy, Sad.\nFrontiers in Neurorobotics | www.frontiersin.org 7 October 2021 | Volume 15 | Article 763100\nGao and Zhao Transformer for Facial Expression Recognition\nFIGURE 5 |Attention maps of several facial images with occlusions. Fo r each input face image, the ﬁrst, second, and third column, re spectively, show the attention\nmap of TFE ( w/o D, w/o T), TFE ( w/ D, w/o T), and TFE. Our proposed TFE is capable of perceiving the impo rtant facial regions for robust FER. A deep red denotes\nlow attention. A deep red means high attention. Better viewe d in color and zoom in.\nWe additionally show the confusion matrixes of our\nproposed TFE on both the original and synthesized occluded\ntest set of RAF-DB dataset in Figure 4. It is clear that\nTFE shows degraded performance on most of the facial\nexpression categories when the facial images are occluded\nin Figure 4B. Besides, TFE shows the lowest FER accuracy\non Disgust category and highest accuracy on Happpy\ncategory. Easily confused expression categories are disgust\nand sad, fear and surprise, and fear and sad. Our above\nobservations are consistent with the conclusions in\nLi et al.\n(2018a).\nWe show the attention maps of the TFE and its variants\nin Figure 5. For each input face, the ﬁrst, second, and third\ncolumn, respectively, show the attention map of TFE ( w/o\nD, w/o T ), TFE ( w/ D, w/o T ), and our proposed TFE. It\nis evident that TFE is capable of shifting attention from the\noccluded facial patches to other unobstructed regions. As a\ncomparison, TFE ( w/o T, w/o D ) and TFE ( w/ D, w/o T )\nare not capable of precisely focusing on the important and\nunobstructed facial parts. Taking facial images labeled wit h\nHappy in the fourth row for example, TFE perceives the eyes\nand the corner or the mouth precisely, irrespective of the faci al\nocclusions. The visualization results show the beneﬁts of t he\nproposed RS-Unit and the auxiliary decoder for robust FER\nunder occlusions.\n5. CONCLUSIONS\nIn this study, we propose a transformer-based FER method\n(TFE) that is capable of adaptatively focusing on the most\nimportant and unoccluded facial regions. Considering that\nfacial expression is represented by several speciﬁc facial\nparts, we propose a RS-Unit to automatically perceive the\ncritical facial parts so as to explicitly perceive the important\nfacial regions for robust FER. To better perceive the ﬁne-\ngrained facial deformations and infer the co-occurrence of\ndiﬀerent facial action units, TFE consists of an auxiliary\ndecoder to reconstruct the facial image. Quantitative and\nqualitative experiments have veriﬁed the feasibility of our\nproposed TFE. TFE also outperforms other state-of-the-art\nFER approaches. Ablation and visualization analyses show\nTFE is capable of shifting attention from the occluded facial\nregions to other important ones. Currently, TFE exploits\nFrontiers in Neurorobotics | www.frontiersin.org 8 October 2021 | Volume 15 | Article 763100\nGao and Zhao Transformer for Facial Expression Recognition\nthe ﬁxed patch size as the input to the transformer layer\nwhile larger facial patch size might be a better choice for\nthe heavily occluded facial images. We will explore this\nin the future work. Besides, we will also explore how to\nreduce the computation overhead and make TFE suit for\nmobile deployment.\nDATA AVAILABILITY STATEMENT\nThe original contributions presented in the study are includ ed\nin the article/supplementary material, further inquiries ca n be\ndirected to the corresponding author/s.\nAUTHOR CONTRIBUTIONS\nJG and YZ cooperatively led the method design and experiment\nimplementation. JG wrote the sections of the manuscript. YZ\nprovided result review, theoretical guidance, and paper revis ion.\nBoth authors have read and approved the ﬁnal manuscript.\nFUNDING\nThis publication of this paper was supported by the Henan key R\n& D and promotion projects (Grant: 212102310551) and the Key\nScientiﬁc Research Project Plan of Henan Province colleges an d\nuniversities (19A520008, 20A413002).\nREFERENCES\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., a nd\nZagoruyko, S. (2020). “End-to-end object detection with tran sformers, ”\nin European Conference on Computer Vision (Glasgow), 213–229.\ndoi: 10.1007/978-3-030-58452-8_13\nCotter, S. F. (2010). “Sparse representation for accurate classiﬁ cation of\ncorrupted and occluded facial expressions, ” in 2010 IEEE International\nConference on Acoustics, Speech and Signal Processing (Dallas,TX), 838–841.\ndoi: 10.1109/ICASSP.2010.5494903\nDing, H., Zhou, P., and Chellappa, R. (2020). “Occlusion-adaptive d eep\nnetwork for robust facial expression recognition, ” in 2020 IEEE\nInternational Joint Conference on Biometrics (IJCB) (Houston, TX), 1–9.\ndoi: 10.1109/IJCB48548.2020.9304923\nDing, H., Zhou, S. K., and Chellappa, R. (2017). “Facenet2expnet: re gularizing\na deep face recognition net for expression recognition, ” in 2017 12th IEEE\nInternational Conference on Automatic Face & Gesture Recognition (FG 2017)\n(Washington, DC), 118–126. doi: 10.1109/FG.2017.23\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., et al. (2020). An Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. Available online at: https://arxiv.org/pdf/2010.11929v1.pdf\nFang, Y., Gao, S., Li, J., Luo, W., He, L., and Hu, B. (2020). Multi -level feature\nfusion based locality-constrained spatial transformer network for video crowd\ncounting. Neurocomputing 392, 98–107. doi: 10.1016/j.neucom.2020.01.087\nGirdhar, R., Carreira, J., Doersch, C., and Zisserman, A. (2019). “ Video\naction transformer network, ” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (Long Beach, CA, USA), 244–253.\ndoi: 10.1109/CVPR.2019.00033\nHe, J., Chen, J.-N., Liu, S., Kortylewski, A., Yang, C., Bai, Y., et al. (2021). Transfg:\nA Transformer Architecture for Fiﬁne-Grained Recognition . Available online at:\nhttps://arxiv.org/abs/2103.07976v1\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learn ing for image\nrecognition, ” in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (Las Vegas, NV), 770–778. doi: 10.1109/CVPR.2016.90\nJiang, X., Zong, Y., Zheng, W., Tang, C., Xia, W., Lu, C., et al. (2 020). “DFEW: a\nlarge-scale database for recognizing dynamic facial expressions in the wild, ” in\nProceedings of the 28th ACM International Conference on Multim edia (Seattle,\nWA), 2881–2889. doi: 10.1145/3394171.3413620\nJin, Y., Han, D., and Ko, H. (2021). TRSEG: transformer for\nsemantic segmentation. Pattern Recogn. Lett . 148, 29–35.\ndoi: 10.1016/j.patrec.2021.04.024\nKotsia, I., Buciu, I., and Pitas, I. (2008). An analysis of fac ial expression recognition\nunder partial facial image occlusion. Image Vis. Comput . 26, 1052–1067.\ndoi: 10.1016/j.imavis.2007.11.004\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). Imagen et classiﬁcation\nwith deep convolutional neural networks. Commun. ACM 60, 84–90.\ndoi: 10.1145/3065386\nLevi, G., and Hassner, T. (2015). “Emotion recognition in the w ild via\nconvolutional neural networks and mapped binary patterns, ” in Proceedings of\nthe 2015 ACM on International Conference on Multimodal Interact ion, 503–510.\ndoi: 10.1145/2818346.2830587\nLi, S., Deng, W., and Du, J. (2017). “Reliable crowdsourcing and deep locality\npreserving learning for expression recognition in the wild, ” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recogniti on (Honolulu,\nHI), 2852–2861. doi: 10.1109/CVPR.2017.277\nLi, Y., Sun, Y., Cui, Z., Shan, S., and Yang, J. (2021). Learnin g fair\nface representation with progressive cross transformer. arXiv preprint\narXiv:2108.04983.\nLi, Y., Zeng, J., and Shan, S. (2020). Learning representations f or facialactions\nfrom unlabeled videos. IEEE Trans. Pattern Anal. Mach. Intell. 99, 1–1.\ndoi: 10.1109/TPAMI.2020.3011063\nLi, Y., Zeng, J., Shan, S., and Chen, X. (2018a). Occlusion aware facial expression\nrecognition using cnn with attention mechanism. IEEE Trans. Image Process .\n28, 2439–2450. doi: 10.1109/TIP.2018.2886767\nLi, Y., Zeng, J., Shan, S., and Chen, X. (2018b). “Patch-gated CNN for\nocclusion aware facial expression recognition, ” in 2018 24th International\nConference on Pattern Recognition (ICPR) (Beijing), 2209–2214.\ndoi: 10.1109/ICPR.2018.8545853\nLi, Y., Zeng, J., Shan, S., and Chen, X. (2019). “Self-supervise d representation\nlearning from videos for facial action unit detection, ” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (Long Beach, CA), 10924–10933. doi: 10.1109/CVPR.2019.\n01118\nLiu, S.-S., Zhang, Y., Liu, K.-P., and Li, Y. (2013). “Facial expression recognition\nunder partial occlusion based on gabor multi-orientation features fusion and\nlocal gabor binary pattern histogram sequence, ” in 2013 Ninth International\nConference on Intelligent Information Hiding and Multimedia Signal Processing\n(Beijing), 218–222. doi: 10.1109/IIH-MSP.2013.63\nMeng, Z., Liu, P., Cai, J., Han, S., and Tong, Y. (2017). “Iden tity-aware\nconvolutional neural network for facial expression recognition, ” in 2017 12th\nIEEE International Conference on Automatic Face & Gesture Re cognition\n(Washington, DC), 558–565. doi: 10.1109/FG.2017.140\nMollahosseini, A., Hasani, B., and Mahoor, M. H. (2017). Aﬀectn et: A database\nfor facial expression, valence, and arousal computing in the wild. IEEE Trans.\nAﬀect. Comput. 10, 18–31. doi: 10.1109/TAFFC.2017.2740923\nNaseer, M., Ranasinghe, K., Khan, S., Hayat, M., Khan, F. S., and Yang, M. H.\n(2021). Intriguing Properties of Vision Transformers . Available online at: https://\narxiv.org/abs/2105.10497\nPan, B., Wang, S., and Xia, B. (2019). “Occluded facial expressio n\nrecognition enhanced through privileged information, ” in Proceedings of\nthe 27th ACM International Conference on Multimedia (Nice), 566–573.\ndoi: 10.1145/3343031.3351049\nRudovic, O., Pantic, M., and Patras, I. (2012). Coupled gaussia n processes for pose-\ninvariant facial expression recognition. IEEE Trans. Pattern Anal. Mach. Intell .\n35, 1357–1369. doi: 10.1109/TPAMI.2012.233\nSchroﬀ, F., Kalenichenko, D., and Philbin, J. (2015). “Facenet : a uniﬁed\nembedding for face recognition and clustering, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (Boston, MA), 815–\n823. doi: 10.1109/CVPR.2015.7298682\nSimonyan, K., and Zisserman, A. (2014). Very Deep Convolutional Networks for\nLarge-Scale Image Recognition. Available online at: https://export.arxiv.org/abs/\n1409.1556\nFrontiers in Neurorobotics | www.frontiersin.org 9 October 2021 | Volume 15 | Article 763100\nGao and Zhao Transformer for Facial Expression Recognition\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jé gou, H. (2021).\n“Training data-eﬃcient image transformers & distillation through a ttention, ” in\nProceedings of the 38th International Conference on Machine Le arning(Virtual\nEvent), 10347–10357.\nWang, K., Peng, X., Yang, J., Lu, S., and Qiao, Y. (2020a). “Suppre ssing\nuncertainties for large-scale facial expression recognition, ” i n Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognit ion, 6897–6906.\ndoi: 10.1109/CVPR42600.2020.00693\nWang, K., Peng, X., Yang, J., Meng, D., and Qiao, Y. (2020b). Re gion attention\nnetworks for pose and occlusion robust facial expression recogniti on.\nIEEE Trans. Image Process . 29, 4057–4069. doi: 10.1109/TIP.2019.29\n56143\nWaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., et al.\n(2017). “Attention is all you need, ” in NIPS.\nWiles, O., Koepke, A., and Zisserman, A. (2018). Self-supervised le arning of\na facial attribute embedding from video. arXiv preprint arXiv:1808.06882 .\ndoi: 10.1109/ICCVW.2019.00364\nXiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P., and Girshic k, R. (2021).\nEarly Convolutions Help Transformers See Better. Available online at: https://\narxiv.org/pdf/2106.14881.pdf\nZeng, J., Shan, S., and Chen, X. (2018). “Facial expression recog nition with\ninconsistently annotated datasets, ” in Proceedings of the European Conference\non Computer Vision (Munich), 222–237. doi: 10.1007/978-3-030-012\n61-8_14\nZhang, L., Tjondronegoro, D., and Chandran, V. (2014). Random g abor based\ntemplates for facial expression recognition in images with facial o cclusion.\nNeurocomputing 145, 451–464. doi: 10.1016/j.neucom.2014.05.008\nZhang, T., Zheng, W., Cui, Z., Zong, Y., and Li, Y. (2018). Spat ial-temporal\nrecurrent neural network for emotion recognition. IEEE Trans. Cybern . 49,\n839–847. doi: 10.1109/TCYB.2017.2788081\nConﬂict of Interest: The authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be c onstrued as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their aﬃliated organizat ions, or those of\nthe publisher, the editors and the reviewers. Any product that may b e evaluated in\nthis article, or claim that may be made by its manufacturer, is not gua ranteed or\nendorsed by the publisher.\nCopyright © 2021 Gao and Zhao. This is an open-access article dist ributed under the\nterms of the Creative Commons Attribution License (CC BY). T he use, distribution\nor reproduction in other forums is permitted, provided the or iginal author(s) and\nthe copyright owner(s) are credited and that the original pub lication in this journal\nis cited, in accordance with accepted academic practice. No use, distribution or\nreproduction is permitted which does not comply with these t erms.\nFrontiers in Neurorobotics | www.frontiersin.org 10 October 2021 | Volume 15 | Article 763100"
}