{
  "title": "SeqGPT: An Out-of-the-Box Large Language Model for Open Domain Sequence Understanding",
  "url": "https://openalex.org/W4393161145",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2142730550",
      "name": "Tianyu Yu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2106987312",
      "name": "Chengyue Jiang",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2134678956",
      "name": "Chao Lou",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2110550660",
      "name": "Shen Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2129051579",
      "name": "Xiaobin Wang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1973321923",
      "name": "Wei Liu",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2128399176",
      "name": "Jiong Cai",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2588464066",
      "name": "Yang-ning Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2100215807",
      "name": "Ying-hui Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2630071572",
      "name": "Kewei Tu",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2508172955",
      "name": "Hai-Tao Zheng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2132377640",
      "name": "Ningyu Zhang",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2944937469",
      "name": "Pengjun Xie",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2018623175",
      "name": "Yong Jiang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2110550660",
      "name": "Shen Huang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2129051579",
      "name": "Xiaobin Wang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2588464066",
      "name": "Yang-ning Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2100215807",
      "name": "Ying-hui Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2630071572",
      "name": "Kewei Tu",
      "affiliations": [
        "Tsinghua University",
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2944937469",
      "name": "Pengjun Xie",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2018623175",
      "name": "Yong Jiang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6852670792",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W6759193872",
    "https://openalex.org/W3194836374",
    "https://openalex.org/W4361193900",
    "https://openalex.org/W4380352309",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W4310628981",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W4319653585",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W4367000490",
    "https://openalex.org/W4321392589",
    "https://openalex.org/W4384807869",
    "https://openalex.org/W4308671340",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W4361019453",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W2914526845",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W4315588884",
    "https://openalex.org/W4283649915",
    "https://openalex.org/W4221166835",
    "https://openalex.org/W1598003989",
    "https://openalex.org/W4308244210",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3015253856",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4366733439",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4388084955",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W4382202688",
    "https://openalex.org/W4385849042",
    "https://openalex.org/W4378498632",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4385570724",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4375869235",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3168867926"
  ],
  "abstract": "Large language models (LLMs) have shown impressive abilities for open-domain NLP tasks. However, LLMs are sometimes too footloose for natural language understanding (NLU) tasks which always have restricted output and input format. Their performances on NLU tasks are highly related to prompts or demonstrations and are shown to be poor at performing several representative NLU tasks, such as event extraction and entity typing. To this end, we present SeqGPT, a bilingual (i.e., English and Chinese) open-source autoregressive model specially enhanced for open-domain natural language understanding. We express all NLU tasks with two atomic tasks, which define fixed instructions to restrict the input and output format but still ``open'' for arbitrarily varied label sets. The model is first instruction-tuned with extremely fine-grained labeled data synthesized by ChatGPT and then further fine-tuned by 233 different atomic tasks from 152 datasets across various domains. The experimental results show that SeqGPT has decent classification and extraction ability, and is capable of performing language understanding tasks on unseen domains. We also conduct empirical studies on the scaling of data and model size as well as on the transfer across tasks. Our models are accessible at https://github.com/Alibaba-NLP/SeqGPT.",
  "full_text": "SeqGPT: An Out-of-the-Box Large Language Model for\nOpen Domain Sequence Understanding\nTianyu Yu1*, Chengyue Jiang2*, Chao Lou2*, Shen Huang4*\nXiaobin Wang4, Wei Liu2, Jiong Cai2, Yangning Li1, Yinghui Li1, Kewei Tu2\nHai-Tao Zheng1, Ningyu Zhang3, Pengjun Xie4, Fei Huang4, Yong Jiang4†\n1Tsinghua University\n2ShanghaiTech University\n3 Zhejiang University\n4DAMO Academy, Alibaba Group\nyiranytianyu@gmail.com {jiangchy,louchao}@shanghaitech.edu.cn\n{pangda,xuanjie.wxb,yongjiang.jy}@alibaba-inc.com\nAbstract\nLarge language models (LLMs) have shown impressive\nabilities for open-domain NLP tasks. However, LLMs are\nsometimes too footloose for natural language understand-\ning (NLU) tasks which always have restricted output and in-\nput format. Their performances on NLU tasks are highly re-\nlated to prompts or demonstrations and are shown to be poor\nat performing several representative NLU tasks, such as event\nextraction and entity typing. To this end, we present SeqGPT,\na bilingual (i.e., English and Chinese) open-source autore-\ngressive model specially enhanced for open-domain natural\nlanguage understanding. We express all NLU tasks with two\natomic tasks, which define fixed instructions to restrict the\ninput and output format but still “open” for arbitrarily var-\nied label sets. The model is first instruction-tuned with ex-\ntremely fine-grained labeled data synthesized by ChatGPT\nand then further fine-tuned by 233 different atomic tasks from\n152 datasets across various domains. The experimental re-\nsults show that SeqGPT has decent classification and extrac-\ntion ability, and is capable of performing language under-\nstanding tasks on unseen domains. We also conduct empir-\nical studies on the scaling of data and model size as well\nas on the transfer across tasks. Our models are accessible at\nhttps://github.com/Alibaba-NLP/SeqGPT.\n1 Introduction\nRecent advancements in large language models (LLMs)\nhave demonstrated their impressive ability across various\nNLP tasks (Kaplan et al. 2020; Wei et al. 2022b; Chung\net al. 2022; Li et al. 2023c,d,b). Regarding natural lan-\nguage understanding (NLU) tasks, although the next-word-\nprediction approach utilized by language models implies lit-\ntle bias to the task-specific output structures, such as spans in\n* Equal first authorship.\n† Corresponding authors.\nThis work was conducted when Tianyu Yu, Chengyue Jiang, Chao\nLou, Wei Liu, Jiong Cai, Yangning Li and Yinghui Li were intern-\ning at Alibaba DAMO Academy.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nnamed entity recognition (NER) and triplets in relation ex-\ntraction (RE), numerous attempts (Qin et al. 2023; Wei et al.\n2023; Wadhwa, Amir, and Wallace 2023; Ashok and Lip-\nton 2023) have been made to apply LLMs to open-domain\nNLU tasks through the application of prompt engineering,\nmainly due to the LLMs’ exceptional ability of generaliza-\ntion and instruction-following (Figure 1). However, the di-\nrect application of LLMs comes with notable drawbacks.\nInstruction-following necessitates the use of a sufficiently\nlarge model (Kaplan et al. 2020; Wei et al. 2022b), for exam-\nple, GPT-3 (Brown et al. 2020) has 175B parameters, which\ncan lead to considerable inference costs and challenges in\ncustomization (Hu et al. 2022; Liu et al. 2022a,b). In addi-\ntion, prompt engineering is crucial to achieve promising per-\nformance and ensure adherence to output format standards.\nHowever, it is highly empirical and the models may not con-\nsistently abide by it (Chase 2022; Gravitas 2023).\nTo perform NLU tasks more effectively, some re-\nsearchers (Wang et al. 2022a, 2023a; Lu et al. 2023; Chen\net al. 2022; Zhang et al. 2023) have focused on contin-\nuing to train moderate-sized foundation models (approx-\nimately 10B parameters, e.g., BLOOM-7B1 (Scao et al.\n2023)), which not only improve computational friendliness\nbut also deliver competitive capabilities, in a manner of\nunifying various tasks. Data consumed in the training pro-\ncedure can be sourced from either an aggregation of ex-\nisting close-domain datasets (Wang et al. 2022a, 2023a)\nor open-domain but noisy datasets generated through ap-\nproaches such as weak supervision (Lu et al. 2023) and in-\nteraction with LLMs (Wang et al. 2023b). The extra train-\ning purportedly empowers moderate-sized models to sur-\npass their large-scale counterparts in zero-shot performance\nacross various NLU benchmarks. These tuned models can\nalso provide a stable standard output interface, making eval-\nuation and downstream application convenient.\nOur research is in the line of enhancing the NLU ability\nof LLMs via training but involves a broader range of NLU\ntasks and incorporates a greater diversity of open-domain\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19458\nFigure 1: An example of ChatGPT and SeqGPT performing\nthe CrossNER task in the zero-shot setting. ChatGPT mis-\nlabeled entities, while SeqGPT succeeded. Italic gray texts\nare the prompt template. SeqGPT uses a different prompt, as\nshown in Figure 2.\ndata than previous work. This is motivated by recent in-\nstruction tuning studies, which emphasize the advantages of\nenhancing task diversity rather than simply increasing data\nvolume (Wang et al. 2022b; Iyer et al. 2023). Specifically,\nwe collect and unify 152 datasets across 11 NLU tasks, en-\ncompassing not only commonly included information ex-\ntraction (IE) tasks like NER (Wang et al. 2022a, 2023a), but\nalso tasks overlooked in prior work, such as natural language\ninference (NLI) and extraction-based machine reading com-\nprehension (MRC). Moreover, to bridge the discrepancy be-\ntween practical scenarios and existing close-domain NLU\ndata, we generate a large-scale open-domain dataset from\nvarious sources. In contrast to earlier studies on automatic\nNLU data generation, which typically rely on a single do-\nmain source (e.g., Wikipedia) and assign labels based on\na predefined knowledge base (Lu et al. 2023), we instruct\nChatGPT to invent appropriate labels for each sample and\nidentify corresponding answers because ChatGPT is profi-\ncient at summarizing and producing annotations at a human\nlevel (Brown et al. 2020; Gilardi, Alizadeh, and Kubli 2023;\nZhu et al. 2023). The generated dataset contains more than\n800 thousand distinct reasonable labels, which is substan-\ntially richer than previous datasets but remains high quality\nupon our manual inspection.\nUsing the two datasets, we train Sequence under-\nstanding enhanced GPT, shortly SeqGPT, based on\nBLOOMZ (Muennighoff et al. 2023), a family of\ninstruction-tuned language models. Our training procedure\nconsists of two stages: initially, pre-training using the di-\nverse, albeit noisy, ChatGPT-generated data and subse-\nquently fine-tuning with the collection of real NLU datasets.\nThis strategy is driven by the intention to first enhance the\nability of generalization with diverse data and then refine the\nmodel to align with human preferences. Our experiments\nrevealed that SeqGPT consistently surpasses ChatGPT on\nzero-shot NLU benchmarks by a large margin. The key find-\nings derived from our study can be summarized as follows:\n• Scaling up the model size enhances performance.\n• Simply scaling up the data size without considering di-\nversity does not consistently yield better performance.\n• Increasing task diversity improves performance, al-\nthough this increase is logarithmic with respect to the\nnumber of tasks.\n• Larger models are capable of generalizing across lan-\nguages and tasks.\n2 Method\n2.1 Unified Approach\nIn order to solve a novel open-domain task, a language\nmodel expects a sequential input encoding both the sentence\nand necessary knowledge of the task and outputs answers ac-\ncordingly. To tackle different NLU tasks with a single model\nand a consistent input-output format, we consider a unified\napproach that translates them into two atomic tasks:\n• Extraction (EXT): This task identifies all relevant spans\nfor each query. A query can be a single word, a phrase\n(as in traditional extraction tasks), or a natural language\ndescription (as in machine reading comprehension and\ninstruction following).\n• Classification (CLS): This task aims to associate the en-\ntire input with a suitable subset of the given labels, which\npermits both multi-class and multi-label classification.\nFor each atomic task, we design a simple prompt tem-\nplate, which consists of (1) some control tokens indicating\ndifferent parts of inputs, (2) the specific text to be analyzed,\nand (3) a list of queries or labels of interest. Regarding the\noutput, the answers are formatted into fixed and easy-to-\nparse forms depending on the type of atomic tasks. Partic-\nularly, for the extraction task, the answer is listed line by\nline. Each line contains a user-typed query, followed by a\nlist of phrases as its corresponding answer. We do not re-\nquire the models to provide the positions from which these\nphrases are extracted, as transformer-based models are not\nproficient in token counting. For the classification task, the\nanswer is formatted as a single-line list containing answer\nlabels taken from the provided label set.\nTypically, most tasks only involve one atomic task. NLI\nand NER exemplify tasks that rely solely on classification\nand extraction, respectively. However, some tasks require\ndecomposition into multiple atomic tasks. For example, re-\nlation extraction (RE) is performed first to identify spans via\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19459\nExtraction\nInput: {text}\nExtract: {label_set}\nOutput:\nClassification\nInput: {text}\nClassify: {label_set}\nOutput:\nEvent Extraction\nRelation Extraction\nNamed Entity Recognition\nSlot Filling ...\nSeqGPT\nInput: I want my checking balance.\nClassify: CheckBalance, TransferMoney\nOutput: CheckBalance\nInput: what is the event type of commenced in The\ntwo-day game commenced on 19 August 1900?\nClassify: Extradition, Process_start\nOutput: Process_start\nInput: The two-day game commenced on 19\nAugust 1900?\nExtract: Process_start event\nOutput: Process_start event: commenced\nInput: MELBOURNE 1996-12-06\nExtract: LOC, PER, ORG, MISC\nOutput: LOC: MELBOURNE\nNLI\nSentiment Analysis\nIntent Detection\nEntity TypingMRC-MC\n...\nMRC -SE\nFigure 2: The overview of SeqGPT. Each NLU task is translated into atomic tasks with consistent input-output formats. Black-\n/blue/red/purple tokens are templates/inputs/query or label lists/outputs.\nextraction, followed by classification to discern the relation-\nships between each span pair. Besides, we only make neces-\nsary efforts of prompt designing to handle task-specific input\nformats. Taking NLI as an example, its input contains two\nsentences (i.e., premise and hypothesis), so we concatenate\nthem with a pre-defined separator. Figure 2 shows a brief\nillustration, and more details are presented in the appendix.\nContrary to previous studies on instruction tuning that re-\nquire significant effort to design task descriptions (Wang\net al. 2022b, 2023b,a), we inject task-specific information to\nour models via informative queries or labels. Therefore, the\nmodel can be generalized to new tasks and domains with-\nout human effort to craft new elaborate task descriptions.\nWhile this approach may potentially limit the performance\ndue to the inflexible prior knowledge injection at inference\ntime, our experiments show that, after continuous training\non massive NLU tasks, the model learns how to solve NLU\ntasks and how to generalize, eliminating the need for addi-\ntional information in the inference time, such that achieves\na balance between efficiency and effectiveness.\nAs prompts are pivotal to achieving high performance,\nwe examine various design possibilities, such as using\nlanguage-specific or language-agnostic templates. A thor-\nough discussion and experimental comparison will is dis-\nplayed in the appendix.\n2.2 Pre-training Data\nMotivated by recent evidence that scaling data diversity ben-\nefits models’ generalization ability on unseen data (Wang\net al. 2022b; Iyer et al. 2023), we construct a large-scale pre-\ntraining (PT) dataset with an extremely diverse label set and\nmultiple domains, including Wikipedia, news, and medicine.\nFor covering both atomic tasks, we consider three tasks:\nclassification, entity typing, and NER, whose annotations\nare created by prompting ChatGPT to invent appropriate la-\nbels for each sample and identify corresponding answers in\nan open-domain setting. Regarding the data quality, we sam-\npled 1% of the generated samples from each domain and\nLang. Task # inst. # token # label\nEn\nCLS 50,172 4,914,471 22,002\nET 212,734 21,594,057 84,461\nNER 60,094 9,803,353 117,300\nZh\nCLS 49,917 7,283,509 32,209\nET 576,839 170,318,622 143,935\nNER 196,515 46,210,373 417,168\nAll 1,146,271 260,124,385 817,075 1\nTable 1: Statistics of the pre-training data. # denotes the\nnumber of.inst. denotes instance.\nassure the average label accuracy for CLS and EXT tasks\nare higher than 80% and 75% respectively. Finally, the PT\ndataset encompasses 1,146,271 instances and 817,075 dis-\ntinct labels. Detailed statistics are shown in Table 1.\nNegative Label Generation The PT data generated by\nChatGPT cannot be used for training directly because of\nthe lack of negative labels. We adopt a simple strategy: aug-\nmenting samples in the PT data with random labels sampled\nfrom the set of all labels occurred in the corresponding PT\ntask (i.e., CLS, ET and NER). Due to the large amount of\nthe set (as shown in Table 1), these sampled labels are likely\nirrelevant to the input sentence, so it is safe to assume the\nabsence of a corresponding answer.\n2.3 Fine-tuning Data\nTo further calibrate models to perform NLU tasks and elim-\ninate effects caused by errors in the PT dataset, we collect\nmassive high-quality NLU datasets from different domains\nfor fine-tuning. As illustrated in Figure 3, our fine-tuning\n(FT) dataset consists of 110 NLU datasets across two lan-\n1Labels with the same literal value but from different tasks are\nconsidered as different labels.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19460\n7/19/23, 8:24 PM sunburst-simple (2).html\nﬁle:///Users/louchao/Downloads/sunburst-simple (2).html\nCLS\nIntentIntent\nDetectionDetection\nTextText\nClassificationClassification\nSentimentSentiment\nAnalysisAnalysis\nNLINLI\nMRC - MCMRC - MCEntity TypingEntity Typing\nEXT\nNERNER\nSlot FillingSlot Filling\nRERE\nMRC - SEMRC - SE\nEEEE\nFigure 3: Ratio of each task in the fine-tuning data.\nguages, English and Chinese, and ten tasks, including IE\ntasks, such as NER, RE, and EE and other tasks which can\nbe translated into the two atomic tasks, such as NLI and\nMRC. Besides a broad coverage of tasks, the data diver-\nsity is also guaranteed by their assorted source domains,\nincluding medicine, news, and dialogue with AI assistants,\nand different labels or queries with various granularity. Each\ntask is translated into a combination of atomic tasks, re-\nsulting in 139 classification atomic tasks and 94 extraction\natomic tasks. We manually select a small portion of the NLU\ndatasets as the held-out set for zero-shot evaluation. We re-\nfer readers to the supplementary for the complete list of the\nincluded datasets.\nBalancing data A large number of datasets are col-\nlected in our FT data to ensure diversity, but meanwhile,\nthis introduces data imbalance. Taking two classification\ndatasets as examples, IFLYTEK (Xu et al. 2020) and AG\nNews (Zhang, Zhao, and LeCun 2015) contains 124 and\n31,900 instances per label in average, respectively. In our\nimplementation, we combine collected and sample data uni-\nformly and randomly. The imbalance potentially causes un-\nderfitting tasks with abundant samples or oversampling on\nsmall datasets. Therefore, we set a quota for each dataset-\nlabel pair for balancing data. We use the whole set of in-\nstances without up-sampling for those dataset-label pair with\nfewer instances than the quota.\n2.4 Two-stage Training\nWe train SeqGPT based on BLOOMZ (Muennighoff et al.\n2023), an instruction-tuned variant of BLOOM (Scao et al.\n2023), with a two-stage training strategy, including pre-\ntraining and fine-tuning, as an allusion to the usage of differ-\nent training data. In our preliminary experiments, this strat-\negy outperforms the alternative: training with a simple mix-\ning of the PT and FT data. Specifically, we use padding to\nbuild batches and mask out supervision on the input tokens\nand train the model with cross-entropy loss. Most hyper-\nparameters, including optimization steps, learning rates, and\nbatch size, are consistent across all experiments. More train-\ning details including hyper-parameters are listed in the ap-\npendix to save space.\n3 Experiments\n3.1 Evaluation\nGiven the fact that LLMs sometimes generate reasonable\nbut not exactly matched answers, the traditional Micro-F1\nmetric is not smooth enough for evaluation. To mitigate\nthis and make the evaluation more minor-flaw-tolerant, we\npropose to combine Micro-F1 and a more smooth ROUGE\nscore as the overall metric. Specifically, we take the aver-\nage of ROUGE-1, ROUGE-2, and ROUGE-L (Lin 2004)\nas ROUGE score and take the average of Micro-F1 and\nROUGE score as the final score.\nTo thoroughly evaluate the generalization ability, we\nevaluate SeqGPT on 233 held-in datasets and 49 held-out\ndatasets. Specifically, the training split of held-in datasets\nis used during training, no sample from held-out datasets\nis seen during training, and all tasks involved in held-out\ndatasets are seen during training. For efficiency, we ran-\ndomly sample 48 records from each evaluation dataset’s\nvalid and test split. Besides, in terms of tasks translated to\nmultiple atomic tasks, we simplify the evaluation to report\nthe average scores over atomic tasks. Unless otherwise spec-\nified, all scores reported in this section are held-out perfor-\nmance for simplicity.\n3.2 Baselines\nWe compared SeqGPT with the well-known large chat lan-\nguage model ChatGPT (OpenAI 2022) and instruction fine-\ntuned model series BLOOMZ (Fan et al. 2022).\n3.3 Main Results\nWe compared the held-out performance of the SeqGPT fam-\nily and baselines in Table 2. Based on the results, we have\nthe following findings:\n(1) The smallest SeqGPT-560M surpasses the perfor-\nmance of ChatGPT by a large margin of 27.4, demonstrat-\ning the effectiveness of our framework and powerful natural\nlanguage understanding ability can be learned by a compact\nsmall model. On the other hand, the overall score of Chat-\nGPT might be hindered by the metric we adopted since the\noutput format generated by ChatGPT is not always aligned\nwith our evaluation data format. Besides, ChatGPT some-\ntimes can not comprehend prompts, resulting in irrelevant\nresponses. We refer readers to Section 3.7 for a more de-\ntailed analysis of comparing ChatGPT with SeqGPT.\n(2) The average score can be further improved to 65.5 by\nusing a larger 7B1 backbone. This improvement can be at-\ntributed to better complex reasoning ability and more diverse\nworld knowledge that comes with larger models.\n(3) The weakly supervised ultra-fine-grained pre-training\ndata are helpful, especially for smaller models like SeqGPT\n560M. Without using the pre-training data, the average per-\nformance of SeqGPT 560M drops from 57.2 to 53.9. Specif-\nically, the scores of entity typing and slot filling, which re-\nquire a diverse range of understanding of entities, drops sig-\nnificantly for SeqGPT of all sizes.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19461\nModel Size CLS EE ID MRC NER NLI RE SF SA ET ALL\nChatGPT - 58.0 34.8 62.3 19.9 11.1 33.5 31.4 30.6 65.6 27.9 38.1\nBLOOMZ\n560M 5.3 1.6 3.6 4.4 0.0 5.8 0.7 0.0 11.30 3.3 3.6\n1B7 5.6 2.4 0.9 3.8 0.0 10.1 4.3 0.0 16.0 3.5 3.7\n3B 6.8 3.9 1.8 4.4 0.0 4.4 3.3 0.0 12.5 3.6 4.7\n7B1 10.3 6.2 2.4 6.4 0.0 14.0 11.2 0.2 24.6 4.2 6.2\n560M 53.7 48.0 64.1 39.1 48.9 48.7 40.5 66.1 71.2 32.8 53.9\nSeqGPT 1B7 62.5 55.1 78.0 45.1 52.0 52.9 50.4 65.4 78.5 34.2 60.1\nw/o pre-training 3B 65.9 59.7 79.9 45.4 53.8 57.9 51.6 70.1 76.0 37.4 62.2\n7B1 72.7 63.4 83.3 49.2 55.5 60.4 57.4 71.7 73.5 43.1 65.4\nSeqGPT\n560M 57.3 56.8 72.9 38.8 50.9 51.4 43.9 70.0 71.7 38.8 57.2\n1B7 67.9 57.2 80.9 43.8 52.7 57.5 56.7 70.1 77.2 48.1 62.8\n3B 68.5 60.9 77.2 48.8 54.8 62.5 54.3 75.1 73.1 48.9 64.0\n7B1 70.9 63.1 80.9 51.0 56.1 58.9 56.0 72.1 74.3 54.1 65.5\nTable 2: Performance on held-out evaluation datasets. CLS: text classification. EE: event extraction. ID: intent detection; MRC:\nmachine reading comprehension. NER: named-entity recognition. NLI: natural language inference. RE: relation extraction. SF:\nslot filling. SA: sentiment analysis. ET: entity typing. ALL: average performance on all tasks.\n(4) Though effective, the performance gains achieved by\nutilizing pre-training data shrinks with larger models. We\nargue that this is because the ultra-fine-grained knowledge\nin our pre-training data can also be learned directly during\nthe pre-training stage of LLMs, and such knowledge is bet-\nter learned with increasing model size of pre-trained LLMs.\nOn the other hand, the naive BLOOMZ 7B1 lags far behind\neven the smallest SeqGPT 560M. We find the output gen-\nerated by BLOOMZ 7B1 can hardly be consistent with the\ninstruction, indicating complex prompt engineering or few-\nshot examples might be required to leverage such general in-\nstruction following model to solve open-domain NLU tasks.\n3.4 Scaling Analysis\nWe extensively study the performance of models with re-\nspect to the scaling of model sizes, number of samples per\ntask, and number of distinct tasks and discover all these fac-\ntors are crucial for building an open-domain sequence un-\nderstanding model.\nModel Size We trained a series of models in different sizes\nbased on the BLOOMZ family (Fan et al. 2022) from 560M\nto 7B1 to explore the scaling effect of model sizes. Results\nin Figure 4 show both the held-in and the held-out perfor-\nmance increase with a larger backbone that complies with\nthe results found in Chowdhery et al. (2022). Furthermore,\nthe large gap between the held-in and held-out performance\nreveals the difficulty of open-domain NLU, indicating that\nthere is still great space for SeqGPT to improve the gener-\nalization ability. We find the improvement in held-in eval-\nuation is fewer compared with the held-out evaluation. We\nbelieve the held-out score can better reflect the performance\nin real applications.\nNumber of Training Datasets Besides the model size, the\nnumber of training datasets is also the major factor to im-\npact the resulting performance, so we also conduct experi-\nments to explore this effect. Results in Figure 5 indicate that\nx\n80\n85\n90\nModel Size Scaling (Held-in and Held-out)\nHeld-in\n5.6e8 1.7e9 3.0e9 7.1e9\nModel Size\n55\n60\n65\nPerformance\nHeld-out\nFigure 4: Held-in and held-out evaluation results of SeqGPT\nin different sizes.\nthe performance of our SeqGPT models increases in a loga-\nrithmic manner with more datasets used for training. Based\non such observation, we believe that adding more training\ndatasets is an efficient and straightforward approach to im-\nprove the performance further since our held-in corpora are\nstill small compared to opulent real application scenarios.\n3.5 Cross-language Generalization\nWe use a great amount of training data from both English\nand Chinese. To explore the effect of data from each lan-\nguage and the cross-language generalization ability of Se-\nqGPT, we conduct extensive experiments, and the main re-\nsults are shown in Table 3. We find that the models trained\nwith a single language (English/Chinese) can generalize to\ntasks in the other language (Chinese/English) and achieve\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19462\n2 4 8 16 32 64 128 233\nNumber of Held-in Dataset\n40\n45\n50\n55\n60\n65Performance\nHeld-in Dataset Scaling\nModel\n560M\n1B7\n3B\n7B1\nFigure 5: Held-out performance of SeqGPT in different sizes\nscaling with respect to the number of training datasets in the\nheld-in set.\nTraining Languages EN Score ZH Score\nEnglish 57.59 51.98\nChinese 52.66 64.57\nChinese + English 58.83 65.23\nTable 3: Performance of SeqGPT-1B7 trained with different\nsettings of training languages.\nreasonable performance. Comparing the model trained with\ndata in English and in both languages, we find the scores\non both English tasks and Chinese tasks can be improved,\nshowing there are skills shared between languages that can\nbe learned through a multilingual training stage.\n3.6 Cross-task Generalization\nThough sharing mostly the same prompts in our framework,\nthe skills needed to solve different tasks is diverse. To an-\nalyze how SeqGPT works on tasks not seen during train-\ning and how the training task affects the performance of\ndifferent test tasks, we train a series of models with only\none task, and results are shown in Figure 7. Based on the\nresults we find models achieve the best evaluation perfor-\nmance when the evaluation task is the same as the train-\ning task except for the NLI task. For NLI performance, we\nfind the model trained on the NLI task even achieves the\nworst performance. We argue this is because the way to clas-\nsify sentence pairs differs across NLI datasets. As a result,\nmodels trained on only NLI datasets can hardly transfer the\nclassification boundaries learned from the held-in datasets\nto held-out datasets. Models trained on EE, MRC, and RE\ncan generalize well to all test tasks, demonstrating the di-\nverse knowledge required to solve these tasks are also cru-\ncial for other tasks and can serve as a great training resource\nfor models targeting general domain NLU.\n102 103 104\nSample per Dataset\n54\n56\n58\n60\n62\n64\n66Performance\nHeld-in Data Scaling\nModel\n560M\n1B7\n3B\n7B1\nFigure 6: Held-out performance of SeqGPT scaling with re-\nspect to the number of samples per dataset.\n3.7 Human Evaluation\nFor a more comprehensive analysis, we perform a human\nevaluation on the held-out datasets. The evaluation recruits\nten well-educated annotators and presents them with an-\nswers generated by ChatGPT and SeqGPT-7B1. Annotators\nare required to decide which model gives the better answer\nor two models are tied with each other. Results are shown\nin Figure 8. From the results, we can find that SeqGPT-7B1\nachieves higher performance on seven out of ten NLU tasks,\ndemonstrating the effectiveness of training the model with\na wide range of NLU tasks incorporating a great diversity\nof open-domain data. Also, we found the output of SeqGPT-\n7B1 is much more concise than the output of ChatGPT, mak-\ning the interpretation easier and consequently reducing the\nengineering complexity to use the model. However, the re-\nsults also indicate that medium-size models like SeqGPT-\n7B1 still lack the complex reasoning abilities to solve com-\nplicated tasks such as NER and SF.\n4 Related Work\n4.1 Large Language Models\nAutoregressive language models have rapidly scaled up,\nreaching billions of parameters and trillions of train-\ning tokens. This has resulted in many emergent abilities\nsuch as few-shot learning, in-context learning, and reason-\ning (Bubeck et al. 2023; Wei et al. 2022b). Examples include\nGPT-3 (Brown et al. 2020), Chinchilla (Hoffmann et al.\n2022), Llama (Touvron et al. 2023a,b) and BLOOM (Scao\net al. 2023). LLMs can be prompted to perform downstream\ntasks without training, such as ChatIE for IE tasks (Wei et al.\n2023), PromptNER for NER tasks (Ashok and Lipton 2023)\nand Liu et al. (2023) for text-to-SQL tasks. We refer the\nreaders to (Zheng et al. 2023; Li et al. 2023a) and references\ntherein for more details.\nIn this study, we adopt BLOOMZ (Muennighoff et al.\n2023), a BLOOM-based instruction-tuned model, as the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19463\nCLS EE ID MRC NER NLI RE SF\nT est T ask\nCLS EE ID MRC NER NLI RE SF\nTraining T ask\n0.92 0.71 0.78 0.71 0.64 1.00 0.70 0.82\n0.96 1.00 0.77 0.85 0.92 0.96 0.86 0.93\n0.91 0.51 1.00 0.54 0.16 0.65 0.61 0.26\n0.86 0.88 0.82 1.00 0.90 0.90 0.81 0.96\n0.52 0.72 0.63 0.86 1.00 0.97 0.68 0.91\n0.73 0.50 0.70 0.57 0.20 0.57 0.47 0.49\n1.00 0.88 0.77 0.87 0.94 0.89 1.00 0.96\n0.50 0.68 0.64 0.82 0.92 0.97 0.69 1.00\nCross T ask Performance (Normalized)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFigure 7: Cross task generalization experiment results.\nScores are normalized column-wise based on the max score\nof each column.\nbackbone due to its exceptional multilingual performance\namong publicly available models and superior generaliza-\ntion capabilities compared to BLOOM.\n4.2 Instruction Tuning\nInstruction tuning (Wei et al. 2022a; Wang et al. 2022b; Sanh\net al. 2022; Li et al. 2023e) is a novel finetuning paradigm\nthat trains language models on numbers of tasks described\nusing natural language instructions. It has shown potential\nbenefits in aligning better with human preferences, yield-\ning more truthful, useful, and less harmful output (Ouyang\net al. 2022; Lou, Zhang, and Yin 2023). Furthermore, it has\ndemonstrated enhanced task-specific performance (Longpre\net al. 2023; Jang et al. 2023; Ivison et al. 2023) even tun-\ning only on a single task (Lee et al. 2023; Gupta et al. 2023;\nChen et al. 2023), as well as generalization capabilities for\nunseen tasks (Wang et al. 2022b, 2023b). Most instruction-\ntuning methods leverage datasets covering some NLU tasks\nbut with poor coverage of tasks and domains. For a special-\nized model, Wang et al. (2023a) train InstructUIE on wide IE\ntasks with various instructions and Parmar et al. (2022) build\na biomedical LLM with a collection of biomedical datasets\nacross multiple tasks with human-crafted instructions.\n4.3 Unified Models for NLU\nDiverse NLU tasks emphasize different aspects of lan-\nguages. Multitask learning has emerged as a prevalent topic,\ntaking advantage of jointly modeling selected subsets of\nNLU tasks, such as enabling the use of more training data\nor modeling similarities between tasks (Thrun 1995; Caru-\nana 1997; Miller et al. 2000; Sutton, McCallum, and Rohan-\nimanesh 2007; Liu, Qiu, and Huang 2016; Liu et al. 2019;\nLu et al. 2022a, among others). When incorporating more\ntasks, sequence generation models become compelling op-\ntions because free texts may be the most straightforward\nSA\nID\nRE\nCLS\nSF\nNER\nNLI\nEE\nMRC\nET\nT asks\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Win Ratio\nSeqGPT-7B1 Tie ChatGPT\nFigure 8: Human evaluation on held-out datasets.\nway to encode all outputs of various NLU tasks. UIE (Lu\net al. 2022b) unify the inputs of IE tasks through a schema-\nbased prompt mechanism and the outputs through the novel\nstructural extraction language. Consequently, given suitable\nprompts, it can perform novel NLU tasks using the com-\nmon semantic understanding ability learned. Subsequently,\nInstructUIE (Wang et al. 2023a) extends UIE by instruc-\ntion tuning a stronger backbone model (e.g., Flan-T5 11B),\nshowing strong zero-shot performance. USM (Lou et al.\n2023) is another unified IE model based on a link predic-\ntion mechanism named semantic matching.\n5 Conclusions\nIn this study, we introduce SeqGPT, a unified model de-\nvised to handle various NLU tasks by translating different\nNLU tasks into two common atomic tasks. In this way, Se-\nqGPT offers a consistent input-output format, enabling it\nto solve unseen tasks by prompting arbitrarily varied label\nsets without tedious prompt engineering. To achieve strong\ngeneralization ability, we train the model using novel ul-\ntra fine-grained synthetic data and a massive collection of\nNLU datasets on various domains. The training is further\nenhanced with effective data balance and randomly sam-\npled negative labels. Both automatic benchmarks and human\nevaluation on unseen tasks show that SeqGPT achieves con-\nsistent improvements over ChatGPT. In addition, we con-\nduct comprehensive experiments to investigate behaviors\nof scaling, revealing a logarithmic correlation between the\nquantity of training tasks and model performance. We have\nalso evaluated SeqGPT’s ability to generalize across various\ntasks and languages. Nevertheless, our findings raise new\nquestions. Why does the PT data fail to enhance SeqGPT-\n7B1, while an increase in FT data does? How to generate\nmore high-quality NLU data to fill the data hunger of Se-\nqGPT? We hope future research on these questions to further\nimprove open-domain NLU models.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19464\nAcknowledgments\nThis research is supported by National Natural Science\nFoundation of China (Grant No.62276154), Research Center\nfor Computer Network (Shenzhen) Ministry of Education,\nthe Natural Science Foundation of Guangdong Province\n(Grant No. 2023A1515012914), Basic Research Fund of\nShenzhen City (Grant No. JCYJ20210324120012033 and\nJSGG20210802154402007), the Major Key Project of PCL\nfor Experiments and Applications (PCL2021A06), and\nOverseas Cooperation Research Fund of Tsinghua Shenzhen\nInternational Graduate School (HW2021008). This work is\npartially supported by the Shenzhen Science and Technol-\nogy Program (WDZC20231128091437002).\nReferences\nAshok, D.; and Lipton, Z. C. 2023. PromptNER: Prompting\nFor Named Entity Recognition.\nBrown, T. B.; Mann, B.; Ryder, N.; et al. 2020. Language\nModels are Few-Shot Learners.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early experiments\nwith GPT-4.\nCaruana, R. 1997. Multitask Learning. Machine Learning.\nChase, H. 2022. LangChain. Original-date: 2022-10-\n17T02:58:36Z.\nChen, H.; Zhang, Y .; Zhang, Q.; Yang, H.; Hu, X.; Ma, X.;\nYanggong, Y .; and Zhao, J. 2023. Maybe Only 0.5\nChen, X.; Zhang, N.; Xie, X.; Deng, S.; Yao, Y .; Tan,\nC.; Huang, F.; Si, L.; and Chen, H. 2022. KnowPrompt:\nKnowledge-Aware Prompt-Tuning with Synergistic Opti-\nmization for Relation Extraction. InProceedings of the ACM\nWeb Conference 2022, WWW ’22.\nChowdhery, A.; Narang, S.; Devlin, J.; et al. 2022. PaLM:\nScaling Language Modeling with Pathways.\nChung, H. W.; Hou, L.; Longpre, S.; et al. 2022. Scaling\nInstruction-Finetuned Language Models.\nFan, A.; Ilic, S.; Wolf, T.; and Gall ´e, M., eds. 2022. Pro-\nceedings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language Models.\nvirtual+Dublin: Association for Computational Linguistics.\nGilardi, F.; Alizadeh, M.; and Kubli, M. 2023. ChatGPT\nOutperforms Crowd-Workers for Text-Annotation Tasks.\nGravitas, S. 2023. AutoGPT.\nGupta, H.; Sawant, S. A.; Mishra, S.; Nakamura, M.; Mi-\ntra, A.; Mashetty, S.; and Baral, C. 2023. Instruction Tuned\nModels are Quick Learners.\nHoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.;\nCai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.;\nWelbl, J.; Clark, A.; Hennigan, T.; Noland, E.; Millican, K.;\nvan den Driessche, G.; Damoc, B.; Guy, A.; Osindero, S.; Si-\nmonyan, K.; Elsen, E.; Rae, J. W.; Vinyals, O.; and Sifre, L.\n2022. Training Compute-Optimal Large Language Models.\nHu, E. J.; yelong shen; Wallis, P.; Allen-Zhu, Z.; Li, Y .;\nWang, S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank\nAdaptation of Large Language Models. In International\nConference on Learning Representations.\nIvison, H.; Smith, N. A.; Hajishirzi, H.; and Dasigi, P. 2023.\nData-Efficient Finetuning Using Cross-Task Nearest Neigh-\nbors. In Findings of ACL.\nIyer, S.; Lin, X. V .; Pasunuru, R.; Mihaylov, T.; Simig, D.;\nYu, P.; Shuster, K.; Wang, T.; Liu, Q.; Koura, P. S.; Li, X.;\nO’Horo, B.; Pereyra, G.; Wang, J.; Dewan, C.; Celikyilmaz,\nA.; Zettlemoyer, L.; and Stoyanov, V . 2023. OPT-IML: Scal-\ning Language Model Instruction Meta Learning through the\nLens of Generalization.\nJang, J.; Kim, S.; Ye, S.; Kim, D.; Logeswaran, L.; Lee, M.;\nLee, K.; and Seo, M. 2023. Exploring the Benefits of Train-\ning Expert Language Models over Instruction Tuning.\nKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.;\nChess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and\nAmodei, D. 2020. Scaling Laws for Neural Language Mod-\nels.\nLee, Y .-S.; Astudillo, R. F.; Florian, R.; Naseem, T.; and\nRoukos, S. 2023. AMR Parsing with Instruction Fine-tuned\nPre-trained Language Models.\nLi, X.; Zhang, T.; Dubois, Y .; Taori, R.; Gulrajani, I.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023a. Al-\npacaEval: An Automatic Evaluator of Instruction-following\nModels.\nLi, Y .; Chen, J.; Li, Y .; Xiang, Y .; Chen, X.; and Zheng,\nH.-T. 2023b. Vision, Deduction and Alignment: An Empir-\nical Study on Multi-Modal Knowledge Graph Alignment.\nIn ICASSP 2023-2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 1–5.\nIEEE.\nLi, Y .; Huang, H.; Ma, S.; Jiang, Y .; Li, Y .; Zhou, F.; Zheng,\nH.; and Zhou, Q. 2023c. On the (In)Effectiveness of Large\nLanguage Models for Chinese Text Correction. CoRR.\nLi, Y .; Li, Y .; Chen, X.; Zheng, H.-T.; and Shen, Y . 2023d.\nActive relation discovery: Towards general and label-aware\nopen relation extraction. Knowledge-Based Systems, 282:\n111094.\nLi, Y .; Ma, S.; Wang, X.; Huang, S.; Jiang, C.;\nZheng, H.-T.; Xie, P.; Huang, F.; and Jiang, Y . 2023e.\nEcomGPT: Instruction-tuning Large Language Model with\nChain-of-Task Tasks for E-commerce. arXiv preprint\narXiv:2308.06966.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of Summaries. In Text Summarization Branches Out,\n74–81. Barcelona, Spain: Association for Computational\nLinguistics.\nLiu, A.; Hu, X.; Wen, L.; and Yu, P. S. 2023. A comprehen-\nsive evaluation of ChatGPT’s zero-shot Text-to-SQL capa-\nbility.\nLiu, H.; Tam, D.; Mohammed, M.; Mohta, J.; Huang, T.;\nBansal, M.; and Raffel, C. 2022a. Few-Shot Parameter-\nEfficient Fine-Tuning is Better and Cheaper than In-Context\nLearning. In NeurIPS.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19465\nLiu, P.; Qiu, X.; and Huang, X. 2016. Recurrent Neural\nNetwork for Text Classification with Multi-Task Learning.\nArXiv.\nLiu, X.; He, P.; Chen, W.; and Gao, J. 2019. Multi-Task Deep\nNeural Networks for Natural Language Understanding. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 4487–4496. Florence, Italy:\nAssociation for Computational Linguistics.\nLiu, X.; Ji, K.; Fu, Y .; Tam, W.; Du, Z.; Yang, Z.; and Tang,\nJ. 2022b. P-Tuning: Prompt Tuning Can Be Comparable\nto Fine-tuning Across Scales and Tasks. In Proceedings of\nthe 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 2: Short Papers), 61–68. Dublin,\nIreland: Association for Computational Linguistics.\nLongpre, S.; Hou, L.; Vu, T.; Webson, A.; Chung, H. W.;\nTay, Y .; Zhou, D.; Le, Q. V .; Zoph, B.; Wei, J.; and Roberts,\nA. 2023. The Flan Collection: Designing Data and Methods\nfor Effective Instruction Tuning.\nLou, J.; Lu, Y .; Dai, D.; Jia, W.; Lin, H.; Han, X.; Sun, L.;\nand Wu, H. 2023. Universal Information Extraction as Uni-\nfied Semantic Matching. AAAI.\nLou, R.; Zhang, K.; and Yin, W. 2023. Is Prompt All You\nNeed? No. A Comprehensive and Broader View of Instruc-\ntion Learning.\nLu, J.; Yang, P.; Gan, R.; Yang, J.; and Zhang, J. 2022a. Uni-\nfied BERT for Few-shot Natural Language Understanding.\nLu, K.; Pan, X.; Song, K.; Zhang, H.; Yu, D.; and Chen, J.\n2023. PIVOINE: Instruction Tuning for Open-world Infor-\nmation Extraction.\nLu, Y .; Liu, Q.; Dai, D.; Xiao, X.; Lin, H.; Han, X.; Sun, L.;\nand Wu, H. 2022b. Unified Structure Generation for Uni-\nversal Information Extraction. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), 5755–5772. Dublin, Ire-\nland: Association for Computational Linguistics.\nMiller, S.; Fox, H.; Ramshaw, L.; and Weischedel, R. 2000.\nA Novel Use of Statistical Parsing to Extract Information\nfrom Text. In 1st Meeting of the North American Chapter of\nthe Association for Computational Linguistics.\nMuennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Bi-\nderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Z.-X.;\nSchoelkopf, H.; Tang, X.; Radev, D.; Aji, A. F.; Almubarak,\nK.; Albanie, S.; Alyafeai, Z.; Webson, A.; Raff, E.; and Raf-\nfel, C. 2023. Crosslingual Generalization through Multitask\nFinetuning.\nOpenAI, T. 2022. Chatgpt: Optimizing language models for\ndialogue. OpenAI.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and\nLowe, R. 2022. Training language models to follow instruc-\ntions with human feedback.\nParmar, M.; Mishra, S.; Purohit, M.; Luo, M.; Mohammad,\nM.; and Baral, C. 2022. In-BoXBART: Get Instructions into\nBiomedical Multi-Task Learning. In Findings of the Asso-\nciation for Computational Linguistics: NAACL 2022, 112–\n128. Seattle, United States: Association for Computational\nLinguistics.\nQin, C.; Zhang, A.; Zhang, Z.; Chen, J.; Yasunaga, M.; and\nYang, D. 2023. Is ChatGPT a General-Purpose Natural Lan-\nguage Processing Task Solver?\nSanh, V .; Webson, A.; Raffel, C.; et al. 2022. Multitask\nPrompted Training Enables Zero-Shot Task Generalization.\nIn International Conference on Learning Representations.\nScao, T. L.; Fan, A.; Akiki, C.; et al. 2023. BLOOM:\nA 176B-Parameter Open-Access Multilingual Language\nModel.\nSutton, C.; McCallum, A.; and Rohanimanesh, K. 2007. Dy-\nnamic Conditional Random Fields: Factorized Probabilistic\nModels for Labeling and Segmenting Sequence Data. Jour-\nnal of Machine Learning Research.\nThrun, S. 1995. Is Learning The n-th Thing Any Easier Than\nLearning The First? In Touretzky, D.; Mozer, M.; and Has-\nselmo, M., eds., NeurIPS.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lam-\nple, G. 2023a. LLaMA: Open and Efficient Foundation Lan-\nguage Models.\nTouvron, H.; Martin, L.; Stone, K.; et al. 2023b. Llama 2:\nOpen Foundation and Fine-Tuned Chat Models.\nWadhwa, S.; Amir, S.; and Wallace, B. 2023. Revisiting\nRelation Extraction in the era of Large Language Models.\nIn ACL.\nWang, C.; Liu, X.; Chen, Z.; Hong, H.; Tang, J.; and Song,\nD. 2022a. DeepStruct: Pretraining of Language Models\nfor Structure Prediction. In Findings of the Association\nfor Computational Linguistics: ACL 2022, 803–823. Dublin,\nIreland: Association for Computational Linguistics.\nWang, X.; Zhou, W.; Zu, C.; Xia, H.; Chen, T.; Zhang, Y .;\nZheng, R.; Ye, J.; Zhang, Q.; Gui, T.; Kang, J.; Yang, J.; Li,\nS.; and Du, C. 2023a. InstructUIE: Multi-task Instruction\nTuning for Unified Information Extraction.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2023b. Self-Instruct: Align-\ning Language Models with Self-Generated Instructions. In\nACL.\nWang, Y .; Mishra, S.; Alipoormolabashi, P.; et al. 2022b.\nSuper-NaturalInstructions: Generalization via Declarative\nInstructions on 1600+ NLP Tasks. In EMNLP.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2022a. Finetuned\nLanguage Models Are Zero-Shot Learners.\nWei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler,\nD.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.; Dean,\nJ.; and Fedus, W. 2022b. Emergent Abilities of Large Lan-\nguage Models.\nWei, X.; Cui, X.; Cheng, N.; Wang, X.; Zhang, X.; Huang,\nS.; Xie, P.; Xu, J.; Chen, Y .; Zhang, M.; Jiang, Y .; and Han,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19466\nW. 2023. Zero-Shot Information Extraction via Chatting\nwith ChatGPT.\nXu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y .; Xu, Y .;\nSun, K.; Yu, D.; Yu, C.; Tian, Y .; Dong, Q.; Liu, W.; Shi,\nB.; Cui, Y .; Li, J.; Zeng, J.; Wang, R.; Xie, W.; Li, Y .; Pat-\nterson, Y .; Tian, Z.; Zhang, Y .; Zhou, H.; Liu, S.; Zhao, Z.;\nZhao, Q.; Yue, C.; Zhang, X.; Yang, Z.; Richardson, K.; and\nLan, Z. 2020. CLUE: A Chinese Language Understanding\nEvaluation Benchmark. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics, 4762–\n4772. Barcelona, Spain (Online): International Committee\non Computational Linguistics.\nZhang, N.; Zhang, J.; Wang, X.; et al. 2023. DeepKE-\nLLM: A Large Language Model Based Knowledge Extrac-\ntion Toolkit. GitHub repository.\nZhang, X.; Zhao, J.; and LeCun, Y . 2015. Character-level\nConvolutional Networks for Text Classification. InNeurIPS.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu, Z.;\nZhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.;\nGonzalez, J. E.; and Stoica, I. 2023. Judging LLM-as-a-\njudge with MT-Bench and Chatbot Arena.\nZhu, Y .; Zhang, P.; Haq, E.-U.; Hui, P.; and Tyson, G.\n2023. Can ChatGPT Reproduce Human-Generated Labels?\nA Study of Social Computing Tasks.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19467",
  "topic": "Sequence (biology)",
  "concepts": [
    {
      "name": "Sequence (biology)",
      "score": 0.5864807367324829
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5481685400009155
    },
    {
      "name": "Computer science",
      "score": 0.47279804944992065
    },
    {
      "name": "Natural language processing",
      "score": 0.35176610946655273
    },
    {
      "name": "Mathematics",
      "score": 0.12165120244026184
    },
    {
      "name": "Biology",
      "score": 0.08207809925079346
    },
    {
      "name": "Genetics",
      "score": 0.054271847009658813
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I30809798",
      "name": "ShanghaiTech University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    }
  ]
}