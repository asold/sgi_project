{
  "title": "Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond",
  "url": "https://openalex.org/W3024566755",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2367810317",
      "name": "Zhang, Zhuosheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2084083973",
      "name": "Zhao Hai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1686891224",
      "name": "Wang Rui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2951873305",
    "https://openalex.org/W2928161238",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W3019858811",
    "https://openalex.org/W2946542158",
    "https://openalex.org/W2982426914",
    "https://openalex.org/W92366497",
    "https://openalex.org/W2794554529",
    "https://openalex.org/W2890399523",
    "https://openalex.org/W2975208319",
    "https://openalex.org/W2734823783",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2891304738",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2789078277",
    "https://openalex.org/W2996251235",
    "https://openalex.org/W2971740165",
    "https://openalex.org/W2182959134",
    "https://openalex.org/W2746097825",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W3046423960",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W2998072062",
    "https://openalex.org/W2963963993",
    "https://openalex.org/W2964087080",
    "https://openalex.org/W2900616773",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2911529999",
    "https://openalex.org/W2988421999",
    "https://openalex.org/W2963506049",
    "https://openalex.org/W2692059227",
    "https://openalex.org/W2810519866",
    "https://openalex.org/W2962922117",
    "https://openalex.org/W2938224028",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2973113793",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2998665041",
    "https://openalex.org/W2552027021",
    "https://openalex.org/W2964047576",
    "https://openalex.org/W2969406527",
    "https://openalex.org/W2970742161",
    "https://openalex.org/W2887356046",
    "https://openalex.org/W2740747242",
    "https://openalex.org/W2963983586",
    "https://openalex.org/W2966491090",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2974593375",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W2897789462",
    "https://openalex.org/W2963010846",
    "https://openalex.org/W2799187742",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W3023612395",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2036973875",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2158108973",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3026992186",
    "https://openalex.org/W2739749670",
    "https://openalex.org/W3022239487",
    "https://openalex.org/W2613681825",
    "https://openalex.org/W2995558462",
    "https://openalex.org/W2978124139",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2966750840",
    "https://openalex.org/W2609826708",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W2979400990",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2891946694",
    "https://openalex.org/W2889048825",
    "https://openalex.org/W2963595025",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2888302696",
    "https://openalex.org/W2985220278",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2949847757",
    "https://openalex.org/W2998579922",
    "https://openalex.org/W2889453388",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2951365061",
    "https://openalex.org/W2516930406",
    "https://openalex.org/W3002535714",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W2953365054",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W3123135857",
    "https://openalex.org/W2963159735",
    "https://openalex.org/W2963769536",
    "https://openalex.org/W2998099211",
    "https://openalex.org/W2912904516",
    "https://openalex.org/W2962790689",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2121300346",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3014438226",
    "https://openalex.org/W2963344337",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962808855",
    "https://openalex.org/W2949961827",
    "https://openalex.org/W2963195889",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2075780421",
    "https://openalex.org/W2086282622",
    "https://openalex.org/W2962718483",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2757715585",
    "https://openalex.org/W2963363070",
    "https://openalex.org/W2798365136",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3015637204",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2963547127",
    "https://openalex.org/W3001393026",
    "https://openalex.org/W2963863909",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2986670164",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2950175955",
    "https://openalex.org/W2891113091",
    "https://openalex.org/W3103111734",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2417356443",
    "https://openalex.org/W2948714225",
    "https://openalex.org/W2970900584",
    "https://openalex.org/W2952862139",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2889646190",
    "https://openalex.org/W2909672886",
    "https://openalex.org/W2938205538",
    "https://openalex.org/W2557764419",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2981573048",
    "https://openalex.org/W1501594139",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W2528904340",
    "https://openalex.org/W2964222271",
    "https://openalex.org/W2955654429",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W3104486441",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W3015031314",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2033047024",
    "https://openalex.org/W2964302308",
    "https://openalex.org/W2963938442",
    "https://openalex.org/W3097239661",
    "https://openalex.org/W2955504197",
    "https://openalex.org/W2987669390",
    "https://openalex.org/W2125436846",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2791374212",
    "https://openalex.org/W2963957489",
    "https://openalex.org/W2977453614",
    "https://openalex.org/W2106705916",
    "https://openalex.org/W2798858969",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2995923603",
    "https://openalex.org/W2130903752",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W2740432174",
    "https://openalex.org/W2963898730",
    "https://openalex.org/W2964146920",
    "https://openalex.org/W2893442076",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2962809918",
    "https://openalex.org/W2968154628",
    "https://openalex.org/W2963301888",
    "https://openalex.org/W2963928014",
    "https://openalex.org/W2752099845",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W3011013442",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W2510759893",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2970169521",
    "https://openalex.org/W2521709538",
    "https://openalex.org/W3011035115",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963681467",
    "https://openalex.org/W2973061659",
    "https://openalex.org/W2904132824",
    "https://openalex.org/W2896342318",
    "https://openalex.org/W2998581211",
    "https://openalex.org/W2949849869",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2550837020",
    "https://openalex.org/W2995807242",
    "https://openalex.org/W3083862392",
    "https://openalex.org/W2946628371",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2804243436",
    "https://openalex.org/W3002072934",
    "https://openalex.org/W2920665390",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2788292930",
    "https://openalex.org/W2251287417"
  ],
  "abstract": "Machine reading comprehension (MRC) aims to teach machines to read and comprehend human languages, which is a long-standing goal of natural language processing (NLP). With the burst of deep neural networks and the evolution of contextualized language models (CLMs), the research of MRC has experienced two significant breakthroughs. MRC and CLM, as a phenomenon, have a great impact on the NLP community. In this survey, we provide a comprehensive and comparative review on MRC covering overall research topics about 1) the origin and development of MRC and CLM, with a particular focus on the role of CLMs; 2) the impact of MRC and CLM to the NLP community; 3) the definition, datasets, and evaluation of MRC; 4) general MRC architecture and technical methods in the view of two-stage Encoder-Decoder solving architecture from the insights of the cognitive process of humans; 5) previous highlights, emerging topics, and our empirical analysis, among which we especially focus on what works in different periods of MRC researches. We propose a full-view categorization and new taxonomies on these topics. The primary views we have arrived at are that 1) MRC boosts the progress from language processing to understanding; 2) the rapid improvement of MRC systems greatly benefits from the development of CLMs; 3) the theme of MRC is gradually moving from shallow text matching to cognitive reasoning.",
  "full_text": "Machine Reading Comprehension:\nThe Role of Contextualized Language Models\nand Beyond\nZhuosheng Zhang\nShanghai Jiao Tong University\nDepartment of Computer Science and\nEngineering\nzhangzs@sjtu.edu.cn\nHai Zhao\nShanghai Jiao Tong University\nDepartment of Computer Science and\nEngineering\nzhaohai@cs.sjtu.edu.cn\nRui Wang\nNational Institute of Information and\nCommunications Technology (NICT)\nwangrui@nict.go.jp\nMachine reading comprehension (MRC) aims to teach machines to read and comprehend human\nlanguages, which is a long-standing goal of natural language processing (NLP). With the burst of\ndeep neural networks and the evolution of contextualized language models (CLMs), the research\nof MRC has experienced two signiﬁcant breakthroughs. MRC and CLM, as a phenomenon,\nhave a great impact on the NLP community. In this survey, we provide a comprehensive and\ncomparative review on MRC covering overall research topics about 1) the origin and development\nof MRC and CLM, with particular focus on the role of CLMs; 2) the impact of MRC and CLM\nto the NLP community; 3) the deﬁnition, datasets, and evaluation of MRC; 4) general MRC\narchitecture and technical methods in the view of two-stage Encoder-Decoder solving archi-\ntecture from the insights of the cognitive process of humans; 5) previous highlights, emerging\ntopics, and our empirical analysis, among which we especially focus on what works in different\nperiods of MRC researches. We propose a full-view categorization and new taxonomies on these\ntopics. The primary views we have arrived at are that 1) MRC boosts the progress from language\nprocessing to understanding; 2) the rapid improvement of MRC systems greatly beneﬁts from\nthe development of CLMs; 3) the theme of MRC is gradually moving from shallow text matching\nto cognitive reasoning.\n1. Introduction\nNatural language processing (NLP) tasks can be roughly divided into two categories:\n1) fundamental NLP , including language modeling and representation, and linguistic\nstructure and analysis, including morphological analysis, word segmentation, syntactic,\nsemantic and discourse paring, etc.; 2) application NLP , including machine question\nanswering, dialogue system, machine translation, and other language understanding\nand inference tasks. With the rapid development of NLP , natural language under-\nstanding (NLU) has aroused broad interests, and a series of NLU tasks have emerged.\nIn the early days, NLU was regarded as the next stage of NLP . With more compu-\ntation resources available, more complex networks become possible, and researchers\n© 2020 Association for Computational Linguistics\narXiv:2005.06249v1  [cs.CL]  13 May 2020\nComputational Linguistics Volume 1, Number 1\nare inspired to move forward to the frontier of human-level language understanding.\nInevitably, machine reading comprehension (MRC) (Richardson, Burges, and Renshaw\n2013; Hermann et al. 2015; Hill et al. 2015; Rajpurkar et al. 2016) as a new typical task has\nboomed in the ﬁeld of NLU. Figure 1 overviews MRC in the background of language\nprocessing and understanding.\nFigure 1: Overview of language processing and understanding.\nMRC is a long-standing goal of NLU that aims to teach a machine to read and com-\nprehend textual data. It has signiﬁcant application scenarios such as question answering\nand dialog systems (Choi et al. 2018; Reddy, Chen, and Manning 2019; Zhang et al.\n2018c; Zhu et al. 2018b; Xu et al. 2020). The related MRC research can be traced back to\nthe studies of story comprehension (Lehnert 1977; Cullingford 1977). After decades of\ndecline, MRC becomes a hot research topic recently and experiences rapid development.\nMRC has a critical impact on NLU and the broader NLP community. As one of the major\nand challenging problems of NLP concerned with comprehensive knowledge repre-\nsentation, semantic analysis, and reasoning, MRC stimulates great research interests in\nthe last decade. The study of MRC has experienced two signiﬁcant peaks, namely, 1)\nthe burst of deep neural networks; 2) the evolution of contextualized language models\n(CLMs). Figure 2 shows the research trend statistics of MRC and CLMs in the past ﬁve\nyears.\nEarly MRC task was simpliﬁed as requiring systems to return a sentence that\ncontains the right answer. The systems are based on rule-based heuristic methods, such\nas bag-of-words approaches (Hirschman et al. 1999), and manually generated rules\n(Riloff and Thelen 2000; Charniak et al. 2000). With the introduction of deep neural\nnetworks and effective architecture like attention mechanisms in NLP (Bahdanau, Cho,\nand Bengio 2014; Hermann et al. 2015), the research interests of MRC boomed since\naround 2015 (Chen, Bolton, and Manning 2016; Bajaj et al. 2016; Rajpurkar et al. 2016;\nTrischler et al. 2017; Dunn et al. 2017; He et al. 2018; Koˇ cisk`y et al. 2018; Yang et al. 2018;\nReddy, Chen, and Manning 2019; Pan et al. 2019a). The main topics were ﬁne-grained\ntext encoding and better passage and question interactions (Seo et al. 2017; Yang et al.\n2017a; Dhingra et al. 2017; Cui et al. 2017; Zhang et al. 2018b).\nCLMs lead to a new paradise of contextualized language representations — using\nthe whole sentence-level representation for language modeling as pre-training, and the\ncontext-dependent hidden states from the LM are used for downstream task-speciﬁc\nﬁne-tuning. Deep pre-trained CLMs (Peters et al. 2018; Devlin et al. 2018; Yang et al.\n2019c; Lan et al. 2019; Dong et al. 2019; Clark et al. 2019c; Joshi et al. 2020) greatly\n2\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nstrengthened the capacity of language encoder, the benchmark results of MRC were\nboosted remarkably, which stimulated the progress towards more complex reading,\ncomprehension, and reasoning systems (Welbl, Stenetorp, and Riedel 2018; Yang et al.\n2018; Ding et al. 2019). As a result, the researches of MRC become closer to human cog-\nnition and real-world applications. On the other hand, more and more researchers are\ninterested in analyzing and interpreting how the MRC models work, and investigating\nthe real ability beyond the datasets, such as performance in the adversarial attack (Jia\nand Liang 2017; Wallace et al. 2019), as well as the benchmark capacity of MRC datasets\n(Sugawara et al. 2018, 2019; Schlegel et al. 2020). The common concern is the over-\nestimated ability of MRC systems, which shows to be still in a shallow comprehension\nstage drawn from superﬁcial pattern-matching heuristics. Such assessments of models\nand datasets would be suggestive for next-stage studies of MRC methodologies.\n3\n24\n29\n62\n87\n45\n78\n115\n179\n289\n1\n1\n1\n3\n29\n0 50 100 150 200 250 300 350 400\n2015\n2016\n2017\n2018\n2019\nMRC\n QA\n CLM\nFigure 2: The number of papers concerning MRC, QA, and CLM collected from 2015\nto 2019. The search terms are MRC: {machine reading comprehension, machine com-\nprehension, machine comprehend, mrc}; QA: {question answering, qa}. Since MRC\npapers are often in the name of QA, we also present the QA papers for reference. MRC\nand QA papers are searched by keywords in paper titles on https://arxiv.org.\nCLM statistics are calculated based on the inﬂuential open-source repository: https:\n//github.com/thunlp/PLMpapers.\nMRC is a generic concept to probe for language understanding capabilities (Schlegel\net al. 2020; Gardner et al. 2019). In the early stage, MRC was regarded as the form\nof triple-style (passage, question, answer) question answering (QA) task, such as the\ncloze-style (Hermann et al. 2015; Hill et al. 2015), multiple-choice (Lai et al. 2017; Sun\net al. 2019a), and span-QA (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang 2018). In\nrecent years, we witness that the concept of MRC has evolved to a broader scope,\nwhich caters to the theme of language understanding based interaction and reasoning,\nin the form of question answering, text generation, conversations, etc. Though MRC\noriginally served as the form of question answering, it can be regarded as not only just\nthe extension of QA but also a new concept used for studying the capacity of language\nunderstanding over some context that is close to cognitive science, instead of a single\ntask itself. Regarding MRC as phenomenon, there is a new emerging interest showing\nthat classic NLP tasks can be cast as span-QA MRC form, with modest performance\ngains than previous methodologies (McCann et al. 2018; Keskar et al. 2019; Li et al.\n2019b,a; Keskar et al. 2019; Gao et al. 2019a, 2020).\n3\nComputational Linguistics Volume 1, Number 1\nAlthough it is clear that computation power substantially fuels the capacity of MRC\nsystems in the long run, building simple, explainable, and practical models is equally\nessential for real-world applications. It is instructive to review the prominent highlights\nin the past. The generic nature, especially what works in the past and the inspirations\nof MRC to the NLP community, would be suggestive for future studies, which are the\nfocus of discussions in this work.\nThis work reviews MRC covering the scope of background, deﬁnition, inﬂuence,\ndatasets, technical and benchmark success, empirical assessments, current trends, and\nfuture opportunities. Our main contributions are summarized as follows:\n• Comprehensive review and in-depth discussions. We conduct a comprehensive\nreview of the origin and the development of MRC, with a special focus on the\nrole of CLMs. We propose new taxonomies of the technical architecture of MRC,\nby formulating the MRC systems as two-stage solving architecture in the view\nof cognition psychology and provide a comprehensive discussion of research\ntopics to gain insights. By investigating typical models and the trends of the main\nﬂagship datasets and leaderboards concerning different types of MRC, along with\nour empirical analysis, we provide observations of the advances of techniques in\ndifferent stages of studies.\n• Wide coverage on highlights and emerging topics . MRC has experienced rapid\ndevelopment. We present a wide coverage of previous highlights and emerging\ntopics, including casting traditional NLP tasks into MRC formation, multiple\ngranularity feature modeling, structured knowledge injection, contextualized sen-\ntence representation, matching interaction, and data augmentation.\n• Outlook on the future. This work summaries the trends and discussions for future\nresearches, including interpretability of datasets and models, decomposition of\nprerequisite skills, complex reasoning, large-scale comprehension, low-resource\nMRC, multimodal semantic grounding, and deeper but efﬁcient model design.\nThe remainder of this survey is organized as follows: ﬁrst, we present the back-\nground, categorization, and derivatives of CLM and discuss the mutual inﬂuence be-\ntween CLM and MRC in §2; an overview of MRC including the impact to general NLP\nscope, formations, datasets, and evaluation metrics is given in §3; then, we discuss the\ntechnical methods in the view of two-stage solving architecture, and summarize the\nmajor topics and challenges in §4; next, our work goes deeper in §5 to discover what\nworks in different stages of MRC, by reviewing the trends and highlights entailed in\nthe typical MRC models. Our empirical analysis is also reported for the veriﬁcation of\nsimple and effective tactic optimizations based on the strong CLMs; ﬁnally, we discuss\nthe trends and future opportunities in §6, together with conclusions in §7;\n2. The Role of Contextualized Language Model\n2.1 From Language Model to Language Representation\nLanguage modeling is the foundation of deep learning methods for natural lan-\nguage processing. Learning word representations has been an active research area,\nand aroused great research interests for decades, including non-neural (Brown et al.\n1992; Ando and Zhang 2005; Blitzer, McDonald, and Pereira 2006) and neural methods\n(Mikolov et al. 2013; Pennington, Socher, and Manning 2014). Regarding language\nmodeling, the basic topic is n-gram language model (LM). An n-gram Language model\nis a probability distribution over word ( n-gram) sequences, which can be regarded\n4\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nTable 1: Comparison of language representation.\nModel Repr. form Context Training object Usage\nn-gram LM One-hot Sliding widow n-gram LM (MLE) Lookup\nWord2vec/GloVe Embedding Sliding widow n-gram LM (MLE) Lookup\nContextualized LM Embedding Sentence n-gram LM (MLE), +ext Fine-tune\nwith a training objective of predicting unigram from ( n−1)-gram. Neural networks\nuse continuous and dense representation, or further embedding of words to make their\npredictions, which is effective for alleviating the curse of dimensionality – as language\nmodels are trained on larger and larger texts, the number of unique words increases.\nCompared with the word embeddings learned by Word2Vec (Mikolov et al. 2013) or\nGloVe (Pennington, Socher, and Manning 2014), sentence is the least unit that delivers\ncomplete meaning as human uses language. Deep learning for NLP quickly found it is\na frequent requirement on using a network component encoding a sentence input so\nthat we have the Encoder for encoding the complete sentence-level context. The encoder\ncan be the traditional RNN, CNN, or the latest Transformer-based architectures, such\nas ELMo (Peters et al. 2018), GPT v1 (Radford et al. 2018), BERT (Devlin et al. 2018),\nXLNet (Yang et al. 2019c), RoBERTa (Liu et al. 2019c), ALBERT (Lan et al. 2019), and\nELECTRA (Clark et al. 2019c), for capturing the contextualized sentence-level language\nrepresentations.1 These encoders differ from sliding window input (e.g., that used in\nWord2Vec) that they cover a full sentence instead of any ﬁxed length sentence segment\nused by the sliding window. Such difference especially matters when we have to handle\npassages in MRC tasks, where the passage always consists of a lot of sentences. When\nthe model faces passages, the sentence, instead of word, is the basic unit of a passage.\nIn other words, MRC, as well as other application tasks of NLP , needs a sentence-\nlevel encoder, to represent sentences into embeddings, so as to capture the deep and\ncontextualized sentence-level information.\nAn encoder model can be trained in a style of n-gram language model so that\nthere comes the language representation, which includes four elements: 1) represen-\ntation form; 2) context; 3) training object (e.g., n-gram language model); 4) usage. For\ncontextualized language representation, the representation for each word depends on\nthe entire context in which it is used, which is dynamic embedding. Table 1 presents a\ncomparison of the three main language representation approaches.\n2.2 CLM as Phenomenon\n2.2.1 Revisiting the Deﬁnition. First, we would like to revisit the deﬁnitions of the\nrecent contextualized encoders. For the representative models, ELMo is called Deep\ncontextualized word representations, and BERT Pre-training of deep bidirectional transformers\nfor language understanding . With the follow-up research goes on, there are studies that\ncall those models as pre-trained (language) models (Sanh et al. 2019; Goldberg 2019).\nWe argue that such a deﬁnition is reasonable but not accurate enough. The focus of\nthese models are supposed to be contextualized (as that show in the name of ELMo), in\n1 This is a non-exhaustive list of important CLMs introduced recently. In this work, our discussions are\nmainly based on these typical CLMs, which are highly related to MRC researches, and most of the other\nmodels can be regarded as derivatives.\n5\nComputational Linguistics Volume 1, Number 1\nterms of the evolution of language representation architectures, and the actual usages of\nthese models nowadays. As a consensus of limited computing resources, the common\npractice is to ﬁne-tune the model using task-speciﬁc data after the public pre-trained\nsources, so that pre-training is neither the necessary nor the core element. As shown in\nTable 1, the training objectives are derived from n-gram language models. Therefore,\nwe argue that pre-training and ﬁne-tuning are just the manners we use the models. The\nessence is the deep contextualized representation from language models; thus, we call\nthese pre-trained models contextualized language models, CLMs) in this paper.\n2.2.2 Evolution of CLM Training Objectives.In this part, we abstract the inherent rela-\ntionship of n-gram language model and the subsequent contextualized LM techniques.\nThen, we elaborate the evolution of the typical CLMs considering the salient role of the\ntraining objectives.\nRegarding the training of language models, the standard and common practice is\nusing the n-gram language modeling. It is also the core training objective in CLMs. An\nn-gram Language model yields a probability distribution over text (n-gram) sequences,\nwhich is a classic maximum likelihood estimation (MLE) problem. The language mod-\neling is also known as autoregressive (AR) scheme.\nw1 w2 wi wi+n-1 wL…\nn-gram\nSequence:\n(Sentence)\n… …\nFigure 3: Example of n-grams.\nSpeciﬁcally, given a sequence of n items w = wi:i+n−1 from a text (Figure 3), the\nprobability of the sequence is measured as\np(w) =p(wi |wi:i+n−2), (1)\nwhere p(wi|wi:i+n−2) denotes the conditional probability ofp(wi) in the sequence, which\ncan be estimated by the context representation over wi:i+n−2. The LM training is per-\nformed by maximizing the likelihood:\nmax\nθ\n∑\nw\nlog pθ(w), (2)\nwhere θdenotes the model parameter.\nIn practice, n-gram models have been shown to be extremely effective in modeling\nlanguage data, which is a core component in modern language applications. The early\ncontextualized representation is obtained by static word embedding and a network\nencoder. For example, CBOW and Skip-gram (Mikolov et al. 2013) either predicts the\nword using context or predict context by word, where the n-gram context is provided\nby a ﬁxed sliding window. The trained model parameters are output as a word embed-\nding matrix (also known as a lookup table), which contains the context-independent\nrepresentations for each word in a vocabulary. The vectors are then used in a low-level\nlayer (i.e., embedding layer) of neural network, and an encoder, such as RNN is further\nused to obtain the contextualized representation for an input sentence.\nFor recent LM-derived contextualized presentations (Peters et al. 2018; Devlin et al.\n2018; Yang et al. 2019c), the central point of the subsequent optimizations are concerning\n6\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nw1 [M] [M] w4 w5\np1 p2 p3 p4 p5\nw2 w3\nw1 w4 w5 [M] [M]\nw2 w3\np1 p4 p5 p2 p3\nw1 w2 w3 w4 w5\np1 p2 p3 p4 p5\nw2 w3\nw1 w4 w5 w2 w3\nw2 w3\np1 p4 p5 p2 p3\nw1 [M] [M] w4 w5\nw2 w3\nw1 [M] w3 w4 w5\nw2 w3\nw1 w2 [M] w4 w5\nw2 w3\n(a)\n(b) (c)\n…\nFigure 4: The possible transformation of MLM and PLM, where wi and pi represent\ntoken and position embeddings. [M] is the special mask token used in MLM. The\nleft side of MLM (a) can be seen as bidirectional AR streams (in blue and yellow,\nrespectively) at the right side. For MLM (b) and PLM (c), the left sides are in original\norder, and the right sides are in permuted order, which are regarded as a uniﬁed view.\nthe context. They are trained with much largern-grams that cover a full sentence where\nn is extended to the sentence length — when n expands to the maximum, the con-\nditional context thus corresponds to the whole sequence . The word representations\nare the function of the entire sentence, instead of the static vectors over a pre-deﬁned\nlookup table. The corresponding functional model is regarded as a contextualized\nlanguage model. Such a contextualized model can be directly used to produce context-\nsensitive sentence-level representations for task-speciﬁc ﬁne-tuning. Table 2 shows the\ncomparisons of CLMs.\nFor an input sentences = w1:L, we extend the objective ofn-gram LM in the context\nof length Lfrom Equation (2):\nL∑\nk=c+1\nlog pθ(wk |w1:k−1), (3)\nwhere c is the cutting point that separate the sequence into a non-target conditional\nsubsequence k≤cand a target subsequence k>c . It can be further written in a bidirec-\ntional form:\nL∑\nk=c+1\n(log pθ(wk |w1:k−1) + logpθ(wk |wk+1:L)), (4)\n7\nComputational Linguistics Volume 1, Number 1\nTable 2: Comparison of CLMs. NSP: next sentence prediction (Devlin et al. 2018). SOP:\nsentence order prediction (Lan et al. 2019). RTD: replaced token detection (Clark et al.\n2019c).\nModel Loss 2nd Loss Direction Encoder arch. Input\nELMo n-gram LM - Bi RNN Char\nGPTv1 n-gram LM - Uni Transformer Subword\nBERT Masked LM NSP Bi Transformer Subword\nRoBERTa Masked LM - Bi Transformer Subword\nALBERT Masked LM SOP Bi Transformer Subword\nXLNet Permu. n-gram LM - Bi Transformer-XL Subword\nELECTRA Masked LM RTD Bi GAN Subword\nwhich corresponds to the bidirectional LM used in ELMo (Peters et al. 2018). The\nbidirectional modeling of ELMo is achieved by the concatenation of independently\ntrained forward and backward LSTMs.\nTo allow simultaneous bidirectional (or non-directional) training, BERT (Devlin\net al. 2018) adopted Transformer to process the whole input at once, and proposed\nMasked LM (MLM) to take advantage of both the left and right contexts. Some tokens in\na sentence are randomly replaced with a special mask symbol with a small probability.\nThen, the model is trained to predict the masked token based on the context. MLM\ncan be seen as a variant of n-gram LM (Figure 4(a)) to a certain extent — bidirectional\nautoregressive n-gram LM.2 Let Ddenote the set of masked positions using the mask\nsymbol [M]. We have wD as the set of masked tokens, and s′ as the masked sentence.\nAs the example shown in the left part of Figure 4(b), D= {2,3}, wD= {w2,w3}and\ns′= {w1,[M],w4,[M],w5}. The objective of MLM is to maximize the following objec-\ntive:\n∑\nk∈D\nlog pθ(wk |s′) (5)\nCompared with Equation (4), it is easy to ﬁnd that the prediction is based on the\nwhole context in Equation (5) instead of only one direction for each estimation, which\nindicates the major difference of BERT and ELMo. However, the essential problem in\nBERT is that the mask symbols are never seen at ﬁne-tuning, which faces a mismatch\nbetween pre-training and ﬁne-tuning.\nTo alleviate the issue, XLNet (Yang et al. 2019c) utilized permutation LM (PLM) to\nmaximize the expected log-likelihood of all possible permutations of the factorization\norder, which is the AR LM objective.3 For the input sentences = w1:L, we haveZLas the\npermutations of set {1,2,··· ,L}. For a permutation z∈ZL, we split zinto a non-target\nconditional subsequence z≤c and a target subsequence z >c, where c is the cutting\npoint. The objective is to maximize the log-likelihood of the target tokens conditioned\n2 In a general view, the idea of MLM can also be derived from CBOW, which is to predict word according\nto the conditional n-gram surrounding context.\n3 In contrast, the language modeling method in BERT is called denoising autoencoding (Yang et al. 2019c)\n(AE). AE can be seen as the natural combination of AR loss and a certain neural network.\n8\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\non the non-target tokens:\nEz∈ZL\nL∑\nk=c+1\nlog pθ(wzk |wz1:k−1 ). (6)\nThe key of both MLM and PLM is predicting word(s) according to a certain context\nderived from n-grams, which can be modeled in a uniﬁed view (Song et al. 2020). In\ndetail, under the hypothesis of word order insensitivity, MLM can be directly uniﬁed as\nPLM when the input sentence is permutable (with insensitive word orders), as shown in\nFigure 4(b-c). It can be satisﬁed thanks to the nature of the Transformer-based models,\nsuch as BERT and XLNet. Transformer takes tokens and their positions in a sentence as\ninputs, and it is not sensitive to the absolute input order of these tokens. Therefore, the\nobjective of MLM can be also written as the permutation form,\nEz∈ZL\nL∑\nk=c+1\nlog pθ(wzk |wz1:c,Mzk:L), (7)\nwhere Mzk:L denote the special mask tokens [M] in positions zk:L.\nFrom Equations (3), (6), and (7), we see that MLM and PLM share similar formula-\ntions with the n-gram LM with slight difference in the conditional context part in p(s):\nMLM conditions on wz1:c and Mk:L, and PLM conditions on wz1:k−1 . Both MLM and\nPLM can be explained by the n-gram LM, and even uniﬁed into a general formation.\nWith similar inspiration, MPNet (Song et al. 2020) combined the Masked LM and\nPremuted LM for taking both of the advantages.\n2.2.3 Architectures of CLMs. So far, there are mainly three leading architectures for\nlanguage modeling,4 RNN, Transformer, and Transformer-XL. Figure 5 depicts the three\nencoder architectures.\nRNN. RNN and its derivatives are popular approaches for language encoding and\nmodeling. The widely-used variants are GRU (Cho et al. 2014) and LSTM (Hochreiter\nand Schmidhuber 1997). RNN models process the input tokens (commonly words\nor characters) one by one to capture the contextual representations between them.\nHowever, the processing speed of RNNs is slow, and the ability to learn long-term\ndependencies is still limited due to vanishing gradients.\nTransformer. To alleviate the above issues of RNNs, Transformer was proposed, which\nemploys multi-head self-attention (Vaswani et al. 2017) modules receive a segment of\ntokens (i.e., subwords) and the corresponding position embedding as input to learn\nthe direct connections of the sequence at once, instead of processing tokens one by one.\nTransformer-XL. Though both RNN and Transformer architectures have reached im-\npressive achievements, their main limitation is capturing long-range dependencies.\nTransformer-XL (Dai et al. 2019) combines the advantages of RNN and Transformer,\n4 Actually, CNN also turns out well-performed feature extractor for some NLP tasks like text classiﬁcation,\nbut RNN is more widely used for MRC, even most NLP tasks; thus we omit the description of CNNs and\nfocus on RNNs as the example for traditional encoders.\n9\nComputational Linguistics Volume 1, Number 1\nFigure 5: RNN, Transformer, and Transformer-XL encoder architectures for CLMs.\nwhich uses the self-attention modules on each segment of input data and a recurrent\nmechanism to learn dependencies between consecutive segments. In detail, two new\ntechniques are proposed:\n1. Segment-level Recurrence . The recurrence mechanism is proposed to model\nlong-term dependencies by using information from previous segments. During\ntraining, the representations computed for the previous segment are ﬁxed and\ncached to be reused as an extended context when the model processes the next\nnew segment. This recurrence mechanism is also effective in resolving the context\nfragmentation issue, providing necessary context for tokens in the front of a new\nsegment.\n2. Relative Positional Encoding . The original positional encoding deals with each\nsegment separately. As a result, the tokens from different segments have the same\npositional encoding. The new relative positional encoding is designed as part of\neach attention module, as opposed to the encoding position only before the ﬁrst\nlayer. It is based on the relative distance between tokens, instead of their absolute\nposition.\n2.2.4 Derivative of CLMs. Pre-training and ﬁne-tuning have become a new paradigm\nof NLP , and the major theme is to build a strong encoder. Based on the inspirations of\nimpressive models like ELMo and BERT, a wide range of CLMs derivatives have been\nproposed. In this part, we discuss various major variants concerning MRC tasks. Table 3\nshows the performance comparison of the CLM derivatives. The advances behind these\nmodels are in four main topics:\nMasking Strategy. The original masking of BERT is based on subword, which would\nbe insufﬁcient for capturing global information using the local subword signals. Span-\nBERT (Joshi et al. 2020) proposed a random span masking strategy based on geometric\ndistribution, indicating that the proposed masking sometimes works even better than\nmasking linguistically-coherent spans. To avoid using the same mask for each training\n10\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nFigure 6: Derivative of CLMs. The main features are noted above the arrow. Solid and\ndotted arrows indicate the direct and implicit inheritance.\ninstance in every epoch, RoBERTa (Liu et al. 2019c) used dynamic masking to generate\nthe masking pattern every time feeding a sequence to the model, indicating that dy-\nnamic masking would be crucial for pre-training a great many steps or with large-scale\ndatasets. ELECTRA (Clark et al. 2019c) improved the efﬁciency of masking by adopting\na replaced token detection objective.\nKnowledge Injection. Extra knowledge can be easily incorporated into CLMs by both\nembedding fusion and masking. SemBERT (Zhang, Zhao, and Zhou 2020) indicated that\nfusing semantic role label embedding and word embedding can yield better semantic-\nlevel language representation, showing that salient word-level high-level tag features\ncan be well integrated with subword-level token representations. SG-Net (Zhang et al.\n2020c) presented a dependency-of-interest masking strategy to use syntax information\nas a constraint for better linguistics inspired representation.\nTraining Objective. Besides the core MLE losses that used in language models, some\nextra objectives were investigated for better adapting target tasks. BERT (Devlin et al.\n2018) adopted the next sentence prediction (NSP) loss, which matches the paired form\nin NLI tasks. To better model inter-sentence coherence, ALBERT (Lan et al. 2019) re-\nplaced NSP loss with a sentence order prediction (SOP) loss. StuctBERT (Wang et al.\n2020a) further leveraged word-level ordering and sentence-level ordering as structural\nobjectives in pre-training. SpanBERT (Joshi et al. 2020) used span boundary objective\n(SBO), which requires the model to predict masked spans based on span boundaries,\nto integrate structure information into pre-training. UniLM (Dong et al. 2019) extended\n11\nComputational Linguistics Volume 1, Number 1\nTable 3: Performance of CLM derivatives. F1 scores for SQuAD1.1 and SQuAD2.0,\naccuracy for RACE. * indicates results that depend on additional data augmentation. †\nindicate the result is from Yang et al. (2019c) as it was not reported in the original paper\n(Devlin et al. 2018). The BERTbase result for SQuAD2.0 is from Wang et al. (2020b). The\nitalic numers are baselines for calculating the D-values ↑.\nMethod SQuAD1.1 SQuAD2.0 RACE\nDev ↑Dev Test ↑Test Dev ↑Dev Test ↑Test Acc ↑Acc\nELMo 85.6 - 85.8 - - - - -\nGPTv1 - - - - - - - - 59.0 -\nBERTbase 88.5 2.9 - - 76.8 - 65.3 6.3\nBERT-PKD 85.3 -0.3 - - 69.8 -7.0 - - 60.3 1.3\nDistilBERT 86.2 0.6 - - 69.5 -7.3 - -\nTinyBERT 87.5 1.9 - - 73.4 -3.4 - - - -\nMiniLM - - - - 76.4 -0.4 - - - -\nQ-BERT 88.4 2.8 - - - - - - - -\nBERTlarge 91.1* 5.5 91.8* 6 81.9 5.1 83.0 - 72.0 † -\nSemBERTlarge - - - - 83.6 6.8 85.2 2.2 - -\nSG-Net - - - - 88.3 11.5 87.9 4.9 74.2 15.2\nSpanBERTlarge - - 94.6 8.8 - - 88.7 5.7 - -\nStructBERTlarge 92.0 6.4 - - - - - - - -\nRoBERTalarge 94.6 9.0 - - 89.4 12.6 89.8 6.8 83.2 24.2\nALBERTxxlarge 94.8 9.2 - - 90.2 13.4 90.9 7.9 86.5 27.5\nXLNetlarge 94.5 8.9 95.1* 9.3 88.8 12 89.1* 6.1 81.8 22.8\nUniLM - - - - 83.4 6.6 - - - -\nELECTRAlarge 94.9 9.3 - - 90.6 13.8 91.4 8.4 - -\nMegatron-LM3.9B 95.5 9.9 - - 91.2 14.4 - - 89.5 30.5\nT511B 95.6 10.0 - - - - - - - -\nthe mask prediction task with three types of language modeling tasks: unidirectional,\nbidirectional, and sequence-to-sequence (Seq2Seq) prediction. The Seq2Seq MLM was\nalso adopted as the objective in T5 (Raffel et al. 2019), which employed a uniﬁed Text-to-\nText Transformer for general-purpose language modeling. ELECTRA Clark et al. (2019c)\nproposed new pre-training task — replaced token detection (RTD) and a generator-\ndiscriminator model was designed accordingly. The generator is trained to perform\nMLM, and then the discriminator predicts whether each token in the corrupted input\nwas replaced by a generator sample or not.\nModel Optimization. RoBERTa (Liu et al. 2019c) found that the model performance can\nbe substantially improved by 1) training the model longer, with bigger batches over\nmore data can; 2) removing the next sentence prediction objective; 3) training on longer\nsequences; 4) dynamic masking on the training data. Megatron (Shoeybi et al. 2019) pre-\nsented an intra-layer model-parallelism approach that can support efﬁciently training\nvery large Transformer models.\nTo obtain light-weight yet powerful models for real-world use, model compression\nis an effective solution. ALBERT (Lan et al. 2019) used cross-layer parameter sharing\nand factorized embedding parameterization to reduce the model parameters. Knowl-\nedge distillation (KD) also aroused hot interests. BERT-PKD proposed a patient KD\n12\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nTable 4: The initial applications of CLMs. The concerned NLU task can also be regarded\nas a special case of MRC as discussed in §3.\nNLU MRC\nSNLI GLUE SQuAD1.1 SQuAD2.0 RACE\nELMo \u0013 \u0017 \u0013 \u0017 \u0017\nGPTv1 \u0013 \u0013 \u0017 \u0017 \u0013\nBERT \u0017 \u0013 \u0013 \u0013 \u0017\nRoBERTa \u0017 \u0013 \u0013 \u0013 \u0013\nALBERT \u0017 \u0013 \u0013 \u0013 \u0013\nXLNet \u0017 \u0013 \u0013 \u0013 \u0013\nELECTRA \u0017 \u0013 \u0013 \u0013 \u0017\nmechanism that learns from multiple intermediate layers of the teacher model for\nincremental knowledge extraction. DistilBERT (Sanh et al. 2019) leveraged a knowledge\ndistillation mechanism during the pre-training phase, which introduced a triple loss\ncombining language modeling, distillation, and cosine-distance losses. TinyBERT (Jiao\net al. 2019) adopted layer-to-layer distillation with embedding outputs, hidden states,\nand self-attention distributions. MiniLM (Wang et al. 2020b) performed the distillation\non self-attention distributions and value relation of the teacherâ ˘A ´Zs last Transformer\nlayer to guide student model training. Moreover, quantization is another optimization\ntechnique by compressing parameter precision. Q-BERT (Shen et al. 2019) applied a\nHessian based mix-precision method to compress the mode with minimum loss in\naccuracy and more efﬁcient inference.\n2.3 Correlations Between MRC and CLM\nIn the view of practice, MRC and CLM are complementary to each other. MRC is a\nchallenging problem concerned with comprehensive knowledge representation, seman-\ntic analysis, and reasoning, which arouses great research interests and stimulates the\ndevelopment of wide ranges of advanced models, including CLMs. As shown in Table\n4, MRC also serves as an appropriate testbed for language representation, which is\nthe focus of CLMs. On the other hand, the progress of CLM greatly promotes MRC\ntasks, achieving impressive gains of model performance. With such an indispensable\nassociation, human-parity performance has been ﬁrst achieved and frequently reported\nafter the release of CLMs.\n3. MRC as Phenomenon\n3.1 Classic NLP Meets MRC\nMRC has great inspirations to the NLP tasks. Most NLP tasks can beneﬁt from the new\ntask formation as MRC. The advantage may lie within both sides of 1) strong capacity\nof MRC-style models, e.g., keeping the pair-wise training mode like the pre-training of\nCLMs and better-contextualized modeling like multi-turn question answering (Li et al.\n2019b); 2) unifying different tasks as MRC formation, and taking advantage of multi-\ntasking to share and transfer knowledge.\n13\nComputational Linguistics Volume 1, Number 1\nTraditional NLP tasks can be cast as QA-formed reading comprehension over a\ncontext, including question answering, machine translation, summarization, natural\nlanguage inference, sentiment analysis, semantic role labeling, zero-shot relation extrac-\ntion, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution\n(McCann et al. 2018). The span extraction task formation of MRC also leads to superior\nor comparable performance for standard text classiﬁcation and regression tasks, includ-\ning those in GLUE benchmarks (Keskar et al. 2019), and entity and relation extraction\ntasks (Li et al. 2019b,a; Keskar et al. 2019). As MRC aims to evaluate how well machine\nmodels can understand human language, the goal is actually similar to the task of\nDialogue State Tracking (DST). There are recent studies that formulate the DST task\ninto MRC form by specially designing a question for each slot in the dialogue state, and\npropose MRC models for dialogue state tracking (Gao et al. 2019a, 2020).\n3.2 MRC Goes Beyond QA\nIn most NLP/CL papers, MRC is usually organized as a question answering task with\nrespect to a given reference text (e.g., a passage). As discussed in Chen (2018), there is\na close relationship between MRC and QA. (Shallow) reading comprehension can be\nregarded as an instance of question answering, but they emphasize different ﬁnal tar-\ngets. We believe that the general MRC is a concept to probe for language understanding\ncapabilities, which is very close to the deﬁnition of NLU. In contrast, QA is a format\n(Gardner et al. 2019), which is supposed to be the actual way to check how the machine\ncomprehends the text. The rationale is the difﬁculty to measure the primary objective of\nMRC — evaluating the degree of machine comprehension of human languages. To this\nend, QA is a fairly simple and effective format. MRC also goes beyond the traditional\nQA, such as factoid QA or knowledge base QA (Dong et al. 2015) by reference to\nopen texts, aiming at avoiding efforts on pre-engineering and retrieving facts from a\nstructured manual-crafted knowledge corpus.\nTherefore, though MRC tasks employ the form of question answering, it can be\nregarded as not only just the extension or variant of QA but also a new concept con-\ncerning studying the capacity of language understanding over some context. Reading\ncomprehension is an old term to measure the knowledge accrued through reading.\nWhen it comes to machines, it concerns that machine is trained to read unstructured\nnatural language texts, such as a book or a news article, comprehend and absorb the\nknowledge without the need of human curation.\nTo some extent, traditional language understanding and inference tasks, such as\ntextual entailment (TE), can be regarded as a type of MRC in theory as well. The\ncommon goal is to give a prediction after reading and comprehending the input texts;\nthus the NLI and standard MRC tasks are often evaluated together for assessing model’s\nlanguage understanding capacity (Peters et al. 2018; Radford et al. 2018; Zhang et al.\n2019e, 2020b). Besides, their forms can be converted to each other. MRC can be formed\nas NLI format (Zhang et al. 2019b), and NLI can also be regarded as multi-choice MRC\n(entailment, neutral, or contradictory).\n3.3 Task Formulation\nGiven the reference document or passage, as the standard form, MRC requires the\nmachine to answer questions about it. The formation of MRC can be described as a\ntuple <P,Q,A> , where P is a passage (context), and Qis a query over the contents of\nP, in which Ais the answer or candidate option.\n14\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nTable 5: Examples of typical MRC forms.\nCloze-style from CNN (Hermann et al. 2015)\nContext ( @entity0 ) – a bus carrying members of a @entity5 unit overturned at an @entity7 military base\nsunday , leaving 23 @entity8 injured , four of them critically , the military said in a news release\n. a bus overturned sunday in @entity7 , injuring 23 @entity8 , the military said . the passengers\n, members of @entity13 , @entity14 , @entity15 , had been taking part in a training exercise at\n@entity19 , an @entity21 post outside @entity22 , @entity7 . they were departing the range at 9:20\na.m. when the accident occurred . the unit is made up of reservists from @entity27 , @entity28 ,\nand @entity29 , @entity7 . the injured were from @entity30 and @entity31 out of @entity29 , a\n@entity32 suburb . by mid-afternoon , 11 of the injured had been released to their unit from the\nhospital . pictures of the wreck were provided to the news media by the military . @entity22 is\nabout 175 miles south of @entity32 . e-mail to a friend\nQuestion bus carrying @entity5 unit overturned at military base\nAnswer @entity7\nMulti-choice from RACE (Lai et al. 2017)\nContext Runners in a relay race pass a stick in one direction. However, merchants passed silk, gold, fruit,\nand glass along the Silk Road in more than one direction. They earned their living by traveling the\nfamous Silk Road. The Silk Road was not a simple trading network. It passed through thousands\nof cities and towns. It started from eastern China, across Central Asia and the Middle East, and\nended in the Mediterranean Sea. It was used from about 200 B, C, to about A, D, 1300, when sea\ntravel offered new routes, It was sometimes called the worldâ ˘A ´Zs longest highway. However, the\nSilk Road was made up of many routes, not one smooth path. They passed through what are now\n18 countries. The routes crossed mountains and deserts and had many dangers of hot sun, deep\nsnow, and even battles. Only experienced traders could return safely.\nQuestion The Silk Road became less important because .\nAnswer A.it was made up of different routes B.silk trading became less popular\nC.sea travel provided easier routes D.people needed fewer foreign goods\nSpan Extraction from SQuAD (Rajpurkar et al. 2016)\nContext Robotics is an interdisciplinary branch of engineering and science that includes mechanical\nengineering, electrical engineering, computer science, and others. Robotics deals with the design,\nconstruction, operation, and use of robots, as well as computer systems for their control, sensory\nfeedback, and information processing. These technologies are used to develop machines that can\nsubstitute for humans. Robots can be used in any situation and for any purpose, but today many\nare used in dangerous environments (including bomb detection and de-activation), manufacturing\nprocesses, or where humans cannot survive. Robots can take on any form, but some are made\nto resemble humans in appearance. This is said to help in the acceptance of a robot in certain\nreplicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting,\nspeech, cognition, and basically anything a human can do.\nQuestion What do robots that resemble humans attempt to do?\nAnswer replicate walking, lifting, speech, cognition\nFree-form from DROP (Dua et al. 2019)\nContext The Miami Dolphins came off of a 0-3 start and tried to rebound against the Buffalo Bills.\nAfter a scoreless ﬁrst quarter the Dolphins rallied quick with a 23-yard interception return for\na touchdown by rookie Vontae Davis and a 1-yard touchdown run by Ronnie Brown along with a\n33-yard ﬁeld goal by Dan Carpenter making the halftime score 17-3. Miami would continue with\na Chad Henne touchdown pass to Brian Hartline and a 1-yard touchdown run by Ricky Williams.\nTrent Edwards would hit Josh Reed for a 3-yard touchdown but Miami ended the game with a\n1-yard touchdown run by Ronnie Brown. The Dolphins won the game 38-10 as the team improved\nto 1-3. Chad Henne made his ﬁrst NFL start and threw for 115 yards and a touchdown.\nQuestion How many more points did the Dolphins score compare to the Bills by the game’s end?\nAnswer 28\nIn the exploration of MRC, constructing a high-quality, large-scale dataset is as\nimportant as optimizing the model structure. Following Chen (2018),5 the existing MRC\n5 We made slight modiﬁcations to adapt to the latest emerging types.\n15\nComputational Linguistics Volume 1, Number 1\nvariations can be roughly divided into four categories, 1) cloze-style; 2) multi-choice; 3)\nspan extraction, and 4) free-form prediction.\n3.4 Typical Datasets\nCloze-style. For cloze-style MRC, the question contains a placeholder and the machine\nmust decide which word or entity is the most suitable option. The standard datasets are\nCNN/Daily Mail (Hermann et al. 2015), Children’s Book Test dataset (CBT) (Hill et al.\n2015), BookTest (Bajgar, Kadlec, and Kleindienst 2016), Who did What (Onishi et al.\n2016), ROCStories (Mostafazadeh et al. 2016), CliCR (Suster and Daelemans 2018).\nMulti-choice. This type of MRC requires the machine to ﬁnd the only correct option in\nthe given candidate choices based on the given passage. The major datasets are MCTest\n(Richardson, Burges, and Renshaw 2013), QA4MRE (Sutcliffe et al. 2013), RACE (Lai\net al. 2017), ARC (Clark et al. 2018), SWAG (Zellers et al. 2018), DREAM (Sun et al.\n2019a), etc.\nSpan Extraction. The answers in this category of MRC are spans extracted from the\ngiven passage texts. The typical benchmark datasets are SQuAD (Rajpurkar et al. 2016),\nTrivialQA (Joshi et al. 2017), SQuAD 2.0 (extractive with unanswerable questions) (Ra-\njpurkar, Jia, and Liang 2018), NewsQA (Trischler et al. 2017), SearchQA (Dunn et al.\n2017), etc.\nFree-form Prediction. The answers in this type are abstractive free-form based on the\nunderstanding of the passage. The forms are diverse, including generated text spans,\nyes/no judgment, counting, and enumeration. For free-form QA, the widely-used\ndatasets are MS MACRO (Bajaj et al. 2016), NarrativeQA (Koˇ cisk`y et al. 2018), Dureader\n(He et al. 2018). This category also includes recent conversational MRC, such as CoQA\n(Reddy, Chen, and Manning 2019) and QuAC (Choi et al. 2018), and discrete reasoning\ntypes involving counting and arithmetic expression as those in DROP (Dua et al. 2019),\netc.\nExcept for the variety of formats, the datasets also differ from 1) context styles, e.g.,\nsingle paragraph, multiple paragraphs, long document, and conversation history; 2)\nquestion types, e.g., open natural question, cloze-style ﬁll-in-blank, and search queries;\n3) answer forms, e.g., entity, phrase, choice, and free-form texts; 4) domains, e.g.,\nWikipedia articles, news, examinations, clinical, movie scripts, and scientiﬁc texts; 5)\nspeciﬁc skill objectives, e.g., unanswerable question veriﬁcation, multi-turn conversa-\ntion, multi-hop reasoning, mathematical prediction, commonsense reasoning, corefer-\nence resolution. A detailed comparison of the existing dataset is listed in Appendix §7.\n3.5 Evaluation Metrics\nFor cloze-style and multi-choice MRC, the common evaluation metric is accuracy. For\nspan-based QA, the widely-used metrics are Exact match (EM) and (Macro-averaged)\nF1 score. EM measures the ratio of predictions that match any one of the ground truth\nanswers exactly. F1 score measures the average overlap between the prediction and\nground truth answers. For non-extractive forms, such as generative QA, answers are\nnot limited to the original context, so ROUGE-L (Lin 2004) and BLEU (Papineni et al.\n2002) are also further adopted for evaluation.\n16\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\n3.6 Towards Prosperous MRC\nMost recent MRC test evaluations are based on an online server, which requires to sub-\nmit the model to assess the performance on the hidden test sets. Ofﬁcial leaderboards\nare also available for easy comparison of submissions. A typical example is SQuAD. 6\nOpen and easy following stimulate the prosperity of MRC studies, which can provide\na great precedent for other NLP tasks. We think the success of the MRC task can be\nsummarized as follows:\n• Computable Deﬁnition : due to the vagueness and complexity of natural lan-\nguage, on the one hand, a clear and computable deﬁnition is essential (e.g., cloze-\nstyle, multi-choice, span-based, etc.);\n• Convincing Benchmarks : to promote the progress of any application, technol-\nogy, open, and comparable assessments are indispensable, including convincing\nevaluation metrics (e.g., EM and F1), and evaluation platforms (e.g., leaderboards,\nautomatic online evaluations).\nThe deﬁnition of a task is closely related to the automatic evaluation. Without\ncomputable deﬁnitions, there will be no credible evaluation.\n3.7 Related Surveys\nPrevious survey papers (Zhang et al. 2019b; Qiu et al. 2019a; Liu et al. 2019b) mainly\noutlined the existing corpus and models for MRC. Our survey differs from previous\nsurveys in several aspects:\n• Our work goes much deeper to provide a comprehensive and comparative review\nwith an in-depth explanation over the origin and the development of MRC in the\nbroader view of the NLP scenario, paying special focus on the role of CLMs. We\nconclude that MRC boosts the progress from language processing to understand-\ning, and the theme of MRC is gradually moving from shallow text matching to\ncognitive reasoning.\n• For the technique side, we propose new taxonomies of the architecture of MRC,\nby formulating MRC systems as two-stage architecture motivated by cognition\npsychology and provide a comprehensive discussion of technical methods. We\nsummarize the technical methods and highlights in different stages of MRC de-\nvelopment. We show that the rapid improvement of MRC systems greatly beneﬁts\nfrom the progress of CLMs.\n• Besides a wide coverage of topics in MRC researches through investigating typical\nmodels and trends from MRC leaderboards, our own empirical analysis is also\nprovided. A variety of newly emerged topics, e.g., interpretation of models and\ndatasets, decomposition of prerequisite skills, complex reasoning, low-resource\nMRC, etc., are also discussed in depth. According to our experience, we demon-\nstrate our observations and suggestions for the MRC researches.7\n6 https://rajpurkar.github.io/SQuAD-explorer/.\n7 We are among the pioneers to research neural machine reading comprehension. We pioneered the\nresearch direction of employing linguistic knowledge for building MRC models, including\nmorphological segmentation (Zhang, Huang, and Zhao 2018; Zhang et al. 2019e, 2018b), semantics\ninjection (Zhang et al. 2019d, 2020b), syntactic guidance (Zhang et al. 2020c), and commonsense (Li,\nZhang, and Zhao 2020). Besides the encoder representation, we investigated the decoder part to\nstrengthen the comprehension, including interactive matching (Zhang et al. 2020a; Zhu, Zhao, and Li\n17\nComputational Linguistics Volume 1, Number 1\nWe believe that this survey would help the audience more deeply understand the\ndevelopment and highlights of MRC, as well as the relationship between MRC and the\nbroader NLP community.\n4. Technical Methods\n4.1 Two-stage Solving Architecture\nInspired by dual process theory of cognition psychology (Wason and Evans 1974; Evans\n1984, 2003; Kahneman 2011; Evans 2017; Ding et al. 2019), the cognitive process of\nhuman brains potentially involves two distinct types of procedures: contextualized\nperception ( reading) and analytic cognition ( comprehension), where the former gather\ninformation in an implicit process, then the latter conduct the controlled reasoning and\nexecute goals. Based on the above theoretical basis, in the view of architecture design, a\nstandard reading system (reader) which solves MRC problem generally consists of two\nmodules or building steps:\n1) building a CLM as Encoder;\n2) designing ingenious mechanisms as Decoder according to task characteristics.\nEncoder DecoderRepresentation...\n...\n...\n...\nInput Output\nFigure 7: Encoder-Decoder Solving Architecture.\nWe ﬁnd that the generic architecture of MRC system can thus be minimized as the\nformulation as two-stage solving architecture in the perspective of Encoder-Decoder\narchitecture (Sutskever, Vinyals, and Le 2014).8 General Encoder is to encoder the inputs\nas contextualized vectors, and Decoder is speciﬁc to the detailed task. Figure 7 shows\nthe architecture.\n4.2 Typical MRC Architecture\nHere we introduce two typical MRC architectures following the above Encoder-Decoder\nframework, 1) traditional RNN-based BiDAF and 2) CLM-powered BERT.\n4.2.1 Traditional RNN-based BiDAF. Before the invention of CLMs, early studies\nwidely adopted RNNs as feature encoders for sequences, among which GRU was the\nmost popular due to the fast and effective performance. The input parts, e.g., passage\nand question, are fed to the encoder separately. Then, the encoded sequences are passed\n2020), answer veriﬁcation (Zhang, Yang, and Zhao 2020), and semantic reasoning (Zhang, Zhao, and\nZhou 2020). Our researches cover the main topics of MRC. The approaches enable effective and\ninterpretable solutions for real-world applications, such as question answering (Zhang and Zhao 2018),\ndialogue and interactive systems (Zhang et al. 2018c; Zhu et al. 2018b; Zhang, Huang, and Zhao 2019).\nWe also won various ﬁrst places in major MRC shared tasks and leaderboards, including CMRC-2017,\nSQuAD 2.0, RACE, SNLI, and DREAM.\n8 We ﬁnd that most NLP systems can be formed as such architecture.\n18\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nto attention layers for matching interaction between passage and questions before\npredicting the answers. The typical MRC model is BiDAF, which is composed of four\nmain layers: 1) encoding layer that transforms texts into a joint representation of the\nword and character embeddings; 2) contextual encoding that employs BiGRUs to obtain\ncontextualized sentence-level representation;9; 3) attention layer to model the semantic\ninteractions between passage and question; 4) answer prediction layer to produce the\nanswer. The ﬁrst two layers are the counterpart of Encoder, and the last two layers serve\nthe role of Decoder.\n4.2.2 Pre-trained CLMs for Fine-tuning. When using CLMs, the input passage and\nquestion are concatenated as a long sequence to feed CLMs, which merges the encoding\nand interaction process in RNN-based MRC models. Therefore, the general encoder has\nbeen well formalized as CLMs, appended with a simple task-speciﬁc linear layer as\nDecoder to predict the answer.\n4.3 Encoder\nThe encoder part plays the role of vectorizing the natural language texts into latent\nspace and further models the contextualized features of the whole sequence.\n4.3.1 Multiple Granularity Features.\nLanguage Units. Utilizing ﬁne-grained features of words was one of the hot topics in\nprevious studies. To solve the out-of-vocabulary (OOV) problem, character-level em-\nbedding was once a common unit besides word embeddings (Seo et al. 2017; Yang et al.\n2017a; Dhingra et al. 2017; Zhang et al. 2018b; Zhang, Huang, and Zhao 2019). However,\ncharacter is not the natural minimum linguistic unit, which makes it quite valuable to\nexplore the potential unit (subword) between character and word to model sub-word\nmorphologies or lexical semantics. To take advantage of both word-level and character\nrepresentations, subword-level representations for MRC were also investigated (Zhang,\nHuang, and Zhao 2018; Zhang et al. 2019e). In Zhang, Huang, and Zhao (2018), we\npropose BPE-based subword segmentation to alleviate OOV issues, and further adopt\na frequency-based ﬁltering method to strengthen the training of low-frequency words.\nDue to the highly ﬂexible grained representation between character and word, subword\nas a basic and effective language modeling unit has been widely used for recent domi-\nnant models (Devlin et al. 2018).\nSalient Features. Linguistic features, such as part-of-speech (POS) and named entity\n(NE) tags, are widely used for enriching the word embedding (Liu et al. 2018). Some\nsemantic features like semantic role labeling (SRL) tags and syntactic structures also\nshow effectiveness for language understanding tasks like MRC (Zhang et al. 2020b,c).\nBesides, the indicator feature, like the binary Exact Match (EM) feature is also simple\nand effective indications, which measures whether a context word is in the question\n(Chen et al. 2019).\n9 Note that BiDAF has the completely contextualized encoding module. Except for the speciﬁc module\nimplementation, the major difference with CLMs is that the BIDAF encoder is not pre-trained.\n19\nComputational Linguistics Volume 1, Number 1\n4.3.2 Structured Knowledge Injection. Incorporating human knowledge into neu-\nral models is one of the primary research interests of artiﬁcial intelligence. Recent\nTransformer-based deep contextual language representation models have been widely\nused for learning universal language representations from large amounts of unlabeled\ndata, achieving dominant results in a series of NLU benchmarks (Peters et al. 2018;\nRadford et al. 2018; Devlin et al. 2018; Yang et al. 2019c; Liu et al. 2019c; Lan et al.\n2019). However, they only learn from plain context-sensitive features such as character\nor word embeddings, with little consideration of explicit hierarchical structures that\nexhibited in human languages, which can provide rich dependency hints for language\nrepresentation. Recent studies show that modeling structured knowledge has shown\nbeneﬁcial for language encoding, which can be categorized into Linguistic Knowledge\nand Commonsense.\nLinguistic Knowledge. Language linguistics is the product of human intelligence, com-\nprehensive modeling of syntax, semantics, and grammar is essential to provide effective\nstructured information for effective language modeling and understanding (Zhang et al.\n2020b,c, 2019d; Zhou, Zhang, and Zhao 2019).\nCommonsense. At present, reading comprehension is still based on shallow segment\nextraction, semantic matching in limited text, and lack of modeling representation\nof commonsense knowledge. Human beings have learned commonsense through the\naccumulation of knowledge over many years. In the eyes of human beings, it is straight-\nforward that “the sun rises in the east and sets in the west\", but it is challenging to\nlearn by machine. Commonsense tasks and datasets were proposed to facilitate the\nresearch, such as ROCStories (Mostafazadeh et al. 2016), SWAG (Zellers et al. 2018),\nCommonsenseQA (Talmor et al. 2019), ReCoRD (Zhang et al. 2018a), and Cosmos QA\n(Huang et al. 2019). Several commonsense knowledge graphs are available as the prior\nknowledge sources, including ConceptNet (Speer, Chin, and Havasi 2017), WebChild\n(Tandon, De Melo, and Weikum 2017) and ATOMIC (Sap et al. 2019). It is an important\nresearch topic to let machines learn and understand human commonsense effectively to\nbe used in induction, reasoning, planning, and prediction.\n4.3.3 Contextualized Sentence Representation. Previously, RNNs, such as LSTM, and\nGRU were seen as the best choice in sequence modeling or language models. However,\nthe recurrent architectures have a fatal ﬂaw, which is hard to parallel in the training pro-\ncess, limiting the computational efﬁciency. Vaswani et al. (2017) proposed Transformer,\nbased entirely on self-attention rather than RNN or Convolution. Transformer can not\nonly achieve parallel calculations but also capture the semantic correlation of any span.\nTherefore, more and more language models tend to choose it to be the feature extractor.\nPre-trained on a large-scale textual corpus, these CLMs well serve as the powerful\nencoders for capturing contextualized sentence representation.\n4.4 Decoder\nAfter encoding the input sequences, the decoder part is used for solving the task\nwith the contextualized sequence representation, which is speciﬁc to the detailed task\nrequirements. For example, the decoder is required to select a proper question for multi-\nchoice MRC or predict an answer span for span-based MRC.\nNot until recently keep the primary focuses of nearly all MRC systems on the\nencoder side, i.e., the deep pre-trained models (Devlin et al. 2018), as the systems may\n20\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nP Q\nMp_q\nP A\nMp_a\nC\nP Q\nMp_q\nQ A\nMq_a\nC\n[1] [Mp_q, Mp_a] [2] [Mp_q, Mq_a]\nPQ A\nMpq_a\nC\nQ A\nMq_a\nP Q\nMp_q\nP A\nMp_a\nC\nPA Q\nMpa_q\nC\n[3] [Mpq_a] [4] [Mp_q, Mp_a, Mq_a]\n[5] [Mpa_q]\nP QA\nMp_qa\nC\n[6] [Mp_qa]\nP A\nMp_a\nQ A\nMq_a\nC\n[1] [Mp_a, Mq_a]\nP A\nHp Ha\nW\nHp\nSp_a\nMp_a\n(b) Matching Attention Alternatives: \nGated Attention, \nBiDAF Attention, \nAttention over Attention, \nMulti-head Attention, etc.\n(a) sequence-aware interaction patterns\n+\n++\n+\nFigure 8: Designs of matching network.\nsimply and straightforwardly beneﬁt from a strong enough encoder. Meanwhile, little\nattention is paid to the decoder side of MRC models (Hu et al. 2019c; Back et al. 2020),\nthough it has been shown that better decoder or better manner of using encoder still has\na signiﬁcant impact on MRC performance, no matter how strong the encoder (i.e., the\nadopted pre-trained CLM) it is (Zhang et al. 2020a). In this part, we discuss the decoder\ndesign in three aspects: 1) matching network; 2) answer pointer, 2) answer veriﬁer, and 3)\nanswer type predictor.\n4.4.1 Matching Network. The early trend is a variety of attention-based interactions\nbetween passage and question, including: Attention Sum (Kadlec et al. 2016), Gated\nAttention (Dhingra et al. 2017), Self-matching (Wang et al. 2017), BiDAF Attention (Seo\net al. 2017), Attention over Attention (Cui et al. 2017), and Co-match Attention (Wang\net al. 2018a).\nSome work is also investigating the attention-based interactions of passage and\nquestion in the era of Transformer-based backbones, such as dual co-match attention\n(Zhang et al. 2020a; Zhu, Zhao, and Li 2020). Figure 8 presents the exhaustive patterns of\nmatching considering three possible sequences: passage ( P), question (Q), and answer\n21\nComputational Linguistics Volume 1, Number 1\nEncoder\nVerifier\nDecoder\n[c] Encoder-(Decoder+I-FV)\n[b] (Encoder+E-FV)-Decoder\n[a] Encoder+Decoder\nEncoder Decoder Encoder\nDecoder\nVerifier Sketchy  Reading\nIntensive Reading\nEncoder\nI-FV\nE-FV\n[e] (Encoder+FV)+FV-(Decoder+RV)\nDecoder\n(R-V)\nSketchy\nIntensive\nDecoder\n[d] Sketchy and Internsive Reading\nFigure 9: Designs of answer veriﬁer.\ncandidate option (A).10 The sequences, P, Qor A, can be concatenated together as one,\nfor example, PQ denotes the concatenation of P and Q. M is deﬁned as the matching\noperation. For example, Mp_a models the matching between the hidden states of P\nand A. We depict the simple but widely-used matching attention M in Figure 8-(b)\nfor example, whose formulation is further described in §5.6.3 for detailed reference.\nHowever, the study of the matching mechanisms has come to a bottleneck facing the\nalready powerful CLM encoders, which are essentially interactive to model paired\nsequences.\n4.4.2 Answer Pointer. Span prediction is one of the major focuses of MRC tasks. Most\nmodels predict the answer by generating the start position and the end position corre-\nsponding to the estimated answer span. Pointer network (Vinyals, Fortunato, and Jaitly\n2015) was used in early MRC models (Wang and Jiang 2016; Wang et al. 2017).\nFor training the model to predict the answer span for an MRC task, standard\nmaximum-likelihood method is used for predicting exactly-matched (EM) start and end\npositions for an answer span. It is a strict objective that encourages exact answers at the\ncost of penalizing nearby or overlapping answers that are sometimes equally accurate.\nTo alleviate the issue and predict more acceptable answers, reinforcement learning\nalgorithm based self-critical policy learning was adopted to measure the reward as word\noverlap between the predicted answer and the ground truth, so as to optimize towards\nthe F1 metric instead of EM metric for span-based MRC (Xiong, Zhong, and Socher\n2018; Hu et al. 2018).\n4.4.3 Answer Veriﬁer. For the concerned MRC challenge with unanswerable questions,\na reader has to handle two aspects carefully: 1) give the accurate answers for answerable\nquestions; 2) effectively distinguish the unanswerable questions, and then refuse to\nanswer. Such requirements complicate the reader’s design by introducing an extra\nveriﬁer module or answer-veriﬁcation mechanism. Figure 9 shows the possible designs\nof the veriﬁers. The variants are mainly three folds (the formulations are elaborated in\n§5.6):\n1) Threshold-based answerable veriﬁcation (TAV). The veriﬁcation mechanism can\nbe simpliﬁed an answerable threshold over predicted span probability that is broadly\n10 Though many well-known matching methods only involve passage and question as for cloze-style and\nspan-based MRC, we present a more general demonstration by also considering multi-choice types that\nhave three types of input, and the former types are also included as counterparts.\n22\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nTable 6: Loss functions for MRC. CE: categorical crossentropy, BCE: binary crossentropy,\nMSE: mean squared error.\nType CE BCE MSE\nCloze-style \u0013\nSpan-based \u0013\n+ (binary) veriﬁcation \u0013 \u0013 \u0013\n+ yes/no \u0013 \u0013 \u0013\n+ count \u0013\nMulti-choice \u0013\nused by powerful enough CLMs for quickly building readers (Devlin et al. 2018; Zhang\net al. 2020b).\n2) Multitask-style veriﬁcation (Intensive). Mostly, for module design, the answer\nspan prediction and answer veriﬁcation are trained jointly with multitask learning\n(Figure 9(c)). Liu et al. (2018) appended an empty word token to the context and added a\nsimple classiﬁcation layer to the reader. Hu et al. (2019c) used two types of auxiliary loss,\nindependent span loss to predict plausible answers and independent no-answer loss to\ndecide the answerability of the question. Further, an extra veriﬁer is adopted to decide\nwhether the predicted answer is entailed by the input snippets (Figure 9(b)). Back et al.\n(2020) developed an attention-based satisfaction score to compare question embeddings\nwith the candidate answer embeddings. It allows explaining why a question is classiﬁed\nas unanswerable by showing unmet conditions within the question (Figure 9(c)). Zhang\net al. (2020c) proposed a linear veriﬁer layer to context embedding weighted by start and\nend distribution over the context words representations concatenated to special pooled\n[CLS] token representation for BERT (Figure 9(c)).\n3) External parallel veriﬁcation (Sketchy). Zhang, Yang, and Zhao (2020) proposed a\nRetro-Reader that integrates two stages of reading and veriﬁcation strategies: 1) sketchy\nreading that brieﬂy touches the relationship of passage and question, and yields an\ninitial judgment; 2) intensive reading that veriﬁes the answer and gives the ﬁnal predic-\ntion (Figure 9(d)). In the implementation, the model is structured as a rear veriﬁcation\n(RV) method that combines the multitask-style veriﬁcation as internal veriﬁcation (IV),\nand external veriﬁcation (EV) from a parallel module trained only for answerability\ndecision, which is both simple and practicable with basically the same performance,\nwhich results in a parallel reading module design at last as the model shown in Figure\n9(e).\n4.4.4 Answer Type Predictor. Most of the neural reading models (Seo et al. 2017; Wang\net al. 2017; Yu et al. 2018) are usually designed to extract a continuous span of text as the\nanswer. For more open and realistic scenarios, where answers are involved with various\ntypes, such as numbers, dates, or text strings, several pre-deﬁned modules are used to\nhandle different kinds of answers (Dua et al. 2019; Gupta et al. 2019; Hu et al. 2019a).\n4.5 Training Objectives\nTable 6 shows the training objectives for different types of MRC. The widely-used\nobjective function is cross-entropy. For some speciﬁc types, such as binary answer\nveriﬁcation, categorical crossentropy, binary crossentropy, and mean squared error are\n23\nComputational Linguistics Volume 1, Number 1\nTable 7: Typical MRC models for comparison of Encoders on SQuAD 1.1 leaderboard.\nTRFM is short for Transformer. Although MRC models often employ ensembles for\nbetter performance, the results are based single models to avoid extra inﬂuence in\nensemble models. * QANet and BERT used back translation and TriviaQA dataset\n(Joshi et al. 2017) for further data augmentation, respectively. The improvements ↑are\ncalculated based on the result (italic) on Match-LSTM.\nModels Encoder EM F1 ↑EM ↑F1\nHuman (Rajpurkar, Jia, and Liang 2018) - 82.304 91.221 - -\nMatch-LSTM (Wang and Jiang 2016) RNN 64.744 73.743 - -\nDCN (Xiong, Zhong, and Socher 2016) RNN 66.233 75.896 1.489 2.153\nBi-DAF (Seo et al. 2017) RNN 67.974 77.323 3.230 3.580\nMnemonic Reader (Hu, Peng, and Qiu 2017) RNN 70.995 80.146 6.251 6.403\nDocument Reader (Chen et al. 2017) RNN 70.733 79.353 5.989 5.610\nDCN+ (Xiong, Zhong, and Socher 2017) RNN 75.087 83.081 10.343 9.338\nr-net (Wang et al. 2017) RNN 76.461 84.265 11.717 10.522\nMEMEN (Pan et al. 2017) RNN 78.234 85.344 13.490 11.601\nQANet (Yu et al. 2018)* TRFM 80.929 87.773 16.185 14.030\nCLMs\nELMo (Peters et al. 2018) RNN 78.580 85.833 13.836 12.090\nBERT (Devlin et al. 2018)* TRFM 85.083 91.835 20.339 18.092\nSpanBERT (Joshi et al. 2020) TRFM 88.839 94.635 24.095 20.892\nXLNet (Yang et al. 2019c) TRFM-XL 89.898 95.080 25.154 21.337\nTable 8: Typical MRC models for comparison of Encoders on SQuAD 2.0 and RACE\nleaderboard. TRFM is short for Transformer. The D-values↑are calculated based on the\nresults (italic) on BERT for SQuAD 2.0 and GTPv1 for RACE.\nModels Encoder SQuAD 2.0 ↑F1 RACE ↑Acc\nHuman (Rajpurkar, Jia, and Liang 2018) - 91.221 - -\nGPTv1 (Radford et al. 2018) TRFM - - 59.0 -\nBERT (Devlin et al. 2018) TRFM 83.061 - 72.0 -\nSemBERT (Zhang et al. 2020b) TRFM 87.864 4.803 - -\nSG-Net (Zhang et al. 2020c) TRFM 87.926 4.865 - -\nRoBERTa (Liu et al. 2019c) TRFM 89.795 6.734 83.2 24.2\nALBERT (Lan et al. 2019) TRFM 90.902 7.841 86.5 27.5\nXLNet (Yang et al. 2019c) TRFM-XL 90.689 7.628 81.8 22.8\nELECTRA (Clark et al. 2019c) TRFM 91.365 8.304 - -\nalso investigated (Zhang, Yang, and Zhao 2020). Similarly, for tasks involve yes or\nno answers, the three alternative functions are also available. For counting, previous\nresearches tend to model it as multi-class classiﬁcation task using crossentropy (Dua\net al. 2019; Hu et al. 2019a; Ran et al. 2019b).\n24\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\n5. Technical Highlights\nIn this part, we summarize the previous and recent dominant techniques by reviewing\nthe systems for the ﬂagship datasets concerning the main types of MRC, cloze-type\nCNN/DailyMail (Hermann et al. 2015), multi-choice RACE (Lai et al. 2017), and span ex-\ntraction SQuAD (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang 2018). Tables 7,8,9,10,11\nshow the statistics, from which we summarize the following observations and thoughts\n(we will elaborate the details in the subsequent sections):\n1) CLMs greatly boost the benchmark of current MRC . Deeper, wider encoders\ncarrying large-scale knowledge become a new major theme. The upper bound of the\nencoding capacity of deep neural networks has not been reached yet; however, training\nsuch CLMs are very time-consuming and computationally expensive. Light and reﬁned\nCLMs would be more friendly for real-world and common usage, which can be realized\nby designing more ingenious models and learning strategies (Lan et al. 2019), as well as\nknowledge distillation (Jiao et al. 2019; Sanh et al. 2019).\n2) Recent years witness a decline of matching networks . Early years witnessed a\nproliferation of attention-based mechanisms to improve the interaction and matching\ninformation between passage and questions, which work well with RNN encoders.\nAfter the popularity of CLMs, the advantage disappeared. Intuitively, the reason might\nbe that CLMs are interaction-based models (e.g., taking paired sequences as input to\nmodel the interactions), but not good feature extractors. This difference might be the\npre-training genre of CLMs, and also potentially due to the transformer architecture.\nIt is also inspiring that it promotes a transformation from shallow text matching into a\nmore complex knowledge understanding of MRC researches to some extent.\n3) Besides the encoding sides, optimizing the decoder modules is also essential\nfor more accurate answers. Especially for SQuAD2.0 that requires the model to decide\nif a question is answerable, training a separate veriﬁer or multitasking with veriﬁcation\nloss generally works.11\n4) Data augmentation from similar MRC datasets sometimes works. Besides some\nwork reported using TraiviaQA (Joshi et al. 2017) or NewsQA (Joshi et al. 2017) datasets\nas extra training data, there were also many submissions whose names contain terms\nabout data augmentation. Similarly, when it comes to the CLMs realm, there is rarely\nwork that uses augmentation. Besides, the pre-training of CLMs can also be regarded as\ndata augmentation, which is highly potential for the performance gains.\nIn the following part, we will elaborate on the major highlights of the previous\nwork. We also conduct a series of empirical studies to assess simple tactic optimizations\nas a reference for interested readers (§5.6).\n5.1 Reading Strategy\nInsights on the solutions to MRC challenges can be drawn from the cognitive process of\nhumans. Therefore, some interesting reading strategies are proposed based on human\nreading patterns, such as Learning to Skim Text (Yu, Lee, and Le 2017), learning to stop\nreading (Shen et al. 2017), and our proposed retrospective reading (Zhang, Yang, and\nZhao 2020). Also, (Sun et al. 2019b) proposed three general strategies: back and forth\nreading, highlighting, and self-assessment to improve non-extractive MRC.\n11 We notice that jointly multitasking veriﬁcation loss and answer span loss has been integrated as a\nstandard module in the released codes in XLNet and ELECTRA for SQuAD2.0.\n25\nComputational Linguistics Volume 1, Number 1\nTable 9: The contributions of CLMs. * indicates results that depend on additional exter-\nnal training data. †indicate the result is from Yang et al. (2019c) as it was not reported in\nthe original paper (Devlin et al. 2018). Since the ﬁnal results were reported by the largest\nmodels, we listed the largemodels for XLNet, BERT, RoBERTa, ELECTRA, andxxlarge\nmodel for ALBERT. GPT is reported as the v1 version.\nMethod Tokens Size Params SQuAD1.1 SQuAD2.0 RACEDev Test Dev Test\nELMo 800M - 93.6M 85.6 85.8 - - -\nGPTv1 985M - 85M - - - - 59.0\nXLNetlarge 33B - 360M 94.5 95.1* 88.8 89.1* 81.8\nBERTlarge 3.3B 13GB 340M 91.1 91.8* 81.9 83.0 72.0 †\nRoBERTalarge - 160GB 355M 94.6 - 89.4 89.8 83.2\nALBERTxxlarge - 157GB 235M 94.8 - 90.2 90.9 86.5\nELECTRAlarge 33B - 335M 94.9 - 90.6 91.4 -\n50\n60\n70\n80\n90\n100\n1e+6\n1e+7\n1e+8\n1e+9\n1e+10\n1e+11\nELMo GPT1.0 BERT XLNet RoBERTa ALBERT ELECTRA\nTokens Params SQuAD1.1 SQuAD2.0 RACE\nTokens SQuAD2.0 RACE\nFigure 10: The contribution of the sizes of pre-trained corpus and CLMs. The right axis is\nthe main metric for the statistics. The numbers of tokens and parameters are normalized\nby log106 (x) + 50where xdenotes the original number. The left axis corresponds to the\noriginal values of tokens and parameters for easy reference.\n5.2 CLMs Become Dominant\nAs shown in Table 9, CLMs improve the MRC benchmarks to a much higher stage.\nBesides the contextualized sentence-level representation, the advance of CLMs is also\nrelated to the much larger model size and large-scale pre-training corpus. From Table 9\nand the further illustration in Figure 10, we see that both the model sizes and the scale\nof training data are increasing remarkably, that contribute the downstream MRC model\nperformance.12\n12 The inﬂuence of model parameters can also be easily veriﬁed at the SNLI leaderboard:\nhttps://nlp.stanford.edu/projects/snli/.\n26\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nTable 10: Typical MRC models for comparisons of decoding designs on multi-choice\nRACE test sets. The matching patterns correspond to those notations in Figure 8. M:\nRACE-M, H: RACE-H. M, H, RACE are the accuracy on two subsets and the overall test\nsets, respectively.\nModel Matching M H RACE\nHuman Ceiling Performance (Lai et al. 2017) 95.4 94.2 94.5\nAmazon Mechanical Turker (Lai et al. 2017) 85.1 69.4 73.3\nHAF (Zhu et al. 2018a) [MP_A; MP_Q; MQ_A] 45.0 46.4 46.0\nMRU (Tay, Tuan, and Hui 2018) [MP_Q_A] 57.7 47.4 50.4\nHCM (Wang et al. 2018a) [MP_Q; MP_A] 55.8 48.2 50.4\nMMN (Tang, Cai, and Zhuo 2019) [MQ_A; MA_Q; MP_Q; MP_A] 61.1 52.2 54.7\nGPT (Radford et al. 2018) [MP_Q_A] 62.9 57.4 59.0\nRSM (Sun et al. 2019b) [MP_QA] 69.2 61.5 63.8\nDCMN (Zhang et al. 2019a) [MPQ_A] 77.6 70.1 72.3\nOCN (Ran et al. 2019a) [MP_Q_A] 76.7 69.6 71.7\nBERTlarge (Pan et al. 2019b) [MP_Q_A] 76.6 70.1 72.0\nXLNet (Yang et al. 2019c) [MP_Q_A] 85.5 80.2 81.8\n+ DCMN+ (Zhang et al. 2020a) [MP_Q; MP_O; MQ_O] 86.5 81.3 82.8\nRoBERTa (Liu et al. 2019c) [MP_Q_A] 86.5 81.8 83.2\n+ MMM (Jin et al. 2019a) [MP_Q_A] 89.1 83.3 85.0\nALBERT (Jin et al. 2019a) [MP_Q_A] 89.0 85.5 86.5\n+ DUMA (Zhu, Zhao, and Li 2020) [MP_QA; MQA_P] 90.9 86.7 88.0\nMegatron-BERT (Shoeybi et al. 2019) [MP_Q_A] 91.8 88.6 89.5\n5.3 Data Augmentation\nSince most high-quality MRC datasets are human-annotated and inevitably relatively\nsmall, another simple method to boost performance is data augmentation. Early ef-\nfective data augmentation is to inject extra similar MRC data for training a speciﬁc\nmodel. Recently, using CLMs, which pre-trained on large-scale unlabeled corpora, can\nbe regarded as a kind of data augmentation as well.\nTraining Data Augmentation. There are various methods to provide extra data to train a\nmore powerful MRC model, including: 1) Combining various MRC datasets as training\ndata augmentation (TDA) (Yang et al. 2019a,b); 2) Multi-tasking (Xu et al. 2018; Fisch\net al. 2019); 3) Automatic question generation, such as back translation (Yu et al. 2018)\nand synthetic generation (Du, Shao, and Cardie 2017; Du and Cardie 2017; Kim et al.\n2019; Zhu et al. 2019; Alberti et al. 2019). However, we ﬁnd the gains become small when\nusing CLMs, which might already contain the most common and important knowledge\nbetween different datasets.\nLarge-scale Pre-training. Recent studies showed that CLMs well acquired linguistic in-\nformation through pre-training (Clark et al. 2019b; Ettinger 2020) (more discussions in\nSection §6.1), which is potential to the impressive results on MRC tasks.\n27\nComputational Linguistics Volume 1, Number 1\nTable 11: Results on cloze CNN /DailyMail test sets. UA: unidirectional attention. BA:\nbidirectional attention. The statistics are from Seo et al. (2017).\nMethod Att. Type CNN DailyMail\nval test val test\nAttentive Reader (Hermann et al. 2015) UA 61.6 63.0 70.5 69.0\nAS Reader (Kadlec et al. 2016) UA 68.6 69.5 75.0 73.9\nIterative Attention (Sordoni et al. 2016) UA 72.6 73.3 - -\nStanford AR (Chen, Bolton, and Manning 2016) UA 73.8 73.6 77.6 76.6\nGAReader (Dhingra et al. 2017) UA 73.0 73.8 76.7 75.7\nAoA Reader (Cui et al. 2017) BA 73.1 74.4 - -\nBiDAF (Seo et al. 2017) BA 76.3 76.9 80.3 79.6\n5.4 Decline of Matching Attention\nAs the results shown in Tables 10-11, it is easy to notice that the attention mechanism is\nthe key component in previous RNN-based MRC systems.13\nWe see that bidirectional attention (BA) works better than unidirectional one, and\nco-attention is a superior matching method, which indicate the advance of more rounds\nof matching that would be effective at capturing more ﬁne-grained information intu-\nitively. When using CLMs as the encoder, we observe that the explicit passage and\nquestion attention could only show quite marginal, or even degradation of perfor-\nmance. The reason might be that CLMs are interaction-based matching models (Qiao\net al. 2019) when taking the whole concatenated sequences of passage and question.\nIt is not suggested to be employed as a representative model. Bao et al. (2019) also\nreported similar observations, showing that the uniﬁed modeling of sequences in BERT\noutperforms previous networks that separately treat encoding and matching.\nAfter contextualized encoder by the CLMs, the major connections for reading com-\nprehension might have been well modeled, and the vital information is aggregated to\nthe representations of special tokens, such as [CLS] and [SEP] for BERT. We ﬁnd that\nthe above encoding process of CLMs is quite different from that in traditional RNNs,\nwhere the hidden states of each token are passed successively in one direction, without\nmass aggregation and degradation of representations.14 The phenomenon may explain\nwhy interactive attentions between input sequences work well with RNN-based feature\nextractors but show no obvious advantage in the realm of CLMs.\n5.5 Tactic Optimization\nThe objective of answer veriﬁcation.For answer veriﬁcation, modeling the objective as clas-\nsiﬁcation or regression would have a slight inﬂuence on the ﬁnal results. However, the\nadvance might vary based on the backbone network, as some work took the regression\nloss due to the better performance (Yang et al. 2019c), while the recent work reported\nthat the classiﬁcation would be better in some cases (Zhang, Yang, and Zhao 2020).\n13 We roughly summarize the matching methods in the previous work using our model notations, which\nmeet their general ideas except some calculation details.\n14 Although the last hidden state is usually used for the overall representation, the other states may not\nsuffer from degradation like in multi-head attention-based deep CLMs.\n28\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nThe dependency inside answer span.Recent CLM-based models simpliﬁed the span predic-\ntion part as independent classiﬁcation objectives. However, the end position is related\nto the start predictions. As a common method in early works (Seo et al. 2017), jointly\nintegrating the start logits and the sequence hidden states to obtain the end logits is\npotential for further enhancement. Another neglected aspect recently is the dependence\nof all the tokens inside an answer span, instead of considering only the start and end\npositions.\nRe-ranking of candidate answers. Answer reranking is adapted to mimic the process of\ndouble-checking. A simple strategy is to use N-best reranking strategy after generating\nanswers from neural networks (Cui et al. 2017; Wang et al. 2018b,c,d; Hu et al. 2019b).\nUnlike previous work that ranks candidate answers, Hu et al. (2019a) proposed an arith-\nmetic expression reranking mechanism to rank expression candidates that are decoded\nby beam search, to incorporate their context information during reranking to conﬁrm\nthe prediction further.\n5.6 Empirical Analysis of Decoders\nTo gain insights on how to further improve MRC, we report our attempts to im-\nprove model performance with general and straightforward tactic optimizations for the\nwidely-used SQuAD2.0 dataset that does not rely on the backbone model. The methods\ninclude three types, Veriﬁcation, Interaction, and Answer Dependency.15\n5.6.1 Baseline. We adopt BERTlarge (Devlin et al. 2018) and ALBERT xxlarge (Lan et al.\n2019) as our baselines.\nEncoding. The input sentence is ﬁrst tokenized to word pieces (subword tokens). Let\nT = {t1,...,t L}denote a sequence of subword tokens of length L. For each token,\nthe input embedding is the sum of its token embedding, position embedding, and\ntoken-type embedding. Let X = {x1,...,x L}be the outputs of the encoder, which are\nembedding features of encoding sentence words of lengthL. The input embeddings are\nthen fed into the deep Transformer (Vaswani et al. 2017) layers for learning contextual\nrepresentations. Let Xg = {xg\nL,...,x g\nL}be the features of the g-th layer. The features of\nthe g+ 1-th layer, xg+1 is computed by\n˜hg+1\ni =\nM∑\nm=1\nWg+1\nm\n\n\n\nn∑\nj=1\nAm\ni,j ·Vg+1\nm xg\nj\n\n\n, (8)\nhg+1\ni = LayerNorm(xg\ni + ˜hg+1\ni ), (9)\n˜xg+1\ni = Wg+1\n2 ·GELU(Wg+1\n1 hg+1\ni + bg+1\n1 ) +bg+1\n2 , (10)\nxg+1\ni = LayerNorm(hg+1\ni + ˜xg+1\ni ), (11)\n15 In this part, we intend to intuitively show what kinds of tactic optimizations potentially work, so we brief\nthe details of the methods and report the best results as a reference after hyper-parameter searching. We\nrecommend interested readers to read our technical report (Zhang, Yang, and Zhao 2020) for the details\nof answer veriﬁcation and sequence interactions. Our sources are publicly available at\nhttps://github.com/cooelf/AwesomeMRC.\n29\nComputational Linguistics Volume 1, Number 1\nwhere m is the index of the attention heads, and Am\ni,j ∝exp[(Qg+1\nm xg\ni)⊤(Kg+1\nm xg\nj)] de-\nnotes the attention weights between elementsiand jin the m-th head, which is normal-\nized by ∑N\nj=1 Am\ni,j = 1. Wg+1\nm ,Qg+1\nm ,Kg+1\nm and Vg+1\nm are learnable weights for the m-th\nattention head, Wg+1\n1 ,Wg+1\n2 and bg+1\n1 ,bg+1\n2 are learnable weights and biases, respec-\ntively. Finally, we have last-layer hidden states of the input sequence H = {h1,...,h L}\nas the contextualized representation of the input the sequence.\nDecoding. The aim of span-based MRC is to ﬁnd a span in the passage as answer, thus\nwe employ a linear layer with SoftMax operation and feed H as the input to obtain the\nstart and end probabilities, sand e:\ns,e ∝SoftMax(Linear(H)). (12)\nThreshold based answerable veriﬁcation (TAV). For unanswerable question prediction,\ngiven output start and end probabilities sand e, and the veriﬁcation probability v, we\ncalculate the has-answer score scorehas and the no-answer score scorena:\nscorehas = max(sk1 + ek2 ),1 <k1 ≤k2 ≤L,\nscorena = s1 + e1,\n(13)\nwhere s1 and e1 denote the corresponding logits for the special token[CLS] as in BERT-\nbased models used for answer veriﬁcation (Devlin et al. 2018; Lan et al. 2019). We obtain\na difference score between has-answer score and the no-answer score as ﬁnal score. An\nanswerable threshold δ is set and determined according to the development set. The\nmodel predicts the answer span that gives thehas-answer score if the ﬁnal score is above\nthe threshold δ, and null string otherwise.\nTraining Objective. The training objective of answer span prediction is deﬁned as cross\nentropy loss for the start and end predictions,\nLspan = −1\nN\nN∑\ni\n[log(ps\nys\ni\n) + log(pe\nye\ni\n)], (14)\nwhere ys\ni and ye\ni are respectively ground-truth start and end positions of example i. N\nis the number of examples.\n5.6.2 Veriﬁcation. Answer veriﬁcation is vital for MRC tasks that involve unanswerable\nanswers. We tried to add an external separate classiﬁer model that is the same as\nthe MRC model except for the training objective (E-FV). We weighted the predicted\nveriﬁcation logits and original heuristic no-answer logits to decide whether the question\nis answerable. Besides, we also investigated adding multitasking the original span loss\nwith veriﬁcation loss as an internal front veriﬁer (I-FV). The internal veriﬁcation loss\ncan be a cross-entropy loss (I-FV-CE), binary cross-entropy loss (I-FV-BE), or regression-\nstyle mean square error loss (I-FV-MSE).\n30\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nThe pooled ﬁrst token (special symbol,[CLS]) representation h1 ∈H, as the overall\nrepresentation of the sequence, 16 is passed to a fully connection layer to get classiﬁca-\ntion logits or regression score. Let ˆyi ∝Linear(h1) denote the prediction and yi is the\nanswerability target, the three alternative loss functions are as deﬁned as follows:\n(1) For cross entropy as loss function for the classiﬁcation veriﬁcation:\nˆyi,k = SoftMax(Linear(h1)),\nLans = −1\nN\nN∑\ni=1\nK∑\nk=1\n[yi,klog ˆyi,k] ,\n(15)\nwhere Kis the number of classes. In this work, K = 2.\n(2) For binary cross entropy as loss function for the classiﬁcation veriﬁcation:\nˆyi = Sigmoid(Linear(h1)),\nLans = −1\nN\nN∑\ni=1\n[yilog ˆyi + (1−yi) log(1−ˆyi)] .\n(16)\n(3) For the regression veriﬁcation, mean square error is adopted as its loss function:\nˆyi = Linear(h1),\nLans = 1\nN\nN∑\ni=1\n(yi −ˆyi)2.\n(17)\nDuring training, the joint loss function for FV is the weighted sum of the span loss\nand veriﬁcation loss.\nL = α1Lspan + α2Lans, (18)\nwhere α1 and α2 are weights. We set α1 = α2 = 0.5 for our experiments.\nWe empirically ﬁnd that training with joint loss can yield better results, so we also\nreport the results of 1) summation of all the I-FV losses (All I-FVs: I-FV-CE, I-FV-BE,\nand I-FV-MSE), 2) combination of external and internal veriﬁcation (All I-FVs + E-FV) by\ncalculating the sum (denoted asv) of the logits of E-FV and I-FVs as the ﬁnal answerable\nlogits. In the later scenario, the TAV is rewritten as,\nscorehas = max(sk1 + ek2 ),1 <k1 ≤k2 ≤L,\nscorena = λ1(s1 + e1) +λ2v,\n(19)\nwhere λ1 and λ2 are weights. We set λ1 = λ2 = 0.5.\n16 Following the initial practice of BERT-tyle models, the ﬁrst token (special symbol, [CLS]) representation\nis supposed to be the overall representation of the sequence owing to the pre-training objective.\n31\nComputational Linguistics Volume 1, Number 1\n5.6.3 Interaction. To obtain the representation of each passage and question, we split\nthe last-layer hidden state H into HQ and HP as the representations of the question\nand passage, according to its position information. Both of the sequences are padded\nto the maximum length in a minibatch. Then, we investigate two potential question-\naware matching mechanisms, 1) Transformer-style multi-head attention (MH-ATT), and\n2) traditional dot attention (DT-ATT).\n•Multi-head Attention We feed the HQ and H to a revised one-layer multi-head\nattention layer inspired by Lu et al. (2019). 17 Since the setting is Q = K = V in multi-\nhead self attention, 18 which are all derived from the input sequence, we replace the\ninput to Q with H, and both of K and V with HQ to obtain the question-aware context\nrepresentation H′.\n•Dot Attention Another alternative is to feed HQ and H to a traditional match-\ning attention layer (Wang et al. 2017), by taking the question presentation HQ as the\nattention to the representation HC:\nM = SoftMax(H(WpHQ + bp ⊗eq)T),\nH′= MHQ,\n(20)\nwhere Wq and bq are learnable parameters. eq is a all-ones vector and used to repeat\nthe bias vector into the matrix. M denotes the weights assigned to the different hidden\nstates in the concerned two sequences. H′is the weighted sum of all the hidden states\nand it represents how the vectors in H can be aligned to each hidden state in HQ.\nFinally, the representation H′ is used for the later predictions as described in the\ndecoding and TAVsection above.\n5.6.4 Answer Dependency. Recent studies separately use H to predict the start and end\nspans for the answer, neglecting the dependency of the start and end representations.\nWe model the dependency between start and end logits by concatenating the start logits\nand H through a linear layer to obtain the end logits:\ne= Linear([s; H]), (21)\nwhere [; ]denotes concatenation.\n5.6.5 Findings. Table 12 shows the results. Our observations are as follows:\n• For answer veriﬁcation, either of the front veriﬁers boosts the baselines, and\nintegrating all the veriﬁers can yield even better results.\n• Adding extra interaction layers after the strong CLMs could only yield marginal\nimprovement, which veriﬁes the CLMs’ strong ability to capture the relationships\nbetween passage and question.\n• Answer dependency can effectively improve the exact match score, which can\nintuitively help yield a more exactly matched answer span.\n17 We do not use HP because H achieved better results in our preliminary experiments.\n18 In this work, Q, K, Vcorrespond to the items Qg+1\nm xg\ni , Kg+1\nm xg\nj ) and V g+1\nm xg\nj , respectively.\n32\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nTable 12: Results (%) of different decoder mechanisms on the SQuAD2.0 dev set. Part of\nthe numbers of veriﬁcation and interactions are adapted from our previous work (Zhang,\nYang, and Zhao 2020) (slight update with further hyperparameter tuning).\nMethod BERT ALBERT\nEM F1 EM F1\nBaseline 78.8 81.7 87.0 90.2\nInteraction\n+ MH-ATT 78.8 81.7 87.3 90.3\n+ DT-ATT 78.3 81.4 86.8 90.0\nVeriﬁcation\n+ E-FV 79.1 82.1 87.4 90.6\n+ I-FV-CE 78.6 82.0 87.2 90.3\n+ I-FV-BE 78.8 81.8 87.2 90.2\n+ I-FV-MSE 78.5 81.7 87.3 90.4\n+ All I-FVs 79.4 82.1 87.5 90.6\n+ All I-FVs + E-FV 79.8 82.7 87.7 90.8\nAnswer Dependency\n+ SED 79.1 81.9 87.3 90.3\n6. Trends and Discussions\n6.1 Interpretability of Human-parity Performance\nRecent years witnessed frequent reports of super human-parity results in MRC leader-\nboards, which further stimulated the research interests of investigating what the ‘real’\nability of MRC systems, and what kind of knowledge or reading comprehension skills\nthe systems have grasped. The interpretation appeal to aspects of CLM models, MRC\ndatasets, and models.\nFor CLM models. Since CLM models serve as the basic module for contextualized text\nrepresentation, ﬁngering out what the knowledge captured, especially what linguistic\ncapacities CLMs process confer upon models, is critical for ﬁne-tuning downstream\ntasks, so is for MRC. There are heated discussions about what CLM models learn re-\ncently. Recent work has tried to give the explanation by investigating the attention maps\nfrom the multi-head attention layers (Clark et al. 2019b), and conducting diagnostic\ntests (Ettinger 2020). Clark et al. (2019b) found that attention heads correspond well\nto linguistic notions of syntax and coreference. Ettinger (2020) introduced a suite of\ndiagnostic tests to assess the linguistic competencies of BERT, indicating that BERT\nperforms sensitivity to role reversal and same-category distinctions. Still, it struggles\nwith challenging inferences and role-based event prediction, and it shows obvious\nfailures with the meaning of negation.\nFor MRC datasets and models. So far, the MRC system is still a black box, and it is very\nrisky to use it in many scenarios in which we have to know how and why the answer\nis obtained. It is critical to deeply investigate the explanation of the MRC models or\ndesign an explainable MRC architecture. Although MRC datasets are emerging rapidly\nand the corresponding models continuously show impressive results, it still hard to\n33\nComputational Linguistics Volume 1, Number 1\ninterpret what MRC systems learned, so is the benchmark capacity of the diversity\nof MRC datasets (Sugawara et al. 2018, 2019; Schlegel et al. 2020). The common ar-\nguments are the overestimated ability of MRC systems as MRC models do not nec-\nessarily provide human-level understanding, due to the unprecise benchmarking on\nthe existing datasets. Although there are many models show human-parity scores so\nfar, we cannot say that they successfully perform human-level reading comprehension.\nThe issue mainly lies within the low interpretability of both of the explicit internal\nprocessing of currently prevalent neural models, and what is measured by the datasets.\nMany questions can be answered correctly by the model that do not necessarily require\ngrammatical and complex reasoning. For example, Jia and Liang (2017) and Wallace\net al. (2019) provided manually crafted adversarial examples to show that MRC systems\nare easily distracted. Sugawara et al. (2019) also indicated that most of the questions\nalready answered correctly by the model do not necessarily require grammatical and\ncomplex reasoning. The distractors can not only assess the vulnerability of the current\nmodels but also serve as salient hard negative samples to strengthen model training\n(Gao et al. 2019b).\nBesides, as discussed in our previous work (Zhang, Yang, and Zhao 2020), since\ncurrent results are relatively high in various MRC benchmark datasets, with relatively\nmarginal improvement, it is rarely conﬁrmed that produced results are statistically\nsigniﬁcant than baseline. For the reproducibility of models, it is necessary to conduct\nstatistical tests in evaluating MRC models.\n6.2 Decomposition of Prerequisite Skills\nAs the experience of human examinations, good comprehension requires different di-\nmensions of skills. The potential solution for our researches is to decompose the skills\nrequired by the dataset and take skill-wise evaluations, thus provide more explainable\nand convincing benchmarking of model capacity. Further, it would be beneﬁcial to\nconsider following a standardized format, which can make it simpler to conduct cross-\ndataset evaluations (Fisch et al. 2019), and train a comprehensive model that can work\non different datasets with speciﬁc skills.\nRegarding the corresponding benchmark dataset construction, it is no coincidence\nthat SQuAD datasets turned out a success and have served as the standard benchmark.\nBesides the high quality and speciﬁc focus of the datasets, an online evaluation platform\nthat limits the submission frequency also ensures the convincing assessment. On the\nother hand, it is natural to be cautious for some comprehensive datasets with many\ncomplex question types, which requires many solver modules, as well as processing\ntricks–we should report convincing evaluation with detailed evaluation on separate\ntypes or subtasks, instead of just pursuing overall SOTA results. Unless honestly re-\nporting and unifying the standards of these processing tricks, the evaluation would be\ntroublesome and hard to replicate.\n6.3 Complex Reasoning\nMost of the previous advances have focused onshallow QA tasks that can be tackled very\neffectively by existing retrieval and matching-based techniques. Instead of measuring\nthe comprehension and understanding of the QA systems in question, these tasks test\nmerely the capability of a method to focus attention on speciﬁc words and pieces of\ntext. To better align the progress in the ﬁeld of QA with the expectations that we have\nof human performance and behavior when solving such tasks, a new class of questions,\n34\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\ne.g., “complex\" or “challenge\" reasoning, has been a hot topic. Complex reasoning can\nmost generally be thought of as instances that require intelligent behavior and reasoning\non the part of a machine to solve.\nAs the knowledge, as well as the questions themselves, become more complex\nand specialized, the process of understanding and answering these questions comes to\nresemble human expertise in specialized domains. Current examples of such complex\nreasoning tasks, where humans presently rule the roost, include customer support,\nstandardized testing in education, and domain-speciﬁc consultancy services, such as\nmedical and legal advice. The study of such complex reasoning would be promising for\nmachine intelligence from current perception to next-stage cognition.\nRecent studies have been proposed for such kind of comprehension, including\nmulti-hop QA (Welbl, Stenetorp, and Riedel 2018; Yang et al. 2018) and conversational\nQA (Reddy, Chen, and Manning 2019; Choi et al. 2018). To deal with the complex multi-\nhop relationship, dedicated mechanism design is needed for multi-hop commonsense\nreasoning. Besides, structured knowledge provides a wealth of prior commonsense con-\ntext, which promotes the research on the fusion of multi-hop commonsense knowledge\nbetween symbol and semantic space in recent years (Lin et al. 2019; Ma et al. 2019). For\nconversational QA, modeling multi-turn dependency requires extra memory designs to\ncapture the context information ﬂow and solve the problems precisely and consistently\n(Huang, Choi, and tau Yih 2019).\nRegarding technical side, graph-based neural networks (GNN), including graph\nattention network, graph convolutional network, and graph recurrent network have\nbeen employed for complex reasoning (Song et al. 2018; Qiu et al. 2019b; Chen, Wu, and\nZaki 2019; Jiang et al. 2019; Tu et al. 2019, 2020). The main intuition behind the design\nof GNN based models is to answer questions that require to explore and reason over\nmultiple scattered pieces of evidence, which is similar to human’s interpretable step-\nby-step problem-solving behavior. Another theme appearing frequently in machine\nlearning in general is the revisiting of the existing models and how they perform in a fair\nexperimental setting. Shao et al. (2020) raised a concern that graph structure may not be\nnecessary for multi-hop reasoning, and graph-attention can be considered as a partic-\nular case of self-attention as that used in CLMs. We can already see a transformation\nfrom heuristic applications of GNNs to more sound approaches and discussions about\nthe effectiveness of graph models. For future studies, an in-depth analysis of GNNs, as\nwell as the connections and differences between GNNs and CLMs would be inspiring.\n6.4 Large-scale Comprehension\nMost current MRC systems are based on the hypothesis of given passages as refer-\nence context. However, for real-world MRC applications, the reference passages, even\ndocuments, are always lengthy and detail-riddled. However, recent LM based models\nwork slowly or even unable to process long texts. The ability of knowledge extraction is\nespecially needed for open-domain and free-form QA whose reference texts are usually\nlarge-scale (Guu et al. 2020). A simple solution is to train a model to select the relevant\ninformation pieces by calculating the similarity with the question (Chen et al. 2017;\nClark and Gardner 2018; Htut, Bowman, and Cho 2018; Tan et al. 2018; Wang et al.\n2018c; Zhang, Zhao, and Zhang 2019; Yan et al. 2019; Min et al. 2018; Nishida et al.\n2019). Another technique is to summarize the signiﬁcant information of the reference\ncontext, by taking advantage of text summarization or compression (Li et al. 2019c).\n35\nComputational Linguistics Volume 1, Number 1\n6.5 Low-resource MRC\nLow-resource processing is a hot research topic since most of the natural languages lack\nabundant annotated data (Wang et al. 2019; Zhang et al. 2019c). Since most MRC studies\nare based on the English datasets, there exists a considerable gap for other languages\nthat do not have high-quality MRC datasets. Such a situation can be alleviated by\ntransferring the well-trained English MRC models through domain adaptation (Wang\net al. 2019), and training semi-supervised (Yang et al. 2017b; Zhang and Zhao 2018) or\nmultilingual MRC systems (Liu et al. 2019a; Lee et al. 2019; Cui et al. 2019).\nThe other major drawback exposed in MRC systems is the inadequate knowledge\ntransferability (Talmor and Berant 2019) as they are trained, and even over-ﬁtted on\nspeciﬁc datasets. Since most of the famous datasets are built from Wikipedia articles,\nthe apparent beneﬁts from CLMs might be the same or similar text patterns contained in\nthe training corpus, e.g., context, topics, etc. It remains a signiﬁcant challenge to design\nrobust MRC models that are immune to real noise. It is also essential to build NLP\nsystems that generalize across domains, especially unseen domains (Fisch et al. 2019).\n6.6 Multimodal Semantic Grounding\nCompared with human learning, the current pure text processing model performance\nis relatively weak, because this kind of model only learns the text features, without\nthe perception of the external world, such as visual information. In human learning,\npeople usually understand the world through visual images, auditory sounds, words,\nand other modes. Human brain perceives the world through multimodal semantic\nunderstanding. Therefore, multimodal semantic modeling is closer to human percep-\ntion, which is conducive to a more comprehensive language understanding. It remains\nan open problem when and how to make full use of different modalities to improve\nreading comprehension and inference. A related research topic is visual question an-\nswering (Goyal et al. 2017), which aims to answer questions according to a given image.\nHowever, it is still in the early stage of research as the QA is concerned with only\none image context. As a more practical scenario, jointly modeling diverse modalities\nwill be potential research interests, and beneﬁcial for real-world applications, e.g., E-\ncommerce customer support. For example, given the mixed text, image, and audio\nbackground conversation context, the machine is required to give responses to the\ninquiry accordingly. With the continuous advance of computational power, we believe\nthe joint supervision of auditory, tactile, and visual sensory information together with\nthe language will be crucial for next-stage cognition.\n6.7 Deeper But Efﬁcient Network\nBesides the high-quality benchmark datasets, the increase the computational resources,\ne.g., GPU, enables us to build deeper and wider networks. The last decade witnessed the\ntraditional feature extractor from the RNN to deep transformers, with a larger capacity\nfor contextualized modeling. In the future, we are conﬁdent that much deeper and\nstronger backbone frameworks will be proposed with the rapid development of GPU\ncapacity and further boost the MRC system benchmark performance. In the meantime,\nsmaller and reﬁned systems, potentially through knowledge distillation from large\nmodels, also occupy a certain market, which relies on rapid and accurate reading\ncomprehension solving ability for real-world application.\n36\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nTable 1: Cloze-style MRC datasets.\nName Size Domain Src Feature\nCNN/ DailyMail (Hermann et al. 2015) 1.4M news article A entity cloze\nChildren’s Book Test (Hill et al. 2015) 688K narrative A large-scale automated\nBookTest (Bajgar, Kadlec, and Kleindienst 2016) 14.1M narrative A similar to CBT,\nbut much larger\nWho did What (Onishi et al. 2016) 200K news article A cloze of person name\nROCStories (Mostafazadeh et al. 2016) 50K*5 narrative C Commonsense Stories\nCliCR (Suster and Daelemans 2018) 100K clinical case\ntext A cloze style queries\nTable 2: Multi-choice MRC datasets.\nName Size Domain Src Feature\nQA4MRE (Sutcliffe et al. 2013) 240 technical\ndocument X exam-level questions\nMCTest\n(Richardson, Burges, and Renshaw 2013) 2.6K written\nstory C children-level narrative\nRACE (Lai et al. 2017) 100K language\nexam X middle/high school\nEnglish exam in China\nStory Cloze Test\n(Mostafazadeh et al. 2017) 3.7K written\nstory C 98,159 stories for training\nTextbookQA\n(Kembhavi et al. 2017) 26K textbook X ﬁgures involved\nARCT\n(Habernal et al. 2018) 2.0K debate\narticle C/X reasoning on argument\nCLOTH\n(Xie et al. 2018) 99K various X cloze exam\nMCScript\n(Ostermann et al. 2018) 30K written\nstory C commonsense reasnoing,\nscript knowledge\nARC\n(Clark et al. 2018) 8K scienceexam X retrieved documents\nfrom textbooks\nMultiRC\n(Khashabi et al. 2018) 6K various\ndocuments C multi-sentence reasoning\nSWAG\n(Zellers et al. 2018) 113K video\ncaptions M commonsense reasoning\nOpenbookQA\n(Mihaylov et al. 2018) 6.0K textbook C commonsense reasoning\nRecipeQA\n(Yagcioglu et al. 2018) 36K recipe\nscript A multimodal questions\nCommonsense QA\n(Talmor et al. 2019) 12K ConceptNet C commonsense reasoning\nDREAM\n(Sun et al. 2019a) 10K language\nexam X dialogue-based,\n6.4k multi-party dialogues\nMSCript 2.0\n(Ostermann, Roth, and Pinkal 2019) 20K narrative C commonsense reasoning,\nscript knowledge\nHellaSWAG\n(Zellers et al. 2019) 70K web\nsnippet A commonsense reasoning,\nWikiHow and ActivityNet\nCosmosQA\n(Huang et al. 2019) 36K narrative C commonsense reasoning\nQuAIL\n(Rogers et al. 2020) 15K various C prerequisite real tasks\n37\nComputational Linguistics Volume 1, Number 1\nTable 3: Span-extraction MRC datasets.\nName Ans Size Domain Src Feature\nSQuAD 1.1\n(Rajpurkar et al. 2016) Ex 100K Wikipedia C large-scale crowdsourced\nNewsQA\n(Trischler et al. 2017) Ex 120K news\narticle C blindly created questions\nSearchQA\n(Dunn et al. 2017) Ex 140K web\nsnippet C/X snippets from search engine\nTriviaQA\n(Joshi et al. 2017) Ex 650K web\nsnippet C/X trivia questions\nQuasar\n(Dhingra, Mazaitis, and Cohen 2017) Ex 80K web\nsnippet Q search queries\nAddSent SQuAD\n(Jia and Liang 2017) Ex 3.6K Wikipedia C distracting sentences injected\nQAngaroo\n(Welbl, Stenetorp, and Riedel 2018) Ex 50K Wikipedia,\nMEDLINE A multi-hop reasoning\nDuoRC\n(Saha et al. 2018) Ex 186K movie\nscript C commonsense reasoning,\nmulti-sentence reasoning\nProPara\n(Dalvi et al. 2018) Ex 2K scienceexam A procedural understanding\nMulti-party Dialog\n(Ma, Jurczyk, and Choi 2018) Ex 13K TV show\ntranscript A 1.7k crowdsourced dialogues,\ncloze query\nSQuAD 2.0\n(Rajpurkar, Jia, and Liang 2018) Ex (+NA) 100K Wikipedia C unanswerable questions\nTextworlds QA\n(Labutov et al. 2018) Ex 1.2M generated\ntext A simulated worlds,\nlogical reasoning\nemrQA\n(Pampari et al. 2018) Ex 400K clinical\ndocuments A using annotated logical forms\non i2b2 dataset\nHotpotQA\n(Yang et al. 2018) Ex (+YN) 113K Wikipedia C multi-hop reasoning\nReCoRD\n(Zhang et al. 2018a) Ex 120K news\narticle C commonsense reasoning,\ncloze query\nNatural Questions\n(Kwiatkowski et al. 2019) Ex (+YN) 323K Wikipedia Q/C short/long answer styles\nQuoref\n(Dasigi et al. 2019) Ex 24K Wikipedia C coreference resolution\nTechQA\n(Castelli et al. 2019) Ex (+NA) 1.4K IT support X technical support domain,\ndomain-adaptation\nTable 4: Free-form MRC datasets.\nName Ans Size Domain Src Feature\nbAbI\n(Weston et al. 2015) FF 10K *\n20\ngenerated\ntext A prerequisite toy tasks\nLAMBADA\n(Paperno et al. 2016) FF 10K narrative C hard language modeling\nWikiReading\n(Hewlett et al. 2016) FF 18M Wikipedia A super large-scale dataset\nMS MARCO\n(Bajaj et al. 2016) FF 100K web\nsnippet Q description on web snippets\nNarrativeQA\n(Koˇ cisk`y et al. 2018) FF 45K movie\nscript C summary/full story tasks\nDuReader\n(He et al. 2018) FF 200K web\nsnippet Q/C Chinese,\nBaidu Search/Knows\nQuAC\n(Choi et al. 2018) FF (+YN) 100K Wikipedia C dialogue-based,\n14k dialogs\nShARC\n(Saeidi et al. 2018) YN∗ 32K web\nsnippet C reasoning on rules taken from\ngovernment documents\nCoQA\n(Reddy, Chen, and Manning 2019) FF (+YN) 127K Wikipedia C dialogue-based,\n8k dialogs\nBoolQ\n(Clark et al. 2019a) YN 16K Wikipedia Q/C boolean questions,\nsubset of Natural Questions\nPubMedQA\n(Jin et al. 2019b) YN 273.5K PubMed X/A biomedical domain,\n1k expert questions\nDROP\n(Dua et al. 2019) FF 96K Wikipedia C discrete reasoning\n38\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\n7. Conclusion\nThis work comprehensively reviews the studies of MRC in the scopes of back-\nground, deﬁnition, development, inﬂuence, datasets, technical and benchmark high-\nlights, trends, and opportunities. We ﬁrst brieﬂy introduced the history of MRC and\nthe background of contextualized language models. Then, we discussed the role of\ncontextualized language models and the inﬂuence of MRC to the NLP community. The\nprevious technical advances were summarized in the framework of Encoder to Decoder.\nAfter going through the mechanisms of MRC systems, we showed the highlights in\ndifferent stages of MRC studies. Finally, we summarized the trends and opportunities.\nThe basic views we have arrived at are that 1) MRC boosts the progress from language\nprocessing to understanding; 2) the rapid improvement of MRC systems greatly beneﬁts\nfrom the progress of CLMs; 3) the theme of MRC is gradually moving from shallow text\nmatching to cognitive reasoning.\nAppendix A: Machine Reading Comprehension Datasets\nThis appendix lists existing machine reading comprehension datasets along with their\nanswer styles, dataset size, type of corpus, sourcing methods, and focuses. Part of\nthe statistics is borrowed from Sugawara, Stenetorp, and Aizawa (2020). Ans denotes\nanswer styles where Ex is answer extraction by selecting a span in the given context,\nand FF is free-form answering. NA denotes that unanswerable questions are involved,\nand YN means yes or no answers. Size indicates the size of the whole dataset, including\ntraining, development, and test sets.Src represents how the questions are sourced where\nX means questions written by experts, C by crowdworkers, A by machines with an\nautomated manner, and Q are search-engine queries. Note that the boundary between\ncloze-style and multi-choice datasets is not clear sometimes; for example, some candi-\ndate choices may be provided for cloze tests, such as Story Cloze Test (Mostafazadeh\net al. 2017) and CLOTH (Xie et al. 2018). In our taxonomy, we regard the ﬁx-choice tasks\nwhose candidates are in a ﬁxed number as multi-choice. In addition, some datasets are\ncomposed of different types of subtasks; we classify them according to the main types\nwith special notations in Ans column.\nReferences\nAlberti, Chris, Daniel Andor, Emily Pitler,\nJacob Devlin, and Michael Collins. 2019.\nSynthetic qa corpora generation with\nroundtrip consistency. In Proceedings of the\n57th Annual Meeting of the Association for\nComputational Linguistics, pages 6168–6173.\nAndo, Rie Kubota and Tong Zhang. 2005. A\nframework for learning predictive\nstructures from multiple tasks and\nunlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817–1853.\nBack, Seohyun, Sai Chetan Chinthakindi,\nAkhil Kedia, Haejun Lee, and Jaegul\nChoo. 2020. NeurQuRI: Neural question\nrequirement inspector for answerability\nprediction in machine reading\ncomprehension. In International Conference\non Learning Representations.\nBahdanau, Dzmitry, Kyunghyun Cho, and\nYoshua Bengio. 2014. Neural machine\ntranslation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473.\nBajaj, Payal, Daniel Campos, Nick Craswell,\nLi Deng, Jianfeng Gao, Xiaodong Liu,\nRangan Majumder, Andrew McNamara,\nBhaskar Mitra, Tri Nguyen, et al. 2016. Ms\nmarco: A human generated machine\nreading comprehension dataset. arXiv\npreprint arXiv:1611.09268.\nBajgar, Ondrej, Rudolf Kadlec, and Jan\nKleindienst. 2016. Embracing data\nabundance: Booktest dataset for reading\ncomprehension. arXiv preprint\narXiv:1610.00956.\nBao, Hangbo, Li Dong, Furu Wei, Wenhui\nWang, Nan Yang, Lei Cui, Songhao Piao,\nand Ming Zhou. 2019. Inspecting\nuniﬁcation of encoding and matching with\n39\nComputational Linguistics Volume 1, Number 1\ntransformer: A case study of machine\nreading comprehension. In Proceedings of\nthe 2nd Workshop on Machine Reading for\nQuestion Answering, pages 14–18.\nBlitzer, John, Ryan McDonald, and Fernando\nPereira. 2006. Domain adaptation with\nstructural correspondence learning. In\nProceedings of the 2006 conference on\nempirical methods in natural language\nprocessing, pages 120–128.\nBrown, Peter F, Peter V Desouza, Robert L\nMercer, Vincent J Della Pietra, and\nJenifer C Lai. 1992. Class-based n-gram\nmodels of natural language. Computational\nlinguistics, 18(4):467–479.\nCastelli, Vittorio, Rishav Chakravarti,\nSaswati Dana, Anthony Ferritto, Radu\nFlorian, Martin Franz, Dinesh Garg,\nDinesh Khandelwal, Scott McCarley, Mike\nMcCawley, et al. 2019. The techqa dataset.\narXiv preprint arXiv:1911.02984.\nCharniak, Eugene, Yasemin Altun, Rodrigo\nde Salvo Braz, Benjamin Garrett, Margaret\nKosmala, Tomer Moscovich, Lixin Pang,\nChanghee Pyo, Ye Sun, Wei Wy, et al. 2000.\nReading comprehension programs in a\nstatistical-language-processing class. In\nProceedings of the 2000 ANLP/NAACL\nWorkshop on Reading comprehension tests as\nevaluation for computer-based language\nunderstanding sytems-Volume 6, pages 1–5,\nAssociation for Computational\nLinguistics.\nChen, Danqi. 2018. Neural Reading\nComprehension and Beyond. Ph.D. thesis,\nStanford University.\nChen, Danqi, Jason Bolton, and\nChristopher D Manning. 2016. A thorough\nexamination of the cnn/daily mail reading\ncomprehension task. In Proceedings of the\n54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 2358–2367.\nChen, Danqi, Adam Fisch, Jason Weston, and\nAntoine Bordes. 2017. Reading wikipedia\nto answer open-domain questions. In\nProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 1870–1879.\nChen, Yu, Lingfei Wu, and Mohammed J\nZaki. 2019. Graphﬂow: Exploiting\nconversation ﬂow with graph neural\nnetworks for conversational machine\ncomprehension. arXiv preprint\narXiv:1908.00059.\nChen, Zhipeng, Yiming Cui, Wentao Ma,\nShijin Wang, and Guoping Hu. 2019.\nConvolutional spatial attention model for\nreading comprehension with\nmultiple-choice questions. In Proceedings\nof the AAAI Conference on Artiﬁcial\nIntelligence, volume 33, pages 6276–6283.\nCho, Kyunghyun, Bart van Merriënboer,\nCaglar Gulcehre, Dzmitry Bahdanau, Fethi\nBougares, Holger Schwenk, and Yoshua\nBengio. 2014. Learning phrase\nrepresentations using rnn\nencoder–decoder for statistical machine\ntranslation. In Proceedings of the 2014\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages\n1724–1734.\nChoi, Eunsol, He He, Mohit Iyyer, Mark\nYatskar, Wen-tau Yih, Yejin Choi, Percy\nLiang, and Luke Zettlemoyer. 2018.\nQuAC: Question answering in context. In\nProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 2174–2184.\nClark, Christopher and Matt Gardner. 2018.\nSimple and effective multi-paragraph\nreading comprehension. In Proceedings of\nthe 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong Papers), pages 845–855.\nClark, Christopher, Kenton Lee, Ming-Wei\nChang, Tom Kwiatkowski, Michael\nCollins, and Kristina Toutanova. 2019a.\nBoolQ: Exploring the surprising difﬁculty\nof natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages\n2924–2936, Association for Computational\nLinguistics, Minneapolis, Minnesota.\nClark, Kevin, Urvashi Khandelwal, Omer\nLevy, and Christopher D Manning. 2019b.\nWhat does bert look at? an analysis of\nbertâ ˘A ´Zs attention. In Proceedings of the\n2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks\nfor NLP, pages 276–286.\nClark, Kevin, Minh-Thang Luong, Quoc V\nLe, and Christopher D Manning. 2019c.\nElectra: Pre-training text encoders as\ndiscriminators rather than generators. In\nInternational Conference on Learning\nRepresentations.\nClark, Peter, Isaac Cowhey, Oren Etzioni,\nTushar Khot, Ashish Sabharwal, Carissa\nSchoenick, and Oyvind Tafjord. 2018.\nThink you have solved question\nanswering? Try ARC, the AI2 reasoning\nchallenge. CoRR, abs/1803.05457.\nCui, Yiming, Wanxiang Che, Ting Liu, Bing\nQin, Shijin Wang, and Guoping Hu. 2019.\nCross-lingual machine reading\n40\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\ncomprehension. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages\n1586–1595.\nCui, Yiming, Zhipeng Chen, Si Wei, Shijin\nWang, Ting Liu, and Guoping Hu. 2017.\nAttention-over-attention neural networks\nfor reading comprehension. In Proceedings\nof the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong Papers), pages 593–602.\nCullingford, Richard E. 1977. Controlling\ninference in story understanding. In IJCAI,\nvolume 77, page 17, Citeseer.\nDai, Zihang, Zhilin Yang, Yiming Yang,\nJaime G Carbonell, Quoc Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl:\nAttentive language models beyond a\nﬁxed-length context. In Proceedings of the\n57th Annual Meeting of the Association for\nComputational Linguistics, pages 2978–2988.\nDalvi, Bhavana, Lifu Huang, Niket Tandon,\nWen-tau Yih, and Peter Clark. 2018.\nTracking state changes in procedural text:\na challenge dataset and models for process\nparagraph comprehension. In Proceedings\nof the 2018 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1595–1604,\nAssociation for Computational\nLinguistics.\nDasigi, Pradeep, Nelson F. Liu, Ana\nMarasovic, Noah A. Smith, and Matt\nGardner. 2019. Quoref: A reading\ncomprehension dataset with questions\nrequiring coreferential reasoning. In\nProceedings of the 2019 Conference on\nEmpirical Methods in Natural Language\nProcessing and the 9th International Joint\nConference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5927–5934,\nAssociation for Computational\nLinguistics, Hong Kong, China.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. 2018. BERT:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint arXiv:1810.04805.\nDhingra, Bhuwan, Hanxiao Liu, Zhilin Yang,\nWilliam Cohen, and Ruslan\nSalakhutdinov. 2017. Gated-attention\nreaders for text comprehension. In\nProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 1832–1846.\nDhingra, Bhuwan, Kathryn Mazaitis, and\nWilliam W Cohen. 2017. Quasar: Datasets\nfor question answering by search and\nreading. arXiv preprint arXiv:1707.03904.\nDing, Ming, Chang Zhou, Qibin Chen,\nHongxia Yang, and Jie Tang. 2019.\nCognitive graph for multi-hop reading\ncomprehension at scale. In Proceedings of\nthe 57th Annual Meeting of the Association\nfor Computational Linguistics, pages\n2694–2703.\nDong, Li, Furu Wei, Ming Zhou, and Ke Xu.\n2015. Question answering over freebase\nwith multi-column convolutional neural\nnetworks. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational\nLinguistics and the 7th International Joint\nConference on Natural Language Processing\n(Volume 1: Long Papers), pages 260–269.\nDong, Li, Nan Yang, Wenhui Wang, Furu\nWei, Xiaodong Liu, Yu Wang, Jianfeng\nGao, Ming Zhou, and Hsiao-Wuen Hon.\n2019. Uniﬁed language model pre-training\nfor natural language understanding and\ngeneration. In Advances in Neural\nInformation Processing Systems, pages\n13042–13054.\nDu, Xinya and Claire Cardie. 2017.\nIdentifying where to focus in reading\ncomprehension for neural question\ngeneration. In Proceedings of the 2017\nConference on Empirical Methods in Natural\nLanguage Processing, pages 2067–2073.\nDu, Xinya, Junru Shao, and Claire Cardie.\n2017. Learning to ask: Neural question\ngeneration for reading comprehension. In\nProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 1342–1352.\nDua, Dheeru, Yizhong Wang, Pradeep\nDasigi, Gabriel Stanovsky, Sameer Singh,\nand Matt Gardner. 2019. Drop: A reading\ncomprehension benchmark requiring\ndiscrete reasoning over paragraphs. In\nProceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short\nPapers), pages 2368–2378.\nDunn, Matthew, Levent Sagun, Mike\nHiggins, V Ugur Guney, Volkan Cirik, and\nKyunghyun Cho. 2017. Searchqa: A new\nq&a dataset augmented with context from\na search engine. arXiv preprint\narXiv:1704.05179.\nEttinger, Allyson. 2020. What bert is not:\nLessons from a new suite of\npsycholinguistic diagnostics for language\nmodels. Transactions of the Association for\n41\nComputational Linguistics Volume 1, Number 1\nComputational Linguistics, 8:34–48.\nEvans, Jonathan St BT. 1984. Heuristic and\nanalytic processes in reasoning. British\nJournal of Psychology, 75(4):451–468.\nEvans, Jonathan St BT. 2003. In two minds:\ndual-process accounts of reasoning. Trends\nin cognitive sciences, 7(10):454–459.\nEvans, Jonathan St BT. 2017. Dual process\ntheory: perspectives and problems. In\nDual process theory 2.0. Routledge, pages\n137–155.\nFisch, Adam, Alon Talmor, Robin Jia,\nMinjoon Seo, Eunsol Choi, and Danqi\nChen. 2019. Mrqa 2019 shared task:\nEvaluating generalization in reading\ncomprehension. In Proceedings of the 2nd\nWorkshop on Machine Reading for Question\nAnswering, pages 1–13.\nGao, Shuyang, Sanchit Agarwal, Tagyoung\nChung, Di Jin, and Dilek Hakkani-Tur.\n2020. From machine reading\ncomprehension to dialogue state tracking:\nBridging the gap. arXiv preprint\narXiv:2004.05827.\nGao, Shuyang, Abhishek Sethi, Sanchit\nAgarwal, Tagyoung Chung, and Dilek\nHakkani-Tur. 2019a. Dialog state tracking:\nA neural reading comprehension\napproach. In Proceedings of the 20th Annual\nSIGdial Meeting on Discourse and Dialogue,\npages 264–273.\nGao, Yifan, Lidong Bing, Piji Li, Irwin King,\nand Michael R Lyu. 2019b. Generating\ndistractors for reading comprehension\nquestions from real examinations. In\nProceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 33, pages\n6423–6430.\nGardner, Matt, Jonathan Berant, Hannaneh\nHajishirzi, Alon Talmor, and Sewon Min.\n2019. Question answering is a format;\nwhen is it useful? arXiv preprint\narXiv:1909.11291.\nGoldberg, Yoav. 2019. Assessing bert’s\nsyntactic abilities. arXiv preprint\narXiv:1901.05287.\nGoyal, Yash, Tejas Khot, Douglas\nSummers-Stay, Dhruv Batra, and Devi\nParikh. 2017. Making the V in VQA\nmatter: Elevating the role of image\nunderstanding in Visual Question\nAnswering. In Conference on Computer\nVision and Pattern Recognition (CVPR).\nGupta, Nitish, Kevin Lin, Dan Roth, Sameer\nSingh, and Matt Gardner. 2019. Neural\nmodule networks for reasoning over text.\narXiv preprint arXiv:1912.04971.\nGuu, Kelvin, Kenton Lee, Zora Tung,\nPanupong Pasupat, and Ming-Wei Chang.\n2020. Realm: Retrieval-augmented\nlanguage model pre-training. arXiv\npreprint arXiv:2002.08909.\nHabernal, Ivan, Henning Wachsmuth, Iryna\nGurevych, and Benno Stein. 2018. The\nargument reasoning comprehension task:\nIdentiﬁcation and reconstruction of\nimplicit warrants. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1\n(Long Papers), pages 1930–1940.\nHe, Wei, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi\nZhao, Xinyan Xiao, Yuan Liu, Yizhong\nWang, Hua Wu, Qiaoqiao She, et al. 2018.\nDureader: a chinese machine reading\ncomprehension dataset from real-world\napplications. In Proceedings of the Workshop\non Machine Reading for Question Answering,\npages 37–46.\nHermann, Karl Moritz, Tomas Kocisky,\nEdward Grefenstette, Lasse Espeholt, Will\nKay, Mustafa Suleyman, and Phil\nBlunsom. 2015. Teaching machines to read\nand comprehend. In Advances in Neural\nInformation Processing Systems 28 (NIPS\n2015), pages 1693–1701.\nHewlett, Daniel, Alexandre Lacoste, Llion\nJones, Illia Polosukhin, Andrew\nFandrianto, Jay Han, Matthew Kelcey, and\nDavid Berthelot. 2016. WikiReading: A\nnovel large-scale language understanding\ntask over wikipedia. In Proceedings of the\n54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1535–1545, Association for\nComputational Linguistics, Berlin,\nGermany.\nHill, Felix, Antoine Bordes, Sumit Chopra,\nand Jason Weston. 2015. The goldilocks\nprinciple: Reading children’s books with\nexplicit memory representations. arXiv\npreprint arXiv:1511.02301.\nHirschman, Lynette, Marc Light, Eric Breck,\nand John D Burger. 1999. Deep read: A\nreading comprehension system. In\nProceedings of the 37th annual meeting of the\nAssociation for Computational Linguistics on\nComputational Linguistics, pages 325–332,\nAssociation for Computational\nLinguistics.\nHochreiter, Sepp and Jürgen Schmidhuber.\n1997. Long short-term memory. Neural\ncomputation, 9(8):1735–1780.\nHtut, Phu Mon, Samuel Bowman, and\nKyunghyun Cho. 2018. Training a ranking\nfunction for open-domain question\nanswering. In Proceedings of the 2018\nConference of the North American Chapter of\n42\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nthe Association for Computational Linguistics:\nStudent Research Workshop, pages 120–127.\nHu, Minghao, Yuxing Peng, Zhen Huang,\nand Dongsheng Li. 2019a. A multi-type\nmulti-span network for reading\ncomprehension that requires discrete\nreasoning. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages\n1596–1606.\nHu, Minghao, Yuxing Peng, Zhen Huang,\nand Dongsheng Li. 2019b. Retrieve, read,\nrerank: Towards end-to-end\nmulti-document reading comprehension.\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics,\npages 2285–2295.\nHu, Minghao, Yuxing Peng, Zhen Huang,\nXipeng Qiu, Furu Wei, and Ming Zhou.\n2018. Reinforced mnemonic reader for\nmachine reading comprehension. In\nProceedings of the 27th International Joint\nConference on Artiﬁcial Intelligence, pages\n4099–4106, AAAI Press.\nHu, Minghao, Yuxing Peng, and Xipeng Qiu.\n2017. Mnemonic reader for machine\ncomprehension. CoRR, abs/1705.02798.\nHu, Minghao, Furu Wei, Yuxing Peng, Zhen\nHuang, Nan Yang, and Dongsheng Li.\n2019c. Read+ verify: Machine reading\ncomprehension with unanswerable\nquestions. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence,\nvolume 33, pages 6529–6537.\nHuang, Hsin-Yuan, Eunsol Choi, and Wen\ntau Yih. 2019. FlowQA: Grasping ﬂow in\nhistory for conversational machine\ncomprehension. In International Conference\non Learning Representations.\nHuang, Lifu, Ronan Le Bras, Chandra\nBhagavatula, and Yejin Choi. 2019.\nCosmos qa: Machine reading\ncomprehension with contextual\ncommonsense reasoning. arXiv preprint\narXiv:1909.00277.\nJia, Robin and Percy Liang. 2017. Adversarial\nexamples for evaluating reading\ncomprehension systems. In Proceedings of\nthe 2017 Conference on Empirical Methods in\nNatural Language Processing, pages\n2021–2031.\nJiang, Yichen, Nitish Joshi, Yen-Chun Chen,\nand Mohit Bansal. 2019. Explore, propose,\nand assemble: An interpretable model for\nmulti-hop reading comprehension. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics,\npages 2714–2725.\nJiao, Xiaoqi, Yichun Yin, Lifeng Shang, Xin\nJiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qun Liu. 2019. Tinybert: Distilling\nbert for natural language understanding.\narXiv preprint arXiv:1909.10351.\nJin, Di, Shuyang Gao, Jiun-Yu Kao, Tagyoung\nChung, and Dilek Hakkani-tur. 2019a.\nMmm: Multi-stage multi-task learning for\nmulti-choice reading comprehension.\narXiv preprint arXiv:1910.00458.\nJin, Qiao, Bhuwan Dhingra, Zhengping Liu,\nWilliam Cohen, and Xinghua Lu. 2019b.\nPubMedQA: A dataset for biomedical\nresearch question answering. In\nProceedings of the 2019 Conference on\nEmpirical Methods in Natural Language\nProcessing and the 9th International Joint\nConference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2567–2577,\nAssociation for Computational\nLinguistics, Hong Kong, China.\nJoshi, Mandar, Danqi Chen, Yinhan Liu,\nDaniel S Weld, Luke Zettlemoyer, and\nOmer Levy. 2020. Spanbert: Improving\npre-training by representing and\npredicting spans. Transactions of the\nAssociation for Computational Linguistics,\n8:64–77.\nJoshi, Mandar, Eunsol Choi, Daniel S Weld,\nand Luke Zettlemoyer. 2017. TriviaQA: A\nlarge scale distantly supervised challenge\ndataset for reading comprehension. In\nProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 1601–1611.\nKadlec, Rudolf, Martin Schmid, Ondrej\nBajgar, and Jan Kleindienst. 2016. Text\nunderstanding with the attention sum\nreader network. In Proceedings of the 54th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 908–918.\nKahneman, Daniel. 2011. Thinking, fast and\nslow. Macmillan.\nKembhavi, Aniruddha, Minjoon Seo, Dustin\nSchwenk, Jonghyun Choi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Are you\nsmarter than a sixth grader? textbook\nquestion answering for multimodal\nmachine comprehension. In the IEEE\nConference on Computer Vision and Pattern\nRecognition.\nKeskar, Nitish Shirish, Bryan McCann,\nCaiming Xiong, and Richard Socher. 2019.\nUnifying question answering, text\nclassiﬁcation, and regression via span\nextraction. arXiv preprint arXiv:1904.09286.\n43\nComputational Linguistics Volume 1, Number 1\nKhashabi, Daniel, Snigdha Chaturvedi,\nMichael Roth, Shyam Upadhyay, and Dan\nRoth. 2018. Looking beyond the surface: A\nchallenge set for reading comprehension\nover multiple sentences. In Proceedings of\nthe 2018 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 252–262,\nAssociation for Computational\nLinguistics.\nKim, Yanghoon, Hwanhee Lee, Joongbo\nShin, and Kyomin Jung. 2019. Improving\nneural question generation using answer\nseparation. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence,\nvolume 33, pages 6602–6609.\nKoˇ cisk`y, Tomáš, Jonathan Schwarz, Phil\nBlunsom, Chris Dyer, Karl Moritz\nHermann, Gábor Melis, and Edward\nGrefenstette. 2018. The narrativeqa\nreading comprehension challenge.\nTransactions of the Association for\nComputational Linguistics, 6:317–328.\nKwiatkowski, Tom, Jennimaria Palomaki,\nOlivia Redﬁeld, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein,\nIllia Polosukhin, Jacob Devlin, Kenton Lee,\nKristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov.\n2019. Natural questions: A benchmark for\nquestion answering research. Transactions\nof the Association for Computational\nLinguistics, 7:453–466.\nLabutov, Igor, Bishan Yang, Anusha Prakash,\nand Amos Azaria. 2018. Multi-relational\nquestion answering from narratives:\nMachine reading and reasoning in\nsimulated worlds. In Proceedings of the 56th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 833–844, Association for\nComputational Linguistics.\nLai, Guokun, Qizhe Xie, Hanxiao Liu,\nYiming Yang, and Eduard H. Hovy. 2017.\nRACE: Large-scale reading\ncomprehension dataset from\nexaminations. In Proceedings of the 2017\nConference on Empirical Methods in Natural\nLanguage Processing, pages 785–794.\nLan, Zhenzhong, Mingda Chen, Sebastian\nGoodman, Kevin Gimpel, Piyush Sharma,\nand Radu Soricut. 2019. ALBERT: A lite\nBERT for self-supervised learning of\nlanguage representations. In International\nConference on Learning Representations.\nLee, Kyungjae, Sunghyun Park, Hojae Han,\nJinyoung Yeo, Seung-won Hwang, and\nJuho Lee. 2019. Learning with limited data\nfor multilingual reading comprehension.\nIn Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language\nProcessing and the 9th International Joint\nConference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2833–2843.\nLehnert, Wendy G. 1977. A conceptual\ntheory of question answering. In\nProceedings of the 5th international joint\nconference on Artiﬁcial intelligence-Volume 1,\npages 158–164.\nLi, Junlong, Zhuosheng Zhang, and Hai\nZhao. 2020. Multi-choice dialogue-based\nreading comprehension with knowledge\nand key turns. arXiv preprint\narXiv:2004.13988.\nLi, Xiaoya, Jingrong Feng, Yuxian Meng,\nQinghong Han, Fei Wu, and Jiwei Li.\n2019a. A uniﬁed mrc framework for\nnamed entity recognition. arXiv preprint\narXiv:1910.11476.\nLi, Xiaoya, Fan Yin, Zijun Sun, Xiayu Li,\nArianna Yuan, Duo Chai, Mingxin Zhou,\nand Jiwei Li. 2019b. Entity-relation\nextraction as multi-turn question\nanswering. In Proceedings of the 57th\nAnnual Meeting of the Association for\nComputational Linguistics, pages 1340–1350.\nLi, Zuchao, Rui Wang, Kehai Chen, Masao\nUtiyama, Eiichiro Sumita, Zhuosheng\nZhang, and Hai Zhao. 2019c. Explicit\nsentence compression for neural machine\ntranslation. arXiv preprint\narXiv:1912.11980.\nLin, Bill Yuchen, Xinyue Chen, Jamin Chen,\nand Xiang Ren. 2019. Kagnet:\nKnowledge-aware graph networks for\ncommonsense reasoning. In Proceedings of\nthe 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th\nInternational Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP),\npages 2822–2832.\nLin, Chin-Yew. 2004. Looking for a few good\nmetrics: Automatic summarization\nevaluation-how many samples are\nenough? In NTCIR.\nLiu, Pengyuan, Yuning Deng, Chenghao\nZhu, and Han Hu. 2019a. Xcmrc:\nEvaluating cross-lingual machine reading\ncomprehension. In CCF International\nConference on Natural Language Processing\nand Chinese Computing, pages 552–564,\nSpringer.\nLiu, Shanshan, Xin Zhang, Sheng Zhang, Hui\nWang, and Weiming Zhang. 2019b. Neural\nmachine reading comprehension: Methods\nand trends. Applied Sciences, 9(18):3698.\n44\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nLiu, Xiaodong, Yelong Shen, Kevin Duh, and\nJianfeng Gao. 2018. Stochastic answer\nnetworks for machine reading\ncomprehension. In Proceedings of the 56th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1694–1704.\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei\nDu, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and\nVeselin Stoyanov. 2019c. Roberta: A\nrobustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nLu, Jiasen, Dhruv Batra, Devi Parikh, and\nStefan Lee. 2019. Vilbert: Pretraining\ntask-agnostic visiolinguistic\nrepresentations for vision-and-language\ntasks. In NeurIPS.\nMa, Kaixin, Jonathan Francis, Quanyang Lu,\nEric Nyberg, and Alessandro Oltramari.\n2019. Towards generalizable\nneuro-symbolic systems for commonsense\nquestion answering. arXiv preprint\narXiv:1910.14087.\nMa, Kaixin, Tomasz Jurczyk, and Jinho D\nChoi. 2018. Challenging reading\ncomprehension on daily conversation:\nPassage completion on multiparty dialog.\nIn Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages\n2039–2048.\nMcCann, Bryan, Nitish Shirish Keskar,\nCaiming Xiong, and Richard Socher. 2018.\nThe natural language decathlon: Multitask\nlearning as question answering. arXiv\npreprint arXiv:1806.08730.\nMihaylov, Todor, Peter Clark, Tushar Khot,\nand Ashish Sabharwal. 2018. Can a suit of\narmor conduct electricity? a new dataset\nfor open book question answering. In\nProceedings of the 2018 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 2381–2391, Association\nfor Computational Linguistics, Brussels,\nBelgium.\nMikolov, Tomas, Ilya Sutskever, Kai Chen,\nGreg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and\nphrases and their compositionality. In\nAdvances in neural information processing\nsystems, pages 3111–3119.\nMin, Sewon, Victor Zhong, Richard Socher,\nand Caiming Xiong. 2018. Efﬁcient and\nrobust question answering from minimal\ncontext over documents. In Proceedings of\nthe 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong Papers), pages 1725–1735.\nMostafazadeh, Nasrin, Nathanael Chambers,\nXiaodong He, Devi Parikh, Dhruv Batra,\nLucy Vanderwende, Pushmeet Kohli, and\nJames Allen. 2016. A corpus and cloze\nevaluation for deeper understanding of\ncommonsense stories. In Proceedings of the\n2016 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies,\npages 839–849.\nMostafazadeh, Nasrin, Michael Roth, Annie\nLouis, Nathanael Chambers, and James\nAllen. 2017. LSDSem 2017 shared task:\nThe story cloze test. In Proceedings of the\n2nd Workshop on Linking Models of Lexical,\nSentential and Discourse-level Semantics,\npages 46–51, Association for\nComputational Linguistics.\nNishida, Kyosuke, Itsumi Saito, Kosuke\nNishida, Kazutoshi Shinoda, Atsushi\nOtsuka, Hisako Asano, and Junji Tomita.\n2019. Multi-style generative reading\ncomprehension. In Proceedings of the 57th\nAnnual Meeting of the Association for\nComputational Linguistics, pages 2273–2284.\nOnishi, Takeshi, Hai Wang, Mohit Bansal,\nKevin Gimpel, and David McAllester.\n2016. Who did What: A large-scale\nperson-centered cloze dataset. In\nProceedings of the 2016 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 2230–2235, Association\nfor Computational Linguistics.\nOstermann, Simon, Ashutosh Modi, Michael\nRoth, Stefan Thater, and Manfred Pinkal.\n2018. MCScript: A novel dataset for\nassessing machine comprehension using\nscript knowledge. In Proceedings of the\nEleventh International Conference on\nLanguage Resources and Evaluation (LREC\n2018), European Language Resources\nAssociation (ELRA).\nOstermann, Simon, Michael Roth, and\nManfred Pinkal. 2019. MCScript2.0: A\nmachine comprehension corpus focused\non script events and participants. In\nProceedings of the Eighth Joint Conference on\nLexical and Computational Semantics (*SEM\n2019), pages 103–117, Association for\nComputational Linguistics, Minneapolis,\nMinnesota.\nPampari, Anusri, Preethi Raghavan, Jennifer\nLiang, and Jian Peng. 2018. emrqa: A large\ncorpus for question answering on\nelectronic medical records. In Proceedings\nof the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages\n2357–2368.\n45\nComputational Linguistics Volume 1, Number 1\nPan, Boyuan, Hao Li, Zhou Zhao, Bin Cao,\nDeng Cai, and Xiaofei He. 2017. MEMEN:\nMulti-layer embedding with memory\nnetworks for machine comprehension.\narXiv preprint arXiv:1707.09098.\nPan, Lin, Rishav Chakravarti, Anthony\nFerritto, Michael Glass, Alﬁo Gliozzo,\nSalim Roukos, Radu Florian, and Avirup\nSil. 2019a. Frustratingly easy natural\nquestion answering. arXiv preprint\narXiv:1909.05286.\nPan, Xiaoman, Kai Sun, Dian Yu3 Jianshu\nChen, Heng Ji, Claire Cardie, and Dong\nYu. 2019b. Improving question answering\nwith external knowledge. In EMNLP 2019\nMRQA Workshop, page 27.\nPaperno, Denis, Germán Kruszewski,\nAngeliki Lazaridou, Ngoc Quan Pham,\nRaffaella Bernardi, Sandro Pezzelle, Marco\nBaroni, Gemma Boleda, and Raquel\nFernández. 2016. The LAMBADA dataset:\nWord prediction requiring a broad\ndiscourse context. In Proceedings of the 54th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1525–1534, Association for\nComputational Linguistics, Berlin,\nGermany.\nPapineni, Kishore, Salim Roukos, Todd\nWard, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of\nmachine translation. In Proceedings of the\n40th annual meeting on association for\ncomputational linguistics, pages 311–318,\nAssociation for Computational\nLinguistics.\nPennington, Jeffrey, Richard Socher, and\nChristopher D Manning. 2014. Glove:\nGlobal vectors for word representation. In\nProceedings of the 2014 conference on\nempirical methods in natural language\nprocessing (EMNLP), pages 1532–1543.\nPeters, Matthew E, Mark Neumann, Mohit\nIyyer, Matt Gardner, Christopher Clark,\nKenton Lee, and Luke Zettlemoyer. 2018.\nDeep contextualized word\nrepresentations. In Proceedings of\nNAACL-HLT, pages 2227–2237.\nQiao, Yifan, Chenyan Xiong, Zhenghao Liu,\nand Zhiyuan Liu. 2019. Understanding the\nbehaviors of bert in ranking. arXiv preprint\narXiv:1904.07531.\nQiu, Boyu, Xu Chen, Jungang Xu, and\nYingfei Sun. 2019a. A survey on neural\nmachine reading comprehension. arXiv\npreprint arXiv:1906.03824.\nQiu, Lin, Yunxuan Xiao, Yanru Qu, Hao\nZhou, Lei Li, Weinan Zhang, and Yong Yu.\n2019b. Dynamically fused graph network\nfor multi-hop reasoning. In Proceedings of\nthe 57th Annual Meeting of the Association\nfor Computational Linguistics, pages\n6140–6150.\nRadford, Alec, Karthik Narasimhan, Tim\nSalimans, and Ilya Sutskever. 2018.\nImproving language understanding by\ngenerative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nRaffel, Colin, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J\nLiu. 2019. Exploring the limits of transfer\nlearning with a uniﬁed text-to-text\ntransformer. arXiv preprint\narXiv:1910.10683.\nRajpurkar, Pranav, Robin Jia, and Percy\nLiang. 2018. Know what you don’t know:\nUnanswerable questions for SQuAD. In\nProceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 2: Short Papers), pages 784–789.\nRajpurkar, Pranav, Jian Zhang, Konstantin\nLopyrev, and Percy Liang. 2016. SQuAD:\n100,000+ questions for machine\ncomprehension of text. In Proceedings of the\n2016 Conference on Empirical Methods in\nNatural Language Processing (EMNLP 2016),\npages 2383–2392.\nRan, Qiu, Peng Li, Weiwei Hu, and Jie Zhou.\n2019a. Option comparison network for\nmultiple-choice reading comprehension.\narXiv preprint arXiv:1903.03033.\nRan, Qiu, Yankai Lin, Peng Li, Jie Zhou, and\nZhiyuan Liu. 2019b. Numnet: Machine\nreading comprehension with numerical\nreasoning. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages\n2474–2484.\nReddy, Siva, Danqi Chen, and Christopher D\nManning. 2019. CoQA: A conversational\nquestion answering challenge. Transactions\nof the Association for Computational\nLinguistics, 7:249–266.\nRichardson, Matthew, Christopher JC\nBurges, and Erin Renshaw. 2013. Mctest: A\nchallenge dataset for the open-domain\nmachine comprehension of text. In\nProceedings of the 2013 Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 193–203.\nRiloff, Ellen and Michael Thelen. 2000. A\nrule-based question answering system for\nreading comprehension tests. In\n46\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nProceedings of the 2000 ANLP/NAACL\nWorkshop on Reading comprehension tests as\nevaluation for computer-based language\nunderstanding sytems-Volume 6, pages\n13–19, Association for Computational\nLinguistics.\nRogers, Anna, Olga Kovaleva, Matthew\nDowney, and Anna Rumshisky. 2020.\nGetting closer to AI complete question\nanswering: A set of prerequisite real tasks.\nIn Proceedings of the Thirty-Fourth AAAI\nConference on Artiﬁcial Intelligence.\nSaeidi, Marzieh, Max Bartolo, Patrick Lewis,\nSameer Singh, Tim Rocktäschel, Mike\nSheldon, Guillaume Bouchard, and\nSebastian Riedel. 2018. Interpretation of\nnatural language rules in conversational\nmachine reading. In Proceedings of the 2018\nConference on Empirical Methods in Natural\nLanguage Processing, pages 2087–2097.\nSaha, Amrita, Rahul Aralikatte, Mitesh M.\nKhapra, and Karthik Sankaranarayanan.\n2018. DuoRC: Towards complex language\nunderstanding with paraphrased reading\ncomprehension. In Proceedings of the 56th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1683–1693, Association for\nComputational Linguistics.\nSanh, Victor, Lysandre Debut, Julien\nChaumond, and Thomas Wolf. 2019.\nDistilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nSap, Maarten, Ronan Le Bras, Emily Allaway,\nChandra Bhagavatula, Nicholas Lourie,\nHannah Rashkin, Brendan Roof, Noah A\nSmith, and Yejin Choi. 2019. Atomic: An\natlas of machine commonsense for if-then\nreasoning. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence,\nvolume 33, pages 3027–3035.\nSchlegel, Viktor, Marco Valentino, André\nFreitas, Goran Nenadic, and Riza\nBatista-Navarro. 2020. A framework for\nevaluation of machine reading\ncomprehension gold standards. arXiv\npreprint arXiv:2003.04642.\nSeo, Minjoon, Aniruddha Kembhavi, Ali\nFarhadi, and Hannaneh Hajishirzi. 2017.\nBidirectional attention ﬂow for machine\ncomprehension. In ICLR 2017.\nShao, Nan, Yiming Cui, Ting Liu, Shijin\nWang, and Guoping Hu. 2020. Is graph\nstructure necessary for multi-hop\nreasoning? arXiv preprint arXiv:2004.03096.\nShen, Sheng, Zhen Dong, Jiayu Ye, Linjian\nMa, Zhewei Yao, Amir Gholami,\nMichael W Mahoney, and Kurt Keutzer.\n2019. Q-bert: Hessian based ultra low\nprecision quantization of bert. arXiv\npreprint arXiv:1909.05840.\nShen, Yelong, Po-Sen Huang, Jianfeng Gao,\nand Weizhu Chen. 2017. Reasonet:\nLearning to stop reading in machine\ncomprehension. In Proceedings of the 23rd\nACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining,\npages 1047–1055.\nShoeybi, Mohammad, Mostofa Patwary, Raul\nPuri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. 2019. Megatron-lm:\nTraining multi-billion parameter language\nmodels using model parallelism. arXiv\npreprint arXiv:1909.08053.\nSong, Kaitao, Xu Tan, Tao Qin, Jianfeng Lu,\nand Tie-Yan Liu. 2020. Mpnet: Masked\nand permuted pre-training for language\nunderstanding. arXiv preprint\narXiv:2004.09297.\nSong, Linfeng, Zhiguo Wang, Mo Yu, Yue\nZhang, Radu Florian, and Daniel Gildea.\n2018. Exploring graph-structured passage\nrepresentation for multi-hop reading\ncomprehension with graph neural\nnetworks. arXiv preprint arXiv:1809.02040.\nSordoni, Alessandro, Philip Bachman, Adam\nTrischler, and Yoshua Bengio. 2016.\nIterative alternating neural attention for\nmachine reading. arXiv preprint\narXiv:1606.02245.\nSpeer, Robyn, Joshua Chin, and Catherine\nHavasi. 2017. Conceptnet 5.5: An open\nmultilingual graph of general knowledge.\nIn Thirty-First AAAI Conference on Artiﬁcial\nIntelligence.\nSugawara, Saku, Kentaro Inui, Satoshi\nSekine, and Akiko Aizawa. 2018. What\nmakes reading comprehension questions\neasier? In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language\nProcessing, pages 4208–4219.\nSugawara, Saku, Pontus Stenetorp, and\nAkiko Aizawa. 2020. Prerequisites for\nexplainable machine reading\ncomprehension: A position paper. arXiv\npreprint arXiv:2004.01912.\nSugawara, Saku, Pontus Stenetorp, Kentaro\nInui, and Akiko Aizawa. 2019. Assessing\nthe benchmarking capacity of machine\nreading comprehension datasets. arXiv\npreprint arXiv:1911.09241.\nSun, Kai, Dian Yu, Jianshu Chen, Dong Yu,\nYejin Choi, and Claire Cardie. 2019a.\nDream: A challenge data set and models\nfor dialogue-based reading\ncomprehension. Transactions of the\nAssociation for Computational Linguistics,\n47\nComputational Linguistics Volume 1, Number 1\n7:217–231.\nSun, Kai, Dian Yu, Dong Yu, and Claire\nCardie. 2019b. Improving machine\nreading comprehension with general\nreading strategies. In Proceedings of the\n2019 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages\n2633–2643.\nSuster, Simon and Walter Daelemans. 2018.\nCliCR: a dataset of clinical case reports for\nmachine reading comprehension. In\nProceedings of the 2018 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages\n1551–1563, Association for Computational\nLinguistics.\nSutcliffe, Richard, Anselmo Peñas, Eduard\nHovy, Pamela Forner, Álvaro Rodrigo,\nCorina Forascu, Yassine Benajiba, and\nPetya Osenova. 2013. Overview of\nQA4MRE main task at CLEF 2013.\nWorking Notes, CLEF.\nSutskever, Ilya, Oriol Vinyals, and Quoc V\nLe. 2014. Sequence to sequence learning\nwith neural networks. In Advances in\nneural information processing systems, pages\n3104–3112.\nTalmor, Alon and Jonathan Berant. 2019.\nMultiqa: An empirical investigation of\ngeneralization and transfer in reading\ncomprehension. In Proceedings of the 57th\nAnnual Meeting of the Association for\nComputational Linguistics, pages 4911–4921.\nTalmor, Alon, Jonathan Herzig, Nicholas\nLourie, and Jonathan Berant. 2019.\nCommonsenseQA: A question answering\nchallenge targeting commonsense\nknowledge. In Proceedings of the 2019\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1\n(Long and Short Papers), pages 4149–4158,\nAssociation for Computational\nLinguistics, Minneapolis, Minnesota.\nTan, Chuanqi, Furu Wei, Nan Yang, Bowen\nDu, Weifeng Lv, and Ming Zhou. 2018.\nS-net: From answer extraction to answer\nsynthesis for machine reading\ncomprehension. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence.\nTandon, Niket, Gerard De Melo, and\nGerhard Weikum. 2017. Webchild 2.0:\nFine-grained commonsense knowledge\ndistillation. In Proceedings of ACL 2017,\nSystem Demonstrations, pages 115–120.\nTang, Min, Jiaran Cai, and Hankz Hankui\nZhuo. 2019. Multi-matching network for\nmultiple choice reading comprehension.\nIn Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 33, pages\n7088–7095.\nTay, Yi, Luu Anh Tuan, and Siu Cheung Hui.\n2018. Multi-range reasoning for machine\ncomprehension. arXiv preprint\narXiv:1803.09074.\nTrischler, Adam, Tong Wang, Xingdi Yuan,\nJustin Harris, Alessandro Sordoni, Philip\nBachman, and Kaheer Suleman. 2017.\nNewsqa: A machine comprehension\ndataset. In Proceedings of the 2nd Workshop\non Representation Learning for NLP, pages\n191–200.\nTu, Ming, Jing Huang, Xiaodong He, and\nBowen Zhou. 2020. Graph sequential\nnetwork for reasoning over sequences.\narXiv preprint arXiv:2004.02001.\nTu, Ming, Kevin Huang, Guangtao Wang,\nJing Huang, Xiaodong He, and Bowen\nZhou. 2019. Select, answer and explain:\nInterpretable multi-hop reading\ncomprehension over multiple documents.\narXiv preprint arXiv:1911.00484.\nVaswani, Ashish, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Ł ukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you\nneed. In Advances in Neural Information\nProcessing Systems 30, pages 5998–6008.\nVinyals, Oriol, Meire Fortunato, and\nNavdeep Jaitly. 2015. Pointer networks. In\nAdvances in neural information processing\nsystems, pages 2692–2700.\nWallace, Eric, Pedro Rodriguez, Shi Feng,\nIkuya Yamada, and Jordan Boyd-Graber.\n2019. Trick me if you can:\nHuman-in-the-loop generation of\nadversarial examples for question\nanswering. Transactions of the Association\nfor Computational Linguistics, 7:387–401.\nWang, Huazheng, Zhe Gan, Xiaodong Liu,\nJingjing Liu, Jianfeng Gao, and Hongning\nWang. 2019. Adversarial domain\nadaptation for machine reading\ncomprehension. In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages\n2510–2520.\nWang, Shuohang and Jing Jiang. 2016.\nMachine comprehension using\nMatch-LSTM and answer pointer. arXiv\npreprint arXiv:1608.07905.\n48\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nWang, Shuohang, Mo Yu, Jing Jiang, and\nShiyu Chang. 2018a. A co-matching model\nfor multi-choice reading comprehension.\nIn Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics\n(Volume 2: Short Papers), pages 746–751.\nWang, Shuohang, Mo Yu, Jing Jiang, Wei\nZhang, Xiaoxiao Guo, Shiyu Chang,\nZhiguo Wang, Tim Klinger, Gerald\nTesauro, and Murray Campbell. 2018b.\nEvidence aggregation for answer\nre-ranking in open-domain question\nanswering. In International Conference on\nLearning Representations.\nWang, Wei, Bin Bi, Ming Yan, Chen Wu,\nJiangnan Xia, Zuyi Bao, Liwei Peng, and\nLuo Si. 2020a. Structbert: Incorporating\nlanguage structures into pre-training for\ndeep language understanding. In\nInternational Conference on Learning\nRepresentations.\nWang, Wenhui, Furu Wei, Li Dong, Hangbo\nBao, Nan Yang, and Ming Zhou. 2020b.\nMinilm: Deep self-attention distillation for\ntask-agnostic compression of pre-trained\ntransformers. arXiv preprint\narXiv:2002.10957.\nWang, Wenhui, Nan Yang, Furu Wei, Baobao\nChang, and Ming Zhou. 2017. Gated\nself-matching networks for reading\ncomprehension and question answering.\nIn Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics\n(Volume 1: Long Papers), pages 189–198.\nWang, Yizhong, Kai Liu, Jing Liu, Wei He,\nYajuan Lyu, Hua Wu, Sujian Li, and\nHaifeng Wang. 2018c. Multi-passage\nmachine reading comprehension with\ncross-passage answer veriﬁcation. In\nProceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics\n(Volume 1: Long Papers), pages 1918–1927.\nWang, Zhen, Jiachen Liu, Xinyan Xiao,\nYajuan Lyu, and Tian Wu. 2018d. Joint\ntraining of candidate extraction and\nanswer selection for reading\ncomprehension. In Proceedings of the 56th\nAnnual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1715–1724.\nWason, Peter C and J St BT Evans. 1974. Dual\nprocesses in reasoning? Cognition,\n3(2):141–154.\nWelbl, Johannes, Pontus Stenetorp, and\nSebastian Riedel. 2018. Constructing\ndatasets for multi-hop reading\ncomprehension across documents.\nTransactions of the Association for\nComputational Linguistics, 6:287–302.\nWeston, Jason, Antoine Bordes, Sumit\nChopra, and Tomas Mikolov. 2015.\nTowards AI-complete question answering:\na set of prerequisite toy tasks. In\nInternational Conference on Learning\nRepresentations.\nXie, Qizhe, Guokun Lai, Zihang Dai, and\nEduard Hovy. 2018. Large-scale cloze test\ndataset created by teachers. In Proceedings\nof the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages\n2344–2356.\nXiong, Caiming, Victor Zhong, and Richard\nSocher. 2016. Dynamic coattention\nnetworks for question answering. arXiv\npreprint arXiv:1611.01604.\nXiong, Caiming, Victor Zhong, and Richard\nSocher. 2017. DCN+: Mixed objective and\ndeep residual coattention for question\nanswering. arXiv preprint\narXiv:1711.00106.\nXiong, Caiming, Victor Zhong, and Richard\nSocher. 2018. DCN+: Mixed objective and\ndeep residual coattention for question\nanswering. In International Conference on\nLearning Representations.\nXu, Canwen, Jiaxin Pei, Hongtao Wu, Yiyu\nLiu, and Chenliang Li. 2020. Matinf: A\njointly labeled large-scale dataset for\nclassiﬁcation, question answering and\nsummarization. arXiv preprint\narXiv:2004.12302.\nXu, Yichong, Xiaodong Liu, Yelong Shen,\nJingjing Liu, and Jianfeng Gao. 2018.\nMulti-task learning with sample\nre-weighting for machine reading\ncomprehension. arXiv preprint\narXiv:1809.06963.\nYagcioglu, Semih, Aykut Erdem, Erkut\nErdem, and Nazli Ikizler-Cinbis. 2018.\nRecipeqa: A challenge dataset for\nmultimodal comprehension of cooking\nrecipes. In Proceedings of the 2018\nConference on Empirical Methods in Natural\nLanguage Processing, pages 1358–1368.\nYan, Ming, Jiangnan Xia, Chen Wu, Bin Bi,\nZhongzhou Zhao, Ji Zhang, Luo Si, Rui\nWang, Wei Wang, and Haiqing Chen. 2019.\nA deep cascade model for multi-document\nreading comprehension. In Proceedings of\nthe AAAI Conference on Artiﬁcial\nIntelligence, volume 33, pages 7354–7361.\nYang, Wei, Yuqing Xie, Luchen Tan, Kun\nXiong, Ming Li, and Jimmy Lin. 2019a.\nData augmentation for bert ﬁne-tuning in\nopen-domain question answering. arXiv\npreprint arXiv:1904.06652.\nYang, Wei, Yuqing Xie, Luchen Tan, Kun\nXiong, Ming Li, and Jimmy Lin. 2019b.\n49\nComputational Linguistics Volume 1, Number 1\nData augmentation for bert ﬁne-tuning in\nopen-domain question answering. arXiv\npreprint arXiv:1904.06652.\nYang, Zhilin, Zihang Dai, Yiming Yang,\nJaime Carbonell, Russ R Salakhutdinov,\nand Quoc V Le. 2019c. Xlnet: Generalized\nautoregressive pretraining for language\nunderstanding. In Advances in neural\ninformation processing systems, pages\n5754–5764.\nYang, Zhilin, Bhuwan Dhingra, Ye Yuan,\nJunjie Hu, William W. Cohen, and Ruslan\nSalakhutdinov. 2017a. Words or\ncharacters? ﬁne-grained gating for reading\ncomprehension. In Proceedings of the\nInternational Conference on Learning\nRepresentations (ICLR 2017).\nYang, Zhilin, Junjie Hu, Ruslan\nSalakhutdinov, and William Cohen. 2017b.\nSemi-supervised qa with generative\ndomain-adaptive nets. In Proceedings of the\n55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers), pages 1040–1050.\nYang, Zhilin, Peng Qi, Saizheng Zhang,\nYoshua Bengio, William Cohen, Ruslan\nSalakhutdinov, and Christopher D\nManning. 2018. Hotpotqa: A dataset for\ndiverse, explainable multi-hop question\nanswering. In Proceedings of the 2018\nConference on Empirical Methods in Natural\nLanguage Processing, pages 2369–2380.\nYu, Adams Wei, David Dohan, Quoc Le,\nThang Luong, Rui Zhao, and Kai Chen.\n2018. QANet: Combining local\nconvolution with global self-attention for\nreading comprehension. In International\nConference on Learning Representations.\nYu, Adams Wei, Hongrae Lee, and Quoc Le.\n2017. Learning to skim text. In Proceedings\nof the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong Papers), pages 1880–1890.\nZellers, Rowan, Yonatan Bisk, Roy Schwartz,\nand Yejin Choi. 2018. SWAG: A large-scale\nadversarial dataset for grounded\ncommonsense inference. In Proceedings of\nthe 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 93–104,\nAssociation for Computational\nLinguistics.\nZellers, Rowan, Ari Holtzman, Yonatan Bisk,\nAli Farhadi, and Yejin Choi. 2019.\nHellaSwag: Can a machine really ﬁnish\nyour sentence? In Proceedings of the 57th\nAnnual Meeting of the Association for\nComputational Linguistics, pages 4791–4800,\nAssociation for Computational\nLinguistics, Florence, Italy.\nZhang, Sheng, Xiaodong Liu, Jingjing Liu,\nJianfeng Gao, Kevin Duh, and Benjamin\nVan Durme. 2018a. Record: Bridging the\ngap between human and machine\ncommonsense reading comprehension.\narXiv preprint arXiv:1810.12885.\nZhang, Shuailiang, Hai Zhao, Yuwei Wu,\nZhuosheng Zhang, Xi Zhou, and Xiang\nZhou. 2019a. Dual co-matching network\nfor multi-choice reading comprehension.\narXiv preprint arXiv:1901.09381.\nZhang, Shuailiang, Hai Zhao, Yuwei Wu,\nZhuosheng Zhang, Xi Zhou, and Xiang\nZhou. 2020a. DCMN+: Dual co-matching\nnetwork for multi-choice reading\ncomprehension. In AAAI.\nZhang, Shuailiang, Hai Zhao, and Junru\nZhou. 2020. Semantics-aware inferential\nnetwork for natural language\nunderstanding. arXiv preprint\narXiv:2004.13338.\nZhang, Xin, An Yang, Sujian Li, and Yizhong\nWang. 2019b. Machine reading\ncomprehension: a literature review. arXiv\npreprint arXiv:1907.01686.\nZhang, Yiqing, Hai Zhao, and Zhuosheng\nZhang. 2019. Examination-style reading\ncomprehension with neural augmented\nretrieval. In 2019 International Conference\non Asian Language Processing (IALP), pages\n182–187, IEEE.\nZhang, Zhuosheng, Kehai Chen, Rui Wang,\nMasao Utiyama, Eiichiro Sumita, Zuchao\nLi, and Hai Zhao. 2019c. Neural machine\ntranslation with universal visual\nrepresentation. In International Conference\non Learning Representations (ICLR).\nZhang, Zhuosheng, Yafang Huang, and Hai\nZhao. 2018. Subword-augmented\nembedding for cloze reading\ncomprehension. In Proceedings of the 27th\nInternational Conference on Computational\nLinguistics (COLING), pages\n1802–â ˘A¸ S1814.\nZhang, Zhuosheng, Yafang Huang, and Hai\nZhao. 2019. Open vocabulary learning for\nneural chinese pinyin ime. In Proceedings\nof the 57th Annual Meeting of the Association\nfor Computational Linguistics (ACL), pages\n1584–1594.\nZhang, Zhuosheng, Yafang Huang, Pengfei\nZhu, and Hai Zhao. 2018b. Effective\ncharacter-augmented word embedding for\nmachine reading comprehension. In CCF\nInternational Conference on Natural\nLanguage Processing and Chinese Computing,\npages 27–39, Springer.\nZhang, Zhuosheng, Jiangtong Li, Pengfei\nZhu, and Hai Zhao. 2018c. Modeling\n50\nZhang et al. Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond\nmulti-turn conversation with deep\nutterance aggregation. In Proceedings of the\n27th International Conference on\nComputational Linguistics (COLING), pages\n3740–3752.\nZhang, Zhuosheng, Yuwei Wu, Zuchao Li,\nand Hai Zhao. 2019d. Explicit contextual\nsemantics for text comprehension. In\nProceedings of the 33rd Paciﬁc Asia\nConference on Language, Information and\nComputation (P ACLIC 33).\nZhang, Zhuosheng, Yuwei Wu, Hai Zhao,\nZuchao Li, Shuailiang Zhang, Xi Zhou,\nand Xiang Zhou. 2020b. Semantics-aware\nbert for language understanding. In the\nThirty-Fourth AAAI Conference on Artiﬁcial\nIntelligence (AAAI-2020).\nZhang, Zhuosheng, Yuwei Wu, Junru Zhou,\nSufeng Duan, Hai Zhao, and Rui Wang.\n2020c. SG-Net: Syntax-guided machine\nreading comprehension. In Proceedings of\nthe Thirty-Fourth AAAI Conference on\nArtiﬁcial Intelligence (AAAI).\nZhang, Zhuosheng, Junjie Yang, and Hai\nZhao. 2020. Retrospective reader for\nmachine reading comprehension. arXiv\npreprint arXiv:2001.09694.\nZhang, Zhuosheng and Hai Zhao. 2018.\nOne-shot learning for question-answering\nin gaokao history challenge. In Proceedings\nof the 27th International Conference on\nComputational Linguistics (COLING), pages\n449–461.\nZhang, Zhuosheng, Hai Zhao, Kangwei Ling,\nJiangtong Li, Shexia He, and Guohong Fu.\n2019e. Effective subword segmentation for\ntext comprehension. IEEE/ACM\nTransactions on Audio, Speech, and Language\nProcessing (TASLP), 27(11):1664–1674.\nZhou, Junru, Zhuosheng Zhang, and Hai\nZhao. 2019. LIMIT-BERT: Linguistic\ninformed multi-task bert. arXiv preprint\narXiv:1910.14296.\nZhu, Haichao, Li Dong, Furu Wei, Wenhui\nWang, Bing Qin, and Ting Liu. 2019.\nLearning to ask unanswerable questions\nfor machine reading comprehension. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics,\npages 4238–4248.\nZhu, Haichao, Furu Wei, Bing Qin, and Ting\nLiu. 2018a. Hierarchical attention ﬂow for\nmultiple-choice reading comprehension.\nIn Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence.\nZhu, Pengfei, Zhuosheng Zhang, Jiangtong\nLi, Yafang Huang, and Hai Zhao. 2018b.\nLingke: a ﬁne-grained multi-turn chatbot\nfor customer service. In Proceedings of the\n27th International Conference on\nComputational Linguistics: System\nDemonstrations, pages 108–112.\nZhu, Pengfei, Hai Zhao, and Xiaoguang Li.\n2020. Dual multi-head co-attention for\nmulti-choice reading comprehension.\narXiv preprint arXiv:2001.09415.\n51",
  "topic": "Comprehension",
  "concepts": [
    {
      "name": "Comprehension",
      "score": 0.6244670748710632
    },
    {
      "name": "Reading (process)",
      "score": 0.5759537220001221
    },
    {
      "name": "Linguistics",
      "score": 0.5640440583229065
    },
    {
      "name": "Computer science",
      "score": 0.4990501403808594
    },
    {
      "name": "Reading comprehension",
      "score": 0.4652256667613983
    },
    {
      "name": "Psychology",
      "score": 0.39375579357147217
    },
    {
      "name": "Cognitive science",
      "score": 0.3663519620895386
    },
    {
      "name": "Natural language processing",
      "score": 0.3495546579360962
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3209603428840637
    },
    {
      "name": "Philosophy",
      "score": 0.1093723475933075
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I90023481",
      "name": "National Institute of Information and Communications Technology",
      "country": "JP"
    }
  ],
  "cited_by": 48
}