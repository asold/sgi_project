{
  "title": "Probing for Constituency Structure in Neural Language Models",
  "url": "https://openalex.org/W4385567355",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2807598581",
      "name": "David Arps",
      "affiliations": [
        "Heinrich Heine University Düsseldorf"
      ]
    },
    {
      "id": "https://openalex.org/A1269697726",
      "name": "Younes Samih",
      "affiliations": [
        "Heinrich Heine University Düsseldorf"
      ]
    },
    {
      "id": "https://openalex.org/A21209263",
      "name": "Laura Kallmeyer",
      "affiliations": [
        "Heinrich Heine University Düsseldorf"
      ]
    },
    {
      "id": "https://openalex.org/A2256769809",
      "name": "Hassan Sajjad",
      "affiliations": [
        "Dalhousie University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2914924671",
    "https://openalex.org/W3196986263",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1508977358",
    "https://openalex.org/W2998386992",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2966280323",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W3102340579",
    "https://openalex.org/W2122825543",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3200130628",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W3166920165",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2159636675",
    "https://openalex.org/W4283691240",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2997244573",
    "https://openalex.org/W3202070718",
    "https://openalex.org/W3104350794",
    "https://openalex.org/W4280538731",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W4301785137",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3170826848",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W3034763191",
    "https://openalex.org/W2515741950"
  ],
  "abstract": "In this paper, we investigate to which extent contextual neural language models (LMs) implicitly learn syntactic structure. More concretely, we focus on constituent structure as represented in the Penn Treebank (PTB). Using standard probing techniques based on diagnostic classifiers, we assess the accuracy of representing constituents of different categories within the neuron activations of a LM such as RoBERTa. In order to make sure that our probe focuses on syntactic knowledge and not on implicit semantic generalizations, we also experiment on a PTB version that is obtained by randomly replacing constituents with each other while keeping syntactic structure, i.e., a semantically ill-formed but syntactically well-formed version of the PTB. We find that 4 pretrained transfomer LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM. Moreover, we show that a complete constituency tree can be linearly separated from LM representations.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6738–6757\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nProbing for Constituency Structure in Neural Language Models\nDavid Arps† Younes Samih† Laura Kallmeyer† Hassan Sajjad‡\n†Heinrich Heine University D¨usseldorf, Germany\n‡Faculty of Computer Science, Dalhousie University, Canada\n{david.arps,younes.samih,laura.kallmeyer}@hhu.de\nhsajjad@dal.ca\nAbstract\nIn this paper, we investigate to which extent\ncontextual neural language models (LMs) im-\nplicitly learn syntactic structure. More con-\ncretely, we focus on constituent structure as\nrepresented in the Penn Treebank (PTB). Us-\ning standard probing techniques based on di-\nagnostic classifiers, we assess the accuracy of\nrepresenting constituents of different categories\nwithin the neuron activations of a LM such as\nRoBERTa. In order to make sure that our probe\nfocuses on syntactic knowledge and not on im-\nplicit semantic generalizations, we also experi-\nment on a PTB version that is obtained by ran-\ndomly replacing constituents with each other\nwhile keeping syntactic structure, i.e., a seman-\ntically ill-formed but syntactically well-formed\nversion of the PTB. We find that 4 pretrained\ntransfomer LMs obtain high performance on\nour probing tasks even on manipulated data,\nsuggesting that semantic and syntactic knowl-\nedge in their representations can be separated\nand that constituency information is in fact\nlearned by the LM. Moreover, we show that\na complete constituency tree can be linearly\nseparated from LM representations.1\n1 Introduction\nOver the last years, neural language models (LMs),\nsuch as BERT (Devlin et al., 2019), XLNet (Yang\net al., 2019), RoBERTa (Liu et al., 2019b), and\nDistilBERT (Sanh et al., 2020), have delivered un-\nmatched results in multiple key Natural Language\nProcessing (NLP) benchmarks (Wang et al., 2018).\nDespite the impressive performance, the black-box\nnature of these models makes it difficult to ascer-\ntain whether they implicitly learn to encode linguis-\ntic structures, such as constituency or dependency\ntrees.\nThere has been a considerable amount of re-\nsearch conducted on questioning which types of lin-\nguistic structure are learned by LMs (Tenney et al.,\n1Code for our experiments is available at https://\ngithub.com/davidarps/constptbprobing\n2019b; Conneau et al., 2018; Liu et al., 2019a). The\nmotivation behind asking this question is two-fold.\nOn the one hand, we want to better understand\nhow pre-trained LMs solve certain NLP tasks, i.e.,\nhow their input features and neuron activations con-\ntribute to a specific classification success. A second\nmotivation is an interest in distributional evidence\nfor linguistic theory. That is, we are interested\nin assessing which types of linguistic categories\nemerge when training a contextual language model,\ni.e., when training a model only on unlabeled text.\nThe research in this paper is primarily motivated\nby this second aspect, focusing on syntactic struc-\nture, more concretely on constituency structure.\nWe investigate, for instance, for pairs of tokens in\na sentence whether a LM implicitly learns which\nconstituent (NP, VP, . . . ) the two tokens belong to\nas their lowest common ancestor ( LCA). We use\nEnglish Penn Treebank data (PTB, Marcus et al.,\n1993) to conduct experiments.\nA number of studies have probed LMs for de-\npendency structure (Hewitt and Manning, 2019;\nChen et al., 2021) and constituency structure (Ten-\nney et al., 2019b). We probe constituency structure\nfor the following reasons. In contrast to depen-\ndency structure, it can be richer concerning the\nrepresented abstract syntactic information, since it\ndirectly assigns categories to groups of tokens. On\nthe other hand, not all dependency labels are rep-\nresented in a standard constituency structure; but\nthey can be incorporated as extensions of the cor-\nresponding non-terminal nodes (see, e.g., the PTB\nlabel NP-SBJ in App. A.1). To quantify the gain\nthat we get from probing constituency rather than\ndependency trees, we compare the unlabeled brack-\netings in the syntactic trees in both formalisms on\nthe PTB (Marcus et al., 1993; de Marneffe et al.,\n2006), where an unlabeled bracketing is the yield\nof a subtree. We find that while 97% of the brack-\netings in a dependency tree are also present in a\nconstituency tree, only 54% of the bracketings in\n6738\nthe constituency tree are present in the dependency\ntree. This shows that constituent trees can contain\nmuch more fine-grained hierarchical information\nthan dependency trees. A further reason for focus-\ning on constituency structure is that this is the type\nof structure most linguistic theories use.\nWe use diagnostic classifiers (Hupkes et al.,\n2018) and perform model-level, layer-level and\nneuron-level analyses. Most work on diagnostic\nclassifiers performs mean pool over representa-\ntions when probing for a relationship between two\nwords (Durrani et al., 2020). We empirically show\nthat mean pool results in lossy representation, and\nwe recommend concatenation of representations as\na better way to probe for relations between words.\nA difficulty when probing a LM for whether cer-\ntain categories are learned is that we cannot be sure\nthat the LM does not learn a different category in-\nstead that is also predictive for the category we\nare interested in. More concretely, when probing\nfor syntax, one should make sure that it is not se-\nmantics that one finds and considers to be syntax\n(since semantic relations influence syntactic struc-\nture). This point was also observed by Gulordava\net al. (2018) and, more recently, Hall Maudslay and\nCotterell (2021). Therefore, before probing the LM\nfor syntactic relations, we manipulate our data by\nreplacing a subset of tokens with other tokens that\nappear in similar syntactic contexts, thereby ob-\ntaining nonsensical text that still has a reasonable\nsyntactic structure. We then conduct a series of\nexperiments that show that even for these nonsen-\nsical sentences, contextual LMs implicitly repre-\nsent constituency structure. Lastly, we questioned\nwhether a full syntactic tree can be reconstructed\nusing the linear probe. We achieve a labeled F1\nscore of 82.6% for RoBERTa when probing on the\nnon-manipulated dataset in comparison to 51.4%\nwith a random representation baseline.\nThe contributions of our work are as follows:\n• We find that constituency structure is linearly\nseparable at various granularity levels: At the\nmodel level, we find that four different LMs\nachieve similar overall performance on our\nsyntactic probing tasks, but make slightly dif-\nferent predictions. At the layer level, the mid-\ndle layers achieve the best results. At the neu-\nron level, syntax is heavily distributed across\nneurons.\n• We use perturbed data to separate the effect\nof semantics when probing for syntax, and\nwe find that different sets of neurons capture\nsyntactic and semantic information.\n• We show that a simple linear probe is effec-\ntive in analyzing representations for syntac-\ntic properties and we show that a full con-\nstituency tree can be linearly separated from\nLM representations.\nThe rest of the paper is structured as follows.\nThe next section introduces related work. We de-\nfine our linguistic probing tasks in Sec. 3. Sec. 4\nintroduces our experimental methodology. Sec. 5,\n6, and 7 discuss our experiments and their results,\nand Sec. 8 concludes.\n2 Related work\nSyntactic information in neural LMs An im-\nportant recent line of research (Adi et al., 2017;\nHupkes et al., 2018; Zhang and Bowman, 2018;\nBlevins et al., 2018; Hewitt and Manning, 2019;\nHewitt and Liang, 2019; Reif et al., 2019; Tenney\net al., 2019a; Manning et al., 2020; Hall Maudslay\net al., 2020; Li et al., 2020; Newman et al., 2021;\nHewitt et al., 2021; Belinkov, 2022; Sajjad et al.,\n2022b; Dalvi et al., 2022) has focused on finding la-\ntent hierarchical structures encoded in neural LMs.\nA common line of work on interpreting models use\na probing classifier to gauge the amount of linguis-\ntic knowledge learned in the representation (Alain\nand Bengio, 2016; Belinkov et al., 2020; Conneau\net al., 2018).\nAn ample body of research exists on probing\nthe sub-sentential structure of contextualized word\nembeddings. Peters et al. (2018) probed neural net-\nworks to see to what extent span representations\ncapture phrasal syntax. Tenney et al. (2019b) de-\nvised a set of edge probingtasks to get new insights\non what is encoded by contextualized word embed-\ndings, focusing on the relationship between spans\nrather than individual words. This enables them to\ngo beyond sequence labeling problems to syntactic\nconstituency, dependencies, entity labels, and se-\nmantic role labeling. Their results on syntactic con-\nstituency are in line with our findings. The major\ndifference is that we employ simpler probes while\nachieving similar results. Moreover, we separate\nthe effect of semantics using corrupted data and we\nreconstruct full constituent trees using our probing\nsetup. Most recently, Wu et al. (2020) propose a\nparameter-free probing technique to analyze LMs\nvia perturbed masking. Their approach is based on\n6739\naccessing the impact that one word has on predict-\ning another word within a sequence in the Masked\nLanguage Model task. They have also shown that\nLMs can capture syntactic information with their\nself-attention layers being capable of surprisingly\neffective learning.\nHewitt and Manning (2019) demonstrated, by\nusing a structural probe, that it is possible to find a\nlinear transformation of the space of the LM’s ac-\ntivation vectors under which the distance between\ncontextualized word vectors corresponds to the dis-\ntance between the respective words in the depen-\ndency tree. In a similar vein, Chen et al. (2021) in-\ntroduced another structural probe, Poincar´e probe\nand have shown that syntactic trees can be better\nreconstructed from the intermediate representation\nof BERT in a hyperbolic subspace.\nSyntactic and semantic knowledge Gulordava\net al. (2018) and Hall Maudslay and Cotterell\n(2021) recently argued that the work on probing\nsyntax does not fully separate the effect of seman-\ntics while probing syntax. Both modify datasets\nsuch that the sentences become semantically non-\nsensical while remaining syntactically well-formed\nin order to assess, based on this data, whether a\nLM represents syntactic information. Gulordava\net al. (2018) modify treebanks in four languages by\nreplacing content words with other content words\nthat have matching POS and morphological fea-\ntures. They focus on the question if agreement\ninformation in the nonce sentences can be recov-\nered from RNN language models trained on regular\ndata. Hall Maudslay and Cotterell (2021) replaced\nwords with pseudowords in a dependency treebank,\nand quantified how much the pseudowords affect\nthe performance of syntactic dependency probes.\nWe followed a similar setup to separate out the\neffect of syntax from semantics. In contrast to\nHall Maudslay and Cotterell (2021), we replace\nwords with other words (not pseudowords) that oc-\ncur in a similar syntactic context but are different\nsemantically. This way, the LM has seen most or\nall words from the semantically nonsensical sen-\ntences in pretraining and has learned their syntactic\nproperties.\nFine-grained LM analysis Durrani et al. (2020)\nused a unified diagnostic classifier approach to\nperform analyses at various granularity levels, ex-\ntending Dalvi et al. (2019a). We follow their ap-\nproach and perform model-level, layer-level and\nneuron-level analyses (Sajjad et al., 2022a). We\nadditionally extend their approach by proposing\nan improved way to probe representations of two\nwords. Previous work has mainly used a bilin-\near probe to investigate syntax. We select a linear\nmodel for our experiments. Selecting a weak model\nensures that the representations learn the linguis-\ntic property, and the probe is not relying on the\nstrength of the classification model used.\n3 Diagnostic Tasks and Constituency\ntrees\nIn this section, we define three classification tasks\nthat are aimed at making different properties of con-\nstituency structure explicit. More specifically, the\ngoal of these tasks is to make explicit if and how the\nLMs encode syntactic categories, such as S, NP,\nVP, PP. The first task, lowest common ancestor\nprediction, focuses on constituents that span large\nportions of the sentence. The second task, chunk-\ning, focuses on constituents with smaller spans.\nThe third task focuses on complete syntactic trees.\nLowest common ancestor (LCA) prediction Let\ns = w0,...,w n be a sentence. Given combined\nrepresentations of two tokens wi,wj with j ≥i,\npredict the label of their lowest common ances-\ntor in the constituency tree. LCA prediction is a\nmulticlass classification task with 28 target classes\n(for the PTB). In the example in Fig. 1, luxury and\nmaker have the LCA NP: the lowest node dominat-\ning both words has label NP (ignoring the function\ntag SBJ). The task also predicts LCA of two iden-\ntical tokens. In this case, the lowest phrasal node\nabove the token is the target label (for example,VP\nis the target label for sold).\nChunking For each token wi, predict whether it is\nthe beginning of a phrase (B), inside a phrase (I),\nthe end of a phrase (E) or if the token constitutes\na single-token phrase (S). A token can be part of\nmore than one phrase, and in this case we consider\nthe shortest possible phrase only. For example,\n1,214 in Fig. 1 has label B because it marks the\nbeginning of a noun phrase. We also propose a\nversion of this task with finer labels that combine\nB,I,E,S with the different phrase labels. In the\ndetailed tagset, 1,214 receives the label B-NP.\nReconstructing full constituent trees Vilares et al.\n(2020) considered constituency parsing as a multi-\nlabel sequence labeling problem. For each token\nwi, three labels are predicted: First, the label of\nthe LCA of the token pair (wi,wi+1). Second, the\n6740\nS\nVP\nPP-LOC\nNP\nNNP\nU.S.\nE\n(·,·)\nDT\nthe\nB\n(NP,1)\nIN\nin\nB\n(PP,1)\nNP\nNNS\ncars\nE\n(VP,-1)\nCD\n1,214\nB\n(NP,1)\nVBD\nsold\nB\n(VP,1)\nNP-TMP\nNN\nyear\nE\n(S,ROOT)\nJJ\nlast\nB\n(NP,1)\nNP-SBJ\nNN\nmaker\nE\n(S,ROOT)\nNN\nauto\nI\n(NP,0)\nNN\nluxury\nI\n(NP,0)\nDT\nThe\nB\n(NP,2)\nFigure 1: Example tree from the PTB. The line below the text shows gold labels for the simple chunking task. The\nbottom line shows label pairs from which the complete tree can be reconstructed.\ndepth of the LCA of (wi,wi+1) in the tree, rela-\ntive to the depth of (wi−1,wi). Third, if the first\ntoken is a single-word constituent, and the label of\nthe internal tree node directly above wi. (Tokens\nin multiword constituents make up >90% of the\ndata and receive a negative label.). For the first two\nclassifications, see Fig. 1. We build separate lin-\near classifiers for each of these tasks and use their\npredictions to reconstruct full constituent trees.\n4 Methods\nDiagnostic classification A common method to\nreveal linguistic representations learned in contex-\ntualised embeddings is to train a classifier, a probe,\nusing the activations of the trained LM as features.\nThe classifier performance provides insights into\nthe strength of the linguistic property encoded in\ncontextualised word representations. For all our ex-\nperimental setups, we employ the NeuroX toolkit\n(Dalvi et al., 2019b) for diagnostic classification,\nas it confers several mechanisms to probe neural\nmodels on the level of both neurons and layers.\nLayer-level probing We probe the activations of\nindividual layers with linear classifiers to measure\nthe linear separability of the syntactic categories at\nthis layer. The performance at each layer serves as\na proxy to how much information it encodes with\nrespect to a given syntactic property.\nNeuron-level probing Layer-wise probing cannot\naccount for all syntactic abstractions encoded by\nindividual neurons in deep networks. Some groups\nof neurons that are spread across many layers might\nrobustly respond to a given linguistic property with-\nout being exclusively specialized for its detection.\nBy operating also at the level of the neurons, we\naim at separating the most salient neurons across\nthe network that learn a given linguistic property.\nWe conduct a linguistic correlation analysis, as\nproposed by Dalvi et al. (2019a). This consists in\naugmenting the linear classifier with elastic net reg-\nularization (Zou and Hastie, 2005). The classifier\nis then trained by minimizing the loss function in\nEq. 1:\nL(θ) = −∑\nj log Pθ(tsj |sj)\n+λ1∥θ∥1 + λ2∥θ∥2\n2\n(1)\nwhere (θ) are the trained weights of the classifier\nand λ1∥θ∥1 and λ2∥θ∥2\n2 correspond to L1 and L2\nregularization. Elastic net regularization strikes a\nbalance between selecting very focused features\n(neurons) ( L1) versus distributed features ( L2)\nshared across many properties. The input neurons\nto the linear classifier are ranked by saliency with\nrespect to the classification task.\nInput Representation We combine the represen-\ntation vectors xi,xj ∈Rr of two tokens in LCA\nprediction and parse tree reconstruction via con-\ncatenation (concat(xi,xj) ∈R2r). In all experi-\nments, concatproduced significantly better results\nthan elementwise averaging or a variant of the max-\nimum. We cover the latter methods in Apps. A.3\nand A.6.\n5 Experimental Setup\n5.1 Data\nWe use data from the English Penn Treebank (PTB,\nMarcus et al., 1993) for all our experiments. As\npreprocessing, we remove punctuation and null el-\nements from the trees. The original dataset makes\nuse of fine-grained category labels that consist of\nthe syntactic category and function tags. Function\ntags indicate grammatical (such as SBJ for sub-\nject) or adverbial (LOC for locative) information.\n6741\nFor chunking, the label distribution is relatively\nbalanced. For LCA prediction, we remove all func-\ntion tags to keep the number of target labels small.\nMost of the token pairs have a relatively large dis-\ntance in the constituent tree, and their LCA is a\nnode very high in the tree (typically with a label\nfor some kind of sentence, such as S or SBAR). In\naddition, some phrase labels are less frequent than\nothers (see App. A.4). We train and evaluate on the\nstandard PTB training/development split.\n5.1.1 Syntactic and semantic knowledge\nTo ensure that the probing classifier captures syn-\ntactic properties and not semantic properties, we\nuse the original PTB data as well as two modified\nversions of the PTB with semantically nonsensical\nsentences that have the same syntactic structure as\nthe original data. The modified versions of the PTB\nare obtained by making use of the dependency PTB\n(de Marneffe et al., 2006):\n1. Record the dependency context of each token\nin the dataset. The dependency context con-\nsists of (i) the POS tag, (ii) the dependency\nrelation of the token to its head, and (iii) the\nlist of dependency relations of the token to its\ndependents.\n2. Replace a fraction of tokens with other tokens\nthat also appear in the dataset in the same\ndependency context.\nTwo versions are created, replacing either a third\nor two thirds of the tokens. See Table 1 for two\nexamples. When creating manipulated datasets,\nwe separate the training and evaluation data. To\ncreate manipulated training data, we look for token\nreplacements in the training split of the PTB (PTB\nsections 0-18). For manipulated evaluation data,\nwe look for token replacements in the development\nand test splits of the PTB (sections 19-24). This\nensures that the training and evaluation data do not\nmix, and at the same time, meanings of the newly\ncreated sentences are as diverse as possible.\n5.2 Probing Classifier Settings\nWe use linear classifiers trained for 10 epochs with\nAdam optimization, an initial learning rate of0.001\nand regularization parameters λ1 = λ2 = 0.001.\nThe contextualized representations for an input to-\nken is created by averaging the representations of\nits subword tokens of the LM tokenizer.\n5.3 Transformer Models\nWe train structural probes on three 12-layered pre-\ntrained transformer models; the base versions of\nBERT (uncased, Devlin et al., 2019), XLNet (cased,\nYang et al., 2019), and RoBERTa (Liu et al., 2019b),\nand a 6-layered model; DistilBERT (uncased, Sanh\net al., 2020). This provides an opportunity to com-\npare the syntactic knowledge learned in models\nwith different hyperparameter settings and pretrain-\ning objectives.\n5.4 Baselines\nWe use three baselines to put the results of our\nprobes into context.\nRandom BERT The first baseline is used in all\nexperiments. It evaluates how much information\nabout linguistic context is accumulated in the LM\nduring pretraining (Belinkov, 2022). The model\nfor this baseline has the same neural architecture,\nvocabulary and (static) input embeddings as BERT\nbase, but all transformer weights are randomized.\nSelectivity To evaluate if the the probe makes lin-\nguistic information explicit or just memorizes the\ntasks, we use the control task proposed by He-\nwitt and Liang (2019) and described in App. A.2.\nThe difference between control task performance\nand linguistic task performance is called selectivity.\nThe higher the selectivity, the more one can be sure\nthat the classifier makes linguistic structure inside\nthe representations explicit and does not memorize\nthe task. This baseline is used for the chunking and,\nin a modified version, the LCA experiments.\nIndividual tokens This baseline evaluates how\nmuch the representation of each token, in contrast\nto token pairs, contributes to the overall perfor-\nmance. This baseline is used only for the LCA\nexperiments. We train two classifiers using the rep-\nresentation of either the first token in the pair or\nthe second token and evaluate the performance on\nthe diagnostic tasks. The trees in the PTB are right-\nbranching. The left token in most cases is closer to\nthe LCA node than the right token, thus we expect\nthat the classifier trained on only the left token has\na better overall performance.\n6 Results for LCA prediction and\nchunking\nWe experimented using four pretrained models.\nDue to the limited space and the consistency of\nresults, we reported the analysis for the RoBERTa\nmodel only in most of the cases. The complete re-\n6742\norig. Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.\n.33 Pierre Berry, 5,400 years old, shall join the board as a nonexecutive director Nov. 29.\n.67 Mesnil Vitulli, 9.76 beers state-owned, ca succeed either board as a cash-rich director October 213,000.\norig. Mr. Vinken is chairman of Elsevier N.V ., the Dutch publishing group.\n.33 Mr. Vinken is growth without Elsevier Hills, each Dutch publishing group.\n.67 Tata Helpern s chairman plus Elsevier Ohls, a Dutch snaking group.\nTable 1: Examples for the manipulated data. Replaced words are printed in boldface.\ntrain/test RoBERTa\ntask sel. ∆Random\nLCA\nconcat\norig./orig. 82.8 58.3 23.4\n.33/orig. 81.1 62.2 27.3\n.33/.33 79.7 56.8 25.6\n.67/orig. 79.3 61.5 25.3\n.67/.67 77.4 59.9 23.1\nchunking\nsimple\norig./orig. 96.0 13.7 22.2\n.33/orig. 95.3 15.0 26.2\n.33/.33 94.4 13.1 25.8\n.67/orig. 94.5 14.2 23.6\n.67/.67 93.3 13.6 23.2\nchunking\ndetailed\norig./orig. 91.2 9.4 32.3\n.33/orig. 90.5 10.4 32.8\n.33/.33/ 89.2 10.1 33.1\n.67/orig. 89.3 8.8 36.5\n.67/.67 87.1 6.7 36.1\nTable 2: Results on different datasets. For each task,\nthere are different setups where the model is trained and\nevaluated on the unchanged treebank (orig.), and two\nversions with either a third (0.33) or two thirds (0.67)\nof the tokens replaced. ’task’ shows the performance on\ntest set. ’sel.’ shows the selectivity (difference to control\ntask), and ∆Random shows the performance difference\nto the Random BERT model.\nsults of all models are shown in appendix sections\nA.5 and A.6. In the following, we assess the over-\nall performance of the probing classifiers on both\nlinguistic tasks. Then, we evaluate how changing\nthe semantic structure of the data influences the\nprobing classifiers. Lastly, we show some insights\ninto layer-level and neuron-level experiments.\n6.1 Overall performance\nTab. 2 shows the performance of the classifiers\ntrained on non-manipulated data using all neurons\nof the network (orig./orig.). We observed high per-\nformance for each diagnostic task. The differences\nto the baselines show that the knowledge about the\ntask is indeed learned in the representation.\nLCA prediction The best results are achieved\nwhen concatenating token representations (82.8%\nacc.). For other representation methods, see\nApp. A.6. We additionally consider single word\nrepresentations from the word pair as input. The\nleft token representations are better predictors\n(66.5% acc. on orig./orig.) than those from the\nright token (40.8%). The large differences between\nconcatand the baselines shows that the probe is not\nmemorizing the task, and that information relevant\nfor predicting LCA is acquired during pretraining.\nChunking Chunking detailed (91.2% acc.) is a\nharder task than chunking simple (96.0%). Al-\nthough the classifier for the detailed tagset shows\nrelatively low selectivity in comparison to chunk-\ning simple, the overall selectivity is high enough to\nclaim that the knowledge about these probing tasks\nis learned in the representation. The difference to\nthe random BERT model is higher for chunking de-\ntailed than for chunking simple, which shows that\nfine-grained syntactic knowledge is indeed learnt\nduring pretraining.\n6.2 Does the probe learn syntax or semantics?\nThe high performance of the classifiers serves as a\nproxy to the amount of syntactic knowledge learned\nin the representations. But Hall Maudslay and Cot-\nterell (2021) argued that due to the presence of\nsemantic cues in the data, high performance of a\nsyntactic probe may not truly reflect the learning\nof syntax in the model. To investigate this, we\nmanipulated our diagnostic task data (Sec. 5.1) to\nseparate syntax from semantics, and then trained\nthe probing classifiers on the manipulated data.\nThe second column in Table 2 shows variations\nof the manipulated data. The classification per-\nformance dropped slightly on the diagnostic tasks\nat 0.33/orig. Moreover, the classifier performed\nslightly better when evaluating on original data\n(∗/orig.) compared to manipulated data (such as\n.33/.33). There are two possible reasons for this:\nFirst, the probing classifiers may still rely on se-\nmantic knowledge, even when trained on the ma-\nnipulated data; second, it is possible that the manip-\nulated data contains syntactically ill-formed sen-\ntences. Nonetheless, performance and differences\nto baselines are reasonably high and give good rea-\nson to believe that the classifiers are able to extrap-\nolate syntactic knowledge even from semantically\nnonsensical data. We now proceed with summa-\n6743\n0 2 4 6 8 10 120.6\n0.7\n0.8\n0.9\n1.0\nLCA\nchunking simple\nchunking detailed\n0 2 4 6 8 10 120.6\n0.7\n0.8\n0.9\n1.0\nLCA\nchunking simple\nchunking detailed\nFigure 2: Acc. of layer-wise retraining probing classi-\nfiers on RoBERTa (left) and BERT (right) representa-\ntions for non-manipulated data.\n0.01 0.10 0.20 0.30 0.40 0.500.6\n0.7\n0.8\n0.9\n1.0\ntop\nrandom\nbot\n0.01 0.10 0.20 0.30 0.40 0.500.6\n0.7\n0.8\n0.9\n1.0\ntop\nrandom\nbot\nFigure 3: Acc. for simple (left) and detailed (right)\nchunking tagsets, on top, random and bottom neurons\nfor RoBERTa on non-manipulated data. The horizontal\naxis plots the fraction of neurons that is selected from\nall layers.\nrizing what our experiments tell about syntactic\nknowledge in specific layers/neurons of the LM.\n6.3 Layer-wise results\nFig. 2 shows how syntactic knowledge is dis-\ntributed across all layers. The embedding layer\nperformed worse while middle layers showed the\nbest results, i.e., syntactic information is better rep-\nresented in the middle layers. The highest layers\nare more heavily influenced by the pretraining ob-\njective, which explains the consistent performance\ndrop across models and tasks on the last layers.\nComparing layer-wise performance with the\noverall performance, none of the individual lay-\ners outperformed the classifier trained on all lay-\ners for chunking. In the case of LCA prediction,\nthe performance of layers 4-5 in RoBERTa (6-8 in\nBERT) are better than the overall performance on\nall layers. Comparing models, we observed that\nRoBERTa learns the syntactic knowledge much\nearlier in the network compared to BERT (see the\nrelatively sharp rise of performance in the lower\nlayers of RoBERTa).\n6.4 Neuron-level results\nIn this section we carry out a more fine-grained\nneuron-level analysis of the representations. Lin-\nguistic correlation analysis (Dalvi et al., 2019a)\nprovides a ranking of neurons with respect to the\ndiagnostic task.\n0 2 4 6 8 10 120\n20\n(a) BERT\n0 2 4 6 8 10 120\n20\n(b) XLNet\n0 2 4 6 8 10 120\n20\n(c) RoBERTa\nFigure 4: Spread of neurons relevant for recognizing S\nin LCA prediction, across layers.\n0 2 4 6 8 10 120\n20\nFigure 5: Spread of neurons in RoBERTa across layers\nthat are relevant for identifying NPs in LCA prediction\nMinimum Subset of Neurons We evaluated\nthe neuron ranking by training classifiers using\ntop/bottom/random N% neurons. Fig. 3 shows the\naccuracy curves for the chunking task. The perfor-\nmance margin between different selected neurons\nis very low. This shows that syntactic information\ncan be extracted from any relatively small subset of\nneurons i.e. 20−30% of neurons suffice for a prob-\ning classifier to perform with the same accuracy as\nwhen trained on full representations. Neuron rank-\ning on combined representations does not work\nwell: In some cases, performance on a fraction of\nrandomly selected neurons is worse than perfor-\nmance on the same fraction of neurons ranked as\nimportant (see App. A.7).\nDistribution of Neurons for LCA prediction\nTraining on subsets of the neurons for LCA predic-\ntion is problematic, because the neuron ranking list\ncontains neurons from both token representations.\nEven though, the distribution of salient neurons\nacross layers yields interesting insights. Fig. 4\npresents the spread of top selected neurons for S.\nAs in Sec. 6.3, we found again that top neurons\nlearning syntactic properties come from the mid-\ndle layers. For 12-layer LMs, we see a trend that\nneurons from the positional encoding in the em-\nbedding layer are utilized to identify distant tokens\nwith LCA S. When comparing the salient neurons\nselected from each layer, we observe that for iden-\ntifying S, neurons from the highest layers are less\nrelevant than when identifying NPs (Fig. 5). This\n6744\nmight be due to the comparatively high structural\ndiversity we find in NPs.2\nNeurons learning Syntax vs. Semantics Com-\nparing the neuron rankings of chunking classifiers\ntrained on the different datasets shows that there is\nrelatively little overlap between the different groups\nof highest-ranking neurons (see App. A.8). This\nmeans that the probing classifiers focus on different\nneurons when training on manipulated data, com-\npared to the original data. Presumably, the probe\nfocuses more on syntactic and less on semantic\ninformation when trained on manipulated data.\n7 Reconstructing full Parse Trees\nWith the insights gained in the previous section, we\ntest if full constituency trees can be linearly sepa-\nrated from LM representations. For this, we train 3\nlinear classifiers that take as input the concatenated\nrepresentations of two adjacent tokens and predict\nthe three labels described in Sec. 3. The classifiers\nfor this task take as input not the full LM representa-\ntion from all layers, but instead the concatenations\nof every third layer for the 12-layer LMs, and every\nsecond layer for the 6-layer LM. This way, the in-\nput dimensionality of the classifier is restricted, but\nthe probe can use information from different parts\nof the LM. The probe is trained (evaluated) on all\n38k (5.5k) sentences of the training (development)\nsplit of the PTB. We find that the constituency trees\nreconstructed from different LMs are of high qual-\nity (Tab. 3, App. A.9). We achieve a labeled F1\nscore of 82.6 on the non-manipulated dataset for\nRoBERTa (80.5 for XLNet, 80.4 for BERT) which\nis 31 points better than the random BERT base-\nline. This outperforms the result from Vilares et al.\n(2020) for BERT by 2.2 points. They also use a\nlinear classifier, but their classifier receives as input\nonly the final layer representation of BERT for the\nfirst token in the token pair.\nWhen comparing trees reconstructed from differ-\nent LMs against each other, we find however that\nthey are quite different. For example, comparing\nthe sentence-level F scores for trees reconstructed\nfrom XLNet to those from RoBERTa yields a Pear-\nson correlation of 0.52 only (compared to 0.64 for\nDistilBERT and BERT, see App. A.10 for the full\ncomparison). This shows that different syntactic\nproperties are linearly separable from the represen-\n2All models are more accurate in LCA prediction when\nthe two tokens are more distant, see App. A.6. Large distance\nbetween tokens correlates with LCA nodes close to the root\nof the syntactic tree, where the LCA often has label S.\nRoBERTa ∆Random\norig. 82.6 31.2\n.33/orig. 80.9 31.3\n.33/.33 77.8 31.7\n.67/orig. 78.3 30.4\n.67/.67 72.8 28.1\nTable 3: Labeled F1 scores for all datasets for recon-\nstructing full parse trees\ntations of different LMs. These results are not a\nshortcoming of our probe. We reconstructed parse\ntrees from RoBERTa for the same dataset twice,\nand a comparison of the two sets of trees gave a\nlabeled F1 score of 96.3. We conclude from this\nthat LMs trained on different data and towards dif-\nferent objectives, such as RoBERTa and XLNet,\nimplicitly make different syntactic generalizations.\nThis insight might have implications for parsing\n(which is not in the scope of our paper): combining\nembeddings from both LMs might improve parsing\nresults, compared to using just one LM, as usually\ndone.\n8 Conclusions\nOur experiments have shown that different pre-\ntrained LMs encode fine-grained linguistic infor-\nmation that is also present in constituency trees.\nMore specifically, LMs are able to identify proper-\nties of different types of constituents, such as S, NP,\nVP, PP. Good results on the chunking task show\nthat the classifiers are able to combine knowledge\nabout the kind of constituents that a token is part\nof, and knowledge of the position of a token in\nthe constituent. Using the sequence labeling tasks\npresented in Vilares et al. (2020), we have shown\nthat full constituency trees are linearly separable\nfrom four different pretrained LMs with high qual-\nity - even for semantically nonsensical data. In line\nwith Gulordava et al. (2018), we observe a mod-\nerate performance drop between performance on\nthe original and nonce dataset. The performance\ndrop is smaller than in Hall Maudslay and Cot-\nterell (2021). who use English pseudowords which\nthe LM has probably never encountered. We use\nEnglish words whose syntactic and semantic prop-\nerties are already well-established inside the LM.\nIn future work, we plan to extend this syntac-\ntic probing approach to other languages and other\nsyntactic annotation schemes (for instance Hock-\nenmaier and Steedman, 2007; Evang et al., 2021).\n6745\nLimitations\nOur work investigates the question whether syntac-\ntic structure is linearly separable from LM repre-\nsentations. However, we make no claim about the\nquestion if the syntactic concepts we probe for are\nactually relevant for LM predictions.\nWe demonstrate the effectiveness of our method\nfor one high-resource language, namely English.\nWhile our methodology is in principle language-\nagnostic, our study requires high-performing LMs\nas well as large amounts of annotated data. Both\nare available for only a relatively small set of lan-\nguages. More specifically, we found in pilot ex-\nperiments that supervised probing in general and\nseparating syntactic and semantic knowledge in\nparticular is very data-hungry. While high probe\nperformance on the original data required less than\n10k sentences of training data, the performance\ndifference between original and semantically ma-\nnipulated data shrank with increasing the size of the\ntraining data from 10k sentences to 38k sentences.\nConsequently, in order to have reliable findings, our\nexperiments require large datasets which reflects\ninto the need of sufficient computational resources.\nFor a general discussion of the limitations of su-\npervised probing classifiers, we refer to (Belinkov,\n2022).\nAcknowledgments\nThe work presented in this paper was partly funded\nby the Deutsche Forschungsgemeinschaft (DFG)\nwithin the project ”Unsupervised Frame Induction:\nEvent Type Hierarchies and Complex Event Types\n(FInd)”. We would like to thank anonymous re-\nviewers for their helpful feedback.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2017. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. In International Conference on Learning\nRepresentations.\nGuillaume Alain and Yoshua Bengio. 2016. Under-\nstanding intermediate layers using linear classifier\nprobes. arXiv preprint arXiv:1610.01644.\nYonatan Belinkov. 2022. Probing classifiers: Promises,\nshortcomings, and advances. Computational\nLinguistics, 48(1):207–219.\nYonatan Belinkov, Nadir Durrani, Hassan Sajjad, Fahim\nDalvi, and James Glass. 2020. On the linguistic\nrepresentational power of neural machine translation\nmodels. Computational Linguistics, 46(1).\nTerra Blevins, Omer Levy, and Luke Zettlemoyer.\n2018. Deep RNNs encode soft hierarchical syntax.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (V olume\n2: Short Papers), pages 14–19, Melbourne, Australia.\nAssociation for Computational Linguistics.\nBoli Chen, Yao Fu, Guangwei Xu, Pengjun Xie,\nChuanqi Tan, Mosha Chen, and Liping Jing. 2021.\nProbing BERT in hyperbolic spaces.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Prob-\ning sentence embeddings for linguistic properties.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (V olume\n1: Long Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, D. Anthony Bau, and James Glass. 2019a.\nWhat is one grain of sand in the desert? analyzing in-\ndividual neurons in deep nlp models. In Proceedings\nof the AAAI Conference on Artificial Intelligence\n(AAAI).\nFahim Dalvi, Abdul Rafae Khan, Firoj Alam, Nadir\nDurrani, Jia Xu, and Hassan Sajjad. 2022. Discover-\ning latent concepts learned in BERT. In International\nConference on Learning Representations.\nFahim Dalvi, Avery Nortonsmith, D Anthony Bau,\nYonatan Belinkov, Hassan Sajjad, Nadir Durrani, and\nJames Glass. 2019b. Neurox: A toolkit for analyzing\nindividual neurons in neural networks. Proceedings\nof the AAAI Conference on Artificial Intelligence\n(AAAI).\nMarie-Catherine de Marneffe, Bill MacCartney, and\nChristopher D. Manning. 2006. Generating typed de-\npendency parses from phrase structure parses. In\nProceedings of the Fifth International Conference\non Language Resources and Evaluation (LREC’06),\nGenoa, Italy. European Language Resources Associ-\nation (ELRA).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nNadir Durrani, Hassan Sajjad, Fahim Dalvi, and\nYonatan Belinkov. 2020. Analyzing individual neu-\nrons in pre-trained language models. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\n6746\nKilian Evang, Tatiana Bladier, Laura Kallmeyer, and\nSimon Petitjean. 2021. Bootstrapping Role and\nReference Grammar treebanks via Universal Depen-\ndencies. In Proceedings of Universal Dependencies\nWorkshop 2021 (UDW 2021). To appear.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Color-\nless green recurrent networks dream hierarchi-\ncally. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long Papers), pages 1195–\n1205, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\nRowan Hall Maudslay and Ryan Cotterell. 2021. Do\nsyntactic probes probe syntax? experiments with\njabberwocky probing. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 124–131, Online. As-\nsociation for Computational Linguistics.\nRowan Hall Maudslay, Josef Valvoda, Tiago Pimentel,\nAdina Williams, and Ryan Cotterell. 2020. A\ntale of a probe and a parser. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7389–7395, On-\nline. Association for Computational Linguistics.\nJohn Hewitt, Kawin Ethayarajh, Percy Liang, and\nChristopher Manning. 2021. Conditional probing:\nmeasuring usable information beyond a baseline. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages\n1626–1639, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2733–2743,\nHong Kong, China. Association for Computational\nLinguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 4129–4138, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nJulia Hockenmaier and Mark Steedman. 2007. CCG-\nbank: A corpus of CCG derivations and depen-\ndency structures extracted from the Penn Treebank.\nComputational Linguistics, 33(3):355–396.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and’diagnostic classifiers’ re-\nveal how recurrent and recursive neural networks\nprocess hierarchical structure. Journal of Artificial\nIntelligence Research, 61:907–926.\nHuayang Li, Lemao Liu, Guoping Huang, and Shuming\nShi. 2020. On the branching bias of syntax extracted\nfrom pre-trained language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 4473–4478, Online. Association for\nComputational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a.\nLinguistic knowledge and transferability of contex-\ntual representations. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, V olume1 (Long and Short\nPapers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nChristopher D Manning, Kevin Clark, John Hewitt, Ur-\nvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artificial neural net-\nworks trained by self-supervision. Proceedings of\nthe National Academy of Sciences, 117(48):30046–\n30054.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated cor-\npus of English: The Penn Treebank. Computational\nLinguistics, 19(2):313–330.\nBenjamin Newman, Kai-Siang Ang, Julia Gong, and\nJohn Hewitt. 2021. Refining targeted syntactic evalu-\nation of language models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3710–3723, Online.\nAssociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018. Dissecting contextual word\nembeddings: Architecture and representation. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages\n1499–1509, Brussels, Belgium. Association for Com-\nputational Linguistics.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nHassan Sajjad, Nadir Durrani, and Fahim Dalvi. 2022a.\nNeuron-level Interpretation of Deep NLP Models:\nA Survey. Transactions of the Association for\nComputational Linguistics.\n6747\nHassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj\nAlam, Abdul Rafae Khan, and Jia Xu. 2022b. An-\nalyzing encoded concepts in transformer language\nmodels. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics, NAACL ’22, Seattle,\nWashington, USA. Association for Computational\nLinguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n4593–4601, Florence, Italy. Association for Com-\nputational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim, Ben-\njamin Van Durme, Sam Bowman, Dipanjan Das, and\nEllie Pavlick. 2019b. What do you learn from con-\ntext? probing for sentence structure in contextualized\nword representations. In International Conference\non Learning Representations.\nDavid Vilares, Michalina Strzyz, Anders Søgaard, and\nCarlos G´omez-Rodr´ıguez. 2020. Parsing as pretrain-\ning. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 9114–9121.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. CoRR,\nabs/1804.07461.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed masking: Parameter-free probing for an-\nalyzing and interpreting BERT. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4166–4176, On-\nline. Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems, volume 32. Curran\nAssociates, Inc.\nKelly Zhang and Samuel Bowman. 2018. Language\nmodeling teaches you more than translation does:\nLessons learned through auxiliary syntactic task anal-\nysis. In Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 359–361, Brussels, Bel-\ngium. Association for Computational Linguistics.\nHui Zou and Trevor Hastie. 2005. Regularization and\nvariable selection via the elastic net. Journal of\nthe Royal Statistical Society. Series B (Statistical\nMethodology), 67(2):301–320.\n6748\nA Appendix\nA.1 Constituency and Dependency Trees\nHere, you see the constituency tree (above the sentence) and the dependency tree (below) for a simple\nsentence, following the annotation scheme of the PTB and its dependency version. More fine-grained\nhierarchical structure is encoded in the node labels of the constituency structure. For example, the\nconstituency tree assigns the label VP to the spans walking on the moonand am walking on the moon.\nHowever, these spans are not reflected by particular entities in the dependency tree.\nS\nVP\nVP\nPP-LOC\nNP\nNN\nmoon\nDT\nthe\nIN\non\nVBG\nwalking\nVBP\nam\nNP-SBJ\nPRP\nI\nROOT\naux\nnsubj\ndobj\ndet\ncase\nA.2 Control tasks and Selectivity\nTo ascertain that our structural probe learns complex structural generalization and does not memorize\nstructural information from the task, following Hewitt and Liang (2019), we design two control tasks\n(CT). This consists in randomizing the labels of syntactic categories, creating a new dataset. In our\nimplementation, the distribution of randomly selected labels approximates the class distribution in the\ntraining set. For chunking, a numerical target label is assigned randomly to each word type. We slightly\nmodify this baseline to be able to handle token pairs. For LCA prediction, we assign a random numeric\nlabel to each pair of word types. For example, the word pair (the, sold) in Fig. 1 always receives the label\n1, regardless of the context where it occurs. And since the dataset contains more word type pairs than\nindividual word types, the control task setup is inherently more complex for LCA prediction than the one\nfor chunking.\nA.3 Token combination methods\nFor LCA prediction, w conducted experiments using three different combination methods: (i) con-\ncatenation (concat(xi,xj) ∈R2r, see Sec.4); (ii) element-wise average avg(xi,xj) ∈Rr; and (iii)\nelement-wise signed absolute maximum maxs of two scalars m,n. maxs(wi,wj) ∈Rr prefers strong\npositive and negative neuron activations while keeping the sign of the activation value in the combined\nvector:\nmaxs(m,n) =\n{\nm if |m|>|n|\nn otherwise (2)\nAveraging is the most lossy combination: Large positive or negative neuron activations are canceled\nout if they are not shared between both vectors. Concatenation is the only lossless combination. The\nconcatenation result concat(xi,xj) ∈ R2r has a larger dimensionality than the other combination\nmethods.\n6749\nA.4 Label distributions for the different probing tasks\nB I E S\nVP 13.77% 1.24% 0.07% 0.96%\nNP 12.53% 8.83% 12.27% 4.05%\nPP 9.73% 0.25% 0.00% 0.03%\nNP-SBJ 2.96% 1.73% 3.11% 2.46%\nSBAR 1.14% 0.10% 0.00%\nADJP 0.82% 0.21% 0.50% 0.38%\nQP 0.51% 0.75% 0.85% 0.00%\nADVP 0.35% 0.04% 0.27% 1.57%\nNP-TMP 0.28% 0.07% 0.29% 0.14%\nS 0.25% 0.29%\nNP-PRD 0.18% 0.18% 0.18% 0.03%\nNP-ADV 0.16% 0.01% 0.16% 0.01%\nNP-LGS 0.12% 0.16% 0.13% 0.02%\nNP-EXT 0.08% 0.01% 0.03% 0.03%\nNX 0.06% 0.06% 0.06% 0.03%\nNAC 0.05% 0.04% 0.00% 0.00%\nWHPP 0.04%\nCONJP 0.04% 0.02% 0.04% 0.00%\nWHNP 0.03% 0.01% 0.04% 0.75%\nUCP 0.03% 0.06% 0.02%\nSQ 0.02% 0.01%\nNP-LOC 0.02% 0.00% 0.02% 0.04%\nNP-TTL 0.02% 0.01% 0.02% 0.01%\nNP-HLN 0.02% 0.02% 0.01% 0.00%\nSINV 0.01% 0.00%\nFRAG 0.01% 0.01% 0.00%\nWHADVP 0.01% 0.00% 0.01% 0.18%\nLST 0.01%\nWHADJP 0.00% 0.00% 0.00%\nX 0.00% 0.00% 0.00% 0.00%\nSBARQ 0.00% 0.00%\nNP-MNR 0.00% 0.00% 0.00%\nNP-CLR 0.00% 0.00% 0.02%\nINTJ 0.00% 0.00% 0.00% 0.01%\nNP-TPC 0.00% 0.00% 0.00% 0.00%\nNP-VOC 0.00% 0.00% 0.00%\nNP-DIR 0.00% 0.00%\nADVP—PRT 0.00%\n(a) Label distribution for the detailed chunking tagset. Empty\ncells indicate that the combination of chunking label and phrase\nlabel is not present in the training data. Cells rounded to 0.00%\nindicate labels that are exceptionally rare in the training data.\nS 39.20%\nVP 25.37%\nNP 22.66%\nPP 5.35%\nSBAR 2.65%\nSINV 2.51%\nADJP 0.68%\nADVP 0.38%\nQP 0.37%\nFRAG 0.29%\nUCP 0.13%\nSQ 0.09%\nWHNP 0.08%\nNX 0.08%\nSBARQ 0.05%\nPRN 0.02%\nWHADVP 0.02%\nNAC 0.02%\nCONJP 0.01%\nWHPP 0.01%\nX 0.01%\nRRC 0.00%\nINTJ 0.00%\nLST 0.00%\nADVP—PRT 0.00%\nWHADJP 0.00%\nPRT—ADVP 0.00%\nPRT 0.00%\n(b) Label distribution for LCA prediction.\nB I E S PCT\n43.3% 14.1% 18.1% 10.7% 13.8%\n(c) B: beginning, I: inside, E: end, S: Single, PCT: punctuation:\npunctuation is not considered for evaluation\nTable 4: Label distributions for the different probing tasks\n6750\nA.5 Chunking results\ntask train/test DistilBERT BERT XLNet\ntask sel. task sel. task sel.\nchunking simple orig./orig. 95.3 14.1 95.2 13.4 94.3 17.1\n.33/orig. 94.1 14.9 94.4 14.9 93.9 18.7\n.33/.33 93.7 15.2 93.7 14.5 92.8 17.8\n.67/orig. 93.6 14.4 93.4 14.0 92.6 17.8\n.67/.67 92.6 14.7 92.4 16.0 91.3 17.1\nchunking detailed orig./orig. 90.3 8.7 90.6 9.4 91.1 16.5\n.33/orig. 89.3 10.4 89.3 10.7 89.6 15.9\n.33/.33/ 88.1 10.6 87.9 10.7 87.8 14.2\n.67/orig. 88.5 9.7 88.1 9.7 87.9 13.8\n.67/.67 86.3 8.5 86.1 8.8 86.4 12.8\n(a) Results for chunking experiments with DistilBERT, BERT and XLNet\nB I E S\nB 108594 1238 65 1026\nI 1683 33345 1504 217\nE 119 836 46980 570\nS 999 222 662 24597\n(b) Confusion matrix for chunking experiments with RoBERTa.\nB-NP\nB-NP-SBJ\nB-NP-TMP\nB-NP-LOC\nB-VP\nB-S\nB-SBAR\nB-PP\nB-ADJP\nB-ADVP\nI-NP\nI-NP-SBJ\nI-NP-TMP\nI-VP\nI-S\nI-SBAR\nI-PP\nI-ADJP\nI-ADVP\nE-NP\nE-NP-SBJ\nE-NP-TMP\nE-NP-LOC\nE-VP\nE-ADJP\nE-ADVP\nS-NP\nS-NP-SBJ\nS-NP-TMP\nS-NP-LOC\nS-VP\nS-PP\nS-ADJP\nS-ADVP\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) Accuracy per category for the most frequent labels in the detailed tagset. All beginnings of frequent constituents are\nrecognized with high accuracy. The classifier is also able to distinguish between different kinds of NPs, such as NP without\nfurther specification, subject NPs (NP-SBJ) or temporal and local NPs (NP-TMP,NP-LOC)\nTable 5: Results for chunking\n6751\nA.6 LCA prediction results\ntask train/test RoBERTa DistilBERT BERT XLNet\ntask sel. task sel. task sel. task sel.\nconcat orig./orig. 82.9 58.3 81.0 55.2 81.4 59.0 83.1 67.8\n.33/orig. 81.1 62.2 79.4 52.9 80.7 60.9 81.2 68.0\n.33/.33 79.7 56.8 78.0 51.8 79.5 56.6 79.7 61.8\n.67/orig. 79.3 61.5 78.6 58.2 79.5 65.2 80.9 59.9\n.67/.67 77.4 59.9 75.9 53.3 77.5 61.5 78.2 61.4\nmaxs orig./orig. 69.3 42.7 68.6 40.1 63.4 39.7 70.5 54.5\n.33/orig. 68.1 46.3 69.0 42.0 61.9 41.4 66.0 48.2\n.33/.33 66.5 45.2 66.4 36.2 60.3 39.6 60.1 43.2\n.67/orig. 65.8 45.0 64.5 41.6 61.0 43.6 59.2 37.8\n.67/.67 62.3 43.9 63.3 38.0 59.0 39.4 55.1 32.0\navg orig./orig. 63.9 36.9 66.8 38.8 62.3 38.4 57.5 42.3\n.33/orig. 64.3 37.2 66.1 38.7 63.9 39.3 58.9 45.0\n.33/.33 62.0 36.8 64.1 31.3 61.8 37.1 54.8 34.5\n.67/orig. 63.4 37.2 63.4 36.5 62.9 41.7 57.3 38.4\n.67/.67 59.6 36.5 61.3 34.0 59.4 37.2 51.5 27.1\n(a) LCA prediction results. The performance gains of concat wrt. avg and maxs are not matched by higher performance in\nthe control task for concat. Thus concat shows not only the best task performance but also the highest selectivity for LCA\nprediction.\nNP VP S SBAR PP ADJP ADVP\nNP 87681 4229 7716 508 2299 337 393\nVP 7817 99200 8389 1089 2228 121 755\nS 5503 3776 159212 1575 691 93 346\nSBAR 193 286 168 5849 212 10 4\nPP 1754 848 331 189 18974 187 91\nADJP 629 384 59 5 4 1861 153\nADVP 50 104 38 20 0 1 2035\n(b) Confusion matrix for LCA prediction for the most frequent constituents labels for RoBERTa when trained and evaluated\non non-manipulated data. The columns represent predicted values, rows represent actual values. Some categories are better\nrepresented in the probing classifiers than others. For example, prepositional phrases are recognized quite reliably, but adjectival\nphrases are confused for VPs and NPs in a number of cases. NPs are frequently confused with all other categories. The reason\nmight be that a variety of different phenomena are collected under NP, such as appositions and relative clauses.\nleft right\nRoBERTa 66.5 40.8\nDistilBERT 68.0 43.6\nBERT 64.8 40.9\nXLNet 62.1 40.7\n(c) Results for single-token baseline on LCA\nprediction\n0 5 10 15\nspan length\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00acc\nbert-base-uncased\nxlnet-base-cased\nroberta-base\ndistilbert-base-uncased\n(d) For LCA prediction, all models are more accurate when the distance\nbetween two tokens is higher\nFigure 6: Results for LCA prediction\n6752\nA.7 Neuron-level results for LCA prediction\n0.0 0.1 0.2 0.3 0.4 0.5\nfraction of neurons\n0.60\n0.65\n0.70\n0.75\n0.80accuracy\ntop\nrandom\nbot\nFigure 7: Accuracy for RoBERTa probes when trained on fractions of top, random or bottom neurons identified\nby linguistic correlation analysis (Durrani et al., 2020). The classifiers are trained on LCA prediction, using\nconcatenated representations. Fractions on the x-axis refer to the dimensions of concatenated representations. Thus,\nthe absolute number of considered neurons is twice as high as in the respective plots for chunking in Fig. 3.\nA.8 Neuron orderings for different datasets\n% neurons orig. & .33 orig. & .67 .33 & .67\n1% 21.2% 16.2% 15.2%\n2% 18.1% 15.6% 23.1%\n3% 17.4% 12.7% 20.4%\n5% 19.8% 16.6% 21.2%\n10% 25.4% 20.7% 26.6%\n20% 35.7% 29.6% 34.4%\n30% 42.6% 38.1% 43.7%\n50% 59.1% 56.4% 59.4%\nTable 6: Overlap of fractions of top neurons for chunking (simple tagset) when classifiers are trained on different\ndatasets. For each dataset, a neuron ranking list is obtained. This table shows the size of the fraction of neurons that\nare ranked among the most salient x% of neurons for two different datasets. For example, 21.2% of the 1% most\nsalient neurons for the original dataset are also among the 1% most salient neurons for the .33 dataset.\nA.9 Results for parse tree reconstructions from all language models\nRoBERTa BERT DistilBERT XLNet Random BERT\norig./orig. 82.58 80.42 79.88 80.52 51.36\n.33/orig. 80.88 77.73 77.65 78.99 49.60\n.33/.33 77.84 73.97 74.09 75.23 46.13\n.67/orig. 78.30 74.63 74.81 75.37 47.95\n.67/.67 72.77 69.63 69.72 69.91 44.71\nTable 7: Labeled F1 scores for all datasets and models for reconstructing full parse trees\n6753\nA.10 Comparing parse trees reconstructed from different models\nRoBERTa vs.\nXLNet\nRoBERTa vs.\nBERT\nBERT vs. XL-\nNet\nDistilBERT\nvs. BERT\nDistilBERT vs.\nRoBERTa\nDistilBERT\nvs. XLNet\norig. 81.83 82.80 80.39 83.17 82.34 80.19\n.33/orig. 79.71 80.21 77.60 81.60 80.53 77.88\n.33/.33 76.02 76.52 73.99 78.72 76.78 74.30\n.67/orig. 76.54 77.08 74.07 78.68 77.78 74.40\n.67/.67 71.75 72.83 69.56 75.01 73.45 69.89\nTable 8: Labeled F scores for comparing constituent trees predicted by different models against each other. The\ncomparison of trees predicted by DistilBERT and BERT yields the highest F scores, hence trees reconstructed from\nthese models are most similar.\nRoBERTa BERT DistilBERT\nBERT 0.59\nDistilBERT 0.57 0.64\nXLNet 0.52 0.52 0.51\nTable 9: Pearson correlation of sentence-level F scores for different LMs on original data. The correlations between\nF scores are lowest when comparing XLNet to BERT, RoBERTa and DistilBERT. The correlations are highest\nbetween the latter three models.\n6754\nA.11 Examples for reconstructed parse trees\nGold tree from PTB\nNP-SBJ\nSBAR\nS\nVP\nNP\nPP\nNP\nPP\nNP\nNNP\nSassyCC\nand\nNNP\nMs.\nIN\nof\nNP\nNN\npublisher\nDT\nthe\nIN\nof\nNP\nNN\nacquisition\nDT\nthe\nVBD\ncompleted\nNP-TMP\nNN\nweek\nDT\nthis\nWHNP\nWP\nwho\n,\n,\nNP\nNNP\nLang\nNP\nDale\nRoberta prediction\nNP\nSBAR\nVP\nNP\nPP\nNP\nPP\nNP\nNNP\nSassyCC\nand\nNNP\nMs.\nIN\nof\nNP\nNN\npublisher\nDT\nthe\nIN\nof\nNP\nNN\nacquisition\nDT\nthe\nVBD\ncompleted\nNP\nNN\nweek\nDT\nthis\nWHNP\nWP\nwho\n,\n,\nNP\nNNP\nLang\nNNP\nDale\nRoBERTa .67:\nNP\nSBAR\nVP\nVP\nPP\nPP\nNP\nNNP\nSassyCC\nand\nNNP\nAldomet\nIN\nafter\nNP\nNN\nbid\nDT\neach\nIN\nat\nNP\nNN\npurchase\nDT\nall\nVBD\ncompleted\nNP\nNN\nmonth\nDT\nthis\nWHNP\nWP\nwho\n,\n,\nNP\nNNP\nLang\nNNP\nMidwest\nFigure 8: Example for reconstructing parse trees: The subject phrase of sentence 23 from the development set.\nIn the RoBERTa prediction, the SBAR and S nodes of the relative clause are conflated to an SBAR node. In the\nRoBERTa prediction with .67 of the tokens replaced, the short chunks are correctly recognized, but the VP of the\nrelative clause is structured differently.\n6755\nGold tree from PTB\nS\nVP\nVP\nNP\nVP\nNP\nNP\nNN\n%CD\n22\nCC\nor\n,\n,\nNP\nQP\nCD\nmillion\nCD\n80.3$\n$\nVBG\ntotaling\nNP\nNNS\nincreases\nVBN\nsought\nVBD\nhad\nNP-SBJ\nNN\ncompany\nDT\nThe\nRoBERTa orig:\nS\nVP\nVP\nNP\nVP\nNP\nNN\n%CD\n22\nCC\nor\n,\n,\nQP\nCD\nmillion\nCD\n80.3$\n$\nVBG\ntotaling\nNP\nNNS\nincreases\nVBN\nsought\nVBD\nhad\nNP\nNN\ncompany\nDT\nThe\nRoBERTa .67:\nS\nVP\nVP\nNP\nVP\nNP\nNP\nNN\nyenCD\n124.5\nCC\n&\n,\n,\nQP\nCD\nmillion\nCD\n656.5$\n$\nVBG\nsurrounding\nNP\nNNS\nefforts\nVBN\nrejected\nVBD\ndid\nNP\nNN\ncompany\nDT\nHalf\nFigure 9: Example for reconstructing parse trees: Sentence 4 from the development set. For the RoBERTa prediction,\nthe unary leaf chain NP →QP is not predicted because it cannot be predicted by our version of the sequence labeling\nalgorithm. Apart from that, the or-NP is not recognized as a single NP. In the RoBERTa prediction for the sentence\nwith .67 of the tokens replaced, the VP is recognized correctly even though there is an agreement mismatch (did\nrejected). Diverging from the prediction for original data, the or-NP is reconstructed correctly\n6756\nA.12 Computing infrastructure and experiment runtime\nAll experiments where run on an Nvidia Titan XP GPU. For a 12-layer model, the full set of chunking\nexperiments takes 4 hours. This includes extracting the full neuron activation values for the training and\nevaluation data, all control task experiments and all experiments on the linguistic task. The full set of LCA\nprediction experiments for a 12-layer model takes around 15 hours when using all ways of combining\ninput representations (concat, avg, maxs ). Compared to the chunking experiments, an additional step is\ncombining the activation values of token pairs. The full set of experiments for reconstructing parse trees\ntakes around 3 hours when using a 12-layered language models and all datasets. Experiments with the\n6-layer model DistilBERT take half the time. The reported time includes time where the GPU itself is not\nactive, e.g. times where representations are combined and written to harddrive. All experiments were\npowered with electricity from renewable energy sources.\n6757",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.9577512741088867
    },
    {
      "name": "Computer science",
      "score": 0.827096700668335
    },
    {
      "name": "Natural language processing",
      "score": 0.7499786615371704
    },
    {
      "name": "Focus (optics)",
      "score": 0.7037278413772583
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6830062866210938
    },
    {
      "name": "Syntactic structure",
      "score": 0.5657596588134766
    },
    {
      "name": "Syntax",
      "score": 0.5479708313941956
    },
    {
      "name": "Tree structure",
      "score": 0.4613748788833618
    },
    {
      "name": "Parsing",
      "score": 0.36629438400268555
    },
    {
      "name": "Data structure",
      "score": 0.21036270260810852
    },
    {
      "name": "Programming language",
      "score": 0.08077719807624817
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I44260953",
      "name": "Heinrich Heine University Düsseldorf",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I129902397",
      "name": "Dalhousie University",
      "country": "CA"
    }
  ],
  "cited_by": 10
}