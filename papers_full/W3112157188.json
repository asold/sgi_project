{
    "title": "Non-Autoregressive Transformer for Speech Recognition",
    "url": "https://openalex.org/W3112157188",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2117872926",
            "name": "Nanxin Chen",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2117472677",
            "name": "Shinji Watanabe",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2108722367",
            "name": "Jesús Villalba",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2228061263",
            "name": "Piotr Zelasko",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A142824529",
            "name": "Najim Dehak",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2117872926",
            "name": "Nanxin Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117472677",
            "name": "Shinji Watanabe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108722367",
            "name": "Jesús Villalba",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2228061263",
            "name": "Piotr Zelasko",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A142824529",
            "name": "Najim Dehak",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6770587305",
        "https://openalex.org/W2985694911",
        "https://openalex.org/W6760017253",
        "https://openalex.org/W6759317646",
        "https://openalex.org/W2988975212",
        "https://openalex.org/W6774835902",
        "https://openalex.org/W3097882114",
        "https://openalex.org/W3097874139",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W6623517193",
        "https://openalex.org/W6779469704",
        "https://openalex.org/W2962780374",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W6746208923",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W2963434219",
        "https://openalex.org/W2972818416",
        "https://openalex.org/W6621543089",
        "https://openalex.org/W6741807409",
        "https://openalex.org/W6728610325",
        "https://openalex.org/W6762122294",
        "https://openalex.org/W2963242190",
        "https://openalex.org/W6637201659",
        "https://openalex.org/W1660460062",
        "https://openalex.org/W3103005696",
        "https://openalex.org/W2912937082",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W4294149591",
        "https://openalex.org/W648786980",
        "https://openalex.org/W3007328579",
        "https://openalex.org/W2991509857",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W3034729383",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3035445001",
        "https://openalex.org/W2542835211",
        "https://openalex.org/W2920538220",
        "https://openalex.org/W2949644922",
        "https://openalex.org/W2963827914",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2953345635",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W854541894",
        "https://openalex.org/W2982095018",
        "https://openalex.org/W3024732798"
    ],
    "abstract": "Recently very deep transformers have outperformed conventional bi-directional\\nlong short-term memory networks by a large margin in speech recognition.\\nHowever, to put it into production usage, inference computation cost is still a\\nserious concern in real scenarios. In this paper, we study two different\\nnon-autoregressive transformer structure for automatic speech recognition\\n(ASR): A-CMLM and A-FMLM. During training, for both frameworks, input tokens\\nfed to the decoder are randomly replaced by special mask tokens. The network is\\nrequired to predict the tokens corresponding to those mask tokens by taking\\nboth unmasked context and input speech into consideration. During inference, we\\nstart from all mask tokens and the network iteratively predicts missing tokens\\nbased on partial results. We show that this framework can support different\\ndecoding strategies, including traditional left-to-right. A new decoding\\nstrategy is proposed as an example, which starts from the easiest predictions\\nto the most difficult ones. Results on Mandarin (Aishell) and Japanese (CSJ)\\nASR benchmarks show the possibility to train such a non-autoregressive network\\nfor ASR. Especially in Aishell, the proposed method outperformed the Kaldi ASR\\nsystem and it matches the performance of the state-of-the-art autoregressive\\ntransformer with 7x speedup. Pretrained models and code will be made available\\nafter publication.\\n",
    "full_text": "Listen and Fill in the Missing Letters:\nNon-Autoregressive Transformer for Speech Recognition\nNanxin Chen Shinji Watanabe Jes ´us Villalba Najim Dehak\nCenter for Language and Speech Processing\nJohns Hopkins University\nBaltimore, MD, USA\nAbstract\nRecently very deep transformers have outper-\nformed conventional bi-directional long short-\nterm memory networks by a large margin in\nspeech recognition. However, to put it into\nproduction usage, inference computation cost\nis still a serious concern in real scenarios.\nIn this paper, we study two different non-\nautoregressive transformer structure for auto-\nmatic speech recognition (ASR): A-CMLM\nand A-FMLM. During training, for both frame-\nworks, input tokens fed to the decoder are ran-\ndomly replaced by special mask tokens. The\nnetwork is required to predict the tokens corre-\nsponding to those mask tokens by taking both\nunmasked context and input speech into con-\nsideration. During inference, we start from\nall mask tokens and the network iteratively\npredicts missing tokens based on partial re-\nsults. We show that this framework can sup-\nport different decoding strategies, including\ntraditional left-to-right. A new decoding strat-\negy is proposed as an example, which starts\nfrom the easiest predictions to the most difﬁ-\ncult ones. Results on Mandarin (Aishell) and\nJapanese (CSJ) ASR benchmarks show the\npossibility to train such a non-autoregressive\nnetwork for ASR. Especially in Aishell, the\nproposed method outperformed the Kaldi ASR\nsystem and it matches the performance of\nthe state-of-the-art autoregressive transformer\nwith 7x speedup. Pretrained models and code\nwill be made available after publication.\n1 Introduction\nIn recent studies, very deep end-to-end automatic\nspeech recognition (ASR) starts to be comparable\nor superior to conventional ASR systems (Karita\net al., 2019; Park et al., 2019; L¨uscher et al., 2019).\nThey mainly use encoder-decoder structures based\non long short-term memory recurrent neural net-\nworks (Chorowski et al., 2015; Chan et al., 2016;\nWatanabe et al., 2018) and transformer networks\n(Dong et al., 2018; Karita et al., 2019; L ¨uscher\net al., 2019). Those systems have common charac-\nteristics: they rely on probabilistic chain-rule based\nfactorization combined with left-to-right training\nand decoding. During training, the ground truth\nhistory tokens are fed to the decoder to predict the\nnext token. During inference, the ground truth his-\ntory tokens are replaced by previous predictions\nfrom the decoder. While this combination allows\ntractable log-likelihood computation, maximum\nlikelihood training, and beam-search based approx-\nimation, it is more difﬁcult to perform parallel com-\nputation during decoding. The left-to-right beam\nsearch algorithm needs to run decoder computation\nmultiple times which usually depends on output\nsequence length and beam size. Those models are\nwell-known as auto-regressive models.\nRecently, non-autoregressive end-to-end models\nhave started to attract attention in neural machine\ntranslation (NMT) (Gu et al., 2017; Lee et al., 2018;\nGu et al., 2019; Stern et al., 2019; Welleck et al.,\n2019; Ghazvininejad et al., 2019). The idea is\nthat the system predicts the whole sequence within\na constant number of iterations which does not\ndepend on output sequence length. In (Gu et al.,\n2017), the author introduced hidden variables de-\nnoted as fertilities, which are integers correspond-\ning to the number of words in the target sentence\nthat can be aligned to each word in the source sen-\ntence. The fertilities predictor is trained to repro-\nduce the predictions from another external aligner.\nLee et al. (2018) used multiple iterations of reﬁne-\nment starting from some “corrupted” predictions.\nInstead of predicting fertilities for each word in\nsource sequence they only need to predict target\nsequence total length. Another direction explored\nin previous studies is to allow the output sequence\nto grow dynamically (Gu et al., 2019; Stern et al.,\n2019; Welleck et al., 2019). All those works in-\nsert words to output sequence iteratively based on\narXiv:1911.04908v2  [eess.AS]  6 Apr 2020\ncertain order or explicit tree structure. This allows\narbitrary output sequence length avoiding deciding\nbefore decoding. However since this insertion or-\nder or tree structure is not provided as ground truth\nfor training, sampling or approximation is usually\nintroduced to infer it. Among all those studies of\ndifferent directions, a common procedure for neural\nmachine translation is to perform knowledge distil-\nlation (Gu et al., 2017). In machine translation, for\na given input sentence, multiple correct translations\nexist. A pre-trained autoregressive model is used\nto provide a unique target sequence for training.\nOur work is mainly inspired by the condi-\ntional language model proposed recently for neural\nmachine translation (Ghazvininejad et al., 2019).\nTraining procedure of this conditional language\nmodel is similar to BERT (Devlin et al., 2019).\nSome random tokens are replaced by a special mask\ntoken and the network is trained to predict original\ntokens. The difference between our approach and\nBERT is that our system makes predictions con-\nditioned on input speech. Based on observations,\nwe further propose to use factorization loss instead,\nwhich bridges the gap between training and infer-\nence. During inference, the network decoder can\ncondition on any subsets to predict the rest given\ninput speech. In reality, we start from an empty\nset (all mask tokens) and gradually complete the\nwhole sequence. The subset we chose can be quite\nﬂexible so it makes any decoding order possible.\nIn ASR, there is no need for knowledge distillation\nsince in most cases unique transcript exists.\nThis paper is organized as follows. Section 2\nintroduces the autoregressive end-to-end model\nand section 3 discusses how to adapt it to non-\nautoregressive. Different decoding strategies are\nalso included. Section 4 introduces the experi-\nmental setup and presents results on different cor-\npora. Further analysis is also included discussing\nthe difference between autoregressive and non-\nautoregressive ASR. Section 5 summarizes this\npaper and provides several directions for future\nresearch in this area.\n2 Autoregressive Transformer-based\nASR\nTo study non-autoregressive end-to-end ASR, it is\nimportant to understand how the current autoregres-\nsive speech recognition system works. As shown\nin Figure 1 top part, general sequence-to-sequence\nmodel consists of encoder and decoder. The en-\ncoder takes speech features xt like log Mel ﬁlter\nbanks as input and produces hidden representations\nht. The decoder predicts a next token yt based on\nthe previous history y<t and all hidden representa-\ntions h = (h1, h2, ···):\nP(yt|y<t, x) =Pdec(yt|y<t, ft(h)) (1)\nwhere f is a t-dependent function on all hidden\nrepresentations h. A common choice for f is an\nattention mechanism, which can be considered to\nbe a weighted combination of all hidden represen-\ntations:\nfatt\nt (h) =\n∑\nt′\nwt,t′ht′ (2)\nwhere t\n′\nenumerates all possible hidden representa-\ntions in h. The weight wt,t′ is usually determined\nby a similarity between the decoder hidden state at\nt and hidden representation ht′.\nDuring training, the ground truth history tokens\ny<t are usually used as input to the decoder in equa-\ntion 1 for two reasons. First, it is faster since the\ncomputation of all P can be performed in parallel,\nas used in (Vaswani et al., 2017). Second, train-\ning can be very difﬁcult and slow if predictions\nare used instead especially for very long sequence\ncases (Bengio et al., 2015; Lamb et al., 2016). The\nexpanded computation graph becomes very deep\nsimilar to recurrent neural networks without trun-\ncating.\nDuring inference, since no ground truth is given,\npredictions need to be used instead. This means\nequation (1) needs to be computed sequentially for\nevery token in output and each prediction needs\nto perform decoder computation once. Depends\non output sequence length and unit used, this pro-\ncedure can be very slow for certain cases, like\ncharacter-based Transformer models.\n3 Non-Autoregressive\nTransformer-based ASR\nBecause of the training/inference discrepancy\nand sequential inference computation, non-\nautoregressive transformer becomes increasingly\npopular.\nOne possibility to make model non-\nautoregressive is to remove y<t so parallel\ncomputation of P(y|h) can be factorized as the\nproduct of P(yt|h). However, this conditional\nindependence might be too strong for ASR. Instead,\nwe replace y<t with some other information like\nFigure 1: Comparison between normal transformer network and non-autoregressive transformer network. The\ntransformer uses ground truth history tokens during training while during inference previous predictions are used\nas shown in the dash line. For non-autoregressive transformer training, random tokens in decoder input are replaced\nby a special ⟨MASK⟩token and the network is required to predict for those positions. Both networks conditions on\nencoder outputs of the whole sequence.\npartial decoding results. Similar to previous\nwork (Lee et al., 2018; Ghazvininejad et al.,\n2019), multiple iterations are adopted to gradually\ncomplete prediction of the whole sentence.\n3.1 Training Frameworks\n3.1.1 Audio-Conditional Masked Language\nModel (A-CMLM)\nOne training framework we considered comes from\nGhazvininejad et al. (2019). The idea is to replace\ny<t with partial decoding results we got from pre-\nvious computations. A new token ⟨MASK⟩is intro-\nduced for training and decoding, similar to the idea\nof BERT (Devlin et al., 2019). Let TM and TU be\nthe sets of masked and unmasked tokens respec-\ntively. The posterior of the masked tokens given\nthe unmasked tokens and the input speech is,\nP(yTM|yTU, x) =\n∏\nt∈TM\nPdec(yt|yTU, ft(h)) .\n(3)\nAs shown in Figure 1 bottom part, during train-\ning some random tokens are replaced by this spe-\ncial token ⟨MASK⟩. The network is asked to predict\noriginal unmasked tokens based on input speech\nand context. The total number of mask tokens is\nrandomly sampled from a uniform distribution of\nwhole utterance length and ground truth tokens\nare randomly selected to be replaced with this\n⟨MASK⟩token. Theoretically, if we mask more to-\nkens model will rely more on input speech and if\nwe mask fewer tokens context will be utilized simi-\nlar to the language model. This combines the ad-\nvantages of both speech recognition and language\nmodeling. We further assume that given unmasked\ntokens, predictions of masked tokens are condition-\nally independent so they can be estimated simulta-\nneously as the product in equation (3).\nASR uses audio as input (e.g., ft(h) in equa-\ntion (3)) instead of source text in NMT so we name\nthis as audio-conditional masked language model\n(A-CMLM).\n3.2 Audio-Factorized Masked Language\nModel (A-FMLM)\nDuring the training of A-CMLM, ground truth to-\nkens at TU in (3) are provided to predict the masked\npart. However, during inference none of those to-\nkens are given. Thus the model needs to predict\nwithout any context information. This mismatch\ncan be arbitrarily large for some cases, like long\nutterances from our observations. We will show it\nin the later experiments.\nInspired by Yang et al. (2019); Dong et al.\n(2019), we formalize the idea to mitigate the train-\ning and inference mismatch as follows. Let Zi ⊂\n[0, 1, ..., T−1] be a length-(N + 1)sequence of\nindices such that\nZ0 = ∅\nZN = [0, 1, ..., T−1]\n∀i Z i ⊂Zi+1\n(4)\nFor both training and inference the objective can\nbe expressed as\nP(y|h) =\nN∏\nt=1\n∏\ni∈Zt∩Zt−1\nPdec(yi|yZt−1, ft(h))\n(5)\nwhere Zt ∩Zt−1 are the indices for decoding in\niteration t. For example, to decode utterance of\nlength 5 with 5 iterations, one common approach\n(left to right) can be considered as:\nZ0 = ∅\nZ1 = 0\nZ2 = 0, 1\nZ3 = 0, 1, 2\nZ4 = 0, 1, 2, 3\nZ5 = 0, 1, 2, 3, 4\n(6)\nHere Zt ∩Zt−1 = t −1 so in this case equation\n(5) is equivalent to equation (1). Similar to A-\nCMLM, ⟨MASK⟩tokens are added to decoder inputs\nwhen corresponding tokens are not decided yet.\nThe autoregressive model is a special case when\nN = T and Zt = [0, 1, ..., t].\nIdeally Zt should be decided based on conﬁ-\ndence scores from the network predictions to match\ninference case. During training we sort all poste-\nriors from iteration t −1 and choose those most\nconﬁdent ones. The size of Zt is also sampled from\nthe uniform distribution between0 and T to support\ndifferent possibilities for decoding. To speed-up,\nwe set N = 2during training so that optimization\nobjective can be also written as\nP(y|h) =\n∏\ni̸∈Z1\nPdec(yi|yt∈Z1, ft(h))\n∗\n∏\nj∈Z1\nPdec(yj|ft(h))\n(7)\nComparing with equation (3), A-CMLM training\nonly includes ﬁrst term if Z1 = TU. However,\nduring inference, some explicit factorization is still\nneeded.\nPseudo-code of our A-FMLM algorithm can be\nfound in Algorithm 1.\nIt is also possible to introduce different net-\nworks Pdec for different iterations under our train-\ning framework.\n3.3 Decoding Strategies\nDuring inference, a multi-iteration process is con-\nsidered. Other than traditional left-to-right, two\ndifferent strategies are studied: easy ﬁrst and mask-\npredict.\n3.3.1 Easy ﬁrst\nThe idea of this strategy is to predict the most ob-\nvious ones ﬁrst similar to easy-ﬁrst parsing intro-\nduced in (Goldberg and Elhadad, 2010). In the\nAlgorithm 1: Minibatch forward pass\ninput : minibatch size n, dataset D, encoder\nnetwork fenc, decoder network fdec\noutput :Posterior P\nSample x = x1, ..., xn, y = y1, ..., yn from\nD;\nh = fenc(x);\nAssign ⟨MASK⟩to all elements in ˆy0;\nP(y1|h) =fdec(ˆy0, h);\nmask = zeros(n, maxlength);\nAssign ⟨MASK⟩to all elements in ˆy1;\nfor i=1,...,n do\nprobs = Pi(y1|h);\nindices = argsort(probs.max(−1));\nZ ∼Uniform (1, length(probs));\nmask[i, indices[Z :]] = 1;\nˆy1[i, indices[Z :]] =y[i, indices[Z :]];\nend\nP(y2|h) =fdec(ˆy1, h);\nP = mask∗P(y1|h)+(1 −mask)∗P(y2|h);\nﬁrst iteration, the decoder is fed with predictions\nˆy0\nt = ⟨MASK⟩tokens for all t since we do not have\nany partial results. After getting decoding results\nP(y1\nt |.)1, we keep those most conﬁdent ones and\nupdate them in y1:\nˆy1\nt =\n{\narg maxV P(y1\nt |.) t ∈largestC(maxV P(y1|.))\nˆy0\nt otherwise\n(8)\nwhere V is the vocabulary, C = ⌈L/K⌉is the\nlargest number of predictions we keep, L is the\nsequence length and K is the total number of itera-\ntions. Conditioned on this new ˆy1, the network is\nrequired to make new predictions if there are still\nmasked tokens.\nComparing with Algorithm 1, we used predic-\ntions arg maxV P(y1\nt |.) instead of ground truth to-\nkens y[i, indices[Z :]].\n3.3.2 Mask-predict\nThis is studied in (Ghazvininejad et al., 2019). Sim-\nilarly to Section 3.3.1, we start with ˆy0\nt = ⟨MASK⟩.\nIn each iteration k, we check the posterior proba-\nbility of the most probable token for each output\nt (i.e., maxV P(yk\nt |.)) and use this probability as\na conﬁdence score to replace least conﬁdent ones\n1we omit the dependecies of the posterior to keep the\nnotation uncluttered\nin an utterance by ⟨MASK⟩tokens. The number of\nmasked tokens in an utterance is ⌈L ∗(1 −k/K)⌉\nfor k-th iteration:\nˆyk\nt =\n{\n⟨MASK⟩ t ∈ smallestC(maxV P(yk\nt |.))\narg maxV P(yk\nt |.) otherwise\n(9)\nwhere C = ⌈L ∗(1 −k/K)⌉. For instance, if\nK = 10, we mask 90% tokens in the ﬁrst iteration,\n80% in second and so on. After getting prediction\nresults we update all tokens previously masked in\nˆyk−1:\nP(yk\nt |.) =\n{\nP(yk\nt |.) ˆyk−1\nt = ⟨MASK⟩\nP(yk−1\nt |.) otherwise (10)\nThe difference between mask-predict and easy ﬁrst\nis that mask-predict will accept all decisions but\nit reverts decisions made earlier if it is less conﬁ-\ndent. Easy ﬁrst is more conservative and it gradu-\nally adopts decisions with the highest conﬁdence.\nFor both strategies, predictions become more and\nmore accurate since it can utilize context informa-\ntion from both future and past directions. This is\nachieved by replacing input y<t with all yt since\nleft-to-right decoding is no longer necessary.\n3.4 Example\nOne example is given in Figure 2. Part (a) shows\neasy ﬁrst and part (b) demonstrates mask-predict.\nIn this example sequence length is 4 but after\nadding ⟨EOS⟩token to the end of the sequence\nwe have L = 5 and K = 3. In the ﬁrst itera-\ntion, the network is inputted with all ⟨MASK⟩. Top\n⌈5/3⌉= 2 tokens get kept in each iteration and\nbased on partial results network predicts again on\nall rest ⟨MASK⟩tokens.\nFor easy ﬁrst, it always ranks conﬁdence from\nthe last iteration and then keep top-2 conﬁdent pre-\ndictions. Based on partial results it will complete\nthe rest.\nFor mask-predict it maintains conﬁdence scores\nfrom multiple iterations. It chooses the least con-\nﬁdent ones from all scores to mask. In the last\niteration, it chooses to change its previous predic-\ntion of “so” because its conﬁdence is less than other\npredictions from the second iteration.\nNormal inference procedure can be considered\nas a special case when K = L and instead of\ntaking the most conﬁdent one, the prediction of\nthe next token is always adopted. In general, this\napproach is ﬂexible enough to support different\ndecoding strategies: left-to-right, right-to-left, easy\nﬁrst, mask-predict and other unexplored strategies.\n3.5 Output sequence length prediction\nIn Ghazvininejad et al. (2019) they introduced a\nspecial token ⟨LENGTH⟩in input to predict output\nsequence length. For word sequence this is reason-\nable but for end-to-end speech recognition, it can\nbe pretty difﬁcult since character or BPE sequence\nlength varies a lot. In this paper a simpler approach\nis proposed: we asked the network to predict end-\nof-sequence token ⟨EOS⟩at the end of the sequence\nas shown in Figure 2.\nDuring inference, we still need to specify the ini-\ntial length. We manually specify it to some constant\nvalue for the ﬁrst iteration. After that, we change\nit to the predicted length in the ﬁrst iteration for\nspeedup.\n4 Experiments\nFor experiments, we mainly use Aishell (Bu et al.,\n2017) and Corpus of Spontaneous Japanese(CSJ)\n(Maekawa, 2003). Our preliminary investigations\nshow that a Latin alphabet based task (e.g., En-\nglish) has some difﬁculties since the mask-based\ncharacter prediction in a word is easily ﬁlled out\nonly with the character context without using any\nspeech input (i.e., the network only learns the de-\ncoder). Although the use of BPE-like sub-words\ncan solve this issue to some extent, this introduces\nextra complexity and also the non-unique BPE se-\nquence decomposition seems to cause some incon-\nsistency for masked language modeling especially\nconditioned with the audio input. Due to these rea-\nsons, we chose ideogram languages (i.e., Mandarin\nand Japanese) in our experiments so that the output\nsequence length is limited and the prediction of\nthe character is enough challenging. Applying the\nproposed method to the Latin alphabet language is\none of our important future work.\nESPnet (Watanabe et al., 2018) is used for all\nexperiments. For the non-autoregressive baseline,\nwe use state-of-the-art transformer end-to-end sys-\ntems Karita et al. (2019). In Aishell experiments\nencoder includes 12 transformer blocks with convo-\nlutional layers at the beginning for downsampling.\nThe decoder consists of 6 transformer blocks. For\nall transformer blocks, 4 heads are used for atten-\ntion. The network is trained for 50 epochs and\nwarmup (Vaswani et al., 2017) is used for early\niterations. For all experiments beam search and\nFigure 2: Illustration of inference procedure. To predict the whole sequence with K = 3 passes, initially, the\nnetwork is fed with all ⟨MASK⟩tokens. Shade here presents the certainties from network outputs. Part (a) shows\neasy ﬁrst process. Since token “so” is conﬁdent enough in the ﬁrst iteration to be decided it will never change in\nthe future. Part (b) shows mask-predict process. In the last iteration, it goes back to the word ”so” because it is less\nconﬁdent in the ﬁrst iteration compared to other predictions in other iterations.\nlanguage model is used and all conﬁguration fol-\nlows autoregressive baseline.\nThe results of Aishell is given in Table 1. For A-\nCMLM, no improvement observed for more than\n3 iterations. For A-FMLM, experiments show that\n1 iteration is enough to get the best performance.\nBecause of the connection between easy ﬁrst and\nA-FMLM, we only use easy ﬁrst for A-FMLM. All\ntransformer-based experiments are based on pure\nPython implementation so we don’t compare them\nwith C++-based Kaldi systems in the table. It is\nstill possible to get further speedup by improving\ncurrent implementation.\nAll decoding methods result in performance very\nclose to state-of-the-art autoregressive models. Es-\npecially A-FMLM matched the performance of au-\ntoregressive baseline but real-time factor reduced\nfrom 1.44 to 0.22, which is around 7x speedup.\nThe reason is that our non-autoregressive systems\nonly perform decoder computation constant num-\nber of times, comparing to the autoregressive model\nwhich depends on the length of output sequence. It\nalso outperformed two different hybrid systems in\nKaldi by 22% and 11% relative respectively.\nCSJ results are given in Table 2. Here\nwe observed a larger difference between non-\nautoregressive models and autoregressive models.\nSystem Dev\nCER\nTest\nCER\nReal Time\nFactor\nBaseline(Transformer) 6.0 6.7 1.44\nBaseline(Kaldi nnet3) - 8.6 -\nBaseline(Kaldi chain) - 7.5 -\nAn et al. (2019) - 6.3 -\nFan et al. (2019) - 6.7 -\nEasy ﬁrst(K=1) 6.8 7.6 0.22\nEasy ﬁrst(K=3) 6.4 7.1 0.22\nMask-predict(K=1) 6.8 7.6 0.22\nMask-predict(K=3) 6.4 7.2 0.24\nA-FMLM(K=1) 6.2 6.7 0.28\nA-FMLM(K=2) 6.2 6.8 0.22\nTable 1: Comparison of baselines, previous work, A-\nCMLM and A-FMLM on Aishell. For A-CMLM, easy\nﬁrst and mask-predict are compared. For A-FMLM,\neasy ﬁrst is utilized since it connects to the factoriza-\ntion used in training.\nMultiple iterations of different decoding strategies\nare not helping to improve. Still, A-FMLM we\nproposed outperforms A-CMLM with up to 9x\nspeedup comparing to the autoregressive baseline.\nTo understand the performance difference be-\ntween the autoregressive model and the non-\nautoregressive model, further analysis is included.\nIn Figure 3 we show the correlation between out-\n0.000\n0.050\n0.100\n0.150\n0.200\n0.250\n0.300\n1 11 21 31 41 51 61 71 81 91 103 116\nAutoregressive Non-autoregressive Non-autoregressive(S) Non-autoregressive(D) Non-autoregressive(I)\nFigure 3: Error analysis of autoregressive and non-autoregressive on different output sequence length. Dash and\ndot lines indicate different errors: substitute(S), deletion(D), insertion(I)\nSystem Eval1\nCER\nEval2\nCER\nEval3\nCER\nReal Time\nFactor\nBaseline(Transformer)5.9 4.1 4.6 9.50\nBaseline(Kaldi) 7.5 6.3 6.9 -\nEasy ﬁrst(K=1) 8.8 6.7 7.4 1.31\nEasy ﬁrst(K=3) 9.3 7.0 8.3 1.05\nMask-predict(K=1) 8.8 6.7 7.4 1.31\nMask-predict(K=3) 11.7 8.8 9.9 1.01\nA-FMLM(K=1) 7.7 5.4 6.2 1.40\nA-FMLM(K=2) 7.7 5.4 6.2 1.11\nTable 2: Comparison of baselines, previous work, A-\nCMLM and A-FMLM on CSJ. Same systems are com-\npared as above.\nput sequence length and character error rate for\nEval1 set. For short utterances performance of the\nnon-autoregressive model (blue line) is close to\nthe autoregressive model (yellow line). However,\nwhen output sequence becomes longer (large than\n100), deletion error (long dash-dot line) and substi-\ntution error (dot line) start to increase dramatically.\nThis suggests the difﬁculty of handling very long\nutterances when the network is trained to predict\nall tokens simultaneously. More speciﬁcally, error\nrate in the ﬁrst iteration may increase since it is\neasy to miss certain token in the long sequence.\nAnd those deletion errors increased discrepancy\nbetween training and inference which can not be\neasily ﬁxed because all following tokens need to\nbe moved. This suggests the potential to apply in-\nsertion based models like Gu et al. (2019); Stern\net al. (2019); Welleck et al. (2019).\n5 Conclusion\nIn this paper, we study two novel non-\nautoregressive training framework for transformer-\nbased automatic speech recognition (ASR). A-\nCMLM applies conditional language model pro-\nposed in (Ghazvininejad et al., 2019) to speech\nrecognition. Besides decoding strategies like left-\nto-right, mask-predict, a new decoding strategy\nis proposed. Based on the connection with a\nclassical dependency parsing (Goldberg and El-\nhadad, 2010), we named this decoding strategy\neasy ﬁrst. Inspired from easy ﬁrst, we further pro-\npose a new training framework: A-FMLM, which\nutilizes factorization to bridge the gap between\ntraining and inference. In experiments, we show\nthat compared to classical left-to-right order these\ntwo show great speedup with reasonable perfor-\nmance. Especially on Aishell, the speedup is up to\n7 times while performance matches the autoregres-\nsive model. We further analyze the problem of the\nnon-autoregressive model for ASR on long output\nsequences. This suggests several possibilities for\nfuture research, e.g., the potential to apply inser-\ntion based non-autoregressive transformer model\nto speech recognition.\nReferences\nKeyu An, Hongyu Xiang, and Zhijian Ou. 2019.\nCat: Crf-based asr toolkit. arXiv preprint\narXiv:1911.08747.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn Advances in Neural Information Processing Sys-\ntems, pages 1171–1179.\nHui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao\nZheng. 2017. Aishell-1: An open-source mandarin\nspeech corpus and a speech recognition baseline. In\n2017 20th Conference of the Oriental Chapter of the\nInternational Coordinating Committee on Speech\nDatabases and Speech I/O Systems and Assessment\n(O-COCOSDA), pages 1–5. IEEE.\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol\nVinyals. 2016. Listen, attend and spell: A neural\nnetwork for large vocabulary conversational speech\nrecognition. In 2016 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 4960–4964. IEEE.\nJan K Chorowski, Dzmitry Bahdanau, Dmitriy\nSerdyuk, Kyunghyun Cho, and Yoshua Bengio.\n2015. Attention-based models for speech recogni-\ntion. In Advances in neural information processing\nsystems, pages 577–585.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019. Uniﬁed\nlanguage model pre-training for natural language\nunderstanding and generation. arXiv preprint\narXiv:1905.03197.\nLinhao Dong, Shuang Xu, and Bo Xu. 2018. Speech-\ntransformer: a no-recurrence sequence-to-sequence\nmodel for speech recognition. In 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5884–5888. IEEE.\nZhiyun Fan, Shiyu Zhou, and Bo Xu. 2019. Unsu-\npervised pre-traing for sequence to sequence speech\nrecognition. arXiv preprint arXiv:1910.12418.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel\ndecoding of conditional masked language models.\narXiv preprint arXiv:1904.09324.\nYoav Goldberg and Michael Elhadad. 2010. An efﬁ-\ncient algorithm for easy-ﬁrst non-directional depen-\ndency parsing. In Human Language Technologies:\nThe 2010 Annual Conference of the North American\nChapter of the Association for Computational Lin-\nguistics, pages 742–750. Association for Computa-\ntional Linguistics.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor OK Li, and Richard Socher. 2017. Non-\nautoregressive neural machine translation. arXiv\npreprint arXiv:1711.02281.\nJiatao Gu, Qi Liu, and Kyunghyun Cho. 2019.\nInsertion-based decoding with automatically\ninferred generation order. arXiv preprint\narXiv:1902.01370.\nShigeki Karita, Nanxin Chen, Tomoki Hayashi,\nTakaaki Hori, Hirofumi Inaguma, Ziyan Jiang,\nMasao Someki, Nelson Enrique Yalta Soplin,\nRyuichi Yamamoto, Xiaofei Wang, et al. 2019. A\ncomparative study on transformer vs rnn in speech\napplications. arXiv preprint arXiv:1909.06317.\nAlex M Lamb, Anirudh Goyal Alias Parth Goyal, Ying\nZhang, Saizheng Zhang, Aaron C Courville, and\nYoshua Bengio. 2016. Professor forcing: A new\nalgorithm for training recurrent networks. In Ad-\nvances In Neural Information Processing Systems,\npages 4601–4609.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1173–\n1182.\nChristoph L ¨uscher, Eugen Beck, Kazuki Irie, Markus\nKitza, Wilfried Michel, Albert Zeyer, Ralf Schl ¨uter,\nand Hermann Ney. 2019. Rwth asr systems for lib-\nrispeech: Hybrid vs attention. Interspeech, Graz,\nAustria, pages 231–235.\nKikuo Maekawa. 2003. Corpus of spontaneous\njapanese: Its design and evaluation. In ISCA &\nIEEE Workshop on Spontaneous Speech Processing\nand Recognition.\nDaniel S. Park, William Chan, Yu Zhang, Chung-\nCheng Chiu, Barret Zoph, Ekin D. Cubuk, and\nQuoc V . Le. 2019. SpecAugment: A Simple Data\nAugmentation Method for Automatic Speech Recog-\nnition. In Proc. Interspeech 2019, pages 2613–\n2617.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion transformer: Flexible se-\nquence generation via insertion operations. In In-\nternational Conference on Machine Learning, pages\n5976–5985.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki\nHayashi, Jiro Nishitoba, Yuya Unno, Nelson-\nEnrique Yalta Soplin, Jahn Heymann, Matthew\nWiesner, Nanxin Chen, et al. 2018. Espnet: End-\nto-end speech processing toolkit. Proc. Interspeech\n2018, pages 2207–2211.\nSean Welleck, Kiant ´e Brantley, Hal Daum ´e Iii, and\nKyunghyun Cho. 2019. Non-monotonic sequential\ntext generation. In International Conference on Ma-\nchine Learning, pages 6716–6726.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237."
}