{
    "title": "Can large language models predict antimicrobial peptide activity and toxicity?",
    "url": "https://openalex.org/W4395055554",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4288158428",
            "name": "Markus Orsi",
            "affiliations": [
                "University of Bern"
            ]
        },
        {
            "id": "https://openalex.org/A4223443570",
            "name": "Jean-Louis Reymond",
            "affiliations": [
                "University of Bern"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2896127999",
        "https://openalex.org/W3040996852",
        "https://openalex.org/W3007285729",
        "https://openalex.org/W2906960763",
        "https://openalex.org/W3125778162",
        "https://openalex.org/W2784920021",
        "https://openalex.org/W2791848964",
        "https://openalex.org/W2884267293",
        "https://openalex.org/W2998197489",
        "https://openalex.org/W2948261638",
        "https://openalex.org/W3091899249",
        "https://openalex.org/W3025501158",
        "https://openalex.org/W3137064251",
        "https://openalex.org/W4288036349",
        "https://openalex.org/W4378212018",
        "https://openalex.org/W4366823167",
        "https://openalex.org/W4391972484",
        "https://openalex.org/W4392162030",
        "https://openalex.org/W3039894784",
        "https://openalex.org/W3009548091",
        "https://openalex.org/W4362506675",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2172140247",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4391561379",
        "https://openalex.org/W4387560715",
        "https://openalex.org/W4327564965",
        "https://openalex.org/W4319996831",
        "https://openalex.org/W4365597205",
        "https://openalex.org/W4389991792",
        "https://openalex.org/W4385671288",
        "https://openalex.org/W4390012432",
        "https://openalex.org/W2076860935",
        "https://openalex.org/W2898320067",
        "https://openalex.org/W1971024387",
        "https://openalex.org/W3035302862",
        "https://openalex.org/W3118695441",
        "https://openalex.org/W3089139172",
        "https://openalex.org/W3206426065",
        "https://openalex.org/W3008588639",
        "https://openalex.org/W4399012107",
        "https://openalex.org/W4380686968",
        "https://openalex.org/W4396854777",
        "https://openalex.org/W4396723768"
    ],
    "abstract": "The large language models GPT-3 and GTP-3.5 were challenged to predict the activity and hemolysis of antimicrobial peptides from their sequence and compared to recurrent neural networks and support vector machines.",
    "full_text": "RSC\nMedicinal Chemistry\nRESEARCH ARTICLE\nCite this:RSC Med. Chem.,2 0 2 4 ,15,\n2030\nReceived 8th March 2024,\nAccepted 19th April 2024\nDOI: 10.1039/d4md00159a\nrsc.li/medchem\nCan large language models predict antimicrobial\npeptide activity and toxicity?†\nMarkus Orsi and Jean-Louis Reymond *\nAntimicrobial peptides (AMPs) are naturally occurring or designed peptides up to a few tens of amino acids\nwhich may help address the antimicrobial resistance crisis. However, their clinical development is limited\nby toxicity to human cells, a parameter which is very difficult to control. Given the similarity between\npeptide sequences and words, large language models (LLMs) might be able to predict AMP activity and\ntoxicity. To test this hypothesis, we fine-tuned LLMs using data from the Database of Antimicrobial Activity\nand Structure of Peptides (DBAASP). GPT-3 performed well but not reproducibly for activity prediction and\nhemolysis, taken as a proxy for toxicity. The later GPT-3.5 performed more poorly and was surpassed by\nrecurrent neural networks (RNN) trained on sequence-activity data or support vector machines (SVM)\ntrained on MAP4C molecular fingerprint-activity data. These simpler models are therefore recommended,\nalthough the rapid evolution of LLMs warrants future re-evaluation of their prediction abilities.\nIntroduction\nAntimicrobial peptides (AMPs) have gained significant\nattention in the field of drug discovery due to their potential\ntherapeutic applications in the fight against antimicrobial\nresistance.1–3 However, the vast number of possible peptide\nsequences and their complex structure–activity relationship\nlandscape mean that it is difficult to rationally design\npeptides with the desired biological activity, in particular\ntuning their activity versus toxicity to human cells, which is\noften measured as hemolysis of human red blood cells.\n4,5\nTo address this issue, several machine-learning models\nhave been developed for thede novo design of antimicrobial\npeptides.6–21 Because property prediction from a peptide\nsequence can be framed as a natural language processing\nproblem, many of these models use architectures specifically\ndesigned for language processing tasks.\n22–24 Furthermore,\nthe emergence of large language models (LLMs), such as\nOpenAI's GPT models,\n25 has opened new possibilities for\nleveraging powerful language processing capabilities in drug\ndiscovery applications. Recent attempts by Jablonkaet al. to\nexplore the capabilities of GPT-3 for predicting properties of\nsmall molecules in various applications have shown that\nGPT-3 was able to perform comparably or even outperform\nconventional statistical models, particularly in the low data\nregime.\n26 There also have been successful efforts into\naugmenting LLM capabilities to tackle tasks related to small\nmolecule chemistry in the areas of organic synthesis, drug\ndiscovery, and materials design. 27–30 Hereby, the models\nmainly orchestrate a set of tools to solve chemistry tasks\nstarting from a natural language prompt.31–33 However, to\nthe best of our knowledge LLMs have not been implemented\nto predict the bioactivity of peptides yet.\nIn this study, we aimed to compare GPT models fine-\ntuned on antimicrobial peptide sequence data with models\nthat have been previously used to predict antimicrobial\nactivity and hemolysis of peptide sequences.\n13,14 Alongside\nevaluating the performance of the fine-tuned GPT models, we\nalso seek to explore the advantages and disadvantages they\noffer in terms of time and cost effectiveness. Furthermore,\nwe compare the performance of models trained on amino\nacid sequences to a support-vector machine (SVM) trained on\nthe MAP4C fingerprint.\n34\nMethods\nDatasets\nThe datasets used in this study were peptide sequences with\nannotated antimicrobial and hemolytic activity collected from\nthe Database of Antimicrobial Activity and Structure of\nPeptides (DBAASP). 13,35 Sequences exhibiting an activity\nmeasure below 10 mM, equivalent to 10 000 nM or 32 mg\nmL−1, against at least one of the selected target organismsP.\naeruginosa, A. baumannii,o r S. aureus were categorized as\nactive. Conversely, sequences with activity measures\nexceeding 10 mM, 10 000 nM, or 32 mg mL−1 against all of\nthese targets were categorized as inactive. When available,\n2030 | RSC Med. Chem., 2024,15,2 0 3 0–2036 This journal is © The Royal Society of Chemistry 2024\nDepartment of Chemistry, Biochemistry and Pharmaceutical Sciences, University of\nBern, Freiestrasse 3, 3012 Bern, Switzerland. E-mail: jean-louis.reymond@unibe.ch\n† Electronic supplementary information (ESI) available. See DOI:https://doi.org/\n10.1039/d4md00159a\nOpen Access Article. Published on 23 April 2024. Downloaded on 11/5/2025 2:53:05 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nRSC Med. Chem.,2 0 2 4 ,15, 2030–2036 | 2031This journal is © The Royal Society of Chemistry 2024\nactivity against human erythrocytes was utilized to classify\nsequences as either hemolytic or non-hemolytic.\nConcentrations were standardized to mM, and sequences\ncausing less than 20% hemolysis at concentrations equal to\nor above 50 mM were categorized as non-hemolytic and\nflagged accordingly. Sequences inducing more than 20%\nhemolysis were classified as hemolytic, irrespective of\nconcentration. The dataset used for the classification tasks\ncontained 9548 (7160 training/2388 validation) sequences\nwith annotated antimicrobial activity, of which 2262 (1723\ntraining/539 validation) sequences had additional hemolytic\nactivity annotations. To test models in low data regimes, we\nrandomly selected subsets from the original training sets,\nrepresenting approximately 20% and 2% of the original\nactivity set, and approximately 10% of the original hemolysis\nset. All datasets are further described in Table 1. To ensure\nconsistency, we maintained the same training and test split\nfor all initial evaluations. For the detailed study, we used the\nsame 5-fold cross-validation sets.\nModels\nAs reference models, we used our previously reported naïve\nBayes (NB), support vector machine (SVM), random forest\n(RF), and recurrent neural network (RNN) classifiers trained\non the same data.\n13 We furthermore trained two additional\nSVM models on alternative representations of peptide\nsequences: one utilizing the MAP4C fingerprint 34 with a\ncustom Jaccard kernel, and another using predicted fraction\nof helical residues and hydrophobic moment with a linear\nkernel. Fraction of helical residues were predicted using\nSPIDER3.\n36 Hydrophobic moment was computed using the\nmethod of Eisenberget al.37\nTo explore the potential of GPT-3 models for antimicrobial\nand hemolytic activity classification, we performed fine-\ntuning of the Ada, Babbage, and Curie models which were\naccessible through the OpenAI API (v0.28.0, accessed between\n25.05.2023 and 01.06.2023). The fine-tuning process involved\ntraining each model using the full, 20% and 2% sets for\nactivity classification and the full and 10% set for the\nhemolysis classification. In the later evaluation with the more\nadvanced LLM GPT-3.5 Turbo, fine-tuning was also\nperformed via OpenAI's Python API (v1.11.1), following the\nprovided guidelines, but we restricted ourselves to the full\nmodel. The utilized fine-tuning datasets contained a system\nrole ( “predicting antimicrobial activity/hemolysis from an\namino acid sequence”), a user message (peptide sequence\nformatted as “SEQUENCE ->”), and a system message (“0”\nfor negative labels and“1” for positive labels).\nMetrics\nAll models were evaluated using five commonly accepted\nperformance metrics: ROC AUC, accuracy, precision, recall\nand F1. Metrics were either calculated using the scikit-\nlearn (v1.4.0) Python (v3.12.1) package (reference models\nand GPT-3.5) or directly obtained from the OpenAI\nplatform after fine-tuning was completed (for all GPT-3\nmodels).\nROC AUC (receiver operating characteristic area under the\ncurve). The ROC AUC measures the area under the receiver\noperating characteristic curve, which plots the true positive\nrate (sensitivity) against the false positive rate. A higher ROC\nAUC value (ranging from 0 to 1) indicates better\ndiscrimination and predictive performance of the model.\nAccuracy. Accuracy measures the overall correctness of the\nmodel's predictions, calculating the ratio of correctly\nclassified instances to the total number of instances. It\nprovides a general understanding of the model's performance\nbut can be misleading in imbalanced datasets.\nAccuracy ¼\nTP þ TN\nTP þ FN þ TN þ FP\nPrecision. Precision measures the proportion of true\npositives out of all predicted positives. It focuses on the\nmodel's ability to avoid false positives.\nPrecision ¼\nTP\nTP þ FP\nRecall. Recall measures the proportion of true positives\nout of all actual positives. It represents the model's ability to\nidentify positive instances accurately.\nRecall ¼\nTP\nTP þ FN\nF1 score. F1 is the harmonic mean of precision and recall.\nIt provides a balanced measure that considers both precision\nand recall.\nF1 ¼\n2 × Precision × Recall\nPrecision þ Recall\nResults and discussion\nModel screening\nStarting from the DBAASP dataset of 9548 peptide sequences\nannotated with antibacterial activity and 2262 peptide\nsequences annotated with hemolysis effect, we had previously\nTable 1 Sizes and composition of the datasets used in the present study.\nDatasets are available at https://github.com/reymond-group/LLM_\nclassifier\nName Size # positive class # negative class\nActivity training 7160 3580 3580\nActivity training 20% 1400 701 699\nActivity training 2% 140 74 66\nActivity validation 2388 1194 1194\nHemolysis training 1723 717 1006\nHemolysis training 10% 170 65 105\nHemolysis validation 539 226 313\nRSC Medicinal Chemistry Research Article\nOpen Access Article. Published on 23 April 2024. Downloaded on 11/5/2025 2:53:05 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n2032 | RSC Med. Chem., 2024,15, 2030–2036 This journal is © The Royal Society of Chemistry 2024\nevaluated NB, RF, SVM and RNN models, and found the\nlatter to perform best for predicting both activity and\nhemolysis from sequence data.\n13,14 For additional reference,\nwe trained an SVM on the fraction of helical residues and the\nhydrophobic moment, two properties commonly known to\ncorrelate with antimicrobial activity, as well as another SVM\non MAP4C, a molecular fingerprint that can reliably encode\nlarge molecules such as natural products and peptides\nincluding their chirality,\n34 a parameter which we considered\nimportant since our data listed sequences containing bothL-\nand D-amino acids.\nAiming to test how LLMs perform in predicting\nantimicrobial activity and hemolysis, we first fine-tuned and\nevaluated GPT-3 Ada, Babbage, and Curie models. As\ndiscussed in our preprint, these models performed slightly\nbetter than the reference models, and even provided good\nperformances when trained in low data regime (20% and 2%\nof full data). However, these models were later deprecated by\nOpenAI and their performance cannot be reproduced. We\ntherefore discuss herein only the results obtained with the\nmore recent GPT-3.5 model, in comparison with the reference\nmodels.\nFor both, prediction of antimicrobial activity and\nprediction of hemolysis, the top-performing models were the\nMAP4C SVM and the RNN model trained on sequence data,\nthe latter being the best performer in our original work\n(Table 2).\n13 The performances for both models were in a\nsimilar range, although the RNN displayed a notably higher\nROC-AUC in both tasks. GPT-3.5 displayed the highest recall\nperformance among the activity models, indicative of the\nmodel's tendency to overly favor positive predictions,\npotentially leading to increased false positive predictions. On\nthe other hand, the features SVM trained only on helicity and\nhydrophobic moment did not perform significantly above\nbackground, and was later used as a negative control model.\nModel comparison\nFollowing the initial model screening, we aimed to validate\nour findings through a more robust approach: a 5-fold cross-\nvalidation involving GPT-3.5, the MAP4C SVM, the RNN, and\nfinally the features SVM as negative control. For this purpose,\nwe generated five data splits and conducted predictions\nanew.\nTable 2 Performance metrics of all models tested on antimicrobial activity and hemolysis classification. The best value for each metric is highlighted in\nbold. NB: naïve Bayes, RF: random forest, SVM: support vector machine, RNN: recurrent neural network,MAP4C: chiral MinHashed atom-pair fingerprint\nof diameter 4, GPT: generative pre-trained transformer\nModel ROC AUC Accuracy Precision Recall F1\nNB act. 0.55 0.55 0.59 0.32 0.42\nRF act. 0.81 0.71 0.7 0.75 0.73\nSVM act. 0.75 0.68 0.68 0.68 0.68\nRNN act. 0.84 0.76 0.74 0.8 0.77\nFeatures SVM act. 0.65 0.65 0.66 0.62 0.64\nMAP4C SVM act. 0.8 0.8 0.79 0.83 0.8\nGPT-3.5 Turbo act. 0.68 0.68 0.62 0.93 0.75\nNB hem. 0.58 0.56 0.48 0.76 0.59\nRF hem. 0.8 0.77 0.81 0.6 0.69\nSVM hem. 0.69 0.73 0.72 0.58 0.65\nRNN hem. 0.87 0.76 0.7 0.76 0.73\nFeatures SVM hem. 0.62 0.63 0.57 0.5 0.54\nMAP4C SVM hem. 0.83 0.83 0.76 0.85 0.8\nGPT-3.5 Turbo hem. 0.65 0.69 0.72 0.43 0.54\nFig. 1 Results of the 5-fold cross-validation study aimed at validating MAP4C SVM, features SVM, RNN, and GPT-3.5 turbo performance for a)\nantimicrobial activity and b) hemolysis predictions. The mean performance across the 5 cross-validations for each metric is shown as a bar, the\nstandard deviation is displayed with an error bar. The results confirmed earlier observations but showed notably higher performances for the RNN\ncompared to the one-shot screening experiment. Both the RNN and MAP4C SVM demonstrated comparable performances.\nRSC Medicinal ChemistryResearch Article\nOpen Access Article. Published on 23 April 2024. Downloaded on 11/5/2025 2:53:05 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nRSC Med. Chem., 2024,15, 2030–2036 | 2033This journal is © The Royal Society of Chemistry 2024\nThe results, depicted in Fig. 1a for antimicrobial activity\nprediction and Fig. 1b for hemolysis prediction, confirmed\nour earlier observations (performances in Table S2 †).\nNotably, the RNN performances were higher than those\nobserved in the screening experiment, and were clearly\nabove those of GTP-3.5. Furthermore, both the RNN and\nMAP4C SVM demonstrated comparable performances,\nindicating the validity of both approaches in predicting\nantimicrobial activity and hemolysis. The finding that\nsimpler machine learning architectures, like SVM, can rival\nthe performance of more complex RNNs in predicting\nantimicrobial activity and hemolysis is particularly\ninteresting. A comparison with models trained on similar\ndatasets, which achieve similar performances as reported in\nthis study, further reinforces the consistency of our\nfindings.\n19–21\nThis raises questions about the importance of model\narchitecture versus foundational elements such as data\nquality and feature engineering. It suggests that a balanced\napproach, prioritizing optimization of these foundational\ncomponents, could prove more beneficial than focusing\nsolely on model complexity.\nData visualization\nThe high performance achieved by the SVM trained on the\nMAP4C fingerprint suggested that the nearest neighbor\nrelationships in the MAP4C feature space could be sufficient\nFig. 2 Chemical space covered by the 9548 peptide sequences with annotated antimicrobial activity extracted from the Database of Antimicrobial\nActivity and Structure of Peptides (DBAASP). The sequences are encoded using the MAP4C fingerprint and the resulting 2048-dimensional space\nreduced to 2D using TMAP. The sequences in the 2D TMAP were colored based on a) heavy atom count, b) fraction of carbon atoms, c) predicted\nfraction of helical residues, d) hydrophobic moment, e) annotated antimicrobial activity and f) annotated hemolysis.\nRSC Medicinal Chemistry Research Article\nOpen Access Article. Published on 23 April 2024. Downloaded on 11/5/2025 2:53:05 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n2034 | RSC Med. Chem.,2 0 2 4 ,15,2 0 3 0–2036 This journal is © The Royal Society of Chemistry 2024\nto distinguish active from inactive and hemolytic from non-\nhemolytic peptide sequences. In our previous work, we\nobserved that the MAP4 fingerprint\n38 correctly clustered\nnatural products, taken from the COCONUT database, 39\naccording to their organism of origin.40,41 In analogy to our\nprevious work, we were curious to see whether a spatial\nseparation of actives/inactives and hemolytic/non-hemolytic\nsequences can be obtained from encoding with MAP4C, the\nchiral version of MAP4, possibly explaining the good\nperformance of the MAP4C SVM model. For this, we reduced\nthe 2048-dimensional feature space of MAP4C to 2D using\nthe dimensionality reduction method TMAP,\n42 and used the\nobtained visualization to display a set of molecular\nproperties.\nFirst, we wanted to confirm that the TMAP visualization\naligns with intuitive distributions of structural features\nrelevant for peptides. For that, we colored the data points\nbased on their heavy atom count (HAC), an indicator of\nmolecular size, and fraction of carbon atoms (fraction C), a\nsimple proxy for the hydrophobicity of a peptide sequence.\nThe TMAP revealed visible clusters for both, HAC (Fig. 2a)\nand fraction C (Fig. 2b), indicating that the reduced MAP4C\nfeatures can reliably represent simple molecular descriptors\nin the underlying chemical space.\nFollowing this first observation, we wanted to test if we\ncan detect clusters within TMAP visualizations of more\ncomplex physicochemical properties, such as the predicted\nfraction of helical residues (Fig. 2c) and the hydrophobic\nmoment (Fig. 2d). In both cases, we could not detect large\nhomogenous clusters as was the case for HAC and fraction C.\nHowever, the data formed a large number of small local\nclusters, indicating that the nearest neighbor relationships in\nthe MAP4C feature space can possibly be used to distinguish\nsequences with high helicity/hydrophobicity opposed to\nsequences with low helicity/hydrophobicity.\nFinally, we analysed the distribution of active versus\ninactive (Fig. 2e) and hemolyticversus non-hemolytic (Fig. 2f)\nsequences in the MAP4C chemical space. Similarly to the\nvisualizations of predicted fraction of helical residues and\nhydrophobic moment, active and inactive or hemolytic and\nnon-hemolytic sequences are spatially separated in a large\nnumber of small, local clusters. This finding is particularly\ninteresting as it suggests that nearest neighbor relationships\nin the MAP4C feature space are sufficient to separate peptide\nsequences based on their antimicrobial activity and\nhemolysis. It further provides an explanation to the good\nperformance obtained with the MAP4C SVM, which can\nleverage the nearest neighbor relationships stored in the\nMAP4C fingerprint feature space when provided with a\ncustom Jaccard kernel function.\nConclusion\nIn the present study we investigated the potential of LLMs as\npredictive tools for antimicrobial activity and hemolysis of\npeptide sequences. We assessed that fine-tuning GPT models\nin cloud is a relatively easy and fast process as access through\nthe API eliminates the need to buy expensive hardware and\nrequires little technical expertise. Duration of fine-tuning was\nshort, and the associated costs were low (Table S3 †). In\ncontrast to cloud-based fine-tuning, local model training\ninvolves setting up and maintaining hardware, which can be\ncostly and require technical expertise. While less complex\nmodels like RNNs and SVMs have lower hardware\nrequirements, training larger models such as LLMs locally\ncan pose challenges in terms of scalability, as one can rapidly\nface limitations in terms of hardware capacity and\nmaintenance costs.\nHowever, the lack of control over the training environment\nin cloud-based approaches raises concerns regarding\nreproducibility of scientific results. In the course of this\nstudy, we had originally fine-tuned GPT-3 models Ada,\nBabbage and Curie. These models performed slightly better\nthan the reference models, even achieving good\nperformances in low data regimes. Unfortunately, these\nmodels were later deprecated by OpenAI and their\nperformance cannot be reproduced. When fine-tuning a\nnewer iteration of GPT-3 (GPT-3.5 Turbo), we observed a\nsignificant decrease in performance for the same task. We\nattribute the drop in performance to the increasing\noptimization of LLMs for conversational interactions, which\nmay negatively impact their effectiveness in out-of-scope\npredictive tasks. These findings highlight the potential risk\nof how not controlling one's own models can compromise\nthe reproducibility and reliability of scientific results.\nThe aforementioned findings suggest a diminishing\nsuitability of chat oriented LLMs for classification tasks over\ntime, a function beyond their intended design. This\nobservation specifically applies to LLMs tailored for\nconversational or human interaction purposes, rather than\nspecialized LLMs trained on domain-specific data.\nUnfortunately, the latter do not provide the ease of access\nand usability that GPT models do. Consequently, we expect\nthat LLMs will increasingly be employed in human\ninteraction settings, facilitating the integration of various\nchemical tools through natural language interfaces as is\nbeing pioneered by Bran\n31 and Boikoet al.32\nFinally, we could demonstrate in the present study that\nclassical machine learning techniques, such as SVMs trained\non MAP4C fingerprint encodings, can achieve state-of-the-art\nperformance in the prediction of antimicrobial activity and\nhemolysis. This finding is especially interesting, as it\nshowcases that good performance can be achieved by less\ncomplex models, putting the emphasis on data quality rather\nthan model complexity.\nCode availability\nThe source codes and datasets used for this study are\navailable at https://github.com/r eymond-group/LLM_\nclassifier.\nRSC Medicinal ChemistryResearch Article\nOpen Access Article. Published on 23 April 2024. Downloaded on 11/5/2025 2:53:05 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nRSC Med. Chem., 2024,15, 2030–2036 | 2035This journal is © The Royal Society of Chemistry 2024\nAuthor contributions\nMO designed and realized the project and wrote the paper.\nJLR designed and supervised the project and wrote the paper.\nBoth authors read and approved the final manuscript.\nConflicts of interest\nThere is no conflict of interest to declare.\nAcknowledgements\nThis work was supported by the Swiss National Science\nFoundation (200020_178998) and the European Research\nCouncil (885076). MO thanks Sacha Javor for the helpful\ndiscussion and comments.\nReferences\n1 M. Lakemeyer, W. Zhao, F. A. Mandl, P. Hammann and S. A.\nSieber, Thinking Outside the Box-Novel Antibacterials To\nTackle the Resistance Crisis, Angew. Chem., Int. Ed. ,\n2018, 57(44), 14440–14475, DOI:10.1002/anie.201804971.\n2 M. Magana, M. Pushpanathan, A. L. Santos, L. Leanse, M.\nFernandez, A. Ioannidis, M. A. Giulianotti, Y. Apidianakis, S.\nBradfute, A. L. Ferguson, A. Cherkasov, M. N. Seleem, C. Pinilla,\nC .D eL aF u e n t e - N u n e z ,T .L a z a r i d i s ,T .D a i ,R .A .H o u g h t e n ,\nR. E. W. Hancock and G. P. Tegos, The Value of Antimicrobial\nPeptides in the Age of Resistance,Lancet Infect. Dis., 2020,20(9),\ne216–e230, DOI:10.1016/S1473-3099(20)30327-3.\n3 N. Mookherjee, M. A. Anderson, H. P. Haagsman and D. J.\nDavidson, Antimicrobial Host Defence Peptides: Functions\nand Clinical Potential, Nat. Rev. Drug Discovery, 2020, 19(5),\n311–332, DOI:10.1038/s41573-019-0058-8.\n4 M. D. T. Torres, S. Sothiselvam, T. K. Lu and C. De La\nFuente-Nunez, Peptide Design Principles for Antimicrobial\nApplications, J. Mol. Biol., 2019, 431(18), 3547–3567, DOI:\n10.1016/j.jmb.2018.12.015.\n5 A. Capecchi and J.-L. Reymond, Peptides in Chemical Space,\nMed. Drug Discovery , 2021, 9, 100081, DOI: 10.1016/j.\nmedidd.2021.100081.\n6 A. T. Müller, J. A. Hiss and G. Schneider, Recurrent Neural\nNetwork Model for Constructive Peptide Design,J. Chem. Inf.\nModel., 2018,58(2), 472–479, DOI:10.1021/acs.jcim.7b00414.\n7 D. Veltri, U. Kamath and A. Shehu, Deep Learning Improves\nAntimicrobial Peptide Recognition, Bioinformatics,\n2018, 34(16), 2740 –2747, DOI: 10.1093/bioinformatics/\nbty179.\n8 S. Liu, Novel 3D Structure Based Model for Activity\nPrediction and Design of Antimicrobial Peptides, Sci. Rep.,\n2018, 8, 11189, DOI:10.1038/s41598-018-29566-5.\n9 X. Su, J. Xu, Y. Yin, X. Quan and H. Zhang, Antimicrobial\nPeptide Identification Using Multi-Scale Convolutional\nNetwork, BMC Bioinf., 2019, 20(1), 730, DOI:10.1186/s12859-\n019-3327-y.\n10 B. Vishnepolsky, G. Zaalishvili, M. Karapetian, T.\nNasrashvili, N. Kuljanishvili, A. Gabrielian, A. Rosenthal,\nD. E. Hurt, M. Tartakovsky, M. Grigolava and M.\nPirtskhalava, De Novo Design and In Vitro Testing of\nAntimicrobial Peptides against Gram-Negative Bacteria,\nPharmaceuticals, 2019,12(2), 82, DOI:10.3390/ph12020082.\n11 F. Plisson, O. Ramírez-Sánchez and C. Martínez-Hernández,\nMachine Learning-Guided Discovery and Design of Non-\nHemolytic Peptides, Sci. Rep. , 2020, 10(1), 16581, DOI:\n10.1038/s41598-020-73644-6.\n12 J. Yan, P. Bhadra, A. Li, P. Sethiya, L. Qin, H. K. Tai, K. H.\nWong and S. W. I. Siu, Deep-AmPEP30: Improve Short\nAntimicrobial Peptides Prediction with Deep Learning,Mol.\nTher.–Nucleic Acids , 2020, 20, 882 –894, DOI: 10.1016/j.\nomtn.2020.05.006.\n13 A. Capecchi, X. Cai, H. Personne, T. Köhler, C. van Delden\nand J.-L. Reymond, Machine Learning Designs Non-\nHemolytic Antimicrobial Peptides, Chem. Sci., 2021, 12(26),\n9221–9232, DOI:10.1039/D1SC01713F.\n14 E. Zakharova, M. Orsi, A. Capecchi and J. Reymond,\nMachine Learning Guided Discovery of Non-Hemolytic\nMembrane Disruptive Anticancer Peptides, ChemMedChem,\n2022, 17(17), DOI:10.1002/cmdc.202200291.\n15 G. Liu, D. B. Catacutan, K. Rathod, K. Swanson, W. Jin, J. C.\nMohammed, A. Chiappino-Pepe, S. A. Syed, M. Fragis, K.\nRachwalski, J. Magolan, M. G. Surette, B. K. Coombes, T.\nJaakkola, R. Barzilay, J. J. Collins and J. M. Stokes, Deep\nLearning-Guided Discovery of an Antibiotic Targeting\nAcinetobacter Baumannii, Nat. Chem. Biol. , 2023, 19,\n1342–1350, DOI:10.1038/s41589-023-01349-8.\n16 F. Wan and C. De La Fuente-Nunez, Mining for\nAntimicrobial Peptides in Sequence Space,Nat. Biomed. Eng.,\n2023, 7, 707–708, DOI:10.1038/s41551-023-01027-z.\n17 M. D. C. Aguilera-Puga and F. Plisson, Structure-Aware\nMachine Learning Strategies for Antimicrobial Peptide\nDiscovery, Research Square, 2024, preprint, DOI: 10.21203/\nrs.3.rs-3938402/v1.\n18 F. Wan, F. Wong, J. J. Collins and C. De La Fuente-Nunez,\nMachine Learning for Antimicrobial Peptide Identification\nand Design, Nat. Rev. Bioeng., 2024, DOI: 10.1038/s44222-\n024-00152-x.\n19 P. B. Timmons and C. M. Hewage, HAPPENN Is a Novel Tool\nfor Hemolytic Activity Prediction for Therapeutic Peptides\nWhich Employs Neural Networks, Sci. Rep. , 2020, 10(1),\n10869, DOI:\n10.1038/s41598-020-67701-3.\n20 M. M. Hasan, N. Schaduangrat, S. Basith, G. Lee, W.\nShoombuatong and B. Manavalan, HLPpred-Fuse: Improved\nand Robust Prediction of Hemolytic Peptide and Its Activity by\nFusing Multiple Feature Representation, Bioinformatics,\n2020, 36(11), 3350–3356, DOI:10.1093/bioinformatics/btaa160.\n21 M. Ansari and A. D. White, Serverless Prediction of Peptide\nProperties with Recurrent Neural Networks, J. Chem. Inf.\nModel., 2023,63(8), 2546–2553, DOI:10.1021/acs.jcim.2c01317.\n22 S. Hochreiter and J. Schmidhuber, Long Short-Term\nMemory, Neural Comput. , 1997, 9(8), 1735 –1780, DOI:\n10.1162/neco.1997.9.8.1735.\n23 K. Cho, B. van Merrienboer, D. Bahdanau and Y. Bengio, On\nthe Properties of Neural Machine Translation: Encoder-\nRSC Medicinal Chemistry Research Article\nOpen Access Article. Published on 23 April 2024. Downloaded on 11/5/2025 2:53:05 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n2036 | RSC Med. Chem., 2024,15, 2030–2036 This journal is © The Royal Society of Chemistry 2024\nDecoder Approaches, arXiv, 2014, preprint, DOI: 10.48550/\narXiv.1409.1259, (accessed 2023-05-31).\n24 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser and I. Polosukhin, Attention Is All\nYou Need, arXiv, 2017, preprint, DOI: 10.48550/\narXiv.1706.03762, (accessed 2023-05-31).\n25 T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P.\nDhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S.\nAgarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A.\nR a m e s h ,D .M .Z i e g l e r ,J .W u ,C .W i n t e r ,C .H e s s e ,M .C h e n ,E .\nSigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S.\nMcCandlish, A. Radford, I. Sutskever and D. Amodei,\nLanguage Models Are Few-Shot Learners,arXiv, 2020, preprint,\nDOI: 10.48550/arXiv.2005.14165, (accessed 2023-05-31).\n26 K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero and B. Smit,\nLeveraging Large Language Models for Predictive Chemistry,\nNat. Mach. Intell., 2024, 6(2), 161–169, DOI: 10.1038/s42256-\n023-00788-1.\n27 A. M. Bran and P. Schwaller, Transformers and Large\nLanguage Models for Chemistry and Drug Discovery,arXiv,\n2023, preprint, DOI:10.48550/arXiv.2310.06083.\n28 T. Guo, K. Guo, B. Nan, Z. Liang, Z. Guo, N. V. Chawla, O.\nWiest and X. Zhang, What Can Large Language Models Do\nin Chemistry? A Comprehensive Benchmark on Eight Tasks,\npart of Advances in Neural Information Processing Systems,\nNeurIPS Proceedings, 2023, vol. 36, pp. 59662–59688.\n29 C. M. Castro Nascimento and A. S. Pimentel, Do Large\nLanguage Models Understand Chemistry? A Conversation\nwith ChatGPT, J. Chem. Inf. Model., 2023, 63(6), 1649–1655,\nDOI: 10.1021/acs.jcim.3c00285.\n30 A. D. White, G. M. Hocky, H. A. Gandhi, M. Ansari, S. Cox,\nG. P. Wellawatte, S. Sasmal, Z. Yang, K. Liu, Y. Singh and\nW. J. Peña Ccoa, Assessment of Chemistry Knowledge in\nLarge Language Models That Generate Code, Digital\nDiscovery, 2023,2(2), 368–376, DOI:10.1039/D2DD00087C.\n31 A. M. Bran, S. Cox, A. D. White and P. Schwaller, ChemCrow:\nAugmenting Large-Language Models with Chemistry Tools,\narXiv, 2023, preprint, DOI: 10.48550/arXiv.2304.05376,\n(accessed 2023-05-31).\n32 D. A. Boiko, R. MacKnight, B. Kline and G. Gomes,\nAutonomous Chemical Research with Large Language\nModels, Nature, 2023, 624(7992), 570 –578, DOI: 10.1038/\ns41586-023-06792-0.\n33 K. M. Jablonka, Q. Ai, A. Al-Feghali, S. Badhwar, J. D.\nBocarsly, A. M. Bran, S. Bringuier, L. C. Brinson, K.\nChoudhary, D. Circi, S. Cox, W. A. De Jong, M. L. Evans, N.\nGastellu, J. Genzling, M. V. Gil, A. K. Gupta, Z. Hong, A.\nImran, S. Kruschwitz, A. Labarre, J. Lála, T. Liu, S. Ma, S.\nMajumdar, G. W. Merz, N. Moitessier, E. Moubarak, B.\nMouriño, B. Pelkie, M. Pieler, M. C. Ramos, B. Ranković,\nS. G. Rodriques, J. N. Sanders, P. Schwaller, M. Schwarting, J.\nShi, B. Smit, B. E. Smith, J. Van Herck, C. Völker, L. Ward, S.\nWarren, B. Weiser, S. Zhang, X. Zhang, G. A. Zia, A. Scourtas,\nK. J. Schmidt, I. Foster, A. D. White and B. Blaiszik, 14\nExamples of How LLMs Can Transform Materials Science\nand Chemistry: A Reflection on a Large Language Model\nHackathon, Digital Discovery , 2023, 2(5), 1233–1250, DOI:\n10.1039/D3DD00113J.\n34 M. Orsi and J.-L. Reymond, One Chiral Fingerprint to Find\nThem All, ChemRxiv, 2023, preprint, DOI: 10.26434/\nchemrxiv-2023-33j02.\n35 G. Gogoladze, M. Grigolava, B. Vishnepolsky, M. Chubinidze,\nP. Duroux, M.-P. Lefranc and M. Pirtskhalava, DBAASP:\nDatabase of Antimicrobial Activity and Structure of Peptides,\nFEMS Microbiol. Lett. , 2014, 357(1), 63–68, DOI: 10.1111/\n1574-6968.12489.\n36 R. Heffernan, K. Paliwal, J. Lyons, J. Singh, Y. Yang and Y.\nZhou, Single-sequence-based Prediction of Protein Secondary\nStructures and Solvent Accessibility by Deep Whole-sequence\nLearning, J. Comput. Chem., 2018, 39(26), 2210–2216, DOI:\n10.1002/jcc.25534.\n37 D. Eisenberg, R. M. Weiss and T. C. Terwilliger, The Helical\nHydrophobic Moment: A Measure of the Amphiphilicity of a\nHelix, Nature, 1982, 299(5881), 371 –374, DOI: 10.1038/\n299371a0.\n38 A. Capecchi, D. Probst and J.-L. Reymond, One Molecular\nFingerprint to Rule Them All: Drugs, Biomolecules, and the\nMetabolome, Aust. J. Chem., 2020, 12(1), 43, DOI: 10.1186/\ns13321-020-00445-4.\n39 M. Sorokina, P. Merseburger, K. Rajan, M. A. Yirik and C.\nSteinbeck, COCONUT Online: Collection of Open Natural\nProducts Database, Aust. J. Chem. , 2021, 13(1), 2, DOI:\n10.1186/s13321-020-00478-9.\n40 A. Capecchi and J.-L. Reymond, Assigning the Origin of\nMicrobial Natural Products by Chemical Space Map and\nMachine Learning, Biomolecules, 2020, 10(10), 1385, DOI:\n10.3390/biom10101385.\n41 A. Capecchi and J.-L. Reymond, Classifying Natural Products\nfrom Plants, Fungi or Bacteria Using the COCONUT\nDatabase and Machine Learning,Aust. J. Chem., 2021, 13(1),\n82, DOI:10.1186/s13321-021-00559-3.\n42 D. Probst and J.-L. Reymond, Visualization of Very Large High-\nDimensional Data Sets as Minimum Spanning Trees,Aust. J.\nChem.,2 0 2 0 ,12(1), 12, DOI:10.1186/s13321-020-0416-x.\nRSC Medicinal ChemistryResearch Article\nOpen Access Article. Published on 23 April 2024. Downloaded on 11/5/2025 2:53:05 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online"
}