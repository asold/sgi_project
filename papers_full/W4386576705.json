{
    "title": "COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models",
    "url": "https://openalex.org/W4386576705",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5087402268",
            "name": "Kanishka Misra",
            "affiliations": [
                null,
                "Purdue University West Lafayette",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5055392823",
            "name": "Julia Taylor Rayz",
            "affiliations": [
                null,
                "Purdue University West Lafayette",
                "University of Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A5012068726",
            "name": "Allyson Ettinger",
            "affiliations": [
                null,
                "Purdue University West Lafayette",
                "University of Chicago"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3118781290",
        "https://openalex.org/W3103816537",
        "https://openalex.org/W4319049323",
        "https://openalex.org/W1571056410",
        "https://openalex.org/W3152996058",
        "https://openalex.org/W4307536702",
        "https://openalex.org/W4205857304",
        "https://openalex.org/W3034775979",
        "https://openalex.org/W3212496002",
        "https://openalex.org/W2985347336",
        "https://openalex.org/W2888922637",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3155744586",
        "https://openalex.org/W1488309867",
        "https://openalex.org/W4312107601",
        "https://openalex.org/W3202546170",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3116216579",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2963583512",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4287110638",
        "https://openalex.org/W2117753247",
        "https://openalex.org/W4310625358",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W2108783283",
        "https://openalex.org/W4389519937",
        "https://openalex.org/W3171696618",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W4287758766",
        "https://openalex.org/W2921890305",
        "https://openalex.org/W3162385798",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W3202712981",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3097977265",
        "https://openalex.org/W4386506836",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W4385574286",
        "https://openalex.org/W3176751053",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2186780112",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3176198948",
        "https://openalex.org/W4280496127",
        "https://openalex.org/W3199748991",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3202070718",
        "https://openalex.org/W3167126457",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W126453896",
        "https://openalex.org/W2125046706",
        "https://openalex.org/W4385569782",
        "https://openalex.org/W4288265479",
        "https://openalex.org/W2890073522",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4221154823",
        "https://openalex.org/W3110879614",
        "https://openalex.org/W4291991132",
        "https://openalex.org/W2027994268",
        "https://openalex.org/W3183248212",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2136480620",
        "https://openalex.org/W1965580172",
        "https://openalex.org/W2985797697",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W4285594979",
        "https://openalex.org/W2119238414",
        "https://openalex.org/W3034912286"
    ],
    "abstract": "A characteristic feature of human semantic cognition is its ability to not only store and retrieve the properties of concepts observed through experience, but to also facilitate the inheritance of properties (can breathe) from superordinate concepts (animal) to their subordinates (dog)—i.e. demonstrate property inheritance. In this paper, we present COMPS, a collection of minimal pair sentences that jointly tests pre-trained language models (PLMs) on their ability to attribute properties to concepts and their ability to demonstrate property inheritance behavior. Analyses of 22 different PLMs on COMPS reveal that they can easily distinguish between concepts on the basis of a property when they are trivially different, but find it relatively difficult when concepts are related on the basis of nuanced knowledge representations. Furthermore, we find that PLMs can show behaviors suggesting successful property inheritance in simple contexts, but fail in the presence of distracting information, which decreases the performance of many models sometimes even below chance. This lack of robustness in demonstrating simple reasoning raises important questions about PLMs' capacity to make correct inferences even when they appear to possess the prerequisite knowledge.",
    "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2928–2949\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nCOMPS : Conceptual Minimal Pair Sentences for testing Robust Property\nKnowledge and its Inheritance in Pre-trained Language Models\nKanishka Misra\nPurdue University\nkmisra@purdue.edu\nJulia Rayz\nPurdue University\njtaylor1@purdue.edu\nAllyson Ettinger\nUniversity of Chicago\naettinger@uchicago.edu\nAbstract\nA characteristic feature of human semantic cog-\nnition is its ability to not only store and retrieve\nthe properties of concepts observed through\nexperience, but to also facilitate the inheri-\ntance of properties (can breathe) from superor-\ndinate concepts (ANIMAL ) to their subordinates\n(DOG )—i.e. demonstrate property inheritance.\nIn this paper, we present COMPS , a collection\nof English minimal pair sentences that jointly\ntests pre-trained language models (PLMs) on\ntheir ability to attribute properties to concepts\nand their ability to demonstrate property inheri-\ntance behavior. Analyses of 22 different PLMs\non COMPS reveal that they can easily distin-\nguish between concepts on the basis of a prop-\nerty when they are trivially different, but find\nit relatively difficult when concepts are related\non the basis of nuanced knowledge represen-\ntations. Furthermore, we find that PLMs can\nshow behaviors suggesting successful property\ninheritance in simple contexts, but fail in the\npresence of distracting information, which de-\ncreases the performance of many models some-\ntimes even below chance. This lack of robust-\nness in demonstrating simple reasoning raises\nimportant questions about PLMs’ capacity to\nmake correct inferences even when they appear\nto possess the prerequisite knowledge.\n1 Introduction\nThe ability to learn, update and deploy one’s knowl-\nedge about concepts ( ROBIN , CHAIR ) and their\nproperties (can fly, can be sat on ), observed dur-\ning everyday experience is fundamental to human\nsemantic cognition (Murphy, 2002; Rogers and Mc-\nClelland, 2004; Rips et al., 2012). Knowledge of\na concept’s properties, combined with the ability\nto infer the IsA relation (Sloman, 1998; Murphy,\n2003) leads to an important behavior known as\nproperty inheritance (Quillian, 1967; Smith and\nEstes, 1978; Murphy, 2002), where subordinates\nof a concept inherit its properties. For instance,\none is likely to infer that an entity called luna can\nmeow, has a tail, is a mammal, etc., even if all\nthey know is that it is a cat. The close connection\nbetween a word’s meaning and its conceptual repre-\nsentation makes these abilities crucial to language\nunderstanding (Murphy, 2002; Lake and Murphy,\n2021), making it critical for computational mod-\nels of language processing to also exhibit behav-\nior consistent with these capacities. Indeed, mod-\nern pre-trained language models (PLMs; Devlin\net al., 2019; Brown et al., 2020, etc.) have made\nimpressive empirical strides in eliciting general\nknowledge about real world concepts and entities\n(Petroni et al., 2019; Weir et al., 2020, i.a.), as well\nas in demonstrating isomorphism with real world\nabstractions like direction and color (Abdou et al.,\n2021; Patel and Pavlick, 2022), often times without\neven having been explicitly trained to do so. At\nthe same time, their ability to robustly demonstrate\nsuch capacities has recently been called to question,\nowing to failures due to reporting bias (Gordon and\nVan Durme, 2013; Shwartz and Choi, 2020), lack\nof consistency (Elazar et al., 2021; Ravichander\net al., 2020), and sensitivity to lexical cues (Kass-\nner and Schütze, 2020; Misra et al., 2020; Pandia\nand Ettinger, 2021).\nIn this work, we cast further light on PLMs’\nability to robustly demonstrate knowledge about\nconcepts and their properties. To this end, we intro-\nduce Conceptual Minimal Pair Sentences (COMPS ),\na collection of English minimal pair sentences,\nwhere each pair attributes a property ( can fly) to\ntwo noun concepts: one which actually possesses\nthe property ( ROBIN ), and one which does not\n(PENGUIN ). Following standard practice in the\nminimal pairs evaluation paradigm (Warstadt et al.,\n2020, etc.), we test whether PLMs prefer sentence\nstimuli expressing correct property knowledge over\nthose expressing incorrect ones. COMPS can be de-\ncomposed into three subsets, each containing stim-\nuli that progressively isolate deeper understanding\nof the task of attributing properties to concepts,\n2928\nby adding controls for more superficial heuristics.\nOur first subset—COMPS -BASE —measures the ex-\ntent to which PLMs attribute properties to the right\nconcepts, while varying the similarity of the posi-\ntive (ROBIN ) and the negative concepts (PENGUIN\n[high] vs. TABLE [low]). This controls for the pos-\nsibility that models are relying on coarse-grained\nconcept distinctions. For instance, in this setup a\nmodel should prefer (1a) over both versions of (1b).\n(1) a. A robin can fly.\nb. *A ( penguin/table) can fly.\nNext, drawing on the phenomenon of property in-\nheritance, the COMPS -WUGS set introduces a novel\nconcept, WUG , expressed as the subordinate of the\npositive and negative concepts from a subset of\nthe COMPS -BASE set, and tests the extent to which\nPLMs successfully attribute it the given property\nwhen it is associated with the positive concept. This\nincreases the complexity of the reasoning task, as\nwell as the distance between the associated concept\n(ROBIN ) and property (can fly). These manipula-\ntions help to control for memorization of the literal\nphrases being tested, forcing models to judge prop-\nerties for a novel concept that inherits the property\nfrom a known concept. In this task, given that a\nmodel successfully prefers (1a) over (1b), it should\nalso prefer (2a) over (2b):\n(2) a. A wug is a robin. Therefore, a wug can fly.\nb. *A wug is a penguin. Therefore, a wug can fly.\nThe final subset—COMPS -WUGS -DIST , combines\nthe aforementioned controls by using negative con-\ncepts as distracting content and inserting them into\nthe COMPS -WUGS stimuli. Specifically, we trans-\nform the stimuli of COMPS -WUGS by creating two\nsubordinates for every minimal pair; one for the\npositive concept (ROBIN , subordinate: WUG ) and\nthe other for the negative concept (PENGUIN , sub-\nordinate: DAX), which acts as a distractor. This\nway, we control for the possibility that models may\nbe relying on simple word associations between\ncontent words—of which there are only two in the\nprior tests—by introducing additional, irrelevant\nbut contentful words into the context. Here, we\nconsider models to be correct if they prefer ( 3a)\nover (3b), given that they prefer (1a) over (1b):\n(3) a. A wug is a robin. A dax is a penguin. Therefore, a\nwug can fly.\nb. *A wug is a robin. A dax is a penguin. Therefore,\na dax can fly.\nTogether, the three sets of stimuli tease apart more\nsuperficial predictive behaviors, such as contex-\ntual word associations, from more robust reasoning\nbehaviors based on understanding of concept prop-\nerties. While we can expect superficial predictive\nstrategies to be brittle in the face of shallow pertur-\nbations and irrelevant distractions, robust property\nknowledge and reasoning behaviors should not.\nWe useCOMPS to analyze robust property knowl-\nedge and its inheritance in 22 different PLMs,\nranging from small masked language models to\nbillion-parameter autoregressive language models.\nIn our experiments with COMPS -BASE , we find\nPLMs to demonstrate strong performance in at-\ntributing properties to the correct concepts in our\nminimal pairs. However, we observe this strong\nperformance largely when the concepts in the min-\nimal pairs are trivially different (e.g., LION and\nTEA for the property is a mammal ). When the\nconcept pairs are similar (on the basis of differ-\nent knowledge representations), we find models’\nperformance to degrade substantially, by as much\nas 25 points. We observe a similar trend in our\nanalyses on COMPS -WUGS —models first appear\nto show desirable behavior, potentially indicating\nproficiency in the more complex property inher-\nitance reasoning. However, their overall perfor-\nmance declines drastically when investigated in\nthe presence of distractors (i.e., on COMPS -WUGS -\nDIST ). This failure is particularly pronounced in\nlarger autoregressive PLMs, whose performance\nin fact drops below chance in cases where distract-\ning information is proximal to the queried prop-\nerty, indicating the presence of a proximity ef-\nfect. Together, our findings highlight brittleness\nof PLMs with conceptual knowledge and reason-\ning, as evidenced by failures in the face of simple\ncontrols. We make our code and data available at:\nhttps://github.com/kanishkamisra/comps.\n2 Conceptual Minimal Pair Sentences\n(COMPS )\n2.1 Connections to prior work\nPrior work in exploring property knowledge in\nPLMs has adopted two different paradigms: one\nwhich uses probing classifiers to test if the applica-\nbility of a property can be decoded from the repre-\nsentations of LMs (Forbes et al., 2019; Da and Ka-\nsai, 2019; Derby et al., 2021); and the other which\nuses cloze-testing, in which LMs are tasked to fill\nin the blank in prompts that describe specific prop-\n2929\nerties/factual knowledge about the world (Petroni\net al., 2019; Weir et al., 2020). We argue that both\napproaches—though insightful—have key limita-\ntions for evaluating property knowledge, and that\nminimal pair testing overcomes these limitations to\na beneficial extent.\nApart from ongoing debates surrounding the va-\nlidity of probing classifiers (see Hewitt and Liang,\n2019; Ravichander et al., 2021; Belinkov, 2022),\nthe probing setup does not allow the testing of prop-\nerty knowledge in a precise manner. Specifically,\nseveral properties are often perfectly correlated in\ndatasets such as the one we use here (see §2.2). For\nexample, the property of being an animal and being\nable to breathe and grow, etc., are all perfectly cor-\nrelated with one another. Even if the model’s true\nknowledge of these properties is highly variable,\nprobing its representations for them would yield the\nexact same result, leading to conclusions that over-\nestimate the model’s capacity for some properties,\nwhile underestimating for others. Evaluation using\nminimal pair sentences overcomes this limitation\nby allowing us to explicitly represent the proper-\nties of interest in language form, thereby allowing\nprecise testing of property knowledge.\nSimilarly, standard cloze-testing of PLMs\n(Petroni et al., 2019; Weir et al., 2020; Jiang et al.,\n2021) also faces multiple limitations. First, it does\nnot allow for testing of multi-word expressions,\nas by definition, it involves prediction of a sin-\ngle word/token. Second, it does not yield faithful\nconclusions about one-to-many or many-to-many\nrelations: e.g. the cloze prompts “Ravens can .”\nand “ can fly.” do not have a single correct\nanswer. This makes our conclusions about mod-\nels’ knowledge contingent on choice of one correct\ncompletion over the other. The minimal pair eval-\nuation paradigm overcomes these issues by gen-\neralizing the cloze-testing method to multi-word\nexpressions—by focusing on entire sentences—\nand at the same time, pairing every prompt with\na negative instance. This allows for a straightfor-\nward way to assess correctness: the choice between\nmultiple correct completions is transformed into\none between correct and incorrect, at the cost of\nhaving several different instances (pairs) for test-\ning knowledge of the same property. Additionally,\nthe minimal pairs paradigm allows us also to shed\nlight on how the nature of negative samples affects\nmodel behavior, which has been missing in ap-\nproaches using probing and cloze-testing. The us-\nage of minimal pairs is a well-established practice\nin the literature, having been widely used in works\nthat analyze syntactic knowledge of LMs (Marvin\nand Linzen, 2018; Futrell et al., 2019; Warstadt\net al., 2020). We complement this growing liter-\nature by introducing minimal-pair testing to the\nstudy of conceptual knowledge in PLMs.\nOur property inheritance analyses closely relate\nto the ‘Leap-of-Thought’ (LoT) framework of Tal-\nmor et al. (2020). In particular, LoT holds the\ntaxonomic relations between concepts implicit and\ntests whether models can abstract over them to\nmake property inferences—e.g., testing the extent\nto which models assign Whales have bellybuttons\nthe ‘True’ label, given that Mammals have belly-\nbuttons (with the implicit knowledge here being\nWhales are mammals). With COMPS -WUGS (and\nCOMPS -WUGS -DIST ), we instead explicitly pro-\nvide the relevant taxonomic knowledge in the con-\ntext and target whether PLMs can behave consis-\ntently with knowledge they have already demon-\nstrated (in the base case, COMPS -BASE ) and at-\ntribute the property in question to the correct subor-\ndinate concept. This also relates to recent work that\nmeasures consistency of PLMs’ word prediction\ncapacities in eliciting factual knowledge (Elazar\net al., 2021; Ravichander et al., 2020).\n2.2 Ground-truth Property Knowledge data\nFor our ground-truth property knowledge resource,\nwe use a subset of the CSLB property norms col-\nlected by Devereux et al. (2014), which was fur-\nther extended by Misra et al. (2022). The origi-\nnal dataset was constructed by asking 123 human\nparticipants to generate properties for 638 every-\nday concepts. Contemporary work has used this\ndataset by taking as positive instances all concepts\nfor which a property was generated, while taking\nthe rest as negative instances (Lucy and Gauthier,\n2017; Da and Kasai, 2019, etc.) for each prop-\nerty. While this dataset has been popularly used in\nrelated literature, Misra et al. (2022) recently dis-\ncovered striking gaps in coverage among the prop-\nerties included in the dataset. 1 For example, the\nproperty can breathe was only generated for 6 out\nof 152 animal concepts, despite being applicable\nfor all of them—as a result, contemporary work can\nbe expected to have wrongfully penalized models\nthat attributed this property to animals that could\n1See also Sommerauer and Fokkens (2018) and Sommer-\nauer (2022), who also discuss this limitation.\n2930\nindeed breathe, and similarly for other properties.\nTo remedy this issue, Misra et al. (2022) manually\nextended CSLB’s coverage for 521 concepts and\n3,645 properties. We refer to this extended CSLB\ndataset as XCSLB, and we use it as our source for\nground-truth property knowledge.\n2.3 Choosing negative samples\nWe rely on a diverse set of knowledge represen-\ntation sources to construct negative samples for\nCOMPS . Each source has a unique representational\nstructure which gives rise to different pairwise sim-\nilarity metrics, on the basis of which we pick out\nnegative samples for each property:\nTaxonomy We consider a hierarchical organiza-\ntion of our concepts, by taking a subset of WordNet\n(Miller, 1995) consisting of our 521 concepts. We\nuse the wup similarity (Wu and Palmer, 1994) as\nour choice of taxonomic similarity.\nProperty Norms We use the XCSLB dataset and\norganize it as a matrix whose rows indicate con-\ncepts and columns indicate properties that are ei-\nther present (indicated as 1) or absent (indicated\nas 0) for each concept. As our similarity measure,\nwe consider the jaccard similarity between the row\nvectors of concepts. This reflects the overlap in\nproperties between concepts, and is prevalent in\nstudies utilizing conceptual similarity in cognitive\nscience (Tversky, 1977; Sloman, 1993, etc.).\nCo-occurrence We use the co-occurrence be-\ntween concept words as an unstructured knowledge\nrepresentation. For quantifying similarity, we use\nthe cosine similarity of the GloVe vectors (Penning-\nton et al., 2014) of our concept words.\nSampling Strategy Each property ( pi) in our\ndataset splits the set of concepts into two: a set\nof concepts that possess the property ( Qpi), and\na set of concepts that do not ( ¬Qpi). We sample\nmin(|Qpi|, 10)—i.e., at most 10—concepts from\nQpi and take them to be our positive set. Then for\neach concept in the positive set, we sample from\n¬Qpi the concept that is most similar (depending\non the source) to the positive concept and take it as\na negative concept for the property. We addition-\nally include a negative concept that is randomly\nsampled from ¬Qpi, leaving out the concepts sam-\npled on the basis of the three previously described\nknowledge sources. Examples of the four types of\nnegative samples for the concept ZEBRA and the\nproperty has striped patterns are shown in Table 1.\nKnowledge Rep. Negative Concept Similarity\nTaxonomy HORSE 0.88\nProperty Norms DEER 0.63\nCo-occurrence GIRAFFE 0.75\nRandom BAT -\nTable 1: Negatively sampled concepts selected on the ba-\nsis of various knowledge representational mechanisms,\nwhere the property is has striped patterns, and the posi-\ntive concept is ZEBRA .\n2.4 Minimal Pair Construction\nFollowing our negative sample generation process,\nwe end up with total of 49,280 pairs of positive and\nnegative concepts that span across 3,645 properties\n(14 pairs per property, on average). Every prop-\nerty is associated with a property phrase—a verb\nphrase which expresses the property in English, as\nprovided in XCSLB. Using these materials, we con-\nstruct our three datasets of minimal pair sentence\nstimuli, examples of which are shown in Figure 1.\nCOMPS -BASE The COMPS -BASE dataset con-\ntains minimal pair sentences that follow the tem-\nplate: “[DET] [CONCEPT] [property-phrase].”\nwhere [DET] is an optional determiner, and\n[CONCEPT] is the noun concept. Applying this\ntemplate to our generated pairs results in 49,280\ninstances. See Figure 1a for an example.\nCOMPS -WUGS We test property inheritance in\nPLMs using only the animal kingdom subset of\nCOMPS -BASE (152 concepts, 944 properties, and\n13,888 pairs), keeping the same negative samples.\nWe convert the original minimal pair sentences in\nCOMPS -BASE , in which the positive concept is an\nanimal, into pairs of two-sentence stimuli by first\nintroducing a new concept ( WUG ) to be the sub-\nordinate of the concepts in the original minimal\npair. We then express its property inheritance in\na separate sentence. Our two sentence stimuli fol-\nlow the template: “A wug is a [CONCEPT]. There-\nfore, a wug [property-phrase].” Although we\nuse wug as our running example for the subordi-\nnate concept, we use four different nonsense words\n{wug, dax, blicket, fep} equal numbers of times,\nto avoid making spurious conclusions based on a\nsingle nonsense word.2 Introducing an intervening\nnovel concept allows us to robustly control for sim-\nple word-level associations between concepts and\nproperties that models might have picked up during\n2As we describe in §4, we also tried a different set of nonce\nwords, to address concerns about possible impacts of using\nnonce words from existing literature (e.g., wug).\n2931\nProperty: can fly\nPositive: ROBIN\nNegative: PENGUIN\nSubordinate: WUG\nCOMPS -BASE : A (robin/penguin) can fly.\nCOMPS -WUGS : A wug is a ( robin/penguin).\nTherefore, a wug can fly.\n(a) Instances of COMPS -BASE and COMPS -WUGS .\nA dax is a penguin.\n         A wug is a robin.         Therefore, a (wug/dax) can fly.\nin-between before \n(b) Distraction scheme for stimuli in COMPS -WUGS -DIST , where\nthe distractor is inserted either before or in between each COMPS -\nWUGS stimulus.\nFigure 1: Examples of materials used in our experiments. In this example, ROBIN is the positive concept.\ntraining. Figure 1a shows an example.\nCOMPS -WUGS -DIST To add distracting infor-\nmation, we follow Pandia and Ettinger (2021)\nand convert the COMPS -WUGS stimuli by associ-\nating a different subordinate concept ( DAX) with\nthe negative concept ([NEG-CONCEPT]), and insert-\ning it before or in-between the sentence contain-\ning the positive concept and its subordinate, sepa-\nrately. This results in two subsets (before and in-\nbetween) of three-sentence minimal pair stimuli,\nwhich differ in the subordinate to which the prop-\nerty is attributed. We use the following template\nto create our stimuli: “A wug is a [CONCEPT]. A\ndax is a [NEG-CONCEPT]. Therefore, a (wug/dax)\n[property-phrase].” That is, we have stimuli\nthat resemble COMPS -WUGS but instead deal with\na pair of competing subordinate concepts in con-\ntext.3 See Figure 1b for an example.\n3 Methodology\n3.1 Models Investigated\nWe investigate property knowledge and property\ninheritance capacities of 22 different PLMs, be-\nlonging to six different families. We evaluate four\nwidely used masked language modeling (MLM)\nfamilies: (1) ALBERT (Lan et al., 2020), (2) BERT\n(Devlin et al., 2019), (3) ELECTRA (Clark et al.,\n2020), and (4) RoBERTa (Liu et al., 2019); as well\nas two auto-regressive language modeling families:\n(1) GPT2 (Radford et al., 2019), and (2) the GPT-\nNeo (Black et al., 2021) and GPT-J models (Wang\nand Komatsuzaki, 2021) from EleutherAI. We also\nuse distilled versions of BERT-base, RoBERTa-\nbase, and GPT2, trained using the method de-\nscribed by Sanh et al. (2019). We list each model’s\nparameters, vocabulary size, and training corpora\nin Table 3 (Appendix A).\n3We again choose from our list of four nonsense words\n(wug, dax, blicket, and fep), which amounts to 12 unique\nordered pairs, after accounting for counterbalancing.\n3.2 Measuring Performance\nTo evaluate models on COMPS , we compare\ntheir log-probabilities for the property phrase—\nconditioned on contexts (to the left) containing the\npositive and negative noun concepts. That is, we\nhold the property phrase constant, and compare\nacross minimally differing conditions to evaluate\nthe probability with which a property is attributed\nto each concept. For example, we score stimuli in\nCOMPS -BASE , e.g., “A dog can bark.” as:\nlog p(can bark. |A dog),\nits corresponding stimulus in COMPS -WUGS , “A\nwug is a dog. Therefore, a wug can bark.” as:\nlog p(can bark. |A wug is a dog. Therefore, a wug),\nand similarly—assuming CAT as the negative\nconcept—the corresponding stimuli in our COMPS -\nWUGS -DIST subset, “A wug is a dog. A dax is a cat.\nTherefore, a wug can bark.” as:4\nlog p(can bark. |A wug is a dog. A dax is a cat. There-\nfore, a wug).\nThis approach to eliciting conditional LM judg-\nments is equivalent to the “scoring by premise”\nmethod (Holtzman et al., 2021), which has been\nshown to result in stable comparisons across items.\nAdditionally, this also takes into account the poten-\ntial noise due to frequency effects or tokenization\ndifferences (Misra et al., 2021). Estimating these\nconditional log-probabilities using auto-regressive\nPLMs can be directly computed in a left-to-right\nmanner. For MLMs, we use their conditional\npseudo-loglikelihoods (Salazar et al., 2020) as a\nproxy for conditional log-probabilities.\nBased on this simple method of eliciting relative\nacceptability measures from PLMs, we evaluate\n4Here we show an example where the distractor is added\nin-between the context specifying the positive concept, and\nthe queried property knowledge.\n2932\n0.61\n0.60\n0.77\n0.60\n0.64\n0.61\n0.81\n0.61\n0.66\n0.63\n0.83\n0.63\n0.73\n0.66\n0.86\n0.68\n0.65\n0.61\n0.80\n0.62\n0.67\n0.62\n0.80\n0.63\n0.64\n0.62\n0.82\n0.62\n0.57\n0.59\n0.75\n0.57\n0.60\n0.61\n0.78\n0.60\n0.61\n0.61\n0.78\n0.61\n0.64\n0.62\n0.82\n0.63\n0.58\n0.61\n0.76\n0.60\n0.76\n0.72\n0.90\n0.71\n0.68\n0.66\n0.86\n0.64\n0.54\n0.60\n0.71\n0.56\n0.71\n0.68\n0.87\n0.67\n0.63\n0.62\n0.81\n0.59\n0.70\n0.66\n0.88\n0.66\n0.67\n0.64\n0.86\n0.63\n0.72\n0.67\n0.88\n0.67\n0.64\n0.61\n0.80\n0.61\n0.70\n0.66\n0.86\n0.67\n0.33 0.36 0.38 0.44 0.36 0.37 0.37 0.320.34 0.34 0.370.33 0.53 0.430.31 0.470.36 0.44 0.40 0.460.36 0.42Overall\nRandom\nCo-oc\nProp. Norm\nTaxonomic\nA-b A-l A-xl A-xxl dB-b B-b B-l E-s E-b E-l dR-b R-b R-l dGPT2GPT2GPT2-mGPT2-lGPT2-xlNeo-125MNeo-1.3bNeo-2.7BGPT-J\n0.2 0.4 0.6 0.8 1.0Accuracy\nFigure 2: Accuracies of PLMs on COMPS -BASE under various negative sampling schemes. Chance performance for\nall rows is 50%, except for ‘Overall,’ where it is 6.25%. Refer to Table 3 for unabbreviated model names.\na model’s accuracy on all COMPS stimuli as the\npercentage of times its log-probability for a prop-\nerty is greater when conditioned on the context\nthat attributes the property to the positive—as op-\nposed to the negative—concept. Since all cases are\nforced-choice tasks between two instances, chance\nperformance is set to 50%. Table 4 (Appendix B)\nshows examples of all COMPS stimuli and GPT-J’s\nconditional log-probabilities for them.\n4 Experiments and Analyses\n4.1 Base property knowledge of PLMs and\ntheir sensitivity to similarity effects\nWe begin by evaluating the 22 PLMs on COMPS -\nBASE . Here we focus on the extent to which models\nrobustly associate properties to the correct concepts\nacross stimuli with varying kinds of similarity be-\ntween the positive and negative concepts. We re-\nport accuracies of the 22 PLMs on COMPS -BASE\nacross the four different negative sampling schemes\nthat we specified in §2.3. We additionally report a\nmore stringent accuracy measure that we refer to\nas ‘Overall accuracy,’ which is calculated for every\nproperty and its positive concept, as the percentage\nof times a model correctly attributes the property\nto the positive concept in all four types of nega-\ntive sampling schemes. Chance performance for\nonly the ‘Overall’ case is then 6.25% (0.54 ×100).\nFigure 2 shows these results.\nFrom Figure 2, we see that models strongly dis-\ntinguish between positive and negative concepts\nin cases where they are dramatically different—\ni.e., where negative concepts were sampled ran-\ndomly (e.g., BEAR [positive] vs BOTTLE [negative]\nfor the property can breathe). However perfor-\nmance drops substantially when there are subtler\ndifferences between the two concepts—e.g, the\nconcepts WALRUS (positive) and SHARK (negative)\nfor the property is a mammal . For instance, the\nbest performing model in any similarity-based neg-\native sampling scheme (GPT-J, 76%, ‘Co-oc’) only\nslightly outperforms the worst model in the random\nnegative sampling scheme (Neo-125M, 71%). The\nperformance of PLMs is not substantially different\nacross the three similarity-based negative sampling\nschemes, suggesting that the dynamics of model\nsensitivity in attributing properties to concepts are\nlargely harmonized across various types of similari-\nties. As a result of models’ insensitivity in presence\nof similar negative concepts, the overall accuracies\nare very modest in value, with the overall accu-\nracy of the best performing model (GPT-J) being\nonly 53%. This overall performance is, however,\nsignificantly above chance (6.25%). We discuss ad-\nditional findings, such as performance by property\ntype and model size, in Appendix C, since they are\nincidental to the main conclusions of this analysis.\n4.2 Property inheritance in PLMs\nHaving established the base property knowledge\nof PLMs, we now investigate the extent to which\nthey can show behavior that is consistent with\nreasoning required to handle property inheritance.\nWe first investigate their performance onCOMPS -\nWUGS , created using the subset of COMPS -BASE\ncontaining only animal concepts (see §2.4 for stim-\nulus construction). Table 2 shows average accu-\nracies obtained by PLMs on our property inheri-\ntance stimuli, and compares them to average ac-\ncuracies on COMPS -BASE —aggregating across all\nnegative sampling schemes. Recall that the stim-\nuli in COMPS -WUGS present a more challenging\nproperty attribution task than in COMPS -BASE , by\n2933\nCOMPSsubset Size Acc.\nBASE 49.3K 68.41.7\nBASE(animal kingdom only) 13.8K 67.12.0\nWUGS 13.8K 68.92.3\nWUGS-DIST(before) 13.8K 59.2 3.9\nWUGS-DIST(in-between) 13.8K 47.2 4.5\nTable 2: Average accuracy (and standard error of the\nmean) of PLMs (N = 22) on each of our COMPS sub-\nsets. Chance performance is 50% throughout.\nnot only controlling for coarse-grained similarity\neffects, but also introducing an intervening novel\nconcept that is expected to inherit the properties of\nthe positive concept. By measuring attribution of\nproperties more indirectly, these stimuli increase\nthe complexity of the reasoning and control for\nmemorization of the literal phrase initially tested\nwith COMPS -BASE .\nTable 2 shows the average accuracy of the PLMs\non each subset of COMPS . Despite the increase\nin complexity, we see that PLMs actually show\nslightly stronger performance on COMPS -WUGS\n(68.9%) than on COMPS -BASE (67.1%). This\nmeans that there are instances in which models\nprefer the property in the positive context over the\nnegative context (4a > 4b), but show the opposite\nbehavior in COMPS -BASE (4d > 4c).\n(4) a. A wug is a robin. Therefore, a wug can fly.\nb. A wug is a penguin. Therefore, a wug can fly.\nc. A robin can fly.\nd. A penguin can fly.\nThis pattern of performance could lead to spurious\nconclusions that models are successfully execut-\ning property inheritance, when in fact they show a\nlack of the pre-requisite property knowledge based\non their failure on COMPS -BASE . We will discuss\nthese inconsistencies in more detail below. Over-\nall, however, the relatively strong performance on\nCOMPS -WUGS suggests that models are largely un-\naffected when we control for simple memorization\nof tested phrases—e.g., robin can fly—by linking\nknown concepts to properties through an interven-\ning subordinate concept (wug). This suggests that\nmodels are not relying on simple memorization, but\ndoes not control for the possibility of simple asso-\nciation between content words (robin and fly)—for\nthis we turn to COMPS -WUGS -DIST .\nThe COMPS -WUGS -DIST test assesses whether\nmodels retain strong property attribution perfor-\nmance when content words in the context are not\nall relevant for the property prediction. The stimuli\nthus include irrelevant distractor concepts and their\nWUGS WUGS-DIST\n(before)\nWUGS-DIST\n(in-between)\n0%\n25%\n50%\n75%\n100%\nCOMPS-BASE outcome\n% correct response\nFigure 3: Distribution of model performance on COMPS -\nWUGS and COMPS -WUGS -DIST (both subsets) across\npossible outcomes (correct = ✓, incorrect = ✗) of the\nmodels on corresponding minimal pairs in COMPS -\nBASE . Error bars indicate 95% CI, while dashed line\nindicates chance performance (50%).\nsubordinates—which, in a robust model, should\nnot affect attribution of the property to the correct\nconcept (see §2.4 for stimulus construction).\nFrom Table 2, the average accuracies of PLMs\non both subsets of COMPS -WUGS -DIST (before\nand in-between) indicate that overall, models now\nshow clear degradation in property inheritance per-\nformance as a result of the distracting informa-\ntion. Specifically, the PLMs’ performance drops\nby 9.7 points on instances when the distracting in-\nformation is added before the relevant context and\nqueried property, and by 21.7 points on instances\nwhere it is addedin-between the two, relative to the\nundistracted property inheritance stimuli (COMPS -\nWUGS ). Notably, the latter drop in performance\nbrings models level with chance accuracy (we fail\nto reject the null hypothesis that avg. accuracy of\nmodels is 50%; p = .62, Wilcoxon signed rank\nexact text), highlighting a pronounced lack of ro-\nbustness in PLMs’ capacity to attribute properties\nto the correct concepts in their input context.\nAccounting for spurious performance The\nCOMPS -WUGS results above raise the concern that\nmodels are often showing spurious performance:\naccurately demonstrating property inheritance be-\nhavior without actually possessing the right prop-\nerty knowledge. To shed more light on this poten-\ntial issue, we plot the distribution of model accura-\ncies on our property inheritance stimuli ( COMPS -\nWUGS and COMPS -WUGS -DIST ) divided based on\ntheir outcomes on the corresponding stimuli in\nCOMPS -BASE . Figure 3 shows these distributions.\nIn COMPS -WUGS and both subsets of COMPS -\nWUGS -DIST , models show this spurious correct\nbehavior on 41.3%, 55.6%, and 42.8% of instances\nin which they produce incorrect judgments on the\ncorresponding COMPS -BASE stimuli (yellow bars\n2934\nChance Performance\n0%\n25%\n50%\n75%\n100%\nA-b A-l A-xl A-xxl dB-b B-b B-l E-s E-b E-l dR-b R-b R-l dGPT2GPT2GPT2-mGPT2-lGPT2-xlNeo-125MNeo-1.3bNeo-2.7BGPT-J\nAccuracy\nWUGS WUGS-DIST (before) WUGS-DIST ( in-between)\nFigure 4: Accuracies of individual models (grouped by family, in increasing order based on number of parameters)\non COMPS -WUGS and COMPS -WUGS -DIST . Black dashed line indicates chance performance (50%). Refer to\nTable 3 for unabbreviated model names. Error bands indicate 95% Bootstrap CIs.\nin Figure 3). This non-trivial proportion of cases\nwith spurious performance further reinforces the\nidea that PLMs’ successful predictions on these\ntests are likely relying on heuristics rather than ro-\nbust inferences about property knowledge. We can\nremove the effects of these spurious instances by\nfiltering to items in which models give the correct\nanswer on COMPS -BASE (blue bars in Figure 3)—\nthough we see that the overall conclusions remain\nthe same after this filtering.\nOn the pronounced effect of proximity in autore-\ngressive PLMs Our previous discussion summa-\nrized the aggregate property inheritance behavior\nof the 22 PLMs we considered—we now zoom in\nfor a model-wise analysis. Figure 4 shows models’\nrelative accuracies on COMPS -WUGS and COMPS -\nWUGS -DIST , filtering to items with correct COMPS -\nBASE performance, as in the blue bars of Figure 3.\nConsistent with our overall findings, we observe\ndistracting content to substantially degrade model\nperformance across the board.5 A particularly note-\nworthy pattern is that the degradation in autore-\ngressive PLM families—GPT2 and EleutherAI—\nshows a stark sensitivity toproximity effects. While\nthese classes of model seem to suffer less when\ndistracting content is added before the context con-\ntaining the positive concept (thus placing the dis-\ntraction farther from the queried property), they\nshow substantially worse performance when the\nopposite is the case (i.e., when distraction is added\nin-between, and is therefore closer to the queried\nproperty). This degradation due to proximity of\nthe distracting content becomes catastrophically\nworse as models grow larger in the number of\n5See also Pandia and Ettinger (2021) for a similar degrada-\ntion of performance on cloze-tasks involving factual retrieval.\npre-trained parameters—in fact bringing their\nperformance down to as much as 26.2 points be-\nlow chance (in GPT-J, which has 6B parameters).\nWhile MLMs also show similar levels of degraded\nperformance in presence of distraction, they do not\nseem to show any systematic sensitivity to proxim-\nity effects, likely due to their bidirectional nature.\nResults on GPT-3 In addition to our main ex-\nperiments, we also evaluate GPT-3 (Brown et al.,\n2020) models on a small subset of COMPS stimuli\n(denoted as miniCOMPS ). Results from this anal-\nysis (shown in Appendix C.1) are largely aligned\nwith our main conclusions, with all GPT-3 models—\nincluding the largest one (175B parameters)—\nperforming worse than chance on the in-between\nsubset of miniCOMPS -WUGS -DIST , while perform-\ning substantially better on miniCOMPS -WUGS and\nminiCOMPS -WUGS -DIST (before). Together with\nour main results, this indicates that scaling alone\nmay be insufficient to elicit robust inferences about\nconcepts and their properties.\nChoice of nonce words Nonce words constitute\nan important design decision for our stimuli—we\nfollowed precedents in language acquisition re-\nsearch (Berko, 1958; Gopnik and Sobel, 2000, i.a.)\nand used previously existing nonce words (such\nas wug and blicket) to represent novel concepts in\ncontext. While these are expected to be novel for\nhumans, they may appear in pre-training corpora\non which PLMs are usually trained. 6 This raises\na potential concern that PLMs could already be\nbiased toward certain properties for these words\n(e.g., wug is commonly depicted as a bird), and\n6e.g., wug appears in wikipedia: https://en.wikipedia.\norg/wiki/Jean_Berko_Gleason (accessed on Jan 23)\n2935\nmay struggle to associate them with different prop-\nerties.7 To explore this empirically, we conducted\nexperiments with alternative nonce words (gener-\nated synthetically, similar to Kim et al. (2022); see\nAppendix C.2). Figure 7 (Appendix C.2) shows\nresults on COMPS -WUGS and COMPS -WUGS -DIST\nwith randomly sampled nonce words. We see that\nthe new results are comparable to those in Figure 4,\nwith models showing the same preference on both\nstimulus versions 80% of the time on average. This\nsuggests that the choice of nonce words is not pro-\nducing any noteworthy bias.\nFraming of novel taxonomic information An-\nother relevant stimulus design decision is the phras-\ning for introducing novel concepts in context.\nWhile we used “A wug is a [CONCEPT]” for our\nmain experiments, we additionally tested with an\nalternate framing: “A wug is a type of [CONCEPT]. ”\nFrom Figure 9 (Appendix C.3), we again see that\nthe overall patterns of results are comparable to\nthe original results, with models showing the same\npreference across both versions of the stimuli on\nCOMPS -WUGS and COMPS -WUGS -DIST 90% of\nthe time, on average.\n5 General Discussion and Conclusion\nThe overall goal ofCOMPS is to shed light on the ex-\ntent to which PLMs can robustly (1) attribute to real\nworld concepts (e.g., HORSE , WHALE ) their correct\nproperties (e.g., is a mammal); and (2) demonstrate\nbehavior consistent with property inheritance: a\nreasoning process in which concepts are endowed\nwith the properties of their superordinates (Smith\nand Estes, 1978; Sloman, 1998; Murphy, 2002).\nTesting PLMs for these abilities allows us to ask\nkey questions about how they encode and trans-\nfer knowledge. To target these capabilities more\nprecisely, and mitigate potential inflation of per-\nformance by superficial heuristics such as coarse-\ngrained similarity and word association, we pro-\npose incrementally increasing levels of controls\nin constructing our minimal pair stimuli, progres-\nsively making the task of attributing properties to\nconcepts more challenging.\nFindings from our initial experiment on COMPS -\nBASE established that the basic capacity of mod-\nels to attribute properties to everyday concepts is\nlargely coarse grained. PLMs were more success-\nful in making correct property attributions when\n7We thank Reviewers 1 and 3, Najoung Kim and Kyle\nMahowald for raising this concern.\nthe candidate concepts were radically different,\nand struggled when the concepts shared seman-\ntic relations or had high co-occurrence. On testing\nfor ‘property inheritance’ behavior (via COMPS -\nWUGS ), PLMs initially appeared to demonstrate\nreasonable success, but they also showed spurious\nbehavior in achieving correct performance on a\nnon-trivial number of instances for which they did\nnot succeed in the prerequisite base condition. Fur-\nthermore, this performance declined substantially\nin the presence of distracting information (COMPS -\nWUGS -DIST ), providing further evidence that what\nproperty knowledge and reasoning we appear to see\nin these PLMs is more reliant on superficial heuris-\ntics than on ideal reasoning behavior. Of particular\nnote is our finding of catastrophic distraction in\nlarge autoregressive PLMs, whose sensitivity to\nproximity effects brings their overall performance\nwell under chance, especially when scaled up to\nbillions of parameters.\nContemporary work has highlighted the promise\nof PLMs on high-level tasks requiring—among\nother things—access to proper relational knowl-\nedge between concepts (see Petroni et al., 2019;\nSafavi and Koutra, 2021; Piantadosi and Hill,\n2022). By drawing on the concept of property\ninheritance, our experiments target reasoning abil-\nity based on perhaps the most well-established of\nrelations—the taxonomic or the IsArelation (Mur-\nphy, 2003). Recent work has also alluded to the\nproficiency of PLMs in capturing taxonomic infor-\nmation about everyday objects and entities (Weir\net al., 2020; Chen et al., 2021, though see Ravichan-\nder et al. (2020)). Findings from our controlled ex-\nperiments suggest that PLMs’ approximation of the\nconsequences of the taxonomic relation is at best\nnoisy, in light of clear failures especially in pres-\nence of similarity-governed competition. We con-\nclude from our analyses that instead of robustly ex-\ntracting relational information and reasoning about\nproperties of concepts, it is likely that the PLMs\ntested here are optimized to prefer superficial cues\nin making word predictions, leading to mistakes\nand inaccuracies in presence of irrelevant and dis-\ntracting information. Since robust natural language\nunderstanding will be critically reliant on under-\nstanding of property knowledge and implications\nof property transfer, we hope that these findings\nwill motivate adoption of rigorous assessment meth-\nods as well as work toward more robust property\nknowledge and reasoning in PLMs.\n2936\nLimitations\nZero-shot setup Using a zero-shot setup to test\nPLMs for human-like capacities such as prop-\nerty inheritance (as we have done in this work)\nhas recently come under scrutiny. In particular,\nLampinen (2022) argues that such a setup could\nbe problematic because PLMs are trained to imi-\ntate the language produced by countless individuals\nwith different beliefs, cultures, and behaviors. As a\nresult, PLMs are likely to be handicapped in assign-\ning sufficient probability mass to the desired fam-\nily of continuations, given minimal prompts with-\nout any particular task-specific context. Instead,\nLampinen (2022) suggests the need for PLMs\n“[...] to be guided into an experiment-\nappropriate behavioral context, analo-\ngously to the way cognitive researchers\nplace humans in an experimental context,\nand orient them toward the task with in-\nstructions and examples.”\nThis criticism is valid, and it is possible that models\ncould overcome their lack of robustness to distrac-\ntion effects by observing examples of our stimuli\nin context, though this has largely been shown in\nPLMs that are significantly larger than the ones\nwe have tested in this work (Brown et al., 2020;\nChowdhery et al., 2022; Wei et al., 2022a).8 Indeed,\nrecent work has demonstrated these larger PLMs\nto achieve strong performance on other types of\nreasoning—such as those required for solving math\nproblems, reversing sequences, etc.—by priming\nmodels to produce additional textual content that\nrepresents intermediate reasoning steps and ex-\nplanations (Nye et al., 2021; Wei et al., 2022b;\nLampinen et al., 2022), in a few-shot setting.9 At\nthe same time, a few-shot version ofCOMPS stimuli\ncould expose models to the possibility of leveraging\nheuristics that are naturally absent in the zero-shot\nsetup, and therefore such a setup would critically\nrequire the design of additional controls, which we\nleave for future work.\n8though see recent work by Shi et al. (2023), who show\ndistraction effects in such large PLMs in solving arith-\nmetic reasoning problems, even after using sophisticated in-\ncontext prompting methods such as Chain-of-Thought (Wei\net al., 2022b), Least-to-Most (Zhou et al., 2022), and Self-\nConsistency (Wang et al., 2022).\n9See also Sinha et al. (2022), who analyze PLMs com-\nparable in size to those studied in this work in a few-shot\nminimal-pair setting.\nIdeal reasoning behavior Another limitation of\nour work is that it takes ideal and robust property in-\nheritance behavior as the monolithic gold-standard\nfor human cognition, something that recent work\nhas cautioned against (Pavlick and Kwiatkowski,\n2019; Dasgupta et al., 2022; Webson et al., 2023).\nAlthough we relied on a database of concept-\nproperty pairs that were largely generated by hu-\nman participants, whether or not humans will be\nrobust to the types of distraction that were observed\nin PLMs is an open question and requires further\ninvestigation. However, notably we are not mak-\ning direct comparisons between models and hu-\nmans here—we argue that our primary contribu-\ntion of controlled stimuli that tease apart shallow\nprocessing from robust conceptual reasoning in\nPLMs bears substantial merit that is independent\nfrom any comparisons between humans and com-\nputational systems. Furthermore, we emphasize\nthat we are setting a reasonable—and to a certain\nextent, human-independent—desideratum in this\nwork, which is that models should robustly capture\nground-truth knowledge about everyday concepts\nand their properties and reflect this knowledge in\ntheir inferences about newly introduced concepts.\nBehavioral evaluation This work tests and anal-\nyses PLMs on property knowledge and property in-\nheritance only from a behavioral perspective, which\nat its core is a correlational endeavor. Potential fu-\nture work could complement our results by provid-\ning evidence from representational analyses, or by\ndevising causal interventions, similar to those re-\ncently explored in the realm of syntactic agreement\n(Finlayson et al., 2021), or in testing of negation\nand hypernymy in NLI models (Geiger et al., 2020),\namong others. Importantly, this would require the\ndevelopment of new methods that shed light on\nhow new information—such as the ones we use in\nCOMPS -WUGS and COMPS -WUGS -DIST —is inte-\ngrated into the model (see Misra et al. (2022) for an\nexample of such an analysis for novel properties).\nTargeted language Finally, COMPS only consists\nof sentences in English, thereby biasing our results\nonly for PLMs trained in that language.\nAcknowledgments\nFor helpful comments we thank Najoung Kim, Tal\nLinzen, Brenden Lake, Kyle Mahowald, Andrew\nLampinen, the anonymous reviewers, and audi-\nences at the Computation and Psycholinguistics\n2937\nLab (NYU), the Human and Machine Learning\nLab (NYU), the UChicago CompLing Lab, UT\nAustin Linguistics Grad Student Seminar , and the\nMIT Department of Brain and Cognitive Sciences.\nAny errors are our own. Our experiments were\nconducted with resources provided by the Rosen\nCenter for Advanced Computing at Purdue Univer-\nsity (McCartney et al., 2014). We are also grateful\nto Sam Huang for helping out with experiments\nconducted on GPT-3/3.5 models.\nReferences\nMostafa Abdou, Artur Kulmizev, Daniel Hershcovich,\nStella Frank, Ellie Pavlick, and Anders Søgaard.\n2021. Can language models encode perceptual struc-\nture without grounding? a case study in color. In\nProceedings of the 25th Conference on Computa-\ntional Natural Language Learning, pages 109–132,\nOnline. Association for Computational Linguistics.\nYonatan Belinkov. 2022. Probing classifiers: Promises,\nshortcomings, and advances. Computational Linguis-\ntics, 48(1):207–219.\nJean Berko. 1958. The child’s learning of english mor-\nphology. Word, 14(2-3):150–177.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nJamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao.\n2009. Clueweb09 data set.\nCatherine Chen, Kevin Lin, and Dan Klein. 2021. Con-\nstructing taxonomies from pretrained language mod-\nels. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 4687–4700, Online. Association for Com-\nputational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining Text Encoders as Discriminators Rather\nThan Generators. In International Conference on\nLearning Representations.\nJeff Da and Jungo Kasai. 2019. Cracking the contex-\ntual commonsense code: Understanding common-\nsense reasoning aptitude of deep contextual repre-\nsentations. In Proceedings of the First Workshop on\nCommonsense Inference in Natural Language Pro-\ncessing, pages 1–12, Hong Kong, China. Association\nfor Computational Linguistics.\nIshita Dasgupta, Andrew K Lampinen, Stephanie CY\nChan, Antonia Creswell, Dharshan Kumaran,\nJames L McClelland, and Felix Hill. 2022. Lan-\nguage models show human-like content effects on\nreasoning. arXiv preprint arXiv:2207.07051.\nSteven Derby, Paul Miller, and Barry Devereux. 2021.\nRepresentation and pre-activation of lexical-semantic\nknowledge in neural language models. In Proceed-\nings of the Workshop on Cognitive Modeling and\nComputational Linguistics, pages 211–221, Online.\nAssociation for Computational Linguistics.\nBarry J Devereux, Lorraine K Tyler, Jeroen Geertzen,\nand Billi Randall. 2014. The Centre for Speech,\nLanguage and the Brain (CSLB) concept property\nnorms. Behavior Research Methods , 46(4):1119–\n1127.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nMatthew Finlayson, Aaron Mueller, Sebastian\nGehrmann, Stuart Shieber, Tal Linzen, and Yonatan\nBelinkov. 2021. Causal analysis of syntactic\nagreement mechanisms in neural language models.\nIn Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics\nand the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long\nPapers), pages 1828–1843, Online. Association for\nComputational Linguistics.\nMaxwell Forbes, Ari Holtzman, and Yejin Choi. 2019.\nDo Neural Language Representations Learn Physical\nCommonsense? In Proceedings of the 41st Annual\nMeeting of the Cognitive Science Society.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic subjects:\nRepresentations of syntactic state. In Proceedings of\nNAACL-HLT 2019, pages 32–42, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\n2938\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nAtticus Geiger, Kyle Richardson, and Christopher Potts.\n2020. Neural natural language inference models\npartially embed theories of lexical entailment and\nnegation. In Proceedings of the Third BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural Net-\nworks for NLP, pages 163–173, Online. Association\nfor Computational Linguistics.\nAaron Gokaslan and Vanya Cohen. 2019. Open-\nwebtext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nAlison Gopnik and David M Sobel. 2000. Detecting\nblickets: How young children use information about\nnovel causal powers in categorization and induction.\nChild development, 71(5):1205–1222.\nJonathan Gordon and Benjamin Van Durme. 2013. Re-\nporting bias and knowledge acquisition. In Proceed-\nings of the 2013 workshop on Automated knowledge\nbase construction, pages 25–30.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.\n2003. English gigaword. Linguistic Data Consor-\ntium, Philadelphia, 4(1):34.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2733–2743.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form com-\npetition: Why the highest probability answer isn’t\nalways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038–7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language models\nfor question answering. Transactions of the Associa-\ntion for Computational Linguistics, 9:962–977.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. Asso-\nciation for Computational Linguistics.\nNajoung Kim, Tal Linzen, and Paul Smolensky. 2022.\nUncontrolled lexical exposure leads to overestima-\ntion of compositional generalization in pretrained\nmodels. arXiv preprint arXiv:2212.10769.\nBrenden M Lake and Gregory L Murphy. 2021. Word\nmeaning in minds and machines. Psychological Re-\nview.\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY\nChan, Kory Matthewson, Michael Henry Tessler,\nAntonia Creswell, James L McClelland, Jane X\nWang, and Felix Hill. 2022. Can language models\nlearn from explanations in context? arXiv preprint\narXiv:2204.02329.\nAndrew Kyle Lampinen. 2022. Can language models\nhandle recursively nested grammatical structures? a\ncase study on comparing models and humans. arXiv\npreprint arXiv:2210.15303.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In Interna-\ntional Conference on Learning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nLi Lucy and Jon Gauthier. 2017. Are distributional\nrepresentations ready for the real world? evaluat-\ning word vectors for grounded perceptual meaning.\nIn Proceedings of the First Workshop on Language\nGrounding for Robotics , pages 76–85, Vancouver,\nCanada. Association for Computational Linguistics.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceedings\nof the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 1192–1202.\nGerry McCartney, Thomas Hacker, and Baijian Yang.\n2014. Empowering Faculty: A Campus Cyberinfras-\ntructure Strategy for Research Communities. Edu-\ncause Review.\nGeorge A Miller. 1995. WordNet: a lexical database\nfor English. Communications of the ACM, 38(11):39–\n41.\nKanishka Misra. 2022. minicons: Enabling flexible be-\nhavioral and representational analyses of transformer\nlanguage models. arXiv preprint arXiv:2203.13112.\nKanishka Misra, Allyson Ettinger, and Julia Rayz. 2020.\nExploring BERT’s sensitivity to lexical cues using\ntests from semantic priming. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 4625–4635, Online. Association for Computa-\ntional Linguistics.\nKanishka Misra, Allyson Ettinger, and Julia Rayz. 2021.\nDo language models learn typicality judgments from\ntext? In Proceedings of the 43rd Annual Conference\nof the Cognitive Science Society.\n2939\nKanishka Misra, Julia Taylor Rayz, and Allyson Et-\ntinger. 2022. A property induction framework for\nneural language models. In Proceedings of the 44th\nAnnual Conference of the Cognitive Science Society.\nGregory L Murphy. 2002. The Big Book of Concepts.\nMIT press.\nM Lynne Murphy. 2003. Semantic relations and the\nlexicon: Antonymy, synonymy and other paradigms.\nCambridge University Press.\nSebastian Nagel. 2016. CC-News.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nLalchand Pandia and Allyson Ettinger. 2021. Sorting\nthrough the noise: Testing robustness of information\nprocessing in pre-trained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1583–\n1596, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nRoma Patel and Ellie Pavlick. 2022. Mapping language\nmodels to grounded conceptual spaces. In Interna-\ntional Conference on Learning Representations.\nEllie Pavlick and Tom Kwiatkowski. 2019. Inherent\ndisagreements in human textual inferences. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:677–694.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of EMNLP 2014, pages\n1532–1543.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473.\nSteven T Piantadosi and Felix Hill. 2022. Meaning\nwithout reference in large language models. arXiv\npreprint arXiv:2208.02957.\nM Ross Quillian. 1967. Word concepts: A theory and\nsimulation of some basic semantic capabilities. Be-\nhavioral science, 12(5):410–430.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI.\nAbhilasha Ravichander, Yonatan Belinkov, and Eduard\nHovy. 2021. Probing the probing paradigm: Does\nprobing accuracy entail task relevance? In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 3363–3377, Online. Association\nfor Computational Linguistics.\nAbhilasha Ravichander, Eduard Hovy, Kaheer Suleman,\nAdam Trischler, and Jackie Chi Kit Cheung. 2020.\nOn the systematicity of probing contextualized word\nrepresentations: The case of hypernymy in BERT. In\nProceedings of the Ninth Joint Conference on Lex-\nical and Computational Semantics , pages 88–102,\nBarcelona, Spain (Online). Association for Computa-\ntional Linguistics.\nLance J Rips, Edward E Smith, and Douglas L Medin.\n2012. Concepts and categories: Memory, meaning,\nand metaphysics. Oxford University Press.\nTimothy T Rogers and James L McClelland. 2004. Se-\nmantic cognition: A parallel distributed processing\napproach. MIT press.\nDana Rubinstein, Effi Levi, Roy Schwartz, and Ari Rap-\npoport. 2015. How well do distributional models\ncapture different types of semantic knowledge? In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers) , pages 726–\n730, Beijing, China. Association for Computational\nLinguistics.\nTara Safavi and Danai Koutra. 2021. Relational World\nKnowledge Representation in Contextual Language\nModels: A Review. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1053–1067, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699–2712, Online. Association for Computational\nLinguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nThibault Sellam, Steve Yadlowsky, Jason Wei, Naomi\nSaphra, Alexander D’Amour, Tal Linzen, Jasmijn\nBastings, Iulia Turc, Jacob Eisenstein, Dipanjan Das,\net al. 2021. The multiberts: Bert reproductions for ro-\nbustness analysis. arXiv preprint arXiv:2106.16163.\n2940\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Chi, Nathanael Schärli, and\nDenny Zhou. 2023. Large language models can be\neasily distracted by irrelevant context. arXiv preprint\narXiv:2302.00093.\nVered Shwartz and Yejin Choi. 2020. Do neural lan-\nguage models overcome reporting bias? In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 6863–6870, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nKoustuv Sinha, Jon Gauthier, Aaron Mueller, Kan-\nishka Misra, Keren Fuentes, Roger Levy, and Ad-\nina Williams. 2022. Language model acceptability\njudgements are not always robust to context. arXiv\npreprint arXiv:2212.08979.\nSteven A Sloman. 1993. Feature-based induction. Cog-\nnitive psychology, 25(2):231–280.\nSteven A Sloman. 1998. Categorical inference is not a\ntree: The myth of inheritance hierarchies. Cognitive\nPsychology, 35(1):1–33.\nEdward E Smith and William K Estes. 1978. Theories\nof semantic memory. Handbook of learning and\ncognitive processes, 6:1–56.\nPia Sommerauer and Antske Fokkens. 2018. Firearms\nand tigers are dangerous, kitchen knives and zebras\nare not: Testing whether word embeddings can tell.\nIn Proceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 276–286, Brussels, Belgium.\nAssociation for Computational Linguistics.\nPia Johanna Maria Sommerauer. 2022. Diagnosing\nsemantic properties in distributional representations\nof word meaning.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge. Advances in Neural\nInformation Processing Systems, 33:20227–20237.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nAmos Tversky. 1977. Features of similarity. Psycholog-\nical review, 84(4):327.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nAlbert Webson, Alyssa Marie Loo, Qinan Yu, and Ellie\nPavlick. 2023. Are language models worse than hu-\nmans at following prompts? it’s complicated. arXiv\npreprint arXiv:2301.07085.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nNathaniel Weir, Adam Poliak, and Benjamin\nVan Durme. 2020. Probing neural language models\nfor human tacit assumptions. In CogSci 2020, pages\n377–383. Cognitive Science Society.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of EMNLP 2020: Demos , pages 38–45,\nOnline. Association for Computational Linguistics.\nZhibiao Wu and Martha Palmer. 1994. Verb Semantics\nand Lexical Selection. In 32nd Annual Meeting of\nthe Association for Computational Linguistics, pages\n133–138, Las Cruces, New Mexico, USA. Associa-\ntion for Computational Linguistics.\nYian Zhang, Alex Warstadt, Xiaocheng Li, and\nSamuel R. Bowman. 2021. When do you need bil-\nlions of words of pretraining data? In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1112–1125, Online.\nAssociation for Computational Linguistics.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\n2941\nand reading books. In 2015 IEEE International Con-\nference on Computer Vision (ICCV) , pages 19–27.\nIEEE.\nA Model Metadata\nTable 3 shows the different models used in our\nexperiments, along with their abbreviation, tok-\nenization scheme, total parameters, vocabulary size,\nnumber of tokens encountered during training, and\ncorpora on which they are pre-trained. All mod-\nels were accessed using minicons(Misra, 2022),10\na python library that serves as a wrapper around\nHuggingface’s transformers(Wolf et al., 2020),\nand provides a unified mechanism for eliciting log-\nprobabilities in batch-wise manner for any autore-\ngressive or masked LM that is accessible through\nthe huggingface hub, or is trained using the trans-\nformers library. Experiments were performed using\nan NVIDIA V100 GPU (32 GB RAM) and took\nabout 6 hours to run, discounting the time it took to\ndownload the models from the Huggingface Hub.11\nB Preview of COMPS stimuli\nWe show examples of stimuli from our COMPS -\nBASE , COMPS -WUGS , and COMPS -WUGS -DIST\ndatasets in Listing 1 and Listing 2, respectively.\nStimuli with distraction—i.e., in COMPS -WUGS -\nDIST —are similar to that in Listing 1, but with the\ndistraction_type value set to either ‘before’\nor ‘in-between’.\n{\n\"id\": 12706,\n\"property\": \"can fly\",\n\"acceptable_concept\": \"owl\",\n\"unacceptable_concept\": \"squirrel\",\n\"prefix_acceptable\": \"an owl\",\n\"property_phrase\": \"can fly.\",\n\"prefix_unacceptable\": \"a squirrel\",\n\"condition\": \"co-occurrence\",\n\"similarity\": 0.62\n}\nListing 1: An instance of COMPS -BASE . “condition”\nrepresents the negative sampling scheme, and\n“similarity” represents the similarity between the\nacceptable concept and the unacceptable concept on\nthe basis of the condition (either Taxonomic, Property\nNorm, Co-occurence, or Random).\n10https://github.com/kanishkamisra/minicons\n11https://huggingface.co/models.\n{\n\"item\": 8343,\n\"comps_id\": 28798,\n\"property\": \"has hooves\",\n\"acceptable_concept\": \"horse\",\n\"unacceptable_concept\": \"dog\",\n\"prefix_acceptable\": \"A dax is a\nhorse. Therefore, a dax\",↪→\n\"prefix_unacceptable\": \"A dax is a\ndog. Therefore, a dax\",↪→\n\"property_phrase\": \"has hooves.\",\n\"negative_sample_type\":\n\"co-occurrence\",↪→\n\"similarity\": 0.62,\n\"distraction_type\": \"undistracted\"\n}\nListing 2: An instance of COMPS -WUGS . “condition”\nand “similarity” are the same as in Listing 1.\n“distraction_type” denotes the type of distraction\nused (undistracted, before, in-between).\nTable 4 shows examples from each subset of\nCOMPS , and the conditional log-probability scores\nas computed by GPT-J (Wang and Komatsuzaki,\n2021), the largest LM tested on the full set of stim-\nuli.\nC Additional findings and analyses\nC.1 Testing GPT-3/3.5\nRecent work in scaling PLMs to hundred billion pa-\nrameters has led to models such as GPT-3 (Brown\net al., 2020), which are significantly larger than the\nlargest model tested in the results discussed above\n(i.e., GPT-J, with 6B parameters). Testing them\non the entire set of COMPS stimuli (49K + 3 ×\n13.8K pairs of sentences) is prohibitively expensive\nsince they are only accessible through paid APIs.\nNonetheless, we sampled a small set of COMPS\nstimuli—which we term as miniCOMPS —in order\nto get a glimpse of how well substantially larger\nPLMs elicit property knowledge and demonstrate\nreasoning behavior compatible with property in-\nheritance. Specifically, we created mini COMPS\nby sampling 1200 minimal pairs from each of our\noriginal COMPS subsets (matched in terms of real\nworld concepts and properties across the subsets),\nsuch that all pairs of nonce words in the resulting\nminiCOMPS -WUGS -DIST end up being sampled\nequal number of times (100 times each).\n2942\nFamily Model (Abbrev.) Parameters Vocab Size Tokenization Corpora Tokens\nALBERT\nalbert-base-v2(A-b) 11M\n30,000 SentencePiece W IKIandBC 3.3Balbert-large-v2(A-l) 17M\nalbert-xlarge-v2(A-xl) 59M\nalbert-xxlarge-v2(A-xxl) 206M\nBERT\ndistilbertbase-uncased(dB-b) 67M\n30,522 WordPiece W IKIandBC 3.3Bbert-base-uncased(B-b) 110M\nbert-large-uncased(B-l) 345M\nELECTRA\nelectra-small(E-s) 13M\n30,522 WordPiece WIKIandBC 3.3Belectra-base(E-b) 34M\nelectra-large(E-l) 51M WIKI, BC, CW,\nCC, and GIGA 33B\nRoBERTa\ndistilroberta-base(dR-b) 82M\n50,265 Byte-pair encoding\nOWTC 2B\nroberta-base(R-b) 124M BC, CC-NEWS,\nOWTC, and STORIES 30Broberta-large(R-l) 355M\nGPT2\ndistilgpt2(dGPT2) 82M 50,257\nByte-pair encoding\nOWTC 2B\ngpt2(GPT2) 124M\n50,257 W EBTEXT 8B∗gpt2-medium(GPT2-m) 355M\ngpt2-large(GPT2-l) 774M\ngpt2-xl(GPT2-xl) 1.5B\nEleutherAI\ngpt-neo-125M(Neo-125M) 125M\n50,257 Byte-pair encodingPILE\n300B\ngpt-neo-1.3B(Neo-1.3B) 1.3B 380B\ngpt-neo-2.7B(Neo-2.7B) 2.7B 420B\ngpt-j-6B(GPT-J) 6B 402B\nTable 3: Summary of the 22 models that we evaluate in this paper. Legend for Corpora: WIKI : Wikipedia; BC:\nBookCorpus (Zhu et al., 2015); CW: ClueWeb (Callan et al., 2009); CC: CommonCrawl GIGA : Gigaword (Graff\net al., 2003); OWTC : OpenWebTextCorpus (Gokaslan and Cohen, 2019); CC-NEWS : CommonCrawl News (Nagel,\n2016); STORIES : Stories corpus (Trinh and Le, 2018); WEBTEXT : WebText corpus (Radford et al., 2019); PILE :\nThe Pile (Gao et al., 2020).\n∗As estimated by Warstadt et al. (2020).\nCOMPS subset Stimulus Score\nBASE\nA horse has hooves. -3.829\nA dog has hooves. -4.963\nWUGS\nA fep is a horse. Therefore, a fep has hooves. -2.153\nA fep is a dog. Therefore, a fep has hooves. -3.392\nWUGS -DIST (before) A wug is a dog. A fep is a horse. Therefore, a fep has hooves. -2.919\nA wug is a dog. A fep is a horse. Therefore, a wug has hooves. -2.895\nWUGS -DIST (in-between) A fep is a horse. A wug is a dog. Therefore, a fep has hooves. -3.616\nA fep is a horse. A wug is a dog. Therefore, a wug has hooves. -3.092\nTable 4: An example of matched stimuli across different COMPS subsets, as well as conditional log-probabilities\nelicited by GPT-J. Here, the property of interest is has hooves, the positive concept is HORSE , and the negative\nconcept is DOG . The negative concept in this case was sampled using the co-occurrence knowledge representation\nmethod (see §2.3). Emboldened words indicate items that are different in the minimal pair. Refer to §3.2 for\ndiscussion on how ‘Score’ is computed.\nModels As test subjects, we chose four GPT-\n3 models (Brown et al., 2020): ada, babbage,\ncurie, davinci, with the last one being the\nlargest (at 175B parameters), and an additional fifth\ndavinci-based model called text-davinci-001,\nwhich fine-tunes davinci on human-written\ndemonstrations. We also test the recently pro-\nposed GPT-3.5 models, text-davinci-002 and\ntext-davinci-003, which improve over davinci\nby additionally fine-tuning it on code and human-\n2943\n0%\n25%\n50%\n75%\n100%\nada babbage curie davinci\nAccuracy\nWUGS WUGS-DIST (before) WUGS-DIST ( in-between)\nFigure 5: Accuracies of GPT-3 models (arranged in\nincreasing order of the number of trained parameters)\non mini COMPS -WUGS and mini COMPS -WUGS -DIST .\nBlack dashed line indicates chance performance (50%).\nError bands indicate 95% Bootstrap CIs.\nwritten demonstrations (Ouyang et al., 2022). 12\nAll these models are autoregressive in nature, so\nwe use the same scoring and evaluation method\nas described in §3.2. Since the four original\nGPT-3 models ( ada, babbage, curie, davinci)\nare trained using the same LM objective on\nthe same corpora, we analyze them separately\nfrom text-davinci-001, text-davinci-002,\nand text-davinci-003, which we only compare\nto davinci. We do this to remain consistent with\nthe way we displayed results in §4—ordering mod-\nels based on their number of trained parameters—\nand also because models in thetext-davinci-XXX\nseries use the same underlyingdavincimodel aug-\nmented with additional training mechanisms (e.g.,\nreinforcement learning and fine-tuning on human-\nfeedback) and data (e.g., code) instead of increas-\ning its size, to our knowledge.\nResults Figure 5 shows the performance of the\nfour GPT-3 models on mini COMPS -WUGS and\nminiCOMPS -WUGS -DIST , while Figure 6 compares\nGPT-3 davinci to its code and human-feedback\nadapted counterparts. From Figure 5, we see ro-\nbustness issues to persist even for GPT-3 models,\nsimilar to our main results. Models perform re-\nmarkably well in the absence of distraction (i.e., on\nminiCOMPS -WUGS ), but struggle in its presence,\nespecially when it is closer to the queried prop-\n12These models are also known as InstructGPT, as\ndiscussed in https://platform.openai.com/docs/\nmodel-index-for-researchers.\n0%\n25%\n50%\n75%\n100%\ndavinci\ntext\ndavinci\n001\ntext\ndavinci\n002\ntext\ndavinci\n003\nAccuracy\nWUGS WUGS-DIST (before) WUGS-DIST ( in-between)\nFigure 6: Accuracies of davinci models (GPT-3\nand GPT-3.5) on miniCOMPS -WUGS and miniCOMPS -\nWUGS -DIST . Black dashed line indicates chance perfor-\nmance (50%). Error bars indicate 95% Bootstrap CIs.\ndavinci and text-davinci-001 are GPT-3 (Brown\net al., 2020) models, while text-davinci-002 and\ntext-davinci-003 are GPT-3.5 models.\nerty. In particular, performance on mini COMPS -\nWUGS -DIST (before) increases with an increase\nin parameters until the largest model ( davinci),\nwhere the performance drops closer to chance. On\nminiCOMPS -WUGS -DIST (in-between), all models\nperform catastrophically worse than chance. This\nnoteworthy pattern of proximity-based degradation\nin performance mimics the results shown in Fig-\nure 4, though we do not see a systematic decline in\nperformance with an increase in parameters as ob-\nserved in the GPT2 and EleutherAI models—with\nthe 175B parameter model (davinci) demonstrat-\ning an increase in performance over the relatively\nsmaller curiemodel.\nWhile the above results demonstrate that sim-\nply scaling autoregressive PLMs is unlikely to\novercome the lack of robustness against distract-\ning content, we now test whether augmenting\nthese large PLMs by additionally training on code\n(GPT-3.5 models) and aligning them with human-\nprovided demonstrations (text-davinci-001 and\nboth GPT-3.5 models) could lead to any improve-\nments. For instance, training on code could pro-\nvide training signals to PLMs that encourage en-\ntity tracking, which could potentially enable them,\nin our case, to resolve which subordinate concept\n(e.g., wug vs. dax) the target property is more likely\nto be associated with. Similarly, aligning with\nhuman-written demonstrations could potentially\nimprove their truthfulness, which in our case, could\nlead to them to prefer correct property assignments.\nHowever, from Figure 6, we see no noteworthy\nimprovements demonstrated by these augmented\nmodels. All augmented models achieved similar ac-\n2944\ncuracies on COMPS -WUGS as the davinci model\n(within 90.5% and 91%), suggesting that their aug-\nmentations preserved the general associations be-\ntween the lexical items that denote everyday con-\ncepts and properties. On stimuli containing dis-\ntraction (i.e., both subsets of COMPS -WUGS -DIST ),\neither the models performed systematically worse\nas compared todavinci(with text-davinci-002\nshowing below-chance performance on both sub-\nsets), or they showed mixed results, where an im-\nprovement on COMPS -WUGS -DIST (before) was\naccompanied by a decline on COMPS -WUGS -DIST\n(in-between).\nTogether, these results suggest that neither an in-\ncrease in scale nor additional training methods such\nas alignment with human instructions/feedback or\ntraining on code prevents models from being dis-\ntracted in associating properties to novel subordi-\nnate concepts introduced in the input context. In\nfact, the catastrophic effects of proximity-based\ndistraction persists even for the most recent state\nof the art GPT-3/3.5 models.\nC.2 Results with alternate nonce words\nHere we report results on COMPS -WUGS and\nCOMPS -WUGS -DIST using an alternate set of nonce\nwords, which we constructed by sampling (with\nreplacement) from 26 lower-case ASCII alphabet\ncharacters. Specifically, we constructed novel char-\nacter sequences—each assigned as a replacement\nfor our original four nonce words—of lengths rang-\ning from 4-8 by sampling in an alternate fashion\nfrom consonants (odd positions) and vowels (even\npositions).13 A replication of Figure 4 using the\nstimuli with these newly sampled nonce words is\nshown in Figure 7. On comparing figures 4 and\n7, we observe largely similar patterns of results\non stimuli containing nonce words constructed us-\ning randomly sampled characters. That is, models\ngenerally performed well on COMPS -WUGS , while\nthey struggled on COMPS -WUGS -DIST . There were\nsome exceptions: (1) GPT-Neo 1.3B and 2.7B\nshowed improvements (relative to the original stim-\nuli) in cases where distraction is added closer to the\nqueried property (i.e., in-between), though they\nstill hover around chance performance, and addi-\ntionally the performance of GPT-J, like in the orig-\ninal results is still substantially below chance; and\n(2) there were non-trivial improvements demon-\n13the resulting set of words is: { ruhisin, kifosa, rosibif,\nlepuvu}, still amounting to 12 unique ordered pairs in the\nCOMPS -WUGS -DIST stimuli.\nStimuli Avg. Agreement\nCOMPS-WUGS 93.10.8\nCOMPS-WUGS-DIST (before) 73.9 4.6\nCOMPS-WUGS-DIST (in-between) 73.0 4.6\nOverall 80.0 2.9\nTable 5: Average agreement ( ×100) in PLMs’ pref-\nerence on stimuli containing original and synthetically\nconstructed nonce words.\nstrated by ALBERT models (large and xl) on the\nbefore subset of COMPS -WUGS -DIST , and BERT-\nlarge on the in-between subset of COMPS -WUGS -\nDIST .\nTo precisely quantify the difference between the\ntwo sets of results, we measured the agreement\nbetween the predictions of the PLMs for both sets\nof stimuli, taken as the proportion of minimal pairs\nin which the models’ relative preference agree.\nFigure 8 shows individual model agreement on\nCOMPS -WUGS and COMPS -WUGS -DIST , while Ta-\nble 5 shows agreement percentages averaged across\nall models. From these results we observe mod-\nels to show greater robustness to the variability\nintroduced by the choice of nonce words in stim-\nuli with one novel concept ( COMPS -WUGS ) than\nin stimuli with multiple novel concepts ( COMPS -\nWUGS -DIST ). Despite this discrepancy, there is\ngenerally a high average agreement (80%) between\na given model’s set of decisions on stimuli with\noriginal and alternative nonce words.\nC.3 Results with alternate templates\nHere we report results on an alternate phrasing of\nour stimuli, where instead of using the original\ntemplate for introducing novel concepts in context\n(a wug is a [CONCEPT]), we use: A wug is a type of\n[CONCEPT], where wug indicates the novel concept.\nIn all cases, we simply alter the template, keeping\neverything else constant, including the choice of\nnonce words.\nFigure 9 shows accuracies of the models on stim-\nuli with this alternate phrasing, while Figure 10\nand Table 6 show individual and averaged overall\nagreement between models’ preference on origi-\nnal and the alternatively-phrased stimuli, respec-\ntively. The agreement percentages between models’\npreferences are quite high (average agreement be-\ning 90%)—in fact even greater than the agreement\nobserved as a result of altering the nonce words\n(Table 5), further cementing the robustness of our\n2945\nChance Performance\n0%\n25%\n50%\n75%\n100%\nA-b A-l A-xl A-xxl dB-b B-b B-l E-s E-b E-l dR-b R-b R-l dGPT2GPT2GPT2-mGPT2-lGPT2-xlNeo-125MNeo-1.3bNeo-2.7BGPT-J\nAccuracy\nWUGS WUGS-DIST (before) WUGS-DIST ( in-between)\nFigure 7: Accuracies of individual models (grouped by family, in increasing order based on number of parameters)\non COMPS -WUGS and COMPS -WUGS -DIST with synthetically constructed nonce words . Black dashed line\nindicates chance performance (50%). Refer to Table 3 for unabbreviated model names.\n0.83\n0.81\n0.76\n0.78\n0.72\n0.72\n0.70\n0.75\n0.77\n0.76\n0.76\n0.77\n0.72\n0.74\n0.78\n0.79\n0.74\n0.71\n0.76\n0.77\n0.70\n0.70\n0.72\n0.73\n0.71\n0.71\n0.68\n0.69\n0.67\n0.67\n0.71\n0.71\n0.73\n0.77\n0.65\n0.66\n0.79\n0.81\n0.66\n0.66\n0.74\n0.74\n0.79\n0.80\n0.94 0.930.93 0.930.94 0.93 0.92 0.92 0.92 0.92 0.93 0.940.92 0.94 0.950.94 0.93 0.93 0.930.930.92 0.93\nWUGS-DIST\n(in-between)\nWUGS-DIST\n(before)\nWUGS\nA-b A-l A-xl A-xxl dB-b B-b B-l E-s E-b E-l dR-b R-b R-l dGPT2GPT2GPT2-mGPT2-lGPT2-xlNeo-125MNeo-1.3bNeo-2.7BGPT-J\n0.6 0.7 0.8 0.9 1.0Agreement\nFigure 8: Proportion of cases (in COMPS -WUGS and both subsets of COMPS -WUGS -DIST ) where each listed\nmodel’s preference on the original stimuli matches that in stimuli with synthetically constructed nonce words,\nmeasured as the ‘Agreement’. An agreement of 1.0 suggests that a given model’s preferences are perfectly matched\nacross both sets of stimuli.\nStimuli Avg. Agreement\nCOMPS-WUGS 93.41.2\nCOMPS-WUGS-DIST (before) 88.5 3.1\nCOMPS-WUGS-DIST (in-between) 88.0 3.6\nOverall 90.0 2.6\nTable 6: Average agreement ( ×100) in PLMs’ pref-\nerence on stimuli containing original ( A wug is a\n[CONCEPT].) and alternate framing of novel taxonomic\ninformation (A wug is a type of [CONCEPT].).\nresults.\nC.4 How does performance on COMPS -BASE\nvary by property type?\nDevereux et al. (2014) have categorized the proper-\nties that we use in our experiments to lie in 5 differ-\nent categories: (1) Taxonomic, e.g., is a mammal,\nis a vehicle , etc.; (2) Functional, e.g., can keep\nthe body warm, is used to hit nails, etc.; (3) Ency-\nclopedic, e.g., uses electricity, is warm blooded,\netc.; (4) Visual Perceptual, e.g., has webbed feet,\nhas thick fur, etc.; and (5) Other Perceptual, e.g.,\nmakes grunting sounds and is sharp, etc. We re-\nport results of the 22 PLMs on the COMPS -BASE\nstimuli across the five different property types, in\nFigure 11.\nFrom Figure 11, we observe that PLMs are sub-\nstantially stronger in eliciting taxonomic properties\nof concepts as compared to other types, with high-\nest overall accuracy being 70%, as compared to\n48% on encyclopedic properties, 50% on visual per-\nceptual properties, 57% on functional properties,\nand 43% on non-visual perceptual properties. Re-\ncall that chance accuracy for the ‘Overall‘ scenario\nis just 6.25%, so these scores are fairly high. This\ncorroborates evidence from previous work in ana-\nlyzing property knowledge of distributional seman-\ntic models as well as LM representations to lack\nperceptual knowledge (Lucy and Gauthier, 2017;\nDa and Kasai, 2019; Rubinstein et al., 2015; Weir\net al., 2020), likely due to reporting bias (Gordon\nand Van Durme, 2013; Shwartz and Choi, 2020).\n2946\nChance Performance\n0%\n25%\n50%\n75%\n100%\nA-b A-l A-xl A-xxl dB-b B-b B-l E-s E-b E-l dR-b R-b R-l dGPT2GPT2GPT2-mGPT2-lGPT2-xlNeo-125MNeo-1.3bNeo-2.7BGPT-J\nAccuracy\nWUGS WUGS-DIST (before) WUGS-DIST ( in-between)\nFigure 9: Accuracies of individual models (grouped by family, in increasing order based on number of parameters)\non COMPS -WUGS and COMPS -WUGS -DIST with alternate framing of novel taxonomic information. Black dashed\nline indicates chance performance (50%). Refer to Table 3 for unabbreviated model names.\n0.90\n0.90\n0.87\n0.87\n0.92\n0.92\n0.86\n0.89\n0.91\n0.91\n0.82\n0.83\n0.79\n0.79\n0.84\n0.86\n0.87\n0.87\n0.86\n0.86\n0.89\n0.89\n0.92\n0.92\n0.91\n0.91\n0.90\n0.90\n0.92\n0.92\n0.89\n0.89\n0.88\n0.89\n0.88\n0.89\n0.87\n0.89\n0.94\n0.92\n0.86\n0.87\n0.85\n0.87\n0.94 0.940.95 0.930.93 0.91 0.92 0.92 0.92 0.92 0.94 0.950.95 0.95 0.950.94 0.93 0.94 0.930.940.93 0.92\nWUGS-DIST\n(in-between)\nWUGS-DIST\n(before)\nWUGS\nA-b A-l A-xl A-xxl dB-b B-b B-l E-s E-b E-l dR-b R-b R-l dGPT2GPT2GPT2-mGPT2-lGPT2-xlNeo-125MNeo-1.3bNeo-2.7BGPT-J\n0.6 0.7 0.8 0.9 1.0Agreement\nFigure 10: Proportion of cases (in COMPS -WUGS and both subsets of COMPS -WUGS -DIST ) where each listed\nmodel’s preference on the original stimuli matches that in stimuli with alternate framing of novel taxonomic\ninformation, measured as the ‘Agreement’. An agreement of 1.0 suggests that a given model’s preferences are\nperfectly matched across both sets of stimuli.\nHowever, different to most of these works, the gap\nbetween performance on perceptual properties and\nnon-perceptual properties is small. We conjecture\nthat this could be primarily due to the extension\nof the CSLB by Misra et al. (2022), which lead to\nan increase in coverage of property knowledge for\nseveral properties. For instance, the property has\nteeth was mentioned only for 45 out of 67 potential\nconcepts, having been left out for concepts such as\nCALF ,14 BUFFALO , KANGAROO , etc. So it could be\nthe case that previous research has underestimated\nthe extent to which property knowledge is encoded\nby PLMs and other distributional semantic models\nof language.\nC.5 Does performance on COMPS -BASE\ndepend on scale?\nWe plot the accuracies of PLMs on COMPS -BASE\nper model family (in order to control for differences\nin training corpora and tokenization) in Figure 12.\n14the young one of a cow, and not the muscles in the verte-\nbrate body\nIn all families except BERT, we see that accuracy\nincreases with the model size, following standard\nscaling laws. We notice that distilBERT-base (Sanh\net al., 2019) is able to outperform even BERT-large\non stimuli with ‘Random’ negative samples, sug-\ngesting that pruning BERT might sometimes unin-\ntentionally improve the model’s ability to associate\nproperties and concepts. We do however caution\nagainst interpreting these results as robust conclu-\nsion for scaling laws on COMPS -BASE . Such an\nendeavor would require comparing performance of\nmodels across multiple checkpoints with varying\nnumber of parameters, paired with rigorous statis-\ntical inference (Sellam et al., 2021; Zhang et al.,\n2021).\n2947\n0.68\n0.65\n0.87\n0.65\n0.72\n0.67\n0.90\n0.72\n0.74\n0.71\n0.91\n0.73\n0.85\n0.75\n0.95\n0.81\n0.72\n0.67\n0.85\n0.70\n0.75\n0.69\n0.88\n0.73\n0.73\n0.65\n0.87\n0.75\n0.66\n0.60\n0.81\n0.63\n0.69\n0.65\n0.86\n0.66\n0.68\n0.64\n0.85\n0.67\n0.69\n0.68\n0.89\n0.70\n0.69\n0.67\n0.83\n0.67\n0.86\n0.82\n0.95\n0.84\n0.78\n0.74\n0.93\n0.76\n0.61\n0.61\n0.78\n0.60\n0.81\n0.77\n0.94\n0.78\n0.67\n0.63\n0.86\n0.64\n0.78\n0.69\n0.94\n0.75\n0.73\n0.70\n0.92\n0.74\n0.81\n0.75\n0.94\n0.79\n0.78\n0.70\n0.88\n0.69\n0.84\n0.79\n0.95\n0.79\n0.42 0.49 0.53 0.60 0.47 0.51 0.47 0.410.44 0.43 0.460.43 0.70 0.550.39 0.620.42 0.56 0.48 0.600.50 0.61\n0.62\n0.61\n0.75\n0.58\n0.65\n0.61\n0.80\n0.60\n0.64\n0.62\n0.81\n0.62\n0.73\n0.66\n0.85\n0.67\n0.65\n0.61\n0.78\n0.61\n0.66\n0.61\n0.79\n0.61\n0.64\n0.60\n0.79\n0.59\n0.57\n0.60\n0.73\n0.56\n0.62\n0.61\n0.77\n0.58\n0.63\n0.61\n0.78\n0.60\n0.64\n0.62\n0.80\n0.62\n0.60\n0.59\n0.76\n0.60\n0.72\n0.70\n0.88\n0.69\n0.68\n0.65\n0.84\n0.63\n0.55\n0.62\n0.71\n0.57\n0.71\n0.66\n0.85\n0.66\n0.63\n0.61\n0.79\n0.59\n0.69\n0.65\n0.86\n0.65\n0.66\n0.63\n0.85\n0.62\n0.70\n0.65\n0.87\n0.67\n0.64\n0.59\n0.78\n0.60\n0.69\n0.64\n0.85\n0.65\n0.32 0.35 0.36 0.44 0.34 0.35 0.34 0.300.32 0.33 0.360.33 0.48 0.410.31 0.450.34 0.40 0.38 0.430.34 0.40\n0.64\n0.62\n0.81\n0.64\n0.65\n0.63\n0.83\n0.64\n0.67\n0.63\n0.85\n0.65\n0.74\n0.67\n0.88\n0.69\n0.67\n0.65\n0.83\n0.66\n0.69\n0.65\n0.83\n0.65\n0.66\n0.65\n0.84\n0.65\n0.58\n0.61\n0.78\n0.61\n0.62\n0.63\n0.80\n0.63\n0.61\n0.64\n0.81\n0.63\n0.66\n0.65\n0.84\n0.67\n0.59\n0.63\n0.79\n0.64\n0.79\n0.74\n0.91\n0.75\n0.71\n0.68\n0.87\n0.69\n0.54\n0.61\n0.72\n0.58\n0.72\n0.70\n0.88\n0.70\n0.65\n0.64\n0.82\n0.63\n0.72\n0.69\n0.90\n0.70\n0.71\n0.68\n0.88\n0.68\n0.76\n0.70\n0.90\n0.71\n0.65\n0.64\n0.81\n0.63\n0.71\n0.68\n0.88\n0.70\n0.36 0.38 0.39 0.46 0.41 0.40 0.40 0.360.37 0.38 0.410.36 0.57 0.470.33 0.510.40 0.49 0.46 0.520.38 0.45\n0.58\n0.58\n0.75\n0.58\n0.64\n0.61\n0.79\n0.59\n0.65\n0.61\n0.82\n0.62\n0.71\n0.64\n0.86\n0.66\n0.64\n0.58\n0.79\n0.60\n0.66\n0.60\n0.80\n0.62\n0.63\n0.61\n0.82\n0.61\n0.57\n0.58\n0.75\n0.57\n0.58\n0.60\n0.77\n0.58\n0.59\n0.61\n0.77\n0.59\n0.63\n0.60\n0.81\n0.61\n0.56\n0.59\n0.73\n0.57\n0.75\n0.70\n0.89\n0.69\n0.67\n0.64\n0.84\n0.61\n0.53\n0.59\n0.71\n0.54\n0.70\n0.65\n0.86\n0.63\n0.62\n0.62\n0.80\n0.57\n0.67\n0.65\n0.87\n0.63\n0.65\n0.63\n0.85\n0.60\n0.70\n0.65\n0.87\n0.64\n0.63\n0.60\n0.80\n0.60\n0.67\n0.63\n0.85\n0.64\n0.31 0.34 0.36 0.41 0.33 0.35 0.35 0.310.33 0.32 0.360.30 0.50 0.400.29 0.430.34 0.41 0.37 0.430.34 0.39\n0.57\n0.55\n0.72\n0.55\n0.56\n0.56\n0.75\n0.56\n0.59\n0.62\n0.81\n0.61\n0.67\n0.66\n0.80\n0.64\n0.60\n0.57\n0.76\n0.60\n0.62\n0.59\n0.75\n0.58\n0.59\n0.59\n0.77\n0.60\n0.49\n0.55\n0.65\n0.51\n0.53\n0.53\n0.70\n0.56\n0.56\n0.58\n0.73\n0.61\n0.55\n0.58\n0.75\n0.60\n0.51\n0.57\n0.68\n0.55\n0.67\n0.68\n0.84\n0.65\n0.60\n0.62\n0.82\n0.61\n0.47\n0.54\n0.62\n0.50\n0.65\n0.63\n0.83\n0.62\n0.54\n0.54\n0.71\n0.53\n0.63\n0.62\n0.83\n0.60\n0.60\n0.59\n0.81\n0.59\n0.66\n0.62\n0.83\n0.62\n0.57\n0.56\n0.73\n0.56\n0.64\n0.61\n0.80\n0.62\n0.29 0.27 0.33 0.40 0.29 0.31 0.31 0.240.24 0.29 0.280.26 0.43 0.360.22 0.410.28 0.36 0.35 0.380.29 0.34\nOther Perceptual\nVisual Perceptual\nFunctional\nEncyclopedic\nTaxonomic\nA-b A-l A-xl A-xxl dB-b B-b B-l E-s E-b E-l dR-b R-b R-l dGPT2GPT2GPT2-mGPT2-lGPT2-xlNeo-125MNeo-1.3bNeo-2.7BGPT-J\nOverall\nRandom\nCo-oc\nProp. Norm\nTaxonomic\nOverall\nRandom\nCo-oc\nProp. Norm\nTaxonomic\nOverall\nRandom\nCo-oc\nProp. Norm\nTaxonomic\nOverall\nRandom\nCo-oc\nProp. Norm\nTaxonomic\nOverall\nRandom\nCo-oc\nProp. Norm\nTaxonomic\n0.2 0.4 0.6 0.8 1.0Accuracy\nFigure 11: COMPS -BASE performance across five property types annotated in CSLB (Devereux et al., 2014).\n2948\nALBERT BERT ELECTRA RoBERTa GPT2 Eleuther\n101 101.2101.4101.6101.8 102 102.2 101.8 102 102.2 102.4 101.2 101.3 101.4 101.5 101.6 101.7101.9 102 102.1102.2102.3102.4102.5 102 102.5 103 102.5 103 103.5\n0%\n25%\n50%\n75%\n100%\nParameters (in million)\nAccuracy\nNegative Sample Type Random Co-oc Prop. Norm Taxonomic\nFigure 12: Accuracy vs. parameters across various negative sampling strategies. Models are shaded based on\nnumber of parameters.\n2949"
}