{
  "title": "Visual Transformers: Token-based Image Representation and Processing for Computer Vision",
  "url": "https://openalex.org/W3033210410",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226110728",
      "name": "Wu, Bichen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1995172584",
      "name": "Xu, Chenfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2069820730",
      "name": "Dai Xiaoliang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287296389",
      "name": "Wan, Alvin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3003609649",
      "name": "Zhang, Peizhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2261397833",
      "name": "Yan Zhicheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742615550",
      "name": "Tomizuka, Masayoshi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223304968",
      "name": "Gonzalez, Joseph",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743083855",
      "name": "Keutzer, Kurt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2258772646",
      "name": "Vajda, Peter",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2963319519",
    "https://openalex.org/W2963844898",
    "https://openalex.org/W2279098554",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W2967733054",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2964091144",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963870605",
    "https://openalex.org/W3016232661",
    "https://openalex.org/W2963978393",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2980137827",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W2963984455",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3034098129",
    "https://openalex.org/W3109944402",
    "https://openalex.org/W2962697884",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2968557240",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W2300242332",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2963896186",
    "https://openalex.org/W2962891704",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3022469715",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W3147352887",
    "https://openalex.org/W2892220259",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2953484813",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2981413347"
  ],
  "abstract": "Computer vision has achieved remarkable success by (a) representing images as uniformly-arranged pixel arrays and (b) convolving highly-localized features. However, convolutions treat all image pixels equally regardless of importance; explicitly model all concepts across all images, regardless of content; and struggle to relate spatially-distant concepts. In this work, we challenge this paradigm by (a) representing images as semantic visual tokens and (b) running transformers to densely model token relationships. Critically, our Visual Transformer operates in a semantic token space, judiciously attending to different image parts based on context. This is in sharp contrast to pixel-space transformers that require orders-of-magnitude more compute. Using an advanced training recipe, our VTs significantly outperform their convolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to 7 points while using fewer FLOPs and parameters. For semantic segmentation on LIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points higher mIoU while reducing the FPN module's FLOPs by 6.5x.",
  "full_text": "Visual Transformers: Token-based Image Representation and Processing for\nComputer Vision\nBichen Wu1, Chenfeng Xu3, Xiaoliang Dai1, Alvin Wan3, Peizhao Zhang1\nZhicheng Yan2, Masayoshi, Tomizuka3, Joseph Gonzalez3, Kurt Keutzer3, Peter Vajda1\n1 Facebook Reality Labs, 2 Facebook AI, 3 UC Berkeley\n{wbc, xiaoliangdai, stzpz, zyan3, vajdap}@fb.com\n{xuchenfeng, alvinwan, tomizuka, jegonzal, keutzer}@berkeley.edu\nAbstract\nComputer vision has achieved remarkable success by\n(a) representing images as uniformly-arranged pixel arrays\nand (b) convolving highly-localized features. However, con-\nvolutions treat all image pixels equally regardless of impor-\ntance; explicitly model all concepts across all images, re-\ngardless of content; and struggle to relate spatially-distant\nconcepts. In this work, we challenge this paradigm by\n(a) representing images as semantic visual tokens and (b)\nrunning transformers to densely model token relationships.\nCritically, our Visual Transformeroperates in a semantic\ntoken space, judiciously attending to different image parts\nbased on context. This is in sharp contrast to pixel-space\ntransformers that require orders-of-magnitude more com-\npute. Using an advanced training recipe, our VTs signiﬁ-\ncantly outperform their convolutional counterparts, raising\nResNet accuracy on ImageNet top-1 by4.6 to 7 pointswhile\nusing fewer FLOPs and parameters. For semantic segmen-\ntation on LIP and COCO-stuff, VT-based feature pyramid\nnetworks (FPN) achieve 0.35 points higher mIoU while re-\nducing the FPN module’s FLOPs by6.5x.\n1. Introduction\nIn computer vision, visual information is captured as ar-\nrays of pixels. These pixel arrays are then processed by\nconvolutions, the de facto deep learning operator for com-\nputer vision. Although this convention has produced highly\nsuccessful vision models, there are critical challenges:\n1) Not all pixels are created equal: Image classiﬁcation\nmodels should prioritize foreground objects over the back-\nground. Segmentation models should prioritize pedestrians\nover disproportionately large swaths of sky, road, vegeta-\ntion etc. Nevertheless, convolutions uniformly process all\nimage patches regardless of importance. This leads to spa-\ntial inefﬁciency in both computation and representation.\n2) Not all images have all concepts: Low-level features\nsuch as corners and edges exist in all natural images, so ap-\nplying low-level convolutional ﬁlters toall images is appro-\npriate. However, high-level features such as ear shape exist\nin speciﬁc images, so applying high-level ﬁlters to all im-\nages is computationally inefﬁcient. For example, dog fea-\ntures may not appear in images of ﬂowers, vehicles, aquatic\nanimals etc. This results in rarely-used, inapplicable ﬁlters\nexpending a signiﬁcant amount of compute.\n3) Convolutions struggle to relate spatially-distant\nconcepts: Each convolutional ﬁlter is constrained to oper-\nate on a small region, but long-range interactions between\nsemantic concepts is vital. To relate spatially-distant con-\ncepts, previous approaches increase kernel sizes, increase\nmodel depth, or adopt new operations like dilated convolu-\ntions, global pooling, and non-local attention layers. How-\never, by working within the pixel-convolution paradigm,\nthese approaches at best mitigate the problem, compensat-\ning for the convolution’s weaknesses by adding model and\ncomputational complexity.\nTo overcome the above challenges, we address the root\ncause, the pixel-convolution paradigm, and introduce theVi-\nsual Transformer (VT) (Figure 1), a new paradigm to rep-\nresent and process high-level concepts in images. Our intu-\nition is that a sentence with a few words (or visual tokens)\nsufﬁces to describe high-level concepts in an image. This\nmotivates a departure from the ﬁxed pixel-array representa-\ntion later in the network; instead, we use spatial attention\nto convert the feature map into a compact set of semantic\ntokens. We then feed these tokens to a transformer, a self-\nattention module widely used in natural language process-\ning [37] to capture token interactions. The resulting visual\ntokens computed can be directly used for image-level pre-\ndiction tasks (e.g., classiﬁcation) or be spatially re-projected\nto the feature map for pixel-level prediction tasks (e.g., seg-\nmentation). Unlike convolutions, our VT can better handle\nthe three challenges above: 1) judiciously allocating com-\n1\narXiv:2006.03677v4  [cs.CV]  20 Nov 2020\nTokenizerTokens Tin Tokens ToutTransformerProjector\nVisual Transformer\nSemantic Grouping\nSpatial Attention\nFeature map Xout\nSemantic Projection\nFeature map Xin\nXin\nInput image\nConv\nFigure 1: Diagram of a Visual Transformer (VT). For a given image, we ﬁrst apply convolutional layers to extract low-level\nfeatures. The output feature map is then fed to VT: First, apply a tokenizer, grouping pixels into a small number of visual\ntokens, each representing a semantic concept in the image. Second, apply transformers to model relationhips between tokens.\nThird, visual tokens are directly used for image classiﬁcation or projected back to the feature map for semantic segmentation.\nputation by attending to important regions, instead of treat-\ning all pixels equally; 2) encoding semantic concepts in a\nfew visual tokens relevant to the image, instead of model-\ning all concepts across all images; and 3) relating spatially-\ndistant concepts through self-attention in token-space.\nTo validate the effectiveness of VT and understanding\nits key components, we run controlled experiments by us-\ning VTs to replace convolutions in ResNet, a common test\nbed for new building blocks for image classiﬁcation. We\nalso use VTs to re-design feature-pyramid networks (FPN),\na strong baseline for semantic segmentation. Our expeir-\nments show that VTs achieve higher accuracy with lower\ncomputational cost in both tasks. For the ImageNet[11]\nbenchmark, we replace the last stage of ResNet[14] with\nVTs, reducing FLOPs of the stage by 6.9x and improving\ntop-1 accuracy by 4.6 to 7 points . For semantic segmen-\ntation on COCO-Stuff [2] and Look-Into-Person [25], VT-\nbased FPN achieves 0.35 points higher mIOU while reduc-\ning regular FPN module’s FLOPs by 6.4x.\n2. Relationship to previous work\nTransformers in vision models : A notable recent and\nrelevant trend is the adoption of transformers in vision mod-\nels. Dosovitskiy et al. propose a Vision Transformer (ViT)\n[12], dividing an image into 16 ×16 patches and feeding\nthese patches (i.e., tokens) into a standard transformer. Al-\nthough simple, this requires transformers to learn dense,\nrepeatable patterns (e.g., textures), which convolutions are\ndrastically more efﬁcient at learning. The simplicity incurs\nan extremely high computational price: ViT requires up to\n7 GPU years and 300M JFT dataset images to outperform\ncompeting convolutional variants. By contrast, we leverage\nthe respective strengths of each operation, using convolu-\ntions for extracting low-level features and transformers for\nrelating high-level concepts. We further use spatial atten-\ntion to focus on important regions, instead of treating each\nimage patch equally. This yields strong performance de-\nspite orders-of-magnitude less data and training time.\nAnother relevant work, DETR[3], adopts transformers to\nsimplify the hand-crafted anchor matching procedure in ob-\nject detection training. Although both adopt transformers,\nDETR is not directly comparable to our VT given their or-\nthogonal use cases, i.e., insights from both works could be\nused together in one model for compounded beneﬁt.\nGraph convolutions in vision models : Our work is\nalso related to previous efforts such as GloRe [6], Latent-\nGNN [47], and [26] that densely relate concepts in latent\nspace using graph convolutions. To augment convolutions,\n[26, 6, 47] adopt a procedure similar to ours: (1) extract-\ning latent variables to represent in graph nodes (analogous\nto our visual tokens) (2) applying graph convolution to cap-\nture node interactions (analogous to our transformer), and\n(3) projecting the nodes back to the feature map. Although\nthese approaches avoid spatial redundancy, they are suscep-\ntible to concept redundancy: the second limitation listed in\nthe introduction. In particular, by using ﬁxed weights that\nare not content-aware, the graph convolution expects a ﬁxed\nsemantic concept in each node, regardless of whether the\nconcept exists in the image. By contrast, a transformer uses\ncontent-aware weights, allowing visual tokens to represent\nvarying concepts. As a result, while graph convolutions re-\nquire hundreds of nodes (128 nodes in [4], 340 in [25], 150\nin [48]) to encode potential semantic concepts, our VT uses\njust 16 visual tokens and attains higher accuracy. Further-\nmore, while modules from [26, 6, 47] can only be added\nto a pretrained network to augment convolutions, VTs can\nreplace convolution layers to save FLOPs and parameters,\nand support training from scratch.\nAttention in vision models: In addition to being used in\n2\ntransformers, attention is also widely used in different forms\nin computer vision models [21, 20, 41, 44, 46, 40, 28, 18,\n19, 1, 49, 30]. Attention was ﬁrst used to modulate the fea-\nture map: attention values are computed from the input and\nmultiplied to the feature map as in [41, 21, 20, 44]. Later\nwork [46, 33, 39] interpret this “modulation” as a way to\nmake convolution spatially adaptive and content-aware. In\n[40], Wang et al. introduced non-local operators, equiva-\nlent to self-attention, to video understanding to capture the\nlong-range interactions. However, the computational com-\nplexity of self-attention grows quadratically with the num-\nber of pixels. [1] use self-attention to augment convolu-\ntions and reduce the compute cost by using small channel\nsizes for attention. [30, 28, 7, 49, 19] on the other hand\nrestrict receptive ﬁeld of self-attention and use it in a con-\nvolutional manner. Starting from [30], self-attentions are\nused as a stand-alone building block for vision models. Our\nwork is different from all above since we propose a novel\ntoken-transformer paradigm to replace the inefﬁcient pixel-\nconvolution paradigm and achieve superior performance.\nEfﬁcient vision models: Many recent research efforts\nhave been focusing on building vision models to achieve\nbetter performance with lower computational cost. Early\nwork in this direction includes [23, 31, 13, 17, 32, 16, 48,\n27, 43]. Recently, people use neural architecture search\n[42, 10, 38, 9, 36, 35] to optimize network’s performance\nwithin a search space that consists of existing convolution\noperators. The efforts above all seek to make the com-\nmon convolutional-neural net more computationally efﬁ-\ncient. In contrast, we propose a new building block that nat-\nurally eliminates the redundant computations in the pixel-\nconvolution paradigm.\n3. Visual Transformer\nWe illustrate the overall diagram of aVisual Transformer\n(VT) based model in Figure 1. First, process the input im-\nage with several convolution blocks, then feed the output\nfeature map to VTs. Our insight is to leverage the strengths\nof both convolutions and VTs: (1) early in the network, use\nconvolutions to learn densely-distributed, low-level patterns\nand (2) later in the network, use VTs to learn and relate\nmore sparsely-distributed, higher-order semantic concepts.\nAt the end of the network, use visual tokens for image-\nlevel prediction tasks and use the augmented feature map\nfor pixel-level prediction tasks.\nA VT module involves three steps: First, group pixels\ninto semantic concepts, to produce a compact set of visual\ntokens. Second, to model relationships between semantic\nconcepts, apply a transformer [37] to these visual tokens.\nThird, project these visual tokens back to pixel-space to ob-\ntain an augmented feature map. Similar paradigms can be\nfound in [6, 47, 26] but with one critical difference: Pre-\nvious methods use hundreds of semantic concepts (termed,\n“nodes”), whereas our VT uses as few as 16 visual tokens\nto achieve superior performance.\n3.1. Tokenizer\nOur intuition is that an image can be summarized by a\nfew handfuls of words, orvisual tokens. This contrasts con-\nvolutions, which use hundreds of ﬁlters, and graph convo-\nlutions, which use hundreds of “latent nodes” to detect all\npossible concepts regardless of image content. To leverage\nthis intuition, we introduce a tokenizer module to convert\nfeature maps into compact sets of visual tokens. Formally,\nwe denote the input feature map by X ∈RHW ×C (height\nH, width W, channels C) and visual tokens by T ∈RL×C\ns.t. L≪HW (Lrepresents the number of tokens).\n3.1.1 Filter-based Tokenizer\nA ﬁlter-based tokenizer, also adopted by [47, 6, 26], utilizes\nconvolutions to extract visual tokens. For feature map X,\nwe map each pixel Xp ∈RC to one of Lsemantic groups\nusing point-wise convolutions. Then, within each group, we\nspatially pool pixels to obtain tokens T. Formally,\nT = SOFTMAX HW (XWA)  \nA∈RHW ×L\nT X (1)\nHere, WA ∈RC×L forms semantic groups from X, and\nSOFTMAX HW (·) translates these activations into a spatial\nattention. Finally, A multiplies with X and computes\nweighted averages of pixels in X to make Lvisual tokens.\nHowever, many high-level semantic concepts are sparse\nand may each appear in only a few images. As a result, the\nﬁxed set of learned weights WA potentially wastes com-\nputation by modeling all such high-level concepts at once.\nWe call this a “ﬁlter-based” tokenizer, since it uses convo-\nlutional ﬁlters WA to extract visual tokens.\nH\nCFeature map XL Visual tokens\nSpatial attention A\nW LConv2d\nC\nWA\nFigure 2: Filter-based tokenizer that use convolution to\ngroup pixels using a ﬁxed convolution ﬁlter.\n3.1.2 Recurrent Tokenizer\nTo remedy the limitation of ﬁlter-based tokenizers, we pro-\npose a recurrent tokenizer with weights that are dependent\n3\non previous layer’s visual tokens. The intuition is to let\nthe previous layer’s tokensTin guide the extraction of new\ntokens for the current layer. The name of “recurrent tok-\nenizer” comes from that current tokens are computed de-\npendent on previous ones. Formally, we deﬁne\nWR = TinWT→R,\nT = SOFTMAX HW (XWR)T X,\n(2)\nwhere WT→R ∈ RC×C. In this way, the VT can in-\ncrementally reﬁne the set of visual tokens, conditioned on\npreviously-processed concepts. In practice, we apply recur-\nrent tokenizers starting from the second VT, since it requires\ntokens from a previous VT.\nH\nCFeature map XL Visual tokens\nSpatial attention A\nW L\nC\nConv2d\nL CPre-visual tokens\nFigure 3: Recurrent tokenizer that uses previous tokens to\nguide the token extraction in the current VT module.\n3.2. Transformer\nAfter tokenization, we then need to model interactions\nbetween these visual tokens. Previous works [6, 47, 26] use\ngraph convolutions to relate concepts. However, these op-\nerations use ﬁxed weights during inference, meaning each\ntoken (or “node”) is bound to a speciﬁc concept, there-\nfore graph convolutions waste computation by modeling all\nhigh-level concepts, even those that only appear in few im-\nages. To address this, we adopt transformers [37], which\nuse input-dependent weights by design. Due to this, trans-\nformers support visual tokens with variable meaning, cov-\nering more possible concepts with fewer tokens.\nWe employ a standard transformer with minor changes:\nT′\nout = Tin + SOFTMAX L\n(\n(TinK)(TinQ)T )\nTin, (3)\nTout = T′\nout + σ(T′\noutF1)F2, (4)\nwhere Tin,T′\nout,Tout ∈RL×C are the visual tokens. Dif-\nferent from graph convolution, in a transformer, weights be-\ntween tokens are input-dependent and computed as a key-\nquery product: (TinK)(TinQ)T ∈ RL×L. This allows\nus to use as few as 16 visual tokens, in contrast to hun-\ndreds of analogous nodes for graph-convolution approaches\n[6, 47, 26]. After the self-attention, we use a non-linearity\nand two pointwise convolutions in Equation (4), where\nF1,F2 ∈RC×C are weights, σ(·) is the ReLU function.\n3.3. Projector\nMany vision tasks require pixel-level details, but such\ndetails are not preserved in visual tokens. Therefore, we\nfuse the transformer’s output with the feature map to reﬁne\nthe feature map’s pixel-array representation as\nXout = Xin + SOFTMAX L\n(\n(XinWQ)(TWK)T )\nT,\n(5)\nwhere Xin,Xout ∈RHW ×C are the input and output fea-\nture map. (XinWQ) ∈RHW ×C is the query computed\nfrom the input feature map Xin. (XinWQ)p ∈ RC en-\ncodes the information pixel- p requires from the visual to-\nkens. (TWK) ∈RL×C is the key computed from the to-\nken T. (TWK)l ∈RC represents the information the l-th\ntoken encodes. The key-query product determines how to\nproject information encoded in visual tokens T to the orig-\ninal feature map. WQ ∈RC×C,WK ∈RC×C are learn-\nable weights used to compute queries and keys.\n4. Using Visual Transformers in vision models\nIn this section, we discuss how to use VTs as building\nblocks in vision models. We deﬁne three hyper-parameters\nfor each VT: channel size of the feature map; channel size\nof the visual tokens; and the number of visual tokens.\nImage classiﬁcation model: For image classiﬁcation,\nfollowing the convention of previous work, we build our\nnetworks with backbones inherited from ResNet [14].\nBased on ResNet-{18, 34, 50, 101 }, we build correspond-\ning visual-transformer-ResNets (VT-ResNets) by replacing\nthe last stage of convolutions with VT modules. The last\nstage of ResNet-{18, 34, 50, 101}contains 2 basic blocks,\n3 basic blocks, 3 bottleneck blocks, and 3 bottleneck blocks,\nrespectively. We replace them with the same number (2, 3,\n3, 3) of VT modules. At the end of stage-4 (before stage-5\nmax pooling), ResNet-{18, 34}generate feature maps with\nthe shape of 142 ×256, and ResNet- {50, 101 }generate\nfeature maps with the shape of 142 ×1024. We set VT’s\nfeature map channel size to be 256, 256, 1024, 1024 for\nResNet-{18, 34, 50, 101}. We adopt 16 visual tokens with\na channel size of 1024 for all the models. At the end of\nthe network, we output 16 visual tokens to the classiﬁcation\nhead, which applies an average pooling over the tokens and\nuse a fully-connected layer to predict the probabilities. A\ntable summarizing the stage-wise description of the model\nis provided in Appendix A. Since VTs only operate on 16\nvisual tokens, we can reduce the last stage’s FLOPs by up\nto 6.9x, as shown in Table 1.\nSemantic segmentation: We show that using VTs for\nsemantic segmentation can tackle several challenges with\nthe pixel-convolution paradigm. First, the computational\ncomplexity of convolution grows with the image resolu-\ntion. Second, convolutions struggles to capture long-term\n4\ninterp2dinterp2dConv2d\nConv2dConv2d\nTransformerTokenizer\nProjector\nIdentity\ninterp2dinterp2dConv2d\nConv2dConv2d\nFigure 4: Feature Pyramid Networks (FPN) (left) vs visual-transformer-FPN (VT-FPN) (right) for semantic segmentation.\nFPN uses convolution and interpolation to merge feature maps with different resolutions. VT-FPN extraxt visual tokens from\nall feature maps, merge them with one transformer, and project back to the original feature maps.\nR18 R34 R50 R101\nFLOPs Total 1.14x 1.16x 1.20x 1.09x\nStage-5 2.4x 5.0x 6.1x 6.9x\nParams Total 0.91x 1.21x 1.19x 1.19x\nStage-5 0.9x 1.5x 1.26x 1.26x\nTable 1: FLOPs and parameter size reduction of VTs on\nResNets by replacing the last stage of convolution modules\nwith VT modules.\ninteractions between pixels. VTs, on the other hand, op-\nerate on a small number of visual tokens regardless of the\nimage resolution, and since it models concept interactions\nin the token-space, it bypasses the “long-range” challenge\nwith pixel-arrays.\nTo validate our hypothesis, we use panoptic feature pyra-\nmid networks (FPN) [24] as a baseline and use VTs to im-\nprove the network. Panoptic FPNs use ResNet as backbone\nto extract feature maps from different stages with various\nresolutions. These feature maps are then fused by a feature\npyramid network in a top-down manner to generate a multi-\nscale and detail preserving feature map with rich semantics\nfor segmentation (Figure 4 left). FPN is computationally\nexpensive since it heavily relies on spatial convolutions op-\nerating on high resolution feature maps with large channel\nsizes. We use VTs to replace convolutions in FPN. We name\nthe new module as VT-FPN (Figure 4 right). From each res-\nolution’s feature map, VT-FPN extract 8 visual tokens with\na channel size of 1024. The visual tokens are combined and\nfed into one transformer to compute interactions between\nvisual tokens across resolutions. The output tokens are then\nprojected back to the original feature maps, which are then\nused to perform pixel-level prediction. Compared with the\noriginal FPN, the computational cost for VT-FPN is much\nsmaller since we only operate on a very small number of vi-\nsual tokens rather than all the pixels. Our experiment shows\nVT-FPN uses 6.4x fewer FLOPs than FPN while preserving\nor surpassing its performance (Table 9 & 10).\n5. Experiments\nWe conduct experiments with VTs on image classiﬁca-\ntion and semantic segmentation to (a) understand the key\ncomponents of VTs and (b) validate their effectiveness.\n5.1. Visual Transformer for Classiﬁcation\nWe conduct experiments on the ImageNet dataset [11]\nwith around 1.3 million images in the training set and 50\nthousand images in the validation set. We implement VT\nmodels in PyTorch [29]. We use stochastic gradient descent\n(SGD) optimizer with Nesterov momentum [34]. We use an\ninitial learning rate of0.1, a momentum of0.9, and a weight\ndecay of 4e-5. We train the model for 90 epochs, and decay\nthe learning rate by 10x every 30 epochs. We use a batch\nsize of 256 and 8 V100 GPUs for training.\nVT vs. ResNet with default training recipe: In Table 2,\nwe ﬁrst compare VT-ResNets and vanilla ResNets under the\nsame training recipe. VT-ResNets in this experiment use a\nﬁlter-based tokenizer for the ﬁrst VT module and recurrent\ntokenizers in later modules. We can see that after replacing\nthe last stage of convolutions in ResNet18 and ResNet34,\nVT-based ResNets use many fewer FLOPs: 244M fewer\nFLOPs for ResNet18 and 384M fewer for ResNet34. Mean-\nwhile, VT-ResNets achieve much higher top-1 validation\naccuracy than the corresponding ResNets by up to 2.2\npoints. This conﬁrms effectiveness of VTs. Also note that\nthe training accuracy achieved by VT-ResNets are much\nhigher than that of baseline ResNets: VT-R18 is 7.9 points\nhigher and VT-R34 is 6.9 points higher. This indicates\nthat VT-ResNets are overﬁtting more heavily than regular\nResNets. We hypothesize this is because VT-ResNets have\nmuch larger capacity and we need stronger regularization\n(e.g., data augmentation) to fully utilize the model capacity.\nWe address this in Section 5.2 and Table 8.\nTokenizer ablation studies : In Table 3, we compare\ndifferent types of tokenizers used by VTs. We consider a\npooling-based tokenizer, a clustering-based tokenizer, and\na ﬁlter-based tokenizer (Section 3.1.1). We use the can-\ndidate tokenizer in the ﬁrst VT module and use recurrent\ntokenizers in later modules. As a baseline, we imple-\nment a pooling-based tokenizer, which spatially downsam-\nples a feature map X to reduce its spatial dimensions from\nHW = 196 to L= 16, instead of grouping pixels by their\nsemantics. As a more advanced baseline, we consider a\nclustering-based tokenizer, which is described in Appendix\nC. It applies K-Means clustering in the semantic space to\ngroup pixels to visual tokens. As can be seen from Ta-\n5\nTop-1\nAcc (%)\n(Val)\nTop-1\nAcc (%)\n(Train)\nFLOPs\n(M)\nParams\n(M)\nR18 69.9 68.6 1814 11.7\nVT-R18 72.1 76.5 1570 11.7\nR34 73.3 73.9 3664 21.8\nVT-R34 75.0 80.8 3280 21.9\nTable 2: VT-ResNet vs. baseline ResNets on the ImageNet\ndataset. By replacing the last stage of ResNets, VT-ResNet\nuses 224M, 384M fewer FLOPs than the baseline ResNets\nwhile achieving 1.7 points and 2.2 points higher validation\naccuracy. Note the training accuracy of VT-ResNets are\nmuch higher. This indicates VT-ResNets have higher model\ncapacity and require stronger regularization (e.g., data aug-\nmentation) to fully utilize the model. See Table 8.\nTop-1\nAcc (%)\nFLOPs\n(M)\nParams\n(M)\nR18\nPooling-based 70.5 1549 11.0\nClustering-based 71.8 1579 11.6\nFilter-based 72.1 1580 11.7\nR34\nPooling-based 73.6 3246 20.6\nClustering-based 75.2 3299 21.8\nFilter-based 74.9 3280 21.9\nTable 3: VT-ResNets using with different types of tokeniz-\ners. Pooling-based tokenizers spatially downsample a fea-\nture map to obtain visual tokens. Clustering-based tokenizer\n(Appendix C) groups pixels in the semantic space. Filter-\nbased tokenizers (3.1.1) use convolution ﬁlters to group pix-\nels. Both ﬁlter-based and cluster-based tokenizers work\nmuch better than pooling-based tokenizers, validating the\nimportance of grouping pixels by their semantics.\nble 3, ﬁlter-based and clustering-based tokenizers perform\nsigniﬁcantly better than the pooling-based baseline, validat-\ning our hypothesis that feature maps contain redundancies,\nand this can be addressed by grouping pixels in semantic\nspace. The difference between ﬁlter-based and clustering-\nbased tokenizers is small and vary between R18 and R34.\nWe hypothesize this is because both tokenizers have their\nown drawbacks. The ﬁlter-based tokenizer relies on ﬁxed\nconvolution ﬁlters to detect and assign pixels to semantic\ngroups, and is limited by the capacity of the convolution\nﬁlters to deal with diverse and sparse high-level semantic\nconcepts. On the other hand, the clustering-based tokenizer\nextracts semantic concepts that exist in the image, but it is\nnot designed to capture the essential semantic concepts.\nIn Table 4, we validate the recurrent tokenizer’s effec-\ntiveness. We use a ﬁlter-based tokenizer in the ﬁrst VT\nmodule and use recurrent tokenizers in subsequent modules.\nExperiments show that using recurrent tokenizers leads to\nhigher accuracy.\nTop-1\nAcc (%)\nFLOPs\n(M)\nParams\n(M)\nR18 w/ RT 72.0 1569 11.7\nw/o RT 71.2 1586 11.1\nR34 w/ RT 74.9 3246 21.9\nw/o RT 74.5 3335 20.9\nTable 4: VT-ResNets that use recurrent tokenizers achieve\nbetter performance, since recurrent tokenizers are content-\naware. RT denotes recurrent tokenizer.\nTop-1\nAcc (%)\nFLOPs\n(M)\nParams\n(M)\nR18\nNone 68.7 1528 8.5\nGraphConv 69.3 1528 8.5\nTransformer 71.5 1580 11.7\nR34\nNone 73.3 3222 17.1\nGraphConv 73.7 3223 17.1\nTransformer 75.2 3299 21.8\nTable 5: VT-ResNets using different modules to model to-\nken relationships. Models using transformers perform bet-\nter than graph-convolution or no token-space operations.\nThis validates that it is important to model relationships\nbetween visual token (semantic concepts) and transformer\nwork better than graph convolution in relating tokens.\nModeling token relationships: In Table 5, we compare\ndifferent methods of capturing token relationships. As a\nbaseline, we do not compute the interactions between to-\nkens. This leads to the worst performance among all vari-\nations. This validates the necessities of capturing the rela-\ntionship between different semantic concepts. Another al-\nternative is to use graph-convolutions similar to [6, 26, 47],\nbut its performance is worse than that of VTs. This is likely\ndue to the fact that graph-convolutions bind each visual to-\nken to a ﬁxed semantic concept. In comparison, using trans-\nformers allows each visual token to encode any semantic\nconcepts as long as the concept appears in the image. This\nallows the models to fully utilize its capacity.\nToken efﬁciency ablation : In Table 6, we test vary-\ning numbers of visual tokens, only to ﬁnd negligible or\nno increase in accuracy. This agrees with our hypothesis\nthat VTs are already capturing a wide variety of concepts\nwith just a few handfuls of tokens–additional tokens are not\nneeded, as the space of possible, high-level concepts is al-\nready covered.\nPojection ablation: In Table 7, we study whether we\nneed to project visual tokens back to feature maps. We hy-\npothesize that projecting the visual tokens is an important\nstep since in vision understanding, pixel-level semantics are\nvery important, and visual tokens are representations in the\nsemantic space that do not encode any spatial information.\nAs validated by Table 7, projecting visual tokens back to\nthe feature map leads to higher performance, even for im-\nage classiﬁcation tasks.\n6\nNo.\nTokens\nTop-1\nAcc (%)\nFLOPs\n(M)\nParams\n(M)\nR18\n16 71.8 1579 11.6\n32 71.9 1711 11.6\n64 72.1 1979 11.6\nR34\n16 75.1 3299 21.8\n32 75.0 3514 21.8\n64 75.0 3952 21.8\nTable 6: Using more visual tokens do not improve the ac-\ncuracy of VT, which agrees with our hypothesis that images\ncan be described by a compact set of visual tokens.\nTop-1\nAcc (%)\nFLOPs\n(M)\nParams\n(M)\nR18 w/ projector 72.0 1569 11.7\nw/o projector 71.0 1498 9.4\nR34 w/ projector 74.8 3280 21.9\nw/o projector 73.9 3159 17.4\nTable 7: VTs that projects tokens back to feature maps per-\nform better. This may be because feature maps still encode\nimportant spatial information.\n5.2. Training with Advanced Recipe\nIn Table 2, we show that under the regular training\nrecipe, the VT-ResNets experience serious overﬁtting. De-\nspite their accuracy improvement on the validation set, its\ntraining accuracy improves by a signiﬁcantly larger margin.\nWe hypothesize that this is because VT-based models have\nmuch higher model capacity. To fully unleash the potential\nof VT, we used a more advanced training recipe to train VT\nmodels. To prevent overﬁtting, we train with more training\nepochs, stronger data augmentation, stronger regularization,\nand distillation. Speciﬁcally, we train VT-ResNet models\nfor 400 epochs with RMSProp. We use an initial learning\nrate of 0.01 and increase to 0.16 in 5 warmup epochs, then\nreduce the learning rate by a factor of 0.9875 per epoch. We\nuse synchronized batch normalization and distributed train-\ning with a batch size of 2048. We use label smoothing and\nAutoAugment [8] and we set the stochastic depth survival\nprobability [22] and dropout ratio to be 0.9 and 0.2, respec-\ntively. We use exponential moving average (EMA) on the\nmodel weights with 0.99985 decay. We use knowledge dis-\ntillation [15] in the training recipe, where the teacher model\nis FBNetV3-G [9]. The total loss is a weighted sum of dis-\ntillation loss (×0.8) and cross entropy loss (×0.2).\nOur results are reported in Table 8. Compared with the\nbaseline ResNet models, VT-ResNet models achieve 4.6 to\n7 points higher accuracy and surpass all other related work\nthat adopt attention of different forms based on ResNets\n[21, 41, 1, 5, 19, 30, 49, 6]. This validates that our ad-\nvanced training recipe better utilizes the model capacity of\nVT-ResNet models to outperform all previous baselines.\nNote that in addition to the architecture differences, pre-\nvious works also used their own training recipes and it is\nModels Top-1\nAcc (%)\nFLOPs\n(G)\nParams\n(M)\nR18[14] 69.8 1.814 11.7\nR18+SE[21, 41] 70.6 1.814 11.8\nR18+CBAM[41] 70.7 1.815 11.8\nLR-R18[19] 74.6 2.5 14.4\nR18[14](ours) 73.8 1.814 11.7\nVT-R18(ours) 76.8 1.569 11.7\nR34[14] 73.3 3.664 21.8\nR34+SE[21, 41] 73.9 3.664 22.0\nR34+CBAM[41] 74.0 3.664 22.9\nAA-R34[1] 74.7 3.55 20.7\nR34[14](ours) 77.7 3.664 21.8\nVT-R34(ours) 79.9 3.236 19.2\nR50[14] 76.0 4.089 25.5\nR50+SE[21, 41] 76.9 3.860* 28.1\nR50+CBAM[41] 77.3 3.864* 28.1\nLR-R50[19] 77.3 4.3 23.3\nStand-Alone[30] 77.6 3.6 18.0\nAA-R50[1] 77.7 4.1 25.6\nA2-R50[5] 77.0 - -\nSAN19[49] 78.2 3.3 20.5\nGloRe-R50[6] 78.4 5.2 30.5\nVT-R50(ours) 80.6 3.412 21.4\nR101[14] 77.4 7.802 44.4\nR101+SE [21, 41] 77.7 7.575* 49.3\nR101+CBAM[41] 78.5 7.581* 49.3\nLR-R101[19] 78.5 7.79 42.0\nAA-R101[1] 78.7 8.05 45.4\nGloRe-R200[6] 79.9 16.9 70.6\nVT-R101(ours) 82.3 7.129 41.5\nTable 8: Comparing VT-ResNets with other attention-\naugmented ResNets on ImageNet. *The baseline ResNet\nFLOPs reported in [41] is lower than our baseline.\ninfeasible for us to try these recipes one by one. So to un-\nderstand the source of the accuracy gain, we use the same\ntraining recipe to train baseline ResNet18 and ResNet34\nand also observe signiﬁcant accuracy improvement on base-\nline ResNets. But note that under the advanced training\nrecipe, the accuracy gap between VT-ResNet and baselines\nincreases from 1.7 and 2.2 points to 2.2 and 3.0 points. This\nfurther validates that a stronger training recipe can better\nutilize the model capacity of VTs. While achieving higher\naccuracy than previous work, VT-ResNets also use much\nfewer FLOPs and parameters, even we only replace the last\nstage of the baseline model. If we consider the reduction\nover the original stage, we observe FLOP reductions of up\nto 6.9x, as shown in Table 1.\n5.3. Visual Transformer for Semantic Segmentation\nWe conduct experiments to test the effectiveness of VT\nfor semantic segmentation on the COCO-stuff dataset [2]\nand the LIP dataset [25]. The COCO-stuff dataset contains\nannotations for 91 stuff classes with 118K training images\nand 5K validation images. LIP dataset is a collection of im-\n7\nFigure 5: Visualization of the spatial attention generated by a ﬁlter-based tokenizer on images from the LIP dataset. Red\ndenotes higher attention values and color blue denotes lower. Without any supervision, visual tokens automatically focus on\ndifferent areas of the image that correspond to different semantic concepts, such as sheep, ground, clothes, woods.\nmIoU\n(%)\nTotal\nFLOPs (G)\nFPN\nFLOPs (G)\nR-50 FPN 40.78 159 55.1\nVT-FPN 41.00 113 (1.41x) 8.5 (6.48x)\nR-101 FPN 41.51 231 55.1\nVT-FPN 41.50 185 (1.25x) 8.5 (6.48x)\nTable 9: Semantic segmentation results on the COCO-stuff\nvalidation set. The FLOPs are calculated with a typical in-\nput resolution of 800×1216.\nmIoU\n(%)\nTotal\nFLOPs (G)\nFPN\nFLOPs (G)\nR50 FPN 47.04 37.1 12.8\nVT-FPN 47.39 26.4 (1.41x) 2.0 (6.40x)\nR101 FPN 47.35 54.4 12.8\nVT-FPN 47.58 43.6 (1.25x) 2.0 (6.40x)\nTable 10: Semantic segmentation results on the Look Into\nPerson validation set. The FLOPs are calculated with a typ-\nical input resolution of 473×473.\nages containing humans with challenging poses and views.\nFor the COCO-stuff dataset, we train a VT-FPN model with\nResNet-{50, 101}backbones. Our implementation is based\non Detectron2 [45]. Our training recipe is based on the se-\nmantic segmentation FPN recipe with 1x training steps, ex-\ncept that we use synchronized batch normalization in the\nVT-FPN, change the batch size to 32, and use a base learn-\ning rate of 0.04. For the LIP dataset, we also use synchro-\nnized batch normalization with a batch size of 96. We opti-\nmize the model via SGD with weight decay of 0.0005 and\nlearning rate of 0.01.\nAs we can see in Table 9 and 10, after replacing FPN\nwith VT-FPN, both ResNet-50 and ResNet-101 based mod-\nels achieve slightly higher mIoU, but VT-FPN requires 6.5x\nfewer FLOPs than a FPN module.\n5.4. Visualizing Visual Tokens\nWe hypothesize that visual tokens extracted in the VT\ncorrespond to different high-level semantics in the image.\nTo better understand this, we visualize the spatial attention\nA ∈RHW ×L generated by ﬁlter-based tokenizers, where\neach A:,l ∈RHW is an attention map to show how each\npixel of the image contributes to token- l. We plot the at-\ntention map in Figure 5, and we can see that without any\nsupervision, different visual tokens attend to different se-\nmantic concepts in the image, corresponding to parts of the\nbackground or foreground objects. More visualization re-\nsults are provided in Appendix B.\n6. Conclusion\nThe convention in computer vision is to represent im-\nages as pixel arrays and to apply the de facto deep learn-\ning operator – the convolution. In lieu of this, we pro-\npose Visual Transformers (VTs), as hallmarks of a new\ncomputer vision paradigm, learning and relating sparsely-\ndistributed, high-level concepts far more efﬁciently: Instead\nof pixel arrays, VTs represent just the high-level concepts\nin an image using visual tokens. Instead of convolutions,\nVTs apply transformers to directly relate semantic concepts\nin token-space. To evaluate this idea, we replace convo-\nlutional modules with VTs, obtaining signiﬁcant accuracy\nimprovements across tasks and datasets. Using an advanced\ntraining recipe, our VT improves ResNet accuracy on Ima-\ngeNet by 4.6 to 7 points. For semantic segmentation on\nLIP and COCO-stuff, VT-based feature pyramid networks\n(FPN) achieve 0.35 points higher mIoU despite 6.5x fewer\nFLOPs than convolutional FPN modules. This paradigm\ncan furthermore be compounded with other contempora-\nneous tricks beyond the scope of this paper, including ex-\ntra training data and neural architecture search. However,\ninstead of presenting a mosh pit of deep learning tricks,\nour goal is to show that the pixel-convolution paradigm is\nfraught with redundancies. To compensate, modern meth-\nods add exceptional amounts of computational complex-\nity. However, as model designers and practitioners, we\ncan tackle the root cause instead of exacerbating compute\ndemands, addressing redundancy in the pixel-convolution\nconvention by adopting the newfound token-transformer\nparadigm moving forward.\n8\nReferences\n[1] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional net-\nworks. In Proceedings of the IEEE International Conference\non Computer Vision, pages 3286–3295, 2019. 3, 7\n[2] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In Computer vision\nand pattern recognition (CVPR), 2018 IEEE conference on .\nIEEE, 2018. 2, 7\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020. 2\n[4] Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and\nAlan L Yuille. Attention to scale: Scale-aware semantic im-\nage segmentation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3640–3649,\n2016. 2\n[5] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\nYan, and Jiashi Feng. Aˆ 2-nets: Double attention net-\nworks. In Advances in Neural Information Processing Sys-\ntems, pages 352–361, 2018. 7\n[6] Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Yan\nShuicheng, Jiashi Feng, and Yannis Kalantidis. Graph-based\nglobal reasoning networks. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n433–442, 2019. 2, 3, 4, 6, 7\n[7] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin\nJaggi. On the relationship between self-attention and con-\nvolutional layers. arXiv preprint arXiv:1911.03584 , 2019.\n3\n[8] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\nstrategies from data. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 113–123,\n2019. 7\n[9] Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zi-\njian He, Zhen Wei, Kan Chen, Yuandong Tian, Matthew\nYu, Peter Vajda, et al. Fbnetv3: Joint architecture-recipe\nsearch using neural acquisition function. arXiv preprint\narXiv:2006.02049, 2020. 3, 7\n[10] Xiaoliang Dai, Peizhao Zhang, Bichen Wu, Hongxu Yin, Fei\nSun, Yanghan Wang, Marat Dukhan, Yunqing Hu, Yiming\nWu, Yangqing Jia, et al. Chamnet: Towards efﬁcient network\ndesign through platform-aware model adaptation. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 11398–11407, 2019. 3\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 2, 5\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2\n[13] Amir Gholami, Kiseok Kwon, Bichen Wu, Zizheng Tai,\nXiangyu Yue, Peter Jin, Sicheng Zhao, and Kurt Keutzer.\nSqueezenext: Hardware-aware neural network design. In\nProceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition Workshops, pages 1638–1647,\n2018. 3\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 2, 4, 7, 12\n[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015. 7\n[16] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 1314–1324, 2019. 3\n[17] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 3\n[18] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3588–3597, 2018. 3\n[19] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Lo-\ncal relation networks for image recognition. In Proceedings\nof the IEEE International Conference on Computer Vision ,\npages 3464–3473, 2019. 3, 7\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\nVedaldi. Gather-excite: Exploiting feature context in convo-\nlutional neural networks. In Advances in Neural Information\nProcessing Systems, pages 9401–9411, 2018. 3\n[21] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7132–7141, 2018. 3, 7\n[22] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q Weinberger. Deep networks with stochastic depth. In\nEuropean conference on computer vision , pages 646–661.\nSpringer, 2016. 7\n[23] Forrest N Iandola, Song Han, Matthew W Moskewicz,\nKhalid Ashraf, William J Dally, and Kurt Keutzer.\nSqueezenet: Alexnet-level accuracy with 50x fewer pa-\nrameters and¡ 0.5 mb model size. arXiv preprint\narXiv:1602.07360, 2016. 3\n[24] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDoll´ar. Panoptic feature pyramid networks. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6399–6408, 2019. 5\n[25] Xiaodan Liang, Ke Gong, Xiaohui Shen, and Liang Lin.\nLook into person: Joint body parsing & pose estimation net-\nwork and a new benchmark. IEEE transactions on pattern\nanalysis and machine intelligence, 41(4):871–885, 2018. 2,\n7\n9\n[26] Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, and\nEric P Xing. Symbolic graph reasoning meets convolu-\ntions. In Advances in Neural Information Processing Sys-\ntems, pages 1853–1863, 2018. 2, 3, 4, 6\n[27] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\nShufﬂenet v2: Practical guidelines for efﬁcient cnn architec-\nture design. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 116–131, 2018. 3\n[28] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. arXiv preprint arXiv:1802.05751, 2018. 3\n[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-\nperative style, high-performance deep learning library. In H.\nWallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E.\nFox, and R. Garnett, editors, Advances in Neural Informa-\ntion Processing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc., 2019. 5\n[30] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019. 3, 7\n[31] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon,\nand Ali Farhadi. Xnor-net: Imagenet classiﬁcation using bi-\nnary convolutional neural networks. In European conference\non computer vision, pages 525–542. Springer, 2016. 3\n[32] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510–4520, 2018. 3\n[33] Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik\nLearned-Miller, and Jan Kautz. Pixel-adaptive convolutional\nneural networks. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019. 3\n[34] Ilya Sutskever, James Martens, George Dahl, and Geoffrey\nHinton. On the importance of initialization and momentum\nin deep learning. In International conference on machine\nlearning, pages 1139–1147, 2013. 5\n[35] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\nMark Sandler, Andrew Howard, and Quoc V Le. Mnas-\nnet: Platform-aware neural architecture search for mobile.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2820–2828, 2019. 3\n[36] Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking\nmodel scaling for convolutional neural networks. arXiv\npreprint arXiv:1905.11946, 2019. 3\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 1,\n3, 4\n[38] Alvin Wan, Xiaoliang Dai, Peizhao Zhang, Zijian He, Yuan-\ndong Tian, Saining Xie, Bichen Wu, Matthew Yu, Tao Xu,\nKan Chen, et al. Fbnetv2: Differentiable neural architecture\nsearch for spatial and channel dimensions. arXiv preprint\narXiv:2004.05565, 2020. 3\n[39] Weiyue Wang and Ulrich Neumann. Depth-aware cnn for\nrgb-d segmentation. In Proceedings of the European Con-\nference on Computer Vision (ECCV), pages 135–150, 2018.\n3\n[40] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794–7803, 2018. 3\n[41] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In\nSo Kweon. Cbam: Convolutional block attention module.\nIn Proceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 3–19, 2018. 3, 7\n[42] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing\nJia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient con-\nvnet design via differentiable neural architecture search. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 10734–10742, 2019. 3\n[43] Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng\nZhao, Noah Golmant, Amir Gholaminejad, Joseph Gonza-\nlez, and Kurt Keutzer. Shift: A zero ﬂop, zero parameter\nalternative to spatial convolutions. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 9127–9135, 2018. 3\n[44] Bichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue, and\nKurt Keutzer. Squeezesegv2: Improved model structure and\nunsupervised domain adaptation for road-object segmenta-\ntion from a lidar point cloud. In ICRA, 2019. 3\n[45] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019. 8\n[46] Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter\nVajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeeze-\nsegv3: Spatially-adaptive convolution for efﬁcient point-\ncloud segmentation. arXiv preprint arXiv:2004.01803, 2020.\n3\n[47] Songyang Zhang, Xuming He, and Shipeng Yan. Latent-\ngnn: Learning efﬁcient non-local relations for visual recog-\nnition. In International Conference on Machine Learning ,\npages 7374–7383, 2019. 2, 3, 4, 6\n[48] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\nShufﬂenet: An extremely efﬁcient convolutional neural net-\nwork for mobile devices. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n6848–6856, 2018. 2, 3\n[49] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-\ning self-attention for image recognition. arXiv preprint\narXiv:2004.13621, 2020. 3, 7\n10\nFeature map X Visual tokens\nDSCLWK\nLloyd’s algorithmniter\n LH\nCW\nL\nSpatial attention A\nC\nConv2d\ns\nFigure 6: Cluster-based tokenizer that group pixels using\nthe K-Means centroids of the pixels in the semantic space.\nA. Stage-wise model description of VT-ResNet\nIn this section, we provide a stage-wise description of the\nmodel conﬁgurations for VT-based ResNet (VT-ResNet).\nWe use three hyper-parameters to control a VT module:\nchannel size of the output feature map C, channel size of\nvisual token CT, and the number of visual tokens L. The\nmodel conﬁgurations are described in Table 11.\nB. More visualization results\nWe provide more visualization of the spatial attention on\nimages from the LIP dataset in Figure 7.\nC. Clustering-based tokenizer\nTo address this limitation of ﬁlter-based tokenizers, we\ndevise a content-aware WK variant of WA to form se-\nmantic groups from X. Our insight is to extract concepts\npresent in the current image by clustering pixels, instead of\napplying the same ﬁlters regardless of the image content.\nFirst, we treat each pixel as a sample {Xp}HW\np=1 , and ap-\nply k-means to ﬁnd Lcentroids, which are stacked to form\nWK ∈RC×L. Each centroid represents one semantic con-\ncept in the image. Second, we replace WA in Equation (1)\nwith WK to form Lsemantic groups of channels:\nWK = KMEANS (X),\nT = SOFTMAX HW (XWK)T X.\n(6)\nPseudo-code for our K-means implementation is provided\nin Listing 1, and can be summarized as: Normalize all\npixels to unit vectors, initialize centroids with a spatially-\ndownsampled feature map, and run Lloyd’s algorithm to\nproduce centroids.\n1 def kmeans(X_nchw, L, niter):\n2 # Input:\n3 # X_nchw - feature map\n4 # L - num token\n5 # niter - num iters of Lloyd’s\n6 N, C, H, W = X_nchw.size()\n7 # Initialization as down-sampled X\n8 U_ncl = downsample(X).view(N, C, L)\n9 X_ncp = X_nchw.view(N, C, H*W) # p = h *w\n10 # Normalize to unit vectors\n11 U_ncl = U_ncl.normalize(dim=1)\n12 X_ncp = X_ncp.normalize(dim=1)\n13 for _ in range(niter): # Lloyd’s algorithm\n14 dist_npl = (\n15 X_ncp[..., None] - U_ncl[:, :, None, :]\n16 ).norm(dim=1)\n17 mask_npl = (dist_npl == dist_npl.min(dim=2)\n18 U_ncl = X_ncp.MatMul(mask_npl)\n19 U_ncl = U_ncl / mask_npl.sum(dim=1)\n20 U_ncl = U_ncl.normalize(dim=1)\n21 return U_ncl # centroids\nListing 1: Pseudo-code of K-Means implemented in\nPyTorch-like language\nAlthough this tokenizer efﬁciently models only concepts\nin the current image, the drawback is that it is not designed\nto choose the most discriminative concepts.\n11\nStage Resolution VT-R18 VT-R34 VT-R50 VT-R101\n1 56×56 conv7×7-C64-S2 →maxpool3×3-S2\n2 56×56 BB-C64 ×2 BB-C64 ×3 BN-C256 ×3 BN-C256 ×3\n3 28×28 BB-C128 ×2 BB-C128 ×4 BN-C512 ×4 BN-C512 ×4\n4 14×14 BB-C256 ×2 BB-C256 ×6 BN-C1024 ×6 BN-C1024 ×23\n5 16 VT-C512-L16\n-CT1024 ×2\nVT-C512-L16\n-CT1024 ×3\nVT-C1024-L16\n-CT1024 ×3\nVT-C1024-L16\n-CT1024 ×3\nhead 1 avgpool-fc1000\nTable 11: Model descriptions for VT-ResNets. VT-R18 denotes visual-transformer-ResNet-18. “conv7×7-C64-S2” denotes\na 7-by-7 convolution with an output channel size of 64 and a stride of 2. “BB-C64 ×2” denotes a basic block [14] with an\noutput channel size of 64 and it is repeated twice. “BN-C256×3” denotes a bottleneck block [14] with an output channel size\nof 256 and it is repeated by three times. “VT-C512-L16-CT1024 ×2” denotes a VT block with a channel size for the output\nfeature map as 512, channel size for visual tokens as 1024, and the number of tokens as 16.\nFigure 7: Visualization of the spatial attention generated by a ﬁlter-based tokenizer. Red denotes higher attention values and\ncolor blue denotes lower. Without any supervision, visual tokens automatically focus on different areas of the image that\ncorrespond to different semantic concepts.\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6986672878265381
    },
    {
      "name": "FLOPS",
      "score": 0.6776587963104248
    },
    {
      "name": "Pixel",
      "score": 0.6613487601280212
    },
    {
      "name": "Security token",
      "score": 0.6531774401664734
    },
    {
      "name": "Transformer",
      "score": 0.6474630832672119
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6366046667098999
    },
    {
      "name": "Computer vision",
      "score": 0.5292441248893738
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.4112810492515564
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33842146396636963
    },
    {
      "name": "Mathematics",
      "score": 0.14069947600364685
    },
    {
      "name": "Engineering",
      "score": 0.09471240639686584
    },
    {
      "name": "Parallel computing",
      "score": 0.08641782402992249
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}