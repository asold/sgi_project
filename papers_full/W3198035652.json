{
    "title": "nnFormer: Interleaved Transformer for Volumetric Segmentation",
    "url": "https://openalex.org/W3198035652",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221472661",
            "name": "Zhou, Hong-Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4307537526",
            "name": "Guo, Jiansen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A82140145",
            "name": "Zhang Ying-hao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746028584",
            "name": "Yu Lequan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1383091939",
            "name": "Wang Liansheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3018032560",
            "name": "Yu, Yizhou",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2798122215",
        "https://openalex.org/W3162316477",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2973465872",
        "https://openalex.org/W3134565071",
        "https://openalex.org/W3182372246",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3112701542",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W3160284783",
        "https://openalex.org/W3186019569",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W2809446072",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3203841574",
        "https://openalex.org/W2803097213",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W3132503749",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3175265357",
        "https://openalex.org/W2965380104",
        "https://openalex.org/W2999972254",
        "https://openalex.org/W3112503277",
        "https://openalex.org/W2419448466",
        "https://openalex.org/W3130695101",
        "https://openalex.org/W3135451226",
        "https://openalex.org/W3160228192",
        "https://openalex.org/W2804047627",
        "https://openalex.org/W3134689216",
        "https://openalex.org/W3144761861",
        "https://openalex.org/W3178812510",
        "https://openalex.org/W3168491317",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2888358068",
        "https://openalex.org/W3097510987",
        "https://openalex.org/W2921526792",
        "https://openalex.org/W2907965334"
    ],
    "abstract": "Transformer, the model of choice for natural language processing, has drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks to overcome their inherent shortcomings of spatial inductive bias. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations. To address this issue, we introduce nnFormer, a 3D transformer for volumetric medical image segmentation. nnFormer not only exploits the combination of interleaved convolution and self-attention operations, but also introduces local and global volume-based self-attention mechanism to learn volume representations. Moreover, nnFormer proposes to use skip attention to replace the traditional concatenation/summation operations in skip connections in U-Net like architecture. Experiments show that nnFormer significantly outperforms previous transformer-based counterparts by large margins on three public datasets. Compared to nnUNet, nnFormer produces significantly lower HD95 and comparable DSC results. Furthermore, we show that nnFormer and nnUNet are highly complementary to each other in model ensembling.",
    "full_text": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2017 1\nnnFormer: Volumetric Medical Image\nSegmentation via a 3D Transformer\nHong-Yu Zhou,Student Member, IEEE, Jiansen Guo, Yinghao Zhang, Xiaoguang Han,\nLequan Yu, Liansheng Wang,Member, IEEE, and Yizhou Yu,Fellow, IEEE\nAbstract‚Äî Transformer, the model of choice for natural\nlanguage processing, has drawn scant attention from the\nmedical imaging community. Given the ability to exploit\nlong-term dependencies, transformers are promising to\nhelp atypical convolutional neural networks to overcome\ntheir inherent shortcomings of spatial inductive bias. How-\never, most of recently proposed transformer-based seg-\nmentation approaches simply treated transformers as as-\nsisted modules to help encode global context into convolu-\ntional representations. To address this issue, we introduce\nnnFormer (i.e., not-another transFormer), a 3D transformer\nfor volumetric medical image segmentation. nnFormer not\nonly exploits the combination of interleaved convolution\nand self-attention operations, but also introduces local\nand global volume-based self-attention mechanism to learn\nvolume representations. Moreover, nnFormer proposes to\nuse skip attention to replace the traditional concatena-\ntion/summation operations in skip connections in U-Net\nlike architecture. Experiments show that nnFormer signif-\nicantly outperforms previous transformer-based counter-\nparts by large margins on three public datasets. Compared\nto nnUNet, nnFormer produces signiÔ¨Åcantly lower HD95\nand comparable DSC results. Furthermore, we show that\nnnFormer and nnUNet are highly complementary to each\nother in model ensembling. Codes and models of nnFormer\nare available at https://git.io/JSf3i.\nIndex Terms‚Äî Transformer, Attention Mechanism, Volu-\nmetric Image Segmentation\nI. INTRODUCTION\nTransformer [1], which has become the de-facto choice\nfor natural language processing (NLP) problems, has recently\nbeen widely exploited in vision-based applications [2]‚Äì[5].\nThe core idea behind is to apply the self-attention mechanism\n(Corresponding author: Liansheng Wang and Yizhou Yu.)\nThis work was done when Hong-Yu Zhou was a visiting student at\nXiamen University.\nHong-Yu Zhou, Jiansen Guo, Yinghao Zhang and Liansheng\nWang are with the Department of Computer Science, Xiamen\nUniversity, Siming District, Xiamen, Fujian Province, P .R. China\n(email: whuzhouhongyu@gmail.com, jsguo@stu.xmu.edu.cn, zhangy-\ninghao@stu.xmu.edu.cn, lswang@xmu.edu.cn).\nHong-Yu Zhou and Yizhou Yu are with the Department of Computer\nScience, The University of Hong Kong, Pokfulam, Hong Kong (e-mail:\nyizhouy@acm.org).\nXiaoguang Han is with the Shenzhen Research Institute of Big\nData, The Chinese University of Hong Kong (Shenzhen), Shenzhen,\nGuangdong Province, P .R. China (email: hanxiaoguang@cuhk.edu.cn).\nLequan Yu is with the Department of Statistics and Actuarial Sci-\nence, The University of Hong Kong, Pokfulam, Hong Kong (e-mail:\nlqyu@hku.hk).\nFirst two authors contributed equally.\nto capture long-range dependencies. Compared to convolu-\ntional neural networks (i.e., convnets [6]), transformer relaxes\nthe inductive bias of locality, making it more capable of\ndealing with non-local interactions [7]‚Äì[9]. It has also been\ninvestigated that the prediction errors of transformers are more\nconsistent with those of humans than convnets [10].\nGiven the fact that transformers are naturally more advanta-\ngeous than convnets, there are a number of approaches trying\nto apply transformers to the Ô¨Åeld of medical image analysis.\nChen et al. [11] Ô¨Årst time proposed TransUNet to explore\nthe potential of transformers in the context of medical image\nsegmentation. The overall architecture of TransUNet is similar\nto that of U-Net [12], where convnets act as feature extractors\nand transformers help encode the global context. In fact, one\nmajor characteristic of TransUNet and most of its followers\n[13]‚Äì[16] is to treat convnets as main bodies, on top of which\ntransformers are further applied to capture long-term depen-\ndencies. However, such feature may cause a problem, which\nis the advantages of transformers are not fully exploited. In\nother words, we believe one- or two-layer transformers are not\nenough to entangle long-term dependencies with convolutional\nrepresentations that often contain precise spatial information\nand provide hierarchical concepts.\nTo address the above issue, some researchers [17]‚Äì[19]\nstarted to use transformers as the main stem in segmen-\ntation models. Karimi et al. [17] Ô¨Årst time introduced a\nconvolution-free segmentation model by forwarding Ô¨Çattened\nimage representations to transformers, whose outputs are then\nreorganized into 3D tensors to align with segmentation masks.\nRecently, Swin Transformer [3] showed that by referring to\nthe feature pyramids used in convnets, transformers can learn\nhierarchical object concepts at different scales by applying\nappropriate down-sampling to feature maps. Inspired by this\nidea, SwinUNet [18] utilized hierarchical transformer blocks\nto construct the encoder and decoder within a U-Net like\narchitecture, based on which DS-TransUNet [19] added one\nmore encoder to accept different-sized inputs. Both SwinUNet\nand DS-TransUNet have achieved consistent improvements\nover TransUNet. Nonetheless, they did not explore how to ap-\npropriately combine convolution and self-attention for building\nan optimal medical segmentation network.\nIn contrast, nnFormer (i.e., not-another transFormer) uses\na hybrid stem where convolution and self-attention are inter-\nleaved to give full play to their strengths. Figure 1 presents\nthe effects of different components used in the encoder of nn-\narXiv:2109.03201v6  [cs.CV]  4 Feb 2022\n2 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2017\nConvolutionalembeddingTransformerblocksConvolutionaldown-samplingTransformerblocks\nIncorporatinglong-termdependenciesintohigh-levelfeatures.1.Precisespatialencoding.2.High-resolutionlow-levelfeatures. Modelingobjectconceptsfromhigh-levelfeaturesatmultiplescales.\nFig. 1: The interleaved stem used in the encoder of nnFormer.\nFormer. Firstly, we put a light-weight convolutional embedding\nlayer ahead of transformer blocks. In comparison to directly\nÔ¨Çattening raw pixels and applying 1D pre-processing in [17],\nthe convolutional embedding layer encodes precise (i.e., pixel-\nlevel) spatial information and provides low-level yet high-\nresolution 3D features. After the embedding block, transformer\nand convolutional down-sampling blocks are interleaved to\nfully entangle long-term dependencies with high-level and\nhierarchical object concepts at various scales, which helps\nimprove the generalization ability and robustness of learned\nrepresentations.\nThe other contribution of nnFormer lies in proposing a\ncomputational-efÔ¨Åcient way to leverage inter-slice dependen-\ncies. To be speciÔ¨Åc, nnFormer proposes to jointly use Lo-\ncal V olume-based Multi-head Self-attention (LV-MSA) and\nGlobal V olume-based Multi-head Self-attention (GV-MSA) to\nconstruct feature pyramids and provide sufÔ¨Åcient receptive\nÔ¨Åeld for learning representations on both local and global\n3D volumes, which are then aggregated to make predictions.\nCompared to the naive multi-head self-attention (MSA) [1],\nthe proposed strategy can greatly reduce the computational\ncomplexity while producing competitive segmentation perfor-\nmance. Moreover, inspired by the attention mechanism used in\nthe task of machine translation [1], we introduce skip attention\nto replace the atypical concatenation/summation operation in\nskip connections of U-Net like architecture, which further\nimproves the segmentation results.\nTo sum up, our contributions can be summarized as follows:\n‚Ä¢ We introduce nnFormer, a 3D transformer for volumetric\nmedical image segmentation. nnFormer achieves sig-\nniÔ¨Åcant improvements over previous transformer-based\nmedical segmentation models on three well-established\ndatasets.\n‚Ä¢ Technically, the contributions of nnFormer are three folds:\ni) an interleaved combination of convolution and self-\nattention operations. ii) the utilization of both local and\nglobal volume-based self-attention to build feature pyra-\nmids and provide large receptive Ô¨Åelds, respectively. iii)\nskip attention is proposed to replace traditional concate-\nnation/summation operations in skip connections.\n‚Ä¢ Thorough experiments have been conducted to validate\nthe advantages of nnFormer over nnUNet. We show that\nnnFormer is signiÔ¨Åcantly better than nnUNet in haus-\ndorff distance and achieves slightly better performance\nin dice coefÔ¨Åcient. Moreover, we found that nnFormer\nand nnUNet are highly complementary to each other as\nsimply averaging their predictions can already greatly\nboost the overall performance.\nII. R ELATED WORK\nIn this section, we mainly review methodologies that resort\nto transformers to improve segmentation results of medical\nimages. Since most of them employ hybrid architecture of\nconvolution and self-attention [1], we divide them into two\ncategories based on whether the majority of the stem is\nconvolutional or transformer-based.\nConvolution-based stem. TransUNet [11] Ô¨Årst time applied\ntransformer to improve the segmentation results of medical\nimages. TransUNet treats the convnet as a feature extractor to\ngenerate a feature map for the input slice. Patch embedding\nis then applied to patches of feature maps in the bottleneck\ninstead of raw images in ViT [2]. Concurrently, similar to\nTransUNet, Li et al. [20] proposed to use a squeezed attention\nblock to regularize the self-attention modules of transformers\nand an expansion block to learn diversiÔ¨Åed representations for\nfundus images, which are all implemented in the bottleneck\nwithin convnets. TransFuse [13] introduced a BiFusion\nmodule to fuse features from the shallow convnet-based\nencoder and transformer-based segmentation network to make\nÔ¨Ånal predictions on 2D images. Compared to TransUNet,\nTransFuse mainly applied the self-attention mechanism to the\ninput embedding layer to improve segmentation models on 2D\nimages. Yun et al. [21] employed transformers to incorporate\nspectral information, which are entangled with spectral\ninformation encoded by convolutional features to address the\nproblem of hyperspectral pathology. Xu et al. [22] extensively\nstudied the trade-off between transformers and convnets\nand proposed a more efÔ¨Åcient encoder named LeViT-UNet.\nLi et al. [23] presented a new up-sampling approach and\nincorporated it into the decoder of UNet to model long-term\ndependencies and global information for better reconstruction\nresults. TransClaw U-Net [15] utilized transformers in UNet\nwith more convolutional feature pyramids. TransAttUNet\n[16] explored the feasibility of applying transformer self\nattention with convolutional global spatial attention. Xie et al.\n[24] adopted transformers to capture long-term dependencies\nof multi-scale convolutional features from different layers\nof convnets. TransBTS [25] Ô¨Årst utilized 3D convnets to\nextract volumetric spatial features and down-sample the\ninput 3D images to produce hierarchical representations. The\noutputs of the encoder in TransBTS are then reshaped into\na vector (i.e. token) and fed into transformers for global\nfeature modeling, after which an ordinary convolutional\ndecoder is appended to up-sample feature maps for the\ngoal of reconstruction. Different from these approaches that\ndirectly employ convnets as feature extractors, our nnFormer\nfunctionally relies on convolutional and transformer-based\nblocks, which are interleaved to take advantages of each other.\nTransformer-based stem. Valanarasu et al. [14] proposed a\ngated axial-attention model (i.e., MedT) which extends the\nexisting convnet architecture by introducing an summational\ncontrol mechanism in the self-attention. Karimi et al. [17]\nremoved the convolutional operations and built a 3D seg-\nmentation model based on transformers. The main idea is to\nAUTHOR zhou et al.: NNFORMER: VOLUMETRIC MEDICAL IMAGE SEGMENTATION VIA A 3D TRANSFORMER 3\nÔ¨Årst split the local volume block into 3D patches, which are\nthen Ô¨Çattened and embedded to 1D sequences and passed to a\nViT-like backbone to extract representations. SwinUNet [18]\nbuilt a U-shape transformer-based segmentation model on top\nof transformer blocks in [3], where observable improvements\nwere achieved. DS-TransUNet [19] further extended Swin-\nUNet by adding one more encoder to handle multi-scale inputs\nand introduced a fusion module to effectively establish global\ndependencies between features of different scales through\nthe self-attention mechanism. Compared to these transformer-\nbased stems, nnFormer inherits the superiority of convolution\nin encoding precise spatial information and producing hier-\narchical representations that help model object concepts at\nvarious scales.\nIII. M ETHOD\nA. Overview\nThe overall architecture of nnFormer is presented in Figure\n2, which maintains a similar U shape as that of U-Net [12]\nand mainly consists of three parts, i.e., the encoder, bottleneck\nand decoder. Concretely, the encoder involves one embedding\nlayer, two local transformer blocks (each block contains two\nsuccessive layers) and two down-sampling layers. Symmet-\nrically, the decoder branch includes two transformer blocks,\ntwo up-sampling layers and the last patch expanding layer for\nmaking mask predictions. Besides, the bottleneck comprises\none down-sampling layer, one up-sampling layer and three\nglobal transformer blocks for providing large receptive Ô¨Åeld\nto support the decoder. Inspired by U-Net [12], we add skip\nconnections between corresponding feature pyramids of the\nencoder and decoder in a symmetrical manner, which helps to\nrecover Ô¨Åne-grained details in the prediction. However, differ-\nent from atypical skip connections that often use summation or\nconcatenation operation, we introduce skip attention to bridge\nthe gap between the encoder and decoder.\nIn the following, we will demonstrate the forward procedure\non Synapse. The forward pass on different datasets can be\neasily inferred based on the procedure on Synapse.\nB. Encoder\nThe input of nnFormer is a 3D patch X ‚àà RH√óW√óD\n(usually randomly cropped from the original image), where\nH, W and D denote the height, width and depth of each\ninput scan, respectively.\nThe embedding layer. On Synapse, the embedding block\nis responsible for transforming each input scan X into\na high-dimensional tensor Xe ‚àà R\nH\n4 √óW\n4 √óD\n2 √óC, where\nH\n4 √óW\n4 √óD\n2 represents the number of the patch tokens and\nC represents the sequence length (these numbers may slightly\nvary on different datasets). Different from ViT [2] and Swin\nTransformer [3] that use large convolutional kernels in the\nembedding block to extract features, we found that applying\nsuccessive convolutional layers with small convolutional\nkernels bring more beneÔ¨Åts in the initial stage, which could\nbe explained from two perspectives, i.e., i) why applying\nsuccessive convolutional layers and ii) why using small-sized\nkernels. For i), we use convolutional layers in the embedding\nblock because they encode pixel-level spatial information,\nmore precisely than patch-wise positional encoding used in\ntransformers. For ii), compared to large-sized kernels, small\nkernel sizes help reduce computational complexity while\nproviding equal-sized receptive Ô¨Åeld. As shown in Figure 2b,\nthe embedding block consists of four convolutional layers\nwhose kernel size is 3. After each convolutional layer (except\nthe last one), one GELU [26] and one layer normalization\n[27] layers are appended. In practice, depending on the size\nof input patch, strides of convolution in the embedding block\nmay accordingly vary.\nLocal Volume-based Multi-head Self-attention (LV-MSA).\nAfter the embedding layer, we pass the high-dimensional\ntensor Xe to transformer blocks. The main point behind is\nto fully entangle the captured long-term dependencies with\nthe hierarchical object concepts at various scales produced\nby the down-sampling layers and the high-resolution spatial\ninformation encoded by the initial embedding layer. Compared\nto Swin Transformer [3], we compute self-attention within 3D\nlocal volumes (i.e., LV-MSA, Local V olume-based Multi-head\nSelf-attention) instead of 2D local windows.\nSuppose that XLV ‚ààRL√óC represents the input of the local\ntransformer block, XLV would be Ô¨Årst reshaped to ÀÜXLV ‚àà\nRNLV√óNT √óC, where NLV is a pre-deÔ¨Åned number of 3D local\nvolumes and NT = SH √óSW √óSD denotes the number of\npatch tokens in each volume. {SH, SW, SD}stand for the\nsize of local volume.\nAs shown in Figure 3a, we follow [3] to conduct two\nsuccessive transformer layers in each block, where the second\nlayer can be regarded as a shifted version of the Ô¨Årst layer (i.e.,\nSLV-MSA). The main difference lies in that our computation is\nbuilt on top of 3D local volumes instead of 2D local windows.\nThe computational procedure can be summarized as follows:\nÀÜXl\nLV = LV-MSA\n(\nNorm\n(\nXl‚àí1\nLV\n))\n+ Xl‚àí1\nLV ,\nXl\nLV = MLP\n(\nNorm\n(\nÀÜXl\nLV\n))\n+ ÀÜXl\nLV,\nÀÜXl+1\nLV = SLV-MSA\n(\nNorm\n(\nXl\nLV\n))\n+ Xl\nLV,\nXl+1\nLV = MLP\n(\nNorm\n(\nÀÜXl+1\nLV\n))\n+ ÀÜXl+1\nLV .\n(1)\nHere, l stands for the layer index. MLP is an abbreviation\nfor multi-layer perceptron. The computational complexity of\nLV-MSA on a volume of h√ów√ód patches is:\n‚Ñ¶(LV-MSA) = 4hwdC2 + 2SHSWSDhwdC. (2)\nSLV-MSA displaces the 3D local volume used in LV-MSA\nby\n(‚åäSH\n2\n‚åã\n,\n‚åäSW\n2\n‚åã\n,\n‚åäSD\n2\n‚åã)\nto introduce more interactions\nbetween different local volumes. In practice, SLV-MSA has\nthe similar computational complexity as that of LV-MSA.\nThe query-key-value (QKV) attention [1] in each 3D local\nvolume can be computed as follows:\nAttention(Q,K,V ) = softmax\n(QKT\n‚àödk\n+ B\n)\nV, (3)\nwhere Q,K,V ‚ààRNT √ódk denote the query, key and value\nmatrices. B ‚àà RNT is the relative position encoding. In\n4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2017\n3DScans\nEmbeddingLayer\nLocalSelf-attentionLayer(√ó2)\nDown-sampling\nLocalSelf-attentionLayer(√ó2)\nDown-sampling\nGlobalSelf-attentionLayer(√ó2)\nDown-sampling\nGlobalSelf-attentionLayer(√ó2)\nUp-sampling\nGlobalSelf-attentionLayer(√ó2)\nUp-sampling\nLocalSelf-attentionLayer(√ó2)\nUp-sampling\nLocalSelf-attentionLayer(√ó2)\nExpandingLayer\n3DMasks\nS\nS\nS\nQKVInputs\nSSkipAttention\na. b.\nc. d. e.\nK=3S=(2,2,2)\nK=3S=(1,1,1)\nK=3S=(2,2,1)GELU+NormK=3S=(1,1,1)\nGELU+Norm\nSynapseData\nGELU+NormK=3S=(2,2,1)\nK=3S=(1,1,1)\nK=3S=(2,2,1)GELU+NormK=3S=(1,1,1)\nGELU+Norm\nACDCData\nGELU+NormK=3S=(2,2,2)\nK=3S=(1,1,1)\nK=3S=(2,2,2)GELU+NormK=3S=(1,1,1)\nGELU+Norm\nTumorData\nGELU+Norm\nK=(3,3,3)S=(2,2,2)\nGELU+NormDK=(2,2,2)S=(2,2,2)Norm\nDK=(4,4,2)S=(4,4,2)\nEncoder\nDecoder\nBottleneck\nFig. 2: Architecture of nnFormer. In (a), we show the overall architecture of nnFormer. In (b), we present more details of\nthe embedding layers on three publicly available datasets. In (c), (d), (e), we display how to implement the down-sampling,\nup-sampling and expanding layers, respectively. In practice, the architecture may slightly vary depending on the input scan\nsize. In (b)-(e), K denotes the convolutional kernel size, DK stands for the deconvolutional kernel size and S represents the\nstride. Norm refers to the layer normalization strategy.\nNorm\nNormMLP\nLV-MSAùëÑùêæùëâ Norm\nNormMLP\nSLV-MSAùëÑùêæùëâ\n(a) LV-MSA\nNorm\nNormMLP\nGV-MSAùëÑùêæùëâ Norm\nNormMLP\nGV-MSAùëÑùêæùëâ\n (b) GV-MSA\nLinearProjection\nùêæ!‚àóùëâ!‚àó\nNorm\nGV/LV-MSA\nùëÑ!‚àó\nNormMLP\n (c) Skip Attention\nFig. 3: Three types of attention mechanism in nnFormer.Norm denotes the layer normalization method. MLP is the abbreviation\nfor multi-layer perceptron, which is a two-layer neural network in practice.\npractice, we Ô¨Årst initialize a smaller-sized position matrix\nÀÜB ‚àà R(2SH‚àí1)√ó(2SW ‚àí1)√ó(2SD‚àí1) and take corresponding\nvalues from ÀÜB to build a larger position matrix B.\nThe down-sampling layer. We found that by replacing the\npatch merging operation in [3] with straightforward strided\nconvolution, nnFormer can provide more improvements on\nvolumetric image segmentation. The intuition behind is that\nAUTHOR zhou et al.: NNFORMER: VOLUMETRIC MEDICAL IMAGE SEGMENTATION VIA A 3D TRANSFORMER 5\nnnFormer nnUNet\nSpacing [1.0, 1.0, 1.0] [1 .0, 1.0, 1.0]\nMedian shape 138 √ó170 √ó138 138 √ó170 √ó138\nCrop size 128 √ó128 √ó128 128 √ó128 √ó128\nBatch size 2 2\nDS Str. [2,2,2],[2,2,2],[2,2,2],\n[2,2,2],[2,2,2]\n[2,2,2],[2,2,2],[2,2,2],\n[2,2,2],[2,2,2]\n(a) Tumor\nnnFormer nnUNet\nSpacing [0.76, 0.76, 3] [0 .76, 0.76, 3]\nMedian shape 512 √ó512 √ó148 512 √ó512 √ó148\nCrop size 128 √ó128 √ó64 192 √ó192 √ó48\nBatch size 2 2\nDS Str. [2,2,2],[2,2,1],[2,2,2],\n[2,2,2],[2,2,2]\n[2,2,1],[2,2,2],[2,2,2],\n[2,2,2],[2,2,1]\n(b) Synapse\nnnFormer nnUNet\nSpacing [1.52, 1.52, 6.35] [1 .52, 1.52, 6.35]\nMedian shape 246 √ó213 √ó13 246 √ó213 √ó13\nCrop size 160 √ó160 √ó14 256 √ó224 √ó14\nBatch size 4 4\nDS Str. [2,2,1],[2,2,1],[2,2,1],\n[2,2,2],[2,2,2]\n[2,2,1],[2,2,1],[2,2,2],\n[2,2,1],[2,2,1]\n(c) ACDC\nTABLE I: Network conÔ¨Ågurations of our nnFormer and\nnnUNet on three public datasets. We only report the down-\nsampling stride (abbreviated as DS Str.) as the correspond-\ning up-sampling stride can be easily inferred according to\nsymmetrical down-sampling operations. Note that the network\nconÔ¨Åguration of nnUNet is automatically determined based on\npre-deÔ¨Åned hand-crafted rules (for self-adaptation).\nconvolutional down-sampling produces hierarchical represen-\ntations that help model object concepts at multiple scales. As\ndisplayed in Figure 2c, in most cases, the down-sampling layer\ninvolves a strided convolution operation where the stride is set\nto 2 in all dimensions. However, in practice, the stride with\nrespect to speciÔ¨Åc dimension can be set to 1 as the number\nof slices is limited in this dimension and over-down-sampling\n(i.e., using a large down-sampling stride) can be harmful.\nC. Bottleneck\nThe original vision transformer (i.e., ViT) [2] employs the\nnaive 2D multi-head self-attention mechanism. In this paper,\nwe extend it to a 3D version (as shown in Figure 3b), whose\ncomputational complexity can be formulated as follows:\n‚Ñ¶(GV-MSA) = 4hwdC2 + 2(hwd)2C. (4)\nCompared to (2), it is obvious that GV-MSA requires much\nmore computational resources when {h,w,d }are relatively\nlarger (e.g., an order of magnitude larger) than {SH,SW,SD}.\nIn fact, this is exactly the reason why we use local transformer\nblocks in the encoder, which are designed to handle large-sized\ninputs efÔ¨Åciently with the local self-attention mechanism.\nHowever, in the bottleneck, {h,w,d }already become much\nsmaller after several down-sampling layers, making the prod-\nuct of them, i.e. hwd, , have a similar size to that ofSHSWSD.\nThis creates the condition for applying GV-MSA, which is\nable to provide larger receptive Ô¨Åeld compared to LV-MSA\nand large receptive Ô¨Åeld has been proven to be beneÔ¨Åcial\nin different applications [28]‚Äì[31]. In practice, we use three\nglobal transformer blocks (i.e., six GV-MSA layers) in the\nbottleneck to provide sufÔ¨Åcient receptive Ô¨Åeld to the decoder.\nD. Decoder\nThe architecture of two transformer blocks in the decoder\nis highly symmetrical to those in the encoder. In contrast to\nthe down-sampling blocks, we employ strided deconvolution\nto up-sample low-resolution feature maps to high-resolution\nones, which in turn are merged with representations from\nthe encoder via skip attention to capture both semantic and\nÔ¨Åne-grained information. Similar to up-sampling blocks, the\nlast patch expanding block also takes the deconvolutional\noperation to produce Ô¨Ånal mask predictions.\nSkip Attention. Atypical skip connections in convnets [12, 32]\nadapt either concatenation or summation to incorporate more\ninformation. Inspired by the machine translation task in [1],\nwe propose to replace the concatenation/summation with an\nattention mechanism, which is named as Skip Attention in\nthis paper. To be speciÔ¨Åc, the output of the l-th transformer\nblock of the encoder, i.e., Xl\n{LV,GV}, is transformed and split\ninto a key matrix Kl‚àó\nand a value matrix Vl‚àó\nafter the linear\nprojection (i.e, a one-layer neural network):\nKl‚àó\n,V l‚àó\n= LP(Xl\n{LV,GV}), (5)\nwhere LP stands for the linear projection. Accordingly, Xl‚àó\nUP,\nthe output feature maps after the l‚àó-th up-sampling layer of\nthe decoder, is treated as the query Ql‚àó\n. Then, we can conduct\nLV/GV-MSA on Ql‚àó\n, Kl‚àó\nand Vl‚àó\nin the decoder like what\nwe have done in (3), i.e.,\nAttention(Ql‚àó\n, Kl‚àó\n, Vl‚àó\n) =softmax\nÔ£´\nÔ£≠Ql‚àó\n(Kl‚àó\n)T\n‚àö\ndl‚àó\nk\n+ Bl‚àó\nÔ£∂\nÔ£∏V l‚àó\n,\n(6)\nwhere l‚àó denotes the layer index. dl‚àó\nk and Bl‚àó\nhave the same\nmeaning as those in (3), whose sizes can be easily inferred,\naccordingly.\nIV. EXPERIMENTS\nFor thoroughly comparing nnFormer to previous convnet-\nand transformer-based architecture, we conduct experiments\non three datasets/tasks: the brain tumor segmentation task in\nMedical Segmentation Decathlon (MSD) [36], Synapse multi-\norgan segmentation [37] and Automatic Cardiac Diagnosis\nChallenge (ACDC) [38]. For each experiment, we repeat\nit for ten times and report their average results. We also\ncalculate p-values to demonstrate the signiÔ¨Åcance of nnFormer.\nBrain tumor segmentation using MRI scans. This task\nconsists of 484 MRI images, each of which includes four\nchannels, i.e., FLAIR, T1w, T1gd and T2w. The data was\nacquired from 19 different institutions and contained a\nsubset of the data used in the 2016 and 2017 Brain Tumor\nSegmentation (BraTS) challenges [39]. The corresponding\n6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2017\nMethods Average WT ET TC\nHD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë\nSETR NUP [33] 13.78 63.7 14.419 69.7 11.72 54.4 15.19 66.9\nSETR PUP [33] 14.01 63.8 15.245 69.6 11.76 54.9 15.023 67.0\nSETR MLA [33] 13.49 63.9 15.503 69.8 10.24 55.4 14.72 66.5\nTransUNet [11] 12.98 64.4 14.03 70.6 10.42 54.2 14.5 68.4\nTransBTS [25] 9.65 69.6 10.03 77.9 9.97 57.4 8.95 73.5\nCoTr w/o CNN encoder [24] 11.22 64.4 11.49 71.2 9.59 52.3 12.58 69.8\nCoTr [24] 9.70 68.3 9.20 74.6 9.45 55.7 10.45 74.8\nUNETR [34] 8.82 71.1 8.27 78.9 9.35 58.5 8.85 76.1\nOur nnFormer 4.05 86.4 3.80 91.3 3.87 81.8 4.49 86.0\nP-values < 1e-2 (HD95), < 1e-2 (DSC)\nTABLE II: Comparison with transformer-based models on brain tumor segmentation. The evaluation metrics are HD95 (mm)\nand DSC in (%). Best results are bolded while second best are underlined. Experimental results of baselines are from [34]. We\ncalculate the p-values between the average performance of our nnFormer and the best performing baseline in both metrics.\nMethods Average Aotra Gallbladder Kidney (Left) Kidney (Right) Liver Pancreas Spleen StomachHD95 ‚Üì DSC ‚Üë\nViT [2] + CUP [11] 36.11 67.86 70.19 45.10 74.70 67.40 91.32 42.00 81.75 70.44\nR50-ViT [2] + CUP [11] 32.87 71.29 73.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95\nTransUNet [11] 31.69 77.48 87.23 63.16 81.87 77.02 94.08 55.86 85.08 75.62\nTransUNet‚ñΩ[11] - 84.36 90.68 71.99 86.04 83.71 95.54 73.96 88.80 84.20\nSwinUNet [18] 21.55 79.13 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\nTransClaw U-Net [15] 26.38 78.09 85.87 61.38 84.83 79.36 94.28 57.65 87.74 73.55\nLeVit-UNet-384s [22] 16.84 78.53 87.33 62.23 84.61 80.25 93.11 59.07 88.86 72.76\nMISSFormer [35] 18.20 81.96 86.99 68.65 85.21 82.00 94.41 65.67 91.92 80.81\nUNETR [34] 22.97 79.56 89.99 60.56 85.66 84.80 94.46 59.25 87.81 73.99\nOur nnFormer 10.63 86.57 92.04 70.17 86.57 86.25 96.84 83.35 90.51 86.83\nP-values < 1e-2 (HD95), < 1e-2 (DSC)\nTABLE III: Comparison with transformer-based models on multi-organ segmentation (Synapse). The evaluation metrics are\nHD95 (mm) and DSC in (%). Best results are bolded while second best are underlined. ‚ñΩdenotes TransUNet uses larger\ninputs, whose size is 512 √ó512. The p-values are calculated based on the average performance of our nnFormer and the best\nperforming baseline in both metrics.\nMethods Average RV Myo LV\nVIT-CUP [2] 81.45 81.46 70.71 92.18\nR50-VIT-CUP [2] 87.57 86.07 81.88 94.75\nTransUNet [11] 89.71 88.86 84.54 95.73\nSwinUNet [18] 90.00 88.55 85.62 95.83\nLeViT-UNet-384s [22] 90.32 89.55 87.64 93.76\nUNETR [34] 88.61 85.29 86.52 94.02\nnnFormer 92.06 90.94 89.58 95.65\nP-value < 1e-2 (DSC)\nTABLE IV: Comparison with transformer-based models on\nautomatic cardiac diagnosis (ACDC). The evaluation metric\nis DSC (%). Best results are bolded while second best are\nunderlined. The default evaluation metric is DSC, based on\nwhich we calculate the p-value.\ntarget ROIs were the three tumor sub-regions, namely edema\n(ED), enhancing tumor (ET), and non-enhancing tumor\n(NET). To be consistent with those results reported in\nUNETR [34], we display the experimental results of the\nwhole tumor (WT), enhancing tumor (ET) and tumor core\n(TC) when comparing our nnFormer with transformer-based\nmodels. For the split of data, we follow the instruction of\nUNETR, where ratios of training/validation/test sets are 80%,\n15% and 5%, respectively. As above, we use both HD95 and\nDice score as evaluation metrics.\nSynapse for multi-organ CT segmentation. This dataset\nincludes 30 cases of abdominal CT scans. Following the split\nused in [11], 18 cases are extracted to build the training\nset while the rest 12 cases are used for testing. We report\nthe model performance evaluated with the 95% Hausdorff\nDistance (HD95) and Dice score (DSC) on 8 abdominal\norgans, which are aorta, gallbladder, spleen, left kidney, right\nkidney, liver, pancreas and stomach 1.\nACDC for automated cardiac diagnosis. ACDC involves\n100 patients, with the cavity of the right ventricle, the\nmyocardium of the left ventricle and the cavity of the left\nventricle to be segmented. Each case‚Äôs labels involve left\nventricle (LV), right ventricle (RV) and myocardium (MYO).\nThe dataset is split into 70 training samples, 10 validation\nsamples and 20 test samples. The evaluation metrics include\n1Here, we follow the evaluation setting of TransUNet.\nAUTHOR zhou et al.: NNFORMER: VOLUMETRIC MEDICAL IMAGE SEGMENTATION VIA A 3D TRANSFORMER 7\nboth HD95 and Dice score 2.\nA. Implementation details\nWe run all experiments based on Python 3.6, PyTorch 1.8.1\nand Ubuntu 18.04. All training procedures have been per-\nformed on a single NVIDIA 2080 GPU with 11GB memory.\nThe initial learning rate is set to 0.01 and we employ a\n‚Äúpoly‚Äù decay strategy as described in Equation 7. The default\noptimizer is SGD where we set the momentum to 0.99. The\nweight decay is set to 3e-5. We utilize both cross entropy loss\nand dice loss by simply summing them up. The number of\ntraining epochs (i.e., max epoch in Equation 7) is 1000 and\none epoch contains 250 iterations. The number of heads of\nmulti-head self-attention used in different encoder stages is [6,\n12, 24, 48] on Synapse. In the rest two datasets, the number\nof heads becomes [3, 6, 12, 24].\nlr = initial lr √ó(1 ‚àí epoch id\nmax epoch)0.9. (7)\nPre-processing and augmentation strategies. All images will\nbe Ô¨Årst resampled to the same target spacing. Augmentations\nsuch as rotation, scaling, gaussian noise, gaussian blur,\nbrightness and contrast adjust, simulation of low resolution,\ngamma augmentation and mirroring are applied in the given\norder during the training process.\nDeep supervision. We also add deep supervision during the\ntraining stage. SpeciÔ¨Åcally, the output of each stage in the\ndecoder is passed to the Ô¨Ånal expanding block, where cross\nentropy loss and dice loss would be applied. In practice,\ngiven the prediction of one typical stage, we down-sample\nthe ground truth segmentation mask to match the prediction‚Äôs\nresolution. Thus, the Ô¨Ånal training objective function is the\nsum of all losses at three resolutions:\nLall = Œ±1L{H, W, D}+ Œ±2L{H\n4 , W\n4 , D\n2 }+ Œ±3L{H\n8 , W\n8 , D\n4 }.\n(8)\nHere, Œ±{1, 2, 3} denote the magnitude factors for losses in\ndifferent resolutions. In practice, Œ±{1, 2, 3} halve with each\ndecrease in resolution, leading to Œ±2 = Œ±1\n2 and Œ±3 = Œ±1\n4 .\nFinally, all weight factors are normalized to 1.\nNetwork conÔ¨Ågurations. In Table I, we display network\nconÔ¨Ågurations of experiments on all three datasets. Compared\nto nnUNet, in nnFormer, better segmentation results can be\nachieved with smaller-sized input patches.\nB. Comparison with transformer-based methodologies\nBrain tumor segmentation. Table II presents experimental\nresults of all models on the task of brain tumor segmentation.\nOur nnFormer achieves the lowest HD95 and the highest DSC\nscores in all classes. Moreover, nnFormer is able to surpass\nthe second best method, i.e., UNETR, by large margins in\n2Similar to Synapse, we also follow the evaluation setting of TransUNet.\nEDNETET\nGt\nOurs\nnnUNet\nUNETR\n(a) Brain tumor segmentation\nGt\nOurs\nnnUNet\nUNETR\nSpleenPancreasKidney(right)Kidney(left)GallbladderLiverStomachAorta\n(b) Multi-organ segmentation (Synapse)\nGt\nOurs\nnnUNet\nUNETR\nRV MyoLV\n(c) Automatic cardiac diagnosis (ACDC)\nFig. 4: Visualization of segmentation results on three well-\nestablished datasets. We mainly compare nnFormer against\nnnUNet and UNETR. In addition to segmentation results, we\nalso provide ground truth masks for better comparison.\nboth evaluation metrics. For instance, nnFormer outperforms\nUNETR by over 4.5 mm in average HD95 and nearly 10\npercents in DSC of each class. In comparison to previous\ntransformer-based methods, nnFormer shows more strength\nin HD95 than in DSC.\nMulti-organ segmentation (Synapse). As shown in Table\nIII, we make experiments on Synapse and to compare our\nnnFormer against a variety of transformer-based approaches.\nAs we can see, the best performing methods are LeViT-UNet-\n384s [22] and TransUNet [11]. LeViT-UNet-384s achieves\nan average HD95 of 16.84 mm while TransUNet produces\n8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2017\nMethods Average WT ET TC ED NET\nHD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë\nnnUNet [40] 4.60 81.87 3.64 91.99 4.06 80.97 4.91 85.35 4.26 84.39 6.14 66.65\nOur nnFormer 4.42 82.02 3.80 91.26 3.87 81.80 4.49 86.02 4.17 83.76 5.76 67.29\nP-values < 1e-2 (HD95), 8.8e-2 (DSC)\nnnAvg 4.09 82.65 3.43 92.33 3.69 82.26 4.17 86.14 3.92 84.95 5.23 67.55\n(a) Brain tumor segmentation\nMethods Average Aotra Gallbladder Kidney (Left) Kidney (Right) Liver Pancreas Spleen Stomach\nHD95 ‚ÜìDSC ‚ÜëHD95 ‚ÜìDSC ‚ÜëHD95 ‚ÜìDSC ‚ÜëHD95 ‚ÜìDSC ‚ÜëHD95 ‚ÜìDSC ‚ÜëHD95 ‚ÜìDSC ‚ÜëHD95 ‚ÜìDSC ‚ÜëHD95 ‚ÜìDSC ‚ÜëHD95 ‚ÜìDSC ‚Üë\nnnUNet [40] 10.78 86.99 5.91 93.01 15.19 71.77 18.60 85.57 6.44 88.18 1.62 97.23 4.52 83.01 24.34 91.86 9.58 85.26\nOur nnFormer 10.63 86.57 11.38 92.04 11.55 70.17 18.09 86.57 12.76 86.25 2.00 96.84 3.72 83.35 16.92 90.51 8.58 86.83\nP-values 2e-2 (HD95), 7.7e-2 (DSC)\nnnAvg 7.70 87.51 5.90 93.11 8.63 72.08 18.42 86.20 8.56 87.76 1.63 97.20 3.64 84.21 9.42 91.94 5.41 87.60\n(b) Multi-organ segmentation (Synapse)\nMethods Average RV Myo LV\nHD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë HD95 ‚Üì DSC ‚Üë\nnnUNet [40] 1.15 91.61 1.31 90.24 1.06 89.24 1.09 95.36\nOur nnFormer 1.12 92.06 1.23 90.94 1.04 89.58 1.09 95.65\nP-values 2e-2 (HD95), < 1e-2 (DSC)\nnnAvg 1.10 92.15 1.19 91.03 1.04 89.75 1.06 95.68\n(c) Automated cardiac diagnosis (ACDC)\nTABLE V: Comparison with nnUNet on three public datasets. nnAvg means that we simply average the predictions of nnUNet\nand nnFormer. Color green denotes the target result of nnAvg is the best among all three approaches. Besides, we also highlight\nthe best results between nnUNet and nnFormer in bold font. We calculate p-values between the average performance of nnUNet\nand our nnFormer in both metrics on three public datasets.\n# Models Average RV Myo LV\n0 1 √óLV-MSA + PM [3] + PE [3] 90.55 88.59 88.47 94.60\n1 1 √óLV-MSA + PM [3] + Conv. Embed. 90.97 88.94 88.84 95.13\n2 1 √óLV-MSA + Conv. Down. + Conv. Embed. 91.26 89.70 89.04 95.04\n3 1 √óLV-MSA + 1 √óGV-MSA + Conv. Down. + Conv. Embed. 91.46 89.82 89.17 95.39\n4 1 √óLV-MSA + 1 √óGV-MSA + Conv. Down. + Conv. Embed. + Skip Att. 91.85 90.41 89.50 95.63\n5 1 √óLV-MSA + 1 √óSLV-MSA + 2 √óGV-MSA + Conv. Down. + Conv. Embed. + Skip Att. 92.06 90.94 89.58 95.65\nTABLE VI: Investigation of the impact of different modules used in nnFormer. PM and PE denote the patch merging and patch\nembedding strategies used in swin transformer [3]. Conv. Embed. and Conv. Down. represent our convolutional embedding\nand down-sampling layers, respectively. Skip Att. refers to the proposed skip attention mechanism. 1 √óLV-MSA in lines 0-2\nmeans that each transformer block contains one transformer layer and each layer consists of one LV-MSA. 1 √óGV-MSA in lines\n3-4 denotes that we replace LV-MSA in the bottleneck with GV-MSA. 1 √óSLV-MSA and 2√óGV-MSA in line 5 mean that we\nincrease the number of transformer layers in each transformer block from one to two. To be speciÔ¨Åc, in the encoder/decoder,\neach transformer block contains 1 √óLV-MSA and 1√óSLV-MSA while in the bottleneck, there are 2 √óGV-MSA in each block.\nan average DSC of 84.36%. In comparison, our nnFormer\nis able to outperform LeViT-UNet-384s and TransUNet by\nover 6 mm and 2 percents in average HD95 and DSC,\nrespectively, which are quite impressive improvements on\nSynapse. To be speciÔ¨Åc, nnFormer achieves the highest\nDSC in six organs, including aotra, kidney (left), kidney\n(right), liver, pancreas and stomach. Compared to previous\ntransformer-based methods, nnFormer is more advantageous\nin segmentation pancreas and stomach, both of which are\ndifÔ¨Åcult to delineate using past segmentation models.\nAutomated cardiac diagnosis (ACDC). Table IV displays\nexperimental results on ACDC. We can see that the best\ntransformer-based model is LeViT-UNet-384s, whose average\nDSC is slightly higher than SwinUNet while TransUNet\nand SwinUNet are more capable of handling the delineation\nof the left ventricle (LV). In contrast, nnFormer surpasses\nLeViT-UNet-384s in all classes and by nearly 1.7 percents in\naverage DSC, which again veriÔ¨Åes its advantages over past\ntransformer-based approaches.\nStatistical signiÔ¨Åcance. In Table II, III and IV, we employ\nindependent two-sample t-test to calculate p-values between\nthe average performance of our nnFormer and the best per-\nforming baseline in both HD95 and DSC. The null hypothesis\nis that our nnFormer has no advantage over the best performing\nbaseline. As we can see, on all three public datasets, nnFormer\nproduces p-values smaller than 1e-2 under both HD95 and\nDSC, which indicate strong evidence against the null hypoth-\nesis. Thus, nnFormer shows signiÔ¨Åcant improvements over\nprevious transformer-based methods on three different tasks.\nAUTHOR zhou et al.: NNFORMER: VOLUMETRIC MEDICAL IMAGE SEGMENTATION VIA A 3D TRANSFORMER 9\nC. Comparison with nnUNet and Discussion\nIn this section, we compare nnFormer with nnUNet, which\nhas been recognized as one of the most powerful 3D medical\nimage segmentation models [40].\nResults. In Table V, we display the class-speciÔ¨Åc results in\nboth HD95 and DSC metrics to make a thorough comparison.\nTo be speciÔ¨Åc, from the perspective of the class-speciÔ¨Åc\nHD95 results, nnFormer outperforms nnUNet in 11 out of 16\ncategories. In the class-speciÔ¨Åc DSC, nnFormer outperforms\nnnUNet in 9 out of 16 categories. Thus, it seems that\nnnFormer is more advantageous under HD95, which means\nnnFormer may better delineate the object boundary. From the\nview of the average performance, we can see that nnFormer\noften achieves better average performance. For example,\nnnFormer outperforms nnUNet on all three public datasets\nwith lower HD95 results, while performing better than\nnnUNet on two out of three datasets with higher DSC results.\nStatistical signiÔ¨Åcance. To further verify the signiÔ¨Åcance\nof nnFormer over nnUNet, we also calculate the p-values\nbetween the average performance of nnFormer and nnUNet.\nSimilar to what we have done in Table II, we provide two\np-values based on HD95 and DSC on three public datasets,\nrespectively. The most obvious observation is that nnFormer\nachieves p-values smaller than 0.05 in HD95 on three public\ndatasets. These results suggest that nnFormer is the Ô¨Årst\nchoice when HD95 is treated as the primary evaluation\nmetric. Besides, the p-values based on DSC on tumor and\nmulti-organ segmentation ( > 0.05) imply that nnFormer is\na model comparable to nnUNet, while the results on ACDC\ndemonstrate the signiÔ¨Åcance of nnFormer. In conclusion,\nnnFormer has slight advantages over nnUNet under DSC.\nModel ensembling. Besides single model performance, we\nalso investigate the diversity between nnFormer and nnUNet,\nwhich is a crucial factor in model ensembling. Somewhat sur-\nprisingly, we found that by simply averaging the predictions of\nnnFormer and nnUNet (i.e., nnAvg in Table V), it can already\nboost the overall performance by large margins. For instance,\nnnAvg achieves the best results in all classes under HD95 and\nDSC on tumor segmentation. Moreover, nnAvg brings nearly\n30% improvements on Synapse when the evaluation metric is\nHD95. These results indicate that nnFormer and nnUNet are\nhighly complementary to each other.\nD. Ablation study\nTable VI displays our ablation study results towards differ-\nent modules in nnFormer. For simplicity, we made experiments\non ACDC and used DSC as the default evaluation metric.\nThe most basic baseline in Table VI (line 0) consists of\nLV-MSA (but without SLV-MSA), the patch merging and\nembedding layers used in [3]. We can see that such com-\nbination can already achieve a higher average DSC than\nLeViT-UNet-38 [22], which is the best performing baseline in\nTable IV. We Ô¨Årstly replaced the patch embedding layer, which\nis implemented with large kernel size and convolutional stride,\nwith our proposed volume embedding layer, i.e., successive\nconvolutional layers with small kernel size and convolutional\nstride. We found that the introduced convolutional embedding\nlayer improves the average DSC by approximate 0.4 percents.\nNext, we removed the patch merging layer and added our\nconvolutional down-sampling layer. We found such simple\nreplacement can further boost the overall performance by 0.3\npercents. Then, we replaced LV-MSA in the bottleneck with\nGV-MSA, where we observed 0.2-percent improvements. This\nphenomenon indicates that providing sufÔ¨Åcient larger recep-\ntive Ô¨Åeld can be beneÔ¨Åcial to the segmentation task. After-\nwards, we use skip attention to replace traditional concatena-\ntion/summation operations. Somewhat surprisingly, we found\nthat the skip attention is able to boost the overall performance\nby 0.4 percents, which demonstrates that the skip attention\nmay serve as an alternative choice other than traditional skip\nconnections. Last but not the least, we investigate adding more\ntransformer layers to each transformer block by cascading\nan SLV-MSA layer with every LV-MSA layer as in Swin\nTransformer and doubling the number of global self-attention\nlayers. We found that introducing more transformer layers\ndoes bring more improvements to the overall performance as\nit entangles more long-range dependencies into the learned\nvolume representations.\nE. Visualization of segmentation results\nIn Figure 4, we visualize some segmentation results of\nour nnFormer, nnUNet and UNETR on three public datasets.\nCompared to UNETR, our nnFormer can greatly reduce the\nnumber of false positive predictions. One typical example is\nthe Ô¨Åfth example on ACDC. We can see that UNETR produces\na large number of wrong right ventricle pixels outside the my-\nocardium. In contrast, our nnFormer generates no prediction\nof right ventricle outside the myocardium, which demonstrates\nthat nnFormer is more discriminative than UNETR on ACDC.\nOn the other hand, we observe that nnUNet displays very\ncompetitive segmentation results, much better than UNETR\nin nearly all examples. However, we still Ô¨Ånd that nnFormer\nmaintains clear advantages over nnUNet, one of which is\nthat nnFormer is better at dealing with the boundary. In\nfact, this phenomenon has been reÔ¨Çected in Table VI, where\nnnFormer is signiÔ¨Åcantly better than nnUNet when HD95\nis the default evaluation metric. In Figure 4, we can also\nobserve some evidences. For instance, in the second example\non Synapse, nnFormer captures the shape of the left kidney\nand stomach better than nnUNet. Also, in the third example\non brain tumor segmentation, nnUNet misses a major part of\nthe non-enhancing tumor enclosed by the edema. These results\nverify that our nnFormer has the potential to be treated as an\nalternative to nnUNet.\nV. CONCLUSION\nIn this paper, we present a 3D transformer, nnFormer, for\nvolumetric image segmentation. nnFormer is constructed on\ntop of an interleaved stem of convolution and self-attention.\nConvolution helps encode precise spatial information and\n10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2017\nbuilds hierarchical object concepts. For self-attention, nn-\nFormer employs three types of attention mechanism to en-\ntangle long-range dependencies. SpeciÔ¨Åcally, local and global\nvolume-based self-attention focus on constructing feature\npyramids and providing large receptive Ô¨Åeld. Skip attention\nis responsible for bridging the gap between the encoder and\ndecoder. Experiments show that nnFormer maintains great\nadvantages over previous transformer-based models in both\nHD95 and DSC. Compared to nnUNet, nnFormer is signiÔ¨Å-\ncantly better in HD95 while producing comparable results in\nDSC. More importantly, we demonstrate that nnFormer and\nnnUNet can be beneÔ¨Åcial to each other in model ensembling,\nwhere the simple averaging operation can already produce\ngreat improvements.\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\net al. , ‚ÄúAttention is all you need,‚Äù in Advances in Neural Information\nProcessing Systems, pp. 5998‚Äì6008, 2017.\n[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, et al. , ‚ÄúAn image is worth 16x16 words: Transformers\nfor image recognition at scale,‚Äù arXiv preprint arXiv:2010.11929, 2020.\n[3] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, ‚ÄúSwin transformer: Hierarchical vision transformer using shifted\nwindows,‚Äù arXiv preprint arXiv:2103.14030 , 2021.\n[4] K. He, X. Chen, S. Xie, Y . Li, P. Doll ¬¥ar, and R. Girshick,\n‚ÄúMasked autoencoders are scalable vision learners,‚Äù arXiv preprint\narXiv:2111.06377, 2021.\n[5] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‚ÄúEnd-to-end object detection with transformers,‚Äù in\nEuropean Conference on Computer Vision, pp. 213‚Äì229, Springer, 2020.\n[6] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, ‚ÄúGradient-based learning\napplied to document recognition,‚Äù Proceedings of the IEEE , vol. 86,\nno. 11, pp. 2278‚Äì2324, 1998.\n[7] H.-Y . Zhou, C. Lu, S. Yang, and Y . Yu, ‚ÄúConvNets vs. Transformers:\nWhose visual representations are more transferable?,‚Äù ICCV Workshop\non Deep Multi-Task Learning in Computer Vision , 2021.\n[8] T. Qu, X. Wang, C. Fang, L. Mao, J. Li, P. Li, J. Qu, X. Li, H. Xue,\nY . Yu,et al. , ‚ÄúM3net: A multi-scale multi-view framework for multi-\nphase pancreas segmentation based on cross-phase non-local attention,‚Äù\nMedical Image Analysis , vol. 75, p. 102232, 2022.\n[9] D. Zhang, G. Huang, Q. Zhang, J. Han, J. Han, and Y . Yu, ‚ÄúCross-\nmodality deep feature learning for brain tumor segmentation,‚Äù Pattern\nRecognition, vol. 110, p. 107562, 2021.\n[10] S. Tuli, I. Dasgupta, E. Grant, and T. L. GrifÔ¨Åths, ‚ÄúAre convolutional\nneural networks or transformers more like human vision?,‚Äù arXiv\npreprint arXiv:2105.07197, 2021.\n[11] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, et al., ‚ÄúTransUNet:\nTransformers make strong encoders for medical image segmentation,‚Äù\narXiv preprint arXiv:2102.04306 , 2021.\n[12] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-Net: Convolutional net-\nworks for biomedical image segmentation,‚Äù in International Confer-\nence on Medical image computing and computer-assisted intervention ,\npp. 234‚Äì241, Springer, 2015.\n[13] Y . Zhang, H. Liu, and Q. Hu, ‚ÄúTransFuse: Fusing transformers and\ncnns for medical image segmentation,‚ÄùarXiv preprint arXiv:2102.08005,\n2021.\n[14] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V . M. Patel, ‚ÄúMedical\nTransformer: Gated axial-attention for medical image segmentation,‚Äù\narXiv preprint arXiv:2102.10662 , 2021.\n[15] Y . Chang, H. Menghan, Z. Guangtao, and Z. Xiao-Ping, ‚ÄúTransClaw\nU-Net: Claw u-net with transformers for medical image segmentation,‚Äù\narXiv preprint arXiv:2107.05188 , 2021.\n[16] B. Chen, Y . Liu, Z. Zhang, G. Lu, and D. Zhang, ‚ÄúTransAttUnet:\nMulti-level attention-guided u-net with transformer for medical image\nsegmentation,‚Äù arXiv preprint arXiv:2107.05274 , 2021.\n[17] D. Karimi, S. Vasylechko, and A. Gholipour, ‚ÄúConvolution-free\nmedical image segmentation using transformers,‚Äù arXiv preprint\narXiv:2102.13645, 2021.\n[18] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang,\n‚ÄúSwin-Unet: Unet-like pure transformer for medical image segmenta-\ntion,‚Äù arXiv preprint arXiv:2105.05537 , 2021.\n[19] A. Lin, B. Chen, J. Xu, Z. Zhang, and G. Lu, ‚ÄúDS-TransUNet: Dual\nswin transformer u-net for medical image segmentation,‚Äù arXiv preprint\narXiv:2106.06716, 2021.\n[20] S. Li, X. Sui, X. Luo, X. Xu, Y . Liu, and R. S. M. Goh, ‚ÄúMedical\nimage segmentation using squeeze-and-expansion transformers,‚Äù arXiv\npreprint arXiv:2105.09511, 2021.\n[21] B. Yun, Y . Wang, J. Chen, H. Wang, W. Shen, and Q. Li, ‚ÄúSpecTr:\nSpectral transformer for hyperspectral pathology image segmentation,‚Äù\narXiv preprint arXiv:2103.03604 , 2021.\n[22] G. Xu, X. Wu, X. Zhang, and X. He, ‚ÄúLeViT-UNet: Make faster\nencoders with transformer for medical image segmentation,‚Äù arXiv\npreprint arXiv:2107.08623, 2021.\n[23] Y . Li, W. Cai, Y . Gao, and X. Hu, ‚ÄúMore than encoder: Introducing\ntransformer decoder to upsample,‚Äù arXiv preprint arXiv:2106.10637 ,\n2021.\n[24] Y . Xie, J. Zhang, C. Shen, and Y . Xia, ‚ÄúCoTr: EfÔ¨Åciently bridging cnn\nand transformer for 3d medical image segmentation,‚Äù arXiv preprint\narXiv:2103.03024, 2021.\n[25] W. Wang, C. Chen, M. Ding, J. Li, H. Yu, and S. Zha, ‚ÄúTransBTS:\nMultimodal brain tumor segmentation using transformer,‚Äù arXiv preprint\narXiv:2103.04430, 2021.\n[26] D. Hendrycks and K. Gimpel, ‚ÄúGaussian error linear units (gelus),‚Äù arXiv\npreprint arXiv:1606.08415, 2016.\n[27] J. L. Ba, J. R. Kiros, and G. E. Hinton, ‚ÄúLayer normalization,‚Äù arXiv\npreprint arXiv:1607.06450, 2016.\n[28] W. Luo, Y . Li, R. Urtasun, and R. Zemel, ‚ÄúUnderstanding the effective\nreceptive Ô¨Åeld in deep convolutional neural networks,‚Äù in Proceedings\nof the 30th International Conference on Neural Information Processing\nSystems, pp. 4905‚Äì4913, 2016.\n[29] S. Liu, D. Huang, et al. , ‚ÄúReceptive Ô¨Åeld block net for accurate and\nfast object detection,‚Äù in Proceedings of the European Conference on\nComputer Vision, pp. 385‚Äì400, 2018.\n[30] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\n‚ÄúDeeplab: Semantic image segmentation with deep convolutional nets,\natrous convolution, and fully connected crfs,‚Äù IEEE Transactions on\nPattern Analysis and Machine Intelligence , vol. 40, no. 4, pp. 834‚Äì848,\n2017.\n[31] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV . Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with convolutions,‚Äù\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 1‚Äì9, 2015.\n[32] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\nrecognition,‚Äù inProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 770‚Äì778, 2016.\n[33] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr, et al. , ‚ÄúRethinking semantic segmentation from\na sequence-to-sequence perspective with transformers,‚Äù in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition ,\npp. 6881‚Äì6890, 2021.\n[34] A. Hatamizadeh, Y . Tang, V . Nath, D. Yang, A. Myronenko, B. Land-\nman, H. R. Roth, and D. Xu, ‚ÄúUNETR: Transformers for 3d medical\nimage segmentation,‚Äù in Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision , pp. 574‚Äì584, January 2022.\n[35] X. Huang, Z. Deng, D. Li, and X. Yuan, ‚ÄúMISSFormer: An\neffective medical image segmentation transformer,‚Äù arXiv preprint\narXiv:2109.07162, 2021.\n[36] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, B. A. Landman,\nG. Litjens, B. Menze, O. Ronneberger, R. M. Summers, B. van Gin-\nneken, et al. , ‚ÄúThe medical segmentation decathlon,‚Äù arXiv preprint\narXiv:2106.05735, 2021.\n[37] B. Landman, Z. Xu, J. E. Igelsias, M. Styner, T. Langerak, and A. Klein,\n‚ÄúMiccai multi-atlas labeling beyond the cranial vault‚Äìworkshop and\nchallenge,‚Äù in Proc. MICCAI: Multi-Atlas Labeling Beyond Cranial\nVault-Workshop Challenge, 2015.\n[38] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A.\nHeng, I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester, et al. ,\n‚ÄúDeep learning techniques for automatic mri cardiac multi-structures\nsegmentation and diagnosis: Is the problem solved?,‚Äù IEEE Transactions\non Medical Imaging , vol. 37, no. 11, pp. 2514‚Äì2525, 2018.\n[39] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani,\nJ. Kirby, Y . Burren, N. Porz, J. Slotboom, R. Wiest, et al. , ‚ÄúThe\nmultimodal brain tumor image segmentation benchmark (brats),‚Äù IEEE\nTransactions on Medical Imaging, vol. 34, no. 10, pp. 1993‚Äì2024, 2014.\n[40] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,\n‚Äúnnu-net: a self-conÔ¨Åguring method for deep learning-based biomedical\nimage segmentation,‚ÄùNature methods, vol. 18, no. 2, pp. 203‚Äì211, 2021."
}