{
  "title": "A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models",
  "url": "https://openalex.org/W4387723780",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4221487188",
      "name": "Zhuang, Shengyao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119041265",
      "name": "Zhuang, Honglei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223229561",
      "name": "Koopman, Bevan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202173991",
      "name": "Zuccon, Guido",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385573057",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3148323213",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W3118831779",
    "https://openalex.org/W4281644150",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4389520486",
    "https://openalex.org/W4386554921",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4368755400",
    "https://openalex.org/W2752061190",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W3026051889",
    "https://openalex.org/W3155114168",
    "https://openalex.org/W4383046915",
    "https://openalex.org/W3180230246",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4393399864",
    "https://openalex.org/W4384641573",
    "https://openalex.org/W4387156646",
    "https://openalex.org/W3134665270",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3100107515",
    "https://openalex.org/W4387293261",
    "https://openalex.org/W4310923309",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W3175111331",
    "https://openalex.org/W4285078071",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4404782892",
    "https://openalex.org/W4366559971",
    "https://openalex.org/W4394708953",
    "https://openalex.org/W4390528686",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "We propose a novel zero-shot document ranking approach based on Large Language Models (LLMs): the Setwise prompting approach. Our approach complements existing prompting approaches for LLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise. Through the first-of-its-kind comparative evaluation within a consistent experimental framework and considering factors like model size, token consumption, latency, among others, we show that existing approaches are inherently characterised by trade-offs between effectiveness and efficiency. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods. This significantly improves the efficiency of LLM-based zero-shot ranking, while also retaining high zero-shot ranking effectiveness. We make our code and results publicly available at \\url{https://github.com/ielab/llm-rankers}.",
  "full_text": "A Setwise Approach for Effective and Highly Efficient Zero-shot\nRanking with Large Language Models\nShengyao Zhuangâˆ—\nCSIRO\nBrisbane, Australia\nshengyao.zhuang@csiro.au\nHonglei Zhuang\nGoogle Research\nMountain View, USA\nhlz@google.com\nBevan Koopman\nCSIRO\nBrisbane, Australia\nbevan.koopman@csiro.au\nGuido Zuccon\nThe University of Queensland\nSt Lucia, Australia\ng.zuccon@uq.edu.au\nABSTRACT\nWe propose a novel zero-shot document ranking approach based on\nLarge Language Models (LLMs): the Setwise prompting approach.\nOur approach complements existing prompting approaches for\nLLM-based zero-shot ranking: Pointwise, Pairwise, and Listwise.\nThrough the first-of-its-kind comparative evaluation within a con-\nsistent experimental framework and considering factors like model\nsize, token consumption, latency, among others, we show that ex-\nisting approaches are inherently characterised by trade-offs be-\ntween effectiveness and efficiency. We find that while Pointwise\napproaches score high on efficiency, they suffer from poor effec-\ntiveness. Conversely, Pairwise approaches demonstrate superior\neffectiveness but incur high computational overhead. Our Setwise\napproach, instead, reduces the number of LLM inferences and the\namount of prompt token consumption during the ranking proce-\ndure, compared to previous methods. This significantly improves\nthe efficiency of LLM-based zero-shot ranking, while also retaining\nhigh zero-shot ranking effectiveness. We make our code and results\npublicly available at https://github.com/ielab/llm-rankers.\nCCS CONCEPTS\nâ€¢ Information systems â†’Language models.\nKEYWORDS\nLarge Language Model for Zero-shot ranking, setwise prompting,\nsorting algorithm\nACM Reference Format:\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon.\n2024. A Setwise Approach for Effective and Highly Efficient Zero-shot\nRanking with Large Language Models. InProceedings of the 47th International\nACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR â€™24), July 14â€“18, 2024, Washington, DC, USA. ACM, New York, NY,\nUSA, 10 pages. https://doi.org/10.1145/3626772.3657813\n1 INTRODUCTION\nLarge Language Models (LLMs) such as GPT-3 [ 3], FlanT5 [ 33],\nGemini [24], LLaMa2 [27], and PaLM [4] are highly effective across\na diverse range of natural language processing tasks under the\nzero-shot settings [1, 3, 10, 32]. Notably, these LLMs have also been\nâˆ—Corresponding author.\nThis work is licensed under a Creative Commons Attribution-\nNonCommercial-ShareAlike International 4.0 License.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0431-4/24/07.\nhttps://doi.org/10.1145/3626772.3657813\nadapted for zero-shot document ranking. [ 12, 15, 20â€“23]. Using\nLLMs in zero-shot ranking tasks can be broadly categorized into\nthree main approaches: Pointwise [12, 22, 37], Listwise [15, 20, 23],\nand Pairwise [21]. These approaches employ different prompting\nstrategies to instruct the LLM to output a relevance estimation for\neach candidate document and rank documents accordingly. Com-\npared to traditional neural ranking methods [14], these LLM-based\nrankers do not require any further supervised fine-tuning and ex-\nhibit strong zero-shot ranking capabilities. While these LLM-based\nzero-shot ranking approaches have been successful individually,\nit is worth noting that there has been a lack of fair comparison in\nthe literature regarding their effectiveness, and in particular, their\nefficiency within the exact same experimental framework. This\nincludes factors such as utilizing the same size LLM, evaluation\nbenchmarks, and computational resources. We believe it is critical\nto establish a rigorous framework for evaluating these LLM-based\nzero-shot ranking approaches. By doing so, we can draw meaningful\nconclusions about their comparative effectiveness and efficiency.\nThus, in this paper, we first conduct a systematic evaluation\nof all existing approaches within a consistent experimental envi-\nronment. In addition to assessing ranking effectiveness, we also\ncompare the efficiency of these methods in terms of computational\nexpense and query latency. Our findings indicate that the Pairwise\napproach emerges as the most effective but falls short in terms of\nefficiency even with the assistance of sorting algorithms aimed at\nimproving efficiency. Conversely, the Pointwise approach stands\nout as the most efficient but lags behind other methods in terms of\nranking effectiveness. The Listwise approach, which relies solely\non the generation of document labels in order, can strike a mid-\ndle ground between efficiency and effectiveness but this varies\nconsiderably based on configuration, implementation and evalua-\ntion dataset (highlighting the importance of thoroughly evaluating\nthese model under multiple settings). Overall, these comprehensive\nresults offer an understanding of the strengths and weaknesses\nof LLM-based zero-shot ranking approaches, providing valuable\ninsights for those seeking to select the most suitable approach for\nreal-world applications.\nHaving considered all the different approaches and their results\nin terms of efficiency and effectiveness tradeoffs, we set about de-\nvising a method that was both effective and efficient. Our approach\nwas to take the most effective model (Pairwise) and to enhance its\nefficiency (without seriously compromising effectiveness). Our solu-\ntion is a novelSetwise prompting approach. This concept stems from\nour realisation that the sorting algorithms employed by Pairwise\napproaches can be accelerated by comparing multiple documents,\nas opposed to just a pair at a time.\narXiv:2310.09497v2  [cs.IR]  30 May 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon\nFigure 1: Different prompting strategies. (a) Pointwise, (b) Listwise, (c) Pairwise and (d) our proposed Setwise.\nOur Setwise prompting approach instructs LLMs to select the\nmost relevant document to the query from a set of candidate doc-\numents. This straightforward adjustment allows the sorting algo-\nrithms to infer relevance preferences for more than two candidate\ndocuments at each step, thus significantly reducing the total num-\nber of comparisons required; this leads to substantial savings in\ncomputational resources. Furthermore, beyond the adjustment to\nPairwise approaches, Setwise prompting allows for the utilization\nof model output logits to estimate the likelihood of ranks of docu-\nment labels, a capability not feasible in existingListwise approaches,\nwhich solely rely on document label ranking generation â€”â€“ a pro-\ncess that is slow and less effective. WeSetwise and other existing\napproaches under the same experimental settings to provide a clear\nand consistent comparison. Our results show that the incorporation\nof our Setwise prompting substantially improves the efficiency of\nboth Pairwise and Listwise approaches. In addition, Setwise sorting\nenhances Pairwise and Listwise robustness to variations in the in-\nternal ordering quality of the initial rankings: no matter what the\ninitial ordering of the top-k documents to rank is, our method pro-\nvides consistent and effective results. This is unlike other methods\nthat are highly susceptible to such initial ordering.\nTo conclude, this paper makes three key contributions to our\nunderstanding of LLM-based zero-shot ranking approaches:\n(1) We introduce an innovativeSetwise prompting approach that en-\nhances the sorting algorithms employed in thePairwise method,\nresulting in highly efficient zero-shot ranking with LLMs.\n(2) We conduct a systematic examination of all existing LLM-based\nzero-shot ranking approaches and our novel Setwise approach\nunder strict and consistent experimental conditions, includ-\ning efficiency comparisons which have been overlooked in the\nliterature. Our comprehensive empirical evaluation on popu-\nlar zero-shot document ranking benchmarks offers valuable\ninsights for practitioners.\n(3) We further adapt how our Setwise prompting approach com-\nputes rankings to the Listwise approach, leveraging the model\noutput logits to estimate the likelihood of rankings. This leads\nto a more effective and efficient Listwise zero-shot ranking.\n2 BACKGROUND & RELATED WORK\nThere are three main prompting approaches for zero-shot document\nranking employing LLMs: Pointwise [12, 22], Listwise [15, 20, 23],\nand Pairwise [21]. In this section, we delve into the specifics of these\nwhile situating our work within the existing literature. As a visual\naid we will refer to Figure 1 as we discuss each method.\n2.1 Pointwise prompting approaches\nFigure 1a shows pointwise approaches. There are two popular di-\nrections of prompting LLMs for ranking documents in a pointwise\nmanner: generation and likelihood. In the generation approach, a\nâ€œyes/no\" generation technique is used: LLMs are prompted to gen-\nerate whether the provided candidate document is relevant to the\nquery, with the process repeated for each candidate document. Sub-\nsequently, these candidate documents are re-ranked based on the\nnormalized likelihood of generating a \"yes\" response [12, 17]. The\nlikelihood approach involves query likelihood modelling (QLM) [18,\n36, 39], wherein LLMs are prompted to produce a relevant query for\neach candidate document. The documents are then re-ranked based\non the likelihood of generating the actual query [ 22]. It is worth\nnoting that both pointwise methods require access to the output\nlogits of the model to be able to compute the likelihood scores.\nThus, it is not possible to use closed-sourced LLMs to implement\nthese approaches if the corresponding APIs do not expose the logits\nvalues: this is the case for example of GPT-4.\n2.2 Listwise prompting approaches\nFigure 1b shows listwise approaches. Here the LLMs receive a query\nalong with a list of candidate documents and are prompted to gener-\nate a ranked list of document labels based on their relevance to the\nquery [15, 20, 23]. However, due to the limited input length allowed\nby LLMs, including all candidate documents in the prompt is not\nfeasible. To address this, current listwise approaches use a sliding\nwindow method. This involves re-ranking a window of candidate\ndocuments, starting from the bottom of the original ranking list\nand progressing upwards. This process can be repeated multiple\ntimes to achieve an improved final ranking and allows for early\nstopping mechanisms to target only the top-ð‘˜ranking, thereby con-\nserving computational resources. In contrast to pointwise methods,\nwhich utilize the likelihood value of the output tokens for ranking\ndocuments, listwise approaches rely on the more efficient process\nof generation of the ranking list.\n2.3 Pairwise prompting approaches\nFigure 1c shows pairwise approaches. LLMs are prompted with\na query alongside a pair of documents, and are asked to gener-\nate the label indicating which document is more relevant to the\nA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\n(a) Heapify with Pairwise prompting (comparing 2 documents at a time).\n(b) Heapify with our Setwise prompting (comparing 4 documents at a time).\n(c) Bubble sort with Pairwise prompting (comparing 2 documents at a time).\n(d) Bubble sort with our Setwise prompting (comparing 3 documents at a time).\nFigure 2: Illustration of the impact of Setwise Prompting vs. Pairwise Prompting on Sorting Algorithms. Nodes are documents, numbers in\nnodes represent the level of relevance assigned by the LLM (higher is more relevant).\nquery [19, 21]. To re-rank all candidate documents, a basic method,\ncalled AllPairs, involves generating all possible permutations of\ndocument pairs from the candidate set. Pairs are independently\nthen fed into the LLM, and the preferred document for each pair\nis determined. Subsequently, an aggregation function is employed\nto assign a score to each document based on the inferred pairwise\npreferences, and the final ranking is established based on the total\nscore assigned to each document [19]. However, this aggregation-\nbased approach suffers from high query latency: LLM inference\non all document pairs can be computationally expensive. To ad-\ndress this efficiency issue in pairwise approaches, prior studies\nhave introduced sampling [8, 16] and sorting [21] algorithms. In\nthis paper, we focus on sorting algorithms because, assuming an\nLLM can provide ideal pairwise preferences, the sorting algorithms\noffer the theoretical assurance of identifying the top-ð‘˜ most rele-\nvant documents from the candidate pool. In prior work [21], two\nsorting algorithms [9], heap sort and bubble sort , were employed.\nUnlike AllPairs, these algorithms leverage efficient data structures\nto selectively compare document pairs, which can quickly pull the\nmost relevant documents out from the candidate pool and place\nthem at the top of the final ranking. This is particularly suitable for\nthe top-ð‘˜ranking task, where only a ranking of theð‘˜most relevant\ndocuments is needed. These sorting algorithms provide a stopping\nmechanism that prevents the need to rank all candidate documents.\nFrom a theoretical standpoint the differences and relative advan-\ntages among these three families of zero-shot document ranking\nthat employ LLMs are clear. However, from an empirical standpoint\nthere has been no fair and comprehensive evaluation of these tech-\nniques in terms of effectiveness vs. efficiency, and across factors\nsuch as sizes of LLMs, benchmarks, and computational resources.\n2.4 Other Directions in using LLMs for Ranking\nThe three families of approaches outlined above directly use prompt-\ning of LLMs to infer document relevance to a given query, lever-\naging LLMs for document re-ranking in a zero-shot setting with\nno training data required for the ranking task. An alternative di-\nrection in using LLMs for retrieval has also emerged, where LLMs\nare instead used as text embedding models for dense document\nretrieval [2, 11, 30, 31]. Because these methods create document\nrepresentations independently of query representations, they can\nbe used as a first-stage document retriever (i.e. in all effects as a\nbi-encoder architecture), rather than being limited to be used as\na re-ranker as instead are the approaches from the three families\nwe have reviewed above. However, these methods require perform-\ning contrastive learning training to enable the generative LLM to\nact as a text-embedding model. An exception to this is the recent\nPromptReps method [38], which does not require contrastive train-\ning, achieving document (and query) embedding simply via prompt\nengineering. Another feature of PromptReps is the ability to ob-\ntain at the same time both a dense and a sparse representation of\ndocuments and queries.\n3 SETWISE RANKING PROMPTING\nIn this section, we discuss the limitations present in the current LLM-\nbased zero-shot ranking methods. We then describe our proposed\nSetwise approach and how it addresses these limitations.\n3.1 Limitations of Current Approaches\nThe efficiency of LLM-based zero-shot ranking methods hinges on\ntwo critical dimensions.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon\nTable 1: Properties of different methods. Logits: requires access to the\nLLM logits. Generate: requires actually generating tokens. Batching:\nallows for batch inference. Top- ð‘˜: allows for early stopping once top-\nð‘˜most relevant documents found. # LLM calls: the number of LLM\nforward passes needed in the worst case. ( ð‘: number of documents\nto re-rank. ð‘Ÿ: number of repeats. ð‘ : step size for sliding window. ð‘˜:\nnumber of top- ð‘˜ documents to find. ð‘: number of compared docu-\nments at each step.)\nMethods Logits Generate Batching Top- ð‘˜ # LLM calls\npointwise.qlm âœ“ âœ“ O(ð‘)\npointwise.yes_no âœ“ âœ“ O(ð‘)\nlistwise.generation âœ“ âœ“ O(ð‘Ÿâˆ—(ð‘/ð‘ ))\nlistwise.likelihood âœ“ âœ“ O(ð‘Ÿâˆ—(ð‘/ð‘ ))\npairwise.allpair âœ“ âœ“ âœ“ O(ð‘2 âˆ’ð‘)\npairwise.heapsort âœ“ âœ“ âœ“ O(ð‘˜âˆ—log2ð‘)\npairwise.bubblesortâœ“ âœ“ âœ“ O(ð‘˜âˆ—ð‘)\nsetwise.heapsort âœ“ âœ“ âœ“ O(ð‘˜âˆ—logð‘ð‘)\nsetwise.bubblesort âœ“ âœ“ âœ“ O(ð‘˜âˆ—(ð‘/(ð‘âˆ’1)))\nFirst, the number of LLM inferences significantly impacts effi-\nciency. Given that LLMs are large neural networks with billions\nof parameters, inference is computationally intensive. Hence, an\nincreased number of LLM inferences introduces a considerable\ncomputational overhead. This is notably observed in the current\nPairwise approach, which is inefficient due to the extensive need\nfor inferring preferences for the many document pairs. While sort-\ning algorithms offer some relief, they do not entirely mitigate the\nefficiency issue.\nSecond, the number of LLM-generated tokens per inference plays\na pivotal role. LLMs employ a transformer decoder for autoregres-\nsive token generation, where the next token generation depend\non previously tokens generated. Each additional generated token\nrequires an extra LLM inference. This accounts for the inefficiency\nof the existing Listwise approach, which relies on generating an\nentire ranking of document label lists, often requiring a substantial\nnumber of generated tokens.\nNext, we introduce our new Setwise prompting approaches, de-\nsigned to overcome the efficiency limitations of both Pairwise and\nListwise methods by minimizing the number of required LLM infer-\nences and leveraging the logits produced by the LLM.\n3.2 Speeding-up Pairwise with Setwise\nTo solve the inefficiency issue of these approaches, we propose a\nnovel Setwise prompting approach. Our prompt, as illustrated in\nFigure 1d, instructs the LLM to select the most relevant document\nfor the given query from a set of documents, hence the termSetwise\nprompting. We specifically treat the collection of documents as\nan unordered set and later experiments will show that Setwise\nprompting is quite robust to document ordering.\nWith our prompt, sorting-based Pairwise approaches can be\nconsiderably accelerated. This is because the original heap sort and\nbubble sort algorithm used in the Pairwise approach only compares\na pair of documents at each step in the sorting process, as illustrated\nin Figure 2a and 2c. These sorting algorithms can be sped up by\ncomparing more than two documents at each step. For example, in\nthe heap sort algorithm, the â€œheapify\" function needs to be invoked\nfor each subtree, where the parent node must be swapped with the\nchild node with the highest value if it exceeds the parent value. In\nthe case of Figure 2a, to perform â€œheapify\" with pairwise prompting,\na minimum of 6 comparisons (each root node paired with each child\nnode) are required. Conversely, if we increase the number of child\nnodes in each subtree to 3 and can compare 4 nodes at a time,\nonly 2 comparisons are needed to â€œheapify\" a tree with 9 nodes,\nas illustrated in Figure 2b. Similarly, for the bubble sort algorithm,\nif we can compare more than a pair of documents at a time, each\nâ€œbubblingâ€ process will be accelerated. For instance, in Figure 2c,\nthere are 4 comparisons in total, but in Figure 2d, with the ability\nto compare 3 documents at once, only 2 comparisons are required\nto be able to bring the node with the largest value to the top. Our\nSetwise prompting is designed to instruct LLMs to compare the\nrelevance of multiple documents at a time, making it well-suited\nfor this purpose.\n3.3 Listwise Likelihoods with Setwise\nOur Setwise prompting can also accelerate the ranking process for\nthe Listwise approach. The original Listwise method relies on the\nLLMâ€™s next token generation to produce the complete ordered list\nof document labels at each step of the sliding window process, as\nillustrated in Figure 1b. As we discussed, generating the document\nlabel list is computationally intensive, because the LLM must do\none inference for each next token prediction. On the other hand,\nthe LLM may generate results in an unexpected format or even de-\ncline to generate the desired document label list [23], thus harming\neffectiveness. Fortunately, if we have access to the LLMâ€™s output\nlogits, these issues can be avoided by evaluating the likelihood\nof generating every conceivable document label list and then se-\nlecting the most probable one as the output. Regrettably, this is\nonly theoretically possible, but in practice, it is unfeasible for the\nexisting Listwise approach due to the very large number of possible\ndocument label permutation, which implies that the process of like-\nlihood checking may actually become even more time-consuming\nthan generating the list itself.\nSetwise prompting again provides a solution: we can easily derive\nan ordered list of document labels from the LLM output logits.\nThis is done by assessing the likelihood of each document label\nbeing chosen as the most relevant, as shown in Figure 1d. This\nstraightforward trick markedly accelerates Listwise ranking, as it\nrequires only a single forward pass of the LLM, and also guarantees\nthat the output matches the desired document label list.\n3.4 Advantages of Setwise\nWe summarize and compare the key different properties of exist-\ning zero-shot LLM ranking approaches along with our proposed\nSetwise prompting approach in Table 1. Notably, pointwise.qlm,\npointwise.yes_no and pairwise.allpair require a brute-force of LLM\ninference for all available documents relevance or preferences. Thus,\nthey are unable to facilitate early-stopping for the top-ð‘˜ ranking.\nHowever, these approaches do allow batch inferences, hence the\nmaximum GPU memory utilization could be easily achieved by us-\ning the highest batch size. On the other hand, other approaches use\nsorting algorithms, enabling early-stopping once the top-ð‘˜ most\nrelevant documents are identified. However, this compromises the\nfeasibility of batching inference, as the LLM inference at each step\nof the sorting algorithms relies on the results from the preced-\ning step. Our Setwise prompting empowers the previous Listwise\napproach (listwise.generation), which relied on LLMâ€™s next token\nA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\ngenerations, to now utilize the LLMâ€™s output logits. We refer to\nthe Listwise approach that incorporates our Setwise prompt as list-\nwise.likelihood. Finally, comparing with Pairwise approaches, our\nSetwise prompting has fewer LLM calls by comparing a minimum\nof ð‘ â‰¥3 documents at each step of the sorting algorithms.\nOn the other hand, model output calibration might be a concern\nfor Pointwise methods because each documentâ€™s relevance is in-\nferred independently. Consequently, ranking documents based on\nPointwise relevance scores necessitates calibration [21]. However,\nfor our Setwise method (as well as for Pairwise and Listwise) the\nmodel output logits are derived from the input of multiple docu-\nments, which do not directly represent the relevance of a single\ndocument but rather serve as an indicator of preference among\ndocuments. Thus, calibration is not necessary.\n4 EXPERIMENTS\n4.1 Datasets and evaluations\nThe first objective of this study is to contribute a fair and comprehen-\nsive evaluation of existing LLM-based zero-shot ranking methods in\nterms ranking effectiveness and efficiency. To achieve this goal, we\ncarried out extensive empirical evaluations using well-established\ndocument ranking datasets: the TREC Deep Learning 2019 [6] and\n2020 [5], along with the BEIR benchmark datasets [25]. To guaran-\ntee a fair comparison across different approaches, we tested all of\nthe methods using the same open-source Flan-t5 LLMs [33], avail-\nable on the Huggingface model hub in various sizes (780M, 3B,\nand 11B parameters). All LLM methods were used to re-rank 100\ndocuments retrieved by a BM25 first-stage retriever. In order to\noptimize efficiency, the focus was on a top-ð‘˜ranking task, whereby\nthe re-ranking process stopped as soon as the top- ð‘˜ most rele-\nvant documents were identified and ranked. Here, we set ð‘˜ = 10.\nThe effectiveness of different approaches was evaluated using the\nNDCG@10 metric, which serves as the official evaluation metric\nfor the employed datasets.\nEfficiency was evaluated with the following metrics:\nâ€¢The average number of LLM inferences per query. LLMs have\nlimited input length. Thus, to re-rank 100 documents, multiple\nLLM inferences are often needed. Itâ€™s important to note that\nan increased number of LLM inferences translates to higher\ncomputational demands. Thus, we regard this as an efficiency\nmetric worth considering.\nâ€¢The average number of prompt tokens inputted to the LLMs per\nquery. This metric takes into account the actual average quan-\ntity of input tokens required in the prompts for each method\nto re-rank 100 documents per query. Given that self-attention\nmechanisms in transformer-based LLMs become prohibitively\ncostly for a large number of input tokens [29], an increase in\ntokens within the prompts also translates to higher compu-\ntational demands. Notably, numerous LLM web API services,\nincluding OpenAI APIs, charge based on the number of input\ntokens in the API calls. As such, we deem this metric valuable\nin assessing efficiency.\nâ€¢The average number of generated tokens outputted by LLMs per\nquery. Much like the assessment of average prompt tokens, this\nmetric provides an evaluation of computational efficiency, but\nfrom a token generation perspective. Instead of focusing on\nthe number of tokens in the prompt, it takes into account the\nnumber of tokens generated. This is particularly significant\nbecause transformer-based generative LLMs produce content\ntoken-by-token, with each subsequent token relying on the\ngeneration of preceding ones. Consequently, an increase in\nnumber of generated tokens leads to a corresponding increase\nin the computational cost, as each additional generated token\nimplies another LLM forward inference. In fact, OpenAI ap-\nplies a pricing structure wherein the cost for the number of\ngenerated tokens is twice that of the number of prompt tokens\nfor their LLM APIs 1. This underscores the substantial impact\nthat generated tokens can have on computational expenses.\nâ€¢The average query latency. We evaluate the run time efficiency\nof all the methods with average query latency. To conduct this\nassessment, a single GPU is employed, and queries are issued\none at a time. The per-query latency is then averaged across\nall the queries in the dataset. Itâ€™s important to highlight that\nfor methods that support batching we always employ the max-\nimum batch size to optimize GPU memory usage and parallel\ncomputation, thus maximizing efficiency for these particular\nmethods. This approach ensures that the evaluation is con-\nducted under conditions most favourable for efficiency gains.\nIt is important to acknowledge that while other methods may\nnot be able to use the batching strategy for individual queries,\nthey do have the capability to utilize batching and parallel\ncomputing across various user queries in real-world scenarios.\nHowever, this lies more in engineering efforts and falls outside\nthe scope of this paper: as such, we do not investigate this\nperspective.\n4.2 Implementation details\nTo establish the initial BM25 first-stage ranking for all datasets, we\nemployed the Pyserini Python library [ 13] with default settings.\nFor LLM-based zero-shot re-rankers, we followed the prompts rec-\nommended in existing literature to guide Flan-t5 models of varying\nsizes (Flan-t5-large with 780M parameters, Flan-t5-xl with 3B pa-\nrameters, and Flan-t5-xxl with 11B parameters) in executing the\nzero-shot ranking task.\nSpecifically, for thepointwise.qlm method, we adopted the prompt\nsuggested by Sachan et al. [22]. For pointwise.yes_no, we use the\nprompt provided by Qin et al . [21]. For listwise.generate, we uti-\nlized the prompt designed by Sun et al. [23]. As for pairwise.allpair,\npairwise.heapsort, and pairwise.bubblesort, we relied on the prompts\nfrom the original paper by Qin et al. [21]. For methods leveraging\nour Setwise prompting (i.e. listwise.likelihood, setwise.heapsort, and\nsetwise.bubblesort), we employed the prompts detailed in Section 3.\nIn the case of Listwise approaches, we configure the window\nsize (ð‘¤) to contain 4 documents, each capped at a maximum of 100\ntokens. The step size (ð‘ ) is set to 2, and the number of repetitions\n(ð‘Ÿ) is set to 5. These settings take into account the token limitations\nimposed by Flan-t5 models, which have an input token cap of 512.\nA window size of 4 documents appears reasonable as it aligns well\nwith the prompt capacity. Additionally, a step size of 2, combined\nwith 5 repetitions, has theoretical guarantees of bringing the 10\nmost relevant documents to the top. For our Setwise approaches,\n1https://openai.com/pricing, last visited 12 October 2023.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon\nTable 2: Results on TREC DL. All the methods re-rank BM25 top 100 documents. We present the ranking effectiveness in terms of NDCG@10,\nbest values highlighted in boldface. Superscripts denote statistically significant improvements (paired Studentâ€™s t-test with ð‘ â‰¤0.05 with\nBonferroni correction). #Inferences denotes the average number of LLM inferences per query. Pro. Tokens is the average number of tokens in\nthe prompt for each query. Gen. tokens is the average number of generated tokens per query. Latency is the average query latency, in seconds.\nTREC DL 2019 TREC DL 2020\n# Methods NDCG@10 #Inferences Pro. tokens Gen. tokens Latency(s)NDCG@10 #Inferences Pro. tokens Gen. tokens Latency(s)\nð‘Ž BM25 .506 - - - - .480 - - - -\nFlan-t5-large\nð‘ pointwise.qlm .557 100 15211.6 - 0.6 .567ð‘Ž 100 15285.2 - 0.5\nð‘ pointwise.yes_no.654ð‘Žð‘ð‘‘ 100 16111.6 - 0.6 .615ð‘Žð‘‘ 100 16185.2 - 0.6\nð‘‘ listwise.generation.561ð‘Ž 245 119120.8 2581.35 54.2 .547ð‘Ž 245 119629.6 2460.1 52\nð‘’ listwise.likelihood.669ð‘Žð‘ð‘‘ 245 94200.7 - 10 .626ð‘Žð‘ð‘‘ 245 95208.3 - 10\nð‘“ pairwise.allpair .666ð‘Žð‘ð‘‘ 9900 3014383.1 49500 109.6 .622ð‘Žð‘ð‘‘ 9900 3014232.7 49500 108.9\nð‘” pairwise.heapsort.657ð‘Žð‘ð‘‘ 230.3 104952.5 2303.3 16.1 .619ð‘Žð‘ð‘‘ 226.8 104242.1 2268.3 16.1\nâ„Ž pairwise.bubblesort.636ð‘Žð‘ð‘‘ 844.2 381386.3 8441.6 58.3 .589ð‘Žð‘‘ 778.5 357358.5 7785.4 54.1\nð‘– setwise.heapsort .670ð‘Žð‘ð‘‘ 125.4 40460.6 626.9 8.0 .618ð‘Žð‘‘ 124.2 40362.0 621.1 8.0\nð‘— setwise.bubblesort.678ð‘Žð‘ð‘‘â„Ž 460.5 147774.1 2302.3 29.1 .624ð‘Žð‘ð‘‘ 457.4 148947.3 2287.1 28.9\nFlan-t5-xl\nð‘ pointwise.qlm .542 100 15211.6 - 1.4 .542ð‘Ž 100 15285.2 - 1.4\nð‘ pointwise.yes_no.650ð‘Žð‘ð‘‘ 100 16111.6 - 1.5 .636ð‘Žð‘ð‘‘ 100 16185.2 - 1.5\nð‘‘ listwise.generation.569ð‘Ž 245 119163.0 2910 71.4 .547ð‘Ž 245 119814.3 2814.7 69\nð‘’ listwise.likelihood.689ð‘Žð‘ð‘‘ 245 94446.1 - 12.5 .672ð‘Žð‘ð‘‘ 245 95298.7 - 12.6\nð‘“ pairwise.allpair .713ð‘Žð‘ð‘ð‘‘ð‘’â„Žð‘– 9900 2953436.2 49500 254.9 .682ð‘Žð‘ð‘ð‘‘ 9900 2949457.6 49500 254.8\nð‘” pairwise.heapsort.705ð‘Žð‘ð‘ð‘‘ 241.9 110126.9 2418.6 20.5 .692ð‘Žð‘ð‘ð‘‘â„Ž 244.3 111341 2443.3 20.8\nâ„Ž pairwise.bubblesort.683ð‘Žð‘ð‘‘ 886.9 400367.1 8869.1 75.1 .662ð‘Žð‘ð‘‘ 863.9 394954.2 8638.5 74.3\nð‘– setwise.heapsort .693ð‘Žð‘ð‘ð‘‘ 129.5 41665.7 647.4 9.6 .678ð‘Žð‘ð‘ð‘‘ 127.8 41569.1 638.9 9.7\nð‘— setwise.bubblesort.705ð‘Žð‘ð‘ð‘‘ 466.9 149949.1 2334.5 35.2 .676ð‘Žð‘ð‘ð‘‘ 463.5 151249.8 2317.6 35.3\nFlan-t5-xxl\nð‘ pointwise.qlm .506 100 15211.6 - 3.7 .492 100 15285.2 - 3.7\nð‘ pointwise.yes_no.644ð‘Žð‘ 100 16111.6 - 3.9 .632ð‘Žð‘ 100 16185.2 - 3.9\nð‘‘ listwise.generation.662ð‘Žð‘ 245 119334.7 2824 100.1 .637ð‘Žð‘ 245 119951.6 2707.9 97.3\nð‘’ listwise.likelihood.701ð‘Žð‘ð‘ð‘‘ 245 94537.5 - 36.6 .690ð‘Žð‘ð‘ð‘‘ 245 95482.7 - 36.9\nð‘“ pairwise.allpair .699ð‘Žð‘ð‘ð‘‘ 9900 2794942.6 49500 730.2 .688ð‘Žð‘ð‘ð‘‘ 9900 2794928.4 49500 730.5\nð‘” pairwise.heapsort.708ð‘Žð‘ð‘ð‘‘â„Ž 239.4 109402 2394 45 .699ð‘Žð‘ð‘ð‘‘ 240.5 110211.8 2404.8 45.2\nh pairwise.bubblesort.679ð‘Žð‘ 870.5 394386 8705.3 162.5 .681ð‘Žð‘ð‘ð‘‘ 842.9 387359.2 8428.5 158.8\ni setwise.heapsort .706ð‘Žð‘ð‘ð‘‘ 130.1 42078.6 650.5 20.2 .688ð‘Žð‘ð‘ð‘‘ 128.1 41633.7 640.6 20.0\nð‘— setwise.bubblesort.711ð‘Žð‘ð‘ð‘‘â„Ž 468.3 150764.8 2341.6 72.6 .686ð‘Žð‘ð‘ð‘‘ 467.9 152709.5 2339.6 73.2\nwe set the number of compared documents ð‘ in each step to 3\nfor the main results. We further investigate the impact of ð‘ in\nSection 5.3. For all other methods, we truncate the documents with\nthe maximum number of tokens to 128.\nWe note that, among all the methods capable of utilizing both\nmodel output logits and generation outputs, we exclusively employ\nthe latter. This choice is made in favor of a more general approach\nthat allows for leveraging generation APIs across a wider range\nof closed-source LLMs. Nevertheless, we investigate the difference\nbetween using model output logits and generation outputs for our\nSetwise approaches in Section 5.1.\nWe carried out the efficiency evaluations on a local GPU work-\nstation equipped with an AMD Ryzen Threadripper PRO 3955WX\n16-Core CPU, a NVIDIA RTX A6000 GPU with 49GB of memory,\nand 128GB of DDR4 RAM.\n5 RESULTS AND ANALYSIS\n5.1 Effectiveness Results\nTable 2 presents results for both ranking effectiveness and efficiency\non TREC DL datasets.\nIn regards to ranking effectiveness, it is notable that all LLM-\nbased zero-shot ranking approaches demonstrate a significant im-\nprovement over the initial BM25 ranking. The only exception to this\ntrend is the pointwise.qlm approach on DL2019 across all models\nand DL2020 with the Flan-t5-xxl model. Interestingly, as the LLM\nsize increases, the effectiveness of pointwise.qlm decreases. This\nfinding is particularly unexpected, given the common assumption\nthat larger LLMs tend to be more effective.\nOn the other hand, pointwise.yes_no method achieved a decent\nNDCG@10 score with Flan-t5-large when compared to other meth-\nods. However, effectiveness also did not increase as model size\nincreased. These unexpected results for both Pointwise methods\nmight be attributed to the requirement of a more refined model\noutput calibration process, ensuring their suitability for comparison\nand sorting across different documents [21].\nThe Listwise approaches (listwise.generation) are far less effec-\ntive when tested with Flan-t5-large and Flan-t5-xl. However, list-\nwise.generation shows some improvement with Flan-t5-xxl. These\nresults may be attributed to the fact that generating a ranking list\nrequires fine-grained relevance preferences across multiple docu-\nments, a task that may exceed the capabilities of smaller models.\nIn contrast, the listwise.likelihood approach, empowered by our\nSetwise prompt, markedly enhances the ranking effectiveness of\nthe Listwise approach, even when utilizing smaller models. We ac-\nknowledge however that listwise.likelihood requires access to the\nmodel output logits, whereas listwise.generation does not. In the\ncase of Pairwise and Setwise approaches, they consistently exhibit\ngood ranking effectiveness across various model sizes and datasets.\nA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 3: Overall NDCG@10 obtained by methods on BEIR datasets. The best results are highlighted in boldface. Superscripts denote statistically\nsignificant improvements (paired Studentâ€™s t-test with ð‘ â‰¤0.05 with Bonferroni correction).\n# Methods Covid NFCorpus Touche DBPedia SciFact Signal News Robust04 Avg\nð‘Ž BM25 .595 .322 .442 .318 .679 .331 .395 .407 .436\nFlan-t5-large\nð‘ pointwise.qlm .664ð‘Ž .322 .260 .305 .644 ð‘ .314ð‘ .413ð‘ .439ð‘Žð‘“ .420\nð‘ pointwise.yes_no .664ð‘Ž .308 .238 .296 .504 .275 .346 .456 ð‘Žð‘“ .386\nð‘‘ listwise.generation.692ð‘Ž .333ð‘Žð‘ .441ð‘ð‘ð‘’ð‘“â„Žð‘– .391ð‘Žð‘ð‘ .650ð‘ .343ð‘Žð‘ð‘’ .428ð‘Žð‘ .441ð‘Žð‘“ .465\nð‘’ listwise.likelihood .756ð‘Žð‘ð‘ð‘‘ .334ð‘ .327ð‘ð‘ .444ð‘Žð‘ð‘ð‘‘ð‘“ð‘”â„Ž .639ð‘ .308ð‘ .453ð‘Žð‘ .475ð‘Žð‘ð‘‘ð‘“ð‘” .467\nð‘“ pairwise.heapsort .761ð‘Žð‘ð‘ð‘‘ð‘” .336ð‘ .318ð‘ð‘ .414ð‘Žð‘ð‘ð‘‘ .671ð‘â„Žð‘– .325ð‘ .440ð‘Žð‘ .402 .458\nð‘” pairwise.bubblesort.714ð‘Ž .341ð‘Žð‘ð‘ð‘‘â„Ž .447ð‘ð‘ð‘’ð‘“â„Žð‘– .416ð‘Žð‘ð‘ð‘‘ .700ð‘ð‘ð‘‘ð‘’ð‘“â„Žð‘– .361ð‘Žð‘ð‘ð‘‘ð‘’ð‘“â„Ž .440ð‘Žð‘ .439ð‘Žð‘“ .482\nâ„Ž setwise.heapsort .768ð‘Žð‘ð‘ð‘‘ð‘” .325ð‘ .303ð‘ .413ð‘Žð‘ð‘ð‘‘ .620ð‘ .319ð‘ .439ð‘ .462ð‘Žð‘ð‘“ .456\nð‘– setwise.bubblesort.761ð‘Žð‘ð‘ð‘‘ð‘” .338ð‘Žð‘ð‘â„Ž .394ð‘ð‘ð‘’ð‘“â„Ž .441ð‘Žð‘ð‘ð‘‘ð‘“ð‘”â„Ž .636ð‘ .351ð‘ð‘ð‘’ð‘“â„Ž .447ð‘Žð‘ .497ð‘Žð‘ð‘ð‘‘ð‘’ð‘“ð‘”â„Ž .483\nFlan-t5-xl\nð‘ pointwise.qlm .679ð‘Ž .330 .216 .310 ð‘ .696ð‘ .299 .422 .427 .422\nð‘ pointwise.yes_no .698ð‘Ž .331 .269 .273 .553 .297 .413 .479 ð‘Žð‘ .414\nð‘‘ listwise.generation.650ð‘Ž .334ð‘Ž .451ð‘ð‘ð‘’ð‘“ð‘”â„Žð‘– .366ð‘Žð‘ð‘ .694ð‘ .349ð‘Žð‘ð‘ð‘’ð‘“â„Ž .437ð‘Ž .475ð‘Žð‘ .470\nð‘’ listwise.likelihood .736ð‘Žð‘ð‘‘ .360ð‘Žð‘ð‘ð‘‘ .310ð‘ .449ð‘Žð‘ð‘ð‘‘ð‘“ð‘”â„Žð‘– .686ð‘ .320 .472 ð‘Žð‘ð‘ .526ð‘Žð‘ð‘ð‘‘ .482\nð‘“ pairwise.heapsort .778ð‘Žð‘ð‘ð‘‘ð‘’ .355ð‘Žð‘ð‘ð‘‘ .303ð‘ .417ð‘Žð‘ð‘ð‘‘ .711ð‘â„Ž .317 .471 ð‘Žð‘ð‘ .550ð‘Žð‘ð‘ð‘‘ð‘’â„Žð‘– .488\nð‘” pairwise.bubblesort.763ð‘Žð‘ð‘ð‘‘ .359ð‘Žð‘ð‘ð‘‘ .400ð‘ð‘ð‘’ð‘“â„Žð‘– .432ð‘Žð‘ð‘ð‘‘ð‘“ .734ð‘Žð‘ð‘ð‘‘ð‘’ð‘“â„Žð‘– .353ð‘ð‘ð‘’ð‘“â„Ž .485ð‘Žð‘ð‘ð‘‘ .553ð‘Žð‘ð‘ð‘‘ð‘’â„Žð‘– .510\nâ„Ž setwise.heapsort .757ð‘Žð‘ð‘ð‘‘ .352ð‘Žð‘ð‘ð‘‘ .283ð‘ .428ð‘Žð‘ð‘ð‘‘ð‘“ .677ð‘ .314 .465 ð‘Žð‘ .520ð‘Žð‘ð‘ð‘‘ .475\nð‘– setwise.bubblesort.756ð‘Žð‘ð‘ð‘‘ .353ð‘Žð‘ð‘ð‘‘ .330ð‘ð‘â„Ž .438ð‘Žð‘ð‘ð‘‘ð‘“â„Ž .691ð‘ .362ð‘Žð‘ð‘ð‘’ð‘“â„Ž .497ð‘Žð‘ð‘ð‘‘ .537ð‘Žð‘ð‘ð‘‘â„Ž .496\nFlan-t5-xxl\nð‘ pointwise.qlm .707ð‘Ž .342ð‘Žð‘ .188 .324 .712 ð‘ .307ð‘ .431 .440 ð‘Ž .431\nð‘ pointwise.yes_no .691ð‘Ž .322 .240 ð‘ .305 .623 .274 .392 .515 ð‘Žð‘ .420\nð‘‘ listwise.generation.664ð‘Ž .344ð‘Žð‘ .453ð‘ð‘ð‘’ð‘“â„Žð‘– .441ð‘Žð‘ð‘ð‘’ð‘“ð‘”â„Žð‘– .736ð‘Žð‘ .353ð‘ð‘ð‘’ð‘“â„Ž .458ð‘Žð‘ .495ð‘Žð‘ .493\nð‘’ listwise.likelihood .749ð‘Žð‘ð‘‘ .352ð‘Žð‘ .307ð‘ð‘ .416ð‘Žð‘ð‘â„Ž .725ð‘Žð‘ .316ð‘ .479ð‘Žð‘ð‘ .518ð‘Žð‘ð‘‘ .483\nð‘“ pairwise.heapsort .738ð‘Žð‘ð‘‘ .359ð‘Žð‘ð‘ð‘‘â„Žð‘– .324ð‘ð‘ .407ð‘Žð‘ð‘ .744ð‘Žð‘ð‘ .328ð‘ .487ð‘Žð‘ð‘ .543ð‘Žð‘ð‘ð‘‘ð‘’â„Ž .491\nð‘” pairwise.bubblesort.733ð‘Žð‘‘ .363ð‘Žð‘ð‘ð‘‘ð‘’â„Žð‘– .423ð‘ð‘ð‘’ð‘“â„Ž .421ð‘Žð‘ð‘ð‘“â„Ž .756ð‘Žð‘ð‘ð‘‘ð‘’â„Ž .355ð‘ð‘ð‘’ð‘“â„Ž .490ð‘Žð‘ð‘ð‘‘ .550ð‘Žð‘ð‘ð‘‘ð‘’â„Žð‘– .511\nâ„Ž setwise.heapsort .752ð‘Žð‘ð‘ð‘‘ .346ð‘Žð‘ .297ð‘ð‘ .402ð‘Žð‘ð‘ .726ð‘Žð‘ .321ð‘ .473ð‘Žð‘ð‘ .513ð‘Žð‘ .479\nð‘– setwise.bubblesort.768ð‘Žð‘ð‘ð‘‘ð‘“ð‘” .346ð‘Žð‘ .388ð‘ð‘ð‘’ð‘“â„Ž .424ð‘Žð‘ð‘ð‘“â„Ž .754ð‘Žð‘ð‘ð‘’â„Ž .343ð‘ð‘ð‘’â„Ž .479ð‘Žð‘ð‘ .534ð‘Žð‘ð‘‘ð‘’â„Ž .505\nIn Table 3, we present the zero-shot ranking effectiveness of\nall methods (with the exception of pairwise.allpair due to its com-\nputationally intensive nature) across 8 widely-used BEIR datasets.\nNotably, we identify several different trends that deviate from ob-\nservations made on the TREC DL datasets.\nFirstly,pointwise.qlm exhibits a slightly higher average NDCG@10\nscore compared to pointwise.yes_no. Moreover, the effectiveness of\npointwise.qlm remains stable even as the model size increases. Sec-\nondly, listwise.generation demonstrates comparable effectiveness to\nlistwise.likelihood, with the majority of gains obtained in the Touche\ndataset, where other methods perform worse. Lastly, both Pairwise\nand Setwise methods that leverage the bubble sort algorithm consis-\ntently demonstrate higher average NDCG@10 compared to when\nthey utilize the heap sort algorithm, regardless of the model size.\nOverall, the variety of results we observe across different experi-\nmental settings shows the importance of not drawing conclusions\nabout effectiveness from single datasets or model sizes.\nWe note that if the LLM output logits are accessible, our Setwise\napproaches can also utilize these logits to estimate the likelihood\nof the most relevant document label. This approach eliminates the\nneed for token generation, requiring only a single LLM forward\ninference to yield the output results, thus avoiding the generation\nof unexpected tokens. Surprisingly, in our experiments we find that\nusing model logits for ourSetwise approaches resulted in no change\nin ranking effectiveness when compare to generation, suggesting\nthat the inference of our Setwise approaches that fully relies on\ntoken generation is very robust.\n5.2 Efficiency Results\nRegarding computational and runtime efficiency, the results pre-\nsented in Table 2 indicate that bothPointwise methods exhibit fewest\ninference, prompt tokens, and no generated tokens. Furthermore,\ntheir computational efficiency and query latency are optimized due\nto efficient GPU-based batched inference. It is worth noting, how-\never, that these methods do come with certain limitations. Specifi-\ncally, they require access to the model output logits (thus currently\nlimiting their use to just open source LLMs) and are less effective\nwhen used with larger models. In contrast, pairwise.allpair appears\nto be the most expensive method that consumes the most number\nof prompt tokens and generated tokens due to the large number of\ndocument pair preferences needed to be inferred. Hence, even with\nGPU batching, pairwise.allpair still has the worst query latency. In\ncontrast, approaches utilizing our Setwise promptingâ€”namely, list-\nwise.likelihood, setwise.heapsort, and setwise.bubblesort, are far more\nefficient than their counterparts, listwise.generate, pairwise.heapsort,\nand pairwise.bubblesort respectively. Notably, these improvements\nare achieved without compromising effectiveness. Section 5.3 will\ndiscuss further approaches on improving efficiency.\nThe setwise.bottlesort and pairwise.heapsort methods show com-\nparable NDCG@10, but pairwise.heapsort is cheaper. On the other\nhand, our setwise.heapsort provides a reduction of â‰ˆ62% in cost by\nonly marginally reducing NDCG@10 (a 0.8% loss).\n5.3 Effectiveness and Efficiency Trade-offs\nOur Setwise prompting is characterized by a hyperparameter ð‘\ncontrolling the number of compared documents within the prompt\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon\n(a) Setwise\n (b) Listwise\nFigure 3: Effectiveness and efficiency trade-offs offered by different approaches. (a â€“ Setwise): The numbers in the scatter plots represent the\nnumber of compared documents ð‘at each step of the sorting algorithm. (b â€“ Listwise) The numbers in the scatter plots represent the number of\nsliding windows repetitions ð‘Ÿ.\nfor each step in the sorting algorithms. In the previous experiments,\nwe always set ð‘ = 3. Adjusting this hyperparameter allows one\nto further enhance efficiency by incorporating more compared\ndocuments into the prompt, thereby reducing the number of LLM\ninference calls. However, we acknowledge that there is an input\nlength limitation to LLMs (in our experiments this is 512 prompt\ntokens) and setting ð‘ to a large value may require more aggressive\ndocument truncation, likely impacting effectiveness.\nTo investigate the trade-off between effectiveness and efficiency\ninherent in our Setwise approach, we set ð‘ = 3,5,7,9 while truncat-\ning the documents in the prompt to 128,85,60,45 tokens, respec-\ntively. This reduction in document length is necessary to ensure\nprompt size is not exceeded. The NDCG@10, along with query\nlatency for all models while varying ð‘, is visualized in Figure 3a for\nthe TREC DL datasets. As expected, larger ð‘ reduces query latency\nbut often degrades effectiveness. Notably, the heap sort algorithm\nconsistently proves more efficient than bubble sort. For instance,\nwith Flan-t5-xl andð‘ = 9, heap sort achieves strong NDCG@10 with\na query latency of â‰ˆ3 seconds. When compared to the other meth-\nods outlined in Table 2, this represents the lowest query latency,\nexcept for the Pointwise approaches with Flan-t5-large, albeit with\nsuperior ranking effectiveness. Itâ€™s worth noting that the ranking\neffectiveness decline with larger ð‘ values could also be attributed\nto the increased truncation of passages. LLMs with extended input\nlength capacity might potentially yield improved ranking effective-\nness for larger ð‘. This area warrants further exploration in future\nstudies.\nSimilarly, theListwise balance effectiveness and efficiency through\nthe adjustment of the repetition count ð‘Ÿ for the sliding window. In\nour prior experiments, we consistently set ð‘Ÿ = 5 to ensure that at\nleast 10 of the most relevant documents can be brought to the top.\nIn Figure 3b, we investigate the influence of varying ð‘Ÿ on Listwise\napproaches. Latency exhibits a linear relationship with ð‘Ÿ, which\naligns with expectations. A larger value of ð‘Ÿ can enhance the effec-\ntiveness of listwise.generate, and beyond ð‘Ÿ > 5, the improvement\nlevels off. Conversely, the listwise.likelihood approach, which lever-\nages our Setwise prompting, showcases notably higher effectiveness\nand efficiency. Even with a small value of ð‘Ÿ the performance of list-\nwise.likelihood exceeds that of listwise.generate, with the highest\nperformance achieved around ð‘Ÿ = 5.\n5.4 Sensitivity to the Initial Ranking\nThe ranking effectiveness of the originalListwise and Pairwise meth-\nods is influenced by the initial ranking order [21, 23]. To investigate\nthis aspect in relation to our approach, we consider different order-\nings of the initial BM25 list; specifically, 1) initial BM25 ranking; 2)\ninverted BM25 ranking; and 3) random shuffled BM25 ranking. Each\nof these initial rankings was used to test different reranking meth-\nods using Flan-t5-large. The results are presented in Figure 4. Differ-\nent initial ranking orders negatively impact listwise.generate, pair-\nwise.heapsort and pairwise.bubblesort; pairwise.heapsort is the most\nrobust method. These findings align with the literature [21, 23].\nIn contrast, Setwise prompting is far more robust to variations in\nthe initial ranking order. Bothlistwise.likelihood and setwise.bubblesort\nexhibit large improvements overlistwise.generate and pairwise.bubblesort,\nin the case of the inverted BM25 ranking and randomly shuffled\nBM25 ranking. Moreover, they demonstrate a similar level of robust-\nness to pairwise.heapsort. This leads us to the conclusion that our\nSetwise prompting approach substantially enhances the zero-shot\nre-ranking with LLMs in relation to the initial ranking.\n5.5 Effectiveness and Costs of other LLMs\nIn the previous sections, we only used Flan-T5 as the backbone LLM.\nFlan-T5 is a transformer encoder-decoder model. In this section, to\nbetter understand the impact of different models, we investigate\npopular transformer decoder-only LLMs (open-sourced: llama2-\nchat-7b2 [28], vicuna-13b-v1.53 [35]; closed-source: OpenAI gpt-\n3.5-turbo-11064) on DL2019 and DL2020. For open-source models\n2https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n3https://huggingface.co/lmsys/vicuna-13b-v1.5\n4https://platform.openai.com/docs/models/gpt-3-5\nA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nNDCG@10\n0.1\n0.3\n0.5\n0.7\nlistwise.generate listwise.likelihood pairwise.heapsort setwise.heapsort pairwise.bubblesort setwise.bubblesort\nBM25 InverseBM25 RandomBM25\n(a) TREC DL 2019\nNDCG@10\n0.1\n0.3\n0.5\n0.7\nlistwise.generate listwise.likelihood pairwise.heapsort setwise.heapsort pairwise.bubblesort setwise.bubblesort\nBM25 InverseBM25 RandomBM25\n(b) TREC DL 2020\nFigure 4: Sensitivity to the initial ranking. We use Flan-t5-large and ð‘ = 4 for the Setwise approach.\nTable 4: Results obtained with other LLMs on TREC DL datasets: We\nreport the cost in terms of query latency in seconds (s) if the LLMâ€™s\nweights are publicly available, or the API call costs in US dollars ($)\nif the LLM is only accessible via API calls.\nTREC DL 2019 TREC DL 2020\n# Methods NDCG@10 Cost(s or $)NDCG@10 Cost(s or $ )\nllama2-chat-7b\nð‘Ž listwise.generation.508 144.9 s .475ð‘ 143.9 s\nð‘ pairwise.bubblesort.538ð‘Žð‘ 64.3 s .503ð‘Žð‘ 61.5 s\nð‘ pairwise.heapsort.465 18.3 s .416 17.9 s\nð‘‘ setwise.bubblesort.578ð‘Žð‘ð‘ 38.2 s .530ð‘Žð‘ 38.5 s\nð‘’ setwise.heapsort.568ð‘ 10.9s .514ð‘ 10.9s\nvicuna-13b\nð‘Ž listwise.generation.639 225.9 s .608 226.3 s\nð‘ pairwise.bubblesort.612 166.6 s .592 169.0 s\nð‘ pairwise.heapsort.617 46.0 s .582 45.6 s\nð‘‘ setwise.bubblesort.622 57.6 s .602 58.8 s\nð‘’ setwise.heapsort.659ð‘ð‘ð‘‘ 16.7s .583 16.4s\ngpt-3.5\nð‘Ž listwise.generation.712 0.045 $ 67.2 0.046 $\nð‘ pairwise.heapsort.694 0.171 $ 65.1 0.169 $\nð‘ setwise.bubblesort.699 0.084 $ 65.9 0.088 $\nð‘‘ setwise.heapsort.693 0.029$ 65.6 0.028$\nwe measure cost in terms of query latency in seconds (s); for closed\nmodels we measure the API call costs in US dollars ($).5 Results are\npresented in the Table 4.\nFlan-T5 in our previous results is a better backbone than Llama2\nand Vicuna, regardless of ranking method. Notably,Setwise exhibits\nthe best overall performance when considering these models, show-\ncasing its robustness. For gpt-3.5-turbo (closed model), we compared\nSetwise and Listwise using 10 documents at the time (ð‘ = 10), as this\nLLM has a longer input context limit; for Listwise, we set window\nsize 10, step size of 5 and repeat sorting twice for fair comparison\nwith Setwise. Listwise achieves the highest effectiveness; however,\nSetwise achieves similar effectiveness (no significant differences)\nbut at only half the cost.\nWe note that there could be potential data contamination with\ninstruction-tuned LLMs: during the instruction fine-tuning tasks,\nthese models could have been fine-tuned on the MS MARCO or\n5We exclude Pointwise approaches as they are less effective, and pairwise.bubblesort\nfor gpt-3.5-turbo as it is too costly.\nBEIR datasets. Although the document ranking task and the rank-\ning prompts used in this paper are unlikely to be part of the in-\nstruction fine-tuning dataset used for these LLMs, we acknowledge\nthat data contamination could still artificially impact the effective-\nness of the considered LLMs in ranking tasks. However, since we\nconducted experiments with different methods based on the same\ninstruction-tuned model, we believe our empirical comparison still\noffers insights for the practical use of LLM-based re-rankers.\n6 CONCLUSION\nWe undertook a comprehensive study of existing LLM-based zero-\nshot document ranking methods, employing strict and consistent\nexperimental conditions. Our primary emphasis was on evaluating\nboth their ranking effectiveness and their efficiency in terms of\ncomputational efficiency and runtime latency â€” factors that are\noften disregarded in previous studies. Our findings unveil some\nunforeseen insights, and effectiveness-efficiency trade-offs between\ndifferent methods. This information equips practitioners with valu-\nable guidance when selecting the most appropriate method for their\nspecific applications.\nTo further boost efficiency of LLM-based zero-shot document\nranking, we introduced an innovative Setwise prompting strategy.\nSetwise has the potential to enhance both effectiveness and effi-\nciency for Listwise approaches provided the model logits are ac-\ncessible. Setwise also notably enhances the efficiency of sorting-\nbased Pairwise approaches. Furthermore, Setwise prompting offers\na straightforward way to balance effectiveness and efficiency by\nincorporating more documents for comparison in the prompt. Ad-\nditionally, approaches equipped with Setwise prompting demon-\nstrated strong robustness to variation in the initial retrieval set used\nfor reranking.\nFuture work should focus on evaluating the Setwise prompting\napproach on a wider array of LLMs, including LLaMA models [26,\n27] as well as the OpenAI LLM APIs. Additionally, recent advanced\nself-supervised prompt learning techniques [7, 34] could be used\nto refine the Setwise approach.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon\nREFERENCES\n[1] Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag.\n2022. Large language models are zero-shot clinical information extractors. arXiv\npreprint arXiv:2205.12689 (2022).\n[2] Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau,\nNicolas Chapados, and Siva Reddy. 2024. LLM2Vec: Large Language Models Are\nSecretly Powerful Text Encoders. arXiv:2404.05961 [cs.CL]\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877â€“1901.\n[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311 (2022).\n[5] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview\nof the TREC 2020 deep learning track. arXiv preprint arXiv:2102.07662 (2021).\n[6] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M\nVoorhees. 2020. Overview of the TREC 2019 deep learning track. arXiv preprint\narXiv:2003.07820 (2020).\n[7] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and\nTim RocktÃ¤schel. 2023. Promptbreeder: Self-Referential Self-Improvement Via\nPrompt Evolution. arXiv preprint arXiv:2309.16797 (2023).\n[8] Lukas Gienapp, Maik FrÃ¶be, Matthias Hagen, and Martin Potthast. 2022. Sparse\nPairwise Re-Ranking with Pre-Trained Transformers. In Proceedings of the 2022\nACM SIGIR International Conference on Theory of Information Retrieval (Madrid,\nSpain) (ICTIR â€™22) . ACM, New York, NY, USA, 72â€“80. https://doi.org/10.1145/\n3539813.3545140\n[9] Donald Ervin Knuth. 1997. The art of computer programming . Vol. 3. Pearson\nEducation.\n[10] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. 2022. Large language models are zero-shot reasoners. Advances in\nneural information processing systems 35 (2022), 22199â€“22213.\n[11] Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole,\nKai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik\nDuddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati,\nPrateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar\nNaim. 2024. Gecko: Versatile Text Embeddings Distilled from Large Language\nModels. arXiv:2403.20327 [cs.CL]\n[12] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.\n2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n(2022).\n[13] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep,\nand Rodrigo Nogueira. 2021. Pyserini: A Python Toolkit for Reproducible Infor-\nmation Retrieval Research with Sparse and Dense Representations. InProceedings\nof the 44th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (Virtual Event, Canada) (SIGIR â€™21) . ACM, New York, NY,\nUSA, 2356â€“2362. https://doi.org/10.1145/3404835.3463238\n[14] Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021. Pretrained Transformers\nfor Text Ranking: BERT and Beyond. arXiv:2010.06467 [cs.IR]\n[15] Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-Shot\nListwise Document Reranking with a Large Language Model. arXiv preprint\narXiv:2305.02156 (2023).\n[16] Aliaksei Mikhailiuk, Clifford Wilmot, Maria Perez-Ortiz, Dingcheng Yue, and\nRafal Mantiuk. 2021. Active Sampling for Pairwise Comparisons via Approximate\nMessage Passing and Information Gain Maximization. In 2020 IEEE International\nConference on Pattern Recognition (ICPR) .\n[17] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Docu-\nment Ranking with a Pretrained Sequence-to-Sequence Model. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020 . 708â€“718.\n[18] Jay M Ponte and W Bruce Croft. 2017. A language modeling approach to in-\nformation retrieval. In ACM SIGIR Forum , Vol. 51. ACM New York, NY, USA,\n202â€“208.\n[19] Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin. 2021. The expando-mono-duo\ndesign pattern for text ranking with pretrained sequence-to-sequence models.\narXiv preprint arXiv:2101.05667 (2021).\n[20] Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023. RankVicuna:\nZero-Shot Listwise Document Reranking with Open-Source Large Language\nModels. arXiv preprint arXiv:2309.15088 (2023).\n[21] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen,\nTianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al. 2023. Large language\nmodels are effective text rankers with pairwise ranking prompting.arXiv preprint\narXiv:2306.17563 (2023).\n[22] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau\nYih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval\nwith Zero-Shot Question Generation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing . Association for Computational\nLinguistics, Abu Dhabi, United Arab Emirates, 3781â€“3797. https://doi.org/10.\n18653/v1/2022.emnlp-main.249\n[23] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun\nRen. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as\nRe-Ranking Agent. arXiv preprint arXiv:2304.09542 (2023).\n[24] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste\nAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,\net al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 (2023).\n[25] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna\nGurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of\nInformation Retrieval Models. In Thirty-fifth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track (Round 2) .\n[26] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[27] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[28] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv\npreprint arXiv:2307.09288 (2023).\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You\nNeed. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems (Long Beach, California, USA) (NIPSâ€™17). Curran Associates\nInc., Red Hook, NY, USA, 6000â€“6010.\n[30] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\nRangan Majumder, and Furu Wei. 2024. Text Embeddings by Weakly-Supervised\nContrastive Pre-training. arXiv:2212.03533 [cs.CL]\n[31] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and\nFuru Wei. 2024. Improving Text Embeddings with Large Language Models.\narXiv:2401.00368 [cs.CL]\n[32] Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon. 2023. Can\nChatGPT Write a Good Boolean Query for Systematic Review Literature Search?.\nIn Proceedings of the 46th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (Taipei, Taiwan) (SIGIR â€™23) . ACM, New\nYork, NY, USA, 1426â€“1436. https://doi.org/10.1145/3539618.3591703\n[33] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian\nLester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned Language Models\nare Zero-Shot Learners. In International Conference on Learning Representations .\n[34] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou,\nand Xinyun Chen. 2023. Large language models as optimizers. arXiv preprint\narXiv:2309.03409 (2023).\n[35] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-\nBench and Chatbot Arena. arXiv:2306.05685 [cs.CL]\n[36] Shengyao Zhuang, Hang Li, and Guido Zuccon. 2021. Deep query likelihood\nmodel for information retrieval. In Advances in Information Retrieval: 43rd Euro-\npean Conference on IR Research, ECIR 2021, Virtual Event, March 28â€“April 1, 2021,\nProceedings, Part II 43 . Springer, 463â€“470.\n[37] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. 2023. Open-\nsource Large Language Models are Strong Zero-shot Query Likelihood Models for\nDocument Ranking. In Findings of the Association for Computational Linguistics:\nEMNLP 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for\nComputational Linguistics, Singapore, 8807â€“8817. https://doi.org/10.18653/v1/\n2023.findings-emnlp.590\n[38] Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido\nZuccon. 2024. PromptReps: Prompting Large Language Models to Gener-\nate Dense and Sparse Representations for Zero-Shot Document Retrieval.\narXiv:2404.18424 [cs.IR]\n[39] Shengyao Zhuang and Guido Zuccon. 2021. TILDE: Term independent likelihood\nmoDEl for passage re-ranking. In Proceedings of the 44th International ACM SIGIR\nConference on Research and Development in Information Retrieval . 1483â€“1492.",
  "topic": "Pointwise",
  "concepts": [
    {
      "name": "Pointwise",
      "score": 0.8972310423851013
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.8007478713989258
    },
    {
      "name": "Computer science",
      "score": 0.6693911552429199
    },
    {
      "name": "Pairwise comparison",
      "score": 0.6037107706069946
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.588891863822937
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5546280741691589
    },
    {
      "name": "Machine learning",
      "score": 0.476578950881958
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4300844669342041
    },
    {
      "name": "Mathematics",
      "score": 0.19685637950897217
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1292875679",
      "name": "Commonwealth Scientific and Industrial Research Organisation",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I165143802",
      "name": "The University of Queensland",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}