{
  "title": "Counterfactual reasoning: Testing language models’ understanding of hypothetical scenarios",
  "url": "https://openalex.org/W4385571244",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100333831",
      "name": "Jiaxuan Li",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A5100562361",
      "name": "Lang Yu",
      "affiliations": [
        "Seattle University"
      ]
    },
    {
      "id": "https://openalex.org/A5012068726",
      "name": "Allyson Ettinger",
      "affiliations": [
        "Seattle University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4298149550",
    "https://openalex.org/W4225573578",
    "https://openalex.org/W1990056467",
    "https://openalex.org/W2970453125",
    "https://openalex.org/W4226251122",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W2952984539",
    "https://openalex.org/W1993979041",
    "https://openalex.org/W4302305823",
    "https://openalex.org/W3159900299",
    "https://openalex.org/W2003386981",
    "https://openalex.org/W3102749280",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 804–815\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCounterfactual reasoning: Testing language models’ understanding of\nhypothetical scenarios\nJiaxuan Li\nUniversity of California Irvine\nIrvine, CA 92617\njiaxuan.li@uci.edu\nLang Yu\nMeta\nSeattle, W A 98109\nlangyu@fb.com\nAllyson Ettinger\nUniversity of Chicago\nChicago, IL 60637\naettinger@uchicago.edu\nAbstract\nCurrent pre-trained language models have en-\nabled remarkable improvements in downstream\ntasks, but it remains difficult to distinguish ef-\nfects of statistical correlation from more sys-\ntematic logical reasoning grounded on the un-\nderstanding of real world. We tease these fac-\ntors apart by leveraging counterfactual condi-\ntionals, which force language models to pre-\ndict unusual consequences based on hypothet-\nical propositions. We introduce a set of tests\nfrom psycholinguistic experiments, as well as\nlarger-scale controlled datasets, to probe coun-\nterfactual predictions from five pre-trained lan-\nguage models. We find that models are consis-\ntently able to override real-world knowledge\nin counterfactual scenarios, and that this ef-\nfect is more robust in case of stronger baseline\nworld knowledge—however, we also find that\nfor most models this effect appears largely to\nbe driven by simple lexical cues. When we\nmitigate effects of both world knowledge and\nlexical cues to test knowledge of linguistic nu-\nances of counterfactuals, we find that only GPT-\n3 shows sensitivity to these nuances, though\nthis sensitivity is also non-trivially impacted by\nlexical associative factors.1\n1 Introduction\nReasoning plays a central role in human commu-\nnication (Frank and Goodman, 2012). While lan-\nguage models have demonstrated remarkable ca-\npacity on downstream tasks (Devlin et al., 2019;\nRadford et al., 2019; Liu et al., 2019), it remains\nunclear to what extent predictions generated by\nlanguage models are consequences of correlation\nwith linguistic heuristics in the context, versus ro-\nbust reasoning about causal relations grounded on\nunderstanding of world knowledge.\nIn this paper we leverage counterfactual condi-\ntionals to investigate the capacity of pre-trained\n1Data and code available at https://github.com/\ngoldengua/Counterfactual_Inference_LM.\nLMs (PLMs) to distinguish hypothetical scenarios\nfrom reality, and to examine how this interacts with\nmodels’ use of existing real world knowledge as\nwell as shallower associative cues. Counterfactuals\nconsist of a premise which is false in the real world\nbut true in the hypothetical world (e.g., “If cats\nwere vegetarians”), and an imaginary consequence\nof this premise (“cats would love cabbages”). Test-\ning language models with counterfactuals allows\nus to use language to manipulate what is true and\nwhat is hypothetical, and to test models’ ability to\nseparate and use this information for predictions.\nPrevious work has established the use of counter-\nfactual scenarios to probe inference ability (Qin\net al., 2019; Zellers et al., 2019; Mostafazadeh\net al., 2016; Meng et al., 2022; Rajani et al., 2019;\nSaparov and He, 2022; Frohberg and Binder, 2021;\nElazar et al., 2021; Rudinger et al., 2020), but the\ndatasets lack systematic control of lexical cues and\nworld knowledge, which makes it likely that the\nperformance could be attributable to spurious cues\nin the datasets (Niven and Kao, 2019).\nFor our tests we draw on and adapt inputs from\nexisting psycholinguistic experiments. We be-\ngin by testing models’ ability to override exist-\ning world knowledge when the context indicates\nthat the correct completion involves a hypothetical\nworld (e.g., “if cats were vegetarian, cats would\nlove cabbages/fish”). We test five popular PLMs,\nand find that models can increase their preference\nfor counterfactual completions given counterfac-\ntual context—however, most models rely strongly\non simple lexical cues. Next we control the ef-\nfect of real world knowledge and lexical triggers,\nto test models’ understanding of what counterfac-\ntual language implies about the world state. We\nfind that most models fail to understand real-world\nimplications of counterfactuals and largely rely\non lexical triggers—with the exception of GPT-\n3, which shows greater sophistication, but contin-\nues to show non-trivial susceptibility to interfer-\n804\nence from lexical-associative cues. We discuss the\nimplications and possible interpretations of these\nfindings with respect to linguistic competence and\npredictive strategies of these models.\n2 Exp1: overriding world knowledge\nOur first experiment investigates whether LMs are\nable to take a counterfactual scenario and predict\na counterfactual-consistent completion that contra-\ndicts general world knowledge.\nItems We draw directly on counterfactual stim-\nuli from the psycholinguistic study of Ferguson\nand Sanford (2008). There are 128 items from\nthe original psycholinguistic experiments, and we\nsynthetically generate 10,720 additional items (see\nAppendix A.2 for illustration of data generation\nprocess). We match target nouns and syntactic\nconstructions across conditions in order to control\nlexical properties that influence language models’\npredictions. Table 1 shows example items from the\nsynthetic large-scale dataset (see Section A.1 for\nexample items from the small-scale dataset).\nCondSentence\nCW If cats were vegetarians, people would love them.\nFamilies would feed cats withfish/cabbages.\nRW Because cats are carnivores, people love them.\nFamilies would feed cats withfish/cabbages.\nBB Families would feed cats withfish/cabbages\nTable 1: Exp1 items (logical completion underlined).\nFigure 1: Illustration of Exp1 set-up\nThe experiment includes two key conditions:\nCounterfactual-World (CW) and Real-World (RW)\n(Fig. 1). The CW condition presents a counterfac-\ntual scenario, e.g., in which cats are vegetarians.\nThe logical target completion in this example is\n“cabbages”, but because in reality cats are more\nlikely to eat fish, this contradicts world knowledge.\nBy contrast, in the RW condition the logical com-\npletion is consistent with the real world (“feed cats\nwith fish”). We also include one Baseline Bias (BB)\ncondition, for a more direct test of the strength of\nmodels’ baseline preference for each completion.\nExperiments We test counterfactual reasoning\nin five pre-trained language models. We include au-\ntoregressive transformers in the GPT family (GPT-\n2 (Radford et al., 2019) and GPT-3 (Brown et al.,\n2020)) and masked language models in the BERT\nfamily (BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019) and MPNet (Song et al., 2020)) 2.\nWe test models by comparing the log-probability\nthat each model assigns to the CW-congruent (“cab-\nbages”) and RW-congruent (“fish”) completions\ngiven the contexts. For all conditions, we compute\nthe percentage of items in which the CW-congruent\ncontinuation has a higher probability than the RW-\ncongruent continuation. This means that in RW\nand BB conditions, lower values reflect better pre-\ndictions, since the CW-congruent completion is the\nless logical completion in these conditions.\nModel Small-scale Large-scale\nCW RW BB CW RW BB\nGPT2 53.1 34.4 40.6 53.7 29.5 31.5\nGPT3 68.8 18.8 18.7 71.3 2.5 14.7\nBERT 46.9 43.8 31.2 34.2 14.3 35.2\nRoBERTa 53.1 21.9 21.9 61.4 26.5 47.2\nMPNet 50.0 21.9 21.9 66.9 15.6 36.6\nTable 2: Percentage of preference for CW-congruent\ncompletion (e.g., “cabbages”) in Exp1. In the CW con-\ndition, higher values reflect better predictions. In RW\nand BB conditions, lower values reflect better predic-\ntions.\nResults Table 2 shows the preferences for CW-\ncongruent completions across all models and con-\nditions, for the small-scale hand-designed items\nfrom the psycholinguistic experiment, and for the\nlarge-scale synthetic items. 3 We see that all mod-\n2We used the smallest uncased variants of GPT-2, BERT,\nRoBERTa, and MPNet, and we used the text-davinci-003 vari-\nant of GPT-3 via API request. Experiments were conducted\nfrom April to August 2022.\n3A potential concern with aggregated percentages shown\nin Table 2 and Table 6 is that given a specific instance, a model\nmay assign a higher probability to a CW-congruent continu-\nation in the CW condition because it incorrectly predicts the\ncorresponding BB/RW item. This concern is mitigated by the\nfact that we focus our conclusions on the difference between\nthe CW and RW conditions, rather than the accuracies in the\n805\nels show stronger preference for CW-congruent\ncontinuations in the counterfactual (CW) context\nthan in the other conditions (though in the case of\nBERT on the small-scale data, this difference is\nnegligible). All models show below-chance pref-\nerence for CW-congruent continuations in the RW\ncondition—which means above-chance preference\nfor the correct RW-congruent continuations. How-\never, though all model preferences for the correct\nCW-congruent continuation are higher in the CW\ncondition than in the RW condition, even in the\nCW condition the preference for CW-congruent\nconditions is at best slightly above chance for most\nmodels. The exception is GPT-3, which is the only\nmodel to prefer the CW-congruent continuation in\ngreater than 70% of items.\nWe also see that GPT-3 shows exceptionally\nstrong performance on both BB and CW condi-\ntions. This suggests, slightly counterintuitively,\nthat stronger grasp of relevant world knowledge\nmay in fact be associated with models more ef-\nfectively overriding that knowledge in a counter-\nfactual. To investigate this effect further, we ex-\namine the impact of world knowledge at the item\nlevel. We quantify strength of world knowledge\nas the difference between models’ log-probability\nof CW- and RW-congruent continuations for a\ngiven item in the BB condition, and the strength\nof counterfactual preference as the difference be-\ntween log-probability of CW- and RW-congruent\ncontinuations for a given item in the CW condi-\ntion. We then compute the Pearson correlation\nbetween these strength measures. We find a signif-\nicant correlation between the robustness of world\nknowledge encoding and strength of counterfactual\npreference in the CW condition across all language\nmodels (see Appendix A.3), further supporting\na relationship between strength of world knowl-\nedge and counterfactual sensitivity. While previ-\nous work has suggested that large language models\nmay have difficulty avoiding memorized texts when\nexplicitly prompted to end famous quotes differ-\nently (McKenzie et al., 2022), our results suggest\nthat world knowledge may in fact facilitate reason-\ning when accompanied with clear structural cues\n(e.g. “if”). To better understand how world knowl-\nedge informs language models’ predictions and in-\nCW condition alone. However, to further address this concern,\nwe calculate the proportion of items in which the model shows\nthe correct preference in both CW and RW conditions. The\nresults are presented in Section A.5 and suggest a comparable\npattern in terms of relative model strengths.\nference, it will be important to continue expanding\nthe scale of tests and more carefully operationalize\ndefinitions of world knowledge in future work.\n3 Exp2: impact of cue words in context\nThe first experiment suggests that models can to\nan extent override world knowledge given a coun-\nterfactual, particularly in cases when models have\na strong handle on the relevant world knowledge.\nHowever, it is possible that in these tests the mod-\nels were not relying on sophisticated understanding\nof counterfactuals, but rather on simple lexical trig-\ngers in context. Consider, for instance, that models\ncould perform well in Exp1 if they simply increase\ntheir preference for “cabbages” in the proximity of\n“vegetarians”, etc. To test the impact of these lexical\ntriggers, we incorporate an additional condition.\nItems Table 3 and Fig. 2 show a sample item and\nillustration of experimental set-up with the new\nadded condition. In this Counterfactual-to-Reality\n(CR) condition, models see the same counterfactual\ncontext, but the subsequent sentence references ac-\ntual reality. So the correct completion is consistent\nwith reality, but inconsistent with the lexical trigger\n(“vegetarians”). We generate sentences in the CR\ncondition by modifying CW sentences to include\nthe discourse connective “In reality” and to include\npresent tense in the second sentence.\nCondSentence\nCR If cats were vegetarians, people would love them.\nIn reality, families feed cats withfish/cabbages.\nTable 3: Exp2 items (logical completion underlined).\nFigure 2: Illustration of Exp2 set-up.\n806\nExperiments As above, we calculate percentage\nof items in which models prefer the CW-congruent\ncontinuations. Models relying on information be-\nyond simple lexical triggers should show a sharp\ndrop in preference for the CW-congruent comple-\ntion in the CR condition, where the correct comple-\ntion should align with real world information.\nResults Table 4 shows the results. We see that\nmost models show non-zero drop between CW and\nCR conditions—however, for most models this re-\nduction is minor. It is only GPT-3 that shows a\ntruly substantial drop in CW-congruent preference,\nand only in the large-scale synthetic dataset. This\nsuggests that most models are largely following\nsimpler lexical triggers, while GPT-3 has some-\nwhat greater sensitivity to more detailed linguistic\ncues. Note, however that GPT-3’s relative success\non the synthetic data over the small-scale data may\nrely on larger distance between lexical triggers and\ntarget positions: see Appendix A.4 for evidence on\nGPT-3’s sensitivity to linear distance.\nModel Small-scale Large-scale\nCW CR CW CR\nGPT2 53.1 50.0 53.7 51.9\nGPT3 68.8 56.2 71.3 28.0\nBERT 46.9 46.9 34.2 39.4\nRoBERTa 53.1 37.5 61.4 57.3\nMPNet 50.0 46.9 66.9 58.1\nTable 4: Percentage of preference for CW-congruent\ncompletion (e.g., “cabbages”) in Exp2. In the CW con-\ndition, higher values reflect better predictions. In the\nCR condition, lower values reflect better predictions.\n4 Exp3: Inferring real world state with\ncounterfactual cues\nThe previous experiments indicate that models can\noverride world knowledge in the face of counter-\nfactual evidence, and that the ability to do this im-\nproves with stronger world knowledge—but for\nmost models this performance appears to be driven\nlargely by simple lexical triggers in the context,\nwith the possible exception of GPT-3. In this\nsection we remove the influence of pre-existing\nworld knowledge, and hold constant lexical trig-\ngers across conditions, for a more direct test of\nmodels’ sensitivity to linguistic indicators of coun-\nterfactuals, and what they say about the true state of\nthe world. This task is particularly challenging be-\ncause language models must infer the true state of\nthe world based on the presence of counterfactuals,\nwith lexical cues often being misleading.\nItems We adapt stimuli from a psycholinguis-\ntic study with 96 controlled sentences (Ferguson,\n2012). We additionally create a larger-scale syn-\nthetic dataset with 12,960 sentences, using the\nsame events as the generated dataset from Section 2.\nWe modify the subject noun phrases such that there\nis no influence of existing world knowledge. For\nexample, we modify the subject “cat” to “pet”, so\nthat there is no prior knowledge about the subject’s\npreference for “cabbages” or “fish”. As a result, ex-\nisting world knowledge cannot inform the correct\ncompletion—instead, models need to infer based\non the counterfactual language that the true state of\nthe world is different from what the counterfactual\nstates. Further, we control the lexical items used\nacross different conditions to minimize effects of\nlexical cues on condition differences (see Table 5).\nCond Sentence\nCWC If the pets were vegetarians, people would love\nthem. In fact, people feed the pets with\nfish/cabbages.\nRWCABecause the pets are vegetarians, people love them.\nIn fact, people feed the pets withfish/cabbages.\nBBC In fact, people feed the pets withfish/cabbages.\nTable 5: Exp3 items (logical completion underlined).\nFigure 3: Illustration of Exp3 set-up.\nFig. 3 shows the set-up of conditions. In the\nCounterfactual-World Context (CWC) condition,\nthe scenario described in the first sentence is neu-\ntral with respect to real world knowledge—it is the\nuse of the counterfactual (“if...were”) that tips us\noff that this scenario is not true in reality. The cor-\nrect completion, then, cannot be informed by world\nknowledge, and is also misaligned with the lexical\n807\ntrigger (e.g., “vegetarians”), so models must rely\nspecifically on this implication from the counter-\nfactual in order to perform well.\nIn the Real-World Context Alternative (RWCA)\ncondition, the context uses the same lexical trig-\ngers (“vegetarians”) as the CWC condition. How-\never, because there is no counterfactual language,\nthe logical completion is now the word associated\nwith the lexical trigger (e.g., “cabbages”, associ-\nated with “vegetarians”).\nGiven that the logical completions in CWC and\nRWCA differ, we also compare against a Baseline\nBias Context (BBC) condition, to establish default\nmodel preference for the target factual completion\nin the presence of the new subject noun phrase.\nExperiments We compare proportion of CWC-\ncongruent completions across conditions. Good\nperformance will assign high values in the CWC\ncondition and low values in the RWCA condition.\nModel Small-scale Large-scale\nCWC RWCA BBC CWC RWCA BBC\nGPT2 66.7 66.7 33.3 35.8 32.2 72.6\nGPT3 62.5 33.3 50.0 47.6 32.2 73.8\nBERT 45.8 33.3 50.0 53.0 53.0 71.5\nRoBERTa 50.0 50.0 50.0 35.7 31.3 72.5\nMPNet 37.5 33.3 62.5 41.4 32.3 68.5\nTable 6: Percentage of preference for CWC-congruent\ncompletion (e.g., “fish”) in Exp3. In the CWC condition,\nhigher values reflect better predictions. In the CWCA\ncondition, lower values reflect better predictions. The\nBBC condition establishes models’ default preference\nfor the CWC-congruent completion.\nResults Table 6 shows the results. In the small-\nscale dataset, most models show a similar prefer-\nence in CWC and RWCA, suggesting again that\ntheir predictions are largely driven by lexical trig-\ngers. Only GPT-3 shows substantial difference be-\ntween CWC and RWCA, indicating finer-grained\nsensitivity to counterfactual structures. This sen-\nsitivity is, however, less pronounced in the large-\nscale dataset. Closer inspection suggests that GPT-\n3’s specific success on the small-scale data may\nin fact be attributable to canceling out of lexical\ntriggers: in the small-scale dataset, there are lexical\ntriggers supporting both continuations (see A.1 for\nmore illustration of the characteristics of the small-\nscale dataset), which may cause lexical cues to\ncancel out, enabling more influence from other lin-\nguistic cues. To take one example, the small-scale\ndataset contains the item “If Helen had received\nher student loan, her bank balance would now be\nin credit. When she checked her bank balance she\nwas worried/happy about her finance.” In this\nitem, among the lexical triggers (“student loan”,\n“in credit”, “bank balance”) there are potential asso-\nciations with both the CWC-congruent completion\n“worried” and the CWC-incongruent completion\n“happy”. By contrast, in the large-scale dataset,\nthe major lexical trigger (“vegetarians”) always\nfavors the CWC-incongruent continuation (“cab-\nbages”), causing strong lexical bias against the\nCWC-congruent continuation (see Appendix A.4\nfor further analysis on the role of conflicting lexical\ntriggers and other linguistic factors). This suggests\nthat GPT-3 does show real sensitivity to linguis-\ntic indicators of counterfactuals, but the effect of\nsuperficial lexical cues remains strong.\n5 Conclusion\nThe experiments above have shown that when pre-\nsented with counterfactual situations, PLMs are\nable to prefer completions that conflict with world\nknowledge—and counterintuitively, this sensitivity\nappears better in cases where that world knowledge\nis stronger. Our results also indicate, however, that\nmodels are in large part relying on simple lexical\ncues to inform these preferences. The only model\nthat shows more sophisticated sensitivity to fine-\ngrained linguistic cues separating counterfactuals\nfrom reality is GPT-3—which successfully distin-\nguishes conditions based on counterfactual cues,\nbut nonetheless still shows strong influences from\nlexical associative cues. Why might world knowl-\nedge aid counterfactual sensitivity? Does GPT-3\ntruly understand counterfactuals? One possibil-\nity worth considering is that explanations in both\nof these cases involve volume of exposure. First,\nmodels’ stronger world knowledge for a given fact\nsuggests that models have encountered that fact\nmore often in training—and this may in turn trans-\nlate to more exposure to that type of knowledge in\ncounterfactual contexts, enabling more straightfor-\nward memorization-based performance. Similarly,\nwhile GPT-3 may robustly understand counterfac-\ntuals, the massive data exposure for that model may\nenable a simpler path to success: GPT-3 could sim-\nply have developed lower-level knowledge of how\nlinguistic cues like “If/had” versus “Because” me-\ndiate levels of association between nearby lexical\ncues and later words. We leave investigation of\nthese hypotheses for future work.\n808\nLimitations\nThe datasets in this paper systematically control\nlexical cues and world knowledge between criti-\ncal conditions, allowing us to tease apart the ef-\nfects of statistical heuristics versus reasoning about\ncausal relations. However, the manipulation brings\nunnaturalness to sentences when scaling up into\nlarge-scale synthetic datasets, and constrains the\nlevel of linguistic complexity. As we have seen in\nExp3, the small-scale dataset has more complex\ncombinations of conflicting lexical triggers than\nthe large-scale dataset, causing language models\nto behave differently across datasets. Though we\nfurther address the effects of conflicting lexical\ncues in Appendix A.4, it will be valuable to carry\nout additional investigation of effects of sentence\nnaturalness, and to consider designing large-scale\ndatasets using naturally-occurring data.\nThe study raises and leaves open a number of\ninteresting questions: How exactly might counter-\nfactual reasoning benefit from world knowledge?\nTo what extent does GPT-3’s stronger performance\nreflect robust logical and counterfactual reasoning?\nWhile we lay out some possible explanations in\nthe Conclusion and investigate the role of other\nlinguistic and non-linguistic factors in the above\nexperiments and in the Appendix, we leave addi-\ntional systematic analysis for future work.\nFinally, the experiments use English, in which\ncounterfactual conditionals have distinct and sys-\ntematic linguistic markers relative to other types of\nconditionals. It would be interesting to investigate\nother languages in which counterfactual condition-\nals are not marked linguistically, and require world\nknowledge to disambiguate. For example, a Chi-\nnese conditional could be ambiguous between “if\nit had rained today” and “if it rains today”.\nEthics Statement\nThe datasets were either created and published by\nreseachers in psycholinguistics, or synthetically\ngenerated by the authors without use of harmful\ninformation. No experiments involving human sub-\njects were included in the paper. The authors do\nnot foresee any ethical concerns in this paper.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav\nGoldberg. 2021. Amnesic probing: Behavioral expla-\nnation with amnesic counterfactuals. Transactions of\nthe Association for Computational Linguistics, 9:160–\n175.\nHeather J Ferguson. 2012. Eye movements reveal rapid\nconcurrent access to factual and counterfactual inter-\npretations of the world. Quarterly Journal of Experi-\nmental Psychology, 65(5):939–961.\nHeather J Ferguson and Anthony J Sanford. 2008.\nAnomalies in real and counterfactual worlds: An\neye-movement investigation. Journal of Memory and\nLanguage, 58(3):609–626.\nMichael C Frank and Noah D Goodman. 2012. Predict-\ning pragmatic reasoning in language games. Science,\n336(6084):998–998.\nJörg Frohberg and Frank Binder. 2021. Crass: A novel\ndata set and benchmark to test counterfactual rea-\nsoning of large language models. arXiv preprint\narXiv:2112.11941.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIan McKenzie, Sam Bowman, and Ethan Perez. 2022.\nInverse scaling prize: Second round winners.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual knowl-\nedge in gpt. arXiv preprint arXiv:2202.05262.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and evaluation framework for deeper under-\nstanding of commonsense stories. arXiv preprint\narXiv:1604.01696.\nTimothy Niven and Hung-Yu Kao. 2019. Probing neu-\nral network comprehension of natural language argu-\nments. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4658–4664.\n809\nLianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra\nBhagavatula, Elizabeth Clark, and Yejin Choi. 2019.\nCounterfactual story reasoning and generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 5043–5053.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4932–4942.\nRachel Rudinger, Vered Shwartz, Jena D Hwang, Chan-\ndra Bhagavatula, Maxwell Forbes, Ronan Le Bras,\nNoah A Smith, and Yejin Choi. 2020. Thinking like\na skeptic: Defeasible inference in natural language.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 4661–4675.\nAbulhair Saparov and He He. 2022. Language models\nare greedy reasoners: A systematic formal analysis of\nchain-of-thought. arXiv preprint arXiv:2210.01240.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding. Advances in\nNeural Information Processing Systems, 33:16857–\n16867.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In ACL, pages\n4791–4800.\nA Appendix\nA.1 Example items in small-scale dataset\nTable 7 shows the example items from Exp1 and\nExp2 in the small-scale psycholinguistic datasets,\nand Table 8 shows the example items from Exp3\nin the small-scale dataset. Semantic association be-\ntween the target word and key lexical items in the\ncontext is less salient (e.g. “language skills” and\n“talk”) in the small-scale dataset as compared to the\nassociation in the large synthetic dataset (e.g. “veg-\netarian” and “carrots”). In particular, sentences\nin Exp3 contain lexical triggers that could sup-\nport both CWC-congruent and RWCA-congruent\ncontinuations. For instance, the key lexical items\n(“student loan”, “bank balance”, “in credit”) could\nbe logically associated with either of the feelings\n(“happy” or “worried”).\nCond Sentence\nCW If cats had developed language skills like humans\nit would be interesting to hear what they have to\nsay. Judith would listen to her catmeow/talkand\nthrow balls of wool for it to play with.\nRW If cats are bored and want something to do they\nare usually very good at letting their owners know.\nJudith would listen to her catmeow/talkand throw\nballs of wool for it to play with.\nBB Judith would listen to her catmeow/talkand throw\nballs of wool for it to play with.\nCR If cats had developed language skills like humans\nit would be interesting to hear what they have to\nsay.In reality, Judith listens to her catmeow/talk\nand throws balls of wool for it to play with.\nTable 7: Example Exp1 and Exp2 items in small-scale\ndataset (logical completion underlined).\nCond Sentence\nCWC If Helen had received her first student loan, her\nbank balance would now be in credit. When\nshe checked her bank balance today she was\nworried/happywith her financial situation.\nRWCABecause Helen had received her first student loan,\nher bank balance was now in credit. When she\nchecked her bank balance today she waswor-\nried/happywith her financial situation.\nBBC When she checked her bank balance today she was\nworried/happywith her financial situation.\nTable 8: Example Exp3 items in small-scale dataset\n(logical completion underlined).\nA.2 Generation process of dataset\nWe design our synthetic dataset to parallel the psy-\ncholinguistic stimuli. We design a series of causal\nevent pairs (e.g. “like”/“feed”), and situate these\npairs within counterfactual conditional templates\n(e.g. “if subject1 liked object1, subject2 would feed\nsubject1 with object2”). For each subject/object\nslot, we define a class of nouns satisfying both se-\nlection restriction of the verb and world knowledge.\nFor example, in the template sentence \"if subject1\nliked vegetables, families would feed them with\ncabbages/chicken\", subject1 can be carnivores (e.g.\n“cats/lions/tigers”). We then vary lexical items in\neach subject/object slot, and other linguistic mark-\ners (e.g. modal, tense) in the template. Table 9\nshows examples illustrating the data generation\nfrom a sample event in the CW-condition in Exp1.\nExp2 and Exp3 use the same template and we ma-\nnipulate the syntactic structure or informativity of\nthe subject as described in Section 3 and Section 4.\n810\nCondition Sentence\nOriginal If cats were vegetarians, families\nwould feed them with cabbages.\nSubject1 If dogs were vegetarians, families\nwould feed them with cabbages.\nObject1 If cats weregreens, families would\nfeed them with cabbages.\nSubject2 If cats were vegetarians,breeders\nwould feed them with cabbages.\nObject2 If cats were vegetarians, breeders\nwould feed them withcabbages.\nModal If cats were vegetarians, families\nmightfeed them with cabbages.\nTense If catshad beenvegetarians, families\nwouldhave fedthem with cabbages.\nTable 9: Illustration of data generation process in large-\nscale synthetic dataset. Different sentences can be gener-\nated based on the original sentence by changing lexical\nitems in subject and object positions.\nA.3 Correlation with world knowledge\nTable 10 shows the correlation between the robust-\nness of world knowledge representation and the\nstrength of counterfactual preference in CW condi-\ntion. Across all language models there is a signifi-\ncant correlation, with all correlation coefficients at\nor above 0.69, indicating that language models ben-\nefit from a good representation of world knowledge\nfor this counterfactual task.\nModel Small-scale Large-scale\ncoef p coef p\nGPT2 .86 <.001*** .82 <.001***\nGPT3 .70 .004** .74 <.001***\nBERT .91 .001** .69 <.001***\nRoBERTa .86 .001*** .77 <.001***\nMPNet .88 <.001*** .61 <.001***\nTable 10: Correlation between robustness of world\nknowledge encoding and strength of counterfactual\npreference in CW condition. p < .05*, p < .01**,\np < .001***.\nA.4 Follow-up analysis on GPT-3’s success\nThe previous experiments indicate that GPT-3 has\nthe best performance in counterfactual tasks. We\nalso find that GPT-3’s success differs non-trivially\nbetween small-scale and large-scale datasets. In\nExp2, GPT-3 is more successful on the large-scale\ndataset. By contrast, in Exp3, GPT-3 is more suc-\ncessful on the small-scale dataset. What kind of\nlinguistic factors are driving the success of GPT-\n3? Why is there an asymmetry between GPT-3’s\nperformance on the small-scale and large-scale\ndatasets? We speculate that there are two possi-\nble reasons related to the design and characteristics\nof the small-scale and large-scale datasets. First,\nthe linear distance between lexical triggers and tar-\nget positions in the large-scale dataset is not con-\ntrolled as carefully as in the small-scale dataset.\nSecond, lexical triggers in the large-scale dataset\nalways favor a specific continuation, whereas in\nthe small-scale dataset the cue can support both\ncontinuations.\nIn this experiment, we further explore these ques-\ntions by investigating to what extent GPT-3’s suc-\ncess relies on other linguistic factors. We first de-\nsign a Baseline dataset by selecting a subset of\nthe large-scale dataset from Exp3, with the crite-\nrion that the selected items have no strong bias\ntoward either completion in the Baseline Bias Con-\ntext (BBC) condition (see examples in Table 11).\nNext, we test GPT-3’s sensitivity to three classes\nof linguistics features: conflicting lexical triggers,\nlinear distance to target position, and several other\nlinguistic markers. We manipulate these linguistic\nfeatures in the items of the CWC and RWCA condi-\ntions, to form three new datasets. The Cue dataset\nintroduces a conflicting cue via a discourse connec-\ntive “rather than” (see examples in Table 12). The\nDistance dataset changes the position of the con-\nflicting lexical cues by using the discourse connec-\ntive “instead of” (see examples in Table 13). The\nMarker dataset manipulates other fine-grained lin-\nguistic markers including sentence boundary, tense,\ndiscourse connective (see examples in Table 14).\nThere are 10,000 items in total. We again calcu-\nlate percentage of items in which the model prefers\nCWC-congruent continuations.\nBaseline We test GPT-3’s preference for CWC-\ncongruent continuations in the Baseline dataset\nto establish a baseline comparison for subsequent\nanalysis. The results are shown in the right-hand\ncolumn of Table 11. Similar to the results in Sec-\ntion 4, GPT-3 shows a greater preference for CWC-\ncongruent continuations in the CWC condition than\nin the RWCA condition, even when there is not a\nstrong preference in the BBC condition, which in-\ndicates GPT-3’s sensitivity to counterfactual struc-\nture.\n811\nConditionSentence GPT-3\nCWC If the pet had loved vegetables,\nit would be very surprising. In\nfact, people feed the pet with\nfish/cabbages.\n34.8\nRWCA Because the pet loved vegetables, it\nwas very surprising. In fact, people\nfeed the pet withfish/cabbages.\n27.3\nBBC In fact, people feed the pet with\nfish/cabbages.\n42.5\nTable 11: Baseline dataset: Example items and percent-\nage of preference for CWC-congruent completion (e.g.,\n“fish”).\nConflicting lexical cue Next, in the Cue dataset\nwe test to what extent GPT-3’s performance reflects\ncanceling out of lexical cues, by adding a conflict-\ning lexical cue to the context, licensed by the dis-\ncourse connective “rather than”. Though a new\nconflicting lexical cue appears, the logical comple-\ntion should remain the same. Table 12 (right-hand\ncolumn) shows that GPT-3 is greatly affected by the\npresence of conflicting lexical cues. After inserting\nthe conflicting cue (e.g., “meat”) into context, the\npercentage of CWC-congruent continuations (e.g.,\n“fish”) increased in both CWC and RWCA condi-\ntions, indicating a strong effect from the presence\nof a conflicting lexical cue.\nConditionSentence GPT-3\nCWC\n(Rather)\nIf the pet had loved vegetables\nrather than meat, it would be very\nsurprising. In fact, people feed the\npet withfish/cabbages.\n48.5\nRWCA\n(Rather)\nBecause the pet loved vegetables\nrather than meat, it was very sur-\nprising. In fact, people feed the pet\nwithfish/cabbages.\n47.0\nTable 12: Cue dataset: Example items and percentage of\npreference for CWC-congruent completion (e.g., “fish”).\nLinear distance to target Next, we use the Dis-\ntance dataset to test the extent to which the salience\nof lexical cues is affected by distance from the tar-\nget word. To do this, we move the conflicting lexi-\ncal cues to the beginning of the sentence, using the\ndiscourse connective “instead of”. As a result, the\nconflicting cue (e.g. “meat”) is moved farther away\nfrom the target, compared to it in Cue dataset. Ta-\nble 13 (right-hand column) shows the results. The\nmodel is less likely to predict the CWC-congruent\ncontinuation (e.g., “fish”) in both conditions. The\nresult suggests that linear distance from lexical cues\nto the prediction target has a strong impact.\nConditionSentence GPT-3\nCWC\n(Instead)\nIf instead of meat, the pet had loved\nvegetables, it would be very surpris-\ning. In fact, people feed the pet with\nfish/cabbages.\n28.5\nRWCA\n(Instead)\nBecause instead of meat, the pet\nloved vegetables, it was very sur-\nprising. In fact, people feed the pet\nwithfish/cabbages.\n33.8\nTable 13: Distance dataset: Example items and percent-\nage of preference for CWC-congruent completion (e.g.,\n“fish”).\nOther linguistic markers Finally, we use the\nMarker dataset to probe how other fine-grained lin-\nguistic markers affect the accuracy of predictions in\ncounterfactual sentences. We test the effect of sen-\ntence boundaries (indicated by a period), discourse\nconnectives (indicated by “In fact”) and tense. All\nthree manipulations make CWC-congruent continu-\nations less coherent relative to the CWC condition\nin the Baseline dataset, while the tense and sen-\ntence boundary manipulations additionally cause\nthe RWCA-congruent continuation to become more\nlogical. Table 14 (right-hand column) shows the\nresults. GPT-3 shows a fair amount of sensitivity to\nthese linguistic markers. For the linguistic markers\n(tense marker, sentence boundary marker) that shift\nthe logical completion from CWC-congruent (e.g.\n“fish”) to RWCA-congruent (e.g. “cabbages”), GPT-\n3 is less likely to prefer the CWC-congruent com-\npletion, with tense generating the strongest effect.\nFor the discourse connective manipulation, which\ndeletes the connective “in fact”, and should de-\ncrease the preference for the CWC-congruent com-\npletion, GPT-3 instead shows a slightly stronger\npreference for those CWC-congruent completions.\nA.5 Additional metrics on small-scale dataset\nTo further evaluate whether models’ success on\ncounterfactual inference disentangle with the pref-\nerence towards a specific continuation, we also con-\nduct by-item analysis on the small-scale datasets,\nand calculate the proportion of trials in which the\nmodel demonstrates a preference for the logical\ncompletion in both CW and RW conditions for\nExp1, and in both CWC and RWCA conditions for\nExp3. Table 15 shows the percentage of preference\nfor logical completions in both counterfactual and\nfactual conditions in Exp1 and Exp3. The results\n812\nConditionSentence GPT-3\nBoundaryIf the pet had loved vegetables,\nit would be very surprising, in\nfact, people feed the pet with\nfish/cabbages.\n28.7\nConnective If the pet loved vegetables, it would\nbe very surprising. People feed the\npet withfish/cabbages.\n35.5\nTense If the pet had loved vegetables, it\nwould be very surprising. In fact,\npeople would feed the pet with\nfish/cabbages.\n14.0\nTable 14: Marker dataset: Example items and percent-\nage of preference for CWC-congruent completion (e.g.,\n“fish”).\nare consistent with the findings we report in Sec-\ntion 2 and Section 4. In Exp1, GPT-3, RoBERTa\nand MPNet show above-chance preference (25%)\nfor logical continuations in both conditions. In\nExp3, only GPT-3 shows substantial preference for\nlogical continuations.\nModel GPT2 GPT3 BERT RoBERTa MPNetExp1 (CW + RW) 18.850.0 9.4 31.3 28.1Exp3 (CWC + RWCA) 029.2 12.5 4.1 4.2\nTable 15: Percentage of items in which both counter-\nfactual (CW/CWC) and real scenarios (RW/RWCA) are\npredicted correctly in Exp1 and Exp3.\n813\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIn page 5 Section Limitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nYes, in page 5, Section Ethics statement\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nIn Section 1 Introduction and in abstract\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nIn Section 2, 3, 4.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nYes, in Section 2 and Section 4.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe dataset is publicly available without a license\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nIn Section 2, 3, 4.\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nIn Section Ethics statement\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nYes, in Section 2 and Section 4.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\nIn Section 2, 3, 4.\n□\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe are using pre-trained language models and it takes about two hours to run the experiment on\ngoogle colab platform without a GPU\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n814\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nWe discuss experiment set up in subsection Experiments\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nwe report percentage of preference for one context-congruent continuation over context-incongruent\ncontinuation\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n815",
  "topic": "Counterfactual thinking",
  "concepts": [
    {
      "name": "Counterfactual thinking",
      "score": 0.9623190760612488
    },
    {
      "name": "Counterfactual conditional",
      "score": 0.8775525689125061
    },
    {
      "name": "Computer science",
      "score": 0.6613461971282959
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5407310724258423
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5134645104408264
    },
    {
      "name": "Natural language processing",
      "score": 0.5039190649986267
    },
    {
      "name": "Language model",
      "score": 0.45809680223464966
    },
    {
      "name": "Associative property",
      "score": 0.4434794485569
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4295569658279419
    },
    {
      "name": "Cognitive psychology",
      "score": 0.39857515692710876
    },
    {
      "name": "Machine learning",
      "score": 0.35367274284362793
    },
    {
      "name": "Psychology",
      "score": 0.256330281496048
    },
    {
      "name": "Social psychology",
      "score": 0.145591139793396
    },
    {
      "name": "Mathematics",
      "score": 0.1383885145187378
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}