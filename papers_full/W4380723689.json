{
  "title": "Efficient 3D Semantic Segmentation with Superpoint Transformer",
  "url": "https://openalex.org/W4380723689",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105606381",
      "name": "Robert Damien",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2950840885",
      "name": "Raguet, Hugo",
      "affiliations": [
        "Institut National des Sciences Appliquées Centre Val de Loire",
        "Laboratoire d'Informatique Fondamentale et Appliquée de Tours",
        "Centre Val de Loire"
      ]
    },
    {
      "id": "https://openalex.org/A2739527331",
      "name": "Landrieu, Loic",
      "affiliations": [
        "Université Gustave Eiffel"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2118246710",
    "https://openalex.org/W2115193009",
    "https://openalex.org/W2460657278",
    "https://openalex.org/W3007809903",
    "https://openalex.org/W6782909102",
    "https://openalex.org/W3112844079",
    "https://openalex.org/W4214773923",
    "https://openalex.org/W2963125977",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3034430142",
    "https://openalex.org/W2150182327",
    "https://openalex.org/W2907101105",
    "https://openalex.org/W2963182550",
    "https://openalex.org/W2620650400",
    "https://openalex.org/W3111535274",
    "https://openalex.org/W3034550906",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3104568842",
    "https://openalex.org/W3012494314",
    "https://openalex.org/W4214704440",
    "https://openalex.org/W4389665127",
    "https://openalex.org/W4313145913",
    "https://openalex.org/W2955873422",
    "https://openalex.org/W2346698422",
    "https://openalex.org/W2963281829",
    "https://openalex.org/W3204034406",
    "https://openalex.org/W3203597819",
    "https://openalex.org/W2804872164",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6765299845",
    "https://openalex.org/W4300483932",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W4322581291",
    "https://openalex.org/W2135249503",
    "https://openalex.org/W4312616477",
    "https://openalex.org/W6739778489",
    "https://openalex.org/W3215666658",
    "https://openalex.org/W6762543013",
    "https://openalex.org/W4312903973",
    "https://openalex.org/W2556802233",
    "https://openalex.org/W4307104049",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2606202972",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W4312334476",
    "https://openalex.org/W2798380924",
    "https://openalex.org/W3036406207",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6847544096",
    "https://openalex.org/W2301720988",
    "https://openalex.org/W4283809036",
    "https://openalex.org/W4214755140",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W3083504878",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2954258401",
    "https://openalex.org/W4310829357",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963121255"
  ],
  "abstract": "We introduce a novel superpoint-based transformer architecture for efficient semantic segmentation of large-scale 3D scenes. Our method incorporates a fast algorithm to partition point clouds into a hierarchical superpoint structure, which makes our preprocessing 7 times times faster than existing superpoint-based approaches. Additionally, we leverage a self-attention mechanism to capture the relationships between superpoints at multiple scales, leading to state-of-the-art performance on three challenging benchmark datasets: S3DIS (76.0% mIoU 6-fold validation), KITTI-360 (63.5% on Val), and DALES (79.6%). With only 212k parameters, our approach is up to 200 times more compact than other state-of-the-art models while maintaining similar performance. Furthermore, our model can be trained on a single GPU in 3 hours for a fold of the S3DIS dataset, which is 7x to 70x fewer GPU-hours than the best-performing methods. Our code and models are accessible at github.com/drprojects/superpoint_transformer.",
  "full_text": "Efficient 3D Semantic Segmentation with Superpoint Transformer\nDamien Robert1, 2\ndamien.robert@ign.fr\nHugo Raguet3\nhugo.raguet@insa-cvl.fr\nLoic Landrieu2,4\nloic.landrieu@enpc.fr\n1CSAI, ENGIE Lab CRIGEN, France\n2 LASTIG, IGN, ENSG, Univ Gustave Eiffel, France\n3INSA Centre Val-de-Loire Univ de Tours, LIFAT, France\n4LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France\nAbstract\nWe introduce a novel superpoint-based transformer ar-\nchitecture for efficient semantic segmentation of large-scale\n3D scenes. Our method incorporates a fast algorithm to par-\ntition point clouds into a hierarchical superpoint structure,\nwhich makes our preprocessing 7 times faster than existing\nsuperpoint-based approaches. Additionally, we leverage a\nself-attention mechanism to capture the relationships be-\ntween superpoints at multiple scales, leading to state-of-the-\nart performance on three challenging benchmark datasets:\nS3DIS (76.0% mIoU 6-fold validation), KITTI-360 (63.5%\non Val), and DALES (79.6%). With only 212k parameters,\nour approach is up to 200 times more compact than other\nstate-of-the-art models while maintaining similar perfor-\nmance. Furthermore, our model can be trained on a single\nGPU in 3 hours for a fold of the S3DIS dataset, which is\n7× to 70× fewer GPU-hours than the best-performing meth-\nods. Our code and models are accessible at github.com/\ndrprojects/superpoint_transformer.\n1. Introduction\nAs the expressivity of deep learning models increases\nrapidly, so do their complexity and resource requirements\n[15]. In particular, vision transformers have demonstrated\nremarkable results for 3D point cloud semantic segmentation\n[61, 41, 18, 25, 36], but their high computational require-\nments make them challenging to train effectively. Addition-\nally, these models rely on regular grids or point samplings,\nwhich do not adapt to the varying complexity of 3D data: the\nsame computational effort is allocated everywhere, regard-\nless of the local geometry or radiometry of the point cloud.\nThis issue leads to needlessly high memory consumption,\nlimits the number of points that can be processed simultane-\nously, and hinders the modeling of long-range interactions.\nSuperpoint-based methods [29, 26, 23, 45] address the\n104 105 10760\n70\n75\nSPT\nSPT-nano\nSPG\nKPConv\nStratified Trans.\nRandLaNet\nPoint Trans.\nMinkowskiNet\nDeep\nView\nAgg\nPointNeXt\nS\nB\nL\nXL\n100h\n25h\n5h\nTraining time\n(GPU-h)\nModel Size\nmIoU 6-Fold\nFigure 1: Model Size vs. Performance. We visualize the\nperformance of different methods on the S3DIS dataset (6-\nfold validation) in relation to their model size in log-scale.\nThe area of the markers indicates the GPU-time to train on a\nsingle fold. Our proposed method Superpoint Transformer\n(SPT) achieves state-of-the-art with a reduction of up to 200-\nfold in model size and 70-fold in training time (in GPU-h)\ncompared to recent methods. The even smaller SPT-nano\nmodel achieves a fair performance with26k parameters only.\nlimitation of regular grids by partitioning large point clouds\ninto sets of points— superpoints—which adapt to the local\ncomplexity. By directly learning the interaction between su-\nperpoints instead of individual points, these methods enable\nthe analysis of large scenes with compact and parsimonious\nmodels that can be trained faster than standard approaches.\nHowever, superpoint-based methods often require a costly\npreprocessing step, and their range and expressivity are lim-\n1\narXiv:2306.08045v2  [cs.CV]  12 Aug 2023\nited by their use of local graph-convolution schemes [51].\nIn this paper, we propose a novel superpoint-based trans-\nformer architecture that overcomes the limitations of both\napproaches, see Figure 1. Our method starts by partition-\ning a 3D point cloud into a hierarchical superpoint struc-\nture that adapts to the local properties of the acquisition at\nmultiple scales simultaneously. To compute this partition\nefficiently, we propose a new algorithm that is an order of\nmagnitude faster than existing superpoint preprocessing al-\ngorithms. Next, we introduce the Superpoint Transformer\n(SPT) architecture, which uses a sparse self-attention scheme\nto learn relationships between superpoints at multiple scales.\nBy viewing the semantic segmentation of large point clouds\nas the classification of a small number of superpoints, our\nmodel can accurately classify millions of 3D points simulta-\nneously without relying on sliding windows. SPT achieves\nnear state-of-the-art accuracy on various open benchmarks\nwhile being significantly more compact and able to train\nmuch quicker than common approaches. The main contribu-\ntions of this paper are as follows:\n• Efficient Superpoint Computation: We propose a new\nmethod to compute a hierarchical superpoint structure for\nlarge point clouds, which is more than 7 times faster than\nexisting superpoint-based methods. Our preprocessing time\nis also comparable or faster than standard approaches, ad-\ndressing a significant drawback of superpoint methods.\n• State-of-the-Art Performance: Our model reaches per-\nformance at or close to the state-of-the-art for three open\nbenchmarks with distinct settings: S3DIS for indoor scan-\nning [3], KITTI-360 for outdoor mobile acquisitions [32],\nand DALES for city-scale aerial LiDAR [55].\n• Resource-Efficient Models: SPT is particularly resource-\nefficient as it only has 212k parameters for S3DIS and\nDALES, a 200-fold reduction compared to other state-of-\nthe-art models such as PointNeXt [44] and takes 70 times\nfewer GPU-h to train than Stratified Transformer [25]. The\neven more compact SPT-nano reaches 70.8% 6-Fold mIoU\non S3DIS with only 26k parameters, making it the smallest\nmodel to reach above 70% by a factor of almost 300.\n2. Related Work\nThis section provides an overview of the main inspira-\ntions for this paper, which include 3D vision transformers,\npartition-based methods, and efficient learning for 3D data.\n3D Vision Transformers. Following their adoption for\nimage processing [10, 34], Transformer architectures [56]\ndesigned explicitly for 3D analysis have shown promising\nresults in terms of performance [61, 18] and speed [41, 36].\nIn particular, the Stratified Transformer of Lai et al. uses\na specific sampling scheme [25] to model long-range inter-\nactions. However, the reliance of 3D vision transformers\non arbitrary K-nearest or voxel neighborhoods leads to high\nmemory consumption, which hinders the processing of large\nscenes and the ability to leverage global context cues.\nPartition-Based Methods. Partitioning images into super-\npixels has been studied extensively to simplify image analy-\nsis, both before and after the widespread use of deep learning\n[1, 54]. Similarly, superpoints are used for 3D point cloud\nsegmentation [40, 33] and object detection [19, 11]. Super-\nPointGraph [29] proposed to learn the relationship between\nsuperpoints using graph convolutions [51] for semantic seg-\nmentation. While this method trains fast, its preprocessing is\nslow and its expressivity and range are limited, as it operates\non a single partition. Recent works have proposed ways\nof learning the superpoints themselves [26, 23, 53], which\nyields improved results but at the cost of an extra training\nstep or a large point-based backbone [24].\nHierarchical partitions are used for image processing\n[2, 59, 60] and 3D analysis tasks such as point cloud com-\npression [12] and object detection [7, 31]. Hierarchical ap-\nproaches for semantic segmentation use Octrees with fixed\ngrids [39, 48]. On the contrary, SPT uses a multi-scale hi-\nerarchical structure that adapts to the local geometry of the\ndata. This leads to partitions that conform more closely\nto semantic boundaries, enabling the network to model the\ninteractions between objects or object parts.\nEfficient 3D Learning. As 3D scans of real-world scenes\ncan contain hundreds of millions of points, optimizing the ef-\nficiency of 3D analysis is an essential area of research. Point-\nNeXt [44] proposes several effective techniques that allow\nsimple and efficient methods [43] to achieve state-of-the-art\nperformance. RandLANet [22] demonstrates that efficient\nsampling strategies can yield excellent results. Sparse [16]\nor hybrid [35] point cloud representations have also helped\nreduce memory usage. However, by leveraging the local sim-\nilarity of dense point clouds, superpoint-based methods can\nachieve an input reduction of several orders of magnitude,\nresulting in unparalleled efficiency.\n3. Method\nOur method has two key components. First, we use an\nefficient algorithm to segment an input point cloud into\na compact multi-scale hierarchical structure. Second, a\ntransformer-based network leverages this structure to classify\nthe elements of the finest scale.\n3.1. Efficient Hierarchical Superpoint Partition\nWe consider a point cloudC with positional and radiomet-\nric information. To learn multiscale interactions, we compute\na hierarchical partition ofC into geometrically-homogeneous\nsuperpoints of increasing coarseness; see Figure 2. We first\ndefine the concept of hierarchical partitions.\n2\n(a) Input point cloud\n (b) Ground truth labels\n(c) First partition level\n (d) First superpoint-graph\n(e) Second partition level\n (f) Second superpoint-graph\nFigure 2: Superpoint Transformer. Our method takes as\ninput a point cloud a) and computes its hierarchical parti-\ntion into geometrically homogeneous superpoints at multiple\nscales: c) and e). For all partition levels, we construct su-\nperpoint adjacency graphs d) and f), which are used by an\nattention-based network to classify the finest superpoints.\nDefinition 1 Hierarchical Partitions. A partition of a set\nX is a collection of subsets ofX such that each element ofX\nis in one and only one of such subsets. P := [P0, · ··, PI]\nis a hierarchical partition of X if P0 = X, and Pi+1 is a\npartition of Pi for i ∈ [0, I− 1].\nThroughout this paper, all functions or tensors related to\na specific partition level i are denoted with an exponent i.\nHierarchical Superpoint Partitions. We propose an ef-\nficient approach for constructing hierarchical partitions of\nlarge point clouds. First, we associate each point c of C\nwith features fc representing its local geometric and radio-\nmetric information. These features can be handcrafted [17]\nor learned [26, 23]. See the Appendix for more details on\npoint features. We also define a graph G encoding the adja-\ncency between points usually based on spatial proximity,e.g.\nk-nearest neighbors.\nWe view the features fc for all c of C as a signal f de-\nfined on the nodes of the graph G. Following the ideas of\nSuperPoint Graph [29], we compute an approximation of f\ninto constant components by solving an energy minimization\nproblem penalized with a graph-based notion of simplicity.\nThe resulting constant components form a partition whose\ngranularity is determined by a regularization strength λ >0:\nhigher values yield fewer and coarser components.\nFor each component of the partition, we can compute the\nmean position (centroid) and feature of its elements, defining\na coarser point cloud on which we can repeat the partitioning\nprocess. We can now compute a hierarchical partition P :=\n[P0, · ··, PI] of C from a list of regularization strengths\nλ1, · ··, λI. First, we set P0 as the point cloud C and f0 as\nthe point features f. Then, for i = 1to I, we compute (i) a\npartition Pi of fi−1 penalized with λi; (ii) the mean signal\nfi for all components of Pi. The coarseness of the resulting\npartitions [P0, · ··, PI] is thus strictly increasing. See the\nAppendix for a more detailed description of this process.\nHierarchical Graph Structure. A hierarchical partition\ndefines a polytree structure across the different levels. Let p\nbe an element of Pi. If i ∈ [0, I− 1], parent(p) is the com-\nponent of Pi+1 which contains p. If i ∈ [1, I], children(p)\nis the set of components of Pi−1 whose parent is p.\nSuperpoints also share adjacency relationships with super-\npoints of the same partition level. For each level i ≥ 1, we\nbuild a superpoint-graph Gi by connecting adjacent compo-\nnents of Pi, i.e. superpoints whose closest points are within a\ndistance gap ϵi > 0. For p ∈ Pi, we denote N(p) ⊂ Pi the\nset of neighbours of p in the graph Gi. More details on the\nsuperpoint-graph construction can be found in the Appendix.\nHierarchical Parallel ℓ0-Cut Pursuit. Computing the hi-\nerarchical components involves solving a recursive sequence\nof non-convex, non-differentiable optimization problems on\nlarge graphs. We propose an adaptation of the ℓ0-cut pursuit\nalgorithm [28] to solve this problem. To improve efficiency,\nwe adapt the graph-cut parallelization strategy initially intro-\nduced by Raguet et al. [46] in the convex setting.\n3.2. Superpoint Transformer\nOur proposed SPT architecture draws inspiration from\nthe popular U-Net [50, 14]. However, instead of using grid,\npoint, or graph subsampling, our approach derives its differ-\nent resolution levels from the hierarchical partition P.\nGeneral Architecture. As represented in Figure 3, SPT\ncomprises an encoder with I stages and a decoder with I −1\nstages: the prediction takes place at the level P1 and not\non individual points. We start by computing the relative\npositions x of all points and superpoints with respect to\ntheir parent. For a superpoint p ∈ Pi, we define xi\np as the\nposition of the centroid of p relative to its parent’s. The\ncoarsest superpoints of PI have no parent and use the center\nof the scene as a reference centroid. We then normalize these\nvalues so that the sets{xi\np|p ∈ children(q)} have a radius of\n1 for all q ∈ Pi+1. We compute features for each 3D point by\n3\nT 1\nenc\nT 2\nenc\nT 1\ndec\nP2\nP1\nP0\nto coarser partition\nto finer partition\nintra-level\ntransformer\nFigure 3: Superpoint Transformer.We represent our pro-\nposed architecture with two partitions levels P1 and P2. We\nuse a transformer-based module to leverage the context at\ndifferent scales, leading to large receptive fields. We only\nclassify the superpoints of the partitionP1 and not individual\n3D points, allowing fast training and inference.\nusing a multi-layer perceptron (MLP) to mix their relative\npositions and handcrafted features: g0 := ϕ0\nenc([x0, f0]),\nwith [·, ·] the channelwise concatenation operator.\nEach level i ≥ 1 of the encoder maxpools the features of\nthe finer partition level i − 1, adds relative positions xi and\npropagates information between neighboring superpoints in\nGi. For a superpoint p in Pi, this translates as:\ngi\np = T i\nenc ◦ ϕi\nenc\n\u0012\u0014\nxi\np, max\nq∈children(p)\n\u0000\ngi−1\nq\n\u0001\u0015\u0013\n(1)\nwith ϕi\nenc an MLP and T i\nenc a transformer module explained\nbelow. By avoiding communication between the 3D points\nof P0, we bypass a potential computational bottleneck.\nThe decoder passes information from the coarser partition\nlevel i + 1to the finer level i. It uses the relative positions xi\nand the encoder features gi to improve the spatial resolution\nof its feature maps hi [50]. For a superpoint p in partition\nPi with 1 ≤ i < I− 1, this can be expressed as:\nhi\np = T i\ndec ◦ ϕi\ndec\n\u0010h\nxi\np, gi\np, hi+1\nparent(p)\ni\u0011\n(2)\nwith hI = gI, ϕi\ndec an MLP, and T i\ndec an attention-based\nmodule similar to T i\nenc.\nSelf-Attention Between Superpoints. We propose a vari-\nation of graph-attention networks [57] to propagate informa-\ntion between neighboring superpoints of the same partition\nlevel. For each level of the encoder and decoder, we as-\nsociate to superpoint p ∈ Pi a triplet of key, query, value\nvectors Kp, Qp, Vp of size Dkey, Dkey and Dval. These values\nare obtained by applying a linear layer to the corresponding\nfeature map m after GraphNorm normalization [5].\nWe then characterize the relationship between two su-\nperpoints p, qof Pi adjacent in Gi by a triplet of fea-\ntures akey\np,q, aque\np,q, aval\np,q of dimensions Dkey, Dkey and Dval, and\nwhose computation is detailed in the next section. Given a su-\nperpoint p, we stack the vectors akey\np,q, aque\np,q, aval\np,q for q ∈ N(p)\nin matrices Akey\np , Aque\np , Aval\np of dimensions | N(p)| ×Dkey or\n| N(p)| ×Dval. The modules T i\nenc and T i\ndec gather contextual\ninformation as follows:\n[T (m)]p\n+\n= att(Q⊺\np ⊕Aque\np , KN(p)+Akey\np , VN(p)+Aval\np ) , (3)\nwith\n+\n= a residual connection [20], ⊕ the addition operator\nwith broadcasting on the first dimension, and KN(p) the\nmatrix of stacked vectors Kq for q ∈ N(p). The attention\nmechanism writes as follows:\natt(Q, K, V) := V ⊺ softmax\n \nQ ⊙ K1p\n| N(p)|\n!\n, (4)\nwith ⊙ the Hadamard termwise product and 1 a column-\nvector with Dkey ones. Our proposed scheme is similar\nto classic attention schemes with two differences: (i) the\nqueries adapt to each neighbor, and (ii) we normalize the\nsoftmax with the neighborhood size instead of the key di-\nmension. In practice, we use multiple independent attention\nmodules in parallel (multi-head attention) and several con-\nsecutive attention blocks.\n3.3. Leveraging the Hierarchical Graph Structure\nThe hierarchical superpoint partition P can be used for\nmore than guidance for graph pooling operations. Indeed,\nwe can learn expressive adjacency encodings capturing the\ncomplex adjacency relationships between superpoints and\nemploy powerful supervision and augmentation strategies\nbased on the hierarchical partitions.\nAdjacency Encoding. While the adjacency between two\n3D points is entirely defined by their distance vector, the\nrelationships between superpoints are governed by additional\nfactors such as their alignment, proximity, and difference in\nsizes or shapes. We characterize the adjacency of pairs of\nadjacent superpoints of the same partition level using a set\nof handcrafted features based on: (i) the relative positions of\ncentroids, (ii) position of paired points in each superpoints,\n(iii) the superpoint principal directions, and (iv) the ratio\nbetween the superpoints’ length, volume, surface, and point\ncount. These features are efficiently computed only once\nduring preprocessing.\nFor each pair of superpoints (p, q) adjacent in Gi, we\njointly compute the concatenated akey\np,q, aque\np,q, aval\np,q by applying\nan MLP ϕi\nadj to the handcrafted adjacency features defined\nabove. Further details on the superpoint-graph construction\nand specific adjacency features are provided in the Appendix.\n4\nHierarchical Supervision. We propose to take advantage\nof the nested structure of the hierarchical partition P into\nthe supervision of our model. We can naturally associate\nthe superpoints of any level i ≥ 1 with a set of 3D points\nin P0. The superpoints at the finest level i = 1are almost\nsemantically pure (see Figure 6), while the superpoints at\ncoarser levels i >1 typically encompass multiple objects.\nTherefore, we use a dual learning objective: (i) we predict\nthe most frequent label within the superpoints of P1 , and\n(ii) we predict the label distribution for the superpoints of\nPi with i > 1. We supervise both predictions with the\ncross-entropy loss.\nLet yi\np denote the true label distribution of the 3D points\nwithin a superpoint p ∈ Pi, and ˆyi\np a one-hot-encoding of\nits most frequent label. We use a dedicated linear layer\nat each partition level to map the decoder feature gi\np to a\npredicted label distribution zi\np. Our objective function can\nbe formulated as follows:\nL =\nX\np∈P1\n−N1\np\n| C |H(ˆy1\np, z1\np)+\nIX\ni=2\nX\np∈Pi\nµiNi\np\n| C |H(yi\np, zi\np) , (5)\nwhere µ2, · ··, µI are positive weights, Ni\np represents the\nnumber of points within a superpoint p ∈ Pi, and |C| is the\ntotal number of points in the point cloud, and H(y, z) =\n−P\nk∈K yk log(zk) and K the class set.\nSuperpoint-Based Augmentations. Although our ap-\nproach classifies superpoints rather than individual 3D points,\nwe still need to load the points of P0 in memory to em-\nbed the superpoints from P1. However, since superpoints\nare designed to be geometrically simple, only a subset of\ntheir points is needed to characterize their shape. There-\nfore, when computing the feature g1\np of a superpoint p of\nP1 containing n points with Eq. (1), we sample only a por-\ntion tanh(n/nmax) of its points, with a minimum of nmin.\nThis sampling strategy reduces the memory load and acts\nas a powerful data augmentation. The lightweight version\nof our model SPT-nano goes even further. It ignores the\npoints entirely and only use handcrafted features to embed\nthe superpoints of P1, thus avoiding entirely the complexity\nassociated with the size of the input point cloud P0.\nTo further augment the data, we exploit the geometric\nconsistency of superpoints and their hierarchical arrange-\nment. During the batch construction, we randomly drop\neach superpoint with a given probability at all levels. Drop-\nping superpoints at the fine levels removes random objects\nor object parts, while dropping superpoints at the coarser\nlevels removes entire structures such as walls, buildings, or\nportions of roads, for example.\nTable 1: Partition Configuration. We report the point count\nof different datasets before and after subsampling, as well as\nthe size of the partitions.\nDataset Points Subsampled | P1 | | P2 |\nS3DIS [3] 273m 32m 979k 292k\nDALES [55] 492m 449m 14.8m 2.56m\nKITTI-360 [32] 919m 432m 16.2m 2.98m\n4. Experiments\nWe evaluate our model on three diverse datasets described\nin Section 4.1. In Section 4.2, we evaluate our approach in\nterms of precision, but also quantify the gains in terms of\npre-processing, training, and inference times. Finally, we\npropose an extensive ablation study in Section 4.3.\n4.1. Datasets and Models\nDatasets. To demonstrate its versatility, we evaluate SPT\non three large-scale datasets of different natures.\nS3DIS [3]. This indoor dataset of office buildings contains\nover 274 million points across 6 building floors—or areas.\nThe dataset is organized by individual rooms, but can also\nbe processed by considering entire areas at once.\nKITTI-360 [32]. This outdoor dataset contains more than\n100 k laser scans acquired in various urban settings on a mo-\nbile platform. We use the accumulated point clouds format,\nwhich consists of large scenes with around 3 million points.\nThere are 239 training scenes and 61 for validation.\nDALES [55]. This 10 km2 aerial LiDAR dataset contains\n500 millions of points across 40 urban and rural scenes,\nincluding 12 for evaluation.\nWe subsample the datasets using a 3cm grid for S3DIS,\nand 10cm for KITTI-360 and DALES. All accuracy metrics\nare reported for the full, unsampled point clouds. We use a\ntwo-level partition (I = 2) with µ2 = 50for all datasets and\nselect the partition parameters to obtain a 30-fold reduction\nbetween P1 and P0 and a further 5-fold reduction for P2.\nSee Table 1 for more details.\nModels. We use the same model configuration for all three\ndatasets with minimal adaptations. All transformer modules\nhave a shared width Dval, a small key space of dimension\nDkey = 4, 16 heads, with 3 blocks in the encoder and 1\nin the decoder. We set Dval = 64for S3DIS and DALES\n(210k parameters), and Dval = 128(777k parameters) for\nKITTI360. See the Appendix and our open repository for\nthe detailed configuration of all modules.\nWe also propose SPT-nano, a lightweight version of our\nmodel that does not compute point-level features but operates\ndirectly on the first partition level P1. The value of the\nmaxpool over points in Eq. (1) for i = 1is replaced by f1,\nthe aggregated handcrafted point features at the level 1 of\n5\nInput\nPartition P2\nGround Truth\nPrediction\n(a) S3DIS\n (b) KITTI-360\n (c) DALES\nFigure 4: Qualitative Results. We represent input samples (with color or intensity) of our approach and its predictions for all\nthree datasets. Additionally, we show the coarsest partition level and demonstrate how superpoints can accurately capture the\ncontours of complex objects and classify them accordingly. Black points are unlabeled in the ground truth.\nthe partition. This model never considers the full point cloud\nP0 but only operates on the partitions. For this model, we\nset Dval = 16for S3DIS and DALES (26k parameters), and\nDval = 32for KITTI360 (70k parameters).\nBatch Construction. Batches are sampled from largetiles:\nentire building floors for S3DIS, and full scenes for KITTI-\n360 or DALES. Each batch is composed of 4 randomly sam-\npled portions of the partition with a radius of 7m for S3DIS\nand 50m for KITTI and DALES, allowing us to model long-\nrange interactions. During training, we apply a superpoint\ndropout rate of 0.2 for each superpoint at all hierarchy levels,\nas well as random rotation, tilting, point jitter and hand-\ncrafted features dropout. When sampling points within each\nsuperpoint, we set nmin = 32and nmax = 128.\nOptimization. We use the ADAMW optimizer [38] with\ndefault parameters, a weight decay of 10−4, a learning rate\nof 10−2 for DALES and KITTI-360 on and 10−1 for S3DIS.\nThe learning rate for the attention modules is 10 times\nsmaller than for other weights. Learning rates are warmed\nup from 10−6 for 20 epochs and progressively reduced to\n10−6 with cosine annealing [37].\n4.2. Quantitative Evaluation\nPerformance Evaluation. As seen in Table 2, SPT per-\nforms at the state-of-the-art on two of three datasets despite\nbeing a significantly smaller model. On S3DIS, SPT beats\n6\nTable 2: Performance Evaluation. We report the Mean\nIntersection-over-Union of different methods on three dif-\nferent datasets. SPT performs on par or better than recent\nmethods with significantly fewer parameters. † superpoint-\nbased. ⋆/∗ model with 777k/70k parameters.\nModel Size S3DIS KITTI DALES×106 6-Fold Area 5 360 val\nPointNet++ [43] 3.0 56.7 - - 68.3\n† SPG [29] 0.28 62.1 58.0 - 60.6\nConvPoint [4] 4.7 68.2 - - 67.4\n† SPG + SSP [26] 0.29 68.4 61.7 - -\n† SPNet [23] 0.32 68.7 - - -\nMinkowskiNet [8, 6] 37.9 69.1 65.4 58.3 -\nRandLANet [22] 1.2 70.0 - - -\nKPConv [52] 14.1 70.6 67.1 - 81.1\nPoint Trans.[61] 7.8 73.5 70.4 - -\nRepSurf-U [47] 0.97 74.3 68.9 - -\nDeepViewAgg [49] 41.2 74.7 67.2 62.1 -\nStrat. Trans. [25, 58] 8.0 74.9 72.0 - -\nPointNeXt-XL [44] 41.6 74.9 71.1 - -\n† SPT (ours) 0.21 76.0 68.9 63.5⋆ 79.6\n† SPT-nano (ours) 0.026 70.8 64.9 57.2 ∗ 75.2\n1 100\n40\n60\n70\n0.2 Training time (GPU-h)\nArea5 test mIoU\nSPT SPT-nano (×0.5)\nSPG (×0.9) PointNet++ (×2)\nKPConv (×5) MinkowskiNet (×9)\nDeepViewAgg (×11) Point Trans (×20)\nStrat. Trans.∗(×67)\nFigure 5: Training Speed. We report the evolution of the\ntest mIoU for S3DIS Area 5 for different methods until the\nbest epoch is reached. The curves are shifted right according\nto the preprocessing time. We report in parenthesis the time\nratio compared to SPT.\nPointNeXt-XL with 196× fewer parameters. On KITTI-360,\nSPT outperforms MinkowskiNet despite a size ratio of 49,\nand surpasses the performance of the even larger multimodal\npoint-image model DeepViewAgg. On DALES, SPT out-\nperforms ConvPoint by more than 12 points with over 21\ntimes fewer parameters. Although SPT is 1.5 points behind\nKPConv on this dataset, it achieves these results with 67\ntimes fewer parameters. SPT achieves significant perfor-\nmance improvements over all superpoint-based methods on\nall datasets, ranging from 7 to 14 points. SPT overtakes the\nSSP and SPNet superpoint methods that learn the partition\nin a two-stage training setup, leading to pre-processing times\nof several hours.\nInterestingly, the lightweight SPT-nano model matches\nKPConv and MinkowskiNet with only 26k parameters.\nSee Figure 4 for qualitative illustrations.\nPreprocessing Speed. As reported in Table 3, our imple-\nmentation of the preprocessing step is highly efficient. We\ncan compute partitions, superpoint-graphs, and handcrafted\nfeatures, and perform I/O operations quickly: 12.4min for\nS3DIS, 117 for KITTI-360, and 148 for DALES using a\nserver with a 48-core CPU. An 8-core workstation can pre-\nprocess S3DIS in 26.6min. Our preprocessing time is as fast\nor faster than point-level methods and 7× faster than Super-\nPoint Graph’s, thus alleviating one of the main drawbacks of\nsuperpoint-based methods.\nTraining Speed. We trained several state-of-the-art meth-\nods from scratch and report in Figure 5 the evolution of test\nperformance as a function of training time. We used the\nofficial training logs for the multi-GPU Point Transformer\nand Stratified Transformer. SPT can train much faster than\nall methods not based on superpoints while attaining similar\nperformance. Although Superpoint Graph trains even faster,\nits performance saturates earlier,6.0 mIoU points below SPT\n. We also report the inference time of our method in Table 3,\nwhich is significantly lower than competing approaches, with\na speed-up factor ranging from 8 to 80. All speed measure-\nments were conducted on a single-GPU server (48 cores,\n512Go RAM, A40 GPU). Nevertheless, our model can be\ntrained on a standard workstation (8 cores, 64Go, 2080Ti)\nwith smaller batches, taking only 1.5 times longer and with\ncomparable results.\nSPT performs on par or better than complex models with\nup to two orders of magnitude more parameters and sig-\nnificantly longer training times. Such efficiency and com-\npactness have many benefits for real-world scenarios where\nhardware, time, or energy may be limited.\n4.3. Ablation Study\nWe evaluate the impact of several design choices in Ta-\nble 4 and reports here our observations.\na) Handcrafted features. Without handcrafted point fea-\ntures, our model perform worse on all datasets. This obser-\nvation is in line with other works which also remarked the\n7\nTable 3: Efficiency Analysis. We report the preprocessing\ntime for the entire S3DIS dataset and the training and infer-\nence time for Area 5. SPT and SPT-nano shows significant\nspeedups in pre-processing, training, and inference times.\nPreprocessing Training Inference\nin min in GPU-h in s\nPointNet++ [43] 8.0 6.3 42\nKPConv [52] 23.1 14.1 162\nMinkowskiNet [8] 20.7 28.8 83\nStratified Trans. [25] 8.0 216.4 30\nSuperpoint Graph [29] 89.9 1.3 16\nSPT (ours) 12.4 3.0 2\nSPT-nano (ours) 12.4 1.9 1\nTable 4: Ablation Study. Impact of some of our design\nchoices on the mIoU for all tested datasets.\nExperiment S3DIS KITTI DALES\n6-Fold 360 Val\nBest Model 76.0 63.5 79.6\na) No handcrafted features -0.7 -4.1 -1.4\nb) No adjacency encoding -6.3 -5.4 -3.0\nb) Fewer edges -3.5 -1.1 -0.3\nc) No point sampling -1.3 -0.9 -0.5\nc) No superpoint sampling -2.7 -2.5 -0.7\nc) Only 1 partition level -8.4 -5.1 -0.9\npositive impact of well-designed handcrafted features on the\nperformance of smaller models [21, 47].\nb) Influence of Edges. Removing the adjacency encoding\nbetween superpoints leads to a significant drop of 6.3 points\non S3DIS; characterizing the relative position and relation-\nship between superpoints appears crucial to exploiting their\ncontext. We also find that pruning the 50% longest edges\nof each superpoint results in a systematic performance drop,\nhighlighting the importance of modeling long relationships.\nc) Partition-Based Improvements. We assess the impact\nof several improvements made possible by using hierarchi-\ncal superpoints. First, we find that using all available points\nwhen embedding the superpoints of P1 instead of random\nsampling resulted in a small performance drop. Second,\nsetting the superpoint dropout rate to 0 worsens the perfor-\nmance by over 2.5 points on S3DIS and KITTI-360.\nWhile we did not observe better results with three or more\npartition levels, only using one level leads to a significant\nloss of performance for all datasets.\nd) Influence of Partition Purity. In Figure 6, we plot\nthe performance of the “oracle” model which associates\n104 105 106 10740\n50\n60\n70\n80\n90\n×1\n×1.5\n×3\n×10\n30 cm\n20 cm\n10 cm\n5 cm 3 cm\nNumber of superpoints / nonempty voxels\nArea5 test mIoU\nV oxel grid oracle\nx cm Grid size\nPartition oracle\nSPT performance\n×n Coarseness ratio\nPerformance gap\nFigure 6: Partition Purity. We plot the highest achievable\n“oracle” prediction for our partitions and a regular voxel grid.\nWe also show the performance of SPT for 4 partitions with a\ncoarseness ratio from ×1 to ×10.\nto each superpoint of P1 with its most frequent true label.\nThis model acts as an upper bound on the achievable per-\nformance with a given partition. Our proposed partition has\nsignificantly higher semantic purity than a regular voxel grid\nwith as many nonempty voxels as superpoints. This implies\nthat our superpoints adhere better to semantic boundaries\nbetween objects.\nWe also report the performance of our model for different\npartitions of varying coarseness, measured as the number\nof superpoints in P1. Using, respectively, ×1.5 (×3) fewer\nsuperpoints leads to a performance drop of 2.2 (4.7) mIoU\npoints, but reduce the training time to 2.4 (1.6) hours. The\nperformance of SPT is more than 20 points below the ora-\ncle, suggesting that the partition does not strongly limit its\nperformance.\nLimitations. See the Appendix.\n5. Conclusion\nWe have introduced the Superpoint Transformer approach\nfor semantic segmentation of large point clouds, combining\nsuperpoints and transformers to achieve state-of-the-art re-\nsults with significantly reduced training time, inference time,\nand model size. This approach particularly benefits large-\nscale applications and computing with limited resources.\nMore broadly, we argue that small, tailored models can offer\na more flexible and sustainable alternative to large, generic\nmodels for 3D learning. With training times of a few hours\non a single GPU, our approach allows practitioners to easily\ncustomize the models to their specific needs, enhancing the\noverall usability and accessibility of 3D learning.\n8\nAcknowledgements. This work was funded by ENGIE\nLab CRIGEN. This work was supported by ANR project\nREADY3D ANR-19-CE23-0007, and was granted ac-\ncess to the HPC resources of IDRIS under the allocation\nAD011013388R1 made by GENCI. We thank Bruno Vallet,\nRomain Loiseau and Ewelina Rupnik for inspiring discus-\nsions and valuable feedback.\nReferences\n[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien\nLucchi, Pascal Fua, and Sabine S¨usstrunk. SLIC superpixels\ncompared to state-of-the-art superpixel methods. TPAMI,\n2012.\n[2] Pablo Arbelaez. Boundary extraction in natural images using\nultrametric contour maps. CVPR Workshop, 2006.\n[3] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis\nBrilakis, Martin Fischer, and Silvio Savarese. 3D semantic\nparsing of large-scale indoor spaces. CVPR, 2016.\n[4] Alexandre Boulch. ConvPoint: Continuous convolutions for\npoint cloud processing. Computers & Graphics, 2020.\n[5] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-yan Liu,\nand Liwei Wang. GraphNorm: A principled approach to\naccelerating graph neural network training. ICML, 2021.\n[6] Thomas Chaton, Nicolas Chaulet, Sofiane Horache, and Loic\nLandrieu. Torch-Points3D: A modular multi-task framework\nfor reproducible deep learning on 3D point clouds. 3DV,\n2020.\n[7] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and\nXinggang Wang. Hierarchical aggregation for 3D instance\nsegmentation. CVPR, 2021.\n[8] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D\nspatio-temporal ConvNets: Minkowski convolutional neural\nnetworks. CVPR, 2019.\n[9] J´erˆome Demantk´e, Cl´ement Mallet, Nicolas David, and Bruno\nVallet. Dimensionality based scale selection in 3D LiDAR\npoint clouds. In Laserscanning, 2011.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Transform-\ners for image recognition at scale. ICLR, 2020.\n[11] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian\nLeibe, and Matthias Nießner. 3D-MPA: Multi-proposal ag-\ngregation for 3D semantic instance segmentation. CVPR,\n2020.\n[12] Yuxue Fan, Yan Huang, and Jingliang Peng. Point cloud com-\npression based on hierarchical point clustering. In APSIPA\nASC, 2013.\n[13] Martin A Fischler and Robert C Bolles. Random sample\nconsensus: a paradigm for model fitting with applications to\nimage analysis and automated cartography. Communications\nof the ACM, 1981.\n[14] Hongyang Gao and Shuiwang Ji. Graph U-Nets. ICML, 2019.\n[15] Charlie Giattino, Edouard Mathieu, Julia Broden, and Max\nRoser. Artificial intelligence. Our World in Data , 2022.\nhttps://ourworldindata.org/artificial-intelligence.\n[16] Benjamin Graham, Martin Engelcke, and Laurens van der\nMaaten. 3D semantic segmentation with submanifold sparse\nconvolutional networks. CVPR, 2018.\n[17] St´ephane Guinard and Loic Landrieu. Weakly supervised\nsegmentation-aided classification of urban scenes from 3D\nLiDAR point clouds. ISPRS Workshop, 2017.\n[18] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R Martin, and Shi-Min Hu. PCT: Point cloud\ntransformer. CVM, 2021.\n[19] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg:\nOccupancy-aware 3D instance segmentation. CVPR, 2020.\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. CVPR, 2016.\n[21] Pai-Hui Hsu and Zong-Yi Zhuang. Incorporating handcrafted\nfeatures into deep learning for point cloud classification. Re-\nmote Sensing, 2020.\n[22] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan\nGuo, Zhihua Wang, Niki Trigoni, and Andrew Markham.\nRandLA-Net: Efficient semantic segmentation of large-scale\npoint clouds. CVPR, 2020.\n[23] Le Hui, Jia Yuan, Mingmei Cheng, Jin Xie, Xiaoya Zhang,\nand Jian Yang. Superpoint network for point cloud overseg-\nmentation. ICCV, 2021.\n[24] Xin Kang, Chaoqun Wang, and Xuejin Chen. Region-\nenhanced feature learning for scene semantic segmentation.\narXiv preprint arXiv:2304.07486, 2023.\n[25] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang\nZhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans-\nformer for 3D point cloud segmentation. CVPR, 2022.\n[26] Loic Landrieu and Mohamed Boussaha. Point cloud overseg-\nmentation with graph-structured deep metric learning. CVPR,\n2019.\n[27] Loic Landrieu and Guillaume Obozinski. Cut pursuit: fast\nalgorithms to learn piecewise constant functions. AISTATS,\n2016.\n[28] Loic Landrieu and Guillaume Obozinski. Cut pursuit: Fast\nalgorithms to learn piecewise constant functions on general\nweighted graphs. In SIAM Journal on Imaging Sciences ,\n2017.\n[29] Loic Landrieu and Martin Simonovsky. Large-scale point\ncloud semantic segmentation with superpoint graphs. CVPR,\n2018.\n[30] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di,\nand Baoquan Chen. Pointcnn: Convolution on χ-transformed\npoints. NeurIPS, 2018.\n[31] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and\nKui Jia. Instance segmentation in 3D scenes using semantic\nsuperpoint tree networks. CVPR, 2021.\n[32] Yiyi Liao, Jun Xie, and Andreas Geiger. KITTI-360: A novel\ndataset and benchmarks for urban scene understanding in 2D\nand 3D. TPAMI, 2022.\n[33] Yangbin Lin, Cheng Wang, Dawei Zhai, Wei Li, and Jonathan\nLi. Toward better boundary preserved supervoxel segmenta-\ntion for 3D point clouds. ISPRS journal of photogrammetry\nand remote sensing, 2018.\n9\n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows.CVPR,\n2021.\n[35] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-\nvoxel CNN for efficient 3D deep learning. NeurIPS, 2019.\n[36] Romain Loiseau, Mathieu Aubry, and Lo¨ıc Landrieu. Online\nsegmentation of LiDAR sequences: Dataset and algorithm.\nECCV, 2022.\n[37] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient\ndescent with warm restarts. ICLR, 2017.\n[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. ICLR, 2019.\n[39] J Narasimhamurthy, Karthikeyan Vaiapury, Ramanathan\nMuthuganapathy, and Balamuralidhar Purushothaman.\nHierarchical-based semantic segmentation of 3D point cloud\nusing deep learning. Smart Computer Vision, 2023.\n[40] Jeremie Papon, Alexey Abramov, Markus Schoeler, and Flo-\nrentin Worgotter. V oxel cloud connectivity segmentation-\nsupervoxels for point clouds. CVPR, 2013.\n[41] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik\nPark. Fast point transformer. CVPR, 2022.\n[42] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointNet: Deep learning on point sets for 3D classification\nand segmentation. CVPR, 2017.\n[43] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nNet++: Deep hierarchical feature learning on point sets in a\nmetric space. NeurIPS, 2017.\n[44] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan\nHammoud, Mohamed Elhoseiny, and Bernard Ghanem. Point-\nNeXt: Revisiting PoinNet++ with improved training and scal-\ning strategies. NeurIPS, 2022.\n[45] Xingwen Quana, Binbin Hea, Marta Yebrab, Changmin Yina,\nZhanmang Liaoa, Xueting Zhanga, and Xing Lia. Hierarchi-\ncal semantic segmentation of urban scene point clouds via\ngroup proposal and graph attention network. International\nJournal of Applied Earth Observations and Geoinformation,\n2016.\n[46] Hugo Raguet and Loic Landrieu. Parallel cut pursuit for\nminimization of the graph total variation. ICML Workshop on\nGraph Reasoning, 2019.\n[47] Haoxi Ran, Jun Liu, and Chengjie Wang. Surface representa-\ntion for point clouds. CVPR, 2022.\n[48] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Oct-\nNet: Learning deep 3D representations at high resolutions.\nCVPR, 2017.\n[49] Damien Robert, Bruno Vallet, and Loic Landrieu. Learn-\ning multi-view aggregation in the wild for large-scale 3D\nsemantic segmentation. CVPR, 2022.\n[50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net:\nConvolutional networks for biomedical image segmentation.\nMICCAI, 2015.\n[51] Martin Simonovsky and Nikos Komodakis. Dynamic edge-\nconditioned filters in convolutional neural networks on graphs.\nCVPR, 2017.\n[52] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,\nBeatriz Marcotegui, Fran c ¸ois Goulette, and Leonidas J\nGuibas. KPConv: Flexible and deformable convolution for\npoint clouds. ICCV, 2019.\n[53] Anirud Thyagharajan, Benjamin Ummenhofer, Prashant Lad-\ndha, Om Ji Omer, and Sreenivas Subramoney. Segment-\nfusion: Hierarchical context fusion for robust 3D semantic\nsegmentation. CVPR, 2022.\n[54] Wei-Chih Tu, Ming-Yu Liu, Varun Jampani, Deqing Sun,\nShao-Yi Chien, Ming-Hsuan Yang, and Jan Kautz. Learning\nsuperpixels with segmentation-aware affinity loss. CVPR,\n2018.\n[55] Nina Varney, Vijayan K Asari, and Quinn Graehling. DALES:\nA large-scale aerial LiDAR data set for semantic segmentation.\nCVPR Workshops, 2020.\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NeurIPS, 2017.\n[57] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adri-\nana Romero, Pietro Lio, and Yoshua Bengio. Graph attention\nnetworks. ICLR, 2018.\n[58] Qi Wang, Shengge Shi, Jiahui Li, Wuming Jiang, and Xi-\nangde Zhang. Window normalization: Enhancing point cloud\nunderstanding by unifying inconsistent point densities. 2022.\n[59] Yongchao Xu, Thierry G ´eraud, and Laurent Najman. Hi-\nerarchical image simplification and segmentation based on\nmumford–shah-salient level line selection. Pattern Recogni-\ntion Letters, 2016.\n[60] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan ¨O\nArik, and Tomas Pfister. Nested hierarchical transformer:\nTowards accurate, data-efficient and interpretable visual un-\nderstanding. AAAI, 2022.\n[61] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and\nVladlen Koltun. Point transformer. ICCV, 2021.\n10\nAppendix\nIn this document, we introduce our interactive visualiza-\ntion tool (Section A-1), share our source code (Section A-2),\ndiscuss limitations of our approach (Section A-3), provide\na description (Section A-4) and an analysis (Section A-5)\nof all handcrafted features used by our method, detail the\nconstruction of the superpoint-graphs (Section A-6) and the\npartition process (Section A-7), and provide guidelines on\nhow to choose the partition’s hyperparameters (Section A-8).\nFinally, we clarify our architecture parameters (Section A-9),\nexplore our model’s salability (Section A-10) and supervi-\nsion (Section A-11), detail the class-wise performance of our\napproach on each dataset (Section A-12), and the color maps\nused in the illustrations of the main paper (Figure A-3).\nA-1. Interactive Visualization\nWe release for this project an interactive plotly visual-\nization tool that produces HTML files compatible with any\nbrowser. As shown in Figure A-1, we can visualize samples\nfrom S3DIS, KITTI-360, and DALES with different point\nattributes and from any angle. These visualizations were\ninstrumental in designing and validating our model and we\nhope that they will facilitate the reader’s understanding as\nwell.\nA-2. Source Code\nWe make our source code publicly available atgithub.\ncom/drprojects/superpoint_transformer.\nThe code provides all necessary instructions for installing\nand navigating the project, simple commands to reproduce\nour main results on all datasets, ready-to-use pretrained\nmodels, and ready-to-use notebooks.\nOur method is developed in PyTorch and relies on Py-\nTorch Geometric, PyTorch Lightning, and Hydra.\nA-3. Limitations\nOur model provides significant advantages in terms of\nspeed and compacity but also comes with its own set of\nlimitations.\nOverfitting and Scaling. The superpoint approach drasti-\ncally simplifies and compresses the training sets: the 274m\n3D points of S3DIS are captured by a geometry-driven mul-\ntilevel graph structure with fewer than 1.25m nodes. While\nthis simplification favors the compacity and speed of the\ntraining of the model, this can lead to overfitting when using\nSPT configurations with more parameters, as shown in Sec-\ntion A-10. Scaling our model to millions of parameters may\nonly yield better results for training sets that are sufficiently\nlarge, diverse, and complex.\nErrors in the Partition. Object boundaries lacking obvi-\nous discontinuities, such as curbs vs. roads or whiteboards\nvs. walls, are not well recovered by our partition. As parti-\ntion errors cannot be corrected with our approach, this may\nlead to classification errors. To improve this, we could re-\nplace our handcrafted point descriptors (Section A-4) with\nfeatures directly learned for partitioning [26, 23]. However,\nsuch methods significantly increase the preprocessing time,\ncontradicting our current focus on efficiency. In line with\n[21, 47], we use easy-to-compute yet expressive handcrafted\nfeatures. Our model SPT-nano without point encoder relies\npurely on such features and reaches 70.8 mIoU on S3DIS\n6-Fold with only 27k param, illustrating this expressivity.\nLearning Through the Partition. The idea of learning\npoint and adjacency features directly end-to-end is a promis-\ning research direction to improve our model. However, this\nimplies efficiently backpropagating through superpoint hard\nassignments, which remains an open problem. Furthermore,\nsuch a method would consider individual 3D points during\ntraining, which would necessitate to perform the partitioning\nstep multiple times during training time, which may negate\nthe efficiency of our method\nPredictions. Finally, our method predicts labels at the su-\nperpoint level P1 and not individual 3D points. Since this\nmay limit the maximum performance achievable by our ap-\nproach, we could consider adding an upsampling layer to\nmake point-level predictions. However, this does not appear\nto us as the most profitable research direction. Indeed, this\nmay negate some of the efficiency of our method. Further-\nmore, as shown in the ablation study 4.3 d) of the main paper,\nthe “oracle” model outperforms ours by a large margin. This\nmay indicate that performance improvements should primar-\nily be searched in superpoint classification rather than in\nimproving the partition.\nOur model also learns features for superpoints and not\nindividual 3D points. This may limit downstream tasks\nrequiring 3D point features, such as surface reconstruction\nor panoptic segmentation. However, we argue that specific\nadaptations could be explored to perform these tasks at the\nsuperpoint level.\nA-4. Handcrafted Features\nOur method relies on simple handcrafted features to build\nthe hierarchical partition and learn meaningful points and\nadjacency relationships. In this section, we provide further\ndetails on the definition of these features and how to com-\npute them. It is important to note that these features are\nonly computed once during preprocessing, and thanks to\nour optimized implementation, this step only takes a few\nminutes.\n11\n(a) Position\n (b) Ground Truth\n (c) Linearity, Planarity & Verticality\n(d) RGB\n (e) Predictions & Errors\n (f) Level-2\nFigure A-1: Interactive Visualization. Our interactive viewing tool allows for the manipulation and visualization of sample\npoint clouds colorized according to their position (a), semantic labels (b), selected geometric features (c), radiometry (d), and\nto visualize our network’s prediction (e) and partitions (f).\nPoint Features. We can associate each 3D point with a set\nof 8 easy-to-compute handcrafted features, described below.\n• Radiometric features (3 or 1): RGB colors are available\nfor S3DIS and KITTI-360, and intensity values for\nDALES. These radiometric features are normalized to\n[0, 1] at preprocessing time. For KITTI-360, we find\nthat using the HSV color model yields better results.\n• Geometric features (5): We use PCA-based features:\nlinearity, planarity, scattering, [9] and verticality [17],\ncomputed on the set of 50-nearest neighbors of each\npoint. This neighbor search is only computed once\nduring preprocessing and is also necessary to build\nthe graph G. We also define elevation as the distance\nbetween a point and the ground below it. Since the\nground is neither necessarily flat nor horizontal, we use\nthe RANSAC algorithm [13] on a coarse subsampling\nof the scene to find a ground plane. We normalize\nthe elevation by dividing it by 4 for S3DIS and 20 for\nDALES and KITTI-360.\nAt preprocessing time, we only use radiometric and ge-\nometric features to compute the hierarchical partition. At\ntraining time, SPT computes point embeddings by mapping\nall available point features, along with the normalized point\nposition to a vector of size Dpoint with a dedicated MLP ϕ0\nenc.\nWe provide an illustration of the geometric point features\nin Figure A-2, to help the reader apprehend these simple\ngeometric descriptors.\nAdjacency Features. The relationship between adjacent\nsuperpoints provides crucial information to leverage their\ncontext. For each edge of the superpoint-graph, we compute\nthe 18 following features:\n• Interface features (7): All adjacent superpoints share an\ninterface, i.e. pairs of points from each superpoint that\nare close and share a line of sight. SuperpointGraph\n[29] uses the Delaunay triangulation of the entire point\ncloud to compute such interfaces, while we propose\na faster heuristic approach in Section A-6 called the\nApproximate Superpoint Gap algorithm. Each pair of\npoints of an interface defines an offset, i.e. a vector\npointing from one superpoint to its neighbor. We com-\npute the mean offset (dim 3), the mean offset length\n(dim 1), and the standard deviation of the offset in each\ncanonical direction (dim 3).\n12\n(a) Input\n (b) Linearity\n (c) Planarity\n(d) Scattering\n (e) Verticality\n (f) Elevation\nFigure A-2: Point Geometric Features. Given an input cloud (a), the computed PCA-based geometric features (b, c, d, e) and\ndistance to the ground (f) offer a simple characterization of the local geometry around each point.\nTable A-1: Ablation on Handcrafted Features. Impact of\nhandcrafted features on the mIoU for all tested datasets.\nExperiment S3DIS KITTI DALES\n6-Fold 360 Val\nBest Model 76.0 63.5 79.6\na) Point Features\nNo radiometric feat. -2.7 -4.0 -1.2\nNo geometric feat. -0.7 -4.1 -1.4\nb) Adjacency Features\nNo interface feat. -0.2 -0.6 -0.7\nNo ratio feat. -1.1 -2.2 -0.4\nNo pose feat. -5.5 -1.2 -0.8\nc) Room Features\nRoom-level samples -3.8 - -\nNormalized Room pos. -0.7 - -\n• Ratio features (4): As defined in [29], we characterize\neach pair of adjacent superpoints with the ratio of their\nlengths, surfaces, volumes, and point counts.\n• Pose features (7): For each superpoint, we define a nor-\nmal vector as its principal component with the smallest\neigenvalue. We then characterize the relative position\nbetween two superpoints with the cosine of the angle\nbetween the superpoint normal vectors (dim: 1) and\nbetween each of the two superpoints’ normal and the\nmean offset direction (dim: 2). Additionally, the off-\nset between the centroids of the superpoints is used to\ncompute the centroid distance (dim: 1) and the unit-\nnormalized centroid offset direction (dim: 3).\nNote that the mean offset and the ratio features are not\nsymmetric and imply that the edges of the superpoint-graphs\nare oriented. As mentioned in Section 3.3, a network ϕi\nadj\nmaps these handcrafted features to a vector of size Dkey +\nDque + Dval, for each level i ≥ 1 of the encoder and the\ndecoder.\nA-5. Influence of Handcrafted Features\nIn Table A-1, we quantify the impact of the handcrafted\nfeatures detailed in Section A-4 on performance. To this end,\nwe retrain SPT without each feature group and evaluate the\nprediction on S3DIS Area 5.\na) Point Features. Our experiments show that removing\nradiometric features has a strong impact on performance,\nwith a drop of 2.7 to 4.0 mIoU. In contrast, removing ge-\nometric features results in a performance drop of 0.7 on\n13\nS3DIS, but 4.1 on KITTI-360.\nWe observe that both outdoor datasets strongly benefit\nfrom local geometric features, which we hypothesize is due\nto their lower resolution and noise level. These results indi-\ncate that radiometric features play an important role for all\ndatasets and that geometric features may facilitate learning\non noisy or subsampled datasets.\nb) Adjacency Features. The analysis of the impact of ad-\njacency features on our model’s performance indicates that\nthey play a crucial role in leveraging contextual information\nfrom superpoints: removing all adjacency features leads to\na significant drop of 3.0 to 6.3 mIoU points on the datasets,\nas shown in 4.3 b) of the main paper. Among the different\ntypes of adjacency features, pose features appear particularly\nuseful in characterizing the adjacency relationships between\nsuperpoints of S3DIS, while interface features have a smaller\nimpact. These results suggest that the relative pose of objects\nin the scene may have more influence on the 3D semantic\nanalysis performed by our model than the precise character-\nization of their interface. On the other hand, interface and\nratio features seem to have more impact on outdoor datasets,\nwhile the pose information seems to be less informative in\nthe semantic understanding of the scene.\nc) S3DIS Room Partition. The S3DIS dataset is divided\ninto individual rooms aligned along the x and y axes. This\nsetup simplifies the classification of classes such as walls,\ndoors, or windows as they are consistently located at the edge\nof the room samples. Some methods also add normalized\nroom coordinates to each points. However, we argue that\nthis partition may not generalize well to other environments,\nsuch as open offices, industrial facilities, or mobile mapping\nacquisitions, which cannot naturally be split into rooms.\nTo address this limitation, we use the absolute room po-\nsitions to reconstruct the entire floor of each S3DIS area\n[52, 6]. This enables our model to consider large multi-room\nsamples, resulting in a performance increase of 3.8 points.\nThis highlights the advantage of capturing long-range contex-\ntual information. Additionally, we remark that SPT performs\nbetter without using room-normalized coordinates, which\nmay lead to overfitting and poor performance on layouts that\ndeviate from the room-based structure of the S3DIS dataset\nsuch as large amphitheaters.\nA-6. Superpoint-Graphs Computation\nThe Superpoint Graph method by Landrieu and Si-\nmonovsky [29] builds a graph from a point cloud using\nDelaunay triangulation, which can take a long time for large\npoint clouds. In contrast, our approach connects two super-\npoints in Pi, where i ≥ 1 if their closest points are within a\ndistance gap ϵi > 0. However, computing pairwise distances\nfor all points is computationally expensive. We propose a\nheuristic to approximately find the closest pair of points for\ntwo superpoints, see Algorithm A-1. We also accelerate the\ncomputation of adjacent superpoints by approximating only\nfor superpoints with centroids closer than the sum of their\nradii plus the gap distance. This approximation helps to\nreduce the number of computations required for adjacency\ncomputation, which leads to faster processing times. All\nsteps involved in the computation of our superpoint-graph\nare implemented on the GPU to further enhance computa-\ntional efficiency.\nAlgorithm A-1 Approximate Superpoint Gap\nInput: superpoints p1 and p2, num steps\nc1 ← centroid(p1)\nc2 ← centroid(p2)\nfor s ∈ num steps do\nc2 ← arg minp∈p2 ∥c1 − p∥\nc1 ← arg minp∈p1 ∥c2 − p∥\nend for\nreturn ∥c1 − c2∥\nRecovering the interface between two adjacent super-\npoints as evoked in Section A-4 involves a notion of visi-\nbility: we connect points from each superpoint which are\nfacing each other. This can be a challenging and ambigu-\nous problem, which SuperPoint Graph [27] tackles using a\nDelaunay triangulation of the points. However, this method\nis impractical for large point clouds. To address this issue,\nwe propose a heuristic approach with the following steps: (i)\nfirst, we use the Approximate Superpoint Gap algorithm to\ncompute the approximate nearest points for each superpoint.\nThen, we restrict the search to only consider points within a\ncertain distance of the nearest points. Finally, we match the\npoints by sorting them along the principal component of the\nselected points.\nA-7. Details on Hierarchical Partitions\nWe present here a more detailed explanation of the hi-\nerarchical partition process. We define for each point c of\nC a feature fc of dimension D, and G := (C, E, w) is the\nk-nn adjacency between the points, with w ∈ RE\n+ a nonnega-\ntive proximity value. Our goal is to compute a hierarchical\nmultilevel partition of the point cloud into superpoints ho-\nmogeneous with respect to f at increasing coarseness.\nPiecewise Constant Approximation on a Graph. We\nfirst explain how to compute a single-level partition of the\npoint cloud. We consider the pointwise features fc as a D-\ndimensional signal f ∈ RD×|C| defined on the nodes of the\nweighted graph G := (C, E, w). We first define an energy\nJ (e; f, G, λ) measuring the fidelity between a vertex-valued\nsignal e ∈ RD×|C| and the length of its contours, defined as\n14\nthe weight of the cut between its constant components [27]:\nJ (e; f, G, λ) := ∥e − f∥2 + λ\nX\n(u,v)∈E\nwu,v [eu ̸= ev] ,\n(A-1)\nwith λ ∈ R+ a regularization strength and [a ̸= b] the\nfunction equals to 0 if a = b and 1 otherwise. Minimizers of\nJ are approximations of f that are piecewise constant with\nrespect to a partition with simple contours in G.\nWe can characterize such signal e ∈ RD×|C| by the\ncoarsest partition Pe of P and its associated variable fe ∈\nRD×|Pe| such that e is constant within each segment p of\nPe with value fe\np . The partition Pe also induces a graph\nˆGe := (Pe, Ee, we) with Ee linking the component of Pe\nadjacent in G and we the weight of the cut between adjacent\nelements of Pe:\nEe := {(U, V) | U, V∈ Pe, (U × V ) ∩ E ̸= ∅} (A-2)\nFor (U, V) ∈ Ee, we\nU,V :=\nX\n(u,v)∈U×V ∩E\nwu,v (A-3)\nWe denote by partition (e) the function mapping e to\nthese uniquely defined variables:\nfe, Pe, ˆGe := partition (e) . (A-4)\nPoint Cloud Hierarchical Partition. A set of partitions\nP := [P0, · ··, Pi] defines a hierarchical partition of C\nwith I levels if P0 = C and Pi+1 is a partition of Pi for\ni ∈ [0, I− 1]. We propose to use the formulations above\nto define a hierarchical partition of the point cloud C char-\nacterized by a list λ1, · ··, λI of nonnegative regularization\nstrengths defining the coarseness of the successive partitions.\nIn particular, We chose λ1 such that |P1|/|P0 ∼ 30 in our\nexperiments.\nWe first define ˆG0 as the point-level adjacency graph ˆG\nand f0 as f. We can now define the levels of a hierarchical\npartition Pi for i ∈ [1, I]:\nfi, Pi, ˆGi := partition( arg min\ne∈RD×|Pi−1|\nJ\n\u0010\ne; fi−1, ˆGi−1, λi−1\n\u0011\n).\n(A-5)\nGiven that the optimization problems defined in Eq. (A-5)\nfor i >1 operate on the component graphs ˆGi, which are\nsmaller than ˆG0, the first partition is the most demanding in\nterms of computation.\nNote that we used the hat notation ˆGi, because these\ngraphs are only used for computing the hierarchical parti-\ntions Pi, and should be distinguished from the the superpoint\ngraphs Gi on which is based our self-attention mechanism,\nconstructed from Pi as explained in Section A-6.\nA-8. Parameterizing the Partition\nWe define G as the k = 10-nearest neighbor adjacency\ngraph and set all edge weights w to 1. The point features fp\nwhose piecewise constant approximation yields the partition\nare of three types: geometric, radiometric, and spatial.\nGeometric features ensure that the superpoints are geo-\nmetrically homogeneous and with simple shapes. We use\nthe normalized dimensionality-based method described in\nSection A-4. Radiometric features encourage the border of\nsuperpoints to follow the color contrast of the scene and are\neither RGB or intensity values; they must be normalized\nto fall in the [0,1] range. Lastly, we can add to each point\ntheir spatial coordinates with a normalization factor µ in\nm−1 to limit the size of the superpoints. We recommend\nsetting µ as the inverse of the maximum radius expected for\na superpoint: the largest sought object (facade, wall, roof) or\nan application-dependent constraint.\nThe coarseness of the partitions depends on the regular-\nization strength λ as defined in Section ??. Finer partitions\nshould generally lead to better results but to an increase in\ntraining time and memory requirement. We chose a ratio\n| P0 | / | P1 |∼ 30 across all datasets as it proved to be a\ngood compromise between efficiency and precision. Depend-\ning on the desired trade-off, different ratios can be chosen\nby trying other values of λ.\nA-9. Implementation Details\nWe provide the exact parameterization of the SPT archi-\ntecture used for our experiments. All MLPs in the architec-\nture use LeakyReLU activations and GraphNorm [5] normal-\nization. For simplicity, we represent an MLP by the list of its\nlayer widths: [in channels, hidden channels, out channels].\nPoint Input Features. We refer here to the dimension\nof point positions, radiometry, and geometric features as\nDpos\npoint = 3, Dradio\npoint, and Dgeof\npoint = 4respectively. As seen in\nSection A-4, S3DIS and KITTI-360 use Dradio\npoint = 3, while\nDALES uses Dradio\npoint = 1.\nModel Architecture. The exact architecture SPT-64 used\nfor S3DIS and DALES is detailed in Table A-2. The other\nmodels evaluated are SPT-16, SPT-32, SPT-128 (used for\nKITTI-360), and SPT-256, which use the same parameters\nexcept for Dval.\nSPT-nano. For SPT-nano, we use andDval = 16, Dadj =\n16, and Dkey = 2. As SPT-nano does not compute point\nembedding, it does not useϕ0, and we set upϕ1\nenc as [Dhf\npoint +\nDpos\npoint, Dval, Dval].\n15\nTable A-2: Model Configuration. We provide the detailed\narchitecture of the SPT-X architecture. In this paper, we use\nX = 64and X = 128.\nParameter Value\nHandcrafted features\nDhf\npoint Dradio\npoint + Dgeof\npoint\nDhf\nadj 18\nEmbeddings sizes\nDpoint 128\nDadj 32\nTransformer blocks\nDval X\nDkey 4\n# blocks encoder 3\n# blocks decoder 1\n# heads 16\nMLPs\nϕi\nadj [Dhf\nadj, Dadj, Dadj, 3Dadj]\nϕ0\nenc [Dhf\npoint + Dpos\npoint, 32, 64, Dpoint]\nϕ1\nenc [Dpoint + Dpos\npoint, Dval, Dval]\nϕ2\nenc [Dval + Dpos\npoint, Dval, Dval]\nϕ1\ndec [Dval + Dval + Dpos\npoint, Dval, Dval]\nA-10. Model Scalability\nWe study the scalability of SPT by comparing models\nwith different parameter counts on each dataset. It is im-\nportant to note that the superpoint approach drastically com-\npresses the training set, which can lead to overfitting, see\nSection A-3. For example, as illustrated in Table A-3, SPT-\n128 with Dval = 128 (777k param.) performs 1.4 points\nbelow Dval = 64on S3DIS.\nWe report a similar behavior for other hyperparameters:\nin Table A-4, Dkey = 8 instead of 4 incurs a drop of 1.0,\nwhile in Table A-5, Nheads = 32 instead of 16 a drop of\n0.1 point. For the larger KITTI-360 dataset ( 13m nodes),\nDval = 128performs 1.9 points above Dval = 64, but 5.4\npoints above Dval = 256(2.7m param.).\nTable A-3: Impact of Model Scaling. Impact of model size\nfor each dataset.\nModel Size S3DIS KITTI DALES\n×106 6-Fold 360 Val\nSPT-32 0.14 74.5 60.6 78.7\nSPT-64 0.21 76.0 61.6 79.6\nSPT-128 0.77 74.6 63.5 78.8\nSPT-256 1.80 74.0 58.1 77.6\nTable A-4: Impact of Query-Key Dimension. Impact of\nDkey on S3DIS 6-Fold.\nDkey 2 4 8 16\nSPT-64 75.6 76.0 75.0 74.7\nTable A-5: Impact of Heads Count. Impact of the number\nof heads Nhead on the S3DIS 6-Fold performance.\nNhead 4 8 16 32\nSPT-64 74.3 75.2 76.0 75.9\nA-11. Hierarchical Supervision\nWe explore, in Table A-6, alternatives to our hierarchical\nsupervision introduced in Section 3.3 : predicting the most\nfrequent label for P1 and the distribution for P2. We use\n“freq-Pi” to refer to the prediction of the most frequent label\napplied the Pi partition. Similarly, “ dist-Pi” denotes the\nprediction of the distribution of labels within each superpoint\nof the partition Pi.\nWe observe a consistent improvement across all datasets\nby adding the dist-Pi supervision. This illustrates the bene-\nfits of supervising higher-level partitions, despite their lower\npurity. Moreover, supervising P1 with the distribution rather\nthan the most frequent label leads to a further performance\ndrop. This validates our choice to consider P1 superpoints\nas sufficiently pure to be supervised using their dominant\nlabel.\nTable A-6: Ablation on Supervision. Impact of our hierar-\nchical supervision for each dataset.\nLoss S3DIS KITTI DALES\n6-Fold 360 Val\nfreq-Pi-P1 dist-Pi-P2 76.0 63.5 79.6\nfreq-P1 -0.2 -0.8 -0.8\ndist-Pi-P1 -0.8 -1.3 -0.8\nA-12. Detailed Results\nWe report in Table A-7 the class-wise performance across\nall datasets for SPT and other methods for which this in-\nformation was available. As previously stated, SPT per-\nforms close to state-of-the-art methods on all datasets, while\nbeing significantly smaller and faster to train. By design,\nsuperpoint-based methods can capture long-range interac-\ntions and their predictions are more spatially regular than\npoint-based approaches. This may explain the performance\nof SPT on S3DIS, which encompasses large, geometrically\nhomogeneous objects or whose identification requires long-\nrange context understanding, such as ceiling, floor, columns,\n16\nTable A-7: Class-wise Performance. Class-wise mIoU across all datasets for our Superpoint Transformer .\nS3DIS Area 5\nMethod mIoU ceiling floor wall beam column window door chair table bookcase sofa board clutter\nPointNet [42] 41.1 88.8 97.3 69.8 0.1 3.9 46.3 10.8 52.6 58.9 40.3 5.9 26.4 33.2\nSPG [29] 58.4 89.4 96.9 78.1 0.0 42.8 48.9 61.6 84.7 75.4 69.8 52.6 2.1 52.2\nMinkowskiNet [8] 65.4 91.8 98.7 86.2 0.0 34.1 48.9 62.4 81.6 89.8 47.2 74.9 74.4 58.6\nSPG + SSP [26] 61.7 91.9 96.7 80.8 0.0 28.8 60.3 57.2 85.5 76.4 70.5 49.1 51.6 53.3\nKPConv [52] 67.1 92.8 97.3 82.4 0.0 23.9 58.0 69.0 91.0 81.5 75.3 75.4 66.7 58.9\nPointTrans.[61] 70.4 94.0 98.5 86.3 0.0 38.0 63.4 74.3 89.1 82.4 74.3 80.2 76.0 59.3\nDeepViewAgg [49] 67.2 87.2 97.3 84.3 0.0 23.4 67.6 72.6 87.8 81.0 76.4 54.9 82.4 58.7\nStratified PT [25] 72.0 96.2 98.7 85.6 0.0 46.1 60.0 76.8 92.6 84.5 77.8 75.2 78.1 64.0\nSPT 68.9 92.6 97.7 83.5 0.2 42.0 60.6 67.1 88.8 81.0 73.2 86.0 63.1 60.0\nSPT-nano 64.9 92.4 97.1 81.6 0.0 38.2 56.4 58.6 86.3 77.3 69.6 82.5 50.5 53.4\nS3DIS 6-FOLD\nPointNet [42] 47.6 88.0 88.7 69.3 42.4 23.1 47.5 51.6 42.0 54.1 38.2 9.6 29.4 35.2\nSPG [29] 62.1 89.9 95.1 76.4 62.8 47.1 55.3 68.4 73.5 69.2 63.2 45.9 8.7 52.9\nConvPoint [4] 68.2 95.0 97.3 81.7 47.1 34.6 63.2 73.2 75.3 71.8 64.9 59.2 57.6 65.0\nMinkowskiNet [8, 49] 69.5 91.2 90.6 83.0 59.8 52.3 63.2 75.7 63.2 64.0 69.0 72.1 60.1 59.2\nSPG + SSP [26] 68.4 91.7 95.5 80.8 62.2 54.9 58.8 68.4 78.4 69.2 64.3 52.0 54.2 59.2\nKPConv [52] 70.6 93.6 92.4 83.1 63.9 54.3 66.1 76.6 57.8 64.0 69.3 74.9 61.3 60.3\nDeepViewAgg [49] 74.7 90.0 96.1 85.1 66.9 56.3 71.9 78.9 79.7 73.9 69.4 61.1 75.0 65.9\nSPT 76.0 93.9 96.3 84.3 71.4 61.3 70.1 78.2 84.6 74.1 67.8 77.1 63.6 65.0\nSPT-nano 70.8 93.1 96.0 80.9 68.4 54.0 62.2 71.3 76.3 70.8 63.3 74.3 51.9 57.6\nKITTI-360 Val\nMethod mIoU\nroad\nsidewalk\nbuilding\nwall\nfence\npole\ntraffic lig.\ntraffic sig.\nvegetation\nterrain\nperson\ncar\ntruck\nmotorcycle\nbicycle\nMinkowskiNet [8, 49] 54.2 90.6 74.4 84.5 45.3 42.9 52.7 0.5 38.6 87.6 70.3 26.9 87.3 66.0 28.2 17.2\nDeepViewAgg [49] 57.8 93.5 77.5 89.3 53.5 47.1 55.6 18.0 44.5 91.8 71.8 40.2 87.8 30.8 39.6 26.1\nSPT 63.5 93.3 79.3 90.8 56.2 45.7 52.8 20.4 51.4 89.8 73.6 61.6 95.1 79.0 53.1 10.9\nSPT-nano 57.2 91.7 74.7 87.8 49.3 38.8 49.0 12.2 39.2 88.0 69.5 39.9 94.2 80.1 33.7 10.4\nDALES\nMethod mIoU ground vegetation car truck power line fence pole building\nPointNet++ [43] 68.3 94.1 91.2 75.4 30.3 79.9 46.2 40.0 89.1\nConvPoint [4] 67.4 96.9 91.9 75.5 21.7 86.7 29.6 40.3 96.3\nSPG [29] 60.6 94.7 87.9 62.9 18.7 65.2 33.6 28.5 93.4\nPointCNN [30] 58.4 97.5 91.7 40.6 40.8 26.7 52.6 57.6 95.7\nKPConv [52] 81.1 97.1 94.1 85.3 41.9 95.5 63.5 75.0 96.6\nSPT 79.6 96.7 93.1 86.1 52.4 94.0 52.7 65.3 96.7\nSPT-nano 75.2 96.5 92.6 78.1 35.8 92.1 50.8 59.9 96.0\nand windows. For all datasets, results show that some\nprogress could be made in analyzing smaller objects with\nintricate geometries. This suggests that a more powerful\npoint-level encoding may be beneficial.\n17\nS3DIS\nceiling floor wall beam column\nwindow door chair table bookcase\nsofa board clutter unlabeled\nKITTI-360\nroad sidewalk building wall fence\npole traffic light traffic sign vegetation terrain\nperson car truck motorcycle bicycle\nignored\nDALES\nground vegetation car truck power line\nfence pole building unknown\nFigure A-3: Colormaps.\n18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7137877345085144
    },
    {
      "name": "Segmentation",
      "score": 0.5050337910652161
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47029516100883484
    },
    {
      "name": "Transformer",
      "score": 0.4567263722419739
    },
    {
      "name": "Natural language processing",
      "score": 0.44520798325538635
    },
    {
      "name": "Computer vision",
      "score": 0.4274942874908447
    },
    {
      "name": "Engineering",
      "score": 0.09267592430114746
    },
    {
      "name": "Electrical engineering",
      "score": 0.08200964331626892
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210154111",
      "name": "Université Gustave Eiffel",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210129154",
      "name": "Centre Val de Loire",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210143826",
      "name": "Institut National des Sciences Appliquées Centre Val de Loire",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210136805",
      "name": "Laboratoire d'Informatique Fondamentale et Appliquée de Tours",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210152518",
      "name": "Laboratoire d'Informatique Gaspard-Monge",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I142631665",
      "name": "École nationale des ponts et chaussées",
      "country": "FR"
    }
  ],
  "cited_by": 90
}