{
  "title": "T-NER: An All-Round Python Library for Transformer-based Named Entity Recognition",
  "url": "https://openalex.org/W3155429205",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2397341542",
      "name": "Asahi Ushio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2565581196",
      "name": "Jose Camacho Collados",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2760505947",
    "https://openalex.org/W2787423662",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W1990057272",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3015529893",
    "https://openalex.org/W2004763266",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2088911157",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2771669035",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034328552",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2971039193",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2983651159",
    "https://openalex.org/W4237579218",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2963625095"
  ],
  "abstract": "Language model (LM) pretraining has led to consistent improvements in many\\nNLP downstream tasks, including named entity recognition (NER). In this paper,\\nwe present T-NER (Transformer-based Named Entity Recognition), a Python library\\nfor NER LM finetuning. In addition to its practical utility, T-NER facilitates\\nthe study and investigation of the cross-domain and cross-lingual\\ngeneralization ability of LMs finetuned on NER. Our library also provides a web\\napp where users can get model predictions interactively for arbitrary text,\\nwhich facilitates qualitative model evaluation for non-expert programmers. We\\nshow the potential of the library by compiling nine public NER datasets into a\\nunified format and evaluating the cross-domain and cross-lingual performance\\nacross the datasets. The results from our initial experiments show that\\nin-domain performance is generally competitive across datasets. However,\\ncross-domain generalization is challenging even with a large pretrained LM,\\nwhich has nevertheless capacity to learn domain-specific features if fine-tuned\\non a combined dataset. To facilitate future research, we also release all our\\nLM checkpoints via the Hugging Face model hub.\\n",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 53–62\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n53\nT-NER: An All-Round Python Library\nfor Transformer-based Named Entity Recognition\nAsahi Ushio and Jose Camacho-Collados\nSchool of Computer Science and Informatics\nCardiff University, United Kingdom\n{ushioa,camachocolladosj}@cardiff.ac.uk\nAbstract\nLanguage model (LM) pretraining has led\nto consistent improvements in many NLP\ndownstream tasks, including named entity\nrecognition (NER). In this paper, we present\nT-NER1 (Transformer-based Named Entity\nRecognition), a Python library for NER LM\nﬁnetuning. In addition to its practical utility,\nT-NER facilitates the study and investigation\nof the cross-domain and cross-lingual general-\nization ability of LMs ﬁnetuned on NER. Our\nlibrary also provides a web app where users\ncan get model predictions interactively for ar-\nbitrary text, which facilitates qualitative model\nevaluation for non-expert programmers. We\nshow the potential of the library by compil-\ning nine public NER datasets into a uniﬁed for-\nmat and evaluating the cross-domain and cross-\nlingual performance across the datasets. The\nresults from our initial experiments show that\nin-domain performance is generally competi-\ntive across datasets. However, cross-domain\ngeneralization is challenging even with a large\npretrained LM, which has nevertheless capac-\nity to learn domain-speciﬁc features if ﬁne-\ntuned on a combined dataset. To facilitate\nfuture research, we also release all our LM\ncheckpoints via the Hugging Face model hub2\n1 Introduction\nLanguage model (LM) pretraining has become one\nof the most common strategies within the natural\nlanguage processing (NLP) community to solve\ndownstream tasks (Peters et al., 2018; Howard and\nRuder, 2018; Radford et al., 2018, 2019; Devlin\net al., 2019). LMs trained over large textual data\nonly need to be ﬁnetuned on downstream tasks\nto outperform most of the task-speciﬁc designed\nmodels. Among the NLP tasks impacted by LM\n1https://github.com/asahi417/tner\n2https://huggingface.co/models?search=\nasahi417/tner.\nFigure 1: System overview of T-NER.\npretraining, named entity recognition (NER) is one\nof the most prevailing and practical applications.\nHowever, the availability of open-source NER li-\nbraries for LM training is limited.3\nIn this paper, we introduce T-NER, an open-\nsource Python library for cross-domain analysis\nfor NER with pretrained Transformer-based LMs.\nFigure 1 shows a brief overview of our library and\nits functionalities. The library facilitates NER ex-\nperimental design including easy-to-use features\nsuch as model training and evaluation. Most no-\ntably, it enables to organize cross-domain analyses\nsuch as training a NER model and testing it on a\ndifferent domain, with a small conﬁguration. We\nalso report initial experiment results, by which we\nshow that although cross-domain NER is challeng-\ning, if it has an access to new domains, LM can\nsuccessfully learn new domain knowledge. The\nresults give us an insight that LM is capable to\nlearn a variety of domain knowledge, but an or-\ndinary ﬁnetuning scheme on single dataset most\nlikely causes overﬁtting and results in poor domain\ngeneralization.\nAs a system design, T-NER is implemented in\n3Recently, spaCy (https://spacy.io/) has released\na general NLP pipeline with pretrained models including a\nNER feature. Although it provides a very efﬁcient pipeline\nfor processing text, it is not suitable for LM ﬁnetuning or\nevaluation on arbitrary NER data.\n54\nPytorch (Paszke et al., 2019) on top of the Trans-\nformers library (Wolf et al., 2019). Moreover, the\ninterfaces of our training and evaluation modules\nare highly inspired by Scikit-learn (Pedregosa et al.,\n2011), enabling an interoperability with recent\nmodels as well as integrating them in an intuitive\nway. In addition to the versatility of our toolkit for\nNER experimentation, we also include an online\ndemo and robust pre-trained models trained across\ndomains. In the following sections, we provide a\nbrief overview about NER in Section 2, explain\nthe system architecture of T-NER with a few ba-\nsic usages in Section 3 and describe experiment\nresults on cross-domain transfer with our library in\nSection 4.\n2 Named Entity Recognition\nGiven an arbitrary text, the task of NER consists\nof detecting named entities and identifying their\ntype. For example, given a sentence ”Dante was\nborn in Florence. ”, a NER model are would iden-\ntify ”Dante” as a person and ”Florence” as a loca-\ntion. Traditionally, NER systems have relied on a\nclassiﬁcation model on top of hand-engineered fea-\nture sets extracted from corpora (Ratinov and Roth,\n2009; Collobert et al., 2011), which was improved\nby carefully designed neural network approaches\n(Lample et al., 2016; Chiu and Nichols, 2016; Ma\nand Hovy, 2016). This paradigm shift was mainly\ndue to its efﬁcient access to contextual information\nand ﬂexibility, as human-crafted feature sets were\nno longer required. Later, contextual representa-\ntions produced by pretrained LMs have improved\nthe generalization abilities of neural network archi-\ntectures in many NLP tasks, including NER (Peters\net al., 2018; Devlin et al., 2019).\nIn particular, LMs see millions of plain texts dur-\ning pretraining, a knowledge that then can be lever-\naged in downstream NLP applications. This prop-\nerty has been studied in the recently literature by\nprobing their generalization capacity (Hendrycks\net al., 2020; Aharoni and Goldberg, 2020; Desai\nand Durrett, 2020; Gururangan et al., 2020). When\nit comes to LM generalization studies in NER, the\nliterature is more limited and mainly restricted to in-\ndomain (Agarwal et al., 2021) or multilingual set-\ntings (Pfeiffer et al., 2020a; Hu et al., 2020b). Our\nlibrary facilitates future research in cross-domain\nand cross-lingual generalization by providing a\nuniﬁed benchmark for several languages and do-\nmain as well as a straightforward implementation\nof NER LM ﬁnetuning.\n3 T-NER: An Overview\nA key design goal was to create a self-contained\nuniversal system to train, evaluate, and utilize NER\nmodels in an easy way, not only for research pur-\npose but also practical use cases in industry. More-\nover, we provide a demo web app (Figure 2) where\nusers can get predictions from a trained model\ngiven a sentence interactively. This way, users\n(even those without programming experience) can\nconduct qualitative analyses on their own or exist-\ning pre-trained models.\nIn the following we provide details on the techni-\ncalities of the package provided, including details\non how to train and evaluate any LM-based archi-\ntecture. Our package, T-NER, allows practitioners\nin NLP to get started working on NER with a few\nlines of code while diving into the recent progress\nin LM ﬁnetuning. We employ Python as our core\nimplementation, as is one of the most prevailing\nlanguages in the machine learning and NLP com-\nmunities. Our library enables Python users to ac-\ncess its various kinds of features such as model\ntraining, in- and cross-domain model evaluation,\nand an interface to get predictions from trained\nmodels with minimum effort.\n3.1 Datasets\nFor model training and evaluation, we compiled\nnine public NER datasets from different domains,\nunifying them into same format: OntoNotes5\n(Hovy et al., 2006), CoNLL 2003 (Tjong Kim Sang\nand De Meulder, 2003), WNUT 2017 (Derczynski\net al., 2017), WikiAnn (Pan et al., 2017), FIN (Sali-\nnas Alvarado et al., 2015), BioNLP 2004 (Collier\nand Kim, 2004), BioCreative V CDR4 (Wei et al.,\n2015), MIT movie review semantic corpus,5 and\nMIT restaurant review.6 These uniﬁed datasets are\nalso made available as part of our T-NER library.\nExcept for WikiAnn that contains 282 languages,\nall the datasets are in English, and only the MIT\ncorpora are lowercased. As MIT corpora are com-\n4The original dataset consists of long documents which\ncannot be fed on LM because of the length, so we split them\ninto sentences to reduce their size.\n5The movie corpus includes two datasets ( eng and\ntrivia10k13) coming from different data sources. While both\nhave been integrated into our library, we only used the largest\ntrivia10k13 in our experiments.\n6The original MIT NER corpora can be downloaded\nfrom https://groups.csail.mit.edu/sls/\ndownloads/.\n55\nFigure 2: A screenshot from the demo web app. In this example, the NER transformer model is ﬁne-tuned on\nOntoNotes 5 and a sample sentence is fetched from Wikipedia (en.wikipedia.org/wiki/Sergio_Mendes).\nmonly used for slot ﬁlling task in spoken language\nunderstanding (Liu and Lane, 2017), the charac-\nteristics of the entities and annotation guidelines\nare quite different from the other datasets, but we\nincluded them for completeness and to analyze the\ndifferences across datasets.\nTable 1 shows statistics of each dataset. In Sec-\ntion 4, we train models on each dataset, and assess\nthe in- and cross-domain accuracy over them.\nDataset format and customization. Users can\nutilize their own datasets for both model training\nand evaluation by formatting them into the IOB\nscheme (Tjong Kim Sang and De Meulder, 2003)\nwhich we used to unify all datasets. In the IOB\nformat, all data ﬁles contain one word per line with\nempty lines representing sentence boundaries. At\nthe end of each line there is a tag which states\nwhether the current word is inside a named entity\nor not. The tag also encodes the type of named\nentity. Here is an example from CoNLL 2003:\nEU B-ORG\nrejects O\nGerman B-MISC\ncall O\nto O\nboycott O\nBritish B-MISC\nlamb O\n. O\n3.2 Model Training\nWe provide modules to facilitate LM ﬁnetuning on\nany given NER dataset. Following Devlin et al.\n(2019), we add a linear layer on top of the last em-\nbedding layer in each token, and train all weights\nwith cross-entropy loss. The model training compo-\nnent relies on the Huggingface transformers library\n(Wolf et al., 2019), one of the largest Python frame-\nworks for distributing pretrained LM checkpoint\nﬁles. Our library is therefore fully compatible with\nthe Transformers framework: once new model was\ndeployed on the Transformer hub, one can imme-\ndiately try those models out with our library as a\nNER model. To reduce computational complexity,\nin addition to enabling multi-GPU support, we im-\nplement mixture precision during model training\nby using the apex library7.\nThe instance of model training in a given\ndataset8 can be used in an intuitive way as dis-\nplayed below:\nfrom tner import TrainTransformersNER\nmodel = TrainTransformersNER(\ndataset=\"ontonotes5\",\ntransformer=\"roberta-base\")\nmodel.train()\nWith this sample code, we would ﬁnetune\n7https://github.com/NVIDIA/apex\n8To use custom datasets, the path to a custom dataset folder\ncan simply be included in the dataset argument.\n56\nName Domain Entity types Data size\nOntoNotes5 News, Blog, Dialogue 18 59,924/8,582/8,262\nCoNLL 2003 News 4 14,041/3,250/3,453\nWNUT 2017 SNS 6 1,000/1,008/1,287\nWikiAnn Wikipedia (282 languages) 3 20,000/10,000/10,000\nFIN Finance 4 1,164/-/303\nBioNLP 2004 Biochemical 5 18,546/-/3,856\nBioCreative V Biomedical 5 5,228/5,330/5,865\nMIT Restaurant Restaurant review 8 7,660/-/1,521\nMIT Movie Movie review 12 7,816/-/1,953\nTable 1: Overview of the NER datasets used in our evaluation and included in T-NER. Data size is the number of\nsentence in training/validation/test set.\nRoBERTaBASE (Liu et al., 2019) on the\nOntoNotes5 dataset. We also provide an easy ex-\ntension to train on multiple datasets at the same\ntime:\nTrainTransformersNER(\ndataset=[\n\"ontonotes5\", \"wnut2017\"\n],\ntransformer=\"roberta-base\")\nOnce training is completed, checkpoint ﬁles with\nmodel weights and other statistics are generated.\nThese are automatically organized for each conﬁg-\nuration and can be easily uploaded to the Hugging\nFace model hub. Ready-to-use code samples can be\nfound in our Google Colab notebook9, and details\nfor additional options and arguments are included\nin the github repository. Finally, our library sup-\nports Tensorboard10 to visualize learning curves.\n3.3 Model Evaluation\nOnce a NER model is trained, users may want to\ntest the models in the same dataset or a different one\nto assess its general performance across domains.\nTo this end, we implemented ﬂexible evaluation\nmodules to facilitate cross-domain evaluation com-\nparison, which is also aided by the uniﬁcation of\ndatasets into the same format (see Section 3.1) with\na unique label reference lookup.\nThe basic usage of the evaluation module is de-\nscribed below.\nfrom tner import TrainTransformersNER\nmodel = TrainTransformersNER(\n\"path-to-model-checkpoint\"\n)\nmodel.test(\"ontonotes5\")\n9https://colab.research.google.com/\ndrive/1AlcTbEsp8W11yflT7SyT0L4C4HG6MXYr?\nusp=sharing\n10www.tensorflow.org/tensorboard\nHere, the model would be tested on OntoNotes5\ndataset, and it could be evaluated on any other test\nset including custom dataset. As with the model\ntraining module, we prepared a Google Colab note-\nbook11 for an example use case, and further details\ncan be found in our github repository.\n4 Evaluation\nIn this section, we assess the reliability of T-NER\nwith experiments in standard NER datasets.\n4.1 Experimental Setting\n4.1.1 Implementation details\nThrough the experiments, we useXLM-R (Liu et al.,\n2019), which has shown to be one of the most re-\nliable multi-lingual pretrained LMs for discrimi-\nnative tasks at the moment. In all experiments we\nmake use of the default conﬁguration and hyper-\npameters of Huggingface’sXLM-R implementation.\nFor WikiAnn/ja (Japanese), we convert the original\ncharacter-level tokenization into proper morpholog-\nical chunk by MeCab12.\n4.1.2 Evaluation metrics and protocols\nAs customary in the NER literature, we reportspan\nmicro-F1 score computed by seqeval13, a Python\nlibrary to compute metrics for sequence predic-\ntion evaluation. We refer to this F1 score as type-\naware F1 score to distinguish it from the the type-\nignored metric used to assess the cross-domain\nperformance, which we explain below.\n11https://colab.research.google.com/\ndrive/1jHVGnFN4AU8uS-ozWJIXXe2fV8HUj8NZ?\nusp=sharing\n12https://pypi.org/project/\nmecab-python3/\n13https://pypi.org/project/seqeval/\n57\nIn a cross-domain evaluation setting, the type-\naware F1 score easily fails to represent the cross-\ndomain performance if the granularity of entity\ntypes differ across datasets. For instance, the MIT\nrestaurant corpus has entities such as amenity and\nrating, while plot and actor are entities from the\nMIT movie corpus. Thus, we report type-ignored\nF1 score for cross-domain analysis. In this type-\nignored evaluation, the entity type from both of\npredictions and true labels is disregarded, reducing\nthe task into a simpler entity span detection task.\nThis evaluation protocol can be customized by the\nuser at test time.\n4.2 Results\nWe conduct three experiments on the nine datasets\ndescribed in Table 1: (i) in-domain evaluation (Sec-\ntion 4.2.1), (ii) cross-domain evaluation (Section\n4.2.2), and (iii) cross-lingual evaluation (Section\n4.2.3). While the ﬁrst experiment tests our imple-\nmentation in standard datasets, the second exper-\niment is aimed at investigating the cross-domain\nperformance of transformer-based NER models.\nFinally, as a direct extension of our evaluation mod-\nule, we show the zero-shot cross-lingual perfor-\nmance of NER models on the WikiAnn dataset.\n4.2.1 In-domain results\nThe main results are displayed in Table 2, where we\nreport the type-aware F1 score from XLM-RBASE\nand XLM-RLARGE models along with current state-\nof-the-art (SoTA). One can conﬁrm that our frame-\nwork with XLM-RLARGE achieves a comparable\nSoTA score, even surpassing it in the WNUT 2017\ndataset. In general, XLM-RLARGE performs consis-\ntently better than XLM-RBASE but, interestingly, the\nbase model performs better than large on the FIN\ndataset. This can be attributed to the limited train-\ning data in this dataset, which may have caused\noverﬁtting in the large model.\nGenerally, it can be expected to get better accu-\nracy with domain-speciﬁc or larger language mod-\nels that can be integrated into our library. Nonethe-\nless, our goal for these experiments were not to\nachieve SoTA but rather to provide a competitive\nand easy-to-use framework. In the remaining ex-\nperiments we report results for XLM-RLARGE only,\nbut the results for XLM-RBASE can be found in the\nappendix.\nDataset BASE LARGE SoTA\nOntoNotes5 89.0 89.1 92.1\nCoNLL 2003 90.8 92.9 94.3\nWNUT 2017 52.8 58.5 50.3\nFIN 81.3 76.4 82.7\nBioNLP 2004 73.4 74.3 77.4\nBioCreative V 88.0 88.6 89.9\nMIT Restaurant 79.4 79.6 -\nMIT Movie 69.9 71.2 -\nWikiAnn/en 82.7 84.0 84.8\nWikiAnn/ja 83.8 86.5 73.3\nWikiAnn/ru 88.6 90.0 91.4\nWikiAnn/es 90.9 92.1 -\nWikiAnn/ko 87.5 89.6 -\nWikiAnn/ar 88.9 90.3 -\nTable 2: In-domain type-aware F1 score for test set\non each dataset with current SoTA. SoTA on each\ndataset is attained from the result of BERT-MRC-DSC\n(Li et al., 2019) for OntoNotes5, LUKE (?) for CoNLL\n2003, CrossWeigh(Wang et al., 2019) for WNUT 2017,\n(Pfeiffer et al., 2020a) for WikiAnn (en, ja, ru, es,\nko, ar), (Salinas Alvarado et al., 2015) for FIN, (Lee\net al., 2020) for BioNLP 2004, (Nooralahzadeh et al.,\n2019) for BioCreative V and (Pfeiffer et al., 2020a) for\nWikiAnn/en.\n4.2.2 Cross-domain results\nIn this section, we show cross-domain evalua-\ntion results on the English datasets14: OntoNotes5\n(ontonotes), CoNLL 2003 (conll), WNUT 2017\n(wnut), WikiAnn/en (wiki), BioNLP 2004 (bionlp),\nand BioCreative V (bc5cdr), FIN (ﬁn). We also\nreport the accuracy of the same XLM-R model\ntrained over a combined dataset resulting from con-\ncatenation of all the above datasets.\nIn Table 3, we present the type-ignored F1 re-\nsults across datasets. Overall cross-domain scores\nare not as competitive as in-domain results. This\ngap reveals the difﬁculty of transferring NER mod-\nels into different domains, which may also be at-\ntributed to different annotation guidelines or data\nconstruction procedures across datasets. Especially,\ntraining on the bionlp and bc5cdr datasets lead to\na null accuracy when they are evaluated on other\ndatasets, as well as others evaluated on them. Those\ndatasets are very domain speciﬁc dataset, as they\nhave entities such as DNA, Protein, Chemical, and\nDisease, which results in a poor adaptation to other\ndomains. On the other hand, there are datasets\n14We excluded the MIT datasets in this setting since they\nare all lowercased.\n58\ntrain\\test ontonotes conll wnut wiki bionlp bc5cdr ﬁn avg\nontonotes 91.6 65.4 53.6 47.5 0.0 0.0 18.3 40.8\nconll 62.2 96.0 69.1 61.7 0.0 0.0 22.7 35.1\nwnut 41.8 85.7 68.3 54.5 0.0 0.0 20.0 31.7\nwiki 32.8 73.3 53.6 93.4 0.0 0.0 12.2 29.6\nbionlp 0.0 0.0 0.0 0.0 79.0 0.0 0.0 8.7\nbc5cdr 0.0 0.0 0.0 0.0 0.0 88.8 0.0 9.8\nﬁn 48.2 73.2 60.9 58.9 0.0 0.0 82.0 38.1\nall 90.9 93.8 60.9 91.3 78.3 84.6 75.5 81.7\nTable 3: Type-ignored F1 score in cross-domain setting over non-lower-cased English datasets. We compute\naverage of accuracy in each test set, named as avg. The model trained on all datasets listed here, is shown as all.\ntest\ntrain en ja ru ko es ar\nen 84.0 46.3 73.1 58.1 71.4 53.2\nja 53.0 86.5 45.7 57.1 74.5 55.4\nru 60.4 53.3 90.0 68.1 76.8 54.9\nko 57.8 62.0 68.6 89.6 66.2 57.2\nes 70.5 50.6 75.8 61.8 92.1 62.1\nar 60.1 55.7 55.7 70.7 79.7 90.3\nTable 4: Cross-lingual type-aware F1 results on vari-\nous languages for the WikiAnn dataset.\nthat are more easily transferable, such as wnut and\nconll. The wnut-trained model achieves 85.7 on\nthe conll dataset and, surprisingly, the conll-trained\nmodel actually works better than the wnut-trained\nmodel when evaluated on the wnut test set. This\ncould be also attributed to the data size, as wnut\nonly has 1,000 sentences, while conll has 14,041.\nNevertheless, the fact that ontonotes has 59,924\nsentences but does not perform better than conll on\nwnut reveals a certain domain similarity between\nconll and wnut.\nFinally, the model trained on the training sets\nof all datasets achieves a type-ignored F1 score\nclose to the in-domain baselines. This indicates\nthat a LM is capable of learning representations of\ndifferent domains. Moreover, leveraging domain\nsimilarity as explained above can lead to better\nresults as, for example, distant datasets such as\nbionlp and bc5cdr surely cause performance drops.\nThis is an example of the type of experiments that\ncould be facilited by T-NER, which we leave for\nfuture work.\n4.2.3 Cross-lingual results\nFinally, we present some results for zero-shot cross-\nlingual NER over the WikiAnn dataset, where\nwe include six distinct languages: English (en),\nJapanese (ja), Russian (ru), Korean (ko), Spanish\n(es), and Arabic (ar). In Table 4, we show the cross-\nlingual evaluation results. The diagonal includes\nthe results of the model trained on the training data\nof the same target language. There are a few inter-\nesting ﬁndings. First, we observe a high correlation\nbetween Russian and Spanish, which are generally\nconsidered to be distant languages and do not share\nthe alphabet. Second, Arabic also transfers well to\nSpanish which, despite the Arabic (lexical) inﬂu-\nence on the Spanish language (Stewart et al., 1999),\nare still languages from distant families.\nClearly, this is a shallow cross-lingual analysis,\nbut it highlights the possibilities of our library for\nresearch in cross-lingual NER. Recently, (Hu et al.,\n2020a) proposed a compilation of multilingual\nbenchmark tasks including the WikiAnn datasets\nas a part of it, and XLM-R proved to be a strong\nbaseline on multilingual NER. This is in line with\nthe results of Conneau et al. (2020), which showed\na high capacity of zero-shot cross-lingual trans-\nferability. On this respect, Pfeiffer et al. (2020b)\nproposed a language/task speciﬁc adapter module\nthat can further improve cross-lingual adaptation in\nNER. Given the possibilities and recent advances\nin cross-lingual language models in recent years,\nwe expect our library to help practitioners to exper-\niment and test these advances in NER.\n5 Conclusion\nIn this paper, we have presented a Python library\nto get started with Transformer-based NER mod-\nels. This paper especially focuses on LM ﬁnetun-\ning, and empirically shows the difﬁculty of cross-\ndomain generalization in NER. Our framework is\ndesigned to be as simple as possible so that any\nlevel of users can start running experiments on\n59\nNER on any given dataset. To this end, we have\nalso facilitated the evaluation by unifying some of\nthe most popular NER datasets in the literature,\nincluding languages other than English. We be-\nlieve that our initial experiment results emphasize\nthe importance of NER generalization analysis, for\nwhich we hope that our open-source library can\nhelp NLP community to convey relevant research\nin an efﬁcient and accessible way.\nAcknowledgements\nWe would like to thank Dimosthenis Antypas for\ntesting our library and the anonymous reviewers\nfor their useful comments.\nReferences\nOshin Agarwal, Yinfei Yang, Byron C Wallace, and\nAni Nenkova. 2021. Entity-switched datasets: An\napproach to auditing the in-domain robustness of\nnamed entity recognition models. arXiv preprint\narXiv:2004.04123.\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7747–\n7763, Online. Association for Computational Lin-\nguistics.\nJason P.C. Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional LSTM-CNNs. Trans-\nactions of the Association for Computational Lin-\nguistics, 4:357–370.\nNigel Collier and Jin-Dong Kim. 2004. Introduc-\ntion to the bio-entity recognition task at JNLPBA.\nIn Proceedings of the International Joint Workshop\non Natural Language Processing in Biomedicine\nand its Applications (NLPBA/BioNLP), pages 73–78,\nGeneva, Switzerland. COLING.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of machine learning research ,\n12(ARTICLE):2493–2537.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Emerging cross-\nlingual structure in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6022–\n6034, Online. Association for Computational Lin-\nguistics.\nLeon Derczynski, Eric Nichols, Marieke van Erp, and\nNut Limsopatham. 2017. Results of the WNUT2017\nshared task on novel and emerging entity recogni-\ntion. In Proceedings of the 3rd Workshop on Noisy\nUser-generated Text, pages 140–147, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nShrey Desai and Greg Durrett. 2020. Calibra-\ntion of pre-trained transformers. arXiv preprint\narXiv:2003.07892.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. arXiv preprint arXiv:2004.06100.\nEduard Hovy, Mitchell Marcus, Martha Palmer, Lance\nRamshaw, and Ralph Weischedel. 2006. OntoNotes:\nThe 90% solution. In Proceedings of the Human\nLanguage Technology Conference of the NAACL,\nCompanion Volume: Short Papers , pages 57–60,\nNew York City, USA. Association for Computa-\ntional Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020a. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In International Conference on Machine\nLearning (ICML).\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020b. Xtreme: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalization. arXiv preprint arXiv:2003.11080.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\n60\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nXiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun\nLiang, Fei Wu, and Jiwei Li. 2019. Dice loss\nfor data-imbalanced nlp tasks. arXiv preprint\narXiv:1911.02855.\nBing Liu and Ian Lane. 2017. Multi-domain adversar-\nial learning for slot ﬁlling in spoken language under-\nstanding. arXiv preprint arXiv:1711.11310.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1064–1074, Berlin, Ger-\nmany. Association for Computational Linguistics.\nFarhad Nooralahzadeh, Jan Tore Lønning, and Lilja\nØvrelid. 2019. Reinforcement-based denoising of\ndistantly supervised ner with partial annotation. In\nProceedings of the 2nd Workshop on Deep Learning\nApproaches for Low-Resource NLP (DeepLo 2019),\npages 225–233.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in neural information processing systems ,\npages 8026–8037.\nFabian Pedregosa, Ga ¨el Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, et al. 2011. Scikit-learn:\nMachine learning in python. the Journal of machine\nLearning research, 12:2825–2830.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020a. Mad-x: An adapter-based frame-\nwork for multi-task cross-lingual transfer. arXiv\npreprint arXiv:2005.00052.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nLev Ratinov and Dan Roth. 2009. Design chal-\nlenges and misconceptions in named entity recog-\nnition. In Proceedings of the Thirteenth Confer-\nence on Computational Natural Language Learning\n(CoNLL-2009), pages 147–155, Boulder, Colorado.\nAssociation for Computational Linguistics.\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Tim-\nothy Baldwin. 2015. Domain adaption of named en-\ntity recognition to support credit risk assessment. In\nProceedings of the Australasian Language Technol-\nogy Association Workshop 2015 , pages 84–90, Par-\nramatta, Australia.\nMiranda Stewart et al. 1999. The Spanish language\ntoday. Psychology Press.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147.\nZihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu, Ji-\nacheng Liu, and Jiawei Han. 2019. Crossweigh:\nTraining named entity tagger from imperfect anno-\ntations. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5157–5166.\nChih-Hsuan Wei, Yifan Peng, Robert Leaman, Al-\nlan Peter Davis, Carolyn J Mattingly, Jiao Li,\nThomas C Wiegers, and Zhiyong Lu. 2015.\nOverview of the biocreative v chemical disease re-\nlation (cdr) task. In Proceedings of the ﬁfth BioCre-\native challenge evaluation workshop, volume 14.\n61\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nA Appendices\nIn all experiments we make use of the default\nconﬁguration and hyperpameters of Huggingface’s\nXLM-R implementation.\nA.1 Cross-lingual Results\nIn this section, we show cross-lingual analysis\non XLM-RBASE, where the result is shown in Ta-\nble 5. For these cross-lingual results, we rely on\nthe WikiAnn dataset where zero-shot cross-lingual\nNER over six distinct languages is conducted: En-\nglish (en), Japanese (ja), Russian (ru), Korean (ko),\nSpanish (es), and Arabic (ar).\nA.2 Cross-domain Results\nIn this section, we show a few more results on\nour cross-domain analysis, which is based on\nnon-lowercased English datasets: OntoNotes5\n(ontonotes), CoNLL 2003 (conll), WNUT 2017\n(wnut), WikiAnn/en (wiki), BioNLP 2004 (bionlp),\nand BioCreative V (bc5cdr), and FIN (ﬁn). Table 6\nshows the type-aware F1 score of the XLM-RLARGE\nand XLM-RBASE models trained on all the datasets.\nFurthermore, Table 7 shows additional results for\nXLM-RBASE in the type-ignored evaluation.\ntest\ntrain en ja ru ko es ar\nen 82.8 38.6 65.7 50.4 73.8 44.5\nja 53.8 83.9 46.9 60.1 71.3 46.3\nru 51.9 39.9 88.7 51.9 66.8 51.0\nko 54.7 51.6 53.3 87.5 63.3 52.3\nes 65.7 44.0 66.5 54.1 90.9 59.4\nar 53.1 49.2 49.4 59.7 73.6 88.9\nTable 5: Cross-lingual type-aware F1 score over\nWikiAnn dataset with XLM-RBASE.\nCross-domain results with lowercased datasets.\nIn this section, we show cross-domain results on the\nEnglish datasets including lowercased corpora such\nas MIT Restaurant (restaurant) and MIT Movie\n(movie). Since those datasets are lowercasd, we\nuppercase lowercase\nDatasets BASE LARGE BASE LARGE\nontonotes 85.8 87.8 81.7 85.6\nconll 87.2 90.3 82.8 87.6\nwnut 49.6 55.1 43.7 51.3\nwiki 79.1 82.7 75.2 80.8\nbionlp 72.9 74.1 71.7 74.0\nbc5cdr 79.4 85.0 78.0 84.2\nﬁn 72.4 72.4 72.4 73.5\nrestaurant - - 76.8 80.9\nmovie - - 67.8 71.8\nTable 6: Type-aware F1 score across different test sets\nof models trained on all uppercase/lowercase English\ndatasets with XLM-RBASE or XLM-RLARGE.\nconverted all datasets into lowercase. Tables 8 and\nTable 9 show thetype-ignored F1 score across mod-\nels trained on different English datasets including\nlowercased corpora with XLM-RLARGE and XLM-\nRBASE, respectively.\n62\ntrain\\test ontonotes conll wnut wiki bionlp bc5cdr ﬁn avg\nontonotes 91.8 62.2 51.7 44.7 0.0 0.0 31.8 40.3\nconll 60.5 95.7 66.6 60.8 0.0 0.0 33.5 45.3\nwnut 41.3 81.3 63.0 56.3 0.0 0.0 20.5 37.5\nwiki 30.2 71.8 45.3 92.6 0.0 0.0 11.5 35.9\nbionlp 0.0 0.0 0.0 0.0 78.5 0.0 0.0 11.2\nbc5cdr 0.0 0.0 0.0 0.0 0.0 87.5 0.0 12.5\nﬁn 49.0 73.5 62.2 60.7 0.0 0.0 82.8 46.9\nall 89.7 92.4 55.8 89.3 78.2 80.0 74.8 80.0\nTable 7: Type-ignored F1 score in cross-domain setting over non-lower-cased English datasets with XLM-RBASE.\nWe compute average of accuracy in each test set, named as avg. The model trained on all datasets listed here, is\nshown as all.\ntrain\\test ontonotes conll wnut wiki bionlp bc5cdr ﬁn restaurant movie avg\nontonotes 89.3 59.9 50.1 44.7 0.0 0.0 15.1 4.5 88.6 39.1\nconll 57.7 94.8 67.0 57.9 0.0 0.0 20.5 23.9 0.0 35.7\nwnut 39.8 80.3 61.3 52.3 0.0 0.0 19.5 18.8 0.0 30.2\nwiki 28.5 69.7 51.2 92.4 0.0 0.0 12.0 3.0 0.0 28.5\nbionlp 0.0 0.0 0.0 0.0 79.0 0.0 0.0 0.0 0.0 8.7\nbc5cdr 0.0 0.0 0.0 0.0 0.0 88.9 0.0 0.0 0.0 9.8\nﬁn 46 72.0 61.5 54.8 0.0 0.0 83.0 24.5 0.0 37.9\nrestaurant 4.6 21.7 22.9 22.3 0.0 0.0 5.4 83.4 0.0 17.8\nmovie 10.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 73.1 9.3\nall 88.5 92.1 58.0 90.0 79.0 84.6 74.5 85.3 74.1 80.7\nTable 8: Type-ignored F1 score in cross-domain setting over lower-cased English datasets with XLM-RLARGE. We\ncompute average of accuracy in each test set, named asavg. The model trained on all datasets listed here, is shown\nas all.\ntrain\\test ontonotes conll wnut wiki bionlp bc5cdr ﬁn restaurant movie avg\nontonotes 88.3 56.7 49.0 41.4 0.0 0.0 11.7 4.2 88.3 37.7\nconll 55.1 93.7 60.5 56.8 0.0 0.0 20.4 21.9 0.0 34.3\nwnut 38.1 73.0 57.5 49.1 0.0 0.0 21.1 20.4 0.0 28.8\nwiki 26.3 66.5 41.4 90.9 0.0 0.0 9.7 7.6 0.0 26.9\nbionlp 0.0 0.0 0.0 0.0 78.7 0.0 0.0 0.0 0.0 8.7\nbc5cdr 0.0 0.0 0.0 0.0 0.0 88.0 0.0 0.0 0.0 9.8\nﬁn 41.3 64.4 45.8 57.8 0.0 0.0 81.5 22.0 0.0 34.8\nrestaurant 8.1 19.1 19.6 19.1 0.0 0.0 13.5 83.6 0.0 18.1\nmovie 14.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 73.1 9.7\nall 86.1 89.5 49.9 86.2 76.9 78.8 75.4 82.4 72.2 77.5\nTable 9: Type-ignored F1 score in cross-domain setting over lower-cased English datasets with XLM-RBASE. We\ncompute average of accuracy in each test set, named asavg. The model trained on all datasets listed here, is shown\nas all.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8902740478515625
    },
    {
      "name": "Python (programming language)",
      "score": 0.8241103887557983
    },
    {
      "name": "Named-entity recognition",
      "score": 0.7777470946311951
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5804412364959717
    },
    {
      "name": "Transformer",
      "score": 0.561082124710083
    },
    {
      "name": "Natural language processing",
      "score": 0.4691760838031769
    },
    {
      "name": "Generalization",
      "score": 0.41371655464172363
    },
    {
      "name": "Information retrieval",
      "score": 0.41346120834350586
    },
    {
      "name": "Programming language",
      "score": 0.3494041860103607
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79510175",
      "name": "Cardiff University",
      "country": "GB"
    }
  ],
  "cited_by": 54
}