{
    "title": "Metric3D v2: A Versatile Monocular Geometric Foundation Model for Zero-Shot Metric Depth and Surface Normal Estimation",
    "url": "https://openalex.org/W4395481595",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2318850155",
            "name": "Hu Mu",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2103353578",
            "name": "Yin Wei",
            "affiliations": [
                "University of Adelaide"
            ]
        },
        {
            "id": "https://openalex.org/A1927213385",
            "name": "Zhang Chi",
            "affiliations": [
                "Westlake University"
            ]
        },
        {
            "id": "https://openalex.org/A1987921559",
            "name": "Cai Zhi-peng",
            "affiliations": [
                "Intel (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4224826508",
            "name": "Long, Xiaoxiao",
            "affiliations": [
                "University of Hong Kong",
                "HKU-Pasteur Research Pole"
            ]
        },
        {
            "id": "https://openalex.org/A2347814779",
            "name": "Wang Kaixuan",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A1963498686",
            "name": "Chen Hao",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A1980596719",
            "name": "Yu Gang",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2360656043",
            "name": "Shen Chun-hua",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2362935933",
            "name": "Shen, Shaojie",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2888702972",
        "https://openalex.org/W4390872491",
        "https://openalex.org/W2963926543",
        "https://openalex.org/W4312969460",
        "https://openalex.org/W4313154783",
        "https://openalex.org/W6839182500",
        "https://openalex.org/W4386523681",
        "https://openalex.org/W4382464460",
        "https://openalex.org/W4390874817",
        "https://openalex.org/W3080961686",
        "https://openalex.org/W2805521962",
        "https://openalex.org/W2535547924",
        "https://openalex.org/W2951730755",
        "https://openalex.org/W6857525286",
        "https://openalex.org/W4385259380",
        "https://openalex.org/W6810227511",
        "https://openalex.org/W3182318349",
        "https://openalex.org/W3173727695",
        "https://openalex.org/W3188511781",
        "https://openalex.org/W2798373498",
        "https://openalex.org/W3035563424",
        "https://openalex.org/W3035508487",
        "https://openalex.org/W6703405610",
        "https://openalex.org/W3174458495",
        "https://openalex.org/W4302275500",
        "https://openalex.org/W2955639361",
        "https://openalex.org/W4214520160",
        "https://openalex.org/W6849845829",
        "https://openalex.org/W2990946490",
        "https://openalex.org/W4402753888",
        "https://openalex.org/W3096087403",
        "https://openalex.org/W3199238138",
        "https://openalex.org/W4394597875",
        "https://openalex.org/W3203887644",
        "https://openalex.org/W6850630192",
        "https://openalex.org/W6800540912",
        "https://openalex.org/W2928601293",
        "https://openalex.org/W6810236230",
        "https://openalex.org/W2981932175",
        "https://openalex.org/W2798927139",
        "https://openalex.org/W6846334842",
        "https://openalex.org/W3109128945",
        "https://openalex.org/W6856157636",
        "https://openalex.org/W4226265017",
        "https://openalex.org/W3109908659",
        "https://openalex.org/W4224313034",
        "https://openalex.org/W6758510812",
        "https://openalex.org/W3106611486",
        "https://openalex.org/W2027560260",
        "https://openalex.org/W2962778872",
        "https://openalex.org/W2890382763",
        "https://openalex.org/W2981978060",
        "https://openalex.org/W3035291735",
        "https://openalex.org/W2132947399",
        "https://openalex.org/W4390872953",
        "https://openalex.org/W125693051",
        "https://openalex.org/W2115579991",
        "https://openalex.org/W6685261749",
        "https://openalex.org/W4390874218",
        "https://openalex.org/W4402727115",
        "https://openalex.org/W2968585821",
        "https://openalex.org/W4312539440",
        "https://openalex.org/W3082498369",
        "https://openalex.org/W1899309388",
        "https://openalex.org/W1905829557",
        "https://openalex.org/W2963408523",
        "https://openalex.org/W337610345",
        "https://openalex.org/W2124907686",
        "https://openalex.org/W4393241127",
        "https://openalex.org/W4214651109",
        "https://openalex.org/W2612236014",
        "https://openalex.org/W4402753900",
        "https://openalex.org/W2963782415",
        "https://openalex.org/W4388469759",
        "https://openalex.org/W2885093229",
        "https://openalex.org/W3206335707",
        "https://openalex.org/W4312842294",
        "https://openalex.org/W4386071550",
        "https://openalex.org/W3035542908",
        "https://openalex.org/W3204126741",
        "https://openalex.org/W6809684969",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W3035086574",
        "https://openalex.org/W6851800889",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W2962833508",
        "https://openalex.org/W3095309006",
        "https://openalex.org/W6763009194",
        "https://openalex.org/W2962807621",
        "https://openalex.org/W2963591054",
        "https://openalex.org/W4402727359",
        "https://openalex.org/W2886322387",
        "https://openalex.org/W218762409",
        "https://openalex.org/W2962809185",
        "https://openalex.org/W6729541841",
        "https://openalex.org/W2959581809",
        "https://openalex.org/W3034346071",
        "https://openalex.org/W6766261854",
        "https://openalex.org/W4312388311",
        "https://openalex.org/W4312443924",
        "https://openalex.org/W6856857450",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W3172942063",
        "https://openalex.org/W2150066425",
        "https://openalex.org/W2594519801",
        "https://openalex.org/W3035574168",
        "https://openalex.org/W2963264757",
        "https://openalex.org/W6766947841",
        "https://openalex.org/W2519683295",
        "https://openalex.org/W2738551266",
        "https://openalex.org/W3180341139",
        "https://openalex.org/W3034604951",
        "https://openalex.org/W2948384918",
        "https://openalex.org/W6803138045",
        "https://openalex.org/W6801880476",
        "https://openalex.org/W2340897893",
        "https://openalex.org/W3139658937",
        "https://openalex.org/W3209102426",
        "https://openalex.org/W2955770542",
        "https://openalex.org/W6773019456",
        "https://openalex.org/W3035172746",
        "https://openalex.org/W2964185501",
        "https://openalex.org/W6764040762",
        "https://openalex.org/W6800673378",
        "https://openalex.org/W3097660860",
        "https://openalex.org/W2741885505",
        "https://openalex.org/W3174541782",
        "https://openalex.org/W4200495456",
        "https://openalex.org/W4312601813",
        "https://openalex.org/W2963583471",
        "https://openalex.org/W2292391751"
    ],
    "abstract": "We introduce Metric3D v2, a geometric foundation model designed for zero-shot metric depth and surface normal estimation from single images, critical for accurate 3D recovery. Depth and normal estimation, though complementary, present distinct challenges. State-of-the-art monocular depth methods achieve zero-shot generalization through affine-invariant depths, but fail to recover real-world metric scale. Conversely, current normal estimation techniques struggle with zero-shot performance due to insufficient labeled data. We propose targeted solutions for both metric depth and normal estimation. For metric depth, we present a canonical camera space transformation module that resolves metric ambiguity across various camera models and large-scale datasets, which can be easily integrated into existing monocular models. For surface normal estimation, we introduce a joint depth-normal optimization module that leverages diverse data from metric depth, allowing normal estimators to improve beyond traditional labels. Our model, trained on over 16 million images from thousands of camera models with varied annotations, excels in zero-shot generalization to new camera settings. As shown in Fig. 1, It ranks the 1st in multiple zero-shot and standard benchmarks for metric depth and surface normal prediction. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. Our model also relieves the scale drift issues of monocular-SLAM (Fig. 3), leading to high-quality metric scale dense mapping. Such applications highlight the versatility of Metric3D v2 models as geometric foundation models.",
    "full_text": "1\nMetric3D v2: A Versatile Monocular Geometric\nFoundation Model for Zero-shot Metric Depth\nand Surface Normal Estimation\nMu Hu1‚àó, Wei Yin2‚àó‚Ä†, Chi Zhang3, Zhipeng Cai4, Xiaoxiao Long5‚Ä°, Hao Chen6,\nKaixuan Wang1, Gang Yu7, Chunhua Shen6, Shaojie Shen1\nAbstract‚ÄîWe introduce Metric3D v2, a geometric foundation model for zero-shot metric depth and surface normal estimation from a\nsingle image, which is crucial for metric 3D recovery. While depth and normal are geometrically related and highly complimentary, they\npresent distinct challenges. State-of-the-art (SoTA) monocular depth methods achieve zero-shot generalization by learning\naffine-invariant depths, which cannot recover real-world metrics. Meanwhile, SoTA normal estimation methods have limited zero-shot\nperformance due to the lack of large-scale labeled data. To tackle these issues, we propose solutions for both metric depth estimation\nand surface normal estimation. For metric depth estimation, we show that the key to a zero-shot single-view model lies in resolving the\nmetric ambiguity from various camera models and large-scale data training. We propose a canonical camera space transformation\nmodule, which explicitly addresses the ambiguity problem and can be effortlessly plugged into existing monocular models. For surface\nnormal estimation, we propose a joint depth-normal optimization module to distill diverse data knowledge from metric depth, enabling\nnormal estimators to learn beyond normal labels. Equipped with these modules, our depth-normal models can be stably trained with\nover 16 million of images from thousands of camera models with different-type annotations, resulting in zero-shot generalization to\nin-the-wild images with unseen camera settings. Our method currently ranks the 1st on various zero-shot and non-zero-shot\nbenchmarks for metric depth, affine-invariant-depth as well as surface-normal prediction, shown in Fig. 1. Notably, we surpassed the\nultra-recent MarigoldDepth and DepthAnything on various depth benchmarks including NYUv2 and KITTI. Our method enables the\naccurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology.\nThe potential benefits extend to downstream tasks, which can be significantly improved by simply plugging in our model. For example,\nour model relieves the scale drift issues of monocular-SLAM (Fig. 3), leading to high-quality metric scale dense mapping. These\napplications highlight the versatility of Metric3D v2 models as geometric foundation models. Our project page is at\nhttps://JUGGHM.github.io/Metric3Dv2.\nIndex Terms‚ÄîMonocular metric depth estimation, surface normal estimation, 3D scene shape estimation\n‚ú¶\n1 I NTRODUCTION\nMonocular metric depth and surface normal estimation is\nthe task of predicting absolute distance and surface direction\nfrom a single image. As crucial 3D representations, depth\nand normals are geometrically related and highly comple-\nmentary. While metric depth excels in capturing data at\nscale, surface normals offer superior preservation of local\ngeometry and are devoid of metric ambiguity compared to\nmetric depth. These unique attributes render both depth and\nsurface normals indispensable in various computer vision\napplications, including 3D reconstruction [1], [2], [3], neural\nrendering (NeRF) [4], [5], [6], [7], autonomous driving [8],\n[9], [10], and robotics [11], [12], [13]. Currently, the commu-\nnity still lacks a robust, generalizable geometry foundation\nmodel [14], [15], [16] capable of producing high-quality\nmetric depth and surface normal from a single image.\nMetric depth estimation and surface normal estima-\ntion confront distinct challenges. Existing depth estimation\nmethods are categorized into learning metric depth [17],\n‚Ä¢ ‚àóEqual contribution.\n‚Ä¢ ‚Ä†WY is the project lead (yvanwy@outlook.com).\n‚Ä¢ ‚Ä°XL is the corresponding author (xxlong@connect.hku.hk).\n‚Ä¢ Contact MH for technical concerns (mhuam@connect.ust.hk).\n‚Ä¢ 1 HKUST 2 Adelaide University\n‚Ä¢ 3 Westlake University 4 Intel 5 HKU 6 Zhejiang University 7 Tencent\nSubmitted for review on Feb. 29th, 2024.\n70\n75\n80\n85\n90\n95\n100\n96.9 IEBins\n96.9 Polymax\n93.9 HDN\n85.1 ZeroDepth\n87.8 Polymax\n89.8 Bae etal.\n71.6 IronDepth.\n91.0 ZeroDepth\n75.4 ZeroDepth\n78.0 HDN88.5 LeRes\n94.6 DPT-L\n40.0 ZoeDepth\n26.9 ZoeDepth\n96.9\n98.7\n97.7\n78.5\n87.1 88.1\n98.9\n89.1\n‚ô† DIODE-Full\n(Affine-invariant-depth)\n‚ô† Eth3d\n(Affine-invariant-depth)\n‚ô† ibims1\n(Affine-invariant-depth)\n‚ô† Nuscenes\n(Metric-depth)\n‚ô† KITTI\n(Metric-depth) ‚ô† NYU\n(Metric-depth)\nKITTI\n(Metric-depth) NYU\n(Metric-depth)\nScanNet\n(Metric-depth)\n‚ô† DIODE-Indoor\n(Metric-depth)\n‚ô† DIODE-Outdoor\n(Metric-depth)\nDDAD\n(Metric-depth)\nNYU (Normal)\nScanNet (Normal)\n‚ô†NYU (Normal)\n‚ô†ibims1 (Normal)\n96.9 Polymax\n87.8 Polymax\n98.0\n87.8 Polymax\n92.3\n88.7\n86.5\n93.2\n84.7\n99.3\n98.9\n‚ñ† Ours Metric 3D\n‚ñ† SoTA\n‚ô†Zero-shot testing \nFig. 1 ‚Äì Comparisons with SoTA methods on 16 depth\nand normal benchmarks. Radar-map of our Metric3D V2\nv.s. SoTA methods from different works, on (1) Metric\ndepth benchmarks, see ‚Äò(Metric-depth)‚Äô. (2) Affine-invariant\ndepth benchmarks, see ‚Äò(Affine-invariant-depth)‚Äô. (3) Sur-\nface normal benchmarks, see ‚Äò(Normal)‚Äô. Zero-shot testing\nis denoted by ‚Äò ‚ô†‚Äô. Here Œ¥1 percentage accuracy is used\nfor depth benchmarks and 30‚ó¶ percentage accuracy is for\nnormal. Both higher values are for better performance. We\nestablish new SoTA on a wide range of depth and normal\nbenchmarks.\narXiv:2404.15506v4  [cs.CV]  3 Jan 2025\n2\nRGBOurs-DMarigold-DMarigold-N Ours-N\nFig. 2 ‚Äì Surface normal (N) and monocular depth (D) comparisons on diverse web images. Our method, directly estimating\nmetric depths and surface normals, shows powerful generalization in a variety of scenarios, including indoor, outdoor, poor-\nvisibility, motion blurred, and fisheye images. Visualized results come from our ViT-large-backbone estimator. Marigold is\na strong and robust diffusion-based monocular depth estimation method, but its recovered surface normals from the depth\nshow various artifacts.\n[18], [19], [20], relative depth [21], [22], [23], [24], and\naffine-invariant depth [25], [26], [27], [28], [29]. Although\nthe metric depth methods [17], [18], [19], [20], [30] have\nachieved impressive accuracy on various benchmarks, they\nmust train and test on the dataset with the same camera\nintrinsics. Therefore, the training datasets of metric depth\nmethods are often small, as it is hard to collect a large\ndataset covering diverse scenes using one identical camera.\nThe consequence is that all these models generalize poorly\nin zero-shot testing, not to mention the camera parameters\nof test images can vary too. A compromise is to learn the\nrelative depth [21], [23], which only represents one point\nbeing further or closer to another one. The application\nof relative depth is very limited. Learning affine-invariant\ndepth finds a trade-off between the above two categories of\nmethods, i.e. the depth is up to an unknown scale and shift.\nWith large-scale data, they decouple the metric information\nduring training and achieve impressive robustness and gen-\neralization ability, such as MiDaS [27], DPT [28], LeReS [25],\n[26], HDN [29]. The problem is the unknown shift will cause\n3D reconstruction distortions [26] and non-metric depth\ncannot satisfy various downstream applications.\nIn the meantime, these models cannot generate surface\nnormals. Although lifting depths to 3D point clouds can\ndo so, it places high demands on the accuracy and fine\ndetails of predicted depths. Otherwise, various artifacts will\nremain in such transformed normals. For example, Fig. 2\nshows noisy normals from Marigold [31] depths, which\nexcels in producing high-resolution fine depths. Instead of\ndirect transformation, state-of-the-art (SoTA) surface normal\nestimation methods [32], [33], [34] tend to train estimators\non high-quality normal annotations. These annotations, un-\nlike sensor-captured ground-truth (GT), are derived from\nmeticulously and densely reconstructed scenes, which have\nextremely rigorous requirements for both the capturing\nequipment and the scene. Consequently, data sources pri-\nmarily consist of either synthetic creation or 3D indoor\nreconstruction [35]. Real and diverse outdoor scenes are\nexceedingly rare. (refer to our data statistics in Tab. 5). Lim-\nited by this label deficiency, SoTA surface normal methods\n[32], [33], [34] typically struggle with strong zero-shot gen-\neralization. This work endeavors to tackle these challenges\nby developing a multi-task foundation model for zero-shot,\nsingle view, metric depth, and surface normal estimation.\nWe propose targeted solutions for the challenges of\nzero-shot metric depth and surface normal estimation. For\nmetric-scale recovery, we first analyze the metric ambiguity\nissues in monocular depth estimation and study different\ncamera parameters in depth, including the pixel size, focal\nlength, and sensor size. We observe that the focal length\nis the critical factor for accurate metric recovery. By de-\nsign, affine-invariant depth methods do not take the focal\nlength information into account during training. As shown\nin Sec. 3.1, only from the image appearance, various focal\nlengths may cause metric ambiguity, thus they decouple the\ndepth scale in training. To solve the problem of varying focal\nlengths, CamConv [38] encodes the camera model in the\nnetwork, which enforces the network to implicitly under-\nstand camera models from the image appearance and then\nbridges the imaging size to the real-world size. However,\ntraining data contains limited images and types of cameras,\nwhich challenges data diversity and network capacity. We\n3\n‚Ä¢ : 24ùëöùëöùëöùëö\nMeta Data:\n‚Ä¢ Focal: 24ùëöùëöùëöùëö\n‚Ä¢ Pixelsize: 2.44ùúáùúáùëöùëö\n‚Ä¢ Size: 4032√ó3024\nMeta Data:\n‚Ä¢ Focal: 35 ùëöùëöùëöùëö\n‚Ä¢ Pixelsize: 1ùúáùúáùëöùëö\n‚Ä¢ Size: 4000 √ó3000\niPhone 14 pro\nSamsung S23\nSingle view metric reconstruction of photos captured by from different cameras\nOursRGB ZoeDepth\nDroid-SLAM\nGround-\ntruth \ntrajectory\nPredicted \ntrajectory\n3D \nrecon.\nMetrologyDense SLAM Mapping\nBlue: GT size\nRed: measured size\nDroid-SLAM +Ours\n3D \nrecon.\nPredicted \ntrajectory\n10.5ùíÑùíÑùíéùíé\n53.1ùíÑùíÑùíéùíé\n36.7ùíÑùíÑùíéùíé\nSingle \nview \nrecon.\nYellow:\nGT size\nRed: \nmeasured \nsize\nFig. 3 ‚Äì Top (metrology for a complex scene): we use two phones (iPhone14 pro and Samsung Galaxy S23) to capture the scene\nand measure the size of several objects, including a drone which has never occurred in the whole training set. With the photos‚Äô\nmetadata, we perform 3D metric reconstruction and then measure object sizes (marked in red), which are close to the ground\ntruth (marked in yellow). Compared with ZoeDepth [36], our measured sizes are closer to ground truth. Bottom (dense SLAM\nmapping): existing SoTA mono-SLAM methods usually face scale drift problems (see the red arrows) in large-scale scenes and\nare unable to achieve the metric scale, while, naively inputting our metric depth, Droid-SLAM [37] can recover much more\naccurate trajectory and perform the metric dense mapping (see the red measurements). Note that all testing data are unseen to\nour model.\npropose a canonical camera transformation method in train-\ning, inspired by the canonical pose space from human body\nreconstruction methods [39]. We transform all training data\nto a canonical camera space where the processed images\nare coarsely regarded as captured by the same camera.\nTo achieve such transformation, we propose two different\nmethods. The first one tries to adjust the image appear-\nance to simulate the canonical camera, while the other one\ntransforms the ground-truth labels for supervision. Camera\nmodels are not encoded in the network, making our method\neasily applicable to existing architectures. During inference,\na de-canonical transformation is employed to recover metric\ninformation. To further boost the depth accuracy, we pro-\npose a random proposal normalization loss. It is inspired\nby the scale-shift invariant loss [25], [27], [29] decoupling\nthe depth scale to emphasize the single image‚Äôs distribu-\ntion. However, they perform on the whole image, which\ninevitably squeezes the fine-grained depth difference. We\npropose to randomly crop several patches from images and\nenforce the scale-shift invariant loss [25], [27] on them. Our\nloss emphasizes the local geometry and distribution of the\nsingle image.\nFor surface normal, the biggest challenge is the lack of\ndiverse (outdoor) annotations. Compared to reconstruction-\nbased annotation methods [35], [40], directly producing\nnormal labels from network-predicted depth is more ef-\nficient and scalable. The quality of such pseudo-normal\nlabels, however, is bounded by the accuracy of the depth\nnetwork. Fortunately, we observe that robust metric depth\nmodels are scalable geometric learners, containing abundant\ninformation for normal estimation. Weak supervision from\nthe pseudo normal annotations transformed by learned\nmetric depth can effectively prevent the normal estimator\nfrom collapsing caused by GT absence. Furthermore, this\nsupervision can guide the normal estimator to generalize\non large-scale unlabeled data. Based on such observation,\nwe propose a joint depth-normal optimization module to\ndistill knowledge from diverse depth datasets. During op-\ntimization, our normal estimator learns from three sources:\n(1) Groundtruth normal labels, though they are much fewer\ncompared to depth annotations (2) An explicit learning\nobjective to constrain depth-normal consistency. (3) Implicit\nand thorough knowledge transfer from depth to normal\nthrough feature fusion, which is more tolerant to unsatis-\nfactory initial prediction than the explicit counterparts [41],\n[42]. To achieve this, we implement the optimization module\nusing deep recurrent blocks. While previous researchers\nhave employed similar recurrent modules to optimize depth\n[42], [43], [44], disparity [45], ego-motion [37], or optical\nflows [46], it is the first time that normal is iteratively\noptimized together with depth in a learning-based scheme.\nBenefiting from the joint optimization module, our models\ncan efficiently learn normal knowledge from large-scale\ndepth datasets even without labels.\nWith the proposed method, we can stably scale up model\ntraining to 16 million images from 16 datasets of diverse\nscene types (indoor and outdoor, real or synthetic data),\ncamera models (tens of thousands of different cameras),\nand annotation categories (with or without normal), lead-\ning to zero-shot transferability and significantly improved\n4\nInput Ours: End-to-end zero-shot metric depth\nZoeDepth: Zero-shot relative depth + Metric heads\nNetwork\n Unified metric distribution Metric depth\nStage2: Scene-specific metric finetuningStage1: Large-\nscale training\nTrain once for all applications \nOurs: Learn normal from depth label\nOmnidata: Annotate normal\nDepth\nSurface normal\nDense \nrecon.\nLoss supervision\nIndoor distribution\nOutdoor distribution Outdoor metric depth \nIndoor metric depth Depth label\nNormal label\nSurface normal\nSingle RGB \nimage\nAffine-invariant depth\nDepth label\n‚Ä¶\nNormal label\nOptional\nExisting assets Newly labeled assets\nFig. 4 ‚Äì Overall methodology.Our method takes a single image to predict the metric depth and surface normal simultaneously.\nWe apply large-scale data training directly for metric depth estimation rather than affine invariant depth, enabling end-to-end\nzero-shot metric depth estimation for various applications using a single model. For normals, we enable learning from depth\nlabels only, alleviating the demand for dense reconstruction to generate large-scale normal labels.\naccuracy. Fig. 4 illustrates how the large-scale data with\ndepth annotations directly facilitate metric depth and sur-\nface normal learning. The metric depth and normal given\nby our model directly broaden the applications in down-\nstream tasks. We achieve state-of-the-art performance on\nover 16 depth and normal benchmarks, see Fig. 1. Our\nmodel can accurately reconstruct metric 3D from randomly\ncollected Internet images, enabling plausible single-image\nmetrology. For examples (Fig. 3), we recovery real-world\nmetric to improve monocular SLAM [37], [47] and facilitate\nlarge-scale 3D reconstruction [48]. Our main contributions\ncan be summarized as:\n‚Ä¢ We propose a canonical camera transformation method\nto address metric depth ambiguity across different\ncamera settings. This approach facilitates training zero-\nshot monocular metric depth models using large-scale\ndatasets.\n‚Ä¢ We design a random proposal normalization loss to\neffectively improve metric depth.\n‚Ä¢ We propose a joint depth-normal optimization module\nto learn normal on large-scale datasets without normal\nannotation, distilling knowledge from the metric depth\nestimator.\n‚Ä¢ Our models rank 1st on a wide variety of depth\nand surface normal benchmarks. It can perform high-\nquality 3D metric structure recovery in the wild and\nbenefit several downstream tasks, such as mono-\nSLAM [37], [49], 3D scene reconstruction [48], and\nmetrology [50].\n2 R ELATED WORK\n3D reconstruction from a single image. The reconstruc-\ntion of diverse objects from a singular image has been\nextensively investigated in prior research [51], [52], [53].\nThese methodologies exhibit proficiency in generating high-\nfidelity 3D models encompassing various entities such as\ncars, planes, tables, and human bodies [54], [55]. The pri-\nmary challenge lies in optimizing the recovery of object\ndetails, devising efficient representations within constrained\nmemory resources, and achieving generalization across a\nbroader spectrum of objects. However, these approaches\ntypically hinge upon learning object-specific or instance-\nspecific priors, often derived from 3D supervision, thereby\nrendering them unsuitable for comprehensive scene recon-\nstruction. In addition to the aforementioned efforts on object\nreconstruction, several studies focus on scene reconstruction\nfrom single images. Saxena et al. [56] adopt an approach\nthat segments the entire scene into multiple small planes,\nwith the 3D structure represented based on the orientation\nand positioning of these planes. More recently, LeReS [25]\nproposed employing a robust monocular depth estimation\nmodel for scene reconstruction. Nonetheless, their method is\nlimited to recovering shapes up to a certain scale. Zhang et\nal. [57] recently introduced a zero-shot geometry-preserving\ndepth estimation model capable of providing depth predic-\ntions up to an unknown scale. In contrast to the aforemen-\ntioned methodologies, our approach excels in recovering the\nmetric 3D structure of scenes.\nSupervised monocular depth estimation. Following the\nestablishment of several benchmarks [58], [59], neural\nnetwork-based methods [17], [19], [30] have dominated this\ntask. These approaches often regress continuous depth by\naggregating the information from an image [60]. However,\nsince depth distribution varies significantly with different\nRGB values, some methods tend to discretize depth and\nreformulate the problem as a classification task [18] for\n5\nbetter performance. The generalization of deep models for\n3D metric recovery faces two challenges: adapting to diverse\nscenes and predicting accurate metric information under\nvarious camera settings. Recent methods [18], [21], [22] have\neffectively addressed the first challenge by creating large-\nscale relative depth datasets like DIW [24] and OASIS [23]\nto learn relative relations, which lose geometric structure\ninformation. To enhance geometry, methods like MiDaS [27],\nLeReS [25], and HDN [29] employ affine-invariant depth\nlearning. These approaches, utilizing large-scale data, have\ncontinuously improved performance and scene generaliza-\ntion. However, they inherently struggle to recover metric\ninformation. Thus, achieving both strong generalization and\naccurate metric data across diverse scenes remains a key\nchallenge to be addressed. Con-currently, ZoeDepth [36],\nZeroDepth [61], and UniDepth [62] apply varying strategies\nto tackle this challenge.\nSurface normal estimation. Compared to metric depth,\nsurface normal suffers no metric ambiguity and preserves\nlocal geometry better. These properties attract researchers\nto apply normal in various vision tasks like localization\n[11], mapping [63], and 3D scene reconstruction [6], [64].\nCurrently, learning-based methods [32], [33], [34], [42], [64],\n[65], [66], [67], [68], [69], [70], [71], [72] have dominated\nmonocular surface normal estimation. Since normal labels\nrequired for training cannot be directly captured by sensors,\nprevious works use [41], [58], [65], [67] kernel functions to\nannotate normal from dense indoor depth maps [58]. These\nannotations become incomplete on reflective surfaces and\ninaccurate at object boundaries. To learn from such imper-\nfect annotations, GeoNet [41] proposes to enforce depth-\nnormal consistency with mutual transformation modules,\nASN [71], [72] propose a novel adaptive surface normal con-\nstraint to facilitate joint depth-normal learning, and Bae et\nal. [33] propose an uncertainty-based learning objective.\nNonetheless, it is challenging for such methods to further\nincrease their generalization, due to the limited dataset size\nand the diversity of scenes, especially for outdoor scenarios.\nOmni-data [35] advances to fill this gap by building 1300M\nframes of normal annotation. Normal-in-the-wild [73] pro-\nposes a pipeline for efficient normal labeling. A con-current\nwork DSINE [74] also employs diverse datasets to train\ngeneralizable surface normal estimators. However, further\nscaling up normal labels remains difficult. This underscores\nresearch significance in finding an efficient way to distill\nprior from other types of annotation.\nDeep iterative refinement for geometry. Iterative refine-\nment enables multi-step coarse-to-fine prediction and ben-\nefits a wide range of geometry estimation tasks, such as\noptical flow estimation [46], [75], [76], depth completion\n[43], [77], [78], and stereo matching [ ?], [45], [79]. Classical\niterative refinements [75], [77] optimize directly on high-\nresolution outputs using high-computing-cost operators,\nlimiting researchers from applying more iterations for better\npredictions. To address this limitation, RAFT [46] proposes\nto optimize an intermediate low-resolution prediction us-\ning ConvGRU modules. For monocular depth estimation,\nIEBins [44] employs similar methods to optimize depth-bin\ndistribution. Differently, IronDepth [42] propagates depth\non pre-computed local surfaces. Regarding surface normal\nrefinement, Lenssen et al. [80] propose a deep iterative\nmethod to optimize normal from point clouds. Zhao et al.\n[81] design a solver to refine depth and normal jointly,\nbut it requires multi-view prior and per-sample post op-\ntimization. Without multi-view prior, such a non-learnable\noptimization method could fail due to unsatisfactory initial\npredictions. All the monocular methods [42], [44], [80],\nhowever, iterate over either depth or normal independently.\nIn contrast, our joint optimization module tightly couples\ndepth and normal with each other.\nLarge-scale data training. Recently, various natural lan-\nguage problems and computer vision problems [82], [83],\n[84] have achieved impressive progress with large-scale\ndata training. CLIP [83] is a promising classification model\ntrained on billions of paired image-language data pairs,\nachieving achieves state-of-the-art performance zero-shot\nclassification benchmarks. Dinov2 [85] collects 142M images\nto conduct vision-only self-supervised learning for vision\ntransformers [86]. Generative models like LDM [87] have\nalso undergone billion-level data pre-training. For depth\nprediction, large-scale data training has been widely ap-\nplied. Ranft et al. [27] mix over 2 million data in training,\nLeReS [26] collects over 300 thousands data, Eftekhar et\nal. [35] also merge millions of data to build a strong depth\nprediction model. To train a zero-shot surface normal es-\ntimator, Omni-data [35] performs dense reconstruction to\ngenerate 14M frames with surface normal annotations.\n3 M ETHOD\nPreliminaries. We consider the pin-hole camera model with\nintrinsic parameters formulated as: [[ ÀÜf/Œ¥,0,u0], [ 0, ÀÜf/Œ¥,v0],\n[0,0,1]], where ÀÜf is the focal length (in micrometers), Œ¥ is\nthe pixel size (in micrometers), and (u0,v0) is the principle\ncenter. f = ÀÜf/Œ¥ is the pixel-represented focal length.\n3.1 Ambiguity Issues in Metric Depth Estimation\nFigure 5 illustrates an instance of photographs captured us-\ning diverse cameras and at varying distances. Solely based\non visual inspection, one might erroneously infer that the\nlast two images originate from a comparable location and\nare captured by the same camera. However, due to differing\nfocal lengths, these images are indeed captured from dis-\ntinct locations. Consequently, accurate knowledge of camera\nintrinsic parameters becomes imperative for metric estima-\ntion from a single image; otherwise, the problem becomes\nill-posed. Recent methodologies, such as MiDaS [27] and\nLeReS [25], mitigate this metric ambiguity by decoupling\nmetric estimation from direct supervision and instead prior-\nitize learning affine-invariant depth.\nFigure 6 (A) depicts the pin-hole perspective projection,\nwhere object A located at distance da is projected to A‚Ä≤.\nAdhering to the principle of similarity, it is obvious that:\nda = ÀÜS\n[ÀÜf\nÀÜS‚Ä≤\n]\n= ÀÜS¬∑Œ± (1)\nwhere ÀÜS and ÀÜS‚Ä≤ are the real and imaging size respectively.\nThe symbol ÀÜ¬∑signifies that variables are expressed in phys-\nical metrics (e.g., millimeters). To ascertain da from a single\nimage, one must have access to the focal length, imaging\nsize of the object, and real-world object size. Estimating\nthe focal length from a single image is challenging and\ninherently ill-posed. Despite numerous methods having\nbeen explored [25], [88], achieving satisfactory accuracy\n6\nfocal=26 ùëöùëö, depth=2 ùëö focal=52 ùëöùëö, depth=2 ùëö focal=26 ùëöùëö, depth=1 ùëö\nFig. 5 ‚Äì Photos of a chair captured at different distances\nwith different cameras . The first two photos are captured\nat the same distance but with different cameras, while the\nlast one is taken at a closer distance with the same camera\nas the first one.\nremains elusive. Hereby, we simplify the scenario by as-\nsuming known focal lengths for the training/test images.\nIn contrast, understanding the imaging size is much easier\nfor a neural network. To obtain the real-world object size,\na neural network needs to understand the semantic scene\nlayout and the object, at which a neural network excels. We\ndefine Œ±= ÀÜf/ÀÜS‚Ä≤, showing that da is proportional to Œ±.\nWe observe the following regarding sensor size, pixel\nsize, and focal length.\nO1: Sensor size and pixel size do not affect metric depth\nestimation. Based on perspective projection (Fig. 6 (A)),\nsensor size only influences the field of view (FOV) and is\nnot relevant to Œ±, hence it does not affect metric depth\nestimation. For pixel size, consider two cameras with dif-\nferent pixel sizes ( Œ¥1 = 2 Œ¥2) but the same focal length ÀÜf,\ncapturing the same object at distance da. Fig. 6 (B) displays\ntheir captured images. According to the preliminaries, the\npixel-represented focal length is f1 = 1\n2 f2. Since the second\ncamera has smaller pixel sizes, the resolution of the pixel-\nrepresented image is given by S‚Ä≤\n1 = 1\n2 S‚Ä≤\n2, despite both\nhaving the same projected imaging size ÀÜS‚Ä≤. According to\nEq. (1), we have\nÀÜf\nŒ¥1¬∑S‚Ä≤\n1\n=\nÀÜf\nŒ¥2¬∑S‚Ä≤\n2\n, which implies Œ±1 = Œ±2 and\nd1 = d2. This means that variations in camera sensors do\nnot impact the estimation of metric depth.\nO2: The focal length is vital for metric depth estimation .\nFigure 5 shows the challenge of metric ambiguity caused by\nan unspecified focal length, which is further discussed in\nFigure 7. In the scenario where two cameras ( ÀÜf1 = 2 ÀÜf2) are\npositioned at distances d1 = 2d2, the imaging sizes remain\nconsistent for both cameras. As a result, the neural network\nstruggles to distinguish between different supervision labels\nbased solely on visual cues. To address this issue, we pro-\npose a canonical camera transformation method to reduce\nconflicts between supervision requirements and image rep-\nresentations.\nÃÇùëìùëì\nùê¥ùê¥\nùê¥ùê¥‚Ä≤\nùëëùëëùëéùëé\nùëÇùëÇ\n(A) (B)\nResolution: 8 √ó 12 Resolution: 16 √ó 24\nFig. 6 ‚Äì Pinhole camera model. (A)Object Apositioned at a\ndistance da undergoes projection onto the image plane. (B)\nEmploying two cameras for capturing an image of the car.\nThe left one has a larger pixel size. Although the projected\nimaging sizes are the same, the pixel-represented images\n(resolution) are different.\nùëë2\n‡∑°ùëì2\nùê¥ ùê¥2\n‚Ä≤object\nùëë1 = 2ùëë2\n‡∑°ùëì1 = 2‡∑°ùëì2\nùê¥ ùê¥1\n‚Ä≤object\nFig. 7 ‚Äì Illustration of two cameras with different focal\nlength at different distance. As f1 = 2f2 and d1 = 2d2, Ais\nprojected to two image planes with the same imaging size\n(i.e. A\n‚Ä≤\n1 = A\n‚Ä≤\n2).\nUnlike depth, surface normal does not have any metric\nambiguity problem. In Fig. 8, we illustrate this concept with\ntwo depth maps at varying scales, denoted as D1 and D2,\nfeaturing distinct metrics d1 and d2, respectively, where\nd1 <d2. After upprojecting the depth to the 3D point cloud,\nthe dolls are in different depths d1 and d2. However, the\nsurface normals n1 and n2 corresponding to a certain pixel\nA‚Ä≤‚ààI remain the same.\nùëÇùëÇ\nùê¥ùê¥2\nùê¥ùê¥1\nùêßùêß1\nùêßùêß2\nùëëùëë1\n ùëëùëë2\nùê¥ùê¥‚Ä≤\nFig. 8 ‚Äì The metric-agnostic property of normal. With\ndifferently predicted metrics d1 and d2, the pixel A\n‚Ä≤\non\nthe image will be back-projected to 3D points A1 and A2,\nrespectively. The surface normal n1 at A1 and n2 at A2\nremain the same.\n3.2 Canonical Camera Transformation\nThe fundamental concept entails establishing a canonical\ncamera space ( (fc\nx,fc\ny), with fc\nx = fc\ny = fc in experi-\nmental settings) and transposing all training data into this\ndesignated space. Consequently, all data can be broadly\nconstrued as being captured by the canonical camera. We\npropose two transformation methods, i.e. either transform-\ning the input image (I ‚ààRH√óW√ó3) or the ground-truth (GT)\nlabel (D ‚ààRH√óW). The initial intrinsics are {f,u0,v0}.\nMethod1: transforming depth labels (CSTM label).\nFig. 5‚Äôs ambiguity is for depths. Consequently, our ini-\ntial approach directly addresses this issue by transforming\nthe ground-truth depth labels. Specifically, we rescale the\nground-truth depth ( D‚àó) using the ratio œâd = fc\nf during\ntraining, denoted as Dc = œâdD‚àó. The original camera\nmodel undergoes transformation to fc,u0,v0. In inference,\nthe predicted depth ( Dc) exists in the canonical space and\nnecessitates a de-canonical transformation to restore metric\ninformation, expressed as D = 1\nœâd\nDc. It is noteworthy that\nthe input I remains unaltered, represented as Ic = I.\nMethod2: transforming input images (CSTM image).\nFrom an alternate perspective, the ambiguity arises due to\nthe resemblance in image appearance. Consequently, this\nmethodology aims to alter the input image to emulate the\nimaging effects of the canonical camera. Specifically, the\nimage I undergoes resizing using the ratioœâr = fc\nf , denoted\nas Ic = T(I,œâr), where T(¬∑) signifies image resizing. As a\nresult of resizing the optical center, the canonical camera\n7\nCanonical\ncamera \ntransform\nmodule\nInput I En-\ndecoderInput Ic\nDe-canonical \ntransform\nOptional: normal GT ùêçùêç‚àó\nDepth GT ùêÉùêÉc‚àóDepth GT ùêÉùêÉ‚àó\n(ùëìùëì, ùë¢ùë¢0, ùë£ùë£0)\nInitial \nnormal ùêçùêç0\nJoint \ndepth-normal \noptimization\nt = 0, 1, ‚Ä¶, T\nPred. depth ùêÉùêÉc\n(Final) pred.\nnormal ùêçùêç\nFinal pred.\ndepth ùêÉùêÉ\nReal world Canonical camera space Real world\nSupervise ùêøùêøùëëùëë\nSupervise ùêøùêøùëëùëë‚àíùëõùëõ\nSupervise ùêøùêøùëõùëõ\n(ùëìùëìùëêùëê, ùë¢ùë¢ùëêùëê, ùë£ùë£ùëêùëê)\nInitial \ndepth ùêÉùêÉc0\nFig. 9 ‚Äì Pipeline. Given an input image I, we first transform it to the canonical space using CSTM. The transformed image\nIc is fed into a standard depth-normal estimation model to produce the predicted metric depth Dc in the canonical space\nand metric-agnostic surface normal N. During training, Dc is supervised by a GT depth D‚àó\nc which is also transformed into\nthe canonical space. In inference, after producing the metric depth Dc in the canonical space, we perform a de-canonical\ntransformation to convert it back to the space of the original input I. The canonical space transformation and de-canonical\ntransformation are executed using camera intrinsics. The predicted normal N is supervised by depth-normal consistency via\nthe recovered metric depth Das well as GT normal N‚àó, if available.\nmodel becomes fc,œâru0,œârv0. The ground-truth labels are\nresized without scaling, represented as D‚àó\nc = T(D‚àó,œâr). In\ninference, the de-canonical transformation involves resizing\nthe prediction to its original dimensions without scaling,\nexpressed as D = T(Dc, 1\nœâr\n).\nWhile similar transformations have been employed in\nMPSD [89] to normalized depth prediction, our approaches\napply these modules to predict metric depth directly.\nFigure 9 shows the pipeline. After adopting either trans-\nformation, a patch is randomly cropped for training pur-\nposes. This cropping operation solely adjusts the field of\nview (FOV) and the optical center, thus averting any po-\ntential metric ambiguity issues. In the labels transformation\napproach, œâr = 1 and œâd = fc\nf , while in the images\ntransformation method, œâd = 1 and œâr = fc\nf . Throughout\nthe training process, the transformed ground-truth depth\nlabels D‚àó\nc are employed as supervision. Importantly, since\nsurface normals are not susceptible to metric ambiguity, no\ntransformation is applied to normal labels N‚àó.\n3.3 Jointly optimizing depth and normal\nWe propose to optimize metric depth and surface normal\njointly in an end-to-end manner. This optimization is pri-\nmarily aimed at leveraging a large amount of annotation\nknowledge available in depth datasets to improve normal\nestimation, particularly in outdoor scenarios where depth\ndatasets contain significantly more annotations than normal\ndatasets. In our experiments, we collect from the community\n9488K images with depth annotations across 14 outdoor\ndatasets while less than 20K outdoor normal-labeled im-\nages, presented in Tab. 5.\nTo facilitate knowledge flow across the depth and nor-\nmal, we implement the learning-based optimization with\nrecurrent refinement blocks, as depicted in Fig 10. Unlike\nprevious monocular methods [42], [44], our method updates\nboth depth and normal iteratively through these blocks.\nInspired by RAFT [45], [46], we iteratively optimize the\nintermediate low-resolution depth ÀÜDc and unnormalized\nnormal ÀÜNu, where ÀÜ denotes low resolution prediction\nÀÜDc ‚àà R\nH\n4 √óW\n4 , and ÀÜNu ‚àà R\nH\n4 √óW\n4 √ó3, and the subscript\nc means the depth ÀÜDc is in canonical space. As sketched\nin Fig. 10, ÀÜDt\nc and ÀÜNt\nu represent the low-resolution depth\nand normal optimized after step t, where t = 0,1,2,...,T\ndenotes the step index. Initially, at step t = 0, ÀÜD0\nc and ÀÜN0\nu\nare given by the decoder. In addition to updating depth\nand normal, the optimization module also updates hidden\nfeature maps Ht, which are initialized by the decoder.\nDuring each iteration, the learned recurrent block F output\nupdates ‚àÜ ÀÜDc, ‚àÜ ÀÜNu and renews the hidden features H:\n‚àÜ ÀÜDt+1\nc ,‚àÜ ÀÜNt+1\nu ,Ht+1 = F( ÀÜDt\nu, ÀÜNt\nu,Ht,H0), (2)\nThe updates are then applied for updating the predictions:\nÀÜDt+1\nc = ÀÜDt\nc + ‚àÜÀÜDt+1\nc , ÀÜNt+1\nu = ÀÜNt\nu + ‚àÜ ÀÜNt+1\nu , (3)\nTo be more specific, the recurrent block F comprises a\nConvGRU sub-block and two projection heads. First, the\nConvGRU sub-block updates the hidden features Ht taking\nall the variables as inputs. Subsequently, the two branched\nprojection heads Gdand Gnestimate the updates ‚àÜ ÀÜDt+1 and\n‚àÜ ÀÜNt+1 respectively. A more comprehensive representation\nof Eq. 2, therefore, can be written as:\nHt+1 = ConvGRU(ÀÜDt, ÀÜNt,H0,Ht),\n‚àÜ ÀÜDt+1 = Gd(Ht+1), ‚àÜ ÀÜNt+1 = Gn(Ht+1).\n(4)\nFor detailed structures of the refinement module F, we\nrecommend readers refer to supplementary materials.\nAfter T+ 1 iterative steps, we obtain the well-optimized\nlow-resolution predictions ÀÜDT+1\nc and ÀÜNT+1\nu . These predic-\ntions are then up-sampled and post-processed to generate\nthe final depth Dc and surface normal N:\nDc = Hd(upsample( ÀÜDT+1\nc ))\nN = Hn(upsample( ÀÜNT+1\nu )),\n(5)\nwhere Hd is the ReLU function to guarantee depth is non-\nnegative, and Hn represents normalization to ensure ‚à•n‚à•=\n1 for all pixels.\nIn a general formulation, the end-to-end network in\nFig. 10 can be rewritten as:\nDc, N = Nd‚àín(Ic,Œ∏) (6)\nwhere Œ∏is the network‚Äôs (Nd‚àín) parameters.\n8\nInput Ic En-decoder\nC-GRU\nProj.\nùêáùêá0\nÔøΩùêÉùêÉùëêùëê0\nÔøΩùëµùëµùë¢ùë¢\n0\nŒî\nC-GRU\nProj.\nC-GRU\nProj.\nŒî Œî\n ÔøΩùêÉùêÉùëêùëê\nùëáùëá+1\nÔøΩùëµùëµùë¢ùë¢\nùëáùëá+1\nNormal ùëµùëµ\nDepth ùë´ùë´ùëêùëê \nUpsample & Post-process\nÔøΩùêÉùêÉùëêùëê1\nÔøΩùëµùëµùë¢ùë¢1\nÔøΩùêÉùêÉùëêùëê\n2\nÔøΩùëµùëµùë¢ùë¢\n2\nFig. 10 ‚Äì Joint depth and normal optimization. In the canonical space, we deploy recurrent blocks composed of ConvGRU\nsub-blocks (C-RGU) and projection heads (Proj.) to predict the updates ‚àÜ. During optimization, intermediate low-resolution\ndepth and normal ÀÜD0\nc ÀÜN0\nu are initially given by the decoder, and then iteratively refined by the predicted updates ‚àÜ. After\nT + 1 iterations, the optimized intermediate predictions ÀÜDT+1\nc ÀÜNT+1\nu are upsampled and post-processed to obtain the final\ndepth Dc in the canonical space and the final normal N.\n3.4 Supervision\nThe training objective is:\nmin\nŒ∏\nL(Nd‚àín(Ic,Œ∏),D‚àó\nc,N‚àó) (7)\nwhere D‚àó\nc and Ic are transformed ground-truth depth labels\nand images in the canonical space c, N‚àó denotes normal\nlabels, Lis the supervision loss to be illustrated as following.\nRandom proposal normalization loss. To boost the per-\nformance of depth estimation, we propose a random pro-\nposal normalization loss (RPNL). The scale-shift invariant\nloss [25], [27] is widely applied for the affine-invariant depth\nestimation, which decouples the depth scale to emphasize\nthe single image distribution. However, such normalization\nbased on the whole image inevitably squeezes the fine-\ngrained depth difference, particularly in close regions. In-\nspired by this, we propose to randomly crop several patches\n(pi(i=0,...,M) ‚ààRhi√ówi) from the ground truth D‚àó\nc and the\npredicted depth Dc. Then we employ the median absolute\ndeviation normalization [90] for paired patches. By normal-\nizing the local statistics, we can enhance local contrast. The\nloss function is as follows:\nLRPNL = 1\nMN\nM‚àë\npi\nN‚àë\nj\n| d‚àó\npi,j ‚àí¬µ(d‚àó\npi,j)\n1\nN\n‚àëN\nj\n‚èê‚èê‚èêd‚àó\npi,j ‚àí¬µ(d‚àó\npi,j)\n‚èê‚èê‚èê\n‚àí\ndpi,j ‚àí¬µ(dpi,j)\n1\nN\n‚àëN\nj |dpi,j ‚àí¬µ(dpi,j)|\n| (8)\nwhere d‚àó ‚àà D‚àó\nc and d ‚àà Dc are the ground truth and\npredicted depth respectively. ¬µ(¬∑) and is the median of\ndepth. M is the number of proposal crops, which is set\nto 32. During training, proposals are randomly cropped\nfrom the image by 0.125 to 0.5 of the original size. Fur-\nthermore, several other losses are employed, including the\nscale-invariant logarithmic loss [60] Lsilog, pair-wise normal\nregression loss [25] LPWN, virtual normal loss [18] LVNL.\nNote Lsilog is a variant of L1 loss. The overall losses are\nas follows.\nLd = LPWN + LVNL + Lsilog + LRPNL. (9)\nNormal loss. To supervise normal prediction, we employ\ntwo distinct loss functions depending on the availability\nof ground-truth (GT) normals N‚àó. As presented in Fig. 9,\nwhen GT normals are provided, we utilize an aleatoric\nuncertainty-aware loss [33] ( Ln(¬∑)) to supervise prediction\nN. Alternatively, in the absence of GT normals, we propose\na consistency loss Ld‚àín(D,N) to align the predicted depth\nand normal. This loss is computed based on the similarity\nbetween a pseudo-normal map generated from the pre-\ndicted depth using the least square method [41], and the\npredicted normal itself. Different from previous methods,\n[33], [41], this loss operates as a self-supervision mechanism,\nrequiring no depth or normal ground truth labels. Note that\nhere we use the depth D in the real world instead of the\none Dc in the canonical space to calculate depth-normal\nconsistency. The overall losses are as follows.\nL= wdLd(Dc,D‚àó\nc) + wnLn(N,N‚àó) + wd‚àínLd‚àín(N,D) (10)\n, where wd = 0.5, wn = 1, wd‚àín = 0.01 serve as weights to\nbalance the loss items.\n4 E XPERIMENTS\nDataset details. We have meticulously assembled a compre-\nhensive dataset incorporating 16 publicly available RGB-D\ndatasets, comprising a cumulative total of over 16 million\ndata points specifically intended for training purposes. This\ndataset encompasses a diverse array of both indoor and\noutdoor scenes. Notably, approximately 10 million frames\nwithin the dataset are annotated with normals, with a pre-\ndominant focus on annotations relating to indoor scenes. It\nis noteworthy to highlight that all datasets have provided\ncamera intrinsic parameters. Additionally, beyond the test\nsplit of training datasets, we have procured 7 previously un-\nobserved datasets to facilitate robustness and generalization\nevaluations. Detailed descriptions of the utilized training\nand testing data are provided in Table 5.\nImplementation details. In our experiments, we employ\ndifferent network architectures and aim to provide diverse\nchoices for the community, including convnets and trans-\nformers. For convnets, we employ an UNet architecture with\nthe ConvNext-large [102] backbone. ImageNet-22K pre-\ntrained weights are used for initialization. For transformers,\nwe apply DINO v2-reg [85], [103] vision transformers [86]\n(ViT) as backbones, DPT [28] as decoders.\nWe use AdamW with a batch size of 192, an initial\nlearning rate 0.0001 for all layers, and the polynomial\ndecaying method with the power of 0.9. We train our\nmodels on 48 A100 GPUs for 800k iterations. Following\nthe DiverseDepth [18], we balance all datasets in a mini-\nbatch to ensure each dataset accounts for an almost equal\nratio. During training, images are processed by the canonical\n9\nTABLE 1 ‚Äì Quantitative comparison on NYUv2 and KITTI\nmetric depth benchmarks. Methods overfitting the bench-\nmark are marked with grey, while robust depth estimation\nmethods are in blue. ‚ÄòZS‚Äô denotes the zero-shot testing, and\n‚ÄòFT‚Äô means the method is further finetuned on the bench-\nmark. Among all zero-shot testing (ZS) results, our methods\nperforms the best and is even better than overfitting meth-\nods. Further fine-tuning (FT) helps our method surpass all\nknown methods, ranked by the averaged ranking among\nall metrics. Best results are in bold and second bests are\nunderlined.\nMethod Œ¥1‚Üë Œ¥2‚Üë Œ¥3‚Üë AbsRel‚Üì log10‚Üì RMS‚Üì\nNYUv2 Metric Depth Benchmark\nLiet al.. [91] 0.788 0.958 0.991 0.143 0.063 0.635Lainaet al.. [92] 0.811 0.953 0.988 0.127 0.055 0.573VNL [30] 0.875 0.976 0.994 0.108 0.048 0.416TrDepth [20]0.900 0.983 0.996 0.106 0.045 0.365Adabins [19]0.903 0.984 0.997 0.103 0.044 0.364NeWCRFs [17]0.922 0.992 0.998 0.095 0.041 0.334IEBins [44]0.936 0.992 0.998 0.087 0.038 0.314ZeroDepth [61] ZS0.901 0.961 - 0.100 - 0.380Polymax [34] ZS0.969 0.996 0.999 0.067 0.029 0.250ZoeDepth [36] FT0.953 0.995 0.999 0.077 0.033 0.277ZeroDepth [61] FT0.954 0.995 1.000 0.074 0.103 0.269DepthAnything [93] FT0.984 0.998 1.000 0.056 0.024 0.206\nOurs Conv-L CSTMimage ZS0.925 0.983 0.994 0.092 0.040 0.341Ours Conv-L CSTMlabel ZS 0.944 0.986 0.995 0.083 0.035 0.310Ours ViT-L CSTMlabel ZS 0.975 0.994 0.998 0.063 0.028 0.251Ours ViT-g CSTMlabel ZS 0.980 0.997 0.999 0.067 0.030 0.260Ours ViT-L CSTMlabel FT0.989 0.998 1.000 0.047 0.020 0.183Ours ViT-g CSTMlabel FT 0.987 0.997 0.999 0.045 0.015 0.187\nMethod Œ¥1‚Üë Œ¥2‚Üë Œ¥3‚Üë AbsRel‚Üì RMS‚Üì RMSlog‚Üì\nKITTI Metric Depth Benchmark\nGuoet al.[94] 0.902 0.969 0.986 0.090 3.258 0.168VNL [30]0.938 0.990 0.998 0.072 3.258 0.117TrDepth [20]0.956 0.994 0.999 0.064 2.755 0.098Adabins [19]0.964 0.995 0.999 0.058 2.360 0.088NeWCRFs [17]0.974 0.997 0.999 0.052 2.129 0.079IEBins [44]0.978 0.998 0.999 0.050 2.011 0.075ZeroDepth [61] ZS0.910 0.980 0.996 0.102 4.044 0.172ZoeDepth [36] FT0.971 0.996 0.999 0.057 2.281 0.082ZeroDepth [61] FT0.968 0.995 0.999 0.053 2.087 0.083DepthAnything [93] FT0.982 0.998 1.000 0.046 1.869 0.069\nOurs Conv-L CSTMimage ZS0.967 0.995 0.999 0.060 2.843 0.087Ours Conv-L CSTMlabel ZS 0.964 0.993 0.998 0.058 2.770 0.092Ours ViT-L CSTMlabel ZS 0.974 0.995 0.999 0.052 2.511 0.074Ours ViT-g CSTMlabel ZS 0.977 0.996 0.999 0.051 2.403 0.080Ours ViT-L CSTMlabel FT 0.985 0.998 0.999 0.044 1.985 0.064Ours ViT-g CSTMlabel FT0.989 0.998 1.000 0.039 1.766 0.060\ncamera transformation module, flipped horizontally with a\n50% chance, and then randomly cropped into 512 √ó960\npixels for convnets and 616 √ó1064 for vision transformers.\nIn the ablation experiments, training settings are different\nas we sample 5000 images from each dataset for training.\nWe trained on 8 GPUs for 150K iterations. Details of net-\nworks architectures, training setups, and efficiency analysis\nare presented in the supplementary materials. Fine-tuning\nexperiments on KITTI and NYU are conducted on 8 GPUs\nwith 20K further steps.\nEvaluation details for monocular depth and normal es-\ntimation. a) To demonstrate the robustness of our met-\nric depth estimation method, we evaluate on 7 zero-shot\nbenchmarks, including NYUv2, KITTI [106], ScanNet [107],\nNuScenes [108], iBIMS-1 [109], and DIODE [110] (both in-\ndoor and outdoor). Following previous studies, we use met-\nrics such as absolute relative error (AbsRel), accuracy under\nthreshold (Œ¥i < 1.25i,i = 1,2,3), root mean squared error\n(RMS), root mean squared error in log space (RMS log), and\nlog10 error (log10). We report results for zero-shot and fine-\ntuning testing on the KITTI and NYU benchmarks. b) For\nnormal estimation tasks and ablations, employ several error\nmetrics to assess performance. Specifically, we calculate the\nmean ( mean), median ( median), and rooted mean square\n(RMS normal) of the angular error as well as the accuracy\nunder threshold of {11.25‚ó¶, 22.5‚ó¶, 30.0‚ó¶}consistent with\nmethodologies established in previous studies [33]. We\nconduct in-domain evaluation using the Scannet dataset,\nTABLE 2 ‚Äì Quantitative comparison of surface normals on\nNYUv2, ibims-1, and ScanNet normal benchmarks. ‚ÄòZS‚Äô\nmeans zero-shot testing and ‚ÄòFT‚Äô performs post fine-tuneing\non the target dataset. Methods trained only on NYU are\nhighlighted with grey. Best results are in bold and second\nbests are underlined. Our method ranks first over all bench-\nmarks.\nMethod11.25‚ó¶‚Üë 22.5‚ó¶‚Üë 30‚ó¶‚Üë mean‚Üì median‚Üì RMSnormal‚Üì\nNYUv2 Normal Benchmark\nLadickyet al.[69] 0.275 0.490 0.587 33.5 23.1 -Fouheyet al.. [95] 0.405 0.541 0.589 35.2 17.9 -Deep3D [66] 0.420 0.612 0.682 20.9 13.2 -Eigenet al.[67] 0.444 0.672 0.759 20.9 13.2 -SkipNet [96] 0.479 0.700 0.778 19.8 12.0 28.2SURGE [97] 0.473 0.689 0.766 20.6 12.2 -GeoNet [41] 0.484 0.715 0.795 19.0 11.8 26.9PAP [98] 0.488 0.722 0.798 18.6 11.7 25.5GeoNet++ [65]0.502 0.732 0.807 18.5 11.2 26.7Baeet al.[33] 0.622 0.793 0.852 14.9 7.5 23.5FrameNet [40] ZS0.507 0 .720 0 .795 18 .6 11 .0 26 .8VPLNet [99] ZS0.543 0 .738 0 .807 18 .0 9 .8 -TiltedSN [32] ZS0.598 0 .774 0 .834 16 .1 8 .1 25 .1Omnidata [35] ZS0.577 0 .777 0 .838 16 .7 9 .6 25.0Baeet al.[33] ZS 0.597 0 .775 0 .837 16 .0 8 .4 24 .7Polymax [34] ZS0.656 0 .822 0 .878 13 .1 7.1 20 .4\nOurs ViT-L CSTMlabel ZS 0.662 0 .831 0 .881 13 .1 7.1 21 .1Ours ViT-g CSTMlabel ZS 0.664 0.831 0 .881 13 .3 7 .0 21.3Ours ViT-L CSTMlabel FT 0.688 0 .849 0.898 12.0 6 .5 19 .2Ours ViT-g CSTMlabel FT 0.662 0.837 0.889 13.2 7.5 20.2\nibims-1 Normal Benchmark\nVNL [30] ZS0.179 0.386 0.494 39.8 30.4 51.0BTS [100] ZS0.130 0.295 0.400 44.0 37.8 53.5Adabins [19] ZS0.180 0.387 0.506 37.1 29.6 46.9IronDepth [42] ZS0.431 0.639 0.716 25.3 14.2 37.4Omnidata [101] ZS0.647 0 .734 0 .768 20 .8 7 .7 35.1\nOurs ViT-L CSTMlabel ZS 0.694 0.758 0.785 19.4 5 .7 34 .9Ours ViT-g CSTMlabel ZS 0.697 0 .762 0.788 19.6 5.7 35.2\nScanNet Normal Benchmark\nOmnidata [101]0.629 0 .806 0 .847 15 .1 8 .6 23.1FrameNet [40]0.625 0 .801 0 .858 14 .7 7 .7 22 .8VPLNet [99]0.663 0 .818 0 .870 12 .6 6 .0 21 .1TiltedSN [32]0.693 0 .839 0 .886 12 .6 6 .0 21 .1Baeet al.[33] 0.711 0 .854 0 .898 11 .8 5 .7 20 .0\nOurs ViT-L CSTMlabel 0.760 0.885 0.923 9.9 5.3 16.4Ours ViT-g CSTMlabel 0.778 0 .901 0.935 9 .2 5 .0 15 .3\nwhile the NYU and iBIMS-1 datasets are reserved for zero-\nshot generalization testing. c) Furthermore, we also follow\ncurrent affine-invariant depth benchmarks [25], [29] (Tab. 4)\nto evaluate the generalization ability on5 zero-shot datasets,\ni.e., NYUv2, DIODE, ETH3D, ScanNet [107], and KITTI. We\nmainly compare with large-scale data trained models. Note\nthat in this benchmark we follow existing methods to apply\nthe scale shift alignment before evaluation.\nWe report results with different canonical transformation\nmethods (CSTM lable and CSTM image) on the ConvNext-\nLarge model (Conv-L in Tab. 1 and Tab. 2). As CSTM label\nis slightly better, more results using this method from multi-\nsize ViT-models (ViT-S for Small, ViT-L for Large, ViT-g\nfor giant2) are reported. Note that all models for zero-\nshot testing use the same checkpoints except for fine-tuning\nexperiments.\nEvaluation details for reconstruction and SLAM. a) To\nevaluate our metric 3D reconstruction quality, we randomly\nsample 9 unseen scenes from NYUv2 and use colmap [111]\nto obtain the camera poses for multi-frame reconstruction.\nChamfer l1 distance and the F-score [112] are used to eval-\nuate the reconstruction accuracy. b) In dense-SLAM experi-\nments, following Li et al. [113], we test on the KITTI odom-\netry benchmark [59] and evaluate the average translational\nRMS(%,trel) and rotational RMS (‚ó¶/100m,rrel) errors [59].\nEvaluation on metric depth benchmarks. To evaluate the\naccuracy of predicted metric depth, firstly, we compare with\nstate-of-the-art (SoTA) metric depth prediction methods\non NYUv2 [58], KITTI [106]. We use the same model to\ndo all evaluations. Results are reported in Tab. 1. Firstly,\n10\nTABLE 3 ‚Äì Quantitative comparison with SoTA metric depth methods on 5 unseen benchmarks. For SoTA methods, we use\ntheir NYUv2 and KITTI models for indoor and outdoor scene evaluation respectively, while we use the same model for all\nzero-shot testing.\nMethod Metric Head DIODE(Indoor) iBIMS-1 DIODE(Outdoor) ETH3D NuScenes\nIndoor scenes (AbsRel‚Üì/RMS‚Üì) Outdoor scenes (AbsRel‚Üì/RMS‚Üì)\nAdabins [19] KITTI or NYU ‚Ä† 0.443 / 1.963 0.212 / 0.901 0.865 / 10.35 1.271 / 6.178 0.445 / 10.658\nNewCRFs [17] KITTI or NYU ‚Ä† 0.404 / 1.867 0.206 / 0.861 0.854 / 9.228 0.890 / 5.011 0.400 / 12.139\nZoeDepth [36] KITTI and NYU ‚Ä° 0.400 / 1.581 0.169 / 0.711 0.269 / 6.898 0.545 / 3.112 0.504 / 7.717\nOurs Conv-L CSTM label Unified 0.252 / 1.440 0.160 / 0.521 0.414 / 6.934 0.416 / 3.017 0.154 / 7.097\nOurs Conv-L CSTM image Unified 0.268 / 1.429 0.144 / 0.646 0.535 / 6.507 0.342 / 2.965 0.147 / 5.889\nOurs ViT-L CSTM image Unified 0.093 / 0.389 0.185 / 0.592 0.221 / 3.897 0.357 / 2.980 0.165 / 9.001\nOurs ViT-g CSTM image Unified 0.081 / 0.359 0.249 / 0.611 0.201 / 3.671 0.363 / 2.999 0.129 / 6.993\n‚Ä† : Two different metric heads are trained on KITTI and NYU respectively. ‚Ä°: Both metric heads are ensembled by an additional router.\nTABLE 4‚Äì Comparison with SoTA affine-invariant depth methods on 5 zero-shot transfer benchmarks. Our model significantly\noutperforms previous methods and sets new state-of-the-art. Following the benchmark setting, all methods have manually\naligned the scale and shift.\nMethod Backbone #Params #Data NYUv2 KITTI DIODE(Full) ScanNet ETH3D\nPretrain Train AbsRel‚ÜìŒ¥1‚Üë AbsRel‚ÜìŒ¥1‚Üë AbsRel‚ÜìŒ¥1‚Üë AbsRel‚ÜìŒ¥1‚Üë AbsRel‚ÜìŒ¥1‚Üë\nDiverseDepth [18]ResNeXt50 [104] 25M 1.3M 320K 0.117 0.875 0.190 0.704 0.376 0.631 0.108 0.882 0.228 0.694\nMiDaS [27] ResNeXt101 88M 1.3M 2M 0.111 0.885 0.236 0.630 0.332 0.715 0.111 0.886 0.184 0.752\nLeres [25] ResNeXt101 1.3M 354K 0.090 0.916 0.149 0.784 0.271 0.766 0.095 0.912 0.171 0.777\nOmnidata [35]ViT-Base 1.3M 12.2M 0.074 0.945 0.149 0.835 0.339 0.742 0.077 0.935 0.166 0.778\nHDN [29] ViT-Large [86] 306M 1.3M 300K 0.069 0.948 0.115 0.867 0.246 0.780 0.080 0.939 0.121 0.833\nDPT-large [28]ViT-Large 1.3M 188K 0.098 0.903 0.100 0.901 0.182 0.758 0.078 0.938 0.078 0.946\nDepthAnything [28]ViT-Large 142M 63.5M 0.043 0.981 0.076 0.947 0.277 0.759 0.042 0.980 0.127 0.882\nMarigold [28]Latent diffusion V2 [87] 899M 5B 74K 0.055 0.961 0.099 0.916 0.308 0.773 0.064 0.951 0.065 0.960\nOurs CSTMlabel ViT-Small 22M 142M 16M 0.056 0.965 0.064 0.950 0.247 0.789 0.033‚Ä† 0.985‚Ä† 0.062 0.955\nOurs CSTMimage ConvNeXt-Large [102] 198M 14.2M 8M 0.058 0.963 0.053 0.965 0.211 0.825 0.074 0.942 0.064 0.965\nOurs CSTMlabel ConvNeXt-Large 14.2M 8M 0.050 0.966 0.058 0.970 0.224 0.805 0.074 0.941 0.066 0.964\nOurs CSTMlabel ViT-Large 306M 142M 16M 0.042 0.980 0.046 0.979 0.141 0.882 0.021‚Ä† 0.993‚Ä† 0.042 0.987\nOurs CSTMlabel ViT-giant [105] 1011M 142M 16M 0.043 0.981 0.044 0.982 0.136 0.895 0.022‚Ä† 0.994‚Ä† 0.042 0.983\n‚Ä† : ScanNet is partly annotated with normal [40]. For samples without normal annotations, these models use depth labels to facilitate normal learning.\ncomparing with existing overfitting methods, which are\ntrained on benchmarks for hundreds of epochs, our zero-\nshot testing (‚ÄòZS‚Äô in the table) without any fine-tuning or\nmetric adjustment already achieves comparable or even\nbetter performance on some metrics. Then comparing with\nrobust monocular depth estimation methods, such as Ze-\nrodepth [61] and ZoeDepth [36], our zero-shot testing is also\nbetter than them. Further post finetuning (‚ÄòFT in the table‚Äô)\nlifts our method to the 1st rank.\nFurthermore, We collect 5 unseen datasets to do more\nmetric accuracy evaluation. These datasets contain a wide\nrange of indoor and outdoor scenes, including rooms, build-\nings, and driving scenes. The camera models are also varied.\nWe mainly compare with the SoTA metric depth estimation\nmethods and take their NYUv2 and KITTI models for indoor\nand outdoor scene evaluation respectively. From Tab. 3, we\nobserve that although NuScenes is similar to KITTI, existing\nmethods face a noticeable performance decrease. In contrast,\nour model is more robust.\nGeneralization over diverse scenes. Affine-invariant depth\nbenchmarks decouple the scale‚Äôs effect, which aims to eval-\nuate the model‚Äôs generalization ability to diverse scenes. Re-\ncent impact works, such as MiDaS, LeReS, DPT, Marigold,\nand DepthAnything achieved promising performance on\nthem. Following them, we test on 5 datasets and manually\nalign the scale and shift to the ground-truth depth before\nevaluation. Results are reported in Tab. 4. Although our\nmethod enforces the network to recover the more challeng-\ning metric, our method outperforms them on all datasets.\nEvaluation on surface normal benchmarks. We evaluate\nour methods on ScanNet, NYU, and iBims-1 surface normal\nbenchmarks. Results are reported in Tab. 2. Firstly, we\norganize a zero-shot testing benchmark on NYU dataset,\nsee methods denoted with ‚ÄòZS‚Äô in the table. We compare\nwith existing methods which are trained on ScanNet or\nTaskonomy and have achieved promising performance on\nthem, such as Polymax [34] and Bae et al. [33]. Our method\nsurpasses them over most metrics. Cmparing with methods\nthat have been overfitted the NYU data domain for hun-\ndreds of epochs (marked with blue), our zero-shot testing\noutperforms them on all metrics. Our post-finetuned models\n(‚ÄòFT‚Äô marks) further boost the performance. Similarly, we\nalso achieve SoTA performance on iBims-1 and Scannet\nbenchmarks. For the iBims-1 dataset, we follow IronDepth\n[42] to generate the ground-truth normal annotations.\nTABLE 5 ‚Äì Training and testing datasets used for experi-\nments.\nDatasets Scenes Source Label Size #Cam.\nTraining Data\nDDAD [114] OutdoorReal-worldDepth ‚àº80K 36+Lyft [115] OutdoorReal-worldDepth ‚àº50K 6+Driving Stereo (DS) [116]OutdoorReal-worldDepth ‚àº181K 1DIML [117] OutdoorReal-worldDepth ‚àº122K 10Arogoverse2 [118] OutdoorReal-worldDepth ‚àº3515K 6+Cityscapes [119] OutdoorReal-worldDepth ‚àº170K 1DSEC [120] OutdoorReal-worldDepth ‚àº26K 1Mapillary PSD [89] OutdoorReal-worldDepth 750K 1000+Pandaset [121] OutdoorReal-worldDepth ‚àº48K 6UASOL [122] OutdoorReal-worldDepth ‚àº1370K 1Virtual KITTI [123] OutdoorSynthesizedDepth 37K 2Waymo [124] OutdoorReal-worldDepth ‚àº1M 5Matterport3d [125] In/Out Real-worldDepth + Normal144K 3Taskonomy [125] Indoor Real-worldDepth + Normal‚àº4M ‚àº1MReplica [126] Indoor Real-worldDepth + Normal‚àº150K 1\nScanNet‚Ä†[107] Indoor Real-worldDepth + Normal‚àº2.5M 1HM3d [127] Indoor Real-worldDepth + Normal‚àº2000K 1Hypersim [128] Indoor SynthesizedDepth + Normal54K 1\nTesting Data\nNYU [58] Indoor Real-worldDepth+Normal654 1KITTI [59] OutdoorReal-worldDepth 652 4\nScanNet‚Ä†[107] Indoor Real-worldDepth+Normal700 1NuScenes (NS) [108]OutdoorReal-worldDepth 10K 6ETH3D [129] OutdoorReal-worldDepth 431 1DIODE [110] In/Out Real-worldDepth 771 1iBims-1 [109] Indoor Real-worldDepth 100 1\n‚Ä† ScanNet is a non-zero-shot testing dataset for our ViT models.\n11\nRGB GT Depth Ours Depth ZoeDepth GT Normal Ours Normal Bae et al.\n OmniData\nFig. 11 ‚Äì Qualitative comparisons of metric depth and surface normals for iBims, DIODE, NYU, Eth3d, Nuscenes, and\nself-collected drone datasets. We present visualization results of our predictions (‚ÄòOurs Depth‚Äô / ‚ÄòOurs Normal‚Äô), groundtruth\nlabels (‚ÄòGT Depth‚Äô / ‚ÄòGT Normal‚Äô) and results from other metric depth (‚ÄòZoeDepth‚Äô [36]) and surface normal methods (‚ÄòBae et\nal.‚Äô [33] and ‚ÄòOmniData‚Äô [35]).\n12\nRGB Ours Depth ZoeDepth Ours Normal Bae et al. OmniData\nFig. 12 ‚Äì Qualitative comparisons of metric depth and surface normals in the wild. We present visualization results of our\npredictions (‚ÄòOurs Depth‚Äô / ‚ÄòOurs Normal‚Äô) and results from other metric depth (‚ÄòZoeDepth‚Äô [36]) and surface normal methods\n(‚ÄòBae et al.‚Äô [33] and ‚ÄòOmniData‚Äô [35]).\n4.1 Zero-shot Generalization\nQualitative comparisons of surface normals and depths.\nWe visualize our predictions in Fig. 11. A comparison with\nanother widely used generalized metric depth method,\nZoeDepth [36], demonstrates that our approach produces\ndepth maps with superior details on fine-grained structures\n(objects in row1, suspension lamp in row4, beam in row8),\nand better foreground/background distinction (row 11, 12).\nIn terms of surface normal prediction, our normal maps\nexhibits significantly finer details compared to Bae. et al. [33]\nand can handle some cases where their method fail (row7,\n8, 9). Our method not only generalizes well across diverse\nscenarios but can also be directly applied to unseen camera\nmodels like the fisheye camera shown in row 12. More\nvisualization results for in-the-wild images are presented\nin Fig. 12, including comic-style (Row 2) and CG(computer\ngraphics)-generated objects (Row5)\n4.2 Applications Based on Our Method\nWe apply the CSTM image model to various tasks.\n3D scene reconstruction . To present our method‚Äôs ability to\nrecover real-world metric 3D, we first conduct a quantitative\ncomparison on 9 unseen NYUv2 scenes. We predict per-\nframe metric depth and fuse these with the provided camera\nposes, with results detailed in Table 6. We compare our\napproach to several methods: the video consistent depth\nprediction method (RCVD [130]), unsupervised video depth\nestimation (SC-DepthV2 [131]), 3D scene shape recovery\n(LeReS [25]), affine-invariant depth estimation (DPT [28]),\nand multi-view stereo reconstruction (DPSNet [48], Sim-\npleRecon [132]). Except for the multi-view approaches and\nour method, all others require aligning scales with ground\ntruth depth for each frame. While our approach is not specif-\nically designed for video or multi-view reconstruction, it\ndemonstrates promising frame consistency and significantly\nmore accurate 3D scene reconstructions in these zero-shot\nscenarios. Qualitative comparisons in Fig. 13 reveal that our\nreconstructions exhibit considerably less noise and fewer\noutliers.\nDense-SLAM mapping. Monocular SLAM is a key robotic\napplication that uses a single video input to create trajec-\ntories and dense 3D maps. However, due to limited photo-\nmetric and geometric constraints, existing methods struggle\nwith scale drift in large scenes and fail to recover accurate\nmetric information. Our robust metric depth estimation\nserves as a strong depth prior for the SLAM system. To\ndemonstrate this, we input our metric depth into the state-\nof-the-art SLAM system, Droid-SLAM [37], and evaluate the\ntrajectory on KITTI without any tuning. Results are shown\nin Table 7. With access to accurate per-frame metric depth,\nDroid-SLAM experiences a significant reduction in transla-\ntion drift (trel). Additionally, our depth data enables Droid-\nSLAM to achieve denser and more precise 3D mapping,\nas illustrated in Fig. 3 and detailed in the supplementary\nmaterials.\nWe also tested on the ETH3D SLAM benchmarks, with\nresults in Table 8. Using our metric depth predictions,\nDroid-SLAM shows improved performance, although the\ngains are less pronounced in the smaller indoor scenes of\nETH3D compared to KITTI.\nMetrology in the wild. To demonstrate the robustness and\naccuracy of our recovered metric 3D shapes, we down-\nloaded Flickr photos taken by various cameras and ex-\ntracted coarse camera intrinsic parameters from their meta-\ndata. We utilized our CSTM image model to reconstruct the\nmetric shapes and measure the sizes of structures (marked\nin red in Fig. 14), with ground-truth sizes shown in blue.\nThe results indicate that our measured sizes closely align\nwith the ground-truth values.\nMonocular reconstruction in the wild. To further visualize\nthe reconstruction quality of our recovered metric depth, we\nrandomly collect images from the internet and recover their\n13\nTABLE 6 ‚Äì Quantitative comparison of 3D scene reconstruction with LeReS [25], DPT [28], RCVD [130], SC-DepthV2 [131],\nand two learning-based MVS methods (DPSNet [48], SimpleRecon [132]) on 9 unseen NYUv2 scenes. Apart from the MVS\napproaches and ours, other methods have to align the scale with ground truth depth for each frame. As a result, our\nreconstructed 3D scenes achieve the best performance.\nMethodBasement0001aBedroom0015Diningroom0004Kitchen0008Classroom0004Playroom0002 Office0024 Office0004 Diningroom0033\nC-l1‚Üì F-score‚Üë C-l1‚ÜìF-score‚ÜëC-l1‚Üì F-score‚Üë C-l1‚ÜìF-score‚ÜëC-l1‚Üì F-score‚Üë C-l1‚Üì F-score‚Üë C-l1‚ÜìF-score‚ÜëC-l1‚ÜìF-score‚ÜëC-l1‚Üì F-score‚Üë\nRCVD [130]0.364 0.276 0.074 0.582 0.462 0.251 0.053 0.620 0.187 0.327 0.791 0.187 0.324 0.241 0.646 0.217 0.445 0.253\nSC-DepthV2 [131]0.254 0.275 0.064 0.547 0.749 0.229 0.049 0.624 0.167 0.267 0.426 0.263 0.482 0.138 0.516 0.244 0.356 0.247\nDPSNet [48]0.243 0.299 0.195 0.276 0.995 0.186 0.269 0.203 0.296 0.195 0.141 0.485 0.199 0.362 0.210 0.462 0.222 0.493\nDPT [25]0.698 0.251 0.289 0.226 0.396 0.364 0.126 0.388 0.780 0.193 0.605 0.269 0.454 0.245 0.364 0.279 0.751 0.185\nLeReS [25]0.081 0.555 0.064 0.616 0.278 0.427 0.147 0.289 0.143 0.480 0.145 0.503 0.408 0.176 0.096 0.497 0.241 0.325\nSimpleRecon [132]0.068 0.695 0.086 0.449 0.199 0.413 0.055 0.624 0.142 0.461 0.092 0.517 0.054 0.638 0.051 0.681 0.165 0.565\nOurs0.042 0.736 0.059 0.610 0.159 0.485 0.050 0.645 0.145 0.445 0.036 0.814 0.069 0.638 0.045 0.700 0.060 0.663\nOurs SimpleRecon\nGT\n DPSNet\n LeReS\nFig. 13 ‚Äì Reconstruction of zero-shot scenes with multiple views. We sample several NYUv2 scenes for 3D reconstruction\ncomparison. As our method can predict accurate metric depth, thus all frame‚Äôs predictions are fused directly for reconstruction.\nBy contrast, LeReS [25]‚Äôs depth is up to an unknown scale and shift, causing noticeable distortions. For MVS methods,\nDPSNet [48] fails on low-texture backgrounds, while SimpleRecon [132] distorts regions without sufficient observations.\nTABLE 7 ‚Äì Comparison with SoTA SLAM methods on\nKITTI. We input predicted metric depth to the Droid-\nSLAM [37] (‚ÄòDroid+Ours‚Äô), which outperforms others by a\nlarge margin on trajectory accuracy.\nMethod Seq 00 Seq 02 Seq 05 Seq 06 Seq 08 Seq 09 Seq 10Translational RMS drift (trel,‚Üì) / Rotational RMS drift (rrel,‚Üì)\nGeoNet [133]27.6/5.7242.24/6.1420.12/7.679.28/4.3418.59/7.8523.94/9.8120.73/9.1VISO2-M [134]12.66/2.739.47/1.1915.1/3.656.8/1.9314.82/2.523.69/1.2521.01/3.26ORB-V2 [12]11.43/0.5810.34/0.269.04/0.2614.56/0.2611.46/0.289.3/0.262.57/0.32Droid [37]33.9/0.2934.88/0.2723.4/0.2717.2/0.2639.6/0.3121.7/0.237/0.25\nDroid+Ours1.44/0.372.64/0.291.44/0.25 0.6/0.2 2.2/0.3 1.63/0.222.73/0.23\nTABLE 8 ‚Äì Comparison of VO error on ETH3D benchmark.\nDroid SLAM system is input with our depth (‚ÄòDroid +\nOurs‚Äô), and ground-truth depth (‚ÄòDroid + GT‚Äô). The average\ntrajectory error is reported.\nEinsteinManquin4 Motion1Plant- sfm house sfmlab\nglobal scene3 loop room2\nAverage trajectory error (‚Üì)\nDroid 4.7 0.88 0.83 0.78 5.64 0.55\nDroid + Ours1.5 0.69 0.62 0.34 4.03 0.53\nDroid + GT 0.7 0.006 0.024 0.006 0.96 0.013\nmetric 3D and normals. As there is no focal length provided,\nwe select proper focal lengths according to the reconstructed\nshape and normal maps. The reconstructed pointclouds are\ncolorized by their corresponding normals (Different views\nare marked by red and orange arrays in Fig. 15).\n4.3 Ablation Study\nAblation on canonical transformation. We examine\nthe impact of our proposed canonical transformations\nfor input images (CSTM input) and ground-truth labels\n(CSTM output). Results are presented in Table 9. We trained\nthe model on a mixed dataset of 90,000 images and tested\nit across six datasets. A naive baseline (Ours w/o CSTM)\nremoves the CSTM modules, enforcing the same supervi-\nsion as our approach. Without CSTM, the model struggles to\nconverge on mixed metric datasets and fails to achieve met-\nric predictions on zero-shot datasets. This limitation is why\nrecent mixed-data training methods often resort to learn-\ning affine-invariant depth to sidestep metric challenges. In\n14\n15.6m/GT: 26m\n49.2m /GT: 52m\nNikon \nD200\n2.2m/GT1.7m2.5m/GT  1.9m\niPhone X\n10.9m /GT 12m\n2.6m/GT 2.5m\n3.1m/GT3.7m\nCanon \n600D\nFig. 14 ‚Äì Metrology of in-the-wild scenes. We collect sev-\neral Flickr photos, which are captured by various cameras.\nWith photos‚Äô metadata, we reconstruct the 3D metric shape\nand measure structures‚Äô sizes. Red and blue marks are ours\nand ground-truth sizes respectively.\nRGB Recon. (View A) Recon. (View B)\nFig. 15 ‚Äì Reconstruction from in-the-wild single images.\nWe collect web images and select proper focal lengths. The\nreconstructed pointclouds are colorized by normals.\ncontrast, both of our CSTM methods enable the model to\nattain metric prediction capabilities and achieve compara-\nble performance. Table 1 confirms this comparable perfor-\nmance. Thus, adjusting supervision and the appearance of\ninput images during training effectively addresses metric\nambiguity issues. Additionally, we compared our approach\nwith CamConvs [38], which incorporates the camera model\nin the decoder using a 4-channel feature. While CamConvs\nuses the same training schedule, model, and data, it relies\non the network to implicitly learn various camera models\nfrom image appearance, linking image size to real-world\ndimensions. We believe this approach strains data diversity\nand network capacity, resulting in lower performance.\nAblation on canonical space. We investigate the impact\nof the canonical camera, specifically the canonical focal\nlength. The model is trained on a small sampled dataset\nTABLE 9 ‚Äì Effectiveness of our CSTM. CamConvs [38]\ndirectly encodes various camera models in the network,\nwhile we perform a simple yet effective transformation\nto solve the metric ambiguity. Without CSTM, the model\nachieve transferable metric prediction ability.\nMethod DDAD Lyft DS NS KITTI NYU\nTest set of train. data (AbsRel‚Üì) Zero-shot test set (AbsRel‚Üì)\nw/o CSTM 0.530 0 .582 0 .394 1.00 0 .568 0 .584\nCamConvs [38] 0.295 0 .315 0 .213 0.423 0 .178 0 .333\nOurs CSTMimage 0.190 0 .235 0 .182 0.197 0 .097 0 .210\nOurs CSTMlabel 0.183 0 .221 0 .201 0.213 0 .081 0 .212\nTABLE 10 ‚Äì Effectiveness of random proposal normaliza-\ntion loss. Baseline is supervised by ‚ÄòLPWN + LVNL + Lsilog‚Äô.\nSSIL is the scale-shift invariant loss proposed in [27].\nMethod DDAD Lyft DS NS KITTI NYUv2\nTest set of train. data (AbsRel‚Üì) Zero-shot test set (AbsRel‚Üì)\nbaseline 0.204 0 .251 0 .184 0.207 0 .104 0 .230\nbaseline + SSIL [27]0.197 0 .263 0 .259 0.206 0 .105 0 .216\nbaseline + RPNL 0.190 0.235 0.182 0.197 0.097 0.210\nand evaluated on both the training and validation sets. We\ncalculate the average Absolute Relative (AbsRel) error for\nthree different focal lengths: 250, 500, 1000, 1500, and 2500.\nOur experiments indicate that a focal length of 1000 yields\nslightly better performance than the others; further details\ncan be found in the supplementary materials.\nEffectiveness of the random proposal normalization loss.\nTo demonstrate the effectiveness of our random proposal\nnormalization loss (RPNL), we conducted experiments on\na sampled small dataset, with results shown in Table 10.\nWe tested on DDAD, Lyft, DrivingStereo (DS), NuScenes\n(NS), KITTI, and NYUv2. The ‚Äôbaseline‚Äô includes all losses\nexcept RPNL, which we compare to ‚Äôbaseline + RPNL‚Äô and\n‚Äôbaseline + SSIL [27]‚Äô. Our RPNL significantly enhances\nperformance, while the scale-shift invariant loss [27], which\nnormalizes the entire image, offers slight improvements.\nEffectiveness of joint optimization. We assess the impact\nof joint optimization on both depth and normal estimation\nRGB Ours normal W.o. depth\nW.o. consistency\nFig. 16 ‚Äì Effect of joint depth-normal optimization. We com-\npare normal maps learned by different strategies on several\noutdoor examples. Learning normal only ‚Äòwithout depth‚Äô\nleads to flattened surfaces, since most of the normal labels\nlie on planes. In addition, ‚Äòwithout consistency‚Äô imposed\nbetween depth and normal, the predictions become much\ncoarser.\n15\nTABLE 11 ‚Äì Effectiveness of joint optimization. Joint op-\ntimization surpasses independent estimation. For outdoor\nnormal estimation, this module introduces geometry clues\nfrom large-scale depth data. The proposed recurrent block\nand depth-normal consistency constraint are essential for\nthe optimization\nMethod DIODE(Outdoor) NYU DIODE(Outdoor) NYU\nDepth (AbsRel‚Üì) Normal (Med. error‚Üì)\nW.o. normal 0.315 0.119 - -\nW.o. depth - - 16.25 8.78\nW.o mixed datasets 0.614 0 .116 18.94 9 .50\nW.o. recurrent block 0.309 0 .127 16.51 9 .31\nW.o. consistency 0.310 0 .121 16.45 9 .72\nOurs 0.304 0.114 14.91 8.77\nusing small datasets sampled with ViT-small models over a\n4-step iteration. The evaluation is conducted on the NYU\nindoor dataset and the DIODE outdoor dataset, both of\nwhich include normal labels for the convenience of eval-\nuation. In Tab. 11, we start by training the same-architecture\nnetworks ‚Äòwithout depth‚Äô or ‚Äòwithout normal‚Äô prediction.\nCompared to our joint optimization approach, both single-\nmodality models exhibit slightly worse performance.To fur-\nther demonstrate the benefit of joint optimization and the in-\ncorporation of large-scale outdoor data prior to normal esti-\nmation, we train a model using only the Taskonomy dataset\n(i.e., ‚ÄôW.o. mixed datasets‚Äô), which shows inferior results on\nDIODE(outdoor). We also verify the effectiveness of the re-\ncurrent blocks and the consistency loss. Removing either of\nthem (‚ÄòW.o. consistency‚Äô / ‚ÄòW.o. recurrent block‚Äô) could lead\nto drastic performance degradation for normal estimation,\nparticularly for outdoor scenarios like DIODE(Outdoor).\nFurthermore, we present some visualization comparisons in\nFig 16. Training surface normal and depth together without\nthe consistency loss (‚ÄôW.o. consistency‚Äô) results in notably\npoorer predicted normals compared to our full method\n(‚ÄôOurs normal‚Äô). Additionally, if the model learns the normal\nindividually (‚ÄôW.o. depth‚Äô), the performance also degrades.\nThe efficiency analysis of the joint optimization module is\npresented in the supplementary materials.\nSelection of intermediate normal representation. During\noptimization, unnormalized normal vectors are utilized as\nthe intermediate representation. Here we explore three addi-\ntional representations (1) A vector defined in so3 represent-\ning 3D rotation upon a reference direction. We implement\nthis vector by lietorch [37]. (2) An azimuthal angle and a po-\nlar angle. (3) A 2D homogeneous vector [81]. All the repre-\nsentations investigated are additive and can be surjectively\ntransferred into surface normal. In this experiment, we only\nchange the representations and compare the performances.\nSurprisingly, according to Table 12, the naive unnormalized\nnormal performs the best. We hypothesize that this simplest\nrepresentation reduces the learning difficulty.\nTABLE 12 ‚Äì More selection of intermediate normal repre-\nsentation.\nRepresentation Taskonomy Scannet DIODE(Outdoor) NYU\nTest set (Med. err.‚Üì) Zero-shot testing (Med. err.‚Üì)\n3D rotation vector 5.28 8 .92 16.00 9 .45\nAzi. and polar angles 5.34 9 .01 15.21 9 .21\nHomo. 2D vector [81] 5.02 8 .50 15.40 8 .79\nOurs (unnormalized 3D)5.01 8.41 14.91 8.77\nBest optimizing steps To determine the optimal number of\noptimization steps for various ViT models, we vary different\nsteps to refine depth and normal. Table 13 illustrates that in-\ncreasing the number of iteration steps does not consistently\nimprove results. Moreover, the ideal number of steps may\ndiffer based on the model size, with larger models generally\nbenefiting from more extensive optimization.\nTABLE 13 ‚Äì Select the best joint optimizing steps for differ-\nent ViT models. We find the best step varying with model\nsize. All models are trained following the settings in Tab. 11\n# Steps\nBackbone ViT-Small ViT-Large ViT-giant\nKITTI Depth (AbsRel‚Üì) / NYU v2 Normal (Med. err.‚Üì)\n2 0.102/9.01 0.070/8.40 0.069/8.25\n4 0.088/8.77 0.067/8.24 0.067/8.23\n8 0.090/8.80 0.065/8.21 0.064/8.22\n16 0.095/8.79 0.068/8.30 0.065/8.27\n5 C ONCLUSION\nIn this paper, we introduce a family of geometric foun-\ndation models for zero-shot monocular metric depth and\nsurface normal estimation. We propose solutions to address\nchallenges in both metric depth estimation and surface\nnormal estimation. To resolve depth ambiguity caused by\nvarying focal lengths, we present a novel canonical camera\nspace transformation method. Additionally, to overcome\nthe scarcity of outdoor normal data labels, we introduce a\njoint depth-normal optimization framework that leverages\nknowledge from large-scale depth annotations.\nOur approach enables the integration of millions of data\nsamples captured by over 10,000 cameras to train a unified\nmetric-depth and surface-normal model. To enhance the\nmodel‚Äôs robustness, we curate a dataset comprising over 16\nmillion samples for training. Zero-shot evaluations demon-\nstrate the effectiveness and robustness of our method. For\ndownstream applications, our models are capable of recon-\nstructing metric 3D from a single view, enabling metrology\non randomly collected internet images and dense mapping\nof large-scale scenes. With their precision, generalization,\nand versatility, Metric3D v2 models serve as geometric\nfoundational models for monocular perception.\nREFERENCES\n[1] B. Yang, S. Rosa, A. Markham, N. Trigoni, and H. Wen, ‚ÄúDense\n3d object reconstruction from a single depth view,‚Äù IEEE trans-\nactions on pattern analysis and machine intelligence , vol. 41, no. 12,\npp. 2820‚Äì2834, 2018. 1\n[2] J. Ju, C. W. Tseng, O. Bailo, G. Dikov, and M. Ghafoorian, ‚ÄúDg-\nrecon: Depth-guided neural 3d scene reconstruction,‚Äù in Proceed-\nings of the IEEE/CVF International Conference on Computer Vision ,\npp. 18184‚Äì18194, 2023. 1\n[3] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and\nA. Geiger, ‚ÄúOccupancy networks: Learning 3d reconstruction\nin function space,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn. ,\npp. 4460‚Äì4470, 2019. 1\n[4] K. Deng, A. Liu, J.-Y. Zhu, and D. Ramanan, ‚ÄúDepth-supervised\nnerf: Fewer views and faster training for free,‚Äù inProceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npp. 12882‚Äì12891, 2022. 1\n[5] B. Roessle, J. T. Barron, B. Mildenhall, P . P . Srinivasan, and\nM. Nie√üner, ‚ÄúDense depth priors for neural radiance fields from\nsparse input views,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 12892‚Äì12901, 2022. 1\n[6] Z. Yu, S. Peng, M. Niemeyer, T. Sattler, and A. Geiger, ‚ÄúMonosdf:\nExploring monocular geometric cues for neural implicit surface\nreconstruction,‚Äù Advances in neural information processing systems ,\nvol. 35, pp. 25018‚Äì25032, 2022. 1, 5\n[7] C. Jiang, H. Zhang, P . Liu, Z. Yu, H. Cheng, B. Zhou, and S. Shen,\n‚ÄúH2-mapping: Real-time dense mapping using hierarchical hy-\nbrid representation,‚Äù arXiv preprint arXiv:2306.03207, 2023. 1\n[8] Y. Li, Z. Ge, G. Yu, J. Yang, Z. Wang, Y. Shi, J. Sun, and Z. Li,\n‚ÄúBevdepth: Acquisition of reliable depth for multi-view 3d object\ndetection,‚Äù arXiv: Comp. Res. Repository, p. 2206.10092, 2022. 1\n16\n[9] Z. Li, Z. Yu, D. Austin, M. Fang, S. Lan, J. Kautz, and J. M.\nAlvarez, ‚ÄúFb-occ: 3d occupancy prediction based on forward-\nbackward view transformation,‚Äù arXiv preprint arXiv:2307.01492,\n2023. 1\n[10] R. Fan, H. Wang, P . Cai, and M. Liu, ‚ÄúSne-roadseg: Incorporat-\ning surface normal information into semantic segmentation for\naccurate freespace detection,‚Äù in European Conference on Computer\nVision, pp. 340‚Äì356, Springer, 2020. 1\n[11] J. Behley and C. Stachniss, ‚ÄúEfficient surfel-based slam using 3d\nlaser range data in urban environments.,‚Äù in Robotics: Science and\nSystems, vol. 2018, p. 59, 2018. 1, 5\n[12] R. Mur-Artal and J. D. Tard ¬¥os, ‚ÄúORB-SLAM2: an open-source\nSLAM system for monocular, stereo and RGB-D cameras,‚Äù IEEE\nTrans. Robot., vol. 33, no. 5, pp. 1255‚Äì1262, 2017. 1, 13\n[13] T. Schops, T. Sattler, and M. Pollefeys, ‚ÄúBad slam: Bundle ad-\njusted direct rgb-d slam,‚Äù in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 134‚Äì144, 2019.\n1\n[14] H. Zhu, H. Yang, X. Wu, D. Huang, S. Zhang, X. He, T. He,\nH. Zhao, C. Shen, Y. Qiao, et al. , ‚ÄúPonderv2: Pave the way for\n3d foundataion model with a universal pre-training paradigm,‚Äù\narXiv preprint arXiv:2310.08586, 2023. 1\n[15] J. Zhou, J. Wang, B. Ma, Y.-S. Liu, T. Huang, and X. Wang,\n‚ÄúUni3d: Exploring unified 3d representation at scale,‚Äù arXiv\npreprint arXiv:2310.06773, 2023. 1\n[16] H. Xu, J. Zhang, J. Cai, H. Rezatofighi, F. Yu, D. Tao, and\nA. Geiger, ‚ÄúUnifying flow, stereo and depth estimation,‚Äù IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2023. 1\n[17] W. Yuan, X. Gu, Z. Dai, S. Zhu, and P . Tan, ‚ÄúNew CRFs: Neural\nwindow fully-connected CRFs for monocular depth estimation,‚Äù\nin Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2022. 1, 2, 4, 9, 10\n[18] W. Yin, Y. Liu, and C. Shen, ‚ÄúVirtual normal: Enforcing geometric\nconstraints for accurate and robust depth prediction,‚ÄùIEEE Trans.\nPattern Anal. Mach. Intell., 2021. 1, 2, 4, 5, 8, 10\n[19] S. F. Bhat, I. Alhashim, and P . Wonka, ‚ÄúAdabins: Depth estimation\nusing adaptive bins,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,\npp. 4009‚Äì4018, 2021. 1, 2, 4, 9, 10\n[20] G. Yang, H. Tang, M. Ding, N. Sebe, and E. Ricci, ‚ÄúTransformer-\nbased attention networks for continuous pixel-wise prediction,‚Äù\nin Proc. IEEE Int. Conf. Comp. Vis., 2021. 1, 2, 9\n[21] K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, and Z. Luo,\n‚ÄúMonocular relative depth perception with web stereo data su-\npervision,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn. , pp. 311‚Äì\n320, 2018. 2, 5\n[22] K. Xian, J. Zhang, O. Wang, L. Mai, Z. Lin, and Z. Cao, ‚ÄúStructure-\nguided ranking loss for single image depth prediction,‚Äù in Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn., pp. 611‚Äì620, 2020. 2, 5\n[23] W. Chen, S. Qian, D. Fan, N. Kojima, M. Hamilton, and J. Deng,\n‚ÄúOasis: A large-scale dataset for single image 3d in the wild,‚Äù in\nProc. IEEE Conf. Comp. Vis. Patt. Recogn., pp. 679‚Äì688, 2020. 2, 5\n[24] W. Chen, Z. Fu, D. Yang, and J. Deng, ‚ÄúSingle-image depth\nperception in the wild,‚Äù in Proc. Advances in Neural Inf. Process.\nSyst., pp. 730‚Äì738, 2016. 2, 5\n[25] W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai, S. Chen, and\nC. Shen, ‚ÄúLearning to recover 3d scene shape from a single\nimage,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn. , 2021. 2, 3,\n4, 5, 8, 9, 10, 12, 13\n[26] W. Yin, J. Zhang, O. Wang, S. Niklaus, S. Chen, Y. Liu, and\nC. Shen, ‚ÄúTowards accurate reconstruction of 3d scene shape\nfrom a single monocular image,‚Äù IEEE Trans. Pattern Anal. Mach.\nIntell., 2022. 2, 5\n[27] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V . Koltun,\n‚ÄúTowards robust monocular depth estimation: Mixing datasets\nfor zero-shot cross-dataset transfer,‚Äù IEEE Trans. Pattern Anal.\nMach. Intell., 2020. 2, 3, 5, 8, 10, 14\n[28] R. Ranftl, A. Bochkovskiy, and V . Koltun, ‚ÄúVision transformers for\ndense prediction,‚Äù in Proc. IEEE Int. Conf. Comp. Vis. , pp. 12179‚Äì\n12188, 2021. 2, 8, 10, 12, 13\n[29] C. Zhang, W. Yin, Z. Wang, G. Yu, B. Fu, and C. Shen, ‚ÄúHierarchi-\ncal normalization for robust monocular depth estimation,‚Äù Proc.\nAdvances in Neural Inf. Process. Syst. , 2022. 2, 3, 5, 9, 10\n[30] W. Yin, Y. Liu, C. Shen, and Y. Yan, ‚ÄúEnforcing geometric con-\nstraints of virtual normal for depth prediction,‚Äù in Proc. IEEE Int.\nConf. Comp. Vis., 2019. 2, 4, 9\n[31] B. Ke, A. Obukhov, S. Huang, N. Metzger, R. C. Daudt, and\nK. Schindler, ‚ÄúRepurposing diffusion-based image generators for\nmonocular depth estimation,‚Äù arXiv preprint arXiv:2312.02145 ,\n2023. 2\n[32] T. Do, K. Vuong, S. I. Roumeliotis, and H. S. Park, ‚ÄúSurface nor-\nmal estimation of tilted images via spatial rectifier,‚Äù in Computer\nVision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August\n23‚Äì28, 2020, Proceedings, Part IV 16 , pp. 265‚Äì280, Springer, 2020.\n2, 5, 9\n[33] G. Bae, I. Budvytis, and R. Cipolla, ‚ÄúEstimating and exploiting the\naleatoric uncertainty in surface normal estimation,‚Äù in Proceed-\nings of the IEEE/CVF International Conference on Computer Vision ,\npp. 13137‚Äì13146, 2021. 2, 5, 8, 9, 10, 11, 12\n[34] X. Yang, L. Yuan, K. Wilber, A. Sharma, X. Gu, S. Qiao, S. Debats,\nH. Wang, H. Adam, M. Sirotenko,et al., ‚ÄúPolymax: General dense\nprediction with mask transformer,‚Äù inProceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision , pp. 1050‚Äì\n1061, 2024. 2, 5, 9, 10\n[35] A. Eftekhar, A. Sax, J. Malik, and A. Zamir, ‚ÄúOmnidata: A scalable\npipeline for making multi-task mid-level vision datasets from 3d\nscans,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn. , pp. 10786‚Äì\n10796, 2021. 2, 3, 5, 9, 10, 11, 12\n[36] S. F. Bhat, R. Birkl, D. Wofk, P . Wonka, and M. M ¬®uller, ‚ÄúZoedepth:\nZero-shot transfer by combining relative and metric depth,‚ÄùarXiv\npreprint arXiv:2302.12288, 2023. 3, 5, 9, 10, 11, 12\n[37] Z. Teed and J. Deng, ‚ÄúDroid-slam: Deep visual slam for monocu-\nlar, stereo, and rgb-d cameras,‚Äù vol. 34, pp. 16558‚Äì16569, 2021. 3,\n4, 12, 13, 15\n[38] J. Facil, B. Ummenhofer, H. Zhou, L. Montesano, T. Brox, and\nJ. Civera, ‚ÄúCAM-Convs: camera-aware multi-scale convolutions\nfor single-view depth,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., pp. 11826‚Äì11835, 2019. 2, 14\n[39] S. Peng, S. Zhang, Z. Xu, C. Geng, B. Jiang, H. Bao, and X. Zhou,\n‚ÄúAnimatable neural implicit surfaces for creating avatars from\nvideos,‚Äù arXiv: Comp. Res. Repository, p. 2203.08133, 2022. 3\n[40] J. Huang, Y. Zhou, T. Funkhouser, and L. J. Guibas, ‚ÄúFramenet:\nLearning local canonical frames of 3d surfaces from a single rgb\nimage,‚Äù in Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 8638‚Äì8647, 2019. 3, 9, 10\n[41] X. Qi, R. Liao, Z. Liu, R. Urtasun, and J. Jia, ‚ÄúGeonet: Geometric\nneural network for joint depth and surface normal estimation,‚Äù\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 283‚Äì291, 2018. 3, 5, 8, 9\n[42] G. Bae, I. Budvytis, and R. Cipolla, ‚ÄúIrondepth: Iterative re-\nfinement of single-view depth using surface normal and its\nuncertainty,‚Äù arXiv preprint arXiv:2210.03676, 2022. 3, 5, 7, 9, 10\n[43] J. Park, K. Joo, Z. Hu, C.-K. Liu, and I. So Kweon, ‚ÄúNon-local\nspatial propagation network for depth completion,‚Äù in Computer\nVision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August\n23‚Äì28, 2020, Proceedings, Part XIII 16, pp. 120‚Äì136, Springer, 2020.\n3, 5\n[44] S. Shao, Z. Pei, X. Wu, Z. Liu, W. Chen, and Z. Li, ‚ÄúIebins:\nIterative elastic bins for monocular depth estimation,‚Äù arXiv\npreprint arXiv:2309.14137, 2023. 3, 5, 7, 9\n[45] L. Lipson, Z. Teed, and J. Deng, ‚ÄúRaft-stereo: Multilevel recurrent\nfield transforms for stereo matching,‚Äù in Int. Conf. 3D. Vis., 2021.\n3, 5, 7\n[46] Z. Teed and J. Deng, ‚ÄúRaft: Recurrent all-pairs field transforms\nfor optical flow,‚Äù in Computer Vision‚ÄìECCV 2020: 16th European\nConference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part II\n16, pp. 402‚Äì419, Springer, 2020. 3, 5, 7\n[47] L. Sun, W. Yin, E. Xie, Z. Li, C. Sun, and C. Shen, ‚ÄúImproving\nmonocular visual odometry using learned depth,‚Äù IEEE Transac-\ntions on Robotics, vol. 38, no. 5, pp. 3173‚Äì3186, 2022. 4\n[48] S. Im, H.-G. Jeon, S. Lin, and I.-S. Kweon, ‚ÄúDpsnet: End-to-end\ndeep plane sweep stereo,‚Äù inProc. Int. Conf. Learn. Representations,\n2019. 4, 12, 13\n[49] R. Mur-Artal and J. D. Tard ¬¥os, ‚ÄúOrb-slam2: An open-source\nslam system for monocular, stereo, and rgb-d cameras,‚Äù IEEE\ntransactions on robotics, vol. 33, no. 5, pp. 1255‚Äì1262, 2017. 4\n[50] R. Zhu, X. Yang, Y. Hold-Geoffroy, F. Perazzi, J. Eisenmann,\nK. Sunkavalli, and M. Chandraker, ‚ÄúSingle view metrology in\nthe wild,‚Äù in Proc. Eur. Conf. Comp. Vis. , pp. 316‚Äì333, Springer,\n2020. 4\n[51] J. T. Barron and J. Malik, ‚ÄúShape, illumination, and reflectance\nfrom shading,‚Äù IEEE Trans. Pattern Anal. Mach. Intell. , vol. 37,\nno. 8, pp. 1670‚Äì1687, 2014. 4\n17\n[52] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang,\n‚ÄúPixel2mesh: Generating 3d mesh models from single RGB im-\nages,‚Äù in Proc. Eur. Conf. Comp. Vis., pp. 52‚Äì67, 2018. 4\n[53] J. Wu, C. Zhang, X. Zhang, Z. Zhang, W. Freeman, and J. Tenen-\nbaum, ‚ÄúLearning shape priors for single-view 3d completion and\nreconstruction,‚Äù in Proc. Eur. Conf. Comp. Vis., pp. 646‚Äì662, 2018.\n4\n[54] S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, and\nH. Li, ‚ÄúPifu: Pixel-aligned implicit function for high-resolution\nclothed human digitization,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., pp. 2304‚Äì2314, 2019. 4\n[55] S. Saito, T. Simon, J. Saragih, and H. Joo, ‚ÄúPifuhd: Multi-level\npixel-aligned implicit function for high-resolution 3d human\ndigitization,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn. , pp. 84‚Äì\n93, 2020. 4\n[56] A. Saxena, M. Sun, and A. Y. Ng, ‚ÄúMake3d: Learning 3d scene\nstructure from a single still image,‚Äù IEEE Trans. Pattern Anal.\nMach. Intell., vol. 31, no. 5, pp. 824‚Äì840, 2008. 4\n[57] C. Zhang, W. Yin, G. Yu, Z. Wang, T. Chen, B. Fu, J. T. Zhou, and\nC. Shen, ‚ÄúRobust geometry-preserving depth estimation using\ndifferentiable rendering,‚Äù in Proc. IEEE Int. Conf. Comp. Vis., 2023.\n4\n[58] N. Silberman, D. Hoiem, P . Kohli, and R. Fergus, ‚ÄúIndoor seg-\nmentation and support inference from rgbd images,‚Äù in Proc. Eur.\nConf. Comp. Vis., pp. 746‚Äì760, Springer, 2012. 4, 5, 9, 10\n[59] A. Geiger, P . Lenz, C. Stiller, and R. Urtasun, ‚ÄúVision meets\nrobotics: The kitti dataset,‚Äù Int. J. Robot. Res., 2013. 4, 9, 10\n[60] D. Eigen, C. Puhrsch, and R. Fergus, ‚ÄúDepth map prediction\nfrom a single image using a multi-scale deep network,‚Äù in Proc.\nAdvances in Neural Inf. Process. Syst. , pp. 2366‚Äì2374, 2014. 4, 8\n[61] V . Guizilini, I. Vasiljevic, D. Chen, R. Ambrus , , and A. Gaidon,\n‚ÄúTowards zero-shot scale-aware monocular depth estimation,‚Äù in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 9233‚Äì9243, 2023. 5, 9, 10\n[62] L. Piccinelli, Y.-H. Yang, C. Sakaridis, M. Segu, S. Li, L. Van Gool,\nand F. Yu, ‚ÄúUnidepth: Universal monocular metric depth estima-\ntion,‚Äù in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 10106‚Äì10116, 2024. 5\n[63] K. Wang, F. Gao, and S. Shen, ‚ÄúReal-time scalable dense surfel\nmapping,‚Äù in 2019 International conference on robotics and automa-\ntion (ICRA), pp. 6919‚Äì6925, IEEE, 2019. 5\n[64] J. Wang, P . Wang, X. Long, C. Theobalt, T. Komura, L. Liu,\nand W. Wang, ‚ÄúNeuris: Neural reconstruction of indoor scenes\nusing normal priors,‚Äù in European Conference on Computer Vision ,\npp. 139‚Äì155, Springer, 2022. 5\n[65] X. Qi, Z. Liu, R. Liao, P . H. Torr, R. Urtasun, and J. Jia, ‚ÄúGeonet++:\nIterative geometric neural network with edge-aware refinement\nfor joint depth and surface normal estimation,‚Äù IEEE Transactions\non Pattern Analysis and Machine Intelligence, vol. 44, no. 2, pp. 969‚Äì\n984, 2020. 5, 9\n[66] X. Wang, D. Fouhey, and A. Gupta, ‚ÄúDesigning deep networks for\nsurface normal estimation,‚Äù in Proceedings of the IEEE conference\non computer vision and pattern recognition , pp. 539‚Äì547, 2015. 5, 9\n[67] D. Eigen and R. Fergus, ‚ÄúPredicting depth, surface normals\nand semantic labels with a common multi-scale convolutional\narchitecture,‚Äù in Proceedings of the IEEE international conference on\ncomputer vision, pp. 2650‚Äì2658, 2015. 5, 9\n[68] S. Liao, E. Gavves, and C. G. Snoek, ‚ÄúSpherical regression: Learn-\ning viewpoints, surface normals and 3d rotations on n-spheres,‚Äù\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 9759‚Äì9767, 2019. 5\n[69] L. Ladick `y, B. Zeisl, and M. Pollefeys, ‚ÄúDiscriminatively trained\ndense surface normal estimation,‚Äù in Computer Vision‚ÄìECCV\n2014: 13th European Conference, Zurich, Switzerland, September 6-\n12, 2014, Proceedings, Part V 13 , pp. 468‚Äì484, Springer, 2014. 5,\n9\n[70] B. Li, C. Shen, Y. Dai, A. Van Den Hengel, and M. He, ‚ÄúDepth\nand surface normal estimation from monocular images using\nregression on deep features and hierarchical crfs,‚Äù in Proceedings\nof the IEEE conference on computer vision and pattern recognition ,\npp. 1119‚Äì1127, 2015. 5\n[71] X. Long, Y. Zheng, Y. Zheng, B. Tian, C. Lin, L. Liu, H. Zhao,\nG. Zhou, and W. Wang, ‚ÄúAdaptive surface normal constraint\nfor geometric estimation from monocular images,‚Äù arXiv preprint\narXiv:2402.05869, 2024. 5\n[72] X. Long, C. Lin, L. Liu, W. Li, C. Theobalt, R. Yang, and W. Wang,\n‚ÄúAdaptive surface normal constraint for depth estimation,‚Äù in\nProceedings of the IEEE/CVF international conference on computer\nvision, pp. 12849‚Äì12858, 2021. 5\n[73] W. Chen, D. Xiang, and J. Deng, ‚ÄúSurface normals in the wild,‚Äù in\nProceedings of the IEEE International Conference on Computer Vision,\npp. 1557‚Äì1566, 2017. 5\n[74] G. Bae and A. J. Davison, ‚ÄúRethinking inductive biases for surface\nnormal estimation,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn. ,\n2024. 5\n[75] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, ‚ÄúPwc-net: Cnns for\noptical flow using pyramid, warping, and cost volume,‚Äù in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 8934‚Äì8943, 2018. 5\n[76] H. Liu, T. Lu, Y. Xu, J. Liu, and L. Wang, ‚ÄúLearning optical flow\nand scene flow with bidirectional camera-lidar fusion,‚Äù arXiv\npreprint arXiv:2303.12017, 2023. 5\n[77] X. Cheng, P . Wang, and R. Yang, ‚ÄúDepth estimation via affinity\nlearned with convolutional spatial propagation network,‚Äù in\nProceedings of the European conference on computer vision (ECCV) ,\npp. 103‚Äì119, 2018. 5\n[78] M. Hu, S. Wang, B. Li, S. Ning, L. Fan, and X. Gong, ‚ÄúPenet:\nTowards precise and efficient image guided depth completion,‚Äù\nin 2021 IEEE International Conference on Robotics and Automation\n(ICRA), pp. 13656‚Äì13662, IEEE, 2021. 5\n[79] G. Xu, X. Wang, X. Ding, and X. Yang, ‚ÄúIterative geometry encod-\ning volume for stereo matching,‚Äù in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 21919‚Äì\n21928, 2023. 5\n[80] J. E. Lenssen, C. Osendorfer, and J. Masci, ‚ÄúDeep iterative surface\nnormal estimation,‚Äù in Proceedings of the ieee/cvf conference on\ncomputer vision and pattern recognition, pp. 11247‚Äì11256, 2020. 5\n[81] W. Zhao, S. Liu, Y. Wei, H. Guo, and Y.-J. Liu, ‚ÄúA confidence-\nbased iterative solver of depths and surface normals for deep\nmulti-view stereo,‚Äù in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 6168‚Äì6177, 2021. 5, 15\n[82] W. Yin, Y. Liu, C. Shen, A. v. d. Hengel, and B. Sun, ‚ÄúThe devil\nis in the labels: Semantic segmentation from sentences,‚Äù arXiv:\nComp. Res. Repository, p. 2202.02002, 2022. 5\n[83] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-\nwal, G. Sastry, A. Askell, P . Mishkin, J. Clark, et al. , ‚ÄúLearning\ntransferable visual models from natural language supervision,‚Äù\nin Proc. Int. Conf. Mach. Learn., pp. 8748‚Äì8763, PMLR, 2021. 5\n[84] J. Lambert, Z. Liu, O. Sener, J. Hays, and V . Koltun, ‚ÄúMseg: A\ncomposite dataset for multi-domain semantic segmentation,‚Äù in\nProc. IEEE Conf. Comp. Vis. Patt. Recogn., pp. 2879‚Äì2888, 2020. 5\n[85] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec,\nV . Khalidov, P . Fernandez, D. Haziza, F. Massa, A. El-Nouby,et al.,\n‚ÄúDinov2: Learning robust visual features without supervision,‚Äù\narXiv preprint arXiv:2304.07193, 2023. 5, 8\n[86] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‚ÄúAn image is worth 16x16 words:\nTransformers for image recognition at scale,‚Äù Proc. Int. Conf.\nLearn. Representations, 2021. 5, 8, 10\n[87] R. Rombach, A. Blattmann, D. Lorenz, P . Esser, and B. Ommer,\n‚ÄúHigh-resolution image synthesis with latent diffusion models,‚Äù\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 10684‚Äì10695, 2022. 5, 10\n[88] Y. Hold-Geoffroy, K. Sunkavalli, J. Eisenmann, M. Fisher, E. Gam-\nbaretto, S. Hadap, and J.-F. Lalonde, ‚ÄúA perceptual measure for\ndeep single image camera calibration,‚Äù in Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., pp. 2354‚Äì2363, 2018. 5\n[89] M. Lopez-Antequera, P . Gargallo, M. Hofinger, S. R. Bul `o,\nY. Kuang, and P . Kontschieder, ‚ÄúMapillary planet-scale depth\ndataset,‚Äù in Proc. Eur. Conf. Comp. Vis. , vol. 12347, pp. 589‚Äì604,\n2020. 7, 10\n[90] D. Singh and B. Singh, ‚ÄúInvestigating the impact of data normal-\nization on classification performance,‚Äù Applied Soft Computing ,\n2019. 8\n[91] J. Li, R. Klein, and A. Yao, ‚ÄúA two-streamed network for estimat-\ning fine-scaled depth maps from single rgb images,‚Äù inProc. IEEE\nConf. Comp. Vis. Patt. Recogn., pp. 3372‚Äì3380, 2017. 9\n[92] I. Laina, C. Rupprecht, V . Belagiannis, F. Tombari, and N. Navab,\n‚ÄúDeeper depth prediction with fully convolutional residual net-\nworks,‚Äù in 2016 Fourth international conference on 3D vision (3DV) ,\npp. 239‚Äì248, IEEE, 2016. 9\n18\n[93] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, ‚ÄúDepth\nanything: Unleashing the power of large-scale unlabeled data,‚Äù\narXiv:2401.10891, 2024. 9\n[94] X. Guo, H. Li, S. Yi, J. Ren, and X. Wang, ‚ÄúLearning monocular\ndepth by distilling cross-domain stereo networks,‚Äù in Proc. Eur.\nConf. Comp. Vis., pp. 484‚Äì500, 2018. 9\n[95] D. F. Fouhey, A. Gupta, and M. Hebert, ‚ÄúUnfolding an indoor\norigami world,‚Äù in Computer Vision‚ÄìECCV 2014: 13th European\nConference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart VI 13, pp. 687‚Äì702, Springer, 2014. 9\n[96] A. Bansal, B. Russell, and A. Gupta, ‚ÄúMarr revisited: 2d-3d\nalignment via surface normal prediction,‚Äù in Proceedings of the\nIEEE conference on computer vision and pattern recognition, pp. 5965‚Äì\n5974, 2016. 9\n[97] P . Wang, X. Shen, B. Russell, S. Cohen, B. Price, and A. L. Yuille,\n‚ÄúSurge: Surface regularized geometry estimation from a single\nimage,‚Äù Advances in Neural Information Processing Systems, vol. 29,\n2016. 9\n[98] Z. Zhang, Z. Cui, C. Xu, Y. Yan, N. Sebe, and J. Yang, ‚ÄúPattern-\naffinitive propagation across depth, surface normal and semantic\nsegmentation,‚Äù in Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pp. 4106‚Äì4115, 2019. 9\n[99] R. Wang, D. Geraghty, K. Matzen, R. Szeliski, and J.-M. Frahm,\n‚ÄúVplnet: Deep single view normal estimation with vanishing\npoints and lines,‚Äù in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 689‚Äì698, 2020. 9\n[100] J. H. Lee, M.-K. Han, D. W. Ko, and I. H. Suh, ‚ÄúFrom big to\nsmall: Multi-scale local planar guidance for monocular depth\nestimation,‚Äù arXiv: Comp. Res. Repository, p. 1907.10326, 2019. 9\n[101] O. F. Kar, T. Yeo, A. Atanov, and A. Zamir, ‚Äú3d common cor-\nruptions and data augmentation,‚Äù in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pp. 18963‚Äì\n18974, 2022. 9\n[102] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie,\n‚ÄúA convnet for the 2020s,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., pp. 11976‚Äì11986, 2022. 8, 10\n[103] T. Darcet, M. Oquab, J. Mairal, and P . Bojanowski, ‚ÄúVision trans-\nformers need registers,‚Äù arXiv preprint arXiv:2309.16588, 2023. 8\n[104] S. Xie, R. Girshick, P . Doll ¬¥ar, Z. Tu, and K. He, ‚ÄúAggregated\nresidual transformations for deep neural networks,‚Äù inProc. IEEE\nConf. Comp. Vis. Patt. Recogn., pp. 1492‚Äì1500, 2017. 10\n[105] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, ‚ÄúScaling\nvision transformers,‚Äù in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 12104‚Äì12113, 2022.\n10\n[106] A. Geiger, P . Lenz, and R. Urtasun, ‚ÄúAre we ready for au-\ntonomous driving? the kitti vision benchmark suite,‚Äù in Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn. , pp. 3354‚Äì3361, IEEE, 2012.\n9\n[107] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,\nand M. Nie√üner, ‚ÄúScannet: Richly-annotated 3d reconstructions\nof indoor scenes,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn. ,\npp. 5828‚Äì5839, 2017. 9, 10\n[108] H. Caesar, V . Bankiti, A. H. Lang, S. Vora, V . E. Liong, Q. Xu,\nA. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, ‚Äúnuscenes: A\nmultimodal dataset for autonomous driving,‚Äù in Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., pp. 11621‚Äì11631, 2020. 9, 10\n[109] T. Koch, L. Liebel, F. Fraundorfer, and M. Korner, ‚ÄúEvaluation of\ncnn-based single-image depth estimation methods,‚Äù in Eur. Conf.\nComput. Vis. Worksh., pp. 0‚Äì0, 2018. 9, 10\n[110] I. Vasiljevic, N. Kolkin, S. Zhang, R. Luo, H. Wang, F. Z. Dai,\nA. F. Daniele, M. Mostajabi, S. Basart, M. R. Walter, et al., ‚ÄúDiode:\nA dense indoor and outdoor depth dataset,‚Äù arXiv: Comp. Res.\nRepository, p. 1908.00463, 2019. 9, 10\n[111] J. L. Sch ¬®onberger, E. Zheng, M. Pollefeys, and J.-M. Frahm,\n‚ÄúPixelwise view selection for unstructured multi-view stereo,‚Äù\nin Proc. Eur. Conf. Comp. Vis., 2016. 9\n[112] A. Knapitsch, J. Park, Q.-Y. Zhou, and V . Koltun, ‚ÄúTanks and\ntemples: Benchmarking large-scale scene reconstruction,‚Äù ACM\nTrans. Graph., vol. 36, no. 4, pp. 1‚Äì13, 2017. 9\n[113] S. Li, X. Wu, Y. Cao, and H. Zha, ‚ÄúGeneralizing to the open\nworld: Deep visual odometry with online adaptation,‚Äù in Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn., pp. 13184‚Äì13193, 2021. 9\n[114] V . Guizilini, R. Ambrus, S. Pillai, A. Raventos, and A. Gaidon,\n‚Äú3d packing for self-supervised monocular depth estimation,‚Äù in\nProc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020. 10\n[115] R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni,\nA. Ferreira, M. Yuan, B. Low, A. Jain, P . Ondruska, S. Omari,\nS. Shah, A. Kulkarni, A. Kazakova, C. Tao, L. Platinsky, W. Jiang,\nand V . Shet, ‚ÄúLevel 5 perception dataset 2020.‚Äù https://level-5.\nglobal/level5/data/, 2019. 10\n[116] G. Yang, X. Song, C. Huang, Z. Deng, J. Shi, and B. Zhou,\n‚ÄúDrivingstereo: A large-scale dataset for stereo matching in au-\ntonomous driving scenarios,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., 2019. 10\n[117] J. Cho, D. Min, Y. Kim, and K. Sohn, ‚ÄúDIML/CVL RGB-D dataset:\n2m RGB-D images of natural indoor and outdoor scenes,‚Äù arXiv:\nComp. Res. Repository, 2021. 10\n[118] B. Wilson, W. Qi, T. Agarwal, J. Lambert, J. Singh, S. Khandelwal,\nB. Pan, R. Kumar, A. Hartnett, J. K. Pontes, D. Ramanan, P . Carr,\nand J. Hays, ‚ÄúArgoverse 2: Next generation datasets for self-\ndriving perception and forecasting,‚Äù in Proc. Advances in Neural\nInf. Process. Syst., 2021. 10\n[119] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\nR. Benenson, U. Franke, S. Roth, and B. Schiele, ‚ÄúThe cityscapes\ndataset for semantic urban scene understanding,‚Äù in Proc. IEEE\nConf. Comp. Vis. Patt. Recogn., 2016. 10\n[120] M. Gehrig, W. Aarents, D. Gehrig, and D. Scaramuzza, ‚ÄúDsec: A\nstereo event camera dataset for driving scenarios,‚Äù IEEE Robotics\nand Automation Letters, 2021. 10\n[121] P . Xiao, Z. Shao, S. Hao, Z. Zhang, X. Chai, J. Jiao, Z. Li, J. Wu,\nK. Sun, K. Jiang, Y. Wang, and D. Yang, ‚ÄúPandaset: Advanced\nsensor suite dataset for autonomous driving,‚Äù in IEEE Int. Intelli-\ngent Transportation Systems Conf., 2021. 10\n[122] Z. Bauer, F. Gomez-Donoso, E. Cruz, S. Orts-Escolano, and\nM. Cazorla, ‚ÄúUasol, a large-scale high-resolution outdoor stereo\ndataset,‚Äù Scientific data, vol. 6, no. 1, pp. 1‚Äì14, 2019. 10\n[123] Y. Cabon, N. Murray, and M. Humenberger, ‚ÄúVirtual kitti 2,‚Äù\narXiv preprint arXiv:2001.10773, 2020. 10\n[124] P . Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V . Patnaik,\nP . Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, et al. , ‚ÄúScalability\nin perception for autonomous driving: Waymo open dataset,‚Äù\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 2446‚Äì2454, 2020. 10\n[125] A. Zamir, A. Sax, , W. Shen, L. Guibas, J. Malik, and S. Savarese,\n‚ÄúTaskonomy: Disentangling task transfer learning,‚Äù in Proc. IEEE\nConf. Comp. Vis. Patt. Recogn., IEEE, 2018. 10\n[126] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. En-\ngel, R. Mur-Artal, C. Ren, S. Verma, et al., ‚ÄúThe replica dataset: A\ndigital replica of indoor spaces,‚Äù arXiv preprint arXiv:1906.05797,\n2019. 10\n[127] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets,\nA. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury,\nA. X. Chang, et al., ‚ÄúHabitat-matterport 3d dataset (hm3d): 1000\nlarge-scale 3d environments for embodied ai,‚Äù arXiv preprint\narXiv:2109.08238, 2021. 10\n[128] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista,\nN. Paczan, R. Webb, and J. M. Susskind, ‚ÄúHypersim: A photore-\nalistic synthetic dataset for holistic indoor scene understanding,‚Äù\nin Proceedings of the IEEE/CVF international conference on computer\nvision, pp. 10912‚Äì10922, 2021. 10\n[129] T. Schops, J. L. Schonberger, S. Galliani, T. Sattler, K. Schindler,\nM. Pollefeys, and A. Geiger, ‚ÄúA multi-view stereo benchmark\nwith high-resolution images and multi-camera videos,‚Äù in Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn., pp. 3260‚Äì3269, 2017. 10\n[130] J. Kopf, X. Rong, and J.-B. Huang, ‚ÄúRobust consistent video depth\nestimation,‚Äù in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2021. 12,\n13\n[131] J.-W. Bian, H. Zhan, N. Wang, T.-J. Chin, C. Shen, and I. Reid,\n‚ÄúAuto-rectify network for unsupervised indoor depth estima-\ntion,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., 2021. 12, 13\n[132] M. Sayed, J. Gibson, J. Watson, V . Prisacariu, M. Firman, and\nC. Godard, ‚ÄúSimplerecon: 3d reconstruction without 3d convo-\nlutions,‚Äù in European Conference on Computer Vision , pp. 1‚Äì19,\nSpringer, 2022. 12, 13\n[133] Z. Yin and J. Shi, ‚ÄúGeonet: Unsupervised learning of dense\ndepth, optical flow and camera pose,‚Äù in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 1983‚Äì1992,\n2018. 13\n[134] S. Song, M. Chandraker, and C. C. Guest, ‚ÄúHigh accuracy monoc-\nular sfm and scale correction for autonomous driving,‚Äù IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 38, no. 4, pp. 730‚Äì743, 2015.\n13\nSupplementary Materials for Metric3D v2: A Versatile Monocular\nGeometric Foundation Model for Zero-shot Metric Depth and\nSurface Normal Estimation\nJanuary 6, 2025\n1 Details for Models\nDetails for ConvNet models. In our work, our en-\ncoder employs the ConvNext [1] networks, whose pre-\ntrained weight is from the official released ImageNet-\n22k pretraining. The decoder follows the adabins [2].\nWe set the depth bins number to 256, and the depth\nrange is [0.3m,150m]. We establish 4 flip connections\nfrom different levels of encoder blocks to the decoder\nto merge more low-level features. An hourglass sub-\nnetwork is attached to the head of the decoders to\nenhance background predictions.\nDetails for ViT models. We use dino-v2 trans-\nformers [3] with registers [4] as our encoders, which\nare pretrained on a curated dataset with 142M im-\nages. DPT [5] is used as the decoders. For the ViT-S\nand ViT-L variants, the DPT decoders take only the\nlast-layer normalized encoder features as the input for\nstabilized training. The giant ViT-g model instead\ntakes varying-layer features, the same as the original\nDPT settings. Different from the convnets models\nabove, we use depth bins ranging from [0 .1m,200m]\nfor ViT models.\nDetails for recurrent blocks. As illustrated in\nFig 1. Each recurrent block updates hierarchical fea-\ntures maps {Ht\n1/14,Ht\n1/7,Ht\n1/4}at {1\n14 ,1\n7 ,1\n4 }scales\nand the intermediate predictions s ÀÜDt\nc, ÀÜNt at each it-\neration step t. This block compromises three Con-\nvGRU sub-blocks to refine feature maps at different\nscales, and two projection heads Gd and Gn to pre-\ndict updates for depth and normal respectively. The\nfeature maps are gradually refined from the coarsest\n(Ht\n1/14) to the finest (Ht\n1/4). For instance, the refined\nfeature map at the 1\n14 scale Ht+1\n1/14 is fed into the sec-\nond ConvGRU sub-block to refine the 1\n7 -scale feature\nmap Ht\n1/7. Finally, the projection heads Gd,Gn em-\nploys a concatenation of original predictions ÀÜDt\nc, ÀÜNt\nand the to finest feature map Ht\n1/4 to predict the up-\ndate items ‚àÜ ÀÜDt+1\nc , ‚àÜ ÀÜNt+1. Both projection heads\nare composed of two linear layers with a sandwiched\nReLU activation layer.\nResource comparison of different models. We\ncompare the resource and performance among our\nmodel families in Tab. 1. All inference-time and GPU\nmemory results are computed on an Nvida-A100 40G\nGPU with the original pytorch implemented mod-\nels (No engineering optimization like TensorRT or\nC-GRU\nC-GRU\nC-GRU\nc\nc\nùí¢ùí¢ùëëùëë\nùí¢ùí¢ùëõùëõ\nÔøΩùêÉùêÉùëêùëêùë°ùë°\nc\nÔøΩùêçùêçùë°ùë°\nc\nùêáùêá1/14\nùë°ùë°+1\nùêáùêá1/7\nùë°ùë°+1\nùêáùêá1/4\nùë°ùë°+1\n‚àÜÔøΩùêÉùêÉùëêùëê\nùë°ùë°+1\n‚àÜÔøΩùêçùêçùë°ùë°+1\nc\nc\ncc\nùêáùêá1/14\nùë°ùë°\nùêáùêá1/7\nùë°ùë°\nùêáùêá1/4\nùë°ùë°\ndown\nup\ndown\nup\nùêáùêáùë†ùë†ùë°ùë° Hidden feature map\nÔøΩùêÉùêÉùëêùëê\nùë°ùë°\nÔøΩùêçùêçùë°ùë° Intermediate predictions\nup  down Up/downsample\ncc Concatenate\nC-GRU ConvGRU\nLinear\nReLU\nLinear\nLinear\nReLU\nLinear\nFigure 1: Detailed structure of the update\nblock. Inspired by RAFTStereo [6], we build slow-\nfast ConvGRU sub-blocks (denoted as ‚ÄòC-GRU‚Äô) to\nrefine hierarchical hidden feature maps. Projection\nheads Gd,Gn are attached to the end of the final Con-\nvGRU to prediction update items for the predictions.\n1\narXiv:2404.15506v4  [cs.CV]  3 Jan 2025\nONNX). Generally, the enormous ViT-Large/giant-\nbackbone models enjoy better performance, while the\nothers are more deployment-friendly. In addition,\nour models built in classical en-decoder schemes run\nmuch faster than the recent diffusion counterpart [7].\n1.1 Datasets and Training and Testing\nWe collect over 16M data from 18 public datasets\nfor training. Datasets are listed in Tab. 2.\nWhen training the ConvNeXt-backbone models,\nwe use a smaller collection containing the fol-\nlowing 11 datasets with 8M images: DDAD [8],\nLyft [9], DrivingStereo [10], DIML [11], Argov-\nerse2 [12], Cityscapes [13], DSEC [14], Maplillary\nPSD [15], Pandaset [16], UASOL[17], and Taskon-\nomy [20]. In the autonomous driving datasets, in-\ncluding DDAD [8], Lyft [9], DrivingStereo [10], Ar-\ngoverse2 [12], DSEC [14], and Pandaset [16], have\nprovided LiDar and camera intrinsic and extrin-\nsic parameters. We project the LiDar to image\nplanes to obtain ground-truth depths. In contrast,\nCityscapes [13], DIML [11], and UASOL [17] only\nprovide calibrated stereo images. We use raft-\nstereo [6] to achieve pseudo ground-truth depths.\nMapillary PSD [15] dataset provides paired RGB-D,\nbut the depth maps are achieved from a structure-\nfrom-motion method. The camera intrinsic parame-\nters are estimated from the SfM. We believe that such\nachieved metric information is noisy. Thus we do not\nenforce learning-metric-depth loss on this data, i.e.,\nLsilog, to reduce the effect of noises. For the Taskon-\nomy [20] dataset, we follow LeReS [31] to obtain the\ninstance planes, which are employed in the pair-wise\nnormal regression loss. During training, we employ\nthe training strategy from [32] to balance all datasets\nin each training batch.\nThe testing data is listed in Tab. 2. All of them\nare captured by high-quality sensors. In testing, we\nemploy their provided camera intrinsic parameters\nto perform our proposed canonical space transforma-\ntion.\n1.2 Details for Some Experiments\nFinetuning protocols. To finetune the large-scale-\ndata trained models on some specific datasets, we use\nthe ADAM optimizer with the initial learning rate\nbeginning at 10‚àí6 and linear decayed to 10‚àí7 within\n6K steps. Notably, such finetune does not require\na large batch size like large-scale training. We use\nin practice a batch-size of 16 for the ViT-g model\nand 32 for ViT-L. The models will converge quickly\nin approximately 2K steps. To stabilize finetuning,\nwe also leverage the predictions ÀúDc ÀúN of the pre-\nfinetuned model as alternative pseudo labels. These\nlabels can sufficiently impose supervision upon the\nannotation-absent regions. The complete loss can be\nformulated as:\nLft = 0.01Ld‚àín(Dc,N) + Ld(Dc,D‚àó\nc) + Ln(N,N‚àó)\n+0.01(Ld(Dc, ÀúDc) + Ln(N, ÀúN)),\n(1)\nwhere Dc and N are the predicted depth in the\ncanonical space and surface normal, D‚àó\nc and N‚àó are\nthe groundtruth labels, Ld, Ln, and Ld‚àín are the\nlosses for depth, normal, and depth-normal consis-\ntency introduced in the main text.\nEvaluation of zero-shot 3D scene reconstruc-\ntion. In this experiment, we use all methods‚Äô re-\nleased models to predict each frame‚Äôs depth and use\nthe ground-truth poses and camera intrinsic param-\neters to reconstruct point clouds. When evaluating\nthe reconstructed point cloud, we employ the iter-\native closest point (ICP) [33] algorithm to match\nthe predicted point clouds with ground truth by a\npose transformation matrix. Finally, we evaluate the\nChamfer ‚Ñì1 distance and F-score on the point cloud.\nReconstruction of in-the-wild scenes. We col-\nlect several photos from Flickr. From their associated\ncamera metadata, we can obtain the focal length ÀÜf\nand the pixel size Œ¥. According to ÀÜf/Œ¥, we can ob-\ntain the pixel-represented focal length for 3D recon-\nstruction and achieve the metric information. We use\nmeshlab software to measure some structures‚Äô size on\npoint clouds. More visual results are shown in Fig.\n7.\nGeneralization of metric depth estimation. To\nevaluate our method‚Äôs robustness of metric recovery,\n2\nTable 1: Comparative analysis of resource and performance across our model families includes evaluation of\nresource utilization metrics such as inference speed, memory usage, and the proportion of optimization mod-\nules. Additionally, we assess metric depth performance on KITTI/NYU datasets and normal performance\non NYUv2 dataset, with results derived from checkpoints without fine-tuning. For ViT models, inference\nspeed is measured using 16-bit precision (Bfloat16), which is the same precision as the training setup.\nModel Resource KITTI Depth NYUv2 DepthNYUv2 Normal\nEncoder Decoder Optim. Speed GPU Memory Optim. timeAbsRel‚Üì Œ¥1‚Üë AbsRel‚Üì Œ¥1‚Üë Median‚Üì30‚ó¶‚Üë\nMarigold[7] VAE+U-net U-net+VAE -0.13 fps 17.3G - No metric No metricNo metric No metric- -\nOurs ConvNeXt-Large Hourglass -10.5 fps 4.2G - 0.053 0.965 0.083 0.944 - -\nOurs ViT-Small DPT 4 steps11.6 fps 2.9G 3.4% 0.070 0.937 0.084 0.945 7.7 0.870\nOurs ViT-Large DPT 8 steps9.5 fps 7.0G 9.5% 0.052 0.974 0.063 0.975 7.0 0.881\nOurs ViT-giant DPT 8 steps5.0 fps 15.6G 25% 0.051 0.977 0.067 0.980 7.1 0.881\nwe test on 7 zero-shot datasets, i.e. NYU, KITTI,\nDIODE (indoor and outdoor parts), ETH3D, iBims-\n1, and NuScenes. Details are reported in Tab. 2. We\nuse the officially provided focal length to predict the\nmetric depths. All benchmarks use the same depth\nmodel for evaluation. We don‚Äôt perform any scale\nalignment.\nEvaluation on affine-invariant depth bench-\nmarks. We follow existing affine-invariant depth es-\ntimation methods to evaluate 5 zero-shot datasets.\nBefore evaluation, we employ the least square fitting\nto align the scale and shift with ground truth [34].\nPrevious methods‚Äô performance is cited from their\npapers.\nDense-SLAM Mapping. This experiment is con-\nducted on the KITTI odometry benchmark. We use\nTable 2: Training and testing datasets used for ex-\nperiments.\nDatasets ScenesSource Label Size #Cam.Training DataDDAD [8] OutdoorReal-worldDepth ‚àº80K 36+Lyft [9] OutdoorReal-worldDepth ‚àº50K 6+Driving Stereo (DS) [10]OutdoorReal-worldDepth ‚àº181K1DIML [11] OutdoorReal-worldDepth ‚àº122K10Arogoverse2 [12]OutdoorReal-worldDepth ‚àº3515K6+Cityscapes [13]OutdoorReal-worldDepth ‚àº170K1DSEC [14] OutdoorReal-worldDepth ‚àº26K 1Mapillary PSD [15]OutdoorReal-worldDepth 750K1000+Pandaset [16]OutdoorReal-worldDepth ‚àº48K 6UASOL [17] OutdoorReal-worldDepth ‚àº1370K1Virtual KITTI [18]OutdoorSynthesizedDepth 37K 2Waymo [19] OutdoorReal-worldDepth ‚àº1M 5Matterport3d [20]In/OutReal-worldDepth + Normal144K3Taskonomy [20]IndoorReal-worldDepth + Normal‚àº4M ‚àº1MReplica [21] IndoorReal-worldDepth + Normal‚àº150K1ScanNet‚Ä†[22] IndoorReal-worldDepth + Normal‚àº2.5M1HM3d [23] IndoorReal-worldDepth + Normal‚àº2000K1Hypersim [24]IndoorSynthesizedDepth + Normal54K 1Testing DataNYU [25] IndoorReal-worldDepth+Normal654 1KITTI [26] OutdoorReal-worldDepth 652 4ScanNet‚Ä†[22] IndoorReal-worldDepth+Normal700 1NuScenes (NS) [27]OutdoorReal-worldDepth 10K 6ETH3D [28] OutdoorReal-worldDepth 431 1DIODE [29] In/OutReal-worldDepth 771 1iBims-1 [30] IndoorReal-worldDepth 100 1\n‚Ä† ScanNet is a non-zero-shot testing dataset for our ViT models.\nour model to predict metric depths, and then naively\ninput them to the Droid-SLAM system as an ini-\ntial depth. We do not perform any finetuning but\ndirectly run their released codes on KITTI. With\nDroid-SLAM predicted poses, we unproject depths to\nthe 3D point clouds and fuse them together to achieve\ndense metric mapping. More qualitative results are\nshown in Fig. 6.\nAbsrel (%)\nCanonical focal length (pixels)\n19.5\n20.5\n21.5\n22.5\n23.5\n24.5\n25.5\n26.5\n27.5\n250 500 1000 1500 2000\nEffect of Canonical Camera Focal Length\nFigure 2: Effect of varying canonical focal\nlengths. We apply different canonical focal lengths\nand find that an intermediate focal length leads to\nthe best performance.\nAblation on canonical space. To study the ef-\nfect of different focal lengths in the canonical space,\nwe train the ConvNext models on the small subset\nof varying datasets and test their performance on\nthe validation set. We compare the absrel error us-\ning {250, 500, 1000, 1500, and 2000 }-pixel canonical\ncamera focal lengths. As shown in Fig. 2, the canon-\nical focal length of 1000 achieves the lowest depth er-\nror. We apply this setting for all other experiments.\n3\n1.3 More Visual Results\nQualitative comparison of depth and normal\nestimation. In Figs 3, 4, we compare visualized\ndepth and normal maps from the Vit-g CSTM label\nmodel with ZoeDepth [35], Bae etal [36], and Om-\nnidata [37]. In Figs. 5, 10, 11, and 12, We show the\nqualitative comparison of our depth maps from the\nConvNeXt-L CSTM label model with Adabins [2],\nNewCRFs [38], and Omnidata [37]. Our results have\nmuch fine-grained details and less artifacts.\nVisualization of iterative refinement. To com-\nprehensively understand the usage of iterative re-\nfinement modules, we visualize the predictions be-\nfore/after optimization and the updates for different\nsteps in Fig. 8. Here we use our publicly released 4-\nstep ViT-small model for visualization. Initially, the\nnetwork produces a coarse prediction. The first step\nupdates the most drastically, while sub-sequential\nsteps focus mainly on object boundaries. Finally, the\nrefined predictions have clearer shapes and sharper\nedges.\nReconstructing 360 ‚ó¶ NuScenes scenes. Cur-\nrent autonomous driving cars are equipped with sev-\neral pin-hole cameras to capture 360 ‚ó¶ views. Cap-\nturing the surround-view depth is important for au-\ntonomous driving. We sampled some scenes from the\ntesting data of NuScenes. With our depth model,\nwe can obtain the metric depths for 6-ring cameras.\nWith the provided camera intrinsic and extrinsic pa-\nrameters, we unproject the depths to the 3D point\ncloud and merge all views together. See Fig. 9 for de-\ntails. Note that 6-ring cameras have different camera\nintrinsic parameters. We can observe that all views‚Äô\npoint clouds can be fused together consistently.\nReferences\n[1] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Dar-\nrell, and S. Xie, ‚ÄúA convnet for the 2020s,‚Äù in Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn. , pp. 11976‚Äì\n11986, 2022.\n[2] S. F. Bhat, I. Alhashim, and P. Wonka, ‚ÄúAdabins:\nDepth estimation using adaptive bins,‚Äù in Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn. , pp. 4009‚Äì\n4018, 2021.\n[3] M. Oquab, T. Darcet, T. Moutakanni, H. Vo,\nM. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza,\nF. Massa, A. El-Nouby, et al. , ‚ÄúDinov2: Learning\nrobust visual features without supervision,‚Äù arXiv\npreprint arXiv:2304.07193, 2023.\n[4] T. Darcet, M. Oquab, J. Mairal, and P. Bojanowski,\n‚ÄúVision transformers need registers,‚Äù arXiv preprint\narXiv:2309.16588, 2023.\n[5] R. Ranftl, A. Bochkovskiy, and V. Koltun, ‚ÄúVision\ntransformers for dense prediction,‚Äù in Proc. IEEE\nInt. Conf. Comp. Vis. , pp. 12179‚Äì12188, 2021.\n[6] L. Lipson, Z. Teed, and J. Deng, ‚ÄúRaft-stereo: Multi-\nlevel recurrent field transforms for stereo matching,‚Äù\nin Int. Conf. 3D. Vis. , 2021.\n[7] B. Ke, A. Obukhov, S. Huang, N. Metzger, R. C.\nDaudt, and K. Schindler, ‚ÄúRepurposing diffusion-\nbased image generators for monocular depth estima-\ntion,‚Äù arXiv preprint arXiv:2312.02145 , 2023.\n[8] V. Guizilini, R. Ambrus, S. Pillai, A. Raventos, and\nA. Gaidon, ‚Äú3d packing for self-supervised monocu-\nlar depth estimation,‚Äù in Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., 2020.\n[9] R. Kesten, M. Usman, J. Houston, T. Pandya,\nK. Nadhamuni, A. Ferreira, M. Yuan, B. Low,\nA. Jain, P. Ondruska, S. Omari, S. Shah, A. Kulka-\nrni, A. Kazakova, C. Tao, L. Platinsky, W. Jiang,\nand V. Shet, ‚ÄúLevel 5 perception dataset 2020.‚Äù\nhttps://level-5.global/level5/data/, 2019.\n[10] G. Yang, X. Song, C. Huang, Z. Deng, J. Shi, and\nB. Zhou, ‚ÄúDrivingstereo: A large-scale dataset for\nstereo matching in autonomous driving scenarios,‚Äù\nin Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2019.\n[11] J. Cho, D. Min, Y. Kim, and K. Sohn, ‚ÄúDIML/CVL\nRGB-D dataset: 2m RGB-D images of natural in-\ndoor and outdoor scenes,‚Äù arXiv: Comp. Res. Repos-\nitory, 2021.\n[12] B. Wilson, W. Qi, T. Agarwal, J. Lambert, J. Singh,\nS. Khandelwal, B. Pan, R. Kumar, A. Hartnett, J. K.\nPontes, D. Ramanan, P. Carr, and J. Hays, ‚ÄúAr-\ngoverse 2: Next generation datasets for self-driving\nperception and forecasting,‚Äù in Proc. Advances in\nNeural Inf. Process. Syst., 2021.\n[13] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. En-\nzweiler, R. Benenson, U. Franke, S. Roth, and\nB. Schiele, ‚ÄúThe cityscapes dataset for semantic\nurban scene understanding,‚Äù in Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., 2016.\n4\nRGB GT Depth Ours Depth ZoeDepth Ours Normal Bae et al.\nGT Normal\nFigure 3: Depth and normal estimation. The visual comparison of predicted depth and normal on\nindoor/outdoor scenes from NYUv2, iBims, Eth3d, and ScanNet. Our depth and normal maps come from\nthe ViT-g CSTM label model.\n5\nRGB GT Depth Ours Depth ZoeDepth Ours Normal Bae et al.\nFigure 4: Depth and normal estimation. The visual comparison of predicted depth and normal on\ndriving scenes from KITTI, Nuscenes, DIML, DDAD, and Waymo. Our depth and normal maps come from\nthe ViT-g CSTM label model.\n6\nRGB  GT   Ours  NewCRFs Adabins Omnidata\nFigure 5: The visual comparison of predicted depth on iBims, ETH3D, and DIODE. Our depth maps come\nfrom the ConvNeXt-L CSTM label model.\n7\nGT \nTrajectory\nDroid-SLAMOursGT \nTrajectory GT \nTrajectory\nDroid-SLAMOurs\nGT \nTrajectory\nDroid-SLAMOurs\nGT \nTrajectory\nDroid-SLAMOurs\nFigure 6: Dense-SLAM Mapping. Existing SOTA mono-SLAM methods usually face scale drift problems\nin large-scale scenes and are unable to achieve the metric scale. We show the ground-truth trajectory and\nDroid-SLAM [39] predicted trajectory and their dense mapping. Then, we naively input our metric depth to\nDroid-SLAM, which can recover a much more accurate trajectory and perform the metric dense mapping.\n8\n15.7m\n6.5m5.6m\n16.8m\n4.9m\nPanasonic \nDMC-FS40\nFujifilm \nX-T10\n3.3m\n2.2m\n3.4m\n3.4m\n1.3m\nKodak \nC913\nRGB Point Cloud (view 1) Point Cloud (view 2)\n4.5m\n4.3m\nOlympus \nX450\nFigure 7: 3D metric reconstruction of in-the-wild images. We collect several Flickr images and use\nour model to reconstruct the scene. The focal length information is collected from the photo‚Äôs metadata.\nFrom the reconstructed point cloud, we can measure some structures‚Äô sizes. We can observe that sizes are\nin a reasonable range.\n9\n(b) Depth Updating\nRGB GT_Depth GT_Normal\n(a) Inputs and Labels\n(c) Normal Updating\nùê¥ùê¥ = ùúãùúãùëüùëü2\nInitial Depth ùë´ùë´0 Final Depth ùë´ùë´4\n‚àÜùë´ùë´0 ‚àÜùë´ùë´1 ‚àÜùë´ùë´2 ‚àÜùë´ùë´3\nInitial Normal ùëµùëµ0 Final Normal ùëµùëµ4\n‚àÜùëµùëµ0 ‚àÜùëµùëµ1 ‚àÜùëµùëµ2 ‚àÜùëµùëµ3\nFigure 8: Visualization for iterative optimization. We use our publicly available ViT-S model (with 4\nrefinement steps) to estimate zero-shot depth and normal maps. The initial and final predictions, as well as\ntheir sequential updating items, are presented in (b) and (c).\n10\nRing RGBs & Depth Point Cloud\nPoint Cloud of Car\nFigure 9: 3D reconstruction of 360‚ó¶ views. Current autonomous driving cars are equipped with several\npin-hole cameras to capture 360 ‚ó¶ views. With our model, we can reconstruct each view and smoothly fuse\nthem together. We can see that all views can be well merged together without scale inconsistency problems.\nTesting data are from NuScenes. Note that the front view camera has a different focal length from other\nviews.\n[14] M. Gehrig, W. Aarents, D. Gehrig, and D. Scara-\nmuzza, ‚ÄúDsec: A stereo event camera dataset for\ndriving scenarios,‚Äù IEEE Robotics and Automation\nLetters, 2021.\n[15] M. Lopez-Antequera, P. Gargallo, M. Hofinger, S. R.\nBul` o, Y. Kuang, and P. Kontschieder, ‚ÄúMapillary\nplanet-scale depth dataset,‚Äù in Proc. Eur. Conf.\nComp. Vis., vol. 12347, pp. 589‚Äì604, 2020.\n[16] P. Xiao, Z. Shao, S. Hao, Z. Zhang, X. Chai, J. Jiao,\nZ. Li, J. Wu, K. Sun, K. Jiang, Y. Wang, and\nD. Yang, ‚ÄúPandaset: Advanced sensor suite dataset\nfor autonomous driving,‚Äù in IEEE Int. Intelligent\nTransportation Systems Conf., 2021.\n[17] Z. Bauer, F. Gomez-Donoso, E. Cruz, S. Orts-\nEscolano, and M. Cazorla, ‚ÄúUasol, a large-scale high-\nresolution outdoor stereo dataset,‚Äù Scientific data ,\nvol. 6, no. 1, pp. 1‚Äì14, 2019.\n[18] Y. Cabon, N. Murray, and M. Humenberger, ‚ÄúVir-\ntual kitti 2,‚Äù arXiv preprint arXiv:2001.10773, 2020.\n[19] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard,\nV. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai,\nB. Caine, et al. , ‚ÄúScalability in perception for au-\ntonomous driving: Waymo open dataset,‚Äù in Pro-\nceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 2446‚Äì2454, 2020.\n[20] A. Zamir, A. Sax, , W. Shen, L. Guibas, J. Malik,\nand S. Savarese, ‚ÄúTaskonomy: Disentangling task\ntransfer learning,‚Äù in Proc. IEEE Conf. Comp. Vis.\nPatt. Recogn., IEEE, 2018.\n[21] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wi-\njmans, S. Green, J. J. Engel, R. Mur-Artal,\nC. Ren, S. Verma, et al. , ‚ÄúThe replica dataset:\nA digital replica of indoor spaces,‚Äù arXiv preprint\narXiv:1906.05797, 2019.\n[22] A. Dai, A. X. Chang, M. Savva, M. Halber,\nT. Funkhouser, and M. Nie√üner, ‚ÄúScannet: Richly-\nannotated 3d reconstructions of indoor scenes,‚Äù\nin Proc. IEEE Conf. Comp. Vis. Patt. Recogn. ,\npp. 5828‚Äì5839, 2017.\n[23] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans,\nO. Maksymets, A. Clegg, J. Turner, E. Under-\nsander, W. Galuba, A. Westbury, A. X. Chang,\net al., ‚ÄúHabitat-matterport 3d dataset (hm3d): 1000\nlarge-scale 3d environments for embodied ai,‚Äù arXiv\npreprint arXiv:2109.08238, 2021.\n[24] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar,\nM. A. Bautista, N. Paczan, R. Webb, and J. M.\nSusskind, ‚ÄúHypersim: A photorealistic synthetic\ndataset for holistic indoor scene understanding,‚Äù in\nProceedings of the IEEE/CVF international confer-\nence on computer vision , pp. 10912‚Äì10922, 2021.\n[25] N. Silberman, D. Hoiem, P. Kohli, and R. Fer-\ngus, ‚ÄúIndoor segmentation and support inference\nfrom rgbd images,‚Äù in Proc. Eur. Conf. Comp. Vis. ,\npp. 746‚Äì760, Springer, 2012.\n11\nRGB  GT   Ours  NewCRFs Adabins Omnidata\nFigure 10: Depth estimation. The visual comparison of predicted depth on iBims, ETH3D, and DIODE.\nOur depth maps come from the ConvNeXt-L CSTM label model.\n12\nRGB  GT   Ours  NewCRFs Adabins Omnidata\nFigure 11: Depth estimation. The visual comparison of predicted depth on iBims, ETH3D, and DIODE.\nOur depth maps come from the ConvNeXt-L CSTM label model.\n13\nRGB  GT   Ours  NewCRFs Adabins Omnidata\nFigure 12: Depth estimation. The visual comparison of predicted depth on iBims, ETH3D, and DIODE.\nOur depth maps come from the ConvNeXt-L CSTM label model.\n14\n[26] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun,\n‚ÄúVision meets robotics: The kitti dataset,‚Äù Int. J.\nRobot. Res., 2013.\n[27] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E.\nLiong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and\nO. Beijbom, ‚Äúnuscenes: A multimodal dataset for\nautonomous driving,‚Äù in Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., pp. 11621‚Äì11631, 2020.\n[28] T. Schops, J. L. Schonberger, S. Galliani, T. Sat-\ntler, K. Schindler, M. Pollefeys, and A. Geiger, ‚ÄúA\nmulti-view stereo benchmark with high-resolution\nimages and multi-camera videos,‚Äù in Proc. IEEE\nConf. Comp. Vis. Patt. Recogn. , pp. 3260‚Äì3269,\n2017.\n[29] I. Vasiljevic, N. Kolkin, S. Zhang, R. Luo, H. Wang,\nF. Z. Dai, A. F. Daniele, M. Mostajabi, S. Basart,\nM. R. Walter, et al. , ‚ÄúDiode: A dense indoor and\noutdoor depth dataset,‚Äù arXiv: Comp. Res. Reposi-\ntory, p. 1908.00463, 2019.\n[30] T. Koch, L. Liebel, F. Fraundorfer, and M. Korner,\n‚ÄúEvaluation of cnn-based single-image depth estima-\ntion methods,‚Äù in Eur. Conf. Comput. Vis. Worksh. ,\npp. 0‚Äì0, 2018.\n[31] W. Yin, J. Zhang, O. Wang, S. Niklaus, S. Chen,\nY. Liu, and C. Shen, ‚ÄúTowards accurate reconstruc-\ntion of 3d scene shape from a single monocular im-\nage,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., 2022.\n[32] W. Yin, X. Wang, C. Shen, Y. Liu, Z. Tian, S. Xu,\nC. Sun, and D. Renyin, ‚ÄúDiversedepth: Affine-\ninvariant depth prediction using diverse data,‚Äù\narXiv: Comp. Res. Repository , p. 2002.00569, 2020.\n[33] P. Besl and N. McKay, ‚ÄúMethod for registration of 3-\nd shapes,‚Äù in Sensor fusion IV: Control Paradigms\nand Data Structures , vol. 1611, pp. 586‚Äì606, Spie,\n1992.\n[34] W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai,\nS. Chen, and C. Shen, ‚ÄúLearning to recover 3d scene\nshape from a single image,‚Äù in Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., 2021.\n[35] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, and\nM. M¬® uller, ‚ÄúZoedepth: Zero-shot transfer by com-\nbining relative and metric depth,‚Äù arXiv preprint\narXiv:2302.12288, 2023.\n[36] G. Bae, I. Budvytis, and R. Cipolla, ‚ÄúEstimat-\ning and exploiting the aleatoric uncertainty in\nsurface normal estimation,‚Äù in Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, pp. 13137‚Äì13146, 2021.\n[37] A. Eftekhar, A. Sax, J. Malik, and A. Zamir, ‚ÄúOm-\nnidata: A scalable pipeline for making multi-task\nmid-level vision datasets from 3d scans,‚Äù in Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn. , pp. 10786‚Äì\n10796, 2021.\n[38] W. Yuan, X. Gu, Z. Dai, S. Zhu, and P. Tan, ‚ÄúNew\nCRFs: Neural window fully-connected CRFs for\nmonocular depth estimation,‚Äù in Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., 2022.\n[39] Z. Teed and J. Deng, ‚ÄúDroid-slam: Deep visual slam\nfor monocular, stereo, and rgb-d cameras,‚Äù vol. 34,\npp. 16558‚Äì16569, 2021.\n15"
}