{
  "title": "REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring",
  "url": "https://openalex.org/W4401072335",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4227560249",
      "name": "Thanh-Cong Ho",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4383783468",
      "name": "Farah Kharrat",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A3036287758",
      "name": "Abderrazek Abid",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2011220966",
      "name": "Fakhri Karray",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A290206851",
      "name": "Anis Koubaa",
      "affiliations": [
        "Prince Sultan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2956640625",
    "https://openalex.org/W4206314215",
    "https://openalex.org/W4313555286",
    "https://openalex.org/W6851658948",
    "https://openalex.org/W2973434860",
    "https://openalex.org/W3173752806",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W3080554130",
    "https://openalex.org/W3204375404",
    "https://openalex.org/W4383746185",
    "https://openalex.org/W6859273379",
    "https://openalex.org/W4383746801",
    "https://openalex.org/W4385322436",
    "https://openalex.org/W4390842513",
    "https://openalex.org/W4388070550",
    "https://openalex.org/W4395091069",
    "https://openalex.org/W6853515732",
    "https://openalex.org/W4399727635",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4389116614",
    "https://openalex.org/W4402671548",
    "https://openalex.org/W4366327625"
  ],
  "abstract": "With the widespread adoption of wearable devices in our daily lives, the demand and appeal for remote patient monitoring have significantly increased. Most research in this field has concentrated on collecting sensor data, visualizing it, and analyzing it to detect anomalies in specific diseases such as diabetes, heart disease and depression. However, this domain has a notable gap in the aspect of human-machine interaction. This paper proposes REMONI, an autonomous REmote health MONItoring system that integrates multimodal large language models (MLLMs), the Internet of Things (IoT), and wearable devices. The system automatically and continuously collects vital signs, accelerometer data from a special wearable (such as a smartwatch), and visual data in patient video clips collected from cameras. This data is processed by an anomaly detection module, which includes a fall detection model and algorithms to identify and alert caregivers of the patient's emergency conditions. A distinctive feature of our proposed system is the natural language processing component, developed with MLLMs capable of detecting and recognizing a patient's activity and emotion while responding to healthcare worker's inquiries. Additionally, prompt engineering is employed to integrate all patient information seamlessly. As a result, doctors and nurses can access real-time vital signs and the patient's current state and mood by interacting with an intelligent agent through a user-friendly web application. Our experiments demonstrate that our system is implementable and scalable for real-life scenarios, potentially reducing the workload of medical professionals and healthcare costs. A full-fledged prototype illustrating the functionalities of the system has been developed and being tested to demonstrate the robustness of its various capabilities.",
  "full_text": "REMONI: An Autonomous System Integrating\nWearables and Multimodal Large Language Models\nfor Enhanced Remote Health Monitoring\nThanh Cong Ho ∗, Farah Kharrat ∗, Abderrazek Abid ∗, Fakhri Karray ∗†\n∗Mohamed bin Zayed University of Artificial Intelligence\nMasdar City, Abu Dhabi, UAE\nEmail:{Thanh.Ho, Farah.Kharrat, Abid.Abderrazek, Fakhri.Karray}@mbzuai.ac.ae\n†Department of Electrical and Computer Engineering\nUniversity of Waterloo, Waterloo, ON, Canada N2L 3G1\nEmail: karray@uwaterloo.ca\n‡College of Computer and Information Sciences\nPrince Sultan University\nEmail: akoubaa@psu.edu.sa\nAbstract—With the widespread adoption of wearable devices\nin our daily lives, the demand and appeal for remote patient\nmonitoring have significantly increased. Most research in this\nfield has concentrated on collecting sensor data, visualizing it,\nand analyzing it to detect anomalies in specific diseases such\nas diabetes, heart disease and depression. However, this domain\nhas a notable gap in the aspect of human-machine interaction.\nThis paper proposes REMONI, an autonomous REmote health\nMONItoring system that integrates multimodal large language\nmodels (MLLMs), the Internet of Things (IoT), and wearable\ndevices. The system automatically and continuously collects vital\nsigns, accelerometer data from a special wearable (such as a\nsmartwatch), and visual data in patient video clips collected from\ncameras. This data is processed by an anomaly detection module,\nwhich includes a fall detection model and algorithms to identify\nand alert caregivers of the patient’s emergency conditions. A\ndistinctive feature of our proposed system is the natural lan-\nguage processing component, developed with MLLMs capable of\ndetecting and recognizing a patient’s activity and emotion while\nresponding to healthcare worker’s inquiries. Additionally, prompt\nengineering is employed to integrate all patient information\nseamlessly. As a result, doctors and nurses can access real-\ntime vital signs and the patient’s current state and mood by\ninteracting with an intelligent agent through a user-friendly web\napplication. Our experiments demonstrate that our system is\nimplementable and scalable for real-life scenarios, potentially\nreducing the workload of medical professionals and healthcare\ncosts. A full-fledged prototype illustrating the functionalities of\nthe system has been developed and being tested to demonstrate\nthe robustness of its various capabilities.\nIndex Terms—Remote Health Monitoring, Wearable Technol-\nogy, Multimodal Large Language Models, Healthcare.\nI. INTRODUCTION\nThe imbalance between the number of patients and medical\nprofessionals has always been a significant challenge for the\nhealthcare system. While the population is rapidly increasing,\nthe workforce of medical professionals is growing slowly due\nto the high requirements of this field. This leads to a high and\nstressful workload for doctors and nurses, which could affect\nthe quality of care services.\nSince the development of the Internet of Things (IoT), many\nstudies have tried to apply it to the healthcare environment\nto support doctors and nurses with their daily tasks, such\nas systematizing medical records [1], automating medical\nappointment booking [2], and moving essential medical ex-\naminations to telehealth [3]. Recently, large language models\n(LLMs) have been experiencing significant growth. Numerous\nLLMs related to healthcare have been developed [4]–[7] to\nimprove human-machine interaction, enabling them to respond\nto medical inquiries from patients or questions in medical\ntextbooks. However, to date, there has been limited research\non creating virtual assistants for medical professionals.\nThis paper proposes REMONI, an autonomous REmote\nhealth MONItoring system. Our system has two main com-\nponents: an anomaly detection module and a smart natural\nlanguage processing component capable of recognizing patient\nstatus (activity and emotion). These two components are built\nwith a scalable and implementable IoT system to operate in\npractice. The main functions of the system are to promptly\nreport the patient’s abnormalities to the doctor, remotely mon-\nitor the patient’s health, and support the doctor in retrieving\npatient data.\nOur contributions are as follows:\n•To the best of our knowledge, this is the first work\nfor the concept of an all-in-one virtual assistant for\nmedical professionals, which automatically gathers data\nfrom sensors, stores it, detects anomalies, and uses LLMs\nto converse with doctors.\n•We propose an IoT system architecture that connects\nsensors, wearable devices, LLMs, and end-user web ap-\nplications using edge devices, cloud storage, and cloud\ncomputing. The architecture is flexible and easy to scale\narXiv:2510.21445v1  [cs.CL]  24 Oct 2025\nFig. 1. Proposed Internet of Things System.\nto a larger number of sensors and users, like in a hospital\nsystem.\n•We demonstrate the medical virtual assistant’s effective-\nness and potential in improving the telehealth field.\nII. RELATEDWORK\nFor a long time, researchers have been keen on developing\nvirtual assistants for the healthcare sector to enhance human-\nmachine interaction and improve care service quality.\nDuring the early stages of Computer Vision development\nwith Convolutional Neural Networks (CNNs), and when Nat-\nural Language Processing (NLP) was still in its infancy, Yoon\net al. [8] created a projection-based augmented reality system\nto assist the elderly daily. They developed a camera capable\nof panning and tilting to monitor a wide area. Additionally,\nthey utilized some deep learning frameworks for posture\nestimation, facial recognition, and object detection. With these\ncapabilities, they constructed a system to project images onto\nsurfaces (walls or tables) for the elderly to interact with. This\nsystem enabled the elderly to use their phones to see who was\nat the door and decide whether to grant entry. It also displayed\ndaily reminders for taking medications or closing doors.\nAs NLP advanced, Nikita et al. [9] proposed a chatbot\nthat employed machine learning techniques with metrics like\nTF-IDF, Stemming, n-grams, and cosine similarity to analyze\nusers’ healthcare-related queries. They aimed to create a\nchatbot to offer patients preliminary advice before consulting\na doctor.\nIn the past two years, with the rapid development of LLMs,\nseveral medical chatbots have been created [4]–[7].These chat-\nbots primarily focus on answering patients’ medical questions.\nYunxiang et al. [10] constructed a dataset of authentic patient-\ndoctor conversations, gathering around 100k interactions from\nthe online medical consultation website HealthCareMagic and\nan additional 10k conversations from another online medical\nconsultation site, iCliniq.\nWhile there is significant research in this field, it is evident\nthat most efforts are centered on the patient side. A noticeable\ngap exists in developing virtual assistants specifically designed\nfor medical professionals.\nIII. PROPOSEDFRAMEWORK\nThe framework proposed in this study comprises three\nmodules: an Anomaly Detection module, a Natural Language\nProcessing Engine, and an Internet of Things module for\nsystem deployment. Each of these is described in details below.\nA. Anomaly Detection\nWithin our system, anomaly detection is a crucial process\nfor recognizing irregular patterns that may indicate falls or\ncritical changes in the patient’s health status.\n1) FallAllD Dataset for Fall Detection:\n•Dataset Description:This study uses the FallAllD\ndataset [11], capturing 35 human falls and 44 daily activities,\ncollected with three data loggers in an outdoor environment,\nwhere the subjects fell on grass instead of on soft mats. Each\ndata-logger is equipped with the inertial module LSM9DS1,\ncontaining a 3-axial accelerometer configured with a sam-\npling frequency of238 Hzand a measurement range of±8 g.\nFifteen participants wore these devices simultaneously on\ntheir necks, wrists, and waists to comprehensively capture\ntheir motion signals. The data collection protocol covers\nvarious fall types, including scenarios followed by recovery,\nand considers different postures and causes such as slips,\ntrips, and syncope. This inclusivity is crucial for developing\nfall detection systems that are robust and effective across\ndiverse real-world situations.\n•Dataset Preprocessing:In preprocessing the FallAllD\ndataset, we focused on the accelerometer data obtained from\nthe subjects’ wrists. This choice is supported by the ability\nof accelerometer data to accurately capture motion patterns\nand dynamics critical to identifying falls, distinguishing\nthem from activities of daily living (ADL) with high relia-\nbility. The literature further validates our approach as many\nresearchers acknowledge the sufficiency of accelerometer\ndata for effective fall detection [12]–[14]. Furthermore, we\nrescaled the original FallAllD dataset’s accelerometer data\nfrom±8 gto±1 gand downsampled it from238 Hzto\n32 Hz. Fig. 2 compares the original FallAllD dataset and\nthe preprocessed data for one random fall activity. It’s\nevident that despite the reduced sampling rate and the range,\nthe peaks associated with the fall are still clear in the\npreprocessed data, which indicates that the preprocessing\nretains the critical features necessary for fall detection.\nFig. 2. Comparison of Original and Preprocessed Fall Activity from FallAllD\nDataset.\n2) Proposed Hybrid Deep Learning Model:We build upon\nthe foundation of our previously proposed hybrid deep learning\n(HDL) model [15], which combines the strengths of convolu-\ntional neural networks (CNNs) for spatial feature extraction\nand long short-term memory (LSTM) networks for temporal\nsequence analysis, to address the time series challenge of\nhuman activity and fall detection. Retaining the model’s core\narchitecture, we have implemented minor refinements to en-\nhance its performance further. The revised model, employing\na sigmoid activation function in the final layer for optimal\nbinary classification and compiled with an Adam optimizer\nand binary cross-entropy loss, offers enhanced performance\nand reliability in our real-world application.\n3) Threshold-Based Assessment of Vital Sign Anomalies:In\naddition to the fall detection capability, our system integrates\na threshold-based algorithm to detect anomalies in vital signs\nand automatically alert caregivers in case of health risks or\nemergencies. This Python model employs predefined healthy\nranges for five key vital signs, namely body temperature (36.5–\n37.2◦C), heart rate (60–100beats per minute), respiration rate\n(12–20breaths per minute at rest), blood pressure (90/60\nto120/80mmHg) [16], and oxygen saturation (SpO 2 ≥\n95%) [17]. REMONI continuously monitors these vital signs\nand upon detecting any values outside the healthy range, it\npromptly sends a notification alerting the medical personnel.\nB. Natural Language Processing Engine\nIn the system, medical professionals communicate directly\nwith REMONI, a virtual assistant, through a web application.\nREMONI is powered by an NLP engine, as shown in Fig. 3,\nwhich operates in three main stages: intention detection, data\npreparation, and final output production.\nInitially, caregiver inquiries are processed by a General\nLarge Language Model (LLM) to discern the user’s intent.\nA system prompt, crafted to detail the intent detection task, is\nsupplied to the General LLM alongside the doctor’s question.\nThis enables the General LLM to identify and output the user’s\nintention in JSON format with the following keys:patient id,\nlist date,list time,vital sign,is plot,is recognition,\nandis image.\nIn the second stage, the engine utilizes thelist dateand\nlist timekeys to determine whether to retrieve necessary\ndata from cloud storage or the edge device. Theis plotand\nis recognitionkeys indicate whether a plotting function or a\nMLLM is required to support the final response. The MLLM,\nequipped with its system prompt, is tasked with describing\nthe patient’s current status in an image, focusing on activity\nand emotion, which are crucial for depicting the patient’s\ncondition.\nFinally, after collating all the information, the agent com-\npiles it into an endpoint prompt that includes the patient’s\npersonal information, activity and emotion (as identified by\nthe MLLM), vital signs (sourced from medical records in\ncloud storage or the edge device), and the user’s question as\ninstructions. A system prompt at the beginning of the endpoint\nprompt instructs the General LLM to use the data in the prompt\nto answer the user’s questions accurately.\nIn summary, the NLP engine employs a General LLM for\ntwo distinct tasks (intent detection and final output produc-\ntion), each with a specific system prompt to guide the LLM’s\noutput according to the task. Additionally, the engine features\na function library that includes a MLLM for recognizing\nthe patient’s activity and emotion and a plot function for\nvisualizing the vital sign data.\nC. Internet of Things System\nThe proposed IoT system comprises four main components:\nsensors, edge devices, cloud storage, and cloud computing. An\noverview of the architecture is illustrated in Fig. 1. Currently,\nour system is compatible with smartwatches and cameras as\nsensors.\nAt the core of our deployment lies the seamless integration\nof wearable technology. A custom wearable application serves\nas the conduit for data acquisition, harnessing the watch’s\nsensors to collect accelerometer and physiological data. This\napplication uses the Wi-Fi API to transmit the collected data\nstreams to a designated Python module hosted on an edge\ndevice for further processing.\nThe edge device collects real-time data (accelerometer read-\nings, vital signs, and visual information) from these sensors.\nIt processes it through anomaly detection models (for falls\nand vital signs) to quickly identify emergencies. The device\nFig. 3. Natural Language Processing Engine.\nimmediately sends an alert to the cloud computing platform if\nan emergency is detected. Otherwise, it periodically uploads\nvital signs and visual data to cloud storage for preservation and\nfuture use. Cloud storage archives historical data and responds\nto requests from the NLP engine for the data required to\naddress user queries.\nRegarding cloud computing, it serves as the host for the\nweb application. Its primary role is to receive emergency\nalerts from the edge device and notify medical professionals.\nAdditionally, it facilitates the NLP engine’s communication\nwith the edge device for current data and with cloud storage\nfor historical data.\nIV. EXPERIMENTS\nA. Deployment\nThe implementation of REMONI involves a carefully or-\nchestrated setup of interconnected components, all operating\nconcurrently on the edge device, each contributing to the\nsystem’s robustness and effectiveness in remote health moni-\ntoring.\nThe project utilizes the ACER Nitro AN515 as the edge\ndevice, which is equipped with an i5 9th-gen CPU, a GTX\n1660 TI GPU, 8 GB of RAM, and 1 TB SSD storage. For\nimaging, we selected the Logitech 720p HD Webcam for its\nimage quality and ease of use.\nFor data acquisition, we deployed a Tizen wearable web app\nspecifically designed for the Samsung Galaxy Watch 3 (4GB\nof RAM, 380 mAh battery, multiple physiological sensors),\nchosen for its advanced sensors, reliability, and seamless\nintegration with our system. AWS S3 and AWS EC2 were\nselected for their stable performance for cloud storage and\ncomputing.\nThese intricately integrated components form a robust de-\nployment framework that delivers scalable, reliable, and re-\nsponsive remote health monitoring capabilities to caregivers\nand healthcare professionals.\nB. Results and Discussion\nTo assess the system’s effectiveness, we examine the perfor-\nmance of the fall detection model, which utilizes accelerometer\ndata from the Samsung Galaxy Watch 3. Additionally, we\nevaluate the accuracy of activity and emotion recognition using\nMLLMs and analyze the implementation time required for\nvarious tasks within the system.\n1) Fall detection model:Fig. 4 illustrates the training and\nvalidation accuracy over epochs. The model quickly reaches\na high level of accuracy during training, which is consistently\nmaintained throughout the validation phase. The convergence\nof training and validation accuracy suggests that the model\ngeneralizes well and exhibits minimal overfitting.\nThe confusion matrix in Fig. 5 displays the model’s perfor-\nmance in classifying fall and non-fall events in the test data\nfrom the preprocessed FallAllD dataset. The high values on\nthe matrix’s diagonal (0.99 for Non-Fall, 0.98 for Fall) indicate\na high true positive rate for both classes, demonstrating the\nmodel’s accuracy. The low off-diagonal values (0.01 and 0.02)\nshow a small fraction of misclassifications, confirming the\nmodel’s reliability in fall detection.\nTable I provides a comparative analysis of various mod-\nels tested on the FallAllD dataset, such as Light Gradient-\nBoosting Machine (LightGBM), Support Vector Machine\n(SVM), Coarse-fine CNN combined with Gated Recurrent\nUnit (GRU), and Late AFVF (feature-level fusion applied\nto Actual Fusion within Virtual Fusion).The proposed HDL\nmodel demonstrates superior performance with a precision\nof 99%, an accuracy, recall, and F1-score all at 98%. This\nhighlights the model’s high performance using solely wrist-\nacquired accelerometer data, underscoring its potential for\nreliable and accurate fall detection in real-world applications.\n2) Activity and Emotion Recognition:To assess the sys-\ntem’s capability to recognize human activity and emotion, we\nselected the HACER dataset [20] for its high relevance to the\nproject. The dataset comprises around 500 video clips featur-\n0 5 10 15 20 25 30 35 40\n0.7\n0.8\n0.9\n1\nEpochs\nAccuracy\nTraining Accuracy\nValidation Accuracy\nFig. 4. CNN-LSTM Traning vs. Validation Accuracy on the Preprocessed\nFallAllD dataset.\nNon Fall Fall\nNon FallFall\n0.99 0.01\n0.02 0.98\nPredicted labels\nTrue labels\n0\n0.2\n0.4\n0.6\n0.8\n1\nFig. 5. CNN-LSTM Model: Confusion Matrix for Test Data on the Prepro-\ncessed FallAllD Dataset.\nTABLE I\nBENCHMARKING FOR THEFALLALLDDATASET\nModel Sensor/Location A P R F1\nLightGBM [18] Acc+Gyro/Wrist 95% N/A 91% 91%\nSVM [19] Acc+Gyro/Wrist 93% N/A 87% 93%\nCNN-GRU [13] Acc/Waist 98% 96% 93% 94%\nLate AFVF [14] Acc/Waist+Wrist N/A N/A N/A 96%\nProposed model Acc/Wrist 98% 99% 98% 98%\nAcc: Accelerometer, Gyro: Gyroscope, A: Accuracy, P: Precision, R: Recall,\nF1: F1-score\ning ten participants, including men and women. Each video is\nlabelled with both activity and emotion categories. The dataset\nfeatures nine activity classes:drinking,putting on glasses,\nputting on jacket,reading,sitting down,standing up,\ntaking off glasses,taking off jacket, andwriting.\nAdditionally, it includes five emotion classes:angry,disgust,\nhappy,neutral, andsad.\nWe crafted a prompt similar to the system prompt used\nby the MLLM in our system. The difference is that we\nexplicitly list the activity and emotion classes in the system\nprompt and ask the models to identify the correct one. If no\nperson is visible in the image, the models should respond with\n‘unidentifiable’ for both activity and emotion. Similarly, if the\nTABLE II\nPERFORMANCE OFMLLMS INACTIVITY ANDEMOTIONRECOGNITION\nModel Accuracy Precision Recall F1-score\nLLaV A-Activity 10% 22% 10% 7%\nVideo-ChatGPT-Activity 35% 49% 35% 33%\nGPT4-Vision-Activity 51% 58% 51% 45%\nLLaV A-Emotion 15% 39% 15% 12%\nVideo-ChatGPT-Emotion 20% 39% 20% 19%\nGPT4-Vision-Emotion 41% 35% 41% 31%\nperson’s face is unclear, particularly in surveillance footage,\nthe models should output ‘unidentifiable’ for the emotion\nwhile still attempting to identify the activity.\nFor evaluation, we utilized three multimodal large lan-\nguage models: LLaV A [21], Video-ChatGPT [22], and GPT4-\nVision [23]. LLaV A and GPT4-Vision analyze a single frame,\nspecifically the middle frame extracted from the video. In\ncontrast, Video-ChatGPT processes approximately 100 frames.\nAs shown in Table II, LLaV A had the lowest performance,\nwith 10% accuracy for activity and 15% for emotion. This may\nbe due to its reliance solely on image input. Additionally, it\nfrequently classifies activities and emotions as ‘unidentifiable’,\nsuggesting a lack of focus on human faces or poses for activity\nand emotion detection.\nWhile the training process of Video-ChatGPT enriched\nthe dataset’s context, enabling it to provide more detailed\nresponses, it ranked second with an accuracy of 35% for\nactivity recognition and 20% for emotion detection. The\nmodel’s propensity for lengthy answers sometimes resulted in\nmissing the correct classes, which did not align with our need\nfor straightforward recognition of emotions and activities.\nGPT4-Vision showed the best overall performance, with\n51% accuracy for activity and 41% for emotion, despite only\nusing the middle frame of the video. Its performance signifi-\ncantly surpasses that of LLaV A. However, a closer examination\nof the precision values for emotion reveals that GPT4-Vision\ntends to output ‘happy’ and ‘neutral’ as the emotion class,\nresulting in slightly lower precision. In contrast, despite their\nlower accuracy, the other two models produce outputs that\nspan across various emotion classes.\nAlthough GPT4-Vision scores the highest in most metrics,\nits performance is still relatively low. This may be attributed to\nour stringent criteria for determining accurate classifications.\nFor instance, if a person is sitting and drinking water, the\ndataset classifies it as drinking, but if the models detect it\nas sitting, it is considered a false detection. We are in the\nprocess of developing a more equitable evaluation method.\nAdditionally, the system is set to be implemented in a real-\nworld scenario shortly and will undergo further fine-tuning\nwith actual data from a clinic in Abu Dhabi. As a result, we\nanticipate an improvement in the performance of the MLLMs.\nFor the demo phase, GPT4-Vision has been selected as the\nMLLM for the NLP engine.\n3) Response Time Delays:The overall system latency was\nevaluated through three main approaches, each encompassing\nvarious questions based on the data requested. Each scenario’s\ntime is measured ten times before calculating the average and\nvariance.\nFirstly, as shown in Table III, the response time to queries\nabout real-time (instant) data, such as the patient’s current\nstate or vital signs, is assessed. . These questions necessitate\ncommunication between the cloud computing system and the\nedge device to retrieve the latest sensor data and provide\nanswers. Moreover, the latency in responding to requests for\nhistorical information stored in the cloud database is also\ninvestigated.\nTABLE III\nRESPONSE TIME ANALYSIS OF THEREMONISYSTEM FOR VARIOUS\nQUESTION TYPES\nQuestion Type\\Data type Instant Data Historical Data\nImage Retrieval 6.84s±2.09s 3.37s±0.15s\nVital Sign Retrieval 5.69s±0.06s 6.62s±8.73s\nImage Description 9.56s±6.11s 9.75s±9.70s\nPlotting Vital Sign N/A 4.61s±1.58s\nOverall, the system’s response time to user queries is under\n20 seconds. Questions requiring the NLP engine to use MLLM\nto determine the patient’s current state (activity and emotion)\ntake longer to answer. Additionally, retrieving instant image\ndata from edge devices takes much longer than accessing\nhistorical image data from cloud storage.\nLastly, the system’s responsiveness in sending emergency\nalerts from the edge device to the caregiver was examined,\nwith response times averaging around 0.2 seconds.\nV. CONCLUSION\nOur proposed system, REMONI, is an autonomous system\nthat enhances human-machine interaction in REmote health\nMONItoring. Utilizing MLLMs, IoT, and wearable devices,\nREMONI provides a comprehensive solution for the contin-\nuous and automatic collection of vital signs, accelerometer\ndata, and visual data. It also facilitates anomaly detection\nand seamless communication between medical professionals\nand the system. The system is designed to be easily scalable\nto larger environments involving multiple medical profession-\nals and many patients. Additionally, more wearable devices\nand anomaly detection algorithms can be quickly integrated\ninto our proposed IoT system architecture. Our experiments\ndemonstrate the system’s feasibility for real-life applications.\nWe believe in its potential to alleviate the workload of medical\nprofessionals and reduce healthcare costs. In future work, we\nplan to deploy the system in real-life settings and further\nenhance the performance of its components.\nREFERENCES\n[1] A. Subahi, “Edge-based iot medical record system: Requirements,\nrecommendations and conceptual design,”IEEE Access, vol. PP, pp.\n1–1, 07 2019.\n[2] Ruchit, P. Suryavanshi, S. Kaushik, and D. Dev, “An automated model\nfor booking appointment in health care sector,” in2021 International\nConference on Technological Advancements and Innovations (ICTAI),\n2021, pp. 7–10.\n[3] L. Romeo, R. Marani, T. D’Orazio, and G. Cicirelli, “Video based\nmobility monitoring of elderly people using deep learning models,”IEEE\nAccess, vol. 11, pp. 2804–2819, 2023.\n[4] Q. Lu, D. Dou, and T. Nguyen, “ClinicalT5: A generative language\nmodel for clinical text,” inFindings of the Association for\nComputational Linguistics: EMNLP 2022, Y . Goldberg, Z. Kozareva,\nand Y . Zhang, Eds. Abu Dhabi, United Arab Emirates: Association\nfor Computational Linguistics, Dec. 2022, pp. 5436–5443. [Online].\nAvailable: https://aclanthology.org/2022.findings-emnlp.398\n[5] Z. Chen, A. H. Cano, A. Romanou, A. Bonnet, K. Matoba, F. Salvi,\nM. Pagliardini, S. Fan, A. K ¨opf, A. Mohtashami, A. Sallinen,\nA. Sakhaeirad, V . Swamy, I. Krawczuk, D. Bayazit, A. Marmet, S. Mon-\ntariol, M.-A. Hartley, M. Jaggi, and A. Bosselut, “Meditron-70b: Scaling\nmedical pretraining for large language models,” 2023.\n[6] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark,\nS. Pfohl, H. Cole-Lewis, D. Neal, M. Schaekermann, A. Wang, M. Amin,\nS. Lachgar, P. Mansfield, S. Prakash, B. Green, E. Dominowska,\nB. A. y Arcas, N. Tomasev, Y . Liu, R. Wong, C. Semturs, S. S.\nMahdavi, J. Barral, D. Webster, G. S. Corrado, Y . Matias, S. Azizi,\nA. Karthikesalingam, and V . Natarajan, “Towards expert-level medical\nquestion answering with large language models,” 2023.\n[7] T. Han, L. C. Adams, J.-M. Papaioannou, P. Grundmann, T. Oberhauser,\nA. L ¨oser, D. Truhn, and K. K. Bressem, “Medalpaca–an open-source\ncollection of medical conversational ai models and training data,”arXiv\npreprint arXiv:2304.08247, 2023.\n[8] Y . Park, H. Ro, N. Lee, and T.-D. Han, “Deep-care: Projection-based\nhome care augmented reality system with deep learning for elderly,”\nApplied Sciences, vol. 9, p. 3897, 09 2019.\n[9] N. V . Shinde, A. Akhade, P. Bagad, H. Bhavsar, S. Wagh, and A. Kam-\nble, “Healthcare chatbot system using artificial intelligence,” in2021\n5th International Conference on Trends in Electronics and Informatics\n(ICOEI), 2021, pp. 1–8.\n[10] Y . Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y . Zhang, “Chatdoctor: A\nmedical chat model fine-tuned on a large language model meta-ai (llama)\nusing medical domain knowledge,”Cureus, vol. 15, no. 6, 2023.\n[11] M. Saleh, M. Abbas, and R. B. L. Jeann `es, “Fallalld: An open dataset of\nhuman falls and activities of daily living for classical and deep learning\napplications,”IEEE Sensors Journal, vol. 21, pp. 1849–1858, 2021.\n[12] K.-C. Liu, K.-H. Hung, C.-Y . Hsieh, H.-Y . Huang, C.-T. Chan, and\nY . Tsao, “Deep-learning-based signal enhancement of low-resolution\naccelerometer for fall detection systems,”IEEE TCDS, vol. 14, pp.\n1270–1281, 2022.\n[13] C.-P. Liu, J.-H. Li, E.-P. Chu, C.-Y . Hsieh, K.-C. Liu, C.-T. Chan, and\nY . Tsao, “Deep learning-based fall detection algorithm using ensemble\nmodel of coarse-fine cnn and gru networks,” 2023.\n[14] D.-A. Nguyen, C. Pham, and N.-A. Le-Khac, “Virtual fusion with\ncontrastive learning for single sensor-based activity recognition,”arXiv\npreprint arXiv:2312.02185, 2023.\n[15] F. Kharrat, W. Gueaieb, F. Karray, and A. Elsaddik, “A hybrid deep\nlearning model for human activity recognition and fall detection for the\nelderly,” in2023 IEEE International Symposium on Medical Measure-\nments and Applications (MeMeA). IEEE, 2023, pp. 1–6.\n[16] Health Encyclopedia, “Vital signs,” (Accessed: Mar. 21, 2024). [Online].\nAvailable: https://www.urmc.rochester.edu/encyclopedia/content.aspx?\nContentTypeID=85&ContentID=P00866\n[17] National Library of Medicine, “Oxygen saturation,” (Accessed: Mar.\n21, 2024). [Online]. Available: https://www.ncbi.nlm.nih.gov/books/\nNBK525974/\n[18] J.-K. Kim, K. Lee, and S. G. Hong, “Detection of important features\nand comparison of datasets for fall detection based on wrist-wearable\ndevices,”Expert Systems with Applications, vol. 234, p. 121034, 2023.\n[19] T. O. Nu ˜nez, A. Ramirez, and L. B. Mar ´on, “Analysis of waist and wrist\npositioning wearable machine learning models to detect falls,” Jan. 2024.\n[20] S. Ammar, T.-C. Ho, F. Karray, and W. Gueaieb, “Hacer: An integrated\nremote monitoring platform for the elderly,” in2023 International Con-\nference on Intelligent Metaverse Technologies & Applications (iMETA),\n09 2023, pp. 1–6.\n[21] H. Liu, C. Li, Q. Wu, and Y . J. Lee, “Visual instruction tuning,” 2023.\n[22] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, “Video-chatgpt: Towards\ndetailed video understanding via large vision and language models,”\narXiv:2306.05424, 2023.\n[23] OpenAI, “Gpt-4 technical report,”ArXiv, vol. abs/2303.08774, 2023.\n[Online]. Available: https://arxiv.org/abs/2303.08774",
  "topic": "Wearable computer",
  "concepts": [
    {
      "name": "Wearable computer",
      "score": 0.7905064821243286
    },
    {
      "name": "Computer science",
      "score": 0.7043070793151855
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5571293830871582
    },
    {
      "name": "Embedded system",
      "score": 0.24168282747268677
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I142024983",
      "name": "Prince Sultan University",
      "country": "SA"
    }
  ]
}