{
    "title": "Analog in-memory computing attention mechanism for fast and energy-efficient large language models",
    "url": "https://openalex.org/W4414064893",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A4225510649",
            "name": "Leroux, Nathan",
            "affiliations": [
                "Forschungszentrum Jülich"
            ]
        },
        {
            "id": "https://openalex.org/A4366951550",
            "name": "Manea, Paul-Philipp",
            "affiliations": [
                "RWTH Aachen University",
                "Forschungszentrum Jülich"
            ]
        },
        {
            "id": "https://openalex.org/A4299208195",
            "name": "Sudarshan, Chirag",
            "affiliations": [
                "Forschungszentrum Jülich"
            ]
        },
        {
            "id": "https://openalex.org/A4287169826",
            "name": "Finkbeiner, Jan",
            "affiliations": [
                "RWTH Aachen University",
                "Forschungszentrum Jülich"
            ]
        },
        {
            "id": "https://openalex.org/A4310557106",
            "name": "Siegel, Sebastian",
            "affiliations": [
                "Forschungszentrum Jülich"
            ]
        },
        {
            "id": "https://openalex.org/A2160504151",
            "name": "Strachan, John Paul",
            "affiliations": [
                "Forschungszentrum Jülich",
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A2746762597",
            "name": "Neftci, Emre",
            "affiliations": [
                "RWTH Aachen University",
                "Forschungszentrum Jülich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4306955484",
        "https://openalex.org/W3190062760",
        "https://openalex.org/W4404862388",
        "https://openalex.org/W4389518760",
        "https://openalex.org/W3111375540",
        "https://openalex.org/W4223424212",
        "https://openalex.org/W4381233128",
        "https://openalex.org/W4401070622",
        "https://openalex.org/W4400985603",
        "https://openalex.org/W3013080934",
        "https://openalex.org/W4280496502",
        "https://openalex.org/W4390017976",
        "https://openalex.org/W4366205946",
        "https://openalex.org/W3167292039",
        "https://openalex.org/W4365152225",
        "https://openalex.org/W4391622561",
        "https://openalex.org/W4385192524",
        "https://openalex.org/W3137059735",
        "https://openalex.org/W3159274266",
        "https://openalex.org/W4390747940",
        "https://openalex.org/W2960778947",
        "https://openalex.org/W4292121737",
        "https://openalex.org/W4386100686",
        "https://openalex.org/W3156580923",
        "https://openalex.org/W4411725013",
        "https://openalex.org/W4394896788",
        "https://openalex.org/W4380302649",
        "https://openalex.org/W2963122961",
        "https://openalex.org/W2954698171",
        "https://openalex.org/W4391582428",
        "https://openalex.org/W3194676777",
        "https://openalex.org/W2946609015",
        "https://openalex.org/W2963015836",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W7084114116",
        "https://openalex.org/W6893809404",
        "https://openalex.org/W6893304497"
    ],
    "abstract": "Abstract Transformer networks, driven by self-attention, are central to large language models. In generative transformers, self-attention uses cache memory to store token projections, avoiding recomputation at each time step. However, graphics processing unit (GPU)-stored projections must be loaded into static random-access memory for each new generation step, causing latency and energy bottlenecks. Here we present a custom self-attention in-memory computing architecture based on emerging charge-based memories called gain cells, which can be efficiently written to store new tokens during sequence generation and enable parallel analog dot-product computation required for self-attention. However, the analog gain-cell circuits introduce non-idealities and constraints preventing the direct mapping of pre-trained models. To circumvent this problem, we design an initialization algorithm achieving text-processing performance comparable to GPT-2 without training from scratch. Our architecture reduces attention latency and energy consumption by up to two and four orders of magnitude, respectively, compared with GPUs, marking a substantial step toward ultrafast, low-power generative transformers.",
    "full_text": "Nature Computational Science | Volume 5 | September 2025 | 813–824\n 813\nnature computational science\nhttps://doi.org/10.1038/s43588-025-00854-1\nArticle\nAnalog in-memory computing attention \nmechanism for fast and energy-efficient \nlarge language models\n \nNathan Leroux    1,4 , Paul-Philipp Manea    2,3,4 , Chirag Sudarshan2, \nJan Finkbeiner    1,3, Sebastian Siegel2, John Paul Strachan2,3 & Emre Neftci    1,3\nTransformer networks, driven by self-attention, are central to large language \nmodels. In generative transformers, self-attention uses cache memory \nto store token projections, avoiding recomputation at each time step. \nHowever, graphics processing unit (GPU)-stored projections must be loaded \ninto static random-access memory for each new generation step, causing \nlatency and energy bottlenecks. Here we present a custom self-attention \nin-memory computing architecture based on emerging charge-based \nmemories called gain cells, which can be efficiently written to store new \ntokens during sequence generation and enable parallel analog dot-product \ncomputation required for self-attention. However, the analog gain-cell \ncircuits introduce non-idealities and constraints preventing the direct \nmapping of pre-trained models. T o circumvent this problem, we design an \ninitialization algorithm achieving text-processing performance comparable \nto GPT-2 without training from scratch. Our architecture reduces attention \nlatency and energy consumption by up to two and four orders of magnitude, \nrespectively, compared with GPUs, marking a substantial step toward \nultrafast, low-power generative transformers.\nTransformers1 are central to modern artificial intelligence (AI), power-\ning advances in language models, image processing and beyond.  \nHowever, their high computational demands lead to substantial energy \nconsumption. Enhancing their efficiency is essential to reduce environ-\nmental impact and to keep pace with the exponentially growing size of \nAI models. The success of transformers as state of the art in sequence \nprocessing and generation is enabled by their attention mechanism2.  \nT o capture dependencies across sequences, the attention mechanism \nperforms dot products between different projections of multiple \nsequence elements, known as tokens. For generative tasks, the best  \nperformance is achieved by autoregressive, decoder-only transfor-\nmers3. At each inference step, the decoder generates a token, which is  \nthen appended to the input sequence, forming the input for the sub-\nsequent step. T o avoid recomputing the keys and values (KV cache)  \nprojections of the previously generated tokens, the so-called \nKV-caching method stores the projections from previous tokens in \nmemory and updates the KV cache with the new projections4.\nIn a graphics processing unit (GPU), for each token, the entire KV \ncache must be transferred from main high-bandwidth memory to cache \nmemory (static random-access memory (SRAM)). In addition, the KV \ncache is often much larger than the available SRAM memory owing to \nthe dimensions of the stored projections and the sequence length5. For \ninstance, the entire KV cache of the model Mistral 7B 6 requires 8 Gb \nfor a batch size of 1, as necessary for inference workloads. In recent \ntechnologies, the energy for data access exceeds the energy required \nfor computations7. Loading the KV cache for the attention mechanism \nis thus a major bottleneck, causing increased energy consumption and \nlatency in large language models (LLMs)8. T o mitigate this bottleneck, \nReceived: 15 November 2024\nAccepted: 22 July 2025\nPublished online: 8 September 2025\n Check for updates\n1PGI-15, Forschungszentrum Jülich, Jülich, Germany. 2PGI-14, Forschungszentrum Jülich, Jülich, Germany. 3Faculty of Electrical Engineering,  \nRWTH Aachen, Aachen, Germany. 4These authors contributed equally: Nathan Leroux, Paul-Philipp Manea.  e-mail: n.leroux@fz-juelich.de;  \np.manea@fz-juelich.de\nNature Computational Science | Volume 5 | September 2025 | 813–824 814\nArticle https://doi.org/10.1038/s43588-025-00854-1\nIn addition, the normalization in softmax requires summing across \nall input elements, requiring global connections with an increased \nhardware complexity scaling with the sequence length 37,38. In our  \nsystem, the activation function is instead operated element-wise with \ncharge-to-pulse circuits implementing HardSigmoid functions.\nT o overcome this discrepancy, we introduce an algorithm that \nadapts a pre-trained language model to our architecture by scaling \neach layer according to its statistics and hardware characteristics. \nWith our adaptation algorithm, our model achieves accuracy similar \nto a pre-trained GPT-2 model without having to train the model from \nscratch. Overall, the contributions of this study are:\n• An in-memory, mixed analog–digital computing design to  \nstore token projections and compute attention dot products  \nwith gain-cell arrays at high energy efficiency.\n• An end-to-end attention mechanism based on analog signals lever-\naging charge-to-pulse circuits to avoid power- and area-intensive \nADCs.\n• Quantitative performance analysis of a scalable architecture  \nwith area floorplan including analog circuits and digital \nperipheries.\n• A software-to-hardware methodology to map pre-trained (ideal) \nmodels to non-traditional hardware reaching an accuracy equiva-\nlent to GPT-2.\n• Our architecture achieves up to five and two orders of magnitude \nlower energy consumption and latency, respectively, compared \nwith GPUs.\nAfter detailing the attention mechanism algorithm, we demon-\nstrate its implementation using gain cells and charge-to-pulse  \ncircuits. We then show how our approach maps a pre-trained model \nto our hardware while maintaining high accuracy on common natural  \nlanguage processing (NLP) benchmarks. Finally, we evaluate the  \narchitecture’s performance in terms of energy consumption, latency \nand area footprint.\nResults\nAttention mechanism\nFigure 1a shows the attention mechanism algorithm. In autoregressive \ntransformers, new token projections called queries (Q), keys (K) and \nvalues (V ) are created for each inference step from the weights \nWQ,K,V ∈ℝ D,d and an input token xi ∈ℝ 1,D as:\nQi,Ki,Vi = WQ,K,Vxi, (1)\nwhere i is the token index, D  is the token dimension and d  is the  \nembedding dimension. The keys and values Ki ∈ℝ 1,d and Vi ∈ℝ 1,d are \nstored as part of the full KV cache with K∈ℝ T,d and V∈ℝ T,d, where  \nT is the sequence length. The query Qi ∈ℝ 1,d is not stored but used  \nfor inference as\nSi = Qi ⋅KT; Ai = ϕ( Si\n√d\n)⋅V. (2)\nThe dot product between the queries and keys produces an attention \nscore matrix Si ∈ℝ 1,T. In standard transformers, the activation func-\ntion ϕ is typically a softmax function, but other nonlinear activation \nfunctions can yield similar accuracy10,39,40. In particular, sigmoid-based \nattention has been shown to match softmax-based attention on  \nmodels up to 7-billion-parameters large40. Recent studies show that in \nthe case of sliding window attention41, the normalization of softmax \nleads to vanishing memory while sigmoid-based attention can lead to \nbetter information42,43. The output of the attention mechanism Ai is \nthen obtained by the dot product between the activation ϕ(Si) and the \nvalues. In the transformer architecture, multiple attention ‘heads’ are \na wide body of literature explores resource-efficient algorithms9. Alter-\nnative architectures to transformers with linear time complexity are \ndeveloped to improve long-sequence processing efficiency10,11. How-\never, transformers continue to exhibit more stable training at scale \nthan alternatives such as Mamba11, which contributes to their ongoing \ndominance despite the efficiency of state-space models. Alternatively, \ndifferent methods have been developed to reduce the memory require-\nments of KV caching through token pruning12, latent KV-cache compres-\nsion13 or low-rank approximations14, or by reusing the same KV-cache \npairs across multiple heads (grouped-query attention)15.\nWhile these algorithmic strategies reduce computational and \nmemory overhead, achieving further energy efficiency increasingly \ndepends on hardware innovation. Hardware systems dedicated to spe-\ncific neural architectures can substantially outperform conventional \ncentral processing units and GPUs in terms of energy efficiency 16. In \nparticular, to mitigate data-transfer overhead of weights loading, \nseveral approaches leverage either near-memory or in-memory com-\nputing (IMC)17–21. IMC is particularly beneficial when using non-volatile \nmemories to store stationary weights in linear layers22. However, a full \noptimization of transformers’ inference also requires addressing the \nattention mechanism, which contributes substantially to the overall \ncomputational cost9,18. Current IMC solutions do not yet meet all the \nrequirements for efficient hardware implementation of attention. \nSpecifically, KV cache demands fast and energy-efficient memory \nwriting as it is input dependent and must be updated at every genera-\ntion step. In addition, high parallelism is crucial for low-latency infer-\nence, while high memory density is needed for scaling to large models. \nFinally, long retention time is essential to avoid frequent memory \nrefresh operations. KV cache has been implemented either by dynamic \nrandom-access memories (DRAMs)21,23, which have limited parallelism \nrequiring many digital sequential adders, or by SRAMs19,24, which are \nlimited by their volatility and relatively low density 25. Non-volatile \nmemories can be used for linear layers of transformers 17, but are too \nslow, energy expensive and are not endurant enough for dynamical \nKV-cache writing18,22.\nIn this work, we propose an IMC hardware architecture based on \nemerging charge-based memory devices, known as gain cells 26,27, to \nstore token projections and compute dot products for the attention \nmechanism. As a result, gain-cell crossbar arrays simultaneously serve \nto store the KV cache and to perform attention computation. Gain \ncells store information in a capacitor, with a dedicated read transistor \ngenerating current based on the capacitor’s voltage. Unlike DRAM, \nthis enables non-destructive read operations, supporting highly \nparallel IMC computations. Gain cells have high endurance, fast write \nspeeds and low write energy, and are multi-level. Oxide semiconductor \nfield effect transistor (OSFET)-based gain cells (for example, indium \ngallium zinc oxide (IGZO) or indium tin oxide (ITO)) are capable of \nretaining their state for several seconds without a power supply28–30, \ncan be manufactured with very small feature sizes, achieving higher \ndensity than SRAM, and also support three-dimensional (3D) integra-\ntion, which can further reduce effective area requirements for IMC \napplications28–33.\nThe analog-to-digital conversion required for analog IMC often \nhinders the advantages this approach offers, as analog-t-digital con-\nverters (ADCs) are power and area intensive34. T o mitigate this issue, \ncharge-based integration is an energy-efficient alternative35,36. Here, \nwe choose to perform the core of the attention mechanism—two \ndot products, scaling and activation function—fully in the analog \ndomains, using charge-to-pulse circuits for activation and inter-module  \ncommunication, combined with pulse counters for final readout.\nPractical applications of LLMs often rely on pre-trained models \nto reduce training costs. However, our co-optimization approach \nintroduces specific hardware constraints to enhance architectural \nperformance, which leads to a divergence from standard pre-trained \nmodels. The multiplications operated with gain cells are non-ideal. \nNature Computational Science | Volume 5 | September 2025 | 813–824\n 815\nArticle https://doi.org/10.1038/s43588-025-00854-1\ncomputed in parallel, concatenated and provided to a subsequent \nlinear layer to produce the final multi-head attention result.\nIn decoder-based transformers, causal attention allows the score \nmatrix S to compare the input token with all previous sequence ele -\nments. However, to prevent the physical memory size from scaling \nwith the entire sequence length, we employ a type of attention that \nis both causal and local: sliding window attention41. In this approach, \nonly a fixed number M  of key and value projections are retained in \nmemory and attention scores for elements older than the last M  are \nmasked (Fig. 2a). Although sliding window attention is local at each \nlayer, it can still capture global information in deep networks because \nthe receptive field grows with the number of layers6.\nEnd-to-end analog hardware attention\nIn this section, we first give an overview of how our architecture  \nperforms operations on analog signals to compute attention. Then, \nwe detail how the different circuits operate. Keys K  and values V  \nare stored in two gain-cell arrays. The query Qi is encoded as pulse- \nwidth modulation (PWM) pulses and is the input of the first array, per-\nforming the dot product Qi ⋅ KT. An intermediate charge-to-voltage \npulse block integrates the output currents from the first array and \noutputs PWM voltage pulses for the second array, while applying a \nHardSigmoid activation function (Fig. 1c). The second array, computing \nϕ(S) ⋅ V is read out using a signed charge-to-voltage pulse block, where \nthe resulting pulse widths are measured by a digital counter.\nThe proposed gain cell, shown in Fig. 1d, contains a write stage \nfor programming the capacitor C1 and a multiplication stage approxi-\nmating the product between the input and the capacitor voltage.\nThe storage capacitor is charged with a multi-level voltage pulse \nemitted by a digital-to-analog converter (DAC). The voltage pulse \nis gated to the designated capacitor by a write-enable transmission  \ngate. Due to leakage in the storage capacitors, the voltages gradually \ndecay over time. Figure 1f shows the simulated transient response \nof the storage capacitor voltage V store, which corresponds to the  \ncell weight for both extreme values 0 V and 0.9 V. An exponential  \ndecay fit of the gain cells leakage reveals that the time constant (that \nis, retention time) of our silicon complementary metal–oxide–semi-\nconductor (CMOS)-based gain cell is τ  = 5 ms. Note that an OSFET-  \nbased gain cell can achieve multiple orders of magnitude longer  \nretention times29.\nThe multiplication stage generates an analog current via a push–\npull transistor pair, with its amplitude set by the stored capacitor  \nVstore  = K i\nN heads\nV\nLinear\nConcat\nLinear\nx\nDot product\nQ\nϕ\nK\nA\nScale\nS\na b\n<M>\nϕ(S)\nϕ(S) V\n<M>\n∑I\nPulses\nS = Q  KT\nQ\nK\nDAC\n<D>\n<D>\nV\nDAC\nCOUNT\n∑I\nHardSigmoid charge-to-pulse\nC 1\nP 2\nN 2\nP 1\nN 1\nWLW\nWLR\nIcell  = Q iKi\nP 3\nN 3\nIout  = ∑–Icell \nWLR\nVin\nVin\nSigned multiplier gain cell\nWEWE\nBL\nVb\nREST\nDCH\nSAMPA\nPulse Vint\nHardSigmoid charge-to-pulse\ne f g h\nGain-cell array\nc\nd\nOutput: I cell  (µA) \nWeight: V store  (V)\nWeight: V store  (V)\nTime (ms)\nOutput pulse width (%)\nInput: ΣQ i .Ki (µA)\nOutput pulse width (%)\nOutput sign (V)\nInput: Σϕ(S i) .Vi (µA)\nVDD\n0 0.45 0.90\n–10\n0\n10\n0\n0 10 0\n0\n50\n100\n0\n50\n100\n50 0\n0\n50–50\n0.45\n0.90\nDot product\nFig. 1 | Building blocks of the analog hardware attention mechanism.  \na, Multi-head attention architecture. The nonlinear activation is denoted by ϕ. \nInputs Q, K and V are the token projections. S = Q ⋅ KT is the attention score  \nand A = ϕ(S) ⋅ V is the final attention output. b, Hardware implementation of the \nattention mechanism. Red and green traces indicate analog input currents and \ngenerated voltage pulses, respectively. ∑I indicates current integration. M and D \ndenote the sliding window and embedding dimensions. COUNT blocks are pulse \ncounters returning the digital attention result A. c, HardSigmoid charge-to-pulse \ncircuit: integrates bitline (BL) current and emits a pulse width proportional to  \nthe accumulated charge during the discharge phase. The circuit is controlled by \nthe signals: sample (SAMP), reset (REST) and discharge (DCH) which control  \nthe three states. Vint, the charge integrated by the charge-to-pulse circuits.  \nd, Signed gain-cell-based multiplier: Vstore encodes the weight (K or V) and is set \nvia write transistors N2 and P2. P1 and N1 modulate the output current based on \nVstore, while P3 and N3 act as switches driven by the input query Q. The signals of the \ncell include two complementary word line read (WLR) signals, which serve as the \ninputs, a word line write (WLW) signal, a complementary write enable (WE) signal \npair, and a BL that collects the output current. e, Simulated output current Icell \nversus stored voltage Vstore for Vin = 0.9 V. Monte Carlo variation bounds are shown \nin green. f, Simulated voltage decay of the storage capacitor over time due  \nto leakage from write transistors. g, Output pulse width of the HardSigmoid \ncharge-to-pulse block versus summed input current ∑iQi ⋅ Ki. h, Output pulse \nwidth and sign from the signed charge-to-pulse block versus summed input \ncurrent ∑iϕ(Si) ⋅ Vi. All simulations assume VDD = 0.9 V.\nNature Computational Science | Volume 5 | September 2025 | 813–824 816\nArticle https://doi.org/10.1038/s43588-025-00854-1\nvoltage (Vstore), as shown in Fig. 1e. This current is enabled only during \nthe input pulse, which gates it onto the shared bitline, where currents \nfrom multiple cells are summed according to Kirchhoff’s law.\nIn each inference step, both arrays are updated with one  \ncolumn from the key and value matrices, as we will show in more  \ndetail in the section ‘ Analog hardware sliding window attention data- \nflow’ . The M columns of each array represent the K  and V  of the  \nprevious M tokens, while the rows correspond to the d distinct  \nembedding elements.\nDue to temporal input encoding, gain-cell outputs also vary \nover time and must be integrated to compute the dot product. \nThis is performed by charge-to-pulse circuits (Fig. 1c ), which emit  \nPWM voltage pulses. The pulses’ width increase linearly with accu -\nmulated charge, up to a saturation threshold Ssat, as shown in Fig. 1g.  \nThe circuit emit pulses only for positive charge, implementing a  \nHardSigmoid activation. Further circuit details are provided in Sup -\nplementary Fig. 2.\nThe pulses representing ϕ(S) ∈ℝ M are fed as inputs to the second \ngain-cell array to perform the dot product ϕ(S) ⋅ V. A different type  \nof charge-to-pulse circuit integrates the output currents of the  \nsecond array. Unlike the first one, this signed charge-to-pulse circuit \nis capable of generating pulses for both positive and negative input \nVint\nTime\nMemory\nQ0\nQ1\nQ2\nQ3\nQ4\nQ5\nK0 K1 K2 K3 K4 K5\nWindow size M\na\nTime\nMemory\nWindow size M\nTime\nMemory\nWindow size M\nb\nt0\nCOUNTER \nWrite V i Write K i + 1\nMAC ϕ(S i) · V \n DIGITAL ADD\n15 ns\n65 ns\nt\nc\nMAC Q i · KT\nRSTK\n RSTV\n10 ns\nQ 0,1\nQ 0,0\nInput pulses\nA 0,1\nA 0,0\nOutput pulses\n∑I \nS0,0 S0,1 S0,2\nWrite voltages\nK0,0\nK0,1\nWrite voltages\nV 0,1\nV 0,0\nQ 1,1\nQ 1,0\nInput pulses\nA 1,1\nA 1,0\nOutput pulses\n∑I \nS1,0 S1,1 S1,2\nWrite voltages\nK1,0\nK1,1\nWrite voltages\nV 1,1\nV 1,0\nQ 2,1\nQ 2,0\nInput pulses\nA 2,1\nA 2,0\nOutput pulses\n∑I \nS2,0 S2,1 S2,2\nWrite voltages\nK2,0\nK2,1\nWrite voltages\nV 2,1\nV 2,0\nt0  +0 t0  +15\n0\n0.45\n0.90\n5 10 15 20 25 30 35 20 25 30 35 40 45 50\nRSTK RSTV COUNTERMAC Q i · KT MAC ϕ(S i) · V\nd e\nVoltage (V)\nVoltage (V)\nTime (ns) Time (ns)\nPulse\nV int\nPulse\nSign\n0\n0.45\n0.90\nQ 0\nQ 1\nQ 2\nQ 3\nQ 4\nQ 5\nK0 K1 K2 K3 K4 K5\nQ 0\nQ 1\nQ 2\nQ 3\nQ 4\nQ 5\nK0 K1 K2 K3 K4 K5\nM A C  ϕ( Si)  · V \nFig. 2 | Analog hardware attention pipeline. a, Three inference steps of a dot \nproduct between Q and K in sliding window attention. The gray boxes represent \ntokens that are attended to and the blank boxes represent the masked tokens. \nb, Equivalent gain-cell-array implementations for an entire attention head. In \nevery inference step, a new column (pointed by red arrows) of K and V is written \nbefore the queries Q are applied at the input. S are the currents summed at the \nbitlines, ∑I represents current integration in charge-to-pulse circuits, and A are \nthe outputs. c, Proposed temporal pipeline. The process begins by resetting \nthe charge-to-pulse readout capacitors for the K array (RSTK). While Q pulses \nare applied to compute S = Q ⋅ KT, the V values for the current token are written \nin parallel to the V array. After the write, the V readout is reset (RSTV), and the \nresulting ϕ(S) pulses from the charge-to-pulse circuits are applied to the V array \nto compute ϕ(S) ⋅ V. COUNTER digitizes the final pulse width and sign, and a \ndigital adder combines results from multiple sub-tiles to produce the attention \noutput A. d,e, Transient simulation of the ϕ(Q ⋅ KT) multiply–accumulate (MAC) \noperation (d) and the ϕ(S) ⋅ V MAC operation including temporal location (e). \nVint is the charge integrated by the charge-to-pulse circuits, ‘Pulse’ is their output \nsignal and ‘Sign’ is the signal current for the counter within the pipeline.\nNature Computational Science | Volume 5 | September 2025 | 813–824\n 817\nArticle https://doi.org/10.1038/s43588-025-00854-1\ncharges, while a D flip-flop stores the result’s sign. The behavior of this \ncircuit for different inputs is highlighted in Fig. 1h. A 16-level digital \ncounter measures the generated pulse widths and multiplies the result \nby the retrieved sign bit, resulting in a total precision of 32 levels.\nAnalog hardware sliding window attention data-flow\nHaving described how inference is performed for one token, we now \ndescribe how the architecture processes multiple tokens sequentially. \nIn sliding window attention, the input query is multiplied only with  \nthe M most recent keys and values, corresponding to the window  \nsize M (Fig. 2a). At each time step, the keys and values must be  \nupdated with the most recent token and the oldest one must be  \nforgotten. All other projections remain stationary until they are \nupdated after M  cycles. In our implementation, we write the array \nthat encodes the keys and values at inference time in a column-wise \nmanner (Fig. 2b).\nFigure 2c illustrates the sequential execution of inference steps \nin the hardware performing sliding window attention. Read and write \noperations are interleaved for efficiency, as further detailed in ‘ Analog \nsliding window attention timing and execution’ in Methods. T o perform \nattention on sliding window sizes and embedding dimensions larger \nthan a single array can support, sub-tiling is used to stack multiple \narrays, as shown in Fig. 3, and detailed in ‘Sub-tiling to scale attention \ndimensions’ in Methods.\nPre-trained model hardware-aware mapping and fine-tuning\nUsing weights from pre-trained models is challenging because  \nour attention mechanism differs from the conventional ones (Fig. 4a). \nThe main differences are:\n• HardSigmoid activation used instead of softmax (Fig. 1b).\n• Sliding window attention is implemented instead of causal atten-\ntion (Fig. 2a).\n• Input, stored projections and output are quantized in four, three \nand five bits, respectively, by digital PWMs, DACs and pulse coun-\nters (Fig. 1b).\n• Gain-cell arrays are split into sub-tiles before final result summa-\ntion (Fig. 3a).\n• The relation between gain-cell input and stored voltages is non-\nlinear (Fig. 1e).\n• Capacitor leakage causes stored value decay (Fig. 1f).\nThe implementation of these hardware constraints in our simula -\ntions is explained in ‘Hardware-based neural network simulations’ in \nMethods. As the nonlinear relation between input voltage and stored \nvoltage in gain cells is described by a third-order polynomial func -\ntion, this substantially increases the computational complexity and \nmemory requirements to train our gain-cell-based model. Therefore, to \nadapt the pre-trained public GPT-2 model to our hardware constraints,  \nHardSigm\n<64,5b>\nSub-tile 16\nGain cell\nHardSigm\n∑I\n∑I\nVj\nGain-cell\narray\n64 × 64\nDAC\n    KjQ\nDAC\nDigital PWM AnalogSignal types:\nCOUNT COUNT\nSub-tile 2\nGain cell\nHardSigm\nDAC DAC\nGain-cell\narray\n64 × 64\nSub-tile 1\nGain cell\nCOUNT\nDAC\n DAC\nGain-cell\narray\n64 × 64\nA\n<64,4b>\nADDRW(K)\nQ\nKj\nVj\nPWM \nADDRW(V)\n<64,1b> <64,4b>\n<64,4b>\nWrite address controller \na b\nWE\nWL_W\nWL_R\nBL\nWrite DACs\nHardSigm ∑I    Pulse \nSign ∑I    Pulse \nK row address V row address\nc d\n200 µm\n240 µm\n1,200 µm\n70 µm\n1 µm\nϕ(S) · V\nMAC\nQ · K T\nMAC\n∑I\narray\n64 × 64\narray\n64 × 64\narray\n64 × 64\n∑I\n∑I\n∑I\nFig. 3 | Multi-tile design and layout for multi-head attention. a, High-level \narchitectural diagram of a hardware unit implementing 1 attention head, \nsupporting a sequence length (M) of 1,024 and embedding dimension (d) of 64. \nLabels of the form ‘<d,pb>’ denote a d-element vector with p-bit digital precision. \nThe design is partitioned into 16 identical sub-tiles, each integrating two 64 × \n64 gain-cell memory arrays to store the K and V projections and perform dot \nproducts. Input queries Q are encoded using PWM, while K and V are converted \ninto analog voltages via DACs. A write address controller selects the active \nmemory row for K and V using ADDRW(K) and ADDRW(V). The result passes \nthrough a nonlinear activation function (HardSigm), and is re-encoded into \nPWM and routed to the next array within the sub-tile. The final PWM output is \ndigitized using a counter (COUNT) block. Outputs from all sub-tiles are summed \nby a digital adder to yield the attention result A. Digital logic is shown in yellow, \nPWM signals in green and intermediate analog voltages in red. b, Physical layout \ncorresponding to a, showing 16 sub-tiles in the middle with shared digital logic \nat the top and bottom. Memory arrays are based on compact 6-transistor gain \ncells, each occupying approximately 1 μm2. The layout is synthesized, placed \nand routed using Synopsys tools and shown with its default color scheme. c, \nZoomed-in floorplan of a sub-tile, showing vertically stacked memory arrays, \nactivation blocks and DACs. Blue and green lines indicate input and output \nsignal paths, respectively. d, Routing scheme for converting signal orientation \nbetween vertical and horizontal. Write DAC signals arrive vertically and connect \nto vertically oriented word lines (WL_R, blue) in the Q ⋅ KT array. The array’s output \nBLs (green in the bottom array) are routed horizontally. T o feed these signals \nto the vertically stacked HardSigmoid activation block, diagonal wire tapping \nredirects the horizontal bitlines upward and reorients them for vertical input. \nThe same scheme applies to the ϕ(S) ⋅ V array. Write enable (WE, pink) and write \nword lines (WL_W, orange) indicate programmable rows.\nNature Computational Science | Volume 5 | September 2025 | 813–824 818\nArticle https://doi.org/10.1038/s43588-025-00854-1\nwe first fine-tune it using an intermediate model. The intermediate \nmodel employs ideal linear dot products, but integrates all the other \nmentioned hardware constraints. The model is trained on predicting the \nnext words of the open-source text collection OpenWebT ext44, and the \nmetric used for evaluation is perplexity, which measures the uncertainty \nof the prediction. In Fig. 4d, we see that our linear intermediate model \n(blue curve) achieves results equivalent to a public GPT-2 model in less \nthan 3,000 iterations, whereas it takes more than 13,000 iterations for \nthe model trained from scratch (magenta curve). This result shows that \nperforming weight transfer is efficient even though the two models are \ndifferent (in particular, HardSigmoid activation instead of softmax).\nAfter fine-tuning the intermediate linear model, we transfer the \nweights to the final hardware model including the gain cell’s non-\nlinearity. This mapping is non-trivial, as all the layers have different \nstatistics, making it difficult to apply a single fit to capture the gain  \ncells’ nonlinearity. T o circumvent this issue, we introduce scaling  \noperations and an adaptation algorithm described in ‘Nonlinear model \nadaptation algorithm’ in Methods. In Fig. 4c, we show how the perplexity  \nof the nonlinear gain-cell model is reduced from 1,757 to 21 during this \nadaption stage. In Supplementary Fig. 5, we show that this adaptation \nalgorithm can generalize to other multiplication nonlinearities.  \nAfter the adaptation algorithm, we can fine-tune the nonlinear model \nusing backpropagation (Fig. 4d, green curve) to further improve the \nresults. The entire process is described in Fig. 4a.\nDownstream task benchmarks\nT o evaluate the proposed hardware attention mechanism, in Table 1, \nwe benchmark two software baselines and three hardware models \non standard language modeling tasks (see details in ‘Downstream \ntasks set-up’ in Methods). Our nonlinear hardware model, adapted \nfrom a linear baseline and fine-tuned, achieves accuracy compara -\nble to the public GPT-2 model, and equal or better performance than \na software model trained from scratch under the same conditions.  \nWe further observe that omitting nonlinearity-specific fine-tuning \nyields near-identical results on most tasks, except LAMBADA and \nWikiT ext-2. T o test scalability, we apply the same training set-up \nas GPT-2-XL (1.5 billion parameters). While the hardware version \nfalls slightly short of the public checkpoint, it clearly outperforms \nthe smaller GPT-2 baseline and matches the from-scratch software \nGPT-2-XL. This indicates that remaining performance gaps are due to \nb\ny = ax + b\ny = ax + b\ny = a'x + b'\nLinear hardware model\nNonlinear hardware model\nbefore adaptation\nNonlinear hardware model\nafter adaptation\nx\nc d\nWeight transfer +\nfine-tuning \nWeight transfer +\n(a, b) adaptation +\nfine-tuning \nLinear\ndot product\ny = x · d 1/2\nSoftmax\nQ\nLinear\ndot product\nOutput\nK V\nPre-trained\nsoftware model\nLinear hardware model\ny = ax + b\nQ VK\nClip[0, 1]\nLinear\ndot product\nClip[0, 1]\nLinear\ndot product\ny = x/S sat\nClip[–1, 1]\nQuantize\nOutput\nQuantize\ny = ax + b\nNonlinear hardware model\ny = a’x + b’\nQ VK\nClip[0, 1]\nNonlinear\ndot product\nClip[0, 1]\nNonlinear\ndot product\ny = x/S sat\ny = x/S sat\nClip[–1, 1]\nQuantize\nOutput\nQuantize\ny = a’x + b’\n–Ssat\nNonlinear model\nAdaptation iterations\n0 8 16\n20\n20\n25\n0\n0\n4,000\n4,000\n30\n40\n24\nBackpropagation iterations\nPerplexity\nPerplexity\nSoftware model trained from scratch\nNonlinear model trained from scratch\nLinear model fine-tuned from GPT-2\nNonlinear model fine-tuned from linear model\nSsat\nSsat\na\nClip\nClip\nClip\ny\n0 0 . 5 1 . 00 0 . 5 1 . 0\nx y\n0 0 . 5 1 . 00 0.5 1 . 0\nx y\n0 0 . 5 1 . 00 0.5 1 . 0\n8,000\n8,000\n12,000\n12,000\n10 2\n10 4\n10 6\ny = x/S sat\n–Ssat Ssat\nSsat\nFig. 4 | Hardware model adaptation and training. a, Pre-trained model \nmapping. Q, K and V are the input projections. d is the embedding dimension, \nSsat is the charge-to-pulse threshold, and a and b are trained scaling parameters. \nFrom a software pre-trained model, we fine-tune an intermediate model that \nintegrates all hardware constraints except dot-product nonlinearity. Then, \nwe use a custom adaptation algorithm to map the intermediate model to the \ngain cell’s nonlinearity. Finally, we fine-tune the nonlinear model. b, Sketch \nof the adaptation algorithm for scaling factors. Scaling factors re-scales the \ninput before clipping and quantization. The nonlinear model leads to different \nstatistics (red histogram) than the linear model (green histogram). The \nadaptation algorithm modifies the scaling factors to match the statistics of the \nnonlinear model to the statistics of the linear one. c, Evolution of perplexity \n(lower the better) during the adaptation algorithm. d, Training curves for the \ndifferent models. The software model is GPT-2, the nonlinear model is the model \nwith the proposed hardware attention, and the linear model is the hardware \nattention with ideal linear gain cells. The inset provides a magnified view of the \nmain training curves to emphasize finer differences in model convergence.\nNature Computational Science | Volume 5 | September 2025 | 813–824\n 819\nArticle https://doi.org/10.1038/s43588-025-00854-1\ntraining iteration differences (the number of iterations for the public \nmodel is undisclosed), not hardware limitations.\nCircuit computing accuracy\nThe accuracy of our circuits for attention computation is highlighted \nin Fig. 5a,b. For each of the two dot products, we simulate one 64 × 64 \narray and the corresponding 64 charge-to-pulse circuits. The results of \nthe first dot product, which are shown in Fig. 5a, are fed as input to the \nsecond dot product and are shown in Fig. 5b. For each plot, we compare \nthe simulations performed with SPICE (a circuit simulation software) \nwith the model used for neural network simulations.\nEnergy consumption and latency\nThe circuit’s operational speed and timing, on which the energy \nassumptions are based, are shown in Fig. 2d. The total latency of atten-\ntion can be estimated to 65 ns.\nThe gain-cell arrays and charge-to-pulse circuits consume 1,120 pJ \nper token computation for the first dot product, and 700 pJ for the \nsecond dot product. The lower energy consumption in the second \ndot-product arrays is attributed to the sparser activation of its input \nϕ(S), leading to less current in the second gain-cell array. The digital \ncontrol and routing block consumes a total power of 113.7 mW, or 4 nJ per \ntoken, while the DACs require 330 pJ. Overall, we can estimate the power \nconsumption of processing 1 token for 1 attention head to 6.1 nJ. A pie \nchart of the power composition attributed to each unit is shown in Fig. 5e.\nThe energy and latency of our architecture, compared with three \ndifferent GPUs, are shown in Fig. 5c,d. Focusing on the attention mecha-\nnism alone, our architecture can lead to a speed-up of ×7,000 compared \nwith Nvidia Jetson Nano, ×300 compared with Nvidia RTX 4090 and \n×100 compared with Nvidia H100, as well as an energy reduction of \n×40,000 compared with Jetson Nano, ×90,000 compared with RTX \n4090 and ×70,000 compared with H100.\nArea and floorplan\nOn the basis of our assumptions, described in ‘ Area estimation’ in  \nMethods, for the worst-case scenario, the area of the proposed gain cell \nis 1 μm2. Figure 3c shows the floorplan of a single tile, which includes \n64 shared DACs for writing the weights, 2-row address decoders and \ncharge-to-pulse circuitry. The total area of 1 head, shown in the floor-\nplan in Fig. 3b, is 500 × 10−3 mm2 including digital control circuitry.\nHowever, other studies have demonstrated substantially smaller \ngain-cell dimensions45. On the basis of this, and following the methodol-\nogy outlined in ‘ Area estimation’ in Methods, we estimate that the area \nof the gain-cell crossbars required for the entire GPT-2 attention-head \nKV cache is approximately 15.7 × 10−3 mm2, excluding digital control \ncircuitry.\nIn Supplementary Fig. 7, we show that multiple attention heads \ncan be executed using parallel tiles on-chip and stacked in 3D with \nmultiple layers, sharing peripheral and digital logic. As discussed in \n‘ Area estimation’ in Methods, 3D stacking can further improve area \nefficiency. On the basis of ref. 45, we estimate the total area required \nfor a GPT attention-head KV cache, excluding digital control, to  \nbe \n36.7\nN\n×10−3 mm2, where N denotes the number of vertical stacks.  \nThe resulting area is:\n• 36.7 × 10−3 mm2 for N = 1\n• 9.2 × 10−3 mm2 for N = 4\n• 4.6 × 10−3 mm2 for N = 8\n• 3.1 × 10−3 mm2 for N = 12\nDiscussion\nIn this work, we proposed an analog IMC architecture addressing the \nenergy consumption and latency bottlenecks of the attention compu-\ntations at the core of generative AI models.\nOur design leverages capacitor-based gain cells, offering an  \nefficient solution for both memory storage and computation, \nsub stantially improving energy efficiency and speed. T o avoid \npower-intensive ADCs, we perform the attention computation in the \nanalog domain, using charge-to-pulse circuits to transmit analog sig-\nnals between computation stages. This approach introduces non-ideal \noperations compared with digital attention computations, but with \nsubstantial efficiency gains. Another contribution is a hardware-aware \nTable 1 | Downstream task results\nARC-E ARC-C WinoGrande HellaSwag LAMBADA LAMBADA PIQA WikiText-2 Average Average\nacc ↑ acc ↑ acc ↑ acc ↑ ppl ↓ acc ↑ acc ↑ ppl ↓ acc ↑ ppl ↓\nPublic software \nmodel\n43.81 22.70 51.62 31.14 35.15 45.96 62.89 37.37 43.02 36.26\nSoftware model \ntrained from \nscratch\n42.34 23.46 50.20 29.73 46.39 41.56 61.48 41.25 41.46 43.82\nLinear hardware \nmodel\n42.80 23.46 52.41 30.31 51.69 38.10 61.21 39.79 41.38 45.74\nNonlinear \nhardware model \nwith adaptation\n42.09 22.87 50.51 30.10 76.59 31.61 61.53 42.34 39.79 59.47\nNonlinear \nhardware model \nwith adaptation \nand fine-tuning\n43.94 22.78 51.14 30.18 43.08 40.16 62.62 39.97 41.80 41.52\nPublic software \nmodel-XL\n58.29 (+14.48) 28.50 (+5.80) 58.33 (+6.71) 50.89 (+19.75) 9.68 (−25.47) 63.87 (+17.91) 70.84 (+7.95) 20.38 (−16.99) 55.12 (+12.10) 15.03 (−21.23)\nSoftware model \ntrained from \nscratch-XL\n53.82 (+11.48) 25.76 (+2.30) 53.75 (+3.55) 42.54 (+12.81) 14.82 (−31.57) 56.33 (+14.77) 68.71 (+7.23) 24.98 (−16.27) 50.15 (+8.69) 19.90 (−23.92)\nLinear hardware \nmodel-XL\n54.08 (+11.28) 27.47 (+4.01) 57.93 (+5.52) 45.51 (+15.20) 12.32 (−39.37) 58.54 (+20.44) 68.01 (+6.80) 23.26 (−16.53) 51.92 (+10.54) 17.79 (−27.95)\nNonlinear \nhardware \nmodel-XL\n53.79 (+9.85) 27.30 (+4.52) 54.70 (+3.56) 46.70 (+16.52) 12.17 (−30.91) 59.48 (+19.32) 68.17 (+5.55) 22.29 (−17.68) 51.69 (+9.89) 17.23 (−24.29)\nThe metrics are the percentage of accurate word predictions (acc), and the perplexity (ppl), a measure of prediction uncertainty. The last two columns average the accuracy results and the \nperplexity results for each model, respectively. Values in parentheses (±x) indicate the improvement of XL models relative to their smaller counterparts (GPT-2-XL results − GPT-2 results). Rows \nin bold correspond to our results.\nNature Computational Science | Volume 5 | September 2025 | 813–824 820\nArticle https://doi.org/10.1038/s43588-025-00854-1\ntraining methodology compensating for the circuit non-idealities. \nNonetheless, future circuit optimizations could further reduce any \ndiscrepancies.\nOur neural network simulations confirm that an LLM imple -\nmented with our hardware attention achieves results comparable to \nsoftware-based networks, even on complex NLP tasks. Nonetheless, \nour larger network slightly underperforms the baseline, and therefore \ndeeper neural network training will require further methods to mitigate \nthe vanishing gradient issue due to clamping values. This slight perfor-\nmance gap should still be put in perspective with the reduced energy \nconsumption. While our study uses device-level simulations to evaluate \ndesign performance, our adaptation algorithm demonstrates potential \nfor measured device implementations, as it allows most of the training \nprocess to proceed without requiring precise device-specific models \nof nonlinear behavior, making the approach generically applicable and \ncomputationally efficient.\nOur architecture can benefit from OSFET transistors that enable \ndense 3D integration45,46. Moreover, the KV-cache size grows modestly \ncompared with the overall models’ parameters count14,15,47. Our system \ncould therefore be applied to larger networks with a moderate area \nfootprint. Latency is reduced by up to two orders of magnitude, and \nenergy consumption by up to four orders for attention computations \nalone compared with GPUs. While we focus on the attention mechanism, \na major bottleneck in generative transformers’ inference, substantial \nreductions in overall energy consumption require optimizing all com-\nponents. In the future, our hardware attention mechanism can be inte-\ngrated with other IMC techniques to implement low-power linear layers.\nIn conclusion, this work demonstrates hardware-algorithm \nco-optimization achieving low latency and energy consumption while \nmaintaining high model accuracy. In addition, it highlights the promise \nof IMC with volatile, low-power memory for attention-based neural \nnetworks, marking an important step toward ultrafast, energy-efficient \ngenerative AI.\nMethods\nHardware-based neural network simulations\nWe implement the sliding window attention by masking the elements \nof S outside the sliding window (blank spaces in the example Fig. 1 ). \nThe HardSigmoid charge-to-pulse circuit is modeled by the equation\nϕ(S)=\n⎧⎪\n⎨⎪\n⎩\nTmax ifS≥ Ssat\nTmax\nSsat\nS if0 < S< Ssat\n0 if S≤ 0\n, (3)\nwhere Tmax = 15 ns is the maximum pulse length for the input pulse \ngenerators. The input queries Q are quantized in 16 levels between 0 \nand 1, the stored K and V projections are quantized in 8 levels between \n0 and 0.9, and the outputs of the second dot product are quantized in \n32 levels between −1 and 1. The quantized models (linear intermedi -\nate hardware model and nonlinear hardware model) are trained with \nquantization aware training48: quantization is done only in the forward \npass and the backward pass is done in full precision.\nFor the nonlinear model of the gain cell, the third-order \npolynomials\nS=\n3\n∑\ni\n3−i\n∑\nj\nQ⋅(KT −Koffset)\ni\nVj\ninCi,j\nA=\n3\n∑\ni\n3−i\n∑\nj\nϕ(S)⋅(V−Voffset)\ni\nVj\ninCi,j\n(4)\nare used with S and A as the outputs, Q and ϕ(S) the input pulse width, \nK and V the stored voltage, the constant Vin = 0.9 V is the input voltage  \nof the cell applied at the word line read (WLR) ports, the constant  \nyoffset = 0.45 V corresponds to half the supply voltage (V DD/2), and Ci,j  \nas fit parameters from the curve Fig. 1e. T o speed-up computation \nDigital and routing\nDAC\nLatency (s)\nEnergy (J)\nc d e\nNvidia RTX 4090\nNvidia H100\nNvidia Jetson Nano\nThis work\na b\nNvidia RTX 4090\nNvidia H100\nNvidia Jetson Nano\nThis work\nϕ(S) · V\nϕ(Q · KT)\nModel ϕ(S)\nSPICE simulated ϕ(S)\nSPICE simulated A\nModel A\n0.8\n0.6\n0.4\n0.2\n0\n0 0.2 0.4 0.6 –0.2 0.2–0.6\n–0.6\n–0.2\n0.2\n0.6\n0.6\n15.0%16.3%\n10.2%\n58.4%\n0.8\n10 –2\n10 –4\n10 –6\n10 –4\n10 –6\n10 –5\n10 –7\nFig. 5 | Analog hardware attention mechanism accuracy and performances. \na, Comparison of expected results model versus SPICE simulation results for the \ncharge-to-pulse circuit output ϕ(S) with S = Q ⋅ KT the results of the first crossbar \narray and ϕ the transfer function of the charge-to-pulse circuit. b, Comparison \nof PyT orch model versus SPICE simulation results for the second crossbar \narray output A = ϕ(S) · V. c,d, Latency and energy consumption per token of the \nattention mechanism for 1 processed token (c) and energy consumption for a \n12-head attention mechanism implemented by a consumer GPU, a server GPU, an \nembedded application-specific GPU and our hardware architecture (d). e, Energy \nconsumption ratio for the different modules of our hardware architecture, \nincluding analog and digital signals.\nNature Computational Science | Volume 5 | September 2025 | 813–824\n 821\nArticle https://doi.org/10.1038/s43588-025-00854-1\nduring training, we compute all the tokens in parallel with Q∈ℝ T,D, \nKT ∈ℝ D,T, V∈ℝ T,D and ϕ(S) ∈ℝ T,T (the batch dimension and the  \nhead dimension are omitted for simplicity).\nThe capacitor leakage leads to an exponential decay in the stored \nvalue. After discretization, the exponential decay is formulated as\nyt = yt−1e−\nΔt\nτ ; Δt = Lδt, (5)\nwhere τ is the time constant of the capacitors, Δ t is the time elapses \nbetween two inference steps, δt is the latency caused by each neural \nnetwork layer, and L is the number of layers. T o model the decay of all \ncapacitors at all time steps in parallel, we introduce a decay mask \nα∈ℝ T,T defined as\nα= e−\nΔt\nτ\nmt,t′\n; mt,t′ = max(0,t−t′), (6)\nwhere m is the relative tokens’ position. T o optimize computation, the \ndecay mask is directly integrated in the dot-product computation as\nS=\n3\n∑\ni\n3−i\n∑\nj\n(Q⋅(KT −Koffset)\ni\nVj\ninCi,j)αi\nA=\n3\n∑\ni\n3−i\n∑\nj\n(ϕ(S)αi)⋅(V−Voffset)\ni\nVj\ninCi,j\n(7)\nIn our simulation, we chose a time constant τ = 5 ms to be consist-\nent with the data from Fig. 1h. We chose δ t = 65 ns to be equal to  \nthe latency of our full hardware attention mechanism (Fig. 2c). Our  \ndecay factor is therefore \nΔt\nτ\n=\n12×65×10−9\n5×10−3\n≃ 1.6×10−4. In a full trans -\nformer implementation, the latency per layer δ t = will be higher  \nthan 65 ns as it will also include latency from other modules, such as \nfeedforward neural networks. However, time constant τ  of three  \norders of magnitude larger were reported in OSFET-based gain-  \ncell memories26,29, and therefore we conclude that the choice of decay \nfactor of 1.6 × 10 −4 is very conservative. In Supplementary Fig. 6, we \nstudy empirically the effect of the decay constant over language pro-\ncessing accuracy. It is noteworthy that the decay of stored keys and \nvalues may not necessarily hinder network performance: several \napproaches in deep learning leverage exponential decay masks to \nenhance memory structure39,49. In Supplementary Information section \n‘Effect of capacitor’s leakage’ , we study the connection between the  \nKV pairs decay and the relative positional embedding called AliBi49.\nT o speed up our training process, we used the library Triton 50 \nto incorporate our simulations into an adapted version of the flash  \nattention algorithm51, which optimizes the GPU resources. This method \nled to a factor of five latency reduction during training.\nFor the adaptation, the algorithm was repeated until the mean  \nand standard deviation of the output of the scaling functions of the \nnonlinear model matches the mean and standard deviation of the  \nlinear model within a tolerance ratio: |σNL −σL| < 0.0001 and |μNL −μL|\n< 0.0001.\nNonlinear model adaptation algorithm\ny= ax+b (8)\nwith distinct scalars a and b for each of the Q, K and V projections, as \nwell as for the output of the attention, with separate factors applied \nacross different attention heads and layers.\nT o choose the scaling parameters a and b, we develop an algorithm \ninspired by ref. 52, detailed in Supplementary Algorithm 1. Given a set \nof input samples, we use an iterative loop that updates the scaling para-\nmeters so that the output of the scaling function of the nonlinear model \nmatches the statistics of the linear model (as sketched in Fig. 4b). First, \nwe measure the standard deviation σL and the mean μ L of the output \nof every scaling stage (see equation (8)) of the linear model on a large  \nset of samples. Then, at each iteration, we measure the standard  \ndeviation σNL and the mean μ NL for the scaling stage of the nonlinear \nmodel. For each iteration, the scaling parameters are updated as\na← a\nσL\nσNL\nb← b+( μL −μNL)\n. (9)\nAnalog sliding window attention timing and execution\nT o support efficient sequential inference, our architecture implements \nsliding window attention using a pipelined read–write mechanism \nacross analog gain-cell arrays. At each inference step, new (K, V) pairs \nare written into the arrays while the current query (Q) is applied, ensur-\ning that memory access and computation overlap.\nEach attention step begins with a 5 ns discharge phase to reset  \nthe storage capacitors of the gain cells. New K and V vectors are  \nwritten to a column of the respective arrays using 10 ns multi-level \nvoltage pulses generated by 3-bit DACs. In parallel, the input query Q \nis encoded as PWM voltage pulses with durations between 0 ns and \nTmax = 15 ns, generated by 4-bit (16 levels) voltage pulse generators \noperating at 1 GHz.\nThis parallelization is possible because the V array is not required \nduring the Q  ⋅ KT computation phase and can therefore be updated \nwhile the first dot product is processed. Once the write is complete, the \ncharge-to-pulse circuit for the V array is reset, and the resulting ϕ(S) \npulses from the K array’s readout are applied to the V array to compute \nthe second dot product ϕ(S) ⋅ V.\nAfter M time steps, when all columns in the K and V arrays have been \npopulated, the first column is overwritten, preserving a sliding atten-\ntion window of fixed size M. The succession of write and read phases \nimplements a sequential sliding window attention mechanism, with \nminimal idle time and continuous throughput. This pipelined execu-\ntion scheme is visualized in Fig. 2c, and forms the basis for the latency \nand energy analysis presented in later sections.\nSub-tiling to scale attention dimensions\nIR drop, caused by resistive losses in interconnects, results in reduced \naccuracy in large-scale analog crossbar arrays 53. T o mitigate IR drop \nissues, we limit the size of our gain-cell arrays to 64 × 64. However, most \nNLP applications require larger either a larger window dimension M \n(columns) or a larger embedding dimension d (rows). T o accommodate \nlarger dimensions, we perform inference across multiple sub-tiles, as \nshown in Fig. 3a.\nIn this paper, we implement a GPT-2 model with an embedding \ndimension d = 64 and a sliding window size M = 1,024. Therefore, the \nentire KV cache of the window size M is divided into 16 sub-tiles, each \nhaving its charge-to-pulse blocks and storing a fraction of the K and \nV in two 64 × 64 arrays. A write address controller keeps track of the \ncurrent write index. All tiles receive the same input Q generated by the \ndigital block in parallel, are measured by pulse counters and summed \nby 64 digital adders, each with 16 inputs (Fig. 3b,c). In sliding window \nattention, the maximum attention span is equal to L(M − 1) + 1 (ref. 43). \nTherefore, in the presented architecture, the maximum attention span \ncan be increased by increasing the number of sub-tiles. However, this \nleads to additional area footprint scaling linearly with the sliding win-\ndow dimension, and additional latency as each digital adder requires \none clock cycle.\nHardware-based neural network training\nT o evaluate our training algorithm and the inference accuracy of  \nour architecture, we implement the analog gain-cell-based atten -\ntion mechanism on the GPT-2 architecture 54. GPT-2 is a transformer  \nneural network with 124 million parameters, 12 layers, an attention \nmechanism input dimension of 768, 12 heads per attention block \nNature Computational Science | Volume 5 | September 2025 | 813–824 822\nArticle https://doi.org/10.1038/s43588-025-00854-1\nand a head dimension of 64. We used the open-source text collec -\ntion OpenWebT ext44 split between training and testing samples, and  \nthe pre-trained GPT-2 tokenizer to encode the plain text into tokens \n(vectors of size 50,304 each). Each training iteration had a batch size  \nof 1,920, with sequences of length 1,024 per sample. We selected a  \nsliding window size of 1,024, which matches the number of gain-cell \nrows in the memory. As the sequence length also equals 1,024, each  \ngain cell is written only once per sequence, eliminating the need to over-\nwrite cells during one sliding window iteration. For a larger sequence \nlength, the gain cells would be overwritten, as described in the section \n‘ Analog hardware sliding window attention data-flow’ . T o train the \nnetwork, the next token in the sequence is predicted for each input \ntoken. Thus, the target sequences are the input sequences shifted \nby one token. The cost function used was cross-entropy, calculated \nbetween the predicted sequence and the target sequence. We used \nbackpropagation with the AdamW optimizer55, with a learning rate of \n6 × 10−4 and a weight decay of 0.1. The results of each evaluation are \naveraged over 4,000 samples.\nDownstream tasks set-up\nThe datasets cover various types of problem. Our benchmarking \nset-up is inspired by refs. 11,56 in terms of evaluated tasks and metrics.  \nARC-Easy and ARC-Challenge 57 focus on question answering, with \nARC-Easy containing straightforward questions and ARC-Challenge \nfeaturing more difficult ones. WinoGrande58 evaluates common-sense \nreasoning and co-reference resolution by presenting minimal pairs \nto resolve ambiguities. HellaSwag 59 tests common-sense inference, \nrequiring models to predict the most plausible continuation of a given \ncontext. LAMBADA60 evaluates models’ text understanding through \na word prediction task that requires comprehension of broader dis-\ncourse, not just local context. PIQA61 assesses physical common-sense \nreasoning, testing a model’s understanding of physical scenarios. \nWikiT ext-262 is a general text corpus derived from Wikipedia articles \nto assess long-term dependencies processing, text prediction and \ngeneration capabilities. For WikiT ext-2, we report perplexity scores \nnormalized by the word count in the original text. For fair compari-\nsons, except for software public GPT-2, all the models were evaluated \nafter the same number of training iterations. The linear hardware \nmodel was trained on 13,000 iterations, the nonlinear hardware \nmodel was mapped from the 13,000 iterations linear model using \nthe adaptation algorithm but without fine-tuning, and the nonlinear \nhardware model with adaptation and fine-tuning was adapted from \na linear model trained on 3,000 iterations, and then fine-tuned on \n10,000 iterations.\nHardware SPICE simulations\nT o assess circuit performance accuracy, energy consumption and \nspeed, we conducted SPICE array simulations using the TSMC 28 nm \nPDK within the Cadence Virtuoso environment. All simulations are \nbased on a 64 × 64 array, corresponding to the tile size in our architec-\nture (Fig. 3a). T o extrapolate the energy and latency for a full attention \nhead with a window size of 1,024, we multiply the per-sub-tile meas -\nurements by 16, reflecting the total number of sub-tiles comprising 1 \nattention head in our architecture. In these simulations, a parasitic wire \ncapacitance of 0.8 fF and a series resistance of 2 Ω per array element are \nincluded. Both arrays, one performing ϕ(Q ⋅ KT) and the other perform-\ning ϕ(S) ⋅ V, are simulated separately, but always in combination with \ntheir specific charge-to-pulse circuitry readout circuitry.\nGPU attention latency and energy consumption \nmeasurements\nT o measure the latency and energy on Nvidia RTX 4090, Nvidia H100 \nand Nvidia Jetson Nano, which are a consumer GPU, a data-center GPU \nand an embedded application GPU, respectively, we perform 10 runs \nof 1,024 steps of autoregressive token generation with 12 attention \nheads using the method FlashAttention-251, which optimizes attention \ncomputation in GPUs. The energy and latency consumption measure-\nment solely focus on attention computation, and for a fair compari -\nson, the linear projections are not implemented in this experiment as  \nthey are also not implemented by our hardware architecture, and  \nthe static power measured before inference is subtracted from the \npower measured during inference. For each run, we measure the latency \nand the power using the Nvidia-SMI python API, and average them.\nArea estimation\nOur floorplan is based on ITO gain cells, an emerging OSFET technology \nthat has enabled low-area gain-cell designs45. A two-transistor ITO gain \ncell occupies an area of 0.14 μm2 (approximately 370 nm × 370 nm)45, \nallowing for denser memories than CMOS-based gain cells. On the \nbasis of the area results presented in these studies 45,46, we estimate \nthe worst-case area of the proposed 6-transistor cell to be 1 μm2, lead-\ning to a 19× area reduction compared with gain cells based on CMOS \nwrite transistors (our CMOS-based gain-cell layout is presented in \nSupplementary Fig. 1). The total area of 1 attention head is derived \nfrom this single-cell area estimation, as well as the charge-to-pulse \ncircuit layout and the total floorplan incorporating the 16 sub-tiles \nand digital circuits, providing a precise representation of the space \nrequirements. This structure is designed to be repetitive (vertical \ndimension in Fig. 3c ), allowing multiple attention heads to be effi -\nciently integrated on a single chip. Each attention head receives inputs \nfrom the lower digital block, while its outputs are processed by the \nupper digital block. T o facilitate the connection of the bitline outputs \nof one array (that is, vertical metal lines) to the wordline input of the \nnext array (that is, horizontal metal line), we employ wire tapping, as \nhighlighted in Fig. 3d.\nWhen considering 3D-stacked gain cells, the effective cell area \nis reported in ref. 45 as 0.14/N μm2, where N denotes the number of \nparallel oxide layers. Consequently, a signed gain-cell implementation \nwould occupy 0.28/N μm2, consisting of 2 gain cells, 1 for the positive \npart and 1 for the negative part.\nData availability\nThe data supporting the figures of this study are publicly available in \na figshare repository63. Source data for Figs. 1, 2, 4 and 5 are available \nwith this paper. Data for Figs. 1, 2 and 5 were generated through simu-\nlations using SPICE. Data for Fig. 4  were produced using evaluations \nperformed in the PyT orch framework. Data for Table 1 were obtained \nusing the Language Model Evaluation Harness toolkit64.\nCode availability\nThe Python scripts used for the experiments are available without \nrestriction at https://github.com/NathanLeroux-git/GainCellAtten-\ntion/, and are archived with a DOI in the Zotero repository65.\nReferences\n1. Vaswani, A. et al. Attention is all you need. In Proc. 31st \nInternational Conference on Neural Information Processing \nSystems, NIPS’17 6000–6010 (Curran Associates, 2017).\n2. Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by \njointly learning to align and translate. Preprint at http://arxiv.org/\nabs/1409.0473 (2016).\n3. Lin, T., Wang, Y., Liu, X. & Qiu, X. A survey of transformers. AI Open \n3, 111–132 (2022).\n4. Pope, R. et al. Efficiently scaling transformer inference.  \nProc. Mach. Learn. Syst. 5, 606–624 (2023).\n5. Liu, Z. et al. KIVI: a tuning-free asymmetric 2bit quantization for KV \ncache. In Proc. 41st International Conference on Machine Learning, \nICML’24 Vol. 235, 32332–32344 (JMLR.org, 2024).\n6. Jiang, A.Q. et al. Mistral 7B. Preprint at http://arxiv.org/abs/ \n2310.06825 (2023).\nNature Computational Science | Volume 5 | September 2025 | 813–824\n 823\nArticle https://doi.org/10.1038/s43588-025-00854-1\n7. Jouppi, N. P. et al. Ten lessons from three generations shaped \nGoogle’s TPUv4i: industrial product. In Proc. 2021 ACM/IEEE 48th \nAnnual International Symposium on Computer Architecture (ISCA) \n1–14 (IEEE, 2021); https://doi.org/10.1109/ISCA52012.2021.00010\n8. Fu, Y. Challenges in deploying long-context transformers:  \na theoretical peak performance analysis. Preprint at https://arxiv.\norg/abs/2405.08944 (2024).\n9. Xu, M. et al. Resource-efficient algorithms and systems of foundation \nmodels: a survey. ACM Comput. Surv. 57, 110–111039 (2025).\n10. Katharopoulos, A., Vyas, A., Pappas, N. & Fleuret, F. Transformers \nare RNNs: fast autoregressive transformers with linear attention. \nIn Proc. 37th International Conference on Machine Learning, \nICML’20 Vol. 119, 5156–5165 (JMLR.org, 2020); https://doi.org/ \n10.5555/3524938.3525416\n11. Gu, A. & Dao, T. Mamba: linear-time sequence modeling with \nselective state spaces. In Proc. Conference on Language Modeling \n(2024); https://openreview.net/forum?id=tEYskw1VY2\n12. Adnan, M. et al. Keyformer: KV cache reduction through key \ntokens selection for efficient generative inference. Proc. Mach. \nLearn. Syst. 6, 114–127 (2024).\n13. DeepSeek-AI et al. Deepseek-v3 technical report. Preprint at \nhttps://arxiv.org/abs/2412.19437 (2024)\n14. Chang, C.-C. et al. Palu: KV-cache compression with \nlow-rank projection. In Proc. 13th International Conference \non Learning Representations (2025); https://openreview.net/\nforum?id=LWMS4pk2vK\n15. Ainslie, J. et al. GQA: training generalized multi-query transformer \nmodels from multi-head checkpoints. In Proc. 2023 Conference \non Empirical Methods in Natural Language Processing  \n(eds Bouamor, H. et al.) 4895–4901 (Association for \nComputational Linguistics, 2023); https://doi.org/10.18653/v1/ \n2023.emnlp-main.298\n16. Vogginger, B. et al. Neuromorphic hardware for sustainable AI \ndata centers. Preprint at https://arxiv.org/abs/2402.02521 (2024).\n17. Yang, X., Yan, B., Li, H., Chen, Y. ReTransformer: ReRAM-based \nprocessing-in-memory architecture for transformer acceleration. \nIn Proc. 39th International Conference on Computer-Aided Design, \nICCAD ’20 92 (Association for Computing Machinery, 2020); \nhttps://doi.org/10.1145/3400302.3415640\n18. Laguna, A. F. Hardware–software co-design of an in-memory \ntransformer network accelerator. Front. Electron. 3, 847069 (2022).\n19. Sridharan, S., Stevens, J. R., Roy, K. & Raghunathan, A. X-former: \nin-memory acceleration of transformers. IEEE Trans. Very Large \nScale Integr. VLSI Syst. 31, 1223–1233 (2023).\n20. Bhattacharjee, A., Moitra, A. & Panda, P. Clipformer: key–value \nclipping of transformers on memristive crossbars for write noise \nmitigation. IEEE Trans. Comput. Aided Design Integr. Circuits Syst. \n44, 592–601 (2025).\n21. Wu, Y., Wang, Z. & Lu, W. D. PIM GPT a hybrid process in memory \naccelerator for autoregressive transformers. Npj Unconv. Comput. \n1, 4 (2024).\n22. Sebastian, A., Le Gallo, M., Khaddam-Aljameh, R. & Eleftheriou, E. \nMemory devices and applications for in-memory computing.  \nNat. Nanotechnol. 15, 529–544 (2020).\n23. Zhou, M., Xu, W., Kang, J. & Rosing, T. TransPIM: a memory-based \nacceleration via software–hardware co-design for transformer. \nIn Proc. 2022 IEEE International Symposium on High-Performance \nComputer Architecture (HPCA) 1071–1085 (IEEE, 2022);  \nhttps://doi.org/10.1109/HPCA53966.2022.00082\n24. Liu, S. et al. HARDSEA: hybrid analog-ReRAM clustering and \ndigital-SRAM in-memory computing accelerator for dynamic \nsparse self-attention in transformer. IEEE Trans. Very Large Scale \nIntegr. VLSI Syst. 32, 269–282 (2024).\n25. Lepri, N. et al. In-memory computing for machine learning and \ndeep learning. IEEE J. Electron Devices Soc. 11, 587–601 (2023).\n26. Wang, Y. et al. An in-memory computing architecture based \non two-dimensional semiconductors for multiply–accumulate \noperations. Nat. Commun. https://doi.org/10.1038/s41467-021-\n23719-3 (2021).\n27. Gou, S. et al. 2T1C DRAM based on semiconducting MoS2 and \nsemimetallic graphene for in-memory computing. Natl Sci. Open \n2, 20220071 (2023).\n28. Shi, M. et al. Counteractive coupling IGZO/CNT hybrid 2T0C \nDRAM accelerating RRAM-based computing-in-memory via \nmonolithic 3D integration for edge AI. In Proc. 2023 International \nElectron Devices Meeting (IEDM) 1–4 (IEEE, 2023); https://doi.org/ \n10.1109/IEDM45741.2023.10413876\n29. Belmonte, A. et al. Lowest IOFF <3×10−21 A/μm in capacitorless \nDRAM achieved by reactive ion etch of IGZO-TFT. In Proc. \n2023 IEEE Symposium on VLSI Technology and Circuits (VLSI \nTechnology and Circuits) 1–2 (IEEE, 2023); https://doi.org/ \n10.23919/VLSITechnologyandCir57934.2023.10185398\n30. Ye, H. et al. Double-gate W-doped amorphous indium oxide \ntransistors for monolithic 3D capacitorless gain cell eDRAM. \nIn Proc. 2020 IEEE International Electron Devices Meeting \n(IEDM) 28.3.–28.3.4 (IEEE, 2020); https://doi.org/10.1109/\nIEDM13553.2020.9371981\n31. Raman, S. R. S., Xie, S. & Kulkarni, J. P. Compute-in-eDRAM \nwith backend integrated indium gallium zinc oxide transistors. \nIn Proc. 2021 IEEE International Symposium on Circuits and \nSystems (ISCAS) 1–5 (IEEE, 2021); https://doi.org/10.1109/\nISCAS51556.2021.9401798\n32. Tang, W. et al. Low-power and scalable BEOL-compatible IGZO \nTFT eDRAM-based charge-domain computing. IEEE Trans. Circuits \nSyst. I 70, 5166–5179 (2023).\n33. Lu, A. et al. High-speed emerging memories for AI hardware \naccelerators. Nat. Rev. Electr. Eng. 1, 24–34 (2024).\n34. Cai, F. et al. A fully integrated reprogrammable memristor– \nCMOS system for efficient multiply–accumulate operations.  \nNat. Electron. 2, 290–299 (2019).\n35. Wan, W. et al. A compute-in-memory chip based on resistive \nrandom-access memory. Nature 608, 504–512 (2022).\n36. Ambrogio, S. et al. An analog-AI chip for energy-efficient speech \nrecognition and transcription. Nature 620, 768–775 (2023).\n37. Vatalaro, M. et al. A low-voltage, low-power reconfigurable \ncurrent-mode softmax circuit for analog neural networks. \nElectronics https://doi.org/10.3390/electronics10091004 (2021).\n38. Dube, A., Manea, P., Gibertini, P., Covi, E. & Strachan, J. P. Analog \nsoftmax with wide input current range for in-memory computing. \nIn Proc. IEEE International Symposium on Circuits and Systems \n(ISCAS), paper 2530 (2025).\n39. Ma, X. et al. Mega: moving average equipped gated attention.  \nIn Proc. 11th International Conference on Learning Representations \n(2023); https://openreview.net/forum?id=qNLe3iq2El\n40. Ramapuram, J. et al. Theory, analysis, and best practices for \nsigmoid self-attention. In Proc. 13th International Conference \non Learning Representations (2025); https://openreview.net/\nforum?id=Zhdhg6n2OG\n41. Beltagy, I., Peters, M. E. & Cohan, A. Longformer: the long- \ndocument transformer. Preprint at https://arxiv.org/abs/ \n2004.05150 (2020).\n42. Gu, X. et al. When attention sink emerges in language models: \nan empirical view. In Proc. 13th International Conference on \nLearning Representations (2025); https://openreview.net/\nforum?id=78Nn4QJTEN\n43. Fu, Z. et al. Sliding window attention training for efficient large \nlanguage models. Preprint at https://arxiv.org/abs/2502.18845 \n(2025).\n44. Gokaslan, A. & Cohen, V. OpenWebText Corpus. GitHub  \nhttp://Skylion007.github.io/OpenWebTextCorpus (2019).\nNature Computational Science | Volume 5 | September 2025 | 813–824 824\nArticle https://doi.org/10.1038/s43588-025-00854-1\n45. Liu, S. et al. Design guidelines for oxide semiconductor gain \ncell memory on a logic platform. IEEE Trans. Electron Devices 71, \n3329–3335 (2024).\n46. Subhechha, S. et al. Demonstration of multilevel multiply \naccumulate operations for AiMC using engineered a-IGZO \ntransistors-based 2T1C gain cell arrays. In Proc. 2023 IEEE \nInternational Memory Workshop (IMW) 1–4 (IEEE, 2023);  \nhttps://doi.org/10.1109/IMW56887.2023.10145946\n47. Brown, T. et al. Language models are few-shot learners.  \nAdv. Neural Inf. Process. Syst. 33, 1877–1901 (2020).\n48. Jacob, B. et al. Quantization and training of neural networks \nfor efficient integer-arithmetic-only inference. In Proc. IEEE \nConference on Computer Vision and Pattern Recognition  \n2704–2713 (IEEE, 2018).\n49. Press, O., Smith, N. A. & Lewis, M. Train short, test long: attention \nwith linear biases enables input length extrapolation. In Proc. \nInternational Conference on Learning Representations (2022); \nhttps://openreview.net/forum?id=R8sQPpGCv0\n50. Tillet, P., Kung, H. T. & Cox, D. Triton: an intermediate language \nand compiler for tiled neural network computations. In Proc. \n3rd ACM SIGPLAN International Workshop on Machine Learning \nand Programming Languages, MAPL 2019 10–19 (Association for \nComputing, 2019); https://doi.org/10.1145/3315508.3329973\n51. Dao, T. FlashAttention-2: faster attention with better parallelism \nand work partitioning. In Proc. 12th International Conference \non Learning Representations (2024); https://openreview.net/\nforum?id=mZn2Xyh9Ec\n52. Mishkin, D. & Matas, J. All you need is a good init. Preprint at \nhttps://arxiv.org/abs/1511.06422 (2015).\n53. Lepri, N., Glukhov, A., Mannocci, P., Porzani, M. & Ielmini, D. \nCompact modeling and mitigation of parasitics in crosspoint \naccelerators of neural networks. IEEE Trans, Electron Devices 71, \n1900–1906 (2024).\n54. Radford, A. et al. Language models are unsupervised \nmultitask learners. OpenAI https://cdn.openai.com/\nbetter-language-models/language_models_are_unsupervised_\nmultitask_learners.pdf (2019).\n55. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. \nIn Proc. International Conference on Learning Representations \n(2019); https://openreview.net/forum?id=Bkg6RiCqY7\n56. Beck, M. et al. xLSTM: extended long short-term memory. In \nProc. 38th Annual Conference on Neural Information Processing \nSystems (2024); https://openreview.net/forum?id=ARAxPPIAhq\n57. Clark, P. et al. Think you have solved question answering? Try \nARC, the AI2 reasoning challenge. Preprint at https://arxiv.org/\nabs/1803.05457 (2018).\n58. Sakaguchi, K., Bras, R. L., Bhagavatula, C. & Choi, Y. WinoGrande: \nan adversarial winograd schema challenge at scale. Commun. \nACM 64, 99–106 (2021).\n59. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. & Choi, Y. HellaSwag: \ncan a machine really finish your sentence? In Proc. 57th Annual \nMeeting of the Association for Computational Linguistics,  \n4791–4800 (ACL, 2019).\n60. Paperno, D. et al. The LAMBADA dataset: word prediction requiring \na broad discourse context. In Proc. 54th Annual Meeting of the \nAssociation for Computational Linguistics (Volume 1: Long Papers) \n(eds Erk, K. & Smith, N. A.) 1525–1534 (Association for Computational \nLinguistics, 2016); https://doi.org/10.18653/v1/P16-1144\n61. Bisk, Y., Zellers, R., Bras, R. L., Gao, J. & Choi, Y. PIQA: reasoning \nabout physical commonsense in natural language. In Proc. 34th \nAAAI Conference on Artificial Intelligence, 7432–7439 (AAAI, 2020).\n62. Merity, S., Xiong, C., Bradbury, J. & Socher, R. Pointer sentinel  \nmixture models. In Proc. International Conference on Learning  \nRepresentations (2017); https://openreview.net/forum?id= \nByj72udxe\n63. Leroux, N. et al. Analog in-memory computing attention mechanism \nfor fast and energy-efficient large language models source data. \nfigshare https://doi.org/10.6084/m9.figshare.27763548 (2025).\n64. Gao, L. et al. A framework for few-shot language model evaluation. \nZenodo https://doi.org/10.5281/zenodo.5371628 (2025).\n65. Leroux, N. et al. GainCellAttention. Zenodo https://doi.org/ \n10.5281/zenodo.15856645 (2025).\nAcknowledgements\nThis work was supported in part by the Federal Ministry of Education \nand Research (BMBF, Germany) in the project NEUROTEC II (project \nnumber 16ME0398K). We gratefully acknowledge the Gauss Centre \nfor Supercomputing e.V. (www.gauss-centre.eu) for funding this \nproject by providing computing time through the John von Neumann \nInstitute for Computing (NIC) on the GCS Supercomputer JUWELS at \nJülich Supercomputing Centre (JSC).\nAuthor contributions\nThe study was designed by N.L. and P.-P.M., and supervised by J.P.S. \nand E.N. The analog circuit system schematic design and electrical \nsimulations were carried out by P.-P.M. C.S. was responsible for the \ndesign and layout of all digital blocks, as well as the overall chip \nfloorplanning. S.S. completed the layout of the analog components. \nHardware parameter extraction was performed by P.-P.M. Neural \nnetwork training was conducted by N.L. and neural network evaluation \nwas conducted by N.L. and J.F. All authors contributed to the analysis \nof the results and writing of the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary \nmaterial available at https://doi.org/10.1038/s43588-025-00854-1.\nCorrespondence and requests for materials should be addressed to \nNathan Leroux or Paul-Philipp Manea.\nPeer review information Nature Computational Science thanks  \nJianshi Tang, Yonghong Tian and the other, anonymous, reviewer(s) for \ntheir contribution to the peer review of this work. Peer reviewer reports \nare available. Primary Handling Editor: Jie Pan, in collaboration with \nthe Nature Computational Science team.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2025"
}