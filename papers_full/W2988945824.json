{
    "title": "Improving Transformer Models by Reordering their Sublayers",
    "url": "https://openalex.org/W2988945824",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5035538068",
            "name": "Ofir Press",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5088517824",
            "name": "Noah A. Smith",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5024311574",
            "name": "Omer Levy",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W2553303224",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W2971191050",
        "https://openalex.org/W2948981900",
        "https://openalex.org/W2962964385",
        "https://openalex.org/W2988841832",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2979636403",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2912521296",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2222512263",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2971524460",
        "https://openalex.org/W581956982",
        "https://openalex.org/W2949433733",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2963403868"
    ],
    "abstract": "Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2996–3005\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n2996\nImproving Transformer Models by Reordering their Sublayers\nOﬁr Press♦ Noah A. Smith♦♠ Omer Levy♣\n♦Paul G. Allen School of Computer Science & Engineering, University of Washington\n♠Allen Institute for AI\n♣Facebook AI Research\nAbstract\nMultilayer transformer networks consist of in-\nterleaved self-attention and feedforward sub-\nlayers. Could ordering the sublayers in a dif-\nferent pattern lead to better performance? We\ngenerate randomly ordered transformers and\ntrain them with the language modeling objec-\ntive. We observe that some of these models\nare able to achieve better performance than the\ninterleaved baseline, and that those successful\nvariants tend to have more self-attention at the\nbottom and more feedforward sublayers at the\ntop. We propose a new transformer pattern that\nadheres to this property, the sandwich trans-\nformer, and show that it improves perplexity\non multiple word-level and character-level lan-\nguage modeling benchmarks, at no cost in pa-\nrameters, memory, or training time. However,\nthe sandwich reordering pattern does not guar-\nantee performance gains across every task, as\nwe demonstrate on machine translation mod-\nels. Instead, we suggest that further explo-\nration of task-speciﬁc sublayer reorderings is\nneeded in order to unlock additional gains.1\n1 Introduction\nThe transformer layer (Vaswani et al., 2017) is cur-\nrently the primary modeling component in natural\nlanguage processing, playing a lead role in recent\ninnovations such as BERT (Devlin et al., 2019) and\nGPT-2 (Radford et al., 2019). Each transformer\nlayer consists of a self-attention sublayer (s) fol-\nlowed by afeedforward sublayer (f), creating an in-\nterleaving pattern of self-attention and feedforward\nsublayers (sfsfsf ···) throughout a multilayer\ntransformer model. To the best of our knowledge,\nthere is no reason to expect this particular pattern\nto be optimal. We conduct a series of explorations\nto obtain insights about the nature of transformer\norderings that work well, and based on this, we\n1Our code is available at https://github.com/\nofirpress/sandwich_transformer\nsfsfsfsfsfsfsfsfsfsfsfsfsfsf\n(a) Interleaved Transformer\nsssssssfsfsfsfsfsfsfsfffffff\n(b) Sandwich Transformer\nFigure 1: A transformer model (a) is composed of inter-\nleaved self-attention (green) and feedforward (purple)\nsublayers. Our sandwich transformer (b), a reordering\nof the transformer sublayers, performs better on lan-\nguage modeling. Input ﬂows from left to right.\ndesign a new transformer ordering pattern that im-\nproves upon the baseline.\nFirst, we generate random transformer models,\nvarying the number of each type of sublayer, and\ntheir ordering, while keeping the number of pa-\nrameters constant. We train these models on the\nstandard WikiText-103 word-level language mod-\neling benchmark (Merity et al., 2016), and observe\nthat some of these random models outperform the\noriginal interleaved transformer model, even when\nthe number of self-attention and feedforward layers\nis not equal. Our analysis shows that models with\nmore self-attention toward the bottom and more\nfeedforward sublayers toward the top tend to per-\nform better in general.\nBased on this insight, we design a new family of\ntransformer models that follow a distinct sublayer\nordering pattern: sandwich transformers (Figure 1).\nOur experiments demonstrate that a sandwich trans-\nformer outperforms the baseline of Baevski and\nAuli (2019). This result is made more interesting\nby the fact that our sandwich transformer is simply\na reordering of the sublayers in the baseline model,\nand does not require more parameters, memory, or\ntraining time.\nFinally, we demonstrate that even though the\n2997\nModel PPL\nf s f s f f f s f f s f s s s f f s f s s f s s s s f f s f f s 20.74\ns f s s f f s f f f f s s s s f s f f f s f s f f s f s s s s f 20.64\nf s f f s s f f s s s s f f s s s s s f f s f s s f s f f f f f 20.33\nf s f f f f f f s s s f s s f f s f s s f f s f s s s f f s s s 20.27\nf s s f f f f f f s f s s s f f f s s s s f f f s s s s f f s s 19.98\ns s s f s s f s f f f f s s f s f s f s s s f f s f s f f f s f 19.92\nf f f s f s s s f s f f s f s f f s f f s s s s s f f s s f f s 19.69\nf f f s f f s s f f s s s f s s f s s s f f f f f s f s s s f s 19.54\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 19.13\nf s f f s s f s s f f f s s s s f f f s s s f f f f s f s s f s 19.08\ns f s f f s s s s f f s s f f f f s s s f f s s s f s f f s f f 18.90\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 18.83\ns s s s s s s f f s f f s f s f s f f f f s f f f s f s s f f s 18.83\ns f f s f s f f s f s s s f f s s f s s s s s s f f f f f f f s 18.77\ns s s f s s f f s f s s f s f f s f f f s s f f s f s f f s s f 18.68\nf f f s s s s s f f f s f s s s s f f s f s f s f s s f f s f f 18.64\ns f f f s s s f s f s s f s s s s s f s s f f f f f s f f f s f 18.61\ns s f f s s f s s s s f f f f f f s s f f s s s f s f f s s f f 18.60\nf s f s s s s s f s f s f f f f f s f f f s f f s s f f s s s s 18.55\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 18.54\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 18.49\nf s f s s s s s f s f f f s s f s f f s f s f s f s f f f f s s 18.38\ns f s s f f s f s f s f f s s s s s f f f s s s f f f s f f s f 18.28\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 18.25\ns f s f s s f s s s f f s f s f s f s f f f f s s f f s f s s f 18.19\nTable 1: Randomly generated models with 16 self-\nattention ( s) sublayers and 16 feedforward ( f) sub-\nlayers, and their perplexity on the WikiText-103 devel-\nopment set. The baselines (the standard transformer\ntrained with different random seeds) are in bold.\nsandwich transformer is motivated by random\nsearch experiments on WikiText-103, it can im-\nprove performance on additional domains and tasks.\nSandwich transformers achieve state-of-the-art re-\nsults on the enwik8 character-level language model-\ning dataset and on an additional word-level corpus,\nbut have no signiﬁcant effect on machine transla-\ntion. We conjecture that tuning transformer reorder-\nings to speciﬁc tasks could yield even larger gains,\nand that further exploration of the ordering space\nmay provide universally beneﬁcial patterns.\n2 Notation\nEach transformer layer consists of a self-attention\nsublayer followed by a feedforward sublayer, mod-\nifying a sequence of vectors X0 as follows:2\nX1 = self-attention(X0) +X0\nX2 = feedforward(X1) +X1\nStacking multiple transformer layers creates an in-\nterleaved network of sublayers. We denote these\n2We omit dropout (Srivastava et al., 2014) and layer nor-\nmalization (Ba et al., 2016) to simplify the notation.\nRandom Models:\n Shuffling\nBaseline\n18\n19\n20\n21Perplexity\nFigure 2: The perplexities on the WikiText-103 devel-\nopment set of 20 randomly generated models with 16\nself-attention and 16 feedforward sublayers and of the\n5 baselines (the standard transformer trained with dif-\nferent random seeds).\nmodels as strings, with s and f representing self-\nattention and feedforward sublayers, respectively.\nA three-layer transformer network, for example,\nwould be denoted sfsfsf, with the ﬂow of com-\nputation moving from input on the left to output on\nthe right. Thus, any string in the regular language\n(s|f)∗deﬁnes a valid network that uses the same\nbuilding blocks as the original transformer. For\nsimplicity, we refer to these alternatives as trans-\nformers as well.\n3 Random Search\nWe conduct a series of experiments to under-\nstand which transformer networks work well and\nwhether particular architectural patterns can im-\nprove performance. First, we generate random\ntransformer models while keeping the number of\nparameters constant. We then train these random\nmodels to determine whether the interleaving pat-\ntern (sfsfsf ···) is optimal (Section 3.1), and\nwhether balancing the number of self-attention and\nfeedforward sublayers is desirable (Section 3.2).\nFinally, we analyze additional properties of these\nrandom models, and ﬁnd that those with more self-\nattention at the beginning and more feedforward\nsublayers near the end tend to outperform the stan-\ndard interleaved model (Section 3.3).\nExperimental Setup Our baseline is the strong\ntransformer language model of Baevski and Auli\n(2019), trained on WikiText-103 (Merity et al.,\n2016). WikiText-103 contains roughly 103 mil-\nlion tokens from English Wikipedia, split into train,\ndevelopment, and test sets by article. The Baevski\n2998\nModel PPL\ns f f f s s f s f s f s s f f f f s f s f f s f f f f f f 22.80\ns f f s s f s s s s s s s s s s s s s f s f s s s f s f f s s s f s s s f s 21.02\ns s s s s s f f s f f f f s s f f f f f s s s f s f s s s s s s s s s 20.98\nf f f f f f f f f s f f s s f f s f f s s s s f s f s s s f 20.75\nf s s f s s s f f f f f f s s f s s s f s f f f s s s s f s f s s 20.43\ns f f s f f f f f f s f s f s s f s s s f s f s f s s f s s f s 20.28\ns f f s s f f s f f f s f s f s s s s f f f f f f s s s s f f 20.02\nf s f f s f s s f f f f s f s f f f s f f f s s f f f s s s 19.93\ns f f s f f s s f f s f s f f s s s f s s s s s f s s s f f f s s s 19.85\ns s f f f f f f f s s f f f s s f s s f f s f s f s f f s f 19.82\ns f s f s f f f s f f f s s f s f f f s f f s s f s f s f s s 19.77\ns f s f f s s s f f s f f s s s f s s f f f f f s s s s f s s s f 19.55\ns f f s f s s f f f s f f s f s s s s f s f s f f f f s f s s s 19.49\ns f f f f s f f s s s s f s s s f s s f f f s s s f s s s s f s f s 19.47\nf s s s f f s s s s s s f s f s f s f f s f f f f s s f s f s s s s 19.25\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 19.13\nf s s s s s s f s f s f s f f f s f s s s f s s f f s s s s f s f f 18.86\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 18.83\ns s f s f s s s f s s s s s f f s f s f s s s f s s f s f s s s s s s s f 18.62\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 18.54\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 18.49\ns s s f s f f s f s s f s s s f f s f f f f f f s s f s f f f 18.34\ns s s f s f s f f s s s f s f f f f f s f s f f f f s s s f f 18.31\ns f s f s f s f s f s f s f s f s f s f s f s f s f s f s f s f 18.25\ns s s s s s f s s s f f f f s f s f f f f f f f f f f f s f 18.12\nTable 2: Randomly generated models with the same\nnumber of parameters as the baseline, and their perplex-\nity on the WikiText-103 development set. The base-\nlines (the standard transformer trained with different\nrandom seeds) are in bold.\nand Auli model contains 16 transformer layers\nof d = 1024 dimensions, with 16 heads in each\nself-attention sublayer, and feedforward sublayers\nwith an inner dimension of 4096. In this setting,\neach self-attention sublayer contains 4d2 param-\neters, while each feedforward sublayer contains\n8d2 parameters (excluding bias terms, which have\na marginal contribution). Thus, each f sublayer\ncontains twice the parameters of a s sublayer, fol-\nlowing the parameter ratio between self-attention\nand feedforward sublayers described in Vaswani\net al. (2017).\nAll of our experiments use the same hyperparam-\neters as Baevski and Auli’s original model. To set\nan accurate baseline, we train the baseline model\n(the standard interleaved transformer) with ﬁve dif-\nferent random seeds, achieving 18.65 ±0.24 per-\nplexity on the development set.\n3.1 Is Interleaving Optimal?\nIn the baseline 16-layer transformer model, 16 sub-\nlayers of each type are interleaved. Can we improve\nmodel performance by simply rearranging them?\nWe thus generate 20 random transformer models\nwith 16 self-attention sublayers and 16 feedforward\nRandom Models:\nParameter Budget\nBaseline\n18\n19\n20\n21\n22\n23Perplexity\nFigure 3: The perplexities on the WikiText-103 devel-\nopment set of 20 randomly generated models with the\nsame number of parameters as the baseline, and of the\n5 baselines (the standard transformer trained with dif-\nferent random seeds).\nsublayers, randomly permuted, and train these mod-\nels from scratch, without modifying any of the hy-\nperparameters. Table 1 shows the entire sample,\nwhile Figure 2 plots the perplexity distributions of\nthe shufﬂed transformers and the baseline side by\nside.\nWe observe that 7 of the 20 randomly-permuted\nmodels perform at least as well as the interleaved\nbaseline’s average performance, with the best\nmodel achieving 18.19 perplexity. While the aver-\nage performance of the baseline model beats the av-\nerage performance of these random models, the fact\nthat a third of our random models outperformed\nthe average baseline suggests that a better ordering\nthan interleaving probably exists.\n3.2 Are Balanced Architectures Better?\nIs it necessary to have an identical number of sub-\nlayers of each type, or could models with more self-\nattention (or more feedforward) sublayers yield bet-\nter results? To ﬁnd out, we generate 20 unbalanced\ntransformer models by randomly selecting one sub-\nlayer at a time (eithers or f with equal probability)\nuntil the parameter budget is exhausted. Since a\nfeedforward sublayer contains double the parame-\nters of a self-attention sublayer, the networks’ depth\nis not necessarily 32 sublayers as before and can\nrange from 24 (all f) to 48 (all s). Table 2 shows\nthe entire sample, while Figure 3 plots the perplex-\nity distributions of the randomly-generated trans-\nformers and the baseline side by side.\nWe see that four of the generated unbalanced\nmodels outperform the average baseline trans-\nformer. The best performing random model reaches\n2999\nModels that are\nworse than baseline\nModels that are\nbetter than baseline\n0\n2\n4\n6\n8\n10\n12\n14\n16\nAverage sublayer count in the\nbottom half of the model Self-attention\nFeedforward\n(a)\nModels that are\nworse than baseline\nModels that are\nbetter than baseline\n0\n2\n4\n6\n8\n10\n12\n14\n16\nAverage sublayer count in the\ntop half of the model\nSelf-attention\nFeedforward (b)\nFigure 4: Analysis of sublayer distribution in models that do better or worse than the average baseline, split across\nbottom (a) and top (b) halves of the model.\na perplexity of 18.12 and has 12 self-attention and\n18 feedforward sublayers. Both the average and\nthe median perplexities of this sample of unbal-\nanced models are worse than those of the balanced\npermuted models (Section 3.1). We do not ob-\nserve any preference for more sublayers of one\ntype over the other; there are self-attention-heavy\nand feedforward-heavy models in both the top ﬁve\nand the bottom ﬁve of the results table. While of-\nfering no guarantees – given the small sample sizes\nand ﬁxed hyperparameters – we conclude that a\nbalanced number of self-attention and feedforward\nsublayers seems to be a desirable property, though\nnot a necessary one.\n3.3 Attention First, Feedforward Later\nSo far, it is not clear which characteristics make\none transformer model more successful than an-\nother; for example, measuring the number of times\neach sublayer type appears in the network does\nnot reveal any strong correlation with performance.\nHowever, analyzing the bottom (or top) half of the\nnetwork in isolation reveals an interesting property.\nWe ﬁrst split the models to those that perform\nbetter than the average baseline and those that do\nnot. We then slice each one of the previously-\ngenerated random models in half by parameter\ncount (e.g., ssssff would be split to ssss and\nff, since every f contains twice as many param-\neters as an s), and count how many sublayers of\neach type appear in each slice.\nFigure 4 shows that models that outperform the\naverage baseline tend to have more self-attention s\nin the ﬁrst (bottom) half of the network and more\nf in the second (top) half. While we do not have a\ngood hypothesis to explain this phenomenon, we\ncan exploit it to improve transformers (Section 4).\n4 Designing a Better Transformer\nOur analysis in the previous section motivates de-\nsigning a transformer model that is heavy on self-\nattention at the bottom and feedforward sublay-\ners at the top, while at the same time containing\na more-or-less balanced amount of both sublayer\ntypes. As a ﬁrst attempt to manually design a better\ntransformer, we take this hypothesis to the extreme,\nand train a transformer model of 16 self-attention\nsublayers followed by 16 feedforward sublayers\n(s16f16). This model achieves 18.82 perplexity,\nwhich is comparable to the performance of the base-\nline with the same number of parameters.\nWe next generalize this model and the original\ninterleaved transformer, creating the family ofsand-\nwich transformers. A sandwichn\nk transformer con-\nsists of 2n sublayers in total (n of each type), con-\nforming to the regular expression sk(sf)n−k fk.\nThe ﬁrst k sublayers are purely self-attention (s),\nwhile the last k are feedforward sublayers (f). In\nbetween, we use the original interleaving pattern\n(sf) to ﬁll the remaining2(n−k) sublayers. When\nk = 0, we get the original transformer model, and\nwhen k = n −1 (its maximal value) we get the\npreviously mentioned snfn model. We refer to k\nas the transformer’ssandwich coefﬁcient.\nWe train sandwich transformers for n = 16\n(to remain within the same parameter budget as\nour baseline language model) and all values of\nk ∈{0, . . . ,15}. Figure 5 shows the transformer’s\nperformance as a function of the sandwich coef-\nﬁcient k. With the exception of k = 14, 15, all\nsandwich transformers achieve lower perplexities\n3000\nModel Test\nBaseline (Baevski and Auli, 2019) 18.70\nTransformer XL (Dai et al., 2019) 18.30\nkNN-LM (Khandelwal et al., 2019) 15.79\nBaseline (5 Runs) 18.63 ± 0.26\nSandwich16\n6 17.96\nTable 3: Performance on the WikiText-103 test set. We\ncompare the best sandwich transformer to the unmod-\niﬁed, interleaved transformer baseline (Baevski and\nAuli, 2019) trained over 5 random seeds and to other\npreviously reported results.\nthan the average baseline transformer. Of those, 6\nmodels outperform the best baseline transformer\n(k = 5, 6, 8, 9, 10, 11). The best performance of\n17.84 perplexity is obtained when k = 6. We com-\npare this model to the baseline on WikiText-103’s\ntest set.\nTable 3 shows that, despite its simple design,\nthe sandwich transformer outperforms the original\ntransformer baseline by roughly double the gap be-\ntween the baseline (Baevski and Auli, 2019) and\nTransformer XL (Dai et al., 2019). This improve-\nment comes at no extra cost in parameters, data,\nmemory, or computation; we did not even change\nany of the original hyperparameters, including the\nnumber of training epochs.\nTo check whether this advantage is consistent,\nwe train 4 more sandwich16\n6 models with different\nrandom seeds (5 in total) and evaluate them on the\ndevelopment set, to avoid evaluating our model\nmore than once on the test set. This is the only\nexperiment in which we modify our model’s ran-\ndom seed. Figure 6 shows that we obtain a mean\nperplexity value of 17.98 with a standard deviation\nof 0.10, while the baseline achieves 18.65 mean\nperplexity, with a larger standard deviation of 0.34\n(these values reﬂect development set performance,\nnot test set performance as in Table 3).\nIn very recent work, kNN-LM (Khandelwal\net al., 2019) set a new state of the art on WikiText-\n103, surpassing other recent models by a wide mar-\ngin. The model achieves this result by storing the\nentire training set in an auxiliary memory com-\nponent. Since this approach appears orthogonal to\nours, it is quite possible that kNN-LM could beneﬁt\nfrom sublayer reordering as well.\n5 One Reordering to Rule Them All?\nThe sandwich transformer is a manually-crafted\npattern motivated by the performance of random\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nSandwich Coefficient\n17.75\n18.00\n18.25\n18.50\n18.75\n19.00Perplexity\nFigure 5: The transformer’s sandwich coefﬁcient ( k)\nand validation perplexity, for k ∈ {1, . . . ,15}. The\ndotted line is the average baseline model’s perplex-\nity (trained with different random seeds), whereas the\ndashed line represents the best baseline model.\nBaseline Sandwich16\n6\n17.5\n18.0\n18.5\n19.0\n19.5Perplexity\nFigure 6: Performance on the WikiText-103 develop-\nment set of the Sandwich 16\n6 transformer and the base-\nline. Each model is trained with 5 different random\nseeds to assess the perplexity distribution.\nsublayer reorderings of the Baevski and Auli (2019)\nmodel, trained on the WikiText-103 word-level lan-\nguage modeling benchmark (Merity et al., 2016).\nDoes this particular pattern improve perfor-\nmance in other settings as well? To ﬁnd out, we\napply sandwich transformers to three other tasks:\nword-level language modeling on a different do-\nmain (Section 5.1), character-level language mod-\neling (Section 5.2), and machine translation (Sec-\ntion 5.3).\nResults show that as we drift away from our\noriginal setting, sandwich transformers provide di-\nminishing gains, but always perform at least as well\nas the baseline transformers (provided that the sand-\nwich coefﬁcient is properly tuned). This ﬁnding\nsuggests that different settings may beneﬁt from\ndifferent sublayer reordering patterns.\n3001\nModel PPL\nBaseline (5 runs) 11.89 ± 0.35\nkNN-LM (Khandelwal et al., 2019) 10.89\nSandwich16\n7 10.83\nTable 4: Performance on the Toronto Books Corpus lan-\nguage modeling test set. The baseline model (Baevski\nand Auli, 2019) is trained over 5 random seeds. The\nsandwich coefﬁcient is tuned on the validation set and\nwe run our model on the test set only once.\n5.1 Books-Domain Language Modeling\nWe ﬁrst apply sandwich transformers to a differ-\nent domain, while retaining the other architectural\naspects and hyperparameter settings from Baevski\nand Auli (2019). Speciﬁcally, we use the Toronto\nBooks Corpus (Zhu et al., 2015), which has previ-\nously been used to train GPT (Radford et al., 2018)\nand also BERT (Devlin et al., 2019) (combined\nwith Wikipedia). The corpus contains roughly\n700M tokens.\nWe use the same train/validation/test split\nas Khandelwal et al. (2019), as well as their to-\nkenization, which uses BERT’s vocabulary of 29K\nbyte-pair encodings. Since the vocabulary is much\nsmaller than WikiText-103’s, we replace the adap-\ntive word embedding and softmax of Baevski and\nAuli (2019) with a tied word embedding and soft-\nmax matrix (Press and Wolf, 2017; Inan et al.,\n2017). Finally, we tune the sandwich coefﬁcient\non the development set for k ∈{4, . . . ,8}, i.e., a\nneighborhood of 2 around the best value we found\nfor WikiText-103 (k = 6).\nTable 4 shows that the sandwich transformer\ntransfers well to the books domain, improving\nperformance by 1.06 perplexity, achieving similar\nperformance to the datastore-augmented kNN-LM\n(Khandelwal et al., 2019), which is the state of the\nart on WikiText-103 (see Section 4).\n5.2 Character-level Language Modeling\nModeling text as a stream of characters, rather than\nword or subword tokens, presents a different mod-\neling challenge: long-range dependencies become\ncritical, and the vocabulary takes on a more uni-\nform distribution. We apply our sandwich reorder-\ning to the adaptive span model of Sukhbaatar et al.\n(2019), which is state of the art on the popular\nEnglish-language benchmark text8 and is currently\na close second on enwik8. 3 The adaptive span\n3Both datasets are taken from http://mattmahoney.\nnet/dc/textdata.html\nmodel learns to control each attention head’s maxi-\nmal attention span, freeing up memory in the bot-\ntom layers (which typically need very short atten-\ntion spans) and applying it to the top layers, allow-\ning the top-level attention heads to reach signiﬁ-\ncantly longer distances. The adaptive span model’s\nefﬁcient use of attention also results in a signiﬁcant\nspeed boost.\nWe tune the sandwich coefﬁcient on the devel-\nopment set for k ∈{1, . . . ,8}(the baseline model\nhas 24 transformer layers). We do not modify any\nhyperparameters, including the number of training\nepochs. Table 5 compares the baseline model’s\nperformance with the sandwich transformer’s. On\ntext8, the sandwich transformer performs within\nthe baseline’s random seed variance. On enwik8,\nthe sandwich transformer gains an improvement\nof about 0.007 bits-per-character, matching the\nstate of the art results obtained by the Transformer-\nXL-based Compressive Transformer of Rae et al.\n(2020).\nHowever, our approach is able to achieve this re-\nsult without applying the Transformer-XL’s recur-\nrent attention, which is much slower (Sukhbaatar\net al., 2019), and without adding additional param-\neters (the compressive transformer uses 277M pa-\nrameters, while our baseline and sandwich models\nuse only 209M).\n5.3 Machine Translation\nSandwich Decoders Tranformer-based transla-\ntion models (Vaswani et al., 2017) consist of an\nencoder and decoder, where the encoder has in-\nterleaved self-attention and feedforward sublayers\n(just as in language models), while the decoder in-\ncludes an additional sublayer, cross-attention (c),\nbetween every pair of self-attention and feedfor-\nward sublayers. Cross-attention sublayers attend\nto the encoder’s representations of the input sen-\ntence’s tokens.\nFollowing our notation from Section 2, a trans-\nformer decoder layer modiﬁes the sequence of to-\nkens in the target language Y0, using the encoded\nsource tokens X, as follows:\nY1 = self-attention(Y0) +Y0\nY2 = cross-attention(Y1, X) +Y1\nY3 = feedforward(Y2) +Y2\nApplying the sandwich pattern to the encoder\nfollows the same methodology as our previous ex-\nperiments. However, for the decoder, we group the\n3002\nModel text8 (BPC) enwik8 (BPC)\nTransformer-XL (Dai et al., 2019) 1.08 0.99\nAdaptive Span (Sukhbaatar et al., 2019) 1.07 0.98\nCompressive (Rae et al., 2020) — 0.97\nBaseline (Adaptive Span; 5 Runs) 1.0802 ± 0.0103 0.9752 ± 0.0008\nSandwich24\n3 1.076 —\nSandwich24\n5 — 0.968\nTable 5: Performance on character-level language modeling, evaluated on the enwik8 and text8 test sets. The\nbaseline model (Sukhbaatar et al., 2019) is trained over 5 random seeds. The sandwich coefﬁcient is tuned on each\nbenchmark’s validation set, and we run our model on the test only once.\nself-attention (s) and cross-attention ( c) sublay-\ners, and treat them as a single unit for reordering\npurposes (sc). For example, a three layer decoder\n(scfscfscf) with a sandwiching coefﬁcient of\nk = 1 would be: scscfscff. We apply the\nsandwich pattern to either the encoder or decoder\nseparately, while keeping the other stack in its orig-\ninal interleaved pattern.\nExperiment Setting As a baseline, we use the\nlarge transformer model (6 encoder/decoder layers,\nembedding size of 1024, feedforward inner dimen-\nsion of 4096, and 16 attention heads) with the hy-\nperparameters of Ott et al. (2018). We also follow\ntheir setup for training and evaluation: we train\non the WMT 2014 En-De dataset which contains\n4.5M sentence pairs; we validate on newstest13 and\ntest on newstest14. We use a vocabulary of 32K\nsymbols based on a joint source and target byte pair\nencoding (Sennrich et al., 2016). For inference we\nuse beam search with a beam width of 4 and length\npenalty of 0.6, following Vaswani et al. (2017) and\nOtt et al. (2018). As before, we do not modify our\nmodel’s hyperparameters or training procedure.\nResults Table 6 shows that reordering of either\nthe encoder or decoder does not have a signiﬁcant\nimpact on performance, across the board. We also\nﬁnd that using the most extreme sandwich decoder\n(sc)6f6 performs almost exactly the same as the\naverage baseline; this result is consistent with our\nobservation from Section 4, where we show that\nthe extreme sandwich language model ( s16f16)\nperforms as well as the baseline.\nDiscussion This experiment indicates that a re-\nordering pattern that beneﬁts one particular task\n(language modeling) might not carry the same per-\nformance gains to another (machine translation).\nHowever, it also demonstrates the general robust-\nness of transformer architectures to sublayer re-\nordering, as we did not observe any major perfor-\nSandwich Encoder Decoder\nCoefﬁcient Sandwich Sandwich\n0 (Baseline) 28.74 ± 0.15\n1 28.71 28.64\n2 28.71 28.56\n3 28.81 28.67\n4 28.48 28.66\n5 28.45 28.76\nTable 6: BLEU on newstest2014 En-De. Our encoder\n(decoder) sandwich model keeps the decoder (encoder)\nunmodiﬁed. We train the baseline model (Transformer-\nlarge with the hyperparameters of Ott et al., 2018) 5\ntimes with different random seeds.\nmance degradation. Since the sandwich pattern\nnaively groups self- and cross-attention sublayers\ntogether, it is also possible that a reordering pat-\ntern that takes all three sublayer types into account\ncould potentially improve performance.\n6 Analysis\nAt the time of writing, we do not have an expla-\nnation for why sublayer reordering improves per-\nformance on language modeling. However, we\nare able to determine that sandwich transformers\nspread their attention in a different fashion than\ninterleaved models.\nWe analyze two baseline models and two\nsandwich16\n6 models trained with different seeds on\nthe WikiText-103 dataset, by ﬁrst recording the at-\ntention values that each token’s heads assign to all\nother tokens during inference on the validation set.\nGiven the attention outputs of two models, we then\ncompute the models’ attention distance for each\ntoken, and for each self-attention sublayer. This\nmetric compares the attention distribution in theith\nself-attention sublayer of the ﬁrst model to that of\nthe ith self-attention sublayer of the second model,\nfor a speciﬁc token.\nGiven a token and a self-attention sublayer,\n3003\nModel Pair Average Attention Distance\nBaseline – Baseline 1.081 · 10−3\nSandwich – Sandwich 1.067 · 10−3\nBaseline – Sandwich 1.289 · 10−3 ± 0.049 · 10−3\nTable 7: The average attention distance, on the\nWikiText-103 validation dataset, of each model pair.\nSince there are two baselines and two sandwich trans-\nformers (initialized with different random seeds), the\ndistance between the baseline and sandwich models\nis averaged over all four baseline-sandwich combina-\ntions.\nwe use the Hungarian algorithm (Kuhn, 1955)\nto ﬁnd a matching of heads in the ﬁrst model\nto heads in the second model [a1, b1], . . . ,[a8, b8]\nsuch that ∑8\ni=1 EMD(ai, bi) is minimized, where\nEMD(ai, bi) is the earth mover’s (Wasserstein) dis-\ntance between the attention distributions of head ai\nin the ﬁrst model and head bi in the second model.\nThat minimal value is the attention distance for that\ntoken, in that layer. We then average the attention\ndistances across all tokens and layers.\nTable 7 shows the average attention distances\nbetween every pair of models. We observe that\nmodels of the same architecture have signiﬁcantly\nlower attention distances than models with differ-\nent sublayer orderings. This indicates that sublayer\nreordering has a strong effect on the attention func-\ntion that the model learns in each head. Future\ninvestigations of what this difference is, in a qual-\nitative sense, could potentially provide important\ninsights for designing better reordering patterns.\n7 Related Work\n7.1 Neural Architecture Search\nIn this paper, we manually search through a con-\nstrained transformer architecture space, after an-\nalyzing the results of two small-scale random\nsearches. This human-in-the-loop method for archi-\ntecture search has advantages over previous meth-\nods (Jozefowicz et al., 2015; Zoph and Le, 2016;\nTan and Le, 2019) since it requires that only a few\ndozen models be trained, unlike typical architec-\nture search methods that require training thousands\nof instances, consuming massive computational re-\nsources.\nWhile we do ﬁnd a better performing trans-\nformer, our goal is not only to do so, but to bet-\nter understand how sublayer ordering affects trans-\nformer models. Future work could apply methods\nfrom the architecture space literature to the sub-\nlayer ordering problem. Furthermore, a better un-\nderstanding of the inner workings of transformers\ncould inspire more efﬁcient, constrained architec-\nture search.\n7.2 Transformer Modiﬁcations\nMuch recent work has been devoted to improving\ntransformers by modifying their sublayers. This in-\ncludes sparsifying their attention patterns, either in\nan input-based manner (as in Correia et al., 2019),\nor in a static manner (as in Guo et al., 2019). So\net al. (2019) proposed modifying the transformer\nby adding convolutions and changing the activation\nfunction, while others have demonstrated that dif-\nferent initialization schemes (Zhang et al., 2019)\nand repositioning the layer normalization (Nguyen\nand Salazar, 2019) can also have a positive effect\non performance.\nIn this paper, we do not modify the sublayers at\nall, but simply rearrange their order. The perfor-\nmance gains from sublayer reordering are orthog-\nonal to improving the sublayers themselves, and\ncould be combined to achieve even better perfor-\nmance.\nRecently, Lu et al. (2019) introduced a new trans-\nformer ordering, where instead of stacking layers\nof the form sf (as in the vanilla interleaved trans-\nformer), they stack layers of the form fsf. In\norder keep the total parameter count unchanged,\nLu et al. cut the hidden dimension of their feed-\nforward sublayers by half. However, the overall\ndepth of the network is increased by 50%, which\ncauses a similar increase in the model’s inference\ntime (Sanh, 2019).\n8 Conclusion\nWe train random transformer models with re-\nordered sublayers, and ﬁnd that some perform bet-\nter than the baseline interleaved transformer in lan-\nguage modeling. We observe that, on average, bet-\nter models contain more self-attention sublayers at\nthe bottom and more feedforward sublayer at the\ntop. This leads us to design a new transformer stack,\nthe sandwich transformer, which signiﬁcantly im-\nproves performance over the baseline at no cost in\nparameters, memory, or runtime.\nWe then show that the sandwich ordering also im-\nproves language modeling performance on a differ-\nent word-level language modeling benchmark, and\nthat the sandwich pattern can be used to achieve\nstate of the art results on character-level language\n3004\nmodeling. Although sandwich ordering does not\nimprove translation models, we show that they are\nrobust to layer order changes, and that even ex-\ntreme reorderings (all attention sublayers at the\nbottom, and all the feedforward sublayers at the\ntop) perform as well as the baseline.\nSublayer reordering can improve the perfor-\nmance of transformer models, but an ordering\nthat improves models on one group of tasks\n(word/character-level language modeling) might\nnot improve the performance on another task. By\nshowing that sublayer ordering can improve mod-\nels at no extra cost, we hope that future research\ncontinues this line of work by looking into optimal\nsublayer ordering for other tasks, such as transla-\ntion, question answering, and classiﬁcation.\nAcknowledgments\nWe thank Tim Dettmers, Jungo Kasai, Sainbayar\nSukhbaatar, and the anonymous reviewers for their\nvaluable feedback.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization. arXiv:1607.06450.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nICLR.\nGonc ¸alo M. Correia, Vlad Niculae, and Andr ´e F. T.\nMartins. 2019. Adaptively sparse transformers.\narXiv:1909.00015.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,\nXiangyang Xue, and Zheng Zhang. 2019. Star-\ntransformer. In NAACL.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In ICLR.\nRafal Jozefowicz, Wojciech Zaremba, and Ilya\nSutskever. 2015. An empirical exploration of recur-\nrent network architectures. In ICLR.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv:1911.00172.\nHarold W. Kuhn. 1955. The hungarian method for\nthe assignment problem. Naval Research Logistics\nQuarterly, 2(1-2):83–97.\nYiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin\nDong, Tao Qin, Liwei Wang, and Tie-Yan Liu.\n2019. Understanding and improving transformer\nfrom a multi-particle dynamic system point of view.\narXiv:1906.02762.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv:1609.07843.\nToan Q. Nguyen and Julian Salazar. 2019. Transform-\ners without tears: Improving the normalization of\nself-attention. arXiv:1910.05895.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In CMT.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In EACL.\nAlec Radford, Karthik Narasimhan, Time Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding with unsupervised learning.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nChloe Hillier, and Timothy P. Lillicrap. 2020. Com-\npressive transformers for long-range sequence mod-\nelling. In ICLR.\nVictor Sanh. 2019. Smaller, faster, cheaper, lighter: In-\ntroducing DistilBERT, a distilled version of BERT.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In ACL.\nDavid So, Quoc Le, and Chen Liang. 2019. The\nevolved transformer. In ICML.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15:1929–1958.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In ACL.\nMingxing Tan and Quoc Le. 2019. EfﬁcientNet: Re-\nthinking model scaling for convolutional neural net-\nworks. In ICML.\n3005\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2019. Im-\nproving deep transformer with depth-scaled initial-\nization and merged attention. arXiv:1908.11365.\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. arXiv:1506.06724.\nBarret Zoph and Quoc V . Le. 2016. Neural\narchitecture search with reinforcement learning.\narXiv:1611.01578."
}