{
  "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models",
  "url": "https://openalex.org/W4389519952",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2100509238",
      "name": "Yongchao Chen",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Rujul Gandhi",
      "affiliations": [
        "Moscow Institute of Thermal Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096237007",
      "name": "Yang Zhang",
      "affiliations": [
        "IBM (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2146244389",
      "name": "Chuchu Fan",
      "affiliations": [
        "Moscow Institute of Thermal Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3039732322",
    "https://openalex.org/W3121030992",
    "https://openalex.org/W2026629052",
    "https://openalex.org/W4286364676",
    "https://openalex.org/W1612453857",
    "https://openalex.org/W2007820193",
    "https://openalex.org/W2277684984",
    "https://openalex.org/W2562465323",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4286969054",
    "https://openalex.org/W2004796166",
    "https://openalex.org/W1607877490",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3208764434",
    "https://openalex.org/W2295357975",
    "https://openalex.org/W1547304883",
    "https://openalex.org/W2982695815",
    "https://openalex.org/W3003444615",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3003205975",
    "https://openalex.org/W3205485021",
    "https://openalex.org/W4382463985",
    "https://openalex.org/W3169068430",
    "https://openalex.org/W1949907236",
    "https://openalex.org/W2807172739",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains. In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages. Our contributions are twofold. First, we develop a framework to create a dataset of NL-TL pairs combining LLMs and human annotation. We publish a dataset with 23K NL-TL pairs. Then, we finetune T5 models on the lifted versions (i.e., the specific Atomic Propositions (AP) are hidden) of the NL and TL. The enhanced generalizability originates from two aspects: 1) Usage of lifted NL-TL characterizes common logical structures, without constraints of specific domains. 2) Application of LLMs in dataset creation largely enhances corpus richness. We test the generalization of trained models on five varied domains. To achieve full NL-TL transformation, we either combine the lifted model with AP recognition task or do the further finetuning on each specific domain. During the further finetuning, our model achieves higher accuracy (> 95%) using only <10% training data, compared with the baseline sequence to sequence (Seq2Seq) model.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15880–15903\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nNL2TL: Transforming Natural Languages to Temporal Logics using Large\nLanguage Models\nYongchao Chen\nMIT / Harvard\nycchen98@mit.edu\nRujul Gandhi\nMIT\nrujul@mit.edu\nYang Zhang\nMIT-IBM Watson AI Lab\nYang.Zhang2@ibm.com\nChuchu Fan\nMIT\nchuchu@mit.edu\nAbstract\nTemporal Logic (TL) can be used to rigorously\nspecify complex high-level specification for\nsystems in many engineering applications. The\ntranslation between natural language (NL) and\nTL has been under-explored due to the lack\nof dataset and generalizable model across dif-\nferent application domains. In this paper, we\npropose an accurate and generalizable trans-\nformation framework of English instructions\nfrom NL to TL, exploring the use of Large Lan-\nguage Models (LLMs) at multiple stages. Our\ncontributions are twofold. First, we develop a\nframework to create a dataset of NL-TL pairs\ncombining LLMs and human annotation. We\npublish a dataset with 28K NL-TL pairs. Then,\nwe finetune T5 models on the lifted versions\n(i.e., the specific Atomic Propositions (AP) are\nhidden) of the NL and TL. The enhanced gen-\neralizability originates from two aspects: 1) Us-\nage of lifted NL-TL characterizes common log-\nical structures, without constraints of specific\ndomains. 2) Application of LLMs in dataset\ncreation largely enhances corpus richness. We\ntest the generalization of trained models on\nfive varied domains. To achieve full NL-TL\ntransformation, we either combine the lifted\nmodel with AP recognition task or do the fur-\nther finetuning on each specific domain. Dur-\ning the further finetuning, our model achieves\nhigher accuracy (>95%) using only <10% train-\ning data, compared with the baseline sequence\nto sequence (Seq2Seq) model.12\n1 Introduction\nTemporal Logic (TL) has been widely used as\na mathematically precise language to specify re-\nquirements in many engineering domains such as\nrobotics (Tellex et al., 2020), electronics design\n(Browne et al., 1986), autonomous driving (Maier-\nhofer et al., 2020). TL can capture the complex\n1Datasets and Codes are available athttps://github.\ncom/yongchao98/NL2TL\n2Project Page is available at https://yongchao98.\ngithub.io/MIT-realm-NL2TL\nspatial, temporal, and logical requirements of both\nhuman languages and environmental constraints,\nand can be transformed into executable actions or\ncontrol inputs for robots (Gundana and Kress-Gazit,\n2022; Raman et al., 2013; Boteanu et al., 2016; Pa-\ntel et al., 2020; Gopalan et al., 2018).\nUnlike many robotics works that try to use end-\nto-end black-box models to infer robotic behaviors\ndirectly from natural language (NL) (Ahn et al.,\n2022), using structured TL as the intermediate has\na twofold benefit – the TL can be used for direct\nplanning, and the TL representation can be used\nto identify specific sources of failure and provide\nautomatic feedback to a non-expert user (Raman\net al., 2013). However, TL has a steep learning\ncurve. Communicating one’s goals and constraints\nthrough NL is much more intuitive to a non-expert.\nTherefore, a model able to transform NL instruc-\ntions into TL is a missing but crucial component\nfor interactive robots and engineering designs.\nCurrently, there is no general tool to perform au-\ntomated translations between TL and NL that takes\nthe following requirements into consideration:\n• Cross-domain generalization. Although TL\nis used in many engineering domains, cur-\nrent NL-to-TL approaches largely constrain\ntheir training data to a single domain. These\ndatasets mostly lack plentiful corpus richness\nof NL-TL and have their own specified for-\nmats of Atomic Propositions (AP). Then the\nmodels fail to generalize to other domains\n(Gopalan et al., 2018), even though the struc-\nture of TL itself is not dependent on the do-\nmain and should be generic.\n• Variability of NL instructions. Past work\noften constructs synthetic data algorithmically,\nrequiring limited forms of the NL input. Real-\nworld NL utterances cannot be encoded into\na small set of rules. Models trained on such\nhomogeneous data fail to generalize to new\n15880\nsentence structures (Brunello et al., 2019).\nOne big bottleneck in the NL-to-TL problem is\nthe lack of data. Although modern statistical meth-\nods can outperform rule-based methods (Buzhin-\nsky, 2019), they typically require a huge dataset.\nThis data is expensive and difficult to collect since\nstrong expertise of annotators is needed (Brunello\net al., 2019). As outlined above, constraining the\ndomain or form of the NL instructions relieves the\npressure of dataset, but also unavoidably under-\nmines the generalizability (Brunello et al., 2019;\nPatel et al., 2019).\nTo supplement the data creation process and si-\nmultaneously overcome the need for a huge dataset,\nwe propose to use pre-trained LLMs. We utilize\nGPT-3 (Brown et al., 2020) to assist dataset cre-\nation and finetune T5 models (Raffel et al., 2020)\nto be specialized in NL-to-TL transformation.\nAnother aspect of our approach is to use ‘lifted’\nversions of NL and TL for finetuning our model,\nwhich greatly enhances generalizability. In previ-\nous work, models trained on full NL-to-TL transfor-\nmation often include converting specific individual\nactions into APs. For example, the AP \"a response\nis created in Slack\"might be formalized as \"cre-\nate_Slack\". As a result, each work has to regularize\nits own content and style of APs, affecting gener-\nalization. Instead of taking this approach, we hide\nall the APs in our data during finetuning, acquiring\na lifted model on lifted NL-to-TL transformation.\nFor the final ground application from full NL into\nfull TL, two methods are proposed, either by com-\nbining the lifted model with AP recognition or fur-\nther transfer learning in one specific domain. For\nfurther transfer learning into specific domains, we\ncompare the models with/without pre-training on\nlifted NL-TL and show its significance.\nIn this paper, we present two key contributions:\n• Constructing a cross-domian NL-TL\ndataset. We generate a dataset of 15K lifted\nNL-TL pairs using a novel GPT-3-assisted\nframework. Ablation studies are conducted\nto show the significance of each part of\nthe framework for dataset construction. In\naddition, we collect and clean previous\ndatasets (13K) from two varied domains,\nadapting original full NL-TL pairs into\nlifted versions. In this way, we publish a\ndataset of 28K lifted NL-TL pairs. Ablation\nstudies show that the newly created data are\nFigure 1: Illustration of lifted NL and lifted STL.\nindispensable since purely training on the\ncollected data fails to work across domains.\n• Finetuning a lifted NL-to-TL model on T5\nusing our data, and demonstrating the im-\nprovement in performance compared to for-\nmer state-of-the-art methods. For application\nin full NL-to-STL transformation, two meth-\nods are proposed. We compare our model to\nSeq2Seq models and direct few-shot learning\nby GPT-3, across five domains. The experi-\nmental results show that our methods achieve\nbetter accuracy (>95% across all domains) and\nare more data-efficient (<10% domain specific\ndata). We also do the ablation study by train-\ning a Seq2Seq model with lifted NL-to-TL\ndataset, revealing that T5’s superior model\ncapacity is essential.\nGPT-4 (Bubeck et al., 2023) comes out when\napproaching the end of this work. To compare\nthe accuracy of direct GPT-4 few-shot end-to-end\ntranslation with our finetuned model, we did an ad-\nhoc test on ChatGPT Plus version with 100 samples\nin each domain. Here we can not test on more\nsamples since we do not have access to GPT-4 API\nand ChatGPT Plus version has access limitation\nper hour. The results show that GPT-4 achieves an\naccuracy of 77.7% over 300 samples, much lower\nthan our model but higher than GPT-3.\n2 Temporal Logic Specifications\n2.1 STL Syntax\nThere are many different versions of TL (Emer-\nson, 1990; Maler and Nickovic, 2004; Koymans,\n1990). They are more or less similar in terms of\nsyntax. We will use Signal Temporal Logic (STL)\n15881\nas a representative formal language that supports\nconstraints over the continuous real-time, which is\nmore suitable to capture time-critical missions. In\nsome previous work, Linear Temporal Logic (LTL)\nis also widely used, which is contained by STL\nwhen the time is discrete. We will construct our\nframework based on STL and show that the trained\nmodel also performs well on datasets and tasks us-\ning LTL. An STL formula is defined recursively\naccording to the following syntax:\nϕ::= πµ |¬ϕ|ϕ∧φ|ϕ∨φ|F[a,b]ϕ|G[a,b]ϕ\n|ϕU[a,b]φ (1)\nwhere ϕ and φ are STL formulas, and πµ is an\natomic predicate. ¬(negation), ∧(and), ∨(or),\n⇒(imply), and ⇔(equal)) are logical operators.\nF[a,b] (eventually/finally), G[a,b] (always/globally),\nand U[a,b] (until) are temporal operators with real-\ntime constraints t ∈ [a,b]. Temporal operators\nwith time constraints are illustrated by Table 4, and\nother operators can be presented using the basic\nsyntax.\n2.2 Lifted STL and Lifted NL\nWe represent our data as ‘lifted’ NL and STL, in\nwhich the specific APs corresponding to individual\nactions are hidden (following nomenclature from\nHsiung et al. (2021)). In our lifted NL and STL,\neach AP is replaced with a placeholder prop_i. In\nthis way, we train our model on the general con-\ntext of the instruction regardless of the specific\nAPs. The correspondences between full and lifted\nNL/STL are shown in Figure 1.\n2.3 STL Expression Formats\nConsider an STL expression as a binary tree, as\nin Figure 2. When finetuning a text-to-text model,\nthere are different ways of representing the target\nSTL in the form of linear text. Specifically, the tar-\ngeted tokens can be linearized in an in-order (left\nsubtree, root, right subtree) or pre-order (root, left\nsubtree, right subtree) manner. Meanwhile, the op-\nerators can also be represented as the words with\ntheir corresponding meanings (rather than as sym-\nbols). The training results show that the in-order\nexpression with all the operators replaced by words\nachieves much better accuracy than other three\nforms (will discuss in the following Section 5).\n3 Related Work\nOver decades, researchers have methods to trans-\nlate English sentences into various TL formulae\n(Brunello et al., 2019; Finucane et al., 2010; Tellex\net al., 2020; Raman et al., 2013). However, to sim-\nplify the tasks, some previous work typically make\nstrong assumptions to restrict the input text or the\noutput formula, thus limiting the flexibility and\ngeneralizability.\nThe first representative attempt is by Finucane\net al. (2010); Tellex et al. (2011); Howard et al.\n(2014), where the traditional methods typically fol-\nlow three steps: 1) pre-process given English input\nby extracting syntactical information, 2) identify\npatterns or rules for TL through classification, and\n3) run an attribute grammar-based parser to derive\na target logical format. These methods only work\nfor restricted input NL (Tellex et al., 2020).\nAnother category of approaches are learning-\nbased. Representative state-of-the-art works are\nGopalan et al. (2018); Wang et al. (2021); He\net al. (2022). In Gopalan et al. (2018) the au-\nthors gather a dataset focusing on Geometric LTL\n(GLTL), in which the NL and GLTL examples are\nall for the navigation of a car in the room. Then\nSeq2Seq models with attention mechanism are\ntrained. Though the accuracy (93.45%) is satis-\nfying, the used GLTLs are relatively simple with\neach GLTL normally including one to three APs\nand the dataset also focuses on one confined task.\nIn He et al. (2022) the authors choose to first trans-\nlate a manually generated set of STL formulae into\nEnglish sentences and train a semantic parser on\nthe synthetic data. Such synthetic data cannot rep-\nresent general NL and therefore the trained parser\nonly works well on the original STL formulae.\nIn Wang et al. (2021) a semantic parser is built\nto learn the latent structure of NL commands for\nground robots. The parser will provide (potentially\nincorrect) intermediate LTL representations to a\nmotion planner, and the planner will give an exe-\ncuted trajectory as the feedback to see whether the\nrobot’s execution satisfies the English input. Such\napproach has no guarantee on the correctness of the\ntranslated TL. In recent months, the work by Fug-\ngitti and Chakraborti (2023) directly applies LLMs\nlike GPT-3 to convert NL to LTL via few-shot learn-\ning. The prompts should be well designed and the\nmodel will fail once the NL and LTL structures are\ntoo complex (we will discuss it in Section 7.1).\nHence, in the domain of NL-to-TL translation,\n15882\nFigure 2: Illustration of different formats of STL expressions. (a) Different expression formats of the\nsame STL. (b) The binary tree representation of STL.\ndata augmentation/synthesis has been done algo-\nrithmically in previous work, not using generative\nmodels. This constrains how natural the resulting\n‘NL’ actually is. In recent years, starting from the at-\ntention mechanism (Vaswani et al., 2017), the rapid\nprogression of pre-trained LLMs in NLP tends to\nunify many previous seemingly independent tasks\ninto one large pre-trained model, especially the\nGPT series from OpenAI (Brown et al., 2020), and\nT5 (Raffel et al., 2020) and PaLM (Chowdhery\net al., 2022) from Google. These models are pre-\ntrained with large amounts of natural sentences and\ncodes, intrinsically encoding much world knowl-\nedge (Creswell et al., 2022). The auto-regressive\nLLMs can naturally generate rich and meaningful\ncorpus based on the given prompt. Then many\nrecent work propose to do the data augmentation\nwith LLMs, such as generating medical dialogues\n(Chintagunta et al., 2021) and python codes (Chen\net al., 2021) via GPT-3. This inspires us the new\nopportunity in NL-to-TL task.\n4 Approach\nThere are 3 steps in our approach. First, generating\nlifted NL-STL dataset with LLMs. Second, finetun-\ning LLMs to get high accuracy on lifted NL-STL\ntransformation. Third, lifting the data and applying\nthe lifted model. Finally, we also consider the case\nwhere lifting is not possible and we must translate\nend to end by further finetuning the model.\n4.1 Data Generation\nWe apply the LLM GPT-3 (Davinci-003) to help\ngenerate multiple lifted NL and STL pairs. The first\nintuitive method is to use various NL-STL pairs as\nprompts and ask GPT-3 to automatically generate\nmore NL-STL pairs. However, it turns out that the\nAlgorithm 1 Algorithm for STL synthesis\nInput:\nMaximum number of APs N\nOutput:\nSynthesized pre-order STL\ntwo_subtree= [∧, ∨, ⇒, ⇔, U, U[a,b]]\none_subtree= [¬, F, G, F[a,b], G[a,b]]\nsub_lists ←Random prop_list with total length\n[1, N] ▷[prop_3, prop_1], [prop_2]\nEach sub_list ← insert operators in\none_subtree+ two_subtree ▷[⇔, ¬, prop_3,\nprop_1], [G, prop_2]\nAssembling sub_lists into pre-order STL by ap-\npending random two_subtreeoperators ▷\n[U[10,30], ⇔, ¬, prop_3, prop_1, G, prop_2]\nmodel always generates STL and NL with similar\nsyntactic structures as the given prompts, thus lim-\niting the sentence richness. To stimulate GPT-3 to\ngenerate sentences with more variations, we ask it\nto generate corresponding NLs from different STLs.\nThe whole framework (referred as Framework1) is\nshown in Figure 3. Various pre-order STLs are\nrandomly synthesized by binary tree generation al-\ngorithm (See Algorithm 1 and specific discussion\nin Appendix B). The pre-order STLs are then trans-\nformed into in-order expressions via rules. To make\nthe input STL more understandable to GPT-3, the\noperators are represented with the words of their\nmeanings ( ⇒ (imply),⇔ (equal),∨(or),etc).\nThen the GPT-3 will try to generate the raw NL\nwhose semantic meaning is close to the STL. Hu-\nman annotators then modify the raw NL to make\nits meaning consistent with the STL. During this\nprocess, the NL-STL pairs in prompts will be ran-\n15883\nFigure 3: Framework1 to generate NL-STL pairs.\nFigure 4: Framework2 to generate NL-STL pairs.\nOne extra loop between NL and STL is added.\ndomly chosen to make the vocabulary and sentence\nstructure more diversified. We gather 200 NL in-\nstructions from 10 volunteers who are familiar with\nrobot tasks and randomly choose 100 NL to serve\nas the prompt pool, while the other 100 NL serve\nas the Manual testing data. In each iteration of\nFramework1, 20 pairs are randomly chosen from\nthe prompt pool to form the prompt of GPT-3 (a\nprompt example is shown in Appendix C.1 and the\ndiscussion on how many examples should be in-\ncluded in GPT-3 prompt is shown in Appendix D).\nWhile Framework1 enhances the sentence rich-\nness, one issue is that the pure rule-based synthesis\nof STL sometimes generates unreasonable seman-\ntic meanings, or that the STL is too complex to\ndescribe it with NL. To solve this problem, an op-\ntimized framework (referred as Framework2) is\nshown in Figure 4. Compared to Framework1, an\nextra loop between STL and NL is added using\nGPT-3. In this way, the initial rule-based STL with\nunreasonable or complex meanings will be auto-\nmatically filtered by GPT-3 itself. In other words,\nduring the mapping from STL-1 to NL-1, GPT-3\nmore or less modifies the meanings of the STLs\nthat it cannot fully translate. Then the translated\nNL-1, though not fully consistent with STL-1, is\nmore reasonable in the view of humans. It turns out\nthat the semantic meanings synthesized by Frame-\nwork2 are closer to the common human languages,\nand NL-STL pairs contain many fewer errors to\nannotate. The average number of annotated pairs is\nabout 80 per person per hour with Framework1, and\nabout 120 per person per hour with Framework2.\nWe in total create 15108 lifted NL-STL pairs\ncombining both Framework1 and Framework2,\nwith the whole cost of around 150 person-hours.\nAppendix C.2 shows a prompt example to trans-\nform from NL-1 back into STL-2 via GPT-3, and\nAppendix E shows some example annotations of\nlifted NL-STL pairs. Appendix F explains the\nwhole process and the license of human annota-\ntion to correct NL-STL pairs.\nApart from synthesizing and annotating lifted\nNL-STL pairs with GPT-3, we also collect and an-\nnotate the data gathered from Wang et al. (2021)\nand He et al. (2022). Wang et al. (2021) focuses on\nrobot navigation task with LTL, and He et al. (2022)\nfocuses on circuit controlling with STL. To clean\nand process the data into lifted NL-STL pairs, the\nAPs in both two datasets are detected and hidden\nby combining hard-coded algorithms with entity\nrecognition package SpaCy (Honnibal and Mon-\ntani, 2017). We gather 5K lifted NL-STL pairs\nfrom Navigation dataset (Wang et al., 2021) and\n8K lifted NL-STL pairs from the Circuit dataset\n(He et al., 2022). Note that the original Navigation\ndataset uses LTL, while we correct some expres-\nsion formats to form into STL. The original Circuit\ndataset contains 120K NL-STL pairs, while we find\nincluding 8K examples into our dataset is informa-\ntive enough to cover the whole corpus richness of\nCircuit dataset.\nHence, in this work a dataset with in total about\n28K lifted NL-STL pairs are created. Appendix J.1\nshows the statistics of this lifted NL-STL dataset.\n4.2 Model Finetuning\nWe mainly apply the T5 model (Raffel et al., 2020)\nto serve as the base LLM for finetuning. To study\nwhether model sizes will influence the performance,\nT5-base (220M) and T5-large (770M) are both fine-\ntuned on the same data. The setting is illustrated in\nAppendix L.\n5 Experimental Results\nThough the corrected data from Navigation and\nCircuit studies already provide multiple examples,\n15884\nFigure 5: Testing accuracy vs. Number of created\nNL-STL pairs. The data collected from Navigation\nand Circuit work are all used during training. The\nGPT-3-assisted data refers to the data generated\nwith the help of GPT-3, and the Manual data refers\nto the instructions collected from volunteers. The\nfigure shows the necessity of the created data.\nthese datasets only cover limited conditions and\nlack generalization. To show the necessity of the\nnewly created data, the T5 models are finetuned\nwith varied number of created NL-STL pairs, as\nshown in Figure 5. During training, all the data\ncollected from Navigation and Circuit studies are\nused and the number of created data are varied\namong different models. The trained models are\nthen tested with either the created data (referred\nas GPT-3-assisted data test) or the NL instructions\ncollected from volunteers (referred as Manual data\ntest). Since minor difference in STLs can cause\nsevere difference in the real meanings, we apply\nthe binary accuracy as the metric, i.e., 100% right\nor not. We find that the Top-1 testing accuracy\nincreases greatly increasing the number of cre-\nated pairs, with the highest accuracy 97.52% and\n90.12% of GPT-3 assisted data and Manual data\ntesting, respectively.\nTable 1 presents the experimental results with the\ntargeted STL of different formats as discussed in\nSection 2.3. We find that using the in-order format\nplus replacing operators with words will largely\nimprove the performance. In-order format is more\nconsistent with natural sentence expressions and\nlowers the difficulty for finetuning an LLM. This\nresult is different from former conclusions when\ntraining Seq2Seq model for NL to STL/LTL tasks,\nwhere the pre-order format is better because it nat-\nurally avoids the issue of parentheses matching\n(Wang et al., 2021).\nT5-base T5-large\nP.O./word 70.00 ±1.42% 73 .10 ±1.05%\nI.O./word 96.43 ±0.72% 97.52 ±0.65%\nP.O./opera. 72.35 ±1.54% 71 .95 ±1.23%\nI.O./opera. 89.94 ±0.89% 88 .17 ±1.02%\nTable 1: Accuracy of GPT-3-assisted data testing\nfor training data with different expression formats.\nP.O. and I.O. represent Pre-order and In-order, re-\nspectively.\n6 Ablation Studies\nHuman Annotation To reveal the significance\nof human annotation, we train the model with the\nsame amount of raw pairs created by GPT-3 and\ntest them on corrected data. The results are shown\nin Appendix H.1. We find that annotated dataset\ncan improve the testing accuracy by around 10%.\nFramework2 To reveal the significance of\nthe data generated by Framework2, we train the\nmodel with either the same amount of data from\npure Framework1 or the data combining two\nframeworks. Utilizing both frameworks improves\nthe accuracy by around 2% (Appendix H.2).\nModel Capacity of T5 To reveal the sig-\nnificance of T5’s superior model capacity, we\ntrain a Seq2Seq model on the same lifted NL-STL\ndataset for comparison, as shown in Appendix I.\nFinetuning on T5 model improves the accuracy by\naround 14.5% compared to the Seq2Seq model.\n7 Application\nRight now we have finetuned T5 model to convert\nlifted NL into lifted STL. For the real applications,\nwe need one model to convert from full NL to full\nSTL, in which the format of APs should be regular-\nized. To reach this destination, we will display two\nmethods in the following discussion, and compare\nthem with other state-of-the-art models. We test\non five datasets across domains Circuit (He et al.,\n2022), Navigation (Wang et al., 2021), Office email\n(Fuggitti and Chakraborti, 2023), GLTL (Gopalan\net al., 2018; Tellex et al., 2020), and CW (Squire\net al., 2015). The examples of full NL-STL pairs\nin each domain are shown in Appendix G, and the\nstatistics of our synthesized dataset and each col-\nlected dataset are shown in Appendix J.2. Note\nthat some lifted NL-STL pairs in Circuit and Navi-\n15885\nCircuit Navigation Office email\nGPT-4 end-to-end (ad-hoc) 62% 87% 84%\nGPT-3 end-to-end 38.25 ±6.51% 50 .51 ±5.08% 58 .73 ±4.86%\nT5-large + GPT-3 AP detect 95.13 ±1.42% 95.03 ±1.20% 96.73 ±1.03%\nT5-base + GPT-3 AP detect 94.61 ±0.74% 94 .73 ±1.02% 96 .08 ±0.97%\nTable 2: Testing accuracy of full NL-to-STL task for each grounding model. The testing domains are:\nCircuit (He et al., 2022), Navigation (Wang et al., 2021), Office email (Fuggitti and Chakraborti, 2023).\nCircuit Navigation Office email\nGPT-3 AP detect accuracy 98.84 ±0.41% 99 .03 ±0.53% 100 .00 ±0.00%\nTable 3: Testing accuracy of recognizing APs with GPT-3 for each domain.\ngation datasets have been used during training the\nlifted model, while all the full NL-STL pairs have\nnot been seen. All the data in other three domains\nare independent of the finetuning in lifted models.\nOur model achieves higher accuracy on full NL-\nSTL transformation with much less training data\nacross all these domains.\n7.1 Lifted Model + GPT-3 AP Recognition\nIn the real applications, we have to formulate how\nthe APs are presented in STL (like ’verb_noun’)\nso that the specified APs can directly connect with\ncontrollers. As shown in Appendix M, we directly\nutilize GPT-3 to recognize APs in the sentence and\nhide them as \"prop_i\". Then the lifted model will\npredict the targeted lifted STL and the hidden APs\nwill be swapped into formatted form to generate\nthe full STL.\nTable 2 displays the performance accuracy of\nthis method. We test on three distinct domains and\ncompare with the GPT-3 end-to-end method, i.e.,\nusing GPT-3 to directly transform NL into STL.\nThe GPT-3 end-to-end method is proposed by Fug-\ngitti and Chakraborti (2023) recently, aiming to\ngeneralize into all various domains. However, in\nthe NL to STL/LTL task, finetuning on a much\nsmaller LLM like T5 is still greatly better than\ndirect few-shot learning on state-of-the-art LLM\nlike GPT-3 and GPT-4. Due to the limitation of\nGPT-4 access, we did an ad-hoc test on ChatGPT\nPlus with 100 samples in each domain. The ex-\nperimental results show that combining finetuned\nlifted model with AP recognition using GPT-3 can\nlead to a full task accuracy over 95% across all\nthree tested domains. Table 3 displays the perfor-\nmance of detecting APs with GPT-3. Compared\nto the direct NL to STL task, AP detection task is\nmuch easier to GPT-3. Hence, dividing the whole\ntask into AP recognition and semantic parsing are\nmore data-efficient and flexible than pure end-to-\nend method.\nTo further test model performance under varied\nsentence complexity, we plot the testing accuracy\nvs. the number of APs in Appendix K. As the\nnumber of APs in each lifted STL increases, the\naccuracy of GPT-3 few-shot learning decreases,\nwhile the finetuned T5-large model still performs\nwell.\n7.2 Transfer Learning\nOn the condition that we know how the users de-\nfine the representation of APs, the aforementioned\nmethod is suitable to predict the full STL. On the\nother hand, there is also the condition that we can-\nnot acquire the specific hard-coded rules to formu-\nlate AP representation, but only full NL-STL pairs.\nIn these cases, the direct further finetuning may\nhelp. In other words, the lifted model has learnt\nto parse the semantic logical relations, and the fur-\nther transfer learning is to learn how the APs are\nregulated in this specific dataset. This direct end-\nto-end transfer learning serves as the second way\nfor ground applications.\nTo show that our method is generalizable and\ndata-efficient, we compare our methods to the orig-\ninal Seq2Seq methods implemented in each dataset.\nSpecifically, in the Circuit dataset the authors train\nthe model from the ground using Transformer ar-\nchitecture (Vaswani et al., 2017), and in GLTL and\nCW datasets the authors implement recurrent neu-\nral network (RNN) encoder-decoder framework\nwith Gated Recurrent Unit (GRU) as the core RNN\n15886\nFigure 6: Experimental results for end-to-end transfer learning on Circuit, Navigation, and GLTL datasets.\nHere the training data number means how many full NL-STL pairs are used during transfer learning or\nSeq2Seq training. The blue curve represents the accuracy where T5 model first pre-trained on 28K lifted\nNL-STL pairs, and then finetuned on full NL-STL examples in that domain. The orange curve represents\nthe condition when T5 model is not pre-trained by lifted NL-STL pairs, but directly finetuned based on\ninitial released weights.\nFigure 7: Experimental results for end-to-end transfer learning on CW datasets. This experiment is to\ncompare generalizability of our method with the original state-of-the-art Seq2Seq method.\ncell. As the work on Navigation dataset uses the\nfinal task completion rate as the criterion not the\ndirect LTL accuracy, the LTL prediction accuracy\nis inherently low. For a fair comparison in Navi-\ngation dataset, we implements the same Seq2Seq\nframework as that in GLTL and CW datasets.\nThe experimental results are shown in Figure 6.\nCompared to the original Seq2Seq model proposed\nin each dataset, transfer learning with LLM is much\nmore efficient, and the pre-training on lifted NL-\nSTL pairs also displays a great saving on train-\ning data requirements. We also find that T5-large\nmodel performs better than T5-base model. In all\nthe three domains, the T5-large model with lifted\nNL-STL pre-training can achieve an accuracy near\n95% with only 200 to 500 full NL-STL examples.\nThis amount of example requirement is one magni-\ntude less than the Seq2Seq baselines.\nThe CW dataset is somewhat unique since it\nonly has 36 different LTLs, meaning there are on\naverage 50 different NLs corresponding to the same\nLTL. The study in Gopalan et al. (2018) applies\nthis dataset to test the generalizability of the models\nwithin the domain. They use some types of LTLs\nas the training examples for transfer learning, and\nthe left types of LTLs as the testing set. This is to\ntest whether the model can predict the LTLs that\nit has not seen during the training. We also carry\nout this experiment and compare with the method\nin the original paper. As shown in Figure 7, the\nLLM with finetuning is apparently better than the\noriginal baseline.\n15887\n8 Limitation\nIn spoken language, coreference is quite common,\nsuch as \"pick up the apple and then bring it to\nme\". Here \"apple\" and \"it\" refer to the same object.\nIn the five datasets we collected and tested, the\ncoreference problem is not severe since most NL\ndo not have pronouns. For further work, the NER\nmodels specialized in resolving coreferences and\nconverting them into normal APs are needed for\nmore unbounded input sentences.\nDuring the synthesis of varied STLs, here we\nuse direct algorithm-based method to generate STL\nbinary trees. To make the semantic meanings of\nSTLs closer to human spoken language, we add the\nextra NL-STL loop via GPT-3. However, another\nintuitive way is to fit the probable distribution of op-\nerators to be close to human spoken language. For\ninstance, the probability of two continued ’nega-\ntion’ operators is nearly zero. In this work we only\nset some hard rules to specify the STL synthesis.\nFurther work can focus on the fitting of operator\ndistributions and apply it into STL generation.\nThe evaluation metric here is pure binary ac-\ncuracy (fully correct or not). Actually, it is quite\ndifficult to judge the similarity or distance of two\nTLs. Simply calculating token matching or com-\nputing truth values both own drawbacks. A more\neffective metric is needed.\nThe output of LLMs may sometimes generate\nincorrect TLs. We build up rule-based methods\nto check syntactic correctness and correct errors\nlike parentheses matching. Further work can be\nadded to improve output correctness by modifying\ntraining procedures and loss functions.\n9 Conclusion\nWe propose a framework to achieve NL-to-TL\ntransformation with the assistance of LLM, from\naspects of both data generation and model train-\ning. One dataset with about 28K lifted NL-TL\npairs is then constructed by which the T5 model\nis finetuned. Two approaches are implemented to\nutilize the trained model into full NL-to-TL trans-\nlation. Experimental results on five varied domains\ndisplay much better accuracy and generalizablity\ncompared to original methods. The created dataset\ncan be used to train future NL-to-TL models and\nserve as the benchmark. The proposed framework\nto finetune LLMs with lifted NL-TL pairs makes\nit possible for generalizable NL-to-TL translation\nwithout the constraints of domains and input in-\nstruction structures.\nAlthough our framework is built on GPT-3, the\nrapid progression of LLMs can promote our frame-\nwork. The strong semantic parsing ability of newly\nreleased GPT-4 will mitigate the burden of human\nannotations in our method. We find that GPT-4 gen-\nerated STLs/NLs are closer to the correct answers,\ncompared to GPT-3 generated STLs/NLs. As fu-\nture work, we believe the model can be improved\nwith larger dataset containing more diversified cor-\npus with GPT-4 as the base model.\nAcknowledgements\nWe thank the help from the volunteers for contribut-\ning the natural language instructions.\nThis work was supported by ONR under Award\nN00014-22-1-2478 and MIT-IBM Watson AI Lab.\nHowever, this article solely reflects the opinions\nand conclusions of its authors. The authors would\nalso like to thank Lifu Huang, Zhiyang Xu, and\nYue Meng for the early-stage exploration and dis-\ncussion of the work.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\ngen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\nmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do as i can\nand not as i say: Grounding language in robotic af-\nfordances. In arXiv preprint arXiv:2204.01691.\nAdrian Boteanu, Thomas Howard, Jacob Arkin, and\nHadas Kress-Gazit. 2016. A model for verifiable\ngrounding and execution of complex natural language\ninstructions. In 2016 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems (IROS), pages\n2649–2654. IEEE.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\n15888\nMichael C. Browne, Edmund M. Clarke, David L. Dill,\nand Bud Mishra. 1986. Automatic verification of\nsequential circuits using temporal logic. IEEE Trans-\nactions on Computers, 35(12):1035–1044.\nAndrea Brunello, Angelo Montanari, and Mark\nReynolds. 2019. Synthesis of ltl formulas from nat-\nural language texts: State of the art and research\ndirections. In 26th International Symposium on Tem-\nporal Representation and Reasoning (TIME 2019).\nSchloss Dagstuhl-Leibniz-Zentrum fuer Informatik.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nIgor Buzhinsky. 2019. Formalization of natural lan-\nguage requirements into temporal logics: a survey.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nBharath Chintagunta, Namit Katariya, Xavier Amatri-\nain, and Anitha Kannan. 2021. Medically aware\ngpt-3 as a data generator for medical dialogue sum-\nmarization. In Machine Learning for Healthcare\nConference, pages 354–372. PMLR.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. arXiv\npreprint arXiv:2205.09712.\nE Allen Emerson. 1990. Temporal and modal logic.\nIn Formal Models and Semantics, pages 995–1072.\nElsevier.\nCameron Finucane, Gangyuan Jing, and Hadas Kress-\nGazit. 2010. Ltlmop: Experimenting with language,\ntemporal logic and robot control. In 2010 IEEE/RSJ\nInternational Conference on Intelligent Robots and\nSystems, pages 1988–1993. IEEE.\nFrancesco Fuggitti and Tathagata Chakraborti. 2023.\nNL2LTL – a python package for converting natural\nlanguage (NL) instructions to linear temporal logic\n(LTL) formulas. In AAAI. System Demonstration.\nNakul Gopalan, Dilip Arumugam, Lawson LS Wong,\nand Stefanie Tellex. 2018. Sequence-to-sequence\nlanguage grounding of non-markovian task specifi-\ncations. In Robotics: Science and Systems, volume\n2018.\nDavid Gundana and Hadas Kress-Gazit. 2022. Event-\nbased signal temporal logic tasks: Execution and\nfeedback in complex environments. IEEE Robotics\nand Automation Letters, 7(4):10001–10008.\nJie He, Ezio Bartocci, Dejan Niˇckovi´c, Haris Isakovic,\nand Radu Grosu. 2022. Deepstl: from english re-\nquirements to signal temporal logic. In Proceedings\nof the 44th International Conference on Software\nEngineering, pages 610–622.\nMatthew Honnibal and Ines Montani. 2017. spacy 2:\nNatural language understanding with bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear, 7(1):411–420.\nThomas M Howard, Stefanie Tellex, and Nicholas Roy.\n2014. A natural language planner interface for mo-\nbile manipulators. In 2014 IEEE International Con-\nference on Robotics and Automation (ICRA), pages\n6652–6659. IEEE.\nEric Hsiung, Hiloni Mehta, Junchi Chu, Xinyu Liu,\nRoma Patel, Stefanie Tellex, and George Konidaris.\n2021. Generalizing to new domains by map-\nping natural language to lifted ltl. arXiv preprint\narXiv:2110.05603.\nRon Koymans. 1990. Specifying real-time proper-\nties with metric temporal logic. Real-time systems,\n2(4):255–299.\nSebastian Maierhofer, Anna-Katharina Rettinger,\nEva Charlotte Mayer, and Matthias Althoff. 2020.\nFormalization of interstate traffic rules in temporal\nlogic. In 2020 IEEE Intelligent Vehicles Symposium\n(IV), pages 752–759.\nOded Maler and Dejan Nickovic. 2004. Monitoring\ntemporal properties of continuous signals. In Formal\nTechniques, Modelling and Analysis of Timed and\nFault-Tolerant Systems, pages 152–166. Springer.\nRoma Patel, Ellie Pavlick, and Stefanie Tellex. 2019.\nLearning to ground language to temporal logical\nform. In NAACL.\nRoma Patel, Ellie Pavlick, and Stefanie Tellex. 2020.\nGrounding language to non-markovian tasks with\nno supervision of task specifications. In Robotics:\nScience and Systems.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nVasumathi Raman, Constantine Lignos, Cameron Finu-\ncane, Kenton CT Lee, Mitchell P Marcus, and Hadas\nKress-Gazit. 2013. Sorry dave, i’m afraid i can’t\ndo that: Explaining unachievable robot tasks using\nnatural language. In Robotics: science and systems,\nvolume 2, pages 2–1. Citeseer.\n15889\nShawn Squire, Stefanie Tellex, Dilip Arumugam, and\nLei Yang. 2015. Grounding english commands to\nreward functions. In Robotics: Science and Systems.\nStefanie Tellex, Nakul Gopalan, Hadas Kress-Gazit, and\nCynthia Matuszek. 2020. Robots that use language.\nAnnual Review of Control, Robotics, and Autonomous\nSystems, 3(1).\nStefanie Tellex, Thomas Kollar, Steven Dickerson,\nMatthew R Walter, Ashis Gopal Banerjee, Seth Teller,\nand Nicholas Roy. 2011. Approaching the symbol\ngrounding problem with probabilistic graphical mod-\nels. AI magazine, 32(4):64–76.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nChristopher Wang, Candace Ross, Yen-Ling Kuo,\nBoris Katz, and Andrei Barbu. 2021. Learning a\nnatural-language to ltl executable semantic parser for\ngrounded robotics. In Conference on Robot Learning,\npages 1706–1718. PMLR.\n15890\nA STL illustration\nF[a,b]ϕ True at time tif there exists a time in the interval [t+ a,t + b] where ϕis true.\nϕU[a,b]φ True at time tif φis true for some time t\n′\nin the interval [t+ a,t + b], and for\nall times between tand t\n′\n, the formula ϕholds.\nG[a,b]ϕ True at time tif for all times in the interval [t+ a,t + b], the formula ϕholds.\nTable 4: STL illustration\n15891\nB Full algorithm to synthesize multiple STLs\nThis is the full algorithm to synthesize multiple varied STLs. The blue-colored words are the example\noutput in each step. All the operators are classified into the operator with only one subtree, or the operator\nwith two subtrees. A random ordered proplist is generated with the length less than the upper limit. Then\nthis full list is split into some sub_lists. For each sub_list, operators are randomly appended in the left\nside until each propoccupy one position in the binary tree. Then these modified sub_lists are assembled\nback into the full STL by appending operators with two subtrees. The STL generated in this way are\nsyntactically correct, but may own some flaws in semantic meanings. Some rules are pre-set to avoid the\nunreasonable conditions, e.g., two negation operation should not appear continually.\nAlgorithm 2 Full algorithm for STL synthesis\nInput:\n1: Maximum number of APs N\nOutput:\n2: Synthesized pre-order STL\n3:\n4: two_subtree= [∧, ∨, ⇒, ⇔, U, U[a,b]],\n5: one_subtree= [¬, F, G, F[a,b], G[a,b]]\n6: AP_num = random.randint(1, N) ▷e.g., 3\n7: prop_list ←Random_ordered Prop list with length AP_num\n8: sub_lists ←randomly divide prop_list ▷e.g., [prop_3, prop_1], [prop_2]\n9:\n10: Creating sub-STLs:\n11: for each sub_list do\n12: num_open_subtree = len(sub_list)\n13: while num_open_subtree > 1 do\n14: operation ←randomly choose item in two_subtree+ one_subtree\n15: if operation in two_subtreethen\n16: num_open_subtree -= 1\n17: end if\n18: if operation in [U[a,b], F[a,b], G[a,b]] then\n19: a,b ←sampling random integers or denoting as infinity\n20: end if\n21: sub_list.insert(0, operation)\n22: end while\n23: save sub_list as sub_STL ▷e.g., [⇔, ¬, prop_3, prop_1], [G, prop_2]\n24: end for\n25:\n26: Assembling sub-STLs:\n27: Assembling sub_STLs into pre-order STL by appending random two_subtreeoperations ▷e.g.,\n[U[10,30], ⇔, ¬, prop_3, prop_1, G, prop_2]\nC Examples of prompt input to GPT-3\nThese are the example prompts for GPT-3 to convert between NL and STL, or detect the spans of Atomic\nProportions.\n15892\nFigure 8: Prompts for converting from synthesized STL to NL via GPT-3.\nC.1 Prompt example from in-order STL to NL via GPT-3\nFigure 8 is a prompt example for GPT-3 to convert from STL to its corresponding NL. The input STL\nfollows in-order expression with all the operators replaced by words with the same meanings. The prompt\ncontains 20 NL-STL pairs, which are randomly picked up from 100 examples and are changed constantly\nduring data creation.\n15893\nFigure 9: Prompts for converting from NL to pre-order STL via GPT-3.\nC.2 Prompt example from NL to pre-order STL via GPT-3\nFigure 9 is a prompt example for GPT-3 to convert from NL to its corresponding STL. The output STL\nfollows pre-order expression. We have tested that GPT-3 acts with close performance when STL follows\neither pre-order or in-order formats.\n15894\nFigure 10: Prompts for AP recognition via GPT-3.\nC.3 Prompt example for AP recognition via GPT-3\nFigure 10 is a prompt example for applying GPT-3 to detect APs in natural sentences. In this example, the\nspecific domain is Navigation.\n15895\nFigure 11: Number of prompt pairs vs. GPT-3 performance.\nD Number of NL-STL pairs in GPT-3 prompts\nAs shown in Figure 11, here we ask GPT-3 to transform from NL to STL and tune the number of NL-STL\npairs in the prompt to detect the accuracy evolution. We test on the NL whose targeted STL have the\nnumber of APs to be two or three. We find that the prediction accuracy given by GPT-3 will arise with the\nnumber of example pairs and turn into a plateau when the number of example pairs increases to larger\nthan 20. Here we choose the number of pairs to be 20 in the prompt.\n15896\nSTL (pre-order+operator) [’<->’, ’->’, ’prop_2’, ’prop_3’, ’F[55,273]’, ’prop_1’]\nSTL (in-order+word) ((prop_2 imply prop_3) equal finally[55,273] prop_1)\nRaw natural sentence If (prop_2) implies (prop_3), then (prop_1) will happen at some point\nduring the next 55 to 273 time units .\nAnnotated natural sentence If (prop_2) implies (prop_3), then (prop_1) will happen at some point\nduring the next 55 to 273 time units, and vice versa .\nSTL (pre-order+operator) [’U[400,infinite]’, ’->’, ’prop_3’, ’prop_1’, ’negation’, ’prop_2’]\nSTL (in-order+word) ((prop_3 imply prop_1) until[400,infinite] negation prop_2)\nRaw natural sentence If (prop_3), then do (prop_1) and keep doing it until (prop_2) hap-\npens, but this should never happen .\nAnnotated natural sentence If (prop_3), then do (prop_1) and keep confirming to the above state\nuntil (prop_2) does not happens at some point after the 400 time units\nfrom now .\nSTL (pre-order+operator) [’<->’, ’negation’, ’prop_1’, ’U[279,438]’, ’prop_3’, ’prop_2’]\nSTL (in-order+word) (negation prop_1 equal (prop_3 until[279,438] prop_2))\nRaw natural sentence The scenario in which ( prop_1 ) happens is the same as the scenario\nin which ( prop_3 ) happens and continues until at a certain time\npoint during the 279 to 438 time units ( prop_2 ) happens .\nAnnotated natural sentence The scenario in which ( prop_1 ) does not happen is the same as the\nscenario in which ( prop_3 ) happens and continues until at a certain\ntime point during the 279 to 438 time units ( prop_2 ) happens .\nTable 5: Example annotations from synthesized STLs to raw natural sentences, and further to annotated\nnatural sentences.\nE Example annotations of lifted NL-STL pairs\nAs shown in Table 5.\nF Process of human annotation\nAll the human annotators are researchers in the area of formal methods and robot planning with\nextensive knowledge in temporal logics. The annotators are all volunteers from the authors’ institute and\ncollaborative institutes. Before the data annotation and collection, we have notified them that the data will\nbe used to train the language model to transform from natural language instructions to temporal logics,\nand that both the annotated data and model will be open to the public. All the voluntary annotators have\nagreed to the statement to use their annotated data. As for the task instruction, the annotators participate\nin the guidance meeting and are provided with a guidance list on temporal logics and some example\nannotation pairs (the guidance list and examples of annotation are available on github page). Each\nannotator annotated 50 pairs initially, and sent their results to other randomly assigned annotators for\ncross-checking. Finally, the authors also checked all pairs to ensure the accuracy of the annotated data.\nG Example full NL-STL pairs of each specialized dataset\nAs shown in Table 6.\n15897\nNavigation STL finally ( acquire_v pear_n ) and globally ( finally ( go_to_v waste_basket_n\n) )\nNL when possible acquire pear and repeatedly go to waste basket .\nNavigation STL finally ( got_to_v house_n ) and finally ( go_near_v house_n )\nNL at any time got to house and when possible go near house .\nNavigation STL advance_to_v tree_n imply finally ( get_to_v flag_n )\nNL advance to tree means that when possible get to flag .\nCircuit STL globally ( signal_1_n math equal 89.3 or ( signal_2_n more 42.4 and\nsignal_2_n less 91.5 ) imply globally [0,34] ( finally [0,98] ( signal_3_n\nmore equal 11.5 and signal_3_n less equal 23.4 ) ) )\nNL In the case the signal_1_n signal is 89.3 , or the signal_2_n signal is greater\nthan 42.4 and below 91.5 , then for every time instant during the coming\n34 time units , there needs to exist a certain time instant during the next\n98 time units , at which the value of signal signal_3_n should be no less\nthan 11.5 and less than or equal to 23.4 eventually .\nCircuit STL finally ( signal_1_n less 92.6 and signal_2_n more equal 57.3 )\nNL At a certain time instant in the future before the end of the simulation\nsignal_1_n is ultimately below 92.6 and signal_2_n will be ultimately at\nleast 57.3 .\nCircuit STL finally ( ( signal_1_n more equal 4.1 and signal_1_n less equal 59.0 ) or\nsignal_2_n math equal 41.1 )\nNL There has to be a certain time instant in the future before the end of the\nsimulation , at which the value of signal_1_n needs to be greater than or\nequal to 4.1 and less than or equal to 59.0 eventually , or signal_2_n finally\nkeeps equal to 41.1 .\nGLTL STL finally ( ( red_room or blue_room ) and finally green_room )\nNL enter the blue or orange room and proceed until the green room .\nGLTL STL ( finally ( blue_room ) and globally ( negation green_room ) )\nNL move to the blue room without entering a lime room .\nGLTL STL ( finally ( yellow_room ) and globally ( negation blue_room ) )\nNL only go through rooms that are not purple to get to the yellow room .\nCW STL finally ( blue_room and finally green_room )\nNL please go to the green room through the blue room .\nCW STL finally red_room\nNL i want you to go into the red room .\nCW STL finally ( ( red_room or yellow_room ) and finally green_room )\nNL go thru the yellow or red box to get to the green box .\nOffice\nemail\nSTL globally ( ( ( a new incident is created in Eventribe ) and ( a response is\ncreated in Trello ) ) imply ( creating an object in Gmail ) )\nNL When the transition action that a new incident is created in Eventribe does\nnot get observed , and a response is created in Trello , then the following\ncondition is true : promptly creating an object in Gmail .\nOffice\nemail\nSTL ( ( sync Microsoft Teams data ) until finally ( sending me an SAP and\nSalesforce ) )\nNL sync Microsoft Teams data until when possible sending me an SAP and\nSalesforce .\nOffice\nemail\nSTL globally ( ( ( a new lead is added in Marketo ) and ( creating a new Marketo\ncard ) ) imply ( a new lead is added in Microsoft Teams ) )\nNL On condition that a new lead is added in Marketo and creating a new\nMarketo card , then the event that a new lead is added in Microsoft Teams\nneeds to occur at the same time instant .\nTable 6: Examples of full NL-STL pairs in each specialized domain.\n15898\nFigure 12: Testing accuracy vs. Number of raw NL-STL pairs. The data collected and re-annotated\nfrom Navigation and Circuit work are all used during training. The GPT-3-assisted data refers to the data\ngenerated with the help of GPT-3, and the Manual data refers to the instructions collected from volunteers.\nH Ablation Studies\nH.1 Significance of Human Annotation\nThis part is to demonstrate the significance of human annotation for the GPT-3 synthesized data. Figure 12\nshows the model accuracy under varied number of training raw pairs. The great thing is that the T5-large\nmodel can still achieve a highest testing accuracy of 87.3% and 79.4% on the GPT-3-assisted data and\nManual data test, even only using the raw data synthesized from GPT-3. However, compared to the results\nin Figure 5, models trained on annotated data achieves accuracy about 10% higher than models trained on\nraw data.\nH.2 Significance of Framework2\n3K dataset 4.5K dataset\nDomain 1.5K F1 + 1.5K F2 3K F1 3K F1 + 1.5K F2 4.5K F1\nRaw data 78.85 ±1.04% 75 .79 ±0.98% 80.48 ±0.71% 79 .04 ±0.64%\nAnnotated data 80.57 ±0.86% 79 .76 ±0.88% 88.32 ±0.84% 86 .51 ±0.77%\nTable 7: Testing accuracy of the models with different training datasets. The training data are either\nraw or annotated, pure from Framework1 (F1) or combining with Framework2 (F2). The experimental\nresults show that the annotated dataset can apparently improve the performance of the model, and models\ncombining the data generated by F1 and F2 outperform the models trained with the same amount of pure\nF1 data.\n15899\nFigure 13: Testing accuracy vs. Number of training lifted NL-STL pairs. Here we use T5-large and\nSeq2Seq models to train on the lifted data. We detect that the Seq2Seq model reaches a highest accuracy\nat 83%, while T5-large model reaches a highest accuracy at 97.5%.\nI Model Capacity\nAs shown in Figure 13, T5-large performs much better than Seq2Seq model when training on the same\nlifted dataset. This reveals the significance to use LLM in this NL-to-TL task.\n15900\nJ Datasets statistics\nJ.1 Statistics of lifted NL-STL dataset\n# APs per STL # Operators per STL\navg. median max avg. median max\n2.906 3 7 3.206 3 8\nTable 8: Lifted STL formula statistics: # APs for each formula, # STL operators for each formula.\n# Words per Sent.\n# Sent. # Vocab avg. median max min\n28466 2296 18.358 17 72 3\nTable 9: Lifted sentence statistics: # unique sentences, # unique words (vocab), # words per sentence.\nJ.2 Statistics of corpus richness\nDomain # STL/ #Sent. # Unique STL # Vocab.\nSynthesized dataset 15K 14438 2121\nCircuit (He et al., 2022) 120K 3653 265\nNavigation (Wang et al., 2021) 5K 149 131\nGLTL (Gopalan et al., 2018) 11K 193 193\nCW (Squire et al., 2015) 3.3K 39 188\nOffice email (Fuggitti and Chakraborti, 2023) 0.15K 23 143\nTable 10: Statistics of STL formulas and NL sentences in our synthesized dataset and the collected dataset\nof each domain. # STL/ #Sent. reveals total number of samples. # Unique STL counts the number of\ndifferent STL formulas. # V ocab. counts the vocabulary in each dataset. Compared to previously collected\ndataset, our synthesized dataset owns much larger number of unique STLs and vocabulary, revealing\ngreater corpus richness.\n15901\nFigure 14: Accuracy vs. number of APs in each lifted STL. We carry out the test with both finetuned\nlifted T5-model and GPT-3 end-to-end method.\nK Accuracy evolution with AP number\nThis section is to illustrate that directly applying GPT-3 to predict STL from NL via few-shot learning\nlargely decreases the accuracy when the sentence structure is complex. Here we hypothesize that sentence\ncomplexity is positively related to the number of APs. As shown in Figure 14, the prediction accuracy\ndecreases rapidly with increasing AP number using GPT-3 end-to-end method. On the other hand, the\nmethod to finetune the T5-large using synthesized NL-STL pairs remains high accuracy across different\nAP numbers.\nL Details of implementation\nFor all the finetuning experiments on both T5-base and T5-large models, we choose the learning rate as\n2e-5, a batch size of 16, a weight decaying ratio as 0.01, and run 20 epochs for each setting. Experiments\non average finish in 3 hours for T5-base, and 10 hours for T5-large, on a single Nvidia RTX 8000 GPU.\nAverage results and standard deviations are typically acquired from 3 runs with seeds [1203, 309, 316],\napart from the transfer learning in CW dataset where 10 runs are carried with seeds [1203, 309, 316, 34,\n64, 128, 256, 512, 1234, 234]. For the finetuning on lifted models, the input dataset is split into training\nset (0.9) and testing set (0.1).\n15902\nFigure 15: Illustration of full STL conversion by combing with AP recognition task using GPT-3.\nM Full STL conversion by combining lifted model with AP recognition\nIllustrated in Figure 15\n15903",
  "topic": "Generalizability theory",
  "concepts": [
    {
      "name": "Generalizability theory",
      "score": 0.7475094199180603
    },
    {
      "name": "Computer science",
      "score": 0.7166733145713806
    },
    {
      "name": "Natural language processing",
      "score": 0.6335393190383911
    },
    {
      "name": "Generalization",
      "score": 0.5877612829208374
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5767261385917664
    },
    {
      "name": "Task (project management)",
      "score": 0.5250601172447205
    },
    {
      "name": "Natural language",
      "score": 0.5218150019645691
    },
    {
      "name": "Baseline (sea)",
      "score": 0.43745267391204834
    },
    {
      "name": "Sequence (biology)",
      "score": 0.43653595447540283
    },
    {
      "name": "Annotation",
      "score": 0.4278775751590729
    },
    {
      "name": "Psychology",
      "score": 0.09582546353340149
    },
    {
      "name": "Mathematics",
      "score": 0.0921860933303833
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    }
  ]
}