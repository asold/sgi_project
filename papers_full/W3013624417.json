{
    "title": "Finnish Language Modeling with Deep Transformer Models",
    "url": "https://openalex.org/W3013624417",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2792067347",
            "name": "Jain, Abhilash",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288976012",
            "name": "Ruohe, Aku",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288936507",
            "name": "Grönroos, Stig-Arne",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221772979",
            "name": "Kurimo, Mikko",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1816313093",
        "https://openalex.org/W46679369",
        "https://openalex.org/W2784121710",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2053306448",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2747917286",
        "https://openalex.org/W2963263347",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2949779614",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2250618788",
        "https://openalex.org/W2963341956"
    ],
    "abstract": "Transformers have recently taken the center stage in language modeling after LSTM's were considered the dominant model architecture for a long time. In this project, we investigate the performance of the Transformer architectures-BERT and Transformer-XL for the language modeling task. We use a sub-word model setting with the Finnish language and compare it to the previous State of the art (SOTA) LSTM model. BERT achieves a pseudo-perplexity score of 14.5, which is the first such measure achieved as far as we know. Transformer-XL improves upon the perplexity score to 73.58 which is 27\\% better than the LSTM model.",
    "full_text": "1\nFinnish Language Modeling with Deep Transformer\nModels\nAbhilash Jain Aalto University\nabhilash.jain@aalto.ﬁ\nAku Ruohe Aalto University\naku.ruohe@aalto.ﬁ\nStig-Arne Grnroos Aalto University\nstig-arne.gronroos@aalto.ﬁ\nMikko Kurimo Aalto University\nmikko.kurimo@aalto.ﬁ\nAbstract—Transformers have recently taken the centre stage in\nlanguage modeling after LSTM’s were considered the dominant\nmodel architecture for a long time. In this project, we investigate\nthe performance of the Transformer architectures-BERT and\nTransformer-XL for the language modeling task. We use a sub-\nword model setting with the Finnish language and compare it\nto the previous State of the art (SOTA) LSTM model. BERT\nachieves a pseudo-perplexity score of 14.5, which is a ﬁrst such\nmeasure achieved as far as we know. Transformer-XL improves\nupon the perplexity score to 73.58 which is 27% better than the\nLSTM model.\nIndex Terms —Language modeling, Transformer, BERT,\nTransformer-XL\nI. I NTRODUCTION\nLanguage modeling is a probabilistic description of lan-\nguage phenomenon. It provides essential context to distinguish\nwords which sound similar and therefore has one of the most\nuseful applications in Natural Language Processing (NLP) es-\npecially in downstreaming tasks like Automatic Speech Recog-\nnition (ASR). Recurrent Neural Networks (RNN) especially\nLong Short Term Memory (LSTM) networks [1] have been the\ntypical solution to language modeling which do achieve strong\nresults. In spite of these results, their fundamental sequential\ncomputation constraint has restricted their use in the modeling\nof long-term dependencies in sequential data. To address these\nissues Transformer architecture was introduced. Transformers\nrelies completely on an attention mechanism to form global\ndependencies between input and output. It also offers more\nparallelization and has achieved SOTA results in language\nmodeling outperforming LSTM models [2].\nIn recent years,we have seen a lot of development based on\nthis standard transformer models particularly on unsupervised\npre-training([3–8] which have set state-of-the art results on\nmultiple NLP benchmarks. One such model architecture has\nbeen the Bidirectional Encoder Representations from Trans-\nformers (BERT) model which uses a deep bidirectional trans-\nformer architecture.\nAnother architecture of interest would be the Transformer-\nXL, which introduces the notion of recurrence in a self-\nattention model.\nThe primary research focus though has been mostly on\nEnglish language for which abundant data is present. It is\ninteresting to see the performance of these models for an\nagglutinative language like Finnish, which is morphologically\nricher than English.\nIn this project, we explore the implementation of\nTransformer-based models (BERT and Transformer-XL) in\nlanguage modeling for Finnish. We will use the same training\ndata as in [9] so that we can do fair comparisons with the\nperformance of the LSTM models. Also, as the BERT model\nis a bi-directional transformer, we will have to approximate the\nconditional probabilities given a sequence of words. We also\nexperiment with using sub-word units with Transformer-XL to\ncope with the large vocabulary problems associated with the\nFinnish Language. With smaller units, the modeled sequences\nare longer, and we hope that the recursive XL architecture can\nallow us to still model long term effects. To the best of our\nknowledge this is the ﬁrst work with the Finnish language to\nuse the following:\n• Approximation of perplexity using a BERT architecture\n• Using Transformer-XL architecture with sub-word units.\n• Comparison of Transformer and LSTM models as lan-\nguage models in the same comparable settings with an\nagglutinative language.\nII. B ACKGROUND & METHODS\nThe goal of an language model is to assign meaningful\nprobabilities to a sequence of words. Given a set of tokens\nX = ( x1,....,x T ), where T is the length of a sequence,\nour task is to estimate the joint conditional probability P(X)\nwhich is\nP(X) =\nT∏\ni=1\np(xi|x1,...,x i−1) , (1)\nwere (x1,...,x i−1) is the context. An Intrinsic evaluation\nof the performance of Language Models is perplexity (PPL)\nwhich is deﬁned as the inverse probability of the set of the\ntokens and taking the Tth root were T is the number of tokens\nPPL(X) =P(X)−1/T . (2)\narXiv:2003.11562v2  [cs.CL]  27 Mar 2020\n2\nIn our two approaches we use transformer based architectures:\nBERT and Transformer-XL as mentioned before. Calculating\nthe auto-regressive P(X) for the transformer-XL is quite\nstraight-forward as the model is unidirectional but it doesn’t\nfactorize the same way for a bi-directional model like BERT.\nBERT’s bi-directional context poses a problem for us to\ncalculate an auto-regressive joint probability. A simple ﬁx\ncould be that we mask all the tokens x>i and we calculate\nthe conditional factors as we do for an unidirectional model.\nBy doing so though, we loose upon the advantage of bi-\ndirectional context the BERT model enables. We propose an\napproximation of the joint probability as,\nP(X) ≈\nT∏\ni=1\np(xi|x1,...,x i−1,xi+1,...,x T ) . (3)\nThis type of approximations has been previously explored with\nBi-directional RNN LM’s [10] but not for deep transformer\nmodels. We therefore, deﬁne a pseudo-perplexity score from\nthe above approximated joint probability.\nThe original BERT has two training objectives: ’Masked\nlanguage modelling’, in which you mask input tokens ran-\ndomly and then predict the masked tokens using the left\nand right context. Additionally, there is the ’next sentence\nprediction’ task that jointly trains text-pair representations.\nFor training the Masked language model the original BERT\nused Byte Pair Encoding (BPE) [11] for subword tokenization\n[12].For example the rare word ”unaffable” to be split up into\nmore frequent subwords such as [”un”, ”##aff”, ”##able”]. To\nremain consistent with experiments performed with LSTM’s\nwe use the morfessor for the subword tokenization in the\nFinnish Language. In Addition, we also apply boundary mark-\ners as in (Table I) and train two separate models using this\ndistinction. We train with left-marked markings as the original\nBERT was trained with such a scheme and the left+right-\nmarked as it was the previous SOTA with the Finnish Lan-\nguage. For the transformer-XL experiments, we just train with\nthe left+right marked scheme.\nTABLE I\nTWO METHODS OF MARKING SUBWORD UNITS SUCH THAT THE ORIGINAL\nSENTENCE ’TWO SLIPPERS ’ IS RECONSTRUCTED\nsubword marking Example\nleft+right-marked (+m+) two slipp+ +er+ +s\nleft-marked (+m) two slipp +er +s\nThe Next Sentence Prediction (NSP) is a binary classiﬁca-\ntion task which predicts whether two segments follow each\nother in the original text. This pre-training task was proposed\nto further improve the performance on downstreaming tasks,\nlike Natural Language Inference(NLI) but in reality removing\nthe NSP loss matches or slightly improves the downstream\ntask performance [13]. In this paper, we have omitted the NSP\ntask from the BERT pre-training procedure and changed the\ninput from a SEGMENT-PAIR input to a SINGLE SEGMENT\ninput. As seen in (Fig 1)\nTransformer-XL introduced the notion of recurrence in self-\nattention by caching the hidden state sequence to compute the\nhidden states of a new segment. It also introduces a novel rela-\ntive positional embedding scheme and both of them combined\naddress the issue of ﬁxed context lengths. Transformer-XL as\nmentioned is a unidirectional deep transformer architecture,\ntherefore the perplexity can be calculated as (Eq 2). The only\nchange is in the input format, were we use sub-word units\nrather than whole word units as Finnish is morphologically\nricher than English.\nIII. D ATA\nThe Finnish text data used for the language modeling task is\nprovided by [14]. The dataset consists mainly of newspapers\nand books of around 144 million word tokens and 4.2 million\nunique tokens. We use a Morfessor 2.0 [15] using the basic\nunsupervised Morfessor Baseline algorithm [16] with a corpus\nweight parameter ( α) of 0.001. We have a vocabulary of 34K\nsubword tokens for the left+right-marked (+m+) markings and\n19K subword tokens for the left-marked (+m) markings. We\nalso pre-process the data to remove any punctuation marks\nsuch that we can use the same data with an ASR system. The\ninput is one sentence per line and we shufﬂe the sentences\nat each epoch. The data is randomly divided into- training\ndataset and a validation dataset. The test dataset consists of\n2850 Finnish news articles obtained from the Finnish national\nbroadcaster YLE.\nIV. E XPERIMENTS & RESULTS\nA. BERT\nAll BERT experiments were trained for 500K steps. The\ncode was written in Python and we used the Tensorﬂow\nlibraries to create the models. The experiments were trained\non a single NVIDIA Tesla V100 32 GB graphic card. The data\nwas ﬁrst processed into Tensorﬂow records as the input to the\nmodel. The set of hyperparameters which we found optimal\nafter experimenting with different sets are in (Table II).\nTABLE II\nBERT HYPERPARAMETERS\nNumber of hidden layers 20\nHidden size of transformer 896\nNumber of attention heads 16\nIntermediate size(Size of the feed forward layer) 3584\nhidden activation function Gaussian Error Linear Units\ndropout probability 0.1\nmax position embeddings 300\nThis set of parameters were chosen as there training per-\nformances were better than smaller models on modelling the\nlong sequences of sub-words. We use the Adam optimizer\n[17] same as the English BERT. A maximum sequence length\nof 300 encompasses 98 percent of the training data and also\nallows us to ﬁt larger models on the GPU card. Hyper-\nparameter optimization is very difﬁcult in case of these models\nas they take around 15 days to train given the resources. The\nhyperparameter choices were therefore more dependant on\nthe original BERT with little tweaks. We assess the training\nperformance of the the model in the (Table III).\nWhen we train the BERT model we mask some percentage\nof the input tokens at random, and then predict those masked\n3\nFig. 1. BERT-Original sentence ’how are you doing today’\nTABLE III\nBERT TRAINING PERFORMANCE\nModel Masked LM Loss Masked LM Accuracy\nleft+right-marked (+m+) 2.24 0.56\nleft-marked (+m) 2.03 0.59\ntokens, this is known as Masked LM. The masked LM loss,\nrefers speciﬁcally to the loss when the masked language model\npredicts on the masked tokens. The masked LM accuracy\nrefers speciﬁcally to the accuracy with which the model\npredicts on the masked tokens. The loss for both the models\nare far off from the Masked LM loss of the English BERT, key\ndifference being the pre-training data for both the language\nmodels are quite different. Google training their model on\n3.3 Billion words from BooksCorpus [18] and the English\nWikipedia and our model being trained on 144 million words.\nComparing the two Finnish models, the left-marked model has\na better training performance than left+right-marked model.\nThe results of the pseudo-perplexity described in the pre-\nvious section to evaluate the above models on the test data-\nset is in table (Table IV).The test dataset is of a different\ncontext when compared to the training data, and interestingly\nboth the models are quite conﬁdent when it comes to the test\ndataset. The pseudo-perplexity values of left-marked are lower\nwhen compared to left-right-marked signifying that it is more\nconﬁdent.\nWe cannot directly compare the perplexity scores BERT\nmodel with a unidirectional LSTM model as both are calcu-\nlated in a different manner. We can experiment to compare\nit with a Bi-directional LSTM or use a downstreaming task\nto compare both the performances. We could also randomly\nmask tokens and then compare the prediction accuracy on the\nmasked tokens.\nTABLE IV\nBERT T EST PERFORMANCE\nModel Pseudo perplexity\nleft+right-marked (+m+) 17.1\nleft-marked (+m) 14.5\nB. Transformer-XL\nAll Transformer-XL experiments are also trained equally for\n500K steps. The code was written in Python and we used the\nPyTorch libraries for model creation. The experiments were\ntrained on a single NVIDIA Tesla V100 32 GB graphic card.\nTwo sets of hyperparameters were chosen to be compared after\nsome initial optimization and are in (Table V) From the above\nTABLE V\nTR-XL HYPERPARAMETERS\nHyperparameters Model 1 Model 2\nNumber of hidden layers 4 4\nHidden size of transformer 512 1024\nNumber of attention heads 8 8\nSize of attention head 80 128\nIntermediate size(Size of the feed forward layer) 2048 4096\nWarmup 10000 40000\nBatch-size 64 224\nSegment Length 150 32\nMemory Length 150 32\nparameter choice, we wanted to experiment, whether providing\nmore Segment and Memory length is advantageous (longer\ncontext) than a larger model. These parameters where chosen\nafter some hyperparameter optimization. Same as for BERT\nwe use the Adam optimizer, but we also use a cosine annealing\nlearning rate scheduler to speed-up training [19]. The training\nperformance results are in (Table VI)\nAs opposed to BERT, the left+right-marked models have\na better training performance than their counterpart. Interest-\ningly the larger model trains much better when compared to\n4\nTABLE VI\nTR-XL TRAINING PERPLEXITY SCORES\nModel Mem-seg len\n150-150 32-32\nleft+right-marked (+m+) 45.22 33.86\nleft-marked (+m) 47.83 35.78\nproviding larger contexts. The same set of parameters for the\n32-32 model cannot be replicated for 150-150 model as the lat-\nter takes a lot of space on the GPU card. The test set is same as\nthat used with BERT and the results are in (Table VII). The test\nperformance is similar to that of the training performance with\nleft-right-marked large model(32-32) performing the best. We\ncan directly compare the perplexity scores with the previous\nbest [20] as both are unidirectional models, Transformer-XL\nmodel has outperformed the latter by 27%.\nTABLE VII\nTR-XL TEST PERPLEXITY SCORES , (-): T HE EXPERIMENT MODELS ARE\nNOT AVAILABLE\nModel Mem-seg len\n150-150 32-32 (prev best)\nleft+right-marked (+m+) 82.3 73.58 93.2\nleft-marked (+m) 84.79 74.39 -\nC. Result comparisons for Transformer architectures\nTransformer-XL and BERT both have low perplexity and\npseudo-perplexity scores, but both cannot be directly com-\npared as they are calculated quite differently (Eq.1, Eq.3).\nThe dramatically low scores of BERT indicate that per word\npredicted probability is higher than that of a uni-directional\nmodel. Thus the predicted word probability distribution is\nmuch sharper when compared to the XL model probability\ndistribution. At this point, we cannot say which model ar-\nchitecture has performed better- BERT or Transformer-XL,\ndespite both of them achieving good low perplexity scores.\nWe would need to experiment with a downstreaming task in-\norder to fairly compare model performances.\nV. C ONCLUSION\nRecent migration to transformer based architectures in\nlanguage modeling from LSTM models is justiﬁed as\nTransformer-XL obtains strong perplexity results. BERT\nmodel also obtains very low pseudo-perplexity scores but\nit is inequitable to the unidirectional models. Our major\ncontributions in this project, is the use of Transformer-XL\narchitectures for the Finnish language in a sub-word setting,\nand the formulation of pseudo perplexity for the BERT model.\nFurther comparisons between the transformer architectures can\nbe made by downstreaming it to an ASR task, which will be\nexplored in the future.\nREFERENCES\n[1] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural computation, vol. 9, no. 8, pp. 1735–\n1780, 1997.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin,\n“Attention is all you need,” in NIPS, pp. 5998–6008,\nCurran Associates, Inc., 2017.\n[3] A. Radford, “Improving language understanding by gen-\nerative pre-training,” 2018.\n[4] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT:\npre-training of deep bidirectional transformers for lan-\nguage understanding,” CoRR, vol. abs/1810.04805, 2018.\n[5] Z. Dai, Z. Yang, Y . Yang, J. G. Carbonell, Q. V . Le, and\nR. Salakhutdinov, “Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context,” in ACL, 2019.\n[6] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhut-\ndinov, and Q. V . Le, “Xlnet: Generalized autoregres-\nsive pretraining for language understanding,” 2019.\narxiv:1906.08237.\n[7] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner,\nC. Clark, K. Lee, and L. Zettlemoyer, “Deep contextual-\nized word representations,” CoRR, vol. abs/1802.05365,\n2018.\n[8] J. Howard and S. Ruder, “Fine-tuned language models for\ntext classiﬁcation,” CoRR, vol. abs/1801.06146, 2018.\n[9] P. Smit, “Modern subword-based models for automatic\nspeech recognition,” pp. 62 + app. 136, 2019.\n[10] X. Chen, A. Ragni, X. Liu, and M. Gales, “Investigating\nbidirectional recurrent neural network language models\nfor speech recognition,” pp. 269–273, 08 2017.\n[11] P. Gage, “A new algorithm for data compression,” C\nUsers J., vol. 12, p. 2338, Feb. 1994.\n[12] R. Sennrich, B. Haddow, and A. Birch, “Neural machine\ntranslation of rare words with subword units,” CoRR,\nvol. abs/1508.07909, 2015.\n[13] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov,\n“Roberta: A robustly optimized BERT pretraining ap-\nproach,” CoRR, vol. abs/1907.11692, 2019.\n[14] CSC - IT Center for Science, “The helsinki\nkorp version of the Finnish text collection, url:\nhttp://urn.ﬁ/urn:nbn:ﬁ:lb-2016050207,” 1998.\n[15] P. Smit, S. Virpioja, S.-A. Gr ¨onroos, and M. Kurimo,\n“Morfessor 2.0: Toolkit for statistical morphological seg-\nmentation,” in Proceedings of the Demonstrations at\nthe 14th Conference of the European Chapter of the\nAssociation for Computational Linguistics, pp. 21–24,\n2014.\n[16] M. Creutz and K. Lagus, “Unsupervised models for mor-\npheme segmentation and morphology learning,” ACM\nTrans. Speech Lang. Process., vol. 4, Feb. 2007.\n[17] D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” CoRR, vol. abs/1412.6980, 2014.\n[18] Y . Zhu, R. Kiros, R. S. Zemel, R. Salakhutdinov,\nR. Urtasun, A. Torralba, and S. Fidler, “Aligning\nbooks and movies: Towards story-like visual explana-\ntions by watching movies and reading books,” CoRR,\nvol. abs/1506.06724, 2015.\n[19] I. Loshchilov and F. Hutter, “SGDR: stochastic gradient\ndescent with restarts,” CoRR, vol. abs/1608.03983, 2016.\n[20] M. K. Peter Smit, Sami Virpioja, “Advances in subword-\n5\nbased hmm-dnn speech recognition across languages.,”\nSubmitted to Language Resources and Evaluation, 29\nNovember 2018."
}