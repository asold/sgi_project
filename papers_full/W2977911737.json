{
  "title": "Dialogue Transformers",
  "url": "https://openalex.org/W2977911737",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288921671",
      "name": "Vlasov, Vladimir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288921670",
      "name": "Mosig, Johannes E. M.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288921672",
      "name": "Nichol, Alan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2438667436",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2167702024",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2947469743",
    "https://openalex.org/W2127838323",
    "https://openalex.org/W1993378086",
    "https://openalex.org/W2985067290",
    "https://openalex.org/W2954492830"
  ],
  "abstract": "We introduce a dialogue policy based on a transformer architecture, where the self-attention mechanism operates over the sequence of dialogue turns. Recent work has used hierarchical recurrent neural networks to encode multiple utterances in a dialogue context, but we argue that a pure self-attention mechanism is more suitable. By default, an RNN assumes that every item in a sequence is relevant for producing an encoding of the full sequence, but a single conversation can consist of multiple overlapping discourse segments as speakers interleave multiple topics. A transformer picks which turns to include in its encoding of the current dialogue state, and is naturally suited to selectively ignoring or attending to dialogue history. We compare the performance of the Transformer Embedding Dialogue (TED) policy to an LSTM and to the REDP, which was specifically designed to overcome this limitation of RNNs.",
  "full_text": "Dialogue Transformers\nVladimir Vlasov, ∗ Johannes E. M. Mosig, † and Alan Nichol ‡\nRasa\nWe introduce a dialogue policy based on a transformer architecture [1], where the self-attention\nmechanism operates over the sequence of dialogue turns. Recent work has used hierarchical recurrent\nneural networks to encode multiple utterances in a dialogue context, but we argue that a pure self-\nattention mechanism is more suitable. By default, an RNN assumes that every item in a sequence\nis relevant for producing an encoding of the full sequence, but a single conversation can consist\nof multiple overlapping discourse segments as speakers interleave multiple topics. A transformer\npicks which turns to include in its encoding of the current dialogue state, and is naturally suited to\nselectively ignoring or attending to dialogue history. We compare the performance of the Transformer\nEmbedding Dialogue (TED) policy to an LSTM and to the REDP [2], which was speciﬁcally designed\nto overcome this limitation of RNNs.\nI. INTRODUCTION\nConversational AI assistants promise to help users\nachieve a task through natural language. Interpreting\nsimple instructions like please turn on the lightsis rela-\ntively straightforward, but to handle more complex tasks,\nthese systems must be able to engage in multi-turn con-\nversations.\nThe goal of this paper is to show that the transformer\narchitecture [1] is more suitable for modeling multi-turn\nconversations than the commonly used recurrent models.\nTo compare the basic mechanisms that are at the heart\nof the sequence encoding we intentionally choose simple\narchitectures. The proposed TED architecture should be\nthought of as a candidate building block for use in de-\nveloping state-of-the-art architectures in various dialogue\ntasks.\nNot every utterance in a conversation has to be a re-\nsponse to the most recent utterance by the other party.\nGroz and Sidner [3] consider conversations as an inter-\nleaved set of discourse segments, where a discourse seg-\nment (or topic) is a set of utterances that directly re-\nspond to each other. These sequences of turns may not\ndirectly follow one another in the conversation. An in-\ntuitive example of this is the need for sub-dialogues in\ntask-oriented dialogue systems. Consider this conversa-\ntion:\nBOT: Your total is $15.50 - shall I\ncharge the card you used last time?\nUSER: Do I still have credit on my\naccount from that refund I got?\nBOT: Yes, your account is $10 in credit.\nUSER: Ok, great.\nBOT: Shall I place the order?\nUSER: Yes.\nBOT: Done. You should have your items tomorrow.\n∗vladimir@rasa.com\n†j.mosig@rasa.com\n‡alan@rasa.com\na. Dialogue Stacks The assistant’s question Shall I\nplace the order?prompts the return to the task at hand:\ncompleting a purchase. One model is to think of these\nsub-dialogues as existing on a stack, where new topics\nare pushed on to the stack when they are introduced and\npopped oﬀ the stack once concluded.\nIn the 1980s, Groz and Sidner [3] argued for represent-\ning dialogue history as a stack of topics, and later the\nRavenClaw [4] dialogue system implemented a dialogue\nstack for the speciﬁc purpose of handling sub-dialogues.\nWhile a stack naturally allows for sub-dialogues to be\nhandled and concluded, the strict structure of a stack\nis also limiting. The authors of RavenClaw argue for\nexplicitly tracking topics to enable the contextual inter-\npretation of the user intents. However, once a topic has\nbeen popped from the dialogue stack, it is no longer avail-\nable to provide this context. In the example above, the\nuser might follow up with a further question like so that\nused up my credit, right?. If the topic of refund credits\nhas been popped from the stack, this can no longer help\nclarify what the user wants to know. Since there is in\nprinciple no restriction to how humans revisit and inter-\nleave topics in a conversation, we are interested in a more\nﬂexible structure than a stack.\nb. Recurrent Neural Networks A common choice in\nrecent years has been to use a recurrent neural net-\nwork (RNN) to process the sequence of previous dialogue\nturns, both for open domain [5, 6] and task-oriented sys-\ntems [7]. Given enough training data, an RNN should be\nable to learn any desired behaviour. However, in a typi-\ncal low-resource setting where no large corpus for train-\ning a particular task is available, an RNN is not guar-\nanteed to learn to generalize these behaviours. Previous\nwork on modifying the basic RNN structure to include\ninductive biases for this behaviour into a dialogue policy\nwas conducted by Vlasov et al. [2] and Sahay et al. [8].\nThese works aim to overcome a feature of RNNs that\nis undesirable for dialogue modeling. RNNs by default\nconsume the entire sequence of input elements to pro-\nduce an encoding, unless a more complex structure like\na Long Short-Term Memory (LSTM) cell is trained on\nsuﬃcient data to explicitly learn that it should ‘forget’\nparts of a sequence.\narXiv:1910.00486v3  [cs.CL]  1 May 2020\n2\nc. Transformers The transformer architecture has\nin recent years replaced recurrent neural networks as the\nstandard for training language models, with models such\nas Transformer-XL [9] and GPT-2 [10] achieving much\nlower perplexities across a variety of corpora and produc-\ning representations that are useful for a variety of down-\nstream tasks [11, 12]. In addition, transformers have re-\ncently shown to be more robust to unexpected inputs\n(such as adversarial examples) [13]. Intuitively, because\nthe self-attention mechanism preselects which tokens will\ncontribute to the current state of the encoder, a trans-\nformer can ignore uninformative (or adversarial) tokens\nin a sequence. To make a prediction at each time step,\nan LSTM needs to update its internal memory cell, prop-\nagating this update to further time steps. If an input at\nthe current time step is unexpected, the internal state\ngets perturbed and at the next time step the neural net-\nwork encounters a memory state unlike anything encoun-\ntered during training. A transformer accounts for time\nhistory via a self-attention mechanism, making the pre-\ndictions at each time step independent of each other. If\na transformer receives an irrelevant input, it can ignore\nit and use only the relevant previous inputs to make a\nprediction.\nSince a transformer chooses which elements in a se-\nquence to use to produce an encoder state at every step,\nwe hypothesise that it could be a useful architecture for\nprocessing dialogue histories. The sequence of utterances\nin a conversation may represent multiple interleaved top-\nics, and the transformer’s self-attention mechanism can\nsimultaneously learn to disentangle these discourse seg-\nments and also to respond appropriately.\nII. RELATED WORK\na. Transformers for open-domain dialogue Multiple\nauthors have recently used transformer architectures in\ndialogue modeling. Henderson et al. [14] train response\nselection models on a large dataset from Reddit where\nboth the dialogue context and responses are encoded with\na transformer. They show that these architectures can\nbe pre-trained on a large, diverse dataset and later ﬁne-\ntuned for task-oriented dialogue in speciﬁc domains. Di-\nnan et al. [15] used a similar approach, using tranform-\ners to encode the dialogue context as well as background\nknowledge for studying grounded open-domain conversa-\ntions. Their proposed architecture comes in two forms:\na retrieval model where another transformer is used to\nencode candidate responses which are selected by rank-\ning, and a generative model where a transformer is used\nas a decoder to produce responses token-by-token. The\nkey diﬀerence with these approaches is that we apply\nself-attention at the discourse level, attending over the\nsequence of dialogue turns rather than the sequence of\ntokens in a single turn.\nb. Topic disentanglement in task-oriented dialogue\nRecent work has attempted to produce neural architec-\ntures for dialogue policies which can handle interleaved\ndiscourse segments in a single conversation. Vlasov et\nal. [2] introduced the Recurrent Embedding Dialogue Pol-\nicy (REDP) architecture. The ablation study in this work\nhighlighted that the improved performance of REDP is\ndue to an attention mechanism over the dialogue history\nand a copy mechanism to recover from unexpected user\ninput. This modiﬁcation to the standard RNN structure\nenables the dialogue policy to ‘skip’ speciﬁc turns in the\ndialogue history and produce an encoder state which is\nidentical before and after the unexpected input. Sahay et\nal. [8] develop this line of investigation further by study-\ning the eﬀectiveness of diﬀerent attention mechanisms for\nlearning this masking behaviour.\nIn this work we do not augment the basic RNN ar-\nchitecture but rather replace it with a transformer. By\ndefault, an RNN processes every item in a sequence to\ncalculate an encoding. REDP’s modiﬁcations were mo-\ntivated by the fact that not all dialogue history is rele-\nvant. Taking this line of reasoning further, we can use\nself-attention in place of an RNN, so there is no a pri-\nori assumption that the whole sequence is relevant, but\nrather that the dialogue policy should select which his-\ntorical turns are relevant for choosing a response.\nIII. TRANSFORMER AS A DIALOGUE POLICY\nWe propose the Transformer Embedding Dialogue\n(TED) policy, which greatly simpliﬁes the architecture\nof the REDP. Similar to the REDP, we do not use a\nclassiﬁer to select a system action. Instead, we jointly\ntrain embeddings for the dialogue state and each of the\nsystem actions by maximizing a similarity function be-\ntween them. At inference time, the current state of the\ndialogue is compared to all possible system actions, and\nthe one with the highest similarity is selected. A simi-\nlar approach is taken by [14, 16, 17] in training retrieval\nmodels for task-oriented dialogue.\nTwo time steps (i.e. dialogue turns) of the TED policy\nare illustrated in Figure 1. A step consists of several key\nparts.\na. Featurization Firstly, the policy featurizes the\nuser input, system actions and slots.\nThe TED policy can be used in an end-to-end or in\na modular fashion. The modular approach is similar to\nthat taken in POMDP-based dialogue policies [18] or Hy-\nbrid Code Networks [7, 19]. An external natural language\nunderstanding system is used and the user input is fea-\nturized as a binary vector indicating the recognized intent\nand the detected entities. The dialogue policy predicts\nan action from a ﬁxed list of system actions. System\nactions are featurized as binary vectors representing the\naction name, following the REDP approach explained in\ndetail in [2].\nBy end-to-end we mean that there is no supervision\nbeyond the sequence of utterances. That is, there are\nno gold labels for the NLU output or the system action\n3\nUser Intent\nEntities\nSlots\nSystem Action\nEmbedding\nLayer\nEmbedding\nLayer\nSimilarity\nPrevious\nSystem Action\nOne dialogue turn\nOne dialogue turn\nUnidirectional\nTransformer\nUser Intent\nEntities\nSlots\nSystem Action\nLoss\nEmbedding\nLayer\nEmbedding\nLayer\nSimilarity\nPrevious\nSystem Action\nUnidirectional\nTransformer\nFIG. 1. A schematic representation of two time steps of the\ntransformer embedding dialogue policy.\nnames. The end-to-end TED policy is still a retrieval\nmodel and does not generate new responses. In the end-\nto-end setup, user and system utterances are encoded as\nbag-of-words vectors.\nSlots are always featurized as binary vectors, indicating\ntheir presence, absence, or that the value is not important\nto the user, at each step of the dialogue. We use a simple\nslot tracking method, overwriting each slot with the most\nrecently speciﬁed value.\nb. Transformer The input to the transformer is the\nsequence of user inputs and system actions. Therefore,\nwe leverage the self-attention mechanism present in the\ntransformer to access diﬀerent parts of dialogue history\ndynamically at each dialogue turn. The relevance of pre-\nvious dialogue turns is learned from data and calculated\nanew at each turn in the dialogue. Crucially, this allows\nthe dialogue policy to take a user utterance into account\nat one turn but ignore it completely at another.\nc. Similarity The transformer output adialogue and\nsystem actions yaction are embedded into a single se-\nmantic vector space hdialogue = E(adialogue), haction =\nE(yaction), where h ∈ I R20. We use the dot-\nproduct loss [14, 20] to maximize the similarity S+ =\nhT\ndialogueh+\naction with the target label y+\naction and minimize\nsimilarities S− = hT\ndialogueh−\naction with negative samples\ny−\naction. Thus, the loss function for one dialogue reads\nLdialogue = −\n⟨\nS+ −log\n(\neS+\n+\n∑\nΩ−\neS−\n)⟩\n, (1)\nwhere the sum is taken over the set of negative samples\nΩ−and the average ⟨.⟩is taken over time steps inside one\ndialogue.\nThe global loss is an average of all loss functions from\nall dialogues.\nAt inference time, the dot-product similarity serves as\na ranker for the next utterance retrieval problem.\nDuring modular training, we use a balanced batching\nstrategy to mitigate class imbalance, as some system ac-\ntions are far more frequent than others.\nIV. EXPERIMENTS\nThe aim of our experiments is to compare the perfor-\nmance of the transformer against that of an LSTM on\nmulti-turn conversations. Speciﬁcally, we want to test\nthe TED policy on the task of picking out relevant turns\nin the dialogue history for next action prediction. There-\nfore, we need a conversational dataset for which system\nactions depend on the dialogue history across several\nturns. This requirement precludes question-answering\ndatasets such as WikiQA [21] as candidates for evalu-\nation.\nIn addition, system actions need to be labeled to eval-\nuate next action retrieval accuracy. Note, that met-\nrics such as Recall@k [22] could be used on unlabeled\ndata, but since typical dialogues contain many generic\nresponses, such as “yes”, that are correct in a large num-\nber of situations, the meaningfulness of Recall@k is ques-\ntionable. We therefore exclude unlabeled dialogue cor-\npora such as the Ubuntu Dialogue Corpus [22] or Metal-\nWOZ [23] from our experiments.\nTo our knowledge, the only publicly available dia-\nlogue datasets that might satisfy both our criteria are\nthe REDP dataset [2], MultiWOZ [24, 25] and Google\nTaskmaster-1 [26]. For the latter, we would have to ex-\ntract action labels from the entity annotations, which is\nnot always possible.\nTwo diﬀerent models serve as baseline in our exper-\niments. First, the REDP model by Vlasov et al. [2],\nwhich was speciﬁcally designed to handle long-range his-\ntory dependencies, but is LSTM-based. Second, another\nLSTM-based policy that is identical to TED, except that\nthe transformer was replaced by an LSTM.\nWe use the ﬁrst (REDP) baseline for the experiments\non the [2] dataset, as this baseline is stronger when long-\nrange dependencies are in play. For the MultiWOZ ex-\nperiments, we only compare to the simple LSTM policy,\nsince the MultiWOZ dataset is nearly history indepen-\ndent as we demonstrate here.\nAll experiments are available online at https://\ngithub.com/RasaHQ/TED-paper.\n4\n40 60 80 100 120 140\nNumber of dialogues present during training\n5\n10\n15\n20\n25\n30Number of correct test dialogues\nLSTM\nTEDP\nREDP\nFIG. 2. Performance of REDP, TED policy, and an LSTM\nbaseline policy on the dataset from [2]. Lines indicate mean\nand shaded area the standard deviation of 3 runs. The hor-\nizontal axis indicates the amount of training conversations\nused to train the model and the vertical axis is the number of\nconversations in the test set in which every action is predicted\ncorrectly.\nA. Conversations containing sub-dialogues\nWe ﬁrst evaluate experiments on the dataset of Vlasov\net al. [2]. This dataset was speciﬁcally designed to test\nthe ability of a dialogue policy to handle non-cooperative\nor unexpected user input. It consists of task-oriented di-\nalogues in hotel and restaurant reservation domains con-\ntaining cooperative (user provides necessary information\nrelated to the task) and non-cooperative (user asks a\nquestion unrelated to the task or makes chit-chat) di-\nalogue turns. One of the properties of this dataset is\nthat the system repeats the previously asked question af-\nter any non-cooperative user behavior. This dataset is\nalso used in [8] to compare the performance of diﬀerent\nattention mechanisms.\nFigure 2 shows the performance of diﬀerent dialogue\npolicies on the held-out test dialogues as a function of the\namount of conversations used to train the model. The\nTED policy performs on par with REDP without any\nspeciﬁcally designed architecture to solve the task and\nsigniﬁcantly outperforms a simple LSTM-based policy.\nIn the extreme low-data regime, the TED policy is out-\nperformed by REDP. It should be noted that REDP relies\nheavily on its copy mechanism to predict the previously\nasked question after a non-cooperative digression. How-\never, the TED policy, being both simpler and more gen-\neral, achieves similar performance without relying on dia-\nlogue properties like repeating a question. Moreover, due\nto the transformer architecture, the TED policy trains\nfaster than REDP and requires fewer training epochs to\nachieve the same accuracy.\nFigure 3 visualizes the attention weights of the TED\n* request_hotel\n    - utter_ask_details\n* inform{\"enddate\": \"May 26th\"}\n    - utter_ask_startdate\n* inform{\"startdate\": \"next week\"}\n    - utter_ask_location\n* inform{\"location\": \"rome\"}\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* inform{\"price\": \"expensive\"}\n    - utter_ask_people\n* inform{\"people\": \"4\"}\n    - utter_filled_slots\n    - action_search_hotel\n    - utter_suggest_hotel\n* affirm\n    - utter_happy\n* request_hotel\n    - utter_ask_details\n* inform{\"enddate\": \"May 26th\"}\n    - utter_ask_startdate\n* inform{\"startdate\": \"next week\"}\n    - utter_ask_location\n* inform{\"location\": \"rome\"}\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* chitchat\n    - utter_chitchat\n    - utter_ask_price\n* inform{\"price\": \"expensive\"}\n    - utter_ask_people\n* inform{\"people\": \"4\"}\n    - utter_filled_slots\n    - action_search_hotel\n    - utter_suggest_hotel\n* affirm\n    - utter_happy\nFIG. 3. Attention weights of the TED policy on an example\ndialogue. On the vertical axis are predicted dialogue turns,\nand on the horizontal axis is the dialogue history over which\nthe TED policy attends. We use a unidirectional transformer,\nso the upper triangle is masked with zeros to avoid attending\nto future dialogue turns.\npolicy on an example dialogue. This example dialogue\ncontains several chit-chat utterances in a row in the mid-\ndle of the conversation. The Figure shows that the series\nof chit-chat interactions is completely ignored by the self-\nattention mechanism when task completion is attempted\n(i.e. further required questions are asked). Note, that the\nlearned weights are sparse, even though the TED pol-\nicy does not use a sparse attention architecture. Impor-\ntantly, the TED policy chooses key dialogue steps from\nthe history that are relevant for the current prediction\nand ignores uninformative history. Here, we visualize\nonly one conversation, but the result is the same for an\narbitrary number of chit-chat dialogue turns.\nB. Comparing the end-to-end and modular\napproaches on MultiWOZ\nHaving demonstrated that the light-weight TED pol-\nicy performs at least on par with the specialized REDP\nand signiﬁcantly outperforms a basic LSTM policy when\nevaluated on conversations that contain long-range his-\ntory dependencies, we now compare TED to an LSTM\npolicy on the MultiWOZ 2.1 dataset. In contrast to the\nprevious Section, the LSTM policy of the present Sec-\ntion is an architecture identical to TED, but with the\ntransformer replaced by an LSTM cell.\nWe chose MultiWOZ for this experiment because it\n5\nconcerns multi-turn conversations and provides system\naction labels. Unfortunately, we discovered that it does\nnot contain many long-range dependencies, as we shall\ndemonstrate later in this Section. Therefore, neither\nTED nor the REDP have any conceptual advantages over\nan LSTM. Subsequently we show that the TED policy\nperforms on par with an LSTM on this commonly used\nbenchmark dataset.\nMultiWOZ 2.1 is a dataset of 10438 human-human dia-\nlogues for a Wizard-of-Oz task in seven diﬀerent domains:\nhotel, restaurant, train, taxi, attraction, hospital, and\npolice. In particular, the dialogues are between a user\nand a clerk (wizard). The user asks for information and\nthe wizard, who has access to a knowledge base about all\nthe possible things that the user may ask for, provides\nthat information or executes a booking. The dialogues\nare annotated with labels for the wizard’s actions, as well\nas the wizard’s knowledge about the user’s goal after each\nuser turn.\nFor our experiments, we split the MultiWOZ 2.1\ndataset into a training and a test set of 7249 and 1812\ndialogues, respectively. Unfortunately, we had to neglect\n1377 dialogues altogether, since their annotations are in-\ncomplete.\na. End-to-end training. As a ﬁrst experiment on\nMultiWOZ 2.1 we study an end-to-end retrieval setup,\nwhere the user utterance is used directly as input to\nthe TED policy, which then has to retrieve the correct\nresponse from a predeﬁned list (extracted from Multi-\nWOZ).\nThe wizard’s behaviour depends on the result of\nqueries to the knowledge base. For example, if only a\nsingle venue is returned, the wizard will probably refer\nto it. We marginalize this knowledge base dependence by\n(i) delexicalizing all user and wizard utterances [27], and\n(ii) introducing status slots that indicate whether a venue\nis available, not available, already booked, or unique (i.e.\nthe wizard is going to recommend or book a particular\nvenue in the next turn). These slots are featurized as a\n1-of-K binary vector.\nTo compute the accuracy and F1 scores of the TED\npolicy’s predictions, we assign the action labels (e.g.\nrequest_restaurant) that are provided by the Multi-\nWOZ dataset to the output utterances, and compare\nthem to the correct labels. If multiple labels are present,\nwe concatenate them in alphabetic order to a single label.\nTable I shows the resulting F1 scores and accura-\ncies on the hold-out test set. The discrepancy be-\ntween the F1 score and the accuracy stems from the\nfact that some labels, s.a. bye_general, occur very\nfrequently (4759 times) compared to most other la-\nbels, s.a. recommend_restaurant_select_restau rant,\nwhich only occurs 11 times.\nThe fact that accuracy and F1 scores are generally low\ncompared to 1.0 stems from a deeper issue with the Mul-\ntiWOZ dialog dataset. Speciﬁcally, because more than\none particular behaviour of the wizard would be consid-\nered ’correct’ in most situations, the MultiWOZ dataset\nmodel N accuracy F1 score\nTED end-to-end 10 0.64 0.28\n2 0.62 0.24\nTED modular 10 0.73 0.63\n2 0.69 0.55\nLSTM end-to-end 10 0.51 0.23\n2 0.57 0.24\nLSTM modular 10 0.68 0.60\n2 0.61 0.54\nTABLE I. Accuracy and F1 scores of the TED policy in end-\nto-end and modular mode, as well as the TED policy with the\ntransformer replaced by an LSTM. Models are evaluated on\nthe MultiWOZ 2.1 dataset using max history N. All scores\nconcern prediction at the action level on the test set.\nis unsuitable for supervised learning of dialogue policies.\nPut diﬀerently, some of the wizards’ actions in Multi-\nWOZ are not deterministic, but probabilistic. For exam-\nple, it cannot be learned when the wizard should ask if\nthe user needs anything else, since this is the personal\npreference of the people who take the wizard’s role. We\nelaborate on this and several other issues of the Multi-\nWOZ dataset in [28].\nb. Modular training. We now repeat the above ex-\nperiment, using the same subset of MultiWOZ dia-\nlogues, but now adopting the modular approach. We\nsimulate an external natural language understanding\npipeline and provide gold user intents and entities to\nthe TED policy instead of the original user utterances.\nWe extract the intents from the changes in the Wiz-\nard’s belief state. This belief state is provided by the\nMultiWOZ dataset in the form of a set of slots (e.g.\nrestaurant_area, hotel_name, etc.) that get updated\nafter each user turn. A typical user intent is thus\ninform{\"restaurant_area\": \"south\"}. The user does\nnot always provide new information, however, so the in-\ntent might be simply inform (without any entities). If\nthe last user intent of the dialogue was uninformative in\nthis way, we assume it is a farewell and thus annotate it\nas bye.\nUsing the modular approach instead of end-to-end\nlearning roughly doubles the F1 score and also increases\naccuracy slightly, as can be seen in Table I. This is not\nsurprising since the modular approach receives additional\nsupervision.\nAlthough the scores suggest that the modular TED\npolicy performs better than the end-to-end TED policy,\nthe kinds of mistakes made are similar. We demonstrate\nthis with one example dialog from our test set, named\nSNG0253, that is displayed in Figure 4.\nThe second column of Figure 4 shows the end-to-\nend predictions. The two predicted responses are\nboth sensible, i.e. the replies could have come from\na human. Nevertheless, both results are marked\nas wrong, because according to the gold dialogue\n(ﬁrst column), the ﬁrst response should only have in-\n6\nFIG. 4. MultiWOZ 2.1 dialogue SNG0253, as-is (ﬁrst column), as predicted by the end-to-end TED policy (second column),\nand as sequence of user intents and system actions predicted by modular TED policy (third column). The two predictions are\nsensible, yet incorrect according to the labels. Furthermore, the end-to-end and modular TED policies make similar kinds of\nmistakes.\ncluded its second sentence ( request_train, but not\ninform_train). For the fourth turn, however, it is the\nother way around: According to the target dialogue\nthe response should have included additional informa-\ntion about the train ( inform_train_request_train),\nwhereas the predicted dialogue only asked for more in-\nformation (request_train).\nThe third column shows that the modular TED policy\nmakes the same kinds of mistakes: instead of predicting\nonly request_train, it predicts to take both actions,\ninform_train and request_train in the second turn.\nIn the ﬁnal turn, instead of request_train, the modu-\nlar TED policy predicts reqmore_general, which means\nthat the wizard asks if the user requires anything else.\nThis reply is perfectly sensible and does in fact occur in\nsimilar dialogues of the training set (see, e.g., Dialogue\nPMUL1883). Thus, the correct behaviour doesn’t exist\nand it is impossible to achieve high scores, as reﬂected\nby the test scores of Table I.\nTo the best of our knowledge, the state of the art F1\nscores on next action retrieval with MultiWOZ are given\nby [17] and [29] with 0 .64 and 0 .72, respectively. How-\never, these numbers are not directly comparable to ours:\nwe retrieve actions out of all 56128 possible responses\nand compare the label of the retrieved response with the\nlabel of correct response, while they retrieve out of 20\nnegative samples and compare text responses directly.\nc. History independence. As Table I shows, taking\ninto account only the last two turns (i.e. the current user\nutterance or intent, and one system action before that),\ninstead of the last 10 turns, the accuracy and F1 scores\ndecrease by no more than 0 .04 for end-to-end and no\nmore than 0 .08 for the modular architecture. For the\nend-to-end LSTM architecture that we discuss in the next\nparagraph, the performance even improves when less his-\ntory is taken into account. Thus, MultiWOZ appears to\ndepend only weakly on the dialogue history, and there-\nfore we cannot evaluate how well the TED policy handles\ndialogue complexity.\nd. Transformer vs LSTM. As a ﬁnal experiment, we\nreplace the transformer in the TED architecture by an\nLSTM and run the same experiments as before. The\nresults are shown in Table I.\nThe F1 scores of the LSTM and transformer versions\ndiﬀer by no more than 0.05, which is to be expected, since\nin MultiWOZ the vast majority of information is carried\nby the most recent turn.\nThe LSTM version lacks the accuracy of the trans-\nformer version, however. Speciﬁcally, the accuracy scores\nfor end-to-end training are up to 0 .13 points lower for\nthe LSTM. It is diﬃcult to assert the reason for this dis-\ncrepancy due to the problems of ambiguity that we have\nidentiﬁed earlier in this Section.\nV. CONCLUSIONS\nWe introduce the transformer embedding dialogue\n(TED) policy in which a transformer’s self-attention\nmechanism operates over the sequence of dialogue turns.\nWe argue that this is a more appropriate architecture\nthan an RNN due to the presence of interleaved topics\nin real-life conversations. We show that the TED pol-\nicy can be applied to the MultiWOZ dataset in both a\nmodular and end-to-end fashion, although we also ﬁnd\nthat this dataset is not ideal for supervised learning of\ndialogue policies, due to a lack of history dependence\nand a dependence on individual crowd-worker prefer-\n7\nences. We also perform experiments on a task-oriented\ndataset speciﬁcally created to test the ability to recover\nfrom non-cooperative user behaviour. The TED policy\noutperforms the baseline LSTM approach and performs\non par with REDP, despite TED being faster, simpler,\nand more general. We demonstrate that learned atten-\ntion weights are easily interpretable and reﬂect dialogue\nlogic. At every dialogue turn, a transformer picks which\nprevious turns to take into account for current predic-\ntion, selectively ignoring or attending to diﬀerent turns\nof the dialogue history.\nAcknowledgments\nWe would like to thank the Rasa team and Rasa com-\nmunity for feedback and support. Special thanks to Elise\nBoyd for supporting us with the illustrations.\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez,  Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Ad-\nvances in neural information processing systems , pages\n5998–6008, 2017.\n[2] Vladimir Vlasov, Akela Drissner-Schmid, and Alan\nNichol. Few-shot generalization across dialogue tasks.\narXiv preprint arXiv:1811.11707 , 2018.\n[3] Barbara J Grosz and Candace L Sidner. Attention, in-\ntentions, and the structure of discourse. Computational\nlinguistics, 12(3):175–204, 1986.\n[4] Dan Bohus and Alexander I Rudnicky. The ravenclaw di-\nalog management framework: Architecture and systems.\nComputer Speech & Language, 23(3):332–361, 2009.\n[5] Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi,\nChristina Lioma, Jakob Grue Simonsen, and Jian-Yun\nNie. A hierarchical recurrent encoder-decoder for gener-\native context-aware query suggestion. In Proceedings of\nthe 24th ACM International on Conference on Informa-\ntion and Knowledge Management , pages 553–562. ACM,\n2015.\n[6] Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. Building end-to-end\ndialogue systems using generative hierarchical neural net-\nwork models. In Thirtieth AAAI Conference on Artiﬁcial\nIntelligence, 2016.\n[7] Jason D Williams, Kavosh Asadi, and Geoﬀrey Zweig.\nHybrid code networks: practical and eﬃcient end-to-end\ndialog control with supervised and reinforcement learn-\ning. arXiv preprint arXiv:1702.03274 , 2017.\n[8] Saurav Sahay, Shachi H. Kumar, Eda Okur, Haroon\nSyed, and Lama Nachman. Modeling intent, dia-\nlog policies and response adaptation for goal-oriented\ninteractions. In Proceedings of the 23rd Workshop\non the Semantics and Pragmatics of Dialogue - Full\nPapers, London, United Kingdom, September 2019.\nSEMDIAL. URL http://semdial.org/anthology/\nZ19-Sahay_semdial_0019.pdf.\n[9] Zihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[10] Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI Blog, 1(8),\n2019.\n[11] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language\nunderstanding. arXiv preprint arXiv:1804.07461 , 2018.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018.\n[13] Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei,\nWen-Lian Hsu, and Cho-Jui Hsieh. On the robustness of\nself-attentive models. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguis-\ntics, pages 1520–1529, 2019.\n[14] Matthew Henderson, Ivan Vuli´ c, Daniela Gerz, I˜ nigo\nCasanueva, Pawe l Budzianowski, Sam Coope, Geor-\ngios Spithourakis, Tsung-Hsien Wen, Nikola Mrkˇ si´ c,\nand Pei-Hao Su. Training neural response selection\nfor task-oriented dialogue systems. arXiv preprint\narXiv:1906.01543, 2019.\n[15] Emily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. Wizard of\nwikipedia: Knowledge-powered conversational agents.\narXiv preprint arXiv:1811.01241 , 2018.\n[16] Antoine Bordes, Y-Lan Boureau, and Jason Weston.\nLearning end-to-end goal-oriented dialog. arXiv preprint\narXiv:1605.07683, 2016.\n[17] Shikib Mehri, Evgeniia Razumovsakaia, Tiancheng Zhao,\nand Maxine Eskenazi. Pretraining methods for di-\nalog context representation learning. arXiv preprint\narXiv:1906.00414, 2019.\n[18] Jason D Williams and Steve Young. Partially observ-\nable markov decision processes for spoken dialog systems.\nComputer Speech & Language, 21(2):393–422, 2007.\n[19] Tom Bocklisch, Joey Faulkner, Nick Pawlowski, and Alan\nNichol. Rasa: Open source language understanding and\ndialogue management. arXiv preprint arXiv:1712.05181,\n2017.\n[20] Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams,\nAntoine Bordes, and Jason Weston. Starspace: Embed\nall the things! arXiv preprint arXiv:1709.03856 , 2017.\n[21] Yi Yang, Wen-tau Yih, and Christopher Meek. WikiQA:\nA Challenge Dataset for Open-Domain Question Answer-\ning. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , pages 2013–\n2018. Association for Computational Linguistics, 2015.\ndoi:10.18653/v1/D15-1237.\n[22] Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle\nPineau. The Ubuntu Dialogue Corpus: A Large Dataset\nfor Research in Unstructured Multi-Turn Dialogue Sys-\ntems. arXiv preprint arXiv:1506.08909 , 2016. URL\nhttp://arxiv.org/abs/1506.08909.\n8\n[23] Hannes Schulz, Adam Atkinson, Mahmoud Adada,\nKaheer Suleman, and Shikhar Sharma. MetaL-\nWOz, 2019. URL https://www.microsoft.com/en-us/\nresearch/project/metalwoz/.\n[24] Pawe l Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan,\nand Milica Gaˇ si´ c. Multiwoz-a large-scale multi-domain\nwizard-of-oz dataset for task-oriented dialogue modelling.\narXiv preprint arXiv:1810.00278 , 2018.\n[25] Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\nSanchit Agarwal, Shuyag Gao, and Dilek Hakkani-\nTur. Multiwoz 2.1: Multi-domain dialogue state cor-\nrections and state tracking baselines. arXiv preprint\narXiv:1907.01669, 2019.\n[26] Bill Byrne, Karthik Krishnamoorthi, Chinnadhu-\nrai Sankar, Arvind Neelakantan, Daniel Duckworth,\nSemih Yavuz, Ben Goodrich, Amit Dubey, Kyu-Young\nKim, and Andy Cedilnik. Taskmaster-1:Toward a\nrealistic and diverse dialog dataset. In 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and 9th International Joint Conference\non Natural Language Processing , 2019. URL https:\n//storage.googleapis.com/dialog-data-corpus/\nTASKMASTER-1-2019/landing_page.html.\n[27] Nikola Mrkˇ si´ c, Diarmuid O S´ eaghdha, Tsung-Hsien Wen,\nBlaise Thomson, and Steve Young. Neural belief tracker:\nData-driven dialogue state tracking. arXiv preprint\narXiv:1606.03777, 2016.\n[28] Johannes EM Mosig, Vladimir Vlasov, and Alan Nichol.\nWhere is the context?–a critique of recent dialogue\ndatasets. arXiv preprint arXiv:2004.10473 , 2020.\n[29] Shikib Mehri and Maxine Eskenazi. Multi-\ngranularity representations of dialog. arXiv preprint\narXiv:1908.09890, 2019.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8129903078079224
    },
    {
      "name": "Conversation",
      "score": 0.7289597392082214
    },
    {
      "name": "Computer science",
      "score": 0.725210964679718
    },
    {
      "name": "ENCODE",
      "score": 0.6672624349594116
    },
    {
      "name": "Embedding",
      "score": 0.6234155893325806
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5596127510070801
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4272093176841736
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4246123433113098
    },
    {
      "name": "Natural language processing",
      "score": 0.3818814158439636
    },
    {
      "name": "Speech recognition",
      "score": 0.37475430965423584
    },
    {
      "name": "Artificial neural network",
      "score": 0.24375179409980774
    },
    {
      "name": "Communication",
      "score": 0.19223693013191223
    },
    {
      "name": "Psychology",
      "score": 0.15180394053459167
    },
    {
      "name": "Engineering",
      "score": 0.09223753213882446
    },
    {
      "name": "Voltage",
      "score": 0.06770363450050354
    },
    {
      "name": "Electrical engineering",
      "score": 0.06031590700149536
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 3
}