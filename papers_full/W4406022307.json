{
  "title": "From Decision to Action in Surgical Autonomy: Multi-Modal Large Language Models for Robot-Assisted Blood Suction",
  "url": "https://openalex.org/W4406022307",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Zargarzadeh, Sadra",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2210031860",
      "name": "Mirzaei Maryam",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2730185089",
      "name": "Ou YaFei",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2254761278",
      "name": "Tavakoli Mahdi",
      "affiliations": [
        "University of Alberta"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4211225190",
    "https://openalex.org/W2061417916",
    "https://openalex.org/W4394708859",
    "https://openalex.org/W6849903772",
    "https://openalex.org/W3196757288",
    "https://openalex.org/W4312578227",
    "https://openalex.org/W2341108136",
    "https://openalex.org/W4387641544",
    "https://openalex.org/W2973815009",
    "https://openalex.org/W4400187978",
    "https://openalex.org/W2016545368",
    "https://openalex.org/W3133389372",
    "https://openalex.org/W3207462099",
    "https://openalex.org/W4323914534",
    "https://openalex.org/W3118688377",
    "https://openalex.org/W4389666586",
    "https://openalex.org/W3198013102",
    "https://openalex.org/W6639175102",
    "https://openalex.org/W4390730760",
    "https://openalex.org/W6839928859",
    "https://openalex.org/W6810640255",
    "https://openalex.org/W6850503672",
    "https://openalex.org/W6856045385",
    "https://openalex.org/W4405787555",
    "https://openalex.org/W4389667233",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W4394828156",
    "https://openalex.org/W4401415530",
    "https://openalex.org/W4401415119",
    "https://openalex.org/W4401414899",
    "https://openalex.org/W4366241052",
    "https://openalex.org/W4387307799",
    "https://openalex.org/W4389237785",
    "https://openalex.org/W4363679378",
    "https://openalex.org/W4405785727"
  ],
  "abstract": "The rise of Large Language Models (LLMs) has impacted research in robotics and automation. While progress has been made in integrating LLMs into general robotics tasks, a noticeable void persists in their adoption in more specific domains such as surgery, where critical factors such as reasoning, explainability, and safety are paramount. Achieving autonomy in robotic surgery, which entails the ability to reason and adapt to changes in the environment, remains a significant challenge. In this work, we propose a multi-modal LLM integration in robot-assisted surgery for autonomous blood suction. The reasoning and prioritization are delegated to the higher-level task-planning LLM, and the motion planning and execution are handled by the lower-level deep reinforcement learning model, creating a distributed agency between the two components. As surgical operations are highly dynamic and may encounter unforeseen circumstances, blood clots and active bleeding were introduced to influence decision-making. Results showed that using a multi-modal LLM as a higher-level reasoning unit can account for these surgical complexities to achieve a level of reasoning previously unattainable in robot-assisted surgeries. These findings demonstrate the potential of multi-modal LLMs to significantly enhance contextual understanding and decision-making in robotic-assisted surgeries, marking a step toward autonomous surgical systems.",
  "full_text": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2025 1\nFrom Decision to Action in Surgical Autonomy:\nMulti-Modal Large Language Models for\nRobot-Assisted Blood Suction\nSadra Zargarzadeh1, Maryam Mirzaei 1, Yafei Ou1, and Mahdi Tavakoli 1,2, Senior Member, IEEE\n©2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of\nany copyrighted component of this work in other works. DOI: 10.1109/LRA.2025.3535184\nAbstract—The rise of Large Language Models (LLMs) has\nimpacted research in robotics and automation. While progress\nhas been made in integrating LLMs into general robotics tasks,\na noticeable void persists in their adoption in more specific\ndomains such as surgery, where critical factors such as reasoning,\nexplainability, and safety are paramount. Achieving autonomy in\nrobotic surgery, which entails the ability to reason and adapt to\nchanges in the environment, remains a significant challenge. In\nthis work, we propose a multi-modal LLM integration in robot-\nassisted surgery for autonomous blood suction. The reasoning\nand prioritization are delegated to the higher-level task-planning\nLLM, and the motion planning and execution are handled by\nthe lower-level deep reinforcement learning model, creating a\ndistributed agency between the two components. As surgical\noperations are highly dynamic and may encounter unforeseen\ncircumstances, blood clots and active bleeding were introduced\nto influence decision-making. Results showed that using a multi-\nmodal LLM as a higher-level reasoning unit can account for these\nsurgical complexities to achieve a level of reasoning previously\nunattainable in robot-assisted surgeries. These findings demon-\nstrate the potential of multi-modal LLMs to significantly enhance\ncontextual understanding and decision-making in robotic-assisted\nsurgeries, marking a step toward autonomous surgical systems.\nIndex Terms—Medical robots and systems, multi-modal large\nlanguage models, surgical robot, planning, laparoscopy.\nI. I NTRODUCTION\nR\nOBOT-assisted surgery (RAS) has enormously changed\nthe way many surgeons operate. Surgical robots can\nenhance accuracy and dexterity, provide better anatomical\naccess, and minimize invasiveness, surgery time, and the need\nfor revision surgery [1]. With the development of surgical\nrobots and the da Vinci Research Kit (dVRK) [2], along\nwith realistic surgical simulation environments [3]–[5], the\nautomation of surgical sub-tasks such as tissue retraction [6],\nsuturing [7], endoscopic camera control [8], cutting [9], and\nManuscript received: August, 10, 2024; Revised November, 5, 2024;\nAccepted January, 19, 2025.\nThis paper was recommended for publication by Editor Pietro Valdastri\nupon evaluation of the Associate Editor and Reviewers’ comments. This\nresearch was supported by the Canada Foundation for Innovation (CFI), the\nNatural Sciences and Engineering Research Council (NSERC) of Canada,\nthe Canadian Institutes of Health Research (CIHR), and Alberta Innovates.\n(Corresponding author: Sadra Zargarzadeh)\n1Sadra Zargarzadeh, Maryam Mirzaei, Yafei Ou, and Mahdi Tavakoli\nare with the Department of Electrical and Computer Engineering, Uni-\nversity of Alberta, Edmonton, AB, Canada. {sadra.zar, mmirzaei,\nyafei.ou, mahdi.tavakoli}@ualberta.ca\n2Mahdi Tavakoli is with the Department of Biomedical Engineering,\nUniversity of Alberta, Edmonton, AB, Canada.\nDigital Object Identifier (DOI): see top of this page.\nAssistant:\nPool 1 has a blood clot and Pool 4 is actively bleeding.\nInput Prompt by User:\nLook at this image of multiple blood pools. Please \nprioritize and suction the blood effectively.\nHigh-Level Reasoning and Task Planning\nLow-Level Motion Planning\nResponse:\n1. Pool 4 \n2. Pool 1 \n3. Pool 3 \n4. Pool 2\nExplanation:\nPool 4 is actively flowing, so it should be cleaned first to prevent the liquid \nfrom spreading further.\nPool 1 contains a blood clot, which might complicate the suction process \nor require special handling, so it should be addressed next.\nPool 3 is the largest remaining pool, so it should be cleaned before Pool 2 \nto tackle the larger volume.\nPool 2 is the smallest and presumably the least complicated, so it should \nbe cleaned last.\n1\n2\n3\n4\nDeep Reinforcement Learning Agent\nT=0 T=2s T=4s T=6s\nT=8s T=10s T=12s T=14s\nFig. 1: The high-level task reasoning and planning for the blood\nsuction task is performed by the LLM, and the low-level motion\nplanning and execution is done by the DRL agent.\nbody fluid removal [10], has been an area of research in the\npast few years. These are the building blocks of surgeries\nthat form the foundation for enhancing bottom-up surgical\nautonomy [11], [12], and automating these commonly faced\nsub-tasks provides the basic robot skills necessary for reaching\na more advanced level of autonomy, including the ability to\nreason and plan tasks.\nAutonomous execution of surgical sub-tasks is typically\nlearned through model-based methods [13] or data-driven\narXiv:2408.07806v2  [cs.RO]  29 Jan 2025\n2 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2025\napproaches like deep reinforcement learning (DRL) [14] and\nimitation learning [15], [16]. However, these methods often\nlead to behavior that lacks human interpretability, explain-\nability, and adaptability in decision-making. They prioritize\nmaximizing cumulative rewards and efficient exploration of\nthe state space but do not implicitly account for the risks\nof the actions taken, thus failing to assure safety standards\n[17]–[19]. Moreover, DRL and imitation learning struggle\nwith dynamic adaptability, particularly in scenarios where\nsurgical conditions deviate from the norm due to unexpected\npatient anatomy or sudden complications such as bleeding.\nThis inability to reason and act in unforeseen circumstances\nunderscores the need for a reasoning framework that can\nadapt to challenges, ensuring efficacy and transparent decision-\nmaking in autonomous surgical tasks. As surgical sub-tasks\ninvolve physical interaction with patients, any automation and\ndecision-making by the robot must be clear and understandable\nby the operator, resembling human-like behavior to ensure\nsafety and reliability.\nAutonomous surgical planning in robot-assisted sub-tasks\ndemands a human-like reasoning unit capable of pre-\noperatively planning task execution and intraoperatively mod-\nifying the plan to accommodate unforeseen circumstances.\nThis approach would enhance the explainability of the robot’s\ndecisions and minimize associated risks. Large Language\nModels (LLMs), trained on vast amounts of text data, have\nrevolutionized natural language understanding and have been\nadopted in various domains beyond NLP, such as planning and\ninteraction for robots. They can serve as a high-level reasoning\nunit, breaking down given commands into smaller subtasks\nto be executed by the robot’s lower-level systems, including\nmotion planning and control.\nIntegrating LLMs into robots poses challenges due to their\nstruggle with real-world complexities. While LLMs offer\ngeneral knowledge and expertise, they lack a connection\nto tangible reality, leading to errors and potentially unsafe\nrecommendations. Extracting information from LLMs for\nrobots requires balancing theoretical knowledge with prac-\ntical understanding to navigate dynamic environments and\nfacilitate effective human-robot interaction. An agent needs\nto comprehend semantic aspects of the world, the range of\navailable skills, how these skills influence the environment,\nand how changes in the world map back to language [20].\nThe integration of robotics and LLMs, as introduced through\nGoogle’s SayCan [21], PaLM-E [22], and more recent methods\n[23]–[30], presents immense opportunities for exploration in\ndomains such as surgical robotics.\nBy leveraging multi-modal LLMs in a zero-shot manner,\nin this work we aim to surmount the limitations of current\nautonomous systems, introducing a level of reasoning and\nadaptability previously unattainable in robot-assisted surgeries.\nIntegrating images with text allows the multi-modal LLM to\ncapture important visual nuances, such as spatial relationships\nand the presence of surgical tools near blood pools, which\nmay not be fully conveyed through text alone. This integration\nis pioneering, as it combines the theoretical knowledge of\nLLMs with the practical demands of surgical environments as\nidentified and communicated by the medical staff. We propose\nText \nPrompt\nDeep Reinforcement \nLearning Agent\nAction\nPool \nSuction \nOrder\nContext-Based \nAugmented Prompt\nMulti-Modal LLM\nRaw Image of Initial Scene\nEnvironment\nStates\nTissue \nHeight \nMap\n“Prioritize \nand suction \nthe blood \npools”\nTarget \nPool \nMask\nBounding Box \nDetection\nHuman\nBlood Pool \nDetection\n 1\n2\n3\n4\nBlood Pool \nDetection\nFig. 2: System architecture.\nan LLM-powered framework capable of high-level reasoning,\nmid-level motion planning, and execution for autonomous\nblood suctioning in robot-assisted surgeries. The reasoning and\nprioritization responsibility is delegated to the higher-level task\nplanning LLM, and the motion planning and execution are\ndelegated to the DRL model, leading to a distributed agency\nbetween the two components. Expanding on the foundation\nlaid with autonomous blood suctioning, our vision extends\nbeyond this surgical subtask and sets a precedent for the future\nof autonomous systems in the surgical field.\nThe main contributions of this work are as follows:\n1) We propose an LLM-powered framework for au-\ntonomous robot-assisted blood suction, where task rea-\nsoning and planning are managed by the LLM, while\nmotion planning and execution are handled by a DRL-\ntrained agent.\n2) We compare the performance of LLM reasoning to\nrandom reasoning and no reasoning modules in terms\nof blood removal time and tool movement.\n3) We analyze how augmenting the prompts with context\nand expert-defined guidelines influences the reasoning\ncapabilities of the LLM in zero-shot prompting. A user\nstudy is also conducted to assess the similarity to human-\nlike behavior across the three modules.\nThe paper is structured as follows. Section II reviews\nthe integration of LLMs in robotics and surgery and their\nimpact on these fields. Section III introduces the methods\nused, including the system architecture, multi-modal LLMs,\nprompt augmentation, the simulation environment, and the\ndeep reinforcement learning module. Section IV presents and\ndiscusses the experiments and results. Section V outlines the\nlimitations and future work, while Section VI concludes the\npaper.\nII. R ELATED WORK\nA. Large Language Models in Robotics\nIn a pioneering work in LLM-Robotics integration, Google\nintroduces SayCan [21], a method that aims to extract knowl-\nZARGARZADEH et al.: FROM DECISION TO ACTION IN SURGICAL AUTONOMY 3\nFig. 3: An example of LLM reasoning with (LRWC) and without (LRWOC) context-based prompt augmentation. The guideline provided to\nthe LLM is as context is as follows: Address active bleeding first, consider pool size next, and address the blood clot pool last, as coagulation\nensures that flow in this pool has ceased and will not propagate further.\nedge stored within LLMs for physically grounded tasks. The\nLLM, ‘Say’, breaks down instructions into subtasks and evalu-\nates each skill’s contribution likelihood. Affordance functions,\n‘Can’, assess each skill’s success potential in the environment.\nThis approach combines these factors to determine the ef-\nfectiveness of each skill in fulfilling the instruction. In their\nlater work, PaLM-E [22], they propose an embodied language\nmodel that processes multi-modal sentences, blending visual,\ncontinuous state estimation, and textual inputs, leading to the\nintegration of real-world continuous sensor data into language\nmodels and creating a direct connection between words and\nsensory experiences.\nWhile these recent works have shown great advancement\nin the integration of LLMs into robotics, they mainly focus\non general tasks such as object manipulation. More specific\napplication domains, such as robot-assisted surgery, pose new\nand important challenges that need to be addressed to ensure\nthe safety of patients in surgical operations. For instance,\ndespite advancements, existing LLM applications have yet to\nfully tackle the real-time adaptation and decision-making re-\nquired in dynamic surgical settings. Nevertheless, the success\nof LLMs in broader robotics tasks can lay a foundational un-\nderstanding that is crucial when approaching more specialized\ndomains such as robot-assisted surgery where data is scarce.\nB. Large Language Models in Surgery and Surgical Planning\nLLMs can assist in surgical planning by analyzing vast\namounts of medical literature, patient data, and clinical guide-\nlines to suggest the most appropriate surgical approaches. This\nincludes evaluating the risks and benefits of different surgical\noptions and considering patient-specific factors such as age\nand previous surgical history and can be used in various\nsurgeries such as joint arthroplasty [31], oral and maxillofacial\nsurgery [32], and orthopedics [33]. It has the ability to process\nand generate complex language-based instructions, enabling\nbidirectional communication with the medical team in real-\ntime in an intuitive, fast, and natural way. This capability is\nessential for improved decision-making in dynamic surgical\nenvironments, where rapid and accurate communication is\ncrucial for patient safety and successful outcomes [34].\nAlthough an initial step towards integrating LLMs in robotic\nsurgery has been made in SUIFA [35] through tasks such\nas needle lift and shunt insertion, the current gaps in the\nliterature include the adoption of LLMs as reasoning units in\nthe planning process of surgical tasks where decision-making\nis crucial. In this work, we investigate the reasoning capability\nof LLMs in prioritizing the suction of multiple blood pools\nunder different circumstances and integrate them into robot-\nassisted surgery for autonomous blood suctioning.\nIII. M ETHODS\nA. Blood Suctioning Task and System Architecture\nBlood is among the most frequent types of fluids encoun-\ntered in surgical settings, as bleeding is a common and some-\ntimes unpredictable occurrence during operations. Surgeons\ntypically need to address bleeding promptly by clearing the\narea and pinpointing its source before proceeding with other\ntasks. Suction of the blood with the proper tool becomes\nnecessary to extract blood from the surgical site. Consequently,\nthis task is indispensable and often consumes significant\ntime and effort, and automating this process reasonably and\nsafely can alleviate the burden on surgeons. In a dynamic\nenvironment, such as the human body, fluids such as blood\nmove around, forming pools with different conditions. For\ninstance, characteristics such as active bleeding, blood clots,\nvariations in blood pool sizes, and the closeness of blood pools\nto critical organs or surgical instruments affect the priority with\nwhich they need to be suctioned.\nAs shown in Fig. 1, an image of the tissue scene containing\nmultiple blood pools is annotated with bounding boxes around\neach pool and passed to the multi-modal large language\nmodel, along with the text prompt, “Look at this image of\nmultiple blood pools. Please prioritize and suction the blood\neffectively.” The proposed system architecture, illustrated in\nFig. 2, depicts how the multi-modal LLM, as the high-level\nreasoning unit, uses its reasoning capability to prioritize the\norder in which the blood pools need to be suctioned and\ninforms the masking sensor accordingly. The pools are masked\nin the order that they need to be suctioned and then fed into\nthe trained DRL agent, which acts as the lower-level motion\nplanner, along with the tissue height map, and leads to an\naction taken by the suction tool. In complex scenarios where\nadditional information is needed for the LLM to prioritize\n4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2025\nsuction, human input can augment the initial prompt with\ncontext.\nB. Multi-Modal Large Language Models\nOur methodology leverages multi-modal LLMs to allow\nfor information from diverse modalities, including text and\nimages. In multi-modal LLMs, textual data undergoes process-\ning through a standard language model architecture. Feature\nembeddings extracted from image inputs are concatenated with\nthe textual embeddings, yielding a multi-input representation.\nThis fused representation is then fed into a multi-layer neural\nnetwork, facilitating joint learning across modalities. In this\nwork, we employed the pre-trained OpenAI GPT-4V model\n[36] for image understanding and reasoning.\nAs shown in Fig. 1 and explained in the system architecture\nsection, the multi-modal LLM accepts a text prompt and an\nimage of the labeled blood pools. The prompt outlines essen-\ntial details about blood pools, including the presence of clots,\nand signs of active bleeding. Each of these aspects is crucial\nin deciding the urgency of suctioning blood pools during\nmedical procedures. Size indicates the amount of blood, while\nclots indicate coagulation and cessation of blood flow. Active\nbleeding is a significant signal that could suggest the potential\nfor blood to spread more extensively. By incorporating these\nfactors into the prompt, we equip the multi-modal LLM with\nessential context for making informed decisions regarding\nblood pool suctioning. The LLM conducts reasoning based\non the image and the prompt, yielding the proper priority for\nsuctioning the blood, and explains why this priority is chosen\nin a zero-shot manner. Our zero-shot approach allows the\nLLM to generate relevant responses for each prompt without\nspecific training examples, relying on its general understanding\nof language semantics and prompt cues to address previously\nunseen conditions effectively.\nIn scenarios where we encounter a combination of factors\nconcerning blood pools simultaneously, such as when both\nactive bleeding and a blood clot exist in two of the pools,\nthe LLM may lack consistency in reasoning due to the lack\nof training on medical data. To address this, we activate the\nswitch as seen in Fig. 2 and exemplified in Fig. 3, and augment\nthe prompt with additional contexts providing a guideline\nfor the model in generating a consistent priority in complex\nscenarios. This approach remains in a zero-shot manner as\nwe refrain from furnishing the model with specific examples.\nInstead, we enhance the prompt through context augmentation\nas part of a prompt engineering process.\nC. Blood Suction Environment and Mask Sensor Mechanism\nIn our recent work [10], a blood suction simulation environ-\nment for RL was built using position-based fluids (PBF) based\non Nvidia PhysX 5, Unity, and Unity’s ML-Agents toolkit.\nPBF is an approach that represents fluids using a large number\nof small particles that interact with each other. PhysX is a real-\ntime physics engine with GPU optimization, which allows for\nPBF simulation. With PhysX 5 as the low-level physics engine,\nthe main simulation environment was built in Unity. In this\nmodel, blood is simulated as particles influenced by forces like\n(a) (b) (c)\n(d) (e) (f)\nFig. 4: Simulation Environment 1. The LLM reasoning prioritizes\nsuctioning the pools based on their size in the absence of surgical\ncomplexities such as active bleeding and blood clots as seen in (a)-\n(f).\ngravity and suction, with a spherical cone-shaped force field\napplied to particles near the suction tool. The force decays with\ndistance to realistically simulate suction, removing particles\nonce they reach a specified height threshold. We leverage the\nexisting simulation environment and further build upon it in\nthis work.\nThe simulation environment consists of a randomly gen-\nerated tissue that contains the blood, the simulated blood,\nand a suction tool. We simulated a fixed amount of blood\n(4000 particles) through PBF and added suction force to\neach particle within a suction range to simulate the effect of\nsuction. Particles that are suctioned close enough to the suction\ntool will be removed and marked as inactive. To introduce\nrandomness in the shape of the tissue, Bezier surfaces with\nrandom control points were used to generate random shapes.\nThe Bezier surfaces are represented by\nS(u, v) =\nnX\ni=0\nmX\nj=0\nPi,j · Bn,i(u) · Bm,j(v), 0 ≤ u, v≤ 1 (1)\nwhere Pi,j are the control points, and Bn,i(u) and Bm,j(v)\nare the Bernstein basis functions defined by\nBn,i(x) = n!\ni! · (n − i)! · xi · (1 − x)n−i. (2)\nIn this work, several features are added to the simulation\nenvironment, including a module that would allow blood\nto continuously add to a random pool representing active\nbleeding, the addition of a capsule-shaped object representing\na blood clot that would be randomly positioned on the tissue,\nand the ability to generate multiple independent blood pools.\nWe also developed a blood pool detector algorithm that takes\nin the raw image of the scene along with the suction orders\nfrom the multi-modal LLM and outputs the mask of the target\nblood pool based on the priority reasoned by the LLM. This\nmechanism would allow the agent to plan the suction motion\nof only the target pool and would blind the agent to other\nexisting pools unless commanded otherwise by the LLM.\nWe developed four simulation environments. In Environ-\nment 1, as can be seen in Fig. 4, four independent pools\nare randomly generated without the presence of a blood clot\nor active bleeding. In Environment 2, active bleeding occurs\nrandomly in one of the pools for a fixed time interval, with no\nZARGARZADEH et al.: FROM DECISION TO ACTION IN SURGICAL AUTONOMY 5\n(a) Environment 1 (No blood clot or active bleeding)\n0 100 200 300 400 500 600 700 800 900\nTime Steps\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of Blood Remaining (%)\nRandom Reasoning (RR)\nNo Reasoning (NR)\nLLM Reasoning (LR) (b) Environment 2 (Only active bleeding)\n0 100 200 300 400 500 600 700 800 900\nTime Steps\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of Blood Remaining (%)\nRandom Reasoning (RR)\nNo Reasoning (NR)\nLLM Reasoning (LR)\n(c) Environment 3 (Only blood clot)\n0 100 200 300 400 500 600 700 800 900\nTime Steps\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of Blood Remaining (%)\nRandom Reasoning (RR)\nNo Reasoning (NR)\nLLM Reasoning Without Context (LRWOC)\nLLM Reasoning With Context (LRWC) (d) Environment 4 (Both active bleeding and blood clot)\n0 100 200 300 400 500 600 700 800 900\nTime Steps\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of Blood Remaining (%)\nRandom Reasoning (RR)\nNo Reasoning (NR)\nLLM Reasoning Without Context (LRWOC)\nLLM Reasoning With Context (LRWC)\nFig. 5: Progression in blood suction in the four environments.\nsign of a blood clot. Environment 3 features the introduction of\na capsule-shaped object representing a blood clot, randomly\ngenerated near one of the pools, indicating coagulation and\nthe cessation of potential bleeding. Finally, Environment 4 in-\ncludes a pool actively bleeding, while another contains a blood\nclot. To test the realism of our simulation environments, visual\ncomparisons with real surgical environments were conducted,\nensuring our model visually reflects the dynamics of blood\nflow and bleeding during real surgical procedures.\nD. Motion Planning Using Deep Reinforcement Learning\nIn our recent work [10], an RL agent for completing\nautonomous blood suction was obtained. To train this agent,\nwe used the following reward function, which consists of a\nreward for the amount of blood removed during each step, an\nextra terminal reward for removing all blood, and an action\npenalty for tool movements. The number of particles being\nremoved during each step is used to determine the amount of\nblood being suctioned out.\nr(st, at, st+1) =Nt\np − Nt+1\np + C1 δ(Nt+1\np ) − C2∥at∥ (3)\nIn the above equation, t is the time step, Nt\np is the number\nof active particles, δ(Nt+1\np ) denotes whether there are active\nparticles remaining, and ∥at∥ is the norm of the actions. The\nweighting factors C1 = 5 and C2 = 0.02 were chosen to\nbalance task efficiency and control stability, emphasizing blood\nremoval while discouraging excessive motion. Specifically,\nC1 is set to a higher value to prioritize the reward for full\nblood clearance, encouraging the agent to complete the task\nefficiently, while C2 is relatively small, penalizing movements\nwithout restricting necessary adjustments. The observation\nincludes the tissue height map, the binary image mask of the\nblood (stacked with 3 from previous steps), and the suction\ntool location (stacked with 4 from previous steps). The binary\nimage mask of the blood is synthesized from the current\npositions of all active particles in the blood.\nIV. E XPERIMENTS AND RESULTS\nWe investigate four reasoning modules in our experiments.\nWhen the LLM, as the high-level reasoning unit, reasons\nthe sequence for suctioning blood pools without any addi-\ntional context, it is termed LLM Reasoning Without Context\n(LRWOC). If additional context is provided by the assistant,\nleading to an augmented prompt for the LLM, we call this\nLLM Reasoning With Context (LRWC). When the DRL agent\nreceives a randomly generated order based on a random\npermutation of the number of blood pools, this is known\nas Random Reasoning (RR). If the DRL agent proceeds to\nsuction blood pools solely based on its reward function without\ninput from a higher-level unit, it is termed No Reasoning (NR).\nA. Comparison of LLM Reasoning with Random Reasoning\nand No Reasoning\nTo evaluate the performance of the RR, NR, LRWOC,\nand LRWC modules, we simulated the blood suction task\nacross 400 distinct scenes (100 scenes per environment). Fig. 5\nillustrates the blood remaining percentage over time, providing\na comparative analysis of the different reasoning modules\nacross different environments. Additionally, Table I presents\n6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2025\n(a) Environment 2 (b) Environment 4\n*\n*\n*\n*\nFig. 6: Time to suction the actively bleeding pool ( TAB) in Envi-\nronments 2 and 4 by different reasoning modules.\nkey metrics such as the mean and standard deviations of the\ntime to suction the actively bleeding pool in Environments 2\nand 4 (TAB), the time to suction 50% ( T50) and 95% ( T95) of\nthe blood, and total tool path length (TTPL).\nIn Environment 1 (Fig. 5a), the LLM reasoning module\nreasons that larger volumes of blood must be addressed\nfirst and prioritizes suctioning the pools based on their size,\nresulting in a more rapid initial suction compared to the other\nmodules leading to a faster average T50. The NR module\nshows a gradual decrease in blood remaining by suctioning\nparts of pools as it moves between them, while the LR and\nRR modules prefer to suction one pool before moving to the\nnext, resulting in intervals where the slope decreases indicating\nmovement between pools.\nIn Environment 2 (Fig. 5b), where active bleeding is present\nin one of the pools, the LLM reasoning module gives priority\nto suctioning the pool with active bleeding first, even if it is\nsmaller than others. This approach results in a faster TAB.\nAfter addressing the pool with active bleeding, the LLM\nthen proceeds to prioritize suctioning the pools based on\ntheir size, similar to the strategy observed in Environment 1.\nProviding additional instructions as the augmented prompt did\nnot change the LLM reasoning in Environments 1 and 2 and\nthe focus remained on prioritizing active bleeding, followed\nby pool size. This resulted in the same LRWOC and LRWC,\ndenoted as LR.\nIn Environment 3 (Fig. 5c), the presence of a blood clot\nadds complexity to suctioning. LRWOC tends to prioritize\nthe suction of the pool with the blood clot potentially due\nto its perceived complexity. However, the user can define\nrules for the LLM to follow, tailoring its reasoning to specific\nconditions. For this environment, we establish the following\nrule: 1) Address active bleeding first, 2) Consider pool size\nnext, and 3) Address the blood clot pool last, as coagulation\nensures that flow in this pool has ceased and will not propagate\nfurther. An example of this is shown in Fig. 3. This context\nleads to a faster average T50 in LRWC, prioritizing blood\npool size after active bleeding. In Environment 4 (Fig. 5d),\nboth LRWOC and LRWC modules start by suctioning actively\nbleeding pools. LRWC prioritizes larger pools next and saves\nthe pool with the blood clot for last, resulting in a faster T50\ncompared to LRWOC, which targets the pool with the blood\nclot before proceeding based on pool size.\nFig. 6 illustrates the time to suction the actively bleeding\nTABLE I: Key metrics in comparison of reasoning modules ex-\npressed as mean ±std (Time step = 0.02 seconds).\nTAB T50 T95 TTPL\nEnvironment 1\nRR – 225 ±82 570 ±128 33.9 ±3.2\nNR – 164 ±38 557 ±110 32.4 ±4.1\nLR – 145±30 554 ±101 32.5 ±2.8\nEnvironment 2\nRR 363 ±176 223 ±89 573 ±130 33.3 ±3.4\nNR 390 ±174 154 ±37 514 ±106 31.5 ±3.9\nLR 128 ±50 161 ±28 523 ±95 33 ±2.8\nEnvironment 3\nRR – 222 ±71 563 ±124 32.4 ±2.9\nNR – 166 ±34 466 ±98 31.2 ±4.2\nLRWOC – 239 ±58 541 ±103 32.4 ±3.0\nLRWC – 151±23 553 ±100 32.7 ±2.9\nEnvironment 4\nRR 359 ±178 227 ±86 535 ±120 33.7 ±3.2\nNR 393 ±172 155 ±34 462 ±104 30.4 ±3.8\nLRWOC 130 ±49 179±55 509 ±93 33.7 ±2.9\nLRWC 130 ±49 137 ±18 470 ±90 32.9 ±2.8\npool ( TAB) in Environments 2 and 4 and shows statistically\nsignificant improvement (*) when LLM reasoning is used.\nTable I shows that the LRWC module results in smaller\nstandard deviations across all metrics and environments, indi-\ncating more consistent performance. Although the NR module\nexhibited a marginally smaller average TTPL, its inability\nto reason and adapt to unforeseen circumstances makes it\nunreliable in highly dynamic surgical settings.\nB. Advantage of Multi-Modal LLMs\nTo further demonstrate the advantage of the multi-modal\nLLM, we conducted an experiment to test its ability to capture\ncontextual details that may not be explicitly provided in text.\nIn this experiment, we presented the model with 10 images in\nwhich a surgical tool was positioned near one of the blood\npools, using our original prompt without any mention of\nthe tool in text. The multi-modal LLM correctly recognized\nthe presence of the tool in 80% of cases (8 out of 10)\nand incorporated this visual information into its decision-\nmaking process, placing this priority right after the active\nbleeding pool. Unlike hard-coded logic, which lacks flexibility\nin unforeseen scenarios, the LLM leverages context awareness\nand visual information to adapt to these conditions interpreting\nnuanced visual cues that a text-only input or a hard-coded logic\nmight overlook.\nC. User Study on Closeness to Human-Like Behaviour\nTo assess the similarity to human-like behavior exhibited\nby the RR, NR, and LRWOC modules during blood suction, a\nsurvey questionnaire was conducted involving ten participants.\nThe participants, consisting of graduate students and senior\nresearchers with no specialized medical backgrounds, were\nchosen to assess how well a non-expert-defined context aligns\nwith broader, non-specialist perspectives in evaluating the\nbehavior of the RR, NR, and LRWOC modules. A total of\nthirty-six videos were collected for the survey, divided among\nthree reasoning modules (RR, NR, LRWOC), with each mod-\nule contributing twelve videos from four environments (three\nvideos per environment), showcasing the blood suctioning\ntask. Participants were presented with pairs of videos and\nZARGARZADEH et al.: FROM DECISION TO ACTION IN SURGICAL AUTONOMY 7\nasked, “If you were the human operator, which of the two\nvideos shown below would you choose to suction the blood\npools?”. This forced choice questioning process was repeated\nto compare all combinations of LRWOC versus NR versus\nRR.\nA second user study survey involved collecting nine videos\neach of LRWOC and LRWC (in Environments 3 and 4), which\nwere then presented to the participants with the same question.\nA human performance score was defined as the number of\nvideos selected by the participants normalized by the total\nnumber of videos, as shown in Fig. 7. The objective of these\nsurveys was to investigate whether LLM reasoning aligns more\nwith actual human decision-making than random reasoning\nand no reasoning modules and also to assess how providing\nour user-defined context impacts the LLM’s ability to mimic\nhuman decision-making. This research was approved by the\nUniversity of Alberta’s Research Ethics Board under approval\nID Pro00139696.\nA one-way analysis of variance (ANOV A) test was used\nin the first survey and a paired t-test in the second survey\nto establish statistical significance among reasoning modules,\nas shown in Fig. 7. The first study yielded a p-value<0.001,\nshowing that LRWOC had significantly more human-like\nblood suction behavior. The second study also resulted in a\np-value<0.001. The user study results, indicating a preference\nfor the LRWC over the LRWOC module’s decision-making,\nsuggest that incorporating contextual understanding in robotic\nsurgery could bridge the gap between automated procedures\nand the intuitive decision-making of humans.\nAlthough results show a promising stride towards explain-\nable surgical autonomy, a thorough evaluation process includ-\ning clinical trials is essential to establish the efficacy and\nsafety of LLM-powered robotic systems in real-world surgical\napplications. Additionally, the development of new training\nprotocols for surgical teams on interacting with and oversee-\ning LLM-enabled systems, along with intuitive interfaces for\nsurgeons to interact with and override the system’s decisions\nwhen necessary, will be vital for adoption.\nV. L IMITATIONS AND FUTURE WORK\nWhile the proposed method demonstrates the effective-\nness of multi-modal LLMs in reasoning and decision-making\nwithin a simulated environment, several limitations and areas\nfor future work remain, particularly for transitioning to real-\nworld applications in surgical settings.\nThis study assumed that blood pools are separate and\nindependent, simplifying interactions within the environment.\nAdditionally, the current system operates below real-time\nperformance due to the generation speed of OpenAI’s GPT-4V ,\nwhich constrains its immediate applicability in time-sensitive\nclinical tasks. The physical experiments demonstrated in our\nprevious work in blood suction [10] show the real-world\nexecution of this task and hence were not the main focus of\nthis study.\nSimulation-based environments, while inherently limited\ncompared to real-world settings, offer the critical advantage\nof encompassing a wide range of scenarios, including rare but\n** *\nFig. 7: Comparison on closeness to human-like behavior of different\nreasoning modules.\npivotal situations that are difficult to consistently reproduce\nin physical setups. This simulation-first approach provides a\nfoundation to test and refine the system’s decision-making\ncapabilities across varied conditions, setting the groundwork\nfor application in real surgical environments.\nApplying this framework to real surgical contexts intro-\nduces practical challenges, such as accurate segmentation of\nblood from live camera images, real-time pose coordination\nof surgical instruments, and adapting to the dynamic and\ncomplex environment of an actual operating room. Future\nwork will address these challenges by refining and validating\nthe framework’s applicability in real surgical tasks, exploring\npredictive mechanisms and pre-emptive prompts to prioritize\ntime-sensitive actions. To enhance real-time feasibility, model\ndistillation, quantization, and smaller, task-specific models will\nbe investigated, enabling the system to better align with the\noperational demands of clinical practice.\nEnsuring the accuracy and safety of LLM-based decisions\nis another priority. LLMs are prone to hallucinations, which\ncould impact decision reliability. To mitigate this, future\niterations will integrate rule-based checks and domain-specific\nconstraints, as well as feedback loops from medical experts,\nto improve the system’s robustness and decision accuracy in\nhigh-stakes environments. Gathering insights from medical\nprofessionals will also help assess the LLM’s alignment with\nexpert-defined surgical priorities, refining its adaptability to\nclinical needs.\nSafety in surgical tasks is essential, and the current frame-\nwork will be expanded to address this by incorporating\nforce-based thresholds to control tool speed and acceleration,\ncollision detection, and Safe Reinforcement Learning (Safe\nRL) techniques, such as reward shaping and risk-sensitive\npolicies, to enhance safe tool proximity to sensitive tissues.\nAdditionally, to better handle dynamic scenarios, future work\nwill explore mid-sequence task re-planning to adapt to external\ndisturbances and refine LLM reasoning capabilities through\nreinforcement learning from human feedback, ultimately im-\nproving the framework’s scalability to other surgical and\nmedical tasks.\n8 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2025\nVI. C ONCLUSION\nIn this study, we proposed a multi-modal LLM integration\nin robot-assisted surgery for autonomous blood suction and\ninvestigated how the addition of a high-level reasoning unit\ncan influence decision-making and performance. Experiments\nwere conducted to analyze LLM reasoning in comparison to\nrandom reasoning and no reasoning modules. Active bleeding\nand blood clots were introduced to influence decision-making\nas is also common in highly dynamic surgical settings. Results\nshowed that the presence of a multi-modal LLM as a higher-\nlevel reasoning unit can account for these surgical complexities\nin decision-making and prioritization to achieve a level of\nreasoning and explainability previously unattainable in robot-\nassisted surgeries. The user study showed that incorporating\ncontextual understanding in robotic surgery could bridge the\ngap between automated procedures and the intuitive decision-\nmaking of humans.\nREFERENCES\n[1] R. H. Taylor, A. Menciassi, G. Fichtinger, P. Fiorini, and P. Dario,\n“Medical robotics and computer-integrated surgery,” Springer handbook\nof robotics, pp. 1657–1684, 2016.\n[2] P. Kazanzides, Z. Chen, A. Deguet, G. S. Fischer, R. H. Taylor, and S. P.\nDiMaio, “An open-source research kit for the da vinci® surgical system,”\nin 2014 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2014, pp. 6434–6439.\n[3] Y . Ou, S. Zargarzadeh, P. Sedighi, and M. Tavakoli, “A realistic surgical\nsimulator for non-rigid and contact-rich manipulation in surgeries with\nthe da vinci research kit,” arXiv preprint arXiv:2404.05888 , 2024.\n[4] P. M. Scheikl, B. Gyenes, R. Younis, C. Haas, G. Neumann, M. Wagner,\nand F. Mathis-Ullrich, “Lapgym-an open source framework for rein-\nforcement learning in robot-assisted laparoscopic surgery,” Journal of\nMachine Learning Research , vol. 24, no. 368, pp. 1–42, 2023.\n[5] J. Xu, B. Li, B. Lu, Y .-H. Liu, Q. Dou, and P.-A. Heng, “Surrol: An open-\nsource reinforcement learning centered and dvrk compatible platform for\nsurgical robot learning,” in 2021 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS) . IEEE, 2021, pp. 1821–1828.\n[6] P. M. Scheikl, E. Tagliabue, B. Gyenes, M. Wagner, D. Dall’Alba,\nP. Fiorini, and F. Mathis-Ullrich, “Sim-to-real transfer for visual rein-\nforcement learning of deformable object manipulation for robot-assisted\nsurgery,” IEEE Robotics and Automation Letters , vol. 8, no. 2, pp. 560–\n567, 2022.\n[7] S. Sen, A. Garg, D. V . Gealy, S. McKinley, Y . Jen, and K. Goldberg,\n“Automating multi-throw multilateral surgical suturing with a mechan-\nical needle guide and sequential convex optimization,” in 2016 IEEE\nInternational Conference on Robotics and Automation (ICRA) . IEEE,\n2016, pp. 4178–4185.\n[8] Y . Ou, S. Zargarzadeh, and M. Tavakoli, “Robot learning incorporating\nhuman interventions in the real world for autonomous surgical endo-\nscopic camera control,” Journal of Medical Robotics Research , vol. 8,\nno. 03n04, p. 2340004, 2023.\n[9] N. D. Nguyen, T. Nguyen, S. Nahavandi, A. Bhatti, and G. Guest,\n“Manipulating soft tissues by deep reinforcement learning for au-\ntonomous robotic surgery,” in 2019 IEEE International Systems Con-\nference (SysCon). IEEE, 2019, pp. 1–7.\n[10] Y . Ou, A. Soleymani, X. Li, and M. Tavakoli, “Autonomous blood\nsuction for robot-assisted surgery: A sim-to-real reinforcement learning\napproach,” IEEE Robotics and Automation Letters , 2024.\n[11] F. Lalys and P. Jannin, “Surgical process modelling: a review,” Interna-\ntional journal of computer assisted radiology and surgery , vol. 9, pp.\n495–511, 2014.\n[12] E. Tagliabue, A. Pore, D. Dall’Alba, E. Magnabosco, M. Piccinelli, and\nP. Fiorini, “Soft tissue simulation environment to learn manipulation\ntasks in autonomous robotic surgery,” in 2020 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS) . IEEE, 2020, pp.\n3261–3266.\n[13] J. Huang, F. Liu, F. Richter, and M. C. Yip, “Model-predictive con-\ntrol of blood suction for surgical hemostasis using differentiable fluid\nsimulations,” in 2021 IEEE International Conference on Robotics and\nAutomation (ICRA), 2021, pp. 12 380–12 386.\n[14] Y . Ou and M. Tavakoli, “Sim-to-real surgical robot learning and au-\ntonomous planning for internal tissue points manipulation using rein-\nforcement learning,” IEEE Robotics and Automation Letters , vol. 8,\nno. 5, pp. 2502–2509, 2023.\n[15] H. Su, A. Mariani, S. E. Ovur, A. Menciassi, G. Ferrigno, and\nE. De Momi, “Toward teaching by demonstration for robot-assisted\nminimally invasive surgery,” IEEE Transactions on Automation Science\nand Engineering, vol. 18, no. 2, pp. 484–494, 2021.\n[16] Y . Hu and M. Tavakoli, “Autonomous ultrasound scanning towards stan-\ndard plane using interval interaction probabilistic movement primitives,”\nin 2023 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS). IEEE, 2023, pp. 3719–3727.\n[17] A. Pore, D. Corsi, E. Marchesini, D. Dall’Alba, A. Casals, A. Farinelli,\nand P. Fiorini, “Safe reinforcement learning using formal verification\nfor tissue retraction in autonomous robotic-assisted surgery,” in 2021\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS). IEEE, 2021, pp. 4025–4031.\n[18] J. Garcıa and F. Fern ´andez, “A comprehensive survey on safe reinforce-\nment learning,” Journal of Machine Learning Research , vol. 16, no. 1,\npp. 1437–1480, 2015.\n[19] K. Fan, Z. Chen, G. Ferrigno, and E. De Momi, “Learn from safe\nexperience: Safe reinforcement learning for task automation of surgical\nrobot,” IEEE Transactions on Artificial Intelligence , 2024.\n[20] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\nJ. Tompson, I. Mordatch, Y . Chebotaret al., “Inner monologue: Embod-\nied reasoning through planning with language models,” arXiv preprint\narXiv:2207.05608, 2022.\n[21] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David,\nC. Finn, C. Fu, K. Gopalakrishnan, K. Hausman et al. , “Do as I can,\nnot as I say: Grounding language in robotic affordances,” arXiv preprint\narXiv:2204.01691, 2022.\n[22] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,\nA. Wahid, J. Tompson, Q. Vuong, T. Yu et al., “Palm-e: An embodied\nmultimodal language model,” arXiv preprint arXiv:2303.03378 , 2023.\n[23] H. Liu, Y . Zhu, K. Kato, I. Kondo, T. Aoyama, and Y . Hasegawa, “LLM-\nbased human-robot collaboration framework for manipulation tasks,”\narXiv preprint arXiv:2308.14972 , 2023.\n[24] S. S. Kannan, V . L. Venkatesh, and B.-C. Min, “Smart-LLM: Smart\nmulti-agent robot task planning using large language models,” arXiv\npreprint arXiv:2309.10062, 2023.\n[25] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,\nS. Rusinkiewicz, and T. Funkhouser, “Tidybot: Personalized robot as-\nsistance with large language models,” arXiv preprint arXiv:2305.05658,\n2023.\n[26] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\nJ. Thomason, and A. Garg, “Progprompt: Generating situated robot\ntask plans using large language models,” in 2023 IEEE International\nConference on Robotics and Automation (ICRA) . IEEE, 2023, pp.\n11 523–11 530.\n[27] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, “ChatGPT for\nrobotics: Design principles and model abilities,” Microsoft Auton. Syst.\nRobot. Res, vol. 2, p. 20, 2023.\n[28] Z. Long, G. Killick, R. McCreadie, and G. A. Camarasa, “Robollm:\nRobotic vision tasks grounded on multimodal large language models,”\narXiv preprint arXiv:2310.10221 , 2023.\n[29] M. G. Arenas, T. Xiao, S. Singh, V . Jain, A. Z. Ren, Q. Vuong, J. Varley,\nA. Herzog, I. Leal, S. Kirmani et al. , “How to prompt your robot: A\npromptbook for manipulation skills with code as policies,” in Towards\nGeneralist Robots: Learning Paradigms for Scalable Skill Acquisition@\nCoRL2023, 2023.\n[30] Y . Dai, R. Peng, S. Li, and J. Chai, “Think, act, and ask:\nOpen-world interactive personalized robot navigation,” arXiv preprint\narXiv:2310.07968, 2023.\n[31] K. Cheng, Z. Li, C. Li, R. Xie, Q. Guo, Y . He, and H. Wu, “The potential\nof GPT-4 as an AI-powered virtual assistant for surgeons specialized in\njoint arthroplasty,” Annals of Biomedical Engineering , pp. 1–5, 2023.\n[32] B. Puladi, C. Gsaxner, J. Kleesiek, F. H ¨olzle, R. R ¨ohrig, and J. Egger,\n“The impact and opportunities of large language models like ChatGPT in\noral and maxillofacial surgery: a narrative review,” International journal\nof oral and maxillofacial surgery , 2023.\n[33] S. Chatterjee, M. Bhattacharya, S. Pal, S.-S. Lee, and C. Chakraborty,\n“ChatGPT and large language models in orthopedics: from education\nand surgery to research,” Journal of Experimental Orthopaedics, vol. 10,\nno. 1, p. 128, 2023.\n[34] K. Cheng, Z. Sun, Y . He, S. Gu, and H. Wu, “The potential impact\nof chatgpt/gpt-4 on surgery: will it topple the profession of surgeons?”\nInternational Journal of Surgery , pp. 10–1097, 2023.\nZARGARZADEH et al.: FROM DECISION TO ACTION IN SURGICAL AUTONOMY 9\n[35] M. Moghani, L. Doorenbos, W. C.-H. Panitch, S. Huver, M. Azizian,\nK. Goldberg, and A. Garg, “Sufia: Language-guided augmented dexterity\nfor robotic surgical assistants,” arXiv preprint arXiv:2405.05226 , 2024.\n[36] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al. , “GPT-4\ntechnical report,” arXiv preprint arXiv:2303.08774 , 2023.",
  "topic": "Robot",
  "concepts": [
    {
      "name": "Robot",
      "score": 0.5957757830619812
    },
    {
      "name": "Artificial intelligence",
      "score": 0.569866418838501
    },
    {
      "name": "Robotics",
      "score": 0.54705810546875
    },
    {
      "name": "Autonomy",
      "score": 0.5378057360649109
    },
    {
      "name": "Computer science",
      "score": 0.5061530470848083
    },
    {
      "name": "Modal",
      "score": 0.4815531075000763
    },
    {
      "name": "Agency (philosophy)",
      "score": 0.48037227988243103
    },
    {
      "name": "Automation",
      "score": 0.43705350160598755
    },
    {
      "name": "Human–computer interaction",
      "score": 0.38808003067970276
    },
    {
      "name": "Engineering",
      "score": 0.2837517261505127
    },
    {
      "name": "Political science",
      "score": 0.14830613136291504
    },
    {
      "name": "Sociology",
      "score": 0.1366647183895111
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": []
}