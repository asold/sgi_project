{
    "title": "Short-Term Electrical Load Forecasting Based on Time Augmented Transformer",
    "url": "https://openalex.org/W4293068799",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2114799311",
            "name": "Guangqi Zhang",
            "affiliations": [
                "Beijing University of Civil Engineering and Architecture"
            ]
        },
        {
            "id": "https://openalex.org/A2156610977",
            "name": "Chuyuan Wei",
            "affiliations": [
                "Beijing University of Civil Engineering and Architecture"
            ]
        },
        {
            "id": "https://openalex.org/A2194011137",
            "name": "Changfeng Jing",
            "affiliations": [
                "Beijing University of Civil Engineering and Architecture"
            ]
        },
        {
            "id": "https://openalex.org/A2344162509",
            "name": "Yanxue Wang",
            "affiliations": [
                "Beijing University of Civil Engineering and Architecture"
            ]
        },
        {
            "id": "https://openalex.org/A2114799311",
            "name": "Guangqi Zhang",
            "affiliations": [
                "Beijing University of Civil Engineering and Architecture"
            ]
        },
        {
            "id": "https://openalex.org/A2156610977",
            "name": "Chuyuan Wei",
            "affiliations": [
                "Beijing University of Civil Engineering and Architecture"
            ]
        },
        {
            "id": "https://openalex.org/A2194011137",
            "name": "Changfeng Jing",
            "affiliations": [
                "Beijing University of Civil Engineering and Architecture"
            ]
        },
        {
            "id": "https://openalex.org/A2344162509",
            "name": "Yanxue Wang",
            "affiliations": [
                "Beijing University of Civil Engineering and Architecture"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2614689352",
        "https://openalex.org/W3109872608",
        "https://openalex.org/W3112697087",
        "https://openalex.org/W3084535629",
        "https://openalex.org/W3128608971",
        "https://openalex.org/W3031648028",
        "https://openalex.org/W3120726020",
        "https://openalex.org/W3045300245",
        "https://openalex.org/W3166512129",
        "https://openalex.org/W2889386826",
        "https://openalex.org/W2137775720",
        "https://openalex.org/W1851998939",
        "https://openalex.org/W2598601173",
        "https://openalex.org/W2969308181",
        "https://openalex.org/W3158121765",
        "https://openalex.org/W2947934295",
        "https://openalex.org/W2538107750",
        "https://openalex.org/W1514611985",
        "https://openalex.org/W2980471844",
        "https://openalex.org/W3136021864",
        "https://openalex.org/W2960560113",
        "https://openalex.org/W2820191439",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3094673547",
        "https://openalex.org/W3002709689",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W4297734170",
        "https://openalex.org/W2964121744"
    ],
    "abstract": "Abstract Electrical load forecasting is of vital importance in intelligent power management and has been a hot spot in industrial Internet application field. Due to the complex patterns and dynamics of the data, accurate short-term load forecasting is still a challenging task. Currently, many tasks use deep neural networks for power load forecasting, and most use recurrent neural network as the basic architecture, including Long Short-Term Memory (LSTM), Sequence to Sequence (Seq2Seq), etc. However, the performance of these models is not as good as expected due to the gradient vanishing problem in recurrent neural network. Transformer is a deep learning model initially designed for natural language processing, it calculates input–output representations and captures long dependencies entirely on attention mechanisms which has great performance for capturing the complex dynamic nonlinear sequence dependence on long sequence input. In this work, we proposed a model Time Augmented Transformer (TAT) for short-term electrical load forecasting. A temporal augmented module in TAT is designed to learn the temporal relationships representation between the input history series to adapt to the short-term power load forecasting task. We evaluate our approach on a real-word dataset for electrical load and extensively compared it to the performance of the existed electrical load forecasting model including statistical approach, traditional machine learning and deep learning methods, the experimental results show that the proposed TAT model results in higher precision and accuracy in short-term load forecasting.",
    "full_text": "Vol.:(0123456789)1 3\nInternational Journal of Computational Intelligence Systems           (2022) 15:67  \nhttps://doi.org/10.1007/s44196-022-00128-y\nRESEARCH ARTICLE\nShort‑Term Electrical Load Forecasting Based on Time Augmented \nTransformer\nGuangqi Zhang1 · Chuyuan Wei2  · Changfeng Jing3 · Yanxue Wang1\nReceived: 26 November 2021 / Accepted: 28 July 2022 \n© The Author(s) 2022\nAbstract\nElectrical load forecasting is of vital importance in intelligent power management and has been a hot spot in industrial \nInternet application field. Due to the complex patterns and dynamics of the data, accurate short-term load forecasting is \nstill a challenging task. Currently, many tasks use deep neural networks for power load forecasting, and most use recurrent \nneural network as the basic architecture, including Long Short-Term Memory (LSTM), Sequence to Sequence (Seq2Seq), \netc. However, the performance of these models is not as good as expected due to the gradient vanishing problem in recurrent \nneural network. Transformer is a deep learning model initially designed for natural language processing, it calculates input–\noutput representations and captures long dependencies entirely on attention mechanisms which has great performance for \ncapturing the complex dynamic nonlinear sequence dependence on long sequence input. In this work, we proposed a model \nTime Augmented Transformer (TAT) for short-term electrical load forecasting. A temporal augmented module in TAT is \ndesigned to learn the temporal relationships representation between the input history series to adapt to the short-term power \nload forecasting task. We evaluate our approach on a real-word dataset for electrical load and extensively compared it to the \nperformance of the existed electrical load forecasting model including statistical approach, traditional machine learning and \ndeep learning methods, the experimental results show that the proposed TAT model results in higher precision and accuracy \nin short-term load forecasting.\nKeywords Short-term load forecast · Transformer · Deep neural network · Time augmentation\nAbbreviations\nLSTM  Long short-term memory\nBiLSTM  Bi-directional long short-term memory\nSeq2Seq  Sequence to sequence\nARIMA  Autoregressive integrated moving average\nSVR  Support vector regression\nTAT   Time augmentation Transformer\nRF  Radom forest\nGBM  Gradient boosting machines\nCNN  Convolutional neural network\nRNN  Recurrent neural networks\nLSSVM  Least squares support vector machine\nMSE  Mean squared error\nGRU   Gate recurrent unit\nRMSE  Root mean squared error\nMAE  Mean absolute error\nMAPE  Mean absolute percentage error\n * Chuyuan Wei \n weichuyuan@bucea.edu.cn\n1 School of Mechanical-Electronic and Vehicle Engineering, \nBeijing University of Civil Engineering and Architecture, \nBeijing 100044, China\n2 School of Electrical and Information Engineering, \nBeijing University of Civil Engineering and Architecture, \nBeijing 100044, China\n3 School of Geomatics and Urban Spatial Informatics, \nBeijing University of Civil Engineering and Architecture, \nBeijing 100044, China\n1 Introduction\nElectricity has become a necessity of daily life in the modern \nworld. Recently, the global demand and usage of electricity \nhas been increasing drastically due to urban development, \nindustrial expansion, climate change, population growth and \nso on [1–3]. However, the process of power scheduling and \ntransmission is costly and the amount of power is insufficient \nto meet global demand. As a solution, many studies aim to \nuse various methods to forecast future electricity demand so \nthat the governments and power companies can plan ahead \n International Journal of Computational Intelligence Systems           (2022) 15:67 \n1 3   67  Page 2 of 11\neffectively and promote energy efficiency among customers \n[4].\nElectrical load forecasting is of vital importance in intel-\nligent power management and has been an interest topic \nin academic and business domains [5 ]. The electrical load \nforecasting is not only a task to reasonably guide power \nplanning, but also an important guarantee for improving the \neconomy of the power system and ensuring the safe opera-\ntion of the electrical grid. Hence, as an essential function for \npower management, electrical load forecasting is crucial to \nthe relevant decision-making. However, accurate forecast \nof electrical load using time series data of historical electric \nconsumption is still a challenging task [6]. Due to the com-\nplex patterns and dynamics of the data, it may be affected \nby various factors, including temperature, seasons, economy \nand some unpredictable events [7]. How to fit these complex \nfactors affecting power demand into the prediction models \nneeds to be solved urgently [8]. With the different forecast-\ning scale time, the work can be categorized as three types: \nshort-term, medium-term and long-term [9]. Short-term load \nforecasting can offer strong support for real-time schedul-\ning and operation planning of power system, and reduce \nthe excessive consumption of energy [10]. It has always \nbeen a hot spot in power research, with more and more new \nmethods being introduced including statistical methods and \nmachine learning methods. Statistical methods commonly \nused for power load and network traffic forecasting such as \nARIMA [11, 12], etc. can effectively use the input histori-\ncal data to predict future power load. But with increasing \ndemand for higher forecast accuracy, the predictive power \nof these models is insufficient, since it’s difficult to deal with \ncomplex patterns and dynamic electrical demand data for \nstatistical approaches. Machine learning methods, such as \nSupport Vector Regression (SVR) [13], Radom Forest (RF), \nGradient Boosting Machines (GBM) [14], etc. are also used \nfor power load forecasting because of their powerful ability \non processing and analyzing some nonlinear and complex \nproblems. In recent years, deep learning methods, which \ngain ground on feature extraction than traditional machine \nlearning methods, have developed rapidly and been able to \npredict power load more accurately. Many models based on \ndeep learning methods are applied for short-term load fore-\ncast such as Convolutional Neural Network (CNN), Recur -\nrent Neural Networks (RNN), LSTM, Bi-directional long \nshort-term memory (Bi-LSTM), Seq2Seq [15, 16], etc.\nTransformer is also a deep learning method with a new \nnetwork architecture initially designed for machine transla-\ntion[17]. It entirely depends on the attention mechanisms \nwithout sequence aligned recurrence and convolutions to \ncalculate input–output representations and captures long \nsequence dependencies. Transformer model has great per -\nformance for capturing the complex dynamic nonlinear \nsequences dependence on long sequence input to provide a \nnew possibility for power load forecasting. In this work, we \nfocus on the short-term forecasting with multivariate time \nseries data, and propose a new model Time Augmented \nTransformer (TAT) based on an adaptation of the recent \ndeep self-attention Transformer architecture incorporating \na time augmentation method for short-term load forecast. \nThe main contributions and novel findings are the following:\n1. A highly accurate electrical short-term load forecasting \napproach based on Transformer Model was developed. \nWe have modified the original Transformer to adapt the \nelectrical load forecasting to successfully improve the \nprediction capacity.\n2. The new Time Augmented Transformer model is pro-\nposed on the basis of an adaptation of the recent deep \nself-attention Transformer architecture. We extracted \nadditional time features as augmentation encoding to \nenhance the temporal representation of the historical \ninput sequences. The TAT model further improved the \nability of learning the nonlinear relationship between \nload data and achieved great improvement.\n3. We carefully designed experiments to demonstrate that \nmultivariate feature input is more appropriate for the \nproposed model in the short-term load forecasting task \nand our approach can use less historical information \nto make more accurate predictions, which means less \nmemory occupancy and faster calculation speed.\n2  Related Work\nPrevious work on short-term electrical load forecast can be \nclassified into statistical approaches, machine learning and \ndeep learning [4]. Many statistical methods used in electrical \nload forecasting just like ARIMA [18, 19]. Wei and Zhang \n[20] proposed an ARIMA model for short-term electrical \nload forecasting. However, it’s difficult to deal with complex \npatterns and dynamic electrical demand data for statistical \napproaches, and it has high requirements for the stationar -\nity of the data, so that the accuracy of the prediction results \nby statistical methods are not enough and the statistical \napproaches fail to achieve the expected forecasting results.\nIn recent years, machine learning methods have gradu-\nally been investigated for power load forecasting. Artifi-\ncial intelligence-based methods have accounted for 90 \npercent of power forecasting research models during \n2010 to 2020 [4 ]. Yi, Niu [21] using a wavelet transform \nwith least squares support vector machine (LSSVM) to \npredict demand power. The random forest was used for \nshort-term load prediction in one day ahead of one-step \nin Tunisia [22]. Besides, Zhang, Li [23] compared three \nInternational Journal of Computational Intelligence Systems           (2022) 15:67  \n1 3 Page 3 of 11    67 \nkinds of models, multiple linear regression, RF and gra-\ndient boosting, for hourly electricity load forecasting in \nsouthern California, the result demonstrated that gradient \nboosting has the best performance.\nAlong the rapid development of artificial intelligence, \ndeep learning has widely been used in natural language \nunderstanding, image processing, autonomous driving \nand other fields [24, 25]. Deep learning methods can \nnot only capture the complex dependencies in nonlin-\near dynamic system, but also achieve remarkable per -\nformance in many prediction applications with higher \naccuracy [26, 27]. Tokgöz and Ünal [28] built a forecast-\ning model based RNN with an ant colony optimization \nalgorithm and improved the prediction accuracy in elec-\ntrical load forecasting. However, RNN has the problem \nof gradient vanishing when dealing with long sequence \ninput that the back-propagation error either decays rap-\nidly or grows beyond the limit, and it is difficult to cap-\nture the long-distance dependencies between sequences. \nLong Short Term Memory (LSTM), which is a further \ndeveloped model based on RNN, realizes the function \nof forgetting or remembering using “Gates” to control \nthe discarding or adding of information to solve the \nproblem of gradient disappearance of RNN [29]. Peng, \nShuai [15] have applied LSTM to improve the forecast \naccuracy of traditional RNN model. Besides, CNN has \nalso been used in load forecast because of its excellent \nability to capture the trend of load data. Wang, Zhao \n[30] proposed a mothed based on the integration of CNN \nand LSTM and the results in higher precision in short-\nterm forecasting. Taking into consideration to utilize the \nglobal historical information, Gong, An [16] developed \na short-term load prediction model based on Seq2Seq, \nwhich use encoder–decoder architecture, has exhibit-\ning better performance. However, Seq2Seq model uses a \nrecurrent neural network structure as encoder to encode \nhistorical information into an intermediate vector, it will \ninevitably lose the dynamic dependencies between his-\ntorical sequences in the encoder vector.\n3  The Proposed Approach\n3.1  Problem Description\nWe can convert the power load forecasting to a supervised \nlearning problem, in multi-step ahead electric load forecast-\ning, the input sequence under the rolling forecasting setting \nwith a sliding window, a history time series of historical \nelectrical load and relative features X =/braceleft.s1xt1\n,xt2\n,… ,xtn\n/uni007C.varxti\n∈ R dx\n/braceright.s1 was given, and the output is the prediction of the \nnext m-step electrical load sequence  \nY =\n/braceleft.s2\nxtn+1\n,xtn+2\n,…,xtn+m\n/uni007C.varxti\n∈ R dy\n/braceright.s2\n , where dx is the number \nof feature in the input vector and x ti\n can be a scaler or a vec-\ntor that consists of multiple features including historical \nelectrical load, dry bulb temperature, wet bulb temperature, \ndew point temperature, hours and electricity price, and \ndy = 1 . Figure 1 shows the sliding window for the input elec-\ntrical load sequence. In this work, for short-term electrical \nload forecasting, we will make predictions for 30 min, 1 h, \n12 h and one day respectively using historical data from the \nprevious 1 day as input, that means m = 1, 2, 24, 48 and \nn = 48 while a time step m denotes 30 min.\n3.2  Time Augmentation Transformer Model\nThe Transformer model entirely depends on the attention \nmechanisms without sequence aligned recurrence and con-\nvolutions to calculate input–output representations and cap-\ntures long sequence dependencies [17]. Furthermore, it does \nnot process data in an ordered sequence manner, but used \nattention mechanisms to process entire sequence to learn \ndependencies without regard to their distance from input \nsequences. Therefore, Transformer-based model has the \nFig. 1  Sliding windows to \nconstruct supervised learning \nexamples for rolling forecasting\n Xt1 Xt2 ... Xtn Xtn+1 ... Xtn+m\nXt1\n1 Xt1\n2 ... Xt1\ndx Xtn+1\nload\nTimes\nXt1Xt1 Xt2Xt2 ...... XtnXtn Xtn+1Xtn+1 ...... Xtn+mXtn+m\nXt1\n1Xt1\n1 Xt1\n2Xt1\n2 ...... Xt1\ndxXt1\ndx Xtn+1\nloadXtn+1\nload\nLoad Consumpption\n International Journal of Computational Intelligence Systems           (2022) 15:67 \n1 3   67  Page 4 of 11\npotential to model complex dynamics of the electrical load \ndata [31]. Because of the transformer design for machine \ntranslation, it cannot be directly used to forecast the electri-\ncal load. To this end we have modified the Transformer to \nadapt our task.\nThe structure of our Time Augmentation Trans-\nformer named TAT is show in Fig.  2. TAT model use \nencoder–decoder architecture. All the historical load and \nfeatures are inputted into the encoder to generate a history \nglobal information coding result after fusion time informa-\ntion in input layer. The decoder uses the one-position shifted \nfuture load data and the historical global attention vector \nencoded by the encoder as the input to predict the electrical \nload on next step.\nInput Layer:  The input layer is composed of a fully \nconnected layer, a position encoding layer and a time aug-\nmented encoding layer. The historical electrical load data \nfirstly entry the input layer. Unlike the original Transformer \narchitecture, the historical observation X ∈ ℝn×dx is trans-\nformed to X ∈ ℝn×dmodel that maps the input data to a vector \nof dimension dmodel by employing a fully connected layer, \nwhere n is the input time step of historical data, dx is the \nnumber of input features for a single time step. Positional-\nencoding PE was added to above the fully connected layer, \nit injects relative and absolute position information of the \ninput sequence using sine and cosine functions:\n(1)\nPE (pos,2i) = sin/parenleft.s1pos∕10000 2i∕dmodel\n/parenright.s1\nPE (pos,2i+1) = cos/parenleft.s1pos∕10000 2i∕dmodel\n/parenright.s1\nPower load forecasting task is a time-dependent forecasting \ntask. However, inputting the global time information split \ninto \"Year, Month, Day, etc.\" as additional feature with \nother variables into the Transformer model has resulted in \ndecrease for prediction accuracy, because too many feature \ninputs will bring more noise to the model. The positional \nembedding of the basic Transformer can only obtain the \nsequential representation between the input sequences but \nfailed to effectively represent the relationship of each point \nin the sequence in the global time. For example, in real-\nworld scenarios, consumers will consume more electricity at \nnight than during the day, and more on weekends than week-\ndays. It is difficult for the basic Transformer model to effec-\ntively utilize the time information in the power load data. To \nbetter learn the time relationship between historical data, we \nproposed a time augmentation layer to enhance the temporal \nrepresentation of the historical input sequence. For each time \nstep of the input sequence, the basic time feature as input \nfor time augmentation layer T ti\n such as “2010/1/1 00:30” \nused for generation of derived features: Year, Y ; Month, M ; \nDay, D ; Time-stamp of the day, divided into 30 min interval \neach, H ; Current day of the week, W ; Holidays represented \nby a binary label L . We convert these discrete temporal fea-\ntures to one-hot encoding and concatenate them to a vector \nT i ∈ ℝn×d t , where d t is total dimension of the one-hot encod-\ning of the temporal features:\n(2)Ti= Concat (one − hot(Y,M ,D,H,W ,L)\nFig. 2  Structure of Time Aug-\nmented Transformer model for \nload forecasting\nFeed Foward\nAdd&Normlaza/g415on\nXt1 Xt2 ... Xtn Xtn\nInput Layer\nN*\nEncoder Decoder\nN*\nLinear\nAdd&Normlaza/g415on\nSelf-A/g425en/g415on\nAdd&Normlaza/g415on\nFeed Foward\nAdd&Normlaza/g415on\nEncoder-Decoder A/g425en/g415on\nAdd&Normlaza/g415on\nMasked Self-A/g425en/g415on\nPosi/g415on-Encoding\nTt1 Tt2 ... Ttn\nXtn+1 Xtn+2 ... Xtn+m\nLinear\nTime \nAugmented \nEncodingOne-Hot \nEmbedding\nLinear\nReLU\nLinear\nAdd&\nNormlaza/g415on\nInternational Journal of Computational Intelligence Systems           (2022) 15:67  \n1 3 Page 5 of 11    67 \nEach time step’s time encoding T i ∈ ℝn×d t is transformed \nto T i ∈ ℝn×dmodel employed two fully connected network and \nReLU activation function:\nWhile W1 , W2 , b1 and b2 are the learnable parameter matrices \nof linear mapping. In addition, to prevent the disappearance \nof gradients caused by the excessive number of layers in \nthe overall model, we use a residual connection and layer \nnormalization which can be expressed as (4):\nThus, we have the final vector X i as the final inputs to \nEncoder and Decoder which contains the original sequence \ninput, absolute position information PE from position \nencoding and time information T i from the time augmenta-\ntion layer:\nEncoder: The encoder is composed of a stack of encoder \nlayers and the number of encoder layers is a free parameter. \nThe vector is fed into the encoder layer after being processed \nby the input layer. There are a multi-head self-attention sub-\nlayer and a fully connected feed-forward sub-layer in each \nencoder layer. As the name implies, self-attention is respon-\nsible for the calculation of the attention of the input sequence \nwithin the encoder. The entered historical sequence uses \nencoder to encode all historical load information and cap-\ntures each of their interdependence, and the context vec -\ntor encoded by encoder will be inputted to the decoder and \nprovide global historical load information for the decoder. \nBesides, to speed up the training and reduce the disappear -\nance of gradients, a residual connection [32] and layer nor -\nmalization [33] was employed for each of the two sub-layers.\nDecoder: The decoder is also consisted of a stack of decoder \nlayers. In the training phase, the input of the encoder is the \nsequence shifted one-position offset to the target output we \npredict and the start token of decoder is the load in last step \nof encoder’s input sequence. In the predicting phase, the input \nof the decoder is just one data which is the load in last step of \nencoder’s input sequence, and predict load in next time step by \nstep. The input of sequence is transformed into a dmodel dimen-\nsional vector representation through input layer and position \nencoding, and then feed to a stack of decoder layer. There are \nthree sub-layers in each of decoder layer: an encoder–decoder \nattention layer, a fully connected feed-forward network layer and \na masked multi-head self-attention layer. For self-attention layer \nin decoder, self-attention is modified to a masked self-attention \nby setting the sequence after the current prediction step to −∞ , \nsince each position can attend to all positions when perform-\ning attention calculations in decoder and it will result in the \n(3)FFN /parenleft.s1Ti\n/parenright.s1= max /parenleft.s10,TiW 1 + b1\n/parenright.s1W 2 + b2\n(4)Ti = LayerNorm/parenleft.s1Ti + FFN /parenleft.s1Ti\n/parenright.s1/parenright.s1\n(5)Xi = Xi + Ti + PE\ndisclosure of future sequence information when decoder makes \nprediction. To train decoder in batches during the training phase, \nwe use an upper triangular matrix as the masking to prevent the \ndecoder from obtaining future information. Encoder–decoder \nattention performs multi-head attention over the input of the \ndecoder and the output of the encoder stack. It converts the vec-\ntor of the encoded historical electrical load feature to generate \nglobal attention vector as the input of the decoder by building \nthe relationship between data from each historical time step and \nevery future time step. Finally, the soft-max layer that is used \nfor classifying in original Transformer is also omitted, we use \na fully connected layer transformed the Y ∈ ℝm×dmodel output \nvector from decoder to Y ∈ ℝm×1 , where m is the number of \nforecasting ahead step and we use Mean Squared Error (MSE) \nloss to measure training loss:\nSelf-Attention: Attention is an indispensable and com-\nplex cognitive function of human beings, which refers to \nthe selective ability of focus on some information while \nignoring others [34]. The attention mechanism draws on the \nhuman brain to improve the ability of the neural network to \nprocess information. When the neural network processes a \nlarge amount of input information, the attention mechanism \nallows the network just to select some key information as \ninputs.\nThe calculation of the attention mechanism can be divided \ninto two steps: the first step is to calculate the attention distri-\nbution on all input sequences, and the second step is to calcu-\nlate the weighted average of the input sequences based on the \nattention distribution [35]. The N group input information is \nrepresented by X = /bracketleft.s1x1 ,…,xN\n/bracketright.s1∈ ℝD×N  , where D-dimension \nvector xn ∈ ℝD ,n ∈ [1, N] . The input information can be rep-\nresented in a query-key-value pair format, for each input xi , first \nmap it linearly to three different spaces to get the query vector \nqi ∈ ℝD k , the key vector ki ∈ ℝD k and the value vector vi ∈ ℝD k . \nFor the entire input sequence X, the linear mapping process can \nbe expressed as (7) (8) (9), while W q ∈ ℝDk×Dx , W k ∈ ℝDk×Dx \nand W v ∈ ℝDv×Dx are the parameter matrices of linear map-\nping. Q = /bracketleft.s1q1 ,…,qN\n/bracketright.s1 , K = /bracketleft.s1k1 ,…,kN\n/bracketright.s1 , V = /bracketleft.s1v1 ,…,vN\n/bracketright.s1 are \nthe matrices composed of query vector, key vector and value \nvector respectively.\nTransformer uses the scaled dot product as the attention \nscoring function to calculate the attention distribution. When \n(6)MSE = 1\nN\nN/uni2211.s1\ni=1\n/parenleft.s1yi − ̂yi\n/parenright.s12\n(7)Q = WqX ∈ ℝDq×N\n(8)K = W kX ∈ ℝDk×N\n(9)V = W vX ∈ ℝDv×N\n International Journal of Computational Intelligence Systems           (2022) 15:67 \n1 3   67  Page 6 of 11\nthe dimension D of the input vector is relatively high, the \nvalue of the dot product model usually has a large variance, \nresulting in a small gradient of the soft-max function. Using \nthe scaled dot product can solve this problem well. The for-\nmula of the dot product model is as follows:\nFor each query vector qn ∈ Q  , the key-value pairs atten-\ntion mechanism of formula (11) is used to obtain the output \nvector:\nwhere n,j∈ [1, N ] is the position of the output and input \nvector sequences, /u1D6FCnj represents the weight of the n-th output \nfocusing on the j-th input. The output vector sequence can \nbe abbreviated as:\nThe self-attention module makes the historical load feature \nsequence and the future load sequence interrelated, so that \nthe embedding representation of the source sequence and the \ntarget sequence will contain more abundant information. The \ninformation input from the attention layer to the subsequent \nFFN also has stronger model representation ability. The self-\nattention mechanism was shown in Fig. 3.\n(10)(x, q) = x⊺q\n√\nD\n(11)\nhn = att/parenleft.s1\n(K ,V ),qn\n/parenright.s1\n=\nN/uni2211.s1\nj=1\n/u1D6FCnjvj\n=\nN/uni2211.s1\nj=1\nsoftmax /parenleft.s1s/parenleft.s1kj,qn\n/parenright.s1/parenright.s1vj\n(12)\nH = softmax\n�\nQK ⊺\n√\nDk\n�\nV\n=\nN�\nn=1\nexp\n�\nQK ⊺\n√\nDk\n�\n∑\njexp\n�\nQK ⊺\n√\nDk\n�V\n4  Experiment\n4.1  Dataset and Preprocessing\nThe electrical load data of New South Wales were publicly \nobtained from the Australian National Electricity Market, \nwhere data points are collected every half hour, 5 years from \n2006 to 2010. Each data point consists of the target value \nelectrical load and other six features including: hours, dry \nbulb temperature, wet bulb temperature, dew point tempera-\nture, humidity and electricity price.\nWe use data from the first 5 years as the training set, the \nfirst 6 months of the last year as the validation set, and the \nlast 6 months as the test set. All of the data was normal-\nized via the zero-mean method. Then a fixed-length sliding \nwindow show in Fig. 1 was applied to construct (X, Y) pairs, \nin which X are previous n-step feature vector including our \ntarget electrical load data and Y are next m-step data as our \nforecast target.\n4.2  Experimental Design\nWe compared our Time Augmented Transformer model \nwith following forecasting models: ① ARIMA; ② SVR; \n③LSTM; ④Bi-LSTM; ⑤CNN-LSTM; ⑥Seq2Seq; ⑦Basic \nTransformer.\nFor all methods, the input history data length for model is \n48 step and the step of predict length is chosen from {1, 2, 24, \n48} that means 30 min, 1 h, 12 h and 1 day. For ARIMA, we \nchoose the parameter as p = 1,d = 2 and q = 1 by analyzing \nthe ACF and PACF diagrams produced from dataset. For SVR \nmodel, we used a multiple regression strategy to use SVR for \nmulti-step prediction. For LSTM, we set a dense connected \nnetwork and a stack of LSTM layers. The data input into \nLSTM-layer for learning historical sequential information, and \nthe final output from the LSTM-layer was feed into dense con-\nnected layer to fit the number of steps for target prediction. For \nseq2seq, we used the Gate Recurrent Unit (GRU) and dense \nconnected network as the basic components. The encoder in \nFig. 3  Self-attention mechanism\nXt1\nK1 V1\nQ1\nQ1K1\nT\nsoftmax\nh1\nXt2\nK2 V2\nQ1K2\nT\nXtn\nKn Vn\nQ1Kn\nT\nĊ\nsoftmax softmax\nInternational Journal of Computational Intelligence Systems           (2022) 15:67  \n1 3 Page 7 of 11    67 \nSeq2seq receive and process historical input data. The results \nof the GRU network in decoder was feed into a fully connected \nfeedforward neural network and then predict backwards step \nby step with autoregressive methods. For LSTM, Bi-LSTM \nand Seq2Seq, the size of hidden state is chosen from {16, 32, \n64, 128, 256} and the number of layers was chosen from {1, \n2, 3, 4}. For the model of CNN-LSTM, we choose one dimen-\nsional convolution and the number of filters is 64 with kernel \nsize 3, and the size of hidden state is 200.\nFor basic Transformer and TAT, the head number of multi-\nhead attention was chosen from {8,16}, the layers of encoder \nand decoder was chosen from {2, 3, 4, 5, 6}, and the dimen-\nsion of multi-head attention’s output was chosen from {16, 32, \n64, 128, 256, 512} respectively. We use grid search to select \noptimal hyper-parameters by observing their performance in \nthe validation set. We set the number of encoder layer to 4, \ndecoder layer to 2, the dimension of model to 64, the number \nof heads to 8, the number of hidden states in FFC layer to \n2048 and the dimension of attention q, k and v to 8. For time \naugmentation layer, we set the hidden state size to 1024. Our \nmodel was optimized with Adam optimizer [36], and we use \n1e−5 as learning rate. For best generalization performance, we \nuse 20 epochs with proper early stopping for all deep learning \nmethods. A mini-batch of size 1024 was used for training. \nBesides, the dropout of 0.2 was applied for our model.\nWe computed Root Mean Squared Error (RMSE), Mean \nAbsolute Error (MAE) and Mean Absolute Percentage Error \n(MAPE) between the actual data and the predicted value to \nevaluate the performance for all the methods. The measures of \ntest error RMSE and MAPE are expressed as follows:\n(13)RMSE =\n/uni221A.t/uni221A.x/uni221A.x/uni221A.s41\nN\nN/uni2211.s1\ni=1\n/parenleft.s1yi − ̂yi\n/parenright.s12\n(14)MAPE = 100%\nN\nN/uni2211.s1\ni=1\n/uni007C.x/uni007C.x/uni007C.x/uni007C.x\nyi − ̂yi\nyi\n/uni007C.x/uni007C.x/uni007C.x/uni007C.x\nAll the experiments were carried out on a personal server \nwith two Nvidia Tesla V100 (16 GB) GPU.\n4.3  Results and Discussion\n4.3.1  Multi‑step Ahead Forecasting for SW Data\nIn our first experiment, we compare different methods for \n30 min (1 step), 1 h (2 steps), 12 h (24 steps) and one \nday (48 steps) ahead forecasting on the electrical dataset \non New South Wales. For all the predictive models, we \nused 24 h of historical data (48 historical steps) as input \nvector. We compared our TAT model’s performance with \nARIMA, LSTM, Bi-LSTM, CNN-LSTM, Seq2Seq and \nbasic Transformer. Table  1 summarizes the MAE, MSE \nand MAPE values for each method for multi-step ahead \nforecasting and our model have the best results in all dif-\nferent time step ahead predictions.\nFigure  4. shows the predictions of the four models at \nprediction steps 1, 12, 24 and 48 ahead in subgraph a, b, \nc and d respectively. It can be seen that the prediction \nresults of each model are both accurate when predicting \n1 step forward. However, our method is more sensitive \nto local changes in the load curve and can more accu-\nrately predict subtle changes in the electrical load over a \nshorter period of time, as show in local zoomed images \nin subgraph Fig.  4 (a). As the prediction step increases, \nthe deviation of the prediction curve of ARIMA, machine \nlearning and other deep learning methods from the real \ncurve gradually increases, while the prediction result of \nour method is closer to the actual value curve than other \nmodels, especially at the bottom and top of the power load \ncurve in subgraph Fig.  4 (b), (c), (d), which demonstrates \nthat our model shows significantly better results than other \nforecasting models.\nTable 1  Comparison of different models for forecasting multi-step electrical load\nItalic values are the best results on traditional and deep neural methods\nModel 30 min 1 h 12 h 1 day\nMAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE\nARIMA 118.39 157.51 1.17% 206.42 306.99 2.08% 734.06 963.28 8.71% 800.57 1015.36 9.43%\nSVR 86.55 109.71 1.01% 197.91 238.67 2.37% 374.90 439.61 4.51% 463.41 556.72 5.55%\nLSTM 93.23 116.46 0.938% 358.80 475.64 3.98% 375.70 493.92 4.33% 498.85 649.25 5.82%\nSeq2Seq 100.40 133.90 0.994% 157.65 223.33 1.59% 310.33 430.05 3.64% 486.88 639.13 5.68%\nBi-LSTM 83.36 106.09 0.858% 117.65 156.66 1.20% 314.21 447.06 3.68% 408.14 562.25 4.67%\nCNN-LSTM 83.20 106.55 0.860% 104.59 137.89 1.07% 301.21 398.55 3.40% 370.51 483.21 4.24%\nTransformer 78.81 99.91 0.744% 84.19 113.38 0.82% 281.10 376.69 3.27% 339.67 449.32 3.89%\nTAT 47.20 62.49 0.465% 65.99 87.59 0.618% 147.55 215.18 1.69% 230.24 325.21 2.62%\n International Journal of Computational Intelligence Systems           (2022) 15:67 \n1 3   67  Page 8 of 11\n4.3.2  Multivariate and Univariate Variable Input \nwith Multi‑step Ahead Forecasting\nOur model can be used for both univariate and multivariate \ninput predictions by adjusting the input layer of encoder. \nTo solve the prediction problems for univariate inputs, \nwe only use load consumption as a single variable time \nseries and construct supervised learning pairs by sliding \nwindows to use the historical load to predict the subse-\nquent multi-step load. For multivariate variable input, we \nuse electrical load and other six features including hours, \ndry bulb temperature, wet bulb temperature, dew point \ntemperature, humidity and electricity price as the input \ndata. In this section, we have validated the validity of the \nunivariate model using only historical power load data \nas a single variable serial data input into our TAT model. \nSame as aforementioned TAT of multivariate, using his-\ntorical univariate data as input, we made predictions for \n30 min, 1 h, 12 h, and 1 day ahead, respectively, and com -\npared them with the multivariate-TAT model. As shown in \nTable 2, we can see that multivariable inputs produce bet -\nter predictions than univariate inputs. Suggesting that the \nchange of electric load is related to many factors, not only \ndepends on its own features, but also is directly interfered \nby random factors. It is shown that more prior knowledge \nis beneficial to the improvement of our model’s prediction \naccuracy because the multivariate variable input brings \nFig. 4  Result in forecast testing of different-step forecasting for 30 min, 1 h, 12 h, and 1 day ahead respectively using 4 models\nTable 2  Comparison of performance for multi-step forecasting under the univariate and multivariate variable input\nItalic values are the best results on traditional and deep neural methods\nMethod 30 min 1 h 12 h 1 day\nMAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE MAE RMSE MAPE\nunivariate 103.06 136.99 1.04% 146.47 195.13 1.48% 200.00 286.55 2.14% 249.91 348.79 2.83%\nmultivariate 47.20 62.49 0.465% 65.99 87.59 0.618% 147.55 215.18 1.69% 230.24 325.21 2.62%\nInternational Journal of Computational Intelligence Systems           (2022) 15:67  \n1 3 Page 9 of 11    67 \nmore dependent features to the model and self-attention \nmechanism have sufficient capacity to capture complex \ndynamical patterns in the multivariate variable data.\nBesides, we ranked the importance of the multivariate \nfeatures to explore how each variable feature contributes to \nthe prediction results. By successively removing the vari-\nable input to the model, the decline of model’s accuracy \nreflects the contribution of the variable to the prediction \nresult, and the results are shown in Fig.  5 The dry bulb \ntemperature has the greatest effect on the predicted results, \nand the electricity price has the weakest effect in all the \nvariables.\n4.3.3  Comparison of Different Input Length for an Hour \nAhead Forecasting\nDuring the experiment, we found that the input time step of \nhistorical data has great influence on the prediction results of \nthe model, so we tried to use historical sequences of different \nlengths as input to predict the power load in the next hour, \nthe comparison graph is shown in Fig. 6. Prediction errors of \nmodels except LSTM are decreasing with the length of input \nhistorical data, because longer historical data may contain \nmore dependencies and provides more historical information \nfor the model. But for LSTM, further increasing causes the \nRMSE to drop since it cannot effectively capture the depend-\nency and regularity of the history records in the case of \nlonger input sequence. Both Bi-LSTM and CNN-LSTM can \nimprove this defect. In the prediction of 1 h (2 steps) ahead, \nour TAT model always performs the best regardless of the \nlength of historical information as input. Our experiments \nshow that the model occupies preferable prediction perfor -\nmance and practicability that can use less data to capture the \nload features and make more accurate predictions. Only 6 \nsteps of historical data needed to achieve the same prediction \neffect as the basic Transformer with 48 steps input, which \nmeans less memory occupancy and faster calculation speed.\n5  Conclusion\nIn this paper, we developed a short-term forecast model \nTAT for electrical load forecasting, which was tested in the \ndata of electrical load in New South Wales. Compared with \nother six methods (ARIMA, LSTM, Bi-LSTM, Seq2Seq, \nCNN-LSTM and basic Transformer), our model has the best \n10.86658\n12.1301\n20.9333\n21.56489\n23.97608\n28.5234\n01 02 03 0\nMSE\ndew point temp\nhours\nwet bulb temp\ndry bulb temp\nhumidity\nprice\nFig. 5  The rank of the importance of the multivariate features. The \nvalue of the horizontal axis represents the decrease of the accuracy of \nthe model after this variable was removed. The greater the decrease, \nthe greater the contribution of which variable for the model predic-\ntion\n05 10 15 20 25\n0\n200\n400\n600\nESMR\nInput Length (step/0.5hour)\nLSTM\nSeq2Seq\nBi-LSTM\nCNN-LSTM\nTransformer\nTAT\n05 10 15 20 25\n0.00\n0.02\n0.04\n0.06\nMAPE\nInput Length (step/0.5hour)\nLSTM\nSeq2Seq\nBi-LSTM\nCNN-LSTM\nTransformer\nTAT\nFig. 6  The RMSE and MAPE of different input for an hour ahead forecast\n International Journal of Computational Intelligence Systems           (2022) 15:67 \n1 3   67  Page 10 of 11\nforecast performance. Moreover, we compare the model with \nunivariate variable input using only historical power load \ndata as a single variable serial to the previous multivariate \nTAT, and multivariable inputs produce better predictions \nthan univariate, suggesting that the multivariate input brings \nmore dependent features to the model and our approach can \nbetter learn the dynamic dependencies in the complex input \nsequence. In addition, we compare the predictive ability of \nthe model with different input steps, it was found that our \napproach can rely on less historical data to obtain better \nprediction results than other models. In summary, it can be \nconcluded that our model is a satisfactory approach in terms \nof electrical load forecasting. Finally, although our approach \nhas been very effective in short-term electrical load forecast-\ning, with the increase of the prediction step, the prediction \naccuracy gradually decreases. In future work, we hope to \nfurther improve the performance of the model from the per-\nspective of external factors of power load.\nAcknowledgements The authors would like to thank the anonymous \nreviewers for their valuable comments and suggestions to improve the \nquality of the article.\nAuthors’ Contributions CW identified this problem and designed the \nmethod. GZ performed the model building and writing the manuscript. \nCJ and YW performed the experiments. All authors read and approved \nthe final manuscript.\nFunding This study was supported by Scientific research project of \nBeijing Municipal Education Commission—General Project of sci-\nence and technology plan (Grant Z20018) and Basic scientific research \nbusiness fee project of municipal colleges and Universities—special \nsubsidy for youth scientific research and innovation (Grantx 18258).\nData Availability The datasets used during the current study are avail-\nable from the corresponding author on reasonable request.\nDeclarations \nConflict of Interest The authors declare no conflict of interest.\nEthics Approval and Consent to Participate Not applicable.\nConsent for Publication The authors consent to this work for publica-\ntion.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Reddy, M., Vishali, N.: Load forecasting using linear regression \nanalysis in time series model for RGUKT, R.K. Valley campus HT \nfeeder. Int J Eng Sci. 6, 624–625 (2017). https:// doi. org/ 10. 17577/ \nijert v6is0 50443\n 2. Jahan, I.S., Snasel, V., Misak, S.: Intelligent systems for power \nload forecasting: a study review. Energies 13, 6105 (2020). https:// \ndoi. org/ 10. 3390/ en132 26105\n 3. Memarzadeh, G., Keynia, F.: Short-term electricity load and price \nforecasting by a new optimal LSTM-NN based prediction algo-\nrithm. Elector Pow Syst Res. 192, 106995 (2021). https:// doi. org/ \n10. 1016/j. epsr. 2020. 106995\n 4. Nti, I.K., Teimeh, M., Nyarko-Boateng, O., Adekoya, A.F.: \nElectricity load forecasting: a systematic review. J Electr \nSyst Inf Technol 7 , 1–19 (2020). https:// doi. org/ 10. 1186/  \ns43067- 020- 00021-8\n 5. Nespoli, A., Ogliari, E., Pretto, S., Gavazzeni, M., Vigani, S., \nPaccanelli, F.: Electrical load forecast by means of LSTM: the \nimpact of data quality. Forecasting 3, 91–101 (2021). https:// doi. \norg/ 10. 3390/ forec ast30 10006\n 6. Sun, G., Jiang, C., Wang, X., Yang, X.: Short-term building load \nforecast based on a data-mining feature selection and LSTM-RNN \nmethod. Ieej T Electr Electr 15, 1002–1010 (2020). https:// doi.  \norg/ 10. 1002/ tee. 23144\n 7. Malek, Y.N., Najib, M., Bakhouya, M., Essaaidi, M.: Multivariate \ndeep learning approach for electric vehicle speed forecasting. Big \nData Min Anal 4, 56–64 (2021). https:// doi. org/ 10. 26599/ bdma. \n2020. 90200 27\n 8. Mamun, A.A., Sohel, M., Mohammad, N., Sunny, M., Hossain, E.: \nA comprehensive review of the load forecasting techniques using \nsingle and hybrid predictive models. IEEE Access 8 , 134911–\n134939 (2020). https:// doi. org/ 10. 1109/ ACCESS. 2020. 30107 02\n 9. Chen, J., Wu, Y., Lin, Z., Zhao, L., Deng, X.: Review of Load \nForecasting Based on Artificial Intelligence Models. 2021 6th \nAsia Conference on Power and Electrical Engineering 2021, \n340–344 (2021). https:// doi. org/ 10. 1109/ acpee 51499. 2021. 94369 \n16\n 10. Yang, A., Li, W., Yang, X.: Short-term electricity load forecast-\ning based on feature selection and Least Squares Support Vector \nMachines. Knowl Based Syst 163, 159–173 (2019). https:// doi.  \norg/ 10. 1016/j. knosys. 2018. 08. 027\n 11. Lu, J.C., Niu, D.X., Jia, Z.Y.: A study of short-term load forecast-\ning based on ARIMA-ANN. Int Conf Mach Learn Cybernet 5, \n3183–3187 (2005). https:// doi. org/ 10. 1109/ icmlc. 2004. 13785 83\n 12. Zhou, D., Chen, S., Dong, S.: Network traffic prediction based on \nARIMA model. arXiv preprint arXiv: 1302. 6324 (2013). https:// \ndoi. org/ 10. 48550/ arXiv. 1302. 6324\n 13. Yang, J.F., Cheng, H.Z.: Application of SVM to power system \nshort-term load forecast. Electric Power Automat Equip 24(2), \n30–32 (2004)\n 14. Huo,J., Shi,T.T., Chang, J.: Comparison of Random Forest and \nSVM for Electrical Short-term Load Forecast with Different Data \nSources. In: 2016 IEEE 7th International Conference on Soft-\nware Engineering and Service Science. 2016, 1077–1080 (2016). \nhttps:// doi. org/ 10. 1109/ ICSESS. 2016. 78832 52\n 15. Peng, L.I., Shuai, H.E., Han, P., Zheng, M., Huang, M., Sun, J.: \nShort-term load forecasting of smart grid based on long-short-\nterm memory recurrent neural networks in condition of real-time \nelectricity price. Power Syst Technol 42(12), 4045–4052 (2018). \nhttps:// doi. org/ 10. 13335/j. 1000- 3673. pst. 2018. 0433\n 16. Gong, G., An, X., Mahato, N.K., Sun, S., Wen, Y.: Research on \nShort-term load prediction based on Seq2seq model. Energies 12, \n3199 (2019). https:// doi. org/ 10. 3390/ en121 63199\nInternational Journal of Computational Intelligence Systems           (2022) 15:67  \n1 3 Page 11 of 11    67 \n 17. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., \nGomez, A.N.: Attention Is All You Need. arXiv preprint arXiv:  \n1706. 03762 (2017). https:// arxiv. org/ abs/ 1706. 03762\n 18. Pooniwala, N., Sutar, R.: Forecasting Short-Term Electric Load \nwith a Hybrid of ARIMA Model and LSTM Network. In: 2021 \nInternational Conference on Computer Communication and Infor-\nmatics (ICCCI). 2021, 1–6 (2021). https:// doi. org/ 10. 1109/ ICCCI \n50826. 2021. 94024 61\n 19. Marrero, L., García-Santander, L., Carrizo, D., Ulloa, F.: An \napplication of load forecasting based on ARIMA models and par-\nticle swarm optimization. In: 2019 11th International Symposium \non Advanced Topics in Electrical Engineering (ATEE). 2019, 1–6 \n(2019). https:// doi. org/ 10. 1109/ atee. 2019. 87248 91\n 20. Wei, L., Zhang, Z.G.: Based on time sequence of ARIMA model \nin the application of short-term electricity load forecasting. Int \nConf Res Challenge Comp Sci 2009, 11–14 (2009). https:// doi.  \norg/ 10. 1109/ ICRCSS. 2009. 12\n 21. Yi, L., Niu, D., Ye, M., Hong, W.C.: Short-term load forecast-\ning based on wavelet transform and least squares support vector \nmachine optimized by improved cuckoo search. Energies 9 , 827 \n(2016). https:// doi. org/ 10. 3390/ en910 0827\n 22. Lahouar, A., Slama, J.: Random forests model for one day ahead \nload forecasting. Renew Energ Congress 2015, 1–6 (2015). https:// \ndoi. org/ 10. 1109/ irec. 2015. 71109 75\n 23. Zhang, N., Li, Z., Zou, X., Quiring, S.M.: Comparison of three \nshort-term load forecast models in Southern California. Energy \n189, 116358 (2019). https:// doi. org/ 10. 1016/j. energy. 2019. 116358\n 24. Sun, Q.Y., Yang, L.X., Zhang, H.G.: Smart energy — Applica-\ntions and prospects of artificial intelligence technology in power \nsystem. Kongzhi yu Juece/Control Decis 33 , 938–949 (2018). \nhttps:// doi. org/ 10. 13195/j. kzyjc. 2017. 1632\n 25. Dong, S., Wang, P., Abbas, K.: A survey on deep learning and its \napplications. Comput Sci Rev 40(1), 100379 (2021). https:// doi.  \norg/ 10. 1016/j. cosrev. 2021. 100379\n 26. Wang, H., Lei, Z., Zhang, X., Zhou, B., Peng, J.: A review of \ndeep learning for renewable energy forecasting. Energ Convers \nManagf. 198, 111799 (2019). https:// doi. org/ 10. 1016/j. encon man. \n2019. 111799\n 27. Mamun, A., Sohel, M., Mohammad, N., Sunny, M.S.H., Dipta, \nD.R., Hossain, E.: A comprehensive review of the load fore-\ncasting techniques using single and hybrid predictive models. \nIEEE Access 8, 134911–134939 (2020). https:// doi. org/ 10. 1109/ \nACCESS. 2020. 30107 02\n 28. Tokgöz, A., Ünal, G.: A RNN based time series approach for \nforecasting turkish electricity load. 2018 26th Signal Processing \nand Communications Applications Conference (SIU). 2018, 1–4 \n(2018): IEEE https:// doi. org/ 10. 1109/ siu. 2018. 84043 13\n 29. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neu-\nral Comput 9(8), 1735–1780 (1997). https:// doi. org/ 10. 1162/ neco. \n1997.9. 8. 1735\n 30. Wang, R., Zhao, J.: Deep learning-based short-term load forecast-\ning for transformers in distribution grid. Int J Comput Int Sys 14, \n1–10 (2021). https:// doi. org/ 10. 2991/ ijcis.d. 201027. 001\n 31. Wu, N., Green, B., Xue, B., O'Banion, S.: Deep Transformer \nModels for Time Series Forecasting: The Influenza Prevalence \nCase. arXiv preprint arXiv: 2001.08317 (2020). https:// doi. org/  \n10. 48550/ arXiv. 2001. 08317\n 32. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for \nImage Recognition. 2016 IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR). 2016, 770–780 (2016). https:// \ndoi. org/ 10. 1109/ CVPR. 2016. 90\n 33. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer Normalization. arXiv \npreprint arXiv: 1607. 06450. (2016). https:// doi. org/ 10. 48550/ \narXiv. 1607. 06450\n 34. Luong, M.T., Pham, H., Manning, C.D.: Effective Approaches to \nAttention-based Neural Machine Translation. Comput Sci. (2015). \nhttps:// doi. org/ 10. 18653/ v1/ D15- 1166\n 35. Bahdanau, D., Cho, K., Bengio, Y.: Neural Machine Translation \nby Jointly Learning to Align and Translate. arXiv preprint arXiv: \n1409. 0473 (2014). https:// doi. org/ 10. 48550/ arXiv. 1409. 0473\n 36. Kingma, D., Ba, J.: Adam: A Method for Stochastic Optimization. \narXiv preprint arXiv: 1412. 6980 (2014). https:// doi. org/ 10. 48550/ \narXiv. 1412. 6980\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}