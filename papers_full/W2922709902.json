{
  "title": "Pre-trained language model representations for language generation",
  "url": "https://openalex.org/W2922709902",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2768388130",
      "name": "Sergey Edunov",
      "affiliations": [
        "Meta (United States)",
        "Menlo School"
      ]
    },
    {
      "id": "https://openalex.org/A2893542027",
      "name": "Alexei Baevski",
      "affiliations": [
        "Meta (United States)",
        "Menlo School"
      ]
    },
    {
      "id": "https://openalex.org/A2139710560",
      "name": "Michael Auli",
      "affiliations": [
        "Meta (United States)",
        "Menlo School"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2920812691",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4297798436",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W104184427",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2963088995",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2962805889",
    "https://openalex.org/W2970119519"
  ],
  "abstract": "Sergey Edunov, Alexei Baevski, Michael Auli. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.",
  "full_text": "Proceedings of NAACL-HLT 2019, pages 4052–4059\nMinneapolis, Minnesota, June 2 - June 7, 2019.c⃝2019 Association for Computational Linguistics\n4052\nPre-trained Language Model Representations for Language Generation\nSergey Edunov∗, Alexei Baevski∗, Michael Auli\nFacebook AI Research\nMenlo Park, CA\n{edunov,abaevski,michaelauli}@fb.com\nAbstract\nPre-trained language model representations\nhave been successful in a wide range of lan-\nguage understanding tasks. In this paper, we\nexamine different strategies to integrate pre-\ntrained representations into sequence to se-\nquence models and apply it to neural ma-\nchine translation and abstractive summariza-\ntion. We ﬁnd that pre-trained representa-\ntions are most effective when added to the en-\ncoder network which slows inference by only\n14%. Our experiments in machine translation\nshow gains of up to 5.3 BLEU in a simulated\nresource-poor setup. While returns diminish\nwith more labeled data, we still observe im-\nprovements when millions of sentence-pairs\nare available. Finally, on abstractive summa-\nrization we achieve a new state of the art on\nthe full text version of CNN-DailyMail. 1\n1 Introduction\nPre-training of language models has been shown\nto provide large improvements for a range of lan-\nguage understanding tasks (Peters et al., 2018;\nRadford et al., 2018; Phang et al., 2018; Devlin\net al., 2018). The key idea is to train a large gen-\nerative model on vast corpora and use the result-\ning representations on tasks for which only limited\namounts of labeled data is available. Pre-training\nof sequence to sequence models has been previ-\nously investigated for text classiﬁcation (Dai and\nLe, 2015) but not for text generation. In neural ma-\nchine translation, there has been work on transfer-\nring representations from high-resource language\npairs to low-resource settings (Zoph et al., 2016).\nIn this paper, we apply pre-trained representa-\ntions from language models to language genera-\n∗Equal contribution.\n1Code and pre-trained models are available at\nhttps://github.com/pytorch/fairseq/tree/\nbi_trans_lm/examples/pretraining\ntion tasks that can be modeled by sequence to se-\nquence architectures. Previous work on integrat-\ning language models with sequence to sequence\nmodels focused on the decoder network and added\nlanguage model representations right before the\noutput of the decoder (Gulcehre et al., 2015). We\nextend their study by investigating several other\nstrategies such as inputting ELMo-style represen-\ntations (Peters et al., 2018) or ﬁne-tuning the lan-\nguage model (§2).\nOur experiments rely on strong transformer-\nbased language models trained on up to six bil-\nlion tokens ( §3). We present a detailed study of\nvarious strategies in different simulated labeled\ntraining data scenarios and observe the largest im-\nprovements in low-resource settings but gains of\nover 1 BLEU are still possible when ﬁve million\nsentence-pairs are available. The most successful\nstrategy to integrate pre-trained representations is\nas input to the encoder network (§4).\n2 Strategies to add representations\nWe consider augmenting a standard sequence to\nsequence model with pre-trained representations\nfollowing an ELMo-style regime (§2.1) as well as\nby ﬁne-tuning the language model (§2.2).\n2.1 ELMo augmentation\nThe ELMo approach of Peters et al. (2018) forms\ncontextualized word embeddings based on lan-\nguage model representations without adjusting\nthe actual language model parameters. Speciﬁ-\ncally, the ELMo module contains a set of parame-\nters λ1 ...λ L,γ to form a linear combination of\nthe L layers of the language model: ELMo =\nγ∑L\ni=0\n1\nZ exp(λi)hk where γ is a learned scalar,\nZ is a constant to normalize the exp(λi) to sum\nto one and hk is the output of the k-th language\nmodel layer; the module also considers the input\nword embeddings of the language model. We also\n4053\napply layer normalization (Ba et al., 2016) to each\nhk before computing ELMo vectors.\nWe experiment with an ELMo module to input\ncontextualized embeddings either to the encoder\n(SRC -ELMO ) or the decoder ( TGT-ELMO ). This\nprovides word representations speciﬁc to the cur-\nrent input sentence and these representations have\nbeen trained on much more data than is available\nfor the text generation task.\n2.2 Fine-tuning approach\nFine-tuning the pre-trained representations adjusts\nthe language model parameters by the learning\nsignal of the end-task (Radford et al., 2018; De-\nvlin et al., 2018). We replace learned input word\nembeddings in the encoder network with the out-\nput of the language model (SRC -FT). Speciﬁcally,\nwe use the language model representation of the\nlayer before the softmax and feed it to the encoder.\nWe also add dropout to the language model out-\nput. Tuning separate learning rates for the lan-\nguage model and the sequence to sequence model\nmay lead to better performance but we leave this\nto future work. However, we do tune the number\nof encoder blocks N as we found this important to\nobtain good accuracy for this setting. We apply the\nsame strategy to the decoder: we input language\nmodel representations to the decoder network and\nﬁne-tune the language model when training the se-\nquence to sequence model (TGT-FT).\n3 Experimental setup\n3.1 Datasets\nPre-training. We train language models on two\nlanguages: One model is estimated on the Ger-\nman newscrawl distributed by WMT’18 compris-\ning 260M sentences or 6B tokens. Another model\nis trained on the English newscrawl data compris-\ning 193M sentences or 5B tokens. We learn a joint\nByte-Pair-Encoding (BPE; Sennrich et al., 2016)\nvocabulary of 37K types on the German and En-\nglish newscrawl and train the language models\nwith this vocabulary.\nMachine translation. We consider two bench-\nmarks: Most experiments are run on the WMT’18\nEnglish-German (en-de) news translation task and\nwe validate our ﬁndings on the WMT’18 English-\nTurkish (en-tr) news task. For WMT’18 English-\nGerman, the training corpus consists of all avail-\nable bitext excluding the ParaCrawl corpus and we\nremove sentences longer than 250 tokens as well\nas sentence-pairs with a source/target length ra-\ntio exceeding 1.5. This results in 5.18M sentence\npairs. We tokenize all data with the Moses tok-\nenizer (Koehn et al., 2007) and apply the BPE vo-\ncabulary learned on the monolingual corpora.\nFor WMT’18 English-Turkish, we use all of the\navailable bitext comprising 208K sentence-pairs\nwithout any ﬁltering. We develop on newstest2017\nand test on newstest2018. For en-tr we only exper-\niment with adding representations to the encoder\nand therefore apply the language model vocabu-\nlary to the source side. For the target vocabulary\nwe learn a BPE code with 32K merge operations\non the Turkish side of the bitext. Both datasets are\nevaluated in terms of case-sensitive de-tokenized\nBLEU (Papineni et al., 2002; Post, 2018).2\nSummarization. We consider the CNN-\nDailyMail abstractive document summarization\ntask comprising over 280K news articles paired\nwith multi-sentence summaries. CNN-DailyMail\nis a widely used dataset for abstractive text sum-\nmarization. Following (See et al., 2017), we report\nresults on the non-anonymized version of CNN-\nDailyMail rather than the entity-anonymized\nversion (Hermann et al., 2015; Nallapati et al.,\n2016) because the language model was trained on\nfull text. Articles are truncated to 400 tokens (See\net al., 2017) and we use a BPE vocabulary of 32K\ntypes (Fan et al., 2017). We evaluate in terms of\nF1-Rouge, that is Rouge-1, Rouge-2 and Rouge-L\n(Lin, 2004).3\n3.2 Language model pre-training\nWe consider two types of architectures: a bi-\ndirectional language model to augment the se-\nquence to sequence encoder and a uni-directional\nmodel to augment the decoder. Both use self-\nattention (Vaswani et al., 2017) and the uni-\ndirectional model contains N = 12 transformer\nblocks, followed by a word classiﬁer to predict the\nnext word on the right. The bi-directional model\nsolves a cloze-style token prediction task at train-\ning time (Baevski et al., 2019). The model consists\nof two towers, the forward tower operates left-to-\nright and the tower operating right-to-left as back-\nward tower; each tower contains N = 12 trans-\n2sacreBLEU signatures: BLEU+case.mixed+lang.en-\n{de,tr}+numrefs.1+smooth.exp+test.wmt18+tok.13a\n+version.1.2.1\n3We use the following parameters for\nROUGE-1.5.5.pl: -m -a -n 2\n4054\n160K 320K 640K 1280K 2560K 5186K\n−1\n0\n1\n2\n3\n4\n5\n6\nBitext tokens\nBLEU delta wrt baseline\nSHARED SRC -ELMO SRC -FT\nTGT-ELMO TGT-FT SRC -ELMO +SHDEMB\nFigure 1: BLEU difference to a bitext-only baseline when adding pre-trained language model representations\nto a neural machine translation model in different simulated bitext settings. Results are based on averaging\nnewstest2012-2017 of WMT English-German translation.\nformer blocks. The forward and backward repre-\nsentations are combined via a self-attention mod-\nule and the output of this module is used to predict\nthe token at positioni. The model has access to the\nentire input surrounding the current target token.\nModels use the standard settings for the Big Trans-\nformer (Vaswani et al., 2017). The bi-directional\nmodel contains 353M parameters and the uni-\ndirectional model 190M parameters. Both models\nwere trained for 1M steps using Nesterov’s accel-\nerated gradient (Sutskever et al., 2013) with mo-\nmentum 0.99 following Baevski and Auli (2018).\nThe learning rate is linearly warmed up from10−7\nto 1 for 16K steps and then annealed using a co-\nsine learning rate schedule with a single phase to\n0.0001 (Loshchilov and Hutter, 2016). We train on\n32 Nvidia V100 SXM2 GPUs and use the NCCL2\nlibrary as well as the torch distributed package for\ninter-GPU communication. Training relies on 16-\nbit ﬂoating point operations (Ott et al., 2018) and\nit took six days for the bi-directional model and\nfour days for the uni-directional model.\n3.3 Sequence to sequence model\nWe use the transformer implementation of the\nfairseq toolkit (Ott et al., 2019). The WMT en-de\nand en-tr experiments are based on the Big Trans-\nformer sequence to sequence architecture with 6\nblocks in the encoder and decoder. For abstractive\nsummarization we use a base transformer model\n(Vaswani et al., 2017). We tune dropout values of\nbetween 0.1 and 0.4 on the validation set. Models\nare optimized with Adam (Kingma and Ba, 2015)\nusing β1 = 0.9, β2 = 0.98, and ϵ = 1e−8 and\nwe use the same learning rate schedule as Vaswani\net al. (2017); we perform 10K-200K depending\non bitext size. All models use label smoothing\nwith a uniform prior distribution over the vocab-\nulary ϵ= 0.1 (Szegedy et al., 2015; Pereyra et al.,\n2017). We run experiments on 8 GPUs and gener-\nate translations with a beam of size 5.\n4 Results\n4.1 Machine translation\nWe ﬁrst present a comparison of the various strate-\ngies in different simulated parallel corpus size set-\ntings. For each experiment, we tune the dropout\napplied to the language model representations,\nand we reduce the number of optimizer steps for\nsmaller bitext setups as models converge faster;\nall other hyper-parameters are equal between se-\ntups. Our baseline is a Big Transformer model\nand we also consider a variant where we share to-\nken embeddings between the encoder and decoder\n(SHARED ; Inan et al., 2016; Press & Wolf, 2016).\nFigure 1 shows results averaged over six test\nsets relative to the baseline which does not share\nsource and target embeddings (Appendix A shows\na detailed breakdown). SHARED performs very\nwell with little labeled data but the gains erode to\npractically zero in large bitext settings. Pre-trained\n4055\n160K 640K 5186K\nbaseline 21.4 33.1 40.1\nSRC -ELMO 26.6 35.6 41.8\nSRC -FT 24.3 34.9 40.8\nTGT-ELMO 21.3 31.9 40.5\nTGT-FT 24.2 31.4 38.8\nSRC -ELMO +SHDEMB 29.0 36.2 41.8\nTable 1: BLEU on newstest2018 of WMT English-\nGerman in three simulated bitext size scenarios.\nnews2017 news2018\nbaseline 9.8 9.5\nSRC -ELMO 12.0 11.3\nSRC -ELMO +SHDEMB 12.9 11.8\nTable 2: WMT English-Turkish translation results in\nterms of BLEU on newstest2017 (valid) and new-\nstest2018 (test) with ELMo inputs to the encoder.\nlanguage model representations are most effective\nin low bitext setups. The best performing strategy\nis ELMo embeddings input to the encoder ( SRC -\nELMO ). This improves the baseline by 3.8 BLEU\nin the 160K bitext setting and it still improves the\n5.2M setting by over 1 BLEU.\nWe further improve SRC -ELMO by sharing\nlearned word representations in the decoder\nby tying input and output embeddings ( SRC -\nELMO +SHDEMB ). This conﬁguration performs\neven better than SRC -ELMO with a gain of 5.3\nBLEU in the 160K setup. Sharing decoder embed-\ndings is equally applicable to SRC -FT. Language\nmodel representations are much less effective in\nthe decoder: TGT-FT improves the 160K bitext\nsetup but yields no improvements thereafter and\nTGT-ELMO performs even worse. We conjecture\nthat pre-trained representations give much easier\nwins in the encoder. Table 1 shows additional re-\nsults on newstest2018.\nPre-trained representations mostly impacts the\ntraining time of the sequence to sequence model\n(see Appendix B): SRC -ELMO slows throughput\nduring training by about 5.3x and SRC -FT is\neven slower because of the need to backpropa-\ngate through the LM for ﬁne-tuning (9.2x). How-\never, inference is only 12-14% slower than the\nbaseline when adding pre-trained embeddings to\nthe encoder ( SRC -ELMO , SRC -FT). This is be-\ncause the LM computation can be paralelized for\nROUGE\n1 2 L\nLead-3 40.34 17.70 36.57\nSee et al. (2017) 39.53 17.28 36.38\nGehrmann et al. (2018) 41.22 18.68 38.34\nbaseline 40.07 17.61 36.78\nSRC -ELMO +SHDEMB 41.56 18.94 38.47\nTable 3: Abstractive summarization results on CNN-\nDailyMail. ELMo inputs achieve a new state of the art.\nall input tokens. Inference is much slower when\nadding representations to the decoder because the\nLM needs to be invoked repeatedly. Our current\nimplementation does not cache LM operations for\nthe previous state and can be made much faster.\nThe baseline uses a BPE vocabulary estimated\non the language model corpora ( §3). Appendix A\nshows that this vocabulary actually leads to sligtly\nbetter performance than a joint BPE code learned\non the bitext as is usual.\nNext, we validate our ﬁndings on the WMT’18\nEnglish-Turkish task for which the bitext is truly\nlimited (208K sentence-pairs). We use the lan-\nguage model vocab for the the English side of the\nbitext and a BPE vocabulary learned on the Turk-\nish side. Table 2 shows that ELMo embeddings for\nthe encoder improve English-Turkish translation.\n4.2 Abstractive summarization\nFollowing See et al. (2017), we experiment on\nthe non-anonymized version of CNN-DailyMail.\nWhen generating summaries, we follow standard\npractice of tuning the maximum output length and\ndisallow repeating the same trigram (Paulus et al.,\n2017; Fan et al., 2017). For this task we train\nlanguage model representations on the combina-\ntion of newscrawl and the CNN-DailyMail train-\ning data. Table 3 shows that pre-trained em-\nbeddings can signiﬁcantly improve on top of a\nstrong baseline transformer. We also compare to\nGehrmann et al. (2018) who use a task-speciﬁc\narchitecture compared to our generic sequence to\nsequence baseline. Pre-trained representations are\ncomplementary to their method.\n5 Conclusion\nWe presented an analysis of different strategies to\nadd pre-trained language model representations to\nsequence to sequence models for neural machine\n4056\ntranslation and abstractive document summariza-\ntion. Adding pre-trained representations is very\neffective for the encoder network and while re-\nturns diminish when more labeled data becomes\navailable, we still observe improvements when\nmillions of examples are available. In future re-\nsearch we will investigate ways to improve the de-\ncoder with pre-trained representations.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. 2016. Layer normalization. arXiv,\nabs/1607.06450.\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\narXiv, abs/1809.10853.\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. arXiv.\nAndrew M. Dai and Quoc V . Le. 2015.\nSemi-supervised sequence learning. arXiv,\nabs/1511.01432.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. CoRR, abs/1810.04805.\nAngela Fan, David Grangier, and Michael Auli. 2017.\nControllable abstractive summarization. arXiv,\nabs/1711.05217.\nSebastian Gehrmann, Yuntian Deng, and Alexander M\nRush. 2018. Bottom-up abstractive summarization.\narXiv, abs/1808.10792.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. arXiv, abs/1503.03535.\nKarl Moritz Hermann, Tom ´aˇs Ko ˇcisk´y, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. In Proc. of NIPS.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. arXiv,\nabs/1611.01462.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nMethod for Stochastic Optimization. In Proc. of\nICLR.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProc. of ACL Demo Session.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Workshop on Text Sum-\nmarization Branches Out.\nIlya Loshchilov and Frank Hutter. 2016. SGDR:\nstochastic gradient descent with restarts. arXiv,\nabs/1608.03983.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre,\nBing Xiang, et al. 2016. Abstractive text summa-\nrization using sequence-to-sequence rnns and be-\nyond. In Proc. of CONLL.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proc. of NAACL\nSystem Demonstrations.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proc. of WMT.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic\nevaluation of machine translation. In Proc. of ACL.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization. arXiv, abs/1705.04304.\nGabriel Pereyra, George Tucker, Jan Chorowski,\nLukasz Kaiser, and Geoffrey E. Hinton. 2017. Reg-\nularizing neural networks by penalizing conﬁdent\noutput distributions. In International Conference on\nLearning Representations (ICLR) Workshop.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proc. of ACL.\nJason Phang, Thibault Fevry, and Samuel R. Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv,\nabs/1811.01088.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. arXiv, abs/1804.08771.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proc. of\nEACL.\nAlec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-training.\nhttps://s3-us-west-2.amazonaws.\ncom/openai-assets/research-covers/\nlanguage-unsupervised/language_\nunderstanding_paper.pdf.\n4057\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proc. of ACL.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proc. of ACL.\nIlya Sutskever, James Martens, George E. Dahl, and\nGeoffrey E. Hinton. 2013. On the importance of ini-\ntialization and momentum in deep learning. In Proc.\nof ICML.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. 2015. Re-\nthinking the Inception Architecture for Computer\nVision. arXiv preprint arXiv:1512.00567.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Proc. of NIPS.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proc. of EMNLP.\n4058\nA Detailed WMT English-German Results\nbitext method 2012 2013 2014 2015 2016 2017 2018 Avg\n160K\nbaseline 13.2 15.7 13.5 15.7 18.6 14.8 21.4 16.1\nSHARED 15.3 18.2 16.7 19.0 21.6 18.2 24.9 19.1\nSHARED +bitext-BPE 15.1 17.9 16.2 18.9 22.0 18.0 25.2 19.0\nSRC -ELMO 16.0 19.4 17.1 19.9 23.0 18.7 26.6 20.1\nSRC -FT 15.3 18.5 16.6 18.9 20.8 17.6 24.3 18.9\nTGT-ELMO 13.3 16.4 14.1 16.2 18.8 14.9 21.3 16.4\nTGT-FT 14.7 17.2 15.8 18.4 21.4 16.9 24.2 18.4\nSRC -ELMO +SHDEMB 17.4 20.8 18.6 21.5 24.9 20.3 29.0 21.8\n320K\nbaseline 17.2 20.4 18.1 21.2 25.0 19.6 28.9 21.5\nSHARED 18.1 21.1 19.1 22.4 26.3 21.2 30.6 22.7\nSHARED +bitext-BPE 17.6 20.6 19.1 22.3 26.1 20.8 29.9 22.3\nsrc-elmo 18.8 22.3 21.1 24.0 27.5 22.2 32.5 24.1\nSRC -FT 19.0 22.5 20.9 23.5 26.9 22.2 32.1 23.9\nTGT-ELMO 16.7 20.7 18.2 20.9 24.1 19.4 28.0 21.1\nTGT-FT 16.1 19.4 17.1 20.0 23.1 18.5 26.3 20.1\nSRC -ELMO +SHDEMB 19.5 22.9 21.2 24.0 27.4 22.4 32.3 24.2\n640K\nbaseline 19.2 22.9 21.2 24.5 27.9 22.4 33.1 24.5\nSHARED 19.9 23.4 22.1 25.1 28.8 23.0 34.1 25.2\nSHARED +bitext-BPE 19.4 22.8 21.7 24.9 28.4 22.9 33.6 24.8\nsrc-elmo 21.0 24.3 23.4 26.5 30.0 24.6 35.6 26.5\nSRC -FT 20.5 24.0 22.9 26.1 29.1 24.4 34.9 26.0\nTGT-ELMO 18.9 22.6 20.8 24.2 27.5 22.3 31.9 24.0\nTGT-FT 18.2 21.8 20.6 23.7 27.0 21.8 31.4 23.5\nSRC -ELMO +SHDEMB 21.2 25.1 23.9 26.7 30.2 24.7 36.2 26.9\n1280K\nbaseline 20.9 24.6 23.6 26.5 30.5 24.7 36.2 26.7\nSHARED 21.1 24.6 24.6 27.6 31.0 25.2 37.3 27.3\nSHARED +bitext-BPE 20.5 24.0 23.9 26.2 30.6 24.8 36.2 26.6\nsrc-elmo 22.1 25.7 25.7 28.5 31.7 26.3 38.2 28.3\nSRC -FT 21.3 25.2 25.3 28.5 31.1 26.2 37.4 27.9\nTGT-ELMO 20.9 24.4 23.6 26.6 30.3 24.9 36.1 26.7\nTGT-FT 20.1 23.7 22.4 25.2 29.1 23.6 34.4 25.5\nSRC -ELMO +SHDEMB 22.3 26.0 26.3 28.9 32.6 26.8 38.6 28.8\n2560K\nbaseline 21.7 25.6 25.4 28.2 32.3 26.2 39.1 28.4\nSHARED 22.2 25.9 25.7 28.3 32.1 26.3 38.9 28.5\nSHARED +bitext-BPE 21.8 25.5 25.5 27.9 32.1 26.0 38.6 28.2\nsrc-elmo 22.9 27.0 27.0 30.0 33.4 28.0 40.0 29.8\nSRC -FT 22.2 26.4 26.3 29.5 32.4 27.3 39.3 29.1\nTGT-ELMO 21.8 25.7 25.8 28.5 32.3 26.6 39.3 28.6\nTGT-FT 21.5 25.3 24.5 27.0 30.2 25.2 36.8 27.2\nSRC -ELMO +SHDEMB 23.1 27.2 27.1 29.7 33.7 27.9 40.0 29.8\n5186K\nbaseline 23.1 26.8 27.7 30.1 33.6 27.9 40.1 29.9\nSHARED 22.6 26.6 27.7 30.5 33.4 28.2 40.2 29.9\nSHARED +bitext-BPE 22.5 26.0 27.0 29.7 33.4 27.7 40.6 29.6\nsrc-elmo 23.7 27.8 28.7 31.1 34.5 29.2 41.8 31.0\nSRC -FT 23.1 27.0 27.8 30.5 33.7 28.3 40.8 30.2\nTGT-ELMO 22.9 26.6 26.9 29.5 33.8 27.7 40.5 29.7\nTGT-FT 22.3 26.1 26.1 28.9 32.5 26.5 38.8 28.7\nSRC -ELMO +SHDEMB 23.4 28.0 28.8 31.2 34.5 28.7 41.8 30.9\nTable 4: BLEU on newstest2012 to newstest2018 of WMT English-German translation in varioius simulated bitext\nsize scenarios (cf. Figure 1).\n4059\nB Training and inference speed\ntrain (tok/sec) inference (tok/sec)\nSHARED 528,802 2,334\nSRC -ELMO 100,636 2,011\nSRC -FT 57,753 2,080\nTGT-ELMO 142,525 259\nTGT-FT 95,313 299\nTable 5: Training and inference speed of models trained on WMT English-German. Training speed based on 32\nV100 GPUs. Inference speed measured on a single V100 and by batching up to 12K source or target tokens.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7038841247558594
    },
    {
      "name": "Computational linguistics",
      "score": 0.5679318904876709
    },
    {
      "name": "Linguistics",
      "score": 0.49298807978630066
    },
    {
      "name": "Language technology",
      "score": 0.47559699416160583
    },
    {
      "name": "Human language",
      "score": 0.4716545641422272
    },
    {
      "name": "Natural language processing",
      "score": 0.46990105509757996
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4628417193889618
    },
    {
      "name": "Language model",
      "score": 0.4372020363807678
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4365486204624176
    },
    {
      "name": "Association (psychology)",
      "score": 0.4205645024776459
    },
    {
      "name": "Natural language",
      "score": 0.29444533586502075
    },
    {
      "name": "Philosophy",
      "score": 0.1306212842464447
    },
    {
      "name": "Comprehension approach",
      "score": 0.11908888816833496
    },
    {
      "name": "Epistemology",
      "score": 0.07145917415618896
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210099336",
      "name": "Menlo School",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210114444",
      "name": "Meta (United States)",
      "country": "US"
    }
  ],
  "cited_by": 123
}