{
    "title": "Compositional Morphology for Word Representations and Language Modelling",
    "url": "https://openalex.org/W1505680913",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287318574",
            "name": "Botha, Jan A.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2900299007",
            "name": "Blunsom, Phil",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2120861206",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W2053306448",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W2129773034",
        "https://openalex.org/W2118090838",
        "https://openalex.org/W1964209958",
        "https://openalex.org/W2251012068",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2131462252",
        "https://openalex.org/W2103078213",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2091812280",
        "https://openalex.org/W2146574666",
        "https://openalex.org/W2100714283",
        "https://openalex.org/W2138204974",
        "https://openalex.org/W2053921957",
        "https://openalex.org/W2054533749",
        "https://openalex.org/W2167419393",
        "https://openalex.org/W2408503330",
        "https://openalex.org/W2155607551",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W2080100102",
        "https://openalex.org/W1633328346",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2125573226",
        "https://openalex.org/W2134800885",
        "https://openalex.org/W2141599568",
        "https://openalex.org/W2056250865",
        "https://openalex.org/W2154391726",
        "https://openalex.org/W2164019165",
        "https://openalex.org/W2040711288",
        "https://openalex.org/W2250379827",
        "https://openalex.org/W2148708890",
        "https://openalex.org/W2102131037",
        "https://openalex.org/W932413789",
        "https://openalex.org/W2143719855",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W1753482797",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W2437005631",
        "https://openalex.org/W645927007"
    ],
    "abstract": "This paper presents a scalable method for integrating compositional morphological representations into a vector-based probabilistic language model. Our approach is evaluated in the context of log-bilinear language models, rendered suitably efficient for implementation inside a machine translation decoder by factoring the vocabulary. We perform both intrinsic and extrinsic evaluations, presenting results on a range of languages which demonstrate that our model learns morphological representations that both perform well on word similarity tasks and lead to substantial reductions in perplexity. When used for translation into morphologically rich languages with large vocabularies, our models obtain improvements of up to 1.2 BLEU points relative to a baseline system using back-off n-gram models.",
    "full_text": "Compositional Morphology for Word Representations and Language Modelling\nJan A. Botha JAN .BOTHA @CS.OX.AC.UK\nPhil Blunsom PHIL .BLUNSOM @CS.OX.AC.UK\nDepartment of Computer Science, University of Oxford, Oxford, OX1 3QD, UK\nAbstract\nThis paper presents a scalable method for inte-\ngrating compositional morphological representa-\ntions into a vector-based probabilistic language\nmodel. Our approach is evaluated in the con-\ntext of log-bilinear language models, rendered\nsuitably efﬁcient for implementation inside a ma-\nchine translation decoder by factoring the vocab-\nulary. We perform both intrinsic and extrinsic\nevaluations, presenting results on a range of lan-\nguages which demonstrate that our model learns\nmorphological representations that both perform\nwell on word similarity tasks and lead to sub-\nstantial reductions in perplexity. When used for\ntranslation into morphologically rich languages\nwith large vocabularies, our models obtain im-\nprovements of up to 1.2 B LEU points relative to\na baseline system using back-offn-gram models.\n1 Introduction\nThe proliferation of word forms in morphologically rich\nlanguages presents challenges to the statistical language\nmodels (LMs) that play a key role in machine translation\nand speech recognition. Conventional back-off n-gram\nLMs (Chen & Goodman, 1998) and the increasingly popu-\nlar vector-based LMs (Bengio et al., 2003; Schwenk et al.,\n2006; Mikolov et al., 2010) use parametrisations that do\nnot explicitly encode morphological regularities among re-\nlated forms, likeabstract, abstraction and abstracted. Such\nmodels suffer from data sparsity arising from morphologi-\ncal processes and lack a coherent method of assigning prob-\nabilities or representations to unseen word forms.\nThis work focuses on continuous space language models\n(CSLMs), an umbrella term for the LMs that represent\nwords with real-valued vectors. Such word representations\nhave been found to capture some morphological regular-\nity (Mikolov et al., 2013b), but we contend that there is\na case for building a priori morphological awareness into\nProceedings of the 31 st International Conference on Machine\nLearning, Beijing, China, 2014. JMLR: W&CP volume 32.\nCopyright 2014 by the author(s).\nthe language models’ inductive bias. Conversely, composi-\ntional vector-space modelling has recently been applied to\nmorphology to good effect (Lazaridou et al., 2013; Luong\net al., 2013), but lacked the probabilistic basis necessary for\nuse with a machine translation decoder.\nThe method we propose strikes a balance between proba-\nbilistic language modelling and morphology-based repre-\nsentation learning. Word vectors are composed as a linear\nfunction of arbitrary sub-elements of the word, e.g. surface\nform, stem, afﬁxes, or other latent information. The effect\nis to tie together the representations of morphologically re-\nlated words, directly combating data sparsity. This is exe-\ncuted in the context of a log-bilinear (LBL) LM (Mnih &\nHinton, 2007), which is sped up sufﬁciently by the use of\nword classing so that we can integrate the model into an\nopen source machine translation decoder 1 and evaluate its\nimpact on translation into 6 languages, including the mor-\nphologically complex Czech, German and Russian.\nIn word similarity rating tasks, our morpheme vectors help\nimprove correlation with human ratings in multiple lan-\nguages. Fine-grained analysis is used to determine the ori-\ngin of our perplexity reductions, while scaling experiments\ndemonstrate tractability on vocabularies of 900k types us-\ning 100m+ tokens.\n2 Additive Word Representations\nA generic CSLM associates with each word type v in the\nvocabulary V a d-dimensional feature vector rv ∈ Rd.\nRegularities among words are captured in an opaque way\nthrough the interaction of these feature values and a set of\ntransformation weights. This leverages linguistic intuitions\nonly in an extremely rudimentary way, in contrast to hand-\nengineered linguistic features that target very speciﬁc phe-\nnomena, as often used in supervised-learning settings.\nWe seek a compromise that retains the unsupervised na-\nture of CSLM feature vectors, but also incorporatesa priori\nlinguistic knowledge in a ﬂexible and efﬁcient manner. In\nparticular, morphologically related words should share sta-\ntistical strength in spite of differences in surface form.\n1Our source code for language model training and integration\ninto cdec is available from http://bothameister.github.io\narXiv:1405.4273v1  [cs.CL]  16 May 2014\nCompositional Morphology for Word Representations and Language Modelling\nTo achieve this, we deﬁne a mapping µ : V ↦→F+ of a\nsurface word into a variable-length sequence offactors, i.e.\nµ(v) = ( f1,...,f K), where v ∈ Vand fi ∈ F. Each\nfactor f has an associated factor feature vector rf ∈Rd.\nWe thereby factorise a word into its surface morphemes,\nalthough the approach could also incorporate other infor-\nmation, e.g. lemma, part of speech.\nThe vector representation ˜rv of a word v is computed as\na function ωµ(v) of its factor vectors. We use addition as\ncomposition function: ˜rv = ωµ(v) = ∑\nf∈µ(v) rf. The\nvectors of morphologically related words become linked\nthrough shared factor vectors (notation: − −− →word, −−−→factor),\n− −−−−−−−− →imperfection = − →im + −−−−→perfect + −→ion− −−−−− →perfectly = −−−−→perfect + − →ly .\nFurthermore, representations for out-of-vocabulary (OOV)\nwords can be constructed using their available morpheme\nvectors.\nWe include the surface form of a word as a factor it-\nself. This accounts for noncompositional constructions\n(−−−−−−−→greenhouse = −−−−−−−→greenhouse + −−−→green + −−−→house), and makes\nthe approach more robust to noisy morphological segmen-\ntation. This strategy also overcomes the order-invariance\nof additive composition (−−−−−−→hangover ̸= −−−−−−→overhang).\nThe number of factors per word is free to vary over the vo-\ncabulary, making the approach applicable across the spec-\ntrum of more fusional languages (e.g. Czech, Russian) to\nmore agglutinative languages (e.g. Turkish). This is in con-\ntrast to factored language models (Alexandrescu & Kirch-\nhoff, 2006), which assume a ﬁxed number of factors per\nword. Their method of concatenating factor vectors to ob-\ntain a single representation vector for a word can be seen\nas enforcing a partition on the feature space. Our method\nof addition avoids such a partitioning and better reﬂects the\nabsence of a strong intuition about what an appropriate par-\ntitioning might be. A limitation of our method compared to\ntheirs is that the deterministic mappingµcurrently enforces\na single factorisation per word type, which sacriﬁces infor-\nmation obtainable from context-disambiguated morpholog-\nical analyses.\nOur additive composition function can be regarded as an in-\nstantiation of the weighted addition strategy that performed\nwell in a distributional compositional approach to deriva-\ntional morphology (Lazaridou et al., 2013). Unlike the\nrecursive neural-network method of Luong et al. (2013),\nwe do not impose a single tree structure over a word,\nwhich would ignore the ambiguity inherent in words like\nun[[lock]able] vs. [un[lock]]able. In contrast to these two\nprevious approaches to morphological modelling, our ad-\nditive representations are readily implementable in a prob-\nabilistic language model suitable for use in a decoder.\n3 Log-Bilinear Language Models\nLog-bilinear (LBL) models (Mnih & Hinton, 2007) are an\ninstance of CSLMs that make the same Markov assump-\ntion as n-gram language models. The probability of a sen-\ntence w is decomposed over its words, each conditioned\non the n–1 preceding words: P(w) ≈∏\niP\n(\nwi|wi−1\ni−n+1\n)\n.\nThese distributions are modelled by a smooth scoring func-\ntion ν(·) over vector representations of words. In contrast,\ndiscrete n-gram models are estimated by smoothing and\nbacking off over empirical distributions (Kneser & Ney,\n1995; Chen & Goodman, 1998).\nThe LBL predicts the vector p for the next word as a func-\ntion of the context vectorsqj ∈Rd of the preceding words,\np =\nn−1∑\nj=1\nqjCj, (1)\nwhere Cj ∈Rd×d are position-speciﬁc transformations.\nν(w) expresses how well the observed wordwﬁts that pre-\ndiction and is deﬁned as ν(w) = p ·rw + bw, where bw is\na bias term encoding the prior probability of a word type.\nSoftmax then yields the word probability as\nP\n(\nwi|wi−1\ni−n+1\n)\n= exp (ν(wi))∑\nv∈Vexp (ν(v)). (2)\nThis model is subsequently denoted as LBL with parame-\nters ΘLBL = (Cj,Q,R, b), where Q,R ∈R|V|×d contain\nthe word representation vectors as rows, and b ∈R|V|. Q\nand Rimply that separate representations are used for con-\nditioning and output.\n3.1 Additive Log-Bilinear Model\nWe introduce a variant of the LBL that makes use\nof additive representations ( §2) by associating the com-\nposed word vectors ˜r and ˜qj with the target and con-\ntext words, respectively. The representation matrices\nQ(f),R(f) ∈R|F|×d thus contain a vector for each factor\ntype. This model is designated LBL++ and has parameters\nΘLBL++ =\n(\nCj,Q(f),R(f),b\n)\n. Words sharing factors are\ntied together, which is expected to improve performance\non rare word forms.\nRepresenting the mapping µwith a sparse transformation\nmatrix M ∈ ZV×|F|\n+ , where a row vector mv has some\nnon-zero elements to select factor vectors, establishes the\nrelation between word and factor representation matrices\nas R = MR(f) and Q = MQ(f). In practice, we ex-\nploit this for test-time efﬁciency—word vectors are com-\npiled ofﬂine so that the computational cost of LBL++ prob-\nability lookups is the same as for the LBL.\nWe consider two obvious variations of the LBL++ to evalu-\nate the extent to which interactions between context and\nCompositional Morphology for Word Representations and Language Modelling\nFigure 1.Model diagram.Illustration of how a 3-gram CLBL++\nmodel treats the Czech phrase pro novou ˇskolu (‘for the new\nschool’), assuming the target word ˇskolu is clustered into word\nclass 17 by the method described in §3.2.\ntarget factors affect the model: LBL+o only factorises\noutput words and retains simple word vectors for the con-\ntext (i.e. Q ≡Q(f)), while LBL+c does the reverse, only\nfactorising context words.2 Both reduce to the LBL when\nsetting µto be the identity function, such that V≡F .\nThe factorisation permits an approach to unknown context\nwords that is less harsh than the standard method of replac-\ning them with a global unknown symbol—instead, a vector\ncan be constructed from the known factors of the word (e.g.\nthe observed stem of an unobserved inﬂected form). A sim-\nilar scheme can be used for scoring unknown target words,\nbut requires changing the event space of the probabilistic\nmodel. We use this vocabulary stretching capability in our\nword similarity experiments, but leave the extensions for\ntest-time language model predictions as future work.\n3.2 Class-based Model Decomposition\nThe key obstacle to using CSLMs in a decoder is the ex-\npensive normalisation over the vocabulary. Our approach\nto reducing the computational cost of normalisation is to\nuse a class-based decomposition of the probabilistic model\n(Goodman, 2001; Mikolov et al., 2011). Using Brown-\nclustering (Brown et al., 1992), 3 we partition the vocabu-\nlary into |C|classes, denoting as Cc the set of vocabulary\nitems in class c, such that V= C1 ∪···∪C |C|.\nIn this model, the probability of a word conditioned on the\nhistory hof n−1 preceding words is decomposed as\nP(w|h) = P(c|h)P(w|h,c). (3)\nThis class-based model, CLBL, extends over the LBL by\nassociating a representation vector sc and bias parameter\ntc to each class c, such that ΘCLBL = (Cj,Q,R,S, b,t).\nThe same prediction vector p is used to compute both class\n2The +c, +o and ++ naming sufﬁxes denote these same dis-\ntinctions when used with the CLBL model introduced later.\n3In preliminary experiments, Brown clusters gave better per-\nplexities than frequency-binning (Mikolov et al., 2011).\nscore τ(c) = p ·sc + tc and word score ν(w), which are\nnormalised separately:\nP(c|h) = exp (τ(c))\n∑|C|\nc′=1 exp (τ(c′))\n(4)\nP(w|h,c) = exp (ν(w))∑\nv′∈Cc exp (ν(v′)). (5)\nWe favour this ﬂat vocabulary partitioning for its computa-\ntional adequacy, simplicity and robustness. Computational\nadequacy is obtained by using |C|≈|V| 0.5, thereby reduc-\ning the O(|V|) normalisation operation of the LBL to two\nO(|V|0.5) operations in the CLBL.\nOther methods for achieving more drastic complexity re-\nductions exist in the form of frequency-based truncation,\nshortlists (Schwenk, 2004), or casting the vocabulary as a\nfull hierarchy (Mnih & Hinton, 2008) or partial hierarchy\n(Le et al., 2011). We expect these approaches could have\nadverse effects in the rich morphology setting, where much\nof the vocabulary is in the long tail of the word distribution.\n3.3 Training & Initialisation\nModel parameters Θ are estimated by optimising an L2-\nregularised log likelihood objective. Training theCLBL and\nits additive variants directly against this objective is fast\nbecause normalisation of model scores, which is required\nin computing gradients, is over a small number of events.\nFor the classless LBLs we use noise-contrastive estimation\n(NCE) (Gutmann & Hyv ¨arinen, 2012; Mnih & Teh, 2012)\nto avoid normalisation during training. This leaves the ex-\npensive test-time normalisation of LBLs unchanged, pre-\ncluding their usage during decoding.\nBias terms b (resp. t) are initialised to the log unigram\nprobabilities of words (resp. classes) in the training cor-\npus, with Laplace smoothing, while all other parameters are\ninitialised randomly according to sharp, zero-mean Gaus-\nsians. Representations are thus learnt from scratch and not\nbased on publicly available embeddings, meaning our ap-\nproach can easily be applied to many languages.\nOptimisation is performed by stochastic gradient descent\nwith updates after each mini-batch of Ltraining examples.\nWe apply AdaGrad (Duchi et al., 2011) and tune the step-\nsize ξon development data.4 We halt training once the per-\nplexity on the development data starts to increase.\n4 Experiments\nThe overarching aim of our evaluation is to investigate the\neffect of using the proposed additive representations across\nlanguages with a range of morphological complexity.\n4L=10k–40k, ξ=0.05–0.08, dependent on |V|and data size.\nCompositional Morphology for Word Representations and Language Modelling\nTable 1.Corpus statistics. The number of sentence pairs for a\nrow X refers to the English →X parallel data (but row E N has\nCzech as source language).\nDATA-1M DATA-MAIN\nToks. |V| Toks. |V| Sent. Pairs\nCS 1m 46k 16.8m 206k 0.7m\nDE 1m 36k 50.9m 339k 1.9m\nEN 1m 17k 19.5m 60k 0.7m\nES 1m 27k 56.2m 152k 2.0m\nFR 1m 25k 57.4m 137k 2.0m\nRU 1m 62k 25.1m 497k 1.5m\nOur intrinsic language model evaluation has two parts. We\nﬁrst perform a model selection experiment on small data\nto consider the relative merits of using additive representa-\ntions for context words, target words, or both, and to vali-\ndate the use of the class-based decomposition.\nThen we consider class-based additive models trained on\ntens of millions of tokens and large vocabularies. These\nlarger language models are applied in two extrinsic tasks:\ni) a word-similarity rating experiment on multiple lan-\nguages, aiming to gauge the quality of the induced word\nand morpheme representation vectors; ii) a machine trans-\nlation experiment, where we are speciﬁcally interested in\ntesting the impact of an LBL LM feature when translating\ninto morphologically rich languages.\n4.1 Data & Methods\nWe make use of data from the 2013 ACL Workshop on Ma-\nchine Translation.5 We ﬁrst describe data used for transla-\ntion experiments, since the monolingual datasets used for\nlanguage model training were derived from that. The lan-\nguage pairs are English→{German, French, Spanish, Rus-\nsian}and English ↔Czech. Our parallel data comprised\nthe Europarl-v7 and news-commentary corpora, except for\nEnglish–Russian where we usednews-commentary and the\nY andexparallel corpus. 6 Pre-processing involved lower-\ncasing, tokenising and ﬁltering to exclude sentences of\nmore than 80 tokens or substantially different lengths.\n4-gram language models were trained on the target data\nin two batches: D ATA-1M consists of the ﬁrst million to-\nkens only, while D ATA-MAIN is the full target-side data.\nStatistics are given in Table 1. newstest2011 was used\nas development data 7 for tuning language model hyper-\nparameters, while intrinsic LM evaluation was done on\nnewstest2012. As metric, we use model perplexity (PPL)\nexp(−1\nN\n∑N\ni=1 ln P(wi)), where N is the number of test\ntokens. In addition to contrasting the LBL variants, we also\nuse modiﬁed Kneser-Ney n-gram models (MKNs) (Chen &\nGoodman, 1998) as baselines.\n5http://www.statmt.org/wmt13/translation-task.html\n6https://translate.yandex.ru/corpus?lang=en\n7For Russian, some training data was held out for tuning.\n+c +o ++\n−14\n−12\n−10\n−8\n−6\n−4\n−2\n0\n2\nPerplexity reduction (%)\nLBL (308)\n+c +o ++\nCLBL (309)\nFigure 2.Model selection results. Box-plots show the spread,\nacross 6 languages, of relative perplexity reductions obtained by\neach type of additive model against its non-additive baseline, for\nwhich median absolute perplexity is given in parentheses; for\nMKN, that is 348. Each box-plot summarises the behaviour of\na model across languages. Circles give sample means, while\ncrosses show outliers beyond 3×the inter-quartile range.\nLanguage Model Vocabularies. Additive representa-\ntions that link morphologically related words speciﬁcally\naim to improve modelling of the long tail of the lexicon, so\nwe do not want to prune away all rare words, as is common\npractice in language modelling and word embedding learn-\ning. We deﬁne a singleton pruning rate κ, and randomly\nreplace that fraction of words occurring only once in the\ntraining data with a global U NK symbol. κ= 1 would im-\nply a unigram count cut-off threshold of 1. Instead, we use\nlow pruning rates8 and thus model large vocabularies.9\nWord Factorisationµ. We obtain labelled morphologi-\ncal segmentations from the unsupervised segmentor Mor-\nfessor Cat-MAP (Creutz & Lagus, 2007). The mapping µ\nof a word is taken as its surface form and the morphemes\nidentiﬁed by Morfessor. Keeping the morpheme labels al-\nlows the model to learn separate vectors for, say, instem the\npreposition and inpreﬁx occurring as in·appropriate. By not\npost-processing segmentations in a more sophisticated way,\nwe keep the overall method more language independent.\n4.2 Intrinsic Language Model Evaluation\nResults on DATA-1M. The use of morphology-based, ad-\nditive representations for both context and output words\n(models++) yielded perplexity reductions on all 6 lan-\nguages when using 1m training tokens. Furthermore, these\ndouble-additive models consistently outperform the ones\nthat factorise only context (+c) or only output (+o) words,\nindicating that context and output contribute complemen-\ntary information and supporting our hypothesis that is it\nbeneﬁcial to model morphological dependencies across\nwords. The results are summarised in Figure 2.\nFor lack of space we do not present numbers for individual\nlanguages, but report that the impact of CLBL++ varies by\n8 DATA-1M: κ= 0.2; DATA-MAIN : κ= 0.05\n9 We also mapped digits to 0, and cleaned the Russian data by\nreplacing tokens having <80% Cyrillic characters with UNK.\nCompositional Morphology for Word Representations and Language Modelling\nTable 2.Test-set perplexities on DATA-MAIN using two vocab-\nulary pruning settings. Percentage reductions are relative to the\npreceding model, e.g. the ﬁrst Czech CLBL improves over MKN\nby 20.8% (Rel.1); the CLBL++ improves over that CLBL by a\nfurther 5.9% (Rel.2).\nMKN CLBL CLBL++\nPPL PPL Rel.1 PPL Rel.2\nκ=0.05 CS 862 683 -20.8% 643 -5.9%\nDE 463 422 -8.9% 404 -4.2%\nEN 291 281 -3.4% 273 -2.8%\nES 219 207 -5.7% 203 -1.9%\nFR 243 232 -4.9% 227 -1.9%\nRU 390 313 -19.7% 300 -4.2%\nκ=1.0 CS 634 477 -24.8% 462 -3.1%\nDE 379 331 -12.6% 329 -0.9%\nEN 254 234 -7.6% 233 -0.7%\nES 195 180 -7.7% 180 0.02%\nFR 218 201 -7.7% 198 -1.3%\nRU 347 271 -21.8% 262 -3.4%\nlanguage, correlating with vocabulary size: Russian bene-\nﬁted most, followed by Czech and German. Even on En-\nglish, often regarded as having simple morphology, the rel-\native improvement is 4%.\nThe relative merits of the +c and +o schemes depend on\nwhich model is used as starting point. With LBL, the\noutput-additive scheme (LBL+o) gives larger improvements\nthan the context-additive scheme ( LBL+c). The reverse is\ntrue for CLBL, indicating the class decomposition damp-\nens the effectiveness of using morphological information\nin output words.\nThe use of classes increases perplexity slightly compared\nto the LBLs, but this is in exchange for much faster compu-\ntation of language model probabilities, allowing the CLBLs\nto be used in a machine translation decoder (§4.4).\nResults on DATA-MAIN . Based on the outcomes of the\nsmall-scale evaluation, we focus our main language model\nevaluation on the additive class-based model CLBL++ in\ncomparison to CLBL and MKN baselines, using the larger\ntraining dataset, with vocabularies of up to 500k types.\nThe overall trend that morphology-based additive represen-\ntations yield lower perplexity carries over to this larger data\nsetting, again with the biggest impact being on Czech and\nRussian (Table 2, top). Improvements are in the 2%–6%\nrange, slightly lower than the corresponding differences on\nthe small data.\nOur hypothesis is that the much of the improvement is due\nto the additive representations being especially beneﬁcial\nfor modelling rare words. We test this by repeating the\nexperiment under the condition where all word types oc-\ncurring only once are excluded from the vocabulary (κ=1).\nIf the additive representations were not beneﬁcial to rare\n−20\n−10\n0\n10\n20\nPercentage\nCS RU\nunk0 1 2 3 4 5 6\nDE\n−20\n−10\n0\n10\n20\nPercentage\nunk0 1 2 3 4 5 6\nES\n34%\nTest set coverage Perplexity reduction\nFigure 3.Perplexity reductions by token frequency,CLBL++\nrelative to CLBL. Dotted bars extending further down are better.\nA bin labelled with a number x contains those test tokens that\noccur y ∈[10x,10x+1) times in the training data. Striped bars\nshow percentage of test-set covered by each bin.\nwords, the outcome should remain the same. Instead, we\nﬁnd the relative improvements become a lot smaller (Ta-\nble 2, bottom) than when only excluding some singletons\n(κ=0.05), which supports that hypothesis.\nAnalysis. Model perplexity on a whole dataset is a con-\nvenient summary of its intrinsic performance, but such a\nglobal view does not give much insight intohow one model\noutperforms another. We now partition the test data into\nsubsets of interest and measure PPL over these subsets.\nWe ﬁrst partition on token frequency, as computed on the\ntraining data. Figure 3 provides further evidence that the\nadditive models have most impact on rare words generally,\nand not only on singletons. Czech, German and Russian\nsee relative PPL reductions of 8%–21% for words occur-\nring fewer than 100 times in the training data. Reductions\nbecome negligible for the high-frequency tokens. These\ntend to be punctuation and closed-class words, where any\nputative relevance of morphology is overwhelmed by the\nfact that the predictive uncertainty is very low to begin with\n(absolute PPL<10 for the highest frequency subset). For\nthe morphologically simpler Spanish case, PPL reductions\nare generally smaller across frequency scales.\nWe also break down PPL reductions by part of speech tags,\nfocusing on German. We used the decision tree-based tag-\nger of Schmid & Laws (2008). Aside from unseen tokens,\nthe biggest improvements are on nouns and adjectives (Fig-\nure 4), suggesting our segmentation-based representations\nhelp abstract over German’s productive compounding.\nCompositional Morphology for Word Representations and Language Modelling\nunkAdjAdv N ProPrp V Rest\n−20\n−10\n0\n10\n20\n30\nPercentage\nDE\nTest set\ncoverage\nPerplexity\nreduction\nFigure 4.Perplexity reductions by part of speech,CLBL++ rel-\native to CLBL on German. Dotted bars extending further down\nare better. Tokens tagged as foreign words or other opaque sym-\nbols resort under “Rest”. Striped bars as in Figure 3\nGerman noun phrases require agreement in gender, case\nand number, which are marked overtly with fusional mor-\nphemes, and we see large gains on such test n-grams: 15%\nimprovement on adjective-noun sequences, and 21% when\nconsidering the more speciﬁc case of adjective-adjective-\nnoun sequences. An example of the latter kind is der\nehemalig·e sozial·ist·isch·e bildung·s·minister (‘the former\nsocialist minister of education’), where the morphological\nagreement surfaces in the repeated e-sufﬁx.\nWe conducted a ﬁnal scaling experiment on Czech by\ntraining models on increasing amounts of data from the\nmonolingual news corpora. Improvements over the MKN\nbaseline decrease, but remain substantial at 14% for the\nlargest setting when allowing the vocabulary to grow with\nthe data. Maintaining a constant advantage over MKN re-\nquires also increasing the dimensionality d of representa-\ntions (Mikolov et al., 2013a), but this was outside the scope\nof our experiment. Although gains from the additive rep-\nresentations over the CLBL diminish down to 2%–3% at\nthe scale of 128m training tokens (Figure 5), these results\ndemonstrate the tractability of our approach on very large\nvocabularies of nearly 1m types.\n4.3 Task 1: Word Similarity Rating\nIn the previous section, we established the positive role that\nmorphological awareness played in building continuous-\nspace language models that better predict unseen text. Here\nwe focus on the quality of the word representations learnt\nin the process. We evaluate on a standard word similarity\nrating task, where one measures the correlation between\ncosine-similarity scores for pairs of word vectors and a\nset of human similarity ratings. An important aspect of\nour evaluation is to measure performance on multiple lan-\nguages using a single unsupervised, model-based approach.\nMorpheme vectors from theCLBL++ enable handling OOV\ntest words in a more nuanced way than using the global\nunknown word vector. In general, we compose a vector\n˜uv = [ ˜qv; ˜rv] for a word v according to a post hoc word\n24 25 26 27\nTraining tokens (m)\n0\n3\n6\n9\n12\n15\n18\n21\n24\n27Perplexity reduction (%)\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nV ocabulary size|Vvar| (k)\n|V|=206k\nCLBL++ vs. MKN\nCLBL++ vs. CLBL\n|Vvar|\nCLBL++ vs. MKN\nCLBL++ vs. CLBL\nFigure 5.Scaling experiment.Relative perplexity reductions ob-\ntained when varying the Czech training data size (16m–128m).\nIn the ﬁrst setting, the vocabulary was held ﬁxed as data size\nincreased(|V|); in the second it varied freely across sizes (|Vvar|).\nmap µ′by summing and concatenating the factor vectorsrf\nand qf, where f ∈µ′(v) ∩F. This ignores unknown mor-\nphemes occurring in OOV words, and uses[qUNK ; rUNK ] for\n˜uUNK only if all morphemes are unknown.\nTo see whether the morphological representations improve\nthe quality of vectors for known words, we also report the\ncorrelations obtained when using theCLBL++ word vectors\ndirectly, resorting to ˜uUNK for all OOV words v /∈V (de-\nnoted “−compose” in the results). This is also the strategy\nthat the baseline CLBL model is forced to follow for OOVs.\nWe evaluate ﬁrst using the English rare-word dataset\n(RW) created by Luong et al. (2013). Its 2034 word\npairs contain more morphological complexity than other\nwell-established word similarity datasets, e.g. crudeness—\nimpoliteness. We compare against their context-sensitive\nmorphological recursive neural network (csmRNN), using\nSpearman’s rank correlation coefﬁcient, ρ. Table 3 shows\nour model obtaining a ρ-value slightly below the best csm-\nRNN result, but outperforming the csmRNN that used an\nalternative set of embeddings for initialisation.\nThis is a strong result given that our vectors come from a\nsimple linear probabilistic model that is also suitable for\nintegration directly into a decoder for translation ( §4.4) or\nspeech recognition, which is not the case for csmRNNs.\nMoreover, the csmRNNs were initialised with high-quality,\npublicly available word embeddings trained over weeks on\nmuch larger corpora of 630–990m words (Collobert & We-\nston, 2008; Huang et al., 2012), in contrast to ours that are\ntrained from scratch on much less data. This renders our\nmethod directly applicable to languages which may not yet\nhave those resources.\nRelative to theCLBL baseline, our method performs well on\nCompositional Morphology for Word Representations and Language Modelling\nTable 3.Word-pair similarity task.Spearman’sρ×100 for the\ncorrelation between model scores and human ratings on the En-\nglish RW dataset. The csmRNNs beneﬁt from initialisation with\nhigh quality pre-existing word embeddings, while our models\nused random initialisation.\n(Luong et al., 2013) Our models\nHSMN 2 CLBL 18\nHSMN+csmRNN 22 CLBL++ 30\nC&W 27 −compose 20\nC&W+csmRNN 34\nFigure 6.English morpheme vectors learnt by CLBL++.\nDimensionality reduction was performed with t-SNE (van der\nMaaten & Hinton, 2008), with shading added for emphasis.\ndatasets across four languages. For the English RW, which\nwas designed with morphology in mind, the gain is 64%.\nBut also on the standard English WS353 dataset (Finkel-\nstein et al., 2002), we get a 26% better correlation with the\nhuman ratings. On German, the CLBL++ obtains correla-\ntions up to three times stronger than the baseline, and 39%\nbetter for French (Table 4).\nA visualisation of the English morpheme vectors (Figure 6)\nsuggests the model captured non-trivial morphological reg-\nularities: noun sufﬁxes relating to persons (writ er, hu-\nmanists) lie close together, while being separated according\nto number; negation preﬁxes share a region (un-, in-, mis-,\ndis-); and relational preﬁxes are grouped (surpa-, super-,\nmulti-, intra-), with a potential explanation for their separa-\ntion from inter- being that the latter is more strongly bound\nup in lexicalisations (international, intersection).\n4.4 Task 2: Machine Translation\nThe ﬁnal aspect of our evaluation focuses on the integra-\ntion of class-decomposed log-bilinear models into a ma-\nchine translation system. To the best of our knowledge, this\n10 ES WS353 (Hassan & Mihalcea, 2009); Gur350 (Gurevych,\n2005); RG65 (Rubenstein & Goodenough, 1965) with F R\n(Joubarne & Inkpen, 2011); ZG222 (Zesch & Gurevych, 2006).\nTable 4.Word-pair similarity task (multi-language),showing\nSpearman’sρ×100 and the number of word pairs in each dataset.\nAs benchmarks, we include the best results from Luong et al.\n(2013), who relied on more training data and pre-existing embed-\ndings not available in all languages. In the penultimate row our\nmodel’s ability to compose vectors for OOV words is suppressed.\nDatasets10 WS Gur RG ZG\nModel / Language EN ES DE EN FR DE\nHSMN 63 – – 63 – –\n+csmRNN 65 – – 65 – –\nCLBL 32 26 36 47 33 6\nCLBL++ 39 28 56 41 45 25\n−compose 40 27 44 41 41 23\n# pairs 353 350 65 222\nis the ﬁrst study to investigate large vocabulary normalised\nCSLMs inside a decoder when translating into a range of\nmorphologically rich languages. We consider 5 language\npairs, translating from English into Czech, German, Rus-\nsian, Spanish and French.\nAside from the choice of language pairs, this evaluation\ndiverges from Vaswani et al. (2013) by using normalised\nprobabilities, a process made tractable by the class-based\ndecomposition and caching of context-speciﬁc normaliser\nterms. Vaswani et al. (2013) relied on unnormalised model\nscores for efﬁciency, but do not report on the performance\nimpact of this assumption. In our preliminary experi-\nments, there was high variance in the performance of un-\nnormalised models. They are difﬁcult to reason about as a\nfeature function that must help the translation model dis-\ncriminate between alternative hypotheses.\nWe use cdec (Dyer et al., 2010; 2013) to build symmetric\nword-alignments and extract rules for hierarchical phrase-\nbased translation (Chiang, 2007). Our baseline system uses\na standard set of features in a log-linear translation model.\nThis includes a baseline 4-gram MKN language model,\ntrained with SRILM (Stolcke, 2002) and queried efﬁciently\nusing KenLM (Heaﬁeld, 2011). The CSLMs are integrated\ndirectly into the decoder as an additional feature function,\nthus exercising a stronger inﬂuence on the search than in\nn-best list rescoring. 11 Translation model feature weights\nare tuned with MERT (Och, 2003) on newstest2012.\nTable 5 summarises our translation results. Inclusion of\nthe CLBL++ language model feature outperforms the MKN-\nonly baseline systems by 1.2 B LEU points for translation\ninto Russian, and by 1 point into Czech and Spanish. The\nEN→DE system beneﬁts least from the additional CSLM\nfeature, despite the perplexity reductions achieved in the\nintrinsic evaluation. In light of German’s productive com-\npounding, it is conceivable that the bilingual coverage of\n11Our source code for using CLBL/CLBL++ with cdec is re-\nleased at http://bothameister.github.io.\nCompositional Morphology for Word Representations and Language Modelling\nTable 5.Translation results. Case-insensitive B LEU scores on\nnewstest2013, with standard deviation over 3 runs given in\nparentheses. The two right-most columns use the listed CSLM\nas a feature in addition to the MKN feature, i.e. these MT systems\nhave at most 2 LMs. Language models are from Table 2 (top).\nMKN CLBL CLBL++\nEN→CS 12.6 (0.2) 13.2 (0.1) 13.6 (0.0)\nDE 15.7 (0.1) 15.9 (0.2) 15.8 (0.4)\nES 24.7 (0.4) 25.5 (0.5) 25.7 (0.3)\nFR 24.1 (0.2) 24.6 (0.2) 24.8 (0.5)\nRU 15.9 (0.2) 16.9 (0.3) 17.1 (0.1)\nCS→EN 19.8 (0.4) 20.4 (0.4) 20.4 (0.5)\nthat system is more of a limitation than the performance of\nthe language models.\nOn the other languages, the CLBL adds 0.5 to 1 B LEU\npoints over the baseline, whereas additional improvement\nfrom the additive representations lies within MERT vari-\nance except for EN→CS.\nThe impact of our morphology-aware language model is\nlimited by the translation system’s inability to generate un-\nseen inﬂections. A future task is thus to combine it with a\nsystem that can do so (Chahuneau et al., 2013).\n5 Related Work\nFactored language models (FLMs) have been used to inte-\ngrate morphological information into both discrete n-gram\nLMs (Bilmes & Kirchhoff, 2003) and CSLMs (Alexan-\ndrescu & Kirchhoff, 2006) by viewing a word as a set of\nfactors. Alexandrescu & Kirchhoff (2006) demonstrated\nhow factorising the representations of context-words can\nhelp deal with out-of-vocabulary words, but they did not\nevaluate the effect of factorising output words and did not\nconduct an extrinsic evaluation.\nA variety of strategies have been explored for bring-\ning CSLMs to bear on machine translation. Rescoring\nlattices with a CSLM proved to be beneﬁcial for ASR\n(Schwenk, 2004) and was subsequently applied to trans-\nlation (Schwenk et al., 2006; Schwenk & Koehn, 2008),\nreaching training sizes of up to 500m words (Schwenk\net al., 2012). For efﬁciency, this line of work relied heav-\nily on small “shortlists” of common words, by-passing the\nCSLM and using a back-off n-gram model for the remain-\nder of the vocabulary. Using unnormalised CSLMs during\nﬁrst-pass decoding has generated improvements in B LEU\nscore for translation into English (Vaswani et al., 2013).\nRecent work has moved beyond monolingual vector-space\nmodelling, incorporating phrase similarity ratings based\non bilingual word embeddings as a translation model fea-\nture (Zou et al., 2013), or formulating translation purely in\nterms of continuous-space models (Kalchbrenner & Blun-\nsom, 2013). Accounting for linguistically derived infor-\nmation such as morphology (Luong et al., 2013; Lazari-\ndou et al., 2013) or syntax (Hermann & Blunsom, 2013)\nhas recently proved beneﬁcial to learning vector represen-\ntations of words. Our contribution is to create morphologi-\ncal awareness in a probabilistic language model.\n6 Conclusion\nWe introduced a method for integrating morphology into\nprobabilistic continuous-space language models. Our\nmethod has the ﬂexibility to be used for morphologically\nrich languages (MRLs) across a range of linguistic ty-\npologies. Our empirical evaluation focused on multiple\nMRLs and different tasks. The primary outcomes are\nthat (i) our morphology-guided CSLMs improve intrin-\nsic language model performance when compared to base-\nline CSLMs and n-gram MKN models; (ii) word and\nmorpheme representations learnt in the process compare\nfavourably in terms of a word similarity task to a recent\nmore complex model that used more data, while obtain-\ning large gains on some languages; (iii) machine transla-\ntion quality as measured by B LEU was improved consis-\ntently across six language pairs when using CSLMs during\ndecoding, although the morphology-based representations\nled to further improvements beyond the level of optimiser\nvariance only for English →Czech. By demonstrating that\nthe class decomposition enables full integration of a nor-\nmalised CSLM into a decoder, we open up many other pos-\nsibilities in this active modelling space.\nReferences\nAlexandrescu, A. & Kirchhoff, K. Factored Neural Language\nModels. In Proc. HLT-NAACL: short papers. ACL, 2006.\nBengio, Y ., Ducharme, R., Vincent, P., & Jauvin, C. A Neural\nProbabilistic Language Model. JMLR, 3:1137–1155, 2003.\nBilmes, J. A. & Kirchhoff, K. Factored Language Models and\nGeneralized Parallel Backoff. In Proc. NAACL-HLT: short pa-\npers. ACL, 2003.\nBrown, P. F., DeSouza, P. V ., Mercer, R. L., Della Pietra, V . J., &\nLai, J. C. Class-Based n-gram Models of Natural Language.\nComp. Ling., 18(4):467–479, 1992.\nChahuneau, V ., Schlinger, E., Smith, N. A., & Dyer, C. Trans-\nlating into Morphologically Rich Languages with Synthetic\nPhrases. In Proc. EMNLP, pp. 1677–1687. ACL, 2013.\nChen, S. F. & Goodman, J. An Empirical Study of Smoothing\nTechniques for Language Modeling. Technical report, Harvard\nUniversity, Cambridge, MA, 1998.\nChiang, D. Hierarchical Phrase-Based Translation. Comp. Ling.,\n33(2):201–228, 2007.\nCollobert, R. & Weston, J. A Uniﬁed Architecture for Natural\nLanguage Processing: Deep Neural Networks with Multitask\nLearning. In Proc. ICML. ACM, 2008.\nCompositional Morphology for Word Representations and Language Modelling\nCreutz, M. & Lagus, K. Unsupervised Models for Morpheme\nSegmentation and Morphology Learning. ACM Trans. on\nSpeech and Language Processing, 4(1):1–34, 2007.\nDuchi, J., Hazan, E., & Singer, Y . Adaptive Subgradient Methods\nfor Online Learning and Stochastic Optimization. JMLR, 12:\n2121–2159, 2011.\nDyer, C., Lopez, A., Ganitkevitch, J., Weese, J., Ture, F., Blun-\nsom, P., Setiawan, H., Eidelman, V ., & Resnik, P. cdec: A\nDecoder, Alignment, and Learning Framework for Finite-State\nand Context-Free Translation Models. In Proc. ACL: demon-\nstration session, pp. 7–12, 2010. ACL.\nDyer, C., Chahuneau, V ., & Smith, N. A. A Simple, Fast, and Ef-\nfective Reparameterization of IBM Model 2. In Proc. NAACL,\npp. 644–648. ACL, 2013.\nFinkelstein, L., Gabrilovich, E., Matias, Y ., Rivlin, E., Solan, Z.,\nWolfman, G., & Ruppin, E. Placing Search in Context: The\nConcept Revisited. ACM Trans. on Information Systems , 20\n(1):116–131, 2002.\nGoodman, J. Classes for Fast Maximum Entropy Training. In\nProc. ICASSP, pp. 561–564. IEEE, 2001.\nGurevych, I. Using the Structure of a Conceptual Network in\nComputing Semantic Relatedness. In Proc. IJCNLP, pp. 767–\n778, 2005.\nGutmann, M. U. & Hyv ¨arinen, A. Noise-Contrastive Estimation\nof Unnormalized Statistical Models , with Applications to Nat-\nural Image Statistics. JMLR, 13:307–361, 2012.\nHassan, S. & Mihalcea, R. Cross-lingual Semantic Relatedness\nUsing Encyclopedic Knowledge. In Proc. EMNLP, pp. 1192–\n1201. ACL, 2009.\nHeaﬁeld, K. KenLM: Faster and Smaller Language Model\nQueries. In Proc. Workshop on Statistical Machine Transla-\ntion, pp. 187–197. ACL, 2011.\nHermann, K. M. & Blunsom, P. The Role of Syntax in Vector\nSpace Models of Compositional Semantics. In Proc. ACL, pp.\n894–904, 2013.\nHuang, E. H., Socher, R., Manning, C. D., & Ng, A. Y . Improving\nWord Representations via Global Context and Multiple Word\nPrototypes. In Proc. ACL, pp. 873–882. ACL, 2012.\nJoubarne, C. & Inkpen, D. Comparison of Semantic Similarity\nfor Different Languages Using the Google N-gram Corpus and\nSecond- Order Co-occurrence Measures. In Proc. Canadian\nConference on Advances in AI, pp. 216–221. Springer-Verlag,\n2011.\nKalchbrenner, N. & Blunsom, P. Recurrent Continuous Transla-\ntion Models. In Proc. EMNLP, pp. 1700–1709. ACL, 2013.\nKneser, R. & Ney, H. Improved Backing-off for m-gram Lan-\nguage Modelling. In Proc. ICASSP, pp. 181–184, 1995.\nLazaridou, A., Marelli, M., Zamparelli, R., & Baroni, M.\nCompositional-ly Derived Representations of Morphologically\nComplex Words in Distributional Semantics. InProc. ACL, pp.\n1517–1526, 2013. ACL.\nLe, H.-S., Oparin, I., Allauzen, A., Gauvain, J.-L., & Yvon, F.\nStructured Output Layer Neural Network Language Model. In\nProc. ICASSP, pp. 5524–5527, 2011. IEEE.\nLuong, M.-T., Socher, R., & Manning, C. D. Better Word Rep-\nresentations with Recursive Neural Networks for Morphology.\nIn Proc. of CoNLL, 2013.\nMikolov, T., Karaﬁ´at, M., Burget, L., ˇCernock´y, J., & Khudanpur,\nS. Recurrent neural network based language model. In Proc.\nInterspeech, pp. 1045–1048, 2010.\nMikolov, T., Kombrink, S., Burget, L., ˇCernock´y, J., & Khudan-\npur, S. Extensions of Recurrent Neural Network Language\nModel. In Proc. ICASSP, 2011.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. Efﬁcient Estima-\ntion of Word Representations in Vector Space. In Proc. ICLR.\narXiv:1301.3781, 2013a.\nMikolov, T., Yih, W.-t., & Zweig, G. Linguistic Regularities\nin Continuous Space Word Representations. In Proc. HLT-\nNAACL. ACL, 2013b.\nMnih, A. & Hinton, G. Three New Graphical Models for Statis-\ntical Language Modelling. In Proc. ICML, pp. 641–648, 2007.\nACM.\nMnih, A. & Hinton, G. A Scalable Hierarchical Distributed Lan-\nguage Model. In NIPS, pp. 1081–1088, 2008.\nMnih, A. & Teh, Y . W. A fast and simple algorithm for training\nneural probabilistic language models. In Proc. ICML, 2012.\nOch, F. J. Minimum Error Rate Training in Statistical Machine\nTranslation. In Proc. ACL, pp. 160–167, 2003.\nRubenstein, H. & Goodenough, J. B. Contextual Correlates of\nSynonymy. Commun. ACM, 8(10):627–633, October 1965.\nSchmid, H. & Laws, F. Estimation of Conditional Probabilities\nWith Decision Trees and an Application to Fine-Grained POS\nTagging. In Proc. COLING, pp. 777–784, 2008. ACL.\nSchwenk, H. Efﬁcient Training of Large Neural Networks for\nLanguage Modeling. In Proc. IEEE Joint Conference on Neu-\nral Networks, pp. 3059–3064. IEEE, 2004.\nSchwenk, H. & Koehn, P. Large and Diverse Language Models\nfor Statistical Machine Translation. In Proc. IJCNLP, 2008.\nSchwenk, H., Dchelotte, D., & Gauvain, J.-L. Continuous Space\nLanguage Models for Statistical Machine Translation. In Proc.\nCOLING/ACL, pp. 723–730, 2006. ACL.\nSchwenk, H., Rousseau, A., & Attik, M. Large, Pruned or Con-\ntinuous Space Language Models on a GPU for Statistical Ma-\nchine Translation. In In Proc. NAACL-HLT Workshop: On the\nFuture of Language Modeling for HLT, pp. 11–19. ACL, 2012.\nStolcke, A. SRILM – An extensible language modeling toolkit.\nIn Proc. ICSLP, pp. 901–904, 2002.\nvan der Maaten, L. & Hinton, G. Visualizing Data using t-SNE.\nJMLR, 9:2579–2605, 2008.\nVaswani, A., Zhao, Y ., Fossum, V ., & Chiang, D. Decoding with\nLarge-Scale Neural Language Models Improves Translation.\nIn Proc. EMNLP, 2013. ACL.\nZesch, T. & Gurevych, I. Automatically creating datasets for mea-\nsures of semantic relatedness. In Proc. Workshop on Linguistic\nDistances, pp. 16–24. ACL, 2006.\nZou, W. Y ., Socher, R., Cer, D., & Manning, C. D. Bilingual Word\nEmbeddings for Phrase-Based Machine Translation. In Proc.\nEMNLP, pp. 1393–1398, 2013. ACL."
}