{
  "title": "Class-Aware Adversarial Transformers for Medical Image Segmentation",
  "url": "https://openalex.org/W4320085508",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222929886",
      "name": "You, Chenyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3013227677",
      "name": "Zhao, Ruihan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2269575529",
      "name": "Liu Feng-lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1949540472",
      "name": "DONG Siyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3111203772",
      "name": "Chinchali, Sandeep",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742649608",
      "name": "Topcu, Ufuk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222929891",
      "name": "Staib, Lawrence",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222929892",
      "name": "Duncan, James S.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998789541",
    "https://openalex.org/W4226133625",
    "https://openalex.org/W3127107873",
    "https://openalex.org/W3084995528",
    "https://openalex.org/W3176466780",
    "https://openalex.org/W4379660275",
    "https://openalex.org/W3008930132",
    "https://openalex.org/W2767128594",
    "https://openalex.org/W2798401174",
    "https://openalex.org/W3139338041",
    "https://openalex.org/W2888358068",
    "https://openalex.org/W3033671339",
    "https://openalex.org/W3194662286",
    "https://openalex.org/W3106295246",
    "https://openalex.org/W3138558221",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W4312398120",
    "https://openalex.org/W4287181769",
    "https://openalex.org/W3129576291",
    "https://openalex.org/W2301358467",
    "https://openalex.org/W2592929672",
    "https://openalex.org/W4379653969",
    "https://openalex.org/W3101639073"
  ],
  "abstract": "Transformers have made remarkable progress towards modeling long-range dependencies within the medical image analysis domain. However, current transformer-based models suffer from several disadvantages: (1) existing methods fail to capture the important features of the images due to the naive tokenization scheme; (2) the models suffer from information loss because they only consider single-scale feature representations; and (3) the segmentation label maps generated by the models are not accurate enough without considering rich semantic contexts and anatomical textures. In this work, we present CASTformer, a novel type of adversarial transformers, for 2D medical image segmentation. First, we take advantage of the pyramid structure to construct multi-scale representations and handle multi-scale variations. We then design a novel class-aware transformer module to better learn the discriminative regions of objects with semantic structures. Lastly, we utilize an adversarial training strategy that boosts segmentation accuracy and correspondingly allows a transformer-based discriminator to capture high-level semantically correlated contents and low-level anatomical features. Our experiments demonstrate that CASTformer dramatically outperforms previous state-of-the-art transformer-based approaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in Dice over previous models. Further qualitative experiments provide a more detailed picture of the model's inner workings, shed light on the challenges in improved transparency, and demonstrate that transfer learning can greatly improve performance and reduce the size of medical image datasets in training, making CASTformer a strong starting point for downstream medical image analysis tasks.",
  "full_text": "Class-Aware Adversarial Transformers\nfor Medical Image Segmentation\nChenyu You1 Ruihan Zhao2 Fenglin Liu3 Siyuan Dong1 Sandeep Chinchali2\nUfuk Topcu2 Lawrence Staib1 James S. Duncan1\n1Yale University 2UT Austin 3University of Oxford\nAbstract\nTransformers have made remarkable progress towards modeling long-range depen-\ndencies within the medical image analysis domain. However, current transformer-\nbased models suffer from several disadvantages: (1) existing methods fail to capture\nthe important features of the images due to the naive tokenization scheme; (2) the\nmodels suffer from information loss because they only consider single-scale feature\nrepresentations; and (3) the segmentation label maps generated by the models\nare not accurate enough without considering rich semantic contexts and anatom-\nical textures. In this work, we present CASTformer, a novel type of adversarial\ntransformers, for 2D medical image segmentation. First, we take advantage of the\npyramid structure to construct multi-scale representations and handle multi-scale\nvariations. We then design a novel class-aware transformer module to better learn\nthe discriminative regions of objects with semantic structures. Lastly, we utilize an\nadversarial training strategy that boosts segmentation accuracy and correspondingly\nallows a transformer-based discriminator to capture high-level semantically corre-\nlated contents and low-level anatomical features. Our experiments demonstrate that\nCASTformer dramatically outperforms previous state-of-the-art transformer-based\napproaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements\nin Dice over previous models. Further qualitative experiments provide a more\ndetailed picture of the model‚Äôs inner workings, shed light on the challenges in\nimproved transparency, and demonstrate that transfer learning can greatly improve\nperformance and reduce the size of medical image datasets in training, making\nCASTformer a strong starting point for downstream medical image analysis tasks.\n1 Introduction\nAccurate and consistent measurements of anatomical features and functional information on medical\nimages can greatly assist radiologists in making accurate and reliable diagnoses, treatment planning,\nand post-treatment evaluation [1]. Convolutional neural networks (CNNs) have been the de-facto\nstandard for medical image analysis tasks. However, such methods fail in explicitly modeling\nlong-range dependencies due to the intrinsic locality and weight sharing of the receptive Ô¨Åelds\nin convolution operations. Such a deÔ¨Åciency in context modeling at multiple scales often yields\nsub-optimal segmentation capability in capturing rich anatomical features of variable shapes and\nscales (e.g., tumor regions with different structures and sizes) [2, 3]. Moreover, using transformers\nhas been shown to be more promising in computer vision [4‚Äì9] for utilizing long-range dependencies\nthan other, traditional CNN-based methods. In parallel, transformers with powerful global relation\nmodeling abilities have become the standard starting point for training on a wide range of downstream\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\narXiv:2201.10737v5  [cs.CV]  15 Dec 2022\nGenerator\nClass-aware Transformer ModuleTransformer Encoder ModuleClass-aware Transformer ModuleTransformer Encoder ModuleClass-aware Transformer ModuleTransformer Encoder Module\nClass-aware Transformer ModuleTransformer Encoder Module HeadReal / Fake?Discriminator\nTransformer Encoder Module\nEncoder (ResNet)\n Decoder\nEncoder (ResNet)\nFigure 1: Our proposed CASTformer consists of a transformer-based generator (i.e., CATformer)\nand a discriminator.\nmedical imaging analysis tasks, such as image segmentation [7, 10‚Äì13], image synthesis [14‚Äì16],\nand image enhancement [17‚Äì26].\nMedical image semantic segmentation can be formulated as a typical dense prediction problem,\nwhich aims at performing pixel-level classiÔ¨Åcation on the feature maps. Recently, Chen et al. [7]\nintroduced TransUNet, which inherits the advantage of both UNet [27] and Transformers [4],\nto exploit high-resolution informative representations in the spatial dimension by CNNs and the\npowerful global relation modeling by Transformers. Although existing transformer-based approaches\nhave proved promising in the medical image segmentation task, there remain several formidable\nchallenges, because (1) the model outputs a single-scale and low-resolution feature representation; (2)\nprior work mainly adopts the standard tokenization scheme, hard splitting an image into a sequence of\nimage patches of size16√ó16, which may fail to capture inherent object structures and the Ô¨Åne-grained\nspatial details for the downstream dense prediction task; (3) compared to the standard convolution,\nthe transformer architecture requires a grid structure, and thus lacks the capability to localize regions\nthat contain objects of interest instead of the uninteresting background; and (4) existing methods\nare usually deÔ¨Åcient in ensuring the performance without capturing both global and local contextual\nrelations among pixels. We argue that transformer-based segmentation models are not yet robust\nenough to replace CNN-based methods, and investigate several above-mentioned key challenges\ntransformer-based segmentation models still face.\nInspired by recent success of vision transformer networks [3, 4, 28‚Äì34], we make a step towards a\nmore practical scenario in which we only assume access to pre-trained models on public computer\nvision datasets, and a relatively small medical dataset, which we can use the weights of the pre-trained\nmodels to achieve higher accuracy in the medical image analysis tasks. These settings are particularly\nappealing as (1) such models can be easily adopted on typical medical datasets; (2) such a setting\nonly requires limited training data and annotations; and (3) transfer learning typically leads to better\nperformance [35‚Äì38]. Inspired by such Ô¨Åndings, we propose several novel strategies for expanding\nits learning abilities to our setting, considering both multi-scale anatomical feature representations of\ninteresting objects and transfer learning in the medical imaging domain.\nFirst, we aim to model multi-scale variations by learning feature maps of different resolutions. Thus,\nwe propose to incorporate the pyramid structure into the transformer framework for medical image\nsegmentation, which enables the model to capture rich global spatial information and local multi-scale\ncontext information. Additionally, we consider medical semantic segmentation as a sequence-to-\nsequence prediction task. The standard patch tokenization scheme in [4] is an art‚Äîsplitting it into\nseveral Ô¨Åxed-size patches and linearly embedding them into input tokens. Even if signiÔ¨Åcant progress\nis achieved, model performance is likely to be sub-optimal. We address this issue by introducing\na novel class-aware transformer module, drawing inspiration from a progressive sampling strategy\nin image classiÔ¨Åcation [ 39], to adaptively and selectively learn interesting parts of objects. This\nessentially allows us to obtain effective anatomical features from spatial attended regions within the\nmedical images, so as to guide the segmentation of objects or entities.\n2\n(a) Class-Aware Transformer Module \nùêÖ! MLPùêñ! I#\nOffsets\n+\nBilinear Interpolation\nSampled Features\nTransformers\nH Norm\nNorm\nMulti-Head Self-Attention+\nMLP+\nEmbedded Patches\n(L=12)\n(b) Transformer Network \n(M=4)\nFigure 2: An illustration of (a) our class-aware transformer module and (b) transformer network. Our\nclass-aware transformer module iteratively samples discriminative locations. A group of points is\ninitialized in the regularly-spaced location. At each step, given the feature map Fi, we iteratively\nupdate its sampling locations by adding them with the estimated offsets of the last step. Note that only\n4 points are shown for a clear presentation, and there are more points in the actual implementation.\nSecond, we adopt the idea of Generative Adversarial Networks (GANs) to improve segmentation\nperformance and correspondingly enable a transformer-based discriminator to learn low-level anatom-\nical features and high-level semantics. The standard GANs are not guaranteed to prioritize the most\ninformative demonstrations on interesting anatomical regions, and mixing irrelevant regions ( i.e.,\nbackground) creates inferior contexts, which drastically underperform segmentation performance\n[2, 40, 3]. Additionally, it is well-known that they are notoriously difÔ¨Åcult to train and prone to model\ncollapse [41]. Training vision transformers is also tedious and requires large amounts of labeled data,\nwhich largely limits the training quality. We use a more reÔ¨Åned strategy, where, for each input, we\ncombine it with the predicted segmentation mask to create the image with anatomical demonstrations.\nWe also leverage the pre-trained checkpoints to compensate the need of large-dataset training, thereby\nproviding a good starting point with more discriminative visual demonstrations. We refer to our\napproach as CASTformer, class-aware adversarial transformers: a strong transformer-based method\nfor 2D medical image segmentation. Our contributions are summarized as follows:\n‚Ä¢ Novel Network Architecture: We make the Ô¨Årst attempt to build a GAN using a\ntransformer-based architecture for the 2D medical image segmentation task. We incor-\nporate the pyramid structure into the generator to learn rich global and local multi-scale\nspatial representations, and also devise a novel class-aware transformer module by progres-\nsively learning the interesting regions correlated with semantic structures of images. To the\nbest of our knowledge, we are the Ô¨Årst work to explore these techniques in the context of\nmedical imaging segmentation.\n‚Ä¢ Better Understanding Inner Workings: We conduct careful analyses to understand the\nmodel‚Äôs inner workings, how the sampling strategy works, and how different training factors\nlead to the Ô¨Ånal performance. We highlight that it is more effective to progressively learn\ndistinct contextual representations with the class-aware transformer module, resulting in\nmore accurate and robust models that applied better to a variety of downstream medical\nimage analysis tasks (See Appendix J and K).\n‚Ä¢ Remarkable Performance Improvements: CASTformer contributes towards a dramatic\nimprovement across three datasets we evaluate on. For instance, we achieve Dice scores\nof 82.55% and 73.82% by obtaining gains up to 5.88% absolute improvement compared to\nprior methods on the Synapse multi-organ dataset. We illustrate the beneÔ¨Åts of leveraging\npre-trained models from the computer vision domain, and provide suggestions for future\nresearch that could be less susceptible to the confounding effects of training data from the\nnatural image domain.\n3\n2 Related Work\nCNN-based Segmentation Networks Before the emergence of transformer-based methods, CNNs\nwere the de facto methods in medical image segmentation tasks [ 42‚Äì61]. For example, Ron-\nneberger et al. [27] proposed a deep 2D UNet architecture, combining skip connections between\nopposing convolution and deconvolution layers to achieve promising performance on a diverse set of\nmedical segmentation tasks. Han et al. [62] developed a 2.5D 24-layer Fully Convolutional Network\n(FCN) for liver segmentation tasks where the residual block was incorporated into the model. To\nfurther improve segmentation accuracy, Kamnitsaset al. [63] proposed a dual pathway 11-layer 3D\nCNN, and also employed a 3D fully connected conditional random Ô¨Åeld (CRF) [64] as an additional\npairwise constraint between neighboring pixels for the challenging task of brain lesion segmentation.\nTransformers in Medical Image Segmentation Recent studies [ 7, 10, 12, 13, 65‚Äì70] have\nfocused on developing transformer-based methods for medical image analysis tasks. Recently,\nChen et al. [7] proposed TransUNet, which takes advantage of both UNet and Transformers, to\nexploit high-resolution informative information in the spatial dimension by CNNs and the global\ndependencies by Transformers. Cao et al. [10] explored how to use a pure transformer for medical\nimage analysis tasks. However, the results do not lead to better performance. These works mainly\nutilized hard splitting some highly semantically correlated regions without capturing the inherent\nobject structures. In this work, beyond simply using the naive tokenization scheme in [4, 7], we aim\nat enabling the transformer to capture global information Ô¨Çow to estimate offsets towards regions of\ninterest.\nTransformer in Generative Adversarial Networks Adversarial learning has proved to be a very\nuseful and widely applicable technique for learning generative models of arbitrarily complex data\ndistributions in the medical domain. As the discriminator Ddifferentiates between real and fake\nsamples, the adversarial loss serves as the regularization constraint to enforce the generator Gto\npredict more realistic samples. Inspired by such recent success [71‚Äì74, 22, 75, 20, 23, 21, 76, 77],\nJiang et al. [71] proposed to build a GAN pipeline with two pure transformer-based architectures\nin synthesizing high-resolution images. Esser et al. [31] Ô¨Årst used a convolutional GAN model to\nlearn a codebook of context-rich visual features, followed by transformer architecture to learn the\ncompositional parts. Hudson et al. [3] proposed a bipartite self-attention on StyleGAN to propagate\nlatent variables to the evolving visual features. Despite such success, it requires high computation\ncosts due to the quadratic complexity, which fundamentally hinders its applicability to the real world.\nBesides the image generation task, we seek to take a step forward in tackling the challenging task of\n2D medical image segmentation.\n3 Method\nOur proposed approach is presented in Figure 1. Given the input image x ‚ààRH√óW√ó3, similar to\nTransUNet architecture [7], our proposed generator network G, termed CATformer, is comprised\nof four key components: encoder (feature extractor) module, class-aware transformer module,\ntransformer encoder module, and decoder module. As shown in Figure 1, our generator has four\nstages with four parallel subnetworks. All stages share a similar architecture, which contains a patch\nembedding layer, class-aware layer, and Li Transformer encoder layers.\nEncoder Module. Our method adopts a CNN-Transformer hybrid model design instead of using a\npure transformer, which uses 40 convolutional layers, to generate multi-scale feature maps. Such a\nconvolutional stem setting provides two advantages: (1) using convolutional stem helps transformers\nperform better in the downstream vision tasks [78‚Äì84]; (2) it provides high-resolution feature maps\nwith parallel medium- and low-resolution feature maps to help boost better representations. In this\nway, we can construct the feature pyramid for the Transformers, and utilize the multi-scale feature\nmaps for the downstream medical segmentation task. With the aid of feature maps of different\nresolutions, our model is capable of modeling multi-resolution spatially local contexts.\nHierarchical Feature Representation. Inspired by recent success in object detection [ 85], we\ndeviate from TransUNet by generating a single-resolution feature map, and our focus is on extracting\nCNN-like multi-level features Fi, where i‚àà{1,2,3,4}, to achieve high segmentation accuracy by\nleveraging high-resolution features and low-resolution features. More precisely, in the Ô¨Årst stage, we\nutilize the encoder module to obtain the dense feature map F1 ‚ààR\nH\n2 √óW\n2 √óC1 , where (H\n2 ,W\n2 ,C1)\n4\nis the spatial feature resolution and the number of feature channels. In a similar way, we can\nformulate the following feature maps as follows: F2 ‚ààR\nH\n2 √óW\n2 √ó(C1¬∑4), F3 ‚ààR\nH\n4 √óW\n4 √ó(C1¬∑8), and\nF4 ‚ààR\nH\n8 √óW\n8 √ó(C1¬∑12). Then, we divide F1 into HW\n162 patches with the patch size P of 16√ó16√ó3,\nand feed the Ô¨Çattened patches into a learnable linear transformation to obtain the patch embeddings\nof size HW\n162 √óC1.\nClass-Aware Transformer Module. The class-aware transformer module (CAT) is designed to\nadaptively focus on useful regions of objects (e.g., the underlying anatomical features and structural\ninformation). Our CAT module is largely inspired by the recent success for image classiÔ¨Åcation [39],\nbut we deviate from theirs as follows: (1) we remove the vision transformer module in [39] to alleviate\nthe computation and memory usage; (2) we use 4 separate Transformer Encoder Modules (TEM),\nwhich will be introduced below; (3) we incorporate M CAT modules on multi-scale representations\nto allow for contextual information of anatomical features to propagate into the representations. Our\nclass-aware transformer module is an iterative optimization process. In particular, we apply the\nclass-aware transformer module to obtain the sequence of tokens IM,1 ‚ààRC√ó(n√ón), where (n√ón)\nand M are the number of samples on each feature map and the total iterative number, respectively. As\nshown in Figure 2, given the feature map F1, we iteratively update its sampling locations by adding\nthem with the estimated offset vectors of the last step, which can be formulated as follows:\nst+1 = st + ot, t‚àà{1,...,M ‚àí1}, (1)\nwhere st ‚ààR2√ó(n√ón), and ot ‚ààR2√ó(n√ón) are the sampling location and the predicted offset vector\nat t-th step. SpeciÔ¨Åcally, the s1 is initialized at the regularly spaced sampling grid. The i-th sampling\nlocation si\n1 is deÔ¨Åned as follows:\nsi\n1 = [Œ≤y\ni œÑh + œÑh/2,Œ≤x\ni œÑw + œÑw/2], (2)\nwhere Œ≤y\ni = ‚åäi/n‚åã, Œ≤x\ni = i‚àíŒ≤y\ni ‚àón. The step sizes in the y (row index) and x(column index)\ndirections denote œÑh = H/nand œÑw = W/n, respectively. ‚åä¬∑‚åãis the Ô¨Çoor operation. We can deÔ¨Åne\nthe initial token on the input feature map in the following form: I\n‚Ä≤\nt = Fi(st), where t‚àà{1,...,M },\nand I\n‚Ä≤\nt ‚ààRC√ó(n√ón) denotes the initial sampled tokens at t-th step. We set the sampling function as\nthe bilinear interpolation, since it is differentiable with respect to both the sampling locations st and\nthe input feature map Fi. We do an element-wise addition of the current positional embedding of the\nsampling locations, the initial sampled tokens, and the estimated tokens of the last step, and then we\ncan obtain the output tokens at each step:\nSt = Wtst\nVt = I\n‚Ä≤\nt ‚äïSt ‚äïIt‚àí1\nIt = Transformer(Vt), t‚àà{1,...,M },\n(3)\nwhere Wt ‚ààRC√ó2 is the learnable matrix that embeds st to positional embedding St ‚ààRC√ó(n√ón),\nand ‚äïis the element-wise addition. Transformer(¬∑) is the transformer encoder layer, as we will show\nin the following paragraphs. We can compute the estimated sampling location offsets as:\not = Œ∏t(It), t‚àà{1,...,M ‚àí1}, (4)\nwhere Œ∏t(¬∑) ‚ààR2√ó(n√ón) is the learnable linear mapping for the estimated sampling offset vectors.\nIt is worth noting that these operations are all differentiable, thus the model can be learned in an\nend-to-end fashion.\nTransformer Encoder Module. The transformer encoder module (TEM) is designed to model\nlong-range contextual information by aggregating global contextual information from the complete\nsequences of input image patches embedding. In implementations, the transformer encoder module\nfollows the architecture in ViT [4], which is composed of Multi-head Self-Attention (MSA), and\nMLP blocks, which can be formulated as:\nE0 = [x1\npH; x2\npH; ¬∑¬∑¬∑ ; xN\np H] +Hpos, (5)\nE‚Ä≤\ni = MSA(LN(Ei‚àí1)) +Ei‚àí1, (6)\nEi = MLP(LN(E‚Ä≤\ni)) +E‚Ä≤\ni, (7)\nwhere i = 1...M , and LN(¬∑) is the layer normalization. H ‚ààR(P2¬∑C)√óD and Hpos ‚ààRN√óD\ndenote the patch embedding projection and the position embedding, respectively.\n5\nGround TruthUNet AttUNet\nSwin-UNetCATformerCASTformer\nR50+UNet\nSETR\nCoTr\n TransUNet\naortagallbladderleft kidneyrightkidneyliverpancreasspleenstomach\nFigure 3: Visual comparisons with other methods on Synapse dataset. As observed, CASTformer\nachieves superior performance with detailed anatomical features and the boundary information of\ndifferent organs.\nDecoder Module. The decoder is designed to generate the segmentation mask based on four output\nfeature maps of different resolutions. In implementations, rather than designing a hand-crafted\ndecoder module that requires high computational demand, we incorporate a lightweight All-MLP\ndecoder [86], and such a simple design allows us to yield a powerful representation much more\nefÔ¨Åciently. The decoder includes the following criteria: 1) the channel dimension of multi-scale\nfeatures is uniÔ¨Åed through the MLP layers; 2) we up-sample the features to 1/4th and concatenate\nthem together; 3) we utilize the MLP layer to fuse the concatenated features, and then predict the\nmulti-class segmentation mask y‚Ä≤from the fused features.\nDiscriminator Network. We use the R50+ViT-B/16 hybrid model pre-trained on ImageNet from\nViT [4] as a starting point for our discriminator design, in this case using the pre-trained strategies to\nlearn effectively on the limited size target task data. Then, we simply apply a two-layer multi-layered\nperceptron (MLP) to make a prediction about the identity of the class-aware image. Following\nprevious work [2], we Ô¨Årst utilize the ground truth image x and the predicted segmentation mask\ny‚Ä≤to obtain the class-aware image Àúx (i.e., pixel-wise multiplication of x and y‚Ä≤). It is important\nto note that this construction re-uses the pre-trained weights and does not introduce any additional\nparameters. Dseeks to classify between real and fake samples [87]. Gand Dcompete with each\nother through attempting to reach an equilibrium point of the minimax game. Using this structure\nenables the discriminator to model long-range dependencies, making it better assess the medical\nimage Ô¨Ådelity. This also essentially endows the model with a more holistic understanding of the\nanatomical visual modality (categorical features).\nTraining Objective. As to the loss function and training conÔ¨Ågurations, we adopt the settings used\nin Wasserstein GAN (WGAN) [88], and use WGAN-GP loss [89]. We jointly use the segmentation\nloss [7, 13] and WGAN-GP loss to train G. Concretely, the segmentation loss includes the dice loss\nand cross-entropy loss. Hence, the training process of CASTformer can be formulated as:\nLG = Œª1LCE + Œª2LDICE + Œª3LWGAN-GP, (8)\nwhere Œª1, Œª2, Œª3 determine the importance of each loss term. See Appendix H for an ablation study.\n4 Experimental Setup\nDatasets. We experiment on multiple challenging benchmark datasets: Synapse1, LiTS, and MP-MRI.\nMore details can be found in Appendix A.\nImplementation Details. We utilize the AdamW optimizer [90] in all our experiments. For training\nour generator and discriminator, we use a learning rate of 5e‚àí4 with a batch size of 6, and train\n1https://www.synapse.org/#!Synapse:syn3193805/wiki/217789\n6\nTable 1: Quantitative segmentation results on the Synapse multi-organ CT dataset.\nFramework Average Aorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach\nEncoder Decoder DSC ‚ÜëJaccard‚Üë95HD‚ÜìASD‚Üì\nUNet[27] 70.11 59.39 44.69 14.41 84.00 56.70 72.41 62.64 86.98 48.73 81.48 67.96\nAttnUNet[91] 71.70 61.38 34.47 10.00 82.61 61.94 76.07 70.42 87.54 46.70 80.67 67.66\nResNet50 UNet[27] 73.51 63.81 29.65 8.83 82.21 55.06 76.71 73.07 89.36 53.52 84.91 73.22\nResNet50 AttnUNet[91] 74.74 62.69 33.04 9.49 83.68 58.63 79.08 74.53 90.81 55.76 83.80 71.68\nSETR[92] 66.30 54.19 29.09 7.16 66.63 38.34 74.45 68.49 92.18 35.91 83.01 71.41\nCoTrw/oCNN-encoder[13] 54.82 42.49 69.58 20.37 63.22 37.86 67.10 60.61 88.48 15.46 60.74 45.12\nCoTr[13] 72.60 61.25 41.55 12.42 83.27 60.41 79.58 73.01 91.93 45.07 82.84 64.67\nTransUNet[7] 77.48 64.78 31.69 8.46 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nSwinUNet[10] 76.33 65.64 27.16 8.32 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\n‚Ä¢CATformer(ours) 82.17 73.22 16.20 4.28 88.98 67.16 85.72 81.69 95.34 66.53 90.74 81.20\n‚ó¶CASTformer(ours) 82.55 74.69 22.73 5.81 89.05 67.48 86.05 82.17 95.61 67.49 91.00 81.55\neach model for 300 epochs for all datasets. We set the sampling number non each feature map and\nthe total iterative number M as 16 and 4, respectively. See Appendix B, C, H for details on the\ntraining conÔ¨Åguration, model architecture and hyperparameters. We also adopt the input resolution\nand patch size P as 224√ó224 and 14, respectively. We set Œª1 = 0.5, Œª2 = 0.5, and Œª3 = 0.1 in this\nexperiments. In the testing stage, we adopt four metrics to evaluate the segmentation performance:\nDice coefÔ¨Åcient (Dice), Jaccard Index (Jaccard), 95% Hausdorff Distance (95HD), and Average\nSymmetric Surface Distance (ASD). All our experiments are implemented in Pytorch 1.7.0. We train\nall models on a single NVIDIA GeForce RTX 3090 GPU with 24GB of memory.\n5 Results\nWe compare our approaches ( i.e., CATformer and CASTformer) with previous state-of-the-art\ntransformer-based segmentation methods, including UNet [27], AttnUNet [91], ResNet50+UNet\n[27], ResNet50+AttnUNet [91], SETR [92], CoTr w/o CNN-encoder [13], CoTr [13], TransUNet\n[7], SwinUnet [10] on Synapse, LiTS, and MP-MRI datasets. More results are in Appendix D and E.\nExperiments: Synapse Multi-organ. The quantitative results on the Synapse dataset are shown in\nTable 1. The results are visualized in Figure 3. It can be observed that ourCATformer outperforms the\nprevious best model by a large margin and achieves a 4.69% ‚àí8.44% absolute improvement in Dice\nand Jaccard, respectively. Our CASTformer achieves the best performance of 82.55% and 74.69%,\ndramatically improving the previous state-of-the-art model (TransUNet) by +5.07% and +9.91%,\nin terms of both Dice and Jaccard scores. This shows that the anatomical visual information is useful\nfor the model to gain Ô¨Åner control in localizing local semantic regions. As also shown in Table 1, our\nCASTformer achieves absolute Dice improvements of+2.77%, +2.51%, +1.35%, +4.95% on large\norgans (i.e., left kidney, right kidney, liver, stomach) respectively. Such improvements demonstrate\nthe effectiveness of learning the evolving anatomical features of the image, as well as accurately\nidentifying the boundary information of large organs. We also observed similar trends that, compared\nto the previous state-of-the-art results, ourCASTformer obtains 89.05%, 67.48%, 67.49% in terms of\nDice on small organs (i.e., aorta, gallbladder, pancreas) respectively, which yields big improvements\nof +1.82%, +0.95%, +10.91%. This clearly demonstrates the superiority of our models, allowing\nfor a spatially Ô¨Åner control over the segmentation process.\nExperiments: LiTS. To further evaluate the effectiveness of our proposed approaches, we compare\nour models on the LiTS dataset. Experimental results on the LiTS CT dataset are summarized\nin Appendix Table 6. As is shown, we observe that our CATformer yields a 72.39% Dice score,\noutperforming all other methods. Moreover, our CASTformer signiÔ¨Åcantly outperforms all previous\napproaches, including the previous best TransUNet, and establishes a new state-of-the-art of 73.82%\nand 64.91% in terms of Dice and Jaccard, which are 5.88% and 4.66% absolute improvements better\nthan TransUNet. For example, our CASTformer achieves the best performance of 95.88% Dice\non the liver region by 2.48%, while it dramatically increases the result from 42.49% to 51.76%\non the tumor region, demonstrating that our model achieves competitive performance on liver and\ntumor segmentation. As shown in Figure 4, our method is capable of predicting high-quality object\nsegmentation, considering the fact that the improvement in such a setting is challenging. This\ndemonstrates: (1) the necessity of adaptively focusing on the region of interests; and (2) the efÔ¨Åcacy\nof semantically correlated information. Compared to previously high-performing models, our two\napproaches achieve signiÔ¨Åcant improvements on all datasets, demonstrating their effectiveness.\n7\nGround TruthUNet AttUNet\nSwin-UNetCATformerCASTformer\nR50+UNet\nSETR\nCoTr\n TransUNet\nlivertumor\nFigure 4: Visual comparisons with other methods on LiTS dataset. As observed, CASTformer\nachieves superior performance with detailed anatomical information (e.g., the tumor regions in red).\n6 Analysis\nIn this section, we conduct thorough analyses of our CASTformer in the following aspects: transfer\nlearning, model components, effects of iteration number N (Appendix F), sampling number n(Ap-\npendix G), hyperparameter selection (Appendix H), different GAN-based loss functions (Appendix\nI), and gain a better understanding of the model‚Äôs inner workings (Appendix J and K).\nTransfer Learning. We consider whether we can leverage the pre-trained model commonly used\nin computer vision literature [4], to provide more evidence for the beneÔ¨Åcial impact of our network\nperformance. We use CATformer (G) as the baseline and evaluate all the settings on the Synapse\nmulti-organ dataset. To put our results in perspective, we compare with six ways of using pre-trained\nR50+ViT-B/16 hybrid model from ViT [4] for transfer learning, namely (1) CATformer: w/o pre-\ntrained and w/ pre-trained; (2) CASTformer: both w/o pre-trained, only w/ pre-trained D, only w/\npre-trained G, and both w/ pre-trained Gand D.\nTable 2: Effect of transfer learning in our\nCATformer and CASTformer on the Synapse\nmulti-organ dataset.\nModel DSC Jaccard 95HD ASD\n‚Ä¢CATformer(w/o pre-trained)74.84 65.61 31.817.23‚Ä¢CATformer(w/ pre-trained) 82.17 73.22 16.20 4.28\n‚ó¶CASTformer(bothw/o pre-trained)73.64 62.68 42.7711.76‚ó¶CASTformer(onlyw/ pre-trainedD) 78.87 69.36 30.54 9.17‚ó¶CASTformer(onlyw/ pre-trainedG) 81.46 71.80 27.366.91‚ó¶CASTformer(bothw/ pre-trained)82.55 74.69 22.73 5.81\nTable 3: Ablation on model component: Baseline;\nCATformer w/o CAT;CATformer w/o TEM; and\nCATformer.\nModel DSC Jaccard 95HD ASD\nBaseline 77.48 64.78 31.69 8.46‚Ä¢CATformerw/o CAT80.09 70.56 25.62 7.30‚Ä¢CATformerw/o TEM81.35 72.66 24.43 7.17‚Ä¢CATformer 82.17 73.22 16.20 4.28‚ó¶CASTformer 82.55 74.69 22.73 5.81\nThe results are in Table 2. Overall, we observe that using ‚Äúw/ pre-trained‚Äù leads to higher accuracy\nthan ‚Äúw/o pre-trained‚Äù, with signiÔ¨Åcant improvements for the smaller sizes of datasets, suggesting\nthat using ‚Äúw/ pre-trained‚Äù provides us a good set of initial parameters for the downstream tasks. With\nusing pre-trained weights, CATformer outperforms the setting without using pre-trained weights by\na large margin and achieves 7.33% and 7.61% absolute improvements in terms of Dice and Jaccard.\nCASTformer (‚Äúboth w/ pre-trained‚Äù) also yields big improvements (+8.91% and +12.01%) in Dice\nand Jaccard. This suggests that CASTformer is better at both initializing from the pre-trained models\nand better at gathering the anatomical information in a more adaptive and selective manner. As shown\nin Table 2, surprisingly, there is a signiÔ¨Åcant discrepancy between only using ‚Äúw/ pre-trainedD‚Äù and\n‚Äúw/ pre-trained G‚Äù: for example, CASTformer achieves 78.87% in Dice with only w/ pre-trained D,\nwhile CASTformer achieves 81.46% if only Guses the pre-trained weights. This demonstrates that\nonly using pre-trained weight in Dmight be the culprit for the exploitation of anatomical information.\n8\nOur results suggest that (1) utilizing pre-trained models in the computer vision domain can help the\nmodel quickly adapt to new downstream medical segmentation tasks without re-building billions\nof anatomical representations; (2) we Ô¨Ånd that leveraging pre-trained weights can further boost the\nperformance because it can mitigate the discrepancy between training and inference; and (3) it also\ncreates a possibility to adapt our model to the typical medical dataset with the smaller size.\nAblation of Model Components. Our key observation is that it is crucial to build high-quality\nanatomical representations through each model component. To show the strengths of our approach,\nwe examine the following variants and inspect each key component on the Synapse multi-organ\nsegmentation dataset: (1) Baseline: we remove the class-aware transformer module and the trans-\nformer encoder module in our CATformer as the baseline, similar to TransUNet deÔ¨Åned in [ 7];\n(2) CATformer w/o CAT: we only remove the class-aware transformer module in our CATformer;\n(3) CATformer w/o TEM: we only remove the transformer encoder module in our CATformer; (4)\nCATformer: this is our Gmodel; and (5) CASTformer: this is our Ô¨Ånal model described in Section\n3. Table 3 summarizes the results of all the variants. As is shown, we observe that compared to\nthe baseline model, both CATformer w/o CAT and CATformer w/o TEM are able to develop a\nbetter holistic understanding of global shapes/structures and Ô¨Åne anatomical details, thus leading\nto large performance improvements (+2.61% and +3.87%) in terms of Dice. Our results show the\nclass-aware transformer module is useful in improving the segmentation performance, suggesting\nthat the discriminative regions of medical images are particularly effective. Finally, one thing worth\nnoticing is that incorporating both the class-aware transformer module and the transformer encoder\nmodule performs better than only using a single module, highlighting the importance of two modules\nin our CATformer.\n7 Conclusion and Discussion of Broader Impact\nIn this work, we have introducedCASTformer, a simple yet effective type of adversarial transformers,\nfor 2D medical image segmentation. The key insight is to integrate the multi-scale pyramid structure\nto capture rich global spatial information and local multi-scale context information. Furthermore,\nCASTformer also beneÔ¨Åts from our proposed class-aware transformer module to progressively and\nselectively learn interesting parts of the objects. Lastly, the generator-discriminator design is used to\nboost segmentation performance and correspondingly enable the transformer-based discriminator\nto capture low-level anatomical features and high-level semantics. Comprehensive experiments\ndemonstrate that our CASTformer outperforms the previous state-of-the-art on three popular medical\ndatasets considerably. We conduct extensive analyses to study the robustness of our approach, and\nform a more detailed understanding of desirable properties in the medical domain (i.e., transparency\nand data efÔ¨Åciency).\nOverall, we hope that this model can serve as a solid baseline for 2D medical image segmentation\nand motivate further research in medical image analysis tasks. It also provides a new perspective on\ntransfer learning in medical domain, and initially shed novel insights towards understanding neural\nnetwork behavior. As such pattern is hard to quantify, we expect more mechanistic explanations for\nclinical practise. We also plan to optimize the transformer-based architectures for the downstream\nmedical image analysis tasks both in terms of data and model parameters.\nBroader Impact. We acknowledge that our research will not pose signiÔ¨Åcant risks of societal harm\nto society. Our work is scientiÔ¨Åc nature and will have the potential to positively contribute to a\nnumber of real-world clinical applications that establish high-quality and end-to-end medical image\nsegmentation systems. We expect our approach to contribute to the grand goal of building more\nsecured and trustworthy clinical AI.\nReferences\n[1] Mehrdad Moghbel, Syamsiah Mashohor, Rozi Mahmud, and M Iqbal Bin Saripan. Review of liver\nsegmentation and computer assisted detection/diagnosis methods in computed tomography. ArtiÔ¨Åcial\nIntelligence, 2017.\n[2] Yuan Xue, Tao Xu, Han Zhang, L Rodney Long, and Xiaolei Huang. Segan: Adversarial network with\nmulti-scale l 1 loss for medical image segmentation. Neuroinformatics, 2018.\n[3] Drew A Hudson and Larry Zitnick. Generative adversarial transformers. In International Conference on\nMachine Learning (ICML), 2021.\n9\n[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. In International Conference on\nLearning Representations (ICLR), 2020.\n[5] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and\nJiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n[6] Aditya Desai, Zhaozhuo Xu, Menal Gupta, Anu Chandran, Antoine Vial-Aussavy, and Anshumali\nShrivastava. Raw nav-merge seismic data to subsurface properties with mlp based multi-modal information\nunscrambler. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n[7] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille,\nand Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. In\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI),\n2021.\n[8] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in\nneural information processing systems, 28, 2015.\n[9] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. Masked autoencoders\nare scalable vision learners. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2022.\n[10] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang.\nSwin-unet: Unet-like pure transformer for medical image segmentation. arXiv preprint arXiv:2105.05537,\n2021.\n[11] Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, and Jiangyun Li. Transbts: Multimodal\nbrain tumor segmentation using transformer. In International Conference on Medical Image Computing\nand Computer-Assisted Intervention (MICCAI), 2021.\n[12] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Medical transformer:\nGated axial-attention for medical image segmentation. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention (MICCAI), 2021.\n[13] Yutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia. Cotr: EfÔ¨Åciently bridging cnn and transformer\nfor 3d medical image segmentation. In International Conference on Medical Image Computing and\nComputer-Assisted Intervention (MICCAI), 2021.\n[14] Lingke Kong, Chenyu Lian, Detian Huang, Yanle Hu, Qichao Zhou, et al. Breaking the dilemma of\nmedical image-to-image translation. In Advances in Neural Information Processing Systems (NeurIPS),\n2021.\n[15] Nicolae-Catalin Ristea, Andreea-Iuliana Miron, Olivian Savencu, Mariana-Iuliana Georgescu, Nicolae\nVerga, Fahad Shahbaz Khan, and Radu Tudor Ionescu. Cytran: Cycle-consistent transformers for\nnon-contrast to contrast ct translation. arXiv preprint arXiv:2110.06400, 2021.\n[16] Onat Dalmaz, Mahmut Yurt, and Tolga √áukur. Resvit: Residual vision transformers for multi-modal\nmedical image synthesis. arXiv preprint arXiv:2106.16031, 2021.\n[17] Yilmaz Korkmaz, Salman UH Dar, Mahmut Yurt, Muzaffer √ñzbey, and Tolga √áukur. Unsupervised mri\nreconstruction via zero-shot learned adversarial transformers. arXiv preprint arXiv:2105.08059, 2021.\n[18] Zhicheng Zhang, Lequan Yu, Xiaokun Liang, Wei Zhao, and Lei Xing. Transct: Dual-path transformer\nfor low dose computed tomography. In International Conference on Medical Image Computing and\nComputer-Assisted Intervention (MICCAI), 2021.\n[19] Qing Lyu, Chenyu You, Hongming Shan, and Ge Wang. Super-resolution mri through deep learning.\narXiv preprint arXiv:1810.06776, 2018.\n[20] Qing Lyu, Chenyu You, Hongming Shan, Yi Zhang, and Ge Wang. Super-resolution mri and ct through\ngan-circle. In Developments in X-ray tomography XII , volume 11113, page 111130X. International\nSociety for Optics and Photonics, 2019.\n[21] Indranil Guha, Syed Ahmed Nadeem, Chenyu You, Xiaoliu Zhang, Steven M Levy, Ge Wang, James C\nTorner, and Punam K Saha. Deep learning based high-resolution reconstruction of trabecular bone\nmicrostructures from low-resolution ct scans using gan-circle. In Medical Imaging 2020: Biomedical\nApplications in Molecular, Structural, and Functional Imaging . International Society for Optics and\nPhotonics, 2020.\n10\n[22] Chenyu You, Qingsong Yang, Hongming Shan, Lars Gjesteby, Guang Li, Shenghong Ju, Zhuiyang Zhang,\nZhen Zhao, Yi Zhang, Wenxiang Cong, et al. Structurally-sensitive multi-scale deep neural network for\nlow-dose ct denoising. IEEE access, 2018.\n[23] Chenyu You, Linfeng Yang, Yi Zhang, and Ge Wang. Low-dose ct via deep cnn with skip connection and\nnetwork-in-network. In Developments in X-Ray tomography XII. International Society for Optics and\nPhotonics, 2019.\n[24] Chenyu You, Lianyi Han, Aosong Feng, Ruihan Zhao, Hui Tang, and Wei Fan. Megan: Memory enhanced\ngraph attention network for space-time video super-resolution. In In Proceedings of WACV 2022, 2021.\n[25] Achleshwar Luthra, Harsh Sulakhe, Tanish Mittal, Abhishek Iyer, and Santosh Yadav. Eformer: Edge\nenhancement based transformer for medical image denoising. arXiv preprint arXiv:2109.08044, 2021.\n[26] Dayang Wang, Zhan Wu, and Hengyong Yu. Ted-net: Convolution-free t2t vision transformer-based\nencoder-decoder dilation network for low-dose ct denoising. arXiv preprint arXiv:2106.04650, 2021.\n[27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In International Conference on Medical Image Computing and Computer-Assisted\nIntervention (MICCAI), 2015.\n[28] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial\nnetworks. In International Conference on Learning Representations (ICLR), 2019.\n[29] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision\n(ECCV), 2020.\n[30] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transformations for\nvideo inpainting. In European Conference on Computer Vision (ECCV), 2020.\n[31] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv√©\nJ√©gou. Training data-efÔ¨Åcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[34] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv√© J√©gou. Going\ndeeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[35] S. Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S. Duncan, Bram Van Ginneken, Anant\nMadabhushi, Jerry L. Prince, Daniel Rueckert, and Ronald M. Summers. A review of deep learning in\nmedical imaging: Imaging traits, technology trends, case studies with progress highlights, and future\npromises. Proceedings of the IEEE, 2021.\n[36] Gonglei Shi, Li Xiao, Yang Chen, and S Kevin Zhou. Marginal loss and exclusion loss for partially\nsupervised multi-organ segmentation. Medical Image Analysis, 2021.\n[37] Qingsong Yao, Li Xiao, Peihang Liu, and S Kevin Zhou. Label-free segmentation of covid-19 lesions in\nlung ct. IEEE Transactions on Medical Imaging, 2021.\n[38] Jiuwen Zhu, Yuexiang Li, Yifan Hu, Kai Ma, S Kevin Zhou, and Yefeng Zheng. Rubik‚Äôs cube+: A\nself-supervised feature learning framework for 3d medical image analysis. Medical Image Analysis, 2020.\n[39] Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei, Philip Torr, Wayne Zhang, and Dahua Lin.\nVision transformer with progressive sampling. In IEEE International Conference on Computer Vision\n(ICCV), 2021.\n[40] Pauline Luc, Camille Couprie, Soumith Chintala, and Jakob Verbeek. Semantic segmentation using\nadversarial networks. arXiv preprint arXiv:1611.08408, 2016.\n[41] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\ntechniques for training gans. In Advances in Neural Information Processing Systems (NeurIPS), 2016.\n11\n[42] Hayit Greenspan, Bram Van Ginneken, and Ronald M Summers. Guest editorial deep learning in medical\nimaging: Overview and future promise of an exciting new technique. IEEE Transactions on Medical\nImaging, 2016.\n[43] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi,\nMohsen Ghafoorian, Jeroen AWM van der Laak, Bram Van Ginneken, and Clara I S√°nchez. A survey on\ndeep learning in medical image analysis. Medical Image Analysis, 2017.\n[44] Yizhe Zhang, Lin Yang, Jianxu Chen, Maridel Fredericksen, David P Hughes, and Danny Z Chen. Deep\nadversarial networks for biomedical image segmentation utilizing unannotated images. In International\nConference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2017.\n[45] Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, and Pheng-Ann Heng. Semi-supervised skin lesion\nsegmentation via transformation consistent self-ensembling model. arXiv preprint arXiv:1808.03887,\n2018.\n[46] Dong Nie, Yaozong Gao, Li Wang, and Dinggang Shen. Asdnet: Attention based semi-supervised deep\nnetworks for medical image segmentation. In International Conference on Medical Image Computing\nand Computer-Assisted Intervention (MICCAI), 2018.\n[47] Yicheng Wu, Yong Xia, Yang Song, Yanning Zhang, and Weidong Cai. Multiscale network followed\nnetwork model for retinal vessel segmentation. In International Conference on Medical Image Computing\nand Computer-Assisted Intervention (MICCAI), 2018.\n[48] Lequan Yu, Shujun Wang, Xiaomeng Li, Chi-Wing Fu, and Pheng-Ann Heng. Uncertainty-aware self-\nensembling model for semi-supervised 3d left atrium segmentation. In International Conference on\nMedical Image Computing and Computer-Assisted Intervention (MICCAI), 2019.\n[49] Yu Zeng, Yunzhi Zhuge, Huchuan Lu, and Lihe Zhang. Joint learning of saliency detection and weakly\nsupervised semantic segmentation. In IEEE International Conference on Computer Vision (ICCV), 2019.\n[50] Gerda Bortsova, Florian Dubost, Laurens Hogeweg, Ioannis Katramados, and Marleen de Bruijne. Semi-\nsupervised medical image segmentation via learning consistency under transformations. In International\nConference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2019.\n[51] Yicheng Wu, Yong Xia, Yang Song, Donghao Zhang, Dongnan Liu, Chaoyi Zhang, and Weidong Cai.\nVessel-net: retinal vessel segmentation under multi-path supervision. In International Conference on\nMedical Image Computing and Computer-Assisted Intervention (MICCAI), 2019.\n[52] Chenyu You, Junlin Yang, Julius Chapiro, and James S. Duncan. Unsupervised wasserstein distance\nguided domain adaptation for 3d multi-domain liver segmentation. In Interpretable and Annotation-\nEfÔ¨Åcient Learning for Medical Image Computing , pages 155‚Äì163. Springer International Publishing,\n2020.\n[53] Linfeng Yang, Rajarshi P Ghosh, J Matthew Franklin, Simon Chen, Chenyu You, Raja R Narayan, Marc L\nMelcher, and Jan T Liphardt. Nuset: A deep learning tool for reliably separating and analyzing crowded\ncells. PLoS computational biology, 2020.\n[54] Shanlin Sun, Kun Han, Deying Kong, Chenyu You, and Xiaohui Xie. Mirnf: Medical image registration\nvia neural Ô¨Åelds. arXiv preprint arXiv:2206.03111, 2022.\n[55] Xiaoran Zhang, Chenyu You, Shawn Ahn, Juntang Zhuang, Lawrence Staib, and James Duncan. Learning\ncorrespondences of cardiac motion from images using biomechanics-informed modeling. arXiv preprint\narXiv:2209.00726, 2022.\n[56] Chenyu You, Ruihan Zhao, Lawrence Staib, and James S Duncan. Momentum contrastive voxel-wise\nrepresentation learning for semi-supervised volumetric medical image segmentation. arXiv preprint\narXiv:2105.07059, 2021.\n[57] Chenyu You, Yuan Zhou, Ruihan Zhao, Lawrence Staib, and James S Duncan. Simcvd: Simple contrastive\nvoxel-wise representation distillation for semi-supervised medical image segmentation.IEEE Transactions\non Medical Imaging, 2022.\n[58] Chenyu You, Jinlin Xiang, Kun Su, Xiaoran Zhang, Siyuan Dong, John Onofrey, Lawrence Staib, and\nJames S Duncan. Incremental learning meets transfer learning: Application to multi-site prostate mri\nsegmentation. arXiv preprint arXiv:2206.01369, 2022.\n12\n[59] Chenyu You, Weicheng Dai, Lawrence Staib, and James S Duncan. Bootstrapping semi-supervised medi-\ncal image segmentation with anatomical-aware contrastive distillation. arXiv preprint arXiv:2206.02307,\n2022.\n[60] Chenyu You, Weicheng Dai, Fenglin Liu, Haoran Su, Xiaoran Zhang, Lawrence Staib, and James S\nDuncan. Mine your own anatomy: Revisiting medical image segmentation with extremely limited labels.\narXiv preprint arXiv:2209.13476, 2022.\n[61] Krishna Chaitanya, Ertunc Erdil, Neerav Karani, and Ender Konukoglu. Contrastive learning of global\nand local features for medical image segmentation with limited annotations. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2020.\n[62] Yuexing Han, Xiaolong Li, Bing Wang, and Lu Wang. Boundary loss-based 2.5 d fully convolutional\nneural networks approach for segmentation: A case study of the liver and tumor on computed tomography.\nAlgorithms, 2021.\n[63] Konstantinos Kamnitsas, Christian Ledig, Virginia FJ Newcombe, Joanna P Simpson, Andrew D Kane,\nDavid K Menon, Daniel Rueckert, and Ben Glocker. EfÔ¨Åcient multi-scale 3d cnn with fully connected crf\nfor accurate brain lesion segmentation. Medical Image Analysis, 2017.\n[64] John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random Ô¨Åelds: Probabilistic\nmodels for segmenting and labeling sequence data. In International Conference on Machine Learning\n(ICML), 2001.\n[65] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman,\nHolger Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. arXiv preprint\narXiv:2103.10504, 2021.\n[66] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein. nnu-net: a\nself-conÔ¨Åguring method for deep learning-based biomedical image segmentation. Nature methods, 2021.\n[67] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840, 2020.\n[68] Georg Hille, Shubham Agrawal, Christian Wybranski, Maciej Pech, Alexey Surov, and Sylvia Saalfeld.\nJoint liver and hepatic lesion segmentation using a hybrid cnn with transformer layers. arXiv preprint\narXiv:2201.10981, 2022.\n[69] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman,\nHolger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In IEEE\nWinter Conference on Applications of Computer Vision (WACV), 2022.\n[70] Yucheng Tang, Dong Yang, Wenqi Li, Holger Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and\nAli Hatamizadeh. Self-supervised pre-training of swin transformers for 3d medical image analysis. arXiv\npreprint arXiv:2111.14791, 2021.\n[71] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two transformers can make one strong gan.\narXiv preprint arXiv:2102.07074, 2021.\n[72] Yu Zeng, Zhe Lin, and Vishal M Patel. Sketchedit: Mask-free local image manipulation with partial\nsketches. arXiv preprint arXiv:2111.15078, 2021.\n[73] Yu Zeng, Zhe Lin, Huchuan Lu, and Vishal M Patel. Cr-Ô¨Åll: Generative image inpainting with auxiliary\ncontextual reconstruction. In IEEE International Conference on Computer Vision (ICCV), 2021.\n[74] Dor Arad Hudson and Larry Zitnick. Compositional transformers for scene generation. In Advances in\nNeural Information Processing Systems (NeurIPS), 2021.\n[75] Chenyu You, Guang Li, Yi Zhang, Xiaoliu Zhang, Hongming Shan, Mengzhou Li, Shenghong Ju, Zhen\nZhao, Zhuiyang Zhang, Wenxiang Cong, et al. CT super-resolution GAN constrained by the identical,\nresidual, and cycle learning ensemble (gan-circle). IEEE Transactions on Medical Imaging, 2019.\n[76] Long Zhao, Zizhao Zhang, Ting Chen, Dimitris N Metaxas, and Han Zhang. Improved transformer for\nhigh-resolution gans. arXiv preprint arXiv:2106.07631, 2021.\n[77] Yanhong Zeng, Huan Yang, Hongyang Chao, Jianbo Wang, and Jianlong Fu. Improving visual quality\nof image synthesis by a token-based generator with transformers. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2021.\n13\n[78] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention\nfor all data sizes. arXiv preprint arXiv:2106.04803, 2021.\n[79] Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2961‚Äì2969, 2017.\n[80] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-\ndecoder with atrous separable convolution for semantic image segmentation. In European Conference on\nComputer Vision (ECCV), 2018.\n[81] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters‚Äìimprove\nsemantic segmentation by global convolutional network. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017.\n[82] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint\narXiv:1511.07122, 2015.\n[83] Tete Xiao, Piotr Dollar, Mannat Singh, Eric Mintun, Trevor Darrell, and Ross Girshick. Early convolutions\nhelp transformers see better. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n[84] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll√°r. Focal loss for dense object\ndetection. In IEEE International Conference on Computer Vision (ICCV), 2017.\n[85] Tsung-Yi Lin, Piotr Doll√°r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2017.\n[86] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Seg-\nformer: Simple and efÔ¨Åcient design for semantic segmentation with transformers. arXiv preprint\narXiv:2105.15203, 2021.\n[87] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. InAdvances in Neural Information Processing\nSystems (NeurIPS), 2014.\n[88] Martin Arjovsky, Soumith Chintala, and L√©on Bottou. Wasserstein generative adversarial networks. In\nInternational Conference on Machine Learning (ICML), 2017.\n[89] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved\ntraining of wasserstein gans. In Advances in Neural Information Processing Systems (NeurIPS), 2017.\n[90] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference\non Learning Representations (ICLR), 2019.\n[91] Jo Schlemper, Ozan Oktay, Michiel Schaap, Mattias Heinrich, Bernhard Kainz, Ben Glocker, and Daniel\nRueckert. Attention gated networks: Learning to leverage salient regions in medical images. Medical\nImage Analysis, 2019.\n[92] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2021.\n[93] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In CVPR Workshops, 2020.\n[94] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking\nthe inception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016.\n[95] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. In International Conference on Learning Representations (ICLR), 2018.\n[96] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classiÔ¨Åers with localizable features. In IEEE International\nConference on Computer Vision (ICCV), 2019.\n[97] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 1992.\n14\n[98] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created\nequal? a large-scale study. arXiv preprint arXiv:1711.10337, 2017.\n[99] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least\nsquares generative adversarial networks. In IEEE International Conference on Computer Vision (ICCV),\n2017.\n[100] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable\nconvolutional networks. In IEEE International Conference on Computer Vision (ICCV), 2017.\n[101] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n[102] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision transformer with deformable\nattention. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4794‚Äì4803,\n2022.\n[103] Alex Zwanenburg, Martin Valli√®res, Mahmoud A Abdalah, Hugo JWL Aerts, Vincent Andrearczyk,\nAditya Apte, Saeed AshraÔ¨Ånia, Spyridon Bakas, Roelof J Beukinga, Ronald Boellaard, et al. The image\nbiomarker standardization initiative: standardized quantitative radiomics for high-throughput image-based\nphenotyping. Radiology, 295(2):328‚Äì338, 2020.\n[104] Joost JM Van Griethuysen, Andriy Fedorov, Chintan Parmar, Ahmed Hosny, Nicole Aucoin, Vivek\nNarayan, Regina GH Beets-Tan, Jean-Christophe Fillion-Robin, Steve Pieper, and Hugo JWL Aerts.\nComputational radiomics system to decode the radiographic phenotype. Cancer research, 77(21):e104‚Äì\ne107, 2017.\n[105] Ahmet Saygƒ±lƒ±. A new approach for computer-aided detection of coronavirus (covid-19) from ct and x-ray\nimages using machine learning methods. Applied Soft Computing, 105:107323, 2021.\n[106] Feng Shi, Liming Xia, Fei Shan, Bin Song, Dijia Wu, Ying Wei, Huan Yuan, Huiting Jiang, Yichu He,\nYaozong Gao, et al. Large-scale screening to distinguish between covid-19 and community-acquired\npneumonia using infection size-aware classiÔ¨Åcation. Physics in medicine & Biology, 66(6):065031, 2021.\n15\nAppendix to\nClass-Aware Adversarial Transformers\nfor Medical Image Segmentation\nSec. A provides additional details on the training datasets.\nSec. B provides additional details on the implementation.\nSec. C provides additional model details.\nSec. D provides more quantitative results on the LiTS dataset.\nSec. E provides more experimental results on the MP-MRI dataset.\nSec. F provides additional analysis on Iteration Number N.\nSec. G provides additional analysis on sampling number n.\nSec. H provides additional details on the hyperparameter selection.\nSec. I provides additional GAN-based loss details.\nSec. J takes a deeper look and understand our proposed class-aware transformer module.\nSec. K forms a better understand the attention mechanism in our proposed CASTformer.\nA Datasets\nSynapse: Synapse multi-organ segmentation dataset includes 30 abdominal CT scans with 3779 axial\ncontrast-enhanced abdominal clinical CT images. Each CT volume consists of 85 ‚àº198 slices of\n512√ó512 pixels, with a voxel spatial resolution of([0.54 ‚àº0.54]√ó[0.98 ‚àº0.98]√ó[2.5 ‚àº5.0])mm3.\nThe dataset is randomly divided into 18 volumes for training (2212 axial slices), and 12 for validation.\nFor each case, 8 anatomical structures are aorta, gallbladder, spleen, left kidney, right kidney, liver,\npancreas, spleen, stomach.\nLiTS: MICCAI 2017 Liver Tumor Segmentation Challenge (LiTS) includes 131 contrast-enhanced\n3D abdominal CT volumes for training and testing. The dataset is assembled by different scanners\nand protocols from seven hospitals and research institutions. The image resolution ranges from\n0.56mm to 1.0mm in axial and 0.45mm to 6.0mm in z direction. The dataset is randomly divided\ninto 100 volumes for training, and 31 for testing.\nMP-MRI: Multi-phasic MRI dataset is an in-house dataset including multi-phasic MRI scans of 20\nlocal patients with HCC, each of which consisted of T1 weighted DCE-MRI images at three-time\npoints (pre-contrast, arterial phase, and venous phases). Three images are mutually registered to the\narterial phase images, with an isotropic voxel size of 1.00 mm. The dataset is randomly divided into\n48 volumes for training, and 12 for testing.\nB More Implementation Details\nThe training conÔ¨Åguration and hyperparameter settings are summarized in Table 4.\nC Model Architecture\nWe present the detailed architecture ofCATformer‚Äôs encoding pipeline in Section 5. We use input/out-\nput names to indicate the direction of the data stream. CATformer applies independent class-aware\nattention on 4 levels of features extracted by theResNetV2 model. Each feature level L-kis processed\nby CATformer-k, consisting of 4 blocks of class-aware transformer modules, followed by 12 layers\nof transformer encoder modules. Outputs from all four feature levels are fed into the decoder pipeline\nto generate the segmentation masks.\nD More Experiments: LiTS\nExperimental results are summarized in Table 6.\n16\nTable 4: Training conÔ¨Åguration and hyperparameter settings.\nTraining ConÔ¨Åg Hyperparameter\nOptimizer AdamW\nBase learning rate 5e-4\nWeight decay 0.05\nOptimizer momentum Œ≤1,Œ≤2=0.9,0.999\nBatch size 6\nTraining epochs 300\nLearning rate schedule cosine decay\nWarmup epochs 5\nWarmup schedule linear\nRandaugment [93] (9, 0.5)\nLabel smoothing [94] 0.1\nMixup [95] 0.8\nCutmix [96] 1.0\nGradient clip None\nExp. mov. avg. (EMA) [97] None\nTable 5: Architecture conÔ¨Åguration of CATformer\nCATformer\nStage Layer Input Name Input Shape Output Name Output Shape\nEncoder ResNetV2 Original Image 224√ó224√ó3\nRN-L1 112√ó112√ó64\nRN-L2 56√ó56√ó256\nRN-L3 28√ó28√ó512\nRN-L4 14√ó14√ó1024\nCATformer-1 CAT√ó4 RN-L1 112√ó112√ó64 CAT-1 (28√ó28)√ó64\nTEM√ó12 CAT-1 (28√ó28)√ó64 F1 (28√ó28)√ó64\nCATformer-2 CAT√ó4 RN-L2 56√ó56√ó256 CAT-2 (28√ó28)√ó256\nTEM√ó12 CAT-2 (28√ó28)√ó256 F2 (28√ó28)√ó256\nCATformer-3 CAT√ó4 RN-L3 28√ó28√ó512 CAT-3 (28√ó28)√ó512\nTEM√ó12 CAT-3 (28√ó28)√ó512 F3 (28√ó28)√ó512\nCATformer-4 CAT√ó4 RN-L4 14√ó14√ó768 CAT-4 (14√ó14)√ó768\nTEM√ó12 CAT-4 (14√ó14)√ó768 F4 (14√ó14)√ó768\nE More Experiments: MP-MRI\nExperimental results are summarized in Table 7. Overall, CATformer and CASTformer outper-\nform the previous results in terms of Dice and Jaccard. Compared to SETR, our CATformer and\nCASTformer perform 1.78% and 2.54% higher in Dice, respectively. We also Ô¨Ånd CASTformer\nperforms better than CATformer, which suggests that using discriminator can make the model better\nassess the medical image Ô¨Ådelity. Figure 5 shows qualitative results, where our CATformer and\nCASTformer provide better anatomical details than all other methods. This clearly demonstrates the\nsuperiority of our models. All these experiments are conducted using the same hyperparameters in\nour CASTformer.\nF Effect of Iteration Number N\nWe explore the effect of different iteration number N in Figure 6 (a). Note that in the case of N = 1,\nthe sampling locations will not be updated. We Ô¨Ånd that more iterations of sampling clearly improve\nnetwork performance in Dice and Jaccard. However, we observe that the network performance\ndoes not further increase from N = 4to N = 6. In our study, we use N = 4for the class-aware\ntransformer module.\n17\nTable 6: Quantitative segmentation results on the LiTS dataset.\nFramework Average Liver Tumor\nEncoder Decoder DSC ‚ÜëJaccard‚Üë95HD‚ÜìASD‚Üì\nUNet[27] 62.88 54.64 57.59 27.74 88.27 37.49\nAttnUNet[91] 66.03 58.49 31.34 16.15 92.26 39.81\nResNet50 UNet [27] 65.25 58.09 27.97 10.02 93.78 36.73\nResNet50 AttnUNet[91] 66.22 59.27 31.47 10.41 93.26 39.18\nSETR[92] 54.79 49.21 36.34 15.04 91.69 17.90\nCoTrw/oCNN-encoder[13] 53.35 47.11 55.82 22.99 85.25 21.45\nCoTr[13] 62.67 55.43 34.75 15.84 89.43 35.92\nTransUNet[7] 67.94 60.25 29.32 12.45 93.40 42.49\nSwinUNet[10] 65.53 57.84 36.45 16.52 92.15 38.92\n‚Ä¢CATformer(ours) 72.39 62.76 22.38 11.5794.1849.60\n‚ó¶CASTformer(ours) 73.82 64.91 23.35 10.1695.8851.76\nGround TruthUNet AttUNet\nSwin-UNetCATformerCASTformer\nR50+UNet\n SETR\nCoTr\n TransUNet\nliver\nFigure 5: Visual comparisons with other methods on MP-MRI dataset.\nG Effect of Sampling Number n\nWe further evaluate the effect of sampling number nof the class-aware transformer module in Figure\n6 (b). Empirically, we observe that results are generally well correlated when we gradually increase\nthe size of n. As is shown, the network performance is optimal when n= 16.\n(a) (b)\nFigure 6: Effects of the iteration number N and the sampling number nin the class-aware transformer\nmodule. We report Dice and Jarrcd of CATformer on the Synapse multi-organ dataset.\nH Hyperparameter Selection\nWe carry out grid-search of Œª1,Œª2,Œª3 ‚àà{0.0,0.1,0.2,0.5,1.0}. As shown in Figure 7, with a\ncarefully tuned hyperparameters Œª1 = 0.5, Œª2 = 0.5, and Œª3 = 0.1, such setting performs generally\nbetter than others.\n18\nTable 7: Quantitative segmentation results on the MP-MRI dataset.\nFramework Average\nEncoder Decoder DSC ‚ÜëJaccard‚Üë95HD‚ÜìASD‚Üì\nUNet[27] 88.38 79.42 39.23 11.14\nAttnUNet[91] 89.79 81.51 30.13 7.85\nResNet50 UNet [27] 91.51 84.39 15.38 4.53\nResNet50 AttnUNet[91] 91.43 84.24 14.14 4.24\nSETR[92] 92.39 85.89 7.66 3.79\nCoTrw/oCNN-encoder[13] 85.21 74.49 44.25 12.58\nCoTr[13] 90.06 81.94 28.91 7.89\nTransUNet[7] 92.08 85.36 23.17 6.03\nSwinUNet[10] 92.07 85.32 7.62 3.88\n‚Ä¢CATformer(ours) 94.17 86.50 6.55 3.33\n‚ó¶CASTformer(ours) 94.93 87.81 8.29 3.02\n(a) (b) (c)\nFigure 7: Effects of hyperparameters Œª1,Œª2,Œª3. We report Dice and Jarrcd of CASTformer on the\nSynapse multi-organ dataset.\nI Importance of Loss Functions\nOne main argument for the discriminator is that modeling long-range dependencies and acquiring\na more holistic understanding of the anatomical visual information can contribute to the improved\ncapability of the generator. Besides the WGAN-GP loss [ 89], the minimax (MM) GAN loss [87],\nthe Non-Saturating (NS) GAN loss [98], and Least Squares (LS) GAN Loss [99] are also commonly\nused as adversarial training. We test these alternatives and Ô¨Ånd that, in most cases, using WGAN-GP\nloss achieves comparable or higher performance than other loss functions. In addition, models trained\nusing MM-GAN loss perform comparably to those trained using LS-GAN loss. In particular, our\napproach outperforms the second-best LS-GAN loss [99] by 1.10 and 2.49 points in Dice and Jaccard\nscores on the Synapse multi-organ dataset. It demonstrates the effectiveness of the WGAN-GP loss\nin our CASTformer.\nTable 8: Ablation on Loss Function: MM-GAN loss [87]; NS-GAN loss [98]; LS-GAN loss [99];\nand WGAN-GP loss [89].\nModel DSC Jaccard 95HD ASD\nMM-GAN loss [87]81.19 71.76 20.75 5.90\nNS-GAN loss [98] 80.02 70.47 26.06 6.96\nLS-GAN loss [99] 81.45 72.20 20.39 6.49\nWGAN-GP loss [89]82.55 74.69 22.73 5.81\nJ Visualization of Learned Sampling Location\nTo gain more insight into the evolving sampling locations learned by our proposed class-aware\ntransformer module, we visualize the predicted offsets in Figure 8. We can see that particular\nsampling points around objects tend to attend to coherent segmented regions in terms of anatomical\nsimilarity and proximity. As is shown, we show the classes with the highly semantically correlated\nregions, indicating that the model coherently attends to anatomical concepts such as liver, right/left\nkidney, and spleen. These visualizations also illustrate how it behaves adaptively and distinctively\nto focus on the content with highly semantically correlated discriminative regions ( i.e., different\norgans). These Ô¨Åndings can thereby suggest that our design can aid the CATformer to exercise Ô¨Åner\n19\naortagallbladderleft kidneyrightkidneyliverpancreasspleenstomach\nFigure 8: Visualization of sampled locations in the proposed class-aware transformer module.\ncontrol emphasizing anatomical features with the intrinsic structure at the object granularity. As is\nindicated (Figure 8 last column), we also Ô¨Ånd evidence that our model is prone to capture some small\nobject cases (e.g., pancreas, aorta, gallbladder). We hypothesize that it is because they contain more\nanatomical variances, which makes the model more difÔ¨Åcult to exploit.\nK Vision Transformer Visualization\nIn this section, we visualize the Ô¨Årst 12 class-aware transformer layers on sequences of28√ó28 feature\npatches in the encoder pipeline. In Figure 9, we plot the attention probabilities from a single patch\nover different layers and heads. Each row corresponds to one CAT layer; each column corresponds to\nan attention head. As we go deeper into the network, we are able to observe three kinds of attention\nbehaviors as further discussed below.\nAttend to similar features: In the Ô¨Årst group of layers (layer 1 through 4), the attention probability\nis spread across a relatively large group of patches. Notably, these patches correspond to areas in the\nimage with similar color and texture to the query patch. These more primitive attention distributions\nindicate that the class-awareness property has not yet been established.\nAttend to the same class and its boundary: In the middle layers of the transformer model, most\nnoticeable in the 5th and 6th layers, the attention probabilities start to concentrate on areas that share\nthe same class label as the query patch (layer 5-2). In some other instances, the model attends to the\nboundary of the current class (layer 5-3, 5-6).\nAttend to other classes: In the deeper layers of the model, the attention probability mainly concen-\ntrates on other classes. This clearly demonstrates persuasive evidence that the model establishes class\nawareness, which is helpful in the downstream medical segmentation tasks.\nL More Ablations on Decoder Modules\nIn this section, we explore another state-of-the-art backbone proposed by Lin et al. [85], termed\nFeature Pyramid Network (FPN). FPN utilizes a top-down pyramid with lateral connections to construct\n20\nImage\nLayer 1Layer 2Layer 3Layer 4Layer 5Layer 6Layer 7Layer 8Layer 9Layer 10Layer 11Layer 12\nFigure 9: Attention probability of our 12 class-aware transformer layers, each with 8 heads. The\nblack box marks the query patch. The input image, ground truth and predicted label are shown on the\nÔ¨Årst row.\nthe semantically strong multi-scale feature pyramid from a single-scale input. The major differences\nbetween FPN and our work are as follows:\n‚Ä¢ The former utilizes a CNN-based decoder ( FPN [85]), and ours uses an All-MLP-based\ndecoder. In particular, our motivation comes from the observation that the attention of\nlower layers tends to be local, and those of the higher layers are highly non-local [ 86].\nAs the decoder design plays an important role in determining the semantic level of the\nlatent representations [9] and Transformers have the larger receptive Ô¨Åelds compared to\nCNNs, how to use large receptive Ô¨Åelds to include context information is the key issue\n21\n[86, 82, 81, 80, 52, 56, 57, 59, 58, 60]. Prior work [86] suggests that the use of MLP-based\ndecoder design can be a very effective tool in learning additional contextual information to\nbuild powerful representations. The key idea is to essentially take beneÔ¨Åts of the Transformer-\ninduced features by leveraging the local attention at the lower layers and highly non-local\n(global) attention at the higher layers to formulate the powerful representations [86]. To this\nend, we utilize an MLP-based decoder instead of a CNN-based decoder to preserve more\ncontextual information, speciÔ¨Åcally for medical imaging data, including more anatomical\nvariances.\n‚Ä¢ We devise the class-aware transformer module to progressively learn interesting anatomical\nregions correlated with semantic structures of images, so as to guide the segmentation of\nobjects or entities. We study the model‚Äôs qualitative behavior through learnable sampling\nlocations inside the class-aware module in Figure 8. As indicated, sampling locations are\nadaptively adjusted according to the interesting regions.\nThe table below shows the comparision results of using an FPN decoder, MLP-based decoder, and\nthe class-aware transformer (CAT) module, all of which include the backbone feature extractor\n(ResNet50), on the Synapse multi-organ CT dataset. All the experiments are conducted under\nthe same experimental setting in Section 4. As we can see, adopting the MLP-based decoder can\noutperform the state-of-the-art FPN decoder in terms of DSC, Jaccard, 95HD, and ASD, respectively.\nSimilarly, incorporating the CAT module can also consistently improve the segmentation performance\nby a large margin on the Synapse multi-organ CT dataset. The results prove the robustness of\nour MLP-based decoder and the effectiveness of our proposed CAT module for medical image\nsegmentation.\nTable 9: Ablation on Decoder Modules: FPN decoder [85]; MLP-based decoder; and Class-Aware\nTransformer (CAT) module.\nEncoder Decoder DSC Jaccard 95HD ASD\nResNet50w/o CAT FPN 74.64 63.91 29.54 8.81ResNet50w/ CAT FPN 78.11 65.63 28.06 8.08ResNet50w/o CAT MLP 80.09 70.56 25.62 7.30ResNet50w/ CAT MLP 82.17 73.22 16.20 4.28\nM More Ablations on Segmentation Losses\nTo deal with the imbalanced medical image segmentation, Lin et al. [84] proposed Focal loss in\nterms of the standard cross entropy to address the extreme foreground-background class imbalance\nby focusing on the hard pixel examples. The table below shows the results of the loss function. We\nfollow Œ≥ = 2in the original paper. As we can see, the setting using Focal loss and the other ( i.e.,\nDice + Cross-Entropy) achieve similar performances.\nTable 10: Ablation on Segmentation Losses: Focal loss [84]; Dice loss; and Cross-Entropy loss.\nModel DSC Jaccard 95HD ASD\nFocal loss [84] 82.08 73.52 16.14 4.99Dice + Focal loss [84] 81.88 72.94 16.52 5.00Dice + Cross-Entropy loss (ours)82.17 73.22 16.20 4.28\nN More Ablations on Sampling Modules\nIn this section, we investigate the effect of recent state-of-the-art sampling modules [ 100‚Äì102].\nHowever, the motivation and the sampling strategy are different from these works [100‚Äì102]. Our\nmotivation comes from the accurate and reliable clinical diagnosis that rely on the meaningful\nradiomic features from the correct ‚Äúregion of interest‚Äù instead of other irrelevant parts [103‚Äì106].\nThe process of extracting different radiomic features from medical images is done in a progressive\nand adaptive manner [104, 105].\nDCN [100] proposed to learn 2D spatial offsets to enable the CNN-based model to generalize the\ncapability of regular convolutions. Because CNNs only have limited receptive Ô¨Åelds compared to\nTransformers, DCN focuses on local information around a certain point of interest. In contrast, our\nCATformer/CASTformer take beneÔ¨Åts of the Transformer-induced features by leveraging the local\n22\nattention at the lower layers and highly non-local (global) attention at the higher layers to formulate\nthe powerful representations.\nDeformable DETR[101] incorporated the deformation attention to focus on a sparse set of keys (i.e.,\nglobal keys are not shared among visual tokens). This is particularly useful for its original experiment\nsetup on object detection. Since there are only a handful of query features corresponding to potential\nobject classes, deformable DETR learns different attention locations for each class. In contrast, our\napproach aims at reÔ¨Åning the anatomical tokens for medical image segmentation. To this end, we\nproposed to iteratively and adaptively focus on the most discriminative region of interests. This\nessentially allows us to obtain effective anatomical features from spatial attended regions within the\nmedical images, so as to guide the segmentation of objects or entities\nDAT [102] introduced deformable attention to make use of global information (i.e., global keys are\nshared among visual tokens) by placing a set of the supporting points uniformly on the feature maps.\nIn contrast, our approach introduces an iterative and progressive sampling strategy to capture the\nmost discriminative region and avoid over-partition anatomical features.\nThe table below shows the comparison results between DCN [100], Deformable DETR[101], DAT\n[102], and ours ( CATformer/CASTformer) on the Synapse multi-organ CT dataset. As we can\nsee, our approach (i.e., CATformer/CASTformer) can outperform existing state-of-the-art models,\ni.e.,DCN [100], and Deformable DETR.\nTable 11: Ablation on Sampling Module: DCN [100], Deformable DETR[101], DAT [102], and ours\n(CATformer/CASTformer).\nModel DSC Jaccard 95HD ASD\nDCN[100] 73.19 62.81 33.46 10.22\nDeformable DETR[101] 79.13 66.58 30.21 8.65\nDAT[102] 80.34 68.15 26.14 7.76\nCATformer(ours) 82.17 73.22 16.20 4.28\nCASTformer(ours) 82.55 74.69 22.73 5.81\nO More Ablations on Architecture Backbone\nIn this section, we conduct the ablation study on the Synapse multi-organ CT dataset to compare\nour approach with the recent state-of-the-art architecture (SwinUnet) [10]. The table below shows\nthe results of our proposed architecture ( e.g., Swin-class-aware transformer (Swin-CAT) module,\nmulti-scale feature extraction module) are superior compared to the other state-of-the-art method on\nthe Synapse multi-organ CT dataset. All the experiments are conducted under the same experimental\nsetting in Section 4. For brevity, we refer our CATformer and CASTformer using SwinUnet as\nthe backbone to Swin-CATformer and Swin-CASTformer. As we can see, using SwinUnet as\nthe backbone, the following observations can be drawn: (1) ‚Äúw/ pre-trained‚Äù consistently achieves\nsigniÔ¨Åcant performance gains compared to the ‚Äúw/o pre-trained‚Äù, which demonstrates the effectiveness\nof the pre-training strategy; (2) we can Ô¨Ånd that incorporating the adversarial training can boost\nthe segmentation performance, which suggests the effectiveness of the adversarial training strategy;\nand (3) our Swin-CASTformer with different modules can also achieves consistently improved\nperformance. The results prove the superiority of our proposed method on the medical image\nsegmentation task.\nTable 12: Effect of transfer learning in ourSwin-CATformer and Swin-CASTformer on the Synapse\nmulti-organ dataset.\nModel DSC Jaccard 95HD ASD\n‚Ä¢Swin-CATformer(w/o pre-trained) 76.82 65.44 29.58 8.58\n‚Ä¢Swin-CATformer(w/ pre-trained) 80.19 70.61 22.66 6.02\n‚ó¶Swin-CASTformer(bothw/o pre-trained) 71.67 61.08 43.01 13.21\n‚ó¶Swin-CASTformer(onlyw/ pre-trainedD) 76.55 64.27 34.62 12.13\n‚ó¶Swin-CASTformer(onlyw/ pre-trainedG) 77.12 65.39 30.99 11.00\n‚ó¶Swin-CASTformer(bothw/ pre-trained) 80.49 71.19 23.94 6.91\n23\nTable 13: Ablation on model component: Baseline; Swin-CATformer w/o Swin-CAT;\nSwin-CATformer w/o multi-scale feature extraction; and Swin-CASTformer.\nModel DSC Jaccard 95HD ASD\nBaseline 76.33 65.64 27.16 8.32‚Ä¢Swin-CATformerw/o Swin-CAT 77.76 68.47 25.26 7.15‚Ä¢Swin-CATformerw/o multi-scale feature extraction78.45 78.26 24.94 7.08‚Ä¢Swin-CATformer 80.19 70.61 22.66 6.02‚ó¶Swin-CASTformer 80.49 71.19 23.94 6.91\n24",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7656946778297424
    },
    {
      "name": "Transformer",
      "score": 0.6605156660079956
    },
    {
      "name": "Segmentation",
      "score": 0.6322546601295471
    },
    {
      "name": "Discriminative model",
      "score": 0.6257097125053406
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6052525639533997
    },
    {
      "name": "Encoder",
      "score": 0.5301015377044678
    },
    {
      "name": "Discriminator",
      "score": 0.47835320234298706
    },
    {
      "name": "Adversarial system",
      "score": 0.4349827468395233
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36714664101600647
    },
    {
      "name": "Machine learning",
      "score": 0.34995394945144653
    },
    {
      "name": "Computer vision",
      "score": 0.32124602794647217
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ],
  "cited_by": 78
}