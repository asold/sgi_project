{
  "title": "S2 Transformer for Image Captioning",
  "url": "https://openalex.org/W4285602612",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2755773801",
      "name": "Pengpeng Zeng",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2117412413",
      "name": "Haonan. Zhang",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2127452354",
      "name": "Jingkuan Song",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2166790520",
      "name": "Lianli Gao",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034339250",
    "https://openalex.org/W2481240925",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W3167939936",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W4300614726",
    "https://openalex.org/W4289542422",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3035497460",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2463955103",
    "https://openalex.org/W3035284526",
    "https://openalex.org/W2885013662",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4288329833",
    "https://openalex.org/W3187679738",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W2901988662",
    "https://openalex.org/W3191818374",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3034655362"
  ],
  "abstract": "Transformer-based architectures with grid features represent the state-of-the-art in visual and language reasoning tasks, such as visual question answering and image-text matching. However, directly applying them to image captioning may result in spatial and fine-grained semantic information loss. Their applicability to image captioning is still largely under-explored. Towards this goal, we propose a simple yet effective method, Spatial- and Scale-aware Transformer (S2 Transformer) for image captioning. Specifically, we firstly propose a Spatial-aware Pseudo-supervised (SP) module, which resorts to feature clustering to help preserve spatial information for grid features. Next, to maintain the model size and produce superior results, we build a simple weighted residual connection, named Scale-wise Reinforcement (SR) module, to simultaneously explore both low- and high-level encoded features with rich semantics. Extensive experiments on the MSCOCO benchmark demonstrate that our method achieves new state-of-art performance without bringing excessive parameters compared with the vanilla transformer. The source code is available at https://github.com/zchoi/S2-Transformer",
  "full_text": "S2 Transformer for Image Captioning\nPengpeng Zeng\u0003, Haonan Zhang\u0003, Jingkuan Songy, Lianli Gao\nSchool of Computer Science and Engineering and Shenzhen Institute for Advanced Study,\nUniversity of Electronic Science and Technology of China, Chengdu, China.\n{is.pengpengzeng, zchiowal, jingkuan.song}@gmail.com and{lianli.gao}@uestc.edu.cn\nAbstract\nTransformer-based architectures with grid features\nrepresent the state-of-the-art in visual and lan-\nguage reasoning tasks, such as visual question an-\nswering and image-text matching. However, di-\nrectly applying them to image captioning may re-\nsult in spatial and ﬁne-grained semantic informa-\ntion loss. Their applicability to image captioning\nis still largely under-explored. Towards this goal,\nwe propose a simple yet effective method, Spatial-\nand Scale-aware Transformer (S2 Transformer) for\nimage captioning. Speciﬁcally, we ﬁrstly propose\na Spatial-aware Pseudo-supervised (SP ) module,\nwhich resorts to feature clustering to help preserve\nspatial information for grid features. Next, to main-\ntain the model size and produce superior results,\nwe build a simple weighted residual connection,\nnamed Scale-wise Reinforcement (SR) module, to\nsimultaneously explore both low- and high-level\nencoded features with rich semantics. Extensive\nexperiments on the MSCOCO benchmark demon-\nstrate that our method achieves new state-of-art per-\nformance without bringing excessive parameters\ncompared with the vanilla transformer. The source\ncode is available at https://github.com/zchoi/S2-\nTransformer.\n1 Introduction\nAs a fundamental task of visual and language reasoning,\nimage captioning, which automatically generates a natu-\nral language description for an image, has attracted exten-\nsive attention [Vinyals et al., 2016; Cornia et al., 2019;\nWang et al., 2020; Chenet al., 2021]. Originally inspired by\nneural machine translation[Sutskever et al., 2014], its general\nparadigm is: ﬁrstly encoding an image to extract visual fea-\ntures, and then feeding those features into an encoder-decoder\nframework to generate descriptions [Xu et al., 2015]. Due to\nits speciﬁc proprieties, such as rich visual information and so-\nphisticated semantics of descriptions, it remain a challenging\nproblem.\n\u0003Pengpeng Zeng and Haonan Zhang contribute equally to this\npaper.\nyCorresponding author: Jingkuan Song.\nFor visual feature extracting, two types of features are\nwidely adopted: region and grid features, as shown in Fig. 1a\n(i) and (ii), respectively. The region features are designed\nto explore object instances, which strongly correlate with\nnouns in textual descriptions (e.g., “giraffe”, “grass” and\n“tree”). To detect explicit object boxes and output region fea-\ntures, existing off-the-shelf methods such as Faster-RCNN\n[Ren et al., 2015] are pre-trained on VG dataset [Krishna\net al., 2017 ], which is computationally expensive and not\nﬂexible. Beyond that, the detected regions may lack con-\ntextual information (e.g., “stands on” and “in the forest”)\nand ﬁne-grained details (e.g., “eat leaves”). By contrast, the\ngrid features are designed to extract all patch information to\ncover the whole image. Previous studies [Jiang et al., 2020;\nZhang et al., 2021] revise the advantage of grid features and\nﬁnd them to perform better than region features both in terms\nof performance and time-cost. However, directly operating\nat grid features in a ﬂatting manner unavoidably disrupts the\nspatial association between grids. One natural solution is to\ncombine the above two visual features as visual inputs, but\nit suffers from computation costs and complex fusion proce-\ndures.\nFurthermore, transformer-based models are applied as the\nencoder-decoder for high quality image captioning [Li et al.,\n2019; Pan et al., 2020; Zhang et al., 2021] due to its strong\nmodeling capabilities and excellent performance, shown as\nFig. 1b (i). Most of them are focused on modifying the at-\ntention block. For example, [Huang et al., 2019] proposes an\n“attention on attention” module, which extends self-attention\nmechanisms to determine the relevance between attention re-\nsults and queries. [Pan et al., 2020] proposes a X-Linear at-\ntention block that fully employs bilinear pooling to capitalize\non visual information or perform multi-modal reasoning se-\nlectively. [Cornia et al., 2020] proposes a M2 transformer\nthat designs a memory-augmented attention to encode a pri-\nori information and a mesh cross attention (MCA) to take\nadvantage of scale-wise features to fully explore rich visual\nsemantics, shown as Fig. 1b (ii). However, M2 transformer\n(w/o memory) based on grid feature has suffered a perfor-\nmance degradation and brought a parameters increase com-\npared with the vanilla transformer, where the results are sum-\nmarized in Fig. 1b (iv). Thus, how to effectively and efﬁ-\nciently incorporate grid features with transformer-based ar-\nchitecture remains to be explored for image captioning.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1608\n(i) Vanilla Transformer\n(iii) Ours\nEncoder 1\nEncoder 2\nEncoder L\nDecoder L\nDecoder 2\nDecoder 1\n...\n...\nSR\nEncoder 1\nEncoder 2\nEncoder L\nDecoder L\nDecoder 2\nDecoder 1\n...\n...\nEncoder 1\nEncoder 2\nEncoder L\nDecoder L\nDecoder 2\nDecoder 1\n...\n...\n（ii）M2  Transformer (w/o memory)\n(iv) Performance Comparison\n(i) Region Features (ii) Grid Features (iii) Pseudo-region \nFeatures (ours)\n Description：A giraffe stands on a grass in the forest and eats leaves from a tree.\n(a) Different visual features\n(b) Different transformer-based architectures\nModel CIDEr Param.\nVanilla  \nTransformer\n+MCA (M2)\n+SR (Ours)\n131.2 33.57M\n130.1(↓1.1) 38.29M(↑4.72)\n133.0(↑1.8) 34.62M(↑1.05)\nMCA\nMCA\nMCA\nFigure 1: (a) Comparison of different visual features. Based on grid\nfeatures, our proposed SP module aims to implicitly learn spatial\ninformation about grids in a pseudo-supervised manner instead of\ndirectly using explicit region features. (b) Comparison of different\ntransformer-based architectures, where all models adopt grid fea-\ntures as visual features. Our SR module simultaneously explores\nboth low- and high-level encoded features to produce superior re-\nsults while maintaining a relatively small model size.\nTo address the above problem, this paper proposes a novel\nSpatial- and Scale-aware Transformer (S 2 Transformer).\nSpeciﬁcally, we ﬁrstly propose a Spatial-aware Pseudo-\nsupervised (SP) module that aims to solve the spatial in-\nformation loss of grid features caused by the ﬂattening oper-\nation. In practice, we utilize a number of learnable semantic\nclusters to quantize grid features into semantic clusters, which\nimplicitly represent discriminative regions. Furthermore, to\nmaintain the model size and produce superior performance,\nwe propose a simple weighted residual connection, named\nScale-wise Reinforcement (SR), module to simultaneously\nexplore both low- and high-level encoded features, shown as\nFig. 1b (iii). From the Fig. 1b (iv), we can see that compared\nwith vanilla transformer, only adopting our SR can achieve\nan improvement of 1.8 CIDEr points with a slight parameters\nincrease (i.e., 1.05M), while M2 with a mesh operation in-\ncreases parameters (i.e., 4.72M) and decreases the CIDEr by\n1.1. To summarize, our contributions are threefold:\n• We devise a S2 Transformer, a simple yet effective\nmethod, which extends the vanilla transformer frame-\nwork to fully exploit gird visual features in terms of spa-\ntial and scale perception.\n• We propose a SP module, which generates valid\npseudo-region features for grid features to capture spa-\ntial information based on their clustering information.\nMoreover, we propose a simple SR module that further\ntakes advantage of both low- and high-level encoded fea-\ntures without excessive increasing model size.\n• We comprehensively evaluate our approach (S2 Trans-\nformer) on the MSCOCO benchmark. Experimental re-\nsults demonstrate that our method performs best while\nmaintaining the small model size.\n2 S2 Transformer\nIn this section, we present a novel Spatial- and Scale-aware\nTransformer (S2 Transformer) for image captioning. The\noverview of the architecture is depicted in Fig. 2.\n2.1 Overview\nGiven an image I, the task of image captioning is to auto-\nmatically generate a description D about visual contents in\nimages, following the paradigm of an encoder-decoder frame-\nwork. Technically, S2 Transformer ﬁrst applies a feature ex-\ntraction to obtain gird features G = {gm}M\nm=1 about an im-\nage, where Mindicates the number of grids. As for the spatial\ninformation loss caused by ﬂattening operation when feed-\ning Ginto an encoder-decoder model, our proposed Spatial-\naware Pseudo-supervised (SP ) module is adopted to im-\nplicitly learn possible and discriminative regions to obtain\npseudo-region features P:\nP = {pn}N\nn=1 = SP(G); (1)\nwhere N means the number of pseudo regions. Then, we use\nthe same encoder to exploit the visual information of original\ngrid features Gand pseudo-region featuresP simultaneously:\n\u0016G= Encoder(G);\n\u0016P = Encoder(P); (2)\nwhere the Encoder is consistent with the vanilla Trans-\nformer’s encoder without any modiﬁcations, which consists\nof two main components: Multi-head Self-Attention (MSA),\nand Feed Forward Network (FFN). Note that for the sake of\nconcise expression, positional encoding, residual operation\nand layer normalization are omitted.\nDifferent from previous Transformer-based models, which\nonly feed the encoded feature obtained from the top encoder\nlayer to the decoder, our proposed Scale-wise Reinforce-\nment (SR) module is to simultaneously explore both low-\nand high-level encoded features to obtain augmented encoded\nfeatures V:\nVG = SR( \u0016G1; \u0016G2;:::; \u0016GL);\nVP = SR( \u0016P1; \u0016P2;:::; \u0016PL); (3)\nwhere \u0016GL (or \u0016PL) means the output of L-th encoder layers\nand VG and VP represent grid and pseudo-region augmented\nencoded features, respectively. Finally, we fuse VG and VP\nto obtain the ﬁnal encoded features V∗and feed it to the de-\ncoder:\nV∗= [VG; VP]WV;\nD= Decoder(V∗); (4)\nwhere [; ] means the operation of concatenate, WV is a learn-\nable parameter and the decoder is the same as the vanilla\nTransformer’s decoder. The detail of our two main compo-\nnents (SP and SR) is described in the next subsection.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1609\nGrid Features Pseudo-region Features\nEncoder\nLayer 1\nEncoder\nLayer 2\nEncoder\nLayer 3\nEncoder\nLayer 1\nEncoder\nLayer 2\nEncoder\nLayer 3\nDecoder\nLayer 3\nDecoder\nLayer 2\nDecoder\nLayer 1\nConcat\nMLP\n<bos> A man holding a white surfboard standing in the ocean.\nA man holding a white surfboard standing in the ocean. <eos> Concat\nMLP\n......\nSpatial-aware Pseudo-supervised\nScale-aware \nReinforcement\nFeature\nExtraction\nImage\nPositional \nEncoding\n(Shared)(Shared)\nScale-aware \nReinforcement\nFigure 2: Overview of our proposed S2 Transformer architecture for image captioning. It consists of ﬁve main components: Feature\nextraction, Encoder, Decoder, Spatial-aware Pseudo-supervised (SP), and Scale-aware Reinforcement (SR), where the encoder and decoder\nboth adopt the same as that of the vanilla Transformer without any modiﬁcation. SP resorts to feature clustering to help preserve spatial\ninformation for grid features while SRsimultaneously explores both low- and high-level encoded features. Note that both two encoders and\ntwo SRsrespectively share parameters.\n2.2 Spatial-aware Pseudo-supervised (SP) Module\nAs discussed above, directly operating at grid features leads\nto the loss of spatial information of regions. A plain idea is\nintroducing region features to compensate for the deﬁciency.\nHowever, combining gird and region features will inevitably\nincrease the computational complexity of the model. Intu-\nitively, if we implicitly select and aggregate discrete grid fea-\ntures into several sub-spaces to obtain pseudo-region features,\nthis operation would become more ﬂexible. Motivated by this\nspirit, we propose a SP module to cluster the grid features\nwith multiple centroids without explicit supervision. The pur-\npose of these centroids is to integrate grids features of similar\nsemantic information together to represent possible and dis-\ncriminative regions.\nFormally, in SP, we ﬁrst design N learnable clusters as\nC = {c1;:::;c N}. Following [Arandjelovic et al., 2016], we\ncalculate the similarity between grid features and clusters by\ndot-product. Given each grid feature gm, it can be mapped to\nthe n-th cluster in the following manner:\nrm;n = exp(gmcT\nn + bn)\nPN\nk=1 exp(gmcT\nk + bk)\n; (5)\nwhere b{n;k}is a trainable parameter. The feature represen-\ntation of each center pn is obtained by a weighted integration\nof all grid features:\npn = Norm(\nMX\nm=1\nrm;n(gm −~cn)); (6)\nwhere “Norm” means `2-normalization operation and ~cn is\na learnable parameter which has the same size as cn. Thus,\nwe deﬁne the ﬁnal features P as pseudo-region features.\n2.3 Scale-aware Reinforcement (SR) Module\nRecently, transformer-based captioning models have been\nproved helpful for image captioning. However, existing mod-\nels neglect the low-level semantic information from the bot-\ntom of the encoder layer during the decoding process. Al-\nthough [Cornia et al., 2020] has provided a solution with a\ncomplex meshed cross-attention, we further propose a novel\nand simple SR module to address the above limitations by in-\ncorporating all features from each encoding layer into the top\nfeatures.\nFor simplicity, we take gird features as an example. Specif-\nically, given the output features ( \u0016G1; \u0016G2;:::; \u0016GL) of each en-\ncoder layer, we ﬁrst concatenate them all together:\n~G= [ \u0016G1;:::; \u0016GL]: (7)\nThen, to integrate both low- and high-level visual informa-\ntion, we employ a Multi-Layer Perception (MLP) which can\nweigh the contribution of features of each layer:\nG\n0\n= ( ~GWT\n1 )WT\n2 ; (8)\nwhere W1 and W2 are trainable projection matrices.\nSince the output of the top encoder layer contains more\nimportant visual information, to prevent the insertion of addi-\ntional noise perturbations, we add featuresG\n0\nto \u0016GL to obtain\nthe ﬁnal grid augmented encoded features VG:\nVG = GL + \u0015G\n0\n; (9)\nwhere the \u0015is an adjustable weighting factor. In a same way,\nwe obtain the pseudo-region augmented encoded featuresVP.\n2.4 Training\nGenerally, the training of captioning model is split into two\nstages [Rennie et al., 2017; Zhang et al., 2021]. In the ﬁrst\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1610\nSP SR B@1 B@4\nM R C S Param. ↓\n% % 80.9 38.9 29.0\n58.5 131.2 22.7 33.57M\n\" % 81.3 39.6 29.4 59.0 133.2 22.7 33.59M (↑0.02)\n% \" 81.0 39.5 29.4 58.9 133.0 22.8 34.62M (↑1.05)\n\" \" 81.1 39.6 29.6 59.1\n133.5 23.2 34.64M (↑1.07)\nTable 1: Ablation studies of the proposed Spatial-aware Pseudo-\nsupervised (SP ) module and Scale-aware Reinforcement (SR )\nmodule.\nModel B@1 B@4 M\nR C S FLOPs\nG 80.9 38.9 29.0 58.5\n131.2 22.7 0.92G\nR 80.0 38.8 28.7 58.5 130.2 22.3 0.76G\nP 80.3 38.2 28.5 57.9 127.6 22.5 0.35G\nG+ R 81.0 39.0 29.2 58.7\n131.5 22.7 1.35G\nG+ P 81.3 39.6 29.4 59.0 133.2 22.7 0.96G\nTable 2: Ablation studies of different visual features. All models\nboth adopt vanilla transformer without SR. G, Rand P represent\ngird features, region features and our pseudo-region features, respec-\ntively.\nstage, we utilize cross-entropy loss to optimize our model:\nLCE = −\nTX\nt=1\nlog(p\u0012(w∗\nt|w∗\n1:t−1)); (10)\nwhere T is the length of word sequence and w∗\n1:t−1 is the\nground truth tokens in the description D.\nIn the second stage, we adopt the strategy of reinforcement\nlearning, which exploits the CIDEr score as reward r(·)with\nself-critical sequence training [Rennie et al., 2017]:\nLRL = −Ew1:T p\u0012[r(w1:T)]: (11)\nIn addition, we employ the gradient expression in [Cornia et\nal., 2020], which computes the reward baseline of the reward\nby the mean operation of rewards, rather than greedy decod-\ning. A sample’s gradient expression is deﬁned as:\n8\n>>\n<\n>>:\nb= 1\nk(\nkP\ni\nr(wi));\n∇\u0012LRL ≈− 1\nk\nkP\ni=1\n((r(wi\n1:T) −b)∇\u0012log p\u0012(wi\n1:T));\n(12)\nwhere k is the number of sampled sequences, wi\n1:T denotes\nthe i-th sampled sequence, and b represents the average re-\nward earned by the sampled sequences.\n3 Experiments\n3.1 Experimental Settings\nDataset and Metric. We conduct experiments to verify the\neffectiveness of our proposed S2 Transformer on commonly-\nused image captioning dataset, i.e., MS-COCO. It consists of\n123,287 images, each associated with ﬁve different descrip-\ntions. In ofﬂine testing, we follow the setting in [Karpathy\nand Fei-Fei, 2015 ], where 113,287 images, 5,000 images,\nand 5,000 images are used as train, validation, and test set,\nFigure 3: Ablation studies of cluster number N inSP and weighting\nfactor \u0015in SR. Note that (a) and (b) only use SP and SR, respec-\ntively.\nModel B@1 B@4 M\nR C S Param.↓\nTransformer 80.9 38.9\n29.0 58.5 131.2 22.7 33.57M\nAoA Transformer 80.8 39.1 29.1 59.1 130.3 22.7 87.37M (↑53.80)\nM2 Transformer 80.8 38.9 29.1 58.5 131.8 22.7 38.66M (↑5.09)\nX-Transformer 81.0 39.7 29.1 59.0 130.2 22.8 56.94M (↑23.37 )\nRSTNet 81.1 39.3 29.4 58.8 133.3 23.0 156.31M (↑122.74)\nOurs 81.1 39.6 29.6 59.1 133.5 23.2 34.64M\n(↑1.07)\nTable 3: Comparing with the state of the art on ResNext101 grid\nfeatures.\nrespectively. The online evaluation is done on the COCO on-\nline test server, where ground-truth annotations of 40,775 im-\nages are not publicly provided. We measure the captioning\nperformance using the standard evaluation metrics, includ-\ning BLEU [Papineni et al., 2002], METEOR [Banerjee and\nLavie, 2005], ROUGR [Lin, 2004], CIDEr [Vedantam et al.,\n2015], and SPICE [Anderson et al., 2016].\nImplementation Details. Following [Zhang et al., 2021], we\nadopt the same pre-trained Faster-RCNN [Ren et al., 2015]\nprovided by [Jiang et al., 2020] to extract grid features, where\nthe gird shape is 7×7 and the dimension of each grid is\n2,048. In practice, our encoder and decoder both have 3 lay-\ners, where each layer uses 8 self-attention heads and the inner\ndimension of FFN is 2,048. The number of cluster centers N\nis 5 and the hyper-parameter \u0015= 0:2 in Eq. 9.\nWe employ Adam optimizer to train all models and set\nbatch size as 50. For cross-entropy (CE) training, we set\nthe minimum epoch as 15. If CIDEr drops in 5 consecu-\ntive epochs, we will choose the model with the best CIDEr\nscore for self-critical sequence training. Speciﬁcally, we use\nan epoch decay schedule to adjust the learning rate for CE by\nModel B@1 B@4 M\nR C S\nSCST - 34.2 26.7\n55.7 114.0 -\nUp-Down 79.8 36.3 27.7 56.9 120.1 21.4\nRFNet 79.1 36.5 27.7 57.3 121.9 21.2\nGCN-LSTM 80.5 38.2 28.5 58.3 127.6 22.0\nSGAE 80.8 38.4 28.4 58.6 127.8 22.1\nORT 80.5 38.6 28.7 58.4 128.3 22.6\nAoANet 80.2 38.9 29.2 58.8 129.8 22.4\nM2 Transformer 80.8 39.1 29.2 58.6 131.2 22.6\nTCIC 80.9 39.7 29.2 58.6 132.9 22.4\nX-Transformer 80.9 39.7 29.5 59.1 132.8 23.4\nRSTNet 81.1 39.3 29.4 58.8 133.3 23.0\nOurs 81.1 39.6 29.6 59.1 133.5 23.2\nTable 4: Performance comparision with the state-of-the-art on the\nMS-COCO “Karpathy” test split.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1611\nModel B@1 B@2 B@3\nB@4 METEOR ROUGE-L CIDEr-D\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nSCST 78.1 93.7 61.9\n86.0 47.9 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7\nUp-Down 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5\nRFNet 80.4 95.0 64.9 89.3 50.1 80.1 38.0 69.2 28.8 37.2 58.2 37.1 122.9 125.1\nGCN-LSTM 80.8 95.9 65.5 89.3 50.8 80.3 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5\nSGAE 81.0 95.3 65.6 89.5 50.7 80.4 38.5 69.7 28.2 37.2 58.6 73.6 123.8 126.5\nETA 81.2 95.0 65.5 89.0 50.9 80.4 38.9 70.2 28.6 38.0 58.6 73.9 122.1 124.4\nAoANet 81.0 95.0 65.8 89.6 51.4 81.3 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6\nM2 Transformer 81.6 96.0 66.4 90.8 51.8 82.7 39.7 72.8 29.4 39.0 59.2 74.8 129.3 132.1\nX-Transformer (ResNet-101) 81.3 95.4 66.3 90.0 51.9 81.7 39.9 71.8 29.5 39.0 59.3 74.9 129.3 131.4\nX-Transformer (SENet-154) 81.9 95.7 66.9 90.5 52.4 82.5 40.3 72.4 29.6 39.2 59.5 75.0 131.1 133.5\nRSTNet (ResNext101) 81.7 96.2 66.5 90.9 51.8 82.7 39.7 72.5 29.3 38.7 59.2 74.2 130.1 132.4\nRSTNet (ResNext152) 82.1 96.4 67.0 91.3 52.2 83.0 40.0 73.1 29.6 39.1 59.5 74.6 131.9 134.0\nOurs (ResNext101) 81.9 96.4 66.7 91.3 52.1 83.1 40.0 73.1 29.5 39.2 59.2\n74.7 131.5 134.5\nOurs (ResNext152) 82.2 96.5 67.0 91.4 52.4 83.3 40.1 73.5 29.6 39.3 59.5 75.0 132.6 135.0\nTable 5: Leaderboard of the published state-of-the-art image captioning models on the MS-COCO online testing server.\nfollowing [Zhang et al., 2021]:\nlr =\n8\n><\n>:\nn=4 ×1e-4; n ≤3;\n1e-4; 3 <n ≤10;\n0:2 ×1e-4; 10 <n ≤12;\n0:2 ×0:2 ×1e-4; otherwise;\n(13)\nwhere n denotes the number of current epoch. For self-\ncritical sequence training, the learning rate is ﬁxed at 5×1e-7.\n3.2 Ablation Studies\nThe core of our proposed S2 Transformer is to generate\nthe high-quality visual descriptions by introducing a spatial-\naware pseudo-supervised (SP ) module and a scale-wise re-\ninforcement (SR) module into a vanilla transformer model.\nIn this section, we conduct comprehensive ablation studies to\nprove the effectiveness of our method.\nEffect of SP and SR. Tab. 1 gives the results of four con-\ntrol experiments to investigate the impact of our proposedSP\nand SR modules: i) baseline: adapting vanilla transformer\nmodel without any modiﬁcations, ii) baseline+SP : integrat-\ning SP into baseline, iii) baseline+SR: integrating SR into\nbaseline, and iv) baseline+SP+SR: integrating both SP and\nSRinto baseline. Obviously, the performances are enhanced\nby individually adding SP and SR to the baseline, partic-\nularly improving 2.0 and 1.8 points on CIDEr, respectively.\nMoreover, the combination of the two components achieves\nfurther improvement. Also, we report the parameters of each\nmodel for measuring its complexity. SP and SR slightly in-\ncrease parameters by 0.02M and 1.05M compared with the\nbaseline. To sum up, our proposed components achieve huge\nimprovements with a small computational cost, indicating our\nmethods’ effectiveness.\nEffect of Pseudo-region feature. In Tab. 2, we execute sev-\neral experiments to examine the effect of different visual fea-\ntures, including grid features (G), region features (R) and our\npseudo-region features (P). All models both adopt a vanilla\ntransformer. Using only a single feature, our P performs\nworst, which indicates that only using pseudo-region features\nmay lose some important visual information. Combining two\nfeatures (i.e., G+P and G+R) can bring performance gains.\nMeanwhile, G+P obtains more signiﬁcant improvement than\nG+ R, thus indicating the practicality of our pseudo-region\nfeatures. Besides, in terms of FLOPs, G+ R brings an ex-\ncessive increase of 0.43G while G+P brings a slight increase\nof 0.04G. The results demonstrate that the highly abstract\npseudo-region features are sufﬁcient and complementary for\ngrid features instead of directly using explicit region features.\nEffect of N in SP. To determine how many pseudo regions\nthe model needs to learn, we set the range of cluster number\nN from 3 to 9 as shown in Fig. 3a. Note that our SP is\nserving for high-level semantic information extraction. From\nthe ﬁgure, we can observe that if N is too large, it may be\ndifﬁcult for the model to ﬁnd discriminative pseudo regions,\nwhich harms the performance of the model. On the contrary,\nif N is too small, much weak semantics will be discarded\nin large quantities, resulting in poor results. Our approach\nachieves the best results with N = 5 clusters.\nEffect of \u0015in SR. To choose the best weighting factor \u0015in\nEq. 9, we conduct a series of experiments by setting the dif-\nferent values of \u0015. The results are shown in Fig. 3b. We ﬁnd\nthat the performance drastically drops with the increase of \u0015\nand the best results are obtained when\u0015= 0:2. It reveals that\ntoo larger \u0015introduces more redundant noise for the decoder.\nThus, we set \u0015= 0:2 in the ﬁnal model.\nFair comparison with strong transformer-based baselines.\nFor a fair comparison, we report experimental results uti-\nlizing the same ResNext101 grid feature as the visual input\nshown in Tab. 3. All models are based on improved ver-\nsions of the vanilla transformer. Speciﬁcally, our method\nachieves state-of-the-art performance on most metrics except\nB@4, which demonstrates superior performance without the\ninterference of diverse features. Moreover, compared to the\nSOTA method RSTNet, which increases the Transformer pa-\nrameters by 122.74M, our model brings only slight growth of\n1.07M on parameters. It further demonstrates that our method\ncan effectively and efﬁciently incorporate grid features with\ntransformer-based architecture.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1612\nGT1: A cat that is sitting next to a laptop.\nTrans: An animal sitting on a desk.\nOurs: An orange cat sitting on a desk next to a laptop.\nGT2: Cat sitting right next to keyboard on laptop.\nGT3: The orange cat is sitting on the laptop.\nGT1: A bundle of fruit on a wooden table.\nTrans: A bunch of bananas and pineapple.\nOurs: A bunch of bananas on a table.\nGT2: A big bunch of green bananas is on a table.\nGT3: Green plantain sitting on a dining table.\nGT1: A man takes a picture of food in a restaurant.\nTrans: A man is eating and taking a picture of food.\nOurs: A man taking a picture of a pizza with a camera.\nGT2: A man taking a picture of his meal at diner table.\nGT3: A man taking a photo of food on a table.\nGT1: A close up of a stop sign in a small town\nTrans: A stop sign sitting on road.\nOurs: A red stop sign on the side of a street.\nGT2: A red stop sign sitting next to country road.\nGT3: A close up of a stop sign in a small town.\nGT1: Four men are  posing for picture at an event.\nTrans: A group of men standing next to each other.\nOurs: Four older men posing for picture at event.\nGT2: A group of man standing next to each other.\nGT3: A group of four older men posing for a photo.\nGT1: A long line of pans that are hanging on the wall.\nTrans: A lot of pots and pans on the counter.\nOurs: A kitchen with pots and pans hanging on the wall.\nGT2: A kitchen with pots hanging over the stove.\nGT3: Hanging frying pans in commercial kitchen.\nFigure 4: Visualization of the proposed S2 Transformer. Each example consists of a raw image, a learned map of cluster indices by SP, the\nground-truth descriptions, and the generated description by the transformer and ours. The size of these learned maps is 7\u00027.\n3.3 Quantitative Analysis\nCompared Methods. In this section, we compare our pro-\nposed S2 Transformer with the state-of-art methods both on\nofﬂine and online evaluation, including SCST [Rennie et al.,\n2017], Up-Down [Anderson et al., 2018], RFNet [Jiang et al.,\n2018], GCN-LSTM [Yao et al., 2018], SGAE [Yang et al.,\n2019], ORT [Herdade et al., 2019], AoANet [Huang et al.,\n2019], M2Transformer [Cornia et al., 2020], X-Transformer\n[Pan et al., 2020 ], TCIC [Fan et al., 2021 ] and RSTNet\n[Zhang et al., 2021].\nOfﬂine Evaluation. In Tab. 4, we show the image captioning\nresults of our method and compare it to the aforementioned\ncompetitors on the ofﬂine test split. Overall, our method\noutperforms all compared methods in terms of B@1, M, R,\nC, and S. Speciﬁcally, compared with the best counterpart\nRSTNet using extra knowledge from a pre-trained language\nmodel, our method yields better gains on all metrics, demon-\nstrating the superiority of our approach.\nOnline Evaluation. To further verify the beneﬁt of our S2\nTransformer, we estimate it on the online COCO test server.\nFollowing the compared methods, we integrate the results\nof four models with different initialization for testing. The\ncomparison results are summarized in Tab. 5. It is clear that\nour S2 Transformer outperforms state-of-the-art models on\nmost metrics. Particularly, with respect to the best competi-\ntor RSTNet (ResNext152), our method S2 Transformer with\nResNext152 achieves improvements of 0.7 and 1.0 CIDEr\npoints on 5 reference captions (c5) and 40 reference captions\n(c40), respectively.\n3.4 Visualization\nFig. 4 provides some qualitative results to show the pseudo\nregions learned via the proposed SP in heat maps and the\nhigh-quality descriptions generated by our proposed model.\nIn the heat map, different colors represent different index val-\nues, which indicate different pseudo-regions. As we can see,\nSP focuses on speciﬁc visual regions in foregrounds but also\nreserves discriminative background information, conﬁrming\nthe usefulness of exploiting pseudo regions to retain the spa-\ntial information. Besides, our model can generate more accu-\nrate and diverse descriptions compared to basic transformer\nmodel. More visualizations are included in the supplemen-\ntary material.\n4 Conclusion\nIn the paper, we study how to effectively and efﬁciently in-\ncorporate grid features with transformer-based architecture\nfor image captioning. To achieve this target, we propose a\nS2 Transformer—a simple yet effective approach that implic-\nitly learns pseudo regions through a series of learnable clus-\nters in a SP module and simultaneously explores both low-\nand high-level encoded features in a SR module. Notice-\nably, pseudo regions can effectively capture spatial informa-\ntion lost by the ﬂattening operation of gird features. Exten-\nsive experiments on the MSCOCO benchmark and visualiza-\ntion analysis conﬁrm the effectiveness and interpretability of\nour method. Besides, our approach does not bring excessive\nparameters compared with the vanilla transformer.\nBroader Impact. Our paper focuses on learning image cap-\ntioning tasks, which has broader application in real-world\nscenarios such as human-machine interaction and visual-\nimpaired assistance. Our method provides positive impacts,\nincluding 1) implicitly learning discriminative region features\ninstead of using explicit region features, which reduces the\nincrease in parameters and computation, and 2) providing a\nsimple task-speciﬁc transformer-based model, which gener-\nates more high-quality descriptions. However, it is still chal-\nlenging to deploy existing models into real-world scenarios\nbecause of their susceptibility to attacks, which remains our\nresponsibility to grow awareness of these potential dangers.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1613\nAcknowledgments\nThis work is supported by the National Natural Science Foun-\ndation of China (Grant No. 62020106008, No. 62122018,\nNo. 61772116, No. 61872064), Sichuan Science and Tech-\nnology Program (Grant No.2019JDTD0005).\nReferences\n[Anderson et al., 2016] Peter Anderson, Basura Fernando,\nMark Johnson, and Stephen Gould. Spice: Semantic\npropositional image caption evaluation. In ECCV, 2016.\n[Anderson et al., 2018] Peter Anderson, Xiaodong He, Chris\nBuehler, Damien Teney, Mark Johnson, Stephen Gould,\nand Lei Zhang. Bottom-up and top-down attention for im-\nage captioning and visual question answering. In CVPR,\n2018.\n[Arandjelovic et al., 2016] Relja Arandjelovic, Petr Gronat,\nAkihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad:\nCnn architecture for weakly supervised place recognition.\nIn CVPR, 2016.\n[Banerjee and Lavie, 2005] Satanjeev Banerjee and Alon\nLavie. Meteor: An automatic metric for mt evaluation\nwith improved correlation with human judgments. In ACL\nworkshop, 2005.\n[Chen et al., 2021] Wenqing Chen, Jidong Tian, Caoyun\nFan, Hao He, and Yaohui Jin. Dependent multi-task learn-\ning with causal intervention for image captioning. In IJ-\nCAI, 2021.\n[Cornia et al., 2019] Marcella Cornia, Lorenzo Baraldi, and\nRita Cucchiara. Show, control and tell: A framework for\ngenerating controllable and grounded captions. In CVPR,\n2019.\n[Cornia et al., 2020] Marcella Cornia, Matteo Stefanini,\nLorenzo Baraldi, and Rita Cucchiara. Meshed-memory\ntransformer for image captioning. In CVPR, 2020.\n[Fan et al., 2021] Zhihao Fan, Zhongyu Wei, Siyuan Wang,\nRuize Wang, Zejun Li, Haijun Shan, and Xuanjing Huang.\nTCIC: theme concepts learning cross language and vision\nfor image captioning. In IJCAI, 2021.\n[Herdade et al., 2019] Simao Herdade, Armin Kappeler,\nKoﬁ Boakye, and Joao Soares. Image captioning: Trans-\nforming objects into words. In NeurIPS, 2019.\n[Huang et al., 2019] Lun Huang, Wenmin Wang, Jie Chen,\nand Xiao-Yong Wei. Attention on attention for image cap-\ntioning. In ICCV, 2019.\n[Jiang et al., 2018] Wenhao Jiang, Lin Ma, Yu-Gang Jiang,\nWei Liu, and Tong Zhang. Recurrent fusion network for\nimage captioning. In ECCV, 2018.\n[Jiang et al., 2020] Huaizu Jiang, Ishan Misra, Marcus\nRohrbach, Erik Learned-Miller, and Xinlei Chen. In de-\nfense of grid features for visual question answering. In\nCVPR, 2020.\n[Karpathy and Fei-Fei, 2015] Andrej Karpathy and Li Fei-\nFei. Deep visual-semantic alignments for generating im-\nage descriptions. In CVPR, 2015.\n[Krishna et al., 2017] Ranjay Krishna, Yuke Zhu, Oliver\nGroth, Justin Johnson, Kenji Hata, Joshua Kravitz,\nStephanie Chen, Yannis Kalantidis, Li-Jia Li, David A.\nShamma, Michael S. Bernstein, and Li Fei-Fei. Visual\ngenome: Connecting language and vision using crowd-\nsourced dense image annotations. IJCV, 2017.\n[Li et al., 2019] Guang Li, Linchao Zhu, Ping Liu, and\nYi Yang. Entangled transformer for image captioning. In\nICCV, 2019.\n[Lin, 2004] Chin-Yew Lin. Rouge: A package for automatic\nevaluation of summaries. In ACL, 2004.\n[Pan et al., 2020] Yingwei Pan, Ting Yao, Yehao Li, and Tao\nMei. X-linear attention networks for image captioning. In\nCVPR, 2020.\n[Papineni et al., 2002] Kishore Papineni, Salim Roukos,\nTodd Ward, and Wei-Jing Zhu. Bleu: a method for au-\ntomatic evaluation of machine translation. In ACL, 2002.\n[Ren et al., 2015] Shaoqing Ren, Kaiming He, Ross B. Gir-\nshick, and Jian Sun. Faster R-CNN: towards real-time ob-\nject detection with region proposal networks. In NeurIPS,\n2015.\n[Rennie et al., 2017] Steven J Rennie, Etienne Marcheret,\nYoussef Mroueh, Jerret Ross, and Vaibhava Goel. Self-\ncritical sequence training for image captioning. In CVPR,\n2017.\n[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and\nQuoc V Le. Sequence to sequence learning with neural\nnetworks. In NeurIPS, 2014.\n[Vedantam et al., 2015] Ramakrishna Vedantam,\nC Lawrence Zitnick, and Devi Parikh. Cider: Consensus-\nbased image description evaluation. In CVPR, 2015.\n[Vinyals et al., 2016] Oriol Vinyals, Alexander Toshev,\nSamy Bengio, and Dumitru Erhan. Show and tell: Lessons\nlearned from the 2015 mscoco image captioning chal-\nlenge. TPAMI, 2016.\n[Wang et al., 2020] Ziwei Wang, Zi Huang, and Yadan Luo.\nHuman consensus-oriented image captioning. In IJCAI,\n2020.\n[Xu et al., 2015] Kelvin Xu, Jimmy Ba, Ryan Kiros,\nKyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,\nRich Zemel, and Yoshua Bengio. Show, attend and tell:\nNeural image caption generation with visual attention. In\nICML, 2015.\n[Yang et al., 2019] Xu Yang, Kaihua Tang, Hanwang Zhang,\nand Jianfei Cai. Auto-encoding scene graphs for image\ncaptioning. In CVPR, 2019.\n[Yao et al., 2018] Ting Yao, Yingwei Pan, Yehao Li, and Tao\nMei. Exploring visual relationship for image captioning.\nIn ECCV, 2018.\n[Zhang et al., 2021] Xuying Zhang, Xiaoshuai Sun, Yun-\npeng Luo, Jiayi Ji, Yiyi Zhou, Yongjian Wu, Feiyue\nHuang, and Rongrong Ji. Rstnet: Captioning with adap-\ntive attention on visual and non-visual words. In CVPR,\n2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1614",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9324595928192139
    },
    {
      "name": "Computer science",
      "score": 0.7913269996643066
    },
    {
      "name": "Transformer",
      "score": 0.7806144952774048
    },
    {
      "name": "Grid",
      "score": 0.5509874820709229
    },
    {
      "name": "Artificial intelligence",
      "score": 0.536675751209259
    },
    {
      "name": "Feature extraction",
      "score": 0.45401185750961304
    },
    {
      "name": "Source code",
      "score": 0.4322197437286377
    },
    {
      "name": "Language model",
      "score": 0.4131524860858917
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3673873245716095
    },
    {
      "name": "Computer vision",
      "score": 0.3505905270576477
    },
    {
      "name": "Image (mathematics)",
      "score": 0.28040310740470886
    },
    {
      "name": "Engineering",
      "score": 0.0800122618675232
    },
    {
      "name": "Programming language",
      "score": 0.07365047931671143
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}