{
  "title": "Dual-decoder Transformer for Joint Automatic Speech Recognition and Multilingual Speech Translation",
  "url": "https://openalex.org/W3102811925",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2151311963",
      "name": "Hang Le",
      "affiliations": [
        "Université Grenoble Alpes",
        "Laboratoire d'Informatique de Grenoble",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2106672292",
      "name": "Juan Pino",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2677230107",
      "name": "Changhan Wang",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2500878826",
      "name": "Jiatao Gu",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2777519208",
      "name": "Didier Schwab",
      "affiliations": [
        "Université Grenoble Alpes",
        "Centre National de la Recherche Scientifique",
        "Laboratoire d'Informatique de Grenoble"
      ]
    },
    {
      "id": "https://openalex.org/A2237966808",
      "name": "Laurent Besacier",
      "affiliations": [
        "Laboratoire d'Informatique de Grenoble",
        "Université Grenoble Alpes",
        "Centre National de la Recherche Scientifique"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4230872509",
    "https://openalex.org/W4301980136",
    "https://openalex.org/W4404389177",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W3034571331",
    "https://openalex.org/W2963779652",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2114483840",
    "https://openalex.org/W3032433061",
    "https://openalex.org/W2998386507",
    "https://openalex.org/W3034332156",
    "https://openalex.org/W2966095117",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W3008125272",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2951456627",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W3105681039",
    "https://openalex.org/W2949328740",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W3008549139",
    "https://openalex.org/W3043665049",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2605131327",
    "https://openalex.org/W2945700568",
    "https://openalex.org/W4300558631",
    "https://openalex.org/W3015698636",
    "https://openalex.org/W3037217258",
    "https://openalex.org/W3037465386",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2936969148",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2407080277",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W197865394",
    "https://openalex.org/W2963542740"
  ],
  "abstract": "We introduce dual-decoder Transformer, a new model architecture that jointly performs automatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based on the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each responsible for one task (ASR or ST). Our major contribution lies in how these decoders interact with each other: one decoder can attend to different information sources from the other via a dual-attention mechanism. We propose two variants of these architectures corresponding to two different levels of dependencies between the decoders, called the parallel and cross dual-decoder Transformers, respectively. Extensive experiments on the MuST-C dataset show that our models outperform the previously-reported highest translation performance in the multilingual settings, and outperform as well bilingual one-to-one results. Furthermore, our parallel models demonstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code and pre-trained models are available at https://github.com/formiel/speech-translation.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 3520–3533\nBarcelona, Spain (Online), December 8-13, 2020\n3520\nDual-decoder Transformer for Joint Automatic Speech Recognition and\nMultilingual Speech Translation\nHang Le1 Juan Pino2 Changhan Wang2\nJiatao Gu2 Didier Schwab1 Laurent Besacier1\n1Univ. Grenoble Alpes, CNRS, LIG 2Facebook AI\n{hang.le, didier.schwab, laurent.besacier}@univ-grenoble-alpes.fr\n{juancarabina, changhan, jgu}@fb.com\nAbstract\nWe introduce dual-decoder Transformer, a new model architecture that jointly performs auto-\nmatic speech recognition (ASR) and multilingual speech translation (ST). Our models are based\non the original Transformer architecture (Vaswani et al., 2017) but consist of two decoders, each\nresponsible for one task (ASR or ST). Our major contribution lies in how these decoders interact\nwith each other: one decoder can attend to different information sources from the other via a\ndual-attention mechanism. We propose two variants of these architectures corresponding to two\ndifferent levels of dependencies between the decoders, called theparallel and cross dual-decoder\nTransformers, respectively. Extensive experiments on the MuST-C dataset show that our models\noutperform the previously-reported highest translation performance in the multilingual settings,\nand outperform as well bilingual one-to-one results. Furthermore, our parallel models demon-\nstrate no trade-off between ASR and ST compared to the vanilla multi-task architecture. Our code\nand pre-trained models are available at https://github.com/formiel/speech-translation.\n1 Introduction\nWhile cascade speech-to-text translation (ST) systems operate in two steps: source language automatic\nspeech recognition (ASR) and source-to-target text machine translation (MT), recent works have at-\ntempted to build end-to-end ST without using source language transcription during decoding (Bérard et\nal., 2016; Weiss et al., 2017; Bérard et al., 2018). After two years of extensions to these pioneering\nworks, the last results of the IWSLT 2020 shared task on ofﬂine speech translation (Ansari et al., 2020)\ndemonstrate that end-to-end models are now on par (if not better) than their cascade counterparts. Such\na ﬁnding motivates even more strongly the works on multilingual (one-to-many, many-to-one, many-to-\nmany) ST (Gangi et al., 2019; Inaguma et al., 2019; Wang et al., 2020a) for which end-to-end models are\nwell adapted by design. Moreover, of these two approaches: cascade proposes a very loose integration\nof ASR and MT (even if lattices or word confusion networks were used between ASR and MT before\nend-to-end models appeared) while most end-to-end approaches simply ignore ASR subtask, trying to\ndirectly translate from source speech to target text. We believe that these are two edge design choices\nand that a tighter coupling of ASR and MT is desirable for future end-to-end ST applications, in which\nthe display of transcripts alongside translations can be beneﬁcial to the users (Sperber et al., 2020).\nThis paper addresses multilingual ST and investigates more closely the interactions between speech\ntranscription (ASR) and speech translation (ST) in a multilingual end-to-end architecture based on Trans-\nformer. While those interactions were previously investigated as a simple multi-task framework for a\nbilingual case (Anastasopoulos and Chiang, 2018), we propose a dual-decoder with an ASR decoder\ntightly coupled with an ST decoder and evaluate its effectiveness on one-to-many ST. Our model is in-\nspired by Liu et al. (2020), but the interaction between ASR and ST decoders is much tighter. 1 Finally,\nexperiments show that our model outperforms theirs on the MuST-C benchmark (Di Gangi et al., 2019).\nOur contributions are summarized as follows: (1) a new model architecture for joint ASR and mul-\ntilingual ST; (2) an integrated beam search decoding strategy which jointly transcribes and translates,\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n1The model of Liu et al. (2020) does not have interaction between internal hidden states of the decoders (c.f . Section 3.4).\n3521\nand that is extended to a wait- k strategy where the ASR hypothesis is ahead of the ST hypothesis by\nktokens and vice-versa; and (3) competitive performance on the MuST-C dataset in both bilingual and\nmultilingual settings and improvements on previous joint ASR/ST work.\n2 Related Work\nMultilingual ST Multilingual translation (Johnson et al., 2016) consists in translating between dif-\nferent language pairs with a single model, thereby improving maintainability and the quality of low\nresource languages. Gangi et al. (2019) adapt this method to one-to-many multilingual speech transla-\ntion by adding a language embedding to each source feature vector. They also observe that using the\nsource language (English) as one of the target languages improves performance. Inaguma et al. (2019)\nsimplify the previous approach by prepending a target language token to the decoder and apply it to\none-to-many and many-to-many speech translation. They do not investigate many-to-one due to the lack\nof a large corpus for this. To ﬁll this void, Wang et al. (2020a) release the CoV oST dataset for ST from\n11 languages into English and demonstrate the effectiveness of many-to-one ST.\nJoint ASR and ST Joint ASR and ST decoding was ﬁrst proposed by Anastasopoulos and Chiang\n(2018) through a multi-task learning framework. Chuang et al. (2020) improve multitask ST by using\nword embedding as an intermediate level instead of text. A two-stage model that performs ﬁrst ASR and\nthen passes the decoder states as input to a second ST model was also studied previously (Anastasopou-\nlos and Chiang, 2018; Sperber et al., 2019). This architecture is closer to cascaded translation while\nmaintaining end-to-end trainability. Sperber et al. (2020) introduce the notion of consistency between\ntranscripts and translations and propose metrics to gauge it. They evaluate different model types for the\njoint ASR and ST task and conclude that end-to-end models with coupled inference procedure are able to\nachieve strong consistency. In addition to existing models having coupled architectures, they also inves-\ntigate a model where the transcripts are concatenated to the translations, and the shared encoder-decoder\nnetwork learns to predict this concatenated outputs. It should be noted that our models have lower latency\ncompared to this approach since the concatenation of outputs makes the two tasks sequential in nature.\nOur work is closely related to that of Liu et al. (2020) who propose an interactive attention mechanism\nwhich enables ASR and ST to be performed synchronously. Both ASR and ST decoders do not only rely\non their previous outputs but also on the outputs predicted in the other task. We highlight three differ-\nences between their work and ours: (a) we propose a more general framework in which (Liu et al., 2020)\nis a special case; (b) tighter integration of ASR and ST is proposed in our work; and (c) we experiment\nin a multilingual ST setting while previous works on joint ASR and ST only investigated bilingual ST.\n3 Dual-decoder Transformer for Joint ASR and Multilingual ST\nWe now present the proposed dual-decoder Transformer for jointly performing ASR and multilingual ST.\nOur models are based on the Transformer architecture (Vaswani et al., 2017) but consist of two decoders.\nEach decoder is responsible for one task (ASR or ST). The intuition is that the problem at hand consists\nin solving two different tasks with different characteristics and different levels of difﬁculty (multilingual\nST is considered more difﬁcult than ASR). Having different decoders specialized in different tasks may\nthus produce better results. In addition, since these two tasks can be complementary, it is natural to allow\nthe decoders to help each other. Therefore, in our models, we introduce a dual-attention mechanism: in\naddition to attending to the encoder, the decoders also attend to each other.\n3.1 Model overview\nThe model takes as input a sequence of speech features x = (x1,x2,...,x Tx) in a speciﬁc source lan-\nguage (e.g. English) and outputs a transcription y = (y0,y1,...,y Ty ) in the same language as well as\ntranslations z1,z2,..., zM in M different target languages (e.g. French, Spanish, etc.). When M = 1,\nthis corresponds to joint ASR and bilingual ST (Liu et al., 2020). For simplicity, our presentation consid-\ners only a single target language with outputz = (z0,z1,...,z Tz ). All results, however, apply to the gen-\neral multilingual case. In the sequel, denote y<t ≜ (y0,y1,...,y t−1) and y>t ≜ (yt+1,yt+2,...,y Ty )\n3522\n(yt is included if “<” and “>” are replaced by “≤” and “≥” respectively). In addition, assume that yt is\nignored if tis outside of the interval [0,Ty]. Notations apply to z as well.\nThe dual-decoder model jointly predicts the transcript and translation in an autoregressive fashion:\np(y,z |x) =\nmax(Ty,Tz)∏\nt=0\np(yt,zt |y<t,z<t,x). (1)\nA natural model would consist of a single decoder followed by a softmax layer. However, even if the\ncapacity of the decoder were large enough for handling both ASR and ST generation, a single softmax\nwould require a very large joint vocabulary (with size VyVz where Vy and Vz are respectively the vocab-\nulary sizes for y and z). Instead, our dual-decoder consists of two sub-decoders that are specialized in\nproducing outputs tailored to the ASR and ST tasks separately. Formally, our model predicts the next\noutput tokens (ˆys,ˆzt) (where 1 ≤s≤Ty,1 ≤t≤Tz) given a pair of previous outputs (y<s,z<t) as:\nhy\ns,hz\nt = DECODER dual(y<s,z<t,ENCODER (x)) ∈Rdy ×Rdz , (2)\np(ys |y<s,z<t,x) = [softmax(Wyhy\ns + by)]ys, ˆys = argmaxys p(ys |y<s,z<t,x), (3)\np(zt |y<s,z<t,x) = [softmax(Wzhz\nt + bz)]zt, ˆzt = argmaxzt p(zt |y<s,z<t,x), (4)\nwhere [v]i denotes the ith element of the vector v. Note that ys and zt are token indices ( 1 ≤ys ≤\nVy,1 ≤zt ≤Vz). In (3) and (4), we detail the intermediate quantities p(ys |·) and p(zt |·) as obtained\nfrom the probability distributions over the output vocabulary. In the above, we have made an important\nassumption about the joint probability p(ys,zt | ·) that it can be factorized into p(ys | ·)p(zt | ·).\nTherefore, the joint distribution (1) encoded by the dual-decoder Transformer can be rewritten as\np(y,z |x) =\nmax(Ty,Tz)∏\nt=0\np(yt |y<t,z<t,x)p(zt |y<t,z<t,x). (5)\nWe also assumed so far that the sub-decoders start at the same time, which is the most basic conﬁguration.\nIn practice, however, one may allow one sequence to advance ksteps compared to the other, known as\nthe wait-kpolicy (Ma et al., 2019). For example, if ST waits for ASR to produce its ﬁrst ktokens, then\nthe joint distribution becomes\np(y,z |x) =p(y<k |x)p(y≥k,z |y<k,x) (6)\n=\nk−1∏\nt=0\np(yt |y<t,x)\nmax(Ty−k,Tz)∏\nt=0\np(yt+k |y<t+k,z<t,x)p(zt |y<t+k,z<t,x). (7)\nIn the next section, we propose two concrete architectures for the dual-decoder, corresponding to\ndifferent levels of dependencies between the two sub-decoders (ASR and ST). Then, we show that several\nknown models in the literature are special cases of these architectures.\n3.2 Parallel and cross dual-decoder Transformers\nThe ﬁrst architecture is called parallel dual-decoder Transformer, which has the highest level of depen-\ndencies: one decoder uses the hidden states of the other to compute its outputs, as illustrated in Figure 1a.\nThe encoder consists of an input embedding layer followed by a positional embedding and a number of\nself-attention and feed-forward network (FFN) layers whose inputs are normalized (Ba et al., 2016). 2\nThis is almost the same as the encoder of the original Transformer (Vaswani et al., 2017) (we refer to\nthe corresponding paper for further details), except that the embedding layer in our encoder is a small\nconvolutional neural network (CNN) (Fukushima and Miyake, 1982; LeCun et al., 1989) of two layers\nwith ReLU activations and a stride of 2, therefore reducing the input length by 4.\n2All the illustrations in this paper are for the so-called pre-LayerNorm conﬁguration, in which the input of the layer is\nnormalized. Likewise, if the output is normalized instead, the conﬁguration is called post-LayerNorm. Since pre-LayerNorm is\nknown to perform better than post-LayerNorm (Wang et al., 2019; Nguyen and Salazar, 2019), we only conducted experiments\nfor the former, although our implementation supports both.\n3523\nM×\nN×\nSpeech inputs\nConv2D\nEmbedding\nLayerNorm\nAttention\nLayerNorm\nFFN\nLayerNorm\nOutputs (shifted right)\nOutput\nEmbedding\nLayerNorm\nAttention\nLayerNorm\nAttention\nLayerNorm\nFFN\nLayerNorm\nLinear\nSotfmax\nAttention\nMerge\nAttention\nMerge\nASR probabilities\nOutputs (shifted right)\nOutput\nEmbedding\nLayerNorm\nAttention\nLayerNorm\nAttention\nLayerNorm\nFFN\nLayerNorm\nLinear\nSotfmax\nAttention\nMerge\nAttention\nMerge\nST probabilities\n(a) The parallel dual-decoder Transformer\nN×N×\nA\nM\nA\nM\nA\nA\nA\nM\nA\nM\nA\nA\n(b) Parallel\nN×\nL\nN×\nA\nM\nA\nM\nA\nA\nA\nM\nA\nM\nA\nA\nL\n(c) Cross\nFigure 1:The dual-decoder Transformers. Figure (a) shows the detailed architecture of theparallel dual-decoder\nTransformer, and Figure (b) shows its simpliﬁed view. Thecross dual-decoder Transformer is very similar to the\nparallel one, except that the keys and values fed to the dual-attention layers come from the previous output, which\nis illustrated by Figure (c). From the above ﬁgures, one can easily infer the detailed architecture of the cross\nTransformer, which can be found in the Appendix. Abbreviations: A (Attention), M (Merge), L (LayerNorm).\nThe parallel dual-decoder consists of: (a) two decoders that follow closely the common Transformer\ndecoder structure, and (b) four additional multi-head attention layers (calleddual-attention layers). Each\ndual-attention layer is complementary to a corresponding main attention layer. We recall that an at-\ntention layer receives as inputs a query Q ∈ Rdk, a key K ∈ Rdk, a value V ∈ Rdv and outputs\nAttention(Q,K,V).3 A dual-attention layer receivesQ from the main branch and K,V from the other\ndecoder at the same level ( i.e. at the same depth in Transformer architecture) to compute hidden repre-\nsentations that will be merged back into the main branch (i.e. one decoder attends to the other in parallel).\nWe present in more detail this merging operation in Section 3.4.\nOur second proposed architecture is called cross dual-decoder Transformer, which is similar to the\nprevious one, except that now the dual-attention layers receive K,V from the previous decoding step\noutputs of the other decoder, as illustrated in Figure 1c. Thanks to this design, each prediction step can\nbe performed separately on the two decoders. The hidden representations hy\ns,hz\nt in (2) produced by the\ndecoders can thus be decomposed into:4\nhy\ns = DECODER asr(y<s,z<t,ENCODER (x)) ∈Rdy , (8)\nhz\nt = DECODER st(z<t,y<s,ENCODER (x)) ∈Rdz . (9)\n3Following Vaswani et al. (2017), we use multi-head attention, in which the inputs are linearly projected multiple times\nbefore feeding to the function, then the outputs are concatenated and projected back to the original dimension.\n4This decomposition is clearly not possible for the parallel dual-decoder Transformer.\n3524\n3.3 Special cases\nIn this section, we present some special cases of our dual-decoder architecture and discuss their links to\nexisting models in the literature.\nIndependent decoders When there is no dual-attention, the two decoders become independent. In\nthis case, the prediction joint probability can be factorized simply as p(ys,zt |y<s,z<t,x) = p(ys |\ny<s,x)p(zt | z<t,x). Therefore, all prediction steps are separable and thus this model is the most\ncomputationally efﬁcient. In the literature, this model is often referred to as multi-task (Anastasopoulos\nand Chiang, 2018; Sperber et al., 2020).\nChained decoders Another special case corresponds to the extreme wait- k policy, in which one de-\ncoder waits for the other to completely ﬁnish before starting its own decoding. For example, if ST\nwaits for ASR, then the prediction joint probability reads p(ys,zt |y<s,z<t,x) =p(ys |y<s,x)p(zt |\nz<t,y,x).This model is called triangle in previous work (Anastasopoulos and Chiang, 2018; Sperber et\nal., 2020). A special case of this model is when the second decoder in the chain is not directly connected\nto the encoder, also referred to as two-stage5 (Sperber et al., 2019; Sperber et al., 2020).\nTo summarize the different cases, we show below the joint probability distributions encoded by the\npresented models, in decreasing level of dependencies:\n(single output) p(y,z |x) =\nT∏\nt=0\np(yt,zt |y<t,z<t,x), (10)\n(dual-decoder) p(y,z |x) =\nT∏\nt=0\np(yt |y<t,z<t,x)p(zt |y<t,z<t,x), (11)\n(chained) p(y,z |x) =\nT∏\nt=0\np(yt |y<t,x)p(zt |z<t,y,x), (12)\n(independent) p(y,z |x) =\nT∏\nt=0\np(yt |y<t,x)p(zt |z<t,x), (13)\nwhere T = max(Ty,Tz). Similar formalization for the wait- kpolicy (7) can be obtained in a straight-\nforward manner. Note that for independent decoders, the distribution is the same as in non-wait-k.\n3.4 Variants\nThe previous section presents special cases of our formulation at a high level. In this section, we intro-\nduce different ﬁne-grained variants of the dual-decoder Transformers used in the experiments (Section 5).\nAsymmetric dual-decoder Instead of using all the dual-attention layers, one may want to allow a\none-way attention: either ASR attends ST or the inverse, but not both.\nAt-self or at-source dual-attention In each decoder block, there are two different attention layers,\nwhich we respectively call self-attention (bottom) and source-attention6 (top). For each, there is an\nassociated dual-attention, named respectively dual-attention at self and dual-attention at source. In the\nexperiments, we study the case where either only the at-self or at-source attention layers are retained.\nMerging operators The Merge layers shown in Figure 1 combine the outputs of the main attention\nHmain and the dual-attention Hdual. We experimented dual-attention with two different merging opera-\ntors: weighted sum or concatenation. We can formally deﬁne the merging operators as\nHout = Merge(Hmain,Hdual) ≜\n\n\n\nHmain if no dual-attention,\nHmain + λHdual, if sum operator,\nlinear ([Hmain; Hdual]) if concat operator.\n(14)\n5Also called cascade by Anastasopoulos and Chiang (2018). We omit this term to avoid confusion with the common cascade\nmodels that are typically not trained end-to-end. Note that our chained-decoder (both triangle and two-stage) are end-to-end.\n6Also referred to as cross-attention in the literature. We use a different name to avoid confusion with thecross dual-decoder.\n3525\nFor the sum operator, in particular, we perform experiments for learnable or ﬁxed λ.\nRemark. The model proposed by Liu et al. (2020) is a special case of our cross dual-decoder Trans-\nformer with no dual-attention at source, no layer normalization for the input embeddings (Figure 1c),\nand sum merging with ﬁxed λ.\n4 Training and Decoding\n4.1 Training\nThe objective, L(ˆy,ˆz,y,z) =αLasr(ˆy,y) + (1−α)Lst(ˆz,z), is a weighted sum of the cross-entropy\nASR and ST losses, where (ˆy,ˆz) and (y,z) denote the predictions and the ground-truths for (ASR,\nST), respectively. The weight α is set to 0.3 in all experiments. Here we favor the ST task based on\nthe intuition that it is more difﬁcult to train than the ASR one, simply because of the multilinguality.\nA hyperparameter search may further improve the results. We also employ label smoothing (Szegedy\net al., 2016) with ϵ = 0.1. For each language pair, training data is sorted by the number of frames.\nEach mini-batch contains all languages such that their numbers of frames are roughly the same. We\nfollow Inaguma et al. (2019) and prepend a language-speciﬁc token to the target sentence. Preliminary\nexperiments showed that this approach was more effective than adding a target language embedding\nalong the temporal dimension to the speech feature inputs (Di Gangi et al., 2019).\n4.2 Decoding\nWe present the beam search strategy used by our model. Since there are two different outputs (ASR and\nST), one may naturally think about two different beams (with possibly some interactions). However, we\nfound that a single joint beam works best for our model. In this beam search strategy, each hypothesis\nincludes a tuple of ASR and ST sub-hypotheses. The two sub-hypotheses are expanded together and\nthe score is computed based on the sum of log probabilities of the output token pairs. For a beam size\nB, the B best hypotheses are retained based on this score. In this setup, both sub-hypotheses evolve\njointly, which resembles the training process more than in the case of two different beams. A limitation\nof this joint-beam strategy is that, in extreme cases, one of the task (ASR or ST) may only have a single\nhypothesis. Indeed, at a decoding step t+ 1, we take the best Bpredictions (ˆyt,ˆzt) in terms of their sum\nof scores s(yt,zt) ≜ log p(yt |y<t,z<t) + logp(zt |y<t,z<t); it can happen that, e.g., some ˆyt has a so\ndominant score that it is selected for all the hypotheses, i.e. the B(different) hypotheses have a single ˆyt\nand Bdifferent ˆzt. We leave the design of a joint-beam strategy with enforced diversity to future work.\nFinally, to produce translations for multiple target languages in our system, it sufﬁces to feed different\nlanguage-speciﬁc tokens to the dual-decoder at decoding time.\n5 Experiments\n5.1 Dataset\nTo build a one-to-many model that can jointly transcribe and translate, we use MuST-C (Di Gangi et al.,\n2019), which is currently the largest publicly available one-to-many speech translation dataset.7 MuST-C\ncovers language pairs from English to eight different target languages including Dutch, French, German,\nItalian, Portuguese, Romanian, Russian, and Spanish. Each language direction includes a triplet of source\ninput speech, source transcription, and target translation, with size ranging from 385 hours (Portuguese)\nto 504 hours (Spanish). We refer to the original paper for more details.\n5.2 Implementation details\nOur implementation is based on the ESPnet-ST toolkit (Inaguma et al., 2020). 8 In the following, we\nprovide details for reproducing the results. The pipeline is identical for all experiments.\n7Smaller datasets include Europarl-ST (Iranzo-Sánchez et al., 2020) and MaSS (Boito et al., 2020). Recently, a very large\nmany-to-many dataset called CoV oST-2 (Wang et al., 2020b) has been released, while its predecessor CoV oST (Wang et al.,\n2020a) only covers the many-to-one scenario.\n8https://github.com/espnet/espnet\n3526\nModels All experiments use the same encoder architecture with 12 layers. The decoder has 6 layers,\nexcept for the independent-decoder model where we also include a 8-layer version ( independent++)\nto compare the effects of dual-attention against simply increasing the number of model parameters.\nText pre-processing Transcriptions and translations were normalized and tokenized using the Moses\ntokenizer (Koehn et al., 2007). The transcription was lower-cased and the punctuation was stripped. A\njoint BPE (Sennrich et al., 2016) with 8000 merge operations was learned on the concatenation of the\nEnglish transcription and all target languages. We also experimented with two separate dictionaries (one\nfor English and another for all target languages), but found that the results are worse.\nSpeech features We used Kaldi (Povey et al., 2011) to extract 83-dimensional features (80-channel\nlog Mel ﬁlter-bank coefﬁcients and 3-dimensional pitch features) that were normalized by the mean\nand standard deviation computed on the training set. Following common practice (Inaguma et al., 2019;\nWang et al., 2020c), utterances having more than 3000 frames or more than 400 characters were removed.\nFor data augmentation, we used speed pertubation (Ko et al., 2015) with three factors of 0.9, 1.0, and\n1.1 and SpecAugment (Park et al., 2019) with three types of deterioration including time warping ( W),\ntime masking (T) and frequency masking (F), where W = 5,T = 40, and F = 30.\nOptimization Following standard practice for training Transformer, we used the Adam opti-\nmizer (Kingma and Ba, 2015) with Noam learning rate schedule (Vaswani et al., 2017), in which the\nlearning rate is linearly increased for the ﬁrst 25K warm-up steps then decreased proportionally to the\ninverse square root of the step counter. We set the initial learning rate to1e−3 and the Adam parameters\nto β1 = 0.9,β2 = 0.98,ϵ = 1e−9. We used a batch size of 32 sentences per GPU, with gradient accu-\nmulation of 2 training steps. All models were trained on a single-machine with 8 32GB GPUs for 250K\nsteps unless otherwise speciﬁed. As for model initialization, we trained an independent-decoder model\nwith the two decoders having shared weights for 150K steps and used its weights to initialize the other\nmodels. This resulted in much faster convergence for all models. We also included this shared model in\nthe experiments, and for a fair comparison, we trained it for additional 250K steps. Finally, for decoding,\nwe used a beam size of 10 with length penalty of 0.5.9\n5.3 Results and analysis\nIn this section, we report detokenized case-sensitive BLEU10 (Papineni et al., 2002) on the MuST-Cdev\nsets (Table 1). Results on the test sets are discussed in Section 5.4. Following previous work (Inaguma\net al., 2020), we remove non-verbal tokens in evaluation.11 In Table 1, there are 3 main groups of mod-\nels, corresponding to independent-decoder, cross dual-decoder ( crx), and parallel dual-decoder ( par),\nrespectively. In particular, independent++ corresponds to a 8-decoder-layer model and will serve as\nour strongest baseline for comparison. Figure 2 shows the relative performance of some representative\nmodels with respect to this baseline, together with their validation accuracies. In the following, when\ncomparing models, we implicitly mean “on average” (over the 8 languages), except otherwise speciﬁed.\nParallel vs. cross Under the same conﬁgurations, parallel models outperform their cross counterparts\nin terms of translation (line 5 vs. line 13, line 6 vs. line 14, and line 7 vs. line 16), showing an improve-\nment of 0.7 BLEU on average. In terms of recognition, however, the parallel architecture has on average\na 0.33% higher (worse) WER compared to the cross models. On the other hand, parallel dual-decoders\nperform better than independent decoders in both translation and recognition tasks, except for the asym-\nmetric case (line 12), the at-self and at-source with sum merging conﬁguration (line 17), and the wait-k\nmodel where ST is ahead of ASR (line 19). This shows that both the translation and recognition tasks\ncan beneﬁt from the tight interaction between the two decoders, i.e. it is possible to achieve no trade-off\nbetween BLEUs and WERs for the parallel models compared to the independent architecture. This is\nnot the case, however, for the cross dual-decoders that feature weaker interaction than the parallel ones.\n9For a hypothesis of length L, a length penalty pmeans a score of pLwill be added to the (log probability) score of that\nhypothesis (Section 4.2). Therefore, longer hypotheses are favored, or equivalently, shorter hypotheses are “penalized”.\n10We also tried sacreBLEU (Post, 2018) and found that the results are identical.\n11This is for a fair comparison with the results of Inaguma et al. (2020), presented in Section 5.4.\n3527\nNo type side self src merge paramsde es fr it nl pt ro ru avg WER\n1 independent (shared) 31.3M 19.40 27.77 24.65 19.93 21.53 24.24 18.19 10.9920.84 14.2\n2 independent 44.8M 20.11 28.18 25.61 20.76 21.83 25.45 18.45 11.3121.46 12.6\n3 independent++ 51.2M 20.25 29.48 26.10 21.05 22.3426.71 19.67 12.10 22.21 12.9\n4 crx st - ✓ sum 46.4M 20.01 28.57 25.86 20.66 22.26 25.36 19.06 12.0021.72 12.7\n5 crx both - ✓ concat 51.2M 20.36 28.51 25.80 21.18 22.10 25.24 19.55 11.89 21.83 12.3\n6 crx both - ✓ sum 48.0M 19.99 28.87 26.09 20.94 21.67 25.42 18.85 11.83 21.71 12.2\n7 crx both ✓ ✓ concat 54.3M 20.07 28.73 26.01 20.93 22.5925.60 19.08 12.46 21.93 12.4\n8 crx both ✓ ✓ sum 51.2M 20.38 28.90 26.64 21.07 22.61 26.23 19.44 12.12 22.17 12.1\n9 crx⋆ both ✓ - sum 48.0M 19.72 27.96 25.49 20.52 21.56 25.01 18.53 11.3321.26 12.8\n10 crx⋆ both ✓ - sum † 48.0M 18.62 27.11 24.41 19.73 20.47 24.49 17.23 11.0920.39 12.8\n11 crx⋆ both ✓ ✓ sum 51.2M 19.54 28.17 25.68 20.95 21.55 24.77 18.76 11.2821.34 12.3\n12 par st ✓ ✓ concat 49.6M 20.57 28.84 26.08 20.85 22.11 25.70 19.36 11.9021.93 13.0\n13 par both - ✓ concat 51.2M 20.84 29.51 26.44 21.53 22.68 25.94 19.04 12.60 22.32 12.5\n14 par both - ✓ sum 48.0M 20.85 29.18 26.38 22.14 22.87 26.49 19.70 12.74 22.54 12.7\n15 par both ✓ - sum 48.0M 20.56 29.21 26.54 21.07 22.51 25.75 19.64 12.80 22.26 12.8\n16 par both ✓ ✓ concat 54.3M 21.22 29.50 26.66 21.74 22.76 26.66 20.25 12.79 22.70 12.7\n17 par both ✓ ✓ sum 51.2M 20.95 28.67 26.45 21.31 22.29 25.87 19.53 12.24 22.16 12.8\n18 par both R3 - ✓ sum 48.0M 21.22 30.12 26.53 22.06 23.37 26.59 19.82 12.54 22.78 12.6\n19 par both T3 - ✓ sum 48.0M 20.35 28.61 25.94 21.22 22.12 25.19 19.36 11.9921.85 13.6\n⋆no normalization for dual-attention input, †sum merging has λ= 0.3 ﬁxed,\nR3ASR is 3 steps ahead of ST, T3ST is 3 steps ahead of ASR.\nTable 1: BLEU and (average) WER on MuST-C dev set. In the second column ( type), crx and parallel\ndenote the cross and parallel dual-decoder, respectively. In the third column (side), st means only ST attends to\nASR. Line 1 corresponds to the independent-decoder model where the weights of the two decoders are shared,\nand independent++ corresponds to the model with 8 decoder layers (instead of 6). It should be noted that\nline 10 corresponds to the model proposed by Liu et al. (2020). The values that are better than the baseline\n(independent++) are underlined and colored in blue, while the best values are highlighted in bold.\nInterestingly, there is a slight trade-off between the parallel and cross designs: the parallel models are\nbetter in terms of BLEUs but worse in terms of WERs. This is to some extent similar to previous work\nwhere models having different types of trade-offs between BLEUs and WERs (He et al., 2011; Sperber\net al., 2020; Chuang et al., 2020). It should be emphasized that most of the dual-decoder models have\nfewer or same numbers of parameters compared to independent++. This conﬁrms our intuition that\nthe tight connection between the two decoders in the parallel architecture improves performance. The\ncross dual-decoders perform relatively well compared to the baseline of two independent decoders with\nthe same number of layers (6), but not so well compared to the stronger baseline with 8 layers.\nSymmetric vs. asymmetric In some experiments, we only allow the ST decoder to attend to the ASR\ndecoder. For the cross dual-decoder, this did not yield noticeable improvements in terms of BLEU (21.72\nat line 4 vs. 21.71 at line 6), while for the parallel architecture, the results are worse (21.93 at line 12 vs.\n22.70 at line 16). The symmetric models also outperform the asymmetric counterparts in terms of WER\n(12.7 at line 4 vs. 12.2 at line 6, 13.0 at line 12 vs. 12.7 at line 16). It is conﬁrmed again that the two\ntasks are complementary and can help each other: removing the ASR-to-ST attention hurts performance.\nIn fact, examining the learnableλin the sum merging operator shows that the decoders learn to attend to\neach other, though at different rates. We observed that for the same layer depth, the ST decoder always\nattends more to the ASR one, and for both of them λincreases with the depth of the layer.\nAt-self dual-attentionvs. at-source dual-attention For the parallel dual-decoder, the at-source dual-\nattention produces better results than the at-self counterpart (BLEU:22.54 vs. 22.26, WER: 12.7 vs. 12.8\nat line 14 vs. line 15), while the combination of both does not improve the results (BLEU 22.16, WER\n12.8 at line 17). For the concat merging, using both yields better results in terms of translation but\nslightly hurts the recognition task (BLEU: 22.70 vs. 22.32, WER: 12.7 vs. 12.5 at line 16 vs. line 13).\nSum vs. concat merging The impact of merging operators is not consistent across different models. If\nwe focus on the parallel dual-decoder, sum is better for models with only at-source attention (line 13 vs.\nline 14) and concat is better for models using both at-self and at-source attention (line 16 vs. line 17).\n3528\n0 50 100 150 200 ×103\n0.59\n0.60\n0.61\n0.62\n0.63\n0.64\n0.65\n0.66\n0.67\nparallel-sum\nparallel-concat\ncross-sum\ncross-concat\nindependent++\nindependent\n(a) ST validation accuracy per training step.\npt es ro it fr nl ru de\n−1.5\n−1\n−0.5\n0\n0.5\n1\n(b) Relative BLEU on MuST-Cdev set.\nFigure 2:Results on the MuST-C dev set. The models listed in the legends correspond respectively to lines 17,\n16, 8, 7, 3, 2 in Table 1. The baseline used for relative BLEU is independent++. One can observe that the\nparallel models consistently outperform the others in terms of validation accuracy. Best viewed in color.\nInput normalization and learnable sumSome experiments conﬁrm the importance of normalizing\nthe input fed to the dual-attention layers ( i.e. the LayerNorm layers shown in Figure 1c). The results\nshow that normalization substantially improves the performance (BLEU:22.17 vs. 21.34, WER: 12.1 vs.\n12.3 at line 8 vs. at line 11). It is also beneﬁcial to use learnable weights compared to a ﬁxed value for\nthe sum merging operator (Equation (14)) (BLEU: 21.26 vs. 20.39 at line 9 vs. line 10). Note that the\nﬁxed weight and non-normalization conﬁguration corresponds to the model of Liu et al. (2020) (line 10).\nWait-kpolicy We compare a non-wait-kparallel dual-decoder (line 14) with its wait-k(k = 3) coun-\nterparts. From the results, one can observe that letting ASR be ahead of ST (line 18) improves the\nperformance (BLEU: 22.78 vs. 22.54, WER: 12.6 vs. 12.7), while letting ST be ahead of ASR (line 19)\nconsiderably worsen the results (BLEU: 21.85, WER: 13.6). This conﬁrms our intuition that the ST task\nis more difﬁcult and should not take the lead in the dual-decoder models.\nASR results From the results (last column of Table 1), one can observe that the dual-decoder models\noutperform the baseline indepedent++, except for the asymmetric case and the wait- k model where\nST is 3 steps ahead of ASR. While using a single decoder leads to an average of 14.2% WER, all other\nsymmetric architectures with two decoders (except the ASR-waits-for-ST) have better and rather stable\nWERs (from 12.1% to 13.0%). Detailed results for each data subset are provided in the Appendix.\n5.4 Comparison to state-of-the-art\nTo avoid a hyper-parameter search over the test set, we only select three of our best models together\nwith the baseline independent++ for evaluation. All of the three models are symmetric parallel dual-\ndecoders, the ﬁrst one has at-source dual-attention with sum merging, the second one has both at-self\nand at-source dual-attentions with concat merging, and the last one is a wait- kmodel in which ASR is\n3 steps ahead of ST. These models correspond to lines 5, 6, and 7 of Table 2, and will be referred to\nrespectively as par++, par, and parR3 in the sequel. For par++ we increase the number of decoder\nlayers from 6 to 8, thus increasing the number of parameters from 48M to 51.2M, matching that of the\nbaseline. We do not do this forparR3 (48M) as this model already has a higher latency due to the wait-k.\nAll models are trained for 550K steps, corresponding to 25 epochs. Following Inaguma et al. (2020), we\nuse the average of ﬁve checkpoints with the best validation accuracies on the dev sets for evaluation.\nWe compare the results with the previous work (Gangi et al., 2019) in the multilingual setting. In\naddition, to demonstrate the competitive performance of our models, we also include the best existing\ntranslation performance on MuST-C (Inaguma et al., 2020), although these results were obtained with\nbilingual systems and from a sophisticated training recipe. Indeed, to obtain the results for each language\npair (e.g. en-de), Inaguma et al. (2020) pre-trained an ASR model and an MT model to initialize the\n3529\nNo type side self src merge epochs de es fr it nl pt ro ru avg WER\n1 Bilingual one-to-one (Inaguma et al., 2020) 5022.91 27.96 32.69 23.75 27.43 28.01 21.9015.7525.0512.0\n2 Multilingual one-to-many (Gangi et al., 2019)17.70 20.90 26.50 18.00 20.00 22.60 - - - -\n3 Multilingual one-to-many (Gangi et al., 2019)16.50 18.90 24.50 16.20 17.80 20.80 15.90 9.8017.55-\n4 independent++ 25 22.82 27.20 32.11 23.34 26.67 28.98 21.37 14.3424.6011.6\n5 par++ both - ✓ sum 25 23.63 28.12 33.45 24.18 27.55 29.95 22.8715.2125.6211.4\n6 par both ✓ ✓ concat 25 22.74 27.59 32.86 23.50 26.97 29.51 21.94 14.8825.0011.6\n7 parR3 both - ✓ sum 25 22.84 27.92 32.12 23.61 27.29 29.48 21.16 14.5024.8711.6\nTable 2: BLEU on MuST-C tst-COMMON test set. Line 2 corresponds to the best multilingual models\nof Gangi et al. (2019) trained separately for {de,nl} and {es,fr,it,pt}, while line 3 is a single model trained\non 8 languages. The WER in line 1 corresponds to the best result reported by Inaguma et al. (2020).12\nweights of (respectively) the encoder and decoder for ST training. This means that to obtain the results\nfor the 8 language pairs, 24 independent trainings had to be performed in total (3 for each language pair).\nThe results in Table 2 show that our models achieved very competitive performance compared to\nbilingual one-to-one models (Inaguma et al., 2020), despite being trained for only half the number of\nepochs. In particular, the par++ model achieved the best results, consistently surpassing the others\non all languages (except on Russian where it is outperformed by the bilingual model). Our results also\nsurpassed those of Gangi et al. (2019) by a large margin. We observe the largest improvements on\nPortuguese (+1.94 at line 5, +1.50 at line 6, and +1.47 at line 7, compared to the bilingual result at line\n1), which has the least data among the 8 language pairs in MuST-C. This phenomenon is also common\nin multilingual neural machine translation where multilingual joint training has been shown to improve\nperformance on low-resource languages (Johnson et al., 2017).\n6 Conclusion\nWe introduced a novel dual-decoder Transformer architecture for synchronous speech recognition and\nmultilingual speech translation. Through a dual-attention mechanism, the decoders in this model are at\nthe same time able to specialize in their tasks while being helpful to each other. The proposed model\nalso generalizes previously proposed approaches using two independent (or weakly tied) decoders or\nchaining ASR and ST. It is also ﬂexible enough to experiment with settings where ASR is ahead of ST\nwhich makes it promising for (one-to-many) simultaneous speech translation. Experiments on the MuST-\nC dataset showed that our model achieved very competitive performance compared to state-of-the-art.\nAcknowledgements\nThis work was supported by a Facebook AI SRA grant, and was granted access to the HPC resources of\nIDRIS under the allocations 2020-AD011011695 and 2020-AP011011765 made by GENCI. It was also\ndone as part of the Multidisciplinary Institute in Artiﬁcial Intelligence MIAI@Grenoble-Alpes (ANR-\n19-P3IA-0003). We thank the anonymous reviewers for their insightful feedback.\nReferences\nAntonios Anastasopoulos and David Chiang. 2018. Tied multitask learning for neural speech translation. In\nMarilyn A. Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-\nHLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 82–91. Association\nfor Computational Linguistics. 1, 2, 5\nEbrahim Ansari, amittai axelrod, Nguyen Bach, Ond ˇrej Bojar, Roldano Cattoni, Fahim Dalvi, Nadir Durrani,\nMarcello Federico, Christian Federmann, Jiatao Gu, Fei Huang, Kevin Knight, Xutai Ma, Ajay Nagesh, Matteo\nNegri, Jan Niehues, Juan Pino, Elizabeth Salesky, Xing Shi, Sebastian Stüker, Marco Turchi, Alexander Waibel,\nand Changhan Wang. 2020. FINDINGS OF THE IWSLT 2020 EV ALUATION CAMPAIGN. In Proceedings\nof the 17th International Conference on Spoken Language Translation , pages 1–34, Online, July. Association\nfor Computational Linguistics. 1\n12https://github.com/espnet/espnet/blob/master/egs/must_c/asr1/RESULTS.md\n3530\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450. 3\nAlexandre Bérard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. 2016. Listen and translate: A proof\nof concept for end-to-end speech-to-text translation. In NIPS Workshop on End-to-end Learning for Speech and\nAudio Processing. 1\nAlexandre Bérard, Laurent Besacier, Ali Can Kocabiyikoglu, and Olivier Pietquin. 2018. End-to-end automatic\nspeech translation of audiobooks. CoRR, abs/1802.04200. 1\nMarcely Zanon Boito, William Havard, Mahault Garnerin, Éric Le Ferrand, and Laurent Besacier. 2020. Mass: A\nlarge and clean multilingual corpus of sentence-aligned spoken utterances extracted from the bible. InProceed-\nings of The 12th Language Resources and Evaluation Conference, pages 6486–6493. 6\nShun-Po Chuang, Tzu-Wei Sung, Alexander H. Liu, and Hung-yi Lee. 2020. Worse wer, but better bleu? leverag-\ning word embedding as intermediate in multitask end-to-end speech translation. In Dan Jurafsky, Joyce Chai,\nNatalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 5998–6003. Association for Computa-\ntional Linguistics. 8\nMattia A Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2019. Must-c: a mul-\ntilingual speech translation corpus. In 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 2012–2017. Association for Computational\nLinguistics. 1, 6\nKunihiko Fukushima and Sei Miyake. 1982. Neocognitron: A self-organizing neural network model for a mech-\nanism of visual pattern recognition. In Competition and cooperation in neural nets , pages 267–285. Springer.\n3\nMattia Antonino Di Gangi, Matteo Negri, and Marco Turchi. 2019. One-to-many multilingual end-to-end speech\ntranslation. In IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019, Singapore,\nDecember 14-18, 2019, pages 585–592. IEEE. 1, 9, 10\nXiaodong He, Li Deng, and Alex Acero. 2011. Why word error rate is not a good metric for speech recognizer\ntraining for the speech translation task? In 2011 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 5632–5635. IEEE. 8\nHirofumi Inaguma, Kevin Duh, Tatsuya Kawahara, and Shinji Watanabe. 2019. Multilingual end-to-end speech\ntranslation. In IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019, Singapore,\nDecember 14-18, 2019, pages 570–577. IEEE. 1, 7\nHirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Enrique Yalta Soplin, Tomoki Hayashi, and\nShinji Watanabe. 2020. Espnet-st: All-in-one speech translation toolkit. arXiv preprint arXiv:2004.10234. 6,\n7, 9, 10\nJavier Iranzo-Sánchez, Joan Albert Silvestre-Cerdà, Javier Jorge, Nahuel Roselló, Adrià Giménez, Albert Sanchis,\nJorge Civera, and Alfons Juan. 2020. Europarl-st: A multilingual corpus for speech translation of parliamentary\ndebates. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 8229–8233. IEEE. 6\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fer-\nnanda B. Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google’s\nmultilingual neural machine translation system: Enabling zero-shot translation. CoRR. 2\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda\nViégas, Martin Wattenberg, Greg Corrado, et al. 2017. Google’s multilingual neural machine translation sys-\ntem: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339–351.\n10\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Yoshua Bengio and\nYann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA,\nUSA, May 7-9, 2015, Conference Track Proceedings. 7\nTom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. 2015. Audio augmentation for speech\nrecognition. In Sixteenth Annual Conference of the International Speech Communication Association. 7\n3531\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke\nCowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical ma-\nchine translation. In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration\nsessions, pages 177–180. Association for Computational Linguistics. 7\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. 1989. Backpropagation applied to handwritten zip code recognition. Neural computa-\ntion, 1(4):541–551. 3\nYuchen Liu, Jiajun Zhang, Hao Xiong, Long Zhou, Zhongjun He, Hua Wu, Haifeng Wang, and Chengqing Zong.\n2020. Synchronous speech recognition and speech-to-text translation with interactive decoding. In The Thirty-\nFourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial\nIntelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8417–8424. AAAI Press. 2\nMingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun\nHe, Hairong Liu, Xing Li, Hua Wu, and Haifeng Wang. 2019. STACL: Simultaneous translation with implicit\nanticipation and controllable latency using preﬁx-to-preﬁx framework. In Proc. of ACL. 3\nToan Q Nguyen and Julian Salazar. 2019. Transformers without tears: Improving the normalization of self-\nattention. arXiv preprint arXiv:1910.05895. 3\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua-\ntion of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics, pages 311–318. 7\nDaniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V . Le. 2019.\nSpecaugment: A simple data augmentation method for automatic speech recognition. In Gernot Kubin and\nZdravko Kacic, editors, Interspeech 2019, 20th Annual Conference of the International Speech Communication\nAssociation, Graz, Austria, 15-19 September 2019, pages 2613–2617. ISCA. 7\nMatt Post. 2018. A call for clarity in reporting bleu scores. In Proceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 186–191. 7\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hanne-\nmann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. 2011. The kaldi speech recognition toolkit. In IEEE\n2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society. 7\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword\nunits. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016,\nAugust 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics. 7\nMatthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2019. Attention-passing models for robust\nand data-efﬁcient end-to-end speech translation. Transactions of the Association for Computational Linguistics,\n7:313–325. 2, 5\nMatthias Sperber, Hendra Setiawan, Christian Gollan, Udhyakumar Nallasamy, and Matthias Paulik. 2020. Con-\nsistent transcription and translation of speech. CoRR, abs/2007.12741. 1, 5, 8\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the\ninception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2818–2826. 6\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008. 1, 2, 3, 7\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. 2019. Learning\ndeep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 1810–1822. 3\nChanghan Wang, Juan Pino, Anne Wu, and Jiatao Gu. 2020a. Covost: A diverse multilingual speech-to-text\ntranslation corpus. In Proceedings of The 12th Language Resources and Evaluation Conference , pages 4197–\n4203, Marseille, France, May. European Language Resources Association. 1, 6\nChanghan Wang, Anne Wu, and Juan Pino. 2020b. Covost 2: A massively multilingual speech-to-text translation\ncorpus. arXiv preprint arXiv:2007.10310. 6\n3532\nChengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, and Zhenglu Yang. 2020c. Curriculum pre-training for end-to-end\nspeech translation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020,\npages 3728–3738. Association for Computational Linguistics. 7\nRon J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-to-sequence\nmodels can directly translate foreign speech. In Francisco Lacerda, editor, Interspeech 2017, 18th Annual\nConference of the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017 ,\npages 2625–2629. ISCA. 1\nAppendix\nWe present the detailed model architecture for thecross dual-decoder Transformer in Figure 3. Detailed\nWER results on MuST-C dev set are presented in Table 3.\nM×\nN× N×\nSpeech inputs\nConv2D\nEmbedding\nLayerNorm\nAttention\nLayerNorm\nFFN\nLayerNorm\nOutputs (shifted right)\nOutput\nEmbedding\nLayerNorm\nAttention\nLayerNorm\nAttention\nLayerNorm\nFFN\nLayerNorm\nLinear\nSotfmax\nAttention\nMerge\nAttention\nMerge\nASR probabilities\nLayerNorm\nOutputs (shifted right)\nOutput\nEmbedding\nLayerNorm\nAttention\nLayerNorm\nAttention\nLayerNorm\nFFN\nLayerNorm\nLinear\nSotfmax\nAttention\nMerge\nAttention\nMerge\nST probabilities\nLayerNorm\nFigure 3:The cross dual-decoder Transformer. Unlike the parallel dual-decoder Transformer, here one decoder\nattends to the previous decoding-step outputs of the other and there is no interaction between their hidden states.\n3533\nNo type side self src merge params de es fr it nl pt ro ru avg\n1 independent (shared) 31.3M 14.9 13.8 14.5 13.6 14.0 13.6 14.8 14.0 14.2\n2 independent 44.8M 12.6 12.6 13.1 11.9 12.4 12.3 13.1 12.4 12.6\n3 independent++ 51.2M 13.4 12.5 14.0 12.2 12.7 12.8 13.3 12.5 12.9\n4 crx st - ✓ sum 46.4M 12.8 13.0 13.2 12.8 12.1 12.4 12.2 12.8 12.7\n5 crx both - ✓ concat 51.2M 12.3 12.9 12.4 11.9 12.3 12.3 12.2 12.4 12.3\n6 crx both - ✓ sum 48.0M 12.2 12.6 12.4 12.0 12.3 12.0 12.0 12.2 12.2\n7 crx both ✓ ✓ concat 54.3M 12.5 12.9 12.6 12.0 12.2 12.3 12.4 12.2 12.4\n8 crx both ✓ ✓ sum 51.2M 12.1 12.7 12.1 11.7 12.0 12.0 12.0 12.1 12.1\n9 crx⋆ both ✓ - sum 48.0M 12.7 12.9 13.3 12.5 12.6 12.5 12.6 12.9 12.8\n10 crx⋆ both ✓ - sum † 48.0M 12.7 12.8 13.2 12.3 11.5 13.1 12.7 12.9 12.8\n11 crx⋆ both ✓ ✓ sum 51.2M 12.2 12.6 12.4 11.8 12.1 12.2 12.4 12.4 12.3\n12 par st ✓ ✓ concat 49.6M 13.0 13.7 13.1 12.5 13.1 12.7 13.3 12.9 13.0\n13 par both - ✓ concat 51.2M 12.6 13.0 12.6 12.1 12.4 12.3 12.5 12.5 12.5\n14 par both - ✓ sum 48.0M 12.8 13.0 13.2 12.2 12.8 12.4 12.6 12.6 12.7\n15 par both R3 - ✓ sum 48.0M 13.1 13.4 12.8 12.3 12.9 12.5 12.7 12.9 12.8\n16 par both T3 - ✓ sum 48.0M 14.0 14.3 13.9 13.4 13.9 13.6 13.8 13.5 13.8\n17 par both ✓ - sum 48.0M 12.6 13.1 13.3 12.4 12.6 12.6 12.6 12.8 12.8\n18 par both ✓ ✓ concat 54.3M 12.5 13.3 12.9 12.3 12.7 12.6 12.6 12.5 12.7\n19 par both ✓ ✓ sum 51.2M 12.7 13.4 12.8 12.5 12.8 12.6 12.7 12.7 12.8\nTable 3:Word error rate on MuST-C dev set. The values that are better than the baseline ( independent++)\nare underlined and colored in blue.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8663320541381836
    },
    {
      "name": "Transformer",
      "score": 0.8330204486846924
    },
    {
      "name": "Speech recognition",
      "score": 0.674532413482666
    },
    {
      "name": "Architecture",
      "score": 0.6522245407104492
    },
    {
      "name": "Decoding methods",
      "score": 0.6307289004325867
    },
    {
      "name": "Speech translation",
      "score": 0.5933290123939514
    },
    {
      "name": "Machine translation",
      "score": 0.5711355209350586
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.48645898699760437
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4815816879272461
    },
    {
      "name": "Natural language processing",
      "score": 0.4688849151134491
    },
    {
      "name": "Algorithm",
      "score": 0.09775590896606445
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ]
}